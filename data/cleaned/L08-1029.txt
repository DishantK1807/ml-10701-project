<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Abhishek Arun</author>
<author>Frank Keller</author>
</authors>
<title>Lexicalization in crosslinguistic probabilistic parsing: The case of French</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05</booktitle>
<pages>306--313</pages>
<location>Ann Arbor, Michigan</location>
<contexts>
<context>positional phrase governed by the preposition “`a”. Subcategorization lexicons can benefit many NLP applications. For example, they can be used to enhance tasks such as parsing (Carroll et al., 1998; Arun and Keller, 2005) and semantic classification (Schulte im Walde and Brew, 2002) as well as applications such as information extraction (Surdeanu et al., 2003) and machine translation. Several subcategorization lexico</context>
</contexts>
<marker>Arun, Keller, 2005</marker>
<rawString>Abhishek Arun and Frank Keller. 2005. Lexicalization in crosslinguistic probabilistic parsing: The case of French. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 306–313, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Didier Bourigault</author>
<author>Marie-Paule Jacques</author>
<author>C´ecile Fabre</author>
<author>C´ecile Fr´erot</author>
<author>Sylwia Ozdowska</author>
</authors>
<title>Syntex, analyseur syntaxique de corpus</title>
<date>2005</date>
<booktitle>In Actes des 12`emes journ´ees sur le Traitement Automatique des Langues Naturelles</booktitle>
<location>Dourdan</location>
<marker>Bourigault, Jacques, Fabre, Fr´erot, Ozdowska, 2005</marker>
<rawString>Didier Bourigault, Marie-Paule Jacques, C´ecile Fabre, C´ecile Fr´erot, and Sylwia Ozdowska. 2005. Syntex, analyseur syntaxique de corpus. In Actes des 12`emes journ´ees sur le Traitement Automatique des Langues Naturelles, Dourdan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
</authors>
<title>From grammar to lexicon: Unsupervised learning of lexical syntax</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<pages>19--203</pages>
<contexts>
<context>re recent Lefff (Sagot et al., 2006) and Dicovalence (http://bach.arts.kuleuven.be/ dicovalence/) lexicons. Some work has been conducted on automatic subcategorization acquisition, mostly on English (Brent, 1993; Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006) but increasingly also on other languages, from which German is just one example (Schulte im Walde, 2002). This work has shown that al</context>
</contexts>
<marker>Brent, 1993</marker>
<rawString>Michael R. Brent. 1993. From grammar to lexicon: Unsupervised learning of lexical syntax. Computational Linguistics, 19:203–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th ACL Conference on Applied Natural Language Processing</booktitle>
<pages>356--363</pages>
<location>Washington, DC</location>
<contexts>
<context>., 2006) and Dicovalence (http://bach.arts.kuleuven.be/ dicovalence/) lexicons. Some work has been conducted on automatic subcategorization acquisition, mostly on English (Brent, 1993; Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006) but increasingly also on other languages, from which German is just one example (Schulte im Walde, 2002). This work has shown that although automatically built lexicons are no</context>
<context>ment with this system, and the lexicon resulting from the initial experiment (which is limited to 104 verbs) is not publicly available. Our new system is similar to the system developed in Cambridge (Briscoe and Carroll, 1997; Preiss et al., 2007) in that it extracts SCFs from data parsed using a shallow dependency parser (Bourigault et al., 2005) and is capable of identifying a large number of SCFs. However, unlike the C</context>
<context>comparison point for us is VALEX – a large verb subcategorization lexicon created for English (Korhonen et al., 2006). This lexicon was acquired automatically using the system developed at Cambridge (Briscoe and Carroll, 1997) which identifies 163 SCF types (these abstract over lexically-governed particles and prepositions). The input data used for building VALEX consisted of 904 million words in total. It was extracted f</context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>Ted Briscoe and John Carroll. 1997. Automatic extraction of subcategorization from corpora. In Proceedings of the 5th ACL Conference on Applied Natural Language Processing, pages 356–363, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Ted Briscoe</author>
</authors>
<title>Can subcategorisation probabilities help a statistical parser</title>
<date>1998</date>
<booktitle>In Proceedings of the 6th ACL/SIGDAT Workshop on Very Large Corpora</booktitle>
<location>Montreal (Canada</location>
<contexts>
<context>rase followed by a prepositional phrase governed by the preposition “`a”. Subcategorization lexicons can benefit many NLP applications. For example, they can be used to enhance tasks such as parsing (Carroll et al., 1998; Arun and Keller, 2005) and semantic classification (Schulte im Walde and Brew, 2002) as well as applications such as information extraction (Surdeanu et al., 2003) and machine translation. Several s</context>
</contexts>
<marker>Carroll, Minnen, Briscoe, 1998</marker>
<rawString>John Carroll, Guido Minnen, and Ted Briscoe. 1998. Can subcategorisation probabilities help a statistical parser? In Proceedings of the 6th ACL/SIGDAT Workshop on Very Large Corpora, Montreal (Canada).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paula Chesley</author>
<author>Susanne Salmon-Alt</author>
</authors>
<title>Automatic extraction of subcategorization frames for french</title>
<date>2006</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference (LREC), Genua</booktitle>
<location>Italy</location>
<contexts>
<context>h which is capable of acquiring large scale lexicons from un-annotated corpus data (Messiant, 2008). To our knowledge, only one previously published system exists for SCF acquisition for French SCFs (Chesley and Salmon-Alt, 2006). However, no further work has been published since the initial experiment with this system, and the lexicon resulting from the initial experiment (which is limited to 104 verbs) is not publicly avai</context>
<context>re. These results are shown in table 1, along with: 1) the results obtained with the only previously published work on automatic subcategorization acquisition (from raw corpus data) for French verbs (Chesley and Salmon-Alt, 2006), and 2) those reported with the previous Cambridge system when the system was used to acquire a large SCF lexicon for English with a baseline filtering technique comparable to the one employed in ou</context>
<context>lies on manual effort. Resources built in this matter are not easily adapted to different tasks and domains. As far as we know, the only published work on subcategorization acquisition for French is (Chesley and Salmon-Alt, 2006) which proposes a method to acquire SCFs from a French cross-domain corpus. The work relies on the VISL parser which has an “unevaluated (and potentially high) error rate” while our system relies on </context>
</contexts>
<marker>Chesley, Salmon-Alt, 2006</marker>
<rawString>Paula Chesley and Susanne Salmon-Alt. 2006. Automatic extraction of subcategorization frames for french. In Proceedings of the Language Resources and Evaluation Conference (LREC), Genua (Italy).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Gardent</author>
<author>Bruno Guillaume</author>
<author>Guy Perrier</author>
<author>Ingrid Falk</author>
</authors>
<date>2005</date>
<booktitle>Maurice Gross’ Grammar Lexicon and Natural Language Processing. In 2nd Language and Technology Conference</booktitle>
<location>Poznan</location>
<contexts>
<context> dictionary including subcategorization information for verbs, adjectives and nouns. It is not ideally suited for computational use but work currently in progress is aimed at addressing this problem (Gardent et al., 2005). Only part of this resource is publicly available. As mentioned earlier, the Tr´esor de la Langue Franc¸aise Informatis´e (TLFI) is derived from a syntax dictionary and (like we noticed with evaluat</context>
</contexts>
<marker>Gardent, Guillaume, Perrier, Falk, 2005</marker>
<rawString>Claire Gardent, Bruno Guillaume, Guy Perrier, and Ingrid Falk. 2005. Maurice Gross’ Grammar Lexicon and Natural Language Processing. In 2nd Language and Technology Conference, Poznan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Gross</author>
</authors>
<title>M´ethodes en syntaxe. Hermann</title>
<date>1975</date>
<location>Paris</location>
<contexts>
<context>ion. Several subcategorization lexicons are available for many languages, but most of them have been built manually. For French these include e.g. the large French dictionnary “Le Lexique Grammaire” (Gross, 1975) and the more recent Lefff (Sagot et al., 2006) and Dicovalence (http://bach.arts.kuleuven.be/ dicovalence/) lexicons. Some work has been conducted on automatic subcategorization acquisition, mostly </context>
<context>y information included in these different lexical resources. 4.1. Dictionaries and lexicons for French The Lexicon-Grammar (LG) is the earliest resource for subcategorization information for French. (Gross, 1975; 3http://infolingu.univ-mlv.fr/ DonneesLinguistiques/Lexiques-Grammaires/ lgpllr.html Gross, 1994) – a manually built dictionary including subcategorization information for verbs, adjectives and noun</context>
</contexts>
<marker>Gross, 1975</marker>
<rawString>Maurice Gross. 1975. M´ethodes en syntaxe. Hermann, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Gross</author>
</authors>
<title>Constructing Lexicon-Grammars. In Computational Approaches to the Lexicon</title>
<date>1994</date>
<pages>213--263</pages>
<publisher>University Press. Anna</publisher>
<location>Oxford. Oxford</location>
<contexts>
<context>ench The Lexicon-Grammar (LG) is the earliest resource for subcategorization information for French. (Gross, 1975; 3http://infolingu.univ-mlv.fr/ DonneesLinguistiques/Lexiques-Grammaires/ lgpllr.html Gross, 1994) – a manually built dictionary including subcategorization information for verbs, adjectives and nouns. It is not ideally suited for computational use but work currently in progress is aimed at addre</context>
</contexts>
<marker>Gross, 1994</marker>
<rawString>Maurice Gross. 1994. Constructing Lexicon-Grammars. In Computational Approaches to the Lexicon, pages 213– 263, Oxford. Oxford University Press. Anna Korhonen, Genevieve Gorrell, and Diana McCarthy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Krymolowski Korhonen</author>
<author>Ted Briscoe</author>
</authors>
<title>Statistical filtering and subcategorization frame acquisition</title>
<date>2000</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Very Large Corpora</booktitle>
<location>Hong Kong. Anna</location>
<marker>Korhonen, Briscoe, 2000</marker>
<rawString>2000. Statistical filtering and subcategorization frame acquisition. In Proceedings of the Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, Hong Kong. Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.</rawString>
</citation>
<citation valid="true">
<title>A large subcategorization lexicon for natural language processing applications</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th international conference on Language Resources and Evaluation</booktitle>
<location>Genova, Italy</location>
<contexts>
<context>s a state-of-the-art lexicon. The type precision and recall scores for each test verb are given in table 2. 2See (Poibeau and Messiant, 2008) for details. Our Chesley &amp; Korhonen work Salmon-Alt &amp; al. (2006) (2006) # test verbs 20 104 183 Precision 0.79 0.87 0.81 Recall 0.55 0.54 0.46 F-Measure 0.65 0.67 0.58 Table 1: Comparison with recent work in French and English Verb # SCFs Precision Recall aimer 5 </context>
<context> been evaluated and discovered accurate by EASY evaluation campaign. We acquired and made publicly available a large subcategorization lexicon for 3268 verbs (336 SCFs) whereas Chesley and Salmon-Alt (2006) only reported an experiment with 104 verbs (27 SCFs). 4.2. The first automatically acquired large scale lexicon for English : VALEX An interesting comparison point for us is VALEX – a large verb subc</context>
<context>ex FrTB 2000 160 ? Lefff mixed 6798 ? ? DV manual 3700 ? 8000 LG manual 5208 ? 13335 Table 3: Comparison of dictionaries and lexicons ’?’ stands for unknown; LS: LexSchem; C&amp;S06: Chesley &amp; Salmon-Alt (2006); DV: DicoValence; LG: LexiconGrammar; LM10: Le Monde 10 years; FrTB: French TreeBank is fairly comprehensive and offers also some ideas for further development of LexSchem. First, five different vers</context>
</contexts>
<marker>2006</marker>
<rawString>2006. A large subcategorization lexicon for natural language processing applications. In Proceedings of the 5th international conference on Language Resources and Evaluation, Genova, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Kup´s´c</author>
</authors>
<title>Extraction automatique de cadres de sous-cat´egorisation verbale pour le franc¸ais `a partir d’un corpus arbor´e</title>
<date>2007</date>
<booktitle>In Actes des 14`emes journ´ees sur le Traitement Automatique des Langues Naturelles</booktitle>
<location>Toulouse</location>
<marker>Kup´s´c, 2007</marker>
<rawString>Anna Kup´s´c. 2007. Extraction automatique de cadres de sous-cat´egorisation verbale pour le franc¸ais `a partir d’un corpus arbor´e. In Actes des 14`emes journ´ees sur le Traitement Automatique des Langues Naturelles, Toulouse, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: a preliminary investigation</title>
<date>1993</date>
<institution>University of Chicago Press, Chicago and London</institution>
<contexts>
<context>thresholds or smoothing using semantic back-off estimates), automatic acquisition of SCFs for other French word classes (e.g. nouns), and automatic classification of verbs using the SCFs as features (Levin, 1993; Schulte im Walde and Brew, 2002). Like mentioned above, we also plan to enhance the lexical entries of the lexicon. It would be useful to include in them information about noun and preposition class</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: a preliminary investigation. University of Chicago Press, Chicago and London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Automatic acquisition of a large subcategorization dictionary from corpora</title>
<date>1993</date>
<booktitle>In Proceedings of the Meeting of the Association for Computational Linguistics</booktitle>
<pages>235--242</pages>
<contexts>
<context>ff (Sagot et al., 2006) and Dicovalence (http://bach.arts.kuleuven.be/ dicovalence/) lexicons. Some work has been conducted on automatic subcategorization acquisition, mostly on English (Brent, 1993; Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006) but increasingly also on other languages, from which German is just one example (Schulte im Walde, 2002). This work has shown that although automati</context>
</contexts>
<marker>Manning, 1993</marker>
<rawString>Christopher D. Manning. 1993. Automatic acquisition of a large subcategorization dictionary from corpora. In Proceedings of the Meeting of the Association for Computational Linguistics, pages 235–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C´edric Messiant</author>
</authors>
<title>Assci : A subcategorization frames acquisition system for french</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL) Student Research Workshop, Colombus, Ohio. Association for Computational Linguistics</booktitle>
<contexts>
<context>SCFs for individual verbs. We have recently developed a system for automatic subcategorization acquisition for French which is capable of acquiring large scale lexicons from un-annotated corpus data (Messiant, 2008). To our knowledge, only one previously published system exists for SCF acquisition for French SCFs (Chesley and Salmon-Alt, 2006). However, no further work has been published since the initial exper</context>
<context>of candidate SCFs for the verb, and a SCF filter which filters out SCFs deemed incorrect. We introduce these modules briefly in the subsequent sections. For a more detailed description of ASSCI, see (Messiant, 2008). 2.1. Preprocessing : Morphosyntactic tagging and syntactic analysis Our system first tags and lemmatizes corpus data using the Tree-Tagger and then parses it using Syntex (Bourigault et al., 2005).</context>
<context> evaluated LexSchem against a gold standard from a dictionary. Although this approach is not ideal (e.g. a dictionary may include SCFs not included in our data, and vice versa – see e.g. (Poibeau and Messiant, 2008) for discussion), it can provide a useful starting point. We chose a set of 20 verbs listed in Appendix to evaluate this resource. These verbs were chosen for their heterogeneity in terms of semantic</context>
<context>ngful. However, their relative similarity seems to suggest that LexSchem is a state-of-the-art lexicon. The type precision and recall scores for each test verb are given in table 2. 2See (Poibeau and Messiant, 2008) for details. Our Chesley &amp; Korhonen work Salmon-Alt &amp; al. (2006) (2006) # test verbs 20 104 183 Precision 0.79 0.87 0.81 Recall 0.55 0.54 0.46 F-Measure 0.65 0.67 0.58 Table 1: Comparison with recen</context>
</contexts>
<marker>Messiant, 2008</marker>
<rawString>C´edric Messiant. 2008. Assci : A subcategorization frames acquisition system for french. In Proceedings of the Association for Computational Linguistics (ACL) Student Research Workshop, Colombus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thierry Poibeau</author>
<author>C´edric Messiant</author>
</authors>
<title>Do we still need gold standard for evaluation</title>
<date>2008</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference (LREC</booktitle>
<location>Marrakech</location>
<contexts>
<context>valuation We evaluated LexSchem against a gold standard from a dictionary. Although this approach is not ideal (e.g. a dictionary may include SCFs not included in our data, and vice versa – see e.g. (Poibeau and Messiant, 2008) for discussion), it can provide a useful starting point. We chose a set of 20 verbs listed in Appendix to evaluate this resource. These verbs were chosen for their heterogeneity in terms of semantic</context>
<context>s is unmeaningful. However, their relative similarity seems to suggest that LexSchem is a state-of-the-art lexicon. The type precision and recall scores for each test verb are given in table 2. 2See (Poibeau and Messiant, 2008) for details. Our Chesley &amp; Korhonen work Salmon-Alt &amp; al. (2006) (2006) # test verbs 20 104 183 Precision 0.79 0.87 0.81 Recall 0.55 0.54 0.46 F-Measure 0.65 0.67 0.58 Table 1: Comparison with recen</context>
</contexts>
<marker>Poibeau, Messiant, 2008</marker>
<rawString>Thierry Poibeau and C´edric Messiant. 2008. Do we still need gold standard for evaluation ? In Proceedings of the Language Resources and Evaluation Conference (LREC), Marrakech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judita Preiss</author>
<author>Ted Briscoe</author>
<author>Anna Korhonen</author>
</authors>
<title>A system for large-scale acquisition of verbal, nominal and adjectival subcategorization frames from corpora</title>
<date>2007</date>
<booktitle>In Proceedings of the Meeting of the Association for Computational Linguistics</booktitle>
<pages>912--918</pages>
<location>Prague</location>
<contexts>
<context>the lexicon resulting from the initial experiment (which is limited to 104 verbs) is not publicly available. Our new system is similar to the system developed in Cambridge (Briscoe and Carroll, 1997; Preiss et al., 2007) in that it extracts SCFs from data parsed using a shallow dependency parser (Bourigault et al., 2005) and is capable of identifying a large number of SCFs. However, unlike the Cambridge system (and </context>
</contexts>
<marker>Preiss, Briscoe, Korhonen, 2007</marker>
<rawString>Judita Preiss, Ted Briscoe, and Anna Korhonen. 2007. A system for large-scale acquisition of verbal, nominal and adjectival subcategorization frames from corpora. In Proceedings of the Meeting of the Association for Computational Linguistics, pages 912–918, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoˆıt Sagot</author>
<author>Lionel Cl´ement</author>
</authors>
<title>Eric de La Clergerie, and</title>
<date>2006</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference (LREC), Genua</booktitle>
<location>Italy</location>
<marker>Sagot, Cl´ement, 2006</marker>
<rawString>Benoˆıt Sagot, Lionel Cl´ement, Eric de La Clergerie, and Pierre Boullier. 2006. The lefff 2 syntactic lexicon for french: architecture, acquisition, use. In Proceedings of the Language Resources and Evaluation Conference (LREC), Genua (Italy).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Chris Brew</author>
</authors>
<title>Inducing German Semantic Verb Classes from Purely Syntactic Subcategorisation Information</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>223--230</pages>
<location>Philadelphia, PA</location>
<contexts>
<context>zation lexicons can benefit many NLP applications. For example, they can be used to enhance tasks such as parsing (Carroll et al., 1998; Arun and Keller, 2005) and semantic classification (Schulte im Walde and Brew, 2002) as well as applications such as information extraction (Surdeanu et al., 2003) and machine translation. Several subcategorization lexicons are available for many languages, but most of them have bee</context>
<context>using semantic back-off estimates), automatic acquisition of SCFs for other French word classes (e.g. nouns), and automatic classification of verbs using the SCFs as features (Levin, 1993; Schulte im Walde and Brew, 2002). Like mentioned above, we also plan to enhance the lexical entries of the lexicon. It would be useful to include in them information about noun and preposition classes and morpho-syntactic propertie</context>
</contexts>
<marker>Walde, Brew, 2002</marker>
<rawString>Sabine Schulte im Walde and Chris Brew. 2002. Inducing German Semantic Verb Classes from Purely Syntactic Subcategorisation Information. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 223–230, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>A Subcategorisation Lexicon for German Verbs induced from a Lexicalised PCFG</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd Conference on Language Resources and Evaluation, volume IV</booktitle>
<pages>pages</pages>
<location>Spain</location>
<contexts>
<context>quisition, mostly on English (Brent, 1993; Manning, 1993; Briscoe and Carroll, 1997; Korhonen et al., 2006) but increasingly also on other languages, from which German is just one example (Schulte im Walde, 2002). This work has shown that although automatically built lexicons are not as accurate and detailed as manually built ones, they can be useful for real-world tasks. This is mostly because they provide </context>
</contexts>
<marker>Walde, 2002</marker>
<rawString>Sabine Schulte im Walde. 2002. A Subcategorisation Lexicon for German Verbs induced from a Lexicalised PCFG. In Proceedings of the 3rd Conference on Language Resources and Evaluation, volume IV, pages 1351–1357, Las Palmas de Gran Canaria, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karel van den Eynde</author>
<author>Claire Blanche-Benveniste</author>
</authors>
<title>Syntaxe et m´ecanismes descriptifs : pr´esentation de l’approche pronominale. Cahiers de Lexicologie</title>
<date>1978</date>
<pages>32--3</pages>
<marker>van den Eynde, Blanche-Benveniste, 1978</marker>
<rawString>Karel van den Eynde and Claire Blanche-Benveniste. 1978. Syntaxe et m´ecanismes descriptifs : pr´esentation de l’approche pronominale. Cahiers de Lexicologie, 32:3– 27.</rawString>
</citation>
</citationList>
</algorithm>

