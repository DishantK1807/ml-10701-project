1:181	Coling 2008 22nd International Conference on Computational Linguistics Proceedings of the 2nd workshop on Information Retrieval for Question Answering Workshop chair: Mark A. Greenwood 24 August 2008 Manchester, UK c2008 The Coling 2008 Organizing Committee Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Nonported license http://creativecommons.org/licenses/by-nc-sa/3.0/ Some rights reserved Order copies of this and other Coling proceedings from: Association for Computational Linguistics (ACL) 209 N. Eighth Street Stroudsburg, PA 18360 USA Tel: +1-570-476-8006 Fax: +1-570-476-0860 acl@aclweb.org ISBN 978-1-905593-55-2 Design by Chimney Design, Brighton, UK Production and manufacture by One Digital, Brighton, UK ii Introduction Open domain question answering (QA) has become a very active research area over the past decade, due in large measure to the stimulus of the TREC Question Answering track (now a track within the recently formed Text Analysis Conference, TAC).
2:181	This track addresses the task of finding answers to natural language questions (e.g. How tall is the Eiffel Tower?, Who is Aaron Copland?, What effect does second-hand smoke have on non-smokers?) from large text collections.
3:181	This task stands in contrast to the more conventional information retrieval (IR) task of finding documents relevant to a query, where the query may be simply a collection of keywords (e.g. Eiffel Tower, American composer, born Brooklyn NY 1900, ).
4:181	Finding answers requires processing texts at a level of detail that cannot be carried out at retrieval time for very large text collections.
5:181	This limitation has led many researchers to rely on, broadly, a two stage approach to the QA task.
6:181	In stage one a subset of question-relevant texts are selected from the whole collection.
7:181	In stage two this subset is subjected to detailed processing for answer extraction.
8:181	Clearly performance at stage two is bounded by performance at stage one, and previous work has shown that, despite the sophistication of standard IR ranking algorithms, they are not well suited to the stage one task of retrieving relevant documents given short natural language questions.
9:181	It is likely that improvements in this area will come from linguistic insights into why QA focused IR is different from the traditional IR model.
10:181	With the continued expansion of QA research into more complex question types and with the speed with which answers are returned becoming an issue, the importance of having good, QA-focused IR techniques is likely to increase.
11:181	To date this topic has received limited explicit attention despite its obvious importance.
12:181	This 2nd IR4QA workshop aims to address this situation by continuing to attract the attention of researchers to the specific IR challenges raised by QA.
13:181	For this workshop, we solicited papers that addressed any aspect of QA-focused IR, in order to improve overall system performance, , suggesting possible topics such as:  parameterizations/optimizations of specific IR systems for QA  studies of query formation strategies suited to QA, e.g. named entity pre-processing of questions  different uses of IR for different question types (e.g. factoid, list, definition, event, how, )  utility of term matching constraints, e.g. term proximity, for QA  analyses of differing IR techniques for QA  impact of IR performance on overall QA performance  QA-orientated corpus pre-processing, e.g. indexing POS tags, named entities, semanticallytagged entities, relationships, etc. rather than simply tokens  evaluation measures for assessing IR for QA  retrieval from semi-structured data i.e. QA from Wikipedia articles From the papers submitted, 10 were selected following peer review.
14:181	These papers are included in this proceedings.
15:181	The enthusiastic response to this workshop confirms the belief that this is an important area of interest to a significant number of researchers.
16:181	Mark A. Greenwood iii  Organizers: Mark A. Greenwood, Univeristy of Sheffield Programme Committee: Matthew W. Bilotti, Carnegie Mellon University Gosse Bouma, University of Groningen Charles Clarke, University of Waterloo Hoa Dang, NIST Robert Gaizauskas, University of Sheffield Eduard Hovy, ISI Jimmy Lin, University of Maryland John Prager, IBM Horacio Saggion, University of Sheffield Jrg Tiedemann, University of Groningen Bonnie Webber, University of Edinburgh Ralph Weischedel, BBN v  Table of Contents Improving Text Retrieval Precision and Answer Accuracy in Question Answering Systems Matthew Bilotti and Eric Nyberg  1 Exact Phrases in Information Retrieval for Question Answering Svetlana Stoyanchev, Young Chol Song and William Lahti9 Simple is Best: Experiments with Different Document Segmentation Strategies for Passage Retrieval Jorg Tiedemann and Jori Mur  17 Passage Retrieval for Question Answering using Sliding Windows Mahboob Khalid and Suzan Verberne26 A Data Driven Approach to Query Expansion in Question Answering Leon Derczynski, Jun Wang, Robert Gaizauskas and Mark A. Greenwood34 Answer Validation by Information Distance Calculation Fangtao Li, Xian Zhang and Xiaoyan Zhu42 Using Lexico-Semantic Information for Query Expansion in Passage Retrieval for Question Answering Lonneke van der Plas and Jorg Tiedemann  50 Evaluation of Automatically Reformulated Questions in Question Series Richard Shaw, Ben Solway, Robert Gaizauskas and Mark A. Greenwood58 Topic Indexing and Retrieval for Factoid QA Kisuh Ahn and Bonnie Webber66 Indexing on Semantic Roles for Question Answering Luiz Augusto Pizzato and Diego Molla  74 vii  Conference Programme Sunday, August 24, 2008 9:159:30 Welcome 9:3010:00 ImprovingTextRetrievalPrecisionandAnswerAccuracyinQuestionAnsweringSystems Matthew Bilotti and Eric Nyberg 10:0010:30 Exact Phrases in Information Retrieval for Question Answering Svetlana Stoyanchev, Young Chol Song and William Lahti 10:3011:00 Coffee Break 11:0011:30 Simple is Best: Experiments with Different Document Segmentation Strategies for Passage Retrieval Jorg Tiedemann and Jori Mur 11:3012:00 Passage Retrieval for Question Answering using Sliding Windows Mahboob Khalid and Suzan Verberne 12:0012:30 A Data Driven Approach to Query Expansion in Question Answering Leon Derczynski, Jun Wang, Robert Gaizauskas and Mark A. Greenwood 12:302:00 Lunch 2:002:30 Answer Validation by Information Distance Calculation Fangtao Li, Xian Zhang and Xiaoyan Zhu 2:303:00 Using Lexico-Semantic Information for Query Expansion in Passage Retrieval for Question Answering Lonneke van der Plas and Jorg Tiedemann 3:003:30 Evaluation of Automatically Reformulated Questions in Question Series Richard Shaw, Ben Solway, Robert Gaizauskas and Mark A. Greenwood 3:304:00 Coffee Break 4:004:30 Topic Indexing and Retrieval for Factoid QA Kisuh Ahn and Bonnie Webber 4:305:00 Indexing on Semantic Roles for Question Answering Luiz Augusto Pizzato and Diego Molla 5:005:30 Discussion Session ix  Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 18 Manchester, UK.
17:181	August 2008 Improving Text Retrieval Precision and Answer Accuracy in Question Answering Systems Matthew W. Bilotti and Eric Nyberg Language Technologies Institute Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213 USA { mbilotti, ehn }@cs.cmu.edu Abstract Question Answering (QA) systems are often built modularly, with a text retrieval component feeding forward into an answer extraction component.
18:181	Conventional wisdom suggests that, the higher the quality of the retrieval results used as input to the answer extraction module, the better the extracted answers, and hence system accuracy, will be.
19:181	This turns out to be a poor assumption, because text retrieval and answer extraction are tightly coupled.
20:181	Improvements in retrieval quality can be lost at the answer extraction module, which can not necessarily recognize the additional answer candidates provided by improved retrieval.
21:181	Going forward, to improve accuracy on the QA task, systems will need greater coordination between text retrieval and answer extraction modules.
22:181	1 Introduction The task of Question Answering (QA) involves taking a question phrased in natural human language and locating specific answers to that question expressed within a text collection.
23:181	Regardless of system architecture, or whether the system is operating over a closed text collection or the web, most QA systems use text retrieval as a first step to narrow the search space for the answer to the question to a subset of the text collection (Hirschman and Gaizauskas, 2001).
24:181	The remainder of the QA process amounts to a gradual narrowing of the search space, using successively more finely-grained filters to extract, validate and present one or more answers to the question.
25:181	c2008.
26:181	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
27:181	Some rights reserved.
28:181	Perhaps the most popular system architecture in the QA research community is the modular architecture, in most variations of which, text retrieval is represented as a separate component, isolated by a software abstraction from question analysis and answer extraction mechanisms.
29:181	The widelyaccepted pipelined modular architecture imposes a strict linear ordering on the systems control flow, with the analysis of the input question used as input to the text retrieval module, and the retrieved results feeding into the downstream answer extraction components.
30:181	Proponents of the modular architecture naturally view the QA task as decomposable, and to a certain extent, it is. The modules, however, can never be fully decoupled, because question analysis and answer extraction components, at least, depend on a common representation for answers and perhaps also a common set of text processing tools.
31:181	This dependency is necessary to enable the answer extraction mechanism to determine whether answers exist in retrieved text, by analyzing it and comparing it against the question analysis modules answer specification.
32:181	In practice, the text retrieval component does not use the common representation for scoring text; either the question analysis module or an explicit query formulation component maps it into a representation queryable by the text retrieval component.
33:181	The pipelined modular QA system architecture also carries with it an assumption about the compositionality of the components.
34:181	It is easy to observe that errors cascade as the QA process moves through downstream modules, and this leads to the intuition that maximizing performance of individual modules minimizes the error at each stage of the pipeline, which, in turn, should maximize overall end-to-end system accuracy.
35:181	It is a good idea to pause to question what this intuition is telling us.
36:181	Is end-to-end QA system performance really a linear function of individual 1 [ARG0 [PERSON John]] [TARGET loves] [ARG1 [PERSON Mary]] Figure 1: Example OpenEphyra semantic representation for the sentence, John loves Mary.
37:181	Note that John is identified as the ARG0, the agent, or doer, of the love action.
38:181	Mary is identified as the ARG1, the patient, or to whom the love action is being done.
39:181	Both John and Mary are also identified as PERSON named entity types.
40:181	components?
41:181	Is component performance really additive?
42:181	This paper argues that the answer is no, not in general, and offers the counterexample of a high-precision text retrieval system that can check constraints against the common representation at retrieval time, which is integrated into a publiclyavailable pipelined modular QA system that is otherwise unchanged.
43:181	Ignoring the dependency between the answer extraction mechanism and the text retrieval component creates a problem.
44:181	The answer extraction module is not able to handle the more sophisticated types of matches provided by the improved text retrieval module, and so it ignores them, leaving end-to-end system performance largely unchanged.
45:181	The lesson learned is that a module improved in isolation does not necessarily provide an improvement in end-to-end system accuracy, and the paper concludes with recommendations for further research in bringing text retrieval and answer extraction closer together.
46:181	2 Improving Text Retrieval in Isolation This section documents an attempt to improve the performance of a QA system by substituting its existing text retrieval component with for highprecision retrieval system capable of checking linguistic and semantic constraints at retrieval time.
47:181	2.1 The OpenEphyra QA System OpenEphyra is the freely-available, open-source version of the Ephyra1 QA system (Schlaefer et al., 2006; Schlaefer et al., 2007).
48:181	OpenEphyra is a pipelined modular QA system having four stages: question analysis, query generation, search and answer extraction and selection.
49:181	OpenEphyra also includes support for answer projection, or the use of the web to find answers to the question, which are then used to find supporting text in the corpus.
50:181	Answer projection support was disabled for the purposes of this paper.
51:181	1See: http://www.ephyra.info The common representation in OpenEphyra is a verb predicate-argument structure, augmented with named entity types, in which verb arguments are labeled with semantic roles in the style of PropBank (Kingsbury et al., 2002).
52:181	This feature requires the separate download2 of a semantic parser called ASSERT (Pradhan et al., 2004), which was trained on PropBank.
53:181	See Figure 1 for an example representation for the sentence, John loves Mary.
54:181	OpenEphyra comes packaged with standard baseline methods for answer extraction and selection.
55:181	For example, it extracts answers from retrieved text based on named entity instances matching the expected answer type as determined by the question analysis module.
56:181	It can also look for predicate-argument structures that match the question structure, and can extract the argument corresponding to the argument in the question representing the interrogative phrase.
57:181	OpenEphyras default answer selection algorithm filters out answers containing question keyterms, merges subsets, and combines scores of duplicate answers.
58:181	2.2 Test Collection The corpus used in this experiment is the AQUAINT corpus (Graff, 2002), the standard corpus for the TREC3 QA evaluations held in 2002 through 2005.
59:181	The corpus was prepared using MXTerminator (Reynar and Ratnaparkhi, 1997) for sentence segmentation, BBN Identifinder (Bikel et al., 1999) for named entity recognition, as well as the aforementioned ASSERT for identification of verb predicate-argument structures and PropBank-style semantic role labeling of the arguments.
60:181	The test collection consists of 109 questions from the QA track at TREC 2002 with extensive document-level relevance judgments (Bilotti et al., 2004; Lin and Katz, 2006) over the AQUAINT corpus.
61:181	A set of sentence-level judgments was pre2See: http://www.cemantix.org 3Text REtrieval Conferences organized by the U.S. National Institute of Standards and Technology 2 Existing query #combine[sentence]( #any:person first person reach south pole ) Top-ranked result Dufek became the first person to land an airplane at the South Pole.
62:181	Second-ranked result He reached the North Pole in 1991.
63:181	High-precision query #combine[sentence]( #max( #combine[target]( scored #max( #combine[./arg1]( #any:person )) #max( #combine[./arg2]( #max( #combine[target]( reach #max( #combine[./arg1]( south pole ))))))))) Top-ranked result [ARG1 Norwegian explorer [PERSON Roald Admundsen]] [TARGET becomes] (relevant) [ARG2 [ARG0 first man] to [TARGET reach] [ARG1 [LOCATION South Pole]]] Figure 2: Retrieval comparison between OpenEphryas existing text retrieval component, and the highprecision version it was a replaced with, for question 1475, Who was the first person to reach the South Pole?
64:181	Note that the top two results retrieved by the existing text retrieval component are not relevant, and the top result from the high-precision component is relevant.
65:181	The existing component does retrieve this answer-bearing sentence, but ranks it third.
66:181	pared by manually determining whether each sentence matching the TREC-provided answer pattern for a given question was answer-bearing according to the definition that an answer-bearing sentence completely contains and supports the answer to the question, without requiring inference or aggregation outside of that sentence.
67:181	Questions without any answer-bearing sentences were removed from the test collection, leaving 91 questions.
68:181	Questions were manually reformulated so that they contain predicates.
69:181	For example, question 1432, Where is Devils Tower?
70:181	was changed to Where is Devils Tower located?, because ASSERT does not cover verbs, including be and have, that do not occur in its training data.
71:181	Handcorrected ASSERT parses for each question were were cached in the question analysis module.
72:181	Reformulated questions are used as input to both the existing and high-precision text retrieval modules, to avoid advantaging one system over the other.
73:181	2.3 High-Precision Text Retrieval OpenEphyras existing text retrieval module was replaced with a high-precision text retrieval system based on a locally-modified version of the Indri (Strohman et al., 2005) search engine, a part of the open-source Lemur toolkit4.
74:181	While the existing version of the text retrieval component supports querying on keyterms, phrases and placeholders 4See: http://www.lemurproject.org for named entity types, the high-precision version also supports retrieval-time constraint-checking against the semantic representation based on verb predicate-argument structures, PropBank-style semantic role labels, and named entity recognition.
75:181	To make use of this expanded text retrieval capability, OpenEphyras query formulation module was changed to source pre-prepared Indri queries that encode using structured query operators the predicate-argument and named entity constraints that match the answer-bearing sentences for each question.
76:181	If questions have multiple queries associated with them, each query is evaluated individually, with the resulting ranked lists fused by Round Robin (Voorhees et al., 1994).
77:181	Round Robin, which merges ranked lists by taking the top-ranked element from each list in order followed by lowerranking elements, was chosen because Indri, the underlying retrieval engine, gives different queries scores that are not comparable in general, making it difficult to choose a fusion method that uses retrieval engine score as a feature.
78:181	Figure 2 shows a comparison of querying and retrieval behavior between OpenEphyras existing text retrieval module and the high-precision version with which it is being replaced for question 1475, Who was the first person to reach the South Pole?
79:181	The bottom of the figure shows an answerbearing sentence with the correct answer, Roald Admundsen.
80:181	The predicate-argument structure, se3 mantic role labels and named entities are shown.
81:181	The high-precision text retrieval module supports storing of extents representing sentences, target verbs and arguments and named entity types as fields in the index.
82:181	At query time, constraints on these fields can be checked using structured query operators.
83:181	The queries in Figure 2 are shown in Indri syntax.
84:181	Both queries begin with #combine[sentence], which instructs Indri to score and rank sentence extents, rather than entire documents.
85:181	The query for the existing text retrieval component contains keyterms as well an #any:type operator that matches instances of the expected answer type, which in this case is person.
86:181	The high-precision query encodes a verb predicate-argument structure.
87:181	The nested #combine[target]operator scores a sentence by the predicate-argument structures it contains.
88:181	The#combine[./role]operators are used to indicate constraints on specific argument roles.
89:181	The dot-slash syntax tells Indri that the argument extents are related to but not enclosed by the target extent.
90:181	Throughout, the #max operator is used to select the best matching extent in the event that more than one satisfy the constraints.
91:181	Figure 3 compares average precision at the top twenty ranks over the entire question set between OpenEphyras existing text retrieval module and the high-precision text retrieval module, showing that the latter performs better.
92:181	2.4 Results To determine what effect improving text retrieval quality has on the end-to-end QA system, it suffices to run the system on the entire test collection, 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 1 6 11 16 High-precision Existing Rank Ave rag e P rec isi on  at  R an k Figure 3: Comparison of average precision at top twenty ranks between OpenEphyras existing text retrieval module, and the high-precision version that took its place.
93:181	replace the text retrieval component with the highprecision version while holding the other modules constant, and repeat the test run.
94:181	Table 1 summarizes the MAP, average end-to-end system accuracy (whether the top-ranked returned answer is correct), and the mean reciprocal rank (MRR) of the correct answer (one over the rank at which the correct answer is returned).
95:181	If the correct answer to a question is returned beyond rank twenty, the reciprocal rank for that question is considered to be zero.
96:181	Table 1: Summary of end-to-end QA system accuracy and MRR when the existing text retrieval module is replaced with a high-precision version Retrieval MAP Accuracy MRR Existing 0.3234 0.1099 0.2080 High-precision 0.5487 0.1319 0.2020 Table 1 shows that, despite the improvement in average precision, the end-to-end system did not realize a significant improvement in accuracy or MRR.
97:181	Viewed in the aggregate, the results are discouraging, because it seems that the performance gains realized after the text retrieval stage of the pipeline are lost in downstream answer extraction components.
98:181	Figure 4 compares OpenEphyra both before and after the integration of the high-precision text retrieval component on the basis of average precision and answer MRR.
99:181	The horizontal axis plots the difference in average precision; a value of positive one indicates that the high-precision version of the module was perfect, ranking all answer-bearing sentences at the top of the ranked list, and that the existing version retrieved no relevant text at all.
100:181	Negative one indicates the reverse.
101:181	The vertical axis plots the difference in answer MRR.
102:181	As before, positive one indicates that the high-precision component led the system to rank the correct answer first, and the existing component did not, and negative one indicates the reverse.
103:181	The zero point on each axis is where the high-precision and existing text retrieval components performed equally well.
104:181	The expectation is that there will be a positive correlation between average precision and answer MRR; when the retrieval component provides higher quality results, the job of the answer extraction module should be easier.
105:181	This is illustrated in the bottom portion of Figure 4, which was cre4 -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 Difference in Average Precision Ideal Answer Extraction OpenEphyra Di ffe ren ce  in  A ns we r M RR -1 -0.5 0 0.5 1 -1 -0.5 0 0.5 1 Figure 4: Scatter plot comparing the difference in average precision between the high-precision retrieval component and the existing retrieval component on the horizontal axis, to the difference in answer MRR on the vertical axis.
106:181	Ideally, there would be a high correlation between the two; as average precision improves, so should answer MRR.
107:181	ated by assuming that the answer extraction module could successfully extract answers without error from all answer-bearing sentences returned by the text retrieval component.
108:181	Interestingly, actual extraction performance, shown in the top portion of Figure 4, bears little resemblance to the ideal.
109:181	Note the large concentration of data points along the line representing zero difference in answer MRR.
110:181	This indicates that, regardless of improvement in average precision of the results coming out of the retrieval module, the downstream answer extraction performance remains the same as it was when the existing text retrieval component was in use.
111:181	This occurs because the answer extraction module does not know how to extract answers from some of the types of answer-bearing sentences retrieved by the high-precision version of the retrieval module and not by the existing version.
112:181	There are several data points in the top righthand quadrant of the top half of Figure 4, indicating that for some questions, answer extraction was able to improve as average precision improved.
113:181	This is likely due to better rankings for types of answer-bearing sentences that answer extraction already knows how to handle.
114:181	Data points occurring in the lower right-hand portion of the graph indicate depressed answer extraction performance as average precision is increasing.
115:181	This phenomenon can be explained by the higher-precision text retrieval module ranking answer-bearing sentences that answer extraction can not handle ahead of those that it can handle.
116:181	3 Failure Analysis The results presented in the previous section confirm that an improvement made to the text retrieval component, in isolation, without a corresponding improvement to the downstream answer extraction modules, can fail to translate into a corresponding improvement in end-to-end QA system accuracy.
117:181	The increased average precision in the retrieved results is coming in the form of answer-bearing sentences of types that the answer extraction machinery does not know how to handle.
118:181	To address this gap in answer extraction coverage, it is first necessary to examine examples of the types of errors made by the OpenEphyra answer extraction module, summarized in Table 2.
119:181	Question 1497, What was the original name before The Star Spangled Banner?
120:181	is an example of a question for which OpenEphyras answer extraction machinery failed outright.
121:181	An answerbearing sentence was retrieved, however, containing the answer inside a quoted phrase: His poem was titled Defense of Fort MHenry and by November 1814 had been published as The StarSpangled Banner.
122:181	The expected answer type of this question does not match a commonly-used named entity type, so OpenEphryas named entitybased answer extractor found no candidates in this sentence.
123:181	Predicate-argument structure-based answer extraction fails as well because the old and new names do not appear within the same structure.
124:181	Because OpenEphyra does not include support for positing quoted phrases as answer candidates, no answer to this question can be found despite the fact that an answer-bearing sentence was retrieved.
125:181	Question 1417, Who was the first person to run the mile in less than four minutes?
126:181	is an example of a question for which average precision improved greatly, by 0.7208, but for which extraction quality remained the same.
127:181	The existing text retrieval module ranks 14 sentences ahead of the first answer-bearing sentence, but only one contains a named entity of type person, so despite the improvement in retrieval quality, the correct answer 5 Table 2: Summary of end-to-end QA system results on the question set Result Type Count Extraction failure 42 Retrieval better, extraction same 20 Retrieval better, extraction worse 13 Retrieval better, extraction better 10 Retrieval worse, extraction better 3 Retrieval worse, extraction worse 3 Total 91 moves up only one rank in the system output.
128:181	For ten questions, extraction performance does improve as average precision improves.
129:181	Question 1409, Which vintage rock and roll singer was known as The Killer?
130:181	For each of these questions, OpenEphyras existing text retrieval module could not rank an answer-bearing sentence highly or retrieve one at all.
131:181	Adding the high-precision version of the text retrieval component solved this problem.
132:181	In each case, named entity-based answer extraction was able extract the correct answer.
133:181	These eleven questions range over a variety of answer types, and have little in common except for the fact that there are relatively few answerbearing sentences in the corpus, and large numbers of documents matched by a bag-of-words query formulated using the keyterms from the question.
134:181	There are three questions for which extraction performance degrades as retrieval performance degrades.
135:181	Question 1463, What is the North Korean national anthem?
136:181	is an example.
137:181	In this case, there is only one relevant sentence, and, owing to an annotation error, it has a predicate-argument structure that is very generic, having North Korea as the only argument: Some of the North Korean coaches broke into tears as the Norths anthem, the Patriotic Song, played.
138:181	The high-precision retrieval component retrieved a large number of sentences matching the that predicate-argument structure, but ranked the one answer-bearing sentence very low.
139:181	Some questions actually worsened in terms of the reciprocal rank of the correct answer when average precision improved.
140:181	An example is question 1504, Where is the Salton Sea?
141:181	The high-precision text retrieval module ranked answer-bearing sentences such as The combination could go a long way to removing much of the pesticides, fertilizers, raw sewage carried by the river into the Salton Sea, the largest lake in California, but a failure of the named entity recognition tool did not identify California as an instance of the expected answer type, and therefore it was ignored.
142:181	Sentences describing other seas near other locations provided answers such as Central Asia, Russia, Turkey and Ukraine that were ranked ahead of California, which was eventually extracted from another answer-bearing sentence.
143:181	And finally, for some questions, high-precision retrieval was more of a hindrance than a help, retrieving more noise than answer-bearing sentences.
144:181	A question for which this is true is question 1470, When did president Herbert Hoover die?
145:181	The high-precision text retrieval module uses a predicate-argument structure to match the target verb die, theme Hoover and a date instance occurring in a temporal adjunct.
146:181	Interestingly, the text collection contains a great deal of die structures that match partially, including those referring to deaths of presidents of other nations, and those referring to the death of J. Edgar Hoover, who was not a U.S. president but the first director of the U.S. Federal Bureau of Investigation (FBI).
147:181	False positives such as these serve to push the true answer down on the ranked list of answers coming out of the QA system.
148:181	4 Improving Answer Extraction The answer extraction and selection algorithms packaged with OpenEphyra are widely-accepted baselines, but are not sophisticated enough to extract answer candidates from the additional answer-bearing text retrieved by the high-precision text retrieval module, which can check linguistic and semantic constraints at query time.
149:181	The named-entity answer extraction method selects any candidate answer that is an instance of the expected answer type, so long as it co-occur with query terms.
150:181	Consider question 1467, What 6 year did South Dakota become a state?
151:181	Given that the corpus consists of newswire text reporting on current events, years that are contemporary to the corpus often co-occur with the question focus, as in the following sentence, Monaghan also seized about $87,000 from a Santee account in South Dakota in 1997.
152:181	Of the top twenty answers returned for this question, all but four are contemporary to the corpus or in the future.
153:181	Minimal sanity-checking on candidate answers could save the system the embarrassment of returning a date in the future as the answer.
154:181	Going one step further would involve using external sources to determine that 1997 is too recent to be the year a state was admitted to the union.
155:181	OpenEphyras predicate-argument structurebased answer extraction algorithm can avoid some of these noisy answers by comparing some constraints from the question against the retrieved text and only extracting answers if the constraints are satisfied.
156:181	Consider question 1493, When was Davy Crockett born?
157:181	One relevant sentence says Crockett was born Aug. 17, 1786, in what is now eastern Tennessee, and moved to Lawrenceburg in 1817.
158:181	The SRL answer extraction algorithm extracts Aug. 17, 1786 because it is located in an argument labeled argm-tmp with respect to the verb, and ignores the other date in the sentence, 1817.
159:181	The named entity-based answer extraction approach proposes both dates as answer candidates, but the redundancy-based answer selection prefers 1786.
160:181	The predicate-argument structure-based answer extraction algorithm is limited because it only extracts arguments from text that shares the structure as the question.
161:181	The high-precision text retrieval approach is actually able to retrieve additional answer-bearing sentences with different predicateargument structures from the question, but answer extraction is not able to make use of it.
162:181	Consider the sentence, At the time of his 100 point game with the Philadelphia Warriors in 1962, Chamberlain was renting an apartment in New York.
163:181	Though this sentence answers the question What year did Wilt Chamberlain score 100 points?, its predicateargument structure is different from that of the question, and predicate-argument structure-based answer extraction will ignore this result because it does not contain a score verb.
164:181	In addition to answer extraction, end-to-end performance could be improved by focusing on answer selection.
165:181	OpenEphyra does not include support for sanity-checking the answers it returns, and its default answer selection mechanism is redundancy-based.
166:181	As a result, nonsensical answers are occasionally retrieved, such as moon for question 1474, What is the lowest point on Earth?
167:181	Sophisticated approaches, however, do exist for answer validation and justification, including use of resources such as gazetteers and ontologies (Buscaldi and Rosso, 2006), Wikipedia (Xu et al., 2002), the Web (Magnini et al., 2002), and combinations of the above (Ko et al., 2007).
168:181	5 Conclusions This paper set out to challenge the assumption of compositionality in pipelined modular QA systems that suggests that an improvement in an individual module should lead to an improvement in the overall end-to-end system performance.
169:181	An attempt was made to validate the assumption by showing an improvement in the end-to-end system accuracy of an off-the-shelf QA system by substituting its existing text retrieval component for a highprecision retrieval component capable of checking linguistic and semantic constraints at query time.
170:181	End-to-end system accuracy remained roughly unchanged because the downstream answer extraction components were not able to extract answers from the types of the answer-bearing sentences returned by the improved retrieval module.
171:181	The reality of QA systems is that there is a high level of coupling between the different system components.
172:181	Ideally, text retrieval should have an understanding of the kinds of results that answer extraction is able to utilize to extract answers, and should not offer text beyond the capabilities of the downstream modules.
173:181	Similarly, question analysis and answer extraction should be agreeing on a common representation for what constitutes an answer to the question so that answer extraction can use that information to locate answers in retrieved text.
174:181	When a retrieval module is available that is capable of making use of the semantic representation of the answer, it should do so, but answer extraction needs to know what it can assume about incoming results so that it does not have to re-check constraints already guaranteed to hold.
175:181	The coupling between text retrieval and answer extraction is important for a QA system to perform well.
176:181	Improving the quality of text retrieval is essential because once the likely location of 7 the answer is narrowed down to a subset of the text collection, anything not retrieved text can not be searched for answers in downstream modules.
177:181	Equally important is the role of answer extraction.
178:181	Even the most relevant retrieved text is useless to a QA system unless answers can be extracted from it.
179:181	End-to-end QA system performance can not be improved by improving text retrieval quality in isolation.
180:181	Improvements in answer extraction must keep pace with progress on text retrieval techniques to reduce errors resulting from a mismatch in capabilities.
181:181	Going forward, research on the linguistic and semantic constraint-checking capabilities of text retrieval systems to support the QA task can drive research in answer extraction techniques, and in QA systems in general.

