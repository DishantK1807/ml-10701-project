GeneratingTailored,Comparative
DescriptionswithContextually
AppropriateIntonation
MichaelWhite
∗
TheOhioStateUniversity
RobertA.J.Clark
∗∗
UniversityofEdinburgh
JohannaD.Moore
†
UniversityofEdinburgh
Generating responses that take user preferences into account requires adaptation at all levels of
the generation process. This article describes a multi-level approach to presenting user-tailored
informationinspokendialogueswhichbringstogetherfortheﬁrsttimemulti-attributedecision
models,strategiccontentplanning,surfacerealizationthatincorporatesprosodyprediction,and
unit selection synthesis that takes the resulting prosodic structure into account. The system
selects the most important options to mention and the attributes that are most relevant to
choosingbetweenthem,basedontheusermodel.Multipleoptionsareselectedwheneachoffersa
compellingtrade-off.Toconveythesetrade-offs,thesystememploysanovelpresentationstrategy
whichstraightforwardlylendsitselftothedeterminationofinformationstructure,aswellasthe
contents of referring expressions. During surface realization, the prosodic structure is derived
from the information structure using Combinatory Categorial Grammar in a way that allows
phrase boundaries to be determined in a ﬂexible, data-driven fashion. This approach to choosing
pitch accents and edge tones is shown to yield prosodic structures with signiﬁcantly higher
acceptability than baseline prosody prediction models in an expert evaluation. These prosodic
structures are then shown to enable perceptibly more natural synthesis using a unit selection
voice that aims to produce the target tunes, in comparison to two baseline synthetic voices. An
expertevaluationandf0analysisconﬁrmthesuperiorityofthegenerator-drivenintonationand
itscontributiontolisteners’ratings.
1.Introduction
Inanevaluationofninespokendialogueinformationsystems,developedaspartofthe
DARPACommunicatorprogram,theinformationpresentationphaseofthedialogues
∗ 1712NeilAve.,Columbus,OH43210,USA.Web:http://www.ling.ohio-state.edu/~mwhite/.
∗∗ 10CrichtonStreet,Edinburgh,ScotlandEH81AB,UK.Web:
http://www.cstr.ed.ac.uk/ssi/people/robert.html.
† 10CrichtonStreet,Edinburgh,ScotlandEH81AB,UK.Web:http://www.hcrc.ed.ac.uk/~jmoore/.
Submissionreceived:31January2008;revisedsubmissionreceived:19May2009;acceptedforpublication:
24September2009.
©2010AssociationforComputationalLinguistics
ComputationalLinguistics Volume36,Number2
Figure1
TypicalinformationpresentationphaseofaCommunicatordialogue.
was found to be the primary contributor to dialogue duration (Walker, Passonneau,
andBoland2001).Duringthisphase,thetypicalsystemsequentiallypresentsthesetof
optionsthatmatchtheuser’sconstraints,asshowninFigure1.Theusercanthennavi-
gatethroughtheseoptionsandreﬁnethembyofferingnewconstraints.Whenmultiple
optionsarereturned,thisprocesscanbeexacting,leadingtoreducedusersatisfaction.
AsWalkeretal.(2004)observe,havingtoaccessthesetofavailableoptionssequen-
tiallymakesithardfortheusertorememberinformationrelevanttomakingadecision.
Toreduceusermemoryload,weneedalternativestrategiesforsequentialpresentation.
Inparticular,werequirebetteralgorithmsfor:
1. selectingthemostrelevantsubsetofoptionstomention,aswellasthe
attributesthataremostrelevanttochoosingamongthem;and
2. determininghowtoorganizeandexpressthedescriptionsoftheselected
optionsandattributes,inwaysthatarebotheasytounderstandand
memorable.
1
Inthisarticle,wedescribehowwehaveaddressedthesepointsintheFLIGHTS
2
system,reviewingandextendingthedescriptiongiveninMooreetal.(2004).FLIGHTS
follows previous work (Carberry, Chu-Carroll, and Elzer 1999; Carenini and Moore
2000;Walkeretal.2002)inapplyingdecision-theoreticmodelsofuserpreferencesto
thegenerationoftailoreddescriptionsofthemostrelevantavailableoptions.Multi-
attributedecisiontheoryprovidesadetailedaccountofhowmodelsofuserpreferences
canbeusedindecisionmaking(EdwardsandBarron1994).Suchpreferencemodels
havebeenshowntoenablesystemstopresentinformationinwaysthatareconciseand
1 Anissuewedonotaddressinthisarticleiswhetheramultimodalsystemwouldbemoreeffectivethana
voice-onlyone.Webelievethattheseneeds,andinparticulartheneedtoexpressinformationwith
contextuallyappropriateprosody,arealsohighlyrelevantformultimodalsystems.Wealsonotethat
thereisstillstrongdemandforvoice-orientedsystemsforeyes-busyuseandfortheblind.
2 FLIGHTSstandsforFancyLinguisticallyInformedGenerationofHighlyTailoredSpeech.
160
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
tailoredtotheuser’sinterests(CareniniandMoore2001;Walkeretal.2004;Carenini
andMoore2006).Decision-theoreticmodelshavealsobeencommerciallydeployedin
Websystems.
3
Topresentmultipleoptions,weintroduceanovelstrategywherethebestoption
(withrespecttotheusermodel)ispresentedﬁrst,followedbythemostcompelling
remainingoptions,intermsoftrade-offsbetweenattributesthatareimportanttothe
user.(Multipleoptionsareselectedonlywheneachoffersacompellingtrade-off.)An
importantpropertyofthisstrategyisthatitnaturallylendsitselftothedetermination
ofinformationstructure,aswellasthecontentsofreferringexpressions.Thus,tohelp
makethetrade-offsamongtheselectedoptionscleartotheuser,FLIGHTS(1)groups
attributesthatarepositivelyandnegativelyvaluedfortheuser,(2)choosesreferringex-
pressionsthathighlightthesalientdistinguishingattributes,(3)determinesinformation
structureandprosodicstructurethatexpresscontrastsintelligibly,and(4)synthesizes
utterances with a unit selection voice that takes the prosodic structure into account.
Assuch,FLIGHTSgoesbeyondprevioussystemsinadaptingitsoutputaccordingto
userpreferencesatalllevelsofthegenerationprocess,notjustatthelevelsofcontent
selectionandtextplanning.
Our approach to generating contextually appropriate intonation follows Prevost
(1995) and Steedman (2000a) in using Combinatory Categorial Grammar (CCG) to
convey the information structure of sentences via pitch accents and edge tones. To
adaptPrevostandSteedman’sapproachtoFLIGHTS,weoperationalizetheinformation
structural notion of theme to correspond to implicit questions that necessarily arise
inpresentingthetrade-offsamongoptions.Wealsoreﬁnethewayinwhichprosodic
structureisderivedfrominformationstructurebyallowingforamoreﬂexible,one-to-
manymappingbetweenthemesorrhemesandintonationalphrases,wheretheﬁnal
choiceofthetypeandplacementofedgetonesisdeterminedby n-grammodels.To
investigatetheimpactofinformationstructuralgrammaticalconstraintsinourhybrid
rule-based,data-drivenapproach,wecomparerealizeroutputswiththoseofbaseline
n-grammodels,andshowthattherealizeryieldstargetprosodicstructureswithsignif-
icantlyhigheracceptabilitythanthebaselinemodelsinanexpertevaluation.
The prosodic structure derived during surface realization is passed as prosodic
markuptothespeechsynthesizer.Thesynthesizerusesthisprosodicmarkupinthe
textanalysisphaseofsynthesisinplaceofthestructuresthatitwouldotherwisehave
topredictfromthetext.Thesynthesizerthenusesthecontextprovidedbythemarkup
toenforcetheselectionofsuitableunitsfromthedatabase.Toverifythattheprosodic
markupyieldsimprovementsinthequalityofsyntheticspeech,wepresentanexper-
imentwhichshowsthatlistenersperceiveaunitselectionvoicethataimstoproduce
thetargetprosodicstructuresassigniﬁcantlymorenaturalthaneitheroftwobaseline
unitselectionvoicesthatdonotusethemarkup.Wealsopresentanexpertevaluation
andf0analysiswhichconﬁrmthesuperiorityofthegenerator-drivenintonationandits
contributiontolisteners’ratings.
Theremainder ofthisarticleisstructured asfollows.Section2presentsourap-
proach to natural language generation (NLG) in the information presentation phase
of a FLIGHTS dialogue, including how multi-attribute decision models are used in
contentselection;howrhetoricalandinformationstructurearedeterminedduringdis-
courseplanning;howlexicalchoiceandreferringexpressionsarehandledinsentence
planning; how prosodic structures are derived insurface realization; and how these
3Seehttp://www.cogentex.com/solutions/recommender/index.shtml,forexample.
161
ComputationalLinguistics Volume36,Number2
Figure2
Tailoreddescriptionsoftheavailableﬂightsforthreedifferentusermodels.
prosodicstructurescomparetothosepredictedbybaselinen-grammodelsinanexpert
evaluation.Section3describeshowtheprosodicstructuresareusedintheunitselection
voice employed in the present study, and compares this voice to the baseline ones
usedinourperceptionexperiment.Section4providesthemethodsandresultsofthe
perceptionexperimentitself,alongwiththeexpertprosodyevaluationandf0analysis.
Section5comparesourapproachtorelatedwork.Finally,Section6concludeswitha
summaryanddiscussionofremainingissues.
2.NLGinFLIGHTS
2.1TailoringFlightDescriptions
Toillustrate howdecision-theoretic models ofuser preferences can beused totailor
descriptionsoftheavailableoptionsatmanypointsinthegenerationprocess,letus
considerthefollowingthreehypotheticalusersoftheFLIGHTSsystem:
Student(S) Astudentwhocaresmostaboutprice,allelsebeingequal.
FrequentFlyer(FF) A business traveler who prefers business class, but cares most
aboutbuildingupfrequent-ﬂyermilesonKLM.
BusinessClass(BC) AnotherbusinesstravelerwhoprefersKLM,butwants,aboveall,
totravelinbusinessclass.
Suppose that each user is interested in ﬂying from Edinburgh to Brussels on a
certainday,andwouldliketoarrivebyﬁveo’clockintheafternoon.FLIGHTSbegins
thedialoguebygatheringthedetailsnecessarytoquerythedatabaseforpossibleﬂights.
Next,itusesthepreferencesencodedintheusermodeltoselectthehighestrankedﬂight
foreachuser,aswellasthoseﬂightsthatofferinterestingtrade-offs.Theseﬂightsare
thendescribedtotheuser,asshowninFigure2.
4
For the student (S),the BMI ﬂight is rated most highly, because itis a fairly in-
expensive, direct ﬂight that arrives near the desired time. The Ryanair ﬂight is also
4 Thesetofavailableﬂightswasobtainedby“screenscraping”datafromseveralonlinesources.The
hypotheticaluserpreferenceswerechosenwithaneyetowardsmakingtheexampleinteresting.Actual
userpreferencesarespeciﬁedaspartofregisteringtousetheFLIGHTSsystem.
162
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
mentionedasapossibility,asithasthebestprice;itendsuprankedloweroverallthan
theBMIﬂightthough,becauseitrequiresaconnectionandarriveswellinadvanceof
thedesiredarrivaltime.FortheKLMfrequentﬂyer(FF),lifeisabitmorecomplicated:
AKLMﬂightwithagoodarrivaltimeisofferedasthetopchoice,eventhoughitisa
connectingﬂightwithnoavailabilityinbusinessclass.Asalternatives,thedirectﬂight
onBMI(withnobusinessclassavailability)andtheBritishAirwaysﬂightwithseats
availableinbusinessclass(butrequiringaconnection)aredescribed.Finally,forthe
must-have-business-classtraveler(BC),theBritishAirwaysﬂightwithbusinessclass
availableispresentedﬁrst,despiteitsrequiringaconnection;thedirectﬂightonBMIis
offeredasanotherpossibility.
Although user preferences have an immediately apparent impact on content se-
lectionandordering,theyalsohavemoresubtleeffectsonmanyaspectsofhowthe
selectedcontentisorganizedandexpressed,asexplainedsubsequently.
Referringexpressions: Ratherthanalwaysreferringtotheavailableﬂightsinthesame
way,ﬂightsofinterestareinsteaddescribedusingtheattributesmostrelevantto
theuser:forexample,directﬂight,cheapestﬂight,KLM ﬂight.
Aggregation: Forconciseness,multipleattributesmaybegiveninasinglesentence,
subjecttotheconstraintthatattributeswhosevaluesarepositive(ornegative)
fortheusershouldbekepttogether.Forexample,inThere’s a KLM ﬂight arriving
Brusselsatfourﬁftyp.m.,butbusinessclassisnotavailableandyou’dneedtoconnectin
Amsterdam,thevaluesoftheattributesairlineandarrival-timeareconsidered
good,andthusaregroupedtogethertocontrastwiththevaluesoftheattributes
fare-classandnumber-of-legs,whichareconsideredbad.
Scalarterms: Scalarmodiﬁerslikegood,asingood price,andjust,asinjust ﬁfty pounds,
arechosentocharacterizeanattribute’svaluetotheuserrelativetovaluesofthe
sameattributeforotheroptions.
Discoursecues: Attributeswithnegativevaluesfortheuserareacknowledgedusing
discoursecues,suchasbutandthough.Interestingtrade-offscanalsobesignaled
usingcuessuchasif-conditionals.
Informationstructureandprosody: Compelling trade-offs are always indicated via
prosodicphrasingandemphasis,asin (There ARE seats in business class)
theme
(on
the British Airways ﬂight)
rheme
(that arrives at four twenty p.m.)
rheme,wherethedi-
visionofthesentenceintothemeandrhemephrasesisshowninformallyusing
parentheses,andcontrastiveemphasis(onARE)isshownusingsmallcaps.(See
Section 2.6.2 for details of how emphasis and phrasing are realized by pitch
accentsandedgetones.)
2.2Architecture
ThearchitectureoftheFLIGHTSgeneratorappearsinFigure3.OAA(Martin,Cheyer,
andMoran1999)servesasacommunicationshub,withthefollowingagentsresponsi-
bleforspeciﬁctasks:DIPPER(Bosetal.2003)fordialoguemanagement;aJavaagent
thatimplementsanadditivemulti-attributevaluefunction(AMVF),adecision-theoretic
modeloftheuser’spreferences(CareniniandMoore2000,2006),forusermodeling;
OPlan(CurrieandTate1991)forcontentplanning;XalanXSLT
5
andOpenCCG(White
5 http://xml.apache.org/xalan-j/.
163
ComputationalLinguistics Volume36,Number2
Figure3
FLIGHTSgenerationarchitecture.
2004,2006a,2006b)forsentenceplanningandsurfacerealization;andFestival(Taylor,
Black,andCaley1998)forspeechsynthesis.Theusermodeling,contentplanning,sen-
tenceplanning,andsurfacerealizationagentsaredescribedintheensuingsubsections.
FLIGHTSfollowsatypicalpipelinearchitecture(ReiterandDale2000)forNLG.
TheNLGsubsystemtakesasinputanabstractcommunicativegoalfromthedialogue
manager.Intheinformationpresentationphaseofthedialogue,thisgoalistodescribe
the available ﬂights that best meet the user’s constraints and preferences. Given a
communicativegoal,thecontentplannerselectsandarrangestheinformationtoconvey
byapplyingtheplanoperatorsthatimplementitspresentationstrategy.Insodoing,
itmakesuseofthreefurtherknowledgesources:theusermodel,thedomainmodel,
andthedialoguehistory.Next,thecontentplanissenttothesentenceplanner,which
usesXSLTtemplatestoperformaggregation,lexicalization,andreferringexpression
generation.Theoutputofsentenceplanningisasequenceoflogicalforms(LFs).The
useofLFtemplatesrepresentsapracticalandﬂexiblewaytodealwiththeinteraction
of decisions made at the sentence planning level, and further blurs the traditional
distinctionbetweentemplate-basedand“real”NLGthatvanDeemter,Krahmer,and
Theune(2005)havecalledintoquestion.EachLFisrealizedasasentenceusingaCCG
lexico-grammar(Steedman2000a,2000b).Notethatincontrasttothegenerationarchi-
tecturesof,forexample,Pan,McKeown,andHirschberg(2002)andWalker,Rambow,
andRogati(2002),theprosodicstructureofthesentenceisdeterminedasanintegral
part of surface realization, rather than in a separate prosody prediction component.
The prosodic structure is passed to the Festival speech synthesizer using Affective
PresentationMarkupLanguage(deCarolisetal.2004;Steedman2004),orAPML,an
XMLmarkuplanguagefortheannotationofaffect,informationstructure,andprosody.
Festival uses the Tones and Break Indices (Silverman et al. 1992), or ToBI,
6
pitch
accents and edge tones—speciﬁed as APML annotations—in determining utterance
phrasingandintonation,andemploysacustomsyntheticvoicetoproducethesystem
utterances.
6Seehttp://www.ling.ohio-state.edu/∼tobi/foranintroductiontoToBIandlinkstoon-line
resources.
164
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
2.3UserModeling
FLIGHTS uses an additive multi-attribute value function (AMVF) to represent the
user’s preferences, as in the GEA real estate recommendation system (Carenini and
Moore2000,2006)andtheMATCHrestaurantrecommendationsystem(Walkeretal.
2004).Decision-theoreticmodelsofthiskindarebasedonthenotionthat,ifanything
isvalued,itisvaluedformultiplereasons,wheretherelativeimportanceofdifferent
reasonsmayvaryamongusers.
The ﬁrst step is to identify good ﬂights for a particular origin, destination, and
arrivalordeparturetime.Thefollowingattributescontributetothisobjective:arrival-
time, departure-time, number-of-legs, total-travel-time, price, airline, fare-
class,andlayover-airport. As in MATCH, these attributes are arranged into a
one-leveltree.
Thesecondstepistodeﬁneavaluefunctionforeachattribute.Avaluefunction
mapsfromthefeaturesofaﬂighttoanumberbetween0and1,representingthevalue
of that ﬂight for that attribute, where 0 is the worst and 1 is the best. For example,
thefunctionfor total-travel-time computesthedifferenceinminutesbetweenthe
ﬂight’sarrivalanddeparturetimes,andthenmultipliestheresultbyascalingfactorto
obtainanevaluationbetween0and1.Thefunctionsfortheairline,layover-airport,
andfare-classattributesmakeuseofuser-speciﬁedpreferredordispreferredvalues
forthatattribute.Inthecurrentversionofthesefunctions,apreferredvalueisgivena
scoreof0.8,adispreferredvalue0.2,andallothervalues0.5.
7
Thestructureandweightsoftheusermodelrepresentauser’sdispositionalbiases
aboutﬂightselection.Situationalfeaturesareincorporatedintwoways.Therequested
originanddestinationareusedasaﬁlterwhenselectingthesetofavailableoptions
by querying the database. In contrast, the requested arrival or departure time—if
speciﬁed—isusedinthecorrespondingattribute’sevaluationfunctiontogiveahigher
scoretoﬂightsthatareclosertothespeciﬁedtime.Ifanarrivalordeparturetimeisnot
speciﬁed,thecorrespondingattributeisdisabledintheusermodel.
As in previous work, the overall evaluation of an option is computed as the
weightedsumofitsevaluationoneachattribute.Thatis,iff representstheoptionbeing
evaluated,Nisthetotalnumberofattributes,andw
i
andv
i
are,respectively,theweight
andthevalueforattributei,thentheevaluationv(f)ofoptionf iscomputedasfollows:
v(f)=
N
summationdisplay
i=1
w
i
v
i
(f)
Tocreateausermodelforaspeciﬁcuser,twotypesofinformationarerequired.The
usermustranktheattributesinorderofimportance,andheorshemustalsospecify
anypreferredordispreferredattributevaluesforthe airline, layover-airport,and
fare-classattributes.InFLIGHTS,wealsoallowuserstospecifyapartialorderingof
therankings,sothatseveralattributescanbegivenequalimportancewhenregistering
tousethesystem.Figure4showstheusermodelsforthestudent(S),frequent-ﬂyer(FF),
andbusiness-class(BC)usersdiscussedearlier;becausenodeparturetimeisspeciﬁed
inthesamplequery,departure-timeisnotincludedintheseexamples.
7 Ininformalexperiments,wedidnotﬁndthesystemtobeparticularlysensitivetotheexactvaluesused
intheattributefunctionswhenselectingcontent.
165
ComputationalLinguistics Volume36,Number2
Figure4
Sampleusermodels.
Basedontheuser’srankingoftheattributes,weightsareassignedtoeachattribute.
Asinpreviouswork,weuseRankOrderCentroid(ROC)weights(EdwardsandBarron
1994).Thisallowsweightstobeassignedbasedonrankings,guaranteeingthatthesum
willbe1.Then
th
ROCweightw
R
n
ofNtotalweightsiscomputedasfollows:
w
R
n
=
1
N
N
summationdisplay
i=n
1
i
Weextendtheseinitialweightstothepartial-orderingcaseasfollows.Ifattributes
i...jallhavethesameranking,thentheweightofeachwillbethemeanoftherelevant
ROCweights;thatis,
(
j
summationdisplay
k=i
w
R
k
)/(j−i+1)
Asaconcreteexample,ifthereisasinglehighest-rankedattributefollowedbyathree-
waytieforsecond,thenw
1
=w
R
1,andw
2
=w
3
=w
4
=
1
3
(w
R
2
+w
R
3
+w
R
4
).
2.4ContentPlanning
2.4.1 Content
Selection.Onceaspeciﬁcusermodelhasbeencreated,theAMVFcanbe
usedtoselectasetofﬂightstodescribeforthatuser,andtodeterminethefeaturesof
thoseﬂightsthatshouldbeincludedinthedescriptions.Weuseanovelstrategythat
combinesfeaturesoftheCompareandRecommendstrategiesofWalkeretal.(2004),
reﬁningthemwithanenhancedmethodofselectingoptionstomention.Inbrief,the
ideabehindthestrategyistoselectthetop-rankedﬂight,alongwithanyotherhighly
rankedﬂightsthatofferacompellingtrade-off—thatis,abettervalue(fortheuser)
on one of its attributes. As we shall see in Section 2.4.2, by guaranteeing that any
optionbeyondtheﬁrstoneofferssuchatrade-off,ourstrategylendsitselfnaturally
tothedeterminationofinformationstructureandthecontentsofreferringexpressions
identifyingtheoption.Bycontrast,Walkeretal.’sRecommendstrategyonlypresents
asingleoption,andtheirComparestrategydoesnotpresentoptionsinarankedorder
that facilitates making trade-offs salient. In addition, we may observe that with our
strategy,theusermodelneedonlycontainaroughapproximationoftheuser’strue
166
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
Figure5
Algorithmforselectingtheoptionstodescribe.
preferencesinorderforittodoitsjobofhelpingtoidentifygoodﬂightsfortheuser
toconsider.
8
Asimilarobservationunderliesthecandidate/critiquemodelofLinden,
Hanks,andLesh’s(1997)Web-basedsystem.
Selecting the Options to Describe. Indeterminingwhetheranoptionisworthmention-
ing,wemakeuseoftwomeasures.Firstly,weusethez-scoreofeachoption;thismea-
sureshowfartheevaluationv(f)ofanoptionf isfromthemeanevaluation.Formally,
it is deﬁned using the mean (µ
V
) and standard deviation (σ
V
) of all evaluations, as
follows:
z(f)=(v(f)−µ
V
)/σ
V
WealsomakeuseofthecompellingnessmeasuredescribedbyCareniniandMoore
(2000, 2006), who provide a formal deﬁnition. Informally, the compellingness of an
attribute measures its strength in contributing to the overall difference between the
evaluationoftwooptions,allotherthingsbeingequal.Foroptions f,g,andthreshold
valuek
c,wedeﬁnethesetcomp(f,g,k
c
)asthesetofattributesthathaveahigherscore
forf thanforg,andforwhichthecompellingnessisabovek
c
.
ThesetSelofoptionstodescribeisconstructedasfollows.First,weincludethetop-
rankedoption.Next,foralloftheotheroptionswhosez-scoreisaboveathresholdk
z,
wecheckwhetherthereisanattributeofthatoptionthatoffersacompellingtrade-off
overthealreadyselectedoptions;ifso,weaddthatoptiontotheset.
9
Thisalgorithmis
presentedinFigure5.
FortheBCusermodel,forexample,thisalgorithmproceedsasfollows.First,it
selectsthetop-rankedﬂight:aconnectingﬂightonBritishAirwayswithavailability
inbusinessclass.Thenext-highest-rankedﬂightisamorningﬂight,whichdoesnot
have any attributes that are compellingly better than those of the top choice, and is
therefore skipped. However, the third option presents an interesting trade-off: even
thoughbusinessclassisnotavailable,itisadirectﬂight,soitisalsoincluded.None
oftheotheroptionsabovethethresholdpresentanyinterestingtrade-offs,soonlythose
twoﬂightsareincluded.
8 Ofcourse,iftheusermodelcouldbereliedupontocontainperfectinformation,thesystemcouldalways
justrecommendasinglebestﬂight;however,becausewedonotexpectourmodelstocaptureauser’s
preferencesperfectly,wehavedesignedthesystemtolettheuserweighthealternativeswhenthere
appeartobeinterestingtrade-offsamongtheavailableoptions.
9 Therequirementthatanoptionofferacompellingtrade-offissimilartotheexclusionofdominated
solutionsinLinden,Hanks,andLesh(1997).
167
ComputationalLinguistics Volume36,Number2
Theselectedﬂightsfortheothersampleusermodelsshowsimilartrade-offs,as
mentionedinthediscussionofFigure2.ForFF,theselectedﬂightsareaconnecting,
economy-classﬂightonthepreferredairline;adirect,economy-classﬂightonaneutral
airline;andaconnecting,business-classﬂightonaneutralairline.ForS,thetopchoices
are a reasonably cheap direct ﬂight that arrives near the desired time, and an even
cheaper,connectingﬂightthatarrivesmuchearlierintheday.
Selecting the Attributes to Include. Whenselectingtheattributestoincludeinthede-
scription, we make use of an additional measure, s-compellingness. Informally, the
s-compellingness of an attribute represents the contribution of that attribute to the
evaluation of a single option; again, the formal deﬁnition is given by Carenini and
Moore(2000,2006).Notethatanattributemaybes-compellingineitherapositiveora
negativeway.Foranoptionf andthresholdk
c,wedeﬁnethesets-comp(f,k
c
)astheset
ofattributeswhoses-compellingnessforf isgreaterthank
c
.
Theset Atts ofattributesisconstructedintwosteps.First,weaddthemostcom-
pellingattributesofthetopchoice.Next,weaddallattributesthatrepresentatrade-off
betweenanytwooftheselectedoptions;thatis,attributesthatarecompellinglybetter
foroneoptionthanforanother.ThealgorithmappearsinFigure6.
FortheBCusermodel,thes-compellingattributesofthetopchoiceare arrival-
time and fare-class;thelatterisalsoacompellingadvantageofthisﬂightoverthe
secondoption.Theadvantageofthesecondoptionovertheﬁrstisthatitisdirect,so
number-of-legs isalsoincluded.Asimilarprocessontheotherusermodelsresults
inprice,arrival-time,andnumber-of-legsbeingselectedforS,andarrival-time,
fare-class,airline,andnumber-of-legsforFF.
2.4.2PlanningTextswithInformationStructure.Basedontheinformationreturnedbythe
contentselectionprocess,togetherwithfurtherinformationfromtheusermodeland
thecurrentdialoguecontext,thecontentplanningagentdevelopsaplanforpresenting
the available options. A distinguishing feature of the resulting content plans is that
theycontainspeciﬁcationsoftheinformationstructureofsentences(Steedman2000a),
including sentence theme (roughly, the topic the sentence addresses) and sentence
rheme(roughly,thenewcontributiononatopic).
Steedman(2000a)characterizesthenotionsofthemeandrhememoreformallyby
statingthatathemepresupposesarhemealternativeset,inthesenseofRooth(1992),
whilearhemerestrictsthisset.BecauseSteedmandoesnotfullyformalizethediscourse
update semantics of sentence themes, we have chosen to operationalize themes in
FLIGHTS in one particular way, namely, as corresponding to implicit questions that
arise in the context of describing the available options. Our reasoning is as follows.
First,wenotethatasetofalternativeanswerscorrespondsformallytothemeaningofa
question.Next,weobservethatwheneveraﬂightoptionpresentsacompellingtrade-off
Figure6
Algorithmforselectingtheattributestoinclude.
168
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
foraparticularattribute,itatleastpartiallyaddressesthequestionofwhetherthereare
anyﬂightsthathaveadesirablevalueforthatattribute;moreover,wheneveraﬂight
ispresentedthathasaless-than-optimalvalueforanattribute,itsmentionimplicitly
raises the question of whether any ﬂights are available with a better value for that
attribute.Finally,weconcludethatbyspecifyingandrealizingcontentasthematic,the
systemcanhelptheuserunderstandwhyaﬂightoptionisbeingpresented,sincethe
theme(byvirtueofitspresupposition)identiﬁeswhatimplicitquestion—thatis,what
trade-off—theoptionisaddressing.
Inall,thecontentplanner’spresentationstrategyperformsthefollowingfunctions:
a114
markingthestatusofitemsasdeﬁnite/indeﬁniteandpredicationsas
theme/rhemeforinformationstructure;
a114
determiningcontrastbetweenoptionsandattributes,orbetweengroupsof
optionsandattributes;
a114
groupingandorderingofsimilaroptionsandattributes(e.g.,presentingthe
topscoringoptionﬁrstvs.last);
a114
choosingthecontentsofreferringexpressions(e.g.,referringtoaparticular
optionbyairline);and
a114
decomposingstrategiesintobasicdialogueactsandhierarchicallyorganized
rhetoricalspeechacts.
Thepresentationstrategyisspeciﬁedviaasmallset(about40)ofcontentplanning
operators.Theseoperatorspresenttheselectedﬂightsasanorderedsequenceofop-
tions,startingwiththebestone.Eachﬂightissuggestedandthenfurtherdescribed.As
partofsuggestingaﬂight,itisidentiﬁedbyitsmostcompellingattribute,accordingto
theusermodel.(Recallthatanyselectedﬂightoptionsbeyondthehighestrankedone
mustofferacompellingtrade-off.)Flightsareadditionallyidentiﬁedbytheirairline,
whichwedeemedsufﬁcientlysigniﬁcanttowarrantspecialtreatment(otherwisethe
strategyisdomain-independent).
Theinformationfortheﬁrstﬂightispresentedasallrheme,asnodirectlinkis
madetotheprecedingquery.Eachsubsequent,alternativeﬂightispresentedwithits
mostcompellingattributeinthetheme,andtheremainingattributesintherheme.For
instance,considerthestudentexample(S)inFigure2.AfterpresentingtheBMIﬂight—
whichhasagoodprice,isdirect,andarrivesnearthedesiredtime—thequestionarises
whetherthereareanycheaperalternatives.Becausethesecondoptionhaspriceasits
mostcompellingattribute—andgiventhatitistheleastexpensiveﬂightavailable—itis
identiﬁedasthe CHEAPESTﬂight,withthisphraseformingthethemeoftheutterance.
Asanotherillustration,considerthebusiness-classtravelerexample(BC)inFigure2.
After presenting the British Airways ﬂight, which has availability in business class
butisnotdirect,thequestionariseswhetherthereareanydirectﬂightsavailable.The
presentationofthesecondoptionaddressesthisimplicitquestion,introducingtheBMI
ﬂightwiththethemephraseThereisaDIRECTﬂight.
Theoutputofthecontentplannerisderivedfromthehierarchicalgoalstructure
producedduringplanning.Figure7showstheresultingcontentplanforthestudent
example.Notethat,aspartofsuggestingthesecondﬂightoptioninthesequence(f2),
subgoalsareintroducedthatidentifytheoptionbyinformingtheuserthattheoption
hastypeflightandthattheoptionhastheattributecheapest,wherethisattributeis
themostcompellingonefortheuser.Bothsubgoalsaremarkedaspartofthetheme
169
ComputationalLinguistics Volume36,Number2
Figure7
Contentplanforstudentexample(S).
(rheme marking is the default); the subgoal for the option type is also marked for
deﬁniteness(indeﬁniteisthedefault),asthecheapestattributeuniquelyidentiﬁesthe
ﬂight.Theremaininginformationforthesecondﬂightispresentedintermsofacontrast
betweenitspositiveandnegativeattributes,asdeterminedbytheusermodel.
Thewayinwhichourpresentationstrategyusesthemephrasestoconnectalterna-
tiveﬂightsuggestionstoimplicitquestionsisrelatedtoPrevost’s(1995)useoftheme
phrases to link system answers to immediately preceding user questions. It is also
170
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
Figure8
Semanticdependencygraphproducedbythesentenceplannerfor(TheCHEAPESTﬂight)
theme
(is
onRYANAIR)
rheme
.
relatedtoKruijff-Korbayov´aetal.’s(2003)useofthemephrasestolinkutteranceswith
questionsunderdiscussion(Ginzburg1996;Roberts1996)inaninformation-statebased
dialoguesystem.Aninterestingchallengethatremainsforfutureworkistodetermine
towhatextentourpresentationstrategycanbegeneralizedtohandletheme/rheme
partitioning,bothforexplicitquestionsacrossturnsaswellforimplicitquestionswithin
turns.
2.5SentencePlanning
ThesentenceplanningagentusestheXalanXSLTprocessortotransformtheoutputof
thecontentplannerintoasequenceofLFsthatcanberealizedbytheOpenCCGagent.
Itisintendedtobearelativelystraightforwardcomponent,asthecontentplannerhas
beendesignedtoimplementthemostimportanthigh-levelgenerationchoices.Itspri-
maryresponsibilityistolexicalizethebasicspeechactsinthecontentplan—whichmay
appearinreferringexpressions—alongwiththerhetoricalspeechactsthatconnectthem
together.Whenalternativelexicalizationsareavailable,allpossibilitiesareincludedin
apackedstructure(FosterandWhite2004;White2006a).Thesentenceplannerisalso
responsibleforaddingdiscoursemarkerssuchas also and but,addingpronouns,and
choosingsentenceboundaries.Itadditionallyimplementsasmallnumberofrhetorical
restructuringoperationsforenhancedﬂuency.
Thesentenceplannermakesuseofapproximately50XSLTtemplatestorecursively
transformcontentplansintologicalforms.Anexamplelogicalformthatresultsfrom
applyingthesetemplatestothecontentplanshowninFigure7appearsinFigure8(with
alternativelexicalizationssuppressed).AsdescribedfurtherinSection2.6.1,thelogical
formsproducedbythesentenceplanneraresemanticdependencystructures,
10
which
makeuseofaninfofeaturetoencodetheme/rhemepartitioning,andakonfeatureto
implementSteedman’s(2006)notionofkontrast(Vallduv´ıandVilkuna1998).Following
Steedman, kontrast is assigned to the interpretations of words which contribute to
distinguishing the theme or rheme of the utterance from other alternatives that the
contextmakesavailable.
Inordertotriggertheinclusionofcontext-sensitivediscoursemarkerssuchasalso
andeither,thesentenceplannercomparestheinformactsforconsecutiveﬂightoptions
10 Thenodesofthegrapharetypicallylabelledbylexicalpredicates.Anexceptionhereisthehas-rel
predicate,whichallowsthepredicativecomplementon Ryanairtointroducethe〈AIRLINE〉roleinaway
thatissimilartothedependencystructurefortheRyanair ﬂight.
171
ComputationalLinguistics Volume36,Number2
toseewhetheractswiththesametypehavethesamevalue.
11
Thesesamecheckscan
alsotriggerde-accenting.Forexample,whentwoconsecutiveﬂightshavenoseatsin
businessclass,thesecondonecanbedescribedusingthephraseithasNOAVAILABILITY
inbusinessclasseither,wherebusinessclasshasbeende-accented.
ThedevelopmentoftheXSLTtemplateswasmadeconsiderablyeasierbytheability
toinvokeOpenCCGtoparseatargetsentenceandthenusetheresultinglogicalform
asthebasisofatemplate.(SeeSection3foradescriptionofhowthetargetsentencesin
theFLIGHTSvoicescriptweredeveloped.)UsingLFtemplates,ratherthantemplates
at the string level, makes it simpler to uniformly handle discourse markers such as
alsoandeither,whichhavedifferentpreferredpositionswithinaclause,dependingin
partonwhichverbtheymodify.LFtemplatesalsosimplifythetreatmentofsubject–
verbagreement.Additionally,byemployingLFtemplateswithasingletheme/rheme
partition,itbecomespossibletounderspecifywhetherthethemeandrhemewillbe
realizedbyoneintonationalphraseeachorbymultiplephrases(seeSection2.6.2).At
thesametime,certainaspectsoftheLFtemplatescanbeleftalone,whenthereisno
needforfurtheranalysisandgeneralization.
Asnotedearlier,theuseofLFtemplatesfurtherblursthetraditionaldistinction
betweentemplate-basedand“real”NLGthatvanDeemter,Krahmer,andTheune(2005)
havecalledintoquestion.Inthecaseofreferringexpressionsespecially,LFtemplatesin
FLIGHTSrepresentapracticalandﬂexiblewaytodealwiththeinteractionofdecisions
madeatthesentenceplanninglevel,asthespeechactsidentifyingﬂightoptionsare
consideredtogetherwiththeotherbasicandrhetoricalspeechactsintheapplicability
conditionsforthetemplatesthatstructureclauses.Inthisway,optionscanbeidentiﬁed
notonlyindeﬁniteNPs,suchasthe CHEAPESTﬂight,butalsointhere-existentialsand
conditionals,suchasthere is a DIRECTﬂight onBMIthat ... orif you prefer to ﬂy DIRECT,
there’saBMIﬂightthat....Wemayfurtherobservethatthetraditionalapproachtogen-
eratingreferringexpressions(ReiterandDale2000),whereadistinguishingdescription
foranentityisconstructedduringsentenceplanningwithoutregardtoausermodel,
wouldnotﬁtinourarchitecture,wheretheusermodeldrivestheselectionofareferring
expression’scontentatthecontentplanninglevel.
Although the use of LF templates in XSLT represents a practical approach to
handling sentence planning tasks that were not the focus of our research, it is not
one that promotes reuse, and thus it is worth noting which aspects of our sentence
plannerwouldposechallengesforamoredeclarativeandgeneraltreatment.Thebulk
ofthetemplatesconcerndomain-speciﬁclexicalizationandarestraightforward;given
the way these were developed from the results of OpenCCG parsing, it is conceiv-
able that this process could be largely automated from example input–output pairs.
The templates for adding pronouns and discourse markers require more expertise
butremainreasonablystraightforward;thetemplatesforrhetoricalrestructuringand
choosingsentenceboundaries,incontrast,arefairlyintricate.Inprinciple,satisfactory
resultsmightbeobtainedusingamoregeneralsetofoptionsforhandlingpronouns,
discoursemarkers,andsentenceboundaries,togetherwithanovergenerate-and-rank
methodology;weleavethispossibilityasatopicforfutureresearch.
2.6SurfaceRealization
2.6.1ChartRealizationwithOpenCCG.Forsurfacerealization,weusetheOpenCCGopen
sourcerealizer(White2004,2006a,2006b).AdistinguishingfeatureofOpenCCGisthat
11 Infuturework,thesecheckscouldbeextendedtoapplyacrossturnsaswell.
172
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
itimplementsahybridsymbolic-statisticalchartrealizationalgorithmthatcombines
(1)atheoreticallygroundedapproachtosyntaxandsemanticcomposition,with(2)the
useofintegratedlanguagemodelsformakingchoicesamongtheoptionsleftopenby
thegrammar.Insodoing,itbringstogetherthesymbolicchartrealization(Kay1996;
Shemtov1997;Carrolletal.1999;Moore2002)andstatisticalrealization(Knightand
Hatzivassiloglou1995;Langkilde2000;BangaloreandRambow2000;Langkilde-Geary
2002;OhandRudnicky2002;Ratnaparkhi2002)traditions.Anotherrecentapproachto
combiningthesetraditionsappearsinCarrollandOepen(2005),whereparseselection
techniquesareincorporatedintoanHPSGrealizer.
Likeotherrealizers,theOpenCCGrealizerispartiallyresponsiblefordetermining
wordorderandinﬂection.Forexample,therealizerdeterminesthatalsoshouldprefer-
ablyfollowtheverbinThereisalsoaverycheapﬂightonAirFrance,whereasinothercases
ittypicallyprecedestheverb,asinIalsohaveaﬂightthatleavesLondonat3:45p.m.Italso
enforcessubject–verbagreement,forexample,betweenisandﬂight,orbetweenareand
seats.Lesstypically,inFLIGHTSandintheCOMIC
12
system,theOpenCCGrealizer
additionallydeterminestheprosodicstructure,intermsofthetypeandplacementof
pitchaccentsandedgetones,basedontheinformationstructureofitsinputlogical
forms.AlthoughOpenCCG’salgorithmicdetailshavebeendescribedintheworkscited
above,detailsofhowprosodicstructurecanbedeterminedfrominformationstructure
inOpenCCGappearfortheﬁrsttimeinthisarticle.
The grammars used in the FLIGHTS and COMIC systems have been manually
writtenwiththeaimofachievingveryhighquality.However,tostreamlinegrammar
development,thegrammarhasbeenallowedtoovergenerateinareaswhererulesare
difﬁculttowriteandwhere n-grammodelscanbereliable;inparticular,itdoesnot
sufﬁcientlyconstrainmodiﬁerorder,whichinthecaseofadverbplacementespecially
canleadtoalargenumberofpossibleorderings.Additionally,itallowsforaone-to-
manymappingfromthemesorrhemestoedgetones,yieldingmanyvariantsthatdiffer
onlyinboundarytypeorplacement.Aswillbeexplainedsubsequently,weconsider
thismoreﬂexible,data-drivenapproachtophrasingtobebettersuitedtotheneedsof
generationthanwouldbeamoredirectimplementationofSteedman’s(2000a)theory,
whichwouldrequireallphrasingchoicestobemadeatthesentenceplanninglevel.
WehavebuiltalanguagemodelforthesystemfromtheFLIGHTSspeechcorpus
describedinSection3.1.Toenhancegeneralization,namedentitiesandscalaradjectives
havebeenreplacedwiththenamesoftheirsemanticclasses(suchasTIME,DATE,CITY,
AIRLINE,etc.),asisoftendoneinlimiteddomainsystems.Notethatinthecorpusand
themodel,pitchaccentsaretreatedasintegralpartsofwords,whereasedgetonesand
punctuationmarksappearasseparatewords.Thelanguagemodelisa4-gramback-off
modelbuiltwiththeSRIlanguagemodelingtoolkit(Stolcke2002),keepingall1-counts
andusingRistad’s(1995)naturaldiscountinglawforsmoothing.OpenCCGhasitsown
implementationforrun-timescoring;inFLIGHTS,themodeladditionallyincorporates
an a/an-ﬁlter, which assigns a score of zero to sequences containing a followed by a
vowel,oranfollowedbyaconsonant,subjecttoexceptionsculledfrombigramcounts.
2.6.2 Deriving
Prosody. CCG is a uniﬁcation-based categorial framework that is both
linguisticallyandcomputationallyattractive.WeprovidehereabriefoverviewofCCG;
anextensiveintroductionappearsinSteedman(2000b).
12 http://www.hcrc.ed.ac.uk/comic/.
173
ComputationalLinguistics Volume36,Number2
Figure9
AsimpleCCGderivation.
Figure10
ACCGderivationwithnon-standardconstituents.
AgrammarinCCGisdeﬁnedalmostentirelyintermsoftheentriesinthelexicon,
whichare(possiblycomplex)categoriesbearingstandardfeatureinformation(suchas
verbform,agreement,etc.)andsubcategorizationinformation.CCGhasasmallsetof
ruleswhichcanbeusedtocombinecategoriesinderivations.Thetwomostbasicrules
areforward(>)andbackward(<)functionapplication.Theserulesareillustratedin
Figure9,whichshowsthederivationofasimplecopularsentence(withnoprosodic
information).
13
Intheﬁgure,thenounandpropernamereceiveatomiccategorieswith
labelsnandnp,respectively.Theremainingwordsreceivefunctionalcategories,suchas
s
dcl
\np/(s
adj
\np)fortheverbis;thiscategoryseeksapredicativeadjective(s
adj
\np)toits
rightandannptoitsleft,andreturnsthecategorys
dcl,foradeclarativesentence.Note
thatdclandadjarevaluesfortheformfeature;otherfeatures,suchasthosefornumber
andcase,havebeensuppressedintheﬁgure(ashasthefeaturelabel,form).
CCG also employs further rules based on the composition (B), type raising (T),
and substitution (S) combinators of combinatory logic. These rules add an element
ofassociativitytothegrammar,makingpossiblemultiplederivationswiththesame
semantics.Theyarecrucialforbuildingthe“non-standard”constituentsthatarethe
hallmarkofcategorialgrammars,andwhichareessentialforCCG’shandlingofcoor-
dination,extraction,intonation,andotherphenomena.Forexample,Figure10shows
howthecategoryfor there canbetype-raisedandcomposedwiththecategoryfor is
inordertoderivetheconstituent there is a direct ﬂight (s
dcl
/(s
adj
\np)),whichcutsacross
the VP in traditional syntax. Because there is a direct ﬂight corresponds to the theme
phrase,and on BMI totherhemephrase,intheﬁrstclauseofthesecondsentencein
13 Thesederivationsareshownfromtheparsingperspective,startingwithanorderedsequenceofwordsat
thetop.Duringrealization,itisthesemanticsratherthanthewordsequencewhichisgiven;theword
sequenceisdeterminedduringthesearchforaderivationthatcoverstheinputsemantics.Seethe
discussionofFigure11(laterinthissection)foradiscussionofthesemanticrepresentationsusedin
OpenCCG.
174
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
Figure11
Aderivationofthemeandrhemephrases.
thebusiness-class(BC)exampleinFigure2,thederivationinFigure10alsoshowshow
CCG’sﬂexiblenotionofconstituencyallowstheintonationstructureandinformation
structure to coincide, where the intonation coincides with surface structure, and the
informationstructureisbuiltcompositionallyfromtheconstituentanalysis.
InSteedman(2000a),themeandrhemetunesarecharacterizedbydistinctpatterns
ofpitchaccentsandedgetones.Thenotationforpitchaccentsandedgetonesistaken
fromPierrehumbert(1980)andToBI(Silvermanetal.1992).Wehaveimplementedare-
ducedversionofSteedman’stheoryinOpenCCG,wherethemetunesgenerallyconsist
ofoneormoreL+H*pitchaccentsfollowedbyaL-H%compoundedgetoneattheend
ofthephrase,
14
andrhemetunesconsistofoneormoreH*pitchaccentsfollowedby
aL-orL-L%boundaryatphraseend.Additionally,yes/noquestionstypicallyreceive
aH-H%ﬁnalboundary,andL-H%boundariesareoftenusedascontinuationrisesto
marktheendofanon-ﬁnalconjoinedphrase.
ThekeyimplementationideainSteedman’s(2000a)approachistousefeatureson
thesyntacticcategoriestoenforceinformationstructuralphrasingconstraints—thatis,
toensurethattheintonationalphrasesareconsistentwiththetheme/rhemepartition
inthesemantics.Forexample,ifthere is a direct ﬂightcorrespondstothethemeandon
BMItherheme,thenthesyntacticfeaturesensurethattheintonationbracketstheclause
as (there is a direct ﬂight) (on BMI),ratherthan (there) (is a direct ﬂight on BMI) or (there
is) (a direct ﬂight on BMI),etc.Toillustrate,Figure11shows,againfromtheparsing
perspective,howthemeandrhemephrasesarederivedinOpenCCGforthesubject
NPandVPofFigure9,respectively.(Tosavespace,pitchaccentsandedgetoneswill
henceforthbewrittenusingsubscriptsandstringelements,ratherthanappearingona
separatetonaltier.)Intheﬁgure,thecategoryforcheapesthastheinfofeatureoneach
14 Toreduceambiguityinthegrammar,unmarkedthemes—thatis,constituentswhichappeartobe
thematicbutwhicharenotgroupedintonationallyintoaseparatephrase—areassumedtobe
incorporatedasbackgroundpartsoftherheme,assuggestedinCalhounetal.(2005).
175
ComputationalLinguistics Volume36,Number2
atomiccategorysetto th(eme),andRyanair hasits info featuresetto rh(eme),dueto
thepresenceoftheL+H*andH*accents,respectively.Theremainingwordshaveno
accents,andthustheircategorieshaveavariable(M,forEME)asthevalueoftheinfo
featureoneachatomiccategory.
15
Allthewordsalsohaveavariable(O)asthevalueof
theownerfeature,discussedsubsequently,oneachatomiccategory.Aswordscombine
into phrases, the info variables serve to propagate the theme or rheme status of a
phrase;thus,thephrasethecheapest
L+H∗
ﬂighthascategorynp
th,O,whereasisonRyanair
H∗
endsupwithcategory s
dcl,rh,O
\np
rh,O
.Becausethesetwophraseshaveincompatible info
values,theycannotcombinebeforebeing“promoted”tointonationalphrasesbytheir
respective edge tones. In this way, the constraint that phrasing choices respect the
theme/rhemepartitionisenforced.
Theedgetonesallowcompleteintermediateorintonationalphrasestocombineby
changing the value of the info feature to phr(ase) in the result category.
16
Note that
theargumentcategoriesoftheedgetonesdonotselectforaparticularvalueofthe
info feature;instead,L-H%has s
h
$
1
asitsargumentcategory,whereasL-L%has s
s
$
1,
whereh(earer)ands(peaker)aretherespectivevaluesoftheownerfeature.
17
Combination
withanedgetoneuniﬁesthe owner featuresthroughoutthephrase.Inaprototypical
themephrase,theheareristheowner,whereasinarhemephrase,thespeakeristhe
owner.
Likeothercompositional grammatical frameworks, CCGallowslogicalformsto
bebuiltinparallelwiththederivationalprocess.Traditionally,theλ-calculushasbeen
usedtoexpresssemanticinterpretations,butOpenCCGinsteadmakesuseofamore
ﬂexiblerepresentationalframework,HybridLogicDependencySemantics(Baldridge
andKruijff2002;Kruijff2003),orHLDS.InHLDS,hybridlogic(Blackburn2000)terms
are used to describe semantic dependency graphs, such as the one seen earlier in
Figure8.AsdiscussedinWhite(2006b),HLDSiswellsuitedtotherealizationtask,in
thatitenablesanapproachtosemanticconstructionthatensuressemanticmonotonicity,
simpliﬁesequalitytests,andavoidscopyingincoordinateconstructions.
To illustrate, four lexical items from Figure 11 appear in Example (1). The three
wordsarederivedbyalexicalrulewhichaddspitchaccentsandinformationstructure
features to the base forms. The format of the entries is lexeme turnstileleft category, where the
categoryisitselfapairintheformatsyntax:logicalform.Thelogicalformisaconjunction
ofelementarypredications(EPs),whichcomeinthreevarieties:lexicalpredications,
such as @
E
be; semantic features, such as @
E
〈TENSE〉pres; and dependency relations,
suchas@
E
〈ARG〉X.
(1) a. cheapest
L+H∗
turnstileleft n
X,th,O
/n
X,th,O
:@
X
〈HASPROP〉P∧@
P
cheapest∧
@
P
〈INFO〉th∧@
P
〈OWNER〉O∧@
P
〈KON〉+
b. Ryanair
H∗
turnstileleft np
X,rh,O
:@
X
Ryanair∧
@
X
〈INFO〉rh∧@
X
〈OWNER〉O∧@
X
〈KON〉+
15 Inpractice,variableslikeMarereplacedwith“fresh”variables(suchasM 1)duringlexicallookup,so
thattheMvariablesaredistinctfordifferentwords.
16 The$variablesrangeovera(possiblyempty)stackofarguments,allowingtheedgetonestoemploya
singles$
1
\s$
1
categorytocombinewiths/(s\np),s\np,andsoon.Asindicatedintheﬁgure,thenew
valueoftheinfofeature(phr)isautomaticallydistributedthroughouttheargumentsoftheresult
category.
17 SeeSteedman(2000a)foradiscussionofthisnotionof“ownership,”orresponsibility.
176
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
c. is turnstileleft s
E,dcl,M,O
\np
X,M,O
/(s
P,adj,M,O
\np
X,M,O
):@
E
be∧
@
E
〈TENSE〉pres∧@
E
〈ARG〉X∧@
E
〈PROP〉P∧
@
E
〈INFO〉M∧@
E
〈OWNER〉O∧@
E
〈KON〉–
d.L-H% turnstileleft s
phr
$
1
\s
h
$
1
Intheseentries,theindices(ornominals,inhybridlogicterms)inthelogicalforms,
such as X, P,andE, correspond to nodes in the semantic graph structure, and are
linkedtothesyntacticcategoriesviatheindexfeature.Similarly,thesyntacticfeatures
info and owner have associated 〈INFO〉 and 〈OWNER〉 features in the semantics. As
discussed previously, in derivations the values of the info and owner features are
propagatedthroughoutanintonationalphrase,whichhastheeffectofpropagatingthe
valuesofthe〈INFO〉and〈OWNER〉semanticfeaturestoeverynodeinthedependency
graph corresponding to the phrase. In this way, a distributed representation of the
theme/rhemepartitionisencoded,inafashionreminiscentofKruijff’s(2003)approach
torepresentingtopic–focusarticulationusinghybridlogic.Bycontrast,the〈KON〉fea-
ture(cf.Section2.5)isapurelylocalone,andthusappearsonlyinthesemantics.Note
thatbecausetheedgetonesdonotaddanyelementarypredications,oneormoreedge
tones—andthusoneormoreintonationalphrases—maybeusedtoderivethesame
themeorrhemeinthelogicalform.
Tosimplifythesemanticrepresentationsthesentenceplannermustproduce,Open-
CCGincludesdefaultrulesthat(whereapplicable)propagatethevalueofthe 〈INFO〉
featuretosubtreesinthelogicalform,setthe〈OWNER〉featuretoitsprototypicalvalue,
andsetthevalueofthe 〈KON〉 featuretofalse.Whenappliedtothelogicalformfor
thesemanticdependencygraphinFigure8,therulesyieldtheHLDSterminExam-
ple(2).AfterthislogicalformisﬂattenedtoaconjunctionofEPs,lexicalinstantiation
looks up relevant lexical entries, such as those in Example (1), and instantiates the
variablestomatchthoseintheEPs.Thechart-basedsearchforcompleterealizations
proceedsfromtheseinstantiatedlexicalentries.
(2) @
e
(be∧〈TENSE〉pres∧〈INFO〉rh∧〈OWNER〉s∧〈KON〉–∧
〈ARG〉(f ∧ﬂight)∧〈DET〉the∧〈NUM〉sg∧〈INFO〉th∧〈OWNER〉h∧〈KON〉–∧
〈HASPROP〉(c∧cheapest∧〈INFO〉th∧〈OWNER〉h∧〈KON〉+))∧
〈PROP〉(p∧has-rel∧〈INFO〉rh∧〈OWNER〉s∧〈KON〉–∧
〈OF〉f ∧
〈AIRLINE〉(r∧Ryanair∧〈INFO〉rh∧〈OWNER〉s∧〈KON〉+)))
GivenourfocusinFLIGHTSonusingintonationtoexpresscontrastsintelligibly,
wehavechosentoemployhardconstraintsintheOpenCCGgrammaronthechoice
ofpitchaccentsandontheuseofedgetonestoseparatethemeandrhemephrases.
However,thegrammaronlypartiallyconstrainsthetypeofedgetone(asexplained
previously),andallowsthethemeandrhemetobeexpressedbyoneormoreintona-
tionalphraseseach;consequently,theﬁnalchoiceofthetypeandplacementofedge
tonesisdeterminedbythe n-grammodel.Toillustrate,considerExample(3),which
showshowthefrequentﬂyersentenceseeninFigure2isdividedintofourintonational
phrases.Otherpossibilities(amongmany)allowedbythegrammarincludeleavingout
theL-L%boundarybetweenﬂightandarriving,whichwouldyieldaphrasethat’slikely
tobeperceivedastoolong,oraddingaL-orL-L%boundarybetween there’s and a,
whichwouldyieldtwounnecessarilyshortphrases.
(3) There’saKLM
H∗
ﬂightL-L%arrivingBrussels
H∗
atfour
H∗
ﬁfty
H∗
p.m.
H∗
L-L%,
butbusiness
H∗
class
H∗
isnot
H∗
available
H∗
L-H%andyou’dneedtoconnect
H∗
in
Amsterdam
H∗
L-L%.
177
ComputationalLinguistics Volume36,Number2
Toallowforthisﬂexiblemappingbetweenthemesandrhemesandoneormoreinto-
nationalphrases,wetakeadvantageofourdistributedapproachtorepresentingthe
theme/rhemepartition,whereedgetonesmarktheendsofintonationalphraseswith-
outintroducingtheirownelementarypredications.Asanalternative,wecouldhave
associatedathemeorrhemepredicationwiththeedgetones,whichwouldbemore
inlinewithSteedman’s(2000a)approach.However,doingsowouldmakeitnecessary
toincludeonesuchpredicationperphraseinthelogicalforms,therebyanticipating
thedesirednumberofoutputthemeandrhemephrasesintherealizer’sinput.Given
thatthenaturalnessofintonationalphrasingdecisionscandependonsurfacefeatures
likephraselength,weconsiderthedistributedapproachtorepresentingtheme/rheme
statustobebettersuitedtotheneedsofgeneration.
2.7ComparisontoBaselineProsodyPredictionModels
As noted in Section 2.2, spoken language dialogue systems often include a separate
prosodypredictioncomponent(Pan,McKeown,andHirschberg2002;Walker,Rambow,
and Rogati 2002), rather than determining prosodic structure as an integral part of
surface realization, as we do here. Although it is beyond the scope of this article to
compare our approach to a full-blown, machine learning–based prosody prediction
model,wedopresentinthissectionanexpertevaluationthatshowsthatourapproach
outperformsstrongbaseline n-grammodels.Inparticular,weshowthattheinforma-
tionstructuralconstraintsinthegrammarplayanimportantroleinproducingtarget
prosodicboundaries,andthattheseboundarychoicesarepreferredtothosemadeby
ann-grammodelinisolation.
AccordingtoPan,McKeown,andHirschberg(2002,page472),word-basedn-gram
modelscanbesurpringlygood:“Theworditselfalsoprovestobeagoodpredictorfor
bothaccentandbreakindexprediction....Sincetheperformanceofthis[word]model
isthebestamongallthe[single-feature]accentpredictionmodelsinvestigated,itseems
tosuggestthatforaCTS[Concept-to-Speech]applicationcreatedforaspeciﬁcdomain,
featureslikewordcanbequiteeffectiveinprosodyprediction.”Indeed,althoughtheir
bestaccentpredictionmodelexceededtheword-basedone,thedifferencedidnotreach
statisticalsigniﬁcance(page485).Additionally,wordpredictability,measuredbythe
logprobabilityofbigramsandtrigrams,wasfoundtosigniﬁcantlycorrelatewithpitch
accentdecisions(pages482–483),andcontributedtotheirbestmachine-learnedmodels
foraccentpresenceandboundarypresence.
18
Asbaselinen-grammodels,wetrained1-to4-grammodelsforpredictingaccents
andboundariesusingtheFLIGHTSspeechcorpus(Section3.1),thesamecorpususedto
traintherealizer’slanguagemodel.Thebaselinen-grammodelsarefactoredlanguage
models(BilmesandKirchhoff2003),withwords,accents,andboundariesasfactors.
Theaccentmodelshaveaccentasthechildvariableand1–4wordsasparentvariables,
startingwiththecurrentword,andincludinguptothreepreviouswords.Theboundary
modelsareanalogous,withboundaryasthechildvariable.Withthesemodels,each
maximumlikelihoodpredictionofpitchaccentoredgetoneisindependentofallother
choices,sothereisnoneedtoperformabest-pathsearch.Themajoritybaselinepredicts
noaccentandnoedgetoneforeachword.
18 Notethatinoursetting,itwouldbeimpossibletoexactlyreplicatePanetal.’smodels,inwhichsyntactic
boundariesplayanimportantrole,asCCGdoesnothavearigidnotionofsyntacticboundary.
178
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
Table1
Baselinepitchaccentandboundarypredictionaccuracyagainsttargettunes.
Accuracy
Majority 1-gram 2-gram 3-gram 4-gram N
AccentPresence 73.3% 98.0% 98.6% 97.4% 97.4% 344
AccentType 73.3% 96.5% 97.4% 96.2% 96.8% 344
BoundaryPresence 81.1% 91.9% 94.5% 95.4% 95.1% 344
BoundaryType 81.1% 91.0% 92.7% 93.0% 93.9% 344
Wetestedtherealizerandthebaselinen-grammodelsonthe31sentencesusedto
synthesizethestimuliintheperceptionexperimentdescribedinSection4(seeFigure13
for an example mini-dialogue). None of the test sentences appear verbatim in the
FLIGHTSspeechcorpus.Thetestsentencescontaintargetprosodicstructuresintended
to be appropriate for the discourse context. Given these structures, we can quantify
the accuracy with which the realizer is able to reproduce the pitch accent and edge
tonechoicesinthetargetsentences,andcompareittotheaccuracywithwhichn-gram
modelspredictthesechoicesusingmaximumlikelihood.Notethatthetargetprosodic
structuresmaynotrepresenttheonlynaturalchoices,motivatingtheneedfortheexpert
evaluationdescribedfurthersubsequently.
Althoughtherealizeriscapableofgeneratingthetargetprosodicstructureofeach
testsentenceexactly,thetestsentence(withitstargetprosody)isnotalwaysthetop-
rankedrealizationofthecorrespondinglogicalform,whichmaydifferinwordorder
orchoiceoffunctionwords.Thus,tocomparetherealizer’schoicesagainstthetarget
accentsandboundaries,wegeneratedn-bestlistsofrealizationsthatincludedthetarget
realization, and compared this realization to others in the n-best list with the same
wordsinthesameorder(ignoringpitchaccentsandedgetones).Ineachcase,thetarget
realizationwasrankedhigherthanallotherrealizationswiththesamewordsequence,
andsowemayconcludethattherealizerreproducesthetargetaccentandboundary
choicesinthetestsentenceswith100%accuracy.
Theaccuracywithwhichthebaseline n-grammodelsreproducethetargettunes
is shown in Table 1. As the test sentences are very similar to those in the FLIGHTS
speech corpus, the accent model performs remarkably well, with the bigram model
reproducing the exact accent type (including no accent) in 97.4% of the cases, and
agreeingonthechoiceofwhethertoaccentthewordatallin98.6%ofthecases.The
boundarymodelalsoperformswell,thoughsubstantiallyworsethantherealizer,with
the4-grammodelreproducingtheboundarytype(includingnoboundary)in93.9%of
thecases,andagreeingonboundarypresencein95.1%ofthecases.
19
InspiredbyMarsi’s(2004)workonevaluatingoptionalityinprosodyprediction,we
askedanexpertToBIannotator,whowasunfamiliarwiththeexperimentalhypotheses
underinvestigation,toindicateforeachtestsentencetherangeofcontextuallyappro-
priatetunesbyprovidingallthepitchaccentsandedgetonesthatwouldbeacceptable
19 Becausetheboundarymodelssometimesfailedtoincludeaboundarybeforeacommaorfullstop,
defaultL-L%boundarieswereaddedinthesecases.
179
ComputationalLinguistics Volume36,Number2
Table2
Examplescomparingtargettunestobaselineprosodypredictionmodels,withexpertcorrections
(Items07-2and06-1).
(a) target theonlydirect
L+H∗
ﬂightL-H%leavesat5:10
H∗
L-L%.
edits none
n-grams theonlydirect
H∗
ﬂightleavesat5:10
H∗
L-L%.
edits theonlydirect
L+H∗
ﬂightL-leavesat5:10
H∗
L-L%.
(b) target there’sadirect
H∗
ﬂightonBritishAirways
H∗
withagood
H∗
priceL-L%.
edits there’sadirect
H∗
ﬂightonBritishAirways
H∗
L-withagood
H∗
priceL-L%.
n-grams there’sadirect
H∗
ﬂightonBritishAirways
H∗
L-L%withagood
L+H∗
priceL-L%.
edits none
for each word.
20
However, our annotator found this task to be too difﬁcult, in part
becauseofthedifﬁcultyofcomingupwithallpossibleacceptabletunes,andinpart
because of dependencies between the choices made for each word. For this reason,
we instead chose to follow the approach taken with the Human Translation Error
Rate(HTER)post-editmetricinMT(Snoveretal.2006),andaskedourannotatorto
indicate,forthetargettuneandthen-grambaselinetune,whichaccentsandboundaries
would need to change in order to yield a tune appropriate for the context. For the
n-grambaselinetune,weusedthechoicesofthebigramaccentmodelandthe4-gram
boundarymodel.
Examplesofhowthetargettunes(andrealizerchoices)comparetothoseofthe
baseline accent and boundary prediction models appear in Table 2, along with the
corrections provided by our expert ToBI annotator.
21
In Table 2(a), the target tune,
whichcontainsthethemephrasethe only direct
L+H∗
ﬂight L-H%,wasconsideredfully
acceptable.Bycontrast,withthetuneofthen-grammodels,theH*accentondirectwas
notconsideredacceptableforthecontext,andatleastaminorphraseboundarywas
considerednecessarytodelimitthethemephrase.InTable2(b),wehaveanexample
wherethetargettunewasnotconsideredfullyacceptable:Althoughthetargetconsisted
ofasingle,all-rhemeintonationalphrase,withnointermediatephrases,ourannotator
indicatedthatatleastaminorphrasebreakwasnecessaryafter British Airways.The
n-grammodelsassignedamajorphrasebreakatthispoint,whichwasalsoconsidered
acceptable.Notealsothatthen-grammodelshadaL+H*accentongood,incontrastto
thetargettune’sH*,butbothchoiceswereconsideredacceptable.
Table3showsthenumberofaccentandboundarycorrectionsforall31testsen-
tencesatdifferentlevelsofspeciﬁcity.Overall,therewerejust10accentorboundary
correctionsforthetargettunes,versus24forthoseofthebaselinemodels,outof688
totalaccentandboundarychoices,asigniﬁcantdifference(p=0.01,Fisher’sExactTest
[FET], 1-tailed). With the accents, there were fewer corrections for the target tunes,
butnotmanyineithercase,andthedifferencewasnotsigniﬁcant.Ofcourse,witha
20 Wethankoneoftheanonymousreviewersforthissuggestion.NotethatinMarsi’sstudy,havingone
annotatorindicateoptionalitywasfoundtobeareasonableapproximationofderivingoptionalityfrom
multipleannotations.
21 Duringrealization,multiwordsaretreatedassinglewords,suchasBritish Airwaysand5:10 (ﬁveten a.m.).
Accentsonmultiwordsaredistributedtotheindividualwordsbeforetheoutputissenttothespeech
synthesizer.
180
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
Table3
Numberofprosodiccorrectionsofdifferenttypesinallutterancesandthemeutterancesforthe
targettunesandtheonesselectedbyn-grammodels.Itemsinboldaresigniﬁcantlydifferentat
p=0.05orlessbyaone-tailedFisher’sExactTest.
AllUtts Theme Utts
Target n-grams Target n-grams
Totalcorrections 10 24 3 11
Accents 4 7 2 5
Presence 3 4 2 3
Boundaries 6 17 1 6
Presence 2 13 0 4
Major 0 90 3
largersamplesize,orwithtestsentencesthatarelesssimilartothoseinthecorpus,
asigniﬁcantdifferencecouldarise.Withtheboundaries,outof344choices,thetarget
tuneshadsixcorrections,onlytwoofwhichinvolvedthepresenceofaboundary,and
noneofwhichinvolvedamissingmajorboundary;bycontrast,the n-grambaseline
had 17, 13, and 9 such corrections, respectively, a signiﬁcant difference in each case
(p=0.02,p=0.003,p=0.002,respectively,FET).Inthesubsetof12sentencesinvolving
themephrases,wheretheintonationismoremarkedthanintheall-rhemesentences,
thetargettunesagainhadsigniﬁcantlyfewercorrectionsoverall(3vs.11corrections
outof220totalchoices;p=0.03,FET),andthedifferenceinboundaryandboundary
presencecorrectionsapproachedsigniﬁcance(p=0.06ineachcase,FET).
Havingshownthattheinformationstructuralconstraintsinthegrammarplayan
importantroleinproducingtargetrealizationswithcontextuallyappropriateprosodic
structures—inparticular,inmakingacceptableboundarychoices,wherethechoiceis
not deterministically rule-governed—we now brieﬂy demonstrate that the realizer’s
n-grammodel(seeSection2.6.1)hasanimportantroletoplayaswell.Table4compares
therealizer’soutputonthetestsentencesagainstitsoutputwiththelanguagemodel
disabled,inwhichcaseanarbitrarychoiceiseffectivelymadeamongthoseoutputs
allowedbythegrammar.Notsurprisingly,giventhatthegrammarhasbeenallowed
to overgenerate, the realizer produces far more exact matches and far higher BLEU
(Papinenietal.2001)scoreswithitslanguagemodelthanwithout.Lookingatthedif-
ferencesbetweentherealizer’shighestscoringoutputsandthetargetrealizations,the
differenceslargelyappeartorepresentcasesofacceptablevariation.Themostfrequent
differenceconcernswhetheranauxiliaryorcopularverbiscontractedornot,where
eitherchoiceseemsreasonable.Mostoftheotherdifferencesrepresentminorvariations
inwordorder,suchasdirect
H∗
Air France
H∗
ﬂightvs.direct
H∗
ﬂight on Air France
H∗
.By
Table4
Impactoflanguagemodelonrealizerchoice.
ExactMatch BLEU
Realizer 61.3% 0.9505
NoLM 0.0% 0.6293
181
ComputationalLinguistics Volume36,Number2
contrast,manyoftheoutputschosenwithnolanguagemodelscoringcontainunde-
sirable variation in word order or phrasing; for example: Air France
H∗
direct
H∗
ﬂight
insteadofdirect
H∗
ﬂight on Air France
H∗,andyou Lthough would need Lto connect
H∗
in
Amsterdam
H∗
L-insteadofyou’d needtoconnect
H∗
inAmsterdam
H∗
though L-L%.
2.8InterimSummary
Inthissection,wehaveintroducedapresentationstrategyforhighlightingthemost
compellingtrade-offsfortheuserthatstraightforwardlylendsitselftothedetermina-
tionofinformationstructure,whichthendrivesthechoiceofprosodicstructureduring
surface realization with CCG. We have also shown how phrase boundaries can be
determinedinaﬂexible,data-drivenfashion,whilerespectingtheconstraintsimposed
bythegrammar.Anexpertevaluationdemonstratedthattheapproachyieldsprosodic
structureswithsigniﬁcantlyhigheracceptabilitythanstrongn-grambaselineprosody
predictionmodels.Inthenexttwosections,weshowhowthegenerator-drivenprosody
canbeusedtoproduceperceptiblymorenaturalsyntheticspeech.
3.UnitSelectionSynthesiswithProsodicMarkup
Inthissectionwedescribetheunitselectionvoicesthatweemployedinourperception
experiment(Section4).Threevoicesintotalwereusedintheevaluation:GEN,ALL,
andAPML.Eachwasastate-of-the-artunitselectionvoiceusingtheFestivalMultisyn
speechsynthesisengine(Clark,Richmond,andKing2007).TheGENvoice,usedas
a baseline, is a general-purpose unit selection voice. The ALL voice is a voice built
using the same data as the GEN voice but with the addition of the data from the
FLIGHTSspeechcorpusdescribedinSection3.1.TheAPMLvoiceaugmentsthein-
domaindataintheALLvoicewithprosodicmarkup,whichisthenusedatrun-timeby
thesynthesizerinconjunctionwithmarked-upinputtoguidetheselectionofunits.
Toprovidein-domaindatafortheALLandAPMLvoices,weneededtorecord
asuitabledatasetfromtheoriginalspeakerusedfortheGENvoice.Wedescribethe
processofconstructingthisspeechcorpusforFLIGHTSnext,inSection3.1.Then,in
Section3.2,wedescribetheunitselectionvoicesindetail,alongwithhowthein-domain
datawasusedinbuildingthem.
3.1TheFLIGHTSSpeechCorpus
TheFLIGHTSspeechcorpuswasintendedtohaveaversionofeachwordthatneedsto
bespokenbythesystem,recordedinthecontextinwhichitwillbespoken.Thiscontext
canbethoughtofasathree-wordwindowcenteredaroundthegivenword,together
with the word’s target pitch accent and edge tone, if any. This would theoretically
providemorethansufﬁcientspeechdataforafulllimiteddomainvoice,andavoice
usingthisdatawouldbeguaranteedtohaveaunitsequenceforanysentencegenerated
byFLIGHTSwhereeachindividualunitisatightcontextmatchforthetargetunit.
Because the system was still limited in its generation capabilities at the time of
recordingthevoicedataforFLIGHTS,wedevelopedarecordingscriptbycombining
thegenerateddatathatwasavailablewithadditionalutterancesthatweanticipated
theﬁnishedsystemwouldgenerate.Todoso,weassembledasetofaround50utter-
ancetemplatesthatdescribeﬂightavailability—withslotstobeﬁlledbytimes,dates,
amounts,airlines,airports,cities,andﬂightdetails—towhichweaddedapproximately
twohundredindividualorsingle-slotutterances,whichaccountfortheintroductions,
182
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
questions,conﬁrmations,andotherresponsesthatthesystemmakes.Wethenmadeuse
ofanalgorithmdesignedbyBaker(2003)toiteratethroughtheﬁllercombinationsfor
eachutterancetemplatetoprovideasuitablerecordingscript.
22
Toillustrate,Example(4)showsanexampleutterancetemplate,alongwithtwo
utterancesintherecordingscriptcreatedfromthistemplate.Theexampledemonstrates
thathavingmultipleslotsinatemplatemakesitpossibletoreducethenumberofsen-
tencesthatneedtoberecordedtocoverallpossiblecombinationsofﬁllers.With(b)and
(c)recordedasillustratedtoﬁllthetemplatein(a),wewouldthenhavetherecorded
datatomixandmatchtheslotﬁllerswithtwodifferentvalueseachtosynthesizeeight
differentsentences.Itisalsooftenpossibletousetheﬁllersforaslotinoneutterance
astheﬁllersforaslotinanotherutterance,ifthephrasestructureissufﬁcientlysimilar.
AmorecomplexcaseisshowninExample(5),involvingadjacentslots.Inthiscase,it
isnotsufﬁcienttojustrecordoneexampleofeachpossibleﬁller,asthecontextaround
thatﬁllerischangedbythepresenceofotherslots;instead,combinationsofﬁllersmust
beconsidered(Baker2003).
(4) a. Itarrivesat〈TIME〉
H∗
L-H%andcostsjust〈AMOUNT〉
H∗
L-L%,butitrequires
aconnection
H∗
in〈CITY〉
H∗
L-L%.
b. Itarrivesatsix
H∗
p.m.
H∗
L-H%andcostsjustﬁfty
H∗
pounds
H∗
L-L%,butit
requiresaconnection
H∗
inParis
H∗
L-L%.
c. Itarrivesatnine
H∗
a.m.
H∗
L-H%andcostsjusteighty
H∗
pounds
H∗
L-L%,butit
requiresaconnection
H∗
inPisa
H∗
L-L%.
(5) a. Thereare〈NUM〉
H∗
〈MOD〉
H∗
ﬂightsL-L%from〈CITY〉
H∗
to〈CITY〉
H∗
todayL-L%.
b. Therearetwo
H∗
earlier
H∗
ﬂightsL-L%fromBordeaux
H∗
toAmsterdam
H∗
todayL-L%.
Therecordingscriptpresentedsimilarutterancesinblocks.Itmadeuseofsmall
capstoindicateintendedword-levelemphasisandpunctuationtopartiallyindicate
desiredphrasing,asthespeakerwasnotfamiliarwithToBIannotation.Theintended
dialoguecontextfortheutteranceswasdiscussedwithourspeaker,buttherecordings
didnotconsistofcompletedialogues,asusingcompletedialogueswouldhavebeentoo
timeconsuming.Therecordingtookplaceduringtwoafternoonsessionsandyielded
approximatelytwohoursofrecordedspeech.Theresultingspeechdatabaseconsisted
of1,237utterances,thebulkofwhichwasderivedfromtheutterancesthatdescribe
ﬂightavailability.
The recordings were automatically split into sentence-length utterances, each of
whichhadageneratedAPMLﬁlewiththepitchaccentsandedgetonespredictedfor
theutterance.Theprosodicstructureswerebasedonintuition,astherewasnocorpus
ofhuman–humandialoguessufﬁcientlysimilartowhatthesystemproducesthatcould
havebeenusedtoinformprosodicchoice.
TherecordingswerethenmanuallycheckedagainstthepredictedAPML,bylisten-
ing to the audio and looking at the f0 contours to see if the pitch accents and edge
tones matched the predicted ones. In the interest of consistency, changes were only
22 Thetargetprosodyforeachutterancetemplatewasbasedonthedevelopers’intuitions,withinputfrom
MarkSteedman.Asoneoftheanonymousreviewersobserved,analternativeapproachmighthavebeen
toconductanelicitationstudyinaWizard-of-Ozsetuptodeterminetargettunes.Thisstrategymayhave
yieldedmorenaturalprosodicvariation,butperhapsattheexpenseofemployinglessdistinctivetunes.
Forthisproject,itwasnotpracticaltotaketheextratimerequiredtoconductsuchanelicitationstudy.
183
ComputationalLinguistics Volume36,Number2
madetotheAPMLﬁleswhentherewereclearcasesofthespeakerdivergingfromthe
predictedphrasingoremphasis;nevertheless,thechangesdidinvolvemismatchesin
boththepresenceandthetypeoftheaccentsandboundaries.Asanexampleofthekind
ofchangesthatweremade,inExample(4),wehadpredictedthatthespeakerwould
useaL-H%boundary(acontinuationrise)priortothewordbut,howeversheinstead
consistentlyusedalowboundarytoneinthislocation.Consequently,wechangedall
theAPMLﬁlesforsentenceswiththispatterntoincludeaL-L%compoundedgetone
atthatpoint.FurtherdetailsofthisdatacleanupprocessaregiveninRocha(2004).
3.2TheSyntheticVoices
Ideally,wewouldliketobuildageneralpurposeunitselectionvoicewherewecan
fullyspecifytheintonationintermsofToBIaccentsandboundariesandthenhavethe
synthesizergeneratethisforus.This,however,iscurrentlynotafullyviableoptionfor
thereasonsdiscussedsubsequently,soinsteadwebuiltanumberofdifferentvoices
using the unit selection technique to investigate how working towards a voice with
fullintonationcontrolwouldfunction.Thereareanumberofwaysinwhichprosodic
generationforunitselectioncanbeapproached(Aylett2005;vanSantenetal.2005;
ClarkandKing2006),withmostsystemsdesignedtoproducebestgeneralprosody.
Thiscontrastswiththeframeworkchosenhere,whichisdesignedtomakemaximum
useofin-domaindata,inanattempttoproduceasystemthatrealizesprosodyaswell
asispossibleinthatdomain.Thiscanbeconsideredanupperboundforwhatmore
generalsystemscouldachieveunderoptimalconditions.
Theidealwaytouseintonationwithintheunitselectionframeworkrequiresthree
components: the database of speech, which must be fully annotated for intonation;
thepredictedintonationforatargetbeingsynthesized;andatargetcostthatensures
suitablecandidatesarefoundtomatchthetarget.Eachoftheserequirementsproveto
bedifﬁculttoachievesuccessfully.
Manuallylabelingalargespeechdatabaseaccuratelyandconsistentlywithintona-
tionlabelsisbothdifﬁcultandexpensive,whereasautomaticlabelingtechniquestend
toproducemediocreresultsatbest.Producingaccentlabelingfortheﬂightinformation
scriptissomewhateasierbecauseofthelimitednumberoftemplatesthatthescriptis
basedupon,andoncethetemplatesarelabeled,intonationforindividualsentencescan
bederived.TobuildageneralpurposeunitselectionvoicefortheFLIGHTSdomain,
wewouldwanttocombinetheFLIGHTSdatawithspeechdatadesignedtoprovide
moregeneralcoverage.Providingaccentlabelingfortheextra,non-FLIGHTS,datais
themainproblemhere.
Predicting intonation for a target utterance is generally a difﬁcult task, and can
onlyreallybedonewellwhenthedomainofthespeechsynthesisisconstrained.For
example,anumberofstatisticalmodelingtechniquesmaybeabletoprovideadequate
accentpredictionforasystemtoreadthenewsorforecasttheweather,becausethesen-
tencestructureisusuallylimitedtoasequenceofsimplestatements.Thetaskbecomes
difﬁcultwhenthesentencestructureismorevariable,forexampleindialoguewherea
numberofdifferentquestionformsmayexistalongwithcontrastivestatements,andso
forth.IntheFLIGHTSdomainwecanside-stepthepredictionproblembyprovidinga
speciﬁcationfortheintonationofasentenceaspartoflanguagegeneration.
Inonesense,deﬁningatarget-costcomponenttodirectthesearchtowardsﬁnding
suitable intonation is not difﬁcult, as a simple penalty for not matching the target
intonation sufﬁces. However, standard unit selection techniques only ever take into
184
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
accountlocaleffects,andnoprovisionismadetoensureasuitableglobalintonation
contour.
Theseproblems,andanumberoftechnicalissuesrelatingtotheuseofmarkup,
automaticsegmentalignment,andthevoicebuildingprocess,prohibitusfrombuilding
ageneralpurposeunitselectionvoicewherewecanspecifytherequiredintonation.
Oneofthequestionsthatthisstudyisattemptingtoansweriswhetheritisworthtrying
toresolvetheseissuestobuildsuchasysteminthefuture.Toaddressthisquestion
wepresentanumberofvoicesdesignedtoinvestigatetheissuesofproducingnatural
speechsynthesisintheFLIGHTSdomain.
3.2.1 The
GEN Voice.TheGENvoiceusesadatabaseofapproximately2,000sentences
ofreadnewspaperspeech.TheFestivalMultisynunitselectionengineselectsdiphone
unitsfromthedatabasebyminimizingacombinationofatargetcostandajoincost.
Thetargetcostscoresthelinguisticcontextofthediphoneintermsofstress,position
ofthediphoneinthecurrentword,syllableandphrase,phoneticcontext,andpartof
speech.Thejoincostscoresthecontinuitybetweenselectedunitsintermsofspectrum,
energy,andf0.Thereisnoexplicitmodelingofprosodyinthissystem.
TheGENvoicewasusedasabaseline.ThereisnothingspeciﬁctotheFLIGHTS
domainassociatedwithanypartofthisvoice,andthequalityoftheresultingsynthesis
iscomparablewithatypicalunitselectionspeechsynthesiser.
3.2.2 The
ALL voice. TheALLvoice wascreated byaugmenting theGENvoice with
thespeechdatarecordedaspartoftheFLIGHTSspeechcorpusdescribedherein.The
motivationhereistoattempttoprovideasystemwhichwouldhavethebestpossible
qualitythatageneralpurposeunitselectionsynthesisercouldhavewhenworkingin
thisdomain.Theadditionaldataincreasestheavailabilityofunitsintheexactcontexts
thatwouldberequiredbytheFLIGHTSsystem.Havingexamplesofairportandairline
names in the database, for example, increases the likelihood that when these words
aresynthesizedthereareappropriate,oftenconsecutive,unitsavailabletosynthesize
them.Naturalnessisimprovedbothbythebettercontextmatchandbytheneedfor
fewerjoinsinconstructingthesewordsandtheutterancestheyareusedin.
3.2.3 The
APML voice. The APML voice is different from the ALL voice in that it is
designedtotakeAPMLinputfromtheFLIGHTSsystemratherthantextinput.The
APML voice comprises the same speech data as the ALL voice, but also includes
the APML annotation for the FLIGHTS part of the corpora. The target cost for the
synthesizerisaugmentedwithaprosodiccomponentwhichpenalizesanymismatch
betweensuppliedAPMLinputandtheAPMLspeciﬁcationassociatedwithaparticular
unit. As the read newspaper component of this voice does not have accompanying
APMLmarkup,andbecausethevoiceisrequiredtoworkfortextinput(althoughthis
capability is not used here), the target cost penalizes (1) the synthesis of an APML-
speciﬁedtargetwithaunitthatdoesnothaveaccompanyingAPMLmarkup;(2)the
synthesisofatargetwithoutAPMLspeciﬁcationwithAPML-speciﬁedunits,and(3)
synthesiswherethereisanAPMLmismatchbetweenthetargetandcandidate.Itis
importantthattheseprosodicmismatchesareonlydiscouraged,ratherthanforbidden,
toallowthesystemtosynthesizeoutoftheoriginaldomainifrequired.
AsworkontheFLIGHTSsystemprogressed,wefoundnumerouscaseswherethe
speechcorpusfailedtoanticipateallthepossibleoutputsofthesystem.Forexample,
althoughweincludedanutterancetemplateforconveyingpriceinaconjoinedverb
phrase, as in Example (4), we did not include a template for conveying price in a
185
ComputationalLinguistics Volume36,Number2
singleindependentclause,asin It costs just ﬁfty pounds.Hadthesystembeenfurther
alonginitsdevelopmentwhenwerecordedthespeechdata,wecouldhavepursueda
strategyofselectingutterancestorecordfromgeneratedoutputs,inordertomaximize
some coverage criterion.
23
In either case, though, we consider it difﬁcult to develop
arecordingscriptthatwillcompletelycoverallthree-wordsequencesthatasystem
willevergenerate,especiallyiffurtherdevelopmentisconsidered.Forthisreason,we
believeittobeessentialtohaveastrategytohandlethecaseswheregenerationneeds
gobeyondtheoriginalplan.Theuseofanaugmentedgeneralpurposeunitselection
system—ratherthan,forexample,astrictlylimiteddomainsystem—meansthatthere
isnodifﬁcultyinsynthesizingextramaterialasneeded,althoughthereistheriskthatit
willnotsoundquiteasgoodasthatwhichiscloserincontexttotheoriginalin-domain
recordings.ForanAPMLvoicewheretheinputis“new”APML,iftherearenosuitable
unitswithintheAPML-annotatedsectionofthecorpus,unitswillbechosenfromthe
mainportionofthecorpus.Therewillbeapenaltyintermsoftargetcostfordoingso,
butthebestsequencewillstillbefound.
4.SynthesisEvaluation
Inthissection,wedescribeaperceptualexperimentwhichwascarriedouttodeter-
minewhethertheprosodicstructuresgeneratedbytheFLlGHTSsystemactuallyresult
in improved naturalness in speech synthesis. We also describe an evaluation of the
prosodyinthesynthesizedspeechthatmakesuseofanexpertannotator’sassessments
oftheacceptabilityoftheperceivedtunesforthegivencontext,andanevaluationthat
examinesobjectivedifferencesinthef0contoursofthethemephrasesinthesynthesized
speech.Thethreetypesofevaluation(subjectratings,expertannotation,andobjective
measures)allshowtheAPMLvoiceoutperformingtheALLvoice,bothonthecomplete
setoftestutterancesaswellasonthesubsetcontainingthemephrases.Additionally,the
threetypesofevaluationsupporteachotherinthatdifferencesinratingsofparticular
itemscorrespondtodifferencesinacceptabilityannotationsandtodifferencesinthe
objectivemeasures,aswillbeexplainedinthefollowing.
4.1PerceptionExperiment
4.1.1 Methodology. Our experimental hypothesis was that listeners would prefer the
APMLvoice,usedwithcontextuallyappropriateintonationmarkup,overtheALLand
GENcontrolvoices.Wefurtherhypothesizedthatthepreferencewouldbelargerfor
theutterancescontainingthemephrases,wheretheintonationismoremarkedthanit
isintheall-rhemeutterances.
Subjectswerepresentedwithmini-dialoguesconsistingofasummaryoftheuser’s
requestintextandthreeversionsofthesystem’sresponse,oneforeachofthethree
voices, as shown in Figure 12. System utterances were presented side-by-side, with
eachsystemturncomprisingtwotofourutterances,andeachvoicelabeledasA,B,
orC.Thelabelassignmentswerebalancedacrossthemini-dialoguessothateachvoice
appearedanequalnumberoftimeslabeledasA,B,orC,andthemini-dialogueswere
presentedinanindividuallyrandomizedorder.Subjectswereaskedtoassignratings
toeachversionofeachutteranceona1–7scale,with7correspondingto“completely
natural”and1correspondingto“completelyunnatural.”Ratingsweregatheredon-line
23 Forexample,indevelopingacustomvoicefortheCOMICsystem,weselectedfromgeneratedutterances
inordertomaximizebigramcoveragewithhighprioritynamedentities.
186
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
Figure12
Screenshotofwebexp2interfaceforgatheringlistenerratings.
usingwebexp2.
24
Subjectswereallowedtoplaythesoundﬁlesanynumberoftimesin
anyorder,butwererequiredtoassignratingstoalltheutterancesbeforeproceedingto
thenextscreen.Inassigningtheirratings,subjectswereinstructedtopayattentionto
thecontextgivenbythesummaryoftheuser’srequest,keepingthefollowingquestions
inmind:
a114
Doestheutterancemakeitclearhowwelltheﬂight(orﬂights)inquestion
meettheuser’sneeds?
a114
Arewordsemphasisedinawaythathighlightsthetrade-offsamongthe
differentoptions?
a114
Forthesecondandsubsequentutterances,isemphasisusedinawaythat
makessensegiventheprevioussystemutterances?
a114
Istheutteranceclearandeasytounderstand,orgarbledanddifﬁcultto
understand?
Twelvemini-dialogueswereusedasstimuli,comprising31utterancesintotal.The
dialogueswereconstructedsoastocontainarepresentativerangeofthemephrases,
witheachmini-dialoguecontainingoneutterancewithathemephrase.Anexample
dialogue,withtargettunesfortheAPMLvoice,appearsinFigure13;thesecondsystem
utterancecontainsathemephrase.Thecompletesetofstimuli,includingsoundﬁles,is
availableon-line
25
fromtheﬁrstauthor’swebpage.
FourteennativeEnglishspeakingsubjectsparticipatedinthestudy.Thesubjects
wereallfromtheUKorUSAandhadnoknownhearingdeﬁcits.Forparticipatingin
thestudy,subjectswereenteredintoaprizedrawing.
24 http://www.webexp.info/.
25 http://www.ling.ohio-state.edu/∼mwhite/flights-stimuli/.
187
ComputationalLinguistics Volume36,Number2
Figure13
Examplemini-dialogue(Dialogue03).
4.1.2 Results.AsshowninFigure14,theAPMLvoicereceivedhigherratingsthanthe
ALL voice, and both the APML and ALL voices scored much higher than the GEN
voice.Overall,theAPMLvoice’saverageratingsurpassedthatoftheALLvoiceby
0.77; its score of 5.83 was close to 6 on the rating scale, corresponding to “mostly
natural,”whiletheALLvoice’sscorewas5.06,justabove5onthescale,corresponding
to“somewhatnatural.”Thedifferencebetweenthetwovoiceswashighlysigniﬁcant
(pairedt-test,t
433
=10.20,p<0.001).Onthethemeutterances,thedifferencebetween
theAPMLandALLvoiceswasevenlarger,at0.91.WiththeAPMLvoice,therewas
nosigniﬁcantdifferencebetweentheaverageratingsofthethemeutterancesandthose
withoutthemephrases.Incontrast,theALLvoiceshowedatrendtowardsthetheme
utterances scoring worse than the remaining ones (t-test, 1-tailed, t
432
=−1.39, p =
0.08),withtheaverageofthethemeutterances0.19lowerthanthatoftheall-rheme
utterances. The GEN voice did considerably worse (0.48) on the theme utterances
(t-test,1-tailed,t
432
=−3.06,p=0.001).
4.2ExpertProsodyEvaluation
4.2.1 Methodology. To more directly examine whether the APML voice yielded more
contextuallyappropriateprosodythantheALLvoice,weaskedanexpertToBIanno-
tator, who was unfamiliar with the experimental hypotheses under investigation, to
annotatetheperceivedtunesinthesynthesizedstimuliforthesetwovoices.Incases
ofuncertainty,multipleannotationsweregiven.ThestimulifromtheALLvoicewere
alwayspresentedﬁrst,sothatlisteningtothetunefromtheAPMLitemwouldnotbias
ourannotatoragainstthecorrespondingALLitem.Becausethereisnotnecessarilya
uniquetunethatisacceptableforthecontext,wealsoaskedourannotatortoindicate
theclosestacceptabletunebyindicatingwhichaccentsandboundarieswouldneedto
Figure14
Meanratingsforeachvoice.Themeutterancesarethesubsetcontainingathemephrase.(Error
barsshowstandarderrors.)
188
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
Figure15
Exampleannotationandf0showingtunecorrections(Item05-2).
changeinordertoyieldatuneappropriateforthecontext,inmuchthesamefashion
asinourprosodypredictionevaluation(Section2.7).
26
Wethencountedthenumberof
accentorboundarycorrectionsatdifferentlevelsofspeciﬁcity.
AnexampleannotationwithtunecorrectionsappearsinFigure15.Thisutterance
isthesecondoneinDialogue05,wheretheuserpreferstoﬂyBMI;accordingly,the
utterancecontainsthethemephrase(withtargettune) the only BMI
L+H∗
ﬂight L-H%.
WiththeAPMLversionoftheutterance,theannotatorperceivedaL+H*accentonBMI
and a L-H% boundary on de-accented ﬂight, as desired. Additionally, the annotator
perceived aH*accent on only,whichwasnotpartofthetargettune.Thetunewas
neverthelessconsideredcompletelyacceptable,astheEditstierisidenticaltotheTones
tier.Bycontrast,withtheALLversionoftheutterance, BMI waslessprominentthan
only and ﬂight, and accordingly it received no pitch accent, whereas only and ﬂight
receivedL+H*andH*accents,respectively;inaddition,aH-boundarywasannotated
on only,andaboundarythatwasuncertainbetweenL-andL-L%wasannotatedon
26 Thetargettuneswerenotpresentedtogetherwiththesynthesizedstimuli,toavoidinﬂuencingthetunes
perceivedbyourexpert.
189
ComputationalLinguistics Volume36,Number2
Table5
Numberofprosodiccorrectionsofdifferenttypesinallutterancesandthemeutterancesforthe
twovoices.Itemsinboldaresigniﬁcantlydifferentatp=0.05orlessbyaone-tailedFisher’s
ExactTest.
All Utts ThemeUtts
APML ALL APML ALL
Totalcorrections 22 49 11 26
Accents 10 33 4 16
Presence 6 20 2 8
L+H* 4 13 2 8
Boundaries 12 16 7 10
Presence 7 6 3 4
ﬂight.Thistuneforthethemephrasewasnotconsideredacceptableforthecontext:As
theEditstiershows,thelackofanaccenton BMI,andthepresenceofanaccenton
ﬂight,wascorrectedbyourannotator,aswastheH-minorphraseboundaryon only.
Thischoicemakessenseforthecontext,asBMIiswhatdistinguishesthisoptionfrom
theonesuggestedintheﬁrstutterance,whileﬂightisgiveninformationatthispointin
thedialogue.Indeed,itisdifﬁculttocomeupwithaninterpretationofonly
L+H∗
BMI,
withnoaccentonBMI,thoughthistunemightmakesenseifthequestionofwhether
theﬂightwascode-sharedwithanotherairlinewasasalientoneinthecontext.
4.2.2Results.TheresultsoftheexpertprosodyevaluationappearinTable5.Acrossall31
utterances,therewere49accentorboundarycorrectionsfortheALLvoice,versusjust
22fortheAPMLvoice,outof688totalaccentandboundarychoices,ahighlysigniﬁcant
difference(p<0.001,Fisher’sExactTest,1-tailed).Withthe12themeutterances,there
were26correctionsfortheALLvoice,versus11fortheAPMLvoice,outof220total
choices (p = 0.008, FET). Looking at the pitch accents, the difference in the number
ofaccentcorrectionswassigniﬁcantineachcase(p < 0.001,FET,andp=0.004,FET,
respectively),aswasthenumberofcorrectionsinvolvingthepresenceorabsenceofa
pitchaccent(p=0.004,FET,andp=0.05,FET,respectively)andthenumberinvolving
L+H*accents(p=0.02,FET,andp=0.05,FET,respectively).
27
Lookingatthebound-
aries,wemayobservethatwiththeALLvoice,themajorityofcorrectionswerewith
accents,whereaswiththeAPMLvoice,themajoritywerewithboundaries.Although
theAPMLvoicehadfewerboundarycorrections,thedifferencewasnotsigniﬁcant.
AsnotedinthediscussionofFigure15,theperceivedtuneswiththeAPMLvoice
did not always exactly match the target tunes, sometimes in ways that our expert
annotatorconsideredacceptable.Morecommonly,however,mismatchesbetweenthe
perceivedandtargettuneswerenotjudgedtobeacceptable.Infact,ofthe22correc-
tions for the APML voice, 19 would have been acceptable had the target tune been
successfullyachieved;thatis,19ofthe22correctionschangedaperceivedaccentor
boundarytotheoneinthetargettune.In12ofthese19cases,ourannotatorperceived
anaccentorboundarywherethetargettunehadnone.Therewerealsoﬁveothercases
27 Acorrectionwascountedasinvolvingthepresenceorabsenceofapitchaccentifthewordwas
perceivedashavinganaccentwhenitwasdeemedthatitshouldhavenone,orwasperceivedashaving
noaccentwhenitwasdeemedthatitshouldhaveone;L+H*accentandboundarypresencecorrections
werecountedinthesamefashion.
190
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
Figure16
Meanratingsofitemswith0–4prosodiccorrectionsfortheAPMLandALLvoicescombined.
(Errorbarsshowstandarderrors.)
wherethetargetwasaL-L%boundary,butarisingboundarywasperceivedinstead.
Theremainingcasesinvolvedaccenttypemismatches.
Toexaminetherelationshipbetweenthenumberofcorrectionsindicatedbyour
expertToBIannotatorandtheratingsintheperceptionexperiment,wegroupedthe
APML and ALL utterances by the number of corrections and calculated the mean
ratings for each group. The results appear in Figure 16, which shows the expected
relationship between mean ratings and the number of corrections, with ratings go-
ing down as the number of corrections go up. The pattern is less clear with two to
fourcorrections,wheretherearefewertokens.One-tailedt-testsshowthatutterances
withzerocorrectionswereratedsigniﬁcantlyhigherthanutteranceswithonetofour
corrections(t
39
=2.96,t
35
=5.14,t
30
=3.03,t
28
=5.35,respectively;p < 0.01ineach
case);utteranceswithonecorrectionwerealsosigniﬁcantlyhigherthanthosewithfour
corrections(t
17
=2.30,p=0.02).
We also examined the relationship between the listener ratings and the number
of errors noted by our expert annotator through a multiple regression analysis. The
regressorswerethenumberofaccentorboundarypresenceerrorsandthenumberof
remainingaccentorboundaryerrors,involvingonlyadifferenceintype.Theregression
equationwas Rating=5.85−0.41×PresenceErrors−0.28×TypeOnlyErrors,whichac-
countedfor32%ofthevarianceandwashighlysigniﬁcant(F
2,59
=15.3,p<0.001).As
expected,ratingswerenegativelyrelatedtoaccentandboundaryerrors,withpresence
errorsshowingagreaterimpactthantype-onlyerrors.Boththeeffectofpresenceerrors
(t
59
=−4.34,p < 0.001)andtheeffectoftype-onlyerrors(t
59
=−2.65,p=0.01)were
signiﬁcant.Becausebothkindsoferrorshadasigniﬁcantimpactonratings,theresults
suggestthatitisworthwhiletomakeuseofﬁne-grainedToBIcategorieswherefeasible,
aswediscussfurtherinSection6.
4.3ObjectiveMeasures
4.3.1 Methodology.Inadditiontotheexpertprosodyevaluation,wealsomeasuredthe
f0valuesanddurationsoftheadjectivesandnounsinthethemephrases,toseewhether
theydifferedsigniﬁcantlybetweentheAPMLandALLvoices.Wealsomeasuredthe
dropinf0betweentheadjective,whichshouldreceiveacontrastivepitchaccent,and
thenoun,whichshouldbedeaccented.Notethatthef0dropisapotentiallymoreﬁne-
grainedmeasurethanpitchaccentcorrections,becausetheaccentscouldbeconsidered
acceptableeventhoughthetuneislessdistinctinsomecases.
AnexampleinwhichthisoccursappearsinFigure17,whichshowsthesecond
utteranceofDialogue03,seenearlierinFigure13,fortheAPMLandALLvoices.Here,
191
ComputationalLinguistics Volume36,Number2
Figure17
Exampleannotationandf0showingdifferenceinf0dropinthethemephrases(Item03-2).
theAPMLversionwasannotatedwiththetargetaccents,namelyL+H*for Lufthansa
andnoneforﬂight,whereastheALLversionwasannotatedwithaL+H*!H*pattern,
whichwasalsoconsideredacceptable.However,withtheAPMLversionthepitchdrops
fromanf0valueof293Hzto177Hz,whereaswiththeALLversionthepitchonlydrops
from260Hzto209Hz.Asaresult,thethemetuneismuchlessdistinctiveintheALL
version,inawaythatcouldimpacttheeasewithwhichthethemephrase’sdiscourse
functionisidentiﬁed.
4.3.2 Results. Figure18showsthemeanf0valuesacrossall12themephrasesforthe
adjective,whichshouldreceivecontrastiveemphasis,andtheheadnoun,whichshould
bereduced.Astheﬁgureshows,thepatternseeninFigure17wasborneoutonthe
wholeinthestimuliwiththemephrases.Inparticular,thethemephrasesfortheAPML
voicehadanf0dropof90Hzonaveragefromtheadjectivetothenoun,whereasthe
ALLvoiceonlyhadanf0dropof50Hz,asigniﬁcantdifference(t-test,1-tailed,t
22
=
2.60,p=0.008).Thedurationsoftheadjectivesandnouns,however,didnotshowa
signiﬁcantdifference.Inadditiontothef0drop,asigniﬁcantdifferencewasalsofound
withthef0maxontheadjectiveandthef0minonthenoun(t-tests,1-tailed,t
28
=1.70,
p=0.05andt
22
=−1.86,p=0.04,respectively).Finally,arelativelyhighcorrelation(r=
0.78)wasalsofoundbetweenthedifferenceinf0dropandthedifferenceintheratings
ofthethemeutterancesforthetwovoices(t
10
=3.94,p=0.001),suggestingthattheless
192
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
distinctivethemetunesproducedbytheALLvoicewereperceivedaslessnaturalthan
theonesproducedbytheAPMLvoice.
4.4Discussion
Theperceptionexperimentconﬁrmedourhypothesisthatlistenerswouldpreferthe
APML voice, used with contextually appropriate intonation markup, over the ALL
andGENcontrolvoices.Bothonthecompletesetofutterances,aswellasthesubset
containingthemephrases,theAPMLvoicewasratedsubstantiallyhigheronaverage
thantheALLvoice,andmuchhigherthantheGENvoice.NotethattheALLvoicewas
alsoratedmuchhigheronaveragethantheGENvoice—whichisnotsurprising,given
thatithasaccesstothesamelimiteddomaindataastheAPMLvoice—showingthat
theALLvoiceservesasatoughbaselinetobeat.Theexpertprosodyevaluationandthe
objectivemeasuresalsoconﬁrmedthesuperiorityoftheAPMLvoice,inparticularon
thethemeutterances.
WealsofoundevidenceinfavorofourhypothesisthatthepreferencefortheAPML
voicewouldbelargerfortheutterancescontainingthemephrases—wheretheintona-
tionismoremarkedthanitisintheall-rhemeutterances—asthedifferencebetween
themeanratingsoftheAPMLandALLvoiceswaslargeronthethemeutterancesthan
ontheremainingones.Additionally,withtheAPMLvoice,therewasnosigniﬁcant
differencebetweenthemeanratingsofthethemeutterancesandthosewithouttheme
phrases,whereastheALLvoiceshowedatrendtowardsthethemeutterancesscoring
worsethantheall-rhemeones,andtheGENvoiceclearlydidconsiderablyworseon
thethemeutterances.OnesmallsurprisewasthatwiththeALLvoice,thedifference
between the mean ratings of the theme and all-rheme utterances did not reach the
standardlevelofstatisticalsigniﬁcance.Ofcourse,itcouldbethatwithalargersample
size,amoresigniﬁcantdifferencewouldbefound.However,itisundoubtedlythecase
thattheALLvoicedroppedofflessonthethemeutterancesthandidtheGENvoice,
andthereasonisalmostcertainlythatthelimiteddomaindatahasgoodcoverageof
thethemephrases,andthustheALLvoiceoftendoesreasonablywellonthetheme
utterancesevenwithoutexplicitprosodiccontrol.Whatisperhapsmoreremarkable
isthattheALLvoicedidnotdobetterontheall-rhemeutterances,ascanbeseenin
thenumberofexpertcorrectionslistedinTable5foralltheutterances,whichgowell
beyondthoseinthethemeutterancesubset.ThattheALLvoicehadmorethandouble
Figure18
Meanf0valuesforemphasizedandreducedwordsinthemephrasesfortheAPMLandALL
voices.(Errorbarsshowstandarderrors.)
193
ComputationalLinguistics Volume36,Number2
thenumberofcorrectionsastheAPMLvoiceonboththecompletesetofutterances
andthesubsetcontainingthemephrasesshowsthattheprosodicspeciﬁcationswere
importantthroughout.
Finally,wemayobservethatalthoughtheAPMLvoicehadfewerboundarycorrec-
tionsthandidtheALLvoice,thedifferencedidnotreachsigniﬁcance,suggestingthat
thereisroomforimprovementinhowboundariesarehandledintheAPMLvoice.In
particular,thisresultsuggeststhatedgetones,andintermediatephraseboundariesin
particular,shouldaffecttheselectionofunitsnon-locally,astheoreticallytheireffecton
thepitchcontourspreadsbacktothelastpitchaccent.Infact,itmaywellbethatbecause
the speech synthesis system only models prosodic effects locally, essentially at the
syllablelevel,anddoesnottakeutterance-levelstructuressuchastuneintoaccount,a
ceilinghasbeenreachedforbothaccentandphrasingperformance.Representingglobal
prosodicstructurestoensureprosodiccoherencewillbeoneofthemajorchallengesfor
futuregenerationsofspeechsynthesissystems.
5.RelatedWork
TheFLIGHTSsystemcombinesandextendsearlierapproachestouser-tailoredgenera-
tioninspokendialogue.AdistinguishingfeatureofFLIGHTSisthatitadaptsitsoutput
accordingtouserpreferencesatalllevelsofthegenerationprocess,fromtheselection
ofcontenttolinguisticrealizationandtheprosodytargetedinspeechsynthesis.
ThemostsimilarsystemtooursisMATCH(Walkeretal.2004),whichemployssim-
plercontentplanningstrategiesanddoesnotexplicitlypointoutthetrade-offsamong
options.MATCHalsousessimpletemplatesforrealization,anddoesnotattemptto
controlintonation.CareniniandMoore’s(2000,2006)systemisalsocloselyrelated,but
itdoesnotmakecomparisons,andgeneratestextratherthanspeech.Carberryetal.’s
(1999) system likewise employs additive decision models in recommending courses,
thoughtheirfocusisondynamicallyacquiringamodelofthestudent’spreferences,and
thesystemislimitedtorecommendingasingleoptionconsideredbetterthantheuser’s
currentone.Inaddition,thesystemonlyaddressestheproblemofselectingpositive
attributestojustifytherecommendation,anddoesnotplanandprosodicallyrealizethe
positiveandnegativeattributesofmultiplesuggestedoptions.
Thesesystemsallemployausermodeltoselectasmallsetofgoodoptions,andto
identifytheattributesthatjustifytheirdesirability,inordertopresentasummary,com-
parison,orrecommendationtotheuser.Evaluationshowedthattailoringrecommen-
dationsandcomparisonstotheuserincreasedargumenteffectivenessandimproved
usersatisfaction(Walkeretal.2004).Thus,theuser-model(UM-)basedapproachisan
appropriatestrategyforspokendialoguesystemswhenthereareasmallnumberof
optionstopresent,eitherbecausethenumberofoptionsislimitedorbecauseuserscan
supplysufﬁcientconstraintstowinnowdownalargesetbeforequeryingthedatabase
ofoptions.
Otherresearchershavearguedthatitisimportanttoallowuserstobrowsethedata
foranumberofreasons:(1)iftherearemanyoptionsthatshareattributevalues,they
willbeverycloseinscorewhenrankedusingtheUM-basedapproach;(2)usersmay
notbeabletoprovideconstraintsuntiltheyhearmoreinformationaboutthespaceof
options;and(3)theUM-basedapproachdoesnotgiveusersanoverviewoftheoption
space,andthismayreducetheirconﬁdencethattheyhavebeentoldaboutthebest
option(s)(DembergandMoore2006).
Polifroni,Chung,andSeneff(2003)proposeda“summarizeandreﬁne”(SR)ap-
proach,inwhichthesystemstructuresalargenumberofoptionsintoasmallnumber
194
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
ofclustersthatshareattributes.Thesystemthensummarizestheclustersbasedontheir
attributes,implicitlypromptingtheusertoprovideadditionalconstraints.Thesystem
producessummariessuchas
I have found983 restaurants.Most ofthemarelocated inBostonandCambridge.Thereare 32
choices for cuisine.I alsohave informationabout price range.
whichhelptheusergetanoverviewoftheoptionspace.Chung(2004)extendedthis
approach by proposing a constraint relaxation strategy for coping with queries that
aretoorestrictive tobesatisﬁed byanyoption. Pon-Barry, Weng, andVarges (2006)
foundthatfewerdialogueturnswerenecessarywhenthesystemproactivelysuggested
reﬁnementsandrelaxations.
However, as argued in Demberg and Moore (2006), there are several limitations
totheSRapproach.First,manyturnsmayberequiredduringthereﬁnementprocess.
Second,ifthereisnooptimalsolution,explorationoftrade-offsisdifﬁcult.Finally,the
attributesonwhichthedatahasbeenclusteredmaybeirrelevantforthespeciﬁcuser.
DembergandMoore(2006)subsequentlydevelopedtheuser-model-basedsummarize
andreﬁneapproach(UMSR)tocombinethebeneﬁtsoftheUMandSRapproaches,by
integratingusermodelingwithautomatedclustering.Whentherearemorethanasmall
numberofrelevantoptions,theUMSRapproachbuildsacluster-basedtreestructure
whichorderstheoptionstoallowforstepwisereﬁnement.Theeffectivenessofthetree
structure,whichdirectsthedialogueﬂow,isoptimizedbytakingtheuser’spreferences
into account. Trade-offs between alternative options are presented explicitly to give
theuserabetteroverviewoftheoptionspaceandleadtheusertoamoreinformed
choice. To give the user conﬁdence that they are being presented with all relevant
options,abriefaccountoftheremaining(irrelevant)optionsisalsoprovided.Resultsof
alaboratoryexperimentcomparingtheSRandUMSRapproachesdemonstratedthat(1)
participantspreferredUMSR,(2)UMSRpresentationsareaseasytounderstandasthose
ofSR,(3)UMSRincreasesoverallusersatisfaction,(4)UMSRsigniﬁcantlyimprovesthe
user’soverviewoftheavailableoptions,and(5)UMSRincreasesusers’conﬁdencein
havingheardaboutallrelevantoptions.AlthoughtheUMSRapproachhasnotbeen
implementedintheFLIGHTSsystem,itcouldbeusedwhentherearealargenumber
ofavailableoptionstowinnowthemdowntoahandfulofrelevantones,whichwould
thenbecomparedfollowingtheapproachdescribedinthisarticle.
Asregardsourworkonintonation,asstatedintheintroduction,Prevost’s(1995)
generatorhasdirectlyinformedourapproachtoinformationstructureandprosody;his
systemdoesnotmakeuseofquantitativeusermodelsthough,andonlydescribessingle
options.Theune(2002)likewisefollowsPrevost’sapproachinhersystem,reﬁningthe
waycontrastisdeterminedinassigningpitchaccents.Theuneetal.(2001)showthata
systememployingsyntactictemplatesandarule-basedprosodyassignmentalgorithm
leadstomorenaturalsynthesis(ofDutch);unlikeFLIGHTSthough,theirD2Ssystem
doesnotemployauserpreferencemodeloranotionofthemephrase,anddoesnot
distinguishdifferenttypesofpitchaccents.AlsocloselyrelatedisKruijff-Korbayov´a
etal.’s(2003)information-statebaseddialoguesystem,inwhichtheauthorsexplorea
similarapproachtousinginformationstructureacrossdialogueturns;however,their
systemdoesnotmakeuseofausermodel,andemploystemplate-basedrealization
with much simpler sentence structures. Kruijff-Korbayov´a et al. likewise present an
evaluation indicating that the contextual appropriateness of spoken output (in Ger-
man)improveswhenintonationisassignedonthebasisofinformationstructure.In
comparisonwithourevaluation,theirsexaminesimprovementsoverageneralpurpose
195
ComputationalLinguistics Volume36,Number2
text-to-speechvoicewithdefaultintonation,ratherthanalimiteddomainvoice,which
providesamuchhigherbaselineintermsofthenaturalnessoftheresultingintonation.
Less closely related is most work on machine-learning approaches to prosody
prediction in text-to-speech (TTS) systems (Hirschberg 1993; Hirschberg and Prieto
1996; Taylor and Black 1998; Dusterhoff, Black, and Taylor 1999; Brenier et al. 2006)
andconcept-to-speech(CTS)systems(Hitzemanetal.1998,1999;Pan,McKeown,and
Hirschberg2002).Theseapproacheshavetypicallyaimedtodevelopgenericmodels
of prosody prediction, by training classiﬁers for accents and boundaries that make
useofaconsiderablevarietyoffeatures.Forexample,inpredictingaccentplacement
(butnottype),Hitzemanetal.’sCTSsystemmakesuseofrhetoricalrelationssuchas
list and contrast, along with the reference type of NPs and whether they represent
ﬁrstmentionsofanentityinthediscourse.InPanetal.’smorecomprehensivestudy,
theirsystempredictsaccentplacement(butnottype),breakindices,andedgetones
basedonfeaturesextractedfromtheSURGErealizer(Elhadad1993),deepsemantic
anddiscoursefeatures,includingsemantictype,semanticabnormalityandgiven/new
status,andsurfacefeatures,suchaspartofspeechandwordinformativeness.However,
neither of these CTS approaches makes use of the theme/rheme distinction, or the
notion of kontrast that stems from Rooth’s (1992) work on alternative sets, both of
whicharecrucialtoSteedman’s(2000a)theoryofhowinformationstructureconstrains
prosodicchoices.Morerecently,Brenieretal.haveshownthattheratioofaccentedto
unaccentedtokensofawordinspontaneousspeechisasurprisinglyeffectivefeaturein
predictingpitchaccents;theyalsoarguedthatusinginformationstatusandcontrastis
unlikelytoimproveuponprominencepredictionbasedonlyonsurfacefeatures,since
thesemanuallylabeledfeaturesdidnotyieldsubstantialimprovementsintheirdecision
treemodels.Again,however,theirapproachdoesnotmakeuseofthetheme/rheme
distinction,anddoesnotattempttopredictpitchaccenttypeoredgetones;inaddition,
theyreportfrequenterrorsonauxiliariesandnegatives(e.g.,no),whichwehavefound
tobeimportantforhighlightingtrade-offsprosodically.
Incontrasttotheseapproaches,wehaveemphasizedthegenerationandsynthesis
of sharply distinctive theme and rheme tunes in the limited domain of a dialogue
system, using a hybrid rule-based and data-driven approach. In particular, whereas
Hitzemanetal.(1998,1999)andPan,McKeown,andHirschberg(2002)makeuseof
individual classiﬁers for prosodic realization decisions—with no means of tying the
decisionsoftheseclassiﬁerstogether—ourapproachinsteadusesrulesandconstraints
inthegrammartospecifyaspaceofpossiblerealizations,throughwhichtherealizer
searchestoﬁndasequenceofwords,pitchaccents,andedgetonesthatmaximizesthe
probabilityassignedbyann-grammodelforthedomain.
Inanapproachthatismoresimilarinspirittoours,BulykoandOstendorf(2002)
likewiseaimtoreproducedistinctiveintonationalpatternsinalimiteddomain.How-
ever,unlikeourapproach,theirsmakesuseofsimpletemplatesforgeneratingpara-
phrases,astheirfocusisonhowdeferringtheﬁnalchoiceofwordingandprosodic
realizationtotheirsynthesizerenablesthemtoachievemorenaturalsoundingsynthetic
speech. Following on the work described in this article, Nakatsu and White (2006)
presentadiscriminativeapproachtorealizationrankingbasedonpredictedsynthesis
qualitythatisdirectlycompatiblewiththeFLIGHTSsystem.
Turningtooursynthesisevaluation,wenotethatdebateoverthestandardizationof
speechsynthesisevaluationcontinues,withtheBlizzardChallenge(BlackandTokuda
2005;FraserandKing2007)provingtobeausefulforumfordiscussingandperforming
evaluationacrossdifferentsynthesisplatforms.Mayo,Clark,andKing(2005)havepro-
posedtoevaluatespeechsynthesisevaluationfromaperceptualviewpointtodiscover
196
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
exactly what subjects pay attention to, in order to ensure that evaluation actually is
evaluatingwhatwethinkitis.Theauthorsfoundthroughtheuseofmultidimensional
scaling(MDS)techniquesthat,whenaskedtomakejudgmentsonthenaturalnessof
syntheticspeech,subjectsmadejudgmentsrelatingtoanumberofdifferentdimensions,
includingbothsegmentalqualityandprosody.Subsequently,Clarketal.(2007)found
correlationsbetweenresultsinMDSspacesandstandardmeanopinionscore(MOS)
tests,butastheMOStestsdidnotcorrespondtosingledimensionsintheMDSspace,
they suggested that it may be possible to design more informative evaluations by
asking subjects to speciﬁcally rate each factor of interest (e.g., prosody), where each
factorrelatestoonedimensionintheMDSspace.However,asnospeciﬁcmethodis
suggestedtoguaranteereliableprosodicjudgmentsfromnaivelisteners,wehaveleft
thisquestionforfutureresearch,optinginsteadtoaugmentthelistenerratingsgathered
inourperceptionexperimentwithanexpertprosodyevaluationandanf0analysisof
thethemephrases.
6.ConclusionsandDiscussion
Inthisarticle,wehavedescribedanapproachtopresentinguser-tailoredinformation
inspokendialogueswhichfortheﬁrsttimebringstogethermulti-attributedecision
models,strategiccontentplanning,surfacerealizationwhichincorporatesprosodicfea-
tures,andunitselectionsynthesisthattakestheresultingprosodicmarkupintoaccount.
Basedontheusermodel,thesystemselectsthemostimportantsubsetoftheavailable
options to mention and the attributes that are most relevant to choosing between
them. To convey these trade-offs, the system employs a novel presentation strategy
whichmakesitstraightforwardtodetermineinformationstructureandthecontentsof
referringexpressions.DuringsurfacerealizationwithOpenCCG,theprosodicstructure
isderivedfromtheinformationstructureinawaythatallowsphraseboundariestobe
determinedinaﬂexible,data-drivenfashion,andwithsigniﬁcantlyhigheracceptability
thanbaselineprosodypredictionmodelsinanexpertevaluation.Wehypothesizethat
theresultingdescriptionsarebothmemorableandeasyforuserstounderstand.Asa
steptowardsverifyingthishypothesis,wehavepresentedanexperimentwhichshows
that listeners perceive a unit selection voice that makes use of the prosodic markup
as signiﬁcantly more natural than either of two baseline synthetic voices. Through
an expert evaluation and f0 analysis, we have also conﬁrmed the superiority of the
generator-drivenintonationanditscontributiontolisteners’ratings.Infuturework,we
intendtoexaminetheimpactofourgenerationandsynthesismethodsonmemorability
orothertask-orientedmeasures.
Thepresentstudyprovidesevidencethatitisworthwhiletoinvestigatemethods
ofdevelopinggeneralpurposesynthesizersthatacceptprosodicspeciﬁcationsintheir
input.Themainreasonthatwedidnotusesuchasynthesizerinourevaluationisthat
inordertobuildageneralpurposeFestivalAPMLvoice,suitableAPMLmarkupwould
berequiredforthe2,000–3,000utterancesthatmakeupthecoreunitselectiondatabase.
AstheseutterancesareoutsideoftheFLIGHTSdomain(andthusnotgeneratedby
theNLGsystem),itwouldnotbepossiblewithcurrenttechnologytoprovideaccurate
APMLmarkupfortheseutterances.Giventhedifﬁcultyofautomaticallyannotating
general texts with APML, it may be worth considering a simpliﬁed version of the
markup for the database. For example, a system which marks the location of pri-
mary phrasal stress, other pitch accents, and a simple categorization of overall tune
(wh-question, yes/no-question, statement, etc.) could be used to annotate the speech
database.Thiscouldbeachievedwithanaccentdetectorandaverysimpleparserto
197
ComputationalLinguistics Volume36,Number2
determinetunetype.TheAPMLspeciﬁcationontheinputcouldeasilybemappedto
informationequivalenttothedatabaseannotationbysimplerules.Reasonablequality
synthesiscouldthenbeachievedwithoutthedatabasebeingfullyparsedandanno-
tated.Additionally,ifthereisaportionofin-domaindatainthedatabasewherefull
annotationisavailable,itcouldbeuseddirectlywhenthoseunitsaresearched.
Aninterestingunresolvedquestionistheextenttowhichmoregenericprosody
predictionmodels,alongthelinesofHitzemanetal.(1999)andPan,McKeown,and
Hirschberg(2002)—whichmakenouseofsuchinformationstructuralnotionsastheme,
rheme,andkontrast(cf.Section2.5)—couldbetrainedtoproducetunesasdistinctive
asthosewehavetargeted.Wesuspectthatsuchmodelswouldhavetroubledoingso,
givendatasparsityissuesandthefactthatmachine-learnedclassiﬁcationmodelstend
todiscovergeneraltrends,ratherthanaimingtoreproduceaspectsofspeciﬁcexamples,
whichmaycontainimportantbutrareevents.Atthesametime,itremainsforusto
investigatewhetherourhybridrule-basedanddata-drivenapproachcanbegeneralized
tobeasﬂexibleandwidelyapplicableasthesemachine-learnedmodels,whileretaining
itsabilitytoexpresscontrastsintelligibly.Insodoing,weexpectinformationstructural
constraintsinthegrammartocontinuetoplayanimportantrole.
Acknowledgments
WethankRachelBaker,SteveConway,Mary
EllenFoster,KallirroiGeorgila,Oliver
Lemon,ColinMatheson,NeideFranca
Rocha,andMarkSteedmanfortheir
contributionstotheFLIGHTSsystem;Eric
Fosler-LussierandCraigeRobertsforhelpful
discussion;andJulieMcGoryforproviding
theexpertToBIannotations.Wealsothank
theanonymousreviewersforhelpful
commentsandsuggestions.Thisworkwas
supportedinpartbyEPSRCgrant
GR/R02450/01andanArts&Humanities
InnovationgrantfromTheOhioState
University.
References
Aylett,Matthew.2005.Mergingdatadriven
andrulebasedprosodicmodelsforunit
selectionTTS.In5thISCASpeechSynthesis
Workshop,pages55–59,Pittsburg,PA.
Baker,RachelElizabeth.2003.Usingunit
selectiontosynthesisecontextually
appropriateintonationinlimiteddomain
synthesis.Master’sthesis,Departmentof
Linguistics,UniversityofEdinburgh.
Baldridge,JasonandGeert-JanKruijff.2002.
CouplingCCGandHybridLogic
DependencySemantics.InProceedingsof
40th AnnualMeetingof theAssociation for
Computational Linguistics,pages319–326,
Philadelphia,PA.
Bangalore,SrinivasandOwenRambow.
2000.Exploitingaprobabilistichierarchical
modelforgeneration.InProceedingsof
COLING-00,pages42–48,Saarbrucken,
Germany.
Bilmes,JeffandKatrinKirchhoff.2003.
Factoredlanguagemodelsandgeneral
parallelizedbackoff.InProceedingsof
HLT-03,pages4–6,Edmonton,Canada.
Black,AlanW.andKeiichiTokuda.2005.
Theblizzardchallenge—2005:Evaluating
corpus-basedspeechsynthesison
commondatasets.InInterspeech2005,
pages77–80,Lisbon.
Blackburn,Patrick.2000.Representation,
reasoning,andrelationalstructures:A
hybridlogicmanifesto.Logic Journal of the
IGPL,8(3):339–625.
Bos,Johan,EwanKlein,OliverLemon,
andTetsushiOka.2003.DIPPER:
Descriptionandformalisationofan
information-stateupdatedialoguesystem
architecture.In4thSIGdial Workshop on
Discourseand Dialogue,pages115–124,
Sapporo.
Brenier,Jason,AniNenkova,Anubha
Kothari,LauraWhitton,DavidBeaver,
andDanJurafsky.2006.The(non)utility
oflinguisticfeaturesforpredicting
prominenceinspontaneousspeech.
IEEE/ACL2006 Workshop onSpoken
Language Technology,pages54–57,Palm
Beach,Aruba.
Bulyko,IvanandMariOstendorf.2002.
Efﬁcientintegratedresponsegeneration
frommultipletargetsusingweightedﬁnite
statetransducers.ComputerSpeechand
Language,16:533–550.
Calhoun,Sasha,MalvinaNissim,Mark
Steedman,andJasonBrenier.2005.A
frameworkforannotatinginformation
structureindiscourse.Proceedingsofthe
ACL-05Workshop on FrontiersinCorpus
198
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
AnnotationII:Pieinthe Sky,pages45–52,
AnnArbor,Michigan.
Carberry,Sandra,JenniferChu-Carroll,and
StephanieElzer.1999.Constructingand
utilizingamodelofuserpreferencesin
collaborativeconsultationdialogues.
ComputationalIntelligenceJournal,
15(3):185–217.
Carenini,GiuseppeandJohannaD.Moore.
2000.Astrategyforgeneratingevaluative
arguments.InProceedingsofINLG-00,
pages47–54,MitzpeRamon.
Carenini,GiuseppeandJohannaD.Moore.
2001.Anempiricalstudyoftheinﬂuence
ofusertailoringonevaluativeargument
effectiveness.InProceedingsofIJCAI-01,
pages1307–1314,Seattle,WA.
Carenini,GiuseppeandJohannaD.Moore.
2006.Generatingandevaluating
evaluativearguments.ArtiﬁcialIntelligence,
170:925–952.
Carroll,John,AnnCopestake,Dan
Flickinger,andVictorPozna´nski.1999.
Anefﬁcientchartgeneratorfor(semi-)
lexicalistgrammars.InProceedingsof
EWNLG-99,pages86–95,Toulouse,France.
Carroll,JohnandStefanOepen.2005.High
efﬁciencyrealizationforawide-coverage
uniﬁcationgrammar.InProceedingsof
IJCNLP-05,pages165–176,JejuIsland,
Korea.
Chung,Grace.2004.Developingaﬂexible
spokendialogsystemusingsimulation.
InProceedingsof ACL’04,pages63–70,
Barcelona.
Clark,RobertA.J.andSimonKing.2006.
Jointprosodicandsegmentalunitselection
speechsynthesis.InProceedingsof
Interspeech2006,pages1312–1315,
Pittsburgh,PA.
Clark,RobertA.J.,MonikaPodsiadlo,Mark
Fraser,CatherineMayo,andSimonKing.
2007.StatisticalanalysisoftheBlizzard
Challenge2007listeningtestresults.In
ProceedingsofBlizzard Workshop(in
Proceedingsofthe 6thISCAWorkshop on
SpeechSynthesis),pages1–6,Bonn,
Germany.
Clark,RobertA.J.,KorinRichmond,and
SimonKing.2007.Multisyn:Open-domain
unitselectionfortheFestivalspeech
synthesissystem.SpeechCommunication,
49(4):317–330.
Currie,K.andA.Tate.1991.O-Plan:The
openplanningarchitecture.Artiﬁcial
Intelligence,52:49–86.
deCarolis,Bernadina,CatherinePelachaud,
IsabellaPoggi,andMarkSteedman.2004.
APML,aMark-upLanguagefor
BelievableBehaviorGeneration.In
H.PrendingerandM.Ishizuka,editors,
Life-likeCharacters.Tools,Affective Functions
andApplications.Springer,Berlin,
pages65–85.
Demberg,VeraandJohannaD.Moore.2006.
Informationpresentationinspoken
dialoguesystems.InProceedingsofthe 11th
Conferenceof theEuropeanChapterofthe
Associationfor Computational Linguistics
(EACL-06),pages5–72,Trento,Italy.
Dusterhoff,KurtE.,AlanW.Black,and
PaulA.Taylor.1999.Usingdecisiontrees
withinthetiltintonationmodelto
predictf0contours.InEurospeech-99,
pages1627–1630,Budapest,Hungary.
Edwards,W.andF.H.Barron.1994.
SMARTSandSMARTER:Improved
simplemethodsformultiattributeutility
measurement.Organizational Behavior and
HumanDecisionProcesses,60:306–325.
Elhadad,Michael.1993.UsingArgumentation
toControlLexical Choice:AFunctional
Uniﬁcation Implementation.Ph.D.thesis,
ColumbiaUniversity.
Foster,MaryEllenandMichaelWhite.2004.
TechniquesforTextPlanningwithXSLT.
InProceedingsof the4thNLPXML Workshop,
pages1–8,Barcelona,Spain.
Fraser,MarkandSimonKing.2007.The
BlizzardChallenge2007.InProceedingsof
BlizzardWorkshop (inProceedingsofthe
6thISCAWorkshop on SpeechSynthesis),
pages7–12,Bonn,Germany.
Ginzburg,Jonathan.1996.Interrogatives:
Questions,factsanddialogue.In
ShalomLappin,editor,The Handbook of
ContemporarySemanticTheory.Blackwell,
Oxford,pages385–422.
Hirschberg,Julia.1993.Pitchaccentin
context:Predictingintonational
prominencefromtext.Artiﬁcial Intelligence,
63:305–340.
Hirschberg,JuliaandPilarPrieto.1996.
Trainingintonationalphrasingrules
automaticallyforEnglishandSpanish
text-to-speech.SpeechCommunication,
18:281–290.
Hitzeman,Janet,AlanW.Black,Chris
Mellish,JonOberlander,MassimoPoesio,
andPaulTaylor.1999.Anannotation
schemeforconcept-to-speechsynthesis.
InProceedingsof EWNLG-99,pages59–66,
Toulouse.
Hitzeman,Janet,AlanW.Black,Chris
Mellish,JonOberlander,andPaulTaylor.
1998.Ontheuseofautomatically
generateddiscourse-levelinformationina
concept-to-speechsynthesissystem.In
199
ComputationalLinguistics Volume36,Number2
Proceedingsof ICSLP-98,pages2763–2768,
Sydney,Australia.
Kay,Martin.1996.Chartgeneration.In
Proceedingsof ACL-96,pages200–204,
SantaCruz,USA.
Knight,KevinandVasileios
Hatzivassiloglou.1995.Two-level,
many-pathsgeneration.InProceedingsof
ACL-95,pages252–260,Cambridge,MA.
Kruijff,Geert-JanM.2003.Bindingacross
boundaries.InGeert-JanM.Kruijffand
RichardT.Oehrle,editors,Resource-
SensitivityinBindingandAnaphora.Kluwer
AcademicPublishers,pages123–158,
Dordrecht,TheNetherlands.
Kruijff-Korbayov´a,Ivana,StinaEricsson,
KepaJ.Rodr´ıguez,andElenaKaragjosova.
2003.Producingcontextuallyappropriate
intonationinaninformation-statebased
dialoguesystem.InProceedingsofEACL-93,
pages227–234,Budapest,Hungary.
Langkilde,Irene.2000.Forest-based
statisticalsentencegeneration.In
Proceedingsof NAACL-00,pages170–177,
Seattle,Washington.
Langkilde-Geary,Irene.2002.Anempirical
veriﬁcationofcoverageandcorrectness
forageneral-purposesentencegenerator.
InProceedingsofINLG-02,pages17–24,
NewYork,NY.
Linden,Greg,SteveHanks,andNealLesh.
1997.Interactiveassessmentofuser
preferencemodels:Theautomated
travelassistant.InProceedingsof User
Modeling ’97,pages67–78,ChiaLaguna,
Sardinia,Italy.
Marsi,Erwin.2004.Optionalityinevaluating
prosodyprediction.InProceedingsof the
5th ISCASpeechSynthesisWorkshop,
pages13–18,Pittsburg,PA.
Martin,D.L.,A.J.Cheyer,andD.B.Moran.
1999.TheOpenAgentArchitecture:A
frameworkforbuildingdistributed
softwaresystems.AppliedArtiﬁcial
Intelligence,13(1):91–128.
Mayo,C.,R.A.J.Clark,andS.King.2005.
Multidimensionalscalingoflistener
responsestosyntheticspeech.In
Interspeech2005,pages1725–1728,Lisbon.
Moore,Johanna,MaryEllenFoster,Oliver
Lemon,andMichaelWhite.2004.
Generatingtailored,comparative
descriptionsinspokendialogue.In
Proceedingsof FLAIRS-04,pages917–922,
MiamiBeach,USA.
Moore,RobertC.2002.Acomplete,efﬁcient
sentence-realizationalgorithmfor
uniﬁcationgrammar.InProceedingsof
INLG-02,pages41–48,NewYork,NY.
Nakatsu,CrystalandMichaelWhite.2006.
Learningtosayitwell:Reranking
realizationsbypredictedsynthesisquality.
InProceedingsofCOLING-ACL’06,
pages1113–1120,Sydney,Australia.
Oh,AliceH.andAlexanderI.Rudnicky.
2002.Stochasticnaturallanguage
generationforspokendialogsystems.
Computer,Speech& Language,
16(3/4):387–407.
Pan,Shimei,KathleenMcKeown,andJulia
Hirschberg.2002.Exploringfeaturesfrom
naturallanguagegenerationforprosody
modeling.ComputerSpeechand Language,
16:457–490.
Papineni,Kishore,SalimRoukos,Todd
Ward,andWei-JingZhu.2001.Bleu:A
methodforAutomaticEvaluationof
MachineTranslation.TechnicalReport
RC22176,IBM.
Pierrehumbert,Janet.1980.ThePhonology
and Phoneticsof EnglishIntonation.Ph.D.
thesis,MIT.
Polifroni,Joseph,GraceChung,and
StephanieSeneff.2003.Towards
automaticgenerationofmixed-initiative
dialoguesystemsfromWebcontent.
InProceedingsofEurospeech’03,
pages193–196,Geneva.
Pon-Barry,Heather,FuliangWeng,and
SebastianVarges.2006.Evaluationof
contentpresentationstrategiesforan
in-carspokendialoguesystem.In
Proceedingsof Interspeech2006,
pages1930–1933,Pittsburgh,PA.
Prevost,Scott.1995.ASemanticsof Contrast
and InformationStructurefor Specifying
IntonationinSpoken Language Generation.
Ph.D.thesis,UniversityofPennsylvania.
Ratnaparkhi,Adwait.2002.Trainable
approachestosurfacenaturallanguage
generationandtheirapplicationto
conversationaldialogsystems.Computer,
Speech&Language,16(3/4):435–455.
Reiter,EhudandRobertDale.2000.Building
NaturalLanguage GenerationSystems.
CambridgeUniversityPress,Cambridge.
Ristad,EricS.˙1995.ANaturalLawof
Succession.TechnicalReport
CS-TR-495-95,PrincetonUniversity.
Roberts,Craige.1996.Information
structure:Towardsanintegrated
formaltheoryofpragmatics.Ohio State
UniversityWorking PapersinLinguistics,
49:91–136.
Rocha,NeideFranca.2004.Evaluating
prosodicmarkupinaspokendialogue
system.Master’sthesis,Departmentof
Linguistics,UniversityofEdinburgh.
200
White,Clark,andMoore GeneratingTailoredDescriptionswithAppropriateIntonation
Rooth,Mats.1992.Atheoryoffocus
interpretation.NaturalLanguage Semantics,
1:75–116.
Shemtov,Hadar.1997.AmbiguityManagement
inNaturalLanguage Generation.Ph.D.
thesis,StanfordUniversity.
Silverman,K.,M.Beckman,J.Pitrelli,
M.Ostendorf,C.Wightman,P.Price,
J.Pierrehumbert,andJ.Hirschberg.1992.
ToBI:AstandardforlabelingEnglish
prosody.Proceedingsof ICSLP92,2:867–870,
Banff,Canada.
Snover,Matthew,BonnieDorr,Richard
Schwartz,LinneaMicciulla,andJohn
Makhoul.2006.Astudyoftranslationedit
ratewithtargetedhumanannotation.In
Proceedingsofthe Associationfor Machine
TranslationintheAmericas (AMTA-06),
pages223–231,Cambridge,MA.
Steedman,Mark.2000a.Information
structureandthesyntax-phonology
interface.LinguisticInquiry,
31(4):649–689.
Steedman,Mark.2000b.The SyntacticProcess.
MITPress,Cambridge,MA.
Steedman,Mark.2004.UsingAPMLto
specifyintonation.MagicsterProject
Deliverable2.5.UniversityofEdinburgh.
Availableathttp://www.ltg.ed.ac.uk/
magicster/deliverables/annex2.5/
apml-howto.pdf
Steedman,Mark.2006.Information-
structuralsemanticsforEnglish
intonation.InChungminLee,Matt
Gordon,andDanielB¨uring,editors,
Topic andFocus: Cross-Linguistic
Perspectiveson MeaningandIntonation.
Springer,Dordrecht,pages245–264.
Stolcke,Andreas.2002.SRILM—An
extensiblelanguagemodelingtoolkit.In
ProceedingsofICSLP-02,pages901–904,
Denver,Colorado.
Taylor,PaulandAlanBlack.1998.Assigning
phrasebreaksfrompart-of-speech
sequences.ComputerSpeechandLanguage,
12:99–117.
Taylor,P.,A.Black,andR.Caley.1998.The
architectureofthetheFestivalspeech
synthesissystem.InThirdInternational
Workshop onSpeechSynthesis,
pages147–151,Sydney.
Theune,Mari¨et.2002.Contrastin
concept-to-speechgeneration.Computer
SpeechandLanguage,16:491–531.
Theune,Mari¨et,EstherKlabbers,Jan-Roelof
dePijper,EmielKrahmer,andJanOdijk.
2001.Fromdatatospeech:Ageneral
approach.NaturalLanguage Engineering,
7(1):47–86.
Vallduv´ı,EnricandMariaVilkuna.1998.On
rhemeandkontrast.InPeterCulicover
andLouiseMcNally,editors,Syntaxand
Semantics,Vol.29: TheLimits ofSyntax.
AcademicPress,SanDiego,CA,
pages79–108.
vanDeemter,Kees,EmielKrahmer,and
Mari¨etTheune.2005.Realversus
template-basednaturallanguage
generation:Afalseopposition?
ComputationalLinguistics,31(1):15–23.
vanSanten,Jan,AlexanderKain,Esther
Klabbers,andTaniyaMishra.2005.
Synthesisofprosodyusingmulti-level
unitsequences.SpeechCommunication,
46(3–4):365–375.
Walker,M.A.,S.Whittaker,A.Stent,
P.Maloor,J.D.Moore,M.Johnston,and
G.Vasireddy.2002.Speech-plans:
Generatingevaluativeresponsesinspoken
dialogue.InProceedingsofINLG’02,
pages73–80,NewYork,NY.
Walker,M.A.,S.J.Whittaker,A.Stent,
P.Maloor,J.D.Moore,M.Johnston,and
G.Vasireddy.2004.Generationand
evaluationofuser-tailoredresponsesin
multimodaldialogue.CognitiveScience,
28:811–840.
Walker,MarilynA.,RebeccaPassonneau,
andJulieE.Boland.2001.Quantitative
andqualitativeevaluationofDARPA
Communicatorspokendialoguesystems.
InProceedingsof ACL-01,pages515–522,
Toulouse,France.
Walker,MarilynA.,OwenC.Rambow,and
MonicaRogati.2002.Trainingasentence
plannerforspokendialogueusing
boosting.ComputerSpeechand Language,
16:409–433.
White,Michael.2004.ReininginCCGChart
Realization.InProceedingsofINLG-04,
pages182–191,Brockenhurst,UK.
White,Michael.2006a.CCGchartrealization
fromdisjunctiveinputs.InProceedingsof
INLG-06,pages12–19,Sydney,Australia.
White,Michael.2006b.Efﬁcientrealizationof
coordinatestructuresinCombinatory
CategorialGrammar.Researchon Language
&Computation,4(1):39–75.
201


