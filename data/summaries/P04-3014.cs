Syntax-light alignment models such as the five IBM models (Brown et al., 1993) and their relatives have proved to be very successful and robust at producing word-level alignments, especially for closely related languages with similar word order and mostly local reorderings, which can be captured via simple models of relative word distortion.
J93-2003
Work The 2003 HLT-NAACL Workshop on Building and Using Parallel Texts (Mihalcea and Pedersen, 2003) reflected the increasing importance of the wordalignment task, and established standard performance measures and some benchmark tasks.
W03-0301
It is crucial for the development of translation models and translation lexica (Tufi¸s, 2002; Melamed, 1998), as well as for translingual projection (Yarowsky et al., 2001; Lopez et al., 2002).
H01-1035
In Rada Mihalcea and Ted Pedersen, editors, Proceedings of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts, pages 1–10.
W03-0301
We use GIZA++ (Och and Ney, 2000) to align the Englishprime with the source language text, yielding alignments in terms of the Englishprime.
C00-2163
Tree-to-string models, such as (Yamada and Knight, 2001) remove this dependency, and such models are well suited for situations with large, cleanly translated training corpora.
P01-1067
It has increasingly attracted attention as a task worthy of study in its own right (Mihalcea and Pedersen, 2003; Och and Ney, 2000).
C00-2163 W03-0301
4; Chinese training: news text from FBIS; Chinese testing: Penn Chinese Treebank news text aligned by Rebecca Hwa, then at the University of Maryland; Chinese dictionary: from the LDC; Romanian training and testing: (Mihalcea and Pedersen, 2003).
W03-0301
