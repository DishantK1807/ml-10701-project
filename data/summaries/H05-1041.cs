In recent work, Miliaraki and Androutsopoulos (2004), hereafter M&A, proposed a method we call 1See, for example, Wikipedia (http://www.wikipedia.org/).
C04-1199
We also note that the post-2003 TREC task has encountered evaluation problems, because it is difficult to agree on which nuggets should be included in the multi-snippet definitions (Hildebrandt et al., 2004).
N04-1007
Ng et al.(2001) use machine learning (C5 with boosting) to classify and rank candidate answers in a general QA system, but they do not treat definition questions in any special way; consequently, their worst results are for â€œWhat.
W01-0509
Since TREC-2003, several researchers have proposed ways to generate multi-snippet definitions (Cui et al., 2004; Fujii and Ishikawa, 2004; Hildebrandt et al., 2004; Xu et al., 2004).
C04-1093 N04-1007
The output is a list of k 250-character snippets from the r documents, at least one of which must contain an acceptable short definition of the target term, much as in the QA track of TREC-2000 and TREC-2001.3 We note that since 2003, TREC requires definition questions to be answered by lists of complementary snippets, jointly providing a range of information nuggets about the target term (Voorhees, 2003).
N03-2037
(Single-tailed difference-of-proportions tests show that all the differences of Table 1 are statisti11We follow the notation of Di Eugenio and Glass (2004).
J04-1005
Many of the proposed approaches, however, rely on manually crafted patterns or heuristics to identify definitions, and do not employ learning algorithms (Liu et al., 2003; Fujii and Ishikawa, 2004; Hildebrandt et al., 2004; Xu et al., 2004).
C04-1093 N04-1007
Hildebrandt et al.(2004) look up target terms in encyclopedias and dictionaries, and then, knowing the answers, try to find supporting evidence for them in the TREC document collection.
N04-1007
