In this work, the strength of association of a sequence of words w\[ = wl,..., wn is computed by two measures: a likelihood-based one p(w'~) (where g is the likelihood ratio given in (Dunning, 93)) and an entropy-based one e(w'~) (Shimohata et al., 1997).
P97-1061
This method allows the efficient retrieval of arbitrary length n-grams (Nagao and Mori, 94; Haruno et al., 96; Ikehara et al., 96; Shimohata et al., 1997; Russell, 1998).
P97-1061
Mapping the identified units (tokens or sequences) to their equivalents in the other language was achieved by training a new translation model (IBM 2) using the EM algorithm as described in (Brown et al., 1993).
J93-2003
It is also possible to focus on non-compositional compounds, a key point in bilingual applications (Su et al., 1994; Melamed, 1997; Lin, 99).
P94-1033 W97-0311
Furthermore, the underlying decoding strategies are too time consuming for our applicationâ€¢ We therefore use a translation model based on the simple linear interpolation given in equation 2 which combines predictions of two translation models -Ms and M~ -both based on IBM-like model 2(Brown et al., 1993).
J93-2003
One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints (Ganssier, 1995; Kupiec, 1993; hua Chen and Chen, 94; Fung, 1995; Evans and Zhai, 1996).
P93-1003 P95-1032 P96-1003
Techniques for weakening the independence assumptions made by the IBM models 1 and 2 have been proposed in recent work (Brown et al., 1993; Berger et al., 1996; Och and Weber, 98; Wang and Waibel, 98; Wu and Wong, 98).
J93-2003 J96-1002
approach to modeling this distribution is based to a large extent on that of the IBM group (Brown et al., 1993), but it differs in one significant aspect: whereas the IBM model involves a "noisy channel" decomposition, we use a linear combination of separate predictions from a language model p(tlt ~) and a translation model p(tls ).
J93-2003
