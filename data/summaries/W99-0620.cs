While there has been some work exploring the use of machine leaning techniques for discourse and dialogue (Marcu, 1997; Samuel et al., 1998), to our knowledge, no computational research on discourse or dialogue so far has addressed the problem of reducing or minimizing the amount of data for training a learning algorithm.
P97-1013 P98-2188
While a direct comparison with other learning schemes in discourse such as a transformation method (Samuel et al., 1998) is not feasible, if Samuel et al.(1998)'s approach is indeed comparable to C5.0, as discussed in Samuel et al.(1998), then the present method might be able to reduce the 164 Figure 2: The committee-based method (with 100 bootstraps) with the error feedback as compared against one without.
P98-2188
Engelson and Dagan (1996) investigated several plausible approaches to the selection function but were unable to find significant differences among them.
P96-1042
In contrast to Samuel et al.(1998), we did not consider relation cues reported in the linguistics literature, since they would be useless unless they contribute to reducing the cue entropy.
P98-2188
with Active Data Selection 3.1 Committee-based Sampling In the committee-based sampling method (CBS, henceforth) (Dagan and Engelson, 1995; Engelson and Dagan, 1996), a training example is selected from a corpus according to its usefulness; a preferred example is one whose addition to the training corpus improves the current estimate of a model parameter which is relevant to classification and also affects a large proportion of examples.
P96-1042
