<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>F B´echet</author>
</authors>
<title>Lia phon : un syst`eme complet de phonetisation de textes. Traitement Automatique des Langues</title>
<date>2001</date>
<pages>42--1</pages>
<marker>B´echet, 2001</marker>
<rawString>F. B´echet. 2001. Lia phon : un syst`eme complet de phonetisation de textes. Traitement Automatique des Langues, 42(1):47–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corpatext</author>
</authors>
<date>2006</date>
<journal>Corpatext</journal>
<volume>1</volume>
<pages>www.lexique.org/public.Corpatext.php.</pages>
<contexts>
<context>ion based on the phoneme bigram language model. Finally we discuss the procedure, the results and future work. 2. Text and recording The recorded text is a set of 3994 sentences in French, chosen in (Corpatext, 2006) for good phonetic and contextual covering. It was read by a male French speaker in a anechoic room and recorded with a high quality microphone This research was partially funded by the French RIAM n</context>
</contexts>
<marker>Corpatext, 2006</marker>
<rawString>Corpatext. 2006. Corpatext 1.02. www.lexique.org/public.Corpatext.php.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Donovan</author>
</authors>
<title>Current status of the IBM trainable speech synthesis system</title>
<date>2001</date>
<booktitle>In Proc. ESCA Workshop on Speech Synthesis</booktitle>
<location>Scotland, UK</location>
<contexts>
<context>e test procedures are discussed and possible improvements are proposed. 1. Introduction Very high quality text-to-speech synthesis can be achieved by unit selection in a large recorded speech corpus (Donovan, 2001). This technique uses some optimal choice of speech units (e.g. phones) in the corpus and concatenates them to produce speech output. For various reasons, synthesis sometimes has to be done from exis</context>
</contexts>
<marker>Donovan, 2001</marker>
<rawString>R.E. Donovan. 2001. Current status of the IBM trainable speech synthesis system. In Proc. ESCA Workshop on Speech Synthesis, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G D Fornay</author>
</authors>
<title>The Viterbi algorithm</title>
<date>1973</date>
<booktitle>Proceedings of the IEEE</booktitle>
<pages>61--3</pages>
<contexts>
<context>pending of the text transcription availability), its associated set of HMMs and an acoustic observation (MFCC), the log probability of any path through the graph can be computed. The Viterbi decoder (Fornay, 1973) then finds the path through the graph which maximises the log probability. Finally, the recogniser outputs the phonetic sequence that best matches the speech data. Two types of model have been exper</context>
</contexts>
<marker>Fornay, 1973</marker>
<rawString>G. D. Fornay. 1973. The Viterbi algorithm. Proceedings of the IEEE, 61(3):268–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lamel</author>
<author>J Gauvain</author>
<author>M Eskenazi</author>
</authors>
<title>BREF, a large vocabulary spoken corpus for french</title>
<date>1991</date>
<booktitle>In Proc. Eurospeech</booktitle>
<contexts>
<context>ephonememodelswhichfinally does not depend on the text. Systemtestresultshavebeenpresentedforthecaseofasingle speaker. However, the system has also been trained on the Bref-80 multispeaker data base (Lamel et al., 1991) and segmentation has been applied to unknown speaker recordings with promising results. More tests are to be done to evaluate Match accuracy and time accuracy in these conditions. A last improvement</context>
</contexts>
<marker>Lamel, Gauvain, Eskenazi, 1991</marker>
<rawString>L. Lamel, J. Gauvain, and M. Eskenazi. 1991. BREF, a large vocabulary spoken corpus for french. In Proc. Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A C Morris</author>
<author>V Maier</author>
<author>P D Green</author>
</authors>
<title>from WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition</title>
<date>2004</date>
<booktitle>In Proc. ICSLP</booktitle>
<contexts>
<context>for which recognition results are more sensitive in the HMMs topology than when considering the multi-pronunciation phonetic graph. HMMs topology is optimised according to the Match Accuracy measure (Morris et al., 2004)MAcc = 100×H/(H+S+D+I) where H, S, D, I are the hit (H), substitution (S), insertion (I) and deletion (D) counts obtained by Viterbi alignment of the given and detected phoneme sequences. This measur</context>
</contexts>
<marker>Morris, Maier, Green, 2004</marker>
<rawString>A.C. Morris, V. Maier, and P. D. Green. 2004. from WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition. In Proc. ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Boeffard S Nefti</author>
<author>T Moudenc</author>
</authors>
<title>Confidence measures for phonetic segmentation of continuous speech</title>
<date>2003</date>
<booktitle>In Proc. Eurospeech</booktitle>
<contexts>
<context>ll largely diminish the cost of hand manipulations. However, error locations are not provided directly in the system described here. Some confidence measures still needs to be computed such as in (S. Nefti and Moudenc, 2003). Another indication of possible errors could probably come from the comparison of the phoneme sequences provided by the segmentation based on the phoneme bigram language model (no text used) and the</context>
</contexts>
<marker>Nefti, Moudenc, 2003</marker>
<rawString>O. Boeffard S. Nefti and T. Moudenc. 2003. Confidence measures for phonetic segmentation of continuous speech. In Proc. Eurospeech.</rawString>
</citation>
</citationList>
</algorithm>

