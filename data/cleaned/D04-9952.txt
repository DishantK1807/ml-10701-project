1:231	Instance-Based Question Answering: A Data-Driven Approach Lucian Vlad Lita Carnegie Mellon University llita@cs.cmu.edu Jaime Carbonell Carnegie Mellon University jgc@cs.cmu.edu Abstract Anticipating the availability of large questionanswer datasets, we propose a principled, datadriven Instance-Based approach to Question Answering.
2:231	Most question answering systems incorporate three major steps: classify questions according to answer types, formulate queries for document retrieval, and extract actual answers.
3:231	Under our approach, strategies for answering new questions are directly learned from training data.
4:231	We learn models of answer type, query content, and answer extraction from clusters of similar questions.
5:231	We view the answer type as a distribution, rather than a class in an ontology.
6:231	In addition to query expansion, we learn general content features from training data and use them to enhance the queries.
7:231	Finally, we treat answer extraction as a binary classification problem in which text snippets are labeled as correct or incorrect answers.
8:231	We present a basic implementation of these concepts that achieves a good performance on TREC test data.
9:231	1 Introduction Ever since Question Answering (QA) emerged as an active research field, the community has slowly diversified question types, increased question complexity, and refined evaluation metrics as reflected by the TREC QA track (Voorhees, 2003).
10:231	Starting from successful pipeline architectures (Moldovan et al. , 2000; Hovy et al. , 2000), QA systems have responded to changes in the nature of the QA task by incorporating knowledge resources (Hermjakob et al. , 2000; Hovy et al. , 2002), handling additional types of questions, employing complex reasoning mechanisms (Moldovan et al. , 2003; Nyberg et al. , 2003), tapping into external data sources such as the Web, encyclopedias, databases (Dumais et al. , 2002; Xu et al. , 2003), and merging multiple agents and strategies into meta-systems (Chu-Carroll et al. , 2003; Burger et al. , 2002).
11:231	In recent years, learning components have started to permeate Question Answering (Clarke et al. , 2003; Ravichandran et al. , 2003; Echihabi and Marcu, 2003).
12:231	Although the field is still dominated by knowledge-intensive approaches, components such as question classification, answer extraction, and answer verification are beginning to be addressed through statistical methods.
13:231	At the same time, research efforts in data acquisition promise to deliver increasingly larger question-answer datasets (Girju et al. , 2003; Fleischman et al. , 2003).
14:231	Moreover, Question Answering is expanding to different languages (Magnini et al. , 2003) and domains other than news stories (Zweigenbaum, 2003).
15:231	These trends suggest the need for principled, statistically based, easily re-trainable, language independent QA systems that take full advantage of large amounts of training data.
16:231	We propose an instance-based, data-driven approach to Question Answering.
17:231	Instead of classifying questions according to limited, predefined ontologies, we allow training data to shape the strategies for answering new questions.
18:231	Answer models, query content models, and extraction models are also learned directly from training data.
19:231	We present a basic implementation of these concepts and evaluate the performance.
20:231	2 Motivation Most existing Question Answering systems classify new questions according to static ontologies.
21:231	These ontologies incorporate human knowledge about the expected answer (e.g. date, location, person), answer type granularity (e.g. date, year, century), and very often semantic information about the question type (e.g. birth date, discovery date, death date).
22:231	While effective to some degree, these ontologies are still very small, and inconsistent.
23:231	Considerable manual effort is invested into building and maintaining accurate ontologies even though answer types are arguably not always disjoint and hierarchical in nature (e.g. Where is the corpus callosum? expects an answer that is both location and body part).
24:231	The most significant drawback is that ontologies are not standard among systems, making individual component evaluation very difficult and re-training for new domains time-consuming.
25:231	2.1 Answer Modeling The task of determining the answer type of a question is usually considered a hard 1 decision problem: questions are classified according to an answer ontology.
26:231	The classification (location, persons name, etc) is usually made in the beginning of the QA process and all subsequent efforts are focused on finding answers of that particular type.
27:231	Several existing QA systems implement feedback loops (Harabagiu et al. , 2000) or full-fledged planning (Nyberg et al. , 2003) to allow for potential answer type re-classification.
28:231	However, most questions can have multiple answer types as well as specific answer type distributions.
29:231	The following questions can accommodate answers of types: full date, year, and decade.
30:231	Question Answer When did Glen lift off in Friendship7?
31:231	Feb. 20, 1962 When did Glen join NASA?
32:231	1959 When did Glen have long hair?
33:231	the fifties However, it can be argued that date is the most likely answer type to be observed for the first question, year the most likely type for the second question, and decade most likely for the third question.
34:231	In fact, although the three questions can be answered by various temporal expressions, the distributions over these expressions are quite different.
35:231	Existing answer models do not usually account for these distributions, even though there is a clear potential for better answer extraction and more refined answer scoring.
36:231	2.2 Document Retrieval When faced with a new question, QA systems usually generate few, carefully expanded queries which produce ranked lists of documents.
37:231	The retrieval step, which is very critical in the QA process, does not take full advantage of context information.
38:231	However, similar questions with known answers do share context information in the form of lexical and structural features present in relevant documents.
39:231	For example all questions of the type When was X born? find their answers in documents which often contain words such as native or record, phrases such as gave birth to X, and sometimes even specific parse trees.
40:231	Most IR research in Question Answering is focused on improving query expansion and structur1the answer is classified into a single class instead of generating a probability distribution over answers ing queries in order to take advantage of specific document pre-processing.
41:231	In addition to automatic query expansion for QA (Yang et al. , 2003), queries are optimized to take advantage of expansion resources and document sources.
42:231	Very often, these optimizations are performed offline, based on the type of question being asked.
43:231	Several QA systems associate this type of information with question ontologies: upon observing questions of a certain type, specific lexical features are sought in the retrieved documents.
44:231	These features are not always automatically learned in order to be used in query generation.
45:231	Moreover, systems are highly dependent on specific ontologies and become harder to re-train.
46:231	2.3 Answer Extraction Given a set of relevant documents, the answer extraction step consists of identifying snippets of text or exact phrases that answer the question.
47:231	Manual approaches to answer extraction have been moderately successful in the news domain.
48:231	Regular expressions, rule and pattern-based extraction are among the most efficient techniques for information extraction.
49:231	However, because of the difficulty in extending them to additional types of questions, learning methods are becoming more prevalent.
50:231	Current systems (Ravichandran et al. , 2003) already employ traditional information extraction and machine learning for extracting answers from relevant documents.
51:231	Boundary detection techniques, finite state transducers, and text passage classification are a few methods that are usually applied to this task.
52:231	The drawback shared by most statistical answer extractors is their reliance on predefined ontologies.
53:231	They are often tailored to expected answer types and require type-specific resources.
54:231	Gazetteers, encyclopedias, and other resources are used to generate type specific features.
55:231	3 Related Work Current efforts in data acquisition for Question Answering are becoming more and more common.
56:231	(Girju et al. , 2003) propose a supervised algorithm for part-whole relations extraction.
57:231	(Fleischman et al. , 2003) also propose a supervised algorithm that uses part of speech patterns and a large corpus to extract semantic relations for Who-is type questions.
58:231	Such efforts promise to provide large and dense datasets required by instance based approaches.
59:231	Several statistical approaches have proven to be successful in answer extraction.
60:231	The statistical agent presented in (Chu-Carroll et al. , 2003) uses Test Question: When did John Glen start working at NASA?
61:231	When did Jay Leno get a job at the NBC?
62:231	When did Columbus arrive at his destination?
63:231	When did <NNP+> <VB> at ? When did Sony begin its VAIO campaign?
64:231	When did Tom Ridge initiate the terror alert system?
65:231	When did <NNP+> <SYNSET to_initiate> ? When did Beethoven die?
66:231	When did Muhammad live?
67:231	When did <NNP> <VB> ? When did the Raiders win their last game?
68:231	When did EMNLP celebrate its 5th anniversary?
69:231	When did <NNP+> ? When did dinosaurs walk the earth?
70:231	When did people discover fire?
71:231	When did <NN> <VB> <NP> ? When did <NP> ? Figure 1: Neighboring questions are clustered according to features they share.
72:231	maximum entropy and models answer correctness by introducing a hidden variable representing the expected answer type.
73:231	Large corpora such as the Web can be mined for simple patterns (Ravichandran et al. , 2003) corresponding to individual question types.
74:231	These patterns are then applied to test questions in order to extract answers.
75:231	Other methods rely solely on answer redundancy (Dumais et al. , 2002): high performance retrieval engines and large corpora contribute to the fact that the most redundant entity is very often the correct answer.
76:231	Predictive annotation (Prager et al. , 1999) is one of the techniques that bring together corpus processing and smarter queries.
77:231	Twenty classes of objects are identified and annotated in the corpus, and corresponding labels are used to enhance IR queries.
78:231	Along the same lines, (Agichtein et al. , 2001) propose a method for learning query transformations in order to improve answer retrieval.
79:231	The method involves learning phrase features for question classification.
80:231	(Wen and Zhang, 2003) address the problem of query clustering based on semantic similarity and analyze several applications such as query re-formulation and index-term selection.
81:231	4 An Instance-Based Approach This paper presents a data driven, instance-based approach for Question Answering.
82:231	We adopt the view that strategies required in answering new questions can be directly learned from similar training examples (question-answer pairs).
83:231	Consider a multi-dimensional space, determined by features extracted from training data.
84:231	Each training question is represented as a data point in this space.
85:231	Features can range from lexical n-grams to parse trees elements, depending on available processing.
86:231	Each test question is also projected onto the feature space.
87:231	Its neighborhood consists of training instances that share a number of features with the new data point.
88:231	Intuitively, each neighbor is similar in some fashion to the new question.
89:231	The obvious next step would be to learn from the entire neighborhood similar to KNN classification.
90:231	However, due to the sparsity of the data and because different groups of neighbors capture different aspects of the test question, we choose to cluster the neighborhood instead.
91:231	Inside the neighborhood, we build individual clusters based on internal similarity.
92:231	Figure 1 shows an example of neighborhood clustering.
93:231	Notice that clusters may also have different granularity i.e. can share more or less features with the new question.
94:231	Cluster1 Models Answer Set 1 Cluster2 Models Answer Set 2 Cluster3 Models Answer Set 3 Clusterk Models Answer Set k Neighborhood Cluster2 Cluster3 Clusterk New Question NE Tagging POS Parsing Cluster1 Figure 2: The new question is projected onto the multidimensional feature space.
95:231	A set of neighborhood clusters are identified and a model is dynamically built for each of them.
96:231	Each model is applied to the test question in order to produce its own set of candidate answers.
97:231	By clustering the neighborhood, we set the stage for supervised methods, provided the clusters are sufficiently dense.
98:231	The goal is to learn models that explain individual clusters.
99:231	A model explains the data if it successfully answers questions from its corresponding cluster.
100:231	For each cluster, a models is constructed and tailored to the local data.
101:231	Models generating high confidence answers are applied to the new question to produce answer candidates (Figure 2) Since the test question belongs to multiple clusters, it benefits from different answerseeking strategies and different granularities.
102:231	Answering clusters of similar questions involves several steps: learning the distribution of the expected answer type, learning the structure and content of queries, and learning how to extract the answer.
103:231	Although present in most systems, these steps are often static, manually defined, or based on limited resources (section 2).
104:231	This paper proposes a set of trainable, cluster-specific models: 1.
105:231	the Answer Model Ai learns the cluster-specific distribution of answer types.
106:231	2.
107:231	the Query Content Model Ui is trained to enhance the keyword-based queries with cluster-specific content conducive to better document retrieval.
108:231	This model is orthogonal to query expansion.
109:231	3.
110:231	the Extraction Model Ei is dynamically built for answer candidate extraction, by classifying snippets of text whether they contain a correct answer or not.
111:231	Answer Model Query Content Model Extraction Model Cluster Models Training Samples (Q, A) Figure 3: Three cluster-specific components are learned in order to better retrieve relevant documents, model the expected answer, and then extract it from raw text.
112:231	Local question-answer pairs (Q,A) are used as training data.
113:231	These models are derived directly from cluster data and collectively define a focused strategy for finding answers to similar questions (Figure 3).
114:231	4.1 The Answer Model Learning cluster-specific answer type distributions is useful not only in terms of identifying answers in running text but also in answer ranking.
115:231	A probabilistic approach has the advantage of postponing answer type decisions from early in the QA process until answer extraction or answer ranking.
116:231	It also has the advantage of allowing training data to shape the expected structure of answers.
117:231	The answer modeling task consists of learning specific answer type distributions for each cluster of questions.
118:231	Provided enough data, simple techniques such as constructing finite state machines or learning regular expressions are sufficient.
119:231	The principle can also be applied to current answer ontologies by replacing the hard classification with a distribution over answer types.
120:231	For high-density clusters, the problem of learning the expected answer type is reduced to learning possible answer types and performing a reliable frequency count.
121:231	However, very often clusters are sparse (e.g. are based on rare features) and a more reliable method is required.
122:231	k-nearest training data points Q1::Qk can be used in order to estimate the probability that the test question q will observe an answer type j: P( j;q) = kX i=0 P( jjQi) (q;Qi) (1) where P( j;Qi) is the probability of observing an answer of type j when asking question Qi.
123:231	(q;Qi) represents a distance function between q and Qi, and is a normalizing factor over the set of all viable answer types in the neighborhood of q. 4.2 The Query Content Model Current Question Answering systems use IR in a straight-forward fashion.
124:231	Query terms are extracted and then expanded using statistical and semantic similarity measures.
125:231	Documents are retrieved and the top K are further processed.
126:231	This approach describes the traditional IR task and does not take advantage of specific constraints, requirements, and rich context available in the QA process.
127:231	The data-driven framework we propose takes advantage of knowledge available at retrieval time and incorporates it to create better cluster-specific queries.
128:231	In addition to query expansion, the goal is to learn content features: n-grams and paraphrases (Hermjakob et al. , 2002) which yield better queries when added to simple keyword-based queries.
129:231	The Query Content Model is a cluster-specific collection of content features that generate the best document set (Table 1).
130:231	Cluster: When did X start working for Y?
131:231	Simple Queries Query Content Model X, Y X joined Y in X, Y start working X started working for Y X, Y start working X was hired by Y Y hired X X, Y job interview  Table 1: Queries based only on X and Y question terms may not be appropriate if the two entities share a long history.
132:231	A focused, cluster-specific content model is likely to generate more precise queries.
133:231	For training, simple keyword-based queries are run through a retrieval engine in order to produce a set of potentially relevant documents.
134:231	Features (n-grams and paraphrases) are extracted and scored based on their co-occurrence with the correct answer.
135:231	More specifically, consider a positive class: documents which contain the correct answer, and a negative class: documents which do not contain the answer.
136:231	We compute the average mutual information I(C; Fi) between a class of a document, and the absence or presence of a feature fi in the document (McCallum and Nigam, 1998).
137:231	We let C be the class variable and Fi the feature variable: I(C; Fi) = H(C) H(CjFi) = X c2C X fi20;1 P(c;fi) log P(c;fi)P(c)P(f i) where H(C) is the entropy of the class variable and H(CjFi) is the entropy of the class variable conditioned on the feature variable.
138:231	Features that best discriminate passages containing correct answers from those that do not, are selected as potential candidates for enhancing keyword-based queries.
139:231	For each question-answer pair, we generate candidate queries by individually adding selected features (e.g. table 1) to the expanded word-based query.
140:231	The resulting candidate queries are subsequently run through a retrieval engine and scored based on the number of passages containing correct answers (precision).
141:231	The content features found in the top u candidate queries are included in the Query Content Model.
142:231	The Content Model is cluster specific and not instance specific.
143:231	It does not replace traditional query expansion both methods can be applied simultaneously to the test questions: specific keywords are the basis for traditional query expansion and clusters of similar questions are the basis for learning additional content conducive to better document retrieval.
144:231	Through the Query Content Model we allow shared context to play a more significant role in query generation.
145:231	4.3 The Extraction Model During training, documents are retrieved for each question cluster and a set of one-sentence passages containing a minimum number of query terms is selected.
146:231	The passages are then transformed into feature vectors to be used for classification.
147:231	The features consist of n-grams, paraphrases, distances between keywords and potential answers, simple statistics such as document and sentence length, part of speech features such as required verbs etc. More extensive sets of features can be found in information extraction literature (Bikel et al. , 1999).
148:231	Under our data-driven approach, answer extraction consists of deciding the correctness of candidate passages.
149:231	The task is to build a model that accepts snippets of text and decides whether they contain a correct answer.
150:231	A classifier is trained for each question cluster.
151:231	When new question instances arrive, the already trained cluster-specific models are applied to new, relevant text snippets in order to test for correctness.
152:231	We will refer to the resulting classifier scores as answer confidence scores.
153:231	5 Experiments We present a basic implementation of the instancebased approach.
154:231	The resulting QA system is fully automatically trained, without human intervention.
155:231	Instance-based approaches are known to require large, dense training datasets which are currently under development.
156:231	Although still sparse, the subset of all temporal questions from the TREC 9-12 (Voorhees, 2003) datasets is relatively dense compared to the rest of the question space.
157:231	This makes it a good candidate for evaluating our instance-based QA approach until larger and denser datasets become available.
158:231	It is also broad enough to include different question structures and varying degrees of difficulty and complexity such as: When did Beethoven die? How long is a quarter in an NBA game? What year did General Montgomery lead the Allies to a victory over the Axis troops in North Africa? The 296 temporal questions and their corresponding answer patterns provided by NIST were used in our experiments.
159:231	The questions were processed with a part of speech tagger (Brill, 1994) and a parser (Collins, 1999).
160:231	The questions were clustered using templatestyle frames that incorporate lexical items, parser labels, and surface form flags (Figure 1).
161:231	Consider the following question and several of its corresponding frames: When did Beethoven die? when did <NNP> die when did <NNP> <VB> when did <NNP> <Q> when did <NP> <Q> when did <Q> where <NNP>,<NP>,<VB>,<Q> denote: proper noun, noun phrase, verb, and generic question term sequence, respectively.
162:231	Initially, frames are generated exhaustively for each question.
163:231	Each frame that applies to more than three questions is then selected to represent a specific cluster.
164:231	One hundred documents were retrieved for each query through the Google API (www.google.com/api).
165:231	Documents containing the full question, question number, references to TREC, NIST, AQUAINT, Question Answering and other similar problematic content were filtered out.
166:231	When building the Query Content Model keyword-based queries were initially formulated and expanded.
167:231	From the retrieved documents a set of content features (n-grams and paraphrases) were selected through average mutual information.
168:231	The features were added to the simple queries and a new set of documents was retrieved.
169:231	The enhanced queries were scored and the corresponding top 10 ngrams/paraphrases were included in the Query Content Model.
170:231	The maximum n-gram and paraphrase size for these features was set to 6 words.
171:231	The Extraction Model uses a support vector machine (SVM) classifier (Joachims, 2002) with a linear kernel.
172:231	The task of the classifier is to decide if text snippets contain a correct answer.
173:231	The SVM was trained on features extracted from one-sentence passages containing at least one keyword from the original question.
174:231	The features consist of: distance between keywords and potential answers, keyword density in a passage, simple statistics such as document and sentence length, query type, lexical ngrams (up to 6-grams), and paraphrases.
175:231	We performed experiments using leave-one-out cross validation.
176:231	The system was trained and tested without any question filtering or manual input.
177:231	Each cluster produced an answer set with corresponding scores.
178:231	Top 5 answers for each instance were considered by a mean reciprocal rank (MRR) metric over all N questions: MRRN = PNi=0 1ranki, where ranki refers to the first correct occurrence in the top 5 answers for question i. While not the focus of this paper, answer clustering algorithms are likely to further improve performance.
179:231	6 Results The most important step in our instance-based approach is identifying clusters of questions.
180:231	Figure 4 shows the question distribution in terms of number of clusters.
181:231	For example: 30 questions belong to exactly 3 clusters.
182:231	The number of clusters corresponding to a question can be seen as a measure of how common the question is the more clusters a question has, the more likely it is to have a dense neighborhood.
183:231	The resulting MRR is 0:447 and 61:5% questions have correct answers among the first five proposed answers.
184:231	This translates into results consistently above the sixth highest score at each TREC 9-12.
185:231	Our results were compared directly to the top performing systems results on the same temporal 2 3 4 5 6 7 8 9 larger0 10 20 30 40 50 60 70 80 Question Distribution With Number of Clusters # clusters # questions (avg) Figure 4: Question distribution each bar shows the number of questions that belong to exactly c clusters.
186:231	1 2 3 4 5 6 7 80 10 20 30 40 50 60 70 80 Cluster Contribution to Top 10 Answers # clusters # questions Figure 5: Number of clusters that contribute with correct answers to the final answer set only the top 10 answers were considered for each question.
187:231	question test set.
188:231	Figure 5 shows the degree to which clusters produce correct answers to test questions.
189:231	Very often, more than one cluster contributes to the final answer set, which suggests that there is a benefit in clustering the neighborhood according to different similarity features and granularity.
190:231	It is not surprising that cluster size is not correlated with performance (Figure 6).
191:231	The overall strategy learned from the cluster When did <NP> die? corresponds to an MRR of 0:79, while the strategy learned from cluster How <Q>? corresponds to an MRR of 0:13.
192:231	Even if the two clusters generate strategies with radically different performance, they have the same size 10 questions are covered by each cluster.
193:231	Figure 7 shows that performance is correlated with answer confidence scores.
194:231	The higher the confidence threshold the higher the precision (MRR) of the predicted answers.
195:231	When small, unstable clusters are ignored, the predicted MRR improves considerably.
196:231	Small clusters tend to produce unsta0 20 40 60 80 100 1200 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Performance And Cluster Size cluster size MRR Figure 6: Since training data is not uniformly distributed in the feature space, cluster size is not well correlated with performance.
197:231	A specific cardinality may represent a small and dense part cluster, or a large and sparse cluster.
198:231	0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.90.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Performance And Confidence Thresholds confidence threshold MRR Cardinality 2+ Cardinality 3+ Cardinality 4+ Cardinality 5+ Figure 7: MRR of predicted answers varies with answer confidence thresholds.
199:231	There is a tradeoff between confidence threshold and MRR . The curves represent different thresholds for minimum cluster size.
200:231	ble strategies and have extremely low performance.
201:231	Often times structurally different but semantically equivalent clusters have a higher cardinality and much better performance.
202:231	For example, the cluster What year did <NP> die? has cardinality 2 and a corresponding MRR of zero.
203:231	However, as seen previously, the cluster When did <NP> die? has cardinality 10 and a corresponding MRR of 0:79.
204:231	Table 2 presents an intuitive cluster and the top ngrams and paraphrases with most information content.
205:231	Each feature has also a corresponding average mutual information score.
206:231	These particular content features are intuitive and highly indicative of a correct answer.
207:231	However, in sparse clusters, the content features have less information content and are more vague.
208:231	For example, the very sparse cluster When was <Q>? yields content features such as April, May, in the spring of, back in which only suggest broad temporal expressions.
209:231	Cluster: When did <QTERM> die?
210:231	N-grams Paraphrases 0.81 his death in 0.80 <Q> died in 0.78 died on 0.78 <Q> died 0.77 died in 0.68 <Q> died on 0.75 death in 0.58 <Q> died at 0.73 of death 0.38 <Q>, who died 0.69 to his death 0.38 <Q> dies 0.66 died 0.38 <Q> died at the age of 0.63, born on 0.38 <Q>, born 0.63 date of death 0.35 <Q> s death on Table 2: Query Content Model: learning n-grams and paraphrases for class When did <NP> die?, where <Q> refers to a phrase in the original question.
211:231	7 Conclusions This paper presents an principled, statistically based, instance-based approach to Question Answering.
212:231	Strategies and models required for answering new questions are directly learned from training data.
213:231	Since training requires very little human effort, relevant context, high information query content, and extraction are constantly improved with the addition of more question-answer pairs.
214:231	Training data is a critical resource for this approach clusters with very few data points are not likely to generate accurate models.
215:231	However, research efforts involving data acquisition are promising to deliver larger datasets in the near future and solve this problem.
216:231	We present an implementation of the instance-based QA approach and we evaluate it on temporal questions.
217:231	The dataset is of reasonable size and complexity, and is sufficiently dense for applying instance-based methods.
218:231	We performed leave-one-out cross validation experiments and obtained an overall mean reciprocal rank of 0:447.
219:231	61:5% of questions obtained correct answers among the top five which is equivalent to a score in the top six TREC systems on the same test set.
220:231	The experiments show that strategies derived from very small clusters are noisy and unstable.
221:231	When larger clusters are involved, answer confidence becomes correlated with higher predictive performance.
222:231	Moreover, when ignoring sparse data, answering strategies tend to be more stable.
223:231	This supports the need for more training data as means to improve the overall performance of the data driven, instance based approach to question answering.
224:231	8 Current & Future Work Data is the single most important resource for instance-based approaches.
225:231	Currently we are exploring large-scale data acquisition methods that can provide the necessary training data density for most question types, as well as the use of trivia questions in the training process.
226:231	Our data-driven approach to Question Answering has the advantage of incorporating learning components.
227:231	It is very easy to train and makes use of very few resources.
228:231	This property suggests that little effort is required to re-train the system for different domains as well as other languages.
229:231	We plan to apply instance-based QA to European languages and test this hypothesis using training data acquired through unsupervised means.
230:231	More effort is required in order to better integrate the cluster-specific models.
231:231	Strategy overlap analysis and refinement of local optimization criteria has the potential to improve overall performance under time constraints.

