And socalled semantic parsing (Gildea and Jurafsky, 2002) provides as end output only a flat classification of semantic arguments of predicates, ignoring much of the semantic content, such as quantifiers.
J02-3001
For this reason, we use a broadcoverage statistical parser (Klein and Manning, 2003) trained on the Penn Treebank.
P03-1054
There is growing awareness in the probabilistic parsing literature that mismatches between training and test set genre can degrade parse accuracy, and that small amounts of correct-genre data can be more important than large amounts of wrong-genre data (Gildea, 2001); we have found corroborating evidence in misparsings of noun phrases common in puzzle texts, such as “Sculptures C and E”, which do not appear in the Wall Street Journal corpus.
W01-0521
The field of Question Answering (Pasca and Harabagiu, 2001; Moldovan et al., 2003) focuses on simple-fact queries.
N03-1022
Traditional approaches to natural language understanding (Woods, 1973; Warren and Pereira, 1982; Alshawi, 1992) provided a good account of mapping from surface forms to semantic representations, when confined to a very limited vocabulary, syntax, and world model, and resulting low levels of syntactic/semantic ambiguity.
J82-3002
Moreover, the domain is another example of “found test material” in the sense of (Hirschman et al., 1999): puzzle texts were developed with a goal independent of the evaluation of natural language processing systems, and so provide a more realistic evaluation framework than specially-designed tests such as TREC QA.
P99-1042
