1:298	Scaling Web-based Acquisition of Entailment Relations Idan Szpektordiamondmath idan@szpektor.net ITC-Irst, Via Sommarive, 18 (Povo) 38050 Trento, Italy DIT University of Trento, Via Sommarive, 14 (Povo) 38050 Trento, Italy Department of Computer Science, Bar Ilan University Ramat Gan 52900, Israel diamondmathDepartment of Computer Science, Tel Aviv University Tel Aviv 69978, Israel Hristo Tanev tanev@itc.it Ido Dagan dagan@cs.biu.ac.il Bonaventura Coppola coppolab@itc.it Abstract Paraphrase recognition is a critical step for natural language interpretation.
2:298	Accordingly, many NLP applications would benefit from high coverage knowledge bases of paraphrases.
3:298	However, the scalability of state-of-the-art paraphrase acquisition approaches is still limited.
4:298	We present a fully unsupervised learning algorithm for Web-based extraction of entailment relations, an extended model of paraphrases.
5:298	We focus on increased scalability and generality with respect to prior work, eventually aiming at a full scale knowledge base.
6:298	Our current implementation of the algorithm takes as its input a verb lexicon and for each verb searches the Web for related syntactic entailment templates.
7:298	Experiments show promising results with respect to the ultimate goal, achieving much better scalability than prior Web-based methods.
8:298	1 Introduction Modeling semantic variability in language has drawn a lot of attention in recent years.
9:298	Many applications like QA, IR, IE and Machine Translation (Moldovan and Rus, 2001; Hermjakob et al. , 2003; Jacquemin, 1999) have to recognize that the same meaning can be expressed in the text in a huge variety of surface forms.
10:298	Substantial research has been dedicated to acquiring paraphrase patterns, which represent various forms in which a certain meaning can be expressed.
11:298	Following (Dagan and Glickman, 2004) we observe that a somewhat more general notion needed for applications is that of entailment relations (e.g.
12:298	(Moldovan and Rus, 2001)).
13:298	These are directional relations between two expressions, where the meaning of one can be entailed from the meaning of the other.
14:298	For example X acquired Y entails X owns Y.
15:298	These relations provide a broad framework for representing and recognizing semantic variability, as proposed in (Dagan and Glickman, 2004).
16:298	For example, if a QA system has to answer the question Who owns Overture? and the corpus includes the phrase Yahoo acquired Overture, the system can use the known entailment relation to conclude that this phrase really indicates the desired answer.
17:298	More examples of entailment relations, acquired by our method, can be found in Table 1 (section 4).
18:298	To perform such inferences at a broad scale, applications need to possess a large knowledge base (KB) of entailment patterns.
19:298	We estimate such a KB should contain from between a handful to a few dozens of relations per meaning, which may sum to a few hundred thousands of relations for a broad domain, given that a typical lexicon includes tens of thousands of words.
20:298	Our research goal is to approach unsupervised acquisition of such a full scale KB.
21:298	We focus on developing methods that acquire entailment relations from the Web, the largest available resource.
22:298	To this end substantial improvements are needed in order to promote scalability relative to current Webbased approaches.
23:298	In particular, we address two major goals: reducing dramatically the complexity of required auxiliary inputs, thus enabling to apply the methods at larger scales, and generalizing the types of structures that can be acquired.
24:298	The algorithms described in this paper were applied for acquiring entailment relations for verb-based expressions.
25:298	They successfully discovered several relations on average per each randomly selected expression.
26:298	2 Background and Motivations This section provides a qualitative view of prior work, emphasizing the perspective of aiming at a full-scale paraphrase resource.
27:298	As there are still no standard benchmarks, current quantitative results are not comparable in a consistent way.
28:298	The major idea in paraphrase acquisition is often to find linguistic structures, here termed templates, that share the same anchors.
29:298	Anchors are lexical elements describing the context of a sentence.
30:298	Templates that are extracted from different sentences and connect the same anchors in these sentences, are assumed to paraphrase each other.
31:298	For example, the sentences Yahoo bought Overture and Yahoo acquired Overture share the anchors {X=Yahoo, Y =Overture}, suggesting that the templates X buy Y and X acquire Y paraphrase each other.
32:298	Algorithms for paraphrase acquisition address two problems: (a) finding matching anchors and (b) identifying template structure, as reviewed in the next two subsections.
33:298	2.1 Finding Matching Anchors The prominent approach for paraphrase learning searches sentences that share common sets of multiple anchors, assuming they describe roughly the same fact or event.
34:298	To facilitate finding many matching sentences, highly redundant comparable corpora have been used.
35:298	These include multiple translations of the same text (Barzilay and McKeown, 2001) and corresponding articles from multiple news sources (Shinyama et al. , 2002; Pang et al. , 2003; Barzilay and Lee, 2003).
36:298	While facilitating accuracy, we assume that comparable corpora cannot be a sole resource due to their limited availability.
37:298	Avoiding a comparable corpus, (Glickman and Dagan, 2003) developed statistical methods that match verb paraphrases within a regular corpus.
38:298	Their limited scale results, obtaining several hundred verb paraphrases from a 15 million word corpus, suggest that much larger corpora are required.
39:298	Naturally, the largest available corpus is the Web.
40:298	Since exhaustive processing of the Web is not feasible, (Duclaye et al. , 2002) and (Ravichandran and Hovy, 2002) attempted bootstrapping approaches, which resemble the mutual bootstrapping method for Information Extraction of (Riloff and Jones, 1999).
41:298	These methods start with a provided known set of anchors for a target meaning.
42:298	For example, the known anchor setMozart, 1756}is given as input in order to find paraphrases for the template X born in Y.
43:298	Web searching is then used to find occurrences of the input anchor set, resulting in new templates that are supposed to specify the same relation as the original one (born in).
44:298	These new templates are then exploited to get new anchor sets, which are subsequently processed as the initial {Mozart, 1756}.
45:298	Eventually, the overall procedure results in an iterative process able to induce templates from anchor sets and vice versa.
46:298	The limitation of this approach is the requirement for one input anchor set per target meaning.
47:298	Preparing such input for all possible meanings in broad domains would be a huge task.
48:298	As will be explained below, our method avoids this limitation by finding all anchor sets automatically in an unsupervised manner.
49:298	Finally, (Lin and Pantel, 2001) present a notably different approach that relies on matching separately single anchors.
50:298	They limit the allowed structure of templates only to paths in dependency parses connecting two anchors.
51:298	The algorithm constructs for each possible template two feature vectors, representing its co-occurrence statistics with the two anchors.
52:298	Two templates with similar vectors are suggested as paraphrases (termed inference rule).
53:298	Matching of single anchors relies on the general distributional similarity principle and unlike the other methods does not require redundancy of sets of multiple anchors.
54:298	Consequently, a much larger number of paraphrases can be found in a regular corpus.
55:298	Lin and Pantel report experiments for 9 templates, in which their system extracted 10 correct inference rules on average per input template, from 1GB of news data.
56:298	Yet, this method also suffers from certain limitations: (a) it identifies only templates with pre-specified structures; (b) accuracy seems more limited, due to the weaker notion of similarity; and (c) coverage is limited to the scope of an available corpus.
57:298	To conclude, several approaches exhaustively process different types of corpora, obtaining varying scales of output.
58:298	On the other hand, the Web is a huge promising resource, but current Web-based methods suffer serious scalability constraints.
59:298	2.2 Identifying Template Structure Paraphrasing approaches learn different kinds of template structures.
60:298	Interesting algorithms are presented in (Pang et al. , 2003; Barzilay and Lee, 2003).
61:298	They learn linear patterns within similar contexts represented as finite state automata.
62:298	Three classes of syntactic template learning approaches are presented in the literature: learning of predicate argument templates (Yangarber et al. , 2000), learning of syntactic chains (Lin and Pantel, 2001) and learning of sub-trees (Sudo et al. , 2003).
63:298	The last approach is the most general with respect to the template form.
64:298	However, its processing time increases exponentially with the size of the templates.
65:298	As a conclusion, state of the art approaches still learn templates of limited form and size, thus restricting generality of the learning process.
66:298	3 The TE/ASE Acquisition Method Motivated by prior experience, we identify two major goals for scaling Web-based acquisition of entailment relations: (a) Covering the broadest possible range of meanings, while requiring minimal input and (b) Keeping template structures as general as possible.
67:298	To address the first goal we require as input only a phrasal lexicon of the relevant domain (including single words and multiword expressions).
68:298	Broad coverage lexicons are widely available or may be constructed using known term acquisition techniques, making it a feasible and scalable input requirement.
69:298	We then aim to acquire entailment relations that include any of the lexicons entries.
70:298	The second goal is addressed by a novel algorithm for extracting the most general templates being justified by the data.
71:298	For each lexicon entry, denoted a pivot, our extraction method performs two phases: (a) extract promising anchor sets for that pivot (ASE, Section 3.1), and (b) from sentences containing the anchor sets, extract templates for which an entailment relation holds with the pivot (TE, Section 3.2).
72:298	Examples for verb pivots are: acquire, fall to, prevent.
73:298	We will use the pivot prevent for examples through this section.
74:298	Before presenting the acquisition method we first define its output.
75:298	A template is a dependency parsetree fragment, with variable slots at some tree nodes (e.g. X subj prevent objY).
76:298	An entailment relation between two templates T1 and T2 holds if the meaning of T2 can be inferred from the meaning of T1 (or vice versa) in some contexts, but not necessarily all, under the same variable instantiation.
77:298	For example, X subj prevent objY entails X subj reduce objY risk because the sentence aspirin reduces heart attack risk can be inferred from aspirin prevents a first heart attack.
78:298	Our output consists of pairs of templates for which an entailment relation holds.
79:298	3.1 Anchor Set Extraction (ASE) The goal of this phase is to find a substantial number of promising anchor sets for each pivot.
80:298	A good anchor-set should satisfy a proper balance between specificity and generality.
81:298	On one hand, an anchor set should correspond to a sufficiently specific setting, so that entailment would hold between its different occurrences.
82:298	On the other hand, it should be sufficiently frequent to appear with different entailing templates.
83:298	Finding good anchor sets based on just the input pivot is a hard task.
84:298	Most methods identify good repeated anchors in retrospect, that is after processing a full corpus, while previous Web-based methods require at least one good anchor set as input.
85:298	Given our minimal input, we needed refined criteria that identify a priori the relatively few promising anchor sets within a sample of pivot occurrences.
86:298	ASE ALGORITHM STEPS: For each pivot (a lexicon entry) 1.
87:298	Create a pivot template, Tp 2.
88:298	Construct a parsed sample corpus S for Tp: (a) Retrieve an initial sample from the Web (b) Identify associated phrases for the pivot (c) Extend S using the associated phrases 3.
89:298	Extract candidate anchor sets from S: (a) Extract slot anchors (b) Extract context anchors 4.
90:298	Filter the candidate anchor sets: (a) by absolute frequency (b) by conditional pivot probability Figure 1: Outline of the ASE algorithm.
91:298	The ASE algorithm (presented in Figure 1) performs 4 main steps.
92:298	STEP (1) creates a complete template, called the pivot template and denoted Tp, for the input pivot, denoted P. Variable slots are added for the major types of syntactic relations that interact with P, based on its syntactic type.
93:298	These slots enable us to later match Tp with other templates.
94:298	For verbs, we add slots for a subject and for an object or a modifier (e.g. X subj prevent objY).
95:298	STEP (2) constructs a sample corpus, denoted S, for the pivot template.
96:298	STEP (2.A) utilizes a Web search engine to initialize S by retrieving sentences containing P. The sentences are parsed by the MINIPAR dependency parser (Lin, 1998), keeping only sentences that contain the complete syntactic template Tp (with all the variables instantiated).
97:298	STEP (2.B) identifies phrases that are statistically associated with Tp in S. We test all noun-phrases in S, discarding phrases that are too common on the Web (absolute frequency higher than a threshold MAXPHRASEF), such as desire.
98:298	Then we select the N phrases with highest tfidf score1.
99:298	These phrases have a strong collocation relationship with the pivot P and are likely to indicate topical (rather than anecdotal) occurrences of P. For example, the phrases patient and American Dental Association, which indicate contexts of preventing health problems, were selected for the pivot prevent.
100:298	Fi1Here, tfidf = freqS(X)  log parenleftBig N freqW(X) parenrightBig where freqS(X) is the number of occurrences in S containing X, N is the total number of Web documents, and freqW(X) is the number of Web documents containing X. nally, STEP (2.C) expands S by querying the Web with the both P and each of the associated phrases, adding the retrieved sentences to S as in step (2.a).
101:298	STEP (3) extracts candidate anchor sets for Tp.
102:298	From each sentence in S we try to generate one candidate set, containing noun phrases whose Web frequency is lower than MAXPHRASEF.
103:298	STEP (3.A) extracts slot anchors  phrases that instantiate the slot variables of Tp.
104:298	Each anchor is marked with the corresponding slot.
105:298	For example, the anchors {antibioticssubj, miscarriage obj} were extracted from the sentence antibiotics in pregnancy prevent miscarriage.
106:298	STEP (3.B) tries to extend each candidate set with one additional context anchor, in order to improve its specificity.
107:298	This anchor is chosen as the highest tfidf scoring phrase in the sentence, if it exists.
108:298	In the previous example, pregnancy is selected.
109:298	STEP (4) filters out bad candidate anchor sets by two different criteria.
110:298	STEP (4.A) maintains only candidates with absolute Web frequency within a threshold range [MINSETF, MAXSETF], to guarantee an appropriate specificity-generality level.
111:298	STEP (4.B) guarantees sufficient (directional) association between the candidate anchor set c and Tp, by estimating Prob(Tp|c) freqW(P c)freq W(c) where freqW is Web frequency and P is the pivot.
112:298	We maintain only candidates for which this probability falls within a threshold range [SETMINP, SETMAXP].
113:298	Higher probability often corresponds to a strong linguistic collocation between the candidate and Tp, without any semantic entailment.
114:298	Lower probability indicates coincidental cooccurrence, without a consistent semantic relation.
115:298	The remaining candidates in S become the input anchor-sets for the template extraction phase, for example,{Aspirinsubj, heart attackobj}for prevent.
116:298	3.2 Template Extraction (TE) The Template Extraction algorithm accepts as its input a list of anchor sets extracted from ASE for each pivot template.
117:298	Then, TE generates a set of syntactic templates which are supposed to maintain an entailment relationship with the initial pivot template.
118:298	TE performs three main steps, described in the following subsections: 1.
119:298	Acquisition of a sample corpus from the Web.
120:298	2.
121:298	Extraction of maximal most general templates from that corpus.
122:298	3.
123:298	Post-processing and final ranking of extracted templates.
124:298	3.2.1 Acquisition of a sample corpus from the Web For each input anchor set, TE acquires from the Web a sample corpus of sentences containing it.
125:298	For example, a sentence from the sample corpus for {aspirin, heart attack} is: Aspirin stops heart attack?.
126:298	All of the sample sentences are then parsed with MINIPAR (Lin, 1998), which generates from each sentence a syntactic directed acyclic graph (DAG) representing the dependency structure of the sentence.
127:298	Each vertex in this graph is labeled with a word and some morphological information; each graph edge is labeled with the syntactic relation between the words it connects.
128:298	TE then substitutes each slot anchor (see section 3.1) in the parse graphs with its corresponding slot variable.
129:298	Therefore, Aspirin stops heart attack? will be transformed into X stop Y.
130:298	This way all the anchors for a certain slot are unified under the same variable name in all sentences.
131:298	The parsed sentences related to all of the anchor sets are subsequently merged into a single set of parse graphs S ={P1,P2,,Pn}(see P1 and P2 in Figure 2).
132:298	3.2.2 Extraction of maximal most general templates The core of TE is a General Structure Learning algorithm (GSL) that is applied to the set of parse graphs S resulting from the previous step.
133:298	GSL extracts single-rooted syntactic DAGs, which are named spanning templates since they must span at least over Na slot variables, and should also appear in at least Nr sentences from S (In our experiments we set Na=2 and Nr=2).
134:298	GSL learns maximal most general templates: they are spanning templates which, at the same time, (a) cannot be generalized by further reduction and (b) cannot be further extended keeping the same generality level.
135:298	In order to properly define the notion of maximal most general templates, we introduce some formal definitions and notations.
136:298	DEFINITION: For a spanning template t we define a sentence set, denoted with (t), as the set of all parsed sentences in S containing t. For each pair of templates t1 and t2, we use the notation t1 precedesequalt2 to denote that t1 is included as a subgraph or is equal to t2.
137:298	We use the notation t1 t2 when such inclusion holds strictly.
138:298	We define T(S) as the set of all spanning templates in the sample S. DEFINITION: A spanning template t  T(S) is maximal most general if and only if both of the following conditions hold: CONDITION A: FortprimeT(S),tprimeprecedesequalt, it holds that (t) = (tprime).
139:298	CONDITION B: FortprimeT(S),ttprime, it holds that (t)(tprime).
140:298	Condition A ensures that the extracted templates do not contain spanning sub-structures that are more general (i.e. having a larger sentence set); condition B ensures that the template cannot be further enlarged without reducing its sentence set.
141:298	GSL performs template extraction in two main steps: (1) build a compact graph representation of all the parse graphs from S; (2) extract templates from the compact representation.
142:298	A compact graph representation is an aggregate graph which joins all the sentence graphs from S ensuring that all identical spanning sub-structures from different sentences are merged into a single one.
143:298	Therefore, each vertex v (respectively, edge e) in the aggregate graph is either a copy of a corresponding vertex (edge) from a sentence graph Pi or it represents the merging of several identically labeled vertices (edges) from different sentences in S. The set of such sentences is defined as the sentence set of v (e), and is represented through the set of index numbers of related sentences (e.g. (1,2) in the third tree of Figure 2).
144:298	We will denote with Gi the compact graph representation of the first i sentences in S. The parse trees P1 and P2 of two sentences and their related compact representation G2 are shown in Figure 2.
145:298	Building the compact graph representation The compact graph representation is built incrementally.
146:298	The algorithm starts with an empty aggregate graph G0 and then merges the sentence graphs from S one at a time into the aggregate structure.
147:298	Lets denote the current aggregate graph with Gi1(Vg,Eg) and let Pi(Vp,Ep) be the parse graph which will be merged next.
148:298	Note that the sentence set of Pi is a single element seti}.
149:298	During each iteration a new graph is created as the union of both input graphs: Gi = Gi1Pi.
150:298	Then, the following merging procedure is performed on the elements of Gi 1.
151:298	ADDING GENERALIZED VERTICES TO Gi.
152:298	For every two vertices vg  Vg,vp  Vp having equal labels, a new generalized vertex vnewg is created and added to Gi.
153:298	The new vertex takes the same label and holds a sentence set which is formed from the sentence set of vg by adding i to it.
154:298	Still with reference to Figure 2, the generalized vertices in G2 are X, Y and stop.
155:298	The algorithm connects the generalized vertex vnewg with all the vertices which are connected with vg and vp.
156:298	2.
157:298	MERGING EDGES.
158:298	If two edges eg Eg and ep  Ep have equal labels and their corresponding adjacent vertices have been merged, then ea and ep are also merged into a new edge.
159:298	In Figure 2 the edges (stop, X) and (stop, Y) from P1 and P2 are eventually merged into G2.
160:298	3.
161:298	DELETING MERGED VERTICES.
162:298	Every vertex v from Vp or Vg for which at least one generalized vertex vnewg exists is deleted from Gi.
163:298	As an optimization step, we merge only vertices and edges that are included in equal spanning templates.
164:298	Extracting the templates GSL extracts all maximal most general templates from the final compact representation Gn using the following sub-algorithm: 1.
165:298	BUILDING MINIMAL SPANNING TREES.
166:298	For every Na different slot variables in Gn having a common ancestor, a minimal spanning tree st is built.
167:298	Its sentence set is computed as the intersection of the sentence sets of its edges and vertices.
168:298	2.
169:298	EXPANDING THE SPANNING TREES.
170:298	Every minimal spanning tree st is expanded to the maximal sub-graph maxst whose sentence set is equal to (st).
171:298	All maximal single-rooted DAGs in maxst are extracted as candidate templates.
172:298	Maximality ensures that the extracted templates cannot be expanded further while keeping the same sentence set, satisfying condition B. 3.
173:298	FILTERING.
174:298	Candidates which contain another candidate with a larger sentence set are filtered out.
175:298	This step guarantees condition A. In Figure 2 the maximal most general template in G2 is X subj stop objY.
176:298	3.2.3 Post-processing and ranking of extracted templates As a last step, names and numbers are filtered out from the templates.
177:298	Moreover, TE removes those templates which are very long or which appear with just one anchor set and in less than four sentences.
178:298	Finally, the templates are sorted first by the number of anchor sets with which each template appeared, and then by the number of sentences in which they appeared.
179:298	4 Evaluation We evaluated the results of the TE/ASE algorithm on a random lexicon of verbal forms and then assessed its performance on the extracted data through human-based judgments.
180:298	P1 : stop subjd122 d122d122 d124d124d122d122d122d122 obj d65d65d65 d32d32d65d65d65d65 P2 : stop subjd122 d122d122 d124d124d122d122d122d122 obj d15d15 byd74 d74d74d74 d37d37d74d74d74d74 G2 : stop(1,2) subj(1,2)d114d114 d114d114 d120d120d114d114d114d114 obj(1,2) d15d15 by(2) d79d79d79d79 d39d39d79d79d79d79 X Y X Y absorbing X(1,2) Y(1,2) absorbing(2) Figure 2: Two parse trees and their compact representation (sentence sets are shown in parentheses).
181:298	4.1 Experimental Setting The test set for human evaluation was generated by picking out 53 random verbs from the 1000 most frequent ones found in a subset of the Reuters corpus2.
182:298	For each verb entry in the lexicon, we provided the judges with the corresponding pivot template and the list of related candidate entailment templates found by the system.
183:298	The judges were asked to evaluate entailment for a total of 752 templates, extracted for 53 pivot lexicon entries; Table 1 shows a sample of the evaluated templates; all of them are clearly good and were judged as correct ones.
184:298	Pivot Template Entailment Templates X prevent Y X provides protection against Y X reduces Y X decreases the risk of Y X be cure for Y X a day keeps Y away X to combat Y X accuse Y X call Y indictable X testifies against Y Y defense before X X acquire Y X snap up Y Y shareholders approve X buyout Y shareholders receive shares of X stock X go back to Y Y allowed X to return Table 1: Sample of templates found by TE/ASE and included in the evaluation test set.
185:298	Concerning the ASE algorithm, threshold parameters3 were set as PHRASEMAXF=107, SETMINF=102, SETMAXF=105, SETMINP=0.066, and SETMAXP=0.666.
186:298	An upper limit of 30 was imposed on the number of possible anchor sets used for each pivot.
187:298	Since this last value turned out to be very conservative with respect to system cover2Known as Reuters Corpus, Volume 1, English Language, 1996-08-20 to 1997-08-19.
188:298	3All parameters were tuned on a disjoint development lexicon before the actual experiment.
189:298	age, we subsequently attempted to relax it to 50 (see Discussion in Section 4.3).
190:298	Further post-processing was necessary over extracted data in order to remove syntactic variations referring to the same candidate template (typically passive/active variations).
191:298	Three possible judgment categories have been considered: Correct if an entailment relationship in at least one direction holds between the judged template and the pivot template in some non-bizarre context; Incorrect if there is no reasonable context and variable instantiation in which entailment holds; No Evaluation if the judge cannot come to a definite conclusion.
192:298	4.2 Results Each of the three assessors (referred to as J#1, J#2, and J#3) issued judgments for the 752 different templates.
193:298	Correct templates resulted to be 283, 313, and 295 with respect to the three judges.
194:298	No evaluations were 2, 0, and 16, while the remaining templates were judged Incorrect.
195:298	For each verb, we calculate Yield as the absolute number of Correct templates found and Precision as the percentage of good templates out of all extracted templates.
196:298	Obtained Precision is 44.15%, averaged over the 53 verbs and the 3 judges.
197:298	Considering Low Majority on judges, the precision value is 42.39%.
198:298	Average Yield was 5.5 templates per verb.
199:298	These figures may be compared (informally, as data is incomparable) with average yield of 10.1 and average precision of 50.3% for the 9 pivot templates of (Lin and Pantel, 2001).
200:298	The comparison suggests that it is possible to obtain from the (very noisy) web a similar range of precision as was obtained from a clean news corpus.
201:298	It also indicates that there is potential for acquiring additional templates per pivot, which would require further research on broadening efficiently the search for additional web data per pivot.
202:298	Agreement among judges is measured by the Kappa value, which is 0.55 between J#1 and J#2, 0.57 between J#2 and J#3, and 0.63 between J#1 and J#3.
203:298	Such Kappa values correspond to moderate agreement for the first two pairs and substantial agreement for the third one.
204:298	In general, unanimous agreement among all of the three judges has been reported on 519 out of 752 templates, which corresponds to 69%.
205:298	4.3 Discussion Our algorithm obtained encouraging results, extracting a considerable amount of interesting templates and showing inherent capability of discovering complex semantic relations.
206:298	Concerning overall coverage, we managed to find correct templates for 86% of the verbs (46 out of 53).
207:298	Nonetheless, presented results show a substantial margin of possible improvement.
208:298	In fact yield values (5.5 Low Majority, up to 24 in best cases), which are our first concern, are inherently dependent on the breadth of Web search performed by the ASE algorithm.
209:298	Due to computational time, the maximal number of anchor sets processed for each verb was held back to 30, significantly reducing the amount of retrieved data.
210:298	In order to further investigate ASE potential, we subsequently performed some extended experiment trials raising the number of anchor sets per pivot to 50.
211:298	This time we randomly chose a subset of 10 verbs out of the less frequent ones in the original main experiment.
212:298	Results for these verbs in the main experiment were an average Yield of 3 and an average Precision of 45.19%.
213:298	In contrast, the extended experiments on these verbs achieved a 6.5 Yield and 59.95% Precision (average values).
214:298	These results are indeed promising, and the substantial growth in Yield clearly indicates that the TE/ASE algorithms can be further improved.
215:298	We thus suggest that the feasibility of our approach displays the inherent scalability of the TE/ASE process, and its potential to acquire a large entailment relation KB using a full scale lexicon.
216:298	A further improvement direction relates to template ranking and filtering.
217:298	While in this paper we considered anchor sets to have equal weights, we are also carrying out experiments with weights based on cross-correlation between anchor sets.
218:298	5 Conclusions We have described a scalable Web-based approach for entailment relation acquisition which requires only a standard phrasal lexicon as input.
219:298	This minimal level of input is much simpler than required by earlier web-based approaches, while succeeding to maintain good performance.
220:298	This result shows that it is possible to identify useful anchor sets in a fully unsupervised manner.
221:298	The acquired templates demonstrate a broad range of semantic relations varying from synonymy to more complicated entailment.
222:298	These templates go beyond trivial paraphrases, demonstrating the generality and viability of the presented approach.
223:298	From our current experiments we can expect to learn about 5 relations per lexicon entry, at least for the more frequent entries.
224:298	Moreover, looking at the extended test, we can extrapolate a notably larger yield by broadening the search space.
225:298	Together with the fact that we expect to find entailment relations for about 85% of a lexicon, it is a significant step towards scalability, indicating that we will be able to extract a large scale KB for a large scale lexicon.
226:298	In future work we aim to improve the yield by increasing the size of the sample-corpus in a qualitative way, as well as precision, using statistical methods such as supervised learning for better anchor set identification and cross-correlation between different pivots.
227:298	We also plan to support noun phrases as input, in addition to verb phrases.
228:298	Finally, we would like to extend the learning task to discover the correct entailment direction between acquired templates, completing the knowledge required by practical applications.
229:298	Like (Lin and Pantel, 2001), learning the context for which entailment relations are valid is beyond the scope of this paper.
230:298	As stated, we learn entailment relations holding for some, but not necessarily all, contexts.
231:298	In future work we also plan to find the valid contexts for entailment relations.
232:298	Acknowledgements The authors would like to thank Oren Glickman (Bar Ilan University) for helpful discussions and assistance in the evaluation, Bernardo Magnini for his scientific supervision at ITC-irst, Alessandro Vallin and Danilo Giampiccolo (ITC-irst) for their help in developing the human based evaluation, and Prof. Yossi Matias (Tel-Aviv University) for supervising the first author.
233:298	This work was partially supported by the MOREWEB project, financed by Provincia Autonoma di Trento.
234:298	It was also partly carried out within the framework of the ITC-IRST (TRENTO, ITALY)  UNIVERSITY OF HAIFA (ISRAEL) collaboration project.
235:298	For data visualization and analysis the authors intensively used the CLARK system (www.bultreebank.org) developed at the Bulgarian Academy of Sciences.
236:298	References Regina Barzilay and Lillian Lee.
237:298	2003.
238:298	Learning to paraphrase: An unsupervised approach using multiple-sequence alignment.
239:298	In Proceedings of HLT-NAACL 2003, pages 1623, Edmonton, Canada.
240:298	Regina Barzilay and Kathleen R. McKeown.
241:298	2001.
242:298	Extracting paraphrases from a parallel corpus.
243:298	In Proceedings of ACL 2001, pages 5057, Toulose, France.
244:298	Ido Dagan and Oren Glickman.
245:298	2004.
246:298	Probabilistic textual entailment: Generic applied modeling of language variability.
247:298	In PASCAL Workshop on Learning Methods for Text Understanding and Mining, Grenoble.
248:298	Florence Duclaye, Francois Yvon, and Olivier Collin.
249:298	2002.
250:298	Using the Web as a linguistic resource for learning reformulations automatically.
251:298	In Proceedings of LREC 2002, pages 390396, Las Palmas, Spain.
252:298	Oren Glickman and Ido Dagan.
253:298	2003.
254:298	Identifying lexical paraphrases from a single corpus: a case study for verbs.
255:298	In Proceedings of RANLP 2003.
256:298	Ulf Hermjakob, Abdessamad Echihabi, and Daniel Marcu.
257:298	2003.
258:298	Natural language based reformulation resource and Web Exploitation.
259:298	In Ellen M. Voorhees and Lori P. Buckland, editors, Proceedings of the 11th Text Retrieval Conference (TREC 2002), Gaithersburg, MD. NIST.
260:298	Christian Jacquemin.
261:298	1999.
262:298	Syntagmatic and paradigmatic representations of term variation.
263:298	In Proceedings of ACL 1999, pages 341348.
264:298	Dekang Lin and Patrick Pantel.
265:298	2001.
266:298	Discovery of inference rules for Question Answering.
267:298	Natural Language Engineering, 7(4):343360.
268:298	Dekang Lin.
269:298	1998.
270:298	Dependency-based evaluation of MINIPAR.
271:298	In Proceedings of the Workshop on Evaluation of Parsing Systems at LREC 1998, Granada, Spain.
272:298	Dan Moldovan and Vasile Rus.
273:298	2001.
274:298	Logic form transformation of WordNet and its applicability to Question Answering.
275:298	In Proceedings of ACL 2001, pages 394401, Toulose, France.
276:298	Bo Pang, Kevin Knight, and Daniel Marcu.
277:298	2003.
278:298	Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences.
279:298	In Proceedings of HLT-NAACL 2003, Edmonton, Canada.
280:298	Deepak Ravichandran and Eduard Hovy.
281:298	2002.
282:298	Learning surface text patterns for a Question Answering system.
283:298	In Proceedings of ACL 2002, Philadelphia, PA. Ellen Riloff and Rosie Jones.
284:298	1999.
285:298	Learning dictionaries for Information Extraction by multilevel bootstrapping.
286:298	In Proceedings of the Sixteenth National Conference on Artificial Intelligence (AAAI-99), pages 474479.
287:298	Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and Ralph Grishman.
288:298	2002.
289:298	Automatic paraphrase acquisition from news articles.
290:298	In Proceedings of Human Language Technology Conference (HLT 2002), San Diego, USA.
291:298	Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
292:298	2003.
293:298	An improved extraction pattern representation model for automatic IE pattern acquisition.
294:298	In Proceedings of ACL 2003.
295:298	Roman Yangarber, Ralph Grishman, Pasi Tapanainen, and Silja Huttunen.
296:298	2000.
297:298	Unsupervised discovery of scenario-level patterns for Information Extraction.
298:298	In Proceedings of COLING 2000 .

