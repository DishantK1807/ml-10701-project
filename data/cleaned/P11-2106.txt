Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 603–608,
Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
Semantic Information and Derivation Rules for Robust Dialogue Act
Detection in a Spoken Dialogue System
Wei-Bin Liang1 Chung-Hsien Wu2
Department of Computer Science and
Information Engineering
National Cheng Kung University
Tainan, Taiwan
1liangnet@gmail.com
2chunghsienwu@gmail.com
Chia-Ping Chen
Department of Computer Science
and Engineering
National Sun Yat-sen University
Kaohsiung, Taiwan
cpchen@mail.cse.nsysu.edu.tw
Abstract
In this study, a novel approach to robust di-
alogue act detection for error-prone speech
recognition in a spoken dialogue system is
proposed. First, partial sentence trees are pro-
posed to represent a speech recognition out-
put sentence. Semantic information and the
derivation rules of the partial sentence trees
are extracted and used to model the relation-
ship between the dialogue acts and the deriva-
tion rules. The constructed model is then used
to generate a semantic score for dialogue act
detection given an input speech utterance. The
proposed approach is implemented and evalu-
ated in a Mandarin spoken dialogue system for
tour-guiding service. Combined with scores
derived from the ASR recognition probabil-
ity and the dialogue history, the proposed ap-
proach achieves 84.3% detection accuracy, an
absolute improvement of 34.7% over the base-
line of the semantic slot-based method with
49.6% detection accuracy.
1 Introduction
An intuitive framework for spoken dialogue system
(SDS) can be regarded as a chain process. Specifi-
cally, the automatic speech recognition (ASR) mod-
ule accepts the user’s utterance nullt and returns a
string of words Wt The spoken language under-
standing (SLU) module converts Wt to an abstract
representation of the user’s dialogue act (DA). The
dialogue management (DM) module determines the
user’s dialogue act null∗t and accordingly decides the
current act of the system. The system DA is con-
verted to a surface representation by natural lan-
Figure 1: Details of the SLU and DM modules.
guage generation in the textual form, which is
passed to a text-to-speech synthesizer for speech
waveform generation. The cycle repeats when the
user responds with a new utterance. Clearly, one can
see that the inference of the user’s overall intention
via DA detection is an important task in SDS.
Figure 1 depicts the training and test phases of
the SLU module and the DM module in our system.
The dataflow for training and testing are indicated
by blue arrows and red arrows, respectively. The
input word sequences are converted to partial sen-
tence trees (PST) (Wu and Chen, 2004) in the PST
Construction block. The derivation rule (DR) Gen-
eration block extracts derivation rules from the train-
ing text. The DR-DA matrix is created after cluster-
ing the sentences into different dialogue acts (DAs),
counting the occurrences the DRs in DA, and intro-
ducing an entropy-based weighting scheme (Belle-
garda, 2000). This matrix is pivotal in the computa-
tion of the lexical score. Finally, the lexical, the his-
tory, and the ASR scores are combined to decide the
603
optimal dialogue act, and a proper action by the sys-
tem is taken. In our system, not only the clean text
data but also the noisy ASR output data are used in
order to take the error-proneness of ASR output into
account. Furthermore, a predefined keyword list is
used and the keyword tokens are replaced by the cor-
responding named entity classes (NEC) in order to
obtain a compact feature set.
2 Models
for Dialogue Act Detection
Referring to the SDS depicted in Figure 1, the DA
detection can be formulated as follows. At turn null,
the most likely DA is determined by
null∗t = argmax
A∈Ω
nullnull(null|nulltnullnullt)null (1)
where nullt is the user’s utterance, nullt is the dialogue
historical information, and Ω = {null1nullnullnullnullnullnullq} is the
set of DAs. Using the maximum approximation for
summation, (1) can be written as
null∗t = argmax
A∈Ω
summationdisplay
W
nullnull(nullnullW|nulltnullnullt)
≈ argmax
A∈Ω
max
W
nullnull(nullnullW|nulltnullnullt)
= argmax
A∈Ω,W
nullnull(W|nulltnullnullt)nullnull(null|Wnullnulltnullnullt)null
(2)
where W is the ASR output. Since the ASR output
is independent of nullt given nullt, the ASR-related first
term in (2) can be re-written as
nullnull(W|nulltnullnullt) = nullnull(W|nullt) ∝ null(Wnullnullt)null (3)
where the function null(Wnullnullt) is introduced as the
ASR score function. In addition, assuming that the
information provided by nullt is completely conveyed
in W, we can approximate the second term in (3) by
the product of two functions
nullnull(null|Wnullnulltnullnullt) = nullnull(null|Wnullnullt)
∝ null(nullnullW) null(nullnullnullt)null (4)
where null(nullnullW) is introduced as the lexical score
function, and null(nullnullnullt) is introduced as the history
score function. Thus, (3) can be re-written as
null∗t ≈ argmax
A∈Ω,W
null(Wnullnullt) null(nullnullW) null(nullnullnullt)null (5)
In Sections 3 and 4, we specify and explain how the
scores in (5) are computed.
Figure 2: An example of a dialogue management mod-
ule using null-gram model for dialogue act sequence in the
domain of historic spot.
3 ASR
Score and History Score
For the ASR score, we use the conventional recog-
nition probability of the ASR recognition model.
For the history score, similar to the schemes used
in (Hori et al., 2009c; Hori et al., 2009b; Hori et al.,
2009a), a back-off bi-gram model for DA sequence
is estimated from the data collected by the SDS. The
estimated bi-gram model is used to calculate the his-
tory score. That is,
null(nullnullnullt) = nullnull(nullt = null | nullt−1)null (6)
Essentially, (6) is based on a Markov model assump-
tion for the chain of the dialogue acts. Figure 2
shows an example of dialogue controlling model of
an SDS. In this example, each state represents a DA.
A dialogue begins with the greeting state and ends
with the ending state. During a session, a user can
inquire the system about the provided services and
then choose one service to continue (e.g., the loop-
back connection in Figure 2).
4 The
Lexical Score Function
The main challenge of this system is the computa-
tion of the lexical score null(nullnullW). In this paper, we
propose a novel data-driven scheme incorporating
many techniques.
4.1 Construction
of Partial Sentence Tree
In an SDS, it is often beneficial to define a set of
keywords K, and a set of non-keywords N. Each
word null ∈ K should be indicative of the DA of
the sentence. The set of sentences S containing
at least one keyword in K, can be represented as
S = N∗ (K N∗)+, where K+ means a string of one
or more words in K. Given a sentence null ∈ S, a par-
tial sentence is formed by keeping all the keywords
in null and some of the non-keywords in null. These
604
Figure 3: Construction of the partial sentence tree for the
sentence Where is the Anping-Fort.
partial sentences can be compiled in a tree, called
the partial sentence tree (PST) and denoted as T (null).
The motivation for using PST is to achieve robust
DA detection as the ASR module could be error-
prone in adverse environments. In addition, words
that are not confidently recognized are replaced by
a special non-keyword token called Filler. Specif-
ically, we compute the null-score (Larsen and Marx,
2000) of each word null in the ASR output. Figure 3
illustrates the PST for the sentencenull: Where is the
Anping-Fort. There are two keywords Where and
Anping-Fort and two non-keywords is and the. Note
that with 2 non-keywords in the original sentence null,
we have 22 = 4 partial sentences in the PST T (null).
4.2 Extraction
of the Derivation Rules
After text processing, a sentence null is parsed by the
statistical Stanford parser (S-parser) (Levy and Man-
ning, 2003). Let the grammar of the S-parser be
denoted as a 5-tuple null = (VnullΣnullPnullnullnullnull) where
V is the variable (non-terminal) set,Σ is the termi-
nal symbol set,P is the production rule set, null is the
sentence symbol, and null is a function defined on P
for rule probability (Jurafsky and Martin, 2009). A
derivation rule is defined to be a derivation of the
form null → null → null where nullnullnull ∈ V and null ∈ Σ.
The parsing result of the exemplar sentence null repre-
sented in the parenthesized expression is shown in
Figure 4. From the parsing result, four DRs are ex-
tracted. Essentially, we have one DR for each lexical
word in the sentence. Totally, given a corpus, null rules
are extracted and defined as D = {null1nullnull2nullnullnullnullnullnulll}.
Based on PST T (null) and DR set D, a vector rep-
resentation null(null) for sentence null can be constructed
according to the DRs used in T (null). That is
nulli(null) =
braceleftBigg
1null if nulli ∈ T (null)
0null otherwise (7)
Parse Result Derivation Rule
(Root DR1: WHADVP (WRB Where)
(SINV DR2: VP (VBZ is)
(FRAG DR3: NP (DT the)
(WHADVP (WRB Where))) DR4: NP (NNP Anping-Fort)
(VP (VBZ is))
(NP (DT the) (NNP Anping-Fort))))
Figure 4: The parse result (left) and the extracted deriva-
tion rules (right) for the exemplar sentence null.
For example, null(null) = [1010]T means that there are
four derivation rules, of which null1 and null3 are used
in T (null). The motivation for using DRs instead of
the lexical words is to incorporate the part-of-speech
(POS) tags information. POS tags are helpful in
the disambiguation of noun-and-verb homonyms in
Chinese. Moreover, the probabilistic nature of the
S-parser renders the DRs extracted from the pars-
ing results quite robust and consistent, even for the
error-prone ASR output sentences.
4.3 Generation
of Dialogue Acts
The basic idea of data-driven DA is to cluster sen-
tences in the set and identify the clusters as formed
by the sentences of the same DA. In this work, the
spectral clustering algorithm (von Luxburg, 2007) is
employed for sentence clustering. Specifically, sup-
pose we have n vectors represented as C = {nullk defines
null(nullk)null null = 1nullnullnullnullnullnull} converted from sentences ac-
cording to (7). From C, we construct an null×null sim-
ilarity matrix null, in which each element nullkknull is
a symmetric nonnegative distance measure between
nullk and nullknull. In this work, we use the cosine measure.
The matrix null can be regarded as the adjacency ma-
trix of a graph null with node set N and edge set E,
where N is 1-to-1 correspondent to the set C, and E
corresponds to the non-zero entries in null. The nor-
malized Laplacian matrix of null is
null defines null −null−12nullnull−12null (8)
where null is a diagonal matrix with entries
nullkknull = nullkknull
nsummationdisplay
j=1
nullkjnull (9)
It has been shown (von Luxburg, 2007) that the mul-
tiplicity of the eigenvalue 0 for null equals the num-
ber of disjoint connected components in null. In our
implementation, we find the null eigenvectors of the
normalized Laplacian matrix of null of the smallest
605
eigenvalues. We put these eigenvectors in an null × null
orthogonal matrix null, and cluster the row vectors to
null clusters. Each cluster correspond to a data-driven
DA nullj , and the null sentences are classified according
to the cluster they belong to.
In order to use the DRs in a PST as a knowl-
edge source for DA detection, we essentially need to
model the relationship between the random DA and
the random DR. Denote the random DA by null and
the random DR by null . Given a text corpus, let nullij be
the accumulated count that nulli occurs in a sentence
labeled as nullj. From nullij, the conditional probability
of null = nullj given null = nulli can be defined as
nullij = ˆnull(null = nullj|null = nulli) defines nullijsummationtextq
jnull=1 nullijnull
null (10)
where null = 1nullnullnullnullnullnull. The normalized entropy for the
conditional probability function (10) is
nulli = − 1lognull
qsummationdisplay
j=1
nullij lognullijnull (11)
From (10) and (11), a matrix Φ can be constructed
by Φij = (1 − nulli)nullij. We call Φ the derivation-
rule dialogue-act (DR-DA) matrix, in which each
row corresponds to a derivation rule and each col-
umn corresponds to a dialogue act.
4.4 Distance
Measure
In our system, the lexical score null(nullnullW) in (5) is
further broken into two terms
null(nullnullW) ≈ nullR(nullnullnull)nullN(nullnullW) (12)
where nullR(nullnullnull) is called the DR score and
nullN(nullnullW) is called the named entity score. Note
that null denotes the sentence after text processing. The
cosine distance measure is employed for the deriva-
tion rule score,
nullR(null = nulljnullnull) = max
σ∈T (s)
bTσaj
|bσ||aj| (13)
where bTσ is the vector representation (using the co-
ordinates of the DRs) of a partial sentence null in T (null),
andaj is the nullth column vector in the DR-DA matrix
Φ. For the named entity score, we use the approxi-
mation
nullN(nullnullW) =
productdisplay
k
null(nullnullnullk) (14)
NEC/SC Name entities/Words
City Tainan, Taipei, Kaohsiung
Spot Anping-Fort, Sun-Moon Lake
Greeting Welcome, Hello
Ending Thanks, Bye
Table 1: Examples of named entity classes (NEC) and
semantic classes (SC)
where nullk is the nullth named entity in W. Note that
null(nullnullnull) is estimated from a training corpus by rela-
tive frequencies.
5 Experiments
and Discussion
To evaluate the proposed method of dialogue act de-
tection for robust spoken dialogue system, we adopt
the commonly-used Wizard-of-Oz approach (Fraser
and Gilbert, 1991) to harvest the Tainan-city tour-
guiding dialogue corpus in a lab environment and
experiment with simulated noisy ASR results. The
details are given in this section. Two types of data
from different sources are collected for this work.
The first type of data, called A-data, is a travel infor-
mation data set harvested from the databases avail-
able on the web, e.g., Wikipedia and Google Map.
A-data consists of 1null603 sentences with 317 word
types. The second type of data, called Q-data, is the
edited transcription of a speech data set simulating
human-computer dialogues in a lab environment. Q-
data is intended for the system to learn to handle the
various situations, e.g., misunderstanding the user’s
intention. It consists of 144 dialogues with 1null586 ut-
terances. From the Q-data, 28 named entity classes
and 796 derivation rules were obtained from the S-
parser. Table 1 gives some examples of the selected
NECs and semantic classes.
5.1 Experimental
Conditions
A Mandarin speech recognition engine was real-
ized using the HTK (Young et al., 2006), which is
commonly used in research and development. For
speech features, 39 dimensions were used, includ-
ing 12 dimensions of mel-frequency cepstral coeffi-
cients (MFCCs), one dimension of log energy, and
their delta and acceleration features. In total, the
acoustic models are composed of 153 subsyllable
and 37 particle models (e.g., EN, MA, OU) based
606
number of DA types 37 38 39
detection accuracy 82.7 84.3 77.2
Table 2: Detection accuracies with varying numbers of
DA types.
on Hidden Markov Model (HMM) with 32 Gaus-
sian mixture components per state. For the lan-
guage model, SRILM toolkit (Stolcke, 2002) was
employed to estimate a bi-gram model with the Q-
data. The average word accuracy of the ASR module
is 86.1% with a lexicon of 297 words. Note that the
vocabulary size is small due to a limited domain. 5-
fold cross validation method was utilized for system
evaluation.
As shown in Table 2, one can see that 38 DA types
achieve the best performance for the proposed detec-
tion model. Therefore, we use 38 DA types (null = 38)
in our system. Note that some exemplar DAs are
shown in Figure 2.
5.2 Incremental
Evaluation
We incrementally add techniques in our SDS un-
til the complete proposed overall system is imple-
mented, to observe the effect of these techniques.
The detection accuracies are shown in Table 3. In
this table, the third column (ASR) represents the re-
sults of the experiment using the ASR transcripts
directly. The fourth column (REF) uses the refer-
ence transcripts, so it represents the case with per-
fect ASR. The first (40%-sim) and second (60%-
sim) column represents the simulation where 40%
and 60% of the words in the reference transcripts
are retained, respectively. There are five sets of ex-
periments summarized in this table. For the base-
line, each keyword corresponds to a coordinate in
the vector representation for a sentence. The results
are shown in the first row (baseline). In the second
set of experiments (NEC), the keywords are replaced
by their NEC. In the third set of experiments (PST),
the PST representation for a sentence is used. In
the fourth set of experiments (DR), the derivation
rule representation of a sentence is used. Finally, the
entropy-normalized DR-DA matrix is used to repre-
sent sentences, and the results are shown in the last
row (DR-DA). There are strong improvements when
NEC (from 49.6% to 56.8%) and PST (from 56.8%
to 76.2%) representations are introduced. Moreover,
40%-sim 60%-sim ASR REF
baseline 17.2 32.6 49.6 60.9
NEC 22.4 36.8 56.8 76.9
PST 29.8 49.2 76.2 91.1
DR 26.3 48.0 81.6 92.1
DR-DA 26.3 47.4 82.9 93.3
Table 3: Detection accuracies of cascading components
for the lexical score.
value of nullL 0.5 0.6 0.7 0.8
Accuracy (%) 84.3 84.6 85.1 84.9
Table 4: Evaluation on different weighted product fusion
the DR and DR-DA representations also lead to sig-
nificant improvements, achieving 81.6% to 82.9%,
respectively. For the other conditions of 40%-sim,
60%-sim, and REF, similar improvements of using
NEC and PST are observed. Using DR-DA, how-
ever, suffers from performance degradation when
the keywords are randomly discarded.
5.3 Evaluation
on the Weighting Scheme
We examine the effect of different weighted product
fusion and rewrite the formulation in (5) as
null∗t ≈ argmax
A∈Ω,W
[null(Wnullnullt)null(nullnullW)]λA [null(nullnullnullt)]λL
(15)
where nullA is the weight for the ASR score and the
lexical score, nullL is the weight of the history score,
and nullA + nullL = 1. Table 4 shows the results that
history information will effect on the DA detection,
because it was estimated by the dialogue turns that
captured the user behaviors.
6 Conclusions
In this paper, a noise-robust dialogue act detection
using named entity classes, partial sentence trees,
derivation rules, and entropy-based dialogue act-
derivation rule matrix is investigated. Data-driven
dialogue acts are created by the spectral cluster-
ing algorithm, which is applied on the vectors of
sentences represented by the derivation rules. Our
spoken dialogue system benefits when the proposed
components are integrated incrementally. For the
fully integrated system, we find that the proposed
approach achieves 84.3% detection accuracy.
607
References
J. Bellegarda. 2000. Exploiting latent semantic informa-
tion in statistical language modeling. Proceedings of
the IEEE, 88:1279–1296.
N. Fraser and G. N. Gilbert. 1991. Simulating speech
systems. Computer Speech and Language, 5(1):81–
99.
C. Hori, K. Ohtake, T. Misu, H. Kashioka, and S. Naka-
mura. 2009a. Recent advances in wfst-based dialog
system. In Proc. INTERSPEECH, pages 268–271.
C. Hori, K. Ohtake, T. Misu, H. Kashioka, and S. Naka-
mura. 2009b. Statistical dialog management applied
to wfst-based dialog systems. In Proc. IEEE Inter-
national Conference on Acoustics Speech and Signal
Processing (ICASSP), pages 4793–4796.
C. Hori, K. Ohtake, T. Misu, H. Kashioka, and S. Naka-
mura. 2009c. Weighted finite state transducer based
statistical dialog management. In Proc. ASRU.
D. Jurafsky and J. H. Martin. 2009. Speech and Lan-
guage Processing, 2nd Edition. Pearson Education.
R. J. Larsen and M. L. Marx. 2000. An Introduction to
Mathematical Statistics and Its Applications, 3rd Edi-
tion. ISBN: 0139223037.
R. Levy and C. Manning. 2003. Is it harder to parse
chinese, or the chinese treebank? In Proc. Annual
Meeting of ACL, pages 439–446.
A. Stolcke. 2002. Srilm an extensible language model-
ing toolkit. In Proc. International Conference on Spo-
ken Language Processing, pages 901–904.
U. von Luxburg. 2007. A tutorial on spectral clustering.
Statistics and Computing, 17(4).
C.-H. Wu and Y.-J. Chen. 2004. Recovery from
false rejection using statistical partial pattern trees for
sentence verification. Speech Communication, 43(1-
2):71–88.
Steve J. Young, D. Kershaw, J. Odell, D. Ollason,
V. Valtchev, and P. Woodland. 2006. The HTK Book
Version 3.4. Cambridge University Press.
608

