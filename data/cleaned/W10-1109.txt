Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 61–67,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics
Extracting Medication Information from Discharge Sumaries 
 
Scott Halgrim, Fei Xia, Imre Solti, Eithon Cadag Özlem Uzuner 
University of Washington University of Albany, SUNY 
PO Box 543450 135 Western Ave 
Seatle, WA 98195, USA Albany, NY 122, USA   
{captnpi,fxia,solti,ecadag}@uw.edu ouzuner@uamail.albany.edu 
 
 
 
 
 
Abstract 
Extracting medication information from 
clinical records has many potential apli-
cations and was the focus of the i2b2 
chalenge in 209. We present a hybrid 
system, comprised of machine learning 
and rule-based modules, for medication 
information extraction. With only a hand-
ful of template-filling rules, the system’s 
core is a cascade of statistical clasifiers 
for field detection. It achieved good per-
formance that was comparable to the top 
systems in the i2b2 chalenge, demon-
strating that a heavily statistical ap-
proach can perform as wel or beter than 
systems with many sophisticated rules. 
The system can easily incorporate adi-
tional resources such as medication name 
lists to further improve performance.  
1 Introduction
  
Narative clinical records store patient medical 
information, and extracting this information from 
these naratives suports data management and 
enables many applications (Levin et al., 207). 
Informatics for Integrating the Biology and the 
Bedside (i2b2) is an NIH-funded National Center 
for Biomedical Computing based at Partners 
HealthCare System, and it has organized annual 
NLP shared tasks and chalenges sic 206 
(https://ww.i2b2.org/). The Third i2b2 Workshop 
on NLP Chalenges for Clinical Records in 209 
studied the extraction of medication information 
from hp dcharge sumaries 
(https:/ww.i2b2.org/NLP/Medication/), a task 
we refer to as the i2b2 challenge in this paper.  
    In the past decade, there has been extensive re-
search on information extraction in both the gen-
eral and biomedical domains (Welner et al., 204; 
Grenager et al., 205; Pon and Domingos, 2007; 
Meystre et al, 208; Rozenfeld and Feldman, 
2008). Interestingly, despite the recent prevalence 
of statistical approaches in most NLP tasks (in-
cluding information extraction), most of the sys-
tems developed for the i2b2 challenge were rule-
based. In this paper we present our hybrid system, 
whose core is a cascade of statistical clasifiers that 
identify medication fields such as medication 
names and dosages. The fields are then asembled 
to form medication entries. While our system did 
not participate in the i2b2 challenge (as we wer 
part of the organizing team), it achieved god re-
sults that matched the top i2b2 systems.  
2 The
i2b2 Challenge 
This section provides a brief introduction to the 
i2b2 challenge. 
2.1 The
task 
The i2b2 chalenge studied the automatic extrac-
tion of information coresponding to the following 
fields from hospital discharge sumaries (Uzuner, 
et al., 2010a): names of medications (m) taken by 
the patient, dosages (do), modes (mo), frequencies 
(f), durations (du), and reasons (r) for taking these 
medications. We refer to the medication field as 
the name field and the other five fields as the non-
name fields. Al non-name fields correspond to 
some name field mention; if they were specified 
within a two-line window of that name mention, 
61
the i2b2 challenge required such fields to be linked 
to the name field to form an entry. For each entry, 
a system must determine whether the entry ap-
peared in a list of medications or in narative text. 
Table 1 shows an excerpt from a discharge sum-
mary and the coresponding entries in the gold 
standard. The first entry apears in narative text, 
and the second in a list of medication information. 
 
Excerpt of Discharge Sumary 
55   the patient noted that he had a recurence of this 
56  vague chest discomfort as he was siting and  
57  talking to friends. He took a sublingual 
58  Nitroglycerin without relief. 
... 
65  Flomax ( Tamsulosin  ) 0.4 mg, po, qd,.. 
Gold standard: 
m=“Nitroglycerin” 58:0 58:0 |do=“nm”|   
   mo=“sublingual” 57:6 57:6 ||f=“nm” ||du=“nm” ||   
   r=“vague chest discomfort” 56:0 56:2 || 
   ln=”narative” 
... 
m="flomax ( tamsulosin )" 65:0 65:3|do="0.4 mg" 
   65:4 65:5|mo="po" 65:6 65:6|f="qd" 65:7 
   65:7|du="nm"|r="nm"|ln="list" 
 
Table 1: A sample discharge sumary excerpt and 
the coresponding entries in the gold standard. The 
fields inside an entry are separated by “|”. Each 
field is represented by the string and its position 
(i.e., “line number: token number” ofsets). “nm” 
means the field value is not mentioned for this me-
dication name.  
2.2 Data
Sets 
The i2b2 chalenge used a total of 1243 discharge 
sumaries: 
• 696 of these summaries were released to par-
ticipants for system development, and the i2b2 
organizing team provided the gold standard 
annotation for 17 of them.  
 
• Participating teams could chose to anotate 
more files themselves. The University of Syd-
ney team annotated 145 out of the 696 suma-
ries (including re-annotating 14 of the 17 files 
annotated by the i2b2 organizing team) and 
generously shared their annotations with i2b2 
after the challenge for future research. We ob-
tained and used 110 of their anotations as our 
training set and the remaining 35 sumaries as 
our development set. 
 
• The participating teams produced system out-
puts for 547 discharge summaries set aside for 
testing. After the challenge, 251 of these sum-
maries were anotated by the challenge par-
ticipants, and these 251 sumaries formed the 
final test set (Uzuner et al., 2010b). 
   The sizes of the data sets used in our experiments 
are shown in Table 2. The training and develop-
ment sets were created by the University of Syd-
ney, and the test data is the i2b2 official challenge 
test set. The average number of entries and fields 
vary among the thre sets because the sumaries 
in the test set were chosen randomly from the 547 
sumaries, whereas the University of Sydney team 
annotated the longest sumaries. 
2.3 Aditional
resources 
Besides the training data, the participating teams 
were alowed to use any aditional tols and re-
sources that they had acess to, including resources 
not available to the public. Al chalenge partici-
pants used additional resources such as UMLS 
(www.nlm.nih.gov/research/umls/), but the exact 
resources used varied from team to team. There-
fore, the chalenge was similar to the so-called 
open-track chalenge in the general NLP field, as 
opposed to a closed-track chalenge that could re-
quire that al the participants use only the list of 
resources specified by the chalenge organizers 
 
2.4 Evaluation
metrics 
 
The i2b2 chalenge used two sets of evaluation 
metrics: horizontal and vertical metrics. Horizon-
tal metrics measured system performance at the 
entry level, whereas vertical metrics measured sys-
tem performance at the field level. Both sets of 
metrics compared the system output and the gold 
standard at the span level for exact match and at 
the token level for inexact match, using precision, 
recal, and F-score (Uzuner et al., 2010a). The pri-
mary metric for the chalenge is exact horizontal F-
score, which is the metric we use to evaluate our 
system.  
 
 
 
 
62
 
Table 2: The data sets used in our experiments. The numbers in parentheses are the average numbers 
of entries or fields per discharge sumary.  
 
2.5 Participating
systems 
Twenty teams participated in the chalenge. Fiften 
teams used rule-based approaches, and the rest 
used statistical or hybrid approaches. The perform-
ances of the top five systems are shown in Table 3. 
Among them, only the top system, developed by 
the University of Sydney, used a hybrid aproach, 
whereas the rest were rule-based. 
 
Rank Precision Recal F-score 
1 89.6 82.0 85.7 
2 84.0 80.3 82.1 
3 86.4 76.6 81.2 
4 78.4 82.3 80.3 
5 84.1 75.8 79.7 
Table 3: The performance (exact horizontal preci-
sion/recal/F-score) of the top five i2b2 systems on 
the test set.  
3 System
description   
We developed a hybrid system with thre process-
ing steps: (1) a preprocesing step that finds sec-
tion boundaries, (2) a field detection step that 
identifies the six fields, and (3) a field linking step 
that links fields together to form entries. The sec-
ond step is a statistical system, whereas the other 
two steps are rule-based. The second step was the 
main focus of this study. 
3.1 Preprocessing

In adition to comon procesing steps such as 
part-of-spech (POS) taging, our preprocessing 
step includes a section segmenter that breaks dis-
charge sumaries into sections. Discharge suma-
ries tend to consist of sections such as ‘ADMIT 
DIAGNOSIS’, ‘PAST MEDICAL HISTORY’, 
and ‘DISCHARGE MEDICATIONS’. Knowing 
section boundaries is important for the i2b2 chal-
lenge because, acording to the anotation guide-
lines for creating the gold standard, medications 
occurring under certain sections (e.g., family his-
tory and allergic reaction) should be excluded from 
the system output. Furthermore, knowing the types 
of sections could be useful for field detection and 
field linking; for example, entries in the 
‘DISCHARGE MEDICATIONS’ section are more 
likely to appear in a list of medications than in nar-
rative text. 
The set of sections and the exact speling of 
section headings vary across discharge sumaries. 
The section segmenter uses regular expressions 
(e.g., ‘^\s*([A-Z\s]+):’ -a line starting with a se-
quence of capitalized words folowed by a coln) 
to collect potential section headings from the train-
ing data, and the headings whose frequencies are 
higher than a threshold are used to identify section 
boundaries in the discharge sumaries.  
3.2 Field
detection  
This step consists of thre modules: the first mod-
ule, find_name, finds medication names in a dis-
charge sumary; the second module, context_type, 
proceses each medication name identified by 
find_name and detrmines whether the medication 
appears in narrative text or in a list of medications; 
the third module, find_others, detects the five non-
name field types.  
    For find_name and find_others, we follow the 
comon practice of treating named-entity (NE) 
detection as a sequence labeling task with the BIO 
Data 
Sets 
# of 
Summaries 
# of 
Entries 
# of 
Fields 
# of 
Names 
# of 
Doses 
# of 
Freq 
# of 
Modes 
# of 
Duration 
# of 
Reason 
Training 
set 
110 
 
5970 
 (54.3) 
14886 
(135.3) 
5684 
(51.7) 
2929 
(26.6) 
2740 
(24.9) 
2146 
(19.5) 
302  
(2.7) 
1085 
(9.9) 
Dev 
set 
35 
 
2401  
(68.6) 
5988 
(171.1) 
2302 
(65.8) 
1163 
(33.2) 
1096 
(31.3) 
880 
(25.1) 
111  
(3.2) 
436 
(12.5) 
Test 
set 
251 
 
8936 
(35.6) 
22041 
(87.8) 
8495 
(33.8) 
4387 
(17.5) 
3999 
(15.9) 
3307 
(13.2) 
511  
(2.0) 
1342 
(5.3) 
63
tagging scheme; that is, each token in the input is 
taged with B-x (beginning an NE of type x), I-x 
(inside an NE of type x) and O (outside any NE).  
3.2.1 The
find_name module 
As this module identifies medication names only, 
the tagset under the BIO scheme has thre tags: B-
m for begining of a name, I-m for inside a name, 
and O for outside. Various features are used for 
this module, which we group into four types:  
 
• (F1) includes word n-gram features (n=1,2,3). 
For instance, the bigram w
i-1
 w
i
 loks at the 
curent word and the previous word.  
 
• (F2) contains features that check properties of 
the curent word and its neighbors (e.g., the 
POS tag, the afixes and the length of a word, 
the type of section that a word apears in, 
whether a word is capitalized, whether a word 
is a number, etc.) 
 
• (F3) checks the BIO tags of previous words 
 
• (F4) contains features that check whether n-
grams formed by neighboring words appear as 
part of medication names in given medication 
name lists. The name lists can come from la-
beled training data or additional resources such 
as UMLS. 
3.2.2 The
context_type module 
This module is a binary clasifier which deter-
mines whether a medication name occurs in a list 
or narative context. Features used by this module 
include the section name as identified by the pre-
procesing step, the number of commas and words 
on the current line, the position of the medication 
name on the current line, and the current and near-
by words.  
3.2.3 The
find_others module 
This module complements the find_name module 
and uses eleven BIO tags to identify five non-name 
fields. The feature set used in this module is very 
similar to the one used in find_name except that 
some features in (F2) and (F4) are modified to suit 
the neds of the non-name fields. For instance, a 
feature wil check whether a word fits a comon 
patern for dosage. In addition, some features in 
(F2) lok at the output of previous modules: e.g., 
the location of nearby medication names as this 
information can be provided by the find_name 
module at test time.  
3.3 Field
linking  
Once medication names and other fields have been 
found, the final step is to form entries by asociat-
ing each medication name with its related fields. 
Our curent implementation uses simple heuristics. 
First, we go over each non-name field and link it 
with the closest preceding medication name unles 
the distance betwen the non-name field and its 
closest following medication name is much shorter. 
Second, we asemble the (name, non-name) pairs 
to form medication entries with a few rules.  
 
   More information about the modules discused in 
this section and features used by the modules is 
available in (Halgrim, 2009). 
4 Experimental
results  
In this section, we report the performance of our 
system on the development set (Section 4.1-4.3) 
and the test set (Section 4.4). The data sets are de-
scribed in Table 2. For al the experiments in this 
section, unless specified otherwise, we report exact 
horizontal precision/recal/F-score, the primary 
metrics for the i2b2 chalenge.  
   For the thre modules in the field detection step, 
we use the Maximum Entropy (MaxEnt) learner in 
the Mallet package (McCallum, 202) because, in 
general, MaxEnt produces good results without 
much parameter tuning and the training time for 
MaxEnt is much faster than more sophisticated 
algorithms such as CRF (Lafferty et al., 201).  
   To determine whether the difference betwen 
two systems’ performances is statistically signifi-
cant, we use approximate randomization tests (No-
reen, 1989) as folows. Given two systems that we 
would like to compare, we first calculate the dif-
ference betwen exact horizontal F-scores. Then 
two pseudo-system outputs are generated by ran-
domly swapping (at 0.5 probability) the two sys-
tem outputs for each discharge sumary. If the 
diference betwn F-scores of these pseudo-
outputs is no les than the original F-score difer-
ence, a counter, cnt, is increased by one. This 
proces was repeated n=10,000 times, and the p-
value of the significance is equal to (cnt+1)/(n+1). 
64
If the p-value is smaler than a predefined thresh-
old (e.g., 0.05), we conclude that the diference 
betwen the two systems is statisticaly significant. 
 
4.1 Performance
of the whole system 
 
4.1.1 Effect
of feature sets 
 
To test the efect of feature sets on system per-
formance, we trained find_name and find_others 
with diferent feature sets and tested the whole sys-
tem on the development set. For (F4), we used two 
medication name lists. The first list consists of 
medication names gathered from the training data. 
The second list includes drug names from the FDA 
database 
(ww.acesdata.fda.gov/scripts/cder/ndc/). We 
use the second list to test whether ading features 
that check the information in an additional re-
source could improve the system performance. 
   The results are in Table 4. For the last two rows, 
F1-F4a uses the first medication name list, and F1-
F4b uses both lists. The F-score diference betwen 
all adjacent rows is statistically significant at 
p≤0.01, except for the pair F1-F3 vs. F1-F4a. It is 
not surprising that using the first medication name 
list on top of F1-F3 does not improve the perform-
ance, as the same kind of information has already 
been captured by F1 features. The improvement of 
F1-F4b over F1-F4a shows that the system can 
easily incorporate additional resources and achieve 
a statistically significant (at p≤0.01) gain. 
 
Features Precision Recal F-score 
F1 72.5 60.3 65.8 
F1-F2 82.5 78.2 80.3 
F1-F3 88.4 77.9 82.8 
F1-F4a 87.4 77.9 82.4 
F1-F4b 88.1 79.4 83.5 
 
Table 4: System performance on the development 
set with diferent feature sets 
4.1.2 Efect
of training data size 
Figure 1 shows the system performance on the de-
velopment set when diferent portions of the train-
ing set are used for training. The curve with “+” 
signs represents the results for F1-F4b, and the 
curve with circles represents the results for F1-F4a. 
The figure ilustrates that, as the training data size 
increases, the F-score with both feature sets im-
proves. In addition, the additional resource is most 
helpful when the training data size is smal, as in-
dicated by the decreasing gap betwen the two sets 
of F-scores when the size of training data in-
creases. 
 
 
Figure 1: System performance on the development 
set with diferent training data sizes (Legend: ○ 
represents F-scores with features in F1-F4a; + rep-
resents F-scores with features in F1-F4b) 
 
4.1.3 Pipeline
vs. find_al   
The curent field detection step is a pipeline ap-
proach with thre modules: find_name, con-
text_type, and find_others. Having thre separate 
modules alows each module to chose the features 
that are most apropriate for it. In adition, later 
modules can use features that check the output of 
the previous modules. A potential downside of the 
pipeline system is that the erors in the early mod-
ule would propagate to later modules. An alterna-
tive is to use a single module to detect al six field 
types together. 
 
Figure 2 shows the result of find_all in compari-
son to the result for the thre-module pipeline. 
Both use the F1-F4b feature sets, except that 
find_others uses some features that check the out-
put of previous modules which are not available to 
find_all. 
 
65
 
Figure 2: Pipeline vs. find_all for field detction 
(Legend: ○ represents F-scores with find_all; + 
represents F-scores with the thre-module pipeline)  
 
   Interestingly, when 10% of the training set is 
used for training, find_all has a higher F-score than 
the pipeline aproach, although the difference is 
not statistically significant at p≤0.05. As more data 
is used for training, the pipeline outperforms 
find_all, and when at least 50% of the training data 
is used, the difference betwen the two is statisti-
cally significant at p≤0.05. One possible explana-
tion for this phenomenon is that as more training 
data becomes available, the early modules in the 
pipeline make fewer erors; as a result, the disad-
vantage of the pipeline approach caused by eror 
propagation is outweighed by the advantage that 
the later modules in the pipeline can use features 
that check the output of the earlier modules. 
4.2 Performance
of the field detection step 
Table 5 shows the exact precision/recal/F-score on 
identifying the six field types, using all the training 
data, F1-F4b features, and the pipeline aproach 
for field detection. A span in the system output 
exactly matches a span in the gold standard if the 
two spans are identical and have the same field 
type. Among the six fields, the results for duration 
and reason are the lowest. That is because duration 
and reason are longer phrases than the other four 
field types and there are fewer strong, reliable cues 
to signal their presence.  
When making the narative/list distinction, the 
accuracy of our context_type module is 95.4%. In 
contrast, the acuracy of the baseline (which treats 
each medication name as in a list context) is only 
55.6%. 
 
 Precision Recall F-score 
Name 91.2 88.5 89.9 
Dosage 96.6 90.8 93.6 
Frequency 93.9 89.0 91.8 
Mode 95.7 90.3 92.9 
Duration 73.8 43.2 54.5 
Reason 72.2 31.0 43.3 
All fields 92.6 84.5 88.4 
Table 5: The performance (exact preci-
sion/recal/F-score) of field detection on the devel-
opment set. 
 
4.3 Performance
of the field linking step 
In order to evaluate the field linking step, we gen-
erated a list of (name, non-name) pairs from the 
gold standard, where the name and non-name 
fields apear in the same entry in the gold stan-
dard. We then compared these pairs with the ones 
produced by the field linking step and calculated 
precision/recal/F-score. Table 6 shows the result 
of two experiments: in the cheating experiment, the 
input to the field linking step is the fields from the 
gold standard; in the non-cheating experiment, the 
input is the output of the field detection step. These 
experiments show that, while the heuristic rules 
used in this step work reasonably wel when the 
input is acurate, the performance deteriorates con-
siderably when the input is noisy, an issue we plan 
to addres in future work. 
 
 Precision Recall F-score 
Non-cheating 87.4 75.1 80.8 
Cheating 96.2 94.5 95.3 
Table 6: The performance of the field linking step 
on the development set (cheating: assuming perfect 
field input; non-cheating: using the output of the 
field detection step) 
4.4 Results
on the test data 
Table 7 shows the system performance on the i2b2 
official test data. The system was trained on the 
union of the training and development data. Com-
pared with the top five i2b2 systems (see Table 3), 
our system was second only to the best i2b2 sys-
tem, which used more resources and more sophis-
ticated rules for field linking (Patrick and Li, 
2009).  
 
66
 Precision Recall F-score 
Horizontal 88.6 80.2 84.1 
Name 92.6 87.1 89.8 
Dosage 96.3 90.2 93.1 
Frequency 95.6 90.8 93.2 
Mode 96.7 90.2 93.3 
Duration 70.6 40.5 51.5 
Reason 73.4 34.7 47.1 
All fields 91.6 82.7 86.9 
Table 7: System performance on the test set when 
trained on the union of the training and the devel-
opment sets with F1-F4b features. 
5 Conclusion

We present a hybrid system for medication extrac-
tion. The system is built around a pipeline of cas-
cading statistical classifiers for field detection. It 
achieves god performance that is comparable to 
the top systems in the i2b2 chalenge, and incorpo-
rating aditonal resources as features further im-
proves the performance. In the futre, we plan to 
replace the current rule-based field linking module 
with a statistical module to improve acuracy. 
Acknowledgments 
This work was suported in part by US DOD grant 
N0024-091-0081 and NIH Grants 
1K99LM010227-0110, U54LM08748, and 
T15LM00742-06. We would also like to thank 
thre anonymous reviewers for helpful coments. 
References 
Trond Grenager, Dan Klein, and Christopher Manning. 
2005. Unsupervised learning of field segmentation 
models for information extraction. In Proc. of ACL-
2005. 
Scott Halgrim. 209. A Pipeline Machine Learning Ap-
proach to Biomedical Information Extraction. Master 
Thesis. University of Washington. 
 
J. Laferty and A. McCalum and F. Pereira. 201. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of the 
18th International Conference on Machine Learning 
(ICML-2001). 
Mathew A. Levin, Marina Krol, Ankur M. Doshi, and 
David L. Reich. 2007. Extraction and maping of 
drug names from fre text to a standardized nomen-
clature. AMIA Symposium Proceedings, pp 438-442. 
 
S. Meystre, G. Savova, K. Kiper-Schuler, and J. Hur-
dle. 208. Extracting Information from Textual 
Documents in the Electronic Health Record: A Re-
view of Recent Research. IMIA Yearbok of Medical 
Informatics Methods Inf Med 208; 47 Supl 1:128-
44. 
 
Andrew McCalum. 202. Malet: A Machine Learning 
for Language Tolkit. http:/malet.cs.umas.edu 
Eric W. Noren. 1989. Computer intensive methods for 
testing hypotheses: an introduction. John Wiley & 
Sons. 
Jon Patrick and Min Li, 2009. A Cascade Approach to  
Extract Medication Event (i2b2 chalenge 209). 
Presentation at the Third i2b2 Workshop, November 
2009, San Francisco, CA.  
Hoifung Pon and Pedro Domingos. 207. Joint infer-
ence in information extraction. In Proc. of the Na-
tional Conference on Artificial Intelligence (AAI), 
pp 913-918. 
Benjamin Rozenfeld and Ronen Feldman. 208. Self-
supervised relation extraction from the web. Knowl-
edge and Information Systems, 17(1):17-33. 
Özlem Uzuner, Imre Solti, and Eithon Cadag, 2010a. 
Extracting Medication Iformati from Clincl 
Text. Submitted to JAMIA. 
Özlem Uzuner, Imre Solti, Fei Xia, and Eithon Cadag, 
2010b. Comunity Anotation Experiment for 
Ground Truth Generation for the i2b2 Medication 
Chalenge. Submitted to JAMIA. 
Ben Welner, Andrew McCalum, Fuchun Peng, and 
Michael Hay. 204. An integrated, conditional model 
of information extraction and coreference with appli-
cation to citation matching. In Proc. of the 20th Con-
ference on Uncertainty in AI (UAI-2004). 
 
 
 
67

