<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>H Ai</author>
<author>D Litman</author>
<author>K Forbes-Riley</author>
<author>M Rotaru</author>
<author>J Tetreault</author>
<author>A Purandare</author>
</authors>
<title>Using system and user performance features to improve emotion detection in spoken tutoring dialogs</title>
<date>2006</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<pages>797--800</pages>
<location>Pittsburgh, PA</location>
<contexts>
<context> that useful predictive models of student affect in general, and student uncertainty specifically, can be built using similar features available in our ITSPOKE corpora (Litman and Forbes-Riley, 2006; Ai et al., 2006). 6. Summary We presented the publicly available Uncertainty corpus, a collection of spoken tutoring dialogues between students and an adaptive Wizard-of-Oz spoken dialogue tutoring system, in which </context>
</contexts>
<marker>Ai, Litman, Forbes-Riley, Rotaru, Tetreault, Purandare, 2006</marker>
<rawString>H. Ai, D. Litman, K. Forbes-Riley, M. Rotaru, J. Tetreault, and A. Purandare. 2006. Using system and user performance features to improve emotion detection in spoken tutoring dialogs. In Proceedings of Interspeech, pages 797–800, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ang</author>
<author>R Dhillon</author>
<author>A Krupski</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
</authors>
<title>Prosody-based automatic detection of annoyance and frustration in human-computer dialog</title>
<date>2002</date>
<journal>In J. H. L</journal>
<contexts>
<context>E project1 contains a large collection of publicly available emotional speech corpora, very few contain naturally occurring human-computer dialogues (e.g. (Batliner et al., 2004; Walker et al., 2001; Ang et al., 2002)). Moreover, only the DARPA Communicator corpus uses English; it contains dialogues in the travel-planning (i.e. form-filling) domain, and user turns are annotated for frustration and annoyance. To s</context>
</contexts>
<marker>Ang, Dhillon, Krupski, Shriberg, Stolcke, 2002</marker>
<rawString>J. Ang, R. Dhillon, A. Krupski, E.Shriberg, and A. Stolcke. 2002. Prosody-based automatic detection of annoyance and frustration in human-computer dialog. In J. H. L.</rawString>
</citation>
<citation valid="false">
<booktitle>Proceedings of the International Conference on Spoken Language Processing (ICSLP</booktitle>
<pages>2037--2039</pages>
<editor>Hansen and B. Pellom, editors</editor>
<location>Denver, USA</location>
<marker></marker>
<rawString>Hansen and B. Pellom, editors, Proceedings of the International Conference on Spoken Language Processing (ICSLP), pages 2037–2039, Denver, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Batliner</author>
<author>K Fischer</author>
<author>R Huber</author>
<author>J Spilker</author>
<author>E Noth</author>
</authors>
<title>How to find trouble in communication</title>
<date>2003</date>
<journal>Speech Communication</journal>
<pages>40--1</pages>
<contexts>
<context>n research on spoken dialogue systems, many promising results have been reported for automatically detecting user affective states (e.g., (Litman and Forbes-Riley, 2006; Vidrascu and Devillers, 2005; Batliner et al., 2003; Shafran et al., 2003)). The larger goal of this work is to improve spoken dialogue system performance by automatically adapting to user affect. The achievement of this goal could be significantly ai</context>
<context>n the prosody of elicited or acted emotions (e.g. (Oudeyer, 2002; Liscombe et al., 2003)); however, these results generally transfer poorly to naturally occurring emotions (Cowie and Cornelius, 2003; Batliner et al., 2003). Thus recent research has focused on analyzing and detecting user affect in naturally occurring dialogue (e.g. (Vidrascu and Devillers, 2005; Batliner et al., 2003; Shafran et al., 2003)). The Uncer</context>
</contexts>
<marker>Batliner, Fischer, Huber, Spilker, Noth, 2003</marker>
<rawString>A. Batliner, K. Fischer, R. Huber, J. Spilker, and E. Noth. 2003. How to find trouble in communication. Speech Communication, 40(1-2):117–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Batliner</author>
<author>C Hacker</author>
<author>S Steidl</author>
<author>E Noth</author>
<author>J Haas</author>
</authors>
<title>From emotion to interaction: Lessons from real human-machine dialogues</title>
<date>2004</date>
<booktitle>Affective Dialogue Systems, Proceedings of a Tutorial and Research Workshop</booktitle>
<volume>3068</volume>
<pages>1--12</pages>
<editor>In E. Andre, L. Dybkjær, W. Minker, and P. Heisterkamp, editors</editor>
<publisher>Springer-Verlag</publisher>
<location>Berlin</location>
<contexts>
<context>ics community. For example, while the HUMAINE project1 contains a large collection of publicly available emotional speech corpora, very few contain naturally occurring human-computer dialogues (e.g. (Batliner et al., 2004; Walker et al., 2001; Ang et al., 2002)). Moreover, only the DARPA Communicator corpus uses English; it contains dialogues in the travel-planning (i.e. form-filling) domain, and user turns are annota</context>
</contexts>
<marker>Batliner, Hacker, Steidl, Noth, Haas, 2004</marker>
<rawString>A. Batliner, C. Hacker, S. Steidl, E. Noth, and J. Haas. 2004. From emotion to interaction: Lessons from real human-machine dialogues. In E. Andre, L. Dybkjær, W. Minker, and P. Heisterkamp, editors, Affective Dialogue Systems, Proceedings of a Tutorial and Research Workshop, volume 3068 of Lecture Notes in Artificial Intelligence, pages 1–12, Berlin. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bhatt</author>
<author>M Evens</author>
<author>S Argamon</author>
</authors>
<title>Hedged responses and expressions of affect in human/human and human/computer tutorial interactions</title>
<date>2004</date>
<booktitle>In Proceedings of Cognitive Science (CogSci</booktitle>
<pages>114--119</pages>
<location>Chicago, USA</location>
<contexts>
<context>ech where emotion is present but not full-blown, including arousal and attitude (Cowie and Cornelius, 2003). Some tutoring researchers also combine emotion and attitude (e.g. (Pon-Barry et al., 2006; Bhatt et al., 2004)). Figure 1: Screenshot of WOZ-TUT Wizard Interface placed by a human “wizard”. The wizard performs speech recognition, correctness annotation, and uncertainty annotation, for each student answer. In</context>
</contexts>
<marker>Bhatt, Evens, Argamon, 2004</marker>
<rawString>K. Bhatt, M. Evens, and S. Argamon. 2004. Hedged responses and expressions of affect in human/human and human/computer tutorial interactions. In Proceedings of Cognitive Science (CogSci), pages 114–119, Chicago, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cowie</author>
<author>R R Cornelius</author>
</authors>
<title>Describing the emotional states that are expressed in speech</title>
<date>2003</date>
<journal>Speech Communication</journal>
<pages>40--1</pages>
<contexts>
<context>the two, but some speech researchers find the narrow sense of “emotion” too restrictive since it excludes states in speech where emotion is present but not full-blown, including arousal and attitude (Cowie and Cornelius, 2003). Some tutoring researchers also combine emotion and attitude (e.g. (Pon-Barry et al., 2006; Bhatt et al., 2004)). Figure 1: Screenshot of WOZ-TUT Wizard Interface placed by a human “wizard”. The wiz</context>
<context>ignificant prior research on the prosody of elicited or acted emotions (e.g. (Oudeyer, 2002; Liscombe et al., 2003)); however, these results generally transfer poorly to naturally occurring emotions (Cowie and Cornelius, 2003; Batliner et al., 2003). Thus recent research has focused on analyzing and detecting user affect in naturally occurring dialogue (e.g. (Vidrascu and Devillers, 2005; Batliner et al., 2003; Shafran et</context>
</contexts>
<marker>Cowie, Cornelius, 2003</marker>
<rawString>R. Cowie and R. R. Cornelius. 2003. Describing the emotional states that are expressed in speech. Speech Communication, 40(1-2):5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Craig</author>
<author>A Graesser</author>
<author>J Sullins</author>
<author>B Gholson</author>
</authors>
<title>Affect and learning: an exploratory look into the role of affect in learning with AutoTutor</title>
<date>2004</date>
<journal>Journal of Educational Media</journal>
<volume>29</volume>
<contexts>
<context>y on student (in)correctness, tutoring researchers view both incorrectness and uncertainty as signals of “learning impasses”; i.e. as opportunities for the student to engage in constructive learning (Craig et al., 2004; VanLehn et al., 2003). This view provides a straightforward adaptation hypothesis to test: Responding to student uncertainty in the same way as incorrectness should significantly increase learning, </context>
</contexts>
<marker>Craig, Graesser, Sullins, Gholson, 2004</marker>
<rawString>S. Craig, A. Graesser, J. Sullins, and B. Gholson. 2004. Affect and learning: an exploratory look into the role of affect in learning with AutoTutor. Journal of Educational Media, 29(3):241–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>D Litman</author>
</authors>
<title>Analyzing dependencies between student certainness states and tutor responses in a spoken dialogue corpus</title>
<date>2008</date>
<booktitle>Recent Trends in Discourse and Dialogue</booktitle>
<pages>275--304</pages>
<editor>In L. Dybkjaer and W. Minker, editors</editor>
<publisher>Springer</publisher>
<contexts>
<context>ond to student affect3 over and above correctness, and are initially targeting student uncertainty for two reasons. First, it occurred more often than other student affective states in our dialogues (Forbes-Riley and Litman, 2008). Second, although most tutoring systems respond based only on student (in)correctness, tutoring researchers view both incorrectness and uncertainty as signals of “learning impasses”; i.e. as opportu</context>
<context> and uncertainty judgments are both binary. In other words, a “correct” answer may be either partially and fully correct, while a “nonuncertain” answer may be either certain or neutral for certainty (Forbes-Riley and Litman, 2008). These checkbox values are sent to the dialogue manager to determine the WOZ-TUT’s response. In the lower left checkboxes, the Wizard annotates whether the answer that is heard is one that is “antic</context>
<context>ion would improve system perfor4In similar ITSPOKE corpora, this wizard displayed interannotator agreement of 0.85 Kappa on labeling binary correctness, and 0.62 Kappa on labeling binary uncertainty (Forbes-Riley and Litman, 2008). mance (e.g. student learning). For use in these 3 conditions, the WOZ-TUT dialogue manager was parameterized, so that it could adapt contingently on the student state of uncertain+correct as discus</context>
</contexts>
<marker>Forbes-Riley, Litman, 2008</marker>
<rawString>K. Forbes-Riley and D. Litman. 2008. Analyzing dependencies between student certainness states and tutor responses in a spoken dialogue corpus. In L. Dybkjaer and W. Minker, editors, Recent Trends in Discourse and Dialogue, pages 275–304. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>D Litman</author>
<author>M Rotaru</author>
</authors>
<title>Responding to student uncertainty during computer tutoring: A preliminary evaluation</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th International Conference on Intelligent Tutoring Systems (ITS</booktitle>
<location>Montreal, Canada</location>
<contexts>
<context>rent discourse structure depths; this discourse structure information is automatically available in our dialogues (ForbesRiley et al., 2008b). We compare these and other metrics across conditions in (Forbes-Riley et al., 2008a). A second use of the Uncertainty corpus is as a resource for analyzing prosody and other linguistic features of naturally occurring user affect in human-computer dialogue, particularly for use in a</context>
</contexts>
<marker>Forbes-Riley, Litman, Rotaru, 2008</marker>
<rawString>K. Forbes-Riley, D. Litman, and M. Rotaru. 2008a. Responding to student uncertainty during computer tutoring: A preliminary evaluation. In Proceedings of the 9th International Conference on Intelligent Tutoring Systems (ITS), Montreal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>M Rotaru</author>
<author>D Litman</author>
</authors>
<title>The relative impact of student affect on performance models in a spoken dialogue tutoring system. User Modeling and User-Adapted Interaction</title>
<date>2008</date>
<pages>18--1</pages>
<contexts>
<context>rent discourse structure depths; this discourse structure information is automatically available in our dialogues (ForbesRiley et al., 2008b). We compare these and other metrics across conditions in (Forbes-Riley et al., 2008a). A second use of the Uncertainty corpus is as a resource for analyzing prosody and other linguistic features of naturally occurring user affect in human-computer dialogue, particularly for use in a</context>
</contexts>
<marker>Forbes-Riley, Rotaru, Litman, 2008</marker>
<rawString>K. Forbes-Riley, M. Rotaru, and D. Litman. 2008b. The relative impact of student affect on performance models in a spoken dialogue tutoring system. User Modeling and User-Adapted Interaction, 18(1-2):11–43, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Liscombe</author>
<author>J Venditti</author>
<author>J Hirschberg</author>
</authors>
<title>Classifying subject ratings of emotional speech using acoustic features</title>
<date>2003</date>
<booktitle>In Proceedings of Interspeech/EuroSpeech</booktitle>
<pages>725--728</pages>
<location>Geneva, Switzerland</location>
<contexts>
<context> human-computer dialogue, particularly for use in automatic affect detection. For example, there has been significant prior research on the prosody of elicited or acted emotions (e.g. (Oudeyer, 2002; Liscombe et al., 2003)); however, these results generally transfer poorly to naturally occurring emotions (Cowie and Cornelius, 2003; Batliner et al., 2003). Thus recent research has focused on analyzing and detecting use</context>
</contexts>
<marker>Liscombe, Venditti, Hirschberg, 2003</marker>
<rawString>J. Liscombe, J. Venditti, and J.Hirschberg. 2003. Classifying subject ratings of emotional speech using acoustic features. In Proceedings of Interspeech/EuroSpeech, pages 725–728, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>K Forbes-Riley</author>
</authors>
<title>Recognizing student emotions and attitudes on the basis of utterances in spoken tutoring dialogues with both human and computer tutors</title>
<date>2006</date>
<journal>Speech Communication</journal>
<volume>48</volume>
<contexts>
<context>ion in complex spoken dialogue systems. 1. Introduction Within research on spoken dialogue systems, many promising results have been reported for automatically detecting user affective states (e.g., (Litman and Forbes-Riley, 2006; Vidrascu and Devillers, 2005; Batliner et al., 2003; Shafran et al., 2003)). The larger goal of this work is to improve spoken dialogue system performance by automatically adapting to user affect. T</context>
<context>n complex (e.g. non-form filling) dialogue systems. 2. WOZ-TUT: Adaptive Wizard-of-Oz Spoken Dialogue Tutoring System In prior work we developed ITSPOKE (Intelligent Tutoring SPOKEn dialogue system) (Litman and Forbes-Riley, 2006). ITSPOKE tutors students in 5 qualitative physics problems. The dialogue manager uses a finite state paradigm; tutor responses (next states) are based on the correctness of the student answer (trans</context>
<context>og files. We have already shown that useful predictive models of student affect in general, and student uncertainty specifically, can be built using similar features available in our ITSPOKE corpora (Litman and Forbes-Riley, 2006; Ai et al., 2006). 6. Summary We presented the publicly available Uncertainty corpus, a collection of spoken tutoring dialogues between students and an adaptive Wizard-of-Oz spoken dialogue tutoring </context>
</contexts>
<marker>Litman, Forbes-Riley, 2006</marker>
<rawString>D. Litman and K. Forbes-Riley. 2006. Recognizing student emotions and attitudes on the basis of utterances in spoken tutoring dialogues with both human and computer tutors. Speech Communication, 48(5):559–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P-Y Oudeyer</author>
</authors>
<title>The production and recognition of emotions in speech: Features and Algorithms</title>
<date>2002</date>
<journal>International Journal of Human Computer Studies</journal>
<volume>59</volume>
<contexts>
<context> user affect in human-computer dialogue, particularly for use in automatic affect detection. For example, there has been significant prior research on the prosody of elicited or acted emotions (e.g. (Oudeyer, 2002; Liscombe et al., 2003)); however, these results generally transfer poorly to naturally occurring emotions (Cowie and Cornelius, 2003; Batliner et al., 2003). Thus recent research has focused on anal</context>
</contexts>
<marker>Oudeyer, 2002</marker>
<rawString>P-Y. Oudeyer. 2002. The production and recognition of emotions in speech: Features and Algorithms. International Journal of Human Computer Studies, 59(12):157–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Pon-Barry</author>
<author>K Schultz</author>
<author>E Owen Bratt</author>
<author>B Clark</author>
<author>S Peters</author>
</authors>
<title>Responding to student uncertainty in spoken tutorial dialogue systems</title>
<date>2006</date>
<journal>International Journal of Artificial Intelligence in Education</journal>
<pages>16--171</pages>
<contexts>
<context>t excludes states in speech where emotion is present but not full-blown, including arousal and attitude (Cowie and Cornelius, 2003). Some tutoring researchers also combine emotion and attitude (e.g. (Pon-Barry et al., 2006; Bhatt et al., 2004)). Figure 1: Screenshot of WOZ-TUT Wizard Interface placed by a human “wizard”. The wizard performs speech recognition, correctness annotation, and uncertainty annotation, for eac</context>
</contexts>
<marker>Pon-Barry, Schultz, Bratt, Clark, Peters, 2006</marker>
<rawString>H. Pon-Barry, K. Schultz, E. Owen Bratt, B. Clark, and S. Peters. 2006. Responding to student uncertainty in spoken tutorial dialogue systems. International Journal of Artificial Intelligence in Education, 16:171–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Shafran</author>
<author>M Riley</author>
<author>M Mohri</author>
</authors>
<title>Voice signatures</title>
<date>2003</date>
<booktitle>In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU</booktitle>
<pages>31--36</pages>
<publisher>St. Thomas, US Virgin Islands</publisher>
<contexts>
<context>alogue systems, many promising results have been reported for automatically detecting user affective states (e.g., (Litman and Forbes-Riley, 2006; Vidrascu and Devillers, 2005; Batliner et al., 2003; Shafran et al., 2003)). The larger goal of this work is to improve spoken dialogue system performance by automatically adapting to user affect. The achievement of this goal could be significantly aided by studying affect</context>
<context>lius, 2003; Batliner et al., 2003). Thus recent research has focused on analyzing and detecting user affect in naturally occurring dialogue (e.g. (Vidrascu and Devillers, 2005; Batliner et al., 2003; Shafran et al., 2003)). The Uncertainty corpus provides an additional resource for this active research area, because it makes available a large number of features derived from the speech files, transcripts, and log file</context>
</contexts>
<marker>Shafran, Riley, Mohri, 2003</marker>
<rawString>I. Shafran, M. Riley, and M. Mohri. 2003. Voice signatures. In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 31–36, St. Thomas, US Virgin Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K VanLehn</author>
<author>S Siler</author>
<author>C Murray</author>
</authors>
<title>Why do only some events cause learning during human tutoring? Cognition and Instruction</title>
<date>2003</date>
<contexts>
<context>rectness, tutoring researchers view both incorrectness and uncertainty as signals of “learning impasses”; i.e. as opportunities for the student to engage in constructive learning (Craig et al., 2004; VanLehn et al., 2003). This view provides a straightforward adaptation hypothesis to test: Responding to student uncertainty in the same way as incorrectness should significantly increase learning, by providing students </context>
</contexts>
<marker>VanLehn, Siler, Murray, 2003</marker>
<rawString>K. VanLehn, S. Siler, and C. Murray. 2003. Why do only some events cause learning during human tutoring? Cognition and Instruction, 21(3):209–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Vidrascu</author>
<author>L Devillers</author>
</authors>
<title>Detection of real-life emotions in dialogs recorded in a call center</title>
<date>2005</date>
<booktitle>In Proceedings of INTERSPEECH</booktitle>
<location>Lisbon, Portugal. Marilyn</location>
<contexts>
<context>systems. 1. Introduction Within research on spoken dialogue systems, many promising results have been reported for automatically detecting user affective states (e.g., (Litman and Forbes-Riley, 2006; Vidrascu and Devillers, 2005; Batliner et al., 2003; Shafran et al., 2003)). The larger goal of this work is to improve spoken dialogue system performance by automatically adapting to user affect. The achievement of this goal co</context>
<context>orly to naturally occurring emotions (Cowie and Cornelius, 2003; Batliner et al., 2003). Thus recent research has focused on analyzing and detecting user affect in naturally occurring dialogue (e.g. (Vidrascu and Devillers, 2005; Batliner et al., 2003; Shafran et al., 2003)). The Uncertainty corpus provides an additional resource for this active research area, because it makes available a large number of features derived fro</context>
</contexts>
<marker>Vidrascu, Devillers, 2005</marker>
<rawString>L. Vidrascu and L. Devillers. 2005. Detection of real-life emotions in dialogs recorded in a call center. In Proceedings of INTERSPEECH, Lisbon, Portugal. Marilyn Walker, Rebecca Passonneau, and Julie Boland.</rawString>
</citation>
</citationList>
</algorithm>

