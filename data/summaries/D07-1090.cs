The underlying architecture is similar to (Zhang et al., 2006).
W06-1626
Recently a two-pass approach has been proposed (Zhang et al., 2006), wherein a lowerorder n-gram is used in a hypothesis-generation phase, then later the K-best of these hypotheses are re-scored using a large-scale distributed language model.
W06-1626
The mathematics of the problem were formalized by (Brown et al., 1993), and re-formulated by (Och and Ney, 2004) in terms of the optimization ˆe = arg maxe Msummationdisplay m=1 λmhm(e,f) (1) where fhm(e,f)g is a set of M feature functions and fλmg a set of weights.
J04-4002 J93-2003
For each training data size, we report the size of the resulting language model, the fraction of 5-grams from the test data that is present in the language model, and the BLEU score (Papineni et al., 2002) obtained by the machine translation system.
P02-1040
Differences of more than 0.51 BP are statistically significant at the 0.05 level using bootstrap resampling (Noreen, 1989; Koehn, 2004).
W04-3250
