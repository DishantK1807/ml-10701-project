J. Clarke, D. Goldwasser, M. Chang, and D. Roth. 2010.
W10-2903
Galley et al., 2004; Xu et al., 2009). While it has
N09-1028
D. Burkett and D. Klein. 2008. Two languages are better
D08-1092
al., 2010a; Birch and Osborne, 2010). This demon-
W10-1749
F. Och. 2003. Minimum error rate training in statistical
P03-1021
Treebank (Marcus et al., 1993) using standard train-
J93-2004
al., 2005; Wang, 2007; Xu et al., 2009) or auto-
D07-1077 N09-1028
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002.
P02-1040
Miyao et al. (2008) compared parser quality in the
P08-1006
D. Burkett, S. Petrov, J. Blitzer, and D. Klein. 2010.
W10-2906
H. Isozaki, K. Sudoh, H. Tsukada, and K. Duh. 2010b.
W10-1736
(Gildea, 2001), especially web text (Foster, 2010).
N10-1060 W01-0521
(Judge et al., 2006), as a source of syntactically an-
P06-1063
bank (Judge et al., 2006). Table 4 shows that both
P06-1063
A. Birch and M. Osborne. 2010. LRscore for evaluating
W10-1749
P. Xu, J. Kang, M. Ringgaard, and F. Och. 2009. Using a
N09-1028
Burkett and Klein (2008) and Burkett et al. (2010),
D08-1092 W10-2906
phrase reordering model (Zens and Ney, 2006) as
W06-3108
the reordering rules proposed by Xu et al. (2009),
N09-1028
matically learned (Xia and McCord, 2004; Habash,
C04-1073
T. Brants. 2000. TnT – a statistical part-of-speech tagger.
A00-1031
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
J93-2004
1997; Charniak, 2000; McDonald et al., 2005;
A00-2018 P05-1012
ger (Brants, 2000) in our experiments, because of
A00-1031
(Petrov and Klein, 2007). Furthermore, parsing was
N07-1051
D. McClosky., E. Charniak, and M. Johnson. 2006. Ef-
N06-1020
E. Charniak. 2000. A maximum–entropy–inspired
A00-2018
M. Collins. 1997. Three generative, lexicalised models
P97-1003
(Chang et al., 2007; Chang et al., 2010; Clarke et al.,
P07-1036
of BLEU score (Papineni et al., 2002), a widely used
P02-1040
D. Genzel. 2010. Automatically learning source-side re-
C10-1043
S. Petrov and D. Klein. 2007. Improved inference for
N07-1051
2007; Genzel, 2010).
C10-1043
(a.k.a. BerkeleyParser) of Petrov et al. (2006). We
P06-1055
C. Wang. 2007. Chinese syntactic reordering for statisti-
D07-1077
Petrov et al., 2006; Nivre, 2008). A common—
J08-4003 P06-1055
J. Judge, A. Cahill, and J. v. Genabith. 2006. Question-
P06-1063
in the spirit of Liang et al. (2006), who train a per-
P06-1096
D. Gildea. 2001. Corpus variation and parser perfor-
W01-0521
R. Zens and H. Ney. 2006. Discriminative reordering
W06-3108
MERT (Och, 2003). All experiments for a given lan-
P03-1021
Y. Zhang and S. Clark. 2008. A tale of two parsers: In-
D08-1059
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
P05-1012
F. Xia and M. McCord. 2004. Improving a statistical MT
C04-1073
training of Burkett et al. (2010) in which they
W10-2906
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
P06-1055
portion of reordering phenomena. Isozaki et al.(2010b), for instance, were able to get impressive
W10-1736
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
P07-1036
M. Collins, P. Koehn, and I. Kuˇcerov´a. 2005. Clause re-
P05-1066
the Penn Treebank (Marcus et al., 1993; Collins,
J93-2004
J. Foster. 2010. “cba to check the spelling”: Investigat-
N10-1060
This idea is similar in vein to McClosky. et al.(2006) and Petrov et al. (2010), except that we use an
N06-1020
Zhang and Clark (2008). The parser uses the fol-
D08-1059
an arc-eager transition strategy (Nivre, 2008). The
J08-4003
J. Nivre. 2008. Algorithms for deterministic incremen-
J08-4003
