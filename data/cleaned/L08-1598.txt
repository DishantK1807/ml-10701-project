<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<title>Ogmios: The UPC Text-to-Speech Synthesis System for Spoken Translation</title>
<date>2006</date>
<booktitle>In Proceedings of the TC-STAR Workshop on Speech-to-Speech Translation</booktitle>
<pages>31--36</pages>
<location>Barcelona, Spain</location>
<marker>2006</marker>
<rawString>(2006). Ogmios: The UPC Text-to-Speech Synthesis System for Spoken Translation. In Proceedings of the TC-STAR Workshop on Speech-to-Speech Translation, Barcelona, Spain, pp. 31-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Hamon</author>
<author>D Mostefa</author>
<author>K Choukri</author>
</authors>
<title>End-to-End Evaluation of a Speech-to-Speech Translation System in TC-STAR</title>
<date>2007</date>
<booktitle>In Proceedings of MT</booktitle>
<location>Summit XI, Copenhagen, Denmark</location>
<contexts>
<context> describes the evaluation methodology used to evaluate the TC-STAR speech-to-speech translation (SST) system and their results from the third year of the project. It follows the results presented in (Hamon et al., 2007), dealing with the first end-to-end evaluation of the project. In this paper, we try to experiment with the methodology and the protocol during the second end-to-end evaluation, by comparing outputs </context>
<context>tput is also built up. Finally, the SLT output is synthesized in Spanish by the TTS module. 3. Protocol In this experiment, we kept the same protocol as that used for the first end-to-end evaluation (Hamon et al., 2007) with few exceptions in order to experiment new methods. The concepts of adequacy and fluency are based on machine translation (White et al., 1994) and calculated over a five-point scale which is fil</context>
<context>ency criteria. Indeed, most of the errors could cause reduction of the quality. We try here to outline issues from both kinds of speech, in addition to those already found in the previous evaluation (Hamon et al, 2007). In many audio outputs, the interpreters hesitate and make repetitions. That is probably due to the delivery of the speaker: there is no feedback when speakers talk and most of the time this is a fa</context>
</contexts>
<marker>Hamon, Mostefa, Choukri, 2007</marker>
<rawString>Hamon O., Mostefa D., Choukri K. (2007). End-to-End Evaluation of a Speech-to-Speech Translation System in TC-STAR. In Proceedings of MT Summit XI, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lamel</author>
<author>J L Gauvain</author>
<author>G Adda</author>
<author>C Barras</author>
<author>E Bilinski</author>
<author>O Galibert</author>
<author>A Pujol</author>
<author>H Schwenk</author>
<author>X Zhu</author>
</authors>
<title>The LIMSI 2006 TC-STAR Transcription Systems</title>
<date>2006</date>
<booktitle>In Proceedings of the TC-STAR Workshop on Speech-to-Speech Translation</booktitle>
<pages>123--128</pages>
<location>Barcelona, Spain</location>
<contexts>
<context>, in English, and the audio output, in Spanish. For this second evaluation, the evaluated TC-STAR system includes the following modules: The ASR module made of a combination of several ASR engines (Lamel et al., 2006), using the Recognizer Output Voting Error Reduction (ROVER) method (Fiscus, 1997); The SLT module made of a combination of several SLT engines, as a ROVER (Matusov et al., 2006); The TTS module </context>
</contexts>
<marker>Lamel, Gauvain, Adda, Barras, Bilinski, Galibert, Pujol, Schwenk, Zhu, 2006</marker>
<rawString>Lamel L., Gauvain J.L., Adda G., Barras C., Bilinski E., Galibert O., Pujol A., Schwenk H., Zhu X. (2006). The LIMSI 2006 TC-STAR Transcription Systems. In Proceedings of the TC-STAR Workshop on Speech-to-Speech Translation, Barcelona, Spain, pp. 123-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>A Vallin</author>
<author>C Ayache</author>
<author>G Erbach</author>
<author>A Peñas</author>
<author>M De Rijke</author>
<author>P Rocha</author>
<author>K Simov</author>
<author>R Sutcliffe</author>
</authors>
<title>Overview of the CLEF 2004 Multilingual Question Answering Track</title>
<date>2004</date>
<booktitle>In Working Notes of the Workshop of CLEF 2004</booktitle>
<location>Bath</location>
<marker>Magnini, Vallin, Ayache, Erbach, Peñas, De Rijke, Rocha, Simov, Sutcliffe, 2004</marker>
<rawString>Magnini B., Vallin A., Ayache C., Erbach G., Peñas A., De Rijke M., Rocha P., Simov K., Sutcliffe R. (2004) Overview of the CLEF 2004 Multilingual Question Answering Track. In Working Notes of the Workshop of CLEF 2004, Bath, 15-17 september 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>N Ueffing</author>
<author>H Ney</author>
</authors>
<title>Automatic Sentence Segmentation and Punctuation Prediction for Spoken Language Translation</title>
<date>2006</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT</booktitle>
<pages>158--165</pages>
<location>Trento, Italy</location>
<contexts>
<context> several ASR engines (Lamel et al., 2006), using the Recognizer Output Voting Error Reduction (ROVER) method (Fiscus, 1997); The SLT module made of a combination of several SLT engines, as a ROVER (Matusov et al., 2006); The TTS module developed by UPC (Bonafonte et al., 2006). Therefore, if we exclude the transit from one module to another, the system is fully automatic: no manual modifications are done on the o</context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>Matusov E., Ueffing N., Ney H. (2006). Automatic Sentence Segmentation and Punctuation Prediction for Spoken Language Translation. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), Trento, Italy, pp. 158-165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mostefa</author>
<author>O Hamon</author>
<author>N Moreau</author>
<author>K Choukri</author>
</authors>
<date>2007</date>
<contexts>
<context>sion of the output speech. That is probably one of the most surprising facts, the TTS output being (normally) the exact synthesis of the SLT output and TTS systems getting good results for synthesis (Mostefa et al., 2007). Actually, the explanation is rather simple and is due, in part, to the quality of the translation. Indeed, when the quality of the SLT output is quite low, the prosody breaks the flow and the outpu</context>
</contexts>
<marker>Mostefa, Hamon, Moreau, Choukri, 2007</marker>
<rawString>Mostefa D., Hamon O., Moreau N., Choukri K. (2007).</rawString>
</citation>
<citation valid="true">
<title>Technological Showcase and End-to-End Evaluation Architecture, Technology and Corpora for Speech to Speech Translation (TC-STAR) projects. Deliverable D30</title>
<date>2007</date>
<marker>2007</marker>
<rawString>Technological Showcase and End-to-End Evaluation Architecture, Technology and Corpora for Speech to Speech Translation (TC-STAR) projects. Deliverable D30, May 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Somers</author>
<author>Y Sugita</author>
</authors>
<title>Evaluating Commercial Spoken Language Translation Software</title>
<date>2003</date>
<booktitle>In Proceedings of the Ninth Machine Translation Summit</booktitle>
<pages>370--377</pages>
<location>New Orleans</location>
<marker>Somers, Sugita, 2003</marker>
<rawString>Somers H. and Sugita Y. (2003). Evaluating Commercial Spoken Language Translation Software. In Proceedings of the Ninth Machine Translation Summit, pp. 370-377, New Orleans.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
<author>H Dang</author>
</authors>
<title>Overview of TREC 2005 question answering track</title>
<date>2005</date>
<booktitle>In Proceedings of TREC2005</booktitle>
<contexts>
<context>are checked manually. Then, all questions and answers are translated into Spanish. For this evaluation, we tried to classify questions into three categories, partly coming from information retrieval (Voorhees and Dang, 2005): Factoid (70% of questions), Boolean (20%) and List (10%). This could determine the quality of the system according to the type of question. Table 1 gives examples, out of the context, for each type</context>
</contexts>
<marker>Voorhees, Dang, 2005</marker>
<rawString>Voorhees E. and Dang H. (2005). Overview of TREC 2005 question answering track. In Proceedings of TREC2005.</rawString>
</citation>
</citationList>
</algorithm>

