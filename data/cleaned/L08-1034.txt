<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>P Cimiano</author>
<author>A Hotho</author>
<author>S Staab</author>
</authors>
<title>Comparing conceptual, divisive and agglomerative clustering for learning taxonomies from text</title>
<date>2004</date>
<booktitle>Proceedings of the European Conference on Artificial Intelligence</booktitle>
<location>Spain</location>
<marker>Cimiano, Hotho, Staab, 2004</marker>
<rawString>Cimiano, P., Hotho, A., Staab, S. (2004). Comparing conceptual, divisive and agglomerative clustering for learning taxonomies from text. Proceedings of the European Conference on Artificial Intelligence, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Degeratu</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>An automatic model for constructing domain-Specific ontology resources</title>
<date>2004</date>
<booktitle>International Conference on Language Resources and Evaluation</booktitle>
<pages>2001--2004</pages>
<location>Portugal</location>
<marker>Degeratu, Hatzivassiloglou, 2004</marker>
<rawString>Degeratu, M., Hatzivassiloglou, V. (2004). An automatic model for constructing domain-Specific ontology resources. International Conference on Language Resources and Evaluation, Portugal, pp. 2001-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dietterich</author>
</authors>
<title>Ensemble learning. The handbook of brain theory and neural networks. Second Edition. Cambridge MA</title>
<date>2002</date>
<publisher>The MIT Press</publisher>
<contexts>
<context>sifiers, the application of ensemble learning (Opitz and Maclin, 1997) is proposed. The combination of various disagreeing classifiers leads to a resulting classifier with better overall predictions (Dietterich, 2002). Experiments have been conducted using the aforementioned classifiers in various combination schemes using bagging, boosting (as mentioned previously) and stacking (Kermanidis, 2007a). C4.5 IB1 Naïv</context>
<context>correctly by older ones. Finally, stacking usually combines the models created by different base classifiers, unlike bagging and stacking where all base models are constructed by the same classifier (Dietterich, 2002). After constructing the different base models, a new instance is fed into them, and each model predicts a class label. These predictions form the input to another, higher-level classifier (the so-ca</context>
</contexts>
<marker>Dietterich, 2002</marker>
<rawString>Dietterich, T. (2002). Ensemble learning. The handbook of brain theory and neural networks. Second Edition. Cambridge MA, The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Domingos</author>
</authors>
<title>Metacost: A general method for making classifiers cost-sensitive</title>
<date>1999</date>
<booktitle>Proceedings of the International Conference on Knowledge Discovery and Data Mining</booktitle>
<pages>155--164</pages>
<location>San Diego, CA</location>
<contexts>
<context>ng of the minority class until it consists of as many examples as the majority class (Japkowicz, 2000), undersampling of the majority class (random or focused), the use of cost-sensitive classifiers (Domingos, 1999), the ROC convex hull method (Provost and Fawcett, 2001). In this work we employ random and focused undersampling. Another interesting aspect of the present work is the language itself. Modern Greek </context>
</contexts>
<marker>Domingos, 1999</marker>
<rawString>Domingos, P. (1999). Metacost: A general method for making classifiers cost-sensitive. Proceedings of the International Conference on Knowledge Discovery and Data Mining, San Diego, CA, pp. 155-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Faure</author>
<author>C Nedellec</author>
</authors>
<title>A Corpus-based conceptual clustering method for verb frames and ontology</title>
<date>1998</date>
<booktitle>Proceedings of the LREC Workshop on Adapting Lexical and Corpus Resources to Sublanguages and Applications</booktitle>
<location>Granada, Spain</location>
<contexts>
<context>e automatically generated ontology is compared against a hand-crafted gold standard ontology for the tourism domain and a maximum lexical recall of 44.6% is reported. Other clustering approaches are (Faure and Nedellec, 1998; Pereira et al., 1993). The former uses a parsed text and utilizes iterative clustering to form new concept graphs. The latter also makes use of verb-object dependencies, relative frequencies and rel</context>
</contexts>
<marker>Faure, Nedellec, 1998</marker>
<rawString>Faure, D., Nedellec., C. (1998). A Corpus-based conceptual clustering method for verb frames and ontology. Proceedings of the LREC Workshop on Adapting Lexical and Corpus Resources to Sublanguages and Applications, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R E Schapire</author>
</authors>
<title>Experiments with a new boosting algorithm</title>
<date>1996</date>
<booktitle>Proceedings of the International Conference on Machine Learning</booktitle>
<pages>148--156</pages>
<location>San Francisco</location>
<contexts>
<context> is assigned to the test instance. Unlike bagging, where the models are created separately, boosting works iteratively, i.e. each new model is influenced by the performance of those built previously (Freund and Schapire, 1996). In other words, new models are forced, by appropriate weighting, to focus on instances that have been handled incorrectly by older ones. Finally, stacking usually combines the models created by dif</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Freund, Y., Schapire, R. E. (1996). Experiments with a new boosting algorithm. Proceedings of the International Conference on Machine Learning, San Francisco, pp. 148-156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Hatzigeorgiu</author>
</authors>
<title>Design and implementation of the online ILSP Greek corpus</title>
<date>2000</date>
<booktitle>2nd International Conference on Language Resources and Evaluation</booktitle>
<pages>1737--1742</pages>
<location>Athens, Greece</location>
<marker>Hatzigeorgiu, 2000</marker>
<rawString>Hatzigeorgiu, N., et al. (2000). Design and implementation of the online ILSP Greek corpus. 2nd International Conference on Language Resources and Evaluation, Athens, Greece, pp. 1737−1742.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora</title>
<date>1992</date>
<booktitle>Proceedings of the International Conference on Computational Linguistics</booktitle>
<pages>539--545</pages>
<location>Nantes, France</location>
<contexts>
<context>erns that govern the co occurrence of two terms, is significant for extracting taxonomic information. For languages with a strict sentence structure, like English, such patterns are easier to detect (Hearst, 1992), and their impact on taxonomy learning more straightforward. Modern Greek presents a larger degree of freedom in the ordering of the constituents of a sentence, due to its rich morphology and its co</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Hearst, M. A. (1992). Automatic acquisition of hyponyms from large text corpora. Proceedings of the International Conference on Computational Linguistics, Nantes, France, pp. 539-545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Japkowicz</author>
</authors>
<title>The class imbalance problem: significance and strategies</title>
<date>2000</date>
<booktitle>Proceedings of the International Conference on Artificial Intelligence</booktitle>
<location>Las Vegas</location>
<contexts>
<context>instances (null class instances). Class imbalance has been dealt with in previous work in various ways: oversampling of the minority class until it consists of as many examples as the majority class (Japkowicz, 2000), undersampling of the majority class (random or focused), the use of cost-sensitive classifiers (Domingos, 1999), the ROC convex hull method (Provost and Fawcett, 2001). In this work we employ rando</context>
</contexts>
<marker>Japkowicz, 2000</marker>
<rawString>Japkowicz, N. (2000). The class imbalance problem: significance and strategies. Proceedings of the International Conference on Artificial Intelligence, Las Vegas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kermanidis</author>
<author>N Fakotakis</author>
<author>G Kokkinakis</author>
</authors>
<title>DELOS: An automatically tagged economic corpus for Modern Greek</title>
<date>2002</date>
<booktitle>3rd International Conference on Language Resources and Evaluation, Las Palmas de Gran Canaria</booktitle>
<pages>93--100</pages>
<location>Spain</location>
<contexts>
<context>ave been used are two document collections: one domain-specific (economic), and one balanced. The use of the two corpora allows for Corpora Comparison for term extraction. The domain-specific corpus (Kermanidis et al., 2002) is a collection of economic texts of approximately 5M words of varying genre, which has been automatically annotated from the ground up to the level of phrase chunking. The balanced corpus (Hatzigeo</context>
</contexts>
<marker>Kermanidis, Fakotakis, Kokkinakis, 2002</marker>
<rawString>Kermanidis, K., Fakotakis, N., Kokkinakis, G. (2002). DELOS: An automatically tagged economic corpus for Modern Greek. 3rd International Conference on Language Resources and Evaluation, Las Palmas de Gran Canaria, Spain, pp. 93-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kermanidis</author>
</authors>
<title>Ensemble learning of economic taxonomy relations from Modern Greek corpora</title>
<date>2007</date>
<booktitle>2nd International Conference on Metadata and Semantics Research</booktitle>
<location>Corfu, Greece</location>
<contexts>
<context>king into account a window of tokens preceding and following the candidate entity. Various experiments have been run to determine the optimal window size, which depends on the entities to be learned (Kermanidis, 2007b). A total of 40,000 tokens were manually tagged with their class value. An interesting observation is the imbalance between the populations of the positive instances (entities) in the dataset (that </context>
<context>all predictions (Dietterich, 2002). Experiments have been conducted using the aforementioned classifiers in various combination schemes using bagging, boosting (as mentioned previously) and stacking (Kermanidis, 2007a). C4.5 IB1 Naïve Bayes SVM Is-a 0.808 0.694 0.419 0.728 Part-of 0.4 00 0 Attribute 0.769 0.765 0.77 0.788 Null 0.938 0.904 0.892 0.907 Table 3: Class f-score for various classifiers. Bagging entails</context>
</contexts>
<marker>Kermanidis, 2007</marker>
<rawString>Kermanidis, K. (2007a). Ensemble learning of economic taxonomy relations from Modern Greek corpora. 2nd International Conference on Metadata and Semantics Research, Corfu, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kermanidis</author>
</authors>
<title>Identifying boundaries and semantic labels of economic entities using stacking and re-sampling</title>
<date>2007</date>
<booktitle>Internaational Workshop on Natural Language Processing and Cognitive Science</booktitle>
<location>Madeira, Portugal</location>
<contexts>
<context>king into account a window of tokens preceding and following the candidate entity. Various experiments have been run to determine the optimal window size, which depends on the entities to be learned (Kermanidis, 2007b). A total of 40,000 tokens were manually tagged with their class value. An interesting observation is the imbalance between the populations of the positive instances (entities) in the dataset (that </context>
<context>all predictions (Dietterich, 2002). Experiments have been conducted using the aforementioned classifiers in various combination schemes using bagging, boosting (as mentioned previously) and stacking (Kermanidis, 2007a). C4.5 IB1 Naïve Bayes SVM Is-a 0.808 0.694 0.419 0.728 Part-of 0.4 00 0 Attribute 0.769 0.765 0.77 0.788 Null 0.938 0.904 0.892 0.907 Table 3: Class f-score for various classifiers. Bagging entails</context>
</contexts>
<marker>Kermanidis, 2007</marker>
<rawString>Kermanidis K. (2007b). Identifying boundaries and semantic labels of economic entities using stacking and re-sampling. Internaational Workshop on Natural Language Processing and Cognitive Science, Madeira, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kermanidis</author>
<author>N Fakotakis</author>
</authors>
<title>One-sided sampling for learning taxonomic relations in the Modern Greek economic domain</title>
<date>2007</date>
<booktitle>IEEE International Conference on Tools with Artificial Intelligence</booktitle>
<location>Patras, Greece</location>
<contexts>
<context> 17% to the attribute class and only 0.5% to the part-of class. The is-a and part-of classes are significantly underrepresented, and this class imbalance has been dealt with using one-sided sampling (Kermanidis and Fakotakis, 2007). One–sided sampling removes from the dataset negative instances that are redundant, noisy or borderline (close to the boundary line that separates the positive from the negative instances). One-side</context>
</contexts>
<marker>Kermanidis, Fakotakis, 2007</marker>
<rawString>Kermanidis,  K., Fakotakis, N. (2007). One-sided sampling for learning taxonomic relations in the Modern Greek economic domain. IEEE International Conference on Tools with Artificial Intelligence, Patras, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kubat</author>
<author>S Matwin</author>
</authors>
<title>Addressing the curse of imbalanced training sets</title>
<date>1997</date>
<booktitle>Proceedings of the International Conference on Machine Learning</booktitle>
<pages>179--186</pages>
<contexts>
<context> instances that are redundant, noisy or borderline (close to the boundary line that separates the positive from the negative instances). One-sided sampling (OSS) has been employed in several domains (Kubat and Matwin, 1997; Laurikkala, 2001; Lewis and Gale, 1994), and generally leads to better classification performance than oversampling, and it avoids the problem of arbitrarily assigning initial costs to instances. Ex</context>
</contexts>
<marker>Kubat, Matwin, 1997</marker>
<rawString>Kubat, M., Matwin, S. (1997). Addressing the curse of imbalanced training sets. Proceedings of the International Conference on Machine Learning, pp. 179186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Laurikkala</author>
</authors>
<title>Improving identification of difficult small classes by balancing class distribution</title>
<date>2001</date>
<booktitle>Proceedings of the 8th Conference on Artificial Intelligence in Medicine in Europe</booktitle>
<pages>63--66</pages>
<location>Cascais, Portugal</location>
<contexts>
<context>ndant, noisy or borderline (close to the boundary line that separates the positive from the negative instances). One-sided sampling (OSS) has been employed in several domains (Kubat and Matwin, 1997; Laurikkala, 2001; Lewis and Gale, 1994), and generally leads to better classification performance than oversampling, and it avoids the problem of arbitrarily assigning initial costs to instances. Experiments were run</context>
</contexts>
<marker>Laurikkala, 2001</marker>
<rawString>Laurikkala, J. (2001). Improving identification of difficult small classes by balancing class distribution. Proceedings of the 8th Conference on Artificial Intelligence in Medicine in Europe, Cascais, Portugal, pp. 63-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lendvai</author>
</authors>
<title>Conceptual taxonomy identification in medical documents</title>
<date>2005</date>
<booktitle>International Workshop on Knowledge Discovery and Ontologies</booktitle>
<pages>31--38</pages>
<location>Porto, Portugal</location>
<marker>Lendvai, 2005</marker>
<rawString>Lendvai, P. (2005). Conceptual taxonomy identification in medical documents. International Workshop on Knowledge Discovery and Ontologies, Porto, Portugal, pp. 31-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
<author>W Gale</author>
</authors>
<title>Training text classifiers by uncertainty sampling</title>
<date>1994</date>
<booktitle>Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval</booktitle>
<pages>3--12</pages>
<location>Dublin</location>
<contexts>
<context>rderline (close to the boundary line that separates the positive from the negative instances). One-sided sampling (OSS) has been employed in several domains (Kubat and Matwin, 1997; Laurikkala, 2001; Lewis and Gale, 1994), and generally leads to better classification performance than oversampling, and it avoids the problem of arbitrarily assigning initial costs to instances. Experiments were run using the C4.5 decisi</context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>Lewis, D., Gale, W. (1994). Training text classifiers by uncertainty sampling. Proceedings of the International ACM SIGIR Conference on Research and Development in Information Retrieval, Dublin, pp. 3-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Maedche</author>
<author>R Volz</author>
</authors>
<title>The ontology extraction and maintenance framework Text-To-Onto</title>
<date>2001</date>
<booktitle>Workshop on Integrating Data Mining and Knowledge Mining</booktitle>
<location>San Jose, California</location>
<marker>Maedche, Volz, 2001</marker>
<rawString>Maedche, A., Volz, R. (2001). The ontology extraction and maintenance framework Text-To-Onto. Workshop on Integrating Data Mining and Knowledge Mining, San Jose, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Makagonov</author>
<author>A R Figueroa</author>
<author>K Sboychakov</author>
<author>A Gelbukh</author>
</authors>
<title>Learning a domain ontology from hierarchically structured texts</title>
<date>2005</date>
<booktitle>Proceedings of the 22nd International Conference on Machine Learning</booktitle>
<location>Bonn, Germany</location>
<contexts>
<context>string inclusion. They use a variety of external resources in order to generate a semantic graph of senses. Another approach that makes use of external hierarchically structured textual resources is (Makagonov et al., 2005). An already existing hierarchical structure of technical documents is mapped to the structure of a domain-specific technical ontology. Words are clustered into concepts, and concepts into topics. Th</context>
</contexts>
<marker>Makagonov, Figueroa, Sboychakov, Gelbukh, 2005</marker>
<rawString>Makagonov, P., Figueroa, A. R., Sboychakov, K., Gelbukh, A. (2005). Learning a domain ontology from hierarchically structured texts. Proceedings of the 22nd International Conference on Machine Learning, Bonn, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Schuetze</author>
</authors>
<title>Foundations of statistical natural language processing</title>
<date>1999</date>
<publisher>MIT Press</publisher>
<contexts>
<context>emantics. The value of semantic similarity is an integer with a value ranging from 0 to 10, which denotes the number of common words two context vectors share. It is inspired by the Dice coefficient (Manning and Schuetze, 1999). Another important semantic feature that is taken into account is how ‘diverse’ the semantic properties of a term are, i.e. the number of other terms that a term shares semantic properties with. We </context>
</contexts>
<marker>Manning, Schuetze, 1999</marker>
<rawString>Manning, C., Schuetze., H. (1999). Foundations of statistical natural language processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>P Velardi</author>
</authors>
<title>Learning domain ontologies from document warehouses and dedicated websites</title>
<date>2004</date>
<journal>Computational Linguistics</journal>
<volume>50</volume>
<publisher>MIT Press</publisher>
<marker>Navigli, Velardi, 2004</marker>
<rawString>Navigli, R., Velardi, P. (2004). Learning domain ontologies from document warehouses and dedicated websites. Computational Linguistics, 50(2). MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partners of ESPRIT-291860</author>
</authors>
<title>Unification of the word classes of the ESPRIT Project 860</title>
<date>1986</date>
<tech>Internal Report BU-WKL-0376</tech>
<marker>ESPRIT-291860, 1986</marker>
<rawString>Partners of ESPRIT-291/860. (1986). Unification of the word classes of the ESPRIT Project 860. Internal Report BU-WKL-0376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Opitz</author>
<author>R Maclin</author>
</authors>
<title>Popular ensemble methods: An empirical study</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research</journal>
<volume>11</volume>
<pages>169--198</pages>
<marker>Opitz, Maclin, 1999</marker>
<rawString>Opitz, D., Maclin, R. (1999). Popular ensemble methods: An empirical study. Journal of Artificial Intelligence Research (11), pp. 169-198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S</author>
</authors>
<title>Taxonomy learning –factoring the structure of a taxonomy into a semantic classification decision</title>
<date>2002</date>
<booktitle>Proceedings of the International Conference on Computational Linguistics</booktitle>
<location>Taipei, Taiwan</location>
<marker>S, 2002</marker>
<rawString>Pekar, V., Staab. S. (2002). Taxonomy learning –factoring the structure of a taxonomy into a semantic classification decision. Proceedings of the International Conference on Computational Linguistics, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional clustering of English words</title>
<date>1993</date>
<booktitle>Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics</booktitle>
<contexts>
<context>ontology is compared against a hand-crafted gold standard ontology for the tourism domain and a maximum lexical recall of 44.6% is reported. Other clustering approaches are (Faure and Nedellec, 1998; Pereira et al., 1993). The former uses a parsed text and utilizes iterative clustering to form new concept graphs. The latter also makes use of verb-object dependencies, relative frequencies and relative entropy as simil</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Pereira, F., Tishby, N., Lee, L. (1993). Distributional clustering of English words. Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Platt</author>
</authors>
<title>Fast training of support vector machines using sequential minimal optimization</title>
<date>1998</date>
<booktitle>Advances in kernel methods support vector learning</booktitle>
<editor>B. Schoelkopf, C. Burges, and A. Smola, eds</editor>
<publisher>MIT Press</publisher>
<contexts>
<context> a methodology that copes well with the sparse data problem, and also with noise in the data. A first degree polynomial kernel function was selected and the Sequential Minimal Optimization algorithm (Platt, 1998) was chosen to train the support vector classifier. A Naïve Bayes learner and the 1-NN instance based-learner (IB1) were also experimented with as baseline reference. The f-scores achieved with every</context>
</contexts>
<marker>Platt, 1998</marker>
<rawString>Platt, J. (1998). Fast training of support vector machines using sequential minimal optimization. Advances in kernel methods support vector learning, B. Schoelkopf, C. Burges, and A. Smola, eds. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Provost</author>
<author>T Fawcett</author>
</authors>
<title>Robust classification for imprecise environments</title>
<date>2001</date>
<journal>Machine Learning</journal>
<volume>42</volume>
<pages>203--231</pages>
<contexts>
<context> as many examples as the majority class (Japkowicz, 2000), undersampling of the majority class (random or focused), the use of cost-sensitive classifiers (Domingos, 1999), the ROC convex hull method (Provost and Fawcett, 2001). In this work we employ random and focused undersampling. Another interesting aspect of the present work is the language itself. Modern Greek is a relatively free-word-order language, i.e. the order</context>
</contexts>
<marker>Provost, Fawcett, 2001</marker>
<rawString>Provost, F. and Fawcett, T. (2001). Robust classification for imprecise environments. Machine Learning, 42(3), pp. 203-231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning</title>
<date>1993</date>
<publisher>Morgan Kaufmann Publishers</publisher>
<location>San Mateo, CA</location>
<contexts>
<context>leads to better classification performance than oversampling, and it avoids the problem of arbitrarily assigning initial costs to instances. Experiments were run using the C4.5 decision tree learner (Quinlan, 1993), as a stand-alone learner, as well as a base learner for boosting (Freund and Schapire, 1997). Decision trees were chosen because of their high representational power, which is very significant for </context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>Quinlan, R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
<author>M Rochery</author>
<author>M Rahim</author>
<author>N Gupta</author>
</authors>
<title>Incorporating prior knowledge into boosting</title>
<date>2002</date>
<booktitle>Proceedings of the Nineteenth International Conference on Machine Learning</booktitle>
<marker>Schapire, Rochery, Rahim, Gupta, 2002</marker>
<rawString>Schapire, R. E., Rochery, M., Rahim, M., Gupta, N. (2002). Incorporating prior knowledge into boosting. Proceedings of the Nineteenth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Stamatatos</author>
<author>N Fakotakis</author>
<author>G Kokkinakis</author>
</authors>
<title>A practical chunker for unrestricted text</title>
<date>2000</date>
<booktitle>Proceedings of the Conference on Natural Language Processing</booktitle>
<pages>139--150</pages>
<location>Patras, Greece</location>
<contexts>
<context>zigeorgiu et al., 2000; Partners, 1986) is a collection of 300,000 words, manually annotated with morphological information. Phrase chunking was performed on both corpora using the tool described in (Stamatatos et al., 2000). 3.1. Semantic Entity Recognition Identifying names is an important step towards the automatic extraction of domain terms, as each of these names may constitute a candidate term. Each token in the d</context>
</contexts>
<marker>Stamatatos, Fakotakis, Kokkinakis, 2000</marker>
<rawString>Stamatatos, E., Fakotakis, N. and Kokkinakis, G.. (2000). A practical chunker for unrestricted text. Proceedings of the Conference on Natural Language Processing, Patras, Greece, pp. 139-150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Thanopoulos</author>
<author>K Kermanidis</author>
<author>N Fakotakis</author>
</authors>
<title>Challenges in extracting terminology from Modern Greek texts. Workshop on Text-based Information Retrieval</title>
<date>2006</date>
<location>Italy</location>
<contexts>
<context>ημερίδα or περιοδικό helps identify them accurately. 4. Extracting Economic Terms The next step of the procedure is the automatic extraction of economic terms, following the methodology described in (Thanopoulos et al., 2006). Corpora comparison was employed for the extraction of economic terms. Corpora comparison detects the difference in statistical behavior that a term presents in a balanced and in a domain-specific c</context>
</contexts>
<marker>Thanopoulos, Kermanidis, Fakotakis, 2006</marker>
<rawString>Thanopoulos, A., Kermanidis, K., Fakotakis, N. (2006). Challenges in extracting terminology from Modern Greek texts. Workshop on Text-based Information Retrieval, Italy.</rawString>
</citation>
</citationList>
</algorithm>

