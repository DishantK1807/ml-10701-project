BusTUC A natural language bus route oracle Tore A m b l e Dept.
of computer and information science University of Trondheim Norway, N-7491 amble@idi, nt nu.
no Abstract The paper describes a natural language based expert system route advisor for the public bus transport in Trondheim, Norway.
The system is available on the Internet,and has been intstalled at the bus company's web server since the beginning of 1999.
The system is bilingual, relying on an internal language independent logic representation.
In between the question and the answer is a process of lexical analysis, syntax analysis, semantic analysis, pragmatic reasoning and database query processing.
One could argue that the information content could be solved by an interrogation, whereby the customer is asked to produce 4 items: s t a t i o n of departure, station of arrival, earliest departure timeand/or latest arrival time.
It Introduction A natural language interface to a computer database provides users with the capability of obtaining information stored in the database by querying the system in a natural language (NL).
With a natural language as a means of communication with a computer system, the users can make a question or a statement in the way they normally think about the information being discussed, freeing them from having to know how the computer stores or processes the information.
The present implementation represents a a major effort in bringing natural language into practical use.
A system is developed that can answer queries about bus routes, stated as natural language texts, and made public through the Internet World Wide Web is a myth that natural language is a better way of communication because it is "natural language".
The challenge is to prove by demonstration that an NL system can be made that will be preferred to the interrogative mode.
To do that, the system has to be correct, user friendly and almost complete within the actual domain.
P r e v i o u s Efforts, C H A T 8 0, P R A T 8 9 and HSQL Trondheim is a small city with a university and 140000 inhabitants.
Its central bus systems has 42 bus lines, serving 590 stations, with 1900 departures per day (in average).
T h a t gives approximately 60000 scheduled bus station passings per day, which is somehow represented in the route data base.
The starting point is to automate the function of a route information agent.
The following example of a system response is using an actual request over telephone to the local route information company: Hi, I live in Nidarvoll and tonight i must reach a train to Oslo at 6 oclock.
The system, called BusTUC is built upon the classical system CHAT-80 (Warren and Pereira, 1982).
CHAT-80 was a state of the art natural language system that was impressive on its own merits, but also established Prolog as a viable and competitive language for Artificial Intelligence in general.
The system was a brilliant masterpiece of software, efficient and sophisticated.
The natural language system was connected to a small query system for international geography.
The following query could be analysed and answered in a split second: Which country bordering the Mediterranean borders a country that is bordered by a country whose population exceeds the population of India?
(The answer 'Turkey' has become incorrect as time has passed.
The irony is that Geography was chosen as a domain without time.) and a typical answer would follow quickly: Bus number 54 passes by Nidarvoll skole at 1710 and arrives at Trondheim Railway Station at 1725.
The abi!ity to answer ridiculously long queries is of course not the main goal.
The main lesson is that complex sentences are analysed with a proper understanding without sacrificing efficiency.
Any superfificial pattern matching technique would prove futile sooner or later.
Making a N o r w e g i a n CHAT-80, PRAT-89 At the University of Trondheim (NTNU), two students made a Norwegian version of CHAT-80,called PRAT-89 (Teigen and Vetland, 1988),(Teigen and Vetland, 1989).
(Also, a similar Swedish project SNACK-85 was reported).
The dictionary was changed from English to Norwegian together with new rules for morphological analysis.
The change of grammar from English to Norwegian proved to be amazingly easy.
It showed that the langauges were more similar than one would believe, given that the languages are incomprehensible to each other's communities.
After changing the dictionary and graramar, the following Norwegian query about the same domain could be answered correctly in a few seconds.
Hvilke afrikanske land som hat en befolkning stoerre enn 3 millioner og mindre enn 50 millioner og er nord for Botswana og oest for Libya hat en hovedstad som hat en befolkning stoerre enn 100 tusen.
Coupling the s y s t e m to an SQL database.
After the remodelling, the system could answer queries in "Scandinavian" to an internal hospital database as well as CHAT-80 could answer Geography questions.
HSQL produced a Prolog-like code FOL (First Order Logic) for execution.
A mapping from FOL to the data base Schema was defined, and a translator from FOL to SQL was implemented.
The example Hvilke menn ligger i en kvinnes seng?
(Which men lie in a woman's bed?
) would be translated dryly into the SQL query: SELECT DISTINCT T3.name,Tl.sex,T2.reg_no,T3.sex, T4.reg_no,T4.bed_no,T5.hosp_no,T5.ward_no FROM PATIENT TI,OCCUPANCY T2,PATIENT T3, OCCUPANCY T4,WARD T5 WHERE (Tl.sex='f') AND (T2.reg_no=Tl.reg_no) AND (T3.sex='m') AND (T4.reg_no=T3.reg_no) AND (T4.bed_no=T2.bed_no) AND (T5.hosp_no=T4.hosp_no) AND (T5.ward_no=T4.ward_no) 2.3 T h e T h e U n d e r s t a n d i n g C o m p u t e r The HSQL was a valuable experience in the effort to make transportable natural language interfaces.
However, the underlying system CHAT-80 restricted the further development.
After the HSQL Project was finished, an internal reseach project TUC (the Understanding Computer) was initiated at NTNU to carry on the results from HSQL.
The project goals differed from those of HSQL in a number of ways, and would not be concerned with multimedia interfaces. On the other hand, portability and versatility were made central issues concerning the generality of the language and its applications.
The research goals could be summarised as to Give computers an operational understanding of natural language.
 Build intelligent systems with natural language capabilities.
 Study common sense reasoning in natural language.
A test criterion for the understanding capacity is that after a set of definitions in a Naturally Readable Logic, NRL, the system's answer to queries in NRL should conform to the answers of an idealised rational agent.
( A translation is beside the point o.f being a long query in Norwegian.) 2.2 HSQL H e l p S y s t e m for SQL A Nordic project HSQL (Help System for SQL) was accomplished in 1988-89 to make a joint Nordic effort interfaces to databases.
The HSQL project was led by the Swedish State Bureau (Statskontoret), with participants from Sweden, Denmark, Finland and Norway (Amble et al., 1990).
The aim of HSQL was to build a natural language interface to SQL databases for the Scandinavian languages Swedish, Danish and Norwegian.
These languages are very similar, and the Norwegian version of CHAT-80 was easily extended to the other Scandinavian languages.
Instead of Geography, a more typical application area was chosen to be a query system for hospital administration.
We decided to target an SQL database of a hospital administration which had been developed already.
The next step was then to change the domain of discourse from Geography to hospital administration, using the same knowledge representation techniques used in CHAT-80.
A semantic model of this domain was made, and then implemented in the CHAT-80 framework.
The modelling technique that proved adequate was to use an extended Entity Relationship (ER) model with a class (type) hierarchy, attributes belonging to each class, single inheritance of attributes and relationships.
Every man that lives loves Mary.
John is a man.
John lives.
Who loves Mary?
==> John 3 Anatomy of the bus route oracle The main components of the bus route information systems are:  A parser system, consisting of a dictionary, a lexical processor, a grammar and a parser.
 A knowledge base (KB), divided into a semantic KB and an application KB  A query processor, contalng a routing logic system, and a route data base.
The system is bilingual and contains a double set of dictionary, morphology and grammar.
Actually, it detects which language is most probable by counting the number of unknown words related to each language, and acts accordingly.
The grammars are surprisingly similar, but no effort is made to coalesce them.
The Norwegian grammar is slightly bigger than the English grammar, mostly because it is more elaborated but also because Norwegian allows a freer word order.
3.1 Features
of BusTUC For the Norwegian systems, the figures give an indication of the size of the domain: 420 nouns, 150 verbs, 165 adjectives, 60 prepositions, etc.
There are 1300 grammar rules ( 810 for English) although half of the rules are very low level.
The semantic net described below contains about 4000 entries.
A big name table of 3050 names in addition to the official station names, is required to capture the variety of naming.
A simple spell correction is a part of the system ( essentially 1 character errors).
The pragmatic reasoning is needed to translate the output from the parser to a route database query language . This is done by a production system called Pragma, which acts like an advanced rewriting system with 580 rules.
In addition, there is another rule base for actually generating the natural language answers (120 rules).
The system is mainly written in Prolog (Sicstus Prolog 3.7), with some Perl programs for the communication and CGI-scripts.
At the moment, there are about 35000 lines of programmed Prolog code (in addition to route tables which are also in Prolog).
Average response time is usually less than 2 seconds, but there are queries that demand up to 10 seconds.
The error rate for single, correct, complete and relevant questions is about 2 percent.
NRL is defined in a closed context.
Thus interfaces to other systems are in principle defined through simulating the environment as a dialogue partner.
TUC is a prototypical natural language processor for English written in Prolog.
It is designed to be a general purpose easily adaptable natural language processor.
It consists of a general grammar for a subset of English, a semantic knowledge base, and modules for interfaces to other interfaces like UNIX, SQL-databases and general textual information sources.
2.4 The
TABOR Project It so happened that a Universtity Project was starteded in 1996, called T A B O R ( " Speech based user interfaces and reasoning systems "), with the aim of building an automatic public transport route oracle, available over the public telephone.
At the onset of the project, the World Wide Web was fresh, and not as widespread as today, and the telephone was still regarded as the main source of information for the public.
Since then, the Internet became the dominant medium, and it is as likeley to find a computer with Internet connection, as to find a local busroute table.
( The consequtive wide spreading of cellular phones changed the picture in favour of the telephone, but that is another story).
It was decided that a text based information system should be built, regardless of the status of the speech rocgnition and speech synthesis effort, which proved to lag behind after a while.
The BusTUC system The resulting system BusTUC grew out as a natural application of TUC, and an English prototype could be built within a few months (Bratseth, 1997).
Since the summer 1996, the prototype was put onto the Internet, and been developed and tested more or less continually until today.
The most important extension was that the system was made bilingual (Norwegian and English) during the fall 1996.
In spring 1999, the BusTUC was finally adopted by the local bus company in Trondheim ( A/S Trondheim Trafikkselskap), which set up a server ( a 300 MHz PC with Linux).
Until today, over 150.000 questions have been answered, and BusTUC seems to stabilize and grow increasingly popular.
3.2 The
Parser S y s t e m The G r a m m a r S y s t e m The grammar is based on a simple grammar for statements, while questions and commands are derived by the use of movements.
The grammar 3 fiformalism which is called Consensical Grammar, (CONtext SENSitive CompositionAL Grammar) is an easy to use variant of Extraposition Grammar (Pereira and Warren, 1980), which is a generalisation of Definite Clause Grammars.
Compositional grammar means that the semantics of a a phrase is composed of the semantics of the subphrases; the basic constituents being a form of verb complements.
As for Extraposition grammars, a grammar is translated to Definite Clause Grammars, and executed as such.
A characteristic syntactic expression in Consensical G r a m m a r m a y define an incomplete construct in terms of a "difference " between complete constructs.
W h e n possible, the parser will use the subtracted part in stead of reading from the input, after a gap if necessary.
The effect is the same as for Exwhich is analysed as for which X is it true that the (X) person has a dog that barked? where the last line is analysed as a s t a t e m e n t . Movement is easily handled in Consensical Grammar without making special phrase rules for each kind of movement.
The following example shows how TUC manages a variety of analyses using movements: Max said Bill thought Joe believed Fido Barked.
Who said Bill thought Joe believed Fido barked?
Who did Max say thought Joe believed Fido barked? traposition grammars, but the this format is more intuitive.
Examples of grammar rules.
Who did Max say Bill thought believed Fido barked?
T h e parser The experiences with Consensical grammars are a bit mixed however.
The main problem is the parsing method itself, which is top down with backtracking.
Many principles that would prove elegant for small domains turned out to be too costly for larger domains, due to the wide variety of modes of expressions, incredible ambiguities and the sheer size of the covered language.
The disambiguation is a major problem for small grammars and large languages, and was solved by the following guidelines:  a semantic type checking was integrated into the parser, and would help to discard sematica/ly wrong parses from the start.
 a heuristics was followed that proved almost irreproachable: The longest possible phrase of a category that is semantically correct is in most cases the preferred interpretation.
 due to the perplexity of the language, some committed choices (cuts) had to be inserted into the grammar at strategic places.
As one could fear however, this implied that wrong choices being made at some point in the parsing could not be recovered by backtracking.
These problems also made it imperative to introduce a timeout on the parsing process of embarassing 10 seconds.
Although most sentences, would be parsed within a second, some legal sentences of moderate size actually need this time.
4 Example: Whose dog barked? is analysed as if the sentence had been Who has a dog t h a t barked? which is analysed as Which p e r s o n has a dog t h a t barked? fi3.3 The semantic knowledge base Adaptability means that the system does not need to be reprogrammed for each new application.
The design principle of TUC is that most of the changes are made in a tabular semantic knowledge base, while there is one general grammar and dictionary.
In general, the logic is generated automatically from the semantic knowledge base.
The nouns play a key role in the understanding part as they constitute the class or type hierarchy.
Nouns are defined in an a k i n d o f hierarchy.
The hierarchy is tree-structured with single inheritance.
The top level also constitute the top level ontology of TUC's world.
In fact, a type check of the compliances of verbs, nouns adjectives and prepositions is not only necessary for the semantic processing but is essential for the syntax analysis for the disambiguation as well.
In TUC, the legal combinations are carefully assembled in the semantic network, which then serves a dual purpose.
These semantic definitions are necessary to allow for instance the following sentences The dog saw a man with a telescope.
The man saw a dog with a telescope.
gives exactly the same code.
% Type of question % tuc is a program % A is a real bus % B isa saturday % Nidar is a place % D is an event Y.
C was known at D Y.
E is an event in C action(go,E), Y.
the action of E is Go actor(A,E), Y.
the actor of E is A srel(to,place,nidar,E),Y.
E is to nidar srel(on,time,B,E), y, E is on the saturday B to be treated differently because with telescope m a y modify the noun man but not the noun dog, while with telescope modifies the verb see, restricted to person.
The event parameter plays an important role in the semantics.
It is used for various purposes.
The most salient role is to identify a subset of time and space in which an action or event occured.
Both the actual time and space coordinates are connected to the actions through the event parameter.
Pragmatic reasoning The TQL is translated to a route database query language (BusLOG) which is actually a Prolog program.
This is done by a production system called Pragma, which acts like an advanced rewriting system with 580 rules.
In addition, there is another rule base for actually generating the natural language answers (120 rules).
4 Conclusions
The TUC approach has as its goal to automate the creation of new natural language interfaces for a well defined subset of the language and with a minimum of explicit programming.
The implemented system has proved its worth, and is interesting if for no other reason.
There is also an increasing interest from other bus companies and route information companies alike to get a similar system for their customers.
Further work remains to make the parser really efficient, and much work remains to make the language coverage complete within reasonable limits.
It is an open question whether the system of this kind will be a preferred way of offering information to the public.
If it is, it is a fair amount of work to make it a portable system that can be implemented elsewhere, also connecting various travelling agencies.
If not, it will remain a curiosity.
But anyway, a system like this will be a contribution to the development of intelligent systems.
3.4 The
Query Processor Event Calculus The semantics of the phrases are built up by a kind of verb complements, where the event play a central role.
The text is translated from Natural language into a form called TQL (Temporal Query Language/ TUC Query Language) which is a first order event calculus expression, a self contained expression containing the literal meaning of an utterance.
A formalism TQL that was defined, inspired by the Event Calculus by Kowalski and Sergot (Kowalski and Sergot, 1986).
The TQL expressions consist of predicates, functions, constants and variables.
The textual words of nouns and verbs are translated to generic predicates using the selected interpretation.
The following question Do you know whether the bus goes to Nidar on Saturday ? would give the TQL expression below.
BusTUC A natural language bus route oracle Tore A m b l e Dept.
of computer and information science University of Trondheim Norway, N-7491 amble@idi, nt nu.
no Abstract The paper describes a natural language based expert system route advisor for the public bus transport in Trondheim, Norway.
The system is available on the Internet,and has been intstalled at the bus company's web server since the beginning of 1999.
The system is bilingual, relying on an internal language independent logic representation.
In between the question and the answer is a process of lexical analysis, syntax analysis, semantic analysis, pragmatic reasoning and database query processing.
One could argue that the information content could be solved by an interrogation, whereby the customer is asked to produce 4 items: s t a t i o n of departure, station of arrival, earliest departure timeand/or latest arrival time.
It Introduction A natural language interface to a computer database provides users with the capability of obtaining information stored in the database by querying the system in a natural language (NL).
With a natural language as a means of communication with a computer system, the users can make a question or a statement in the way they normally think about the information being discussed, freeing them from having to know how the computer stores or processes the information.
The present implementation represents a a major effort in bringing natural language into practical use.
A system is developed that can answer queries about bus routes, stated as natural language texts, and made public through the Internet World Wide Web is a myth that natural language is a better way of communication because it is "natural language".
The challenge is to prove by demonstration that an NL system can be made that will be preferred to the interrogative mode.
To do that, the system has to be correct, user friendly and almost complete within the actual domain.
P r e v i o u s Efforts, C H A T 8 0, P R A T 8 9 and HSQL Trondheim is a small city with a university and 140000 inhabitants.
Its central bus systems has 42 bus lines, serving 590 stations, with 1900 departures per day (in average).
T h a t gives approximately 60000 scheduled bus station passings per day, which is somehow represented in the route data base.
The starting point is to automate the function of a route information agent.
The following example of a system response is using an actual request over telephone to the local route information company: Hi, I live in Nidarvoll and tonight i must reach a train to Oslo at 6 oclock.
The system, called BusTUC is built upon the classical system CHAT-80 (Warren and Pereira, 1982).
CHAT-80 was a state of the art natural language system that was impressive on its own merits, but also established Prolog as a viable and competitive language for Artificial Intelligence in general.
The system was a brilliant masterpiece of software, efficient and sophisticated.
The natural language system was connected to a small query system for international geography.
The following query could be analysed and answered in a split second: Which country bordering the Mediterranean borders a country that is bordered by a country whose population exceeds the population of India?
(The answer 'Turkey' has become incorrect as time has passed.
The irony is that Geography was chosen as a domain without time.) and a typical answer would follow quickly: Bus number 54 passes by Nidarvoll skole at 1710 and arrives at Trondheim Railway Station at 1725.
The abi!ity to answer ridiculously long queries is of course not the main goal.
The main lesson is that complex sentences are analysed with a proper understanding without sacrificing efficiency.
Any superfificial pattern matching technique would prove futile sooner or later.
Making a N o r w e g i a n CHAT-80, PRAT-89 At the University of Trondheim (NTNU), two students made a Norwegian version of CHAT-80,called PRAT-89 (Teigen and Vetland, 1988),(Teigen and Vetland, 1989).
(Also, a similar Swedish project SNACK-85 was reported).
The dictionary was changed from English to Norwegian together with new rules for morphological analysis.
The change of grammar from English to Norwegian proved to be amazingly easy.
It showed that the langauges were more similar than one would believe, given that the languages are incomprehensible to each other's communities.
After changing the dictionary and graramar, the following Norwegian query about the same domain could be answered correctly in a few seconds.
Hvilke afrikanske land som hat en befolkning stoerre enn 3 millioner og mindre enn 50 millioner og er nord for Botswana og oest for Libya hat en hovedstad som hat en befolkning stoerre enn 100 tusen.
Coupling the s y s t e m to an SQL database.
After the remodelling, the system could answer queries in "Scandinavian" to an internal hospital database as well as CHAT-80 could answer Geography questions.
HSQL produced a Prolog-like code FOL (First Order Logic) for execution.
A mapping from FOL to the data base Schema was defined, and a translator from FOL to SQL was implemented.
The example Hvilke menn ligger i en kvinnes seng?
(Which men lie in a woman's bed?
) would be translated dryly into the SQL query: SELECT DISTINCT T3.name,Tl.sex,T2.reg_no,T3.sex, T4.reg_no,T4.bed_no,T5.hosp_no,T5.ward_no FROM PATIENT TI,OCCUPANCY T2,PATIENT T3, OCCUPANCY T4,WARD T5 WHERE (Tl.sex='f') AND (T2.reg_no=Tl.reg_no) AND (T3.sex='m') AND (T4.reg_no=T3.reg_no) AND (T4.bed_no=T2.bed_no) AND (T5.hosp_no=T4.hosp_no) AND (T5.ward_no=T4.ward_no) 2.3 T h e T h e U n d e r s t a n d i n g C o m p u t e r The HSQL was a valuable experience in the effort to make transportable natural language interfaces.
However, the underlying system CHAT-80 restricted the further development.
After the HSQL Project was finished, an internal reseach project TUC (the Understanding Computer) was initiated at NTNU to carry on the results from HSQL.
The project goals differed from those of HSQL in a number of ways, and would not be concerned with multimedia interfaces. On the other hand, portability and versatility were made central issues concerning the generality of the language and its applications.
The research goals could be summarised as to Give computers an operational understanding of natural language.
 Build intelligent systems with natural language capabilities.
 Study common sense reasoning in natural language.
A test criterion for the understanding capacity is that after a set of definitions in a Naturally Readable Logic, NRL, the system's answer to queries in NRL should conform to the answers of an idealised rational agent.
( A translation is beside the point o.f being a long query in Norwegian.) 2.2 HSQL H e l p S y s t e m for SQL A Nordic project HSQL (Help System for SQL) was accomplished in 1988-89 to make a joint Nordic effort interfaces to databases.
The HSQL project was led by the Swedish State Bureau (Statskontoret), with participants from Sweden, Denmark, Finland and Norway (Amble et al., 1990).
The aim of HSQL was to build a natural language interface to SQL databases for the Scandinavian languages Swedish, Danish and Norwegian.
These languages are very similar, and the Norwegian version of CHAT-80 was easily extended to the other Scandinavian languages.
Instead of Geography, a more typical application area was chosen to be a query system for hospital administration.
We decided to target an SQL database of a hospital administration which had been developed already.
The next step was then to change the domain of discourse from Geography to hospital administration, using the same knowledge representation techniques used in CHAT-80.
A semantic model of this domain was made, and then implemented in the CHAT-80 framework.
The modelling technique that proved adequate was to use an extended Entity Relationship (ER) model with a class (type) hierarchy, attributes belonging to each class, single inheritance of attributes and relationships.
Every man that lives loves Mary.
John is a man.
John lives.
Who loves Mary?
==> John 3 Anatomy of the bus route oracle The main components of the bus route information systems are:  A parser system, consisting of a dictionary, a lexical processor, a grammar and a parser.
 A knowledge base (KB), divided into a semantic KB and an application KB  A query processor, contalng a routing logic system, and a route data base.
The system is bilingual and contains a double set of dictionary, morphology and grammar.
Actually, it detects which language is most probable by counting the number of unknown words related to each language, and acts accordingly.
The grammars are surprisingly similar, but no effort is made to coalesce them.
The Norwegian grammar is slightly bigger than the English grammar, mostly because it is more elaborated but also because Norwegian allows a freer word order.
3.1 Features
of BusTUC For the Norwegian systems, the figures give an indication of the size of the domain: 420 nouns, 150 verbs, 165 adjectives, 60 prepositions, etc.
There are 1300 grammar rules ( 810 for English) although half of the rules are very low level.
The semantic net described below contains about 4000 entries.
A big name table of 3050 names in addition to the official station names, is required to capture the variety of naming.
A simple spell correction is a part of the system ( essentially 1 character errors).
The pragmatic reasoning is needed to translate the output from the parser to a route database query language . This is done by a production system called Pragma, which acts like an advanced rewriting system with 580 rules.
In addition, there is another rule base for actually generating the natural language answers (120 rules).
The system is mainly written in Prolog (Sicstus Prolog 3.7), with some Perl programs for the communication and CGI-scripts.
At the moment, there are about 35000 lines of programmed Prolog code (in addition to route tables which are also in Prolog).
Average response time is usually less than 2 seconds, but there are queries that demand up to 10 seconds.
The error rate for single, correct, complete and relevant questions is about 2 percent.
NRL is defined in a closed context.
Thus interfaces to other systems are in principle defined through simulating the environment as a dialogue partner.
TUC is a prototypical natural language processor for English written in Prolog.
It is designed to be a general purpose easily adaptable natural language processor.
It consists of a general grammar for a subset of English, a semantic knowledge base, and modules for interfaces to other interfaces like UNIX, SQL-databases and general textual information sources.
2.4 The
TABOR Project It so happened that a Universtity Project was starteded in 1996, called T A B O R ( " Speech based user interfaces and reasoning systems "), with the aim of building an automatic public transport route oracle, available over the public telephone.
At the onset of the project, the World Wide Web was fresh, and not as widespread as today, and the telephone was still regarded as the main source of information for the public.
Since then, the Internet became the dominant medium, and it is as likeley to find a computer with Internet connection, as to find a local busroute table.
( The consequtive wide spreading of cellular phones changed the picture in favour of the telephone, but that is another story).
It was decided that a text based information system should be built, regardless of the status of the speech rocgnition and speech synthesis effort, which proved to lag behind after a while.
The BusTUC system The resulting system BusTUC grew out as a natural application of TUC, and an English prototype could be built within a few months (Bratseth, 1997).
Since the summer 1996, the prototype was put onto the Internet, and been developed and tested more or less continually until today.
The most important extension was that the system was made bilingual (Norwegian and English) during the fall 1996.
In spring 1999, the BusTUC was finally adopted by the local bus company in Trondheim ( A/S Trondheim Trafikkselskap), which set up a server ( a 300 MHz PC with Linux).
Until today, over 150.000 questions have been answered, and BusTUC seems to stabilize and grow increasingly popular.
3.2 The
Parser S y s t e m The G r a m m a r S y s t e m The grammar is based on a simple grammar for statements, while questions and commands are derived by the use of movements.
The grammar 3 fiformalism which is called Consensical Grammar, (CONtext SENSitive CompositionAL Grammar) is an easy to use variant of Extraposition Grammar (Pereira and Warren, 1980), which is a generalisation of Definite Clause Grammars.
Compositional grammar means that the semantics of a a phrase is composed of the semantics of the subphrases; the basic constituents being a form of verb complements.
As for Extraposition grammars, a grammar is translated to Definite Clause Grammars, and executed as such.
A characteristic syntactic expression in Consensical G r a m m a r m a y define an incomplete construct in terms of a "difference " between complete constructs.
W h e n possible, the parser will use the subtracted part in stead of reading from the input, after a gap if necessary.
The effect is the same as for Exwhich is analysed as for which X is it true that the (X) person has a dog that barked? where the last line is analysed as a s t a t e m e n t . Movement is easily handled in Consensical Grammar without making special phrase rules for each kind of movement.
The following example shows how TUC manages a variety of analyses using movements: Max said Bill thought Joe believed Fido Barked.
Who said Bill thought Joe believed Fido barked?
Who did Max say thought Joe believed Fido barked? traposition grammars, but the this format is more intuitive.
Examples of grammar rules.
Who did Max say Bill thought believed Fido barked?
T h e parser The experiences with Consensical grammars are a bit mixed however.
The main problem is the parsing method itself, which is top down with backtracking.
Many principles that would prove elegant for small domains turned out to be too costly for larger domains, due to the wide variety of modes of expressions, incredible ambiguities and the sheer size of the covered language.
The disambiguation is a major problem for small grammars and large languages, and was solved by the following guidelines:  a semantic type checking was integrated into the parser, and would help to discard sematica/ly wrong parses from the start.
 a heuristics was followed that proved almost irreproachable: The longest possible phrase of a category that is semantically correct is in most cases the preferred interpretation.
 due to the perplexity of the language, some committed choices (cuts) had to be inserted into the grammar at strategic places.
As one could fear however, this implied that wrong choices being made at some point in the parsing could not be recovered by backtracking.
These problems also made it imperative to introduce a timeout on the parsing process of embarassing 10 seconds.
Although most sentences, would be parsed within a second, some legal sentences of moderate size actually need this time.
4 Example: Whose dog barked? is analysed as if the sentence had been Who has a dog t h a t barked? which is analysed as Which p e r s o n has a dog t h a t barked? fi3.3 The semantic knowledge base Adaptability means that the system does not need to be reprogrammed for each new application.
The design principle of TUC is that most of the changes are made in a tabular semantic knowledge base, while there is one general grammar and dictionary.
In general, the logic is generated automatically from the semantic knowledge base.
The nouns play a key role in the understanding part as they constitute the class or type hierarchy.
Nouns are defined in an a k i n d o f hierarchy.
The hierarchy is tree-structured with single inheritance.
The top level also constitute the top level ontology of TUC's world.
In fact, a type check of the compliances of verbs, nouns adjectives and prepositions is not only necessary for the semantic processing but is essential for the syntax analysis for the disambiguation as well.
In TUC, the legal combinations are carefully assembled in the semantic network, which then serves a dual purpose.
These semantic definitions are necessary to allow for instance the following sentences The dog saw a man with a telescope.
The man saw a dog with a telescope.
gives exactly the same code.
% Type of question % tuc is a program % A is a real bus % B isa saturday % Nidar is a place % D is an event Y.
C was known at D Y.
E is an event in C action(go,E), Y.
the action of E is Go actor(A,E), Y.
the actor of E is A srel(to,place,nidar,E),Y.
E is to nidar srel(on,time,B,E), y, E is on the saturday B to be treated differently because with telescope m a y modify the noun man but not the noun dog, while with telescope modifies the verb see, restricted to person.
The event parameter plays an important role in the semantics.
It is used for various purposes.
The most salient role is to identify a subset of time and space in which an action or event occured.
Both the actual time and space coordinates are connected to the actions through the event parameter.
Pragmatic reasoning The TQL is translated to a route database query language (BusLOG) which is actually a Prolog program.
This is done by a production system called Pragma, which acts like an advanced rewriting system with 580 rules.
In addition, there is another rule base for actually generating the natural language answers (120 rules).
4 Conclusions
The TUC approach has as its goal to automate the creation of new natural language interfaces for a well defined subset of the language and with a minimum of explicit programming.
The implemented system has proved its worth, and is interesting if for no other reason.
There is also an increasing interest from other bus companies and route information companies alike to get a similar system for their customers.
Further work remains to make the parser really efficient, and much work remains to make the language coverage complete within reasonable limits.
It is an open question whether the system of this kind will be a preferred way of offering information to the public.
If it is, it is a fair amount of work to make it a portable system that can be implemented elsewhere, also connecting various travelling agencies.
If not, it will remain a curiosity.
But anyway, a system like this will be a contribution to the development of intelligent systems.
3.4 The
Query Processor Event Calculus The semantics of the phrases are built up by a kind of verb complements, where the event play a central role.
The text is translated from Natural language into a form called TQL (Temporal Query Language/ TUC Query Language) which is a first order event calculus expression, a self contained expression containing the literal meaning of an utterance.
A formalism TQL that was defined, inspired by the Event Calculus by Kowalski and Sergot (Kowalski and Sergot, 1986).
The TQL expressions consist of predicates, functions, constants and variables.
The textual words of nouns and verbs are translated to generic predicates using the selected interpretation.
The following question Do you know whether the bus goes to Nidar on Saturday ? would give the TQL expression below.
Machine Translation of Very Close Languages Jan HAJI(~ Computer Science Dept.
Johns Hopkins University 3400 N.
Charles St., Baltimore, MD 21218, USA hajic@cs.jhu.edu Jan HRIC KTI MFF UK Malostransk6 nfim.25 Praha 1, Czech Republic, 11800 hric@barbora.m ff.cuni.cz Vladislav KUBON OFAL MFF UK Malostransk6 mim.25 Praha 1, Czech Republic, 11800 vk@ufal.mff.cuni.cz Abstract Using examples of the transfer-based MT system between Czech and Russian RUSLAN and the word-for-word MT system with morphological disambiguation between Czech and Slovak (~ESILKO we argue that for really close languages it is possible to obtain better translation quality by means of simpler methods.
The problem of translation to a group of typologically similar languages using a pivot language is also discussed here.
demonstrate that this assumption holds only for really very closely related languages.
1. Czech-to-Russian MT system RUSLAN 1.1 History Introduction Although the field of machine translation has a very long history, the number of really successful systems is not very impressive.
Most of the funds invested into the development of various MT systems have been wasted and have not stimulated a development of techniques which would allow to translate at least technical texts from a certain limited domain.
There were, of course, exceptions, which demonstrated that under certain conditions it is possible to develop a system which will save money and efforts invested into human translation.
The main reason why the field of MT has not met the expectations of sci-fi literature, but also the expectations of scientific community, is the complexity of the task itself.
A successful automatic translation system requires an application of techniques from several areas of computational linguistics (morphology, syntax, semantics, discourse analysis etc).
as a necessary, but not a sufficient condition.
The general opinion is that it is easier to create an MT system for a pair of related languages.
In our contribution we would like to The first attempt to verify the hypothesis that related languages are easier to translate started in mid 80s at Charles University in Prague.
The project was called RUSLAN and aimed at the translation of documentation in the domain of operating systems for mainframe computers.
It was developed in cooperation with the Research Institute of Mathematical Machines in Prague.
At that time in former COMECON countries it was obligatory to translate any kind of documentation to such systems into Russian.
The work on the Czech-to-Russian MT system RUSLAN (cf.
Oliva (1989)) started in 1985.
It was terminated in 1990 (with COMECON gone) for the lack of funding.
System description The system was rule-based, implemented in Colmerauer's Q-systems.
It contained a fullfledged morphological and syntactic analysis of Czech, a transfer and a syntactic and morphological generation of Russian.
There was almost no transfer at the beginning of the project due to the assumption that both languages are similar to the extent that does not require any transfer phase at all.
This assumption turned to be wrong and several phenomena were covered by the transfer in the later stage of the project (for example the translation of the Czech verb "b~" [to be] into one of the three possible Russian equivalents: empty form, the form "byt6" in future fitense and the verb "javljat6sja"; or the translation of verbal negation).
At the time when the work was terminated in 1990, the system had a main translation dictionary of about 8000 words, accompanied by so called transducing dictionary covering another 2000 words.
The transducing dictionary was based on the original idea described in Kirschner (1987).
It aimed at the exploitation o f the fact that technical terms are based (in a majority o f European languages) on Greek or Latin stems, adopted according to the particular derivational rules o f the given languages.
This fact allows for the "translation" o f technical terms by means of a direct transcription of productive endings and a slight (regular) adjustment o f the spelling of the stem.
For example, the English words localization and discrimination can be transcribed into Czech as "lokalizace" and "diskriminace" with a productive ending -ation being transcribed to -ace.
It was generally assumed that for the pair Czech/Russian the transducing dictionary would be able to profit from a substantially greater number o f productive rules.
This hypothesis proved to be wrong, too (see B6mov~, Kubofi (1990)).
The set o f productive endings for both pairs (English/Czech, as developed for an earlier MT system from English to Czech, and Czech/Russian) was very similar.
The evaluation o f results o f RUSLAN showed that roughly 40% o f input sentences were translated correctly, about 40% with minor errors correctable by a human post-editor and about 20% of the input required substantial editing or re-translation.
There were two main factors that caused a deterioration of the translation.
The first factor was the incompleteness o f the main dictionary of the system.
Even though the system contained a set of so-called fail-soft rules, whose task was to handle such situations, an unknown word typically caused a failure o f the module o f syntactic analysis, because the dictionary entries contained besides the translation equivalents and morphological information very important syntactic information.
The second factor was the module of syntactic analysis o f Czech.
There were several reasons of parsing failures.
Apart from the common inability of most rule-based formal grammars to cover a particular natural language to the finest detail o f its syntax there were other problems.
One o f them was the existence of non-projective constructions, which are quite common in Czech even in relatively short sentences.
Even though they account only for 1.7/'o of syntactic dependencies, every third Czech sentence contains at least one, and in a news corpus, we discovered as much as 15 non-projective dependencies; see also Haji6 et al.(1998). An example o f a non-projective construction is "Soubor se nepodafilo otev~it".
[lit.: File Refl.
was_not._possible to_open.
It was not possible to open the file].
The formalism used for the implementation (Q-systems) was not meant to handle non-projective constructions.
Another source of trouble was the use o f so-called semantic features.
These features were based on lexical semantics o f individual words.
Their main task was to support a semantically plausible analysis and to block the implausible ones.
It turned out that the question o f implausible combinations o f semantic features is also more complex than it was supposed to be.
The practical outcome o f the use o f semantic features was a higher ratio of parsing failures semantic features often blocked a plausible analysis.
For example, human lexicographers assigned the verb 'to run' a semantic feature stating that only a noun with semantic features o f a human or other living being may be assigned the role o f subject of this verb.
The input text was however full o f sentences with 'programs' or 'systems' running etc.
It was o f course very easy to correct the semantic feature in the dictionary, but the problem was that there were far too many corrections required.
On the other hand, the fact that both languages allow a high degree o f word-order freedom accounted for a certain simplification o f the translation process.
The grammar relied on the fact that there are only minor word-order differences between Czech and Russian.
1.3 Lessons
learned from RUSLAN We have learned several lessons regarding the MT o f closely related languages:  The transfer-based approach provides a similar quality o f translation both for closely related and typologically different languages  Two main bottlenecks o f full-fledged transfer-based systems are: ficomplexity o f the syntactic dictionary relative unreliability o f the syntactic analysis of the source language Even a relatively simple component (transducing dictionary) was equally complex for English-to-Czech and Czech-to-Russian translation Limited text domains do not exist in real life, it is necessary to work with a high coverage dictionary at least for the source language.
2. Translation and localization 2.1 A pivot language Localization o f products and their documentation is a great problem for any company, which wants to strengthen its position on foreign language market, especially for companies producing various kinds o f software.
The amounts o f texts being localized are huge and the localization costs are huge as well.
It is quite clear that the localization from one source language to several target languages, which are typologically similar, but different from the source language, is a waste of money and effort.
It is o f course much easier to translate texts from Czech to Polish or from Russian to Bulgarian than from English or German to any o f these languages.
There are several reasons, why localization and translation is not being performed through some pivot language, representing a certain group o f closely related languages.
Apart from political reasons the translation through a pivot language has several drawbacks.
The most important one is the problem o f the loss o f translation quality.
Each translation may to a certain extent shift the meaning o f the translated text and thus each subsequent translation provides results more and more different from the original.
The second most important reason is the lack of translators from the pivot to the target language, while this is usually no problem for the translation from the source directly to the target language.
MAHT (Machine-aided human translation) systems.
We have chosen the TRADOS Translator's Workbench as a representative system o f a class o f these products, which can be characterized as an example-based translation tools.
IBM's Translation Manager and other products also belong to this class.
Such systems uses so-called translation memory, which contains pairs o f previously translated sentences from a source to a target language.
When a human translator starts translating a new sentence, the system tries to match the source with sentences already stored in the translation memory.
If it is successful, it suggests the translation and the human translator decides whether to use it, to modify it or to reject it.
The segmentation o f a translation memory is a key feature for our system.
The translation memory may be exported into a text file and thus allows easy manipulation with its content.
Let us suppose that we have at our disposal two translation memories one human made for the source/pivot language pair and the other created by an MT system for the pivot/target language pair.
The substitution o f segments o f a pivot language by the segments of a target language is then only a routine procedure.
The human translator translating from the source language to the target language then gets a translation memory for the required pair (source/target).
The system o f penalties applied in TRADOS Translator's Workbench (or a similar system) guarantees that if there is already a human-made translation present, then it gets higher priority than the translation obtained as a result o f the automatic MT.
This system solves both problems mentioned above the human translators from the pivot to the target language are not needed at all and the machinemade translation memory serves only as a resource supporting the direct human translation from the source to the target language.
3. M a c h i n e translation o f (very) closely related Slavic languages In the group o f Slavic languages, there are more closely related languages than Czech and Russian.
Apart from the pair o f Serbian and Croatian languages, which are almost identical and were Translation memory is the key The main goal of this paper is to suggest how to overcome these obstacles by means o f a combination of an MT system with commercial ficonsidered one language just a few years ago, the most closely related languages in this group are Czech and Slovak.
This fact has led us to an experiment with automatic translation between Czech and Slovak.
It was clear that application of a similar method to that one used in the system RUSLAN would lead to similar results.
Due to the closeness of both languages we have decided to apply a simpler method.
Our new system, (~ESILKO, aims at a maximal exploitation of the similarity of both languages.
The system uses the method of direct word-for-word translation, justified by the similarity of syntactic constructions of both languages.
Although the system is currently being tested on texts from the domain of documentation to corporate information systems, it is not limited to any specific domain.
Its primary task is, however, to provide support for translation and localization of various technical texts.
3.1 System
( ~ E S i L K O and its governing noun.
An alternative way to the solution of this problem was the application of a stochastically based morphological disambiguator (morphological tagger) for Czech whose success rate is close to 92/'0.
Our system therefore consists of the following modules: 1.
Import of the input from so-called 'empty' translation memory 2.
Morphological analysis of Czech 3.
Morphological disambiguation 4.
Domain-related bilingual glossaries (incl.
singleand multiword terminology) 5.
General bilingual dictionary 6.
Morphological synthesis of Slovak 7.
Export of the output to the original translation memory Letus now look in a more detail at the individual modules of the system: ad 1.
The input text is extracted out of a translation memory previously exported into an ASCII file.
The exported translation memory (of TRADOS) has a SGML-Iike notation with a relatively simple structure (cf.
the following example): Example 1.
A sample of the exported translation memory <RTF Preamble>...</RTF Preamble> <TrU> <CrD>23051999 <CrU>VK <Seg L=CS_01>Pomoci v~kazu ad-hoc m65ete rychle a jednoduge vytv~i~et regerge.
<Seg L=SK_01 >n/a </TrU> Our system uses only the segments marked by <Seg L=CS_01>, which contain one source language sentence each, and <Seg L=SK_01>, which is empty and which will later contain the same sentence translated into the target language The greatest problem of the word-for-word translation approach (for languages with very similar syntax and word order, but different morphological system) is the problem of morphological ambiguity of individual word forms.
The type of ambiguity is slightly different in languages with a rich inflection (majority of Slavic languages) and in languages which do not have such a wide variety of forms derived from a single lemma.
For example, in Czech there are only rare cases of part-of-speech ambiguities (st~t [to stay/the state], zena [woman/chasing] or tri [three/rub(imperative)]), much more frequent is the ambiguity of gender, number and case (for example, the form of the adjective jam[ [spring] is 27-times ambiguous).
The main problem is that even though several Slavic languages have the same property as Czech, the ambiguity is not preserved.
It is distributed in a different manner and the "form-for-form" translation is not applicable.
Without the analysis of at least nominal groups it is often very difficult to solve this problem, because for example the actual morphemic categories of adjectives are in Czech distinguishable only on the basis of gender, number and case agreement between an adjective by CESiLKO.
ad 2.
The morphological analysis of Czech is based on the morphological dictionary developed by Jan Haji6 and Hana Skoumalov~i in 1988-99 (for latest description, see Haji~ (1998)).
The dictionary contains over 700 000 dictionary entries and its typical coverage varies between fi99% (novels) to 95% (technical texts).
The morphological analysis uses the system of positional tags with 15 positions (each morphological.category, such as Part-of-speech, Number, Gender, Case, etc.
has a fixed, singlesymbol place in the tag).
Example 2 tags assigned to the word-form "pomoci" (help/by means of) pomoci: NFP2 ......
A ....
]NFS7 ......
A ....
I R--2 ........... where : N noun; R preposition F feminine gender S singular, P plural 7, 2 case (7 instrumental, 2 genitive) A affirmative (non negative) ad 3.
The module of morphological disambiguation is a key to the success o f the translation.
It gets an average number of 3.58 tags per token (word form in text) as an input.
The tagging system is purely statistical, and it uses a log-linear model of probability distribution see Haji~, Hladkfi (1998).
The learning is based on a manually tagged corpus of Czech texts (mostly from the general newspaper domain).
The system learns contextual rules (features) automatically and also automatically determines feature weights.
The average accuracy o f tagging is between 91 and 93% and remains the same even for technical texts (if we disregard the unknown names and foreign-language terms that are not ambiguous anyway).
The lemmatization immediately follows tagging; it chooses the first lemma with a possible tag corresponding to the tag selected.
Despite this simple lemmatization method, and also thanks to the fact that Czech words are rarely ambiguous in their Part-of-speech, it works with an accuracy exceeding 98%.
The multiple-word terms are sequences of lemmas (not word forms).
This structure has several advantages, among others it allows to minimize the size of the dictionary and also, due to the simplicity of the structure, it allows modifications of the glossaries by the linguistically naive user.
The necessary morphological information is introduced into the domain-related glossary in an off-line preprocessing stage, which does not require user intervention.
This makes a big difference when compared to the RUSLAN Czech-to-Russian MT system, when each multiword dictionary entry cost about 30 minutes of linguistic expert's time on average.
ad 5.
The main bilingual dictionary contains data necessary for the translation o f both lemmas and tags.
The translation of tags (from the Czech into the Slovak morphological system) is necessary, because due to the morphological differences both systems use close, but slightly different tagsets.
Currently the system handles the 1:1 translation of tags (and 2:2, 3:3, etc.).
Different ratio of translation is very rare between Czech and Siovak, but nevertheless an advanced system of dictionary items is under construction (for the translation 1:2, 2:1 etc.).
It is quite interesting that the lexically homonymous words often preserve their homonymy even after the translation, so no special treatment of homonyms is deemed necessary.
ad 6.
The morphological synthesis of Slovak is based on a monolingual dictionary of SIovak, developed by J.Hric (1991-99), covering more than ]00,000 dictionary entries.
The coverage of the dictionary is not as high as o f the Czech one, but it is still growing.
It aims at a similar coverage of Slovak as we enjoy for Czech.
ad 7.
The export o f the output of the system (~ESILKO into the translation memory (of TRADOS Translator's Workbench) amounts mainly to cleaning of all irrelevant SGML markers.
The whole resulting Slovak sentence is inserted into the appropriate location in the original translation memory file.
The following example also shows that the marker <CrU> contains an information that the target language sentence was created by an M T system.
ad 4.
The domain-related bilingual glossaries contain pairs of individual words and pairs of multiple-word terms.
The glossaries are organized into a hierarchy specified by the user; typically, the glossaries for the most specific domain are applied first.
There is one general matching rule for all levels of glossaries the longest match wins.
languages, namely for Czech-to-Polish translation.
Although these languages are not so similar as Czech and Slovak, we hope that an addition of a simple partial noun phrase parsing might provide results with the quality comparable to the fullfledged syntactic analysis based system RUSLAN (this is of course true also for the Czechoto-Slovak translation).
The first results of Czech-to Polish translation are quite encouraging in this respect, even though we could not perform as rigorous testing as we did for Slovak.
Acknowledgements 3.2 Evaluation of results The problem how to evaluate results of automatic translation is very difficult.
For the evaluation of our system we have exploited the close connection between our system and the TRADOS Translator's Workbench.
The method is simple the human translator receives the translation memory created by our system and translates the text using this memory.
The translator is free to make any changes to the text proposed by the translation memory.
The target text created by a human translator is then compared with the text created by the mechanical application of translation memory to the source text.
TRADOS then evaluates the percentage of matching in the same manner as it normally evaluates the percentage of matching of source text with sentences in translation memory.
Our system achieved about 90% match (as defined by the TRADOS match module) with the results of human translation, based on a relatively large (more than 10,000 words) test sample.
This project was supported by the grant GAt~R 405/96/K214 and partially by the grant GA(~R 201/99/0236 and project of the Ministry of Education No.
VS96151. The accuracy of the translation achieved by our system justifies the hypothesis that word-forword translation might be a solution for MT of really closely related languages.
The remaining problems to be solved are problems with the oneto many or many-to-many translation, where the lack of information in glossaries and dictionaries sometimes causes an unnecessary translation error.
A u t o m a t i c construction of parallel English-Chinese corpus for cross-language information retrieval Jiang Chen and Jian-Yun Nie D ~ p a r t e m e n t d ' I n f o r m a t i q u e et R e c h e r c h e O p ~ r a t i o n n e l l e Universit~ de M o n t r e a l C.P. 6128, succursale C E N T R E V I L L E M o n t r e a l (Quebec), C a n a d a H 3 C 3 J 7 {chen, nie} @iro.
umontreal, ca Abstract A major obstacle to the construction of a probabilistic translation model is the lack of large parallel corpora.
In this paper we first describe a parallel text mining system that finds parallel texts automatically on the Web.
The generated Chinese-English parallel corpus is used to train a probabilistic translation model which translates queries for Chinese-English cross-language information retrieval (CLIR).
We will discuss some problems in translation model training and show the preliminary C U R results.
1 Introduction
2 Parallel Text Mining Algorithm The PTMiner system is an intelligent Web agent that is designed to search for large amounts of parallel text on the Web.
The mining algorithm is largely language independent.
It can thus be adapted to other language pairs with only minor modifications.
Taking advantage of Web search engines as much as possible, PTMiner implements the following steps (illustrated in Fig.
1): 1 Search for candidate sites Using existing Web search engines, search for the candidate sites that may contain parallel pages; 2 File name fetching For each candidate site, fetch the URLs of Web pages that are indexed by the search engines; 3 Host crawling Starting from the URLs collected in the previous step, search through each candidate site separately for more URLs; 4 Pair scan From the obtained URLs of each site, scan for possible parallel pairs; 5 Download and verifying Download the parallel pages, determine file size, language, and character set of each page, and filter out non-parallel pairs.
2.1 Search
for candidate Sites We take advantage of the huge number of Web sites indexed by existing search engines in determining candidate sites.
This is done by submitting some particular requests to the search engines.
The requests are determined according to the following observations.
In the sites where parallel text exists, there are normally some pages in one language containing links to the parallel version in the other language.
These are usually indicated by those links' anchor texts 1.
For example, on some English page there may be a link to its Chinese version with the anchor text "Chinese Version" or "in Chinese".
1An a n c h o r t e x t is a piece of text on a W e b page which, w h e n clicked on, will take you to a n o t h e r linked page.
To be helpful, it u s u a l l y c o n t a i n s t h e key i n f o r m a t i o n about the linked page.
Parallel texts have been used in a number of studies in computational linguistics.
Brown et al.(1993) defined a series of probabilistic translation models for MT purposes.
While people may question the effectiveness of using these models for a full-blown MT system, the models are certainly valuable for developing translation assistance tools.
For example, we can use such a translation model to help complete target text being drafted by a human translator (Langlais et al., 2000).
Another utilization is in cross-language information retrieval (CLIR) where queries have to be translated from one language to another language in which the documents are written.
In CLIR, the quality requirement for translation is relatively low.
For example, the syntactic aspect is irrelevant.
Even if the translated word is not a true translation but is strongly related to the original query, it is still helpful.
Therefore, CLIR is a suitable application for such a translation model.
However, a major obstacle to this approach is the lack of parallel corpora for model training.
Only a few such corpora exist, including the Hansard English-French corpus and the HKUST EnglishChinese corpus (Wu, 1994).
In this paper, we will describe a method which automatically searches for parallel texts on the Web.
We will discuss the text mining algorithm we adopted, some issues in translation model training using the generated parallel corpus, and finally the translation model's performance in CLIR.
21 Figure 1: The workflow of the mining process.
T h e same phenomenon can be observed on Chinese pages.
Chances are t h a t a site with parallel texts will contain such links in some of its documents.
This fact is used as the criterion in searching for candidate sites.
Therefore, to determine possible sites for EnglishChinese parallel texts, we can request an English document containing the following anchor: Host Crawling anchor : "english version H ["in english",...].
Similar requests are sent for Chinese documents.
From the two sets of pages obtained by the above queries we extract two sets of Web sites.
T h e union of these two sets constitutes then the candidate sites.
T h a t is to say, a site is a candidate site when it is found to have either an English page linking to its Chinese version or a Chinese page linking to its English version.
A host crawler is slightly different from a Web crawler.
Web crawlers go through innumerable pages and hosts on the Web.
A host crawler is a Web crawler t h a t crawls through documents on a given host only.
A breadth-first crawling algorithm is applied in P T M i n e r as host crawler.
The principle is t h a t when a link to an unexplored document on the same site is found in a document, it is added to a list t h a t will be explored later.
In this way, most file names from the candidate sites are obtained.
Pair Scan File N a m e Fetching We now assume t h a t a pair of parallel texts exists on the same site.
To search for parallel pairs on a site, P T M i n e r first has to obtain all (or at least p a r t of) the H T M L file names on the site.
From these names pairs are scanned.
It is possible to use a Web crawler to explore the candidate sites completely.
However, we can take advantage of the search engines again to accelerate the process.
As the first step, we submit the following query to the search engines: to fetch the Web pages t h a t they indexed from this site.
If we only require a small a m o u n t of parallel texts, this result m a y be sufficient.
For our purpose, however, we need to explore the sites more thoroughly using a host crawler.
Therefore, we continue our search for files with a host crawler which uses the documents found by the search engines as the starting point.
After collecting file names for each candidate site, the next task is to determine the parallel pairs.
Again, we t r y to use some heuristic rules to guess which files m a y be parallel texts before downloading them.
The rules are based on external features of the documents.
By external feature, we mean those features which m a y be known without analyzing the contents of the file, such as its URL, size, and date.
This is in contrast with the internal features, such as language, character set, and H T M L structure, which cannot be known until we have downloaded the page and analyzed its contents.
The heuristic criterion comes from the following observation: We observe t h a t parallel text pairs usually have similar n a m e patterns.
The difference between the names of two parailel pages usually lies in a segment which indicates the language.
For example, "file-ch.html" (in Chinese) vs.
"file-en.html" (in English).
T h e difference m a y also appear in the path, such as ".../chinese/.../file.html" vs.
".../english/.../file.html'. T h e n a m e patterns described above are commonly used by webmasters to help organize their sites.
Hence, we can suppose t h a t a pair of pages with this kind of p a t t e r n are probably parallel texts.
First, we establish four lists for English prefixes, English suffixes, Chinese prefixes and Chinese suffixes.
For example: E n g l i s h P r e f i x = {e, en, e_, en_, e -, e n -, ...}.
For each file in one language, if a segment in its name corresponds to one of the language affixes, several new names are generated by changing the segment to the possible corresponding affixes of the other language.
If a generated name corresponds to an existing file, then the file is considered as a candidate parallel document of the original file.
Filtering Next, we further examine the contents of the paired files to determine if they are really parallel according to various external and internal features.
This may further improve the pairing precision.
The following methods have been implemented in our system.
usually have similar H T M L structures.
However, we also noticed that parallel texts may have quite different HTML structures.
One of the reasons is that the two files may be created using two HTML editors.
For example, one may be used for English and another for Chinese, depending on the language handling capability of the editors.
Therefore, caution is required when measuring structure difference numerically.
Parallel text alignment is still an experimental area.
Measuring the confidence values of an alignment is even more complicated.
For example, the alignment algorithm we used in the training of the statistical translation model produces acceptable alignment results but it does not provide a confidence value that we can "confidently" use as an evaluation criterion.
So, for the moment this criterion is not used in candidate pair evaluation.
Generated Corpus and Translation Model Training In this section, we describe the results of our parallel text mining and translation model training.
3 Text
Length Parallel files often have similar file lengths.
One simple way to filter out incorrect pairs is to compare the lengths of the two files.
The only problem is to set a reasonable threshold that will not discard too many good pairs, i.e. balance recall and precision.
The usual difference ratio depends on the language pairs we are dealing with.
For example, ChineseEnglish parallel texts usually have a larger difference ratio than English-French parallel texts.
The filtering threshold had to be determined empirically, from the actual observations.
For Chinese-English, a difference up to 50% is tolerated.
2.5.2 L
a n g u a g e a n d Character Set It is also obvious that the two files of a pair have to be in the two languages of interest.
By automatically identifying language and character set, we can filter out the pairs that do not satisfy this basic criterion.
Some Web pages explicitly indicate the language and the character set.
More often such information is omitted by authors.
We need some language identification tool for this task.
SILC is a language and encoding identification system developed by the RALI laboratory at the University of Montreal.
It employs a probabilistic model estimated on tri-grams.
Using these models, the system is able to determine the most probable language and encoding of a text (Isabelle et al., 1997).
2.5.3 H
T M L Structure and Alignment In the STRAND system (Resnik, 1998), the candidate pairs are evaluated by aligning them according to their H T M L structures and computing confidence values.
Pairs are assumed to be wrong if they have too many mismatching markups or low confidence values.
Comparing H T M L structures seems to be a sound way to evaluate candidate pairs since parallel pairs 23 The Corpus Using the above approach for Chinese-English, 185 candidate sites were searched from the domain hk.
We limited the mining domain to hk because Hong Kong is a bilingual English-Chinese city where high quality parallel Web sites exist.
Because of the small number of candidate sites, the host crawler was used to thoroughly explore each site.
The resulting corpus contains 14820 pairs of texts including 117.2Mb Chinese texts and 136.5Mb English texts.
The entire mining process lasted about a week.
Using length comparison and language identification, we refined the precision of the corpus to about 90%.
The precision is estimated by examining 367 randomly picked pairs.
Statistical Translation Model Many approaches in computational linguistics try to extract translation knowledge from previous translation examples.
Most work of this kind establishes probabilistic models from parallel corpora.
Based on one of the statistical models proposed by Brown et al.(1993), the basic principle of our translation model is the following: given a corpus of aligned sentences, if two words often co-occur in the source and target sentences, there is a good likelihood that they are translations of each other.
In the simplest case (model 1), the model learns the probability, p(tls), of having a word t in the translation of a sentence containing a word s.
For an input sentence, the model then calculates a sequence of words that are most probable to appear in its translation.
Using a similar statistical model, Wu (1995) extracted a largescale English-Chinese lexicon from the H K U S T corFigure 2: An alignment example using pure length-based method.
pus which is built manually.
In our case, the probabilistic translation model will be used for CLIR.
The requirement on our translation model may be less demanding: it is not absolutely necessary that a word t with high p(tls ) always be a true translation of s.
It is still useful if t is strongly related to s.
For example, although "railway" is not a true translation of "train" (in French), it is highly useful to include "railway" in the translation of a query on "train".
This is one of the reasons why we think a less controlled parallel corpus can be used to train a translation model for CLIR.
very noisy.
Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.
A number of alignment techniques have been proposed, varying from statistical methods (Brown et al., 1991; Gale and Church, 1991) to lexical methods (Kay and RSscheisen, 1993; Chen, 1993).
The method we adopted is t h a t of Simard et al.(1992). Because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length-based methods.
Cognates are identical sequences of characters in corresponding words in two languages.
T h e y are commonly found in English and French.
In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the H T M L markup in both texts are taken as cognates.
Because the H T M L structures of parallel pages are normally similar, the markup was found to be helpful for alignment.
To illustrate how markup can help with the alignment, we align the same pair with both the pure length-based method of Gale & Church (Fig.
2), and the method of Simard et al.(Fig. 3).
First of all, we observe from the figures that the two texts are Parallel Text Alignment Before the mined documents can be aligned into parallel sentences, the raw texts have to undergo a series of some preprocessing, which, to some extent, is language dependent.
For example, the major operations on the Chinese-English corpus include encoding scheme transformation (for Chinese), sentence level segmentation, parallel text alignment, Chinese word segmentation (Nie et al., 1999) and English expression extraction.
The parallel Web pages we collected from various sites are not all of the same quality.
Some are highly parallel and easy to align while others can be Figure 3: An alignment example considering cognates.
divided into sentences.
The sentences are marked by <s i d = " x x x x " > and < / s > . Note that we determine sentences not only by periods, but also by means of H T M L markup.
We further notice that it is difficult to align sentences 0002.
The sentence in the Chinese page is much longer than its counterpart in the English page because some additional information (font) is added.
The length-based method thus tends to take sentence 0002, 0003, and 0004 in the English page as the translation of sentence 0002 in the Chinese page (Fig.
2), which is wrong.
This in turn provocated the three following incorrect alignments.
As we can see in Fig.
3, the cognate method did not make the same mistake because of the noise in sentence 0002.
Despite their large length difference, the two 0002 sentences are still aligned as a 1-1 pair, because the sentences in the following 4 alignments (0003 0003; 0004 0004, 0005; 0005 0006; 0006 0007) have rather similar H T M L markups and are taken by the program to be the most likely alignments.
Beside HTML markups, other criteria may also be incorporated.
For example, it would be helpful to consider strong correspondence between certain English and Chinese words, as in (Wu, 1994).
We hope to implement such correspondences in our future research.
3.4 Lexicon
Evaluation To evaluate the precision of the English-Chinese translation model trained on the Web corpus, we examined two sample lexicons of 200 words, one in each direction.
The 200 words for each lexicon were randomly selected from the training source.
We examined the most probable translation for each word.
The Chinese-English lexicon was found to have a precision of 77%.
The English-Chinese lexicon has a higher precision of 81.5%.
Part of the lexicons are shown in Fig.
4, where t / f indicates whether a translation is true or false.
These precisions seem to be reasonably high.
They are quite comparable to that obtained by Wu (1994) using a manual Chinese-English parallel corpus.
Effect of Stopwords We also found that stop-lists have significant effect on the translation model.
Stop-list is a set of the most frequent words that we remove from the train3.5 English word t/f access adaptation add adopt agent agree airline amendment, appliance apply attendance auditor -,average base_on t t t t t t t t t t t/f t t t t t t t t t t Translation office protection report prepare local follow standard adult inadequate part financial visit bill vehicle saving Figure 4: Part of the evaluation lexicons.
Figure 5: Effect of stop lists in C-E translation.
ing source.
Because these words exist in most alignments, the statistical model cannot derive correct translations for them.
More importantly, their existence greatly affects the accuracy of other translations.
They can be taken as translations for many words.
A priori, it would seem that both the English and Chinese stop-lists should be applied to eliminate the noise caused by them.
Interestingly, from our observation and analysis we concluded that for better precision, only the stop-list of the target language should be applied in the model training.
We first explain why the stop-list of the target language has to be applied.
On the left side of Fig.
5, if the Chinese word C exists in the same alignments with the English word E more than any other Chinese words, C will be the most probable translation for E.
Because of their frequent appearance, some Chinese stopwords may have more chances to be in the same alignments with E.
The probability of the translation E --+ C is then reduced (maybe even less than those of the incorrect ones).
This is the reason why many English words are translated to " ~ ' (of) by the translation model trained without using the Chinese stop-list.
We also found that it is not necessary to remove the stopwords of the source language.
In fact, as illustrated on the right side of Fig.
5, the existence of the English stopwords has two effects on the probability of the translation E -~ C: 1 They may often be found together with the Chinese word C.
Owing to the Expectation Maximization algorithm, the probability of E -~ C may therefore be reduced.
2 On
the other hand, there is a greater likelihood that English stopwords will be found together with the most frequent Chinese words.
Here, we use the term "Chinese frequent words" instead of "Chinese stopwords" because even if a stop-list is applied, there may still remain some common words that have the same effect as the stopwords.
The coexistence of English and Chinese frequent words reduces the probability that the Chinese frequent words are the translations of E, and thus raise the probability of E -+ C.
The second effect was found to be more significant than the first, since the model trained without the English stopwords has better precision than the model trained with the English stopwords.
For the correct translations given by both models, the model Mono-Lingual IR Translation Model Dictionary trained without considering the English stopwords gives higher probabilities.
4 English-Chinese CLIR Results Our final goal was to test the performance of the translation models trained on the Web parallel corpora in CLIR.
We conducted CLIR experiments using the Smart IR system.
4.1 Results
The English test corpus (for C-E CLIR) was the AP corpus used in TREC6 and TREC7.
The short English queries were translated manually into Chinese and then translated back to English by the translation model.
The Chinese test corpus was the one used in the TREC5 and TREC6 Chinese track.
It contains both Chinese queries and their English translations.
Our experiments on these two corpora produced the results shown in Tab.
1. The precision of monolingual IR is given as benchmark.
In both E-C and C-E CLIR, the translation model achieved around 40% of monolingual precision.
To compare with the dictionary-based approach, we employed a ChineseEnglish dictionary, CEDICT (Denisowski, 1999), and an English-Chinese online dictionary (Anonymous, 1999a) to translate queries.
For each word of the source query, all the possible translations given by the dictionary are included in the translated query.
The Chinese-English dictionary has about the same performace as the translation model, while the English-Chinese dictionary has lower precision than that of the translation model.
We also tried to combine the translations given by the translation model and the dictionary.
In both C-E and E-C CLIR, significant improvements were achieved (as shown in Tab.
1). The improvements show that the translations given by the translation model and the dictionary complement each other well for IR purposes.
The translation model may give either exact translations or incorrect but related words.
Even though these words are not correct in the sense of translation, they are very possibly related to the subject of the query and thus helpful for IR purposes.
The dictionary-based approach expands a query along another dimension.
It gives all the possible translations for each word including those that are missed by the translation model.
4.2 C
o m p a r i s o n W i t h M T S y s t e m s One advantage of a parallel text-based translation model is that it is easier to build than an MT system.
Now that we have examined the CLIR performance of the translation model, we will compare it with two existing MT systems.
Both systems were tested in E-C CLIR.
4.2.1 S
u n s h i n e W e b T r a n Server Using the Sunshine WebTran server (Anonymous, 1999b), an online Engiish-Chinese MT system, to translate the 54 English queries, we obtained an average precision of 0.2001, which is 50.3% of the mono-lingual precision.
The precision is higher than that obtained using the translation model (0.1804) or the dictionary (0.1427) alone, but lower than the precison obtained using them together (0.2232).
4.2.2 Transperfect
Kwok (1999) investigated the CLIR performance of an English-Chinese MT software called Transperfect, using the same TREC Chinese collection as we used in this study.
Using the MT software alone, Kwok achieved 56% of monolingual precision.
The precision is improved to 62% by refining the translation with a dictionary.
Kwok also adopted pretranslation query expansion, which further improved the precison to 70% of the monolingual results.
In our case, the best E-C CLIR precison using the translation model (and dictionary) is 56.1%.
It is lower than what Kwok achieved using Transperfect, however, the difference is not large.
4.3 F
u r t h e r P r o b l e m s The Chinese-English translation model has a fax lower CLIR performance than that of the EnglishFrench model established using the same method (Nie et al., 1999).
The principal reason for this is the fact that English and Chinese are much more different than English and French.
This problem surfaced in many phases of this work, from text alignment to query translation.
Below, we list some further factors affecting CLIR precision.
 The Web-collected corpus is noisy and it is difficult to align English-Chinese texts.
The alignment method we employed has performed more poorly than on English-French alignment.
This in turn leads to poorer performance of the translation model.
In general, we observe a higher fivariability in Chinese-English translations than in English-French translations.
 For E-C CLIR, although queries in both languages were provided, the English queries were not strictly translated from the original Chinese ones.
For example, A J g, ~ (human right situation) was translated into human right issue.
We cannot expect the translation model to translate issue back to ~ (situation).
 The training source and the CLIR collections were from different domains.
The Web corpus are retrieved from the parallel sites in Hong Kong while the Chinese collection is from People's Daily and Xinhua News Agency, which are published in mainland China.
As the result, some important terms such as ~ $ $ (mostfavored-nation) and --I!!
~ ~ (one-nation-twosystems) in the collection are not known by the model.
5 Summary
The goal of this work was to investigate the feasibility of using a statistical translation model trained on a Web-collected corpus to do English-Chinese CLIR.
In this paper, we have described the algorithm and implementation we used for parallel text mining, translation model training, and some results we obtained in CLIR experiments.
Although further work remains to be done, we can conclude that it is possible to automatically construct a Chinese-English parallel corpus from the Web.
The current system can be easily adapted to other language pairs.
Despite the noisy nature of the corpus and the great difference in the languages, the evaluation lexicons generated by the translation model produced acceptable precision.
While the current CLIR results are not as encouraging as those of English-French CLIR, they could be improved in various ways, such as improving the alignment method by adapting cognate definitions to HTML markup, incorporating a lexicon and/or removing some common function words in translated queries.
We hope to be able to demonstrate in the near future that a fine-tuned English-Chinese translation model can provide query translations for CLIR with the same quality produced by MT systems.
D i s t i l l i n g dialogues A m e t h o d using natural dialogue c o r p o r a for dialogue s y s t e m s d e v e l o p m e n t Arne Department JSnsson and Nils Dahlb~ick of Computer and Information Science LinkSping University S-581 83, L I N K O P I N G SWEDEN nilda@ida.liu.se, arnjo@ida.liu.se Abstract We report on a method for utilising corpora collected in natural settings.
It is based on distilling (re-writing) natural dialogues to elicit the type of dialogue that would occur if one the dialogue participants was a computer instead of a human.
The method is a complement to other means such as Wizard of Oz-studies and un-distilled natural dialogues.
We present the distilling method and guidelines for distillation.
We also illustrate how the method affects a corpus of dialogues and discuss the pros and cons of three approaches in different phases of dialogue systems development.
1 Introduction
on measures for inter-rater reliability (Carletta, 1996), on frameworks for evaluating spoken dialogue agents (Walker et al., 1998) and on the use of different corpora in the development of a particular system (The Carnegie-Mellon Communicator, Eskenazi et al.(1999)). The question we are addressing in this paper is how to collect and analyse relevant corpora.
We begin by describing what we consider to be the main advantages and disadvantages of the two currently used methods; studies of human dialogues and Wizard of Oz-dialogues, especially focusing on the ecological validity of the methods.
We then describe a m e t h o d called 'distilling dialogues', which can serve as a supplement to the other two.
It has been known for quite some time now, that the language used when interacting with a computer is different from the one used in dialogues between people, (c.f.
JSnsson and Dahlb~ick (1988)).
Given that we know that the language will be different, but not how it will be different, we need to base our development of natural language dialogue systems on a relevant set of dialogue corpora.
It is our belief that we need to clarify a number of different issues regarding the collection and use of corpora in the development of speech-only and multimodal dialogue systems.
Exchanging experiences and developing guidelines in this area are as important as, and in some sense a necessary pre-requisite to, the development of computational models of speech, language, and dialogue/discourse.
It is interesting to note the difference in the state of art in the field of natural language dialogue systems with that of corpus linguistics, where issues of the usefulness of different samples, the necessary sampling size, representativeness in corpus design and other have been discussed for quite some time (e.g.
(Garside et al., 1997; Atkins et al., 1992; Crowdy, 1993; Biber, 1993)).
Also the neighboring area of evaluation of NLP systems (for an overview, see Sparck Jones and Galliers (1996)) seems to have advanced further.
Some work have been done in the area of natural language dialogue systems, e.g. on the design of Wizard of Oz-studies (Dahlb~ck et al., 1998), 2 Natural and Wizard of Oz-Dialogues The advantage of using real dialogues between people is that they will illustrate which tasks and needs that people actually bring to a particular service provider.
Thus, on the level of the users' general goals, such dialogues have a high validity.
But there are two drawbacks here.
First; it is not self-evident that users will have the same task expectations from a computer system as they have with a person.
Second, the language used will differ from the language used when interacting with a computer.
These two disadvantages have been the major force behind the development of Wizard of Ozmethods.
The advantage here is that the setting will be human-computer interaction.
But there are important disadvantages, too.
First, on the practical side, the task of setting up a high quality simulation environment and training the operators ('wizards') to use this is a resource consuming task (Dahlb~ck et al., 1998).
Second, and probably even more important, is that we cannot then observe real users using a system for real life tasks, where they bring their own needs, motivations, resources, and constraints to bear.
To some extent this problem can be overcome using well-designed so called 'scenarios'.
As pointed out in Dahlb~ck (1991), on many levels of analysis the artificiality of the situation will not affifect the language used.
An example of this is the pattern of pronoun-antecedent relations.
But since the tasks given to the users are often pre-described by the researchers, this means t h a t this is not a good way of finding out which tasks the users actually want to perform.
Nor does it provide a clear enough picture on how the users will act to find something t h a t satisfies their requirements.
If e.g. the task is one of finding a charter holiday trip or buying a TVset within a specified set of constraints (economical and other), it is conceivable t h a t people will stay with the first item t h a t matches the specification, whereas in real life they would probably look for alternatives.
In our experience, this is primarily a concern if the focus is on the users' goals and plans, but is less a problem when the interest is on lowerlevel aspects, such as, syntax or patterns of pronounantecedent relationship (c.f.
Dahlb~ick (1991)).
To summarize; real life dialogues will provide a reasonably correct picture of the way users' approach their tasks, and what tasks they bring to the service provider, but the language used will not give a good approximation of what the system under construction will need to handle.
Wizard of Ozdialogues, on the other hand, will give a reasonable approximation of some aspects of the language used, but in an artificial context.
The usual approach has been to work in three steps.
First analyse real h u m a n dialogues, and based on these, in the second phase, design one or more Wizard of Oz-studies.
The final step is to fine-tune the system's performance on real users.
A good example of this method is presented in Eskenazi et al.(1999). But there are also possible problems with this approach (though we are not claiming that this was the case in their particular project).
Eskenazi et al.(1999) asked a h u m a n operator to act 'computerlike' in their Wizard of Oz-phase.
The advantage is of course that the h u m a n operator will be able to perform all the tasks t h a t is usually provided by this service.
The disadvantage is t h a t it puts a heavy burden on the h u m a n operator to act as a computer.
Since we know that lay-persons' ideas of what computers can and cannot do are in m a n y respects far removed from what is actually the case, we risk introducing some systematic distortion here.
And since it is difficult to perform consistently in similar situations, we also risk introducing non-systematic distortion here, even in those cases when the 'wizard' is an NLP-professional.
Our suggestion is therefore to supplement the above mentioned methods, and bridge the gap between them, by post-processing h u m a n dialogues to give them a computer-like quality.
The advantage, compared to having people do the simulation on the fly, is both that it can be done with more consistency, and also that it can be done by researchers t h a t actually know what h u m a n c o m p u t e r natural language dialogues can look like.
A possible disadvantage with using both Wizard of Oz-and real computer dialogues, is that users will quickly a d a p t to what the system can provide t h e m with, and will therefore not try to use it for tasks they know it cannot perform.
Consequently, we will not get a full picture of the different services they would like the system to provide.
A disadvantage with this method is, of course, t h a t post-processing takes some time compared to using the natural dialogues as they are.
There is also a concern on the ecological validity of the results, as discussed later.
Distilling dialogues Distilling dialogues, i.e. re-writing h u m a n interactions in order to have them reflect what a humancomputer interaction could look like involves a number of considerations.
The main issue is t h a t in corp o r a of natural dialogues one of the interlocutors is not a dialogue system.
The system's task is instead performed by a h u m a n and the problem is how to anticipate the behaviour of a system that does not exist based on the performance of an agent with different performance characteristics.
One important aspect is how to deal with h u m a n features that are not part of what the system is supposed to be a b l e to handle, for instance if the user talks about things outside of the domain, such as discussing an episode of a recent T V show.
It also involves issues on how to handle situations where one of the interlocuters discusses with someone else on a different topic, e.g. discussing the up-coming Friday party with a friend in the middle of an information providing dialogue with a customer.
It is i m p o r t a n t for the distilling process to have at least an outline of the dialogue system t h a t is under development: Will it for instance have the capacity to recognise users' goals, even if not explicitly stated?
Will it be able to reason about the discourse domain?
W h a t services will it provide, and what will be outside its capacity to handle?
In our case, we assume that the planned dialogue system has the ability to reason on various aspects of dialogue and properties of the application.
In our current work, and in the examples used for illustration in this paper, we assume a dialogue model that can handle any relevant dialogue phenomenon and also an interpreter and speech recogniser being able to understand any user input that is relevant to the task.
There is is also a powerful domain reasoning module allowing for more or less any knowledge reasoning on issues that can be accomplished within the domain (Flycht-Eriksson, 1999).
Our current system does, however, not have an explicit user task model, as opposed to a system task model (Dahlb~ick fiand JSnsson, 1999), which is included, and thus, we can not assume that the 'system' remembers utterances where the user explains its task.
Furthermore, as our aim is system development we will not consider interaction outside the systems capabilities as relevant to include in the distilled dialogues.
The context of our work is the development a multi-modal dialogue system.
However, in our current work with distilling dialogues, the abilities of a multi-modal system were not fully accounted for.
The reason for this is that the dialogues would be significantly affected, e.g. a telephone conversation where the user always likes to have the n e x t connection, please will result in a table if multi-modal output is possible and hence a fair amount of the dialogne is removed.
We have therefore in this paper analysed the corpus assuming a speech-only system, since this is closer to the original telephone conversations, and hence needs fewer assumptions on system performance when distilling the dialogues.
distilling. The system might in such cases provide less information.
The principle of providing all relevant information is based on the assumption that a computer system often has access to all relevant information when querying the background system and can also present it more conveniently, especially in a multimodal system (Ahrenberg et al., 1996).
A typical example is the dialogue fragment in figure 1.
In this fragment the system provides information on what train to take and how to change to a bus.
The result of distilling this fragment provides the revised fragment of figure 2.
As seen in the fragment of figure 2 we also remove a number of utterances typical for human interaction, as discussed below.
* S y s t e m utterances are m a d e m o r e computer-like and do n o t include irrelevant i n f o r m a t i o n. The Distillation guidelines Distilling dialogues requires guidelines for how to handle various types of utterances.
In this section we will present our guidelines for distilling a corpus of telephone conversations between a human information provider on local buses 1 to be used for developing a multimodal dialogue system (Qvarfordt and JSnsson, 1998; Flycht-Eriksson and JSnsson, 1998; Dahlb~ick et al., 1999; Qvarfordt, 1998).
Similar guidelines are used within another project on developing Swedish Dialogue Systems where the domain is travel bureau information.
We can distinguish three types of contributors: 'System' (i.e.
a future systems) utterances, User utterances, and other types, such as moves by other speakers, and noise.
latter is seen in $9 in the dialogue in figure 3 where the provided information is not relevant.
It could also be possible to remove $5 and respond with $7 at once.
This, however, depends on if the information grounded in $5-U6 is needed for the 'system' in order to know the arrival time or if that could be concluded from U4.
This in turn depends on the system's capabilities.
If we assume that the dialogue system has a model of user tasks, the information in $5-U6 could have been concluded from that.
We will, in this case, retain $5-U6 as we do not assume a user task model (Dahlb/ick and JSnsson, 1999) and in order to stay as close to the original dialogue as possible.
The next problem concerns the case when 'system' utterances are changed or removed.
 Dialogue contributions provided by s o m e t h i n g or s o m e o n e other than the u s e r or the ' s y s t e m ' are removed.
These are regarded as not being part Modifying system utterances The problem of modifying 'system' utterances can be divided into two parts: how to change and when to change.
They are in some respects intertwined, but as the how-part affects the when-part more we will take this as a starting point.
 The ' s y s t e m ' provides as m u c h relevant inform a t i o n as possible at once.
This depends on the capabilities of the systems output modalities.
If we have a screen or similar output device we present as much as possible which normally is all relevant information.
If we, on the other hand, only have spoken output the amount of information that the hearer can interpret in one utterance must be considered when 1The bus time table dialogues are collected at LinkSping University and are available (in Swedish) on http://www.ida.liu.se/~arnjo/kfb/dialoger.html of the interaction.
This means that if someone interrupts the current interaction, say that the telephone rings during a face-to-face interaction, the interrupting interaction is normally removed from the corpus.
Furthermore, 'system' interruptions are removed.
A human can very well interrupt another human interlocuter, but a computer system will not do that.
However, this guideline could lead to problems, for instance, when users follow up such interruptions.
If no information is provided or the interrupted sequence does not affect the dialogue, we have no problems removing the interruption.
The problem is what to do when information from the 'system' is used in the continuing dialogue.
For such cases we have no fixed strategy, yes I wonder if you have any m m buses or ().
like express buses leaving from LinkSping to Vadstena ().
on sunday ja ville undra om ni hade ndgra 5h bussar eUer ().
typ expressbussar sore dkte frdn LinkSping till Vadstena ().
pd sSnda $5: no the bus does not run on sundays nej bussen g~r inte pd sSndagar U6: how can you ().
can you take the train and then change some way ().
because ().
to MjSlby 'n' so hur kan man ().
kan man ta tdg d sen byta p~ ndtt sStt ().
fSr de ().
till mjSlby ~ sd $7: that you can do too yes de kan du gSra ocksd ja U8: how ().
do you have any such suggestions hut ().
har du n~ra n~gra s~na fSrslag $9: yes let's see (4s) a m o m e n t (15s) now let us see here ().
was it on the sunday you should travel ja ska se h~ir (4s) eft 5gonblick (15s) nu ska vise hSr ().
va de p~ sSndagen du skulle dka pd U10: yes right afternoon preferably ja just de eftermidda ggirna $11: afternoon preferable ().
you have train from LinkSping fourteen twenty nine eftermidda gSrna ().
du hat t~g frdn LinkSping fjorton d tjugonie U12: m m mm S13: and then you will change from MjSlby station six hundred sixty sd byter du frdn MjSlby station sexhundrasexti sexhundrasexti $15: fifteen and ten Figure 1: Dialogue fragment from a real interaction on bus time-table information U4: S5: U6: $7: I wonder if you have any buses or ().
like express buses going from LinkSping to Vadstena ().
on sunday no the bus does not run on sundays how can you ().
can you take the train and then change some way ().
because ().
to MjSlby and so you can take the train from LinkSping fourteen and twenty nine and then you will change at MjSlby station to bus six hundred sixty at fifteen and ten Figure 2: A distilled version of the dialogue in figure 1 the dialogue needs to be rearranged depending on how the information is to be used (c.f.
the discussion in the final section of this paper).
in figure 4).
A common case of this is when the ' s y s t e m ' is talking while looking for information, $5 in the dialogue fragment of figure 4 is an example of this.
Related to this is when the system provides its own comments.
If we can assume that it has such capabilities they are included, otherwise we remove them.
 'System' utterances which are no longer valid are removed.
Typical examples of this are the utterances $7, $9, $11 and $13 in the dialogue fragment of figure 1.
* Remove sequences of utterances where the 'system' behaves in a way a computer would not do.
For instance jokes, irony, humor, commenting on the other dialogue participant, or dropping the telephone (or whatever is going on in $7 The system does not repeat information that has already been provided unless explicitly asked to do so.
In human interaction it is not uncommon to repeat what has been uttered for purposes other than to provide grounding information or feedback.
This is for instance common during 'n' I must be at Resecentrum before fourteen and thirty five ().
'cause we will going to the interstate buses $5: U6: $7: aha ().
'n' then you must be there around twenty past two something then yes around t h a t ja ungefgir let's see here ( l l s ) two hundred and fourteen R y d end station leaves forty six ().
thirteen 'n' forty six then you will be down fourteen oh seven (.) jaha 'n' ().
the next one takes you there ().
fourteen thirty seven ().
but t h a t is too late Figure 3: Dialogue fragment from a real interaction on bus time-table information U2: $3: U4: $5: U6: $7: Well, hi ().
I a m going to Ugglegatan eighth ja hej ().
ja ska till Ugglegatan dtta Yes ja and ().
I wonder ().
it is somewhere in Tannefors och ().
jag undrar ().
det ligger ndnstans i Tannefors Yes ().
I will see here one one I will look exactly where it is one m o m e n t please ja ().
jag ska se hhr eft eft jag ska titta exakt vat det ligger eft 6gonblick barn Oh Yeah (operator disconnects) (25s) m m ().
okey (hs) what the hell (2s) (operator connects again) hello yes ((Telefonisten kopplar ur sig)) (25s) iihh ().
okey (hs) de va sore ]aan (2s) ((Telefonisten kopplar in sig igen)) halld ja ja hej It is bus two hundred ten which runs on old tannefors road t h a t you have to take and get off at the bus stop at t h a t bus stop named vetegatan Figure 4: Dialogue fragment from a natural bus timetable interaction search procedures as discussed above.
want to develop systems where the user needs to restrict his/her behaviour to the capabilities of the dialogue system.
However, there are certain changes m a d e to user utterances, in most cases as a consequence of changes of system utterances.
 The system does not ask for information it has already achieved.
For instance asking again if it is on Sunday as in $9 in figure 1.
This is not uncommon in h u m a n interaction and such utterances from the user are not removed.
However, we can assume t h a t the dialogue system does not forget what has been talked about before.
4.2 M
o d i f y i n g u s e r u t t e r a n c e s The general rule is to change user utterances as little as possible.
The reason for this is that we do not Utterances that are no longer valid are removed.
The most common cases are utterances whose request has already been answered, as seen in the distilled dialogue in figure 2 of the dialogue in figure 1.
sixteen fifty five sexton ]emti/em U12: sixteen fifty five ().
aha sexton f e m t i / e m ().
jaha S13: bus line four hundred thirty five linje ]yrahundra tretti/em Figure 5: Dialogue fragment from a natural bus timetable interaction  Utterances are removed where the user discusses things that are in the environment.
For instance commenting the 'systems' clothes or hair.
This also includes other types of communicative signals such as laughter based on things outside the interaction, for instance, in the environment of the interlocuters.
 User utterances can also be added in order to make the dialogue continue.
In the dialogue in figure 5 there is nothing in the dialogue explaining why the system utters S13.
In such cases we need to add a user utterance, e.g.
Which bus is that?.
However, it might turn out that there are cues, such as intonation, found when listening to the tapes.
If such detailed analyses are carried out, we will, of course, not need to add utterances.
Furthermore, it is sometimes the case t h a t the telephone operator deliberately splits the information into chunks t h a t can be comprehended by the user, which then must be considered in the distillation.
5 Applying
the method To illustrate the m e t h o d we will in this section t r y to characterise the results from our distillations.
The illustration is based on 39 distilled dialogues from the previously mentioned corpus collected with a telephone operator having information on local bus time-tables and persons calling the information service.
The distillation took a b o u t three hours for all 39 dialogues, i.e. it is reasonably fast.
The distilled dialogues are on the average 27% shorter.
However, this varies between the dialogues, at most 73% was removed but there were also seven dialogues t h a t were not changed at all.
At the most 34 utterances where removed from one single dialogue and that was from a dialogue with discussions on where to find a parking lot, i.e. discussions outside the capabilities of the application.
There was one more dialogue where more t h a n 30 utterances were removed and that dialogue is a typical example of dialogues where distillation actually is very useful and also indicates what is normally removed from the dialogues.
This particular dialogue begins with the user asking for the telephone number to 'the Lost property office' for a specific bus operator.
However, the operator starts a discussion on what bus the traveller traveled on before providing the requested telephone number.
The reason for this discussion is probably t h a t the operator knows that different bus companies are utilised and would like to make sure that the user really understands his/her request.
The interaction t h a t follows can, thus, in t h a t respect be relevant, but for our purpose of developing systems based on an overall goal of providing information, not to understand human interaction, our dialogue system will not able to handle such phenomenon (JSnsson, 1996).
The dialogues can roughly be divided into five different categories based on the users task.
The discussion in twenty five dialogues were on bus times between various places, often one departure and one arrival but five dialogues involved more places.
In five dialogues the discussion was one price and various types of discounts.
Five users wanted to know the telephone number to 'the Lost property office', two discussed only bus stops and two discussed how they could utilise their season ticket to travel outside the trafficking area of the bus company.
It is interesting to note that there is no correspondence between the task being performed during the interaction and the amount of changes made to the d i a logue.
Thus, if we can assume that the amount of distillation indicates something about a user's interaction style, other factors t h a n the task are important when characterising user behaviour.
Looking at what is altered we find t h a t the most i m p o r t a n t distilling principle is that the 'system' provides all relevant information at once, c.f. figures 1 and 2.
This in turn removes utterances provided by both 'system' and user.
Most added utterances, both from the user and the 'system', provide explicit requests for information that is later provided in the dialogue, e.g. utterance $3 in figure 6.
We have added ten utterances in all 39 dialogues, five 'system' utterances and five user utterances.
Note, however, that we utilised the transcribed dialogues, without information on intonation.
We would probably not have needed to add this m a n y utterances if we had utilised the tapes.
Our reason for not using information on intonation is that we do not assume t h a t our system's speech recogniser can recognise intonation.
Finally, as discussed above, we did not utilise the full potential of multi-modality when distilling the dialogues.
For instance, some dialogues could be further distilled if we had assumed t h a t the system had presented a time-table.
One reason for this is t h a t we wanted to capture as m a n y interesting aspects intact as possible.
The advantage is, thus, that we have a better corpus for understanding humanYees hi Anna Nilsson is my name and I would like to take the bus from Ryd center to Resecentrum in LinkSping jaa hej Anna Nilsson heter jag och jag rill ~ka buss ~r~n Ryds centrum till resecentrum i LinkSping.
$3: U4: mm When do you want to leave? mm N~ir r i l l d u  k a ? 'n' I must be at Resecentrum before fourteen and thirty five ().
'cause we will going to the interstate buses ja ska va p~ rececentrum innan fjorton d trettifem ().
f5 vi ska till l~ngfiirdsbussarna Figure 6: Distilled dialogue fragment with added utterance computer interaction and can from t h a t corpus do a second distillation where we focus more on multimodal interaction.
One example of this is whether the system is meant to acquire information on the user's underlying motivations or goals or not.
In the examples presented, we have not assumed such capabilities, but this assumption is not an absolute necessity.
We believe, however, that the distilling process should be based on one such model, not the least to ensure a consistent t r e a t m e n t of similar recurring phenomena at different places in the corpora.
The validity of the results based on analysing distilled dialogues depends p a r t l y on how the distillation has been carried out.
Even when using natural dialogues we can have situations where the interaction is somewhat mysterious, for instance, if some of the dialogue participants behaves irrational such as not providing feedback or being too elliptical.
However, if careful considerations have been made to stay as close to the original dialogues as possible, we believe that distilled dialogues will reflect what a hum a n would consider to be a natural interaction.
Acknowledgments This work results from a n u m b e r of projects on development of natural language interfaces supported by The Swedish Transport & Communications Research Board (KFB) and the joint Research P r o g r a m for Language Technology ( H S F R / N U T E K ) . We are indebted to the participants of the Swedish Dialogue Systems project, especially to Staffan Larsson, Lena S a n t a m a r t a, and Annika Flycht-Eriksson for interesting discussions on this topic.
Discussion We have been presenting a method for distilling hum a n dialogues to make t h e m resemble h u m a n computer interaction, in order to utilise such dialogues as a knowledge source when developing dialogue systems.
Our own main purpose has been to use t h e m for developing multimodal systems, however, as discussed above, we have in this p a p e r rather assumed a speech-only system.
But we believe that the basic approach can be used also for multi-modal systems and other kinds of natural language dialogue systems.
It is i m p o r t a n t to be aware of the limitations of the method, and how 'realistic' the produced result will be, compared to a dialogue with the final system.
Since we are changing the dialogue moves, by for instance providing all required information in one move, or never asking to be reminded of what the user has previously requested, it is obvious t h a t what follows after the changed sequence would probably be affected one way or another.
A consequence of this is that the resulting dialogue is less accurate as a model of the entire dialogue.
It is therefore not an ideal candidate for trying out the systems over-all performance during system development.
But for the smaller sub-segments or sub-dialogues, we believe that it creates a good approximation of what will take place once the system is up and running.
Furthermore, we believe distilled dialogues in some respects to be more realistic than Wizard of Ozdialogues collected with a wizard acting as a computer.
Another issue, t h a t has been discussed previously in the description of the method, is t h a t the distilling is made based on a particular view of what a dialogue with a computer will look like.
While not necessarily being a detailed and specific model, it is at least an instance of a class of computer dialogue models.
Plan-Based Dialogue Management in a Physics Tutor Reva Freedman Learning Research and Development Center University of Pittsburgh Pittsburgh, PA 15260 freedrk+@pitt, edu http://www.pitt, edu/~freedrk Abstract This paper describes an application of APE (the Atlas Planning Engine), an integrated planning and execution system at the heart of the Atlas dialogue management system.
APE controls a mixedinitiative dialogue between a human user and a host system, where turns in the 'conversation' may include graphical actions and/or written text.
APE has full unification and can handle arbitrarily nested discourse constructs, making it more powerful than dialogue managers based on finitestate machines.
We illustrate this work by describing Atlas-Andes, an intelligent tutoring system built using APE with the Andes physics tutor as the host.
1 Introduction
The purpose of the Atlas project is to enlarge the scope of student interaction in an intelligent tutoring system (ITS) to include coherent conversational sequences, including both written text and GUI actions.
A key component o f Atlas is APE, the Atlas Planning Engine, a "just-intime" planner specialized for easy construction and quick generation of hierarchically organized dialogues.
APE is a domainand task-independent system.
Although to date we have used APE as a dialogue manager for intelligent tutoring systems, APE could also be used to manage other types of human-computer conversation, such as an advicegiving system or an interactive help system.
Planning is an essential component of a dialogue-based ITS.
Although there are many reasons for using natural language in an ITS, as soon as the student gives an unexpected response to a tutor question, the tutor needs to be able to plan in order to achieve its goals as well as respond appropriately to the student's statement.
Yet classical planning is inappropriate for dialogue generation precisely because it assumes an unchanging world.
A more appropriate approach is the "practical reason" approach pioneered by Bratman (1987, 1990).
According to Bratman, human beings maintain plans and prefer to follow them, but they are also capable of changing the plans on the fly when needed.
Bratman's approach has been introduced into computer science under the name of reactive planning (Georgeff and Ingrand 1989, Wilkins et al.1995). In this paper we discuss the rationale for the use of reactive planning as well as the use of the hierarchical task network (HTN) style o f plan operators.
Then we describe APE (the Atlas Planning Engine), a dialogue planner we have implemented to embody the above concepts.
We demonstrate the use of APE by showing how we have used it to add a dialogue capability to an existing ITS, the Andes physics tutor.
By showing dialogues that Atlas-Andes can generate, we demonstrate the advantages of this architecture over the finite-state machine approach to dialogue management.
Integrated planning and execution for dialogue generation 2.1 'Practical reason' and the BDI model For an ITS, planning is required in order to ensure a coherent conversation as well as to accomplish tutorial goals.
But it is impossible to plan a whole conversation in advance when the student can respond freely at every turn, just as human beings cannot plan their daily lives in advance because of possible changes in conditions.
Classical planning algorithms are inappropriate because the tutor must be able to change plans based on the This research was supported by NSF grant number 9720359 to CIRCLE, the Center for Interdisciplinary Research on Constructive Learning Environments at the University of Pittsburgh and Carnegie-Mellon University.
fistudent's responses.
For this reason we have adopted the ideas of the philosopher Michael Bratman (1987, 1990).
Bratman uses the term "practical reason" to describe his analysis since he is concerned with how to reason about practical matters.
For human beings, planning is required in order to accomplish one's goals.
Bratman's key insight is that human beings tend to follow a plan once they have one, although they are capable of dropping an intention or changing a partial plan when necessary.
In other words, human beings do not decide what to do from scratch at each turn.
Bratman and others who have adopted his approach use a tripartite mental model that includes beliefs, desires and intentions (Bratman, Israel and Pollack 1988, Pollack 1992, Georgeff et al.1998), hence the name "BDI model".
Beliefs, which are uninstantiated plans in the speaker's head, are reified by the plan library.
Desires are expressed as the agent's goals.
Intentions, or plan steps that the agent has committed to but not yet acted on, are stored in an agenda.
Thus the agent's partial plan for achieving a goal is a network of intentions.
A plan can be left in a partially expanded state until it is necessary to refine it further.
2.2 Implementation
via reactive planning can be achieved via a series of subgoals instead of relying on means-end reasoning.
Hierarchical decomposition is more appropriate to dialogue generation for a number of reasons.
First, decomposition is better suited to the type of largescale dialogue planning required in a real-world tutoring system, as it is easier to establish what a human speaker will say in a given situation than to be able to understand why in sufficient detail and generality to do means-end planning.
Second, Hierarchical decomposition minimizes search time.
Third, our dialogues are task-oriented and have a hierarchical structure (Grosz and Sidner 1986).
In such a case, matching the structure of the domain simplifies operator development because they can often be derived from transcripts of human tutoring sessions.
The hierarchy information is also useful in determining appropriate referring expressions.
Fourth, interleaved planning and execution is important for dialogue generation because we cannot predict the human user's future utterances.
In an HTN-based system, it is straightforward to implement interleaved planning and execution because one only needs to expand the portion of the plan that is about to be executed.
Finally, the conversation is in a certain sense the trace of the plan.
In other words, we care much more about the actions generated by the planner than the states involved, whether implicitly or explicitly specified.
Hierarchical decomposition provides this trace naturally.
3 Background: the Andes physics tutor Andes (Gertner, Conati and VanLehn 1998) is an intelligent tutoring system in the domain of firstyear college physics.
Andes teaches via coached problem solving (VanLehn 1996).
In coached problem solving, the tutoring system tracks the student as the latter attempts to solve a problem.
If the student gets stuck or deviates too far from a correct solution path, the tutoring system provides hints and other assistance.
A sample Andes problem is shown in midsolution in Figure 1.
A physics problem is given in the upper-left corner with a picture below it.
Next to the picture the student has begun to sketch the vectors involved using the GUI buttons along the left-hand edge of the screen.
As the fistudent draws vectors, Andes and the student cooperatively fill in the variable definitions in the upper-right corner.
Later the student will use the space below to write equations connecting the variables.
In this example, the elevator is decelerating, so the acceleration vector should face the opposite direction from the velocity vector.
(If the acceleration vector went the same direction as the velocity vector, the speed of the elevator would increase and it would crash into the ground).
This is an important issue in beginning physics; it occurs in five Andes problems.
When such errors occur, Andes turns the incorrect item red and provides hints to students in the lower-left corner of the screen.
A sample of these hints, shown in the order a student would encounter them, is shown in Fig.
2. But hints are an output-only form of natural language; the student can't take the initiative or ask a question.
In addition, there is no way for the system to ask the student a question or lead the student through a multi-step directed line of reasoning.
Thus there is no way to use some of the effective rhetorical methods used by skilled human tutors, such as analogy and reductio ad absurdum.
Current psychological research suggests that active methods, where students have to answer questions, will improve the performance of tutoring systems.
Structure of the Atlas Planning Engine Figure3 shows a sample plan operator.
For legibility, the key elements have been rendered in English instead of in Lisp.
The hiercx slot provides a way for the planner to be aware of the context in which a decomposition is proposed.
Items in the hiercx slot are instantiated and added to the transient database only so long as the operator which spawned them is in the agenda.
To initiate a planning session, the user invokes the planner with an initial goal.
The system searches the operator library to find all operators whose goal field matches the next goal on the agenda and whose filter conditions and preconAn elevator slows to a stop from an initial downward velocity of 10.0 m]s in 2.00 seconds.
A passenger in the elevator is holding a 3.00 kilogram package by a vertical string.
What is the tension in the string during the process? e',ev~o, at 10 m/s elev~or at a stop mass of p~:w'.,I,,~ magnitude of the inst~~taneous Velocity of pack,age ~ {rkneTO magnitude of the avelage Acceleratiorl of package,dudngTO... pkg Figure I: Screen shot of the Andes physics tutor fiS: T: S: T: S: T: S: T: S: (draws acceleration vector in same direction as velocity) Wrong.
What's wrongwith that?
Think about the direction of the acceleration vector.
Please explain further.
Remember that the direction of acceleration is the direction of the change in velocity.
Please explain further.
The'direction o f the acceleration vector is straight up.
(draws acceleration vector correctly) Figure 2: Andes hint sequence formatted as dialogue ditions are satisfied.
Goals are represented in first-order logic without quantifiers and matched via unification.
Since APE is intended especially for generation of hierarchically organized taskoriented discourse, each operator has a multi-step recipe in the style of Wilkins (1988).
When a match is found, the matching goal is removed from the agenda and is replaced by the steps in the recipe.
APE has two kinds of primitive actions; one ends a turn and the other doesn't.
From the point of view of discourse generation, the most important APE recipe items are those allowing the planner to change the agenda when necessary.
These three types of recipe items make APE more powerful than a classical planner.
 Fact: Evaluate a condition.
If false, skip the rest of the recipe.
Fact is used to allow run-time decision making by bypassing the rest o f an operator when circumstances change during its execution.
Fact can be used with retry-at to implement a loop just as in Prolog.
 Retry-at.
The purpose of retry-at is to allow the planner to back up to a choice point and make a new decision.
It removes goals sequentially from the top of the agenda, a full operator at a time, until the supplied argument is false.
Then it restores the parent goal of the last operator removed, so that further planning can choose a new way to achieve it.
Retry-at implements a Prolog-like choice of alternatives, but it differs from backtracking in that the new operator is chosen based on conditions that apply when the retry operation is executed, rather than on a list of possible operators formed when the original operator was chosen.
For retry-at to be useful, the author must provide multiple operators for the same goal.
Each operator must have a set of preconditions enabling it to be chosen at the appropriate time.
 Prune-replace: The intent of prune-replace is (def-operator handle-same-direction :goal ()... :filter () :precond ()...
We h a v e a s k e d a q u e s t i o n a b o u t a c c e l e r a t i o n ;...
a n d t h e s t u d e n t h a s g i v e n an a n s w e r ; ...
f r o m w h i c h we c a n d e d u c e t h a t s / h e t h i n k s a c c e l, a n d v e l o c i t y go in ; the same direction ; a n d we h a v e n o t g i v e n t h e e x p l a n a t i o n below yet : r e c i p e ()...
Tell the student: "But if the a c c e l e r a t i o n went the same direction as t h e v e l o c i t y, t h e n t h e e l e v a t o r w o u l d be s p e e d i n g u p . " ; M a r k t h a t we a r e g i v i n g t h i s e x p l a n a t i o n ; T e l l t h e s t u d e n t t h a t t u t o r is r e q u e s t i n g another answer ("Try again").
Edit the agenda ( u s i n g prune-replace) so t h a t r e s p o n d i n g to a n o t h e r a n s w e r is at t h e t o p of t h e a g e n d a :hiercx ()) Figure 3: Sample plan operator 55 fito allow the planner to remove goals from the agenda based on a change in circumstances.
It removes goals sequentially from the top of the agenda, one at a time, until the supplied argument becomes false.
Then it replaces the removed goals with an optional list of new goals.
Prune-replace allows a type of decision-making frequently used in dialogue generation.
When a conversation partner does not give the expected response, one would often like to remove the next goal from the agenda and replace it with one or more replacement goals.
Prune-replace implements a generalized version of this concept.
APE is domain-independent and communicates with a host system via an API.
As a partner in a dialogue, it needs to obtain information from the world as well as produce output turns.
Preconditions on plan operators can be used to access information from external knowledge sources.
APE contains a recipe item type that can be used to execute an external program such as a call to a GUI interface.
APE also has recipe items allowing the user to assert and retract facts in a knowledge base.
Further details about the APE planner can be found in (Freedman, 2000).
I m p l e m e n t a t i o n of Atlas-Andes 5.1 Architecture of Atlas-Andes The first system we have implemented with APE is a prototype Atlas-Andes system that replaces the hints usually given for an incorrect acceleration vector by a choice of generated subdialogues.
Figure 4 shows the architecture of Atlas-Andes; any other system built with APE would look similar.
Robust natural language understanding in Atlas-Andes is provided by Ros6's CARMEL system (Ros6 2000); it uses the spelling correction algorithm devised by Elmi and Evens (1998).
5.2 Structure
of human tutorial dialogues In an earlier analysis (Kim, Freedman and Evens 1998) we showed that a significant portion of human-human tutorial dialogues can be modeled with the hierarchical structure o f task-oriented dialogues (Grosz and Sidner 1986).
Furthermore, a main building block o f the discourse hierarchy, corresponding to the transaction level in Conversation Analysis (Sinclair and Coulthard 1975), matches the tutoring episode defined by VanLehn et al.(1998). A tutoring episode consists of the turns necessary to help the student make one correct entry on the interface.
NLU (CARMEL) Plan Library User Interface Host (Andes) GUI Interpreter (Andes) Transient Knowledge Base Figure 4: Interface between Atlas and host system fiTo obtain empirical data for the Atlas-Andes plan operators, we analyzed portions of a corpus of human tutors helping students solve similar physics problems.
Two experienced tutors were used.
Tutor A was a graduate student in computer science who had majored in physics; tutor B was a professional physics tutor.
The complete corpus contained solutions to five physics problems by 41 students each.
We analyzed every tutoring episode dealing with the acceleration vector during deceleration, totaling 29 examples divided among 20 students and both tutors.
The tutors had very different styles.
Tutor A tended to provide encouragement rather than content, making those transcripts less useful for deriving an information-based approach.
Tutor B used an information-based approach, but after one wrong answer tended to complete the solution as a monologue.
Largely following tutor B's approach to sequence and content, we isolated six ways of teaching the student about direction of acceleration.
Tutoring schemata Switching between schemata API and GUI handling Answer handling Domain-dep.
lex. insertion Domain-indep.
lex. insertion TOTAL 5.3 Sample output and evaluation Figure 5 shows an example of text that can be generated by the Atlas-Andes system, showing an analogy-based approach to teaching this content.
The operator library used to generate this text could generate a combinatorially large number of versions of this dialogue as well as selected examples of other ways o f teaching about direction of acceleration.
This operator library used to generate this text contained 1 l 1 plan operators, divided as follows: We are currently working on components that will allow us to increase the number of physics concepts covered without a corresponding increase in the number of operators.
The schema switching operators prevent the tutor from repeating itself during a physics problem.
They could be reduced or eliminated by a general discourse history component that tutoring schema operators could refer to.
Domain-dependent lexical insertion refers to the choice of lexical items such as car and east in the sample dialogue, while domain-independent iexical insertion refers to items such as O K and exactly.
Both categories could be eliminated, or at least severely reduced, through the use of a text realization package.
Together that would provide a one-third reduction in the number o f operators needed.
As the set of API and GUI handling operators is fixed, that would reduce by half the number of application operators needed.
The largest remaining category of operators is the answer handlers.
These operators handle a variety of answers for each o f the five questions that the system can ask.
The answers we recognize include categories such as "don't know" as well as specific answers (e.g.
a direction perpendicular to the correct answer) which we recognize because the tutor has specific replies for them.
In order to reduce the number o f S: T: S: T: S: T: S: (draws acceleration vector in same direction as velocity) What is the definition of acceleration?
Don't know.
OK, let's try this.
If a car was driving along east, which way would you have to push on it to make it stop?
West. Exactly.
The opposite direction.
So the net force goes the opposite direction, and so does the acceleration.
Try to draw the acceleration vector again now.
(draws acceleration vector correctly) Figure 5: Example of generated dialogue 57 fioperators further, we must investigate more general methods of handling student errors.
In particular, we plan to investigate error-classifying predicates that apply to more than one question as well as the use of intention-based predicates.
Since the system only covers one rule of physics, albeit in a variety of ways, we plan to make some of these efficiency improvements before adding new rules o f physics and testing it with users.
Preconditions for the operators in the plan library utilize discourse or interaction history, the current goal hierarchy, recent information such as the tutor's current goal and the student's latest response, shared information such as a model o f objects on the screen, and domain knowledge.
As an example of the latter, if the student draws an acceleration vector which is incorrect but not opposite to the velocity vector, a different response will be generated.
Related work Wenger (1987), still the chief textbook on ITSs, states that using a global planner to control an ITS is too inefficient to try.
This is no longer true, if indeed it ever was.
Vassileva (1995) proposes a system based on AND-OR graphs with a separate set of rules for reacting to unexpected events.
Lehuen, Nicolle and Luzzati (1996) present a method of dialogue analysis that produces schemata very similar to ours.
Earlier dialoguebased ITSs that use augmented finite-state machines or equivalent include CIRCSIM-Tutor (Woo et al.1991, Z h o u e t al.
1999) and the system described by Woolf (1984).
Cook (1998) uses levels of finite-state machines.
None of these systems provides for predicates with variables or unification.
Conclusions 5.4 Discussion Many previous dialogue-based ITSs have been implemented with finite-state machines, either simple or augmented.
In the most common finite state mode[, each time the human user issues an utterance, the processor reduces it to one of a small number of categories.
These categories represent the possible transitions between states.
Thus history can be stored, and context considered, only by expanding the number o f states.
This approach puts an arbitrary restriction on the amount of context or depth of conversational nesting that can be considered.
More importantly, it misses the significant generalization that these types of dialogues are hierarchical: larger units contain repeated instances of the same smaller units in different sequences and instantiated with different values.
Furthermore, the finite-state machine approach does not allow the author to drop one line of attack and replace it by another without hardcoding every possible transition.
It is also clear that the dialogue-based approach has many benefits over the hint-sequence approach.
In addition to providing a multi-step teaching methods with new content, it can respond flexibly to a variety of student answers at each step and take context into account when generating a reply.
In this paper we described APE, an integrated planner and execution system that we have implemented as part o f the Atlas dialogue manager.
APE uses HTN-style operators and is based on reactive planning concepts.
Although APE is intended largely for use in domains with hierarchical, multi-turn plans, it can be used to implement any conversation-based system, where turns in the 'conversation' may include graphical actions and/or text.
We illustrated the use of APE with an example from the Atlas-Andes physics tutor.
We showed that previous models based on finite-state machines are insufficient to handle the nested subdialogues and abandoned partial subdialogues that occur in practical applications.
We showed how APE generated a sample dialogue that earlier systems could not handle.
Acknowledgments We thank Abigail Gertner for her generous assistance with the Andes system, and Michael Ringenberg for indispensible programming support.
Carolyn Ros6 built the CARMEL natural language understanding component.
Mohammed EImi and Michael Glass of Illinois Institute o f Technology provided the spelling correction code.
We thank Pamela Jordan and the referees for their comments.
Bratman's approach has been elaborated in a computer science context by subsequent researchers (Bratman, Israel and Pollack 1988, Pollack 1992, Georgeff et al.1998). Reactive planning (Georgeff and Ingrand 1989, Wilkins et al.1995), originally known as "integrated planning and execution," is one way of implementing Bratman's model.
Originally developed for real-time control of the space shuttle, reactive planning has since been used in a variety of other domains.
For the Atlas project we have developed a reactive planner called APE (Atlas Planning Engine) which uses these ideas to conduct a conversation.
After each student response, the planner can choose to continue with its previous intention or change something in the plan to respond better to the student's utterance.
Like most reactive planners, APE is a hierarchical task network (HTN) style planner (Yang 1990, Erol, Hendler and Nau 1994).
A Framework for MT and Multilingual NLG Systems Based on Uniform Lexico-Structural Processing Benoit Lavoie CoGenTex, Inc.
840 Hanshaw Road Ithaca, NY USA, 14850 benoit@cogentex.com Richard Kittredge CoGenTex, Inc.
840 Hanshaw Road Ithaca, NY USA, 14850 richard @cogentex.com Tanya Korelsky CoGenTex, Inc.
840 Hanshaw Road Ithaca, NY USA, 14850 tanya @cogentex.com Owen Rambow * ATT Labs-Research, B233 180 Park Ave, PO Box 971 Florham Park, NJ USA, 07932 rambow @research.att.com Abstract In this paper we describe an implemented framework for developing monolingual or multilingual natural language generation (NLG) applications and machine translation (MT) applications.
The framework demonstrates a uniform approach to generation and transfer based on declarative lexico-structural transformations of dependency structures of syntactic or conceptual levels ("uniform lexico-structural processing").
We describe how this framework has been used in practical NLG and MT applications, and report the lessons learned.
1 Introduction
The framework consists of a portable Java environment for building NLG or MT applications by defining modules using a core tree transduction engine and single declarative ASCII specification language for conceptual or syntactic dependency tree structures 1 and their transformations.
Developers can define new modules, add or remove modules, or modify their connections.
Because the processing of the transformation engine is restricted to transduction of trees, it is computationally efficient.
Having declarative rules facilitates their reuse when migrating from one programming environment to another; if the rules are based on functions specific to a programming language, the implementation of these functions might no longer be available in a different environment.
In addition, having all lexical information and all rules represented declaratively makes it relatively easy to integrate into the framework techniques for generating some of the rules automatically, for example using corpus-based methods.
The declarative form of transformations makes it easier to process them, compare them, and cluster them to achieve proper classification and ordering.
In this paper we present a linguistically motivated framework for uniform lexicostructural processing.
It has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation (NLG) and for transfer in machine translation (MT).
Our work extends directions taken in systems such as Ariane (Vauquois and Boitet, 1985), FoG (Kittredge and Polgu6re, 1991), JOYCE (Rainbow and Korelsky, 1992), and LFS (Iordanskaja et al., 1992).
Although it adopts the general principles found in the abovementioned systems, the approach presented in this paper is more practical, and we believe, would eventually integrate better with emerging statistics-based approaches to MT.
* The work performed on the framework by this coauthor was done while at CoGenTex, Inc.
1 In
this paper, we use the term syntactic dependency (tree) structure as defined in the Meaning-Text Theory (MTT; Mel'cuk, 1988).
However, we extrapolate from this theory when we use the term conceptual dependency (tree) structure, which has no equivalent in MTT (and is unrelated to Shank's CD structures proposed in the 1970s).
Thus, the framework represents a generalized processing environment that can be reused in different types of natural language processing (NLP) applications.
So far the framework has been used successfully to build a wide variety of NLG and MT applications in several limited domains (meteorology, battlefield messages, object modeling) and for different languages (English, French, Arabic, and Korean).
In the next sections, we present the design of the core tree transduction module (Section 2), describe the representations that it uses (Section 3) and the linguistic resources (Section 4).
We then discuss the processing performed by the tree transduction module (Section 5) and its instantiation for different applications (Section 6).
Finally, we discuss lessons learned from developing and using the framework (Section 7) and describe the history of the framework comparing it to other systems (Section 8).
2 The
Framework's Tree Transduction Module Input Lexico-Structm'al Processing Dependency SUucturc Lexico-Structural Postprocessing Figure 1: Design of the Tree Transduction Module 3 The Framework's Representations The core processing engine of the framework is a generic tree transduction module for lexicostructural processing, shown in Figure 1.
The module has dependency stuctures as input and output, expressed in the same tree formalism, although not necessarily at the same level (see Section 3).
This design facilitates the pipelining of modules for stratificational transformation.
In fact, in an application, there are usually several instantiations of this module.
The transduction module consists of three processing steps: lexico-structural preprocessing, main lexico-structural processing, and lexico-structural post-processing.
Each of these steps is driven by a separate grammar, and all three steps draw on a common feature data base and lexicon.
The grammars, the lexicon and the feature data base are referred to as the linguistic resources (even if they sometimes apply to a conceptual representation).
All linguistic resources are represented in a declarative manner.
An instantiation of the tree transduction module consists of a specification of the linguistic resources.
The representations used by all instantiations of the tree transduction module in the framework are dependency tree structures.
The main characteristics of all the dependency tree structures are:  A dependency tree is unordered (in contrast with phrase structure trees, there is no ordering between the branches of the tree).
 All the nodes in the tree correspond to lexemes (i.e., lexical heads) or concepts depending on the level of representation.
In contrast with a phrase structure representation, there are no phrase-structure nodes labeled with nonterminal symbols.
Labelled arcs indicate the dependency relationships between the lexemes.
The first of these characteristics makes a dependency tree structure a very useful representation for MT and multilingual NLG, since it gives linguists a representation that allows them to abstract over numerous crosslinguistic divergences due to language specific ordering (Polgu~re, 1991).
We have implemented 4 different types of dependency tree structures that can be used for NLG, MT or both:  Deep-syntactic structures (DSyntSs);  Surface syntactic structures (SSyntSs); Conceptual structures (ConcSs); Parsed syntactic structures (PSyntSs).
The DSyntSs and SSyntSs correspond closely to the equivalent structures of the Meaning-Text Theory (MTT; Mel'cuk, 1988): both structures are unordered syntactic representations, but a DSyntS only includes full meaning-bearing lexemes while a SSyntS also contains function words such as determiners, auxiliaries, and strongly governed prepositions.
In the implemented applications, the DSyntSs are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic-based MT (Hutchins and Somers, 1997).
Figure 2 illustrates a DSyntS from a meteorological application, MeteoCogent (Kittredge and Lavoie, 1998), represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework (Lavoie and Rambow, 1997).
As Figure 2 illustrates, there is a straightforward mapping between the graphical notation and the ASCII notation supported in the framework.
This also applies for all the transformation rules in the framework which illustrates the declarative nature of our approach, Figure 3 illustrates the mapping between an interlingua defined as a ConcS and a corresponding English DSyntS.
This example, also taken from MeteoCogent, illustrates that the conceptual interlingua in NLG can be closer to a database representation of domain data than to its linguistic representations.
As mentioned in (Polgu~re, 1991), the high level of abstraction of the ConcSs makes them a suitable interlingua for multilingual NLG since they bridge the semantic discrepancies between languages, and they can be produced easily from the domain data.
However, most off-the-shelf parsers available for MT produce only syntactic structures, thus the DSyntS level is often more suitable for transfer.
Cones TO ItlGH LOW Low 5 to Mlgh 20 Figure 3: ConcS Interlingua and English DSyntS Finally, the PSyntSs correspond to the parser outputs represented using RealPro's dependency structure formalism.
The PSyntSs may not be valid directly for realization or transfer since they may contain unsupported features or dependency relations.
However, the PSyntSs are represented in a way to allow the framework to convert them into valid DSyntS via lexicostructural processing.
This conversion is done via conversion grammars customized for each parser.
There is a practical need to convert one syntactic formalism to another and so far we have implemented converters for three off-theshelf parsers (Palmer et al., 1998).
4 The
Framework's Linguistic Resources 't Low S to high 20 Figure 2: DSyntS(Graphicaland ASCIINotation) The ConcSs correspond to the standard framelike structures used in knowledge representation, with labeled arcs corresponding to slots.
We have used them only for a very limited meteorological domain (in MeteoCogent), and we imagine that they will typically be defined in a domain-specific manner.
As mentioned previously, the framework is composed of instantiations of the tree fitransduction module shown in Figure 1.
Each module has the following resources:  Feature Data-Base: This consists of the feature system defining available features and their possible values in the module.
 Lexicon: This consists of the available lexemes or concepts, depending on whether the module works at syntactic or conceptual level.
Each lexeme and concept is defined with its features, and may contain specific lexico-structural rules: transfer rules for MT, mapping rules to the next level of representation for surface realization of DSyntS or lexicalization of ConcS.
 Main Grammar: This consists of the lexicostructural mapping rules that apply at this level and which are not lexemeor conceptspecific (e.g.
DSynt-rules for the DSyntmodule, Transfer-rules for the Transfer module, etc).
 Preprocessing grammar: This consists of the lexico-structural mapping rules for transforming the input structures in order to make them compliant with the main grammar, if this is necessary.
Such rules are used to integrate new modules together when discrepancies in the formalism need to be fixed.
This grammar can also be used for adding default features (e.g.
setting the default number of nouns to singular) or for applying default transformations (e.g.
replacing non meaning-bearing lexemes with features).
Postprocessing grammar: This consists of lexico-structural mapping rules for transforming the output structures before they can be processed by the next module.
As for the preprocessing rules, these rules can be used to fix some discrepancies between modules.
Our representation of the lexicon at the lexical level (as opposed to conceptual) is similar to the one found in RealPro.
Figure 4 shows a specification for the lexeme SELL.
This lexeme is defined as a verb of regular morphology with two lexical-structural mappings, the first one introducing the preposition TO for its 3r actant, and the preposition FOR for its 4 th actant: (a seller) X1 sells (merchandise) X2 to (a buyer) X3 f o r (a price) X4.
What is important is that 63 each mapping specifies a transformation between structures at different levels of representation but that are represented in one and the same representation formalism (DSyntS and SSyntS in this case).
As we will see below, grammar rules are also expressed in a similar way.
( c o m p l e t i v e 3 FOR ( prepositional Figure 4: Specification of Lexeme SELL At the conceptual level, the conceptual lexicon associates lexical-structural mapping with concepts in a similar way.
Figure 5 illustrates the mapping at the deep-syntactic level associated with the concept #TEMPERATURE.
Except for the slight differences in the labelling, this type of specification is similar to the one used on the lexical level.
The first mapping rule corresponds to one of the lexico-structural transformations used to convert the interlingual ConcS of Figure 3 to the corresponding DSyntS.
SY SX Note that since each lexicon entry can have more than one lexical-structural mapping rule, the list of these rules represents a small grammar specific to this lexeme or concept.
Realization grammar rules of the main grammar include generic mapping rules (which are not lexeme-specific) such as the DSyntS-rule illustrated in Figure 6, for inserting a determiner.
DSYNT-RULE: More general lexico-structural rules for transfer can also be implemented using our grammar rule formalism.
Figure 8 gives an English-French transfer rule applied to a weather domain for the transfer of a verb modified by the adverb ALMOST: It almost rained.
--o II a failli pleuvoir.
Figure 6: Deep-Syntactic Rule for Determiner Insertion The lexicon formalism has also been extended to implement lexeme-specific lexico-structural transfer rules.
Figure 7 shows the lexicostructural transfer of the English verb lexeme MOVE to French implemented for a military and weather domain (Nasr et al., 1998): Cloud will move into the western regions.
Des nuages envahiront les rdgions ouest.
They moved the assets forward.
-.9 lls ont amen~ les ressources vers l 'avant.
The 79 dcg moves forward.
---~La 79 dcg a v a n c e vers l'avant.
A disturbance will move north of Lake Superior.
--~ Une perturbation se diplacera au nord du lac supdrieur.
Figure 8: English to French Lexico-Structural Transfer Rule with Verb Modifier ALMOST More details on how the structural divergences described in (Dorr, 1994) can be accounted for using our formalism can be found in (Nasr et al., 1998).
5 The
Rule Processing Before being processed, the rules are first compiled and indexed for optimisation.
Each module applies the following processing.
The rules are assumed to be ordered from most specific to least specific.
The application of the rules to the structures is top-down in a recursive way from the f'n-st rule to the last.
For the main grammar, before applying a grammar rule to a given node, dictionary lookup is carried out in order to first apply the lexemeor conceptspecific rules associated with this node.
These are also assumed to be ordered from the most specific to the least specific.
If a lexico-structural transformation involves switching a governor node with one of its dependents in the tree, the process is reapplied with the new node governor.
When no more rules can be applied, the same process is applied to each dependent of the current governor.
When all nodes have been processed, the processing is completed, 6 Using the Framework to build Applications Figure 7: Lexico-Structural Transfer of English Lexerne MOVE to French Figure 9 shows how different instantiations of the tree transduction module can be combined to fibuild NLP applications.
The diagram does not represent a particular system, but rather shows the kind of transformations that have been implemented using the framework, and how they interact.
Each arrow represents one type of processing implemented by an instantiation of the tree transduction module.
Each triangle represents a different level of representation.
Sentence interlingua can also support the generation of French but this functionality has not yet been implemented).
MT:  Transfer on the DSyntS level and realization via SSyntS level for English--French, English--Arabic, English---Korean and Korean--English.
Translation in the meteorology and battlefield domains (Nasr et al., 1998).
 Conversion of the output structures from off-the-shelf English, French and Korean parsers to DSyntS level before their processing by the other components in the framework (Palmer et al., 1998).
7 Lessons
Learned Using the Framework PI "ng Scopeof the Framework SSyntSLI Parsing Input Sentence LI yntS ealization Generated Sentence 1.2 Generated Sentence LI Sentence SSyntS Figure 9: Scope of the Framework's Transformations For example, in Figure 9, starting with the "Input Sentence LI" and passing through Parsing, Conversion, Transfer, DSyntS Realization and SSyntS Realization to "Generated Sentence L2" we obtain an Ll-to-L2 MT system.
Starting with "Sentence Planning" and passing through DSyntS Realization, and SSyntS Realization (including linearization and inflection) to "Generated Sentence LI", we obtain a monolingual NLG system for L1.
So far the framework has been used successfully for building a wide variety of applications in different domains and for different languages: NLG:  Realization of English DSyntSs via SSyntS level for the domains of meteorology (MeteoCogent; Kittredge and Lavoie, 1998) and object modeling (ModelExplainer; Lavoie et al., 1997).
 Generation of English text from conceptual interlingua for the meteorology domain (MeteoCogent).
(The design of the Empirical results obtained from the applications listed in Section 6 have shown that the approach used in the framework is flexible enough and easily portable to new domains, new languages, and new applications.
Moreover, the time spent for development was relatively short compared to that formerly required in developing similar types of applications.
Finally, as intended, the limited computational power of the transduction module, as well as careful implementation, including the compilation of declarative linguistic knowledge to Java, have ensured efficient run-time behavior.
For example, in the MT domain we did not originally plan for a separate conversion step from the parser output to DSyntS.
However, it quickly became apparent that there was a considerable gap between the output of the parsers we were using and the DSyntS representation that was required, and furthermore, that we could use the tree transduction module to quickly bridge this gap.
Nevertheless, our tree transduction-based approach has some important limitations.
In particular, the framework requires the developer of the transformation rules to maintain them and specify the order in which the rules must be applied.
For a small or a stable grammar, this does not pose a problem.
However, for large or rapidly changing grammar (such as a transfer grammar in MT that may need to be adjusted when switching from one parser to another), the fiburden of the developer's task may be quite heavy.
In practice, a considerable amount of time can be spent in testing a grammar after its revision.
Another major problem is related to the maintenance of both the grammar and the lexicon.
On several occasions during the development of these resources, the developer in charge of adding lexical and grammatical data must make some decisions that are domain specific.
For example, in MT, writing transfer rules for terms that can have several meanings or uses, they may simplify the problem by choosing a solution based on the context found in the current corpus, which is a perfectly natural strategy.
However, later, when porting the transfer resources to other domains, the chosen strategy may need to be revised because the context has changed, and other meanings or uses are found in the new corpora.
Because the current approach is based on handcrafted rules, maintenance problems of this sort cannot be avoided when porting the resources to new domains.
An approach such as the one described in (Nasr et al., 1998; and Palmer and al., 1998) seems to be solving a part of the problem when it uses corpus analysis techniques for automatically creating a first draft of the lexical transfer dictionary using statistical methods.
However, the remaining work is still based on handcrafting because the developer must refine the rules manually.
The current framework offers no support for merging handcrafted rules with new lexical rules obtained statistically while preserving the valid handcrafted changes and deleting the invalid ones.
In general, a better integration of linguistically based and statistical methods during all the development phases is greatly needed.
8 History
of the Framework and Comparison with Other Systems realization of deep-syntactic structures in NLG (Lavoie and Rambow, 1997).
It was later extended for generation of deep-syntactic structures from conceptual interlingua (Kittredge and Lavoie, 1998).
Finally, it was applied to MT for transfer between deep-syntactic structures of different languages (Palmer et al., 1998).
The current framework encompasses the full spectrum of such transformations, i.e. from the processing of conceptual structures to the processing of deep-syntactic structures, either for NLG or MT.
Compared to its predecessors (Fog, LFS, JOYCE), our approach has obvious advantages in uniformity, declarativity and portability.
The framework has been used in a wider variety of domains, for more languages, and for more applications (NLG as well as MT).
The framework uses the same engine for all the transformations at all levels because all the syntactic and conceptual structures are represented as dependency tree structures.
In contrast, the predecessor systems were not designed to be rapidly portable.
These systems used programming languages or scripts for the implementation of the transformation rules, and used different types of processing at different levels of representation.
For instance, in LFS conceptual structures were represented as graphs, whereas syntactic structures were represented as trees which required different types of processing at these two levels.
Our approach also has some disadvantages compared with the systems mentioned above.
Our lexico-structural transformations are far less powerful than those expressible using an arbitrary programming language.
In practice, the formalism that we are using for expressing the transformations is inadequate for long-range phenomena (inter-sentential or intra-sentential), including syntactic phenomena such as longdistance wh-movement and discourse phenomena such as anaphora and ellipsis.
The formalism could be extended to handle intrasentential syntactic effects, but inter-sentential discourse phenomena probably require procedural rules in order to access lexemes in The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS (Iordanskaja et al., 1992), and JOYCE (Rambow and Korelsky, 1992).
The framework was originally developed for the fiother sentences.
In fact, LFS and JOYCE include a specific module for elliptical structure processing.
Similarly, the limited power of the tree transformation rule formalism distinguishes the framework from other NLP frameworks based on more general processing paradigms such as unification of FUF/SURGE in the generation domain (Elhadad and Robin, 1992).
9 Status
The framework is currently being improved in order to use XML-based specifications for representing the dependency structures and the transformation rules in order to offer a more standard development environment and to facilitate the framework extension and maintenance.
Acknowledgements A first implementation of the framework (C++ processor and ASCII formalism for expressing the lexico-structural transformation rules) applied to NLG was developed under SBIR F30602-92-C-0015 awarded by USAF Rome Laboratory.
The extensions to MT were developed under SBIR DAAL01-97-C-0016 awarded by the Army Research Laboratory.
The Java implementation and general improvements of the framework were developed under SBIR DAAD17-99-C-0008 awarded by the Army Research Laboratory.
We are thankful to Ted Caldwell, Daryl McCullough, Alexis Nasr and Mike White for their comments and criticism on the work reported in this paper.
REES: A Large-Scale Relation and Event Extraction System Abstract This paper reports on a large-scale, end-toend relation and event extraction system.
At present, the system extracts a total of 100 types of relations and events, which represents a much wider coverage than is typical of extraction systems.
The system consists of three specialized pattem-based tagging modules, a high-precision coreference resolution module, and a configurable template generation module.
We report quantitative evaluation results, analyze the results in detail, and discuss future directions.
Introduction One major goal of information extraction (IE) technology is to help users quickly identify a variety of relations and events and their key players in a large volume of documents.
In contrast with this goal, state-of-the-art information extraction systems, as shown in the various Message Understanding Conferences (MUCs), extract a small number of relations and events.
For instance, the most recent MUC, MUC-7, called for the extraction of 3 relations (person-employer, maker-product, and organization-location) and 1 event (spacecraft launches).
Our goal is to develop an IE system which scales up to extract as many types of relations and events as possible with a minimum amount of porting effort combined with high accuracy.
Currently, REES handles 100 types of relations and events, and it does so in a modular, configurable, and scalable manner.
Below, Section 1 presents the ontologies of relations and events that we have developed.
Section 2 describes REES' system architecture.
Section 3 evaluates the system's performance, and offers a qualitative analysis of system errors.
Section 4 discusses future directions.
1 Relation
and Event Ontologies As the first step in building a large-scale relation and event extraction system, we developed ontologies of the relations and events to be extracted.
These ontologies represent a wide variety of domains: political, financial, business, military, and life-related events and relations.
"Relations" covers what in MUC-7 are called Template Elements (TEs) and Template Relations (TRs).
There are 39 types of relations.
While MUC TE's only dealt with singular entities, REES extracts both singular and plural entities (e.g., "five executives").
The TR relations are shown in italic in the table below.
Relations 'Artifact Relations Artifact-Name&Aliases Artifact-Type Artifact-Subtype Artifact-Descriptor Place Relations Place-Name&Aliases Place-Type Place-Subtype Place-Descriptor Place-Country Artifact-Maker Artifact-Owner Person Relations Person-Name&Aliases Person-Type Person-Subtype Person-Descriptor Person-Honorific Person-Age Person-PhoneNumber Person-Nationality Organization Relations Org-Name&Aliases Org-Descriptor Org-FoundationDate Org-Nationality Org-TickerSymbol Org-Location Org-ParentOrg Org-Owner Org-Founder Org-StockMarket Person-Affiliation Person-Sibling Person-Spouse Person-Parent Person-Grandparent Person-OtherRelative Person-BirthPlace Person-BirthDate Table 1: Relation Ontology "Events" are extracted along with their event participants, e.g., "who did what to whom when and where"?
For example, for a BUYING event, REES extracts the buyer, the artifact, the seller, and the time and location of the BUYING event.
REES currently covers 61 types of events, as shown below.
Figures 1 and 2 show sample relation and event templates.
Figure 1 shows a Person-Affiliation relation template for "Frank Ashley, a spokesman for Occidental Petroleum Corp'".
<PERSON TYPE: PERSON: ORG: AFFILIATION-AP8802230207-54> := PERSON AFFILIATION [TE for"Frank Ashley"] [TE for "Occidental Petroleum"] Figure 1: Example of Relation Template Figure 2 shows an Attack Target event template for the sentence "an Iraqi warplane attacked the frigate Stark with missiles May 17, 1987.
" <ATTACK TARGET-AP8804160078-12>: = TYPE: CONFLICT SUBTYPE: ATTACK TARGET ATTACKER: [TE for "an Iraqi warplane"] TARGET: [TE for "the frigate Stark"] WEAPON: [TE for "missiles"] TIME: "May 17, 1987" PLACE: [TE for "the gulf'] COMMENT: "attacked" Events Vehicle Vehicle departs Vehicle arrives Spacecraft launch Vehicle crash Personnel Change Hire Terminate contract Promote Succeed Start office Transaction Buy artifact Sell artifact Import artifact Export artifact Give money Business Start business Close business Make artifact Acquire company Sell company Sue organization Merge company Financial Currency moves up Currency moves down Stock moves up Stock moves down Stock market moves up Stock market moves down Stock index moves up Stock index moves down Conflict Kill Injure Hijack vehicle Hold hostages Attack target Fire weapon Weapon hit Invade land Move forces Retreat Surrender Evacuate Figure 2: Example of Event Template Crime Sexual assault Steal money Seize drug Indict Arrest Try Convict Sentence Jail Political Nominate Appoint Elect Expel person Reach agreement Hold meeting Impose embargo Topple Family Die Marry System Architecture and Components Figure 3 illustrates the REES system architecture.
REES consists of three main components: a tagging component (cf.
Section 2.1), a co-reference resolution module (cf.
Section 2.2), and a template generation module (cf.
Section 2.3).
Figure 3 also illustrates that the user may run REES from a Graphical User Interface (GUI) called TemplateTool (cf.
Section 2.4).
Tagging Modules The tagging component consists of three modules as shown in Figure 3: NameTagger, NPTagger and EventTagger.
Each module relies on the same pattern-based extraction engine, but uses different sets o f patterns.
The NameTagger recognizes names o f people, organizations, places, and artifacts (currently only vehicles).
Table 2: Event Ontology GUI interaction Figure 3: The REES System Architecture syntactically-based generic patterns.
These The NPTagger then takes the XML-tagged output of the NameTagger through two phases.
First, it recognizes non-recursive Base Noun Phrase (BNP) (our specifications for BNP resemble those in Ramshaw and Marcus 1995).
Second, it recognizes complex NPs for only the four main semantic types of NPs, i.e., Person, Organization, Location, and Artifact (vehicle, drug and weapon).
It makes postmodifier attachment decisions only for those NPs that are crucial to the extraction at hand.
During this second phase, relations which can be recognized locally (e.g., Age, Affiliation, Maker) are also recognized and stored using the XML attributes for the NPs.
For instance, the XML tag for "President of XYZ Corp".
below holds an AFFILIATION attribute with the ID for "XYZ Corp".
<PNP ID="03" AFFILIATION="O4">Presidentof <ENTITY ID="04">XYZ Corp.</ENTITY> </PNP> patterns tag events in the presence of at least one of the arguments specified in the lexical entry for a predicate.
Subsequent pattems try to find additional arguments as well as place and time adjunct information for the tagged event.
As an example of the EventTagger's generic patterns, consider the simplified pattern below.
This pattem matches on an event-denoting verb that requires a direct object of type weapon (e.g., "fire a gun") (& {AND $VP {ARG2_SYN=DO} {ARG2_SEM=WEAPON}} {AND $ARTIFACT {SUBTYPE=WEAPON}})1 The important aspect of REES is its declarative, lexicon-driven approach.
This approach requires a lexicon entry for each event-denoting word, which is generally a I &=concatenation, AND=Boolean operator, $VP and SARTIFACT are macro references for complex phrases.
71:1 Building upon the XML output of the NPTagger, the EventTagger recognizes events applying its lexicon-driven, fiverb.
The lexicon entry specifies the syntactic and semantic restrictions on the verb's arguments.
For instance, the following lexicon entry is for the verb "attack".
It indicates that the verb "attack" belongs to the CONFLICT ontology and to the ATTACK_TARGET type.
The first argument for the verb "attack" is semantically an organization, location, person, or artifact (ARGI_SEM), and syntactically a subject (ARGI_SYN).
The second argument is semantically an organization, location, person or artifact, and syntactically a direct object.
The third argument is semantically a weapon and syntactically a prepositional phrase introduced by the preposition "with".
ATTACK {{{CATEGORY VERB} {ONTOLOGY CONFLICT} {TYPE ATTACK_TARGET} {ARGI_SEM {ORGANIZATION LOCATION PERSON ARTIFACT} } {ARGI_SYN {SUBJECT}} {ARG2_SEM {ORGANIZATION LOCATION PERSON ARTIFACT} } {ARG2_SYN {DO}} {ARG3_SEMWEAPON} } {ARG3_SYN {WITH}}}} About 50 generic event extraction patterns, supported by lexical information as shown above, allow extraction of events and their arguments in cases like: An lraqi warplane attacked the frigate Stark with missiles May 17, 1987.
This generic, lexicon-driven event extraction approach makes REES easily portable because new types of events can be extracted by just adding new verb entries to the lexicon.
No new patterns are required.
Moreover, this approach allows for easy customization capability: a person with no knowledge of the pattern language would be able to configure the system to extract new events.
While the tagging component is similar to other pattern-based IE systems (e.g., Appelt et al.1995; Aone et al.1998, Yangarber and Grishman 1998), our EventTagger is more portable through a lexicon-driven approach.
Co-reference Resolution After the tagging phase, REES sends the XML output through a rule-based co-reference resolution module that resolves:   definite noun phrases of Organization, Person, and Location types, and singular person pronouns: he and she.
Only "high-precision" rules are currently applied to selected types of anaphora.
That is, we resolve only those cases of anaphora whose antecedents the module can identify with high confidence.
For example, the pronoun rules look for the antecedents only within 3 sentences, and the definite NP rules rely heavily on the head noun matches.
Our highprecision approach results from our observation that unless the module is very accurate (above 80% precision), the coreference module can hurt the overall extraction results by over-merging templates.
Template Generation Module A typical template generation module is a hard-coded post-processing module which has to be written for each type of template.
By contrast, our Template Generation module is unique as it uses declarative rules to generate and merge templates automatically so as to achieve portability.
Declarative Template Generation REES outputs the extracted information in the form of either MUC-style templates, as illustrated in Figure 1 and 2, or XML.
A crucial part of a portable, scalable system is to be able to output different types of relations and events without changing the template generation code.
REES maps XML-tagged output of the co-reference module to templates using declarative template definitions, which specifies the template label (e.g., ATTACK_TARGET), XML attribute names (e.g., ARGUMENT l), corresponding template slot names (e.g., ATTACKER), and the type restrictions on slot values (e.g., string).
Event Merging One of the challenges of event extraction is to be able to recognize and merge those event descriptions which refer to the same event.
The Template Generation module uses a set of declarative, customizable rules to merge coreferring events into a single event.
Often, the rules reflect pragmatic knowledge of the world.
For example, consider the rule below for the DYING event type.
This rule establishes that if two die events have the same subject, then they refer to the same event (i.e., a person cannot die more than once).
{merge {EVENT 1 {AND {SUBTYPE DIE} {PERSON training set (200 texts) and the blind set (208 texts) from about a dozen news sources.
Each set contains at least 3 examples of each type of relations and events.
As we mentioned earlier, "relations" includes MUC-style TEs and TRs.
Text Set Task Train Blind Rel.
Events Rel.
& Events Rel.
Events Rel.
& Events Templates in keys 9955 2525 10707 F-M {EVENT 2 {AND {SUBTYPE DIE} {PERSON Table 3: Evaluation Results 2.4 Graphical User Interface (GUI) For some applications such as database population, the user may want to validate the system output.
REES is provided with a Javabased Graphical User Interface that allows the user to run REES and display, delete, or modify the system output.
As illustrated in Figure 4, the tool displays the templates on the bottom half of the screen, and the user can choose which template to display.
The top half of the screen displays the input document with extracted phrases in different colors.
The user can select any slot value, and the tool will highlight the portion of the input text responsible for the slot value.
This feature is very useful in efficiently verifying system output.
Once the system's output has been verified, the resulting templates can be saved and used to populate a database.
3 System
Evaluation The blind set F-Measure for 31 types of relations (73.95%) exceeded our initial goal of 70%.
While the blind set F-Measure for 61 types o f events was 53.75%, it is significant to note that 26 types of events achieved an FMeasure over 70%, and 37 types over 60% (cf.
Table 4).
For reference, though not exactly comparable, the best-performing MUC-7 system achieved 87% in TE, 76% in TR, and 51% in event extraction.
F-M in blind set 90-100 80-89 Event types 2 : Buy artifact.
Marry 9 : Succeed, Merge company, Kill, Surrender, Arrest, Convict, Sentence, Nominate, Expel.
15 : Die, Sell artif~/ct,Export Artifact, Hire, Start office, Make artifact, Acquire company, Sue organization, Stock Index moves down, Steal money, Indict, Jail, Vehicle crash, Elect, Hold meeting.
Table 4: Top-performing Event Types The table below shows the system's recall, precision, and F-Measure scores for the Regarding relation extraction, the difference in the score between the training and blind sets was very small.
In fact, the total F-Measure on the blind set is less than 2 points lower than that of the training set.
It is also interesting to note that for 8 of the 12 relation types where the F-Measure dropped more than 10 points, the training set includes less than 20 instances.
In other words, there seems to be a natural correlation between low number of instances in the training set and low performance in the blind set.
There was a significant drop between the training and blind sets in event extraction: 11 points.
We believe that the main reason is that the total number of events in the training set is fairly low: 801 instances of 61 types of events (an average of 13/event), where 35 o f the event types had fewer than 10 instances.
In fact, 9 out of the 14 event types which scored lower than 40% F-Measure had fewer than I0 examples.
In comparison, there were 34,000 instances of 39 types of relations in the training set.
The contribution o f the co-reference module is illustrated in the table below.
Co-reference resolution consistently improves F-Measures both in training and blind sets.
Its impact is larger in relation than event extraction.
Text set Task Coreference rules No coreference rules Training Blind Relations Events Relations & Events Relations Events Relations & Events Table 5: Comparative results with and without co-reference rules In the next two sections, we analyze both false positives and false negatives.
False Positives (or Precision Errors) REES produced precision errors in the following cases:  Most of the errors were due to overgeneration of templates.
These are mostly cases of co-referring noun phrases that the system failed to resolve.
For example: "Panama...
the nation ...
this country.., his country" Rules for the co-reference module are still under development, and at present REES handles only limited types of plural noun phrase anaphora.
Spurious events resulted from verbs in conditional constructions (e.g., "if ...
then")... or from ambiguous predicates.
For instance, "appoint" as a POLITICAL event vs.
a PERSONNEL CHANGE event.
The subject of a verb was misidentified.
This is particularly frequent in reduced relative clauses.
Kabul radio said the latest deaths brought to 38 the number o f people killed in the three car bomb explosions, (Wrong subject: "the number of people" as the KILLER instead of the victim) False Negatives (or Recall Errors) Below, we list the most frequent recall errors in the training set.
 Some event arguments are mentioned with event nouns instead of event verbs.
The current system does not handle noun-based event extraction.
India's acquisition last month of the nuclear submarine from the Soviet Union...
(SELLER="Soviet Union" and TIME="last month'" come with the nounbased event "acquisition").
 Pronouns "it" and "they," which carry little semantic information, are currently not resolved by the co-reference module.
It also has bought three late-1970s vintage ICilo class Soviet submarines and two West German HDW 209 subs (Missed BUYER=India because of unresolved it).
Verb arguments are a conjunction of noun phrases.
The current system does not handle coordination of verb arguments.
Hezbollah killed 21 lsraelis and 43 o f Lahad's soldiers (The system gets only the first object: 21 Israelis.
) Ellipsis cases.
The current system does not handle ellipsis.
The two were sentenced to five-year prison terms with hard labor by the state security court...
(Missed PERSON_SENTENCED fill because of unresolved the two).
The subject of the event is relatively far from the event-denoting verb: Vladislav Listyev, 38, who brought television interview shows in the style of Phil Donahue or Larry King to Russian viewers and pioneered hard-hitting television journalism in the 1980s, was shot in the heart by unknown assailants and died immediately...
(The system missed subject Vladislav Listyev for attack event shot) Missed ORG LOCATION relations for locations that are part o f the organization's name.
Larnaca General Hospital (Missed ORG_LOCATION TR for this and Larnaca.
) We asked a person who is not involved in the development of REES to review the event extraction output for the blind set.
This person reported that:  In 35% of the cases where the REES system completely missed an event, it was because the lexicon was missing the predicate.
REES's event predicate lexicon is rather small at present (a total of 140 verbs for 61 event types) and is mostly based on the examples found in the training set,  In 30% of the cases, the subject or object was elliptical.
The system does not currently handle ellipsis.
In 25% of the cases, syntactic/semantic argument structures were missing from existing lexical entries.
It is quite encouraging that simply adding additional predicates and predicate argument structures to the lexicon could significantly increase the blind set performance.
Desmond Tutu and Albertina Sisulu are important...
We plan to develop a generic set of patterns for noun-based event extraction to complement the set of generic verb-based extraction patterns.
5 4 Future Directions We believe that improving co-reference resolution and adding noun-based event extraction capability are critical to achieving our ultimate goal of at least 80% F-Measure for relations and 70% for events.
4.1 Co-reference Resolution Conclusions As discussed in Section 3.1 and 3.2, accurate co-reference resolution is crucial to improving the accuracy of extraction, both in terms of recall and precision.
In particular, we identified two types of high-payoff coreference resolution:  definite noun phrase resolution, especially plural noun phrases  3 rd person neutral pronouns "it" and "they".
4.2 Noun-based Event Extraction In this paper, we reported on a fast, portable, large-scale event and relation extraction system REES.
To the best of our knowledge, this is the first attempt to develop an IE system which can extract such a wide range of relations and events with high accuracy.
It performs particularly well on relation extraction, and it achieves 70% or higher F-Measure for 26 types of events already.
In addition, the design of REES is highly portable for future addition of new relations and events.
Acknowledgements This project would have not been possible without the contributions of Arcel Castillo, Lauren Halverson, and Sandy Shinn.
Our thanks also to Brandon Kennedy, who prepared the hand-tagged data.
REES currently handles only verb-based events.
Noun-based event extraction adds more complexity because: Nouns are often used in a generic, nonreferential manner (e.g., "We see a m e r g e r as being in the consumer's interest"), and When referential, nouns often refer to verb-based events, thus requiring nounverb co-reference resolution ("An F-14 crashed shortly after takeoff...
The crash").
PROBLEMS IN NATURAL-LANGUAGE INTERFACE WITH EXAMPLES FROM EUFID Marjorie T e m p l e t o n John Burger S y s t e m Development Corporation Santa Mortice, California TO DSMS ABSTRACT For five years t h e End-User Friendly Interface to Data management (EUFID) project team at System Development Corporation worked on the design and implementation of a Natural-Language Interface (NLI) system that was to be independent of both the application and the database management system.
In this paper we describe application, n a t u r a l -l a n g u a g e and d a t a b a s e management problems involved in NLI development, with specific reference to the EUFID system as an example.
I INTRODUCTION users.
Tools that could assist in automating this process are badly needed.
The second set of issues involves language processing techniques: how to assign constituent structure and interpretation to queries using robust and general methods that allow extension to additional lexical items, sentence types and semantic relationships.
Some NLI systems d i s t i n g u i s h the assignment of syntactic structure, o r parsing, from the interpretation.
Other systems, including EUFID, combine information about constituent and semantic structure into an integrated semantic grammar.
The third class involves database issues: how to actually perform the intent of the natural-language question by formulating the correct structured query and e f f i c i e n t l y n a v i g a t i n g through the database to retrieve the right answer.
This involves a thorough understanding of the DBMS structure underlying the a p p l i c a t i o n, the operations and functions the query language supports, and the nature and volatility of the database.
Obviously issues in these three areas are related, and the knowledge needed to deal with them may be distributed throughout a natural-language interface system.
The purpose of this paper is to show how such issues might be addressed in NLI development, with illustrations from EUFID.
The next section includes a brief review of related work, and an o v e r v i e w of the EUFID system.
The third section describes the goals that EUFID achieved, and section four discusses in detail ~ome of the major application, language, and database problems that arose.
Section five suggests guidelines for determining whether an application is an appropriate target for a n a t u r a l l a n g u a g e interface.
From 1976 t o 1981 SDC was involved in the development of the End-User Friendly Interface to Data management (EUFID) system, a n a t u r a l l a n g u a g e interface (NLI) that is designed to be independent of both the application and the underlying d a t a b a s e management system (DBMS).
[TEMP79, TEMP80, BURG80, BURG82].
The EUFID system permits users to communicate with database management systems in natural English rather than formal query languages.
It is assumed that the application domain is well defined and bounded, that users share a common language to address the application, and that users may have little experience with computers or DBMSs but are competent in the application area.
At least three broad categories of issues had to be addressed during EUFID development, and it is apparent that they are common to any general naturallanguage interface to database management systems.
The first category involves the application: how to c h a r a c t e r i z e the requirements of the human-machine dialogue and interaction, capture that information efficiently, formalize the information and incorporate that knowledge into a framework that can be used by the system.
The major problems in this area are knowledge acquisition and representation.
For many NLI systems, bringing up a new application requires extensive effort by system designers with cooperation from a representative set of endfiII BACKGROUND Over the past two decades a considerable amount of work has gone into the d e v e l o p m e n t of natural-language systems.
Early developments were in the areas of text processing, syntactic parsing techniques, machine translation, and early attempts at English-language question answering systems.
Several early question-answering experiments are reviewed by R.
F. Simmons in [SIMM65].
Waltz has edited a collection of short papers on topics related to naturallanguage and artificial intelligence in a survey of NLI research [WALT77].
A survey of NLIs and evaluation of several systems with respect t o their applicability to command and control environments can be found in [OS179].
A. RELATED WORK involved with problems of semantics and has three separate layers of semantic u n d e r s t a n d i n g. The layers are called "English Formal Language", "World Model Language", and "Data Base Language" and appear to c o r r e s p o n d roughly to the "external", "conceptual", and "internal" views of data as d e s c r i b e d by C.
J. Date [DATE77].
PHLIQAI can interface to a v a r i e t y of d a t a b as e structures and DBMSs.
5. The Programmed LANguage-based Enquiry System (PLANES) [WALT78] uses an ATN based parser and a semantic case frame analysis to understand questions.
Case frames are used to handle pronominal and elliptical reference and to g e n e r a t e responses to clarify partially interpreted questions.
REL [THOM69], initially written entirely in assembler code for an IBM36@, has been in continuous development since 1967.
REL allows a user to make interactive extensions to the g r a m m a r and semantics of the system.
It uses a formal grammar expressed as a set of general re-write rules with semantic transformations attached to each rule.
Answers are obtained from a b u i l t i n database.
RENDEZVOUS [CODD74] addresses the problem of c e r t a i n t y regarding the machine's understanding of the user's question.
It engages the user in d i a l o g u e to specify and disambiguate the question and will not route the formal query to the relational DBMS until the user is satisfied with the machine's interpretation.
ROBOT [HARR78] is one of the few NLI systems currently a v a i l a b l e on the commercial market.
It is the basis for Cullinane's OnLine English [CULL80] and Artificial Intelligence C o r p o r a t i o n ' s Intellect [EDP82].
It uses an extracted version of the database for lexical data to assist the ATN parser.
TORUS [MYLO76], like RENDEZVOUS, engages the user in a d i a l o g u e to specify and d i s a m b i g u a t e the user's question.
It is a research o r i e n t e d system looking at the problems of knowledge representation, and some effort has been spent on the understanding of text as well as questions.
While few NLIs have reached the commercial marketplace, many systems have c o n t r i b u t e d to advancing the state of the art.
Several representative systems and the problems they addressed are described in this section.
i. CONVERSE [KELLT1] used formal syntactic analysis to g e n e r a t e surfaceand d e e p s t r u c t u r e parsings together with formal semantic t r a n s f o r m a t i o n rules to produce queries for a built-in relational DBMS.
It was written in SDC LISP and ran on IBM 37@ computers.
Started in 1968, it was one of the first naturallanguage processors to be built for the purpose of querying a separate data m a n a g e m e n t system.
LADDER [HEND77] was designed to access large d i s t r i b u t e d databases.
it is implemented in INTERLISP, runs on a PDP-I@, and can interface to different DBMSs with proper configuration.
It uses a semantic g r a m mar and, like EUFID and most NLIs, a different grammar must be defined for each application.
The Lunar Rocks system LSNLIS [WOOD72] was the first to use the Augmented Transition Network (ATN) grammar.
Wrl~ten in LISP, it transformed formally parsed questions into representations of the first-order predicate calculus for deductive processing against a built-in DBMS.
PHLIQAI [SCHA77] uses a syntactic parser which runs as a separate pass from the semantic understanding passes.
This system is mainly fiB.
OVERVIEW OF EUFID EUFID is a general purpose naturallanguage front-end for database management.
The original design goals for EUFID were: to b e application independent.
This means that the program must be table driven.
The tables contain the dictionary and semantic information and are loaded with a p p l i c a t i o n s p e c i f l c data.
It was desired that the tables could be constructed by someone other than the EUFID staff, so t h a t users could build new applications on their o w n . to be database independent.
This means that the organization of the data in the database must be representable in tables that drive the query generator.
~ A database reorganization that does not change the semantics of the application should be transparen~ to the user.
written in a high level language; initially a customer required code to be written in FORTRAN, later we were able to use the "C" programming language.
to support different views data for security purposes.
of the The design which met these requirements is a modular system which uses an Intermediate Language (IL) as the output of the natural-language analysis system [BURG82].
This language represents, in many ways, the union of the c a p a b i l i t i e s of many "target" DBMS q u e r y languages.
The EUFID system consists of three major modules, not counting the DBM3 (see Figure I).
The analyzer (parser) module is table driven.
It is n e c e s s a r y only to properly build and load the tables to interface EUFID to a new application.
Mapping a question from its d i c t i o n a r y (user) representation to DBMS representation is handled by mapping functions contained in a table and applied by a separate module, t h e "mapper".
Each c o n tent (application dependent) word in the d i c t i o n a r y has one or more mapping functions defined for it.
A final stage of the mapper is a q u e r y l a n g u a g e generator containing the syntax of IL.
This stage writes a query in IL using the group/field names found by the mapper t o represent the user's concepts and the structural relationships between them.
This design satisfies t h e requirement of application independence.
ENGLISH QUESTION to be DBMS independent.
This means that it must be able to generate requests to different DBMSs in the DBMS's query language and that the interface of EUF~D to a different DBMS should not require changes to t h e NLI modules.
Transferring the same database with the same semantic content to another DBMS should be transparent to the natural-language users.
to run on a mini-computer that might possibly be different from the computer with the DBMS.
to have a fast response time, even when the question cannot be interpreted.
This means it must be able quickly to recognize unanalyzable constructs.
Figure i: EUFID Block Diagram to handle nonstandard or poorlyformed (but, nevertheless, meaningful) questions.
to be portable to various machines.
This means that the system had to be * We make a technical distinction between the words "question" and "query".
A question is any string entered by the user to the EUFID analyzer, regardless of the terminating punctuation.
This is consistent with the design since EUFID treats all input as a request for information.
A query is a formal representation of a question in either the EUFID intermediate language IL, or in the formal query language of a DBMS.
For each different DBMS used by a EUFID application, a "translator" module needs to be written to convert a query in IL to the equivalent in the DBMS query language.
This design satisfies the requirement of DBMS independence.
Other modules are the system controller, a "help" module, and a " s y n o n y m editor".
An "Application Definition Module" is used off-line to assist in the creation of the run-time application description tables.
The following subsections descrloe each of the modules of the EUFID system, and give our m o t i v a t i o n for design.
i. A~plication Definitions Bringing up a new a p p l i c a t i o n is a long and complex process.
The d a t a b a s e d e f i n i t i o n must be transmitted to EUFID.
A large corpus of "typical" user questions must be collected from a representative set of users and from these the dictionary and mapping tables are designed.
A "semantic graph" is defined for the application.
This graph is implicitly realized in t h e dictionary where the nodes of the graph are the definitions of English content words and the c o n n e c t i v i t y of the graph is implied by the case-structure relationships defined for the nodes.
All d i c t i o n a r y and mapping-function are then entered into computer files which are processed by the Application Definition Module (ADM) to produce t h e run-time tables.
These final tables are complex structures of pointers, character strings, and index tables, designed to decrease access time to the information required by the analyzer and mapper modules.
data considered.
Frequently, desig~ :o,~s i d e r a t i o n s in the m a p p i n g f u n c t i o n list necessitate going back and m o d i f y i n g the content of the d i c t i o n a r y . This is an example of the o v e r l a p of the l i n g u i s t i c and database issues in assigning an interpretation to a question.
c. Database Representation The ADM, typically, needs to be run several times to "debug" the tables.
EUFID interfaces to three applications currently exist, and building tables for each new a p p l i c a t i o n took less time than the previous one, b u t it still requires several staff-months to bring up a new application.
a. User-View Representation The structure o f the data in the user's database is represented in two tables, called the CAN (for canonical) and REL (for relationships) tables.
Taking advantage of the fact that any database can be represented in relational form, EUFID lists each d a t a b a s e g r o u p as if it were a relation.
Group-to-group linkage (represented in the REL table) is d e a l t with as if a join* were necessary to implement the link.
For h i e r a r c h i c a l and network DBMSs the join will not be needed: the link is "wired in" to the d a t a b a s e structure.
EUFID nevertheless assumes a join m a i n l y in order to facilitate the writing of g r o u p t o g r o u p links in IL, which is a relational language.
The CAN table includes database-specific information for each field (attribute) of each group (relation), such as field name, containing group, name of d o m a i n from which attributed gets its values, and a pointer to a set of c o n v e r s i o n functions for numeric v a l u e s which can be be used to convert from one unit of m e a s ure to another (e.g., feet to meters).
These data are used by the run-time modules which map and translate the t r e e s t r u c t u r e d output of the analyzer to IL on the actual g r o u p / f i e l d names of the database, and then co the language of the DBMS.
These modules are d i s c u s s e d in the next sections.
2. The EUFID Analyzer All information on the user's view of the database is kept in the d i c t i o n ary.
The dictionary consists of two kinds of words and definitions.
Function words, such as p r e p o s i t i o n s and Conjunctions, are pre-stored in each a p p l i c a t i o n ' s d i c t i o n a r y and are used by the analyzer for direction on how to connect the semantic-graph nodes during analysis.
Content words are application dependent.
The d c r O o n s of content words are semantic-graph nodes.
The connectivity o the graph is indicated by semantic case slots and pointers contained in the nodes.
A form of semantic-case is used to indicate the attributes of an entity (e.g., adjectives, prepositional phrases, and other modifiers of a noun).
b. Mapping Functions The current version of the EUFID analyzer employs a variant of the CockeK a s a m i Y o u n g e r algorithm for parsing its input.
This classical nonpredictive b o t t o m u p algorithm has been used in a family of "chart parsers" developed by Kay, Earley, and others [AHO72].
The main features of these parsers are: (i) They use a r b i t r a r y c o n t e x t f r e e grammars.
There are no r e s t r i c t i o n s on rules which have l e f t r e c u r s i o n or other c h a r a c t e r i s tics which sometimes cause difficulty.
(2) They produce all possible parses of a given input string.
The g r a m m a r s they use may be ambiguous at either the nonterminalor t e r m i n a l s y m b o l levels.
In natural-language processing, this allows for a precise r e p r e s e n t a t i o n of * The t e r m "join" refers to a composite o p e r a t i o n between two relations in a relational DBMS.
The list of mapping functions is derived from the dictionary.
Every possible connection of every node has to be fiboth the syntactic and lexical ambiguities which may be present in an input sentence.
(3) They provide partial parses of the input.
Each non-terminal symbol derives some input substring.
Even if no such substring spans the entire sentence, i.e., no complete parse is achieved, analyses of various regions o f t h e s e n t e n c e a r e available.
(4) They are conceptually straightforward and easy t o implement.
The speed and storage considerations which have kept such parsers from being widely used in compilers are less relevant in the analysis o f short strings such as queries to a DBMS.
The grammar used b y the EUFID parser is essentially semantic.
The symbols of the grammar r e p r e s e n t t h e concepts underlying lexical items, and the rules specify the ways in which these concepts can be combined.
More s p e c i f i c a l l y, the concepts are o r g a n i z e d into a case system.
Each rule states that a given pair of constituents can be linked if the conc e p t u a l head of o n e constituent fills a case on the conceptual head of t h e other.
A degree of context sensitivity is achieved b y attaching predicates to the rules.
These predicates b l o c k application of t h e rules unless certain (usually syntactic) conditions hold true.
The parser uses syntactic information only "on demand", that is, only when such information is necessary to resolve semantic ambiguities.
This a d d s to its coverage and robustness, and makes it relatively insensitive to the phrasing variations which must be explicitly accounted for in many other systems.
3. Mapping to-field and g r o u p t o g r o u p tions of t h e database.
connecThe mapper makes use of a table of mapping functions.
The table contains at least one mapping function for every content word in the dictionary.
The analyzer's tree is traversed bottom up, applying mapping functions to each node on t h e way.
Mapping f u n c t i o n s are context sensitive with respect to those nodes below it in the tree: nodes that have already been mapped.
A new tree is g r a d u a l l y formed and connected this way.
Mapping functions may indicate that the map of a semantic-graph node is a database node (that is, a group or field name), o r a pre-connected sub-tree of database nodes.
The mapping function may also indicate removal of a database node or m o d i f i c a t i o n to the existing structure of the tree being constructed.
The new t r e e i s c r e a t e d in terms of the database groups and f i e l d s and i t s structure reflects the connectivity of the database.
A final stage of the mapper traverses this new tree and generates the EL statement of the query using a table of the syntax and keywords of EL and the database names from the tree.
The mapper module converts the output of the analyzer to input for the translator module.
Analyzer output is a tree structure where the nodes are semantic-graph nodes corresponding to the content words in the user's question and obtained from the dictionary.
An alternative method of mapping that is now being investigated involves breaking the process into two basic parts.
The first step would be to map the tree o u t p u t o f the analyzer t o an IL query on what C.
J. Date calls the "conceptual schema" of the database [DATE77].
A second step would take this IL input and re-arrange the schema connectivity (and names of groups and fields) from that of the conceptual schema to that of the actual target database, generating another IL query as input to the current translators.
Input to the translator module is a string in the syntax of IL which contains the names of actual groups and fields in the database.
The mapping algorithm, thus, has to make several levels of conversion simultaneously: it must convert a into a linear string it must convert into database names, and tree structure of tokens, semantic-graph nodes groupand fieldit must convert the connectivity of the tree (representing concept-toconcept linkage in English) into the (frequently very different) groupThe final run-time module in EUFID is a syntax translator that converts IL to the actual DBMS query language.
If necessary, the translator can also add access-path information related t o database search.
Currently, two translators have been written.
One converts IL to QUEL, a relatively simple conversion into the language of the relational database management system INGRES [STONY6].
The other translator converts IL into the query language of the World-Wide Data Management System (WWDMS) [HONE76] used by the Department of Defense, and also handles additional access path information.
This translator was quite difficult to design and build because of the highly procedural nature of the WWDMS query system.
The output of a translator is sent to the appropriate DBMS.
In the EUFID system running at SDC, a QUEL query is submitted directly to INGRES running on the same PDP-II/70 as EUFID.
For testing purposes, queries generated by the WWDMS translator were transmitted from a PDP11/70 to a Honeywell H6000 with a WWDMS database.
5. Application Description some coming from open-ended domains.
A I R E P has a network database structure and contains the same data s t r u c t u r e in four d i f f e r e n t files.
III LEVEL OF SUCCESS EUFID runs o n three d i f f e r e n t application databases.
The METRO a p p l i c a t i o n involves monitoring of shipping transactions between companies in a city called "Metropolis".
There are ten companies located in any one of three n e i g h b o r hoods.
Each company rents warehouse space for shipping/recelving transactions, and has local offices which receive goods.
The data is organized telationally using the INGRES database m a n a g e m e n t system.
That means that there are no n a v i g a t i o n a l links stored in the records (called "relations") and there is no predefined "root" to the database structure.
Access may be made from any relation to any other relation as long as there is a field in each of the two relations which has the same "domain" (set of values).
AIREP (ADP Incident REPorting) is a network database, implemented in WWDMS.
It c o n t a i n s reports about hardware and software failures and resolution of the problems in a large computer system.
Active problems are maintained in an active file and old, solved problems are moved to an historical file.
If a problem [s reported more than once, an abbreviated record is made for the additional report, called the "duplicate incident" record.
This means that there are four basic type of report: active incidents, duplicate incidents, historical incidents, and historical duplicate incidents.
In addition, there are records about sites, problems, and solutions.
The A P P L I C A N T database is a relational database implemented in INGRES that contains information about job applicants and their backgrounds.
The central entity is the "applicant", while other relations describe the a p p l i c a n t ' s specialties, education, previous employment, computer experience, and interviews.
Each database has d i f f e r e n t features chat may present problems for a naturallanguage interface but which are typical of 'real-world' applications.
METRO has relatively few entities but has complex relationships among them.
APPLICANT has many updates and many different values, Most of the EUFID d e s i g n g o a l s were actually met.
EUFID runs on a minicomputer, a DEC PDP 11/70.
It is application, database, and DBMS independent.
A typical q u e s t i o n is analyzed, mapped and translated in five to fifteen seconds even with g r a m m a t i c a l l y incorrect input.
The analyzer c o n t a i n s a good spelling corrector and a good morphology a l g o r i t h m that strips inflectional endings so that all inflected forms of words need not be stored explicitly.
A "synonym editor" permits the user to replace any word or string of words in the dicionary with another word or string, to accommodate personal jargon and expressability.
A "Concept Graph Editor s allows a database administrator to m o d i f y tables and define user profiles so that d i f f e r e n t users may have limited views of the data for s e c u r i t y purposes.
The analysis strategy, based on a semantic grammar, permits easy and natural paraphrase recognition, although there are linguistic c o n s t r u c t s it cannot handle.
These are d i s c u s s e d below.
An English word may have more than one definition without c o m p l i c a t i n g the analysis strategy.
For example, "ship" as a vessel and as a verb meaning "to send" can be defined in the same d i c t i o n ary.
Words used as database values, such as names, may also have m u l t i p l e definitions, e.g., "New York" used as the name of both a city and a state.
The mapper, despite its many limitations, can c o r r e c t l y map almost all trees output by the analyzer.
It is able to handle English c o n j u n c t i o n s, mapping them a p p r o p r i a t e l y to logical ANDs or ORs, and understanding that some "ands" may need to be interpreted as OR and vice-versa under certain c i r c u m s t a n c e s . It is able to g e n e r a t e calls on DBMS calculations (e.g., average) and user-defined functions (e.g., marine great-circle distance) if the user-function exists and is supported by the DBMS.
Questions involving time are interpreted in a reasonable way.
Functions are defined for "between" and "during" in the METRO application.
The AIREP application allows time comparisons such as "What system was running when incident J123 occurred" which require a test to see if a point in time is within an interval.
The mapper can translate "user values" (e.g., "Russian") to database values (e.g., "USSR"), and convert one unit of measure (e.g., feet) to another (e.g., meters).
EUFID c a n i n t e r f a c e to very complex relational and CODASYL-type databases having difficult n a v i g a t i o n and parallel structures.
In t h e AIREP application a consistent WWDMS navigational m e t h o d o l o g y is used to access non-key records.
The system c a n also map to t h e parallel, but not identical, structures for duplicate and historical incidents.
I n the INGRES applications, EUFID is able to use and correctly map to = r e l a tionship relations" which relate two or more other relations.
For example, the METRO relation =cw" contains a company name, a warehouse name, a n d a date.
This represents the initial business contact.
A user might ask, =When d i d C o l o n i a l start t o do b u s i n e s s w i t h Superior?
= or  When d i d b u s i n e s s b e g i n b e t w e e n C o l o n i a l and S u p e r i o r ? =, e i t h e r of which must ~oin both t h e c o m p a n y ( " c =) a n d t h e w a r e h o u s e ('w') relations t o the =cw" relation.
The system c o n t r o l module keeps a journal of all user-system interaction together with internal module-to-module data such as the IL for the user's question and the generated DBMS query.
The system also employs a very effective HELP module which, under certain circumstances, is context sensitive t o the problem affecting the user.
IV PROBLEMS APPLICANT database may wish to fill a specific Job opening while others may collect statistics on types of appli~ cants.
The language used for these two functions can be quite different, and it is n e c e s s a r y to have extensive interaction with cooperative users in order to characterize the kinds of dialogues they will have with the system.
Not only must representative language protocols be collected, but desired responses must be understood.
For example, to answer a question such as =What is t h e status of our forces in Europe =, the system must know whether 'our' refers to U.S. or NATO or some other unit.
The importance of this interaction between potential users and system developers should n o t b e underestimated, as it is the basis for defining much of the knowledge base needed by the system, and may also be t h e basis for eventual user acceptance o r rejection of the NLI system.
2. Value R e c o g n i t i o n This section describes problems associated with EUFID development that appear to be common to natural-language interfaces to database management systems.
They are loosely classified into the major areas Of application, language and database management issues, although there may be overlap.
Criteria for evaluating whether an application is appropriate for a natural-language front-end are also described.
A. APPLICATION DEFINITION PROBLEMS A "value = is a specific datum stored in the database, and is the smallest piece of data obtainable as the result o f a database query.
For example, in response to the question "What companies in North Hills shipped light freight to Superior?
= the METRO DBMS returns two values: "Colonial" and "Supreme'.
Values can also be used in a query to qualify or select certain records for output, e.g., in t h e above question "North Hills" and "Superior" are values that must be represented in the query to the DBMS.
As long as the alphanumeric values used in a particular database field are the same as words in t h e English questions, there are no difficult problems involved in recognizing values as selectors in a query.
There are three basic ways to recognize these value words in a question.
They can be explicitly listed in the dictionary, recognized by a pattern or context, or found in the database itself.
If the value words are stored in the dictionary, they can be subject to spelling correction because the spelling corrector uses the dictionary to locate words which are a close match to unrecognized words in a question.
This means, though, that all possible values and variant legitimate spellings of values for a concept must be put either into the dictionary or into the synonym list.
This is reasonable for concepts which have a small and controlled set of _values* such as the names of the * A set of v a l u e s is called a "domain,r.
The primary issue in this area is concerned with problems of defining, creating, and bringing up the necessary data for a new application.
The discussion points out the difficulties associated with systematic knowledge acquisition.
I. User Model A single database may be used by different groups of users for different purposes.
For example, some users of the ficompanies in METRO, but may u n w i e l d y for large sets of values.
become If a value can be recognized by a pattern, it is not n e c e s s a r y to itemize all instances in the dictionary.
For example, a date may be entered as "yy/mm/dd" so that any input matching the pattern "nn/nn/nn" is recognized as a date.
This is the approach used for dates and for names of applicants in the A P P L I C A N T database, where names of people match the pattern "I.I.Lastname".
In another approach, OnLine English [CULL80] and Intellect [HARR78, EDP82] (two v a r i a t i o n s of ROBOT) used the database to recognize values.
This is a s a t i s f a c t o r y solution if the database is small or if the small number of d i f f e r e n t values is stored in an index accessible to the NLI, and if the values in the database are suitable for use in English questions.
Each of these solutions has disadvantages.
If values are stored in the d i c t i o n a r y there may be many different ways to spell each particular value.
For example, the company name for "System Development Corporation" may also be given as "S.D.C.", "S D C", or "System Development Cotp".
While each d i f f e r e n t spelling could be entered as a synonym for the "correct" spelling in the database, this would result in an enormous proliferation of the d i c t i o n a r y entries and problems with concurrency control between the updates directed to the data m a n a g e m e n t system and the updates to the dictionary.
A creative solution might he to define rules for synonym generation and apply them to database updates.
A somewhat different example is from the A P P L I C A N T application which has many open ended domains, such as names of applicants and previous employers.
In this case, the application designer may have to treat certain fields as "retrieve-only", meaning that the data can be asked ~or but not used as a selection criterion.
A database with a large number of retrieve-only fields may be a poor candidate for an NLI.
Patterns can be used only if they can be enforced, and probably few values really fit the patterns nicely.
Proper names ate a poor choice for patterns because of variations such as middle initial or title such as "Dr".
or "Jr.".
Also, spelling correction cannot be performed unless the value is stored in the dictionary.
Finally, the solution of using the database itself to recognize v a l u e s is u n s a t i s f a c t o r y to a general NLI for anything other than trivial databases, unless an inverted index of values is easily accessible.
There are the problems of spelling c o r r e c t i o n and synonyms for database values, the inefficiency involved in accessing the DBMS for every unrecognized word, and the d i f f i culty of knowing which fields in the d a t a b a s e to search.
3. Semantic Variation By Value Databases are generally designed with a m i n i m u m number of d i f f e r e n t record types.
When there are entities which are similar, but p o s s i b l y have a small number of a t t r i b u t e s which are not shared, the entities will be stored in the same record type with null values for the attributes that do not apply.
The user, in his questions, may view these similar entities as very d i f f e r e n t e nt i t i e s and talk about them d i f f e r e n t l y . We did not encounter the problem with METRO or AIREP.
For example, in METRO, the user asks the same type of questions about the c o m p a n y named "Colonial" as about the company named "Supreme".
In APPLICANT, however, each a p p l i c a n t has a set of "specialties" such as "computer programmer", "a c c o u n t i n g clerk", or "gardener".
These are all stored as values of the s p e c i a l t y field in the database.
Unfortunately, in this case different specialties evoke completely d i f f e r e n t concepts to the end user.
The user may ask q u e s t i o n s such as, "What p r o g r a m m e r s know COBOL?", "Who can program in COBOL?", and "How m a n y a p p l i c a n t s with a s p e c i a l t y in computer programming applied in 1982?".
Notice the new nouns and verbs that are introduced by this s p e c i a l t y name.
A value domain such as specialties should be handled with an ISA hierarchy.
Each d i f f e r e n t type of s p e c i a l t y such as gardener or programmer could have a different concept that is a subset of the concept "specialty".
Some questions could be asked about all s p e c i al t i e s and others could be directed only to certain subconcepts.
However, there is no [SA hierarchy in EUFID, and it would have been inefficient to treat each specialty and subspecialty as a separate concept since there are 30 specialties and 196 subspecialties.
Therefore, we required the users to know the exact values, to know which values are for s p e c i a l t i e s and which are for subspecialties, and to ask q u e s t i o n s using the values only as nouns.
This is not "user friendly".
Even if it were possible to build a different concept for each different skill, there is an update problem.
When a new value is a d d e d to a v a l u e domain where there ace uniform semantics (as in adding a new company name in METRO), the new value is simply attached to the existing concept, when the new value has different semantics, t h e newly associated concepts, nouns, and verbs cannot be added automatically.
If t h e NLI supports an ISA hierarchy, someone w i l l need to categorize t h e new value and add a new node to the hierarchy or specify a position in the hierarchy.
4. Automation of D e f i n i t i o n subset who l i v e in Nevada.
One s o l u t i o n is to provide commands that allow u s e r s to d e f i n e s u b s e t s of the database to which to address questions.
This removes the ambiguity and speeds up retrieval time on a large d a t a b a s e . However, it moves the NLI interaction toward that of a structured query language, and forces the user to be a w a r e of the level of subset b e i n g accessed.
It is also difficult to implement because a subset may involve projections and joins to build a new relation containing the subset.
The NLI must be able dynamically and temporarily to change the mapping tables t o map t o this new relation.
2. Intelll~ent Interaction A natural-language interface system will not be practical u n t i l a new a p p l i c a t i o n can b e installed easily.
"Easily" means that the end-user organization must be able to create and modify the driving tables for the application relatively quickly without the help of the NLI developer, and must b e able to use the NLI without restructuring the d a t a b a s e . Each EUFID application required "handcrafted" tables that were built by the development staff.
Each new application was done in less time than the previous one, but still required several staff-months to bring up.
Clearly, the goal of facilitating the building of the tables by end users was not met.
Computer-assisted tools for defining new applications are a prerequisite for practical NLIs.
B. LANGUAGE PROBLEMS One of the EUFID design goals was to r e s p o n d promptly either with an answer or with a message that the question could not be interpreted.
The system handles spelling or typographical errors by interacting with the user t o select the correct word.
However, when all of the words are recognized but do n o t connect semantically, It is difficult to identify a single point in analysis which caused the failure.
It is i n this a r e a that the absence of a syntactic mechanism for determining well-formedness was most noticeable.
There are times when a question has a proper syntactic structure, but co n t a i n s semantic relationships u n r e c o g n i z a b l e to the application as in "What is the locatlon of North Hills?".
A response of "Location is not defined f o r North Hills in this appllcacion" should be derivable from the recognizable semantic failure.
Similarly, it would be useful to have a framework for interpreting partial trees, as in the question "What companies does Mohawk ship to"? where Mohawk is not a recognized word within the application.
An appropriate response might be "Companies ship to receiving offices and companies; Mohawk is neither a receiving office nor a company.
The names of offices and companies are ...".
Interpretation of partial a n a l y s e s is not possible within the EUFID system; it either succeeds or fails completely.
3. Yes/No Questions The basic approach to language analysis in EUFID involves a bottom up parser using a semantic grammar.
The symbols of the grammar are concepts underlying lexical items, and the rules of the grammar ace based o n a case framework.
Essentially syntactic information is used only when needed to resolve ambiguity.
The language features that this technique has t o handle are common to any NLI, and some of the problem areas are described in the following sections.
I. Anaphora and Ellipsis To support natural interaction it is desirable to allow the use of anaphoric reference and elliptical constructions across sentence sequences, such as "What applicants know Fortran and C?", "Which of them live in California?", "In Nevada?", "How many know Pascal?'.
One of the biggest problems is to define the scope of the reference in such cases.
In the example, it is not clear whether the user wishes to retrieve the set of all applicants who know Pascal or only the II In normal NLI interaction users may wish to ask "yes/no" questions, yet no DBMS has the ability to answer "yes" or "no" explicitly.
The EUFID mapper maps a yes/no question into a query which will retrieve some data, such as an " o u t p u t identifier" or default name for a concept, if the answer is "yes" and no data if the answer if "no".
However, the answer may be "no" for several reasons.
For example, a "no" response to the question "Has John Smith been interviewed"? may mean that the database has knowledge about John Smith and about interviews and Smith is not listed as having had an interview*, or the database knows about John Smith and no data about interviews is available.
A third p o s s i b i l i t y could be that the database has information about John Smith and his employment situation (already hired), and the response might include that information, as in "No, but he has already been hired'.
4. Conjunctions uncertain whether they should be returned in the answer.
It is also d i f f i c u l t to take a c o m p l e m e n t of a set of data using the m a n y data m a n a g e m e n t systems that do not support set o p e r a t o r s between relations.
Questions which require a "yes" or "no" response are difficult to answer because often the "no" is due to a p r e s u p p o s i t i o n which is invalid.
This is e s p e c i a l l y true with negation.
For example, if the user asks, "Does e v e r y company in North Hills except Supreme use NH2?", the answer may be "no" because Supreme is not in North Hills.
The current i m p l e m e n t a t i o n of EUFID does not allow explicit negation, a l t h o u g h some n e g a t i v e concepts are handled such as "What c o m p a n i e s ship to companies other than Colonial?".
"Other than" is interpreted as the "!-" o p e r a t o r in e x a c t l y the same way that "greater than" is interpreted as ">".
C. INTERPRETATION AND DATABASE ISSUES T h e s c o p e of c o n j u n c t i o n s is a difficult problem for any parsing or analyzing algorithm.
The n a t u r a l l a n g u a g e use of "and" and "or" does not n e c e s s a r i l y correspond to the logical meaning, as in the question "List the applicants who live in C a l i f o r n i a a n d Arizona.".
Multiple c o n j u n c t i o n s in a single q u e s t i o n can be ambiguous as in "which minority and female applicants know Fortran and Cobol?'.
This could be interpreted with logical "and" or with logical "or" as in "Which a p p l i c a n t s who are minority or female know either Fortran or Cobol?".
The EUFID mapper will change English "and" to logical "or" when the two phrases within the scope of the conjunction are values for the same field.
In the example above, an applicant has only one state of residence.
Many q u e s t i o n s make perfect sense semantically but are difficult to map into DBMS q u e r i e s because of the d a t a b a s e structure.
The problems become worse when access is through an NLI because of increased e x p e c t a t i o n s on the part of the user and because it may be d i f f i c u l t for a help system a d e q u a t e l y to d e s c r i b e the problem to the user who is unaware of the database structure.
I. IL Limitations Nepption Negative requests may contain explicit negative words such as "not" and "never" or may contain implicit negatives such as "only", "except" and "other than" [OLNE78].
The interpretation of negatives can be very difficult.
For example, "Which c o m p a n i e s did not ship any perishable freight in 1976" could mean either "Which (of all the companies) shipped no perishable freight in 1976"? or "Which (of the companies that ship perishable freight) shipped none in 1976?'.
Moreover, if some companies were only receivers and never shippers it is "-"~e is the important d i s t i n c t i o n between a "closed world" database in which the assumption is that the database covers the whole world (of the application) and an "open world" database in which it is understood that the database does not represent all there is to the real world of the application.
In the open world database, which we encounter most of the time, a response of "not that this database knows of" might be more appropriate.
~Z The design of the IL is critical.
It must be rich enough to support retrieval from all the underlying DBMSs.
However, if it c o n t a i n s c a p a b i l i t i e s that do not exist in a specific DBMS, it is difficult to d e s c r i b e this d e f i c i e n c y to the user.
In APPLICANT, the user cannot get both the major and minor fields of study by asking "List applicants and field of study", because a limitation in the EUFID IL prevents making two joins between education and subject records.
This problem was corrected in a subsequent version of IL with the addition of a "range" statement similar to that used by QUEL [STON76].
The current IL does not contain an "EXISTS" or "FAILS" operator which can test for the existence of a record.
Such an operator is frequently used to test an interrecord link in a network or hierarchical DBMS.
It is needed to express "What problems are unsolved"? to the AIREP application, which requires a test for a database link between a set and a solution Mixed Case Values set.
generate the IL q u e r y EUFID allows a value in the database to be upper or lower case and will c o n v e r t a value in the question either to all upper or all lower case in the IL, or leave it as input b y the user.
If the d a t a b a s e values are mixed case, it is not possible to convert the user's input to a single case.
If the user does not enter each letter in t h e p r o p e r c a s e, t h e v a l u e will n o t match.
3. Granularit~ Differences retrieve [cct.scname] where (cct.date  198~) and (cct.lf >{retrieve [avg (cct.lf)] where (cct.date 1980)}) Here, =cct" i s t h e name o f the companyto-company transaction relation.
" S c n a m e " is the name of a shipping company in this relation.
Note again that the qualification on " 1 9 8 ~ " n e e d s to be done both inside and o u t s i d e the nested p a r t o f t h e query.
In the query language for INGRES such a request is expressed in a manner very similtar t o t h e IL e x p r e s s i o n s . For WWDMS a very complex procedure is generated.
In all cases, t h e DBMS n e e d s to answer the inner request and s a v e t h e result for usa in qualifying the outer request.
There are many database management systems that cannot handle such questions and t h e s e I L s t a t e m e n t s cannot be translated into the system's query language.
5. Inconsistency In Retrieval The NLI user is n o t expected to understand exactly how d a t a is stored, and yet must understand something about the g r a n u l a r i t y of the data.
Time fields often cause problems because time m a y be given by year or by fractions of a second.
U s e r s may make t i m e comparisons that require more granularity than is stored in t h e database.
For example, t h e user can ask "What incidents were reported at SAC while system release 3.4 was installed?".
If incidents were reported by day but system release dates were given by month, the system would return i n c i d e n t s which occurred in the days of the month before the system release was i n s t a l l e d . 4.
Nested Queries A very simple question in English can turn into a very complicated request in t h e query language if it involves retrieval of data which must b e used f o r qualification in another part of the same query.
In IL these are called "nested queries".
Most o f t e n some qualification needs to be done b o t h "inside" and "outside" t h e clause of the query that does the internal retrieve.
For example, t h e question "What i n c i d e n t at SAC had the longest d o w n t i m e ? " f r o m o u r AIREP a p p l i cation i s e x p r e s s e d i n I L as retrieve [INCA.
ID] where (INCA.SITENAME = "SAC") and (INCA.DNTM [retrieve [ max (INCA.DNTM)] where (INCA.SITENAME = "SAC")}) The nested part of t h e query is enclosed in braces.
"INCA" is the database name of the active incident records.
Notice that removing the "INCA.SITENAME = 'SAC'" clause from either the inner or outer query would result in an incorrect formulation of the question.
A similar example from the METRO application is the question, "What company shipped more than the average amount of light freight in 198~"? which will 13 The NLI presents a uniform view of all d a t a b a s e s a n d DBMSs, but it is difficult to truly mask all differences in the behavior o f t h e DBMSS b e c a u s e t h e y d o n o t all process the equivalent query in the same way.
For example, when data are retrieved from two relations in a relational database, the two relations must be J o i n e d on a common attribute.
The answer forms a new relation which may be displayed to t h e user o r stored.
Since the join clause acts as qualification, a record (tuple) in either relation which has no corresponding t u p l e in t h e other relation does not participate in the result.
This is a different concept from the hierarchical and network models where the system retrieves all records from a master record and then retrieves corresponding records from a subfile.
This difference can cause anomalies with retrieval.
For example, in a pure relational system "List applicants and thei~ interviews" would be treated as "List applicants who have had interviews together with their interview information".
A h i e r a r c h i c a l or network DBMS would treat it as "List all applicants (whether or n o t they have been interviewed) plus any interview information that exists".
This second interpretation is more likely to be the correct one.
fiD. OVERALL NLI DESIGN There are several problems that affect the selection of a p p l i c a t i o n s for the NLI.
Some d a t a b a s e s and data m a n a g e ment systems may not be a p p r o p r i a t e targets for natural-language interfaces.
Some DBMS functions may be d i f f i c u l t to support.
It is important to have a clear understanding of these problems so that the NLI can mediate between the user view, as represented by the naturallanguage questions, and the underlying d a t a b a s e structure.
i. ~ Design C o n s i d e r a t i o n map q u e r i e s and t o explain problems to t h e u s e r when t h e m a p p i n g c a n n o t b e m a d e . However, there can be "reasonable" queries that cannot be answered d i r e c t l y because of the database structure.
Hierarchical DBMSs present the most problems with n a v i g a t i o n because access must start from the root.
For example, if the APPLICANT database were under an hierarchical DBMS, the q u e s t i o n "List t h e s p e c i a l t i e s for each applicant" could be answered directly but not "What are the specialties"? as there would be no way to get to the s p e c i a l t y records except via particular applicant records.
An array allows more than one instance of a field or set of fields in a single record.
There may be arrays of values or even arrays of sets of values in nonrelatlonal databases.
When the user retrieves a field that is an array the DBMS requires a subscript into the array.
Either the user must s p e c i f l y this s u b s c r i p t or the NLI must map to all members of the array with a test for missing data.
3. Class of DBMS to Supp%rt For any d a t a b a s e there are naturallanguage q u e s t i o n s that cannot be interpreted because the concepts involved lle outside the world of the database.
Questions can also involve structural complexity that is n o t r e p r e s e n t a b l e in the DBMS q u e r y language.
A p a r t i c u l a r l y difficult d e c i s i o n in the overall design of an NLI is the issue of where in the chain of events of processing a user's question into a DBMS q u e r y to trap these q u e s t i o n s and stop processing.
One approach is to decide that if a question is not meaningful to the world of the d a t a b a s e it should not be m e a n i n g ful to the NLI and, therefore, not analyzable on semantic grounds.
Another assumes that if the NLI can analyze a question that cannot be asked of the database, it has a much better chance of d e s c r i b i n g to the user what is wrong with the question and how it might be rephrased to get the desired information.
Codd made good use of the dialogue procedures of the RENDEZVOUS [CODD74] system to avoid questions that the DBMS could not handle, as well as avoiding g e n e r a t i o n of DBMS queries that did not represent the user's intent.
Such a system, however, requires a very large semantic base (much larger than that of the database) in order to make meaningful communication with the user during the dialogue.
2. Class of Database to Support For systems such as EUFID, the database must be organized within a data m a n a g d m e n t system so that the data is structured and individual fields are named.
If the data is just text, the EUFID approach cannot be used.
Current NLI systems are de s i g n e d to be used interactively by a user, which means that the DBMS should also have an interactive query language.
However, noc all data m a n a g e m e n t systems are interactive.
WWDMS [HONE76] has a user query language, b u t queries are entered into a batch job queue and answers may not return for many minutes.
If an Nil front end is to be added to such a DBMS, i~ must have the capability to generate query programs without any access to the database for parsing or for processing the returned answer.
The query language should support operations equivalent to the relational o p e r a t i o n s of select, project, and join.
Also, the query language should support some arithmetic capability.
Most have aggregate functions such as SUM and COUNT.
WWDMS does not have an easy-touse average operation, but it does have a procedural language with arithmetic operators so that EUFID can produce a "query" that p r o c e d u r a l l y calculates an average.
Basic c a l c u l a t i o n s should be supported such as " a g e = t o d a y b i r t h d a t e " . It is also d e s i r a b l e to be able to call special functions to do complex c a l c u l a t i o n s Some databases are simply not good candidates for an NLI because of characteristics mentioned in previous sections such as many retrieve-only fields, or domains that have a high update rate but cannot be recognized by a pattern.
There are also some structural problems chat must be recognized.
If the database contains "flat" files about one basic entity, it is reasonably easy to fisuch as required in navigational calculations a naval database.
the input standardize must be values, controlled to Support for Metadata Metadata is data about the data in the database.
It would be able to tell the user of the METRO application, for example, the kind of information the database has for warehouses and other entities in the application.
Such metadata might be extensions of active integrated data dictionaries now available i n some DBMSs.
I n an a p p l i c a t i o n l e v e l system the user should be able to query the metadata to learn about the structure of the database.
A different mode, such as the menus used by the EUFID help system, could be used to access metadata, or English language questions to both meta information and the database could be supported.
there should be few fields than have values that change rapidly, cannot be recognized by a pattern, and that must be used in qualification, the users of the NLI should have a common use for the data and a common vlew of the data, and there must be some user who understands the questions that will be asked and is available to work with the d e v e l o p e r s of the NLI.
Updates Some potential users would like a n a t u r a l l a n g u a g e interface to include the capability to update the database.
Currently, updating through any high level view of the database should be avoided, especially when the view contains joins or derlve4 data, because of the risk of inadvertently entering incorrectly-interpreted data.
SUMMARY AND CONCLUSIONS We believe that current system development is limited by the need for good semantic modelling techniques and the length of time needed to build the knowledge base required to interface with a new application.
When the knowledge base for the NLI is developed, the database as well as sample input must be considered in the design.
Parsing of questions to a database cannot be divorced from the database contents since semantic interpretation can only be determined in the context of that database.
On the other hand, a robust system cannot be developed by considering only database structure and content, because the range of the questions allowed would not accurately reflect the user view of the application and also would not account for all the information that is inferred at some level.
For many years, researchers have been attempting to build robust systems for natural-language access to databases.
It is not clear that such a system exists for general use [0SI79].
There are problems that need to be solved on both the front end, the parsing of the English question, and the back end, the translation of the question into a data management system query.
It is important to understand the types of requests, types of functions, and types of databases that can be supported by a specific NLI.
Some general guidelines that can be applied to the selection of applications for current NLI front ends are suggested below: lo the underlying DBMS should interactive query language, have an the DMS view should be relational or at least support multiple access paths, the database arrays either tures, should not contain of values or of strucACKNOWLEDGEMENTS We would like to acknowledge the many people who have contributed to EUFID development: David Brill, Marilyn Crilley, Dolores Dawson, LeRoy Gates, Iris Kameny, Philip Klahr, Antonio Leal, Charlotte Linde, Eric Lund, Fillp Machi, Kenneth Miller, Eileen Lepoff, Beatrice Oshika, Roberta Peeler, Douglas Pintar, Arie Shoshani, Martin Vago, and Jim Weiner.
REFERENCES [AHO72] Aho, A.
V. and J.
D. Ullman.
"The Theory of Parsing, Translation, and Compiling", Vol.
I: Parsing, Prentice-Hall, 1972, pp.
314-23Z. [BURGBff] Burger, J.
F . "Semantic Database Mapping in EUFID", Proceedings of the 198Z ACM/SIGMOD Conference, ~3"n~-a'-o~caY-'-Ca~., May 14-16, 198ff.
[BURG82] Burger, J.
F. and Marjorie Templeton.
"Recommendations for an Internal Input Language Eor the Knowledge-Based System', System Development Corporation internal paper N-(L)-24890/021/00, January 5, 1982.
[CODD74] Codd, E.
F., "Seven Steps to Rendezvous with the Casual User', Proc.
IFIP TC-2 Working Conference on Data-'5"~e'-~a~a~emen~ ~ystems, Car ~ gese, Corsica, April 1-5, 1974, in J.
W. Kimbie and K.
I. Koffeman (Eds.), "Data Base Management" North-Holland, 1974.
[CULL80] Cullinane Corporation, "IQS Summary Description", May 1980.
[DATE77] Date, C.
J., "An Introduction to Database Systems', second edition, Addison-Wesley Publishing, Menlo Park, CA, 1977.
[EDP82] "Query Systems for End Users", EDP Analyzer, Vol.
20, No.
9, September, 1982.
[HARR78] Harris, L.
R., "The ROBOT System: Natural Language Processing Applied to Data Base Query', Proceedings ACM 78 Annual Conference, 1978.
[HEND77] Hendrix, G.
G., E.
D. Sacerdoti, D.
Sagalowicz, and J.
Slocum, "Developing a Natural Language Interface to Complex Data" SRI Report 78-305, August 1977.
[HONE76] Honeywell, WWMCCS: World Wide Data Management System User's Guide, Honeywell DE97 Ray.3, April 1976.
[KELL71] Kellogg, C.
H., J.
F. Burger, T.
billet, and K.
Fogt, "The CONVERSE Natural Language Data management System: Current Status and Plans", Proceedings of the ACM SZmposium on :ntormation ~ ~ a n d ~etrleval-~, University o Maryland, College Park, MD, 1971, pp.
33-46. [MYLO76] Mylopoulos, J., A.
8 o r g i d a, P.
Cohen, N.
Roussopoulos, J.
Tsotsos, and H.
Wong, "TORUS: A Step Towards Bridging the Gap between Data Bases and the Casual User", in Information Volume 2 1976, Pergamon Press, pp 49-64.
[OLNE78] Olney, John, "Enabling EUFID to Handle Negative Expressions", SDC SP-3996, August 1978.
[OS179] Operating Systems, Inc., "An Assessment of Natural Language Interfaces for Command and Control Database Query", Logicon/OSI Division report for WWMCCS System 16 [SCHA77] Scha, R.
J. H., "Phillips Question-Answering System PHLIQAI", in SIGART Newsletter Number 61, February 1977, Association for Computing machinery, New York.
[SIMM65] Simmons, R.
F., "Answering English Questions by Computer -a Survey', Comm.
ACM 8,1, January 1965, 53-70.
[STON76] Stonebraker, M., et.
al., "The Design and Implementation of INGRES', Electronics Research Laboratory, College of Engineering, University of California at Berkeley, Memorandum No.
ERL-M577, 27 January 1976.
[TEMP79] Templeton, M.
P., "EUFID: A Friendly and Flexible Frontend for Data Management Systems", Proceedings of the 1979 National Conference Association of Computational Linguistics, August, 1979.
[TEMP80] Templeton, M.
P., "A Natural Language User Interface", Proceedings of "Pathwazs ~o System rn~ri%7", washington DYC.
C a h ~ o ACM, 1980.
[THOM69], Thompson, F.
B., P.
C. Lockemann, B.
H. Dostert, and R.
Deverill, "REL: A Rapidly Extensible Language System", in Proceedings of the 24th ACM National Conference, s~ociation--"~or Computing machinery, New York, 1969, pp 399-417.
[WALT77] Waltz, D.
L., " N a t u r a l Language Interfaces", in SIGART Newsletter Number 61, F e b r u a r y ' ~ 7, Association for Computing machinery, New York.
[WALT78] Waltz, D.
L., "An English language Question Answering System for a Large Relational Database", Communications of the ACM 21, 7(July 1978), pp 526-539.
[WOOD72] Woods, W.
A., R.
M. Kaplan, B.
Nash-Webber, The Lunar Sciences N a t u r a l L a n @ u a ~ e " r n f o r m a t i o n ' System ~'~Report, Report number~, Bolt, Beranek, and Newman, Inc., Cambridge, MA, 15 June 1972 .
INTRODUCING ASK, A SIMPLE KNOWLEDGEABLE SYSTEM Bozenn H.
Thompson F r e d e r i c k B.
Thompson California Inatitnce of Technology Pasadena, California 91125 ABSTRACT ASK, ~ ~ i m p l e K n o w l e d g e a b l e S y s t e m, i s a t o t a l system for the structuring, manipulation and communication of information.
It is a simple system in t h e sense thaC its development concentrated on c l e a n e n g i n e e r i n g solutions to w h a t c o u l d be d o n e now w i t h g o o d r e s p o n s e t i m e s. The user interface is a limited dialect of English.
In contrast to expert systems, in which experts build the knowledge base and users make u s e o f t h i s e x p e r t k n o w l e d g e, ASK i s a i m e d a t t h e u s e r who w i s h e s t o c r e a t e, test, modify, extend a n d m a k e u s e o f h i s own k n o w l e d g e b a s e . It is a s y s t e m for a research team, a m a n a g e m e n t or military staff, or a business office.
some h a v e t h e f o l l o w i n g n u m b e r a t t r i b u t e s : speed length beam >List the destinations a n d home p o r t o f each ship, ship destination home p o r t Ubu New York Naples Tokyo --Morn 0slo Tokyo Kittyhawk Naples Boston Boston -London This paper is designed to give you a feel for the general performance of t h e ASK S y s t e m a n d overview of its operational capabilities.
To Chin end, the movie you see will continue throughout the talk.
Indeed, the talk itself is a commentary on t h i s b a c k g r o u n d m o v i e . The m o v i e i s bona f i d e and in real time, i t i s o f t h e ASK S y s t e m i n action.
(Many o f t h e i l l u s t r a t i o n s from the movie are reproduced in the written paper.) I.
ASK AS A DATABASE SYSTEM A.
Examples o f ASK English To i n t r o d u c e a few examples you to ASK, we w i l l s t a r t o u t w i t h of queries of a simple data base The uninitiated user may wish London London N e w York --North Scar London New York gimitz London Norfolk Saratoga unknown Norfolk >What c i t i e s a r e t h e home p o r t s o f s h i p s whose d e s t i n a t i o n i s London?
Boston London New York Norfolk >Are t h e r e s h i p s t h a t do n o t h a v e a c a r g o ? yes >What i s t h e number o f New York s h i p s ? There are 2 answers: ( 1 ) New York ( d e s t i n a t i o n ) ships 2 ( 2 ) New York (home p o r t ) s h i p s 1 >How many s h i p s a r e t h e r e w i t h l n e g t h g r e a t e r t h a n 600 f e e t ? Spelling correction: " l n e g t h " to " l e n g t h " 4 >What ships t h a t carry wheat go to London or Alamo Oslo? ships that carry wheat London Maru Oslo Alamo >Does the Maru carry wheat and go co London? yes concerning ships.
simply to ask: >How many ships are there? 7 >What is known about ships? some are in the following classes: Navy freighter old S.
The ASK Data Structures A l t h o u g h in the t e r m i n o l o g y of data base theory, ASK can be considered as an "entityrelation" system, ASK retains its information in records w h i c h are interlinked in a s e m a n t i c net.
One reason we refer to ALE as simple is because ic uses only a few kinds of nodes in its s e m a n t i c tanker a l l have t h e f o l l o w i n g a t t r i b u t e s : destination home p o r t some have t h e f o l l o w i n g a t t r i b u t e s : cargo a l l have t h e f o l l o w i n g number a t t r i b u t e s : age 17 net, namely: fio Attributes Relations and the ctbvious c o r r e s p o n d i n g arcs.
We speak of this as the COAR structure.
A~tributes are single valued, e.g., "father", "home port", " t i t l e " ; relations may be m u l t i p l e valued, e.g., "child"~ "cargo", "author".
The d i f f e r e n c e between attributes and relations can be seen in the following p r o t o c o l . >What is the cargo and home port of the Maru? cargo home port wheat London >The home port of Maru is Boston.
London has been replaced by Boston as the home port of Maru.
>The cargo of Maru is coal.
coal has been added as the cargo of Maru.
>What i s the cargo and home port of the Maru? cargo home port wheat BosCon coal -->definition:long:paper whose number of pages e x c e e d s 49 Defined.
>definition:long:book whose number o f p a g e s e x c e e d s 800 Defined.
>What AI bibliography i t e m s a r e long?
There are 2 answers: (1) long:paper whose number of pages exceeds 49 Physical Symbol Systems A General Syntactic P r o c e s s o r (2) long:book whose number of pages exceeds 800 Human Problem Solving >What long books were written in 19727 long:book whose number of pages exceeds 800 Human Problem Solving Family relationships make for a g o o d illustration of definitions; we switch to a small family relationship context.
>What are attributes? individual/individual attributes : spouse >What are relations? individua I/individual relations : parent >What are classes? individual classes : male female >What are definitions? definition:mother :female parent definition: father :male parent definition:child:converse of parent definition:sibling:child of parent bur not oneself definition'cousin:child of sibling of parent >List the father and mother of each of Billy Smith's cousins.
Billy Smith's cousins father mother Baby Boyd R o b e r t Boyd J i l l Boy C.
Extendin K and Hodifyin~ I.
Definitions t h e Dat~ To make such a system more knowledgeable, one needs to be able co add d e f i n i t i o n s that e m b o d y interrelationships a m o n g the basic classes, objects, a t t r i b u t e s and relations of the data.
The simplest form of definition is synonym: >definition:tub:old Defined.
ship Although this form of definition allows one to introduce abbreviations and many forms of jargon, more extensive forms of definition are desirable.
Here are three illustrations using the same "ship" file as above.
In the third definition, note the use of quotes to create local '~ariables".
>definition:area:length * beam Defined >List the length, beam and area of each tub.
tub length beam area foot foot foot**2 Ubu 231.667 48 11120.016 Alamo 564.5 84 47418.
>definition:meter:39.37 * (foot / 12) Defined.
>beam of the Alamo squared in square meters? 655.526472343 square meters >definition:longest "ship":"ship" whose length is the maximum length of "ship"s Defined.
>What is the length in meters of the longest ship whose home port is Naples? 121.920243840 meters T h e n o t i o n of w h a t is l o n g m a y be q u i t e different in another context, say in the context of b i b l i o g r a p h y of a r t i f i c i a l intelligence literature.
18 2.
Verbs Most verbs e m b o d y k n o w l e d g e specific to the application in which they are used, the exceptions being the copula verbs.
Therefore the only verbs initially known to the ASK System are "to be" and "to have".
The user c a n add n e w v e r b s by paraphrase.
>verb:ships "go" to New York:destination of ships is New York Defined.
>verb:ships "carry" coal from London to Boston:ships have coal as cargo, have L o n d o n as home port and go to Boston Defined.
>Each old ship carries what cargo to each port? old ship port cargo Ubu New York oil Tokyo oil Alamo London wheat coal fi>What i s c a r r i e d by t h e Alamo? wheat coal >Wheat i s c a r r i e d to London from what p o r t s ? New York >What c i t i e s does t h e Alamo c a r r y wheat to?
London Pronouns and Ellinses >Create t h e a t t r i b u t e : r a t i n g The a t t r i b u t e r a t i n g h a s been a d d e d . >Create i n d i v i d u a l s : s e m i n a l, e x c e l l e n t, f a i r and i m p o s s i b l e The f o l l o w i n g i n d i v i d u a l s have been added: seminal excellent fair impossible >The r a t i n g o f W i n o g r a d ' s 1980 p a p e r i n Cognitive Science is excellent.
e x c e l l e n t h a s been added a s t h e r a t i n g o f W i n o g r a d ' 8 1980 p a p e r in C o g n i t i v e S c i e n c e >Rating o f A Framework f o r R e p r e s e n t i n g In p r a c t i c a l s y s t e m s f o r e x p e r t s, a b b r e v i a t e d f o r m s of a d d r e s s i n g t h e c o m p u t e r a r e common.
Thus the ability to h a n d l e p r o n o m i n a l and e l l i p t i c a l constructions are of considerable importance.
A l t h o u g h t h e r e has been p r o g r e s s i n t h e l a s t few years in the linguistic understanding of these c o n s t r u c t i o n s, many d i f f i c u l t i e s r e m a i n . However, b u i l d i n g on t h e work t h a t ham been a c c o m p l i s h e d, many o f t h e s e c o n s t r u c t i o n s can be h a n d l e d by t h e ASK System.
In o r d e r to a v o i d m i s l e a d i n g t h e u s e r when t h e c o m p u t a t i o n a l a l g o r i t h m does not make t h e c o r r e c t i n t e r p r e t a t i o n, echo i s u s e d t o i n f o r m t h e u s e r of t h e i n t e r p r e t a t i o n t h a t h a s been t a k e n . >Is t h e r e a s h i p whose d e s t i n a t i o n i s unknown? yes >What is it?
What is it [ship whose destination is ..knov.] ? Saratoga >Is its cargo wheat or c o a l ? I s i t s [ s h i p whose d e s t i n a t i o n i s unknown] cargo wheat or c o a l ? wheat no coal yes >Does t h e A l a m o ' s l e n g t h exceed t h a t of t h e Ubu? yes >What about t h e N o r t h e r n S t a r ? [the Northern Star?] does t h e A l a m o ' s l e n g t h exceed t h a t of t h e Northern Star? no >Is i t s cargo wheat?
Is its [Northern Star] cargo wheat? no >Is it that of the Ubu?
Is i t [Norther, Star cargo] that [cargo] of the Obu? yes >Wheat is carried by how many ships? 2 >Coal and oil? c o a l and o i l i s c a r r i e d by how many ship*? coal I oil 2 >by the Ubu? c o a l and o i l i s c a r r i e d by t h e Ubu? coal no oil yes Knowledge i s i m p o s s i b l e . i m p o s s i b l e h a s been added as t h e r a t i n g o f A remework f o r R e p r e s e n t i n g Knowledge.
>Scripts, P l a n s, Goals and U n d e r s t a n d i n g ' s rating is se~.nal.
s e m i n a l h a s been added a s t h e r a t i n g o f S c r i p t s, P l a n s, Goals and U n d e r s t a n d i n g . >The r a t i n g o f M i n s k y ' s paper i n The P s y c h o l o g y of Computer V i s i o n is seminal.
i m p o s s i b l e has been r e p l a c e d by s e m i n a l as t h e r a t i n g o f M i n s k y ' s p a p e r i n The P s y c h o l o g y o f Computer V i s i o n . > L i s t t h e a u t h o r of e a c h paper t h a t h a s a r a t i n g o f e i t h e r s e m i n a l or e x c e l l e n t . paper S c r i p t s, Plans, Goals and Understanding R.
C. Schauk A Framework f o r R e p r e s e n t i n g Knowledge M.
Minsky What Does I t Mean to U n d e r s t a n d N a t u r a l Language To Winograd > r e l a t i o n : keyword The r e l a t i o n keyword h a s been added.
>individuals:context, natural language, knowledge r e p r e s e n t a t i o n The f o l l o w i n g i n d i v i d u a l s have been added: c o n t e x t n a t u r a l l a n g u a g e knowledge representation >Context and knowledge r e p r e s e n t a t i o n a r e k e ~ o r d a of S c r i p t s, P l a n s, Goals and U n d e r s t a n d i n g and A Framework f o r R e p r e s e n t i n g Knowledge.
c o n t e x t knowledge r e p r e s e n t a t i o n have been added as keywords o f S c r i p t s, P l a n s, Goals and U n d e r s t a n d i n g . c o n t e x t knowledge r e p r e s e n t a t i o n have been added as keywords o f A Framework f o r R e p r e s e n t i n g Knowledge.
> N a t u r a l l a n g u a g e i s a keyword of S c r i p t s, P l a n s, Goals and U n d e r s t a n d i n g . n a t u r a l l a n g u a g e h a s been added a s keyword of Scripts, Plans, Goals and Understanding.
>definition:about "booka'*:converse of keyword of "books" Defined.
>What AI Sibliosraphy items are about knowledge r e p r e s e n t a t i o n ? S c r i p t s, P l a n s, Goals and U n d e r s t a n d i n g A Framework f o r R e p r e s e n t i n g Knowledge D.
Inheritanc~ 4.
E x t e n d i n 2 th_...eeVocabular 7 an...~dDa~a Bas~ New o b j e c t s, c l a s s e s, a t t r i b u t e s and r e l a t i o n s may be a d d e d a t w i l l . T h i s w i l l be i l l u s t r a t e d using the AI Bibliography.
19 Classes, as well as objects, may be t h e a r g u m e n t s or v a l u e s o f a t t r i b u t e s and r e l a t i o n s . T h i s r a i s e s t h e q u e s t i o n of i n h e r i t a n c e of properties and attributes.
The a n s w e r is to be fifound in the following protocol, using attribute of color; inheritance for relations h a n d l e d i n s i m i l a r ways.
>class: r a c i n g sloop, sports b o a t, Norwegian The following classes have been added: racing sloop sports boat Norwegian >attribute: color The a t t r i b u t e c o l o r h a s been a d d e d . >individuals: white, blue, b l a c k, g r a y The following i n d i v i d u a l s have b e e n added: white blue black gray >Saratoga is a racing sloop.
S a r a t o g a has b e e n added to r a c i n g s l o o p . >Racing s l o o p s a r e s p o r t s b o a t s . r a c i n g s l o o p s have b e e n added to s p o r t s boats.
>Alamo is industrial and Norwegian.
Alamo has b e e n added to i n d u s t r i a l Norwegian.
>The c o l o r o f s p o r t s b o a t s i s w h i t e . w h i t e h a s b e e n added a s c o l o r o f s p o r t s boats  >Maru's color is gray.
g r a y has b e e n added a s c o l o r o f Maru.
>Black i s t h e c o l o r of i n d u s t r i a l . b l a c k has b e e n added as t h e c o l o r o f industrial.
>Blue is the color of Norwegian.
b l u e h a s b e e n added a s t h e color of Norwegian.
>What i s the color of each s h i p ? There are 3 answers: (t) ship color Maru gray Saratoga white (2) Alamo as industrial ship color Alamo black (3) Alamo as Norwegian ship color Alamo blue the is There are 2 answers: ( i ) T h e r e a r e no p o r t s ( 2 ) T h e r e a r e no U.S. d e s t i n a t i o n s o f Maru.
> I s some E u r o p e a n p o r t a p o r t o f Maru?
The f o l l o w i n g word i s n o t i n t h e v o c a b u l a r y : portof Correction: Is some European port a port of Maru?
There is no port.
>London is Alamo's port.
London h a s b e e n added a s t h e p o r t o f Alamo.
> I s som E u r o p e a n p o r t a p r o t o f Maru?
Spelling corrections: "son" to "some" "prot" tO " p o r t " T h e r e i s no p o r t o f Maru.
>New York i s M a r u ' s p o r t . New York h a s b e e n added a s p o r t o f Maru.
> I s some E u r o p e a n p o r t a p o r t o f Maru.
I s some E u r o p e a n p o r t a p o r t o f Maru? no II.
INTEGRATION OF MULTIPLE OBJECT TYPES A.
Extension of COAR ~o Multiple Object Types So far we have i l l u s t r a t e d ASK using only two types of objects: capabilities individuals, e.g., "John Jones", "Maru" numbers, e.g., "34.6 feet", "length of Maru", "number of ships".
ASK has been designed, however, to facilitate many kinds of objects.
This is a capability orthogonal to the simple COAR s t r u c t u r e in that for any types of objects there m a y be c o r r e s p o n d i n g classes, attributes and relations.
B. An Examvle: E.
DiaKnostics Our objectives in the design respond to the experienced user: o o of ASK are to with the responses s/he desires in at l e a s t 90% o f t h e c a s e s ; w i t h a r e s p o n s e that includes what s/he desires, but perhaps additional information also, in 90% of t h e remaining cases; with a response, although oot the desired one, but one which clearly indicates the system's interpretation of the request in the remaining cases.
We will illustrate this m u l t i p l e object type capability with the additional object type: text.
Once this new object type was added (together with p r o c e d u r e s t o m a n i p u l a t e texts, i.e., a "word processor") then text classes, individual/text and text/individual a t t r i b u t e s and r e l a t i o n s w e r e immediately available.
It was a small task to add an e l e c t r o n i c mail s y s t e m to ASK; all that w a s required was an a d d i t i o n to the a u t h o r i z a t i o n procedure that assigned to each newly authorized person a new text class as his/her mail box.
>Create the t e x t / t e x t relation:comment The t e x t / t e x t r e l a t i o n co-~.ent has been added.
>Create a file: budget correspondence The text class budget correspondence has been added.
>File the Budget Proposal in budget correspondence.
Budget Proposal has been added to the text class budget correspondence.
>Who have I received mail from?
J o h n Smith Peter Jackson 20 This third objective calls for good diagnostics.
The diagnostic capabilities of ASK are illustrated in the following protocol.
>attribute:port The a t t r i b u t e p o r t has been added.
>What a r e t h e p o r t s and U.S. d e s t i n a t i o n s t h e Maru? of fi> D i s p l a y my m a i l f r o m J o h n S m i t h . F r e d, Your B u d g e t P r o p o s a l l o o k s good t o me.
S u g g e s t you s u b m i t i t a s i s . John > F i l e i t i n my b u d g e t c o r r e s p o n d e n c e f i l e . F i l e i t [my m a i l f r o m J o h n S m i t h ] i n my budget correspondence file.
my m a i l f r o m J o h n S m i t h h a s b e e n a d d e d co the text class budget correspondence.
>amm-va i t f r o m my m a i l b o x . Bemove i c [my m a i l f r o m J o h n S m i t h ] f r o m my m a i l box.
my m a i l f r o m J o h n S m i t h h a s b e e n r e m o v e d from your mail.
> C r e a t e a b u d g e t c o r r e s p o n d e n c e named Budget Plans Please e n t e r t e x t : S t a f f l e v e l b u d g e t m e e t i n g on Wed.
a t 3 i n Tom's o f f i c e . P l e a s e s e n d me y o u r c o m m e n t s b e f o r e t h e m e e t i n g ; f i l e t h e m a s "commenCe on B u d g e t P l a n e " . \ Budget Plane class budget >Mail Budget Budget plane manager.
h a s b e e n a d d e d Co t h e t e x t correspondence.
Plans to each section manager.
h a s b e e n s e n t to e a c h s e c t i o n Ill.
MORE GENERAL ASPECTS OF THE ASK SYSTEM A.
R e s v o n s e Times The movie, which accompanied the oral presentation of this paper, demonstrated that the response rime, i.e., the time between completion of t h e t y p i n g of t h e i n p u t by t h e u s e r Co t h e appearance of t h e r e s p o n s e on t h e t e r m i n a l, is very good.
But the data bases used in the illustrations have been small, Coy d a t a b a s e s . The f o l l o w i n g t a b l e g i v e s a v e r a g e r e s p o n s e t i m e s for a few cases using larger data bases.
The query used for this illustration is: >What arm t h e d e s t i n a t i o n s of tankers?
The r e s p o u s e t i m e i s r a t h e r i n s e n s t i t i v e to the Coral number of individuals, classes, attributes and relations in the data base, depending primarily on the size of the relation (destination) and i t s a r g u m e n t ( C a n k e r s ) . Suppose t h a t t h e r e a r e m t a n k e r s i n t h e d a t a b a s e and t h a t n individuals have destinations, i.e., the size of the destination relation i s n.
T h e t a b l e g i v e s time in seconds.
no. of tankers > D i s p l a y t h e commence on B u d g e t P l a n s by e a c h section manager.
D i s p l a y i n g commence on B u d g e t P l a n s by e a c h section manager: J o h n Dobbs: D ( i s p l a y ), S(kip), o r Q(uit): 2 2 a 9 dastinscions I I I I C.
A d d i n z New O b i e c t T y ~ e s A l t h o u g h t h e ASK S y s t e m h a s b e e n d e s i g n e d t o allow the addition o f new o b j e c t t y p e s, t h i s c a n be d o n e o n l y by a n a p p l i c a t i o n programmer.
The major obstacle is the necessity to provide a procedure to initialize instances of the new object type and procedures that carry out their intrinsic manipulation.
However, we expect the addition of new object types to be a c o m m o n occurrence in the applications of the ASK System.
In any potential applicaion areas, using groups have accumulations of data already structured in specific ways and families of procedures that they have developed to manipulate these structures.
In ASK, they can identify these data structures as a new object type, design simple syucax for them to invoke their procedures, and thus embed their familar objects and manipulations within the ASK English dialect and within the same context as other associated aspects of their tasks.
The class, attributed and relation constructions become immediately available.
B e s p o n e e Time i n S e c o n d s f o r : >What a r e t h e d e s t i n a t i o n s of tankers?
B. The C q n c e v t o f A Use ~ C o n t e x t an_...dd the Basing Ooeration In the t e r m i n o l o g y of ASK, a user "Context" is a knowledge base together with the vocabulary and definitions that S o with it.
A given user will usually have several Contexts for v a r i o u s purposes, some of which may be the small "Ships" Context, a (truncated) bibliography of Artificial Intelligence literature and an a d m i n i s t r a t i v e Context concerning budget matters.
When one initiates a session with the ASK System, one is initially in the Command Context: >Welcome to ASK Please identify yourself.
>Fred >Pass word: You have mail.
Fred is in COMMAND, proceed.
At this point, you can list the Directory of Contexts available to you, create or delete Contexts, authorize others to use Contexts which you have created, and enter any of the Contexts in 21 fi>Directory context BASE Ships AI Bibliography Family Management Matters creator MASTER Fred Fred Fred Fred enter no yes yes yes yes b~s~ yes yes yes yes yes >enter Management Matters You are in Management Matters, proceed.
>Who have I received mail from?
Peter Jackson John Dobbs A new C o n t e x t is c r e a t e d by basing it on an already existing one.
Consider a u s e r who h a s b e e n a u t h o r i z e d f o r b a s i n g on t h e AI B i b l i o g r a p h y Context illustrated a b o v e and who w a n t s t o b u i l d a w i d e r b i b l i o g r a p h y C o n t e x t ( a d d i n g new i n f o r m a t i o n --vocabulary, data and definitions), however, without disturbing the old one.
To do s o, a l l s / h e n e e d s t o do i s s e l e c t a new n a m e, s a y CS B i b l i o g r a p h y, and t y p e : >exit You a r e >Base CS The new created >individual: E x p e r i e n c e w i t h ROBOT, L.
H a r r i s The f o l l o w i n g i n d i v i d u a l s have been added: E x p e r i e n c e w i t h ROBOT L.
H a r r i s >The a u t h o r o f E x p e r i e n c e w i t h ROBOT i s L.
Harris. L.
H a r r i s h a s b e e n a d d e d a s a u t h o r o f E x p e r i e n c e w i t h ROBOT.
>Keyword o f E x p e r i e n c e w i t h ROBOT i s d a t a b a s e . database has been added as keyword of E x p e r i e n c e with ROBOT.
>Who wrote what about databases? author D.
L. W a l t z N a t u r a l L a n g u a g e A c c e s s t o a Large Data Base L.
H a r r i s E x p e r i e n c e w i t h ROBOT > e x i t t o CB B i b l i o g r a p h y, You a r e i n CS B i b l i o g r a p h y, proceed.
>Who w r o t e w h a t a b o u t d a t a b a s e s ? author D.
L. W a l t z N a t u r a l L a n g u a g e A c c e s s t o a Large Data Base C.
J. D a t e An I n t r o d u c t i o n to Database Systems L.
H a r r i s E x p e r i e n c e w i t h ROBOT Several C o n t e x t s can be based on a g i v e n one, and one C o n t e x t can be b a s e d on several, thus a hierarchical structure of Contexts can be realized.
All Contexts are directly or indirectly based upon the BASE Context, w h i c h c o n t a i n s the f u n c t i o n words and g r a m m a r of the ASK d i a l e c t of English, the mathematical and statistical capabilities, and the word processor.
i n COMMAND, p r o c e e d . Bibliography on AI Bibliography context CS Bibliography has been b a s e d on AI B i b l i o g r a p h y basing action is t h i s new C o n t e x t : a ne w The r e s u l t of this Context.
Upon e n t e r i n g > E n t e r CS B i b l i o g r a p h y You a r e in CS Bibliography, one c a n make a d d i t i o n s : proceed.
C. T~anspo~tabilitv It is easy and fast to apply ASK to a n e w domain, given that a data base for this new domain is a v a i l a b l e in m a c h i n e r e a d a b l e form.
The vehicle is t h e ASK dialogue-driven Bulk Data Input capability, w h i c h can be called upon to build an existing database into one's Context.
The result not only i n t e g r a t e s this n e w data w i t h that already in the C o n t e x t and under the ASK d i a l e c t of English, but in m a n y c i r c u m s t a n c e s w i l l make the use of this data m o r e r e s p o n s i v e to users" > i n d i v i d u a l s :An I n t r o d u c t i o n to Database S y s t e m s, C.
J . D a t e The f o l l o w i n g i n d i v i d u a l s h a v e b e e n a d d e d : An I n t r o d u c t i o n to D a t a b a s e S y s t e m s C.
J . D a t e >The a u t h o r o f An I n t r o d u c t i o n to Database S y s t e m s i s C.
J . D a t e . C.
J . D a t e h a s been added a s a u t h o r of An Introduction to Database Systems.
>Keyword o f An I n t r o d u c t i o n t o D a t a b a s e Systems is database.
d a t a b a s e h a s been a d d e d a s k e y w o r d o f An Introduction to Database Systems.
>Who w r o t e w h a t a b o u t d a t a b a s e s ? author D.
L. W a l t z N a t u r a l L a n g u a g e A c c e s s t o a L a r g e D a t a Base C.
J. D a t e An I n t r o d u c t i o n to D a t a b a s e Systems These additions to the CS B i b l i o g r a p h y would not, of c o u r s e, e f f e c t She AI B i b l i o g r a p h y Context.
However, a d d i t i o n s and m o d i f i c a t i o n s that are subsequently made in the AI Bibliography Context would automatically be reflected in the CS Bibliography.
>exit You are in COMMAND, proceed.
>Enter AI Bibliography You are in AI Bibliography, proceed.
22 needs.
The Bulk Data Input D i a l o g u e p r o m p t s the user for n e c e s s a r y i n f o r m a t i o n to (i) e s t a b l i s h t h e physical structure of the d a t a b a s e to be included, (2) add necessary classes and attributes as needed for the new data entries.
The user also indicates, using E n g l i s h c o n s t r u c t i o n s, the i n f o r m a t i o n a l r e l a t i o n s h i p s a m o n g the fields in the p h y s i c a l records of the d a t a b a s e file that s/he wishes carried over to the ASK Context.
IV. DIALOGUES IN ASK Some have raised the question whether natural language is always the most desirable medium for a user to c o m m u n i c a t e w i t h the computer.
Expert systems, for example, have tended to use computer guided dialogues.
One simple form such a dialogue fimight take is illustrated by t h e f o l l o w i n s in w h i c h a new e n t r y i s a d d e d t o t h e AI B i b l i o g r a p h y : >New b i b l i o g r a p h y i t e m >Add to what b i b l i o g r a p h y ? AI B i b l i o g r a p h y >Title: Natural Language Processing >Author: Harry Tennant >Keyword: n a t u r a l l a n g u a g e >Keyword: s y n t a x p r o c e s s i n g >Keyword: s p e e c h a c t s >Keyword: Natural Language Processing has been added t o AI B i b l i o g r a p h y . >Title: The "new b i b l i o g r a p h y item" dialogue i8 completed.
>What A I B i b l i o g r a p h y items were written by Harry Tennant?
E x p e r i e n c e with the Evaluation of Natural Language Question Answerers Natural Language Processing necessary, respond with a diagnostic, (2) f i l l in other fields with data developed from the knowledge base, (3) extend the knowledge base, adding to the vocabulary and adding or changing the data itself, (4) file the completed form in p r e s c r i b e d f i l e s o r i n t h o s e i n d i c a t e d by t h e u s e r and a l s o m a i l it t o a s p e c i f i e d d i s t r i b u t i o n list through the electronic mail subsystem.
Since the Form p r o c e s s i n g c a n c h e c k c o n s i s t e n c y and m o d i f y the knowledge base, Forms can be used to facilitate data input.
S i n c e Form p r o c e s s i n g c a n fill f i e l d s in t h e Form, the forms c a p a b i l i t y includes the functions of a report generator.
L e t t e r s and memos c a n be written a s s p e c i a l c a s e s of Form filling, automatically adding dates, addresses, etc.
and filing and dispatching the result.
It must be easy and natural to add new Forms, if they are to be a convenient tool.
That is the function of the Forms Designing Dialogue.
Much like the Bulk Data Input Dialogue, the Forms Designing Dialogue holds a dialogue with the the user through which s/he can specify the fields of the Form itself and the processing of the above k i n d s t o be a u t o m a t i c a l l y accomplished at the time the Form is filled in.
Here is a simple example of a from that was designed using the Forms Designing Dialogue.
>What i s t h e bona p o r t and c o ~ a n d e r old ship?
There are 2 answers: (i) The~e is no c o ~ . n d e r . of each Other alternative media for user/system communication are menu boards, selection arrays and q u e r y by e x a m p l e . Many o t h e r c r y p t i c w a y s to communicate user needs to a knowledgeable system c a n be t h o u g h t o f ; o f t e n t h e m o s t u s e f u l m e a n s will be highly specific to the particular application.
For e x a m p l e, i n p o s i t i o n i n g c a r g o i n t h e h o l d o f a s h i p, o n e w o u l d l i k e t o be a b l e t o display the particular cargo space, showing its current cargo, and call for and move into place o t h e r i t e m s t h a t a r e to be i n c l u d e d . In the past, enabling the system to respond more intelligently to the user's needs required the provision of elaborate programs since the u s e r ' s t a s k s m a y be q u i t e i n v o l v e d, w i t h c o m p l e x decision structures.
The introduction of terse, effective communication has incurred lout delays and thus the changing needs of a user had little c h a n c e o f b e i n g m e t . I n t h e ASK S y s t e m, t h e u s e r s themselves can provide this knowledge.
They c a n i n s t r u c t t h e system on how to e l i c i t the necessary i n f o r m a t i o n and how to c o m p l e t e t h e r e q u i r e d t a s k . This ASK capability is quite facile, opening the way f o r i t s u b i q u i t o u s use in extending the knowledgeable responsiveness of the computer to user's immediate needs.
ASK i n c l u d e s two s y s t e m guided dialogues, similar to the Bulk Data Input d i a l o g u e by w h i c h u s e r s c a n i n s t r u c t t h e S y s t e m on how to be more r e s p o n s i v e t o t h e i r n e e d s . A.
Forms Desi~nin2 Dialogue The Form is an efficient means of communication with which we are all familiar.
A number of computer systems include a Forms package.
For most of these, however, filling in a Form results only in a document; the Form does not constitute a medium for interacting with the knowledge base or controllin K the actions of the system.
The ASK Forms capability enlarges the roles and ways in which Forms can be used as a m e d i u m for user interaction.
As the user fills in the fields of a Form, the System can make use of the information being supplied to (1) check its consistency with the data already in the k n o w l e d g e base and, if old ship hone port Ubu Naples Alamo London >Who i s J o h n S m i t h ? The f o l l o w i n s w o r d s a r e n o t i n t h e v o c a b u l a r y : John Smith > I n v e n t o r y o f wheat and c o r n o i l ? w h e a t and c o r n o i l i n v e n t o r y wheat 86.7 corn oil 123~00.
Note that the home port of the Alamo is London and that it does not have a commander, further that John Smith is not known to the System.
>Fill s h i p p i n g (For the purposes of the published paper, in contrast to the film shown at the presentation of the paper, only the initial and final copies of the form are given, under~ines indicate fields filled in by the "user", the o t h e r f i e l d s automatically being filled by the System).
(before) Shipping Form ship: port: quantity item $ price $ total commander: 23 fi(after) S h i p p i n g Form ship: port: A;amQ London item whvac corn oi~ J@hn SmiC~ price $ 35.75 $ 2.50 total $ 107.25 $1250.00 quantity ! 500 colmander: natural language programming capabiltty.
We hasten to add that it is not a general purpose program environment.
It is for "ultra-high" level programming, gaining its programming efficiency t h r o u g h t h e a s s u m p t i o n o f an e x t e n s i v e v o c a b u l a r y and knowledge base on which it can draw.
The illustrative d i a l o g u e a b o v e, w h i c h a d d s ' a new i t e m to a bibliography, is an example of a simple d i a l o g u e d e s i g n e d u s i n g DDD.
V. ACKNOWLEDGEMENTS AND CURRENT STATUS Shipping List for Alamo has been filed in Shipping Invoice File.
S h i p p i n g L i s t for Alamo h a s b e e n m a i l e d to J o n e s . mail t o : Fill shipping has been completed.
> L i s t t h e home p o r t and co-w,a n d e r o f e a c h old ship.
old ship home p o r t commander Ubu Naples -Alamo London John Smith >Inventory of wheat and corn oil? w h e a t and c o r n o i l i n v e n t o r y wheat 83.7 c o r n oil 122900.
>What is in the Shipping Invoice File?
Shipping List for Alamo The three System guided dialogues, Bulk Data Input, Dialogue Designing Dialogue and Forms D e s i g n i n g D i a l o g u e, are f r o m the d o c t o r a l dissertation of Tai-Ping Ho.
The aspects of ASK c o n c e r n i n g b a s i n g o n e C o n t e x t on a n o t h e r a r e f r o m the doctoral dissertation o f K w a n 8 I Yu.
The methods for handling anaphora, fragments and correction of inputs are from the doctoral dissertation of David Trawick.
ASK is implemented on the Hewlett Packard HP9836 desktop computer.
To handle Contexts of r e a s o n a b l e s i z e, one n e e d s a b a r d d i s k . An HP9836 with an HP9725 disk was used in the illustrations in this paper.
Our work is supported by the Hewlett Packard Corporation, Desktop Computer Division.
B. DialoKue Desi~nin~ Dialogue In the day-by-day use of an interactive system, users are very often involved in repetitive tasks.
They c o u l d be r e l i e v e d o f much o f t h e d r u d g e r y o f such tasks if the system were more knowledgeable.
Such a knowledgeable system, as it goes about a t a s k f o r t h e u s e r, may need a d d i t i o n a l information from the user.
What information it needs aCa particular point may depend on earlier user inputs and the current state of the database.
The user must provide the system with knowledge of a particular cask; more precisely s/he must program this knowledge into t h e system.
The result of this programming will be a system guided dialogue which the user can subsequently initiate and which will then elicit the necessary inputs.
Using these inputs in c o n j u n c t i o n w i t h the knowledge already available, particularly the data base, the system completes the task.
It is this system-guided dialogue chat the user needs to be able to d e s i g n . In the ASK System, there is a special dialogue w h i c h can be used co d e s i g n s y s t e m g u i d e d dialogues Co accomplish particular casks.
We call this the Dialogue Designing Dialogue (DDD).
Using DDD, the user becomes a computer-aided designer.
Since DDD, in conducting its dialogue with the user, only requires simple responses or responses phrased in ASK English, the user need have little programming skill or experience.
Using DDD, the user alone can replace a tedious, repetitive cask with an efficient system guided dialogue, all in a natural language environment.
The ASK Dialogue Designing Dialogue constitutes a high level, 24
A Robust P o r t a b l e N a t u r a l L a n g u a g e D a t a B a s e I n t e r f a c e Jerrold M.
Ginsparg Bell Laboratories Murray Hill, New Jersey 07974 A BSTRA C T This paper describes a NL data base interface which consists oF two parts: a Natural Language Processor (NLP) and a data base application program (DBAP).
The NLP is a general pur!~se language processor which builds a formal representation of the meaning of the English utterances it is given.
The DBAP is an algorithm with builds a query in a augmented relational algebra from the output of the NLP.
This approach yields an interface which is both extremely robust and portable.
where "colored", "'color" and "'house" are system primitives called concepts.
Each concept is an extended case frame, [Fillmore 2].
The meaning of each concept to the system is implicit in its relation to the other system concepts and the way the system manipulates it.
Each concept has case preferences associated with =IS cases.
For example, the case preference of color is "color and the case preference of coloredis "physical-object.
The case preferences induce a network among the concepts.
For example, "color is connected to "physical-object via the path: ['physical-object colored'colored color "color].
In addition.
"color is connected to "writing,implement, a refinement ot" "physicalobject, by a path whose meaning is that the writing implement writes with that color.
This network is used by the NLP to determine the meaning of many modifications, For example, "red pencil" is either a pencil which is red or a pencil that writes red, depending on which path is chosen.
In the absence of contextual information, the NLP chooses the shortest path.
In normal usage, case preferences are often broken.
The meaning of the broken preference involves coercing the offending concept to another one via a path in the network.
Examples are: "Turn on the soup".
"Turn on the burner that has soup on it".
"My car will drink beer".
"The passengers in my car will drink beer" This paper describes an extremely robust and portable NL data base interface which consists of two parts: a Natural Language Processor (NLP) and a data base application program (DBAP).
The NLP is a general purpose language processor which builds a formal representation of the meaning of the English utterances it is given.
The DBAP is an algorithm with builds a query in an augmented relational algebra from the output of the NLP.
The system is portable, or data base independent, because all that is needed to set up a new data base interface are definitions for concepts the NLP doesn't have, plus what I will call the +data base connection", i.e,, the connection between the relations in the data base and the NLP's concepts.
Demonstrating the portability and the robustness gained from using a general purpose NLP are the main subjects of this paper Discussion of the NLP will be limited to its interaction with the DBAP and the data base connection, which by design, is minimal.
[Ginsparg 5] contains a description of the NLP parsing algorithm.
3. The Data Base Connection 2.
NLP overview The formal language the NLP uses to represent meaning is a variant of semantic nets [Quillian 8].
For example, the utterances "The color of the house is green," "The house's color is green".
"Green,~ the color that the house is".
would all be ~ransformed to: Consider the data base given by the following scheme: Parts(pno,pname,color.cosl,weight) Spj( sno,pno,jno.quantity,m, y ) Suppliers and proiects have a number, )~ame and c~tV Parts ha'.,: a number, name, color, cost and weight Supplier wl(~,,unphe,, a quanntYof parts pno to prolect /no in month,nor year The data base connection has four parts: gl Isa: "colored Tense: present Colored: g2 Color: g3 lsa: "house Definite: the Isa: "color Value: green I.
Connecting each relation to the appropriate concept: Suppliers > "supplier fi2.
Connecting each attribute to the appropriate concept: leg., Spj(sno,pno,jno,cost,quantity)) depended on the supplier in which the cost ol a part 4.
C r e a t i n g pseudo relations Pseudo Cities jcity,scity This creates a pseudo relation, Cities(cname), so that the query building algorithm can treat all attributes as if they belong to a relation.
The query produced by the system will refer to the Cities relation.
A postprocessor is used to remove references to pseudo relations from the final query.
Pseudo relations are important because they ensure uniform handling of attributes.
With the pseudo Cities relation, questions like "Who supplies every city?
= and "List the cities".
can be treated identically to "Who supplies every project'"? and "List the suppliers".
The remainder of the data base connection is a set of switches which provide information on how to print out the relations.
whether all proper nouns have been defined or are to be inferred.
whether relations are multivalued, etc.
The switch settings and the four components above constitute the entire data base connection, Nothing else iS needed.
The network of concepts in the N L P should only be augmented for a particular data base; never changed.
Yet different data base schemes will require different representations for the same word.
For example, depending on the data base scheme, it could be correct to represent "box" as either, gl g2 g3 [sa: "part Conditions: "named(gl,box) Isa: "container Conditions: "named(g2.box) [sa: "box 3.
Capturing the information implicit in each relation: Parts(pno,pname,color,cost,weight ) "indexnumberp i n d e x n u m b e r > pno n u m b e r e d > Parts "named n a m e > pname n a m e d > Parts "colored color > color c o l o r e d > Parts costobj > Parts "weighs w e i g h t > weight w e i g h t o b j > Parts Projects(jno.jnamedcity) "indexnumberp indexnumber > jno n u m b e r e d > Projects "named name > jname n a m e d > Projects "located location > jetty located > Prolects Suppliers(sno,sname,scity) "indexnumberp indexnumber > sno n u m b e r e d > Suppliers "named name > sname n a m e d > Suppliers "located location > sctty located > Suppliers %pl O~no.pno.lno.quant Hv.m.y ) "supply supplier > '.no supplied > pno suppliee > mo (cardinality-of pno) > quantity u m e > m.y "spend spender > 1no s p e n d f o r -> pno amount (" cost quantity) The a m o u m case of "spend maps to a c o m p u t a t i o n rather than a,~mgle a t t r i b u t e It' all the attributes in the c o m p u t a h o n are not present,n the relation being defined, the query building program ioms,n the necessary extra relations.
So the definition of "spend ~mrks equally well irl tile example scheme as well as in a scheme 26 The solution is to define each word to map to the lowest possible concept.
W h e n a concept is e n c o u n t e r e d that has a data base relation associated with )t.
there is no problem.
If there )s no relauon associated with a concept, the N L p searchs For a concept that d o e s correspond to a relation and is also a generalization ot" the concept in question.
I f one is found, it is used with an appropriate condilion, usually "tilled or "named.
So "box" has a definition which m a p s to "box.
In the data base c o n n e c t i o n given above.
"box" would be instantiated as a "=part" since " ' b o x " is a r e f i n e m e n t of "'part" and no relation maps to "box," Using the Connection The information in the data base connection ts primarily used m building the query (section.~).
But It IS ~llso used Io augment the knowledge base of Ihe N L P The data base connection is used to overwrite the NLP's ca~e preferences.
Since I o c a w d > S u p p h e r s ()r Projects.
the preference ot" localed ts spec)fied to "suppliers or "protects.
This enables the NLP to interpret the first noun group )n "Do,m', suppliers that supply widgets located nl london also supply,~cre',vs )" as "'suppliers in London that supply widgets" rather than "supphers that,;upph London wldgets" This )s in contrast to [Gawron 31 which u'..es,i separate "disambiguator" phase to ehmlnale parses that do 11()i make sense =n the conceptual scheme of the dala base.
Tile additional preference informamm supplied bv the data base connection is used to induce coercions (section 2).
thai would rlot be made in the absence of the connection (~r under,mother data fibase scheme.
"Who supplies London" does not break any real world preferences, but does break one of the preferences induced by this data base scheme, namely that Suppliee is a "project.
London. a "city, is coerced to "project via the path [*project located *located /ocanon cityl and the question is understood to mean "Who supplies projects which are in London".
As mentioned in Section 2., the NLP determines the meanin~ of many modifications by searching for connections in a semantic net.
The data base connection is used to augment and highlight the existing network of the NLP.
I f the user says, "What colors do parts come in?', the NLP can infer that the meaning of "come-in" intended by the user is "colored since the only path through the net between "color and "part derived from the case preferences induced by the data base connection is ['part colored "colored color "color] Similarly, when given the noun group "London suppliers" the meaning is determined by tracing the shortest path through the highlighted net, ['supplier located'located Iocanon "city] The longer path connecting "supplier and "city, ['supplier supplier "supply suppliee *project located "location location *city] which means "the suppliers that supply to london projects" is found when the NLP rejects the first meaning because of context, If the user says "What are the locations of the London suppliers" the system assumes the second meaning since the first (in the domain of this data base scheme) leads to a tautological reading.
The NLP is able to infer that "The locations of the suppliers located in London" is tautological while "The locations of the suppliers located in England" is not, because the data base connection has specified "located to be a single valued concept with its Iocarton case typed to "city.
I f the system were asked for the locations of suppliers in England, and it knew England was a country, the question would be interpreted as "the cities of the suppliers that are located in cities located in England." The NLP treats most true-l'aise questions with indefinites as requests for the data which would make the statement true.
The question's meaning is "to show the subset of london proiects that are supplied by Blake".
The query building algorithm builds up the query recursively Given an instantiated concept with cases, =t expands the contents of each case and links the results together with the relation corresponding to the concept.
Given an instantiated concept with conditions, it expands each condition.
For the example, we have.
Expand gl Expand g2, the Element of gl Expand gg, the Condition of g2.
Expand g3, the Supplier case of gg.
Expand g9, the Condition of g3.
From the data base connection, a "named whose named case is a *supplier is realized by the Suppliers relation using the sname attribute So we have, g9 select tram Suppliers where sname -blake From the data base connection, a "supply is realized by the Spj relation.
This results in, gga -project]no/i'om.joinSpj to g9 g8 -joingga toProjects g8 is the projects supplied by Blake.
Expand 84, the set gl is a subset of, by expanding its element.
g6 Expand glO.
the Condition of gb Expand g7, the location case of glO yielding g l l -select #am Cities wherecname london A "located with a "project in the Iocotedcase ~s realized by the Projects relation using the ]city attribute.
So we have.
glOa -join Projects Io gl I where]city = cname glOb proiect ]no /'romglOa glO ]oinglOb toProjects g[0 is the projects in London.
Intersect the expansions of g2 and g4 and project the prolect names.
gl3 = pro/eel]name lrom imersectton g 8 glO 5.
A trtee of the query building algorithm.
The query budding algorithm is illustrated by tracmg its operation on the question, "Does blake supply any prolects in london'?" The entire query is, The NLP's meaning representation I'or this question ts shown below.
gO Isa: "show g5 Isa: "name Value: blake g9 Isa: "named Tense: present Named: g3 Name: g5 Isa: "tocated Tense: present Located: gb Location: g7 Isa: "named Tense: present Named: g7 Name: g12 Isa: *name Value: london Tense: present Toshow: g l gO [sa: "set Element: 2 Subset-of': g4 Isa: "protect Isa: "project Element-of: g4 Conditions: glO glO lsa: city Conditions: gll Isa: "supply Tense: Present Suppler: g3 Suppliee: g2 gl [ g9 = select/romSuppliers where s n a m e = blake g8a -/~'oiecrjno #om ioin Spj to g9 g8 = loin gga to Projects gl0 = select #am Projects where icily = london g 13 -prelect iname lrom mter'~e('tlo~t g8 g I0 where the: extra loin resulting f'rom the pseudo (:h=e~ relation ha', been rernoved by the post processor (section 3 ) Entirely as a side effe,'t of the way the query rs generated, the -,,,,,tern can easily correct any l'alse assumptions made by the u~,,2r [Kaplan 71.
For example, if there were no projects in London.
gill would be empty and system would respond, generating Irom the instantiated concept glO li.e., the names used in query correspond to the names used in the knowledge representatmnL "There arc no suppliers located in London".
No additional "'.=oiated presupposition" mechanism is requ+red.
The remainder of this section discusses several aspects o the query building process that the trace does not show.
Negations are handled by introducing a set difference when necessary If the example query were "Does Blake supply any projects that aren't in London?", the expansion of g7 would have been.
I f we ask.
"Who frequents a bar that serves a beer John likes?".
we get the following query.
81 82 g3 84 85 =" select from Likes where drinker john project beer l'rom g 1 .join Serves to 82 =" project bar I~om g3 "" join Frequents to 84 Expand g7.
the location case of glO yielding g i l a select [romCities wherecname -london gl 1 difference o f Cities a n d g l la Conjunctions are handled by introducing an an intersection or union.
I f the example query were "Does Blake supply any projects in London or Paris'?', the /ocanon case of g10 would have.
been the conjunction 813.
I f we ask "Who frequents a bar that serves a beer that he likes"? the correct query, is.
isa "conjunction Type: or Conjoins: g7 g14 [sa: "city Conditions: g l 5 lsa: "named Named: g15 Name: g l 6 Isa: "name Value: paris glb In the first query "beer" was the only attribute projected from g l [n the second, the system projected both "beer" and "drinker", because in expanding "a beer he likes" it needed to expand an instantiated concept (the one representing "who") that was already being expanded.
All of these cases interact gracefully with one another.
For example.
there is no problem in handling "Who supplies every project that is not supplied by blake and bowles".
The result of expanding gl3 would be, [n general, "or" becomes a union and "and" becomes an intersection.
However, if an "and" conjunction is in a single valued case (information obtained from the data base connection), a union is used instead.
Thus "Who supplies london and paris"? is interpreted as "Who supplies both London and Paris'"? and "Who is in London and Pans"? is interpreted as "Who is in London and who ~s m Paris"?
)n the example data base scheme.
6. Advantages of this approach The system can understand anything it has a concept about.
regardless o f whether the concept is attached to a relation in the data base scheme.
In the Suppliers data base from Secuon 4., parts had costs and weights associated with them, but not sizes.
I f a user asks "How big are each of the parts"? and the interface has a "size primitive (which it does), the query building process wdl attempt to find the relation which "size maps to and on fading wdl report back to the user.
"There is no information in the data base about the size of the parts".
This gives the user some informatmn about the what the data base contains, An answer like "1 don't know what "big" means".
would leave the user wondering whether size information was in the data base and obtainable if only the "right" word was used.
The system can interpret user statements that are not queries.
If the user says "A big supplier is a supplier that supplies more than 3 projects" the NLP can use, the definition qn answering later queries.
The definition is not made on a "string" basis e.g., substttuting the words of one side of the definition for the other Instead.
whenever the query building algorithm encounters an mstantiated concept that is a supplier wnh the condition "size~x.
big) it builds a query substnuting the condiuon from the definition that it can expand as a data base query Thus the .~vstern can handle "big london suppliers" and answer "Which sunpliers are big" which it couldn't if ~t were doing strlct string substitution.
This Facility can be used to bootstrap common definitions In,~ commercial flights application, with data base scheme, Flights(fl#,carrier,from.to,departure,arrival.stops.cost ) the word "nonstop" is defined to the system in English as, "A nonstop flight is a night that does not make any stops " and then saved along wuh the rest of the system's defimt~ons.
28 Quantifiers are handled by a post processing phase.
"Does blake supply every project in London"? is handled identically to "Does Blake supply a prolect in London'"? except that the expansion of "projects m London" is marked so that the post processor will be called.
The post processor adds on a set of commands which check that the set difference of London projects and London prolects that Blake supplies is empty.
The rasulhn 8 query is.
gl = =_2 g3 = g4 = =_5 = gO = g7 = g8 = ~e/ect lrom Suppliers w/weresname = blake ~elect l m m Projects where jcity london /otnSpl togl tomg3 to g2 protect jno from g2 protect ino /tom g4 {hl]~'rem'e org5 andgO empn, g7 ] h e first tour commands are the query for "Does Blake supply a llrolect m London'?".
The last tour check that no project in London is not supplied by Blake.
-\ minor modification is needed to cover cases in which the query building algorithm is expanding an instantiated concept that refercnces an instuntiated concept that is being expanded in a higher recursmve call The following examples illustrate this.
Consider the data base scheme below, taken from [Ullman ql.
Coercions (section 2).
can be used solve problems that may require inferences in other systems.
[Grosz 6] discusses the query "Is there a doctor within 200 miles of Philadelphia" in the context of a scheme in which doctors are on ships and ships have distances from cities, and asserts that a system which handles this query must be able to inter that if a doctor is on a ship, and the ship is with 200 miles of Philadelphia, then the doctor is within 200 miles of Philadelphia.
Using coercions, the query would be understood as "is there a ship with a doctor on it that is within 200 miles of Philadelphia?', which solves the problem immediately.
Since the preference information is only used to choose among competing interpretations, broken preferences can still be understood and responded to.
The preference for the supplier case is specified to supplier but if the user says "How many parts does the sorter project supply"? the NLP will find the only interpretation and respond "projects do not supply parts, suppliers do".
Ambiguities inherent in attribute values are handled using the same methods which handles words with multiple definitions.
For example, 1980 may be an organization number, a telephone extension, a number, or a year.
The NLP has a rudimentary (so far) expert system inference mechanism which can easily be used by the DBAP.
One of the rules it uses is "If x is a precondition of y and z knows y is true then z knows x was and may still be true" One of the ['acts in the NLP knowledge base is that being married is a precondition of being divorced or widowed.
I f a user asks "Did Fred Smith used to be married"? in a data base with the relation Employees(name, marital-status) the system can answer correctly by using its inference mechanism.
The exact method is as follows.
The data base application receives the true-false question: "Fred Smith was married and Fred Smith is no longer married" The system handles all the examples in this paper as well as a wide range of others (Appendix A.).
Several different data bases schemes have been connected to the system for demonstrations, including one "real data base" abstracted from the on-line listing of the Bell Laboratories Company Directory.
9. Since the data base includes only current marital status information.
the only way to answer the first part of the question is to inl'cr it from some other information in the data base.
The data base application sends the query to the NLP inference mechanism which would ordinarily attempt to answer it by matching it against its knowledge base or by finding a theorem which would gives it something else to match ['or When called by the data base application, the inference mechanism simply uses its rules base to decide what it should match ['or, and then returns to the data base program.
In this, example, the inference mechanism receives "Fred Smith was married" and using the precondition rule mentioned above, returns to the data base program, "Is Fred Smith divorced" or "is Fred Smith widowed", which can be answered by the data base.
The DBAP can call the inference mechanism recursively if necessary.
Appendix A: Sample Questions and Answers  does every department head in 1135 have an office in holmde[?
No. all heads Ihat work For center number 11.]5 are not located in an office in ho  who isn't alles,h g is the head not located in an ofi~ce in ho that works for center number [ 135  who has been with us for longer than fred smith? 7.
Implementation Status and Details The DBAP ~s fully implemented and debugged.
The NLP is ~mplemented and sail growing.
Both are implemented in Franz Lisp.
a dialect of LISP Language processing and query generai~on are performed m virtually real time (average 1-3 cpu seconds) on a Vax I 1-780 The system ~s intended to be used with u Data Base Management system.
The interface between the DBAP and the I-)BMS is a,,tralghtforward translator from relational algebra to the query language of Ihe DflMS I have written a Ir;.mslator I'or Polaris [Gielan 41.
There are no employees that have worked ('or you ['or more than the length or" the time fred smith has worked for ~.'~'~u  list the employees who used to be married and are not married now, paul rochester, verlon biggs and willie davis are the empin~ee,~ that were married and that are not married now.
TEAM: A TRANSPORTABLE NATURAL-LANGUAGE INTERFACE SYSTEM B a r b a r a J.
G r o s z Artificial Intelligence Center SRI I n t e r n a t i o n a l Menlo P a r k, CA 94025 A.
Overview A major benefit of u s i n g n a t u r a l language to the i n f o r m a t i o n in a database is that it shifts o n t o t h e system t h e b u r d e n of m e d i a t i n g b e t w e e n two v i e w s o f t h e d a t a : t h e way i n which t h e d a t a i s s t o r e d ( t h e " d a t a b a s e v i e w " ), and t h e way i n which an e n d u s e r thinks about it (the "user*s view").
Database information is recorded in terms of files, r e c o r d s, and fields, while natural-language expressions refer t o the same information i n terms of entities and relationships in the world.
A major problem in constructing a natural-language interface is determining how to encode and use the information needed to bridge these two views.
Current natural-language interface systems require extensive efforts by specialists in natural-language processing to p r o v i d e them w i t h t h e i n f o r m a t i o n t h e y need t o do the bridging.
The systems are, in effect, handtallored to provide access to particular databases.
access how Co o b t a i n t h e information requested.
Moving s u c h s y s t e m s to a new d a t a b a s e r e q u i r e s c a r e f u l handcrafting that involves d e t a i l e d knowledge o f such things ae p a r s i n g p r o c e d u r e s, t h e p a r t i c u l a r way i n which domain i n f o r m a t i o n i s stored, and data-access procedures.
To provide for transportability, TEAM s e p a r a t e s i n f o r m a t i o n a b o u t language, about the domain, and a b o u t the database.
The d e c i s i o n t o p r o v i d e t r a n s p o r t a b i l i t y to existing conventional databases (which d i s t i n g u i s h e s TEAM from CHAT [ W a r r e n, 1981]) means that t h e d a t a b a s e c a n n o t be r e s t r u c t u r e d t o make t h e way i n w h i c h i t s t o r e s d a t a more c o m p a t i b l e w i t h t h e way i n which a u s e r may a s k a b o u t t h e data.
A l t h o u g h many p r o b l e m s can be a v o i d e d i f one i s a l l o w e d t o d e s i g n t h e d a t a b a s e a s w e l l a s the natural-language system, given the prevalence of existing conventional databases, approaches w h i c h make t h i s assumption are likely t o have limited applicability in the near-term.
This paper f o c u s e s on the p r o b l e m of constructing transportable natural-language interfaces, i. e ., s y s t e m s t h a t can be a d a p t e d t o p r o v i d e a c c e s s t o d a t a b a s e s f o r which t h e y were not specifically handtailored.
It describes an initial version of a transportable system, called TEAM (for ~ransportable E_ngllsh A_ccess Data manager).
The hypothesis underlying the research described in this paper is that the i n f o r m a t i o n required for the adaptation can be obtained through an Lnteractlve dialogue with database management personnel who are not familiar with natural-language processing techniques.
The TEAM s y s t e m h a s three major components: ( 1 ) an a c q u Z s t t i o n component, ( 2 ) t h e DIALOGIC language system [Grosz, et al., 1982], and (3) a data-access ccaponent.
Section C descrlbes how the language and data-access components were designed to accommodate the needs of transportability.
S e c t i o o D d e s c r i b e s the d e s i g n of the acquisition component to allow flexible interaction ~rlth a database expert and discusses acquisition problems caused by the differences between the database view and user view.
Section E shows how end-user queries are interpreted after an acquisition has been completed.
Section F describes the current state of development of TEAM and lists several problems currently under investigation.
B. I s s u e s of T r a n s p o r t a b i l i t y C.
System Design The insistence on transportability distinguishes TEAM from previous systems such as LADDER [Hendrlx ec al., [978] LUNAR [Woods, Kaplan, and Webber, 1972], PLANES [Waltz, 1975], REL [Thompson, [975], and has affected ~he design of the natural-language processln~ system in several ways.
Most previously built naturallanguage interface systems have used techniques that make them inherently difficult to transfer to new domains and databases.
The internal representations [n these systems typically intermix (in their data structures and procedures) information about language with information about the domain and the database.
In addition, in Interpretln~ a query, the systems conflate what a user is requesting (what hls query "means") with 39 I n TEAM, t h e t r a n s l a t i o n o f an E n g l i s h q u e r y into a database query takes place in two s t e p s . First, the DIALOGIC system constructs a representation of the literal meaning or "logical form" of the query [Moore, 1981].
Second, the data-access component translates the logical form into a formal database query.
Each of these steps requires a combination of some information that is dependent on the domain or the database wlth some information that is not.
To provide for transportability, the TEAM system carefully separates these two kinds of information.
fiI. Domainand Database-Dependent Information To adapt TEAM to a new database three kinds of information must be acquired: information about words, about concepts, and about the structure of the database.
The data structures that encode this information--and the language processing and data-access procedures that use them--are designed to allow for acquiring new information automatically.
Information about words, lexlcal information, includes the syntactic properties of the words that will be used in querying the database and semantic information about the kind of concept t o which a particular word refers.
TEAM records the lexlcal information specific to a given domain in a lexicon.
Conceptual information includes information about taxonomic relationships, about the kinds of objects that can serve as arguments to a predicate, and a b o u t t h e k i n d s o f p r o p e r t i e s an object can have.
I n TEAM, t h e internal representation of information about the entities in the domain of discourse and the relationships that can hold among them is provided by a conceptual schema.
This schema includes a sort hierarchy encoding the taxonomic relationships among objects in the domain, information about constraints on arguments to predicates, and information about relationships among certain types of predicates.
database schema encodes information about how concepts in the conceptual schena map onto the structures of a particular database.
In particular, it links conceptual-schema representations of entities and relationships in the domain to their realization in a particular database.
TEAM currently assumes a relational database with a number of f i l e s . (No languageprocesslng-related problems are entailed in moving TEAM to other database models).
Each file is about some kind of object (e.g., employees, students, ships, processor chips); the fields of the file record properties of the object (e.g., department, age, length).
A To provide access to the informa=,on in a particular database, each of the components of DIALOG~C must access domain-speciflc information about the words and concepts relevant to that database.
The information required by the syntactic rules is found in the lexicon.
Information required by the semantic and pragmatic rules is found in the lexicon or the conceptual schema.
The rules themselves however do not include such domain-dependent information and therefore do not need to be changed for different databases.
In a similar manner, the data-access component separates general rules for translating logical forms into database queries from information about a particular database.
The rules access information i n the conceptual and database schemata to interpret queries for a particular database.
D. Acquisition TEAM i s d e s i g n e d t o i n t e r a c t w i t h two k i n d s o f u s e r s : a d a t a b a s e e x p e r t (DBE) and an e n d u s e r . The DBE provides information about the files and fields in the database through a system-dlrected acquisition dialogue.
As a result of this dlaloEue, the language-processlng and data-access components are extended so that the end-user may query the new database in natural-language.
i. Acquisition Questions Because the DBE is assumed to be familiar with database structures, but not with language-processlng techniques, the acquisition dialogue is oriented around database structures.
That is, the questions are about the kinds of things in the files and fields of the database, rather than about lexlcal entries, sort hierarchies, and predicates.
The disparity between the database view of the data and the end-user's view make the acquisition process nontrlvlal.
For instance, consider a database of information about students in a university.
From the perspective of an enduser "sophomore" refers to a subset of all of the students, those who are in their second year at the university.
The fact that a particular student is a sophomore might be recorded in the database in a number of ways, including: (l) in a separate file containing information about the sophomore students; (2) by a special value in a symbolic field (e.g., a CLASS field [n which the value SOPH indicates "sophomore"); (3) by a "true" value in a Boolean field (e.g., a * in an [S-$O?H field).
For natural-language querying to be useful, the end-user must be protected from having to know which type of representation was chosen.
The questions posed to the DBE for each kind of database construct must be sufficient to allow DIALOGIC to handle approximately the same range of Domain-lndependent Information The language executive [Grosz, e t a l ., 1982; Walker, 1978|, DIALOGIC, coordinates syntactic, semantic, and basic pragmatic rules in translating an English query into logical form.
DIALOGIC's syntactic rules provide a general grammar of English [Robinson, 1982].
A semantic "translation" rule associated with each syntactic phrase rule specifies how the constituents of the phrase are to be interpreted.
Basic pragmatic functions take local context into account in providing the interpretation of such things as noun-noun combinations.
DIALOGIC also includes a quantlfler-scoping algorithm.
filinguistic expressions (e.g., for referring to "students i n t h e sophomore c l a s s ' ) r e g a r d l e s s o f the particular database implementation chosen.
In all c a s e s, TEAM w i l l c r e a t e a l e x i c a l e n t r y f o r " s o p h o m o r e " and an e n t r y i n t h e c o n c e p t u a l schema to represent the concept of sophomores.
The database attachment for thls concept will depend on t h e p a r t i c u l a r d a t a b a s e s t r u c t u r e, as w i l l the kinds of predicates f o r which i t can be an argument.
Example of Acquisition Queeclons I n d e s i g n i n g TEAM we f o u n d i t i m p o r t a n t to distinguish three differanc kinds of fields N arlthmeCic, feature (Boolean), and s y m b o l l c o n the b a s i s of t h e r a n g e of l i n g u i s t i c expressions to which each gives r i s e . AriChmetic fields contain numeric values on which comparisons and computations llke averaging are likely to be done.
(Fields containing dates a r e n o t y e t h a n d l e d by TEAM).
Feature fields contain true/false values w h i c h r e c o r d w h e t h e r o r n o t some a t t r i b u t e i s a property of the object d e s c r i b e d by t h e file.
Symbolic f i e l d s typically contain values that c o r r e s p o n d to n o u n s o r a d j e c t i v e s t h a t d e n o t e t h e s u b t y p e s o f t h e domain d e n o t e d by t h e f i e l d . D i f f e r e n t a c q u i s i t i o n q u e s t i o n s a r e asked f o r each type of field.
These are illustrated in the example i n S e c t i o n D.3.
To illustrate the acquisition of information, consider a database, called CHIP, containing information about processor chips.
In particular, the fields in this database contain the following information: the identification number o f a c h i p ( I D ), its m a n u f a c t u r e r (MAKER) its width i n b i t s (WIDTH), ice speed in m e g a h e r t z (SPEED), its cost i n d o l l a r s (PRICE), the kind of technology (FAMILY), and a flag indicating wheCher o r noc t h e r e is an e x p o r t l i c e n s e f o r t h e c h i p (EXP).
In the figures discussed below, the DBE's r e s p o n s e is indicated in uppercase.
For many quesClone the DBE is presented wlch a llst of options from which ha can choose.
For these questions, the complete llst is shown and the answer indicated in boldface.
F i g u r e i shows t h e short-form of the questions asked about the file itself.
In r e s p o n s e to q u e s t i o n ( 1 ), t h e DBE t e l l s TEAM w h a t fields are in the file.
Responses to the r e m a i n i n g quesCloms allow TEAM t o identify t h e kind of object the file contains information about (2), types of linguistic expressions used to refer to It [ (6) and (7)], how to identify individual objects in the database (4), and how to s p e c i f y i n d i v i d u a l o b j e c t s to the u s e r ( 5 ) . These responses result in the words "chip" and " p r o c e s s o r " b e i n g added t o t h e l e x i c o n, a new s o r t added to the taxonomy (providing the interpretation f o r t h e s e w o r d s ), and a l i n k made i n t h e d a t a b a s e schema b e t w e e n t h i s sort and records i n the file CHIP.
Figure 2 gives the short-form of the most central questions asked about symbolic fields, using the field MAKER (chip manufacturers) as exemplar.
These questions are used to determine the kinds of properties represented, how t h e s e r e l a t e t o p r o p e r t i e s i n o t h e r f i e l d s, and the k i n d s of linguistic expressions the field values can give rise to.
Question (4) allows TEAM to determine that individual field values refer to manufacturers rather than chips.
The long-form of Q u e s t i o n (7) i s : Will you want to ask, for example, "How many MOTOROLA processors are there"? to get a count of the number of PROCESSORS with CHIP-MAKER-MOTOROLA?
Question (8) expands to: Will you want to ask, for example, "How many HOTOROLAS are there"? to get a count of the number of PROCESSORS with CHIP-MAKER-MOTOROLA?
Acquisition Strategy The ~ a J o r features of the s tra te gy developed for acquiring information about a database from a DBE include: (1) providiu E multiple levels of detail for each question posed to the DBE; (2) allowing a DBE to review previous answers and change them; and (3) checking for legal answers.
At present, TEAM initially presents the DBE wlth the short-form of a quesclou.
A more detailed version ("long-form') of the question, including examples illustratlng different kinds of responses, can be requested by the DBE.
An obvious excenslon to this strategy would be to present different Inltial levels t o different users ( d e p e n d i n g, f o r e x a m p l e, on t h e i r p r e v i o u s experience wlth the system).
A c q u i s i t i o n I s e a s i e r i f e a c h new p i e c e of information is immediately i n t e g r a t e d into the u n d e r l y i n g knowledge s t r u c t u r e s o f t h e p r o g r a m . 8 o w e v e r, we a l s o wanted Co a l l o w t h e DSE t o change a n s w e r s to p r e v i o u s q u e s t i o n s ( t h i s has t u r n e d o u t to be an essential feature of TEAM).
Some questions (e.g., those about irregular plural forms and synonyms) affect only a single part of TEAM (the lexicon).
Other questions (e.g., those about feature fields) affect all components of the system.
Because of the complex interaction between acquisition questions and components of the system to be updated, immediate integration of new information is not possible.
As a result, updating of the lexicon, conceptual schema, and database schema Is not done until an acqulsition dialogue is completed.
In t h i s ease, t h e a n s w e r to q u e s t i o n ( 7 ) I s " y e s " and to q u e s t i o n ( 8 ) " n o " ; the field has v a l u e s that can be used as explicit, but not implicit, classifiers.
Contrast this wlth a symbolic field in a file about students that contains the class of a student; in this case the answer to both fiauesclons would be affirmative because, for example, the phrases "sophomore woman" and "sophomores" can be used to refer to refer to STUDENTS with CLASS=SOPHOMORE.
In other cases, the values may serve neither as explicit nor as implicit classifiers.
For example, one cannot say *"the shoe employees" or *"the shoes" to mean "employees in the SHOE department".
For both questions (7) and (8) a positive answer i s the default.
It i s i m p o r t a n t to allow the user to override thls default, because TEAM must be able to avoid spurious ambiguities (e.g., where two fields have identical field values, but where the values can be classifiers for only one field.).
Following acquisition of this field, lexical entries are made for "maker" and any synonyms supplied by the user.
Again a new s o n is created.
It i s marked a s h a v i n g v a l u e s t h a t can be explicit, b u t not implicit, classifiers.
Later, when the actual connection to the database is made, individual field values (e.g., "Motorola") will be made individual instances of this new sort.
Figure (3) presents the questions asked about arithmetic fields, using the PRICE field as exemplar.
Because dates, measures, and count quantities are all handled differently, TEAM must first determine which kind of arithmetic object is in the field (2).
In this case we have a unit of "worth" (6) measured in "dollars" (4).
Questions (8) and (9) supply information needed for interpreting expressions Involvlng comparatives (e.g., "What chips are more expensive than the Z8080")? and superlatives (e--~7, "What is the cheapest chip?").
Figure 4 gives the expanded version of these questions.
As a result of thls acquisition, a new subsort of the (measure) sort WORTH i s added to the taxonomy for PRICE, and is noted as measured in dollars.
In addition, lexlcal entries are created for adjectives indicating positive ("expensive") and negative ("cheap") degrees of price and are linked to a binary predicate that relates a chip to its price.
Feature fields are the most difficult fields to handle.
They represent a single (arbitrary) property of an entity, with values that indicate whether or not the entity has the property, and they give rise to a wide range of linguistic expresslons--adJectlvals, nouns, phrases.
The short-form of the questions asked about feature fields are given in Figure 5, using the field EXP; the value YES indicates there is an export license for a given processor, and NO indicates there is not.
Figures 6, 7, and 8 give the expanded form of questions (4), (6), and (B) respectively.
The expanded form illustrates the kinds of end-user queries that TEAM can handle after the DBE has answered these questions (see also Figure 9).
Providing thls kind of illustration has turned out to be essential for getting these questions answered correctly.
Each of these types of expression leads to new lexlcal, conceptual schema, and database schema entries.
I n general in the conceptual schema, feature field adJectlvals and abstract nouns result in the creation of new predicates (see Section E for an example); count nouns result in the creation of new subsorts of the file subject sort.
The database schema contains informatlon about which field to access and what field value is required.
TEAM also includes a limlted capability for acqulrln8 verbs.
At present, only transitive verbs can be acquired.
One of the arguments to the predicate cozTespondlng to a verb must be of the same sort as the file subject.
The other argument must correspond to the sort of one of the fields.
For the CHIP database, the DBE could specify that the verb "make" (and/or "manufacture") takes a CHIP as one argument and a MAKER as the second argument.
E. Sample Q u e r i e s and T h e i r [nterpretatlons After the DBE has completed an acquisition session for a file, TEAM can interpret and respond Co end-user queries.
Figure 9 lists some sample end-user queries for the file illustrated in the previous section.
The role of the different kinds of informatlon acquired above can be seen by considering the logical forms produced for several queries and the database attachments for the sorts and predicates that appear in them.
The following examples illustrate the information acquired for the three different fields described in the preceding section.
Given the query, What are the Motorola chips?
DIALOGIC produces the following logical form: (Query (WHAT tl (THING tl) (THE p2 (AND (PROCESSOR p2) (MAKER-OF p2 MOTOROLA)) (EQ p2 tl)))) where WHAT and THE are quantifiers; 1 tl and p2 are variables; AND and EQ have their usual interpretation.
The predicates PROCESSOR and MAKER-OF and the constant MOTOROLA were created as a result of acquisition.
The schema: PROCESSOR: MAKER-OF: following information in the database 1 Because the current version of DIALOGIC takes no account of the slngular/plural distinction, the uniqueness presupposition normally associated with "the" is not enforced.
42 fii s u s e d, a l o n g with s o r ~ h i e r a r c h y i n f o r m a t i o n i n the conceptual schema, t o g e n e r a t e the actual database query.
Similarly, t h e e n d u s e r query chips? new acqulslClon component allows t h e user more flexibility i n answering questions and provides a wider range of default answers.
TEAM c u r r e n t l y h a n d l e s m u l t i p l e files and provides transportability to a l i m i t e d r a n g e o f databases.
As menCloned previously, a relational database model is assumed.
Currently, TEAM also assumes all files are In third normal form.
The acquisition of verbs is limited Co allowing t h e DBE Co s p e c i f y t r a n s I C l v e v e r b s, as described in S e c t i o n D.3.
We a r e c u r r e n t l y excending TEAM t o What a r e t h e e x p o r t a b l e would l e a d to t h e l o g i c a l form: ( Q u e r y (WHAT t l (THING cl) (THE p2 (AND (PROCESSOR p2) where EXP-POS is a predlcace created by acquisIClon; it is true if its argumanC is exportable.
In thls case the relevant database scheme information I s : PROCESSOR: EXP-POS: file-CHIP keyfleld-[D file-CHIP fleld-EXP fieldvalue-T (I) Provide for interpretation of expressions involving such things as mass terms, aggregates, quantified c o a m a n d s, and commands t h a c r e q u i r e t h e s y s t e m Co p e r f o r m f u n c t i o n s o t h e r t h a n q u e r y i n g che d a t a b a s e . Provide for efficient p r o c e s s i n g of the m o s t common f o r m s o f c o n j u n c t i o n . Generalize the verb acquisition p r o c e d u r e s and e x t e n d TEAM t o h a n d l e more complex verbs, including such Chings as verbs wlth mulClple delineations, verbs chat require special prepositions, and verbs that allow senCenclel complements.
Handle d a t a b a s e s encoding time-related information and e x t e n d DIALOGIC to handle expressions involving clme and tense.
Finally, co illustrate how TEAM h a n d l e s arithmetic f i e l d s, and I n p a r t i c u l a r the use of comparatives, consider the query: What c h i p i s c h e a p e r chart 5 d o l l a r s ? The l o g i c a l form f o r Chin q u e r y I s ( Q u e r y (WHAT pl (PROCESSOR pl) ((MORE C ~ A P ) pl (DOLLAH 5)))) The conceptual schema encodes the relationship between the predicates CHEAP and PRICE-OF (again, both concepts created as a result of acquisition), wlCh t h e following information CHEAP: measure-predlcate-PRICE-OF scale-negative G.
Acknowledgments The d e v e l o p m e n t of TEAM has involved the efforts of many people.
Doug Appelc, Armar Archbold, Bob Moore, Jerry Hobbs, Paul Marcln, Pernando Pereira, Jane Robinson, Daniel Sagalowicz, and David Warren have made ~ a J o r contributions.
This research was supported by the Defense Advanced Research Projects Agency with the Naval Electronic Systems Command under Contract N0003980-'<:-0645.
The views and conclusions contained in Chin document are chose of the author and should not be interpreted as representative of the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the United States Government.
And the relevant database schema Informaclon is: PROCESSOR: PRICE-OF: file-CHIP keyfield-[D flit-CHIP field(argl)=[D fleld(arg2)-PRICE F.
Status and Future Research An initial version of TEAM was implemented in a combination of Incerlisp (acquisition and DIALOGIC components) and Prolog (data access component) on the DEC2060, but address space llmicatlons made continued development difficult.
Current research on TEAM is being done on the Symbolics LISP machine.
The acquisition component has been redesigned co cake advantage of capabilities provided by che blcmap display.
The 43 File nameC H ~ (1} Fields (ID MAKER WIDTH SPEED PRICE FAMILY EXP) (2) Subject P R O C E S S O R (31 Synonyms for P R O C E S S O R C H I P (4} Primazy key ID {5} IdentifyingfieldsM A K E R ID (8) Can one say W h o are the P R O C E S S O R S ? Y E S N O (7) Pronouns for filesubject H E S H E IT T H E Y (8) Field containing the name of each file subject ID Figure 1: Questions About File Field P R I C E ( 1) Type of field SYMBOLIC A R I T l t M E T I C FEATURE (2) Value t y p e . DATES M E A S U R E S COUNTS [3) Are the units implicit?
Y E S N O (4) Enter implicit unit DOLLAR (5) Abbreviation for this unit.~ (6) Measure type of this trait TIME WEIGHT SPEED VOLUME LINEAR AREA W O R T H OTHER {7) Minimum and maximum numeric valucs(1,100) (8} Positive adjectives (EXPENSIVE COSTLY) (9) Negative adjective (CHEAP) Figure 3: Questions for Arithmetic Field P R I C E Please specify any adjectives that can be used in their comparative or superlative form to indicate how much each P R O C E S S O R is in a positivedirectionon the scale measured by the values of CHIP-PRICE.
In a file about machine-tools with a numeric field called PRICE, one could ask: How E X P E N S I V E is each tool? to mean What is the price of each tool.~ EXPENSIVE, COSTLY, AND (HIGH PRICED) ~re positive adjectives designating the upper range of the PRICE scale.
C H E A P and (LOW PRICED), which designate the lower range of the PRICE scale, are negative adjectives.
Field M A K E R ( I ) Type of field S Y M B O L I C ARITHMETIC FEATURE (2) .Axe field values units of measure?
YES N O (3} Noun subvategory P R O P E R COUNT MASS (4} Domain of field value's reference SUBJECT F I E L D (5) Can you say W h o is the C H I P M A K E R t Y E S N O (6) Typical value M O R T O R O L A (7) Will values of this field be used as cia~sifers.~ E S N O Y {8) Will the values in this field be used alone as implicit classifiers?
YES N O Figure 2: Questions for Symbolic Field M A K E R Please enter any such adjectives you will want to ~ querying the database.
Figure 4: Expanded Version of Adjective Questions (Arithmetic Field} in Field E X P (I) Type of field SYMBOLIC ARITHMETIC F E A T U R E (2) Positive value YES (3) Negative value NO (4) Positive adjectives EXPORTABLE (5) Negative adjectives UNEXPORTABLE (6) Positive abstraA't nouns EXPORT AUTHORIZATION (7) Negative abstract no~.1 (8) Pmitive common nouns (9) Negative common nouns Figure 5: Questions for Feature Field ]gXP List any count nous~ ammciated with positive field value YES.
In general, this is any word wwww such that you might want to u k : What PROCESSORS are wwww-s! to mean What PROCESSORS have a CHIP-EXP of YES?
For example, in a file about EMPLOYEEs with  feature field CITIZEN having a positive field value Y and n e ~ t i v e field value N, you might want to aek: Which employees are citizens? instead of Which employees have a CITIZEN of Y?
Figure 8: Feature Field Count Nouns What adjectivab are aasoeiated with the field values YES in this field?
In general these are word.5 wwww such that you might want to Mk: Which PROCESSORS are www~' Which PROCESSORS have  CHIP-EXP of YES!
For example, in s medical file about PATIENTs with a feature field IMM having a positive field value Y and a negative filed value N, you might want to ask: Which patients are IMMUNE (or RESISTANT, PROTECTED)!
Figure 6: Feature Field Adjectivals ~,Vhat 8 bit chips are cheaper than the fastest exportable chip made by Zilogt Who makes the fastest exportable N M O $ chip costing less than 10 dollars!
By whom is the most expensive chip reader Who b the cheapest exportable chip made by!
Who is the most expensive chip made?
What is the fastest exportable chip that Motorola makes?
What 16 bit chips does Zilog make?
Who makes the fastest exportable N M O S chip?
Who makes the faatest exportable chip.~ Does Zilog make a chip that is faster than every chip that Intel makes?
Are there any 8 bit Ziiog chipe? is some exportable chip faster than 12 mhz?
Is every Ziiog chip that is f ~ t e r than 5 mhz exportable?
How faat is the faate~t exportable chip?
How expensive is the f~stest ~'~MOS chipt Figure 9: Sample questions for CHIP databaae List any abstrart nouns ~k~tociated with the positive feature value YES.
In general this is any word wwww such that you might want to ask a question of the form: Which PROCESSORS hove wwww? tO m e a n Which PROCESSORS have CHIP-EXP of YES!
For example, in a medical databaae about PATIENTs with a feature field IMM having a positive field value Y and a negative field value N, you might want to a~k: ~,Vhich patients have IMMUNITY? instead of Which patients have aa IMM of Y?
Figure 7: Feature Field Abstract Nouns REFERENCES Grosz, B.
et al . [1982] "DIALOGIC: A Core Natural Language Processing System," Proceedings of the Ninth International Conference on Computational Linguistics, Prague, Czechoslovakia (July 1982).
Moore, R.
C. [1981] "Problems in Logical Form," in Proceedings of the 19th Annual Meeting of the Association for Computaional Linguistics, pp.
117-L24. The Association for Computaional Linguistics, SRI International, Menlo Park, Californla (June 1981)..
Waltz, D.
[1975] "Natural Language Access to a Large Data Base: An Engineering Approach," Proc.
4th International Joint Conference on Artificial Intelligence, Tbillsl, USSR, pp.
868-872 (September 1975).
Warren, D . R . [1981] "Efficient Processing of Interactive Relational Database Queries Expressed in Logic," Proc.
Seventh International Conference on Very Large DataBases, Cannes, France, pp.
2"'2~-2--~', Robinson, J.
[1982] "DIAGRAM: A Grammar for Dialogues," Communications of the ACM, Vol.
25, No.
1, pp.
27-47 (January 1982).
Thompson, g . B . and Thompson, B . H . [1975] "Practical Natural Language Processing: The REL System as Prototype," H.
Rubinoff and M.
C. Yovlts, eds., pp.
109-168, Advances in Computers 13, Academic Press, New York, (New York 1975).
Woods, W.
A., R.
M. Kaplan, and B.
N-Nebber [I972] "The Lunar Sciences Natural Language Information System," BBN Report 2378, Bolt Beranek and Newman, Cambridge, Massachusetts (1972).
Walker, D.
E. (ed).
[1978] Understanding Spoken Language, Elsevier North-Hollam~, New York, New York, (1978) .
Customizable Descriptions of Object-Oriented Models Benoit Lavoie CoGenTex, Inc.
840 Hanshaw Road Ithaca, NY 14850, USA benoitOcogentex, com Owen Rambow CoGenTex, Inc.
840 Hanshaw Road Ithaca, NY 14850, USA owen~cogentex, com Ehud Reiter Department of Computer Science University of Aberdeen Aberdeen AB9 2UE, Scotland ereiter~csd, abdn.
ac. uk 1 Introduction: Object Models With the emergence of object-oriented technology and user-centered software engineering paradigms, the requirements analysis phase has changed in two important ways: it has become an iterative activity, and it has become more closely linked to the design phase of software engineering (Davis, 1993).
A requirements analyst builds a formal object-oriented (OO) domain model.
A user (domain expert) validates the domain model.
The domain model undergoes subsequent evolution (modification or adjustment) by a (perhaps different) analyst.
Finally, the domain model is passed to the designer (system analyst), who refines the model into a OO design model used as the basis for implementation.
Thus, we can see that the OO models form the basis of many important flows of information in OO software engineering methodologies.
How can this information best be communicated?
It is widely believed that graphical representations are easy to learn and use, both for modeling and for communication among the engineers and domain experts who tqgether develop the OO domain model.
This belief is reflected by the large number of graphical OO modeling tools currently in research labs and on the market.
However, this belief is not accurate, as some recent empirical studies show.
For example, Kim (1990) simulated a modeling task with experienced analysts and a validation task with sophisticated users not familiar with the particular graphical language.
Both user groups showed semantic error rates between 25% and 70% for the separately scored areas of entities, attributes, and relations.
Relations were particularly troublesome to both analysts and users.
Petre (1995) compares diagrams with textual representations of nested conditional structures (which can be compared to OO modeling in the complexity of the "paths" through the system).
She finds that "the intrinsic difficulty of the graphics mode was the strongest effect observed" (p.35).
We therefore conclude that graphics, in order to assure maximum communicative efficiency, needs to be complemented by an alternate view of the data.
We claim that the alternate view should be provided by an explanation tool that represents the data in the form of a fluent English text.
This paper presents such a tool, the MODELEXPLAINER, or MODEx for short, and focuses on the customizability of the system.1 Automatically generating natural-language descriptions of software models and specifications is not a new idea.
The first such system was Swartout's GIST Paraphraser (Swartout, 1982).
More recent projects include the paraphraser in ARIES (Johnson et al., 1992); the GEMA data-flow diagram describer (Scott and de Souza, 1989); and Gulla's paraphraser for the PPP system (Gulla, 1993).
MoDEx certainly belongs in the tradition of these specification paraphrasers, but the combination of features that we will describe in the next section (and in particular the customizability) is, to our knowledge, unique.
2 Features
of MoDEx MODEx was developed in conjunction with Andersen Consulting, a large systems consulting company, and the Software Engineering Laboratory at the Electronic Systems Division of Raytheon, a large Government contractor.
Our design is based on initial interviews with software engineers working on a project at Raytheon, and was modified in response to feedback during iterative prototyping when these software engineers were using our system.
 MoDEx output integrates tables, text generated automatically, and text entered freely by the user.
Automatically generated text includes paragraphs describing the relations between classes, and paral(Lavoie et al., 1996) focuses on an earlier version of MoDEx which did not yet include customization.
253 graphs describing examples.
The human-anthored text can capture information not deducible from the model (such as high-level descriptions of purpose associated with the classes).
 MoDEx lets the user customize the text plans at run-time, so that the text can reflect individual user or organizational preferences regarding the content and/or layout of the output.
 MoDEx uses an interactive hypertext interface (based on standard HTML-based WWW technology) to allow users to browse through the model.
 Input to MoDEx is based on the ODL standard developed by the Object Database Management Group (Cattell, 1994).
This allows for integration with most existing commercial off the shelf OO modeling tools.
Some previous systems have paraphrased complex modeling languages that are not widely used outside the research community (GIST, PPP).
 MODEX does not have access to knowledge about the domain of the OO model (beyond the OO model itself) and is therefore portable to new domains.
3 A
MoDEx Scenario Suppose that a university has hired a consulting company to build an information system for its administration.
Figure 1 shows a sample object model for the university domain (adapted from (Cattell, 1994, p.56), using the notation for cardinality of Martin and Odell (1992)) that could be designed by a requirements analyst.
Figure 1: The University OoO Diagram Once the object model is specified, the analyst must validate her model with a university administrator (and maybe other university personnel, such as dataentry clerks); as domain expert, the university administrator may find semantic errors undetected by the analyst.
However, he is unfamiliar with the "crow's foot" notation used in Figure 1.
Instead, he uses MoDEx to generate fluent English descriptions of the model, which uses the domain terms from the model.
Figure 2 shows an example of a description generated by MoDEx for the university model.
Suppose that in browsing through the model 254 using the hypertext interface, the university administrator notices that the model allows a section to belong to zero courses, which is in fact not the case at his university.
He points out the error to the analyst, who can change the model.
Suppose now that the administrator finds the texts useful but insufficient.
To change the content of the output texts, he can go to the Text Plan Configuration window for the text he has been looking at, shown in Figure 3.
He can add to the text plan specification one or more constituents (paragraphs) from the list of pre-built constituents (shown in the lower right corner of Figure 3).
After saving his modifications, he can return to browsing the model and obtain texts with his new specifications.
File Edit View Go Bookmarks Options Directory ~indow Help [List of Classes] [List of Models] [Reload Models] [Configuration] ~ [About ModeIF.xolame,] Description of the Class" Section' General Observations: A Section must be taught by exactly one F$ofesso, and may ~clong to zezo oz more Cqu~e s.
It must be tako by one ca more Students and may have at most one TA.
Examples: For example, Sectl is a Section and is taught by the professor Jolm Brown.
It belongs to two Courses, Math165 and Math201, and is take~ by two Students.
Frank Belfo~d and Sue Jones.
It has the TA Sally Blake.
Figure 2: Description Used for Validation............,-.,m~ .................
= ....................
I;|? i[Jlc Edll ~ew Go Bookmarks ~Jans Dlrc~r/ ~qndow Help Text Plsm Conflgm'aflon Tat Plmv V -'~'4'~"-(2~  ..,:L ~..cr=,.on o= = .=.,c~s ] 0 z~.~ ~.: ~===~==) i --=~'~ ~omponent I . -[ ~ose ~butes 3peretions :telafions-Teble :~elQ~ons-Te)d -:xemples-Long :xemples-Shod ~~ ~ ~'~ " Rle-Reference Figure 3: Text Plan Configuration Interface Once the model has been validated by the univerFile Edit View Go Bookmarks Options Directory Window _Help [List of Classes] [List of Model.~] [Reload Models] [Co:ffi~otation] [H~ [About ModelEx~01amer] [Q3_~] ~==:==~=~=~=~==::~==~==~:~:~====~===~::::::::::::::::::::::::::::~=~====~ Business Class: "Section' Purpose/Role: Course unit a student can take.
Ed11. Pu~o.~e Attributes: ii Am~u~ JiDeser~ t~n . iiTY~e .................. i i i ...................................... n~ber iSecUo n T'--"T""'7""" identifier ~#1NTF~3 ~ .................
]~'~ :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: Edit Attdbutee Relationships: A Section must be taught by exactly one Ptofee~ot and may belong to zero or more Cotuses.
It must b e taken by one or more Stud~nt.~ and may have at most one TAD server which receives requests via a standard Web CGI interface and returns HTML-formatted documents which can be displayed by any standard Web browser.
The documents generated by MoDEx are always generated dynamically in response to a request, and are composed of human-authored text, generated text and/or generated tables.
The main requests are the following: ModEx m Request i Figure 4: Description Used for Documentation sity administrator, the analyst needs to document it, including annotations about the purpose and rationale of classes and attributes.
To document it, she configures an output text type whose content and structure is compatible with her company's standard for OO documentation.
An example of a description obtained in modifying the text plan of Figure 3 is shown in Figure 4.
(This description follows a format close to Andersen Consulting's standard for documentation).
This description is composed of different types of information: text generated automatically (section Relationships), text entered manually by the analyst because the information required is not retrievable from the CASE tool object model (section Purpose), and tables composed both of information generated automatically and information entered manually (section Attributes).
The analyst then saves the text plan under a new name to use it subsequently for documentation purposes.
Note that while the generated documentation is in hypertext format and can be browsed interactively (as in the I-DOC system of Johnson and Erdem (1995)), it can of course also be printed for traditional paperbased documentation and/or exported to desktop publishing environments.
4 How
MODEX Works As mentioned above, MODEx has been developed as a WWW application; this gives the system a platform-independent hypertext interface.
Figure 5 shows the MoDEx architecture.
MoDEx runs as a Figure 5: MODEx Server Architecture  Text Plan Editing.
This generates an HTML document such as that shown in Figure 3 which allows a user to load/edit/save a text plan macro-structure specification.
A representation corresponding to the text plan of Figure 3 is shown in Figure 6.
Once edited, this representation can be stored permanently in the library of text plans and can be used to generate descriptions.
In this representation, User Text indicates free text entered for a title, while RelationsText and Examples-Short are schema names referring to two of the eight predefined text functions found in a C++ class library supplied with MoDEx.
alidation-Class) Ti~e, User Text Ti~e Schema ~itle Schema i I I I User Text Relations-Text User Text Examples-Short Figure 6: Macro-Stucture for Text Plan of Figure 3  Object Model Loading.
This loads an object model specification and generates a document displaying the list of classes found in the model.
 Description Generation.
This returns a description such as that shown in Figures 2 or 4.
To generate a description, the text planner creates a text structure corresponding to the text plan configuration selected by the user.
This text structure is a constituency tree where the internal nodes define the text organization, while the bottom nodes define its content.
The text content can be specified as syntactic repre255 sentations, as table specification and/or as humanauthored text for the titles and the object model annotations.
The text structure is transformed by the sentence planner which can aggregate the syntactic representations (cf.
conjunctions and in description on Figure 2) or introduce cue words between constituents (cf.
expression For example on Figure 2).
The resulting text structure is then passed to the text realizer which uses REALPRO (Lavoie and Rambow, 1997), a sentence realizer, to realize each individual syntactic representation in the text structure.
Finally, a formatter takes the final text structure to produce an HTML document.
 Object Model Annotation Editing.
This allows the user to edit human-authored annotations of the object model.
This editing can be done via links labelled Edit ...
which appear in Figure 4.
These human-authored texts are used by some of the predefined text functions to generate the descriptions.
5 Outlook
MoDEx is implemented in C++ on both UNIX and PC platforms.
It has been integrated with two object-oriented modeling environments, the ADM (Advanced Development Model) of the KBSA (Knowledge-Based Software Assistant) (Benner, 1996), and with Ptech, a commercial off-the-shelf object modeling tool.
MoDEx has been fielded at a software engineering lab at Raytheon, Inc.
The evaluation of MoDEx is based on anecdotal user feedback obtained during iterative prototyping.
This feedback showed us that the preferences regarding the content of a description can vary depending on the organization (or type of user).
The control that MoDEx gives over the text macro-structure is one step toward satisfying different types of text requirements.
We are currently extending MoDEx in order to give the user a better control over the text micro-structure, by replacing the set of predefined C++ text functions with customizable ASCII specifications.
This feature should make MODEx more easely portable among different types of users.
In addition, we intend to port MODEX to at least two new OO modeling environments in the near future.
Acknowledgments The first version of MoDEx for ADM was supported by USAF Rome Laboratory under contract F30602-92-C0015.
General enhancements to the linguistic machinery were supported by SBIR F30602-92-C-0124, awarded by USAF Rome Laboratory.
Current work on MODEx is supported by the TRP-ROAD cooperative agreement F30602-95-2-0005 with the sponsorship of DARPA and Rome Laboratory.
We are thankful to K.
Benner, M.
DeBellis, J.
Silver and S.
Sparks of Andersen Consulting, and to F.
Ahmed and B.
Bussiere of Raytheon Inc., for their comments and suggestions made during the development of MoDEx.
We also thank T.
Caldwell, R.
Kittredge, T.
Korelsky, D.
McCullough, A.
Nasr and M.
White for their comments and criticism of MoDEx.
Identifying Terms by their Family and Friends Diana Maynard Sophia Ananiadou Dept.
of Computer Science University of Sheffield Regent Court, 211 Portobello St Sheffield, $1 4DP, UK d.
maynard0dcs, shef.
ac. uk Computer Science, School of Sciences University of Saltbrd, Newton Building Saltbrd, M5 4WT, U.K. s.
ananiadou@salf ord.
ac. uk Abstract Multi-word terms are traditionally identified using statistical techniques or, more recently, using hybrid techniques combining statistics with shallow linguistic information.
Al)proaches to word sense disambiguation and machine translation have taken advantage of contextual information in a more meaningflfl way, but terminology has rarely followed suit.
We present an approach to t e r m recognition which identifies salient parts of the context and measures their strength of association to relevant candidate terms.
The resulting list of ranked terms is shown to improve on that produced by traditional methods, in terms of precision and distribution, while the information acquired in the process can also be used for a variety of other applications, such as disambiguation, lexical tuning and term clustering.
Introduction Although contextual information has been previously used, e.g. in general language (Grefenstette, 1994) mid in the NC-Value method for term recognition (Frantzi, 1998; Frantzi and Ananiadou, 1999), only shallow syntactic information is used in these cases.
The T R U C K S approach identifies different; elements of the context which are combined to form the Information Weight, a measure of how strongly related the context is to a candidate term.
The hffbrmation Weight is then combined with the statistical information about a candidate t e r m and its context, acquired using the NC-Value method, to form the SNC-Value.
Section 2 describes the NCValue method.
Section 3 discusses the importance of contextual information and explains how this is acquired.
Sections 4 and 5 describe the hffbrmation Weight and the SNC-VMue respectively.
We finish with an evaluation of the method and draw some conclusions about the work and its fllture.
Although statistical approaches to automatic term recognition, e.g.
(Bourigault, 1992; Daille et al., 1994; Enguehard and Pantera, 1994; 3usteson and Katz, 1995; Lauriston, 1996), have achieved relative success over the years, the addition of suitable linguistic information has the potential to enhance results still further, particularly in the case of small corpora or very specialised domains, where statistical information may not be so accurate.
One of the main reasons for the current lack of diversity in approaches to term recognition lies in the difficulty of extracting suitable semantic information from speeialised corpora, particularly in view of the lack of appropriate linguistic resources.
The increasing development of electronic lexieal resources, coupled with new methods for automatically creating and fine-tuning them from corpora, has begun to pave the way for a more dominant appearance of natural language processing techniques in the field of terminology.
The T R U C K S approach to t e r m recognition (Term Recognition Using Combined Knowledge Sources) focuses on identifying relevant contextual information from a variety of sources, in order to enhance traditional statistical techniques of t e r m recognition.
The NC-Value m e t h o d The NC-Value method uses a combination of linguistic and statistical information.
Terms are first extracted from a corpus using the C-Value method (Frantzi and Ananiadou, 1999), a measure based on frequency of occurrence and term length.
This is defined formally as: is not nested l~('n,) ~b~T~f(b)) a is nested where a is the candidate string, f(a) is its frequency in the corpus, eT, is the set of candidate terms that contain a, P(Ta) is the number of these candidate terms.
Two different cases apply: one for terms t h a t are found as nested, and one for terms that are not.
If a candidate string is not found as nested, its termhood is calculated from its total frequency and length.
If it is found as nested, termhood is calculated from its total frequency, length, frequency as a nested string, fiand the tmmber of longer candidate terms it; ai)l)ears in.
The NC-Value metho(1 builds oil this by incorl)orating contextual information in the form of a context factor for each candidate term.
A context word can be any noun, adjective or verb apI)earing within a fixed-size window of tim candidate term.
Each context word is assigned a weight, based on how frequently it appears with a ca lldidate term.
Ttmse weights m'e titan SUllslned for all colltext words relative to a candidate term.
The Context l"actor is combined with the C-Value to form tlm NC-Value: Category Verb Prep Noun Adj Weight 1.2 1.1 0.9 0.7 Table 1: We.ights for categories of boundary words where a is tile candidate term, Cvahte(a) is the Cvalue fin' tlm candidate term, CF(a) is the context factor tbr the candidate term.
Terminological knowledge Ternfinological knowledge concerns the terminological sta.tus of context words.
A context word whicll is also a term (whicll we call a context term) is likely to 1)e a better indicator than one wlfich is not.
The terminological status is determined by applying the NC-Value at)proach to the corlms, and considering tile top third of the list; of ranked results as valid terms.
A context term (CT) weight is then produced fin" each candidate term, based on its total frequency of occurrence with all relewmt context terms.
The CT weight is formally described as follows: Contextual Information: a Term's where a is the candidate term, 7', is the set: of context terms of a, d is a word from Ta, fa(d) is the frequency of d as a context term of a.
Semantic knowledge Semantic knowledge is obtained about context terms using the UMLS Metathesaurus and Semantic Network (NLM, 1997).
The former provides a semantic tag for each term, such as Acquired Abnormality.
The latte, r provides a hierarchy of semantic types, from wlfich we compute the similarity between a candidate term and the context I;erms it occurs with.
An example of part of tim network is shown in Figure Social Life Just as a person's social life can provide valuable clues al)out their i)ersonality, so we can gather much information about the nature of a term by investigating the coral)any it keeps.
We acquire this knowledge by cxtra:ting three different types of contextual information: 1.
syntactic; 2.
terminologic~fl; Syntactic knowledge Syntactic knowledge is based on words in the context which occur immediately t)efore or afl;er a candidatc term, wtfich we call boundary words.
Following "barrier word" al)proaches to term recoglfition (Bourigault, 1992; Nelson et al., 1995), where partitular syntactic categories are used to delimit era> didate terms, we develop this idea fllrther by weighting boundary words according to tlmir category.
The weight for each category, shown in Table 1, is all)cate(1 according to its relative likelihood of occurring with a term as opposed to a non-term.
A verb, therefore, occurring immediately before or after a candidate, term, is statistically a better indicator of a term than an adjective is.
By "a better indicator", we mean that a candidate term occurring with it is more likely to be valid.
Each candidate term is assigned a syntactic weight, calculated by summing the category weights tbr the context bomsdary words occurring with it.
Similarity is measured because we believe that a context term which is semantically similar to a candidate term is more likely to be significant than one wlfieh is less similar.
We use tim method for semantic distance described in (M~\ynard and Ananiadou, 1999a), wtfich is based on calculating the vertical position and horizontal distance between nodes in a hierarchy.
Two weights are cMculated:  positionah measured by the combined distance from root to each node measured by the number of shared common ancestors multiplied by the munber of words (usuMly two).
Similarity between the nodes is calculated by dividing tim commomflity weight by the 1)ositional weight to t)roduce a figure between 0 and 1, I being the ease The Information Weight The three individual weights described above are calculated for all relevant context words or context terms.
The total weights for the context are then combined according to the following equation: beC.
[TAIII OIIGANISM ITAIIlll ALGA Figure 1: Fragment of the Semantic Network where tile two nodes are identical, and 0 being the case where there is no common ancestor.
This is formally defined as follows: where a is the candidate term, Cais the set of context words of a, b is a word from C,, f,(b) is tlm frequency of b as a context word of a, syn~(b) is the syntactic weight of b as a context word of a, T.
is the set of context terms of a, d is a word fl'om T., fi,(d) is the frequency of d as a context term of a, sims(d) is the similarity weight of d as a context term of a.
This basically means t h a t the Infornlation Weight is composed of the total terminological weight, 511151tiplied by tile total semantic weight, and then added to the total syntactic weight of all the context words or context terms related to the candidate term.
where corn(w1...w,~) is the commonality weight of words The SNC-Value pos('wl...w,~) is the positional weight of words Let us take an example from the UMLS.
The similarity between a term t)elonging to the semantic category Plant and one belonging to the category Fungus would be calculated as follows:Tile Information Weight gives a score for each candidate term based on the ilnt)ortance of the contextual intbrmation surrounding it.
To obtain the final SNCValue ranking, the Information Weight is combined with the statistical information obtained using the NC-Vahm nmthod, as expressed formally below: where  Plant has the semantic code T A l l l and Fungus has the semantic code T A l l 2 .  The commonality weight is the number of nodes in common, multiplied by the number of terms we are considering.
T A l l l and T A l l 2 have 4 nodes in common (T, TA, TA1 and T A l l ) . So the weight will be 4 * 2 = 8.
 The positional weight is the total height of each of the terms (where tile root node has a height of 1).
T A l l l has a height of 5 (T, TA, TA1, T A l l and T A l l 1 ), and TAl12 also has a height of 5 (T, TA, TA1, T A l l and T A l l 2 ) . The weight will therefore be 5 + 5 = 10.
 The similarity weight is tile comlnonality weight divided by the positional weight, i.e. a is the candidate t e r m NCValue(a) is the NC-Value of a I W is the Inqmrtance Weight of a For details of the NC-Value, see (l:5'antzi and Ananiadou, 1999).
An example of the final result is shown in Table 2.
This corot)ares tile top 20 results from the SNCValue list with the top 20 from the NC-Value list.
The terms in italics are those which were considered as not valid.
We shall discuss the results in more detail in the next section, but we can note here three points.
Firstly, the weights for the SNC-Value are substantially greater than those for the NC-Vahm.
This, in itself, is not important, since it, is the position in the list, i.e. the relative weight, rather t h a n the absolute weight, which is important.
Secondly, we can see that there are more valid terms in the SNC-Value results than in the NC-Value results.
It Table 2: Top 20 results for the SNC-VaIue and NC-Value in hard to make flu:ther judgements based on this list alone, 1)ecause we cmmot s~3; wlmther on(; ter]u is 1)etter than another, if tiE(; two terms are both valid.
Thirdly, we can nee that more of the top 20 terms are valid tin' tim SNC-Vahm than for the NCValue: 17 (851X,) as ot)t)osed to 10 (50%).
discrei)an(:y 1)etween this lint and the lint validated by the manual experts (only 20% of the terms they judged valid were fOtlEl(1 ill the UMLS).
There are also further limitations to the UMLS, such as the fact that it is only nl)e(:ific to medicine in general, 1)ut not to eye t)athology, and the fact that it; is organised ill nllch a way that only the preferred terms, and not lexical variants, m'e actively and (:onnistently 1)r(~sent.
We first evaluate the similarity weight individually, since this is the main 1)rinciple on which the SNC-\Sflue method relies.
We then ewduate the SNC-VaIue as a whole t)y comparing it with the NCValue, so I;hat we can ewfluate the impact of tile addition of the deel)er forms of linguistic information incorl)orated in {:he hnI)ortance Weight.
Evaluation The SNC-Value method wan initially t(;sted on a eorl)US of 800,000 eye t)athoh)gy reI)ortn, which had 1)een tagged with the Brill t)art-of-nl)eeeh tagger (Brill, 1992).
The ca.ndidate terms we,'e first extracted using the NC-Value method (lhantzi, 1998), and the SNC-Value was then (:alculated.
To exvduate the results, we examined the p(.'rformanee of the similarity weight alone, and the overall 1)erformance of the system.
Similarity Weight Evaluation m e t h o d s The main evaluation i)rocedure was carried out with resl)ect to a manual assessment of tim list of terms l)y 2 domain exI)erts.
There are, however, 1)roblems associated with such an evaluation.
Firstly, there ix no gold standm:d of evaluation, and secondly, manual evaluation is both fallil)le and sul)jective.
To avoid this 1)rol)lem, we measure the 1)erformance of the system ill relative termn rather than in absolute terms, by measuring the improveln(mt over the results of tile NC-Value as eomt)ared with mmmal evahlation.
Although we could have used the list of terms 1)rovided in the UMLS, instead of a manu~ ally evahlated list, we found that there was a huge One of the 1)roblems with our method of calculating similarity is that it relies on a 1)re-existing lexi(:al resource, which Eneans it is 1)rone to errors and omissions.
Bearing in mind its innate inadequacies, we can nevertheless evaluate the expected theoretical performance of tilt measure by concerning ourselves only with what is covered by the thesaurus.
This means that we assume COml)leteness (although we know that this in not the case) and evahtate it accordingly, ignoring anything which may be inissing.
The semantic weight ix based on the premise that tile more similar a context term is to the candidate term it occurs with, the better an indicator that context term is.
So the higher the total semantic weight Section top set middle set b o t t o m set Table 3: Semantic weights of terms and non-terms for the candidate term, the higher the ranking of the term and the better the chance that the candidate term is a valid one.
To test the performmme of the semantic weight, we sorted the terms in descending order of their semantic weights and divided the list into 3, such that the top third contained the terms with the highest semantic weights, and the b o t t o m third contained those with the lowest.
We then compared how m a n y valid and non-valid terms (according to the manual evaluation) were contained in each section of the list,.
Tile results, depicted in Table 3, can be interpreted as follows.
In the top third of the list;, 76% were terms and 24% were non-terms, whilst in the middle third, 56% were terms and 44% were non-terms, and so on.
This means that most of the valid terms are contained in the top third of tile list mid the fewest valid terms are contained in the bottom third of the list.
Also, the proportion of terms to non-terms in tile top of tile list is such that there are more terms than non-terms, whereas in the b o t t o m of the list; there are more non-terms than ternis.
This therefore demonstrates two things:  more of' the terms with the highest semantic weights are valid, and fewer of those with the lowest semmitic weights are valid;  more valid terms have high semantic weights than non-terms, mid more non-terms have lower semantic weights than valid terms.
We also tested the similarity measure to see whether adding sosne statistical information would improve its results, and regulate any discrepancies in tile uniformity of the hierarchy.
The methods which intuitively seem most plausible are based on information content, e.g.(Resnik, 1995; Smeaton and Quigley, 1996).
The informatiosl content of a node is related to its probability of occurrence in the corpus.
Tile snore fi'equently it appears, the snore likely it is to be important in terms of conveying information, and therefore the higher weighting it should receive.
We performed experiments to cosnpare two such methods with our similarity measure.
The first considers the probability of the MSCA of the two terms (the lowest node which is an ancestor of both), whilst the second considers the probability of the nodes of the terms being colnpared.
However, the tindings showed a negligible difference between the three methods, so we conchlde that there is no Table 4: Precision of SNC-Vahle and NC-Value advantage to be gained by adding statistical int'ormation, fbr this particular corpus.
It; is possible that with a larger corlms or different hierarchy, this might slot be the case.
Overall E v a l u a t i o n of t h e S N C V a l u e We first; compare the precision rates for the SNCValue and the NC-Value (Table 4), by dividing tile ranked lists into 10 equal sections.
Each section contains 250 terms, marked as valid or invalid by the manual experts.
In the top section, the precision is higher for the SNC-Value, and in the b o t t o m section, it is lower.
This indicates that the precision span is greater fl~r the SNC-Value, and therefore that the ranking is improved.
The distribution of valid terms is also better for the SNC-Value, since of the valid terms, more appear at the top of the list than at the bottom.
Looking at Figure 2, we can see that the SNCValue graph is smoother than that of the NC-Vahle.
We can compare the graphs niore accurately using a method we call comparative upward trend.
Becruise there is no one ideal graph, we instead measure how much each graph deviates from a monotonic line downwards.
This is calculated by dividing the total rise in precision percentage by the length of the graph.
A graph with a lower upward trend will therefore be better than a graph with a higher upward trend.
If we compare the upward trends of the two graphs, we find that the trend for the SNCValue is 0.9, whereas the trend for the NC-Value is 2.7.
This again shows that the SNC-Value rmiking is better thmi the NC-Value ranking, since it is more consistent.
Table 5 shows a more precise investigation of the top portion of the list, (where it is to be expected that ternis are most likely to be wflid, and which is therefore the inost imi)ortant part of the list) We see that the precision is most iml)roved here, both in terms of accuracy and in terms of distribution of weights.
At the I)ottom of the top section, the PlccJshm T~ T T I Scctionollist tics for creating such a thesaurus automatically, or entrancing an existing one, using the contextual information we acquire (Ushioda, 1996; MaynaM and Anmfiadou, 1999b).
There is much scope tbr filrther extensions of this research.
Firstly, it; could be extended to other (lomains and larger corpora, in order to see the true benefit of such a.n apl)roach.
Secondly, the thesaurus could be tailored to the corpus, as we have mentioncd.
An incremental approach might be possible, whereby the similarity measure is combined with statistical intbrmation to tune an existing ontology.
Also, the UMLS is not designed as a linguistic resource, but as an information resource.
Some kind of integration of the two types of resource would be usefifl so that, for example, lexical variation could be more easily handled.
Table 5: Precision of SNC-\Sdue and NC-Vahm for top 250 terms precision is much higher for the SNC-Value.
This is important because ideally, all the terms in this part of the list should be valid, 7 Conclusions In this paper, we have described a method for multiword term extraction which improves on traditional statistical at)proaches by incorporating more specific contextual information.
It focuses particularly on measuring the strength of association (in semantic terms) l)etween a candidate term and its context.
Evahlation shows imi)rovement over the NC-Vahm approach, although the percentages are small.
This is largely l)ecmlse we have used a very small corpus for testing.
The contextuM information acquired can also be used for a mmlber of other related tasks, such as disambiguation and clustering.
C1D2D8CTD6D0CTCPDACTCS D7CTD1CPD2D8CXCR CXD2D8CTD6D4D6CTD8CPD8CXD3D2 CXD2 CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS D4CPD6D7CXD2CV A3 CFCXD0D0CXCPD1 CBCRCWD9D0CTD6 BVD3D1D4D9D8CTD6 CPD2CS C1D2CUD3D6D1CPD8CXD3D2 CBCRCXCTD2CRCT BWCTD4D8BA CDD2CXDACTD6D7CXD8DD D3CU C8CTD2D2D7DDD0DACPD2CXCP C8CWCXD0CPCSCTD0D4CWCXCPB8 C8BT BDBLBDBCBF D7CRCWD9D0CTD6BSD0CXD2CRBACRCXD7BAD9D4CTD2D2BACTCSD9 BTCQD7D8D6CPCRD8 CCCWCXD7 D4CPD4CTD6 CTDCD8CTD2CSD7 CP D4D3D0DDD2D3D1CXCPD0B9D8CXD1CT D4CPD6D7CXD2CV CPD0B9 CVD3D6CXD8CWD1 D8CWCPD8 D6CTD7D3D0DACTD7 D7D8D6D9CRD8D9D6CPD0 CPD1CQCXCVD9CXD8DD CXD2 CXD2D4D9D8 D7CTD2D8CTD2CRCTD7 CQDD CRCPD0CRD9D0CPD8CXD2CV CPD2CS CRD3D1D4CPD6CXD2CV D8CWCT CSCTD2D3B9 D8CPD8CXD3D2D7 D3CU D6CXDACPD0 CRD3D2D7D8CXD8D9CTD2D8D7B8 CVCXDACTD2 D7D3D1CT D1D3CSCTD0 D3CU D8CWCT CPD4D4D0CXCRCPD8CXD3D2 CTD2DACXD6D3D2D1CTD2D8 B4CBCRCWD9D0CTD6B8 BEBCBCBDB5BA CCCWCT CPD0CVD3D6CXD8CWD1 CXD7 CTDCD8CTD2CSCTCS D8D3 CXD2CRD3D6D4D3D6CPD8CT CP CUD9D0D0 D7CTD8 D3CU D0D3CVCXCRCPD0 D3D4CTD6CPD8D3D6D7B8 CXD2CRD0D9CSCXD2CV D5D9CPD2D8CXACCTD6D7 CPD2CS CRD3D2CYD9D2CRB9 D8CXD3D2D7B8 CXD2D8D3 D8CWCXD7 CRCPD0CRD9D0CPD8CXD3D2 DBCXD8CWD3D9D8 CXD2CRD6CTCPD7CXD2CV D8CWCT CRD3D1D4D0CTDCCXD8DD D3CU D8CWCT D3DACTD6CPD0D0 CPD0CVD3D6CXD8CWD1 CQCTDDD3D2CS D4D3D0DDD2D3B9 D1CXCPD0 D8CXD1CTB8 CQD3D8CW CXD2 D8CTD6D1D7 D3CU D8CWCT D0CTD2CVD8CW D3CU D8CWCT CXD2B9 D4D9D8 CPD2CS D8CWCT D2D9D1CQCTD6 D3CU CTD2D8CXD8CXCTD7 CXD2 D8CWCT CTD2DACXD6D3D2D1CTD2D8 D1D3CSCTD0BA BD C1D2D8D6D3CSD9CRD8CXD3D2 CCCWCT CSCTDACTD0D3D4D1CTD2D8 D3CU D7D4CTCPCZCTD6B9CXD2CSCTD4CTD2CSCTD2D8 D1CXDCCTCSB9 CXD2CXD8CXCPD8CXDACT D7D4CTCTCRCWCXD2D8CTD6CUCPCRCTD7B8 CXD2 DBCWCXCRCW D9D7CTD6D7 D2D3D8 D3D2D0DD CPD2D7DBCTD6 D5D9CTD7D8CXD3D2D7 CQD9D8 CPD0D7D3 CPD7CZ D5D9CTD7D8CXD3D2D7 CPD2CS CVCXDACT CXD2B9 D7D8D6D9CRD8CXD3D2D7B8 CXD7 CRD9D6D6CTD2D8D0DD D0CXD1CXD8CTCS CQDD D8CWCT CXD2CPCSCTD5D9CPCRDD D3CU CTDCCXD7D8CXD2CV CRD3D6D4D9D7B9CQCPD7CTCS CSCXD7CPD1CQCXCVD9CPD8CXD3D2 D8CTCRCWD2CXD5D9CTD7BA CCCWCXD7 D4CPD4CTD6 CTDCD4D0D3D6CTD7 D8CWCT D9D7CT D3CU D7CTD1CPD2D8CXCR CPD2CS D4D6CPCVB9 D1CPD8CXCR CXD2CUD3D6D1CPD8CXD3D2B8 CXD2 D8CWCT CUD3D6D1 D3CU D8CWCT CTD2D8CXD8CXCTD7 CPD2CS D6CTD0CPD8CXD3D2D7 CXD2 D8CWCT CXD2D8CTD6CUCPCRCTCS CPD4D4D0CXCRCPD8CXD3D2B3D7 D6D9D2B9D8CXD1CT CTD2B9 DACXD6D3D2D1CTD2D8B8 CPD7 CPD2 CPCSCSCXD8CXD3D2CPD0 D7D3D9D6CRCT D3CU CXD2CUD3D6D1CPD8CXD3D2 D8D3 CVD9CXCSCT CSCXD7CPD1CQCXCVD9CPD8CXD3D2BA C1D2 D4CPD6D8CXCRD9D0CPD6B8 D8CWCXD7 D4CPD4CTD6 CTDCD8CTD2CSD7 CPD2 CTDCCXD7D8CXD2CV D4CPD6D7B9 CXD2CV CPD0CVD3D6CXD8CWD1 D8CWCPD8 CRCPD0CRD9D0CPD8CTD7 CPD2CS CRD3D1D4CPD6CTD7 D8CWCT CSCTB9 D2D3D8CPD8CXD3D2D7 D3CU D6CXDACPD0 D4CPD6D7CT D8D6CTCT CRD3D2D7D8CXD8D9CTD2D8D7 CXD2 D3D6CSCTD6 D8D3 D6CTD7D3D0DACT D7D8D6D9CRD8D9D6CPD0 CPD1CQCXCVD9CXD8DD CXD2 CXD2D4D9D8 D7CTD2D8CTD2CRCTD7 B4CBCRCWD9D0CTD6B8 BEBCBCBDB5BA CCCWCT CPD0CVD3D6CXD8CWD1 CXD7 CTDCD8CTD2CSCTCS D8D3 CXD2CRD3D6B9 D4D3D6CPD8CT CP CUD9D0D0 D7CTD8 D3CU D0D3CVCXCRCPD0 D3D4CTD6CPD8D3D6D7 CXD2D8D3 D8CWCXD7 CRCPD0CRD9B9 D0CPD8CXD3D2 D7D3 CPD7 D8D3 CXD1D4D6D3DACT D8CWCT CPCRCRD9D6CPCRDD D3CU D8CWCT D6CTD7D9D0D8CXD2CV CSCTD2D3D8CPD8CXD3D2D7 DF CPD2CS D8CWCTD6CTCQDD CXD1D4D6D3DACT D8CWCT CPCRCRD9D6CPCRDD D3CU D4CPD6D7CXD2CV DF DBCXD8CWD3D9D8 CXD2CRD6CTCPD7CXD2CV D8CWCT CRD3D1D4D0CTDCCXD8DD D3CU D8CWCT D3DACTD6CPD0D0 CPD0CVD3D6CXD8CWD1 CQCTDDD3D2CS D4D3D0DDD2D3D1CXCPD0 D8CXD1CT B4CQD3D8CW CXD2 D8CTD6D1D7 D3CU D8CWCT D0CTD2CVD8CW D3CU D8CWCT CXD2D4D9D8 CPD2CS D8CWCT D2D9D1CQCTD6 D3CU CTD2D8CXD8CXCTD7 CXD2 D8CWCT CTD2DACXD6D3D2D1CTD2D8 D1D3CSCTD0B5BA CCCWCXD7 D4CPD6D7CXD1D3D2DD CXD7 CPCRCWCXCTDACTCS CQDD D0D3CRCPD0CXDECXD2CV CRCTD6D8CPCXD2 CZCXD2CSD7 D3CU D7CTD1CPD2D8CXCR D6CTD0CPD8CXD3D2D7 CSD9D6CXD2CV D4CPD6D7CXD2CVB8 D4CPD6D8CXCRD9D0CPD6D0DD D8CWD3D7CT CQCTD8DBCTCTD2 D5D9CPD2D8CXACCTD6D7 CPD2CS D8CWCTCXD6 D6CTD7D8D6CXCRD8D3D6 CPD2CS CQD3CSDD CPD6CVD9D1CTD2D8D7 A3 CCCWCT CPD9D8CWD3D6 DBD3D9D0CS D0CXCZCT D8D3 D8CWCPD2CZ BWCPDACXCS BVCWCXCPD2CVB8 C3CPD6CXD2 C3CXD4B9 D4CTD6B8 CPD2CS BTD0CTDCCPD2CSCTD6 C3D3D0D0CTD6B8 CPD7 DBCTD0D0 CPD7 D8CWCT CPD2D3D2DDD1D3D9D7 D6CTDACXCTDBCTD6D7 CUD3D6 CRD3D1D1CTD2D8D7 D3D2 D8CWCXD7 D1CPD8CTD6CXCPD0BA CCCWCXD7 DBD3D6CZ DBCPD7 D4CPD6D8CXCPD0D0DD D7D9D4B9 D4D3D6D8CTCS CQDD C6CBBY C1C1CBB9BLBLBCBCBEBLBJ CPD2CS BWBTCAC8BT C6BIBIBCBCBDB9BCBCB9BDB9BKBLBDBHBA B4D7CXD1CXD0CPD6 D8D3 D8CWCT DBCPDD CSCTD4CTD2CSCTD2CRCXCTD7 CQCTD8DBCTCTD2 D4D6CTCSCXCRCPD8CT CPD2CS CPD6CVD9D1CTD2D8 CWCTCPCS DBD3D6CSD7 CPD6CT D0D3CRCPD0CXDECTCS CXD2 D0CTDCCXCRCPD0CXDECTCS CUD3D6D1CPD0CXD7D1D7 D7D9CRCW CPD7 D8D6CTCT CPCSCYD3CXD2CXD2CV CVD6CPD1D1CPD6D7B5B8 CXD2 D3D6B9 CSCTD6 D8D3 CPDAD3CXCS CRCPD0CRD9D0CPD8CXD2CV CTDCD4D3D2CTD2D8CXCPD0 CWCXCVCWCTD6B9D3D6CSCTD6 CSCTB9 D2D3D8CPD8CXD3D2D7 CUD3D6 CTDCD4D6CTD7D7CXD3D2D7 D0CXCZCT CVCTD2CTD6CPD0CXDECTCS D5D9CPD2D8CXACCTD6D7BA BE BUCPD7CXCR CPD0CVD3D6CXD8CWD1 CCCWCXD7 D7CTCRD8CXD3D2 CSCTD7CRD6CXCQCTD7 D8CWCT CQCPD7CXCR CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS D4CPD6D7CTD6 B4CBCRCWD9D0CTD6B8 BEBCBCBDB5 DBCWCXCRCW DBCXD0D0 CQCT CTDCD8CTD2CSCTCS CXD2 CBCTCRB9 D8CXD3D2 BFBA BUCTCRCPD9D7CT CXD8 DBCXD0D0 CRD6D9CRCXCPD0D0DD D6CTD0DD D3D2 D8CWCT CSCTD2D3D8CPB9 D8CXD3D2D7 B4D3D6 CXD2D8CTD6D4D6CTD8CPD8CXD3D2D7B5 D3CU D4D6D3D4D3D7CTCS CRD3D2D7D8CXD8D9CTD2D8D7 CXD2 D3D6CSCTD6 D8D3 CVD9CXCSCT CSCXD7CPD1CQCXCVD9CPD8CXD3D2B8 D8CWCT D4CPD6D7CTD6 DBCXD0D0 CQCT CSCTACD2CTCS D3D2 CRCPD8CTCVD3D6CXCPD0 CVD6CPD1D1CPD6D7 B4BTCYCSD9CZCXCTDBCXCRDEB8 BDBLBFBHBN BUCPD6B9C0CXD0D0CTD0B8 BDBLBHBFB5B8 DBCWD3D7CT CRCPD8CTCVD3D6CXCTD7 CPD0D0 CWCPDACTDBCTD0D0 CSCTB9 ACD2CTCS D8DDD4CTD7 CPD2CS DBD3D6D7D8B9CRCPD7CT CSCTD2D3D8CPD8CXD3D2D7BA CCCWCTD7CT CRCPD8B9 CTCVD3D6CXCTD7 CPD6CT CSD6CPDBD2 CUD6D3D1 CP D1CXD2CXD1CPD0 D7CTD8 D3CU D7DDD1CQD3D0D7 BV D7D9CRCW D8CWCPD8BM C6C8 BEBVCPD2CS CB BEBVBN CXCU ADBN BEBVD8CWCTD2 ADBP BEBVCPD2CS ADD2 BEBVBM C1D2D8D9CXD8CXDACTD0DDB8 D8CWCT CRCPD8CTCVD3D6DD C6C8 CSCTD7CRD6CXCQCTD7 CP D2D3D9D2 D4CWD6CPD7CT CPD2CS D8CWCT CRCPD8CTCVD3D6DD CB CSCTD7CRD6CXCQCTD7 CP D7CTD2D8CTD2CRCTB8 CPD2CS D8CWCT CRD3D1D4D0CTDC CRCPD8CTCVD3D6CXCTD7 ADBPCPD2CS ADD2 CSCTD7CRD6CXCQCT COCP AD D0CPCRCZCXD2CV CP  D8D3 D8CWCT D6CXCVCWD8B3 CPD2CS COCP AD D0CPCRCZCXD2CV CP  D8D3 D8CWCT D0CTCUD8B3 D6CTD7D4CTCRD8CXDACTD0DDBN D7D3 CUD3D6 CTDCCPD1D4D0CT CBD2C6C8 DBD3D9D0CS CSCTD7CRD6CXCQCT CP CSCTCRD0CPD6CPD8CXDACTDACTD6CQ D4CWD6CPD7CT D0CPCRCZCXD2CV CPD2 C6C8 D7D9CQCYCTCRD8 D8D3 CXD8D7 D0CTCUD8 CXD2 D8CWCT CXD2D4D9D8BA CCCWCT D8DDD4CT CC CPD2CS DBD3D6D7D8B9CRCPD7CT B4D1D3D7D8 CVCTD2CTD6CPD0B5 CSCTD2D3D8CPB9 D8CXD3D2 CF D3CU CTCPCRCW D4D3D7D7CXCQD0CT CRCPD8CTCVD3D6DD CPD6CT CSCTACD2CTCS CQCTD0D3DBB8 CVCXDACTD2 CP D7CTD8 D3CU CTD2D8CXD8CXCTD7 BX CPD7 CPD2 CTD2DACXD6D3D2D1CTD2D8BM CCB4CBB5 BP D8 BM D8D6D9D8CW DACPD0D9CT CFB4CBB5 BP CUCCCACDBXBNBYBTC4CBBXCV CCB4C6C8B5BPCT BMCTD2D8CXD8DD CFB4C6C8B5 BP BX CCB4ADBPB5BPCWCCB4B5BNCCB4ADB5CX CFB4ADBPB5BPCFB4B5 A2CFB4ADB5 CCB4ADD2B5BPCWCCB4B5BNCCB4ADB5CX CFB4ADD2B5BPCFB4B5 A2CFB4ADB5 CCCWCT CSCTD2D3D8CPD8CXD3D2 BW D3CU CPD2DD D4D6D3D4D3D7CTCS CRD3D2D7D8CXD8D9CTD2D8 CXD7 CRD3D2D7D8D6CPCXD2CTCS D8D3 CQCT CP D7D9CQD7CTD8 D3CU D8CWCT DBD3D6D7D8B9CRCPD7CT CSCTD2D3B9 D8CPD8CXD3D2 CF D3CU D8CWCT CRD3D2D7D8CXD8D9CTD2D8B3D7 CRCPD8CTCVD3D6DDBN D7D3 CP CRD3D2B9 D7D8CXD8D9CTD2D8 D3CU CRCPD8CTCVD3D6DD C6C8 DBD3D9D0CS CSCTD2D3D8CT CP D7CTD8 D3CU CTD2B9 D8CXD8CXCTD7B8 CUCT BD BNCT BE BNBMBMBMCVB8 CPD2CS CP CRD3D2D7D8CXD8D9CTD2D8 D3CU CRCPD8CTCVD3D6DD CBD2C6C8 DBD3D9D0CS CSCTD2D3D8CT CP D7CTD8 D3CU CTD2D8CXD8DD A2 D8D6D9D8CW DACPD0D9CT D4CPCXD6D7B8 CUCWCT BD BNCCCACDBXCXBNCWCT BE BNBYBTC4CBBXCXBNBMBMBMCVBA C6D3D8CT D8CWCPD8 D2D3 CSCTD2D3D8CPD8CXD3D2 D3CU CP CRD3D2D7D8CXD8D9CTD2D8 CRCPD2 CRD3D2D8CPCXD2 D1D3D6CT D8CWCPD2 C7B4CYBXCY DA B5 CSCXABCTD6CTD2D8 CTD0CTD1CTD2D8D7B8 DBCWCTD6CT DA CXD7 CP DACPD0CTD2CRDD D1CTCPB9 D7D9D6CT D3CU D8CWCT D2D9D1CQCTD6D3CUC6C8D7DDD1CQD3D0D7 D3CRCRD9D6D6CXD2CV DBCXD8CWCXD2 D8CWCT CRD3D2D7D8CXD8D9CTD2D8B3D7 CRCPD8CTCVD3D6DDBA CCCWCXD7 D4CPD4CTD6 DBCXD0D0 D9D7CT D8CWCT CUD3D0D0D3DBCXD2CV CSCTACD2CXD8CXD3D2 D3CU CP CRCPD8CTCVD3D6CXCPD0 CVD6CPD1D1CPD6 B4BVBZB5BM BWCTACD2CXD8CXD3D2 BT CRCPD8CTCVD3D6CXCPD0 CVD6CPD1D1CPD6 BZ CXD7 CP CUD3D6D1CPD0 CVD6CPD1D1CPD6 B4C6BNA6BNC8B5 D7D9CRCW D8CWCPD8BM AF A6 CXD7 CP ACD2CXD8CT D7CTD8 D3CU DBD3D6CSD7 DBBN AF C8 CXD7 CP ACD2CXD8CT D7CTD8 D3CU D4D6D3CSD9CRD8CXD3D2D7 CRD3D2D8CPCXD2CXD2CVBM AD AX DB CUD3D6 CPD0D0 DBBEA6B8 DBCXD8CW AD BEBVB8 AD AX ADBP  CUD3D6 CTDACTD6DD D6D9D0CT ADBP AX BMBMBM CXD2 C8B8 AD AX  ADD2 CUD3D6 CTDACTD6DD D6D9D0CT ADD2 AX BMBMBM CXD2 C8B8 CPD2CS D2D3D8CWCXD2CV CTD0D7CTBN AF C6 CXD7 D8CWCT D2D3D2D8CTD6D1CXD2CPD0 D7CTD8 CUAD CY AD AX BMBMBM BE C8CVBA CPD2CS D8CWCT CUD3D0D0D3DBCXD2CV CSCTCSD9CRD8CXDACT D4CPD6D7CTD6B8 BD DBCWCXCRCW DBCXD0D0 CQCT CTDCD8CTD2CSCTCS D0CPD8CTD6 D8D3 CWCPD2CSD0CT CP D6CXCRCWCTD6 D7CTD8 D3CU D7CTD1CPD2D8CXCR D3D4B9 CTD6CPD8CXD3D2D7BA CCCWCT D4CPD6D7CTD6 CXD7 CSCTACD2CTCS DBCXD8CWBM AF CRD3D2D7D8CXD8D9CTD2D8CRCWCPD6D8 CXD8CTD1D7 CJCXBNCYBNADCL CSD6CPDBD2 CUD6D3D1 C1 D2 BC A2 C1 D2 BC A2C6B8 CXD2CSCXCRCPD8CXD2CV D8CWCPD8 D4D3D7CXD8CXD3D2D7 CX D8CWD6D3D9CVCW CY CXD2 D8CWCT CXD2D4D9D8 CRCPD2 CQCT CRCWCPD6CPCRD8CTD6CXDECTCS CQDD CRCPD8CTCVD3D6DD ADBN AF CP D0CTDCCXCRCPD0 CXD8CTD1 CJCXBNCYBNADCL CUD3D6 CTDACTD6DD D6D9D0CT AD AX DB BE C8 CXCU DB D3CRCRD9D6D7 CQCTD8DBCTCTD2 D4D3D7CXD8CXD3D2D7 CX CPD2CS CY CXD2 D8CWCT CXD2D4D9D8BN AF CP D7CTD8 D3CU D6D9D0CTD7 D3CU D8CWCT CUD3D6D1BM CJCXBNCZBNADBPCLCJCZBNCYBNCL CJCXBNCYBNADCL CUD3D6 CPD0D0 AD AX ADBP  BE C8BN CXBNCYBNCZ BE C1 D2 BC B8 CJCZBNCYBNADD2CLCJCXBNCZBNCL CJCXBNCYBNADCL CUD3D6 CPD0D0 AD AX  ADD2 BE C8BN CXBNCYBNCZ BE C1 D2 BC BA CPD2CS CRCPD2 D6CTCRD3CVD2CXDECT CPD2 D2B9D0CTD2CVD8CW CXD2D4D9D8 CPD7 CP CRD3D2D7D8CXD8D9CTD2D8 D3CU CRCPD8CTCVD3D6DD AD B4CUD3D6 CTDCCPD1D4D0CTB8 CPD7 CPD2 CBB5 CXCU CXD8 CRCPD2 CSCTCSD9CRCT D8CWCT CRCWCPD6D8 CXD8CTD1 CJBCBND2BNADCLBA CCCWCXD7 D4CPD6D7CTD6 CRCPD2 CQCT CXD1D4D0CTD1CTD2D8CTCS CXD2 CP CSDDD2CPD1CXCR D4D6D3B9 CVD6CPD1D1CXD2CV CPD0CVD3D6CXD8CWD1B8 D9D7CXD2CV D8CWCT D6CTCRD9D6D7CXDACT CUD9D2CRD8CXD3D2BM BYB4DCB5BP CN CP BD BMBMBMCP CZ D7BMD8BM CP BD BMBMBMCP CZ DC CZ CM CXBPBD BYB4CP CX B5 B4DBCWCTD6CT DCBNCP BD BMBMBMCP CZ CPD6CT D4D6D3D4D3D7CTCS CRD3D2D7D8CXD8D9CTD2D8D7 CSD6CPDBD2 CUD6D3D1 C1 D2 BC A2C1 D2 BC A2C6B8 CF BN BPBYBTC4CBBXB8 CPD2CS CE BN BPCCCACDBXB5B8 CQDD D6CTCRD3D6CSCXD2CV D8CWCT D6CTD7D9D0D8 D3CU CTDACTD6DD D6CTCRD9D6D7CXDACT D7D9CQB9CRCPD0D0 D8D3 BYB4DCB5CXD2CPCRCWCPD6D8B8 D8CWCTD2 CRD3D2D7D9D0D8CXD2CV D8CWCXD7 CRCWCPD6D8 D3D2 D7D9CQB9 D7CTD5D9CTD2D8 CRCPD0D0D7 D8D3 BYB4DCB5 CUD3D6 D8CWCT D7CPD1CT DC CRD3D2D7D8CXD8D9CTD2D8BA BE CBCXD2CRCT D8CWCT CXD2CSCXCRCTD7 CXD2 CTDACTD6DD D6D9D0CTB3D7 CPD2D8CTCRCTCSCTD2D8 CRD3D2B9 D7D8CXD8D9CTD2D8D7 CP BD BMBMBMCP CZ CTCPCRCW CRD3DACTD6 D7D1CPD0D0CTD6 D7D4CPD2D7 D8CWCPD2 D8CWD3D7CT CXD2 D8CWCT CRD3D2D7CTD5D9CTD2D8 DCB8 D8CWCT CPD0CVD3D6CXD8CWD1 DBCXD0D0 D2D3D8 CTD2D8CTD6 CXD2D8D3 CPD2 CXD2ACD2CXD8CT D6CTCRD9D6D7CXD3D2BN CPD2CS D7CXD2CRCT D8CWCTD6CT CPD6CT D3D2D0DD D2 BE CYC6CY CSCXABCTD6CTD2D8DACPD0D9CTD7 D3CU DCB8 CPD2CS D3D2D0DD BED2 CSCXABCTD6B9 CTD2D8 D6D9D0CTD7 D8CWCPD8 CRD3D9D0CS D4D6D3DACTCPD2DDCRD3D2D7CTD5D9CTD2D8 DC B4D8DBD3D6D9D0CT CUD3D6D1D7 CUD3D6 BP CPD2CS D2B8 CTCPCRCW DBCXD8CW D2 CSCXABCTD6CTD2D8DACPD0D9CTD7 D3CU CZB5B8 D8CWCT CPD0CVD3D6CXD8CWD1 D6D9D2D7 CXD2 D4D3D0DDD2D3D1CXCPD0 D8CXD1CTBM C7B4D2 BF CYC6CYB5BA CCCWCT D6CTD7D9D0D8CXD2CV CRCWCPD6D8 CRCPD2 D8CWCTD2 CQCT CPD2D2D3D8CPD8CTCS DBCXD8CW CQCPCRCZ D4D3CXD2D8CTD6D7 D8D3 D4D6D3CSD9CRCT CP D4D3D0DDD2D3D1CXCPD0B9D7CXDECTCS D7CWCPD6CTCS CUD3D6CTD7D8 BD BYD3D0D0D3DBCXD2CV CBCWCXCTCQCTD6 CTD8 CPD0BA B4BDBLBLBHB5BA BE BYD3D0D0D3DBCXD2CV BZD3D3CSD1CPD2 B4BDBLBLBLB5BA D6CTD4D6CTD7CTD2D8CPD8CXD3D2 D3CU CPD0D0 D4D3D7D7CXCQD0CT CVD6CPD1D1CPD8CXCRCPD0 D8D6CTCTD7 B4BUCXD0B9 D0D3D8 CPD2CS C4CPD2CVB8 BDBLBKBLB5BA CCD6CPCSCXD8CXD3D2CPD0 CRD3D6D4D9D7B9CQCPD7CTCS D4CPD6D7CTD6D7 D7CTD0CTCRD8 D4D6CTCUCTD6D6CTCS D8D6CTCTD7 CUD6D3D1 D7D9CRCW CUD3D6CTD7D8D7 CQDD CRCPD0CRD9D0CPD8CXD2CV CECXD8CTD6CQCX D7CRD3D6CTD7 CUD3D6 CTCPCRCW D4D6D3D4D3D7CTCS CRD3D2D7D8CXD8D9CTD2D8B8 CPCRCRD3D6CSCXD2CV D8D3 D8CWCT D6CTB9 CRD9D6D7CXDACT CUD9D2CRD8CXD3D2BM CB CE B4DCB5BP D1CPDC CP BD BMBMBMCP CZ D7BMD8BM CP BD BMBMBMCP CZ DC AW CZ CH CXBPBD CB CE B4CP CX B5 AX A1C8B4CP BD BMBMBMCP CZ CY DCB5 CCCWCTD7CT D7CRD3D6CTD7 CRCPD2 CQCT CRCPD0CRD9D0CPD8CTCS CXD2 D4D3D0DDD2D3D1CXCPD0 D8CXD1CTB8 D9D7CXD2CV D8CWCT D7CPD1CT CSDDD2CPD1CXCR D4D6D3CVD6CPD1D1CXD2CV CPD0CVD3D6CXD8CWD1 CPD7 D8CWCPD8 CSCTD7CRD6CXCQCTCS CUD3D6 D4CPD6D7CXD2CVBA BT D8D6CTCT CRCPD2 D8CWCTD2 CQCT D7CTB9 D0CTCRD8CTCSB8 CUD6D3D1 D8CWCT D8D3D4 CSD3DBD2B8 CQDD CTDCD4CPD2CSCXD2CV D8CWCT CWCXCVCWCTD7D8B9 D7CRD3D6CXD2CV D6D9D0CT CPD4D4D0CXCRCPD8CXD3D2 CUD3D6 CTCPCRCW CRD3D2D7D8CXD8D9CTD2D8BA CCCWCT CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS D4CPD6D7CTD6 CSCTD7CRD6CXCQCTCS CWCTD6CT D9D7CTD7 CP D7CXD1CXD0CPD6D1CTCRCWCPD2CXD7D1 D8D3 D7CTD0CTCRD8 D4D6CTCUCTD6D6CTCSD8D6CTCTD7B8 CQD9D8 D8CWCT D7CRD3D6CTD7 CPD6CT CQCPD7CTCS D3D2 D8CWCT D4D6CTD7CTD2CRCT D3D6 CPCQD7CTD2CRCT D3CU CTD2D8CXB9 D8CXCTD7 CXD2 D8CWCT CSCTD2D3D8CPD8CXD3D2 B4CXD2D8CTD6D4D6CTD8CPD8CXD3D2B5 D3CU CTCPCRCW D4D6D3B9 D4D3D7CTCS CRD3D2D7D8CXD8D9CTD2D8BM BF CB BW B4DCB5BP D1CPDC CP BD BMBMBMCP CZ D7BMD8BM CP BD BMBMBMCP CZ DC AW CZ CG CXBPBD CB BW B4CP CX B5 AX B7 B4 BD CXCU BWB4DCB5BIBPBN BC D3D8CWCTD6DBCXD7CT DBCWCTD6CT D8CWCT CSCTD2D3D8CPD8CXD3D2 BWB4DCB5 D3CU CP D4D6D3D4D3D7CTCS CRD3D2D7D8CXD8D9CTD2D8 DC CXD7 CRCPD0CRD9D0CPD8CTCS D9D7CXD2CV CPD2D3D8CWCTD6 D6CTCRD9D6D7CXDACT CUD9D2CRD8CXD3D2BM BWB4DCB5BP CJ CP BD BMBMBMCP CZ D7BMD8BM CP BD BMBMBMCP CZ DC AW AP CZ D3D2 CXBPBD BWB4CP CX B5 AX D3D2 B4 CAB4DCB5 CXCU CZ BPBC CUCWCXCV D3D8CWCTD6DBCXD7CT CXD2 DBCWCXCRCW CAB4DCB5 CXD7 CP D0CTDCCXCRCPD0 D6CTD0CPD8CXD3D2 CSCTACD2CTCS CUD3D6 CTCPCRCW CPDCCXD3D1 DC D3CU CRCPD8CTCVD3D6DD AD CTD5D9CPD0 D8D3 D7D3D1CT D7D9CQD7CTD8 D3CU ADB3D7 DBD3D6D7D8B9CRCPD7CT CSCTD2D3D8CPD8CXD3D2 CFB4ADB5B8 CPD7 CSCTACD2CTCS CPCQD3DACTBA BG CCCWCT D3D4CTD6CPD8D3D6 D3D2 CXD7 D2CPD8D9D6CPD0 B4D6CTD0CPD8CXD3D2CPD0B5 CYD3CXD2 D3D2 D8CWCT ACCTD0CSD7 D3CU CXD8D7 D3D4CTD6CPD2CSD7BM BTD3D2BU BP CUCWCT BD BMBMBMCT D1CPDCB4CPBNCQB5 CXCYCWCT BD BMBMBMCT CP CXBEBTBNCWCT BD BMBMBMCT CQ CXBEBUCV DBCWCTD6CT CPBNCQ AL BCBN CPD2CS AP CXD7 CP D4D6D3CYCTCRD8CXD3D2 D8CWCPD8 D6CTD1D3DACTD7 D8CWCT ACD6D7D8 CTD0CTD1CTD2D8 D3CU D8CWCT D6CTD7D9D0D8 B4CRD3D6D6CTD7D4D3D2CSCXD2CV D8CWCT D1D3D7D8 D6CTCRCTD2D8D0DD CSCXD7CRCWCPD6CVCTCS CPD6CVD9D1CTD2D8 D3CU D8CWCT CWCTCPCS D3D6 CUD9D2CRD8D3D6 CRCPD8CTCVD3D6DDB5BM APBT BP CUCWCT BE BMBMBMCT CP CXCYCWCT BD BMBMBMCT CP CXBEBTCV CCCWCXD7 CXD2D8CTD6D0CTCPDACXD2CV D3CU D7CTD1CPD2D8CXCR CTDACPD0D9CPD8CXD3D2 CPD2CS D4CPD6D7B9 CXD2CV CUD3D6 D8CWCT D4D9D6D4D3D7CT D3CU CSCXD7CPD1CQCXCVD9CPD8CXD3D2 CWCPD7 D1D9CRCW CXD2 CRD3D1D1D3D2 DBCXD8CW D8CWCPD8 D3CU BWD3DBCSCXD2CV CTD8 CPD0BA B4BDBLBLBGB5B8 CTDCCRCTD4D8 BF C0CTD6CTB8 D8CWCT D7CRD3D6CT CXD7 D7CXD1D4D0DD CTD5D9CPD0 D8D3 D8CWCT D2D9D1CQCTD6 D3CU D2D3D2B9 CTD1D4D8DD CRD3D2D7D8CXD8D9CTD2D8D7 CXD2 CPD2 CPD2CPD0DDD7CXD7B8 CQD9D8 D3D8CWCTD6 D1CTD8D6CXCRD7 CPD6CT D4D3D7B9 D7CXCQD0CTBA BG CBD3 CP D0CTDCCXCRCPD0 D6CTD0CPD8CXD3D2 CUD3D6 D8CWCT CRD3D2D7D8CXD8D9CTD2D8 COD0CTD1D3D2B3 D3CU CRCPD8CTCVD3D6DD C6C8 DBD3D9D0CS CRD3D2D8CPCXD2 CPD0D0 CPD2CS D3D2D0DD D8CWCT D0CTD1D3D2D7 CXD2 D8CWCT CTD2DACXD6D3D2D1CTD2D8B8 CPD2CS CP D0CTDCCXCRCPD0 D6CTD0CPD8CXD3D2 CUD3D6 D8CWCT CRD3D2B9 D7D8CXD8D9CTD2D8 COCUCPD0D0CXD2CVB3 D3CU CRCPD8CTCVD3D6DD CBD2C6C8 DBD3D9D0CS CRD3D2D8CPCXD2 CP D1CPD4B9 D4CXD2CV CUD6D3D1 CTDACTD6DD CTD2D8CXD8DD CXD2 D8CWCT CTD2DACXD6D3D2D1CTD2D8 D8D3 D7D3D1CT D8D6D9D8CW DACPD0D9CT B4CCCACDBX CXCU D8CWCPD8 CTD2D8CXD8DD CXD7 CUCPD0D0CXD2CVB8 BYBTC4CBBX D3D8CWCTD6DBCXD7CTB5BM CTBACVBA CUCWD0CTD1D3D2 BD BNCCCACDBXCXBNCWD0CTD1D3D2 BE BNBYBTC4CBBXCXBNBMBMBMCVBA C6C8CJD0CTD1D3D2CL CUD0 BD BND0 BE BND0 BF BND0 BG CV C8BMC6C8D2C6C8BBC6C8CJCXD2CL CUCWCQ BD BNCWD0 BD BND0 BD CXCXBNCWD1 BD BNCWD0 BE BND0 BE CXCXCV C6C8CJCQCXD2CL CUCQ BD BNCQ BE CV C8BMC6C8D2C6C8BBC6C8CJCQDDCL CUCWD1 BD BNCWCQ BD BNCQ BD CXCXBNCWD1 BE BNCWCQ BE BNCQ BE CXCXCV C6C8CJD1CPCRCWCXD2CTCL CUD1 BD BND1 BE BND1 BF CV C8C8BMC6C8D2C6C8CJCXD2CL CUCWD0 BD BND0 BD CXCV C8C8BMC6C8D2C6C8CJCQDDCL CUCWCQ BD BNCQ BD CXBNCWCQ BE BNCQ BE CXCV C6C8CJD0CTD1D3D2CL CUD0 BD CV C6C8CJCQCXD2CL CUCQ BD BNCQ BE CV C8C8BMC6C8D2C6C8CJCXD2CL CUCWD0 BD BND0 BD CXCV C6C8CJD0CTD1D3D2CL CUD0 BD CVCJBN BYCXCVD9D6CT BDBM BWCTD2D3D8CPD8CXD3D2B9CPD2D2D3D8CPD8CTCS CUD3D6CTD7D8 CUD3D6 COD0CTD1D3D2 CXD2 CQCXD2 CQDD D1CPCRCWCXD2CTBAB3 D8CWCPD8 CXD2 D8CWCXD7 CRCPD7CTB8 CRD3D2D7D8CXD8D9CTD2D8D7 CPD6CT D2D3D8 D3D2D0DD D7CTD1CPD2B9 D8CXCRCPD0D0DD D8DDD4CTB9CRCWCTCRCZCTCSB8 CQD9D8 CPD6CT CPD0D7D3 CUD9D0D0DD CXD2D8CTD6D4D6CTD8CTCS CTCPCRCW D8CXD1CT D8CWCTDD CPD6CT D4D6D3D4D3D7CTCSBA BYCXCVD9D6CT BD D7CWD3DBD7 CP D7CPD1D4D0CT CSCTD2D3D8CPD8CXD3D2B9CPD2D2D3D8CPD8CTCS CUD3D6CTD7D8 CUD3D6 D8CWCT D4CWD6CPD7CT COD8CWCT D0CTD1D3D2 CXD2 D8CWCT CQCXD2 CQDD D8CWCT D1CPCRCWCXD2CTB3B8 D9D7CXD2CV D8CWCT D0CTDCCXCRCPD0CXDECTCS CVD6CPD1D1CPD6BM D0CTD1D3D2B8 CQCXD2B8 D1CPCRCWCXD2CT BM C6C8 D8CWCTBMC6C8BPC6C8 CXD2B8 CQDDBMC6C8D2C6C8BPC6C8 CXD2 DBCWCXCRCW D8CWCT CSCTD2D3D8CPD8CXD3D2 D3CU CTCPCRCW CRD3D2D7D8CXD8D9CTD2D8 B4D8CWCT D7CTD8 CXD2 CTCPCRCW D6CTCRD8CPD2CVD0CTB5 CXD7 CRCPD0CRD9D0CPD8CTCS D9D7CXD2CV CP CYD3CXD2 D3D2 D8CWCT CSCTD2D3D8CPD8CXD3D2D7D3CU CTCPCRCWD4CPCXD6D3CU CRD3D2D7D8CXD8D9CTD2D8D7D8CWCPD8 CRD3D1CQCXD2CT D8D3 D4D6D3CSD9CRCT CXD8BA C1D2 D8CWCXD7 CTDCCPD1D4D0CTB8 D8CWCT D6CXCVCWD8B9CQD6CPD2CRCWCXD2CV D8D6CTCT DBD3D9D0CS CQCT D4D6CTCUCTD6D6CTCS CQCTCRCPD9D7CT D8CWCT CSCTD2D3D8CPD8CXD3D2 D6CTB9 D7D9D0D8CXD2CV CUD6D3D1 D8CWCT CRD3D1D4D3D7CXD8CXD3D2 CPD8 D8CWCT D6D3D3D8 D3CU D8CWCT D3D8CWCTD6 D8D6CTCT DBD3D9D0CS CQCT CTD1D4D8DDBA CBCXD2CRCT D8CWCXD7 D9D7CT D3CU D8CWCT CYD3CXD2 D3D4CTD6CPD8CXD3D2 CXD7 D0CXD2CTCPD6 D3D2 D8CWCT D7D9D1 D3CU D8CWCT CRCPD6CSCXD2CPD0CXD8CXCTD7 D3CU CXD8D7 D3D4CTD6CPD2CSD7B8 CPD2CS D7CXD2CRCT D8CWCT CSCTD2D3D8CPD8CXD3D2D7 D3CU D8CWCT CRCPD8CTCVD3D6CXCTD7 CXD2 CP CVD6CPD1D1CPD6 BZ CPD6CT CQD3D9D2CSCTCS CXD2 CRCPD6CSCXD2CPD0CXD8DDCQDD C7B4CYBXCY DA B5 DBCWCTD6CT DA CXD7 D8CWCT D1CPDCCXD1D9D1 DACPD0CTD2CRDD D3CU D8CWCT CRCPD8CTCVD3D6CXCTD7 CXD2 BZB8 D8CWCT D8D3D8CPD0 CRD3D1D4D0CTDCCXD8DD D3CU D8CWCT CPCQD3DACT CPD0CVD3D6CXD8CWD1 CRCPD2 CQCT D7CWD3DBD2 D8D3 CQCT C7B4D2 BF CYBXCY DA B5BM D4D3D0DDD2D3D1CXCPD0 D2D3D8 D3D2D0DD D3D2 D8CWCT D0CTD2CVD8CW D3CU D8CWCT CXD2D4D9D8 D2B8 CQD9D8 CPD0D7D3D3D2 D8CWCT D7CXDECT D3CU D8CWCT CTD2DACXD6D3D2D1CTD2D8BX B4CBCRCWD9D0CTD6B8 BEBCBCBDB5BA BF BXDCD8CTD2CSCTCS CPD0CVD3D6CXD8CWD1 CCCWCT CPCQD3DACT CPD0CVD3D6CXD8CWD1 DBD3D6CZD7 DBCTD0D0 CUD3D6 CPD8D8CPCRCWCXD2CV D3D6CSCXB9 D2CPD6DD CRD3D1D4D0CTD1CTD2D8D7 CPD2CS D1D3CSCXACCTD6D7B8 CQD9D8 CPD7 CP D7CTD1CPD2D8CXCR D8CWCTD3D6DD CXD8 CXD7 D2D3D8 D7D9CRCXCTD2D8D0DD CTDCD4D6CTD7D7CXDACTD8D3 D4D6D3CSD9CRCT CRD3D6B9 D6CTCRD8 CSCTD2D3D8CPD8CXD3D2D7CXD2 CPD0D0CRCPD7CTD7BA BYD3D6 CTDCCPD1D4D0CTB8 D8CWCT D0CTDCCXCRCPD0 D6CTD0CPD8CXD3D2D7 CSCTACD2CTCS CPCQD3DACT CPD6CT CXD2D7D9CRCXCTD2D8 D8D3 D6CTD4D6CTD7CTD2D8 D5D9CPD2D8CXACCTD6D7 D0CXCZCT COD2D3B3 B4D9D7CXD2CV CRCPD8CTCVD3D6DD C6C8BPC6C8B5CXD2D8CWCT D4CWD6CPD7CT COD8CWCT CQD3DD DBCXD8CW D2D3 CQCPCRCZD4CPCRCZBAB3 BH BT D7CXD1CXD0CPD6 D4D6D3CQB9 D0CTD1 D3CRCRD9D6D7 DBCXD8CW CRD3D2CYD9D2CRD8CXD3D2D7BN CUD3D6 CTDCCPD1D4D0CTB8 D8CWCT DBD3D6CS COCPD2CSB3 B4D9D7CXD2CV CRCPD8CTCVD3D6DDC6C8D2C6C8BPC6C8B5 CXD2 D8CWCT D4CWD6CPD7CT COD8CWCT CRCWCXD0CS DBCTCPD6CXD2CV CVD0CPD7D7CTD7 CPD2CS CQD0D9CT D4CPD2D8D7B3B8 CPD0D7D3 CRCPD2D2D3D8 CQCT D4D6D3D4CTD6D0DD D6CTD4D6CTD7CTD2D8CTCS CPD7 CP D0CTDCCXCRCPD0 D6CTD0CPD8CXD3D2BA BI CCCWCXD7 D6CPCXD7CTD7 D8CWCT D5D9CTD7D8CXD3D2BM CWD3DB D1D9CRCW CTDCD4D6CTD7D7CXDACXD8DD CRCPD2 CQCT CPD0D0D3DBCTCS CXD2 CP D7CWCPD6CTCS D7CTD1CPD2D8CXCR CXD2D8CTD6D4D6CTD8CPD8CXD3D2 DBCXD8CWD3D9D8 CTDCCRCTCTCSCXD2CV D8CWCT D8D6CPCRD8CPCQD0CT D4CPD6D7CXD2CV CRD3D1D4D0CTDCCXD8DD D2CTCRCTD7B9 D7CPD6DD CUD3D6 D4D6CPCRD8CXCRCPD0 CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS D4CPD6D7CXD2CVBR C1D2 D8D6CPCSCXD8CXD3D2CPD0 CRCPD8CTCVD3D6CXCPD0 D7CTD1CPD2D8CXCRD7 B4C5D3D2D8CPCVD9CTB8 BDBLBJBFBN BUCPD6DBCXD7CT CPD2CS BVD3D3D4CTD6B8 BDBLBKBDBN C3CTCTD2CPD2 CPD2CS CBD8CPDACXB8 BDBLBKBIB5 D5D9CPD2D8CXACCTD6D7 CPD2CS D2D3D9D2 D4CWD6CPD7CT CRD3D2CYD9D2CRD8CXD3D2D7 CSCTB9 D2D3D8CT CWCXCVCWCTD6B9D3D6CSCTD6 D6CTD0CPD8CXD3D2D7BM D8CWCPD8 CXD7B8 D6CTD0CPD8CXD3D2D7 CQCTB9 D8DBCTCTD2 DBCWD3D0CT D7CTD8D7 D3CU CTD2D8CXD8CXCTD7 CXD2D7D8CTCPCS D3CU CYD9D7D8 CQCTB9 D8DBCTCTD2 CXD2CSCXDACXCSD9CPD0D7BA CDD2CSCTD6 D8CWCXD7 CXD2D8CTD6D4D6CTD8CPD8CXD3D2B8 CP D5D9CPD2D8CXACCTD6 D0CXCZCT COD2D3B3 DBD3D9D0CS CSCTD2D3D8CT CP D7CTD8 D3CU D4CPCXD6D7 CUCWBT BD BNBU BD CXBNCWBT BE BNBU BE CXBNBMBMBMCV DBCWCTD6CT CTCPCRCW BT CX CPD2CS BU CX CPD6CT CSCXD7CYD3CXD2D8 D7D9CQD7CTD8D7 D3CU BXB8 CRD3D6D6CTD7D4D3D2CSCXD2CV D8D3 CPD2 CPCRCRCTD4D8B9 CPCQD0CT D4CPCXD6 D3CU D6CTD7D8D6CXCRD8D3D6 CPD2CS CQD3CSDD D7CTD8D7 D7CPD8CXD7CUDDCXD2CV D8CWCT D5D9CPD2D8CXACCTD6 COD2D3B3BA CDD2CUD3D6D8D9D2CPD8CTD0DDB8 D7CXD2CRCT D8CWCT CRCPD6CSCXD2CPD0CXD8CXCTD7 D3CU D8CWCTD7CT CWCXCVCWCTD6B9D3D6CSCTD6 CSCTD2D3D8CPD8CXD3D2D7 CRCPD2 CQCT CTDCD4D3D2CTD2D8CXCPD0 D3D2 D8CWCT D7CXDECT D3CU D8CWCT CTD2DACXD6D3D2D1CTD2D8 BX B4D8CWCTD6CT CPD6CT BE CYBXCY D4D3D7B9 D7CXCQD0CT D7D9CQD7CTD8D7 D3CU BX CPD2CS BE BECYBXCY D4D3D7D7CXCQD0CT CRD3D1CQCXD2CPD8CXD3D2D7 D3CU D8DBD3 D7D9CRCW D7D9CQD7CTD8D7B5B8 D7D9CRCW CPD2 CPD4D4D6D3CPCRCWDBD3D9D0CS CSCTD7D8D6D3DD D8CWCT D4D3D0DDD2D3D1CXCPD0 CRD3D1D4D0CTDCCXD8DD D3CU D8CWCT CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS D4CPD6D7CXD2CV CPD0CVD3D6CXD8CWD1BA BH BTD7D7CXCVD2CXD2CV D8CWCT CXCSCTD2D8CXD8DD D6CTD0CPD8CXD3D2 CUCWCT BD BNCT BD CXBNCWCT BE BNCT BE CXBNBMBMBMCV D8D3 D8CWCT D5D9CPD2D8CXACCTD6 DBD3D9D0CS CXD2CRD3D6D6CTCRD8D0DD DDCXCTD0CS D8CWCT D7CTD8 D3CU CQD3DDD7 DBCXD8CW CP CQCPCRCZD4CPCRCZ CPD7 CP CSCTD2D3D8CPD8CXD3D2 CUD3D6 D8CWCT CUD9D0D0 D2D3D9D2 D4CWD6CPD7CTBN CPD2CS CPD7D7CXCVD2B9 CXD2CV D8CWCT CRD3D2DACTD6D7CT D6CTD0CPD8CXD3D2 B4CUD6D3D1 CTCPCRCWCTD2D8CXD8DD CXD2 D8CWCT CTD2DACXD6D3D2D1CTD2D8 D8D3 CTDACTD6DD D3D8CWCTD6 CTD2D8CXD8DD CUCWCT BD BNCT BE CXBNCWCT BD BNCT BF CXBNBMBMBMCVB5DBD3D9D0CS CXD2CRD3D6D6CTCRD8D0DD DDCXCTD0CS D8CWCT D7CTD8 D3CU CQD3DDD7 DBCXD8CW CPD2DDD8CWCXD2CV D8CWCPD8 CXD7 D2D3D8 CP CQCPCRCZD4CPCRCZBA BI CCCWCT CXCSCTD2D8CXD8DD D6CTD0CPD8CXD3D2 CUCWCT BD BNCT BD BNCT BD CXBNCWCT BE BNCT BE BNCT BE CXBNBMBMBMCVB8 DBCWCXCRCW DDCXCTD0CSD7 CP CRD3D6D6CTCRD8 CXD2D8CTD6D4D6CTD8CPD8CXD3D2 CXD2 DACTD6CQ D4CWD6CPD7CT CRD3D2CYD9D2CRD8CXD3D2B8 DBD3D9D0CS DDCXCTD0CS CPD2 CXD2CRD3D6D6CTCRD8 CSCTD2D3D8CPD8CXD3D2 CUD3D6 D8CWCT D2D3D9D2 D4CWD6CPD7CT COCVD0CPD7D7CTD7 CPD2CS CQD0D9CT D4CPD2D8D7B8B3 CRD3D2D8CPCXD2CXD2CV D3D2D0DD CTD2D8CXD8CXCTD7 DBCWCXCRCW CPD6CT CPD8 D3D2CRCT CQD3D8CW CVD0CPD7D7CTD7 CPD2CS D4CPD2D8D7BA C0D3DBCTDACTD6B8 CXCU D8CWCT D2D9D1CQCTD6 D3CU D4D3D7D7CXCQD0CT CWCXCVCWCTD6B9D3D6CSCTD6 CUD9D2CRD8CXD3D2D7 CXD7 D6CTD7D8D6CXCRD8CTCS D8D3 CP ACD2CXD8CT D7CTD8 B4D7CPDDB8 D8D3 D7D3D1CT D7D9CQD7CTD8 D3CU DBD3D6CSD7 CXD2 CP D0CTDCCXCRD3D2B5B8 CXD8 CQCTCRD3D1CTD7 D8D6CPCRD8CPCQD0CT D8D3 D7D8D3D6CT D8CWCTD1 CQDD D2CPD1CT D6CPD8CWCTD6 D8CWCPD2 CQDD CSCTD2D3D8CPD8CXD3D2 B4CXBACTBA CPD7 D7CTD8D7B5BA CBD9CRCW CUD9D2CRD8CXD3D2 CRCPD2 D8CWCTD2 CSCXD7CRCWCPD6CVCT CPD0D0 D8CWCTCXD6 ACD6D7D8B9D3D6CSCTD6 CPD6CVD9D1CTD2D8D7 CXD2 CP D7CXD2CVD0CT CSCTD6CXDACPD8CXD3D2CPD0 D7D8CTD4 D8D3 D4D6D3CSD9CRCT CP ACD6D7D8B9D3D6CSCTD6 D6CTD7D9D0D8B8 CXD2 D3D6CSCTD6 D8D3 CPDAD3CXCS CVCTD2CTD6CPD8CXD2CV D3D6 CTDACPD0D9CPD8CXD2CV CPD2DD CWCXCVCWCTD6B9D3D6CSCTD6 D4CPD6D8CXCPD0 D6CTB9 D7D9D0D8D7BA CBDDD2D8CPCRD8CXCRCPD0D0DDB8 D8CWCXD7 DBD3D9D0CS CQCT CPD2CPD0D3CVD3D9D7 D8D3 CRD3D1B9 D4D3D7CXD2CV CP D5D9CPD2D8CXACCTD6 DBCXD8CW CQD3D8CW CP D2D3D9D2 D4CWD6CPD7CT D6CTD7D8D6CXCRB9 D8D3D6 CPD2CS CP CQD3CSDD D4D6CTCSCXCRCPD8CT B4CTBACVBA CP DACTD6CQ D3D6 DACTD6CQ D4CWD6CPD7CTB5 CPD8 D8CWCT D7CPD1CT D8CXD1CTB8 D8D3 D4D6D3CSD9CRCT CPD2D3D8CWCTD6 ACD6D7D8B9D3D6CSCTD6 D4D6CTCSCXCRCPD8CT B4CTBACVBA CP DACTD6CQ D4CWD6CPD7CT D3D6 D7CTD2D8CTD2CRCTB5BA CBCXD2CRCT CP CVCTD2CTD6CPD0CXDECTCS D5D9CPD2D8CXACCTD6 CUD9D2CRD8CXD3D2 D1CTD6CTD0DD CRD3D9D2D8D7 CPD2CS CRD3D1D4CPD6CTD7 D8CWCT CRCPD6CSCXD2CPD0CXD8CXCTD7 D3CU CXD8D7 CPD6CVD9D1CTD2D8D7 CXD2 CP D0CXD2B9 CTCPD6 D8CXD1CT D3D4CTD6CPD8CXD3D2B8 D8CWCXD7 CPD2CPD0DDD7CXD7 D4D6D3DACXCSCTD7 CP D8D6CPCRD8CPCQD0CT D7CWD3D6D8CRD9D8 D8D3 D8CWCT CTDCD4D3D2CTD2D8CXCPD0 CRCPD0CRD9D0CPD8CXD3D2D7 D6CTD5D9CXD6CTCS CXD2 D8CWCT CRD3D2DACTD2D8CXD3D2CPD0 CPD2CPD0DDD7CXD7BA C6D3D8CT D8CWCPD8 D8CWCXD7 CPD2CPD0DDD7CXD7 CQDD CXD8D7CTD0CU CSD3CTD7 D2D3D8 CPCSD1CXD8 D4D6D3CSD9CRD8CXDACT D1D3CSCXACCRCPD8CXD3D2 D3CU D5D9CPD2D8CXACCTD6D7 B4CQCTCRCPD9D7CT D8CWCTCXD6 CUD9D2CRD8CXD3D2D7 CPD6CT CSD6CPDBD2 CUD6D3D1 D7D3D1CT ACD2CXD8CT D7CTD8B5 D3D6 D3CU D5D9CPD2B9 D8CXACCTCS D2D3D9D2 D4CWD6CPD7CTD7 B4CQCTCRCPD9D7CT D8CWCTDD CPD6CT D2D3 D0D3D2CVCTD6 CSCTB9 D6CXDACTCS CPD7 CP D4CPD6D8CXCPD0 D6CTD7D9D0D8B5BA CCCWCXD7 CRCPD9D7CTD7 D2D3 CSCXD7D6D9D4D8CXD3D2 D8D3 D8CWCT CPD8D8CPCRCWD1CTD2D8 D3CU D2D3D2B9CRD3D2CYD9D2CRD8CXDACT D1D3CSCXACCTD6D7B8 CQCTB9 CRCPD9D7CT D3D6CSCXD2CPD6DD D7DDD2D8CPCRD8CXCR D1D3CSCXACCTD6D7 D3CU D5D9CPD2D8CXACCTD6 CRD3D2B9 D7D8CXD8D9CTD2D8D7 CPD6CT D7CTD0CSD3D1 D4D6D3CSD9CRD8CXDACT B4CXD2 D8CWCT D7CTD2D7CT D8CWCPD8 D8CWCTCXD6 CRD3D1D4D3D7CXD8CXD3D2 CSD3CTD7 D2D3D8 DDCXCTD0CS CUD9D2CRD8CXD3D2D7 D3D9D8D7CXCSCT D7D3D1CT ACD2CXD8CT D7CTD8B5B8 CPD2CS D7DDD2D8CPCRD8CXCR D1D3CSCXACCTD6D7 D3CU C6C8 CRD3D2B9 D7D8CXD8D9CTD2D8D7 D9D7D9CPD0D0DD D3D2D0DD D1D3CSCXCUDD D8CWCT D6CTD7D8D6CXCRD8D3D6 D7CTD8 D3CU D8CWCT D5D9CPD2D8CXACCTD6 D6CPD8CWCTD6 D8CWCPD2 D8CWCT CTD2D8CXD6CT D5D9CPD2D8CXACCTCS CUD9D2CRD8CXD3D2B8 CPD2CS CRCPD2 D8CWCTD6CTCUD3D6CT D7CPCUCTD0DD CQCT D8CPCZCTD2 D8D3 CPD8D8CPCRCW CQCTD0D3DB D8CWCT D5D9CPD2D8CXACCTD6B8 D8D3 D8CWCT D9D2D5D9CPD2D8CXACCTCS C6C8BA BUD9D8 D8CWCXD7 CXD7 D2D3D8 D8D6D9CT CXD2 CRCPD7CTD7 CXD2DAD3D0DACXD2CV CRD3D2CYD9D2CRB9 D8CXD3D2BA BVD3D2CYD3CXD2CTCS D5D9CPD2D8CXACCTD6D7B8 D0CXCZCT COD7D3D1CT CQD9D8 D2D3D8 CPD0D0B8B3 CRCPD2D2D3D8 CPD0DBCPDDD7CQCT CSCTACD2CTCS D9D7CXD2CV CP D7CXD2CVD0CT D7D8CPD2CSCPD6CS D0CTDCB9 CXCRCPD0 CUD9D2CRD8CXD3D2BN CPD2CS CRD3D2CYD9D2CRD8CXD3D2D7 D3CU D5D9CPD2D8CXACCTCS D2D3D9D2 D4CWD6CPD7CTD7B8 D0CXCZCT COD3D2CT D3D6CPD2CVCT CPD2CS D3D2CT D0CTD1D3D2B3B8 CRCPD2D2D3D8 CQCT CPD4D4D0CXCTCS D8D3 D9D2D5D9CPD2D8CXACCTCS D7D9CQCRD3D2D7D8CXD8D9CTD2D8D7 B4D7DDD2D8CPCRB9 D8CXCRCPD0D0DDB8 CQCTCRCPD9D7CT D8CWCXD7 DBD3D9D0CS CUCPCXD0 D8D3 D7D9CQD7D9D1CT D8CWCT D7CTCRB9 D3D2CS D5D9CPD2D8CXACCTD6B8 CPD2CS D7CTD1CPD2D8CXCRCPD0D0DDB8 CQCTCRCPD9D7CT CXD8 CXD7 D2D3D8 D8CWCT D6CTD7D8D6CXCRD8D3D6 D7CTD8D7 DBCWCXCRCW CPD6CT CRD3D2CYD3CXD2CTCSB5BA C3CTCTD2CPD2 CPD2CS CBD8CPDACX B4BDBLBKBIB5 D1D3CSCTD0 CRD3D2CYD9D2CRD8CXD3D2D7 D3CU D5D9CPD2D8CXACCTD6D7 CPD2CS D5D9CPD2D8CXACCTCS D2D3D9D2 D4CWD6CPD7CTD7 D9D7CXD2CV D0CPD8D8CXCRCT D3D4CTD6CPD8CXD3D2D7 D3D2 CWCXCVCWCTD6B9D3D6CSCTD6 D7CTD8D7B8 CQD9D8 CPD7 D4D6CTDACXD3D9D7D0DD D7D8CPD8CTCSB8 D8CWCTD7CT CWCXCVCWCTD6B9D3D6CSCTD6 D7CTD8D7 D4D6CTCRD0D9CSCT D8D6CPCRD8CPCQD0CT CXD2D8CTD6D0CTCPDACXD2CV D3CU D7CTD1CPD2D8CXCR CXD2D8CTD6D4D6CTD8CPD8CXD3D2 DBCXD8CW D4CPD6D7CXD2CVBA CCCWCT D7D3D0D9D8CXD3D2 D4D6D3D4D3D7CTCS CWCTD6CT CXD7 D8D3 D8D6CTCPD8 CTCPCRCW D5D9CPD2B9 D8CXACCTD6 D3D6 D5D9CPD2D8CXACCTCS D2D3D9D2 D4CWD6CPD7CT CRD3D2CYD9D2CRD8CXD3D2 CPD7 CPD2 CTD0B9 D0CXD4D8CXCRCPD0 CRD3D2CYD9D2CRD8CXD3D2 D3CU D8DBD3 CRD3D1D4D0CTD8CT ACD6D7D8B9D3D6CSCTD6 D4D6CTCSB9 CXCRCPD8CTD7 B4CTBACVBA DACTD6CQ D4CWD6CPD7CTD7 D3D6 D7CTD2D8CTD2CRCTD7B5B8 CTCPCRCW D7D9CQD7D9D1B9 CXD2CV CP CSCXABCTD6CTD2D8 D5D9CPD2D8CXACCTD6 CPD2CS D2D3D9D2 D4CWD6CPD7CT D6CTD7D8D6CXCRD8D3D6 B4CXD2 D8CWCT CRCPD7CT D3CU C6C8 CRD3D2CYD9D2CRD8CXD3D2B5B8 CQD9D8 D7CWCPD6CXD2CV D3D6 CSD9B9 D4D0CXCRCPD8CXD2CV CP CRD3D1D1D3D2 CQD3CSDD D4D6CTCSCXCRCPD8CTBA CCCWCXD7 CPD2CPD0DDD7CXD7 D6CTD5D9CXD6CTD7 D1D9D0D8CXD4D0CT CRD3D1D4D3D2CTD2D8D7 D8D3 CZCTCTD4 D8D6CPCRCZ D3CU D8CWCT CSD9D4D0CXCRCPD8CTCS D1CPD8CTD6CXCPD0 CPCQD3DACT D8CWCT CRD3D2CYD9D2CRD8CXD3D2B8 CQD9D8 CPD7 D0D3D2CV CPD7 D8CWCT D2D9D1CQCTD6 D3CU CRD3D1D4D3D2CTD2D8D7 CXD7 CQD3D9D2CSCTCSB8 D8CWCT D4D3D0DDD2D3D1CXCPD0 CRD3D1D4D0CTDCCXD8DD D3CU D8CWCT D4CPD6D7CXD2CV CPD0CVD3D6CXD8CWD1 CXD7 CRD3D2D8CPCXD2CXD2CV B4CSD9D4D0CXCRCPD8CTCSB5 D3D2CT D3D6CPD2CVCT B4D9D2CSD9D4D0CXCRCPD8CTCSB5 CPD2CS D3D2CT D0CTD1D3D2 B4D9D2CSD9D4D0CXCRCPD8CTCSB5 BYCXCVD9D6CT BEBM BWD9D4D0CXCRCPD8CTCS DACTD6CQ CXD2 C6C8 CRD3D2CYD9D2CRD8CXD3D2BA D6CTD8CPCXD2CTCSBA BJ BYCXCVD9D6CT BE D7CWD3DBD7 CP CSD9D4D0CXCRCPD8CTCS DACTD6CQ D4D6CTCSCXCRCPD8CT CXD2 D8CWCT CSCTD6CXDACPD8CXD3D2 D3CU CPD2 C6C8 CRD3D2CYD9D2CRD8CXD3D2BA CCCWCT CRD3D2CYD3CXD2CTCS CRD3D2D7D8CXD8D9CTD2D8D7 B4D8CWCT D7CWCPCSCTCS D6CTCVCXD3D2D7 CXD2 D8CWCT ACCVD9D6CTB5 CPD6CT CTCPCRCW CRD3D1D4D3D7CTCS D3CU D8DBD3 CRD3D1D4D3D2CTD2D8D7BM D3D2CT CUD3D6 D8CWCT C6C8 CXD8D7CTD0CUB8 CRD3D2D8CPCXD2CXD2CV D8CWCT D5D9CPD2D8CXACCTD6 CPD2CS D8CWCT D6CTD7D8D6CXCRD8D3D6 D4D6CTCSCXCRCPD8CTB8 CPD2CS D3D2CT CUD3D6 D8CWCT DACTD6CQ DBCWCXCRCW D7D9D4D4D0CXCTD7 D8CWCT CQD3CSDD D4D6CTCSCXCRCPD8CT D3CU D8CWCT D5D9CPD2D8CXACCTD6BA CBCXD2CRCT D8CWCT CRD3D2CYD3CXD2CTCS CRD3D2D7D8CXD8D9CTD2D8D7 CQD3D8CW CRD3D6D6CTD7D4D3D2CS D8D3 CRD3D1D4D0CTD8CT D5D9CPD2D8CXB9 ACCTD6 CTDCD4D6CTD7D7CXD3D2D7 DBCXD8CW D2D3 D9D2D7CPD8CXD7ACCTCS ACD6D7D8B9D3D6CSCTD6 CPD6CVD9B9 D1CTD2D8D7B8 D8CWCTCXD6 CRCPD8CTCVD3D6CXCTD7 CPD6CT D8CWCPD8 D3CU D7CXD1D4D0CT ACD6D7D8B9D3D6CSCTD6 D4D6CTCSCXCRCPD8CTD7 B4D8CWCTDD CPD6CT CTCPCRCW CRD3D1D4D0CTD8CT DACTD6CQ D4CWD6CPD7CTD7 CXD2 CTD7D7CTD2CRCTBM COCRD3D2D8CPCXD2CXD2CV D3D2CT D3D6CPD2CVCTB3 CPD2CS COCRD3D2D8CPCXD2CXD2CV D3D2CT D0CTD1D3D2B3B5BA CCCWCT CRD3D2CYD9D2CRD8CXD3D2 D8CWCTD2 CUD3D6D1D7 CP D0CPD6CVCTD6 CRD3D2B9 D7D8CXD8D9CTD2D8 D3CU D8CWCT D7CPD1CT CUD3D6D1 B4D8CWCT D9D2D7CWCPCSCTCS D3D9D8D0CXD2CT CXD2 D8CWCT ACCVD9D6CTB5B8 DBCXD8CW CP D0D3DBCTD6 CRD3D1D4D3D2CTD2D8 CRD3D2D8CPCXD2CXD2CV D8CWCT CRD3D2CYD3CXD2CTCS CRD3D2D7D8CXD8D9CTD2D8D7B3 C6C8 CRD3D1D4D3D2CTD2D8D7 CRD3D2CRCPD8CTB9 D2CPD8CTCS CXD2 D8CWCT D9D7D9CPD0 DBCPDDB8 CPD2CS CPD2 D9D4D4CTD6 CRD3D1D4D3D2CTD2D8CXD2 DBCWCXCRCW D8CWCT CRD3D2CYD3CXD2CTCS CRD3D2D7D8CXD8D9CTD2D8D7B3 D2D3D2B9C6C8 CRD3D1D4D3B9 D2CTD2D8D7 CPD6CT CXCSCTD2D8CXACCTCS D3D6 D3DACTD6D0CPD4D4CTCSBA C1CU D8CWCT CSD9D4D0CXCRCPD8CTCS CRD3D1D4D3D2CTD2D8D7 CSD3 D2D3D8 CRD3DACTD6 D8CWCT D7CPD1CT D7D8D6CXD2CV DDCXCTD0CSB8 D8CWCT CRD3D2CYD9D2CRD8CXD3D2 CSD3CTD7 D2D3D8 CPD4D4D0DDBA C6D3D8CT D8CWCPD8B8 D7CXD2CRCT D8CWCTDD CPD6CT D3D2D0DD CPD4D4D0CXCTCS D8D3 D3D6CSCXD2CPD6DD ACD6D7D8B9D3D6CSCTD6 D4D6CTCSCXCRCPD8CTD7 B4CTBACVBA D7CTD2D8CTD2CRCTD7 D3D6 DACTD6CQ D4CWD6CPD7CTD7B5 CXD2 D8CWCXD7 CPD2CPD0DDD7CXD7B8 CRD3D2CYD9D2CRD8CXD3D2D7 CRCPD2 D2D3DB D7CPCUCTD0DD CQCT CPD7B9 D7CXCVD2CTCS D8CWCT CUCPD1CXD0CXCPD6 D8D6D9D8CWB9CUD9D2CRD8CXD3D2CPD0 CSCTD2D3D8CPD8CXD3D2D7 CXD2 CTDACTD6DD CRCPD7CTBA BK BTD0D7D3B8 D7CXD2CRCT D8CWCT D6CTD7D9D0D8CXD2CV CRD3D2D7D8CXD8D9CTD2D8 CWCPD7 D8CWCT D7CPD1CT D2D9D1CQCTD6 D3CU CRD3D1D4D3D2CTD2D8D7 CPD7 D8CWCT CRD3D2CYD3CXD2CTCS CRD3D2D7D8CXD8D9CTD2D8D7B8 D8CWCTD6CT CXD7 D2D3D8CWCXD2CV D8D3 D4D6CTDACTD2D8 CXD8D7 D9D7CT CPD7 CPD2 CPD6CVD9D1CTD2D8 CXD2 D7D9CQD7CTD5D9CTD2D8 CRD3D2CYD9D2CRD8CXD3D2 D3D4CTD6CPD8CXD3D2D7BA BT D7CPD1D4D0CT D1D9D0D8CXB9CRD3D1D4D3D2CTD2D8 CPD2CPD0DDD7CXD7 CUD3D6 D5D9CPD2D8CXACCTD6D7 CXD7 D7CWD3DBD2 CQCTD0D3DBB8 CPD0D0D3DBCXD2CV D1CPD8CTD6CXCPD0 D8D3 CQCT CSD9D4D0CXCRCPD8CTCS CQD3D8CW D8D3 D8CWCT D0CTCUD8 CPD2CS D8D3 D8CWCT D6CXCVCWD8 D3CU CP CRD3D2CYD3CXD2CTCS C6C8BM D7D3D1CTB8CPD0D0B8D2D3B8CTD8CRBA BM CGD2C6C8 D5 A1 C6C8 D5 D2C6C8 D5 A1C6C8 D5 BPC6C8 AF CGBPC6C8 D5 A1 C6C8 D5 D2C6C8 D5 A1C6C8 D5 BPC6C8 AF CCCWCT D0CTDCCXCRCPD0 CTD2D8D6DD CUD3D6 CP D5D9CPD2D8CXACCTD6 CRCPD2 CQCT D7D4D0CXD8 CXD2 D8CWCXD7 BJ BWCPCWD0 CPD2CS C5CRBVD3D6CS B4BDBLBKBFB5 D4D6D3D4D3D7CT CP D7CXD1CXD0CPD6 CSD9D4D0CXCRCPD8CXD3D2 D1CTCRCWCPD2CXD7D1 D8D3 D4D6D3CSD9CRCT CPD4D4D6D3D4D6CXCPD8CT D7CTD1CPD2D8CXCR D6CTD4D6CTD7CTD2D8CPD8CXD3D2D7 CUD3D6 C6C8 CPD2CS D3D8CWCTD6 CRD3D2CYD9D2CRD8CXD3D2D7B8 CQD9D8 CUD3D6 CSCXABCTD6CTD2D8 D6CTCPD7D3D2D7BA BK CTBACVBA CUD3D6 D8CWCT DBD3D6CS COCPD2CSB3BM CUCWBMBMBMCCCACDBXBNBMBMBMCCCACDBXBNBMBMBMCCCACDBXCXBN CWBMBMCCCACDBXBNBMBMBYBTC4CBBXBNBMBMBYBTC4CBBXCXBN CWBMBMBYBTC4CBBXBNBMBMCCCACDBXBNBMBMBYBTC4CBBXCXBN CWBMBMBYBTC4CBBXBNBMBMBYBTC4CBBXBNBMBMBYBTC4CBBXCXCV DBCPDDCXD2D8D3CPD2D9D1CQCTD6 D3CU CRD3D1D4D3D2CTD2D8D7B8 D8CWCT D0CPD7D8 B4D3D6 D0D3DBB9 CTD7D8B5 D3CU DBCWCXCRCW CXD7 D2D3D8 CSD9D4D0CXCRCPD8CTCS CXD2 CRD3D2CYD9D2CRD8CXD3D2 DBCWCXD0CT D3D8CWCTD6D7 D1CPDD D3D6 D1CPDD D2D3D8 CQCTBA CCCWCTD7CT CXD2CRD0D9CSCT CP CRD3D1B9 D4D3D2CTD2D8 CUD3D6 D8CWCT D5D9CPD2D8CXACCTD6 C6C8 D5 BPC6C8 AF B4DBCWCXCRCW DBCXD0D0 D9D0D8CXB9 D1CPD8CTD0DD CPD0D7D3 CRD3D2D8CPCXD2 CP D2D3D9D2 D4CWD6CPD7CT D6CTD7D8D6CXCRD8D3D6 D3CU CRCPD8CTB9 CVD3D6DD C6C8 AF B5B8 CP CRD3D1D4D3D2CTD2D8 CUD3D6 D6CTD7D8D6CXCRD8D3D6 C8C8D7 CPD2CS D6CTD0CPB9 D8CXDACT CRD0CPD9D7CTD7 D3CU CRCPD8CTCVD3D6DD C6C8 D5 D2C6C8 D5 D8CWCPD8 CPD6CT CPD8D8CPCRCWCTCS CPCQD3DACT D8CWCT D5D9CPD2D8CXACCTD6 CPD2CS CSD9D4D0CXCRCPD8CTCS CXD2 D8CWCT CRD3D2CYD9D2CRB9 D8CXD3D2B8 CPD2CS CP CRD3D1D4D3D2CTD2D8 CUD3D6 D8CWCT CQD3CSDD B4CP DACTD6CQ D3D6 DACTD6CQ D4CWD6CPD7CT D3D6 D3D8CWCTD6 D4D6CTCSCXCRCPD8CTB5 D3CU CRCPD8CTCVD3D6DD CGD2C6C8 D5 D3D6 CGBPC6C8 D5 BA CCCWCT D7D9CQD7CRD6CXD4D8 D5 D7D4CTCRCXACCTD7 D3D2CT D3CU CP ACD2CXD8CT D7CTD8 D3CU D5D9CPD2D8CXACCTD6D7B8 CPD2CS D8CWCT D7D9CQD7CRD6CXD4D8 AF CXD2CSCXCRCPD8CTD7 CPD2 D9D2D5D9CPD2D8CXACCTCS C6C8BA CCCWCT CSCTCSD9CRD8CXDACT D4CPD6D7CTD6 D4D6CTD7CTD2D8CTCS CXD2 CBCTCRD8CXD3D2 BE CRCPD2 D2D3DB CQCT CTDCD8CTD2CSCTCS CQDD CXD2CRD3D6D4D3D6CPD8CXD2CV D7CTD5D9CTD2CRCTD7 D3CU D6CTCRB9 D3CVD2CXDECTCS CPD2CS D9D2D6CTCRD3CVD2CXDECTCS CRD3D1D4D3D2CTD2D8D7 CXD2D8D3 D8CWCT CRD3D2B9 D7D8CXD8D9CTD2D8 CRCWCPD6D8 CXD8CTD1D7BA BTD7 CRD3D2D7D8CXD8D9CTD2D8D7 CPD6CT CRD3D1B9 D4D3D7CTCSB8 CRD3D1D4D3D2CTD2D8D7 CPD6CT D7CWCXCUD8CTCS CUD6D3D1 D8CWCT D9D2D6CTCRD3CVB9 D2CXDECTCS D7CTD5D9CTD2CRCT AD BD A1A1A1AD CR D8D3 D8CWCT D6CTCRD3CVD2CXDECTCS D7CTD5D9CTD2CRCT CWCX BD BNCY BD BNAD BD CXA1A1A1CWCX CR BNCY CR BNAD CR CXB8 D9D2D8CXD0 D8CWCT D9D2D6CTCRD3CVD2CXDECTCS D7CTB9 D5D9CTD2CRCT CXD7 CTD1D4D8DDBA CCCWCT CTDCD8CTD2CSCTCS D4CPD6D7CTD6 CXD7 CSCTACD2CTCS DBCXD8CWBM AF CRCWCPD6D8 CXD8CTD1D7 D3CU D8CWCT CUD3D6D1 CJCXBNCYBNA1BNA6CLB8 DBCWCTD6CT A1 CXD7 CP D7CTD5D9CTD2CRCT D3CU D9D2D6CTCRD3CVD2CXDECTCS CRD3D1D4D3D2CTD2D8D7 ADB8A6CXD7 CP D7CTD5D9CTD2CRCT D3CU D6CTCRD3CVD2CXDECTCS CRD3D1D4D3D2CTD2D8D7 CWCPBNCQBNADCXB8 CPD2CS CXBNCYBNCZBNCPBNCQBNCR CPD6CT CXD2CSCXCRCTD7 CXD2 D8CWCT CXD2D4D9D8BA BXCPCRCW CXD8CTD1 CJCXBNCYBNA1A1ADBNCWCX BD BNCY BD BNAD BD CXA1A1A1CWCX CR BNCY CR BNAD CR CXCL CXD2CSCXCRCPD8CTD7 D8CWCPD8 D8CWCT D7D4CPD2 CUD6D3D1 CX D8D3 CY CXD2 D8CWCT CXD2D4D9D8 CRCPD2 CQCT CRCWCPD6CPCRD8CTD6CXDECTCS CQDD D8CWCT CRCPD8CTCVD3D6CXCTD7 AD BD D8CWD6D3D9CVCW AD CR CPD8 D4D3D7CXD8CXD3D2D7CX BD D8D3CY BD D8CWD6D3D9CVCWCX CR D8D3CY CR D6CTD7D4CTCRD8CXDACTD0DDB8D7D3 D8CWCPD8 CXCU D8CWCTD7CT D7D4CPD2D7 CPD6CT CRD3D2CRCPD8CTD2CPD8CTCS CXD2 DBCWCPD8CTDACTD6 D3D6CSCTD6 D8CWCTDD D3CRCRD9D6 CXD2 D8CWCT CXD2D4D9D8 D7D8D6CXD2CVB8 D8CWCTDD CUD3D6D1 CP CVD6CPD1D1CPD8CXCRCPD0 CRD3D2D7D8CXD8D9CTD2D8 D3CU CRCPD8CTCVD3D6DD AD DBCXD8CW D9D2D6CTCRD3CVD2CXDECTCS CRD3D1D4D3D2CTD2D8D7 A1BA AF CP D0CTDCCXCRCPD0 CXD8CTD1 CJCXBNCYBNADBNCWCXBNCYBNADCXCL CUD3D6 CTDACTD6DD D6D9D0CT AD AX DB BE C8 CXCU DB D3CRCRD9D6D7 CQCTD8DBCTCTD2 D4D3D7CXD8CXD3D2D7 CX CPD2CS CY CXD2 D8CWCT CXD2D4D9D8BN AF CP D7CTD8 D3CU D6D9D0CTD7 CUD3D6 CPD0D0 CXBNCYBNCZBNCPBNCQBNCR BE C1 D2 BC CPD7 CQCTD0D3DBBA CCDBD3 D6D9D0CTD7 D8D3 CXD2DAD3CZCT D0CTCUD8 CPD2CS D6CXCVCWD8 CUD9D2CRD8CXD3D2 CPD4B9 D4D0CXCRCPD8CXD3D2 D8D3 CPD2 CTDCCXD7D8CXD2CV CRD3D1D4D3D2CTD2D8BM CJCXBNCZBNADBPBNCWCXBNCZBNADBPCXCLCJCZBNCYBNA1A1BNCWCZBNCQBNBPAYCXA1A6CL CJCXBNCYBNA1A1ADBNCWCXBNCQBNADBPAYCXA1A6CL ADAXADBP BEC8B8 CJCZBNCYBNADD2BNCWCZBNCYBNADD2CXCLCJCXBNCZBNA1A1BNCWCPBNCZBND2AYCXA1A6CL CJCXBNCYBNA1A1ADBNCWCPBNCYBNADD2AYCXA1A6CL ADAXADD2BEC8B8 CCDBD3 D6D9D0CTD7 D8D3 CXD2DAD3CZCT D0CTCUD8 CPD2CS D6CXCVCWD8 CUD9D2CRD8CXD3D2 CPD4B9 D4D0CXCRCPD8CXD3D2 D8D3 CP CUD6CTD7CW CRD3D1D4D3D2CTD2D8BM CJCXBNCZBNADBPBNCWCXBNCZBNADBPCXCLCJCZBNCYBNA1A1ADBPA1BNA6CL CJCXBNCYBNA1A1ADBNCWCXBNCZBNADBPCXA1A6CL ADAXADBP BEC8B8 CJCZBNCYBNADD2BNCWCZBNCYBNADD2CXCLCJCXBNCZBNA1A1ADD2A1BNA6CL CJCXBNCYBNA1A1ADBNCWCZBNCYBNADD2CXA1A6CL ADAXADD2BEC8B8 CCDBD3 D6D9D0CTD7 D8D3 CSCXD7CRCWCPD6CVCT CTD1D4D8DD CRD3D1D4D3D2CTD2D8D7BM CJCXBNCYBNA1A1ADBPA1BNA6CL CJCXBNCYBNA1A1ADBNA6CL CJCXBNCYBNA1A1ADD2A1BNA6CL CJCXBNCYBNA1A1ADBNA6CL CCCWD6CTCT D6D9D0CTD7 D8D3 D7CZCXD4 CRD3D2CYD9D2CRD8CXD3D2D7B8 CQDD CPCSCSCXD2CV CP CVCPD4 CQCTD8DBCTCTD2 D8CWCT CRD3D1D4D3D2CTD2D8D7 CXD2 CP CRD3D2D7D8CXD8D9CTD2D8 B4D8CWCT ACD6D7D8 D6D9D0CT CRD3D2D7D9D1CTD7 D8CWCT CRD3D2CYD9D2CRD8CXD3D2 D8D3 CRD6CTB9 CPD8CT CP D4CPD6D8CXCPD0 D6CTD7D9D0D8 D3CU CRCPD8CTCVD3D6DD BVD3D2CY BC  B8 CPD2CS D8CWCT D0CPD8D8CTD6 D8DBD3 D9D7CT D8CWCXD7 D8D3 D7CZCXD4 D8CWCT D3D4D4D3D7CXD2CV C6C8B5BM CJCZBNCYBNA1A1BNA6CL CJCXBNCYBNA1A1BVD3D2CY BC  BNA6CL CJCXBNCZBNBVD3D2CYBNCWCXBNCZBNBVD3D2CYCXCL CJCZBNCYBNA1A1BVD3D2CY BC  BNA6CL CJCXBNCYBNA1A1BNA6CL CJCXBNCZBNA1A1BN CL CJCXBNCZBNA1A1BNA6CL CJCXBNCYBNA1A1BNA6CL CJCZBNCYBNA1A1BVD3D2CY BC  BN CL CCDBD3 D6D9D0CTD7 D8D3 D6CTCPD7D7CTD1CQD0CT CSCXD7CRD3D2D8CXD2D9D3D9D7 CRD3D2B9 D7D8CXD8D9CTD2D8D7 B4CPCVCPCXD2B8 D9D7CXD2CV CP D4CPD6D8CXCPD0 D6CTD7D9D0D8 BVD3D2CY BC  D8D3 D6CTCSD9CRCT D8CWCT D2D9D1CQCTD6 D3CU D6CPD2CVCXD2CV DACPD6CXCPCQD0CTD7B5BM CJCPBNCRBNBVD3D2CYBNCWCPBNCRBNBVD3D2CYCXCLCJCXBNCYBNADBNA6A1CWCRBNCQBNCXCL CJCXBNCYBNADBNA6A1CWCPBNCQBNBVD3D2CY BC  CXCL CJCXBNCYBNADBNA6A1CWCRBNCQBNBVD3D2CY BC  CXCLCJCXBNCYBNADBNA6A1CWCPBNCRBNCXCL CJCXBNCYBNADBNA6A1CWCPBNCQBNCXCL CCDBD3 D6D9D0CTD7 D8D3 CRD3D1CQCXD2CT CPCSCYCPCRCTD2D8 CRD3D1D4D3D2CTD2D8D7BM CJCXBNCYBNADBNA6A1CWCPBNCRBNBPAYCXA1CWCRBNCQBNAYCXCL CJCXBNCYBNADBNA6A1CWCPBNCQBNCXCL CJCXBNCYBNADBNA6A1CWCRBNCQBND2AYCXA1CWCPBNCRBNAYCXCL CJCXBNCYBNADBNA6A1CWCPBNCQBNCXCL BTD2CS D3D2CT D6D9D0CT D8D3 CPD4D4D0DD D5D9CPD2D8CXACCTD6 CUD9D2CRD8CXD3D2D7BM CJCXBNCYBNADBNA6A1CWCPBNCQBN D5 CXCL CJCXBNCYBNADBNA6A1CWCPBNCQBN AF CXCL CCCWCT D4CPD6D7CXD2CV CPD2CS D7CRD3D6CXD2CV CUD9D2CRD8CXD3D2D7 D6CTD1CPCXD2 CXCSCTD2D8CXB9 CRCPD0 D8D3 D8CWD3D7CT CXD2 CBCTCRD8CXD3D2 BEB8 CQD9D8 CPD2 CPCSCSCXD8CXD3D2CPD0 CZ BP BD CRCPD7CT CRD3D2D8CPCXD2CXD2CV CP D1D3CSCXACCTCS D4D6D3CYCTCRD8CXD3D2 CUD9D2CRD8CXD3D2 AP CXD7 D2D3DB CPCSCSCTCS D8D3 D8CWCT CXD2D8CTD6D4D6CTD8CPD8CXD3D2 CUD9D2CRD8CXD3D2B8 CXD2 D3D6CSCTD6 D8D3 D1CPCZCT D8CWCT CSCTD2D3D8CPD8CXD3D2D7 D3CU D5D9CPD2D8CXACCTCS CRD3D2D7D8CXD8D9CTD2D8D7 CSCTD4CTD2CS D3D2 D8CWCTCXD6 CPD7D7D3CRCXCPD8CTCS D5D9CPD2D8CXACCTD6D7BM BWB4DCB5BP CJ CP BD BMBMBMCP CZ D7BMD8BM CP BD BMBMBMCP CZ DC BK BQ BQ BQ BQ BQ BQ BQ BO BQ BQ BQ BQ BQ BQ BQ BM CAB4DCB5 CXCU CZ BPBC AP D5 BWB4CP BD B5 CXCU CZ BP BD CPD2CS CP BD DC BP CJBMBMBMCWBMBMBM D5 CXCL CJBMBMBMCWBMBMBM AF CXCL CZ D3D2 CXBPBD BWB4CP CX B5 D3D8CWCTD6DBCXD7CT CCCWCT D1D3CSCXACCTCS D4D6D3CYCTCRD8CXD3D2 CUD9D2CRD8CXD3D2 CTDACPD0D9CPD8CTD7 CP D5D9CPD2B9 D8CXACCTD6 CUD9D2CRD8CXD3D2 D5 D3D2 D7D3D1CT CPD6CVD9D1CTD2D8 CSCTD2D3D8CPD8CXD3D2 BTB8 CRD3D1D4CPD6CXD2CV D8CWCT CRCPD6CSCXD2CPD0CXD8DD D3CU D8CWCT CXD1CPCVCT D3CU D8CWCT D6CTB9 D7D8D6CXCRD8D3D6 D7CTD8 CXD2 BT DBCXD8CW D8CWCT D8CWCT CRCPD6CSCXD2CPD0CXD8DD D3CU CXD1CPCVCT D3CU D8CWCT CXD2D8CTD6D7CTCRD8CTCS D6CTD7D8D6CXCRD8D3D6 CPD2CS CQD3CSDD D7CTD8D7 CXD2 BTBM BL AP D5 BT BP CUCWCT BE BMBMBMCT CP BND8CXCYCW BNCT BE BMBMBMCT CP BN CXBEBTBN D8 BP D5B4CYCACYBNCYCBCYB5 CA BP BTD3D2CUCW BNCT BE BMBMBMCT CP BN CXCVBN CB BP BTD3D2CUCW BNCT BE BMBMBMCT CP BNCCCACDBXCXCVCV CCCWCXD7 CPD0CVD3D6CXD8CWD1 D4CPD6D7CTD7 CP CRCPD8CTCVD3D6CXCPD0 CVD6CPD1D1CPD6 CXD2 D8CWCT D9D7D9CPD0 DBCPDD DF CRD3D2D7D8CXD8D9CTD2D8D7 CPD6CT CXD2CXD8CXCPD0D0DD CPCSCSCTCS D8D3 D8CWCT CRCWCPD6D8 CPD7 D7CXD2CVD0CT CRD3D1D4D3D2CTD2D8D7 CRD3DACTD6CXD2CV CP CRCTD6D8CPCXD2 DDCXCTD0CS CXD2 D8CWCT CXD2D4D9D8 D7D8D6CXD2CV B4D8CWCT CXD2CSCXCRCTD7 D3CU D8CWCT CRD3D1D4D3D2CTD2D8 CPD6CT D8CWCT D7CPD1CT CPD7 D8CWCT CXD2CSCXCRCTD7 D3CU D8CWCT CRD3D2D7D8CXD8D9CTD2D8 CXD8D7CTD0CUB5B8 CPD2CS D8CWCTDD CPD6CT CRD3D1CQCXD2CTCS CQDD CRD3D2CRCPD8CTD2CPD8CXD2CV D8CWCT DDCXCTD0CSD7 D3CU D7D1CPD0D0CTD6 CRD3D2D7D8CXD8D9CTD2D8D7 D8D3 D1CPCZCT D0CPD6CVCTD6 D3D2CTD7 DF D9D2D8CXD0 CP CRD3D2CYD9D2CRD8CXD3D2 CXD7 CTD2CRD3D9D2D8CTD6CTCSBA CFCWCTD2 CP CRD3D2CYD9D2CRD8CXD3D2 CXD7 BL BYD3D0D0D3DBCXD2CV C3CTCTD2CPD2 CPD2CS CBD8CPDACX B4BDBLBKBIB5BA BC BD BE BF BG CRD3D2D8CPCXD2CXD2CV D3D2CT D3D6CPD2CVCT CPD2CS D3D2CT D0CTD1D3D2 CJBCBNBDBNCBD2C6C8 D5 BPC6C8 D5 BCBN CJBDBNBEBNCGD2C6C8 BL A1 C6C8 BL D2C6C8 BL A1 C6C8 BL BN CJBEBNBFBNBVD3D2CYBN CJBFBNBGBNCGD2C6C8 BL A1 C6C8 BL D2C6C8 BL A1 C6C8 BL BN CWBCBNBDBNCBD2C6C8 D5 BPC6C8 D5 BCCXCL CWBDBNBEBNC6C8 BL CXCL CUD3 BD BND3 BE BND3 BF BND3 BG CV CWBEBNBFBNBVD3D2CYCXCL CWBFBNBGBNC6C8 BL CXCL CUD0 BD BND0 BE BND0 BF CV CUCWD3 BD BNDC BD CXBNCWD0 BE BNDC BD CXBNCWD0 BF BNDC BF CXCV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BDB5 CJBDBNBGBNCGD2C6C8 BL A1 C6C8 BL D2C6C8 BL A1 C6C8 BL BNCWBDBNBEBNC6C8 BL CXCL CUD3 BD BND3 BE BND3 BF BND3 BG CV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BEB5 CJBDBNBGBNCGD2C6C8 BL A1 C6C8 BL D2C6C8 BL A1 C6C8 BL BNCWBFBNBGBNC6C8 BL CXCL CUD0 BD BND0 BE BND0 BF CV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BFB5 CJBDBNBGBNCGD2C6C8 BL A1 C6C8 BL BNCWBDBNBEBNC6C8 BL CXCL CUD3 BD BND3 BE BND3 BF BND3 BG CV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BGB5 CJBDBNBGBNCGD2C6C8 BL A1 C6C8 BL BNCWBFBNBGBNC6C8 BL CXCL CUD0 BD BND0 BE BND0 BF CV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BHB5 CJBCBNBGBNCBD2C6C8 D5 BNCWBCBNBDBNCBD2C6C8 D5 BPC6C8 BL CXA1CWBDBNBEBNC6C8 BL CXCL CUCWD3 BD BNDC BD CXCV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BIB5 CJBCBNBGBNCBD2C6C8 D5 BNCWBCBNBDBNCBD2C6C8 D5 BPC6C8 BL CXA1CWBFBNBGBNC6C8 BL CXCL CUCWD0 BE BNDC BD CXBNCWD0 BF BNDC BF CXCV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BJB5 CJBCBNBGBNCBD2C6C8 D5 BNCWBCBNBDBNCBD2C6C8 D5 BPC6C8 AF CXA1CWBDBNBEBNC6C8 AF CXCL CUDC BD CV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BKB5 CJBCBNBGBNCBD2C6C8 D5 BNCWBCBNBDBNCBD2C6C8 D5 BPC6C8 AF CXA1CWBFBNBGBNC6C8 AF CXCL CUDC BD BNDC BF CV B4BLB5 CJBCBNBGBNCBD2C6C8 D5 BNCWBCBNBDBNCBD2C6C8 D5 BPC6C8 AF CXA1CWBDBNBGBNC6C8 AF CXCL CUDC BD CV B4BDBCB5 CJBCBNBGBNCBD2C6C8 D5 BNCWBCBNBGBNCBD2C6C8 D5 CXCL CUDC BD CV BYCXCVD9D6CT BFBM CBCPD1D4D0CT CSCTD6CXDACPD8CXD3D2 D3CU CRD3D2CYD3CXD2CTCS C6C8BA CTD2CRD3D9D2D8CTD6CTCS CXD1D1CTCSCXCPD8CTD0DD D8D3 D8CWCT D0CTCUD8 D3D6 D6CXCVCWD8 D3CU CP D6CTCRB9 D3CVD2CXDECTCS CRD3D2D7D8CXD8D9CTD2D8 CRD3D2D7D8CXD8D9CTD2D8 DCB8 CPD2CS CPD2D3D8CWCTD6 CRD3D2B9 D7D8CXD8D9CTD2D8 D3CU D8CWCT D7CPD1CT CRCPD8CTCVD3D6DD CXD7 CUD3D9D2CS CXD1D1CTCSCXCPD8CTD0DD CQCTDDD3D2CS D8CWCPD8 CRD3D2CYD9D2CRD8CXD3D2B8 D8CWCT D4CPD6D7CTD6 CRD6CTCPD8CTD7 CP D2CTDB CRD3D2D7D8CXD8D9CTD2D8 D8CWCPD8 CWCPD7 D8CWCT CRD3D1CQCXD2CTCS DDCXCTD0CS D3CU CQD3D8CW CRD3D2B9 D7D8CXD8D9CTD2D8D7B8 CQD9D8 CRD3D4CXCTD7 DCB3D7 CRD3D1D4D3D2CTD2D8 DDCXCTD0CS B4D8CWCT D7D8D6CXD2CV CXD2CSCXCRCTD7 D3CU DCB3D7 D3D6CXCVCXD2CPD0 CRD3D1D4D3D2CTD2D8D7B5 DBCXD8CW D2D3 CRCWCPD2CVCTBA CCCWCXD7 CWCPD7 D8CWCT CTABCTCRD8 D3CU CRD6CTCPD8CXD2CV D8DBD3 D2CTDB CRD3D2D7D8CXD8D9CTD2D8D7 CTDACTD6DD D8CXD1CT D8DBD3 CTDCCXD7D8CXD2CV CRD3D2D7D8CXD8D9CTD2D8D7 CPD6CT CRD3D2CYD3CXD2CTCSBM CTCPCRCW DBCXD8CW CP CSCXABCTD6CTD2D8 CRD3D1D4D3D2CTD2D8 DDCXCTD0CSB8 CQD9D8 CQD3D8CW DBCXD8CW D8CWCT D7CPD1CT B4CRD3D1CQCXD2CTCSB5 CRD3D2D7D8CXD8D9CTD2D8 DDCXCTD0CSBA CCCWCTD7CT D2CTDB CSCXD7CRD3D2D8CXD2D9D3D9D7 CRD3D2D7D8CXD8D9CTD2D8D7 B4DBCXD8CW CRD3D1D4D3D2CTD2D8 DDCXCTD0CSD7 D8CWCPD8 CSD3 D2D3D8 CTDCCWCPD9D7D8 D8CWCTCXD6 CRD3D2D7D8CXD8D9CTD2D8 DDCXCTD0CSD7B5 CPD6CT D7D8CXD0D0 D8D6CTCPD8CTCS CPD7 D3D6CSCXD2CPD6DDCRD3D2D7D8CXD8D9CTD2D8D7 CQDD D8CWCT D4CPD6D7CTD6B8 DBCWCXCRCW CRD3D1CQCXD2CTD7 D8CWCTD1 DBCXD8CW CPD6CVD9D1CTD2D8D7 CPD2CS D1D3CSCXACCTD6D7 D9D2D8CXD0 CPD0D0 D3CU D8CWCTCXD6 CPD6CVD9D1CTD2D8 D4D3D7CXD8CXD3D2D7 CWCPDACT CQCTCTD2 D7D9CRCRCTD7D7B9 CUD9D0D0DD CSCXD7CRCWCPD6CVCTCSB8 CPD8 DBCWCXCRCW D4D3CXD2D8 D4CPCXD6D7 D3CU CSCXD7CRD3D2D8CXD2D9B9 D3D9D7 CRD3D2D7D8CXD8D9CTD2D8D7 DBCXD8CW D8CWCT D7CPD1CT CRD3D2D7D8CXD8D9CTD2D8 DDCXCTD0CS CRCPD2 CQCT D6CTCPD7D7CTD1CQD0CTCS CXD2D8D3 DBCWD3D0CT DF D3D6 CPD8 D0CTCPD7D8 D0CTD7D7 CSCXD7CRD3D2B9 D8CXD2D9D3D9D7 DF CRD3D2D7D8CXD8D9CTD2D8D7 CPCVCPCXD2BA BT D7CPD1D4D0CT CSCTD6CXDACPD8CXD3D2 CUD3D6 D8CWCT DACTD6CQ D4CWD6CPD7CT COCRD3D2B9 D8CPCXD2CXD2CV D3D2CT D3D6CPD2CVCT CPD2CS D3D2CT D0CTD1D3D2B8B3 CXD2DAD3D0DACXD2CV CRD3D2B9 CYD9D2CRD8CXD3D2 D3CU CTDCCXD7D8CTD2D8CXCPD0D0DD D5D9CPD2D8CXACCTCS D2D3D9D2 D4CWD6CPD7CTD7B8 CXD7 D7CWD3DBD2 CXD2 BYCXCVD9D6CT BFB8 D9D7CXD2CV D8CWCT CPCQD3DACT D4CPD6D7CT D6D9D0CTD7 CPD2CS D8CWCT D0CTDCCXCRCPD0CXDECTCS CVD6CPD1D1CPD6BM CRD3D2D8CPCXD2CXD2CV BM CBD2C6C8 D5 BPC6C8 D5 BC D3D2CT BM CGD2C6C8 D5 A1 C6C8 D5 D2C6C8 D5 A1 C6C8 D5 BPC6C8 AF CGBPC6C8 D5 A1 C6C8 D5 D2C6C8 D5 A1C6C8 D5 BPC6C8 AF D3D6CPD2CVCTB8 D0CTD1D3D2 BM C6C8 AF CPD2CS BM BVD3D2CY BYCXD6D7D8 D8CWCT D4CPD6D7CTD6 CPD4D4D0CXCTD7 D8CWCT D7CZCXD4 CRD3D2CYD9D2CRD8CXD3D2 D6D9D0CTD7 D8D3 D3CQD8CPCXD2 D8CWCT CSCXD7CRD3D2D8CXD2D9D3D9D7 CRD3D2D7D8CXD8D9CTD2D8D7 D7CWD3DBD2 CPCUB9 D8CTD6 D7D8CTD4D7 B4BDB5 CPD2CS B4BEB5B8 CPD2CS CP CRD3D1D4D3D2CTD2D8 CXD7 CSCXD7CRCWCPD6CVCTCS CUD6D3D1 CTCPCRCW D3CU D8CWCT D6CTD7D9D0D8CXD2CV CRD3D2D7D8CXD8D9CTD2D8D7 D9D7CXD2CV D8CWCT CTD1D4D8DD CRD3D1D4D3D2CTD2D8 D6D9D0CT CXD2 D7D8CTD4D7 B4BFB5 CPD2CS B4BGB5BA CCCWCT CRD3D2D7D8CXD8D9CTD2D8D7 D6CTD7D9D0D8CXD2CV CUD6D3D1 B4BFB5 CPD2CS B4BGB5 CPD6CT D8CWCTD2 CRD3D1B9 D4D3D7CTCS DBCXD8CW D8CWCT DACTD6CQ CRD3D2D7D8CXD8D9CTD2D8 CUD3D6 COCRD3D2D8CPCXD2CXD2CVB3 CXD2 D7D8CTD4D7 B4BHB5 CPD2CS B4BIB5B8 D9D7CXD2CV D8CWCT D0CTCUD8 CPD8D8CPCRCWD1CTD2D8 D6D9D0CT CUD3D6 CUD6CTD7CW CRD3D1D4D3D2CTD2D8D7BA CCCWCT D5D9CPD2D8CXACCTD6D7 CPD6CT D8CWCTD2 CPD4D4D0CXCTCS CXD2 D7D8CTD4D7 B4BJB5 CPD2CS B4BKB5B8 CPD2CS D8CWCT D6CTD7D9D0D8CXD2CV CRD3D2D7D8CXD8D9CTD2D8D7 CPD6CT D6CTCPD7D7CTD1CQD0CTCS D9D7CXD2CV D8CWCT CRD3D2CYD9D2CRD8CXD3D2 D6D9D0CTD7 CXD2 D7D8CTD4 B4BLB5BA CCCWCT CPCSCYCPCRCTD2D8 CRD3D1D4D3D2CTD2D8D7 CXD2 D8CWCT CRD3D2D7D8CXD8D9CTD2D8 D6CTD7D9D0D8CXD2CV CUD6D3D1 D7D8CTD4 B4BLB5 CPD6CT D8CWCTD2 D1CTD6CVCTCS D9D7CXD2CV D8CWCT CRD3D1CQCXD2CPD8CXD3D2 D6D9D0CT CXD2 D7D8CTD4 B4BDBCB5B8 D4D6D3CSD9CRCXD2CV CP CRD3D1D4D0CTD8CT CVCPD4D0CTD7D7 CRD3D2D7D8CXD8D9CTD2D8 CUD3D6 D8CWCT CTD2D8CXD6CT CXD2D4D9D8BA CBCXD2CRCT D8CWCT D4CPD6D7CTD6 D6D9D0CTD7 CPD6CT ACDCCTCSB8 CPD2CS D8CWCT D2D9D1CQCTD6 D3CU CRD3D1D4D3D2CTD2D8D7 CXD2 CPD2DDCRCWCPD6D8 CRD3D2D7D8CXD8D9CTD2D8 CXD7 CQD3D9D2CSCTCS CQDD D8CWCT D1CPDCCXD1D9D1 D2D9D1CQCTD6 D3CU CRD3D1D4D3D2CTD2D8D7 CXD2 CP CRCPD8CTCVD3D6DD B4CXD2CPD7D1D9CRCW CPD7 D8CWCT D6D9D0CTD7 CRCPD2 D3D2D0DD CPCSCS CP CRD3D1D4D3D2CTD2D8 D8D3 D8CWCT D6CTCRD3CVD2CXDECTCS D0CXD7D8 CQDD D7D9CQD8D6CPCRD8CXD2CV D3D2CT CUD6D3D1 D8CWCT D9D2D6CTCRD3CVD2CXDECTCS D0CXD7D8B5B8 D8CWCT CPD0CVD3D6CXD8CWD1 D1D9D7D8 D6D9D2 CXD2 D4D3D0DDB9 D2D3D1CXCPD0 D7D4CPCRCT CPD2CS D8CXD1CT D3D2 D8CWCT D0CTD2CVD8CW D3CU D8CWCT CXD2D4D9D8 D7CTD2D8CTD2CRCTBA CBCXD2CRCT D8CWCT CRCPD6CSCXD2CPD0CXD8DD D3CU CTCPCRCW CRD3D2D7D8CXD8D9CTD2D8B3D7 CSCTD2D3D8CPD8CXD3D2 CXD7 CQD3D9D2CSCTCS CQDD CYBXCY DA B4DBCWCTD6CT BX CXD7 D8CWCT D7CTD8 D3CU CTD2D8CXD8CXCTD7 CXD2 D8CWCT CTD2DACXD6D3D2D1CTD2D8 CPD2CS DA CXD7 D8CWCT D1CPDCCXB9 D1D9D1 DACPD0CTD2CRDD D3CU CPD2DD CRCPD8CTCVD3D6DDB5B8 D8CWCT CPD0CVD3D6CXD8CWD1 D6D9D2D7 CXD2 DBD3D6D7D8B9CRCPD7CT D4D3D0DDD2D3D1CXCPD0 D7D4CPCRCT D3D2 CYBXCYBN CPD2CS D7CXD2CRCT D8CWCTD6CT CXD7 D2D3 D1D3D6CT D8CWCPD2 D3D2CT D7CTD8 CRD3D1D4D3D7CXD8CXD3D2 D3D4CTD6CPD8CXD3D2 D4CTD6B9 CUD3D6D1CTCS DBCWCTD2 CP D6D9D0CT CXD7 CPD4D4D0CXCTCSB8 CPD2CS CTCPCRCW CRD3D1D4D3D7CXD8CXD3D2 D3D4CTD6CPD8CXD3D2 D6D9D2D7 CXD2 DBD3D6D7D8B9CRCPD7CT D5D9CPCSD6CPD8CXCR D8CXD1CT D3D2 D8CWCT D7CXDECT D3CU CXD8D7 CRD3D1D4D3D7CTCS D7CTD8D7 B4CSD9CT D8D3 D8CWCT D5D9CPD2D8CXACCTD6 D3D4CTD6B9 CPD8CXD3D2B5B8 D8CWCT CPD0CVD3D6CXD8CWD1 D6D9D2D7 CXD2 DBD3D6D7D8B9CRCPD7CT D4D3D0DDD2D3D1CXCPD0 D8CXD1CT D3D2 CYBXCY CPD7 DBCTD0D0BA BG BXDACPD0D9CPD8CXD3D2 CCCWCT CTDCD8CTD2CSCTCS D4CPD6D7CTD6 CSCTD7CRD6CXCQCTCS CPCQD3DACT CWCPD7 CQCTCTD2 CXD1B9 D4D0CTD1CTD2D8CTCS CPD2CS CTDACPD0D9CPD8CTCS D3D2 CP CRD3D6D4D9D7 D3CU BFBGBC D7D4D3B9 CZCTD2 CXD2D7D8D6D9CRD8CXD3D2D7 D8D3 D7CXD1D9D0CPD8CTCS CWD9D1CPD2B9D0CXCZCT CPCVCTD2D8D7 CXD2 CP CRD3D2D8D6D3D0D0CTCS BFB9BW CTD2DACXD6D3D2D1CTD2D8 B4D8CWCPD8 D3CU CRCWCXD0CSD6CTD2 D6D9D2B9 D2CXD2CV CP D0CTD1D3D2CPCSCT D7D8CPD2CSB8 DBCWCXCRCWDBCPD7 CSCTCTD1CTCS D7D9CXD8CPCQD0DD CUCPD1CXD0CXCPD6 D8D3 D9D2CSCTD6CVD6CPCSD9CPD8CT D7D8D9CSCTD2D8 D7D9CQCYCTCRD8D7B5BA CCCWCT D4CPD6D7CTD6 DBCPD7 D6D9D2 D3D2 D8CWCT DBD3D6CS D0CPD8D8CXCRCT D3D9D8D4D9D8 D3CU CPD2 D3ABB9D8CWCTB9D7CWCTD0CU D7D4CTCTCRCW D6CTCRD3CVD2CXDECTD6 B4BVC5CD CBD4CWCXD2DC C1C1B5 CPD2CS D8CWCT D4CPD6D7CTD6 CRCWCPD6D8 DBCPD7 D7CTCTCSCTCS DBCXD8CW CTDACTD6DD CWDDD4D3D8CWCTD7CXDECTCS DBD3D6CSBA CCCWCT D4CPD6D7CTD6 DBCPD7 CPD0D7D3 CRD3D1D4CPD6CTCS DBCXD8CW D8CWCT D6CTCRB9 D3CVD2CXDECTD6 CQDD CXD8D7CTD0CUB8 CXD2 D3D6CSCTD6 D8D3 CSCTD8CTD6D1CXD2CT D8CWCT CSCTCVD6CTCT D8D3 DBCWCXCRCW CPD2 CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS CPD4D4D6D3CPCRCW CRD3D9D0CS CRD3D1B9 D4D0CTD1CTD2D8 CRD3D6D4D9D7B9CQCPD7CTCS CSCXD7CPD1CQCXCVD9CPD8CXD3D2BA CCCWCT D7DDD7D8CTD1D7 DBCTD6CT CTDACPD0D9CPD8CTCS CPD7 DBD3D6CS D6CTCRD3CVD2CXDECTD6D7 B4CXBACTBA CXCVD2D3D6CXD2CV D8CWCT CQD6CPCRCZCTD8D7 CXD2 D8CWCT D4CPD6D7CTD6 D3D9D8D4D9D8B5 D3D2 D8CWCT ACD6D7D8 BDBCBC D7CTD2B9 D8CTD2CRCTD7 D3CU D8CWCT CRD3D6D4D9D7 B4CRD3D6D6CTD7D4D3D2CSCXD2CV D8D3 D8CWCT ACD6D7D8 D7CTDACTD2 D3CU BFBF D7D9CQCYCTCRD8D7B5BN D8CWCT D0CPD8D8CTD6 BEBGBC D7CTD2D8CTD2CRCTD7 DBCTD6CT D6CTB9 D7CTD6DACTCS CUD3D6 D8D6CPCXD2CXD2CV D8CWCT D6CTCRD3CVD2CXDECTD6 CPD2CS CUD3D6 CSCTDACTD0D3D4CXD2CV D8CWCT CVD6CPD1D1CPD6 CPD2CS D7CTD1CPD2D8CXCR D0CTDCCXCRD3D2BA CCCWCT CPDACTD6CPCVCT D9D8D8CTD6CPD2CRCT D0CTD2CVD8CW DBCPD7 CPD4D4D6D3DCCXD1CPD8CTD0DD D8CWD6CTCT D7CTCRD3D2CSD7 B4D7D9CQD7D9D1CXD2CV CPCQD3D9D8 BFBCBC CUD6CPD1CTD7 D3D6 D4D3D7CXB9 D8CXD3D2D7 CXD2 D8CWCT D4CPD6D7CTD6 CRCWCPD6D8B5B8 CRD3D2D8CPCXD2CXD2CV CPD2 CPDACTD6CPCVCT D3CU D2CXD2CT DBD3D6CSD7BA C8CPD6D7CXD2CV D8CXD1CT CPDACTD6CPCVCTCS D9D2CSCTD6 BGBC D7CTCRD3D2CSD7 D4CTD6 D7CTD2D8CTD2CRCT D3D2 CP C8BGB9BDBHBCBCC5C0DEB8 D1D3D7D8 D3CU DBCWCXCRCWDBCPD7 D7D4CTD2D8 CXD2 CUD3D6CTD7D8 CRD3D2D7D8D6D9CRD8CXD3D2 D6CPD8CWCTD6 D8CWCPD2 CSCTD2D3D8CPD8CXD3D2 CRCPD0CRD9D0CPD8CXD3D2BA BTCRCRD9D6CPCRDD D6CTD7D9D0D8D7 D7CWD3DB D8CWCPD8 D8CWCT D4CPD6D7CTD6 DBCPD7 CPCQD0CT D8D3 CRD3D6D6CTCRD8D0DD CXCSCTD2D8CXCUDD CP D7CXCVD2CXACCRCPD2D8D2D9D1CQCTD6 D3CU DBD3D6CSD7 D8CWCPD8 D8CWCT D6CTCRD3CVD2CXDECTD6 D1CXD7D7CTCS B4CPD2CS DACXCRCT DACTD6D7CPB5B8 D7D9CRCW D8CWCPD8 CP D4CTD6CUCTCRD8 D7DDD2D8CWCTD7CXD7 D3CU D8CWCT D8DBD3 B4CRCWD3D3D7CXD2CV D8CWCT CRD3D6D6CTCRD8 DBD3D6CS CXCU CXD8 CXD7 D6CTCRD3CVD2CXDECTCS CQDD CTCXD8CWCTD6 D7DDD7D8CTD1B5 DBD3D9D0CS D4D6D3B9 CSD9CRCT CPD2 CPDACTD6CPCVCT D3CU BK D4CTD6CRCTD2D8CPCVCT D4D3CXD2D8D7 D1D3D6CT D6CTCRCPD0D0 D8CWCPD2 D8CWCT D6CTCRD3CVD2CXDECTD6 CQDD CXD8D7CTD0CU D3D2 D7D9CRCRCTD7D7CUD9D0 D4CPD6D7CTD7B8 CPD2CS CPD7 D1D9CRCW CPD7 BDBL D4CTD6CRCTD2D8CPCVCT D4D3CXD2D8D7 D1D3D6CT CUD3D6 D7D3D1CT D7D9CQCYCTCRD8D7BM BDBC D6CTCRD3CVD2CXDECTD6 D4CPD6D7CTD6 CYD3CXD2D8 D7D9CQCYCTCRD8 D4D6CTCR D6CTCRCPD0D0 CUCPCXD0 D4D6CTCR D6CTCRCPD0D0 D6CTCRCPD0D0 BC BJBI BJBL BDBK BJBE BJBG BLBE BD BJBJ BJBH BEBK BIBF BHBH BKBF BE BJBC BJBD BFBF BGBL BHBG BIBL BF BJBD BIBJ BGBF BGBL BGBH BIBL BG BIBI BHBG BFBJ BGBG BFBL BIBJ BH BHBF BHBE BHBG BFBI BFBD BJBE BI BKBG BKBG BHBC BHBI BIBF BKBF CPD0D0 BIBK BIBJ BFBJ BHBF BHBC BJBH DBCWCXCRCW CXD2CSCXCRCPD8CTD7 D8CWCPD8 D8CWCT CTD2DACXD6D3D2D1CTD2D8 D1CPDD D3ABCTD6 CP D9D7CTCUD9D0 CPCSCSCXD8CXD3D2CPD0 D7D3D9D6CRCT D3CU CXD2CUD3D6D1CPD8CXD3D2 CUD3D6 CSCXD7CPD1B9 CQCXCVD9CPD8CXD3D2BA CCCWD3D9CVCW CXD8 D1CPDD D2D3D8 CQCT D4D3D7D7CXCQD0CT D8D3 CXD1D4D0CTB9 D1CTD2D8 CP D4CTD6CUCTCRD8 D7DDD2D8CWCTD7CXD7 D3CU D8CWCT CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS BDBC CBD9CRCRCTD7D7CUD9D0 D4CPD6D7CTD7 CPD6CT D8CWD3D7CT D8CWCPD8 D6CTD7D9D0D8 CXD2 D3D2CT D3D6 D1D3D6CT CRD3D1D4D0CTD8CT CPD2CPD0DDD7CTD7 D3CU D8CWCT CXD2D4D9D8B8 CTDACTD2 CXCU D8CWCT CRD3D6D6CTCRD8 D8D6CTCT CXD7 D2D3D8 CPD1D3D2CV D8CWCTD1BA CPD2CS CRD3D6D4D9D7B9CQCPD7CTCS CPD4D4D6D3CPCRCWCTD7B8 CXCU CTDACTD2 CWCPD0CU D3CU D8CWCT CPCQD3DACT CVCPCXD2D7 CRCPD2 CQCT D6CTCPD0CXDECTCSB8 CXD8 DBD3D9D0CS D1CPD6CZ CP D7CXCVB9 D2CXACCRCPD2D8 CPCSDACPD2CRCTBA BH BVD3D2CRD0D9D7CXD3D2 CCCWCXD7 D4CPD4CTD6 CWCPD7 CSCTD7CRD6CXCQCTCS CPD2 CTDCD8CTD2D7CXD3D2 D8D3 CPD2 CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS D4CPD6D7CXD2CV CPD0CVD3D6CXD8CWD1B8 CXD2CRD6CTCPD7CXD2CV CXD8D7 D7CTD1CPD2D8CXCR CRD3DACTD6CPCVCT D8D3 CXD2CRD0D9CSCT D5D9CPD2D8CXACCTD6 CPD2CS CRD3D2CYD9D2CRB9 D8CXD3D2 D3D4CTD6CPD8CXD3D2D7 DBCXD8CWD3D9D8 CSCTD7D8D6D3DDCXD2CV CXD8D7 D4D3D0DDD2D3D1CXCPD0 DBD3D6D7D8B9CRCPD7CT CRD3D1D4D0CTDCCXD8DDBA BXDCD4CTD6CXD1CTD2D8D7 D9D7CXD2CV CPD2 CXD1D4D0CTB9 D1CTD2D8CPD8CXD3D2 D3CU D8CWCXD7 CPD0CVD3D6CXD8CWD1 D3D2 CP CRD3D6D4D9D7 D3CU D7D4D3CZCTD2 CXD2D7D8D6D9CRD8CXD3D2D7 CXD2CSCXCRCPD8CT D8CWCPD8 BDB5 D8CWCT D3CQD7CTD6DACTCS CRD3D1D4D0CTDCB9 CXD8DD D3CU D8CWCT CPD0CVD3D6CXD8CWD1 CXD7 D7D9CXD8CPCQD0CT CUD3D6 D4D6CPCRD8CXCRCPD0 D9D7CTD6 CXD2B9 D8CTD6CUCPCRCT CPD4D4D0CXCRCPD8CXD3D2D7B8 CPD2CS BEB5 D8CWCT CPCQCXD0CXD8DD D8D3 CSD6CPDB D3D2 D8CWCXD7 CZCXD2CS D3CU CTD2DACXD6D3D2D1CTD2D8 CXD2CUD3D6D1CPD8CXD3D2 CXD2 CPD2 CXD2D8CTD6B9 CUCPCRCTCS CPD4D4D0CXCRCPD8CXD3D2 CWCPD7 D8CWCT D4D3D8CTD2D8CXCPD0 D8D3 CVD6CTCPD8D0DD CXD1B9 D4D6D3DACT D6CTCRD3CVD2CXD8CXD3D2 CPCRCRD9D6CPCRDD CXD2 D7D4CTCPCZCTD6B9CXD2CSCTD4CTD2CSCTD2D8 D1CXDCCTCSB9CXD2CXD8CXCPD8CXDACTCXD2D8CTD6CUCPCRCTD7BA CACTCUCTD6CTD2CRCTD7 C3CPDECXD1CXCTD6DE BTCYCSD9CZCXCTDBCXCRDEBA BDBLBFBHBA BWCXCT D7DDD2D8CPCZD8CXD7CRCWCT CZD3D2D2CTDCB9 CXD8CPD8BA C1D2 CBBA C5CRBVCPD0D0B8 CTCSCXD8D3D6B8 C8D3D0CXD7CW C4D3CVCXCR BDBLBEBCB9BDBLBFBLB8 D4CPCVCTD7 BEBCBJDFBEBFBDBA C7DCCUD3D6CS CDD2CXDACTD6D7CXD8DD C8D6CTD7D7BA CCD6CPD2D7D0CPD8CTCS CUD6D3D1 CBD8D9CSCXCP C8CWCXD0D3D7D3D4CWCXCRCP BDBM BDDFBEBJBA CHCTCWD3D7CWD9CP BUCPD6B9C0CXD0D0CTD0BA BDBLBHBFBA BT D5D9CPD7CXB9CPD6CXD8CWD1CTD8CXCRCPD0 D2D3D8CPB9 D8CXD3D2 CUD3D6 D7DDD2D8CPCRD8CXCR CSCTD7CRD6CXD4D8CXD3D2BA C4CPD2CVD9CPCVCTB8 BEBLBMBGBJDFBHBKBA C2D3D2 BUCPD6DBCXD7CT CPD2CS CAD3CQCXD2 BVD3D3D4CTD6BA BDBLBKBDBA BZCTD2CTD6CPD0CXDECTCS D5D9CPD2D8CXACCTD6D7 CPD2CS D2CPD8D9D6CPD0 D0CPD2CVD9CPCVCTBA C4CXD2CVD9CXD7D8CXCRD7 CPD2CS C8CWCXB9 D0D3D7D3D4CWDDB8BGBA CBDDD0DACXCT BUCXD0D0D3D8 CPD2CS BUCTD6D2CPD6CS C4CPD2CVBA BDBLBKBLBA CCCWCT D7D8D6D9CRD8D9D6CT D3CU D7CWCPD6CTCS CUD3D6CTD7D8D7 CXD2 CPD1CQCXCVD9D3D9D7 D4CPD6D7CXD2CVBA C1D2 C8D6D3CRCTCTCSCXD2CVD7 D3CU D8CWCT BEBJ D8CW BTD2D2D9CPD0 C5CTCTD8CXD2CV D3CU D8CWCT BTD7D7D3CRCXCPD8CXD3D2 CUD3D6 BVD3D1B9 D4D9D8CPD8CXD3D2CPD0 C4CXD2CVD9CXD7D8CXCRD7 B4BTBVC4 B3BKBLB5B8 D4CPCVCTD7 BDBGBFDFBDBHBDBA CECTD6AJD3D2CXCRCP BWCPCWD0 CPD2CS C5CXCRCWCPCTD0 BVBA C5CRBVD3D6CSBA BDBLBKBFBA CCD6CTCPD8CXD2CV CRD3D3D6CSCXD2CPD8CXD3D2 CXD2 D0D3CVCXCR CVD6CPD1D1CPD6D7BA BTD1CTD6CXCRCPD2 C2D3D9D6D2CPD0 D3CU BVD3D1D4D9D8CPD8CXD3D2CPD0 C4CXD2CVD9CXD7D8CXCRD7B8 BLB4BEB5BMBIBLDFBLBDBA C2D3CWD2 BWD3DBCSCXD2CVB8 CAD3CQCTD6D8 C5D3D3D6CTB8 BYD6CPD2AOCRD3CXD7 BTD2CSCTD6DDB8 CPD2CS BWD3D9CVD0CPD7 C5D3D6CPD2BA BDBLBLBGBA C1D2D8CTD6D0CTCPDACXD2CV D7DDD2D8CPDC CPD2CS D7CTD1CPD2B9 D8CXCRD7 CXD2 CPD2 CTCRCXCTD2D8 CQD3D8D8D3D1B9D9D4 D4CPD6D7CTD6BA C1D2 C8D6D3CRCTCTCSCXD2CVD7 D3CU D8CWCT BFBED2CS BTD2D2D9CPD0 C5CTCTD8CXD2CV D3CU D8CWCT BTD7D7D3CRCXCPD8CXD3D2 CUD3D6 BVD3D1B9 D4D9D8CPD8CXD3D2CPD0 C4CXD2CVD9CXD7D8CXCRD7 B4BTBVC4B3BLBGB5BA C2D3D7CWD9CP BZD3D3CSD1CPD2BA BDBLBLBLBA CBCTD1CXD6CXD2CV D4CPD6D7CXD2CVBA BVD3D1D4D9D8CPB9 D8CXD3D2CPD0 C4CXD2CVD9CXD7D8CXCRD7B8 BEBHB4BGB5BMBHBJBFDFBIBCBHBA BXBA C3CTCTD2CPD2 CPD2CS C2BA CBD8CPDACXBA BDBLBKBIBA BT D7CTD1CPD2D8CXCR CRCWCPD6CPCRD8CTD6CXDECPB9 D8CXD3D2 D3CU D2CPD8D9D6CPD0 D0CPD2CVD9CPCVCT CSCTD8CTD6D1CXD2CTD6D7BA C4CXD2CVD9CXD7D8CXCRD7 CPD2CS C8CWCXD0D3D7D3D4CWDDB8 BLBMBEBHBFDFBFBEBIBA CACXCRCWCPD6CS C5D3D2D8CPCVD9CTBA BDBLBJBFBA CCCWCT D4D6D3D4CTD6 D8D6CTCPD8D1CTD2D8D3CU D5D9CPD2B9 D8CXACCRCPD8CXD3D2 CXD2 D3D6CSCXD2CPD6DD BXD2CVD0CXD7CWBA C1D2 C2BA C0CXD2D8CXCZCZCPB8 C2BAC5BABXBA C5D3D6CPDACRD7CXCZB8 CPD2CS C8BA CBD9D4D4CTD7B8 CTCSCXD8D3D6D7B8 BTD4D4D6D3CPCRCWCTD7 D8D3 C6CPD8B9 D9D6CPD0 C4CPD2CVCPD9CVCTB8 D4CPCVCTD7 BEBEBDDFBEBGBEBA BWBA CACXCTCSCTD0B8 BWD3D6CSD6CTCRCWD8BA CACTD4D6CXD2D8CTCS CXD2 CABA C0BA CCCWD3D1CPD7D3D2 CTCSBAB8 BYD3D6D1CPD0 C8CWCXD0D3D7D3D4CWDDB8 CHCPD0CT CDD2CXDACTD6D7CXD8DD C8D6CTD7D7B8 BDBLBLBGBA CFCXD0D0CXCPD1 CBCRCWD9D0CTD6BA BEBCBCBDBA BVD3D1D4D9D8CPD8CXD3D2CPD0 D4D6D3D4CTD6D8CXCTD7 D3CU CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS CSCXD7CPD1CQCXCVD9CPD8CXD3D2BA C1D2 C8D6D3CRCTCTCSCXD2CVD7 D3CU D8CWCT BFBLD8CW BTD2D2D9CPD0 C5CTCTD8CXD2CV D3CU D8CWCT BTD7D7D3CRCXCPD8CXD3D2 CUD3D6 BVD3D1B9 D4D9D8CPD8CXD3D2CPD0 C4CXD2CVD9CXD7D8CXCRD7 B4BTBVC4 B3BCBDB5B8CCD3D9D0D3D9D7CTB8 BYD6CPD2CRCTBA CBD8D9CPD6D8 C5BA CBCWCXCTCQCTD6B8 CHDACTD7 CBCRCWCPCQCTD7B8 CPD2CS BYCTD6D2CPD2CSD3 BVBAC6BA C8CTD6CTCXD6CPBA BDBLBLBHBA C8D6CXD2CRCXD4D0CTD7 CPD2CS CXD1D4D0CTD1CTD2D8CPD8CXD3D2 D3CU CSCTB9 CSD9CRD8CXDACT D4CPD6D7CXD2CVBA C2D3D9D6D2CPD0 D3CU C4D3CVCXCR C8D6D3CVD6CPD1D1CXD2CVB8 BEBGBMBFDF BFBIBA References Kazimierz Ajdukiewicz.
1935. Die syntaktische konnexitat.
In S.
McCall, editor, Polish Logic 1920-1939, pages 207231.
Oxford University Press.
Translated from Studia Philosophica 1: 127.
Yehoshua Bar-Hillel.
1953. A quasi-arithmetical notation for syntactic description.
Language, 29:4758.
Jon Barwise and Robin Cooper.
1981. Generalized quanti ers and natural language.
Linguistics and Philosophy, 4.
Sylvie Billot and Bernard Lang.
1989. The structure of shared forests in ambiguous parsing.
In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics (ACL '89), pages 143151.
Veronica Dahl and Michael C.
McCord. 1983.
Treating coordination in logic grammars.
American Journal of Computational Linguistics, 9(2):6991.
John Dowding, Robert Moore, Francois Andery, and Douglas Moran.
1994. Interleaving syntax and semantics in an ecient bottom-up parser.
In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL'94).
Joshua Goodman.
1999. Semiring parsing.
Computational Linguistics, 25(4):573605.
E. Keenan and J.
Stavi. 1986.
A semantic characterization of natural language determiners.
Linguistics and Philosophy, 9:253326.
Richard Montague.
1973. The proper treatment of quanti cation in ordinary English.
In J.
Hintikka, J.M.E.
Moravcsik, and P.
Suppes, editors, Approaches to Natural Langauge, pages 221242.
D. Riedel, Dordrecht.
Reprinted in R.
H. Thomason ed., Formal Philosophy, Yale University Press, 1994.
William Schuler.
2001. Computational properties of environment-based disambiguation.
In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL '01), Toulouse, France.
Stuart M.
Shieber, Yves Schabes, and Fernando C.N.
Pereira. 1995.
Principles and implementation of deductive parsing.
Journal of Logic Programming, 24:336.
 The Elimination of Grammatical Restrictions M.
Salkoff and in a String Grammar of English N.
Sager Institute for Computer Research in the Humanities New York University, New York i.
Sun~nary of String Theory In writing a grammar of a natural language, one is faced with the problem o f e x p r e s s i n g grammatica NVN number: sequence N1 and N 2 (N, noun: V, verb), the subject The boy eats the meat; Q N1 P N2 N For example, i n t h e s e n t e n c e form and the verb V must agree in Or, in the five feet in length, One of the ~ The boys eats the meat.
e.g., (Q a number; P, preposition), are of particular subclasses: theories of linguistic structure which is particularly relevant to this problem is linguistic string analysis[1].
In this theory, the major syntactic (a string is structures of English are stated as a set of elementary strings a sequence of word categories, e.g., N V____NN, N V P N, eta).
Each sentence of the language consists of one elementary sentence (its center string) plus zero or more elementary adjunct strings which are adjoined either to the right or left or in place of particular elements of other elementary strings in the sentence.
17.~ The elementary strings can be grouped into classes according to how and where they can be inserted into other strings.
an elementary string, X If Y = X 1 X 2. . . Xn is ranging over the category symbols, the following classes of strings are defined: left adjuncts of X: adjoined to a string Y to the left of X in Y, or to the left of an ~X adjoined to Y in this manner.
rX right adjuncts of X: adjoined to a string Y to the right of X in Y, or to the right of an rX adjoined to Y in this manner.
replacement strings of X: adjoined to a string Y, replacing X in Y.
sentences adjuncts of the string Y, adjoined to the left of X 1 or after X i in Y (l~ i ~ n), or to the right of an Sy adjoined to Y in this manner.
Cy, i conjunctional strings of Y, conjoined after X i in Y (i< i < n), or to _ _ the right of a Cy, i adjoined to Y in this manner.
z center strings, not adjoined to any string.
These string-class definitions, with various restrictions on the repetition ~and order of members of the classes, constitute rules of combination on the elementary strings to form sentences.
Roughly speaking, a center string is the skeleton of a sentence and the adjuncts are modifiers.
green we met in in An example of a left adjunct of N is the adjective A right adjunet of N is the clause whom the green blackboard.
the man whom we met.
in the sentence A replacement formula of N is, for example, The same sentence what he said What he said was interesting.
with a noun instead of a noun replacement string might be interesting.
since he left.
An example is are Examples of sentence adjuncts are The lecture was in general, at this time, The c strings have coordinating conjunctions at their head.
but left in He was here but left.
Examples of center strings He understood and also We wondered whether he understood.
The grammatical dependencies are expressed by restrictions on the strings as to the word subcategories which can occur together in a string or in strings related by the rules of combination.
Thus, in the center string N 1 V N2, the figrammatical dependency mentioned above is formulated by the restriction: if N1 is plural, theh V does not carry the singular morpheme -_ss.
The string grammar with restrictions gives a compact representation of the linguistic data of a language, and provides a framework within which it is relatively simple to incorporate more linguistic refinement, restrictions.
J i.e., more detailed One may ask whether it is possible to write such a string grammar without any restrictions at all, i.e., to express the grammatical dependencies (restrictions) in the syntactic structures themselves.
In the resulting restrictionless grammar, any elements which are related by a grammatical dependency w i l L b e e l e m e n t s relations, other of the same elementary string.
No grammatical than those given by the simple rule of string combination, The result of this paper is to obtain between two strings of a sentence.
demonstrate that such a restrictionless grammar can be written [4].
In order to obtain a restrictionless form of a string grammar of English, we take as a point of departure the grammar used by the computer program for string decomposition of sentences, developed at the University of Pennsylvania [2,3].
This gran~nar is somewhat more detailed than the sketch of an English A summary of the form of the computer grammar is In section 3 we show how the restrictions can be string grammar in Ill.
presented below in section 2.
eliminated from the gran~nar.
An example of a typical output obtained for a short sentence from a text of a medical abstract is shown in Figs.
1 and 2.
The decomposition of the sentence into a sequence of nested strings is indicated in the output by the numbering of the strings.
As indicated in line 1., the sentence consists of the two assertion centers in lines 2.and ~ ~ conjoined by and.
The line B  ficontains a sentence adjunct th~_~) on the assertion center as a whole . The assertion center 2 . is of the form N V A : Spikes would be effective . The noun spikes has a left adjunct (such enhanced) in line 5  as indicated by the appearance of 5 . to the left of spikes . The object effective has a left adjunct ~ 9 _ ~ ) in line 6 . and a right adjunct in line 7  In the same wsy, each of the elements of the adjunct strings may have its own left and right adjuncts.
Line IO . contains an assertion center in which the subject and the This zeroing is indicated in the modal verb (woul____dd)have been zeroed.
output by printing the zeroe~ element in parentheses.
The difference between the two analyses in Figs.
decomposition of the sequence in initiating analysis (Fig.
i an~ 2 lies in the In the first synaptlc action.
I), this sequence is taken as a P_~N right adjunct on of the effective, where initiating synaptlc is a left adjunct (onaction) form of a repeated adjective (parallel to escaping toxic in the sequence in eseap.ing toxic gases) . In the second analysis (~ig.
2), this same sequence is taken as a ~ right adjunct of effective, where initiating is the Ving, and synaptic action is the Object of initiating.
The Computer String Grammar.
In representing the string grammar in the computer, a generalized grammar where Y-.
= Y' IS where Y' is a grammar string like Y.
This system of nested gram~nar strings terminates when one of the grammar strings is equal to an atomic string (one of the word-category symbols).
The Y.
are called the options of Y, and each option Y.
consists of the elements Y... l l 13 Not every option of a grammar string Y will be well-formed each time the sentence analysis program finds an instance of Y in the sentence being analyzed.
Associated with each option Yi is a series of zero or more tests, called restrictions.
'If RiP is the set of tests associated with Yi then the grammar A restriction is a test (which will be descrfbed below) so written that if it does not give a positive result its attached option may not be chosen.
All of the restrictions in the grammar fall into two types: TypeA: The restrictions of type A enable one to avoid defining many The options of the grammar string Y similar related sets of grammar strings.
have been chosen so that Y represents a group of strings which have related filinguistic properties.
This allows the grammar to be written very compactly, and each grammar string can be formulated as best suits the linguistic data.
However, when a grammar string Y appears as a Y' ij of some other string Y', some of the options of Y may lead to . non-wellformed sequences.
In order to retain the group of options of Y and yet not allow non-wellformed sequences wherever options of Y which would have that effect are used, we attach a restriction of type A to th0s~ options of Y.
For example, let Y be where and YI = which Z V (e.g., which he chose) Y2 = what E V (e.g., what he chose) Then Y can appear in the subject Z of the linguistic center string CI: Cl = z v n What he chose was impDrtant.
This yields Which he chose was important; As it is defined here, Y can also be used to represent the wh-clauses in the right adjuncts of the noun: but in rN only the which option of Y gives wellformed sequences: 3 the book which he chose the book what he chose Hence a restriction R a is attached to the what option of Y (eq.
5) whose effect is to prevent that option from being used in rN.
Type B: With some given set of rather broadly defined major categories (noun, verb, adjective, etc).
it is always possible to express more detailed linguistic relations by defining sub-categories of the major categories.
These relations then appear as constraints on how the sub-categories may appear together in the grammar strings Y.
If some element Yij of Yi is an atomic string (hence a word-category symbol) representing some major category, say C, then Rb may exclude the subcategory Cj as value of Yij if some other element Yik of Yi has the value Ck.
Y i k m a y also be a grammar string, in which case R b m a y exclude a particular option of Yik when Yij has value C..
The restrictions Rb may be classified into three kinds: (a) Between elements of some string Y.
where the Y..
correspond to elements 1 i~ of a linguistic string.
For example, A noun in the sub-category singular cannot appear with a verb in the sub-category plural.
~ The man agree.
Only a certain sub-category of adjective can appear in the sentence adjunct P__AA : in general, (b) in particular, ~ in ha~py.
Between a Yij and a Yik where Yij corresponds to an element of a linguistic For example, string and Yik corresponds to a set of adjuncts of that element.
In rN, the string to V 2 cannot adjoin a noun of sub-categoryN 2 (proper names): the man to do the job ~ John to do the ~ob.
Only a certain adjective sub-category (e.g., re~/.e~, available) can appear in rN without any left or right adjunct of its own: the people present ; (c) ~ the people happy.
Between Yij and Yik ' where one corresponds to an element of a linguistic string and the other corresponds to an adjunct set which can repeat itself, i.e., which allows 2 or more adjuncts on the same linguistic element.
These restrictions enable one to express the ordering among adjuncts in some adjunct sets.
For example, Q (quantifier) and A (adjective) are both in the set N ' the left adjuncts of the noun.
However, _Q can precede A but A cannot precede _Q when both are adjuncts of the same N in a sentence: 3 Q A N books, but ~AQN e.g., five green e.g., green five books.
The string grammar defined by eqs.
i-3, together with the atomic strings (word-category symbols) have the form of a BNF definition.
The system with eq.
4, however, departs from a BNF definition in two important respects : (a) it contains restrictions (tests) on the options of a definition; (b) the atomic strings (word-categories) of the grammar have sub-classifications.
With the elimination of the restrictions, the computer grammar will again have the form of a BNF definition.
fi3. Elimination of the Restrictions The restrictionless string grammar is obtained from the grammar (in described above by the methods of (A) and (B) below.
Initially this paper), conjunctional restrictionless strings have not been included in the grammar.
We estimate that the addition of conjunctions/ grammar by a strings will increase the size of the restrictionless factor of about 5.
(A) The linguistic strings represented in the computer graz~,ar are reformulated in accordance with the following requirement.
any utterance of a language containing Given grammatical dependency obtains between A and B, the elementary strings of a restrictionless string grammar are defined so that A and B appear together in the same linguistic string, and any iterable sequence between A and B is an adjunct of that string.
Iterable sequences of the type seemed to begin to in It seemed to be~in to surprise him that we in It is said to be known worked seriously, or is said to be known to to surprise him that we worked seriuusly are analyzed as adjuncts.
If we place such sequences among the left adjuncts of the verb, v ' then the sentences above can be put in the form It~_v surprise him that we worked seriously fi~v = seemed to begin to ; However, when the adjunct by definition), surprise verb of ~v is said to be known to ; etc.
takes on the value zero (as can all adjuncts, sequence It then (9) above becomes the non-grammatical him that we worked seriously.
~v (seemed ~ This happens because the first and the latter is__) carries the tense morpheme, disappears when We separate the tense morpheme from the verb, and place it in the center string as one of the required elements.
(i0) C1 = Z t ~ V g; This formulation of the assertion center string C1 (lO), in which the tense morpheme is an independent element and iterable sequences are taken as adjuncts, is necessary in ord@r to preserve, for example, the dependence surprises him that we In the between the particle it and the succeeding sequence worked seriously: grammar~which ~ The book surprises him that we worked seriously.
includes restrictions, this formulation is not necessary because this dependence can be checked by a restriction.
(B) Turning to the computer form of the grammar, all the restrictions of the grammar are eliminated either by defining new grammar strings (for the elimination of the restrictions categories by the particular required by the restriction Ra) ' or by replacing the general wordsubclasses of those categories which are (to eliminate Rb).
The application of this procedure increases the number of strings in the grammar, of course.
The restrictions R a can be eliminated in the following manner.
Suppose the option Yi of Y has a restriction R a on it which prevents it from being chosen in Y' (Y is a Y'ij of Y').
Then define a new grammar string Y ' w h i c h ficontains all the options of Y but Y.
: Then the new gran~nar string Y* replaces Y in Y'.
R a on p.
5, the string Y* = which Z t fv V / ....
Thus, in the example of (in the modified treatment of tense and iterable sequences) would replace Y in r N.
The restrictions R b are eliminated in a different way, according to the types described on p.
6. (a) New strings must be written in which only the wellformed sequences In the example of subject-verb agreement, the of subcategories appear.
where N s and Np are singular and plural nouns, V s and Vp singular and plural verbs.
(b) If an element of a particular subcategory, say Ai, can take only a rAi is defined.
It subset of the adjuncts rA, then a new adjunct s~ring contains those options~_ of rA which can appear only with A i plus all the options of r A which are common to all the sub-categ0ries of A.
When this to rA, : has been done f0r  all A i having some particular behavior w i t h r e s p e c t all the remaining sub-categories A rA ~ AlrA1 of A will have a common adjunct string r a As many new sets rAi must be defined as there were special sub-categories A.
A similar argument holds for ~A and other adjunct sets which depend on A.
A new element corresponding to the/adjunct set must be defined in which the adjuncts appear correctly ordered with respect to each other, and each one must be able to take on the value zero.
This procedure for eliminating restrictions is also the algorithm for introducing further grammatical refinements into the restrictionless grammar.
Such a general procedure can be formulated because of an essential property of a string grammar: In terms of linguistic (elementary) strings, all a) between elements of a string, or b) between an restrictions are either element of the string and its adjunct, or same string.
c) between related adjuncts of the Further, there is no problem with discontinuous elements in a all elements which depend in some way on each other grammaticstring grammar: ally appear in the same string or in strings which are contiguous by adjunction.
The cost of the elimination of all restrictions in this way is about an order of magnitude increase in the number of strings of the grammar.
Instead of about 200 strings of the computer grammar, the grammar presented here has about 2000 strings.
It is interesting that the increase in the size of the This suggests that in a program Also, since grammar is not greater than roughly one order of magnitude.
there may be practical applications for such a grammar, e.g. designed to carry out all analyses of a sentence in real time.
the restrictionless grammar is equivalent to a B.N.F. grammar of English, it may prove useful in adding English-language features to programming languages which are written in B.N.F. fiSENTENCE N E U H I B  SUCE ENHANCED SPIKES WOULD BE MORE E F F E C T I V E IN I N I T I A T I N G SYNAPTIC ACTION AND THUS BE RESPONSIBLE FOR THE OBSERVED POST-TETANIC POTENTIATION  Ol I.
PARSE SENTENCE INTRODUCER CENTER AND Z, AND CI ASSERTION SUBJECT 5 . SPIKES VERB $ OBJECT gOULD BE 6.
EFFECTIVE RV T, ACVERB ADVERB THUS CONJUNCTION LN ARTICLE QUANTIFIER SUCH ADJECTIVE ENHANCED TYPE-NS" NOUN AEVERB PN PREPOSITION IN ACTION lO.
CI ASSERTION BE OBJECT RESPONSIBLE II.
LN ARTICLE QUANTIFIER ADJECTIVE INITIATING TYPE-MS SYNAPTIC NOUN PN POTENTIATION LN GUANTIFIER ADJECTIVE OBSERVED P O S T T E T A N I C TYPE-NS NOUN SENIENCE NEUH-.IB  SUCH ENHANCED SPIKES kOULD BE MORE E F F E C T I V E IN I N I | i A T I N G S Y N A P T I C A C T I O N AND THUS UE R E S P O N S I B L E FOR THE OBSERVED P O S T T E T A N I C P O T E N T I A T I O N  02 = |NTROOUCER CENTER AND Z.
AND 3 6 END MARK  PARSE SENTENCE CI VERB  kOULD BE RV T, ACVERB S ADVERB IHUS LN ARTICLE QUANTIFIER SUCH ADJECTIVE ENHANCED TYPE-NS NOUN lCVERB To P NS V I N G I O F | 0 = PREPOSITION IN SN INIIIATING ACTION CI ASSERTION VERB (WOULD) OBJECT RESPONSIBLE LN QUANTIFIER TYPE-NS NOUN PN = LP P R E P O S I T I C N FOR POTENTIATION QUANTIFIER ADJECTIVE OBSERVEO P O S T T E T A N I C TYPE-NS NOUN NG MCRE PARSES Conclusion 4.
This problem was suggested by Professor J.
Schwartz of the Courant institute of Mathematical Sciences, New York University.
5. The option Yi here corresponds to the linguistic string Y of the previous section.
The symbol / separates the options of a string definition.
Academic Press, REFERENCES 1.
Harris, Z.
S., . String Analysis of Sentence Structure.
Papers on Formal Linguistics, No.
l, Mouton and Co., The Hague, 1962.
2. Sager, N., Salkoff, M., Morris, J., and Raze, C., . Report on the String Analysis Programs.
Department of Linguistics, University of Pennsylvania, March 1966.
3. Sager, N., . Syntactic Analysis of Natural Language.
Advances in Computers (Alt, F.
and Rubinoff, M., eds.), vol.
8, pp.
153-188. New York, 1967 .
CONTEXTUAL GRAMMARS Solomon Marcus InsUtutul de Matematica Str, Mihal Eminescuo 47 BuchareSt 9, ROMANIA In the following, we shall introduce a type of generative grammars, called contextual grammars.
They are not comparable with regular grammarsBut every language generated by a contextual grammar is a context-ree language.
Generalized contextual grammars are introduced, which may generate non-cox,text-free languages.
Let V be a finite non-void set ; V lary.
Every finite sequence of elements in ia called a vocnbuV is said to be a string on V.
Given a string x = ala2...an, the number n is called the length of x.
The string of length zero is called the n tring and is denoted by r~J. Any set of strings on V is called a language on V.
The set of all strings on V (the null-string inclusively)is called the universal language on V.
By a nwe denote the string a...a, where a is iterated n times.
Any ordered pair (u,v~ of strings on V_ is said to be a contex~ on V.
The string x is admitted by the context <u,v> With respect to the language L if u~ G L.
Let .~ be a finite set of strings on the vocabulary V~ and let@be a finite se@ of contexts on V.
The triple (v,~, ~)) (1) is said to be a contextual l~rammar ; V is the vocabulary of the grammar, ~ is the ba_s_e_ of the grammar and ~is the co m~,-2textual ccmoonent of the grammar.
Let us denote by ~ the contextual grammar defined by (1).
Oonsider the smallest language L on Vj fulfilling the following two conditiom8 (~J Iz ~ and <u,v>,(~), th-~=,L.
The language L is said to be the lsmguage generated by the contextual grammar G.
This means that the language generated by G is the intersection of all languages L fulfilling the conai~ions (~) and (pj . A language ~L is said to be a eonteF~ual language if there exists a contextual grammar G which generates L.
Proposition i.
Eyer~ finite language is a cont~ual lanProo__f.
Let V be a vocabulary and let ~ be a finite lan.
guage on V.
It is obvious that the contextual grammar (V,L4jO), where 9 dauotes the void set of contexts, gauerates the language L I.
The same language may be gamerated by means of the .g  contextual grammar (V,I~, where is formed by the nu~ cont ex~ only.
Two contextual grammars are called e~uivalemt if they gemssame language.
The grammars CV,LI, O ) and (V,~,~ rate the are equivalent, since they both generate the language ~ The converse of Proposition 1 is not true.
Indeed, we have Proposition 2.
The universal language is a contextual language.
ProOf. Let V = ~alLa2,...~ ~.
De~ote by I~ the umiver.
Sal language on V.'T.et us put ~" S~.~.~ and t <~''i" " C -3~,a~,,...s(~,ian> ~ It is easy to see that thegrammar . (V,~ generates the universal language on V.
Remarks. If we put, in the proof Of Proposition 2, LI-V instead of h =~' then the grammar (V_,h,@) does not generate the universal language on V, since the language it generates dDes not contain the nu/l-strlng.
In order to illustrate the activity of the grammar (V,~, ~defined in the of let consider the proof proposition 2, US particular case when the vocabulary iS formed by two elements only : V =(a.b~.
The general form of a string x on V is x = a ~b ~a-~b~...a ~b~, where il, Jl, i2,j2,...,~,j ~ are arbitra~non-negative integers.
In order to generate the string x, we start with the null.string @@ and we apply il times the context ~,a~ . The result of this operation is the string a 11,to which we apply Jl times the context <~,b> and obtain the string al~ ~I . Now we apply i~ times the context <~,a>, than J2 ti~es the context ~,b_> and we continue so alternatively.
~hen, ~dter 2p-2 steps, we have obtained the J = ai bJlai b 2ooo LP" ib -i, it is ply .~ ti~es ~ne context ~@,~ and, to the string so obtained, jp times ~:e contex~ ~gb>, in order to generate completely Uhe string Xo Haskell Curry considered Ghe larlg~age L = {abn~ (n=l,2~..o) as a model of ~he set of natural numbers \[5~ o We call L the language of Curry.
Prooosiuion 3.
The language of Curry is a contextual langu~eo proof.
The considered language is generated by the grammar (V,LI,~), where V= ~a.b~, I~ =~a 3 -nd~ ~<~,b~.
We recall that a language is Said to be regular if it may be generated by means of a finite aatoma$on (or, equivalently, by means of a finite state grammar in the sense of Ohomsky).
Proposition 4.
There exis$~ a contextual language which is Proof.
Let us consider the language L = ~a-nb n} (n=l,2,)...
If we put V = {ab}, L 1 = ~ab~ and ~ ~<a.b>}, then it is easy to see that L is generated by the con~extuai grammar (V, LI, ~.
On the other hand, L is n~t a regular language.
This fact was assel~ed by Ghomsky in \[3~ and\[~\], but the proof he gives is wrong.
A correct proof of this assertion and a.discussion of Chomsky' s proof were given in \[~\].
and ~.
Propositions 2,3 and # show that there are many infinite languages w~ioh are oontextual.
This fact may be explained by means of P~posi~ion 5.
If the set ~ is non-void and if the set~ contains at least one non-nu/1 contex~ I ~hen the contextual gramma___r_r (V.Ll, ~ ~enerate s an infinite language.
Proof. Since L A is non-void, we may find a string x be@ longing to ~i o Since contains, at least one non-nu/\] context, @  let ~u,v~ be a non-null context belonging to . l~rom these assumptions, we infer that the strings, u2xv 2, . .,un~,,..
are mutually distinc~ and belong all to the language generated by the grammar (V,I~,).
Thus, ~his language is infinite.
The converse of Proposition 5 is true.
Indeed, we have / / -5Proposition 6.
If the contextual 6rammar (V, LI~ gau~ rates an infillite language, then ~Ll.
is non-void, whereas@ c0nrains a no~-nult context.
proof. Let L be the language generated by ~V)LI~.
If is void, L is void too, hence it cannot be infinite.
If contains no non-null context, we have L = L I.
But ~d is in any ease flni~e ; ~hus, L is finite, in contradictiom With the h vpothesis.
Since there are contextual language which are not regular (see Proposition 4 above), it would be interesting to establish whether all contextual languages are context-free ls~guages.
The amswer is affirmative : Proposition 7.
_Every contextua~ lan?.ua~e is a context-free PrP~oof.
Let b be a contextual language.
If L is finite, it is a regular language.
But i~ is well knowm that every regular language is a context-free language.
Therefore, L is a context-free language.
Nowe let us suppose that L is infinite.
Deao~e by G = (V,,L l, a contextual grammar which generates the language L, In view of Proposition 6, L I is non-void,wheream there exists an integer i, l~ i~p, such that the con~ext ~ui,vi~ is non-nu~ Joe.
at least one of the equalities ui =co, v i =~ is false.
Let us make a choice a~d suppose tha@ ~.i ~ ~ Let L~ = {xcA,xp_, ...
9~a} and (~) ={<ultVl>, ~.,U,.~,V."y} . We define a context-free grammar ~)..@| as follows.
The terminal vocabulary of ~ is V.
The nonterminal vocabulary of ~ contains one element onlydenoted by S which is, of course, the axiom of the grammar ~ . The ter-6minal ~ rul es o f ~' are, S -->_x 1, g--~x a, S.--* _Xn whereas uhe non-terminal rules are S ---> u~5% v-i ' S ---) uqS v~, It is obvious tha~ the number of terminal rules is equal to the number of strings in ~, whereas the number of non-terminal rules is precisely the number of conuexts in ~.
Among the nonterminal rules, there is one at least which is non-trivial : it is the rule S ---> UiS vv~., where '*4 {~ " It is not difficult to nrove that the grammar ~ generates the given language L.
Indeed, the general form of a string in L__ is where yG V and <~i, V~ >E~ for s = 1,2,...,p.
In order to generate the considered string we begin by applying --Jl $imes She rule In this way, we obtain the expression h The next step consists in applying J~2 times the rule v -7-a ~,~2 s vv~.~., which yields the expression J~ Ja -Ja -J~ s Ha N 1 ))t~..., -7! l Oontinuing in this way, we arrive, after pression,~z "ia ~-i s J~-\].
J2 ~a us-1 ~,a .... .'$1 "-%'i "'" ~12 ".% " Vie now apply j.p times the rule and thus we obtain the expression p-1 steps, to the ex--Jp.~ .~!2-1 . Ja ':ll ..J.l.,.i2. u jp-1 ~ S "Ull '~i2 ....
~-l . V~p ~-l "" vi2-V-il where, by applying the terminal rule S---@ Z, the considered string is completely generated, Thus, we have proved that L is contained in the language generated by ~, Conversely, let z be a string generated by ~ . The general form of this generation involves sev(ral consecutive applications of non-terminal rules (the number of these applications may be eventually equal to zero) followed by one and only one application of a terminal rule.
It is easy to see that the result of this generation  is always a string of the form (2).
Thus we have proved that the language generated by ~ is contaiued in L, In view of the precedim~ eonsiderations, L iS precisely the language generated by ~ o Proposition 7 easily permits to obtain simple examples of .....
J -8languages which are not contextual l~guages.
For instance, ~he l~~uage of Kleene~an~ (m=~,2,...), the first example of an infinite language which is not regular, is a very simple example of ~ contextual language.
It is enough to remark that the sequence ~n2} (~ = 1,2,)... contains nO subsequaucewhioh is an infinite ari~hmebio progression ~ (We have (n+l)2-n22~+l and lira (~+i)=~, therefore for every subsequence of ~n2} the difi'erance of two consecutive terms has the limiu equal to +oo wh~ n-@ ~ ).
But a result of\[4\] asserts, among others, that given am infinite contex~-f1~ee lan~ guage L, the set of integers which represent the len~hs of the strings in L contains an infinite arithmetic progression.
It follows uhab b~Je language of Kleene is not context-free and, in view of Prooosition 7, it is not a conbex~ual language.
T~ sa-~ ~a~T ~@low* ~,~ ~h~-,~ :3.A,~.
~ \[g'J, ~,,#.
A natural question now arrises : Do there exist non-contextual languages a~ong context-free languages 7 The affirmative answer follows fro~ the following remark : The converse of Proposition 7 is not true.
Indeed, we have Prooositiou 8.
There exists a.
cont e.~-free language which is not a contextual language.
Proof. Let V = ~a,b~.
In view of a theorem of Gru~Lkl ~ ~__.-----~there exists, for every positive integer _n~ a context-free language I~ on V, such that every context-free grammar of I~ contains at least n non-terminal symbols.
But, as we can see in the proof of Proposition 7, every contextual language may be generated with a context-free grammar containing only one non-~erminal symbol.
Therefore, if _n ~ 2, ~ is not a contex~usl l~guage.
Proposition 8 suggests the natural question whe~bsr ~bere exist regular languages which are not contextual lan~ages.
The I -9answer is affirmative : Pronosition 9There exists a regular language which is not a context ual language.
Proof~ Let us consider the laugaage L = {abm-~c~a.~ n,) ~,n= =1,2,...), which was used b~ H.B.Curry \[5\], in order to descrlbe the set of mathematical (true or not) propositions.
This language is regular, since it can be generated by the rules S--> Abj Ac->Ab, A.--~ Ba, B--> CCC., G_--~ ~, C--~ Db, .D_--~ a, We shall show that .L is not a contextual language, Tndeed, let us admit that the contrary holds and let G = <V,~, ~> be a contextual grammar of L_ 2 Here, the gene_.-al form of a string in L is ~ ""-us ~I x~ -~ ...
vi= (3), wh eas (t = 1,2,...,=) where "'',Pn are arbitrary positive integers.
This means that ul,~2,... .---,_Un, Vl,Y2,-..,v ~ in the expression (3) are formed only by those elements of V whnse number of occurences in the strings of L is unlimited.
Only h satisfies this requirement.
It follows that in any string of .L both occurrences of sand the occurence of ~ are terms of the string x in (3).
But this implies that the intermediate terms between the occurrences of a are terms of x, hence we can find two strings y and, m l The string y is obvioasly .the null-string ~o the form 11~., hence " z such that,whereas z is of But m may be here an arbitrary positive integer.
Therefore, since -loX6~, it follows that ~ is an infinite se~ of mtrimgs.
This fact contradicts the assus~tion concern_tug G ! v, is ~t a contextual language and Proposition 9 is proved.
The contextual grammars may be generalized in order to generate some lauguages which are not context-free.
A generalized contextual ~r~mmar is a quadruple G =~,,L2, ~, where V, L I and ~have the same meal~g as in bhe definition of a contextual grammar, whereas J'2 is a finite set of strings on the vocabulary V.
We define the language L G generabed by G in the following way : Y~ is a language on V a~d xe~ if and only if we may e~press x in the form .where z~, y~Le, <ui,Yi>~for i : 1,2,...,n and pl,P2,...,pn, p are positive integers such that pl+P2..,~n=p. Every language generated by a generalized contextual grammar is said to be a generalized contextual lsnguage.
I~, in the delini~ion of G, we take L~ =~c~}, G is equivalent ~o a contextual grammar ! the lang,.% is then precisely the language generated by the contextual grammar ~V,LI~.In_ deed, the general form of a string in the contextual language generated by ~Y~LA, ~ is l P~a Pn Pa P2 Pl p roy ed ~roposition lo.
\]~ery oontex~ual language ~s a ~eneralized context ual lan~uaKe, llWe may consider a conte~ual grammar as a parbicular case of generalized contextual grammar, .by ideatifyimg the contextual grammar ~'~1~ with the generalized contextual grammam~,V,,~, " It is interesting to point out that somet~imes a cont~ual language may be easy generated by a generalized contextual grammar which is not a contextual grammar.
For instance, let.
us consider the l~.~e L= (~=} (~X,2,)...
. ~ ~is, or the proof of ProDosition A, L is a contextual language.
We map generate L by the generalized contextual grammar (which is not a co=textual ~r~a~) <v, h~>, where v : {~,b}, .Li_\[c~}, ~ = Ibm, ~= \[a,~ . It is known that,~_ is not reS~L%ag.
We ma~ give a similar example, wi~h a language which is regular.
In this respect let us consider the language of G~x~V~.~.
In view of Proposition 5, it is a contextual language.
It is a regular language too~ since it may be generated by the regular gramm~r contain~ ~he following two rules : q--~ Sb and S--> a.
Now let us consider the generalized contextual grammar < ~i' This grammar generates the language of Curlew, but if'is not a cent ext,~ al gran~nar.
~ow let us show that generalized contextual languages are an effective generalization of contextual languages.
Propo .sit ion ii.
Th ere _ exist s a_g en=e=~ iaed_gA~nt ext ua! language which is ~IQ~ a eon~ext~sl language, ~, Let us consider the language T, = an_b.n~..
n} (n:=-l,2,.
4 It
is known that this language is not context-free (see,or instance,66\],p.~).
7n view of Proposition 7, every contextual 12language is a context-free language ; hence~ ~ is not a con.
textual language.
Now let us consider the generalized contextual gr~m~ G = <V,~,~2,~>, .here v = ~,~, ~ ~,,~{~ and~ ~(~a>~ . It is easy to see that G generate the ieaguage L.
Yrom the proof of Proposition ll it follows immediately; Proposition 12.
There exists a ~eneralized contextual lang u_~e which is not a ~.nnteYt-f~ee language.
We may now ask whether the converse of Proposition 12 is true.
The answer is given by Proposition 13.
Th ere exists a cont ext-free~a~e~ even a regular language,~ which is.,not a generalized contextual !~ua~e.-~ P#oof.
We may consider the language L = ~sbmc_abn-} (~,n= =1,2,)... used in the proof of Proposition 9.
It was showed in the proof of Proposition 9 that L is regular.
Let us admit that ~ is a generalized contextual language.
Given a string x in L, its representation is of the form Pl P2 Pn P P~ P2Pl ~m ~ : ui u~ ....~n..~.
y'v". ...
v2v i where ~ui,vi~ ~ (i = 1 .....,n),ZG~, y~L2,pl+...+pn = p end G = ~V, L1,L~, ~ is the grsmmar of L.
By a reasoning similar to "that used in the proof of Proposition 9, we find that for every positive integer m there exists a string z in \]i I such that z = abmcab s~, where s is a non.negative integer, depending mf m.
But thls means that ~ eontain~ infinitely ma~ strips.
This fact con... tradicts the definition of a generalized contextu~ grammar.
It / L 13 follows that L is not a generalized contextual language, It is to be expected ~hat every generalized contextual language is a contex~-s~itive language.
But the construction of the corresponding context-sensitive grammar seems to be very complicated, if we thin~ to the generation of the language ~u.A.~reider has introduced a new type of grammars, called gralamatlkl) and defined i~ neighborhood ~ira~.L~ars (okrestnostnye ' the following way (\[4o); see ~4\].
Our presentation is some what different).
Given a finite set V called vocabulary, two strings x and y on V, and a context <u,v> on V, We say that the pair ~u.v>,y) is a neighborhood of y with respect to x if we can find two strings z and w, such that x=zu~vw.
Every pair of the i or~ ~<u,v>, ~\], where ~u,v> is a context on ~, Whereas y is a string on V, is called a neighborhood on V.
Let us consider an element e which does not belong to V ; G will be called the bo~3dary element.
A neighborhood grammar is a triple of the form ~ V, e,~, where V is a vocabulary, is the boundary element and ~is a finite set of neighborhoods on the vocabulary VU(e} . Let L be a l~aguage on V.
2e say that L is generated by the considered neighborhood grammar if ~i every string x of the form x =~ye (with ymL).and only in such strings there ~ists in ~, far every tera a i of X=~la2...a s, a neighborhood of a i with respect to x.
Neighborhood gray, mrs are closely related to the notion of context, since this notion occurs in the definition of a neighborhood.
There is another notion, due to Ja.p.L.Vasilevski~ and 14 ~.V.Ghom~ak6v (see ~he refermnce in~2\],p,~o), which e~lains this fact.
Following these authors, a grammar of contexts (this name is imp_roper, since no context occurs among its objects) is a triple <V, e,9>, where .V and @ have the s-me meaning as in the definition of a neighborhood grammar, whereas Q is a finite set of strings on the vocabulary Vt3e~  This grammar generates the language _L on V in the following way : x6 if and only if for every string y and a~y strings z and w for which there exist strings u and v such that @ x@ = = uzyuv we have either l) y = rasp, where sE Q, whereas the strings m and p may be eventually or 2) (~x@ = urynt, where qr = z, n t = w mad ryn is a string belong~g to Q.
A string belonging to Q is said to be closed from t~ le~ (from the right) if its first (last) term is @ . A string belonging to Q is said to be ~ if it is closed bosh from the left and f~m the right.
A grammar of contexts is said ~o be k-bounded if every non-closed string of _~ is of length _k, whereas every Clesed string of ~ is of length not greater than _kj An important theorem of Bor~ev asserts the equivalance between languages generated by neighborhood grammars and languages generated by k-bounded grammars of con~s (~3,p.4o).
Since grammars of contexts and contextual grammars have some similarities in their definitions, it is Interesting to establish more ~xac~ly the relation b~een them.
v 15 Proposition 14.
There exists a contextual language ~hioh is regular, but which is not a neighborhood language.
Proof. Let us consider the language L = ~a~n~ (n=l,2,...).
This language is regular, since it is generated by the regular grammar consisting in the rules S ~ ~a, T--->Ua, U--->Ta, --->a, where ~ is the start symbol, La~ is the terminal vocabulary, whereas {S,T,U} is the non-terlainal vocabulary.
Let us consider the contextual gramnu~r G =~ {a},{CO}, {~a,a>~.
I@ is easy to see that G generates the language ~ $ therefore L is a contextual language.
We shall show that L is not a neighborhood language.
In this respect, our method will be the following.
We shall consider all systems of possible neighborhoods of the terms of ~he string 0aae and we shall show t~}at every such sysbem is either a system of aeighborhoods of the ~erms of every string Cane (n= = 2,3,4,)... or it is not a system of nei@\]borhoods of the terms 0t the string ea@e . It is easy to see that the first ~erm of the string @aa@ admits ~he following neighborhoods : 1)e, 2) Ca, 3) @aa, ~) eaa~ . The second term has the neighborhooas : l) G_a,~)-a~) aa, 4) ~e ., 5) e_a_a, 6) e_~ae.
The neighborhoods of the third term are : i) e@a, . 2) aa, ~) a, ~) _ae, 5) eaa8, 6)_aaE) . The lass term has the neighborhoods 1) 8, 2) _a~_, 3) a a@_, 4) @aa~ . The noration _u_xv.
represents hier the neighborh~d {<u,v>,x} . It is easy to see that the fourth neighborhood of the firs~ and of the lass term c~t b'e a neighborhood of e with respect @o @g48 . On She other hand, a is a neighborhood of .aa with respect to ea~@ for every n = 1,2, ....
It follows that no 16 neighborhood grammar of L = ~a2ZX 3 may contain one of She neigh.
borhoods _0a2@, Q a2~ and a.
Thus, if a neighborhood grammar of \]~ exists, it contains at leas~ one neighborhood from every group of the following four groups of neighborhoods : ~) _0, _~a, _ea 2 . ~) e_a, _aa, _aae, G~a, ~.aaO.
b') 6~-~, aa, ..aO, ea_a~, agO.
We shall consider all possible combinations betweau a neighborhood of the group ~ and a neighborhood of the group E . By mn we shall denote the combination formed by the m.th neighborhood of ~ and the n-th neighborhood of ~ . It is easy to see Chat every neighborhood grammar containing one of the combinations 12, 22, 23, 25, 42 generates a language whioh eontain~ every string a n with n $ 2.
On the other hand~ every neighborhood grammar containing one of the combinations ll, 13, 14, 15, 21, 24, 31, 32, 33, ~, 35, 41, 43, 44, 45, 51, 52, 53, 5~, 55 generates a language which either does not contain the string a 4 or contains every string a n with n~ 2  (This depends on the fact if the neighborhoods aa or aa belong or not to the considered neighborhood grammar).
Thus, there exists no neighborhood grammar which generates the language ~2n 3.
But the definition of (generalized) contextual grammars, though adequate to the investigation of the generative power of purely contextual operations, does not correspond to ~he situation existing in real (natural or artificial) l~guages, where every string is admired only by some contexts and every o~u~ / 17admits only some strings.
Let us try to obtain a type of grammar corresponding to this more complex situation.
We define a con___y textual grammar with choice as a system G_ =<V,L,~,~o>, where V, L1 and~are the objects of a contextual grammar, whereas is a mappi~ defined on the universal language on V and havi~ the values in the set of subsets of~.
We define the language generated by G as the smallest language L having the follow1  ~ L l x ~ L 2  ing properties : If x, ! If ye L, <u,y>6 ~(y) and Z&~l, then u~L, z v~L and ~L.
Thus, every strin~ chooses some contexts and every context chooses some strings.
We define a contextual language with choice a language which is generated by a contextual grammar wit~oioe.
The investigation of these grammars and languages would better show the generative power of contextual operations, in a manner which corresponds to the situation existing in real languages.
--~$ References i.
Y.BAR-HILLEL, ~I.PEEL~, E.SHA~R : On formal properties of simole phrase structure grammars.
Zei~schrift fur Phonetik, Sorachwissensc~aft und ~ommunikatio~forechung,vol.14,1961, p.145-172. 2.
V.B.BOP~EV : O krestnostn~e gram~atiki.
Nau~o-Techni~eskaja Informacija, Serija 2, 1967, ~o.ll, p.39-41. 3.
N.GHO~SKY : Three models for the description of language.IRE Transactions on Information Theory, IT-2, 3,1956, p.ll~-12@. 4.
N.~{O~KY : Syntactic Structures ~ Gravenhage,1957.
5. H.CURRY : Some logical aspects of ~ra~matical structure, proeeedings of the Symposium in Applied ~athematics, vol.12, S~ructure of language and its mathematical aspects, Amer~ath.
~oc., 1961, p.56-68. 6.
S.GI~SBURG : The mathe~atical ~heo~f context-free languages.
~cGraw-Hill Book Company, New York, 1966.
7. J~XA : On a classification of context-fre~lauguages.
Kybernetika, vol.3, me.l, 1967, p.22-29. 8.
S.~CUS : Gramatici ~i automate finite.
Editura Academiei R.P.R., Bucure~ti,196@.
9. S.~AR~JS : Su~_ les grammaires & un hombre d' ~tats fini.
Ca. hiers de lin~uistique th@orique et appliqu~%vole~,196~,p,~W~-~&@.
io, Ju,A.~REIDER : 0krestnos~naja model jazyka, Tru~ 8impoziuma pO primenenijam poro~.daju~ioh grammatik, Tar~u,septjabr~1967.
ii. Ju.A.~REIDER : Topologi~eskie mod~ll dazvka.
Vsesojuzz~yi stitu$ nau~noi i techni~eskoi informacii, ~oscou,1968 .,., , SERGE BOISVERT ANDI~ DUGAS DENISE BI'LANGER OBLING: A TESTER.
FOIL TR`ANSFORMATIONAL' GKAMMAKS ~.
INTRODUCTION Transformational grammars have developed with recent research in linguistics.
They appear to be a powerful and explicit device for characterizing the description of sentences; they also meet conditions of adequacy that can be applied to check that a sentence, or a set of rules, is well-formed.
A transformational grammar tester is part of a strategy for the selection of a well-formed grammar matching the data base.
To put it more explicitly, a tester of this sort should provide the linguist a class of the possible grammars which concerns precisely the linguistic theory.
These grammars have the form given by CUOMSKY in Aspects (see bibliograIShy).
2. GENERAL DESCRIPTION OF THE SYSTEM O~UNO is a .program for dealing with the structures of the French language: it performs the verification of phrase structure rules, the derivation of sentences according to the transformational component and the graphic illustration of the intermediate or final structures.
In the program, UNG is the routine that controls all the subroutines and the matching of the input structures with those allowed by the phrase structure rules.
If the matching is impossible, a comment is Acknowledgments.
This work was supported in part by Canada Council of Arts grants @69-0404 and @71-0819.
We are also indebted to the staff of the Computer Center of the Universit4 du Qu4bec ~ Montr4al for providing computing facilities, and giving this project high priority.
David Sankoff, of the Universit4 de Montr6al, is also responsible for the first version of the tree editing program, l~inaUy, Jossdyne G4rard helped debugging linguistic rules.
122 SERGE BOISVERTANDIL~ DUGASDENISE BI~,/ANGER ieeIIIItiIeletlIillllllelIlIlIllll~ : o :.
-*:: Iieeee~ ...........................~> ...........................9= ~z *,0,~o : .....
~Z ...................
~g o D~O o aO OUd 3N1 z o..~ ~ 3z O.
~ Z o k9 bO.
~3J ATd o:r =:~ o 0~3 qD_~  :: u.
0. tk NNN IOH U~ ~ Uj~ (~ ~ZD~-Z NUd ~dO NWZDU~ Fig.
1. Tree for an input sentence OBLING: A TESTER FOR TRANSFORMATIONAL GRAMMARS 123 made.
Otherwise, the output gives the graphic illustration of the tree for this sentence, or the input structures are immediately processed using transformational rules.
For example, General transformational rules are operated by a number of subroutines of which the main are explained hereafter.
3. GENERAL CAPACITIES OF THE SYSTEM The system OBLING is divided into four parts: a main program LING, the service subroutines, the phrase structure grammar tester and the transformational grammar testers.
LING and the service subroutines are stored in the central memory while the two grammars testers operate on disks.
The main program invokes the various linguistic rules and controls the application of these rules to the data base structure(s) or the derived structure(s).
The service subroutines are called by the routines concerning the application of the transformational rules and work in parallel with LING during the processing.
Phrase structure grammar tester " LING Service I Subroutines (processing memories) 4--1~ ' grammar testers Fig.
2. The OBLING system 4.
SPECIFIC CAPABILITIES OF THE PROGRAM LING The program LING will first initialize the working areas.
Then, it loads and operates the program V~rlCATEU~ which, after the reading and the verification of the input data, returns control to LINe.
124 SERGE BOISVERTANDI~ DUGASDENISE BELANGER ZING will then load and execute, using an overlay technique, the small control programs cYcH Q1, CYCLI Q2 ....., cYcLI Qi.
Each of these handles, in conjunction with HNG, the mapping on the input structure of a fixed number of transformation rules.
In the current version of the program, cYctI Q1 deals with the linguistic transformational rules T1 to Ts included, cYcrI Q2 the rules To to T10 included, etc.
The total number of these control programs cYcrI Q depends on the memory space allowed; processing is most efficient if the number of these control programs is as small as possible.
5. INFORMATION PATTERN BETWEEN LING AND VERIFICATEUR When VERI~CATEUR (the phrase structure grammar tester) is in memory, the structure to be analysed is read from the standard input unit (punched cards) and is processed by the subroutine Cr~RB~ to LING v V~ICATBD~ c'zcLi qO  T CKARBRE ARBRE verification printing of syntagmatic of the tree rules Fig.
3. The Vm~L~CAWSUa program (see figure 1 for updated tree and structure) OBLING: A TESTER FOR TRANSFORMATIONAL GRAMMARS 125 be validated.
This subroutine first checks if the phrase structure is consistent, then calls up eke which tests the names of the constituents describing the structure; finally, it compares this structure with those allowed by the phrase structure rules.
When errors are discovered during the processing, various sorts of comments are printed and followed if possible by a partial or full tree of the sentence.
When updating is done, the tree is printed and the program VERrFACATEUR passes control to LING.
The following illustrations concern first, the program VERIHCATEUR and second, an example of an updated tree and structure.
6. INFORMATION PATTERN BETWEEN LING AND THE TRANSFORMATIONAL GRAMMAR TESTERS Each time LING receives the control from VERIFICATEUR, that is, when no further errors have been detected, it loops in order to call successively the monitors CYCLI ql ....., CYCLI Q9 which contain up 45 different rules; we suppose that we are working now with a specific version of a grammar.
The first of these monitors has the following structure.
Transformational rule # 1 Transformational rule # 2 Transformational rule # 3 Transformational rule # 4 Transformational rule # 5 Fig.
4. The cYcLI Q1 program When CYCLI Q1 gets control, it is botmd to the application of 7'1,  .., Ts which correspond to the first five transformational rules; then control is switched to LING which calls cYCLI Q2.
The programs CYCLI qn process cyclic rules and the output structure is the input structure for the following rule.
When all the cyclic rules have been applied to the input structure, LING starts over again at CYCLI Q1.
If no modifications 126 SERGE BOISVERTANDR~ DUGASDENISE BI~LANGER to the already processed structures occur, or if new errors are discovered, control returns to LING.
After all the cyclic rules have been applied, the post-cyclic rules are processed in a similar manner: cYcu qA comprises the first five post-cyclic rules CYCLI Q~, the five following, and so on.
This chart illustrates the general interaction between the programs for the processing of cyclic or post-cyclic rules.
I_ cYcI.t Q1 cYcu Q2 CYCI, I Q9 CYCLI QA I CYCLI QB cYC~i Qi I End Fig.
5. Flow of Control between control programs under the direction of LInG 7.
SERVICE SUBROUTINES They are implemented within the main monitor ZING.
All but a few of these subroutines are called during the execution of the routines corresponding to the 88 rules, that is during the phrase structure analysis or the mapping of n structures.
A short description of the main subroutines follows: ^R~ (tree).
This subroutine is responsible for printing the tree.
At the input, we find a vector D of ND elements which represents the tree.
The horizontal distance for printing is calculated along with the total number and the relative position of these nodes; the vertical one is fixed.
OBLING: A TESTER FOR TRANSFORMATIONAL GRAMMARS For example, fACHE~ 2 # NOMRRE DE NOEUDS NOMBRE DE NOEUDS CHA/NE 1 = $ O(.
I) 2 = LE D( 21 3 = N D(3) 4 = V O( ~) 5 = $ D( 5\] 6 = $ D( 6) 7 = PRP D( 7) 8 = OIJE O( 8) g : LE O( 91 I0 = N D(IO) 11 = V O(ll 12 = $ D(12) 13 : DET D(13) 14 : DET 0(14) 15 : GN D(iS) 16 = GV D(16) 17 = C D(17) 18 = GN O(18) 19 = GV D(lg) ~0 = P O(~O) ~I = P D(21 21 TERMINAUX = 2'0 = 13 = 15 = 16 = 20 = 21 = 17 = 17 = 16 = 18 ) = Ig = 21 = 15 = 18 = 20 = 20 = 21 = 21 = -I ) = -I 12 ARBQRESCENCE NON PRODUITE SUR DEMANDE : AUCU~E REGLES IGNORFES SUR DENANDE t AIJCUNES Fig.
6a. Representation of the tree in memory 127 FIW~ f~ 7.1.
OT~ (Remove).
This subroutine is needed when nodes are erased; another subroutine, NEWTgF_~ will erase the nodes.
In the example below, oxv sets D(6), D(7), 9(13) to zero, and NEWTREE erases nodes numbered 6, 7 and 13.
If node 12 was also erased, OT~ and NEWT~E would have erased node 28 automatically.
The same holds for the node 32, where all the nodes between 6 and 13 would have been erased.
7.2. DFER, DFERX, GFER, GFERX.
Except for a few details, these four subroutines do the same work.
For example, Dr~RX \[I, J\] is applied to a structure J that has to be moved to the right under a dominating node L As illustrated below, Dr~Rx \[31, 30\] moves the structure headed by 30 to the right under the node 128 SERGE BOISVERTANDRE DUGASDENISE B~LANGER ....................~.
0":> ~ IJ-: .....
~g ............. 9~ : : ............
~ m : .....
~g ............
~ ~ ........... ..~z ~ : .....
~ ~w ....................~.
~Z~ ZU wz mz~wz ~ ~z~z m Fig.
6b. Corresponding printed tree OBLING: A TESTER FOR TRANSFORMATIONAL GRAMMARS 129 .......,.......,..,.....~........,.....~.~,     ,cu~ W~ud ~ : ".
...........o...............~ ....................~.
=> .~> g~ .............~z ~z ow .....
'.~ : ~J ......
~ : .....
~ ............. 9~  o ....~......,..................,...~> Fig.
7a. Sample tree before OTE and NEwra~ apply ~z~wz ~z~wz wz wz~o ~ w O~d 3NI II O~Do H~N ~ZD~ ION N~d dOD HN  NH" JI  + o=~Do 06d II ~NN ' I0~ O~DO 3~d dO0 ~AV ~I" ~ ~zz~ dN" c~o 130 SERGE BOISVERTANDP~ DUGASDENISE BELANGER,........,......,.......,................~, 8~ U oo.o o~ .oo*o--~ : ............
~ ...........................~ elost$ eset0 t eQ o o  " "  " o   o   "Z      oJo :     oz      .~ : ..... h~o :e ...... go ............
~ o . ~  --~ o.,' .......
., ...........  ....
.> m .....
0 ....  ........,..*0o2 ~z **.~,..,oo, o,,o-,o,~o.,,..,....o....,.oo.,~ ~ q~L0 tn>uz NgCW {gh.w DX~ 02 ~uu 08d JNr a.=~..=oc :~ o=II cz Frld 0~3 u~ .J m snN ION ado 0.
~--u o.
3Z :~d03 ~ 0.
N, u~-,x~..~h.Z NO" ~cxo..~ -~wl     oliN"  ~NI cc~ x'W-Jcc II Bo.
o_ ngd ~D~>O_:E0C GND r-o.J ~ :g uJ o_ HnN : o.
i.-.~ ~ u.
x: |ON H~d,o 3~d ~dO :g :~ ~ 0 dO3  ~ uJ .J~c EZZo.
~zzo--I + O~D W W Z D bJ .J O: _J,Xb.C~O.
Fig. 7b.
Sample tree where oT\]~ and N~WTREB have applied OBLING: A TESTER FOR TRANSFORMATIONAL GRAMMARS 131 31.
(Node 31 was created by rule T2 and DF~R was applied on the resuiting tree) \[I,/\] DFERX \[;r, j\] GF R \[1, J\] G RX \[I, j\] makes node J the next younger brother of node I makes node J the youngest son of node I makes node J the next older brother of node I makes node J the oldest son in node I The general technique for these four subroutines is the following.
Before modification, the tree is copied in another area of memory.
All the terminal nodes identified with the structure J take the place of the terminal nodes identified with the structure L Then, the terminal nodes of I are requested in the copied structure and parsed with their dominating nonterminal nodes at the right place.
Gr~R,permits the new numbering of the sequence and, if necessary, prunes the tree.
In the example illustrated below (Fig.
9a and 9b), Gr~R \[14, 13\] is applied and node CPL (13) has been attached to node 16, the father of node 14.
If GF~RX \[14, 13\] had been specified, node CPL would have been attached directly to node 14, rather than the father of node 14.
7.3. INTERV, This subroutine is used for the permutation of 2 structures.
For example, INT~RV.
\[I, J\] where I = 24 andJ = 28 gives rise to the structural change illustrated below.
7.4. INSERT.
This subroutine is used for the insertion of a new terminal node; for example, INSERT \[4, 1HE, 1HT, 1HR, 9\] introduces node with name ETR which becomes a new son of node 9.
7.5. Other subroutines.
There is a number of other subroutines concerning conditions specified within a rule, such as the presence or absence of a node or of a feature in the whole structure, the logical restrictions of loose equality or strict equality.
132 SERGE BOISVERTANDR~ DUGASDENISE B\]~LAlqGER ..................................S.
.o*~*~oi~*~*~Z ,~,..~~ ~W ~,*...**~  ~W :~.**~ ..................................~.
..................................~ .-~z : ...................
~.~ oi ............ ....~.,.,.,..~.........>   .  ......
                * .z ......
~_~ ~ : ~>~ ~Z~W 0~td O~ ~ g ~ 3NI II 177d N~N IOH H~d =l~d ~ldO  ~wz~w~ o .-INI  o0N33 flTd IOH un bJ z ~ h2 .-J ~ N~d ~o.o_ Od0 4" c3~ a~ zza-~x :~zo~co dO3 ~..~-a:Q.~ .-e~   j   ...aAV m~w~2w OFig.
8a. Sample tree before GFER applied OBLING~ A TESTER FOR TRANSFORMATIONAL GRAMMARS .........................................~.
d8 ......
~ *~ ...... g~> ............
"~> : .............
~z ......
~g  :.,.,.~..,. o,~ .~ : --z      m(  o      ~o ; .,-w .     . ,   .   ,.j ....................~,~: .~ ............
~.~ ......................0...........~.
.,..,,~.,~,,,,~,~,..,,,~...,..~ ......
~g ~  ,~ 0 , ., , .,., m m ,,,,,,,,,,,,,~ 133 =>~ ~z ~z~z ~z~z~o rOBd  4NI,o II H3.-I + HAN oz~o lOW 3~d ~ 0.
o o.
(~ < P..~ o(J =0 z dOO . ~IAV 30" eU NO" HN ~ * 3NI o~x~o II 0~ Ld z ~ W,J ~' .J  :~ h.
(l. ~ 1~3_4 fVld ~AN ION O~Z~O H~d hJ Z ~ ~J.Ja: ~"~  ~ h.
a.o. 3~d ~dO dO3,O ~AV 30"+ o~-~o NO'* .-I  ~ u.
0. 0.
HN" NH" Fig.
8b. Sample tree where Grsa has applied 134 SERGE BOISVERTANDRI~ DUGASDENISE B~LANGER .....................,,......,..........9~ :...................9~  ~ ~ m ~Z o O~ld JNI II nqd c k~RN e... ozc,~,~.~ zw~g 3aa ~dO 0=~ dO3 . ~ t,.
cL cL ~AV 30" Q.
t-.(.,.~ O.
~2 --)-Z NO" O.
C30. J ~ v.J 0 J ~ ~ HN* NH" ~O_ N  U..~ .:~oAI" e3 oz~ ha~ zh4 ha Fig.
9a. Input structure OBLING: A TESTER FOR TRANSFORMATIONAL GRAMMARS 135 ~i.
i le~eee ee$ lse eee eeeee~ e,e,eee,ee,,,~,,,le,e,~Z ~z ...........-.~z II  -g .......
~ ~ ......
~ , ~ ,1 ,, ,..
 ~ ~, , .,   ,.
1~ ozJ~z 2 08d II ~z~ W33 ~z~wz ~Id WnN 8~d 3~d HdO dO3~AV 30  NO" Fig.
9b. Output structure after cFm~ has applied 136 SERGE BOISVERTANDI~ DUGASDENISE BIILANGER ...................,.....................~, Ii J .~ : ....
~ ....... o~  ~~ ...........
~ ~ : ......
~  ~: .....
~ : .~ ~  ...
.,~.0,~... mz ~,~,,,,,,',,~,~,.,,,,,,,.~ Fig.
lOa. Input structure O~d 4N\] TT nqd HnN zoH HHd * dOC) 0~:~0 ~IAV NO" _~I.
'-)~_ N  b.~o:~ no" O~d JNI II N3J Flqd HnN IOH ~d 3~d ~dO OBLING: A TESTER POR TRANSFORMATIONAL GRAMMARS @ @ i@ Ii @@I@ @@@ @Ii@ I@ i oII iI @ iI i@I @Ii@ kU II~leo~$e~Ioo$oi@$~ ...... g ............. o,.~ ......
~g .....
~ ............
R~ ~d I M! ~ed .,,....,.,,..,,,~z o~ ...~..No .,.,~.,.....,......,,.
..................................~>,~..,,.~z . "-~ "''   "  "''''  "" "" " "" "  ''~ .....,..,,..,...........,...,.-,, .... ..~ 137 ~ ~z~o  m2~Nz m + O~DO  m oz~w zww~ 08d ..4NI II F mqd 083 k(nN IOn k4~d 38d 8dO dO:) SAY 30" NO" 8N  NH" 4IFIO" dN" 08d 3NI II H3../ f)ld 083 HNN ION HSd 38,:t tldO dO3 dAY :)0"4.
NO., NN" NH" AI" NO-~dN" Fig.
lob. Output structure after ~r~v has applied 138 SERGE BOISVERT ANDl~ DUGAS DENISE B\]~LANGER :...................~ u_ ,,*,0,,o$1,$1,,, ............~z >.
m ~Z .,-,,, : ~ "...  . . .............~, + oxx~O D.
Fig. 11a.
Insertion of a node (before) OBLING" A TESTER FOR TRANSFORMATIONAL GRAMMARS 139 .   . . ..
 ,,.........~.,,,,......,.....~.=.
***eeeoeo$ooe~ o e~ x tn mo~ o~ ~Z~WZ ~S~W m   Fig.
1lb. Insertion of a node (after) 140 SERGE BOISVERTANDRI~ DUGASDENISE BELANGER 8.
CONCLUSIONS OBJ-mC is a system which has been implemented in low-level tORTeN IV for the CDC 6400.
It occupies 55,000s 60-bit words of memory.
It has about 7000 lines of comments and instructions.
REFERENCES N.
CHOMSKY, Aspects of the Theory of syntax, Cambridge (Mass.), 1965.
A. Ducas, et al., Description syntaxique ~l/mentaire du franfais inspir~ des th/ories transformationnelles, Montr6al, 1969.
J. FIUEDM~, A Computer Model of Transformational Grammar, New York, 1971.
D. LIEBERMAN, (ed), Specification and Utilization of a Transformational Grammar, Yorktown Heights (N.Y.), 1966.
D. L.
LotrD~, W.
J. SCHO~N~, TOT: A Transformational Grammar Tester, in Proc.
Spring Joint Computer Conference, Part I, 1968, pp.
385-393. R.
PETRICK, A Recognition Procedure for Transformational Grammars, Ph.-D.
Dissertation, Cambridge (Mass.), 1965.
J. R.
Ross, A proposed rule of tree-pruning, NSF-17, Computation Laboratory, Harvard University, IV (1966), pp.
1-18. A.
M. 7.WICKY, J.
FRIEDMAN, \]3.
HALL, D.
E. W.~a.g.F.R, The MrrRE Syntactic Analysis Procedure for Transformational Grammars, in Proc.
Fall Joint Computer Conference, Vol.
27, Pt.
1, 1965, pp. 317-326 .
S$ I:Mct ur a 1 Cor r e AponfJet\]Ge S~ec \[ f I c_a t j O0_#DvLEonmer~ Yongfeng YAN Groupe d'Etudes pour la TradlJctlon Automatlque (GETA) B.P. 68 Unlverslty of Grenoble 38402 Saint Martln d'H~res FRANCE ABSTRACT This article presents tile Structural Correspondence Speclflcatlon Environment (S('SE) being Implemented at GETA The SCSE is designed to help linguists to develop, consult and verify the SCS Gr'alt~nar s (SCSG) which specify I lngulst ic models.
It Integrates the t eclln 1 clues of' data bases, structured edltors and language interpreters.
We argue that formalisms and tools of specification are as Important as the specification itself.
z NT ROD_UCT tqN For quite some time, It has been recognized that tile specification Is very important in tile development of large computer systems as well as the linguistic computer systenls.
But it ls very difficult to make good use of specification without a well defined formalism and convenient tool.
The Structural Correspondence Specification Gran~ar (SCSG) is a powerful linguist ic specification Formalism.
the SCSGs were ftrst studied in S.Chappuy's thesis (1}, under the supervision of Professor B.
VaLIqUOt s.
In their paper presented at Colgate University in 1985 {6} SCSG was called Static Greener, as opposed to dynamic grammars which are executable programs, because the 8CSG aims at specifying WI4AT the linguistic models are rather than IIOW they are calculated, A SCSG describes a llnqulstlc medel by specifying the correspondence between the valid surface strings of words and the multi=level structures or a language.
Thus, from a SCSG, one can obtain at the same tlme valid str lngs, valid structures and the relat ton between them.
A SCSG can be used for the synthesis of' dynamic gralr~}lars (analyser and generator) and as a reference for large linguistic systems.
An SOS Language (SCSL) has been designed at GETA, tn whlcll the SCSG can be \]lnearly written.
The SCS Environment (SCSE) presented here ts a compLIter aided SCSG des lgn system.
I t wl 1 1 al low lhlgulsts to create, modify, consult and verify their granlnars in a convenient way and therefore to augment their productivity.
Sect 1on I gives a outline of the system: Its architecture, pr Inciple, data structure and comdnand syntax.
Section II describes the malrl functions of the system.
We conclude by gtvtng a perspective for luther developments el' the system.
I=.AN OVERVIEW OF TI4E S YSTE_M 1.
ARC HI\]EC T URE The SCSE can be logically divided tn five parts: 1 SCSG base 2.
monitor 3.
input 4.
output 5.
procedures The SCSG base consists of a set of files Contalnlng tile grarrlnars, lhe base has a hlerarchtca\] structure.
A tree form directory describes tile relationship between the data of the base.
The monitor Is the interface between the system and the user.
It reads and analyses colTinands from the input and then calls the procedures to execute the cormlands.
1he input is the support containing the COlrrnands to be executed and the data to update the base.
rhere is a standard input (usually the keyboard) from which the data and cormlands sllould be read unless an Input ls explicitly specified by a conlnand.
The output is a supper t receiving the system's dialogue messages and execution results.
There is a standard output (usual ly the screen) to which tile message and results should be sent unless all output Is explicitly specified by a con~and.
The procedures are the most irnportant part of tl~e systenl.
It ls the execution of procedures that carries out a COn~land.
The procedures can communicate directly wtth the user and with other procedures.
2. THE_E.RJNCU}LE An SCSE session begins by loading the original SCSG base or the one saved from the last Session, Then the monitor reads lines from tile com~nand input and calls the corresponding procedures to execute the COmd~lands found.
When an SCSE session Is ended by the colm~and "QUIT", the current state of tlqe base Is saved.
The SCSG base can only be updated by the execution of c omrlland s, The original SCSG base contains two SCSGs : one describes the syntax of the SCSI_ and the other gives the correspondence between the directory's nodes and the syntactic units of the SCSL.
The first gralmlar ls read-only but the second one can be modified by a user.
This allows a user to have his prefered logical view over the base's physical data.
These two grammars serve also as all Oil-line reference of the system.
Several Interactive levels can be chosen by the user or by the system according to the number of errors in the con~aapd lines.
The system sends a prc~npt message only when a "RETURN" ls met in the CO~nand lines.
So gee carl avoid prompt messages by entering several cen~nands at a time.
;3. DATA S:\[f~UCTURE There are two data structure levels.
The lower one Is linear, supported by the host system.
Tile base Is a set of files containing a llst of strings of characters.
Tile base carl be seen as a single string of characters thai: Is the concatenation of all lines tn the ft\]es of the Llase so that tile structure is said to be llnear.
TIlls structure is the physical structure.
The higher one Is hierarchical, defined by the directory of the base.
Tile base is composed of a number of SCSGs ; each gral~ar contains a declaration sect Ion, a rule (chart) sect Ion...
etc. and the components of a gran~nar (declarat 1Ol1, rules . . . etc, ) have their own structure.
The hierarchical structure ts the logical structure of the base.
The directory has a tree form.
A node In the tree represents a logical data unit that ts its content (for instance a gran~nar).
Every node has a type and a list of attributes characterlslng the node's content, rhe lnternode's content is the composition of those of its descendents, \]he lear's content Is directly associated 81 with a physical data unit (a string o1' characters).
The following figure shows the relation between the two structures.
LOGICAL STRUCTURE (i) 7, 2Y LOGICAL S'\[RUCTURE (2) language date \[Grammar English -----i node type attributes The directory is slmllar to a UNIX directory.
But In our directory, tile leaves do not correspond to Flies but to loglcal data units and Furthermore an attribute list is attached to each node.
The correspondence between two structures is maintained by SCSE.
We shall see later that this organlsatlon allows a more efficient Information retrieval.
It ls possible For" users to have access to the data by means of both structures.
The logical one Is more convenient but the physical one may be more efficient in some cases.
4:~ _COMMANp__SyNTAX The general command format is : <operator> <operand> <options> The "operator" is a word or an abbreviation recalling the operation of tile colmland.
The "operand" is a pattern giving the range OF the operation.
The "options" is a list of optlonal parameters of the COw,land.
For example, the Con~nand : V GRAMMAR ( LANGUAGE = ENGLISH ) visualizes, at the standard output, all the English grammars In the base.
Here V is the operator, GRAMMAR(LANGUAGE=ENGLISti) ls tile operand pattern and no option Is given.
The operand being mostly a node in the directory tree, the pattern is USUally a tree pattern.
When the pattern matches a subtree of the directory, the part that matches a specially marked node Is the effective operand.
The pattern is expressed by a geometric structure and a constraint condition.
The structure ts a tree written in parenthesized form perhaps containing variables eacll representing a tree or a forest.
The coeditlon Is a first order logic predicate In terms of the attributes of the nodes occurring in the geometric structure.
More sophisticated conditions may be expressed by a predicate combined with geometric structure to efficiently select information from the base.
Pattern writing should be reduced to a minimum.
In the abeve example, the geometric structure is shnply a grammar type node and the constraint is the node's language attribote having the value= Erlgllsl\].
The use of a current node tn the directory allows not only the simplification of pattern writing but also the reduction of the pattern matching range.
The effective operand becomes the new current node after the execution of a command.
II. THE MAIN FUNCTIONS We shall Just descrlbe the functions ttiat seem essential, lhe functions may be divided Into four groups= 1.
general 2.
SCSG base updating 3.
SCSG base inquiry 4.
SCSG verification.
_1 ~ _GI~ t>\[E__R A L _F U_N__C__T._I.D_N S These functions Include: SCSE session options setting, the system's miscellaneous lnformatlon inquiry and access to host system's commands.
The following options can'be set by user co,hands: 1.
tnteractivtty 2.
dlalogue language 3.
auto--verlfilcatlon 4.
session trace 5.
standard Input/output.
One of the 4 Following Interactive modes may be chosen: 1.
non-interactive 2.
brief 3.
detalled 4.
system controled.
In non-interactive mode, no question is asked by tile system.
An error con~and Is ignored and a message will be sent but the process continues.
In brief mode, the current accesslble command names are displayed when a corm, and Is completed and a RETURN in the command lines is Found.
In detailed mode, the functton and parameters of the accessible commands are displayed and 1F an error ts Found in the user's Input data, the system will diagnose it and help him to complete the command.
A prompt message ls sent every time RETURN is Found in the COn~nand lines.
In the system controlled mode, the lnteractlvlty Is dynamically chosen by tile system according to the system=user dialogue.
For the tlme being, only French is used as the dialogue language.
But the mu.ltl-langueage dialogue is taken tnto account tn design.
It is simpler In PROLOG to add a new dialogue language.
The auto-verification option Indicates whether the static coherence (see 4.
SCSG verification) of a gra~nar will be verified each time it ls modified.
The trace option is a switch that turns on or off the trace of the session.
The standard Input/output option changes the standard input/output.
Some Inquiries about the system's general Information, such as the current options and directory content, are also ~ncluded in this group of Functions.
The access to host system's co~Ylands without leaving SCSE can augment the efficiency.
But any object modlfted out of SCSE is consided no more coherent.
2. SCSG BASE UPDATING This group of fiuectlons are: CREATE, COPY, CHANGE, LOCATE, DESTROY and MODIFY.
\]hey may be found In all the classic editors or file management systems.
The advantage of our system is that the operand of commands can be specified according to the logical structure of the base.
For example, the col~nand : DESTROY CI4ARTS(TYPE=NP) Destroys all the charts which describe a Noun Phrase.
82 The SCSE has a syntaci Ic editor that knows the logical structure of the texts being edited.
Ihls editor Is used by tile con'Jnands MODIF and CREATE.
The command CREA1 <operand> <options> calls the edltor, creating a logical data unit specified by tile operaod.
If the interactive option ts demanded, the editor will guide the user to write correct ly according to the nature of the data.
Following the same tdea of different interact lye levels, we try to improve on tile classical structural editor, Per instance that of Cornell University \]\[5}, so that one carl enter a piece of text longer than that prompted by the system.
If the interactive option Is not demanded, one Just enters into the editor wlth an empty work space.
The CO~T~nand "MODIF <logical unit>" calls the system's edltor with the logical data untt as the workspace.
The data ill the workspace may be displayed In a legible form which reflects Its logical structure.
The mul t l-w \]ndows facll ity of the editor makes it possible to see simultaneously on tile screen the source text and tile text In structured form.
The SCSE editor inherits the usual editing con~llands from the host editor.
Thus one can change all the occurrences Of a rule's name fn a grarrnlar without cilanglng the strlngs containing the same characters, using a loglcal structure change : C NAME(type=rule) old name new _nan/e, while tile physlcal structure command : C/o 1 d..
name/new .name/* * changes all the strings "old_name" In the workspace by new name.
When an obJect's deflnltloo Is modified, all Its occurrences may need to be revised and vice versa even if the modification does not cause a syntactic error.
A structure location command flndlng the definition and all the occurrences of an object can be used In this case.
Only tile logical units defined in the directory and the SCSL syntax can be manipulated by the structural COrr~land s.
SCSGBA=SI~_INQUIRY These functions allow users to express what they are interested ill and to get the Inquiry results In a legible form.
A part of the on-llne manual of usage in the form of SCSG may also be consulted by them.
The operand patterns discussed above are used to select the relevant data.
The operator and options of co~nands choose the output device and corresponding parameters.
A parametered output form for each logical data unit has been defined.
The data matching the operand pattern are shaped according to their output form.
The data may of course be obtained in their source form.
One may wish to examine an object at different levels (e.g.
Just tile abstract or some comments).
The options of the con~and can specify this.
If one Just wants to change the current node in the directory for factlltatlng the following retrieval, the same locating co~nand as before may be used.
4. SCSG VERIEICAT#ONS.
Two klnds of verifications may be distinguished : static and dynamic.
Tile static verification checks whether a grammar or a part of a gra~nar respects the syntax and semantics of the formalism.
The dynamic verification tests whether a given gran'mnar specifies what we want It to.
S tatlc_ve, r Ifica~ton All internal representation of the analyzed text ts produced and used by the system for structural manipulation, the analyser may produce a list of cross references of = nameable objects and a list of syntaxo-semantlc errors found In the text.
The exemples of nameable objects are the charts, tile macros, the attributes.
The list of cross-references reveals the objects which are used but never defined or those defined but never used.
A chart may refer to other charts.
This reference relation can be represented by an oriented graph where the nodes stand for a set of charts.
A hlerarciltcal reference graph is often given before writing the charts.
A program can calculate the effective graph of a grammar according to the result o analysis and compare It with the given one.
The cornlland options may cancel the output of tllese two llsts and the graph calculat Ion.
The graph calculation may also be executed alone.
One of optlons Indicates whether the analysis wtll be Interactive.
D.y.n ~!# J c.
v. ~gr :1 f i canon Tile dynamic verification Is tile calculatlon of a subset of the st ring-tree relation defined by a gr altrnar.
A member of the relation is a pair <string,tree>.
Ti)e command gives the granYnar and the subset to be calculated.
The subset may be one of the four following forms : I.
a pair with a given string and a given tree (to see whether It belongs to the relation) 2.
pairs with a given string and an arbitrary tree 3.
pairs with an arbitrary string and a given tree 4.
all possible pairs rhe calculation is carried out by all interpreter.
The user may give interpretation parameters Indicating interactive and trace modes, slze o the subset to be calculated and other constraints such as a list of passive (or active) charts during this interpretation, the depth and width of trees and length of the string etc..
As SCSGs are statlc gral~nars, no heuristic strategy wllt be used In the lnterprete's algorithm.
So the interpretation will not be efficient.
Since the goal ts rather to test gramnars than to apply them on a real scale, the efficiency of the interpreter Is of no import ance.
CONCLUS I0N The system presented Is being implemented at GETA.
In thls article, we Put emphasis on the system's design principles and specification rather tilan on the detalis of lmplementatlon.
We have to1 lowed three widely recommended des ign principles: a} early focus on users and tasks, b) empirical measurement and c) Interactive design \]\[2\]\[.
The specification of the functions are checked by the system's future users before implementation.
The user's advice Is taken into account.
This dtalogue continues during lhe implementation.
The top-down and modular programming approaches are followed so ttlat, even 1f the Implementation ls not completly acilieved, the implemented part can still be used.
The system Is designed for being rapidly implemented and east ly modt f led thanks to Its modular lty and especially to a htgh level logic programming language: PROLOG (3\].
We have tried our best to make the system as user-fr lendly as possible.
The system's most remarkable character is that the users manage their data according to the loglcal structure adapted to tile human be I rig.
What ts interesting In our system ls not that it shows sonle very original ideas or the most recent techniques In state-of-the-art but tt shows that tile combination of well-known techniques used orignally In different fields may flnd its application in other fields.
83 Long term perspectives of the system are numerous.
Wlth the evaluation o the SCSG, some strategic and heuristic meta.-rules may be added to a grammar.
Equipped by an expert system of SCSG, SCSE could lnterprete effclently a static grammar and synthetlse from It efficacious dynamic grammars.
It Is also interesting to integrate into SCSE an expert system which could compare two SCSGs of two languages and produce a transfer grammar or' at least glve some advice for constructing it.
Using its logical structure manipulation mechanism, SCSE can be extended to deal with other types of structured texts.
Thanks to Its efficient Interpreter or in Cooperation with a powerful machine translation system such as ARIANE, SCSE could be capable of offering multi-llngual editing facilities (4~.
-O--O--O-O--O-O-O-O84 BIBLIOGRAPHY S.Chappuy, "Formallsatlon de la Description des Niveaux d'Intepretation des Langues Nature\]les.
Etude Men~e en Vue de l'Analyse et de la G6n@ratlon au Moyeo de Transducteur.", Th~se de trotsl~me cycle & I'USMG-INPG, Juillet 1983.
2. John G.
Gould and Clayton Lewis.
"Designing for Useabllity: Key Principles and What Designers Think", Co~nunIcatlon of the ACM, March 1985 Volume 28 N .
Ph. Donz, "PROLOGCRISS, une extention du langage PROLOG", CRISS, Unlverslte II de Grenoble, Verston 4.0, Juillet 1985.
HEIDORN G.E., JENSEN K., MILLER L.A., BYRD R.J.0 CHODOROW M.S., "The EPISTLE text-crltlauing system.", IBM Syst.
Journal, 21/3, 1982.
TEITELBAUM 1.
et al, "The Cornell Program Synthesizer: a syntax directed pr'ogra~ntng environments.
", Co~nunicatlon of ACM, 24(9), Sept.
1981. VAUOOIS & S.
CHAPPUY, "Static Gran~ars : a formalism for the descrlbtion of linguistic models", Proceedings of the conference on theoretical and methodological issues in machtne translation of natural language, Colgate University, Hamilton N.-Y., USA, August 14-16, 1985 .
CondiLioned UnificaLiou for Natural l,an~uage Processinp, A\]\]STV~ACT This paper prescnLs what wc c.all a condiLiol'md unification, a r'm'w meLhod of unificatiol'~ for processing natural languages.
The key idea is to annotate Lhe patterns wiLh a cerLcdn sort of conditions, so that they carry abundant inforrnation.
'\]'his met.bed t.ransmits inforrnaLion frorn one pattern to another more efl'icienLly Lhan proecdurc aLLachmenLs, in which information cortLaincd in the procedure is embcddcd in the progranl rather Lhan dirccl./y aLLachcd Lo paLL(ms Coupled wilt techniques in forrnal linguistics> i\]\]orcovcr, conditiorled unification serves most.
types o1" opcrations for natlu'ai \]ar/guage processil'q~,.
KSiti f/asida \]'\]\]ecLroLechllica\[ 1,abe ' A.ory Ul\]lczorlo I } 4, 7~aktlra MtlFa, Niibari-Gurl, Ibaraki, \[tOb Japan (\[3) ptlt_tllS psrl nrnb(prcsonL, P, N) : notAlrd ~'-tng(P, N) l~ut_Lns, psn nl:nl)(T, l", N) : not_pres(T) noL_3rd sng(lst.,,N).
not pres(past) not_~rd~'-;ng(;~nd, N) not pres(past4~a'rtlclp\]e ) rlotA/rd, sr~g(,'Wd, plural) not pres(basc).
1. Introduction A currcr'd, major t.rcrY.t of naturul la~/guage processing is ehara.cterized by Lhc overall use o\[ unification (Sttiebc~r (198'l), Kay (1985), Proudir:~ and Pollard (1,985), Percira (198b), Shicbcr (1985), etc) reflecLing lhe recent develop merits in nonLra.nsformaLJonal linguistic \[ormalisrns, such as Lexical FuncUonal Grammar (}lrcsnan (198E)), Generalizcd Phrase St.r'tJcl.ur( (\]rarnrnar (GPS(\]) (Gazdar, Klein, Pulhlm and Sag (I 985)), i Icad Grammar (Pol}ard (19f1,1)), and tIcadl)riven Phrase Structure Grammar (lIPS(;) (Pc\]lard (1985a.,b)). These formalisnls dispense wiLt,qlobal opcraLioits sueh as t.ransfornlaLion, alld instead cxp!oit h~cal operations each C'Ollf'lrled wttJ/i\[l a local tree Such local operations ar'c forunulatcd in Lcrms of uni~caLion \]Iowevcr, Lhe ordinary unification as in Prolog is insufficient, seeu rrorn both scientific (here, alias liriguJsl,ic) and cngin(.'ering poilfl'. of vicw '\['he F, robh-'trl is that p,t tc\]~\[\]s Lo bc tl\[li(ie({ wiL\]l each other lack the cape.city rot carrying irfforrnaLion In Lhis papcr we \[)rcscnl a new mcLhod of unificaLion which we (call conditioned unification.
The essence of the method is t.o deal wit.h paLLcrns aimoLated by seine sort of condit, ioils.
These eondiLioi<ls are so cortsl, raincd /-Is Lo 'oe cfficicntly operated on, and yet to be able to carry rich enough informaLion t.o caDLure linguistic gcneralizations.
2. The Problem Ordinary patterns as it/ Pr(;h)g Is.el< cxprcssivc power, t)esatlSC var\[ablcs theFcirl arc Sil)i\[)ly il\](iClCl"tlliltdt(7 alld Ihtis car'ry almost no irffqrrnalion 'l'hercforc, stie}l palL(ms aud unification among thcm arc msuffiei0nt for' capturlng t\]le {~,i<'al/l rYlat ic al <!,>erm r'al ixat ior~ and tim process:n~> effici(ncy.
\],It us look a.t some c:<amph.~s below A ~,,l'anl matical catc>ory is assumed Lo be it llst of features A feature consisLs of a feature nalnc and a w~hic, and rcprcscnLcd as a t.cmn like tt~rn.e (vat,z().
'\['hc \]cxical entry of English vcrb p'u,t, for instance, can not.
be described as a I'roloc patLcrn, bill needs some arlllOLation (i.c.,p~zt Im.s~)s?t.~zmS(T, P, N)) as in (1) (1) k:xicorl(puL, I tensc('I'), p(msorl(\]')> number(N)I) : put_Lns_ psn nrnb(T, P, N) }let(?, fcaLLIICS olincr Lhan ten, se., perso?t, and ?lattnher arc omitLed, arm predicate p~ztmtm.spsTt~z?n.b is dcfinc.d c,s in (2) For a biL morc COIT/I)I i:aLcd instance, conmdcr the relationship between a synLacLic gap and iLs filler.
In GPSG, IIPSC, cte., tiffs relationship is; captured in terms of the SI,ASI\[ fea/.urc, which reprcscnts gaps in the category of \] U~.i~tk is craz U, for cxamplc, thc SI,ASII feature is spcciflcd as \[NI j\] ller'e SI,ASI is assumed to take as its wdue alist of catcgorncs.
SLaLcd below is a simplil3cd principle about the disti'ibution of this featqrc in I.yptca\] cascs (3) lu a local tr'ec, Lhc rllotl;cr catcgor/s S\],ASI\] tea.Lure is obl.aJncd by coneatcr~atir, g from h.fi\[.
Lo rip, ht the S I,ASII fcat,wcs of hey de.,.ightcrs In order to describe: this principle, s('nnetlting :uorc than a nlerc pat t.crn ts lcquir(x\] again: (i) IocalJr'cc.(lslc, sh(X)\], Ismsh()l, Islash(Z)\])append(Y, Z, X) l'eaturcs othcr thanSI,AS}l arc omitted herc.
The so called procedure altachmcnts is the most colnrnon way or conllflclncntJnp, the \])oor clcscriplivc capacM.y or ordinary patterns \["or instance, you may regard Lhc bodies of \]h)rn elaus(s (1) and (4) as at la,_hed procedures The dr'awbacl< of procedure atLachr~lcnl is ut the fact t.hnt the ouly way of using Lhc proccclurcs Is to execute thorn I"or t.his reason, proecdur,}s arc Irmrcly embedded lu programs, rcAhcr than at.lath(x\] to those paLterrls which th(sc itrotu'ams operate on The irtforrnaLion which \[)ro((durcs cantain car/rx~t {U.'nera\]\]y be I~.',rricd aFOlllld a( ross scvc;a/ part ial s\[rtlC\[ tlI( s ci\]ch Of which it pFoocc\]arc dircclly operates on, bccausc> oncc a procc(lurc is cxc cnt.td, the informs.lion whkh it c.ontainc:d is palqially lost For instance, when Icxical entry (1) is cxploiLecl, p~ztJ.n.s pstt.n;/m,6(?', I), /\i) is cxecut.cd and 7' and /~ arc il~stanliatcd Lo be preset~.t and Ist, icsp(cLivc\[y 'l'h'ds Ic\[L bchh~d is the informaLion abotlL the other ways Lo instanLiaLc those wwiablcs.
Actual procedure attaclu'ncrd.s musL be arr-ar<e, ed so that infornlat on shouhl not be it)st whelt procedures arc cxccutcd Freeze of Prolog (Colmcraucr (1982)), for instanc/, is a mcans of tins arr~ingerncnt.
\]\]y exc(tll.i\[\]g freeze(V, "~), atomic formula ~ is frozen; i.c, the exccuLlon of'~ is >-uspcnded until w~riable X is instanttat.cd \]f' contams.'(, thcl'cforc, }lop(fully uot.
so rnuch lrtforrnat.iol~ is test.
whcc  is cxecuLed Ncvc.rthcless, freeze is problematic in two rcspt(ts Virsk, irJorn\]ation cart still be lost when the frozen pro-ccdtll'CS LIFe cxccnted.
Seeond, too nltlCh illforrllatiol3 cat\] be accumulatcd whilc several procedures arc frozen Sup pose, for itlst.ance, thaL freeze IX, t~tet~ber(.Y, \[a, 6 })) and fr<,t~.~<~.
(r,,~.',~,.~.,'(}'. I~'.~ I)) have bccn execut, cd '\['herr, X and Y can be ulfif\]cd with each other witt~ouL awakening ciLbcr procedure.
In that (asc, Lho iifforn/at, ion that X may bc t) is redundant bctwccn Lhc I.wo proccdures, and Litc other part or irlfornmLion those Droecdtlres contain is Ill(Of\]" sistcitL What one might hope here is \[o Jrlstitntiatc )( (and Y) to be b If we had cxectitcd freeze(Y, member(Y, It, cL ) iristcad of freeze (Y, rn.ernber(Y, Ib, c I), computational 85 resources would be wasted as the price for a wrong processint.
After all, it is up to a programmer to Lake a deliberate care so that information should t)e efficiently transmitted across patterns This causes sewral problems interwoven with one another.
First, since those programs reflect the intended order of execution, they fad to straightforwardly capture the nniforrnitJes captured by rules or principles such as (3).
Accordingly, programnnng takes rnuch labor'.
Moreover, the resulting programs work efficiently only along t.he initially inLer~ded order.
3o Conditioned Unification 3.1.
Conditioned Patterns These problems will be.
settled if we earl attach information to patterns, instead of attaching procedures to programs l\[ere wc consider that such information is carried by some conditions on variables Variables are then regarded as carrying some information rather than remain:ing simply indcterminatc I-}y a conditioned pal.tern let.
us refer to a pair o\[a pat tern and a condition on the w~riables contained in that pat.tern.
l~'or simplicity, assume LhaL the condition of a conditioned pattern consists of atomic formulas of Pro/og whose argument positions are filled with variables appearing m tile pattx.'rn, and that the pre(hcates heading those atomic for mulas are defh~ed in l.erms of Horn clauses.
For instance, we would hkc to regard the whole tbing in (\[) or (4) as a condJtioncd pattern.
 3.2.
Modular Conditions The conditions in conditioned patterns must not be executed, or the contained information would be partially lost Tile conditions have to be somehow joined when conditioned patterns are unified, so t.hat the information they contain should be transmitted properly in tile sense that the resulting condit.ion is equivalent \[o the Logical conjunction of tam input renditions and contains nciCrmr rcdnndant nor ineon sistent information.
We call suet a unification a conditioned unification A simple way to reduce redundancy and inconsistency in a ('.ondiLion is to let each part of each possible value of each variable be sLlbjcct to at, most one constraint.
\],eL us formulate this below.
We say that a condition is superficially modular, when no variable appears twice in that rendition For instance, (Sa) is a superficially modular condition, whereas (Sb,c) are not.
(Conditions are some.
times wr'itterr as lists of atomic forrnulas ) (',9 a \[a(X, Y), b(Z), a(U, v)\] b.
l a(X, Y), b(Y)\] e \[a(X,Y,X)\] l,'urther we say that a condition ~I' is modular, when all the relevant renditions are superficially modular, lIere, the relevant, conditions are {I} and the bodies of Horn clauses reached by descending along the definitions of the predicat.es appearing in .
A predicate is said to be modular when its definition contains only those Iiorn clauses whose bodies are modular conditions.
A predicate is potentially modular when it is equivalent to some modular predicate A modular condition does not.
impose two constraints on any one part.
of any variable, and thcrcfore contains ne> kher redundancy nor ineonsistency, ltereaRer we consider that the condition m (.'very conditioned pattern should be modular.
a.a. l'Jxpressive Power Conditioned patterns can carry rich enough information for capturing the linguistic generality.
Obviously, at 86 'st., they can describe any finite set of finite patterns.
\];'or instance, (I) is regarded as a conditioned pattern with modular condition \[pztt_g'ms_pstt~q,r~zb (T, P, N)\].
Moreover, also some recursivc predicates are modular, as is demonstrated below.
(6) a appcnd(\[\], Y, Y): append(\[U I X\], Y, \[U I Z\]) :append(X, Y, Z).
b sublist(\[\], Y).
sub\]ist(\[U I<I, \[U I Y\]) :sublist(X, Y).
sublJst(X, \[U IY\]):-sublist(X, Y).
Thus, (4) is also a conditioned pattern.
\]lowever, some recursive predicates are not potentially modular.
They include reverse (the binary predieate which is satisfied i~r its two arguments are the reversals of each oilier, as in reverse(\[tot, b\], c, all, \[d, c,\[ct b\]\])), .perm (Lbe binary predicate satisfied iff its arguments arc permutat, ions of each other, as inperm(\[i, 2, 3\], \[2, 1, 3\])), subset (the binary predicate which obtains iff the first argument is a subset of the second, as in s~zbset(\[d! b\], to, b, c, all)), etc.
New.'rtheless, t.his causes no problem regarding natural language proeessing, because potentially infinite patterns corne up only out of features such as SLASt\[, which do not require those non ruodular predicates.
3.4. The Unifier Shown below is a'trace of the conditioned unification between conditioned patter'us (7) and (8) (here we use the same notation for eondit.ioned patterns as for IIorn clauses), where the predicates therein have been defied as in (9).
(The definitions of c0 and e3 are not exploited).
First, we unify iX, )2 Z, g/\] and \[A, 7\], C, D\] with one another and get.
XA, Y : /3, Z = C, and W = D \]n the environment under lifts unification, the two conditions are concatenated, resulting in \[c0(X), e I(Y, Z), e2(Z, W)\].
The major task of this conditioned unification is to obtain a modular condition equivalent to this rmn-rnodular conditiorl This is tire job of funcl.ion ~tod~zlayi, ze.
Mod~zla.~tze rails function ~;~ttegrctte, which r'eLtlrns an atomic formula equiwrlent Lo the gives condition.
The Lcrminatior~ of a ?rtodulct,'ize or anir~fegrate is indicated by ~ preceding the reLurn-waluc, with the same amount of indentation as the outset of this functionrail was indicated witb When an {~ztegro, te calls a ~zodula~'ize, the alphabetic identifier of the exploited Ilorn clause is indicated to the h.'ft hand side, and the temporal unification to the right-hand side.
Atomic formulas made in integrate is written following 4.
Each lIorn clause entered into the definition is shown following % and given an alphabetic identifier indicatedto the right-hand side.
(7) IX, Y, Z, W\] :-e0(X), el (Y, Z).
(8) \[A, \]~, C, D\] :e2(C, D).
(9) e*(0, \]).
(a) e ~ (q, e) (b) ca(l, P):-e:Xl').
(c) c~(e, 0).
(d) modularize(\[e0(X), cl(Y, Z), c2(Z, W)\]) integrate(\[e0(X) \]) cO(X) integPate(\[cl(Y, Z), e2(Z, W)\]) c4(Y, Z, W) (a) modularize(\[e2(1, W)\]) Y = 0, Z = 1 integrate(to2(1' W)\]) * eS(W) (c) rnodularize(\[e3(P)\]) W = P integrate(\[e3(P)\]) =~ e3(P) tea(p)\] c~(p) :ca(P).
(0 =:> eS(W) -~ \[c,~)(w)\] 1' o4(o, :, w) :.
~:',~(w).
(j) (b) n:odular:'zc(\[c2(2, W)i)  :q, '/, :~ i:,t.o~ra~,'.(Im<3(a W)\]) * cO(W) (d) nladularizc(l I) w =.: o =-~ \[\] cS(0) (k) =+ c6(W) > I cs(w)J " o,3(q, ~, w) : o6(w).
(I) => c4(Y, Z, W) > \[co(x), <:4(< z, W) l We CaN refine Lhe progra\]'n o\[ "btt~.grcs, ta so that it should avoid ally predicaLe w}iose definiLion coiuLains only one llorn clause.
For instance, Lhe definiLion of cb consisLs only o\[(i) InsLead of (j), LhereR)re, we may }rove cd(0, 1, P) : c3(P) Also (1) can bc replaced by c 4(0, 2> 0), based on (k) We are able Lo work out r'ccursivc condiLions from F, lvor; recursivc coI:dit.iolls, lVor example, considor how X and Z arc unifiod under" t, ha conclit,iol: (10), whore ~rte'n~be.r is defined as in (1 1) (10) \[n,e:nher(X, Y), o0(z)\] (11) n/cinber(A, IA I IJl).
(a) member(A, IC I i~i) :-i ....... her(A, it) (b) The Lracc of this ulfif\]cat.k~n is showl~ b('\]:'w, whc's'c prcdica.l~' c 1 is rccursivcly (\]o/~ll/Cd based on Lhc i'(,ctlrsiv(! dcfillJLioH of 77"~ ( 77}, t) I~, '? modularizc(lmcmbcr(X, Y), cO(X)I) int.cgraLo(I member(X, Y), cO(X)\]), el(X, Y) (a) modularize(\[cO(A)\]) X = A, V -.
\[A Ill\] int.e<~';ratc (I c O(A)I) = > oO(A) =~ 1 <:)(A)\] 1' c I(A, \[A I ~<J) :-c0(A) (1)) n:odularizc(lmcn,bcr(A, 11), c0(A)l) X :: A, Y :: \[c!i;I Jnl.c<qral.e(\[ nlernbcr(A, I~), cO(A)I) :~ el(A, 1~) ::> \[cl(A, 33)1 1'ci(A, Iclt<l): c~(A, 13).
.... > c:(X, Y) :.~ \[o:(x, Y)I IL Js a job of in, tegra, te, Lo handle re,cursive de,hiLton.
The lasL g?l, te.g?,ts, te.
above recognizes Lhat.
the first 4m.tegrate, which is Lrying Lo (\]cNr/c c 1, was called wit.h the same arguITlCrlI.S except, the variable narnes, llencc t.he last "inttegrctte simply reLul'ns c.
I(A, H), because t,hc conLent, or cl is now bring worked ouL tlrldoY Lhc J'\]rsl.
~?ttegro.te arid thus it is rednndanL fol' t, he lasL {~tfegrate Lo \[urLhcr examine c 1.
It is not.
a\]ways possible fro' the above unifier t.~ unify paLL(2i"\[/s tlrl(\](~r roc.
tlr'sivo Col\]d\[Liol/S \["or J//sLalloL', J\[ Cf/illIOL unify X with )" under \[appe~td.(X, Y, Z)\], becal_tsc Lhe result ing condiLion is noL potsnLia\]\]y rnodular, llowcver, such a situaLiol'l CtOCS FioL seoln t.o occur Jn actual \[al:g:lagc proccssir:g.
4. Conclusiori We have prc.'-~er, Led a new nle/hod a\[ umfiealion, wh,ch wc call o.
coildiLior~c(\] tltti(loaLioli, Whcl'e paLLorils to be uniPlc(\] "'re annoLaked by a certain sort.
of corldit.ions on lhe variables wifich octroi" ill those patt.crn.<;.
Theso condiLions are so r'est.ricted t.haL Lhey conlain as lit.Lie redundancy a<'; possible, and d'ms arc always assured to be satisfiable.
This method has Lhe fo/h)wtng welcome characteristics l"h'st, I.he \])attorns to bc unified can carry at)llllda\]'lL infos' mat.ion rcprcscnLcd by t.he conditions han:,~in!,; on t.hClll The expressive capacity of Lhe,<c condiLion,s is sl:fl'Jcen\[ for capt.uring \]JH~U, IIihLJ(: ~sCHCl'a\]i',,iOS ~ccorld, such irfformat.ion is cfreclivcly Ir'ansrnitLed, by h~t.egrat.\[n? the col~dil,ion.~ v;her, pat.'..crl:,<s o.ro unif'ied Unlike procedure aLLacl:ment.s, in thil~ COllne(:lion, Ll/c infornGaLioi~-conveying <.'fficicl:cy of our Colt dilioiued unif'icat.\[on is no afl'c'(gcd by the direct.ion of t.i~c daLa.flow Therefore, O/ll" col'l(\]{lioned unifies.Lion is oo;rnplel/ly r(ver'sJbk< and ',hns is \[n'on:ising its a Los\] for dcscril)h'T> <~{l'all/lllilrs fOF bolh SCllL(Hb':C comprel/ensiol: slid prochl(d toll Owing t.o Lhese cllar'act(!risLics, Otll conditioned unif'caLian l)r'avh|es a now prog, ra.unniiug 1.1aradigtn foi I/illt/l'/tl lar/y,/lag(".
\[)l'OCCSSil/lJ>, rcph~.cing proccd/1Fc aLt, o.ctlI3:ont.s which haw3 tradlLionally el2joyed i.\]lc Llbiq/lity Lhat.
t.hcy do noL descrvc References Bresnan, J.
(ed). (1982) The Mental Representation of Grammatical Relations, MIT Press, Cambridge, Massachusetts.
Cohnerauer, A.
(1982) Prolog II Reference Manual and Theoretical Model ERA CNRS 363, Groupe d'Intelligence Artificielle, Universit de Marseilic, Marselle.
Gazdar, G.
E. Klein, G.
K. Pullum, and J.
A. Sag (1985) Generalized Phrase Structure Grammar; Basil Blackwell.
Oxford. Kay, M.
(1985) "Parsing in Functional Unification Grammar".
Natural Language Parsing, pp.
251--278, Cambridge University Press, Cambridge, England.
Fernando C.
N. Pereira, A structure-sharing representation for unification-based grammar formalisms, Proceedings of the 23rd annual meeting on Association for Computational Linguistics, p.137-144, July 08-12, 1985, Chicago, Illinois Pollard, C.
J. (1984) Generalized Phrase Structure Grammar.
Head Grammars, and Natural Languages.
Doctoral dissertation, Stanford University, Stanford, California.
Pollard, C.
J. (1985a) Lecture Notes on Head-Driven Phrase Structure Grammar.
Center for the Study of Language and Information.
Pollard, C.
J. (1985b) "Phrase Structure Grammar without Metarules," Proceedings of the Fourth West Coast Conference on Formal Linguistics, University of Southern California, Los Angeles, California.
Derek Proudian, Carl Pollard, Parsing Head-driven Phrase Structure Grammar, Proceedings of the 23rd annual meeting on Association for Computational Linguistics, p.167-171, July 08-12, 1985, Chicago, Illinois Stuart M.
Shieber, The design of a computer language for linguistic information, Proceedings of the 22nd annual meeting on Association for Computational Linguistics, p.362-366, July 02-06, 1984, Stanford, California Stuart M.
Shieber, Using restriction to extend parsing algorithms for complex-feature-based formalisms, Proceedings of the 23rd annual meeting on Association for Computational Linguistics, p.145-152, July 08-12, 1985, Chicago, Illinois
LA RESOUTION D'ANAPHORE A PARTIR D'UN LEXIQUEGRAMMAIRE DE45 VERBES ANAPHORIQUES Blandine GEL.AIN & Gelestin SEDOGBO 26 place Ovale BULL cediag 94230 Cachan 78430 Leuveciennes (France) Abstract This paper presents a system which intends to resolve anaphora in the framework of the Discourse Representation Theory, arrd using a lexicon-grammar of anaphoric verbs, through the application of selection criteria for assignment of a referent to an anaphora.
From a semantic representation of text provided by a DRT system implemented in Prolog, the system uses several criteria of selection of referent.
One of these criteria is the anaphoric conditions of verbs described as a lexicon-grammar of anaphoric verbs.
The present paper investigates a transformational analysis of verbs related to their anaphoric behaviour, and the adequacy of extension of the lexicon~grammar of MGROSS to anaphonc conditions on verbs.
1. Introduction La Th~orie de la Representation du Discours (ou DRT 1) propose une approche unifiee de phenom~nes du langage naturel tels que le temps, I'evenernent, I'anaphore Elle se caracterise par sa filiation avec la semantique Iogique, et sa distance d'avec les niveaux de representations basees sur la Iogique des predieats et ses extensions.
Ainsi les notions de consequence Iogique et de validite (\[SEDO 87\]) peuvent s'appliquet naturellement aux structures maniputees par la DR1 Cette theorie propose une explication de la formation de I'anaphore, sans en proposer ta resolution.
Celle-ci passe en general par I'application de orit~res de selection syntaxiques, semantiques et pragmatiques, qui levent les ambiguites engendrees par t'usage de I'anaphore.
\[GUIN 85\], \[CARB 88\], \[RICH 88\] entre autres ont intloduit les notions de foyer et de contramtes successives 8 activer Cependant cos criteres sont paffois insufiisants pour etablir une relation evidente entre un nomet un pronom Dar~s sa theorie du Oouvernement-liage, 1 H.KAMP "A Theory of T\[uth and SemarY0c Interpretation" GroP.nendiik Amsterdam 1-(:JR t NCHOMSKY 2 explique I'anaphore ~ partir d'un mecanisme de liage et s'appuie sur la remarque que le liage d'une anaphore A son referent depend aussi des proprietes anaphoriques du vetbe.
Aucune etude empirique des proprietes anaphoriques n'a ete faite ~) ce jour, alors que toute approche de resolution d'anaphore devrait etre basee sur un texique des verbes et leurs proprietes anaphonques associees.
Le present article decdt une approche de resolution d'anaphore qui repose sur: la mise en oeuvre de ta DRT pour representer la semantique d'un discours; I'elaboration d'un lexique-.grammaire des verbes anaphotique@; un systeme de filtre base stir differents criteres de selection Ce systeme illustte la resolution automatique de certaines anaphores en partant de la representation semantique d'un texte obtenue d'apres la DRT los ptonoms que nous avons etudies font partie d'un type d'anaphore qui represente une relation pouvant s'etablir entre deux phrases saris mettre forcement en jeu une regle syntaxique (le pronom pout identifier un referent dans le discours precedent): Jean croit que Mane IU/ offre un //we Cet article est divise en cinq parties: introduction ~ notre travail, presentation de la DRT, puis de son implementation en Prolog, description des lexique-grammaires et leur extension aux proprietes anaphoriques, presentation generale de I'architecture de notre systeme de resolution, et enfin perspectives de creation systematique d'un lexique-grammaire.
2. La reprdsentatlon s~manticlue La DRT se fonde done sur un ensemble de regtes de construction traduisant un disoours en une representation semantique formelte: la Structure de Representation du Discours.
Pour chaque pattie de discours, une DRS est construite, boite pouvant err contenir d'autres, 2 "Government Binding", notee GB 3 Exlension des tables de verbes d#vetoppt~es au LADL, aux propri~tes anaphonques (cf \[GELA g2\]) Ac:iEs DE COLING-92.
NAtCI~S, 23-28 AO~ t992 90 1 PROC.
OI; COLING=92, NANI'E,'I.
AUG. 23-28, 1992 qui reprL~sente le contenu significatif de cette par'tie Une DRS complete est I'ensemble de plusieurs DRSs apparaissant t~ mesure que le discours continue.
La DRT etudie doric les contraintes sur cette continuation.
La forme d'une DRS consiste en une paire <U,Con> constituant deux z6nes, o0 U (univers) est un ensemble des r~fdrents #u discours representant les entites du disc, ours, et Con un ensemble de conditions qua doivent satisfaire ces referents.
Celles-ci sont des predicats et des relations de referents du discours mais peuvent etre plus complexes.
Notees comme en Iogique des pr~dicats, les conditions de verite sont definies par rapport & la possibilit~ d'incture la DRS dens un modele (pour plus de details, consulter \[GUEN 85\]).
D'autre part, I'extension d'une DRS ne peut changer les valeurs d(~j& assignees: tout Ce qui etait vrai auparavant restera vrai par la suite: Un aamion bansporte une charge u.K=\[xl,x2\], Con.K=\[carnion(Xl),charge(X2),t~'ansporte(Xl,X2)\].
Tout camion b'ansporte une charge U.K=\[ \], Con.K = \[:>,K1,K2\]; U.KI=\[Xl\], Con.K1= \[camion(X1)\]; U.K2=\[X2\], Con.
K2=\[charge(X2),transporte(Xl,X2.)\].
2. I La notion d'accessibilite On peut representer des restrictions configurationnelles sur les relations anaphoriques possibles entre les pronoms et leurs antecedents.
Ces restrictions sont obtenues en reduisant I'accessibilitd des referents.
L'accessibilit~ permet donc de determiner les liens anaphoriques entre un marqueur pronominal et un marqueur de discours.
Toute DRS est accessible d'eltem~me; son univets de marqueurs accessibles est I'unNers du discours de la DRS.
Dans une DRS implicative, la DRS antecedente est accessible de la DRS consequente Enfin, la relation d'accessJbilite est transitive.
Donc pour "tout camion qui transporte une charge la declare", I'antecedent du pronom obJet laest une charge.
Ici, U.K1 est accessible & K2.
Les conditions de continuite d'un discours sont aussi fonotion de I'accessibilite: la phrase: "il va & Berlin" ne peut continuer la precedente puisqu'aucun marqueur de diseours n'est accessible de la DRS K (voir ce sujet \[KASP 86\]) Ceci explique pourquoi une phrase comme: "Cheque chauffeur possC~de un camion, fl te conduit": doit etre formulae: "Chaque chauffeur qui poss~de un camion le conduit* (avec "qui poss~e un cam/on" comma extension de "cheque chauffeur") pour Otre representable.
Mais la DRT a des limites; elle n'explique pas la bonne formation de ce texte, par example: Cheque chauffeur transpo~te une charge.
II n)et plusieurs jours & la tivrer.
Ella ne sere livr~e qu'au bout de 3lOUtS Les cleux dernieres phrases, selon la DRT, ne peuvent suivre la premiere, & cause de la portee du quantificateur cheque.
Par contre la phrase "Cheque chauffeur fransporfe une charge qui ne sere livree qu'au bout de 3 jours" sere parfaitement representee par la DRT.
Ces r~gles obligeraient doric le Iocuteur & d~crire une situation en une seule phrase! 2.2 Imp.~.
mentation de la DRT Notre analyseur semantique demane avec des arbres syntaxiques resultant d'une grammaire de type GPSG, programmee en Prolog La grammaire semantique est bas~e sur les memes principes: unification de structures, augmentation de listes ordonnees, presentee sous forme de regles de reecriture suivies de contraintes, de type: ph -> gn gv <ph drs courante>=<gn drs courante> <gn suite drs>=<gv drs_courante> <gn argument>=<gv arg_sujet> <ph arg sujet>=<gv arg suJet> <ph arg objet>=<gv arg_objet> Ceci donne, 8 partir de regles DCG issues de la compilation des precedentes: traduire(ph(GN, G V), P) :~'aduire(G V, P2), traduire(GN, PI), imerge(P,\[courante\],Pl,\[courante), imerge(P l, \[suite\], F~2, \[courente\]), tmerge(Pl, \[arg\], P2,\[arg sujet\], imerge(P,\[arg sujet\], P2,\[erg_sulet), tmerge(P, \[arg_objet\], P2,\[arg_objet\]) soit la formule semantique: drs (arg sujet(Xl),arg objet(X2), cour(cond( \[imp(drs(cond(\[camion(X l)\]), univ(\[X1\])), drs(cond( \[charge(X2), transporte(X l, X2)\]),univ(\[X2\])) )\] )) ).
correspondant ~ la phrase "tout camion ffansporfe une charge".
C'est 8 partir d'une telle formula que commence la resolution anaphorique 3.Le lexique-clrammaire Le lexique-grammaire, represente sous forme de tables (matrices composees de colonnes ACIT~ DE COLING-92, NAN-IT~, 23-28 AOt~Zr 1992 9 0 2 PRoc.
OF COLING-92, NANTES, AUO.
23-28, 1992 et de rangees), contient les phrases strnples, les diff~rents emplois verbaux et los propri~tes qui leur sent assoei~es: nature semantique des arguments, trallsforrnations possibles et tours conditions, nombre et structure des complements, type de la preposition associ~e etc I/ consid~re la nominalisation comma la transformation d'une phrase contenant un op~rateur verbal, en une autre phrase contenarlt url op~rateur nominal.
On y introduit un verbe predicativement vide -verbe supportdent le r,~le est d'actualiser le substantif qui n'a pas de marques morphologiques susceptibles de le taire: l.uc complimente los acteurs = Luc fail des compliments aux acteuts 3 1 L'entoura~e lexical Nous nous limiterons dans cet article aux possessifs, sur lesquels \[GULL 81\], \[DANL 80\], \[GROS 89\] et \[VtVE 83\] notamment nous ont fourni des informations fort utiles.
L'examen des roots voisins du possessir est important, e.
g; Luci donne # L~.aj son i+ j argent Luc i donne a L~,a son i amour Dens le premier cas, donner est un verbe ordinaire (plein) alors qua dens le second, 41 est un verbe support (V-sup) Seut le substantif N2 change (\[NO donne ~ N1 N2jaoss\]).
Pourtant dens la premiere phrase, son peut ref~rer & trois personnes: si t'argent est & Lea, son est relie ~ L#a; si I'argent est ALuc, son est relie a Luc; si I'argent n'est ni ~ I'un ni ~ I'autre, son est relic,9 une tierce personne du discours pr~o~dent.
Dans la seconde phrase par centre il n'y a qu'un r#ferent: Luc, ~ cause du terme amour qui appartient aux roots "abstFaits" ou de sentiments, pour lesquels on ne peut pas trouver, dans ce type de structure, d'autre relation ~ sot\] qua le sujet, ici l,uc.
II s'agit de cor~f~rence obligatoire au sujet Dens "Luci cheque L~j par sos i id~'es ~ et "Luc i cheque Leajdans sos11rig, as", il y a le verbe chequer Pourtant dens ta pre.miere phrase sos est forcement relic 8 Luc.
II s'agit d'un cas de coreference obligatoire au sujet, induit par la preposition par.
Alors qua dens la seconde, la cor(~f~rence est oblJgMoire au cempl~ment d'objet sos est reli~ ~ L~4a, ~ cause de la preposition dens 3.2 Phrases construites aulour d'un V-sUlq On trouve des expressions verbales figees et d'autres mettant enjeu la paire Vsup/Npred mais sans 6tre des expressions figees.
I.es expressions verbMes fi~es sent de la torme \[NO V Nl-poss/, construites autour de variantes aspectuelles et d'op~.rateur causatif du verbe avozr, dent le N1 est teuioum "pattie du corps" ou "abstrait" ("Luc i rettent sos i tarmes'), et dent la hansformation en gnest impossible.
Elles peuvent aussi etre completees par url troisieme argument (\[NO V N1 Pr#p N2\] avec V support ou non.
L& encore, le nom (N1 ou N2) d~tenniue pal le possessif est "pantie du corps" ou "abstrait", et aucune restructuration n'est possible: L.ucHelte son i d#volu sur L~a l.uci #erda L#a sous sa i protection Si le possessif d~termine I'objet direct dens une structure INO V Nl-poss Prep N2\], \[Prep N2\] pout 0tre remplac# par une compl~tive I'infinitive, donnant \[NO V Nl-poss Pr~p Vlnf\]: Luc i passe son i temps au travail Luc~ passe son i temps ~ travailler Daris toutes ces phrases, la cor~ference est touIours obligatoire au sujet \[.es expressions non #g#es sent construites autaur de verbes support ou de variantes aspectuelles 1NO V N%poss\] ou \[NO V Nl-poss Prep N2\]: Luci a perdu ses i illusions (~ur L~a).
On ne peut pas transformer \[NO V Npred (Prep N1)\] en \[NO V Npred de N2 (Prep N1)\] (* Luc a pe~du les illusions de Paul sur L~a).
En d'autres termes, I'argument NO de Nprep est le sujet du verbe, mais ce verbene pout prendre une expression \[Npred Pr#p NO\] comme complement (ici: los illusions de Paul).
La presence ou non du complement d'objet indirect n'a pas d'incidence sur la relation qui lie le possessif au sujet.
Ces phrases nese construisent pas avec une infinitive.
Certaines expressions non fig~es se construisent avec le possessif comma d(~terminant de N3; dens cecas, los trois arguments sent obligatoires etla cor~f~rence n'est pas obtigatoire au suiet, II s'agit de cas de norPcor~f~rence obligatoire ~ I'objet ("Lucj met Lea ~ sai4 j disposition") On pout restructurer en reliant les complements par ~tre: Lea est ~ ta disposition de (L uc, Paul) Fin resume, parmi les phrases construites autour dt t Vsup, I'adjectif possessif qu'elles contieonent n'est jamals coref~rent ~ I'objet, AcrEs DE COLING-92, NANrHS, 23-28 AO~r 1992 9 0 3 l'rtOC.
Ot; C()I.ING-92, NAN'I'ES, AUG.
23-28, 1992 mais toulours au moins cor(~f~rent a.
sujet En reconnaissant d'emblee ces phrases et teur verbe comma Vsup, on pourra r/esoudre automatiquement I'anaphore Pour cela nous proposons de reperer les autres phrases, pu~s de considerer les phrases non reconnues comme ~tant de carte categorie 33 Phrases construites autour de verbes ordinalres Elles s'articulent autour d'un verbe ~ un ou deux arguments, rNement predicatif de la phrase puisqu'il definit la structure des arguments II est determin#, par: le nombre d'arguments I'articulation syntaxique de ces arguments les traits semantiques de ces aguments Dans la st\[ucture \[NO V Nl-poss\] que ron ne peut poursuivre avec \[&/de N2\], la coreference est obligatoire aun autre nero que le sujet NO (du discours anterieur) Luc approuve son choix Luc apptouve le choix de L#a Par centre, si une phrase a deux arguments et qu'elle peut etre completee par un troJsiCme, la presence ou non de ce dernier fait varier la coref~rence, ou tout au moins la preference entre lee antecedents possibles: Luc i avoue son i depit.
Lucj avoue soni+ j d~p/t a Lea k La relation de coreference existe toujours entre le possessif et le sujet dane ~es phrases simples \[NO V NI\]; ou complet~es par \[Pr~p N2\] oQ la relation peut aussi exister entre le poss et un referent du discours ant~rieur.
n a donc la un cas de non-cor#f~rence obligatoire ~ I'objet Tous les verbes qui donnent ces r~sultats dans une telle structure appartiennent a la table 9.
Quand le syntagme prepositionnel est obligatoire, on distingue les phrases ou I'adjeetif possessif determine le N1 et celles oO il d~terrnine le N2.
Parmi tes premieres, on trouve une cer~fdrence obligatoire au sujet Iorsqu'il y a possibilit~ de pronominalisation: Luc i consacre sa i vie a ta pemture.
Luc se consacre e la peinture OU de verbalisation simple: Luc i accorde son i pardon ~ L#a Luc pardonne a L#.~ Mais pour celles dent la transformation donne une completive ~nfinitive \[nO V N1 VinfJ, la coref~rence est obligatolre ~ I'ebjet: Luc i motive L~ajdans son 1 travail Luc motive Lea g~ travailter Tous ces verbes appartiennent a la table 11 Parmi les phrases de structure \[NO V NI Pr~p N2-poss\], la relation est obligatoire au sujet, ou obIigatoire ~ t'objet Dans les exemples suivants (verbe de la table 4), oQ la 1estructuration est possible en IN2 de NO V N1\], la relation est etablie entre le possessif et le sujet, Iorsque ta preposition est par ou avec: Luc i cheque L&aj paffavec see i idles.
Lea Ides de Luc choquent Lea.
Pa~ centre, la mC~me phrase avec clans ou pout; par exemple, donne \[a transformation \[NO V N2 de N1\], et on etablit alors la relation entre le possessif et le ~:ompldment d'ebjet: LUC I cheque Lea\] darts.~esj Ideas.
Luc cheque tee id#es de L&a.
4.Architecture .qdndralle du s.ystdme Notre systeme se compose donc d'un analyseur sfntaxirlue qui donne des arbres & paftir desquels un analyseur s~mantique produit des ORS.
C'est sur etles qu'oNrera le piogramme de ~euolution anaphorique Ce systerne se complete d'url fichier de verbes par tables, et d'un fichier "fonetionnel", constitue ~ mesure de I'arlalyse semantique, ou sent stockes tousles norns et pronoms, et leur forlction grammaticale.
La procedure de r~solution: apres reperage des pronoms, commence par une recherche des verbee (A chacun est associ~ un trait pour sa table d'appartenance) et de leur structure, dans le fichier lexique-grammaire.
Si cela est trouve, on cherche sila coreference est obligatoire.
Si oui, le traiternent est termine.
Sinon, il faut activer d'autres filtres syntaxicos~mantiques: en partant des listes ordonn~es de pronoms, univers et conditions, on verifie la compatibilite de fonction 4, de genre et de hombre, semantique 5.
II faut parfois chercher te verbe darts plusieurs tables.
Si I'identite de structure entre le texte etles fables n'est pas ~tabtie, on examine I'entuurage substantival du verbe si robjet est concret, il taut activer tes autres filtres.
S'il est abstrait ou "partie du corps", on a affaire ~ une phrase a verbe suppolt (peut-t~tle figee) dont le statut induit la (non) coreference obtigatoire.
&ConclUsion et perspectives \[~emarques sur Des travaux: 4 Un candidat sulet est pr~f~e aun autre pour tre reli~ & un pronom suJet, dans deux phrases dtes paralleles 5 Des traits semanhques sent associes aux roots lexicaux ACRES I)E C()LIN(;-92, NAm'~s, 23-28 aot~rr 1992 9 0 4 Pr~oc.
oJ; COLfNG-92, NANtES, AUrJ.
23-28, 1992 -lous les ver'~;t; donl~uHt lieH ~'~ Hn type de construction partmulier, appadiormuHt ~'~ k~ ~me table.
Sur chaque table on p~;\[IL contraindre la relation de (no~ 0 coiet~rence entre le possessif et Hn argurnent du w~'ibe.
..Tous les uornpl6ments d'obje/ d6termine par le possessif sent "abstraits" uu "pattie du colps".
Avec d'aubes 1~ulns, nun pr~dicatifs, les verbes sotR ordinailt;s et on ne peut resoudre I'ambiguRe anapholique Done en ajoutant ces caract~ristiques dabs la table en question on peat resoudrc, automatiquement I'anaptiofe Ccci ~)11i/iHi~: I'hypath~se que routes les tables peuvent, a priori, tre uinsi complete;as par les sp~.cillcites liees a remploi d'adjectif..
possessifs et permethe ainsi aH syst~me d'viter d'autres filtres plus co0teHx en calClll et pas toujours fiables.
I1 taut done ~.tablir HI~ lexique-grammaim des ve~bes anaphorique'., (pris darts une structure mettant en jeLl till pronom ou, ici, tin adjectif ~x~ssessif) Darts la table 4 du lexiquegran.nu.e, pal exemple, nous repaltissons leS verbes erl: roul~.~__J_: verL~+par (ou avec et pa~ft~isd~;), et cor~f~mnce obligatoite au a~jet (NO): \[ uc i d,~prime L g, aj par son i attilu&e.
r u.q.E~_~e~: verbe+darts, e.t co~ef~len(;u obligatoire ~ I'objet: \[.uc i d~lonne l.g~j dg~tla ~esj propo~.
NOUS avons ajot~te A la table 1me colol,le concernant la pr6sence ou non de #)t~p N2\], divisee en deux colorines: les deux cas,3u cor~ference obligatoile.
Les verbes de la table 11 suHi ~;partis an: ~: V N1 darts \[NO V NI & VOinfl puut etre remplace par pronomi~-~#sation (hi verbe Le possessif est tore.~.ment coref~lent au sujer Luc consacre se vie (e d~.ssme:) au d~s~it~ Luc se consacre (,~ dessJne 0 au dessin Si N1 n'est pas abstrait la pior~t~mirlalisation est imF~ssible:(*Luc con~a:m son hao#~ au dessm): Grou~: M6rne structt.e sans prouontina.lisation.
II peut y avoir simple ve~r-b~l/saiJ~a~ L.e possessit est toujol,s cor~f6rc'r~t au soj~ N1 est toujours abst~ait et pemiet M verbalisation: Luc appolle SOn,sogtie~l ~ c(flh ~.
',dfa#O Luc ~outJent celia ~nb6"prl~e; ~: Verbes qHi, clans la tlarlsknmatiu1~ de \[NO V N1 &/pour VOilffJ eH \[NO V Nf ~/dans N2\] oQ N2 est d,~terniin,} par un poss~ssif, induisent obligatoirement une I t ~~t.,!~fin Ittk; I.(~:? pou; #av~#liu'~ Nuuu avo~ls &t~;Hdll t;~tke tabk; (a~ y ajoutaHt ulle CLtla(..tk')if, ti(\]iit;i CXi'G\[t\](I(;(!
~)11 111011 d'Llll(; Stlucttll~; lllOllOlllilal(~, .~:;tl\[}~iJvik::k'~e {;11 110i14, pol_~r k;,,,,';|ltli;klli~tu i\]hrastiq111:,-,; (,~hldi(~e~ (;t I'oblig~ioil (:lt~ colOi:Or<~i .L, qtli k'H~ c(~lll.~,'.po|ld \[(:Ab',l ~ \[\]~q JCAFd ~,()1\11<l !, I-~ I~ROWN ~_~lq~//h,:v~\] Re.~o!~jti_u_t;: q mul(V :gha2?g~ A#pl~?:~uIE ( .:131 IN(;, l }uduHa.-~t.
AI tj 1988 \[CI IAN,L~TJ \].1 "4 I/\Nll !R, !C,! il All\J, (; SEDf)(;I t ) Ih~, A_~rc,.hu Uni~eyL't~ .SVq/~xe~ ~21 rift M,Mrn_a-f~J~Ll(~J!.{:c;,9 6s,'IFC(k I, Sofia Antipoti~a Nov 1987 \[I)ANL 89\] L,DANL(:),% Rep!~se.~jt~#90.~: d','ni.fm~#i~p~; huguL.4igU~L.~.
COf*St~O*~Ql*S /V t}~rt; t/*e~'p X.
I |10.~,t) de 3~;I m~ cycle, LADI Pail! 7, !i\]\[;U JGt I/~, 91} HGFAAIrq these de ductorat (~'~ p:aHii,t~) I ~aH.'; /, 199>' \[~z,t ~()~ 89\] (:; (~Ro,~.~; lug~t~J~.~ammair~_:.
!AUI.,::L Lh,v.
Paris; ? S\[fMANTICA f>afi:-: Juin igB.q \[GROS /sJ M GhtOSS M~t!ode.u.~jL~A/~{~x P.
L::d.{ k;in|arln, i'ari-u 1!)\[,*; \[k-Jk ILiN 85\] F.GI IENTI tNLR, P,'4ABAIli \]-~ ser~{qtjgp.
F:NS, I )nivel-sit0 t tlbingeu I )(.'-u 1985 \[GIlIN 85\] R GIJIN\[)ON Focusing.
MCC Austin AC.I.i.'185 \[Gt I1\[ 81\] A.Gt IILI.EI ~, CLECI,ERIZ f 3\[n~s.
~ e tpj~_ff;\]&; :;S3ff_(tntiq ~!.'..
\[.angages n"63.
Ed I alottsse Paris "H;WI \[I(AX;t > 8.5\] W KASPER Dtscourse R~#/es~/!t~liun 7hec~E.
Rapport ACORD, Ur~iv.Stuttgart, Mai 1986 \[IdCtl 88\] ERICH, S.LUPERFC>Y tir~ A.
MCC A u sti n, A.
';. l..
f#,v 1988 \[t;I \[)O 871 C SEI )( )GB( ) SEatet~le r~ueJ_.
~OJL-U~t2g)!!s~{. Thbse de DooLorat d'Etat \[J ive~sit#, dr; Ma~seille, 1:)87 \[VlVl/83J I VIVES ::~}rne Cyck; IAI)I Pad:; 7 1983 \[WAI)A 811 tt WAI\]A, N.AsI let4 /~e~o/(~hlo q t hfivn.~.ity Texas, Austin, .l~m 1.987 References 1 Jaime G.
Carbonell, Ralf D.
Brown, Anaphora resolution: a multi-strategy approach, Proceedings of the 12th conference on Computational linguistics, p.96-101, August 22-27, 1988, Budapest, Hungry 2 {CHAN 87} T.
CHAWIER, B.
GELAIN, C.
SEDOGBO Une Approcho Unifice de le Syntaxe et de la Smantigue.
Congios AFCET, Sofia Antipolis Nov 1987 3 {DANL 80} L.
DANLOS Reprsentations d'informations linguistigues, constructions N ire prep X.
These de 3eme cycle, LADL, Paris 7, 1980 4 {GFLA 91} B.
GELAIN Thse de doctorat ( paratre) Paris 7, 1992 5 {GROS 89} G.
GROSS Dsanbiguisation smantique  l'aido d'un lexique grammaire.
LADL et Univ.
Paris 7.
SEMANTICA. Paris Juin 1989 6 {GROS 75} M.
GROSS Mthodes en syntaxe.
Ed. Hermann, Paris 1975 7 {GUEN 85} F.
GUENTHNER, P.
SABATIER Formal Semantics and Knowledge Representation.
FNS, Univ.
Tubingen. Dc 1985 8 {GUIN 85} R.
GUINDON Anaphora Resolution: Short-Term Memory and Focusing.
MCC Austin.
A.C.I., 1985 9 {GUIL 81} A.
GUILLET, C.
LECLERE Formes syntaxiques et prdicats smantiques.
Languages n"63.
Ed Laroussa.
Paris 1981 10 {KASP 85} W.
KASPER Montague Grammar, Situation semantics and Discourse Representation Theory.
Rapport ACORD, Univ.
Stuttgart, Mai 1986 11 {RICH 88} E.
RICH, S.
LUPERFOY An Architeture Program for Anaphora Resolution.
MCC Austin.
A.C.I., fv 1988 12 {SEDO 87} C.
SEDOGBO De la Grammaire en Chaine du Franais  on Systme Question-Rponse.
Thse de Doctorat d'Etat.
Univ. de Marseille, 1987 13 {VIVE 83} R.
VIVES Avoir, prendre, perdre: constructions  verbe support et extensions aspactuelles.
Thse de 3me cycle.
LADI. Paris 7 1983 14 {WADA 87} H.
WADA, N.
ASHER A Computational Account of Syntactic, Semantic and Discourse Principles for Anaphora Resolution.
University Texas, Austin, Jan 1987
Parole et traduction automatique : le module de reconnaissance R A P H A E L
Mohammad AKBAR GEOD, CLIPS/IMAG Universitd Joseph Fourier, BP. 53 38041 Grenoble cedex 9, France Mohammad.Akbar@imag. fr Jean CAELEN GEOD, CLIPS/IMAG Universitd Joseph Fourier, BP. 53 38041 Grenoble cedex 9, France Jean.Caelen@imag.fr

R6sum6

Pour la traduction de parole, il est ndcessaire de disposer d'un syst~me de reconnaissance de la parole spontande grand vocabulaire, tournant en temps rdel. Le module RAPHAEL a dtd congu sur la plateforme logicielle de JANUS-III ddveloppde au laboratoire ISL (Interactive Systems Laboratory) des universitds Karlsruhe et Carnegie Mellon. Le corpus BREF-80 (textes lus extraits du Journal Le Monde) a 6td utilis6 pour le ddveloppement, l'apprentissage et l'dvaluation du module. Les rdsultats obtenus sont de l'ordre de 91% de bonne reconnaissance de mots. L'article ddcrit l'architecture du module de reconnaissance et son int6gration /~ un module de traduction automatique.
Introduction

La traduction des documents dcrits a fait de rdels progrbs pendant ces dcrni6res anndes. Nous pouvons constater l'6mergence de nouveaux syst6mes de traduction de textes qui proposent une traduction soignde en diffdrentes I Synthbse ~ de la parole ] ~

langues[1]. I1 semble envisageable de les adapter pour la traduction de l'oral, /t condition d'en amdliorer le temps de rdponse et la robustesse : c'est le <<challenge >7 pos6 fi ces systbmes mais aussi au module de reconnaissance de la parole. Un syst6me de traduction de l'oral repose sur l'intdgration des modules de reconnaissance et de synth6se de la parole et des modules de traduction, pour obtenir une boucle complbte d'analyse et de synth6se entre les deux interlocuteurs [Fig. 1]. Le projet CSTAR-II [3] est un projet international dans lequel toutes les dquipes travaillent sur tousles aspects de ce mod61e. Pour permettre /t deux personnes de communiquer, il faut deux sdries de processus symdtriques dans les deux langues : un module dc reconnaissance pour acqudrir et transcrire les dnoncds dits par un locuteur dans sa langue puis un module de traduction qui traduit la transcription dans la langue du destinateur ou dans un format d'dchange standard (IF = Interchange Format) et enfin un module de synth6se de la parole (et de gdndration si on utilise le format IF) dans la langue cible du

rReconnaissance Traduction la instantan_._____~ ) ~ ,___~de parole ._J

!/
(Reconnaissance'~_~ T r a d u c t i o n ' ~ ~ ~. instantan6 __) ~. de la p a r o l e )
Fig. 1. L'architecture d'un syst~me de traduction instantan~e.

36

destinateur. Dans le cadre du projet C-STAR II nous avons en charge la conception et la rdalisation du module de reconnaissance de la parole continue h grand vocabulaire pour le fiangais. Nous collaborons avec l'6quipe GETA du laboratoire CLIPS-IMAG et le laboratoire LATL pour la traduction automatique et le laboratoire LAIP pour la synth~se de la parole. Ce consortium s'est fix6 l'objeetif de r6aliser un syst6me de traduction de I'oral pour le frangais. Dans cet article nous allons tout d'abord presenter l'architecture du syst6me de traduction et la plate-forme de ddveloppement JANUS-III [2], puis les diff6rentes 6tapes du d6veloppement du module RAPHAEL et enfin, les premiers rdsultats obtenus. 1 R A P H A E L p o u r la T r a d u e t i o n L'architecture du syst6me de traduction de parole est compos6e de trois modules essentiels (la reconnaissance, la traduction et la synth~se de la parole) [Fig. 2]. Dans ee projet nous utilisons ARIANE et GB [3] pour la traduction ct LAIP-TTS [4] pour la synth6se. Le

point de vue de la robustesse) nous envisageons l'intdgration d'une seconde couche de contr61e pour permettre le <<rescoring >> des hypoth6ses en tenant compte des taux de confiance associds aux diff6rents mots de l'dnoncd reconnu.

1.1

Plate-forme de J A N U S I l l

I

Recolmaissance la Parole ] de RAPIIAEL(CLIPS/IMAG-ISL) Texte
V

Contr61e

Cette plate-forme de traduction a dtd ddvelopp6e dans le laboratoire d'ISL des universitds Carnegie Mellon et Karlsruhe et contient tous les composants ndcessaires au ddveloppement d'un syst6me de reconnaissance phondmique/t grand vocabulaire h base de Cha~nes de Markov Cach6es (CMC) et de rdseaux de neurones. La facilitd d'dcrire un module de reconnaissance en langage Tcl/Tk avec JANUS-III nous permet d'adapter ses capacitds selon les besoins d'application et les caractdristiques du frangais. De cette plate-forme, seul le moteur de reconnaissance est directement exploit& Mais le travail de pr6paration des bases de donndes, l'apprentissage des mod6les de phon6mes, l'dvaluation sont dgalement effectuds dans cet environnement de programmation. Le langage PERL est en grand partie utilisd parall61ement pour traitement du texte du corpus. Les d6tails techniques de JANUS-III sont donnds dans [2], [5], [6]. Cependant nous en prdsentons bri6vement quelques points ci-apr6s. 2 Le Module RAPHAEL L'architecture du module de reconnaissance RAPHAEL est pr6sent6e sur la [Fig. 3]. L'analyse de la parole produit une suite de vecteurs de param6tres acoustiques. Ces vecteurs sont utilis6s par un moteur de recherche base de CMC pour estimer la suite des phon6mes 6nonc6s. Un mod61e de langage stochastique h bigramme et trigramme, et un dictionnaire des variantes phon6tiques sont en parall61e exploit6s pour restreindre le champ de recherche I. ALl cours de la recherche le dictionnaire phon6tique fournit le(s) phon6me(s) suivant(s). Le modble probabiliste de langage base de bigramme et de trigramme est utilis6 Iors de la transition entre deux mots pour fournir un ensemble de roots [Fig. 4]. 1 Avec 45 phon6mes en moyenne une suite de cinq phon6mes se transforme th6oriquement en un arbre de d6cision de 455 = 184,528,125 feuilles ! 37

~_~Traduction Automatique IANE(GETA),GB(I,ATL) J


I Synth~se la Parole de LA1P-~VI'S (LAII') 1 Fig.2. Lescomposantsdu syst~me
d6veloppement du module de reconnaissance RAPHAEL a dt6 effectu6 sur la plate-forme iogicielle de JANUS-Ill. RAPHAEL donne en sortie un treillis de roots sous le protocole TCP/IP. Le traducteur utilise ce rdsultat pour en donner une version traduite. Cette version est ensuite envoyde au synthdtiseur de la parole. Dans cet article nous nous int6resserons seulement au module de reconnaissance RAPHAEL. Pour l'instant la stratdgie d'dchange entre les modules est enti6rement sdquentielle. Afin d'amdliorer le rdsultat final (surtout du

~_~__~ Acquisitionde 1 la parole

I,

Base de donndesdes paramdtres ] des ChaTnesde Markov Cachdes

J
~Z%%? ' o

I Traitementnumdrique,Estimationdes param~tresacoustiques ~ ModUlestochastiquede langage (bigrammeet trigramme)

Chainesde MarkovCachdespour la reconnaissancephondmique Dictionnairephondtique Dict (vocabulaire de reconnaissance)

Fig. 3. Sehdmadu modulede reconnaissance phondmiqueRAPHAEL.
2.1 C
h a i n e d e M a r k o v Cachdes cet alignement l'algorithme de Baum-Welch [5] procdde ~ l'estimation des paramdtres de chaque CMC prdsente dans la cha~ne. Ce procddd est rdpdt6 pour tous les 6noncds du corpus d'apprentissage et cela plusieurs fois. La prdsence des diffdrents contextes phondmiques permet /i ce procdd6 de minimiser le taux d'erreur de reconnaissance. L'dvaluation du taux d'erreur /l la fin de chaque itdration permet d'dtudier l'avancement de l'apprentissage.

Pour utiliser les CMC il faut conduire une phase d'apprentissage prdalable dans laquelle on adapte les probabilitds des transitions et des symboles sortis pour un phondme donnd de manidre ii ce que la probabilit6 du processus associd soit maximale. Les paramdtres des moddles et la transcription phondtique des 6noncds du corpus sont deux 616ments essentiels d'apprentissage. RAPHAEL comporte 45 CMC reprdsentant 42 phondmes de base du fran~ais et 3 moddles pour le silence et le bruit. A quelques exceptions prds les CMC se composent de trois 6tats. Le vecteur de paramdtres d'entrde est de dimension 122. Les CMC ont 16 distributions Gaussiennes pour chaque dtat. Lots de l'apprentissage nous produisons la transcription phondtique correspondante chaque dnoncd (cela se fait ~ l'aide du dictiomaaire phondtique). Pour chaque 6nonc~ les CMC correspondant aux phondmes sont concatdndes pour crder une longue chaine. Ensuite i'algorithme de Viterbi [5] propose un alignement de I'dnoncd avec cette chaine. Avec

2.2

Mod/~lede iangage stochastique

2 Les
coefficients MFCC [5] d'ordre 16 sont calculds sur une trame de 16 ms de parole, avec un pas d'avancement de 10ms. La parole est 6chantillonnde /l 16 kHz et sur 16 bits. Les MFCC, l'dnergie du signal, et leurs premiere et seconde ddrivdes (51 valeurs) subissent ensuite une analyse en composantes principales (ACP) pour rdduire la dimension du vecteur /l 12. La matrice d'ACP est calculde avant la phase d'apprentissage, sur un grand corpus enregistrd. 38

Afin de rdduire le champ de recherche, un moddle de langage doit ~tre utilisd. Bien que dans les systdmes /i commande vocale qui utilisent une syntaxe rdduite les grammaires finies ou rdcurrentes peuvent ~tre utilisdes, ceiles-ci ne sont pas capables de ddcrire tousles phdnomdnes de la langue par[de (ellipses, hdsitations, rdpdtitions, etc.). Pour cette raison il est souhaitable d'utiliser un moddle stochastique qui estime dans un contexte donnd, la probabilit6 de succession des mots. Dans le moddle actuel les contextes gauches d'ordres un et deux (bigramme et trigramme) Sont en marne temps exploitds. Le bigramme est utilisd dans la premidre phase de recherche pour crder un treillis de mots, puis le trigramme est utilisd pour raffiner le rdsultat et ddterminer les N meilleurs phrases plausibles. Le moddle de langage se charge en m~me temps de ia rdsolution de l'aceord en frangais. Le calcui des paramdtres de ce moddle a dt6 effectual /t partir des corpus enregistrds et transcrits. Dans l'6tat actuel un vocabulaire de 7000 mots a dt6 sdlectionnd.

-..

-

L'hypoth6se e mot #1

--:~,.

L'hypoth6se de mot #2

Repr6sentation d'un phoneme

Dans un mot le dictionnaire phon6tique est utilis6 pour trouver et enchMner les phonemes suivants selon les variantes phon6tiques disponibles.

Pour d6terminer les roots et les phon6mes suivants le module stochastique du langage et le vocabulairc transcrit en phon6tique sont en m6me temps utilis6s.

Fig. 4. Representation de I'algorithme de recherche

2.3 Dictionnaire
Phon~tique
La conversion d'une chMne d'hypoth~ses phon6tiques en une chMne orthographique se fair ~t partir d'un dictionnaire phon6tique. Pour couvrir un grand nombre de prononciations diff&entes dues aux diff&ents dialectes de la langue et aux habitudes des locuteurs, ce dictionnaire contient pour chaque mot un ensemble de variantes phon6tiques. A chaque hypoth6se de mot propos6 par le mod61e de langage on associe cet ensemble de variantes. Ind6pendamment donc de la variante utilis6e dans 1'6nonc6, nous obtenons la m6me transcription orthographique. Nous utilisons sp6cifiquement cette technique pour couvrir les variantes produites par ia liaison, par exemple : Je suis parti de la maison. Je suis all~ h la maison. 3 (Z& sHi paRti ...) (Z& sHiz ale ...)

ensemble de BREF-80 comprenant les 6nonc6s de 4 femmes et 4 hommes a 6t6 utilis6 pour l'6valuation 4. Le vocabulaire a 6t6 transcrit soit manuellement, soit ~ partir du dictionnaire phon6tique BDLEX-23000. Le module de langage a 6t6 estim6 fi partir de BREF-80 et un corpus de texte d'~ peu pr6s 10 millions de mots extrait du journal Le Monde. Pour l'initialisation des CMC, au lieu d'utiliser les valeurs al6atoires (technique habituelle), nous avons choisi d'utiliser les mod61es issus du projet GlobalPhone [7]. Pour chaque phon6me de notre module nous avons manuellement choisi un phon6me dans une des langues support6es par GlobalPhone (principalement allemande) et nous avons utilis6 ses param6tres comme valeurs initiales de nos CMC. Ensuite ces mod61es ont 6t6 adapt6s au frangais au moyen de l'algorithme d'apprentissage d6crit en 2.1. A la fin de chaque it6ration et ce pour 3

L'apprentissage
4 Les
sous-corpus de l'apprentissage et de l'6valuation n'ont aucun 6nonc6 et locuteur en commun. En r6alit6, nous avons enlev6 tousles 6nonc6s en communs entre ces deux sous corpus. Ainsi le sous-corpus d'apprentissage comprend 4854 6nonc6s et le sous-corpus d'6valuation 371 6nonc6s. Nous avons retir6 105 6noncds pour assurer la disjonction des deux sous-corpus. 39

Le corpus BREF-80 [8] comportant 5330 6nonc6s par 80 locuteurs (44 femmes et 36 hommes) 3 a 6t6 utilis6 pour les phases d'apprentissage et d'6valuation. Un sous3 BREF-80 contient 3747 textes diff6rents et environ 150,000 mots.

it6rations, le syst~me a 6t~ 6valu6 avec le sous corpus de l'6valuation.

4 R6sultats
Les r6sultats d'dvaluation en terme de taux de reconnaissance sont donn6s dans le [Tableau 1]. Systdmes M0dbles issus de GlobalPhone Premiere it6ration Troisi6me it6ration % mots reconnus 29 88,8 91,1

Tableau 1. Les r6sultats de 1'6valuation

4.1

Commentaires

obtenus. Notre but est d'am61iorer le taux de reconnaissance par l'utilisation des mod61es phon6tiques contextuels et d'61argir le vocabulaire utilis6/t plus de 10000 mots. Pour atteindre ce but nous allons sp6cialiser le vocabulaire dans le domaine du tourisme et utiliser d'autres corpus de la parole spontan6e dans ce domaine avec un nombre plus important de locuteurs. En mfime temps nous d6finirons un protocole d'6change plus 61abor6 avec le module de traduction afin de permettre la communication d'informations linguistiques et statistiques au module de traduction, toujour dans le but d'amdliorer les performances de notre syst6me.

Une tr6s bonne initialisation de certaines consonnes identiques dans des diff6rentes langues (p, t, k, b, d, g, etc.) a rapidement permis d'obtenir un syst6me fonctionnel. On constate une saturation tr6s rapide du taux de reconnaissance d6s la troisi+me it6ration. Nous pouvons distinguer trois types de probl6me qui nous empachent d'atteindre un meilleur taux de reconnaissance :  Fautes de frappe dans le texte du corpus,  Transcription erron6e ou insuffisamment d6taill6e des ~noncds, ,, La couverture partielle de toutes les variantes phon6tiques d'un mot. Ces trois probl~mes sont les causes d'un grand nombre d'erreurs d'alignement qui vont directement influencer le r6sultat final. Nous devons donc effectuer une v6rification compl6te du corpus et du dictionnaire phon6tique. Les mots hors du vocabulaire sont fi l'origine d'un pourcentage important d'erreurs. En effet, dans 371 6noncds du sous-corpus de l'6valuation nous rencontrons environ 300 mots hors vocabulaire. Ces mots repr6sentent environ 3,5 % de la taille du vocabulaire. I I n e sont pas reprdsentds dans le corpus d'apprentissage et leur transcription n'existe pas dans le dictionnaire phon6tique.

Remerciement
Nous remercions Alex Waibel pour la mise /t disposition de JANUS-III et Tanja Schultz pour son support scientifique et technique dans l'utilisation des r6sultats du projet GlobalPhone.

References

1 Hutchins
W. J. (1986) Machine Translation : Past, Present, Future. Ellis Horwood, John Wiley & Sons, Chichester, England, 382 p. 

2 Finke
M., Geutner P., Hild H., Kemp T., Ries K., Westphal M. (1997) : The KarlsruheVerbmobil Speech Recognition Engine, Proc. of ICASSP, Munich, Germany. 
																		
3 Boitet
Ch., (1986) GETA's MTmethodology and a blueprint for its adaptation to speech translation within C-STARI1, ATR International Workshop on Speech Translation, Kyoto, Japan. 

4 Keller, E. (1997). Simplification of TTS architecture versus Operational quality, Proceedings of EuroSpeech'97, Rhodes, Greece. 

5 Rabiner
L., Juang B.H. (1993), Fundamentals of Speech Recognition, Prentice Hall, 507 p. 

6 Haton
J.P., Pierrel J.M., Perennou G., Caelen J., Gauvain J.L. (1991), Reconnaissance automatique de laparole, BORDAS, Paris, 239 p. 

Schultz T. Waibel A., Fast Bootstrapping of LVCSR systems with multilingual phonem sets, Proceedings of EuroSpeech'97, Rhodes, Greece. 

Lamel L.F., Gauvain J.L., Eskenazi M. (1991), BREF, a Large Vocabulary Spoken Colpus for French, Proceedings of. EuroSpeech'91, Genoa, Italy.
References 1 Appelt D.
and Israel D.
(1999). Introduction to Information Extraction Technology.
(IJCAI-99) Tutorial, Stockholm, Sweden (available at: http://www.ai.sri.
com/~appelt/ie-tutorial/) 2 Masayuki Asahara, Yuji Matsumoto, Extended models and tools for high-performance part-of-speech tagger, Proceedings of the 18th conference on Computational linguistics, p.21-27, July 31-August 04, 2000, Saarbrcken, Germany 3 Frdric Bchet, Alexis Nasr, Franck Genet, Tagging unknown proper names using decision trees, Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, p.77-84, October 03-06, 2000, Hong Kong 4 Daniel M.
Bikel, Scott Miller, Richard Schwartz, Ralph Weischedel, Nymble: a high-performance learning name-finder, Proceedings of the fifth conference on Applied natural language processing, p.194-201, March 31-April 03, 1997, Washington, DC 5 Andrew Eliot Borthwick, Ralph Grishman, A maximum entropy approach to named entity recognition, 1999 6 Collins M.
and Singer Y.
(1999) Unsupervised models for named entity classification.
In Proceedings of EMNLP/WVLC, 1999, MA, pp.
189--196. 7 Cucchiarelli A.
and Velardi P.
(1999) Adaptability of linguistic resources to new domains: an experiment with proper noun dictionaries.
In Proceedings of the Vextal Conference, Venice, Italy, pp.
25--30. 8 Andrei Mikheev, Marc Moens, Claire Grover, Named Entity recognition without gazetteers, Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics, June 08-12, 1999, Bergen, Norway 9 Raymond J.
Mooney, Induction Over the Unexplained: Using Overly-General Domain Theories to Aid Concept Learning, Machine Learning, v.10 n.1, p.79-110, Jan.
1993 10 MUC-6 (1995) Proceedings of the Sixth Message Understanding Conference (DARPA), Morgan Kaufmann Publishers, San Francisco.
11 Poibeau
T and Kosseim L.
(2001) Proper-name Extraction from Non-Journalistic Texts.
Proceeding of the 11th Conference Computational Linguistics in the Netherlands, Tilburg.
Netherlands, Rodopi.
12 Satoshi
Sekine, Yoshio Eriguchi, Japanese named entity extraction evaluation: analysis of results, Proceedings of the 18th conference on Computational linguistics, July 31-August 04, 2000, Saarbrcken, Germany 13 Silberztein M.
(1993) Dictionnaires lectroniques.
Masson, Paris.
14 David
Yarowsky, Unsupervised word sense disambiguation rivaling supervised methods, Proceedings of the 33rd annual meeting on Association for Computational Linguistics, p.189-196, June 26-30, 1995, Cambridge, Massachusetts
References 1 Benson, M.
(1990). Collocations and general-purpose dictionaries.
International Journal of Lexicography, 3(1), 23--35.
2 Peter
F.
Brown, Jennifer C.
Lai, Robert L.
Mercer, Aligning sentences in parallel corpora, Proceedings of the 29th annual meeting on Association for Computational Linguistics, p.169-176, June 18-21, 1991, Berkeley, California 3 Catizone R., Russell G., and Warwick S.
(1989). Deriving Translation Data from Bilingual Texts.
In Proceedings of the First International Lexical Acquisition Workshop, Detroit.
4 Church, K., Gale, W., Hanks, P., and Hindle, D.
(1991). Using Statistics in Lexical Analysis.
In Zernick, U.
(ed.), Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon, Lawrence Erlbaum Associates, pp.
115--164. 5 Ted Dunning, Accurate methods for the statistics of surprise and coincidence, Computational Linguistics, v.19 n.1, March 1993 6 William A.
Gale, Kenneth W.
Church, A program for aligning sentences in bilingual corpora, Computational Linguistics, v.19 n.1, March 1993 7 Gross, G.
(1996). Les expressions figes en franais.
OPHRYS, Paris.
8 Pierre
Isabelle, Marc Dymetman, George Foster, Jean-Marc Jutras, Elliott Macklovitch, Francois Perrault, Xiaobo Ren, Michel Simard, Translation analysis and translation automation, Proceedings of the 1993 conference of the Centre for Advanced Studies on Collaborative research: distributed computing, October 24-28, 1993, Toronto, Ontario, Canada 9 Laenzlinger, C.
and Wehrli, E.
(1991). Fips, un analyseur interactif pour le franais.
TA informations, 32(2): 35--49.
10 Christopher
D.
Manning, Hinrich Schtze, Foundations of statistical natural language processing, MIT Press, Cambridge, MA, 1999 11 Melby A.
(1982). A Bilingual Concordance System and its Use in Linguistic Studies.
In Proceedings of the Eighth LACUS Forum, Columbia, SC, pp.
541--549. 12 Romary L.
and Bonhomme P.
(2000). Parallel alignment of structured documents.
Vronis J.
(Ed.). Parallel Text Processing.
Dordrecht: Kluwer.
13 Michel
Simard, George F.
Foster, Pierre Isabelle, Using cognates to align sentences in bilingual corpora, Proceedings of the 1993 conference of the Centre for Advanced Studies on Collaborative research: distributed computing, October 24-28, 1993, Toronto, Ontario, Canada 14 Frank Smadja, Retrieving collocations from text: Xtract, Computational Linguistics, v.19 n.1, March 1993 15 Eric Wehrli, Parsing and Collocations, Proceedings of the Second International Conference on Natural Language Processing, p.272-282, June 02-04, 2000
Classifying Biological Full-Text Articles for Multi-Database Curation Wen-Juan Hou, Chih Lee and Hsin-Hsi Chen Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan {wjhou, clee}@nlg.csie.ntu.edu.tw; hhchen@csie.ntu.edu.tw Abstract In this paper, we propose an approach for identifying curatable articles from a large document set.
This system considers three parts of an article (title and abstract, MeSH terms, and captions) as its three individual representations and utilizes two domain-specific resources (UMLS and a tumor name list) to reveal the deep knowledge contained in the article.
An SVM classifier is trained and cross-validation is employed to find the best combination of representations.
The experimental results show overall high performance.
Track (http://ir.ohsu.edu/genomics) of TREC 2004 and 2005 organized categorization tasks.
The former focused on simplified GO terms while the latter included the triage for "tumor biology", "embryologic gene expression", "alleles of mutant phenotypes" and "GO" articles.
The increase of the numbers of participants at Genomics Track shows that biological classification problems attracted much attention.
This paper employs the domain-specific knowledge and knowledge learned from full-text articles to classify biological text.
Given a collection of articles, various methods are explored to extract features to represent a document.
We use the experimental data provided by the TREC 2005 Genomics Track to evaluate different methods.
The rest of this paper is organized as follows.
Section 2 sketches the overview of the system architecture.
Section 3 specifies the test bed used to evaluate the proposed methods.
The details of the proposed system are explained in Section 4.
The experimental results are shown and discussed in Section 5.
Finally, we make conclusions and present some further work.
1 Introduction
Organism databases play a crucial role in genomic and proteomic research.
It stores the up-to-date profile of each gene of the species interested.
For example, the Mouse Genome Database (MGD) provides essential integration of experimental knowledge for the mouse system with information annotated from both literature and online sources (Bult et al., 2004).
To provide biomedical scientists with easy access to complete and accurate information, curators have to constantly update databases with new information.
With the rapidly growing rate of publication, it is impossible for curators to read every published article.
Since fully automated curation systems have not met the strict requirement of high accuracy and recall, database curators still have to read some (if not all) of the articles sent to them.
Therefore, it will be very helpful if a classification system can correctly identify the curatable or relevant articles in a large number of biological articles.
Recently, several attempts have been made to classify documents from biomedical domain (Hirschman et al., 2002).
Couto et al.(2004) used the information extracted from related web resources to classify biomedical literature.
Hou et al.(2005) used the reference corpus to help classifying gene annotation.
The Genomics System Overview Figure 1 shows the overall architecture of the proposed system.
At first, we preprocess each training article, and divide it into three parts, including (1) title and abstract, (2) MeSH terms assigned to this article, and (3) captions of figures and tables.
They are denoted as "Abstract", "MeSH", and "Caption" in this paper, respectively.
Each part is considered as a representation of an article.
With the help of domain-specific knowledge, we obtain more detail representations of an article.
In the model selection phase, we perform feature ranking on each representation of an article and employ cross-validation to determine the number of features to be kept.
Moreover, we use cross-validation to obtain the best combination of all the representations.
Finally, a support vector machine (SVM) (Vapnik, 1995; Hsu et al., 2003) classifier is obtained.
Abstract AbsSEM/TM Full-Text Training Articles Preprocessing MeSH MeSHSEM Model Selection Caption CapSEM/TM Domain-Specific Knowledge A New Full-Text Article Preprocessing Multiple Parts PartsSEM/TM SVM Classifier Yes/No Figure 1.
System Architecture Experimental Data We train classifiers for classifying biomedical articles on the Categorization Task of the TREC 2005 Genomics Track.
The task uses data from the Mouse Genome Informatics (MGI) system (http://www.informatics.jax.org/) for four categorization tasks, including tumor biology, embryologic gene expression, alleles of mutant phenotypes and GO annotation.
Given a document and a category, we have to identify whether it is relevant to the given category.
The document set consists of some full-text data obtained from three journals, i.e., Journal of Biological Chemistry, Journal of Cell Biology and Proceedings of the National Academy of Science in 2002 and 2003.
There are 5,837 training documents and 6,043 testing documents.
Methods Document Preprocessing In the preprocessing phase, we perform acronym expansion on the articles, remove the remaining tags from the articles and extract three parts of interest from each article.
Abbreviations are often used to replace long terms in writing articles, but it is possible that several long terms share the same short form, especially for gene/protein names.
To avoid ambiguity and enhance clarity, the acronym expansion operation replaces every tagged abbreviation with its long form followed by itself in a pair of parentheses.
4.2 Employing
Domain-Specific Knowledge can identify the gene names contained in an article.
Moreover, by further consulting organism databases, we can get the properties of the genes.
Two domain-specific resources are exploited in this study.
One is the Unified Medical Language System (UMLS) (Humphreys et al., 1998) and the other is a list of tumor names obtained from Mouse Tumor Biology Database (MTB)1.
UMLS contains a huge dictionary of biomedical terms  the UMLS Metathesaurus and defines a hierarchy of semantic types  the UMLS Semantic Network.
Each concept in the Metathesaurus contains a set of strings, which are variants of each other and belong to one or more semantic types in the Semantic Network.
Therefore, given a string, we can obtain a set of semantic types to which it belongs.
Then we obtain another representation of the article by gathering the semantic types found in the part of the article.
Consequently, we get another three much deeper representations of an article after this step.
They are denoted as "AbstractSEM", "MeSHSEM" and "CaptionSEM".
We use the list of tumor names on the Tumor task.
We first tokenize all the tumor names and stem each unique token.
With the resulting list of unique stemmed tokens, we use it as a filter to remove the tokens not in the list from the "Abstract" and "Caption", which produce "AbstractTM" and "CaptionTM".
4.3 Model
Selection As mentioned above, we generate several representations for an article.
In this section, we explain how feature selection is done and how the best combination of the representations With the help of domain-specific knowledge, we can extract the deeper knowledge in an article.
For example, with a gene name dictionary, we fiof an article is obtained.
For each representation, we first rank all the tokens in the training documents via the chi-square test of independence.
Postulating the ranking perfectly reflects the effectiveness of the tokens in classification, we then decide the number of tokens to be used in SVM classification by 4-fold cross-validation.
In cross-validation, we use the TF*IDF weighting scheme.
Each feature vector is then normalized to a unit vector.
We set C+ to ur* Cbecause of the relatively small number of positive examples, where C+ and Care the penalty constants on positive and negative examples in SVMs.
After that, we obtain the optimal number of tokens and the corresponding SVM parameters Cand gamma, a parameter in the radial basis kernel.
In the rest of this paper, "Abstract30" denotes the "Abstract" representation with top-30 tokens, "CaptionSEM10" denotes "CaptionSEM" with top-10 tokens, and so forth.
After feature selection is done for each representation, we try to find the best combination by the following algorithm.
Given the candidate representations with selected features, we start with an initial set containing some or zero representation.
For each iteration, we add one representation to the set by picking the one that enhances the cross-validation performance the most.
The iteration stops when we have exhausted all the representations or adding more representation to the set doesn't improve the cross-validation performance.
For classifying the documents with better features, we run the algorithm twice.
We first start with an empty set and obtain the best combination of the basic three representations, e.g., "Abstract10", "MeSH30" and "Caption10".
Then, starting with this combination, we attempt to incorporate the three semantic representations, e.g., "Abstract30SEM", "MeSH30SEM" and "Caption10SEM", and obtain the final combination.
Instead of using this algorithm to incorporate the "AbstractTM" and "CaptionTM" representations, we use them to replace their unfiltered counterparts "Abstract" and "Caption" when the cross-validation performance is better.
Results and Discussions Utility (NU)2 measure).
For category Allele, "Caption" and "AbstractSEM" perform the best among the basic and semantic representations, respectively.
For category Expression, "Caption" plays an important role in identifying relevant documents, which agrees with the finding by the winner of KDD CUP 2002 task 1 (Regev et al., 2002).
Similarly, MeSH terms are crucial to the GO category, which are used by top-performing teams (Dayanik et al., 2004; Fujita, 2004) in TREC Genomics 2004.
For category Tumor, MeSH terms are important, but after semantic type extraction, "AbstractSEM" exhibits relatively high cross-validation performance.
Since only 10 features are selected for the "AbstractSEM", using this representation alone may be susceptible to over-fitting.
Finally, by comparing the performance of the "AbstractTM" and "Abstract", we find the list of tumor names helpful for filtering abstracts.
We list the results for the test data in Table 2.
Column "Experiment" identifies our proposed methods.
We show six experiments in Table 2: one for Allele (AL), one for Expression (EX), one for GO (GO) and three for Tumor (TU, TN and TS).
Column "cv NU" shows the cross-validation NU measure, "NU" shows the performance on the test data and column "Combination" lists the combination of the representations used for each experiment.
In this table, "M30" is the abbreviation for "MeSH30", "CS10" is for "CaptionSEM10", and so on.
The combinations for the first 4 experiments, i.e., AL, EX, GO and TU, are obtained by the algorithm described in Section 4.3, while the combination for TN is obtained by substituting "AbstractTM30" for "Abstract30" in the combination for TU.
The experiment TS only uses the "AbstractSEM10" because its cross-validation performance beats all other combinations for the Tumor category.
The combinations of the first 5 experiments illustrate that adding other inferior representations to the best one enhances the performance, which implies that the inferior ones may contain important exclusive information.
The cross-validation performance fairly predicts the performance on the test data, except for the last experiment TS, which relies on only 10 features and is therefore susceptible to over-fitting.
Table 1 lists the cross-validation results of each representation for each category (in Normalized Please refer to the TREC 2005 Genomics Track Protocol (http://ir.ohsu.edu/genomics/2005protocol.html).
Abstract MeSH Caption AbstractSEM MeSHSEM CaptionSEM AbstractTM CaptionTM Table 1.
Partial Cross-validation Results.
Experiment AL (for Allele) EX (for Expression) GO (for GO) TU (for Tumor) TN (for Tumor) TS (for Tumor) cv NU 0.8717 0.7691 0.5402 0.8742 0.8764 0.8814 NU 0.8423 0.7515 0.5332 0.8299 0.8747 0.5699 Recall 0.9488 0.8190 0.8803 0.9000 0.9500 0.6500 Precision 0.3439 0.1593 0.1873 0.0526 0.0518 0.0339 F-score 0.5048 0.2667 0.3089 0.0994 0.0982 0.0645 Combination M30+C10+A10+CS10+AS10+MS10 M10+C10+CS10+MS10 M10+C10+MS10 M30+C30+A30+AS10+CS30 M30+C30+AT30+AS10+CS30 AS10 Table 2.
Evaluation Results.
Subtask Allele Expression GO Annotation Tumor NU (Best/Median) 0.8710/0.7773 0.8711/0.6413 0.5870/0.4575 0.9433/0.7610 Recall (Best/Median) 0.9337/0.8720 0.9333/0.7286 0.8861/0.5656 1.0000/0.9500 Precision (Best/Median) 0.4669/0.3153 0.1899/0.1164 0.2122/0.3223 0.0709/0.0213 F-score (Best/Median) 0.6225/0.5010 0.3156/0.2005 0.3424/0.4107 0.1325/0.0417 Table 3.
Best and Median Results for Each Subtask on TREC 2005 (Hersh et al., 2005).
To compare with our performance, we list the best and median results for each subtask on the genomics classification task of TREC 2005 in Table 3.
Comparing to Tables 2 and 3, it shows our experimental results have overall high performance.
Conclusions and Further Work In this paper, we demonstrate how our system is constructed.
Three parts of an article are extracted to represent its content.
We incorporate two domain-specific resources, i.e., UMLS and a list of tumor names.
For each categorization work, we propose an algorithm to get the best combination of the representations and train an SVM classifier out of this combination.
Evaluation results show overall high performance in this study.
Except for MeSH terms, we can try other sections in the article, e.g., Results, Discussions and Conclusions as targets of feature extraction besides the abstract and captions in the future.
Finally, we will try to make use of other available domain-specific resources in hope of enhancing the performance of this system.
Acknowledgements Research of this paper was partially supported by National Science Council, Taiwan, under the contracts NSC94-2213-E-002-033 and NSC94-2752-E-001-001-PAE.
References Bult, C.
J., Blake, J.
A., Richardson, J.
E., Kadin, J.
A., Eppig, J.
T. and the Mouse Genome Database Group.
The Mouse Genome Database (MGD): Integrating Biology with the Genome.
Nucleic Acids Research, 32, D476D481, 2004.
Couto, F.
M., Martins, B.
and Silva, M.
J . Classifying Biological Articles Using Web Resources.
Proceedings of the 2004 ACM Symposium on Applied Computing, 111-115, 2004.
Dayanik, A., Fradkin, D., Genkin, A., Kantor, P., Lewis, D.
D., Madigan, D.
and Menkov, V . DIMACS at the TREC 2004 Genomics Track.
Proceedings of the Thirteenth Text Retrieval Conference, 2004.
Fujita, S., . Revisiting Again Document Length Hypotheses TREC-2004 Genomics Track Experiments at Patolis.
Proceedings of the Thirteenth Text Retrieval Conference, 2004.
Hersh, W., Cohen, A., Yang, J., Bhuptiraju, R.T., Toberts, P.
and Hearst, M . TREC 2005 Genomics Track Overview.
Proceedings of the Fourteenth Text Retrieval Conference, 2005.
Hirschman, L., Park, J., Tsujii, J., Wong, L.
and Wu, C.
H . Accomplishments and Challenges in Literature Data Mining for Biology.
Bioinformatics, 18(12): 1553-1561, 2002.
Hou, W.
J., Lee, C., Lin, K.
H. Y.
and Chen, H.
H . A Relevance Detection Approach to Gene Annotation.
Proceedings of the First International Symposium on Semantic Mining in Biomedicine, http://ceur-ws.org, 148: 15-23, 2005.
Hsu, C.
W., Chang, C.
C. and Lin, C.
J . A Practical Guide to Support Vector Classification.
http://www.csie.ntu.edu.tw /~cjlin/libsvm/index.html, 2003.
Humphreys, B.
L., Lindberg, D.
A., Schoolman, H.
M. and Barnett, G.
O . The Unified Medical Language System: an Informatics Research Collaboration.
Journal of American Medical Information Association, 5(1):1-11, 1998.
Regev, Y., Finkelstein-Landau, M.
and Feldman, R . Rule-based Extraction of Experimental Evidence in the Biomedical Domain the KDD Cup (Task 1).
SIGKDD Explorations, 4(2):90-92, 2002.
Vapnik, V . The Nature of Statistical Learning Theory, Springer-Verlag, 1995 .
Multidocument Summarization via Information Extraction Michael White and Tanya Korelsky CoGenTex, Inc.
Ithaca, NY Claire Cardie, Vincent Ng, David Pierce, and Kiri Wagstaff Department of Computer Science Cornell University, Ithaca, NY mike,tanya@cogentex.com We present and evaluate the initial version of RIPTIDES, a system that combines information extraction, extraction-based summarization, and natural language generation to support userdirected multidocument summarization.
IE system and the Summarizer in turn.
2.1 IE
System The domain for the initial IE-supported summarization system and its evaluation is natural disasters.
Very briefly, a top-level natural disasters scenario template contains: document-level information (e.g.
docno, date-time); zero or more agent elements denoting each person, group, and organization in the text; and zero or more disaster elements.
Agent elements encode standard information for named entities (e.g.
name, position, geo-political unit).
For the most part, disaster elements also contain standard event-related fields (e.g.
type, number, date, time, location, damage sub-elements).
The final product of the RIPTIDES system, however, is not a set of scenario templates, but a user-directed multidocument summary.
This difference in goals influences a number of template design issues.
First, disaster elements must distinguish different reports or views of the same event from multiple sources.
As a result, the system creates a separate disaster event for each such account.
Disaster elements should also include the reporting agent, date, time, and location whenever possible.
In addition, damage elements (i.e.
human and physical effects) are best grouped according to the reporting event.
Finally, a slight broadening of the IE task was necessary in that extracted text was not constrained to noun phrases.
In particular, adjectival and adverbial phrases that encode reporter confidence, and sentences and clauses denoting relief effort progress appear beneficial for creating informed summaries.
Figure 2 shows the scenario template for one of 25 texts tracking the 1998 earthquake in Afghanistan (TDT2 Topic 89).
The texts were also manually annotated for noun phrase coreference; any phrase involved in a coreference relation appears underlined in the running text.
The RIPTIDES system for the most part employs a traditional IE architecture [4].
In addition, we use an in-house implementation of the TIPSTER architecture [8] to manage all linguistic annotations.
A preprocessor first finds sentences and tokens.
For syntactic analysis, we currently use the Charniak [5] parser, which creates Penn Treebank-style parses [9] rather than the partial parses used in most IE systems.
Output from the parser is converted automatically into TIPSTER parse and part-of-speech annotations, which are added to the set of linguistic annotations for the document.
The extraction phase of the system identifies domain-specific relations among relevant entities in the text.
It relies on Autoslog-XML, an XSLT implementation of the Autoslog-TS system [12], to acquire extraction patterns.
Autoslog-XML is a weakly supervised learning system that requires two sets of texts for training -one set comprises texts relevant to the domain of interest and the other, texts not relevant Although recent years has seen increased and successful research efforts in the areas of single-document summarization, multidocument summarization, and information extraction, very few investigations have explored the potential of merging summarization and information extraction techniques.
This paper presents and evaluates the initial version of RIPTIDES, a system that combines information extraction (IE), extraction-based summarization, and natural language generation to support userdirected multidocument summarization.
(RIPTIDES stands for RapIdly Portable Translingual Information extraction and interactive multiDocumEnt Summarization).
Following [10], we hypothesize that IE-supported summarization will enable the generation of more accurate and targeted summaries in specific domains than is possible with current domain-independent techniques.
In the sections below, we describe the initial implementation and evaluation of the RIPTIDES IE-supported summarization system.
We conclude with a brief discussion of related and ongoing work.
Figure 1 depicts the IE-supported summarization system.
The system first requires that the user select (1) a set of documents in which to search for information, and (2) one or more scenario templates (extraction domains) to activate.
The user optionally provides filters and preferences on the scenario template slots, specifying what information s/he wants to be reported in the summary.
RIPTIDES next applies its Information Extraction subsystem to generate a database of extracted events for the selected domain and then invokes the Summarizer to generate a natural language summary of the extracted information subject to the user's constraints.
In the subsections below, we describe the fiuser information need Summarizer multi-document template merging event-oriented structure text collection IE System slot filler slot slot filler filler slot filler slotslot filler filler slot filler slot filler...
... slot filler slot filler filler slot slot filler slot filler slot filler ...
... slot filler slot filler scenario templates content selection event-oriented structure with slot importance scores A powerful earthquake struck Afghanistan on May 30 at 11:25...
Damage VOA (06/02/1998) estimated that 5,000 were killed by the earthquake, whereas AP (APW, 06/02/1998) instead reported ...
Relief Status NLG of summary CNN (06/02/1998): Food, water, medicine and other supplies have started to arrive.
[...] summary Figure 1.
RIPTIDES System Design to the domain.
Based on these and a small set of extraction pattern templates, the system finds a ranked list of possible extraction patterns, which a user then annotates with the appropriate extraction label (e.g.
victim). Once acquired, the patterns are applied to new documents to extract slot fillers for the domain.
Selectional restrictions on allowable slot fillers are implemented using WordNet [6] and BBN's Identifinder [3] named entity component.
In the current version of the system, no coreference resolution is attempted; instead, we rely on a very simple set of heuristics to guide the creation of output templates.
The disaster scenario templates extracted for each text are provided as input to the summarization component along with all linguistic annotations accrued in the IE phase.
No relief slots are included in the output at present, since there was insufficient annotated data to train a reliable sentence categorizer.
Selected News Excerpts, as shown in the two sample summaries appearing in Figures 3 and 4, and discussed further in Section 2.2.5 below.
2.2.1 Summarization
Stages The Summarizer produces each summary in three main stages.
In the first stage, the output templates are merged into an eventoriented structure, while keeping track of source information.
The merge operation currently relies on simple heuristics to group extracted facts that are comparable; for example, during this phase damage reports are grouped according to whether they pertain to the event as a whole, or instead to damage in the same particular location.
Heuristics are also used in this stage to determine the most relevant damage reports, taking into account specificity, recency and news source.
Towards the same objective but using a more surface-oriented means, simple word-overlap clustering is used to group sentences from different documents into clusters that are likely to report similar content.
In the second stage, a base importance score is first assigned to each slot/sentence based on a combination of document position, document recency and group/cluster membership.
The base importance scores are then adjusted according to user-specified preferences and matching 2.2 The Summarizer In order to include relief and other potentially relevant information not currently found in the scenario templates, the Summarizer extracts selected sentences from the input articles and adds them to the summaries generated from the scenario templates.
The extracted sentences are listed under the heading Document no.: ABC19980530.1830.0342 Date/time: 05/30/1998 18:35:42.49 Disaster Type: earthquake location: Afghanistan date: today magnitude: 6.9 magnitude-confidence: high epicenter: a remote part of the country PAKISTAN MAY BE PREPARING FOR ANOTHER TEST Thousands of people are feared dead following...
(voiceover) ...a powerful earthquake that hit Afghanistan today.
The quake registered 6.9 on the Richter scale, centered in a remote part of the country.
(on camera) Details now hard to come by, but reports say entire villages were buried by the quake.
human-effect: victim: Thousands of people number: Thousands outcome: dead confidence: medium confidence-marker: feared physical-effect: object: entire villages outcome: damaged confidence: medium confidence-marker: Details now hard to come by / reports say Figure 2.
Example scenario template for the natural disasters domain criteria.
The adjusted scores are used to select the most important slots/sentences to include in the summary, subject to the userspecified word limit.
In the third and final stage, the summary is generated from the resulting content pool using a combination of top-down, schema-like text building rules and surface-oriented revisions.
The extracted sentences are simply listed in document order, grouped into blocks of adjacent sentences.
(2) any intermediate estimates that are lower than the maximum estimate.1 In the content determination stage, scores are assigned to the derived information units based on the maximum score of the underlying units.
In the summary generation stage, a handful of text planning rules are used to organize the text for these derived units, highlighting agreement and disagreement across sources.
2.2.2 Specificity
of Numeric Estimates In order to intelligently merge and summarize scenario templates, we found it necessary to explicitly handle numeric estimates of varying specificity.
While we did find specific numbers (such as 3,000) in some damage estimates, we also found cases with no number phrase at all (e.g.
entire villages).
In between these extremes, we found vague estimates (thousands) and ranges of numbers (anywhere from 2,000 to 5,000).
We also found phrases that cannot be easily compared (more than half the region's residents).
To merge related damage information, we first calculate the numeric specificity of the estimate as one of the values NONE, VAGUE, RANGE, SPECIFIC, or INCOMPARABLE, based on the presence of a small set of trigger words and phrases (e.g.
several, as many as, from ...
to). Next, we identify the most specific current estimates by news source, where a later estimate is considered to update an earlier estimate if it is at least as specific.
Finally, we determine two types of derived information units, namely (1) the minimum and maximum estimates across the news sources, and 2.2.3 Improving the Coherence of Extracted Sentences In our initial attempt to include extracted sentences, we simply chose the top ranking sentences that would fit within the word limit, subject to the constraint that no more than one sentence per cluster could be chosen, in order to help avoid redundancy.
We found that this approach often yielded summaries with very poor coherence, as many of the included sentences were difficult to make sense of in isolation.
To improve the coherence of the extracted sentences, we have experimented with trying to boost coherence by favoring sentences in the context of the highest-ranking sentences over those with lower ranking scores, following the hypothesis that it is better to cover fewer topics in more depth than to change topics excessively.
In particular, we assign a score to a set of sentences by summing the base scores plus increasing coherence boosts for adjacent sentences, sentences that precede ones with an initial Less specific estimates such as "hundreds" are considered lower than more specific numbers such as "5000" when they are lower by more than a factor of 10.
Earthquake strikes Afghanistan A powerful earthquake struck Afghanistan last Saturday at 11:25.
The earthquake was centered in a remote part of the country and had a magnitude of 6.9 on the Richter scale.
Earthquake strikes quake-devastated villages in northern Afghanistan A earthquake struck quake-devastated villages in northern Afghanistan Saturday.
The earthquake had a magnitude of 6.9 on the Richter scale on the Richter scale.
Damage Estimates of the death toll varied.
VOA (06/02/1998) provided the highest estimate of 5,000 dead.
CNN (05/31/1998) and CNN (06/02/1998) supplied lower estimates of 3,000 and up to 4,000 dead, whereas APW (06/02/1998) gave the lowest estimate of anywhere from 2,000 to 5,000 dead.
People were injured, while thousands more were missing.
Thousands were homeless.
Quake-devastated villages were damaged.
Estimates of the number of villages destroyed varied.
CNN (05/31/1998) provided the highest estimate of 50 destroyed, whereas VOA (06/04/1998) gave the lowest estimate of at least 25 destroyed.
In Afghanistan, thousands of people were killed.
Damage Estimates of the death toll varied.
CNN (06/02/1998) provided the highest estimate of 4,000 dead, whereas ABC (06/01/1998) gave the lowest estimate of 140 dead.
In capital: Estimates of the number injured varied.
Selected News Excerpts CNN (06/01/98): Thousands are dead and thousands more are still missing.
Red cross officials say the first priority is the injured.
Getting medicine to them is difficult due to the remoteness of the villages affected by the quake.
PRI (06/01/98): We spoke to the head of the international red cross there, Bob McCaro on a satellite phone link.
He says it's difficult to know the full extent of the damage because the region is so remote.
There's very little infrastructure.
PRI (06/01/98): Bob McCaro is the head of the international red cross in the neighboring country of Pakistan.
He's been speaking to us from there on the line.
APW (06/02/98): The United Nations, the Red Cross and other agencies have three borrowed helicopters to deliver medical aid.
Figure 4.
200 word summary of actual IE output, with emphasis on Red Cross Further Details Heavy after shocks shook northern afghanistan.
More homes were destroyed.
More villages were damaged.
Landslides or mud slides hit the area.
Another massive quake struck the same region three months earlier.
Some 2,300 victims were injured.
Selected News Excerpts ABC (05/30/98): PAKISTAN MAY BE PREPARING FOR ANOTHER TEST Thousands of people are feared dead following...
ABC (06/01/98): RESCUE WORKERS CHALLENGED IN AFGHANISTAN There has been serious death and devastation overseas.
In Afghanistan...
CNN (06/02/98): Food, water, medicine and other supplies have started to arrive.
But a U.N. relief coordinator says it's a "scenario from hell".
Figure 3.
200 word summary of simulated IE output, with emphasis on damage cases.
We then perform a randomized local search for a good set of sentences according to these scoring criteria.
2.2.4 Implementation
The Summarizer is implemented using the Apache implementation of XSLT [1] and CoGenTex's Exemplars Framework [13].
The Apache XSLT implementation has provided a convenient way to rapidly develop a prototype implementation of the first two processing stages using a series of XML transformations.
In the first step of the third summary generation stage, the text building component of the Exemplars Framework constructs a "rough draft" of the summary text.
In this rough draft version, XML markup is used to partially encode the rhetorical, referential, semantic and morpho-syntactic structure of the text.
In the second generation step, the Exemplars text polishing component makes use of this markup to trigger surfacepronoun, and sentences that preceded ones with strongly connecting discourse markers such as however, nevertheless, etc.
We have also softened the constraint on multiple sampling from the same cluster, making use of a redundancy penalty in such fioriented revision rules that smooth the text into a more polished form.
A distinguishing feature of our text polishing approach is the use of a bootstrapping tool to partially automate the acquisition of application-specific revision rules from examples.
2.2.5 Sample
Summaries Figures 3 and 4 show two sample summaries that were included in our evaluation (see Section 3 for details).
The summary in Figure 3 was generated from simulated output of the IE system, with preference given to damage information; the summary in Figure 4 was generated from the actual output of the current IE system, with preference given to information including the words Red Cross.
While the summary in Figure 3 does a reasonable job of reporting the various current estimates of the death toll, the estimates of the death toll shown in Figure 4 are less accurate, because the IE system failed to extract some reports, and the Summarizer failed to correctly merge others.
In particular, note that the lowest estimate of 140 dead attributed to ABC is actually a report about the number of school children killed in a particular town.
Since no location was given for this estimate by the IE system, the Summarizer's simple heuristic for localized damaged reports -namely, to consider a damage report to be localized if a location is given that is not in the same sentence as the initial disaster description -did not work here.
The summary in Figure 3 also suffered from some problems with merging: the inclusion of a paragraph about thousands killed in Afghanistan is due to an incorrect classification of this report as a localized one (owing to an error in sentence boundary detection), and the discussion of the number of villages damaged should have included a report of at least 80 towns or villages damaged.
Besides the problems related to slot extraction and merging mentioned above, the summaries shown in Figures 3 and 4 suffer from relatively poor fluency.
In particular, the summaries could benefit from better use of descriptive terms from the original articles, as well as better methods of sentence combination and rhetorical structuring.
Nevertheless, as will be discussed further in Section 4, we suggest that the summaries show the potential for our techniques to intelligently combine information from many articles on the same natural disaster.
earlier version of the Summarizer uses the simulated output of the IE system as its input, including the relief annotations; in the second variant (RIPTIDES-SIM2), the current version of the Summarizer uses the simulated output of the IE system, without the relief annotations; and in the third variant (RIPTIDES-IE), the Summarizer uses the actual output of the IE system as its input.2 Summaries generated by the RIPTIDES variants were compared to a Baseline system consisting of a simple, sentence-extraction multidocument summarizer relying only on document position, recency, and word overlap clustering.
(As explained in the previous section, we have found that word overlap clustering provides a bare bones way to help determine what information is repeated in multiple articles, thereby indicating importance to the document set as a whole, as well as to help reduce redundancy in the resulting summaries).
In addition, the RIPTIDES and Baseline system summaries were compared against the summaries of two human authors.
All of the summaries were graded with respect to content, organization, and readability on an A-F scale by three graduate students, all of whom were unfamiliar with this project.
Note that the grades for RIPTIDES-SIM1, the Baseline system, and the two human authors were assigned during a first evaluation in October, 2000, whereas the grades for RIPTIDESSIM2 and RIPTIDES-IE were assigned by the same graders in an update to this evaluation in April, 2001.
Each system and author was asked to generate four summaries of different lengths and emphases: (1) a 100-word summary of the May 30 and May 31 articles; (2) a 400-word summary of all test articles, emphasizing specific, factual information; (3) a 200-word summary of all test articles, focusing on the damage caused by the quake, and excluding information about relief efforts, and (4) a 200-word summary of all test articles, focusing on the relief efforts, and highlighting the Red Cross's role in these efforts.
The results are shown in Tables 1 and 2.
Table 1 provides the overall grade for each system or author averaged across all graders and summaries, where each assigned grade has first been converted to a number (with A=4.0 and F=0.0) and the average converted back to a letter grade.
Table 2 shows the mean and standard deviations of the overall, content, organization, and readability scores for the RIPTIDES and the Baseline systems averaged across all graders and summaries.
Where the differences vs.
the Baseline system are significant according to the t-test, the p-values are shown.
Given the amount of development effort that has gone into the system to date, we were not surprised that the RIPTIDES variants fared poorly when compared against the manually written summaries, with RIPTIDES-SIM2 receiving an average grade of C, vs.
Aand B+ for the human authors.
Nevertheless, we were pleased to find that RIPTIDES-SIM2 scored a full grade ahead of the Baseline summarizer, which received a D, and that 3.
EVALUATION AND INITIAL RESULTS To evaluate the initial version of the IE-supported summarization system, we used Topic 89 from the TDT2 collection -25 texts on the 1998 Afghanistan earthquake.
Each document was annotated manually with the natural disaster scenario templates that comprise the desired output of the IE system.
In addition, treebank-style syntactic structure annotations were added automatically using the Charniak parser.
Finally, MUC-style noun phrase coreference annotations were supplied manually.
All annotations are in XML.
The manual and automatic annotations were automatically merged, leading to inaccurate annotation extents in some cases.
Next, the Topic 89 texts were split into a development corpus and a test corpus.
The development corpus was used to build the summarization system; the evaluation summaries were generated from the test corpus.
We report on three different variants of the RIPTIDES system here: in the first variant (RIPTIDES-SIM1), an Note that since the summarizers for the second and third variants did not have access to the relief sentence categorizations, we decided to exclude from their input the two articles (one training, one test) classified by TDT2 Topic 89 as only containing brief mentions of the event of interest, as otherwise they would have no means of excluding the largely irrelevant material in these documents.
Table 1 Baseline D RIPTIDES-SIM1 C/CRIPTIDES-SIM2 C RIPTIDES-IE D+ Person 1 APerson 2 B+ RIPTIDES-IE managed a slightly higher grade of D+, despite the immature state of the IE system.
As Table 2 shows, the differences in the overall scores were significant for all three RIPTIDES variants, as were the scores for organization and readability, though not for content in the cases of RIPTIDESSIM1 and RIPTIDES-IE.
our efforts in this area on attempting to balance coherence and informativeness in selecting sets of sentences to include in the summary.
In ongoing work, we are investigating techniques for improving merging accuracy and summary fluency in the context of summarizing the more than 150 news articles we have collected from the web about each of the recent earthquakes in Central America and India (January, 2001).
We also plan to investigate using tables and hypertext drill-down as a means to help the user verify the accuracy of the summarized information.
By perusing the web collections mentioned above, we can see that trying to manually extricate the latest damage estimates from 150+ news articles from multiple sources on the same natural disaster would be very tedious.
Although estimates do usually converge, they often change rapidly at first, and then are gradually dropped from later articles, and thus simply looking at the latest article is not satisfactory.
While significant challenges remain, we suggest that our initial system development and evaluation shows that our approach has the potential to accurately summarize damage estimates, as well as identify other key story items using shallower techniques, and thereby help alleviate information overload in specific domains.
4. RELATED AND ONGOING WORK The RIPTIDES system is most similar to the SUMMONS system of Radev and McKeown [10], which summarized the results of MUC-4 IE systems in the terrorism domain.
As a pioneering effort, the SUMMONS system was the first to suggest the potential of combining IE with NLG in a summarization system, though no evaluation was performed.
In comparison to SUMMONS, RIPTIDES appears to be designed to more completely summarize larger input document sets, since it focuses more on finding the most relevant current information, and since it includes extracted sentences to round out the summaries.
Another important difference is that SUMMONS sidestepped the problem of comparing reported numbers of varying specificity (e.g.
several thousand vs.
anywhere from 2000 to 5000 vs.
up to 4000 vs.
5000), whereas we have implemented rules for doing so.
Finally, we have begun to address some of the difficult issues that arise in merging information from multiple documents into a coherent event-oriented view, though considerable challenges remain to be addressed in this area.
The sentence extraction part of the RIPTIDES system is similar to the domain-independent multidocument summarizers of Goldstein et al.[7] and Radev et al.[11] in the way it clusters sentences across documents to help determine which sentences are central to the collection, as well as to reduce redundancy amongst sentences included in the summary.
It is simpler than these systems insofar as it does not make use of comparisons to the centroid of the document set.
As pointed out in [2], it is difficult in general for multidocument summarizers to produce coherent summaries, since it is less straightforward to rely on the order of sentences in the underlying documents than in the case of single-document summarization.
Having also noted this problem, we have focused We thank Daryl McCullough for implementing the coherence boosting randomized local search, and we thank Ted Caldwell, Daryl McCullough, Corien Bakermans, Elizabeth Conrey, Purnima Menon and Betsy Vick for their participation as authors and graders.
This work has been partially supported by DARPA TIDES contract no.
N66001-00-C-8009. References [1] The Apache XML Project.
2001. "Xalan Java." [2] Barzilay, R., Elhadad, N.
and McKeown, K . 2001.
"Sentence Ordering in Multidocument Summarization".
In Proceedings of HLT 2001.
[3] Bikel, D., Schwartz, R.
and Weischedel, R . 1999.
"An Algorithm that Learns What's in a Name".
Machine Learning 34:1-3, 211-231.
[4] Cardie, C . 1997.
"Empirical Methods in Information [5] Charniak, E . 1999.
"A maximum-entropy-inspired parser".
Brown University Technical Report CS99-12.
[6] Fellbaum, C . 1998.
WordNet: An Electronic Lexical Database.
MIT Press, Cambridge, MA.
[7] Goldstein, J., Mittal, V., Carbonell, J.
and Kantrowitz, M . 2000.
"Multi-document summarization by sentence extraction".
In Proceedings of the ANLP/NAACL Workshop on Automatic Summarization, Seattle, WA.
[8] Grishman, R . 1996.
"TIPSTER Architecture Design Document Version 2.2".
DARPA, available at http://www.tipster.org/.
[9] Marcus, M., Marcinkiewicz, M.
and Santorini, B . 1993.
"Building a Large, Annotated Corpus of English: The Penn Treebank".
Computational Linguistics 19:2, 313-330.
[10] Radev, D.
R. and McKeown, K.
R . 1998.
"Generating natural language summaries from multiple on-line sources".
Computational Linguistics 24(3):469-500.
[11] Radev, D.
R., Jing, H.
and Budzikowska, M . 2000.
"Summarization of multiple documents: clustering, sentence extraction, and evaluation".
In Proceedings of the ANLP/NAACL Workshop on Summarization, Seattle, WA.
[12] Riloff, E . 1996.
"Automatically Generating Extraction Patterns from Untagged Text".
In Proceedings of the Thirteenth National Conference on Artificial Intelligence, Portland, OR, 1044-1049.
AAAI Press / MIT Press.
[13] White, M.
and Caldwell, T . 1998.
"EXEMPLARS: A Practical, Extensible Framework for Dynamic Text Generation".
In Proceedings of the Ninth International Workshop on Natural Language Generation, Niagara-onthe-Lake, Canada, 266-275 .
Structural variation in generated health reports Catalina Hallett and Donia Scott Centre for Research in Computing The Open University Walton Hall Milton Keynes MK7 6AA {c.hallett,d.scott}@open.ac.uk Abstract We present a natural language generator that produces a range of medical reports on the clinical histories of cancer patients, and discuss the problem of conceptual restatement in generating various textual views of the same conceptual content.
We focus on two features of our system: the demand for "loose paraphrases" between the various reports on a given patient, with a high degree of semantic overlap but some necessary amount of distinctive content; and the requirement for paraphrasing at primarily the discourse level.
which aims at providing tools to facilitate easy access to a patient's medical history.
In particular, we describe a natural language generation system that produces a range of summarised reports of patient records from data-encoded views of patient histories which we call chronicles.
Although we are concentrating on cancer patients, we aim to produce good quality reports without the need to construct extensive domain models.
Our typical user is a GP or clinician who uses electronic patient records at the point of care to familiarise themselves with a patient's medical history and current situation.
A number of specific requirements arise from this particular setting:  Reports that provide a quick potted overview of the patient's history are essential; this type of report should not be too long (ideally they should fit entirely on a computer screen) and should take less than a minute to read;  At the same time, a complete view of the medical history must always be available on demand;  Clinicians often need to examine a patient's history from a particular perspective (e.g., tests administered, treatments undertaken, drugs prescribed), and having focussed reports is also a requirement;  Reports should be formatted to enhance readability;  The selection of events for inclusion in a report should follow some basic rules: 1 Introduction Patient records are typically large collections of documents that reflect the medical history of a patient over a period of time.
On average, the electronic patient record of a cancer patient contains information from over 150 documents, representing consult notes, referral letters, letters to and from the patient's GP, hospital admission and discharge notes, laboratory test results, surgery and other treatment descriptions, and drug dispensing notes.
Although each document in this collection will have a specified purpose, there tends to be a high degree of redundancy between documents, but the sheer volume of information makes access extremely difficult.
The work presented in this paper is part of the Clinical E-Science Framework project (CLEF), fi Events that deviate from what is considered to be normal are more important than normal events (for example, an examination of the lymphnodes that reveals lymphadenopathy is more important than an examination that doesn't).
 Some events are more important than others and should not only be included in the report but also highlighted (e.g., through colour coding, graphical timelines or similar display features).
 Less important events should be available on a need-to-know basis These requirements impose important restrictions on the content of the reports and implicitly on the variety of lexical and syntactical devices we can employ: (a) the veracity of the report is essential, therefore we are not at liberty to employ synonymy or lexical paraphrasing that may alter (however slightly) the meaning of the original input, (b) we are required to maintain a certain syntactical ordering throughout a report in order to allow the user to quickly scan through the report with ease, and (c) we have to produce several types of reports from the same input data.
In this paper, we focus on this last requirement, describing the methods we employ for reformulating content according to the type and focus of the generated report.
example displays a fragment of a generated longitudinal report1 : Example 1 The patient is diagnosed with grade 9 invasive medullary carcinoma of the breast.
She was 39 years old when the first cell became malignant.
The history covers 1517 weeks, from week 180 to week 1697.
During this time, the patient attended 38 consults.
YEAR 3: Week 183  Radical mastectomy on the breast was performed to treat primary cancer of the left breast.
 Histopathology revealed primary cancer of the left breast.
Week 191  Examination of the abdomen revealed no enlargement of the liver or of the spleen.
 Examination of the axillary lymphnodes revealed no lymphadenopathy of the left axillary lymphnodes.
 Examination of the breast revealed no recurrent cancer of the left breast.
 Testing of the blood revealed no abnormality of the haemoglobin concentration or of the leucocyte count.
 Radiotherapy was initiated to treat primary cancer of the left breast.
Week 192  First radiotherapy cycle was performed.
...
2 Types
of report In the current implementation, the generator produces two main types of report.
The first is a longitudinal report, which is intended to provide a quick historical overview of the patient's illness, whilst preserving the main events (such as diagnoses, investigations and interventions).
It presents the events in the patient's history ordered chronologically and grouped according to type.
In this type of report, events are fully described (i.e., an event description includes all the attributes of the event) and aggregation is minimal (events with common attributes are aggregated, but there is no aggregation through generalization, for example).
The following The second type of report focusses on a given type of event in a patient's history, such as the history of diagnoses, interventions, investigations or drug prescription.
Under this category fall user-defined reports as well, where the user selects classes of interesting events (for example, Investigations of type CT scan and Interventions of type surgery).
A report of the diagnoses, for example, will focus on the Problem events that are recorded in the chronicle (e.g., cancer, anaemia, lymphadenopathy); other event types will only All the examples presented in this paper are extracted from summaries produced by our Report generator.
fiappear if they are directly related to a Problem.
As it can be seen in Example 2, this type of report is necessarily more condensed, since the events do not have to appear chronologically and can be grouped in larger clusters.
Secondary events are also more highly aggregated.
Example 2  In week 483, primary cancer of the right breast was revealed by the histopathology report.
The cancer was treated with radical mastectomy on the breast.
 In week 491, no abnormality of the leucocyte count or of the haemoglobin concentration, no lymphadenopathy of the right axillary lymphnodes, no enlargement of the spleen or of the liver and no recurrent cancer of the right breast were found.
Radiotherapy was initiated to treat primary cancer of the right breast.
 In the weeks 492 to 496, 5 radiotherapy cycles were performed.
of the right breast.
It is important to note that although the reports are generated from the same input content, they are not exact reformulations of each other, but rather different views of the same content with a large degree of overlap.
This feature is a direct result of the report requirements.
3 Input
As mentioned earlier, the input to our Report Generator is a data-encoded chronicle of the patient's medical history.
Technically, the chronicle is the partial result of information extraction applied on clinical narratives, combined with structured data (such as radiology results or demographic data), and supplemented with inferences.
However, in developing our report generator, we are currently using a Chronicle Simulator, which constructs invented chronicles, allowing us to ignore for the time being some problems that can appear when using an information extraction system (being developed in parallel).
Firstly, the resulting data is complete and correct, thus allowing us to concentrate on the design and testing of the generation and summarisation system without having to take into account at this point errors in the Information Extraction.
Secondly, our data on cancer patients is highly confidential, which makes presentation of the output of the report generator (e.g., for evaluation with real subjects, or dissemination purposes) very difficult.
Using a simulator also means that we can have instant access to a large number of randomly generated chronicles, which at this stage of the project are not yet available.
The Chronicle Simulator simulates the history of a patient's illness, and links the events in the history in a manner that closely resembles the expected output of the real Automatic Chronicler.
The current output format of the simulator is a relational database that stores six types of event2 (interventions, investigations, consults, drugs, problems and loci) and 14 types of relation between events (e.g., Problem 2 The term event is loosely used to denote dynamic (such as interventions) as well as static concepts (such as problems).
If the focus is on Interventions, the same information in the previous example will be presented as: Example 3  In week 483, histopathology revealed primary cancer of the right breast.
Radical mastectomy on the breast was performed to treat the cancer.
Radiotherapy was initiated to treat primary cancer of the right breast.
 In the weeks 492 to 496, 5 radiotherapy cycles were performed.
In an Investigation-focussed report, the intervention will be omitted, since they are not directly relevant: Example 4  In week 483, histopathology revealed primary cancer of the right breast  In week 491, examination revealed no abnormality of the leucocyte count or of the haemoglobin concentration, no lymphadenopathy of the right axillary lymphnodes, no enlargement of the spleen or of the liver and no recurrent cancer Locus, Intervention CAUSED-BY Problem, Intervention SUBPART-OF Intervention, Investigation HAS-INDICATION Problem).
Each event has a variable number of attributes, and each dynamic event is time-stamped with a start date and an end date3 . A typical chronicle contains around 350 events and about 600 relations.
HAS-LOCUS Locus Investigation Problem Intervention Locus Problem Investigation Intervention Locus Problem Drug Problem 4 Architecture The design of the Report Generator follows a classical pipeline architecture, with a content selector, content planner and syntactic realiser.
The Content Planner is tightly coupled to the Content Selector, since part of the discourse structure is already determined in the event selection phase.
Aggregation is mostly conceptual rather than syntactic, and thus it is performed in the content planning stage as well as during realisation (Reape and Mellish, 1999).
4.1 Content
selection Figure 1: Example of a generated spine structure The Content selection process represents the most important component of the Report Generator.
Although in some contexts it may be useful to generate reports containing all the events in a chronicle, the most useful types of report are the focused, summarised ones, for which good selection of important events is essential.
The process of content selection is currently driven by two parameters of a report: type and length.
We define the concept of report spine to represent a list of concepts that are essential to the construction of a given type of report.
For example, in a report of the diagnoses, all events of type Problem will be part of the spine.
Events linked to the spine through some kind of relation may or may not be included in the summary, depending on the type and length of the summary (see Figure 1).
The design of the system does not restrict the spine to containing only events of the same type.
In future extensions to the system where the user will be able to select facts they want in the summary, a spine could contain, for example, problems of type cancer, investigations of type x-ray and interventions of type surgery.
3 In
the current implementation of the chronicle, time stamps are week numbers starting with the date of the first diagnosis.
Spines are not predefined templates, but structures that are constructed dynamically with each request and they depend on the type of request and on the length of the summary.
Important events are selected according to semantic relations.
The first step in the selection process is to cluster related events based on the relations stored in the chronicle.
A cluster of events may tell us, for example, that a patient was diagnosed with cancer following a clinical examination, for which she had a mastectomy to remove the tumour, was given a histopathological test of the removed tumour, which confirmed the cancer, and had a complete radiotherapy course to treat the cancer; the radiotherapy caused an ulcer, which in turn was treated with some drug.
A typical chronicle contains a small number of clusters, typically one or two large clusters and several small ones.
Smaller clusters are generally not related to the main thread of events.
The summarisation process starts with the removal of small clusters, which in the current implementation are defined as clusters containing at most three events4 . This excludes some specified types of information that will be included in the report even when they only appear in short clusters; for example, all reports will contain essential information such as the initial diagnosis and the cause of death (if available).
The next step is the selection of important events, as defined by the type of report.
Each cluster of events is a graph, with some nodes representing spine events.
For each cluster, the spine events are selected, as well as all nodes that are at a distance of less than n from spine events, This threshold was set following a series of experiments.
fiwhere the depth n is a user-defined parameter used to adjust the size of the report.
For example, in the cluster presented in Fig.
2, assuming a depth value of 1, the content selector will choose cancer, left breast and radiotherapy but not radiotherapy cycle or ulcer.
Document planning cancer Ind i d_ cate By Has cus radiotherapy left breast Cau sed radiotherapy cycle ulcer Figure 2: Example of a cluster The first stage in structuring the body of the report is to combine messages linked through attributive relations (e.g., combining messages of type Problem with messages of type Locus if the Problem has a HAS-LOCUS relation pointing to a Locus).
In the second stage, messages are grouped according to specific rules, depending on the type of report.
For longitudinal reports, the rules stipulate that events occurring in the same week should be grouped together, and further grouped into years.
In event-specific reports, patterns of similar events are first identified and then grouped according to the week(s) they occur in.
For example, if in week 1 the patient was examined for enlargement of the liver and of the spleen with negative results and in week 2 the patient was again examined with the same results and had a mastectomy, two groups of events will be constructed: Example 5  In weeks 1 and 2, examination of the abdomen revealed no enlargement of the liver or of the spleen.
 In week 2, the patient underwent a mastectomy.
A document plan is typically a hierarchical structure that contains and combines the messages to be conveyed by the report generator.
Technically, a document plan is an ordered collection of message clusters, where messages within a cluster are combined using rhetorical relations, while individual clusters are ordered and linked according to the type of report.
The construction of document plans is partly performed in the content selection phase, since the content is selected according to the relations between events, which in turn provide information about the structure of the target text.
The actual document planner component is concerned with the construction of complete document plans, according to the type of report and cohesive relations identified in the previous stage.
A report typically consists of three parts: (a) a schematic description of the patient's demographic information (name, age, gender), (b) a two sentence summary of the patient's record (presenting the time span of the illness, the number of consults the patient attended, and the number of investigations and interventions performed) and (c) the actual report of the record produced from the events selected to be part of the content.
We focus here on this last part.
Within groups, messages are structured according to discourse relations that are either deduced from the input database or automatically inferred by applying domain specific rules.
At the moment, the input provides three types of rhetorical relation: Cause, Result and Sequence.
The domain specific rules specify the ordering of messages, and always introduce a Sequence relation.
An example of such a rule is that a histopathology event has to follow a biopsy event, if both of them are present and they start and end at the same time.
These rules facilitate the construction of a partial rhetorical structure tree.
Messages that are not connected in the tree are by default assumed to be in a List relation to other messages in the group, and their position is set arbitrarily.
The document planner also applies aggregation rules between similar messages and employs ellipsis and conjunction in order to create a more fluent text.
Simple aggregation rules state, for example, that two investigations with Figure 3: Aggregation of Investigation messages on the HAS-TARGET field the same name and two different target loci can be collapsed into one investigation with two target loci (Fig.3).
Aggregation rules of this type are designed to make the resulting text more fluent, however they do not always provide the degree of condensation required by the summary.
For example, each clinical examination consists of examinations of the abdomen for enlargement of internal organs (liver and spleen) and examination of the lymphnodes.
Thus, each clinical examination will typically consist of three independent Investigation events.
When fully aggregated according to conceptual and syntactical rules, the three Investigation messages are collapsed into one structure such as: Example 6 Examination revealed no enlargement of the spleen or of the liver and no lymphadenopathy of the axillary nodes.
this can be described as Clinical examination was normal, apart from an enlargement of the spleen.
4.3 Maintaining
the thread of discourse In producing multiple reports on the same patient from different perspectives, or of different types, we operate under the strong assumption that event-focussed reports should be organised in a way that emphasises the importance of the event in focus.
From a document structure viewpoint, this equates to constructing rhetorical structures where the focus event (i.e., the spine event) is expressed in a nuclear unit, and skeleton events are preferably in sattelite units.
Within sentences, spine events are assigned salient syntactical roles that allows them to be kept in focus.
For example, a relation such as Problem CAUSED-BY Intervention is more likely to be expressed as : The patient developed a Problem as a result of an Intervention.
However, this level of aggregation that only takes into account the semantics of individual messages may be not enough, since clinical examinations are performed repeatedly and consist of the same types of investigation.
Two approaches have been implemented in the Report Generator, both of which make use of domain specific rules.
The first is to report only events that deviate from the norm.
In the case of investigations, for example, this equates to reporting only those that have abnormal results.
The second, which produces larger reports, is to produce synthesised descriptions of events.
In the case of clinical examination for example, we could describe a sequence of investigations such as the one in example (5) as Clinical examination was normal.
If the examination deviates from the norm on a restricted numbers of parameters only, when the focus is on Problem events, and, when the focus is on Interventions as: An Intervention caused a Problem.
This kind of variation reflects the different emphasis that is placed on spine events, although the wording in the actual report may be different.
Rhetorical relations holding between simple event descriptions are most often realised as a single sentence (as in the examples above).
Complex individual events are realised in individual clauses or sentences which are connected to other accompanying events through the appropriate rhetorical relation.
For example, a Problem event has a large number of attributes, consisting of name, status, existence, number of nodes counted, number of nodes involved, clinical course, tumour size, genotype, grade, tumour marker and histology, as well as the usual time stamp.
The selection of attributes that are going to be included in a Problem description depends on a number of factors, including whether the Problem is a spine or a skeleton event, and whether the event is mentioned for the first time or is a subsequent mention.
Aditionally, the number of attributes included in the description of a Problem is a decisive factor in realising the Problem as a phrase, a sentence or a group of sentences.
In the following two examples, there are two Problem events (cancer and lymphnode count) linked through an Investigation event (excision biopsy, which is indicated by the first problem and has as a finding the second problem.
In Example 7, the problems are first mentioned spine events, while in Example 8, the problems are skeleton events (the cancer is a subsequent mention and the lymphnode count is a first mention), with the Investigation being the spine event.
Example 7 A 10mm, EGFR +ve, HER-2/neu +ve, oestrogen receptor positive cancer was found in the left breast (histology: invasive tubular adenocarcinoma).
Consequently, an excision biopsy was performed which revealed no metastatic involvement in the 5 nodes sampled.
Example 8 An excision biopsy on the left breast was performed because of cancer.
It revealed no metastatic involvement in the 5 nodes sampled.
5 Evaluation
Automatic evaluation of the generated reports is not possible, as there is no gold standard for such documents.
Additionally, a full-blown quantitative evaluation is not yet feasible, since our users are cancer specialists who cannot easily dedicate time to evaluating large numbers of reports.
However, we have conducted an informal survey with two cancer clinicians to gain feedback on the quality of the current output of the Report Generator.
To do this, we showed them three patient records encoded as chronicles, and, for each patient, two types of report produced from that record: a longitudinal report, and a summarised report of diagnoses.
The three patient records were selected to display a variety of events and sizes (a 6-year history containing 621 events, a 12-year history with 1418 events, and a 9-year history with 717 events).
Although they were (unusually) familiar with the coding scheme of the chronicles, the clinicians found it very difficult to extract a useful overview of the patients' histories from the three chronicles we showed them.
In contrast, they found the generated reports to be much more useful and the quality of the text to be very good.
The clinicians commended the reports for their ability to provide a quick and clear view of data that would be otherwise difficult to access and process.
Most importantly, the various report types were judged to be highly appropriate for use in clinical care.
Whilst this preliminary evaluation was conducted with the aim of finding early shortcomings of the Report Generator and receiving feedback from potential users, we are now embarking on a more extensive formal evaluation with cancer clinicians and medical researchers with specialist knowledge in the area of cancer.
We believe, however, that the true test of utility will be the actual use of the Report Generator in practice.
As can be seen from the examples above, the same basic rhetorical structure consisting of three nodes and two relations (causality and consequence) is realised differently in a Problemfocussed report compared to an Investigationbased report.
The conceptual reformulation is guided by the type of report, which in turn has consequences at syntactical level.
6 Conclusions
We have described a system that generates a range of health reports on individual cancer patients.
At present, our intended readership is composed of clinicians and medical researchers, and the fitype of report will depend on his or her stated needs.
Reports that are required at the point of care (e.g., for a doctor interviewing a newly referred patient, or a team of medics on ward rounds) are likely to be short "30-second" potted histories.
At other times longer, more detailed reports will be required, as will reports that focus on particular aspects of the patient's "journey" through their disease (e.g., from the perspective of the diagnoses that have been made, the drugs they have been prescribed, or surgery they have undergone).
The system is fully implemented in Java and currently generates this full range of reports on-the-fly.
A summarised report based on about 1000 input events is constructed in less than 2 seconds, a speed which is highly appropriate to the demands of clinical practice.
While the various types of generated report all share the same input (i.e., the patient's chronicle), and thus will have a large degree of conceptual overlap, clearly there will be occassions when information that is included in some reports will not be in others.
The range of reports for any given patient at any given point in their illness thus present a special class of paraphrase, with a looser adherance to semantic equivalence between versions than is typically found in other paraphrase generators, for example Kozlowski et al (2003), McKeown et al (1994), Power, Scott and Bouyaad-Agha (2003), Rosner and Stede (1994),(1996), and Scott and Souza (1990).
In this sense, our Report Generator is rather closer in spirit to Hovy's PAULINE system, which generates descriptions of given news events from different perspectives and with different stylistic goals (Hovy, 1988).
However, we achieve our goal with less reliance on terminological variation and more on structural variation at the discourse level.
Syntactic variation, where it does occur, is almost always simply a side-effect of an earlier discourse choice.
Terminological variation is deliberately avoided to prevent false implicatures; however, we are about to introduce a further class of readership, namely patients, at which stage we will make fuller use of our lexical resources.
7 Acknowledgments
CLEF is supported in part by grant G0100852 under the E-Science Initiative.
Thanks are due its clinical collaborators at the Royal Marsden and Royal Free hospitals, to colleagues at the National Cancer Research Institute (NCRI) and NTRAC and to its industrial collaborators.
Special thanks to Dr.
Jeremy Rogers who provided us with the automated Chronicle Simulator that we have used in all our experiments.
References Eduard H.
Hovy. 1988.
Generating natural language under pragmatic constraints.
Lawrence Erlbaum, Hillsdale, New Jersey.
Raymond Kozlowski, Kathleen F.
McCoy, and K.
Vijay-Shanker. 2003.
Generation of singlesentence paraphrases from predicate/argument structure using lexico-grammatical resources.
In Kentaro Inui and Ulf Hermjakob, editors, Proceedings of the Second International Workshop on Paraphrasing, pages 18.
Kathleen McKeown, Karen Kukich, and James Shaw.
1994. Practical issues in automatic document generation.
In Proceedings of the Fourth Conference on Applied Natural-Language Processing (ANLP-1994), pages 714, Stuttgart, Germany.
Richard Power, Donia Scott, and Nadjet BouayadAgha.
2003. Document structure.
Computational Linguistics, 29(2):211260.
Mike Reape and Chris Mellish.
1999. Just what is aggregation anyway?
In Proceedings of the 7th European Workshop on Natural Language Generation (EWNLG'99), pages 2029, Toulouse, France.
Dietmar R sner and Manfred Stede.
1994. Generating multilingual documents from a knowledge base: the TECHDOC project.
In Proceedings of the 15th conference on Computational Linguistics (Coling'94), pages 339343, Kyoto, Japan.
Donia Scott and Clarisse de Souza.
1990. Getting the message across in RST-based text generation.
In R.
Dale C.
Mellish and M.
Zock, editors, Current Research in Natural Language Generation, pages 31  56.
Academic Press.
Manfred Stede.
1996. Lexical paraphrases in multilingual sentence generation.
Machine Translation, 11:75107 .
Automatic Summarization of Open-Domain Multiparty Dialogues in Diverse Genres Klaus Zechner Educational Testing Service Automatic summarization of open-domain spoken dialogues is a relatively new research area.
This article introduces the task and the challenges involved and motivates and presents an approach for obtaining automatic-extract summaries for human transcripts of multiparty dialogues of four different genres, without any restriction on domain.
We address the following issues, which are intrinsic to spoken-dialogue summarization and typically can be ignored when summarizing written text such as news wire data: (1) detection and removal of speech disfluencies; (2) detection and insertion of sentence boundaries; and (3) detection and linking of cross-speaker information units (question-answer pairs).
A system evaluation is performed using a corpus of 23 dialogue excerpts with an average duration of about 10 minutes, comprising 80 topical segments and about 47,000 words total.
The corpus was manually annotated for relevant text spans by six human annotators.
The global evaluation shows that for the two more informal genres, our summarization system using dialoguespecific components significantly outperforms two baselines: (1) a maximum-marginal-relevance ranking algorithm using TF*IDF term weighting, and (2) a LEAD baseline that extracts the first n words from a text.
1. Introduction Although the field of summarizing written texts has been explored for many decades, gaining significantly increased attention in the last five to ten years, summarization of spoken language is a comparatively recent research area.
As the number of spoken audio databases is growing rapidly, however, we predict that the need for high-quality summarization of information contained in this medium will increase substantially.
Summarization of spoken dialogues, in particular, may aid in the archiving, indexing, and retrieval of various records of oral communication, such as corporate meetings, sales interactions, or customer support.
The purpose of this article is to explore the issues of spoken-dialogue summarization and to describe and evaluate an implementation addressing some of the core challenges intrinsic to the task.
We will use an implementation of a state-of-the-art text summarization method (maximum marginal relevance, or MMR) as the main baseline for comparative evaluations, and then add a set of components addressing issues specific to spoken dialogues to this MMR module to create our spoken dialogue summarization system, which we call DIASUMM.
We consider the following dimensions to be relevant for our research; the combination of these dimensions distinguishes our work from most other work in the field Educational Testing Service, Rosedale Road MS 11-R, Princeton, NJ 08541.
E-mail: kzechner@ets.org c 2002 Association for Computational Linguistics Computational Linguistics Volume 28, Number 4 of summarization:     spoken versus written language multiparty dialogues versus texts written by one author unrestricted versus restricted domains diverse genres versus a single genre The main challenges this work has to address, in addition to the challenges of writtentext summarization, are as follows:     coping with speech disfluencies identifying the units for extraction maintaining cross-speaker coherence coping with speech recognition errors We will discuss these challenges in more detail in the following section.
Although we have addressed the issue of speech recognition errors in previous related work (Zechner and Waibel 2000b), for the purpose of this article, we exclusively use human transcripts of spoken dialogues.
Intrinsic evaluations of text summaries usually use sentences as their basic units.
For our data, however, sentence boundaries are typically not available in the first place.
Thus we devise a word-based evaluation metric derived from an average relevance score from human relevance annotations (section 6.2).
The organization of this article is as follows: Section 2 provides the motivation for our research, introducing and discussing the main challenges of spoken-dialogue summarization, followed by a section on related work (section 3).
Section 4 describes the corpus we use to develop and evaluate our system, along with the procedures employed for corpus annotation.
The system architecture and its components are described in detail in section 5, along with evaluations thereof.
Section 6 presents the global evaluation of our approach, before we conclude the article with a discussion of our results, contributions, and directions for future research in this field (sections 7 and 8).
2. Motivation Consider the following example from a phone conversation drawn from the English CALLHOME database (LDC 1996).
It is a transcript of a conversation between two native speakers of American English; one person is in the New York area (speaker a), the other one (speaker b) in Israel.
It was recorded about a month after Yitzhak Rabin's assassination (1995).
This dialogue segment is about one minute of real time.
The audio is segmented into speaker turns using silence heuristics,1 and each turn is marked with a turn number and with the speaker label.
Noises are removed to increase readability.2 1 Therefore, in some cases, we can find several turns of one speaker following each other.
2 Hence
there can be "missing" turns (e.g., turn 37), in case they contain only noises and no actual words.
Zechner Automatic Summarization of Dialogues 28 a: oh 29 b: they didn't know he was going to get shot but it was at a peace rally so i mean it just worked out 30 b: i mean it was a good place for the poor guy to die i mean because it was you know right after the rally and everything was on film and everything 31 a: yeah 32 b: oh the whole country we just finished the thirty days mourning for him now you know it's uh oh everybody's still in shock it's 33 a: oh 34 a: i know 35 b: terrible what's going on over here 36 b: and this guy that killed him they show him on t v smiling he's all happy he did it and everything he isn't even sorry or anything 38 a: there are i 39 b: him him he and his brother you know the two of them were in it together and there's a whole group now it's like a a conspiracy oh it's eh 40 a: mm 41 a: with the kahane chai 42 b: unbelievable 43 b: yeah yeah it's all those people yeah you probably see them running around new york don't you they're all 44 a: yeah 45 a: oh yeah they're here 46 b: new york based yeah 47 a: oh there's 48 a: all those fanatics 49 a: like the extreme 50 b: oh but 51 b: but whwhat's the reaction in america really i mean i mean do people care you know i mean you know do they 52 a: yeah momost pei mean uh 53 a: i don't know what commui mean like the jewish community 54 a: a lot eall of us were 55 a: very upset and there were lots all the 56 b: yeah 57 a: like two days after did it happen like on a sunday 58 b: yeah it hapit happened on it happened on a saturday night By looking at this transcript we can readily identify some of the phenomena that would cause difficulties for conventional summarizers of written texts:  Some turns (e.g., turn 51) contain many disfluencies that (1) make them hard to read and (2) reduce the relevance of the information contained therein.
Some (important) pieces of information are distributed over a sequence of turns (e.g., turns 535455, 45474849); this is due to a silence-based 449 Computational Linguistics Volume 28, Number 4 segmentation algorithm that causes breaks in logically connected clauses.
A traditional summarizer might render these sequences incompletely.
 Some turns are quite long (e.g., 36, 39) and contain several sentences; a within-turn segmentation seems necessary to avoid the extraction of too much extraneous information when only parts of a turn contain relevant information.
Some of the information is constructed interactively by both speakers; the prototypical cases are question-answer pairs (e.g., turns 5152ff., turns 5758).
A traditional text summarizer might miss either question or answer and hence produce a less meaningful summary.
We shall discuss these arising issues along with an indication of our computational remedies in the following subsections.
We want to stress beforehand, though, that the originality of our system should not be seen in the particular implementation of its individual components, but rather in their selection and specific composition to address the issues at hand in an effective and also efficient way.
2.1 Disfluency
Detection The two main negative effects speech disfluencies have on summarization are that they (1) decrease the readability of the summary and (2) increase its noncontent noise.
In particular for informal conversations, the percentage of disfluent words is quite high, typically around 20% of the total words spoken.3 This means that this issue should, in our opinion, be addressed to improve the quality (readability and conciseness) of the generated summaries.
In section 5.3 we shall present three components for identifying most of the major classes of speech disfluencies in the input of the summarization system, such as filled pauses, repetitions, and false starts.
All detected disfluencies are marked in this process and can be selectively excluded during summary generation.
2.2 Sentence
Boundary Detection Unlike written texts, in which punctuation markers clearly indicate clause and sentence boundaries, spoken language is generated as a sequence of streams of words, in which pauses (silences between words) do not always match linguistically meaningful segments: A speaker can pause in the middle of a sentence or even a phrase, or, on the other hand, might not pause at all after the end of a sentence or clause.
This mismatch between acoustic and linguistic segmentation is reflected in the output of a speech recognizer, which typically generates a sequence of speaker turns whose boundaries are marked by periods of silence (or nonspeech).
As a result, one speaker's turn may contain multiple sentences, or, on the other hand, a speaker's sentence might span more than one turn.
In a test corpus of five English CALLHOME dialogues with an average length of 320 turns, we found on average of about 30 such continuations of logical clauses over automatically determined acoustic segments per dialogue.
The main problems for a summarizer would thus be (1) the lack of coherence and readability of the output because of incomplete sentences and (2) extraneous information due to extracted units consisting of more than one sentence.
In section 5.4 we 3 Although other studies have found percentages lower than this figure, we included content-less categories such as discourse markers or rhetorical connectives, which are often not regarded as disfluencies per se.
Zechner Automatic Summarization of Dialogues describe a component for sentence segmentation that addresses both of these problems.
2.3 Distributed
Information Since we have multiparty conversations as opposed to monologues, sometimes the crucial information is found in a sequence of turns from several speakers, the prototypical case of this being a question-answer pair.
If the summarizer were to extract only the question or only the answer, the lack of the corresponding answer or question would often cause a severe reduction of coherence in the summary.
In some cases, either the question or the answer is very short and does not contain any words with high relevance that would yield a substantial weight in the summarizer.
In order not to lose these short sentences at a later stage, when only the most relevant sentences are extracted, we need to identify matching question-answer pairs ahead of time, so that the summarizer can output the matching sentences during summary generation as one unit.
We describe our approach to cross-speaker information linking in section 5.5. 2.4 Other Issues We see the work reported in this article as the first in-depth analysis and evaluation in the area of open-domain spoken-dialogue summarization.
Given the large scope of this undertaking, we had to restrict ourselves to those issues that are, in our opinion, the most salient for the task at hand.
A number of other important issues for summarization in general and for speech summarization in particular are either simplified or not addressed in this article and left for future work in this field.
In the following, we briefly mention some of these issues, indicating their potential relevance and promise.
2.4.1 Topic
Segmentation.
In many cases, spoken dialogues are multitopical.
For the English CALLHOME corpus, we determined an average topic length of about one to two minutes' speaking time (or about 200400 words).
Summarization can be accomplished faster and more concisely if it operates on smaller topical segments rather than on long pieces of input consisting of diverse topics.
Although we have implemented a topic segmentation component as part of our system for these reasons, all of the evaluations are based on the topical segments determined by human annotators.
Therefore, this component will not be discussed in this article.
Furthermore, topical segmentation is not an issue intrinsic to spoken dialogues, which in our opinion justifies this simplification.
2.4.2 Anaphora
Resolution.
An analogous reasoning holds for the issue of anaphora resolution: Although it would certainly be desirable, for the sake of increased coherence and readability, to employ a well-working anaphora resolution component, this issue is not specific to the task at hand, either.
One could argue that particularly for summarization of more informal conversations, in which personal pronouns are rather frequent, anaphora resolution might be more helpful than for, say, summarization of written texts.
But we conjecture that this task might also prove more challenging than written-text anaphora resolution.
In our system, we did not implement a module for anaphora resolution.
2.4.3 Discourse
Structure.
Previous work indicates that information about discourse structure from written texts can help in identifying the more salient and relevant sentences or clauses for summary generation (Marcu 1999; Miike et al.1994). Much 451 Computational Linguistics Volume 28, Number 4 less exploration has been done, however, in the area of automatic analysis of discourse structure for non-task-oriented spoken dialogues in unrestricted domains, such as CALLHOME (LDC 1996).
Research for those kinds of corpora reported in Jurafsky et al.(1998), Stolcke et al.(2000), Levin et al.(1999), and Ries et al.(2000) focuses more on detecting localized phenomena such as speech acts, dialogue games, or functional activities.
We conjecture that there are two reasons for this: (1) free-flowing spontaneous conversations have much less structure than task-oriented dialogues, and (2) the automatic detection of hierarchical structure would be much harder than it is for written texts or dialogues based on a premeditated plan.
Although we believe that in the long run attempts to automatically identify the discourse structure of spoken dialogues may benefit summarization, in this article, we greatly simplify this matter and exclusively look at local contexts in which speakers interactively construct shared information (the question-answer pairs).
2.4.4 Speech
Recognition Errors.
Throughout this article, our simplifying assumption is that our input comes from a perfect speech recognizer; that is, we use human textual transcripts of the dialogues in our corpus.
Although there are cases in which this assumption is justifiable, such as transcripts provided by news services in parallel to the recorded audio data, we believe that in general a spoken dialogue summarizer has to be able to accept corrupted input from an automatic speech recognizer (ASR), as well.
Our system is indeed able to work with ASR output; it is integrated in a larger system (Meeting Browser) that creates, summarizes, and archives meeting records and is connected to a speech recognition engine (Bett et al.2000). Further, we have shown in previous work how we can use ASR confidence scores (1) to reduce the word error rate within the summary and (2) to increase the summary accuracy (Zechner and Waibel 2000b).
2.4.5 Prosodic
Information.
A further simplifying assumption of this work is that prosodic information is not available, with the exception of start and end times of speaker turns.
Considering the results reported by Shriberg et al.(1998) and Shriberg et al.(2000), we conjecture that future work in this field will demonstrate the additional benefit of incorporating prosodic information, such as stress, pitch, and intraturn pauses, into the summarization system.
In particular, we would expect improved system performance when speech recognition hypotheses are used as input: In that case, the prosodic information could compensate to some extent for incorrect word information.
3. Related Work The vast majority of summarization research in the past clearly has focused exclusively on written text.
A good selection of both early seminal papers and more recent work can be found in Mani and Maybury (1999).
In general, most summarization approaches can be classified as either corpus-based, statistical summarization (such as Kupiec, Pedersen, and Chen [1995]), or knowledge-based summarization (such as Reimer and Hahn [1988]) in which the text domain is restricted.
(The MMR method [Carbonell, Geng, and Goldstein 1997], which we are using as the summarization engine for our DIASUMM system, belongs to the first category).
More recently, Marcu (1999) presented work on using automatically detected discourse structure for summarization.
Knight and Marcu (2000) and Berger and Mittal (2000) presented approaches in which summarization can be reformulated as a problem of machine translation: 452 Zechner Automatic Summarization of Dialogues translating a long sentence into a shorter sentence, or translating a Web page into a brief gist, respectively.
Two main areas are exceptions to the focus on text summarization in past work: (1) summarization of task-oriented dialogues in restricted domains and (2) summarization of spoken news in unrestricted domains.
We shall discuss both of these areas in the following subsections, followed by a discussion of prosody-based emphasis detection in spoken language, and finally by a summary of research most closely related to the topic of this work.
3.1 Summarization
of Dialogues in Restricted Domains During the past decade, there has been significant progress in the area of closeddomain spoken-dialogue translation and understanding, even with automatic speech recognition input.
Two examples of systems developed in that time frame are JANUS (Lavie et al.1997) and VERBMOBIL (Wahlster 1993).
In that context, several spoken-dialogue summarization systems have been developed whose goal it is to capture the essence of the task-based dialogues at hand.
The MIMI system (Kameyama and Arima 1994; Kameyama, Kawai, and Arima 1996) deals with the travel reservation domain and uses a cascade of finite-state pattern recognizers to find the desired information.
Within VERBMOBIL, a more knowledge-rich approach is used (Alexandersson and Poller 1998; Reithinger et al.2000). The domain here is travel planning and negotiation of a trip.
In addition to finite-state transducers for content extraction and statistical dialogue act recognition, VERBMOBIL also uses a dialogue processor and a summary generator that have access to a world knowledge database, a domain model, and a semantic database.
The abstract representations built by this summarizer allow for summary generation in multiple languages.
3.2 Summarization
of Spoken News Within the context of the Text Retrieval Conference (TREC) spoken document retrieval (SDR) conferences (Garofolo et al.1997; Garofolo et al.1999) as well as the recent Defense Advanced Research Project Agency (DARPA) broadcast news workshops, a number of research groups have been developing multimedia browsing tools for text, audio, and video data, which should facilitate the access to news data, combining different modalities.
Hirschberg et al.(1999) and Whittaker et al.(1999) present a system that supports local navigation for browsing and information extraction from acoustic databases, using speech recognizer transcripts in tandem with the original audio recording.
Although their interface helps users in the tasks of relevance ranking and fact finding, it is less helpful in the creating of summaries, partly because of imperfect speech recognition.
Valenza et al.(1999) present an audio summarization system that combines acoustic confidence scores with relevance scores to obtain more accurate and reliable summaries.
An evaluation showed that human judges preferred summaries with a compression rate of about 15% (30 words per minute at a speaking rate of about 200 words per minute) and that the summary word error rate was significantly smaller than the word error rate for the full transcript.
Hori and Furui (2000) use salience features in combination with a language model to reduce Japanese broadcast news captions by about 3040% while keeping the meaning of about 72% of all sentences in the test set.
Another speech-related reduction approach was presented recently by Koumpis and Renals (2000), who summarize voice mail in the Small Message format.
453 Computational Linguistics Volume 28, Number 4 3.3 Prosody-Based Emphasis Detection in Spoken Audio Whereas most approaches to summarizing acoustic data rely on the word information (provided by a human or ASR transcript), there have been attempts to generate summaries based on emphasized regions in a discourse, using only prosodic features.
Chen and Withgott (1992) train a hidden Markov model on transcripts of spontaneous speech, labeled for different degrees of emphasis by a panel of listeners.
Their "audio summaries" on an unseen (but rather small) test set achieve a remarkably good agreement with human annotators ( > 0.5).
Stifelman (1995) uses a pitch-based emphasis detection algorithm developed by Arons (1994) to find emphasized passages in a 13-minute discourse.
In her analysis, she finds good agreement between these emphasized regions and the beginnings of manually marked discourse segments (in the framework of Grosz and Sidner [1986]).
Although these are promising results, being suggestive of the role of prosody for determining emphasis, relevance, or salience in spoken discourse, in this work we restrict the use of prosody to the turn length and interturn pause features.
We conjecture, however, that the integration of prosodic and word level information would be a fruitful research area that would have to be explored in future work.
3.4 Spoken
Dialogue Summarization in Unrestricted Domains Waibel, Bett, and Finke (1998) report results of their summarizer on automatically transcribed SWITCHBOARD (SWBD) data (Godfrey, Holliman, and McDaniel 1992), the word error rate being about 30%.
Their implementation used an algorithm inspired by MMR, but they did not address any dialogueor speech-related issues in their summarizer.
In a question-answer test with summaries of five dialogues, participants could identify most of the key concepts using a summary size of only five turns.
These results varied widely (between 20% and 90% accuracy) across the five different dialogues tested in this experiment.
Our own previous work (Zechner and Waibel 2000a) addressed for the first time the combination of challenges of dialogue summarization with summarization of spoken language in unrestricted domains.
We presented a first prototype of DIASUMM that addressed the issues of disfluency detection and removal and sentence boundary detection, as well as cross-speaker information linking.
This work extends and expands these initial attempts substantially, in that we are now focusing on (1) a systematic training of the major components of the DIASUMM system, enabled by the recent availability of a large corpus of disfluency-annotated conversations (LDC 1999b), and (2) the exploration of three more genres of spoken dialogues in addition to the English CALLHOME corpus (NEWSHOUR, CROSSFIRE, GROUP MEETINGS).
Further, the relevance annotations are now performed by a set of six human annotators, which makes the global system evaluation more meaningful, considering the typical divergence among different annotators' relevance judgments.
4. Data Annotation 4.1 Corpus Characteristics Table 1 provides the statistics on the corpus used for the development and evaluation of our system.
We use data from four different genres, two being more informal, two more formal:  English CALLHOME and CALLFRIEND: from the Linguistic Data Consortium (LDC) collections, eight dialogues for the devtest set Zechner Automatic Summarization of Dialogues Table 1 Data characteristics for the corpus (average over dialogues).
8E-CH, 4E-CH: English CallHome; NHOUR: NewsHour; XFIRE: CrossFire; G-MTG: Group Meetings.
Data Set 8E-CH 4E-CH NHOUR formal yes 3 8 2 25 101 4.1 6.3 2.0 1224 12.1 5.1 48 0.48 64.8 17.2 3.4 0.7 13.8 XFIRE G-MTG Formal/informal informal informal Topics predetermined no no Dialogue excerpts (total) 8 4 Topical segments (total) 28 23 Different speakers 2.1 2 Turns 242 276 Sentences 280 366 Sentences per turn 1.2 1.3 Questions (in %) 3.7 6.4 False starts (in %) 12.1 11.0 Words 1685 1905 Words per sentence 6.0 5.2 Disfluent (in %) 16.0 16.3 Disfluencies 222 259 Disfluencies per sentence 0.79 0.71 Empty coordinating conjunctions (in %) 30.3 30.4 Lexicalized filled pauses (in %) 18.8 21.0 Editing terms (in %) 3.6 1.6 Nonlexicalized filled pauses (in %) 20.8 29.9 Repairs (in %) 26.6 17.1 (8E-CH) and four dialogues for the eval set (4E-CH).4 These are recordings of phone conversations between two family members or friends, typically about 30 minutes in length; the excerpts we used were matched with the transcripts, which typically represent 510 minutes of speaking time.
   NEWSHOUR (NHOUR): Excerpts from PBS's NewsHour television show with Jim Lehrer (recorded in 1998).
CROSSFIRE (XFIRE): Excerpts from CNN's CrossFire television show with Bill Press and Robert Novak (recorded in 1998).
GROUP MEETINGS (G-MTG): Excerpts from recordings of project group meetings in the Interactive Systems Labs at Carnegie Mellon University.
Furthermore, we used the Penn Treebank distribution of the SWITCHBOARD corpus, annotated with disfluencies, to train the major components of the system (LDC 1999b).
From Table 1 we can see that the two more formal corpora, NEWSHOUR and CROSSFIRE, have longer sentences, more sentences per turn, and fewer disfluencies (particularly nonlexicalized filled pauses and false starts) than English CALLHOME and the GROUP MEETINGS.
This means that their flavor is more like that of written text and not so close to the conversational speech typically found in the SWITCHBOARD or CALLHOME corpora.
4 We
used the devtest set corpus for system development and tuning and set aside the eval set for the final global system evaluation.
For the other three genres, two dialogue excerpts each were used for the devtest set, the remainder for the eval set.
Computational Linguistics Volume 28, Number 4 4.2 Corpus Annotation 4.2.1 First Annotation Phase.
All the annotations were performed on human-generated transcripts of the dialogues.
The CALLHOME and GROUP MEETINGS dialogues were automatically partitioned into speaker turns (by means of a silence heuristic); the other corpora were segmented manually (based on the contents and flow of the conversation).5 There were six naive human annotators performing the task;6 only four, however, completed the entire set of dialogues.
Thus, the number of annotations available for each dialogue varies from four to six.
Prior to the relevance annotations, the annotators had to mark topical boundaries, because we want to be able to define and then create summaries for each topical segment separately (as opposed to a whole conversation consisting of multiple topics).
The notion of a topic was informally defined as a region in the text that ends, according to the annotation manual, "when the speakers shift their topic of discussion".
Once the topical segments were marked, for each such segment, each annotator had to identify the most relevant information units (IUs), called nucleus IUs, and somewhat relevant IUs, called satellite IUs.
IUs are often equivalent to sentences but can span longer or shorter contiguous segments of text, dependent on the annotator's choice.
The overall goal of this relevance markup was to create a concise and readable summary containing the main information present in the topical segment.
Annotators were also asked to mark the most salient words within their annotated IUs with a +, which would render a summary with a somewhat more telegraphic style (+-marked words).
We also asked that the human annotators stay within a preset target length for their summaries: The +-marked words in all IUs within a topical segment should be 1020% of all the words in the segment.
The guideline was enforced by a checker program that was run during and after annotation of a transcript and that also ensured that no markup errors and no accidental word deletions occurred.
We provide a brief example here (n[, n] mark the beginning and end of a nucleus IU, the phrase they fly to Boston was +-marked as the core content within this IU): B: heck it might turn out that you know n[ if +they +fly in +to +boston i can n] 4.2.2 Creation of Gold-Standard Summaries.
After the first annotation phase, in which each coder worked independently according to the guidelines described above, we devised a second phase, in which two coders from the initial group were asked to create a common-ground annotation, based on the majority opinion of the whole group.
To construct such a majority opinion guideline automatically, we assigned weights to all words in nucleus IUs and satellite IUs and added all weights for all marked words of all coders for every turn.7 The total turn weights were then sorted by decreasing value to provide a guide for the two coders in the second phase as to which turns they should focus their annotations on for the common-ground or gold-standard summaries.
5 This
fact may partially account for NEWSHOUR and CROSSFIRE turns being longer than CALLHOME and GROUP MEETING turns.
6 Naive
in this context means that they were nonexperts in linguistics or discourse analysis.
7 The
weights were set as follows: nucleus IUs: 3.0 if +-marked, 2.0 otherwise; satellite IUs: 1.0 if +-marked, 0.5 otherwise.
Zechner Automatic Summarization of Dialogues Table 2 Nuclei and satellites: Length in tokens and relative frequency (in % of all tokens).
Annotator/ Data Set LB BR SC RW RC JK Gold CALLHOME NEWSHOUR CROSSFIRE MEETINGS All Dialogues Avg.
Nuc. Avg.
Sat. Nuc-Tokens Nuc-+-Marked Sat-Tokens Sat-+-Marked Length Length (in %) (in %) (in %) (in %) 12.993 16.507 20.720 22.899 23.741 39.203 21.763 17.108 25.828 33.923 37.674 23.152 13.732 14.551 14.093 19.576 18.553 9.794 6.462 13.099 16.733 22.132 23.413 16.173 11.646 11.978 29.412 19.352 43.573 26.355 13.934 21.962 29.536 21.705 23.034 22.796 8.558 8.339 18.045 11.332 15.434 11.204 6.573 11.003 13.530 10.615 9.222 10.807 5.363 10.558 6.517 2.757 12.749 0.711 0.179 5.126 4.300 1.853 7.456 4.665 3.818 7.645 4.796 1.718 0.333 0.465 0.000 1.932 2.947 0.976 1.123 1.636 Other than this guideline, the requirements were almost exactly identical to those in phase 1, except that (1) the pair of annotators was required to work together on this task to be able to reach a consensus opinion, and (2) the preset relative word length of the gold summary (1020%) applied only to the nucleus IUs.
As for the topical boundaries, which obviously vary among coders, a list of boundary positions chosen by the majority (at least half) of the coders in the first phase was provided.
In this gold-standard phase, the two coders mostly stayed with these suggestions and changed less than 15% of the suggested topic boundaries, the majority of which were minor (less than two turns' difference in boundary position).
4.2.3 General
Annotation Analysis.
Table 2 provides the statistics on the frequencies of the annotated nucleus and satellite IUs.
We make the following observations:  On average, about 23% of all tokens were assigned to a nucleus IU and 5% to a satellite IU; counting only the +-marked tokens, this reduces to about 11% and 2% of all tokens, respectively.
The average total lengths of nuclei and satellites vary widely across corpora: between 17.1 (13.1) tokens for CALLHOME and 37.7 (23.4) tokens for GROUP MEETINGS data.
This is most likely a reflection on the typical length of turns in the different subcorpora.
A similar variation is also observed across annotators: between 12 and 40 tokens for nucleus-IUs and between 9 and 20 tokens for satellites.
The granularity of IUs is quite different across annotators.
Since some annotators mark a larger number of IUs than others, there is an even larger discrepancy in the relative number of words assigned to nucleus IUs and satellite IUs among the different annotators: 1144% (nucleus IUs) and 013% (satellite IUs).
The ratio of nucleus versus satellite tokens also varies greatly among the annotators: from about 1:1 to 40:1.
457 Computational Linguistics Volume 28, Number 4 The ratio of nucleus and satellite tokens that are +-marked varies greatly: between 36 and 77% for nucleus IUs and between 2 and 80% for satellite IUs.
From these observations, we conclude that merging the nucleus and satellite IUs into one class would yield a more consistent picture than keeping them separate.
A similar argument can be made for the +-marked passages, in which we also find a quite high intercoder variation in relative +-marking.
This led us to the decision of giving equal weight to any word in an IU, irrespective of IU type or marking, for the purpose of global system evaluation.
Finally, we conjecture that the average length of our extraction units should be in the 1040 words range, which roughly corresponds to about 312 seconds of real time, assuming an average word length of 300 milliseconds.
As a comparison, we note that Valenza et al.(1999) found summaries with 30-grams8 working well in their experiments, a finding that is in line with our observations here on typical human IU lengths.
4.2.4 Intercoder
Agreement.
Agreement between coders (and between automatic methods and coders) has been measured in the summarization literature with quite a wide range of methods: Rath, Resnick, and Savage (1961) use Kendall's ; Kupiec, Pedersen, and Chen (1995) (among many others) use percentage agreement; and Aone, Okurowski, and Gorlinsky (1997) (among others) use the notions of precision, recall, and F1 -score, which are commonly employed in the information retrieval community.
Similarly, in the literature on discourse segmentation and labeling, a variety of different agreement measures have been used, including precision and recall (Hearst 1997; Passonneau and Litman 1997), Krippendorff's (1980) (Passonneau and Litman 1997) and Cohen's (1960) (Carletta et al.1997). In this work, we use the two following metrics: (1) the -statistic in its extension for more than two coders (Davies and Fleiss 1982); and (2) precision, recall, and F1 -score.9 We will discuss the -statistic first.
For intercoder agreement with respect to topical boundaries, agreement is found if boundaries fall within the same 50-word bin of a dialogue.
Relevance agreements are computed at the word level.
For relevance markings, we compute both for the three-way case (nucleus IUs, satellite IUs, unmarked) and the two-way case (any IUs, unmarked).10 Topical-boundary agreement was not evaluated for two of the GROUP MEETINGS dialogues, in which only one of four annotators marked any text-internal topic boundary.
We compute agreements for each dialogue separately and report the arithmetic means for the five subcorpora in Table 3.
We observe that agreement for topical boundaries is much higher than for relevance markings.
Furthermore, agreement is generally higher for CALLHOME and comparatively low for the GROUP MEETINGS corpus.
As a second evaluation metric, we compute precision, recall, and F1 -scores for the same four annotators and the same sets of subcorpora as before.
For topical boundaries, a match means that the boundaries fall within 3 turns of each other, and for relevant 8 A 30-gram is a passage of text containing 30 adjacent words.
9 Precision
is the ratio of correctly matched items over all items (boundaries, marked words); recall is the ratio of correctly matched items over all items that need to be matched; and the F1 -score combines 2PR precision (P) and recall (R) in the following way: F1 = P+R. 10 These computations were performed for those four (out of six) annotators who completed the entire corpus markup.
Zechner Automatic Summarization of Dialogues Table 3 Intercoder annotation agreement for topical boundaries and relevance markings.
8E-CH Topical boundaries Relevance markings (3 way) Relevance markings (2 way) 0.503 0.147 0.157 4E-CH 0.402 0.161 0.169 NHOUR 0.256 0.123 0.124 XFIRE 0.331 0.089 0.100 G-MTG 0.174 0.040 0.046 Overall 0.384 0.117 0.126 Table 4 Intercoder annotation F1 -agreement for topical boundaries and relevance markings.
8E-CH Topical boundaries Relevance markings (2 way) .54 .38 4E-CH .44 .39 NHOUR .53 .38 XFIRE .38 .32 G-MTG .18 .32 Overall .45 .36 words a match means that the two words to be compared are both in a nucleus or satellite IU.
The results can be seen in Table 4.
4.2.5 Disfluency
and Sentence Boundary Annotation.
In addition to the annotation for topic boundaries and relevant text spans, the corpus was also annotated for speech disfluencies in the same style as the Penn Treebank SWITCHBOARD corpus (LDC 1999b).
One coder (different from the six annotators mentioned before) manually tagged the corpus for disfluencies and sentence boundaries following the SWITCHBOARD disfluency annotation style book (Meteer et al.1995). 4.2.6 Question-Answer Annotation.
A final type of annotation was performed on the entire corpus to mark all questions and their answers, for the purpose of training and evaluation of the question-answer linking system component.
Questions and answers were annotated in the following way: Every sentence that is a question was marked as either a Yes-No-question or a Wh-question.
Exceptions were back-channel questions, such as "Is that right?"; rhetorical questions, such as "Who would lie in public?"; and other questions that do not refer to a propositional content.
These were not supposed to be marked (even if they have an apparent answer), since we see the latter class of questions as irrelevant for the purpose of increasing the local coherence within summaries.
For each Yes-No-question and Wh-question that has an answer, the answer was marked with its relative offset to the question to which it belongs.
Some answers are continued over several sentences, but only the core answer (which usually consists of a single sentence) was marked.
This decision was made to bias the answer detection module toward brief answers and to avoid the question-answer regions' getting too lengthy, at the expense of summary conciseness.
5. Dialogue Summarization System 5.1 System Architecture The global system architecture of the spoken-dialogue summarization system presented in this article (DIASUMM) is depicted in Figure 1.
The input data are a timeordered sequence of speaker turns with the following quadruple of information: start time, end time, speaker label, and word sequence.
The seven major components are executed sequentially, yielding a pipeline architecture.
459 Computational Linguistics dialogue transcript Volume 28, Number 4 POS Tagger Disfluency Detection Sentence Boundary Detection Extraction Unit Identification False Start Detection (+ Chunk Parser) Question & Answer Detection Repetition Filter Topic Segmentation Sentence Ranking & Selection dialogue summary Figure 1 Global system architecture.
The following subsections describe the components of the system in more detail.
As argued earlier, the topic detection component is not relevant for the way we conduct the global system evaluation and hence is not discussed here.
(We implemented a variant of Hearst's [1997] TextTiling algorithm).
The three components involved in disfluency detection are the part-of-speech (POS) tagger, the false-start detection module, and the repetition filter.
They are discussed in subsection 5.3, followed by a subsection on sentence boundary detection (5.4).
The question-answer pair detection is described in subsection 5.5, and the sentence selection module, performing relevance ranking, in subsection 5.6. 5.2 Input Tokenization We eliminate all human and nonhuman noises and incomplete words from the input transcript.
Further, we eliminate all information on case and punctuation, since 460 Zechner Automatic Summarization of Dialogues we emulate the ASR output in that regard, which does not provide this information.
Contractions such as don't or I'll are divided and treated as separate words--in these examples we would obtain do n't and I 'll.
5.3 Disfluency
Detection 5.3.1 Motivation.
Conversational, informal spoken language is quite different from written language in that a speaker's utterances are typically much less well-formed than a writer's sentences.
We can observe a set of disfluencies such as false starts, hesitations, repetitions, filled pauses, and interruptions.
Additionally, in speech there is no good match between linguistically motivated sentence boundaries and turn boundaries or recognition hypotheses from automatic speech recognition.
5.3.2 Types
of Disfluencies.
The classification of disfluencies in this work follows Shriberg (1994), Meteer et al.(1995), and Rose (1998).
It is worth noting, however, that any disfluency classification will be only an approximation of the assumed real phenomena and that often boundaries between different classes are fuzzy and hard to decide for human annotators (cf.
Meteer et al.[1995] on annotators' problems with the classification of the word so).
 Filled pauses: We follow Rose's (1998) classification of nonlexicalized filled pauses (typically uh, um) and lexicalized filled pauses (e.g., like, you know).
Whereas the former are usually nonambiguous and hence easy to detect, the latter are ambiguous and much harder to detect accurately.
Restarts or repairs: These are fragments that are resumed, but without completely abandoning the first attempt.
We follow the notation in Meteer et al.(1995) and Shriberg (1994), which has these parts: (1) reparandum, (2) interruption point (+), (3) interregnum (editing phase, {.
. . }), and (4) repair.
    Repetition: A restart with a verbatim repetition of a word or a sequence of words: [ she is + she is ] happy.
Insertion: A repetition of the reparandum, with some word(s) inserted: [ she liked + {um} she really liked ] it.
Substitution: The reparandum is not repeated: [ she + {uh} my wife ] liked it.
False starts: These are abandoned, incomplete clauses.
In some cases, they may occur at the end of an utterance, and they can be due to interruption by another speaker.
Example: so we didn't--they have not accepted our proposal.
5.3.3 Related
Work.
The past decade has produced a substantial amount of research in the area of detecting intonational and linguistic boundaries in conversational speech, as well as in the area of detecting and correcting speech disfluencies.
Whereas earlier work tended to look at these phenomena in isolation (Nakatani and Hirschberg 1994; Stolcke and Shriberg 1996), more recent work has attempted to solve several tasks within one framework (Heeman and Allen 1999; Stolcke et al.1998). Most approaches use some kind of prosodic information, such as duration of pauses, stress, and pitch contours, and most of them combine this prosodic information with information about word identity and sequence (n-grams, hidden Markov 461 Computational Linguistics Volume 28, Number 4 models).
In the study of Stolcke et al.(1998), the goal was to detect sentence boundaries and a variety of speech disfluencies on a large portion of the SWITCHBOARD corpus.
An explicit comparison was made between prosodic and word-based models, and the results showed that an n-gram model, enhanced with segmental information about turn boundaries, significantly outperformed the prosodic model.
Model combination improved the overall results, but only to a small extent.
In more recent research, Shriberg et al.(2000) reported that for sentence boundary detection in two different corpora (BROADCAST NEWS and SWITCHBOARD), prosodic models outperform word-based language models and a model combination yields additional performance gains.
5.3.4 Overview.
In the following, we will discuss the three components of the DIASUMM system that perform disfluency detection:  a POS tagger that tags, in addition to the standard SWITCHBOARD Treebank-3 tag set (LDC 1999b), the following disfluent regions or words: 1.
coordinating conjunctions that don't serve their usual connective role, but act more as links between subsequent speech acts of a speaker (e.g., and then; we call these empty coordinating conjunctions in this work) lexicalized filled pauses (labeled as discourse markers in the Treebank-3 corpus; e.g., you know, like) editing terms within speech repairs (e.g., I mean) nonlexicalized filled pauses (e.g., um, uh) a decision tree (supported by a shallow chunk parser) that decides whether to label a particular sentence as a false start a repetition detection script (for repeated sequences of up to four words) 5.3.5 Training Corpus.
For training, we used a part of the SWITCHBOARD transcripts that was manually annotated for sentence boundaries, POS, and the following types of disfluent regions (LDC 1999b):       {A.
. . }: asides (very rare; we ignore them in our experiments) {C.
. . }: empty coordinating conjunctions (e.g., and then) {D.
. . }: discourse markers (i.e., lexicalized filled pauses in our terminology, e.g., you know) {E.
. . }: editing terms (within repairs; e.g., I mean) {F.
. . }: filled pauses (nonlexicalized; e.g., uh) [.
. . + . . .]: repairs: the part before the + is called reparandum (to be removed), the part after the + repair (proper) Sentence boundaries can be at the end of completed sentences (E S) or of noncompleted sentences, such as false starts or abandoned clauses (N S).
462 Zechner Automatic Summarization of Dialogues Table 5 Precision, recall and F1 -scores of the four disfluency tag categories for the SWITCHBOARD test set.
Description Empty coordinating conjunctions Lexicalized filled pauses Editing terms Nonlexicalized filled pauses Count 5,990 5,787 1,004 12,926 Tag CO DM ET UH Precision 0.84 0.95 0.98 0.98 Recall 0.93 0.90 0.94 0.98 F1 0.88 0.93 0.96 0.98 Table 6 POS tagging accuracy on five subcorpora (evaluated on 500-word samples).
8E-CH Known words Unknown words (total) Overall 92.8 48.0 (25) 90.6 4E-CH 90.6 44.4 (9) 89.8 NHOUR 92.7 69.6 (23) 91.6 XFIRE 90.6 86.4 (22) 90.4 G-MTG 93.2 92.6 (27) 93.2 5.3.6 POS Tagger.
We are using Brill's rule-based POS tagger (Brill 1994).
Its basic algorithm at run time (after training) can be described as follows: 1.
2. Tag every word with its most likely tag, predicting tags of unknown words based on rules.
Change every tag according to its right and left context (both words and tags are considered), following a list of rules.
For preprocessing, we replaced the tags in the regions of {C.
. . }, {D.
. . }, and {E.
. . } with the tags CO (coordinating), DM (discourse marker), and ET (editing term), respectively.
(The filler regions {F.
. . } are already tagged with UH in the corpus).
Lines that contain typographical errors were excluded from the training corpus.
We further eliminated all incomplete words (XX tag) and combined multiwords, marked by a GW tag, into a single word (hence eliminating the GW tag).11 The entire resulting new tag set had 42 tags.12 Training of the POS tagger proceeded in three stages, using about 250,000 tagged words for each stage.
The trained POS tagger's performance on an unseen test set of about 185,000 words is 94.1% tag accuracy (untrained baseline: 84.8% accuracy).
Table 5 shows precision, recall, and F1 -scores for the four categories of disfluency tags, measured on the test set after the last training phase.
We see that the nonlexicalized filler words are almost perfectly tagged (F1 = 0.98), whereas the hardest task for the tagger is the empty coordinating conjunctions (F1 = 0.88): There are a few highly ambiguous words in that set, such as and, so, and or.
Table 6 shows the POS tagging accuracy on the five subcorpora of our dialogue corpus, evaluated on a sample of 500 words per subcorpus.
We see that the POStagging accuracy is slightly lower than for the SWITCHBOARD set that was used for 11 The sole function of the GW tag is to label words that are considered to be parts of other words but were transcribed separately, such as: drug/GW testing/NN.
12 For
a description of the POS tags used in that database see Santorini (1990) and LDC (1999a).
Computational Linguistics Volume 28, Number 4 Table 7 Disfluency tag detection (F1 ) for five subcorpora (results in parentheses: less than 10 tags to be detected).
8E-CH CO DM ET UH .89 .93 .95 .56 4E-CH .89 .73 .95 .62 NHOUR .38 .90 (.94) (.14) XFIRE .77 .82 .85 (.28) G-MTG .54 .30 .88 .45 training (approximately 9093%; global average: 91.1%).
Further we observe that with the exception of the CALLHOME corpora, the majority of unknown words were actually tagged correctly.
The most frequent errors were (1) conjunctions tagged as empty coordinated conjunctions, (2) proper names tagged as regular nouns, and (3) adverbs tagged as adjectives.
Finally, we look at the POS tagger's performance for the four disfluency tags CO, DM, ET, and UH in our five subcorpora; the results of this evaluation are presented in Table 7.
We can see that the detection accuracy is generally lower than for the corpus on which we trained the tagger (SWITCHBOARD), but still quite good in general.
The major exceptions are the UH tags, on which the F1 -scores are comparatively low for all subcorpora.
The reason for this can be found mostly in words like yes, no, uh-huh, right, okay, and yeah, which are often tagged with UH in SWITCHBOARD but frequently are not considered to be irrelevant words in our corpus and hence not marked as disfluent (e.g., if they are considered to be the answer to a question or a summary-relevant acknowledgment).
We circumvent potential exclusion from the summary output of these and other words that might be erroneously tagged as nonlexicalized filled pauses (UH) by marking a small set of words as exempt from removal (see section 5.5.6). 5.3.7 False Start Detection.
False starts are quite frequent in spontaneous speech, occurring at a rate of about 1015% of all sentences (SWITCHBOARD, CALLHOME).
They involve less than 10% of the total words of a dialogue; about 34% of the words in these incomplete sentences are part of some other disfluencies, such as filled pauses or repairs.
(In complete sentences, only about 15% of the words are part of these disfluencies).
For CALLHOME, the average length of complete sentences is about 6 words, of incomplete sentences about 4.1 words (including disfluencies).
We trained a C4.5 decision tree (Quinlan 1992) on 8,000 sentences of SWITCHBOARD.
As features we use the first and last four trigger words (words that have a high incidence around sentence boundaries) and POS of every sentence, as well as the first and last four chunks from a POS-based chunk parser.
This chunk parser is based on a simple context-free POS grammar for English.
It outputs a phrasal bracketing of the input string (e.g., noun phrases or prepositional phrases).
Further, we encode the length of the sentence in words and the number of the words not parsed by the chunk parser.
We observed that whereas the chunk information itself does not improve performance over the baseline of using trigger words and POS information only, the derived feature of "number of not parsed words" actually does improve the results.
We ran the decision tree on data with perfect POS tags (for SWITCHBOARD only), disfluency tags (except for repairs), and sentence boundaries.
The evaluations were performed on independent test sets of about 3,000 sentences for SWITCHBOARD and of our complete dialogue corpus.
Table 8 shows the results of these experiments.
Typical errors, where complete sentences were classified as incomplete, are inverted forms or 464 Zechner Automatic Summarization of Dialogues Table 8 False start classification results for different corpora (F1 ).
SWBD False start frequency (in %) False start detection (F1 ) 12.3 .611 8E-CH 12.1 .545 4E-CH 11.0 .640 NHOUR 2.0 .286 XFIRE 7.2 .352 G-MTG 13.9 .557 Table 9 Detection accuracy for repairs on the basis of individual word tokens using the repetition filter.
8E-CH Repair tokens (%) Precision Recall F1 -score 4.7 .88 .41 .56 4E-CH 3.8 .78 .32 .45 NHOUR 2.2 .25 .01 .02 XFIRE 1.3 .35 .04 .08 G-MTG 7.9 .91 .27 .41 ellipsis at the end of a sentence (e.g., neither do I, it seems to).
The performance for the informal corpora (CALLHOME, GROUP MEETINGS) is better than that for the formal corpora (NEWSHOUR, CROSSFIRE); this is related to the fact that the relative frequency of false starts is markedly lower in these latter data sets and that these corpora are more dissimilar to the training corpus (SWITCHBOARD).
5.3.8 Repetition
Detection.
The repetition detection component is concerned with (verbatim) repetitions within a speaker's turn, the most frequently occurring case of all speech repairs for informal dialogues (insertions and substitutions are comparatively less frequent).
Repeated phrases can potentially be interrupted by other disfluencies, such as filled pauses or editing terms.
Repetition detection is performed with a script that can identify repetitions of word/POS sequences of length one to four (longer repetitions are extremely rare: on average, less than 1% of all repetitions).
Words that have been marked as disfluent by the POS tagger are ignored when the repeated sequences are considered, so we can correctly detect repetitions such as [ he said uh to + he said to ] him.
. . . We are evaluating the precision, recall, and F1 -scores for this component at the level of individual words when the POS tagger and the sentence boundary detection component are used.
Table 9 shows the results.
We see that for the informal subcorpora, we get very good precision (only a few repetitions detected are incorrect), and recall is in the 2545% range (since we cannot detect substitution or insertion type of repairs).
The results for the formal subcorpora are considerably worse, so this filter should probably not be used for corpora with as few repetitions as NEWSHOUR or CROSSFIRE.
We checked all of the 95 false positives of this evaluation and observed that in the majority of cases (41%), the repetition was correctly detected but was not marked by the human annotator, since it might be considered a case of emphasis.
We believe that although some nuances of the sentence(s) might be lost, for the purpose of summarization it makes perfect sense to reduce this information.
Sometimes, individual words are repeated for emphasis, sometimes whole sentences (e.g., "Good./ Good./").
In the following example from English CALLHOME, the emphasis is rather extreme: 203 B: [...] How is the new person doing? q/ 204 A: Very very very very very well.
/ [...] Computational Linguistics Volume 28, Number 4 Further, about 19% of false positives were correct but not annotated because they span multiple turns, and about 14% were erroneously missed by the human annotator.
Only the remaining cases (26%) were actual false positives, caused by incorrect POS tags (5%, typically an incorrectly tagged "that/WDT that/DT" sequence at the beginning of a relative clause) or incorrect sentence boundaries (21%).
There have been attempts to get a more complete coverage of detection and correction of all types of speech repairs (Heeman and Allen 1999).
We decided, however, to use a simple method here that works well for a large subset of cases and is very efficient at the same time.
5.3.9 Disfluency
Correction in DIASUMM.
After detection, the correction of disfluencies is straightforward.
When DIASUMM generates its output from the ranked list of sentences, it skips the false starts, the repetitions, and the words that were tagged with CO, DM, ET, or UH by the POS tagger.
5.4 Sentence
Boundary Detection 5.4.1 Introduction.
The purpose of the sentence boundary detection component is to insert linguistically meaningful sentence boundaries in the text, given a POS-tagged input.
We consider all intraturn and interturn boundary positions for every speaker in a conversation.
We use the abbreviations EOS for end of complete sentence (E S in the SWITCHBOARD corpus) and NEOS for end of noncomplete sentence (N S in the SWITCHBOARD corpus).
The frequency of sentence boundaries (with respect to the total number of words) is about 13.3%, most of the boundaries (almost 90%) being end markers of completed sentences (SWITCHBOARD).
5.4.2 Training
and Testing.
We trained a C4.5 decision tree and computed its input features from a context of four words before and after a potential sentence boundary, motivated by the results of Gavald`, Zechner, and Aist (1997).
Also following Gavald`, a a Zechner, and Aist (1997), we used 60 trigger words with high predictive potential, employing the score computation method described in this article.
The decision tree input features for every word position are as follows:     POS tag (42 different tags) trigger word (60 different trigger words) turn boundary before this word? if turn boundary: length of pause after last turn of same speaker Since NEOS boundaries occur very infrequently (only about 10% of all boundaries, which is only about 1% of all potential boundaries), we decided to merge this class with the EOS class and report results for this combined class only (CEOS).
(We relied on the false-start detection module described above to identify the NEOS sentences within this merged class of sentences after the sentence boundary classification).
For training, we used 25,000 words from the Treebank-3 corpus; the test set size was 1,000 words.
Table 10 shows the results in detail for the various parameter combinations.
We see that for good performance we need to know about one of these two features: "is there a turn boundary before this word"? or "pause duration after last turn from same speaker".
466 Zechner Automatic Summarization of Dialogues Table 10 Sentence boundary detection accuracy (F1 -score).
With Interturn Pause Duration?
With Turn Boundary Info?
Training set Test set Yes Yes .904 .887 No .903 .884 Yes .900 .884 No No .884 .825 Table 11 Interand intraturn boundary detection (BD) results on 1,000-word test set.
Occurrence (%) Interturn Interturn Intraturn Intraturn non-BD BD non-BD BD 12 112 809 61 (1.2) (11.3) (81.4) (6.1) Detection Accuracy (F1 ) .56 .95 .99 .77 5.4.3 Effect of Imperfect POS Tagging.
To see how much influence an imperfect POS tagging might have on these results, we POS-tagged the test set data using the POS tagger described above.
For this and the following experiments, we increased the training corpus for the decision tree to 40,000 words.
The POS tagger accuracy for this test set was about 95.3%, and the F1 -score for CEOS was .882, which is 98.9% of .892 on perfect POS-tagged input.
This is encouraging, since it shows that the decision tree is not very sensitive to the majority of POS errors.
5.4.4 Interturn
and Intraturn Boundaries.
In this analysis, we are interested in comparing the detection of sentence boundaries between turns (interturn) to the detection of boundaries within a turn (intraturn).
Table 11 shows the results of this analysis (same test set as above).
As might be expected, the performance is very good for the two frequent classes: sentence boundaries at the end of turns and nonboundaries within turns (F1 > .95), but considerably worse for the two more infrequent cases.
The very rare cases (around 1% only) of nonsentence boundaries at the end of turns (i.e.
turn continuations) show the lowest performance (F1 = .56).
5.4.5 Sentence
Boundary Detection on Dialogue Corpus.
To get a picture of the realistic performance of the sentence boundary detection component, using the (imperfect) POS tagger and a faster, but slightly less accurate, decision tree,13 we evaluate the sentence boundary detection accuracy for all five subcorpora of our dialogue corpus.
Table 12 provides the results of these experiments.
The results reflect a trend very similar to that for the SWITCHBOARD corpus, in that the two more frequent classes (interturn boundaries and intraturn nonboundaries) have high detection scores, whereas the two more infrequent classes are less well detected.
Furthermore, we observe that in cases in which the relative frequency of rare classes is further reduced, the classification accuracy declines overproportionally (particularly for the rarest class of the interturn nonboundaries).
Also, overall boundary detection is better for the two more informal corpora, CALLHOME and GROUP MEETINGS (F1 > .72).
13 This
decision tree uses a different type of encoding, but the same input features.
Computational Linguistics Volume 28, Number 4 5.5 Cross-Speaker Information Linking 5.5.1 Introduction.
One of the properties of multiparty dialogues is that shared information is created between dialogue participants.
The most obvious interactions of this kind are question-answer (Q-A) pairs.
The purpose of this component is to create automatically such coherent pieces of relevant information, which can then be extracted together while generating the summary.
The effects of such linkings on actual summaries can be seen in two dimensions: (1) increased local coherence in the summary and (2) a potentially higher informativeness of the summary.
Since Q-A linking has a side effect in that other information will be lost with respect to a summary of the same length without Q-A linking, the second claim is much less certain to hold than the first.
We investigated these questions in related work (Zechner and Lavie 2001) and found that although Q-A linking does not significantly change the informativeness of summaries on average, it does increase summary coherence (fluency) significantly.
In this section, we will be concerned with the following two intuitive subtasks of Q-A linking: (1) identifying questions (Qs) and (2) finding their corresponding answers.
5.5.2 Related
Work.
Detecting a question and its corresponding answer can be seen as a subtask of the speech act detection and classification task.
Recently, Stolcke et al.(2000) presented a comprehensive approach to dialogue act modeling with statistical techniques.
A good overview and comparison of recent related work can also be found in Stolcke et al.'s article.
Results from their evaluations on SWITCHBOARD data show that word-based speech act classifiers usually perform better than prosody-based classifiers, but that a model combination of the two approaches can yield an improvement in classification accuracy.
5.5.3 Corpus
Statistics.
For training of the question detection module, we used the manually annotated set of about 200,000 SWITCHBOARD speech acts14 (SAs);15 for training of the answer detection component, we used the eight English CALLHOME dialogues (8E-CH), which were manually annotated for Q-A pairs.
Although we were aiming to detect all questions in the question detection module, the answer detection module focuses on Q-A pairs only: We exclude all questions from consideration that are not Yes-No(YN) or Wh-questions (such as rhetorical or back-channel questions), 14 In this work, the notions of speech acts and sentences can be considered equivalent.
15 From
the Johns Hopkins University Large Vocabulary Continuous Speech Recognition (LVCSR) Summer Workshop 1997.
Thanks to Klaus Ries for providing the data, which are also available from http://www.colorado.edu/ling/jurafsky/ws97/.
Zechner Automatic Summarization of Dialogues Table 13 Frequency of different types of questions in the 8E-CH data set.
Sentences Wh-questions total . . . With immediate answers YN-questions total . . . With immediate answers Qs excluded for Q-A detection Questions total 2,211 20 15 (75%) 48 38 (79%) 15 83 (3.75%) as well as those that do not have an answer in the dialogue.
Thus we employ only 68 pf the 83 questions marked in the 8E-CH data set for these evaluations.
Table 13 provides the statistics concerning questions and answers for the 8E-CH subcorpus and shows that for a small but significant number of questions, the answer does not immediately follow the question speech act (delayed answers).
5.5.4 Automatic
Question Detection.
We used two different methods, both trained on SWITCHBOARD data: (1) a speech act tagger16 and (2) a decision tree based on trigger word and part-of-speech information.
Speech act tagger.
The speech act tagger tags one speech act at a time and hence can make use only of speech act unigram information.
Within a speech act, it uses a language model based on POS and the 500 most frequent word/POS pairs.
It was trained on the aforementioned SWITCHBOARD speech act training set.
It was not optimized for the task of question detection.
Its typical run time for speech act classification is about 10 speech acts per second.
Decision tree question classifier.
The decision tree classifier (C4.5) uses the following set of features: (1) POS and trigger word information for the first and last five tokens of each speech act;17 (2) speech act length, and (3) occurrence of POS bigrams.
The set of trigger words is the same as for the sentence boundary detection module.
The POS bigrams were designed to be most discriminative between question speech acts (q-SAs) and nonquestion speech acts (non-q-SAs).
The bigrams were obtained as follows: 1.
For a balanced set of q-SAs and non-q-SAs (about 9,000 SAs each): Count all the POS bigrams in positions 1 . . . 5 and (n 4) . . . n (using START and END for the first and last bigrams, respectively) and memorize position (beginning or end of SA) and type (q-SA vs.
non-q-SA). For all bigrams: (a) (b) (c) (d) 3.
Add one to the count (to prevent division by zero).
Divide the q-SA count by the non-q-SA count.
If the ratio is smaller than one, invert it (ratio := 1/ratio).
Multiply the result of (c) by the sum of q-SA count and non-q-SA count.18 Extract the 100 bigrams with the highest value.
16 Thanks
to Klaus Ries for providing us with the software.
17 Shorter
speech acts are padded with dummies.
18 Leaving
out this step favors low-frequency, high-discriminative bigrams too much and causes a slight reduction in overall Q-detection performance.
Computational Linguistics Volume 28, Number 4 Table 14 Question detection on the 8E-CH corpus using two different classifiers.
SA Tagger Overall error Precision Recall F1 Typical classification time (SAs/sec) 3.2% .57 .61 .59 10 Decision Tree 4.7% .63 .51 .56 1,000 Experiments and results.
The question detection decision tree was trained on a set of about 20,000 speech acts from the SWITCHBOARD corpus.
We first evaluated the speech act tagger and the decision tree classifier on the 8E-CH data set.
Whereas in the later stage of answer detection, questions without answers and nonpropositional questions are ignored, at this point we are interested in the detection of all annotated questions in the corpus.
This also reflects the fact that the training set contains all possible types of questions.
Table 14 reports the results of the question detection experiments with the two classifiers used on the 8E-CH subcorpus.
We note that whereas the decision tree is performing only slightly worse than the speech act tagger, its typical classification time is two orders of magnitude faster.
Based on these observations, we decided to use the question detection decision tree in the Q-A linking component of the DIASUMM system.
5.5.5 Detecting
the Answers.
After identifying which sentences are questions, the next step is to identify the answers to them.
From the 8E-CH statistics of Table 13 we observe that for more than 75% of the YNand Wh-questions, the answer is to be found in the first sentence of the speaker talking after the speaker uttering the question.
In the remainder of cases, the majority of answers are in the second (instead of the first) sentence of the responding speaker.
Further, the speaker who has posed a question usually utters no (or only very few) sentences after the question is asked and before the next speaker starts talking.
In addition to detecting sequential Q-A pairs, we also want to be able to detect simple embedded questions, as shown in this example of a brief clarification dialogue: Q Q 1 2 3 4 A: B: A: B: When are we meeting then?
You mean tomorrow?
Yes. At 4pm.
We devise the following heuristics to detect answers to question speech acts which have been previously identified:  If the first speaker change after the question occurs more than maxChg sentences after the question, the search is stopped and no Q-A pair is returned.
Answer hypotheses are sought for maximally maxSeek sentences after the first speaker change following the question, but not over interruptions by any other speaker; that is, we check within a single speaker region Zechner Automatic Summarization of Dialogues (this is the stopping criterion for the following two heuristics).
An exception occurs if there is an embedded question in the first single speaker region: In that case, we look at the next region where a speaker different from the initial Q-speaker is active.19   Answers have to be minimally minAns words long; if they are shorter, we add the next sentence to the current answer hypothesis.
Even if the minimum answer length is reached, the answer can be optionally extended if at least one word in the answer matches a word from the question (one of two different stop lists (StopShort, StopLong) or no stop list is used to remove function words from consideration).20 We have these further restrictions for the case of embedded questions: 1.
If we detect a potential embedded Q-A pair, the answer to the surrounding question must immediately follow the answer to the embedded question (i.e., the region following the potential answer region of the embedded question--sentence 4 in our above example--must (1) not contain a question itself and (2) be from a different speaker than the surrounding question).
A crossover is prohibited; that is, we eliminate all pairs Qj, Al when a pair Qi, Ak was already detected, with i < j < k < l (k, l being start indices of answer spans).
The output of the algorithm is a list of triples Q, Astart, Aend, where Q is the sentence ID of the question and Astart the first and Aend the last sentence of the answer.
As mentioned above, we use only 68 of the 83 questions marked in the 8ECH data set for these evaluations, since only these are YNor Wh-questions that actually have answers in the dialogue.
There are four possible outcomes for each triple: (1) irrelevant: a Q-A pair with an incorrectly hypothesized question (this is the fault of the question detection module, not of this heuristic); (2) missed: the answer was missed entirely; (3) completely correct: Aend coincides with the correct answer sentence ID; and (4) correct range: the answer is contained in the interval [Astart, Aend ] but does not coincide with Aend . For the calculation of precision, recall, and F1 -score, we count classes (3) and (4) as correct and use the sum of all classes for the denominator of precision and the total number of Q-A pairs (68 in this development set) as the denominator of recall.
To determine the best parameters, we varied them across a reasonable set of values and ran the answer detection script for all combinations of parameters.
The best results (with respect to F1 -score) using questions detected by the speech act tagger and the decision tree are reported in Table 15.
In the DIASUMM system, we use the following optimal parameter settings for the answer detection heuristics: maxChg = 2, maxSeek = 4, minAns = 10, sim = on, stop = no.
Finally, we evaluated the performance of both the Q-detection module and the combined Q-A detection on all five subcorpora, using the decision tree for question detection; the results are reported in Table 16.
Except for the rather small NEWSHOUR 19 This would be sentence 4 in the example above.
20 StopLong contains 571 words, StopShort only 89 words, most of which are auxiliary verbs and filler words.
Computational Linguistics Volume 28, Number 4 Table 15 Q-A detection results using two different classifiers for question detection (68 Q-A pairs to be detected).
SA Tagger All hypothesized Q-A pairs Correct [(3) and (4)] maxChg (15) maxSeek (24) minAns (110) Similarity extension (on/off) Stop list (no/short/long) Precision Recall F1 -score 80 42 4 34 510 on no/short .53 .62 .57 Decision Tree 54 31 2 24 210 on no/short .57 .46 .51 Table 16 Performance comparison for Qand Q-A detection (Q-detection with the decision tree question classifier).
8E-CH Q to detect Q-hypotheses Q-detection (F1 ) Q-A pairs to detect Q-A pair hypotheses Q-A detection (F1 ) 83 67 .56 68 54 .51 4E-CH 94 60 .58 69 54 .60 NHOUR 19 16 .80 18 14 .81 XFIRE 110 71 .60 79 54 .51 G-MTG 49 52 .59 32 33 .51 corpus (with fewer than 20 questions or Q-A pairs to identify), the typical Q-detection F1 -score is around .6 and the Q-A detection F1 -score around .5.
In two cases, the Q-A detection performance is slightly better than the Q-detection performance.
This can be explained by the fact that the answer detection algorithm prunes away a number of Q-hypotheses, reducing the space for potential Q-A hypotheses.
5.5.6 Q-A Detection within DIASUMM.
When we use the Q-A detection module as part of the DIASUMM system, we want to ensure that (1) there are no Q-A pairs containing Q-sentences that are false starts and that (2) the initial part of an answer is not lost in case the disfluency detection component marks some indicative words as disfluencies.
To satisfy the first constraint, we block Q-detection of sentences that have been previously classified as false starts; as for the second constraint, we create a list of indicative words (relevant for YN-questions) that are not to be removed by the summary generator if they appear in the beginning (leading five words) of answers.21 5.6 Sentence Ranking and Selection 5.6.1 Introduction.
The sentence ranking and selection component is an implementation of the MMR algorithm (Carbonell, Geng, and Goldstein 1997), applied to extracting the most relevant sentences from a topical segment of a dialogue.
The component's output in isolation serves as the MMR baseline for the global system evaluation.
Its 21 The current list comprises the following words: no, yes, yeah, yep, sure, uh-huh, mhm, nope.
Zechner Automatic Summarization of Dialogues purpose is to determine weights for terms and sentences, to rank the sentences according to their relevance within each topical segment of the dialogue, and finally to select the sentences for the summary output according to their rank, as well as to other criteria, such as question-answer linkages, established by previous components.
The selected sentences are presented to the user in text order.
5.6.2 Tokenization.
In addition to the tokenization rules for the global system (section 5.2), we apply a simple six-character truncation for stemming and use a stop word list to eliminate frequent noncontent words.
In the experiments, we used the following five different stop word lists:      the original SMART list (Salton 1971) (SMART-O) a manually edited stop list based on SMART (SMART-M) a stop list with all closed-class words from the POS tagger's lexicon (POS-O) a manually edited stop list based on the POS tagger's lexicon and frequent closed-class words in the CALLHOME training corpus (POS-M) an empty stop list (EMPTY) 5.6.3 Term and Sentence Weighting.
The basic idea for determining the most relevant sentences within a topical segment is as follows: First, we compute a vector of word weights for the segment tfq (including all stemmed nonstop words) and do the same for each sentence (tft ), then we compute the similarity between sentence and segment vectors for each sentence.
That way, sentences that have many words in common with the segment vector are rewarded and receive a higher relevance weight.
Whereas we compose the sentence vectors tft using direct term frequency counts, the weights for segment terms are determined according to one of the three formulae in equation (1) (freq, smax, and log), inspired by Cornell University's SMART system (Salton 1971): fi,s or 1 + log fi,s, (1) tfi,s = fi,s or 0.5 + 0.5 fsmax where fi,s are the in-segment frequencies of a stem and fsmax are maximal segment frequencies of any stem in the segment.
Finally, we multiply an inverse document frequency (IDF) weight to tfs to obtain the segment vectors tfq, as shown in equations (2) and (3): tfi,q IDFi,s = tfi,s IDFi,s = 1 + log Nseg iseg or Nseg . iseg (2) (3) IDF values are computed with respect to a collection of topical segments, either the current dialogue (DIALOGUE) or a set of dialogues (CORPUS).
Nseg is the total number of topical segments in the IDF corpus, and iseg is the number of segments in which the token i appears at least once.
The effect of using IDF values is to boost those words that are (relatively) unique to any given segment over those that are more evenly distributed across the corpus.
As stated above, the main algorithm is a version of the MMR algorithm (Carbonell, Geng, and Goldstein 1997; Carbonell and Goldstein 1998), which emphasizes sentences 473 Computational Linguistics Volume 28, Number 4 that contain many highly weighted terms for the current segment (salience) and are sufficiently dissimilar to previously ranked sentences (diversity or antiredundancy).
The MMR formula is given in equation (4): nextsentence = arg max(sim 1 (query, tnr,j ) (1 ) max sim 2 (tnr,j, tr,k )).
The MMR formula describes an iterative algorithm and states that the next sentence to be put in the ranked list will be taken from the sentences that have not yet been ranked (tnr ).
This sentence is (1) maximally similar to a query and (2) maximally dissimilar to the sentences that have already been ranked (tr ).
We use the topical segment word vector tfq as query vector.
The parameter (0.0 1.0) is used to trade off the influence of salience against that of redundancy.
Both similarity metrics (sim 1, sim 2 ) are inner vector products of stemmed-term frequencies (equations (5) and (6)).
sim 1 can be normalized in different ways: (1) to yield a cosine vector product (division by product of vector lengths), (2) division by number of content words,22 and (3) no normalization: sim 1 = tfq tft |tfq ||tft | tft1 tft2 |tft1 ||tft2 | or tfq tft 1 + i tfi,t or tfq tft (5) Emphasis factors.
Every sentence's similarity weight (sim 1 ) can be (de)emphasized, based on a number of its properties.
We implemented optional emphasis factors for:     Lead emphasis: for the leading n% of a segment's sentences: sim 1 = sim 1 l, with l being the lead factor.
Q-A emphasis: for all sentences that belong to a question-answer pair: sim 1 = sim 1 q, with q being the Q-A emphasis factor.
False-start deemphasis: for all sentences that are false starts: sim 1 = sim 1 f, with f being the false-start factor.
Speaker emphasis: for each individual speaker s, an emphasis factor se can be defined: sim 1 = sim 1 se for all sentences of speaker s.23 These parameters can serve to fine-tune the system for particular applications or user preferences.
For example, if the false starts are deemphasized, they are less likely to trigger a question's being linked to them in the linking process.
If questions and answers are emphasized, more of them will show up in the summary, increasing its coherence and readability.
In a situation in which a particular speaker's statements are of higher interest than those of other speakers, his sentences can be emphasized, as well.
Since sim 2 is a cosine vector product and hence in [0,1], we have to normalize sim 1 to [0,1] as well to enable a proper application of the MMR formula.
For this normalization of sim 1, we divide each sim 1 score by the maximum of all sim 1 scores in a segment after initial computation and application of the various emphasis factors described here.
22 To avoid division by zero, we add one to every sentence length.
23 Speaker emphasis is not used in our evaluations.
Zechner Automatic Summarization of Dialogues 5.6.4 Q-A Linking.
While generating the output summary from the MMR-ranked list of sentences, whenever a question or an answer is encountered (detected previously by the Q-A detection module), the corresponding answer/question is linked to it and moved up the relevance ranking list to immediately follow the current question/answer.
If the question-answer pair consists of more than two sentences, the linkages are repeated until no further questions or answers can be added to the current linkage cluster.
5.6.5 Summary
Types.
DIASUMM can generate several different types of summaries, the two main versions being (1) the CLEAN summary, which is based on the output of all DIASUMM components (disfluency detection, sentence boundary detection, Q-A linking), and (2) the TRANS summary, in which all dialogue specific components are ignored (essentially, this is an MMR summary of the original dialogue transcript).
For the purpose of the global system evaluation, we use only these two versions of summaries, as well as LEAD baseline summaries, where the summary is formed by extracting the first n words from a topical segment.24 Furthermore, the system can generate phrasal summaries, which render the sentences in the same ranking order as the CLEAN summary but reduce the output to noun phrases and potentially other phrases, depending on the setting of parameters.25 In Figure 2 we show an example of a set of LEAD, TRANS, CLEAN, and PHRASAL summaries.
The set was generated from the CALLHOME transcript presented in section 2.
5.6.6 System
Tuning.
This section describes how we arrive at an optimal parameter setting for each subcorpus (CALLHOME, NEWSHOUR, CROSSFIRE, GROUP MEETINGS).
We want to establish an MMR baseline for the global system evaluations with which we can then compare the results of the entire DIASUMM system.
Note that for all the tuning experiments described in this subsection, we did not make use of any other DIASUMM components, namely, disfluency detection, sentence boundary detection, and questionanswer linking.
All experiments were based on the human gold standard with respect to topical segments.
We used only the devtest set for the four subcorpora here (8ECH = CALLHOME, DT-NH = NEWSHOUR, DT-XF = CROSSFIRE, and DT-MTG = GROUP MEETINGS).
Since the length of turns varies widely, one could argue that an easy way to increase performance for the MMR baseline (which does not use automatic sentence boundary detection) might be to split overly long turns evenly into shorter chunks.
This was done by Valenza et al.(1999), who experimented with lengths of 1030 words per extract fragment.
We add this option as an additional parameter to the MMR baseline.
If the parameter is set to n words, turns with a length l 1.5n get cut into pieces of lengths n iteratively until the last remaining piece is l < 1.5n.
Evaluation metric.
To evaluate the performance of this component, we use the word-based evaluation metric described in section 6.2, which gives the highest scores to summaries containing words with the highest average relevance scores, as marked by human annotators.
We then average these scores over all topical segment summaries of a particular subcorpus.
24 Note that LEAD summaries are to be distinguished from summaries in which lead emphasis is used, as described above.
In the latter case, the segment-initial sentence weights are increased, whereas in the former case, we strictly extract the leading n words from a given segment.
25 To determine these constituents, we use the output of the chunk parser employed by the false start detection component.
Computational Linguistics Volume 28, Number 4 LEAD: 1 a: Oh 2 b: They didn't know he was going to get shot but it was at a peace rally so I mean it just worked out 3 b: I mean it was a good place for the poor guy to die I mean because it was you know right after the rally and everything was on film and everything [...] TRANS: 2 b: They didn't know he was going to get shot but it was at a peace rally so I mean it just worked out 3 b: I mean it was a good place for the poor guy to die I mean because it was you know right after the rally and everything was on film and everything 11 b: Him [...] CLEAN: 7 b: We just finished the thirty days mourning for him now it's everybody's still in shock it's terrible what's going on over here 31 b: What's the reaction in america really do people care [...] 34 a: Most I don't know what I mean like the jewish community a lot all of us were very upset PHRASAL: 4 b: it just worked ...
it was a good place for the poor guy to die ...
it was [...] 7 b: we just finished the thirty days mourning for him ...
it's ...
everybody's ...
in shock it's ...
going ...
31 b: 's the reaction in america ...
do people care ...
34 a: i don't know ...
mean like the jewish community a lot ...
of us were Note: The turn IDs are just indicating the relative position of the turns within the original text and do not always correspond to the turn numbers of the original or to the turn numbers of the other summaries.
The . . . marks the position in those sentences where the length threshold for a summary was reached.
Figure 2 Example summaries of 20% length: LEAD, TRANS, CLEAN and PHRASAL.
Parameter tuning.
The system tuning proceeded in three phases, in which we held the summary size constant to 15% and optimized the following set of parameters: 1.
2. 3.
4. 5.
476 Term weight type: freq, smax, log Normalization: cos, length, none IDF type: corpus, dialogue, none IDF method: log, mult Extract span: 1030 or original turn (orig) Zechner Automatic Summarization of Dialogues Table 17 Optimally tuned parameters for MMR baseline system (tuning on devtest set subcorpora).
8E-CH Term weight type Normalization IDF type IDF method Extract span MMRStop list Lead factor smax cos corpus log 20 0.85 SMART-M 1.0 DT-NH DT-XF DT-Mtg smax none corpus log orig 0.8 POS-M 1.0 smax cos corpus mult 25 1.0 POS-M 1.0 smax none corpus log orig 0.8 POS-M 2.0 MMR-: 0.81.0 Stop lists: SMART-O, SMART-M, POS-O, POS-M, EMPTY Lead factor: 1.05.0 (applied to first 20% of sentences) Table 17 shows the parameter settings that were determined to be optimal for the MMR baseline system (TRANS summaries).
5.7 System
Performance The majority of the system components are implemented in Perl5, except for the C4.5 decision tree (Quinlan 1992), the chunk parser (Ward 1991), and the POS tagger (Brill 1994), which were implemented in C by the respective authors.
We measured the system runtime on a 300 MHz Sun Ultra60 dual-processor workstation with 1 GB main memory, summarizing all 23 dialogue excerpts from our corpus.
The average runtime for the whole system, including all of its components except for the topic segmentation module, was 17.8 seconds, and for the sentence selection component alone 7.0 seconds (per-dialogue average).
The average ratio of system runtime to dialogue duration was 0.029 (2.9% of real speaking time).
6. Evaluation 6.1 Introduction Traditionally, summarization systems have been evaluated in two major ways: (1) intrinsically, measuring the amount of the core information preserved from the original text (Kupiec, Pedersen, and Chen 1995; Teufel and Moens 1997), and (2) extrinsically, measuring how much the summary can benefit in accomplishing another task (e.g., finding a document relevant to a query or classifying a document into a topical category) (Mani et al.1998). In this work, we focus on intrinsic evaluation exclusively.
That is, we want to assess how well the summaries preserve the essential information contained in the original texts.
As other studies have shown (Rath, Resnick, and Savage 1961; Marcu 1999), the level of agreement between human annotators about which passages to choose to form a good summary is usually quite low.
Our own findings, reported in section 4.2.4, support this in that the intercoder agreement, here measured on a word level, is rather low.
We decided to minimize the bias that would result from selecting either a particular human annotator, or even the manually created gold standard, as a reference 477 Computational Linguistics Volume 28, Number 4 for automatic evaluation; instead, we weigh all annotations from all human coders equally.
Intuitively, we want to reward summaries that contain a high number of words considered to be relevant by most annotators.
We formalize this notion in the following subsection.
6.2 Evaluation
Metric All evaluations are based on topically coherent segments from the dialogue excerpts of our corpus.
As mentioned before, the segment boundaries were chosen from the human gold standard for the purpose of the global system evaluation.
For each segment s, for each annotator a, and for each word position wi, we define a boolean word vector of annotations ws,a, each component ws,a,i being 1 if the word wi is part of a nucleus IU or a satellite IU for that annotator and segment, and 0 otherwise.
We then sum over all annotators' annotation vectors and normalize them by the number of annotators per segment (A) to obtain the average relevance vector for segment s, rs : A ws,a,i rs,i = a=1 . (7) A To obtain the summary accuracy score sas,n for any segment summary with length n, we multiply the boolean summary vector summs 26 by the average relevance vector rs, and then divide this product by the sum of the n highest scores within rs (maximum achievable score), rsorts being the vector rs sorted by relevance weight in descending order: summs rs sas,n = n (8) i=1 rsorts,i It is easy to see that the summary accuracy score always is in the interval [0.0, 1.0]. 6.3 Global System Evaluation Whereas section 5 was concerned with the design and evaluation of the individual system components, the goal here is to describe and analyze the quality of the global system, with all its components combined.
In this section, we compare our DIASUMM system with the MMR baseline system, which operates without any dialogue-specific components, and with the LEAD baseline.
We described the optimization and finetuning of the MMR system in subsection 5.6.6.
The second column of Table 18 presents the average relevance scores for this MMR baseline, averaged over the five summary sizes of 5%, 10%, 15%, 20%, and 25% length, for the four devtest set and the four eval set subcorpora; the first column of this table shows the results for the LEAD baseline.
We used the optimized baseline MMR parameters and varied only the emphasis parameters for (1) false starts, (2) lead factor, and (3) Q-A sentences, to optimize the CLEAN summaries further.
(Again, for this step, we used only the devtest subcorpora).
For each corpus in the devtest set, we determined the optimal parameter settings and report the corresponding results also for the eval set subcorpora.
Column 3 in Table 18 provides the results for this optimized DIASUMM system.
Further, in column 4, we provide the summary accuracy averages for the human gold standard (nucleus IUs only, fixed-length summaries).
Table 19 shows the best emphasis parameter combinations for the DIASUMM summaries used in these evaluations.
We determined the statistical differences between the DIASUMM system and the two baselines for the eval set, using the Wilcoxon rank sum test for each of the four 26 Definition: 1 if summs,i is contained in the summary, 0 otherwise.
Zechner Automatic Summarization of Dialogues Table 18 Average summary accuracy scores: devtest set and eval set subcorpora on optimized parameters, comparing LEAD, MMR baseline, DIASUMM, and the human gold standard.
Subcorpus 8E-CH DT-NH DT-XF DT-MTG EVAL-NH EVAL-XF EVAL-MTG Table 19 Best emphasis parameters for the DIASUMM system, trained on the devtest set.
Corpus CALLHOME NEWSHOUR CROSSFIRE GROUP MEETINGS False Start 0.5 0.5 0.5 0.5 Q-A 1.0 2.0 1.0 1.0 Lead Factor 2.0 1.0 1.0 3.0 Table 20 Average summary accuracy scores for different system configurations for the four different subcorpora.
Corpus 4E-CH EVAL-NH EVAL-XF EVAL-GMTG LEAD .438 .692 .378 .324 MMR .526 .526 .564 .449 DFF-ONLY .599 .551 .528 .488 SB-ONLY .547 .608 .525 .513 NO-QA DIASUMM .614 .506 .566 .583 subcorpora.
Comparisons were made for each of the five summary sizes within each topical segment.
For the CALLHOME and GROUP MEETINGS subcorpora, our DIASUMM system is significantly better than the MMR baseline (p < 0.01); for the two more formal subcorpora, NEWSHOUR and CROSSFIRE, the differences between the performance of the two systems are not significant.
Except for on the NEWSHOUR subcorpus, both the MMR baseline and the DIASUMM system perform significantly better than the LEAD baseline.
6.4 Discussion
Table 20 shows the average performance of the following six system configurations, averaged over all topical segments and all summary sizes (525% length summaries; in configurations 35 below, components used are in addition to the core MMR summarizer): 1.
2. LEAD: using the first n% of the words in a segment MMR: the MMR baseline (tuned; see above) 479 Computational Linguistics Volume 28, Number 4 DFF-ONLY: using the disfluency detection components (POS tagger, false-start detection, repetition detection), but no sentence boundary detection or question-answer linking SB-ONLY: using the sentence boundary detection module, but no other dialogue-specific modules NO-QA: a combination of DFF-ONLY and SB-ONLY (all preprocessing components used except for question-answer linking) DIASUMM: complete system with all components (all disfluency detection components, sentence boundary detection, and Q-A linking) We observe that in all subcorpora, except for CROSSFIRE, the addition of either the disfluency components or the sentence boundary component improves the summary accuracy over that of the MMR baseline.
As we would expect, given the much higher frequency of disfluencies in the two informal subcorpora (CALLHOME, GROUP MEETINGS), the relative performance increase of DFF-ONLY over the MMR baseline is much higher here (about 1015%) than for the two more formal subcorpora (5% and below).
Looking at the performance increase of SB-ONLY, we find marked improvements over the MMR baseline for those two subcorpora that use the true original turn boundaries in the MMR baseline: GROUP MEETINGS and NEWSHOUR (>10%); for the two other subcorpora, the improvement is below 5%.
Furthermore, the combination of the disfluency detection and sentence boundary detection components (NO-QA) improves the results over the configurations DFF-ONLY and SB-ONLY.
The situation is much less uniform when we add the question-answer detection component (this then corresponds to the full DIASUMM system): In the CROSSFIRE corpus, we have the largest performance increase (we also have the highest relative frequency of question speech acts here).
For the two informal corpora, the change is only minor; for NEWSHOUR, the performance decreases substantially.
We showed in Zechner and Lavie (2001), however, that in general, for dialogues with relatively frequent Q-A exchanges, the accuracy of a summary (informativeness) does not change significantly when the Q-A detection component is applied.
On the other hand, the (local) coherence of the summary does increase significantly, but we cannot measure this increase with the evaluation criterion of summary accuracy used here.
To conclude, we have shown that using dialogue-specific components, with the possible exception of the Q-A detection module, can help in creating more accurate summaries for more informal, casual, spontaneous dialogues.
When more formal conversations (which may even be partially scripted), containing relatively few disfluencies, are involved, either a simple LEAD method or a standard MMR summarizer will be much harder to improve upon.
7. Discussion and Directions for Future Work The problem of how to generate readable and concise summaries automatically for spoken dialogues of unrestricted domains involves many challenges that need to be addressed.
Some of the research issues are similar or identical to those faced in summarizing written texts (such as topic segmentation, determining the most salient/relevant information, anaphora resolution, summary evaluation), but other additional dimensions are added on top of this list, including speech disfluency detection, sentence boundary detection, cross-speaker information linking, and coping with imperfect speech recognition.
The line of argument of this article has been that whereas using a 480 Zechner Automatic Summarization of Dialogues traditional approach for written text summarization (such as the MMR-based sentence selection component within DIASUMM) may be a good starting point, addressing the dialogue-specific issues is key for obtaining better summaries for informal genres.
We decided to focus on the three problems of (1) speech disfluency detection, (2) sentence boundary detection, and (3) cross-speaker information linking and implemented trainable system components to address each of these issues.
Both the evaluations of the individual components of our spoken-dialogue summarization system and the global evaluations as well have shown that we can successfully make use of the SWITCHBOARD corpus (LDC 1999b) to train a system that works well on two other genres of informal dialogues, CALLHOME and GROUP MEETINGS.
We conjecture that the reasons why the DIASUMM system was not able to improve over the MMR baseline for the two other corpora, which are more formal, lies in their very nature of being of a quite different genre: the NEWSHOUR and CROSSFIRE corpora have longer turns and sentences, as well as fewer disfluencies.
We would also conjecture that their sentence structures are more complex than what we typically find in the other corpora of more colloquial, spontaneous conversations.
Future work will have to address the issue of whether the availability of training data for more formal dialogues (in size and annotation style comparable to the SWITCHBOARD corpus, though) could lead to an improvement in performance on those data sets, as well, or if even then a standard written-text-based summarizer would be hard to improve upon.
Given the complexity of the task, we had to make a number of simplifying assumptions, most notably about the input data for our system: We use perfect transcripts by humans instead of ASR transcripts, which, for these genres, typically show word error rates (WERs) ranging from 15% to 35%.
Previous related work (Valenza et al.1999; Zechner and Waibel 2000b) demonstrated that the actual WERs in summaries generated from ASR output are usually substantially lower than the full-ASR-transcript WER and can further be reduced by taking acoustically derived confidence scores into account.
We further did not explore the potential improvements of components as well as of the system overall when prosodic information such as stress and pitch is added as an input feature.
Past work in related fields (Shriberg et al.1998; Shriberg et al.2000) suggests that particularly for ASR input, noticeable improvements might be achievable when such input is provided.
Although presegmentation of the input into topically coherent segments certainly is a useful step in summarization for any kind of texts (written or spoken), we have not addressed and discussed this issue in this article.
Finally, we think that there is more work needed in the area of automatically deriving discourse structures for spoken dialogues in unrestricted domains, even if the text spans covered might be only local (because of a lack of global discourse plans).
We believe that a summarizer, in addition to knowing about the interactively constructed and coherent pieces of information (such as in question-answer pairs), could make good use of such structured information and be better guided in making its selections for summary generation.
In addition, this discourse structure might aid modules that perform automatic anaphora detection and resolution.
8. Conclusions We have motivated, implemented, and evaluated an approach for automatically creating extract summaries for open-domain spoken dialogues in informal and formal genres of multiparty conversations.
Our dialogue summarization system DIASUMM 481 Computational Linguistics Volume 28, Number 4 uses trainable components to detect and remove speech disfluencies (making the output more readable and less noisy), to determine sentence boundaries (creating suitable text spans for summary generation), and to link cross-speaker information units (allowing for increased summary coherence).
We used a corpus of 23 dialogue excerpts from four different genres (80 topical segments, about 47,000 words) for system development and evaluation and the disfluencyannotated SWITCHBOARD corpus (LDC 1999b) for training of the dialogue-specific components.
Our corpus was annotated by six human coders for topical boundaries and relevant text spans for summaries.
Additionally, we had annotations made for disfluencies, sentence boundaries, question speech acts, and the corresponding answers to those question speech acts.
In a global system evaluation we compared the MMR-based sentence selection component with the DIASUMM system using all of its components discussed in this article.
The results showed that (1) both a baseline MMR system as well as DIASUMM create better summaries than a LEAD baseline (except for NEWSHOUR) and that (2) DIASUMM performs significantly better than the baseline MMR system for the informal dialogue corpora (CALLHOME and GROUP MEETINGS).
Acknowledgments We are grateful to Alex Waibel, Alon Lavie, Jaime Carbonell, Vibhu Mittal, Jade Goldstein, Klaus Ries, Lori Levin, and Marsal Gavald` for many discussions, a suggestions, and comments regarding this work.
We also want to commend the corpus annotators for their efforts.
Finally, we want to thank the four anonymous reviewers for their detailed feedback on a preliminary draft, which greatly helped improve this article.
This work was performed while the author was affiliated with the Language Technologies Institute at Carnegie Mellon University and was supported in part by grants from the U.S.
Department of Defense.
References Alexandersson, Jan and Peter Poller.
1998. Towards multilingual protocol generation for spontaneous speech dialogues.
In Proceedings of INLG-98, Niagara-on-the-Lake, Canada, August.
Aone, Chinatsu, Mary Ellen Okurowski, and James Gorlinsky.
1997. Trainable, scalable summarization using robust NLP and machine learning.
In ACL/EACL-97 Workshop on Intelligent and Scalable Text Summarization, Madrid.
Arons, Barry.
1994. Pitch-based emphasis detection for segmenting speech.
In Proceedings of ICSLP-94, pages 19311934.
Berger, Adam L.
and Vibhu O.
Mittal. 2000.
OCELOT: A system for summarizing Web pages.
In Proceedings of the 23rd ACM-SIGIR Conference.
Bett, Michael, Ralph Gross, Hua Yu, Xiaojin Zhu, Yue Pan, Jie Yang, and Alex Waibel.
2000. Multimodal meeting tracker.
In Proceedings of the Conference on Content-Based Multimedia Information Access (RIAO-2000), Paris, April.
Brill, Eric.
1994. Some advances in transformation-based part of speech tagging.
In Proceedings of AAAI-94.
Carbonell, Jaime, Yibing Geng, and Jade Goldstein.
1997. Automated query-relevant summarization and diversity-based reranking.
In Proceedings of the IJCAI-97 Workshop on AI and Digital Libraries, Nagoya, Japan.
Carbonell, Jaime and Jade Goldstein.
1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries.
In Proceedings of the 21st ACM-SIGIR International Conference on Research and Development in Information Retrieval, Melbourne, Australia.
Carletta, Jean, Amy Isard, Stephen Isard, Jacqueline C.
Kowtko, Gwyneth Doherty-Sneddon, and Anne H.
Anderson. 1997.
The reliability of a dialogue structure coding scheme.
Computational Linguistics, 23(1):1331.
Chen, Francine R.
and Margaret Withgott.
1992. The use of emphasis to automatically summarize a spoken discourse.
In Proceedings of ICASSP-92, pages 229332.
Cohen, Jacob.
1960. A coefficient of agreement for nominal scales.
Educational and Psychological Measurement, 20(1):3746.
Davies, Mark and Joseph L.
Fleiss. 1982.
Measuring agreement for multinomial data.
Biometrics, 38:10471051, December.
Garofolo, John S., Ellen M.
Voorhees, Cedric G.
P. Auzanne, and Vincent M.
Stanford. Zechner Automatic Summarization of Dialogues 1999.
Spoken document retrieval: 1998 evaluation and investigation of new metrics.
In Proceedings of the ESCA Workshop: Accessing Information in Spoken Audio, pages 17, Cambridge, UK, April.
Garofolo, John S., Ellen M.
Voorhees, Vincent M.
Stanford, and Karen Sparck Jones.
1997. TREC-6 1997 spoken document retrieval track overview and results.
In Proceedings of the 1997 TREC-6 Conference, pages 8391, Gaithersburg, MD, November.
Gavalda, Marsal, Klaus Zechner, and Gregory Aist.
1997. High performance segmentation of spontaneous speech using part of speech and trigger word information.
In Proceedings of the fifth ANLP Conference, Washington, DC, pages 1215.
Godfrey, J.
J., E.
C. Holliman, and J.
McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for research and development.
In Proceedings of ICASSP-92, volume 1, pages 517520.
Grosz, Barbara J.
and Candace L.
Sidner. 1986.
Attention, intentions, and the structure of discourse.
Computational Linguistics, 12(3):175204.
Hearst, Marti A.
1997. TextTiling: Segmenting text into multi-paragraph subtopic passages.
Computational Linguistics, 23(1):3364.
Heeman, Peter A.
and James F.
Allen. 1999.
Speech repairs, intonational phrases, and discourse markers: Modeling speakers' utterances in spoken dialogue.
Computational Linguistics, 25(4):527571.
Hirschberg, Julia, Steve Whittaker, Don Hindle, Fernando Pereira, and Amit Singhal.
1999. Finding information in audio: A new paradigm for audio browsing/retrieval.
In Proceedings of the ESCA Workshop: Accessing Information in Spoken Audio, pages 117122, Cambridge, UK, April.
Hori, Chiori and Sadaoki Furui.
2000. Automatic speech summarization based on word significance and linguistic likelihood.
In Proceedings of ICASSP-00, pages 15791582, Istanbul, Turkey, June.
Jurafsky, Daniel, Rebecca Bates, Noah Coccaro, Rachel Martin, Marie Meteer, Klaus Ries, Elizabeth Shriberg, Andreas Stolcke, Paul Taylor, and Carol Van Ess-Dykema.
1998. SwitchBoard discourse language modeling project: Final report.
Research Note 30, Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD.
Kameyama, Megumi, and I.
Arima. 1994.
Coping with aboutness complexity in information extraction from spoken dialogues.
In Proceedings of ICSLP 94, pages 8790, Yokohama, Japan.
Kameyama, Megumi, Goh Kawai, and Isao Arima.
1996. A real-time system for summarizing human-human spontaneous spoken dialogues.
In Proceedings of ICSLP-96, pages 681684.
Knight, Kevin and Daniel Marcu.
2000. Statistics-based summarization--Step one: Sentence compression.
In Proceedings of the 17th National Conference of the AAAI.
Koumpis, Konstantinos and Steve Renals.
2000. Transcription and summarization of voicemail speech.
In Proceedings of ICSLP-00, pages 688691, Beijing, China, October.
Krippendorff, Klaus.
1980. Content Analysis.
Sage, Beverly Hills, CA.
Kupiec, J., J.
Pedersen, and F.
Chen. 1995.
A trainable document summarizer.
In Proceedings of the 18th ACM-SIGIR Conference, pages 6873.
Lavie, Alon, Alex Waibel, Lori Levin, Michael Finke, Donna Gates, Marsal Gavalda, Torsten Zeppenfeld, and Puming Zhan.
1997. Janus III: Speech-to-speech translation in multiple languages.
In IEEE International Conference on Acoustics, Speech and Signal Processing, Munich.
Levin, Lori, Klaus Ries, Ann Thyme-Gobbel, and Alon Lavie.
1999. Tagging of speech acts and dialogue games in Spanish call home.
In Proceedings of the ACL-99 Workshop on Discourse Tagging, College Park, MD.
Linguistic Data Consortium (LDC).
1996. CallHome and CallFriend LVCSR databases.
Linguistic Data Consortium (LDC).
1999a. Addendum to the part-of-speech tagging guidelines for the Penn Treebank project (Modifications for the SwitchBoard corpus).
LDC CD-ROM LDC99T42.
Linguistic Data Consortium (LDC).
1999b. Treebank-3: Databases of disfluency annotated Switchboard transcripts.
LDC CD-ROM LDC99T42.
Mani, Inderjeet, David House, Gary Klein, Lynette Hirschman, Leo Obrst, Therese Firmin, Michael Chrzanowski, and Beth Sundheim.
1998. The TIPSTER SUMMAC text summarization evaluation.
Technical Report MTR 98W0000138, Mitre Corporation, October 1998.
Mani, Inderjeet and Mark T.
Maybury, editors.
1999. Advances in Automatic Text Summarization.
MIT Press, Cambridge.
Marcu, Daniel.
1999. Discourse trees are good indicators of importance in text.
In I.
Mani and M.
T. Maybury, editors, Computational Linguistics Volume 28, Number 4 Advances in Automatic Text Summarization.
MIT Press, Cambridge, pages 123136.
Meteer, Marie, Ann Taylor, Robert MacIntyre, and Rukmini Iyer.
1995. Dysfluency annotation stylebook for the Switchboard corpus.
Linguistic Data Consortium (LDC) CD-ROM LDC99T42.
Miike, Seiji, Etuso Itoh, Kenji Onon, and Kazuo Sumita.
1994. A full-text retrieval system with a dynamic abstract generation function.
In Proceedings of the 17th ACM-SIGIR Conference, pages 318 327.
Nakatani, Christine H.
and Julia Hirschberg.
1994. A corpus-based study of repair cues in spontaneous speech.
Journal of the Acoustic Society of America, 95(3):16031616.
Passonneau, Rebecca J.
and Diane J.
Litman. 1997.
Discourse segmentation by human and automated means.
Computational Linguistics, 23(1):103139.
Quinlan, J.
Ross. 1992.
C4.5: Programs for Machine Learning.
Morgan Kaufmann, San Mateo, CA.
Rath, G.
J., A.
Resnick, and T.
R. Savage.
1961. The formation of abstracts by the selection of sentences.
American Documentation, 12(2):139143.
Reimer, U.
and U.
Hahn. 1988.
Text condensation as knowledge base abstraction.
In Proceedings of the fourth Conference on Artificial Intelligence Applications, pages 338344, San Diego.
Reithinger, Norbert, Michael Kipp, Ralf Engel, and Jan Alexandersson.
2000. Summarizing multilingual spoken negotiation dialogues.
In Proceedings of the 38th Conference of the Association for Computational Linguistics, pages 310317, Hong Kong, China, October.
Ries, Klaus, Lori Levin, Liza Valle, Alon Lavie, and Alex Waibel.
2000. Shallow discourse genre annotation in CALLHOME Spanish.
In Proceedings of the Second Conference on Language Resources and Evaluation (LREC-2000), Athens, May/June.
Rose, Ralph Leon.
1998. The Communicative Value of Filled Pauses in Spontaneous Speech.
Ph.D. thesis, University of Birmingham, Birmingham, UK.
Salton, Gerard, editor.
1971. The SMART Retrieval System--Experiments in Automatic Text Processing.
Prentice Hall, Englewood Cliffs, NJ.
Santorini, Beatrice.
1990. Part-of-Speech Tagging guidelines for the Penn Treebank project.
Linguistic Data Consortium (LDC) CD-ROM LDC99T42.
Shriberg, Elizabeth E . 1994.
Preliminaries to a Theory of Speech Disfluencies.
Ph.D. thesis, University of Berkeley, Berkeley.
Shriberg, Elizabeth, Rebecca Bates, Andreas Stolcke, Paul Taylor, Daniel Jurafsky, Klaus Ries, Noah Coccaro, Rachel Martin, Marie Meteer, and Carol Van Ess-Dykema.
1998. Can prosody aid the automatic classification of dialog acts in conversational speech?
Language and Speech, 41(34):439487.
Shriberg, Elizabeth, Andreas Stolcke, Dilek Hakkani-Tur, and Gokhan Tur.
2000. Prosody-based automatic segmentation of speech into sentences and topics.
Speech Communication, 32(12):127154.
Stifelman, Lisa J . 1995.
A discourse analysis approach to structured speech.
In AAAI-95 Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, Stanford, March.
Stolcke, Andreas, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema, and Marie Meteer.
2000. Dialogue act modeling for automatic tagging and recognition of conversational speech.
Computational Linguistics, 26(3):339373.
Stolcke, Andreas and Elizabeth Shriberg.
1996. Automatic linguistic segmentation of conversational speech.
In Proceedings of ICSLP-96, pages 10051008.
Stolcke, Andreas, Elizabeth Shriberg, Rebecca Bates, Mari Ostendorf, Dilek Hakkani, Madeleine Plauche, Gokhan Tur, and Yu Lu.
1998. Automatic  detection of sentence boundaries and disfluencies based on recognized words.
In Proceedings of ICSLP-98, volume 5, pages 22472250, Sydney, December.
Teufel, Simone and Marc Moens.
1997. Sentence extraction as a classification task.
In ACL/EACL-97 Workshop on Intelligent and Scalable Text Summarization, Madrid.
Valenza, Robin, Tony Robinson, Marianne Hickey, and Roger Tucker.
1999. Summarisation of spoken audio through information extraction.
In Proceedings of the ESCA Workshop: Accessing Information in Spoken Audio, pages 111116, Cambridge, UK, April.
Wahlster, Wolfgang.
1993. Verbmobil--Translation of face-to-face dialogs.
In Proceedings of MT Summit IV, Kobe, Japan.
Waibel, Alex, Michael Bett, and Michael Finke.
1998. Meeting browser: Tracking and summarizing meetings.
In Proceedings of the DARPA Broadcast News Workshop.
Ward, Wayne.
1991. Understanding spontaneous speech: The PHOENIX system.
In Proceedings of ICASSP-91, Zechner Automatic Summarization of Dialogues pages 365367.
Whittaker, Steve, Julia Hirschberg, John Choi, Don Hindle, Fernando Pereira, and Amit Singhal.
1999. SCAN: Designing and evaluating user interfaces to support retrieval from speech archives.
In Proceedings of the 22nd ACM-SIGIR International Conference on Research and Development in Information Retrieval, pages 2633, Berkeley, August.
Zechner, Klaus and Alon Lavie.
2001. Increasing the coherence of spoken dialogue summaries by cross-speaker information linking.
In Proceedings of the NAACL-01 Workshop on Automatic Summarization, pages 2231, Pittsburgh, June.
Zechner, Klaus and Alex Waibel.
2000a. DIASUMM: Flexible summarization of spontaneous dialogues in unrestricted domains.
In Proceedings of COLING-2000, pages 968974, Saarbrucken, Germany,  July/August.
Zechner, Klaus and Alex Waibel.
2000b. Minimizing word error rate in textual summaries of spoken language.
In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-2000), pages 186193, Seattle, April/May .
From Conceptual Time to Linguistic Time Michel Gagnon* Machina Sapiens Guy Lapalme t Universit4 de Montr4al In this paper, we present a method for generating French texts conveying temporal information that integrates Discourse Representation Theory (DRT) and Systemic Grammar Theory.
DRT is used to represent temporal information and an intermediate semantic level for the temporal localization expressed by temporal adverbial phrases and verb phrases.
This representation is then translated into a syntactic form using Systemic Grammar Theory.
We have implemented this method in a working prototype called Prdtexte.
1. Introduction In speaker-generated texts, reference is made to facts taking place in time.
To use the same kind of references in automatically generated text, the mechanisms that govern the expression of temporal concepts must be identified.
There is no simple or direct mapping between conceptual time, as it is perceived in the real world, and linguistic time, which refers to the way time is formulated in language.
There may be different ways to present the same temporal concept in a text, and a single linguistic marker can be used to convey different temporal meanings.
For example, the discourse below (Discourse 1) is a text generated by Pr4texte, a system we developed for implementing the expression of temporal localization in French texts 1.
It is a slightly modified version of an example used by Bras (1990) for the extraction of temporal information in text analysis.
The sentences report occurrences that are facts taking place in time.
We have inserted labels in parentheses to distinguish the twelve occurrences reported in the text.
Hier l'avion a effectu6 un vol (ol).
A 8h00 il a quitt6 Paris (02).
Quand il a survol6 Barcelone (o3), le r6acteur fonctionnait (o4).
~, 10h15, un voyant a clignot6 (o5).
Auparavant, il s'6tait allure6 (o6).
Puis il s'6tait 6teint (o7).
Pendant 35 minutes, l'avion a survol6 lamer (o8).
Puis il a atteint la c6te (o9).
Jusqu'a 10h50, il a survol6 l'Alg6rie (o10).
A llh30 il 6tait sur la piste (o11).
A ce moment-la le r6acteur a explos6 (o12).
Yesterday the plane made a flight (ol).
At 8:00 A.M. it left Paris (o2).
When it flew over Barcelona (o3), the engine was working (o4).
At 10:15, a warning light flashed (o5).
Previously it had come on (o6).
Then it had gone out (o7).
For 35 minutes the plane flew over the sea (o8).
Then it reached the coast (o9).
Until 10:50 it flew over Algeria (olo).
At 11:30 it was on the landing runway (o11).
At this moment the engine exploded (o12).
Discourse 1 * 3535 Queen-Mary, Bur.
420, Montr6al (Quebec), Canada H3V 1H8, Tel: (514) 733-3959.
E-maih gagnon@iro.umontreal.ca.
This article was written while the author was at "Laboratoire Langue, Raisonnement et Calcul" of IRIT, Toulouse, France.
JD6partement cl'informatique et de recherche op6rationnelle, C.
P. 6128, Succ.
Centre-Ville, Montr6al (Qu6bec), Canada H3C 3J7.
E-mail: lapalme@iro.umontreal.ca.
1 The
French text shown on the left in Discourse 1 was generated by Pr6texte; we give an English translation on the right.
@ 1996 Association for Computational Linguistics Computational Linguistics Volume 22, Number 1 In Discourse 1, we find two types of temporal markers: verb tense and what we call adverbials of temporal location (ATL).
An ATL is an adjunct, such as yesterday, until 10:50, or when it flew over Barcelona, that provides information about the temporal localization of an occurrence or its duration, or both at the same time.
For verb tense, we distinguish different ways of indicating localization in the past.
Three French verb tenses can be used: pass~ composG imparfait, and plus-que-parfait; their closest equivalents in English are the simple past, past progressive, and past perfect.
The passf compos~ il a survold 'it flew over' presents the occurrence as an event and localizes it in relation to the time of speech.
With the plus-que-parfait il s'dtait allum~ 'it had come on', the occurrence is also presented as an event, but localized in relation to a perspective point other than speech time.
The imparfait le r~acteur fonctionnait 'the engine was working' presents the occurrence as being in progress.
For present and future tenses there are fewer options than for past tense, but more than one form is available for theses tenses as well.
For ATLs, temporal localization can be achieved in many ways; for example, in relation to the time of speech (hier 'yesterday'), by designating an absolute temporal location (~ 8hO0 'at 8:00 A.M.'), or in relation to another fact (puis 'then', ~ ce moment-ld 'at this moment', quand il a survold Barcelone 'when it flew over Barcelona').
To this variety in the semantics of localization we must add the variety of syntactic forms.
Localization can be expressed by an adverb (puis 'then'), a prepositional phrase (jusqu'd 10h50 'until 10:50'), a nominal phrase (le lendemain 'the day after') or a subordinate clause (quand il a survold Barcelone 'when it flew over Barcelona').
No text generator has yet been developed to solve the problem of the expression of time.
The ones that have tackled this problem have focused on the production of verb tenses, without solving the choice of temporal adverbs.
The work presented in this paper addresses the problem of generating the elements that convey temporal localization in French, including both verbs and temporal adverbs.
In a previous paper (Gagnon and Lapalme 1992), we proposed a method of integrating the expression of temporal concepts into the text-generation process.
In particular, we showed how to produce different types of text in French from a single representation of events.
Unfortunately, the method governing the planning process was too determined by temporal concepts, so it was difficult to link this planning process with other frameworks, such as the schema proposed by McKeown (1985) or Rhetorical Structure Theory (RST) (Mann 1991; Hovy 1991).
As we were not really successful in integrating the expression of time in French into the text-generation process, we decided to pursue our research with a different perspective.
We designed a system covering many of the possibilities of expressing time in French, our hypothesis being that the achievement of this task would facilitate the design of a text-planning process.
We believed it would be easier to organize the structure of the discourse with a better understanding of the way temporal information can be expressed by adverbs and verb tenses.
We started from the work of Bras (1990), who proposes a method of extracting the temporal structure of a text, according to Discourse Representation Theory (Kamp 1981), that relies on an analysis of adverbials of temporal location made by Molin6s (1990).
To implement the production of ATLs and verb tenses, we have chosen Systemic Grammar (Halliday 1985; Berry 1975, 1976), which formulates the syntactic structure of a sentence as the result of a sequence of semantic choices.
We developed a grammar interpreter inspired by Nigel (Matthiessen and Bateman 1991), but departing from it in many respects; in particular, our representation of the production of verb tenses and adverbs is quite different.
In this paper, we discuss the elements required to produce a text such as Dis92 Gagnon and Lapalme From Conceptual Time to Linguistic Time / DEEP / GENERATIONN f SURFACE GENERATION", Conceptual representation Discourse representation t Semantic representation 1 Syntactic representation Occurrences as primitive concepts Temporal relations The tempolral Jocaligafion is represented as an overlapmg relation Sellmentafion of the conceptual ret~resentaaon Structured information Rhetorical relations Linearization Choice of aspect Identification of temporal markers Temporal adverbial Verb tense Figure 1 The global process.
course 1.
The process starts from a conceptual representation that encodes the facts to be reported in the text, associated with their position in time.
The information at this objective conceptual level must be translated into a semantic representation where the facts are presented according to a subjective perspective.
The semantic representation is then used to produce the text.
We have concentrated our attention on this last stage, but we cannot avoid the problem of determining how the representation used at this level is obtained from previous levels.
In the following sections, we describe the two stages of the text-generation process.
2. The Global Process It is generally accepted that the generation process requires at least two parts.
The first part, deep generation, is a planning process in which the content and the overall structure of the text are established.
In the second part, surface generation, the words and the syntactic structure of the text are chosen.
Figure 1 summarizes our view of the global process, starting from a conceptual representation that contains occurrences and relations between them.
The fact that an occurrence takes place at a certain time is expressed by an overlapping relation between this occurrence and the object representing this time.
The deep generation process is decomposed into two steps.
In the first step, the conceptual representation is segmented and structured to build a discourse representation.
In our discourse representation, which uses Segmented Discourse Representa93 Computational Linguistics Volume 22, Number 1 tion Theory (SDRT) (Asher 1993), the information is cut into smaller segments each of which contains the information to be expressed by a single sentence.
The structure linking these segments relies on a set of rhetorical relations.
In the second step of the deep generation process, the discourse representation is traversed and, for each segment, rules are applied to identify the feature values needed to translate it into a sentence.
We thus obtain a linear structure in which each element is a set of features that determine the syntactic form of the sentence.
In the surface generation process, the information in the semantic representation is used to select the appropriate syntactic structure for the expression of time: an adverbial of temporal location (ATL) or a verb phrase (VP), or both.
3. The Deep Generation Process Although our work focuses on surface generation, we cannot ignore the issue of deep generation, because the nature of the semantic representation is determined not only by the syntax of the language, but also by the temporal concepts available.
Therefore, in this section, we first present the conceptual representation that induces the semantic representation used by our generator.
We then explain the intermediate discourse level.
We do not know yet in detail how to produce the semantic representation from the conceptual representation, but we do have an idea of what information each level of representation must contain and what choices must be made at each stage of the process.
3.1 Conceptual
Representation To represent temporal concepts in Pr6texte, we chose the principles of Discourse Representation Theory (DRT), which offers one of the most interesting explanations of how temporal notions are conveyed by a text.
DRT was developed to deal with specific problems of discourse understanding: in particular, problems with anaphora and the differences between some verb tenses, with respect to temporal localization.
Our goal is not to show how this theory can be used for generation but rather to use its principles as a convenient formalism for the representation of time.
In DRT, a text is associated with a Discourse Representation Structure (DRS) that is updated incrementally by the processing of each sentence.
A DRS is a structure containing a set of entities and a set of conditions on these entities.
There are different types of entities in DRT: a temporal fact can either be presented as an event (having a punctual aspect), or as a situation (having a certain extent in time, but considered from an internal perspective at a given moment in time); a temporal constant that designates a segment of the temporal axis; entities that participate in the events or situations.
In Pr6texte, conceptual knowledge is represented as a DRS, which we adapted slightly for text generation.
We do not distinguish between events and situations in the conceptual representation, because we want this level to remain independent of the language.
Furthermore, we feel that this distinction should not be encoded at the conceptual level, rather the generation system should choose among these possibilities.
Therefore, at the conceptual level we use the concept of occurrence for either an event or a situation.
94 Gagnon and Lapalme From Conceptual Time to Linguistic Time n tl t2 t3 t4 01 02 03 04 a l p b r plane(a) flight(l) engine(r) city(p) Paris(p) city(b) o20 t2 Barcelona(b) ol < n o1: make(a, 1) o3 < n o 2" leave(a,p) o~ C tl o3: flightover(a, b) o4 < n 04: work(r) o3 C 04 n c t3 03 C) t4 tl = Sept.
10 1992 t2 = Sept.
10 1992 at 8:10 am t3 = Sept.
11 1992 t4 = Sept.
10 1992 at 9:00 am 02 <n Figure 2 Conceptual representation for the first three sentences of Discourse 1.
There are essentially two ways of considering time or, to be more precise, the notion of temporal location: either temporal location is determined using a preexisting time scale, or it is deduced from the occurrence.
Following Kamp (1981), we think that the second possibility, in which temporal location is a relative concept, is more suitable for natural text processing.
Treating occurrences as entities, rather than making them subordinate to temporal intervals or points, has been proposed by Davidson (1967).
An occurrence may be represented in relation to another temporal object, without any reference to its own location in time.
This approach eases the representation of underspecified temporal localizers--an important point for our semantics.
For further discussion of the advantages of taking occurrences as primitives, see (Bras 1990; Kamp 1979, 1981).
In the conceptual representation, we find four types of information:  the description of occurrences;  the description of participants in the occurrences;  the description of temporal localizers, which are called temporal constants (they usually refer to time periods of the calendar);  temporal relations between occurrences and temporal localizers: The relation < represents temporal precedence.
The overlap relation C) indicates that two temporal objects are somehow simultaneous.
Thus, in our representation, "Y happens at time X" is represented by "Y temporally overlaps X".
The relation C expresses the fact that the temporal extent of a temporal object is a subset of the temporal extent of another object.
Figure 2 shows the part of the DRS used to generate the first three sentences of Discourse 1.
It contains five temporal constants: n, tl, t2, t3 and t4.
It is not clear how 95 Computational Linguistics Volume 22, Number 1 these temporal constants are to be described in DRT, so we have proposed elsewhere a formalization of the type of objects designated by these constants (Gagnon and Bras 1994).
In this article, we give only an English description of them: n represents speech time, which, in Figure 2, is included in the time represented by t3 (September 11 1992).
Four occurrences are represented: ol, o2, o3, and o4, all of which take place before n.
Not all temporal relations in the DRS need to be given as input because many relations can be inferred using three kinds of knowledge:  The representation of conventional time to identify a specific period in time; this representation relies on a structure of conventional time, together with reasoning mechanisms to deduce temporal relations (see Gagnon and Bras \[1994\] for an implementation of such a structure).
For example, from this knowledge we can deduce that September 10 must be before September 11, which would be represented as tl < t3.
Similarly, we can deduce t2 C tl, t4 C tl, and t2 < t4.
 World knowledge about the occurrences: knowing that o2, 03, and o4 are part of Ol implies that they are all temporally included in it.
 A reasoning mechanism on the temporal relations, using a set of axioms, such as: Vx, y(xOyvx < yvy < x) Vu, v,x,y(uOxAvOyAx < y~ uOvVu < v) The first axiom states that for any two times, either they overlap or one precedes the other.
The second axiom states that if two other times u, v overlap two times x, y that are in a precedence relation, either u overlaps v, or it precedes it.
So in Figure 2, from the relations 02 O t2, 03 O t4, and t2 < t4, we can infer o2 < 03 V o20 o3.
From world knowledge, we can infer that o2 and 03 cannot overlap (leaving of Paris cannot overlap flying over Barcelona); Therefore, we conclude that 02 ~ 03.
3.2 The
Discourse Representation To generate a text from an input such as Figure 2, we must choose a discourse structure that segments the message into sentences.
Figure 3 illustrates one discourse representation, inspired by the Segmented Discourse Representation Theory (SDRT) proposed by Asher (1993), which extends DRT by adding rhetorical relations such as those found in RST (Mann and Thompson 1987).
A discourse structure that contains the same information as in Figure 2, except that it has been segmented, we call a Segmented Discourse Representation Structure (or SDRS).
The top-level DRS contains three small DRSs that are linked by rhetorical relations: each DRS corresponds to a sentence.
In addition to these three small DRSs, the top-level DRS contains the global text information: the description of participants and the description of speech time.
We do not yet produce this discourse structure, but we are working on this problem, using the results of researchers who have applied SDRT to the analysis process (Lascarides and Asher 1993; Bras and Asher 1994).
In the discourse structure of Figure 3, one sentence is elaborated by two other sentences that constitute a narration.
96 Gagnon and Lapalme From Conceptual Time to Linguistic Time plane(a) fight(l) city(p) Paris(p) n t3 a l p b r Ol tl o1: make(a, 1) tl = Sept.
10 1992 Ol Kn Ol C tl Elaboration Barcelona(b) engine(r) t3 = Sept.
11 1992 n C t3 city(b) 02 t2 o2: leave(a, p) t2 = Sept.
10 1992 at 8:10 am 02 Kn 02 0 t2 Narration 03 04 t4 03: flightover(a,b) 04: work(r) O3 (n 04 <n t4 = Sept.
10 1992 at 9:00 am 03 C 04 03 0 t4 Figure 3 Discourse representation for the first three sentences of Discourse 1.
3.3 Semantic
Representation The discourse structure is then translated into a semantic representation of the form $1, $2,..., Sn where Si designates the i th element of a semantic representation S.
Translation of the SDRS is obtained by a depth-first traversal of the DRSs it contains.
For each DRS, we establish its corresponding feature structure in the semantic representation.
Figure 4 is a semantic representation produced from the SDRS of Figure 3.
Each structure contains five features.
The feature message refers to the occurrence that must be reported by the sentence, and specifies its aspect.
We distinguish, as Kamp does, two aspects that can be used to present an occurrence: event or situation.
Situations can be open or resulting.
A situation is open when the speaker/hearer is located at a time within an occurrence.
A resulting situation is the state following the termination of an occurrence.
In French, the event aspect for a past occurrence results in the use of the pass~ composd (simple past in English).
The imparfait and the plus-que-parfait correspond to open and resulting situations (the closest tenses in English are the past progressive and the past perfect).
In the first two elements of Figure 4, the occurrence is presented as an event, whereas in the last one it is presented as an open situation.
Among the four occurrences contained in the DRS, only three of them constitute the main "message" of the text: ol, o2, and o4.
The four other features in a structure Si give the value of four temporal markers that express the localization of the occurrence.
These markers correspond to the four markers proposed by Kamp and Rohrer (Bras 1990) for the analysis of texts, which we have adapted for text generation.
They can be seen as an extension of Reichenbach's markers (1947).
Essentially, the values of these four features depend on two data:  the DRSs to which the visited DRS is attached in the discourse structure, and  the rhetorical relations.
97 Computational Linguistics Volume 22, Number 1 Figure 4 message = event(ol) N=n REF = nil PERSP = n LOC = tl S1 message = event(o2) N=n R =Ol PERSP = n LOG = t2 $2 message : open_situation ( o 4 ) N=n R=o2 PERSP = n LOC = o3 S3 Semantic representation for the first three sentences of Discourse 1.
O1 :....
~. Elaboration 102 ....
LNa a onL04 ....
INaatinIO5 ....
I a atin ~.
Flashback h 08: ....
I 06: ....
\[ Narration ~, 07: ....
\] Figure 5 Discourse representation for the first seven sentences of Discourse 1.
The marker N represents the time of speech.
This marker is constant in our example, but it could be locally altered in the discourse, in indirect speech for example.
We did not study such cases, but we think that the marker N would be required to deal with them.
Perspective point PERSP refers to an instant from which the occurrence must be considered.
Usually it is the same as the time of speech, but in some cases, such as a flashback, it has a different value.
In Discourse 1, there is one such case.
The fifth and sixth sentences (where occurrences o6 and o7 are reported) constitute a flashback: the perspective point is the occurrence of the fourth sentence (os).
In discourse structure, the flashback is represented with a rhetorical structure.
Consider for example the discourse structure for the first seven sentences of Discourse 1 as sketched in Figure 5.
For the translation of the two DRSs containing 06 and o7, the value of PERSP will be the occurrence o5, since the DRS containing this occurrence dominates the two others with the relation flashback.
For the next DRS, the one containing os, the perspective point will be reset to the value it had before entering the flashback, that is, the value when the DRS of o6 was considered.
The value of PERSP is used for the choice of verb tense.
In Discourse 1, the flashback results in the choice of the plus-que-parfait.
LOC represents the temporal location of the occurrence reported.
If this occurrence overlaps another temporal object, this object may be used as a value for LOC.
In 98 Gagnon and Lapalme From Conceptual Time to Linguistic Time Figure 4, the values of LOC show that tl and t2 are used to localize the first two occurrences.
In the third sentence, the situation corresponding to o4 is presented at the instant where o3 takes place.
If no other temporal object overlaps the one that constitutes the message, the temporal location represented by LOC can be defined in relation to another temporal object.
We will see examples of this in the next section.
LOC represents the information to be expressed by an ATL in the sentence and does not necessarily have a value, because a sentence may not contain an ATL.
In a text, when we want to express a succession of occurrences, we need a way to memorize the occurrence that is used as a reference for the localization of the next one.
This is exactly the role of the marker REF.
The values of REF are used to represent the progression of time in the discourse.
Each time a sentence expresses a new temporal location (which can be an occurrence or a temporal constant), the value of REF is updated to this new value, and the temporal localization in the following sentence may be achieved in relation to this reference.
The following rules are used for identifying the value of REF: 1.
Identify the S-antecedent, the DRS to which the current DRS is attached in the discourse structure, and Sa, the feature structure associated with this DRS in the semantic representation.
2. If the occurrence reported as message in Sa is presented as a situation, it cannot be used as a reference point, since a situation cannot state a progression in time.
3. If LOC has a value that is temporally more precise than the occurrence in the message, REF will take on its value, otherwise REF is bound to the occurrence in the message, if this occurrence is not presented as a situation.
4. If LOC has no value and the occurrence in the message is a situation, the antecedent sentence does not state a progress in time.
Therefore, REF takes the same value as in Sa.
In Figure 4, the context for the first sentence is empty, so no value is given to REE For the second and third sentences, the value of REF is the event presented in the previous sentence.
The occurrence in the third sentence is expressed as a situation, so it cannot be the reference for the fourth sentence (not shown in the figure).
Consequently, REF for the fourth sentence takes the value of LOC in the structure of the third sentence: t4.
We will see in Section 5 how the value of REF is used to produce the temporal adverb.
The choice of aspect in building the semantic structure is achieved by taking into account pragmatic information and the interaction with other choices, such as the type of temporal localizer.
Currently, we first identify the localizer that constrains the selection of aspect, but more study is needed to clarify their interaction.
If an occurrence is presented as a situation, the temporal localizer must be a time included in it; but an event aspect cannot be combined with a localizer.
In Figure 4, the occurrence of $1 must be presented as an event, since the localizer tl includes the occurrence.
In $2, the occurrence is also an event, even if the localizer overlaps the occurrence: the overlapping relation does not prevent the existence of an inclusion relation.
If an inclusion relation between t2 and o2 could be deduced, then the situation aspect could be chosen.
In $3, the localizer is included in the occurrence of the message, so the situation aspect is selected.
99 Computational Linguistics Volume 22, Number 1 message = event (oi) N=n R = nil P=n L = ch message = event(o2) N=n ' R =01 P=n L = ct2 Figure 6 An alternative semantic representation.
message = event(03) N=n ' R =02 P=n L = ct4 message = open_situation ( o 4 ) N=n ' R =03 P=n L = ct4 ) The semantic representation given in Figure 4 is not unique.
Figure 6 shows another semantic representation built from the DRS of Figure 2.
It contains a fourth sentence.
The main difference from the previous representation is that 03, instead of acting as a localizer for o4, is the message of a sentence; t3, referring to a moment located two hours after t2, localizes o3.
Therefore, instead of the third sentence of Discourse 1, we would obtain these two sentences: Deux heures plus tard, il a survol6 Two hours later, it flew over Barcelona (o'3).
At Barcelone (o~).
Ace moment-la, le r6acteur this moment, the engine was working (o'4).
foncfionnait (o~).
Once the semantic representation is produced, the adverbial or temporal location (ATL) and the verb phrase (VP) can be generated independently.
The syntactic form of the verb phrase is determined by the combination of the following information:  temporal relation between localizer LOC and speech time N;  temporal relation between localizer LOC and perspective point PERSP;  aspect of the occurrence.
The choice of the syntactic structure of the ATL depends on the value of LOC, which may refer to N or REF.
The interaction of temporal information conveyed by verb tense and adverbs is taken into account in the process of translation from the conceptual representation to the semantic level, where the choices of aspect, perspective point PERSP and localizer LOC are made.
We still have not entirely solved the problem of choosing among all semantic representations that can be built from a DRS.
In the current implementation of Pr6texte, we have identified a set of rules to produce the semantic representation.
In particular, these rules insure that the values of the four temporal markers are coherent with the aspect chosen to present the occurrences.
What remains to be done, essentially, is to identify the knowledge that governs these rules causing them to select a particular semantic representation.
3.4 Representation
of Localization We have argued in the previous section that four temporal markers are needed to express the temporal location of an occurrence.
In this section, we discuss the marker LOC, the localizer that provides information about the location in time of the occurrence using another entity.
The localizer is usually a temporal constant, but it can also be another occurrence, whose approximate location in time is already known.
An ATL can convey localizers of two types: in the first type, a localizer directly identities the temporal zone of an occurrence using another temporal object that overlaps it; in the second type the temporal zone of an occurrence is conveyed in relation 100 Gagnon and Lapalme From Conceptual Time to Linguistic Time to another localizer.
In Figure 4, all occurrences are localized directly.
Occurrence 04 is localized directly by another occurrence, whereas 01 and o2 are localized directly by a temporal constant.
For $1 and $2, the values for LOC are simply constants tl and t2.
The same temporal localization can usually be expressed in many ways, and we must also specify how these constants are worded.
For example, in Discourse 1, T1 has been translated into hier 'yesterday' but it could also have been translated into le 10 septembre 1992 'September 10th 1992', or mercredi dernier 'last Wednesday'.
Thus, the value of LOC in the semantic representation determines the expression of the localization, giving rise to three main problems:  how to represent the temporal constants in the conceptual representation;  how to determine the link between these conceptual temporal constants and their semantic representation, which specifies how they are to be expressed in the text; and  how to implement the selection mechanism, which relies on pragmatic and stylistic information to choose between the many different ways of expressing the same temporal localization.
In Gagnon and Bras (1994), we gave a solution to the first two problems but the last one still remains to be solved.
Here, we discuss only the second problem: the semantic expression of localization.
First, a few words about temporal context: usually, temporal localizers may be understood only in reference to some time in the context.
For example, in on April 15th, it is assumed that it is clear which year this time is part of.
Thus, we take for granted that all expressions of temporal localization are made in relation to such a contextual time.
Let ti be a temporal constant, taken from the conceptual representation, which is to be used as a localizer in the semantic representation, and tcont the contextual time.
The expression in the semantic representation is based on a term of the following form: \[ti, Type, Naming\] The first argument is the identifier of the constant in the conceptual representation.
The second argument is the type of the temporal localizer (day, month, year, etc.).
Thus, tcont may be decomposed into times of type Type, of which t i is one.
The last argument names the time referred to by the localizer ti.
There is exactly one time in the "real world" that corresponds to the temporal constant ti.
We call it objective time.
We use the notation t* to represent the objective time that corresponds to a localizer ti.
For example, the expression for the temporal localizer en avril 'in April', would be something like this (here t67 is the corresponding temporal constant in the conceptual representation): \[t67, mort th, april\] The naming april is not the syntactic word "April" but an internal keyword that helps distinguish between the time referred to and the other months of the contextual year.
An important distinction is made between a temporal constant ti and a objective time t*.
The constant ti pertains to the way a temporal location is expressed in the discourse, whereas t~ can be considered the corresponding portion of time in the real world.
More than one temporal constant may correspond to a single objective time.
101 Computational Linguistics Volume 22, Number 1 Table 1 Relative localizers.
Localizer Description inclin(\[ti, Ti, Ni\],\[tj, Tj, Nj\]) incl(\[ti, Ti, Ni\],\[tj, Tj, Nj\]) begin(\[ti, Ti, Ni\],\[tj, Tj, Nj\]) end(\[ti, Ti, Ni\],\[tj, Tj, Nj\]) after(\[ti, Ti, Ni\],\[tj, Tj, Nj\],D ) before(\[ti, T,, Ni\],\[tj, Tj, Nj\],D) relpos(X,\[ti, T, Ni\],\[tj, T, Nj\]) extent(\[ti, Ti, Ni\],\[G Tj, Nj\],\[tk, Tk, Nk\]) t~ is a time included in the time t 7 t* is a time that includes the time tj* t~ is a time whose beginning overlaps the time t~ t~ is a time whose ending overlaps the time t; t~ is a time after the time t~ with a temporal distance D (expressed as a duration) t~ is a time before time t 7 with a temporal distance D (expressed as a duration) t~ is the X th (-X th, if X<0) time of type T after (before, if X<0) time tj* t~ is a time period starting at time t7 and ending at time t; Suppose, for example, that the discourse contains two temporal constants, corresponding to yesterday and two days after Robert's departure.
We can imagine a situation where both designate exactly the same day.
But it is not possible for a single temporal constant to correspond to more than one objective time.
If it were, it would mean that an ATL could be ambiguous.
In our computational perspective, we accept some underspecified ATLs (not precisely specifying the temporal location), but not ambiguous ones.
A triplet is the simplest expression of a temporal localization.
Usually, expressing a temporal localization is more complex, because the temporal constant used as localizer cannot be rendered directly in relation to the contextual time.
This is the case, for example, if the localizer is a day, and the contextual time a year, because there is no natural way of decomposing a year into days 2.
In these cases, we must express relations with some intermediate localizers, until we reach one that can be related to the contextual time.
Let \[ti, Ti, Nj\] be the localizer and \[tj, Tj, Nj\], \[tk, Tk, Nk\] be intermediates localizers.
Many kinds of relations can be distinguished.
They are listed in Table 1.
Note that these relations can be combined recursively.
This means that the triplets used as arguments may also be represented using a relation.
We will show examples of this in the following discussion.
Among the arguments of these relations, one pertains to the temporal localizer, and one (two, in the case of the relation extent) pertains to an intermediate localizer to which the temporal localizer is related.
We call this last argument an anchor, since it represents a time to which the relation must be "anchored" in order to deduce the time of localization.
We will now give a short discussion of the relations of Table 1.
In the following examples, hoe designates the temporal constant corresponding to the localizer of the occurrence, and n and trey designate the time of speech and the reference time, respectively.
2 It
is possible to name the day using the religious calendar, using something like the day of St-Andrew, but it is far from usual to do so, except maybe for holidays such as Christmas, Easter, or Thanksgiving.
102 Gagnon and Lapalme From Conceptual Time to Linguistic Time The first relation is the most frequent for expressing temporal localization.
It is used to express localizations like le 3 avril 'on April 3rd'.
In this case, the localizer could be formulated as: inclin ( \[ tto, day, 3\], \[t~, month, april\]) As expected, the intermediate localizer tl is to be interpreted in relation to the contextual year.
The semantics of a more complex localizer like le matin du 3 avril 1994 'on the morning of April 3rd 1994' is an example of using recursivity for the expression: inclin ( \[hoc, momen t-of-day, morning\], inclin ( \[ h, day, 3\], inclin (\[t2, month, april\], It3, year, 1994\] ))) The second relation, incl, is required to deal with adverbials such as le jour oft Paul est parti 'the day when Paul left', aujourd'hui 'today' and ce jour-l~ 'that day'.
All of these refer to a day, but this day is not expressed in relation to a time that includes it.
On the contrary, the other time is included in it: the time when Paul left in the first example, the time of speech in the second example, and the referent time associated with REF in the third one.
Suppose that in the conceptual representation, 023 is the object representing the departure of Paul.
These three examples could be represented, respectively, as: incl(\[hoc, day, _\], \[O23, -, -\]) incl(\[ttoc, day, _\], \[n, _, _\]) incl ( \[ tloc, day, _\], \[ tref, -, -\] ) where "_" is used for arguments whose values are not relevant or unknown.
The relations begin and end represent the case where only one boundary of the localizer is known.
This results in an ATL such as depuis le 3 avril 'since April 3rd' or jusqu'au 3 avril 'until April 3rd' where meaning is ambiguous.
What do we mean exactly, when we write that ti begins at time tj?
That the initial boundary of ti is included in tj or that the ending of tj "meets" the beginning of ti?
If the answer is that tj includes the initial boundary of ti, what is the constraint on the duration of ti?
It is clear that it must be shorter than tj.
We do not have any answers to these crucial questions, and other similar ones.
These are problems that pertain to the deep generation process, which is not the focus of this paper.
We think that at the level of the semantic representation, this relation need not be further clarified, since it corresponds to the way time is expressed in the language.
In fact, all our relations have this underspecified nature.
For the relations before and after, the value of the temporal distance is given as a duration, using an expression of this form: duration (N, Type) The value of the duration is obtained by calculating the period corresponding to N periods of type Type.
For example, the adverbial deuxjours apr~s le ddpart de Paul 'two days after Paul's departure' would be represented as (023 represents the occurrence of Paul's departure): afler(\[hoc, -, -\], \[O23, -, -\], duration(2, day)) If the temporal distance is not known (or irrelevent), it is indicated by indefinite.
Thus, aprbs le ddpart de Paul 'after Paul's departure' would be represented as: after(\[ttoc, -,-\], \[023,-, -\], indefinite) 103 Computational Linguistics Volume 22, Number 1 Now, let's suppose that the localizer ti is of type T.
tn some cases, a good way to express it is by giving its position relative to another time tj of the same type.
For example, the ATL cinq jours plus tard 'five days later' is not used to mean "at a time in the future, five days from the reference time," but rather "the 5th day after the one which included the reference time".
If the reference time is itself a day, the semantics of this ATL could be: relpos( 5, \[hoc, day, _\], \[trey,-,-\]) If trey is not a day, we must express the relation by taking as anchor the day which includes it: relpos (5, \[ Gc, day, _\], incl ( \[ t l, day, _\], \[trey, day, -\])) (This takes for granted that the time trey is not bigger than a day.
If trey were bigger than a day, it would not make any sense to express relative position by specifying the temporal distance in days).
Similarly, hier 'yesterday' would be expressed semantically as "the day that is the first one before the day that includes the time of speech": relpos(-1, \[tloc, day, -\], incl ( \[\[h, day, _\], \[n, _, _\])) We have seen a way of expressing duration, by giving the length as a number of time units.
But there is another way of expressing duration: by indicating the two boundaries of the period.
By using this method, not only the duration of an occurrence is expressed, but so is its temporal location, at least partially.
The relation extent is used to express this kind of duration.
For example, the semantics of du 3 avril au 5 mai 'from April 3rd to May 5th' is formulated as: extent(\[hoe, _, _\], inclin ( \[ h, day, 3\], \[t2, month, april\]), inclin ( \[t3, day, 5\], \[t4, month, may\])) The semantics of du 3 au 10 avril 'from April 3 to 10' should represent the fact that the whole duration is included in the same month: inclin (extent (\[boo -, -\], \[h, daY, 3\], \[t2, day, 10\]), \[t3, month, april\]) The relation extent is also used to represent adverbials like depuis trois jours 'for three days' and pendant trois jours ?z partir du 3 avril 'during three days starting from April 3rd'.
These adverbials explicitly express one of the two boundaries.
In the first example, it is either the time of speech or the reference time (the ATL means "for three days until now" or "for three days until then").
In the second example, it is the time corresponding to April 3rd.
The other boundary is indicated implicitly by giving a temporal distance from the anchor.
The first example, supposing that the explicit boundary is speech time, would be represented as: extent(\[tloc, -, -\], before(It1, -, -\], \[n, _, _\], duration (3, day)), In, _, _\]) This expresses a period whose ending is the time of speech and whose beginning must be calculated by finding the time that is three days before speech time.
The second example would be represented as: extent( \[tloc, _, _\], inclin (\[tl, day, 3\], \[t2, month, april\]), after(It3, -, -\], \[t l, -, -\], duration(3, day))) Note that in both examples, the same temporal constant represents both the explicit boundary and the anchor of the relation after or before used to express the implicit boundary (n and tl, respectively).
104 Gagnon and Lapalrne From Conceptual Time to Linguistic Time Table 2 Relative localizers.
Adverbial Semantics entre le 3 avril et le 10 mai inclin(\[hoo_,d,extent(\[ti,_,d, (between April 3rd and May lOth) inclin(\[t2,day,3\],\[t3,month,april\]), inclin(lt4,day,10\],lts,month,aprill))) jusqu'tt il y a trois jours end(\[h ..... -\], (until three days ago) before(lt2,_,_\],\[n,_,d,duration(3,day))) jusqu'/i mercredi de cette semaine end(\[tloo_,_\],inclin(\[t2,moment-of-day,wednesday\] (until Wednesday of this week) incl(\[tg,week,_\],\[n,_,_\]))) Considering the examples we have just given, one may think that recursivity applies only to the anchor.
This is not the case.
The triplet that gives the location time in the expression can be replaced by a complex expression.
We have such a situation with le 3 avril dernier 'the last April 3rd' represented as: relpos (1, inclin ( \[tlo, day, 3\], \[tl, month, april\]), incl( \[t2, day, _\], \[n, _, _\])) More precisely, this expression means "the April 3rd that is the first one in the past, taking speech time as starting point".
Finally, to illustrate the richness of our semantics for expressing ATLs, in Table 2 we give a list of more adverbials with their semantics.
Note the extensive use of the combination property.
Thus, to specify the localization of an occurrence, we can use another simultaneous object, or use a localizer that is expressed in relation to another localizer.
The list of relations given in Table 1, together with the possibility of combining them, offers a very powerful way of expressing the great diversity in the semantics of temporal localizers.
Of course, not all the combinations may be expressed naturally in the language, but we are convinced that most of the ATLs can be expressed with this semantics.
3 The
problem of representing temporal location has received a lot of attention in the past (Dowty 1979, 1982, 1986; Bach 1986; Verkuyl 1989; and Vlach 1993).
But these works have focused on the aspectual structure of adverbials and their relation to tense and aspect.
We have not found any previous proposals of a recursive semantics like ours for representing the various types of localizations.
More related to our work is Allen (1983) who proposes a set of primitive relations to represent all possible relations between two temporal intervals.
The relations defined in Table I differ in many respects from the relations proposed by Allen.
As mentioned before, ours are less precise.
For example, the moment represented using the relation end in our model corresponds to three relations in Allen's model.
Suppose that t* and tj* are the objective times corresponding to the localizer and the anchor, respectively.
Then, in Allen's model, the end of t~ could coincide with the beginning of tj*, could coincide with the end of tj*, or could be included in t 7.
The main reason for using a different set of primitives is to represent as closely as possible the way temporal localization is dealt with in language.
The result of our 3 In fact, the set of relations described here is not sufficient.
In Gagnon and Bras (1994), we define a more complete set of relations.
105 Gagnon and Lapalme From Conceptual Time to Linguistic Time I t ENVIRONMENT ...... ------o-M"-'--"~-~'........ n-Knowte.l.e.,; I base I structure concepts t I I I I I I ! ! I ! _J P R E T E R I SEMANTIC INTERFACE I -.~ ENGINE <  GRAMMAR t (Systemic I t network) I I Ii I t I t < --.... -*BLACK E I BOARD I R I Figure 8 Implementation of the surface generation process in Prdtexte.
To produce a sentence, the network is processed from left to right.
When a system is entered, a choice is made that may lead to another system or to a conjunction of systems processed concurrently.
The syntactic structure of the phrase results from a set of constraints determined by features selected during the traversal of the entire network.
The choices made in the first traversal of the network determine the overall structure of the sentence, represented as an ordered sequence of functions that must be fulfilled.
The term "function," in this context, refers to the role played by a constituent of a phrase in achieving a communicative goal (Halliday 1985).
For example, a sentence can often be decomposed into three constituents fulfilling the following functions: Subject, Predicate, and Object.
4 Once
the functional structure of the sentence is established, its network is traversed again to determine the syntactic structure, which is then further refined until each function is realized by a single word.
Figure 8 illustrates the organization of the modules in Pr6texte, inspired by Nigel (Mann 1983; Mann 1985; Matthiessen 1985; Matthiessen and Bateman 1991).
To produce a sentence, Pr6texte uses three information components: the environment, containing the information about the message and a knowledge base describing how to achieve lexicalization; the grammar, represented as a systemic network; and the blackboard, used to determine the syntactic structure.
The engine controls the surface generation process and accesses the three information components through three interface modules: semantic interface, interpreter and realizer.
The solver determines the final structure of the constituent, using constraints posted in the blackboard during the traversal of the network.
4 In
this text, names of functions will always be capitalized.
107 Computational Linguistics Volume 22, Number 1 Before starting the surface generation process, the environment is augmented with information that determines the message:  a semantic structure such as the one illustrated in Figure 4;  a set of relevant concepts, which are the elements of the conceptual representation pertaining to the entities in the semantic structure;  some pragmatic information, which specifies how to transmit the message.
The engine starts by posting on the blackboard the description of the realized constituent representing the sentence.
It then activates the traversal of the network by the interpreter.
To select a feature in a system, the interpreter transmits inquiries to the engine.
If the information necessary to respond to the inquiry is in the environment, the engine transmits the inquiry to the semantic interface.
If an inquiry is about a decision previously made in the surface generation process (for example, a question about what features have been selected in a system visited earlier), then the engine transmits the inquiry to the realizer.
Answers to inquiries enable the selection of a feature in the visited system.
The interpreter then extracts a set of realization statements associated with the selected feature.
After the execution of these statements by the realizer, the information about the structure of the realized constituent, contained in the blackboard, is updated.
Three kinds of action may be executed by the realizer:  addition of a new constituent with the appropriate semantic information fulfilling a specific function;  updating of the information pertaining to one constituent;  addition of partial ordering constraints that identify the sequence of functions composing the final structure.
This process goes on until no more system can be visited.
The solver then solves the ordering constraints on the blackboard.
We thus obtain a sequence of functions that constitutes the final structure of the realized constituent.
Each of these functions is associated with semantic information extracted from the environment.
For example, the sentence may contain the function Temp_loc (temporal localizer), whose semantic information will be the expression associated with the temporal marker LOC in the input.
If a function is to be lexicalized as a word, the lexicon is consulted to identify the word, taking into account the features selected during the traversal.
Otherwise the grammar is re-entered using the function as the new realized constituent with some features preselected.
For example, to generate the sentence in 2: (2) Jusqu'a 10h50, il a survol6 l'Alg6rie 'until 10:50, it flew over Algeria' the following semantic structure and relations are posted in the environment: message = event(olo) N=n R =09 PERSP = n LOC = end(\[hl, _, _\], inclin(\[t6, minute, 50\], \[t4, hour, lO\])) (a) o10 < n (b) o10 C) tu 108" Gagnon and Lapalme From Conceptual Time to Linguistic Time Positioner \[ Jusque I A A /\ Reference Zon 4 il \ I 1 h lOh50 a survol4 l'Alg6rie Figure 9 The structure of jusqu'd 10h50, il a survold l'Alg&ie.
where o10 must be expressed as an event and tll is a period terminating at 10:50.
The event o10 is before the time of speech and it coincides more or less with the time used as localizer.
The first traversal of the grammar determines the overall structure of the sentence made of the four functions at the top of Figure 9.
As none of them may be lexicalized directly as a word, they must be realized by re-entering the grammar.
We describe only the realization of the function Tempdoc.
The semantics associated with Temp_loc is the value of LOC in the semantic representation.
Traversal of the network for the realization of Temploc results in a structure with two functions: the Positioner and the Reference Zone.
The first function is lexicalized with the preposition jusque; for the second function, the grammar is re-entered, taking as semantics the anchor of the expression associated with Tempdoc: inclin(\[t6,minute,50\], \[t4,hour, lO\]).
Again, this results in a structure with two functions; in this case, both functions can be lexicalized, ending the realization of the function Tempdoc.
The same kind of processing is done for the other functions in the sentence.
5. The Production of ATLs In Section 3.3, we showed how the semantics of ATLs is represented; in this section, we present how ATLs can be lexicalized.
Table 3 gives a list of semantic representations and their translations into ATLs produced by our generator.
As in Section 3.4, we use ttoc for the temporal constant corresponding to the localizer of the occurrence, n for speech time and tref for the reference time.
5.1 Syntactic
Compositions Some ATLs (1-7, 16) are simple, while others (8-15, 17-18) contain an embedded temporal adverbial.
For example, in (11) jusqu'?l mercredi de cette semaine 'until Wednesday of this week' contains another ATL, which itself contains another embedded ATL cette semaine 'this week'.
109 Computational Linguistics Volume 22, Number 1 Table 3 List of adverbial temporal locations.
Semantics ATL (1) relpos(-1, \[tloc, day, _\], incl(\[t2, day, _\], \[n, _, _\])) (2) relpos(-1, \[hoc, daY,-\], incl(\[t2, day, _\], \[tref, _, _\])) (3) incl(\[tloc, day, _\], \[n, _, -\]) (4) incl(\[tloc, month, _\], \[n, _, _\]) (5) incl(\[tloc, month, _\], \[tref, -, -\]) (6) \[tloc,month,April\] (7) \[01,-,-\] (8) inclin(\[tloc,moment-of-day, morning\], inclin(\[t2,season,summer\],\[t3,year,1995\])) (9) inclin(\[ tloc,half-hour,1\],occurrence(ol ) ) (10) duration(3,day) (11) end(\[tloc,_,_\],inclin(\[t2,day, wednesday\], incl(\[t3,week,_\],\[n,_,_\]))) (12) begin(\[tloc,_,_\],relpos(1,\[t2,month,_\], incl (\[ t 3,month,-\],\[ tref,-,-\]) ) ) (13) begin(\[tloc,_,_\],inclin(\[t2,day, lO\],\[t3,month,may\])) (14) relpos(3,\[tloc,day,_\],occurrence(ol )) (15) after(\[tloc,day,_\],\[tref,_,_\],duration(3,day)) (16) after(\[tloc,_,_\],\[tref,_,_\],indefinite) (17) extent(\[tloc,_,_\], inclin(\[t2,day,3\],\[t3,month,april\]), inclin(\[t4,day, lO\],\[ts,month,may\])) (18) extent(\[tloc,_,_\], before(\[t2,-,-\],\[tre f,-,-\],duration(3,day)), \[tre/,-,-l) hier (yesterday) la veille (the day before) aujourd'hui (today) ce mois-ci (this month) ce mois-la (that month) en avril (in April) quand Robert est parti (when Robert left) le matin du 3 avril 1995 (the morning of April 3rd 1995) la premi6re demi-heure de l'6mission (the first half hour of the program) durant trois jours (during three days) jusqu'a mercredi de cette semaine (until Wednesday of this week) a partir du mois suivant (from the following month) depuis le 10 mai (sinceMay lOth) trois jours apr6s le d6part de Robert (three days after Robert's departure) trois jours plus tard (three days later) puis (then) du 3 avril au 10 mai (from April 3rd to May lOth) depuis trois jours (since three days) Unfortunately, the combination of localizers in the semantics does not always correspond to the combination of adverbials.
For example, if there were such a correspondence, the adverbial in (1) would be something like lejour avant lejour qui contient l'instant d'~nonciation 'the day before the day that contains the time of speech'.
Instead, we get the simple adverbial hier 'yesterday'.
For complex semantic expressions, in all examples except (15), (16) and (18), there is one embedded adverbial corresponding to each anchor.
For example, the anchor relpos(1,\[t2,month,_\], incl(\[tB,month,_\], \[tref,-,-\])) in (12) corresponds to le mois suivant 'the following month' in the adverbial.
5 Examples
(1-5) are special, since the relation and the anchor are combined in the same syntactic structure.
In (15), direct translation of the anchor into an adverbial would produce trois jours apr~s ce mornent-l?l 'three days after that moment', and in (18), pendant trois jours jusqu'?~ ce moment-l?l 'during three days until that moment'.
Since there is not always a direct correspondence between semantic and syntactic 5 Note that in the adverbial, du is a contraction of de le.
110 Gagnon and Lapalme From Conceptual Time to Linguistic Time QTY" ANCHORS I unique double ( Figure 10 Section of Pr4texte's grammar for adverbials.
ANCHOR ANCHORI ANCIIOR2 deictic ~ anaphoric autonomous ~ deictic anaphoric autonomous i deictic anaphoric autonomous forms, which one should be used in the grammar to distinguish among ATLs?
We have chosen the semantic form because adverbials are distinguished not only by the number of anchors but also by their nature.
Examples (4) and (5) are both syntactically simple---the anchor in the semantic form is not expressed--but it is the anchor that explains their difference: the first uses speech time, whereas the second uses reference time.
Figure 10 illustrates the part of the network taking into account the combination property.
We first identify the number of anchors.
If there is only one, the feature unique is selected in the system QTY_ANCHORS.
Otherwise, double is selected.
Then, for each anchor, we must establish if it is deictic, anaphoric, or autonomous.
If the localization represented by the anchor is made in relation to the time of speech, deictic is selected; if the localization is made in relation to the reference time, anaphoric is selected; if the anchor achieves a localization without using either of the two temporal markers, autonomous is selected.
In Table 4, we indicate the features selected in the systems of Figure 10, for the production of the adverbials given in Table 3.
Some adverbials may be distinguished using the systems of Figure 10, but Table 4 shows that these systems are not enough.
The features have a strong influence on the most embedded adverbials.
For example, in (11), the selection of deictic results in cette semaine 'this week' for the most embedded adverbial but if the feature anaphoric had been selected, it would have produced cette semaine-lit 'that week'.
These features alone do not explain the recursive form of adverbials.
Figure 11 shows the structure of two adverbials from Table 3.
The structure of adverbial (11) is given in (a).
It has three levels, each one corresponding to one adverbial.
The simple structure of adverbial (4) is shown in (b).
Their difference is not only due to the number of levels in the structure.
In (a), the structure contains a function, the Positioner, that expresses the relation to the anchor; there is no function in (b).
Sometimes an anchor is not realized syntactically at all.
In Figure 12, we consider (13) and (15): the anchor is expressed in (13), shown in (a), but not in (15), shown in (b).
The structure of (15) contains the Positioner and a function conveying the temporal distance to an implicit anchor.
These examples show that features are not sufficient; to determine the syntactic structure of the adverbial, we need more systems in our grammar, such as the network of Figure 7.
The two networks of Figure 7 and Figure 10 must be traversed in parallel.
The system ZONE_DESIGNATION first distinguishes between adverbials that ex111 Computational Linguistics Volume 22, Number 1 Table 4 Distinction of adverbials using the anchor.
(The numbers correspond to the adverbial's position in Table 3).
Adverbial QTY_ANCHORS ANCHOR ANCHOR1 ANCHOR2 (1) hier (3) aujourd'hui (4) ce mois-ci (11) jusqu'h mercredi de cette semaine (2) la veille (5) ce mois-lh (12) a partir du mois suivant (15) trois jours plus tard (16) puis (6) en avril (7) quand Robert est parti (8) le matin du 3 avril 1995 (9) la premi6re demi-heure de l'6mission (10) durant trois jours (13) depuis le 10 rnai (14) trois jours apr6s le d6part de Robert (17) du 3 avril au 10 mai (18) depuis trois jours unique umque unique umque umque unique unique unique umque umque unique unique umque unique umque umque double double deictic deictic deictic deictic anaphoric anaphonc anaphoric anaphoric anaphoric autonomous autonomous autonomous autonomous autonomous autonomous autonomous autonomous anaphonc autonomous anaphoric Positioner \[ Jusque Until Reference Zon~ \ \[Zone Designator\[ Attributor Reference Zon~ h mercredi de \[,-Zone Designator\] Wednesday of / NNN, N cette semaine this week (a) (b) Figure 11 Difference of structure for (11) and (4).
Zone Designator\] /\ ce mois-ci this month press localization directly, as in (1-9), and adverbials that relate it to other localizers.
Selection of direct includes a function Zone Designator in the structure for the most embedded adverbial of (a) and the adverbial of (b) in Figure 11.
This function is realized by a phrase expressing the temporal location, which may be a temporal constant (feature chronological) or an occurrence (feature occurrential).
If relational is selected in ZONE_DESIGNATION, a function Positioner is inserted in the structure.
This function is realized by a phrase that expresses the relation of the localizer to its anchor.
There are two types of relational localizers: those that express a 112 Gagnon and Lapalme From Conceptual Time to Linguistic Time Positioner \[ RefercnceZonc( /\ depuis le 10 mai Since May lOth (a) Figure 12 Difference of structure for (13) and (15).
I T m ora, sta eol pvs,tionor I trois jours plus tard three days later (b) Table 5 Distinction of adverbials with type of designation.
(The numbers correspond to Table 3).
Adverbial ZONE DESIGNATION TYPE LOC ZONE LOCT ASPECT (1) hier direct chronological -(2) la veille direct chronological -(3) aujourd'hui direct chronological -(4) ce mois-ci direct chronological -(5) ce mois-la direct chronological -(6) en avril direct chronological -(8) le matin du 3 avril 1995 direct chronological -(9) la premi6re demi-heure de l'6mission direct chronological -(7) quand Robert est parti direct occurrential -(10) durant trois jours relational -durative (11) jusqu'~ mercredi de cette semaine relational -durative (12) a partir du mois suivant relational -durative (13) depuis le 10 mai relational -durative (17) du 3 avril au 10 mai relational -durative (18) depuis trois jours relational -durative (14) trois jours apr6s le d6part de Robert relational -punctual (15) trois jours plus tard relational -punctual (16) puis relational -punctual duration (see (a) in Figure 12) and those that designate a punctual temporal location (see (a) in Figure 11 and (b) in Figure 12).
The classification of adverbials using these distinctions is shown in Table 5.
But even by combining this classification with that of Table 4, we cannot distinguish between all adverbials.
For example, (1, 3, and 4) select the same features in both networks, as do (2 and 5), (6, 8, and 9), and (15 and 16).
For each of the four cases of Table 5, we will show how the adverbials can be distinguished.
5.2 Relational
Localizers 5.2.1 Punctual Localizers.
The function Positioner is always present in adverbials for which the feature punctual has been selected; this is a consequence of the selection of the feature relational.
In addition to Positioner, there can be two more functions.
One is the Temporal Reference Zone, which conveys the localizer to which the relation expressed by the Positioner pertains.
In our list, only adverbial (14) contains this function: le ddpart de Robert 'Robert's departure'.
The other function is Temporal Distance, which expresses the length of time from the localizer used as anchor.
This function occurs in 113 Computational Linguistics Volume 22, Number 1 Positioner l Puis Then Auparavant Before (a) \[ Positioner \] Temporal Distance \[ Dans trois jours In three days from now I1 y a trois jours Three days ago (b) \[ T~Zone\[ Positioner \] Trois jours pllas tard Three days tater Trois jours av ant Three -days earlier (c) \[ Positioner \] Temp Ref Zone \[ a~re~s le d61~art de Robert R o berY s departure avant 8h00 before 8:00 \] Temporal Distance \] Positioner \[ Temp Ref Zone\[ /\ /\ T,~trOiS jours al~r~s !e d6partde Robert ee aays after Kooert's aeparture Trois jours avant le d6part de Robert Three ~ays before Robert's departure (d) (e) Figure 13 Structure for punctual adverbials.
RELATION TYPE V before punctual L_ after f explicit REFERENCE ZONE E implicit 3-TEMtK)RALDISTANCE F definite ~-indefinite Figure 14 Grammar section for relational punctual adverbials.
adverbials (14) and (15): trois jours 'three days'.
Thus, (14) contains both functions and (15) contains only the Temporal Distance.
Adverbial (16) is distinguished from (14) and (15) because it contains neither of these functions.
In dans trois jours 'in three days from now' and apr~s le 9 octobre 'after October 9th', we find two different structures.
The elements of the first structure are exactly the same as the structure of adverbial (15), but they occur in a different order: Temporal Distance comes before the Positioner.
In the second structure, there is a Temporal Reference Zone, le 9 octobre, but no Temporal Distance.
Thus, for punctual adverbials, there are five possible structures, illustrated in Figure 12.
To distinguish between these adverbials, we use a network, part of which is shown in Figure 13.
Two features are expressed by the system RELATION TYPE: before and after.
To realize the Positioner, there is no need to re-enter the grammar, since it may be found directly in the lexicon.
The lexical choice depends not only on the selection achieved 114 Gagnon and Lapalme From Conceptual Time to Linguistic Time Table 6 List of punctual adverbials.
Adverbial RELATION REFERENCE TEMPORAL TYPE ZONE DISTANCE (14) trois jours apr6s le d6part de Robert after explicit definite (three days after Robert's departure) apr6s le 9 octobre (after October 9th) after explicit indefinite (15) trois jours plus tard (three days later) after implicit definite dans trois jours (three days from now) after implicit definite (16) puis (then) after implicit indefinite trois jours avant le d6part de Robert before explicit definite (three days before Robert's departure) avant 8h00 (before 8:00) before explicit indefinite trois jours avant (three days earlier) before implicit definite il y a trois jours(three days ago) before implicit definite auparavant (before) before implicit indefinite in RELATION TYPE, but also in the choice made in the system ANCHOR of figure 10.
For example, in the cases where after is chosen, the Positioner could be lexicalized as puis 'then' or plus tard 'later'; if anaphoric is chosen in ANCHOR it can be lexicalized as dans 'in' if deictic is chosen or apr~s 'after' if autonomous is chosen.
The fact that Temporal Distance and Temporal Reference Zone are optional in the structure is represented in the grammar by two parallel systems: REFERENCE ZONE and TEMPORAL DISTANCE.
If, in REFERENCE ZONE, explicit is chosen, the function Temporal Reference Zone is included in the structure.
Since this function represents another localizer, the anchor, it is realized by re-entering the grammar, taking as input the semantic representation of this anchor.
In TEMPORAL DISTANCE, the selection of definite results in the inclusion of the function Temporal Distance.
To realize it, the grammar must be re-entered, and some features must be preselected so that it is realized as a noun phrase.
Table 6 lists all possible adverbials represented by the network of Figure 13 together with their selected features.
The three adverbials taken from Table 3 are preceded by their reference number to ease the comparison of their semantics with the selected features; their relations will be discussed later.
The distinction between structures (b) and (c) in Figure 12 is not explained by the grammar section shown in Figure 13, since the same features are selected for trois jours plus tard and dans trois jours.
However, structure (b) in Figure 12 is found only for deictic localizers.
Therefore, features for deictic localizers selected in the system ANCHOR, will distinguish structure (b) from structure (c).
Let us now see how the features are selected for the production of relational punctual localizers.
In RELATION TYPE, the feature reflects the relation used in the semantics, if this relation is before or after.
Adverbial (14) deserves some explanation.
Since its semantic expression uses the relation relpos, we would expect its syntactic realization to be: le troisi~me jour apr~s le jour du d6part de Robert 'the third day after the day of Robert's departure', but this usage is rare.
Instead we find trois jours apr~s le d6part de Robert 'three days after Robert's departure', which is what we would expect if the semantic expression used the relation after.
This seems to be because if the temporal distance is one unit, a direct localizer is preferred.
So, instead of generating un mois plus tard 'one month later', we produce le mois suivant 'the next month'.
Our intuition 115 Computational Linguistics Volume 22, Number 1 durative DURATION TYPE bound quantified DURATION ANCHOR DURATION PERSPECTWE -anterior posterior double QUANTDURATION E anterior ANCHOR posterior nil Figure 15 Grammar section for relational durative adverbials.
-internal external \[P.~i--, \] D~oo Q~,ty i l I ''ia'= \] Bo.d~, \] ..~e~ ~ays . Jusque Is mercredi de three days Sm.ce for ~rom that moment) A r~oir de le 10 mai may lOth (a) (b) Figure 16 Structure for durative localizers.
Du 3 avril au lOmai From April 3rd to May 10th (c) is that when X is '%ig" we have this equivalence: relpos(X, \[ti, Ti, Ni\], Z) ::~ afler(\[ti, Ti, Ni\], Z, duration(X, Zi) ) and similarly for X negative and the relation before.
More study is needed to determine the threshold at which the two relations become equivalent in the linguistic realization.
We are sure that for X = I or X = -1, they are not equivalent, so in our implementation, we use 2 and -2 as thresholds.
In the system REFERENCE ZONE, the feature implicit is chosen if the anchor is a simple localizer using the reference time or the time of speech, otherwise explicit is chosen.
Feature selection in TEMPORAL DISTANCE depends on whether the third argument in the semantic expression is indefinite or a specified duration.
5.2.2 Durative
Adverbials.
We now show how the durative adverbials of Table 5 can be differentiated.
The part of the network that generates these adverbials is shown in Figure 14.
We give the three kinds of structure identified for these adverbials in Figure 15, and, finally, in Table 7 we list the durative adverbials of Table 5 with their corresponding features according to the systems of Figure 14.
To give a complete illustration of all adverbials generated with the network of Figure 14, we added one adverbial to the list: pendant trois jours 'for three days from now', which is symmetrical to depuis trois jours 'for three days until now'.
In Figure 14, we distinguish two types of durative adverbial phrases: bound, if the duration is expressed by specifying one or two of its boundaries; and quantified, 116 Gagnon and Lapalme From Conceptual Time to Linguistic Time Table 7 List of durative adverbials.
Adverbial DUR.
DUR. DUR.
QUANZ TYPE ANCH.
PERSE DUR.
ANCH. (13) depuis le 10 mai (since May lOth) bound ant.
int. -(12) a partir du rnois suivant bound ant.
ext. -(from next month) (11) jusqu'~ mercredi de cette semaine bound post.
--(until Wednesday of this week) (17) du 3 avril au 10 rnai bound double --(from April 3rd to May lOth) -pendant trois jours quant.
--ant.
(for three days until that moment) (18) depuis trois jours quant.
--post.
(for three days from that moment) (10) durant trois jours (during three days) quant.
--nil if the duration is expressed as a quantity of time units.
In the first case, we get a structure such as (b) or (c) in Figure 15.
In (b) there is only one boundary, and the Positioner indicates which one is used: its lexicalization depends on the feature selected in DURATION ANCHOR.
If anterior is selected, one further distinction is required to lexicalize the Positioner, as represented by the system DURATION PERSPECTIVE.
The Positioner is realized by the phrase ~ partir de if the feature chosen is external, otherwise it is realized as depuis.
The choice depends on the aspect of the occurrence reported: the feature external is chosen if the occurrence is presented as an event, and internal is chosen if the occurrence is presented as a situation.
These two cases are exemplified in the following two sentences: (3) a.
A partir de 1972, il enseigna a l'Universit6 de Montr6al.
'From 1972 on, he taught at Universit6 de Montr6al'.
b. Depuis 1972, il enseignait a l'Universit6 de Montr6al.
'Since 1972 he was teaching at Universit6 de Montr6al'.
In (3a), since the occurrence is presented as an event, the feature external is selected during the determination of the ATL, thus resulting in the form ~ partir de 1972 'from 1972'.
In (3b), the same occurrence is presented as a situation considered from the reference time.
Thus, internal is selected, resulting in the form depuis 1972 'since 1972'.
This is a good example of the interaction of ATLs with the aspect of the occurrence.
In the interpretation of the ATL in (3b), the duration is anchored not only on the year 1972 but also on the reference time included in the occurrence.
But in the semantics of the ATL, which uses the relation begin, as well as ATL (12) in Table 3, there is only one anchor.
Even if the reference time is involved in the understanding of the whole sentence, it is not directly expressed in the semantics of the ATL.
An alternative would be to express the same localization using the relation extent, as in (18) of Table 3.
If the reference time is included in 1982 (the beginning of the duration expressed in (3b) thus being 10 years before), the semantics would be: extent(\[hoc,_,_\],before(\[tl,year, d,\[t2,_,-\],duration(lO,year)), incl(\[ta,year, d,\[tref,_,_\])) 117 Computational Linguistics Volume 22, Number 1 In depuis dix ans 'For 10 years', the meaning of the ATL, which is "since 10 years in the past starting from this moment" requires the use of the reference time.
If, in the system DURATION ANCHOR, the feature double is chosen, we get a structure containing two boundaries, as in (c) in Figure 15.
A boundary, in the structure of a bound localizer, is always realized as a temporal adverbial, by re-entering the grammar.
When the feature quantified is selected, the structure in (a) of Figure 15 is obtained.
To realize the Positioner in this case, another system is required, because the quantity of time that constitutes the duration can be worded in many ways.
We can express the duration of the occurrence without giving any hint about its location in time, as in durant trois jours 'during three days', or we can indicate a duration that begins or ends at some time.
To see how the features are selected in the grammar section of Figure 14, compare the adverbials of Table 7 with their semantics as given in Table 3 (the semantics for pendant trois jours is the same as depuis trois jours, but the relation after is substituted before and the two anchors are reversed).
First, the feature bound in DURATION TYPE is selected if the relation used in the semantic representation is either begin or end, or if it is extent and the two anchors are autonomous.
In DURATION ANCHOR, features corresponding to these three cases are selected.
In DURATION PERSPECTIVE, the selection depends on the aspect of the occurrence reported in the sentence.
In DURATION TYPE, quantified is selected if the semantics uses the relation extent and one anchor is deictic or anaphoric, as in our examples, or if it uses the relation duration.
In the first case, the selection depends on the position of the anaphoric or deictic anchor in the expression.
If the relation duration is used, since there is no anchor, the feature nil is selected in the next system.
5.3 Direct
Localizers We complete our discussion of adverbials by explaining how the direct localizers can be differentiated.
Figure 16 shows the structure of the direct adverbials that constitute the first half of Table 5; there are three possible structures for a direct localizer.
The simplest ones, in (a) and (b), contain only one function, Zone Designator, that expresses the temporal location zone designated by the adverbial.
This function is realized directly by an adverb, in (a), using the lexicon depending on the system ANCHOR of Figure 10.
In (b), the grammar must be re-entered to generate a nominal phrase whose form also depends on the choice in ANCHOR.
In some cases, it is not sufficient to specify a temporal location zone: we must also add what we call a Pointer to relate the occurrence with this zone.
In our examples, the Pointer indicates that the occurrence takes place during the month of April, or when Robert left.
The existence of such a localizer in the structure seems to depend on the level of the adverbial in the embedding structure.
For example, we find a Pointer in the adverbial ~ ShOO 'at 8:00' if it is used alone, but not if it is embedded in another adverbial, like depuis 8hO0 'since 8:00'.
Our approach to this problem may be contrasted with Forster's (1989), who determines the realization of the Pointer by the temporal aspect of the Zone Designator (durative or punctual) using a constraint propagation technique.
Other possible structures for direct localizers are illustrated in (d) and (e) in Figure 16.
One function is the Zone Designator, which designates the direct expression of temporal zone.
If this zone is included in another localizer or in a position relative to another localizer, we must include another function in the structure: the Reference Zone, which corresponds to this second localizer.
The Attributor links these two functions.
In (e), le matin du 3 avril 1995 'the morning of April 3rd 1995' directly expresses 118 Gagnon and Lapalme From Conceptual Time to Linguistic Time Izo=~i~o~l Iz/;n~sign~,rl \[Po~i~ tznem~'a~"q /X /X Aujourd'hui Ce mois-ci En avril Today This month In April l-Iier Ce mois-l~t Quand Robert est parti Yesterday That month When Robert lift La veille The day before (a) Co) (c) I ZoneDesignat~rAttributor \] ReferenceZonb \[Zone Designatot de I Zne DesignatOr l /\ La premi&e demi-heure 1'6mission The first half-hour the program (d) I ZneDesignat1 Attributor Referen.eZorle / Zone Designatoq Auributor I Kelerence z.o~e /\ I d~ | \[ ZoneDestgnator I Attn0utor li~c~crencez-on~ \[ Zone Designator I of I the3rd I avril ~ t ator I ! Le matin I April ~ I j//N'N I The morning ! I 1995 I I=_-I I ............
=_----:-'---(e) Figure 17 Structure for direct localizers.
a morning.
This morning is itself part of another localizer, and so on.
The Attributor is sometimes lexicalized as an empty item.
To these direct localizers, we must add the embedded direct adverbials found in the relational adverbials of table 5.
In Figure 17, we use by dashed-line boxes to indicate those that differ from Figure 16.
The system shown in Figure 18 differentiates among these different forms of direct adverbials.
For occurrential adverbials, for example, the second adverbial of (b) in Figure 16 and the embedded adverbials in (d) in Figure 16 and (b) in Figure 17, the occurrence may be nominalized or not.
We do not have any satisfying answers to the question of how to choose between these two possibilities.
We will state only that when the adverbial is embedded, a nominalized form may be preferred to another embedded adverbial.
For chronological adverbials, the system AUTONOMOUS ZONE distinguishes between those that have an anaphoric or deictic temporal location zone, and those for which the temporal location zone is autonomous.
Adverbials of the first type always have a simple structure: aujourd'hui, hier, demain, ce mois-ci, ce mois-lh, cette semaine, le mois suivant.
The temporal location zone is different from the anchor.
In mercredi de cette semaine "Wednesday of this week', the temporal location zone, expressed by mercredi, is autonomous whereas the anchor expressed by cette semaine is deictic.
When the network is traversed the first time, yes is selected in the system AUTONOMOUS ZONE.
Its only in the second traversal, when cette semaine is generated, that no is selected in this system.
We must further distinguish adverbials with an autonomous temporal location zone, by deciding if their structure contains a Reference Zone or not.
The feature 119 Computational Linguistics Volume 22, Number 1 I Bouna y I Jusque IZoneDesignatorl Attributor I Referencezon4 Until / \[ Pointer \[Zone Designator I /\ a mercredi Wednesday de of ! r ..... .T ......
V \[Zone Designator \] /\ cette semaine this week ........... i ..................................
! (a) \[ Temporal Distance I Positioner \] Temp Ref Zone I /\ Trois jours Three days apr~s after \[Zone Designator \[ /\ le d6part de Robert Robert's departure I Positionerl Boundary \] A partir de \[Zone Designator \[ Fro,.
/~ Depuis From le mois prochain the next month le 10 mai May lOth ...........
_1 (b) (c) Figure 18 Structure for direct localizers.
TYPE_LOC_ZONE AUTONOMOUS no I chronological ZONE E yes hNCLUD1NG TIME implicit E explicit nominalised occurrential NOMINAL not nominalized Figure 19 Grammar section for direct adverbials.
implicit, implying the non-existence of Reference Zone, is selected in the system INCLUDING TIME if the semantic form is a single triplet \[ti Type, Naming\].
There is yet another system that decides if there is a Pointer or not, but as the problem of the existence of the Pointer is not completely solved and not really important to our discussion, we do not consider this system here.
120 Gagnon and Lapalme From Conceptual Time to Linguistic Time Table 8 List of durative adverbials.
Adverbial AUTONOMOUS INCLUDING ZONE TIME aujourd'hui (today) no hier (yesterday) no demain (tomorrow) no ce mois-ci (this month) no ce mois-l& (that month) no le mois suivant (the following month) no en avril (in April) yes mercredi de cette semaine (Wednesday of this week) yes cette semaine (this week) no la premi6re demi-heure de l'6mission yes (the first half-hour of the program) le matin du 3 avril 1995 (the morning of April 3rd 1995) yes le 3 avril 1995 yes avril 1995 yes 1995 yes implicit explicit explicit explicit explicit explicit implicit In Table 8, we present the features selected for direct chronological adverbials, embedded or not.
The systems in Figure 19 do not suffice to distinguish all direct adverbials.
The selections in these systems must be combined with the selections made in the systems of Figure 10.
5.4 Related
Work on the Generation of ATLs The problem of temporal localization has already been studied by many researchers, but most of them have focused on the aspectual interaction of the adverbials with verb tense; the problem of the semantic and syntactic structures of ATLs has been neglected.
Molin6s' (1990) study from a linguistic perspective characterizes the adverbials based on noun phrases.
Our work extends hers because our computational perspective has made us go farther in the formalization.
Bras and Molin6s (1993) made a similar attempt, but from the perspective of discourse understanding.
Since the problems of understanding are very different from the problems of generation, we could not simply use their method in a "reversed mode".
Their method relies on a compositional analysis of the language, where all information units extracted from the semantic structure are combined to select one meaning for the adverbial.
This compositional approach is not easily reversible, and it does not provide any insight into the selection problem inherent to the generation task.
Ehrich (1987) classifies adverbials in the context of generation, but she does not cover all the cases presented in this section.
Concerning the problem of the generation of ATLs, Maybury (1991) shows how the notion of focus as used by McKeown (1985) can be extended to include a temporal focus that corresponds essentially to the reference point in the Reichenbach model (1947).
An operation on the temporal focus, in combination with the value of speech and event times, selects the temporal adverbial and the verb tense.
Since the emphasis in this work was on the planning aspect of the task, the variety of adverbials that can be generated is limited.
Forster (1989) explains how the syntactic structure of a temporal adverbial may be controlled by semantic information such as the durative or punctual nature of the localizer.
Essential134 the final structure is obtained by propagating constraints associated with each syntactic subpart of the structure.
In particular, he focuses on the interaction between prepositional phrases and noun phrases.
For example, the preposition on is 121 Computational Linguistics Volume 22, Number 1 selected in on Sunday because Sunday is identified as a punctual localizer; this rules out in, which implies a durative localizer.
We have already presented one problem with this approach: it is not clear how the choice of these prepositions can be achieved by propagating semantic constraints.
The choice of preposition in French is very different from English and it often appears to be arbitrary or conventional.
Furthermore, many aspects of the problem are neglected, such as the type of reference expressed by the adverbial: it is not clear how Forster's system can represent the distinctions between anaphoric, deictic and autonomous localizers because the link between the semantic and syntactic levels is not fully explained.
Nigel (Matthiessen and Bateman 1991) offers the widest coverage of English but the variety of forms for ATLs is quite limited.
The temporal localization that may be expressed by different types of syntactic structures is represented in Nigel by systems dispersed throughout the whole grammar network.
For the expression of temporal localizers, their grammar is more dependent on the syntactic structure than ours, which is mainly determined by the semantics.
To summarize, our approach departs from previous approaches by covering more types of adverbials, by proposing a semantics for localization, and by explaining in detail how the different syntactic structures may be obtained from this semantics.
6. The Production of Verb Phrases In our work, we have focused on the generation of adverbials because we felt this problem had not received enough attention and because the temporal localization achieved by ATLs is more complex and more diversified than that expressed by verb tenses.
To generate a discourse like Discourse 1, however, we cannot avoid the problem of determining the structure of the verb phrase, because part of the localization is achieved by the verb, and because of the relations between verbs and adverbials.
In our implemention of the expression of temporal localization, the relation between the verb and the adverbial is taken into account mainly in the deep generation process.
In the semantic representation, we find traces of this interaction.
By keeping these decisions in the deep generation process, the verb phrase and the ATL can be generated independently.
Our method for generating the verb phrase takes advantage of the kind of information directly represented in DRT: the relation of the occurrence to speech time, which we call the primary localization, the aspect of the occurrence, and the presence or absence of a perspective point.
It is implemented by the grammar section illustrated in Figure 19.
In Prdtexte, the production of verb phrases requires many traversals of the network.
First, when the structure of the sentence is determined, choices are made regarding localization, aspect, and perspective.
After a first traversal of the network, the sentence's structure contains a function called Predicate, realized as a verb phrase.
The grammar must be re-entered to realize the Predicate.
The systems visited during this second traversal (not shown here) classify verb tenses in French.
Most of the selections during this second traversal were preselected during the first traversal.
For each verb tense, there is one associated structure, which contains a main verb and one or two auxiliaries.
To generate each verb or auxiliary, another traversal is needed.
In the first system of Figure 19, PRIMARY LOC, the selection depends on the temporal relation between the localized occurrence and the speech time.
The features of the systems ASPECT and SIT TYPE reflect the value of aspect in the semantic representation.
If the aspect is event, the system PERSPECTIVE determines if this event is presented using a perspective.
If there is one, another choice must be made regarding its type.
122 Gagnon and Lapalme From Conceptual Time to Linguistic Time,---past /I'~YL~ ~ present situation ASI~CT event ~RS~CTIVE E resultirlg open I.
Figure 20 Generation of verb phrase--sentence level.
perspective no perspective PERSPECTIVfi a n teri ority TYPE posteriority Table 9 Production of VP--examples.
Selections during first traversal Tense selected during Example second traversal past situation resulting past situation open past event perspective anteriority past event perspective posteriority past event no perspective plus-que-parfait pass4 ant6rieur imparfait imparfait conditionnel plus-que-parfait pass6 compos6 A 8h00, il avait terrain4.
(At 8:00, he had finished).
Une fois qu'il eut termin4 (Once he had finished) A 8h00, Robert regardait la t616vision.
(At 8:00, Robert was watching television).
J'ai rencontr4 Robert jeudi dernier.
I1 partait le lendemain.
(I met Robert last Thursday.
He was going to leave the day after).
J'ai rencontr6 Robert jeudi dernier.
I1 m'a dit qu'il partirait le lendemain.
(I met Robert last Thursday.
He told me that he would leave the next day).
J'ai rencontr6 Robert jeudi dernier.
I1 4tait arriv4 la veille.
(I met Robert last Thursday.
He had arrived the day before).
Robert a parl4 ~ Marie.
(Robert talked to Marie).
Table 9 shows examples of verb phrases, including the list of features selected for each example during the two traversals.
The same tense can be used for different feature patterns.
This is the case with the imparfait and the plus-que-parfait: the imparfait expresses an open situation or an anterior perspective, while the plus-que-parfait presents a resulting situation or a posterior perspective.
This may be a problem in an understanding process, since it is a source of ambiguity, but not in a generation process since it does not matter if two different inputs map into the same syntactic structure.
More than one verb tense may be used for the same features.
This means that our grammar is not complete: more systems would be needed to distinguish among these different cases.
For example, to distinguish the two tenses used with the first feature pattern of Table 9, we would have to augment the grammar section of Figure 19 123 Computational Linguistics Volume 22, Number 1 to determine if the verb phrase is part of a temporal adverbial or not.
For the two cases in the third feature pattern, the difference relates to the use of indirect discourse.
Here, not only would the grammar have to be modified, but so would the semantic representation, to take into account indirect speech.
In Discourse 1, this problem is not apparent: all verb tenses used are distinguished in our grammar because we limited ourselves to a subset of the data.
Thus, the production of VP is more complex than what we have implemented and we have not completely identified all the rules for the selection of verb tense: indirect discourse is not implemented and we have not identified how modal information can be used to select forms such as the subjunctive and the conditional.
But our approach is a good start and it could be extended by adding more systems and their selection rules, without changing the overall structure of the network.
We can see from the approximate translations given in Table 9 that the systems for generating French and English verb tenses differ greatly.
For English verb tenses, the method implemented in Nigel resorts to a recursive semantics involving temporal markers, as proposed by Halliday (Matthiessen and Bateman 1991).
The purpose in this approach is to deal correctly with complex structures, such as will have been eating.
Put simply, the idea is that each auxiliary reflects a relation between two temporal markers.
This suggests a network that displays a recursive process.
Thus, the phrase will have been going to eat would be represented semantically as to eat at a time that is in the future relative to another time that is in the past relative to a time that is in the future relative to speech time.
This method may be adequate for English, since it seems to capture the recursive structure of verb tenses; but in French, this recursive structure is not found.
Furthermore, nothing is said about how a deep generation process could produce the corresponding semantic structure with the intermediate temporal markers.
In fact, we are not convinced that this could be easily done.
Rather, we think that it is the overall structure that is selected for a particular usage.
This completes our brief description of the generation of verb tenses.
We have not completely solved the problem.
In particular, we have chosen to put most of the problems pertaining to verb tense in the deep generation process, in order to facilitate their generation at the surface level.
This approach greatly simplifies the process and our grammar could be easily completed to encompass all cases.
Once the semantic demands are better understood, it should be easier to solve the problem of deep generation.
7. Conclusion and Future Work In this paper, we have presented a method that has been successfully used to produce text conveying temporal information.
Our method combines the principles of two theories: Kamp's Discourse Representation Theory, which guides the expression of temporal information, and Halliday's Systemic Functional Grammar, which provides a generation process controlled by a set of semantic choices, with the syntactic form resulting from these choices.
We argued for the use of a conceptual structure, a Discourse Representation Structure, combined with rhetorical principles and pragmatic information, and for its translation into a semantic structure that is easily realized syntactically.
The deep generation process is hard to implement, mainly because of the difficulty in formalizing this information.
Since we assume that the deep generation 124 References 1 James F.
Allen, Maintaining knowledge about temporal intervals, Communications of the ACM, v.26 n.11, p.832-843, Nov.
1983 2 Asher, Nicholas.
(1993). Reference to Abstract Objects in Discourse.
Kluwer Academics, Dordrecht.
3 Bach, Emmon.
(1986). The Algebra of Events.
Linguistics and Philosophy, 9(1): 5--16.
4 Berry, M.
(1975). An Introduction to Systemic Linguistics, vol.
1 Structures
and Systems.
St. Martin Press, New York.
5 Orna
Berry, Mathias Hein, Ellon Littwitz, Introduction to Ethernet Switching, International Thomson Computer Press, Boston, MA, 1995 6 Bras, M.
(1990). Calcul des structures temporelles du discours.
Thse de doctorat, Universit Paul-Sabatier.
7 Bras, Myriam and Nicholas Asher.
(1994). Le raisonnement non monotone dans la construction de la structure temporelle de textes en franais.
In 9me congrs AFCET-RFIA.
Paris. 8 Bras, Myriam and Molins, Frdrique.
(1993). "Adverbials of Temporal Location: Linguistic Description and Automatic Processing".
In Sprache Kommunikation Informatik.
Linguistiche Arbeiten 293, edited by J.
Darski and Z.
Vetulani. Max Niemeyer Verlag, Tubingen.
9 Davidson, D.
(1967). "The Logical Form of Action Sentences".
In Essays on Action and Events, edited by D.
Davidson. Clarendon Press.
10 Dowty, D.
(1979). Word Meaning and Montague Grammar.
Reidel, Dordrecht.
11 Dowty, D.
(1982). Tenses, Time Adverbs, and Compositional Semantic Theory.
Linguistics and Philosophy, 5(1): 23--55.
12 Dowty, D.
(1986). The Effects of Aspectual Class on the Temporal Structure of Discourse: Semantics or Pragramatics?
Linguistics and Philosophy, 9: 37--61.
13 Ehrich, Veronika.
(1987). "The generation of tense".
In Natural Language Generation: New Results in Artificial Intelligence, Psychology and Linguistics, edited by Gerard Kempen.
Martinus Nijhoff Publishers, Boston, Dordrecht, 423--440.
14 Fawcett, R.
(1988). "Language Generation as Choice in Social Interaction".
In Advances in Natural Language Generation---An Interdisciplinary Perspective, edited by Michael Zock and Gerard Sabah.
Pinter Publishers, London, 27--48.
15 Forster, D.
(1989). Generating Temporal Expressions in Natural Language.
In Proceedings, 11th Annual Conference of the Cognitive Science Society.
16 Gagnon, M., and Lapalme, G.
(1992). Un gnrateur de texte exprimant des concepts temporels.
Technique et science informatiques, 11(2): 25--44.
17 Gagnon, Michel and Bras, Myriam.
(1994). "Discourse Interpretation and Time Representation".
Technical report 94/54-r, IRIT.
18 Halliday, M.
(1985). An Introduction to Functional Grammar.
Edward Arnold, London.
19 Hovy, Eduard H.
(1991). "Approaches to Planning of Coherent Text".
In Natural Language Generation in Artificial Intelligence and Computational Linguistics, edited by W.
Swartout and W.
Mann. Kluwer Academics Publishers, Boston, 83--102.
20 Kamp, H.
(1979). "Events, Instants and Temporal Reference".
In Semantics from different points of view, edited by R.
Bauerle, U.
Egli, and A.
von Stechow.
Springer Verlag, Berlin, 376--417.
21 Kamp, H.
(1981). Evnements, reprsentations discursives et rfrence temporelle.
Langages, 64: 34--64.
22 Lascarides, Alex, and Asher, Nicholas.
(1993). Temporal Interpretation, Discourse Relations and Commonsense Entailment.
Linguistics and Philosophy, 16: 437--493.
23 Mann, W.
(1983). "An Overview of the Nigel Generation Grammar".
Technical report ISI-RR-83-113, USC/ISI.
24 Mann, W.
(1985). An Introduction to the Nigel Text Generation System.
In Systemic Perspective on Discourse, vol.
1, edited by James D.
Benson and William S.
Greaves. Ablex, Norwood, 84--95.
25 Mann, W., and Thompson, S.
(1987). "Rhetorical Structure Theory: Description and Construction of Text Structure".
In Natural Language Generation, edited by Gerard Kempen.
Martinus Nijhoff, Dordrecht, 85--95.
26 William C.
Mann, Discourse structures for text generation, Proceedings of the 22nd annual meeting on Association for Computational Linguistics, p.367-375, July 02-06, 1984, Stanford, California 27 Matthiessen, C.
(1985). The Systemic Framework in Text Generation: Nigel.
In Systemic Perspective on Discourse, vol.
1, edited by James D.
Benson and William S.
Greaves. Ablex, Norwood, 96--118.
28 Matthiessen, C., and Bateman, J.
(1991). Text Generation and Systemic-Functional Linguistics.
Pinter, London.
29 Maybury, Mark T.
(1991). Topical, Temporal, and Spatial Constraints on Linguistic Realization.
Computational intelligence, 7(4): 266--275.
30 Kathleen R.
McKeown, Discourse strategies for generating natural-language text, Artificial Intelligence, v.27 n.1, p.1-41, Sept.
1985 31 Molins, F.
(1990). Acceptabilit et interprtation des adverbiaux de localisation temporelle.
Mmorie de D.E.A., Universit de Toulouse---Le Mirail.
32 Reichenbach, H.
(1947). Elements of Symbolic Logic.
McMillan, New York.
33 Verkuyl, H.
J. (1989).
Aspectual Classes and Aspectual Composition.
Linguistics and Philosophy, 12: 39--94.
34 Vlach, Frank.
(1993). Temporal Adverbials, Tenses and the Perfect.
Linguistics and Philosophy, 16: 231--283.
35 Winograd, T.
(1983). Language as a Cognitive Process.
Addison Wesley .
Finite-State Transducers in Language and Speech Processing Mehryar Mohri* AT&T Labs-Research Finite-state machines have been used in various domains of natural language processing.
We consider here the use of a type of transducer that supports very efficient programs: sequential transducers.
We recall classical theorems and give new ones characterizing sequential string-tostring transducers.
Transducers that output weights also play an important role in language and speech processing.
We give a specific study of string-to-weight transducers, including algorithms for determinizing and minimizing these transducers very efficiently, and characterizations of the transducers admitting determinization and the corresponding algorithms.
Some applications of these algorithms in speech recognition are described and illustrated.
1. Introduction Finite-state machines have been used in many areas of computational linguistics.
Their use can be justified by both linguistic and computational arguments.
Linguistically, finite automata are convenient since they allow one to describe easily most of the relevant local phenomena encountered in the empirical study of language.
They often lead to a compact representation of lexical rules, or idioms and cliches, that appears natural to linguists (Gross 1989).
Graphic tools also allow one to visualize and modify automata, which helps in correcting and completing a grammar.
Other more general phenomena, such as parsing context-free grammars, can also be dealt with using finitestate machines such as RTN's (Woods 1970).
Moreover, the underlying mechanisms in most of the methods used in parsing are related to automata.
From the computational point of view, the use of finite-state machines is mainly motivated by considerations of time and space efficiency.
Time efficiency is usually achieved using deterministic automata.
The output of deterministic machines depends, in general linearly, only on the input size and can therefore be considered optimal from this point of view.
Space efficiency is achieved with classical minimization algorithms (Aho, Hopcroft, and Ullman 1974) for deterministic automata.
Applications such as compiler construction have shown deterministic finite automata to be very efficient in practice (Aho, Sethi, and Ullman 1986).
Finite automata now also constitute a rich chapter of theoretical computer science (Perrin 1990).
Their recent applications in natural language processing, which range from the construction of lexical analyzers (Silverztein 1993) and the compilation of morphological and phonological rules (Kaplan and Kay 1994; Karttunen, Kaplan and Zaenen 1992) to speech processing (Mohri, Pereira, and Riley 1996) show the usefulness of finite-state machines in many areas.
In this paper, we provide theoretical and algorithmic bases for the use and application of the devices that support very efficient programs: sequential transducers.
* 600 Mountain Avenue, Murray Hill, NJ 07974, USA.
(~) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 2 We extend the idea of deterministic automata to transducers with deterministic input, that is, machines that produce output strings or weights in addition to (deterministically) accepting input.
Thus, we describe methods consistent with the initial reasons for using finite-state machines, in particular the time efficiency of deterministic machines, and the space efficiency achievable with new minimization algorithms for sequential transducers.
Both time and space concerns are important when dealing with language.
Indeed, one of the recent trends in language studies is a large increase in the size of data sets.
Lexical approaches have been shown to be the most appropriate in many areas of computational linguistics ranging from large-scale dictionaries in morphology to large lexical grammars in syntax.
The effect of the size increase on time and space efficiency is probably the main computational problem of language processing.
The use of finite-state machines in natural language processing is certainly not new.
The limitations of the corresponding techniques, however, are pointed out more often than their advantages, probably because recent work in this field is not yet described in computer science textbooks.
Sequential finite-state transducers are now used in all areas of computational linguistics.
In the following sections, we give an extended description of these devices.
We first consider string-to-string transducers, which have been successfully used in the representation of large-scale dictionaries, computational morphology, and local grammars and syntax, and describe the theoretical bases for their use.
In particular, we recall classical theorems and provide some new ones characterizing these transducers.
We then consider the case of sequential string-to-weight transducers.
Language models, phone lattices, and word lattices are among the objects that can be represented by these transducers, making them very interesting from the point of view of speech processing.
We give new theorems extending the known characterizations of stringto-string transducers to these transducers.
We define an algorithm for determinizing string-to-weight transducers, characterize the unambiguous transducers admitting determinization, and describe an algorithm to test determinizability.
We also give an algorithm to minimize sequential transducers that has a complexity equivalent to that of classical automata minimization and that is very efficient in practice.
Under certain restrictions, the minimization of sequential string-to-weight transducers can also be performed using the determinization algorithm.
We describe the corresponding algorithm and give the proof of its correctness in the appendix.
We have used most of these algorithms in speech processing.
In the last section, we describe some applications of determinization and minimization of string-to-weight transducers in speech recognition, illustrating them with several results that show them to be very efficient.
Our implementation of the determinization is such that it can be used on the fly: only the necessary part of the transducer needs to be expanded.
This plays an important role in the space and time efficiency of speech recognition.
The reduction in the size of word lattices that these algorithms provide sheds new light on the complexity of the networks involved in speech processing.
2. Sequential String-to-String Transducers Sequential string-to-string transducers are used in various areas of natural language processing.
Both determinization (Mohri 1994c) and minimization algorithms (Mohri 1994b) have been defined for the class of p-subsequential transducers, which includes sequential string-to-string transducers.
In this section, the theoretical basis of the use of sequential transducers is described.
Classical and new theorems help to indicate the usefulness of these devices as well as their characterization.
270 Mohri Transducers in Language and Speech b:E Figure 1 Example of a sequential transducer.
2.1 Sequential
Transducers We consider here sequential transducers, namel3~ transducers with a deterministic input.
At any state of such transducers, at most one outgoing arc is labeled with a given element of the alphabet.
Figure 1 gives an example of a sequential transducer.
Notice that output labels might be strings, including the empty string ~.
The empty string is not allowed on input, however.
The output of a sequential transducer is not necessarily deterministic.
The one in Figure 1 is not since, for instance, two distinct arcs with output labels b leave the state 0.
Sequential transducers are computationally interesting because their use with a given input does not depend on the size of the transducer but only on the size of the input.
Since using a sequential transducer with a given input consists of following the only path corresponding to the input string and in writing consecutive output labels along this path, the total computational time is linear in the size of the input, if we consider that the cost of copying out each output label does not depend on its length.
Definition More formally, a sequential string-to-string transducer T is a 7-tuple (Q, i, F, G, A, 6, or), with:  Q the set of states,  i E Q the initial state,  F c Q the set of final states,  ~ and A finite sets corresponding respectively to the input and output alphabets of the transducer,  6 the state transition function, which maps Q x G to Q,   the output function, which maps Q x G to A'.
The functions 6 and rr are generally partial functions: a state q c Q does not necessarily admit outgoing transitions labeled on the input side with all elements of the alphabet.
These functions can be extended to mappings from Q x G* by the following classical recurrence relations: Vs E Q, Vw E ~*,Va c G, 6(s,) = s, 6(s, wa) = 6(6(s,w),a); = wa) = 271 Computational Linguistics Volume 23, Number 2 Figure 2 Example of a 2-subsequential transducer T1.
Thus, a string w E ~* is accepted by Tiff 6(i, w) C F, and in that case the output of the transducer is or(i, w).
2.2 Subsequential
and p-Subsequential Transducers Sequential transducers can be generalized by introducing the possibility of generating an additional output string at final states (Sch~itzenberger 1977).
The application of the transducer to a string can then possibly finish with the concatenation of such an output string to the usual output.
Such transducers are called subsequential transducers.
Language processing often requires a more general extension.
Indeed, the ambiguities encountered in language--ambiguity of grammars, of morphological analyzers, or that of pronunciation dictionaries, for instance---cannot be taken into account when using sequential or subsequential transducers.
These devices associate at most a single output to a given input.
In order to deal with ambiguities, one can introduce p-subsequential transducers (Mohri 1994a), namely transducers provided with at most p final output strings at each final state.
Figure 2 gives an example of a 2-subsequential transducer.
H~re, the input string w = aa gives two distinct outputs aaa and aab.
Since one cannot find any reasonable case in language in which the number of ambiguities would be infinite, p-subsequential transducers seem to be sufficient for describing linguistic ambiguities.
However, the number of ambiguities could be very large in some cases.
Notice that 1-subsequential transducers are exactly the subsequential transducers.
Transducers can be considered to represent mappings from strings to strings.
As such, they admit the composition operation defined for mappings, a useful operation that allows the construction of more complex transducers from simpler ones.
The result of the application of T 20 T 1 to a string s can be computed by first considering all output strings associated with the input s in the transducer T1, then applying T2 to all of these strings.
The output strings obtained after this application represent the result (T2 o T1)(S).
In fact, instead of waiting for the result of the application of T1 to be completely given, one can gradually apply T2 to the output strings of ~-1 yet to be completed.
This is the basic idea of the composition algorithm, which allows the transducer T 2 o T 1 to be directly constructed given T1 and 72.
We define sequential (resp.
p-subsequential) functions to be those functions that can be represented by sequential (resp.
p-subsequential) transducers.
We noted previously that the result of the composition of two transducers is a transducer that can be directly constructed.
There exists an efficient algorithm for the general case of the composition of transducers (transducers subsequential or not, having e-transitions or not, and with outputs in ~*, or in ~* t2 {oo} x T4+ U {oo}) (Mohri, Pereira, and Riley 1996).
The following theorem gives a more specific result for the case of subsequential and p-subsequential functions, which expresses their closure under composition.
We use the expression p-subsequential in two ways here.
One means that a finite number of 272 Mohri Transducers in Language and Speech Figure 3 Example of a subsequential transducer v2.
a ambiguities is admitted (the closure under composition matches this case), the second indicates that this number equals exactly p.
Theorem 1 Let f: E* --* A* be a sequential (resp.
p-subsequential) and g: A* -, f~* be a sequential (resp.
q-subsequential) function, then g of is sequential (resp.
pq-subsequential). Proof We prove the theorem in the general case of p-subsequential transducers.
The case of sequential transducers, first proved by Choffrut (1978), can be derived from the general case in a trivial way.
Let 7-1 be a p-subsequential transducer representing f, T1 = (Ql, i1,F1, E,A, 61,a1,pl), and 7"2 = (Q2, i2, F2, A,~,62, cr2, P2) a q-subsequential transducer representing g.
pl and p2 denote the final output functions of T1 and r2, which map F1 to (A*)P and F2 to (f~*)q, respectively, pl(r) represents, for instance, the set of final output strings at a final state r.
Define the pq-subsequential transducer T = (Q,i,F,E,f~,6,a,p) by Q = Q1 x Q2, i = (il, i2), F -{(91,92) E Q: 91 c F1,62(q2,p1(q1))f3 F2 ~ 0}, with the following transition and output functions: Va E E, V(ql, q2) E Q,6((ql, q2),a) = (61(ql,a),62(q2,al(ql,a))) o'((ql,q2),a) = cra(q2, crl(ql,a)) and with the final output function defined by: V(ql,q2) E F, p((ql, q2)) =a2(q2, Pl(ql))p2(6(q2, Pl(ql))) Clearly, according to the definition of composition, the transducer ~realizes g of.
The definition of p shows that it admits at most pq distinct output strings for a given input one.
This ends the proof of the theorem.
\[\] Figure 3 gives an example of a 1-subsequential or subsequential transducer T2.
The result of the composition of the transducers rl and T2 is shown in Figure 4.
States in the transducer 73 correspond to pairs of states of ;1 and "r2.
The composition consists essentially of making the intersection of the outputs of T1 with the inputs of ~'2.
Transducers admit another useful operation: union.
Given an input string w, a transducer union of "0 and T2 gives the set union of the strings obtained by application of 71 to w and r2 to w.
We denote by ~-1 + r2 the union of 7"1 and T2.
The following theorem specifies the type of the transducer 71 + ~'2, implying in particular the closure under union of p-subsequential transducers.
It can be proved in a way similar to the composition theorem.
Theorem 2 Let f: E* ~ A* be a sequential (resp.
p-subsequential) and g: E* --* A* be a sequential (resp.
q-subsequential) function, then g + f is 2-subsequential (resp.
(p + q)subsequential).
273 Computational Linguistics Volume 23, Number 2 Figure 4 2-subsequential transducer r3, obtained by composition of T1 and r2.
The union transducer T1 + ~-2 Can be constructed from rl and r2 in a way close to the union of automata.
One can indeed introduce a new initial state connected to the old initial states of rl and "r2 by transitions labeled with the empty string both on input and output.
But the transducer obtained using this construction is not sequential, since it contains c-transitions on the input side.
There exists, however, an algorithm to construct the union of p-subsequential and q-subsequential transducers directly as a p + q-subsequential transducer.
The direct construction consists of considering pairs of states (ql, q2), ql being a state of ~-1 or an additional state that we denote by an underscore, q2 a state of r2 or an additional state that we denote by an underscore.
The transitions leaving (ql, q2) are obtained by taking the union of the transitions leaving ql and q2, or by keeping only those of ql if q2 is the underscore state, similarly by keeping only those of q2 if ql is the underscore state.
The union of the transitions is performed in such a way that if ql and q2 both have transitions labeled with the same input label a, then only one transition labeled with a is associated to (O, q2)The output label of that transition is the longest common prefix of the output transitions labeled with a leaving ql and q2.
See Mohri (1996b) for a full description of this algorithm.
Figure 5 shows the 2-subsequential transducer obtained by constructing the union of the transducers rl and ;2 this way.
Notice that according to the theorem the result could be a priori 3-subsequential, but these two transducers share no common accepted string.
In such cases, the resulting transducer is max(p, q)-subsequential.
2.3 Characterization
and Extensions The linear complexity of their use makes sequential or p-subsequential transducers both mathematically and computationally of particular interest.
However, not all transducers, even when they realize functions (rational functions), admit an equivalent sequential or subsequential transducer.
Consider, for instance, the function f associated with the classical transducer represented in Figure 6; f can be defined by: 1 Vw c {x} +, f(w) = alwl if Iwl is even, (1) = blwl otherwise This function is not sequential, that is, it cannot be realized by any sequential transducer.
Indeed, in order to start writing the output associated to an input string w = x n, a or b according to whether n is even or odd, one needs to finish reading the whole input string w, which can be arbitrarily long.
Sequential functions, namely functions that 1 We denote by \]w\[ the length of a string w.
274 Mohri Transducers in Language and Speech a:E b:a Figure 5 2-subsequential transducer v4, union of 7-1 and r2.
x:a x:a x:b Figure 6 Transducer T with no equivalent sequential representation.
can be represented by sequential transducers do not allow such unbounded delays.
More generally, sequential functions can be characterized among rational functions by the following theorem: Theorem 3 (Ginsburg and Rose 1966) Letf be a rational function mapping G* to A*.
f is sequential iff there exists a positive integer K such that: Vu 6 G*,Va E G, 3w 6 A*, Iwl < K: f(ua) =f(u)w (2) In other words, for any string u and any element of the alphabet a, f(ua) is equal to f(u) concatenated with some bounded string.
Notice that this implies that flu) is always a prefix of f(ua), and more generally that if f is sequential then it preserves prefixes.
275 Computational Linguistics Volume 23, Number 2 Q x:xl Figure 7 Left-to-right sequential transducer L.
x.x2 xl:a Figure 8 Right-to-left sequential transducer R.
The fact that not all rational functions are sequential could reduce the interest of sequential transducers.
The following theorem, due to Elgot and Mezei (1965), shows, however, that transducers are exactly compositions of left and right sequential transducers.
Theorem 4 (Elgot and Mezei 1965) Letf be a partial function mapping G" to A*.f is rational iff there exists a left sequential function h ~* --* f~* and a right sequential function r: f~* --* A ~ such thatf = r o I.
Left sequential functions or transducers are those we previously defined.
Their application to a string proceeds from left to right.
Right sequential functions apply to strings from right to left.
According to the theorem, considering a new sufficiently large alphabet f~ allows one to define two sequential functions I and r that decompose a rational function f.
This result considerably increases the importance of sequential functions in the theory of finite-state machines as well as in the practical use of transducers.
Berstel (1979) gives a constructive proof of this theorem.
Given a finite-state transducer T, one can easily construct a left sequential transducer L and a right sequential transducer R such that R o L = T.
Intuitively, the extended alphabet f~ keeps track of the local ambiguities encountered when applying the transducer from left to right.
A distinct element of the alphabet is assigned to each of these ambiguities.
The right sequential transducer can be constructed in such a way that these ambiguities can then be resolved from right to left.
Figures 7 and 8 give a decomposition of the nonsequential transducer T of Figure 6.
The symbols of the alphabet f~ = {xl, x2} store information about the size of the input string w.
The output of L ends with xl iff Iwl is odd.
The right sequential function R is then easy to construct.
276 Mohri Transducers in Language and Speech Sequential transducers offer other theoretical advantages.
In particular, while several important tests, such as equivalence, are undecidable with general transducers, sequential transducers have the following decidability property.
Theorem 5 Let T be a transducer mapping G* to A ".
It is decidable whether T is sequential.
A constructive proof of this theorem was given by Choffrut (1978).
An efficient polynomial algorithm for testing the sequentiability of transducers based on this proof was given by Weber and Klemm (1995).
Choffrut also gave a characterization of subsequential functions based on the definition of a metric on G*.
Denote by u/~ v the longest common prefix of two strings u and v in G*.
It is easy to verify that the following defines a metric on G*: d(u,v) = tul + Ivl 21u A v I (3) The following theorem describes this characterization of subsequential functions.
Theorem 6 Letf be a partial function mapping G* to A*.
f is subsequential iff: 1.
f has bounded variation (according to the metric defined above).
2. for any rational subset Y of A*,f-I(Y) is rational.
The notion of bounded variation can be roughly understood here as follows: if d(x,y) is small enough, namely if the prefix that x and y share is sufficiently long compared to their lengths, then the same is true of their images by f, f(x) and f(y).
This theorem can be extended to describe the case of p-subsequential functions by defining a metric d~ on (A*)p.
For any u = (u I.....,Up) and v = (vl .....,Vp) C (A*)P, we define: doo(u, v) = max d(ui, vi) (4) 1Ki~p Theorem 7 Let f = (fl ..... fp) be a partial function mapping Dom(f) c G* to (A*)P.
f is psubsequential iff: 1.
f has bounded variation (using the metric d on G* and d~ on (A*)p).
2. for all i (1 < i < p) and any rational subset Y of A*,f/-I(Y) is rational.
Proof Assume f p-subsequential, and let T be a p-subsequential transducer realizing f.
A transducer Ti, 1 ~ i < p, realizing a component fi of f can be obtained from T simply by keeping only one of the p outputs at each final state of T.
Ti is subsequential by construction, hence the component )~ is subsequential.
Then the previous theorem implies that each component fi has bounded variation, and by definition of d~, f has also bounded variation.
Conversely, if the first condition holds, afortiori each)~ has bounded variation.
This combined with the second condition implies that each)~ is subsequential.
A transducer T realizing f can be obtained by taking the union of p subsequential transducers realizing each componentS.
Thus, in view of the theorem 2,f is p-subsequential.
\[\] 277 Computational Linguistics Volume 23, Number 2 One can also give a characterization of p-subsequential transducers irrespective of the choice of their components.
Let d~ be the semimetric defined by: V(u,v) E \[(A*)P\] 2, d'p(U,V) = max d(ui, vj) l<i,j<p (5) The following theorem that follows then gives that characterization.
Theorem 8 Letf be a rational function mapping E* to (A*)P.
f is p-subsequential iff it has bounded variation (using the semimetric d~ on (A*)P).
Proof According to the previous theorem the condition is sufficient since: V(u,v) c < a'p(u,v) Conversely if f is p-subsequential, let T = (Q, i,F, ~, A, 6,or, p) be a p-subsequential transducer representing f, where p = (pl ..... pp) is the output function mapping Q to (A*)P.
Let N and M be defined by: N= max Ipi(q)l and M= max Icffq, a)l (6) qEF, l <id< p aE~,qEQ We denote by Dom(T) the set of strings accepted by T.
Let k > 0 and (ul, U2) E \[Dora(T)\] 2 such that d(ul, u2) _< k.
Then, there exists u E E* such that: U 1 = UVl, U 2 = UV2, and \[vii + Iv2l G k (7) Hence, f(Ul) = {cr(i,u)rr(~(i,u),vl)pj(6(i, ul)): 1 G j < p} f(u2) = {rr(i,u)cr(6(i,u),v2)Pj(~(i, u2)): 1 < j < p} (8) Let K = kM + 2N.
We have: dp(f(ul),f(u2)) M(ivl\[ + Iv2I) + dlp(p(6(i, ul)),p(~(i, u2))) < kM+2N = K Thus, f has bounded variation using d~.
This ends the proof of the theorem.
\[\] 2.4 Application to Language Processing We briefly mentioned several theoretical and computational properties of sequential and p-subsequential transducers.
These devices are used in many areas of computational linguistics.
In all those areas, the determinization algorithm can be used to obtain a p-subsequential transducer (Mohri 1996b), and the minimization algorithm to reduce the size of the p-subsequential transducer used (Mohri 1994b).
The composition, union, and equivalence algorithms for subsequential transducers are also useful in many applications.
278 Mohri Transducers in Language and Speech 2.4.1 Representation of Dictionaries.
Very large-scale dictionaries can be represented by p-subsequential dictionaries because the number of entries and that of the ambiguities they contain are finite.
The corresponding representation offers fast look-up since the recognition does not depend on the size of the dictionary but only on that of the input string considered.
The minimization algorithm for sequential and p-subsequential transducers allows the size of these devices to be reduced to the minimum.
Experiments have shown that these compact and fast look-up representations for large natural language dictionaries can be efficiently obtained.
As an example, a French morphological dictionary of about 21.2 Mb can be compiled into a p-subsequential transducer of 1.3 Mb, in a few minutes (Mohri 1996b).
2.4.2 Compilation
of Morphological and Phonological Rules.
Similarly, context-dependent phonological and morphological rules can be represented by finite-state transducers (Kaplan and Kay 1994).
Most phonological and morphological rules correspond to p-subsequential functions.
The result of the computation described by Kaplan and Kay (1994) is not necessarily a p-subsequential transducer.
But, it can often be determinized using the determinization algorithm for p-subsequentiable transducers.
This considerably increases the time efficiency of the transducer.
It can be further minimized to reduce its size.
These observations can be extended to the case of weighted rewrite rules (Mohri and Sproat 1996).
2.4.3 Syntax.
Finite-state machines are also currently used to represent local syntactic constraints (Silberztein 1993; Roche 1993; Karlsson et al.1995; Mohri 1994d).
Linguists can conveniently introduce local grammar transducers that can be used to disambiguate sentences.
The number of local grammars for a given language and even for a specific domain can be large.
The local grammar transducers are mostly p-subsequential.
Determinization and minimization can then be used to make the use of local grammar transducers more time efficient and to reduce their size.
Since p-subsequential transducers are closed under composition, the result of the composition of all local grammar transducers is a p-subsequential transducer.
The equivalence of local grammars can also be tested using the equivalence algorithm for sequential transducers.
For a more detailed overview of the applications of sequential string to string transducers to language processing, see Mohri (1996a).
Because they are so time and space efficient, sequential transducers will likely be used increasingly often in natural language processing as well as in other connected fields.
In the following, we consider the case of string-to-weight transducers, which are also used in many areas of computational linguistics.
3. Power Series and Subsequential String-to-Weight Transducers We consider string-to-weight transducers, namely transducers with input strings and output weights.
These transducers are used in various domains, such as language modeling, representation of word or phonetic lattices, etc., in the following way: one reads and follows a path corresponding to a given input string and outputs a number obtained by combining the weights along this path.
In most applications to natural language processing, the weights are simply added along the path, since they are interpreted as (negative) logarithms of probabilities.
In case the transducer is not sequential, that is, when it does not have a deterministic input, one proceeds in the same way for all the paths corresponding to the input string.
In natural language processing, specifically in speech processing, one keeps the minimum of the weights associated to 279 Computational Linguistics "Volume 23, Number 2 Figure 9 Example of a string-to-weight transducer.
these paths.
This corresponds to the Viterbi approximation in speech recognition or in other related areas for which hidden Markov models (HMM's) are used.
In all such applications, one looks for the best path, i.e., the path with the minimum weight.
3.1 Definitions
In this section, we give the definition of string-to-weight transducers and other deftnitions useful for the presentation of the theorems of the following sections.
In addition to the output weights of the transitions, string-to-weight transducers are provided with initial and output weights.
For instance, when used with the input string ab, the transducer in Figure 9 outputs: 5 + 1 + 2 + 3 = 11, 5 being the initial and 3 the final weight.
Definition More formally, a string-to-weight transducer T is defined by T = (Q, ~, I, F, E, A, p) with:  Q a finite set of states,  G the input alphabet,  I C Q the set of initial states,  F C Q the set of final states,  E C Q x G x T4+ x Q a finite set of transitions,  A the initial weight function mapping I to 7Z+,  p the final weight function mapping F to 7"4+.
One can define for T a transition (partial) function 6 mapping Q x E to 2 Q by: V(q,a) E Q x G,~(q,a) = {q' I 3x E 7"4+: (q,a,x,q') E E} and an output function cr mapping E to T4+ by: Vt = (p,a,x,q) E E, cr(t) = x A path ~r in T from q E Q to q' E Q is a set of successive transitions from q to q': 7r = ((qo, ao, xo, ql) ....., (qm-l,am-l, Xm-l, qm)), with Vi E \[0, m1\], qi+l E 6(qi, ai).
We can extend the definition of a to paths by: cr(Tr) -XoXl""Xm-1.
We denote by ~E q w q, the set of paths from q to q' labeled with the input string w.
The definition of 6 can be extended to Q x G* by: V(q,w) E Q x ~*,6(q,w) = {q': 3 path 7r in T, Tr E qW q,} 280 Mohri Transducers in Language and Speech and to 2 Q x E*, by: VR G Q, Vw E ~*, 6(R, w) = U t~(q, w) qER For (q, w, q') E Q x ~ x Q such that there exists a path from q to ql labeled with w, we define O(q, w, q') as the minimum of the outputs of all paths from q to q' with input w: O(q, w,q') = min cr(Tr) w IrEq...~q' A successful path in T is a path from an initial state to a final state.
A string w E E* is accepted by Tiff there exists a successful path labeled with w: w E 6(L w) N F.
The output corresponding to an accepted string w is then obtained by taking the minimum of the outputs of all successful paths with input label w: min (A(i) + O(i, w,f) + p(f)) (i,f)EIxF: fE6(i,w) A transducer T is said to be trim if all states of T belong to a successful path.
String-toweight transducers clearly realize functions mapping ~* to 74+.
Since the operations we need to consider are addition and min, and since (74+ U {oo}, min, +, cxD, 0) is a semiring, we call these functions formal power series.
2 We
adopt the terminology and notation used in formal language theory (Berstel and Reutenauer 1988; Kuich and Salomaa 1986; Salomaa and Soittola 1978): the image by a formal power series S of a string w is denoted by (S, w) and called the coefficient of w in S, the notation S = ~,w~.
(S, w)w is then used to define a power series by its coefficients, the support of S is the language defined by: suep(S) = {w (S,w) # The fundamental theorem of Schtitzenberger (1961), analogous to Kleene's theorem for formal languages, states that a formal power series S is rational iff it is recognizable, that is, realizable by a string-to-weight transducer.
The semiring (74+ U {cx~}, rain, +, c~, 0) used in many optimization problems is called the tropical semiring.
3 So, the functions we consider here are more precisely rational power series over the tropical semiring.
A string-to-weight transducer T is said to be unambiguous if for any given string w there exists at most one successful path labeled with w.
In the following, we examine, more specifically, efficient string-to-weight transducers: subsequential transducers.
A transducer is said to be subsequential if its input is 2 Recall that a semiring is essentially a ring that may lack negation, namely in which the first operation does not necessarily admit inversion.
(TZ, +,., 0,1), where 0 and 1 are, respectively, the identity elements for + and., or, for any non-empty set E, (2 E, U, n, 0, E), where 0 and E are, respectively, the identity elements for U and O, are other examples of semirings.
3 This
terminology is often used more specifically when the set is restricted to natural integers (Nu {oo},min, +,~,0).
281 Computational Linguistics Volume 23, Number 2 deterministic, that is if at any state there exists at most one outgoing transition labeled with a given element of the input alphabet G.
Subsequential string-to-weight transducers are sometimes called weighted automata, or weighted acceptors, or probabilistic automata, or distance automata.
Our terminology is meant to favor the functional view of these devices, which is the view that we consider here.
Not all string-to-weight transducers are subsequential but we define an algorithm to determinize nonsubsequential transducers when possible.
Definition More formally a string-to-weight subsequential transducer "r = (Q, i, F, ~, 6, or, )~, p) is an 8-tuple, with:  Q the set of its states,  i E Q its initial state,  F c_ Q the set of final states,  G the input alphabet,  6 the transition function mapping Q x E to Q, 6 can be extended as in the string case to map Q x G* to Q,  cr the output function, which maps Q x G to 7%+, cr can also be extended to Q x ~,*,  ;~ E T4+ the initial weight,  p the final weight function mapping F to T4+.
A string w E ~,* is accepted by a subsequential transducer T if there exists f E F such that 6(i, w) =f.
The output associated to w is then: )~ + or(i, w) + p(f).
We will use the following definition for characterizing the transducers that admit determinization.
Definition Two states q and q' of a string-to-weight transducer T = (Q, I, F, G, 6, rr, )~, p), not necessarily subsequential, are said to be twins if: V(u,v) E (~,)2, ({q,q,} C 6(I,u),q E 6(q,v),q' E 6(q',v)) ~ ~(q,v,q) = ~(q',v,q') (9) In other words, q and q' are twins if, when they can be reached from the initial state by the same string u, the minimum outputs of loops at q and q' labeled with any string v are identical.
We say that T has the twins property when any two states q and q' of T are twins.
Notice that according to the definition, two states that do not have cycles with the same string v are twins.
In particular, two states that do not belong to any cycle are necessarily twins.
Thus, an acyclic transducer has the twins property.
In the following section, we consider subsequential power series in the tropical semiring, that is, functions that can be realized by subsequential string-to-weight transducers.
Many rational power series defined on the tropical semiring considered in practice are subsequential, in particular, acyclic transducers represent subsequential power series.
282 Mohri Transducers in Language and Speech We introduce a theorem giving an intrinsic characterization of subsequential power series irrespective of the transducer realizing them.
We then present an algorithm that allows one to determinize some string-to-weight transducers.
We give a general presentation of the algorithm since it can be used with many other semirings, in particular, with string-to-string transducers and with transducers whose output labels are pairs of strings and weights.
We then use the twins property to define a set of transducers to which the determinization algorithm applies.
We give a characterization of unambiguous transducers admitting determinization, and then use this characterization to define an algorithm to test if a given transducer can be determinized.
We also present a very efficient minimization algorithm that applies to subsequential string-to-weight transducers.
In many cases, the determinization algorithm can also be used to minimize a subsequential transducer; we describe this use of the algorithm and give the related proofs in the appendix.
3.2 Characterization
of Subsequential Power Series Recall that one can define a metric on E* by: d(u,v) = lu\[ + Iv\[ 21u A v\[ (10) where we denote by u A v the longest common prefix of two strings u and v in E*.
The definition we gave for subsequential power series depends on the transducers representing them.
The theorem that follows gives an intrinsic characterization of subsequential power series.
4 Theorem
9 Let S be a rational power series defined on the tropical semiring.
S is subsequential iff it has bounded variation.
Proof Assume that S is subsequential.
Let ~= (Q, i, F, E, 6, or, A, p) be a subsequential transducer.
5 denotes the transition function associated with T, cr its output function, and )~ and p the initial and final weight functions.
Let L be the maximum of the lengths of all output labels of T: L= max \[cr(q,a)\[ (11) (q,a)CQxE and R the upper bound of all output differences at final states: R= max \[p(q)-p(q')\[ (12) (q,q')EF 2 and define M as M = L + R.
Let (Ul, u2) be in (E*) 2.
By definition of d, there exists u E E* such that: Ul = uvl, u2 = uv2, and Iv1\[ + Iv2\[ = d(ul,u2) (13) Hence, cr(i,u,) = cr(i,u) + cr(5(i,u),vl) (i, u2) = cr(i,u) + cr(6(i,u),v2) 4 This is an extension of the characterization theorem of Choffrut (1978) for string-to-string functions.
The extension is not straightforward because the length of an output string is a natural integer.
Here we deal with real numbers.
283 Computational Linguistics Volume 23, Number 2 Since \[(6(i,u),vl) ~(6(i,u),v2)l <_ L.
(Iv1\] + Iv21) = L.d(ul, u2) and \[p(6(i, ul))-p(~(i, u2))l <_ R we have IA+(i, ul) + p(6(i, ul)) A+~(i, u2) + p(6(i, u2))\[ <_ L.d(ul, u2) + a Notice that if ul # u2, R <_ R.
d(ul, u2).
Thus \[A + a(i, ul) + p(6(i, Ul)) )~ + or(i, u2) + p(6(i, u2))\[ <_ (L + n) . d(ul, u2) Therefore: V(Ul, U2) E (E*) 2, \[S(Ul)S(u2)\[ < M'd(Ul, U2) (14) This proves that S is M-Lipschitzian s and afortiori that it has bounded variation.
Conversely, suppose that S has bounded variation.
Since S is rational, according to the theorem of Schtitzenberger (1961) it is recognizable and therefore there exists a string-to-weight transducer ~= (Q,I,F, E, ~, or,,k, p) realizing S.
As in the case of string-to-string transducers, one can show that any transducer admits an equivalent trim unambiguous transducer.
So, without loss of generality we can assume T trim  and unambiguous.
Furthermore, we describe in the next sections a determinization algorithm.
We show that this algorithm applies to any transducer that has the twins property.
Thus, in order to show that S is subsequentiable, it is sufficient to show that ~has the twins property.
Consider two states q and q' of ~and let (u,v) E (E*) 2 be such that: {q, q'} c ~(1, u), q E ~(q, v), q' ~ ~(ql, v) Since ~is trim there exists (w,w') E (~.)2 such that 6(q,w) NF 0 and ~(q, w') NF # 0.
Notice that Vk >_ O, cl(uvkw, uvkw ') = a(w, w') Thus, since S has bounded variation 3K >_ 0,Vk _> 0, IS(uvkw) S(uv~w')l <_ K Since r is unambiguous, there is only one path from I to F corresponding to uvkw (resp.
uvkw'). We have: S(uvkw) = O(I, uw, F) +kO(q,v,q) S(uvkw ') = O(I, uw',F) + kO(q',v,q') 5 This implies in particular that the subsequential power series over the tropical semiring define continuous functions for the topology induced by the metric d on E*.
Also this shows that in the theorem one can replace has bounded variation by is Lipschitzian.
284 Mohri Transducers in Language and Speech 9 10 11 12 13 14 15 Figure 10 Power Series Determinization(T~, T2) 1 F2 *-0 2 )~2 ~ (~E~AI(i) iEI1 3 i2 *-U(i,)~21  ~1(i))} iEI1 4 Q ~ {/2} 5 while Q # 0 6 do q2 ~-head\[Q\] 7 if (there exists (q, x) E q2 such that q E F1) 8 then F2 *--F2 U {q2} /:}2 (q2) *-~ X @ PA (q) qEFl,(q,x) Eq2 for each a such that F(q2,a) # 0 do a2(q2,a) *(~ \[x (~ cq (t)\] (q,x)EI'(q2,a) t=(q,a,crl(t),nl(t))EE1 62(q2,a)*-U {(q" ED \[cr2(q2"a)\]-lx(gal(t)} q'Ev(q2,a) (q,x,t) ET(q2,a),nl(t)=q' if (62(q2,a) is a new state) then ENQUEUE(Q, 62(q2, a)) DEQUEUE(Q) Algorithm for the determinization of a transducer ~-~ representing a power series defined on the semiring (S, E3, , 0, i).
Hence 3K > 0,Vk _> 0, i(O(I, uw, F) O(I, uw',F)) + k(O(q,v,q) O(q',v,q')) I <_ K ==~ #(q,v,q) ~)(q',v,q') = 0 Thus T has the twins property.
This ends the proof of the theorem.
\[\] 3.3 General Determinization Algorithm for Power Series We describe in this section an algorithm for constructing a subsequential transducer "1" 2 = (Q2, i2, F2,~,~2, cr2,,,~2, P2 ) equivalent to a given nonsubsequential one ~-I = (Q1, G, I1, F1, El, A1, pl).
The algorithm extends our determinization algorithm for stringto-string transducers representing p-subsequential functions to the case of transducers outputting weights (Mohri 1994c).
Figure 10 gives the pseudocode of the algorithm.
We present the algorithm in the general case of a semiring (S, ~, , 0,1) on which the transducer T1 is defined.
Indeed, the algorithm we are describing here applies as well to transducers representing power series defined on many other semirings.
6 We
describe the algorithm in the case of the tropical semiring.
For the tropical semiring, one can replace @ by min and  by + in the pseudocode of Figure 10.
7 6 In particular, the algorithm also applies to string subsequentiable transducers and to transducers that output pairs of strings and weights.
We will come back to this point later.
7 Similarly, ~21 should be interpreted as -A, and \[~2(q2,a)\] -1 as -cr2(q2,a ).
285 Computational Linguistics Volume 23, Number 2 The algorithm is similar to the powerset construction used for the determinization of automata.
However, since the outputs of two transitions bearing the same input label might differ, one can only output the minimum of these outputs in the resulting transducer, therefore one needs to keep track of the residual weights.
Hence, the subsets q2 that we consider here are made of pairs (q, x) of states and weights.
The initial weight &2 of T2 is the minimum of all the initial weights of ~-1 (line 2).
The initial state i2 is a subset made of pairs (i, x), where i is an initial state of T1, and x = &l (i) )~2 (line 3).
We use a queue Q to maintain the set of subsets q2 yet to be examined, as in the classical powerset construction, s Initially, Q contains only the subset i2.
The subsets q2 are the states of the resulting transducer, q2 is a final state of T2 iff it contains at least one pair (q, x), with q a final state of ~1 (lines 7-8).
The final output associated to q2 is then the minimum of the final outputs of all the final states in q2 combined with their respective residual weight (line 9).
For each input label a such that there exists at least one state q of the subset q2 admitting an outgoing transition labeled with a, one outgoing transition leaving q2 with the input label a is constructed (lines 10-14).
The output o'2(q2, a) of this transition is the minimum of the outputs of all the transitions with input label a that leave a state in the subset q2, when combined with the residual weight associated to that state (line 11).
The destination state 62(q2, a) of the transition leaving q2 is a subset made of pairs (q', x'), where q' is a state of T1 that can be reached by a transition labeled with a, and x' the corresponding residual weight (line 12).
x' is computed by taking the minimum of all the transitions with input label a that leave a state q of q2 and reach q', when combined with the residual weight of q minus the output weight cr2(q2,a).
Finally, 62(q2,a) is enqueued in Q iff it is a new subset.
We denote by nl (t) the destination state of a transition t E El.
Hence nl (t) = q', if t -(q,a,x,q') E El.
The sets F(q2,a), 7(q2,a), and v(q2,a) used in the algorithm are defined by:  F(q2,a) = {(q,x) E q2: 3t = (q,a, rrl(t),nl(t)) E El}  7(q2,a) = {(q,x,t) E q2 x El: t= (q,a, cq(t),nl(t)) E El}  ~(q2,a) = {q': 3(q,x) E q2,3t = (q,a, rrfft),q') E El} F(q2, a) denotes the set of pairs (q, x), elements of the subset q2, having transitions labeled with the input a.
7(q2, a) denotes the set of triples (q, x, t) where (q, x) is a pair in q2 such that q admits a transition with input label a.
v(q2,a) is the set of states q' that can be reached by transitions labeled with a from the states of the subset q2.
The algorithm is illustrated in Figures 11 and 12.
Notice that the input ab admits several outputs in #1:{1 + 1 = 2,1 + 3 = 4,3 + 3 = 6,3 + 5 = 8}.
Only one of these outputs (2, the smallest) is kept in the determinized transducer 1'2, since in the tropical semiring one is only interested in the minimum outputs for any given string.
Notice that several transitions might reach the same state with a priori different residual weights.
Since one is only interested in the best path, namely the path corresponding to the minimum weight, one can keep the minimum of these weights for a given state element of a subset (line 11 of the algorithm of Figure 10).
In the next section, we give a set of transducers "rl for which the determinization algorithm terminates.
The following theorem shows the correctness of the algorithm when it terminates.
8 The
algorithm works with any queue discipline chosen for Q.
286 Mohri Transducers in Language and Speech Figure 11 Transducer #1 representing a power series defined on (7"4+ U {o0}, min, +).
Figure 12 Transducer #2 obtained by power series determinization of #1.
Theorem 10 Assume that the determinization algorithm terminates, then the resulting transducer ~'2 is equivalent to ~1.
Proof We denote by Offq, w,q') the minimum of the outputs of all paths from q to q'.
By construction we have: )~2 "~-min/~1 (il) il E I1 We define the residual output associated to q in the subset (~2(/2, W) as the weight c(q, w) associated to the pair containing q in ~2(i2, w).
It is not hard to show by induction on Iwl that the subsets constructed by the algorithm are the sets 62(i2, w), w E ~*, such that: Vw E E*, ~2(i2,w) = U {(q,c(q,w)} qE61 (ll,w) c(q,w) = m~1~1(/~1(/1 ) q-Ol(il, w,q)) -cr2(/2,w) --/~2 ff2(i2,w) = min w (/~1(/1) qOl(il, w,q)) -~2 q~1(I1, ) (15) 287 Computational Linguistics Volume 23, Number 2 Notice that the size of a subset never exceeds \[QI\[: card(62(i2,w)) ~ IQI\[.
A state q belongs at most to one pair of a subset, since for all paths reaching q, only the minimum of the residual outputs is kept.
Notice also that, by definition of min, in any subset there exists at least one state q with a residual output c(q, w) equal to 0.
A string w is accepted by ~-1 iff there exists q E F1 such that q c 61 (I1, w).
Using equations 15, it is accepted iff 62(i2,w) contains a pair (q,c(q,w)) with q E F1.
This is exactly the definition of the final states F2 (line 7).
So ~1 and T2 accept the same set of strings.
Let w C E* be a string accepted by ~1 and ~-2.
The definition of p2 in the algorithm of figure 10, line 9, gives: p2(62(i2,w)) = rain pl(q) + m'.m(Al(h) + 81(il, w,q)) a2(i2,w) )~2 (16) qE 61 ( II,w )NF1 ll El1 Thus, if we denote by S the power series realized by "rl, we have: p2(62(i2,w)) = (S,w) cr2(/2,w) )~2 (17) Hence: &2 + cr2(i2, w) + p2(62(/2, w)) = (S, w).
\[\] The power series determinization algorithm is equivalent to the usual determinization of automata when the initial weight, the final weights, and all output labels are equal to 0.
The subsets considered in the algorithm are then exactly those obtained in the powerset determinization of automata, all residual outputs c(q, w) being equal to 0.
Both space and time complexity of the determinization algorithm for automata are exponential.
There are minimal deterministic automata with exponential size with respect to an equivalent nondeterministic one.
A fortiori the complexity of the determinization algorithm in the weighted case we just described is also exponential.
However, in some cases in which the degree of nondeterminism of the initial transducer is high, the determinization algorithm turns out to be fast and the resulting transducer has fewer states than the initial one.
We present examples of such cases, which appear in speech recognition, in the last section.
We also present a minimization algorithm that allows the size of subsequential transducers representing power series to be reduced.
The complexity of the application of subsequential transducers is linear in the size of the string to which it applies.
This property makes it worthwhile to use the power series determinization to speed up the application of transducers.
Not all transducers can be determinized using the power series determinization.
In the following section, we define a set of transducers that admit determinization, and characterize unambiguous transducers that admit the application of the algorithm.
Since determinization does not apply to all transducers, it is important to be able to test the determinizability of a transducer.
We present, in the next section, an algorithm to test this property in the case of unambiguous trim transducers.
The proofs of some of the theorems in the next two sections are complex; they can be skipped on first reading.
3.4 Determinizable
Transducers There are transducers with which determinization does not halt, but rather generates an infinite number of subsets.
We define determinizable transducers as those transducers with which the algorithm terminates.
We first show that a large set of transducers 288 Mohri Transducers in Language and Speech admit determinization, then give a characterization of unambiguous transducers admitting determinization.
In what follows, the states of the transducers considered will be assumed to be accessible from the initial one.
The following lemma will be useful in the proof of the theorems.
Lemma 1 W Let T = (Q, E, I, F, E, A, p) be a string-to-weight transducer, ~r E p "-* q a path in T from the state p ~ Q to q ~ Q, and ~" E p',G q' a path from p' E Q to q' ~ Q both labeled with the input string w ~ E*.
Assume that the lengths of 7r and ~-' are greater than \[Q\[2 _ 1, then there exist strings Ul, u2, u3 in E*, and states pit p2, p~r and p~ such that \[u2\[ > 0, UlU2U3 = w and such that 7r and 7r' be factored in the following way: 7r' E p,,,.,u' P~',,~ p~,,~ q, Proof The proof is based on the use of a cross product of two transducers.
Given two transducers T1 = (Q1, E, I1, F1, El, A1, pl) and T2 = (Q2, G,/2, F2, E2, A2, #2), we define the cross product of T1 and T2 as the transducer: T1 X T2 = (Q1 x Q2,G, I1 x I2,F1 x F2,E,A,#) with outputs in 7"4+ x T4+ such that t = ((ql, q2), a, (Xl, x2), (q~, q~)) E Q1 x E x 14+ x 74+ x Q2 is a transition of T1 x T2, namely t E E, iff (ql,a, xl, q~) E E1 and (q2,a, x2,2) E E2.
We also define A and p by: V(i1,i2) E/1 x I2, A(il, i2) = (Al(i1),A2(i2)), V(fl,f2) E F1 x F2, #(fl,f2) ---(Pl 0Cl ), P20c2)).
Consider the cross product of T with itself, T x T.
Let 7r and 7r' be two paths in T with lengths greater than IQI 2 1, (m > IQI2 _ 1): ~r = ( (p = qo, ao, xo, ql) .....
(qm-l,am-l, Xm-l, qm = q) ) V V z ! a x ! ! 7r' = ((p' = q'o, ao, xo, qO .....
~qm-1, m-I, m-l, qm = q')) then: II = (((q0, q~), a0, (x0, x~), (ql, q~)),  ., ((qm-1, q~-x), am-I, (Xm-l., Xm_ 1),, (qm, Cm))) is a path in T x T with length greater than \[Q\]2 _ 1.
Since T x T has exactly \[Q\[2 states, II admits at least one cycle at some state (pl, p~) labeled with a non-empty input string u2.
This shows the existence of the factorization above and proves the lemma.
\[\] Theorem 11 Let 7-1 -(Q1, E, I1, F1, El, A1, pl) be a string-to-weight transducer defined on the tropical semiring.
If T1 has the twins property then it is determinizable.
Proof Assume that ~has the twins property.
If the determinization algorithm does not halt, there exists at least one subset of 2 Q, {q0 ..... qm}, such that the algorithm generates an infinite number of distinct weighted subsets {(q0, Co) .....
(qm, Cm)}.
289 Computational Linguistics Volume 23, Number 2 Then we have necessarily m > 1.
Indeed, we mentioned previously that in any subset there exists at least one state qi with a residual output ci = 0.
If the subset contains only one state qo, then Co = 0.
So there cannot be an infinite number of distinct subsets ((qo, Co)}.
Let A c G* be the set of strings w such that the states of 62(i2, w) be {qo ..... qm}.
We have: Vw E A, ~2(/2, W) = {(q0, c(qo, w)),..., (qm, C(qm, w))}.
Since A is infinite, and since in each weighted subset there exists a null residual output, there exist i0, 0 ~ i0 ~ m, such that c(q/0, w) -0 for an infinite number of strings w E A.
Without loss of generality we can assume that i0 -0.
Let B C_ A be the infinite set of strings w for which c(q0, w) = 0.
Since the number of subsets ((qo, c(qo, w)) .....
(qm, C(qm, W))}, w E B, is infinite, there exists j, 0 < j _< m, such that c(qj, w) be distinct for an infinite number of strings w E B.
Without loss of generality we can assume j = 1.
Let C c B be an infinite set of strings w with c(ql, w) all distinct.
Define R(qo, ql) to be the finite set of differences of the weights of paths leading to q0 and ql labeled with the same string w, \[w\[ _< \[Qll 2 1: R(qo, q,) = {(A(i,) +~(~-,)) (A(io) + (~'o)): 7to E io w qo, Tr, E i, w q', io E/,il E/,Iw I _( IQ, I2 1} We will show that {c(ql, w): w E C} C_ R(qo, ql).
This will yield a contradiction with the infinity of C, and will therefore prove that the algorithm terminates.
Let w E C, and consider a shortest path 7r0 from a state i0 E I to q0 labeled with the input string w and with total cost c~(zr0).
Similarly consider a shortest path ~rl from il E I to ql labeled with the input string w and with total cost cr(zrl).
By definition of the subset construction we have: (A(h) + ~r(~rl)) (A(/o) + er(~r0)) = c(ql, w).
Assume that Iw\[ > \[Q1\] 2 1.
Using the lemma 1, there exists a factorization of ~r0 and zrl of the type: ~ro E io "~ po "~ po d~ qo 7rl E il d2, pl ~ pl ~ ql with \]u2\] > 0.
Since Po and pl are twins, 01(po, u2,po) = 01(pl, u2,Pl).
Define zr~ and ~r~ by: ~r~ E i0 G p0 "~ q0 7r~ E il G pl "~ ql Since ~r and ~r' are shortest paths, we have: a(Tro) = cr(zr~) + 01(po, u2,po) and a0rl ) = cr(zr~) + 01 (Pl, u2, pl).
Hence: (A(il) + rr0r~)) (A(io) + a(Tr~)) = c(ql, w).
By induction on \]w I, we can therefore find shortest paths Ho and H1 from io to qo (resp.
il to ql) with length less or equal to \]Q112 1 and such that (A(h) +a(H1)) (A(io) +a(Ho)) = c(ql, w).
Since a(H1) a(IIo) E R(qo, ql), c(ql, w) E R(qo, ql) and C is finite.
This ends the proof of the theorem.
\[\] There are transducers that do not have the twins property and that are still determinizable.
To characterize such transducers, more complex conditions that we will not describe here are required.
However, in the case of trim unambiguous transducers, the twins property provides a characterization of determinizable transducers.
290 Mohri Transducers in Language and Speech Theorem 12 Let "rl = (Q1,E, I1,F1, El, ~1,Pl) be a trim unambiguous string-to-weight transducer defined on the tropical semiring.
Then ~rl is determinizable iff it has the twins property.
Proof According to the previous theorem, if ~1 has the twins property, then it is determinizable.
Assume now that T does not have the twins property, then there exist at least two states q and q~ in Q that are not twins.
There exists (u, v) E E* such that: ({q,q'} C 61(I,u),q E 51(q,v),q' E 5ffq',v)) and ~l(q,v,q) ~ ~l(q',v,q').
Consider the weighted subsets 62(/2, uvk), with k E N, constructed by the determinization algorithm.
A subset 62(/2, uv k) contains the pairs (q, c(q, uvk)) and (q', c(q', uvk)).
We will show that these subsets are all distinct.
This will prove that the determinization algorithm does not terminate if ~-1 does not have the twins property.
Since T1 is a trim unambiguous transducer, there exits only one path in ~-1 from I to q or to qt with input string u.
Similarly, the cycles at q and q~ labeled with v are unique.
Thus, there exist i E I and i ~ E I such that: VkEN, c(q, uv k) = )~l(i)+01(i,u,q)+kOl(q,v,q)-~2(i2,uvk)-)~2 (18) Vk E ./V', c(q', uv k) = )~1(/') q81 (i', u, q') + k01(q', v, q') or2(/2, uv k) ~2 Let )~ and 0 be defined by: )~ = (,~1(i') )~1(i)) q( l(i',u,q') 01(i,u,q)) 0 = Ol(q',v,q') Offq, v,q) (19) We have: 'Ok E N, c(q', uv k) c(q, uv k) = ~ + k~) (20) Since (1 ~ 0, equation 20 shows that the subsets 62(i2, uv k) are all distinct.
\[\] 3.5 Test of Determinizability The characterization of determinizable transducers provided by theorem 12 leads to the definition of an algorithm for testing the determinizability of trim unambiguous transducers.
Before describing the algorithm, we introduce a lemma that shows that it suffices to examine a finite number of paths to test the twins property.
Lemma 2 Let "rl = (Q1, E, I1,F1, El, )~1,Pl) be a trim unambiguous string-to-weight transducer defined on the tropical semiring.
7-1 has the twins property iff V(u,v) E (E*) 2, luvl <_ 2IQ112 1, ({q,q'} C 61(I,u),q E 51(q,v),q' E 61(q',v)) ~ 01(q,v,q) = 01(q',v,q') (21) Proof Clearly if "O has the twins property, then (21) holds.
Conversely, we prove that if (21) holds, then it also holds for any (u,v) E (E*) 2, by induction on luvI . Our proof is similar to that of Berstel (1979) for string-to-string transducers.
Consider (u, v) E (E*) 2 and (q,q') E IQll 2 such that: {q,q'} c 61(I,u),q E 61(q,v),q' E 61(q',v).
Assume that luvI > 21Qll 2 1 with Iv I > o.
Then either luI > IQll 2 1 or IvI > IQll 2 1.
Assume that lul > IQ112-1.
Since T1 is a trim unambiguous transducer there exists a unique path ~r in rl from i E I to q labeled with the input string u, and a unique path 291 Computational Linguistics Volume 23, Number 2 7r' from i' E I to q'.
In view of lemma 2, there exist strings ul, u2, u3 in ~*, and states pl, p2, p~, and p~ such that \]u2\] > 0, UlU2U3 -~ U and such that lr and 7r' be factored in the following way: ~r E i u"G~ pl "~ pl,G q ~r' E i',-,,*ul Pl',`% P~ "~ q' Since \]ulu3vl < luv\], by induction 01(q,v,q) = Offq',v,q').
Next, assume that Iv I > \]Qll 2 1.
Then according to lemma 1, there exist strings Vl, v2, v3 in E*, and states ql, q2, q~, and q~ such that Iv21 > 0, vlv2v3 = v and such that lr and lr' be factored in the following way: ~r E q~ ql~ ql ~ q ~r' E q',G q~,~ q~,~ q' Since lUVl V3\] < \]uv\], by induction, 81 ( q, vl v3, q) = 81 ( q', vl v3, q').
Similarly, since luvl v2\] < l uv\], 01 (ql, v2, ql) = 01 (q~, v2, q~).
"rl is a trim unambiguous transducer, so: 01(q,v,q) =01(q, vlv3,q)+Ol(ql, v2,ql) 01(q', v, q') = 01(q', vlv3, q') + 01 (q~, v2, q~) Thus, 01 (q, v, q) = 01(q', v, q').
This completes the proof of the lemma.
\[\] Theorem 13 Let T1 = (Q1, E, 11, F1, El, A1, Pl) be a trim unambiguous string-to-weight transducer defined on the tropical semiring.
There exists an algorithm to test the determinizability of ~1.
Proof According to theorem 12, testing the determinizability of T1 is equivalent to testing for the twins property.
We define an algorithm to test this property.
Our algorithm is close to that of Weber and Klemm (1995) for testing the sequentiability of string-to-string transducers.
It is based on the construction of an automaton A = (Q,/, F, E) similar to the cross product of ~-1 with itself.
Let K C T~ be the finite set of real numbers defined by: K= ((t~)-cr(ti)): l <k <2iQ1\]2-1,Vi<_k(ti, tl) EE We define A by the following:  The set of states of A is defined by Q = Q1 x Q1 x K,  The set of initial states by I = h x/1 x {0},  The set of final states by F = F1 x F1 x K,  The set of transitions by: E = {((ql, q2,c),a,(q~,q~2,c')) E.
Q x E x Q: 3 (ql,a, x, q2) E El, ' ' ' x' x}.
(ql,a,x,q2) EEl, C'-C= 292 Mohri Transducers in Language and Speech By construction, two states ql and q2 of Q can be reached by the same string u, lut < 21Qll 2 1, iff there exists c E K such that (ql, q2,c) can be reached from I in A.
The set of such (ql, q2, c) is exactly the transitive closure of I in A.
The transitive closure of I can be determined in time linear in the size of A, O(IQI + IEI).
Two such states ql and q2 are not twins iff there exists a path in A from (ql, q2, 0) to (ql, q2, c), with c # 0.
Indeed, this is exactly equivalent to the existence of cycles at ql and q2 with the same input label and distinct output weights.
According to lemma 2, it suffices to test the twins property for strings of length less than 21Qll 2 1.
So the following gives an algorithm to test the twins property of a transducer ~-1:, 2.
3. Compute the transitive closure of h T(I).
Determine the set of pairs (ql, q2) of T(I) with distinct states ql # q2For each such {ql, q2}, compute the transitive closure of (ql, q2, 0) in A.
If it contains (ql, q2, c) with c # 0, then ~-1 does not have the twins property.
The operations used in the algorithm (computation of the transitive closure, determination of the set of states) can all be done in polynomial time with respect to the size of A, using classical algorithms (Aho, Hopcroft, and Ullman 1974).
\[\] This provides an algorithm for testing the twins property of an unambiguous trim transducer T.
It is very useful when T is known to be unambiguous.
In many practical cases, the transducer one wishes to determinize is ambiguous.
It is always possible to construct an unambiguous transducer T' from T (Eilenberg 1974-1976).
The complexity of such a construction is exponential in the worst case.
Thus the overall complexity of the test of determinizability is also exponentia ! in the worst case.
Notice that if one wishes to construct the result of the determinization of T for a given input string w, one does not need to expand the whole result of the determinization, but only the necessary part of the determinized transducer.
When restricted to a finite set the function realized by any transducer is subsequentiable, since it has bounded variation?
Acyclic transducers have the twins property, so they are determinizable.
Therefore, it is always possible to expand the result of the determinization algorithm for a finite set of input strings, even if T is not determinizable.
3.6 Determinization
in Other Semirings The determinization algorithm that we previously presented applies as well to transducers mapping strings to other semirings.
We gave the pseudocode of the algorithm in the general case.
The algorithm applies for instance to the real semiring (7"4, +,., 0,1).
One can also verify that (~* U {oc}, A,., cx~, e), where A denotes the longest common prefix operation and  concatenation, o~ a new element such that for any string w E (~* U {~}), w A oo = oo A w = w and woo = eo.
w = oo, defines a left semiring}  We call this semiring the string semiring.
The algorithm of Figure 10 used with the string semiring is exactly the determinization algorithm for subsequentiable string-to-string transducers, as defined by Mohri (1994c).
The cross product of two semirings defines a semiring.
The algorithm also applies when the semiring is the cross product of 9 Using the proof of the theorem of the previous section, it is easy to convince oneself that this assertion can be generalized to any rational subset Y of E* such that the restriction of S, the function T realizes, to Y has bounded variation.
10 A
left semiring is a semiring that may lack right distributivity.
293 Computational Linguistics Volume 23, Number 2 a:b/3 Figure 13 Transducer 7-1 with outputs in ~* x 74.
a:b/3 ~ c:c/5 = b.a/~ d:a/~ Figure 14 Sequential transducer r2 with outputs in ~,,* x 74 obtained from fll by determinization.
(E* Ucx~}, A,., cx~, c) and (T4+ Uoo}, min, +, oo, 0), which allows transducers outputting pairs of strings and weights to be determined.
The determinization algorithm for such transducers is illustrated in Figures 13 and 14.
Subsets in this algorithm are made of triples (q, w, x) E Q x E* u {oo} x 7-4 u {cx~}, where q is a state of the initial transducer, w a residual string, and x a residual output weight.
3.7 Minimization
We here define a minimization algorithm for subsequential power series defined on the tropical semiring, which extends the algorithm defined by Mohri (1994b) in the case of string-to-string transducers.
For any subset L of G* and any string u we define u-lL by: u-IL = {w: uw E L} (22) Recall that L is a regular language iff there exists a finite number of distinct u-lL Nerode (1958).
In a similar way, given a power series S we define a new power series u-is by: n u-iS = y~ (S, uw)w (23) wE~* 11 One can prove that S, a power series defined on a field, is rational if it admits a finite number of independent u-iS (Carlyle and Paz 1971).
This is the equivalent, for power series, of Nerode's theorem for regular languages.
294 Mohri Transducers in Language and Speech For any subsequential power series S we can now define the following relation on ~,*: V(u, v) E ~*, u Rs v 4=~ 3k E T4, (u-lsupp(S) = v-lsupp(S)) and (\[u-iS -1 -V S\]/u-lsupp(S ) = k) (24) It is easy to show that Rs is an equivalence relation.
(u-lsupp(S) = v-lsupp(S)) defines the equivalence relation for regular languages.
Rs is a finer relation.
The additional condition in the definition of Rs is that the restriction of the power series \[u-iS -v-iS\] to u-lsupp(S) = v-lsupp(S) is constant.
The following lemma shows that if there exists a subsequential transducer T computing S with a number of states equal to the number of equivalence classes of Rs, then T is a minimal transducer computing f.
Lemma 3 If S is a subsequential power series defined on the tropical semiring, Rs has a finite number of equivalence classes.
This number is bounded by the number of states of any subsequential transducer realizing S.
Proof Let T -(Q, i, F, ~, 6, or, ~, p) be a subsequential transducer realizing S.
Clearly, ~(U,V) E (~*)2, 6(i,u) = 6(i,v) ~ Vw E u-lsupp(S),6(i, uw) E F ~ 6(i, vw) E F u-lsupp(S) = v-lsupp(S) Also, if u-lsupp(S) = v-lsupp(S), V(u, v) E (~,)2, 6(i, u) = 6(i,v) ~ VW E u-lsupp(S), (S, uw) (S, vw) = or(i, u) (i,v) 4=~ \[u-iS v-lS\]/u-~supp(S) = cr(i,u) cr(i,v) So V(u,v) E (G,)2, 6(i,u) = 6(i,v) ~ (uRsv).
This proves the lemma.
\[\] The following theorem proves the existence of a minimal subsequential transducer representing S.
Theorem 14 For any subsequential function S, there exists a minimal subsequential transducer computing it.
Its number of states is equal to the index of Rs.
Proof Given a subsequential power series S, we define a power series f by: Vu E ~*: u-lsupp(S) = 0, (f,u) = 0 Vu E G*: u-lsupp(S) # O, u) = min (S, uw) wEu-lsupp(S) We then define a subsequential transducer T = (Q, i, F, ~, 6, or, )~, p) by: 12  Q={~: uEG*}; 12 We denote by ~ the equivalence class of u E G*.
295 Computational Linguistics Volume 23, Number 2  i=~;  F = {a: u E ~,* Msupp(S)};  Vu e ~*,Va e ~,6(a,a) = Ca;  vu ~ y,*,va ~ z,~(~,a) = (f,u~) (f,u); ,~ = ff,~);  VqEQ, p(q)=0.
Since the index of Rs is finite, Q and F are well-defined.
The definition of 6 does not depend on the choice of the element u in ~, since for any a E ~, u Rs v implies (ua) Rs (va).
The definition of rr is also independent of this choice, since by definition of Rs, if uRsv, then (ua) Rs (va) and there exists k E T4 such that Vw E ~*, (S, uaw) (S, vaw)= (S, uw) (S, vw) = k.
Notice that the definition of G implies that: Vw ~ s*,(i,w) = (f,w) ff,~) (25) So: Vw E supp(S), A + r(i, w) + p(q) -= (f, w) = rnin (S, ww') w' ew-lsupp(S) S is subsequential, hence: Vw' E w-lsupp(S), (S, ww') < (S, w).
Since Vw E supp(S),  E w-lsupp(S), we have: m:m (S, ww') = (S,w) w' ew-lsupp(S) T realizes S.
This ends the proof of the theorem.
\[\] Given a subsequential transducer T = (Q, i, F, G, 6, cr, A, p), we can define for each state q E Q, d(q) by: d(q) -rnin (er(q,w) + p(6(q,w))) (26) 6(q,w) EF Definition We define a new operation of pushing, which applies to any transducer T.
In particular, if T is subsequential the result of the application of pushing to T is a new subsequential transducer T' --(Q, i, F, ~.., 6, er', A', p') that only differs from T by its output weights in the following way:  ' = ;~ +,~(i);  V(q,a) E Q x Z, G'(q,a) = rr(q,a) +d(6(q,a))-d(q);  Vq E Q,#(q) = O.
According to the definition of d, we have: Vw E G*: 6(q, aw) E F,d(q) < cr(q,a) + er(6(q,a),w) + p(6(6(q,a),w)) This implies that: a(q) <_ o(q,a) + a(6(q,a)) So, r ~ is well-defined: v(q,a) ~ Q x z,~'(q,a) > o 296 Mohri Transducers in Language and Speech Lemma 4 Let T' be the transducer obtained from T by pushing.
T ~ is a subsequential transducer which realizes the same function as T.
Proof That T' is subsequential follows immediately its definition.
Also, Vw E ~',q C Q, cr'(q,w) = rr(q,w) + d(6(q,w) ) -d(q) Since 6(i,w) E F =~ d(6(i,w)) = p(6(i,w)), we have: + o'(i,w) + p'(6(i,w)) = ;, + a(i) + o(q,w) + w)) d(i) + 0 This proves the lemma.
\[\] The following theorem defines the minimization algorithm.
Theorem 15 Let T be a subsequential transducer realizing a power series on the tropical semiring.
Then applying the following two operations: 1.
pushing 2.
automata minimization leads to a minimal transducer.
This minimal transducer is exactly the one defined in the proof of theorem 14.
The automata minimization step in the theorem consists of considering pairs of input labels and associated weights as a single label and of applying classical minimization algorithms for automata (Aho, Hopcroft, and Ullman 1974).
We do not give the proof of the theorem; it can be proved in a way similar to what is indicated in Mohri (1994b).
In general, there are several distinct minimal subsequential transducers realizing the same function.
Pushing introduces an equivalence relation on minimal transducers: T Rp T' iff p(T) = p(T'), where p(T) (resp.
p(T')) denotes the transducer obtained from T (resp.
T t) by pushing.
Indeed, if T and T t are minimal transducers realizing the same function, then p(T) and p(T') are both equal to the unique minimal transducer equivalent to T and T t as defined in theorem 14.
So, two equivalent minimal transducers only differ by their output labels, they have the same topology.
They only differ by the way the output weights are spread along the paths.
Notice that if we introduce a new super final state @ to which each final state q is connected by a transition of weight p(q), then d(q) in the definition of T' is exactly the length of a shortest path from  to q.
Thus, T' can be obtained from T using the classical single-source shortest paths algorithms such as that of Dijkstra (Cormen, Leiserson, and Rivest 1992).
13 In
case the transducer is acyclic, a classical linear time algorithm based on a topological sort of the graph allows one to obtain d.
13 This
algorithm can be extended to the case where weights are negative.
If there is no negative cycle the Bellman-Ford algorithm can be used.
297 Computational Linguistics Volume 23, Number 2 d/O / ~ ~ c/1 e,,Q Figure 15 Transducer ill.
d/O / N ~ c/1 e/O =Q Figure 16 Transducer ~'1 obtained from fll by pushing.
Once the function d is defined, the transformation of T into T' can be done in linear time, namely O(IQ\]+IEI), if we denote by E the set of transitions of T.
The complexity of pushing is therefore linear (O(IQI + IEI)) if the transducer is acyclic.
In the general case, the complexity of pushing is O(IE \] log IQI) if we use classical heaps, O(\]E I + IQI log \]Q\]) if we use Fibonacci heaps, and O(IE I log log IQI) if we use the efficient implementation of priority queues by Thorup (1996).
In case the maximum output weight W is small, we can use the algorithm of Ahuja et al.(1988); the complexity of pushing is then O(IEI + IQIx/fwI).
In case the transducer is acyclic, we can use a specific automata minimization algorithm (Revuz 1992) with linear time complexity, O(\]Q\] + IE\]).
In the general case, an efficient implementation of Hopcroft's algorithm (Aho, Hopcroft, and Ullman 1974) leads to O(\]E\] log \]Q\]).
Thus, the overall complexity of the minimization of subsequential transducers is always as good as that of classical automata minimization: O(IQI + IE\]) in the acyclic case, and O(\]E I log \]Q\[) in the general case.
Figures 15 to 17 illustrate the minimization algorithm.
131 (Figure 15) represents a subsequential string-to-weight transducer.
Notice that the size of fll cannot be reduced using the automata minimization.
71 represents the transducer obtained by pushing, and 51 a minimal transducer realizing the same function as fll in the tropical semiring.
298 Mohri Transducers in Language and Speech d/O c/1 Figure 17 Minimal transducer 61 obtained from "n by automata minimization.
The transducer obtained by this algorithm is the one defined in the proof of theorem 14 and has the minimal number of states.
This raises the question of whether there exists a subsequential transducer with the minimal number of transitions and computing the same function as a given subsequential transducer T.
The following corollary offers an answer.
Corollary 1 A minimal subsequential transducer has also the minimal number of transitions among all subsequential transducers realizing the same function.
Proof This generalizes the analogous theorem that holds in the case of automata.
The proof is similar.
Let T be a subsequential transducer with a minimal number of transitions.
Clearly, pushing does not change the number of transitions of T and automatan minimization, which consists of merging equivalent states, reduces or does not change this number.
Thus, the number of transitions of the minimal transducer equivalent to T as previously defined is less or equal to that of T.
This proves the corollary since, as previously pointed out, equivalent minimal transducers all have the same topology: in particular, they have the same number of states and transitions.
\[\] Given two subsequential transducers, one might wish to test their equivalence.
The importance of this problem was pointed out by Hopcroft and Ullman (1979, 284).
The following corollary addresses this question.
Corollary 2 There exists an algorithm to determine if two subsequential transducers are equivalent.
Proof The algorithm of theorem 15 associates a unique minimal transducer to each subsequential transducer T.
More precisely, this minimal transducer is unique up to a renumbering of the states.
The identity of two subsequential transducers with different numbering of states can be tested in the same way as that of two deterministic automata; for instance, by testing the equivalence of the automata and the equality of their number of states.
An efficient algorithm for testing the equivalence of two deterministic automata is given in Aho, Hopcroft, and Ullman (1974).
14 Since
the min14 The automata minimization step can in fact be omitted if this equivalence algorithm is used, since it does not affect the equivalence of the two subsequential transducers, considered as automata.
299 Mohri Transducers in Language and Speech weight transducer, each path of which corresponds to a sentence.
The weight of the path can be interpreted as a negative log of the probability of that sentence given the sequence of acoustic observations (utterance).
Such acyclic string-to-weight transducers are called word lattices.
4.2 Word
Lattices For a given utterance, the word lattice obtained in such a way contains many paths labeled with the possible sentences and their associated weights.
A word lattice often contains a lot of redundancy: many paths correspond to the same sentence but with different weights.
Word lattices can be directly searched to find the most probable sentences, those which correspond to the best paths, the paths with the smallest weights.
Figure 18 shows a word lattice obtained in speech recognition for the 2,000-word ARPA ATIS Task.
It corresponds to the following utterance: Which flights leave Detroit and arrive at Saint Petersburg around nine am?
Clearly the lattice is complex; it contains about 83 million paths.
Usually, it is not enough to consider the best path of a word lattice.
It is also necessary to correct the best path approximation by considering the n best paths, where the value of n depends on the task considered.
Notice that in case n is very large, one would need to consider, for the lattice in Figure 18, all 83 million paths.
The transducer contains 106 states and 359 transitions.
Determinization applies to this lattice.
The resulting transducer W2 (Figure 19) is sparser.
Recall that it is equivalent to W1, realizing exactly the same function mapping strings to weights.
For a given sentence s recognized by W1, there are many different paths with different total weights.
W2 contains a path labeled with s and with a total weight equal to the minimum of the weights of the paths of W1.
Let us insist on the fact that no pruning, heuristic, or approximation has been used here.
The lattice W2 only contains 18 paths.
Obviously, the search stage in speech recognition is greatly simplified when applied to W2 rather than W1.
W2 admits 38 states and 51 transitions.
The transducer W2 can still be minimized.
The minimization algorithm described in the previous section leads to the transducer W3 shown in Figure 20.
It contains 25 states and 33 transitions and of course the same number of paths as W2, 18.
The effect of minimization appears to be less important.
This is because, in this case, determinizafion includes a large part of the minimization by reducing the size of the first lattice.
This can be explained by the degree of nondeterminism of word lattices such as 14/1.15 Many states can be reached by the same set of strings.
These states are grouped into a single subset during determinization.
Also, the complexity of determinization is exponential in general, but in the case of the lattices considered in speech recognition, it is not.
16 Since
they contain a lot of redundancy, the resulting lattice is smaller than the initial one.
In fact, the time complexity of determinization can be expressed in terms of the initial and resulting lattices, W1 and W2, by O(l~ I log IGl(IWllIW21)2), where IWll and IW21 denote the sizes of W1 and W2.
Clearly if we restrict determinization to the cases where I w21 < I W1 I, its complexity is polynomial in terms of the size of the initial transducer \[Wll.
This also 15 The notion of ambiguity of a finite automaton can be formalized conveniently using the tropical semiring.
Many important studies of the degree of ambiguity of automata have been done in connection with the study of the properties of this semiring (Simon 1987).
16 A
more specific determinization can be used in the cases often encountered in natural language processing where the graph admits a loop at the initial state over the elements of the alphabet (Mohri 1995).
301 Mohri Transducers in Language and Speech Figure 19 Equivalent word lattice W2 obtained by determinization of W1.
@ ....
 .... .@ ~,, @ ~,o ...
@  Figure 20 Equivalent word lattice Wa obtained by minimization from W2.
rescoring l approximate_~J detailed \]._..l~ cde4Pls ~ lattice ~ models Figure 21 Rescoring.
applies to the space complexity of the algorithm.
In practice, the algorithm appears to be very efficient.
As an example, it took about 0.02s on a Silicon Graphics (Indy 100 MHZ Processor, 64 Mb RAM) to determinize the transducer of Figure 18.17 Determinization makes the use of lattices much faster.
Since at any state there exists at most one transition labeled with the word considered, finding the weight associated with a sentence does not depend on the size of the lattice.
The time and space complexity of such an operation is simply linear in the size of the sentence.
When dealing with large tasks, most speech recognition systems use a rescoring method (Figure 21).
This consists of first using a simple acoustic and grammar model to produce a word lattice, and then to reevaluate this word lattice with a more sophisticated model.
The size of the word lattice is then a critical parameter in the time and space efficiency of the system.
The determinization and minimization algorithms we presented allow the size of such word lattices to be considerably reduced, as seen in the examples.
We experimented with both determinization and minimization algorithms in the ATIS task.
Table I illustrates these results.
It shows these algorithms to be very effective in reducing the redundancy of speech networks in this task.
The reduction is also illustrated by an example in the ATIS task.
17 Part
of this time corresponds to I/O's and is therefore independent of the algorithm.
303 Computational Linguistics Volume 23, Number 2 Table 1 Word lattices in the ATIS task.
Determinization Determinization + Minimization Objects reduction factor reduction factor States ~ 3 ~ 5 Transitions,-~ 9 ~ 17 Paths > 232 > 232 Table 2 Subsequential word lattices in the NAB task.
Minimization results Objects reduction factor States ~ 4 Transitions,~ 3 Example 1 Example of a word lattice in the ATIS task.
States: 187 --* 37 Transitions: 993 ~ 59 Paths: > 232 ~ 6,993 The number of paths of the word lattice before determinization was larger than that of the largest integer representable with 32 bit machines.
We also experimented with the minimization algorithm by applying it to several word lattices obtained in the 60,000-word ARPA North American Business News task (NAB).
These lattices were already determinized.
Table 2 shows the average reduction factors we obtained when using the minimization algorithms with several subsequential lattices obtained for utterances of the NAB task.
The reduction factors help to measure the gain of minimization alone, since the lattices are already subsequential.
The numbers in example 2, an example of reduction we obtained, correspond to a typical case.
Example 2 Example of a word lattice in NAB task.
Transitions: 10,8211 ~ 37,563 States: 10,407 --* 2,553 4.3 On-the-fly Implementation of Determinization An important characteristic of the determinization algorithm is that it can be used on-the-fly.
Indeed, the determinization algorithm is such that given a subset representing a state of the resulting transducer, the definition of the transitions leaving that state depends only on that state or, equivalently, on the states of that subset, and on the transducer to determinize.
In particular, the definition and construction of these transitions do not depend directly on the previous subsets constructed.
We have produced an implementation of the determinization that allows one both to completely expand the result or to expand it on demand.
Arcs leaving a state of 304 Mohri Transducers in Language and Speech the determinized transducer are expanded only if necessary.
This characteristic of the implementation is important.
It can then be used, for instance, at any step in an onthe-fly cascade of composition of transducers in speech recognition to expand only the necessary part of a lattice or transducer (Pereira and Riley 1996; Mohri, Pereira, and Riley 1996).
One of the essential implications of the implementation is that it contributes to saving space during the search stage.
It is also very useful in speeding up the n-best decoder in speech recognition.
TM The determinization and minimization algorithms for string-to-weight transducers seem to have other applications in speech processing.
Many new experiments can be done using these algorithms at different stages of speech recognition, which might lead to the reshaping of some of the methods used in this field and create a renewed interest in the theory of automata and transducers.
5. Conclusion We have briefly presented the theoretical bases, algorithmic tools, and practical use of a set of devices that seem to fit the complexity of language and provide efficiency in space and time.
From the theoretical point of view, the understanding of these objects is crucial.
It helps to describe the possibilities they offer and to guide algorithmic choices.
Many new theoretical issues arise when more precision is sought.
The notion of determinization can be generalized to that of E-determinization for instance (Salomaa and Soittola 1978, chapter 3, exercise) requiring more general algorithms.
It can also be extended to local determinization: determinization at only those states of a transducer that admit a predefined property, such as that of having a large number of outgoing transitions.
An important advantage of local determinization is that it can be applied to any transducer without restriction.
Furthermore, local determinization also admits an on-the-fly implementation.
New characterizations of rational functions shed new light on some aspects of the theory of finite-state transducers (Reutenauer and Schiitzenberger 1995).
We have also offered a generalization of the operations we use based on the notions of semiring and power series, which help to simplify problems and algorithms used in various cases.
In particular, the string semiring that we introduced makes it conceptually easier to describe many algorithms and properties.
Subsequential transducers admit very efficient algorithms.
The determinization and minimization algorithms in the case of string-to-weight transducers presented here complete a large series of algorithms that have been shown to give remarkable results in natural language processing.
Sequential machines lead to useful algorithms in many other areas of computational linguistics.
In particular, subsequential power series allow for efficient results in indexation of natural language texts (Crochemore 1986; Mohri 1996b).
We briefly illustrated the application of these algorithms to speech recognition.
More precision in acoustic modeling, finer language models, large lexicon grammars, and a larger vocabulary will lead, in the near future, to networks of much larger sizes in speech recognition.
The determinization and minimization algorithms might help to limit the size of these networks while maintaining their time efficiency.
These algorithms can also be used in text-to-speech synthesis.
In fact, the same operations of composition of transducers (Sproat 1995) and perhaps more important size issues can be found in this field.
18 We
describe this application of determinization elsewhere.
305 Computational Linguistics Volume 23, Number 2 Figure 22 Subsequential power series S nonbisubsequential.
Appendix The determinization algorithm for power series can also be used to minimize transducers in many cases.
Let us first consider the case of automata.
Brzozowski (1962) showed that determinization can be used to minimize automata.
This nice result has also been proved more recently in elegant papers by Bauer (1988) and Urbanek (1989).
These authors refine the method to obtain better complexities.
19 Theorem
16 (Brzozowski 1962) Let A be a nondeterministic automaton.
Then the automaton A' = (Q', i', F', E, 6') obtained by reversing A, applying determinization, rereversing the obtained automaton and determinizing it is the minimal deterministic automaton equivalent to A.
We generalize this theorem to the case of string-to-weight transducers.
We say that a rational power series S is bisubsequential when S is subsequential and the power series S R = Y~w~, (S, wR)w is also subsequential.
2 Not
all subsequential transducers are bisubsequential.
Figure 22 shows a transducer representing a power series S that is not bisubsequential.
S is such that: Vn E.M, (S, ba") = n+l (27) Vn E Af, (S, ca n) = 0 The transducer of Figure 22 is subsequential so S is subsequential.
But the reverse S a is not, because it does not have bounded variation.
Indeed, since: We have: Vn E Af, (S a,anb) = n + l VnE.Af, (Sa, anc) = 0 Vn ~ A/', I(Sa, a"b) (Sa, a"c)l = n + 1 (28) 19 See Watson (1993) for a taxonomy of minimization algorithms for automata; see also Courcelle, Niwinski, and Podelski 1991.
20 For any string w E ~*, we denote by w a its reverse.
306 Mohri Transducers in Language and Speech A characterization similar to that of string-to-string transducers (Choffrut 1978) is possible for bisubsequential power series defined on the tropical semiring.
In particular, the theorem of the previous sections shows that S is bisubsequential iff S and S n have bounded variation.
We similarly define bideterminizable transducers as the transducers T defined on the tropical semiring admitting two applications of determinization, as follows: . . The reverse of T, T a can be determinized.
We denote by det(T a) the resulting transducer.
The reverse of det(TR), \[det(Ta)\] R can also be determinized.
We denote by det(\[det(Ta)\] ~) the resulting transducer.
In this definition, we assume that the reverse operation is performed simply by reversing the direction of the transitions and exchanging initial and final states.
Given this definition, we can present the extension of the theorem of Brzozowski (1962) to bideterminizable transducers.
21 Theorem 17 Let T be a bideterminizable transducer defined on the tropical semiring.
Then the transducer det(\[det(TR)\] R) obtained by reversing T, applying determinization, rereversing the obtained transducer and determinizing it is a minimal subsequential transducer equivalent to T.
Proof We denote by:  T1 = (Q1,il,F1,G,61,Crl,)~l, p1) det(Ta),  T'= (Q',i',F',E,6',',&',p') det(\[det(Ta)\] a)  T" = (Q', i', F', G, ~', rr', &', p') the transducer obtained from T by pushing.
The double reverse and determinization algorithms clearly do not change the function that T realizes.
So T' is a subsequential transducer equivalent to T.
We only need to prove that T ~ is minimal.
This is equivalent to showing that T" is minimal, since T' and T" have the same number of states.
T1 is the result of a determinization, hence it is a trim subsequential transducer.
We show that T' = det(T~) is minimal if T1 is a trim subsequential transducer.
Notice that the theorem does not require that T be subsequential.
Let $1 and $2 be two states of T" equivalent in the sense of automata.
We prove that $1 = $2, namely that no two distinct states of T" can be merged.
This will prove that T" is minimal.
Since pushing only affects the output labels, T' and T" have the same set of states: Q' = Q".
Hence $1 and $2 are also states of T'.
The states of T' can be viewed as weighted subsets whose state elements belong to T1, because T' is obtained by determinization of T~.
Let (q, c) E Q1 x T4 be a pair of the subset $1.
Since T1 is trim there exists w c G* such that 61(il, w) = q, so 6~($1,w) E F'.
Since $1 and S 2 are equivalent, we also have: 21 The theorem also holds in the case of string-to-string bideterminizable transducers.
We give the proof in the more complex case of string-to-weight transducers.
307 Computational Linguistics Volume 23, Number 2 b/1 Figure 23 Transducer f12 obtained by reversing ill.
a/O c/l a/4 ~b/1 ~ Figure 24 Transducer/33 obtained by determinizafion of/32.
d/0 a/4 Figure 25 Minimal transducer f14 obtained by reversing fig and applying determinization.
6~($2, w) E F ~.
Since T1 is subsequential, there exists only one state of T~ admitting a path labeled with w to il; that state is q.
Thus, q E $2.
Therefore any state q member of a pair of $1 is also member of a pair of $2.
By symmetry the reverse is also true.
Thus exactly the same states are members of the pairs of $1 and $2.
There exists k > 0 such that: Sl = {(qo, clo), (ql, c11) .....
(qk, clk)} S 2 ~{(qo, c20), (ql, c21) .....
(qk, C2k)} (29) We prove that weights are also the same in $1 and $2.
Let IIj, (0 > j > k), be the set of strings labeling the paths from il to qi in T1.
Crl(il, w) is the weight output corresponding to a string w E IIj.
Consider the accumulated weights cq, 1 < i < 2, 0 < j < k, in determinization of T~.
Each Clj for instance corresponds to the weight not yet output in the paths reaching $1.
It needs to be added to the weights of any path from qj c $1 to a final state in rev(T1).
In other terms, the determinization algorithm will assign the weight Clj qff1(/1, W) q.X1 to a path labeled with w a reaching a final state of T ~ from $1.
T" is obtained by pushing from T ~.
Therefore the weight of such 308 References 1 Alfred V.
Aho, John E.
Hopcroft, The Design and Analysis of Computer Algorithms, Addison-Wesley Longman Publishing Co., Inc., Boston, MA, 1974 2 Alfred V.
Aho, Ravi Sethi, Jeffrey D.
Ullman, Compilers: principles, techniques, and tools, Addison-Wesley Longman Publishing Co., Inc., Boston, MA, 1986 3 Ahuja, Ravindra K., Kurt Mehlhorn, James B.
Orlin, and Robert Tarjan.
1988. Faster algorithms for the shortest path problem.
Technical Report 193, MIT Operations Research Center.
4 Bauer, W.
1988. On minimizing finite automata.
EATCS bULLETIN, 35.
5 Berstel, Jean.
1979. Transductions and Context-Free Languages.
Teubner Studienbucher, Stuttgart.
6 Jean
Berstel, JJr., Christophe Reutenauer, Rational series and their languages, Springer-Verlag New York, Inc., New York, NY, 1988 7 Brzozowski, J.
A. 1962.
Canonical regular expressions and minimal state graphs for definite events.
Methematical Theory of Automata, 12:529--561.
8 Carlyle, J.
W. and A.
Paz. 1971.
Realizations by stochastic finite automaton.
Journal of Computer and System Sciences, 5:26--40.
9 Choffrut, Christian.
1978. Contributions  l'tude de quelques familles remarquables de functions rationnelles.
Ph.D. thesis, (thse de doctorat d'Etat), Universit Paris 7, LITP, Paris.
10 Thomas
T.
Cormen, Charles E.
Leiserson, Ronald L.
Rivest, Introduction to algorithms, MIT Press, Cambridge, MA, 1990 11 Courcelle, Bruno, Damian Niwinski, and Andreas Podelski.
1991. A geometrical view of the determinization and minimization of finite-state utomata.
Mathematical Systems Theory, 24:117--146.
12 Maxine
Crochemore, Transducers and repetitions, Theoretical Computer Science, v.45 n.1, p.63-86, Sept.
1986 13 Samuel Eilenberg, Automata, Languages, and Machines, Academic Press, Inc., Orlando, FL, 1976 14 Elgot, C.
C. and J.
E. Mezei.
1965. On relations defined by generalized finite automata.
IBM Journal of Research and Development, 9.
15 Ginsburg, S.
and G.
F. Rose.
1966. A characterization of machine mappings.
Canadian Journal of Mathematics, 18.
16 Gross, Maurice.
1989. The use of finite automata in the lexical representation of natural language.
Lecture Notes in Computer Science, 377.
17 John
E.
Hopcroft, Jeffrey D.
Ullman, Introduction To Automata Theory, Languages, And Computation, Addison-Wesley Longman Publishing Co., Inc., Boston, MA, 1990 18 Ronald M.
Kaplan, Martin Kay, Regular models of phonological rule systems, Computational Linguistics, v.20 n.3, September 1994 19 Fred Karlsson, Atro Voutilainen, Juha Heikkila, Arto Anttila, Constraint Grammar: A Language-Independent System for Parsing Unrestricted Text, Mouton de Gruyter, 1995 20 Lauri Karttunen, Ronald M.
Kaplan, Annie Zaenen, Two-level morphology with composition, Proceedings of the 14th conference on Computational linguistics, August 23-28, 1992, Nantes, France 21 Krob, daniel.
1994. The equality problem for rational series with multiplicities in the tropical semiring is undecidable.
Journal of Algebra and Computation, 4.
22 Werner Kuich, Arto Salomaa, Semirings, automata, languages, Springer-Verlag, London, 1985 23 Mehryar Mohri, Compact representations by finite-state transducers, Proceedings of the 32nd annual meeting on Association for Computational Linguistics, p.204-209, June 27-30, 1994, Las Cruces, New Mexico 24 Mehryar Mohri, Minimization of Sequential Transducers, Proceedings of the 5th Annual Symposium on Combinatorial Pattern Matching, p.151-163, June 05-08, 1994 25 Mohri, Mehryar.
1994c. On some applications of finite-state automata theory to natural language processing: Representation of morphological dictionaries, compaction, and indexation.
Technical Report IGM 94--22, Institut Gaspard Monge, Noisy-le-Grand.
26 Mohri, Mehryar.
1994b. Syntactic analysis by local grammars automata: An efficient algorithm.
In Proceedings of the International Conference on Computational Lexicography (COMPLEX94).
Linguistic Institute, Hungarian Academy of Science, Budapest, Hungary.
27 Mohri, Mehryar.
1995. Matching patterns of an automaton.
Lecture Notes in Computer Science, 937.
28 Mohri, Mehryar, 1996a.
On The Use of Sequential Transducers in Natural Language Processing.
In Yves Shabes, editor, Finite State Devices in Natural Language Processing.
MIT Press, Cambridge, MA.
To appear.
29 Mehryar Mohri, On some applications of finite-state automata theory to natural language processing, Natural Language Engineering, v.2 n.1, p.61-80, March 1996 30 Morhi, Mehryar, Fernando C.
N. Pereira, and Michael Riley.
1996. Weighted automata in text and speech processing.
In ECAI-96 Workshop, Budapest, Hungary.
ECAI. 31 Mehryar Mohri, Richard Sproat, An efficient compiler for weighted rewrite rules, Proceedings of the 34th annual meeting on Association for Computational Linguistics, p.231-238, June 24-27, 1996, Santa Cruz, California 32 Nerode, Anil.
1958. Linear automaton transformations.
In Proceedings of AMS, volume 9.
33 Pereira, Fernando C.
N. and Michael Riley, 1996.
Weighted Rational Transductions and their Application to Human Language Processing.
In Yves Shabes, editor, Finite State Devices in Natural Language Processing.
MIT Press, Cambridge, MA.
To appear.
34 Dominique Perrin, Finite automata, Handbook of theoretical computer science (vol.
B): formal models and semantics, MIT Press, Cambridge, MA, 1991 35 Christophe Reutenauer, Marcel Paul Schtzenberger, Varieties and rational functions, Theoretical Computer Science, v.145 n.1-2, p.229-240, July 10, 1995 36 Dominique Revuz, Minimisation of acyclic deterministic automata in linear time, Theoretical Computer Science, v.92 n.1, p.181-189, Jan.
6, 1992 37 Roche, Emmanuel.
1993. Analyse syntaxique transformationnelle du franais par transducteurs et lexique-grammaire.
Ph.D. thesis, Universit Paris 7, Paris.
38 Arto Salomaa, M.
Soittola, Automata: Theoretic Aspects of Formal Power Series, Springer-Verlag New York, Inc., Secaucus, NJ, 1978 39 Schtzenberger, Marcel Paul.
1961. On the definition of a family of automata.
Information and Control, 4.
40 Schtzenberger, Marcel Paul.
1977. Sur une variante des fonctions squentielles.
Theoretical Computer Science.
41 Schtzenberger, Marcel Paul.
1987. Polynomial decomposition of rational functions.
In Lecture Notes in Computer Science, volume 386.
Springer-Verlag, Berlin, Heidelberg, and New York.
42 Silberztein Max.
1993. Dictionnaires lectroniques et analyse automatique de textes: le systme INTEX.
Masson, Paris.
43 Simon, Imre.
1987. The nondeterministic complexity of finite automata.
technical Report RT-MAP-8073, Instituto de Matemtica e Estatistica da Universidade de So Paulo.
44 Sproat, Richard.
1995. A finite-state architecture for tokenization and grapheme-to-phoneme conversion in multilingual text analysis.
In Proceedings of the ACL SIGDAT Workshop, Dublin, Ireland.
ACL. 45 Mikkel Thorup, On RAM priority queues, Proceedings of the seventh annual ACM-SIAM symposium on Discrete algorithms, p.59-67, January 28-30, 1996, Atlanta, Georgia, United States 46 Urbanek, F.
1989. On minimizing finite automata.
EATCS Bulletin, 39.
47 Watson, Bruce W.
1993. A taxonomy of finite automata minimization algorithms.
Technical Report 93/44, Eindhoven University of Technology, The Netherlands.
48 Weber, Andreas and Reinhard Klemm.
1995. Economyof description for single-valued transducers.
Information and Computation, 119.
49 W.
A. Woods, Transition network grammars for natural language analysis, Communications of the ACM, v.13 n.10, p.591-606, Oct. 1970
Automatic Word Sense Discrimination Hinrich Schitze* Xerox Palo Alto Research Center This paper presents context-group discrimination, a disambiguation algorithm based on clustering.
Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word.
Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity.
Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus.
The algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources.
The paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words.
1. Introduction Word sense disambiguation is the task of assigning sense labels to occurrences of an ambiguous word.
This problem can be divided into two subproblems: sense discrimination and sense labeling.
Sense discrimination divides the occurrences of a word into a number of classes by determining for any two occurrences whether they belong to the same sense or not.
Sense labeling assigns a sense to each class, and, in combination with sense discrimination, to each occurrence of the ambiguous word.
This view of disambiguation as a two-stage process may not be completely general (for example, it may not be appropriate for the iterative process by which a lexicographer arrives at the sense divisions of a dictionary entry), but it seems applicable to most work on disambiguation in computational linguistics.
In this paper, we will address the problem of sense discrimination as defined above.
That is, we will not be concerned with the sense-labeling component of word sense disambiguation.
Word sense discrimination is easier than full disambiguation since we need only determine which occurrences have the same meaning and not what the meaning actually is.
Focusing solely on word sense discrimination also liberates us of a serious constraint common to other work on word sense disambiguation.
If sense labeling is part of the task, an outside source of knowledge is necessary to define the senses.
Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al.1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996), thesauri (Yarowsky 1992; Walker and Amsler 1986), bilingual corpora (Brown et al.1991; Church and Gale 1991), or hand-labeled training sets (Hearst 1991; Leacock, Towell, and Voorhees 1993; Niwa and Nitta 1994; Bruce and Wiebe 1994), providing information for sense definitions can be a considerable burden.
What makes our approach unique is that, since we narrow the problem to sense discrimination, we can dispense of an outside source of knowledge for defining senses.
* Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304 Q 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 1 We therefore call our approach automatic word sense discrimination, since we do not require manually constructed sources of knowledge.
In many applications, word sense disambiguation must both discriminate and label occurrences; for example, in order to find the correct translation of an ambiguous word in machine translation or the right pronunciation in a text-to-speech system.
The application of interest to us is information access, i.e., making sense of and finding information in large text databases.
For many problems in information access, it is sufficient to solve the discrimination problem only.
In one study, we measured document-query similarity based on word senses rather than words and achieved a considerable improvement in ranking relevant documents ahead of nonrelevant documents (Schi.itze and Pedersen 1995).
Since the measurement of similarity is a systeminternal process, no reference to externally defined senses need be made.
Another potentially beneficial application of word sense discrimination in information access is the design of interfaces that take account of ambiguity.
If a user enters a query that contains an ambiguous word, a system capable of discrimination can give examples of the different senses of the word in the text database.
The user can then decide which sense was intended and only documents with the intended sense would be retrieved.
Again, a reference to external sense definitions is not required for this task.
The algorithm we propose in this paper is context-group discrimination.
1 Contextgroup
discrimination groups the occurrences of an ambiguous word into clusters, where clusters consist of contextually similar occurrences.
Words, contexts, and clusters are represented in a high-dimensional, real-valued vector space.
Context vectors capture the information present in second-order co-occurrence.
Instead of forming a context representation from the words that the ambiguous word directly occurs with in a particular context (first-order co-occurrence), we form the context representation from the words that these words in turn co-occur with in the training corpus.
Second-order co-occurrence information is less sparse and more robust than first-order information.
In context-group discrimination, the context of each occurrence of the ambiguous word in the training corpus is represented as a context vector formed from secondorder co-occurrence information.
The context vectors are then clustered into coherent groups such that occurrences judged similar according to second-order co-occurrence are assigned to the same cluster.
Clusters are represented by their centroids, the average of their elements.
An occurrence in a test text is disambiguated by computing the second-order representation of the relevant context, and assigning it to the cluster whose centroid is closest to that representation.
Since the choice of representation influences the formation of clusters, we will experiment with several representations in this paper, some involving a dimensionality reduction using singular value decomposition (SVD).
Context-group discrimination can be generalized to do a discrimination task that goes beyond the notion of sense that underlies many other contributions to the disambiguation literature.
If the ambiguous word's occurrences are clustered into a large number n of clusters (e.g., n = 10), then the clusters can capture fine contextual distinctions.
Consider the example of space.
For a small number of clusters, only the senses "outer space" and "limited extent in one, two, or three dimensions" are separated.
If the word's occurrences are clustered into more clusters, then finer distinctions such as the one between "office space" and "exhibition space" are also discovered.
Note that differences between sense entries in dictionaries are often similarly fine-grained.
1 The
basic idea of the algorithm was first described in Schi~tze (1992b).
98 Schiitze Automatic Word Sense Discrimination I WORD I TRAINING TEXT \[VECTORS I WORD SPACE ----T:xxOx!
~x \[\] xi/ L.
Xx~". '......x.
.......... ", TEST CONTEXT / Figure 1 The basic design of context-group discrimination.
Contexts of the ambiguous word in the training set are mapped to context vectors in Word Space (upper dashed arrow) by summing the vectors of the words in the context.
The context vectors are grouped into clusters (dotted lines) and represented by sense vectors, their centroids (squares).
A context of the ambiguous word ("test context") is disambiguated by mapping it to a context vector in Word Space (lower dashed arrow ending in circle).
The context is assigned to the sense with the closest sense vector (solid arrow).
Even if the contextual distinctions captured by generalized context-group discrimination do not line up perfectly with finer distinctions made in dictionaries, they still help characterize the contextual meaning in which the ambiguous word is used in a particular instance.
Such a characterization is useful for the information-access applications described above, among others.
The basic idea of context-group discrimination is to induce senses from contextual similarity.
There is some evidence that contextual similarity also plays a crucial role in human semantic categorization.
Miller and Charles (1991) found evidence in several experiments that humans determine the semantic similarity of words from the similarity of the contexts they are used in.
We hypothesize that, by extension, senses are also based on contextual similarity: a sense is a group of contextually similar occurrences of a word.
The following sections describe the disambiguation algorithm, our evaluation, and the results of the algorithm for a test set drawn from the New York Times News Wire, and discuss the relevance of our approach in the context of other work on word sense disambiguation.
2. Context-Group Discrimination Context-group discrimination groups a set of contextually similar occurrences of an ambiguous word into a cluster, which is then interpreted as a sense.
The particular implementation of this idea described here makes use of a high-dimensional, real-valued vector space.
Context-group discrimination is a corpus-based method: all representations are derived from a large text corpus.
The basic design of context-group discrimination is shown in Figure 1.
Each occurrence of the ambiguous word in the training set is mapped to a point in Word Space (shown for one example occurrence: see dashed line from training text to Word Space).
The mapping is based on word vectors that are looked up in Word Space (to be described below).
Once all training-text contexts have been mapped to Word Space, the resulting point cloud is clustered into groups of points such that points are close to each other in each group and that groups are as distant from each other as 99 Computational Linguistics Volume 24, Number 1 possible.
The resulting clusters are delimited by dotted lines in the figure.
Each cluster is assumed to correspond to a sense of the ambiguous word (an assumption to be evaluated later).
The representative of each group is its centroid, depicted as a square.
After training, a new occurrence of the ambiguous word (labeled "test context" in the figure) is disambiguated by mapping its context to Word Space (see lower dashed line; the context's point is depicted as a circle).
The context is then assigned to the context group whose centroid is closest (solid arrow).
Finally, the context is categorized as being a use of the sense corresponding to this context group.
There are three types of entities that we need to represent: words, contexts, and senses.
They are represented as word vectors, context vectors, and sense vectors, respectively.
Word vectors are derived from neighbors in the corpus, context vectors are derived from word vectors, and sense vectors are derived by way of clustering from the distribution of context vectors.
The representational medium of a vector space was chosen because of its wide acceptance in information retrieval (IR) (see, e.g., Salton and McGill \[1983\]).
The vectorspace model is arguably the most common framework in IR.
Systems based on it have ranked among the best in many evaluations of IR performance (Harman 1993).
The success of the vector-space model motivates us to use it for the representation of words.
We represent words in a space in which each dimension corresponds to a word, just as documents and queries are commonly represented in this space in IR.
Another approach to computing word similarity is the representation of words in a document space in which each dimension corresponds to a document (Lesk 1969; Salton 1971; Qiu and Frei 1993).
There are fewer occurrence-in-document than wordco-occurrence events, so these word representations tend to be more sparse and, arguably, less informative than word-based representations.
Word vectors have also been based on hand-encoded features (Gallant 1991) and dictionaries (Sparck-Jones 1986; Wilks et al.1990). Corpus-based methods like the one proposed here have the advantage that no manual labor is required and that a possible mismatch between a general dictionary and a specialized text (e.g., on chemistry) is avoided.
Finally, word similarity can be computed from structural features like head-modifier relationships (Grefenstette 1994b; Ruge 1992; Dagan, Marcus, and Markovitch 1993; Pereira, Tishby, and Lee 1993; Dagan, Pereira, and Lee 1994).
Like document-based representations, structure-based representations are sparser than those based on co-occurrence.
It is debatable whether structural features are more informative than associational features (Grefenstette 1992, 1996) or not (Schtitze and Pedersen 1997).
Approaches to word representation closely related to ours were proposed by Niwa and Nitta (1994) and Burgess and Lund (1997).
Instead of co-occurrence counts, vector entries are mutual information scores between the word that is to be represented and the dimension words, in Niwa and Nitta's approach.
The algorithms for vector derivation and sense discrimination are described in what follows.
2.1 Word
Vectors A vector for word w is derived from the close neighbors of w in the corpus.
Close neighbors are all words that co-occur with w in a sentence or a larger context.
In the simplest case, the vector has an entry for each word that occurs in the corpus.
The entry for word v in the vector for w records the number of times that word v occurs close to w in the corpus.
It is this representational vector space that we refer to as WOrd Space.
Figure 2 gives a schematic example of two words being represented in a twodimensional space.
The representation is based on the co-occurrence counts of a hypo100 Schiitze Automatic Word Sense Discrimination Table 1 Co-occurrence counts for four words in a hypothetical corpus.
The words legal and clothes are interpreted as dimensions in Figure 2, judge and robe as vectors.
Vector Dimension judge robe legal 300 133 clothes 75 200 LEGAL 300 JUDGE / 133 f f/ ROBE I I 75 200 CLOTHES Figure 2 The derivation of word vectors, judge and robe are represented as word vectors in a two-dimensional space with the dimensions 'legal' and 'clothes'.
Co-occurrence data are from Table 1.
thetical corpus in Table 1.
The word judge has a value of 300 on the dimension "legal" because judge and legal co-occur 300 times with each other (see below for which words are selected as dimensions; a word can be a dimension of Word Space and represented as a word vector in Word Space at the same time).
This vector representation captures the typical topic or subject matter of a word.
For example, words like judge and law are closer to the "legal" dimension; words like robe and tailor are closer to the "clothes" dimension.
By looking at the amount of overlap between two vectors, one can roughly determine how closely they are related semantically.
This is because related meanings are often expressed by similar sets of words.
Semantically related words will therefore co-occur with similar neighbors and their vectors will have considerable overlap.
This similarity can be measured by the cosine between two vectors.
The cosine is equivalent to the normalized correlation coefficient: corr( fi, ~ ) = ~iN=l ViWi where ff and ~ are vectors and N is the dimension of the vector space.
The value of the cosine is higher, the more overlap there is between the neighbors of the two words whose vectors are compared.
If two words occur with exactly the same neighbors 101 Computational Linguistics Volume 24, Number 1 (perfect overlap), then the value of the cosine is 1.0.
If there is no overlap at all, then the value of the cosine is 0.0.
The cosine can therefore be used as a rough measure of semantic relatedness between words.
What words should serve as the dimensions of Word Space?
We will experiment with two strategies: a global and a local one.
The local strategy focuses on the contexts of the ambiguous words and ignores the rest of the corpus.
The global strategy is to select the n most frequent words of the corpus as features and use them regardless of the word that is to be disambiguated.
(See Karov and Edelman \[1996\] for a different approach that selects features according to a combination of global frequency and local salience).
For local selection, we can also use a frequency cutoff.
As an alternative, we will test selection according to a X 2 test.
For the frequency-based selection criterion, the neighbors of the ambiguous word in the corpus are counted.
A neighbor is any word that occurs at a distance of at most 25 words from the ambiguous word (that is, in a 50word window centered on the ambiguous word).
The 1,000 most frequent neighbors are chosen as the dimensions of the space.
For the x2-based criterion, a x2-measure of dependence is applied to a contingency table containing the number of contexts of the ambiguous word in which the candidate word occurs (N++) and does not occur (N+_), and the number of contexts without an occurrence of the ambiguous word in which the candidate word occurs (N_+) and does not occur (N__).
X2 = N(N++N__ N+_N_+) 2 (N++ + N+_)(N_+ + N__)(N++ + N_+)(N+_ + N__) The underlying assumption in using the x2-test is that candidate words whose occurrence depends on whether the ambiguous word occurs will be indicative of one of the senses of the ambiguous word and hence useful for disambiguation.
2 After
1,000 words have been selected in local selection, word vectors are formed by collecting a 1,000-by-I, 000 matrix C, such that element cq records the number of times that words i and j co-occur in a window of size k.
Column n (or, equivalently, row n) of matrix C represents word n.
Note that C is symmetric since the words that are represented as word vectors are also those that form the dimensions of the 1,000dimensional space.
We chose a window size of k = 50 because no improvement of discrimination performance was found in Schfitze (1997) for k > 50.
For global selection, we choose the 20,000 most frequent words as features and the 2,000 most frequent words as dimensions of Word Space.
A global 20, 000-by-2, 000 co-occurrence matrix is derived from the corpus.
Association data were extracted from the training set consisting of 17 months of the New York Times News Service, June 1989 through October 1990.
The size of this set is about 435 megabytes and 60.5 million words.
Two months (November 1990 and May 1989; 46 megabytes, 5.4 million words) were set aside as a test set.
2.2 Context
Vectors The representation for words derived above conflates senses.
For example, both senses of the word suit ('lawsuit' and 'garment') are summed in its word vector, which will therefore be positioned somewhere between the 'legal' and 'clothes' dimensions in Figure 2.
We need to go back to individual contexts in the corpus to acquire information about sense distinctions.
Contexts are represented as context vectors in Word Space.
2 Candidate
words are selected after a list of 930 stopwords has been removed.
This stop list was based on the one used in the Text Data Base system (Cutting, Pedersen, and Halvorsen 1991).
102 Sch~tze Automatic Word Sense Discrimination LEGAL CENTROID LAW / ~JUDGE /iJ~ISTATUTE / I I /// /#SUIT /// I/~// I/1/ CLOTHES Figure 3 The derivation of context vectors.
A context vector is computed as the centroid of the words occurring in the context.
The words in this example context are law, judge, statute, and suit.
A context vector is the centroid (or sum) of the vectors of the words occurring in the context.
Figure 3 shows the context vector of an example context of suit containing the words law, judge, statute, and suit.
Note that the context vector is closer to the "legal' than to the 'clothes' dimension, thus capturing that the context is a 'legal' use of suit.
(The true sum of the four vectors is longer than shown.
Since all correlation coefficients are normalized, the length of a vector does not play a role in the computations).
The centroid "averages" the direction of a set of vectors.
If many of the words in a context have a strong component for one of the topics (like 'legal' in Figure 3), then the average of the vectors, the context vector, will also have a strong component for the topic.
Conversely, if only one or two words represent a particular topic, then the context vector will be weak on this component.
The context vector hence represents the strength of different topical or semantic components in a context.
In the computation of the context vector, we will weight a word vector according to its discriminating potential.
A rough measure of how well word wi discriminates between different topics is the log inverse document frequency used in information retrieval (Salton and Buckley 1990): ai = lg/~ / where ni is the number of documents that wi occurs in and N is the total number of documents.
Poor discriminators of topics are words such as idea or help that are relatively uniformly distributed and therefore have a high document frequency.
Good content discriminators like automobile or China have a bursty distribution (they have several occurrences in a short interval if they occur at all \[Church and Gale 1995\]), and therefore a low document frequency relative to their absolute frequency.
Other algorithms for computing context vectors have been proposed by Wilks et al.(1990) (based on dictionary entries), Gallant (1991) (based on hand-encoded semantic features), Grefenstette (1994b) (based on light parsing), and Niwa and Nitta (1994) (a comparison of dictionary-based and corpus-based context vectors).
103 Computational Linguistics Volume 24, Number 1 LEGAL SENSE 1 Cl..,,,7,s ~SENSE CLOTHES Figure 4 The derivation of sense vectors.
Sense vectors are derived by clustering the context vectors of an ambiguous word (here, cl, c2, c3, c4, c5, c6, c7, and cs), and computing sense vectors as the centroids of the resulting clusters.
The vectors SENSE 1 and SENSE 2 are the sense vectors of clusters {cl, c2, c3, c4} and {cs, c6, c7, cs}, respectively.
2.3 Sense
Vectors Sense representations are computed as groups of similar contexts.
All contexts of the ambiguous word are collected from the corpus.
For each context, a context vector is computed.
This set of context vectors is then clustered into a predetermined number of coherent clusters or context groups using Buckshot (Cutting et al.1992), a combination of the EM algorithm and agglomerative clustering.
The representation of a sense is simply the centroid of its cluster.
It marks the portion of the multidimensional space that is occupied by the cluster.
We chose the EM algorithm for clustering since it is guaranteed to converge on a locally optimal solution of the clustering problem.
In our case, the solution is optimal in that the sum of the squared distances between context vectors and their centroids will be minimal.
In other words, the centroids are optimal representatives for the context vectors in their cluster.
One problem with the EM algorithm is that it finds a solution that is only locally optimal.
It is therefore important to find a good starting point since a bad starting point will lead to a local minimum that is not globally optimal.
Some experimental evidence given below shows that cluster quality varies considerably depending on the initial parameters.
In order to find a good starting point, we use group-average agglomerative clustering (GAAC) on a sample of context vectors.
For each of the 2,000 clustering experiments described below, we first choose a random sample of 50.
This size is roughly equal to v'~, the number of context vectors to be clustered.
Since GAAC is of time complexity O(n2), this guarantees overall linear time complexity of the clustering procedure.
If the training set has more than 2,000 instances of the ambiguous word, 2,000 context vectors are selected randomly.
The centroids of the resulting clusters are then the parameters for the first iteration of EM.
We compute five iterations of the EM algorithm for all experiments since in most cases only a few, if any, context vectors were reassigned in the fifth iteration.
Both the EM algorithm and group-average agglomerative clustering are described in more detail in the appendix.
104 Schfitze Automatic Word Sense Discrimination An example is shown in Figure 4.
The clustering step has grouped context vectors cl, c2, c3, and c4 in the first group and c5, c6, c7, and c8 in the second group.
The sense vector of the first group is the centroid labeled SENSE 1, the sense vector of the second group the centroid labeled SENSE 2.
The result of clustering depends on the representation of context vectors.
For this reason, we also investigate a transformation of the multidimensional space via a singular value decomposition (SVD) (Golub and van Loan 1989).
SVD is a form of dimensionality reduction that finds the major axes of variation in Word Space.
Context vectors can then be represented by their values on these principal dimensions.
The motivation for applying SVD here is much the same as the use of Latent Semantic Indexing (LSI) in information retrieval (Deerwester et al.1990). LSI abstracts away from the surface word-based representation and detects underlying features.
When similarity is computed on these features (via cosine between SVD-reduced context vectors), contextual similarity can be, potentially, better measured than via cosine between unreduced context vectors.
The appendix defines SVD and gives an example matrix decomposition.
In this paper, the word vectors will be reduced to 100 dimensions.
The experiments reported in Schfitze (1992b, 1997) give evidence that reduction to this dimensionality does not decrease accuracy of sense discrimination.
Space requirements for context vectors are reduced to about 1/10 and 1/20 for a 1,000-dimensional and a 2,000dimensional Word Space, respectively.
Although most word vectors are sparse, context vectors are dense, since they are the sum of many word vectors.
Time efficiency is increased on the same order of magnitude when the correlation of context vectors and sense vectors is computed.
The computation of the SVD's in this paper took from a few minutes per word for the local feature set to about three hours for the global feature set.
2.4 Application
of Context-Group Discrimination Context-group discrimination uses word vectors and sense vectors as follows to discriminate occurrences of the ambiguous word.
For an occurrence t of the ambiguous word v:  Map t into its corresponding context vector ~ in Word Space using the vectors of the words in t's context (the lower dashed line in Figure 1).
 Retrieve all sense vectors ~j of v (the two points marked as squares in the figure).
 Assign t to the sense j whose sense vector ~j is closest to ~ (assignment shown as a solid arrow).
This algorithm selects the context group whose sense vector is closest to the context vector of the occurrence of the word that is to be disambiguated.
Context vectors and sense vectors capture semantic characteristics of the corresponding context and sense, respectively.
Consequently, the sense vector that is closest to the context vector has the best semantic match with the context.
Therefore, context-group discrimination categorizes the occurrence as belonging to that sense.
3. Evaluation We test context-group discrimination on the 10 natural ambiguous words that formed the test set in Schfitze (1992b) and on 10 artificial ambiguous words.
Table 2 glosses the major senses of the 20 words.
105 Computational Linguistics Volume 24, Number 1 Table 2 Number of occurrences of test words in training and test set, percent rare senses in test set, baseline performance (all occurrences assigned to most frequent sense), and the two main senses of each of the 20 artificial and natural ambiguous words used in the experiment.
Word Training Test.
Rare Senses Baseline Frequent Senses wide range/ consulting firm 1,422 149 0% 62% heart disease/ reserve board 1,197 115 0% 54% urban development/ cease fire 1,582 101 0% 50% drug administration / fernando valley 1,465 122 0% wide range consulting firm heart disease reserve board urban development cease fire 52% drug administration fernando valley economic development / right field 1,030 88 0% 68% national park/ judiciary committee 1,279 122 0% 70% japanese companies / city hall 1,569 208 0% 58% drug dealers / paine webber 1,183 104 0% 55% league baseball/ square feet 1,097 143 0% 66% pete rose/ nuclear power 1,245 103 0% 52% capital/s 13,015 200 2% 64% interest/s 21,374 200 4% 58% motion/s 2,705 200 0% 55% plant/s 12,833 200 0% 54% economic development right field national park judiciary committee japanese companies city hall drug dealers paine webber league baseball square feet pete rose nuclear power stock of goods seat of government a feeling of special attention a charge for borrowed money movement proposal for action a factory living being 106 Schiitze Automatic Word Sense Discrimination Table 2 Continued.
Word Training Test Rare Senses Baseline Frequent Senses ruling 5,482 200 3.5% 60% an authoritative decision to exert control, or influence space 9,136 200 0% 56% area, volume outer space suit/s 7,467 200 12.5% 57% an action or process in a court a set of garments tank/s 3,909 200 4.5% 90% a combat vehicle a receptacle for liquids train/s 4,271 200 1.5% 74% a line of railroad cars to teach vessel/s 1,618 144 13.9% 69% a ship or plane a tube or canal (as an artery) Artificial ambiguous words or pseudowords are a convenient means of testing disambiguation algorithms (Schtitze 1992a; Gale, Church, and Yarowsky 1992).
It is time-consuming to hand-label a large number of instances of an ambiguous word for evaluating the performance of a disambiguation algorithm.
Pseudowords circumvent this need: Two or more words, e.g., banana and door, are conflated into a new type: banana~door.
All occurrences of either word in the corpus are then replaced by the new type.
It is easy to evaluate disambiguation performance for pseudowords since one can go back to the original text to decide whether a correct decision was made.
To create the pseudowords shown in Table 2, all word pairs were extracted from the corpus, i.e., all pairs of words that occurred adjacent to each other in the corpus in a particular order.
All numbers were discarded, since numbers do not seem to involve sense ambiguity.
Pseudowords were then created by randomly drawing two pairs from those that had a frequency between 500 and 1,000 in the corpus.
Pseudowords were generated from pairs rather than simple words because pairs are less likely than words to be ambiguous themselves.
Pair-based pseudowords are therefore good examples of ambiguous words with two clearly distinct senses.
Table 2 indicates how often the ambiguous word occurred in the training and test sets, how many instances were instances of rare senses, and the baseline performance that is achieved by assigning all occurrences to the most frequent sense.
In the evaluation given here, only senses that account for at least 15% of the occurrences of the ambiguous word are taken into account.
Rare senses are those that account for fewer than 15% of the occurrences.
The words in Table 2 each had two frequent senses.
The frequency of rare senses ranges from 0% to 13.9%, with an average of 2.1%.
Rare senses are not eliminated from the training set.
The training and test sets were taken from the New York Times News Service as described above (training set: June 1989-October 1990; test set: November 1990, May 1989).
If a word had more than 200 occurrences in the test set, then only the first 200 occurrences were included in the evaluation.
The labeling of words in the test corpus was performed by the author.
The distinc107 Computational Linguistics Volume 24, Number 1 tions between the senses in Table 2 are intuitively clear.
For example, the probability of a context in which suit could at the same time refer to a set of garments and an action in court is very low.
Consequently, there were fewer than five instances where the appropriate sense was not obvious from the immediate context.
In these cases, the sense that seemed more plausible to the author was assigned.
It is important to evaluate on a test set that is separate from the training set.
Context-group discrimination is based on the distribution of context vectors in the training set.
The distribution in the training set is often a bad model for the distribution in the test set.
In practice, the intended text of application will be from a time period not covered in the training set (for example, newswire text from after the date of training).
Word distributions can change considerably over time.
The test set was therefore constructed to be from a time period different from the time period of the training set.
This is also the reason that we do not do cross-validation.
Cross-validation respecting the constraint that test and training sets be from different time periods would have required a test set several times larger than the one that was available.
Clustering and evaluating on the same set is also problematic because of sampling variation.
Consider the following example.
We have a set of three context vectors C : {C 1 : (1), c2 = (2), C 3 : (3)} in a one-dimensional space.
Contexts 1 and 2 are uses of sense 1, context 3 is a use of sense 2.
If C is used as both training and evaluation set, then average performance is 83% (with probability 0.5, we get centroids 1.5 and 3 and 100% accuracy, with probability 0.5, we get centroids 1 and 2.5 and 67% accuracy).
If we split C into a training set T of size 2 and a test set E of size 1, we get an average performance of 67% (100% for E = {cl}, 50% for E = {c2}, 50% for E = {3}), which is lower than 83%.
This example shows that conflating training and test set can result in artificially high performance.
An advantage of context-group discrimination is that the granularity of sense distinctions is an adjustable parameter of the algorithm.
Experiments run directly for the senses in Table 2 will test the algorithm's ability to discriminate coarse sense distinctions.
To test performance for fine-grained sense distinctions (e.g., 'office space' vs.
'exhibition space'), we will run two experiments, one that evaluates performance for clustering the context vectors of a word into ten clusters and an information retrieval experiment in which the number of clusters is also large for sufficiently frequent words.
The goal of the 10-cluster experiments is to induce more fine-grained sense distinctions than in the 2-cluster experiments.
However, it is harder to determine the ground truth for fine sense distinctions.
When it comes to fine distinctions, a large number of occurrences are indeterminate or compatible with several of the more finely individuated senses (cf.
Kilgarriff \[1993\]).
For this reason, experiments with a large number of clusters were evaluated using two indirect measures.
The first measure is accuracy for two-way discriminations, i.e., the degree to which each of the ten clusters contained only one of the two "coarse" senses.
This evaluation is indirect because a cluster that contains, say, only 'limited extent in one, two, or three dimensions' uses of space would be deemed 100% correct, yet it could be randomly mixed as far as fine sense distinctions are concerned (e.g., 'office space' vs.
'exhibition space').
The author inspected the data and found good separation of fine-grained senses in the 10-cluster experiments to the extent that the evaluation measure indicated good performance on the two-way discrimination task.
However, because of the above-mentioned subjectivity of judgements for fine sense distinctions, this is hard to quantify.
Results from a second evaluation on an information retrieval task will be presented in Section 4.2 below.
We will show that sense-based information retrieval (in which the relevance of documents to a query is determined using context-group discrimination) 108 Schiitze Automatic Word Sense Discrimination improves the performance of an IR system considerably.
Since the success of sensebased retrieval depends on the accuracy of context-group discrimination, we can infer that the algorithm reliably assigns ambiguous instances to induced senses even in the fine-grained case.
4. Experiments 4.1 Word Sense Discrimination Table 3 shows experimental results for context-group discrimination.
There were four conditions that were varied in the experiments (as described in Section 2):  local vs.
global feature selection  feature selection according to frequency vs.
X 2  term representations vs.
SVD-reduced representations  number of clusters (2 vs.
10) For local feature selection, the other three parameters are varied systematically (the first eight columns of Table 3).
For global feature selection, selection according to X 2 is not possible, since the X 2 test presupposes an event (like the occurrence of an ambiguous word) that the occurrence of candidate words depends on.
There is no such event for global feature selection.
A larger number of dimensions (2,000) is used for the global variant of the algorithm in order to get coverage of a large range of topics that might be relevant for disambiguation.
We therefore apply SVD in the global feature selection case.
Even if word vectors are sparse, context vectors are usually not.
Clustering 2,000dimensional vectors is computationally expensive, so that we only ran experiments with SVD-reduced vectors for the global variant.
Ten experiments with different randomly chosen initial parameters were run for each of the 200 combinations of the different levels of Word, Representation, and Clustering.
The mean percentage correctness and the standard deviation for each such set of 10 experiments is shown in the cells of Table 3.
We give mean and deviation of the percentage of correctly labeled occurrences of all instances in the training set ("total" = "t.'), of the instances of sense 1 ("$1") and of the instances of sense 2 ("$2").
The bottom row of the table gives averages of the total percentage correct numbers over the 20 words covered.
The rightmost column gives averages of the means over the 10 experiments.
We analyzed the results in Table 3 via analysis of variance (ANOVA, see, for example, Ott \[1992\]).
An ANOVA was performed for a 20 x 5 x 2 design with 10 replicates.
The factors were Word, Representation (local, frequency-based, terms; local, frequency-based, SVD; local, Xa-based, terms; local, x2-based, SVD; global, frequencybased, SVD), and Clustering (coarse = 2 clusters, fine = 10 clusters).
Percentages were transformed using the functionf(X) = 2 x sin -1 (v/X) as recommended by Winer (1971).
The transformed percentages have a distribution that is close to a normal distribution as required for the application of ANOVA.
We found that the effects of all three factors and all interactions was significant at the 0.001 level.
These effects are discussed in what follows.
Factor Word.
In general, performance for pseudowords is better than for natural words.
This can be explained by the fact that pseudowords have two focussed senses--the two word pairs they are composed of.
In contrast, some of the senses of natural ambiguous 109 Computational Linguistics Volume 24, Number 1 Table 3 Results of disambiguation experiments.
Rows give total accuracy for each word ("t').
as well as accuracy for the two senses separately ("$1", "$2").
The average in the bottom row is an average over total ("t').
accuracy numbers only.
Columns describe experimental conditions and the mean ("\]~") and standard deviation ("a") of 10 replications of each experiment.
The rightmost column contains an average over the mean values of the 10 experiments.
Pseudowords are abbreviated to the first words of pairs.
Local Global wide~consul.
$1 $2 55 16 100 0 69 31 92 9 74 25 92 13 69 24 82 10 89 6 94 4 t.
51 4 62 0 60 4 66 3 56 6 64 3 65 8 66 2 87 3 87 3 heart~reserve $1 66 0 78 11 100 0 99 4.
72 0 75 12 100 0 98 2 100 0 100 0 $2 100 0 90 7 100 0 100 0 100 0 94 5 98 0 100 1 100 0 100 0 t.
84 0:85 2 100 0 99 287 0 85 3 99 0 99 1 100 0 100 0 urban~cease $1 86 1 87 2 96 0 97 191 4 90 8 98 0 98 1 100 0 98 2 ~rugffern.
.ocon./right nat./jud.
iap./city $1 71rug/paine $1 !eague/square $1 pete~nuclear $1 :apital $1 interest $1,wtion $1 ~lant $1 ruling $1 ;pace $1 ~uit S1 ~ank $1 ~rain $1 vessel $1 Average X 2 Frequency Frequency Terms I SVD Terms I SVD SVD 2 10 2 10 2 10 2 10 2 10 45 16 0 0 45 47 22 22 25 26 19 30 59 37 39 17 84 2 76 8 41.4 81.6 66.4 88.8 98.2 93.8 94.1 $2 78 l J 70 7 100 0 100 1 73 24 80 11 100 0 96 5 100 0 100 0 89.7 t.
82 0i 79 3 98 0 99 1 I 82 10 85 3 99 0 97 2 100 0 99 1 92.0 S1 89 l i 87 7 98 0 100 1 I 94 4 88 5 98 0 95 1 100 0 100 0 94.9 $2 78 1 77 12 95 0 100 1 60 35 90 7 59 8 96 2 100 0 100 1 85.5 t.
84 I 82 3 97 0 100 0 78 15 89 2 80 4 96 1 100 0 100 0 90.6 $1 72 2 89 6 92 1 95 1 92 0 87 5 98 0 98 3 100 0 100 0 92.3 $2 89 0 67 13 96 0 96 2 87 2 91 5 96 0 97 2 100 0 100 0 91.9 t.
78 1 82 1 93 1 95 1 90 1 88 2 98 0 97 1 100 0 100 0 92.1 S1 91 1 96 3 98 0 97 0 99 0 99 0 98 01 97 1 100 0 100 0 97.5 $2 73 0 53 14 100 0 100 1 70 0 61 9 92 0i 96 4 100 0 98 2 84.3 t.
85 1 83 3 98 0 98 0 90 0 87 3 96 0i 97 1 100 0 99 1 93.3 84 18 90 7 96 1 95 1 94 2 91 4 97 2 93 2 99 0 99 1 93.8 i $2 56 10 63 15 71 23 87 4 66 17 71 10 88 5 90 5 99 0 99 1 79.0 t.
73 12 79 3 86 9 92 1 82 6 83 2 93 1 92 1 99 0 99 0 87.8 68 6 76 9 86 1 81 9 70 18 81 14 95 0 85 4 100 0 97 3 83.9 $2 86 13 86 8 100 0 99 1 68 23 87 14 100 0 98 3 100 0 100 0 92.4 t.
76 9 80 2 93 0 89 5 69 19 83 3 97 0 91 2 100 0 98 2 87.6 54 8 77 8 66 41 96 3 32 31 77 10 56 32 90 4 100 0 100 1 74.-----8 $2 60 20 94 3 100 0 99 1 91 18 94 5 100 0 96 4 100 0 99 2 93.3 t.
58 16 88 1 88 14 98 2 71 13 88 1 85 11 94 3 100 0 99 li 86.9 91 0 78 10 94 1 98 2 72 21 90 10 86 19 95 6 100 0 99 1 90.3 $2 78 0 80 8 94 0 91 2 96 1 81 13 88 20 91 7 100 0 99 1 I 89.8 t.
84 0 79 2 94 0 95 2 83 11 86 3 87 19 93 4 100 0 99 1 90.0 88 16 97 3 91 4 96 2 91 3 97 3 93 1 93 2 92 1 93 1 93.1 $2 27 23 36 11 23 34 87 7 36 34 57 9 80 27 88 6 96 1 89 5 61.9 t.
66 7 75 3 66 13 93 2 71 13 82 2 88 10 91 1 94 0 91 1; 81.7 82 18 77 8 95 1 86 5 96 0 93 3 94 1 91 4 96 0 89 3 89.9 $2 43 37 87 4 90 6 96 2 83 1 85 3 71 35 91 4 88 1 93 31 82.7 t.
66 14 81 4 93 2 90 2 90 0 90 1 84 15 91 2 93 0 91 li 86.9 57 14 72 6 58 1 84 1 61 17 88 6 90 15 93 4 85 1 91 5 I 77.-----------9 $2 60 15 70 10 97 0 91 8 58 20 63 16 51 24 77 7 88 13 71 151 72.6 t.
58 10 71 3 76 1 87 3 59 12 77 4 73 12 86 2 86 5 82 5 i 75.5 i 73 20 0 0 92 4 " 0 0 91 16 0 0 54 46 2 5 70 37 0 0' 38.-----2 $2 47 12 100 0 37 5 100 0 41 30 100 0 59 36 100 0 70 26 100 0 75.4 t.
59 8 54 0 63 4 54 0 64 11 54 0 56 7 55 2 70 13 54 0 58.3 75 1 61 13 84 2 71 14 81 1 65 15 79 7 79 13 85 0 82 3 76.2 $2 86 1 90 4 93 1 96 3 87 1 93 4 93 5 95 2 95 0 95 1 92.3 t.
82 0 78 3 90 1 86 4 84 0 82 4 88 1 89 4 91 0 90 1 86.0 10 25 48 30 0 0 48 22 15 25 38 24 16 25 51 15 8 25 54 16 28.8 $2 87 7 91 7 96 0 95 3 97 1 96 2 96 2 96 2 94 10 93 3 94.1 t.
53 7 72 9 54 0 74 8 61 11 71 10 60 12 76 6 56 5 75 6 65.2 83 1 77 5 80 2 85 6 81 2 84 7 94 2 88 8 95 0 83 6 85.0 $2 80 0 i 84 4 93 0 94 2 92 2 88 6 86 29 97 2 96 0 97 2 90.7 t.
82 1 I 80 2 85 1 89 3 86 1 86 2 91 12\] 92 4 95 0 89 3 87.5 29 91 7 6 80 8 32 13 88 5 12 14 86 29i~ 31 22 92 3 28 19 48.5 $2 94 15 100 0 92 95 1 100 0 87 5\] 99 2 84 1 99 2 94.9 4 99 0 t.
87 13 I 90 1 90 3 92 1 95 1 91 1 87 2i 92 1 85 1 92 2 90.1 60 21 100 0 74 16 100 0 89 20 100 0 95 81100 1 79 19 100 0 89.7 $2 40 21 0 0 12 20 0 0 18 29 0 0 8 21! 1 3 55 31 0 0 13.4 t.
55 10' 74 0 58 7 74 0 69 11 74 0 72 1' 74 0 73 8 74 0 69.7 84 18 86 14 100 0 99 1 85 30 90 7 20 42 94 2 30 48 79 5 76.7 $2 76 14 84 9 100 0 100 0 89 3 92 5 79 17 100 0 81 9 100 0 90.1 t.
79 15\] 85 2 100 0 100 0 88 11 91 2 61 14\] 98 1 65 13 93 1 ..
86.O i\[ 72.1 I 77.9 \[ 84.1 i 88.5 i 77.8 i 81.8 i 82.9 \[ 88.3 i 89.7 i 90.6 I\] 110 Schfitze Automatic Word Sense Discrimination Table 4 The Tukey W test shows significantly different performance for the five representations.
Proportions are transformed using fiX) = 2 x sin -1 (v/X).
The rightmost column contains the accuracy A in percent that would correspond to the average value Y in the second column (i.e., f(A) = Y).
Significant difference for a = 0.01:0.034 Average of Difference Corresponding Level 2 x sin-l(V'X) from Closest Accuracy local, )/2, terms 2.11 0.13 76% local, frequency, terms 2.24 0.13 81% local, frequency, SVD 2.44 0.06 88% local, X 2, SVD 2.50 0.06 90% global, frequency, SVD 2.66 0.16 94% words (for example, space and interest) are composed of many different subsenses that are hard to identify for both people and computers.
The only pseudoword with poor performance is wide range/consultingfirm.
This is an illustrative example of a weakness of the particular implementation of contextgroup discrimination chosen here.
Since we only rely on topical information, a word composed of a nontopical sense, like wide range, that can occur in almost any subject area is disambiguated poorly.
The 'area, volume' sense of space and the 'teaching' sense of train are similarly topically amorphous and therefore hard if only topical information is considered.
The poor performance for 'plant' in the 10-cluster experiments is probably due to the way training-set clusters were assigned to senses.
The training set was clustered into 20 clusters and each cluster was given a sense label.
This procedure introduces many misclassifications of individual instances in the training set.
In contrast, a performance of 92% was achieved in Schiitze (1992b) by hand-categorizing the training set, instance by instance.
Note that for some experimental conditions and for some words, performance of two-group clustering is below baseline.
In a completely unsupervised setting, we have to make the assumption that the two induced clusters correspond to two different senses.
In the worst case, we will get, two clusters with identical proportions of the two senses and an accuracy of 50%, below the baseline of assigning all occurrences to a sense that occurs in more than 50% of all cases.
For example, for vessel the worst case would be two clusters each with 69% 'ship' instances and 31% 'tube' instances.
Overall accuracy would be 0.5 x .69 + 0.5 x .31 = 0.5.
It could be argued that the true baseline for unsupervised two-group clustering is 50%, not the proportion of the most frequent sense.
Factor Representation.
A Tukey W test (Ott 1992) was performed to evaluate the factor Representation.
The Tukey W test determines the least significant difference between sample means.
That is, it yields a threshold such that if two levels of a factor differ by more than the threshold, then they are significantly different.
For the factor Representation, in our case, this least significant difference is 0.034 for a = 0.01.
Table 4 shows that all differences are significant.
This is evidence that SVD representations perform better than term representations and that global representations perform better than local representations.
The advantage of SVD representations is partly due to the use of a normality assumption in clustering.
This is a poor approximation for term 111 Computational Linguistics Volume 24, Number 1 Table 5 Occurrence of selected term features in the test set.
The table shows number of words occurring in the test set (averaged over the 20 ambiguous words); number of words occurring per context (averaged over contexts); proportion of words from one representation occurring in another (averaged first over contexts, then over ambiguous words; e.g., on average 91% of X2-selected terms were also in the set selected by local frequency); average number of contexts that a selected term occurred in (e.g., on average a xa-selected term occurred in 8.7 contexts of the artificial ambiguous words, averaged over the words in a context).
~2 Local Frequency Global Frequency Words Occurring in Test Set 283.0 571.2 489.6 Words per Context 6.1 11.1 9.2 Term Overlap X 2 100% 91% 53% local frequency 51% 100% 68% global frequency 34% 78% 100% Average Frequency of terms artificial words 8.7 6.7 16.7 natural words 39.5 22.4 17.5 representations, but is more accurate for SVD-reduced representations.
Why do globally selected features perform better?
Table 5 presents data on the occurrence of selected terms in the test set that are relevant to this question.
Note first that locally selected features seem to do better than globally selected ones on several measures.
More locally selected features occur in the test set ("words occurring in test set": 571.2 vs.
489.6), more local features occur in the individual contexts ("words per context": 11.1 vs.
9.2), and more global features are also local features than vice versa (on a per-context basis, 78% of global features are also local features, but only 68% of local features are also global features), suggesting that local features capture more information than global features.
The first two measures also show that X2-selected features suffer from sparseness.
Both the total number of features that occur in the training set and the number of words per context are small.
This evidence explains why SVD representations that address sparseness do better than term representations for X 2.
To explain the difference in performance between local and global frequency features, we have to break down average accuracy according to artificial and natural ambiguous words.
Average accuracy for artificial ambiguous words is 89.9% (2 clusters) and 92.2% (10 clusters) for local features and 98.6% (2 clusters) and 98.0% (10 clusters) for global features.
Average accuracy for natural ambiguous words is 76.0% (2 clusters) and 84.4% (10 clusters) for local features and 80.8% (2 clusters) and 83.1% (10 clusters) for global features.
These data show a clear split.
Performance of local and global features is comparable for natural ambiguous words.
Global features perform clearly better for artificial ambiguous words.
The last two rows of Table 5 explain this difference in behavior.
The numbers correspond to the average number of contexts that the selected features occur in (averaged first over the words in a context, then over contexts; e.g., a context with three selected terms occurring in 10, 3, and 15 contexts of the ambiguous word in the training set would have an average number of contexts of (10+3+15)/3 = 9.3).
These averages are 11.2 Schi~tze Automatic Word Sense Discrimination small for X 2 and local frequency in the case of artificial ambiguous words.
Clustering can only work well if contexts have enough elements in common so that similarity can be determined robustly.
Apparently, there were too few elements in common for X 2 and local frequency in the case of artificial ambiguous word (and the patterns were so sparse that even SVD was not an effective remedy).
The problem is that artificial ambiguous words are much less frequent in the training set than natural ambiguous words (average frequencies of 1,306.9 vs.
8,231.0), so that reliable feature selection is harder for artificial ambiguous words.
With ample information on natural ambiguous words available in the training set, features can be selected that will occur densely in the test set.
The quality of feature selection for artificial ambiguous words was less successful due to smaller training set sizes.
This analysis reiterates the importance of a clear separation of training and test sets.
Performance numbers will be artificially high if feature selection is done on both training and test sets, avoiding the problems with feature coverage demonstrated in Table 5.
Since global feature selection is simpler and as effective as local approaches, global feature selection is the preferred implementation of context-group discrimination in the general case.
Note, however, that different words may have different optimal representations.
For example, local features work best for vessel.
There are similar individual differences for frequency vs.
x2-based selection.
Frequency-based selection is best for suit, but x2-based selection is better for vessel, at least for SVD-reduced representations.
Factor Clustering.
Fine clustering is generally better than coarse clustering.
The one case for which coarse clustering comes close to the performance of fine clustering is global feature selection.
But this small difference is almost entirely due to the bad performance of fine clustering for plant, which is likely to be due to insufficient handcategorization of the training set, as explained above.
That fine clustering performs better than coarse clustering is not surprising, since more information is used in the evaluation of fine clustering: the labeling of clusters in the training set.
Only coarse clustering is evaluated as strictly unsupervised disambiguation, since we do not have an evaluation set for fine sense distinctions.
Variance. In general, the variance of discrimination accuracy is higher for coarse clustering than for fine clustering.
This is not surprising, given the fact that we evaluate both types of clustering on how well they do on a two-way distinction.
There may be several quite different ways of dividing a set of context vectors into two groups.
But if we first cluster into ten groups and assign these groups to two senses, then the resulting two-way partitions are more likely to resemble each other (even if the initial 10-group clusterings are not very similar).
The experiments indicate that context-group discrimination based on globally selected features is the best implementation in the general case.
The algorithm achieves above-baseline performance (with a small number of exceptions for certain parameter settings).
The average performance of the SVD-based representations of 83% to 91% is satisfactory, although inferior by about 5% to 10%, to disambiguation with minimal manual intervention (e.g., Yarowsky \[1995\]).
3 3 Manually supplied priming information about senses is not the only difference between context-group discrimination and other disambiguation algorithms.
Could one of the other differences be responsible for the difference in performance?
The fact that the error rate more than doubles when the seeds in Yarowsky's (1995) experiments are reduced from a sense's best collocations to just one word per sense suggests that the error rate would increase further if no seeds were provided.
113 Computational Linguistics Volume 24, Number 1 4.2 Application to Information Retrieval Our principal motivation for concentrating on the discrimination subtask is to apply disambiguation to information retrieval.
While there is evidence that ambiguity resolution improves the performance of IR systems (Krovetz and Croft 1992), several researchers have failed to achieve consistent experimental improvements for practically realistic rates of disambiguation accuracy.
Voorhees (1993) compared two term-expansion methods for information retrieval queries, one in which each term was expanded with all related terms and one in which it was only expanded with terms related to the sense used in the query.
She found that disambiguation did not improve the performance of term expansion.
In our study, we will use disambiguation to eliminate document-query matches that are due to sense mismatches (that is, the word in question is used in different types of context in the query and the document).
This approach decreases the number of documents that a query matches with whereas term expansion increases it.
Another important difference in this study is that longer queries are used.
Long queries (as they may arise in an IR system after relevance feedback) provide more context than the short queries Voorhees worked with in her experiments.
Sanderson (1994) modified a test collection by creating pseudowords similar to the ones used in this study.
He found that even unrealistically high rates of disambiguation accuracy had little or no effect on retrieval performance.
An analysis presented in Schfitze and Pedersen (1995) suggests that the main reason for the minor effect of disambiguation is that most of the pseudowords created in the study had a major sense that accounted for almost all occurrences of the pseudoword.
Creating this type of pseudoword amounts to adding a small amount of noise to an unambiguous word, which is not expbcted to have a large effect on retrieval performance.
To some extent, actual dictionary senses have the same property: one sense often accounts for a large proportion of occurrences.
However, this is not necessarily true when rare senses are not taken into account and when high-frequency senses are broken up into smaller groups (the example of 'office space' vs.
'exhibition space').
Large dictionaries tend to break up high-frequency senses into such more narrowly defined subsenses.
The successful use of disambiguation in our study may be due to the fact that rare senses, which are less likely to be useful in IR, are not taken into account and that frequent senses are further subdivided.
Good evidence for the potential utility of disambiguation in information retrieval was provided by Krovetz and Croft (1992).
They showed that there is a considerable amount of ambiguity even in technical text (which is often assumed to be less ambiguous than nonspecialized writing).
Many technical terms have nontechnical meanings that are used in addition to more specialized senses even in technical text (e.g., window and application in computer magazines, convertible in automobile magazines \[Krovetz 1997\]).
Krovetz and Croft also showed that sense mismatches (i.e., spurious matching words that were used in different senses in query and document) occurred significantly more often in nonrelevant than in relevant documents.
This suggests that eliminating spurious matches could improve the separation between nonrelevant and relevant documents and hence the overall quality of retrieval results.
In order to show that context-group discrimination is an approach to disambiguation that is beneficial in information retrieval, we will now summarize the experiment presented in Schfitze and Pedersen (1995).
That experiment evaluates sense-based retrieval, a modification of the standard vector-space model in information retrieval.
(We refer to the standard vector-space model as word-based retrieval).
In word-based retrieval, documents and queries are represented as vectors in a multidimensional space in which each dimension corresponds to a word (similar to the way that we repre114 Schi~tze Automatic Word Sense Discrimination sent word vectors in Word Space).
In sense-based retrieval, documents and queries are also represented in a multidimensional space, but its dimensions are senses, not words.
Words are disambiguated using context-group discrimination.
Documents and queries that contain a word assigned to a particular sense have a nonzero value on the corresponding dimension.
The test corpus in Sch~tze and Pedersen (1995) is the Category B TREC-1 collection (about 170,000 documents from the Wall Street Journal) in conjunction with its queries 51-75 (Harman 1993).
Sense-based retrieval improved average precision by 7.4% when compared to word-based retrieval.
A combination of word-based and sense-based retrieval increased performance by 14.4%.
The greater improvement of the combination is probably due to discrimination errors (i.e., the fact that discrimination is less than 100% correct), which are partially undone by combining sense and word evidence.
Improvement was particularly high when small sets of documents were requested, for example, 16.5% (sense-based) and 19.4% (wordand sense-based combined) for a recall level of 10% of relevant documents.
This experiment suggests a high utility of sense discrimination for information retrieval.
At first sight, sense-based retrieval may seem related to term expansion.
Both sense-based retrieval and term expansion take individual terms as the starting point for modifying the similarity measure that determines which documents are deemed most closely related to the query.
However, the two approaches are actually opposites of each other in the following sense.
Term expansion increases the number of matching documents for a query.
For example, if the query contains cosmonaut and expansion adds astronaut, then documents containing astronaut become additional nonzero matches.
Sense-based retrieval decreases the number of matches.
For example, if the word suit occurs in the query and is disambiguated as being used in the 'legal' sense, then documents that contain suit in a different sense will no longer match with the query.
5. Discussion What distinguishes context-group discrimination from other work on disambiguation is that no outside source of information need be supplied as input to the algorithm.
Other disambiguation algorithms employ various sources of information.
Kelly and Stone (1975) consider hand-constructed disambiguation rules; Lesk (1986), Krovetz and Croft (1989), Guthrie et al.(1991), and Karov and Edelman (1996) use on-line dictionaries; Hirst (1987) constructs knowledge bases; Cottrell (1989) uses syntactic and semantic structure encoded in a connectionist net; Brown et al.(1991) and Church and Gale (1991) exploit bilingual corpora; Dagan, Itai, and Schwall (1991) use a bilingual dictionary; Hearst (1991), Leacock, Towell, and Voorhees (1993), Niwa and Nitta (1994), and Bruce and Wiebe (1994) exploit a hand-labeled training set; and Yarowsky (1992) and Walker and Amsler (1986) perform computations based on a hand-constructed semantic categorization of words (Roget's Thesaurus and Longman's subject codes, respectively).
For some of these algorithms, the expense of supplying information to the disambiguation algorithm is relatively small.
For example, in many of the methods using hand-labeled training sets (e.g., Hearst \[1991\]), a relatively small number of training examples is sufficient.
Yarowsky has proposed an algorithm that requires as little user input as one seed word per sense to start the training process (Yarowsky 1995).
Such minimal user input will be a negligible burden for users in some situations.
However, consider the interactive information-access application described above.
When asked to improve their initial ambiguous information request many users will be reluctant to 115 Computational Linguistics Volume 24, Number 1 give a seed word or a set of good features for each sense of the word.
They are more likely to satisfy a request by the system to choose the correct sense (e.g., by mouse click), if example contexts corresponding to different senses are presented without the requirement of additional user interaction.
In an application like this, it is of great advantage that context-group discrimination does not require any manual intervention to induce senses.
Another body of related work is the literature on word clustering in computational linguistics (Brown et al.1992; Finch 1993; Pereira, Tishby, and Lee 1993; Grefenstette 1994a) and document clustering in information retrieval (van Rijsbergen 1979; Willett 1988; Sparck-Jones 1991; Cutting et al.1992). In contrast to this earlier work, we cluster contexts or, equivalently, word tokens here, not words (or, more precisely, word types) or documents.
The straightforward extension of word-type clustering and document clustering to word-token clustering would be to represent a token by all words it cooccurs with in its context and cluster these representations.
Such an approach based on first-order co-occurrence is used, for example, by Hearst and Plaunt (1993) for the representation of tiles or document subunits that are similar to our notion of context.
Instead, we use second-order co-occurrence to represent the tokens of ambiguous words: the words that occur with the token are in turn looked up in the training corpus and the words they co-occur with are used to represent the token.
Secondorder representations are less sparse and more robust than first-order representations.
In a cluster-based approach, the subdivision of the universe of elements into clusters depends on the representation.
If the representation does not capture the information crucial for distinguishing senses, then context-group discrimination performs poorly.
The clearest such example in the above experiments is the pseudoword wide range~consulting firm.
The algorithm does not do better than the baseline of always choosing the most frequent sense.
The reason is that the representation captures only topic information.
So a cluster will contain a group of contexts that are about the same topic.
Unfortunately, the pair wide range can come up in text about almost any topic.
Since there is no clear topical characterization of one sense of the pseudoword, context-group discrimination performs poorly.
The reliance on topical similarity may also be the reason that performance for pseudowords is generally better than performance for natural ambiguous words.
All pseudowords except for wide range/consultingJirm are composed of two pairs from different topics.
For example, heart disease and reserve board pertain to biology and finance, respectively, two clearly distinct topics.
On the other hand, the senses of some of the ambiguous words have less clear associations with particular topics.
For example, one can be trained to perform a wide variety of activities, so the 'teaching' sense of train can be invoked in many different topics.
Part of the superior performance for pseudowords is due to this different topic sensitivity of natural and artificial ambiguous words.
The limitation to topical distinctions is not so much a flaw of context-group discrimination as a flaw of the particular implementation we have presented here.
It is possible to integrate information in the context vectors that reflect syntactic or subcategorization behavior of different senses, such as the output of a shallow parser as used in Pereira, Tishby, and Lee (1993).
For example, one good indicator of the two senses of the word interest is a preposition occurring to its right.
The phrase interest in invokes the 'feeling of attention' sense, the phrase interest on, the sense 'charge on borrowed money'.
It seems plausible that performance could be improved for words whose senses are less sensitive to topical distinctions if such "proximity" information is integrated.
In some recent experiments, Pedersen and Bruce (1997) have used proximity features (tags of close words and the presence or absence of close functions words 116 Schfttze Automatic Word Sense Discrimination and content words) with some promising results.
This suggests that a combination of the topical features used here and proximity features may give optimal performance of context-group discrimination.
4 We
have used only one source of information (topical features) in the interest of simplicity, not because we see any inherent advantage of topical features compared to a combination of multiple sources of evidence.
Our justification for the basic idea of context-group discrimination, inducing senses from contextual similarity, has been that its results seem to align well with the ground truth of senses defined in dictionaries.
However, there is also some evidence that contextual similarity plays a crucial role in human semantic categorization.
In one study, Miller and Charles (1991) found evidence that human subjects determine the semantic similarity of words from the similarity of the contexts they are used in.
They summarized this result in the following hypothesis: Strong Contextual Hypothesis: Two words are semantically similar to the extent that their contextual representations are similar.
(p. 8) A contextual representation of a word is knowledge of how that word is used.
The hypothesis states that semantic similarity is determined by the degree of similarity of the sets of contexts that the two words can be used in.
The hypothesis that underlies context-group discrimination is an extension of the Strong Contextual Hypothesis to senses: Contextual Hypothesis for Senses: Two occurrences of an ambiguous word belong to the same sense to the extent that their contextual representations are similar.
So a sense is simply a group of occurrence tokens with similar contexts.
The analogy between the contextual hypotheses for words and senses is that both word types and word tokens are semantically similar to the extent that their contexts are semantically similar.
A group of contextually similar word tokens is a sense.
Miller and Charles's work thus provides a justification for our framework, the induction of senses from contextual similarity.
There are several issues that need to be addressed in future work on context-group discrimination.
First, our experiments only considered words with two major senses.
The algorithm also needs to be tested for words with more than two frequent senses and for infrequent senses.
Second, our test set consisted of a relatively small number of natural ambiguous words.
This is a flaw of almost all contemporary work on word sense disambiguation, but in the future more extensive test sets will be required to establish the general applicability of disambiguation algorithms.
Finally, the implementation of context-group discrimination proposed here is based on topical similarity only.
It will be necessary to incorporate other, more structural constraints (such as the interest in vs.
interest on case discussed above) to achieve adequate performance for a wide variety of ambiguous words.
Appendix A: Singular Value Decomposition A singular value decomposition factors an m-by-n matrix A into a product of three matrices: (,)A = U diag (o1,...,o.p)V T 4 See Leacock (1993) for a discussion of proximity and topical features in supervised disambiguation.
117 Computational Linguistics Volume 24, Number 1 Table 6 Co-occurrence counts for eight words in a five-dimensional word space.
judge suit robe gangster criminal police gun bail legal 300 210 133 30 200 160 120 150 clothes 75 182 200 10 5 10 20 15 cop 100 75 25 250 10 140 200 160 fashion 5 100 200 5 5 5 5 5 pants 5 110 190 5 5 5 5 5 Table 7 SVD reduction to two dimensions of the matrix in Table 6.
judge suit robe gangster criminal police gun bail dim1 -0.47 -0.46 -0.41 -0.22 -0.31 -0.30 -0.30 -0.30 dim2 0.13 -0.31 -0.69 0.41 0.05 0.25 0.33 0.28 Table 8 Correlation coefficients of three words before and after SVD dimensionality reduction.
criminal robe Word Space SVD Space Word Space SVD Space gangster 0.17 0.61 0.15 -0.52 criminal 0.41 0.37 where p = minm, n}, U (the left matrix) is an orthonormal m-by-p matrix, V (the right matrix) is an orthonormal n-by-p matrix and diag(o.1 ....., o.p) is a matrix with the diagonal elements o.1 _> o'2 > "" _> o.p ~_ 0 (and the value zero for nondiagonal elements) (Golub and van Loan 1989).
Dimensionality reduction can be based on SVD by keeping only the first k singular values o.1    c~k and setting the remaining ones to zero.
It can be shown that the product A' = U diag(o'l ..... o.k)V T is the closest approximation to A in a k-dimensional space (that is, there is no matrix of rank k with a smaller least-square distance to A than A').
See Golub and van Loan (1989) and Berry (1992) for a detailed description of SVD and efficient algorithms to compute it.
The benefits of dimensionality reduction for our purposes can best be explained using an example.
Table 6 shows co-occurrence counts from a hypothetical corpus (e.g., legal and robe co-occur 133 times with each other).
Note that two semantically similar words, gangster and criminal, have a low correlation in the words they co-occur with because they belong to different registers (this is one of reasons that topically similar words can have few neighbors in common).
Table 7 shows the two columns of the right matrix V of the SVD of the matrix in Table 6.
Table 7 is therefore a dimensionality reduction of Table 6 to two dimensions.
The advantage of the reduced space is that it directly represents the similar topicality of gangster and criminal: their vectors are close to each other in the space, as shown in Figure 5.
On the other hand, both words' vectors 118 Schiitze Automatic Word Sense Discrimination DIMENSION 2 GANGSTER CRIMINA L ROBE r DIMENSION 1 Figure 5 The vectors for robe, gangster, and criminal in the reduced SVD space.
The words gangster and criminal are represented as semantically similar.
Both are represented as semantically dissimilar from robe.
are less correlated with a topically dissimilar word like robe in the reduced space.
The correlation coefficients of the three words are shown in Table 8 for the unreduced and the reduced space.
The correlation of the topically related words (gangster and criminal) increases from 0.17 to 0.61, whereas the correlation of both words with robe decreases.
This example demonstrates the effect of SVD dimensionality reduction: topically similar words are projected closer to each other in the reduced space; topically dissimilar words are projected to distant locations.
Part of the motivation for using SVD for word vectors is the success of latent semantic indexing (LSI) in information retrieval (Deerwester et al.1990). LSI projects topically similar documents to close locations in the reduced space, just as we project topically similar words to close locations.
Appendix B: The EM Algorithm The clustering algorithm used in this paper is the EM algorithm.
The observed data (context vectors in our case) are interpreted as being generated by hidden causes, the clusters.
The EM algorithm is an iterative procedure that, starting from an initial hypothesis of the cluster parameters, improves the estimates of the parameters in each iteration.
We follow here the discussion and notation in Dempster, Laird, and Rubin (1977) and Ghahramani (1994).
We make the assumption that each cluster j is a Gaussian source with density ~j: ~j(~) exp\[ where \]/j is the mean and Gj the covariance matrix of a;j.
We write Oj = (fij, Gj) for the parameters of cluster j.
119 Computational Linguistics Volume 24, Number 1 Assume that we have N d-dimensional context vectors,g = {Xl ...
XN} C T4 d generated by M Gaussians COl...
CVM. The EM algorithm iteratively applies the Expectation step (E step) and the Maximization step (M step).
The E step is the estimation of parameters hq where hq is the probability of event zij, the event that cluster j generated Xi (context vector i).
hij = E(zij I ~i; O k) = O k is 0 at iteration k.
P(~ I ~J ;0~) .p(~l~j;o k) -~j(~;)P(~j) G~ P(~ I ~,; o~) The M step computes the most likely parameters of the distribution given the cluster membership probabilities: ~\]i=1 /j k+l E/N1 hq(2:i lif)(Y:i 11~) T ~j = ~N=lhq These are the well-known maximum-likelihood estimates for mean and variance of a Gaussian.
Recomputed means and variances are the parameters for the next iteration k+l.
For reasons of computational efficiency, we chose the implementation of the EM clustering known as k-means or hard clustering (Duda and Hart 1973).
In each iteration, context vectors are first assigned to the cluster with the closest mean; then cluster means are recomputed as the centroid of all members of the cluster.
This amounts to assuming a very small fixed variance for all clusters and only re-estimating the means in each step.
The initial cluster parameters are computed by applying group-average agglomerative clustering to a sample of size v'N.
Appendix C: Agglomerative Clustering Agglomerative clustering is a clustering technique that starts by assigning each element to a different cluster and then iteratively merges clusters according to a goodness criterion until the desired number of clusters has been reached.
Two such goodness measures give rise to single-link clustering and complete-link clustering.
Single-link clustering in each step merges the two clusters that have two elements with the smallest distance of any two clusters.
Complete-link clustering in each step executes the merger whose resulting cluster has the smallest diameter of all possible mergers.
Single-link clustering has been found in practice to produce elongated clusters (e.g., two parallel lines) that do not correspond well to the intuitive notion of a cluster as a mass of points with a center.
Complete-link clustering is strongly affected by outliers and has a time complexity cubic in the number of points to be merged and, hence, is less efficient than single-link clustering (which can be computed in quadratic time).
In this paper, we chose group-average agglomerative clustering (GAAC) as our clustering algorithm, a hybrid of single-link and complete-link clustering.
GAAC in each iteration executes the merger that gives rise to the cluster F with the largest average correlation C(P): 1 1 C(P) 2 IPl(\[rl1) ~ ~ corr(~,7~) ~cF/~cP 120 References 1 Berry, Michael W.
1992. Large-scale sparse singular value computations.
The International Journal of Supercomputer Applications, 6(1):13--49.
2 Peter
F.
Brown, Stephen A.
Della Pietra, Vincent J.
Della Pietra, Robert L.
Mercer, Word-sense disambiguation using statistical methods, Proceedings of the 29th annual meeting on Association for Computational Linguistics, p.264-270, June 18-21, 1991, Berkeley, California 3 Peter F.
Brown, Peter V.
deSouza, Robert L.
Mercer, Vincent J.
Della Pietra, Jenifer C.
Lai, Class-based n-gram models of natural language, Computational Linguistics, v.18 n.4, p.467-479, December 1992 4 Rebecca Bruce, Janyce Wiebe, Word-sense disambiguation using decomposable models, Proceedings of the 32nd annual meeting on Association for Computational Linguistics, p.139-146, June 27-30, 1994, Las Cruces, New Mexico 5 Burgess, Curt and Kevin Lund.
1997. Modelling parsing constraints with high-dimensional context space.
Language and Cognitive Processes, 12.
To appear.
6 Church, Kenneth W.
and William A.
Gale. 1991.
Concordances for parallel text.
In Proceedings of the Seventh Annual Conference of the UW Centre for the New OED and Text Research, pages 40--62, Oxford, England.
7 Church, Kenneth and William Gale.
1995. Poisson mixtures.
Journal of Natural Language Engineering, 1(2):163--190.
8 Garrison
W.
Cottrell, A connectionist approach to word sense disambiguation, Morgan Kaufmann Publishers Inc., San Francisco, CA, 1989 9 Douglass R.
Cutting, David R.
Karger, Jan O.
Pedersen, Constant interaction-time scatter/gather browsing of very large document collections, Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, p.126-134, June 27-July 01, 1993, Pittsburgh, Pennsylvania, United States 10 Cutting, Douglass R., Jan O.
Pedersen, and Per-Kristian Halvorsen.
1991. An object-oriented architecture for text retrieval.
In Proceedings of RIAO'91, pages 285--298, Barcelona, Spain.
11 Douglass
R.
Cutting, David R.
Karger, Jan O.
Pedersen, John W.
Tukey, Scatter/Gather: a cluster-based approach to browsing large document collections, Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval, p.318-329, June 21-24, 1992, Copenhagen, Denmark 12 Ido Dagan, Alon Itai, Ulrike Schwall, Two languages are more informative than one, Proceedings of the 29th annual meeting on Association for Computational Linguistics, p.130-137, June 18-21, 1991, Berkeley, California 13 Ido Dagan, Shaul Marcus, Shaul Markovitch, Contextual word similarity and estimation from sparse data, Proceedings of the 31st annual meeting on Association for Computational Linguistics, p.164-171, June 22-26, 1993, Columbus, Ohio 14 Ido Dagan, Fernando Pereira, Lillian Lee, Similarity-based estimation of word cooccurrence probabilities, Proceedings of the 32nd annual meeting on Association for Computational Linguistics, p.272-278, June 27-30, 1994, Las Cruces, New Mexico 15 Deerwester, Scott, Susan T.
Dumais, George W.
Furnas, Thomas K.
Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis.
Journal of the American Society for Information Science, 41(6):391--407.
16 Dempster, A.
P., N.
M. Laird, and D.
B. Rubin.
1977. Maximum likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society, Series B, 39:1--38.
17 Richard
O.
Duda, Peter E.
Hart, David G.
Stork, Pattern Classification (2nd Edition), Wiley-Interscience, 2000 18 Finch, Steven Paul.
1993. Finding Structure in Language.
Ph.D. thesis, University of Edinburgh.
19 Gale, William A., Kenneth W.
Church, and David Yarowsky.
1992. Work on statistical methods for word sense disambiguation.
In Robert Goldman, Peter Norvig, Eugene Charniak, and Bill Gale, editors, Working Notes of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language, pages 54--60, AAAI Press, Menlo Park, CA.
20 Stephen I.
Gallant, A practical approach for representing context and for performing word sense disambiguation using neural networks, Neural Computation, v.3 n.3, p.293-309, Fall 1991 21 Ghahramani, Zoubin.
1994. Solving inverse problems using an EM approach to density estimation.
In Michael C.
Mozer, Paul Smolensky, David S.
Touretzky, and Andreas S.
Weigend, editors, Proceedings of the 1993 Connectionist Models Summer School, Erlbaum Associates, Hillsdale, NJ.
22 Golub, Gene H.
and Charles F.
van Loan.
1989. Matrix Computations.
The Johns Hopkins University Press, Baltimore and London.
23 Gregory Grefenstette, Use of syntactic context to produce term association lists for text retrieval, Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval, p.89-97, June 21-24, 1992, Copenhagen, Denmark 24 Grefenstette, Gregory.
1994a. Corpus-derived first, second and third-order word affinities.
In Proceedings of the Sixth Euralex International Congress, Amsterdam.
25 Gregory Grefenstette, Explorations in Automatic Thesaurus Discovery, Kluwer Academic Publishers, Norwell, MA, 1994 26 Gregory Grefenstetti, Evaluation techniques for automatic semantic extraction: comparing syntactic and window based approaches, Corpus processing for lexical acquisition, MIT Press, Cambridge, MA, 1996 27 Joe A.
Guthrie, Louise Guthrie, Yorick Wilks, Homa Aidinejad, Subject-dependent co-occurrence and word sense disambiguation, Proceedings of the 29th annual meeting on Association for Computational Linguistics, p.146-152, June 18-21, 1991, Berkeley, California 28 Harman, D.
K., editor.
1993. The First Text REtrieval Conference (TREC-1).
U.S. Department of Commerce, Washington, DC.
NIST Special Publication 500--207.
29 Hearst, Marti A.
1991. Noun homograph disambiguation using local context in large text corpora.
In Proceedings of the Seventh Annual Conference of the UW Centre for the New OED and Text Research: Using Corpora, pages 1--22, Oxford.
30 Marti A.
Hearst, Christian Plaunt, Subtopic structuring for full-length document access, Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, p.59-68, June 27-July 01, 1993, Pittsburgh, Pennsylvania, United States 31 Graeme Hirst, Semantic interpretation and the resolution of ambiguity, Cambridge University Press, New York, NY, 1987 32 Anil K.
Jain, Richard C.
Dubes, Algorithms for clustering data, Prentice-Hall, Inc., Upper Saddle River, NJ, 1988 33 Karov, Yael and Shimon Edelman.
1996. Learning similarity-based word sense disambiguation from sparse data.
In Proceedings of the Fourth Workshop on Very Large Corpora.
34 Kelly, Edward and Phillip Stone.
1975. Computer Recognition of English Word Senses.
North-Holland, Amsterdam.
35 Kilgarriff, Adam.
1993. Dictionary word sense distinctions: An enquiry into their nature.
Computers and the Humanities, 26:365--387.
36 Robert Krovetz, Homonymy and polysemy in information retrieval, Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics, p.72-79, July 07-12, 1997, Madrid, Spain 37 R.
Krovetz, W.
B. Croft, Word sense disambiguation using machine-readable dictionaries, Proceedings of the 12th annual international ACM SIGIR conference on Research and development in information retrieval, p.127-136, June 25-28, 1989, Cambridge, Massachusetts, United States 38 Robert Krovetz, W.
Bruce Croft, Lexical ambiguity and information retrieval, ACM Transactions on Information Systems (TOIS), v.10 n.2, p.115-141, April 1992 39 Leacock, Claudia, Geoffrey Towell, and Ellen Voorhees.
1993. Towards building contextual representations of word senses using statistical models.
In Branimir Boguraev and James Pustejovsky, editors, Acquisition of Lexical Knowledge From Text: Workshop Proceedings, pages 10--21, Ohio.
40 Claudia Leacock, Geoffrey Towell, Ellen Voorhees, Corpus-based statistical sense resolution, Proceedings of the workshop on Human Language Technology, March 21-24, 1993, Princeton, New Jersey 41 Lesk, M.
E. 1969.
Word-word association in document retrieval systems.
American Documentation, 20(1):27--38.
42 Michael Lesk, Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone, Proceedings of the 5th annual international conference on Systems documentation, p.24-26, June 1986, Toronto, Ontario, Canada 43 Miller, George A.
and Walter G.
Charles. 1991.
Contextual correlates of semantic similarity.
Language and Cognitive Processes, 6(1):1--28.
44 Yoshiki Niwa, Yoshihiko Nitta, Co-occurrence vectors from corpora vs.
distance vectors from dictionaries, Proceedings of the 15th conference on Computational linguistics, August 05-09, 1994, Kyoto, Japan 45 Ott, Lyman.
1992. An Introduction to Statistical Methods and Data Analysis.
Wadsworth, Belmont, CA.
46 Pedersen, Ted and Rebecca Bruce.
1997. Distinguishing word senses in untagged text.
In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 197--207, Providence, RI.
47 Fernando Pereira, Naftali Tishby, Lillian Lee, Distributional clustering of English words, Proceedings of the 31st annual meeting on Association for Computational Linguistics, p.183-190, June 22-26, 1993, Columbus, Ohio 48 Yonggang Qiu, Hans-Peter Frei, Concept based query expansion, Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, p.160-169, June 27-July 01, 1993, Pittsburgh, Pennsylvania, United States 49 Gerda Ruge, Experiment on linguistically-based term associations, Information Processing and Management: an International Journal, v.28 n.3, p.317-332, 1992 50 Salton, Gerard.
1971. Experiments in automatic thesaurus construction for information retrieval.
In Proceedings IFIP Congress, pages 43--49.
51 Salton, Gerard and Chris Buckley.
1990. Improving retrieval performance by relevance feedback.
Journal of the American Society for Information Science, 41(4):288--297.
52 Gerard Salton, Michael J.
McGill, Introduction to Modern Information Retrieval, McGraw-Hill, Inc., New York, NY, 1986 53 Mark Sanderson, Word sense disambiguation and information retrieval, Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, p.142-151, July 03-06, 1994, Dublin, Ireland 54 Schtze, Hinrich.
1992a. Context space.
In Robert Goldman, Peter Norvig, Eugene Charniak, and Bill Gale, editors, Working Notes of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language, pages 113--120, AAAI Press, Menlo Park, CA.
55 H.
Schtze, Dimensions of meaning, Proceedings of the 1992 ACM/IEEE conference on Supercomputing, p.787-796, November 16-20, 1992, Minneapolis, Minnesota, United States 56 Schtze, Hinrich.
1997. Ambiguity Resolution in Language Learning.
CSLI Publications, Stanford, CA.
57 Schtze, Hinrich and Jan O.
Pedersen. 1995.
Information retrieal based on word senses.
In Proceedings for the Fourth Annual Symposium on Document Analysis and Information Retrieval, pages 161--175, Las Vegas, NV.
58 Hinrich Schtze, Jan O.
Pedersen, A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management: an International Journal, v.33 n.3, p.307-318, May 1997 59 Sparck-Jones, Karen.
1986. Synonymy and Semantic Classification.
Edinburgh University Press, Edinburgh.
(Publication of Ph.D. thesis, University of Cambridge, 1964).
60 Karen Sparck Jones, Notes and references on early automatic classification work, ACM SIGIR Forum, v.25 n.1, p.10-17, Spring 1991 61 C.
J. Van Rijsbergen, Information Retrieval, Butterworth-Heinemann, Newton, MA, 1979 62 Ellen M.
Voorhees, Using WordNet to disambiguate word senses for text retrieval, Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, p.171-180, June 27-July 01, 1993, Pittsburgh, Pennsylvania, United States 63 Walker, Donald E.
and Robert A.
Amsler. 1986.
The use of machine-readable dictionaries in sublanguage analysis.
In Ralph Grishman and Richard Kittredge, editors, Analyzing Language in Restricted Domains: Sublanguage Description and Processing.
L. Erlbaum Associates, Hillsdale, NJ, pages 69--84.
64 Wilks, Yorick A., Dan C.
Fass, Cheng Ming Guo, James E.
McDonald, Tony Plate, and Brian M.
Slator. 1990.
Providing machine tractable dictionary tools.
Journal of Computers and Translation, 2.
65 Peter Willett, Recent trends in hierarchic document clustering: a critical review, Information Processing and Management: an International Journal, v.24 n.5, p.577-597, 1988 66 Winer, B.
J. 1971.
Statistical Principles in Experimental Design.
Second edition.
McGraw-Hill, New York, NY.
67 David Yarowsky, Word-sense disambiguation using statistical models of Roget's categories trained on large corpora, Proceedings of the 14th conference on Computational linguistics, August 23-28, 1992, Nantes, France 68 David Yarowsky, Unsupervised word sense disambiguation rivaling supervised methods, Proceedings of the 33rd annual meeting on Association for Computational Linguistics, p.189-196, June 26-30, 1995, Cambridge, Massachusetts
NEW YORK UNIVERSITY DESCRIPTION OF THE PROTEUS SYSTEM AS USED FOR MUC4 Ralph Grishman, Catherine Macleod, and John Sterling, The PROTEUS Project Computer Science Departmen t New York University 715 Broadway, 7th Floor New York, NY 1000 3 { grishman,macleod,sterling)@cs.nyu.edu HISTORY The PROTEUS system which we have used for MUC-4 is largely unchanged from that used for MUC-3 . It has three main components : a syntactic analyzer, a semantic analyzer, and a template generator . The PROTEUS Syntactic Analyzer was developed starting in the fall of 1984 as a common base for all th e applications of the PROTEUS Project.
Many aspects of its design reflect its heritage in the Linguistic Strin g Parser, previously developed and still in use at New York University . The current system, including the Restriction Language compiler, the lexical analyzer, and the parser proper, comprise approximately 4500 lines of Commo n Lisp.
The Semantic Analyzer was initially developed in 1987 for the MUCK-I (RAINFORMs) application, extended for the MUCK-II (OPREPs) application, and has been incrementally revised since . It currently consist s of about 3000 lines of Common Lisp (excluding the domain-specific information) . The Template Generator was written from scratch for the MUC-3 task and then revised for the MUC-4 templates; it is about 1200 lines of Common Lisp..
STAGES OF PROCESSING The text goes through the five major stages of processing : lexical analysis, syntactic analysis, semantic analysis, reference resolution, and template generation (see Figure 1) . In addition, some restructuring of the logical form is performed both after semantic analysis and after reference resolution (only the restructuring after referenc e resolution is shown in Figure 1) . Processing is basically sequential: each sentence goes through lexical, syntactic, and semantic analysis and reference resolution ; the logical form for the entire message is then fed to template generation . However, semantic (selectional) checking is performed during syntactic analysis, employing essentiall y the same code later used for semantic analysis . Each of these stages is described in a section which follows . LEXICAL ANALYSI S Dictionary Format Our dictionaries contain only syntactic information : the parts of speech for each word, information about the complement structure of verbs, distributional information (e .g ., for adjectives and adverbs), etc . We follow closely the set of syntactic features established for the NYU Linguistic String Parser . This information is entered in LISP form using noun, verb, adjective, and adverb macros for the open-class words, and a word macro for other parts of speech : (ADVERB "ABRUPTLY" :ATTRIBUTES (DSA) ) (ADJECTIVE "ABRUPT" ) (NOUN :ROOT "ABSCESS" :ATTRIBUTES (NCOUNT) ) Knowledge Sources Dictionary Text Lexical Analysi s Grammar Syntactic Analysi s Semantic Models Semantic Analysis Concept Hier.
Reference Resolutio n Mapping Rules LF Transformatio n Template Generatio n Template s Figure 1 . Structure of the Proteus System as used for MUC-4 (VERB :ROOT "ABSCOND" :OBJLIST (NULLOBJ PN (PVAL (FROM WITH))) ) The noun and verb macros automatically generate the regular inflectional forms . Dictionary Files The primary source of our dictionary information about open-class words (nouns, verbs, adjectives, an d adverbs) is the machine-readable version of the Oxford Advanced Learner's Dictionary ("OALD") . We have writ ten programs which take the SGML (Standard Generalized Markup Language) version of the dictionary, extrac t information on inflections, parts of speech, and verb subcategorization (including information on adverbial particles and prepositions gleaned from the examples), and generate the LISP-ified form shown above . This is supplemented by a manually-coded dictionary (about 1500 lines, 900 entries) for closed-class words, words no t fiadequately defined in the OALD, and a few very common words . For MUC-4 we used several additional dictionaries . There was a dictionary (about 900 lines) for domain specific English words not defined in the OALD, or too richly defined there . In addition, we extracted from the tex t and templates lists of organizations, locations, and proper names, and prepared small dictionaries for each (abou t 2500 lines total) . Looku p The text reader splits the input text into tokens and then attempts to assign to each token (or sequence o f tokens, in the case of an idiom) a definition (part of speech and syntactic attributes) . The matching process proceeds in four steps: dictionary lookup, lexical pattern matching, spelling correction, and prefix stripping . Dictionary lookup immediately retrieves definitions assigned by any of the dictionaries (including inflected forms), while lexical pattern matching is used to identify a variety of specialized patterns, such as numbers, dates, times, and possessive forms . If neither dictionary lookup nor lexical pattern matching is successful, spelling correction and prefix strippin g are attempted.
For words of any length, we identify an input token as a misspelled form of a dictionary entry if on e of the two has a single instance of a letter while the other has a doubled instance of the letter (e .g ., "mispelled" and "misspelled") . For words of 8 or more letters, we use a more general spelling corrector which allows for any singl e insertion, deletion, or substitution.
[ The prefix stripper attempts to identify the token as a combination of a prefix and a word defined in the dictionary.
We currently use a list of 17 prefixes, including standard English ones like "un" and MUC-3/MUC-4 specials like "narco-" . If all of these procedures fail, the word is tagged as a proper noun (name), since we found that most of ou r remaining undefined words were names . For MUC-4, we have incorporated the stochastic part-of-speech tagger from BBN in order to assign probabilities to each part-of-speech assigned by the lexical analyzer.
The log probabilities are used as scores, and combined with other scores to determine the overall score of each parsing hypothesis . Filtering In order to avoid full processing of sentences which would make no contribution to the templates, we per form a keyword-based filtering at the sentence level : if a sentence contains no key terms, it is skipped . This filtering is done after lexical analysis because the lexical analysis has identified the root form of all inflected words ; these root forms provide links into the semantic hierarchy . The filtering can therefore be specified in terms of a small number of word classes, one of which must be present for the sentence to be worth processing . SYNTACTIC ANALYSI S Syntactic analysis involves two stages of processing : parsing and syntactic regularization . At the core of th e system is an active chart parser.
The grammar is an augmented context-free grammar, consisting of BNF rules plu s procedural restrictions which check grammatical constraints not easily captured in the BNF rules . Most restrictions are stated in PROTEUS Restriction Language (a variant of the language developed for the Linguistic String Parser ) and translated into LISP ; a few are coded directly in LISP [1] . For example, the count noun restriction (that singular countable nouns have a determiner) is stated as WCOUNT = IN LNR AFTER NVAR : IF BOTH CORE Xcore IS NCOUNT AND Xcore IS SINGULA R THEN IN LN, TPOS IS NOT EMPTY . Associated with each BNF rule is a regularization rule, which computes the regularized form of each node i n the parse tree from the regularized forms of its immediate constituents . These regularization rules are based on lambda-reduction, as in GPSG . The primary function of syntactic regularization is to reduce all clauses to a standard form consisting of aspect and tense markers, the operator (verb or adjective), and syntactically marked cases . ' The minimum word length requirement is needed to avoid false hits where proper names are incorrectly identified as misspellings of words defined in the dictionary.
For example, the definition of assertion, the basic S structure in our grammar, i s <assertion> . ._ <sa> <subject> <sa> <verb> <sa> <object> <sa > :(s !(<object> <subject> <verb> <sa*>)) . Here the portion after the single colon defines the regularized structure.
Coordinate conjunction is introduced by a metarule (as in GPSG), which is applied to the context-free components of the grammar prior to parsing . The regularization procedure expands any conjunction into a conjuntio n of clauses or of noun phrases . The output of the parser for the first sentence of TST2-0048, "SALVADORAN PRESIDENT-ELEC T ALFREDO CRISTIANI CONDEMNED THE TERRORIST KILLING OF ATTORNEY GENERAL ROBERTO GARCIA ALVARADO AND ACCUSED THE FARABUNDO MARTI NATIONAL LIBERATION FRON T (FMLN) OF THE CRIME.", i s (SENTENC E (CENTERS (CENTE R (ASSERTIO N (ASSERTIO N (SUBJEC T (NSTG (LNR (LN (NPOS (NPOSVAR (LCDN (ADJ "SALVADORAN")) (N "PRESIDENT" "-" "ELECT"))) ) (NVAR (NAMESTG (LNAMER (N "ALFREDO") (MORENAME (N "CRISTIANI"))))))) ) (VERB (LTVR (TV "CONDEMNED")) ) (OBJECT (NSTGO (NSTG (LNR (LN (TPOS (LTR (T "THE"))) (NPOS (NPOSVAR (N "TERRORIST"))) ) (NVAR (N "KILLING") ) (RN (RN-VA L (PN (P "OF" ) (NSTGO (NST G (LNR (LN (NPOS (NPOSVAR (N "ATTORNEY" "GENERAL"))) ) (NVA R (NAMEST G (LNAMER (N "ROBERTO" ) (MORENAME (N "GARCIA") (MORENAME (N "ALVARADO")))))))))))))))) ) (CONJ-WORD ("AND" "AND") ) (ASSERTION (SUBJEC T (NSTG (LN R (LN (NPOS (NPOSVAR (LCDN (ADJ "SALVADORAN")) (N "PRESIDENT" "-" "ELECT"))) ) (NVAR (NAMESTG (LNAMER (N "ALFRED O " ) (MORENAME (N "CRISTIANI"))))))) ) (VERB (LTVR (TV "ACCUSED")) ) (OBJEC T (NPN (NSTG O (NST G (LNR (LN (TPOS (LTR (T "THE"))) ) (NVAR (NAMESTG (LNAMER (N "FARABUNDO" "MARTI" "NATIONAL" "LIBERATION" "FRONT") ) fi(NAME-APPOS ("(" "(" ) (NSTG (LNR (NVAR (NAMESTG (LNAMER (N "FMLN")))))) (' ')".
")")))))) ) (PN (P "OF" ) (NSTGO (NSTG (LNR (LN (TPOS (LTR (T "THE")))) (NVAR (N "CRIME"))))))))))) ) (ENDMARK (" ".
" ")).
) and the corresponding regularized structure is (AND (S CONDEMN (VTENSE PAST ) (SUBJEC T (NP A-NAME SINGULAR (NAMES (ALFREDO CRISTIANI)) (SN NP1499 ) (N-POS (NP PRESIDENT-ELECT SINGULAR (SN NP1489) (A-POS SALVADORAN)))) ) (OBJEC T (NP KILLING SINGULAR (SN NP1532) (T-POS THE ) (N-POS (NP TERRORIST SINGULAR (SN NP1504)) ) (OF (NP A-NAME SINGULAR (NAMES (ROBERTO GARCIA ALVARADO)) (SN NP1531 ) (N-POS (NP 'ATTORNEY GENERAL' SINGULAR (SN NP1506))))))) ) (S ACCUSE (VTENSE PAST ) (SUBJECT (NP A-NAME SINGULAR (NAMES (ALFREDO CRISTIANI)) (SN NP1499 ) (N-POS (NP PRESIDENT-ELECT SINGULAR (SN NP1489) (A-POS SALVADORAN)))) ) (OBJEC T (NP FMLN SINGULAR (RN-APPOS (NP FMLN SINGULAR (SN NP1539))) (SN NP1544 ) (T-POS THE)) ) (OF (NP CRIME SINGULAR (SN NP1543) (T-POS THE)))) ) The system uses a chart parser operating top-down, left-to-right . As edges are completed (i.e., as nodes of the parse tree are built), restrictions associated with those productions are invoked to assign and test features of th e parse tree nodes . If a restriction fails, that edge is not added to the chart . When certain levels of the tree are complete (those producing noun phrase and clause structures), the regularization rules are invoked to compute a regularized structure for the partial parse, and selection is invoked to verify the semantic well-formedness of the structure (as noted earlier, selection uses the same "semantic analysis" code subsequently employed to translate the tre e into logical form).
One unusual feature of the parser is its weighting capability . Restrictions may assign scores to nodes ; th e parser will perform a best-first search for the parse tree with the highest score . This scoring is used to implement various preference mechanisms:  closest attachment of modifiers (we penalize each modifier by the number of words separating it from it s head)  preferred narrow conjoining for clauses (we penalize a conjoined clause structure by the number of words i t subsumes)  preference semantics (selection does not reject a structure, but imposes a heavy penalty if the structure doe s not match any lexico-semantic model, and a lesser penalty if the structure matches a model but with som e operands or modifiers left over) [2,3 ]  relaxation of certain syntactic constraints, such as the count noun constraint, adverb position constraints, and comma constraint s  disfavoring (penalizing) headless noun phrases and headless relatives (this is important for parsing efficiency) The grammar is based on Harris's Linguistic String Theory and adapted from the larger Linguistic Strin g Project (LSP) grammar developed by Naomi Sager at NYU [4] . The grammar is gradually being enlarged to cove r more of the LSP grammar.
The current grammar is 1600 lines of BNF and Restriction Language plus 300 lines o f Lisp; it includes 186 non-terminals, 464 productions, and 132 restrictions . Over the course of the MUCs we have added several mechanisms for recovering from sentences the gram mar cannot fully parse : allowing the grammar to skip a single word, or a series of words enclosed in parentheses or dashes, with a large score penalty if no parse is obtained for the entire sentence, taking the analysis which, starting at the first word, subsumes the most word s optionally, taking the remainder of the sentence and "covering" it with noun phrases and clauses, preferrin g the longest noun phrases or clauses which can be identifie d SEMANTIC ANALYSIS AND REFERENCE RESOLUTIO N The output of syntactic analysis goes through semantic analysis and reference resolution and is then added t o the accumulating logical form for the message . Following both semantic analysis and reference resolution certai n transformations are performed to simplify the logical form . All of this processing makes use of a concept hierarch y which captures the class/subclass/instance relations in the domain.
Semantic analysis uses a set of lexico-semantic models to map the regularized syntactic analysis into a semantic representation.
Each model specifies a class of verbs, adjectives, or nouns and a set of operands ; for eac h operand it indicates the possible syntactic case markers, the semantic class of the operand, whether or not th e operand is required, and the semantic case to be assigned to the operand in the output representation . For example, the model for "<explosive-object> damages <target>" i s (add-clause-model :id 'clause-damage3 :parent 'clause-an y :constraint 'damag e :operands (list (make-specifie r :marker 'subjec t :class 'explosive-objec t :case :instrument ) (make-specifie r :marker 'objec t :class 'target-entit y :case :patien t :essential-required 'required)) ) The models are arranged in a shallow hierarchy with inheritance, so that arguments and modifiers which are shared by a class of verbs need only be stated once.
The model above inherits only from the most general clause model, clause--any, which includes general clausal modifiers such as negation, time, tense, modality, etc . The evaluated MUC-4 system had 124 clause models, 21 nominalization models, and 39 other noun phrase models, a total of about 2500 lines . The class explosive--object in the clause model refers to the concept in the concept hierarchy, whose entries have the form : (defconcept explosive-object :typeof instrument-type ) (defconcept explosive :typeof explosive-objec t :muctype explosive ) (defconcept grenade :typeof explosive ) (defconcept explosive-charge :typeof explosiv e :alias (dynamite-charge) ) (defconcept bomb :typeof explosive-objec t :muctype bomb ) (defconcept (VEHICLE BOMB( :typeof explosive-objec t :muctype (VEHICLE BOMB( ) (defconcept car-bomb :typeof VEHICLE BOMB( ) (defconcept bus-bomb :typeof IVEHICLE BOMB( ) (defconcept dynamite :typeof explosive-objec t :alias tnt :muctype DYNAMITE ) There are currently a total of 2474 concepts in the hierarchy, of which 1734 are place names . The output of semantic analysis is a nested set of entity and event structures, with arguments labeled by key words primarily designating semantic roles . For the first sentence of TST2-0048, the output i s Reference Resolution Reference resolution is applied to the output of semantic analysis in order to replace anaphoric noun phrase s (representing either events or entities) by appropriate antecedents . Each potential anaphor is compared to prior entities or events, looking for a suitable antecedent such that the class of the anaphor (in the concept hierarchy) i s equal to or more general than that of the antecedent, the anaphor and antecedent match in number, the restrictiv e modifiers in the anaphor have corresponding arguments in the antecedent, and the non-restrictive modifiers (e .g ., apposition) of the anaphor are not inconsistent with those of the antecedent . Special tests are provided for names (people may be referred to a subset of the ir names) and for referring to groups by typical members ("terrorist force" Logical Form Transformation s The transformations which are applied after semantic analysis and after reference resolution simplify an d regularize the logical form in various ways . For example, if a verb governs an argument of a nominalization, th e argument is inserted into the event created from the nominalization : "x conducts the attack", "x claims responsibility for the attack", "x was accused of the attack" etc . are all mapped to "x attacks" (with appropriate settings of th e confidence slot) . For example, the rule to take "X was accused of Y" and make X the agent of Y i s (((event :predicate accusation-even t :agent ?agent1 :event (event :identifier ?id-1 . ?R2 ) . ?R1 ) (event :identifier ?id-1 . ?R4) ) -> ((modify 2 '( :agent ?agent-1 :confidence 'SUSPECTED OR ACCUSEDI) ) (delete 1)) ) Transformations are also used to expand conjoined structures . For example, there is a rule to expand "the towns o f x and y " into "the town of x and the town of y", and there is a rule to expand "event at location-1 and location-2 " into "event at location-1 and event at location-2" . There are currently 32 such rules.
These transformations are written as productions and applied using a simple data-driven production system interpreter which is part of the PROTEUS system . TEMPLATE GENERATO R Once all the sentences in an article have been processed through syntactic and semantic analysis, the resulting logical forms are sent to the template generator . The template generator operates in four stages . First, a frame structure resembling a simplified template (with incident-type, perpetrator, physical-target, human-target, date, location, instrument, physical-effect, and human-effect slots) is generated for each event . Date and locatio n expressions are reduced to a normalized form at this point . In particular, date expressions such as "tonight", "last month", " last April", "a year ago", etc.
are replaced by explicit dates or date ranges, based on the dateline of th e article.
Second, a series of heuristics attempt to merge these frames, mergin g  frames referring to a common target  frames arising from the same sentenc e  an effect frame following an attack frame (e.g ., "The FMLN attacked the town . Seven civilians died ".
) This merging is blocked if the dates or locations are different, the incident types are incompatible, or the perpetrators are incompatible . Third, a series of filters removes frames involving only military targets and those involvin g events more than two months old . Finally, MUC templates are generated from these frames . fiSPONSORSHIP The development of the entire PROTEUS system has been sponsored primarily by the Defense Advanced Research Projects Agency as part of the Strategic Computing Program, under Contract N00014-85-K-0163 an d Grant N00014-90-J-1851 from the Office of Naval Research . Additional support has been received from th e National Science Foundation under grant DCR-85-01843 for work on enhancing system robustness . REFERENCES [1] Grishman, R . PROTEUS Parser Reference Manual . PROTEUS Project Memorandum #4-C, Computer Science Department, New York University, May 1990.
[2] Grishman, R., and Sterling, J . Preference Semantics for Message Understanding.
Proc . DARPA Speech and Natural Language Workshop, Morgan Kaufman, 1990 (proceedings of the conference at Harwich Port, MA, Oct . 15-18, 1989) . [3] Grishman, R., and Sterling, J . Information Extraction and Semantic Constraints . Proc . 13th Int' I Conf Computational Linguistics (COLING 90), Helsinki, August 20-25, 1990 . [4] Sager, N . Natural Language Information Processing, Addison-Wesley, 1981 .
Shallow Semantic Parsing of Chinese Honglin Sun1 Center for Spoken Language Research University of Colorado at Boulder Daniel Jurafsky2 Center for Spoken Language Research University of Colorado at Boulder Abstract In this paper we address the question of assigning semantic roles to sentences in Chinese.
We show that good semantic parsing results for Chinese can be achieved with a small 1100-sentence training set.
In order to extract features from Chinese, we describe porting the Collins parser to Chinese, resulting in the best performance currently reported on Chinese syntactic parsing; we include our headrules in the appendix.
Finally, we compare English and Chinese semantic-parsing performance.
While slight differences in argument labeling make a perfect comparison impossible, our results nonetheless suggest significantly better performance for Chinese.
We show that much of this difference is due to grammatical differences between English and Chinese, such as the prevalence of passive in English, and the strict word order constraints on adjuncts in Chinese.
based on the SVM-based algorithm proposed for English by Pradhan et al (2003).
We first describe our creation of a small 1100-sentence Chinese corpus labeled according to principles from the English and (in-progress) Chinese PropBanks.
We then introduce the features used by our SVM classifier, and show their performance on semantic parsing for both seen and unseen verbs, given hand-corrected (Chinese TreeBank) syntactic parses.
We then describe our port of the Collins (1999) parser to Chinese.
Finally, we apply our SVM semantic parser to a matching English corpus, and discuss the differences between English and Chinese that lead to significantly better performance on Chinese.
2 Semantic
Annotation and the Corpus Work on semantic parsing in English has generally related on the PropBank, a portion of the Penn TreeBank in which the arguments of each verb are annotated with semantic roles.
Although a project to produce a Chinese PropBank is underway (Xue and Palmer 2003), this data is not expected to be available for another year.
For these experiments, we therefore hand-labeled a small corpus following the Penn Chinese Propbank labeling guidelines (Xue, 2002).
In this section, we first describe the semantic roles we used in the annotation and then introduce the data for our experiments.
2.1 Semantic
roles Semantic roles in the English (Kingsbury et al 2002) and Chinese (Xue 2002) PropBanks are grouped into two major types: (1) arguments, which represent central participants in an event.
A verb may require one, two or more arguments and they are represented with a contiguous sequence of numbers prefixed by arg, as arg0, arg1.
(2) adjuncts, which are optional for an event but supply more information about an event, such as time, location, 1 Introduction Thematic roles (AGENT, THEME, LOCATION, etc) provide a natural level of shallow semantic representation for a sentence.
A number of algorithms have been proposed for automatically assigning such shallow semantic structure to English sentences.
But little is understood about how these algorithms may perform in other languages, and in general the role of language-specific idiosyncracies in the extraction of semantic content and how to train these algorithms when large hand-labeled training sets are not available.
In this paper we address the question of assigning semantic roles to sentences in Chinese.
Our work is Currently at Department of Computer Science, Queens College, City University of New York.
Email: sunh@qc.edu.
2 Currently
at Department of Linguistics, Stanford University.
Email: jurafsky@stanford.edu.
fireason, condition, etc.
An adjunct role is represented with argM plus a tag.
For example, argM-TMP stands for temporal, argM-LOC for location.
In our corpus three argument roles and 15 adjunct roles appear.
The whole set of roles is given at Table 1.
Role arg0 arg1 arg2 argM-ADV argM-BFY argM-CMP argM-CND argM-CPN argM-DGR argM-FRQ argM-LOC argM-MNR argM-PRP argM-RNG argM-RST argM-SRC argM-TMP argM-TPC Table1 The list of semantic roles Freq Freq Note 556 872 23 train of senses, argument numbers and frequencies are given in Table 2.
List of verbs for experiments # of Arg Freq senses number /set up 1 2 106 /emerge 1 1 80 /publish 1 2 113 /give 2 3/2 41 /build into 2 2/3 113 /enter 1 2 123 /take place 1 2 230 /pass 3 2 75 /hope 1 2 90 /increase 1 2 167 Table 2 Verb Test adverbial beneficiary(e.g.
give support [to the plan]) object to be compared condition companion (e.g.
talk [with you]) degree frequency location manner purpose or reason range(e.g.
help you [in this aspect]) result(e.g.
increase [to $100]) source(e.g.
increase [from $50] to $100) temporal topic 3 Semantic Parsing 3.1 Architecture and Classifier Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem.
For each (non-aux/non-copula) verb in each sentence, our classifier examines each node in the syntactic parse tree for the sentence and assigns it a semantic role label.
Most constituents are not arguments of the verb, and so the most common label is NULL.
Our architecture is based on a Support Vector Machine classifier, following Pradhan et al.(2003). Since SVMs are binary classifiers, we represent this 1-of-19 classification problem (18 roles plus NULL) by training 19 binary one-versus-all classifiers.
Following Pradhan et al.(2003), we used tinySVM along with YamCha (Kudo and Matsumoto 2000, 2001) as the SVM training and test software.
The system uses a polynominal kernel with degree 2; the cost per unit violation of the margin, C=1; tolerance of the termination criterion e=0.001.
3.2 Features
The literature on semantic parsing in English relies on a number of features extracted from the input sentence and its parse.
These include the constituent's syntactic phrase type, head word, and governing category, the syntactic path in the parse tree connecting it to the verb, whether the constitutent is before or after the verb, the subcategorization bias of the verb, and the voice (active/passive) of the verb.
We investigated each of these features in Chinese; some acted quite similarly to English, while others showed interesting differences.
Features that acted similarly to English include the target verb, the phrase type, the syntactic category of the constituent.
(NP, PP, etc), and the subcategorization of the target verb.
The sub-categorization feature represents the phrase structure rule for the verb phrase 2.2 The training and test sets We created our training and test corpora by choosing 10 Chinese verbs, and then selecting all sentences containing these 10 verbs from the 250K-word Penn Chinese Treebank 2.0.
We chose the 10 verbs by considering frequency, syntactic diversity, and word sense.
We chose words that were frequent enough to provide sufficient training data.
The frequencies of the 10 verbs range from 41 to 230, with an average of 114.
We chose verbs that were representative of the variety of verbal syntactic behavior in Chinese, including verbs with one, two, and three arguments, and verbs with various patterns of argument linking.
Finally, we chose verbs that varied in their number of word senses.
In total, we selected 1138 sentences.
The first author then labeled each verbal argument/adjunct in each sentence with a role label.
We created our training and test sets by splitting the data for each verb into two parts: 90% for training and 10% for test.
Thus there are 1025 sentences in the training set and 113 sentences in the test set, and each test set verb has been seen in the training set.
The list of verbs chosen and their number ficontaining the target verb (e.g., VP -> VB NP, etc).
Five features (path, position, governing category, headword, and voice) showed interesting patterns that are discussed below.
3.2.1 Path
in the syntactic parse tree.
The path feature represents the path from a constituent to the target verb in the syntactic parse tree, using "^" for ascending a parse tree, and ""! for descending.
This feature manifests the syntactic relationship between the constituent and the target verb.
For example the path "NP^IP!VP!VP!VV" indicates that the constituent is an "NP" which is the subject of the predicate verb.
In general, we found the path feature to be sparse.
In our test set, 60% of path types and 39% of path tokens are unseen in the training.
The distributions of paths are very uneven.
In the whole corpus, paths for roles have an average frequency of 14.5 while paths for non-roles have an average of 2.7.
Within the role paths, a small number of paths account for majority of the total occurrences; among the 188 role path types, the top 20 paths account for 86% of the tokens.
Thus, although the path feature is sparse, its sparsity may not be a major problem in role recognition.
Of the 291 role tokens in our test set, only 9 have unseen paths, i.e., most of the unseen paths are due to non-roles.
Table 3 The positional distribution of roles Role arg0 arg1 arg2 argM-ADV argM-BFY argM-CMP argM-CND argM-CPN argM-DGR argM-FRQ argM-LOC argM-MNR argM-PRP argM-RNG argM-RST argM-SRC argM-TMP argM-TPC Total Before verb 547 319 223 28 38 15 10 233 11 11 9 12 408 14 1878 After verb 72 644 28 Total 619 963 28 223 28 38 15 10 57 3 238 11 11 9 16 12 421 14 2716 example, 88% of arg0s are before the verb, 67% of arg1s are after the verb and all the arg2s are after the verb.
Adjuncts have even a stronger bias.
Ten of the adjunct types can only occur before the verb, while three are always after the verb.
The two most common adjunct roles, argM-LOC and argM-TMP are almost always before the verb, a sharp difference from English.
The details are shown seen in Table 3.
3.2.3 Governing
Category.
The governing category feature is only applicable for NPs.
In the original formulation for English in Gildea and Jurafsky (2002), it answers the question: Is the NP governed by IP or VP?
An NP governed by an IP is likely to be a subject, while an NP governed by a VP is more likely to be an object.
For Chinese, we added a third option in which the governing category of an NP is neither IP nor VP, but an NP.
This is caused by the "DE" construction, in which a clause is used as a modifier of an NP.
For instance, in the example indicated in Figure 1, for the last NP, " "("international Olympic conference") the parent node is NP, from where it goes down to the target verb " "("taking place").
NP CP VP      in Paris take place  DEC DE             NP intl Olympic conf.
"the international Olympic Conference held in Paris" Figure 1 Example of DE construction Since the governing category information is encoded in the path feature, it may be redundant; indeed this redundancy might explain why the governing category feature was used in Gildea & Jurafsky(2002) but not in Gildea and Palmer(2002).
Since the "DE" construction caused us to modify the feature for Chinese, we conducted several experiments to test whether the governing category feature is useful or whether it is redundant with the path and position features.
Using the paradigm to be described in section 3.4, we found a small improvement using governing category, and so we include it in our model.
3.2.4 Head
word and its part of speech.
The head word is a useful but sparse feature.
In our corpus, of the 2716 roles, 1016 head words (type) are used, in which 646 are used only once.
The top 20 words are given in Table 4.
3.2.2 Position
before or after the verb.
The position feature indicates that a constituent is before or after the target verb.
In our corpus, 69% of the roles are before the verb while 31% are after the verb.
As in English, the position is a useful cue for role identity.
For 3.3 Experimental Results for Seen Verbs We now test the performance of our classifier, trained on the 1025-sentence training set and tested on the 113sentence test set introduced in Section 2.2.
Recall that in this `stratified' test set, each verb has been seen in the training data.
The last row in Table 5 shows the current best performance of our system on this test set.
The preceding rows show various subsets of the feature set, beginning with the path feature.
Table 5 Semantic parsing results on seen verbs feature set P R F (%) (%) (%) path 71.8 59.4 65.0 path + pt 72.9 62.9 67.5 path + position 72.5 60.8 66.2 path + head POS 77.6 63.3 69.7 path + sub-cat 80.8 63.6 71.2 path + head word 85.0 66.0 74.3 path + target verb 85.8 68.4 76.1 path + pt + gov + position + subcat + target + head word + head POS 91.7 76.0 83.1 As Table 5 shows, the most important feature is path, followed by target verb and head word.
In general, the lexicalized features are more important than the other features.
The combined feature set outperforms any other feature sets with less features and it has an Fscore of 83.1.
The performance is better for the arguments (i.e., only ARG0-2), 86.7 for arg0 and 89.4 for arg1.
3.4 Experimental
Results for Unseen Verbs To test the performance of the semantic parser on unseen verbs, we used cross-validation, selecting one verb as test and the other 9 as training, and iterating with each verb as test.
All the results are given in Table 6.
The results for some verbs are almost equal to the performance on seen verbs.
For example for "    " and "    ", the F-scores are over 80.
However, for some verbs, the results are much worse.
The worst case is the verb " ", which has an F-score of 11.
This is due to the special syntactic characteristics of this verb.
This verb can only have one argument and this argument most often follows the verb, in object position.
In the surface structure, there is often an NP before the verb working as its subject, but semantically this subject cannot be analyzed as arg0.
For example: (1) /China  /not  /will  /emerge  /food  /crisis.
(A food crisis won't emerge in China).
(2) /Finland /economy  /emerge  /AUX  /post-war  /most     /serious  /AUX     /depression.
(The most severe post-war depression emerged in the Finland economy.) In the top 20 words, 4 are prepositions (" /in    /at /than /for") and 3 are temporal nouns("  /today      /present      /recently") and 2 are adverbs("  /already,  /will").
These closed class words are highly correlated with specific semantic roles.
For example," /for" occurs 195 times as the head of a constituent, of which 172 are non-roles, 19 are argM-BFYs, 3 are arg1s and 1 is an argM-TPC." /in" occurs 644 times as a head, of which 430 are nonroles, 174 are argM-LOCs, 24 are argM-TMPs, 9 are argM-RNGs, and 7 are argM-CND.
"  /already" occurs 135 times as a head, of which 97 are non-roles and 38 are argM-ADVs.
" /today" occurs 69 times as a head, of which 41 are argM-TMPs and 28 are nonroles.
Within the open class words, some are closely correlated to the target verb.
For example, "     /meeting; conference" occurs 43 times as a head for roles, of which 24 are for the target " /take place" and 19 for "    /pass".
"    /ceremony" occurs 28 times and all are arguments of " "(take place)."  /statement" occurs 19 times, 18 for "    /release; publish" and one for " /hope".
These statistics emphasize the key role of the lexicalized head word feature in capturing the collocation between verbs and their arguments.
Due to the sparsity of the head word feature, we also use the part-of-speech of the head word, following Surdeanu et al (2003).
For example, "7  26  /July 26" may not be seen in the training, but its POS, NT(temporal noun), is a good indicator that it is a temporal.
3.2.5 Voice.
The passive construction in English gives information about surface location of arguments.
In Chinese the marked passive voice is indicated by the use of the preposition " /by" (POS tag LB in Penn Chinese Treebank).
This passive, however, is seldom used in Chinese text.
In our entire 1138-sentence corpus, only 13 occurrences of "LB" occur, and only one (in the training set) is related to the target verb.
Thus we do not use the voice feature in our system.
The subjects, " /China" in (1) and " /Finland /economy", are locatives, i.e. argM-LOC, and the objects, " /food  /crisis" in (1) and " /postwar  /most  /serious  /AUX  /depression" in (2), are analyzed as arg0.
But the parser classified the subjects as arg0 and the objects as arg1.
These are correct for most common verbs but wrong for this particular verb.
It is difficult to know how common this problem would be in a larger, test set.
The fact that we considered diversity of syntactic behavior when selecting verbs certainly helps make this test set reflect the difficult cases.
If most verbs prove not to be as idiosyncratic as " /emerge", the real performance of the parser on unseen verbs may be better than the average given here.
Table 6 Experimental Results for Unseen Verbs target P(%) R(%) F(%)  /publish 90.7 72.9 80.8  /increase 49.6 34.3 40.5   /take place 90.1 63.3 74.4  /build into 65.2 55.5 60.0  /give 65.7 37.9 48.1  /pass 85.9 77.0 81.2  /emerge 12.6 10.2 11.3  /enter 81.9 58.8 68.4  /set up 79.0 61.1 68.9  /hope 77.7 35.9 49.1 Average 69.8 50.7 58.3 Another important difficulty in processing unseen verbs is the fact that roles in PropBank are defined in a verb-dependent way.
This may be easiest to see with an English example.
The roles arg2, arg3, arg4 have different meaning for different verbs; underlined in the following are some examples of arg2: (a) The state gave CenTrust 30 days to sell the Rubens.
(b) Revenue increased 11 to 2.73 billion from 2.46 billion.
(c) One of Ronald Reagan 's attributes as President was that he rarely gave his blessing to the claptrap that passes for consensus in various international institutions.
In (a), arg2 represents the goal of "give", in (b), it represents the amount of increase, and in (c) it represents yet another role.
These complete different semantic relations are given the same semantic label.
For unseen verbs, this makes it difficult for the semantic parser to know what would count as an arg2.
parser, the Collins (1999) parser, ported to Chinese.
We first describe how we ported the Collins parser to Chinese and then present the results of the semantic parser with features drawn from the automatic parses.
4.1 The
Collins parser for Chinese The Collins parser is a state-of-the-art statistical parser that has high performance on English (Collins, 1999) and Czech(Collins et al.1999). There have been attempts in applying other algorithms in Chinese parsing (Bikel and Chiang, 2000; Chiang and Bikel 2002; Levy and Manning 2003), but there has been no report on applying the Collins parser on Chinese.
The Collins parser is a lexicalized statistical parser based on a head-driven extended PCFG model; thus the choice of head node is crucial to the success of the parser.
We analyzed the Penn Chinese Treebank data and worked out head rules for the Chinese Treebank grammar (we were unable to find any published head rules for Chinese in the literature).
There are two major differences in the head rules between English and Chinese.
First, NP heads in Chinese are rigidly rightmost, that is to say, no modifiers of an NP can follow the head.
In contrast, in English a modifier may follow the head.
Second, just as with NPs in Chinese, the head of ADJP is rigidly rightmost.
In English, by contrast, the head of an ADJP is mainly the leftmost constituent.
Our head rules for the Chinese Treebank grammar are given in the Appendix.
In addition to the head rules, we modified the POS tags for all punctuation.
This is because all cases of punctuation in the Penn Chinese Treebank are assigned the same POS tag "PU".
The Collins parser, on the other hand, expects the punctuation tags in the English TreeBank format, where the tag for a punctuation mark is the punctuation mark itself.
We therefore replaced the POS tags for all punctuation marks in the Chinese data to conform to the conventions in English.
Finally, we made one further augmentation also related to punctuation.
Chinese has one punctuation mark that does not exist in English.
This commonly used mark, `semi-stop', is used in Chinese to link coordinates within a sentence (for example between elements of a list).
This function is represented in English by a comma.
But the comma in English is ambiguous; in addition to its use in coordination and lists, it can also represent the end of a clause.
In Chinese, by contrast the semi-stop has only the conjunction/list function.
Chinese thus uses the regular comma only for representing clause boundaries.
We investigated two ways to model the use of the Chinese semi-stop: (1) just converting the semi-stop to the comma, thus conflating the two functions as in English; and (2) by giving the semi-stop the POS tag "CC", a conjunction.
We compared parsing results with these two methods; the latter (conjunction) method gained 0.5% net 4 Using Automatic Parses The results in the last section are based on the use of perfect (hand-corrected) parses drawn from the Penn Chinese Treebank.
In practical use, of course, automatic parses will not be as accurate.
In this section we describe experiments on semantic parsing when given automatic parses produced by an automatic fiimprovement in F-score over the former one.
We therefore include it in our Collins parser port.
We trained the Collins parser on the Penn Chinese Treebank(CTB) Release 2 with 250K words, first removing from the training set any sentences that occur in the test set for the semantic parsing experiments.
We then tested on the test set used in the semantic parsing which includes 113 sentences(TEST1).
The results of the syntactic parsing on the test set are shown in Table 7.
Table 7 Results for syntactic parsing, trained on CTB Release 2, tested on test set in semantic parsing LP(%) LR(%) F1(%) overall 81.6 82.1 81.0 len<=40 86.1 85.5 86.7 To compare the performance of the Collins parser on Chinese with those of other parsers, we conducted an experiment in which we used the same training and test data (Penn Chinese Treebank Release 1, with 100K words) as used in those reports.
In this experiment, we used articles 1-270 for training and 271-300 as test(TEST2).
Table 8 shows the results and the comparison with other parsers.
Table 8 only shows the performance on sentences 40 words.
Our performance on all the sentences TEST2 is P/R/F=82.2/83.3/82.7.
It may seem surprising that the overall F-score on TEST2 (82.7) is higher than the overall F-score on TEST1 (81.0) despite the fact that our TEST1 system had more than twice as much training as our TEST2 system.
The reason lies in the makeup of the two test sets; TEST1 consists of randomly selected long sentences; TEST2 consists of sequential text, including many short sentences.
The average sentence length in TEST1 is 35.2 words, vs.
22.1 in TEST2.
TEST1 has 32% long sentences (>40 words) while TEST2 has only 13%.
Comparison with other parsers: TEST2 40 words LP(%) LR(%) F1(%) Bikel & Chiang 2000 77.2 76.2 76.7 Chiang & Bikel 2002 81.1 78.8 79.9 Levy & Manning 2003 78.4 79.2 78.8 Collins parser 86.4 85.5 85.9 4.2 Semantic parsing using Collins parses In the test set of 113 sentences, there are 3 sentences in which target verbs are given the wrong POS tags, so they can not be used for semantic parsing.
For the remaining 100 sentences, we used the feature set containing eight features (path, pt, gov, position, subcat, target, head word and head POS), the same as Table 8 that used in the experiment on perfect parses.
The results are shown in Table 9.
Table 9 Result for semantic parsing using automatic syntactic parses P(%) R(%) F(%) 110 sentences 86.0 70.8 77.6 113 sentences 86.0 69.2 76.7 Compared to the F-score using hand-corrected syntactic parses from the TreeBank, using automatic parses decreases the F-score by 6.4. 5 Comparison with English English build emerge enter found give Freq 46 30 108 248 124 Chinese      English hold hope increase pass publish Freq 120 63 231 143 77 Chinese      Table 12 Role argM-ADV argM-LOC argM-MNR argM-TMP Before verb 22 25 22 119 After verb 43 82 75 164 The comparison between adjuncts in English and Chinese English Chinese Freq in PRF Before After Freq in test (%) verb verb test 5 0 00 223 0 37 11 80 36.4 50 233 5 31 14 0 00 11 0 1 37 66.7 27 38.5 408 13 44 F 70 88.5 0 78.4 After the verbs were chosen, we extracted every sentence containing these verbs from section 02 to section 21 of the Wall Street Journal data from the Penn English Propbank.
The number of sentences for each verb is given in Table 10.
5.2 Experimental
Results As in our Chinese experiments, we used our SVMbased classifier, using N one-versus-all classifiers.
Table 11 shows the performance on our English test set (with Chinese for comparison), beginning with the path feature, and incrementally adding features until in the last row we combine all 8 features together.
Experimental results of English Chinese English feature set R/F/P P/R/F path 71.8/59.4/65.0 78.2/48.3/59.7 path + pt 72.9/62.9/67.5 77.4/51.2/61.6 path + position 72.5/60.8/66.2 75.7/50.9/60.8 path + hd POS 77.6/63.3/69.7 79.1/49.7/61.0 path + sub-cat 80.8/63.6/71.2 79.9/45.3/57.8 path + hd word 85.0/66.0/74.3 84.0/47.7/60.8 path + target 85.8/68.4/76.1 85.7/49.1/62.5 COMBINED 91.7/76.0/83.1 84.1/62.2/71.5 It is immediately clear from Table 11 that using similar verbs, the same amount of data, the same classifier, the same number of roles, and the same features, the results from English are much worse than those for Chinese.
While some part of the difference is probably due to idiosyncracies of particular sentences in the English and Chinese data, other aspects of the difference might be accounted for systematically, as we discuss in the next section.
5.3 Discussion: English/Chinese differences We first investigated whether the differences between English and Chinese could be attributed to particular semantic roles.
We found that this was indeed the case.
The great bulk of the error rate difference between English and Chinese was caused by the 4 adjunct classes argM-ADV, argM-LOC, argM-MNR, and argM-TMP, which together account for 19.6% of the role tokens in our English corpus.
The average F-score in English for the four roles is 36.7, while in Chinese Table 11 the F-score for the four roles is 78.6.
Why should these roles be so much more difficult to identify in English than Chinese?
We believe the answer lies in the analysis of the position feature in section 3.2.2.
This is repeated, with error rate information in Table 12.
We see there that adjuncts in English have no strong preference for occurring before or after the verb.
Chinese adjuncts, by contrast, are well-known to have an extremely strong preference to be preverbal, as Table 12 shows.
The relatively fixed word order of adjuncts makes it much easier in Chinese to map these roles from surface syntactic constituents than in English.
If the average F-score of the four adjuncts in English is raised to the level of that in Chinese, the overall Fscore on English would be raised from 71.5 to 79.7, accounting for 8.2 of the 11.6 difference in F-scores between the two languages.
We next investigated the one feature from our original English-specific feature set that we had dropped in our Chinese system: passive.
Recall that we dropped this feature because marked passives are extremely rare in Chinese.
When we added this feature back into our English system, the performance rose from P/R/F=84.1/62.2/71.5 to 86.4/65.1/74.3.
As might be expected, this effect of voice is mainly reflected in an improvement on arg0 and arg1, as Table 13 shows below: Table 13.
Improvement in English semantic parsing with the addition of the voice feature -voice +voice P RF P R F arg0 88.9 75.3 81.5 94.4 80 86.6 arg1 86.5 82.8 84.6 88.5 86.2 87.3 A third source of English-Chinese differences is the distribution of roles; the Chinese data has proportionally more adjuncts (ARGMs), while the English data has proportionally more oblique arguments (ARG2, ARG3, ARG4).
Oblique arguments are more difficult to process than other arguments, as was discussed in section 3.4.
This difference is most likely to be caused by labeling factors rather than by true structural differences between English in Chinese.
In summary, the higher performance in our Chinese system is due to 3 factors: the importance of passive in English; the strict word-order constraints of Chinese adverbials, and minor labeling differences.
Conclusions We can draw a number of conclusions from our investigation of semantic parsing in Chinese.
First, reasonably good performance can be achieved with a very small (1100 sentences) training set.
Second, the features that we extracted for English semantic parsing worked well when applied to Chinese.
Many of these features required creating an automatic parse; in doing so we showed that the Collins (1999) parser when ported to Chinese achieved the best reported performance on Chinese syntactic parsing.
Finally, we showed that semantic parsing is significantly easier in Chinese than in English.
We show that this counterintuitive result seems to be due to the strict constraints on adjunct ordering in Chinese, making adjuncts easier to find and label.
Acknowledgements This work was partially supported by the National Science Foundation via a KDD Supplement to NSF CISE/IRI/Interactive Systems Award IIS-9978025.
Many thanks to Ying Chen for her help on the Collins parser port, and to Nianwen Xue and Sameer Pradhan for providing the data.
Thanks to Kadri Hacioglu, Wayne Ward, James Martin, Martha Palmer, and three anonymous reviewers for helpful advice.
Appendix: Head rules for Chinese Parent ADJP ADVP CLP CP DNP DP DVP IP LCP LST NP PP PRN QP UCP VCD VP VPT VRD VSB Direction Right Right Right Right Right Left Right Right Right Right Right Left Left Right Left Left Left Left Left Right Priority List ADJP JJ AD ADVP AD CS JJ NP PP P VA VV CLP M NN NP CP IP VP DEG DNP DEC QP M(r) DP DT OD DEV AD VP VP IP NP LCP LC CD NP QP NP NN IP NR NT P PP PU QP CLP CD IP NP VP VV VA VE VE VC VV VNV VPT VRD VSB VCD VP VA VV VVl VA VV VE References Baker, Collin F., Charles J.
Fillmore, and John B.
Lowe. 1998.
The Berkekey FrameNet Project.
In Proceeding of COLING/ACL.
Bikel, Daniel and David Chiang.
2000. Two Statistical Parsing models Applied to the Chinese Treebank.
In Proceedings of the Second Chinese Language Processing Workshop, pp.
1-6. Chiang, David and Daniel Bikel.
2002. Recovering Latent Information in Treebanks.
In Proceedings of COLING-2002, pp.183-189.
Collins, Michael.
1999. Head-driven Statistical Models for Natural Language Parsing.
Ph.D. dissertation, University of Pennsylvannia.
Collins, Michael, Jan Hajic, Lance Ramshaw and Christoph Tillmann.
1999. A th Statistical Parser for Czech.
In Proceedings of the 37 Meeting of the ACL, pp.
505-512. Gildea, Daniel and Daniel Jurafsky.
2002. Automatic Labeling of Semantic Roles.
Computational Linguistics, 28(3):245-288.
Gildea, Daniel and Martha Palmer.
2002. The Necessity of Parsing for Predicate Argument Recognition, In Proceedings of the 40th Meeting of the ACL, pp.
239-246. Kingsbury, Paul, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the Penn Treebank.
In Proceedings of HLT-02.
Kudo, Taku and Yuji Matsumoto.
2000. Use of support vector learning for chunk Identification.
In Proceedings of the 4th Conference on CoNLL, pp.
142-144. Kudo, Taku and Yuji Matsumoto.
2001 Chunking with Support Vector Machines.
In Proceeding of the 2nd Meeting of the NAACL.
pp.192-199. Levy, Roger and Christopher Manning.
2003. Is it harder to parse Chinese, or the Chinese Treebank?
ACL 2003, pp.
439-446. Pradhan, Sameer, Kadri Hacioglu,.
Wayne Ward, James Martin, and Daniel Jurafsky.
2003. "Semantic Role Parsing: Adding Semantic Structure to Unstructured Text".
In the Proceedings of the International Conference on Data Mining (ICDM2003), Melbourne, FL, 2003 Surdeanu, Mihai, Sanda Harabagiu, John Williams and Paul Aarseth.
2003. Using Predicate-Argument Structures for Information Extraction, In Proceedings of ACL.
Xue, Nianwen.
2002.stGuidelines for the Penn Chinese Proposition Bank (1 Draft), UPenn.
Xue, Nianwen, Fu-Dong Chiou and Martha Palmer.
2002. Building a large-scale annotated Chinese corpus.
In Proceedings of COLING-2002.
Xue, Nianwen, Martha Palmer.
2003. Annotating the propositions in the Penn Chinese Treebank.
In Proceedings of the 2nd SIGHAN Workshop on Chinese Language Processing.
Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp.
64-71. Generalized Encoding of Description Spaces and its Application to Typed Feature Structures Gerald Penn Department of Computer Science University of Toronto 10 King's College Rd.
Toronto M5S 3G4, Canada Abstract This paper presents a new formalization of a unificationor join-preserving encoding of partially ordered sets that more essentially captures what it means for an encoding to preserve joins, generalizing the standard definition in AI research.
It then shows that every statically typable ontology in the logic of typed feature structures can be encoded in a data structure of fixed size without the need for resizing or additional union-find operations.
This is important for any grammar implementation or development system based on typed feature structures, as it significantly reduces the overhead of memory management and reference-pointer-chasing during unification.
adj noun nom acc plus minus subst case bool head Figure 1: A sample type system with appropriateness conditions.
the types of values (value restrictions) those feature values must have.
In Figure 1,1 for example, all head-typed TFSs must have bool-typed values for the features MOD and PRD, and no values for any other feature.
Relative to data structures like arrays or logical terms, typed feature structures (TFSs) can be regarded as an expressive refinement in two different ways.
First, they are typed, and the type system allows for subtyping chains of unbounded depth.
Figure 1 has a chain of length from to noun.
Pointers to arrays and logical terms can only monotonically "refine" their (syntactic) type from unbound (for logical terms, variables) to bound.
Second, although all the TFSs of a given type have the same features because of appropriateness, a TFS may acquire more features when it promotes to a subtype.
If a head-typed TFS promotes to noun in the type system above, for example, it acquires one extra casevalued feature, CASE.
When a subtype has two or 1 In this paper, Carpenter's (1992) convention of using as the most general type, and depicting subtypes above their supertypes is used.
1 Motivation
The logic of typed feature structures (Carpenter, 1992) has been widely used as a means of formalizing and developing natural language grammars that support computationally efficient parsing, generation and SLD resolution, notably grammars within the Head-driven Phrase Structure Grammar (HPSG) framework, as evidenced by the recent successful development of the LinGO reference grammar for English (LinGO, 1999).
These grammars are formulated over a finite vocabulary of features and partially ordered types, in respect of constraints called appropriateness conditions.
Appropriateness specifies, for each type, all and only the features that take values in feature structures of that type, along with fiAn order-embedding preserves the behavior of the order relation (for TFS type systems, subtyping; more incomparable supertypes, a TFS can also multiply inherit features from other supertypes when it promotes.
The overwhelmingly most prevalent operation when working with TFS-based grammars is unification, which corresponds mathematically to finding a least upper bound or join.
The most common instance of unification is the special case in which a TFS is unified with the most general TFS that satisfies a description stated in the grammar.
This special case can be decomposed at compile-time into more atomic operations that (1) promote a type to a subtype, (2) bind a variable, or (3) traverse a feature path, according to the structure of the description.
TFSs actually possess most of the properties of fixed-arity terms when it comes to unification, due to appropriateness.
Nevertheless, unbounded subtyping chains and acquiring new features conspire to force most internal representations of TFSs to perform extra work when promoting a type to a subtype to earn the expressive power they confer.
Upon being repeatedly promoted to new subtypes, they must be repeatedly resized or repeatedly referenced with a pointer to newly allocated representations, both of which compromise locality of reference in memory and/or involve pointer-chasing.
These costs are significant.
Because appropriateness involves value restrictions, simply padding a representation with some extra space for future features at the outset must guarantee a proper means of filling that extra space with the right value when it is used.
Internal representations that lazily fill in structure must also be wary of the common practice in description languages of binding a variable to a feature value with a scope larger than a single TFS -for example, in sharing structure between a daughter category and a mother category in a phrase structure rule.
In this case, the representation of a feature's value must also be interpretable independent of its context, because two separate TFSs may refer to that variable.
These problems are artifacts of not using a representation which possesses what in knowledge representation is known as a join-preserving encoding of a grammar's TFSs -in other words, a representation with an operation that naturally behaves like TFS-unification.
The next section presents the standard definition of join-preserving encodings and provides a generalization that more essentially captures what it means for an encoding to preserve joins.
Section 3 formalizes some of the defining characteristics of TFSs as they are used in computational linguistics.
Section 4 shows that these characteristics quite fortuitously agree with what is required to guarantee the existence of a joinpreserving encoding of TFSs that needs no resizing or extra referencing during type promotion.
Section 5 then shows that a generalized encoding exists in which variable-binding scope can be larger than a single TFS -a property no classical encoding has.
Earlier work on graph unification has focussed on labelled graphs with no appropriateness, so the central concern was simply to minimize structure copying.
While this is clearly germane to TFSs, appropriateness creates a tradeoff among copying, the potential for more compact representations, and other memory management issues such as locality of reference that can only be optimized empirically and relative to a given grammar and corpus (a recent example of which can be found in Callmeier (2001)).
While the present work is a more theoretical consideration of how unification in one domain can simulate unification in another, the data structure described here is very much motivated by the encoding of TFSs as Prolog terms allocated on a contiguous WAM-style heap.
In that context, the emphasis on fixed arity is really an attempt to avoid copying, and lazily filling in structure is an attempt to make encodings compact, but only to the extent that join preservation is not disturbed.
While this compromise solution must eventually be tested on larger and more diverse grammars, it has been shown to reduce the total parsing time of a large corpus on the ALE HPSG benchmark grammar of English (Penn, 1993) by a factor of about 4 (Penn, 1999).
2 Join-Preserving Encodings We may begin with a familiar definition from discrete mathematics: and Definition 1 Given two partial orders, a function is an orderembedding iff, for every, iff. fif Figure 3: A non-classical join-preserving encoding between BCPOs for which no classical joinpreserving encoding exists.
Bounded completeness ensures that unification or joins are well-defined among consistent types.
Definition 3 Given two BCPOs, and, is a classical join-preserving encoding of into iff: injectivity is an injection, Join-preserving encodings are automatically orderembeddings because iff . There is actually a more general definition: Definition 4 Given two BCPOs, and, is a (generalized) join-preserving encoding of into iff: disjointness to mean We use the notation to mean is defined.
totality for all iff is undefined, and join homomorphism, where they exist.
zero preservation, and iff Definition 2 A partial order is bounded complete (BCPO) iff every set of elements with a common upper bound has a least upper bound.
When maps elements of to singleton sets in, then reduces to a classical join-preserving encoding.
It is not necessary, however, to require that only one element of represent an element of, provided that it does not matter which representative we choose at any given time.
Figure 3 shows a generalized join-preserving encoding between two partial orders for which no classical encoding exists.
There is no classical encoding of into because no three elements can be found in that pairwise unify to a common join.
A generalized encoding exists because we can choose three potential representatives for : one ( ) for unifying the representatives of and, one ( ) for unifying the representatives of and, and one ( ) for unifying the representatives of and . Notice that the set of representatives for must be closed under unification.
Although space does not permit here, this generalization has been used to prove that well-typing, an alternative interpretation of appropriateness, is equivalent in its expressive power to the interpretation used here (called total well-typing; Carpenter, 1992); that multi-dimensional inheritance (Erbach, 1994) adds no expressive power to any TFS type system; that TFS type systems can encode systemic networks in polynomial space using extensional types (Carpenter, 1992); and that certain uses of parametjoin homomorphism for all and,, where they exist.
zero preservation for all, iff, and for TFSs themselves, subsumption) in the encoding codomain.
As shown in Figure 2, however, order embeddings do not always preserve operations such as least upper bounds.
The reason is that the image of may not be closed under those operations in the codomain.
In fact, the codomain could provide joins where none were supposed to exist, or, as in Figure 2, no joins where one was supposed to exist.
Mellish (1991; 1992) was the first to formulate join-preserving encodings correctly, by explicitly refor the quiring this preservation.
Let us write join of and in partial order . Figure 2: An example order-embedding that cannot translate least upper bounds.
and fihead (Upward Closure / Right Monotonicity) if F and, then F and F F . The function Approp maps a feature and type to the value restriction on that feature when it is appropriate to that type.
If it is not appropriate, then Approp is undefined at that pair.
Feature introduction ensures that every feature has a least type to which it is appropriate.
This makes description compilation more efficient.
Upward closure ensures that subtypes inherit their supertypes' features, and with consistent value restrictions.
The combination of these two properties allows us to annotate a BCPO of types with features and value restrictions only where the feature is introduced or the value restriction is refined, as in Figure 1.
A very useful property for type systems to have is static typability.
This means that if two TFSs that are well-formed according to appropriateness are unifiable, then their unification is automatically well-formed as well -no additional work is necessary.
Theorem 1 (Carpenter, 1992) An appropriateness specification is statically typable iff, for all types such that, and all F : unrestricted if only if only otherwise if and if (Feature type Introduction) MOD PRD plus plus Figure 5: A TFS of type head from the type system in Figure 1.
Not all type systems are statically typable, but a type system can be transformed into an equivalent statically typable type system plus a set of universal constraints, the proof of which is omitted here.
In linguistic applications, we normally have a set of universal constraints anyway for encoding principles of grammar, so it is easy and computationally inexpensive to conduct this transformation.
4 Static
Encodability As mentioned in Section 1, what we want is an encoding of TFSs with a notion of unification that naturally corresponds to TFS-unification.
As discussed in Section 3, static typability is something we can reasonably guarantee in our type systems, and is therefore something we expect to be reflected in our encodings -no extra work should be done apart from combining the types and recursing on feature values.
If we can ensure this, then we have avoided the extra work that comes with resizing or unnecessary referencing and pointer-chasing.
As mentioned above, what would be best from the standpoint of memory management is simply a fixed array of memory cells, padded with extra space to accommodate features that might later be added.
We will call these frames.
Figure 4 depicts a frame for the head-typed TFS in Figure 5.
In a frame, the representation of the type can either be (1) a bit vector encoding the type,3 or (2) a reference pointer Instead of a bit vector, we could also use an index into a table if least upper bounds are computed by table look-up.
PQ Definition 5 A TFS type system consists of a finite BCPO of types,, a finite set of features Feat, and a partial function, such that, for every F : Figure 4: A fixed array representation of the TFS in Figure 5.
There are only a few common-sense restrictions we need to place on our type systems: 3 TFS Type Systems ric typing with TFSs also add no expressive power to the type system (Penn, 2000).
(head representation) (MOD representation) (PRD representation) fiF: G: H: Figure 6: A type system with three features and a three-colorable feature graph.
module of its type.
Even this number can normally be reduced: Definition 7 The feature graph,, of module is an undirected graph, whose vertices correspond to the features introduced in, and in which there is an edge,, iff and are appropriate to a common maximally specific type in . Proposition 1 The least number of feature slots required for a frame of any type in is the least for which is -colorable.
There are type systems, of course, for which modularization and graph-coloring will not help.
Figure 6, for example, has one module, three features, and a three-clique for a feature graph.
There are statistical refinements that one could additionally make, such as determining the empirical probability that a particular feature will be acquired and electing to pay the cost of resizing or referencing for improbable features in exchange for smaller frames.
4.2 Correctness
of Frames Restricting the Size of Frames At first blush, the prospect of adding as many extra slots to a frame as there could be extra features in a TFS sounds hopelessly unscalable to large grammars.
While recent experience with LinGO (1999) suggests a trend towards modest increases in numbers of features compared to massive increases in numbers of types as grammars grow large, this is nevertheless an important issue to address.
There are two discrete methods that can be used in combination to reduce the required number of extra slots:, the set of Definition 6 Given a finite BCPO, modules of is the finest partition of,, such that (1) each is upward-closed (with respect to subtyping), and (2) if two types have a least upper bound, then they belong to the same module.
Trivially, if a feature is introduced at a type in one module, then it is not appropriate to any type in any other module.
As a result, a frame for a TFS only needs to allow for the features appropriate to the Prolog terms require one additional unbound variable per TFS (sub)term in order to preserve the intensionality of the logic -unlike Prolog terms, structurally identical TFS substructures are not identical unless explicitly structure-shared.
With the exception of extra slots for unused feature values, frames are clearly isomorphic in their structure to the TFSs they represent.
The implementation of unification that we prefer to avoid resizing and referencing is to (1) find the least upper bound of the types of the frames being unified, (2) update one frame's type to the least upper bound, and point the other's type representation to it, and (3) recurse on respective pairs of feature values.
The frame does not need to be resized, only the types need to be referenced, and in the special case of promoting the type of a single TFS to a subtype, the type only needs to be trailed.
If cyclic TFSs are not supported, then acyclicity must also be enforced with an occurscheck.
The correctness of frames as a join-preserving encoding of TFSs thus depends on being able to make sense of the values in these unused positions.
The to another frame.
If backtracking is supported in search, changes to the type representation must be trailed.
For each appropriate feature, there is also a pointer to a frame for that feature's value.
There are also additional pointers for future features (for head, CASE) that are grounded to some distinguished value indicating that they are unused -usually a circular reference to the referring array position.
Cyclic TFSs, if they are supported, would be represented with cyclic (but not 1-cyclic) chains of pointers.
Frames can be implemented either directly as arrays, or as Prolog terms.
In Prolog, the type representation could either be a term-encoding of the type, which is guaranteed to exist for any finite BCPO (Mellish, 1991; Mellish, 1992), or in extended Prologs, another trailable representation such as a mutable term (Aggoun and Beldiceanu, 1990) or an attributed value (Holzbaur, 1992).
Padding the representation with extra space means using a Prolog term with extra arity.
A distinguished value for unused arguments must then be a unique unbound variable.4 Figure 7: A type system that introduces a feature at a join-reducible type.
head Figure 9: The frame for Figure 8.
MOD PRD PRD (head representation) Figure 11: The frame for Figure 10.
5 The
sole exception is a TFS of type, which by definition belongs to no module and has no features.
Its representation is a distinguished circular reference, unless two or more feature values share a single -typed TFS value, in which case one is a circular reference and the rest point to it.
The circular one can be chosen canonically to ensure that the encoding is still classical.
(MOD/PRD representation) problem is that features may be introduced at joinreducible types, as in Figure 7.
There is only one module, so the frames for a and b must have a slot available for the feature F.
When an a-typed TFS unifies with a b-typed TFS, the result will be of type c, so leaving the slot marked unused after recursion would be incorrect -we would need to look in a table to see what value to assign it.
An alternative would be to place that value in the frames for a and b from the beginning.
But since the value itself must be of type a in the case of Figure 7, this strategy would not yield a finite representation.
The answer to this conundrum is to use a distinguished circular reference in a slot iff the slot is either unused or the value it contains is (1) the most general satisfier of the value restriction of the feature it represents and (2) not structure-shared with any other feature in the TFS.5 During unification, if one TFS is a circular reference, and the other is not, the circular reference is referenced to the other.
If both values are circular references, then one is referenced to the other, which remains circular.
The feature structure in Figure 8, for example, has the frame representation shown in Figure 9.
The PRD value is a TFS of type bool, and this value is not shared with any other structure in the TFS.
If the values of MOD and PRD are both bool-typed, then if Figure 10: A TFS of type head in which both feature values are most general satisfiers of the value restrictions, but they are shared.
lar references (Figure 11), and if they are not shared (Figure 12), both of them use a different circular reference (Figure 13).
With this convention for circular references, frames are a classical join-preserving encoding of the TFSs of any statically typable type system.
Although space does not permit a complete proof here, the intuition is that (1) most general satisfiers of value restrictions necessarily subsume every other value that a totally well-typed TFS could take at that feature, and (2) when features are introduced, their initial values are not structure-shared with any other substructure.
Static typability ensures that value restrictions unify to yield value restrictions, except in the final case of Theorem 1.
The following lemma deals with this case: Lemma 1 If Approp is statically typable, and for some F, F F, then either F, and or MOD Figure 8: A TFS of type head in which one feature value is a most general satisfier of its feature's value restriction.
head bool PQ plus bool they are shared (Figure 10), they do not use circu(head representation) (MOD representation) fihead MOD PRD (head representation) Figure 15: A statically typable "type system" that multiply introduces F at join-reducible elements with different value restrictions.
duction, but in fact, the result holds if we allow for multiple introducing types, provided that all of them agree on what the value restriction for the feature should be.
Would-be type systems that multiply introduce a feature at join-reducible elements (thus requiring some kind of distinguished-value encoding), disagree on the value restriction, and still remain statically typable are rather difficult to come by, but they do exist, and for them, a frame encoding will not work.
Figure 15 shows one such example.
In this signature, the unification: s Figure 13: The frame for Figure 12.
Proof: does not exist, but the unification of their frame encodings must succeed because the -typed TFS's F value must be encoded as a circular reference.
To the best of the author's knowledge, there is no fixedsize encoding for Figure 15.
5 Generalized
Term Encoding In practice, this classical encoding is not good for much.
Description languages typically need to bind variables to various substructures of a TFS,, and then pass those variables outside the substructures of where they can be used to instantiate the value of another feature structure's feature, or as arguments to some function call or procedural goal.
If a value in a single frame is a circular reference, we can properly understand what that reference encodes with the above convention by looking at its context, i.e., the type.
Outside the scope of that frame, we have no way of knowing which feature's value restriction it is supposed to encode.
Figure 14: The second case in the proof of Lemma 1.
. F, so F and F So there are three cases to consider: Intro2 F : then the result trivially holds.
Intro F but Intro F4 (or by symmetry, the opposite): then we have the situation in Figure 14.
It must be that F4, so by static typability, the lemma holds.
Intro F and Intro F : and F, so and F4 are consistent.
By bounded completeness, F and F . By upward closure, F F4 and by static typability, F F4 F F.
Furthermore, F ; thus by static typability the lemma holds.
This lemma is very significant in its own right -it says that we know more than Carpenter's Theorem 1.
An introduced feature's value restriction can always be predicted in a statically typable type system.
The lemma implicitly relies on feature introF F Suppose Figure 12: A TFS of type head in which both feature values are most general satisfiers of the value restrictions, and they are not shared.
F: F: bool bool H' F: PQ FG!
FG! FG!
E Introduced feature has variable encoding Figure 16: A pictorial overview of the generalized encoding.
A generalized term encoding provides an elegant solution to this problem.
When a variable is bound to a substructure that is a circular reference, it can be filled in with a frame for the most general satisfier that it represents and then passed out of context.
Having more than one representative for the original TFS is consistent, because the set of representatives is closed under this filling operation.
A schematic overview of the generalized encoding is in Figure 16.
Every set of frames that encode a particular TFS has a least element, in which circular references are always opted for as introduced feature values.
This is the same element as the classical encoding.
It also has a greatest element, in which every unused slot still has a circular reference, but all unshared most general satisfiers are filled in with frames.
Whenever we bind a variable to a substructure of a TFS, filling pushes the TFS's encoding up within the same set to some other encoding.
As a result, at any given point in time during a computation, we do not exactly know which encoding we are using to represent a given TFS.
Furthermore, when two TFSs are unified successfully, we do not know exactly what the result will be, but we do know that it falls inside the correct set of representatives because there is at least one frame with circular references for the values of every newly introduced feature.
variable binding 6 Conclusion Simple frames with extra slots and a convention for filling in feature values provide a join-preserving encoding of any statically typable type system, with no resizing and no referencing beyond that of type representations.
A frame thus remains stationary in memory once it is allocated.
A generalized encoding, moreover, is robust to side-effects such as extra-logical variable-sharing.
Frames have many potential implementations, including Prolog terms, WAM-style heap frames, or fixed-sized records.
References A.
Aggoun and N.
Beldiceanu. 1990.
Time stamp techniques for the trailed data in constraint logic programming systems.
In S.
Bourgault and M.
Dincbas, editors, Programmation en Logique, Actes du 8eme Seminaire, pages 487509.
U. Callmeier.
2001. Efficient parsing with large-scale unification grammars.
Master's thesis, Universitaet des Saarlandes.
B. Carpenter.
1992. The Logic of Typed Feature Structures.
Cambridge. G.
Erbach. 1994.
Multi-dimensional inheritance.
In Proceedings of KONVENS 94.
Springer. C.
Holzbaur. 1992.
Metastructures vs.
attributed variables in the context of extensible unification.
In M.
Bruynooghe and M.
Wirsing, editors, Programming Language Implementation and Logic Programming, pages 260268.
Springer Verlag.
LinGO. 1999.
The LinGO grammar and lexicon.
Available on-line at http://lingo.stanford.edu.
C. Mellish.
1991. Graph-encodable description spaces.
Technical report, University of Edinburgh Department of Artificial Intelligence.
DYANA Deliverable R3.2B.
C. Mellish.
1992. Term-encodable description spaces.
In D.R.
Brough, editor, Logic Programming: New Frontiers, pages 189207.
Kluwer. G.
Penn. 1993.
The ALE HPSG benchmark grammar.
Available on-line at http://www.cs.toronto.edu/ gpenn/ale.html.
G. Penn.
1999. An optimized Prolog encoding of typed feature structures.
In Proceedings of the 16th International Conference on Logic Programming (ICLP-99), pages 124138.
G. Penn.
2000. The Algebraic Structure of Attributed Type Signatures.
Ph.D. thesis, Carnegie Mellon University .
A High-Performance Semi-Supervised Learning Method for Text Chunking Rie Kubota Ando Tong Zhang IBM T.J.
Watson Research Center Yorktown Heights, NY 10598, U.S.A.
rie1@us.ibm.com tongz@us.ibm.com Abstract In machine learning, whether one can build a more accurate classifier by using unlabeled data (semi-supervised learning) is an important issue.
Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear.
This paper presents a novel semi-supervised method that employs a learning paradigm which we call structural learning.
The idea is to find "what good classifiers are like" by learning from thousands of automatically generated auxiliary classification problems on unlabeled data.
By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem.
The method produces performance higher than the previous best results on CoNLL'00 syntactic chunking and CoNLL'03 named entity chunking (English and German).
(Blum and Mitchell, 1998) automatically bootstraps labels, and such labels are not necessarily reliable (Pierce and Cardie, 2001).
A related idea is to use Expectation Maximization (EM) to impute labels.
Although useful under some circumstances, when a relatively large amount of labeled data is available, the procedure often degrades performance (e.g.
Merialdo (1994)).
A number of bootstrapping methods have been proposed for NLP tasks (e.g.
Yarowsky (1995), Collins and Singer (1999), Riloff and Jones (1999)).
But these typically assume a very small amount of labeled data and have not been shown to improve state-of-the-art performance when a large amount of labeled data is available.
Our goal has been to develop a general learning framework for reliably using unlabeled data to improve performance irrespective of the amount of labeled data available.
It is exactly this important and difficult problem that we tackle here.
This paper presents a novel semi-supervised method that employs a learning framework called structural learning (Ando and Zhang, 2004), which seeks to discover shared predictive structures (i.e.
what good classifiers for the task are like) through jointly learning multiple classification problems on unlabeled data.
That is, we systematically create thousands of problems (called auxiliary problems) relevant to the target task using unlabeled data, and train classifiers from the automatically generated `training data'.
We learn the commonality (or structure) of such many classifiers relevant to the task, and use it to improve performance on the target task.
One example of such auxiliary problems for chunking tasks is to `mask' a word and predict whether it is "people" or not from the context, like language modeling.
Another example is to predict the pre1 Introduction In supervised learning applications, one can often find a large amount of unlabeled data without difficulty, while labeled data are costly to obtain.
Therefore, a natural question is whether we can use unlabeled data to build a more accurate classifier, given the same amount of labeled data.
This problem is often referred to as semi-supervised learning.
Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear.
For example, co-training Proceedings of the 43rd Annual Meeting of the ACL, pages 19, Ann Arbor, June 2005.
c 2005 Association for Computational Linguistics fidiction of some classifier trained for the target task.
These auxiliary classifiers can be adequately learned since we have very large amounts of `training data' for them, which we automatically generate from a very large amount of unlabeled data.
The contributions of this paper are two-fold.
First, we present a novel robust semi-supervised method based on a new learning model and its application to chunking tasks.
Second, we report higher performance than the previous best results on syntactic chunking (the CoNLL'00 corpus) and named entity chunking (the CoNLL'03 English and German corpora).
In particular, our results are obtained by using unlabeled data as the only additional resource while many of the top systems rely on hand-crafted resources such as large name gazetteers or even rulebased post-processing.
model complexity.
ERM-based methods for discriminative learning are known to be effective for NLP tasks such as chunking (e.g.
Kudoh and Matsumoto (2001), Zhang and Johnson (2003)).
2.2 Linear
model for structural learning We present a linear prediction model for structural learning, which extends the traditional model to multiple problems.
Specifically, we assume that there exists a low-dimensional predictive structure shared by multiple prediction problems.
We seek to discover this structure through joint empirical risk minimization over the multiple problems.
Consider  problems indexed by   , each with  samples   indexed by   . In our joint linear model, a predictor for problem takes the following form 2 A Model for Learning Structures This work uses a linear formulation of structural learning.
We first briefly review a standard linear prediction model and then extend it for structural learning.
We sketch an optimization algorithm using SVD and compare it to related methods.
2.1 Standard
linear prediction model In the standard formulation of supervised learning, we seek a predictor that maps an input vector  to the corresponding output   . Linear prediction models are based on real-valued predictors of  the form  where is called a weight vector.
For binary problems, the sign of the linear prediction gives the class label.
For -way classification (with ), a typical method is winner takes all, where we train one predictor per class and choose the class with the highest output value.
A frequently used method for finding an accurate predictor is regularized empirical risk minimization (ERM), which minimizes an empirical loss of the predictor (with regularization) on the  training examples  : where we use to denote the identity matrix.
Matrix  (whose rows are orthonormal) is the common structure parameter shared by all the problems; and are weight vectors specific to each prediction problem . The idea of this model is to discover a common low-dimensional predictive structure (shared by the  problems) parameterized by the projection matrix .
In this setting, the goal of structural learning may also be regarded as learning a good feature map  -a low-dimensional feature vector parameterized by .
In joint ERM, we seek  (and weight vectors) that minimizes the empirical risk summed over all the problems: It can be shown that using joint ERM, we can reliably estimate the optimal joint parameter  as long as  is large (even when each  is small).
This is the key reason why structural learning is effective.
A formal PAC-style analysis can be found in (Ando and Zhang, 2004).
2.3 Alternating
structure optimization (ASO)  is a loss function to quantify the difference between the prediction   and the true output, and  is a regularization term to control the The optimization problem (2) has a simple solution using SVD when we choose square regularization: ter is given.
For clarity, let be a weight vector   Then, for problem such that: (2) becomes the minimization of the joint empirical risk written as: Input: training data ( ) Parameters: dimension and regularization param Output: matrix with rows , and arbitrary Initialize: iterate to  do for With fixed and, solve for : This minimization can be approximately solved by the following alternating optimization procedure: minimizes the joint empirical risk ((3)., and find  minimizes the joint empirical risk (3).
that  Let endfor Compute the SVD of   Let the rows of be the left singular vectors of corresponding to the largest singular values.
until converge Figure 1: SVD-based Alternating Structure Optimization (SVD-ASO) Algorithm  Iterate until a convergence criterion is met.
In the first step, we train  predictors independently.
It is the second step that couples all the problems.
Its solution is given by the SVD (singular value decomposition) of the predictor matrix   : the rows of the optimum  are given by the most sigIntuitively, the nificant left singular vectors1 of optimum  captures the maximal commonality of the  predictors (each derived from ).
These  predictors are updated using the new structure matrix  in the next iteration, and the process repeats.
Figure 1 summarizes the algorithm sketched above, which we call the alternating structure optimization (ASO) algorithm.
The formal derivation can be found in (Ando and Zhang, 2004).
Comparison with existing techniques It is important to note that this SVD-based ASO (SVD-ASO) procedure is fundamentally different from the usual principle component analysis (PCA), which can be regarded as dimension reduction in the data space . By contrast, the dimension reduction performed in the SVD-ASO algorithm is on the predictor space (a set of predictors).
This is possible because we observe multiple predictors from multiple learning tasks.
If we regard the observed predictors as sample points of the predictor distribution in 1 is computed so that the best low-rank In other words, in the least square sense is obtained by approximation of see e.g.
Golub and Loan projecting onto the row space of (1996) for SVD.
the predictor space (corrupted with estimation error, or noise), then SVD-ASO can be interpreted as finding the "principle components" (or commonality) of these predictors (i.e., "what good predictors are like").
Consequently the method directly looks for low-dimensional structures with the highest predictive power.
By contrast, the principle components of input data in the data space (which PCA seeks) may not necessarily have the highest predictive power.
The above argument also applies to the feature generation from unlabeled data using LSI (e.g.
Ando (2004)).
Similarly, Miller et al.(2004) used word-cluster memberships induced from an unannotated corpus as features for named entity chunking.
Our work is related but more general, because we can explore additional information from unlabeled data using many different auxiliary problems.
Since Miller et al.(2004)'s experiments used a proprietary corpus, direct performance comparison is not possible.
However, our preliminary implementation of the word clustering approach did not provide any improvement on our tasks.
As we will see, our starting performance is already high.
Therefore the additional information discovered by SVD-ASO appears crucial to achieve appreciable improvements.
3 Semi-supervised Learning Method For semi-supervised learning, the idea is to create many auxiliary prediction problems (relevant to the task) from unlabeled data so that we can learn the fishared structure  (useful for the task) using the ASO algorithm.
In particular, we want to create auxiliary problems with the following properties: we need to automatically generate various "labeled" data for the auxiliary problems from unlabeled data.
auxiliary problems should be related to the target problem.
That is, they should share a certain predictive structure.
The final classifier for the target task is in the form of (1), a linear predictor for structural learning.
We fix  (learned from unlabeled data through auxiliary problems) and optimize weight vectors and on the given labeled data.
We summarize this semisupervised learning procedure below.
Ex 3.1 Predict words.
Create auxiliary problems by regarding the word at each position as an auxiliary label, which we want to predict from the context.
For instance, predict whether a word is "Smith" or not from its context.
This problem is relevant to, for instance, named entity chunking since knowing a word is "Smith" helps to predict whether it is part of a name.
One binary classification problem can be created for each possible word value (e.g., "IBM", "he", "get",    ).
Hence, many auxiliary problems can be obtained using this idea.
More generally, given a feature representation of the input data, we may mask some features as unobserved, and learn classifiers to predict these `masked' features based on other features that are not masked.
The automatic-labeling requirement is satisfied since the auxiliary labels are observable to us.
To create relevant problems, we should choose to (mask and) predict features that have good correlation to the target classes, such as words on text tagging/chunking tasks.
3.1.2 Partially-supervised strategy Auxiliary problem creation The idea is to discover useful features (which do not necessarily appear in the labeled data) from the unlabeled data through learning auxiliary problems.
Clearly, auxiliary problems more closely related to the target problem will be more beneficial.
However, even if some problems are less relevant, they will not degrade performance severely since they merely result in some irrelevant features (originated from irrelevant -components), which ERM learners can cope with.
On the other hand, potential gains from relevant auxiliary problems can be significant.
In this sense, our method is robust.
We present two general strategies for generating useful auxiliary problems: one in a completely unsupervised fashion, and the other in a partiallysupervised fashion.
3.1.1 Unsupervised
strategy In the first strategy, we regard some observable as auxiliary class substructures of the input data labels, and try to predict these labels using other parts of the input data.
The second strategy is motivated by co-training.
We use two (or more) distinct feature maps:  and  . First, we train a classifier  for the target task, using the feature map  and the labeled data.
The auxiliary tasks are to predict the behavior of this classifier  (such as predicted labels) on the unlabeled data, by using the other feature map  . Note that unlike co-training, we only use the classifier as a means of creating auxiliary problems that meet the relevancy requirement, instead of using it to bootstrap labels.
Ex 3.2 Predict the topchoices of the classifier.
Predict the combination of (a few) classes to which  assigns the highest output (confidence) values.
For instance, predict whether  assigns the highest confidence values to CLASS1 and CLASS2 in this order.
By setting , the auxiliary task is simply to predict the label prediction of classifier  . By set, fine-grained distinctions (related to inting trinsic sub-classes of target classes) can be learned.
From a -way classification problem,   binary prediction problems can be created.
fi4 Algorithms Used in Experiments Using auxiliary problems introduced above, we study the performance of our semi-supervised learning method on named entity chunking and syntactic chunking.
This section describes the algorithmic aspects of the experimental framework.
The taskspecific setup is described in Sections 5 and 6.
4.1 Extension
of the basic SVD-ASO algorithm In our experiments, we use an extension of SVDASO.
In NLP applications, features have natural grouping according to their types/origins such as `current words', `parts-of-speech on the right', and so forth.
It is desirable to perform a localized optimization for each of such natural feature groups.
Hence, we associate each feature group with a submatrix of structure matrix .
The optimization algorithm for this extension is essentially the same as SVD-ASO in Figure 1, but with the SVD step performed separately for each group.
See (Ando and Zhang, 2004) for the precise formulation.
In addition, we regularize only those components of which correspond to the non-negative part of . The motivation is that positive weights are usually directly related to the target concept, while negative ones often yield much less specific information representing `the others'.
The resulting extension, in effect, only uses the positive components of in the SVD computation.
(Zhang, 2004).
As we will show in Section 7.3, our formulation (rowis relatively insensitive to the change in dimension of the structure matrix).
We fix (for each feature group) to 50, and use it in all settings.
The most time-consuming process is the training of  auxiliary predictors on the unlabeled data in Figure 1).
Fixing the number of (computing iterations to a constant, it runs in linear to  and the number of unlabeled instances and takes hours in our settings that use more than 20 million unlabeled instances.
Baseline algorithms Supervised classifier For comparison, we train a classifier using the same features and algorithm, but in effect).
without unlabeled data ( Chunking algorithm, loss function, training algorithm, and parameter settings Co-training We test co-training since our idea of partially-supervised auxiliary problems is motivated by co-training.
Our implementation follows the original work (Blum and Mitchell, 1998).
The two (or more) classifiers (with distinct feature maps) are trained with labeled data.
We maintain a pool of  unlabeled instances by random selection.
The classifier proposes labels for the instances in this pool.
We choose  instances for each classifier with high confidence while preserving the class distribution observed in the initial labeled data, and add them to the labeled data.
The process is then repeated.
We explore =50K, 100K, =50,100,500,1K, and commonly-used feature splits: `current vs.
context' and `current+left-context vs.
current+right-context'. Self-training Single-view bootstrapping is sometimes called self-training.
We test the basic selftraining2, which replaces multiple classifiers in the co-training procedure with a single classifier that employs all the features.
co/self-training oracle performance To avoid the issue of parameter selection for the coand selftraining, we report their best possible oracle performance, which is the best F-measure number among all the coand self-training parameter settings including the choice of the number of iterations.
2 We
also tested "self-training with bagging", which Ng and Cardie (2003) used for co-reference resolution.
We omit results since it did not produce better performance than the supervised baseline.
As is commonly done, we encode chunk information into word tags to cast the chunking problem to that of sequential word tagging.
We perform Viterbistyle decoding to choose the word tag sequence that maximizes the sum of tagging confidence values.
In all settings (including baseline methods), the loss function is a modification of the Huber's ro   bust loss for regression:    if  ; and  otherwise; with square regularization (  ).
One may select other loss functions such as SVM or logistic regression.
The specific choice is not important for the purpose of this paper.
The training algorithm is stochastic gradient descent, which is argued to perform well for regularized convex ERM learning formulations words, parts-of-speech (POS), character types, 4 characters at the beginning/ending in a 5-word window.
words in a 3-syntactic chunk window.
labels assigned to two words on the left.
bi-grams of the current word and the label on the left.
labels assigned to previous occurrences of the current word.
Figure 2: Feature types for named entity chunking.
POS and syntactic chunk information is provided by the organizer.
# of aux.
problems 1000 1000 1000 72 72 72 72 Auxiliary labels previous words current words next words  's top-2 choices  's top-2 choices  's top-2 choices 's top-2 choices Features used for learning aux problems all but previous words all but current words all but next words  (all but left context)  (left context) (all but right context)  (right context) Figure 3: Auxiliary problems used for named entity chunking.
3000 problems `mask' words and predict them from the other features on unlabeled data.
288 problems predict classifier 's predictions on unlabeled data, where is trained with labeled data using feature map . There are 72 possible top-2 choices from 9 classes (beginning/inside of four types of name chunks and `outside').
5 Named
Entity Chunking Experiments We report named entity chunking performance on the CoNLL'03 shared-task3 corpora (English and German).
We choose this task because the original intention of this shared task was to test the effectiveness of semi-supervised learning methods.
However, it turned out that none of the top performing systems used unlabeled data.
The likely reason is that the number of labeled data is relatively large ( 200K), making it hard to benefit from unlabeled data.
We show that our ASO-based semi-supervised learning method (hereafter, ASO-semi) can produce results appreciably better than all of the top systems, by using unlabeled data as the only additional resource.
In particular, we do not use any gazetteer information, which was used in all other systems.
The CoNLL corpora are annotated with four types of named entities: persons, organizations, locations, and miscellaneous names (e.g., "World Cup").
We use the official training/development/test splits.
Our unlabeled data sets consist of 27 million words (English) and 35 million words (German), respectively.
They were chosen from the same sources  Reuters and ECI Multilingual Text Corpus  as the provided corpora but disjoint from them.
5.1 Features
of the classifier" using feature splits `left context vs.
the others' and `right context vs.
the others'.
For word-prediction problems, we only consider the instances whose current words are either nouns or adjectives since named entities mostly consist of these types.
Also, we leave out all but at most 1000 binary prediction problems of each type that have the largest numbers of positive examples to ensure that auxiliary predictors can be adequately learned with a sufficiently large number of examples.
The results we report are obtained by using all the problems in Figure 3 unless otherwise specified.
5.3 Named
entity chunking results methods Our feature representation is a slight modification of a simpler configuration (without any gazetteer) in (Zhang and Johnson, 2003), as shown in Figure 2.
We use POS and syntactic chunk information provided by the organizer.
5.2 Auxiliary
problems test diff.
from supervised data F prec.
recall F English, small (10K examples) training set ASO-semi dev.
81.25 +10.02 +7.00 +8.51 co/self oracle 73.10 +0.32 +0.39 +0.36 ASO-semi test 78.42 +9.39 +10.73 +10.10 co/self oracle 69.63 +0.60 +1.95 +1.31 English, all (204K) training examples ASO-semi dev.
93.15 +2.25 +3.00 +2.62 co/self oracle 90.64 +0.04 +0.20 +0.11 ASO-semi test 89.31 +3.20 +4.51 +3.86 co/self oracle 85.40 German, all (207K) training examples ASO-semi dev.
74.06 +7.04 +10.19 +9.22 co/self oracle 66.47 +4.39 +1.63 ASO-semi test 75.27 +4.64 +6.59 +5.88 co/self oracle 70.45 +2.59 +1.06 Figure 4: Named entity chunking results.
No gazetteer.
Fmeasure and performance improvements over the supervised baseline in precision, recall, and F.
For coand self-training (baseline), the oracle performance is shown.
As shown in Figure 3, we experiment with auxiliary problems from Ex 3.1 and 3.2: "Predict current (or previous or next) words"; and "Predict top-2 choices Figure 4 shows results in comparison with the supervised baseline in six configurations, each trained fiwith one of three sets of labeled training examples: a small English set (10K examples randomly chosen), the entire English training set (204K), and the entire German set (207K), tested on either the development set or test set.
ASO-semi significantly improves both precision and recall in all the six configurations, resulting in improved F-measures over the supervised baseline by +2.62% to +10.10%.
Coand self-training, at their oracle performance, improve recall but often degrade precision; consequently, their F-measure improvements are relatively low: 0.05% to +1.63%.
Comparison with top systems As shown in Figure 5, ASO-semi achieves higher performance than the top systems on both English and German data.
Most of the top systems boost performance by external hand-crafted resources such as: large gazetteers4 ; a large amount (2 million words) of labeled data manually annotated with finer-grained named entities (FIJZ03); and rule-based post processing (KSNM03).
Hence, we feel that our results, obtained by using unlabeled data as the only additional resource, are encouraging.
System ASO-semi FIJZ03 CN03 KSNM03 Eng.
89.31 88.76 88.31 86.31 Ger.
75.27 72.41 65.67 71.90 Additional resources unlabeled data gazetteers; 2M-word labeled data (English) gazetteers (English); (also very elaborated features) rule-based post processing uniand bi-grams of words and POS in a 5-token window.
word-POS bi-grams in a 3-token window.
POS tri-grams on the left and right.
labels of the two words on the left and their bi-grams.
bi-grams of the current word and two labels on the left.
Figure 6: Feature types for syntactic chunking.
POS information is provided by the organizer.
prec. 93.83 94.57 93.76 recall 93.37 94.20 93.56 supervised ASO-semi co/self oracle Figure 7: Syntactic chunking results.
use the WSJ articles in 1991 (15 million words) from the TREC corpus as the unlabeled data.
6.1 Features
and auxiliary problems Our feature representation is a slight modification of a simpler configuration (without linguistic features) in (Zhang et al., 2002), as shown in Figure 6.
We use the POS information provided by the organizer.
The types of auxiliary problems are the same as in the named entity experiments.
For word predictions, we exclude instances of punctuation symbols.
6.2 Syntactic
chunking results Figure 5: Named entity chunking.
F-measure on the test sets.
Previous best results: FIJZ03 (Florian et al., 2003), CN03 (Chieu and Ng, 2003), KSNM03 (Klein et al., 2003).
As shown in Figure 7, ASO-semi improves both precision and recall over the supervised baseline.
It   in F-measure, which outperforms achieves the supervised baseline by  .
Coand selftraining again slightly improve recall but slightly degrade precision at their oracle performance, which demonstrates that it is not easy to benefit from unlabeled data on this task.
Comparison with the previous best systems As shown in Figure 8, ASO-semi achieves performance higher than the previous best systems.
Though the space constraint precludes providing the detail, we note that ASO-semi outperforms all of the previous top systems in both precision and recall.
Unlike named entity chunking, the use of external resources on this task is rare.
An exception is the use of output from a grammar-based full parser as features in ZDJ02+, which our system does not use.
KM01 and CM03 boost performance by classifier combinations.
SP03 trains conditional random fields for NP 6 Syntactic Chunking Experiments Next, we report syntactic chunking performance on the CoNLL'00 shared-task5 corpus.
The training and test data sets consist of the Wall Street Journal corpus (WSJ) sections 1518 (212K words) and section 20, respectively.
They are annotated with eleven types of syntactic chunks such as noun phrases.
We 4 Whether or not gazetteers are useful depends on their coverage.
A number of top-performing systems used their own gazetteers in addition to the organizer's gazetteers and reported significant performance improvements (e.g., FIJZ03, CN03, and ZJ03).
5 http://cnts.uia.ac.be/conll2000/chunking firow# ASO-semi KM01 CM03 SP03 ZDJ02 ZDJ02+ all 94.39 93.91 93.74  93.57 94.17 NP 94.70 94.39 94.41 94.38 93.89 94.38 description +unlabeled data SVM combination perceptron in two layers conditional random fields generalized Winnow +full parser output 4 7 9 11 15 26 Figure 8: Syntactic chunking F-measure.
Comparison with previous best results: KM01 (Kudoh and Matsumoto, 2001), CM03 (Carreras and Marquez, 2003), SP03 (Sha and Pereira, 2003), ZDJ02 (Zhang et al., 2002).
Features corresponding to significant entries Ltd, Inc, Plc, International, Ltd., Association, Group, Inc.
Co, Corp, Co., Company, Authority, Corp., Services PCT, N/A, Nil, Dec, BLN, Avg, Year-on-year, UNCH New, France, European, San, North, Japan, Asian, India Peter, Sir, Charles, Jose, Paul, Lee, Alan, Dan, John, James June, May, July, Jan, March, August, September, April Interpretation organizations organizations no names locations persons months (noun phrases) only.
ASO-semi produces higher NP chunking performance than the others.
Figure 10: Interpretation of computed from wordprediction (unsupervised) problems for named entity chunking.
words beginning with upper-case letters (i.e., likely to be names in English).
Our method captures the spirit of predictive word-clustering but is more general and effective on our tasks.
It is possible to develop a general theory to show that the auxiliary problems we use are helpful under reasonable conditions.
The intuition is as follows.
Suppose we split the features into two parts  and  and predict  based on .
Suppose features in  are correlated to the class labels (but not necessarily correlated among themselves).
Then, the auxiliary prediction problems are related to the target task, and thus can reveal useful structures of  . Under some conditions, it can be shown that features in  with similar predictive performance tend to map to similar low-dimensional vectors through .
This effect can be empirically observed in Figure 10 and will be formally shown elsewhere.
7 Empirical
Analysis 7.1 Effectiveness of auxiliary problems English named entity German named entity 90 76 89 74 88 72 87 70 86 68 85 1 supervised set dev w/ "Predict (previous, current, or next) words" w/ "Predict top-2 choices" w/ "Predict words" + "Predict top-2 choices" Figure 9: Named entity F-measure produced by using individual types of auxiliary problems.
Trained with the entire training sets and tested on the test sets.
Figure 9 shows F-measure obtained by computing  from individual types of auxiliary problems on named entity chunking.
Both types  "Predict words" and "Predict top-2 choices of the classifier"  are useful, producing significant performance improvements over the supervised baseline.
The best performance is achieved when  is produced from all of the auxiliary problems.
7.2 Interpretation
of  Effect of the  dimension 20 40 ASO-semi supervised 60 80 100 dimension To gain insights into the information obtained from unlabeled data, we examine the  entries associated with the feature `current words', computed for the English named entity task.
Figure 10 shows the features associated with the entries of  with the largest values, computed from the 2000 unsupervised auxiliary problems: "Predict previous words" and "Predict next words".
For clarity, the figure only shows Figure 11: F-measure in relation to the row-dimension of English named entity chunking, test set.
Recall that throughout the experiments, we fix the row-dimension of  (for each feature group) to 50.
Figure 11 plots F-measure in relation to the rowdimension of , which shows that the method is relatively insensitive to the change of this parameter, at least in the range which we consider.
We presented a novel semi-supervised learning method that learns the most predictive lowdimensional feature projection from unlabeled data using the structural learning algorithm SVD-ASO.
On CoNLL'00 syntactic chunking and CoNLL'03 named entity chunking (English and German), the method exceeds the previous best systems (including those which rely on hand-crafted resources) by using unlabeled data as the only additional resource.
The key idea is to create auxiliary problems automatically from unlabeled data so that predictive structures can be learned from that data.
In practice, it is desirable to create as many auxiliary problems as possible, as long as there is some reason to believe in their relevancy to the task.
This is because the risk is relatively minor while the potential gain from relevant problems is large.
Moreover, the auxiliary problems used in our experiments are merely possible examples.
One advantage of our approach is that one may design a variety of auxiliary problems to learn various aspects of the target problem from unlabeled data.
Structural learning provides a framework for carrying out possible new ideas.
Acknowledgments Part of the work was supported by ARDA under the NIMD program PNWD-SW-6059.
References Rie Kubota Ando and Tong Zhang.
2004. A framework for learning predictive structures from multiple tasks and unlabeled data.
Technical report, IBM.
RC23462. Rie Kubota Ando.
2004. Semantic lexicon construction: Learning from unlabeled data via spectral analysis.
In Proceedings of CoNLL-2004.
Avrim Blum and Tom Mitchell.
1998. Combining labeled and unlabeled data with co-training.
In proceedings of COLT-98.
Xavier Carreras and Lluis Marquez.
2003. Phrase recognition by filtering and ranking with perceptrons.
In Proceedings of RANLP-2003.
Hai Leong Chieu and Hwee Tou Ng.
2003. Named entity recognition with a maximum entropy approach.
In Proceedings CoNLL-2003, pages 160163.
Michael Collins and Yoram Singer.
1999. Unsupervised models for named entity classification.
In Proceedings of EMNLP/VLC'99.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong Zhang.
2003. Named entity recognition through classifier combination.
In Proceedings CoNLL-2003, pages 168171.
Gene H.
Golub and Charles F.
Van Loan.
1996. Matrix computations third edition.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christopher D.
Manning. 2003.
Named entity recognition with character-level models.
In Proceedings CoNLL2003, pages 188191.
Taku Kudoh and Yuji Matsumoto.
2001. Chunking with support vector machines.
In Proceedings of NAACL 2001.
Bernard Merialdo.
1994. Tagging English text with a probabilistic model.
Computational Linguistics, 20(2):155171.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discriminative training.
In Proceedings of HLT-NAACL-2004.
Vincent Ng and Claire Cardie.
2003. Weakly supervised natural language learning without redundant views.
In Proceedings of HLT-NAACL-2003.
David Pierce and Claire Cardie.
2001. Limitations of co-training for natural language learning from large datasets.
In Proceedings of EMNLP-2001.
Ellen Riloff and Rosie Jones.
1999. Learning dictionaries for information extraction by multi-level bootstrapping.
In Proceedings of AAAI-99.
Fei Sha and Fernando Pereira.
2003. Shallow parsing with conditional random fields.
In Proceedings of HLT-NAACL'03.
David Yarowsky.
1995. Unsupervised word sense disambiguation rivaling supervised methods.
In Proceedings of ACL-95.
Tong Zhang and David E.
Johnson. 2003.
A robust risk minimization based named entity recognition system.
In Proceedings CoNLL-2003, pages 204207.
Tong Zhang, Fred Damerau, and David E.
Johnson. 2002.
Text chunking based on a generalization of Winnow.
Journal of Machine Learning Research, 2:615 637.
Tong Zhang.
2004. Solving large scale linear prediction problems using stochastic gradient descent algorithms.
In ICML 04, pages 919926 .
Scaling Conditional Random Fields Using Error-Correcting Codes Trevor Cohn Andrew Smith Department of Computer Science Division of Informatics and Software Engineering University of Edinburgh University of Melbourne, Australia United Kingdom tacohn@csse.unimelb.edu.au a.p.smith-2@sms.ed.ac.uk Miles Osborne Division of Informatics University of Edinburgh United Kingdom miles@inf.ed.ac.uk Abstract Conditional Random Fields (CRFs) have been applied with considerable success to a number of natural language processing tasks.
However, these tasks have mostly involved very small label sets.
When deployed on tasks with larger label sets, the requirements for computational resources mean that training becomes intractable.
This paper describes a method for training CRFs on such tasks, using error correcting output codes (ECOC).
A number of CRFs are independently trained on the separate binary labelling tasks of distinguishing between a subset of the labels and its complement.
During decoding, these models are combined to produce a predicted label sequence which is resilient to errors by individual models.
Error-correcting CRF training is much less resource intensive and has a much faster training time than a standardly formulated CRF, while decoding performance remains quite comparable.
This allows us to scale CRFs to previously impossible tasks, as demonstrated by our experiments with large label sets.
models that define a conditional distribution over label sequences given an observation sequence.
They allow the use of arbitrary, overlapping, non-independent features as a result of their global conditioning.
This allows us to avoid making unwarranted independence assumptions over the observation sequence, such as those required by typical generative models.
Efficient inference and training methods exist when the graphical structure of the model forms a chain, where each position in a sequence is connected to its adjacent positions.
CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003), simplified part-of-speech (POS) tagging (Lafferty et al., 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of tabular data (Pinto et al., 2003), among other tasks.
CRFs are usually estimated using gradient-based methods such as limited memory variable metric (LMVM).
However, even with these efficient methods, training can be slow.
Consequently, most of the tasks to which CRFs have been applied are relatively small scale, having only a small number of training examples and small label sets.
For much larger tasks, with hundreds of labels and millions of examples, current training methods prove intractable.
Although training can potentially be parallelised and thus run more quickly on large clusters of computers, this in itself is not a solution to the problem: tasks can reasonably be expected to increase in size and complexity much faster than any increase in computing power.
In order to provide scalability, the factors which most affect the resource usage and runtime of the training method Introduction Conditional random fields (CRFs) (Lafferty et al., 2001) are probabilistic models for labelling sequential data.
CRFs are undirected graphical Proceedings of the 43rd Annual Meeting of the ACL, pages 1017, Ann Arbor, June 2005.
c 2005 Association for Computational Linguistics fimust be addressed directly  ideally the dependence on the number of labels should be reduced.
This paper presents an approach which enables CRFs to be used on larger tasks, with a significant reduction in the time and resources needed for training.
This reduction does not come at the cost of performance  the results obtained on benchmark natural language problems compare favourably, and sometimes exceed, the results produced from regular CRF training.
Error correcting output codes (ECOC) (Dietterich and Bakiri, 1995) are used to train a community of CRFs on binary tasks, with each discriminating between a subset of the labels and its complement.
Inference is performed by applying these `weak' models to an unknown example, with each component model removing some ambiguity when predicting the label sequence.
Given a sufficient number of binary models predicting suitably diverse label subsets, the label sequence can be inferred while being robust to a number of individual errors from the weak models.
As each of these weak models are binary, individually they can be efficiently trained, even on large problems.
The number of weak learners required to achieve good performance is shown to be relatively small on practical tasks, such that the overall complexity of error-correcting CRF training is found to be much less than that of regular CRF training methods.
We have evaluated the error-correcting CRF on the CoNLL 2003 named entity recognition (NER) task (Sang and Meulder, 2003), where we show that the method yields similar generalisation performance to standardly formulated CRFs, while requiring only a fraction of the resources, and no increase in training time.
We have also shown how the errorcorrecting CRF scales when applied to the larger task of POS tagging the Penn Treebank and also the even larger task of simultaneously noun phrase chunking (NPC) and POS tagging using the CoNLL 2000 data-set (Sang and Buchholz, 2000).
model are connected by edges to form a linear chain.
The joint distribution of the label sequence, y, given the input observation sequence, x, is given by p(y|x) = k fk (t, yt-1, yt, x) where T is the length of both sequences and k are the parameters of the model.
The functions fk are feature functions which map properties of the observation and the labelling into a scalar value.
Z(x) is the partition function which ensures that p is a probability distribution.
A number of algorithms can be used to find the optimal parameter values by maximising the loglikelihood of the training data.
Assuming that the training sequences are drawn IID from the population, the conditional log likelihood L is given by L= T (i) +1 (i) (i) Conditional random fields CRFs are undirected graphical models used to specify the conditional probability of an assignment of output labels given a set of input observations.
We consider only the case where the output labels of the where x(i) and y(i) are the ith observation and label sequence.
Note that a prior is often included in the L formulation; it has been excluded here for clarity of exposition.
CRF estimation methods include generalised iterative scaling (GIS), improved iterative scaling (IIS) and a variety of gradient based methods.
In recent empirical studies on maximum entropy models and CRFs, limited memory variable metric (LMVM) has proven to be the most efficient method (Malouf, 2002; Wallach, 2002); accordingly, we have used LMVM for CRF estimation.
Every iteration of LMVM training requires the computation of the log-likelihood and its derivative with respect to each parameter.
The partition function Z(x) can be calculated efficiently using dynamic programming with the forward algorithm.
Z(x) is given by y T (y) where are the forward values, defined recursively as t+1 (y) = t (y ) exp The derivative of the log-likelihood is given by L k = Error Correcting Output Codes T (i) +1 (i) (i) The first term is the empirical count of feature k, and the second is the expected count of the feature under the model.
When the derivative equals zero  at convergence  these two terms are equal.
Evaluating the first term of the derivative is quite simple.
However, the sum over all possible labellings in the second term poses more difficulties.
This term can be factorised, yielding p(Yt-1 = y, Yt = y|x(i) )fk (t, y, y, x(i) ) This term uses the marginal distribution over pairs of labels, which can be efficiently computed from the forward and backward values as t-1 (y ) exp The backward probabilities are defined by the recursive relation t (y) = t+1 (y ) exp Typically CRF training using LMVM requires many hundreds or thousands of iterations, each of which involves calculating of the log-likelihood and its derivative.
The time complexity of a single iteration is O(L2 N T F ) where L is the number of labels, N is the number of sequences, T is the average length of the sequences, and F is the average number of activated features of each labelled clique.
It is not currently possible to state precise bounds on the number of iterations required for certain problems; however, problems with a large number of sequences often require many more iterations to converge than problems with fewer sequences.
Note that efficient CRF implementations cache the feature values for every possible clique labelling of the training data, which leads to a memory requirement with the same complexity of O(L2 N T F )  quite demanding even for current computer hardware.
Since the time and space complexity of CRF estimation is dominated by the square of the number of labels, it follows that reducing the number of labels will significantly reduce the complexity.
Error-correcting coding is an approach which recasts multiple label problems into a set of binary label problems, each of which is of lesser complexity than the full multiclass problem.
Interestingly, training a set of binary CRF classifiers is overall much more efficient than training a full multi-label model.
This is because error-correcting CRF training reduces the L2 complexity term to a constant.
Decoding proceeds by predicting these binary labels and then recovering the encoded actual label.
Error-correcting output codes have been used for text classification, as in Berger (1999), on which the following is based.
Begin by assigning to each of the m labels a unique n-bit string Ci, which we will call the code for this label.
Now train n binary classifiers, one for each column of the coding matrix (constructed by taking the labels' codes as rows).
The j th classifier, j, takes as positive instances those with label i where Cij = 1.
In this way, each classifier learns a different concept, discriminating between different subsets of the labels.
We denote the set of binary classifiers as = { 1, 2,. . ., n }, which can be used for prediction as follows.
Classify a novel instance x with each of the binary classifiers, yielding a n-bit vector (x) = { 1 (x), 2 (x), . . ., n (x)}.
Now compare this vector to the codes for each label.
The vector may not exactly match any of the labels due to errors in the individual classifiers, and thus we chose the actual label which minimises the distance argmini ((x), Ci ).
Typically the Hamming distance is used, which simply measures the number of differing bit positions.
In this manner, prediction is resilient to a number of prediction errors by the binary classifiers, provided the codes for the labels are sufficiently diverse.
3.1 Error-correcting CRF training Error-correcting codes can also be applied to sequence labellers, such as CRFs, which are capable of multiclass labelling.
ECOCs can be used with CRFs in a similar manner to that given above for ficlassifiers.
A series of CRFs are trained, each on a relabelled variant of the training data.
The relabelling for each binary CRF maps the labels into binary space using the relevant column of the coding matrix, such that label i is taken as a positive for the j th model example if Cij = 1.
Training with a binary label set reduces the time and space complexity for each training iteration to O(N T F ); the L2 term is now a constant.
Provided the code is relatively short (i.e.
there are few binary models, or weak learners), this translates into considerable time and space savings.
Coding theory doesn't offer any insights into the optimal code length (i.e.
the number of weak learners).
When using a very short code, the error-correcting CRF will not adequately model the decision boundaries between all classes.
However, using a long code will lead to a higher degree of dependency between pairs of classifiers, where both model similar concepts.
The generalisation performance should improve quickly as the number of weak learners (code length) increases, but these gains will diminish as the inter-classifier dependence increases.
3.2 Error-correcting CRF decoding While training of error-correcting CRFs is simply a logical extension of the ECOC classifier method to sequence labellers, decoding is a different matter.
We have applied three decoding different strategies.
The Standalone method requires each binary CRF to find the Viterbi path for a given sequence, yielding a string of 0s and 1s for each model.
For each position t in the sequence, the tth bit from each model is taken, and the resultant bit string compared to each of the label codes.
The label with the minimum Hamming distance is then chosen as the predicted label for that site.
This method allows for error correction to occur at each site, however it discards information about the uncertainty of each weak learner, instead only considering the most probable paths.
The Marginals method of decoding uses the marginal probability distribution at each position in the sequence instead of the Viterbi paths.
This distribution is easily computed using the forward backward algorithm.
The decoding proceeds as before, however instead of a bit string we have a vector of probabilities.
This vector is compared to each of the label codes using the L1 distance, and the closest label is chosen.
While this method incorporates the uncertainty of the binary models, it does so at the expense of the path information in the sequence.
Neither of these decoding methods allow the models to interact, although each individual weak learner may benefit from the predictions of the other weak learners.
The Product decoding method addresses this problem.
It treats each weak model as an independent predictor of the label sequence, such that the probability of the label sequence given the observations can be re-expressed as the product of the probabilities assigned by each weak model.
A given labelling y is projected into a bit string for each weak learner, such that the ith entry in the string is Ckj for the j th weak learner, where k is the index of label yi . The weak learners can then estimate the probability of the bit string; these are then combined into a global product to give the probability of the label sequence p(y|x) = 1 Z (x) pj (bj (y)|x) where pj (q|x) is the predicted probability of q given x by the j th weak learner, bj (y) is the bit string representing y for the j th weak learner and Z (x) is the partition function.
The log probability is {Fj (bj (y), x)  j log Zj (x)} log Z (x) where Fj (y, x) = T +1 fj (t, yt-1, yt, x).
This log t=1 probability can then be maximised using the Viterbi algorithm as before, noting that the two log terms are constant with respect to y and thus need not be evaluated.
Note that this decoding is an equivalent formulation to a uniformly weighted logarithmic opinion pool, as described in Smith et al.(2005). Of the three decoding methods, Standalone has the lowest complexity, requiring only a binary Viterbi decoding for each weak learner.
Marginals is slightly more complex, requiring the forward and backward values.
Product, however, requires Viterbi decoding with the full label set, and many features  the union of the features of each weak learner  which can be quite computationally demanding.
fi3.3 Choice of code The accuracy of ECOC methods are highly dependent on the quality of the code.
The ideal code has diverse rows, yielding a high error-correcting capability, and diverse columns such that the weak learners model highly independent concepts.
When the number of labels, k, is small, an exhaustive code with every unique column is reasonable, given there are 2k-1 1 unique columns.
With larger label sets, columns must be selected with care to maximise the inter-row and inter-column separation.
This can be done by randomly sampling the column space, in which case the probability of poor separation diminishes quickly as the number of columns increases (Berger, 1999).
Algebraic codes, such as BCH codes, are an alternative coding scheme which can provide near-optimal error-correcting capability (MacWilliams and Sloane, 1977), however these codes provide no guarantee of good column separation.
Model Decoding Multiclass Coded standalone marginals product MLE 88.04 88.23 88.23 88.69 Regularised 89.78 88.67 89.19 89.69 Table 1: F1 scores on NER task.
trained without regularisation and with a Gaussian prior.
An exhaustive code was created with all 127 unique columns.
All of the weak learners were trained with the same feature set, each having around 315,000 features.
The performance of the standard and error-correcting models are shown in Table 1.
We tested for statistical significance using the matched pairs test (Gillick and Cox, 1989) at p < 0.001.
Those results which are significantly better than the corresponding multiclass MLE or regularised model are flagged with a, and those which are significantly worse with a . These results show that error-correcting CRF training achieves quite similar performance to the multiclass CRF on the task (which incidentally exceeds McCallum (2003)'s result of 89.0 using feature induction).
Product decoding was the better of the three methods, giving the best performance both with and without regularisation, although this difference was only statistically significant between the regularised standalone and the regularised product decoding.
The unregularised error-correcting CRF significantly outperformed the multiclass CRF with all decoding strategies, suggesting that the method already provides some regularisation, or corrects some inherent bias in the model.
Using such a large number of weak learners is costly, in this case taking roughly ten times longer to train than the multiclass CRF.
However, much shorter codes can also achieve similar results.
The simplest code, where each weak learner predicts only a single label (a.k.a.
one-vs-all), achieved an F score of 89.56, while only requiring 8 weak learners and less than half the training time as the multiclass CRF.
This code has no error correcting capability, suggesting that the code's column separation (and thus interdependence between weak learners) is more important than its row separation.
Experiments Our experiments show that error-correcting CRFs are highly accurate on benchmark problems with small label sets, as well as on larger problems with many more labels, which would be otherwise prove intractable for traditional CRFs.
Moreover, with a good code, the time and resources required for training and decoding can be much less than that of the standardly formulated CRF.
4.1 Named
entity recognition CRFs have been used with strong results on the CoNLL 2003 NER task (McCallum, 2003) and thus this task is included here as a benchmark.
This data set consists of a 14,987 training sentences (204,567 tokens) drawn from news articles, tagged for person, location, organisation and miscellaneous entities.
There are 8 IOB-2 style labels.
A multiclass (standardly formulated) CRF was trained on these data using features covering word identity, word prefix and suffix, orthographic tests for digits, case and internal punctuation, word length, POS tag and POS tag bigrams before and after the current word.
Only features seen at least once in the training data were included in the model, resulting in 450,345 binary features.
The model was fiAn exhaustive code was used in this experiment simply for illustrative purposes: many columns in this code were unnecessary, yielding only a slight gain in performance over much simpler codes while incurring a very large increase in training time.
Therefore, by selecting a good subset of the exhaustive code, it should be possible to reduce the training time while preserving the strong generalisation performance.
One approach is to incorporate skew in the label distribution in our choice of code  the code should minimise the confusability of commonly occurring labels more so than that of rare labels.
Assuming that errors made by the weak learners are independent, the probability of a single error, q, as a function of the code length n can be bounded by 90 89 88 F1 score 87 86 85 84 83 random random with replacement minimum loss bound oracle MLE multiclass CRF Regularised multiclass CRF 10 15 20 25 30 code length 35 40 45 50 Figure 1: NER F1 scores for standalone decoding with random codes, a minimum loss code and a greedy oracle.
Coding Multiclass Coded 200 One-vs-all Decoding standalone marginals product MLE 95.69 95.63 95.68 94.90 Regularised 95.78 96.03 96.03 96.57 where p(l) is the marginal probability of the label l, hl is the minimum Hamming distance between l and any other label, and p is the maximum probability ^ of an error by a weak learner.
The performance achieved by selecting the code with the minimum loss bound from a large random sample of codes is shown in Figure 1, using standalone decoding, where p was estimated on the development set.
For ^ comparison, randomly sampled codes and a greedy oracle are shown.
The two random sampled codes show those samples where no column is repeated, and where duplicate columns are permitted (random with replacement).
The oracle repeatedly adds to the code the column which most improves its F1 score.
The minimum loss bound method allows the performance plateau to be reached more quickly than random sampling; i.e. shorter codes can be used, thus allowing more efficient training and decoding.
Note also that multiclass CRF training required 830Mb of memory, while error-correcting training required only 380Mb.
Decoding of the test set (51,362 tokens) with the error-correcting model (exhaustive, MLE) took between 150 seconds for standalone decoding and 173 seconds for integrated decoding.
The multiclass CRF was much faster, taking only 31 seconds, however this time difference could be reduced with suitable optimisations.
Table 2: POS tagging accuracy.
4.2 Part-of-speech Tagging CRFs have been applied to POS tagging, however only with a very simple feature set and small training sample (Lafferty et al., 2001).
We used the Penn Treebank Wall Street Journal articles, training on sections 221 and testing on section 24.
In this task there are 45,110 training sentences, a total of 1,023,863 tokens and 45 labels.
The features used included word identity, prefix and suffix, whether the word contains a number, uppercase letter or a hyphen, and the words one and two positions before and after the current word.
A random code of 200 columns was used for this task.
These results are shown in Table 2, along with those of a multiclass CRF and an alternative one-vsall coding.
As for the NER experiment, the decoding performance levelled off after 100 bits, beyond which the improvements from longer codes were only very slight.
This is a very encouraging characteristic, as only a small number of weak learners are required for good performance.
The random code of 200 bits required 1,300Mb of RAM, taking a total of 293 hours to train and 3 hours to decode (54,397 tokens) on similar machines to those used before.
We do not have figures regarding the resources used by Lafferty et al.'s CRF for the POS tagging task and our attempts to train a multiclass CRF for full-scale POS tagging were thwarted due to lack of sufficient available computing resources.
Instead we trained on a 10,000 sentence subset of the training data, which required approximately 17Gb of RAM and 208 hours to train.
Our best result on the task was achieved using a one-vs-all code, which reduced the training time to 25 hours, as it only required training 45 binary models.
This result exceeds Lafferty et al.'s accuracy of 95.73% using a CRF but falls short of Toutanova et al.(2003)'s state-of-the-art 97.24%.
This is most probably due to our only using a first-order Markov model and a fairly simple feature set, where Tuotanova et al.include a richer set of features in a third order model.
4.3 Part-of-speech Tagging and Noun Phrase Segmentation The joint task of simultaneously POS tagging and noun phrase chunking (NPC) was included in order to demonstrate the scalability of error-correcting CRFs.
The data was taken from the CoNLL 2000 NPC shared task, with the model predicting both the chunk tags and the POS tags.
The training corpus consisted of 8,936 sentences, with 47,377 tokens and 118 labels.
A 200-bit random code was used, with the following features: word identity within a window, prefix and suffix of the current word and the presence of a digit, hyphen or upper case letter in the current word.
This resulted in about 420,000 features for each weak learner.
A joint tagging accuracy of 90.78% was achieved using MLE training and standalone decoding.
Despite the large increase in the number of labels in comparison to the earlier tasks, the performance also began to plateau at around 100 bits.
This task required 220Mb of RAM and took a total of 30 minutes to train each of the 200 binary CRFs, this time on Pentium 4 machines with 1Gb RAM.
Decoding of the 47,377 test tokens took 9,748 seconds and 9,870 seconds for the standalone and marginals methods respectively.
Sutton et al.(2004) applied a variant of the CRF, the dynamic CRF (DCRF), to the same task, modelling the data with two interconnected chains where one chain predicted NPC tags and the other POS tags.
They achieved better performance and training times than our model; however, this is not a fair comparison, as the two approaches are orthogonal.
Indeed, applying the error-correcting CRF algorithms to DCRF models could feasibly decrease the complexity of the DCRF, allowing the method to be applied to larger tasks with richer graphical structures and larger label sets.
In all three experiments, error-correcting CRFs have achieved consistently good generalisation performance.
The number of weak learners required to achieve these results was shown to be relatively small, even for tasks with large label sets.
The time and space requirements were lower than those of a traditional CRF for the larger tasks and, most importantly, did not increase substantially when the number of labels was increased.
Related work Most recent work on improving CRF performance has focused on feature selection.
McCallum (2003) describes a technique for greedily adding those feature conjuncts to a CRF which significantly improve the model's log-likelihood.
His experimental results show that feature induction yields a large increase in performance, however our results show that standardly formulated CRFs can perform well above their reported 73.3%, casting doubt on the magnitude of the possible improvement.
Roark et al.(2004) have also employed feature selection to the huge task of language modelling with a CRF, by partially training a voted perceptron then removing all features that the are ignored by the perceptron.
The act of automatic feature selection can be quite time consuming in itself, while the performance and runtime gains are often modest.
Even with a reduced number of features, tasks with a very large label space are likely to remain intractable.
Conclusion Standard training methods for CRFs suffer greatly from their dependency on the number of labels, making tasks with large label sets either difficult or impossible.
As CRFs are deployed more widely to tasks with larger label sets this problem will become more evident.
The current `solutions' to these scaling problems  namely feature selection, and the use of large clusters  don't address the heart of the problem: the dependence on the square of number of labels.
Error-correcting CRF training allows CRFs to be applied to larger problems and those with larger label sets than were previously possible, without requiring computationally demanding methods such as feature selection.
On standard tasks we have shown that error-correcting CRFs provide comparable or better performance than the standardly formulated CRF, while requiring less time and space to train.
Only a small number of weak learners were required to obtain good performance on the tasks with large label sets, demonstrating that the method provides efficient scalability to the CRF framework.
Error-correction codes could be applied to other sequence labelling methods, such as the voted perceptron (Roark et al., 2004).
This may yield an increase in performance and efficiency of the method, as its runtime is also heavily dependent on the number of labels.
We plan to apply error-correcting coding to dynamic CRFs, which should result in better modelling of naturally layered tasks, while increasing the efficiency and scalability of the method.
We also plan to develop higher order CRFs, using error-correcting codes to curb the increase in complexity.
Acknowledgements This work was supported in part by a PORES travelling scholarship from the University of Melbourne, allowing Trevor Cohn to travel to Edinburgh.
References Adam Berger.
1999. Error-correcting output coding for text classification.
In Proceedings of IJCAI: Workshop on machine learning for information filtering.
Thomas G.
Dietterich and Ghulum Bakiri.
1995. Solving multiclass learning problems via error-correcting output codes.
Journal of Artificial Intelligence Reseach, 2:263286.
L. Gillick and Stephen Cox.
1989. Some statistical issues in the comparison of speech recognition algorithms.
In Proceedings of the IEEE Conference on Acoustics, Speech and Signal Processing, pages 532535, Glasgow, Scotland.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models for segmenting and labelling sequence data.
In Proceedings of ICML 2001, pages 282289.
Florence MacWilliams and Neil Sloane.
1977. The theory of error-correcting codes.
North Holland, Amsterdam.
Robert Malouf.
2002. A comparison of algorithms for maximum entropy parameter estimation.
In Proceedings of CoNLL 2002, pages 4955.
Andrew McCallum and Wei Li.
2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.
In Proceedings of CoNLL 2003, pages 188191.
Andrew McCallum.
2003. Efficiently inducing features of conditional random fields.
In Proceedings of UAI 2003, pages 403410.
David Pinto, Andrew McCallum, Xing Wei, and Bruce Croft.
2003. Table extraction using conditional random fields.
In Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 235242.
Brian Roark, Murat Saraclar, Michael Collins, and Mark Johnson.
2004. Discriminative language modeling with conditional random fields and the perceptron algorithm.
In Proceedings of ACL 2004, pages 4855.
Erik F.
Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task: Chunking.
In Proceedings of CoNLL 2000 and LLL 2000, pages 127132.
Erik F.
Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In Proceedings of CoNLL 2003, pages 142147, Edmonton, Canada.
Fei Sha and Fernando Pereira.
2003. Shallow parsing with conditional random fields.
In Proceedings of HLT-NAACL 2003, pages 213220.
Andrew Smith, Trevor Cohn, and Miles Osborne.
2005. Logarithmic opinion pools for conditional random fields.
In Proceedings of ACL 2005.
Charles Sutton, Khashayar Rohanimanesh, and Andrew McCallum.
2004. Dynamic conditional random fields: Factorized probabilistic models for labelling and segmenting sequence data.
In Proceedings of the ICML 2004.
Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer.
2003. Feature rich part-of-speech tagging with a cyclic dependency network.
In Proceedings of HLTNAACL 2003, pages 252259.
Hanna Wallach.
2002. Efficient training of conditional random fields.
Master's thesis, University of Edinburgh .
Logarithmic Opinion Pools for Conditional Random Fields Andrew Smith Trevor Cohn Miles Osborne Division of Informatics Department of Computer Science Division of Informatics University of Edinburgh and Software Engineering University of Edinburgh United Kingdom University of Melbourne, Australia United Kingdom a.p.smith-2@sms.ed.ac.uk tacohn@csse.unimelb.edu.au miles@inf.ed.ac.uk Abstract Recent work on Conditional Random Fields (CRFs) has demonstrated the need for regularisation to counter the tendency of these models to overfit.
The standard approach to regularising CRFs involves a prior distribution over the model parameters, typically requiring search over a hyperparameter space.
In this paper we address the overfitting problem from a different perspective, by factoring the CRF distribution into a weighted product of individual "expert" CRF distributions.
We call this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs).
We apply the LOP-CRF to two sequencing tasks.
Our results show that unregularised expert CRFs with an unregularised CRF under a LOP can outperform the unregularised CRF, and attain a performance level close to the regularised CRF.
LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyperparameter search.
1 Introduction
In recent years, conditional random fields (CRFs) (Lafferty et al., 2001) have shown success on a number of natural language processing (NLP) tasks, including shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003) and information extraction from research papers (Peng and McCallum, 2004).
In general, this work has demonstrated the susceptibility of CRFs to overfit the training data during parameter estimation.
As a consequence, it is now standard to use some form of overfitting reduction in CRF training.
Recently, there have been a number of sophisticated approaches to reducing overfitting in CRFs, including automatic feature induction (McCallum, 2003) and a full Bayesian approach to training and inference (Qi et al., 2005).
These advanced methods tend to be difficult to implement and are often computationally expensive.
Consequently, due to its ease of implementation, the current standard approach to reducing overfitting in CRFs is the use of a prior distribution over the model parameters, typically a Gaussian.
The disadvantage with this method, however, is that it requires adjusting the value of one or more of the distribution's hyperparameters.
This usually involves manual or automatic tuning on a development set, and can be an expensive process as the CRF must be retrained many times for different hyperparameter values.
In this paper we address the overfitting problem in CRFs from a different perspective.
We factor the CRF distribution into a weighted product of individual expert CRF distributions, each focusing on a particular subset of the distribution.
We call this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs), and provide a procedure for learning the weight of each expert in the product.
The LOPCRF framework is "parameter-free" in the sense that it does not involve the requirement to adjust hyperparameter values.
LOP-CRFs are theoretically advantageous in that their Kullback-Leibler divergence with a given distribution can be explicitly represented as a function of the KL-divergence with each of their expert distributions.
This provides a well-founded framework for designing new overfitting reduction schemes: Proceedings of the 43rd Annual Meeting of the ACL, pages 1825, Ann Arbor, June 2005.
c 2005 Association for Computational Linguistics filook to factorise a CRF distribution as a set of diverse experts.
We apply LOP-CRFs to two sequencing tasks in NLP: named entity recognition and part-of-speech tagging.
Our results show that combination of unregularised expert CRFs with an unregularised standard CRF under a LOP can outperform the unregularised standard CRF, and attain a performance level that rivals that of the regularised standard CRF.
LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyperparameter search.
E p(o,s) [ fk ] E p(s|o) [ fk ] = 0, k ~ In general this cannot be solved for the k in closed form so numerical routines must be used.
Malouf (2002) and Sha and Pereira (2003) show that gradient-based algorithms, particularly limited memory variable metric (LMVM), require much less time to reach convergence, for some NLP tasks, than the iterative scaling methods (Della Pietra et al., 1997) previously used for log-linear optimisation problems.
In all our experiments we use the LMVM method to train the CRFs.
For CRFs with general graphical structure, calculation of E p(s|o) [ fk ] is intractable, but for the linear chain case Lafferty et al.(2001) describe an efficient dynamic programming procedure for inference, similar in nature to the forward-backward algorithm in hidden Markov models.
Conditional Random Fields A linear chain CRF defines the conditional probability of a state or label sequence s given an observed sequence o via1 : 1 exp Z(o) Logarithmic Opinion Pools where T is the length of both sequences, k are parameters of the model and Z(o) is the partition function that ensures (1) represents a probability distribution.
The functions f k are feature functions representing the occurrence of different events in the sequences s and o.
The parameters k can be estimated by maximising the conditional log-likelihood of a set of labelled training sequences.
The log-likelihood is given by: L ( ) = = ~ p(o, s) log p(s | o; ) In this paper an expert model refers a probabilistic model that focuses on modelling a specific subset of some probability distribution.
The concept of combining the distributions of a set of expert models via a weighted product has previously been used in a range of different application areas, including economics and management science (Bordley, 1982), and NLP (Osborne and Baldridge, 2004).
In this paper we restrict ourselves to sequence models.
Given a set of sequence model experts, indexed by, with conditional distributions p (s | o) and a set of non-negative normalised weights w, a logarithmic opinion pool 2 is defined as the distribution: pLOP (s | o) = 1 [p (s | o)]w ZLOP (o) (2) where p(o, s) and p(o) are empirical distributions ~ ~ defined by the training set.
At the maximum likelihood solution the model satisfies a set of feature constraints, whereby the expected count of each feature under the model is equal to its empirical count on the training data: this paper we assume there is a one-to-one mapping between states and labels, though this need not be the case.
with w 0 and w = 1, and where ZLOP (o) is the normalisation constant: ZLOP (o) = [p (s | o)]w (1999) introduced a variant of the LOP idea called Product of Experts, in which expert distributions are multiplied under a uniform weight distribution.
2 Hinton
The weight w encodes our confidence in the opinion of expert. Suppose that there is a "true" conditional distribution q(s | o) which each p (s | o) is attempting to model.
Heskes (1998) shows that the KL divergence between q(s | o) and the LOP, can be decomposed into two terms: K(q, pLOP ) = E A = log Z(o) = log ZLOP (o) + w log Z (o) LOP This relationship will be useful below, when we describe how to train the weights w of a LOP-CRF.
In this paper we will use the term LOP-CRF weights to refer to the weights w in the weighted product of the LOP-CRF distribution and the term parameters to refer to the parameters k of each expert CRF . 3.2 Training LOP-CRFs In our LOP-CRF training procedure we first train the expert CRFs unregularised on the training data.
Then, treating the experts as static pre-trained models, we train the LOP-CRF weights w to maximise the log-likelihood of the training data.
This training process is "parameter-free" in that neither stage involves the use of a prior distribution over expert CRF parameters or LOP-CRF weights, and so avoids the requirement to adjust hyperparameter values.
The likelihood of a data set under a LOP-CRF, as a function of the LOP-CRF weights, is given by: L(w) = = This tells us that the closeness of the LOP model to q(s | o) is governed by a trade-off between two terms: an E term, which represents the closeness of the individual experts to q(s | o), and an A term, which represents the closeness of the individual experts to the LOP, and therefore indirectly to each other.
Hence for the LOP to model q well, we desire models p which are individually good models of q (having low E) and are also diverse (having large A).
3.1 LOPs
for CRFs Because CRFs are log-linear models, we can see from equation (2) that CRF experts are particularly well suited to combination under a LOP.
Indeed, the resulting LOP is itself a CRF, the LOP-CRF, with potential functions given by a log-linear combination of the potential functions of the experts, with weights w . As a consequence of this, the normalisation constant for the LOP-CRF can be calculated efficiently via the usual forward-backward algorithm for CRFs.
Note that there is a distinction between normalisation constant for the LOP-CRF, ZLOP as given in equation (3), and the partition function of the LOP-CRF, Z.
The two are related as follows: 1 [p (s | o)]w ZLOP (o) 1 U (s | o) Z (o) ZLOP (o) [U (s | o)] ZLOP (o) [Z (o)]w LOP 1 p (s | o)w ZLOP (o; w) After taking logs and rearranging, the loglikelihood can be expressed as: ~ p(o, s) w log p (s | o) ~ p(o) log Z LOP (o; w) T +1 where U = exp t=1 k k fk (st-1, st, o,t) and so For the first two terms, the quantities that are multiplied by w inside the (outer) sums are independent of the weights, and can be evaluated once at the fibeginning of training.
The third term involves the partition function for the LOP-CRF and so is a function of the weights.
It can be evaluated efficiently as usual for a standard CRF.
Taking derivatives with respect to w and rearranging, we obtain: use a prior distribution over the LOP-CRF weights.
As the weights are non-negative and normalised we use a Dirichlet distribution, whose density function is given by: p(w) = ( ) w -1 ( ) L (w) w where the are hyperparameters.
Under this distribution, ignoring terms that are independent of the weights, the regularised loglikelihood involves an additional term: logUt (o, s) where Ut (o, s) is the value of the potential function for expert on clique t under the labelling s for observation o.
In a way similar to the representation of the expected feature count in a standard CRF, the third term may be re-written as: pLOP (st-1 = s, st = s, o) logUt (s, s, o) o t s,s We assume a single value across all weights.
The derivative of the regularised log-likelihood with respect to weight w then involves an additional 1 term w ( 1).
In our experiments we use the development set to optimise the value of . We will refer to a LOP-CRF with weights trained using this procedure as a regularised LOP-CRF.
Hence the derivative is tractable because we can use dynamic programming to efficiently calculate the pairwise marginal distribution for the LOP-CRF.
Using these expressions we can efficiently train the LOP-CRF weights to maximise the loglikelihood of the data set.3 We make use of the LMVM method mentioned earlier to do this.
We will refer to a LOP-CRF with weights trained using this procedure as an unregularised LOP-CRF.
3.2.1 Regularisation
The "parameter-free" aspect of the training procedure we introduced in the previous section relies on the fact that we do not use regularisation when training the LOP-CRF weights w . However, there is a possibility that this may lead to overfitting of the training data.
In order to investigate this, we develop a regularised version of the training procedure and compare the results obtained with each.
We 3 We must ensure that the weights are non-negative and normalised.
We achieve this by parameterising the weights as functions of a set of unconstrained variables via a softmax transformation.
The values of the log-likelihood and its derivatives with respect to the unconstrained variables can be derived from the corresponding values for the weights w . The Tasks In this paper we apply LOP-CRFs to two sequence labelling tasks in NLP: named entity recognition (NER) and part-of-speech tagging (POS tagging).
4.1 Named
Entity Recognition NER involves the identification of the location and type of pre-defined entities within a sentence and is often used as a sub-process in information extraction systems.
With NER the CRF is presented with a set of sentences and must label each word so as to indicate whether the word appears outside an entity (O), at the beginning of an entity of type X (B-X) or within the continuation of an entity of type X (I-X).
All our results for NER are reported on the CoNLL-2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003).
For this dataset the entity types are: persons (PER), locations (LOC), organisations (ORG) and miscellaneous (MISC).
The training set consists of 14, 987 sentences and 204, 567 tokens, the development set consists of 3, 466 sentences and 51, 578 tokens and the test set consists of 3, 684 sentences and 46, 666 tokens.
fi4.2 Part-of-Speech Tagging POS tagging involves labelling each word in a sentence with its part-of-speech, for example noun, verb, adjective, etc.
For our experiments we use the CoNLL-2000 shared task dataset (Tjong Kim Sang and Buchholz, 2000).
This has 48 different POS tags.
In order to make training time manageable4, we collapse the number of POS tags from 48 to 5 following the procedure used in (McCallum et al., 2003).
In summary:  All types of noun collapse to category N.
 All types of verb collapse to category V.
 All types of adjective collapse to category J.
 All types of adverb collapse to category R.
 All other POS tags collapse to category O.
The training set consists of 7, 300 sentences and 173, 542 tokens, the development set consists of 1, 636 sentences and 38, 185 tokens and the test set consists of 2, 012 sentences and 47, 377 tokens.
4.3 Expert
sets For each task we compare the performance of the LOP-CRF to that of the standard CRF by defining a single, complex CRF, which we call a monolithic CRF, and a range of expert sets.
The monolithic CRF for NER comprises a number of word and POS tag features in a window of five words around the current word, along with a set of orthographic features defined on the current word.
These are based on those found in (Curran and Clark, 2003).
Examples include whether the current word is capitalised, is an initial, contains a digit, contains punctuation, etc.
The monolithic CRF for NER has 450, 345 features.
The monolithic CRF for POS tagging comprises word and POS features similar to those in the NER monolithic model, but over a smaller number of orthographic features.
The monolithic model for POS tagging has 188, 448 features.
Each of our expert sets consists of a number of CRF experts.
Usually these experts are designed to 4 See (Cohn et al., 2005) for a scaling method allowing the full POS tagging task with CRFs.
focus on modelling a particular aspect or subset of the distribution.
As we saw earlier, the aim here is to define experts that model parts of the distribution well while retaining mutual diversity.
The experts from a particular expert set are combined under a LOP-CRF and the weights are trained as described previously.
We define our range of expert sets as follows:  Simple consists of the monolithic CRF and a single expert comprising a reduced subset of the features in the monolithic CRF.
This reduced CRF models the entire distribution rather than focusing on a particular aspect or subset, but is much less expressive than the monolithic model.
The reduced model comprises 24, 818 features for NER and 47, 420 features for POS tagging.
 Positional consists of the monolithic CRF and a partition of the features in the monolithic CRF into three experts, each consisting only of features that involve events either behind, at or ahead of the current sequence position.
 Label consists of the monolithic CRF and a partition of the features in the monolithic CRF into five experts, one for each label.
For NER an expert corresponding to label X consists only of features that involve labels B-X or IX at the current or previous positions, while for POS tagging an expert corresponding to label X consists only of features that involve label X at the current or previous positions.
These experts therefore focus on trying to model the distribution of a particular label.
 Random consists of the monolithic CRF and a random partition of the features in the monolithic CRF into four experts.
This acts as a baseline to ascertain the performance that can be expected from an expert set that is not defined via any linguistic intuition.
Experiments To compare the performance of LOP-CRFs trained using the procedure we described previously to that of a standard CRF regularised with a Gaussian prior, we do the following for both NER and POS tagging: fi Train a monolithic CRF with regularisation using a Gaussian prior.
We use the development set to optimise the value of the variance hyperparameter.
 Train every expert CRF in each expert set without regularisation (each expert set includes the monolithic CRF, which clearly need only be trained once).
 For each expert set, create a LOP-CRF from the expert CRFs and train the weights of the LOP-CRF without regularisation.
We compare its performance to that of the unregularised and regularised monolithic CRFs.
 To investigate whether training the LOP-CRF weights contributes significantly to the LOPCRF's performance, for each expert set we create a LOP-CRF with uniform weights and compare its performance to that of the LOP-CRF with trained weights.
 To investigate whether unregularised training of the LOP-CRF weights leads to overfitting, for each expert set we train the weights of the LOP-CRF with regularisation using a Dirichlet prior.
We optimise the hyperparameter in the Dirichlet distribution on the development set.
We then compare the performance of the LOP-CRF with regularised weights to that of the LOP-CRF with unregularised weights.
Expert Monolithic unreg.
Monolithic reg.
Reduced Positional 1 Positional 2 Positional 3 Label LOC Label MISC Label ORG Label PER Label O Random 1 Random 2 Random 3 Random 4 Table 1: Development set F scores for NER experts 6.2 LOP-CRFs with unregularised weights In this section we present results for LOP-CRFs with unregularised weights.
Table 2 gives F scores for NER LOP-CRFs while Table 3 gives accuracies for the POS tagging LOP-CRFs.
The monolithic CRF scores are included for comparison.
Both tables illustrate the following points:  In every case the LOP-CRFs outperform the unregularised monolithic CRF  In most cases the performance of LOP-CRFs rivals that of the regularised monolithic CRF, and in some cases exceeds it.
We use McNemar's matched-pairs test (Gillick and Cox, 1989) on point-wise labelling errors to examine the statistical significance of these results.
We test significance at the 5% level.
At this threshold, all the LOP-CRFs significantly outperform the corresponding unregularised monolithic CRF.
In addition, those marked with show a significant improvement over the regularised monolithic CRF.
Only the value marked with in Table 3 significantly under performs the regularised monolithic.
All other values a do not differ significantly from those of the regularised monolithic CRF at the 5% level.
These results show that LOP-CRFs with unregularised weights can lead to performance improvements that equal or exceed those achieved from a conventional regularisation approach using a Gaussian prior.
The important difference, however, is that the LOP-CRF approach is "parameter-free" in the Results 6.1 Experts Before presenting results for the LOP-CRFs, we briefly give performance figures for the monolithic CRFs and expert CRFs in isolation.
For illustration, we do this for NER models only.
Table 1 shows F scores on the development set for the NER CRFs.
We see that, as expected, the expert CRFs in isolation model the data relatively poorly compared to the monolithic CRFs.
Some of the label experts, for example, attain relatively low F scores as they focus only on modelling one particular label.
Similar behaviour was observed for the POS tagging models.
Expert set Monolithic unreg.
Monolithic reg.
Simple Positional Label Random Development set 88.33 89.84 90.26 90.35 89.30 88.84 Test set 81.87 83.98 84.22 84.71 83.27 83.06 Expert set Simple Positional Label Random Development set 98.30 97.97 97.85 97.82 Test set 98.12 97.79 97.73 97.74 Table 2: F scores for NER unregularised LOP-CRFs Expert set Monolithic unreg.
Monolithic reg.
Simple Positional Label Random Development set 97.92 98.02 98.31 98.03 97.99 97.99 Test set 97.65 97.84 98.12 97.81 97.77 97.76 Table 4: Accuracies for POS tagging uniform LOPCRFs general LOP-CRFs with uniform weights, although still performing significantly better than the unregularised monolithic CRF, generally under perform LOP-CRFs with trained weights.
This suggests that the choice of weights can be important, and justifies the weight training stage.
6.4 LOP-CRFs with regularised weights To investigate whether unregularised training of the LOP-CRF weights leads to overfitting, we train the LOP-CRF with regularisation using a Dirichlet prior.
The results we obtain show that in most cases a LOP-CRF with regularised weights achieves an almost identical performance to that with unregularised weights, and suggests there is little to be gained by weight regularisation.
This is probably due to the fact that in our LOP-CRFs the number of experts, and therefore weights, is generally small and so there is little capacity for overfitting.
We conjecture that although other choices of expert set may comprise many more experts than in our examples, the numbers are likely to be relatively small in comparison to, for example, the number of parameters in the individual experts.
We therefore suggest that any overfitting effect is likely to be limited.
6.5 Choice
of Expert Sets We can see from Tables 2 and 3 that the performance of a LOP-CRF varies with the choice of expert set.
For example, in our tasks the simple and positional expert sets perform better than those for the label and random sets.
For an explanation here, we refer back to our discussion of equation (5).
We conjecture that the simple and positional expert sets achieve good performance in the LOP-CRF because they consist of experts that are diverse while simultaneously being reasonable models of the data.
The label expert set exhibits greater diversity between the experts, because each expert focuses on modelling a particular label only, but each expert is a relatively Table 3: Accuracies for POS tagging unregularised LOP-CRFs sense that each expert CRF in the LOP-CRF is unregularised and the LOP weight training is also unregularised.
We are therefore not required to search a hyperparameter space.
As an illustration, to obtain our best results for the POS tagging regularised monolithic model, we re-trained using 15 different values of the Gaussian prior variance.
With the LOP-CRF we trained each expert CRF and the LOP weights only once.
As an illustration of a typical weight distribution resulting from the training procedure, the positional LOP-CRF for POS tagging attaches weight 0.45 to the monolithic model and roughly equal weights to the other three experts.
6.3 LOP-CRFs with uniform weights By training LOP-CRF weights using the procedure we introduce in this paper, we allow the weights to take on non-uniform values.
This corresponds to letting the opinion of some experts take precedence over others in the LOP-CRF's decision making.
An alternative, simpler, approach would be to combine the experts under a LOP with uniform weights, thereby avoiding the weight training stage.
We would like to ascertain whether this approach will significantly reduce the LOP-CRF's performance.
As an illustration, Table 4 gives accuracies for LOPCRFs with uniform weights for POS tagging.
A similar pattern is observed for NER.
Comparing these values to those in Tables 2 and 3, we can see that in fipoor model of the entire distribution and the corresponding LOP-CRF performs worse.
Similarly, the random experts are in general better models of the entire distribution but tend to be less diverse because they do not focus on any one aspect or subset of it.
Intuitively, then, we want to devise experts that provide diverse but accurate views on the data.
The expert sets we present in this paper were motivated by linguistic intuition, but clearly many choices exist.
It remains an important open question as to how to automatically construct expert sets for good performance on a given task, and we intend to pursue this avenue in future research.
Conclusion and future work In this paper we have introduced the logarithmic opinion pool of CRFs as a way to address overfitting in CRF models.
Our results show that a LOPCRF can provide a competitive alternative to conventional regularisation with a prior while avoiding the requirement to search a hyperparameter space.
We have seen that, for a variety of types of expert, combination of expert CRFs with an unregularised standard CRF under a LOP with optimised weights can outperform the unregularised standard CRF and rival the performance of a regularised standard CRF.
We have shown how these advantages a LOPCRF provides have a firm theoretical foundation in terms of the decomposition of the KL-divergence between a LOP-CRF and a target distribution, and how this provides a framework for designing new overfitting reduction schemes in terms of constructing diverse experts.
In this work we have considered training the weights of a LOP-CRF using pre-trained, static experts.
In future we intend to investigate cooperative training of LOP-CRF weights and the parameters of each expert in an expert set.
Acknowledgements We wish to thank Stephen Clark, our colleagues in Edinburgh and the anonymous reviewers for many useful comments.
References R.
F. Bordley.
1982. A multiplicative formula for aggregating probability assessments.
Management Science, (28):1137 1148.
T. Cohn, A.
Smith, and M.
Osborne. 2005.
Scaling conditional random fields using error-correcting codes.
In Proc.
ACL 2005.
J. Curran and S.
Clark. 2003.
Language independent NER using a maximum entropy tagger.
In Proc.
CoNLL-2003. S.
Della Pietra, Della Pietra V., and J.
Lafferty. 1997.
Inducing features of random fields.
In IEEE PAMI, volume 19(4), pages 380393.
L. Gillick and S.
Cox. 1989.
Some statistical issues in the comparison of speech recognition algorithms.
In International Conference on Acoustics, Speech and Signal Processing, volume 1, pages 532535.
T. Heskes.
1998. Selecting weighting factors in logarithmic opinion pools.
In Advances in Neural Information Processing Systems 10.
G. E.
Hinton. 1999.
Product of experts.
In ICANN, volume 1, pages 16.
J. Lafferty, A.
McCallum, and F.
Pereira. 2001.
Conditional random fields: Probabilistic models for segmenting and labeling sequence data.
In Proc.
ICML 2001.
R. Malouf.
2002. A comparison of algorithms for maximum entropy parameter estimation.
In Proc.
CoNLL-2002. A.
McCallum and W.
Li. 2003.
Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.
In Proc.
CoNLL-2003. A.
McCallum, K.
Rohanimanesh, and C.
Sutton. 2003.
Dynamic conditional random fields for jointly labeling multiple sequences.
In NIPS-2003 Workshop on Syntax, Semantics and Statistics.
A. McCallum.
2003. Efficiently inducing features of conditional random fields.
In Proc.
UAI 2003.
M. Osborne and J.
Baldridge. 2004.
Ensemble-based active learning for parse selection.
In Proc.
NAACL 2004.
F. Peng and A.
McCallum. 2004.
Accurate information extraction from research papers using conditional random fields.
In Proc.
HLT-NAACL 2004.
Y. Qi, M.
Szummer, and T.
P. Minka.
2005. Bayesian conditional random fields.
In Proc.
AISTATS 2005.
F. Sha and F.
Pereira. 2003.
Shallow parsing with conditional random fields.
In Proc.
HLT-NAACL 2003.
E. F.
Tjong Kim Sang and S.
Buchholz. 2000.
Introduction to the CoNLL-2000 shared task: Chunking.
In Proc.
CoNLL2000. E.
F. Tjong Kim Sang and F.
De Meulder.
2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In Proc. CoNLL-2003 .
Supersense Tagging of Unknown Nouns using Semantic Similarity James R.
Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Abstract The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.
Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.
Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples.
We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger.
We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity.
WORDNET with the UMLS medical resource and found only a very small degree of overlap.
Also, lexicalsemantic resources suffer from: bias towards concepts and senses from particular topics.
Some specialist topics are better covered in WORDNET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.
Ciaramita and Johnson (2003) found that common nouns missing from WORDNET 1.6 occurred every 8 sentences in the BLLIP corpus.
By WORDNET 2.0, coverage has improved but the problem of keeping up with language evolution remains difficult.
consistency when classifying similar words into categories.
For instance, the WORDNET lexicographer file for ionosphere (location) is different to exosphere and stratosphere (object), two other layers of the earth's atmosphere.
These problems demonstrate the need for automatic or semi-automatic methods for the creation and maintenance of lexical-semantic resources.
Broad semantic classification is currently used by lexicographers to organise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy.
Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET's hierarchical structure to create many annotated training instances from the synset glosses.
This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences.
Instead, we use vector-space similarity to retrieve a number of synonyms for each unknown common noun.
The supersenses of these synonyms are then combined to determine the supersense.
This approach significantly outperforms the multi-class perceptron on the same dataset based on WORDNET 1.6 and 1.7.1. 1 Introduction Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001).
In particular, WORDNET (Fellbaum, 1998) has significantly influenced research in NLP.
Unfortunately, these resource are extremely timeconsuming and labour-intensive to manually develop and maintain, requiring considerable linguistic and domain expertise.
Lexicographers cannot possibly keep pace with language evolution: sense distinctions are continually made and merged, words are coined or become obsolete, and technical terms migrate into the vernacular.
Technical domains, such as medicine, require separate treatment since common words often take on special meanings, and a significant proportion of their vocabulary does not overlap with everyday vocabulary.
Burgun and Bodenreider (2001) compared an alignment of Proceedings of the 43rd Annual Meeting of the ACL, pages 2633, Ann Arbor, June 2005.
c 2005 Association for Computational Linguistics fiLEX-FILE DESCRIPTION act animal artifact attribute body cognition communication event feeling food group location motive object person phenomenon plant possession process quantity relation shape state substance time acts or actions animals man-made objects attributes of people and objects body parts cognitive processes and contents communicative processes and contents natural events feelings and emotions foods and drinks groupings of people or objects spatial position goals natural objects (not man-made) people natural phenomena plants possession and transfer of possession natural processes quantities and units of measure relations between people/things/ideas two and three dimensional shapes stable states of affairs substances time and temporal relations ing directly underneath.
Other alternative sets of supersenses can be created by an arbitrary cut through the WORDNET hierarchy near the top, or by using topics from a thesaurus such as Roget's (Yarowsky, 1992).
These topic distinctions are coarser-grained than WORDNET senses, which have been criticised for being too difficult to distinguish even for experts.
Ciaramita and Johnson (2003) believe that the key sense distinctions are still maintained by supersenses.
They suggest that supersense tagging is similar to named entity recognition, which also has a very small set of categories with similar granularity (e.g.
location and person) for labelling predominantly unseen terms.
Supersense tagging can provide automated or semiautomated assistance to lexicographers adding words to the WORDNET hierarchy.
Once this task is solved successfully, it may be possible to insert words directly into the fine-grained distinctions of the hierarchy itself.
Clearly, this is the ultimate goal, to be able to insert new terms into lexical resources, extending the structure where necessary.
Supersense tagging is also interesting for many applications that use shallow semantics, e.g. information extraction and question answering.
Table 1: 25 noun lexicographer files in WORDNET Previous Work Supersenses There are 26 broad semantic classes employed by lexicographers in the initial phase of inserting words into the WORDNET hierarchy, called lexicographer files (lexfiles).
For the noun hierarchy, there are 25 lex-files and a file containing the top level nodes in the hierarchy called Tops.
Other syntactic classes are also organised using lex-files: 15 for verbs, 3 for adjectives and 1 for adverbs.
Lex-files form a set of coarse-grained sense distinctions within WORDNET.
For example, company appears in the following lex-files in WORDNET 2.0: group, which covers company in the social, commercial and troupe fine-grained senses; and state, which covers companionship.
The names and descriptions of the noun lex-files are shown in Table 1.
Some lex-files map directly to the top level nodes in the hierarchy, called unique beginners, while others are grouped together as hyponyms of a unique beginner (Fellbaum, 1998, page 30).
For example, abstraction subsumes the lex-files attribute, quantity, relation, communication and time.
Ciaramita and Johnson (2003) call the noun lex-file classes supersenses.
There are 11 unique beginners in the WORDNET noun hierarchy which could also be used as supersenses.
Ciaramita (2002) has produced a miniWORDNET by manually reducing the WORDNET hierarchy to 106 broad categories.
Ciaramita et al.(2003) describe how the lex-files can be used as root nodes in a two level hierarchy with the WORDNET synsets appearA considerable amount of research addresses structurally and statistically manipulating the hierarchy of WORDNET and the construction of new wordnets using the concept structure from English.
For lexical FreeNet, Beeferman (1998) adds over 350 000 collocation pairs (trigger pairs) extracted from a 160 million word corpus of broadcast news using mutual information.
The co-occurrence window was 500 words which was designed to approximate average document length.
Caraballo and Charniak (1999) have explored determining noun specificity from raw text.
They find that simple frequency counts are the most effective way of determining the parent-child ordering, achieving 83% accuracy over types of vehicle, food and occupation.
The other measure they found to be successful was the entropy of the conditional distribution of surrounding words given the noun.
Specificity ordering is a necessary step for building a noun hierarchy.
However, this approach clearly cannot build a hierarchy alone.
For instance, entity is less frequent than many concepts it subsumes.
This suggests it will only be possible to add words to an existing abstract structure rather than create categories right up to the unique beginners.
Hearst and Sch tze (1993) flatten WORDNET into 726 u categories using an algorithm which attempts to minimise the variance in category size.
These categories are used to label paragraphs with topics, effectively repeating Yarowsky's (1992) experiments using the their categories rather than Roget's thesaurus.
Sch tze's (1992) u WordSpace system was used to add topical links, such as between ball, racquet and game (the tennis problem).
Further, they also use the same vector-space techniques to label previously unseen words using the most common class assigned to the top 20 synonyms for that word.
Widdows (2003) uses a similar technique to insert words into the WORDNET hierarchy.
He first extracts synonyms for the unknown word using vector-space similarity measures based on Latent Semantic Analysis and then searches for a location in the hierarchy nearest to these synonyms.
This same technique as is used in our approach to supersense tagging.
Ciaramita and Johnson (2003) implement a supersense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.
Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy.
They developed an efficient algorithm for estimating the model over hierarchical training data.
WORDNET 1.6 NOUN SUPERSENSE WORDNET 1.7.1 NOUN SUPERSENSE stock index fast food bottler subcompact advancer cash flow downside discounter trade-off billionaire communication food group artifact person possession cognition artifact act person week buyout insurer partner health income contender cartel lender planner time act group person state possession person group person artifact Table 2: Example nouns and their supersenses largest NLP processed corpus described in published research.
The corpus consists of the British National Corpus (BNC), the Reuters Corpus Volume 1 (RCV1), and most of the Linguistic Data Consortium's news text collected since 1987: Continuous Speech Recognition III (CSR-III); North American News Text Corpus (NANTC); the NANTC Supplement (NANTS); and the ACQUAINT Corpus.
The components and their sizes including punctuation are given in Table 3.
The LDC has recently released the English Gigaword corpus which includes most of the corpora listed above.
CORPUS BNC RCV1 CSR-III NANTC NANTS ACQUAINT DOCS.
SENTS. WORDS Evaluation Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.
They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation.
Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.
Our evaluation will use exactly the same test sets as Ciaramita and Johnson (2003).
The WORDNET 1.7.1 test set consists of 744 previously unseen nouns, the majority of which (over 90%) have only one sense.
The WORDNET 1.6 test set consists of several cross-validation sets of 755 nouns randomly selected from the BLLIP training set used by Ciaramita and Johnson (2003).
They have kindly supplied us with the WORDNET 1.7.1 test set and one cross-validation run of the WORDNET 1.6 test set.
Our development experiments are performed on the WORDNET 1.6 test set with one final run on the WORDNET 1.7.1 test set.
Some examples from the test sets are given in Table 2 with their supersenses.
Table 3: 2 billion word corpus statistics We have tokenized the text using the Grok-OpenNLP tokenizer (Morton, 2002) and split the sentences using MXTerminator (Reynar and Ratnaparkhi, 1997).
Any sentences less than 3 words or more than 100 words long were rejected, along with sentences containing more than 5 numbers or more than 4 brackets, to reduce noise.
The rest of the pipeline is described in the next section.
Semantic Similarity Corpus We have developed a 2 billion word corpus, shallowparsed with a statistical NLP pipeline, which is by far the Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts.
This hypothesis suggests that semantic similarity can be measured by comparing the contexts each word appears in.
In vector-space models each headword is represented by a vector of frequency counts recording the contexts that it appears in.
The key parameters are the context extraction method and the similarity measure used to compare context vectors.
Our approach to fivector-space similarity is based on the SEXTANT system described in Grefenstette (1994).
Curran and Moens (2002b) compared several context extraction methods and found that the shallow pipeline and grammatical relation extraction used in SEXTANT was both extremely fast and produced high-quality results.
SEXTANT extracts relation tuples (w, r, w ) for each noun, where w is the headword, r is the relation type and w is the other word.
The efficiency of the SEXTANT approach makes the extraction of contextual information from over 2 billion words of raw text feasible.
We describe the shallow pipeline in detail below.
Curran and Moens (2002a) compared several different similarity measures and found that Grefenstette's weighted JACCARD measure performed the best: min(wgt(w1, r, w ), wgt(w2, r, w )) max(wgt(w1, r, w ), wgt(w2, r, w )) (1) RELATION DESCRIPTION adj dobj iobj nn nnprep subj Table 4: Grammatical relations from SEXTANT against the CELEX lexical database (Minnen et al., 2001)  and is very efficient, analysing over 80 000 words per second.
morpha often maintains sense distinctions between singular and plural nouns; for instance: spectacles is not reduced to spectacle, but fails to do so in other cases: glasses is converted to glass.
This inconsistency is problematic when using morphological analysis to smooth vector-space models.
However, morphological smoothing still produces better results in practice.
6.3 Grammatical
Relation Extraction where wgt(w, r, w ) is the weight function for relation (w, r, w ).
Curran and Moens (2002a) introduced the TTEST weight function, which is used in collocation extraction.
Here, the t-test compares the joint and product probability distributions of the headword and context: p(w, r, w ) p(, r, w )p(w,, ) p(, r, w )p(w,, ) (2) where indicates a global sum over that element of the relation tuple.
JACCARD and TTEST produced better quality synonyms than existing measures in the literature, so we use Curran and Moen's configuration for our supersense tagging experiments.
6.1 Part
of Speech Tagging and Chunking Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al., 1994).
The only similar performing tool is the Trigrams `n' Tags tagger (Brants, 2000) which uses a much simpler statistical model.
Our implementation uses a maximum entropy chunker which has similar feature types to Koeling (2000) and is also trained on chunks extracted from the entire Penn Treebank using the CoNLL 2000 script.
Since the Penn Treebank separates PPs and conjunctions from NPs, they are concatenated to match Grefenstette's table-based results, i.e. the SEXTANT always prefers noun attachment.
6.2 Morphological
Analysis Our implementation uses morpha, the Sussex morphological analyser (Minnen et al., 2001), which is implemented using lex grammars for both affix splitting and generation.
morpha has wide coverage  nearly 100% After the raw text has been POS tagged and chunked, the grammatical relation extraction algorithm is run over the chunks.
This consists of five passes over each sentence that first identify noun and verb phrase heads and then collect grammatical relations between each common noun and its modifiers and verbs.
A global list of grammatical relations generated by each pass is maintained across the passes.
The global list is used to determine if a word is already attached.
Once all five passes have been completed this association list contains all of the nounmodifier/verb pairs which have been extracted from the sentence.
The types of grammatical relation extracted by SEXTANT are shown in Table 4.
For relations between nouns (nn and nnprep), we also create inverse relations (w, r, w) representing the fact that w can modify w.
The 5 passes are described below.
Pass 1: Noun Pre-modifiers This pass scans NPs, left to right, creating adjectival (adj) and nominal (nn) pre-modifier grammatical relations (GRs) with every noun to the pre-modifier's right, up to a preposition or the phrase end.
This corresponds to assuming right-branching noun compounds.
Within each NP only the NP and PP heads remain unattached.
Pass 2: Noun Post-modifiers This pass scans NPs, right to left, creating post-modifier GRs between the unattached heads of NPs and PPs.
If a preposition is encountered between the noun heads, a prepositional noun (nnprep) GR is created, otherwise an appositional noun (nn) GR is created.
This corresponds to assuming right-branching PP attachment.
After this phrase only the NP head remains unattached.
Tense Determination The rightmost verb in each VP is considered the head.
A fiis initially categorised as active.
If the head verb is a form of be then the VP becomes attributive.
Otherwise, the algorithm scans the VP from right to left: if an auxiliary verb form of be is encountered the VP becomes passive; if a progressive verb (except being) is encountered the VP becomes active.
Only the noun heads on either side of VPs remain unattached.
The remaining three passes attach these to the verb heads as either subjects or objects depending on the voice of the VP.
Pass 3: Verb Pre-Attachment This pass scans sentences, right to left, associating the first NP head to the left of the VP with its head.
If the VP is active, a subject (subj) relation is created; otherwise, a direct object (dobj) relation is created.
For example, antigen is the subject of represent.
Pass 4: Verb Post-Attachment This pass scans sentences, left to right, associating the first NP or PP head to the right of the VP with its head.
If the VP was classed as active and the phrase is an NP then a direct object (dobj) relation is created.
If the VP was classed as passive and the phrase is an NP then a subject (subj) relation is created.
If the following phrase is a PP then an indirect object (iobj) relation is created.
The interaction between the head verb and the preposition determine whether the noun is an indirect object of a ditransitive verb or alternatively the head of a PP that is modifying the verb.
However, SEXTANT always attaches the PP to the previous phrase.
Pass 5: Verb Progressive Participles The final step of the process is to attach progressive verbs to subjects and objects (without concern for whether they are already attached).
Progressive verbs can function as nouns, verbs and adjectives and once again a nave api proximation to the correct attachment is made.
Any progressive verb which appears after a determiner or quantifier is considered a noun.
Otherwise, it is a verb and passes 3 and 4 are repeated to attach subjects and objects.
Finally, SEXTANT collapses the nn, nnprep and adj relations together into a single broad noun-modifier grammatical relation.
Grefenstette (1994) claims this extractor has a grammatical relation accuracy of 75% after manually checking 60 sentences.
VP SUFFIX EXAMPLE SUPERSENSE remoteness annulment statesman bowling viscosity electronics arsine mariner entomology attribute act person act attribute cognition substance person cognition Table 5: Hand-coded rules for supersense guessing fall-back method is a simple hand-coded classifier which examines the unknown noun and makes a guess based on simple morphological analysis of the suffix.
These rules were created by inspecting the suffixes of rare nouns in WORDNET 1.6.
The supersense guessing rules are given in Table 5.
If none of the rules match, then the default supersense artifact is assigned.
The problem now becomes how to convert the ranked list of extracted synonyms for each unknown noun into a single supersense selection.
Each extracted synonym votes for its one or more supersenses that appear in WORDNET 1.6.
There are many parameters to consider:     how many extracted synonyms to use; how to weight each synonym's vote; whether unreliable synonyms should be filtered out; how to deal with polysemous synonyms.
Approach Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a supersense for the unknown nouns.
This technique is similar to Hearst and Sch tze (1993) and Widdows (2003).
u However, sometimes the unknown noun does not appear in our 2 billion word corpus, or at least does not appear frequently enough to provide sufficient contextual information to extract reliable synonyms.
In these cases, our The experiments described below consider a range of options for these parameters.
In fact, these experiments are so quick to run we have been able to exhaustively test many combinations of these parameters.
We have experimented with up to 200 voting extracted synonyms.
There are several ways to weight each synonym's contribution.
The simplest approach would be to give each synonym the same weight.
Another approach is to use the scores returned by the similarity system.
Alternatively, the weights can use the ranking of the extracted synonyms.
Again these options have been considered below.
A related question is whether to use all of the extracted synonyms, or perhaps filter out synonyms for which a small amount of contextual information has been extracted, and so might be unreliable.
The final issue is how to deal with polysemy.
Does every supersense of each extracted synonym get the whole weight of that synonym or is it distributed evenly between the supersenses like Resnik (1995)?
Another alternative is to only consider unambiguous synonyms with a single supersense in WORDNET.
A disadvantage of this similarity approach is that it requires full synonym extraction, which compares the unknown word against a large number of words when, in fiSYSTEM WN Ciaramita and Johnson baseline Ciaramita and Johnson perceptron Similarity based results WN WORDNET 1.6 SUPERSENSE N P R F WORDNET 1.7.1 N P R F Table 6: Summary of supersense tagging accuracies fact, we want to calculate the similarity to a small number of supersenses.
This inefficiency could be reduced significantly if we consider only very high frequency words, but even this is still expensive.
Results We have used the WORDNET 1.6 test set to experiment with different parameter settings and have kept the WORDNET 1.7.1 test set as a final comparison of best results with Ciaramita and Johnson (2003).
The experiments were performed by considering all possible configurations of the parameters described above.
The following voting options were considered for each supersense of each extracted synonym: the initial voting weight for a supersense could either be a constant (IDENTITY) or the similarity score (SCORE) of the synonym.
The initial weight could then be divided by the number of supersenses to share out the weight (SHARED).
The weight could also be divided by the rank (RANK) to penalise supersenses further down the list.
The best performance on the 1.6 test set was achieved with the SCORE voting, without sharing or ranking penalties.
The extracted synonyms are filtered before contributing to the vote with their supersense(s).
This filtering involves checking that the synonym's frequency and number of contexts are large enough to ensure it is reliable.
We have experimented with a wide range of cutoffs and the best performance on the 1.6 test set was achieved using a minimum cutoff of 5 for the synonym's frequency and the number of contexts it appears in.
The next question is how many synonyms are considered.
We considered using just the nearest unambiguous synonym, and the top 5, 10, 20, 50, 100 and 200 synonyms.
All of the top performing configurations used 50 synonyms.
We have also experimented with filtering out highly polysemous nouns by eliminating words with two, three or more synonyms.
However, such a filter turned out to make little difference.
Finally, we need to decide when to use the similarity measure and when to fall-back to the guessing rules.
This is determined by looking at the frequency and number of attributes for the unknown word.
Not surprisingly, the similarity system works better than the guessing rules if it has any information at all.
The results are summarised in Table 6.
The accuracy of the best-performing configurations was 68% on the Table 7: Breakdown of results by supersense WORDNET 1.6 test set with several other parameter combinations described above performing nearly as well.
On the previously unused WORDNET 1.7.1 test set, our accuracy is 63% using the best system on the WORDNET 1.6 test set.
By optimising the parameters on the 1.7.1 test set we can increase that to 64%, indicating that we have not excessively over-tuned on the 1.6 test set.
Our results significantly outperform Ciaramita and Johnson (2003) on both test sets even though our system is unsupervised.
The large difference between our 1.6 and 1.7.1 test set accuracy demonstrates that the 1.7.1 set is much harder.
Table 7 shows the breakdown in performance for each supersense.
The columns show the number of instances of each supersense with the precision, recall and f-score measures as percentages.
The most frequent supersenses in both test sets were person, attribute and act.
Of the frequent categories, person is the easiest supersense to get correct in both the 1.6 and 1.7.1 test sets, followed by food, artifact and substance.
This is not surprising since these concrete words tend to have very fewer other senses, well constrained contexts and a relatively high frequency.
These factors are conducive for extracting reliable synonyms.
These results also support Ciaramita and Johnson's view that abstract concepts like communication, cognition and state are much harder.
We would expect the location fisupersense to perform well since it is quite concrete, but unfortunately our synonym extraction system does not incorporate proper nouns, so many of these words were classified using the hand-built classifier.
Also, in the data from Ciaramita and Johnson all of the words are in lower case, so no sensible guessing rules could help.
Other Alternatives and Future Work An alternative approach worth exploring is to create context vectors for the supersense categories themselves and compare these against the words.
This has the advantage of producing a much smaller number of vectors to compare against.
In the current system, we must compare a word against the entire vocabulary (over 500 000 headwords), which is much less efficient than a comparison against only 26 supersense context vectors.
The question now becomes how to construct vectors of supersenses.
The most obvious solution is to sum the context vectors across the words which have each supersense.
However, our early experiments suggest that this produces extremely large vectors which do not match well against the much smaller vectors of each unseen word.
Also, the same questions arise in the construction of these vectors.
How are words with multiple supersenses handled?
Our preliminary experiments suggest that only combining the vectors for unambiguous words produces the best results.
One solution would be to take the intersection between vectors across words for each supersense (i.e.
to find the common contexts that these words appear in).
However, given the sparseness of the data this may not leave very large context vectors.
A final solution would be to consider a large set of the canonical attributes (Curran and Moens, 2002a) to represent each supersense.
Canonical attributes summarise the key contexts for each headword and are used to improve the efficiency of the similarity comparisons.
There are a number of problems our system does not currently handle.
Firstly, we do not include proper names in our similarity system which means that location entities can be very difficult to identify correctly (as the results demonstrate).
Further, our similarity system does not currently incorporate multi-word terms.
We overcome this by using the synonyms of the last word in the multi-word term.
However, there are 174 multi-word terms (23%) in the WORDNET 1.7.1 test set which we could probably tag more accurately with synonyms for the whole multi-word term.
Finally, we plan to implement a supervised machine learner to replace the fallback method, which currently has an accuracy of 37% on the WORDNET 1.7.1 test set.
We intend to extend our experiments beyond the Ciaramita and Johnson (2003) set to include previous and more recent versions of WORDNET to compare their difficulty, and also perform experiments over a range of corpus sizes to determine the impact of corpus size on the quality of results.
We would like to move onto the more difficult task of insertion into the hierarchy itself and compare against the initial work by Widdows (2003) using latent semantic analysis.
Here the issue of how to combine vectors is even more interesting since there is the additional structure of the WORDNET inheritance hierarchy and the small synonym sets that can be used for more fine-grained combination of vectors.
Conclusion Our application of semantic similarity to supersense tagging follows earlier work by Hearst and Sch tze (1993) u and Widdows (2003).
To classify a previously unseen common noun our approach extracts synonyms which vote using their supersenses in WORDNET 1.6.
We have experimented with several parameters finding that the best configuration uses 50 extracted synonyms, filtered by frequency and number of contexts to increase their reliability.
Each synonym votes for each of its supersenses from WORDNET 1.6 using the similarity score from our synonym extractor.
Using this approach we have significantly outperformed the supervised multi-class perceptron Ciaramita and Johnson (2003).
This paper also demonstrates the use of a very efficient shallow NLP pipeline to process a massive corpus.
Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense tag.
This application of semantic similarity demonstrates that an unsupervised methods can outperform supervised methods for some NLP tasks if enough data is available.
Acknowledgements We would like to thank Massi Ciaramita for supplying his original data for these experiments and answering our queries, and to Stephen Clark and the anonymous reviewers for their helpful feedback and corrections.
This work has been supported by a Commonwealth scholarship, Sydney University Travelling Scholarship and Australian Research Council Discovery Project DP0453131.
References L.
Douglas Baker and Andrew McCallum.
1998. Distributional clustering of words for text classification.
In Proceedings of the 21st annual international ACM SIGIR conference on Research and Development in Information Retrieval, pages 96103, Melbourne, Australia.
Doug Beeferman.
1998. Lexical discovery with an enriched semantic network.
In Proceedings of the Workshop on Usage of WordNet in Natural Language Processing Systems, pages 358364, Montreal, Quebec, Canada.
Thorsten Brants.
2000. TnT a statistical part-of-speech tagger.
In Proceedings of the 6th Applied Natural Language Processing Conference, pages 224231, Seattle, WA USA.
Anita Burgun and Olivier Bodenreider.
2001. Comparing terms, concepts and semantic classes in WordNet and the Unified Medical Language System.
In Proceedings of the Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, pages 7782, Pittsburgh, PA USA.
Sharon A.
Caraballo and Eugene Charniak.
1999. Determining the specificity of nouns from text.
In Proceedings of the Joint ACL SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 6370, College Park, MD USA.
Massimiliano Ciaramita and Mark Johnson.
2003. Supersense tagging of unknown nouns in WordNet.
In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 168175, Sapporo, Japan.
Massimiliano Ciaramita, Thomas Hofmann, and Mark Johnson.
2003. Hierarchical semantic classification: Word sense disambiguation with world knowledge.
In Proceedings of the 18th International Joint Conference on Artificial Intelligence, Acapulco, Mexico.
Massimiliano Ciaramita.
2002. Boosting automatic lexical acquisition with morphological information.
In Proceedings of the Workshop on Unsupervised Lexical Acquisition, pages 1725, Philadelphia, PA, USA.
Stephen Clark and David Weir.
2002. Class-based probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187206, June.
Koby Crammer and Yoram Singer.
2001. Ultraconservative online algorithms for multiclass problems.
In Proceedings of the 14th annual Conference on Computational Learning Theory and 5th European Conference on Computational Learning Theory, pages 99115, Amsterdam, The Netherlands.
James R.
Curran and Stephen Clark.
2003. Investigating GIS and smoothing for maximum entropy taggers.
In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics, pages 9198, Budapest, Hungary.
James R.
Curran and Marc Moens.
2002a. Improvements in automatic thesaurus extraction.
In Proceedings of the Workshop on Unsupervised Lexical Acquisition, pages 5966, Philadelphia, PA, USA.
James R.
Curran and Marc Moens.
2002b. Scaling context space.
In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 231238, Philadelphia, PA, USA.
Christiane Fellbaum, editor.
1998. WordNet: An Electronic Lexical Database.
MIT Press, Cambridge, MA USA.
Gregory Grefenstette.
1994. Explorations in Automatic Thesaurus Discovery.
Kluwer Academic Publishers, Boston, MA USA.
Marti A.
Hearst and Hinrich Schutze.
1993. Customizing a lexicon to better suit a computational task.
In Proceedings of the Workshop on Acquisition of Lexical Knowledge from Text, pages 5569, Columbus, OH USA.
Rob Koeling.
2000. Chunking with maximum entropy models.
In Proceedings of the 4th Conference on Computational Natural Language Learning and of the 2nd Learning Language in Logic Workshop, pages 139141, Lisbon, Portugal.
Mitchell P.
Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
1993. Building a large annotated corpus of English: the Penn Treebank.
Computational Linguistics, 19(2):313330.
Guido Minnen, John Carroll, and Darren Pearce.
2001. Applied morphological processing of English.
Natural Language Engineering, 7(3):207223.
Tom Morton.
2002. Grok tokenizer.
Grok OpenNLP toolkit.
Marius Pasca and Sanda M.
Harabagiu. 2001.
The informative role of WordNet in open-domain question answering.
In Proceedings of the Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, pages 138143, Pittsburgh, PA USA.
Darren Pearce.
2001. Synonymy in collocation extraction.
In Proceedings of the Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, pages 4146, Pittsburgh, PA USA.
Philip Resnik.
1995. Using information content to evaluate semantic similarity.
In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 448453, Montreal, Canada.
Jeffrey C.
Reynar and Adwait Ratnaparkhi.
1997. A maximum entropy approach to identifying sentence boundaries.
In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 1619, Washington, D.C.
USA. Hinrich Schutze.
1992. Context space.
In Intelligent Probau bilistic Approaches to Natural Language, number FS-92-04 in Fall Symposium Series, pages 113120, Stanford University, CA USA.
Dominic Widdows.
2003. Unsupervised methods for developing taxonomies by combining syntactic and statistical information.
In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 276283, Edmonton, Alberta Canada.
David Yarowsky.
1992. Word-sense disambiguation using statistical models of Roget's categories trained on large corpora.
In Proceedings of the 14th international conference on Computational Linguistics, pages 454460, Nantes, France.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 121??24, Prague, June 2007.
c2007 Association for Computational Linguistics Predicting Evidence of Understanding by Monitoring User?s Task Manipulation in Multimodal Conversations Yukiko I.
Nakano ??
Yoshiko Arimoto ??
?? Tokyo University of Agriculture and Technology 2-24-16 Nakacho, Koganeishi, Tokyo 184-8588, Japan {nakano, kmurata, menomoto}@cc.tuat.ac.jp Kazuyoshi Murata ??
Yasuhiro Asa ??
?? Tokyo University of Technology 1404-1 Katakura, Hachioji, Tokyo 192-0981, Japan ar@mf.teu.ac.jp Mika Enomoto ??
Hirohiko Sagawa ??
?? Central Research Laboratory, Hitachi, Ltd.
1-280, Higashi-koigakubo Kokubunji-shi, Tokyo 185-8601, Japan {yasuhiro.asa.mk, hirohiko.sagawa.cu}@hitachi.com Abstract The aim of this paper is to develop animated agents that can control multimodal instruction dialogues by monitoring user?s behaviors.
First, this paper reports on our Wizard-of-Oz experiments, and then, using the collected corpus, proposes a probabilistic model of fine-grained timing dependencies among multimodal communication behaviors: speech, gestures, and mouse manipulations.
A preliminary evaluation revealed that our model can predict a instructor?s grounding judgment and a listener?s successful mouse manipulation quite accurately, suggesting that the model is useful in estimating the user?s understanding, and can be applied to determining the agent?s next action.
1 Introduction
In face-to-face conversation, speakers adjust their utterances in progress according to the listener?s feedback expressed in multimodal manners, such as speech, facial expression, and eye-gaze.
In taskmanipulation situations where the listener manipulates objects by following the speaker?s instructions, correct task manipulation by the listener serves as more direct evidence of understanding (Brennan 2000, Clark and Krych 2004), and affects the speaker?s dialogue control strategies.
Figure 1 shows an example of a software instruction dialogue in a video-mediated situation (originally in Japanese).
While the learner says nothing, the instructor gives the instruction in small pieces, simultaneously modifying her gestures and utterances according to the learner?s mouse movements.
To accomplish such interaction between human users and animated help agents, and to assist the user through natural conversational interaction, this paper proposes a probabilistic model that computes timing dependencies among different types of behaviors in different modalities: speech, gestures, and mouse events.
The model predicts (a) whether the instructor?s current utterance will be successfully understood by the learner and grounded (Clark and Schaefer 1989), and (b) whether the learner will successfully manipulate the object in the near future.
These predictions can be used as constraints in determining agent actions.
For example, if the current utterance will not be grounded, then the help agent must add more information.
In the following sections, first, we collect human-agent conversations by employing a Wizardof-Oz method, and annotate verbal and nonverbal behaviors.
The annotated corpus is used to build a Bayesian network model for the multimodal instruction dialogues.
Finally, we will evaluate how ?That??(204ms pause) Pointing gesture <preparation> <stroke> Mouse move Instructor: Learner: ?at the most??(395ms pause) ?left-hand side??
Instructor: Learner: Instructor: Mouse move Figure 1: Example of task manipulation dialogue 121 accurately the model can predict the events in (a) and (b) mentioned above.
2 Related
work In their psychological study, Clark and Krych (2004) showed that speakers alter their utterances midcourse while monitoring not only the listener?s vocal signals, but also the listener?s gestural signals as well as through other mutually visible events.
Such a bilateral process functions as a joint activity to ground the presented information, and task manipulation as a mutually visible event contributes to the grounding process (Brennan 2000, Whittaker 2003).
Dillenbourg, Traum, et al.(1996) also discussed cross-modality in grounding: verbally presented information is grounded by an action in the task environment.
Studies on interface agents have presented computational models of multimodal interaction (Cassell, Bickmore, et al.2000). Paek and Horvitz (1999) focused on uncertainty in speech-based interaction, and employed a Bayesian network to understand the user?s speech input.
For user monitoring, Nakano, Reinstein, et al.(2003) used a head tracker to build a conversational agent which can monitor the user?s eye-gaze and head nods as nonverbal signals in grounding.
These previous studies provide psychological evidence about the speaker?s monitoring behaviors as well as conversation modeling techniques in computational linguistics.
However, little has been studied about how systems (agents) should monitor the user?s task manipulation, which gives direct evidence of understanding to estimate the user?s understanding, and exploits the predicted evidence as constraints in selecting the agent?s next action.
Based on these previous attempts, this study proposes a multimodal interaction model by focusing on task manipulation, and predicts conversation states using probabilistic reasoning.
3 Data
collection A data collection experiment was conducted using a Wizard-of-Oz agent assisting a user in learning a PCTV application, a system for watching and recording TV programs on a PC.
The output of the PC operated by the user was displayed on a 23-inch monitor in front of the user, and also projected on a 120-inch big screen, in front of which a human instructor was standing (Figure 2 (a)).
Therefore, the participants shared visual events output from the PC (Figure 2 (b)) while sitting in different rooms.
In addition, a rabbit-like animated agent was controlled through the instructor?s motion data captured by motion sensors.
The instructor?s voice was changed through a voice transformation system to make it sound like a rabbit agent.
4 Corpus
We collected 20 conversations from 10 pairs, and annotated 11 conversations of 6 pairs using the Anvil video annotating tool (Kipp 2004).
Agent?s verbal behaviors: The agent?s (actually, instructor?s) speech data was split by pauses longer than 200ms.
For each inter pausal unit (IPU), utterance content type defined as follows was assigned.
?? Identification (id): identification of a target object for the next operation ??
Operation (op): request to execute a mouse click or a similar primitive action on the target ??
Identification + operation (idop): referring to identification and operation in one IPU In addition to these main categories, we also used: State (referring to a state before/after an operation), Function (explaining a function of the system), Goal (referring to a task goal to be accomplished), and Acknowledgment.
The intercoder agreement for this coding scheme is very high K=0.89 (Cohen?s Kappa), suggesting that the assigned tags are reliable.
Agent?s nonverbal behaviors: As the most salient instructor?s nonverbal behaviors in the collected data, we annotated agent pointing gestures: ??
Agent movement: agent?s position movement ??
Agent touching target (att): agent?s touching the target object as a stroke of a pointing gesture (a) Instructor (b) PC output Figure 2: Wizard-of-Oz agent controlled by instructor 122 User?s nonverbal behaviors: We annotated three types of mouse manipulation for the user?s task manipulation as follows: ??
Mouse movement: movement of the mouse cursor ??
Mouse-on-target: the mouse cursor is on the target object ??
Click target: click on the target object 4.1 Example of collected data An example of an annotated corpus is shown in Figure 3.
The upper two tracks illustrate the agent?s verbal and nonverbal behaviors, and the other two illustrate the user?s behaviors.
The agent was pointing at the target (att) and giving a sequence of identification descriptions [a1-3].
Since the user?s mouse did not move at all, the agent added another identification IPU [a4] accompanied by another pointing gesture.
Immediately after that, the user?s mouse cursor started moving towards the target object.
After finishing the next IPU, the agent finally requested the user to click the object in [a6].
Note that the collected Wizard-of-Oz conversations are very similar to the human-human instruction dialogues shown in Figure 1.
While carefully monitoring the user?s mouse actions, the Wizard-of-Oz agent provided information in small pieces.
If it was uncertain that the user was following the instruction, the agent added more explanation without continuing.
5 Probabilistic
model of user-agent multimodal interaction 5.1 Building a Bayesian network model To consider multiple factors for verbal and nonverbal behaviors in probabilistic reasoning, we employed a Bayesian network technique, which can infer the likelihood of the occurrence of a target event based on the dependencies among multiple kinds of evidence.
We extracted the conversational data from the beginning of an instructor's identification utterance for a new target object to the point that the user clicks on the object.
Each IPU was split at 500ms intervals, and 1395 intervals were obtained.
As shown in Figure 4, the network consists of 9 properties concerning verbal and nonverbal behaviors for past, current, and future interval(s).
5.2 Predicting
evidence of understanding As a preliminary evaluation, we tested how accurately our Bayesian network model can predict an instructor?s grounding judgment, and the user?s mouse click.
The following five kinds of evidence were given to the network to predict future states.
As evidence for the previous three intervals (1.5 sec), we used (1) the percentage of time the agent touched the target (att), (2) the number of the user?s mouse movements.
Evidence for the current interval is (3) current IPU?s content type, (4) whether the end of the current interval will be the end of the IPU (i.e.
whether a pause will follow after the current interval), and (5) whether the mouse is on the target object.
Well, Yes View the TV right of Yes Beside the DVD There is a button starts with ?V??
Ah, yes Er, yes Press it This User Agent Speech Gesture Mouser actions id id id id id+op Mouse move click att att att Mouse on target [a2] [a3] [a4] [a5] [a6][a1] ack ack ack ack Speech Off On Figure 3: Example dialogue between Wizard-of-Oz agent and user Figure 4: Bayesian network model 123 (a) Predicting grounding judgment: We tested how accurately the model can predict whether the instructor will go on to the next leg of the instruction or will give additional explanations using the same utterance content type (the current message will not be grounded).
The results of 5-fold cross-validation are shown in Table 1.
Since 83% of the data are ?same content??cases, prediction for ?same content??is very accurate (F-measure is 0.90).
However, it is not very easy to find ?content change??case because of its less frequency (F-measure is 0.68).
It would be better to test the model using more balanced data.
(b) Predicting user?s mouse click: As a measure of the smoothness of task manipulation, the network predicted whether the user?s mouse click would be successfully performed within the next 5 intervals (2.5sec).
If a mouse click is predicted, the agent should just wait without annoying the user by unnecessary explanation.
Since randomized data is not appropriate to test mouse click prediction, we used 299 sequences of utterances that were not used for training.
Our model predicted 84% of the user?s mouse clicks: 80% of them were predicted 3-5 intervals before the actual occurrence of the mouse click, and 20% were predicted 1 interval before.
However, the model frequently generates wrong predictions.
Improving precision rate is necessary.
6 Discussion
and Future Work We employed a Bayesian network technique to our goal of developing conversational agents that can generate fine-grained multimodal instruction dialogues, and we proposed a probabilistic model for predicting grounding judgment and user?s successful mouse click.
The results of preliminary evaluation suggest that separate models of each modality for each conversational participant cannot properly describe the complex process of on-going multimodal interaction, but modeling the interaction as dyadic activities with multiple tracks of modalities is a promising approach.
The advantage of employing the Bayesian network technique is that, by considering the cost of misclassification and the benefit of correct classification, the model can be easily adjusted according to the purpose of the system or the user?s skill level.
For example, we can make the model more cautious or incautious.
Thus, our next step is to implement the proposed model into a conversational agent, and evaluate our model not only in its accuracy, but also in its effectiveness by testing the model with various utility values.
References Brennan, S.
2000. Processes that shape conversation and their implications for computational linguistics.
In Proceedings of 38th Annual Meeting of the ACL.
Cassell, J., Bickmore, T., Campbell, L., Vilhjalmsson, H.
and Yan, H.
(2000). Human Conversation as a System Framework: Designing Embodied Conversational Agents.
Embodied Conversational Agents.
J. Cassell, J.
Sullivan, S.
Prevost and E.
Churchill. Cambridge, MA, MIT Press: 29-63.
Clark, H.
H. and Schaefer, E.
F. 1989.
Contributing to discourse.
Cognitive Science 13: 259-294.
Clark, H.
H. and Krych, M.
A. 2004.
Speaking while monitoring addressees for understanding.
Journal of Memory and Language 50(1): 62-81.
Dillenbourg, P., Traum, D.
R. and Schneider, D.
1996. Grounding in Multi-modal Task Oriented Collaboration.
In Proceedings of EuroAI&Education Conference: 415-425.
Kipp, M.
2004. Gesture Generation by Imitation From Human Behavior to Computer Character Animation, Boca Raton, Florida: Dissertation.com.
Nakano, Y.
I., Reinstein, G., Stocky, T.
and Cassell, J.
2003. Towards a Model of Face-to-Face Grounding.
In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics: 553-561.
Paek, T.
and Horvitz, E.
(1999). Uncertainty, Utility, and Misunderstanding: A Decision-Theoretic Perspective on Grounding in Conversational Systems.
Working Papers of the AAAI Fall Symposium on Psychological Models of Communication in Collaborative Systems.
S. E.
Brennan, A.
Giboin and D.
Traum: 85-92.
Whittaker, S.
(2003). Theories and Methods in Mediated Communication.
The Handbook of Discourse Processes.
A. Graesser, MIT Press.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 125??28, Prague, June 2007.
c2007 Association for Computational Linguistics Automatically Assessing the Post Quality in Online Discussions on Software Markus Weimer and Iryna Gurevych and Max Muhlhauser Ubiquitous Knowledge Processing Group, Division of Telecooperation Darmstadt University of Technology, Germany http://www.ukp.informatik.tu-darmstadt.de [mweimer,gurevych,max]@tk.informatik.tu-darmstadt.de Abstract Assessing the quality of user generated content is an important problem for many web forums.
While quality is currently assessed manually, we propose an algorithm to assess the quality of forum posts automatically and test it on data provided by Nabble.com.
We use state-of-the-art classification techniques and experiment with five feature classes: Surface, Lexical, Syntactic, Forum specific and Similarity features.
We achieve an accuracy of 89% on the task of automatically assessing post quality in the software domain using forum specific features.
Without forum specific features, we achieve an accuracy of 82%.
1 Introduction
Web 2.0 leads to the proliferation of user generated content, such as blogs, wikis and forums.
Key properties of user generated content are: low publication threshold and a lack of editorial control.
Therefore, the quality of this content may vary.
The end user has problems to navigate through large repositories of information and find information of high quality quickly.
In order to address this problem, many forum hosting companies like Google Groups1 and Nabble2 introduce rating mechanisms, where users can rate the information manually on a scale from 1 (low quality) to 5 (high quality).
The ratings have been shown to be consistent with the user community by Lampe and Resnick (2004).
However, the 1http://groups.google.com 2http://www.nabble.com percentage of manually rated posts is very low (0.1% in Nabble).
Departing from this, the main idea explored in the present paper is to investigate the feasibility of automatically assessing the perceived quality of user generated content.
We test this idea for online forum discussions in the domain of software.
The perceived quality is not an objective measure.
Rather, it models how the community at large perceives post quality.
We choose a machine learning approach to automatically assess it.
Our main contributions are: (1) An algorithm for automatic quality assessment of forum posts that learns from human ratings.
We evaluate the system on online discussions in the software domain.
(2) An analysis of the usefulness of different classes of features for the prediction of post quality.
2 Related
work To the best of our knowledge, this is the first work which attempts to assess the quality of forum posts automatically.
However, on the one hand work has been done on automatic assessment of other types of user generated content, such as essays and product reviews.
On the other hand, student online discussions have been analyzed.
Automatic text quality assessment has been studied in the area of automatic essay scoring (Valenti et al., 2003; Chodorow and Burstein, 2004; Attali and Burstein, 2006).
While there exist guidelines for writing and assessing essays, this is not the case for forum posts, as different users cast their rating with possibly different quality criteria in mind.
The same argument applies to the automatic assessment of product review usefulness (Kim et al., 2006c): 125 Stars Label on the website Number star Poor Post 1251 starstar Below Average Post 44 starstarstar Average Post 69 starstarstarstar Above Average Post 183 starstarstarstarstar Excellent Post 421 Table 1: Categories and their usage frequency.
Readers of a review are asked ?Was this review helpful to you???with the answer choices Yes/No.
This is very well defined compared to forum posts, which are typically rated on a five star scale that does not advertise a specific semantics.
Forums have been in the focus of another track of research.
Kim et al.(2006b) found that the relation between a student?s posting behavior and the grade obtained by that student can be assessed automatically.
The main features used are the number of posts, the average post length and the average number of replies to posts of the student.
Feng et al.(2006) and Kim et al.(2006a) describe a system to find the most authoritative answer in a forum thread.
The latter add speech act analysis as a feature for this classification.
Another feature is the author?s trustworthiness, which could be computed based on the automatic quality classification scheme proposed in the present paper.
Finding the most authoritative post could also be defined as a special case of the quality assessment.
However, it is definitely different from the task studied in the present paper.
We assess the perceived quality of a given post, based solely on its intrinsic features.
Any discussion thread may contain an indefinite number of good posts, rather than a single authoritative one.
3 Experiments
We seek to develop a system that adapts to the quality standards existing in a certain user community by learning the relation between a set of features and the perceived quality of posts.
We experimented with features from five classes described in table 2: Surface, Lexical, Syntactic, Forum specific and Similarity features.
We use forum discussions from the Software category of Nabble.com.5 The data consists of 1968 rated posts in 1788 threads from 497 forums.
Posts can be rated by multiple users, but that happens 5http://www.nabble.com/Software-f94.html rarely.
1927 posts were rated by one, 40 by two and 1 post by three users.
Table 1 shows the distribution of average ratings on a five star scale.
From this statistics, it becomes evident that users at Nabble prefer extreme ratings.
Therefore, we decided to treat the posts as being binary rated.: Posts with less than three stars are rated ?bad??
Posts with more than three stars are ?good??
We removed 61 posts where all ratings are exactly three stars.
We removed additional 14 posts because they had contradictory ratings on the binary scale.
Those posts were mostly spam, which was voted high for commercial interests and voted down for being spam.
Additionally, we removed 30 posts that did not contain any text but only attachments like pictures.
Finally, we removed 331 non English posts using a simple heuristics: Posts that contained a certain percentage of words above a pre-defined threshold, which are non-English according to a dictionary, were considered to be non-English.
This way, we obtained 1532 binary classified posts: 947 good posts and 585 bad posts.
For each post, we compiled a feature vector, and feature values were normalized to the range [0.0,...,1.0].
We use support vector machines as a state-of-theart-algorithm for binary classification.
For all experiments, we used a C-SVM with a gaussian RBF kernel as implemented by LibSVM in the YALE toolkit (Chang and Lin, 2001; Mierswa et al., 2006).
Parameters were set to C = 10 and  = 0.1.
We performed stratified ten-fold cross validation6 to estimate the performance of our algorithm.
We repeated several experiments according to the leave-one-out evaluation scheme and found comparable results to the ones reported in this paper.
4 Results
and Analysis We compared our algorithm to a majority class classifier as a baseline, which achieves an accuracy of 62%.
As it is evident from table 3, most system configurations outperform the baseline system.
The best performing single feature category are the Forum specific features.
As we seek to build an adaptable system, analyzing the performance without these features is worthwhile: Using all other features, we 6See (Witten and Frank, 2005), chapter 5.3 for an in-depth description.
126 Feature category Feature name Description Surface Features Length The number of tokens in a post.
Question Frequency The percentage of sentences ending with ????
Exclamation Frequency The percentage of sentences ending with ????
Capital Word Frequency The percentage of words in CAPITAL, which is often associated with shouting.
Lexical Features Information about the wording of the posts Spelling Error Frequency The percentage of words that are not spelled correctly.3 Swear Word Frequency The percentage of words that are on a list of swear words we compiled from resources like WordNet and Wikipedia4, which contains more than eighty words like ?asshole?? but also common transcriptions like ?f*ckin??
Syntactic Features The percentage of part-of-speech tags as defined in the PENN Treebank tag set (Marcus et al., 1994).
We used TreeTagger (Schmid, 1995) based on the english parameter files supplied with it.
Forum specific features Properties of a post that are only present in forum postings IsHTML Whether or not a post contains HTML.
In our data, this is encoded explicitly, but it can also be determined by regular expressions matching HTML tags.
IsMail Whether or not a post has been copied from a mailing list.
This is encoded explicitly in our data.
Quote Fraction The fraction of characters that are inside quotes of other posts.
These quotes are marked explicitly in our data.
URL and Path Count The number of URLs and filesystem paths.
Post quality in the software domain may be influenced by the amount of tangible information, which is partly captured by these features.
Similarity features Forums are focussed on a topic.
The relatedness of a post to the topic of the forum may influence post quality.
We capture this relatedness by the cosine between the posts unigram vector and the unigram vector of the forum.
Table 2: Features used for the automatic quality assessment of posts.
achieve an only slightly worse classification accuracy.
Thus, the combination of all other features captures the quality of a post fairly well.
SUF LEX SYN FOR SIM Avg.
accuracy Baseline 61.82%?????????? 89.10%??
????????61.82% ??????????71.82% ??????????82.64% ??????????85.05% ??????????62.01% ??????????89.10%??
????????89.36%???? ??????85.03%??????
????82.90%???????? ??88.97% ??????????88.56%??
????????85.12% ??????????88.74% Table 3: Accuracy with different feature sets.
SUF: Surface, LEX: Lexical, SYN: Syntax, FOR: Forum specific, SIM: similarity.
The baseline results from a majority class classifier.
We performed additional experiments to identify the most important features from the Forum specific ones.
Table 4 shows that IsMail and Quote Fraction are the dominant features.
This is noteworthy, as those features are not based on the domain of discussion.
Thus, we believe that these features will perform well in future experiments on other data.
ISM ISH QFR URL PAC Avg.
accuracy?????????? 85.05%??
????????73.30% ??????????61.82% ??????????73.76% ??????????61.29% ??????????61.82% ??????????74.41%??
????????85.05%???? ??????73.30%??????
????85.05%???????? ??85.05%??
????????84.99%?????? ????85.05% Table 4: Accuracy with different forum specific features.
ISM: IsMail, ISH: IsHTML, QFR: QuoteFraction, URL: URLCount, PAC: PathCount.
Error Analysis Table 5 shows the confusion matrix of the system using all features.
Many posts that were misclassified as good ones show no apparent reason to be classified as bad posts to us.
The understanding of their rating seems to require deep knowledge about the specific subject of discussion.
The few remaining posts are either spam or rated negatively to signalize dissent with the opinion expressed in the post.
Posts that were misclassified as bad ones often contain program code, digital signatures or other non-textual parts in the body.
We plan to address these issues with better preprocessing in 127 true good true bad sum pred.
good 490 72 562 pred.
bad 95 875 970 sum 585 947 1532 Table 5: Confusion matrix for the system using all features.
the future.
However, the relatively high accuracy already achieved shows that these issues are rare.
5 Conclusion
and Future Work Assessing post quality is an important problem for many forums on the web.
Currently, most forums need their users to rate the posts manually, which is error prone, labour intensive and last but not least may lead to the problem of premature negative consent (Lampe and Resnick, 2004).
We proposed an algorithm that has shown to be able to assess the quality of forum posts.
The algorithm applies state-of-the-art classification techniques using features such as Surface, Lexical, Syntactic, Forum specific and Similarity features to do so.
Our best performing system configuration achieves an accuracy of 89.1%, which is significantly higher than the baseline of 61.82%.
Our experiments show that forum specific features perform best.
However, slightly worse but still satisfactory performance can be obtained even without those.
So far, we have not made use of the structural information in forum threads yet.
We plan to perform experiments investigating speech act recognition in forums to improve the automatic quality assessment.
We also plan to apply our system to further domains of forum discussion, such as the discussions among active Wikipedia users.
We believe that the proposed algorithm will support important applications beyond content filtering like automatic summarization systems and forum specific search.
Acknowledgments This work was supported by the German Research Foundation as part of the Research Training Group ?Feedback-Based Quality Management in eLearning??under the grant 1223.
We are thankful to Nabble for providing their data.
References Yigal Attali and Jill Burstein.
2006. Automated essay scoring with e-rater v.2.
The Journal of Technology, Learning, and Assessment, 4(3), February.
Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: a library for support vector machines.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
Martin Chodorow and Jill Burstein.
2004. Beyond essay length: Evaluating e-raters performance on toefl essays.
Technical report, ETS.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard Hovy.
2006. Learning to detect conversation focus of threaded discussions.
In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NNACL).
Jihie Kim, Grace Chern, Donghui Feng, Erin Shaw, and Eduard Hovya.
2006a. Mining and assessing discussions on the web through speech act analysis.
In Proceedings of the Workshop on Web Content Mining with Human Language Technologies at the 5th International Semantic Web Conference.
Jihie Kim, Erin Shaw, Donghui Feng, Carole Beal, and Eduard Hovy.
2006b. Modeling and assessing student activities in on-line discussions.
In Proceedings of the Workshop on Educational Data Mining at the conference of the American Association of Artificial Intelligence (AAAI-06), Boston, MA.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco Penneacchiotti.
2006c. Automatically assessing review helpfulness.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 423430, Sydney, Australia, July.
Cliff Lampe and Paul Resnick.
2004. Slash(dot) and burn: Distributed moderation in a large online conversation space.
In Proceedings of ACM CHI 2004 Conference on Human Factors in Computing Systems, Vienna Austria, pages 543 550.
Mitchell P.
Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
1994. Building a Large Annotated Corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313??30.
Ingo Mierswa, Michael Wurst, Ralf Klinkenberg, Martin Scholz, and Timm Euler.
2006. YALE: Rapid prototyping for complex data mining tasks.
In KDD ??6: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 935??40, New York, NY, USA.
ACM Press.
Helmut Schmid.
1995. Probabilistic Part-of-Speech Tagging Using Decision Trees.
In International Conference on New Methods in Language Processing, Manchester, UK.
Salvatore Valenti, Francesca Neri, and Alessandro Cucchiarelli.
2003. An overview of current research on automated essay grading.
Journal of Information Technology Education, 2:319??29.
Ian H.
Witten and Eibe Frank.
2005. Data Mining: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2 edition.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 129??32, Prague, June 2007.
c2007 Association for Computational Linguistics WordNet-based Semantic Relatedness Measures in Automatic Speech Recognition for Meetings Michael Pucher Telecommunications Research Center Vienna Vienna, Austria Speech and Signal Processing Lab, TU Graz Graz, Austria pucher@ftw.at Abstract This paper presents the application of WordNet-based semantic relatedness measures to Automatic Speech Recognition (ASR) in multi-party meetings.
Different word-utterance context relatedness measures and utterance-coherence measures are defined and applied to the rescoring of Nbest lists.
No significant improvements in terms of Word-Error-Rate (WER) are achieved compared to a large word-based ngram baseline model.
We discuss our results and the relation to other work that achieved an improvement with such models for simpler tasks.
1 Introduction
As (Pucher, 2005) has shown different WordNetbased measures and contexts are best for word prediction in conversational speech.
The JCN (Section 2.1) measure performs best for nouns using the noun-context.
The LESK (Section 2.1) measure performs best for verbs and adjectives using a mixed word-context.
Text-based semantic relatedness measures can improve word prediction on simulated speech recognition hypotheses as (Demetriou et al., 2000) have shown.
(Demetriou et al., 2000) generated N-best lists from phoneme confusion data acquired from a speech recognizer, and a pronunciation lexicon.
Then sentence hypotheses of varying Word-ErrorRate (WER) were generated based on sentences from different genres from the British National Corpus (BNC).
It was shown by them that the semantic model can improve recognition, where the amount of improvement varies with context length and sentence length.
Thereby it was shown that these models can make use of long-term information.
In this paper the best performing measures from (Pucher, 2005), which outperform baseline models on word prediction for conversational telephone speech are used for Automatic Speech Recognition (ASR) in multi-party meetings.
Thereby we want to investigate if WordNet-based models can be used for rescoring of ?real??N-best lists in a difficult task.
1.1 Word
prediction by semantic similarity The standard n-gram approach in language modeling for speech recognition cannot cope with long-term dependencies.
Therefore (Bellegarda, 2000) proposed combining n-gram language models, which are effective for predicting local dependencies, with Latent Semantic Analysis (LSA) based models for covering long-term dependencies.
WordNet-based semantic relatedness measures can be used for word prediction using long-term dependencies, as in this example from the CallHome English telephone speech corpus: (1) B: I I well, you should see what the floorleftstudentsfloorright B: after they torture them for six floorleftyearsfloorright in middle floorleftschoolfloorright and high floorleftschoolfloorright they don?t want to do anything in floorleftcollegefloorright particular.
In Example 1 college can be predicted from the noun context using semantic relatedness measures, 129 here between students and college.
A 3-gram model gives a ranking of college in the context of anything in.
An 8-gram predicts college from they don?t want to do anything in, but the strongest predictor is students.
1.2 Test
data The JCN and LESK measure that are defined in the next section are used for N-best list rescoring.
For the WER experiments N-best lists generated from the decoding of conference room meeting test data of the NIST Rich Transcription 2005 Spring (RT05S) meeting evaluation (Fiscus et al., 2005) are used.
The 4-gram that has to be improved by the WordNet-based models is trained on various corpora from conversational telephone speech to web data that together contain approximately 1 billion words.
2 WordNet-based semantic relatedness measures 2.1 Basic measures Two similarity/distance measures from the Perl package WordNet-Similarity written by (Pedersen et al., 2004) are used.
The measures are named after their respective authors.
All measures are implemented as similarity measures.
JCN (Jiang and Conrath, 1997) is based on the information content, and LESK (Banerjee and Pedersen, 2003) allows for comparison across Part-of-Speech (POS) boundaries.
2.2 Word
context relatedness First the relatedness between words is defined based on the relatedness between senses.
S(w) are the senses of word w.
Definition 2 also performs wordsense disambiguation.
rel(w,wprime) = max ci?S(w) cj?S(wprime) rel(ci,cj) (2) The relatedness of a word and a context (relW) is defined as the average of the relatedness of the word and all words in the context.
relW(w,C) = 1| C | summationdisplay wi?C rel(w,wi) (3) 2.3 Word utterance (context) relatedness The performance of the word-context relatedness (Definition 3) shows how well the measures work for algorithms that proceed in a left-to-right manner, since the context is restricted to words that have already been seen.
For the rescoring of N-best lists it is not necessary to proceed in a left-to-right manner.
The word-utterance-context relatedness can be used for the rescoring of N-best lists.
This relatedness does not only use the context of the preceding words, but the whole utterance.
Suppose U = ?w1,...,wn??is an utterance.
Let pre(wi,U) be the set uniontextj<i wj and post(wi,U) be the set uniontextj>i wj.
Then the word-utterance-context relatedness is defined as relU1(wi,U,C) = relW(wi,pre(wi,U) ??post(wi,U) ??C). (4) In this case there are two types of context.
The first context comes from the respective meeting, and the second context comes from the actual utterance.
Another definition is obtained if the context C is eliminated (C = ?? and just the utterance context U is taken into account.
relU2(wi,U) = relW(wi,pre(wi,U) ??post(wi,U)) (5) Both definitions can be modified for usage with rescoring in a left-to-right manner by restricting the contexts only to the preceding words.
relU3(wi,U,C) = relW(wi,pre(wi,U) ??C) (6) relU4(wi,U) = relW(wi,pre(wi,U)) (7) 2.4 Defining utterance coherence Using Definitions 4-7 different concepts of utterance coherence can be defined.
For rescoring the utterance coherence is used, when a score for each element of an N-best list is needed.
U is again an utterance U = ?w1,...,wn?? 130 cohU1(U,C) = 1| U | summationdisplay w?U relU1(w,U,C) (8) The first semantic utterance coherence measure (Definition 8) is based on all words in the utterance as well as in the context.
It takes the mean of the relatedness of all words.
It is based on the wordutterance-context relatedness (Definition 4).
cohU2(U) = 1| U | summationdisplay w?U relU2(w,U) (9) The second coherence measure (Definition 9) is a pure inner-utterance-coherence, which means that no history apart from the utterance is needed.
Such a measure is very useful for rescoring, since the history is often not known or because there are speech recognition errors in the history.
It is based on Definition 5.
cohU3(U,C) = 1| U | summationdisplay w?U relU3(w,U,C) (10) The third (Definition 10) and fourth (Definition 11) definition are based on Definition 6 and 7, that do not take future words into account.
cohU4(U) = 1| U | summationdisplay w?U relU4(w,U) (11) 3 Word-error-rate (WER) experiments For the rescoring experiments the first-best element of the previous N-best list is added to the context.
Before applying the WordNet-based measures, the N-best lists are POS tagged with a decision tree tagger (Schmid, 1994).
The WordNet measures are then applied to verbs, nouns and adjectives.
Then the similarity values are used as scores, which have to be combined with the language model scores of the N-best list elements.
The JCN measure is used for computing a noun score based on the noun context, and the LESK measure is used for computing a verb/adjective score based on the noun/verb/adjective context.
In the end there is a lesk score and a jcn score for each N-best list.
The final WordNet score is the sum of the two scores.
The log-linear interpolation method used for the rescoring is defined as p(S) ??pwordnet(S) pn-gram(S)1??(12) where ??denotes normalization.
Based on all WordNet scores of an N-best list a probability is estimated, which is then interpolated with the n-gram model probability.
If only the elements in an Nbest list are considered, log-linear interpolation can be used since it is not necessary to normalize over all sentences.
Then there is only one parameter  to optimize, which is done with a brute force approach.
For this optimization a small part of the test data is taken and the WER is computed for different values of .
As a baseline the n-gram mixture model trained on all available training data (??1 billion words) is used.
It is log-linearly interpolated with the WordNet probabilities.
Additionally to this sophisticated interpolation, solely the WordNet scores are used without the n-gram scores.
3.1 WER
experiments for inner-utterance coherence In this first group of experiments Definitions 8 and 9 are applied to the rescoring task.
Similarity scores for each element in an N-best list are derived according to the definitions.
The first-best element of the last list is always added to the context.
The context size is constrained to the last 20 words.
Definition 8 includes context apart from the utterance context, Definition 9 only uses the utterance context.
No improvement over the n-gram baseline is achieved for these two measures.
Neither with the log-linearly interpolated models nor with the WordNet scores alone.
The differences between the methods in terms of WER are not significant.
3.2 WER
experiments for utterance coherence In the second group of experiments Definitions 10 and 11 are applied to the rescoring task.
There is again one measure that uses dialog context (10) and one that only uses utterance context (11).
Also for these experiments no improvement over the n-gram baseline is achieved.
Neither with the 131 log-linearly interpolated models nor with the WordNet scores alone.
The differences between the methods in terms of WER are also not significant.
There are also no significant differences in performance between the second group and the first group of experiments.
4 Summary
and discussion We showed how to define more and more complex relatedness measures on top of the basic relatedness measures between word senses.
The LESK and JCN measures were used for the rescoring of N-best lists.
It was shown that speech recognition of multi-party meetings cannot be improved compared to a 4-gram baseline model, when using WordNet models.
One reason for the poor performance of the models could be that the task of rescoring simulated Nbest lists, as presented in (Demetriou et al., 2000), is significantly easier than the rescoring of ?real??Nbest lists.
(Pucher, 2005) has shown that WordNet models can outperform simple random models on the task of word prediction, in spite of the noise that is introduced through word-sense disambiguation and POS tagging.
To improve the wordsense disambiguation one could use the approach proposed by (Basili et al., 2004).
In the above WER experiments a 4-gram baseline model was used, which was trained on nearly 1 billion words.
In (Demetriou et al., 2000) a simpler baseline has been used.
650 sentences were used there to generate sentence hypotheses with different WER using phoneme confusion data and a pronunciation lexicon.
Experiments with simpler baseline models ignore that these simpler models are not used in today?s recognition systems.
We think that these prediction models can still be useful for other tasks where only small amounts of training data are available.
Another possibility of improvement is to use other interpolation techniques like the maximum entropy framework.
WordNetbased models could also be improved by using a trigger-based approach.
This could be done by not using the whole WordNet and its similarities, but defining word-trigger pairs that are used for rescoring.
5 Acknowledgements
This work was supported by the European Union 6th FP IST Integrated Project AMI (Augmented Multiparty Interaction, and by Kapsch Carrier-Com AG and Mobilkom Austria AG together with the Austrian competence centre programme Kplus.
References Satanjeev Banerjee and Ted Pedersen.
2003. Extended gloss overlaps as a measure of semantic relatedness.
In Proceedings of the 18th Int.
Joint Conf.
on Artificial Intelligence, pages 805??10, Acapulco.
Roberto Basili, Marco Cammisa, and Fabio Massimo Zanzotto.
2004. A semantic similarity measure for unsupervised semantic tagging.
In Proc.
of the Fourth International Conference on Language Resources and Evaluation (LREC2004), Lisbon, Portugal.
Jerome Bellegarda.
2000. Large vocabulary speech recognition with multispan statistical language models.
IEEE Transactions on Speech and Audio Processing, 8(1), January.
G. Demetriou, E.
Atwell, and C.
Souter. 2000.
Using lexical semantic knowledge from machine readable dictionaries for domain independent language modelling.
In Proc.
of LREC 2000, 2nd International Conference on Language Resources and Evaluation.
Jonathan G.
Fiscus, Nicolas Radde, John S.
Garofolo, Audrey Le, Jerome Ajot, and Christophe Laprun.
2005. The rich transcription 2005 spring meeting recognition evaluation.
In Rich Transcription 2005 Spring Meeting Recognition Evaluation Workshop, Edinburgh, UK.
Jay J.
Jiang and David W.
Conrath. 1997.
Semantic similarity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference on Research in Computational Linguistics, Taiwan.
Ted Pedersen, S.
Patwardhan, and J.
Michelizzi. 2004.
WordNet::Similarity Measuring the relatedness of concepts.
In Proc.
of Fifth Annual Meeting of the North American Chapter of the ACL (NAACL-04), Boston, MA.
Michael Pucher.
2005. Performance evaluation of WordNet-based semantic relatedness measures for word prediction in conversational speech.
In IWCS 6, Sixth International Workshop on Computational Semantics, Tilburg, Netherlands.
H Schmid.
1994. Probabilistic part-of-speech tagging using decision trees.
In Proceedings of International Conference on New Methods in Language Processing, Manchester, UK, September .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 133??36, Prague, June 2007.
c2007 Association for Computational Linguistics Building Emotion Lexicon from Weblog Corpora Changhua Yang Kevin Hsin-Yih Lin Hsin-Hsi Chen Department of Computer Science and Information Engineering National Taiwan University #1 Roosevelt Rd.
Sec. 4, Taipei, Taiwan 106 {d91013, f93141, hhchen}@csie.ntu.edu.tw Abstract An emotion lexicon is an indispensable resource for emotion analysis.
This paper aims to mine the relationships between words and emotions using weblog corpora.
A collocation model is proposed to learn emotion lexicons from weblog articles.
Emotion classification at sentence level is experimented by using the mined lexicons to demonstrate their usefulness.
1 Introduction
Weblog (blog) is one of the most widely used cybermedia in our internet lives that captures and shares moments of our day-to-day experiences, anytime and anywhere.
Blogs are web sites that timestamp posts from an individual or a group of people, called bloggers.
Bloggers may not follow formal writing styles to express emotional states.
In some cases, they must post in pure text, so they add printable characters, such as ??-)??(happy) and ??-(??(sad), to express their feelings.
In other cases, they type sentences with an internet messengerstyle interface, where they can attach a special set of graphic icons, or emoticons.
Different kinds of emoticons are introduced into text expressions to convey bloggers??emotions.
Since thousands of blog articles are created everyday, emotional expressions can be collected to form a large-scale corpus which guides us to build vocabularies that are more emotionally expressive.
Our approach can create an emotion lexicon free of laborious efforts of the experts who must be familiar with both linguistic and psychological knowledge.
2 Related
Works Some previous works considered emoticons from weblogs as categories for text classification.
Mishne (2005), and Yang and Chen (2006) used emoticons as tags to train SVM (Cortes and Vapnik, 1995) classifiers at document or sentence level.
In their studies, emoticons were taken as moods or emotion tags, and textual keywords were taken as features.
Wu et al.(2006) proposed a sentencelevel emotion recognition method using dialogs as their corpus.
?Happy, ?Unhappy?? or ?Neutral?? was assigned to each sentence as its emotion category.
Yang et al.(2006) adopted Thayer?s model (1989) to classify music emotions.
Each music segment can be classified into four classes of moods.
In sentiment analysis research, Read (2005) used emoticons in newsgroup articles to extract instances relevant for training polarity classifiers.
3 Training
and Testing Blog Corpora We select Yahoo!
Kimo Blog1 posts as our source of emotional expressions.
Yahoo! Kimo Blog service has 40 emoticons which are shown in Table 1.
When an editing article, a blogger can insert an emoticon by either choosing it or typing in the corresponding codes.
However, not all articles contain emoticons.
That is, users can decide whether to insert emoticons into articles/sentences or not.
In this paper, we treat these icons as emotion categories and taggings on the corresponding text expressions.
The dataset we adopt consists of 5,422,420 blog articles published at Yahoo!
Kimo Blog from January to July, 2006, spanning a period of 212 days.
In total, 336,161 bloggers??articles were collected.
Each blogger posts 16 articles on average.
We used the articles from January to June as the training set and the articles in July as the testing set.
Table 2 shows the statistics of each set.
On average, 14.10% of the articles contain emotion-tagged expressions.
The average length of articles with tagged emotions, i.e., 272.58 characters, is shorter 1 http://tw.blog.yahoo.com/ 133 than that of articles without tagging, i.e., 465.37 characters.
It seems that people tend to use emoticons to replace certain amount of text expressions to make their articles more succinct.
Figure 1 shows the three phases for the construction and evaluation of emotion lexicons.
In phase 1, 1,185,131 sentences containing only one emoticon are extracted to form a training set to build emotion lexicons.
In phase 2, sentence-level emotion classifiers are constructed using the mined lexicons.
In phase 3, a testing set consisting of 307,751 sentences is used to evaluate the classifiers.
4 Emotion
Lexicon Construction The blog corpus contains a collection of bloggers?? emotional expressions which can be analyzed to construct an emotion lexicon consisting of words that collocate with emoticons.
We adopt a variation of pointwise mutual information (Manning and Schtze, 1999) to measure the collocation strength co(e,w) between an emotion e and a word w: )()( ),(log),(),o( wPeP wePwecwec = (1) where P(e,w)=c(e,w)/N, P(e)=c(e)/N, P(w)=c(w)/N, c(e) and c(w) are the total occurrences of emoticon e and word w in a tagged corpus, respectively, c(e,w) is total co-occurrences of e and w, and N denotes the total word occurrences.
A word entry of a lexicon may contain several emotion senses.
They are ordered by the collocation strength co.
Figure 2 shows two Chinese example words, ???(ha1ha1) and ???
(ke3wu4). The former collocates with ?laughing?? and ?big grin??emoticons with collocation strength 25154.50 and 2667.11, respectively.
Similarly, the latter collocates with ?angry??and ?phbbbbt??
When all collocations (i.e., word-emotion pairs) are listed in a descending order of co, we can choose top n collocations to build an emotion lexicon.
In this paper, two lexicons (Lexicons A and B) are extracted by setting n to 25k and 50k.
Lexicon A contains 4,776 entries with 25,000 sense pairs and Lexicon B contains 11,243 entries and 50,000 sense pairs.
5 Emotion
Classification Suppose a sentence S to be classified consists of n emotion words.
The emotion of S is derived by a mapping from a set of n emotion words to m emotion categories as follows: },...,{?},...,{ 11 m tionclassifican eeeewewS ???
Table 1.
Yahoo! Kimo Blog Emoticon Set.
ID Emoticon Code Description ID Emoticon Code Description ID Emoticon Code Description ID Emoticon Code Description 1 :) happy 11 :O surprise 21 0:) angel 31 (:| yawn 2 :( sad 12 X-( angry 22 :-B nerd 32 =P~ drooling 3 ;) winking 13 :> smug 23 =; talk to the hand 33 :-? thinking 4 :D big grin 14 B-) cool 24 I-) asleep 34 ;)) hee hee 5 ;;) batting eyelashes 15 :-S worried 25 8-) rolling eyes 35 =D> applause 6 :-/ confused 16 >:) devil 26 :-& sick 36 [-o< praying 7 :x love struck 17 :(( crying 27 :-$ don't tell anyone 37 :-< sigh 8 :?? blushing 18 :)) laughing 28 [-( not talking 38 >:P phbbbbt 9 :p tongue 19 :| straight face 29 :o) clown 39 @};rose 10 :* kiss 20 /:) raised eyebrow 30 @-) hypnotized 40 :@) pig Table 2.
Statistics of the Weblog Dataset.
Dataset Article # Tagged # Percentage Tagged Len.
Untagged L.
Training 4,187,737 575,009 13.86% 269.77 chrs.
468.14 chrs.
Testing 1,234,683 182,999 14.92% 281.42 chrs.
455.82 chrs.
Total 5,422,420 764,788 14.10% 272.58 chrs.
465.37 chrs.
Testing Set Figure 1.
Emotion Lexicon Construction and Evaluation.
Extraction Blog Articles Features Classifiers Evaluation Lexicon Construction Training Set Phase 2 Phase 3 Emotion Lexicon Phase 1 134 For each emotion word ewi, we may find several emotion senses with the corresponding collocation strength co by looking up the lexicon.
Three alternatives are proposed as follows to label a sentence S with an emotion: (a) Method 1 (1) Consider all senses of ewi as votes.
Label S with the emotion that receives the most votes.
(2) If more than two emotions get the same number of votes, then label S with the emotion that has the maximum co.
(b) Method 2 Collect emotion senses from all ewi.
Label S with the emotion that has the maximum co.
(c) Method 3 The same as Method 1 except that each ewi votes only one sense that has the maximum co.
In past research, the approach used by Yang et al.(2006) was based on the Thayer?s model (1989), which divided emotions into 4 categories.
In sentiment analysis research, such as Read?s study (2006), a polarity classifier separated instances into positive and negative classes.
In our experiments, we not only adopt fine-grain classification, but also coarse-grain classification.
We first select 40 emoticons as a category set, and also adopt the Thayer?s model to divide the emoticons into 4 quadrants of the emotion space.
As shown in Figure 3, the top-right side collects the emotions that are more positive and energetic and the bottom-left side is more negative and silent.
A polarity classifier uses the right side as positive and the left side as negative.
6 Evaluation
Table 3 shows the performance under various combinations of lexicons, emotion categories and classification methods.
?Hit #??stands for the number of correctly-answered instances.
The baseline represents the precision of predicting the majority category, such as ?happy??or ?positive?? as the answer.
The baseline method?s precision increases as the number of emotion classes decreases.
The upper bound recall indicates the upper limit on the fraction of the 307,751 instances solvable by the corresponding method and thus reflects the limitation of the method.
The closer a method?s actual recall is to the upper bound recall, the better the method.
For example, at most 40,855 instances (14.90%) can be answered using Method 1 in combination with Lexicon A.
But the actual recall is 4.55% only, meaning that Method 1?s recall is more than 10% behind its upper bound.
Methods which have a larger set of candidate answers have higher upper bound recalls, because the probability that the correct answer is in their set of candidate answers is greater.
Experiment results show that all methods utilizing Lexicon A have performance figures lower than the baseline, so Lexicon A is not useful.
In contrast, Lexicon B, which provides a larger collection of vocabularies and emotion senses, outperforms Lexicon A and the baseline.
Although Method 3 has the smallest candidate answer set and thus has the smallest upper bound recall, it outperforms the other two methods in most cases.
Method 2 achieves better precisions when using ???? (ha1ha1) ?hah hah??
Sense 1.
(laughing) ??co: 25154.50 e.g., ?...
??? ?hah hah??
I am getting lucky~??
Sense 2.
(big grin) ??co: 2667.11 e.g., ??~? ?I only memorized vowels today~ haha ??
???????? (ke3wu4) ?darn??
Sense 1.
(angry) ??co: 2797.82 e.g., ????..?? ?What's the hacker doing... darn it ??
Sense 2.
(phbbbbt) ??co: 619.24 e.g., ??????
?Damn those aliens ??
Figure 2.
Some Example Words in a Lexicon.
Arousal (energetic) Valence (negative) (positive) (silent) unassigned: Figure 3.
Emoticons on Thayer?s model.
135 Thayer?s emotion categories.
Method 1 treats the vote to every sense equally.
Hence, it loses some differentiation abilities.
Method 1 performs the best in the first case (Lexicon A, 40 classes).
We can also apply machine learning to the dataset to train a high-precision classification model.
To experiment with this idea, we adopt LIBSVM (Fan et al., 2005) as the SVM kernel to deal with the binary polarity classification problem.
The SVM classifier chooses top k (k = 25, 50, 75, and 100) emotion words as features.
Since the SVM classifier uses a small feature set, there are testing instances which do not contain any features seen previously by the SVM classifier.
To deal with this problem, we use the class prediction from Method 3 for any testing instances without any features that the SVM classifier can recognize.
In Table 4, the SVM classifier employing 25 features has the highest precision.
On the other hand, the SVM classifier employing 50 features has the highest F measure when used in conjunction with Method 3.
7 Conclusion
and Future Work Our methods for building an emotional lexicon utilize emoticons from blog articles collaboratively contributed by bloggers.
Since thousands of blog articles are created everyday, we expect the set of emotional expressions to keep expanding.
In the experiments, the method of employing each emotion word to vote only one emotion category achieves the best performance in both fine-grain and coarse-grain classification.
Acknowledgment Research of this paper was partially supported by Excellent Research Projects of National Taiwan University, under the contract of 95R0062-AE0002.
We thank Yahoo!
Taiwan Inc.
for providing the dataset for researches.
References Corinna Cortes and V.
Vapnik. 1995.
Support-Vector Network.
Machine Learning, 20:273??97.
Rong-En Fan, Pai-Hsuen Chen and Chih-Jen Lin.
2005. Working Set Selection Using Second Order Information for Training Support Vector Machines.
Journal of Machine Learning Research, 6:1889??918.
Gilad Mishne.
2005. Experiments with Mood Classification in Blog Posts.
Proceedings of 1st Workshop on Stylistic Analysis of Text for Information Access.
Jonathon Read.
2005. Using Emotions to Reduce Dependency in Machine Learning Techniques for Sentiment Classification.
Proceedings of the ACL Student Research Workshop, 43-48.
Robert E.
Thayer. 1989.
The Biopsychology of Mood and Arousal, Oxford University Press.
Changhua Yang and Hsin-Hsi Chen.
2006. A Study of Emotion Classification Using Blog Articles.
Proceedings of Conference on Computational Linguistics and Speech Processing, 253-269.
Yi-Hsuan Yang, Chia-Chu Liu, and Homer H.
Chen. 2006.
Music Emotion Classification: A Fuzzy Approach.
Proceedings of ACM Multimedia, 81-84.
Chung-Hsien Wu, Ze-Jing Chuang, and Yu-Chung Lin.
2006. Emotion Recognition from Text Using Semantic Labels and Separable Mixture Models.
ACM Transactions on Asian Language Information Processing, 5(2):165-182.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 137??40, Prague, June 2007.
c2007 Association for Computational Linguistics Construction of Domain Dictionary for Fundamental Vocabulary Chikara Hashimoto Faculty of Engineering, Yamagata University 4-3-16 Jonan, Yonezawa-shi, Yamagata, 992-8510 Japan Sadao Kurohashi Graduate School of Informatics, Kyoto University 36-1 Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501 Japan Abstract For natural language understanding, it is essential to reveal semantic relations between words.
To date, only the IS-A relation has been publicly available.
Toward deeper natural language understanding, we semiautomatically constructed the domain dictionary that represents the domain relation between Japanese fundamental words.
This is the first Japanese domain resource that is fully available.
Besides, our method does not require a document collection, which is indispensable for keyword extraction techniques but is hard to obtain.
As a task-based evaluation, we performed blog categorization.
Also, we developed a technique for estimating the domain of unknown words.
1 Introduction
We constructed a lexical resource that represents the domain relation among Japanese fundamental words (JFWs), and we call it the domain dictionary.1 It associates JFWs with domains in which they are typically used.
For example, a0a2a1a4a3a6a5a6a7 home run is associated with the domain SPORTS2.
That is, we aim to make explicit the horizontal relation between words, the domain relation, while thesauri indicate the vertical relation called IS-A.3 1In fact, there have been a few domain resources in Japanese like Yoshimoto et al.(1997). But they are not publicly available.
2Domains are CAPITALIZED in this paper.
3The lack of the horizontal relationship is also known as the ?tennis problem??(Fellbaum, 1998, p.10). 2 Two Issues You have to address two issues.
One is what domains to assume, and the other is how to associate words with domains without document collections.
The former is paraphrased as how people categorize the real world, which is really a hard problem.
In this study, we avoid being too involved in the problem and adopt a simple domain system that most people can agree on, which is as follows: CULTURE RECREATION SPORTS HEALTH LIVING DIET TRANSPORTATION EDUCATION SCIENCE BUSINESS MEDIA GOVERNMENT It has been created based on web directories such as Open Directory Project with some adjustments.
In addition, NODOMAIN was prepared for those words that do not belong to any particular domain.
As for the latter issue, you might use keyword extraction techniques; identifying words that represent a domain from the document collection using statistical measures like TF*IDF and matching between extracted words and JFWs.
However, you will find that document collections of common domains such as those assumed here are hard to obtain.4 Hence, we had to develop a method that does not require document collections.
The next section details it.
4Initially, we tried collecting web pages in Yahoo!
JAPAN. However, we found that most of them were index pages with a few text contents, from which you cannot extract reliable keywords.
Though we further tried following links in those index pages to acquire enough texts, extracted words turned out to be site-specific rather than domain-specific since many pages were collected from a particular web site.
137 Table 1: Examples of Keywords for each Domain Domain Examples of Keywords CULTURE a0a2a1 movie, a3a5a4 music RECREATION a6a5a7 tourism, a8a10a9 firework SPORTS a11a13a12 player, a14a5a15 baseball HEALTH a12a13a16 surgery, a17a19a18 diagnosis LIVING a20a13a21 childcare, a22a10a23 furniture DIET a24 chopsticks, a25a27a26 lunch TRANSPORTATION a28 station, a29a13a30 road EDUCATION a31a10a32 teacher, a33a5a34 arithmetic SCIENCE a35a10a36 research, a37a13a38 theory BUSINESS a39a13a40 import, a41a10a42 market MEDIA a43a13a44 broadcast, a45a27a46 reporter GOVERNMENT a47a13a48 judicatory, a49 tax 3 Domain Dictionary Construction To identify which domain a JFW is associated with, we use manually-prepared keywords for each domain rather than document collections.
The construction process is as follows: 1 Preparing keywords for each domain (3.1).
2 Associating
JFWs with domains (3.2).
3 Reassociating
JFWs with NODOMAIN (3.3).
4 Manual
correction (3.5).
3.1 Preparing
Keywords for each Domain About 20 keywords for each domain were collected manually from words that appear most frequently in the Web.
Table 1 shows examples of the keywords.
3.2 Associating
JFWs with Domains A JFW is associated with a domain of the highest Ad score.
An Ad score of domain is calculated by summing up the top five Ak scores of the domain.
Then, an Ak score, which is defined between a JFW and a keyword of a domain, is a measure that shows how strongly the JFW and the keyword are related (Figure 1).
Assuming that two words are related if they cooccur more often than chance in a corpus, we adopt the ?2 statistics to calculate an Ak score and use web pages as a corpus.
The number of co-occurrences is approximated by the number of search engine hits when the two words are used as queries.
Among various alternatives, the combination of the ?2 statistics and web pages is adopted following Sasaki et al.(2006). Based on Sasaki et al.(2006), Ak score between JFWs JFW1 JFW2 JFW3  DOMAIN1 kw1a kw1b  DOMAIN2 kw2a kw2b   Adscore JFWm kwna kwnb  DOMAINn Ak scores Figure 1: Associating JFWs with Domains a JFW (jw) and a keyword (kw) is given as below.
Ak(jw,kw) = n(ad ??bc) 2 (a + b)(c + d)(a + c)(b + d) where n is the total number of Japanese web pages, a = hits(jw & kw), b = hits(jw) ??a, c = hits(kw) ??a, d = n ??(a + b + c).
Note that hits(q) represents the number of search engine hits when q is used as a query.
3.3 Reassociating
JFWs with NODOMAIN JFWs that do not belong to any particular domain, i.e. whose highest Ad score is low should be reassociated with NODOMAIN.
Thus, a threshold for determining if a JFW?s highest Ad score is low is required.
The threshold for a JFW (jw) needs to be changed according to hits(jw); the greater hits(jw) is, the higher the threshold should be.
To establish a function that takes jw and returns the appropriate threshold for it, the following semiautomatic process is required after all JFWs are associated with domains: (i) Sort all tuples of the form < jw, hits(jw), the highest Ad of the jw > by hits(jw).5 (ii) Segment the tuples.
(iii) For each segment, extract manually tuples whose jw should be associated with one of the 12 domains and those whose jw should be deemed as NODOMAIN.
Note that the former tuples usually have higher Ad scores than the latter tuples.
(iv) For each segment, identify a threshold that distinguishes between the former tuples and the latter tuples by their Ad scores.
At this point, pairs of the number of hits (represented by each segment) and the appropriate threshold for it are obtained.
(v) Approximate the relation between 5Note that we acquire the number of search engine hits and the Ad score for each jw in the process 2. 138 the number of hits and its threshold by a linear function using least-square method.
Finally, this function indicates the appropriate threshold for each jw.
3.4 Performance
of the Proposed Method We applied the method to JFWs installed on JUMAN (Kurohashi et al., 1994), which are 26,658 words consisting of commonly used nouns and verbs.
As an evaluation, we sampled 380 pairs of a JFW and its domain, and measured accuracy.6 As a result, the proposed method attained the accuracy of 81.3% (309/380).
3.5 Manual
Correction Our policy is that simpler is better.
Thus, as one of our guidelines for manual correction, we avoid associating a JFW with multiple domains as far as possible.
JFWs to associate with multiple domains are restricted to those that are EQUALLY relevant to more than one domain.
4 Blog
Categorization As a task-based evaluation, we categorized blog articles into the domains assumed here.
4.1 Categorization
Method (i) Extract JFWs from the article.
(ii) Classify the extracted JFWs into the domains using the domain dictionary.
(iii) Sort the domains by the number of JFWs classified in descending order.
(iv) Categorize the article as the top domain.
If the top domain is NODOMAIN, the article is categorized as the second domain under the condition below.
|W(2ND DOMAIN)|  |W(NODOMAIN)| > 0.03 where |W(D)| is the number of JFWs classified into the domain D.
4.2 Data
We prepared two blog collections; Bcontrolled and Brandom.
As Bcontrolled, 39 blog articles were collected (3 articles for each domain including NODOMAIN) by the following procedure: (i) Query the Web using a keyword of the domain.7 (ii) From 6In the evaluation, one of the authors judged the correctness of each pair.
7To collect articles that are categorized as NODOMAIN, we used a0a2a1 diary as a query.
Table 2: Breakdown of Brandom Domain # CULTURE 4 RECREATION 1 SPORTS 3 HEALTH 1 Domain # DIET 4 BUSINESS 12 NODOMAIN 5 the top of the search result, collect 3 articles that meet the following conditions; there are enough text contents in it, and people can confidently make a judgment about which domain it is categorized as.
As Brandom, 30 articles were randomly sampled from the Web.
Table 2 shows its breakdown.
Note that we manually removed peripheral contents like author profiles or banner advertisements from the articles in both Bcontrolled and Brandom.
4.3 Result
We measured the accuracy of blog categorization.
As a result, the accuracy of 89.7% (35/39) was attained in categorizing Bcontrolled, while Brandom was categorized with 76.6% (23/30) accuracy.
5 Domain
Estimation for Unknown Words We developed an automatic way of estimating the domain of unknown word (uw) using the dictionary.
5.1 Estimation
Method (i) Search the Web by using uw as a query.
(ii) Retrieve the top 30 documents of the search result.
(iii) Categorize the documents as one of the domains by the method described in 4.1.
(iv) Sort the domains by the number of documents in descending order.
(v) Associate uw with the top domain.
5.2 Experimental
Condition (i) Select 10 words from the domain dictionary for each domain.
(ii) For each word, estimate its domain by the method in 5.1 after removing the word from the dictionary so that the word is unknown.
5.3 Result
Table 3 shows the number of correctly domainestimated words (out of 10) for each domain.
Accordingly, the total accuracy is 67.5% (81/120).
139 Table 3: # of Correctly Domain-estimated Words Domain # CULTURE 7 RECREATION 4 SPORTS 9 HEALTH 9 LIVING 3 DIET 7 Domain # TRANSPORTATION 7 EDUCATION 9 SCIENCE 6 BUSINESS 9 MEDIA 2 GOVERNMENT 9 As for the poor accuracy for RECREATION, LIVING, and MEDIA, we found that it was due to either the ambiguous nature of the words of domain or a characteristic of the estimation method.
The former brought about the poor accuracy for MEDIA.
That is, some words of MEDIA are often used in other contexts.
For example, a0a2a1 live coverage is often used in the SPORTS context.
On the other hand, the method worked poorly for RECREATION and LIVING for the latter reason; the method exploits the Web.
Namely, some words of the domains, such as a3a5a4 tourism and a6a5a7 a7a9a8 a1 shampoo, are often used in the web sites of companies (BUSINESS) that provide services or goods related to RECREATION or LIVING.
As a result, the method tends to wrongly associate those words with BUSINESS.
6 Related
Work HowNet (Dong and Dong, 2006) and WordNet provide domain information for Chinese and English, but there has been no domain resource for Japanese that are publicly available.8 Domain dictionary construction methods that have been developed so far are all based on highly structured lexical resources like LDOCE or WordNet (Guthrie et al., 1991; Agirre et al., 2001) and hence not applicable to languages for which such highly structured lexical resources are not available.
Accordingly, contributions of this study are twofold: (i) We constructed the first Japanese domain dictionary that is fully available.
(ii) We developed the domain dictionary construction method that requires neither document collections nor highly structured lexical resources.
8Some human-oriented dictionaries provide domain information.
However, domains they cover are all technical ones rather than common domains such as those assumed here.
7 Conclusion
Toward deeper natural language understanding, we constructed the first Japanese domain dictionary that contains 26,658 JFWs.
Our method requires neither document collections nor structured lexical resources.
The domain dictionary can satisfactorily classify blog articles into the 12 domains assumed in this study.
Also, the dictionary can reliably estimate the domain of unknown words except for words that are ambiguous in terms of domains and those that appear frequently in web sites of companies.
Among our future work is to deal with domain information of multiword expressions.
For example, a10a5a11 fount and a12a14a13 collection constitute a10a14a11 a12a14a13 tax deduction at source.
Note that while a10a5a11 itself belongs to NODOMAIN, a10a15a11 a12a14a13 should be associated with GOVERNMENT.
Also, we will install the domain dictionary on JUMAN (Kurohashi et al., 1994) to make the domain information fully and easily available.
References Eneko Agirre, Olatz Ansa, David Martinez, and Ed Hovy.
2001. Enriching wordnet concepts with topic signatures.
In Proceedings of the SIGLEX Workshop on ?WordNet and Other Lexical Resources: Applications, Extensions, and Customizations??in conjunction with NAACL.
Zhendong Dong and Qiang Dong.
2006. HowNet And the Computation of Meaning.
World Scientific Pub Co Inc.
Christiane Fellbaum.
1998. WordNet: An Electronic Lexical Database.
MIT Press.
Joe A.
Guthrie, Louise Guthrie, Yorick Wilks, and Homa Aidinejad.
1991. Subject-Dependent Co-Occurence and Word Sense Disambiguation.
In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 146??52.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and Makoto Nagao.
1994. Improvements of Japanese Mophological Analyzer JUMAN.
In Proceedings of the International Workshop on Sharable Natural Language Resources, pages 22??8.
Yasuhiro Sasaki, Satoshi Sato, and Takehito Utsuro.
2006. Related Term Collection.
Journal of Natural Language Processing, 13(3):151??76.
(in Japanese).
Yumiko Yoshimoto, Satoshi Kinoshita, and Miwako Shimazu.
1997. Processing of proper nouns and use of estimated subject area for web page translation.
In tmi97, pages 10??8, Santa Fe .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 141??44, Prague, June 2007.
c2007 Association for Computational Linguistics Extracting Word Sets with Non-Taxonomical Relation Eiko Yamamoto Hitoshi Isahara Computational Linguistics Group National Institute of Information and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan {eiko, isahara}@nict.go.jp Abstract At least two kinds of relations exist among related words: taxonomical relations and thematic relations.
Both relations identify related words useful to language understanding and generation, information retrieval, and so on.
However, although words with taxonomical relations are easy to identify from linguistic resources such as dictionaries and thesauri, words with thematic relations are difficult to identify because they are rarely maintained in linguistic resources.
In this paper, we sought to extract thematically (non-taxonomically) related word sets among words in documents by employing case-marking particles derived from syntactic analysis.
We then verified the usefulness of word sets with non-taxonomical relation that seems to be a thematic relation for information retrieval.
1. Introduction Related word sets are useful linguistic resources for language understanding and generation, information retrieval, and so on.
In previous research on natural language processing, many methodologies for extracting various relations from corpora have been developed, such as the ?is-a??relation (Hearst 1992), ?part-of??relation (Berland and Charniak 1999), causal relation (Girju 2003), and entailment relation (Geffet and Dagan 2005).
Related words can be used to support retrieval in order to lead users to high-quality information.
One simple method is to provide additional words related to the key words users have input, such as an input support function within the Google search engine.
What kind of relation between the key words that have been input and the additional word is effective for information retrieval?
As for the relations among words, at least two kinds of relations exist: the taxonomical relation and the thematic relation.
The former is a relation representing the physical resemblance among objects, which is typically a semantic relation such as a hierarchal, synonymic, or antonymic relation; the latter is a relation between objects through a thematic scene, such as ?milk??and ?cow??as recollected in the scene ?milking a cow,??and ?milk?? and ?baby,??as recollected in the scene ?giving baby milk,??which include causal relation and entailment relation.
Wisniewski and Bassok (1999) showed that both relations are important in recognizing those objects.
However, while taxonomical relations are comparatively easy to identify from linguistic resources such as dictionaries and thesauri, thematic relations are difficult to identify because they are rarely maintained in linguistic resources.
In this paper, we sought to extract word sets with a thematic relation from documents by employing case-marking particles derived from syntactic analysis.
We then verified the usefulness of word sets with non-taxonomical relation that seems to be a thematic relation for information retrieval.
2. Method In order to derive word sets that direct users to obtain information, we applied a method based on the Complementary Similarity Measure (CSM), which can determine a relation between two words in a corpus by estimating inclusive relations between two vectors representing each appearance pattern for each words (Yamamoto et al.2005). 141 We first extracted word pairs having an inclusive relation between the words by calculating the CSM values.
Extracted word pairs are expressed by a tuple <w i, w j >, where CSM(V i, V j ) is greater than CSM(V j, V i ) when words w i and w j have each appearance pattern represented by each binary vector V i and V j. Then, we connected word pairs with CSM values greater than a certain threshold and constructed word sets.
A feature of the CSM-based method is that it can extract not only pairs of related words but also sets of related words because it connects tuples consistently.
Suppose we have <A, B>, <B, C>, <Z, B>, <C, D>, <C, E>, and <C, F> in the order of their CSM values, which are greater than the threshold.
For example, let <B, C> be an initial word set {B, C}.
First, we find the tuple with the greatest CSM value among the tuples in which the word C at the tail of the current word set is the left word, and connect the right word behind C.
In this example, word ?D??is connected to {B, C} because <C, D> has the greatest CSM value among the three tuples <C, D>, <C, E>, and <C, F>, making the current word set {B, C, D}.
This process is repeated until no tuples exist.
Next, we find the tuple with the greatest CSM value among the tuples in which the word B at the head of the current word set is the right word, and connect the left word before B.
This process is repeated until no tuples exist.
In this example, we obtain the word set {A, B, C, D}.
Finally, we removed ones with a taxonomical relation by using thesaurus.
The rest of the word sets have a non-taxonomical relation ??including a thematic relation ??among the words.
We then extracted those word sets that do not agree with the thesaurus as word sets with a thematic relation.
3. Experiment In our experiment, we used domain-specific Japanese documents within the medical domain (225,402 sentences, 10,144 pages, 37MB) gathered from the Web pages of a medical school and the 2005 Medical Subject Headings (MeSH) thesaurus 1 . Recently, there has been a study on query expansion with this thesaurus as domain information (Friberg 2007).
1 The
U.S.
National Library of Medicine created, maintains, and provides the MeSH  thesaurus.
We extracted word sets by utilizing inclusive relations of the appearance pattern between words based on a modified/modifier relationship in documents.
The Japanese language has casemarking particles that indicate the semantic relation between two elements in a dependency relation.
Then, we collected from documents dependency relations matching the following five patterns; ?A <no (of)> B,???P <wo (object)> V,???Q <ga (subject)> V,???R <ni (dative)> V,??and ?S <ha (topic)> V,??where A, B, P, Q, R, and S are nouns, V is a verb, and <X> is a case-marking particle.
From such collected dependency relations, we compiled the following types of experimental data; NN-data based on co-occurrence between nouns for each sentence, NV-data based on a dependency relation between noun and verb for each case-marking particle <wo>, <ga>, <ni>, and <ha>, and SO-data based on a collocation between subject and object that depends on the same verb V as the subject.
These data are represented with a binary vector which corresponds to the appearance pattern of a noun and these vectors are used as arguments of CSM.
We translated descriptors in the MeSH thesaurus into Japanese and used them as Japanese medical terms.
The number of terms appearing in this experiment is 2,557 among them.
We constructed word sets consisting of these medical terms.
Then, we chose 977 word sets consisting of three or more terms from them, and removed word sets with a taxonomical relation from them with the MeSH thesaurus in order to obtain the rest 847 word sets as word sets with a thematic relation.
4. Verification In verifying the capability of our word sets to retrieve Web pages, we examined whether they could help limit the search results to more informative Web pages with Google as a search engine.
We assume that addition of suitable key words to the query reduces the number of pages retrieved and the remaining pages are informative pages.
Based on this assumption, we examined the decrease of the retrieved pages by additional key words and the contents of the retrieved pages in order to verify the availability of our word sets.
Among 847 word sets, we used 294 word sets in which one of the terms is classified into one category and the rest are classified into another.
142 ovary spleen palpation (NN) variation cross reactions outbreaks secretion (Wo) bleeding pyrexia hematuria consciousness disorder vertigo high blood pressure (Ga) space flight insemination immunity (Ni) cough fetus bronchiolitis obliterans organizing pneumonia (Ha) latency period erythrocyte hepatic cell (SO) Figure 1.
Examples of word sets used to verify.
Figure 1 shows examples of the word sets, where terms in a different category are underlined.
In retrieving Web pages for verification, we input the terms composed of these word sets into the search engine.
We created three types of search terms from the word set we extracted.
Suppose the extracted word set is {X 1, ..., X n, Y}, where X i is classified into one category and Y is classified into another.
The first type uses all terms except the one classified into a category different from the others: {X 1, ..., X n } removing Y.
The second type uses all terms except the one in the same category as the rest: {X 1, ..., X k-1, X k+1, ..., X n } removing X k from Type 1.
In our experiment, we removed the term X k with the highest or lowest frequency among X i . The third type uses terms in Type 2 and Y: {X 1, ..., X k-1, X k+1, ..., X n, Y}.
In other words, when we consider the terms in Type 2 as base key words, the terms in Type 1 are key words with the addition of one term having the highest or lowest frequency among the terms in the same category; i.e., the additional term has a feature related to frequency in the documents and is taxonomically related to other terms.
The terms in Type 3 are key words with the addition of one term in a category different from those of the other component terms; i.e., the additional term seems to be thematically related ??at least nontaxonomically related ??to other terms.
First, we quantitatively compared the retrieval results.
We used the estimated number of pages retrieved by Google?s search engine.
Suppose that we first input Type 2 as key words into Google, did not satisfy the result extracted, and added one word to the previous key words.
We then sought to determine whether to use Type 1 or Type 3 to obtain more suitable results.
The results are shown in Figures 2 and 3, which include the results for the highest frequency and the lowest frequency, respectively.
In these figures, the horizontal axis is the number of pages retrieved with Type 2 and the vertical axis is the number of pages retrieved when 1 10 100 1000 10000 100000 1000000 10000000 100000000 1 10 100 1000 10000 100000 1000000 10000000 100000000 1000000000 Number of Web pages retrieved with Type2 (base key words) Number of Web pages retrieved when a term is added to Type2 Type3: With additional term in a different category Type1: With additional term in same category Figure 2.
Fluctuation of number of pages retrieved (with the high frequency term).
NV Type of Data NN Wo Ga Ni Ha Word sets for verification 175 43 23 13 26 Cases in which Type 3 defeated Type 1 in retrieval 108 37 15 12 18 Table 1.
Number of cases in which Type 3 defeated Type 1 with the high frequency term.
a certain term is added to Type 2.
The circles (?? show the retrieval results with additional key word related taxonomically (Type 1).
The crosses () show the results with additional key word related non-taxonomically (Type 3).
The diagonal line shows that adding one term to the base key words does not affect the number of Web pages retrieved.
In Figure 2, most crosses fall further below the line.
This graph indicates that when searching by Google, adding a search term related nontaxonomically tends to make a bigger difference than adding a term related taxonomically and with high frequency.
This means that adding a term related non-taxonomically to the other terms is crucial to retrieving informative pages; that is, such terms are informative terms themselves.
Table 1 shows the number of cases in which term in different category decreases the number of hit pages more than high frequency term.
By this table, we found that most of the additional terms with high frequency contributed less than additional terms related non-taxonomically to decreasing the number of Web pages retrieved.
This means that, in comparison to the high frequency terms, which might not be so informative in themselves, the terms in the other category ??related nontaxonomically ??are effective for retrieving useful Web pages.
In Figure 3, most circles fall further below the line, in contrast to Figure 2.
This indicates that 143 Figure 3.
Fluctuation of number of pages retrieved (with the low frequency term).
NV Type of Data NN Wo Ga Ni Ha Word sets for verification 175 43 23 13 26 Cases in which Type 3 defeated Type 1 in retrieval 61 18 7 6 13 Table 2.
Number of cases in which Type 3 defeated Type 1 with the low frequency term.
adding a term related taxonomically and with low frequency tends to make a bigger difference than adding a term with high frequency.
Certainly, additional terms with low frequency would be informative terms, even though they are related taxonomically, because they may be rare terms on the Web and therefore the number of pages containing the term would be small.
Table 2 shows the number of cases in which term in different category decreases the number of hit pages more than low frequency term.
In comparing these numbers, we found that the additional term with low frequency helped to reduce the number of Web pages retrieved, making no effort to determine the kind of relation the term had with the other terms.
Thus, the terms with low frequencies are quantitatively effective when used for retrieval.
However, if we compare the results retrieved with Type 1 search terms and Type 3 search terms, it is clear that big differences exist between them.
For example, consider ?latency period erythrocyte hepatic cell??obtained from SO-data in Figure 1.
?Latency period??is classified into a category different from the other terms and ?hepatic cell?? has the lowest frequency in this word set.
When we used all the three terms, we obtained pages related to ?malaria??at the top of the results and the title of the top page was ?What is malaria???in Japanese.
With ?latency period??and ?erythrocyte,??we again obtained the same page at the top, although it was not at the top when we used ?erythrocyte??and ?hepatic cell??which have a taxonomical relation.
Type3: With additional term in a different category Type1: With additional term in same category 1 10 100 1000 10000 100000 1000000 10000000 As we showed above, the terms with thematic relations with other search terms are effective at directing users to informative pages.
Quantitatively, terms with a high frequency are not effective at reducing the number of pages retrieved; qualitatively, low frequency terms may not effective to direct users to informative pages.
We will continue our research in order to extract terms in thematic relation more accurately and verify the usefulness of them more quantitatively and qualitatively.
5. Conclusion We sought to extract word sets with a thematic relation from documents by employing casemarking particles derived from syntactic analysis.
We compared the results retrieved with terms related only taxonomically and the results retrieved with terms that included a term related nontaxonomically to the other terms.
As a result, we found adding term which is thematically related to terms that have already been input as key words is effective at retrieving informative pages.
References Berland, M.
and Charniak, E.
1999. Finding parts in very large corpora, In Proceedings of ACL 99, 57??4.
Friberg, K.
2007. Query expansion using domain information in compounds, In Proceedings of NAACL-HLT 2007 Doctoral Consortium, 1??.
Geffet, M.
and Dagan, I.
2005. The distribution inclusion hypotheses and lexical entailment.
In Proceedings of ACL 2005, 107??14.
Girju, R.
2003. Automatic detection of causal relations for question answering.
In Proceedings of ACL Workshop on Multilingual summarization and question answering, 76??14.
Hearst, M.
A. 1992, Automatic acquisition of hyponyms from large text corpora, In Proceedings of Coling 92, 539??45.
Wisniewski, E.
J. and Bassok.
M. 1999.
What makes a man similar to a tie?
Cognitive Psychology, 39: 208??238.
Yamamoto, E., Kanzaki, K., and Isahara, H.
2005. Extraction of hierarchies based on inclusion of co-occurring words with frequency information.
In Proceedings of IJCAI 2005, 1166??172 .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 145??48, Prague, June 2007.
c2007 Association for Computational Linguistics A Linguistic Service Ontology for Language Infrastructures Yoshihiko Hayashi Graduate School of Language and Culture, Osaka University 1-8 Machikaneyama-cho, Toyonaka, 560-0043 Japan hayashi@lang.osaka-u.ac.jp Abstract This paper introduces conceptual framework of an ontology for describing linguistic services on network-based language infrastructures.
The ontology defines a taxonomy of processing resources and the associated static language resources.
It also develops a sub-ontology for abstract linguistic objects such as expression, meaning, and description; these help define functionalities of a linguistic service.
The proposed ontology is expected to serve as a solid basis for the interoperability of technical elements in language infrastructures.
1 Introduction
Several types of linguistic services are currently available on the Web, including text translation and dictionary access.
A variety of NLP tools is also available and public.
In addition to these, a number of community-based language resources targeting particular domains of application have been developed, and some of them are ready for dissemination.
A composite linguistic service tailored to a particular user's requirements would be composable, if there were a language infrastructure on which elemental linguistic services, such as NLP tools, and associated language resources could be efficiently combined.
Such an infrastructure should provide an efficient mechanism for creating workflows of composite services by means of authoring tools for the moment, and through an automated planning in the future.
To this end, technical components in an infrastructure must be properly described, and the semantics of the descriptions should be defined based on a shared ontology.
2 Architecture
of a Language Infrastructure The linguistic service ontology described in this paper has not been intended for a particular language infrastructure.
However we expect that the ontology should be first introduced in an infrastructure like the Language Grid 1, because it, unlike other research-oriented infrastructures, tries to incorporate a wide range of NLP tools and community-based language resources (Ishida, 2006) in order to be useful for a range of intercultural collaboration activities.
The fundamental technical components in the Language Grid could be: (a) external web-based services, (b) on-site NLP core functions, (c) static language resources, and (d) wrapper programs.
Figure 1 depicts the general architecture of the infrastructure.
The technical components listed above are deployed as shown in the figure.
Computational nodes in the language grid are classified into the following two types as described in (Murakami et al., 2006).
ces. Tthe most important desideratum for the ontology, therefore, is that it be able to specify the input/output constraints of a linguistic service properly.
Such input/output specifications enable us to derive a taxonomy of linguistic service and the associated language resources.
3 The
Upper Ontology 3.1 The top level We have developed the upper part of the service ontology so far, and have been working on detailing some of its core parts.
Figure 2 shows the top level of the proposed linguistic service ontology.
Figure 2.
The Top Level of the Ontology.
The topmost class is NL_Resource, which is partitioned into ProcessingResource, and LanguageResource.
Here, as in GATE (Cunningham, 2002), processing resource refers to programmatic or algorithmic resources, while language resource refers to data-only static resources such as lexicons or corpora.
The innate relation between these two classes is: a processing resource can use language resources.
This relationship is specifically introduced to properly define linguistic services that are intended to provide access functions to language resources.
As shown in the figure, LinguisticService is provided by a processing resource, stressing that any linguistic service is realized by a processing resource, even if its prominent functionality is accessing language resources in response to a user?s query.
It also has the meta-information for advertising its non-functional descriptions.
The fundamental classes for abstract linguistic objects, Expression, Meaning, and Description and the innate relations among them are illustrated in Figure 3.
These play roles in defining functionalities of some types of processing resources and associated language resources.
As shown in Fig.
3, an expression may denote a meaning, and the meaning can be further described by a description, especially for human uses.
Figure 3.
Classes for Abstract Linguistic Objects.
In addition to these, NLProcessedStatus and LinguisticAnnotation are important in the sense that NLP status represents the so-called IOPE (Input-Output-Precondition-Effect) parameters of a linguistic processor, which is a subclass of the processing resource, and the data schema for the results of a linguistic analysis is defined by using the linguistic annotation class.
3.2 Taxonomy
of language resources The language resource class currently is partitioned into subclasses for Corpus and Dictionary.
The immediate subclasses of the dictionary class are: (1) MonolingualDictionary, (2) BihasNLProcessedStatus* NLP Tool Linguistic Service External Linguistic Service Language Resource Access Mechanism Language Resource maintains -profiles registry -workflows Core Node Service Node Application Program wrapper 146 lingualDictionary, (3) MultilingualTerminology, and (4) ConceptLexicon.
The major instances of (1) and (2) are so-called machine-readable dictionaries (MRDs).
Many of the community-based special language resources should fall into (3), including multilingual terminology lists specialized for some application domains.
For subclass (4), we consider the computational concept lexicons, which can be modeled by a WordNet-like encoding framework (Hayashi and Ishida, 2006).
3.3 Taxonomy
of processing resources The top level of the processing resource class consists of the following four subclasses, which take into account the input/output constraints of processing resources, as well as the language resources they utilize.
urce it accesses.
The input to a language resource accessor is a query (LR_AccessQuery, sub-class of Expression), and the output is a kind of ?dictionary meaning??(DictionaryMeaning), which is a sub-class of meaning class.
The dictionary meaning class is further divided into sub-classes by referring to the taxonomy of dictionary.
notations by itself or by incorporating some external standard, such as LAF (Ide and Romary, 2004).
3.5 NLP
status and the associated issues Figure 5 illustrates our working taxonomy of NLP processed status.
Note that, in this figure, only the portion related to linguistic analyzer is detailed.
Benefits from the NLP status class will be twofold: (1) as a part of the description of a linguistic analyzer, we assign corresponding instances of this class as its precondition/effect parameters, (2) any instance of the expression class can be concisely 147 ?tagged??by instances of the NLP status class, according to how ?deeply??the expression has been linguistically analyzed so far.
Essentially, such information can be retrieved from the attached linguistic annotations.
In this sense, the NLP status class might be redundant.
Tagging an instance of expression in that way, however, can be reasonable: we can define the input/output constraints of a linguistic analyzer concisely with this device.
Figure 5.
Taxonomy of NLP Status.
Each subclass in the taxonomy represents the type or level of a linguistic analysis, and the hierarchy depicts the processing constraints among them.
For example, if an expression has been parsed, it would already have been morphologically analyzed, because parsing usually requires the input to be morphologically analyzed beforehand.
The subsumption relations encoded in the taxonomy allow simple reasoning in possible composite service composition processes.
However note that the taxonomy is only preliminary.
The arrangement of the subclasses within the hierarchy may end up being far different, depending on the languages considered, and the actual NLP tools, these are essentially idiosyncratic, that are at hand.
For example, the notion of ?chunk??may be different from language to language.
Despite of these, if we go too far in this direction, constructing a taxonomy would be meaningless, and we would forfeit reasonable generalities.
4 Related
Works Klein and Potter (2004) have once proposed an ontology for NLP services with OWL-S definitions.
Their proposal however has not included detailed taxonomies either for language resources, or for abstract linguistic objects, as shown in this paper.
Graa, et al.(2006) introduced a framework for integrating NLP tools with a client-server architecture having a multi-layered repository.
They also proposed a data model for encoding various types of linguistic information.
However the model itself is not ontologized as proposed in this paper.
5 Concluding
Remarks Although the proposed ontology successfully defined a number of first class objects and the innate relations among them, it must be further refined by looking at specific NLP tools/systems and the associated language resources.
Furthermore, its effectiveness in composition of composite linguistic services or wrapper generation should be demonstrated on a specific language infrastructure such as the Language Grid.
Acknowledgments The presented work has been partly supported by NICT international joint research grant.
The author would like to thank to Thierry Declerck and Paul Buitelaar (DFKI GmbH, Germany) for their helpful discussions.
References H.
Cunningham, et al.2002. GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications.
Proc. of ACL 2002, pp.168-175.
J. Graa, et al.2006. NLP Tools Integration Using a Multi-Layered Repository.
Proc. of LREC 2006 Workshop on Merging and Layering Linguistic Information.
Y. Hayashi and T.
Ishida. 2006.
A Dictionary Model for Unifying Machine Readable Dictionaries and Computational Concept Lexicons.
Proc. of LREC 2006, pp.1-6.
N. Ide and L.
Romary. 2004.
International Standard for a Linguistic Annotation Framework.
Journal of Natural Language Engineering, Vol.10:3-4, pp.211-225.
T. Ishida.
2006. Language Grid: An Infrastructure for Intercultural Collaboration.
Proc. of SAINT-06, pp.
96-100, keynote address.
E. Klein and S.
Potter. 2004.
An Ontology for NLP Services.
Proc. of LREC 2004 Workshop on Registry of Linguistic Data Categories.
Y. Murakami, et al.2006. Infrastructure for Language Service Composition.
Proc. of
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 149??52, Prague, June 2007.
c2007 Association for Computational Linguistics Empirical Measurements of Lexical Similarity in Noun Phrase Conjuncts Deirdre Hogan??
Department of Computer Science Trinity College Dublin Dublin 2, Ireland dhogan@computing.dcu.ie Abstract The ability to detect similarity in conjunct heads is potentially a useful tool in helping to disambiguate coordination structures a difficult task for parsers.
We propose a distributional measure of similarity designed for such a task.
We then compare several different measures of word similarity by testing whether they can empirically detect similarity in the head nouns of noun phrase conjuncts in the Wall Street Journal (WSJ) treebank.
We demonstrate that several measures of word similarity can successfully detect conjunct head similarity and suggest that the measure proposed in this paper is the most appropriate for this task.
1 Introduction
Some noun pairs are more likely to be conjoined than others.
Take the follow two alternate bracketings: 1.
busloads of ((executives) and (their spouses)) and 2.
((busloads of executives) and (their spouses)).
The two head nouns coordinated in 1 are executives and spouses, and (incorrectly) in 2: busloads and spouses.
Clearly, the former pair of head nouns is more likely and, for the purpose of discrimination, a parsing model would benefit if it could learn that executives and spouses is a more likely combination than busloads and spouses.
If nouns co-occurring in coordination patterns are often semantically similar, and if a simi??Now at the National Centre for Language Technology, Dublin City University, Ireland.
larity measure could be defined so that, for example: sim(executives, spouses) > sim(busloads, spouses) then it is potentially useful for coordination disambiguation.
The idea that nouns co-occurring in conjunctions tend to be semantically related has been noted in (Riloff and Shepherd, 1997) and used effectively to automatically cluster semantically similar words (Roark and Charniak, 1998; Caraballo, 1999; Widdows and Dorow, 2002).
The tendency for conjoined nouns to be semantically similar has also been exploited for coordinate noun phrase disambiguation by Resnik (1999) who employed a measure of similarity based on WordNet to measure which were the head nouns being conjoined in certain types of coordinate noun phrase.
In this paper we look at different measures of word similarity in order to discover whether they can detect empirically a tendency for conjoined nouns to be more similar than nouns which co-occur but are not conjoined.
In Section 2 we introduce a measure of word similarity based on word vectors and in Section 3 we briefly describe some WordNet similarity measures which, in addition to our word vector measure, will be tested in the experiments of Section 4.
2 Similarity
based on Coordination Co-occurrences The potential usefulness of a similarity measure depends on the particular application.
An obvious place to start, when looking at similarity functions for measuring the type of semantic similarity common for coordinate nouns, is a similarity function based on distributional similarity with context de149 fined in terms of coordination patterns.
Our measure of similarity is based on noun co-occurrence information, extracted from conjunctions and lists.
We collected co-occurrence data on 82, 579 distinct word types from the BNC and the WSJ treebank.
We extracted all noun pairs from the BNC which occurred in a pattern of the form: noun cc noun1, as well as lists of any number of nouns separated by commas and ending in cc noun.
Each noun in the list is linked with every other noun in the list.
Thus for a list: n1, n2, and n3, there will be co-occurrences between words n1 and n2, between n1 and n3 and between n2 and n3.
To the BNC data we added all head noun pairs from the WSJ (sections 02 to 21) that occurred together in a coordinate noun phrase.2 From the co-occurrence data we constructed word vectors.
Every dimension of a word vector represents another word type and the values of the components of the vector, the term weights, are derived from the coordinate word co-occurrence counts.
We used dampened co-occurrence counts, of the form: 1 + log(count), as the term weights for the word vectors.
To measure the similarity of two words, w1 and w2, we calculate the cosine of the angle between the two word vectors, vectorw1 and vectorw2.
3 WordNet-Based Similarity Measures We also examine the following measures of semantic similarity which are WordNet-based.3 Wu and Palmer (1994) propose a measure of similarity of two concepts c1 and c2 based on the depth of concepts in the WordNet hierarchy.
Similarity is measured from the depth of the most specific node dominating both c1 and c2, (their lowest common subsumer), and normalised by the depths of c1 and c2.
In (Resnik, 1995) concepts in WordNet are augmented by corpus statistics and an informationtheoretic measure of semantic similarity is calculated.
Similarity of two concepts is measured 1It would be preferable to ensure that the pairs extracted are unambiguously conjoined heads.
We leave this to future work.
2We did not include coordinate head nouns from base noun phrases (NPB) (i.e.
noun phrases that do not dominate other noun phrases) because the underspecified annotation of NPBs in the WSJ means that the conjoined head nouns can not always be easily identified.
3All of the WordNet-based similarity measure experiments, as well as a random similarity measure, were carried out with the WordNet::Similarity package, http://search.cpan.org/dist/WordNet-Similarity.
by the information content of their lowest common subsumer in the is-a hierarchy of WordNet.
Both Jiang and Conrath (1997) and Lin (1998) propose extentions of Resnik?s measure.
Leacock and Chodorow (1998)?s measure takes into account the path length between two concepts, which is scaled by the depth of the hierarchy in which they reside.
In (Hirst and St-Onge, 1998) similarity is based on path length as well as the number of changes in the direction in the path.
In (Banerjee and Pedersen, 2003) semantic relatedness between two concepts is based on the number of shared words in their WordNet definitions (glosses).
The gloss of a particular concept is extended to include the glosses of other concepts to which it is related in the WordNet hierarchy.
Finally, Patwardhan and Pederson (2006) build on previous work on second-order co-occurrence vectors (Schutze, 1998) by constructing second-order co-occurrence vectors from WordNet glosses, where, as in (Banerjee and Pedersen, 2003), the gloss of a concept is extended so that it includes the gloss of concepts to which it is directly related in WordNet.
4 Experiments
We selected two sets of data from sections 00, 01, 22 and 24 of the WSJ treebank.
The first consists of all nouns pairs which make up the head words of two conjuncts in coordinate noun phrases (again not including coordinate NPBs).
We found 601 such coordinate noun pairs.
The second data set consists of 601 word pairs which were selected at random from all head-modifier pairs where both head and modifier words are nouns and are not coordinated.
We tested the 9 different measures of word similarity just described on each data set in order to see if a significant difference could be detected between the similarity scores for the coordinate words sample and non-coordinate words sample.
Initially both the coordinate and non-coordinate pair samples each contained 601 word pairs.
However, before running the experiments we removed all pairs where the words in the pair were identical.
This is because identical words occur more often in coordinate head words than in other lexical dependencies (there were 43 pairs with identical words in the coordination set, compared to 3 such pairs in the 150 SimTest ncoord xcoord SDcoord nnonCoord xnonCoord SDnonCoord 95% CI p-value coordDistrib 503 0.11 0.13 485 0.06 0.09 [0.04 0.07] 0.000 (Resnik, 1995) 444 3.19 2.33 396 2.43 2.10 [0.46 1.06] 0.000 (Lin, 1998) 444 0.27 0.26 396 0.19 0.22 [0.04 0.11] 0.000 (Jiang and Conrath, 1997) 444 0.13 0.65 395 0.07 0.08 [-0.01 0.11] 0.083 (Wu and Palmer, 1994) 444 0.63 0.19 396 0.55 0.19 [0.06 0.11] 0.000 (Leacock and Chodorow, 1998) 444 1.72 0.51 396 1.52 0.47 [0.13 0.27] 0.000 (Hirst and St-Onge, 1998) 459 1.599 2.03 447 1.09 1.87 [0.25 0.76] 0.000 (Banerjee and Pedersen, 2003) 451 114.12 317.18 436 82.20 168.21 [-1.08 64.92] 0.058 (Patwardhan and Pedersen, 2006) 459 0.67 0.18 447 0.66 0.2 [-0.02 0.03] 0.545 random 483 0.89 0.17 447 0.88 0.18 [-0.02 0.02] 0.859 Table 1: Summary statistics for 9 different word similarity measures (plus one random measure):ncoord and nnonCoord are the sample sizes for the coordinate and non-coordinate noun pairs samples, respectively; xcoord, SDcoord and xnonCoord, SDnonCoord are the sample means and standard deviations for the two sets.
The 95% CI column shows the 95% confidence interval for the difference between the two sample means.
The p-value is for a Welch two sample two-sided t-test.
coordDistrib is the measure introduced in Section 2.
non-coordination set).
If we had not removed them, a statistically significant difference between the similarity scores of the pairs in the two sets could be found simply by using a measure which, say, gave one score for identical words and another (lower) score for all non-identical word pairs.
Results for all similarity measure tests on the data sets described above are displayed in Table 1.
In one final experiment we used a random measure of similarity.
For each experiment we produced two samples, one consisting of the similarity scores given by the similarity measure for the coordinate noun pairs, and another set of similarity scores generated for the non-coordinate pairs.
The sample sizes, means, and standard deviations for each experiment are shown in the table.
Note that the variation in the sample size is due to coverage: the different measures did not produce a score for all word pairs.
Also displayed in Table 1 are the results of statistical significance tests based on the Welsh two sample t-test.
A 95% confidence interval for the difference of the sample means is shown along with the p-value.
5 Discussion
For all but three of the experiments (excluding the random measure), the difference between the mean similarity measures is statistically significant.
Interestingly, the three tests where no significant difference was measured between the scores on the coordination set and the non-coordination set (Jiang and Conrath, 1997; Banerjee and Pedersen, 2003; Patwardhan and Pedersen, 2006) were the three top scoring measures in (Patwardhan and Pedersen, 2006), where a subset of six of the above WordNetbased experiments were compared and the measures evaluated against human relatedness judgements and in a word sense disambiguation task.
In another comparative study (Budanitsky and Hirst, 2002) of five of the above WordNet-based measures, evaluated as part of a real-word spelling correction system, Jiang and Conrath (1997)?s similarity score performed best.
Although performing relatively well under other evaluation criteria, these three measures seem less suited to measuring the kind of similarity occurring in coordinate noun pairs.
One possible explanation for the unsuitability of the measures of (Patwardhan and Pedersen, 2006) for the coordinate similarity task could be based on how context is defined when building context vectors.
Context for an instance of the the word w is taken to be the words that surround w in the corpus within a given number of positions, where the corpus is taken as all the glosses in WordNet.
Words that form part of collocations such as disk drives or task force would then tend to have very similar contexts, and thus such word pairs, from non-coordinate modifier-head relations, could be given too high a similarity score.
Although the difference between the mean similarity scores seems rather slight in all experiments, it is worth noting that not all coordinate head words are semantically related.
To take a couple of examples from the coordinate word pair set: work/harmony extracted from hard work and harmony, and power/clause extracted from executive power and the appropriations clause.
We would not expect these word pairs to get a high similarity score.
On the other hand, it is also possible that 151 some of the examples of non-coordinate dependencies involve semantically similar words.
For example, nouns in lists are often semantically similar, and we did not exclude nouns extracted from lists from the non-coordinate test set.
Although not all coordinate noun pairs are semantically similar, it seems clear, on inspection of the two sets of data, that they are more likely to be semantically similar than modifier-head word pairs, and the tests carried out for most of the measures of semantic similarity detect a significant difference between the similarity scores assigned to coordinate pairs and those assigned to non-coordinate pairs.
It is not possible to judge, based on the significance tests alone, which might be the most useful measure for the purpose of disambiguation.
However, in terms of coverage, the distributional measure introduced in Section 2 clearly performs best4.
This measure of distributional similarity is perhaps more suited to the task of coordination disambiguation because it directly measures the type of similarity that occurs between coordinate nouns.
That is, the distributional similarity measure presented in Section 2 defines two words as similar if they occur in coordination patterns with a similar set of words and with similar distributions.
Whether the words are semantically similar becomes irrelevant.
A measure of semantic similarity, on the other hand, might find words similar which are quite unlikely to appear in coordination patterns.
For example, Cederberg and Widdows (2003) note that words appearing in coordination patterns tend to be on the same ontological level: ?fruit and vegetables??is quite likely to occur, whereas ?fruit and apples??is an unlikely cooccurrence.
A WordNet-based measure of semantic similarity, however, might give a high score to both of the noun pairs.
In the future we intend to use the similarity measure outlined in Section 2 in a lexicalised parser to help resolve coordinate noun phrase ambiguities.
Acknowledgements Thanks to the TCD Broad Curriculum Fellowship and to the SFI Research Grant 04/BR/CS370 for funding this research.
Thanks also to Padraig Cunningham, Saturnino Luz and Jennifer Foster for helpful discussions.
4Somewhat unsurprisingly given it is part trained on data from the same domain.
References Satanjeev Banerjee and Ted Pedersen.
2003 Extended Gloss Overlaps as a Measure of Semantic Relatedness.
In Proceeding of the 18th IJCAI.
Alexander Budanitsky and Graeme Hirst.
2002 Semantic Distance in WordNet: An experimental, application-oriented Evaluation of Five Measures In Proceedings of the 3rd CICLING.
Sharon Caraballo.
1999 Automatic construction of a hypernym-labeled noun hierarchy from text In Proceedings of the 37th ACL.
Scott Cederberg and Dominic Widdows.
2003. Using LSA and Noun Coordination Information to Improve the Precision and Recall of Automatic Hyponymy Extraction.
In Proceedings of the 7th CoNLL.
G. Hirst and D.
St-Onge 1998.
Lexical Chains as representations of context for the detection and correction of malapropisms.
WordNet: An electronic lexical database.
MIT Press.
J. Jiang and D.
Conrath. 1997.
Semantic similarity based on corpus statistics and lexical taxonomy.
In Proceedings of the ROCLING.
C. Leacock and M.
Chodorow. 1998.
Combining local context and WordNet similarity for word sense identification.
Word-Net: An electronic lexical database.
MIT Press.
D. Lin.
1998. An information-theoretic definition of similarity.
In Proceedings of the 15th ICML.
Siddharth Patwardhan and Ted Pedersen.
2006. Using WordNet-based Context Vectors to Estimate the Semantic Relatedness of Concepts.
In Proceedings of Making Sense of Sense Bringing Computational Linguistics and Psycholinguistics Together, EACL.
Philip Resnik.
1995. Using Information Content to Evaluate Semantic Similarity.
In Proceedings of IJCAI.
Philip Resnik.
1999. Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language.
In Journal of Artificial Intelligence Research, 11:95-130.
Ellen Riloff and Jessica Shepherd 1997.
A Corpus-based Approach for Building Semantic Lexicon.
In Proceedings of the 2nd EMNLP.
Brian Roark and Eugene Charniak 1998.
Noun-phrase Co-occurrence Statistics for Semi-automatic semantic lexicon construction.
In Proceedings of the COLING-ACL.
Hinrich Schutze.
1998. Automatic Word Sense Discrimination.
Computational Linguistics, 24(1):97-123.
Dominic Widdows and Beate Dorow.
2002. A Graph Model for Unsupervised Lexical Acquisition.
In Proceedings of the 19th COLING.
Zhibiao Wu and Martha Palmer.
1994. Verb Semantics and Lexical Selection.
In Proceedings of the ACL.
kProceedings of the ACL 2007 Demo and Poster Sessions, pages 153??56, Prague, June 2007.
c2007 Association for Computational Linguistics Automatic Discovery of Named Entity Variants ??Grammar-driven Approaches to Non-alphabetical Transliterations Chu-Ren Huang Institute of Linguistics Academia Sinica, Taiwan churenhuang@gmail.com Petr ?Simon Institute of Linguistics Academia Sinica, Taiwan sim@klubko.net Shu-Kai Hsieh DoFLAL NIU, Taiwan shukai@gmail.com Abstract Identification of transliterated names is a particularly difficult task of Named Entity Recognition (NER), especially in the Chinese context.
Of all possible variations of transliterated named entities, the difference between PRC and Taiwan is the most prevalent and most challenging.
In this paper, we introduce a novel approach to the automatic extraction of diverging transliterations of foreign named entities by bootstrapping cooccurrence statistics from tagged and segmented Chinese corpus.
Preliminary experiment yields promising results and shows its potential in NLP applications.
1 Introduction
Named Entity Recognition (NER) is one of the most difficult problems in NLP and Document Understanding.
In the field of Chinese NER, several approaches have been proposed to recognize personal names, date/time expressions, monetary and percentage expressions.
However, the discovery of transliteration variations has not been well-studied in Chinese NER.
This is perhaps due to the fact that the transliteration forms in a non-alphabetic language such as Chinese are opaque and not easy to compare.
On the hand, there is often more than one way to transliterate a foreign name.
On the other hand, dialectal difference as well as different transliteration strategies often lead to the same named entity to be transliterated differently in different Chinese speaking communities.
Corpus Example (Clinton) Frequency XIN ????24382 CNA ????150 XIN ??0 CNA ??120842 Table 1: Distribution of two transliteration variants for ?Clinton??in two sub-corpora Of all possible variations, the cross-strait difference between PRC and Taiwan is the most prevalent and most challenging.1The main reason may lie in the lack of suitable corpus.
Even given some subcorpora of PRC and Taiwan variants of Chinese, a simple contrastive approach is still not possible.
It is because: (1) some variants might overlap and (2) there are more variants used in each corpus due to citations or borrowing crossstrait.
Table 1 illustrates this phenomenon, where CNA stands for Central News Agency in Taiwan, XIN stands for Xinhua News Agency in PRC, respectively.
With the availability of Chinese Gigaword Corpus (CGC) and Word Sketch Engine (WSE) Tools (Kilgarriff, 2004).
We propose a novel approach towards discovery of transliteration variants by utilizing a full range of grammatical information augmented with phonological analysis.
Existing literatures on processing of transliteration concentrate on the identification of either the transliterated term or the original term, given knowledge of the other (e.g.
(Virga and Khudanpur, 1For instance, we found at least 14 transliteration variants for Lewinsky,such as ?fl ?fl fl?fl?
???? fl?fl fl flfl?fl  fl and so on.
153 2003)).
These studies are typically either rule-based or statistics-based, and specific to a language pair with a fixed direction (e.g.
(Wan and Verspoor, 1998; Jiang et al., 2007)).
To the best of our knowledge, ours is the first attempt to discover transliterated NE?s without assuming prior knowledge of the entities.
In particular, we propose that transliteration variants can be discovered by extracting and comparing terms from similar linguistic context based on CGC and WSE tools.
This proposal has great potential of increasing robustness of future NER work by enabling discovery of new and unknown transliterated NE?s.
Our study shows that resolution of transliterated NE variations can be fully automated.
This will have strong and positive implications for cross-lingual and multi-lingual informational retrieval.
2 Bootstrapping
transliteration pairs The current study is based on Chinese Gigaword Corpus (CGC) (Graff el al., 2005), a large corpus contains with 1.1 billion Chinese characters containing data from Central News Agency of Taiwan (ca.
700 million characters), Xinhua News Agency of PRC (ca.
400 million characters).
These two subcorpora represent news dispatches from roughly the same period of time, i.e. 1990-2002.
Hence the two sub-corpora can be expected to have reasonably parallel contents for comparative studies.2 The premises of our proposal are that transliterated NE?s are likely to collocate with other transliterated NE?s, and that collocates of a pair of transliteration variants may form contrasting pairs and are potential variants.
In particular, since the transliteration variations that we are interested in are those between PRC and Taiwan Mandarin, we will start with known contrasting pairs of these two language variants and mine potential variant pairs from their collocates.
These potential variant pairs are then checked for their phonological similarity to determine whether they are true variants or not.
In order to effectively select collocates from specific grammatical constructions, the Chinese Word Sketch3 is adopted.
In particular, we use the Word Sketch dif2To facilitate processing, the complete CGC was segmented and POS tagged using the Academia Sinica segmentation and tagging system (Ma and Huang, 2006).
3http://wordsketch.ling.sinica.edu.tw ference (WSDiff) function to pick the grammatical contexts as well as contrasting pairs.
It is important to bear in mind that Chinese texts are composed of Chinese characters, hence it is impossible to compare a transliterated NE with the alphabetical form in its original language.
The following characteristics of a transliterated NE?s in CGC are exploited to allow discovery of transliteration variations without referring to original NE.
??frequent co-occurrence of named entities within certain syntagmatic relations ??named entities frequently co-occur in relations such as AND or OR and this fact can be used to collect and score mutual predictability.
??foreign named entities are typically transliterated phonetically ??transliterations of the same name entity using different characters can be matched by using simple heuristics to map their phonological value.
??presence and co-occurrence of named entities in a text is dependent on a text type ??journalistic style cumulates many foreign named entities in close relations.
??many entities will occur in different domains ??famous person can be mentioned together with someone from politician, musician, artist or athlete.
Thus allows us to make leaps from one domain to another.
There are, however, several problems with the phonological representation of foreign named entities in Chinese.
Due to the nature of Chinese script, NE transliterations can be realized very differently.
The following is a summary of several problems that have to be taken into account: ??word ending: ?? vs.??
???Arafat??or ? vs.?
???Mubarak?? The final consonant is not always transliterated.
XIN transliterations tend to try to represent all phonemes and often add vowels to a final consonant to form a new syllable, whereas CNA transliteration tends to be shorter and may simply leave out a final consonant.
??gender dependent choice of characters: ? ?Leslie??vs.?fl???Chris??or ?? vs.
??fl 154 ??
Some occidental names are gender neutral.
However, the choice of characters in a personal name in Chinese is often gender sensitive.
So these names are likely to be transliterated differently depending on the gender of its referent.
??divergent representations caused by scope of transliteration, e.g. both given and surname vs.
only surname:  ?/ ? ??Venus Williams??
??difference in phonological interpretation: ?
?vs. ?Rafter??or??flvs.
?fl?Connors?? ??native vs.
non-native pronunciation: ?fl?? vs.
fl???Escudero??or ? vs.
?? ?Federer?? 2.1 Data collection All data were collected from Chinese Gigaword Corpus using Chinese Sketch Engine with WSDiff function, which provides side-by-side syntagmatic comparison of Word Sketches for two different words.
WSDiff query for wi and wj returns patterns that are common for both words and also patterns that are particular for each of them.
Three data sets are thus provided.
We neglect the common patterns set and concentrate only on the wordlists specific for each word.
2.2 Pairs
extraction Transliteration pairs are extracted from the two sets, A and B, collected with WSDiff using default set of seed pairs : for each seed pair in seeds retrieve WSDiff for and/or relation, thus have pairs of word lists, < Ai,Bi > for each word wii ??Ai find best matching counterpart(s) wij ??Bi.
Comparison is done using simple phonological rules, viz.
2.3 use newly extracted pairs as new seeds (original seeds are stored as good pairs and not queried any more) loop until there are no new pairs Notice that even though substantial proportion of borrowing among different communities, there is no mixing in the local context of collocation, which means, local collocation could be the most reliable way to detect language variants with known variants.
2.3 Phonological
comparison All word forms are converted from Chinese script into a phonological representation4 during the pairs extraction phase and then these representations are compared and similarity scores are given to all pair candidates.
A lot of Chinese characters have multiple pronunciations and thus multiple representations are derived.
In case of multiple pronunciations for certain syllable, this syllable is commpared to its counterpart from the other set.
E.g. (??has three pronunciations: y`e, xie, sh`e.
When comparing syllables such as [pei,fei] and [fei], will be represented as [fei].
In case of pairs such as ??[ye er qin] and ? [ye er qin], which have syllables with multiple pronunciations and this multiple representations.
However, since these two potential variants share the first two characters (out of three), they are considered as variants without superfluous phonological checking.
Phonological representations of whole words are then compared by Levenstein algorithm, which is widely used to measure the similarity between two strings.
First, each syllable is split into initial and final components: gao:g+ao.
In case of syllables without initials like er, an ??is inserted before the syllable, thus er:??er.
Before we ran the Levenstein measure, we also apply phonological corrections on each pair of candidate representations.
Rules used for these corrections are derived from phonological features of Mandarin Chinese and extended with few rules from observation of the data: (1) For Initials, (a): voiced/voiceless stop contrasts are considered as similar for initials: g:k, e.g.
[gao] ( ?? vs.
[ke] ( ??,d:t, b:p, (b): r:l ??[rui] ( ??
??[lie] ( ?) is added to distinctive feature set based on observation.
(2). For Finals, (a): pair ei:ui is evaluated as equivalent.5 (b): oppositions of nasalised final is evaluated as dissimilar.
4http://unicode.org/charts/unihan.html 5Pinyin representation of phonology of Mandarin Chinese does not follow the phonological reality exactly: [ui] = [uei] etc.
155 2.4 Extraction algorithm Our algorithm will potentially exhaust the whole corpus, i.e. find most of the named entities that occur with at least few other names entities, but only if seeds are chosen wisely and cover different domains6.
However, some domains might not overlap at all, that is, members of those domains never appear in the corpus in relation and/or.
And concurrence of members within some domains might be sparser than in other, e.g. politicians tend to be mentioned together more often than novelists.
Nature of the corpus also plays important role.
It is likely to retrieve more and/or related names from journalistic style.
This is one of the reasons why we chose Chinese Gigaword Corpus for this task.
3 Experiment
and evaluation We have tested our method on the Chinese Gigaword Second Edition corpus with 11 manually selected seeds Apart from the selection of the starter seeds, the whole process is fully automatic.
For this task we have collected data from syntagmatic relation and/or, which contains words co-occurring frequently with our seed words.
When we make a query for peoples names, it is expected that most of the retrieved items will also be names, perhaps also names of locations, organizations etc.
The whole experiment took 505 iterations in which 494 pairs were extracted.
Our complete experiment with 11 pre-selected transliteration pairs as seed took 505 iterations to end.
The iterations identified 494 effective transliteration variant pairs (i.e.
those which were not among the seeds or pairs identified by earlier iteration).
All the 494 candidate pairs were manually evaluated 445 of them are found to be actual contrast pairs, a precision of 90.01%.
In addition, the number of new transliteration pairs yielded is 4,045%, a very productive yield for NE discovery.
Preliminary results show that this approach is competitive against other approaches reported in previous studies.
Performances of our algorithms is calculated in terms of precision rate with 90.01%. 6The term domain refers to politics,music,sport, film etc.
4 Conclusion
and Future work In this paper, we have shown that it is possible to identify NE?s without having prior knowledge of them.
We also showed that, applying WSE to restrict grammatical context and saliency of collocation, we are able to effectively extract transliteration variants in a language where transliteration is not explicitly represented.
We also show that a small set of seeds is all it needs for the proposed method to identify hundreds of transliteration variants.
This proposed method has important applications in information retrieval and data mining in Chinese data.
In the future, we will be experimenting with a different set of seeds in a different domain to test the robustness of this approach, as well as to discover transliteration variants in our fields.
We will also be focusing on more refined phonological analysis.
In addition, we would like to explore the possibility of extending this proposal to other language pairs.
References Jiang, L.
and M.Zhou and L.f.
Chien. 2007.
Named Entity Discovery based on Transliteration and WWW [In Chinese].
Journal of the Chinese Information Processing Society.
2007 no.1. pp.23-29.
Graff, David et al.2005. Chinese Gigaword Second Edition.
Linguistic Data Consortium, Philadelphia.
Ma, Wei-Yun and Huang, Chu-Ren.
2006. Uniform and Effective Tagging of a Heterogeneous Giga-word Corpus.
Presented at the 5th International Conference on Language Resources and Evaluation (LREC2006), 24-28 May.
Genoa, Italy.
Kilgarriff, Adam et al.2004. The Sketch Engine.
Proceedings of EURALEX 2004.
Lorient, France.
Paola Virga and Sanjeev Khudanpur.
2003. Transliteration of proper names in cross-lingual information retrieval.
In Proc.
of the ACL Workshop on Multi-lingual Named Entity Recognition, pp.57-64.
Wan, Stephen and Cornelia Verspoor.
1998. Automatic English-Chinese Name Transliteration for Development of Multiple Resources.
In Proc.
of COLING/ACL, pp.1352-1356.

