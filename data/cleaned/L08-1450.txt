<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Arvind Arasu</author>
<author>Hector Garcia-Molina</author>
<author>Stanford University</author>
</authors>
<title>Extracting structured data from web pages</title>
<date>2003</date>
<contexts>
<context>processing literature discusses several approaches to extract structure information from PDF, HTML and other structured documents, see (Laender et al., 2002) for an overview. Arasu and Garcia-Molina (Arasu et al., 2003) et. al and Rosenfeld et. al. (Crescenzi et al., 2001) approaches are based templates that characterize each part of the document. These templates are either extracted manually or semi-automatically.</context>
<context>lements refer to the same information blocks and Cross-references (e.g. image caption) can support how each text paragraph or sentence related to each image, example include (Crescenzi et al., 2001) (Arasu et al., 2003) (Rosenfeld et al., 2002). Co-occurrence of patterns across different media types can improve the detection of the most informative text-image associations. 4.3.1. Implicit Associations Some associat</context>
</contexts>
<marker>Arasu, Garcia-Molina, University, 2003</marker>
<rawString>Arvind Arasu, Hector Garcia-Molina, and Stanford University. 2003. Extracting structured data from web pages.</rawString>
</citation>
<citation valid="false">
<booktitle>In SIGMOD ’03: Proceedings of the 2003 ACM SIGMOD international conference on Management of data</booktitle>
<pages>337--348</pages>
<publisher>ACM</publisher>
<location>New York, NY, USA</location>
<marker></marker>
<rawString>In SIGMOD ’03: Proceedings of the 2003 ACM SIGMOD international conference on Management of data, pages 337–348, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Breuel</author>
</authors>
<title>Information extraction from html document by structural matching</title>
<date>2003</date>
<booktitle>In Second International Wrokshop on Web Document Analysis</booktitle>
<pages>11--14</pages>
<location>Edinburgh, UK</location>
<contexts>
<context>ag of words, such stripping down throws away layout or relational information which would otherwise provide valuable features to an IE algorithm (Shin and Doermann, 2000)(Maderlechner and Suda, 1998)(Breuel, 2003). This is particularly true with modern multimedia documents, where valuable features can sometimes be found under and across different structural contexts and media within the document. Each documen</context>
</contexts>
<marker>Breuel, 2003</marker>
<rawString>Thomas M Breuel. 2003. Information extraction from html document by structural matching. In Second International Wrokshop on Web Document Analysis, pages 11– 14, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Brickley</author>
<author>R V Guha</author>
</authors>
<title>Resource description framework (rdf) schema specification</title>
<date>1999</date>
<tech>Technical report, W3C</tech>
<marker>Brickley, Guha, 1999</marker>
<rawString>D Brickley and Guha R V. 1999. Resource description framework (rdf) schema specification. Technical report, W3C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Chakravarthy</author>
<author>F Ciravegna</author>
<author>V Lanfranchi</author>
</authors>
<title>Cross-media document annotation and enrichment</title>
<date>2006</date>
<booktitle>In 1st Semantic Authoring and Annotation Workshop (SAAW2006</booktitle>
<location>Athens GA USA</location>
<contexts>
<context>e the original document. However, such method is not advisable in ODF document annotation without considerable effort. Therefore, we have developed a toolkit for HTML and ODF annotation. AKTiveMedia (Chakravarthy et al., 2006) is a user centric system for cross-media document enrichment; it uses semantic web and language technologies for acquiring, storing and reusing knowledge in a collaborative way, sharing it with othe</context>
</contexts>
<marker>Chakravarthy, Ciravegna, Lanfranchi, 2006</marker>
<rawString>A Chakravarthy, F Ciravegna, and V Lanfranchi. 2006. Cross-media document annotation and enrichment. In 1st Semantic Authoring and Annotation Workshop (SAAW2006), Athens GA USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valter Crescenzi</author>
<author>Giansalvatore Mecca</author>
<author>Paolo Merialdo</author>
</authors>
<title>Roadrunner: Towards automatic data extraction from large web sites</title>
<date>2001</date>
<booktitle>In Proceedings of 27th International Conference on Very Large Data Bases</booktitle>
<pages>109--118</pages>
<contexts>
<context> to extract structure information from PDF, HTML and other structured documents, see (Laender et al., 2002) for an overview. Arasu and Garcia-Molina (Arasu et al., 2003) et. al and Rosenfeld et. al. (Crescenzi et al., 2001) approaches are based templates that characterize each part of the document. These templates are either extracted manually or semi-automatically. Rosenfeld et. al. (Rosenfeld et al., 2002) implemente</context>
<context>htforward to know which elements refer to the same information blocks and Cross-references (e.g. image caption) can support how each text paragraph or sentence related to each image, example include (Crescenzi et al., 2001) (Arasu et al., 2003) (Rosenfeld et al., 2002). Co-occurrence of patterns across different media types can improve the detection of the most informative text-image associations. 4.3.1. Implicit Assoc</context>
</contexts>
<marker>Crescenzi, Mecca, Merialdo, 2001</marker>
<rawString>Valter Crescenzi, Giansalvatore Mecca, and Paolo Merialdo. 2001. Roadrunner: Towards automatic data extraction from large web sites. In Proceedings of 27th International Conference on Very Large Data Bases, pages 109–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Ferraro</author>
<author>Luca Gilardoni</author>
<author>Chrustian Biasuzzi</author>
<author>Piercarlo Slavazza</author>
<author>Fabrizio Lovato</author>
</authors>
<title>Requirement analysis for compound document processing</title>
<date>2007</date>
<tech>Technical report</tech>
<institution>Quinary Spa</institution>
<contexts>
<context> type, i.e. OpenDocument text, spreadsheet, and presentation. In complex IE tasks, information to be extracted is present in more than one media or document (e.g. in cross-media knowledge extraction (Ferraro et al., 2007)), therefore we need to model information across documents of different formats. For instance, with this model we can relate numerical data from an OpenDocument spreadsheet to a textual description i</context>
</contexts>
<marker>Ferraro, Gilardoni, Biasuzzi, Slavazza, Lovato, 2007</marker>
<rawString>Massimo Ferraro, Luca Gilardoni, Chrustian Biasuzzi, Piercarlo Slavazza, and Fabrizio Lovato. 2007. Requirement analysis for compound document processing. Technical report, Quinary Spa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e Iria</author>
<author>Fabio Ciravegna</author>
</authors>
<title>A Methodology and Tool for Representing Language Resources for Information Extraction</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<location>Genoa, Italy</location>
<contexts>
<context>e without a effective document structure representation. Our document graph model has demonstrated, with its support, the document and content segmentation can be much easily achieved. 5. Tools Runes(Iria and Ciravegna, 2006) is a framework 2 for representing objects in a graph. It is possible to use runes to represent heterogeneous resources under a common representation, where relations are available for aid of IE task</context>
</contexts>
<marker>Iria, Ciravegna, 2006</marker>
<rawString>Jos´e Iria and Fabio Ciravegna. 2006. A Methodology and Tool for Representing Language Resources for Information Extraction. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006), Genoa, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Laender</author>
<author>B Ribeiro-Neto</author>
<author>A Silva</author>
<author>J Teixeira</author>
</authors>
<title>A brief survey of web data extraction tools</title>
<date>2002</date>
<booktitle>In SIGMOD Record</booktitle>
<volume>31</volume>
<contexts>
<context>Document Structure Analysis Using Document Graph Model Document processing literature discusses several approaches to extract structure information from PDF, HTML and other structured documents, see (Laender et al., 2002) for an overview. Arasu and Garcia-Molina (Arasu et al., 2003) et. al and Rosenfeld et. al. (Crescenzi et al., 2001) approaches are based templates that characterize each part of the document. These </context>
</contexts>
<marker>Laender, Ribeiro-Neto, Silva, Teixeira, 2002</marker>
<rawString>A. Laender, B. Ribeiro-Neto, A. Silva, and J. Teixeira. 2002. A brief survey of web data extraction tools. In SIGMOD Record, volume 31, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerd Maderlechner</author>
<author>Peter Suda</author>
</authors>
<title>Information extraction from document images using white space and graphics analysis</title>
<date>1998</date>
<booktitle>In SSPR/SPR</booktitle>
<pages>468--474</pages>
<contexts>
<context>ford to model the text as a bag of words, such stripping down throws away layout or relational information which would otherwise provide valuable features to an IE algorithm (Shin and Doermann, 2000)(Maderlechner and Suda, 1998)(Breuel, 2003). This is particularly true with modern multimedia documents, where valuable features can sometimes be found under and across different structural contexts and media within the document</context>
</contexts>
<marker>Maderlechner, Suda, 1998</marker>
<rawString>Gerd Maderlechner and Peter Suda. 1998. Information extraction from document images using white space and graphics analysis. In SSPR/SPR, pages 468–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Moens</author>
<author>R De Busser</author>
</authors>
<title>Information Extraction: Algorithms and Prospects in a Retrieval Context, chapter 1</title>
<date>2006</date>
<publisher>Springer, The</publisher>
<location>Netherlands</location>
<marker>Moens, De Busser, 2006</marker>
<rawString>M. Moens and R. De Busser, 2006. Information Extraction: Algorithms and Prospects in a Retrieval Context, chapter 1. Springer, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Binyamin Rosenfeld</author>
<author>Ronen Feldman</author>
<author>Yonatan Aumann</author>
</authors>
<title>Structural extraction from visual layout of documents</title>
<date>2002</date>
<booktitle>In CIKM ’02: Proceedings of the eleventh international conference on Information and knowledge management</booktitle>
<pages>203--210</pages>
<publisher>ACM</publisher>
<location>New York, NY, USA</location>
<contexts>
<context>ld et. al. (Crescenzi et al., 2001) approaches are based templates that characterize each part of the document. These templates are either extracted manually or semi-automatically. Rosenfeld et. al. (Rosenfeld et al., 2002) implemented a learning algorithm to extract information (author,title,date,etc.). They chose to ignore text content and only use features such as font, physical positioning and other graphical chara</context>
<context>same information blocks and Cross-references (e.g. image caption) can support how each text paragraph or sentence related to each image, example include (Crescenzi et al., 2001) (Arasu et al., 2003) (Rosenfeld et al., 2002). Co-occurrence of patterns across different media types can improve the detection of the most informative text-image associations. 4.3.1. Implicit Associations Some associations can be better delimi</context>
</contexts>
<marker>Rosenfeld, Feldman, Aumann, 2002</marker>
<rawString>Binyamin Rosenfeld, Ronen Feldman, and Yonatan Aumann. 2002. Structural extraction from visual layout of documents. In CIKM ’02: Proceedings of the eleventh international conference on Information and knowledge management, pages 203–210, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization</title>
<date>2002</date>
<journal>ACM Computing Surveys</journal>
<volume>34</volume>
<pages>47</pages>
<contexts>
<context>cessing complexity, IE systems tend to simply strip documents down to their core textual format, typically sets or sequences of tokens. While this may be enough for tasks such as text categorization (Sebastiani, 2002), where state-of-the-art approaches can afford to model the text as a bag of words, such stripping down throws away layout or relational information which would otherwise provide valuable features to</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Fabrizio Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1– 47.</rawString>
</citation>
</citationList>
</algorithm>

