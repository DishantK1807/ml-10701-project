1:224	Trained Named Entity Recognition Using Distributional Clusters Dayne Freitag HNC Software, LLC 3661 Valley Centre Drive San Diego, CA 92130 DayneFreitag@fairisaac.com Abstract This work applies boosted wrapper induction (BWI), a machine learning algorithm for information extraction from semi-structured documents, to the problem of named entity recognition.
2:224	The default feature set of BWI is augmented with features based on distributional term clusters induced from a large unlabeled text corpus.
3:224	Using no traditional linguistic resources, such as syntactic tags or specialpurpose gazetteers, this approach yields results near the state of the art in the MUC 6 named entity domain.
4:224	Supervised learning using features derived through unsupervised corpus analysis may be regarded as an alternative to bootstrapping methods.
5:224	1 Introduction The problem of named entity recognition (NER) has recently received increasing attention.
6:224	Identification of generic semantic categories in textsuch as mentions of people, organizations, locations, and temporal and numeric expressionsis a necessary first step in many applications of information extraction, information retrieval, and question answering.
7:224	To a large extent, knowledge-poor methods suffice to yield good recognition performance.
8:224	In particular, supervised learning can be used to produce a system with performance at or near the state of the art (Bikel et al. , 1997).
9:224	In the supervised learning framework, a corpus of (typically) a few hundred documents is annotated by hand to identify the entities of interest.
10:224	Features of local context are then used to train a system to distinguish instances from non-instances in novel texts.
11:224	Such features may include literal word tests, patterns of orthography, parts of speech, semantic categories, or membership in special-purpose gazetteers.
12:224	While supervised training greatly facilitates the development of a robust NER system, the requirement of a substantial training corpus remains an impediment to the rapid deployment of NER in new domains or new languages.
13:224	A number bush peters reagan noriega john robert james david  president chairman head owner  japan california london chicago  Table 1: Sample members of four clusters from the Wall Street Journal corpus.
14:224	of researchers have therefore sought to exploit the availability of unlabeled documents, typically by bootstrapping a classifier using automatic labellings (Collins and Singer, 1999; Cucerzan and Yarowsky, 1999; Thelen and Riloff, 2002).
15:224	Here, we investigate a different approach.
16:224	Using a distributional clustering technique called coclustering, we produce clusters which, intuitively, should be useful for NER.
17:224	Table 1 shows example terms from several sample clusters induced using a collection of documents from the Wall Street Journal (WSJ).
18:224	Several papers have shown that distributional clustering yields categories that have high agreement with part of speech (Schutze, 1995; Clark, 2000).
19:224	As the table illustrates, these clusters also tend to have a useful semantic dimension.
20:224	Clustering on the WSJ portion of the North American News corpus yields two clusters that clearly correspond to personal names, one for first names and one for last names.
21:224	As an experiment, we scanned the MUC6 NER data set for token sequences consisting of zero or more members of the first name cluster (or an initial followed by a period), followed by one or more members of the last name cluster.
22:224	This simple procedure identified 64% of personal names with 77% precision.
23:224	In this paper, we attempt to improve on this result by converting the clusters into features to be exploited by a general-purpose machine learning algorithm for information extraction.
24:224	In Section 2, we provide a brief description of Boosted Wrapper Induction (BWI), a pattern learner that has yielded promising results on semi-structured information extraction problems (Freitag and Kushmerick, 2000).
25:224	In Section 3, we describe our clustering approach and its particular application.
26:224	Section 4 presents the results of our experiments.
27:224	Finally, in Section 5, we assess the significance of our contribution and attempt to identify promising future directions.
28:224	2 BWI BWI decomposes the problem of recognizing field instances into two Boolean classification problems: recognizing field-initial and field-terminal tokens.
29:224	Given a target field, a separate classifier is learned for each of these problems, and the distribution of field lengths is modeled as a frequency histogram.
30:224	At application time, tokens that test positive for initial are paired with those testing positive for terminal.
31:224	If the length of a candidate instance, as defined by such a pair, is determined to have non-zero likelihood using the length histogram, a prediction is returned.
32:224	Each of the three parts of a full predictioninitial boundary, terminal boundary, and lengthis assigned a real-valued confidence.
33:224	The confidence of a boundary detection is its strength as determined by AdaBoost, while that of the length assessment is the empirical length probability, which is determined using the length histogram.
34:224	The confidence of the full prediction is the product of these three individual confidence scores.
35:224	In the event that overlapping predictions are found in this way (a rare event, empirically), the predictions with lower confidence are discarded.
36:224	In this section, we sketch those aspects of BWI relevant to the current application.
37:224	More details are available in the paper in which BWI was defined (Freitag and Kushmerick, 2000).
38:224	2.1 Boosting BWI uses generalized AdaBoost to produce each boundary classifier (Schapire and Singer, 1998).
39:224	Boosting is a procedure for improving the performance of a weak learner by repeatedly applying it to a training set, at each step modifying example weights to emphasize those examples on which the learner has done poorly in previous steps.
40:224	The output is a weighted collection of weak learner hypotheses.
41:224	Classification involves having the individual hypotheses vote, with strengths proportional to their weights, and summing overlapping votes.
42:224	Although this is the first application of BWI to NER, boosting has previously been shown to work well on this problem.
43:224	Differing from BWI in the details of the application, two recent papers nevertheless demonstrate the effectiveness of the boosting Cap Initial capital AllCap All capitals Uncap Initial lower case Alpha Entirely alphabetic characters ANum Entirely alpha-numeric characters Punc Punctuation Num Entirely numeric characters Schar Single alphabetic character Any Anything Table 2: Default wildcards used in these experiments.
44:224	paradigm for NER in several languages (Carreras et al. , 2002; Wu et al. , 2002), one of them achieving the best overall performance in a comparison of several systems (Sang, 2002).
45:224	2.2 Boundary Detectors The output of a single invocation of the weak learner in BWI is always an individual pattern, called a boundary detector.
46:224	A detector has two parts, one to match the text leading up to a boundary, the other for trailing text.
47:224	Each part is a list of zero or more elements.
48:224	In order for a boundary to match a detector, the tokens preceding the boundary (or following it) must match the corresponding elements in sequence.
49:224	For example, the detector [ms .][jones] matches boundaries preceded by the (case-normalized) two-token sequence ms . and followed by the single token jones.
50:224	Detectors are grown iteratively, beginning with an empty detector and repeatedly adding the element that best increases the ability of the current detector to discriminate true boundaries from false ones, using a cost function sensitive to the example weighting.
51:224	A look-ahead parameter allows this decision to be based on several additional context tokens.
52:224	The process terminates when no extensions yield a higher score than the current detector.
53:224	2.3 Wildcards The elements of the detector [ms .][jones]are literal elements, which match tokens using casenormalized string comparison.
54:224	More interesting elements can be introduced by defining token wildcards.
55:224	Each wildcard defines some Boolean function over the space of tokens.
56:224	Table 2 lists the baseline wildcards.
57:224	Using wildcards from this list, the example detector can be generalized to match a much broader range of boundaries (e.g. , [ms <Any>][<Cap>]).
58:224	By defining new wildcards, we can inject useful domain knowledge into the inference process, potentially improving the performance of the resulting extractor.
59:224	For example, we might define a wildcard called Honorific that matches any of ms, mr, mrs, and dr.
60:224	2.4 Boundary Wildcards In the original formulation of BWI, boundaries are identified without reference to the location of the opposing boundary.
61:224	However, we might expect that the end of a name, say, would be easier to identify if we know where it begins.
62:224	We can build detectors that exploit this knowledge by introducing a special wildcard (called Begin) that matches the beginnings of names.
63:224	In these experiments, therefore, we modify boundary detection in the following way.
64:224	Instead of two detector lists, we learn fourthe two lists as in the original formulation (call them a0a2a1a4a3a6a5a7 and a0a9a8a11a10a13a12a15a14 ), and two more lists (a0a16a1a4a3a6a5a7a18a17 and a0a9a8a11a10a13a12a15a14a19a17 ).
65:224	In generating the latter two lists, we give the learner access to these special wildcards (e.g. , the wildcard End in generating a0a20a1a4a3a21a5a7a18a17 ).
66:224	At extraction time, a0a22a1a4a3a21a5a7 and a0a20a8a11a10a13a12a15a14 are first used to detect boundaries, as before.
67:224	These detections are then used to determine which tokens match the special wildcards used by a0 a1a4a3a21a5a7a23a17 and a0 a8a11a10a13a12a15a14a19a17 . Then, instead of pairing a0 a1a4a3a6a5a7 predictions with those of a0a22a8a11a10a13a12a15a14, they are paired with those made by a0a20a8a11a10a13a12a15a14a19a17 (and a0a20a1a4a3a21a5a7a23a17 with a0a20a8a11a10a13a12a24a14 ).
68:224	In informal experiments, we found that this procedure tended to increase F1 performance by several points on a range of tasks.
69:224	We adopt it uniformly in the experiments reported here.
70:224	3 Co-Clustering As in Brown, et al (1992), we seek a partition of the vocabulary that maximizes the mutual information between term categories and their contexts.
71:224	To achieve this, we use information theoretic coclustering (Dhillon et al. , 2003), in which a space of entities, on the one hand, and their contexts, on the other, are alternately clustered to maximize mutual information between the two spaces.
72:224	3.1 Background The input to our algorithm is two finite sets of symbols, say a25a27a26a29a28a31a30a33a32a35a34a36a30a38a37a31a34a40a39a40a39a40a39a41a34a36a30a33a42a44a43a46a45 (e.g. , terms) and a47 a26 a28a31a48a49a32a35a34a36a48a50a37a40a34a40a39a40a39a40a39a41a34a36a48a51a42a53a52a54a45 (e.g. , term contexts), together with a set of co-occurrence count data consisting of a non-negative integer a55a57a56a31a58a4a59a61a60 for every pair of symbols a62a15a30a64a63a65a34a36a48a41a66a41a67 from a25 and a47 . The output is two partitions: a25 a17 a26 a28a31a30 a17 a32 a34a40a68a4a68a4a68a4a34a36a30 a17 a42 a43a70a69 a45 and a47 a17 a26a71a28a31a48 a17 a32 a34a40a68a4a68a4a68a4a34a36a48 a17 a42 a52a72a69 a45, where each a30 a17 a63 is a subset of a25 (a cluster), and each a48 a17 a63 a subset of a47 . The co-clustering algorithm chooses the partitions a25 a17 and a47 a17 to (locally) maximize the mutual information between them, under a constraint limiting the total number of clusters in each partition.
73:224	Recall that the entropy or Shannon information of a discrete distribution is: a73a75a74 a26a77a76a79a78 a56a81a80 a62a15a30a44a67a83a82a85a84 a80 a62a15a30a53a67a86a68 (1) This quantifies average improvement in ones knowledge upon learning the specific value of an event drawn from a25 . It is large or small depending on whether a25 has many or few probable values.
74:224	The mutual information between random variables a25 and a47 can be written: a87a88a74a90a89 a26a91a78 a56a40a59 a80 a62a15a30a70a34a36a48a92a67a83a82a93a84 a80 a62a15a30a70a34a36a48a92a67 a80 a62a15a30a44a67 a80 a62a15a48a92a67 (2) This quantifies the amount that one expects to learn indirectly about a25 upon learning the value of a47, or vice versa.
75:224	3.2 The Algorithm Let a25 be a random variable over vocabulary terms as found in some text corpus.
76:224	We define a47 to range over immediately adjacent tokens, encoding co-occurrences in such a way as to distinguish left from right occurrences.
77:224	Given co-occurrence matrices tabulated in this way, we perform an approximate maximization of a94a95a74 a69 a89 a69 using a simulated annealing procedure in which each trial move takes a symbol a30 or a48 out of the cluster to which it is tentatively assigned and places it into another.
78:224	Candidate moves are chosen by selecting a non-empty cluster uniformly at random, randomly selecting one of its members, then randomly selecting a destination cluster other than the source cluster.
79:224	When temperature 0 is reached, all possible moves are repeatedly attempted until no further improvements are possible.
80:224	For efficiency and noise reduction, we first cluster only the 5000 most frequent terms and context terms.
81:224	The remaining terms in the corpus vocabulary are then added by assigning each term to the cluster that maximizes the mutual information objective function.
82:224	4 Evaluation We experimented with the MUC 6 named entity data set, which consists of a training set of 318 documents, a validation set of 30 documents, and a test set of 30 documents.
83:224	All documents are annotated to identify three types of name (PERSON, ORGANIZATION, [][september]DATE [in <Num>][<Punc>] [][5 <ANum> . <ANum>]TIME [midnight][] [][$]MONEY [$ <Any> billion][] [<Alph> <Punc>][<Any> %]PCT.
84:224	[<Num> percentage <Alph>][] [mr <Any>][<Cap>]PERSON [<Cap>][, vice] [][nissan]ORG.
85:224	[inc <Any>][] [in][<Cap>, <Alph> <Punc>]LOC.
86:224	[germany][] Table 3: Sample boundary detectors for the seven MUC 6 fields produced by BWI using the baseline feature set.
87:224	An initial and terminal detector is shown for each field.
88:224	LOCATION), two types of temporal expression (DATE, TIME), and two types of numeric expression (MONEY, PERCENT).
89:224	It is common to report performance in terms of precision, recall, and their harmonic mean (F1), a convention to which we adhere.
90:224	4.1 Baseline Using the wildcards listed in Table 2, we trained BWI for 500 boosting iterations on each of the seven entity fields.
91:224	The output out each of these training runs consists of a0a2a1a3a1a5a4a7a6 a26a9a8a2a1a3a1a3a1 boundary detectors.
92:224	Look-ahead was set to 3.
93:224	Table 3 shows a few of the boundary detectors induced by this procedure.
94:224	These detectors were selected manually to illustrate the kinds of patterns generated.
95:224	Note how some of the detectors amount to field-specific gazetteer entries.
96:224	Others have more interesting (and typically intuitive) structure.
97:224	We defer quantitative evaluation to the next section, where a comparison with the cluster-enhanced extractors will be made.
98:224	4.2 Adding Cluster Features The MUC 6 dataset was produced using articles from the Wall Street Journal.
99:224	In order to produce maximally relevant clusters, we used documents from the WSJ portion of the North American News corpus as input to co-clusteringsome 119,000 documents in total.
100:224	Note that there is a temporal disparity between the MUC 6 corpus and this clustering corpus, which has an undetermined impact on performance.
101:224	[<C95>][<C73>]PERS.
102:224	[<C144> <Any> <C106>][<Uncap>] [][<C178> express]ORG.
103:224	[bank <ANum> <C146>][] [][<C72> korea]LOC.
104:224	[<C160>][<Punc>] Table 4: Sample boundary detectors for the seven MUC 6 fields produced by BWI using the expanded feature set.
105:224	72 general south north poor  73 john robert james david  95 says adds asks recalls  106 clinton dole johnson gingrich  144 mr ms dr sen  146 japan american china congress  160 washington texas california  178 american foreign local  Table 5: Most frequent members of clusters referenced by detectors in Table 4.
106:224	We used this data to produce 200 clusters, as described in Section 3.
107:224	Treating each of these clusters as an unlabeled gazetteer, we then defined corresponding wildcards.
108:224	For example, the value of wildcard <C35> only matches a term belonging to Cluster 35.
109:224	In order to reduce the training time of a given boundary learning problem, we tabulated the frequency of wildcard occurrence within three tokens of any occurrences of the target boundary and omitted from training wildcards testing true fewer than ten times.1 Table 4, which lists sample detectors from these runs, includes some that are clearly impossible to express using the baseline feature set.
110:224	An example is the first row, which matches a third-person present-tense verb used in quote attribution, followed by a first name (see Table 5).
111:224	At the same time, some of the new wildcards are employed trivially, such as the use of <C178> in the field-initial detector for the ORGANIZATION field.
112:224	Table 6 shows performance of the two variants on the individual MUC 6 fields, tested over the dryrun and formal test sets combined.
113:224	In this table, we scored each field individually using our own evaluation software.
114:224	An entity instance was judged to be correctly extracted if a prediction precisely identified its boundaries (ignoring ALT at1For the TIME field, which occurs a total of six times in the training set, this cut-off was a single occurrence.
115:224	Field F1 Prec Rec Base 0.766 0.765 0.768DATE Clust 0.782 0.776 0.789 Base 0.667 1.000 0.500TIME Clust 0.667 1.000 0.500 Base 0.938 0.926 0.949MONEY Clust 0.943 0.938 0.949 Base 0.922 0.855 1.000PERCENT Clust 0.930 0.869 1.000 Base 0.827 0.810 0.844PERSON Clust 0.892 0.859 0.927 Base 0.587 0.811 0.460ORG.
116:224	Clust 0.733 0.796 0.680 Base 0.726 0.675 0.785LOCATION Clust 0.724 0.648 0.821 Table 6: Performance on the seven MUC 6 fields, without (Base) and with (Clust) cluster-based features.
117:224	Significantly better precision or recall scores, at the 95% confidence level, are in boldface.
118:224	tributes).
119:224	Non-matching predictions and missed entities were counted as false positives and false negatives, respectively.
120:224	We assessed the statistical significance of precision and recall scores by computing beta confidence intervals at the 95% level.
121:224	In the table, the higher precision or recall is in boldface if its separation from the lower score is significant.
122:224	Except for TIME and LOCATION, all fields benefit from inclusion of the cluster features.
123:224	TIME, which is scarce in the training and test sets, is insensitive to their inclusion.
124:224	The effect on LOCATION is more interesting.
125:224	It shares in the general tendency of cluster features to increase recall, but loses precision as a result.2 Although the increase in recall is approximately the same as the loss in precision, the F1 score, which is more heavily influenced by the lower of precision and recall, drops slightly.
126:224	While the effect of the cluster features on precision is inconsistent, they typically benefit recall.
127:224	This effect is most dramatic in the case of ORGANIZATION, where, at the expense of a small drop in precision, recall increases by more than 20 points.
128:224	The somewhat counter-intuitive improvements in precision on some fields (particularly the significant improvement on PERSON) is attributable to our learning framework.
129:224	Boosting for a sufficient number of iterations forces a learner to account for all boundary tokens through one or more detectors.
130:224	To the exent that the baselines features are unable to 2Note, however, that none of the differences observed for LOCATION are significant at the 95% level.
131:224	account for as many of the boundary tokens, it is forced to learn a larger number of over-specialized detectors that rely on questionable patterns in the data.
132:224	Depending on the task, these detectors can lead to a larger proportion of false positives.
133:224	The relatively weak result for DATE comes as a surprise.
134:224	Inspection of the data leads us to attribute this to two factors.
135:224	On the one hand, there is considerable temporal drift between the training and test sets.
136:224	Many of the dates are specific to contemporaneous events; patterns based on specific years, therefore, generalize in only a limited way.
137:224	At the same time, the notion of date, as understood in the MUC 6 corpus, is reasonably subtle.
138:224	Meaning roughly non-TIME temporal expression, it includes everything from shorthand date expressions to more interesting phrases, such as, the first six months of fiscal 1994. In passing we note a few potentially relevant idiosyncrasies in these experiments.
139:224	Most significant is a representational choice we made in tokenizing the cluster corpus.
140:224	In tallying frequencies we treated all numeric expressions as occurrences of a special term, *num*.
141:224	Consequently, the tokens 1989 and 10,000 are treated as instances of the same term, and clustering has no opportunity to distinguish, say, years from monetary amounts.
142:224	The (perhaps) disappointing performance on the relatively simple fields, TIME and PERCENT, somewhat under-reports the strength of the learner.
143:224	As noted above, TIME occurs only very infrequently.
144:224	Consequently, little training data is available for this field and mistakes (BWI missed one of the three instances in the test set) have a large effect on the TIME-specific scores.
145:224	In the case of PERCENT, we ignored MUC instructions not to attempt to recognize instances in tabular regions.
146:224	One of the documents contains a significant number of unlabeled percentages in such a table.
147:224	BWI duly recognized theseto the detriment of the reported precision.
148:224	4.3 MUC Evaluation For comparison with numbers reported in the literature, we used the learned extractors to produce mark-up and evaluated the result using the MUC 6 scorer.
149:224	The MUC 6 evaluation framework differs from ours in two key ways.
150:224	Most importantly, all entity types are to be processed simultaneously.
151:224	We benefit from this framework, since spurious predictions for one entity type may be superseded by correct predictions for a related type.
152:224	The opportunity is greatest for the three name types; in inspecting the false positives, we observed a number of confuField F1 Prec Rec Base 0.91 0.91 0.91DATE Clust 0.92 0.90 0.94 Base 0 0 0TIME Clust 0 0 0 Base 0.95 0.94 0.96MONEY Clust 0.95 0.95 0.96 Base 0.97 0.94 1.0PERCENT Clust 1.0 1.0 1.0 Base 0.88 0.91 0.86PERSON Clust 0.94 0.94 0.95 Base 0.62 0.78 0.52ORG.
153:224	Clust 0.79 0.84 0.74 Base 0.86 0.86 0.87LOCATION Clust 0.86 0.80 0.92 Base 0.79 0.85 0.73ALL Clust 0.87 0.88 0.86 Table 7: Performance on the markup task, as scored by the MUC 6 scorer.
154:224	sions among these fields.3 The MUC scorer is also more lenient than ours, awarding points for extraction of alternative strings and forgiving the inclusion of certain functional tokens in the extracted text.
155:224	In moving to the multi-entity extraction setting, the obvious approach is to collect predictions from all extractors simultaneously.
156:224	However, this requires a strategy for dealing with overlapping predictions (e.g. , a single text fragment labeled as both a person and organization).
157:224	We resolve such conflicts by preferring in each case the extraction with the highest confidence.
158:224	In order to render confidence scores more comparable, we normalized the weights of detectors making up each boundary classifier so they sum to one.
159:224	A comparison of Table 7 with Table 6 suggests the extent to which BWI benefits from the multifield mark-up setting.
160:224	Note that, here, we used only the formal test set for evaluation, in contrast with the numbers in Table 6, which combine the two test sets.
161:224	The lift we observe from cluster features is also in evidence here, and is most evident as an increase in recall, particularly of PERSON and ORGANIZATION.
162:224	There is now also an increase in global precision, attributable in large part to the benefit of extracting multiple fields simultaneously.
163:224	The F1 score produced by BWI is comparable to the best machine-learning-based results re3For example, companies are occasionally named after people (e.g. , Liz Claiborne).
164:224	ported elsewhere.
165:224	Bikel, at al (1997), reports summary F1 of 0.93 on the same test set, but using a model trained on 450,000 words.
166:224	We count approximately 130,000 words in the experiments reported here.
167:224	The numbers reported by Bennett, et al (1997), for PERSON, ORGANIZATION, and LOCATION (F1 of 0.947, 0.815, and 0.925, respectively), are slightly better than the numbers BWI reaches on the same fields.
168:224	Note, however, that the features provided to their learner include syntactic labels and carefully engineered semantic categories, whereas we eschew knowledgeand labor-intensive resources.
169:224	This has important implications for the portability of the approaches to new domains and languages.
170:224	By taking a few post-processing steps, it is possible to realize further improvements.
171:224	For example, the learner occasionally identifies terms and phrases which some simple rules can reliably reject.
172:224	By suppressing any prediction that consists entirely of a stopword, we increase the precision of both ORGANIZATION and LOCATION to 0.86 (from 0.84 and 0.80) and overall F1 to 0.88.
173:224	We can also exploit what Cucerzan and Yarowsky (1999) call the one sense per discourse phenomenon, the tendency of terms to have a fixed meaning within a single document.
174:224	By marking up unmarked strings that match extracted entity instances in the same document, we can improve the recall of some fields.
175:224	We added this postprocessing step for the PERSON and ORGANIZATION fields.
176:224	This increased recall of PERSON from 0.95 to 0.98 and of ORGANIZATION from 0.74 to 0.79 with minimal changes to precision and a slight improvement in summary F1.
177:224	4.4 Analysis and Related Work The promise of this general methodsupervised learning on small training set using features derived from a larger unlabeled setlies in the support it provides for rapid deployment in novel domains and languages.
178:224	Without relying on any linguistic resources more advanced than a tokenizer and some orthographic features, we can produce a NER module using only a few annotated documents.
179:224	How few depends ultimately on the difficulty of the domain.
180:224	We might also expect the benefit of distributional features to decrease with increasing training set size.
181:224	Figure 1 displays the F1 learningcurve performance of BWI, both with and without cluster features, on the two fields that benefit the greatest from these features, PERSON and ORGANIZATION.
182:224	As expected, the difference appears to be greatest on the low end of the horizontal axis (al0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 50 100 150 200 250 300 350 Formal test F1 Training documents PERSON (clust.)
183:224	PERSON (base) ORG.
184:224	(clust.)
185:224	ORG.
186:224	(base) Figure 1: F1 as a function of training set size in number of documents.
187:224	though overfitting complicates the comparison).
188:224	At the same time, the improvement is fairly consistent at all training set sizes.
189:224	Either the baseline feature set is ultimately too impoverished for this task, or, more likely, the complete MUC 6 training set (318 documents) is small for this class of learner.
190:224	Techniques to lessen the need for annotation for NER have received a fair amount of attention recently.
191:224	The prevailing approach to this problem is a bootstrapping technique, in which, starting with a few hand-labeled examples, the system iteratively adds automatic labels to a corpus, training itself, as it were.
192:224	Examples of this are Cucerzan and Yarowsky (1999), Thelen and Riloff (2002), and Collins and Singer (1999).
193:224	These techniques address the same problem as this paper, but are otherwise quite different from the work described here.
194:224	The labeling method (seeding) is an indirect form of corpus annotation.
195:224	The promise of all such approaches is that, by starting with a small number of seeds, reasonable results can be achieved at low expense.
196:224	However, it is difficult to tell how much labeling corresponds to a given number of seeds, since this depends on the coverage of the seeds.
197:224	Note, too, that any bootstrapping approach must confront the problem of instability; poor initial decisions by a bootstrapping algorithm can lead to large eventual performance degradations.
198:224	We might expect a lightly supervised learner with access to features based on a full-corpus analysis to yield more consistently strong results.
199:224	Of the three approaches mentioned above, only Cucerzan and Yarowsky do not presuppose a syntactic analysis of the corpus, so their work is perhaps most comparable to this one.
200:224	Of course, comparisons must be strongly qualified, given the different labeling methods and data sets.
201:224	Nevertheless, performance of cluster-enhanced BWI at the low end of the horizontal axis compares favorably with the English F1 performance of 0.543 they report using 190 seed words.
202:224	And, arguably, annotating 10-20 documents is no more labor intensive than assembling a list of 190 seed words.
203:224	Strong corroboration for the approach advocated in this paper is provided by Miller, et al (2004), in which cluster-based features are combined with a sequential maximum entropy model proposed in Collins (2002) to advance the state of the art.
204:224	In addition, using active learning, the authors are able to reduce human labeling effort by an order of magnitude.
205:224	Miller, et al, use a proprietary data set for training and testing, so it is difficult to make a close comparison of outcomes.
206:224	At roughly comparable training set sizes, they appear to achieve a score of about 0.89 (F1) with a conventional HMM, versus 0.93 using the discriminative learner trained with cluster features (compared with 0.86 reached by BWI).
207:224	Both the HMM and Collins model are constrained to account for an entire sentence in tagging it, making determinations for all fields simultaneously, in contrast to the individual, local boundary detections made by BWI.
208:224	This characteristic probably accounts for the accuracy advantage they appear to enjoy.
209:224	An interesting distinguishing feature of Miller, et al, is their use of hierarchical clustering.
210:224	While much is made of the ability of their approach to accomodate different levels of granularity automatically, no evidence is provided that the hierarchy provides real benefit.
211:224	At the same time, our work shows that significant gains can be realized with a single, sufficiently granular partition of terms.
212:224	It is known, moreover, that greedy agglomerative clustering leads to partitions that are sub-optimal in terms of a mutual information objective function (see, for example, Brown, et al (1992)).
213:224	Ultimately, it is left to future research to determine how sensitive, if at all, the NER gains are to the details of the clustering.
214:224	5 Conclusion There are several ways in which this work might be extended and improved, both in its particular form and in general: a0 BWI models initial and terminal boundaries, but ignores characteristics of the extracted phrase other than its length.
215:224	We are exploring mechanisms for modeling relevant phrasal structure.
216:224	a0 While global statistical approaches, such as sequential averaged perceptrons or CRFs (McCallum and Li, 2003), appear better suited to the NER problem than local symbolic learners, the two approaches search different hypothesis spaces.
217:224	Based on the surmise that, by combining them, we can realize improvements over either in isolation, we are exploring mechanisms for integration.
218:224	a0 The distributional clusters we find are independent of the problem to which we want to apply them and may sometimes be inappropriate or have the wrong granularity.
219:224	We are exploring ways to produce groupings that are sensitive to the task at hand.
220:224	Our results clearly establish that an unsupervised distributional analysis of a text corpus can produce features that lead to enhanced precision and, especially, recall in information extraction.
221:224	We have successfully used these features in lieu of domainspecific, labor-intensive resources, such as syntactic analysis and special-purpose gazetteers.
222:224	Distributional analysis, combined with light supervision, is an effective, stable alternative to bootstrapping methods.
223:224	Acknowledgments This material is based on work funded in whole or in part by the U.S. Government.
224:224	Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors, and do not necessarily reflect the views of the U.S. Government.

