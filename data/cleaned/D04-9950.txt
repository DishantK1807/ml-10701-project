1:178	Unsupervised Domain Relevance Estimation for Word Sense Disambiguation Al o Gliozzo and Bernardo Magnini and Carlo Strapparava ITC-irst, Istituto per la Ricerca Scienti ca e Tecnologica, I-38050 Trento, ITALY fgliozzo, magnini, strappag@itc.it Abstract This paper presents Domain Relevance Estimation (DRE), a fully unsupervised text categorization technique based on the statistical estimation of the relevance of a text with respect to a certain category.
2:178	We use a pre-de ned set of categories (we call them domains) which have been previously associated to WORDNET word senses.
3:178	Given a certain domain, DRE distinguishes between relevant and non-relevant texts by means of a Gaussian Mixture model that describes the frequency distribution of domain words inside a large-scale corpus.
4:178	Then, an Expectation Maximization algorithm computes the parameters that maximize the likelihood of the model on the empirical data.
5:178	The correct identi cation of the domain of the text is a crucial point for Domain Driven Disambiguation, an unsupervised Word Sense Disambiguation (WSD) methodology that makes use of only domain information.
6:178	Therefore, DRE has been exploited and evaluated in the context of a WSD task.
7:178	Results are comparable to those of state-ofthe-art unsupervised WSD systems and show that DRE provides an important contribution.
8:178	1 Introduction A fundamental issue in text processing and understanding is the ability to detect the topic (i.e. the domain) of a text or of a portion of it.
9:178	Indeed, domain detection allows a number of useful simpli cations in text processing applications, such as, for instance, in Word Sense Disambiguation (WSD).
10:178	In this paper we introduce Domain Relevance Estimation (DRE) a fully unsupervised technique for domain detection.
11:178	Roughly speaking, DRE can be viewed as a text categorization (TC) problem (Sebastiani, 2002), even if we do not approach the problem in the standard supervised setting requiring category labeled training data.
12:178	In fact, recently, unsupervised approaches to TC have received more and more attention in the literature (see for example (Ko and Seo, 2000).
13:178	We assume a pre-de ned set of categories, each de ned by means of a list of related terms.
14:178	We call such categories domains and we consider them as a set of general topics (e.g. SPORT, MEDICINE, POLITICS) that cover the main disciplines and areas of human activity.
15:178	For each domain, the list of related words is extracted from WORDNET DOMAINS (Magnini and Cavagli a, 2000), an extension of WORDNET in which synsets are annotated with domain labels.
16:178	We have identi ed about 40 domains (out of 200 present in WORDNET DOMAINS) and we will use them for experiments throughout the paper (see Table 1).
17:178	DRE focuses on the problem of estimating a degree of relatedness of a certain text with respect to the domains in WORDNET DOMAINS.
18:178	The basic idea underlying DRE is to combine the knowledge in WORDNET DOMAINS and a probabilistic framework which makes use of a large-scale corpus to induce domain frequency distributions.
19:178	Speci cally, given a certain domain, DRE considers frequency scores for both relevant and non-relevant texts (i.e. texts which introduce noise) and represent them by means of a Gaussian Mixture model.
20:178	Then, an Expectation Maximization algorithm computes the parameters that maximize the likelihood of the empirical data.
21:178	DRE methodology originated from the effort to improve the performance of Domain Driven Disambiguation (DDD) system (Magnini et al. , 2002).
22:178	DDD is an unsupervised WSD methodology that makes use of only domain information.
23:178	DDD assignes the right sense of a word in its context comparing the domain of the context to the domain of each sense of the word.
24:178	This methodology exploits WORDNET DOMAINS information to estimate both Domain #Syn Domain #Syn Domain #Syn Factotum 36820 Biology 21281 Earth 4637 Psychology 3405 Architecture 3394 Medicine 3271 Economy 3039 Alimentation 2998 Administration 2975 Chemistry 2472 Transport 2443 Art 2365 Physics 2225 Sport 2105 Religion 2055 Linguistics 1771 Military 1491 Law 1340 History 1264 Industry 1103 Politics 1033 Play 1009 Anthropology 963 Fashion 937 Mathematics 861 Literature 822 Engineering 746 Sociology 679 Commerce 637 Pedagogy 612 Publishing 532 Tourism 511 Computer Science 509 Telecommunication 493 Astronomy 477 Philosophy 381 Agriculture 334 Sexuality 272 Body Care 185 Artisanship 149 Archaeology 141 Veterinary 92 Astrology 90 Table 1: Domain distribution over WORDNET synsets.
25:178	the domain of the textual context and the domain of the senses of the word to disambiguate.
26:178	The former operation is intrinsically an unsupervised TC task, and the category set used has to be the same used for representing the domain of word senses.
27:178	Since DRE makes use of a xed set of target categories (i.e. domains) and since a document collection annotated with such categories is not available, evaluating the performance of the approach is a problem in itself.
28:178	We have decided to perform an indirect evaluation using the DDD system, where unsupervised TC plays a crucial role.
29:178	The paper is structured as follows.
30:178	Section 2 introduces WORDNET DOMAINS, the lexical resource that provides the underlying knowledge to the DRE technique.
31:178	In Section 3 the problem of estimating domain relevance for a text is introduced.
32:178	In particular, Section 4 brie y sketchs the WSD system used for evaluation.
33:178	Finally, Section 5 describes a number of evaluation experiments we have carried out.
34:178	2 Domains, WORDNET and Texts DRE heavily relies on domain information as its main knowledge source.
35:178	Domains show interesting properties both from a lexical and a textual point of view.
36:178	Among these properties there are: (i) lexical coherence, since part of the lexicon of a text is composed of words belonging to the same domain; (ii) polysemy reduction, because the potential ambiguity of terms is sensibly lower if the domain of the text is speci ed; and (iii) lexical identi ability of texts domain, because it is always possible to assign one or more domains to a given text by considering term distributions in a bag-of-words approach.
37:178	Experimental evidences of these properties are reported in (Magnini et al. , 2002).
38:178	In this section we describe WORDNET DOMAINS1 (Magnini and Cavagli a, 2000), a lexical resource that attempts a systematization of relevant aspects in domain organization and representation.
39:178	WORDNET DOMAINS is an extension of WORDNET (version 1.6) (Fellbaum, 1998), in which each synset is annotated with one or more domain labels, selected from a hierarchically organized set of about two hundred labels.
40:178	In particular, issues concerning the completeness of the domain set, the balancing among domains and the granularity of domain distinctions, have been addressed.
41:178	The domain set used in WORDNET DOMAINS has been extracted from the Dewey Decimal Classi cation (Comaroni et al. , 1989), and a mapping between the two taxonomies has been computed in order to ensure completeness.
42:178	Table 2 shows how the senses for a word (i.e. the noun bank) have been associated to domain label; the last column reports the number of occurrences of each sense in Semcor2.
43:178	Domain labeling is complementary to information already present in WORDNET.
44:178	First of all, a domain may include synsets of different syntactic categories: for instance MEDICINE groups together senses from nouns, such as doctor#1 and hospital#1, and from verbs, such as operate#7.
45:178	Second, a domain may include senses from different WORDNET sub-hierarchies (i.e. deriving from different unique beginners or from different lexicographer les ).
46:178	For example, SPORT contains senses such as athlete#1, deriving from life form#1, game equipment#1 from physical object#1, sport#1 1WORDNET DOMAINS is freely available at http://wndomains.itc.it 2SemCor is a portion of the Brown corpus in which words are annotated with WORDNET senses.
47:178	Sense Synset and Gloss Domains Semcor frequencies #1 depository nancial institution, bank, banking concern, banking company (a nancial institution.
48:178	) ECONOMY 20 #2 bank (sloping land.
49:178	) GEOGRAPHY, GEOLOGY 14 #3 bank (a supply or stock held in reserve.
50:178	) ECONOMY #4 bank, bank building (a building.
51:178	) ARCHITECTURE, ECONOMY #5 bank (an arrangement of similar objects) FACTOTUM 1 #6 savings bank, coin bank, money box, bank (a container.
52:178	) ECONOMY #7 bank (a long ridge or pile.
53:178	) GEOGRAPHY, GEOLOGY 2 #8 bank (the funds held by a gambling house.
54:178	) ECONOMY, PLAY #9 bank, cant, camber (a slope in the turn of a road.
55:178	) ARCHITECTURE #10 bank (a ight maneuver.
56:178	) TRANSPORT Table 2: WORDNET senses and domains for the word bank . from act#2, and playing field#1 from location#1.
57:178	Domains may group senses of the same word into thematic clusters, which has the important sideeffect of reducing the level of ambiguity when we are disambiguating to a domain.
58:178	Table 2 shows an example.
59:178	The word bank has ten different senses in WORDNET 1.6: three of them (i.e. bank#1, bank#3 and bank#6) can be grouped under the ECONOMY domain, while bank#2 and bank#7 both belong to GEOGRAPHY and GEOLOGY.
60:178	Grouping related senses is an emerging topic in WSD (see, for instance (Palmer et al. , 2001)).
61:178	Finally, there are WORDNET synsets that do not belong to a speci c domain, but rather appear in texts associated with any domain.
62:178	For this reason, a FACTOTUM label has been created that basically includes generic synsets, which appear frequently in different contexts.
63:178	Thus the FACTOTUM domain can be thought of as a placeholder for all other domains.
64:178	3 Domain Relevance Estimation for Texts The basic idea of domain relevance estimation for texts is to exploit lexical coherence inside texts.
65:178	From the domain point of view lexical coherence is equivalent to domain coherence, i.e. the fact that a great part of the lexicon inside a text belongs to the same domain.
66:178	From this observation follows that a simple heuristic to approach this problem is counting the occurrences of domain words for every domain inside the text: the higher the percentage of domain words for a certain domain, the more relevant the domain will be for the text.
67:178	In order to perform this operation the WORDNET DOMAINS information is exploited, and each word is assigned a weighted list of domains considering the domain annotation of its synsets.
68:178	In addition, we would like to estimate the domain of the text locally.
69:178	Local estimation of domain relevance is very important in order to take into account domain shifts inside the text.
70:178	The methodology used to estimate domain frequency is described in subsection 3.1.
71:178	Unfortunately the simple local frequency count is not a good domain relevance measure for several reasons.
72:178	The most signi cant one is that very frequent words have, in general, many senses belonging to different domains.
73:178	When words are used in texts, ambiguity tends to disappear, but it is not possible to assume knowing their actual sense (i.e. the sense in which they are used in the context) in advance, especially in a WSD framework.
74:178	The simple frequency count is then inadequate for relevance estimation: irrelevant senses of ambiguous words contribute to augment the nal score of irrelevant domains, introducing noise.
75:178	The level of noise is different for different domains because of their different sizes and possible differences in the ambiguity level of their vocabularies.
76:178	In subsection 3.2 we propose a solution for that problem, namely the Gaussian Mixture (GM) approach.
77:178	This constitutes an unsupervised way to estimate how to differentiate relevant domain information in texts from noise, because it requires only a large-scale corpus to estimate parameters in an Expectation Maximization (EM) framework.
78:178	Using the estimated parameters it is possible to describe the distributions of both relevant and non-relevant texts, converting the DRE problem into the problem of estimating the probability of each domain given its frequency score in the text, in analogy to the bayesian classi cation framework.
79:178	Details about the EM algorithm for GM model are provided in subsection 3.3.
80:178	3.1 Domain Frequency Score Let t 2 T, be a text in a corpus T composed by a list of words wt1;::: ;wtq.
81:178	Let D = fD1;D2;:::;Ddg be the set of domains used.
82:178	For each domain Dk the domain frequency score is computed in a window of c words around wtj.
83:178	The domain frequency score is de ned by formula (1).
84:178	F(Dk; t; j) = j+cX i=j c Rword(Dk; wti)G(i; j; ( c2)2) (1) where the weight factor G(x; ; 2) is the density of the normal distribution with mean and standard deviation at point x and Rword(D;w) is a function that return the relevance of a domain D for a word w (see formula 3).
85:178	In the rest of the paper we use the notation F(Dk;t) to refer to F(Dk;t;m), where m is the integer part of q=2 (i.e. the central point of the text q is the text length).
86:178	Here below we see that the information contained in WORDNET DOMAINS can be used to estimate Rword(Dk;w), i.e. domain relevance for the word w, which is derived from the domain relevance of the synsets in which w appears.
87:178	As far as synsets are concerned, domain information is represented by the function Dom : S ) P(D)3 that returns, for each synset s 2 S, where S is the set of synsets in WORDNET DOMAINS, the set of the domains associated to it.
88:178	Formula (2) denes the domain relevance estimation function (remember that d is the cardinality of D): Rsyn(D; s) = 8< : 1=jDom(s)j : if D 2 Dom(s) 1=d : if Dom(s) = fFACTOTUMg 0 : otherwise (2) Intuitively, Rsyn(D;s) can be perceived as an estimated prior for the probability of the domain given the concept, as expressed by the WORDNET DOMAINS annotation.
89:178	Under these settings FACTOTUM (generic) concepts have uniform and low relevance values for each domain while domain concepts have high relevance values for a particular domain.
90:178	The de nition of domain relevance for a word is derived directly from the one given for concepts.
91:178	Intuitively a domain D is relevant for a word w if D is relevant for one or more senses c of w. More formally let V = fw1;w2;:::wjV jg be the vocabulary, let senses(w) = fsjs 2 S;s is a sense of wg (e.g. any synset in WORDNET containing the word w).
92:178	The domain relevance function for a word R : D V ) [0;1] is de ned as follows: Rword(Di; w) = 1jsenses(w)j X s2senses(w) Rsyn(Di; s) (3) 3P(D) denotes the power set of D 3.2 The Gaussian Mixture Algorithm As explained at the beginning of this section, the simple local frequency count expressed by formula (1) is not a good domain relevance measure.
93:178	In order to discriminate between noise and relevant information, a supervised framework is typically used and signi cance levels for frequency counts are estimated from labeled training data.
94:178	Unfortunately this is not our case, since no domain labeled text corpora are available.
95:178	In this section we propose a solution for that problem, namely the Gaussian Mixture approach, that constitutes an unsupervised way to estimate how to differentiate relevant domain information in texts from noise.
96:178	The Gaussian Mixture approach consists of a parameter estimation technique based on statistics of word distribution in a large-scale corpus.
97:178	The underlying assumption of the Gaussian Mixture approach is that frequency scores for a certain domain are obtained from an underlying mixture of relevant and non-relevant texts, and that the scores for relevant texts are signi cantly higher than scores obtained for the non-relevant ones.
98:178	In the corpus these scores are distributed according to two distinct components.
99:178	The domain frequency distribution which corresponds to relevant texts has the higher value expectation, while the one pertaining to non relevant texts has the lower expectation.
100:178	Figure 1 describes the probability density function (PDF) for domain frequency scores of the SPORT domain estimated on the BNC corpus4 (BNC-Consortium, 2000) using formula (1).
101:178	The empirical PDF, describing the distribution of frequency scores evaluated on the corpus, is represented by the continuous line.
102:178	From the graph it is possible to see that the empirical PDF can be decomposed into the sum of two distributions, D = SPORT and D = non-SPORT . Most of the probability is concentrated on the left, describing the distribution for the majority of non relevant texts; the smaller distribution on the right is assumed to be the distribution of frequency scores for the minority of relevant texts.
103:178	Thus, the distribution on the left describes the noise present in frequency estimation counts, which is produced by the impact of polysemous words and of occasional occurrences of terms belonging to SPORT in non-relevant texts.
104:178	The goal of the technique is to estimate parameters describing the distribution of the noise along texts, in order to as4The British National Corpus is a very large (over 100 million words) corpus of modern English, both spoken and written.
105:178	0 50 100 150 200 0 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 Density Non-relevant Relevant F(D, t) density function Figure 1: Gaussian mixture for D = SPORT sociate high relevance values only to relevant frequency scores (i.e. frequency scores that are not related to noise).
106:178	It is reasonable to assume that such noise is normally distributed because it can be described by a binomial distribution in which the probability of the positive event is very low and the number of events is very high.
107:178	On the other hand, the distribution on the right is the one describing typical frequency values for relevant texts.
108:178	This distribution is also assumed to be normal.
109:178	A probabilistic interpretation permits the evaluation of the relevance value R(D;t;j) of a certain domain D for a new text t in a position j only by considering the domain frequency F(D;t;j).
110:178	The relevance value is de ned as the conditional probability P(DjF(D;t;j)).
111:178	Using Bayes theorem we estimate this probability by equation (4).
112:178	R(D; t; j) = P(DjF(D; t; j)) = (4) = P(F(D; t; j)jD)P(D)P(F(D; t; j)jD)P(D) + P(F(D; t; j)jD)P(D) where P(F(D;t;j)jD) is the value of the PDF describing D calculated in the point F(D;t;j), P(F(D;t;j)jD) is the value of the PDF describing D, P(D) is the area of the distribution describing D and P(D) is the area of the distribution for D. In order to estimate the parameters describing the PDF of D and D the Expectation Maximization (EM) algorithm for the Gaussian Mixture Model (Redner and Walker, 1984) is exploited.
113:178	Assuming to model the empirical distribution of domain frequencies using a Gaussian mixture of two components, the estimated parameters can be used to evaluate domain relevance by equation (4).
114:178	3.3 The EM Algorithm for the GM model In this section some details about the algorithm for parameter estimation are reported.
115:178	It is well known that a Gaussian mixture (GM) allows to represent every smooth PDF as a linear combination of normal distributions of the type in formula 5 p(xj ) = mX j=1 ajG(x; j; j) (5) with aj 0 and mX j=1 aj = 1 (6) and G(x; ; ) = 1p2 e (x ) 2 2 2 (7) and = ha1; 1; 1;::: ;am; m; mi is a parameter list describing the gaussian mixture.
116:178	The number of components required by the Gaussian Mixture algorithm for domain relevance estimation is m = 2.
117:178	Each component j is univocally determined by its weight aj, its mean j and its variance j. Weights represent also the areas of each component, i.e. its total probability.
118:178	The Gaussian Mixture algorithm for domain relevance estimation exploits a Gaussian Mixture to approximate the empirical PDF of domain frequency scores.
119:178	The goal of the Gaussian Mixture algorithm is to nd the GM that maximize the likelihood on the empirical data, where the likelihood function is evaluated by formula (8).
120:178	L(T ;D; ) = Y t2T p(F(D;t)j ) (8) More formally, the EM algorithm for GM models explores the space of parameters in order to nd the set of parameters such that the maximum likelihood criterion (see formula 9) is satis ed.
121:178	D = argmax 0 L(T ;D; 0) (9) This condition ensures that the obtained model ts the original data as much as possible.
122:178	Estimation of parameters is the only information required in order to evaluate domain relevance for texts using the Gaussian Mixture algorithm.
123:178	The Expectation Maximization Algorithm for Gaussian Mixture Models (Redner and Walker, 1984) allows to ef ciently perform this operation.
124:178	The strategy followed by the EM algorithm is to start from a random set of parameters 0, that has a certain initial likelihood value L0, and then iteratively change them in order to augment likelihood at each step.
125:178	To this aim the EM algorithm exploits a growth transformation of the likelihood function ( ) = 0 such that L(T ;D; ) 6 L(T ;D; 0).
126:178	Applying iteratively this transformation starting from 0 a sequence of parameters is produced, until the likelihood function achieve a stable value (i.e. Li+1 Li 6 ).
127:178	In our settings the transformation function is de ned by the following set of equations, in which all the parameters have to be solved together.
128:178	( ) = (ha1; 1; 1;a2; 2; 2i) (10) = ha01; 01; 01;a02; 02; 02i a0j = 1jT j jT jX k=1 ajG(F(D;tk); j; j) p(F(D;tk); ) (11) 0j = PjT j k=1 F(D;tk) ajG(F(D;tk); j; j) p(F(D;tk); )P jT j k=1 ajG(F(D;tk); j; j) p(F(D;tk); ) (12) 0j = PjT j k=1 (F(D;tk) 0j)2 aiG(F(D;tk); i; i) p(F(D;tk); )P jT j k=1 ajG(F(D;tk); j; j) p(F(D;tk); ) (13) As said before, in order to estimate distribution parameters the British National Corpus (BNCConsortium, 2000) was used.
129:178	Domain frequency scores have been evaluated on the central position of each text (using equation 1, with c = 50).
130:178	In conclusion, the EM algorithm was used to estimate parameters to describe distributions for relevant and non-relevant texts.
131:178	This learning method is totally unsupervised.
132:178	Estimated parameters has been used to estimate relevance values by formula (4).
133:178	4 Domain Driven Disambiguation DRE originates to improve the performance of Domain Driven Disambiguation (DDD).
134:178	In this section, a brief overview of DDD is given.
135:178	DDD is a WSD methodology that only makes use of domain information.
136:178	Originally developed to test the role of domain information for WSD, the system is capable to achieve a good precision disambiguation.
137:178	Its results are affected by a low recall, motivated by the fact that domain information is suf cient to disambiguate only domain words . The disambiguation process is done comparing the domain of the context and the domains of each sense of the lemma to disambiguate.
138:178	The selected sense is the one whose domain is relevant for the context5.
139:178	In order to represent domain information we introduced the notion of Domain Vectors (DV), that are data structures that collect domain information.
140:178	These vectors are de ned in a multidimensional space, in which each domain represents a dimension of the space.
141:178	We distinguish between two kinds of DVs: (i) synset vectors, which represent the relevance of a synset with respect to each considered domain and (ii) text vectors, which represent the relevance of a portion of text with respect to each domain in the considered set.
142:178	More formally let D = fD1;D2;:::;Ddg be the set of domains, the domain vector ~s for a synset s is de ned as hR(D1;s);R(D2;s);::: ;R(Dd;s)i where R(Di;s) is evaluated using equation (2).
143:178	In analogy the domain vector ~tj for a text t in a given position j is de ned as hR(D1;t;j);R(D2;t;j);::: ;R(Dd;t;j)i where R(Di;t;j) is evaluated using equation (4).
144:178	The DDD methodology is performed basically in three steps: 1.
145:178	Compute ~t for the context t of the word w to be disambiguated 2.
146:178	Compute ^s = argmaxs2Senses(w)score(s; w; t) where score(s;w; t) = P(sjw) sim(~s;~t)P s2Senses(w) P(sjw) sim(~s;~t) 3.
147:178	if score(^s; w; t) > k (where k 2 [0; 1] is a con dence threshold) select sense ^s, else do not provide any answer The similarity metric used is the cosine vector similarity, which takes into account only the direction of the vector (i.e. the information regarding the domain).
148:178	P(sjw) describes the prior probability of sense s for word w, and depends on the distribution of the sense annotations in the corpus.
149:178	It is estimated by statistics from a sense tagged corpus (we used SemCor)6 or considering the sense order in 5Recent works in WSD demonstrate that an automatic estimation of domain relevance for texts can be pro table used to disambiguate words in their contexts.
150:178	For example, (Escudero et al. , 2001) used domain relevance extraction techniques to extract features for a supervised WSD algorithm presented at the Senseval-2 competion, improving the system accuracy of about 4 points for nouns, 1 point for verbs and 2 points for adjectives, con rming the original intuition that domain information is very useful to disambiguate domain words, i.e. words which are strongly related to the domain of the text.
151:178	6Admittedly, this may be regarded as a supervised component of the generally unsupervised system.
152:178	Yet, we considered this component as legitimate within an unsupervised frameWORDNET, which roughly corresponds to sense frequency order, when no example of the word to disambiguate are contained in SemCor.
153:178	In the former case the estimation of P(sjw) is based on smoothed statistics from the corpus (P(sjw) = occ(s;w)+ occ(w)+jsenses(w)j, where is a smoothing factor empirically determined).
154:178	In the latter case P(sjw) can be estimated in an unsupervised way considering the order of senses in WORDNET (P(sjw) = 2(jsenses(w)j sensenumber(s;w)+1)jsenses(w)j(jsenses(w)j+1) where sensenumber(s;w) returns the position of sense s of word w in the sense list for w provided by WORDNET.
155:178	5 Evaluation in a WSD task We used the WSD framework to perform an evaluation of the DRE technique by itself.
156:178	As explained in Section 1 Domain Relevance Estimation is not a common Text Categorization task.
157:178	In the standard framework of TC, categories are learned form examples, that are used also for test.
158:178	In our case information in WORDNET DOMAINS is used to discriminate, and a test set, i.e. a corpus of texts categorized using the domain of WORDNET DOMAINS, is not available.
159:178	To evaluate the accuracy of the domain relevance estimation technique described above is thus necessary to perform an indirect evaluation.
160:178	We evaluated the DDD algorithm described in Section 4 using the dataset of the Senseval-2 allwords task (Senseval-2, 2001; Preiss and Yarowsky, 2002).
161:178	In order to estimate domain vectors for the contexts of the words to disambiguate we used the DRE methodology described in Section 3.
162:178	Varying the con dence threshold k, as described in Section 4, it is possible to change the tradeoff between precision and recall.
163:178	The obtained precision-recall curve of the system is reported in Figure 2.
164:178	In addition we evaluated separately the performance on nouns and verbs, suspecting that nouns are more domain oriented than verbs.
165:178	The effectiveness of DDD to disambiguate domain words is con rmed by results reported in Figure 3, in which the precision recall curve is reported separately for both nouns and verbs.
166:178	The performances obtained for nouns are sensibly higher than the one obtained for verbs, con rming the claim that domain information is crucial to disambiguate domain words.
167:178	In Figure 2 we also compare the results obtained by the DDD system that make use of the DRE technique described in Section 3 with the rework since it relies on a general resource (SemCor) that does not correspond to the test data (Senseval all-words task).
168:178	0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 Precision Recall DDD new DDD old Figure 2: Performances of the system for all POS 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 Precision Recall Nouns Verbs Figure 3: Performances of the system for Nouns and Verbs sults obtained by the DDD system presented at the Senseval-2 competition described in (Magnini et al. , 2002), that is based on the same DDD methodology and exploit a DRE technique that consists basically on the simply domain frequency scores described in subsection 3.1 (we refer to this system using the expression old-DDD, in contrast to the expression new-DDD that refers to the implementation described in this paper).
169:178	Old-DDD obtained 75% precision and 35% recall on the of cial evaluation at the Senseval-2 English all words task.
170:178	At 35% of recall the new-DDD achieves a precision of 79%, improving precision by 4 points with respect to old-DDD.
171:178	At 75% precision the recall of new-DDD is 40%.
172:178	In both cases the new domain relevance estimation technique improves the performance of the DDD methodology, demonstrating the usefulness of the DRE technique proposed in this paper.
173:178	6 Conclusions and Future Works Domain Relevance Estimation, an unsupervised TC technique, has been proposed and evaluated inside the Domain Driven Disambiguation framework, showing a signi cant improvement on the overall system performances.
174:178	This technique also allows a clear probabilistic interpretation providing an operative de nition of the concept of domain relevance.
175:178	During the learning phase annotated resources are not required, allowing a low cost implementation.
176:178	The portability of the technique to other languages is allowed by the usage of synset-aligned wordnets, being domain annotation language independent.
177:178	As far as the evaluation of DRE is concerned, for the moment we have tested its usefulness in the context of a WSD task, but we are going deeper, considering a pure TC framework.
178:178	Acknowledgements We would like to thank Ido Dagan and Marcello Federico for many useful discussions and suggestions.

