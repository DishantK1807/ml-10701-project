1:219	VERBOCEAN: Mining the Web for Fine-Grained Semantic Verb Relations Timothy Chklovski and Patrick Pantel Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey, CA 90292 {timc, pantel}@isi.edu Abstract Broad-coverage repositories of semantic relations between verbs could benefit many NLP tasks.
2:219	We present a semi-automatic method for extracting fine-grained semantic relations between verbs.
3:219	We detect similarity, strength, antonymy, enablement, and temporal happens-before relations between pairs of strongly associated verbs using lexicosyntactic patterns over the Web.
4:219	On a set of 29,165 strongly associated verb pairs, our extraction algorithm yielded 65.5% accuracy.
5:219	Analysis of error types shows that on the relation strength we achieved 75% accuracy.
6:219	We provide the resource, called VERBOCEAN, for download at http://semantics.isi.edu/ocean/.
7:219	1 Introduction Many NLP tasks, such as question answering, summarization, and machine translation could benefit from broad-coverage semantic resources such as WordNet (Miller 1990) and EVCA (English Verb Classes and Alternations) (Levin 1993).
8:219	These extremely useful resources have very high precision entries but have important limitations when used in real-world NLP tasks due to their limited coverage and prescriptive nature (i.e. they do not include semantic relations that are plausible but not guaranteed).
9:219	For example, it may be valuable to know that if someone has bought an item, they may sell it at a later time.
10:219	WordNet does not include the relation X buys Y happens-before X sells Y since it is possible to sell something without having bought it (e.g. having manufactured or stolen it).
11:219	Verbs are the primary vehicle for describing events and expressing relations between entities.
12:219	Hence, verb semantics could help in many natural language processing (NLP) tasks that deal with events or relations between entities.
13:219	For tasks which require canonicalization of natural language statements or derivation of plausible inferences from such statements, a particularly valuable resource is one which (i) relates verbs to one another and (ii) provides broad coverage of the verbs in the target language.
14:219	In this paper, we present an algorithm that semiautomatically discovers fine-grained verb semantics by querying the Web using simple lexicosyntactic patterns.
15:219	The verb relations we discover are similarity, strength, antonymy, enablement, and temporal relations.
16:219	Identifying these relations over 29,165 verb pairs results in a broad-coverage resource we call VERBOCEAN.
17:219	Our approach extends previously formulated ones that use surface patterns as indicators of semantic relations between nouns (Hearst 1992; Etzioni 2003; Ravichandran and Hovy 2002).
18:219	We extend these approaches in two ways: (i) our patterns indicate verb conjugation to increase their expressiveness and specificity and (ii) we use a measure similar to mutual information to account for both the frequency of the verbs whose semantic relations are being discovered as well as for the frequency of the pattern.
19:219	2 Relevant Work In this section, we describe application domains that can benefit from a resource of verb semantics.
20:219	We then introduce some existing resources and describe previous attempts at mining semantics from text.
21:219	2.1 Applications Question answering is often approached by canonicalizing the question text and the answer text into logical forms.
22:219	This approach is taken, inter alia, by a top-performing system (Moldovan et al. 2002).
23:219	In discussing future work on the systems logical form matching component, Rus (2002 p. 143) points to incorporating entailment and causation verb relations to improve the matchers performance.
24:219	In other work, Webber et al.25:219	(2002) have argued that successful question answering depends on lexical reasoning, and that lexical reasoning in turn requires fine-grained verb semantics in addition to troponymy (is-a relations between verbs) and antonymy.
26:219	In multi-document summarization, knowing verb similarities is useful for sentence compression and for determining sentences that have the same meaning (Lin 1997).
27:219	Knowing that a particular action happens before another or is enabled by another is also useful to determine the order of the events (Barzilay et al. 2002).
28:219	For example, to order summary sentences properly, it may be useful to know that selling something can be preceded by either buying, manufacturing, or stealing it.
29:219	Furthermore, knowing that a particular verb has a meaning stronger than another (e.g. rape vs. abuse and renovate vs. upgrade) can help a system pick the most general sentence.
30:219	In lexical selection of verbs in machine translation and in work on document classification, practitioners have argued for approaches that depend on wide-coverage resources indicating verb similarity and membership of a verb in a certain class.
31:219	In work on translating verbs with many counterparts in the target language, Palmer and Wu (1995) discuss inherent limitations of approaches which do not examine a verbs class membership, and put forth an approach based on verb similarity.
32:219	In document classification, Klavans and Kan (1998) demonstrate that document type is correlated with the presence of many verbs of a certain EVCA class (Levin 1993).
33:219	In discussing future work, Klavans and Kan point to extending coverage of the manually constructed EVCA resource as a way of improving the performance of the system.
34:219	A widecoverage repository of verb relations including verbs linked by the similarity relation will provide a way to automatically extend the existing verb classes to cover more of the English lexicon.
35:219	2.2 Existing resources Some existing broad-coverage resources on verbs have focused on organizing verbs into classes or annotating their frames or thematic roles.
36:219	EVCA (English Verb Classes and Alternations) (Levin 1993) organizes verbs by similarity and participation / nonparticipation in alternation patterns.
37:219	It contains 3200 verbs classified into 191 classes.
38:219	Additional manually constructed resources include PropBank (Kingsbury et al. 2002), FrameNet (Baker et al. 1998), VerbNet (Kipper et al. 2000), and the resource on verb selectional restrictions developed by Gomez (2001).
39:219	Our approach differs from the above in its focus.
40:219	We relate verbs to each other rather than organize them into classes or identify their frames or thematic roles.
41:219	WordNet does provide relations between verbs, but at a coarser level.
42:219	We provide finer-grained relations such as strength, enablement and temporal information.
43:219	Also, in contrast with WordNet, we cover more than the prescriptive cases.
44:219	2.3 Mining semantics from text Previous web mining work has rarely addressed extracting many different semantic relations from Web-sized corpus.
45:219	Most work on extracting semantic information from large corpora has largely focused on the extraction of is-a relations between nouns.
46:219	Hearst (1992) was the first followed by recent larger-scale and more fully automated efforts (Pantel and Ravichandran 2004; Etzioni et al. 2004; Ravichandran and Hovy 2002).
47:219	Recently, Moldovan et al.48:219	(2004) present a learning algorithm to detect 35 fine-grained noun phrase relations.
49:219	Turney (2001) studied word relatedness and synonym extraction, while Lin et al.50:219	(2003) present an algorithm that queries the Web using lexical patterns for distinguishing noun synonymy and antonymy.
51:219	Our approach addresses verbs and provides for a richer and finer-grained set of semantics.
52:219	Reliability of estimating bigram counts on the web via search engines has been investigated by Keller and Lapata (2003).
53:219	Semantic networks have also been extracted from dictionaries and other machine-readable resources.
54:219	MindNet (Richardson et al. 1998) extracts a collection of triples of the type ducks have wings and duck capable-of flying.
55:219	This resource, however, does not relate verbs to each other or provide verb semantics.
56:219	3 Semantic relations among verbs In this section, we introduce and motivate the specific relations that we extract.
57:219	Whilst the natural language literature is rich in theories of semantics (Barwise and Perry 1985; Schank and Abelson 1977), large-coverage manually created semantic resources typically only organize verbs into a flat or shallow hierarchy of classes (such as those described in Section 2.2).
58:219	WordNet identifies synonymy, antonymy, troponymy, and cause.
59:219	As summarized in Figure 1, Fellbaum (1998) discusses a finer-grained analysis of entailment, while the WordNet database does not distinguish between, e.g., backward presupposition (forget :: know, where know must have happened before forget) from proper temporal inclusion (walk :: step).
60:219	In formulating our set of relations, we have relied on the finer-grained analysis, explicitly breaking out the temporal precedence between entities.
61:219	In selecting the relations to identify, we aimed at both covering the relations described in WordNet and covering the relations present in our collection Figure 1.
62:219	Fellbaums (1998) entailment hierarchy.
63:219	+Temporal Inclusion Entailment -Temporal Inclusion +Troponymy (coextensiveness) march-walk -Troponymy (proper inclusion) walk-step Backward Presupposition forget-know Cause show-see of strongly associated verb pairs.
64:219	We relied on the strongly associated verb pairs, described in Section 4.4, for computational efficiency.
65:219	The relations we identify were experimentally found to cover 99 out of 100 randomly selected verb pairs.
66:219	Our algorithm identifies six semantic relations between verbs.
67:219	These are summarized in Table 1 along with their closest corresponding WordNet category and the symmetry of the relation (whether V 1 rel V 2 is equivalent to V 2 rel V 1 ).
68:219	Similarity.
69:219	As Fellbaum (1998) and the tradition of organizing verbs into similarity classes indicate, verbs do not neatly fit into a unified is-a (troponymy) hierarchy.
70:219	Rather, verbs are often similar or related.
71:219	Similarity between action verbs, for example, can arise when they differ in connotations about manner or degree of action.
72:219	Examples extracted by our system include maximize :: enhance, produce :: create, reduce :: restrict.
73:219	Strength.
74:219	When two verbs are similar, one may denote a more intense, thorough, comprehensive or absolute action.
75:219	In the case of change-of-state verbs, one may denote a more complete change.
76:219	We identify this as the strength relation.
77:219	Sample verb pairs extracted by our system, in the order weak to strong, are: taint :: poison, permit :: authorize, surprise :: startle, startle :: shock.
78:219	Some instances of strength sometimes map to WordNets troponymy relation.
79:219	Strength, a subclass of similarity, has not been identified in broad-coverage networks of verbs, but may be of particular use in natural language generation and summarization applications.
80:219	Antonymy.
81:219	Also known as semantic opposition, antonymy between verbs has several distinct subtypes.
82:219	As discussed by Fellbaum (1998), it can arise from switching thematic roles associated with the verb (as in buy :: sell, lend :: borrow).
83:219	There is also antonymy between stative verbs (live :: die, differ :: equal) and antonymy between sibling verbs which share a parent (walk :: run) or an entailed verb (fail :: succeed both entail try).
84:219	Antonymy also systematically interacts with the happens-before relation in the case of restitutive opposition (Cruse 1986).
85:219	This subtype is exemplified by damage :: repair, wrap :: unwrap.
86:219	In terms of the relations we recognize, it can be stated that restitutive-opposition(V 1, V 2 ) = happensbefore(V 1, V 2 ), and antonym(V 1, V 2 ).
87:219	Examples of antonymy extracted by our system include: assemble :: dismantle; ban :: allow; regard :: condemn, roast :: fry.
88:219	Enablement.
89:219	This relation holds between two verbs V 1 and V 2 when the pair can be glossed as V 1 is accomplished by V 2.
90:219	Enablement is classified as a type of causal relation by Barker and Szpakowicz (1995).
91:219	Examples of enablement extracted by our system include: assess :: review and accomplish :: complete.
92:219	Happens-before.
93:219	This relation indicates that the two verbs refer to two temporally disjoint intervals or instances.
94:219	WordNets cause relation, between a causative and a resultative verb (as in buy :: own), would be tagged as instances of happens-before by our system.
95:219	Examples of the happens-before relation identified by our system include marry :: divorce, detain :: prosecute, enroll :: graduate, schedule :: reschedule, tie :: untie.
96:219	4 Approach We discover the semantic relations described above by querying the Web with Google for lexico-syntactic patterns indicative of each relation.
97:219	Our approach has two stages.
98:219	First, we identify pairs of highly associated verbs co-occurring on the Web with sufficient frequency using previous work by Lin and Pantel (2001), as described in Section 4.4.
99:219	Next, for each verb pair, we tested lexico-syntactic patterns, calculating a score for each possible semantic relation as described in Section 4.2.
100:219	Finally, as described in Section 4.3, we compare the strengths of the individual semantic relations and, preferring the most specific and then strongest relations, output a consistent set as the final output.
101:219	As a guide to consistency, we use a simple theory of semantics indicating which semantic relations are subtypes of other ones, and which are compatible and which are mutually exclusive.
102:219	4.1 Lexico-syntactic patterns The lexico-syntactic patterns were manually selected by examining pairs of verbs in known semantic relations.
103:219	They were refined to decrease capturing wrong parts of speech or incorrect semantic relations.
104:219	We used 50 verb pairs and the overall process took about 25 hours.
105:219	We use a total of 35 patterns, which are listed in Table 2 along with the estimated frequency of hits.
106:219	Table 1.
107:219	Semantic relations identified in VERBOCEAN.
108:219	Siblings in the WordNet column refers to terms with the same troponymic parent, e.g. swim and fly.
109:219	SEMANTIC RELATION EXAMPLE Alignment with WordNet Symmetric similarity transform :: integrate synonyms or siblings Y strength wound :: kill synonyms or siblings N antonymy open :: close antonymy Y enablement fight :: win cause N happensbefore buy :: sell; marry :: divorce cause entailment, no temporal inclusion N Note that our patterns specify the tense of the verbs they accept.
110:219	When instantiating these patterns, we conjugate as needed.
111:219	For example, both Xed and Yed instantiates on sing and dance as both sung and danced.
112:219	4.2 Testing for a semantic relation In this section, we describe how the presence of a semantic relation is detected.
113:219	We test the relations with patterns exemplified in Table 2.
114:219	We adopt an approach inspired by mutual information to measure the strength of association, denoted S p (V 1, V 2 ), between three entities: a verb pair V 1 and V 2 and a lexico-syntactic pattern p: )()()( ),,( ),( 21 21 21 VPVPpP VpVP VVS p  = The probabilities in the denominator are difficult to calculate directly from search engine results.
115:219	For a given lexico-syntactic pattern, we need to estimate the frequency of the pattern instantiated with appropriately conjugated verbs.
116:219	For verbs, we need to estimate the frequency of the verbs, but avoid counting other parts-of-speech (e.g. chair as a noun or painted as an adjective).
117:219	Another issue is that some relations are symmetric (similarity and antonymy), while others are not (strength, enablement, happens-before).
118:219	For symmetric relations only, the verbs can fill the lexico-syntactic pattern in either order.
119:219	To address these issues, we estimate S p (V 1,V 2 ) using: N CVtohits N CVtohits N phits N VpVhits VVS vvest P      )"(")"(")( ),,( ),( 21 21 21 for asymmetric relations and N CVtohits N CVtohits N phits N VpVhits N VpVhits VVS vvest P     +  )"(")"(")(*2 ),,(),,( ),( 21 1221 21 for symmetric relations.
120:219	Here, hits(S) denotes the number of documents containing the string S, as returned by Google.
121:219	N is the number of words indexed by the search engine (N  7.2  10 11 ), C v is a correction factor to obtain the frequency of the verb V in all tenses from the frequency of the pattern to V.
122:219	Based on several verbs, we have estimated C v = 8.5.
123:219	Because pattern counts, when instantiated with verbs, could not be estimated directly, we have computed the frequencies of the patterns in a part-of-speech tagged 500M word corpus and used it to estimate the expected number of hits hits est (p) for each pattern.
124:219	We estimated the N with a similar method.
125:219	We say that the semantic relation S p indicated by lexico-syntactic pattern p is present between V 1 and V 2 if S p (V 1,V 2 ) > C 1 As a result of tuning the system on a tuning set of 50 verb pairs, C 1 = 8.5.
126:219	Additional test for asymmetric relations.
127:219	For the asymmetric relations, we require not only that ),( 21 VVS P exceed a certain threshold, but that there be strong asymmetry of the relation: 2 12 21 12 21 ),,( ),,( ),( ),( C VpVhits VpVhits VVS VVS p p >= From the tuning set, C 2 = 5.
128:219	4.3 Pruning identified semantic relations Given a pair of semantic relations from the set we identify, one of three cases can arise: (i) one Table 2.
129:219	Semantic relations and the 35 surface patterns used to identify them.
130:219	Total number of patterns for that relation is shown in parentheses.
131:219	In patterns, * matches any single word.
132:219	Punctuation does not count as words by the search engine used (Google).
133:219	SEMANTIC RELATION Surface Patterns Hits est for patterns narrow similarity (2) * X ie Y Xed ie Yed 219,480 broad similarity (2) * Xed and Yed to X and Y 154,518,326 strength (8) X even Y Xed even Yed X and even Y Xed and even Yed Y or at least X Yed or at least Xed not only Xed but Yed not just Xed but Yed 1,016,905 enablement (4) Xed * by Ying the Xed * by Ying or to X * by Ying the to X * by Ying or 2,348,392 antonymy (7) either X or Y either Xs or Ys either Xed or Yed either Xing or Ying whether to X or Y Xed * but Yed to X * but Y 18,040,916 happens-before (12) to X and then Y to X * and then Y Xed and then Yed Xed * and then Yed to X and later Y Xed and later Yed to X and subsequently Y Xed and subsequently Yed to X and eventually Y Xed and eventually Yed 8,288,871 * narrowand broadsimilarity overlap in their coverage and are treated as a single category, similarity, when postprocessed.
134:219	Narrow similarity tests for rare patterns and hits est for it had to be approximated rather than estimated from the smaller corpus.
135:219	relation is more specific (strength is more specific than similarity, enablement is more specific than happens-before), (ii) the relations are compatible (antonymy and happens-before), where presence of one does not imply or rule out presence of the other, and (iii) the relations are incompatible (similarity and antonymy).
136:219	It is not uncommon for our algorithm to identify presence of several relations, with different strengths.
137:219	To produce the most likely output, we use semantics of compatibility of the relations to output the most likely one(s).
138:219	The rules are as follows: If the frequency was too low (less than 10 on the pattern X * Y OR Y * X OR X * * Y OR Y * * X), output that the statements are unrelated and stop.
139:219	If happens-before is detected, output presence of happens-before (additional relation may still be output, if detected).
140:219	If happens-before is not detected, ignore detection of enablement (because enablement is more specific than happens-before, but is sometimes falsely detected in the absence of happens-before).
141:219	If strength is detected, score of similarity is ignored (because strength is more specific than similarity).
142:219	Of the relations strength, similarity, opposition and enablement which were detected (and not ignored), output the one with highest S p . If nothing has been output to this point, output unrelated.
143:219	4.4 Extracting highly associated verb pairs To exhaustively test the more than 64 million unordered verb pairs for WordNets more than 11,000 verbs would be computationally intractable.
144:219	Instead, we use a set of highly associated verb pairs output by a paraphrasing algorithm called DIRT (Lin and Pantel 2001).
145:219	Since we are able to test up to 4000 verb pairs per day on a single machine (we issue at most 40 queries per test and each query takes approximately 0.5 seconds), we are able to test several dozen associated verbs for each verb in WordNet in a matter of weeks.
146:219	Lin and Pantel (2001) describe an algorithm called DIRT (Discovery of Inference Rules from Text) that automatically learns paraphrase expressions from text.
147:219	It is a generalization of previous algorithms that use the distributional hypothesis (Harris 1985) for finding similar words.
148:219	Instead of applying the hypothesis to words, Lin and Pantel applied it to paths in dependency trees.
149:219	Essentially, if two paths tend to link the same sets of words, they hypothesized that the meanings of the corresponding paths are similar.
150:219	It is from paths of the form subject-verb-object that we extract our set of associated verb pairs.
151:219	Hence, this paper is concerned only with relations between transitive verbs.
152:219	A path, extracted from a parse tree, is an expression that represents a binary relation between two nouns.
153:219	A set of paraphrases was generated for each pair of associated paths.
154:219	For example, using a 1.5GB newspaper corpus, here are the 20 most associated paths to X solves Y generated by DIRT: Y is solved by X, X resolves Y, X finds a solution to Y, X tries to solve Y, X deals with Y, Y is resolved by X, X addresses Y, X seeks a solution to Y, X does something about Y, X solution to Y, Y is resolved in X, Y is solved through X, X rectifies Y, X copes with Y, X overcomes Y, X eases Y, X tackles Y, X alleviates Y, X corrects Y, X is a solution to Y, X makes Y worse, X irons out Y This list of associated paths looks tantalizingly close to the kind of axioms that would prove useful in an inference system.
155:219	However, DIRT only outputs pairs of paths that have some semantic relation.
156:219	We used these as our set to extract finergrained relations.
157:219	5 Experimental results In this section, we empirically evaluate the accuracy of VERBOCEAN 1 . 5.1 Experimental setup We studied 29,165 pairs of verbs.
158:219	Applying DIRT to a 1.5GB newspaper corpus 2, we extracted 4000 paths that consisted of single verbs in the relation subject-verb-object (i.e. paths of the form X verb Y) whose verbs occurred in at least 150 documents on the Web.
159:219	For example, from the 20 most associated paths to X solves Y shown in Section 4.4, the following verb pairs were extracted: solves :: resolves solves :: addresses solves :: rectifies solves :: overcomes solves :: eases solves :: tackles solves :: corrects 5.2 Accuracy We classified each verb pair according to the semantic relations described in Section 2.
160:219	If the system does not identify any semantic relation for a verb pair, then the system tags the pair as having 1 VERBOCEAN is available for download at http://semantics.isi.edu/ocean/.
161:219	2 The 1.5GB corpus consists of San Jose Mercury, Wall Street Journal and AP Newswire articles from the TREC-9 collection.
162:219	no relation.
163:219	To evaluate the accuracy of the system, we randomly sampled 100 of these verb pairs, and presented the classifications to two human judges.
164:219	The adjudicators were asked to judge whether or not the system classification was acceptable (i.e. whether or not the relations output by the system were correct).
165:219	Since the semantic relations are not disjoint (e.g. mop is both stronger than and similar to sweep), multiple relations may be appropriately acceptable for a given verb pair.
166:219	The judges were also asked to identify their preferred semantic relations (i.e. those relations which seem most plausible).
167:219	Table 3 shows five randomly selected pairs along with the judges responses.
168:219	The Appendix shows sample relationships discovered by the system.
169:219	Table 4 shows the accuracy of the system.
170:219	The baseline system consists of labeling each pair with the most common semantic relation, similarity, which occurs 33 times.
171:219	The Tags Correct column represents the percentage of verb pairs whose system output relations were deemed correct.
172:219	The Preferred Tags Correct column gives the percentage of verb pairs whose system output relations matched exactly the humans preferred relations.
173:219	The Kappa statistic (Siegel and Castellan 1988) for the task of judging system tags as correct and incorrect is  = 0.78 whereas the task of identifying the preferred semantic relation has  = 0.72.
174:219	For the latter task, the two judges agreed on 73 of the 100 semantic relations.
175:219	73% gives an idea of an upper bound for humans on this task.
176:219	On these 73 relations, the system achieved a higher accuracy of 70.0%.
177:219	The system is allowed to output the happens-before relation in combination with other relations.
178:219	On the 17 happens-before relations output by the system, 67.6% were judged correct.
179:219	Ignoring the happens-before relations, we achieved a Tags Correct precision of 68%.
180:219	Table 5 shows the accuracy of the system on each of the relations.
181:219	The stronger-than relation is a subset of the similarity relation.
182:219	Considering a coarser extraction where stronger-than relations are merged with similarity, the task of judging system tags and the task of identifying the preferred semantic relation both jump to 68.2% accuracy.
183:219	Also, the overall accuracy of the system climbs to 68.5%.
184:219	As described in Section 2, WordNet contains verb semantic relations.
185:219	A significant percentage of our discovered relations are not covered by WordNets coarser classifications.
186:219	Of the 40 verb pairs whose system relation was tagged as correct by both judges in our accuracy experiments and whose tag was not no relation, only 22.5% of them existed in a WordNet relation.
187:219	5.3 Discussion The experience of extracting these semantic relations has clarified certain important challenges.
188:219	While relying on a search engine allows us to query a corpus of nearly a trillion words, some issues arise: (i) the number of instances has to be approximated by the number of hits (documents); (ii) the number of hits for the same query may fluctuate over time; and (iii) some needed counts are not directly available.
189:219	We addressed the latter issue by approximating these counts using a smaller corpus.
190:219	Table 3.
191:219	Five randomly selected pairs along with the system tag (in bold) and the judges responses.
192:219	CORRECT PREFERRED SEMANTIC RELATION PAIRS WITH SYSTEM TAG (IN BOLD) JUDGE 1 JUDGE 2 JUDGE 1 JUDGE 2 X absolve Y is similar to X vindicate Y Yes Yes is similar to is similar to X bottom Y has no relation with X abate Y Yes Yes has no relation with has no relation with X outrage Y happens-after / is stronger than X shock Y Yes Yes happens-before / is stronger than happens-before/ is stronger than X pool Y has no relation with X increase Y Yes No has no relation with can result in X insure Y is similar to X expedite Y No No has no relation with has no relation with Table 4.
193:219	Accuracy of system-discovered relations.
194:219	ACCURACY Tags Correct Preferred Tags Correct Baseline Correct Judge 1 66% 54% 24% Judge 2 65% 52% 20% Average 65.5% 53% 22% Table 5.
195:219	Accuracy of each semantic relation.
196:219	SEMANTIC RELATION SYSTEM TAGS Tags Correct Preferred Tags Correct similarity 41 63.4% 40.2% strength 14 75.0% 75.0% antonymy 8 50.0% 43.8% enablement 2 100% 100% no relation 35 72.9% 72.9% happens before 17 67.6% 55.9% We do not detect entailment with lexicosyntactic patterns.
197:219	In fact, we propose that whether the entailment relation holds between V 1 and V 2 depends on the absence of another verb V 1 ' in the same relationship with V 2 . For example, given the relation marry happens-before divorce, we can conclude that divorce entails marry.
198:219	But, given the relation buy happens-before sell, we cannot conclude entailment since manufacture can also happen before sell.
199:219	This also applies to the enablement and strength relations.
200:219	Corpus-based methods, including ours, hold the promise of wide coverage but are weak on discriminating senses.
201:219	While we hope that applications will benefit from this resource as is, an interesting next step would be to augment it with sense information.
202:219	6 Future work There are several ways to improve the accuracy of the current algorithm and to detect relations between low frequency verb pairs.
203:219	One avenue would be to automatically learn or manually craft more patterns and to extend the pattern vocabulary (when developing the system, we have noticed that different registers and verb types require different patterns).
204:219	Another possibility would be to use more relaxed patterns when the part of speech confusion is not likely (e.g. eat is a common verb which does not have a noun sense, and patterns need not protect against noun senses when testing such verbs).
205:219	Our approach can potentially be extended to multiword paths.
206:219	DIRT actually provides two orders of magnitude more relations than the 29,165 single verb relations (subject-verb-object) we extracted.
207:219	On the same 1GB corpus described in Section 5.1, DIRT extracted over 200K paths and 6M unique paraphrases.
208:219	These provide an opportunity to create a much larger corpus of semantic relations, or to construct smaller, in-depth resources for selected subdomains.
209:219	For example, we could extract that take a trip to is similar to travel to, and that board a plane happens before deplane.
210:219	If the entire database is viewed as a graph, we currently leverage and enforce only local consistency.
211:219	It would be useful to enforce global consistency, e.g. V 1 stronger-than V 2, and V 2 strongerthan V 3 indicates that V 1 stronger-than V 3, which may be leveraged to identify additional relations or inconsistent relations (e.g. V 3 stronger-than V 1 ).
212:219	Finally, as discussed in Section 5.3, entailment relations may be derivable by processing the complete graph of the identified semantic relation.
213:219	7 Conclusions We have demonstrated that certain fine-grained semantic relations between verbs are present on the Web, and are extractable with a simple patternbased approach.
214:219	In addition to discovering relations identified in WordNet, such as opposition and enablement, we obtain strong results on strength relations (for which no wide-coverage resource is available).
215:219	On a set of 29,165 associated verb pairs, experimental results show an accuracy of 65.5% in assigning similarity, strength, antonymy, enablement, and happens-before.
216:219	Further work may refine extraction methods and further process the mined semantics to derive other relations such as entailment.
217:219	We hope to open the way to inferring implied, but not stated assertions and to benefit applications such as question answering, information retrieval, and summarization.
218:219	Acknowledgments The authors wish to thank the reviewers for their helpful comments and Google Inc. for supporting high volume querying of their index.
219:219	This research was partly supported by NSF grant #EIA-0205111.

