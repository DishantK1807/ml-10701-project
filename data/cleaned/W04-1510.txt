Extensible Dependency Grammar: A New Methodology Ralph Debusmann Programming Systems Lab Saarland University Postfach 15 11 50 66041 Saarbr¨ucken Germany rade@ps.uni-sb.de Denys Duchier ´Equipe Calligramme LORIA – UMR 7503 Campus Scientifique, B.
P. 239 54506 Vandœuvre l`es Nancy CEDEX France duchier@loria.fr Geert-Jan M.
Kruijff Computational Linguistics Saarland University Postfach 15 11 50 66041 Saarbr¨ucken Germany gj@coli.uni-sb.de Abstract This paper introduces the new grammar formalism of Extensible Dependency Grammar (XDG), and emphasizes the benefits of its methodology of explaining complex phenomena by interaction of simple principles on multiple dimensions of linguistic description.
This has the potential to increase modularity with respect to linguistic description and grammar engineering, and to facilitate concurrent processing and the treatment of ambiguity.
1 Introduction
We introduce the new grammar formalism of Extensible Dependency Grammar (XDG).
In XDG, complex phenomena arise out of the interaction of simple principles on multiple dimensions of linguistic description.
In this paper, we point out how this novel methodology positions XDG in between multi-stratal approaches like LFG (Bresnan and Kaplan, 1982) and MTT (Mel’ˇcuk, 1988), see also (Kahane, 2002), and mono-stratal ones like HPSG (Pollard and Sag, 1994), attempting to combine their benefits and avoid their problems.
It is the division of linguistic analyses into different dimensions which makes XDG multi-stratal.
On the other, XDG is mono-stratal in that its principles interact to constrain all dimensions simultaneously.
XDG combines the benefits of these two positions, and attempts to circumvent their problems.
From multi-stratal approaches, XDG adopts a high degree of modularity, both with respect to linguistic description as well as for grammar engineering.
This also facilitates the statement of cross-linguistic generalizations.
XDG avoids the problem of placing too high a burden on the interfaces, and allows interactions between all and not only adjacent dimensions.
From mono-stratal approaches, XDG adopts a high degree of integration, facilitating concurrent processing and the treatment of ambiguity.
At the same time, XDG does not lose its modularity.
XDG is a descendant of Topological Dependency Grammar (TDG) (Duchier and Debusmann, 2001), pushing the underlying methodology further by generalizing it in two aspects: number of dimensions: two in TDG (ID and LP), arbitrary many in XDG set of principles: fixed in TDG, extensible principle library in XDG The structure of this paper is as follows: In x2, we introduce XDG and the XDG solver used for parsing and generation.
In x3, we introduce a number of XDG principles informally, before making use of them in an idealized example grammar in x4.
In x5 we argue why XDG has the potential to be an improvement over multi-stratal and mono-stratal approaches, before we conclude in x6.
2 Extensible
Dependency Grammar In this section, we introduce XDG formally and mention briefly the constraint-based XDG solver for parsing and generation.
2.1 Formalization
Formally, an XDG grammar is built up of dimensions, a lexicon and principles, and characterizes a set of well-formed analyses.
A dimension is a tuple D = (Lab;Fea;Val;Pri) of a set Lab of edge labels, a set Fea of features, a set Val of feature values, and a set of one-dimensional principles Pri.
A lexicon for the dimension D is a set Lex Fea ! Val of total feature assignments called lexical entries.
An analysis on dimension D is a triple (V;E;F) of a set V of nodes, a set E V V Lab of directed labeled edges, and an assignment F : V ! (Fea ! Val) of lexical entries to nodes.
V and E form a graph.
We write AnaD for the set of all possible analyses on dimension D.
The principles characterize subsets of AnaD.
We assume that the elements of Pri are finite representations of such subsets.
An XDG grammar ((Labi;Feai;Vali;Prii)ni=1;Pri; Lex) consists of n dimensions, multi-dimensional principles Pri, and a lexicon Lex.
An XDG analysis (V;Ei;Fi)ni=1 is an element of Ana = Ana1 Anan where all dimensions share the same set of nodes V.
We call a dimension of a grammar grammar dimension.
Multi-dimensional principles specify subsets of Ana, i.e. of tuples of analyses for the individual dimensions.
The lexicon Lex Lex1 Lexn constrains all dimensions at once, thereby synchronizing them.
An XDG analysis is licensed by Lex iff (F1(v); : : : ;Fn(v)) 2 Lex for every node v 2V. In order to compute analyses for a given input, we employ a set of input constraints (Inp), which again specify a subset of Ana.
XDG solving then amounts to finding elements of Ana that are licensed by Lex, and consistent with Inp and Pri.
The input constraints determine whether XDG solving is to be used for parsing or generation.
For parsing, they specify a sequence of words, and for generation, a multiset of semantic literals.
2.2 Solver
XDG solving has a natural reading as a constraint satisfaction problem (CSP) on finite sets of integers, where well-formed analyses correspond to the solutions of the CSP (Duchier, 2003).
We have implemented an XDG solver using the Mozart-Oz programming system.
XDG solving operates on all dimensions concurrently.
This means that the solver can infer information about one dimension from information on another, if there is either a multi-dimensional principle linking the two dimensions, or by the synchronization induced by the lexical entries.
For instance, not only can syntactic information trigger inferences in syntax, but also vice versa.
Because XDG allows us to write grammars with completely free word order, XDG solving is an NP-complete problem (Koller and Striegnitz, 2002).
This means that the worst-case complexity of the solver is exponential.
The average-case complexity of many smaller-scale grammars that we have experimented with seems polynomial, but it remains to be seen whether we can scale this up to largescale grammars.
3 Principles
The well-formedness conditions of XDG analyses are stipulated by principles.
Principles are parametrizable, e.g. by the dimensions on which they are applied, or by lexical features.
They can be lexicalized or non-lexicalized, and can be onedimensional or multi-dimensional.
Principles are taken from an extensible principle library, and we introduce some of the most important principles in the following.
3.1 Tree
principle tree(i) The analysis on dimension i must be a tree.
The tree principle is non-lexicalized and parametrized by the dimension i.
3.2 Dag
principle dag(i) The analysis on dimension i must be a directed acyclic graph.
The dag principle is non-lexicalized and parametrized by the dimension i.
3.3 Valency
principle valency(i;ini;outi) All nodes on dimension i must satisfy their in and out specifications.
The valency principle is lexicalized and serves to lexically describe dependency graphs.
It is parametrized by the dimension i, the in specification ini and the out specification outi.
For each node, ini stipulates the licensed incoming edges, and outi the licensed outgoing edges.
In the example grammar lexicon part in Figure 1 below, the in specification is inID and outID is the out specification on the ID dimension.
For the common noun Roman, the in specification licenses zero or one incoming edges labeled subj, and zero or one incoming edges labeled obj (fsubj?obj?g), i.e. it can be either a subject or an object.
The out specification requires precisely one outgoing edge labeled det (fdet!g), i.e. it requires a determiner.
3.4 Government
principle government(i;casesi;governi) All edges in dimension i must satisfy the government specification of the mother.
The government principle is lexicalized.
Its purpose is to constrain the case feature of a dependent.1 It is parametrized by the dimension i, the cases specification casesi and the government specification govern.
cases assigns to each word a set of possible cases, and govern a mapping from labels to sets of cases.
In Figure 1, the cases specification for the determiner den is faccg (i.e.
it can only be accusative).
By its government specification, the finite verb versucht requires its subject to exhibit nominative case (subj 7!fnomg).
3.5 Agreement
principle agreement(i;casesi;agreei) All edges in dimension i must satisfy the agreement specification of the mother.
1We restrict ourselves to the case feature only for simplicity.
In a fully-fledged grammar, the government principle would be used to constrain also other morphological aspects like number, person and gender.
The agreement principle is lexicalized.
Its purpose is to enforce the case agreement of a daughter.2 It is parametrized by dimension i, the lexical cases specification casesi, assigning to each word a set of possible cases, and the agreement specification agreei, assigning to each word a set of labels.
As an example, in Figure 1, the agreement specification for the common noun Roman is fdetg, i.e. the case of the common noun must agree with its determiner.
3.6 Order
principle.
order(i;oni; i) On dimension i, 1) each node must satisfy its node labels specification, 2) the order of the daughters of each node must be compatible with i, and 3) the node itself must be ordered correctly with respect to its daughters (using its node label).
The order principle is lexicalized.
It is parametrized by the dimension i, the node labels specification oni mapping each node to set of labels from Labi, and the total order i on Labi.
Assuming the node labels specification given in Figure 2, and the total order in (5), the tree in (11) satisfies the order principle.3 For instance for the node versucht: 1) The node label of versucht is lbf, satisfying the node labels specification.
2) The order of the daughters Roman (under the edge labeled vf), Peter (mf) and lesen (rbf) is compatible with the total order prescribing vf mf rbf.
3) The node versucht itself is ordered correctly with respect to its daughters (the total order prescribes vf lbf mf).
3.7 Projectivity
principle projectivity(i) The analysis on dimension i must be projective.
The projectivity principle is non-lexicalized.
Its purpose is to exclude non-projective analyses.4 It is parametrized by dimension i.
3.8 Climbing
principle climbing(i; j) The graph on dimension i must be flatter than the graph on dimension j.
The climbing principle is non-lexicalized and two-dimensional.
It is parametrized by the two dimensions i and j.
For instance, the tree in (11) is flatter than the corresponding tree in (10).
This concept was introduced as lifting in (Kahane et al., 1998).
2Again, we restrict ourselves to case for simplicity.
3The node labels are defined in (2) below.
4The projectivity principle of course only makes sense in combination with the order principle.
3.9 Linking
principle linking(i; j;linki; j) All edges on dimension i must satisfy the linking specification of the mother.
The linking principle is lexicalized and twodimensional.
It is parametrized by the two dimensions i and j, and by the linking specification linki; j, mapping labels from Labi to sets of labels from Lab j.
Its purpose is to specify how dependents on dimension i are realized by (or linked to) dependents on dimension j.
In the lexicon part in Figure 3, the linking specification for the transitive verb lesen requires that its agent on the PA dimension must be realized by a subject (ag 7!fsubjg), and the patient by an object (pat 7!fobjg).
The linking principle is oriented.
Symmetric linking could be gained simply by using the linking principle twice (in both directions).
4 Example
grammar In this section, we elucidate XDG with an example grammar fragment for German.
With it, we demonstrate three aspects of the methodology of XDG: How complex phenomena such as topicalization and control arise by the interaction of simple principles on different dimensions of linguistic description.
How the high degree of integration helps to reduce ambiguity.
How the high degree of modularity facilitates the statement of cross-linguistic generalizations.
Note that this grammar fragment is an idealized example, and does not make any claims about XDG as a grammar theory.
Its purpose is solely to substantiate our points about XDG as a framework.
Moreover, the grammar is fully lexicalized for simplicity.
However, XDG of course allows the grammar writer to formulate lexical abstractions using inheritance (like in HPSG) or crossings (Candito, 1996).
4.1 Dimensions
The grammar fragment make use of two dimensions: Immediate Dominance (ID) and Linear Precedence (LP).
The models on the ID dimension are unordered, syntactic dependency trees whose edge labels correspond to syntactic functions like subject and object.
On the LP dimension, the models are ordered, projective topological dependency trees whose edge labels are topological fields like Vorfeld and Mittelfeld.
4.2 Labels
The set LabID of labels on the ID dimension is: LabID = fdet;subj;obj;vinf;partg (1) These correspond resp.
to determiner, subject, object, infinitive verbal complement, and particle.
The set LabLP of labels on the LP dimension is: LabLP = fdetf;nounf;vf;lbf;mf;partf;rbfg (2) Corresponding resp.
to determiner field, noun field, Vorfeld, left bracket field, Mittelfeld, particle field, and right bracket field.
4.3 Principles
On the ID dimension, we make use of the following one-dimensional principles: tree(ID) valency(ID;inID;outID) government(ID;casesID;governID) agreement(ID;casesID;agreeID) (3) The LP dimension uses the following principles: tree(LP) valency(LP;inLP;outLP) order(LP;onLP; LP) projectivity(LP) (4) where the total order LP is defined as: detf nounf vf lbf mf partf rbf (5) We make use of the following multi-dimensional principles: climbing(LP; ID) linking(LP; ID) (6) 4.4 Lexicon We split the lexicon into two parts.
The ID and LP parts are displayed resp.
in Figure 15 and Figure 2.
The LP part includes also the linking specification for the LP,ID-application of the linking principle.6 4.5 Government and agreement Our first example is the following sentence: Peter versucht einen Roman zu lesen: Peter tries aacc novel to read: Peter tries to read a novel.
(7) We display the ID analysis of the sentence below:.
Peter versucht einen Roman zu lesen subj vinf partobj det (8) 5Here, stands for “don’t care”, this means e.g. for the verb versucht that it has unspecified case.
6We do not make use of the linking specification for the German grammar fragment (the mappings are all empty), but we will do so as we switch to Dutch in x4.8 below.
Here, Peter is the subject of versucht.
lesen is the infinitival verbal complement of versucht, zu the particle of lesen, and Roman the object of lesen.
Finally, einen is the determiner of Roman.
Under our example grammar, the sentence is unambiguous, i.e. the given ID tree is the only possible one.
Other ID trees are ruled out by the interaction of the principles on the ID dimension.
For instance, the government and agreement principles conspire to rule out the reading where Roman is the subject of versucht (and Peter the object).
How? By the agreement principle, Roman must be accusative, since it agrees with its accusative determiner einen.
By the government principle, the subject of versucht must be nominative, and the object of lesen accusative.
Thus Roman, by virtue of being accusative, cannot become the subject of versucht.
The only other option for it is to become the object of lesen.
Consequently, Peter, which is unspecified for case, must become the subject of versuchen (versuchen must have a subject by the valency principle).
4.6 Topicalization
Our second example is a case of topicalization, where the object has moved into the Vorfeld, to the left of the finite verb: Einen Roman versucht Peter zu lesen: (9) Here is the ID tree and the LP tree analysis:.
Einen Roman versucht Peter zu lesen subj vinf partobj det (10).
Einen Roman versucht Peter zu lesen detf nounf lbf nounf partf rbf vf detf mf rbf partf (11) The ID tree analysis is the same as before, except that the words are shown in different positions.
In the LP tree, Roman is in the Vorfeld of versucht, Peter in the Mittelfeld, and lesen in the right bracket field.
versucht itself is (by its node label) in the left bracket field.
Moreover, Einen is in the determiner field of Roman, and zu in the particle field of lesen.
Again, this is an example demonstrating how complex phenomena (here: topicalization) are explained by the interaction of simple principles.
Topicalization does not have to explicitly taken care of, it is rather a consequence of the interacting principles.
Here, the valency, projectivity and climbing inID outID casesID governID agreeID den fdet?g fg faccg fg fg Roman fsubj?obj?g fdet!g fnom;dat;accg fg fdetg Peter fsubj?obj?g fg fnom;dat;accg fg fg versucht fg fsubj!vinf!g fsubj 7!fnomgg fg zu fpart?g fg fg fg lesen fvinf?g fobj!g fobj 7!faccgg fg Figure 1: Lexicon for the example grammar fragment, ID part inLP outLP onLP linkLP;ID den fdetf?g fg fdetfg fg Roman fvf?mf?g fdetf!g fnounfg fg Peter fvf?mf?g fg fnounfg fg versucht fg fvf?mf ;rbf?g flbfg fg zu fpartf?g fg fpartfg fg lesen frbf?g fg frbfg fg Figure 2: Lexicon for the example grammar fragment, LP part principles conspire to bring about the “climbing up” of the NP Einen Roman from being the daughter of lesen in the ID tree to being the daughter of versucht in the LP tree: The out specification of lesen does not license any outgoing edge.
Hence, Roman must become the daughter of another node.
The only possibility is versucht.
The determiner Einen must then also “climb up” because Roman is its only possible mother.
The result is an LP tree which is flatter with respect to the ID tree.
The LP tree is also projective.
If it were not be flatter, then it would be non-projective, and ruled out by the projectivity principle.
4.7 Negative
example Our third example is a negative example, i.e. an ungrammatical sentence: Peter einen Roman versucht zu lesen: (12) This example is perfectly legal on the unordered ID dimension, but has no model on the LP dimension.
Why? Because by its LP out specification, the finite verb versucht allows only one dependent to the left of it (in its Vorfeld), and here we have two.
The interesting aspect of this example is that although we can find a well-formed ID tree for it, this ID tree is never actually generated.
The interactions of the principles, viz.
here of the principles on the LP dimension, rule out the sentence before any full ID analysis has been found.
4.8 From
German to Dutch For the fourth example, we switch from German to Dutch.
We will show how to use the lexicon to concisely capture an important cross-linguistic generalization.
We keep the same grammar as before, but with two changes, arising from the lesser degree of inflection and the higher reliance on word order in Dutch: The determiner een is not case-marked but can be either nominative, dative or accusative: casesID = fnom;dat;accg.
The Vorfeld of the finite verb probeert cannot be occupied by an object (but only by an object): linkLP;ID = fvf 7!fsubjgg.7 Now to the example, a Dutch translation of (7): Peter probeert een roman te lezen: Peter tries a novel to read: Peter tries to read a novel.
(13) We get only one analysis on the ID dimension, where Peter is the subject and roman the object.
An analysis where Peter is the object of lezen and roman the subject of probeert is impossible, as in the German example.
The difference is, however, how this analysis is excluded.
In German, the accusative inflection of the determiner einen triggered the agreement and the government principle to rule it out.
In Dutch, the determiner is not inflected.
The unwanted analysis is excluded on the grounds of word order instead: By the linking principle, the Vorfeld of probeert must be filled by a subject, and not by an object.
That means that Peter in the Vorfeld (to the left of probeert) must be a subject, and consequently, the only other choice for roman is that it becomes the object of lezen.
4.9 Predicate-Argument Structure Going towards semantics, we extend the grammar with another dimension, Predicate-Argument Structure (PA), where the models are not trees but directed acyclic graphs (dags), to model re-entrancies 7Of course, this is an idealized assumption.
In fact, given the right stress, the Dutch Vorfeld can be filled by objects.
e.g. caused by control constructions.
Thanks to the modularity of XDG, the PA part of the grammar is the same for German and Dutch.
The set LabPA of labels on the PA dimension is: LabPA = fag;pat;propg (14) Corresponding resp.
to agent, patient and proposition.
The PA dimension uses the following onedimensional principles: dag(PA) valency(PA;inPA;outPA) (15) Note that we re-use the valency principle again, as we did on the ID and LP dimensions.
And also the following multi-dimensional principles: climbing(ID; PA) linking(PA; ID) (16) Here, we re-use the climbing and linking principles.
That is, we state that the ID tree is flatter than the corresponding PA dag.
This captures raising and control, where arguments of embedded infinite verbs can “climb up” and become arguments of a raising or control verb, in the same way as syntactic arguments can “climb up” from ID to LP.
We use the linking principle to specify how semantic arguments are to be realized syntactically (e.g.
the agent as a subject etc.).
We display the PA part of the lexicon in Figure 3.8 Here is an example PA dag analysis of example sentence (7): . Peter versucht einen Roman zu lesen ag prop patag (17) Here, Peter is the agent of versucht, and also the agent of lesen.
Furthermore, lesen is a proposition dependent of versucht, and Roman is the patient of lesen.
Notice that the PA dag is indeed a dag and not a tree since Peter has two incoming edges: It is simultaneously the agent of versucht and of lesen.
This is enforced by by the valency principle: Both versucht and lesen require an agent.
Peter is the only word which can be the agent of both, because it is a subject and the agents of versucht and lesen must be subjects by the linking principle.
The climbing principle ensures that predicate arguments can 8Notice that we specify linking lexically, allowing us to capture deviations from the typical linking patterns.
Still, we can also accommodate linking generalizations using lexical abstractions.
be “raised” on the ID structure with respect to the PA structure.
Again, this example demonstrates that XDG is able to reduce a complex phenomenon such as control to the interaction of per se fairly simple principles such as valency, climbing and linking.
5 Comparison
This section includes a more in-depth comparison of XDG with purely multiand mono-stratal approaches.
Contrary to multi-stratal approaches like LFG or MTT, XDG is more integrated.
For one, it places a lighter burden the interfaces between the dimensions.
In LFG for instance, the φ-mapping from cstructure to f-structure is rather specific, and has to be specifically adapted to new c-structures, e.g. in order to handle a new construction with a different word order.
That is, not only the grammar rules for the c-structure need to be adapted, but also the interface between cand f-structure.
In XDG, complex phenomena arise out of the interaction of simple, maximally general principles.
To accommodate the new construction, the grammar would ideally only need to be adapted on the word order dimension.
Furthermore, XDG allows interactions of relational constraints between all dimensions, not only between adjacent ones (like cand f-structure), and in all directions.
For one, this gets us bidirectionality for free.
Secondly, the interactions of XDG have the potential to help greatly in reducing ambiguity.
In multi-stratal approaches, ambiguity must be duplicated throughout the system.
E.g. suppose there are two candidate c-structures in LFG parsing, but one is ill-formed semantically.
Then they can only be ruled out after duplicating the ambiguity on the f-structure, and then filtering out the ill-formed structure on the semantic σ-structure.
In XDG on the other hand, the semantic principles can rule out the ill-formed analysis much earlier, typically on the basis of a partial syntactic analysis.
Thus, ill-formed analyses are never duplicated.
Contrary to mono-stratal ones, XDG is more modular.
For one, as (Oliva et al., 1999) note, mono-stratal approaches like HPSG usually give precedence to the syntactic tree structure, while putting the description of other aspects of the analysis on the secondary level only, by means of features spread over the nodes of the tree.
As a result, it becomes a hard task to modularize grammars.
Because syntax is privileged, the phenomena ascribing to semantics cannot be described independently, and whenever the syntax part of the grammar changes, the semantics part needs to be adapted.
In XDG, no dimension is privileged to another.
Semantic pheinPA outPA linkPA;ID den fg fg fg Roman fag?pat?g fg fg Peter fag?pat?g fg fg versucht fg fag!prop!g fag 7!fsubjg;prop 7!fvinfgg zu fg fg fg lesen fprop?g fag!pat!g fag 7!fsubjg;pat 7!fobjgg Figure 3: Lexicon of the example grammar fragment, PA part nomena can be described much more independently from syntax.
This facilitates grammar engineering, and also the statement of cross-linguistic generalizations.
Assuming that the semantics part of a grammar stay invariant for most natural languages, in order to accommodate a new language, ideally only the syntactic parts would need to be changed.
6 Conclusion
In this paper, we introduced the XDG grammar framework, and emphasized that its new methodology places it in between the extremes of multiand mono-stratal approaches.
By means of an idealized example grammar, we demonstrated how complex phenomena are explained as arising from the interaction of simple principles on numerous dimensions of linguistic description.
On the one hand, this methodology has the potential to modularize linguistic description and grammar engineering, and to facilitate the statement of linguistic generalizations.
On the other hand, as XDG is a inherently concurrent architecture, inferences from any dimension can help reduce the ambiguity on others.
XDG is a new grammar formalism, and still has many open issues.
Firstly, we need to continue work on XDG as a framework.
Here, one important goal is to find out what criteria we can give to restrict the principles.
Secondly, we need to evolve the XDG grammar theory, and in particular the XDG syntaxsemantics interface.
Thirdly, for practical use, we need to improve our knowledge about XDG solving (i.e.
parsing and generation).
So far, our only good results are for smaller-scale handwritten grammars, and we have not good results yet for largerscale grammars induced from treebanks (NEGRA, PDT) or converted from other grammar formalisms (XTAG).
Finally, we need to incorporate statistics into the picture, e.g. to guide the search for solutions, in the vein of (Dienes et al., 2003).
References Joan Bresnan and Ronald Kaplan.
1982. Lexicalfunctional grammar: A formal system for grammatical representation.
In Joan Bresnan, editor, The Mental Representation of Grammatical Relations, pages 173–281.
The MIT Press, Cambridge/USA.
Marie-H`el´ene Candito.
1996. A principle-based hierarchical representation of LTAG.
In Proceedings of COLING 1996, Kopenhagen/DEN.
Peter Dienes, Alexander Koller, and Marco Kuhlmann.
2003. Statistical A* Dependency Parsing.
In Prospects and Advances in the Syntax/Semantics Interface, Nancy/FRA.
Denys Duchier and Ralph Debusmann.
2001. Topological dependency trees: A constraintbased account of linear precedence.
In Proceedings of ACL 2001, Toulouse/FRA.
Denys Duchier.
2003. Configuration of labeled trees under lexicalized constraints and principles.
Research on Language and Computation, 1(3– 4):307–336.
Sylvain Kahane, Alexis Nasr, and Owen Rambow.
1998. Pseudo-projectivity: a polynomially parsable non-projective dependency grammar.
In 36th Annual Meeting of the Association for Computational Linguistics (ACL 1998), Montr´eal/CAN.
Sylvain Kahane.
2002. Grammaire d’Unification Sens-Texte: Vers un mod`ele math´ematique articul´e de la langue.
Universit´e Paris 7.
Document de synth`ese de l’habilitation `a diriger les recherches.
Alexander Koller and Kristina Striegnitz.
2002. Generation as dependency parsing.
In Proceedings of ACL 2002, Philadelphia/USA.
Igor Mel’ˇcuk.
1988. Dependency Syntax: Theory and Practice.
State Univ.
Press of New York, Albany/USA.
Karel Oliva, M.
Andrew Moshier, and Sabine Lehmann.
1999. Grammar engineering for the next millennium.
In Proceedings of the 5th Natural Language Processing Pacific Rim Symposium 1999 “Closing the Millennium”, Beijing/CHI.
Tsinghua University Press.
Carl Pollard and Ivan A.
Sag. 1994.
HeadDriven Phrase Structure Grammar.
University of Chicago Press, Chicago/USA .

