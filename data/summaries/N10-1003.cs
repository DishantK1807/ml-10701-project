ized (Collins, 1999; Charniak, 2000), unlexicalized
A00-2018
M. Johnson. 1998. PCFG models of linguistic tree rep-
J98-4004
J. Henderson and E. Brill. 1999. Exploiting diversity
W99-0623
A. Smith, T. Cohn, and M. Osborne. 2005. Logarithmic
P05-1003
the results of Huang (2008), where the oracle score
P08-1067
Manning, 2003a; Sun and Tsujii, 2009), but we did
E09-1088
G Charniak (2000) 82.9 82.9 31.7
A00-2018
(Johnson, 1998; Klein and Manning, 2003b) or au-
J98-4004 N03-1016 P03-1054
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
P05-1010
the search space (Goodman, 1996; Titov and Hen-
P96-1024
(Sagae and Lavie, 2006; Fossum and Knight, 2009;
N06-2033 N09-2064
S. Petrov and D. Klein. 2008. Sparse multi-scale gram-
D08-1091
Zhang et al. (2009) 93.3 92.0 -
D09-1161
is shown in Petrov et al. (2006) to reduce the vari-
P06-1055
two methods work on top of the Charniak (2000)
A00-2018
the variational approximation of Matsuzaki et al.(2005). We extend the algorithm to work over poste-
P05-1010
Zhang et al., 2009). In contrast to that line of work,
D09-1161
son, 2005; Huang, 2008).
P08-1067
Petrov and Klein (2007) 90.2 90.1 36.7
N07-1051
higher than the one of the lexicalized parser of Charniak (2000).
A00-2018
L. Huang and D. Chiang. 2005. Better k-best parsing. In
W05-1506
L. Huang. 2008. Forest reranking: Discriminative pars-
P08-1067
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
J93-2004
Petrov et al. (2006), where an Expectation Maxi-
P06-1055
past (Henderson and Brill, 1999; Sagae and Lavie,
W99-0623
P. Xu and F. Jelinek. 2004. Random forests in language
W04-3242
X. Sun and J. Tsujii. 2009. Sequential labeling with la-
E09-1088
Z. Huang and M. Harper. 2009. Self-training PCFG
D09-1087
McClosky et al. (2006) 92.5 92.1 45.3
N06-1020
Smith et al. (2005) interpret Logarithmic Opinion
P05-1003
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
N06-1020
tomatically learned (Matsuzaki et al., 2005; Petrov
P05-1010
variable grammars of Petrov et al. (2006) vary
P06-1055
E. Charniak. 2000. A maximum–entropy–inspired
A00-2018
RE Charniak et al. (2005) 91.8 91.2 44.8Huang (2008) 92.2 91.2 43.5
P05-1022 P08-1067
LF Huang and Harper (2009) 91.37 91.57 39.37
D09-1087
are of similar accuracy (Smith et al., 2005).
P05-1003
H. Zhang, M. Zhang, C. L. Tan, and H. Li. 2009. K-best
D09-1161
Fossum and Knight (2009) 93.2 91.7 -
N09-2064
Petrov and Klein (2008) 80.6 80.8 43.9
D08-1091
5See Gildea (2001) for the exact setup.
W01-0521
Petrov and Klein (2007) 83.9 83.8 29.6
N07-1051
E. Charniak and M. Johnson. 2005. Coarse-to-Fine N-
P05-1022
mate the posterior distribution, as in Huang (2008).
P08-1067
Carreras et al. (2008) 91.4 90.7 -
W08-2102
in Zhang et al. (2009) are achieved by combining k-
D09-1161
G Petrov and Klein (2007) 80.0 80.2 42.4
N07-1051
of Huang (2008), except that we work with several
P08-1067
K. Sagae and A. Lavie. 2006. Parser combination by
N06-2033
D. Klein and C. Manning. 2003b. Accurate unlexicalized
N03-1016 P03-1054
W. Skut, B. Krenn, T. Brants, and H. Uszkoreit. 1997.
A97-1014
D. Klein and C. Manning. 2003a. A* parsing: fast exact
N03-1016 P03-1054
V. Fossum and K. Knight. 2009. Combining constituent
N09-2064
niak and Johnson (2005) and Huang (2008), achiev-
P08-1067
J. Goodman. 1996. Parsing algorithms and metrics. ACL
P96-1024
(Skut et al., 1997) 1-18,602 18,603-19,602 19,603-20,602
A97-1014
X. Carreras, M. Collins, and T. Koo. 2008. TAG, dy-
W08-2102
In related work, Zhang et al. (2009) achieve ex-
D09-1161
McClosky et al. (2006). Clearly, replacing the sin-
N06-1020
search space. Petrov and Klein (2007) present such
N07-1051
bracket recall algorithm of Goodman (1996), or in
P96-1024
J. Henderson and E. Brill. 2000. Bagging and boosting a
A00-2005
D. Gildea. 2001. Corpus variation and parser perfor-
W01-0521
S. Petrov and D. Klein. 2007. Improved inference for
N07-1051
al., 2006; Petrov and Klein, 2007) the final grammar
N07-1051
(Marcus et al., 1993) 2-21
J93-2004
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
P06-1055
RE Charniak et al. (2005) 86.1 85.2 36.8
P05-1022
refer the reader to Matsuzaki et al. (2005) and Petrov
P05-1010
best lists from Charniak (2000) and Petrov et al.(2006), they obtain an F1 score of 91.0 (which they
A00-2018 P06-1055
and Knight, 2009; Zhang et al., 2009). Even then,
D09-1161
Matsuzaki et al. (2005) derive an EM algorithm
P05-1010
training procedures. Xu and Jelinek (2004) made
W04-3242
I. Titov and J. Henderson. 2006. Loss minimization in
W06-1666
derson, 2006; Petrov and Klein, 2007).
N07-1051
O Sagae and Lavie (2006) 93.2 91.0 -
N06-2033
E Charniak (2000) 89.9 89.5 37.2
A00-2018
