For natural language problems in general, of course, it is widely recognized that significant accuracy gains can often be achieved by generalizing over relevant feature combinations (e.g., Kudo and Matsumoto (2003)).
P03-1004
Na¨ıve Bayes models (e.g., Mooney (1996), Chodorow et al.(1999), Pedersen (2001), Yarowsky and Florian (2002)) as well as maximum entropy models (e.g., Dang and Palmer (2002), Klein and Manning (2002)) in particular have shown a large degree of success for WSD, and have established challenging state-of-the-art benchmarks.
W02-0813 W02-1002 W96-0208
Note that these results are consistent with the larger study of supervised models conducted by Wu et al.(2004). The composite semisupervised KPCA model outperforms all of the three supervised models, and in particular, it further improves the Table 5: Semi-supervised KPCA is not necessary when supervised KPCA is very confident.
P04-1081
Wu et al.(2004) propose an efficient and accurate new supervised learning model for word sense disambiguation (WSD), that exploits a nonlinear Kernel Principal Component Analysis (KPCA) technique to make predictions implicitly based on generalizations over feature combinations.
P04-1081
