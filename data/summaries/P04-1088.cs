POS tags don’t help LSA either, as has already been observed by (Wiemer-Hastings, 2001; Kanejiya et al., 2003) for other tasks.
W03-0208
It has been annotated for a variety of features, including four DAs3 (Glass et al., 2002): problem solving, the tutor gives problem solving directions; judgment, the tutor evaluates the student’s actions or diagnosis; domain knowledge, the tutor imparts domain knowledge; and other, when none of the previous three applies.
W02-0205
To support this claim, first, we used the κ coefficient (Krippendorff, 1980; Carletta, 1996) to assess the agreement between the classification made by FLSA and the classification from the corpora — see Table 8.
J96-2004
In recent years, a variety of empirical techniques have been used to train the dialogue act classifier (Samuel et al., 1998; Stolcke et al., 2000).
J00-3003 P98-2188
(Stolcke et al., 2000) employs a combination of HMM, neural networks and decision trees trained on all available features (words, prosody, sequence of DAs and speaker identity).
J00-3003
In this sort of situations, tag categories are often collapsed when running experiments so as to get meaningful frequencies (Stolcke et al., 2000).
J00-3003
Dialogue game annotation is based on the MapTask notion of a dialogue game, a set of utterances starting with an initiation and encompassing all utterances up until the purpose of the game has been fulfilled (e.g., the requested information has been transferred) or abandoned (Carletta et al., 1997).
J97-1002
We also compared the confusion matrix from (Carletta et al., 1997) with the confusion matrix we obtained for our best result on MapTask (FLSA using Game + Speaker).
J97-1002
To put Table 8 in perspective, note that expert human coders achieved κ = 0.83 on DA classification for MapTask, but also had available the speech source (Carletta et al., 1997).
J97-1002
(Stolcke et al., 2000) reports an impressive 71% accuracy on transcribed Switchboard dialogues, using a tag set of 42 DAs.
J00-3003
We are also exploring whether FLSA can be used as the basis for semi-automatic annotation of dialogue acts, to be incorporated into MUP, an annotation tool we have developed (Glass and Di Eugenio, 2002).
W02-0205
As regards results on DA classification for other corpora, the best performances obtained are up to 75% for task-oriented dialogues such as Verbmobil (Samuel et al., 1998).
P98-2188
