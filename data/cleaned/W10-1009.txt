Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 66–73,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics
Leveraging Hiden Dialogue State to Select Tutorial Moves 
 
Kristy 
Elizabeth  
  Boyer
a
 
Robert 
   Phillips
ab
 
Eun Young 
Ha
a 
Michael 
D.  
  Wallis
ab 
Mladen A.  
 Vouk
a
 
James C. 
Lester
a 
 
 
a
Department of Computer Science, North Carolina State University 
b
Aplied Research Asociates 
Raleigh, NC, USA 
 
{keboyer, rphili, eha, mdwalis, vouk, lester}@ncsu.edu 
 
 
 
Abstract 
A central chalenge for tutorial dialogue 
systems is selecting an apropriate move 
given the dialogue context. Corpus-based 
aproaches to creating tutorial dialogue 
management models may facilitate more 
flexible and rapid development of tutorial 
dialogue systems and may increase the 
efectivenes of these systems by alowing 
data-driven adaptation to learning contexts 
and to individual learners. This paper presents 
a family of models, including first-order 
Markov, hiden Markov, and hierarchical 
hiden Markov models, for predicting tutor 
dialogue acts within a corpus. This work takes 
a step toward fuly data-driven tutorial 
dialogue management models, and the results 
highlight important directions for future work 
in unsupervised dialogue modeling. 
1 Introduction

A central chalenge for dialogue systems is 
selecting apropriate system dialogue moves 
(Bangalore, Di Fabrizio, & Stent, 208; Frampton 
& Lemon, 209; Young et al., 209). For tutorial 
dialogue systems, which aim to suport learners 
during conceptual or aplied learning tasks, 
selecting an apropriate dialogue move is 
particularly important because the tutorial 
aproach could significantly influence cognitive 
and afective outcomes for the learner (Chi, 
Jordan, VanLehn, & Litman, 209). The strategies 
implemented in tutorial dialogue systems have 
historicaly ben based on handcrafted rules 
derived from observing human tutors (e.g., Aleven, 
McLaren, Rol, & Koedinger, 204; Evens & 
ichael, 206; Graeser, Chipman, Haynes, & 
Olney, 205; Jordan, Makatchev, Papuswamy, 
VanLehn, & Albacete, 206). While these systems 
can achieve results on par with unskiled human 
tutors, tutorial dialogue systems have not yet 
matched the efectivenes of expert human tutors 
(VanLehn et al., 207). 
A more flexible model of strategy selection may 
enable tutorial dialogue systems to increase their 
efectivenes by responding adaptively to a broader 
range of contexts. A promising method for 
deriving such a model is to learn it directly from 
corpora of efective human tutoring. Data-driven 
aproaches have shown promise in task-oriented 
domains outside of tutoring (Bangalore et al., 
208; Hardy et al., 206; Young et al., 209), and 
automatic dialogue policy creation for tutoring has 
ben explored recently (Chi, Jordan, VanLehn, & 
Hal, 208; Tetreault & Litman, 208). Ultimately, 
devising data-driven aproaches for developing 
tutorial dialogue systems may constitute a key step 
towards achieving the high learning gains that have 
ben observed with expert human tutors. 
The work presented in this paper focuses on 
learning a model of tutorial moves within a corpus 
of human-human dialogue in the task-oriented 
domain of introductory computer science. Unlike 
the majority of task-oriented domains that have 
ben studied to date, our domain involves the 
separate creation of a persistent artifact by the user 
(the student). The modification of this artifact, in 
our case a computer program, is the focus of the 
dialogues. Our corpus consists of textual dialogue 
uterances and a separate synchronous stream of 
66
task actions. Our goal is to extract a data-driven 
dialogue management model from the corpus, as 
evidenced by predicting system (tutor) dialogue 
acts. 
In this paper, we present an anotation aproach 
that adreses dialogue uterances and task actions, 
and we propose a unified sequential representation 
for these separate synchronous streams of events. 
We explore the predictive power of thre 
stochastic models — first-order Markov models, 
hiden Markov models, and hierarchical hiden 
Markov models — for predicting tutor dialogue 
acts in the unified sequences. By leveraging these 
models to capture efective tutorial dialogue 
strategies, this work takes a step toward creating 
data-driven tutorial dialogue management models. 
2 Related
Work 
Much of the research on selecting system dialogue 
acts relies on a Markov asumption (Levin, 
Pieracini, & Eckert, 200). This formulation is 
often used in conjunction with reinforcement 
learning (RL) to derive optimal dialogue policies 
(Frampton & Lemon, 209). Sparse data and large 
state spaces can pose serious obstacles to RL, and 
recent work aims to adres these isues (Ai, 
Tetreault, & Litman, 207; Henderson, Lemon, & 
Georgila, 208; Heman, 207; Young et al., 
209). For tutorial dialogue, RL has ben aplied 
to selecting a state space representation that best 
facilitates learning an optimal dialogue policy 
(Tetreault & Litman, 208). RL has also ben used 
to compare specific tutorial dialogue tactic choices 
(Chi et al., 208). 
While RL learns a dialogue policy through 
exploration, our work asumes that a flexible, god 
(though posibly not optimal) dialogue policy is 
realized in sucesful human-human dialogues. We 
extract this dialogue policy by predicting tutor 
(system) actions within a corpus. Using human 
dialogues directly in this way has ben the focus of 
work in other task-oriented domains such as 
finance (Hardy et al., 206) and catalogue ordering 
(Bangalore et al., 208). Like the parse-based 
models of Bangalore et al., our hierarchical hiden 
Markov models (HM) explicitly capture the 
hierarchical nesting of tasks and subtasks in our 
domain. In other work, this level of structure has 
ben studied from a slightly diferent perspective 
as conversational game (Poesio & Mikhev, 198). 
For tutorial dialogue, there is compeling 
evidence that human tutoring is a valuable model 
for extracting dialogue system behaviors. The 
CIRCSIM-TUTOR (Evens & Michael, 206), 
ITSPOKE (Forbes-Riley, Rotaru, Litman, & 
Tetreault, 207; Forbes-Riley & Litman, 209), 
and KSC-PAL (Kersey, Di Eugenio, Jordan, & 
Katz, 209) projects have made extensive use of 
data-driven techniques based on human corpora. 
Perhaps most directly comparable to the curent 
work are the bigram models of Forbes-Riley et al.; 
we explore first-order Markov models, which are 
equivalent to bigram models, for predicting tutor 
dialogue acts. In adition, we present HMs and 
HMs trained on our corpus. We found that 
both of these models outperformed the bigram 
model for predicting tutor moves. 
3 Corpus
and Annotation 
The corpus was colected during a human-human 
tutoring study in which tutors and students worked 
to solve an introductory computer programing 
problem (Boyer et al., in pres). The dialogues 
were efective: on average, students exhibited a 7% 
absolute gain from pretest to postest (N=48, paired 
t-test p<0.001). 
The corpus contains 48 textual dialogues with a 
separate, synchronous task event stream. Tutors 
and students colaborated to solve an introductory 
computer programing problem using an online 
tutorial environment with shared workspace 
viewing and textual dialogue. Each student 
participated in exactly one tutoring sesion. The 
corpus contains 1,468 student uterances, 3,38 
tutor uterances, and 3,793 student task actions. In 
order to build the dialogue model, we anotated 
the corpus with dialogue act tags and task 
anotation labels. 
3.1 Dialogue
Act Anotation 
We have developed a dialogue act tagset inspired 
by schemes for conversational spech (Stolcke et 
al., 200), task-oriented dialogue (Core & Alen, 
197), and tutoring (Litman & Forbes-Riley, 
2006). The dialogue act tags are displayed in Table 
1. Overal reliability on 10% of the corpus for two 
anotators was ĸ=0.80. 
 
 
67
 
Table 1. Dialogue act tags 
DA  Description 
Stu. 
Rel. 
Freq. 
Tut. 
Rel. 
Freq.  κ 
ASESING 
QUESTION (AQ) 
Request for fedback on 
task or conceptual 
uterance. 
.20  .11  .91 
EXTRA‐DOMAIN 
(EX) 
Asides not relevant to the 
tutoring task. 
.08  .04  .79 
GROUNDING (G) Acknowledgement/thanks 
.26  .06  .92 
LUKEWARM 
CONTENT 
FEDBACK (LCF) 
Negative asesment with 
explanation. 
.01  .03  .53 
LUKEWARM 
FEDBACK (LF) 
Lukewarm asesment of 
task action or conceptual 
uterance. 
.02  .03  .49 
NEGATIVE 
CONTENT 
FEEDBACK 
(NCF) 
Negative asesment with 
explanation. 
.01  .10  .61 
NEGATIVE 
FEDBACK (NF) 
Negative asesment of 
task action or conceptual 
uterance. 
.05  .02  .76 
POSITIVE 
CONTENT 
FEEDBACK (PCF) 
Positive asesment with 
explanation. 
.02  .03  .43 
POSITIVE 
FEDBACK (PF) 
Positive asesment of 
task action or conceptual 
uterance. 
.09  .16  .81 
QUESTION (Q) 
Task or conceptual 
question. 
.09  .03  .85 
STATEMENT (S) 
Task or conceptual 
asertion. 
.16  .41  .82 
3.2 Task
Annotation 
The dialogues focused on the task of solving an 
introductory computer programing problem. The 
task actions were recorded as a separate but 
synchronous event stream. This stream included 
97,509 keystroke-level user task events. These 
events were manualy agregated and anotated for 
subtask structure and then for corectnes. The task 
anotation scheme was hierarchical, reflecting the 
nested nature of the subtasks. An excerpt from the 
task anotation scheme is depicted in Figure 1; the 
ful scheme contains 6 leaves. The task anotation 
scheme was designed to reflect the diferent depth 
of posible subtasks nested within the overal task. 
Each labeled task action was also judged for 
corectnes acording to the requirements of the 
task, with categories CORECT, BUGGY, 
INCOMPLETE, and DISPREFERED (technicaly 
corect but not acomplishing the pedagogical 
goals of the task). 
Each group of task keystrokes that ocured 
betwen dialogue uterances was taged, posibly 
with many subtask labels, by a human judge. A 
second judge taged 20% of the corpus in a 
reliability study for which one-to-one subtask 
identification was not enforced (giving judges 
maximum flexibility to aply the tags). To ensure a 
conservative reliability statistic, al unmatched 
subtask tags were treated as disagrements. The 
resulting unweighted kapa statistic was ĸ
simple
= 
0.58, but the weighted Kapa ĸ
weighted
=0.86 is ore 
meaningful because it takes into acount the 
ordinal nature of the labels that result from 
sequential subtasks. On task actions for which the 
two judges agred on subtask tag, the agrement 
statistic for corectnes was ĸ
simple
=0.80. 
 
Figure 1. Portion of task anotation scheme 
3.3 Adjacency
Pair Joining 
Some dialogue acts establish an expectation for 
another dialogue act to ocur next (Scheglof & 
Sacks, 1973). Our previous work has found that 
identifying the statisticaly significant adjacency 
pairs in a corpus and joining them as atomic 
observations prior to model building produces 
more interpretable descriptive models. The models 
reported here were trained on hybrid sequences of 
dialogue acts and adjacency pairs. A ful 
description of the adjacency pair identification 
methodology and joining algorithm is reported in 
(Boyer et al., 209). A partial list of the most 
highly statisticaly significant adjacency pairs, 
68
which for this work include task actions, is 
displayed in Table 2. 
 
Table 2. Subset of significant adjacency pairs 
CORECTASKACTION‐CORECTASKACTION; 
EXTRADOMAIN
S
‐EXTRADOMAIN
T
; GROUNDING
S
‐GROUNDING
T
; 
ASESINGQUESTION
T
‐POSITIVEFEDBACK
S
; 
ASESINGQUESTION
S
‐POSITIVEFEDBACK
T
; QUESTION
T
‐STATEMENT
S
; 
ASESINGQUESTION
T
‐STATEMENT
S
; EXTRADOMAIN
T
‐EXTRADOMAIN
S
; 
QUESTION
S
‐TATEMENT; NEGATIVEFEDBACK
S
‐GROUNDING
T
; 
INCOMPLETETASKACTION‐INCOMPLETETASKACTION; 
POSITIVEFEDBACK
S
‐GROUNDING
T
; 
BUGGYTASKACTION‐BUGGYTASKACTION 
4 Models

We learned thre types of models using cros-
validation with systematic sampling of training and 
testing sets. 
4.1 First-Order Markov Model 
The simplest model we discus is the first-order 
Markov model (M), or bigram model (Figure 2). 
A M that generates observation (state) sequence 
o
1
o
2
…o
t
 is defined in the folowing way. The 
observation symbols are drawn from the alphabet 
∑={σ
1, σ
2, …, σ
M
}, and the initial probability 
distribution is Π=[π
i
] where π
i
 is the probability of 
a sequence begining with observation symbol σ
i
. 
The transition probability distribution is A=[a
ij
], 
where a
ij
 is the probability of observation j 
ocuring imediately after observation i. 
 
Figure 2. Time-slice topology of M 
 
We trained Ms on our corpus of dialogue acts 
and task events using ten-fold cros-validation to 
produce a model that could be queried for the next 
predicted tutorial dialogue act given the history. 
4.2 Hidden
Markov Model 
A hiden Markov model (HM) augments the 
M framework, resulting in a doubly stochastic 
structure (Rabiner, 1989). For a first-order HM, 
the observation symbol alphabet is defined as 
above, along with a set of hiden states 
S={s
1,s
2,…,s
N
}. The transition and initial 
probability distributions are defined analogously to 
Ms, except that they operate on hiden states 
rather than on observation symbols (Figure 3). 
That is, Π=[π
i
] where π
i
 is the probability of a 
sequence begining in hiden state s
i
. The 
transition matrix is A=[a
ij
], where a
ij
 is the 
probability of the model transitioning from hiden 
state i to hiden state j. This framework constitutes 
the first stochastic layer of the model, which can be 
thought of as modeling hiden, or unobservable, 
structure. The second stochastic layer of the model 
governs the production of observation symbols: the 
emision probability distribution is B=[b
ik
] where 
b
ik
 is the probability of state i emiting observation 
symbol k. 
 
Figure 3. Time-slice topology of HM 
 
The notion that dialogue has an overarching 
unobservable structure that influences the 
observations is widely acepted. In tutoring, this 
overarching structure may corespond to tutorial 
strategies. We have explored HMs’ descriptive 
power for extracting these strategies (Boyer et al., 
209), and this paper explores the hypothesis that 
HMs provide beter predictive power than Ms 
on our dialogue sequences. We trained HMs on 
the corpus using the standard Baum-Welch 
expectation maximization algorithm and aplied 
state labels that reflect post-hoc interpretation 
(Figure 4). 
 
 
Figure 4. Portion of learned HM 
69
4.3 Hierarchical
Hidden Markov Model 
Hierarchical hiden Markov models (HMs) 
alow for explicit representation of multilevel 
stochastic structure. A complete formal definition 
of HMs can be found in (Fine, Singer, & 
Tishby, 198), but here we present an informal 
description. HMs include two types of hiden 
states: internal nodes, which do not produce 
observation symbols, and production nodes, which 
do produce observations. An internal node includes 
a set of substates that corespond to its potential 
children, S={s
1, s
2, …, s
N
}, each of which is itself 
the rot of an HM. The initial probability 
distribution Π=[π
i
] for each internal node governs 
the probability that the model wil make a vertical 
transition to substate s
i
 from this internal node; that 
is, that this internal node wil produce substate s
i
 as 
its leftmost child. Horizontal transitions are 
governed by a transition probability distribution 
similar to that described above for flat HMs. 
Production nodes are defined by their observation 
symbol alphabet and an emision probability 
distribution over the symbols; HMs do not 
require a global observation symbol alphabet. The 
generative topology of our HMs is ilustrated 
in Figure 5. 
 
Figure 5. Generative topology of HM 
 
HMs of arbitrary topology can be trained using 
a generalized version of the Baum-Welch 
algorithm (Fine et al., 198). Our HMs 
featured a pre-specified model topology based on 
known task/subtask structure. A Bayesian view of 
a portion of the best-fit HM is depicted in 
Figure 6.  This model was trained using five-fold 
cros-validation to adres the absence of symbols 
from the training set that were present in the 
testing set, a sparsity problem that arose from 
spliting the data hierarchicaly. 
Figure 6. Portion of learned HM 
70
5 Results

We trained and tested Ms, HMs, and HMs 
on the corpus and compared prediction acuracy 
for tutorial dialogue acts by providing the model 
with partial sequences from the test set and 
querying for the next tutorial move. The baseline 
prediction acuracy for this task is 41.1%, 
coresponding to the most frequent tutorial 
dialogue act (STATEMENT). As depicted in 
Figure 7, a first-order  performed worse than 
baseline (p<0.01)
1
 at 27% average prediction 
acuracy (
€ 
ˆ 
σ 
MM
=6%). HMs performed beter 
than baseline (p<0.001), with an average acuracy 
of 48% (
€ 
ˆ 
σ 
HM
=3%). HMs averaged 57% 
accuracy, significantly higher than baseline 
(p=0.02) but weakly significantly higher than 
HMs (p=0.04), and with high variation 
(
€ 
ˆ 
σ
HM
=23%). 
 
Figure 7. Average prediction acuracies of thre 
model types on tutor dialogue acts 
 
To further explore the performance of the 
HMs, Figure 8 displays their prediction 
accuracy on each of six labeled subtasks. These 
subtasks corespond to the top level of the 
hierarchical task/subtask anotation scheme. The 
UNDERSTAND THE PROBLEM subtask coresponds 
to the initial phase of most tutoring sesions, in 
which the student and tutor agre to some extent 
on a problem-solving plan. Subtasks 1, 2, and 3 
acount for the implementation and debuging of 
thre distinct modules within the learning task, and 
Subtask 4 involves testing and asesing the 
student’s finalized program. The EXTRA-DOMAIN 
subtask involves side conversations whose topics 
are outside of the domain. 
The HM performed as wel as or beter 
(p<0.01) than baseline on the first thre in-domain 
subtasks. The performance on SUBTASK 4 was not 
distinguishable from baseline (p=0.06); relatively 
few students reached this subtask. The model did 
                                                
1
 Al p-values in this section were produced by two-sample 
one-tailed t-tests with unequal sample variances. 
not outperform baseline (p=0.40) for the 
UNDERSTAND THE PROBLEM subtask, and 
qualitative inspection of the corpus reveals that the 
dialogue during this phase of tutoring exhibits 
limited regularities betwen students. 
 
Figure 8. Average prediction acuracies of 
HMs by subtask 
6 Discussion

The results suport our hypothesis that HMs, 
because of their capacity for explicitly representing 
dialogue structure at an abstract level, perform 
beter than Ms for predicting tutor moves. The 
results also sugest that explicitly modeling 
hierarchical task structure can further improve 
prediction acuracy of the model. The below-
baseline performance of the bigram model 
illustrates that in our complex task-oriented 
domain, an imediately preceding event is not 
highly predictive of the next move. While this 
finding may not hold for conversational dialogue 
or some task-oriented dialogue with a more 
balanced distribution of uterances betwen 
speakers, the unbalanced nature of our tutoring 
sesions may not be as easily captured. 
In our corpus, tutor uterances outnumber 
student uterances by more than two to one. This 
large diference is due to the fact that tutors 
frequently guided students and provided multi-turn 
explanations, the impetus for which are not 
captured in the corpus, but rather, involve external 
pedagogical goals. The M, or bigram model, has 
no mechanism for capturing this layer of stochastic 
behavior. On the other hand, the HM can 
acount for unobserved influential variables, and 
the HM can do so to an even greater extent by 
explicitly modeling task/subtask structure. 
Considering the performance of the HM on 
individual subtasks reveals interesting properties of 
our dialogues. First, the HM is unable to 
outperform baseline on the UNDERSTAND THE 
PROBLEM subtask. To adres this isue, our 
ongoing work investigates taking into acount 
71
student characteristics such as incoming 
knowledge level and self-confidence. On al four 
in-domain subtasks, the HM achieved a 30% to 
50% increase over baseline. For extra-domain 
dialogues, which involve side conversations that 
are not task-related, the HM achieved 86% 
prediction acuracy on tutor moves, which 
constitutes a 15% improvement over baseline. 
This high acuracy may be due in part to the fact 
that out-of-domain asides were almost exclusively 
initiated by the student, and tutors rarely engaged 
in such exchanges beyond providing a single 
response. This regularity likely facilitated 
prediction of the tutor’s dialogue moves during 
out-of-domain talk. 
We are aware of only one recent project that 
reports extensively on predicting system actions 
from a corpus of human-human dialogue. 
Bangalore et al.’s (208) flat task/dialogue model 
in a catalogue-ordering domain achieved a 
prediction acuracy of 5% for system dialogue 
acts, a 175% improvement over baseline. When 
explicitly modeling the hierarchical task/subtask 
dialogue structure, they report a prediction 
acuracy of 35.6% for system moves, 
aproximately 75% above baseline (Bangalore & 
Stent, 209). These findings were obtained by 
utilizing a variety of lexical and syntactic features 
along with manualy anotated dialogue acts and 
task/subtask labels. In comparison, our HM 
achieved an average 42% improvement over 
baseline using only anotated dialogue acts and 
task/subtask labels. In ongoing work we are 
exploring the utility of aditional features for this 
prediction task. 
Our best model performed beter than baseline 
by a significant margin. The absolute prediction 
acuracy achieved by the HM was 57% acros 
the corpus, which at first blush may apear to low 
to be of practical use. However, the choice of 
tutorial move involves some measure of 
subjectivity, and in many contexts there may be no 
uniquely apropriate dialogue act. Work in other 
domains has dealt with this uncertainty by 
maintaining multiple hypotheses (Wright Hastie, 
Poesio, & Isard, 202) and by maping to clustered 
sets of moves rather than maintaining policies for 
each posible system selection (Young et al., 
209). Such aproaches may prove useful in our 
domain as wel, and may help to more fuly realize 
the potential of a learned dialogue management 
model. 
7 Conclusion
and Future Work 
Learning models that predict system moves within 
a corpus is a first step toward building fuly data-
driven dialogue management models. We have 
presented Markov models, hiden Markov models, 
and hierarchical hiden Markov models trained on 
sequences of manualy anotated dialogue acts and 
task events. Of the thre models, the hierarchical 
models apear to perform best in our domain, 
which involves an intrinsicaly hierarchical 
task/subtask structure. 
The models’ performance points to promising 
future work that includes utilizing aditional 
lexical and syntactic features along with fixed user 
(student) characteristics within a hierarchical 
hiden Markov modeling framework. More 
broadly, the results point to the importance of 
considering task structure when modeling a 
complex domain such as those that often 
accompany task-oriented tutoring. Finaly, a key 
direction for data-driven dialogue management 
models involves learning unsupervised dialogue 
act and task clasification models. 
 
Acknowledgements.  This work is suported in 
part by the North Carolina State University 
Department of Computer Science and the National 
Science Foundation through a Graduate Research 
Felowship and Grants CNS-0540523, REC-
0632450 and IS-081291. Any opinions, findings, 
conclusions, or recomendations expresed in this 
report are those of the participants, and do not 
necesarily represent the oficial views, opinions, 
or policy of the National Science Foundation. 
References  
Ai, H., Tetreault, J. R., & Litman, D. J. (207). 
Comparing user simulation models for dialog strategy 
learning. Procedings of NACL HLT, Companion 
Volume, Rochester, New York. 1-4. 
Aleven, V., McLaren, B., Rol, I., & Koedinger, K. 
(204). Toward tutoring help seking: Aplying 
cognitive modeling to meta-cognitive skils. 
Procedings of ITS, 227-239. 
Bangalore, S., Di Fabrizio, G., & Stent, A. (208). 
Learning the structure of task-driven human-human 
dialogs. IEE Transactions on Audio, Spech, and 
Language Procesing, 16(7), 1249-1259. 
72
Bangalore, S., & Stent, A. J. (209). Incremental 
parsing models for dialog task structure. Procedings 
of the EACL, 94-102. 
Boyer, K. E., Philips, R., Ha, E. Y., Walis, M. D., 
Vouk, M. A., & Lester, J. C. (209). odeling 
dialogue structure with adjacency pair analysis and 
hiden Markov models. Procedings of NACL HLT 
(Short Papers), 19-26. 
Boyer, K. E., Philips, R., Ingram, A., Ha, E. Y., Walis, 
M. D., Vouk, M. A., & Lester, J. C. (In pres). 
Characterizing the efectivenes of tutorial dialogue 
with hiden Markov models. Procedings of ITS, 
Pitsburgh, Pensylvania.  
Chi, M., Jordan, P., VanLehn, K., & Hal, M. (208). 
Reinforcement learning-based feature selection for 
developing pedagogicaly efective tutorial dialogue 
tactics. Procedings of EDM, Montreal, Canada. 258-
265. 
Chi, M., Jordan, P., VanLehn, K., & Litman, D. (209). 
To elicit or to tel: Does it mater? Procedings of 
AIED, 197-204. 
Core, M., & Alen, J. (197). Coding dialogs with the 
DASL anotation scheme. AAI Fal Symposium on 
Comunicative Action in Humans and Machines, 28–
35.  
Evens, M., & Michael, J. (206). One-on-one tutoring 
by humans and computers. Mahwah, New Jersey: 
Lawrence Erlbaum Asociates. 
Fine, S., Singer, Y., & Tishby, N. (198). The 
hierarchical hiden Markov model: Analysis and 
aplications. Machine Learning, 32(1), 41-62. 
Forbes-Riley, K., Rotaru, M., Litman, D. J., & 
Tetreault, J. (207). Exploring afect-context 
dependencies for adaptive system development. 
Procedings of NACL HLT (Short Papers), 41-4. 
Forbes-Riley, K., & Litman, D. (209). Adapting to 
student uncertainty improves tutoring dialogues. 
Procedings of AIED, 33-40. 
Frampton, M., & Lemon, O. (209). Recent research 
advances in reinforcement learning in spoken dialogue 
systems. The Knowledge Enginering Review, 24(4), 
375-408. 
Graeser, A. C., Chipman, P., Haynes, B. C., & Olney, 
A. (205). AutoTutor: An inteligent tutoring system 
with mixed-initiative dialogue. IEE Transactions on 
Education, 48(4), 612-618. 
Hardy, H., Bierman, A., Inouye, R. B., McKenzie, A., 
Strzalkowski, T., Ursu, C., Web, N., & Wu, M. 
(206). The Amitiés system: Data-driven techniques 
for automated dialogue. Spech Comunication, 48(3-
4), 354-373. 
Heman, P. A. (207). Combining reinforcement 
learning with information-state update rules. 
Procedings of NACL HLT, 268-275. 
Henderson, J., Lemon, O., & Georgila, K. (208). 
Hybrid reinforcement/supervised learning of dialogue 
policies from fixed data sets. Computational 
Linguistics, 34(4), 487-51. 
Jordan, P., Makatchev, M., Papuswamy, U., VanLehn, 
K., & Albacete, P. (206). A natural language tutorial 
dialogue system for physics. Procedings of FLAIRS, 
521-526. 
Kersey, C., Di Eugenio, B., Jordan, P., & Katz, S. 
(209). KSC-PaL: A per learning agent that 
encourages students to take the initiative. Procedings 
of the NACL HLT Workshop on Inovative use of 
NLP for Building Educational Aplications, Boulder, 
Colorado. 5-63. 
Levin, E., Pieracini, R., & Eckert, W. (200). A 
stochastic model of human-machine interaction for 
learning dialog strategies. IEE Transactions on 
Spech and Audio Procesing, 8(1), 1-23. 
Litman, D., & Forbes-Riley, K. (206). Corelations 
betwen dialogue acts and learning in spoken tutoring 
dialogues. Natural Language Enginering, 12(2), 161-
176. 
Poesio, M., & Mikhev, A. (198). The predictive 
power of game structure in dialogue act recognition: 
Experimental results using maximum entropy 
estimation. Procedings of ICSLP, 90-97. 
Rabiner, L. R. (1989). A tutorial on hiden Markov 
models and selected aplications in spech 
recognition. Procedings of the IEE, 7(2), 257-286. 
Scheglof, E., & Sacks, H. (1973). Opening up closings. 
Semiotica, 7(4), 289-327. 
Stolcke, A., Ries, K., Cocaro, N., Shriberg, E., Bates, 
R., Jurafsky, D., Taylor, P., Martin, R., Van Es-
Dykema, C., & Meter, M. (200). Dialogue act 
modeling for automatic taging and recognition of 
conversational spech. Computational Linguistics, 
26(3), 39-373. 
Tetreault, J. R., & Litman, D. J. (208). A 
reinforcement learning aproach to evaluating state 
representations in spoken dialogue systems. Spech 
Comunication, 50(8-9), 683-696. 
VanLehn, K., Graeser, A. C., Jackson, G. T., Jordan, 
P., Olney, A., & Rose, C. P. (207). When are tutorial 
dialogues more efective than reading? Cognitive 
Science, 31(1), 3-62. 
Wright Hastie, H., Poesio, M., & Isard, S. (202). 
Automaticaly predicting dialogue structure using 
prosodic features. Spech Comunication, 36(1-2), 
63-79. 
Young, S., Gasic, M., Keizer, S., Mairese, F., 
Schatzman, J., Thomson, B., & Yu, K. (209). The 
hiden information state model: A practical 
framework for POMDP-based spoken dialogue 
management. Computer Spech and Language, 24(2), 
150-174. 
73

