<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>AMI</author>
</authors>
<title>The AMI meeting corpus</title>
<date>2005</date>
<note>http://www.amiproject.org</note>
<contexts>
<context>s). With the aim of comparing the performance of QA systems for both monologues and dialogues, two scenarios were introduced: lectures and meetings in English from the CHIL (CHIL, 2004 2007) and AMI (AMI, 2005) projects. From the combination of these two viewpoints, QAST covered the following four tasks: • T1: Question Answering in manual transcripts of lectures • T2: Question Answering in automatic transc</context>
<context>eetings is design of television remote control. The language is European English. As for the lectures, meetings have been produced with simple tags and are formatted as plain text files (ISO-8859-1) (AMI, 2005). For each one of the scenarios, two sets of questions have been provided to the participants. The development data set (30-January-2007) had 50 questions each for Lectures (10 seminars) and Meetings</context>
</contexts>
<marker>AMI, 2005</marker>
<rawString>AMI. 2005. The AMI meeting corpus. http://www.amiproject.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christelle Ayache</author>
<author>Brigitte Grau</author>
<author>Anne Vilnat</author>
</authors>
<date>2006</date>
<contexts>
<context>r given a specific question (e.g. The answer to Who won the 2005 Tour de France? is Lance Armstrong.). In the QA and Information Retrieval domains progress has been assessed via evaluation campaigns (Ayache et al., 2006; Kando, 2006; Voorhees and Buckland, 2007; Nunzio et al., 2007; Giampiccolo et al., 2007). In the Question-Answering evaluations, the systems handle independent questions and should provide one answe</context>
</contexts>
<marker>Ayache, Grau, Vilnat, 2006</marker>
<rawString>Christelle Ayache, Brigitte Grau, and Anne Vilnat. 2006.</rawString>
</citation>
<citation valid="true">
<title>Evaluation of question-answering systems : The French EQueR-EVALDA Evaluation Campaign</title>
<date></date>
<journal>CHIL</journal>
<booktitle>In Proceedings of LREC’06</booktitle>
<pages>2004--2007</pages>
<location>Genoa Italy</location>
<marker></marker>
<rawString>Evaluation of question-answering systems : The French EQueR-EVALDA Evaluation Campaign. In Proceedings of LREC’06, Genoa Italy, 24-26 May. CHIL. 2004-2007. http://chil.server.de.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Galliano</author>
<author>E Geoffrois</author>
<author>G Gravier</author>
<author>J F Bonastre</author>
<author>D Mostefa</author>
<author>K Choukri</author>
</authors>
<title>Corpus description of the ESTER Evaluation Campaign for the Rich Transcription of French Broadcast News</title>
<date>2006</date>
<booktitle>In Proceedings of LREC’06</booktitle>
<location>Genoa</location>
<contexts>
<context>n QA, and to benefit from the availability of multiple speech recognizers that have been developed for these languages and tasks in the context of European or national projects (Gravier et al., 2004; Galliano et al., 2006; TC-Star, 2004 2008). In order to avoid some alignment problems encountered in the 2007 evaluation on automatic speech transcripts, the automatic speech transcripts are provided with time stamps. 4.1</context>
<context>essions in Spanish (EPPS Spanish corpus) • T5b: Question Answering in automatic transcriptions of European Parliament Plenary in Spanish (EPPS Spanish corpus) French broadcast news: the ESTER corpus (Galliano et al., 2006) is made of 10 hours of broadcast news in French, recorded from different sources (France Inter, Radio France International, Radio Classique, France Culture, Radio Television du Maroc). There are 3 d</context>
</contexts>
<marker>Galliano, Geoffrois, Gravier, Bonastre, Mostefa, Choukri, 2006</marker>
<rawString>S. Galliano, E. Geoffrois, G. Gravier, J.F. Bonastre, D. Mostefa, and K. Choukri. 2006. Corpus description of the ESTER Evaluation Campaign for the Rich Transcription of French Broadcast News. In Proceedings of LREC’06, Genoa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Giampiccolo</author>
<author>P Forner</author>
<author>A Peas</author>
<author>C Ayache</author>
<author>D Cristea</author>
<author>V Jijkoun</author>
<author>P Osenova</author>
<author>P Rocha</author>
<author>B Sacaleanu</author>
<author>R Sutcliffe</author>
</authors>
<title>Multilingual Question Answering Track</title>
<date>2007</date>
<journal>Overview of the CLEF</journal>
<booktitle>In Working Notes for the CLEF 2007 Workshop</booktitle>
<location>Budapest, Hungary</location>
<contexts>
<context>Lance Armstrong.). In the QA and Information Retrieval domains progress has been assessed via evaluation campaigns (Ayache et al., 2006; Kando, 2006; Voorhees and Buckland, 2007; Nunzio et al., 2007; Giampiccolo et al., 2007). In the Question-Answering evaluations, the systems handle independent questions and should provide one answer to each question, extracted from textual data, for both open domain and restricted doma</context>
</contexts>
<marker>Giampiccolo, Forner, Peas, Ayache, Cristea, Jijkoun, Osenova, Rocha, Sacaleanu, Sutcliffe, 2007</marker>
<rawString>D. Giampiccolo, P. Forner, A. Peas, C. Ayache, D. Cristea, V. Jijkoun, P. Osenova, P. Rocha, B. Sacaleanu, and R. Sutcliffe. 2007. Overview of the CLEF 2007 Multilingual Question Answering Track. In Working Notes for the CLEF 2007 Workshop, Budapest, Hungary, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gravier</author>
<author>J F Bonastre</author>
<author>S Galliano</author>
<author>E Geoffrois</author>
<author>K McTait</author>
</authors>
<title>The ESTER evaluation campaign of Rich Transcription of French Broadcast News</title>
<date>2004</date>
<booktitle>In Proceedings of LREC’04</booktitle>
<location>Lisbon</location>
<contexts>
<context> used to assess written QA, and to benefit from the availability of multiple speech recognizers that have been developed for these languages and tasks in the context of European or national projects (Gravier et al., 2004; Galliano et al., 2006; TC-Star, 2004 2008). In order to avoid some alignment problems encountered in the 2007 evaluation on automatic speech transcripts, the automatic speech transcripts are provide</context>
</contexts>
<marker>Gravier, Bonastre, Galliano, Geoffrois, McTait, 2004</marker>
<rawString>G. Gravier, J.F. Bonastre, S. Galliano, E. Geoffrois, K. McTait, , and K. Choukri. 2004. The ESTER evaluation campaign of Rich Transcription of French Broadcast News. In Proceedings of LREC’04, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hain</author>
<author>L Burget</author>
<author>J Dines</author>
<author>G Garau</author>
<author>M Karafiat</author>
<author>M Lincoln</author>
<author>J Vepa</author>
<author>V Wan</author>
</authors>
<title>The AMI system for the Transcription of meetings</title>
<date>2007</date>
<booktitle>In Proceedings of IEEE ICASSP’07</booktitle>
<location>Hawaii</location>
<contexts>
<context> (Mostefa et al., 2007). The AMI corpus is comprised of about 100 hours (168 meetings) of speech with both manual and automatic transcriptions. The AMI Rich Transcription 2006 ASR data has been used (Hain et al., 2007)). The domain of this meetings is design of television remote control. The language is European English. As for the lectures, meetings have been produced with simple tags and are formatted as plain t</context>
</contexts>
<marker>Hain, Burget, Dines, Garau, Karafiat, Lincoln, Vepa, Wan, 2007</marker>
<rawString>T. Hain, L. Burget, J. Dines, G. Garau, M. Karafiat, M. Lincoln, J. Vepa, and V. Wan. 2007. The AMI system for the Transcription of meetings. In Proceedings of IEEE ICASSP’07, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kando</author>
</authors>
<title>Overview of the Sixth NTCIR Workshop</title>
<date>2006</date>
<booktitle>In Proceedings of the 6th NTCIR Workshop Meeting</booktitle>
<location>Tokyo, Japan</location>
<contexts>
<context>estion (e.g. The answer to Who won the 2005 Tour de France? is Lance Armstrong.). In the QA and Information Retrieval domains progress has been assessed via evaluation campaigns (Ayache et al., 2006; Kando, 2006; Voorhees and Buckland, 2007; Nunzio et al., 2007; Giampiccolo et al., 2007). In the Question-Answering evaluations, the systems handle independent questions and should provide one answer to each que</context>
</contexts>
<marker>Kando, 2006</marker>
<rawString>N. Kando. 2006. Overview of the Sixth NTCIR Workshop. In Proceedings of the 6th NTCIR Workshop Meeting, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lamel</author>
<author>G Adda</author>
<author>E Bilinski</author>
<author>J-L Gauvain</author>
</authors>
<title>Transcribing Lectures and Seminars</title>
<date>2005</date>
<booktitle>In in InterSpeech’05</booktitle>
<location>Lisbon, Portugal</location>
<contexts>
<context>mary speaker (the person presenting the lecture) and a small amount of speech from the audience (mostly questions or comments). The manual transcriptions were done by ELDA and the ASR transcriptions (Lamel et al., 2005) were produced by LIMSI (Lamel et al., 2005). In addition to the best word hypotheses, a set of lattices and confidences for each lecture has been provided. The domain of the lectures is speech and l</context>
</contexts>
<marker>Lamel, Adda, Bilinski, Gauvain, 2005</marker>
<rawString>L. Lamel, G. Adda, E. Bilinski, and J.-L. Gauvain. 2005. Transcribing Lectures and Seminars. In in InterSpeech’05, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mostefa</author>
<author>N Moreau</author>
<author>K Choukri</author>
<author>G Potamianos</author>
<author>S M Chu</author>
<author>A Tyagi</author>
<author>J R Casas</author>
<author>J Turmo</author>
<author>L Cristoforetti</author>
<author>F Tobia</author>
<author>A Pnevmatikakis</author>
<author>V Mylonakis</author>
<author>F Talantzis</author>
<author>S Burger</author>
<author>R Stiefelhagen</author>
<author>K Bernardin</author>
<author>C Rochet</author>
</authors>
<title>The CHIL Audiovisual Corpus for Lecture and Meeting Analysis inside Smart Rooms</title>
<date>2007</date>
<journal>Journal on Language Resources and Evaluation</journal>
<pages>41--3</pages>
<contexts>
<context>nd language processing. The language is European English (mostly spoken by non native speakers). Lectures have been provided with simple tags. Seminars are formatted as plain text files (ISO-8859-1) (Mostefa et al., 2007). The AMI corpus is comprised of about 100 hours (168 meetings) of speech with both manual and automatic transcriptions. The AMI Rich Transcription 2006 ASR data has been used (Hain et al., 2007)). T</context>
</contexts>
<marker>Mostefa, Moreau, Choukri, Potamianos, Chu, Tyagi, Casas, Turmo, Cristoforetti, Tobia, Pnevmatikakis, Mylonakis, Talantzis, Burger, Stiefelhagen, Bernardin, Rochet, 2007</marker>
<rawString>D. Mostefa, N. Moreau, K. Choukri, G. Potamianos, S. M. Chu, A. Tyagi, J. R. Casas, J. Turmo, L. Cristoforetti, F. Tobia, A. Pnevmatikakis, V. Mylonakis, F. Talantzis, S. Burger, R. Stiefelhagen, K. Bernardin, and C. Rochet. 2007. The CHIL Audiovisual Corpus for Lecture and Meeting Analysis inside Smart Rooms. Journal on Language Resources and Evaluation, 41(3-4):389–407, December.</rawString>
</citation>
</citationList>
</algorithm>

