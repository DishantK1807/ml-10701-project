<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>S Engelson</author>
<author>I Dagan</author>
</authors>
<title>Minimizing manual annotation cost in supervised training from corpora.” ACL</title>
<date>1996</date>
<pages>319--326</pages>
<contexts>
<context>ld the annotators focus? This is the question addressed by active learning. Active Learning (AL) can be employed to reduce the costs of corpus annotation (Ringger et al., 2007; Tomanek, et al., 2007; Engelson &amp; Dagan, 1996). Our previous work (Ringger et al., 2007) demonstrates that by applying active learning techniques, a state of the art tagging model can be trained on as little as one-half of the amount of data req</context>
</contexts>
<marker>Engelson, Dagan, 1996</marker>
<rawString>Engelson, S. and Dagan, I. (1996). “Minimizing manual annotation cost in supervised training from corpora.” ACL. Pp. 319-326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
</authors>
<title>Sample selection for statistical grammar induction</title>
<date>2000</date>
<booktitle>In Proceedings of EMNLP/VLC-2000</booktitle>
<pages>45--52</pages>
<contexts>
<context>odel presented in Ngai and Yarowsky (2000) which predicts monetary cost given time spent, this model estimates time spent from characteristics of a sentence. Many of the costs employed in other work (Hwa, 2000; Osborne &amp; Baldridge, 2004) can be seen as estimating only some portion of the hourly cost. These distinctions make our work a novel contribution. This model reflects the abilities of the annotators </context>
</contexts>
<marker>Hwa, 2000</marker>
<rawString>Hwa, R. (2000). “Sample selection for statistical grammar induction”. In Proceedings of EMNLP/VLC-2000. Pp. 45-52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
<author>J Catlett</author>
</authors>
<title>Heterogeneous uncertainty sampling for supervised learning.” ICML</title>
<date>1994</date>
<marker>Lewis, Catlett, 1994</marker>
<rawString>Lewis, D., and Catlett, J. (1994). “Heterogeneous uncertainty sampling for supervised learning.” ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
<author>W Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers: Corrigendum and additional data</title>
<date>1995</date>
<journal>SIGIR Forum</journal>
<volume>29</volume>
<pages>13--19</pages>
<marker>Lewis, Gale, 1995</marker>
<rawString>Lewis, D., and Gale, W. (1995). “A sequential algorithm for training text classifiers: Corrigendum and additional data.” SIGIR Forum, 29 (2), Pp. 13-19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ngai</author>
<author>D Yarowsky</author>
</authors>
<title>Rule Writing or An-notation: Cost-efficient Resource Usage for Base Noun Phrase Chunking.” ACL</title>
<date>2000</date>
<pages>117--125</pages>
<marker>Ngai, Yarowsky, 2000</marker>
<rawString>Ngai, G. and Yarowsky, D. (2000) “Rule Writing or An-notation: Cost-efficient Resource Usage for Base Noun Phrase Chunking.” ACL. Pp. 117-125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Osborne</author>
<author>J Baldridge</author>
</authors>
<title>Ensemble-based AL for Parse Selection</title>
<date>2004</date>
<booktitle>HLT-NAACL 2004</booktitle>
<pages>89--96</pages>
<location>Boston, Massachusetts, USA</location>
<contexts>
<context>ted in Ngai and Yarowsky (2000) which predicts monetary cost given time spent, this model estimates time spent from characteristics of a sentence. Many of the costs employed in other work (Hwa, 2000; Osborne &amp; Baldridge, 2004) can be seen as estimating only some portion of the hourly cost. These distinctions make our work a novel contribution. This model reflects the abilities of the annotators in the study may not be rep</context>
</contexts>
<marker>Osborne, Baldridge, 2004</marker>
<rawString>Osborne, M., &amp; Baldridge, J. (2004). “Ensemble-based AL for Parse Selection”. HLT-NAACL 2004, Boston, Massachusetts, USA. Pp. 89―96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-Of-Speech Tagging.” EMNLP</title>
<date>1996</date>
<contexts>
<context>ng; hence, the data sheds some light on the time required to annotate in such circumstances. For tagging, we employed a probabilistic tagger, namely a Maximum Entropy Conditional Markov Model tagger (Ratnaparkhi, 1996; Toutanova&amp; Manning, 2000; Toutanova et al., 2003). Such taggers are referred to alternatively in the literature as MaxEnt CMMs, MEMMs, or simply “MaxEnt” taggers. The second control variable is the </context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Ratnaparkhi, A. (1996). “A Maximum Entropy Model for Part-Of-Speech Tagging.” EMNLP.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Ringger</author>
<author>P McClanahan</author>
<author>R Haertel</author>
<author>G Busby</author>
<author>M Carmen</author>
<author>J Carroll</author>
<author>K Seppi</author>
<author>D Lonsdale</author>
</authors>
<marker>Ringger, McClanahan, Haertel, Busby, Carmen, Carroll, Seppi, Lonsdale, </marker>
<rawString>Ringger, E., McClanahan, P., Haertel, R., Busby, G., Carmen, M., Carroll, J., Seppi, K., &amp; Lonsdale, D.</rawString>
</citation>
<citation valid="true">
<title>Active Learning for Part-of-Speech Tagging: Accelerating Corpus Annotation</title>
<date>2007</date>
<booktitle>ACL Linguistic Annotation Workshop (LAW</booktitle>
<location>Prague, Czech Republic</location>
<marker>2007</marker>
<rawString>(2007). “Active Learning for Part-of-Speech Tagging: Accelerating Corpus Annotation”. ACL Linguistic Annotation Workshop (LAW) 2007.  Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Seung</author>
<author>M Opper</author>
<author>H Sompolinsky</author>
</authors>
<title>Query by committee</title>
<date>1992</date>
<journal>COLT. Pp</journal>
<pages>287--294</pages>
<contexts>
<context> Several heuristic AL methods have been investigated for determining which data will provide the most information and hopefully the best accuracy. Perhaps the best known are Query by Committee (QBC) (Seung, Opper, and Sompolinsky, 1992) and Uncertainty Sampling (or Query by Uncertainty, QBU) (Thrun and Moeller, 1992). A second aspect of the focus question is: at which granularity should the annotators direct their efforts? This pap</context>
</contexts>
<marker>Seung, Opper, Sompolinsky, 1992</marker>
<rawString>Seung, H., Opper, M., &amp; Sompolinsky, H. (1992). “Query by committee”.  COLT. Pp. 287-294.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Thrun</author>
<author>K Moeller</author>
</authors>
<title>Active exploration in dynamic environments.” NIPS</title>
<date>1992</date>
<contexts>
<context>he most information and hopefully the best accuracy. Perhaps the best known are Query by Committee (QBC) (Seung, Opper, and Sompolinsky, 1992) and Uncertainty Sampling (or Query by Uncertainty, QBU) (Thrun and Moeller, 1992). A second aspect of the focus question is: at which granularity should the annotators direct their efforts? This paper focuses on this particular aspect and describes a user study designed specifica</context>
</contexts>
<marker>Thrun, Moeller, 1992</marker>
<rawString>Thrun S., and Moeller, K. (1992). “Active exploration in dynamic environments.” NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tomanek</author>
<author>J Wermter</author>
<author>U Hahn</author>
</authors>
<title>An Approach to Text Corpus Construction which Cuts Annotation Costs and Maintains Reusability of An-notated Data.” EMNLP</title>
<date>2007</date>
<pages>486--495</pages>
<contexts>
<context>tances in the data should the annotators focus? This is the question addressed by active learning. Active Learning (AL) can be employed to reduce the costs of corpus annotation (Ringger et al., 2007; Tomanek, et al., 2007; Engelson &amp; Dagan, 1996). Our previous work (Ringger et al., 2007) demonstrates that by applying active learning techniques, a state of the art tagging model can be trained on as little as one-half o</context>
</contexts>
<marker>Tomanek, Wermter, Hahn, 2007</marker>
<rawString>Tomanek, K., Wermter, J., &amp; Hahn, U. (2007). “An Approach to Text Corpus Construction which Cuts Annotation Costs and Maintains Reusability of An-notated Data.” EMNLP. Pp. 486-495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C Manning</author>
</authors>
<title>Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger.” EMNLP</title>
<date>2000</date>
<pages>63--70</pages>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Toutanova, K. &amp; Manning, C. (2000). “Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger.” EMNLP. Pp. 63-70.</rawString>
</citation>
</citationList>
</algorithm>

