<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Wolfgang F¨orstner</author>
</authors>
<title>10 pros and cons against performance characterization of vision algorithms</title>
<date>1996</date>
<booktitle>In Workshop on Performance Characteristics of Vision Algorithms</booktitle>
<marker>F¨orstner, 1996</marker>
<rawString>Wolfgang F¨orstner. 1996. 10 pros and cons against performance characterization of vision algorithms. In Workshop on Performance Characteristics of Vision Algorithms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Galliano</author>
<author>Edouard Geoffrois</author>
<author>Djamel Mostefa</author>
<author>Khalid Choukri</author>
<author>Jean-Franc¸ois Bonastre</author>
<author>Guillaume Gravier</author>
</authors>
<title>The ESTER phase II evaluation campaign for the rich transcription of French broadcast news</title>
<date>2005</date>
<booktitle>In European Conference on Speech Communication and Technology (Interspeech-Eurospeech</booktitle>
<contexts>
<context>d steering evaluation-oriented programs for both human language and image processing technologies (Technolangue1 and Techno-Vision1, Quaero2) and campaigns withing these programs (Technolangue/ESTER (Galliano et al., 2005), Techno-Vision/RIMES (Grosicki et al., 2006)), as well as on many discussions with actors involved in well established international campaigns such as those organized by NIST or CLEF (Cross-Language</context>
</contexts>
<marker>Galliano, Geoffrois, Mostefa, Choukri, Bonastre, Gravier, 2005</marker>
<rawString>Sylvain Galliano, Edouard Geoffrois, Djamel Mostefa, Khalid Choukri, Jean-Franc¸ois Bonastre, and Guillaume Gravier. 2005. The ESTER phase II evaluation campaign for the rich transcription of French broadcast news. In European Conference on Speech Communication and Technology (Interspeech-Eurospeech).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanu`ele Grosicki</author>
<author>Edouard Geoffrois</author>
<author>Matthieu Carr´e</author>
<author>Emmanuel Augustin</author>
<author>Franc¸oise Prˆeteux</author>
</authors>
<title>La campagne d’´evaluation RIMES pour la reconnaissance de courriers manuscrits</title>
<date>2006</date>
<booktitle>In Colloque International Francophone sur l’Ecrit et le Document (CIFED</booktitle>
<marker>Grosicki, Geoffrois, Carr´e, Augustin, Prˆeteux, 2006</marker>
<rawString>Emmanu`ele Grosicki, Edouard Geoffrois, Matthieu Carr´e, Emmanuel Augustin, and Franc¸oise Prˆeteux. 2006. La campagne d’´evaluation RIMES pour la reconnaissance de courriers manuscrits. In Colloque International Francophone sur l’Ecrit et le Document (CIFED).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Henry S Thompson</author>
</authors>
<title>Overview of evaluation in speech and natural language processing</title>
<date>1996</date>
<booktitle>Survey of the State of the Art in Human Language Technology</booktitle>
<editor>In R. Cole et al., editor</editor>
<publisher>Cambridge University Press</publisher>
<contexts>
<context>ward a common challenge. This is useful to increase visibility, organize a community, foster exchanges, create emulation and encourage innovation. Evaluation campaigns can also have some limitations (Hirschman and Thompson, 1996) or imply some constraints which can be seen as negative aspects. For example, the research teams have to synchronize their work, both in terms of calendar and in terms of tasks under study, which ca</context>
</contexts>
<marker>Hirschman, Thompson, 1996</marker>
<rawString>Lynette Hirschman and Henry S. Thompson. 1996. Overview of evaluation in speech and natural language processing. In R. Cole et al., editor, Survey of the State of the Art in Human Language Technology. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles I Jones</author>
</authors>
<title>Introduction to Economic</title>
<date>2001</date>
<note>2nd edition</note>
<marker>Jones, 2001</marker>
<rawString>Charles I. Jones. 2001. Introduction to Economic Growth. W. W. Norton, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alvin F Martin</author>
<author>John S Garofolo</author>
<author>Jonathan C Fiscus</author>
<author>Audrey N Le</author>
<author>David S Pallett</author>
<author>Mark A Przybocki</author>
<author>Gregory A Sanders</author>
</authors>
<title>NIST language technology evaluation cookbook</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context>s, and gather for a debriefing workshop. This methodology is more and more widely accepted for evaluating various types of human language technologies, much experience has been gained over the years (Martin et al., 2004), and evaluation campaigns are organized by an increasing number of organizations in various countries. It also gradually extends to neighboring domains such as image processing. However, in comparis</context>
</contexts>
<marker>Martin, Garofolo, Fiscus, Le, Pallett, Przybocki, Sanders, 2004</marker>
<rawString>Alvin F. Martin, John S. Garofolo, Jonathan C. Fiscus, Audrey N. Le, David S. Pallett, Mark A. Przybocki, and Gregory A. Sanders. 2004. NIST language technology evaluation cookbook. In Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger K Moore</author>
</authors>
<title>The NATO research study group on speech processing: RSG10</title>
<date>1986</date>
<booktitle>In Proc. Speech Tech 86 (Media Dimensions</booktitle>
<pages>201--203</pages>
<contexts>
<context>rong implication in term of research policy and public funding. 1. Introduction Evaluation campaigns were introduced in the field of speech and natural language processing more than twenty years ago (Moore, 1986; Pallett, 2003). In this framework, several research teams agree on common evaluation protocols, simultaneously submit the outputs of their systems for scoring according to these protocols, and gathe</context>
</contexts>
<marker>Moore, 1986</marker>
<rawString>Roger K. Moore. 1986. The NATO research study group on speech processing: RSG10. In Proc. Speech Tech 86 (Media Dimensions), pages 201–203, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David S Pallett</author>
</authors>
<title>A look at NIST’s benchmark ASR tests: past, present, and future</title>
<date>2003</date>
<booktitle>In IEEE workshop on Automatic Speech Recognition and Understanding (ASRU</booktitle>
<pages>483--488</pages>
<contexts>
<context>ion in term of research policy and public funding. 1. Introduction Evaluation campaigns were introduced in the field of speech and natural language processing more than twenty years ago (Moore, 1986; Pallett, 2003). In this framework, several research teams agree on common evaluation protocols, simultaneously submit the outputs of their systems for scoring according to these protocols, and gather for a debrief</context>
</contexts>
<marker>Pallett, 2003</marker>
<rawString>David S. Pallett. 2003. A look at NIST’s benchmark ASR tests: past, present, and future. In IEEE workshop on Automatic Speech Recognition and Understanding (ASRU), pages 483–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Przybocki</author>
<author>Gregory Sanders</author>
<author>Audrey Le</author>
</authors>
<date>2006</date>
<contexts>
<context>ey have to evaluate, are based on software. Once they are designed, the level of human intervention is limited, even though there are exception such as for the edit distance used in the GALE project (Przybocki et al., 2006). The measurement tools are thus costly to develop but not to duplicate. This implies that there is not only a scientific need but also a strong economic benefit in sharing them. Furthermore, when th</context>
</contexts>
<marker>Przybocki, Sanders, Le, 2006</marker>
<rawString>Mark Przybocki, Gregory Sanders, and Audrey Le. 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edit distance</author>
</authors>
<title>A metric for machine translation evaluation</title>
<booktitle>In International Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>2038--2043</pages>
<marker>distance, </marker>
<rawString>Edit distance: A metric for machine translation evaluation. In International Conference on Language Resources and Evaluation (LREC), pages 2038–2043.</rawString>
</citation>
</citationList>
</algorithm>

