F. J. Och. 2003. Minimum error rate training for sta-
J03-1002 P03-1021
algorithms (Gimpel and Smith, 2009). Cube sum-
D09-1086
K. Gimpel and N. A. Smith. 2008. Rich source-
W08-0302
D. Chiang. 2005. A hierarchical phrase-based model
P05-1033
F. C. N. Pereira and Y. Schabes. 1992. Inside-outside
P92-1017
L. Shen, J. Xu, and R. Weischedel. 2008. A new
P08-1066
H. Mi, L. Huang, and Q. Liu. 2008. Forest-based
P08-1023
F. J. Och and H. Ney. 2002. Discriminative train-
P02-1038
S. Banerjee and A. Lavie. 2005. METEOR: An au-
W05-0909
et al., 2003; Yamada and Knight, 2001). Hence a
P01-1067
D. Klein and C. D. Manning. 2004. Corpus-based
P04-1061
D.Marcu,W.Wang,A.Echihabi,andK.Knight. 2006.
W06-1606
patterns(Liangetal.,2006; Blunsometal.,2008). Thisoffers
D08-1023 P08-1024
are usually heuristic and inconsistent (Koehn et al., 2003).
N03-1017
et al., 2000; Ding and Palmer, 2005). Smith and
P05-1067
K. Yamada and K. Knight. 2001. A syntax-based sta-
P01-1067
ing models (Koehn et al., 2007). In syntax-based
P07-2045
rithms in this paper; see Eisner et al. (2005).
H05-1036
of target-side syntax (Galley et al., 2006; Marcu
P06-1121
J. Eisner, E. Goldlust, and N. A. Smith. 2005. Com-
H05-1036
(Smith and Eisner, 2009), and various monolingual recog-
D09-1086
rithm (Chiang, 2007; Gimpel and Smith, 2009).
D09-1086
(Koehn et al., 2003); they can overlap.5 Addi-
N03-1017
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
P05-1034
M. Wang, N. A. Smith, and T. Mitamura. 2007. What
D07-1003
D. Das and N. A. Smith. 2009. Paraphrase identifica-
D09-1086
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
J03-1002 N03-1017 P03-1021
with gaps (Chiang, 2005; Ittycheriah and Roukos,
P05-1033
pendency model of Klein and Manning (2004).
P04-1061
tems such as Moses (Koehn et al., 2007) explic-
P07-2045
is fixed (Pereira and Schabes, 1992). Let S(j,i,t)
P92-1017
F. J. Och and H. Ney. 2003. A systematic comparison
J03-1002 P03-1021
machinetranslationaswell(Blunsometal.,2008).
D08-1023 P08-1024
2006; Wang et al., 2007). It runs in O(mn2) time
D07-1003
nition and scoring tasks (Wang et al., 2007; Das and Smith,
D07-1003
Y. Ding and M. Palmer. 2005. Machine translation us-
P05-1067
P. Blunsom and M. Osborne. 2008. Probabilistic infer-
D08-1023 P08-1024
ing GIZA++ (Och and Ney, 2003), symmetrize
J03-1002 P03-1021
than a channel model (Brown et al., 1993). This
J93-2003
tures are possible. For example, Quirk et al.(2005) use features involving phrases and source-
P05-1034
H. Alshawi, S. Bangalore, and S. Douglas. 2000.
J00-1004
mum error-rate training (MERT; Och, 2003); it is
J03-1002 P03-1021
P. Blunsom, T. Cohn, and M. Osborne. 2008. A dis-
D08-1023 P08-1024
D. A. Smith and J. Eisner. 2009. Parser adaptation
D09-1086
language model‚Äù features (Shen et al., 2008).
P08-1066
etal., 1997; OchandNey, 2002)thatencodesmost
P02-1038
els (Koehn et al., 2003) and lexicalized reorder-
N03-1017
A. Ittycheriah and S. Roukos. 2007. Direct translation
N07-1008
side dependency trees and Mi et al. (2008) use
P08-1023
D. Chiang, Y. Marton, and P. Resnik. 2008. On-
D08-1024
T. Koo and M. Collins. 2005. Hidden-variable models
H05-1064
K. Gimpel and N. A. Smith. 2009. Cube summing,
D09-1086
els (Gimpel and Smith, 2008; Haque et al., 2009;
W08-0302 D09-1086
et al., 2006; Shen et al., 2008). In addition, re-
P08-1066
Chiang et al., 2008).
D08-1024
probability estimates (Och and Ney, 2002). These estimates
P02-1038
