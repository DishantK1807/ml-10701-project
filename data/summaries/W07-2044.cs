Nature of Target Words Leacock et al.(1998), for example, observed that ?the benefits of adding topical to local context alone depend on syntactic category as well as on the characteristics of the individual word??
J98-1006
Mihalcea, 2002; Escudero et al., 2004).
W04-0807
OntoNotes (Hovy et al., 2006) is a project that has annotated several layers of semantic information ??including word senses, at a high inter-annotator agreement of over 90%.
N06-2015
For instance, in the OntoNotes project (Hovy et al., 2006) senses are grouped until a 90% inter-annotator agreement is achieved.
N06-2015
subcategorization, lexical classes) can now be acquired automatically from parsed data (McCarthy and Carroll, 2003; Schulte im Walde, 2006; Preiss et al., 2007).
W07-2009
(Leacock et al., 1998) demonstrated the use of related monosemous words (monosemous relatives) to collect examples for a given sense from the Internet.
J98-1006
McCarthy. 2007.
W07-2009
First, two estimates of importance on words have been used very successfully both in generic and query-focused summarization: frequency (Luhn, 1958; Nenkova et al., 2006; Vanderwende et al., 2006) and loglikelihood ratio (Lin and Hovy, 2000; Conroy et al., 2006; Lacatusu et al., 2006).
N06-2015
As pointed out by Mihalcea et al.(2004), among the participating systems in the SENSEVAL-3 English lexical sample task, ?several of the top performance systems are based on combination of multiple classifiers, which shows once again that voting scheme that combine several learning algorithms outperform the accuracy of individual classifiers??
W04-0807
TSWEB2: Inspired by the work of (Leacock et al., 1998), these Topic Signatures were constructed using monosemous relatives from WordNet (synonyms, hypernyms, direct and indirect hyponyms, and siblings), querying Google and retrieving up to one thousand snippets per query (that is, a word sense), extracting the words with distinctive frequency using TFIDF.
J98-1006
of Prepositions The Word Sense Disambiguation of Prepositions Task (Litkowski and Hargraves, 2007), provided training and test data for sense disambiguation of 34 prepositions.
W07-2005
The texts come from the WallStreetJournalcorpus, andwerehand-annotated with OntoNotes senses (Hovy et al., 2006).
N06-2015
(Yuret, 2004) observed that approximately half of the test instances do not match any of the contextual features learned from the training data for an all words disambiguation task.
W04-0864
This is because identical words occur more often in coordinate head words than in other lexical dependencies (there were 43 pairs with identical words in the coordination set, compared to 3 such pairs in the 150 SimTest ncoord xcoord SDcoord nnonCoord xnonCoord SDnonCoord 95% CI p-value coordDistrib 503 0.11 0.13 485 0.06 0.09 [0.04 0.07] 0.000 (Resnik, 1995) 444 3.19 2.33 396 2.43 2.10 [0.46 1.06] 0.000 (Lin, 1998) 444 0.27 0.26 396 0.19 0.22 [0.04 0.11] 0.000 (Jiang and Conrath, 1997) 444 0.13 0.65 395 0.07 0.08 [-0.01 0.11] 0.083 (Wu and Palmer, 1994) 444 0.63 0.19 396 0.55 0.19 [0.06 0.11] 0.000 (Leacock and Chodorow, 1998) 444 1.72 0.51 396 1.52 0.47 [0.13 0.27] 0.000 (Hirst and St-Onge, 1998) 459 1.599 2.03 447 1.09 1.87 [0.25 0.76] 0.000 (Banerjee and Pedersen, 2003) 451 114.12 317.18 436 82.20 168.21 [-1.08 64.92] 0.058 (Patwardhan and Pedersen, 2006) 459 0.67 0.18 447 0.66 0.2 [-0.02 0.03] 0.545 random 483 0.89 0.17 447 0.88 0.18 [-0.02 0.02] 0.859 Table 1: Summary statistics for 9 different word similarity measures (plus one random measure):ncoord and nnonCoord are the sample sizes for the coordinate and non-coordinate noun pairs samples, respectively; xcoord, SDcoord and xnonCoord, SDnonCoord are the sample means and standard deviations for the two sets.
J98-1006
On average there were 223 training and 49 testing instances for each word tagged with an OntoNote sense tag (Hovy et al., 2006).
N06-2015
Hovy. 2006.
N06-2015
In English, the OntoNotes project (Hovy et al., 2006) is combining sense tags with the Penn treebank.
N06-2015
The best system in Senseval-3 (Mihalcea et al., 2004; Grozea, 2004) achieved 72.9% fine grained, 79.3% coarse grained accuracy.
W04-0807 W04-0831
Lexical Substitution The English Lexical Substitution Task (McCarthy and Navigli, 2007), for both human annotators and systems is to replace a target word in a sentence with as close a word as possible.
W07-2009
The details of the subtasks and scoring can be found in (McCarthy and Navigli, 2007).
W07-2009
Overall, however, this data indicates that the approach suggested by (Palmer, 2000) and that is being adopted in the ongoing OntoNotes project (Hovy et al., 2006) does result in higher system performance.
N06-2015
The data set and the results we present are from the Semeval task, WSD of Preposition (Litkowski 2007).
W07-2005 W07-2005
objective ??have attracted considerable attention in the recent years (Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006; Turney and Littman, 2003; Yu and Hatzivassiloglou, 2003).
N06-2015
Leacock and Chodorow (1998)?s measure takes into account the path length between two concepts, which is scaled by the depth of the hierarchy in which they reside.
J98-1006
I used a smoothing method loosely based on the one-count method given in (Chen and Goodman, 1996).
P96-1041
Litkowski CL Research 9208 Gue Road Damascus, MD 20872 ken@clres.com Abstract In SemEval-2007, CL Research participated in the task for Frame Semantic Structure Extraction.
W07-2005
The top part of Table 1 gives the breakdown of the best score, see (McCarthy and Navigli, 2007) for details.
W07-2009
