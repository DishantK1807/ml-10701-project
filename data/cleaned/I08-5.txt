1:285	IJCNLP-08Workshop On NERforSouthandSouth EastAsianLanguages ProceedingsoftheWorkshop 12January2008 IIIT, Hyderabad,India c2008 Asian Federation of Natural Language Processing ii Introduction Welcome to the IJCNLP Workshop on Named Entity Recognition for South and South East Asian Languages, a meeting held in conjunction with the Third International Joint Conference on Natural Language Processing at Hyderabad, India.
2:285	The goal of this workshop is to ascertain the state of the art in Named Entity Recognition (NER) specifically for South and South East Asian (SSEA) languages.
3:285	This workshop continues the work started in the NLPAI Machine Learning Contest 2007 which was focused on NER for South Asian languages.
4:285	NER was selected this time for the contest as well as for this workshop because it is one of the fundamental and most important problems in NLP for which systems with good accuracy have not been built so far for SSEA languages.
5:285	The primary reason for this is that the characteristics of SSEA languages relevant for NER are different in many respects from English, on which a lot of work has been done with a significant amount of success in the last few years.
6:285	An introductory article further explains the background of and motivation for this workshop.
7:285	It also presents the results of an experiment on a reasonable baseline and compares the results obtained by the participating teams with the results for this baseline.
8:285	The workshop had two tracks: One track for regular research papers on NER for SSEA languages and the second track on the lines of a shared task.
9:285	The workshop attracted a lot of interest, especially from the South Asian region.
10:285	Participation from most of the research centers in South Asia working on NER ensured that the workshop met its goal of ascertaining and advancing the state of the art in NER for SSEA languages.
11:285	Another major achievement was that a good quantity of named entity annotated corpus was created in five South Asian languages.
12:285	The notable point about this effort was that this was done almost informally on a voluntary basis, without funding.
13:285	This is an important point in the context of SSEA languages because lack of annotated corpora has held back progress in many areas of NLP so far in this region.
14:285	Each paper was reviewed by three reviewers to ensure satisfactory quality of the selected papers.
15:285	Anothermajorfeatureoftheworkshopisthatitincludestwoinvitedtalksbyseniorresearchersworking on the NER problem for South Asian languages.
16:285	The only drawback of the workshop was that there was no paper on any South East Asian language.
17:285	We would like to thank the program committee members for all the hard work that they did during the reviewing process.
18:285	We would also like to thank all the people involved in organizing the IJCNLP conference.
19:285	We hope that this workshop will help in creating interest in NER for SSEA languages and we will soon be able to achieve results comparable to those for languages like English.
20:285	Rajeev Sangal, Dipti Misra Sharma and Anil Kumar Singh (Chairs) iii  Organizers: Rajeev Sangal, IIIT, Hyderabad, India Dipti Misra Sharma, IIIT, Hyderabad, Hyderabad, India Anil Kumar Singh, IIIT, Hyderabad, India Program Committee: Rajeev Sangal, IIIT, Hyderabad, India Dekai Wu, The Hong Kong University of Science & Technology, Hong Kong Ted Pedersen, University of Minnesota, USA Dipti Misra Sharma, IIIT, Hyderabad, Hyderabad, India Virach Sornlertlamvanich, TCL, NICT, Thailand Alexander Gelbukh, Center for Computing Research, National Polytechnic Institute, Mexico M. Sasikumar, CDAC, Mumbai, India Sudeshna Sarkar, Indian Institute of Technology, Kharagpur, India Thierry Poibeau, CNRS, France Sobha L., AU-KBC, Chennai, India Tzong-Han Tsai, National Taiwan University, Taiwan Prasad Pingali, IIIT, Hyderabad, India Canasai Kreungkrai, NICT, Japan Manabu Sassano, Yahoo Japan Corporation, Japan Kavi Narayana Murthy, University of Hyderabad, India Sivaji Bandyopadhyay, Jadavpur University, Kolkata, India Anil Kumar Singh, IIIT, Hyderabad, Hyderabad, India Doaa Samy, Universidad Autnoma de Madrid, Spain Ratna Sanyal, IIIT, Allahabad, India V. Sriram, IIIT, Hyderabad, Hyderabad, India Anagha Kulkarni, Carnegie Mellon University, USA Soma Paul, IIIT, Hyderabad, Hyderabad, India Sofia Galicia-Haro, National Autonomous University, Mexico Grigori Sidorov, National Polytechnic Institute, Mexico Special Acknowledgment: Samar Husain, IIIT, Hyderabad, India Harshit Surana, IIIT, Hyderabad, India Invited Speakers: Sobha L., AU-KBC, Chennai, India Sivaji Bandyopadhyay, Jadavpur University, Kolkata, India v  Table of Contents Invited Talk: Named Entity Recognition: Different Approaches Sobha L  1 Invited Talk: Multilingual Named Entity Recognition Sivaji Bandyopadhyay  3 Named Entity Recognition for South and South East Asian Languages: Taking Stock Anil Kumar Singh  5 A Hybrid Named Entity Recognition System for South and South East Asian Languages Sujan Kumar Saha, Sanjay Chatterji, Sandipan Dandapat, Sudeshna Sarkar and Pabitra Mitra . 17 Aggregating Machine Learning and Rule Based Heuristics for Named Entity Recognition Karthik Gali, Harshit Surana, Ashwini Vaidya, Praneeth Shishtla and Dipti Misra Sharma25 Language Independent Named Entity Recognition in Indian Languages Asif Ekbal, Rejwanul Haque, Amitava Das, Venkateswarlu Poka and Sivaji Bandyopadhyay  33 Named Entity Recognition for Telugu P Srikanth and Kavi Narayana Murthy41 Bengali Named Entity Recognition Using Support Vector Machine Asif Ekbal and Sivaji Bandyopadhyay  51 Domain Focused Named Entity Recognizer for Tamil Using Conditional Random Fields Vijayakrishna R and Sobha L  59 A Character n-gram Based Approach for Improved Recall in Indian Language NER Praneeth M Shishtla, Prasad Pingali and Vasudeva Varma67 An Experiment on Automatic Detection of Named Entities in Bangla Bidyut Baran Chaudhuri and Suvankar Bhattacharya75 Hybrid Named Entity Recognition System for South and South East Asian Languages Praveen P and Ravi Kiran V  83 Named Entity Recognition for South Asian Languages Amit Goyal  89 Named Entity Recognition for Indian Languages Animesh Nayan, B. Ravi Kiran Rao, Pawandeep Singh, Sudip Sanyal and Ratna Sanyal  97 Experiments in Telugu NER: A Conditional Random Field Approach Praneeth M Shishtla, Karthik Gali, Prasad Pingali and Vasudeva Varma105 vii viii Workshop Program Saturday, January 12, 2008 Session 1: 09:00-9:30 Opening Remarks: Named Entity Recognition for South and South East Asian Languages: Taking Stock Anil Kumar Singh 09:30-10:00 Invited Talk: Named Entity Recognition: Different Approaches Sobha L 10:00-10:30 A Hybrid Named Entity Recognition System for South and South East Asian Languages Sujan Kumar Saha, Sanjay Chatterji, Sandipan Dandapat, Sudeshna Sarkar and Pabitra Mitra 10:30-11:00 Break Session 2: 11:00-11:30 Invited Talk: Multilingual Named Entity Recognition Sivaji Bandyopadhyay 11:30-12:00 Aggregating Machine Learning and Rule Based Heuristics for Named Entity Recognition Karthik Gali, Harshit Surana, Ashwini Vaidya, Praneeth Shishtla and Dipti Misra Sharma 12:00-12:30 Language Independent Named Entity Recognition in Indian Languages Asif Ekbal, Rejwanul Haque, Amitava Das, Venkateswarlu Poka and Sivaji Bandyopadhyay 12:30-14:00 Lunch Session 3: 14:00-14:30 Named Entity Recognition for Telugu P Srikanth and Kavi Narayana Murthy 14:30-15:00 Poster Display and Discussion An Experiment on Automatic Detection of Named Entities in Bangla Bidyut Baran Chaudhuri and Suvankar Bhattacharya ix Saturday, January 12, 2008 (continued) Hybrid Named Entity Recognition System for South and South East Asian Languages Praveen P and Ravi Kiran V Named Entity Recognition for South Asian Languages Amit Goyal Named Entity Recognition for Indian Languages Animesh Nayan, B. Ravi Kiran Rao, Pawandeep Singh, Sudip Sanyal and Ratna Sanyal Experiments in Telugu NER: A Conditional Random Field Approach Praneeth M Shishtla, Karthik Gali, Prasad Pingali and Vasudeva Varma 15:30-16:00 Break Session 4: 16:00-16:30 Bengali Named Entity Recognition Using Support Vector Machine Asif Ekbal and Sivaji Bandyopadhyay 16:30-17:00 Domain Focused Named Entity Recognizer for Tamil Using Conditional Random Fields Vijayakrishna R and Sobha L 17:00-17:30 A Character n-gram Based Approach for Improved Recall in Indian Language NER Praneeth M Shishtla, Prasad Pingali and Vasudeva Varma 17:30-18:00 Closing Discussion x Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 12, Hyderabad, India, January 2008.
21:285	c2008 Asian Federation of Natural Language ProcessingNamed Entity Recognition: Different Approaches Sobha, L AU-KBC Research Centre MIT Campus of Anna University Chennai-44 sobha@aukbc.org  Abstract The talk deals with different approaches used for Named Entity recognition and how they are used in devloping arobust Named Entiy Recognizer.
22:285	Th talk includes th devlopment oftagset for NER and manual annotation of text.
23:285	1  2 Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 34, Hyderabad, India, January 2008.
24:285	c2008 Asian Federation of Natural Language Processing Multilingual Named Entity Recognition Sivaji Bandyopadhyay Computer Science & Engineering Department Jadavpur University Kolkata, INDIA.
25:285	sbandyopadhyay@cse.jdvu.ac.in     Abstract The computational research aiming at automatically identifying named entities (NE) in texts forms a vast and heterogeneous pool of strategies, techniques and representations from hand-crafted rules towards machine learning approaches.
26:285	Hand-crafted rule based systems provide good performance at a relatively high system engineering cost.
27:285	The availability of a large collection of annotated data is the prerequisite for using supervised learning techniques.
28:285	Semi-supervised and unsupervised learning techniques promise fast deployment for many NE types without the prerequisite of an annotated corpus.
29:285	The main technique for semi-supervised learning is called bootstrapping and involves a small degree of supervision, such as a set of seeds, for starting the learning process.
30:285	The typical approach in unsupervised learning is clustering where systems can try to gather NEs from clustered groups based on the similarity of context.
31:285	The techniques rely on lexical resources (e.g., Wordnet), on lexical patterns and on statistics computed on a large unannotated corpus.
32:285	In multilingual named entity recognition (NER), it must be possible to use the same method for many different languages and the extension to new languages must be easy and fast.
33:285	Person names can be recognized in text through a lookup procedure, by analyzing the local lexical context, by looking at part of a sequence of candidate words that is a known name component etc. Some organization names can be identified by looking at contain organization-specific candidate words.
34:285	Identification of place names necessarily involves lookup against a gazetteer, as most context markers are too weak and ambiguous.
35:285	An important feature in multilingual person name detection is that the same person can be referred to by different name variants.
36:285	The main reasons for these variations are: the reuse of name parts to avoid repetition, morphological variants such as the added suffixes, spelling mistakes, adaptation of names to local spelling rules, transliteration differences due to different transliteration rules or different target languages etc Name variants can be found within the same language documents.
37:285	The major challenges for looking up place names in a multilingual gazetteer are the following: place names are frequently homographic with common words or with person names, presence of a number of exonyms (foreign language equivalences), endonyms (local variants) and historical variants for many place names etc Application of NER to multilingual document sets helps to find more and more accurate informa-tion on each NE, while at the same time rich in-formation about NEs is helpful and can even be a crucial ingredient for text analysis applications that cross the language barrier.
38:285	3  4 Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 516, Hyderabad, India, January 2008.
39:285	c2008 Asian Federation of Natural Language Processing Named Entity Recognition for South and South East Asian Languages: Taking Stock Anil Kumar Singh Language Technologies Research Centre IIIT, Hyderabad, India anil@research.iiit.ac.in Abstract In this paper we first present a brief discussion of the problem of Named Entity Recognition (NER) in the context of the IJCNLP workshop on NER for South and South East Asian (SSEA) languages1.
40:285	We also present a short report on the development of a named entity annotated corpus in five South Asian language, namely Hindi, Bengali, Telugu, Oriya and Urdu.
41:285	We present some details about a new named entity tagset used for this corpus and describe the annotation guidelines.
42:285	Since the corpus was used for a shared task, we also explain the evaluation measures used for the task.
43:285	We then present the results of our experiments on a baseline which uses a maximum entropy based approach.
44:285	Finally, we give an overview of the papers to be presented at the workshop, including those from the shared task track.
45:285	We discuss the results obtained by teams participating in the task and compare their results with the baseline results.
46:285	1 Introduction One of the motivations for organizing a workshop (NERSSEAL-08) focused on named entities (NEs) was that they have a special status in Natural Language Processing (NLP) because they have some properties which other elements of human languages do not have, e.g. they refer to specific things or concepts in the world and are not listed in the grammars 1http://ltrc.iiit.ac.in/ner-ssea-08 or the lexicons.
47:285	Identifying and classifying them automatically can help us in processing text because they form a significant portion of the types and tokens occurring in a corpus.
48:285	Also, because of their very nature, machine learning techniques have been found to be very useful in identifying them.
49:285	In order to use these machine learning techniques, we need corpus annotated with named entities.
50:285	In this paper we describe such a corpus developed for five South Asian languages.
51:285	These languages are Hindi, Bengali, Oriya, Telugu and Urdu.
52:285	This paper also presents an overview of the work done for the IJCNLP workshop on NER for SSEA languages.
53:285	The workshop included two tracks.
54:285	The first track was for regular research papers, while the second was organized on the lines of a shared task.
55:285	Fairly mature named entity recognition systems are now available for European languages (Sang, 2002; Sang and De Meulder, 2003), especially English, and even for East Asian languages (Sassano and Utsuro, 2000).
56:285	However, for South and South East Asian languages, the problem of NER is still far from being solved.
57:285	Even though we can gain much insight from the methods used for English, there are many issues which make the nature of the problem different for SSEA languages.
58:285	For example, these languages do not have capitalization, which is a major feature used by NER systems for European languages.
59:285	Another characteristic of these languages is that most of them use scripts of Brahmi origin, which have highly phonetic characteristics that could be utilized for multilingual NER.
60:285	For some languages, there are additional issues like word segmentation 5 (e.g. for Thai).
61:285	Large gazetteers are not available for most of these languages.
62:285	There is also the problem of lack of standardization and spelling variation.
63:285	The number of frequently used words (common nouns) which can also be used as names (proper nouns) is very large for, unlike for European languages where a larger proportion of the first names are not used as common words.
64:285	For example, Smith, John, Thomas and George etc. are almost always used as person names, but Anand, Vijay, Kiran and even Manmohan can be (more than often) used as common nouns.
65:285	And the frequency with which they can be used as common nouns as against person names is more or less unpredictable.
66:285	The context might help in disambiguating, but this issue does make the problem much harder than for English.
67:285	Among other problems, one example is that of the various ways of representing abbreviations.
68:285	Because of the alpha-syllabic nature of the SSEA scripts, abbreviation can be expressed through a sequence of letters or syllables.
69:285	In the latter case, the syllables are often combined together to form a pseudo-word, e.g. BAjapA (bhaajapaa) for Bharatiya Janata Party or BJP.
70:285	But most importantly, there is a serious lack of labeled data for machine learning.
71:285	As part of this workshop, we have tried to prepare some data but we will need much more data for really accurate NER systems.
72:285	Since most of the South and South East Asian languages are scarce in resources as well as tools, it is very important that good systems for NER be available, because many problems in information extraction and machine translation (among others) are dependent on accurate NER.
73:285	The need for a workshop specifically for SSEA languages was felt because the South and South East Asian region has many major and numerous minor languages.
74:285	In terms of the number of speakers there are at least four in any list of top ten languages of the world.
75:285	For practical reasons, we focus only on the major languages in the workshop (and in this paper).
76:285	Most of the major languages belong to two families: Indo-European and Dravidian.
77:285	There are a lot of differences among these languages, but there are a lot of similarities too, even across families (Emeneau, 1956; Emeneau, 1980).
78:285	For the reasons mentioned above, NER is perhaps more difficult for SSEA languages than for European languages.
79:285	For better or for worse, there too many languages and too few resources.
80:285	Moreover, these languages are also comparatively less studied by researchers.
81:285	However, we can benefit from the similarities across these languages to build multilingual systems so as to reduce the overall cost and effort required.
82:285	All the issues mentioned above show that we might need different methods for solving the NER problem for SSEA languages.
83:285	However, for comparing the results of these different methods, we will need a reasonably good baseline.
84:285	A mature system tuned for English but trained on SSEA language data can become such a baseline.
85:285	We will describe such a baseline in a later section.
86:285	This baseline system has been tested on the data provided for the shared task.
87:285	We present the results for all five languages under the settings required for the shared task.
88:285	2 Related Work Various techniques have been used for solving the NER problem (Mikheev et al., 1999; Borthwick, 1999; Cucerzan and Yarowsky, 1999; Chieu and Ng, 2003; Klein et al., 2003; Kim and Woodland, 2000) ranging from naively using gazetteers to rules based techniques to purely statistical techniques, even hybrid approaches.
89:285	Several workshops consisting of shared tasks (Sang, 2002; Sang and De Meulder, 2003) have been held with specific focus on this problem.
90:285	In this section we will mention some of techniques used previously.
91:285	Most of the approaches can be classified based on the features they use, whether they are rule based or machine learning based or hybrid approaches.
92:285	Some of the commonly used features are:  Word form and part of speech (POS) tags  Orthographic features like capitalization, decimal, digits  Word type patterns  Conjunction of types like capitalization, quotes, functional words etc.  Bag of words  Trigger words like New York City 6 Tag Name Description NEP Person Bob Dylan, Mohandas Gandhi NED Designation General Manager, Commissioner NEO Organization Municipal Corporation NEA Abbreviation NLP, B.J.P. NEB Brand Pepsi, Nike (ambiguous) NETP Title-Person Mahatma, Dr., Mr. NETO Title-Object Pride and Prejudice, Othello NEL Location New Delhi, Paris NETI Time 3rd September, 1991 (ambiguous) NEN Number 3.14, 4,500 NEM Measure Rs.
93:285	4,500, 5 kg NETE Terms Maximum Entropy, Archeology Table 1: The named entity tagset used for the shared task  Affixes like Hyderabad, Rampur, Mehdipatnam, Lingampally  Gazetteer features: class in the gazetteer  Left and right context  Token length, e.g. the number of letters in a word  Previous history in the document or the corpus  Classes of preceding NEs The machine learning techniques tried for NER include the following:  Hidden Markov Models or HMM (Zhou and Su, 2001)  Decision Trees (Isozaki, 2001)  Maximum Entropy (Borthwick et al., 1998)  Support Vector Machines or SVM (Takeuchi and Collier, 2002)  Conditional Random Fields or CRF (Settles, 2004) Different ways of classifying named entities have been used, i.e., there are more than one tagsets for NER.
94:285	For example, the CoNLL 2003 shared task2 had only four tags: persons, locations, organizations 2http://www.cnts.ua.ac.be/conll2003/ner/ and miscellaneous.
95:285	On the other hand, MUC-63 has a near ontology for information extraction purposes.
96:285	In this (MUC-6) tagset, there are three4 main kinds of NEs: ENAMEX (persons, locations and organizations), TIMES (time expressions) and NUMEX (number expresssions).
97:285	There has been some previous work on NER for SSEA languages (McCallum and Li, 2003; Cucerzan and Yarowsky, 1999), but most of the time such work was an offshoot of the work done for European languages.
98:285	Even including the current workshop, the work on NER for SSEA languages is still in the initial stages as the results reported by papers in this workshop clearly show.
99:285	3 A New Named Entity Tagset The tagset being used for the NERSSEAL-08 shared task consists of more tags than the four tags used for the CoNLL 2003 shared task.
100:285	The reason we opted for these tags was that we needed a slightly finer tagset for machine translation (MT).
101:285	The initial aim was to improve the performance of the MT system.
102:285	As annotation progressed, we realized that there were some problems that we had not anticipated.
103:285	Some classes were hard to distinguish in some contexts, making the task hard for annotators and bringing in inconsistencies.
104:285	For example, it was not always clear whether something should be marked as 3http://cs.nyu.edu/cs/faculty/grishman/muc6.html 4http://cs.nyu.edu/cs/faculty/grishman/NEtask20.book 6.html 7 Number or as Measure.
105:285	Similarly for Time and Measure.
106:285	Another difficult class was that of (technical) terms.
107:285	Is agriculture a term or not?
108:285	If no (as most people would say), is horticulture a term or not?
109:285	In fact, Term was the most difficult class to mark.
110:285	An option that we explored was to merge the above mentioned confusable classes and ignore the Term class.
111:285	But we already had a relatively large corpus marked up with these classes.
112:285	If we merged some classes and ignored the Term class (which had a very large coverage and is definitely going to be useful for MT), we would be throwing away a lot of information.
113:285	And we also had some corpus annotated by others which was based on a different tagset.
114:285	So some problems were inevitable.
115:285	Finally, we decided to keep the original tagset, with one modification.
116:285	The initial tagset had only eleven tags.
117:285	The problem was that there was one Title tag but it had two different meanings: Mr. is a Title, but The Seven Year Itch is also a Title.
118:285	This tag clearly needed to be split into two: Title-Person and TitleObject We should mention here that we considered using another tagset developed at AUKBC, Chennai.
119:285	This was based on ENAMEX, TIMEX and NUMEX.
120:285	The total number of tags in this tagset is more than a hundred and it is meant specifically for MT and only for certain domains (health, tourism).
121:285	Moreover, this is a tagset for entities in general, not just named entities.
122:285	The twelve tags in our tagset are briefly explained in Table-1.
123:285	In the next section we mention the constraints under which the annotated corpus was created, using this tagset.
124:285	4 Annotation Constraints The annotated corpus was created under severe constraints.
125:285	The annotation was to be for five languages by different teams, sometimes with very little communication during the process of annotation.
126:285	As a result, there were many logistical problems.
127:285	There were other practical constraints like the fact that this was not a funded project and all the work was mainly voluntary.
128:285	Another major constraint for all the languages except Hindi was time.
129:285	There was not enough time for cross validation as the corpus was required by a deadline.
130:285	To keep annotation reasonably consistent, annotation guidelines were created and a common format was specified.
131:285	5 Annotation Guidelines The annotation guidelines were of two kinds.
132:285	One was meant for preparing training data through manual annotation.
133:285	The other one was meant for preparing reference data as well as for automatic annotation.
134:285	The main guidelines for preparing the training data are as follows:  Specificity: The most important criterion while deciding whether some expression is a named entity or not is to see whether that expression specifies something definite and identifiable as if by a name or not.
135:285	This decision will have to be based on the context.
136:285	For example, aanand (in South Asian languages, where there is no capitalization) is not a named entity in saba aanand hii aanand hai (There is bliss everywhere).
137:285	But it is a named entity in aanand kaa yaha aakhiri saala hai (Anand is in the last year (of his studies)).
138:285	Number, Measure and Term may be seen as exceptions (see below).
139:285	 Maximal Entity: Only the maximal entities have to be annotated for training data.
140:285	Structure of entities will not be annotated by the annotators, even though it has to be learnt by the NER systems.
141:285	For example, One Hundred Years of Solitude has to be annotated as one entity.
142:285	One Hundred is not to be marked as a Number here, nor is One Hundred Years to be made marked as a Measure in this case.
143:285	The purpose of this guideline is to make the task of annotation for several languages feasible, given the constraints.
144:285	 Ambiguity: In cases where an entity can have two valid tags, the more appropriate one is to be used.
145:285	The annotator has to make the decision in such cases.
146:285	It is recommended that the annotation be validated by another person, or even more preferably, two different annotators have to work on the same data independently and inconsistencies have to be resolved by an adjudicator.
147:285	Abbreviation is an exception to the Ambiguity guideline (see below).
148:285	8 Some other guidelines for specific tags are listed below:  Abbreviations: All abbreviations have to be marked as Abbreviations, Even though every abbreviation is also some other kind of named entity.
149:285	For example, APJ is an Abbreviation, but also a Person.
150:285	IBM is also an Organization.
151:285	Such ambiguity cannot be resolved from the context because it is due to the (wrong?)
152:285	assumption that a named entity can have only one tag.
153:285	Multiple annotations were not allowed.
154:285	This is an exception to the third guideline above.
155:285	 Designation and Title-Person: An entity is a Designation if it represents something formal and official status with certain responsibilities.
156:285	If it is just something honorary, then it is a Title-Object.
157:285	For example, Event Coordinator or Research Assistant is a Designation, but Chakravarti or Mahatma are Titles.
158:285	 Organization and Brand: The distinction between these two has to be made based on the context.
159:285	For example, Pepsi could mean an Organization, but it is more likely to mean a Brand.
160:285	 Time and Location: Whether something is to be marked as Time or Location or not is to be decided based on the Specificity guideline and the context.
161:285	 Number, Measure and Term: These three may not be strictly named entities in the way a person name is. However, we have included them because they are different from other words of the language.
162:285	For problems like machine translation, they can be treated like named entities.
163:285	For example, a Term is a word which can be directly translated into some language if we have a dictionary of technical terms.
164:285	Once we know a word is a Term, there is likely to be less ambiguity about the intended sense of the word, unlike for other normal words.
165:285	The second set of guidelines are different from the first set mainly in one respect: the corpus has to be annotated with not just the maximal NEs, but with all levels of NEs, i.e., nested NEs also have to be marked.
166:285	Nested entities were introduced because one of the requirements was that the corpus be useful for building systems which can become parts of a machine translation (MT) system.
167:285	Nested entities can be useful for MT systems because, quite often, parts of the entities can need to be translated, while the others can just be transliterated.
168:285	An example of a nested named entity is Mahatma Gandhi International Hindi University.
169:285	This would be translated in Hindi as mahaatmaa gaandhii antarraashtriya hindii vishvavidyaalaya.
170:285	Only International and University are to be translated, while the other words are to be transliterated.
171:285	The nested named entities in this case are: Mahatma (NETO), Gandhi (NEP), Mahatma Gandhi (NEP), and Mahatma Gandhi International Hindi University (NEO).
172:285	6 Named Entity Annotated Corpus For Hindi, Oriya and Telugu, all the annotation was performed at IIIT, Hyderabad.
173:285	For Bengali, the corpus was developed at IIIT, Hyderabad and Jadavpur University (Ekbal and Bandyopadhyay, 2008b), Calcutta.
174:285	For Urdu, annotation was performed at CRULP, Lahore (Hussain, 2008) and IIIT, Allahabd.
175:285	Even though all the annotation was done by native speakers of respective languages, named entity annotation was a new task for everyone involved.
176:285	This was because of practical constraints as explained in an earlier section.
177:285	The corpus was divided into two parts, one for training and one for testing.
178:285	The testing corpus was annotated with nested named entities, while the training corpus was only annotated with maximal named entities.
179:285	Since different teams were working on different languages, in some cases even the same language, and also because most of the corpus was created on short notice, each team made its own decisions regarding the kind of corpus to be annotated.
180:285	As a result, the characteristics of the corpus differ widely among the five languages.
181:285	The Hindi and Bengali (partly) text that was annotated was from the multilingual comparable corpus known as the CIIL (Central Institute of Indian Languages) corpus.
182:285	The Oriya corpus was part of the Gyan Nidhi corpus.
183:285	9 NE Hindi Bengali Oriya Telugu Urdu Trn Tst Trn Tst Trn Tst Trn Tst Trn Tst NEP 4025 199 1299 728 2079 698 1757 330 365 145 NED 935 61 185 11 67 216 87 77 98 41 NEO 1225 44 264 20 87 200 86 12 155 40 NEA 345 7 111 9 8 20 97 112 39 3 NEB 5 0 22 0 11 1 1 6 9 18 NETP 1 5 68 57 54 201 103 2 36 15 NETO 964 88 204 46 37 28 276 118 4 147 NEL 4089 211 634 202 525 564 258 751 1118 468 NETI 1760 50 285 46 102 122 244 982 279 59 NEN 6116 497 407 144 124 232 1444 391 310 47 NEM 1287 17 352 146 280 139 315 53 140 40 NETE 5658 843 1165 314 5 0 3498 138 30 4 NEs 26432 2022 5000 1723 3381 2421 8178 3153 2584 1027 Words 503179 32796 112845 38708 93173 27007 64026 8006 35447 12805 Sentences 19998 2242 6030 1835 1801 452 5285 337 1508 498 Trn: Training Data, Tst: Testing Data Table 2: Statistics about the corpus: counts of various named entity classes and the size of the corpus as the number of words and the number of sentences.
184:285	Note that the values for the testing part are of nested NEs.
185:285	Also, the number of sentences, especially in the case or Oriya is not accurate because the sentences were not correctly segmented as there was no automatic sentence splitter available for these languages and manual splitting would have been too costly: without much benefit for the NER task.
186:285	Both of these (CIIL and Gyan Nidhi) corpora consist of text from educational books written on various topics for common readers.
187:285	The Urdu text was partly news corpus.
188:285	The same was the case with Telugu, but the text for both these languages included text from other domains too.
189:285	Admittedly, the texts selected for annotation were not the ideal ones.
190:285	For example, many documents had very few named entities.
191:285	Also, the distribution of domains as well as the classes of NEs was not representative.
192:285	The size of the annotated corpora for different languages is also widely varying, with Hindi having the largest corpus and Urdu the smallest.
193:285	However, this corpus is hopefully just a starting point for much more work in the near future.
194:285	Some statistics about the annotated corpus are given in Table-2.
195:285	7 Shared Task In the shared task, the contestants having their own NER systems were given some annotated test data.
196:285	The contestants had the freedom to use any technique for NER, e.g. a purely rule based technique or a purely statistical technique.
197:285	The contestants could build NER systems targeted for a specific language, but they were required to report results for their systems on all the languages for which training data had been provided.
198:285	This condition was meant to provide a somewhat fair ground for comparison of systems, since the amount of training data is different for different languages.
199:285	The data released for the shared task has been made accessible to all for non-profit research word, not just for the shared task participants, with the hope others will contribute in improving this data and adding to it.
200:285	The task in this contest was different in one important way.
201:285	The NER systems also had to identify nested named entities.
202:285	For example, in the sentence The Lal Bahadur Shastri National Academy of Administration is located in Mussoorie, Lal Bahadur Shastri is a Person, but Lal Bahadur Shastri National Academy of Administration is an Organization.
203:285	In this case, the NER systems had to identify both Person and Organization in the given sentence.
204:285	An evaluation script was also provided to evaluate the performance of different systems in a uniform way.
205:285	10 8 Evaluation Measures As part of the evaluation process for the shared task, precision, recall and F-measure had to be calculated for three cases: maximal named entities, nested named entities and lexical matches.
206:285	Thus, there were nine measures of performance:  Maximal Precision: Pm = cmrm  Maximal Recall: Rm = cmtm  Maximal F-Measure: Fm = 2PmRmPm+Rm  Nested Precision: Pn = cnrn  Nested Recall: Rn = cntn  Nested F-Measure: Fn = 2PnRnPn+Rn  Lexical Precision: Pl = clrl  Lexical Recall: Rl = cltl  Lexical F-Measure: Fl = 2PlRlPl+Rl where c is the number of correctly retrieved (identified) named entities, r is the total number of named entities retrieved by the system being evaluated (correct plus incorrect) and t is the total number of named entities in the reference data.
207:285	The participants were encouraged to report results for specific classes of NEs.
208:285	Evaluation was automatic and was against the manually prepared reference data given to the participants.
209:285	An evaluation script for this purpose was also provided.
210:285	This script assumes that there are single test and reference file and the number and order of sentences is the same in both.
211:285	The format accepted by the evaluation script (which was also the format used for annotated data) was explained in an online tutorial5.
212:285	9 Experiments on a Baseline For our baseline experiments, we used an open source implementation of maximum entropy based Natural Languages Processing tools which are part of the OpenNLP6 package.
213:285	This package includes a name finder tool.
214:285	5http://ltrc.iiit.ac.in/ner-ssea-08/NER-SAL-TUT.pdf 6http://opennlp.sourceforge.net/ This name finder was trained for all the twelve classes of NEs and for all the five languages.
215:285	The test data, which was the same as that given to the shared task participants, was run through this name finder.
216:285	Note that this NER tool is tuned for English in terms of the features used, even though it was trained on different SSEA languages in our case.
217:285	Since the goal of the shared task was to encourage investigation of techniques (especially features) specific to the SSEA languages, this fairly mature NER system (for English) could be used as a baseline against which to evaluate systems tuned (or specially designed) for the five South Asian languages.
218:285	The overall results of the baseline experiments are shown in Table-3.
219:285	The performance on specific NE classes is given in Table-4.
220:285	It can be seen from the tables that the results are drastically low in comparison to the state of the art results reported for English.
221:285	These results clearly show that even a machine learning based system cannot be directly used for SSEA languages even when it has been trained with annotated data for these languages.
222:285	In the next section we present a brief overview of the papers selected for the workshop including the shared task papers.
223:285	10 An Overview of the Papers In all, twelve papers were selected for the workshop, out of which four were in the shared task track.
224:285	Saha et al., who were able to achieve the best results in the shared task, describe a hybrid system that applies maximum entropy models, language specific rules, and gazetteers.
225:285	For Hindi, the features they utilized include orthographic features, information about suffixes and prefixes, morphological features, part of speech information, and information about the surrounding words.
226:285	They used rules for numbers, measures and time classes.
227:285	For designation, title-person and some terms (NETE), they built lists or gazetteers.
228:285	They also used gazetteers for person and location.
229:285	They did not use rules or gazetteers for Oriya, Urdu and Telugu.
230:285	To identify some kinds of nested entities, they applied a set of rules.
231:285	Gali et al. also combined machine learning with language specific heuristics.
232:285	In a separate section, they discussed at some length the issues relevant to NER for SSEA languages.
233:285	Some of these have al11 Measure  Precision Recall F-Measure Language  Pm Pn Pl Rm Rn Rl Fm Fn Fl Bengali 50.00 44.90 52.20 07.14 06.90 06.97 12.50 11.97 12.30 Hindi 75.05 73.61 73.99 18.16 17.66 15.53 29.24 28.48 25.68 Oriya 29.63 27.46 48.25 09.11 07.60 12.18 13.94 11.91 19.44 Telugu 00.89 02.83 22.85 00.20 00.67 5.41 00.32 01.08 08.75 Urdu 47.14 43.50 51.72 18.35 16.94 18.94 26.41 24.39 27.73 m: Maximal, n: Nested, l: Lexical Table 3: Results for the experiments on a baseline for the five South Asian languages Bengali Hindi Oriya Telugu Urdu NEP 06.62 26.23 28.48 00.00 04.39 NED 00.00 12.20 00.00 00.00 00.00 NEO 00.00 15.50 03.30 00.00 11.98 NEA 00.00 00.00 00.00 00.00 00.00 NEB NP NP 00.00 00.00 00.00 NETP 00.00 NP 11.62 00.00 00.00 NETO 00.00 05.92 04.08 00.00 00.00 NEL 03.03 44.79 25.49 00.00 40.21 NETI 34.00 47.41 22.38 01.51 38.38 NEN 62.63 62.22 10.65 03.51 09.52 NEM 13.61 24.39 08.03 00.71 07.15 NETE 00.00 00.18 00.00 00.00 00.00 NP: Not present in the reference data Table 4: Baseline results for specific named entity classes (F-Measures for nested lexical match) ready been mentioned, but two others are the agglutinative property of these (especially Dravidian) languages and the low accuracy of available part of speech taggers, particularly for nouns.
234:285	They used a Conditional Random Fields (CRF) based method for machine learning and applied heuristics to take care of the language specific issues.
235:285	They also point out that a very high percentage of NEs in the Hindi corpus were marked as NETE and machine learning failed to take care of this class of NEs.
236:285	This has been validated by our results on the baseline too (Table4) and is understandable because terms are hard to identify even for humans.
237:285	Ekbal et al. also used an approach based on CRFs.
238:285	They also used some language specific features for Hindi and Bengali.
239:285	Srikanth and Murthy describe the results of their experiments on NER using CRFs for Telugu.
240:285	They concentrated only on person, place and organization names and used newspaper text as the corpus.
241:285	In this focused setting, they were able to achieve overall F-measures between 80% and 97% in various experiments.
242:285	Chaudhuri and Bhattacharya also experimented on a news corpus for Bengali using a three stage NER system.
243:285	The three stages were based on an NE dictionary, rules and contextual co-occurrence statistics.
244:285	They only tried to identify the NEs, not classify them.
245:285	For this task, they were able to achieve an overall F-measure of 89.51%.
246:285	Praveen and Ravi Kumar present the results of experiments (as part of the shared task) using two approaches: Hidden Markov Models (HMM) and CRF.
247:285	Surprisingly, they obtained better results with HMM for all the five languages.
248:285	Goyal described experiments using a CRF based model.
249:285	He also used part of speech information.
250:285	He experimented only on Hindi and was able to achieve results above 60%.
251:285	One notable fact about this paper is that it also de12 Language  BL IK IH1 IH2 JU Bengali 12.30 65.96 40.63 39.77 59.39 Hindi 25.68 65.13 50.06 46.84 33.12 Oriya 19.44 44.65 39.04 45.84 28.71 Telugu 08.75 18.74 40.94 46.58 04.75 Urdu 27.73 35.47 43.46 44.73 35.52 Average 18.78 45.99 42.83 44.75 32.30 BL: Baseline, IK: IIT Kharagpur JU: Jadavpur University, Calcutta IH1: Karthik et al., IIIT Hyderabad IH2: Praveen and Ravi Kiran, IIIT Hyderabad Table 5: Comparison of NER systems which participated in the NERSSEAL-08 shared task against a baseline that uses maximum entropy based name finder tuned for English but trained on data from five South Asian languages scribes experiments on the CoNLL 2003 shared task data for English, which shows that the significantly higher results for English are mainly due to the fact that the CoNLL 2003 data is already POS tagged and chunked with high accuracy.
252:285	Goyal was also able to show that capitalization is a major clue for English, either directly or indirectly (e.g., for accurate POS tagging and chunking).
253:285	He also indicated that the characteristics of the Hindi annotated corpus were partly responsible for the low results on Hindi.
254:285	Nayan et al. mainly describe how an NER system can benefit from approximate string matching based on phonetic edit distance, both for a single language (to account for spelling variations) and for crosslingual NER.
255:285	Shishtla et al.256:285	(Experiments in Telugu NER) experimented only on Telugu and used the CoNLL shared task tagset.
257:285	Using a CRF based approach, they were able to achieve an F-measure of 44.91%.
258:285	Ekbal and Bandyopadhyay describe a method based on Support Vector Machines (SVMs) for Bengali NER.
259:285	On a news corpus and with sixteen NE classes, they were able to achieve an F-measure of 91.8%.
260:285	Vijayakrishna and Sobha describe a CRF based system for Tamil using 106 NE classes.
261:285	Their system is a multi-level system which gave an overall F-measure of 80.44%.
262:285	They also mention that their system achieved this level of performance on a domain focused corpus.
263:285	Shishtla et al.264:285	(Character n-gram Based Approach) used a character n-gram based method to identify NEs.
265:285	They experimented on Hindi as well as English and achieved F-measure values up to 45.48% for Hindi and 68.46% for English.
266:285	Apart from the paper presentations, the workshop will also have two invited talks.
267:285	The first one is titled Named Entity Recognition: Different Approaches by Sobha L. and the second one is Multilingual Named Entity Recognition by Sivaji Bandyopadhyay.
268:285	11 Shared Task Results Five teams participated in the shared task.
269:285	However, only four submitted papers for the workshop.
270:285	All the teams tried to combine machine learning with some language specific heuristics, at least for one of the languages.
271:285	The results obtained by the four teams are summarized in Table-5, which shows only the Fmeasure for lexical match.
272:285	It can be seen from the table that all the teams were able to get significantly better results than the baseline.
273:285	Overall, the performance of the IIT Kharagpur team was the best, followed by the two teams from IIIT Hyderabad.
274:285	Even though all the teams obtained results much better than the baseline, it is still quite evident that the state of the art for NER for SSEA languages leaves much to be desired.
275:285	At around 46% maximum F-measure on lexical matching, the results mean that the NER systems built so far for SSEA languages are not quite practically useful.
276:285	But, after this workshop, we at least know where we stand and how far we still have to go.
277:285	However, it may be noted that the conditions for 13 the shared task were very stringent compared to the previous shared tasks on NER, e.g. neither the corpus was tagged with parts of speech or chunks, nor were good POS taggers or chunkers available for the languages involved.
278:285	This indicates that with progress in building better resources and basic tools for these languages, the accuracy of NER systems should also increase.
279:285	Already, some very high accuracies are being reported under less stringent conditions, e.g. for domain focused NER.
280:285	12 Conclusions We started by discussing the problem of NER for South and South East Asian languages and the motivations for organizing a workshop on this topic.
281:285	We also described a named entity annotated corpus for five South Asian languages used for this workshop.
282:285	We presented some statistics about the corpus and also the problems we encountered in getting the corpus annotated by teams located in distant places.
283:285	We also presented a new named entity tagset that was developed for annotation of this corpus.
284:285	Then we presented the results for our experiments on a reasonable baseline.
285:285	Finally we gave an overview of the papers selected for the NERSSEAL-08 workshop and discussed the systems described in these papers and the results obtained, including those for the shared task which was one of the two tracks in the workshop.

