We will briefly review the perceptron algorithm, and its convergence properties – see Collins (2002) for a full description.
P02-1034 W02-1001
When the FSLC has been applied and the set is restricted to those occurring more than once 2See Johnson (1998) for a presentation of the transform/detransform paradigm in parsing.
J98-4004
Before inducing the left-child chains and allowable triples from the treebank, the trees are transformed with a selective left-corner transformation (Johnson and Roark, 2000) that has been flattened as presented in Roark (2001b).
C00-1052 J01-2004
Collins (2000) and Collins and Duffy (2002) rerank the top N parses from an existing generative parser, but this kind of approach 1Dynamic programming methods (Geman and Johnson, 2002; Lafferty et al., 2001) can sometimes be used for both training and decoding, but this requires fairly strong restrictions on the features in the model.
P02-1034 P02-1036 W02-1001
For example, Johnson et al.(1999) and Riezler et al.(2002) use all parses generated by an LFG parser as input to an MRF approach – given the level of ambiguity in natural language, this set can presumably become extremely large.
P02-1035 P99-1069
We can then state the following theorem (see (Collins, 2002) for a proof): Theorem 1 For any training sequence (xi;yi) that is separable with margin, for any value of T, then for the perceptron algorithm in figure 1 Ne R 2 2 where R is a constant such that 8i;8z 2 GEN(xi) jj (xi;yi) (xi;z)jj R.
P02-1034 W02-1001
For this paper, we used POS tags that were provided either by the Treebank itself (gold standard tags) or by the perceptron POS tagger3 presented in Collins (2002).
P02-1034 W02-1001
Models for NLP We follow the framework outlined in Collins (2002; 2004).
P02-1034 W02-1001
Examples of such techniques are Markov Random Fields (Ratnaparkhi et al., 1994; Abney, 1997; Della Pietra et al., 1997; Johnson et al., 1999), and boosting or perceptron approaches to reranking (Freund et al., 1998; Collins, 2000; Collins and Duffy, 2002).
C00-1052 J97-4005 J98-4004 P02-1034 P99-1069 W02-1001
Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word.
J01-2004
Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002).
P02-1034 W02-1001
In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999).
J01-2004
full description of the parsing approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights.
J01-2004
The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank.
J01-2004
Freund and Schapire (1999) discuss how the theory for classification problems can be extended to deal with both of these questions; Collins (2002) describes how these results apply to NLP problems.
P02-1034 W02-1001
All of the convergence and generalization results in Collins (2002) depend on notions of separability rather than the size of GEN.
P02-1034 W02-1001
This paper explores an alternative approach to parsing, based on the perceptron training algorithm introduced in Collins (2002).
P02-1034 W02-1001
As a final note, following Collins (2002), we used the averaged parameters from the training algorithm in decoding test examples in our experiments.
P02-1034 W02-1001
We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search.
J01-2004
