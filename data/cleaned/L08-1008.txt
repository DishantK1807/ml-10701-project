<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>G Kacmarick</author>
<author>C Brocket</author>
</authors>
<title>Automatically Harvesting Katakana-English Term Pairs from Search Engine Query Logs</title>
<date>2001</date>
<booktitle>In Proceedings of the Sixth Natural Language Processing Pacific Rim Symposium</booktitle>
<location>Tokyo, Japan</location>
<contexts>
<context>rocedures such as orthographic normalization cannot be based on probabilistic methods like bigramming and algorithmic methods alone. Many attempts have been made along these lines (Goto et al., 2001; Brill et al. 2001), with some claiming performance equivalent to lexicon-driven methods, while others report good results with only a small lexicon and simple segmentor (Kwok, 1997). It has been reported that a robust</context>
</contexts>
<marker>Brill, Kacmarick, Brocket, 2001</marker>
<rawString>Brill, E., Kacmarick, G., Brocket, C. (2001). Automatically Harvesting Katakana-English Term Pairs from Search Engine Query Logs. In Proceedings of the Sixth Natural Language Processing Pacific Rim Symposium, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carbonell</author>
</authors>
<title>Context-Based Machine Translation</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Conference of the Association of Machine Translation in the Americas</booktitle>
<location>Cambridge, MA</location>
<marker>Carbonell, 2006</marker>
<rawString>Carbonell, J.  et al (2006). Context-Based Machine Translation. In Proceedings of the 7th Conference of the Association of Machine Translation in the Americas, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Emerson</author>
</authors>
<title>Segmenting Chinese in Unicode</title>
<date>2000</date>
<booktitle>In Proceedings of the 16th International Unicode Conference</booktitle>
<location>Amsterdam</location>
<contexts>
<context>curate word segmentation, which involves identifying word boundaries by breaking a text stream into semantic units for dictionary lookup and indexing purposes. Good progress in this area is reported (Emerson, 2000; Yu, et al., 2000). 5. Miscellaneous retrieval technologies such as synonym expansion and cross-language information retrieval (CLIR) (Goto, 2001). 6. Proper nouns pose special difficulties as they a</context>
<context>rsions such as shown in the Incorrect column above. Because of segmentation ambiguities, such conversion must be done with the aid of a segmentor that can break the text stream into meaningful units (Emerson, 2000). 3.2.3 Lexemic Conversion A more sophisticated, and far more challenging, approach to C2C conversion is to map SC and TC lexemes that are semantically, not orthographically, equivalent. For example,</context>
<context>mentor (Kwok, 1997). It has been reported that a robust morphological analyzer capable of processing lexemes, rather than bigrams or n-grams, must be supported by a large-scale computational lexicon (Emerson, 2000) in what is often referred to as the hybrid approach. This experience is shared by many of the world's major portals and MT developers, who make extensive use of lexical databases. Unlike in the past</context>
</contexts>
<marker>Emerson, 2000</marker>
<rawString>Emerson, T. (2000). Segmenting Chinese in Unicode. In Proceedings of the 16th International Unicode Conference, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Goto</author>
<author>N Uratani</author>
<author>T Ehara</author>
</authors>
<title>Cross-Language Information Retrieval of Proper Nouns using Context Information</title>
<date>2001</date>
<booktitle>In Proceedings of the Sixth Natural Language Processing Pacific Rim Symposium</booktitle>
<location>Tokyo, Japan</location>
<contexts>
<context>uages and Arabic, procedures such as orthographic normalization cannot be based on probabilistic methods like bigramming and algorithmic methods alone. Many attempts have been made along these lines (Goto et al., 2001; Brill et al. 2001), with some claiming performance equivalent to lexicon-driven methods, while others report good results with only a small lexicon and simple segmentor (Kwok, 1997). It has been rep</context>
</contexts>
<marker>Goto, Uratani, Ehara, 2001</marker>
<rawString>Goto, I., Uratani, N., Ehara T. (2001). Cross-Language Information Retrieval of Proper Nouns using Context Information. In Proceedings of the Sixth Natural Language Processing Pacific Rim Symposium, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Halpern</author>
<author>J Kerman</author>
</authors>
<title>The Pitfalls and Complexities of Chinese to Chinese Conversion</title>
<date>1999</date>
<booktitle>In Proceedings of the Fourteenth International Unicode Conference</booktitle>
<location>Cambridge, MA</location>
<marker>Halpern, Kerman, 1999</marker>
<rawString>Halpern, J., Kerman J. (1999). The Pitfalls and Complexities of Chinese to Chinese Conversion.  In Proceedings of the Fourteenth International Unicode Conference, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Halpern</author>
</authors>
<title>The Challenges of Intelligent Japanese Searching, working paper (www.cjk.org/cjk/joa/joapaper.htm), The CJK Dictionary Institute</title>
<date>2003</date>
<location>Saitama, Japan</location>
<contexts>
<context>lack of a standard orthography. To process the extremely large number of orthographic variants (especially in Japanese) requires support for advanced methodology such as cross-orthographic searching (Halpern, 2003). 2. The accurate conversion between Simplified Chinese (SC) and Traditional Chinese (TC), deceptively simple but in fact extremely difficult (Halpern, Kerman, 1999). 3. Morphological complexity pose</context>
<context> any other major language, including Chinese. A major factor is the complex interaction of the four scripts, resulting in countless words that can be written in a variety of often unpredictable ways (Halpern, 2003). Japanese is also a highly agglutinative language. Verbs can have numerous inflected and derived forms (tens of thousands), Japanese NLP applications must be capable of performing stemming, i.e. be </context>
</contexts>
<marker>Halpern, 2003</marker>
<rawString>Halpern, J. (2003). The Challenges of Intelligent Japanese Searching, working paper (www.cjk.org/cjk/joa/joapaper.htm), The CJK Dictionary Institute, Saitama, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Halpern</author>
</authors>
<title>The Role of Lexical Resources in CJK Natural Language Processing</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL 2006</booktitle>
<location>Sydney</location>
<contexts>
<context>s-language information retrieval (CLIR) (Goto, 2001). 6. Proper nouns pose special difficulties as they are extremely numerous, difficult to detect without a lexicon and have an unstable orthography (Halpern, 2006). 7. The Arabic orthography is ambiguous for various reasons: the omission of short vowels, multiple ways of writing long vowels, and complex hamza rules. Arabic is also highly ambiguous morphologica</context>
<context>na variants 4.3 Cross-Script Variants Japanese is written in a mixture of four scripts: kanji (Chinese characters), two syllabic scripts called hiragana and katakana, and romaji (the Latin alphabet) (Halpern, 2006). Orthographic variation across scripts, as illustrated in Table 7, is extremely common and mostly unpredictable, so that the same word can be written in hiragana, katakana or kanji, or even in a mix</context>
<context>mu チーム ティーム Traditional big ookii おおきい おうきい づ vs. ず continue tsuzuku つづく つずく Table 8: Katakana and hiragana variants Other types of Japanese orthographic variants of less importance are described in (Halpern, 2006). 4.5 Lexicon-driven Normalization Lexicon-driven normalization of Japanese orthographic variants can be achieved by orthographic mapping tables such as the one shown below, using various techniques </context>
<context>poses various challenges to developers of NLP applications. The issues are similar to Japanese in principle but differ in detail and scale. The details of Korean orthographic variation, described in (Halpern, 2006), are beyond the scope of this paper. Briefly, Korean has variant hangul spellings in the writing of loanwords, such as케이크keikeu and케잌keik for 'cake', and in the writing of non-Korean personal names,</context>
</contexts>
<marker>Halpern, 2006</marker>
<rawString>Halpern, J. (2006). The Role of Lexical Resources in CJK Natural Language Processing. In Proceedings of COLING/ACL 2006,  Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Halpern</author>
</authors>
<title>The Challenges and Pitfalls of Arabic Romanization and Arabization</title>
<date>2007</date>
<booktitle>In Proceedings of Computational Approaches to Arabic Script-based Languages</booktitle>
<location>Palo Alto, CA</location>
<contexts>
<context>ons: the omission of short vowels, multiple ways of writing long vowels, and complex hamza rules. Arabic is also highly ambiguous morphologically, so that a string can often represent multiple words (Halpern, 2007). 2. Lexicon Driven Approach The various attempts to tackle these tasks using purely statistical and algorithmic methods have had only limited success (Kwok, 1997). Indeed Kay (2004) argues that &amp;quot;sta</context>
<context>are used sparingly, while the use of consonants to indicate long vowels is ambiguous. On the whole, unvocalized Arabic is highly ambiguous and poses major challenges to Arabic information processing (Halpern, 2007). 6.2 Morphological Ambiguity Arabic is a highly inflected language. Inflection is indicated by changing the vowel patterns as well as by adding various suffixes, prefixes, and clitics. A full paradi</context>
</contexts>
<marker>Halpern, 2007</marker>
<rawString>Halpern, J. (2007). The Challenges and Pitfalls of Arabic Romanization and Arabization. In Proceedings of Computational Approaches to Arabic Script-based Languages, Palo Alto, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Arabic Script based Languages deserve to be studied linguistically</title>
<date>2004</date>
<booktitle>In COLING 2004</booktitle>
<location>Geneva</location>
<marker>Kay, 2004</marker>
<rawString>Kay, M. (2004). Arabic Script based Languages deserve to be studied linguistically. In COLING 2004, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K L Kwok</author>
</authors>
<title>Lexicon Effects on Chinese Information Retrieval</title>
<date>1997</date>
<booktitle>In Proceedings of 2nd Conference on Empirical Methods in NLP, ACL</booktitle>
<pages>141--148</pages>
<contexts>
<context>n often represent multiple words (Halpern, 2007). 2. Lexicon Driven Approach The various attempts to tackle these tasks using purely statistical and algorithmic methods have had only limited success (Kwok, 1997). Indeed Kay (2004) argues that &amp;quot;statistics are a surrogate for knowledge of the world&amp;quot; and that &amp;quot;this is an alarming trend that computational linguists ... should resist with great determination.&amp;quot; H</context>
<context>ese lines (Goto et al., 2001; Brill et al. 2001), with some claiming performance equivalent to lexicon-driven methods, while others report good results with only a small lexicon and simple segmentor (Kwok, 1997). It has been reported that a robust morphological analyzer capable of processing lexemes, rather than bigrams or n-grams, must be supported by a large-scale computational lexicon (Emerson, 2000) in </context>
</contexts>
<marker>Kwok, 1997</marker>
<rawString>Kwok, K.L. (1997). Lexicon Effects on Chinese Information Retrieval. In Proceedings of 2nd Conference on Empirical Methods in NLP, ACL 141-148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lunde</author>
</authors>
<title>CJKV Information Processing. O’Reilly &amp; Associates</title>
<date>1999</date>
<location>Sebastopol, CA</location>
<contexts>
<context>f orthographic variants in TC. 3.2 Script Conversion Automatically converting SC to/from TC, referred to as C2C conversion, is full of complexities (Halpern, Kerman, 1999) and technical difficulties (Lunde, 1999). The conversion can be implemented on three levels in increasing order of sophistication, briefly described below. 3.2.1 Code Conversion The simplest, but least reliable, method is on a code point-t</context>
<context>vant components, such as MT dictionaries, search engine indexes and the related documents should be normalized. An extra complication is that Taiwanese and Hong Kong variants are sometimes different (Lunde, 1999). Var. 1 Var. 2 English Comment 裏 裡 inside 100% interchangeable 敎 教 teach 100% interchangeable 著 着 particle variant 2 not in Big5 為 爲 for variant 2 not in Big5 沉 沈 sink; surname partially interchange</context>
</contexts>
<marker>Lunde, 1999</marker>
<rawString>Lunde, K. (1999). CJKV Information Processing. O’Reilly &amp; Associates. Sebastopol, CA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B K Tsou</author>
<author>W F Tsoi</author>
<author>T B Y Lai</author>
<author>J Hu</author>
<author>S W K Chan</author>
</authors>
<marker>Tsou, Tsoi, Lai, Hu, Chan, </marker>
<rawString>Tsou, B.K., Tsoi, W.F., Lai, T.B.Y., Hu, J., Chan S.W.K.</rawString>
</citation>
<citation valid="true">
<title>LIVAC, a Chinese synchronous corpus, and some applications</title>
<date>2000</date>
<booktitle>In 2000 International Conference on Chinese Language Computing (ICCLC2000</booktitle>
<location>Chicago, IL</location>
<marker>2000</marker>
<rawString>(2000). LIVAC, a Chinese synchronous corpus, and some applications. In 2000 International Conference on Chinese Language Computing (ICCLC2000), Chicago, IL.</rawString>
</citation>
</citationList>
</algorithm>

