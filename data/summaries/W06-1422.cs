Such exercises are now common in most areas of NLP, and have had a major impact on many areas, including machine translation and information extraction (see discussion of history of shared-task initiatives and their impact in Belz and Kilgarriff (2006)).
E06-1040 W06-1421
We are planning to increase the number and complexity of tasks from one round to the next, as this has been useful in other NLP evaluations (Belz and Kilgarriff, 2006); for example, we will add surface realisation as a separate task in round 2 and layout/structuring task in round 3.
E06-1040 W06-1421
One of the best-known comparative studies of evaluation techniques was by Papineni et al.(2002)whoproposedthe BLEU metricformachine translation and showed that BLEU correlated well with human judgements when comparing several machine translation systems.
P02-1040
Proposal We intend to apply for funding for a three-year projecttocreatemoresharedinput/outputdatasets (we are focusing on data-to-text tasks for the reasons discussed in Belz and Kilgarriff (2006)), organise shared task workshops, and create and test a range of methods for evaluating submitted systems.
E06-1040 W06-1421
We intend to use a fairly standard organisation (Belz and Kilgarriff, 2006).
E06-1040 W06-1421
As we showed previously (Belz and Reiter,2006)thattherearesignificantinter-subject differences in ratings, one thing we want to determine is how many subjects are needed to get reliable and reproducible results.
E06-1040 W06-1421
In our work (Belz and Reiter, 2006), we used several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts.
E06-1040 W06-1421
