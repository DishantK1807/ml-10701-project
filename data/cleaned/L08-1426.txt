<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Using lexical chains for text summarization</title>
<date>1999</date>
<booktitle>In Inderjeet Mani &amp; Mark T. Maybury (Eds.), Advances in Text Summarization, Chp. 10</booktitle>
<pages>111--121</pages>
<publisher>MIT Press</publisher>
<location>Cambridge, Massachusetts, USA</location>
<contexts>
<context>03) is based on the idea that lexical chains can represent the discourse structure. Lexical chains are so called “sequences of related words” which “provide the lexical cohesive structure of a text” (Barzilay &amp; Elhadad, 1999). Morris &amp; Hirst (1991) described the first computational model for creating lexical chains, which was not implemented. The authors used lexical cohesion relations, which are (in their description) c</context>
</contexts>
<marker>Barzilay, Elhadad, 1999</marker>
<rawString>Barzilay, Regina &amp; Michael Elhadad (1999). Using lexical chains for text summarization. In Inderjeet Mani &amp; Mark T. Maybury (Eds.), Advances in Text Summarization, Chp. 10, pp. 111–121. Cambridge, Massachusetts, USA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Statistical models for text segmentation</title>
<date>1999</date>
<booktitle>Machine Learning</booktitle>
<pages>34--1</pages>
<contexts>
<context>oundary detection. The approaches fall into three categories: the first type of approach works with statistical and supervised learning methods (Kan et al., ; Passonneau &amp; Litman, 1997; Reynar, 1999; Beeferman et al., 1999; Utiyama &amp; Isahara, 2001; T¨ur et al., 2001; Lagus &amp; Kuusisto, 2002). The second type of approach is based on lexical cohesion via lexical chain building using external knowledge sources (like thesau</context>
<context> and Window 3. Window 1 and Window 2 share at least one word and therefore they belong together. 5. Evaluation with Pk and WD For the evaluation of the automatic method two metrics are used here: Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner &amp; Hearst, 2002). Both have been developed to take into account that topic boundaries do not necessarily match exactly. Beeferman et al. (1999) describes an error metric </context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Beeferman, Doug, Adam Berger &amp; John Lafferty (1999). Statistical models for text segmentation. Machine Learning, 34(1-3):177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Choi</author>
</authors>
<title>Advances in independent linear text segmentation</title>
<date>2000</date>
<journal></journal>
<booktitle>In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics</booktitle>
<volume>3</volume>
<pages>26--32</pages>
<location>Seattle, Wash</location>
<marker>Choi, 2000</marker>
<rawString>Choi, Freddy (2000). Advances in independent linear text segmentation. In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics, Seattle, Wash., 29 April – 3 May, 2000, pp. 26–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database</title>
<date>1998</date>
<publisher>MIT Press</publisher>
<location>Cambridge, Mass</location>
<contexts>
<context>o disambiguating the word. But Morris &amp; Hirst (1991) did not require the words in the chains to which they belonged to have the same sense. Hirst &amp; St-Onge (1998) proposed a method for using WordNet (Fellbaum, 1998) for building lexical chains. WordNet is organised in synonym sets (synsets), which are the sets of all the words sharing a common sense. Polysemous words appear in more than one synset. Words of the</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, Christiane (Ed.) (1998). WordNet: An Electronic Lexical Database. Cambridge, Mass.: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter W Foltz</author>
<author>Walter Kintsch</author>
<author>Thomas K Landauer</author>
</authors>
<title>The measurement of textual coherence with latent semantic analysis</title>
<date>1998</date>
<booktitle>Discourse Processes</booktitle>
<pages>25--285</pages>
<contexts>
<context>building using external knowledge sources (like thesauri) (Morris &amp; Hirst, 1991; 1The work reported in this paper was done while the first author was affiliated with EML Research gGmbH. Kozima, 1993; Foltz et al., 1998; Landauer et al., 1998; Galley et al., 2003; Stokes, 2003; Popescu-Belis et al., 2004; Olney &amp; Cai, 2005). The third type combines statistical with similarity measures as e.g. Hearst (1997) and Choi </context>
</contexts>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>Foltz, Peter W., Walter Kintsch &amp; Thomas K. Landauer (1998). The measurement of textual coherence with latent semantic analysis. Discourse Processes, 25:285–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Galley</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Eric Fosler-Lussier &amp; Hongyan Jing (2003). Discourse segmentation of multiparty conversation</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>562--569</pages>
<location>Sapporo, Japan, 7–12</location>
<marker>Galley, McKeown, 2003</marker>
<rawString>Galley, Michael, Kathleen R. McKeown, Eric Fosler-Lussier &amp; Hongyan Jing (2003). Discourse segmentation of multiparty conversation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, Sapporo, Japan, 7–12 July 2003, pp. 562–569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Georgescul</author>
</authors>
<title>Alexander Clark &amp; Susan Armstrong (2006). Word distributions for thematic segmentaion in a support vector machine approach</title>
<date>2006</date>
<booktitle>In Proceedings of the CoNLL-X conference held at HLT-NAACL</booktitle>
<pages>101--108</pages>
<location>New York, NY, USA</location>
<marker>Georgescul, 2006</marker>
<rawString>Georgescul, Maria, Alexander Clark &amp; Susan Armstrong (2006). Word distributions for thematic segmentaion in a support vector machine approach. In Proceedings of the CoNLL-X conference held at HLT-NAACL 2006, New York, NY, USA, pp. 101–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>TextTiling: Segmenting text into multiparagraph subtopic passages</title>
<date>1997</date>
<journal>Computational Linguistics</journal>
<volume>23</volume>
<contexts>
<context>ost collections they are limited and mostly rather general in context, which can be a problem when dealing with specialized texts. By combining statistical methods and similarity measures TextTiling (Hearst, 1997) was able to overcome these limitations somewhat. But most approaches so far have only been developed and tested on written texts rather than speech or transcribed speech. 3. The ICSI Meeting Recorde</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Hearst, Marti (1997). TextTiling: Segmenting text into multiparagraph subtopic passages. Computational Linguistics, 23(1):33–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
<author>D St-Onge</author>
</authors>
<title>Lexical chains as representation of context for the detection and correction malapropisms</title>
<date>1998</date>
<journal>In C. Fellbaum (Ed.), WordNet. An Electronic Lexical Database, Chp</journal>
<volume>13</volume>
<pages>305--332</pages>
<publisher>MIT Press</publisher>
<location>Cambridge, Mass</location>
<marker>Hirst, St-Onge, 1998</marker>
<rawString>Hirst, G. &amp; D. St-Onge (1998). Lexical chains as representation of context for the detection and correction malapropisms. In C. Fellbaum (Ed.), WordNet. An Electronic Lexical Database, Chp. 13, pp. 305–332. Cambridge, Mass.: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pei-Yun Hsueh</author>
<author>Johanna D Moore</author>
<author>Steve Renals</author>
</authors>
<title>Automatic segmentation of multiparty dialogue</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics</booktitle>
<pages>273--280</pages>
<location>Trento, Italy</location>
<marker>Hsueh, Moore, Renals, 2006</marker>
<rawString>Hsueh, Pei-Yun, Johanna D. Moore &amp; Steve Renals (2006). Automatic segmentation of multiparty dialogue. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy, 3–7 April 2006, pp. 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Janin</author>
<author>Don Baron</author>
<author>Jane Edwards</author>
<author>Dan Ellis</author>
</authors>
<date>2003</date>
<booktitle>Shriberg, Andreas Stolcke &amp; Chuck Wooters</booktitle>
<location>David Gelbart, Nelson Morgan, Barbara Peskin, Thilo Pfau, Elizabeth</location>
<contexts>
<context>ings. In this paper we introduce a dialogue segmentation module that automatically puts topic boundaries in meeting data1. As reference data we use the ICSI Meeting Recorder Project Data (ICSI data) (Janin et al., 2003). Recent work on automatic dialogue segmentation was done not only on text, but also on meeting data (Galley et al., 2003; Georgescul et al., 2006). The first uses Lexical Cohesion for detecting topi</context>
<context> texts rather than speech or transcribed speech. 3. The ICSI Meeting Recorder Project Data The meetings that serve as basis for this work have been collected within the ICSI Meeting Recorder Project (Janin et al., 2003) (ICSI data). This collection contains 75 meetings recorded at ICSI during research meetings. The data has been transcribed manually and divided into “segments”. These are turn-like elements, but ver</context>
</contexts>
<marker>Janin, Baron, Edwards, Ellis, 2003</marker>
<rawString>Janin, Adam, Don Baron, Jane Edwards, Dan Ellis, David Gelbart, Nelson Morgan, Barbara Peskin, Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke &amp; Chuck Wooters (2003).</rawString>
</citation>
<citation valid="true">
<title>The ICSI meeting corpus</title>
<date>2003</date>
<booktitle>In Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</booktitle>
<pages>364--367</pages>
<location>Hong Kong, China, 6–10</location>
<contexts>
<context>ype combines statistical with similarity measures as e.g. Hearst (1997) and Choi (2000). Two most recent approaches are presented by Hsueh et al. (2006), where the approach presented by Galley et al. (2003) is extended in two ways: first, it used automatic speech recognition output, and second it aimed at detecting subtopic boundaries as well as maintopic boundaries. Another recent approach is presented</context>
<context>ts of all the words sharing a common sense. Polysemous words appear in more than one synset. Words of the same category are linked through semantic relations like synonymy and hyponymy. Galley et al. (2003) used only identiy between terms to form the chains. Afterwards the chains are divided into subchains. A weighting scheme is also applied, where chains are weighted based on frequency and compactness.</context>
<context>ere tested and evaluated. First, we tested the segmentation. In order to evaluate the differences between spurtand segment-based annotations, we used the manual annotation provided by Galley et al. (2003) in its original version, which was done on segments. For the comparison of the spurt based data we used a gold standard generated over the manual annotation done on 12 meetings by at least 2 human an</context>
<context>her hand if only the boundaries are considered which were marked by both annotators it can easily be the case that they agree on few or none at all. Therefore, κ would be close to 0. In Galley et al. (2003) Cochran’s Q was suggested to provide a method to evaluate manual annotations for this case. Cochran’s Q tests whether the assignment of boundaries by the human annotators is randomly distributed. Thi</context>
<context>of positive answers for all annotators for the jth example. The resulting number gives the degrees of freedom under which the significance level can be checked in the appropriate table. Galley et al. (2003) report that on 19 of 25 (76%) meetings manually annotated for topic boundaries the interannotator reliability is significant on the 0.05 level. In our data we found that 9 out of 12 (75%) meetings, w</context>
</contexts>
<marker>2003</marker>
<rawString>The ICSI meeting corpus. In Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing, Hong Kong, China, 6–10 April 2003, pp. 364–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min-Yen Kan</author>
<author>Judith L Klavans</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Linear segmentation and segment significance</title>
<date>1998</date>
<booktitle>In 6th Workshop on Very Large Corpora</booktitle>
<pages>197--205</pages>
<location>Montral, Canada, 5–16</location>
<marker>Kan, Klavans, McKeown, 1998</marker>
<rawString>Kan, Min-Yen, Judith L. Klavans &amp; Kathleen R. McKeown. Linear segmentation and segment significance. In 6th Workshop on Very Large Corpora, Montral, Canada, 5–16 August 1998, pp. 197–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Kozima</author>
</authors>
<title>Text segmentation based on similarity between words</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>286--288</pages>
<location>Columbus, Ohio, 22–26</location>
<contexts>
<context>lexical chain building using external knowledge sources (like thesauri) (Morris &amp; Hirst, 1991; 1The work reported in this paper was done while the first author was affiliated with EML Research gGmbH. Kozima, 1993; Foltz et al., 1998; Landauer et al., 1998; Galley et al., 2003; Stokes, 2003; Popescu-Belis et al., 2004; Olney &amp; Cai, 2005). The third type combines statistical with similarity measures as e.g. Hea</context>
</contexts>
<marker>Kozima, 1993</marker>
<rawString>Kozima, Hideki (1993). Text segmentation based on similarity between words. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, Columbus, Ohio, 22–26 June 1993, pp. 286–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krista Lagus</author>
<author>Jukka Kuusisto</author>
</authors>
<title>Topic identification in natural language dialogues using neural networks</title>
<date>2002</date>
<booktitle>In Proceedings of the Third SIGdial Workshop on Discourse and Dialogue</booktitle>
<pages>95--102</pages>
<location>Philadelphia, Penn</location>
<contexts>
<context>rst type of approach works with statistical and supervised learning methods (Kan et al., ; Passonneau &amp; Litman, 1997; Reynar, 1999; Beeferman et al., 1999; Utiyama &amp; Isahara, 2001; T¨ur et al., 2001; Lagus &amp; Kuusisto, 2002). The second type of approach is based on lexical cohesion via lexical chain building using external knowledge sources (like thesauri) (Morris &amp; Hirst, 1991; 1The work reported in this paper was done</context>
</contexts>
<marker>Lagus, Kuusisto, 2002</marker>
<rawString>Lagus, Krista &amp; Jukka Kuusisto (2002). Topic identification in natural language dialogues using neural networks. In Proceedings of the Third SIGdial Workshop on Discourse and Dialogue, Philadelphia, Penn., July 2002, pp. 95–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An introduction to latent semantic analysis</title>
<date>1998</date>
<booktitle>Discourse Processes</booktitle>
<pages>25--259</pages>
<contexts>
<context>nal knowledge sources (like thesauri) (Morris &amp; Hirst, 1991; 1The work reported in this paper was done while the first author was affiliated with EML Research gGmbH. Kozima, 1993; Foltz et al., 1998; Landauer et al., 1998; Galley et al., 2003; Stokes, 2003; Popescu-Belis et al., 2004; Olney &amp; Cai, 2005). The third type combines statistical with similarity measures as e.g. Hearst (1997) and Choi (2000). Two most recent</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Landauer, Thomas K., Peter W. Foltz &amp; Darrell Laham (1998). An introduction to latent semantic analysis. Discourse Processes, 25:259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text</title>
<date>1991</date>
<journal>Computational Linguistics</journal>
<volume>17</volume>
<contexts>
<context>yama &amp; Isahara, 2001; T¨ur et al., 2001; Lagus &amp; Kuusisto, 2002). The second type of approach is based on lexical cohesion via lexical chain building using external knowledge sources (like thesauri) (Morris &amp; Hirst, 1991; 1The work reported in this paper was done while the first author was affiliated with EML Research gGmbH. Kozima, 1993; Foltz et al., 1998; Landauer et al., 1998; Galley et al., 2003; Stokes, 2003; P</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Morris, Jane &amp; Graeme Hirst (1991). Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17(1):21–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph M¨uller</author>
</authors>
<title>Automatic detection on nonreferential it in spoken multi-party dialog</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics</booktitle>
<pages>49--56</pages>
<location>Trento, Italy</location>
<marker>M¨uller, 2006</marker>
<rawString>M¨uller, Christoph (2006). Automatic detection on nonreferential it in spoken multi-party dialog. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, Trento, Italy, 3–7 April 2006, pp. 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Olney</author>
<author>Zhigiang Cai</author>
</authors>
<title>An orthonormal basis for topic segmentation in tutorial dialogue</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing</booktitle>
<pages>971--978</pages>
<contexts>
<context>s paper was done while the first author was affiliated with EML Research gGmbH. Kozima, 1993; Foltz et al., 1998; Landauer et al., 1998; Galley et al., 2003; Stokes, 2003; Popescu-Belis et al., 2004; Olney &amp; Cai, 2005). The third type combines statistical with similarity measures as e.g. Hearst (1997) and Choi (2000). Two most recent approaches are presented by Hsueh et al. (2006), where the approach presented by </context>
</contexts>
<marker>Olney, Cai, 2005</marker>
<rawString>Olney, Andrew &amp; Zhigiang Cai (2005). An orthonormal basis for topic segmentation in tutorial dialogue. In Proceedings of Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing, pp. 971–978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Diane J Litman</author>
</authors>
<title>Discourse segmentation by human and automated means</title>
<date>1997</date>
<journal>Computational Linguistics</journal>
<volume>23</volume>
<contexts>
<context>een done on the task of automatic topic boundary detection. The approaches fall into three categories: the first type of approach works with statistical and supervised learning methods (Kan et al., ; Passonneau &amp; Litman, 1997; Reynar, 1999; Beeferman et al., 1999; Utiyama &amp; Isahara, 2001; T¨ur et al., 2001; Lagus &amp; Kuusisto, 2002). The second type of approach is based on lexical cohesion via lexical chain building using e</context>
</contexts>
<marker>Passonneau, Litman, 1997</marker>
<rawString>Passonneau, Rebecca J. &amp; Diane J. Litman (1997). Discourse segmentation by human and automated means. Computational Linguistics, 23(1):103–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Pevzner</author>
<author>Marti Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation</title>
<date>2002</date>
<journal>Computational Linguistics</journal>
<volume>28</volume>
<contexts>
<context> least one word and therefore they belong together. 5. Evaluation with Pk and WD For the evaluation of the automatic method two metrics are used here: Pk (Beeferman et al., 1999) and WindowDiff (WD) (Pevzner &amp; Hearst, 2002). Both have been developed to take into account that topic boundaries do not necessarily match exactly. Beeferman et al. (1999) describes an error metric as a formalisation that one segmenter is bett</context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>Pevzner, Lev &amp; Marti Hearst (2002). A critique and improvement of an evaluation metric for text segmentation. Computational Linguistics, 28(1):19–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Popescu-Belis</author>
<author>Alexander Clark</author>
<author>Maria Georgescul</author>
</authors>
<title>Denis Lalanne &amp; Sandrine Zufferey (2004). Shallow discourse processing using machine learning algorithms (or not</title>
<booktitle>In S. Bengio &amp; H. Bourlard (Eds.), Machine Learning for Multimodal Interaction, Vol. 3361, Springer Lecture Series in Computer Science</booktitle>
<pages>277--290</pages>
<publisher>Springer</publisher>
<marker>Popescu-Belis, Clark, Georgescul, </marker>
<rawString>Popescu-Belis, Andrei, Alexander Clark, Maria Georgescul, Denis Lalanne &amp; Sandrine Zufferey (2004). Shallow discourse processing using machine learning algorithms (or not). In S. Bengio &amp; H. Bourlard (Eds.), Machine Learning for Multimodal Interaction, Vol. 3361, Springer Lecture Series in Computer Science, pp. 277–290. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
</authors>
<title>Statistical models for topic segmentation</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>357--364</pages>
<location>College Park, Md</location>
<contexts>
<context>omatic topic boundary detection. The approaches fall into three categories: the first type of approach works with statistical and supervised learning methods (Kan et al., ; Passonneau &amp; Litman, 1997; Reynar, 1999; Beeferman et al., 1999; Utiyama &amp; Isahara, 2001; T¨ur et al., 2001; Lagus &amp; Kuusisto, 2002). The second type of approach is based on lexical cohesion via lexical chain building using external knowle</context>
</contexts>
<marker>Reynar, 1999</marker>
<rawString>Reynar, Jeffrey C. (1999). Statistical models for topic segmentation. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, College Park, Md., 20–26 June 1999, pp. 357–364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Don Baron</author>
</authors>
<title>Observations on overlap: Findings and implications for automatic processing of multi-party conversation</title>
<date>2001</date>
<booktitle>In Proceedings of the 7th European Conference on Speech Communication and Technology (EUROSPEECH ’01</booktitle>
<volume>2</volume>
<pages>1359--1362</pages>
<location>Aalborg, Denmark, 3–7</location>
<contexts>
<context>een transcribed manually and divided into “segments”. These are turn-like elements, but very often they interrupt sentences and thoughts. Therefore, we created an additional division called “spurts” (Shriberg et al., 2001). If, for a certain speaker, the pause within his/her speech is longer than 500 ms, the amount of speech beFigure 1: Illustration of the DiaSeg Method tween two such pauses is one ”spurt”. Details on</context>
</contexts>
<marker>Shriberg, Stolcke, Baron, 2001</marker>
<rawString>Shriberg, Elizabeth, Andreas Stolcke &amp; Don Baron (2001). Observations on overlap: Findings and implications for automatic processing of multi-party conversation. In Proceedings of the 7th European Conference on Speech Communication and Technology (EUROSPEECH ’01), Aalborg, Denmark, 3–7 September 2001, Vol. 2, pp. 1359–1362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Stokes</author>
</authors>
<title>Spoken and written news story segmentation using lexical chains</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technolgy Conference of the North American Chapter of the Association for Computational Linguistics Student Research Workshop</booktitle>
<volume>1</volume>
<pages>49--54</pages>
<location>Edmonton, Alberta, Canada, 27</location>
<contexts>
<context>s &amp; Hirst, 1991; 1The work reported in this paper was done while the first author was affiliated with EML Research gGmbH. Kozima, 1993; Foltz et al., 1998; Landauer et al., 1998; Galley et al., 2003; Stokes, 2003; Popescu-Belis et al., 2004; Olney &amp; Cai, 2005). The third type combines statistical with similarity measures as e.g. Hearst (1997) and Choi (2000). Two most recent approaches are presented by Hsueh </context>
</contexts>
<marker>Stokes, 2003</marker>
<rawString>Stokes, Nicola (2003). Spoken and written news story segmentation using lexical chains. In Proceedings of the Human Language Technolgy Conference of the North American Chapter of the Association for Computational Linguistics Student Research Workshop, Edmonton, Alberta, Canada, 27 May –1 June, 2003, pp. 49–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨okhan T¨ur</author>
</authors>
<title>Dilek Hakkani-T¨ur, Andreas Stolcke &amp; Elizabeth Shriberg</title>
<date>2001</date>
<journal>Computational Linguistics</journal>
<volume>27</volume>
<marker>T¨ur, 2001</marker>
<rawString>T¨ur, G¨okhan, Dilek Hakkani-T¨ur, Andreas Stolcke &amp; Elizabeth Shriberg (2001). Integrating prosodic and lexical cues for automatic topic segmentation. Computational Linguistics, 27(1):31–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A statistical model for domain-independent text segmentation</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>1--8</pages>
<location>Toulouse</location>
<contexts>
<context>pproaches fall into three categories: the first type of approach works with statistical and supervised learning methods (Kan et al., ; Passonneau &amp; Litman, 1997; Reynar, 1999; Beeferman et al., 1999; Utiyama &amp; Isahara, 2001; T¨ur et al., 2001; Lagus &amp; Kuusisto, 2002). The second type of approach is based on lexical cohesion via lexical chain building using external knowledge sources (like thesauri) (Morris &amp; Hirst, 1991</context>
</contexts>
<marker>Utiyama, Isahara, 2001</marker>
<rawString>Utiyama, Masao &amp; Hitoshi Isahara (2001). A statistical model for domain-independent text segmentation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, Toulouse, pp. 1–8.</rawString>
</citation>
</citationList>
</algorithm>

