Much of the existing applied research on lexical cohesion uses WordNet-based (Miller, 1990) lexical chains to identify the cohesive texture for a larger text processing application (Barzilay and Elhadad, 1997; Stokes et al., 2004; Moldovan and Novischi, 2002; Al-Halimi and Kazman, 1998).
C02-1167 W97-0703
Sometimes the relation between the members of a tie is easy to identify, like near-synonymy (disease/illness), complementarity (boy/girl), whole-topart (box/lid), but the bulk of lexical cohesive texture is created by relations that are difficult to classify (Morris and Hirst, 2004).
W04-2607
We use the by now standard a0 statistic (Di Eugenio and Glass, 2004; Carletta, 1996; Marcu et al., 1999; Webber and Byron, 2004) to quantify the degree of above-chance agreement between multiple annotators, and the a1 statistic for analysis of sources of unreliability (Krippendorff, 1980).
J04-1005 J96-2004 W99-0307
As it often happens with tasks related to semantics/pragmatics (Poesio and Vieira, 1998; Morris and Hirst, 2005), the inter-reader agreement levels did not reach the accepted reliability thresholds.
J98-2001
We use Siegel and Castellanâ€™s (1988) version of a0; although it assumes similar distributions of categories across coders in that it uses the average to estimate the expected agreement (see equation 2), the current experiment employs 22 coders, so averaging is a much better justified enterprise than in studies with very few coders (2-4), typical in discourse annotation work (Di Eugenio and Glass, 2004).
J04-1005
of Experimental Data Most of the existing research in computational linguistics that uses human annotators is within the framework of classification, where an annotator decides, for every test item, on an appropriate tag out of the pre-specified set of tags (Poesio and Vieira, 1998; Webber and Byron, 2004; Hearst, 1997; Marcus et al., 1993).
J93-2004 J97-1003 J98-2001
Some of those structures for example, cohesion achieved through repeated reference have been subjected to reader based tests, often while trying to produce gold standard data for testing computational models, a task requiring sufficient inter-annotator agreement (Hirschman et al., 1998; Mitkov et al., 2000; Poesio and Vieira, 1998).
J98-2001
