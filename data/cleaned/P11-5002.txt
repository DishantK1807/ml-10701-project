Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Formal and Empirical Grammatical Inference
Jeffrey Heinz, Colin de la Higuera and Menno van Zaanen
heinz@udel.edu, cdlh@univ-nantes.fr, mvzaanen@uvt.nl
1
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Outline of the tutorial
I. Formal GI and learning theory (de la Higuera)
II. Empirical approaches to regular and subregular natural
language classes (Heinz)
III. Empirical approaches to nonregular natural language
classes (van Zaanen)
2
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
I Formal GI and learning theory
What is grammatical inference?
What does learning or having learnt imply?
Reasons for considering formal learning
Some criteria to study learning in a probabilistic and a non
probabilistic setting
3
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A simple definition
Grammatical inference is about learning a grammar given
information about a language
Vocabulary
Learning = building, inferring
Grammar= finite representation of a possibly infinite set of
strings, or trees, or graphs
Information=you can learn from text, from an informant, by
actively querying
Language= possibly infinite set of strings, or trees, or graphs
4
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A Dfa (Ack: Jeffrey Heinz)
The (CV)* language representing licit sequences of sounds in many
languages in the world. Consonants and vowels must alternate;
words must begin with C and must end with V. States show the
regular expression indicating its “good tails”.
(CV)null V(CV)null
C
V
5
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A context free grammar and a parse tree
(de la Higuera 2010)
S
NP VP
John V NP
hit Det N
the ball
S null NP VP
VPnull V NP
NPnull Det N
6
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A categorial dependency grammar (B´echet et al. 2011)
elle null [pred],
la null [#(null clit nullanullobj)]nullclitnullanullobj,
lui null [#(null clit null3d nullobj)]nullclitnull3dnullobj,
a null [#(null clit null3d nullobj)null#(null
clit nullanullobj)nullprednullSnullaux nullanulld],
donn´ee null [aux nullanulld]nullclitnull3dnullobjnullclitnullanullobj
7
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A finite state transducer (Ack: Jeffrey Heinz)
A subsequential transducer illustrating a common phonological rule
of palatalization ( k nullnull >tS / i). States are labelled with a
number and then the output string given by the null function for that
state.
0,null 1,k
k:null
k:kk, C:kC, V:kV
i:>tSi
C,V,i k
Σ = nullCnullVnullknullinull
8
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
So for example:
w t(w)
kata kata
kita >tSita
tak tak
taki ta>tSi
...
9
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Our definition
Grammatical inference is about learning a grammar given
information about a language
Questions
Why grammar and not language?
Why a and not the?
10
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Why not write “learn a language”?
Because you always learn a representation of a language
Paradox
Take two learners learning a context-free language, one is learning
a quadratic normal form and the other a Greibach normal form,
they cannot agree that they have learnt the same thing
(undecidable question).
Worth thinking about...is it a paradox? Do two English speakers
agree they speak the same language?
11
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Our definition
Grammatical inference is about learning a grammar given
information about a language
How can a become the?
Ask for the grammar to be the smallest, best (re a score). null
Combinatorial characterisation
The learning problem becomes an optimisation problem!
Then we often have theorems saying that
If our algorithm does solve the optimisation problem, what we
have learnt is correct
If we can prove that we can’t solve the optimisation problem,
then the class is not learnable
12
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Optimal with respect of some score
Score should take into account:
Simplicity
Coverage
Usefulness
What scores?
Occam argument
Compression argument
Kolmogorov complexity
MDL argument
13
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Moreover
GI is not only about building a grammar from some data. It is
concerned with saying something about:
the quality of the result,
the quality of the learning process,
the properties of the process.
14
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Naive example
Suppose you are building a random number generator.
How are you convinced that it works?
Because it follows sound principles as defined by number
theory specialists?
Because you have tested and the number 772356191 has been
produced?
Because you have proved that the series of numbers that will
be produced is incompressible?
Empirical approach
Experimental approach
Formal approach
15
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Empirical approach: using good (safe?) ideas
For example, genetic algorithms or neural networks
Or some mathematical principle (Occam, Kolmogorov,
MDL,...)
Can become a principled approach
Alternative point of view
Empirical approach is about imitating what nature (or humans) do
16
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Experimental approach
Benchmarks
Competitions
Necessary but not sufficient
How do we know that all the cases are covered?
How do we know that we dont have a hidden bias?
17
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Formal approach: showing that the algorithm has converged
Is impossible:
Just one run
Can’t prove that 23 is random
But we can say something about the algorithm:
That in the near future, given some string, we can predict if
this string belongs to the language or not;
Choose between defining clearly “near future” and accepting
probable truths (or error bounds) or leaving it undefined and
using identification.
18
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
What else would we like to say?
That if the solution we have returned is not good, then that is
because the initial data was bad (insufficient, biased)
Idea:
Blame the data, not the algorithm
19
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Suppose we cannot say anything of the sort?
Then that means that we may be terribly wrong even in a
favourable setting
Thus there is a hidden bias
Hidden bias: the learning algorithm is supposed to be able to
learn anything inside class L1, but can really only learn things
inside class L2, with L2 nullL1
20
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Saying something about the process itself
Key idea: if there is something to learn and the data is not
corrupt, then, given enough time, we will learn it
Replace the notion of learning by that of identifying
21
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
In practise, does it make sense?
No, because we never know if we are in the ideal conditions
(something to learn + good data + enough of it)
Yes, because at least we get to blame the data, not the
algorithm
22
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Complexity issues
Complexity theory should be used: the total or update
runtime, the size of the data needed, the number of mind
changes, the number and weight of errors...
...should be measured and limited.
23
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
A linguistic criterion
One argument appealing to linguists (we hope) is that if the
criteria are not met for some class of languages that a human
is supposed to know how to learn, something is wrong
somewhere
(preposterously, the maths can’t be wrong...)
24
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Non probabilistic settings
Identification in the limit
Resource bounded identification in the limit
Active learning (query learning)
25
Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns
Identification in the limit
Information is presented to the learner who updates its
hypothesis after inspecting each piece of data
At some point, always, the learner will have found the correct
concept and not change from it
(Gold 1967 & 1978)
26

