by running GIZA++ (Och and Ney, 2003) in both
J03-1002 P03-1021
al.,2008; Tsengetal.,2005). Phraseswereextracted
I05-3027
David Chiang, Kevin Knight, and Wei Wang. 2009.
N09-1025
(Snow et al., 2008; Callison-Burch, 2009).
D08-1027 D09-1030
rewrite rules (Galley et al., 2006), or by making use
P06-1121
the grow-diag-final heuristic (Koehn et al., 2003).
N03-1017
BLEU (Papineni et al., 2002) uses the percentage
P02-1040
ments than BLEU (Snover et al., 2009; Przybocki
W09-0441
scrambling (Callison-Burch et al., 2006), and since
E06-1032
4Agarwal and Lavie (2008) report γ = 0.45, however the
W08-0312
error rate training (MERT) (Och, 2003) and mar-
J03-1002 P03-1021
Michel Galley and Christopher D. Manning. 2008. A
D08-1089
lation quality” (Och, 2003), these results show im-
J03-1002 P03-1021
Chris Callison-Burch. 2009. Fast, cheap, and creative:
D09-1030
Abhaya Agarwal and Alon Lavie. 2008. METEOR,
W08-0312
Omar F. Zaidan and Chris Callison-Burch. 2009. Feasi-
D09-1006 D09-1030
BLEU (Papineni et al., 2002) is relatively simple,
P02-1040
Franz Josef Och. 2003. Minimum error rate training in
J03-1002 P03-1021
the Berkeley cross-EM aligner (Liang et al., 2006).
N06-1014
TER-Plus (TERp) (Snover et al., 2009) extends
W09-0441
metric (Och, 2003). Learning such a model cannot
J03-1002 P03-1021
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
N06-1014
models (Koehn et al., 2007) but that models swaps
P07-2045
(Zaidan and Callison-Burch, 2009), which directly
D09-1006 D09-1030
Franz Josef Och and Hermann Ney. 2003. A system-
J03-1002 P03-1021
David Chiang, Yuval Marton, and Philip Resnik. 2008.
D08-1024
and Singer, 2003; Watanabe et al., 2007; Chiang et
D07-1080
(Koehn et al., 2003). However, we threw away all
N03-1017
