For example, the metonymical use of “door” to represent a whole building as in the phrase “girl next door” is under the same parent as sixty other synsets containing “bridge,” “balcony,” “body,” “arch,” “floor,” and “corner.” Surrounded by such common terms that are also likely to co-occur with the more conventional meanings of door, this very rare sense becomes the preferred disambiguation of “door.” 6 Related Work Abney and Light’s initial probabilistic WSD approach(1999)wasfurtherdevelopedintoaBayesian network model by Ciaramita and Johnson (2000), who likewise used the appearance of monosemous termsclosetoambiguousonesto“explainaway”the usage of ambiguous terms in selectional restrictions.
C00-1028 W99-0901
Like McCarthy (2004), our unsupervised system acquires a single predominant sense for a domain based on a synthesis of information derived from a 1031 textual corpus, topics, and WORDNET-derived similarity, a probabilistic information content measure.
P04-1036
Using the WORDNET hierarchy, we embed the construction of Abney and Light (1999) in the topic model and show that automatically learned domains improve WSD accuracy compared to alternative contexts.
W99-0901
Given the difficulties of using existing connections in WORDNET, Mihalcea (2005) proposed creating links between adjacent synsets that might comprise a sentence, initially setting weights to be equal to the Lesk overlap between the pairs, and then using the PageRank algorithm to determine the stationary distribution over synsets.
H05-1052
Of the two data sets used during the course of our evaluation, the primary dataset was SEMCOR (Miller et al., 1993), which is a subset of the Brown corpus with many nouns manually labeled with the correct WORDNET sense.
H93-1061
Topic models have recentlybeenappliedtoinformationretrieval(Weiand Croft, 2006), text classification (Blei et al., 2003), and dialogue segmentation (Purver et al., 2006).
P06-1003
As first described by Abney and Light (1999), we imagine an agent who starts at synset [entity], which points to every noun in WORDNET 2.1 by some sequence of hyponomy relations, and then chooses the next node in its random walk from the hyponyms of its current position.
W99-0901
This is equivalent to the need for balancing as noted by Abney and Light (1999).
W99-0901
