Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 271–276,
Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
Model-Portability Experiments for Textual Temporal Analysis 
Oleksandr Kolomiyets, Steven Bethard and Marie-Francine Moens 
Department of Computer Science 
Katholieke Universiteit Leuven 
Celestijnenlan 200A, Heverle, 3001, Belgium 
{oleksandr.kolomiyets, steven.bethard, sien.moens}@cs.kuleuven.be 
 
 
 
Abstract 
We explore a semi-supervised aproach for 
improving the portability of time expresion 
recognition to non-newswire domains: we 
generate additional training examples by 
substituting temporal expression words with 
potential synonyms. We explore using 
synonyms both from ordNet and from the 
Latent Words Language Model (LWLM), 
which predicts synonyms in context using 
an unsupervised approach. We evaluate a 
state-of-the-art time expression recognition 
system trained both with and without the 
additional training examples using data from 
TempEval 2010, Reuters and Wikipedia. 
We find that the LWLM provides substan-
tial improvements on the Reuters corpus, 
and smaller improvements on the Wikipedia 
corpus. We find that WordNet alone never 
improves performance, though intersecting 
the examples from the LWLM and WordNet 
provides more stable results for Wikipedia. 
1 Introduction

The recognition of time expresions such as April 
2011, mid-September and early next week is a cru-
cial first step for applications like question answer-
ing that must be able to handle temporally 
anchored queries. This need has inspired a variety 
of shared tasks for identifying time expresions, 
including the Mesage Understanding Conference 
named entity task (Grishman and Sundheim, 
1996), the Automatic Content Extraction time 
normalization task (http:/fofoca.mitre.org/tern.html) 
and the TempEval 2010 time expression task 
(Verhagen et al., 2010). Many resarchers com-
peted in these tasks, applying both rule-based and 
machine-learning aproaches (Mani and Wilson, 
2000; Negri and Marseglia, 204; Hacioglu et al., 
2005; Ahn et al., 2007; Poveda et al., 207; 
Strötgen and Gertz 2010; Llorens et al., 2010), and 
achieving F1 measures as high as 0.86 for recog-
nizing temporal expressions. 
Yet in most of these recent evaluations, models 
are both trained and evaluated on text from the 
same domain, typicaly newswire. Thus we know 
little about how well time expresion recognition 
systems generalize to other sorts of text. We there-
fore take a state-of-the-art time recognizer and eva-
luate it both on TempEval 2010 and on two new 
test sets drawn from Reuters and Wikipedia. 
At the same time, we are interested in helping 
the model recognize more types of time expres-
sions than are available explicitly in the newsire 
training data. We therefore introduce a semi-
supervised aproach for expanding the training 
data, where we take words from temporal expres-
sions in the data, substitute these words with likely 
synonyms, and ad the generated examples to the 
training set. We select synonyms both via Word-
Net, and via predictions from the Latent ords 
Language Model (LWLM) (Deschacht and Moens, 
2009). We then evaluate the semi-supervised mod-
el on the TempEval, Reuters and Wikipedia test 
sets and observe how wel the model has expanded 
its temporal vocabulary. 
271
2 Related
Work 
Semi-supervised aproaches have ben applied to a 
wide variety of natural language procesing tasks, 
including word sense disambiguation (Yarowsky, 
1995), named entity recognition (Collins and 
Singer, 199), and document clasification (Sur-
deanu et al., 206). 
The most relevant research to our work her is 
that of (Poveda et al., 209), which investigated a 
semi-supervised aproach to time expression rec-
ognition. They begin by selecting 100 time expres-
sions as seeds, selecting only expressions that are 
almost always annotated as times in the training 
half of the Automatic Content Extraction corpus. 
Then they begin an iterative proces where they 
search an unlabeled corpus for paterns given their 
seeds (with paterns consisting of surounding to-
kens, parts-of-spech, syntactic chunks etc.) and 
then search for new seds given their paterns. The 
paterns resulting from this iterative proces 
achieve F1 scores of up to 0.604 on the test half of 
the Automatic Content Extraction corpus. 
Our aproach is quite diferent from that of (Po-
veda et al., 2009) – we use our trainig corpus for 
learning a supervised model rather than for se-
lecting high precision seds, we generate adi-
tional training examples using synonyms rather 
than botstraping based on patterns, and we 
evaluate on Reuters and Wikipedia data that differ 
from the domain on which our model was trained. 
3 Method

The proposed method implements a supervised 
machine learning aproach that clasifies each 
chunk-phrase candidate top-down starting at the 
parse tre root provided by the OpenNLP parser. 
Time expresions are identified as phrasal chunks 
with spans derived from the parse as described in 
(Kolomiyets and Moens, 2010). 
3.1 Basic
TempEval Model 
We implemented a logistic regresion model with 
the following features for each phrase-candidate: 
• The head word of the phrase 
• The part-of-spech tag of the head word 
• All tokens and part-of-spech tags in the 
phrase as a bag of words 
• The word-shape representation of the head 
word and the entire phrase, e.g. Xxxxx 9 
for the expresion April 30 
• The condensed word-shape representation for 
the head word and the entire phrase, e.g. 
X(x) (9) for the expresion April 30 
• The concatenated string of the syntactic types 
of the children of the phrase in the parse tre 
• The depth in the parse tre 
3.2 Lexical
Resources for Bootstraping 
Sparsity of anotated corpora is the bigest chal-
lenge for any supervised machine learning tech-
nique and especialy for porting the trained models 
onto other domains. To overcome this problem we 
hypothesize that knowledge of semantically similar 
words, like temporal triggers, could be found by 
associating words that do not ocur in the training 
set to similar words that do ocur in the trainig 
set. Furthermore, we would like to learn these 
similarities automatically to be independent of 
knowledge sources that might not be available for 
all languages or domains. The first option is to use 
the Latent Words Language Model (LWLM) 
(Deschacht and Moens, 209) – a language model 
that learns from an unlabeled corpus how to pro-
vide a weighted set of synonyms for words in con-
text. The LWLM model is trained on the Reuters 
news article corpus of 80 milion words. 
WordNet (Miller, 195) is another resource for 
synonyms widely used in research and aplications 
of natural language procesing. Synonyms from 
WordNet sem to be very useful for botstraping 
as they provide replacement words to a specifc 
word in a particular sense. For each synset in 
WordNet there is a colection of other “sister” syn-
sets, caled cordinate terms, which are topologi-
cally located under the same hypernym. 
3.3 Bootstraping
Strategies 
Having a list of synonyms for each token in the 
sentence, we can replace one of the original tokens 
by its synonym while stil mostly preserving the 
sentence semantics. We chose to replace just the 
headword, under the asumption that since tempo-
ral triger words usualy ocur at the headword 
position, ading alternative synonyms for the 
headword should alow our model to learn tempo-
ral trigers that did not appear in the training data. 
272
We designed the folowing botstraping strate-
gies for generating new temporal expresions: 
• LWLM: the phrasal head is replaced by one of 
the LWLM synonyms. 
• WordNet 1
st
 Sense: Synonyms and cordinate 
terms for the most comon sense of the 
phrasal head are selected and used for generat-
ing new examples of time expresions. 
• WordNet Pseudo-Lesk: The synset for the 
phrasal head is selected as having the largest 
intersection betwen the synset’s words and 
the LWLM synoyms. Then, synonyms and 
coordinate terms are used for generating new 
examples of time expressions. 
• LWLM+WordNet: The intersection of the 
LWLM synonyms and the WordNet synset 
found by pseudo-Lesk are used. 
In this way for every anotated time expresion we 
generate n new examples (n∈[1,10]) and use them 
for training botstraped clasification models. 
4 Experimental
Setup 
The tested model is trained on the oficial Tem-
pEval 2010 training data with 53450 tokens and 
2117 annotated TIMEX3 tokens. For testing the 
portability of the model to other domains we ano-
tated two small target domain document collec-
tions with TIMEX3 tags. The first corpus is 12 
Reuters news articles from te Reuters corpus 
(Lewis et al., 204), containing 2960 total tokens 
and 240 annotated TIMEX3 tokens (inter-
annotator agrement 0.909 F1-score). The second 
corpus is the Wikipedia article for Bark Obama 
(http:/en.wikipedia.org/wiki/Obama), containing 
7029 total tokens and 512 anotated TIMEX3 to-
kens (inter-annotator agreement 0.901 F1-score). 
The basic TempEval model is evaluated on the 
source domain (TempEval 2010 evaluation set – 
9599 tokens in total and 269 TIMEX3 annotated 
tokens) and target domain data (Reuters and 
Wikipedia) using the TempEval 2010 evaluation 
metrics. Since porting the model onto other do-
mains usualy causes a performance drop, our ex-
periments are focused on improving the results by 
employing diferent botstrapping strategies
1
. 
5 Results

The recognition performance of the model is re-
ported in Table 1 (colun “Basic TepEval Mod-
el”) for the source and the target domains. The 
basic TempEval model itself achieves F1-score of 
0.834 on the official TempEval 2010 evaluation 
corpus and has a potential rank 8 among 15 par-
ticipated systems. The top seven TempEval-2 sys-
tems achieved F1-score betwen 0.83 and 0.86. 
                                                             
1
 The anotated datasets are available at 
http:/ww.cs.kuleuven.be/groups/lir/software.php 
Bootstraped Models  
 
Basic 
TempEval 
Model 
LWLM 
WordNet 1
st
 
Sense 
WordNet 
Pseudo-Lesk 
LWLM+ 
WordNet 
# Syn 0 1 1 1 2 
P 0.916 0.865 0.881 0.894 0.857 
R 0.770 0.807 0.773 0.781 0.830 
TempEval 2010 
F1 0.834 0.835 0.824 0.833 0.829 
# Syn 0 5 7 6 4 
P 0.896 0.841 0.820 0.839 0.860 
R 0.679 0.812 0.721 0.717 0.742 
Reuters 
F1 0.773 0.826 0.767 0.773 0.796 
# Syn 0 3 1 6 5 
P 0.959 0.924 0.922 0.909 0.913 
R 0.770 0.830 0.781 0.820 0.844 
Wikipedia 
F1 0.859 0.874 0.858 0.862 0.877 
Table 1: Precision, recall and F1 scores for all models on the source (TempEval 2010) and target (Reuters 
and Wikipedia) domains. Bootstrapped models were asked to generate betwen one and ten additional train-
ing examples per instance. The maximum P, R, F1 and the number of synonyms at which this maximum 
was achieved are given in the P, R, F1 and # Syn rows. F1 scores more than 0.010 above the Basic Tem-
pEval Model are marked in bold. 
273
However, this model does not port wel to the 
Reuters corpus (0.73 vs. 0.834 F1-score). For the 
Wikipedia-based corpus, the basic TempEval mod-
el actually performs a litle better than on the 
source domain (0.859 vs. 0.834 F1-score). 
Four botstraping strategies were proposed and 
evaluated. Table 1 shows the maximu F1 score 
achieved by each of these strategies, along with the 
number of generated synonyms (betwen one and 
ten) at which this maximum was achieved. None of 
the botstraped models outperformed the basic 
TempEval model on the TempEval 2010 evalua-
tion data, and the WordNet 1
st
 Sense strategy and 
the WordNet Pseudo-Lesk strategy never outper-
formed the basic TempEval model on any corpus. 
However, for the Reuters and Wikipedia cor-
pora, the LWLM and LWLM+WordNet bootstrap-
ping strategies outperformed the basic TempEval 
model. The LWLM strategy gives a large bost to 
model performance on the Reuters corpus from 
0.773 up to 0.826 (a 23.3% eror reduction) when 
using the first 5 synonyms. This puts performance 
on Reuters near performance on the TepEval 
domain from which the model was trained (0.834). 
This sugests that the (Reuters-trained) LWLM is 
finding exactly the right kinds of synonyms: those 
that were not originally present in the TempEval 
data but are present in the Reuters test data. On the 
Wikipedia corpus, the LWLM botstraping strat-
egy results in a moderate bost, from 0.859 up to 
0.874 (a 10.6% eror reduction) when using the 
first thre synonyms. Figure 1 shos that using 
more synonyms with this strategy drops perform-
ance on the Wikipedia corpus back down to the 
level of the basic TempEval model. 
The LWLM+WordNet strategy gives a moderate 
boost on the Reuters corpus from 0.773 up to 0.796 
(a 10.1% error reduction) when four synonyms are 
used. Figure 2 shows that using six or more syno-
nyms drops this performance back to just above the 
basic TempEval model. On the Wikipedia corpus, 
the LWLM+WordNet strategy results in a moder-
ate bost, from 0.859 up to 0.87 (a 12.8% error 
reduction), with five synonyms. Using additional 
synonyms results in a smal decline in perform-
ance, though even with ten synonyms, the per-
formance is beter than the basic TempEval model. 
In general, the LWLM strategy gives the best 
performance, while the LWLM+WordNet strategy 
is les sensitive to the exact number of synonyms 
used when expanding the training data. 
6 TempEval
Error Analysis 
We were curious why synonym-based boot-
straping did not improve performance on the 
source-domain TempEval 2010 data. A eror 
analysis sugested that some time expressions 
might have ben left unanotated by the human 
annotators. Two of the authors re-annotated the 
TempEval evaluation data, finding inter-annotator 
agrement of 0.912 F1-score with each other, but 
only 0.868 and 0.887 F1-score with the TempEval 
annotators, primarily due to unannotated time ex-
presions such as 23-year, a few days and third-
quarter. 
 
Figure 1: F1 score of the LWLM botstraping strat-
egy, generating from zero to ten additional training 
examples per instance. 
 
Figure 2: F1 score of the LWLM+WordNet botstrap-
ping strategy, generating from zero to ten additional 
training examples per instance. 
274
Using this re-annotated TempEval 2010 data
2, 
we re-evaluated the proposed botstrapping tech-
niques. Figure 3 and Figure 4 compare perform-
ance on the original TempEval data to performance 
on the re-annotated version. We now see the same 
trends for the TempEval data as were observed for 
the Reuters and Wikipedia corpora: using a small 
number of synonyms from the LWLM to generate 
new training examples leads to performance gains. 
The LWLM botstraping model using the first 
synonym achieves 0.861 F1 score, a 2.8% eror 
reduction over the baseline of 0.820 F1 score. 
7 Discusion
and Conclusions 
We have presented model-portability experiments 
on time expresion recognition with a number of 
bootstrapping strategies. These bootstrapping strat-
egies generate additional training examples by 
substituting temporal expression words with poten-
tial synonyms from two sources: WordNet and the 
Latent Word Language Model (LLM). 
Bootstrapping with LWLM synonyms provides 
a large bost for Reuters data and TempEval data 
and a decent bost for Wikipedia data when the top 
few synonyms are used. Aditional synonyms do 
not help, probably because they are too newswire-
specific: both the contexts from the TempEval 
training data and the synonyms from the Reuters-
trained LWLM come from newswire text, so the 
                                                             
2
 Available at 
http:/ww.cs.kuleuven.be/groups/lir/software.php 
lower synonyms are probably more domain-
specific. 
Intersecting the synonyms generated by the 
LWLM and by WordNet moderates the LWLM, 
making the botstraping strategy less sensitive to 
the exact number of synonyms used. However, 
while the intersected model performs as wel as the 
LWLM model on Wikipedia, the gains over the 
non-bootstrapped model on Reuters and TempEval 
data are smaler. 
Overal, our results show that when porting time 
expression recognition models to other domains, a 
performance drop can be avoided by synonym-
based bootstraping. Future work will focus on 
using synonym-based expansion in the contexts 
(not just the time expresions headwords), and on 
incorporating contextual information and syntactic 
transformations. 
Acknowledgments 
This work has ben funded by the Flemish gov-
ernment as a part of the project AMAS+ (Ad-
vanced Multimedia Alignment and Structured 
Sumarization) (Grant: IWT-SBO-060051). 
References 
David Ahn, Joris van Rantwijk, and Maarten de Rijke. 
2007. A Cascaded Machine Learning Aproach to 
Interpreting Temporal Expresions. In Procedings 
of the Annual Conference of the North American 
Chapter of the Asociation for Computational Lin-
guistics (NACL-HLT 207). 
 
Figure 3: F1 score of the LWLM botstraping strat-
egy, comparing performance on the original TempEval 
data to the re-annotated version. 
 
Figure 4: F1 score of the LWLM+WordNet botstrap-
ping strategy, comparing performance on the original 
TempEval data to the re-annotated version. 
275
Michael Colins and Yoram Singer. 199. Unsupervised 
odels for Named Entity Clasification. In Proced-
ings of the Joint SIGDAT Conference on Empirical 
Methods in Natural Language Procesing and Very 
Large Corpora, pp. 100–110, Colege Park, MD. 
ACL. 
Koen Deschacht and Marie-Francine Moens. 209. Us-
ing the Latent Words Language odel for Semi-
Supervised Semantic Role Labeling. In Procedings 
of the 2009 Conference on Empirical Methods in 
Natural Language Procesing. 
Ralph Grishman and Beth Sundheim. 196. Mesage 
Understanding Conference-6: A Brief History. In 
Procedings of the 16th Conference on Computa-
tional Linguistics, pp. 46–471. 
Kadri Hacioglu, Ying Chen, and Benjamin Douglas 
2005. Automatic Time Expresion Labeling for Eng-
lish and Chinese Text. In Gelbukh, A. (ed.) CICLing 
2005. LNCS, vol. 3406, p. 548–559. Springer, Hei-
delberg. 
Oleksandr Kolomiyets, Marie-Francine Moens. 2010. 
KUL: Recognition and Normalization of Temporal 
Expresions. In Procedings of SemEval-2 5th Work-
shop on Semantic Evaluation. p. 325-328. Uppsala, 
Sweden. ACL. 
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan 
Li. 204. RCV1: A New Benchmark Colection for 
Text Categorization Research. Machine Learning Re-
search. 5: 361-397 
Inderjet Mani, and George Wilson. 200. Robust Tem-
poral Procesing of News. In Proceedings of the 38th 
Annual Meting on Association for Computational 
Linguistics, pp. 69-76, Moristown, NJ. ACL. 
George A. Miler. 195. WordNet: A Lexical Database 
for English. Comunications of the ACM, 38(1): 
39-41. 
Mateo Negri, and Luca Marseglia. 204. Recognition 
and Normalization of Time Expressions: ITC-irst at 
TERN 204. Technical Report, ITC-irst, Trento. 
Hector Llorens, Estela Saquete, and Borja Navaro. 
2010. TIPSem (English and Spanish): Evaluating 
CRFs and Semantic Roles in TempEval 2. In Pro-
ceedings of the 5th International Workshop on Se-
mantic Evaluation, pp. 284–291, Uppsala, Sweden. 
ACL. 
Jordi Poveda, Mihai Surdeanu, and Jordi Turmo. 207. 
A Comparison of Statistical and Rule-Induction 
Learners for Automatic Taging of Time Expresions 
in English. In Procedings of the International Sym-
posium on Temporal Representation and Reasoning, 
pp. 141-149. 
Jordi Poveda, Mihai Surdeanu, and Jordi Turmo. 209. 
An Analysis of Botstraping for the Recognition of 
Temporal Expresions. In Procedings of the NAACL 
HLT 209 Workshop on Semi-Supervised Learning 
for Natural Language Procesing, p. 49-57, 
Stroudsburg, PA, USA. ACL. 
Janik Strötgen and Michael Gertz. 2010. HeidelTime: 
High Quality Rule-Based Extraction and Normaliza-
tion of Temporal Expresions. In Procedings of the 
5th International Workshop on Semantic Evaluation, 
pp. 321–324, Uppsala, Sweden. ACL. 
Mihai Surdeanu, Jordi Turmo, and Alicia Ageno. 206. 
A Hybrid Approach for the Acquisition of Informa-
tion Extraction Patterns. In Procedings of the EACL 
2006 Workshop on Adaptive Text Extraction and 
Mining (ATEM 206). ACL. 
Marc Verhagen, Roser Sauri, Tomaso Caseli, and 
James Pustejovsky. 2010. SemEval-2010 Task 13: 
TempEval 2. In Procedings of the 5th International 
Workshop on Semantic Evaluation, pp. 57–62, Upp-
sala, Sweden. ACL. 
	  David Yarowsky. 195. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 3rd Anual Meeting of the 
Association for Computational Linguistics, pp. 189–
196, Cambridge, MA. ACL. 
 
 
276

