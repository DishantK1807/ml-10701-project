Proceedings of BioNLP Shared Task 2011 Workshop, pages 130–137,
Portland, Oregon, USA, 24 June, 2011. c©2011 Association for Computational Linguistics
ComplexBiologicalEvent Extractionfrom Full Text using Signatures of
Linguisticand SemanticFeatures
Liam R. McGrath and Kelly Domico and CourtneyD. Corley and Bobbie-Jo Webb-Robertson
Pacific NorthwestNationalLaboratory
902 BattelleBLVD, PO BOX 999
Richland,WA 99352
{liam | kelly.domico| court | bj}@pnl.gov
Abstract
Building on technical advances from the
BioNLP 2009 Shared Task Challenge, the
2011 challenge sets forth to generalize tech-
niques to other complex biological event ex-
traction tasks. In this paper, we present the
implementationand evaluationof a signature-
based machine-learningtechnique to predict
events from full texts of infectious disease
documents. Specifically, our approach uses
novel signatures composed of traditional lin-
guistic features and semantic knowledge to
predictevent triggersand theircandidateargu-
ments. Using a leave-one out analysis,we re-
port the contribution of linguisticand shallow
semanticfeaturesin the triggerpredictionand
candidateargumentextraction. Lastly, we ex-
amine evaluations and posit causes for errors
in our complex biologicalevent extraction.
1 Introduction
The BioNLP 2009 Shared Task (Kim et al., 2009)
was the first shared task to address fine-grainedin-
formationextraction for the bio-moleculardomain,
by defining a task involving extraction of event
types from the GENIAontology. The BioNLP2011
Shared Task ( (Kim et al., 2011)) series generalized
this defining a series of tasks involving more text
types, domains and target event types. Among the
tasksfor the new seriesis the Infection Diseasetask,
proposedand investigated by (Pyysalo et al., 2011;
Pyysaloet al., 2010; Bjorneet al., 2010).
Like the other tasks for the BioNLPShared Task
series, the goal is to extract mentions of relevant
events from biomedical publications. To extract
an event, the event trigger and all arguments must
be identified in the text by exact offset and typed
according to a given set of event and argument
classes (Miwa et al., 2010). Entity annotationsare
given for a set of entity types that fill many of the
arguments.
Herewe describePacificNorthwestNationalLab-
oratory’s (PNNL) submissionto the BioNLP 2011
InfectiousDiseaseshared task. We describethe ap-
proach and then discussresults, includingan analy-
sis of errors and contribution of variousfeatures.
2 Approach
Oursystemusesa signature-basedmachine-learning
approach. The system is domain-independent,
using a primary task description vocabulary and
training data to learn the task, but domain re-
sources can be incorporated as additional features
when available, as described here. The approach
can be broken down into 4 components: an au-
tomated annotation pipeline to provide the basis
for features, classification-basedtrigger identifica-
tion and argument identificationcomponents,and a
post-processingcomponentto apply semantic con-
straints. The UIMAframework1 is used to integrate
the componentsinto a pipelinearchitecture.
2.1 PrimaryTasks
A definitionof the events to be extracted is used to
define candidatesfor classificationand post-process
the results of the classification. First a list of
domain-specificentity classes is given. Entities of
1http://uima.apache.org/
130
Event Class Arguments
Gene expression Theme(Protein|Regulon-operon)
Transcription Theme(Protein|Regulon-operon)
Protein catabolism Theme(Protein)
Phosphorylation Theme(Protein),Site(entity)?
Localization Theme(coreentity),AtLoc(entity)?,ToLoc(entity)?
Binding Theme(coreentity)+,Site(entity)*
Regulation Theme(coreentity|event), Cause(coreentity|event)?, Site(entity)?,CSite(entity)?
Positive regulation Theme(coreentity|event), Cause(coreentity|event)?, Site(entity)?,CSite(entity)?
Negative regulation Theme(coreentity|event), Cause(coreentity|event)?, Site(entity)?,CSite(entity)?
Process Participant(coreentity)?
Table 1: Summaryof the target events. Type restrictionson fillers of each argument type are shown in parenthesis.
Multiplicityof each argumenttype is also marked (+ = one-to-many, ? = zero-to-one,* = zero-to-many, otherwise=
one).
theseclassesare assumedto be annotatedin the data,
as is the case for the InfectiousDiseasetask. Then,
each event class is given, with a list of argument
types for each. Each argument is marked with its
multiplicity, indicatinghow many of this argument
typeis valid for eachevent, either: one – exactlyone
is required,one-to-many – one or more is required,
zero-to-one – one is optional, and zero-to-many –
one or many are optional. Also, restrictionson the
classes of entities that can fill each argument are
given, by listing: one or more class names– indicat-
ing the valid domain-specificentityclassesfrom the
definition,core entity – indicatingthat any domain-
specificentity in the definitionis valid, event – indi-
catingthat any event in the definitionis valid, or en-
tity – indicatingthat any span from the text is valid.
Table 1 shows the summaryof the event extraction
tasks for the InfectiousDiseasetrack.
2.2 Annotation
Linguisticand domainannotationsare automatically
applied to the document to be used for trigger and
argumentidentificationin framingthe tasksfor clas-
sificationand generatingfeatures for each instance.
Linguistic annotations include sentence splits, to-
kens, parts of speech, tree parses, typed dependen-
cies (deMarneffe et al., 2006; MacKinlay et al.,
2009), and stems. For the Infectious Disease task,
the parsesfromthe StanfordParser(Kleinand Man-
ning, 2003) provided by the Supporting Analysis
(Stenetorp et al., 2011) was used to obtain all of
these linguistic annotations, except for the stems,
which were obtainedfrom the Porter Stemmer(van
Rijsbergen et al., 1980).
For the Infectious Disease task, two sets of do-
main specific annotations are included: known
trigger words for each event class and semantic
tags from the Unified Medical Language System
(UMLS) (Bodenreider, 2004). Annotations for
known trigger words are created using a dictionary
of word stem-event class pairs created from anno-
tated trainingdata. An entry is createdin the dictio-
nary every time a new stem is seen as a trigger for
an event class. Whena word withone of thesestems
is seen duringprocessing,it is annotatedas a typical
triggerword for that event class.
Semantic tags are calculating using MetaMap
2010(Aronsonand Lang, 2010). MetaMapprovides
semantic tags for terms in a document with up to
three levels of specificity, from most to least spe-
cific: concept, type and group (Torii et al., 2011).
Word sense disambiguationis used to identify the
best tags for each term. For example, consider the
tags identified by MetaMap for the phrase Human
peripheral B cells:
Human
concept: Homosapiens
type: Human
group: Living Beings
Peripheral
type: SpatialConcept
group: Concepts& Ideas
B-Cells
concept: B-Lymphocytes
type: Cell
131
group: Anatomy
In this example,semanticmappingswere found for
three terms: Human, Peripheral and B-Cells. Hu-
man and B-Cellswere mappedto specificconcepts,
but Peripheral was mappedto a moregeneralgroup.
Entities are also annotated at this point. For the
Infectious Disease task, annotations for five entity
types are given: Protein, Two-component system,
Chemical,Organism,or Regulon/Operon.
2.3 TriggerIdentification
Triggersare identifiedusingan SVMclassifier(Vap-
nik, 1995; Joachims,1999). Candidatetriggers are
chosenfrom the words in the text by part-of-speech.
Basedon known triggersseenin the trainingdata,all
nouns, verbs, adjectives, prepositions and adverbs
are selectedas candidates.A binarymodelis trained
for each event type, and candidatetriggersare tested
against each classifier.
The following featuresare used to classifycandi-
date event triggers:
• term – the candidatetrigger
• stem – the stem of the term
• part of speech– the part of speechof the term
• capitalization– capitalizationof the term
• punctuation– individualfeaturesfor the pres-
ence of different punctuationtypes
• numerics – the presence of a number in the
term
• ngrams– 4-gramsof charactersfrom the term
• known triggertypes – tags from list of known
triggerterms for each event type
• lexicalcontext– terms in the same sentence
• syntactic dependencies – the type and role
(governoror dependent)of typeddependencies
involvingthe trigger
• semantictype – type mappingfrom MetaMap
• semantic group – group mapping from
MetaMap
For training data, both the Infectious Disease
training set and the GENIA training set were used.
Although the GENIA training set represents a dif-
ferent genre and is annotatedwith a slightly differ-
ent vocabularythan the InfectiousDiseasetask data,
it is similar enough to provide some beneficial su-
pervision. The Infectious Disease training data is
relatively small at 154 documentsso including the
larger GENIAtrainingset at 910 documentsresults
in a much more larger training set. Testing on the
InfectiousDisease developmentdata, a 1 point im-
provement in fscore in overall results is seen with
the additionaltrainingdata.
2.4 ArgumentIdentification
Argumentsare also identifiedusing an SVM classi-
fier. For each predictedtrigger, candidatearguments
are selected based on the argument types. For ar-
guments that are restricted to being filled by some
set of specific entity and event types, each anno-
tated entity and predictedevent is selectedas a can-
didate. For argumentsthat can be filled by any span
of text, each span correspondingto a constituent of
the tree parse is selected as a candidate. Each pair
of an event trigger and a candidateargumentserves
as an instancefor the classification.A binarymodel
is trainedfor each event type, and each pair is tested
against each classifier.
Many of the features used are inspired by those
used in semantic role labeling systems (Gildea and
Jurafsky, 2002). Given an event trigger and a can-
didate argument, the following features are used to
classifyevent arguments:
• trigger type – the predicted event type of the
trigger
• argumentterms– the text of the argument
• argument type – entity or event type annota-
tion on the argument
• argumentsuper-type – core entity or core ar-
gument
• trigger and argument stems – the stems of
each
• trigger and argument parts of speech – the
part of speechof each
• parse tree path – from the trigger to argument
via least common ancestor in tree parse, as a
list of phrasetypes
• voice of sentence– active or passive
• trigger and argument partial paths – from
the triggeror argumentto the leastcommonan-
cestorin tree parse, as a list of phrasetypes
132
• relative positionof argumentto trigger– be-
fore or after
• triggersub-categorization– representationof
the phrasestructurerule that describesthe rela-
tionship between the trigger, its parent and its
siblings.
The training data used is the same as for trig-
ger identification:the InfectiousDiseasetrainingset
plus the Geniatrainingset.
2.5 Post-processing
A post-processingcomponentis used to turn output
from the various classifiers into semanticallyvalid
output according to the target task. For each pre-
dictedtrigger, the positive predictionsfor each argu-
mentmodelare collected,and the set is comparedto
the argument restrictionsin the target task descrip-
tion.
For example, the types on argument predictions
are compared to the argument restrictions in the
target task, and non-conformingones are dropped.
Then the multiplicityof the argumentsfor each pre-
dicted event is checked against the task vocabulary.
Where there were not sufficient positive argument
predictions to make a full event, the best negative
predictionsfrom the model are tried. When a com-
pliant set of argumentscan not be createdfor a pre-
dictedevent, it is dropped.
3 Resultsand
Discussion
Resultsfor the systemon both the developmentdata
and the official test data for the task are shown in
Table 2 and Table 5, respectively. For the develop-
ment data, a system using gold-standardevent trig-
gers is included,to isolate the performanceof argu-
ment identification. In all cases, the total fscore for
non-regulationeventsweremuch higherthanregula-
tion events. On the official test data, the systemper-
formed the best in predicting Phosphorylation(fs-
core = 71.43),GeneExpression(fscore= 53.33)and
Process events (fscore = 51.04), but was unable to
find any Transcriptionand Regulation events. This
is also evidentin the resultson the developmentdata
using predicted triggers; additionally, no matches
were found for localizationand bindingevents. The
total fscoreon the developmentdata usinggold trig-
gers was 55.33, more than 13 points higher than
whenusingpredictedtriggers. In the discussionthat
follows, we detail the importanceof individual fea-
tures and their contribution to evaluationfscores.
3.1 Feature
Importance
The effect of each argumentand triggerfeaturetype
on the InfectiousDiseasedevelopmentdata was de-
termined using a leave-one-out approach. The ar-
gument and trigger feature effect results are shown
in Table 3 and Table 4, respectively. In a series of
experiments,each feature type is left out of the full
feature set one-by-one. The differencein fscore be-
tween each of these systemsand the full feature set
systemis the effect of the featuretype; a high nega-
tive effect indicatesa significantcontribution to the
systemsince the removal of the featureresultedin a
lower fscore.
Features fscore effect
all features 41.66
w/o argumentterms 36.16 -5.50
w/o argumenttype 39.50 -2.16
w/o triggerpartialpath 40.65 -1.01
w/o argumentpart of speech 40.98 -0.68
w/o argumentpartialpath 41.16 -0.50
w/o triggersub-categorization 41.45 -0.21
w/o argumentstem 41.48 -0.18
w/o argumentsuper-type 41.63 -0.03
w/o triggertype 41.63 -0.03
w/o triggerpart of speech 41.81 0.15
w/o triggerstem 41.81 0.15
w/o voice of sentence 41.85 0.19
w/o relative position 42.21 0.55
w/o parse tree path 42.67 1.01
Table 3: Effect of each argument feature type on Infec-
tious Diseasedevelopmentdata.
Within the argumentfeatureset system,the parse
tree path feature had a notable positive effect of
1.01. The features providing the greatest contribu-
tion were argument terms and argument type with
effects of -5.50 and -2.16, respectively. Within the
trigger feature set system, the lexical context and
syntacticdependenciesfeatures showed the highest
negative effectsignifyingpositive contributionto the
system. The text and known trigger types features
showed a negative contribution to the system.
133
UsingGold Triggers UsingPredictedTriggers
Event Class gold/ans./match recall prec. fscore gold/ans./match recall prec. fscore
Gene expression 134 / 110 / 100 74.63 90.00 81.60 134 / 132 / 85 64.18 64.39 64.29
Transcription 35 / 26 / 23 65.71 88.46 75.41 25 / 0 / 0 0.00 0.00 0.00
Protein catabolism 0 / 0 / 0 0.00 0.00 0.00 0 / 0 / 0 0.00 0.00 0.00
Phosphorylation 13 / 13 / 13 100.00 100.00 100.00 13 / 14 / 13 100.00 92.86 96.30
Localization 1 / 1 / 0 0.00 0.00 0.00 1 / 10 / 0 0.00 0.00 0.00
Binding 17 / 6 / 0 0.00 0.00 0.00 17 / 3 / 0 0.00 0.00 0.00
Process 206 / 180 / 122 59.22 67.78 63.21 207 / 184 / 108 52.17 58.70 55.24
Regulation 81 / 61 / 20 24.69 32.79 28.17 80 / 0 / 0 0.00 0.00 0.00
Positive regulation 113 / 91 / 36 31.86 39.56 35.29 113 / 42 / 13 11.50 30.95 16.77
Negative regulation 90 / 71 / 32 35.56 45.07 39.75 90 / 42 / 11 12.22 26.19 16.67
TOTAL 690 / 559 / 346 50.14 61.72 55.33 680 / 427 / 230 33.97 53.86 41.66
Table 2: Results on InfectiousDisease developmentdata. The system is comparedto a system using gold standard
triggersto isolateperformanceof argumentidentification.
Features fscore effect
all features 41.66
w/o lexical context 40.14 -1.52
w/o syntacticdependencies 40.28 -1.38
w/o ngrams 40.88 -0.78
w/o part of speech 41.48 -0.18
w/o capitalization 41.51 -0.15
w/o numerics 41.51 -0.15
w/o semanticgroup 41.55 -0.11
w/o punctuation 41.59 -0.07
w/o stem 41.74 0.08
w/o semantictype 41.82 0.16
w/o known triggertypes 42.11 0.45
w/o text 42.31 0.65
Table 4: Effect of each trigger feature type on Infectious
Diseasedevelopmentdata.
3.2 Transcriptionand
Regulationevents
Lastly, we presentrepresentative examplesof errors
(e.g., false positive, false negative, poor recall) pro-
duced by our system in the InfectiousDiseasetrack
core tasks. The discussion herein will cover eval-
uations where our system did not correctly predict
(transcriptionand regulation)any events or partially
predicted(bindingand +/regulation)event triggers
andarguments.In the text examplesthatfollow, trig-
gers are underlinedand argumentsare italicized.
The following are transcription events from the
documentPMC1804205-02-Results-03in the devel-
opmentdata.
• In contrast to the phenotype of the pta ackA
doublemutant,pbgPtranscriptionwas reduced
in the pmrD mutant(Fig. 3).
• Growth at pH 5.8 resulted in pmrD
transcriptlevels that were approximately3.5-
fold higher than in organisms grown at pH 7.7
(Fig. 4A).
In both the development and test data evaluations,
our system did not predict any transcription events,
resulting in a 0.0 fscore; however, the system
achieved 75.41 fscore when the gold-standardtrig-
gers were provided to the evaluation. Because ar-
gument predictionperformedwell, the system will
benefit most by improving transcriptionevent trig-
ger prediction.
The following are regulationevents from the doc-
ument PMC1804205-02-Results-01inthe develop-
ment data.
• . . . we grew Salmonellacells harbouringchro-
mosomallacZYA transcriptionalfusionsto the
PmrA-regulated genes pbgP, pmrC and ugd
(Wosten and Groisman, 1999) in N-minimal
mediabuffered at pH 5.8 or 7.7.
• We determinedthat Chelex 100 was effective at
chelatingiron becauseexpressionof the pmrA-
independentiron-repressediroA gene . . .
Similar to the transcriptiontask, our system did not
predict any regulation events, resulting in a 0.0 fs-
core. Unlike transcriptionevents though, our sys-
tem performedpoorly on both argument identifica-
tion and trigger prediction. The system achieved a
28.17 fscore when gold-standardtriggerswere used
134
Event Class gold (match) answer (match) recall prec. fscore
Gene expression 152 80 148 80 52.63 54.05 53.33
Transcription 50 0 0 0 0.00 0.00 0.00
Protein catabolism 5 1 12 1 20.00 8.33 11.76
Phosphorylation 16 10 12 10 62.50 83.33 71.43
Localization 7 4 22 4 57.14 18.18 27.59
Binding 56 7 14 7 12.50 50.00 20.00
Regulation 193 0 0 0 0.00 0.00 0.00
Positive regulation 193 34 87 34 17.62 39.08 24.29
Negative regulation 181 32 68 32 17.68 47.06 25.70
Process 516 234 401 234 45.35 58.35 51.04
TOTAL 1369 402 764 402 29.36 52.62 37.69
Table 5: Official resultson InfectiousDiseasetest data
in the evaluation. Hypothesesfor poor performance
on candidate argument prediction are addressed in
the following sections.
We posit that false negative triggeridentifications
are due to the limitedfull text trainingdata(i.e. tran-
scription events) and the inability of our system to
predict non-verb triggers (i.e. second transcription
example above). The SVM classifier was unable
to distinguishbetween true transcriptionevent trig-
gers and transcription-relatedterms and ultimately,
did not predict any transcriptionevent in the devel-
opment or test evaluations. To improve transcrip-
tion event prediction, immediate effort should fo-
cus on 1) providing additional training data (e.g.,
BioCreative˜citeBioCreative) and 2) introducea trig-
ger word filter that definesa subsetof event triggers
that have the best hit rate in the corpus. The hit rate
is the number of occurrencesof the word in a sen-
tenceper event type,dividedby the totalcountin the
gold standard(Nguyenet al., 2010).
3.3 +/-Regulationand Binding
The following positive regulationevent is from doc-
umentPMC1874608-03-RESULTS-03in the devel-
opmentdata.
• Invasiveness for HEp-2 cells was reduced to
39.1% of the wild-typelevel by mlc mutation,
whereas it was increased by 1.57-foldby hilE
mutation(Figure3B).
In the preceding example, our system correctly
predictedthe +regulationtriggerand the themehilE;
however, the correct argument was a gene expres-
sion event, not the entity. Many errorsin the positive
and negative regulationevents were of this type; the
predictedargumentwas a themeand not an event.
Evaluation of our system’s binding event predic-
tions resulted in low recall (12.50 or 0.0) in the
test and development evaluations. The proceeding
binding events are from document PMC1874608-
03-RESULTS-05 in the development data. In both
of the examples,our system correctlypredictedthe
trigger binding; however, no arguments were pre-
dicted. Evaluation on the development data with
gold standard triggers also resulted in an fscore of
0.0; thus, further algorithmrefinementis needed to
improve bindingscores.
• MlcdirectlyrepresseshilEbybindingto theP3
promoter
• These results clearly demonstrate that Mlc
can regulate directly the hilE P3 promoter by
bindingto the promoter.
The following binding event is from document
PMC1874608-01-INTRODUCTION in the devel-
opment data and is representative of errors across
many of the tasks. Here, the triggeris correctlypre-
dicted; however, the candidate arguments did not
match with the reference data. Upon closer look,
the argumentswere drawn from the entire sentence,
rather than an independent clause. The syntactic
parse feature was not sufficient to prevent over-
predictingarguments for the trigger, a potentialso-
lution is to add the argumentssyntacticdependency
135
to the trigger as a feature to the candidate argument
selection.
• Using two-hybrid analysis, it has been shown
that HilE interacts with HilD, which suggests
that HilE represseshilA expressionby inhibit-
ing the activity of HilD through a protein-
proteininteraction(19,20).
4 Summary
This articlereportsPacific NorthwestNationalLab-
oratory’s entry to the BioNLPSharedTask 2011 In-
fectiousDiseasetrackcompetition.Our systemuses
a signature-basedmachine-learningapproachincor-
porating traditional linguistic features and shallow
semantic concepts from NIH’s METAMAP The-
saurus. We examine the contribution of each of
the linguistic and semantic features to the over-
all fscore for our system. This approach performs
well on gene expression, process and phosphoryla-
tion event prediction. Transcription,regulation and
binding events each achieve low fscores and war-
rant further research to improve their effectiveness.
Lastly, we present a performance analysis of the
transcription,regulation and binding tasks. Future
work to improve our system’s performancecouldin-
clude pre-processingusing simplepatterns(Nguyen
et al., 2010),informationextractionfromfigurecap-
tions (Kim and Yu, 2011) and text-to-text event ex-
traction. The last suggestedimprovement is to add
semanticfeaturesto the candidateargumentpredic-
tionalgorithmin additionto usingrichfeatures,such
as semanticroles (Torii et al., 2011).
Acknowledgements
The authors thank the Signature Discovery Initia-
tive, part of the LaboratoryDirected Research and
DevelopmentProgramat PacificNorthwestNational
Laboratory(PNNL). PNNL is operated by Battelle
for the U.S. Department of Energy under contract
DE-ACO5-76RLO1830.
References
Alan R Aronson and Franc¸ois-MichelLang. 2010. An
overview of metamap: historical perspective and re-
centadvances. JAmMedInformAssoc, 17(3):229–36,
May.
J Bjorne, F Ginter, S Pyysalo,J Tsujii, and T Salakoski.
2010. Complex event extraction at pubmed scale.
Bioinformatics, 26(12):i382–i390,Jun.
O. Bodenreider. 2004. The unified medical language
system (UMLS): integrating biomedicalterminology.
Nucleicacidsresearch, 32(suppl1):D267.
M.C. deMarneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrasestructureparses. InProceedingsofLREC2006.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. ComputationalLinguistics,
28(3):245–288.
T. Joachims. 1999. Making large scale SVM learning
practical.AdvancesinKernelMethods– SupportVec-
torLearning.
Daehyun Kimand HongYu. 2011. Figuretext extraction
in biomedical literature.PLoSONE, 6(1):e15338,Jan.
JD Kim, T Ohta, S Pyysalo,Y Kano, and J Tsujii. 2009.
Overview of bionlp’09sharedtaskon event extraction.
ProceedingsoftheWorkshoponBioNLP:SharedTask,
pages 1–9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun’ichi Tsujii. 2011. Overview of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop CompanionVolume for
Shared Task, Portland, Oregon, June. Associationfor
ComputationalLinguistics.
D. Klein and C.D. Manning. 2003. Accurateunlexical-
ized parsing. In Proceedingsof the41stAnnualMeet-
ing on Association for Computational Linguistics-
Volume 1, pages 423–430. Associationfor Computa-
tional Linguistics.
A MacKinlay, D Martinez, and T Baldwin. 2009.
Biomedical event annotation with crfs and precision
grammars. Proceedingsof the Workshopon BioNLP:
SharedTask, pages 77–85.
Makoto Miwa, Rune Saetre,Jin-DongKim, and Jun’ichi
Tsujii. 2010. Event extraction with complex event
classificationusing rich features. J. Bioinform.Com-
put.Biol., 8(1):131–46,Feb.
Quang Long Nguyen, Domonkos Tikk, and Ulf Leser.
2010. Simple tricks for improving pattern-basedin-
formationextractionfrom the biomedicalliterature. J
BiomedSemantics, 1(1):9,Jan.
S. Pyysalo, T. Ohta, H.C. Cho, D. Sullivan, C. Mao,
B. Sobral,J. Tsujii,and S. Ananiadou.2010. Towards
event extractionfrom full texts on infectiousdiseases.
In Proceedingsof the 2010 Workshopon Biomedical
Natural Language Processing, pages 132–140.Asso-
ciationfor ComputationalLinguistics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun’ichi Tsujii, and Sophia Ananiadou. 2011.
136
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop CompanionVolume for
Shared Task, Portland, Oregon, June. Associationfor
ComputationalLinguistics.
PontusStenetorp,GoranTopi´c, SampoPyysalo,Tomoko
Ohta, Jin-Dong Kim, and Jun’ichi Tsujii. 2011.
BioNLPSharedTask 2011: SupportingResources. In
Proceedingsof the BioNLP2011WorkshopCompan-
ion Volume for Shared Task, Portland, Oregon, June.
Associationfor ComputationalLinguistics.
Manabu Torii, Lanlan Yin, Thang Nguyen, Chand T
Mazumdar, Hongfang Liu, David M Hartley, and
Noele P Nelson. 2011. An exploratorystudy of a text
classification framework for internet-based surveil-
lance of emerging epidemics. InternationalJournal
of MedicalInformatics, 80(1):56–66,Jan.
C.J. van Rijsbergen, S.E. Robertson, and M.F. Porter.
1980. New models in probabilistic information re-
trieval.
V. Vapnik. 1995. TheNatureofStatisticalLearningThe-
ory. Springer, New York.
137

