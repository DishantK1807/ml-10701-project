BusTUC A natural language bus route oracle Tore A m b l e Dept.
of computer and information science University of Trondheim Norway, N-7491 amble@idi, nt nu.
no Abstract The paper describes a natural language based expert system route advisor for the public bus transport in Trondheim, Norway.
The system is available on the Internet,and has been intstalled at the bus company's web server since the beginning of 1999.
The system is bilingual, relying on an internal language independent logic representation.
In between the question and the answer is a process of lexical analysis, syntax analysis, semantic analysis, pragmatic reasoning and database query processing.
One could argue that the information content could be solved by an interrogation, whereby the customer is asked to produce 4 items: s t a t i o n of departure, station of arrival, earliest departure timeand/or latest arrival time.
It Introduction A natural language interface to a computer database provides users with the capability of obtaining information stored in the database by querying the system in a natural language (NL).
With a natural language as a means of communication with a computer system, the users can make a question or a statement in the way they normally think about the information being discussed, freeing them from having to know how the computer stores or processes the information.
The present implementation represents a a major effort in bringing natural language into practical use.
A system is developed that can answer queries about bus routes, stated as natural language texts, and made public through the Internet World Wide Web is a myth that natural language is a better way of communication because it is "natural language".
The challenge is to prove by demonstration that an NL system can be made that will be preferred to the interrogative mode.
To do that, the system has to be correct, user friendly and almost complete within the actual domain.
P r e v i o u s Efforts, C H A T 8 0, P R A T 8 9 and HSQL Trondheim is a small city with a university and 140000 inhabitants.
Its central bus systems has 42 bus lines, serving 590 stations, with 1900 departures per day (in average).
T h a t gives approximately 60000 scheduled bus station passings per day, which is somehow represented in the route data base.
The starting point is to automate the function of a route information agent.
The following example of a system response is using an actual request over telephone to the local route information company: Hi, I live in Nidarvoll and tonight i must reach a train to Oslo at 6 oclock.
The system, called BusTUC is built upon the classical system CHAT-80 (Warren and Pereira, 1982).
CHAT-80 was a state of the art natural language system that was impressive on its own merits, but also established Prolog as a viable and competitive language for Artificial Intelligence in general.
The system was a brilliant masterpiece of software, efficient and sophisticated.
The natural language system was connected to a small query system for international geography.
The following query could be analysed and answered in a split second: Which country bordering the Mediterranean borders a country that is bordered by a country whose population exceeds the population of India?
(The answer 'Turkey' has become incorrect as time has passed.
The irony is that Geography was chosen as a domain without time.) and a typical answer would follow quickly: Bus number 54 passes by Nidarvoll skole at 1710 and arrives at Trondheim Railway Station at 1725.
The abi!ity to answer ridiculously long queries is of course not the main goal.
The main lesson is that complex sentences are analysed with a proper understanding without sacrificing efficiency.
Any superfificial pattern matching technique would prove futile sooner or later.
Making a N o r w e g i a n CHAT-80, PRAT-89 At the University of Trondheim (NTNU), two students made a Norwegian version of CHAT-80,called PRAT-89 (Teigen and Vetland, 1988),(Teigen and Vetland, 1989).
(Also, a similar Swedish project SNACK-85 was reported).
The dictionary was changed from English to Norwegian together with new rules for morphological analysis.
The change of grammar from English to Norwegian proved to be amazingly easy.
It showed that the langauges were more similar than one would believe, given that the languages are incomprehensible to each other's communities.
After changing the dictionary and graramar, the following Norwegian query about the same domain could be answered correctly in a few seconds.
Hvilke afrikanske land som hat en befolkning stoerre enn 3 millioner og mindre enn 50 millioner og er nord for Botswana og oest for Libya hat en hovedstad som hat en befolkning stoerre enn 100 tusen.
Coupling the s y s t e m to an SQL database.
After the remodelling, the system could answer queries in "Scandinavian" to an internal hospital database as well as CHAT-80 could answer Geography questions.
HSQL produced a Prolog-like code FOL (First Order Logic) for execution.
A mapping from FOL to the data base Schema was defined, and a translator from FOL to SQL was implemented.
The example Hvilke menn ligger i en kvinnes seng?
(Which men lie in a woman's bed?
) would be translated dryly into the SQL query: SELECT DISTINCT T3.name,Tl.sex,T2.reg_no,T3.sex, T4.reg_no,T4.bed_no,T5.hosp_no,T5.ward_no FROM PATIENT TI,OCCUPANCY T2,PATIENT T3, OCCUPANCY T4,WARD T5 WHERE (Tl.sex='f') AND (T2.reg_no=Tl.reg_no) AND (T3.sex='m') AND (T4.reg_no=T3.reg_no) AND (T4.bed_no=T2.bed_no) AND (T5.hosp_no=T4.hosp_no) AND (T5.ward_no=T4.ward_no) 2.3 T h e T h e U n d e r s t a n d i n g C o m p u t e r The HSQL was a valuable experience in the effort to make transportable natural language interfaces.
However, the underlying system CHAT-80 restricted the further development.
After the HSQL Project was finished, an internal reseach project TUC (the Understanding Computer) was initiated at NTNU to carry on the results from HSQL.
The project goals differed from those of HSQL in a number of ways, and would not be concerned with multimedia interfaces. On the other hand, portability and versatility were made central issues concerning the generality of the language and its applications.
The research goals could be summarised as to Give computers an operational understanding of natural language.
 Build intelligent systems with natural language capabilities.
 Study common sense reasoning in natural language.
A test criterion for the understanding capacity is that after a set of definitions in a Naturally Readable Logic, NRL, the system's answer to queries in NRL should conform to the answers of an idealised rational agent.
( A translation is beside the point o.f being a long query in Norwegian.) 2.2 HSQL H e l p S y s t e m for SQL A Nordic project HSQL (Help System for SQL) was accomplished in 1988-89 to make a joint Nordic effort interfaces to databases.
The HSQL project was led by the Swedish State Bureau (Statskontoret), with participants from Sweden, Denmark, Finland and Norway (Amble et al., 1990).
The aim of HSQL was to build a natural language interface to SQL databases for the Scandinavian languages Swedish, Danish and Norwegian.
These languages are very similar, and the Norwegian version of CHAT-80 was easily extended to the other Scandinavian languages.
Instead of Geography, a more typical application area was chosen to be a query system for hospital administration.
We decided to target an SQL database of a hospital administration which had been developed already.
The next step was then to change the domain of discourse from Geography to hospital administration, using the same knowledge representation techniques used in CHAT-80.
A semantic model of this domain was made, and then implemented in the CHAT-80 framework.
The modelling technique that proved adequate was to use an extended Entity Relationship (ER) model with a class (type) hierarchy, attributes belonging to each class, single inheritance of attributes and relationships.
Every man that lives loves Mary.
John is a man.
John lives.
Who loves Mary?
==> John 3 Anatomy of the bus route oracle The main components of the bus route information systems are:  A parser system, consisting of a dictionary, a lexical processor, a grammar and a parser.
 A knowledge base (KB), divided into a semantic KB and an application KB  A query processor, contalng a routing logic system, and a route data base.
The system is bilingual and contains a double set of dictionary, morphology and grammar.
Actually, it detects which language is most probable by counting the number of unknown words related to each language, and acts accordingly.
The grammars are surprisingly similar, but no effort is made to coalesce them.
The Norwegian grammar is slightly bigger than the English grammar, mostly because it is more elaborated but also because Norwegian allows a freer word order.
3.1 Features
of BusTUC For the Norwegian systems, the figures give an indication of the size of the domain: 420 nouns, 150 verbs, 165 adjectives, 60 prepositions, etc.
There are 1300 grammar rules ( 810 for English) although half of the rules are very low level.
The semantic net described below contains about 4000 entries.
A big name table of 3050 names in addition to the official station names, is required to capture the variety of naming.
A simple spell correction is a part of the system ( essentially 1 character errors).
The pragmatic reasoning is needed to translate the output from the parser to a route database query language . This is done by a production system called Pragma, which acts like an advanced rewriting system with 580 rules.
In addition, there is another rule base for actually generating the natural language answers (120 rules).
The system is mainly written in Prolog (Sicstus Prolog 3.7), with some Perl programs for the communication and CGI-scripts.
At the moment, there are about 35000 lines of programmed Prolog code (in addition to route tables which are also in Prolog).
Average response time is usually less than 2 seconds, but there are queries that demand up to 10 seconds.
The error rate for single, correct, complete and relevant questions is about 2 percent.
NRL is defined in a closed context.
Thus interfaces to other systems are in principle defined through simulating the environment as a dialogue partner.
TUC is a prototypical natural language processor for English written in Prolog.
It is designed to be a general purpose easily adaptable natural language processor.
It consists of a general grammar for a subset of English, a semantic knowledge base, and modules for interfaces to other interfaces like UNIX, SQL-databases and general textual information sources.
2.4 The
TABOR Project It so happened that a Universtity Project was starteded in 1996, called T A B O R ( " Speech based user interfaces and reasoning systems "), with the aim of building an automatic public transport route oracle, available over the public telephone.
At the onset of the project, the World Wide Web was fresh, and not as widespread as today, and the telephone was still regarded as the main source of information for the public.
Since then, the Internet became the dominant medium, and it is as likeley to find a computer with Internet connection, as to find a local busroute table.
( The consequtive wide spreading of cellular phones changed the picture in favour of the telephone, but that is another story).
It was decided that a text based information system should be built, regardless of the status of the speech rocgnition and speech synthesis effort, which proved to lag behind after a while.
The BusTUC system The resulting system BusTUC grew out as a natural application of TUC, and an English prototype could be built within a few months (Bratseth, 1997).
Since the summer 1996, the prototype was put onto the Internet, and been developed and tested more or less continually until today.
The most important extension was that the system was made bilingual (Norwegian and English) during the fall 1996.
In spring 1999, the BusTUC was finally adopted by the local bus company in Trondheim ( A/S Trondheim Trafikkselskap), which set up a server ( a 300 MHz PC with Linux).
Until today, over 150.000 questions have been answered, and BusTUC seems to stabilize and grow increasingly popular.
3.2 The
Parser S y s t e m The G r a m m a r S y s t e m The grammar is based on a simple grammar for statements, while questions and commands are derived by the use of movements.
The grammar 3 fiformalism which is called Consensical Grammar, (CONtext SENSitive CompositionAL Grammar) is an easy to use variant of Extraposition Grammar (Pereira and Warren, 1980), which is a generalisation of Definite Clause Grammars.
Compositional grammar means that the semantics of a a phrase is composed of the semantics of the subphrases; the basic constituents being a form of verb complements.
As for Extraposition grammars, a grammar is translated to Definite Clause Grammars, and executed as such.
A characteristic syntactic expression in Consensical G r a m m a r m a y define an incomplete construct in terms of a "difference " between complete constructs.
W h e n possible, the parser will use the subtracted part in stead of reading from the input, after a gap if necessary.
The effect is the same as for Exwhich is analysed as for which X is it true that the (X) person has a dog that barked? where the last line is analysed as a s t a t e m e n t . Movement is easily handled in Consensical Grammar without making special phrase rules for each kind of movement.
The following example shows how TUC manages a variety of analyses using movements: Max said Bill thought Joe believed Fido Barked.
Who said Bill thought Joe believed Fido barked?
Who did Max say thought Joe believed Fido barked? traposition grammars, but the this format is more intuitive.
Examples of grammar rules.
Who did Max say Bill thought believed Fido barked?
T h e parser The experiences with Consensical grammars are a bit mixed however.
The main problem is the parsing method itself, which is top down with backtracking.
Many principles that would prove elegant for small domains turned out to be too costly for larger domains, due to the wide variety of modes of expressions, incredible ambiguities and the sheer size of the covered language.
The disambiguation is a major problem for small grammars and large languages, and was solved by the following guidelines:  a semantic type checking was integrated into the parser, and would help to discard sematica/ly wrong parses from the start.
 a heuristics was followed that proved almost irreproachable: The longest possible phrase of a category that is semantically correct is in most cases the preferred interpretation.
 due to the perplexity of the language, some committed choices (cuts) had to be inserted into the grammar at strategic places.
As one could fear however, this implied that wrong choices being made at some point in the parsing could not be recovered by backtracking.
These problems also made it imperative to introduce a timeout on the parsing process of embarassing 10 seconds.
Although most sentences, would be parsed within a second, some legal sentences of moderate size actually need this time.
4 Example: Whose dog barked? is analysed as if the sentence had been Who has a dog t h a t barked? which is analysed as Which p e r s o n has a dog t h a t barked? fi3.3 The semantic knowledge base Adaptability means that the system does not need to be reprogrammed for each new application.
The design principle of TUC is that most of the changes are made in a tabular semantic knowledge base, while there is one general grammar and dictionary.
In general, the logic is generated automatically from the semantic knowledge base.
The nouns play a key role in the understanding part as they constitute the class or type hierarchy.
Nouns are defined in an a k i n d o f hierarchy.
The hierarchy is tree-structured with single inheritance.
The top level also constitute the top level ontology of TUC's world.
In fact, a type check of the compliances of verbs, nouns adjectives and prepositions is not only necessary for the semantic processing but is essential for the syntax analysis for the disambiguation as well.
In TUC, the legal combinations are carefully assembled in the semantic network, which then serves a dual purpose.
These semantic definitions are necessary to allow for instance the following sentences The dog saw a man with a telescope.
The man saw a dog with a telescope.
gives exactly the same code.
% Type of question % tuc is a program % A is a real bus % B isa saturday % Nidar is a place % D is an event Y.
C was known at D Y.
E is an event in C action(go,E), Y.
the action of E is Go actor(A,E), Y.
the actor of E is A srel(to,place,nidar,E),Y.
E is to nidar srel(on,time,B,E), y, E is on the saturday B to be treated differently because with telescope m a y modify the noun man but not the noun dog, while with telescope modifies the verb see, restricted to person.
The event parameter plays an important role in the semantics.
It is used for various purposes.
The most salient role is to identify a subset of time and space in which an action or event occured.
Both the actual time and space coordinates are connected to the actions through the event parameter.
Pragmatic reasoning The TQL is translated to a route database query language (BusLOG) which is actually a Prolog program.
This is done by a production system called Pragma, which acts like an advanced rewriting system with 580 rules.
In addition, there is another rule base for actually generating the natural language answers (120 rules).
4 Conclusions
The TUC approach has as its goal to automate the creation of new natural language interfaces for a well defined subset of the language and with a minimum of explicit programming.
The implemented system has proved its worth, and is interesting if for no other reason.
There is also an increasing interest from other bus companies and route information companies alike to get a similar system for their customers.
Further work remains to make the parser really efficient, and much work remains to make the language coverage complete within reasonable limits.
It is an open question whether the system of this kind will be a preferred way of offering information to the public.
If it is, it is a fair amount of work to make it a portable system that can be implemented elsewhere, also connecting various travelling agencies.
If not, it will remain a curiosity.
But anyway, a system like this will be a contribution to the development of intelligent systems.
3.4 The
Query Processor Event Calculus The semantics of the phrases are built up by a kind of verb complements, where the event play a central role.
The text is translated from Natural language into a form called TQL (Temporal Query Language/ TUC Query Language) which is a first order event calculus expression, a self contained expression containing the literal meaning of an utterance.
A formalism TQL that was defined, inspired by the Event Calculus by Kowalski and Sergot (Kowalski and Sergot, 1986).
The TQL expressions consist of predicates, functions, constants and variables.
The textual words of nouns and verbs are translated to generic predicates using the selected interpretation.
The following question Do you know whether the bus goes to Nidar on Saturday ? would give the TQL expression below.
BusTUC A natural language bus route oracle Tore A m b l e Dept.
of computer and information science University of Trondheim Norway, N-7491 amble@idi, nt nu.
no Abstract The paper describes a natural language based expert system route advisor for the public bus transport in Trondheim, Norway.
The system is available on the Internet,and has been intstalled at the bus company's web server since the beginning of 1999.
The system is bilingual, relying on an internal language independent logic representation.
In between the question and the answer is a process of lexical analysis, syntax analysis, semantic analysis, pragmatic reasoning and database query processing.
One could argue that the information content could be solved by an interrogation, whereby the customer is asked to produce 4 items: s t a t i o n of departure, station of arrival, earliest departure timeand/or latest arrival time.
It Introduction A natural language interface to a computer database provides users with the capability of obtaining information stored in the database by querying the system in a natural language (NL).
With a natural language as a means of communication with a computer system, the users can make a question or a statement in the way they normally think about the information being discussed, freeing them from having to know how the computer stores or processes the information.
The present implementation represents a a major effort in bringing natural language into practical use.
A system is developed that can answer queries about bus routes, stated as natural language texts, and made public through the Internet World Wide Web is a myth that natural language is a better way of communication because it is "natural language".
The challenge is to prove by demonstration that an NL system can be made that will be preferred to the interrogative mode.
To do that, the system has to be correct, user friendly and almost complete within the actual domain.
P r e v i o u s Efforts, C H A T 8 0, P R A T 8 9 and HSQL Trondheim is a small city with a university and 140000 inhabitants.
Its central bus systems has 42 bus lines, serving 590 stations, with 1900 departures per day (in average).
T h a t gives approximately 60000 scheduled bus station passings per day, which is somehow represented in the route data base.
The starting point is to automate the function of a route information agent.
The following example of a system response is using an actual request over telephone to the local route information company: Hi, I live in Nidarvoll and tonight i must reach a train to Oslo at 6 oclock.
The system, called BusTUC is built upon the classical system CHAT-80 (Warren and Pereira, 1982).
CHAT-80 was a state of the art natural language system that was impressive on its own merits, but also established Prolog as a viable and competitive language for Artificial Intelligence in general.
The system was a brilliant masterpiece of software, efficient and sophisticated.
The natural language system was connected to a small query system for international geography.
The following query could be analysed and answered in a split second: Which country bordering the Mediterranean borders a country that is bordered by a country whose population exceeds the population of India?
(The answer 'Turkey' has become incorrect as time has passed.
The irony is that Geography was chosen as a domain without time.) and a typical answer would follow quickly: Bus number 54 passes by Nidarvoll skole at 1710 and arrives at Trondheim Railway Station at 1725.
The abi!ity to answer ridiculously long queries is of course not the main goal.
The main lesson is that complex sentences are analysed with a proper understanding without sacrificing efficiency.
Any superfificial pattern matching technique would prove futile sooner or later.
Making a N o r w e g i a n CHAT-80, PRAT-89 At the University of Trondheim (NTNU), two students made a Norwegian version of CHAT-80,called PRAT-89 (Teigen and Vetland, 1988),(Teigen and Vetland, 1989).
(Also, a similar Swedish project SNACK-85 was reported).
The dictionary was changed from English to Norwegian together with new rules for morphological analysis.
The change of grammar from English to Norwegian proved to be amazingly easy.
It showed that the langauges were more similar than one would believe, given that the languages are incomprehensible to each other's communities.
After changing the dictionary and graramar, the following Norwegian query about the same domain could be answered correctly in a few seconds.
Hvilke afrikanske land som hat en befolkning stoerre enn 3 millioner og mindre enn 50 millioner og er nord for Botswana og oest for Libya hat en hovedstad som hat en befolkning stoerre enn 100 tusen.
Coupling the s y s t e m to an SQL database.
After the remodelling, the system could answer queries in "Scandinavian" to an internal hospital database as well as CHAT-80 could answer Geography questions.
HSQL produced a Prolog-like code FOL (First Order Logic) for execution.
A mapping from FOL to the data base Schema was defined, and a translator from FOL to SQL was implemented.
The example Hvilke menn ligger i en kvinnes seng?
(Which men lie in a woman's bed?
) would be translated dryly into the SQL query: SELECT DISTINCT T3.name,Tl.sex,T2.reg_no,T3.sex, T4.reg_no,T4.bed_no,T5.hosp_no,T5.ward_no FROM PATIENT TI,OCCUPANCY T2,PATIENT T3, OCCUPANCY T4,WARD T5 WHERE (Tl.sex='f') AND (T2.reg_no=Tl.reg_no) AND (T3.sex='m') AND (T4.reg_no=T3.reg_no) AND (T4.bed_no=T2.bed_no) AND (T5.hosp_no=T4.hosp_no) AND (T5.ward_no=T4.ward_no) 2.3 T h e T h e U n d e r s t a n d i n g C o m p u t e r The HSQL was a valuable experience in the effort to make transportable natural language interfaces.
However, the underlying system CHAT-80 restricted the further development.
After the HSQL Project was finished, an internal reseach project TUC (the Understanding Computer) was initiated at NTNU to carry on the results from HSQL.
The project goals differed from those of HSQL in a number of ways, and would not be concerned with multimedia interfaces. On the other hand, portability and versatility were made central issues concerning the generality of the language and its applications.
The research goals could be summarised as to Give computers an operational understanding of natural language.
 Build intelligent systems with natural language capabilities.
 Study common sense reasoning in natural language.
A test criterion for the understanding capacity is that after a set of definitions in a Naturally Readable Logic, NRL, the system's answer to queries in NRL should conform to the answers of an idealised rational agent.
( A translation is beside the point o.f being a long query in Norwegian.) 2.2 HSQL H e l p S y s t e m for SQL A Nordic project HSQL (Help System for SQL) was accomplished in 1988-89 to make a joint Nordic effort interfaces to databases.
The HSQL project was led by the Swedish State Bureau (Statskontoret), with participants from Sweden, Denmark, Finland and Norway (Amble et al., 1990).
The aim of HSQL was to build a natural language interface to SQL databases for the Scandinavian languages Swedish, Danish and Norwegian.
These languages are very similar, and the Norwegian version of CHAT-80 was easily extended to the other Scandinavian languages.
Instead of Geography, a more typical application area was chosen to be a query system for hospital administration.
We decided to target an SQL database of a hospital administration which had been developed already.
The next step was then to change the domain of discourse from Geography to hospital administration, using the same knowledge representation techniques used in CHAT-80.
A semantic model of this domain was made, and then implemented in the CHAT-80 framework.
The modelling technique that proved adequate was to use an extended Entity Relationship (ER) model with a class (type) hierarchy, attributes belonging to each class, single inheritance of attributes and relationships.
Every man that lives loves Mary.
John is a man.
John lives.
Who loves Mary?
==> John 3 Anatomy of the bus route oracle The main components of the bus route information systems are:  A parser system, consisting of a dictionary, a lexical processor, a grammar and a parser.
 A knowledge base (KB), divided into a semantic KB and an application KB  A query processor, contalng a routing logic system, and a route data base.
The system is bilingual and contains a double set of dictionary, morphology and grammar.
Actually, it detects which language is most probable by counting the number of unknown words related to each language, and acts accordingly.
The grammars are surprisingly similar, but no effort is made to coalesce them.
The Norwegian grammar is slightly bigger than the English grammar, mostly because it is more elaborated but also because Norwegian allows a freer word order.
3.1 Features
of BusTUC For the Norwegian systems, the figures give an indication of the size of the domain: 420 nouns, 150 verbs, 165 adjectives, 60 prepositions, etc.
There are 1300 grammar rules ( 810 for English) although half of the rules are very low level.
The semantic net described below contains about 4000 entries.
A big name table of 3050 names in addition to the official station names, is required to capture the variety of naming.
A simple spell correction is a part of the system ( essentially 1 character errors).
The pragmatic reasoning is needed to translate the output from the parser to a route database query language . This is done by a production system called Pragma, which acts like an advanced rewriting system with 580 rules.
In addition, there is another rule base for actually generating the natural language answers (120 rules).
The system is mainly written in Prolog (Sicstus Prolog 3.7), with some Perl programs for the communication and CGI-scripts.
At the moment, there are about 35000 lines of programmed Prolog code (in addition to route tables which are also in Prolog).
Average response time is usually less than 2 seconds, but there are queries that demand up to 10 seconds.
The error rate for single, correct, complete and relevant questions is about 2 percent.
NRL is defined in a closed context.
Thus interfaces to other systems are in principle defined through simulating the environment as a dialogue partner.
TUC is a prototypical natural language processor for English written in Prolog.
It is designed to be a general purpose easily adaptable natural language processor.
It consists of a general grammar for a subset of English, a semantic knowledge base, and modules for interfaces to other interfaces like UNIX, SQL-databases and general textual information sources.
2.4 The
TABOR Project It so happened that a Universtity Project was starteded in 1996, called T A B O R ( " Speech based user interfaces and reasoning systems "), with the aim of building an automatic public transport route oracle, available over the public telephone.
At the onset of the project, the World Wide Web was fresh, and not as widespread as today, and the telephone was still regarded as the main source of information for the public.
Since then, the Internet became the dominant medium, and it is as likeley to find a computer with Internet connection, as to find a local busroute table.
( The consequtive wide spreading of cellular phones changed the picture in favour of the telephone, but that is another story).
It was decided that a text based information system should be built, regardless of the status of the speech rocgnition and speech synthesis effort, which proved to lag behind after a while.
The BusTUC system The resulting system BusTUC grew out as a natural application of TUC, and an English prototype could be built within a few months (Bratseth, 1997).
Since the summer 1996, the prototype was put onto the Internet, and been developed and tested more or less continually until today.
The most important extension was that the system was made bilingual (Norwegian and English) during the fall 1996.
In spring 1999, the BusTUC was finally adopted by the local bus company in Trondheim ( A/S Trondheim Trafikkselskap), which set up a server ( a 300 MHz PC with Linux).
Until today, over 150.000 questions have been answered, and BusTUC seems to stabilize and grow increasingly popular.
3.2 The
Parser S y s t e m The G r a m m a r S y s t e m The grammar is based on a simple grammar for statements, while questions and commands are derived by the use of movements.
The grammar 3 fiformalism which is called Consensical Grammar, (CONtext SENSitive CompositionAL Grammar) is an easy to use variant of Extraposition Grammar (Pereira and Warren, 1980), which is a generalisation of Definite Clause Grammars.
Compositional grammar means that the semantics of a a phrase is composed of the semantics of the subphrases; the basic constituents being a form of verb complements.
As for Extraposition grammars, a grammar is translated to Definite Clause Grammars, and executed as such.
A characteristic syntactic expression in Consensical G r a m m a r m a y define an incomplete construct in terms of a "difference " between complete constructs.
W h e n possible, the parser will use the subtracted part in stead of reading from the input, after a gap if necessary.
The effect is the same as for Exwhich is analysed as for which X is it true that the (X) person has a dog that barked? where the last line is analysed as a s t a t e m e n t . Movement is easily handled in Consensical Grammar without making special phrase rules for each kind of movement.
The following example shows how TUC manages a variety of analyses using movements: Max said Bill thought Joe believed Fido Barked.
Who said Bill thought Joe believed Fido barked?
Who did Max say thought Joe believed Fido barked? traposition grammars, but the this format is more intuitive.
Examples of grammar rules.
Who did Max say Bill thought believed Fido barked?
T h e parser The experiences with Consensical grammars are a bit mixed however.
The main problem is the parsing method itself, which is top down with backtracking.
Many principles that would prove elegant for small domains turned out to be too costly for larger domains, due to the wide variety of modes of expressions, incredible ambiguities and the sheer size of the covered language.
The disambiguation is a major problem for small grammars and large languages, and was solved by the following guidelines:  a semantic type checking was integrated into the parser, and would help to discard sematica/ly wrong parses from the start.
 a heuristics was followed that proved almost irreproachable: The longest possible phrase of a category that is semantically correct is in most cases the preferred interpretation.
 due to the perplexity of the language, some committed choices (cuts) had to be inserted into the grammar at strategic places.
As one could fear however, this implied that wrong choices being made at some point in the parsing could not be recovered by backtracking.
These problems also made it imperative to introduce a timeout on the parsing process of embarassing 10 seconds.
Although most sentences, would be parsed within a second, some legal sentences of moderate size actually need this time.
4 Example: Whose dog barked? is analysed as if the sentence had been Who has a dog t h a t barked? which is analysed as Which p e r s o n has a dog t h a t barked? fi3.3 The semantic knowledge base Adaptability means that the system does not need to be reprogrammed for each new application.
The design principle of TUC is that most of the changes are made in a tabular semantic knowledge base, while there is one general grammar and dictionary.
In general, the logic is generated automatically from the semantic knowledge base.
The nouns play a key role in the understanding part as they constitute the class or type hierarchy.
Nouns are defined in an a k i n d o f hierarchy.
The hierarchy is tree-structured with single inheritance.
The top level also constitute the top level ontology of TUC's world.
In fact, a type check of the compliances of verbs, nouns adjectives and prepositions is not only necessary for the semantic processing but is essential for the syntax analysis for the disambiguation as well.
In TUC, the legal combinations are carefully assembled in the semantic network, which then serves a dual purpose.
These semantic definitions are necessary to allow for instance the following sentences The dog saw a man with a telescope.
The man saw a dog with a telescope.
gives exactly the same code.
% Type of question % tuc is a program % A is a real bus % B isa saturday % Nidar is a place % D is an event Y.
C was known at D Y.
E is an event in C action(go,E), Y.
the action of E is Go actor(A,E), Y.
the actor of E is A srel(to,place,nidar,E),Y.
E is to nidar srel(on,time,B,E), y, E is on the saturday B to be treated differently because with telescope m a y modify the noun man but not the noun dog, while with telescope modifies the verb see, restricted to person.
The event parameter plays an important role in the semantics.
It is used for various purposes.
The most salient role is to identify a subset of time and space in which an action or event occured.
Both the actual time and space coordinates are connected to the actions through the event parameter.
Pragmatic reasoning The TQL is translated to a route database query language (BusLOG) which is actually a Prolog program.
This is done by a production system called Pragma, which acts like an advanced rewriting system with 580 rules.
In addition, there is another rule base for actually generating the natural language answers (120 rules).
4 Conclusions
The TUC approach has as its goal to automate the creation of new natural language interfaces for a well defined subset of the language and with a minimum of explicit programming.
The implemented system has proved its worth, and is interesting if for no other reason.
There is also an increasing interest from other bus companies and route information companies alike to get a similar system for their customers.
Further work remains to make the parser really efficient, and much work remains to make the language coverage complete within reasonable limits.
It is an open question whether the system of this kind will be a preferred way of offering information to the public.
If it is, it is a fair amount of work to make it a portable system that can be implemented elsewhere, also connecting various travelling agencies.
If not, it will remain a curiosity.
But anyway, a system like this will be a contribution to the development of intelligent systems.
3.4 The
Query Processor Event Calculus The semantics of the phrases are built up by a kind of verb complements, where the event play a central role.
The text is translated from Natural language into a form called TQL (Temporal Query Language/ TUC Query Language) which is a first order event calculus expression, a self contained expression containing the literal meaning of an utterance.
A formalism TQL that was defined, inspired by the Event Calculus by Kowalski and Sergot (Kowalski and Sergot, 1986).
The TQL expressions consist of predicates, functions, constants and variables.
The textual words of nouns and verbs are translated to generic predicates using the selected interpretation.
The following question Do you know whether the bus goes to Nidar on Saturday ? would give the TQL expression below.
Machine Translation of Very Close Languages Jan HAJI(~ Computer Science Dept.
Johns Hopkins University 3400 N.
Charles St., Baltimore, MD 21218, USA hajic@cs.jhu.edu Jan HRIC KTI MFF UK Malostransk6 nfim.25 Praha 1, Czech Republic, 11800 hric@barbora.m ff.cuni.cz Vladislav KUBON OFAL MFF UK Malostransk6 mim.25 Praha 1, Czech Republic, 11800 vk@ufal.mff.cuni.cz Abstract Using examples of the transfer-based MT system between Czech and Russian RUSLAN and the word-for-word MT system with morphological disambiguation between Czech and Slovak (~ESILKO we argue that for really close languages it is possible to obtain better translation quality by means of simpler methods.
The problem of translation to a group of typologically similar languages using a pivot language is also discussed here.
demonstrate that this assumption holds only for really very closely related languages.
1. Czech-to-Russian MT system RUSLAN 1.1 History Introduction Although the field of machine translation has a very long history, the number of really successful systems is not very impressive.
Most of the funds invested into the development of various MT systems have been wasted and have not stimulated a development of techniques which would allow to translate at least technical texts from a certain limited domain.
There were, of course, exceptions, which demonstrated that under certain conditions it is possible to develop a system which will save money and efforts invested into human translation.
The main reason why the field of MT has not met the expectations of sci-fi literature, but also the expectations of scientific community, is the complexity of the task itself.
A successful automatic translation system requires an application of techniques from several areas of computational linguistics (morphology, syntax, semantics, discourse analysis etc).
as a necessary, but not a sufficient condition.
The general opinion is that it is easier to create an MT system for a pair of related languages.
In our contribution we would like to The first attempt to verify the hypothesis that related languages are easier to translate started in mid 80s at Charles University in Prague.
The project was called RUSLAN and aimed at the translation of documentation in the domain of operating systems for mainframe computers.
It was developed in cooperation with the Research Institute of Mathematical Machines in Prague.
At that time in former COMECON countries it was obligatory to translate any kind of documentation to such systems into Russian.
The work on the Czech-to-Russian MT system RUSLAN (cf.
Oliva (1989)) started in 1985.
It was terminated in 1990 (with COMECON gone) for the lack of funding.
System description The system was rule-based, implemented in Colmerauer's Q-systems.
It contained a fullfledged morphological and syntactic analysis of Czech, a transfer and a syntactic and morphological generation of Russian.
There was almost no transfer at the beginning of the project due to the assumption that both languages are similar to the extent that does not require any transfer phase at all.
This assumption turned to be wrong and several phenomena were covered by the transfer in the later stage of the project (for example the translation of the Czech verb "b~" [to be] into one of the three possible Russian equivalents: empty form, the form "byt6" in future fitense and the verb "javljat6sja"; or the translation of verbal negation).
At the time when the work was terminated in 1990, the system had a main translation dictionary of about 8000 words, accompanied by so called transducing dictionary covering another 2000 words.
The transducing dictionary was based on the original idea described in Kirschner (1987).
It aimed at the exploitation o f the fact that technical terms are based (in a majority o f European languages) on Greek or Latin stems, adopted according to the particular derivational rules o f the given languages.
This fact allows for the "translation" o f technical terms by means of a direct transcription of productive endings and a slight (regular) adjustment o f the spelling of the stem.
For example, the English words localization and discrimination can be transcribed into Czech as "lokalizace" and "diskriminace" with a productive ending -ation being transcribed to -ace.
It was generally assumed that for the pair Czech/Russian the transducing dictionary would be able to profit from a substantially greater number o f productive rules.
This hypothesis proved to be wrong, too (see B6mov~, Kubofi (1990)).
The set o f productive endings for both pairs (English/Czech, as developed for an earlier MT system from English to Czech, and Czech/Russian) was very similar.
The evaluation o f results o f RUSLAN showed that roughly 40% o f input sentences were translated correctly, about 40% with minor errors correctable by a human post-editor and about 20% of the input required substantial editing or re-translation.
There were two main factors that caused a deterioration of the translation.
The first factor was the incompleteness o f the main dictionary of the system.
Even though the system contained a set of so-called fail-soft rules, whose task was to handle such situations, an unknown word typically caused a failure o f the module o f syntactic analysis, because the dictionary entries contained besides the translation equivalents and morphological information very important syntactic information.
The second factor was the module of syntactic analysis o f Czech.
There were several reasons of parsing failures.
Apart from the common inability of most rule-based formal grammars to cover a particular natural language to the finest detail o f its syntax there were other problems.
One o f them was the existence of non-projective constructions, which are quite common in Czech even in relatively short sentences.
Even though they account only for 1.7/'o of syntactic dependencies, every third Czech sentence contains at least one, and in a news corpus, we discovered as much as 15 non-projective dependencies; see also Haji6 et al.(1998). An example o f a non-projective construction is "Soubor se nepodafilo otev~it".
[lit.: File Refl.
was_not._possible to_open.
It was not possible to open the file].
The formalism used for the implementation (Q-systems) was not meant to handle non-projective constructions.
Another source of trouble was the use o f so-called semantic features.
These features were based on lexical semantics o f individual words.
Their main task was to support a semantically plausible analysis and to block the implausible ones.
It turned out that the question o f implausible combinations o f semantic features is also more complex than it was supposed to be.
The practical outcome o f the use o f semantic features was a higher ratio of parsing failures semantic features often blocked a plausible analysis.
For example, human lexicographers assigned the verb 'to run' a semantic feature stating that only a noun with semantic features o f a human or other living being may be assigned the role o f subject of this verb.
The input text was however full o f sentences with 'programs' or 'systems' running etc.
It was o f course very easy to correct the semantic feature in the dictionary, but the problem was that there were far too many corrections required.
On the other hand, the fact that both languages allow a high degree o f word-order freedom accounted for a certain simplification o f the translation process.
The grammar relied on the fact that there are only minor word-order differences between Czech and Russian.
1.3 Lessons
learned from RUSLAN We have learned several lessons regarding the MT o f closely related languages:  The transfer-based approach provides a similar quality o f translation both for closely related and typologically different languages  Two main bottlenecks o f full-fledged transfer-based systems are: ficomplexity o f the syntactic dictionary relative unreliability o f the syntactic analysis of the source language Even a relatively simple component (transducing dictionary) was equally complex for English-to-Czech and Czech-to-Russian translation Limited text domains do not exist in real life, it is necessary to work with a high coverage dictionary at least for the source language.
2. Translation and localization 2.1 A pivot language Localization o f products and their documentation is a great problem for any company, which wants to strengthen its position on foreign language market, especially for companies producing various kinds o f software.
The amounts o f texts being localized are huge and the localization costs are huge as well.
It is quite clear that the localization from one source language to several target languages, which are typologically similar, but different from the source language, is a waste of money and effort.
It is o f course much easier to translate texts from Czech to Polish or from Russian to Bulgarian than from English or German to any o f these languages.
There are several reasons, why localization and translation is not being performed through some pivot language, representing a certain group o f closely related languages.
Apart from political reasons the translation through a pivot language has several drawbacks.
The most important one is the problem o f the loss o f translation quality.
Each translation may to a certain extent shift the meaning o f the translated text and thus each subsequent translation provides results more and more different from the original.
The second most important reason is the lack of translators from the pivot to the target language, while this is usually no problem for the translation from the source directly to the target language.
MAHT (Machine-aided human translation) systems.
We have chosen the TRADOS Translator's Workbench as a representative system o f a class o f these products, which can be characterized as an example-based translation tools.
IBM's Translation Manager and other products also belong to this class.
Such systems uses so-called translation memory, which contains pairs o f previously translated sentences from a source to a target language.
When a human translator starts translating a new sentence, the system tries to match the source with sentences already stored in the translation memory.
If it is successful, it suggests the translation and the human translator decides whether to use it, to modify it or to reject it.
The segmentation o f a translation memory is a key feature for our system.
The translation memory may be exported into a text file and thus allows easy manipulation with its content.
Let us suppose that we have at our disposal two translation memories one human made for the source/pivot language pair and the other created by an MT system for the pivot/target language pair.
The substitution o f segments o f a pivot language by the segments of a target language is then only a routine procedure.
The human translator translating from the source language to the target language then gets a translation memory for the required pair (source/target).
The system o f penalties applied in TRADOS Translator's Workbench (or a similar system) guarantees that if there is already a human-made translation present, then it gets higher priority than the translation obtained as a result o f the automatic MT.
This system solves both problems mentioned above the human translators from the pivot to the target language are not needed at all and the machinemade translation memory serves only as a resource supporting the direct human translation from the source to the target language.
3. M a c h i n e translation o f (very) closely related Slavic languages In the group o f Slavic languages, there are more closely related languages than Czech and Russian.
Apart from the pair o f Serbian and Croatian languages, which are almost identical and were Translation memory is the key The main goal of this paper is to suggest how to overcome these obstacles by means o f a combination of an MT system with commercial ficonsidered one language just a few years ago, the most closely related languages in this group are Czech and Slovak.
This fact has led us to an experiment with automatic translation between Czech and Slovak.
It was clear that application of a similar method to that one used in the system RUSLAN would lead to similar results.
Due to the closeness of both languages we have decided to apply a simpler method.
Our new system, (~ESILKO, aims at a maximal exploitation of the similarity of both languages.
The system uses the method of direct word-for-word translation, justified by the similarity of syntactic constructions of both languages.
Although the system is currently being tested on texts from the domain of documentation to corporate information systems, it is not limited to any specific domain.
Its primary task is, however, to provide support for translation and localization of various technical texts.
3.1 System
( ~ E S i L K O and its governing noun.
An alternative way to the solution of this problem was the application of a stochastically based morphological disambiguator (morphological tagger) for Czech whose success rate is close to 92/'0.
Our system therefore consists of the following modules: 1.
Import of the input from so-called 'empty' translation memory 2.
Morphological analysis of Czech 3.
Morphological disambiguation 4.
Domain-related bilingual glossaries (incl.
singleand multiword terminology) 5.
General bilingual dictionary 6.
Morphological synthesis of Slovak 7.
Export of the output to the original translation memory Letus now look in a more detail at the individual modules of the system: ad 1.
The input text is extracted out of a translation memory previously exported into an ASCII file.
The exported translation memory (of TRADOS) has a SGML-Iike notation with a relatively simple structure (cf.
the following example): Example 1.
A sample of the exported translation memory <RTF Preamble>...</RTF Preamble> <TrU> <CrD>23051999 <CrU>VK <Seg L=CS_01>Pomoci v~kazu ad-hoc m65ete rychle a jednoduge vytv~i~et regerge.
<Seg L=SK_01 >n/a </TrU> Our system uses only the segments marked by <Seg L=CS_01>, which contain one source language sentence each, and <Seg L=SK_01>, which is empty and which will later contain the same sentence translated into the target language The greatest problem of the word-for-word translation approach (for languages with very similar syntax and word order, but different morphological system) is the problem of morphological ambiguity of individual word forms.
The type of ambiguity is slightly different in languages with a rich inflection (majority of Slavic languages) and in languages which do not have such a wide variety of forms derived from a single lemma.
For example, in Czech there are only rare cases of part-of-speech ambiguities (st~t [to stay/the state], zena [woman/chasing] or tri [three/rub(imperative)]), much more frequent is the ambiguity of gender, number and case (for example, the form of the adjective jam[ [spring] is 27-times ambiguous).
The main problem is that even though several Slavic languages have the same property as Czech, the ambiguity is not preserved.
It is distributed in a different manner and the "form-for-form" translation is not applicable.
Without the analysis of at least nominal groups it is often very difficult to solve this problem, because for example the actual morphemic categories of adjectives are in Czech distinguishable only on the basis of gender, number and case agreement between an adjective by CESiLKO.
ad 2.
The morphological analysis of Czech is based on the morphological dictionary developed by Jan Haji6 and Hana Skoumalov~i in 1988-99 (for latest description, see Haji~ (1998)).
The dictionary contains over 700 000 dictionary entries and its typical coverage varies between fi99% (novels) to 95% (technical texts).
The morphological analysis uses the system of positional tags with 15 positions (each morphological.category, such as Part-of-speech, Number, Gender, Case, etc.
has a fixed, singlesymbol place in the tag).
Example 2 tags assigned to the word-form "pomoci" (help/by means of) pomoci: NFP2 ......
A ....
]NFS7 ......
A ....
I R--2 ........... where : N noun; R preposition F feminine gender S singular, P plural 7, 2 case (7 instrumental, 2 genitive) A affirmative (non negative) ad 3.
The module of morphological disambiguation is a key to the success o f the translation.
It gets an average number of 3.58 tags per token (word form in text) as an input.
The tagging system is purely statistical, and it uses a log-linear model of probability distribution see Haji~, Hladkfi (1998).
The learning is based on a manually tagged corpus of Czech texts (mostly from the general newspaper domain).
The system learns contextual rules (features) automatically and also automatically determines feature weights.
The average accuracy o f tagging is between 91 and 93% and remains the same even for technical texts (if we disregard the unknown names and foreign-language terms that are not ambiguous anyway).
The lemmatization immediately follows tagging; it chooses the first lemma with a possible tag corresponding to the tag selected.
Despite this simple lemmatization method, and also thanks to the fact that Czech words are rarely ambiguous in their Part-of-speech, it works with an accuracy exceeding 98%.
The multiple-word terms are sequences of lemmas (not word forms).
This structure has several advantages, among others it allows to minimize the size of the dictionary and also, due to the simplicity of the structure, it allows modifications of the glossaries by the linguistically naive user.
The necessary morphological information is introduced into the domain-related glossary in an off-line preprocessing stage, which does not require user intervention.
This makes a big difference when compared to the RUSLAN Czech-to-Russian MT system, when each multiword dictionary entry cost about 30 minutes of linguistic expert's time on average.
ad 5.
The main bilingual dictionary contains data necessary for the translation o f both lemmas and tags.
The translation of tags (from the Czech into the Slovak morphological system) is necessary, because due to the morphological differences both systems use close, but slightly different tagsets.
Currently the system handles the 1:1 translation of tags (and 2:2, 3:3, etc.).
Different ratio of translation is very rare between Czech and Siovak, but nevertheless an advanced system of dictionary items is under construction (for the translation 1:2, 2:1 etc.).
It is quite interesting that the lexically homonymous words often preserve their homonymy even after the translation, so no special treatment of homonyms is deemed necessary.
ad 6.
The morphological synthesis of Slovak is based on a monolingual dictionary of SIovak, developed by J.Hric (1991-99), covering more than ]00,000 dictionary entries.
The coverage of the dictionary is not as high as o f the Czech one, but it is still growing.
It aims at a similar coverage of Slovak as we enjoy for Czech.
ad 7.
The export o f the output of the system (~ESILKO into the translation memory (of TRADOS Translator's Workbench) amounts mainly to cleaning of all irrelevant SGML markers.
The whole resulting Slovak sentence is inserted into the appropriate location in the original translation memory file.
The following example also shows that the marker <CrU> contains an information that the target language sentence was created by an M T system.
ad 4.
The domain-related bilingual glossaries contain pairs of individual words and pairs of multiple-word terms.
The glossaries are organized into a hierarchy specified by the user; typically, the glossaries for the most specific domain are applied first.
There is one general matching rule for all levels of glossaries the longest match wins.
languages, namely for Czech-to-Polish translation.
Although these languages are not so similar as Czech and Slovak, we hope that an addition of a simple partial noun phrase parsing might provide results with the quality comparable to the fullfledged syntactic analysis based system RUSLAN (this is of course true also for the Czechoto-Slovak translation).
The first results of Czech-to Polish translation are quite encouraging in this respect, even though we could not perform as rigorous testing as we did for Slovak.
Acknowledgements 3.2 Evaluation of results The problem how to evaluate results of automatic translation is very difficult.
For the evaluation of our system we have exploited the close connection between our system and the TRADOS Translator's Workbench.
The method is simple the human translator receives the translation memory created by our system and translates the text using this memory.
The translator is free to make any changes to the text proposed by the translation memory.
The target text created by a human translator is then compared with the text created by the mechanical application of translation memory to the source text.
TRADOS then evaluates the percentage of matching in the same manner as it normally evaluates the percentage of matching of source text with sentences in translation memory.
Our system achieved about 90% match (as defined by the TRADOS match module) with the results of human translation, based on a relatively large (more than 10,000 words) test sample.
This project was supported by the grant GAt~R 405/96/K214 and partially by the grant GA(~R 201/99/0236 and project of the Ministry of Education No.
VS96151. The accuracy of the translation achieved by our system justifies the hypothesis that word-forword translation might be a solution for MT of really closely related languages.
The remaining problems to be solved are problems with the oneto many or many-to-many translation, where the lack of information in glossaries and dictionaries sometimes causes an unnecessary translation error.
A u t o m a t i c construction of parallel English-Chinese corpus for cross-language information retrieval Jiang Chen and Jian-Yun Nie D ~ p a r t e m e n t d ' I n f o r m a t i q u e et R e c h e r c h e O p ~ r a t i o n n e l l e Universit~ de M o n t r e a l C.P. 6128, succursale C E N T R E V I L L E M o n t r e a l (Quebec), C a n a d a H 3 C 3 J 7 {chen, nie} @iro.
umontreal, ca Abstract A major obstacle to the construction of a probabilistic translation model is the lack of large parallel corpora.
In this paper we first describe a parallel text mining system that finds parallel texts automatically on the Web.
The generated Chinese-English parallel corpus is used to train a probabilistic translation model which translates queries for Chinese-English cross-language information retrieval (CLIR).
We will discuss some problems in translation model training and show the preliminary C U R results.
1 Introduction
2 Parallel Text Mining Algorithm The PTMiner system is an intelligent Web agent that is designed to search for large amounts of parallel text on the Web.
The mining algorithm is largely language independent.
It can thus be adapted to other language pairs with only minor modifications.
Taking advantage of Web search engines as much as possible, PTMiner implements the following steps (illustrated in Fig.
1): 1 Search for candidate sites Using existing Web search engines, search for the candidate sites that may contain parallel pages; 2 File name fetching For each candidate site, fetch the URLs of Web pages that are indexed by the search engines; 3 Host crawling Starting from the URLs collected in the previous step, search through each candidate site separately for more URLs; 4 Pair scan From the obtained URLs of each site, scan for possible parallel pairs; 5 Download and verifying Download the parallel pages, determine file size, language, and character set of each page, and filter out non-parallel pairs.
2.1 Search
for candidate Sites We take advantage of the huge number of Web sites indexed by existing search engines in determining candidate sites.
This is done by submitting some particular requests to the search engines.
The requests are determined according to the following observations.
In the sites where parallel text exists, there are normally some pages in one language containing links to the parallel version in the other language.
These are usually indicated by those links' anchor texts 1.
For example, on some English page there may be a link to its Chinese version with the anchor text "Chinese Version" or "in Chinese".
1An a n c h o r t e x t is a piece of text on a W e b page which, w h e n clicked on, will take you to a n o t h e r linked page.
To be helpful, it u s u a l l y c o n t a i n s t h e key i n f o r m a t i o n about the linked page.
Parallel texts have been used in a number of studies in computational linguistics.
Brown et al.(1993) defined a series of probabilistic translation models for MT purposes.
While people may question the effectiveness of using these models for a full-blown MT system, the models are certainly valuable for developing translation assistance tools.
For example, we can use such a translation model to help complete target text being drafted by a human translator (Langlais et al., 2000).
Another utilization is in cross-language information retrieval (CLIR) where queries have to be translated from one language to another language in which the documents are written.
In CLIR, the quality requirement for translation is relatively low.
For example, the syntactic aspect is irrelevant.
Even if the translated word is not a true translation but is strongly related to the original query, it is still helpful.
Therefore, CLIR is a suitable application for such a translation model.
However, a major obstacle to this approach is the lack of parallel corpora for model training.
Only a few such corpora exist, including the Hansard English-French corpus and the HKUST EnglishChinese corpus (Wu, 1994).
In this paper, we will describe a method which automatically searches for parallel texts on the Web.
We will discuss the text mining algorithm we adopted, some issues in translation model training using the generated parallel corpus, and finally the translation model's performance in CLIR.
21 Figure 1: The workflow of the mining process.
T h e same phenomenon can be observed on Chinese pages.
Chances are t h a t a site with parallel texts will contain such links in some of its documents.
This fact is used as the criterion in searching for candidate sites.
Therefore, to determine possible sites for EnglishChinese parallel texts, we can request an English document containing the following anchor: Host Crawling anchor : "english version H ["in english",...].
Similar requests are sent for Chinese documents.
From the two sets of pages obtained by the above queries we extract two sets of Web sites.
T h e union of these two sets constitutes then the candidate sites.
T h a t is to say, a site is a candidate site when it is found to have either an English page linking to its Chinese version or a Chinese page linking to its English version.
A host crawler is slightly different from a Web crawler.
Web crawlers go through innumerable pages and hosts on the Web.
A host crawler is a Web crawler t h a t crawls through documents on a given host only.
A breadth-first crawling algorithm is applied in P T M i n e r as host crawler.
The principle is t h a t when a link to an unexplored document on the same site is found in a document, it is added to a list t h a t will be explored later.
In this way, most file names from the candidate sites are obtained.
Pair Scan File N a m e Fetching We now assume t h a t a pair of parallel texts exists on the same site.
To search for parallel pairs on a site, P T M i n e r first has to obtain all (or at least p a r t of) the H T M L file names on the site.
From these names pairs are scanned.
It is possible to use a Web crawler to explore the candidate sites completely.
However, we can take advantage of the search engines again to accelerate the process.
As the first step, we submit the following query to the search engines: to fetch the Web pages t h a t they indexed from this site.
If we only require a small a m o u n t of parallel texts, this result m a y be sufficient.
For our purpose, however, we need to explore the sites more thoroughly using a host crawler.
Therefore, we continue our search for files with a host crawler which uses the documents found by the search engines as the starting point.
After collecting file names for each candidate site, the next task is to determine the parallel pairs.
Again, we t r y to use some heuristic rules to guess which files m a y be parallel texts before downloading them.
The rules are based on external features of the documents.
By external feature, we mean those features which m a y be known without analyzing the contents of the file, such as its URL, size, and date.
This is in contrast with the internal features, such as language, character set, and H T M L structure, which cannot be known until we have downloaded the page and analyzed its contents.
The heuristic criterion comes from the following observation: We observe t h a t parallel text pairs usually have similar n a m e patterns.
The difference between the names of two parailel pages usually lies in a segment which indicates the language.
For example, "file-ch.html" (in Chinese) vs.
"file-en.html" (in English).
T h e difference m a y also appear in the path, such as ".../chinese/.../file.html" vs.
".../english/.../file.html'. T h e n a m e patterns described above are commonly used by webmasters to help organize their sites.
Hence, we can suppose t h a t a pair of pages with this kind of p a t t e r n are probably parallel texts.
First, we establish four lists for English prefixes, English suffixes, Chinese prefixes and Chinese suffixes.
For example: E n g l i s h P r e f i x = {e, en, e_, en_, e -, e n -, ...}.
For each file in one language, if a segment in its name corresponds to one of the language affixes, several new names are generated by changing the segment to the possible corresponding affixes of the other language.
If a generated name corresponds to an existing file, then the file is considered as a candidate parallel document of the original file.
Filtering Next, we further examine the contents of the paired files to determine if they are really parallel according to various external and internal features.
This may further improve the pairing precision.
The following methods have been implemented in our system.
usually have similar H T M L structures.
However, we also noticed that parallel texts may have quite different HTML structures.
One of the reasons is that the two files may be created using two HTML editors.
For example, one may be used for English and another for Chinese, depending on the language handling capability of the editors.
Therefore, caution is required when measuring structure difference numerically.
Parallel text alignment is still an experimental area.
Measuring the confidence values of an alignment is even more complicated.
For example, the alignment algorithm we used in the training of the statistical translation model produces acceptable alignment results but it does not provide a confidence value that we can "confidently" use as an evaluation criterion.
So, for the moment this criterion is not used in candidate pair evaluation.
Generated Corpus and Translation Model Training In this section, we describe the results of our parallel text mining and translation model training.
3 Text
Length Parallel files often have similar file lengths.
One simple way to filter out incorrect pairs is to compare the lengths of the two files.
The only problem is to set a reasonable threshold that will not discard too many good pairs, i.e. balance recall and precision.
The usual difference ratio depends on the language pairs we are dealing with.
For example, ChineseEnglish parallel texts usually have a larger difference ratio than English-French parallel texts.
The filtering threshold had to be determined empirically, from the actual observations.
For Chinese-English, a difference up to 50% is tolerated.
2.5.2 L
a n g u a g e a n d Character Set It is also obvious that the two files of a pair have to be in the two languages of interest.
By automatically identifying language and character set, we can filter out the pairs that do not satisfy this basic criterion.
Some Web pages explicitly indicate the language and the character set.
More often such information is omitted by authors.
We need some language identification tool for this task.
SILC is a language and encoding identification system developed by the RALI laboratory at the University of Montreal.
It employs a probabilistic model estimated on tri-grams.
Using these models, the system is able to determine the most probable language and encoding of a text (Isabelle et al., 1997).
2.5.3 H
T M L Structure and Alignment In the STRAND system (Resnik, 1998), the candidate pairs are evaluated by aligning them according to their H T M L structures and computing confidence values.
Pairs are assumed to be wrong if they have too many mismatching markups or low confidence values.
Comparing H T M L structures seems to be a sound way to evaluate candidate pairs since parallel pairs 23 The Corpus Using the above approach for Chinese-English, 185 candidate sites were searched from the domain hk.
We limited the mining domain to hk because Hong Kong is a bilingual English-Chinese city where high quality parallel Web sites exist.
Because of the small number of candidate sites, the host crawler was used to thoroughly explore each site.
The resulting corpus contains 14820 pairs of texts including 117.2Mb Chinese texts and 136.5Mb English texts.
The entire mining process lasted about a week.
Using length comparison and language identification, we refined the precision of the corpus to about 90%.
The precision is estimated by examining 367 randomly picked pairs.
Statistical Translation Model Many approaches in computational linguistics try to extract translation knowledge from previous translation examples.
Most work of this kind establishes probabilistic models from parallel corpora.
Based on one of the statistical models proposed by Brown et al.(1993), the basic principle of our translation model is the following: given a corpus of aligned sentences, if two words often co-occur in the source and target sentences, there is a good likelihood that they are translations of each other.
In the simplest case (model 1), the model learns the probability, p(tls), of having a word t in the translation of a sentence containing a word s.
For an input sentence, the model then calculates a sequence of words that are most probable to appear in its translation.
Using a similar statistical model, Wu (1995) extracted a largescale English-Chinese lexicon from the H K U S T corFigure 2: An alignment example using pure length-based method.
pus which is built manually.
In our case, the probabilistic translation model will be used for CLIR.
The requirement on our translation model may be less demanding: it is not absolutely necessary that a word t with high p(tls ) always be a true translation of s.
It is still useful if t is strongly related to s.
For example, although "railway" is not a true translation of "train" (in French), it is highly useful to include "railway" in the translation of a query on "train".
This is one of the reasons why we think a less controlled parallel corpus can be used to train a translation model for CLIR.
very noisy.
Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.
A number of alignment techniques have been proposed, varying from statistical methods (Brown et al., 1991; Gale and Church, 1991) to lexical methods (Kay and RSscheisen, 1993; Chen, 1993).
The method we adopted is t h a t of Simard et al.(1992). Because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length-based methods.
Cognates are identical sequences of characters in corresponding words in two languages.
T h e y are commonly found in English and French.
In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the H T M L markup in both texts are taken as cognates.
Because the H T M L structures of parallel pages are normally similar, the markup was found to be helpful for alignment.
To illustrate how markup can help with the alignment, we align the same pair with both the pure length-based method of Gale & Church (Fig.
2), and the method of Simard et al.(Fig. 3).
First of all, we observe from the figures that the two texts are Parallel Text Alignment Before the mined documents can be aligned into parallel sentences, the raw texts have to undergo a series of some preprocessing, which, to some extent, is language dependent.
For example, the major operations on the Chinese-English corpus include encoding scheme transformation (for Chinese), sentence level segmentation, parallel text alignment, Chinese word segmentation (Nie et al., 1999) and English expression extraction.
The parallel Web pages we collected from various sites are not all of the same quality.
Some are highly parallel and easy to align while others can be Figure 3: An alignment example considering cognates.
divided into sentences.
The sentences are marked by <s i d = " x x x x " > and < / s > . Note that we determine sentences not only by periods, but also by means of H T M L markup.
We further notice that it is difficult to align sentences 0002.
The sentence in the Chinese page is much longer than its counterpart in the English page because some additional information (font) is added.
The length-based method thus tends to take sentence 0002, 0003, and 0004 in the English page as the translation of sentence 0002 in the Chinese page (Fig.
2), which is wrong.
This in turn provocated the three following incorrect alignments.
As we can see in Fig.
3, the cognate method did not make the same mistake because of the noise in sentence 0002.
Despite their large length difference, the two 0002 sentences are still aligned as a 1-1 pair, because the sentences in the following 4 alignments (0003 0003; 0004 0004, 0005; 0005 0006; 0006 0007) have rather similar H T M L markups and are taken by the program to be the most likely alignments.
Beside HTML markups, other criteria may also be incorporated.
For example, it would be helpful to consider strong correspondence between certain English and Chinese words, as in (Wu, 1994).
We hope to implement such correspondences in our future research.
3.4 Lexicon
Evaluation To evaluate the precision of the English-Chinese translation model trained on the Web corpus, we examined two sample lexicons of 200 words, one in each direction.
The 200 words for each lexicon were randomly selected from the training source.
We examined the most probable translation for each word.
The Chinese-English lexicon was found to have a precision of 77%.
The English-Chinese lexicon has a higher precision of 81.5%.
Part of the lexicons are shown in Fig.
4, where t / f indicates whether a translation is true or false.
These precisions seem to be reasonably high.
They are quite comparable to that obtained by Wu (1994) using a manual Chinese-English parallel corpus.
Effect of Stopwords We also found that stop-lists have significant effect on the translation model.
Stop-list is a set of the most frequent words that we remove from the train3.5 English word t/f access adaptation add adopt agent agree airline amendment, appliance apply attendance auditor -,average base_on t t t t t t t t t t t/f t t t t t t t t t t Translation office protection report prepare local follow standard adult inadequate part financial visit bill vehicle saving Figure 4: Part of the evaluation lexicons.
Figure 5: Effect of stop lists in C-E translation.
ing source.
Because these words exist in most alignments, the statistical model cannot derive correct translations for them.
More importantly, their existence greatly affects the accuracy of other translations.
They can be taken as translations for many words.
A priori, it would seem that both the English and Chinese stop-lists should be applied to eliminate the noise caused by them.
Interestingly, from our observation and analysis we concluded that for better precision, only the stop-list of the target language should be applied in the model training.
We first explain why the stop-list of the target language has to be applied.
On the left side of Fig.
5, if the Chinese word C exists in the same alignments with the English word E more than any other Chinese words, C will be the most probable translation for E.
Because of their frequent appearance, some Chinese stopwords may have more chances to be in the same alignments with E.
The probability of the translation E --+ C is then reduced (maybe even less than those of the incorrect ones).
This is the reason why many English words are translated to " ~ ' (of) by the translation model trained without using the Chinese stop-list.
We also found that it is not necessary to remove the stopwords of the source language.
In fact, as illustrated on the right side of Fig.
5, the existence of the English stopwords has two effects on the probability of the translation E -~ C: 1 They may often be found together with the Chinese word C.
Owing to the Expectation Maximization algorithm, the probability of E -~ C may therefore be reduced.
2 On
the other hand, there is a greater likelihood that English stopwords will be found together with the most frequent Chinese words.
Here, we use the term "Chinese frequent words" instead of "Chinese stopwords" because even if a stop-list is applied, there may still remain some common words that have the same effect as the stopwords.
The coexistence of English and Chinese frequent words reduces the probability that the Chinese frequent words are the translations of E, and thus raise the probability of E -+ C.
The second effect was found to be more significant than the first, since the model trained without the English stopwords has better precision than the model trained with the English stopwords.
For the correct translations given by both models, the model Mono-Lingual IR Translation Model Dictionary trained without considering the English stopwords gives higher probabilities.
4 English-Chinese CLIR Results Our final goal was to test the performance of the translation models trained on the Web parallel corpora in CLIR.
We conducted CLIR experiments using the Smart IR system.
4.1 Results
The English test corpus (for C-E CLIR) was the AP corpus used in TREC6 and TREC7.
The short English queries were translated manually into Chinese and then translated back to English by the translation model.
The Chinese test corpus was the one used in the TREC5 and TREC6 Chinese track.
It contains both Chinese queries and their English translations.
Our experiments on these two corpora produced the results shown in Tab.
1. The precision of monolingual IR is given as benchmark.
In both E-C and C-E CLIR, the translation model achieved around 40% of monolingual precision.
To compare with the dictionary-based approach, we employed a ChineseEnglish dictionary, CEDICT (Denisowski, 1999), and an English-Chinese online dictionary (Anonymous, 1999a) to translate queries.
For each word of the source query, all the possible translations given by the dictionary are included in the translated query.
The Chinese-English dictionary has about the same performace as the translation model, while the English-Chinese dictionary has lower precision than that of the translation model.
We also tried to combine the translations given by the translation model and the dictionary.
In both C-E and E-C CLIR, significant improvements were achieved (as shown in Tab.
1). The improvements show that the translations given by the translation model and the dictionary complement each other well for IR purposes.
The translation model may give either exact translations or incorrect but related words.
Even though these words are not correct in the sense of translation, they are very possibly related to the subject of the query and thus helpful for IR purposes.
The dictionary-based approach expands a query along another dimension.
It gives all the possible translations for each word including those that are missed by the translation model.
4.2 C
o m p a r i s o n W i t h M T S y s t e m s One advantage of a parallel text-based translation model is that it is easier to build than an MT system.
Now that we have examined the CLIR performance of the translation model, we will compare it with two existing MT systems.
Both systems were tested in E-C CLIR.
4.2.1 S
u n s h i n e W e b T r a n Server Using the Sunshine WebTran server (Anonymous, 1999b), an online Engiish-Chinese MT system, to translate the 54 English queries, we obtained an average precision of 0.2001, which is 50.3% of the mono-lingual precision.
The precision is higher than that obtained using the translation model (0.1804) or the dictionary (0.1427) alone, but lower than the precison obtained using them together (0.2232).
4.2.2 Transperfect
Kwok (1999) investigated the CLIR performance of an English-Chinese MT software called Transperfect, using the same TREC Chinese collection as we used in this study.
Using the MT software alone, Kwok achieved 56% of monolingual precision.
The precision is improved to 62% by refining the translation with a dictionary.
Kwok also adopted pretranslation query expansion, which further improved the precison to 70% of the monolingual results.
In our case, the best E-C CLIR precison using the translation model (and dictionary) is 56.1%.
It is lower than what Kwok achieved using Transperfect, however, the difference is not large.
4.3 F
u r t h e r P r o b l e m s The Chinese-English translation model has a fax lower CLIR performance than that of the EnglishFrench model established using the same method (Nie et al., 1999).
The principal reason for this is the fact that English and Chinese are much more different than English and French.
This problem surfaced in many phases of this work, from text alignment to query translation.
Below, we list some further factors affecting CLIR precision.
 The Web-collected corpus is noisy and it is difficult to align English-Chinese texts.
The alignment method we employed has performed more poorly than on English-French alignment.
This in turn leads to poorer performance of the translation model.
In general, we observe a higher fivariability in Chinese-English translations than in English-French translations.
 For E-C CLIR, although queries in both languages were provided, the English queries were not strictly translated from the original Chinese ones.
For example, A J g, ~ (human right situation) was translated into human right issue.
We cannot expect the translation model to translate issue back to ~ (situation).
 The training source and the CLIR collections were from different domains.
The Web corpus are retrieved from the parallel sites in Hong Kong while the Chinese collection is from People's Daily and Xinhua News Agency, which are published in mainland China.
As the result, some important terms such as ~ $ $ (mostfavored-nation) and --I!!
~ ~ (one-nation-twosystems) in the collection are not known by the model.
5 Summary
The goal of this work was to investigate the feasibility of using a statistical translation model trained on a Web-collected corpus to do English-Chinese CLIR.
In this paper, we have described the algorithm and implementation we used for parallel text mining, translation model training, and some results we obtained in CLIR experiments.
Although further work remains to be done, we can conclude that it is possible to automatically construct a Chinese-English parallel corpus from the Web.
The current system can be easily adapted to other language pairs.
Despite the noisy nature of the corpus and the great difference in the languages, the evaluation lexicons generated by the translation model produced acceptable precision.
While the current CLIR results are not as encouraging as those of English-French CLIR, they could be improved in various ways, such as improving the alignment method by adapting cognate definitions to HTML markup, incorporating a lexicon and/or removing some common function words in translated queries.
We hope to be able to demonstrate in the near future that a fine-tuned English-Chinese translation model can provide query translations for CLIR with the same quality produced by MT systems.
D i s t i l l i n g dialogues A m e t h o d using natural dialogue c o r p o r a for dialogue s y s t e m s d e v e l o p m e n t Arne Department JSnsson and Nils Dahlb~ick of Computer and Information Science LinkSping University S-581 83, L I N K O P I N G SWEDEN nilda@ida.liu.se, arnjo@ida.liu.se Abstract We report on a method for utilising corpora collected in natural settings.
It is based on distilling (re-writing) natural dialogues to elicit the type of dialogue that would occur if one the dialogue participants was a computer instead of a human.
The method is a complement to other means such as Wizard of Oz-studies and un-distilled natural dialogues.
We present the distilling method and guidelines for distillation.
We also illustrate how the method affects a corpus of dialogues and discuss the pros and cons of three approaches in different phases of dialogue systems development.
1 Introduction
on measures for inter-rater reliability (Carletta, 1996), on frameworks for evaluating spoken dialogue agents (Walker et al., 1998) and on the use of different corpora in the development of a particular system (The Carnegie-Mellon Communicator, Eskenazi et al.(1999)). The question we are addressing in this paper is how to collect and analyse relevant corpora.
We begin by describing what we consider to be the main advantages and disadvantages of the two currently used methods; studies of human dialogues and Wizard of Oz-dialogues, especially focusing on the ecological validity of the methods.
We then describe a m e t h o d called 'distilling dialogues', which can serve as a supplement to the other two.
It has been known for quite some time now, that the language used when interacting with a computer is different from the one used in dialogues between people, (c.f.
JSnsson and Dahlb~ick (1988)).
Given that we know that the language will be different, but not how it will be different, we need to base our development of natural language dialogue systems on a relevant set of dialogue corpora.
It is our belief that we need to clarify a number of different issues regarding the collection and use of corpora in the development of speech-only and multimodal dialogue systems.
Exchanging experiences and developing guidelines in this area are as important as, and in some sense a necessary pre-requisite to, the development of computational models of speech, language, and dialogue/discourse.
It is interesting to note the difference in the state of art in the field of natural language dialogue systems with that of corpus linguistics, where issues of the usefulness of different samples, the necessary sampling size, representativeness in corpus design and other have been discussed for quite some time (e.g.
(Garside et al., 1997; Atkins et al., 1992; Crowdy, 1993; Biber, 1993)).
Also the neighboring area of evaluation of NLP systems (for an overview, see Sparck Jones and Galliers (1996)) seems to have advanced further.
Some work have been done in the area of natural language dialogue systems, e.g. on the design of Wizard of Oz-studies (Dahlb~ck et al., 1998), 2 Natural and Wizard of Oz-Dialogues The advantage of using real dialogues between people is that they will illustrate which tasks and needs that people actually bring to a particular service provider.
Thus, on the level of the users' general goals, such dialogues have a high validity.
But there are two drawbacks here.
First; it is not self-evident that users will have the same task expectations from a computer system as they have with a person.
Second, the language used will differ from the language used when interacting with a computer.
These two disadvantages have been the major force behind the development of Wizard of Ozmethods.
The advantage here is that the setting will be human-computer interaction.
But there are important disadvantages, too.
First, on the practical side, the task of setting up a high quality simulation environment and training the operators ('wizards') to use this is a resource consuming task (Dahlb~ck et al., 1998).
Second, and probably even more important, is that we cannot then observe real users using a system for real life tasks, where they bring their own needs, motivations, resources, and constraints to bear.
To some extent this problem can be overcome using well-designed so called 'scenarios'.
As pointed out in Dahlb~ck (1991), on many levels of analysis the artificiality of the situation will not affifect the language used.
An example of this is the pattern of pronoun-antecedent relations.
But since the tasks given to the users are often pre-described by the researchers, this means t h a t this is not a good way of finding out which tasks the users actually want to perform.
Nor does it provide a clear enough picture on how the users will act to find something t h a t satisfies their requirements.
If e.g. the task is one of finding a charter holiday trip or buying a TVset within a specified set of constraints (economical and other), it is conceivable t h a t people will stay with the first item t h a t matches the specification, whereas in real life they would probably look for alternatives.
In our experience, this is primarily a concern if the focus is on the users' goals and plans, but is less a problem when the interest is on lowerlevel aspects, such as, syntax or patterns of pronounantecedent relationship (c.f.
Dahlb~ick (1991)).
To summarize; real life dialogues will provide a reasonably correct picture of the way users' approach their tasks, and what tasks they bring to the service provider, but the language used will not give a good approximation of what the system under construction will need to handle.
Wizard of Ozdialogues, on the other hand, will give a reasonable approximation of some aspects of the language used, but in an artificial context.
The usual approach has been to work in three steps.
First analyse real h u m a n dialogues, and based on these, in the second phase, design one or more Wizard of Oz-studies.
The final step is to fine-tune the system's performance on real users.
A good example of this method is presented in Eskenazi et al.(1999). But there are also possible problems with this approach (though we are not claiming that this was the case in their particular project).
Eskenazi et al.(1999) asked a h u m a n operator to act 'computerlike' in their Wizard of Oz-phase.
The advantage is of course that the h u m a n operator will be able to perform all the tasks t h a t is usually provided by this service.
The disadvantage is t h a t it puts a heavy burden on the h u m a n operator to act as a computer.
Since we know that lay-persons' ideas of what computers can and cannot do are in m a n y respects far removed from what is actually the case, we risk introducing some systematic distortion here.
And since it is difficult to perform consistently in similar situations, we also risk introducing non-systematic distortion here, even in those cases when the 'wizard' is an NLP-professional.
Our suggestion is therefore to supplement the above mentioned methods, and bridge the gap between them, by post-processing h u m a n dialogues to give them a computer-like quality.
The advantage, compared to having people do the simulation on the fly, is both that it can be done with more consistency, and also that it can be done by researchers t h a t actually know what h u m a n c o m p u t e r natural language dialogues can look like.
A possible disadvantage with using both Wizard of Oz-and real computer dialogues, is that users will quickly a d a p t to what the system can provide t h e m with, and will therefore not try to use it for tasks they know it cannot perform.
Consequently, we will not get a full picture of the different services they would like the system to provide.
A disadvantage with this method is, of course, t h a t post-processing takes some time compared to using the natural dialogues as they are.
There is also a concern on the ecological validity of the results, as discussed later.
Distilling dialogues Distilling dialogues, i.e. re-writing h u m a n interactions in order to have them reflect what a humancomputer interaction could look like involves a number of considerations.
The main issue is t h a t in corp o r a of natural dialogues one of the interlocutors is not a dialogue system.
The system's task is instead performed by a h u m a n and the problem is how to anticipate the behaviour of a system that does not exist based on the performance of an agent with different performance characteristics.
One important aspect is how to deal with h u m a n features that are not part of what the system is supposed to be a b l e to handle, for instance if the user talks about things outside of the domain, such as discussing an episode of a recent T V show.
It also involves issues on how to handle situations where one of the interlocuters discusses with someone else on a different topic, e.g. discussing the up-coming Friday party with a friend in the middle of an information providing dialogue with a customer.
It is i m p o r t a n t for the distilling process to have at least an outline of the dialogue system t h a t is under development: Will it for instance have the capacity to recognise users' goals, even if not explicitly stated?
Will it be able to reason about the discourse domain?
W h a t services will it provide, and what will be outside its capacity to handle?
In our case, we assume that the planned dialogue system has the ability to reason on various aspects of dialogue and properties of the application.
In our current work, and in the examples used for illustration in this paper, we assume a dialogue model that can handle any relevant dialogue phenomenon and also an interpreter and speech recogniser being able to understand any user input that is relevant to the task.
There is is also a powerful domain reasoning module allowing for more or less any knowledge reasoning on issues that can be accomplished within the domain (Flycht-Eriksson, 1999).
Our current system does, however, not have an explicit user task model, as opposed to a system task model (Dahlb~ick fiand JSnsson, 1999), which is included, and thus, we can not assume that the 'system' remembers utterances where the user explains its task.
Furthermore, as our aim is system development we will not consider interaction outside the systems capabilities as relevant to include in the distilled dialogues.
The context of our work is the development a multi-modal dialogue system.
However, in our current work with distilling dialogues, the abilities of a multi-modal system were not fully accounted for.
The reason for this is that the dialogues would be significantly affected, e.g. a telephone conversation where the user always likes to have the n e x t connection, please will result in a table if multi-modal output is possible and hence a fair amount of the dialogne is removed.
We have therefore in this paper analysed the corpus assuming a speech-only system, since this is closer to the original telephone conversations, and hence needs fewer assumptions on system performance when distilling the dialogues.
distilling. The system might in such cases provide less information.
The principle of providing all relevant information is based on the assumption that a computer system often has access to all relevant information when querying the background system and can also present it more conveniently, especially in a multimodal system (Ahrenberg et al., 1996).
A typical example is the dialogue fragment in figure 1.
In this fragment the system provides information on what train to take and how to change to a bus.
The result of distilling this fragment provides the revised fragment of figure 2.
As seen in the fragment of figure 2 we also remove a number of utterances typical for human interaction, as discussed below.
* S y s t e m utterances are m a d e m o r e computer-like and do n o t include irrelevant i n f o r m a t i o n. The Distillation guidelines Distilling dialogues requires guidelines for how to handle various types of utterances.
In this section we will present our guidelines for distilling a corpus of telephone conversations between a human information provider on local buses 1 to be used for developing a multimodal dialogue system (Qvarfordt and JSnsson, 1998; Flycht-Eriksson and JSnsson, 1998; Dahlb~ick et al., 1999; Qvarfordt, 1998).
Similar guidelines are used within another project on developing Swedish Dialogue Systems where the domain is travel bureau information.
We can distinguish three types of contributors: 'System' (i.e.
a future systems) utterances, User utterances, and other types, such as moves by other speakers, and noise.
latter is seen in $9 in the dialogue in figure 3 where the provided information is not relevant.
It could also be possible to remove $5 and respond with $7 at once.
This, however, depends on if the information grounded in $5-U6 is needed for the 'system' in order to know the arrival time or if that could be concluded from U4.
This in turn depends on the system's capabilities.
If we assume that the dialogue system has a model of user tasks, the information in $5-U6 could have been concluded from that.
We will, in this case, retain $5-U6 as we do not assume a user task model (Dahlb/ick and JSnsson, 1999) and in order to stay as close to the original dialogue as possible.
The next problem concerns the case when 'system' utterances are changed or removed.
 Dialogue contributions provided by s o m e t h i n g or s o m e o n e other than the u s e r or the ' s y s t e m ' are removed.
These are regarded as not being part Modifying system utterances The problem of modifying 'system' utterances can be divided into two parts: how to change and when to change.
They are in some respects intertwined, but as the how-part affects the when-part more we will take this as a starting point.
 The ' s y s t e m ' provides as m u c h relevant inform a t i o n as possible at once.
This depends on the capabilities of the systems output modalities.
If we have a screen or similar output device we present as much as possible which normally is all relevant information.
If we, on the other hand, only have spoken output the amount of information that the hearer can interpret in one utterance must be considered when 1The bus time table dialogues are collected at LinkSping University and are available (in Swedish) on http://www.ida.liu.se/~arnjo/kfb/dialoger.html of the interaction.
This means that if someone interrupts the current interaction, say that the telephone rings during a face-to-face interaction, the interrupting interaction is normally removed from the corpus.
Furthermore, 'system' interruptions are removed.
A human can very well interrupt another human interlocuter, but a computer system will not do that.
However, this guideline could lead to problems, for instance, when users follow up such interruptions.
If no information is provided or the interrupted sequence does not affect the dialogue, we have no problems removing the interruption.
The problem is what to do when information from the 'system' is used in the continuing dialogue.
For such cases we have no fixed strategy, yes I wonder if you have any m m buses or ().
like express buses leaving from LinkSping to Vadstena ().
on sunday ja ville undra om ni hade ndgra 5h bussar eUer ().
typ expressbussar sore dkte frdn LinkSping till Vadstena ().
pd sSnda $5: no the bus does not run on sundays nej bussen g~r inte pd sSndagar U6: how can you ().
can you take the train and then change some way ().
because ().
to MjSlby 'n' so hur kan man ().
kan man ta tdg d sen byta p~ ndtt sStt ().
fSr de ().
till mjSlby ~ sd $7: that you can do too yes de kan du gSra ocksd ja U8: how ().
do you have any such suggestions hut ().
har du n~ra n~gra s~na fSrslag $9: yes let's see (4s) a m o m e n t (15s) now let us see here ().
was it on the sunday you should travel ja ska se h~ir (4s) eft 5gonblick (15s) nu ska vise hSr ().
va de p~ sSndagen du skulle dka pd U10: yes right afternoon preferably ja just de eftermidda ggirna $11: afternoon preferable ().
you have train from LinkSping fourteen twenty nine eftermidda gSrna ().
du hat t~g frdn LinkSping fjorton d tjugonie U12: m m mm S13: and then you will change from MjSlby station six hundred sixty sd byter du frdn MjSlby station sexhundrasexti sexhundrasexti $15: fifteen and ten Figure 1: Dialogue fragment from a real interaction on bus time-table information U4: S5: U6: $7: I wonder if you have any buses or ().
like express buses going from LinkSping to Vadstena ().
on sunday no the bus does not run on sundays how can you ().
can you take the train and then change some way ().
because ().
to MjSlby and so you can take the train from LinkSping fourteen and twenty nine and then you will change at MjSlby station to bus six hundred sixty at fifteen and ten Figure 2: A distilled version of the dialogue in figure 1 the dialogue needs to be rearranged depending on how the information is to be used (c.f.
the discussion in the final section of this paper).
in figure 4).
A common case of this is when the ' s y s t e m ' is talking while looking for information, $5 in the dialogue fragment of figure 4 is an example of this.
Related to this is when the system provides its own comments.
If we can assume that it has such capabilities they are included, otherwise we remove them.
 'System' utterances which are no longer valid are removed.
Typical examples of this are the utterances $7, $9, $11 and $13 in the dialogue fragment of figure 1.
* Remove sequences of utterances where the 'system' behaves in a way a computer would not do.
For instance jokes, irony, humor, commenting on the other dialogue participant, or dropping the telephone (or whatever is going on in $7 The system does not repeat information that has already been provided unless explicitly asked to do so.
In human interaction it is not uncommon to repeat what has been uttered for purposes other than to provide grounding information or feedback.
This is for instance common during 'n' I must be at Resecentrum before fourteen and thirty five ().
'cause we will going to the interstate buses $5: U6: $7: aha ().
'n' then you must be there around twenty past two something then yes around t h a t ja ungefgir let's see here ( l l s ) two hundred and fourteen R y d end station leaves forty six ().
thirteen 'n' forty six then you will be down fourteen oh seven (.) jaha 'n' ().
the next one takes you there ().
fourteen thirty seven ().
but t h a t is too late Figure 3: Dialogue fragment from a real interaction on bus time-table information U2: $3: U4: $5: U6: $7: Well, hi ().
I a m going to Ugglegatan eighth ja hej ().
ja ska till Ugglegatan dtta Yes ja and ().
I wonder ().
it is somewhere in Tannefors och ().
jag undrar ().
det ligger ndnstans i Tannefors Yes ().
I will see here one one I will look exactly where it is one m o m e n t please ja ().
jag ska se hhr eft eft jag ska titta exakt vat det ligger eft 6gonblick barn Oh Yeah (operator disconnects) (25s) m m ().
okey (hs) what the hell (2s) (operator connects again) hello yes ((Telefonisten kopplar ur sig)) (25s) iihh ().
okey (hs) de va sore ]aan (2s) ((Telefonisten kopplar in sig igen)) halld ja ja hej It is bus two hundred ten which runs on old tannefors road t h a t you have to take and get off at the bus stop at t h a t bus stop named vetegatan Figure 4: Dialogue fragment from a natural bus timetable interaction search procedures as discussed above.
want to develop systems where the user needs to restrict his/her behaviour to the capabilities of the dialogue system.
However, there are certain changes m a d e to user utterances, in most cases as a consequence of changes of system utterances.
 The system does not ask for information it has already achieved.
For instance asking again if it is on Sunday as in $9 in figure 1.
This is not uncommon in h u m a n interaction and such utterances from the user are not removed.
However, we can assume t h a t the dialogue system does not forget what has been talked about before.
4.2 M
o d i f y i n g u s e r u t t e r a n c e s The general rule is to change user utterances as little as possible.
The reason for this is that we do not Utterances that are no longer valid are removed.
The most common cases are utterances whose request has already been answered, as seen in the distilled dialogue in figure 2 of the dialogue in figure 1.
sixteen fifty five sexton ]emti/em U12: sixteen fifty five ().
aha sexton f e m t i / e m ().
jaha S13: bus line four hundred thirty five linje ]yrahundra tretti/em Figure 5: Dialogue fragment from a natural bus timetable interaction  Utterances are removed where the user discusses things that are in the environment.
For instance commenting the 'systems' clothes or hair.
This also includes other types of communicative signals such as laughter based on things outside the interaction, for instance, in the environment of the interlocuters.
 User utterances can also be added in order to make the dialogue continue.
In the dialogue in figure 5 there is nothing in the dialogue explaining why the system utters S13.
In such cases we need to add a user utterance, e.g.
Which bus is that?.
However, it might turn out that there are cues, such as intonation, found when listening to the tapes.
If such detailed analyses are carried out, we will, of course, not need to add utterances.
Furthermore, it is sometimes the case t h a t the telephone operator deliberately splits the information into chunks t h a t can be comprehended by the user, which then must be considered in the distillation.
5 Applying
the method To illustrate the m e t h o d we will in this section t r y to characterise the results from our distillations.
The illustration is based on 39 distilled dialogues from the previously mentioned corpus collected with a telephone operator having information on local bus time-tables and persons calling the information service.
The distillation took a b o u t three hours for all 39 dialogues, i.e. it is reasonably fast.
The distilled dialogues are on the average 27% shorter.
However, this varies between the dialogues, at most 73% was removed but there were also seven dialogues t h a t were not changed at all.
At the most 34 utterances where removed from one single dialogue and that was from a dialogue with discussions on where to find a parking lot, i.e. discussions outside the capabilities of the application.
There was one more dialogue where more t h a n 30 utterances were removed and that dialogue is a typical example of dialogues where distillation actually is very useful and also indicates what is normally removed from the dialogues.
This particular dialogue begins with the user asking for the telephone number to 'the Lost property office' for a specific bus operator.
However, the operator starts a discussion on what bus the traveller traveled on before providing the requested telephone number.
The reason for this discussion is probably t h a t the operator knows that different bus companies are utilised and would like to make sure that the user really understands his/her request.
The interaction t h a t follows can, thus, in t h a t respect be relevant, but for our purpose of developing systems based on an overall goal of providing information, not to understand human interaction, our dialogue system will not able to handle such phenomenon (JSnsson, 1996).
The dialogues can roughly be divided into five different categories based on the users task.
The discussion in twenty five dialogues were on bus times between various places, often one departure and one arrival but five dialogues involved more places.
In five dialogues the discussion was one price and various types of discounts.
Five users wanted to know the telephone number to 'the Lost property office', two discussed only bus stops and two discussed how they could utilise their season ticket to travel outside the trafficking area of the bus company.
It is interesting to note that there is no correspondence between the task being performed during the interaction and the amount of changes made to the d i a logue.
Thus, if we can assume that the amount of distillation indicates something about a user's interaction style, other factors t h a n the task are important when characterising user behaviour.
Looking at what is altered we find t h a t the most i m p o r t a n t distilling principle is that the 'system' provides all relevant information at once, c.f. figures 1 and 2.
This in turn removes utterances provided by both 'system' and user.
Most added utterances, both from the user and the 'system', provide explicit requests for information that is later provided in the dialogue, e.g. utterance $3 in figure 6.
We have added ten utterances in all 39 dialogues, five 'system' utterances and five user utterances.
Note, however, that we utilised the transcribed dialogues, without information on intonation.
We would probably not have needed to add this m a n y utterances if we had utilised the tapes.
Our reason for not using information on intonation is that we do not assume t h a t our system's speech recogniser can recognise intonation.
Finally, as discussed above, we did not utilise the full potential of multi-modality when distilling the dialogues.
For instance, some dialogues could be further distilled if we had assumed t h a t the system had presented a time-table.
One reason for this is t h a t we wanted to capture as m a n y interesting aspects intact as possible.
The advantage is, thus, that we have a better corpus for understanding humanYees hi Anna Nilsson is my name and I would like to take the bus from Ryd center to Resecentrum in LinkSping jaa hej Anna Nilsson heter jag och jag rill ~ka buss ~r~n Ryds centrum till resecentrum i LinkSping.
$3: U4: mm When do you want to leave? mm N~ir r i l l d u  k a ? 'n' I must be at Resecentrum before fourteen and thirty five ().
'cause we will going to the interstate buses ja ska va p~ rececentrum innan fjorton d trettifem ().
f5 vi ska till l~ngfiirdsbussarna Figure 6: Distilled dialogue fragment with added utterance computer interaction and can from t h a t corpus do a second distillation where we focus more on multimodal interaction.
One example of this is whether the system is meant to acquire information on the user's underlying motivations or goals or not.
In the examples presented, we have not assumed such capabilities, but this assumption is not an absolute necessity.
We believe, however, that the distilling process should be based on one such model, not the least to ensure a consistent t r e a t m e n t of similar recurring phenomena at different places in the corpora.
The validity of the results based on analysing distilled dialogues depends p a r t l y on how the distillation has been carried out.
Even when using natural dialogues we can have situations where the interaction is somewhat mysterious, for instance, if some of the dialogue participants behaves irrational such as not providing feedback or being too elliptical.
However, if careful considerations have been made to stay as close to the original dialogues as possible, we believe that distilled dialogues will reflect what a hum a n would consider to be a natural interaction.
Acknowledgments This work results from a n u m b e r of projects on development of natural language interfaces supported by The Swedish Transport & Communications Research Board (KFB) and the joint Research P r o g r a m for Language Technology ( H S F R / N U T E K ) . We are indebted to the participants of the Swedish Dialogue Systems project, especially to Staffan Larsson, Lena S a n t a m a r t a, and Annika Flycht-Eriksson for interesting discussions on this topic.
Discussion We have been presenting a method for distilling hum a n dialogues to make t h e m resemble h u m a n computer interaction, in order to utilise such dialogues as a knowledge source when developing dialogue systems.
Our own main purpose has been to use t h e m for developing multimodal systems, however, as discussed above, we have in this p a p e r rather assumed a speech-only system.
But we believe that the basic approach can be used also for multi-modal systems and other kinds of natural language dialogue systems.
It is i m p o r t a n t to be aware of the limitations of the method, and how 'realistic' the produced result will be, compared to a dialogue with the final system.
Since we are changing the dialogue moves, by for instance providing all required information in one move, or never asking to be reminded of what the user has previously requested, it is obvious t h a t what follows after the changed sequence would probably be affected one way or another.
A consequence of this is that the resulting dialogue is less accurate as a model of the entire dialogue.
It is therefore not an ideal candidate for trying out the systems over-all performance during system development.
But for the smaller sub-segments or sub-dialogues, we believe that it creates a good approximation of what will take place once the system is up and running.
Furthermore, we believe distilled dialogues in some respects to be more realistic than Wizard of Ozdialogues collected with a wizard acting as a computer.
Another issue, t h a t has been discussed previously in the description of the method, is t h a t the distilling is made based on a particular view of what a dialogue with a computer will look like.
While not necessarily being a detailed and specific model, it is at least an instance of a class of computer dialogue models.
Plan-Based Dialogue Management in a Physics Tutor Reva Freedman Learning Research and Development Center University of Pittsburgh Pittsburgh, PA 15260 freedrk+@pitt, edu http://www.pitt, edu/~freedrk Abstract This paper describes an application of APE (the Atlas Planning Engine), an integrated planning and execution system at the heart of the Atlas dialogue management system.
APE controls a mixedinitiative dialogue between a human user and a host system, where turns in the 'conversation' may include graphical actions and/or written text.
APE has full unification and can handle arbitrarily nested discourse constructs, making it more powerful than dialogue managers based on finitestate machines.
We illustrate this work by describing Atlas-Andes, an intelligent tutoring system built using APE with the Andes physics tutor as the host.
1 Introduction
The purpose of the Atlas project is to enlarge the scope of student interaction in an intelligent tutoring system (ITS) to include coherent conversational sequences, including both written text and GUI actions.
A key component o f Atlas is APE, the Atlas Planning Engine, a "just-intime" planner specialized for easy construction and quick generation of hierarchically organized dialogues.
APE is a domainand task-independent system.
Although to date we have used APE as a dialogue manager for intelligent tutoring systems, APE could also be used to manage other types of human-computer conversation, such as an advicegiving system or an interactive help system.
Planning is an essential component of a dialogue-based ITS.
Although there are many reasons for using natural language in an ITS, as soon as the student gives an unexpected response to a tutor question, the tutor needs to be able to plan in order to achieve its goals as well as respond appropriately to the student's statement.
Yet classical planning is inappropriate for dialogue generation precisely because it assumes an unchanging world.
A more appropriate approach is the "practical reason" approach pioneered by Bratman (1987, 1990).
According to Bratman, human beings maintain plans and prefer to follow them, but they are also capable of changing the plans on the fly when needed.
Bratman's approach has been introduced into computer science under the name of reactive planning (Georgeff and Ingrand 1989, Wilkins et al.1995). In this paper we discuss the rationale for the use of reactive planning as well as the use of the hierarchical task network (HTN) style o f plan operators.
Then we describe APE (the Atlas Planning Engine), a dialogue planner we have implemented to embody the above concepts.
We demonstrate the use of APE by showing how we have used it to add a dialogue capability to an existing ITS, the Andes physics tutor.
By showing dialogues that Atlas-Andes can generate, we demonstrate the advantages of this architecture over the finite-state machine approach to dialogue management.
Integrated planning and execution for dialogue generation 2.1 'Practical reason' and the BDI model For an ITS, planning is required in order to ensure a coherent conversation as well as to accomplish tutorial goals.
But it is impossible to plan a whole conversation in advance when the student can respond freely at every turn, just as human beings cannot plan their daily lives in advance because of possible changes in conditions.
Classical planning algorithms are inappropriate because the tutor must be able to change plans based on the This research was supported by NSF grant number 9720359 to CIRCLE, the Center for Interdisciplinary Research on Constructive Learning Environments at the University of Pittsburgh and Carnegie-Mellon University.
fistudent's responses.
For this reason we have adopted the ideas of the philosopher Michael Bratman (1987, 1990).
Bratman uses the term "practical reason" to describe his analysis since he is concerned with how to reason about practical matters.
For human beings, planning is required in order to accomplish one's goals.
Bratman's key insight is that human beings tend to follow a plan once they have one, although they are capable of dropping an intention or changing a partial plan when necessary.
In other words, human beings do not decide what to do from scratch at each turn.
Bratman and others who have adopted his approach use a tripartite mental model that includes beliefs, desires and intentions (Bratman, Israel and Pollack 1988, Pollack 1992, Georgeff et al.1998), hence the name "BDI model".
Beliefs, which are uninstantiated plans in the speaker's head, are reified by the plan library.
Desires are expressed as the agent's goals.
Intentions, or plan steps that the agent has committed to but not yet acted on, are stored in an agenda.
Thus the agent's partial plan for achieving a goal is a network of intentions.
A plan can be left in a partially expanded state until it is necessary to refine it further.
2.2 Implementation
via reactive planning can be achieved via a series of subgoals instead of relying on means-end reasoning.
Hierarchical decomposition is more appropriate to dialogue generation for a number of reasons.
First, decomposition is better suited to the type of largescale dialogue planning required in a real-world tutoring system, as it is easier to establish what a human speaker will say in a given situation than to be able to understand why in sufficient detail and generality to do means-end planning.
Second, Hierarchical decomposition minimizes search time.
Third, our dialogues are task-oriented and have a hierarchical structure (Grosz and Sidner 1986).
In such a case, matching the structure of the domain simplifies operator development because they can often be derived from transcripts of human tutoring sessions.
The hierarchy information is also useful in determining appropriate referring expressions.
Fourth, interleaved planning and execution is important for dialogue generation because we cannot predict the human user's future utterances.
In an HTN-based system, it is straightforward to implement interleaved planning and execution because one only needs to expand the portion of the plan that is about to be executed.
Finally, the conversation is in a certain sense the trace of the plan.
In other words, we care much more about the actions generated by the planner than the states involved, whether implicitly or explicitly specified.
Hierarchical decomposition provides this trace naturally.
3 Background: the Andes physics tutor Andes (Gertner, Conati and VanLehn 1998) is an intelligent tutoring system in the domain of firstyear college physics.
Andes teaches via coached problem solving (VanLehn 1996).
In coached problem solving, the tutoring system tracks the student as the latter attempts to solve a problem.
If the student gets stuck or deviates too far from a correct solution path, the tutoring system provides hints and other assistance.
A sample Andes problem is shown in midsolution in Figure 1.
A physics problem is given in the upper-left corner with a picture below it.
Next to the picture the student has begun to sketch the vectors involved using the GUI buttons along the left-hand edge of the screen.
As the fistudent draws vectors, Andes and the student cooperatively fill in the variable definitions in the upper-right corner.
Later the student will use the space below to write equations connecting the variables.
In this example, the elevator is decelerating, so the acceleration vector should face the opposite direction from the velocity vector.
(If the acceleration vector went the same direction as the velocity vector, the speed of the elevator would increase and it would crash into the ground).
This is an important issue in beginning physics; it occurs in five Andes problems.
When such errors occur, Andes turns the incorrect item red and provides hints to students in the lower-left corner of the screen.
A sample of these hints, shown in the order a student would encounter them, is shown in Fig.
2. But hints are an output-only form of natural language; the student can't take the initiative or ask a question.
In addition, there is no way for the system to ask the student a question or lead the student through a multi-step directed line of reasoning.
Thus there is no way to use some of the effective rhetorical methods used by skilled human tutors, such as analogy and reductio ad absurdum.
Current psychological research suggests that active methods, where students have to answer questions, will improve the performance of tutoring systems.
Structure of the Atlas Planning Engine Figure3 shows a sample plan operator.
For legibility, the key elements have been rendered in English instead of in Lisp.
The hiercx slot provides a way for the planner to be aware of the context in which a decomposition is proposed.
Items in the hiercx slot are instantiated and added to the transient database only so long as the operator which spawned them is in the agenda.
To initiate a planning session, the user invokes the planner with an initial goal.
The system searches the operator library to find all operators whose goal field matches the next goal on the agenda and whose filter conditions and preconAn elevator slows to a stop from an initial downward velocity of 10.0 m]s in 2.00 seconds.
A passenger in the elevator is holding a 3.00 kilogram package by a vertical string.
What is the tension in the string during the process? e',ev~o, at 10 m/s elev~or at a stop mass of p~:w'.,I,,~ magnitude of the inst~~taneous Velocity of pack,age ~ {rkneTO magnitude of the avelage Acceleratiorl of package,dudngTO... pkg Figure I: Screen shot of the Andes physics tutor fiS: T: S: T: S: T: S: T: S: (draws acceleration vector in same direction as velocity) Wrong.
What's wrongwith that?
Think about the direction of the acceleration vector.
Please explain further.
Remember that the direction of acceleration is the direction of the change in velocity.
Please explain further.
The'direction o f the acceleration vector is straight up.
(draws acceleration vector correctly) Figure 2: Andes hint sequence formatted as dialogue ditions are satisfied.
Goals are represented in first-order logic without quantifiers and matched via unification.
Since APE is intended especially for generation of hierarchically organized taskoriented discourse, each operator has a multi-step recipe in the style of Wilkins (1988).
When a match is found, the matching goal is removed from the agenda and is replaced by the steps in the recipe.
APE has two kinds of primitive actions; one ends a turn and the other doesn't.
From the point of view of discourse generation, the most important APE recipe items are those allowing the planner to change the agenda when necessary.
These three types of recipe items make APE more powerful than a classical planner.
 Fact: Evaluate a condition.
If false, skip the rest of the recipe.
Fact is used to allow run-time decision making by bypassing the rest o f an operator when circumstances change during its execution.
Fact can be used with retry-at to implement a loop just as in Prolog.
 Retry-at.
The purpose of retry-at is to allow the planner to back up to a choice point and make a new decision.
It removes goals sequentially from the top of the agenda, a full operator at a time, until the supplied argument is false.
Then it restores the parent goal of the last operator removed, so that further planning can choose a new way to achieve it.
Retry-at implements a Prolog-like choice of alternatives, but it differs from backtracking in that the new operator is chosen based on conditions that apply when the retry operation is executed, rather than on a list of possible operators formed when the original operator was chosen.
For retry-at to be useful, the author must provide multiple operators for the same goal.
Each operator must have a set of preconditions enabling it to be chosen at the appropriate time.
 Prune-replace: The intent of prune-replace is (def-operator handle-same-direction :goal ()... :filter () :precond ()...
We h a v e a s k e d a q u e s t i o n a b o u t a c c e l e r a t i o n ;...
a n d t h e s t u d e n t h a s g i v e n an a n s w e r ; ...
f r o m w h i c h we c a n d e d u c e t h a t s / h e t h i n k s a c c e l, a n d v e l o c i t y go in ; the same direction ; a n d we h a v e n o t g i v e n t h e e x p l a n a t i o n below yet : r e c i p e ()...
Tell the student: "But if the a c c e l e r a t i o n went the same direction as t h e v e l o c i t y, t h e n t h e e l e v a t o r w o u l d be s p e e d i n g u p . " ; M a r k t h a t we a r e g i v i n g t h i s e x p l a n a t i o n ; T e l l t h e s t u d e n t t h a t t u t o r is r e q u e s t i n g another answer ("Try again").
Edit the agenda ( u s i n g prune-replace) so t h a t r e s p o n d i n g to a n o t h e r a n s w e r is at t h e t o p of t h e a g e n d a :hiercx ()) Figure 3: Sample plan operator 55 fito allow the planner to remove goals from the agenda based on a change in circumstances.
It removes goals sequentially from the top of the agenda, one at a time, until the supplied argument becomes false.
Then it replaces the removed goals with an optional list of new goals.
Prune-replace allows a type of decision-making frequently used in dialogue generation.
When a conversation partner does not give the expected response, one would often like to remove the next goal from the agenda and replace it with one or more replacement goals.
Prune-replace implements a generalized version of this concept.
APE is domain-independent and communicates with a host system via an API.
As a partner in a dialogue, it needs to obtain information from the world as well as produce output turns.
Preconditions on plan operators can be used to access information from external knowledge sources.
APE contains a recipe item type that can be used to execute an external program such as a call to a GUI interface.
APE also has recipe items allowing the user to assert and retract facts in a knowledge base.
Further details about the APE planner can be found in (Freedman, 2000).
I m p l e m e n t a t i o n of Atlas-Andes 5.1 Architecture of Atlas-Andes The first system we have implemented with APE is a prototype Atlas-Andes system that replaces the hints usually given for an incorrect acceleration vector by a choice of generated subdialogues.
Figure 4 shows the architecture of Atlas-Andes; any other system built with APE would look similar.
Robust natural language understanding in Atlas-Andes is provided by Ros6's CARMEL system (Ros6 2000); it uses the spelling correction algorithm devised by Elmi and Evens (1998).
5.2 Structure
of human tutorial dialogues In an earlier analysis (Kim, Freedman and Evens 1998) we showed that a significant portion of human-human tutorial dialogues can be modeled with the hierarchical structure o f task-oriented dialogues (Grosz and Sidner 1986).
Furthermore, a main building block o f the discourse hierarchy, corresponding to the transaction level in Conversation Analysis (Sinclair and Coulthard 1975), matches the tutoring episode defined by VanLehn et al.(1998). A tutoring episode consists of the turns necessary to help the student make one correct entry on the interface.
NLU (CARMEL) Plan Library User Interface Host (Andes) GUI Interpreter (Andes) Transient Knowledge Base Figure 4: Interface between Atlas and host system fiTo obtain empirical data for the Atlas-Andes plan operators, we analyzed portions of a corpus of human tutors helping students solve similar physics problems.
Two experienced tutors were used.
Tutor A was a graduate student in computer science who had majored in physics; tutor B was a professional physics tutor.
The complete corpus contained solutions to five physics problems by 41 students each.
We analyzed every tutoring episode dealing with the acceleration vector during deceleration, totaling 29 examples divided among 20 students and both tutors.
The tutors had very different styles.
Tutor A tended to provide encouragement rather than content, making those transcripts less useful for deriving an information-based approach.
Tutor B used an information-based approach, but after one wrong answer tended to complete the solution as a monologue.
Largely following tutor B's approach to sequence and content, we isolated six ways of teaching the student about direction of acceleration.
Tutoring schemata Switching between schemata API and GUI handling Answer handling Domain-dep.
lex. insertion Domain-indep.
lex. insertion TOTAL 5.3 Sample output and evaluation Figure 5 shows an example of text that can be generated by the Atlas-Andes system, showing an analogy-based approach to teaching this content.
The operator library used to generate this text could generate a combinatorially large number of versions of this dialogue as well as selected examples of other ways o f teaching about direction of acceleration.
This operator library used to generate this text contained 1 l 1 plan operators, divided as follows: We are currently working on components that will allow us to increase the number of physics concepts covered without a corresponding increase in the number of operators.
The schema switching operators prevent the tutor from repeating itself during a physics problem.
They could be reduced or eliminated by a general discourse history component that tutoring schema operators could refer to.
Domain-dependent lexical insertion refers to the choice of lexical items such as car and east in the sample dialogue, while domain-independent iexical insertion refers to items such as O K and exactly.
Both categories could be eliminated, or at least severely reduced, through the use of a text realization package.
Together that would provide a one-third reduction in the number o f operators needed.
As the set of API and GUI handling operators is fixed, that would reduce by half the number of application operators needed.
The largest remaining category of operators is the answer handlers.
These operators handle a variety of answers for each o f the five questions that the system can ask.
The answers we recognize include categories such as "don't know" as well as specific answers (e.g.
a direction perpendicular to the correct answer) which we recognize because the tutor has specific replies for them.
In order to reduce the number o f S: T: S: T: S: T: S: (draws acceleration vector in same direction as velocity) What is the definition of acceleration?
Don't know.
OK, let's try this.
If a car was driving along east, which way would you have to push on it to make it stop?
West. Exactly.
The opposite direction.
So the net force goes the opposite direction, and so does the acceleration.
Try to draw the acceleration vector again now.
(draws acceleration vector correctly) Figure 5: Example of generated dialogue 57 fioperators further, we must investigate more general methods of handling student errors.
In particular, we plan to investigate error-classifying predicates that apply to more than one question as well as the use of intention-based predicates.
Since the system only covers one rule of physics, albeit in a variety of ways, we plan to make some of these efficiency improvements before adding new rules o f physics and testing it with users.
Preconditions for the operators in the plan library utilize discourse or interaction history, the current goal hierarchy, recent information such as the tutor's current goal and the student's latest response, shared information such as a model o f objects on the screen, and domain knowledge.
As an example of the latter, if the student draws an acceleration vector which is incorrect but not opposite to the velocity vector, a different response will be generated.
Related work Wenger (1987), still the chief textbook on ITSs, states that using a global planner to control an ITS is too inefficient to try.
This is no longer true, if indeed it ever was.
Vassileva (1995) proposes a system based on AND-OR graphs with a separate set of rules for reacting to unexpected events.
Lehuen, Nicolle and Luzzati (1996) present a method of dialogue analysis that produces schemata very similar to ours.
Earlier dialoguebased ITSs that use augmented finite-state machines or equivalent include CIRCSIM-Tutor (Woo et al.1991, Z h o u e t al.
1999) and the system described by Woolf (1984).
Cook (1998) uses levels of finite-state machines.
None of these systems provides for predicates with variables or unification.
Conclusions 5.4 Discussion Many previous dialogue-based ITSs have been implemented with finite-state machines, either simple or augmented.
In the most common finite state mode[, each time the human user issues an utterance, the processor reduces it to one of a small number of categories.
These categories represent the possible transitions between states.
Thus history can be stored, and context considered, only by expanding the number o f states.
This approach puts an arbitrary restriction on the amount of context or depth of conversational nesting that can be considered.
More importantly, it misses the significant generalization that these types of dialogues are hierarchical: larger units contain repeated instances of the same smaller units in different sequences and instantiated with different values.
Furthermore, the finite-state machine approach does not allow the author to drop one line of attack and replace it by another without hardcoding every possible transition.
It is also clear that the dialogue-based approach has many benefits over the hint-sequence approach.
In addition to providing a multi-step teaching methods with new content, it can respond flexibly to a variety of student answers at each step and take context into account when generating a reply.
In this paper we described APE, an integrated planner and execution system that we have implemented as part o f the Atlas dialogue manager.
APE uses HTN-style operators and is based on reactive planning concepts.
Although APE is intended largely for use in domains with hierarchical, multi-turn plans, it can be used to implement any conversation-based system, where turns in the 'conversation' may include graphical actions and/or text.
We illustrated the use of APE with an example from the Atlas-Andes physics tutor.
We showed that previous models based on finite-state machines are insufficient to handle the nested subdialogues and abandoned partial subdialogues that occur in practical applications.
We showed how APE generated a sample dialogue that earlier systems could not handle.
Acknowledgments We thank Abigail Gertner for her generous assistance with the Andes system, and Michael Ringenberg for indispensible programming support.
Carolyn Ros6 built the CARMEL natural language understanding component.
Mohammed EImi and Michael Glass of Illinois Institute o f Technology provided the spelling correction code.
We thank Pamela Jordan and the referees for their comments.
Bratman's approach has been elaborated in a computer science context by subsequent researchers (Bratman, Israel and Pollack 1988, Pollack 1992, Georgeff et al.1998). Reactive planning (Georgeff and Ingrand 1989, Wilkins et al.1995), originally known as "integrated planning and execution," is one way of implementing Bratman's model.
Originally developed for real-time control of the space shuttle, reactive planning has since been used in a variety of other domains.
For the Atlas project we have developed a reactive planner called APE (Atlas Planning Engine) which uses these ideas to conduct a conversation.
After each student response, the planner can choose to continue with its previous intention or change something in the plan to respond better to the student's utterance.
Like most reactive planners, APE is a hierarchical task network (HTN) style planner (Yang 1990, Erol, Hendler and Nau 1994).
A Framework for MT and Multilingual NLG Systems Based on Uniform Lexico-Structural Processing Benoit Lavoie CoGenTex, Inc.
840 Hanshaw Road Ithaca, NY USA, 14850 benoit@cogentex.com Richard Kittredge CoGenTex, Inc.
840 Hanshaw Road Ithaca, NY USA, 14850 richard @cogentex.com Tanya Korelsky CoGenTex, Inc.
840 Hanshaw Road Ithaca, NY USA, 14850 tanya @cogentex.com Owen Rambow * ATT Labs-Research, B233 180 Park Ave, PO Box 971 Florham Park, NJ USA, 07932 rambow @research.att.com Abstract In this paper we describe an implemented framework for developing monolingual or multilingual natural language generation (NLG) applications and machine translation (MT) applications.
The framework demonstrates a uniform approach to generation and transfer based on declarative lexico-structural transformations of dependency structures of syntactic or conceptual levels ("uniform lexico-structural processing").
We describe how this framework has been used in practical NLG and MT applications, and report the lessons learned.
1 Introduction
The framework consists of a portable Java environment for building NLG or MT applications by defining modules using a core tree transduction engine and single declarative ASCII specification language for conceptual or syntactic dependency tree structures 1 and their transformations.
Developers can define new modules, add or remove modules, or modify their connections.
Because the processing of the transformation engine is restricted to transduction of trees, it is computationally efficient.
Having declarative rules facilitates their reuse when migrating from one programming environment to another; if the rules are based on functions specific to a programming language, the implementation of these functions might no longer be available in a different environment.
In addition, having all lexical information and all rules represented declaratively makes it relatively easy to integrate into the framework techniques for generating some of the rules automatically, for example using corpus-based methods.
The declarative form of transformations makes it easier to process them, compare them, and cluster them to achieve proper classification and ordering.
In this paper we present a linguistically motivated framework for uniform lexicostructural processing.
It has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation (NLG) and for transfer in machine translation (MT).
Our work extends directions taken in systems such as Ariane (Vauquois and Boitet, 1985), FoG (Kittredge and Polgu6re, 1991), JOYCE (Rainbow and Korelsky, 1992), and LFS (Iordanskaja et al., 1992).
Although it adopts the general principles found in the abovementioned systems, the approach presented in this paper is more practical, and we believe, would eventually integrate better with emerging statistics-based approaches to MT.
* The work performed on the framework by this coauthor was done while at CoGenTex, Inc.
1 In
this paper, we use the term syntactic dependency (tree) structure as defined in the Meaning-Text Theory (MTT; Mel'cuk, 1988).
However, we extrapolate from this theory when we use the term conceptual dependency (tree) structure, which has no equivalent in MTT (and is unrelated to Shank's CD structures proposed in the 1970s).
Thus, the framework represents a generalized processing environment that can be reused in different types of natural language processing (NLP) applications.
So far the framework has been used successfully to build a wide variety of NLG and MT applications in several limited domains (meteorology, battlefield messages, object modeling) and for different languages (English, French, Arabic, and Korean).
In the next sections, we present the design of the core tree transduction module (Section 2), describe the representations that it uses (Section 3) and the linguistic resources (Section 4).
We then discuss the processing performed by the tree transduction module (Section 5) and its instantiation for different applications (Section 6).
Finally, we discuss lessons learned from developing and using the framework (Section 7) and describe the history of the framework comparing it to other systems (Section 8).
2 The
Framework's Tree Transduction Module Input Lexico-Structm'al Processing Dependency SUucturc Lexico-Structural Postprocessing Figure 1: Design of the Tree Transduction Module 3 The Framework's Representations The core processing engine of the framework is a generic tree transduction module for lexicostructural processing, shown in Figure 1.
The module has dependency stuctures as input and output, expressed in the same tree formalism, although not necessarily at the same level (see Section 3).
This design facilitates the pipelining of modules for stratificational transformation.
In fact, in an application, there are usually several instantiations of this module.
The transduction module consists of three processing steps: lexico-structural preprocessing, main lexico-structural processing, and lexico-structural post-processing.
Each of these steps is driven by a separate grammar, and all three steps draw on a common feature data base and lexicon.
The grammars, the lexicon and the feature data base are referred to as the linguistic resources (even if they sometimes apply to a conceptual representation).
All linguistic resources are represented in a declarative manner.
An instantiation of the tree transduction module consists of a specification of the linguistic resources.
The representations used by all instantiations of the tree transduction module in the framework are dependency tree structures.
The main characteristics of all the dependency tree structures are:  A dependency tree is unordered (in contrast with phrase structure trees, there is no ordering between the branches of the tree).
 All the nodes in the tree correspond to lexemes (i.e., lexical heads) or concepts depending on the level of representation.
In contrast with a phrase structure representation, there are no phrase-structure nodes labeled with nonterminal symbols.
Labelled arcs indicate the dependency relationships between the lexemes.
The first of these characteristics makes a dependency tree structure a very useful representation for MT and multilingual NLG, since it gives linguists a representation that allows them to abstract over numerous crosslinguistic divergences due to language specific ordering (Polgu~re, 1991).
We have implemented 4 different types of dependency tree structures that can be used for NLG, MT or both:  Deep-syntactic structures (DSyntSs);  Surface syntactic structures (SSyntSs); Conceptual structures (ConcSs); Parsed syntactic structures (PSyntSs).
The DSyntSs and SSyntSs correspond closely to the equivalent structures of the Meaning-Text Theory (MTT; Mel'cuk, 1988): both structures are unordered syntactic representations, but a DSyntS only includes full meaning-bearing lexemes while a SSyntS also contains function words such as determiners, auxiliaries, and strongly governed prepositions.
In the implemented applications, the DSyntSs are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic-based MT (Hutchins and Somers, 1997).
Figure 2 illustrates a DSyntS from a meteorological application, MeteoCogent (Kittredge and Lavoie, 1998), represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework (Lavoie and Rambow, 1997).
As Figure 2 illustrates, there is a straightforward mapping between the graphical notation and the ASCII notation supported in the framework.
This also applies for all the transformation rules in the framework which illustrates the declarative nature of our approach, Figure 3 illustrates the mapping between an interlingua defined as a ConcS and a corresponding English DSyntS.
This example, also taken from MeteoCogent, illustrates that the conceptual interlingua in NLG can be closer to a database representation of domain data than to its linguistic representations.
As mentioned in (Polgu~re, 1991), the high level of abstraction of the ConcSs makes them a suitable interlingua for multilingual NLG since they bridge the semantic discrepancies between languages, and they can be produced easily from the domain data.
However, most off-the-shelf parsers available for MT produce only syntactic structures, thus the DSyntS level is often more suitable for transfer.
Cones TO ItlGH LOW Low 5 to Mlgh 20 Figure 3: ConcS Interlingua and English DSyntS Finally, the PSyntSs correspond to the parser outputs represented using RealPro's dependency structure formalism.
The PSyntSs may not be valid directly for realization or transfer since they may contain unsupported features or dependency relations.
However, the PSyntSs are represented in a way to allow the framework to convert them into valid DSyntS via lexicostructural processing.
This conversion is done via conversion grammars customized for each parser.
There is a practical need to convert one syntactic formalism to another and so far we have implemented converters for three off-theshelf parsers (Palmer et al., 1998).
4 The
Framework's Linguistic Resources 't Low S to high 20 Figure 2: DSyntS(Graphicaland ASCIINotation) The ConcSs correspond to the standard framelike structures used in knowledge representation, with labeled arcs corresponding to slots.
We have used them only for a very limited meteorological domain (in MeteoCogent), and we imagine that they will typically be defined in a domain-specific manner.
As mentioned previously, the framework is composed of instantiations of the tree fitransduction module shown in Figure 1.
Each module has the following resources:  Feature Data-Base: This consists of the feature system defining available features and their possible values in the module.
 Lexicon: This consists of the available lexemes or concepts, depending on whether the module works at syntactic or conceptual level.
Each lexeme and concept is defined with its features, and may contain specific lexico-structural rules: transfer rules for MT, mapping rules to the next level of representation for surface realization of DSyntS or lexicalization of ConcS.
 Main Grammar: This consists of the lexicostructural mapping rules that apply at this level and which are not lexemeor conceptspecific (e.g.
DSynt-rules for the DSyntmodule, Transfer-rules for the Transfer module, etc).
 Preprocessing grammar: This consists of the lexico-structural mapping rules for transforming the input structures in order to make them compliant with the main grammar, if this is necessary.
Such rules are used to integrate new modules together when discrepancies in the formalism need to be fixed.
This grammar can also be used for adding default features (e.g.
setting the default number of nouns to singular) or for applying default transformations (e.g.
replacing non meaning-bearing lexemes with features).
Postprocessing grammar: This consists of lexico-structural mapping rules for transforming the output structures before they can be processed by the next module.
As for the preprocessing rules, these rules can be used to fix some discrepancies between modules.
Our representation of the lexicon at the lexical level (as opposed to conceptual) is similar to the one found in RealPro.
Figure 4 shows a specification for the lexeme SELL.
This lexeme is defined as a verb of regular morphology with two lexical-structural mappings, the first one introducing the preposition TO for its 3r actant, and the preposition FOR for its 4 th actant: (a seller) X1 sells (merchandise) X2 to (a buyer) X3 f o r (a price) X4.
What is important is that 63 each mapping specifies a transformation between structures at different levels of representation but that are represented in one and the same representation formalism (DSyntS and SSyntS in this case).
As we will see below, grammar rules are also expressed in a similar way.
( c o m p l e t i v e 3 FOR ( prepositional Figure 4: Specification of Lexeme SELL At the conceptual level, the conceptual lexicon associates lexical-structural mapping with concepts in a similar way.
Figure 5 illustrates the mapping at the deep-syntactic level associated with the concept #TEMPERATURE.
Except for the slight differences in the labelling, this type of specification is similar to the one used on the lexical level.
The first mapping rule corresponds to one of the lexico-structural transformations used to convert the interlingual ConcS of Figure 3 to the corresponding DSyntS.
SY SX Note that since each lexicon entry can have more than one lexical-structural mapping rule, the list of these rules represents a small grammar specific to this lexeme or concept.
Realization grammar rules of the main grammar include generic mapping rules (which are not lexeme-specific) such as the DSyntS-rule illustrated in Figure 6, for inserting a determiner.
DSYNT-RULE: More general lexico-structural rules for transfer can also be implemented using our grammar rule formalism.
Figure 8 gives an English-French transfer rule applied to a weather domain for the transfer of a verb modified by the adverb ALMOST: It almost rained.
--o II a failli pleuvoir.
Figure 6: Deep-Syntactic Rule for Determiner Insertion The lexicon formalism has also been extended to implement lexeme-specific lexico-structural transfer rules.
Figure 7 shows the lexicostructural transfer of the English verb lexeme MOVE to French implemented for a military and weather domain (Nasr et al., 1998): Cloud will move into the western regions.
Des nuages envahiront les rdgions ouest.
They moved the assets forward.
-.9 lls ont amen~ les ressources vers l 'avant.
The 79 dcg moves forward.
---~La 79 dcg a v a n c e vers l'avant.
A disturbance will move north of Lake Superior.
--~ Une perturbation se diplacera au nord du lac supdrieur.
Figure 8: English to French Lexico-Structural Transfer Rule with Verb Modifier ALMOST More details on how the structural divergences described in (Dorr, 1994) can be accounted for using our formalism can be found in (Nasr et al., 1998).
5 The
Rule Processing Before being processed, the rules are first compiled and indexed for optimisation.
Each module applies the following processing.
The rules are assumed to be ordered from most specific to least specific.
The application of the rules to the structures is top-down in a recursive way from the f'n-st rule to the last.
For the main grammar, before applying a grammar rule to a given node, dictionary lookup is carried out in order to first apply the lexemeor conceptspecific rules associated with this node.
These are also assumed to be ordered from the most specific to the least specific.
If a lexico-structural transformation involves switching a governor node with one of its dependents in the tree, the process is reapplied with the new node governor.
When no more rules can be applied, the same process is applied to each dependent of the current governor.
When all nodes have been processed, the processing is completed, 6 Using the Framework to build Applications Figure 7: Lexico-Structural Transfer of English Lexerne MOVE to French Figure 9 shows how different instantiations of the tree transduction module can be combined to fibuild NLP applications.
The diagram does not represent a particular system, but rather shows the kind of transformations that have been implemented using the framework, and how they interact.
Each arrow represents one type of processing implemented by an instantiation of the tree transduction module.
Each triangle represents a different level of representation.
Sentence interlingua can also support the generation of French but this functionality has not yet been implemented).
MT:  Transfer on the DSyntS level and realization via SSyntS level for English--French, English--Arabic, English---Korean and Korean--English.
Translation in the meteorology and battlefield domains (Nasr et al., 1998).
 Conversion of the output structures from off-the-shelf English, French and Korean parsers to DSyntS level before their processing by the other components in the framework (Palmer et al., 1998).
7 Lessons
Learned Using the Framework PI "ng Scopeof the Framework SSyntSLI Parsing Input Sentence LI yntS ealization Generated Sentence 1.2 Generated Sentence LI Sentence SSyntS Figure 9: Scope of the Framework's Transformations For example, in Figure 9, starting with the "Input Sentence LI" and passing through Parsing, Conversion, Transfer, DSyntS Realization and SSyntS Realization to "Generated Sentence L2" we obtain an Ll-to-L2 MT system.
Starting with "Sentence Planning" and passing through DSyntS Realization, and SSyntS Realization (including linearization and inflection) to "Generated Sentence LI", we obtain a monolingual NLG system for L1.
So far the framework has been used successfully for building a wide variety of applications in different domains and for different languages: NLG:  Realization of English DSyntSs via SSyntS level for the domains of meteorology (MeteoCogent; Kittredge and Lavoie, 1998) and object modeling (ModelExplainer; Lavoie et al., 1997).
 Generation of English text from conceptual interlingua for the meteorology domain (MeteoCogent).
(The design of the Empirical results obtained from the applications listed in Section 6 have shown that the approach used in the framework is flexible enough and easily portable to new domains, new languages, and new applications.
Moreover, the time spent for development was relatively short compared to that formerly required in developing similar types of applications.
Finally, as intended, the limited computational power of the transduction module, as well as careful implementation, including the compilation of declarative linguistic knowledge to Java, have ensured efficient run-time behavior.
For example, in the MT domain we did not originally plan for a separate conversion step from the parser output to DSyntS.
However, it quickly became apparent that there was a considerable gap between the output of the parsers we were using and the DSyntS representation that was required, and furthermore, that we could use the tree transduction module to quickly bridge this gap.
Nevertheless, our tree transduction-based approach has some important limitations.
In particular, the framework requires the developer of the transformation rules to maintain them and specify the order in which the rules must be applied.
For a small or a stable grammar, this does not pose a problem.
However, for large or rapidly changing grammar (such as a transfer grammar in MT that may need to be adjusted when switching from one parser to another), the fiburden of the developer's task may be quite heavy.
In practice, a considerable amount of time can be spent in testing a grammar after its revision.
Another major problem is related to the maintenance of both the grammar and the lexicon.
On several occasions during the development of these resources, the developer in charge of adding lexical and grammatical data must make some decisions that are domain specific.
For example, in MT, writing transfer rules for terms that can have several meanings or uses, they may simplify the problem by choosing a solution based on the context found in the current corpus, which is a perfectly natural strategy.
However, later, when porting the transfer resources to other domains, the chosen strategy may need to be revised because the context has changed, and other meanings or uses are found in the new corpora.
Because the current approach is based on handcrafted rules, maintenance problems of this sort cannot be avoided when porting the resources to new domains.
An approach such as the one described in (Nasr et al., 1998; and Palmer and al., 1998) seems to be solving a part of the problem when it uses corpus analysis techniques for automatically creating a first draft of the lexical transfer dictionary using statistical methods.
However, the remaining work is still based on handcrafting because the developer must refine the rules manually.
The current framework offers no support for merging handcrafted rules with new lexical rules obtained statistically while preserving the valid handcrafted changes and deleting the invalid ones.
In general, a better integration of linguistically based and statistical methods during all the development phases is greatly needed.
8 History
of the Framework and Comparison with Other Systems realization of deep-syntactic structures in NLG (Lavoie and Rambow, 1997).
It was later extended for generation of deep-syntactic structures from conceptual interlingua (Kittredge and Lavoie, 1998).
Finally, it was applied to MT for transfer between deep-syntactic structures of different languages (Palmer et al., 1998).
The current framework encompasses the full spectrum of such transformations, i.e. from the processing of conceptual structures to the processing of deep-syntactic structures, either for NLG or MT.
Compared to its predecessors (Fog, LFS, JOYCE), our approach has obvious advantages in uniformity, declarativity and portability.
The framework has been used in a wider variety of domains, for more languages, and for more applications (NLG as well as MT).
The framework uses the same engine for all the transformations at all levels because all the syntactic and conceptual structures are represented as dependency tree structures.
In contrast, the predecessor systems were not designed to be rapidly portable.
These systems used programming languages or scripts for the implementation of the transformation rules, and used different types of processing at different levels of representation.
For instance, in LFS conceptual structures were represented as graphs, whereas syntactic structures were represented as trees which required different types of processing at these two levels.
Our approach also has some disadvantages compared with the systems mentioned above.
Our lexico-structural transformations are far less powerful than those expressible using an arbitrary programming language.
In practice, the formalism that we are using for expressing the transformations is inadequate for long-range phenomena (inter-sentential or intra-sentential), including syntactic phenomena such as longdistance wh-movement and discourse phenomena such as anaphora and ellipsis.
The formalism could be extended to handle intrasentential syntactic effects, but inter-sentential discourse phenomena probably require procedural rules in order to access lexemes in The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS (Iordanskaja et al., 1992), and JOYCE (Rambow and Korelsky, 1992).
The framework was originally developed for the fiother sentences.
In fact, LFS and JOYCE include a specific module for elliptical structure processing.
Similarly, the limited power of the tree transformation rule formalism distinguishes the framework from other NLP frameworks based on more general processing paradigms such as unification of FUF/SURGE in the generation domain (Elhadad and Robin, 1992).
9 Status
The framework is currently being improved in order to use XML-based specifications for representing the dependency structures and the transformation rules in order to offer a more standard development environment and to facilitate the framework extension and maintenance.
Acknowledgements A first implementation of the framework (C++ processor and ASCII formalism for expressing the lexico-structural transformation rules) applied to NLG was developed under SBIR F30602-92-C-0015 awarded by USAF Rome Laboratory.
The extensions to MT were developed under SBIR DAAL01-97-C-0016 awarded by the Army Research Laboratory.
The Java implementation and general improvements of the framework were developed under SBIR DAAD17-99-C-0008 awarded by the Army Research Laboratory.
We are thankful to Ted Caldwell, Daryl McCullough, Alexis Nasr and Mike White for their comments and criticism on the work reported in this paper.

