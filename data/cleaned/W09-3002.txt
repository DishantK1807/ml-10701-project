Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 10–18,
Suntec, Singapore, 6-7 August 2009. c 2009 ACL and AFNLP
Complex Linguistic Anotation – No Easy Way Out! 
 A Case from Bangla and Hindi POS Labeling Tasks 
 
Sandipan Dandapat
1 
       Priyanka Biswas
1
       Monojit Choudhury Kalika Bali 
Dublin City University       LDCIL         Microsoft Research Labs India 
Ireland                   CIIL-Mysore, India               Bangalore, India 
E-mail:  sdandapat@computing.dcu.ie, biswas.priyanka@gmail.com, 
monojitc@microsoft.com, kalikab@microsoft.com 
 
 
Abstract 
Alternative paths to linguistic anotation, 
such as those utilizing games or exploiting 
the web users, are becoming popular in recent 
times owing to their very high benefit-to-cost 
ratios. In this paper, however, we report a 
case study on POS anotation for Bangla and 
Hindi, where we observe that reliable linguis-
tic anotation requires not only expert ano-
tators, but also a great deal of supervision. 
For our hierarchical POS anotation scheme, 
we find that close supervision and training is 
necesary at every level of the hierarchy, or 
equivalently, complexity of the tagset. Never-
theles, an inteligent anotation tol can sig-
nificantly acelerate the anotation proces 
and increase the inter-anotator agrement 
for both expert and non-expert anotators. 
These findings lead us to believe that reliable 
anotation requiring dep linguistic knowl-
edge (e.g., POS, chunking, Trebank, seman-
tic role labeling) requires expertise and su-
pervision. The focus, therefore, should be on 
design and development of apropriate ano-
tation tols equiped with machine learning 
based predictive modules that can signifi-
cantly bost the productivity of the anota-
tors.
1
 
1 Introduction

Aces to reliable anotated data is the first hur-
dle encountered in most NLP tasks be it at the 
level of Parts-of-Spech (POS) taging or a more 
complex discourse level anotation. The per-
formance of the machine learning aproaches 
which have become de rigueur for most NLP 
tasks are dependent on acurately anotated large 
datasets. Creation of such databases is, hence, a 
highly resource intensive task both in terms of 
time and expertise. 
                                                
1
 This work has ben done during the authors’ internship at 
Microsoft Research Lab India. 
While the cost of an anotation task can be 
characterized by the number of man-hours and 
the level of expertise required, the productivity 
or the benefit can be measured in terms of the 
reliability and usability of the end-product, i.e., 
the anotated dataset. It is thus no surprise that 
considerable efort has gone into developing 
techniques and tools that can efectively bost 
the benefit-to-cost ratio of the anotation proc-
es. These include, but are not limited to: 
(a) exploiting the reach of the web to reduce the 
efort required for anotation (se, e.g., 
Snow et al. (208) and references therein) 
(b) smartly designed User Interfaces for aiding 
the anotators (se, e.g., Eryigit (207); 
Koutsis et al. (207); Reidsma et al. (204) 
(c) using supervised learning to botstrap a 
smal anotated dataset to automaticaly la-
bel a larger corpus and geting it corected by 
human anotators (se, e.g., Tomanek et al. 
(207); Wu et al. (207) 
(d) Active Learning (Ringer et al. 207) where 
only those data-points which are directly re-
levant for training are presented for manual 
anotation. 
Methods exploiting the web-users for linguis-
tic anotation are particularly popular these days, 
presumably because of the suces of the ESP-
Game (von Ahn and Dabish, 204) and its suc-
cesors in image anotation. A more recent study 
by (Snow et al., 208) shows that anotated data 
obtained from non-expert anonymous web-users 
is as god as those obtained from experts. How-
ever, unlike the game model, here the task is dis-
tributed among non-experts through an Internet 
portal such as Amazon Mechanical Turk, and the 
users are paid for their anotations. 
This might lead to an impresion that the ex-
pert knowledge is dispensable for NLP anota-
tion tasks. However, while these aproaches may 
work for more simple tasks like those described 
in (Snow et al., 208), most NLP related anota-
tion tasks such as POS taging, chunking, se-
mantic role labeling, Trebank anotation and 
10
discourse level taging, require expertise in the 
relevant linguistic area. In this work, we present 
a case study of POS anotation in Bangla and 
Hindi using a hierarchical tagset, where we ob-
serve that reliable linguistic anotation requires 
not only expert anotators, but also a great deal 
of supervision. A generic user interface for facili-
tating the task of hierarchical word level linguis-
tic anotation was designed and experiments 
conducted to measure the inter-anotator 
agrement (IA) and anotation time. It is ob-
served that the tol can significantly acelerate 
the anotation proces and increase the IA. The 
productivity of the anotation proces is further 
enhanced through botstraping, whereby a litle 
amount of manualy anotated data is used to 
train an automatic POS tager. The anotators 
are then asked to edit the data already taged by 
the automatic tager using an apropriate user 
interface.     
However, the most significant observation to 
emerge from these experiments is that irespec-
tive of the complexity of the anotation task (se 
Sec. 2 for definition), language, design of the 
user interface and the acuracy of the automatic 
POS tager used during botstraping, the pro-
ductivity and reliability of the expert anotators 
working under close supervision of the dataset 
designer is higher than that of non-experts or 
those working without expert-supervision. This 
leads us to believe that among the four aforemen-
tioned aproaches for improving the benefit-to-
cost ratio of the anotation tasks, solution (a) 
does not sem to be the right choice for involved 
linguistic anotations; rather, aproaches (b), (c) 
and (d) show more promise. 
The paper is organized as folows: Section 2 
provides a brief introduction to IL-POST – a hi-
erarchical POS Tag framework for Indian Lan-
guages which is used for defining the specific 
anotation tasks used for the experiments. The 
design and features of the data anotation tol 
are described in Section 3. Section 4 presents the 
experiments conducted for POS labeling task of 
Bangla and Hindi while the results of these ex-
periments are discused in Section 5. The con-
clusions are presented in Section 6. 
2 IL-POST 
IL-POST is a POS-tagset framework for Indian 
Languages, which has ben designed to cover the 
morphosyntactic details of Indian Languages 
(Baskaran et al. 208). It suports a three-level 
 
Figure 1: A schematic of IL-POST framework 
 
hierarchy of Categories, Types  and  Atributes 
that provides a systematic method to anotate 
language specific categories without disregarding 
the shared traits of the Indian languages. This 
alows the framework to ofer flexibility, cros-
linguistic compatibility and reusability acros 
several languages and aplications. An important 
consequence of its hierarchical structure and 
decomposable tags is that it alows users to spec-
ify the morpho-syntactic information aplicable 
at the desired granularity acording to the spe-
cific language and task. The complete framework 
suports 1 categories at the top level with 32 
types at the second level to represent the main 
POS categories and their sub-types. Further, 18 
morphological atributes or features are asoci-
ated with the types. The framework can thus, be 
used to derive a flat tagset of only 11 categories 
or a complex thre level tagset of several thou-
sand tags depending on the language and/or ap-
plication. Figure 1 shows a schematic of the IL-
POST framework. The curent framework has 
ben used to derive maximaly specified tagsets 
for Bangla and Hindi (se Baskaran et al. (208) 
for the descriptions of the tagsets), which have 
ben used to design the experiments presented in 
this paper. 
3 Annotation
Tool  
Though a number of POS anotation tols are 
available none are readily suitable for hierarchi-
cal taging. The tols from other domains (like 
discourse anotation, for example) that use hier-
archical tagsets require considerable customiza-
tion for the task described here. Thus, in order to 
facilitate the task of word-level linguistic anota-
tion for complex tagsets we developed a generic 
anotation tol. The anotation tol can be cus-
tomized to work for any tagset that has up to 
11
 
Figure 2: The basic Interface Window and Controls. Se the text for details. 
 
thre levels of hierarchy and for any word level 
linguistic anotation task, such as Named Entity 
anotation and Chunk boundary labeling. In this 
section we describe the design of the user inter-
face and other features of the anotation tol. 
3.1 Interface
Design Principles 
The anotation scheme folowed for linguistic 
data creation is heavily dependent on the end-
aplication the data wil cater to. Moreover, an-
notations are often performed by trained linguists 
who, in the Indian context, are either novice or 
intermitent users of computer. These observa-
tions led us to adopt the folowing principles: (1) 
customizability of the interface to any word level 
anotation task; (2) mouse driven selection of 
tags for faster and les eroneous anotation; and 
(3) display of al posible choices at every stage 
of the task to reduce memorization overload.  
 
3.2 Basic
Interface 
Figure 2 depicts the basic interface of the anota-
tion tool 
3.2.1 Automatic
Handling 
Apart from the surface controls, the interface 
also suports automatic selection facility that 
highlights the next unlabeled word that neds to 
be anotated. After loading the task (i.e., a sen-
tence) it automaticaly highlights the first unla-
beled word. Once a tag is asigned to the high-
lighted word, the next unlabeled word is auto-
maticaly selected. However, the automatic se-
lection module can be stoped by selecting a par-
ticular word through a mouse click. 
3.2.2 Handling
Hierarchical Anotation 
The first two levels of the IL-POST hierarchy are 
displayed (on a right mouse click) as a two level 
context menu. This is ilustrated in Fig. 3(a). On 
selection of the category and type by left clicks, a 
window is dynamicaly generated for the as-
signment of the atribute values, i.e., the third 
level of the hierarchy. A drop down box is aso-
ciated with each atribute for selecting the apro-
priate values. This is shown in Fig. 3(b). The 
default values for each of the atributes are set 
based on the frequency of ocurence of the val-
ues in a general corpus. This further reduces the 
time of tag asignment. When the user clicks 
“OK” on the atribute asignment window, the 
system automaticaly generates the tag as per the 
user’s selection and displays it in the Text-box 
just after the selected word. 
 
3.3 Edit
Mode Anotation 
While performing the anotation task, human 
anotators ned to label every word of a sen-
tence. Instead of anotating every word from 
scratch, we incorporate machine inteligence to 
automaticaly label every word in a sentence. 
Supose that we have an automatic POS tag pre-
diction module that does a fairly acurate job. In 
that case, the task of anotation would mean ed-
iting the pre-asigned tags to the words. We hy-
pothesize that such an editing based anotation 
task that incorporates some inteligence in the 
form of a tager wil be much faster than purely 
manual anotation, provided that the pre-
asigned tags are “suficiently acurate”. Thus, 
human anotators only ned to edit a particular 
word whenever machine asigns an incorect tag 
making the proces faster. We also make certain 
changes to the basic interface for facilitating easy 
editing. In particular, when the corpus is loaded 
using the interface, the predicted tags are shown 
for each word and the first category-type is high-
lighted automaticaly. The user can navigate 
12
                                                       
(a)                                (b) 
 
Figure 3: Anotation at a) Category-Type level, b) Atribute level 
 
to the next or pervious editable positions (Cate-
gory-Type or Atributes) by using the Shift and 
the Ctrl keys respectively. The user may edit a 
particular pre-asigned tag by making a right 
mouse click and chosing from the usual context 
menus or atribute editing window.  The user 
also has the provision to chose an editable loca-
tion by left mouse-click. 
3.3.1 Automatic
POS Tager 
We developed a statistical POS tager based on 
Cyclic Dependency Network (Toutanova et al., 
203) as an initial anotator for the Edit mode 
anotation. The tager was trained for Bangla 
and Hindi on the data that was created during the 
first phase of anotation (i.e. anotation from 
scratch). We developed tagers for both Cate-
gory+Type level (CT) and Category+Type+ At-
tribute level (CTA). We also developed two ver-
sions of the same tager with high and low acu-
racies for each level of the anotation by control-
ling the amount of training data. As we shal se 
in Sec. 4 and 5, the diferent versions of the tag-
ger at various levels of the hierarchy and acu-
racy wil help us to understand the relation be-
twen the Edit mode anotation, and the com-
plexity of the tagset and the acuracy of the tag-
ger used for initial anotation. The tagers were 
trained on 1457 sentences (aproximately 20,00 
words) for Bangla and 236 sentences (aproxi-
mately 45,00 words) for Hindi. The tagers 
were tested on 256 sentences (~ 3,50 words) for 
Bangla and 591 sentences for Hindi, which are 
disjoint from the training corpus. The evaluation 
of a hierarchical tagset is non-trivial because the 
eror in the machine taged data with respect to 
the gold standard should take into acount the 
level of the hierarchy where the mismatch be-
twen the two takes place. Clearly, mismatch at 
the category or type level should incur a higher 
penalty than one at the level of the atributes. If 
for a word, there is a mismatch betwen the type 
asigned by the machine and that present in the 
gold standard, then it is asumed to be a ful eror 
(equivalent to 1 unit). On the other hand, if the 
type asigned is corect, then the eror is 0.5 
times the fraction of atributes that do not agre 
with the gold standard. 
Table 1 reports the acuracies of the various 
tagers. Note that the atributes in IL-POST cor-
respond to morphological features. Unlike 
Bangla, we do not have aces to a morphologi-
cal analyzer for Hindi to predict the atributes 
during the POS taging at the CTA level. There-
fore, the taging acuracy in the CTA level for 
Hindi is lower than that of Bangla even though 
the amount of training data used in Hindi is 
much higher than that in Bangla. 
4 Experiments

The objective of the curent work is to study the 
cognitive load asociated with the task of linguis-
tic anotation, more specificaly, POS anota-
tion. Cognitive load relates to the higher level of 
procesing required by the working memory of 
an anotator when more learning is to be done in 
a shorter time. Hence, a higher cognitive load 
implies more time required for anotation and 
higher eror rates. The time required for anota-
tion can be readily measured by keping track of 
the time taken by the anotators while taging a 
sentence. The timer facility provided with the 
anotation tol helps us kep track of the anota-
tion time. Measuring the eror rate is slightly 
trickier as we do not have any ground truth (gold 
standard) against which we can measure the ac-
curacy of the manual anotators. Therefore, we 
measure the IA, which should be high if the eror 
rate is low. Details of the evaluation metrics are 
discused in the next section. 
13
 
Table 1: Taging acuracy in % for Bangla and 
Hindi 
 
The cognitive load of the anotation task is de-
pendent on the complexity of the tagset, 
(un)availability of an apropriate anotation tol 
and botstraping facility. Therefore, in order to 
quantify the efect of these factors on the anota-
tion task, anotation experiments are conducted 
under eight diferent setings. Four experiments 
are done for anotation at the Category+Type 
(CT) level. These are: 
• CT-AT: without using anotation tol, 
i.e., using any standard text editor
2
. 
• CT+AT: with the help of the basic ano-
tation tol. 
• CT+ATL: with the help of the anota-
tion tol in the edit mode, where the 
POS tager used has low acuracy. 
• CT+ATH: in the edit mode where the 
POS tager used has a high acuracy. 
Similarly, four experiments are conducted at the 
Category+Type+Atribute (CTA) level, which 
are named folowing the same convention: CTA-
AT, CTA+AT, CTA+ATL, CTA+ATH. 
 
4.1 Subjects

The reliability of anotation is dependent on the 
expertise of the anotators. In order to analyze 
the efect of anotator expertise, we chose sub-
jects with various levels of expertise and pro-
vided them diferent amount of training and su-
pervision during the anotation experiments. 
The experiments for Bangla have ben con-
ducted with 4 users (henceforth refered to as B1, 
B2, B3 and B4), al of whom are trained linguists 
having at least a post-graduate degre in linguis-
tics. Two of them, namely B1 and B2, were pro-
vided rigorous training in-house before the ano-
tation task. During the training phase the tagset 
and the anotation guidelines were explained to 
them in detail. This was folowed by 3-4 rounds 
of trial anotation tasks, during which the ano-
                                                
2
 The experiments without the tol were also conducted 
using the basic interface, where the anotator has to type in 
the tag strings; the function of the tol here is limited to 
loading the corpus and the timer. 
tators were asked to anotate a set of 10-15 sen-
tences and they were given fedback regarding 
the corectnes of their anotations as judged by 
other human experts. For B1 and B2, the experi-
ments were conducted in-house and under close 
supervision of the designers of the tagset and the 
tol, as wel as a senior research linguist. 
 
The other two anotators, B3 and B4, were 
provided with the data, the required anotation 
tols and the experimental setup, anotation 
guidelines and the tol usage guidelines, and the 
task were described in another document. Thus, 
the anotators were self-trained as far as the tol 
usage and the anotation scheme were con-
cerned. They were asked to return the anotated 
data (and the time logs that are automaticaly 
generated during the anotation) at the end of al 
the experiments. This situation is similar to that 
of linguistic anotation using the Internet users, 
where the anotators are self-trained and work 
under no supervision. However, unlike ordinary 
Internet users, our subjects are trained linguists. 
   
Experiments in Hindi were conducted with 
two users (henceforth refered to as H1 and H2), 
both of whom are trained linguists. As in the case 
of B1 and B2, the experiments were conducted 
under close supervision of a senior linguist, but 
H1 and H2 were self-trained in the use of the 
tol. 
 
The tasks were randomized to minimize the 
efect of familiarity with the task as wel as the 
tol. 
4.2 Data

The anotators were asked to anotate aproxi-
mately 200 words for CT+AT and CTA+AT 
experiments and around 100 words for CT-AT 
and CTA-AT experiments. The edit mode ex-
periments (CT+ATL, CT+ATH, CTA+ATL and 
CTA+ATH) have ben conducted on aproxi-
mately 100 words. The amount of data was de-
cided based primarily on the time constraints for 
the experiments. For al the experiments in a par-
ticular language, 25-35% of the data was com-
mon betwen every pair of anotators. These 
comon sets have ben used to measure the IA. 
However, there was no single complete set 
comon to al the anotators. In order to meas-
ure the influence of the pre-asigned labels on 
the judgment of the anotators, some amount of 
data was kept comon betwen CTA+AT and 
CTA+ATL/H experiments for every anotator. 
CT CTA 
Language 
High Low High Low 
Bangla 81.43 6.73 76.98 64.52 
Hindi 87.6 67.85 69.53 57.90 
14
  
 
(a)        (b) 
Figure 4: Mean anotation time (in sec per word) for diferent users at (a) CT and (b) CTA levels 
 
Mean Time (in Sec) 
Level 
-AT +AT +ATL +ATH 
CT 6.3 5.0 (20.7) 2.6 (59.4) 2.5 (59.8) 
CTA 15.2 10.9 (28.1) 5.2 (6.0) 4.8 (68.3) 
Table 2: Mean anotation time for Bangla ex-
periments (%reduction in time with respect to –
AT is given within parentheses). 
5 Analysis
of Results 
In this section we report the observations from 
our anotation experiments and analyze those to 
identify trends and their underlying reasons.  
5.1 Mean
Anotation Time 
We measure the mean anotation time by com-
puting the average time required to anotate a 
word for a sentence and then average it over al 
sentences for a given experiment by a specific 
anotator. Fig. 4 shows the mean anotation time 
(in seconds per word) for the diferent experi-
ments by the diferent anotators. It is evident 
that complex anotation task (i.e., CTA level) 
takes much more time compared to a simple one 
(i.e., CT level). We also note that the tol efec-
tively reduces the anotation time for most of the 
subjects. There is some variation in time (for ex-
ample, B3) where the subject tok longer to get 
acustomed to the anotation tol. As expected, 
the anotation proces is acelerated by bot-
straping. In fact, the higher the acuracy of the 
automatic tager, the faster is the anotation. 
Table 2 presents the mean time averaged over the 
six subjects for the 8 experiments in Bangla 
along with the %reduction in the time with re-
spect to the case when no tol is present (i.e., “-
AT”). We observe that (a) the tol is more efec-
tive for complex anotation, (b) on average, an-
notation at the CTA level take twice the time of 
their CT level counterparts, and (c) botstraping 
 
IA (in %) 
Level 
-AT +AT +ATL +ATH 
CT 68.9 79.2 (15.0) 7.2 (12.2) 89.9 (30.6) 
CTA 51.4 72.5 (41.0) 79.3 (54.2) 83.4 (62.1) 
Table 3: Average IA for Bangla experiments 
(%increase in IA with respect to –AT is given 
within parentheses). 
 
can significantly acelerate the anotation proc-
es.  We also note that experts working under 
close supervision (B1 and B2) are in general 
faster than self-trained anotators (B3 and B4). 
5.2 Inter-anotator Agrement 
Inter-anotator agrement (IA) is a very god 
indicator of the reliability of an anotated data. A 
high IA denotes that at least two anotators agre 
on the anotation and therefore, the probability 
that the anotation is eroneous is very smal. 
There are various ways to quantify the IA rang-
ing from a very simple percentage agrement to 
more complex measures such as the kapa statis-
tics (Cohen, 1960; Gertzen and Bunt, 206). For 
a hierarchical tagset the measurement of IA is 
non-trivial because the extent of disagrement 
should take into acount the level of the hierar-
chy where the mismatch betwen two anotators 
takes place. Here we use percentage agrement 
which takes into consideration the level of hier-
archy where the disagrement betwen the two 
anotators takes place. For example, the differ-
ence in IA at the category level betwen say, a 
Noun and a Nominal Modifier, versus the difer-
ence at the number atribute level betwen singu-
lar and plural. The extent of agrement for each 
of the tags is computed in the same way as we 
have evaluated our POS tager (Sec.3.2.1). We 
have also measured the Cohen’s Kapa (Cohen, 
1960) for the CT level experiments. Its behavior 
is similar to that of percentage agrement. 
15
 
(a)      (b) 
Figure 5: Pair-wise IA (in %) at (a) CT and (b) CTA levels 
 
Fig. 5 shows the pair-wise percentage IA for 
the eight experiments and Table 3 sumarizes 
the %increase in IA due to the use of 
tol/botstraping with respect to the “-AT” ex-
periments at CT and CTA levels. We observe the 
folowing basic trends: (a) IA is consistently 
lower for a complex anotation (CTA) task than 
a simpler one (CT), (b) use of anotation tol 
helps in improvement of the IA, more so for the 
CTA level experiments, (c) botstraping helps 
in further improvement in IA, especialy when 
the POS tager used has high acuracy, and (d) 
IA betwen the trained subjects (B1 and B2) is 
always higher than the other pairs. 
IA is dependent on several factors such as the 
ambiguity in the tagset, inherently ambiguous 
cases, underspecified or ambiguously specified 
anotation guidelines, and erors due to careless-
nes of the anotator. However, manual inspec-
tion reveals that the factor which results in very 
low IA in “-AT” case that the tol helps improve 
significantly is the typographical erors made by 
the anotators while using a standard text editor 
for anotation (e.g., NC mistyped as MC). This 
is more prominent in the CTA level experiments, 
where typing the string of atributes in the wrong 
order or mising out on some atributes, which 
are very comon when anotation tol is not 
used, lead to a very low IA. Thus, memorization 
has a huge overload during the anotation proc-
es, especialy for complex anotation schemes, 
which the anotation tol can efectively handle. 
In fact, more than 50% erors in CTA level are 
due to the above phenomenon. The analysis of 
other factors that lower the IA is discused in 
Sec. 5.4. 
We would like to emphasize the fact that al-
though the absolute time diference betwen the 
trained and un-trained users reduces when the 
tol and/or botstraping is used, the IA does not 
decrease significantly in case of the untrained 
users for the complex anotation task.  
 
Subjects 
Level Tager 
B1 B2 B3 B4 
Low 89.6 89.8 74.2 81.8 
CT 
High 90.8 90.1 64.8 7.8 
Low 85.4 85.1 68.2 76.1 
CTA 
High 86.4 85.4 59.1 73.4 
Table 4: Percentage agrement betwen the edit 
and the normal mode anotations (for Bangla). 
 
5.3 Machine
Influence 
We have sen that the IA increases in the edit 
mode experiments. This aparent positive result 
might be an unaceptable artifact of machine 
influence, which is to say that the anotators, 
whenever in confusion, might blindly agre with 
the pre-asigned labels.  In order to understand 
the influence of the pre-asigned labels on the 
anotators, we calculate the percentage agree-
ment for a subject betwen the data anotated 
from scratch using the tol (+AT) and that in the 
edit mode (+ATL and +ATH). The results are 
sumarized in Table 4. 
The low agrement betwen the data ano-
tated under the two modes for the untrained an-
notators (B3 and B4) shows that there is a strong 
influence of pre-asigned labels for these users. 
Untrained anotators have lower agrement 
while using a high acuracy initial POS tager 
compared to the case when a low acuracy POS 
tager is used. This is because the high acuracy 
tager asigns an eroneous label mainly for the 
highly ambiguous cases where a larger context is 
required to disambiguate. These cases are also 
dificult for human anotators to verify and un-
trained anotators tend to mis these cases during 
edit mode experiments. The trained anotators 
show a consistent performance. Nevertheles, 
there is stil some influence of the pre-asigned 
labels. 
16
5.4 Eror
Paterns 
In order to understand the reasons of disagree-
ment betwen the anotators, we analyze the 
confusion matrix for diferent pairs of users for 
the various experimental scenarios. We observe 
that the causes of disagrement are primarily of 
thre kinds: (1) unspecified and/or ambiguous 
guidelines, (2) ignorance about the guidelines, 
and (3) inherent ambiguities present in the sen-
tences. We have found that a large number of the 
erors are due to type (1). For example, in atrib-
ute level anotation, for every atribute two spe-
cial values are ‘0’ (denotes ‘not aplicable for 
the particular lexical item’) and ‘x’ (denotes 
‘undecided or doubtful to the anotator’). How-
ever, we find that both trained and untrained an-
notators have their own distinct paterns of as-
signing ‘0’ or ‘x’. Later we made this point 
clearer with examples and enumerated posible 
cases of ‘0’ and ‘x’ tags. This was very helpful in 
improving the IA.  
A major portion of the erors made by the un-
trained users are due to type (2). For example, it 
was clearly mentioned in the anotation guide-
lines that if a borowed/foreign word is writen in 
the native script, then it has to be taged acord-
ing to its normal morpho-syntactic function in 
the sentence. However, if a word is typed in for-
eign script, then it has to be taged as a foreign 
word.  However, none of the untrained anota-
tors adhered to these rules strictly. 
Finaly, there are instances which are inher-
ently ambiguous. For example, in noun-noun 
compounds, a comon confusion is whether the 
first noun is to be taged as a nouns or an adjec-
tive. These kinds of confusions are evenly dis-
tributed over al the users and at every level of 
anotation. 
One important fact that we arive at through 
the analysis of the confusion matrices is that the 
trained anotators working under close supervi-
sion have few and consistent eror paterns over 
al the experiments, whereas the untrained ano-
tators exhibit no consistent and clearly definable 
eror paterns. This is not surprising because the 
training helps the anotators to understand the 
task and the anotation scheme clearly; on the 
other hand, constant supervision helps clarifying 
doubts arising during anotation.  
6 Conclusion

In this paper we reported our observations for 
POS anotation experiments for Bangla and 
Hindi using the IL-POST anotation scheme un-
der various scenarios. Experiments in Tamil and 
Sanskrit are planed in the future. 
We argue that the observations from the various 
experiments make a case for the ned of training 
and supervision for the anotators as wel as the 
use of apropriate anotation interfaces and 
techniques such as botstraping. The results are 
indicative in nature and ned to be validated with 
larger number of anotators. We sumarize our 
salient contributions/conclusions: 
• The generic tol described here for complex 
and hierarchical word level anotation is ef-
fective in acelerating the anotation task as 
wel as improving the IA. Thus, the tol 
helps reducing the cognitive load asociated 
with anotation. 
• Botstraping, whereby POS tags are pre-
asigned by an automatic tager and human 
anotators are required to edit the incorect 
labels, further acelerates the task, at the risk 
of slight influence of the pre-asigned labels. 
• Although with the help of the tol and tech-
niques such as botstraping we are able to 
bring down the time required by untrained 
anotators to the level of their trained coun-
terparts, the IA, and hence the reliability of 
the anotated data for the former is always 
porer. Hence, training and supervision is 
very important for reliable linguistic anota-
tion. 
We would like to emphasize the last point be-
cause recently it is being argued that Internet and 
other game based techniques can be efectively 
used for gathering anotated data for NLP. While 
this may be suitable for certain types of anota-
tions, such as word sense, lexical similarity or 
afect (se Snow et al. (208) for details), we 
argue that many mainstream linguistic anotation 
tasks such as POS, chunk, semantic roles and 
Trebank anotations cal for expertise, training 
and close supervision. We believe that there is no 
easy way out to this kind of complex linguistic 
anotations, though smartly designed anotation 
interfaces and methods such as botstraping and 
active learning can significantly improve the 
productivity and reliability, and therefore, should 
be explored and exploited in future. 
Acknowledgements 
We would like to thank the anotators Dripta 
Piplai, Anumitra Ghosh Dastidar, Narayan 
Choudhary and Mansi Sharma. We would also 
like to thank Prof. Girish Nath Jha for his help in 
conducting the experiments. 
17
References  
S. Baskaran, K. Bali, M. Choudhury, T. Bhatacharya, 
P. Bhatacharya, G. N. Jha, S. Rajendran, K. Sa-
ravanan, L. Sobha and K.V. Subarao. 208. A 
Comon Parts-of-Spech Tagset Framework for 
Indian Languages. In Proc. of LREC 208. 
J. Cohen. 1960. A coeficient of agrement for nomi-
nal scales. Educational and Psychological Meas-
urement, 20 (1):37-46 
J. Gertzen, and H. Bunt. 206. Measuring anotator 
agrement in a complex hierarchical dialogue act 
anotation scheme. In Proc. of the Workshop on 
Discourse and Dialogue, ACL 206, p. 126-133. 
G. Eryigit. 207. ITU Trebank anotation tol. In 
Proc. of Linguistic Anotation Workshop, ACL 
2007, p. 17-120. 
I. Koutsis, G. Markopoulos, and G. Mikros. 207. 
Episimiotis: A ultilingual Tol for Hierarchical 
Anotation of Texts. In Corpus Linguistics, 207. 
D. Reidsma, N. Jovanovi, and D. Hofs. 204. Design-
ing anotation tols based on the properties of an-
notation problems. Report, Centre for Telematics 
and Information Technology, 204. 
E. Ringer, P. McClanahan, R. Haertel, G. Busby, M. 
Carmen, J. Carol, K. Sepi, and D. Lonsdale. 
207. Active Learning for Part-of-Spech Taging: 
Acelerating Corpus Anotation. In Proc. of Lin-
guistic Anotation Workshop, ACL 207, p. 101-
108. 
R. Snow, B. O’Conor, D. Jurafsky and A. Y. Ng. 
208. Cheap and Fast — But is it God? Evaluat-
ing Non-Expert Anotations for Natural Language 
Tasks. In Proc of EMNLP-08 
K. Tomanek, J. Wermter, and U. Hahn. 207. Efi-
cient anotation with the Jena ANotation Environ-
ment (JANE). In Proc. of Linguistic Anotation 
Workshop, ACL 207, p. 9-16. 
L. von Ahn and L. Dabish. 204. Labeling Images 
with a Computer Game. In ACM Conference on 
Human Factors in Computing Systems, CHI 204. 
Y. Wu, P. Jin, T. Guo and S. Yu. 207. Building Chi-
nese sense anotated corpus with the help of soft-
ware tols. In Proc. of Linguistic Anotation 
Workshop, ACL 207, p. 125-131. 
 
18

