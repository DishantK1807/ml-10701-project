An Acquisition Model for both Choosing and Resolving Anaphora in Conjoined Mandarin Chinese Sentences Benjamin L.
Chen Application Software Department Computer and Communication Research Laboratories Industrial Technology Research Institute W300/CCL, #1 RA~D Rd.
I, Hsinchu, 30045, Taiwan, R.
O. C.
e-mail: blchen@jaguar.ccl.it ri.org.tw Von-Wun Soo Department of Computer Science National Tsing-Hua University Hsinchu, 30043, Taiwan, R.
O. C.
e-mail: eoo@es.nthu.edu.tw Abstract Anaphoric reference is an important linguistic phenomenon to understand the discourse structure and content.
In Chinese natural language processing, there are both the problems of choosing and resolving anaphora.
In Mandarin Chinese, several linguists have attempted to propose criteria to ezplain the phenomenon of anaphora but with controversial results.
On the other hand, search-based computational techniques for resolving anaphora are neither the best way to resolve Chinese anaphora nor to facilitate choosing anaphora.
Thus, to facilitate both choosing and resolving anaphora with accuracy and efficiency, we propose a case-based learning model G-UNIMEM to automatically acquire anaphorie regularity from a sample set of training sentences i, which are annotated with a list of features.
The regularity acquired from training was then tested and compared with other approaches in both choosing and resolving anaphora.
Keywords: anaphoric reference, semantic roles(case), natural language acquisition, easebased learning.
i Introduction In discourse, there may be anaphora in two consecutive sentences.
When anaphora appear in a pair of consecutive sentences, the two consecutive sentences are called conjoined sentences.
In real life conversation, we frequently choose and resolve anaphora to understand the utterances.
There are primarily three types of anaphora in Mandarin Chinese: zero (ellipsis), pronominal (using pronoun) and nominal anaphora\[4\].
Let's take the conjoined Chinese sentencez in (B) to illustrate the phenomenon.
The con1This paper is partially supported" by the Minister of Economic Affairs, R.O.C. under the project no.
33H3100 at ITRI and by National Science Councial of R.O.C. under the grant no.
NSC81-0408-E007-02 at National Tsing Hua University.
The authors also want to thank Dr.
Martha Pollack for valuable comments during 1991 UCSC linguistic institute.
joined sentence in (C) is the English translation of the Chinese sentences m (B).
(B) Yueh-haa sheng-bing.
\[ \] i-thing hui-chia-le.
John got sick \[ \] already gone home (C) Because John .as sick, he has gone home.
Because the anaphora in (B) is a zero anaphora and there is no zero anaphora in English, the antecedent of zero anaphora in (B) must be resolved first before choosing an appropriate pronominal anaphora in Chinese to English translation.
In the translation from (C) to (B), it is not good to directly translate an English pronoun to a Chinese pronoun.
A better way is to resolve the anaphora ire (C) and then choose an appropriate type of anaphora in Chinese.
In natural language processing, better results seems to be attainable if rich linguistic or domain knowledge is available.
However it generally costs much and doesn't seem to be realistic.
The same situation applies for resolving and choosing anaphora in Mandarin Chinese.
If we only used search-based approaches(those that merely used heuristic and algorithmic methods without much linguistic knowledge), the performance was limited.
However, when we intended to adopt linguistic knowledge, we found linguists' theories tended to be controversial and less computable.
Thus, it motivated us to pursue an acquisition model that could acquire linguistic regularity from corpora and then used the regularity to resolve and choose anaphora.
2 Review
of previous approaches 2.1 Search-based approaches Both history list \[1\] and Hobbs's naive syntactic .algorithm \[7\] are search-bused approaches for re~lvmg anaphora.
However, it's not quite obvious to tell which was better than the other with only few exampies.
Thus, we collected 120 testing instances to test them.
Those instances were selected from linguists' ACTES DE COLING-92, NANTES, 23-28 Aotrr 1992 2 7 4 PROC.
OF COLING.92, NANTES, AUO.
23-28, 1992 examples, textbooks, essays and novels.
Half of them contained zero anaphora the other pronominal.
The result showed that the correct number was 111(92.5%) with Hobbs's syntactic algorithm and 87(72.5%) with the history list approach if first matched were selected.
There was 109(90.8%) correct for history list if the last matched were selected.
It seemed that both approaches were applicable to resolve anaphora, tiowever, when there are several NPs with the same semantic features, both approaches may get into troubles.
\]~harthermore, both cannot be used to choose anaphora.
2.2 LinguisCs
criteria Among linguists' works \[31 \[51 \[lll \[121 \[141, Tai's criteria \[14\] was applicable to both choose and resolve anaphora.
Others' suffered from difficulties of extracting features or resolving anaphora.
Table 1 shows 4 co-references for Tai's citeria, which are all applicable when co-referred NPs are human.
For example, consider the following conjoined sentences: Tai:\[ Lao Zhang \] dao-le Meiguoyihou.
\[ \] jiac-le hen-duo pengyou.
John came U.S.A after \[\] made many friends Since John came to the U.S.A., he has made many friends.
The subject in the first sentence is human and coreferred by the subject in the second sentence, so this is a subject-subject co-reference.
According to Table 1, zero anaphora is preferred to the pronominal one and nominal anaphora is not permitted in this example.
Though Tal didn't propose the criteria for resolving anaphora, it was possible to get these criteria just by transforming the choosing criteria in reverse order.
After Tai's criteria were applied to choose and resolve anaphora on the 120 testing instances, we got the success numbers 86(71.7%) and 65(54.2%) respectively.
The results failed to meet our satisfaction.
Through above paragraphs, it appears that searchbased methods have their limitations due to lack of enough linguistisc knowledge and Tai's criteria seems to be applicable to both choose and resolve anaphora.
It might be that Tai's criteria were too general to lead to a high success rate.
More reliable method to acquire regularity might be required to promote the success rate.
We hypothesized the regularity of anaphora could be accounted by causal relations between the features in the conjoined sentences and the antecedents.
In the following section, an acquisition model is introduced.
3 G-UNIMEM: A Case-Based Learning Model In natural language acquisition problem, the restriction of positive-only examples \[2\] has prohibited many machine learning models as a feasible natural language model.
However, a case-baaed learning approach such as Lebowitz's UNIMEM \[9\] \[I0\] seems to be a candidate due to its capability to form concepts incrementally from a rich input domain.
Nevertheless, to apply UNIMEM directly to the acquisition of anaphoric regularity in Mandarin Chinese is still not sufficient.
We have therefore modified UNIMEM into G-UNIMEM.
G-UNIMEM, a modified version of UNIMEM, is an incremental learning system that uses GBM(Geueralized-based Memory) to generalize concepts from a large set of training instances.
The program was implemented in Quintus PROLOG and on SUN workstation.
G-UNIMEM differs from UNIMEM in two respects.
Firstly, if a drinker got drunk many times after taking either whiskey and water or brandy and water, he would induce that water made him drunk with UNIMEM.
This is intuitively incorrect.
Whereas, with G-UNIMEM, he would induce that whiskey and water, brandy and water or water would cause him drunk.
In this case, G-UNIMEM retains the possible causal accounts without committing to erroneous conclusion.
Secondly, G-UNIMEM can extract explicit causal rules from memory hierarchy.
Similar to UNIMEM, G-UNIMEM organizes input training instances into a memory hierarchy according to the frequencies of features.
However, its goal is to explicitly express the generalized causal relationships between two specified types of features: cause features and goal features.
Since there may be inconsistency due to lack of cause features, further refinemeat is needed to obtain consistent causal relations.
Thus, there are four different modules in G-UNIMEM to complete different functions in order to achieve this purpose.
3.1 The
classifier The classifier is the first module that processes all training instances for G-UNIMEM.
Its function is close to UNIMEM that organizes a hierarchy structure to incrementally accommodate a training instance and at the same time generalize the features based on similarities among training instances.
The forming hierarchy is organized as either a g-c-hierarchy or a c-ghierarchy depending on the setup of system, which is defined in Definition 1.
In Appendix A we show the basic classifier algorithm.
Definition 1 A g-c-hierarchy is the hierarchy that every generalized goal feature resides in a GEN-NODE and there is no generalized cause feature that resides between the root node and this GEN-NODE.
A c-ghierarchy doesn't allow any generalized goal feature to reside in the GEN-NODE between the root node and any GEN-NODE where generalized cause features reside.
Figure 1 and Figure 2 show the forming g-chierarchy and c-g-hierarchy respectively after 13 annotated training sentences are entered into G-UNIMEM.
Generally, g-c-hierarchy would be chosen since it retained all possible causal accounts.
For example, the drinker with g-c-hierarchy would induce that whiskey and water, brandy and water or water would cause him drunk; whereas, he would induce whiskey and Ac'rY.s DE COLING-92, NANTES, 23-28 AOU'r 1992 2 7 5 FROC.
OI' COLING-92, NAhrrES, AUG.
23-28, 1992 water, brandy and water with e-g-hierarchy.
The cg-hierarchy is more efficient since no rules are needed to be generated.
Fig. 3 and Fig.
4 show the updating of a GBM before and after inserting a new training instance.
3.2 The
rule generator Once a hierarchy has been constructed by the classifter, the causal rules can be extracted.
The rule generator module serves as the role to extract causal rules from the hierarchy.
It generates all causal rules from the hierarchy as the regularity is retrieved for predictions.
In Fig.
6, if a testing instance is given for choosing anaphora with a query feature list \[ (g,type(*?)), (g,ante(theme)), (c,fl(theme)), (c,anaphor(theme)), (c,s2(obj)), (c,p(pv))\], the retrieval process is searched with a post-order traverse, namely, in the order sequence of the node number 1, 2, 3, 4, 5 and 6.
Since there may be more than one candidate, the system can be setup to select either the first or the most specific one.
If the first one is preferred, type(nil) is yielded as the prediction.
If the most specific answer is preferred, all possible rules will be tried and the one with the most number of contingent features matched will be the answer(i.e, type(pronoun) ).
The sample rules generated from Fig.
1 are shown in Fig.
5. Before generating rules, the GBM is adjusted so that all children of a GEN-NODE are ordered according to their confidence scores of features.
Then all rules are generated in a post-order traversal.
3.3 The
rule filter The rule filter removes those rules that are illformed and useless.
For example, the causal rule 5 in Fig.
5 has no causes which is not a well-formed rule.
It also detects conflicting rules.
Conflicting rules are those that have different goal feature descriptions, which are accounted by the same cause.
For example, the rule I and rule 6 in Fig.
5 are conflicting.
These rules will be detected in this module and then to be resolved by the feature selector.
3.4 The
feature selector Any two conflicting rules are resolved by the feature selector through augmenting the two rules with mutual exclusive contingent cause features, which are prepared in advance.
Dominant features were used in initial regularity acquisition stage; whereas conting ent features were used in feature selection stage.
The ominant features such as goal features are assumed to be those that must be present in every anaphoric rule.
Contingent features are optional.
Fig. 6.
shows the GBM with g-c hierarchy after feature selection proc¢08.
4 Tests
using sentences annotated with mixed features We trained G-UNIMEM with 30, 60, 90, 120 instances using those features mentioned by Tai, and used all the 120 instances as testing instances.
It showed that the approach using Tai's criteria was not promising.
There are two reasons.
First, none of the success rates was as high as those using the history list approach or IIobbs's algorithm.
Second, many conflicting rules remained conflicting due to either that no further features from feature selection were available or too many specific training leading to too many specific rules.
These factors decreased the success rate.
4.1 Selecting
mixed features Since Tal's features were not sufficient, more semantic features were considered.
Among several linguists' works, we tentatively selected some computational feasible syntactic and semantic features from different sources \[3\] \[5\] \[11\] \[12\] \[13\] \[14\] \[15\] as in Table 2.
An example with annotated features is shown below.
Tile notation \[ \] represents zero anaphora.
(C)\[Lao zheng\]i qu-le ji-ge \[nurenli.\[ \]j hen hui zuo-cai.
John married a woman t \] wetlcan cook.
agent theme agent hm sub,by nondefinite John married a woman, and the woman cooked well.
The training feature list for the sentences (C)is : \[(g ante(theme)), (g,type(nil)), (c,fl(agent)), (c,f2(theme)), (c,anaphor(agent)), (c,p(bv)), (c,s2(sub)), (c,h(hm)) (c d(nondefinite)) where the notations g and c represent goa and cause features respectively.
4.2 Testing
using mixed features After semantic features has been determined, we trained G-UNIMEM with 30, 60, 90, 120 instances and used all the 120 instances as testing instances each time.We hypothesized to choose semantic roles(i.e.
ease) as dominant cause features.
The features such as ante(CASE), type(X), anaphor(CASE) and fi (CASE) are dominant features and the number of fi is variant.
The hypothesis was motivated by Sidner \[13\] who used semantic roles to determine focus and resolve definite anaphora.
The others such as h(Hm), p(POS), s2(SYN); d(D), con(s) belong to contingent features.
4.3 The
experimental results It is interesting that the success numbers in Table 3 increased with the number of training instances.
Finally, our results showed that experiments with cg-hierarchy had a little high accuracy rates (95.8% for resolving and 90.8% for choosing anaphora with 120 training instances) than thoee with g-e-hierarchy.
Both accuracy rates were higher than those with TaPs criteria \[14\].
Thus, G-UNIMEM with semantic roles as dominant features promised much higher accuracy rate.
In Appendix B we show some sample rules acquired in Horn-like clauses.
After examination, either the agent or ~heme of first sentence is most likely to AcrEs DE COLING-92, NANTES, 2.3-28 AOOT 1992 2 7 6 PROC.
OF COLING-92.
NANTES, AUG.
23-28, 1992 act as antecedents of anaphora.
Tiffs phenomenon is in coincidence with the investigation on anaphora by Sidner.
That is, the agent often appeared as actor focus and theme as default focus . This is similar to Tai's criteria but is in more compact interpretation.
5 Discussion
There are two concerns in implementing GUNIMEM: (1) The feature set : Is the assignment of dominant features and contingent features objective?
If there is any contingent feature in the assignment that obvi~ ously improves the accuracy rate, it shonld be assigned as dominant feature.
We use statistical methods \[8\] to analyze if contingent features actually improve accuracy rates.
If there is no obvious improvement with contingent features, the division of dominant and corrtingent features is acceptable.
We made the null hypothesis "G-UNIMEM with cg-hierarchy doesn't have obvious improvement with contingent features" and the alternative hypothesis "G-UNIMEM with c-g-hierarchy has obvious improvement with contingent features".
We titan got two test values from test statistics: tl = 0.8472 and t2 < 0.
Both test statistic.q were less than t~ = .05 (= 1.734 with d.f.
= 18).
Thus, the null hypothesis "GUNIMEM with c-g-lfierarchy doesn't have obvious improvement with contingent features" was not rejected, which justified that G-UNIMEM using semantic roles as dominant features was valid.
(2)The sample size : Compared with actual linguistic domain, the 120 training and testing instances are small.
A large corpus is desirable to test the system's performance.
If it becomes available, our resnlts would be more objective and reliable.
6 Conclusion
We have illustrated a way of using machine learning techniques to acquire anaphoric regularity in conjoined Mandarin Chinese sentences.
The regularity was used to both choose and resolve anaphora with considerable accuracy.
Table 4 shows a comparison between different approaches.
In comparison to other approaches, tire proposal of using G-UNIMEM as the acquisition model and using semantic roles as dominant features is practical and serves multiple purposes.
A(:Iq~S I)E COLING-92, NANTES, 23-28 Aolrr 1992 2 7 7 PRO:.
ov COLIN(L92, NANTES, AUG.
23-28, 1992 g,type(nil)):11 1' (g,type(pronoun)):2 I \ \[ (g,ante(theme)):2 i\ I (c,anaphor(theme)):2 (g,ante(theme)):7 \\[ ic,fl(thenm)): 2 (c,fl(theme)):7 \[\~ c,anaphor(t heine) ): 7 i \.
(g,ante(agent)):4 \[ /I (c,fl(agent)):4 "--l~x~ tc,anaphor(theme)):2 (c,f2(theme)):2 i \[, .\] I (c,anaphor(agent)):~ Fig.
1. A g-c-hierarchy of GBM (g,ante(theme)):3 inst:\[(g,type(pronoun))\] j Fig.
4 A
new GBM after inserting a new instance \[(g,type(pronoun)),(g,ante(theme))\] 1: \[(g,type(nil)),(g,ante(theme))\] :\[(c,:fl(gheme)),(c,anaphor(theme))\] 2: \[(g,type(nil)),(g,aute(agent))\]:\[(c,:fl(agent)),(c,anaphor(theme))\] 3: \[(g,type(nil)),(g,ante(agen¢))J:\[(c,fl(agent)),(c,:f2(theme)), (¢, a.aaphor (agent) )'1 4: \[(g,type(nil)), (g,ante(agent))\] :\[(c,f l(agent))\] 5: \[(g,type(nil))\] :\[\] (c,fl(theme)):9 \] I (c'fl(agent)):4 16: \[(g,type(pronotm) ), (g, a.ate (gheme))\] :c,amaphor(theme)):9\[ / \[(c,:f 1 (theme)) (c,aaaphor(th~e))\] (g,ante(theme)):9 \[ Fig.
5 The
sample rules generated :from \[ (c,anaphor(theme)):2 " Fig.
1. \[ (g,type(nil)):2 ~g,ante(agent)):2, 3 ......
(c,anaphor(agent)):~ lg,type(pr°n°un))i~ (c,f2(theme)):2 \[ (g,type(nil)):2 | g,type(nil)):7 \] (g,ante(agent)):2,.
\[ Fig.
2. A c-g-hierarchy of GBM (g,type(nil)):2 (g,ante(theme)):2 1 Fig.
3 A
GBM with two training instances I/c,anaphor/theme/): ' / / k (c,anaphor\[theme)):2 \ (g,type(nil)):2 \ 2 ~ \] (g,ante(agent)):2 \ (g,type(pronoun)):2' 5 (c,anaphor(agent)):~ (g,ante(theme)):2 (c,f2(theme)):2 | (c,s2(obj)):2 (g,type(nil)):2 / (c,p(pv)):2 (g,ante(agent)):2 \] 1 (g,type(nit)):7 (g,ante(theme)):7 1 Fig.
6 The
c-g hierarchy after conflict resolution from Fig.
2 AcrEs
DE COLING-92.
NANTES. 23-28 AO0r 1992 2 7 8 PROC.
OF COLING-92, NANTES.
AUG. 23-28, 1992 Table 1: TaPs criteria for choosing anaphora I .... wneos_e \] zero--5-~ro~ n~onoun~rouou~-~~_nom,nal j I Not permitted \[ nominal \] zero \[ ~~----ze-m \] no~.
u j~~J.
c~-#&Tgf&__ ~ ~ i~pre~rre~ g87 T~m:es and efe e ces ~ . feature ~ notation .... semantic antecedent ante CASE)~ semanttc role \[ ti(CASE) '~ I --anaphoriCXSE) -~an or no~~~n(nonhm~-syntactic anaphora \[ type(X) 1___ subject ....
\[ sz(suz~, ...
I position of anap\[i-o~--p~gv~o~-\] I bv: before verb; pv: post-verb \[_____.~finite . \[ d(uondefinite) __ I co.ne~or I c°n/s) notation :'GAS rE-rE-rE-rE-rE-rE-~g-presents a variable for a semantic role an~ order in a sequence of roles.
Tans = Chm~ -ru~-lg number Accuracy rate (for resolving) Accuracy rate (for choosing ~g number (after resolving) Table 3: Comparison of G-UNIMEM using ~ against Tai's criteria Group Candidate E 3 ~Y-~"qS"0~ --~-'E~20 "~ TaP~4-choice criteria 30** specific 58/62 87/96 104/110 114/120=95.0%,,one 58/62 86/96 104/110 111/120=:92.5% trst\]'?/~ 53/60 83/99-~ 95/109 109/120:-'90.8°70~TT2ffg"TE=71.7~U-o specific 56/65 83/100 88/110 101/120=84.2% 4 -----~*~-notation: * ".
for choosin~r~ ~g; accuracy ~g~'cc~sthat have applicable rules; none, in column 2 means no contingent features are used.
Table 4: Comparison of different approach~ MethOds ~_~os~ 1.
--history list L~___~_~e search \[" llobbs"s syntactic -~'-----/'-~-~~e~y-~-y~ \].
al~;orithm ~ J ~s mter,'g----c oo~ pre,ct~~ \[.~ Chen's criteria \[ eho~ pl_~.dlc~ not easy J \] G-UNIMEM & dominant \] choose & relive \] predict \] easy J I features \] ~_~ J AcrEs DE COLING-92, NANTES, 23-28 noun" 1992 2 7 9 PROC.
OF COL1NG -92, NANTES, AUG.
23 -28, 1992 Appendix A.
The basic classifier algorithm input: The current node N of the concept hierarchy.
The name I of an unclassified instance.
The set of I's unaccounted features F.
Results: The concept hierarchy that classifies the instance.
Top-level call: classifier( Top-node, I, F) Variables: N, N', C and NC are nodes in the hierarchy.
G, H, and K are sets of features.
J is an instance stored on a node.
P~ is a variable of set.
Classifier(N, 1, F).
Let G be the set of features stores in N.
Let H be the features in F that match features in G.
Let K1 be the features in F that do not match features in G.
Let K2 be the features in G that do not match features in H.
Let H', KI' and K2' be the sets of features after Adjust(H,K1,Ki,H',KI',Ki') /* adjust goal and cause features for g-c-hierarchy or c-g-hierarchy */ if N is not the root node, then if H is empty set/* no features match */ then return False else if both H' and KI' are not empty sets then ~/* split node N */ spht N into N' and NC where NC is a child of N'; N' contains features in H' with confidence scores and I as a instance with features KI'; each confidence score in tl' is increased by 1; the remaining features and instances belong to NC; return Split.
) else if It' and H are equal/* all features match */ then increase each confidence score in N by 1.
for each child C of node N/* continue match remaining features */ call Classifier(C, I, KI') and collect returns to the set It, if any Classifier(C, I, KI') call return True or Split then break.
if 1% is \[False \] /* All trials fail, try to do generalization */ then for each instance J of node N call Generalize(N, J, I, KI') and collect returns to the set 1%, if any Generallze(N, J, I, KI') call return True then break.
if tt is \[ False \]/* All trials fail, insert I as an instance of N */ then store I as an instance of node N with features KI'.
return True.
Appendix B.
Sample rtllen of regularity ~ith high probability of appearance in Horn-like clauses \[an~;e (agent), gype(nil)\] :\[anaphor(agent),f2(theme),ft (agent)\] \[ante(agent),type(nil)\] :\[anaphor (agent),f l(agent)\] \[ante(theme),type(nil)\] :\[p(bv).
s2 (sub),d(nondefinite), anaphor (agent), f I (agent), f2 (theme)\] \[ante(theme).
type(nil)\] :\[h(nonhm), anaphor (theme).
f i (agent), fi(theme)\] \[ante(theme) .type(nil)\] :\[anaphor (theme),fl (theme)\] \[ante(ar E) .type(pronoun)\] :\[anaphor(agent), f2(pred), fl(arg)\] \[ant • (agent).
type (pronoun) \] : \[h(hm).
d(definite), con(s) 0anaphor(agent).
f2 (theme), f i (agent)\] \[unt e (art),type(pronoun)\] :\[k(hm), anaphor (a~ent), f 2(pred),f l(arg)\] ..
\[ante(theme). type(pronoun)\] :\[s2(obj ),p(pv),d(definite),anaphor(theme),f I (agent),f2(theme)J \[ant e (art) .type(pronoun)\] :\[h(hm), anaphor (theme).
f2 (pred),f I (arg)\] Acrf~s DE COLING-92, NAN'IT.S, 23-28 AoU'r 1992 2 8 0 PROC.
OF COLING-92, NANTES.
AUG. 23-28, 1992 References James Allen (1987), Natural Language Understanding, The Benjamin/Cummings Publishing Co.
Robert Berwick (1986), Learning From Positive-only Examples: The Subjective Principle and Three Case Studies.
In R.Michalski, J.
Carbonell, and T.
Mitchell(cal). Machine Learning: An Artificial Intelligence Approach Vol.
\[I, Chapter 21, Morgan Kanfmann Publishers, Inc.
Ping Chen (1984), A Discourse Analysis of Third Person Zero Anaphora in Chinese, reproduced by Indianan University Linguistics Club, Bloomington, Indiana.
Ping Chert (1987), A Discourse Approach to Zero Anaph0ra in Chinese, ghonggua Ynwen, Beijiug.
Chauncey C.
Chu (1983), Definiteness, Presuppositimt, Topic and Focus in Mandarin Chine~e, Student Book company, Taipei, Taiwan.
John H, Gennari, Pat Lmlgley and Doug.
Fisher (1989), Models of Incremental Concept Formation, Artificial Inielligence 40.
J. llobbs (1978), Resolving Pronoun References, Lingua 44, North-tlolland Publishing Co.
Richard Johnson and Court Bhattacharyya (1985), Statistics: Principles and Methods, Chapter 6 (Section,I), 9, 10, 11, Published by Hohn Wiley & Sons.
Michael Lebowitz (1986), Concept Learning in ~ Rich Input Domain: Generalization-based Memory.
Ira It.
Midralski, J.
Carbonell, and T.
Mitcholl(ed). Machine Learning: An Artificial \[nlelligence Approach Vol.
1I, Chapter 8, Morgan Kaufmann Publishers, Inc.
Michael Lebowitz (1986), Integrated Learning: Cmttrolling Explanation, Cognitive Science 10.
Cherry lug Li (1985), Participant anapho~ in Mandarin Chinese, University of Florida Press.
Met Du Li (1986), Anapho,~c Structure of Chinese, Student Book CO., Taipei, Taiwan.
C. Sidner (1983), Focusing in tim Compre ~ hens\[on of Definite Anaphora, Iu Computational Models of Discourse, M.
Brady and R.
Berwick, eds., MIT Press.
J. lI.
Tat (1978), Auaphoric Constraints in Mandarin Chinese Narrative Discourse.
In J.
Hinds (ed).
Anaphora in Discourse, Linguistic Research, Edmonton, Alberta.
Ting-Chi chanties Tang (1975), A Case Grammar Classification of Chinese Verbs, Ha\[Gun Book Company, Taipei, Taiwaa .

