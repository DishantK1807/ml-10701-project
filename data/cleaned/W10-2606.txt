Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 37–44,
Uppsala, Sweden, 15 July 2010. c©2010 Association for Computational Linguistics
Self-Training without Reranking for Parser Domain Adaptation and 
Its Impact on Semantic Role Labeling 
 
Kenji Sagae 
Institute for Creative Technologies 
University of Southern California 
Marina del Rey, CA 90292 
sagae@ict.usc.edu 
 
 
  
 
Abstract 
We compare self-training with and with-
out reranking for parser domain adapta-
tion, and examine the impact of syntactic 
parser adaptation on a semantic role la-
beling system.  Although self-training 
without reranking has ben found not to 
improve in-domain acuracy for parsers 
trained on the WSJ Pen Trebank, we 
show that it is surprisingly efective for 
parser domain adaptation. We also show 
that simple self-training of a syntactic 
parser improves out-of-domain acuracy 
of a semantic role labeler. 
1 Introduction

Improvements in data-driven parsing aproaches, 
coupled with the development of trebanks that 
serve as training data, have resulted in acurate 
parsers for several languages.  However, port-
ability acros domains remains a chalenge: pars-
ers trained using a trebank for a specific domain 
generaly perform comparatively porly in other 
domains. In English, the most widely used train-
ing set for parsers comes from the Wal Stret 
Journal portion of the Pen Trebank (Marcus et 
al., 193), and constituent parsers trained on this 
set are now capable of labeled bracketing preci-
sion and recal of over 90% (Charniak and John-
son, 205; Huang, 208) on WSJ testing sen-
tences.  When aplied without adaptation to the 
Brown portion of the Pen Trebank, however, 
an absolute drop of over 5% in precision and re-
cal is typicaly observed (McClosky et al., 
206b).  In pipelined NLP aplications that in-
clude a parser, this drop often results in severely 
degraded results downstream. 
We present experiments with a simple self-
training aproach to semi-supervised parser do-
main adaptation that produce results that contra-
dict the comonly held asumption that im-
proved parser acuracy canot be obtained by 
self-training a generative parser without rerank-
ing (Charniak, 197; Stedman et al., 203; 
McClosky et al., 206b, 208).
1
 We compare 
this simple self-training aproach to the self-
training with reranking aproach proposed by 
McClosky et al. (206b), and show that although 
cClosky et al.’s aproach produces beter la-
beled bracketing precision and recal on out-of-
domain sentences, higher F-score on syntactic 
parses may not lead to an overal improvement in 
results obtained in NLP aplications that include 
parsing, contrary to our expectations.  This is 
evidenced by results obtained when diferent ad-
aptation aproaches are aplied to a parser that 
serves as a component in a semantic role labeling 
(SRL) system.  This is, to our knowledge, the 
first atempt to quantify the benefits of semi-
supervised parser domain adaptation in semantic 
role labeling, a task in which parsing acuracy is 
crucial. 
2 Semi-supervised parser domain adap-
tation with self-training 
Because trebanks are expensive to create, while 
plain text in most domains is easily obtainable, 
semi-supervised aproaches to parser domain 
adaptation are a particularly atractive solution to 
the domain portability problem.  This usualy 
involves a manualy anotated training set (a 
                                                
1
 Reichart and Rapoport (207) show that self-training 
without reranking is efective when the manualy anotated 
training set is smal.  We show that this is true even for a 
large training set (the standard WSJ Pen Trebank training 
set, with over 40k sentences). 
37
trebank), and a larger set of unlabeled data 
(plain text).  
Bachiani and Roark (203) obtained positive 
results in unsupervised domain adaptation of 
language models by using a spech recognition 
system with an out-of-domain language model to 
produce an automaticaly anotated training cor-
pus that is used to adapt the language model us-
ing a maximum a posteriori (MAP) adaptation 
strategy.  In subsequent work (Roark and Bac-
chiani, 203), this MAP adaptation aproach was 
aplied to PCFG adaptation, where an out-of-
domain parser was used to anotate an in-domain 
corpus automaticaly with multiple candidate 
tres per sentence.  A substantial improvement 
was achieved in out-of-domain parsing, although 
the obtained acuracy level was stil far below 
that obtained with domain-specific training data. 
More recent work in unsupervised domain ad-
aptation for state-of-the-art parsers has achieved 
acuracy levels on out-of-domain text that is 
comparable to that achieved with domain-
specific training data (McClosky et al., 206b). 
This is done in a self-training seting, where a 
parser trained on a trebank (in a sed domain) is 
used to parse a large amount of unlabeled data in 
the target domain (asigning only one parse per 
sentence).  The automaticaly parsed corpus is 
then used as aditional training data for the 
parser.  Although initial atempts to improve in-
domain parsing acuracy with self-training were 
unsucesful (Charniak, 197; Stedman et al., 
203), recent work has shown that self-training 
can work in specific conditions (McClosky et al., 
206b), and in particular it can be used to im-
prove parsing acuracy on out-of-domain text 
(Reichart and Rapoport, 207). 
2.1 Self-training with reraking 
McClosky et al. (206b) presented the most suc-
cesful semi-supervised aproach to date for ad-
aptation of a WSJ-trained parser to Brown data 
containing several genres of text (such as relig-
ion, mystery, romance, adventure, etc.), obtain-
ing a substantial acuracy improvement using 
only unlabeled data. Their aproach involves the 
use of a first-stage n-best parser and a reranker, 
which together produce parses for the unlabeled 
dataset.  The automaticaly parsed in-domain 
corpus is then used as aditional training mate-
rial.  In light of previous failed atempts to im-
prove generative parsers through self-training 
(Charniak, 197; Stedman et al., 203), 
McClosky et al. (206a) argue that the use of a 
reranker is an important factor in the suces of 
their aproach. That work used text from the LA 
Times (taken from the North American News 
Corpus, or NANC), which is presumably more 
similar to the parser’s training material than to 
text in the Brown corpus, and resulted not only in 
an improvement of parser acuracy on out-of-
domain text (from the Brown corpus), but also in 
an improvement in acuracy on in-domain text 
(the standard WSJ test set of the Pen Trebank). 
It can be argued that the McClosky et al. ap-
proach is not a pure instance of self-training, 
since two parsing models are used: the first-stage 
generative model, and a discriminative model for 
reranking.  The generative parser is improved 
based on the output of the discriminative model, 
but McClosky et al. found that the discriminative 
model does not improve when retrained with its 
own output. 
2.2 Self-training without reraking 
Although there have ben instances of self-
training (or similar) aproaches that produced 
improved parser acuracy without reranking, the 
suces of these eforts are often atributed to 
other specific factors. 
Reichart and Rapoport (207) obtained posi-
tive results in in-domain and out-of-domain sce-
narios with self-training without reranking, but 
under the constant condition that only a rela-
tively smal set of manualy labeled data is used 
as the sed training set. Sagae and Tsuji (207) 
improved the out-of-domain acuracy of a de-
pendency parser trained on the entire WSJ train-
ing set (40k sentences) by using unlabeled data 
in the same domain as the out-of-domain test 
data (biomedical text).  However, they used 
agrement betwen diferent parsers to estimate 
the quality of automaticaly generated training 
instances and selected only sentences with high 
estimated acuracy.  Although the parser im-
proves when trained with its own output, the 
training instances are selected through the use of 
a separate dependency parsing model. 
2.3 Simple
self-training without reranking 
for domain adaptation 
It is now comonly asumed that the simplest 
form of self-training, where a single parsing 
model is retrained with its own output (a single 
parse tre per sentence, without reranking or 
other means of training instance selection or es-
timation of parse quality), does not improve the 
38
model’s acuracy.
2
 This asumption, however, is 
largely based on previous atempts to improve 
in-domain acuracy through self-training 
(Stedman et al., 203; Charniak, 197; 
McClosky et al., 206a, 208). We wil refer to 
this type of self-training as simple self-training, 
to avoid confusion with other self-training set-
tings, such as McClosky et al.’s, where a 
reranker is involved. 
We propose a simple self-training framework 
for domain adaptation, as folows: 
1. A generative parser is trained using a tree-
bank in a specific source domain. 
2. The parser is used to generate parse tres 
from text in a target domain, diferent 
from the source domain. 
3. The parser is retrained using the original 
trebank, augmented with the parse tres 
generated in step 2. 
There are intuitive reasons that may lead one 
to asume that simple self-training should not 
work.  One is that no aditional information is 
provided to the model.  In self-training with 
reranking, the generative model can be enriched 
with information produced by the discriminative 
model.  When two parsers are used for training 
instance selection, one parser informs the other. 
In simple self-training, however, there is no adi-
tional source of syntactic knowledge with which 
the self-trained model would be enriched. 
Another posible reason is that the output of 
the self-trained parser should be expected to in-
clude the same erors found in the automaticaly 
generated training material.  If the initial parser 
has por acuracy on the target domain, the 
training data it generates wil be of por quality, 
resulting in no improvement in the resulting 
trained model. The self-trained model may sim-
ply learn to make the same mistakes as the origi-
nal model. 
Conversely, there are also intuitive reasons for 
why it might work.  A posible source of por 
performance in new domains is that the model 
lacks coverage.  Specific lexical items and syn-
tactic structures in a new domain apear in a va-
riety of contexts, acompanied by diferent 
words and structures.  The parser trained on the 
source domain may analyze some of these new 
                                                
2
 Except for in cases where the initial model is trained using 
a very smal trebank. 
items and structures corectly, and it may also 
make mistakes.  As long as erors in the auto-
maticaly generated training material are not al 
systematic, the benefits of ading target-domain 
information could outweigh the adition of noise 
in the model. 
Naturaly, it may be that these conditions hold 
for some pairs of source and target domains but 
not others.  In the next section, we present ex-
periments that investigate whether simple self-
training is efective for one particular set of train-
ing (WSJ) and testing (Brown) corpora, which 
are widely used in parsing research for English. 
3 Domain
adaptations experiments 
In our experiments we use primarily the 
Charniak (200) parser.  In a few specific ex-
periments we also use the Charniak and Johnson 
(205) reranker; such cases are noted explicitly 
and are not central to the paper, serving mostly 
for comparisons.  We folow the thre steps de-
scribed in section 2.3.  The manualy labeled 
training corpus is the standard WSJ training sec-
tions of the Pen Trebank (sections 02 to 21). 
Sections 2 and 23 are used as in-domain devel-
opment and testing sets, respectively.  The out-
of-domain material is taken from the Brown por-
tion of the Pen Trebank.  We use the same 
Brown test set as McClosky et al. (206b), every 
tenth sentence in the corpus.  Another tenth of 
the corpus is used as a development set, and the 
rest of the Brown corpus is not used. The out-of-
domain text then contains not one but several 
genres of text. The larger set of unlabeled data is 
composed of aproximately 5.3 milion words 
(320k sentences) of 20th century novels available 
from Project Gutenberg
3, which do not match 
exactly the target domain, but is closer to it in 
general than to the source domain (WSJ). 
3.1 Simple
self-training results 
The precision, recal and F-score of labeled 
brackets of the initial parser, trained only on the 
WSJ Pen Trebank, are shown in the first row 
of results in Table 1 for the WSJ (in-domain) test 
set and the Brown (out-of-domain) test set. 
These figures serve as our baseline. The second 
row of results in Table 1 shows the results ob-
tained with a model produced using simple self-
training. The baseline model is used to parse the 
entire unlabeled dataset (320k sentences), and 
                                                
3
 htp:/ww.gutenberg.org 
39
the resulting parse tres are aded to the WSJ 
training set to produce the self-trained model.  
A substantial improvement is observed for the 
target test set (Brown), close to an absolute im-
provement of 2% in precision, recal and F-score. 
Table 1 also shows that parser acuracy fel by 
1% on WSJ.  Although we do not se this as a 
problem, since the our goal is to produce an im-
proved model for parsing Brown, it is interesting 
that, unlike in the work of McClosky et al. 
(206a, 206b) where self-training includes 
reranking, simple self-training is efective spe-
cificaly for domain adaptation, but not for im-
proving the acuracy of the parser on in-domain 
data.  At least in this case, simple self-training 
does not result in an absolutely improved parsing 
model (as apears to be the case with McClosky 
et al.’s self-training), although it does result in an 
improved model for the target data. 
Finaly, the last row in Table 1 shows the re-
sults on WSJ and Brown obtained by McClosky 
et al. (206a, 206b) using self-training with 
reranking.  As they have shown, the discrimina-
tive reranker can be used to provide further im-
provements, as discused in the next subsection. 
Unlike McClosky et al. (206a), we did not 
give diferent weights to the original and auto-
maticaly generated training instances.  In our 
experiments with the Brown development data, 
varying the weight of the gold-standard WSJ 
training data from 1 to 7, we observed only smal 
diferences in F-score (Table 2). The highest F-
score, obtained when the WSJ training corpus is 
given a relative weight of 3, was only 0.07 
higher than the F-score obtained when the WSJ 
training corpus is given a relative weight of 1. 
 
WSJ relative weight Brown dev F-score 
1 84.51 
2 84.52 
3 84.58 
4 84.53 
5 84.51 
6 84.5 
7 84.57 
Baseline (WSJ only) 82.91 
Table 2: Brown development set F-scores ob-
tained with self-trained models with diferent 
relative weights given to the gold-standard WSJ 
training data. The last row shows the F-score for 
the original model (without adaptation). 
 
 
Table 3 shows results on the Brown develop-
ment set when diferent amounts of unlabeled 
data are used to create the self-trained model. 
Although F-score generaly increases with more 
unlabeled data, the efect is not monotonic. 
McClosky et al. observed a similar efect in their 
self-training experiments, and hypothesized that 
this may be due to diferences betwen portions 
of the unlabeled data and the target corpus, and 
to varying parsing dificulty in portions of the 
unlabeled data, which results in varying quality 
of the parse tres produced automaticaly for 
training.  A large improvement in F-score over 
the baseline is observed when ading only 30k 
sentences.  Aditional improvement is observed 
when aditional sentences are aded, but these 
are smal in comparison. One interesting note is 
 WSJ Brown 
 Precision Recal F-score Precision Recal F-score 
Baseline 89.49 8.78 89.13 83.93 83.19 83.56 
Self-trained 8.26 87.86 8.06 85.78 85.05 85.42 
MCJ   91.0   87.1 
 
Table 1. Labeled constituent precision, recal and F-score for the WSJ and Brown test sets, ob-
tained with the baseline model (trained only on the WSJ training set) and with the self-trained 
model.  Results on Brown show an absolute improvement of almost 2%, while results on WSJ 
show a drop of about 1%. The last row shows the results obtained by McClosky et al. (206a, 
206b) using self-training with reranking (denoted as MCJ), for comparison purposes. 
40
that, although self-training produced improved 
bracketing precision and recal, part-of-spech 
taging acuracy of Brown remained largely un-
changed from the baseline, in the range of 
94.42% to 94.50% acuracy.  It is posible that 
separate adaption for part-of-spech taging may 
improve parsing F-score further. 
The results in this section show that simple 
self-training is efective in adapting WSJ-trained 
parser to Brown, but more experiments are 
neded to determine if the same efects observed 
in our simple self-training experiments would 
also be observed with other pairs of sed training 
data and target datasets, and what characteristics 
of the datasets may afect domain adaptation. 
3.2 Self-training with reranking results 
To provide a more informative comparison be-
twen the results obtained with simple self-
training and other work, we also performed 
McClosky et al.’s self-training with reranking 
using our unlabeled dataset.  In this experiment, 
intended to provide a beter understanding of the 
role of the unlabeled data (20th century novels 
vs. LA Times articles), we parse the unlabeled 
dataset with the Charniak (200) parser and the 
Charniak and Johnson (205) discriminative 
reranker to produce aditional training material 
for the generative parser.  The resulting genera-
tive parser produces slightly improved F-scores 
compared to the simple self-training seting 
(8.78% on WSJ and 86.01 on Brown), although 
a slight drop in WSJ F-score is stil observed, 
indicating that the use of news text is likely an 
important factor in McClosky et al.’s superior F-
score figures. 
Al of these models can be used to produce n-
best parses with the Charniak parser, and these 
can be reranked with the Charniak and Johnson 
reranker, whether or not the self-training proce-
dure that created the generative model involved 
reranking.  McClosky et al. found that although 
their self-training procedure involves reranking, 
the gains in acuracy are orthogonal to those 
provided by a final reranking step, aplied to the 
output of the self-trained model.   As in their 
case, aplying the WSJ-trained reranker to our 
self-trained model improves its acuracy. In the 
case of our simple self-trained model, the im-
provement is of about 1.7%, which means that if 
a reranker is used at run-time (but not during 
self-training), F-score goes up to 87.12%. Inter-
estingly, aplying a final pas of reranking to the 
model obtained with self-training with reranking 
brings F-score up only by les than 1.2%, to 
87.17%.  So at least in our case, improvements 
provided by the use the reranker apear not to be 
completely orthogonal. 
4 Semantic
Role Labeling with syntactic 
parser adaptation 
To investigate the impact of parser domain adap-
tation through self-training on aplications that 
depend on parser output, we use an existing se-
mantic role labeling (SRL) system, the Ilinois 
Semantic Role Labeler
4, replacing the provided 
parsing component with our (WSJ) baseline and 
(adapted) self-trained parsers. 
We tested the SRL system using the datasets 
of the CoNL 205 shared task (Careras and 
Màrquez, 205).  The system is trained on the 
WSJ domain using PropBank (Palmer et al. 
205), and the shared task includes WSJ and 
Brown evaluation sets.  Using the baseline WSJ 
syntactic parser, the SRL system has an F-score 
of 7.49 on WSJ, which is a competitive result 
for systems using a single syntactic analysis per 
sentence.  The highest scoring system (also a 
UIUC system) in the shared task has 79.4 F-
score, and used multiple parse tres, which has 
ben shown to improve results (Punyakanok et 
al., 205).  On the Brown evaluation, F-score is 
64.75, a step drop from the performance of the 
system on WSJ, which reflects that not just the 
syntactic parser, but also other system compo-
nents, were trained with WSJ material.  The 
                                                
4
 htp:/l2r.cs.uiuc.edu/~cogcomp/asoftware.php?skey=SRL 
Sentences aded Brown dev. F-score 
0 (baseline) 82.91 
10k 83.76 
20k 84.02 
30k 84.29 
50k 84.26 
100k 84.19 
150k 84.38 
200k 84.51 
250k 84.42 
300k 84.51 
Table 3: Brown development set F-scores 
obtained with self-trained models created 
with diferent amounts of unlabeled data. 
41
highest scoring system on the Brown evaluation 
in the CoNL 205 shared task had 67.75 F-
score. 
Table 4 shows the results on the Brown 
evaluation set using the baseline WSJ SRL sys-
tem and the results obtained under thre self-
training parser domain adaptation schemes: sim-
ple self-training using novels as unlabeled data 
(section 3.1), the self-trained model of McClosky 
et al.5, and the reranked results of the cClosky 
et al. self-trained model (which has F-score com-
parable to that of a parser trained on the Brown 
corpus). 
As expected, the contributions of the thre 
adapted parsing models alowed the system to 
produce overal SRL results that are beter than 
those produced with the baseline seting.  Sur-
prisingly, however, the use of the model created 
using simple self-training and sentences from 
novels (sections 2.3 and 3.1) resulted in beter 
SRL results than the use of McClosky et al.’s 
reranking-based self-trained model (whether its 
results go through one aditional step of rerank-
ing or not), which produces substantialy higher 
syntactic parsing F-score. Our self-trained pars-
ing model results in an absolute increase of 4% 
in SRL F-score, outscoring al participants in the 
shared task (of course, systems in the shared task 
did not use adapted parsing models or external 
resources, such as unlabeled data). The im-
provement in the precision of the SRL system 
                                                
5
 htp:/ww.cs.brown.edu/~dmc/selftraining.html 
using simple self-training is particularly large. 
Improvements in the precision of the core argu-
ments Arg0, Arg1, Arg2 contributed heavily to 
the improvement of overal scores. 
We note that other parts of the SRL system 
remained constant, and the diference in the re-
sults shown in Table 4 come solely from the use 
of diferent (adapted) parsers. 
5 Conclusion

We explored the use of simple self-training, 
where no reranking or confidence measurements 
are used, for parser domain adaptation.  We 
found that self-training can in fact improve the 
acuracy of a parser in a diferent domain from 
the domain of its training data (even when the 
training data is the entire standard WSJ training 
material from the Pen Trebank), and that this 
improvement can be caried on to modules that 
may use the output of the parser.  We demon-
strated that a semantic role labeling system 
trained with WSJ training data can improve sub-
stantialy (4%) on Brown just by having its 
parser be adapted using unlabeled data. 
Although the fact that self-training produces 
improved parsing results without reranking does 
not necesarily conflict with previous work, it 
does contradict the widely held asumption that 
this type of self-training does not improve parser 
acuracy.  One way to reconcile expectations 
based on previous atempts to improve parsing 
acuracy with self-training (Charniak, 197; 
 Precision Recal F-score 
Baseline (WSJ parser) 6.57 63.02 64.75 
Simple self-trained parser 
(this paper) 
71.6 6.10 68.7 
MCJ self-trained parser 69.18 65.37 67.2 
MCJ self-train and rerank 68.62 65.78 67.17 
 
Table 4. Semantic role labeling results using the Ilinois Semantic Role Labeler (trained on 
WSJ material from PropBank) using four diferent parsing models: (1) a model trained on 
SJ, (2) a model built from the WSJ training data and 320k sentences from novels as unla-
beled data, using the simple self-training procedure described in sections 2.3 and 3.1, (3) the 
McClosky et al. (206a) self-trained model, and (4) the McClosky et al. self-trained model, 
reranked with the Charniak and Johnson (205) reranker. 
42
Stedman et al., 203) and the results observed 
in our experiments is that we focus specificaly 
on domain adaptation.  In fact, the in-domain 
acuracy of our adapted model is slightly inferior 
to that of the baseline, more in line with previous 
findings. 
This work represents only one aditional step 
towards understanding of how and when self-
training works for parsing and for domain adap-
tation.  Aditional analysis and experiments are 
neded to understand under what conditions and 
in what domains simple self-training can be ef-
fective.  
One question that sems particularly interest-
ing is why the models adapted using self-training 
with reranking and news text, which produce 
substantialy higher parsing F-scores, did not 
outperform our model built with simple self-
training in contribution to the SRL system.  Al-
though we do not have an answer to this ques-
tion, two factors that may play a role are the do-
main of the training data and the use of the 
reranker, which may provide improvements in 
parse quality that are of a diferent kind of those 
most neded by the SRL system. This points to 
another interesting direction, where adapted 
parsers can be combined. Having diferent ways 
to perform semi-supervised parser adaptation 
may result in the creation of adapted models with 
improved acuracy on a target domain but difer-
ent characteristics.  The output of these parsers 
could then be combined in a voting scheme 
(Henderson and Bril, 199) for aditional im-
provements on the target domain. 
Acknowledgments 
We thank Andrew S. Gordon and the anonymous 
reviewers for valuable coments and suges-
tions.  The work described here has ben spon-
sored by the U.S. Army Research, Development, 
and Enginering Comand (RDECOM).  State-
ments and opinions expresed do not necesarily 
reflect the position or the policy of the United 
States Government, and no oficial endorsement 
should be infered. 
References 
Michiel Bachiani and Brian Roark. 203. Unsuper-
vised language model adaptation. In Procedings 
of the International Conference on Acoustics, 
Spech and Signal Procesing (ICASP). 
Xavier Careras and Lluís Màrquez. 205. Introduc-
tion to the CoNL-205 Shared Task: Semantic 
Role Labeling. In Procedings of the CoNL 205 
shared task. 
Eugene Charniak. 197. Statistical parsing with a con-
text-fre gramar and word statistics. In Proced-
ings of AAI, pages 598–603. 
Eugene Charniak. 200. A maximum-entropy-
inspired parser. In Procedings of the First Met-
ing of the North American Chapter of the Asocia-
tion for Computational Linguistics (NACL). 
Pages 132-139. Seatle, WA. 
Eugene Charniak and Mark Johnson. 205. Coarse
to-fine n-best parsing and MaxEnt discriminative 
reranking. In Procedings of the 205 Meting of 
the Asociation for Computational Linguistics 
(ACL), pages 173–180. 
John C. Henderson, Eric Bril. 199. Exploiting Di-
versity in Natural Language Procesing: Combin-
ing Parsers. In Procedings of the Fourth Confer-
ence on Empirical Methods in Natural Language 
Procesing (EMNLP-9), p. 187–194. Colege 
Park, Maryland. 
Liang Huang (208). Forest Reranking: Discrimina-
tive Parsing with Non-Local Features. In Proced-
ings of the 208 Meting of the Asociation for 
Computational Linguistics (ACL). Columbus, OH. 
Mitchel P. Marcus, Mary An Marcinkiewicz and 
Beatrice Santorini. 193. Building a large ano-
tated corpus of English: the Pen Trebank. In 
Computational Linguistics 19(2), 313-30. 
David McClosky, Eugene Charniak and Mark John-
son. 206a. Efective self-training for parsing. In 
Procedings of the Main Conference on Human 
Language Technology Conference of the North 
American Chapter of the Asociation of Computa-
tional Linguistics (NACL). New York, NY. 
David McClosky, Eugene Charniak and Mark John-
son. 206b. Reranking and self-training for parser 
adaptation. In Procedings of the 21st international 
Conference on Computational Linguistics and the 
4th Anual Meting of the Asociation For Com-
putational Linguistics (ACL). Sydney, Australia. 
David McClosky, Eugene Charniak and Mark John-
son. 208. When is self-training efective for pars-
ing? In Procedings of the 2nd international Con-
ference on Computational Linguistics (COLING) 
Volume 1. Manchester, United Kingdom. 
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 
205. The proposition bank: An anotated corpus 
of semantic roles. Computational Linguistics, 
31(1). 
Vasin Punyakanok, Peter Komen, Dan Roth and 
Wen-tau Yih. 205. Generalized Inference with 
Multiple Semantic Role Labeling Systems. In Pro-
cedings of the CoNL 205 shared task. 
43
Roi Reichart and Ari Rapoport. 207. Self-training 
for enhancement and domain adaptation of statisti-
cal parsers trained on smal datasets.  In Proced-
ings of the 45th Anual Meting of the Asociation 
for Computational Linguistics (ACL). Pages 616-
623. Prague, Czech Republic. 
Brian Roark and Michiel Bachiani. 203. Supervised 
and unsupervised PCFG adaptation to novel do-
mains. In Procedings of the 203 Conference of 
the North American Chapter of the Asociation for 
Computational Linguistics on Human Language 
Technology – Volume 1 (NACL-HLT). 
Kenji Sagae and Jun’ichi Tsuji. 207.  Multilingual 
dependency parsing and domain adaptation with 
data-driven LR models and parser ensembles.  In 
Procedings of the CoNL 207 shared task. Pra-
gue, Czech Republic. 
Mark Stedman, Rebeca Hwa, Stephen Clark, Miles 
Osborne, Anop Sarkar, Julia Hockenmaier, Paul 
Ruhlen, Steven Baker and Jeremiah Crim. 203. 
Botstraping statistical parsers from smal 
datasets. In Procedings of Tenth Conference of the 
European Chapter of the Asociation for Computa-
tional Linguistics (EACL) – Volume 1. Budapest, 
Hungary. 
44

