Cross Language Text Categorization Using a Bilingual Lexicon
Ke Wu, Xiaolin Wang and Bao-Liang Lu⁄
Department of Computer Science and Engineering, Shanghai Jiao Tong University
800 Dong Chuan Rd., Shanghai 200240, China
fwuke,arthur general,bllug@sjtu.edu.cn
Abstract
With the popularity of the Internet at a phe-
nomenal rate, an ever-increasing number of
documents in languages other than English
are available in the Internet. Cross lan-
guage text categorization has attracted more
and more attention for the organization of
these heterogeneous document collections.
In this paper, we focus on how to con-
duct effective cross language text catego-
rization. To this end, we propose a cross
language naive Bayes algorithm. The pre-
liminary experiments on collected document
collections show the effectiveness of the pro-
posed method and verify the feasibility of
achieving performance close to monolingual
text categorization, using a bilingual lexicon
alone. Also, our algorithm is more ef cient
than our baselines.
1 Introduction
Due to the popularity of the Internet, an ever-
increasing number of documents in languages other
than English are available in the Internet. The or-
ganization of these heterogeneous document collec-
tions increases cost of human labor signi cantly. On
the one hand, experts who know different languages
are required to organize these collections. On the
other hand, maybe there exist a large amount of la-
belled documents in a language (e.g. English) which
are in the same class structure as the unlabelled doc-
uments in another language. As a result, how to ex-
⁄Corresponding author.
ploit the existing labelled documents in some lan-
guage (e.g. English) to classify the unlabelled doc-
uments other than the language in multilingual sce-
nario has attracted more and more attention (Bel et
al., 2003; Rigutini et al., 2005; Olsson et al., 2005;
Fortuna and Shawe-Taylor, 2005; Li and Shawe-
Taylor, 2006; Gliozzo and Strapparava, 2006). We
refer to this task as cross language text categoriza-
tion. It aims to extend the existing automated text
categorization system from one language to other
languages without additional intervention of human
experts. Formally, given two document collections
fDe;Dfg from two different languages e and f re-
spectively, we use the labelled document collection
De in the language e to deduce the labels of the doc-
ument collection Df in the language f via an algo-
rithm A and some external bilingual resources.
Typically, some external bilingual lexical re-
sources, such as machine translation system (MT),
large-scale parallel corpora and multilingual ontol-
ogy etc., are used to alleviate cross language text
categorization. However, it is hard to obtain them
for many language pairs. In this paper, we focus on
using a cheap bilingual resource, e.g. bilingual lexi-
con without any translation information, to conduct
cross language text categorization. To my knowl-
edge, there is little research on using a bilingual lex-
icon alone for cross language text categorization.
In this paper, we propose a novel approach for
cross language text categorization via a bilingual
lexicon alone. We call this approach as Cross Lan-
guage Naive Bayes Classi er (CLNBC). The pro-
posed approach consists of two main stages. The
 rst stage is to acquire a probabilistic bilingual lex-
165
icon. The second stage is to employ naive Bayes
method combined with Expectation Maximization
(EM) (Dempster et al., 1977) to conduct cross lan-
guage text categorization via the probabilistic bilin-
gual lexicon. For the  rst step, we propose two dif-
ferent methods. One is a naive and direct method,
that is, we convert a bilingual lexicon into a proba-
bilistic lexicon by simply assigning equal translation
probabilities to all translations of a word. Accord-
ingly, the approach in this case is named as CLNBC-
D. The other method is to employ an EM algorithm
to deduce the probabilistic lexicon. In this case, the
approach is called as CLNBC-EM. Our preliminary
experiments on our collected data have shown that
the proposed approach (CLNBC) signi cantly out-
performs the baselines in cross language case and is
close to the performance of monolingual text cate-
gorization.
The remainder of this paper is organized as fol-
lows. In Section 2, we introduce the naive Bayes
classi er brie y. In Section 3, we present our cross
language naive Bayes algorithm. In Section 4, eval-
uation over our proposed algorithm is performed.
Section 5 is conclusions and future work.
2 The
Naive Bayes Classi er
The naive Bayes classi er is an effective known al-
gorithm for text categorization (Domingos and Paz-
zani, 1997). When it is used for text categorization
task, each document d 2D corresponds to an exam-
ple. The naive Bayes classi er estimates the prob-
ability of assigning a class c 2 C to a document d
based on the following Bayes’ theorem.
P(cjd) / P(djc)P(c) (1)
Then the naive Bayes classi er makes two as-
sumptions for text categorization. Firstly, each word
in a document occurs independently. Secondly, there
is no linear ordering of the word occurrences.
Therefore, the naive Bayes classi er can be fur-
ther formalized as follows:
P(cjd) / P(c)
Y
w2d
P(wjc) (2)
The estimates of P(c) and P(wjc) can be referred
to (McCallum and Nigam, 1998)
Some extensions to the naive Bayes classi er with
EM algorithm have been proposed for various text
categorization tasks. The naive Bayes classi er was
combined with EM algorithm to learn the class label
of the unlabelled documents by maximizing the like-
lihood of both labelled and unlabelled documents
(Nigam et al., 2000). In addition, the similar way
was adopted to handle the problem with the positive
samples alone (Liu et al., 2002). Recently, transfer
learning problem was tackled by applying EM algo-
rithm along with the naive Bayes classi er (Dai et
al., 2007). However, they all are monolingual text
categorization tasks. In this paper, we apply a simi-
lar method to cope with cross language text catego-
rization using bilingual lexicon alone.
3 Cross
Language Naive Bayes Classi er
Algorithm
In this section, a novel cross language naive Bayes
classi er algorithm is presented. The algorithm con-
tains two main steps below. First, generate a prob-
abilistic bilingual lexicon; second, apply an EM-
based naive Bayes learning algorithm to deduce the
labels of documents in another language via the
probabilistic lexicon.
Table 1: Notations and explanations.
Notations Explanations
e Language of training set
f Language of test set
d Document
De Document collection in language e
Df Document collection in language f
Ve Vocabulary of language e
Vf Vocabulary of language f
L Bilingual lexicon
T  Ve £Vf Set of links in L
‚ Set of words whose translation is  in L
E  Ve Set of words of language e in L
we 2 E Word in E
F  Vf Set of words of language f in L
wf 2 F Word in F
jEj Number of distinct words in set E
jFj Number of distinct words in set F
N(we) Word frequency in De
N(wf;d) Word frequency in d in language f
De Data distribution in language e
166
For ease of description, we  rst de ne some nota-
tions in Table 1. In the next two sections, we detail
the mentioned-above two steps separately.
3.1 Generation
of a probabilistic bilingual
lexicon
To  ll the gap between different languages, there are
two different ways. One is to construct the multi-
lingual semantic space, and the other is to transform
documents in one language into ones in another lan-
guage. Since we concentrate on use of a bilingual
lexicon, we adopt the latter method. In this paper,
we focus on the probabilistic model instead of se-
lecting the best translation. That is, we need to cal-
culate the probability of the occurrence of word we
in language e given a document d in language f, i.e.
P(wejd). The estimation can be calculated as fol-
lows:
P(wejd) =
X
wf2d
P(wejwf;d)P(wfjd) (3)
Ignoring the context information in a document
d, the above probability can be approximately esti-
mated as follows:
P(wejd) ’
X
wf2d
P(wejwf)P(wfjd) (4)
where P(wfjd) denotes the probability of occur-
rence of wf in d, which can be estimated by relative
frequency of wf in d.
In order to induce P(wejd), we have to know the
estimation of P(wejwf). Typically, we can obtain a
probabilistic lexicon from a parallel corpus. In this
paper, we concentrate on using a bilingual lexicon
alone as our external bilingual resource. Therefore,
we propose two different methods for cross language
text categorization.
First, a naive and direct method is that we assume
a uniform distribution on a word’s distribution. For-
mally, P(wejwf) = 1‚w
f, where (we;wf) 2T ; oth-
erwise P(wejwf) = 0.
Second, we can apply EM algorithm to deduce
the probabilistic bilingual lexicon via the bilingual
lexicon L and the training document collection at
hand. This idea is motivated by the work (Li and Li,
2002).
We can assume that each word we in language e
is independently generated by a  nite mixture model
as follows:
P(we) =
X
wf2F
P(wf)P(wejwf) (5)
Therefore we can use EM algorithm to estimate
the parameters of the model. Speci cally speaking,
we can iterate the following two step for the purpose
above.
† E-step
P(wfjwe) = P(wf)P(wejwf)P
w2F P(w)P(wejw)
(6)
† M-step
P(wejwf) = (N(we) + 1)P(wfjwe)P
w2E (N(w) + 1)P(wfjw)(7)
P(wf) = ‚¢
X
we2E
P(we)P(wfjwe)
+ (1¡‚)¢P0(wf) (8)
where 0 • ‚ • 1, and
P0(wf) =
P
d2Df N(wf;d) + 1P
wf2F
P
d2Df N(wf;d) +jFj
(9)
The detailed algorithm can be referred to Algorithm
1. Furthermore, the probability that each word in
language e occurs in a document d in language f,
P(wejd), can be calculated according to Equation
(4).
3.2 EM-based Naive Bayes Algorithm for
Labelling Documents
In this sub-section, we present an EM-based semi-
supervised learning method for labelling documents
in different language from the language of train-
ing document collection. Its basic model is naive
Bayes model. This idea is motivated by the transfer
learning work (Dai et al., 2007). For simplicity of
description, we  rst formalize the problem. Given
the labelled document set De in the source language
and the unlabelled document set Df, the objective is
to  nd the maximum a posteriori hypothesis hMAP
167
Algorithm 1 EM-based Word Translation Probabil-
ity Algorithm
Input: Training document collectionD(l)e , bilingual
lexicon L and maximum times of iterations T
Output: Probabilistic bilingual lexicon P(wejwf)
1: Initialize P(0)(wejwf) = 1j‚w
f j, where
(we;wf) 2T ; otherwise P(0)(wejwf) = 0
2: Initialize P(0)(wf) = 1jFj
3: for t =1 to T do
4: Calculate P(t)(wfjwe) based on
P(t¡1)(wejwf) and P(t¡1)(wf) accord-
ing to Equation (6)
5: Calculate P(t)(wejwf) and P(t)(wf) based
on P(t)(wfjwe) according to Equation (7)
and Equation (8)
6: end for
7: return P(T)(wejwf)
from the hypothesis space H under the data distri-
bution of the language e, De, according to the fol-
lowing formula.
hMAP = arg max
h2H
PDe(hjDe;Df) (10)
Instead of trying to maximize PDe(hjDe;Df) in
Equation (10), we can work with ‘(hjDe;Df), that
is, log (PDe(h)P(De;Dfjh)) . Then, using Equa-
tion (10), we can deduce the following equation.
‘(hjDe;Df) / logPDe(h)
+
X
d2De
log
X
c2C
PDe(djc)PDe(cjh)
+
X
d2Df
log
X
c2C
PDe(djc)PDe(cjh)
(11)
EM algorithm is applied to  nd a local maximum
of ‘(hjDe;Df) by iterating the following two steps:
† E-step:
PDe(cjd) / PDe(c)PDe(djc) (12)
† M-step:
PDe(c) =
X
k2fe;fg
PDe(Dk)PDe(cjDk) (13)
PDe(wejc) =
X
k2fe;fg
PDe(Dk)PDe(wejc;Dk)
(14)
Algorithm 2 Cross Language Naive Bayes Algo-
rithm
Input: Labelled document collection De, unla-
belled document collection Df, a bilingual lexi-
con L from language e to language f and maxi-
mum times of iterations T.
Output: the class label of each document in Df
1: Generate a probabilistic bilingual lexicon;
2: Calculate P(wejd) according to Equation (4).
3: Initialize P(0)De (cjd) via the traditional naive
Bayes model trained from the labelled collec-
tion D(l)e .
4: for t =1 to T do
5: for all c 2C do
6: Calculate P(t)De(c) based on P(t¡1)De (cjd) ac-
cording to Equation (13)
7: end for
8: for all we 2 E do
9: Calculate P(t)De(wejc) based on P(t¡1)De (cjd)
and P(wejd) according to Equation (14)
10: end for
11: for all d 2Df do
12: Calculate P(t)De(cjd) based on P(t)De(c) and
P(t)De(wejc) according to Equation (12)
13: end for
14: end for
15: for all d 2Df do
16: c = arg max
c2C
P(T)De (cjd)
17: end for
For the ease of understanding, we directly put the
details of the algorithm in cross-language text cate-
gorization algorithmin which we ignore the detail of
the generation algorithm of a probabilistic lexicon.
In Equation (12), PDe(djc) can be calculated by
PDe(djc) =
Y
fwejwe2‚wf ^wf2dg
PDe(wejc)NDe(we;d)
(15)
where NDe(we;d) = jdjPDe(wejd).
168
In Equation (13), PDe(cjDk) can be estimated as
follows:
PDe(cjDk) =
X
d2Dk
PDe(cjd)PDe(djDk) (16)
In Equation (14), similar to section 2, we can es-
timate PDe(wejc;Dk) through Laplacian smoothing
as follows:
PDe(wejc;Dk) = 1 + NDe(we;c;Dk)jV
kj+ NDe(c;Dk)
(17)
where
NDe(we;c;Dk) =
X
d2Dk
jdjPDe(wejd)PDe(cjd)
(18)
NDe(c;Dk) =
X
d2Dk
jdjPDe(cjd) (19)
In addition, in Equation (13) and (14), PDe(Dk)
can be actually viewed as the trade-off parame-
ter modulating the degree to which EM algorithm
weights the unlabelled documents translated from
the language f to the language e via a bilingual lex-
icon. In our experiments, we assume that the con-
straints are satis ed, i.e. PDe(De) + PDe(Df) = 1
and PDe(djDk) = 1jDkj.
4 Experiments
4.1 Data
Preparation
We chose English and Chinese as our experimen-
tal languages, since we can easily setup our exper-
iments and they are rather different languages so
that we can easily extend our algorithm to other
language pairs. In addition, to evaluate the per-
formance of our algorithm, experiments were per-
formed over the collected data set. Standard evalu-
ation benchmark is not available and thus we devel-
oped a test data from the Internet, containing Chi-
nese Web pages and English Web pages. Speci -
cally, we applied RSS reader1 to acquire the links
to the needed content and then downloaded the Web
pages. Although category information of the con-
tent can be obtained by RSS reader, we still used
three Chinese-English bilingual speakers to organize
these Web pages into the prede ned categories. As
a result, the test data containing Chinese Web pages
1http://www.rssreader.com/
and English Web pages from various Web sites are
created. The data consists of news during Decem-
ber 2005. Also, 5462 English Web pages are from
18 different news Web sites and 6011 Chinese Web
pages are from 8 different news Web sites. Data dis-
tribution over categories is shown in Table 2. They
fall into  ve categories: Business, Education, Enter-
tainment, Science and Sports.
Some preprocessing steps are applied to Web
pages. First we extract the pure texts of all Web
pages, excluding anchor texts which introduce much
noise. Then for Chinese corpus, all Chinese charac-
ters with BIG5 encoding  rst were converted into
ones with GB2312 encoding, applied a Chinese seg-
menter tool2 by Zhibiao Wu from LDC to our Chi-
nese corpus and removed stop words and words
with one character and less than 4 occurrences; for
English corpus, we used the stop words list from
SMART system (Buckley, 1985) to eliminate com-
mon words. Finally, We randomly split both the En-
glish and Chinese document collection into 75% for
training and 25% for testing.
we compiled a large general-purpose English-
Chinese lexicon, which contains 276,889 translation
pairs, including 53,111 English entries and 38,517
Chinese entries. Actually we used a subset of the
lexicon including 20,754 English entries and 13,471
Chinese entries , which occur in our corpus.
Table 2: Distribution of documents over categories
Categories English Chinese
Sports 1797 2375
Business 951 1212
Science 843 1157
Education 546 692
Entertainment 1325 575
Total 5462 6011
4.2 Baseline
Algorithms
To investigate the effectiveness of our algorithms
on cross-language text categorization, three baseline
methods are used for comparison. They are denoted
by ML, MT and LSI respectively.
ML (Monolingual). We conducted text catego-
rization by training and testing the text categoriza-
2http://projects.ldc.upenn.edu/Chinese/LDC ch.htm
169
20 40 80 160 320 640 1280 40960.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
# of training samples
Accuracy
 
 
ML MT LSI CLNBC−D CLNBC−EM
Figure 1: Comparison of the best performance of
different methods with various sizes of training set
and the entire test set. Training is conducted over
Chinese corpus and testing is conducted over En-
glish corpus in the cross language case, while both
training and testing are performed over English cor-
pus in the monolingual case.
tion system on document collection in the same lan-
guage.
MT (Machine Translation). We used Systran
premium 5.0 to translate training data into the lan-
guage of test data, since the machine translation sys-
tem is one of the best machine translation systems.
Then use the translated data to learn a model for
classifying the test data.
LSI (Latent Semantic Indexing). We can use
the LSI or SVD technique to deduce language-
independent representations through a bilingual par-
allel corpus. In this paper, we use SVDS command
in MATLAB to acquire the eigenvectors with the
 rst K largest eigenvalues. We take K as 400 in our
experiments, where best performance is achieved.
In this paper, we use SVMs as the classi er of our
baselines, since SVMs has a solid theoretic founda-
tion based on structure risk minimization and thus
high generalization ability. The commonly used
one-vs-all framework is used for the multi-class
case. SVMs uses the SVMlight software pack-
age(Joachims, 1998). In all experiments, the trade-
off parameter C is set to 1.
4.3 Results
In the experiments, all results are averaged on 5 runs.
Results are measured by accuracy, which is de ned
as the ratio of the number of labelled correctly docu-
20 40 80 160 320 640 1280 40960.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
# of training samples
Accuracy
 
 
ML MT LSI CLNBC−D CLNBC−EM
Figure 2: Comparison of the best performance of
different methods with various sizes of training set
and the entire test set. Training is conducted over
English corpus and testing is conducted over Chi-
nese corpus in the cross language case, while both
training and testing are performed over Chinese cor-
pus in the monolingual case.
ments to the number of all documents. When inves-
tigating how different training data have effect on
performance, we randomly select the corresponding
number of training samples from the training set 5
times. The results are shown in Figure 1 and Fig-
ure 2. From the two  gures, we can draw the fol-
lowing conclusions. First, CLNBC-EM has a stable
and good performance in almost all cases. Also, it
can achieve the best performance among cross lan-
guage methods. In addition, we notice that CLNBC-
D works surprisingly better than CLNBC-EM, when
there are enough test data and few training data. This
may be because the quality of the probabilistic bilin-
gual lexicon derived from CLNBC-EM method is
poor, since this bilingual lexicon is trained from in-
suf cient training data and thus may provide biased
translation probabilities.
To further investigate the effect of varying the
amount of test data, we randomly select the cor-
responding number of test samples from test set 5
times. The results are shown in Figure 3 and Fig-
ure 4, we can draw the following conclusions . First,
with the increasing test data, performance of our two
approaches is improved. Second, CLNBC-EM sta-
tistically signi cantly outperforms CLNBC-D.
From  gures 1 through 4, we also notice that MT
and LSI always achieve some poor results. For MT,
170
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.5
0.6
0.7
0.8
0.9
1
Ratio of test data
Accuracy
 
 
ML MT LSI CLNBC−D CLNBC−EM
Figure 3: Comparison of the best performance of
different methods with the entire training set and
various sizes of test set. Training is conducted over
Chinese corpus and testing is conducted over En-
glish corpus in the cross language case, while both
training and testing are performed over English cor-
pus in the monolingual case.
maybe it is due to the large difference of word usage
between original documents and the translated ones.
For example, a0a2a1 (Qi Shi) has two common trans-
lations, which are cavalier and knight. In sports do-
main, it often means a basketball team of National
Basketball Association (NBA) in U.S. and should
be translated into cavalier. However, the transla-
tion knight is provided by Systran translation system
we use in the experiment. In term of LSI method,
one possible reason is that the parallel corpus is too
limited. Another possible reason is that it is out-of-
domain compared with the domain of the used doc-
ument collections.
From Table 3, we can observe that our algorithm
is more ef cient than three baselines. The spent time
are calculated on the machine, which has a 2.80GHz
Dual Pentium CPU.
5 Conclusions
and Future Work
In this paper, we addressed the issue of how to con-
duct cross language text categorization using a bilin-
gual lexicon. To this end, we have developed a cross
language naive Bayes classi er, which contains two
main steps. In the  rst step, we deduce a proba-
bilistic bilingual lexicon. In the second step, we
adopt naive Bayes method combined with EM to
conduct cross language text categorization. We have
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Ratio of test data
Accuracy
 
 
ML MT LSI CLNBC−D CLNBC−EM
Figure 4: Comparison of the best performance of
different methods with the entire training set and
various sizes of test set. Training is conducted over
English corpus and testing is conducted over Chi-
nese corpus in the cross language case, while both
training and testing are performed over Chinese cor-
pus in the monolingual case.
proposed two different methods, namely CLNBC-D
and CLNBC-EM, for cross language text categoriza-
tion. The preliminary experiments on collected data
collections show the effectiveness of the proposed
two methods and verify the feasibility of achieving
performance near to monolingual text categorization
using a bilingual lexicon alone.
As further work, we will collect larger compara-
ble corpora to verify our algorithm. In addition, we
will investigate whether the algorithm can be scaled
to more  ne-grained categories. Furthermore, we
will investigate how the coverage of bilingual lex-
icon have effect on performance of our algorithm.
Table 3: Comparison of average spent time by dif-
ferent methods, which are used to conduct cross-
language text categorization from English to Chi-
nese.
Methods Preparation Computation
CLNBC-D »1 Min
CLNBC-EM »2 Min
ML »10 Min
MT »48 Hra »14 Min
LSI »90 Minb »15 Min
aMachine Translation Cost
bSVD Decomposition Cost
171
Acknowledgements. The authors would like to
thank three anonymous reviewers for their valu-
able suggestions. This work was partially sup-
ported by the National Natural Science Founda-
tion of China under the grants NSFC 60375022 and
NSFC 60473040, and the Microsoft Laboratory for
Intelligent Computing and Intelligent Systems of
Shanghai Jiao Tong University.
References
Nuria Bel, Cornelis H. A. Koster, and Marta Villegas.
2003. Cross-lingual text categorization. In ECDL,
pages 126 139.
Chris Buckley. 1985. Implementation of the SMART
information retrieval system. Technical report, Ithaca,
NY, USA.
Wenyuan Dai, Gui-Rong Xue, Qiang Yang, and Yong
Yu. 2007. Transferring naive Bayes classi ers for text
classi cation. In Proceedings of Twenty-Second AAAI
Conference on Arti cial Intelligence (AAAI 2007),
pages 540 545, July.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1 38.
Pedro Domingos and Michael J. Pazzani. 1997. On the
optimality of the simple bayesian classi er under zero-
one loss. Machine Learning, 29(2-3):103 130.
Bla z Fortuna and John Shawe-Taylor. 2005. The use
of machine translation tools for cross-lingual text min-
ing. In Learning With Multiple Views, Workshop at the
22nd International Conference on Machine Learning
(ICML).
Al o Massimiliano Gliozzo and Carlo Strapparava.
2006. Exploiting comparable corpora and bilingual
dictionaries for cross-language text categorization. In
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics. The Association for
Computer Linguistics, July.
Thorsten Joachims. 1998. Making large-scale sup-
port vector machine learning practical. In A. Smola
B. Schcurrency1olkopf, C. Burges, editor, Advances in Kernel
Methods: Support Vector Machines. MIT Press, Cam-
bridge, MA.
Cong Li and Hang Li. 2002. Word translation disam-
biguation using bilingual bootstrapping. In Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 343 351.
Yaoyong Li and John Shawe-Taylor. 2006. Using KCCA
for Japanese-English cross-language information re-
trieval and document classi cation. Journal of Intel-
ligent Information Systems, 27(2):117 133.
Bing Liu, Wee Sun Lee, Philip S. Yu, and Xiaoli Li.
2002. Partially supervised classi cation of text doc-
uments. In ICML ’02: Proceedings of the Nineteenth
International Conference on Machine Learning, pages
387 394, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Andrew McCallum and Kamal Nigam. 1998. A compar-
ison of event models for naive bayes text classi cation.
In Proceedings of AAAI-98, Workshop on Learning for
Text Categorization.
Kamal Nigam, Andrew McCallum, Sebastian Thrun, and
Tom Mitchell. 2000. Text classi cation from labeled
and unlabeled documents using EM. Machine Learn-
ing, 39(2/3):103 134.
J. Scott Olsson, Douglas W. Oard, and Jan Haji c. 2005.
Cross-language text classi cation. In Proceedings of
the 28th Annual international ACM SIGIR Confer-
ence on Research and Development in information Re-
trieval, pages 645 646, New York, NY, August. ACM
Press.
Leonardo Rigutini, Marco Maggini, and Bing Liu. 2005.
An EM based training algorithm for cross-language
text categorization. In Proceedings of Web Intelligence
Conference (WI-2005), pages 529 535, Compi egne,
France, September. IEEE Computer Society.
172

