1:169	Coling 2008 22nd International Conference on Computational Linguistics Proceedings of the workshop on Knowledge and Reasoning for Answering Questions Workshop chairs: Marie-Francine Moens, Patrick Saint-Dizier 23 August 2008 Manchester, UK c2008 The Coling 2008 Organizing Committee Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Nonported license http://creativecommons.org/licenses/by-nc-sa/3.0/ Some rights reserved Order copies of this and other Coling proceedings from: Association for Computational Linguistics (ACL) 209 N. Eighth Street Stroudsburg, PA 18360 USA Tel: +1-570-476-8006 Fax: +1-570-476-0860 acl@aclweb.org ISBN 978-1-905593-53-8 Design by Chimney Design, Brighton, UK Production and manufacture by One Digital, Brighton, UK ii Introduction Welcome to the ACL Workshop on Unresolved Matters.
2:169	We received 17 submissions, and due to a rigerous review process, we rejected 16.
3:169	iii  Table of Contents Semantic Chunk Annotation for complex questions using Conditional Random Field Shixi Fan, Yaoyun Zhang, Wing W. Y. Ng, Xuan Wang and Xiaolong Wang  1 Context Inducing Nouns Charlotte Price, Valeria de Paiva and Tracy Holloway King9 Know-Why Extraction from Textual Data for Supporting What Questions Chaveevan Pechsiri, Phunthara Sroison and J. Janviriyasopak  17 Context Modelling for IQA: the Role of Tasks and Entities Raffaella Bernardi and Manuel Kirschner  25 Personalized, Interactive Question Answering on the Web Silvia Quarteroni  33 Creating and Querying a Domain dependent Know-How Knowledge Base of Advices and Warnings Lionel Fontan and Patrick Saint-Dizier41 v  Conference Programme Saturday, 23 August 2008 09:0010:00 Semantic Chunk Annotation for complex questions using Conditional Random Field Shixi Fan, Yaoyun Zhang, Wing W. Y. Ng, Xuan Wang and Xiaolong Wang 10:0010:30 Context Inducing Nouns Charlotte Price, Valeria de Paiva and Tracy Holloway King 10.3011.00 Break 11.0012.00 Invited Talk 12.0013.30 Lunch 13:3014:00 Know-Why Extraction from Textual Data for Supporting What Questions Chaveevan Pechsiri, Phunthara Sroison and J. Janviriyasopak 14:0014:30 Context Modelling for IQA: the Role of Tasks and Entities Raffaella Bernardi and Manuel Kirschner 14:3015:00 Personalized, Interactive Question Answering on the Web Silvia Quarteroni 15:0015:30 Creating and Querying a Domain dependent Know-How Knowledge Base of Advices and Warnings Lionel Fontan and Patrick Saint-Dizier 15.3016.00 Break 16.0017.30 Panel: Multimedia question answering vii  Coling 2008: Proceedings of the workshop on Knowledge and Reasoning for Answering Questions, pages 18 Manchester, August 2008 Semantic Chunk Annotation for complex questions using Conditional Random Field Shixi Fan Department of computer science Harbin Institute of Technology Shenzhen Graduate School, Shenzhen,518055, china fanshixi@hit.edu.cn Yaoyun Zhang Department of computer science Harbin Institute of Technology Shenzhen Graduate School, Shenzhen,518055, china Xiaoni5122@gmail.com Wing W. Y. Ng Department of computer science Harbin Institute of Technology Shenzhen Graduate School, Shenzhen,518055, china wing@hitsz.edu.cn Xuan Wang Department of computer science Harbin Institute of Technology Shenzhen Graduate School, Shenzhen,518055, china wangxuan@insun.hit.edu.cn Xiaolong Wang Department of computer science Harbin Institute of Technology Shenzhen Graduate School, Shenzhen,518055, china wangxl@insun.hit.edu.cn   Abstract This paper presents a CRF (Conditional Random Field) model for Semantic Chunk Annotation in a Chinese Question and Answering System (SCACQA).
4:169	The model was derived from a corpus of real world questions, which are collected from some discussion groups on the Internet.
5:169	The questions are supposed to be answered by other people, so some of the questions are very complex.
6:169	Mutual information was adopted for feature selection.
7:169	The training data collection consists of 14000 sentences and the testing data collection consists of 4000 sentences.
8:169	The result shows an F-score of 93.07%.
9:169	 2008.
10:169	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-ncsa/3.0/).
11:169	Some rights reserved.
12:169	1 Introduction 1.1 Introduction of Q&A System Automated question answering has been a hot topic of research and development since the earliest AI applications (A.M. Turing, 1950).
13:169	Since then there has been a continual interest in processing knowledge and retrieving it efficiently to users automatically.
14:169	The end of the 1980s saw a boost in information retrieval technologies and applications, with an unprecedented growth in the amount of digital information available, an explosion of growth in the use of computers for communications, and the increasing number of users that have access to all this information (Diego Moll and Jose Luis Vicedo, 2007).
15:169	Search engines such as Google, Yahoo, Baidu and etc have made a great success for peoples information need.
16:169	Anyhow, search engines are keywords-based which can only return links of relevant web pages, failing to provide a friendly user-interface with queries expressed in natural language sentences or questions, or to return precise answers to users.
17:169	Especially from the end of the 1990s, as 1 information retrieval technologies and methodologies became mature and grew more slowly in pace, automated question answering(Q&A) systems which accept questions in free natural language formations and return exactly the answer or a short paragraph containing relevant information has become an urgent necessity.
18:169	Major international evaluations such as TREC, CLEF  and NTCIR have attracted the participation of many powerful systems.
19:169	The architecture of a Q&A system generally includes three modules: question processing, candidate answer/document retrieval, and answer extraction and re-ranking.
20:169	1.2 Introduction of Question Analyzing Question Analyzing, as the premise and foundation of the latter two modules, is of paramount importance to the integrated performance of a Q&A system.
21:169	The reason is quite intuitive: a question contains all the information to retrieve the corresponding answer.
22:169	Misinterpretation or too much loss of information during the processing will inevitably lead to poor precision of the system.
23:169	The early research efforts and evaluations in Q&A were focused mainly on factoid questions asking for named entities, such as time, numbers, and locations and so on.
24:169	The questions in the test corpus of TREC and other organizations are also in short and simple form.
25:169	Complex hierarchy in question types (Dragomir Radev et al, 2001), question templates (Min-Yuh Day et al, 2005), question parsing (Ulf Hermjakob, 2001) and various machine learning methods (Dell Zhang and Wee Sun Lee, 2003)are used for factoid question analysis, aiming to find what named entity is asked in the question.
26:169	There are some questions which are very complicated or even need domain restricted knowledge and reasoning technique.
27:169	Automatic Q&A system can not deal with such questions with current technique.
28:169	In china, there is a new kind of web based Q&A system which is a special kind of discussion group.
29:169	Unlike common discussion group, in the web based Q&A system one user posts a question, other users can give answers to it.
30:169	It is found that at least 50% percent questions (Valentin Jijkoun and Maarten de Rijke, 2005)posted by users are non-factoid and surely more complicated both in question pattern and information need than those questions in the test set of TREC and other FAQ.
31:169	An example is as follows:  This kind of Q&A system can complement the search engines effectively.
32:169	As the best search engines in china, Baidu open the Baidu Knowledge 2  Q&A system from 2003, and now it has more than 29 million question-answer pairs.
33:169	There are also many other systems of this kind such as Google Groups, Yahoo Answers and Sina Knowledge 3 . This kind of system is a big question-answer pair database which can be treated as a FAQ database.
34:169	How to search from the database and how to analyze the questions in the database needs new methods and techniques.
35:169	More deeper and precise capture of the semantics in those complex questions is required.
36:169	This phenomenon has also been noticed by some researchers and organizations.
37:169	The spotlight gradually shifted to the processing and semantic understanding of complex questions.
38:169	From 2006, TREC launched a new annually evaluation CIQ&A (complex, interactive Question Answering), aiming to promote the development of interactive systems capable of addressing complex information needs.
39:169	The targets of national programs AQUAINT and QUETAL are all at new interface and new enhancements to current stateof-the-art Q&A systems to handle more complex inputs and situations.
40:169	A few researchers and institutions serve as pioneers in complex questions study.
41:169	Different technologies, such as definitions of different sets of question types, templates and sentence patterns (Noriko Tomuro, 2003) (Hyo-Jung Oh et al, 2005) machine learning methods (Radu Soricut and Eric Brill, 2004), language translation model (Jiwoon Jeon, W et al, 2005), composition of information needs of the complex question (Sanda Harabagiu et al, 2006) and so on, have been experimented on the processing of complex question, gearing the acquired information to the facility of other Q&A modules.
42:169	Several major problems faced now by researcher of complex questions are stated as follow: First: Unlike factoid questions, it is very difficult to define a comprehensive type hierarchy for complex questions.
43:169	Different domains under research may require definitions of different sets of question types, as shown in (Hyo-Jung Oh et al, 2005).
44:169	Especially, the types of certain ques 2  http://zhidao.baidu.com/ 3  http://iask.sina.com.cn/ 2 tions are ambiguous and hard to identify.
45:169	For example:  This question type can be treated as definition, procedure or entity.
46:169	Second: Lack of recognition of different semantic chunks and the relations between them.
47:169	FAQFinder (Radu Soricut and Eric Brill, 2004) also used semantic measure to credit the similarity between different questions.
48:169	Nevertheless, the question similarity is only a simple summation of the semantic similarity between words from the two question sentences.
49:169	Question pattern are very useful and easy to implement, as justified by previous work.
50:169	However, just like the problem with question types, question patterns have limitation on the coverage of all the variations of complex question formation.
51:169	Currently, after the question processing step in most systems, the semantic meaning of large part of complex questions still remain vague.
52:169	Besides, confining users input only within the selection of provided pattern may lead to unfriendly and unwelcome user interface.
53:169	(Ingrid Zukerman and Eric Horvitz, 2001) used decision tree to model and recognize the information need, question and answer coverage, topic, focus and restrictions of a question.
54:169	Although features employed in the experiments were described in detail, no selection process of those feature, or comparison between them was mentioned.
55:169	This paper presents a general method for Chinese question analyzing.
56:169	Our goal is to annotate the semantic chunks for the question automatically.
57:169	2 Semantic Chunk Annotation Chinese language differs a lot from English in many aspects.
58:169	Mature methodologies and features well-justified in English Q&A systems are valuable sources of reference, but no direct copy is possible.
59:169	The Ask-Answer system 4  is a Chinese online Q&A system where people can ask and answer questions like other web based Q&A system.
60:169	The characteristic of this system is that it can give the answer automatically by searching from the asked question database when a new question is presented by people.
61:169	The architecture of the automatically answer system is shown in figure 1.
62:169	The system contains a list of question-answer pairs on particular subject.
63:169	When users input a     4  http://haitianyuan.com/qa question from the web pages, the question is submitted to the system and then questionanswer pair is returned by searching from the questions asked before.
64:169	The system includes four main parts: question pre-processing, question analyzing, searching and answer getting.
65:169	The question pre-processing part will segment the input questions into words, label POS tags for every word.
66:169	Sometimes people ask two or more questions at one time, the questions should be made into simple forms by conjunctive structure detection.
67:169	The question analyzing program will find out the question type, topic, focus and etc. The answer getting part will get the answer by computing the similarity between the input question and the questions asked before.
68:169	The question analyzing part annotates the semantic chunks for the question.
69:169	So that the question can be mapped into semantic space and the question similarity can be computed semantically.
70:169	The Semantic chunk annotation is the most important part of the system.
71:169	Question Preprocessing  Segmentation and  pos tagging Detect conjunctive structure Question Analyzing Semantic chunk annotationGet and extend key words Question pattern and knowledge base Search reference question-answer pairs form database Answer getting Score the constituent answers Out put the top five answers  Figure 1 the architecture of the automatically answer system Currently, no work has been reported yet on the question semantic chunk annotation in Chinese.
72:169	The prosperity of major on-line discussion groups provides an abundant ready corpus for question answering research.
73:169	Using questions collected from on-line discussion groups; we make a deep research on semantic meanings and build a question semantic chunk annotation model based on Conditional Random Field.
74:169	Five types of semantic chunks were defined: Topic, Focus, Restriction, Rubbish information and Interrogative information.
75:169	The topic of a 3 question which is the topic or subject asked is the most important semantic chunk.
76:169	The focus of a question is the asking point of the question.
77:169	The restriction information can restrict the questions information need and the answers.
78:169	The rubbish information is those words in the question that has no semantic meanings for the question.
79:169	Interrogative information is a semantic tag set which corresponds to the question type.
80:169	The interrogative information includes interrogative words, some special verbs and nouns words and all these words together determine the question type.
81:169	The semantic chunk information is shown in table 1.
82:169	Semantic chunk   tag Abbreviation Meaning Topic T The question subject Focus F The additional information of topic Restrict  Re Such as Time restriction and location restriction Rubbish information Ru Words no meaning for the question Other O other information without semantic meaning The following is interrogative information Quantity Wqua Description Wdes The answer need description Yes/No Wyes The answer should be yes or no List Wlis The answer should be a list of entity Definition Wdef The answer is the definition of topic Location Wloc The answer is location Reason Wrea The answer can explain the question Contrast Wcon The answer is the comparison of the items proposed in the question People Wwho The answer is about the peoples information Choice Wcho The answer is one of the choice proposed in the question Time Wtim The answer is the data or time length about the event in the question Entity Went The answer is the attribute of the topic.
83:169	Table 1: Semantic chunks An annotation example question is as follows:  This question can be annotated as follows:  This kind of annotation is not convenient for CRF model, so the tags were transfer into the B I O form.
84:169	(Shown as follows)  Then the Semantic chunk annotation can be treated as a sequence tag problem.
85:169	3 Semantic Chunk Annotation model 3.1 Overview of the CRF model The conditional random field (CRF) is a discriminative probabilistic model proposed by John Lafferty, et al (2001) to overcome the long-range dependencies problems associated with generative models.
86:169	CRF was originally designed to label and segment sequences of observations, but can be used more generally.
87:169	Let X, Y be random variables over observed data sequences and corresponding label sequences, respectively.
88:169	For simplicity of descriptions, we assume that the random variable sequences X and Y have the same length, and use [ ] m xxxx , 21 = and [ ] m yyyy , 21 =  to represent instances of X and Y, respectively.
89:169	CRF defines the conditional probability distribution P(Y |X) of label sequences given observation sequences as follows )),(exp( )( 1 )|( 1  = = n i ii YXf XZ XYP       (1) Where  is the normalizing factor that ensures equation 2.
90:169	)(XZ    = y xyP 1)|(                    (2) In equation 2 the i  is a model parameter and  is a feature function (often binaryvalued) that becomes positive (one for binaryvalued feature function) when X contains a certain feature in a certain position and Y takes a certain label, and becomes zero otherwise.
91:169	Unlike Maximum Entropy model which use single normalization constant to yield a joint distribution, CRFs use the observation-dependent normalization  for conditional distributions.
92:169	So CRFs can avoid the label biased problem.
93:169	Given a set of training data ),( YXf i )(XZ  }2,1),,{( nkyxT kk ==  With an empirical distribution , CRF ),( ~ YXP 4 determines the model parameters }{ i  =  by maximizing the log-likelihood of the training set )|(log),( )|(log)( , ~ 1 xyPyxP xyPP yx N k kk      = =                       (3) 3.2 Features for the model The following features, which are used for training the CRF model, are selected according to the empirical observation and some semantic meanings.
94:169	These features are listed in the following table.
95:169	Feature type index Feature type name 1 Current word 2 Current POS tag 3 Pre-1 word POS tag 4 Pre-2 word POS tag 5 Post -1 word POS tag 6 Post -2 word POS tag 7 Question pattern 8 Question type 9 Is pattern key word 10 Pattern tag Table 2: the Features for the model Current word: The current word should be considered when adding semantic tag for it.
96:169	But there are too many words in Chinese language and only part of them will contribute to the performance, a set of words was selected.
97:169	The word set includes segment note and some key words such as time key word and rubbish key word.
98:169	When the current word is in the word set the current word feature is the current word itself, and null on the other hand.
99:169	Current POS tag: Current POS tag is the part of speech tag for the current word.
100:169	Pre-1 word POS tag: Pre1 word POS tag is the POS tag of the first word before the labeling word in the sentence.
101:169	If the Pre-1 word does not exit (the current is the first word in the sentence), the Pre1 word POS tag is set to null.
102:169	Pre-2 word POS tag: Pre2 word POS tag is the POS tag of the second word before the labeling word in the sentence.
103:169	If the Pre-2 word does not exit, the Pre2 word POS tag is set to null.
104:169	Post -1 word POS tag: Post 1 word POS tag is the POS tag of the first word after the labeling word in the sentence.
105:169	If the Post -1 word does not exit (the current is the first word in the sentence), the Post 1 word POS tag is set to null.
106:169	Post -2 word POS tag: Post 2 word POS tag is the POS tag of the second word after the labeling word in the sentence.
107:169	If the Post-2 word does not exit, the Pre2 word POS tag is set to null.
108:169	Question pattern: Question pattern which is associated with question type, can locate question topic, question focus by surface string matching.
109:169	For example, (where is <topic>).
110:169	The patterns are extracted from the training data automatically.
111:169	When a pattern is matched, it is treated as a feature.
112:169	There are 1083 question patterns collected manually.
113:169	Question type: Question type is an important feature for question analyzing.
114:169	The question patterns have the ability of deciding the question type.
115:169	If there is no question pattern matching the question, the question type is defined by a decision tree algorithm.
116:169	Is pattern key word: For each question pattern, there are some key words.
117:169	When the current word belongs to the pattern key word this feature is set to yes, else it is set to no.
118:169	Pattern tag: When a pattern is matched, the topic, focus and restriction can be identified by the pattern.
119:169	We can give out the tags for the question and the tags are treated as features.
120:169	If there is no pattern is matched, the feature is set to null.
121:169	4 Feature Selection experiment Feature selection is important in classifying systems such as neural networks (NNs), Maximum Entropy, Conditional Random Field and etc. The problem of feature selection has been tackled by many researchers.
122:169	Principal component analysis (PCA) method and Rough Set Method are often used for feature selection.
123:169	Recent years, mutual information has received more attention for feature selection problem.
124:169	According to the information theory, the uncertainty of a random variable X can be measured by its entropy . For a classifying problem, there are class label set represented by C and feature set represented by F. The conditional entropy  measures the uncertainty about )(XH )|( FCH 5 C when F is known, and the Mutual information I(C, F) is defined as:  F)|(C -(C));( HHFCI =                   (4) The feature set is known; so that the objective of training the model is to minimize the conditional entropy   equally maximize the mutual information . In the feature set F, some features are irrelevant or redundant.
125:169	So that the goal of a feature selection problem is to find a feature S ( ), which achieve the higher values of . The set S is a subset of F and its size should be as small as possible.
126:169	There are some algorithms for feature selection problem.
127:169	The ideal greedy selection algorithm using mutual information is realized as follows (Nojun Kwak and Chong-Ho Choi, 2002): )|( FCH );( FCI FS  );( FCI  Input:   San empty set              FThe selected feature set Output:  a small reduced feature set S which is equivalent to F Step 1: calculate the MI with the Class set C , , compute  Ff i  );( i fCI Step 2: select the feature that maximizes , set );( i fCI }{},{\ ii fSfFF  Step 3: repeat until desired number of features are selected.
128:169	1) Calculate the MI with the Class set C and S, Ff i  , compute  ),;( i fSCI 2) Select the feature that maximizes , set ),;( i fSCI }{},{\ ii fSfFF  Step 4: Output the set S  that contains the selected features To calculate MI the PDFs (Probability Distribution Functions) are required.
129:169	When features and classing types are dispersing, the probability can be calculated statistically.
130:169	In our system, the PDFs are got from the training corpus statistically.
131:169	The training corpus contains 14000 sentences.
132:169	The training corpus was divided into 10 parts, with each part 1400 sentences.
133:169	And each part is divided into working set and checking set.
134:169	The working set, which contains 90% percent data, was used to select feature by MI algorithm.
135:169	The checking set, which contains 10% percent data, was used to test the performance of the selected feature sequence.
136:169	When the feature sequence was selected by the MI algorithm, a sequence of CRF models was trained by adding one feature at each time.
137:169	The checking data was used to test the performance of these models.
138:169	The open test result Selected feature sequence 1 2 3 4 5 6 7 8 9 10 7, 10, 3, 1, 5, 2, 4, 6, 89 0.5104 0.8764 0.8864 0.8918 0.8925 0.8977 0.8992 0.9023 0.9025 0.9018 7, 10, 1, 3, 5, 2, 4689 0.5241 0.8775 0.8822 0.8911 0.8926 0.8956 0.8967 0.9010 0.9005 0.9007 7, 10, 1, 3, 5, 2, 4, 689 0.5090 0.8691 0.8748 0.8851 0.8852 0.8914 0.8929 0.8955 0.8955 0.8949 7, 10, 1, 3, 5, 2, 4, 698 0.5157 0.8769 0.8823 0.8913 0.8925 0.8978 0.8985 0.9017 0.9018 0.9010 7, 10, 1, 3, 5, 2, 4, 689 0.5144 0.8821 0.8856 0.8921 0.8931 0.8972 0.8981 0.9010 0.9009 0.9007 7, 10, 3, 1, 5, 2, 4689 0.5086 0.8795 0.8876 0.8914 0.8919 0.8960 0.8967 0.9016 0.9013 0.9011 7, 10, 1, 3, 5, 2, 4, 6, 8, 9 0.5202 0.8811 0.8850 0.8920 0.8931 0.8977 0.8980 0.9015 0.9013 0.9009 7, 10, 1, 3, 5, 2, 4, 689 0.5015 0.8858 0.8879 0.8948 0.8942 0.8998 0.8992 0.9033 0.9027 0.9023 7, 10, 1, 3, 5, 2, 4, 689 0.5179 0.8806 0.8805 0.8898 0.8908 0.8954 0.8958 0.8982 0.8982 0.8986 7, 10, 1, 3, 5, 2, 4, 6, 89 0.5153 0.8921 0.8931 0.9006 0.9012 0.9041 0.9039 0.9071 0.9068 0.9067 Table 3: the feature selection result and the test result In table 3, each row contains data corresponding to one part of the training corpus so there are ten rows with data in the table.
139:169	The third row corresponds to the first part and the last row corresponds to the tenth part.
140:169	There are eleven columns in the table, the first columns is the features sequence selected by the mutual information algorithm for each part.
141:169	The second column is the open test result with the first feature in the feature sequence.
142:169	The third column is the open test result with the first two features in the feature sequence and so on.
143:169	From the table, it is 6 clear that the feature 7(Question pattern) and 10(Pattern tag) are very important, while the feature 8(Question type) and 9(Is pattern key word) are not necessary.
144:169	The explanation about this phenomenon is that the pattern key word and Question type information can be covered by the Question patterns.
145:169	So feature 8 and 9 are not used in the Conditional Random Field model.
146:169	5 Semantic Chunk Annotation Experiment The test and training data used in our system are collected from the website (Baidu knowledge and the Ask-Answer system), where people proposed questions and answers.
147:169	The training data consists of 14000 and the test data consists of 4000 sentences.
148:169	The data set consists of word tokens, POS and semantic chunk tags.
149:169	The POS and semantic tags are assigned to each word tokens.
150:169	The performance is measured with three rates: precision (Pre), recall (Rec) and F-score (F1).
151:169	Pre = Match/Model                     (5) Rec=Match/Manual                    (6) F1=2*Pre*Rec/(Pre+Rec)              (7) Match is the count of the tags that was predicted right.
152:169	Model is the count of the tags that was predicted by the model.
153:169	Manual is the count of the tags that was labeled manually.
154:169	Table 4 shows the performance of annotation of different semantic chunk types.
155:169	The first column is the semantic chunk tag.
156:169	The last three columns are precision, recall and F1 value of the semantic chunk performance, respectively.
157:169	Label Manual Model Match Pre.() Rec.() F1 B-TI-T 1706178462 1632780488 1482576461 90.8095.00 86.8997.45 88.8096.21 B-FI-F  507213029  507913583  465712259  91.6990.25  91.8294.09  91.7592.13 B-RuI-Ru 77530 110 20 18.180.00 0.260.00 0.510.00 O 8354 8459 6676 78.92 79.91 79.41 B-WquaI-Wqua  1363934  13271028  1298881  97.8185.70  95.2394.33  96.5189.81 B-Wyes I-Wyes 56691162  57021098  55501083  97.3398.63  97.9093.20  97.6295.84 B-WdesI-Wdes  2907278  2855185  2779184  97.3499.46  95.6066.19  96.4679.48 B-WlisI-Wlis 603257 563248 560248 99.47100 92.8796.50 96.0598.22 B-WdefI-Wdef 14201813 14301878 12801695 89.5190.26 90.149 3.49 89.8291.85 B-WlocI-Wloc 683431 665395 661 392 99.4099.24 96.7890.95 98.0794.92 B-WreaI-Wrea 902159 87383 843 82 96.5698.80 93.4651.57 94.9967.77 B-WconI-Wcon 552317 515344 503 291 97.6784.59 91.1291.80 94.2888.05 B-WwhoI-Wwho 420364 357350 348 336 97.4896.00 82.8692.31 89.5894.12 B-WchoI-Wcho 85785 7380 6860 92.950.00 80.050.00 86.020.
158:169	00 B-WtimI-Wtim 408427 401419 355 380 88.5390.69 87.0188.99 87.7689.83 B-WentI-Went 284150 9581 93 80 97.8998.77 32.7553.33 49.0869.26 Avg 145577 145577 135488 93.07 93.07 93.07 Table 4: the performance of different semantic chunk  The semantic chunk type of Topic and Focus can be annotated well.
159:169	Topic and focus semantic chunks have a large percentage in all the semantic chunks and they are important for question analyzing.
160:169	So the result is really good for the whole Q&A system.
161:169	As for Rubbish semantic chunk, it only has 0.51 and 0.0 F1 measure for B-Ru and I-Ru.
162:169	One reason is lacking enough training examples, for there are only 1031 occurrences in the training data.
163:169	Another reason is sometimes restriction is complex.
164:169	6 Conclusion and future work This paper present a new method for Chinese question analyzing based on CRF.
165:169	The features are selected by using mutual information algorithm.
166:169	The selected features work effectively for the CRF model.
167:169	The experiments on the test data set achieve 93.07% in F1 measure.
168:169	In the future, new features should be discovered and new methods will be used.
169:169	Acknowledgment This work is supported by Major Program of National Natural Science Foundation of China (No.60435020 and No. 90612005) and the High Technology Research and Development Program of China (2006AA01Z197).

