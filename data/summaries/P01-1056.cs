Recently, several approaches for automatically training modules of an NLG system have been proposed (Langkilde and Knight, 1998; Mellish et al., 1998; Walker, 2000).
W98-1411
See (Walker et al., 2001) for more detail on the sp-tree.
N01-1003
For this type of problem it is possible to evaluate the generator by the degree to which it matches human performance (Yeh and Mellish, 1997).
J97-1007
Furthermore, maintenance of the collection of templates becomes a software engineering problem as the complexity of the dialog system increases.1 The second approach is natural language generation (NLG), which customarily divides the generation process into three modules (Rambow and Korelsky, 1992): (1) Text Planning, (2) Sentence Planning, and (3) Surface Realization.
A92-1006
Previous work on evaluation of natural language generation has utilized three different approaches to evaluation (Mellish and Dale, 1998).
W98-1411
and Future Work Other work has also explored automatically training modules of a generator (Langkilde and Knight, 1998; Mellish et al., 1998; Walker, 2000).
W98-1411
In (Walker et al., 2001) we propose a new model of sentence planning called SPOT.
N01-1003
Figure 4: List of clause combining operations with examples sentence planners (Lavoie and Rambow, 1997).
A97-1039
Walker et al.(2001) describe SPOT in detail.
N01-1003
The first approach is a subjective evaluation methodology such as we use here, where human subjects rate NLG outputs produced by different sources (Lester and Porter, 1997).
J97-1004
In (Walker et al., 2001), we evaluate the performance of the learning component of SPOT, and show that SPOT learns to select sentence plans that are highly rated by the two human judges.
N01-1003
