1:188	Multi-document Biography Summarization Liang Zhou, Miruna Ticrea, Eduard Hovy University of Southern California Information Sciences Institute 4676 Admiralty Way Marina del Rey, CA 90292-6695 {liangz, miruna, hovy} @isi.edu Abstract In this paper we describe a biography summarization system using sentence classification and ideas from information retrieval.
2:188	Although the individual techniques are not new, assembling and applying them to generate multi-document biographies is new.
3:188	Our system was evaluated in DUC2004.
4:188	It is among the top performers in task 5short summaries focused by person questions.
5:188	1 Introduction Automatic text summarization is one form of information management.
6:188	It is described as selecting a subset of sentences from a document that is in size a small percentage of the original and yet is just as informative.
7:188	Summaries can serve as surrogates of the full texts in the context of Information Retrieval (IR).
8:188	Summaries are created from two types of text sources, a single document or a set of documents.
9:188	Multi-document summarization (MDS) is a natural and more elaborative extension of single-document summarization, and poses additional difficulties on algorithm design.
10:188	Various kinds of summaries fall into two broad categories: generic summaries are the direct derivatives of the source texts; specialinterest summaries are generated in response to queries or topic-oriented questions.
11:188	One important application of special-interest MDS systems is creating biographies to answer questions like who is Kofi Annan?.
12:188	This task would be tedious for humans to perform in situations where the information related to the person is deeply and sparsely buried in large quantity of news texts that are not obviously related.
13:188	This paper describes a MDS biography system that responds to the who is questions by identifying information about the person-inquestion using IR and classification techniques, and creates multi-document biographical summaries.
14:188	The overall system design is shown in Figure 1.
15:188	To determine what and how sentences are selected and ranked, a simple IR method and experimental classification methods both contributed.
16:188	The set of top-scoring sentences, after redundancy removal, is the resulting biography.
17:188	As yet, the system contains no inter-sentence smoothing stage.
18:188	In this paper, work in related areas is discussed in Section 2; a description of our biography corpus used for training and testing the classification component is in Section 3; Section 4 explains the need and the process of classifying sentences according to their biographical state; the application of the classification method in biography extraction/summarization is described in Section 5; an accompanying evaluation on the quality of the biography summaries is shown in Section 6; and future work is outlined in Section 7.
19:188	2 Recent Developments Two trends have dominated automatic summarization research (Mani, 2001).
20:188	One is the work focusing on generating summaries by extraction, which is finding a subset of the document that is indicative of its contents (Kupiec et al. , 1995) using shallow linguistic analysis and statistics.
21:188	The other influence is the exploration of Figure 1.
22:188	Overall design of the biography summarization system.
23:188	deeper knowledge-based methods for condensing information.
24:188	Knight and Marcu (2000) equate summarization with compression at sentence level to achieve grammaticality and information capture, and push a step beyond sentence extraction.
25:188	Many systems use machinelearning methods to learn from readily aligned corpora of scientific articles and their corresponding abstracts.
26:188	Zhou and Hovy (2003) show a summarization system trained from automatically obtained text-summary alignments obeying the chronological occurrences of news events.
27:188	MDS poses more challenges in assessing similarities and differences among the set of documents.
28:188	The simple idea of extract-andconcatenate does not respond to problems arisen from coherence and cohesion.
29:188	Barzilay et al.30:188	(1999) introduce a combination of extracted similar phrases and a reformulation through sentence generation.
31:188	Lin and Hovy (2002) apply a collection of known single-document summarization techniques, cooperating positional and topical information, clustering, etc. , and extend them to perform MDS.
32:188	While many have suggested that conventional MDS systems can be applied to biography generation directly, Mani (2001) illustrates that the added functionality of biographical MDS comes at the expense of a substantial increase in system complexity and is somewhat beyond the capabilities of present day MDS systems.
33:188	The discussion was based in part on the only known MDS biography system (Schiffman et al. , 2001) that uses corpus statistics along with linguistic knowledge to select and merge description of people in news.
34:188	The focus of this work was on synthesizing succinct descriptions of people by merging appositives from semantic processing using WordNet (Miller, 1995).
35:188	3 Corpus Description In order to extract information that is related to a person from a large set of news texts written not exclusively about this person, we need to identify attributes shared among biographies.
36:188	Biographies share certain standard components.
37:188	We annotated a corpus of 130 biographies of 12 people (activists, artists, leaders, politicians, scientists, terrorists, etc.).
38:188	We found 9 common elements: bio (info on birth and death), fame factor, personality, personal, social, education, nationality, scandal, and work.
39:188	Collected biographies are appropriately marked at clauselevel with one of the nine tags in XML format, for example: Martin Luther King <nationality> was born in Atlanta, Georgia </nationality>.
40:188	 He <bio>was assassinated on April 4, 1968 </bio>.
41:188	 King <education> entered the Boston University as a doctoral student </education>.
42:188	 In all, 3579 biography-related phrases were identified and recorded for the collection, among them 321 bio, 423 fame, 114 personality, 465 personal, 293 social, 246 education, 95 nationality, 292 scandal, and 1330 work.
43:188	We then used 100 biographies for training and 30 for testing the classification module.
44:188	4 Sentence Classification Relating to human practice on summarizing, three main points are relevant to aid the automation process (Sprck Jones, 1993).
45:188	The first is a strong emphasis on particular purposes, e.g., abstracting or extracting articles of particular genres.
46:188	The second is the drafting, writing, and revision cycle in constructing a summary.
47:188	Essentially as a consequence of these first two points, the summarizing process can be guided by the use of checklists.
48:188	The idea of a checklist is especially useful for the purpose of generating biographical summaries because a complete biography should contain various aspects of a persons life.
49:188	From a careful analysis conducted while constructing the biography corpus, we believe that the checklist is shared and common among all persons in question, and consists the 9 biographical elements introduced in Section 3.
50:188	The task of fulfilling the biography checklist becomes a classification problem.
51:188	Classification is defined as a task of classifying examples into one of a discrete set of possible categories (Mitchell, 1997).
52:188	Text categorization techniques have been used extensively to improve the efficiency on information retrieval and organization.
53:188	Here the problem is that sentences, from a set of documents, need to be categorized into different biographyrelated classes.
54:188	4.1 Task Definitions We designed two classification tasks: 1) 10-Class: Given one or more texts about a person, the module must categorize each sentence into one of ten classes.
55:188	The classes are the 9 biographical elements plus a class called none that collects all sentences without biographical information.
56:188	This fine-grained classification task will be beneficial in generating comprehensive biographies on people of interest.
57:188	The classes are: bio fame personality social education nationality scandal personal work none 2) 2-Class: The module must make a binary decision of whether the sentence should be included in a biography summary.
58:188	The classes are: bio none The label bio appears in both task definitions but bears different meanings.
59:188	Under 10-Class, class bio contains information on a persons birth or death, and under 2-Class it sums up all 9 biographical elements from the 10-Class.
60:188	4.2 Machine Learning Methods We experimented with three machine learning methods for classifying sentences.
61:188	Nave Bayes The Nave Bayes classifier is among the most effective algorithms known for learning to classify text documents (Mitchell, 1997), calculating explicit probabilities for hypotheses.
62:188	Using k features F j : j = 1, , k, we assign to a given sentence S the class C:  C=argmax C P(|F 1, 2,,F k ) It can be expressed using Bayes rule, as (Kupiec et al. , 1995):  P(SC|F 1, 2,F k )= P(F 1, 2,F j |SC)P(SC) P( 1, 2,F k ) Assuming statistical independence of the features:  P(SC|F 1, 2,F k )= P(F j |SC)P(SC) j=1 k  P(F j j=1 k ) Since P(F j ) has no role in selecting C:  P(SC|F 1, 2,F k )=P(F j |SC)P(SC) j=1 k  We trained on the relative frequency of P(F j |SC) and P(SC), with add-one smoothing.
63:188	This method was used in classifying both the 10Class and the 2-Class tasks.
64:188	Support Vector Machine Support Vector Machines (SVMs) have been shown to be an effective classifier in text categorization.
65:188	We extend the idea of classifying documents into predefined categories to classifying sentences into one of the two biography categories defined by the 2-Class task.
66:188	Sentences are categorized based on their biographical saliency (a percentage of clearly identified biography words) and their non-biographical saliency (a percentage of clearly identified non-biography words).
67:188	We used LIBSVM (Chang and Lin, 2003) for training and testing.
68:188	Decision Tree (4.5) In addition to SVM, we also used a decision-tree algorithm, C4.5 (Quinlan, 1993), with the same training and testing data as SVM.
69:188	4.3 Classification Results The lower performance bound is set by a baseline system that randomly assigns a biographical class given a sentence, for both 10Class and 2-Class.
70:188	2599 testing sentences are from 30 unseen documents.
71:188	10-Class Classification The Nave Bayes classifier was used to perform the 10-Class task.
72:188	Table 1 shows its performance with various features.
73:188	Table 1.
74:188	Performance of 10-Class sentence classification, using Nave Bayes Classifier.
75:188	Part-of-speech (POS) information (Brill, 1995) and word stems (Lovins, 1968) were used in some feature sets.
76:188	We bootstrapped 10395 more biographyindicating words by recording the immediate hypernyms, using WordNet (Fellbaum, 1998), of the words collected from the controlled biography corpus described in Section 3.
77:188	These words are called Expanded Unigrams and their frequency scores are reduced to a fraction of the original words frequency score.
78:188	Some sentences in the testing set were labeled with multiple biography classes due to the fact that the original corpus was annotated at clause level.
79:188	Since the classification was done at sentence level, we relaxed the matching/evaluating program allowing a hit when any of the several classes was matched.
80:188	This is shown in Table 1 as the Relaxed cases.
81:188	A closer look at the instances where the false negatives occur indicates that the classifier mislabeled instances of class work as instances of class none.
82:188	To correct this error, we created a list of 5516 work specific words hoping that this would set a clearer boundary between the two classes.
83:188	However performance did not improve.
84:188	2-Class Classification All three machine learning methods were evaluated in classifying among 2 classes.
85:188	The results are shown in Table 2.
86:188	The testing data is slightly skewed with 68% of the sentences being none.
87:188	In addition to using marked biographical phrases as training data, we also expanded the marking/tagging perimeter to sentence boundaries.
88:188	As shown in the table, this creates noise.
89:188	5 Biography Extraction Biographical sentence classification module is only one of two components that supply the overall system with usable biographical contents, and is followed by other stages of processing (see system design in Figure 1).
90:188	We discuss the other modules next.
91:188	5.1 Name-filter A filter scans through all documents in the set, eliminating sentences that are direct quotes, dialogues, and too short (under 5 words).
92:188	Personoriented sentences containing any variation (first name only, last name only, and the full name) of the persons name are kept for subsequent steps.
93:188	Sentences classified as biography-worthy are merged with the name-filtered sentences with duplicates eliminated.
94:188	5.2 Sentence Ranking An essential capability of a multi-document summarizer is to combine text passages in a useful manner for the reader (Goldstein et al. , 2000).
95:188	This includes a sentence ordering parameter (Mani, 2001).
96:188	Each of the sentences selected by the name-filter and the biography classifier is either related to the person-in-question via some news event or referred to as part of this persons biographical profile, or both.
97:188	We need a mechanism that will select sentences that are of informative significance within the source document set.
98:188	Using inverse-term-frequency (ITF), i.e. an estimation of information value, words with high information value (low ITF) are distinguished from those with low value (high ITF).
99:188	A sorted list of words along with their ITF scores from a document settopic ITFsdisplays the important events, persons, etc. , from this particular set of texts.
100:188	This allows us to identify passages that are unusual with respect to the texts about the person.
101:188	However, we also need to identify passages that are unusual in general.
102:188	We have to quantify how these important words compare to the rest of the world.
103:188	The world is represented by 413307562 words from TREC-9 corpus (http://trec.nist.gov/data.html), with corresponding ITFs.
104:188	The overall informativeness of each word w is:  C w = d itf w W itf w where d itf is the document set ITF of word w and W itf is the world ITF of w. A word that occurs frequently bears a lower C w score compared to a rarely used word (bearing high information value) with a higher C w score.
105:188	Top scoring sentences are then extracted according to: Table 2.
106:188	Classification results on 2-Class using Nave Bayes, SVM, and C4.5.
107:188	 C s = C w i i=1 n  len(s) The following is a set of sentences extracted according to the method described so far.
108:188	The person-in-question is the famed cyclist Lance Armstrong.
109:188	1.
110:188	Cycling helped him win his battle with cancer, and cancer helped him win the Tour de France.
111:188	2.
112:188	Armstrong underwent four rounds of intense chemotherapy.
113:188	3.
114:188	The surgeries and chemotherapy eliminated the cancer, and Armstrong began his cycling comeback.
115:188	4.
116:188	The foundation supports cancer patients and survivors through education, awareness and research.
117:188	5.
118:188	He underwent months of chemotherapy.
119:188	5.3 Redundancy Elimination Summaries that emphasize the differences across documents while synthesizing common information would be the desirable final results.
120:188	Removing similar information is part of all MDS systems.
121:188	Redundancy is apparent in the Armstrong example from Section 5.2.
122:188	To eliminate repetition while retaining interesting singletons, we modified (Marcu, 1999) so that an extract can be automatically generated by starting with a full text and systematically removing a sentence at a time as long as a stable semantic similarity with the original text is maintained.
123:188	The original extraction algorithm was used to automatically create large volume of (extract, abstract, text) tuples for training extraction-based summarization systems with (abstract, text) input pairs.
124:188	Top-scoring sentences selected by the ranking mechanism described in Section 5.2 were the input to this component.
125:188	The removal process was repeated until the desired summary length was achieved.
126:188	Applying this method to the Armstrong example, the result leaves only one sentence that contains the topics chemotherapy and cancer.
127:188	It chooses sentence 3, which is not bad, though sentence 1 might be preferable.
128:188	6 Evaluation 6.1 Overview Extrinsic and intrinsic evaluations are the two classes of text summarization evaluation methods (Sparck Jones and Galliers, 1996).
129:188	Measuring content coverage or summary informativeness is an approach commonly used for intrinsic evaluation.
130:188	It measures how much source content was preserved in the summary.
131:188	A complete evaluation should include evaluations of the accuracy of components involved in the summarization process (Schiffman et al. , 2001).
132:188	Performance of the sentence classifier was shown in Section 4.
133:188	Here we will show the performance of the resulting summaries.
134:188	6.2 Coverage Evaluation An intrinsic evaluation of biography summary was recently conducted under the guidance of Document Understanding Conference (DUC2004) using the automatic summarization evaluation tool ROUGE (Recall-Oriented Understudy for Gisting Evaluation) by Lin and Hovy (2003).
135:188	50 TREC English document clusters, each containing on average 10 news articles, were the input to the system.
136:188	Summary length was restricted to 665 bytes.
137:188	Brute force truncation was applied on longer summaries.
138:188	The ROUGE-L metric is based on Longest Common Subsequence (LCS) overlap (Saggion et al. , 2002).
139:188	Figure 2 shows that our system (86) performs at an equivalent level with the best systems 9 and 10, that is, they both lie within our systems 95% upper confidence interval.
140:188	The 2class classification module was used in generating the answers.
141:188	The figure also shows the performance data evaluated with lower and higher confidences set at 95%.
142:188	The performance data are from official DUC results.
143:188	Figure 3 shows the performance results of our system 86, using 10-class sentence classification, comparing to other systems from DUC by replicating the official evaluating process.
144:188	Only system 9 performs slightly better with its score being higher than our systems 95% upper confidence interval.
145:188	A baseline system (5) that takes the first 665 bytes of the most recent text from the set as the resulting biography was also evaluated amongst the peer systems.
146:188	Clearly, humans still perform at a level much superior to any system.
147:188	Measuring fluency and coherence is also important in reflecting the true quality of machinegenerated summaries.
148:188	There is no automated tool for this purpose currently.
149:188	We plan to incorporate one for the future development of this work.
150:188	6.3 Discussion N-gram recall scores are computed by ROUGE, in addition to ROUGE-L shown here.
151:188	While cosine similarity and unigram and bigram overlap demonstrate a sufficient measure on content coverage, they are not sensitive on how information is sequenced in the text (Saggion et al. , 2002).
152:188	In evaluating and analyzing MDS results, metrics, such as ROUGE-L, that consider linguistic sequence are essential.
153:188	Radev and McKeown (1998) point out when summarizing interesting news events from multiple sources, one can expect reports with contradictory and redundant information.
154:188	An intelligent summarizer should attain as much information as possible, combine it, and present it in the most concise form to the user.
155:188	When we look at the different attributes in a persons life reported in news articles, a person is described by the job positions that he/she has held, by education institutions that he/she has attended, and etc. Those data are confirmed biographical information and do not bear the necessary contradiction associated with evolving news stories.
156:188	However, we do feel the need to address and resolve discrepancies if we were to create comprehensive and detailed 0.25 0.3 0.35 0.4 0.45 0.5 0.55 BEFHGADC91011121386151617181920522232425262728293031 ROUGE-L 95% CI Lower 95% CI Higher Figure 2.
157:188	Official ROUGE performance results from DUC2004.
158:188	Peer systems are labeled with numeric IDs.
159:188	Humans are numbered AH. 86 is our system with 2-class biography classification.
160:188	Baseline is 5.
161:188	0.17 0.22 0.27 0.32 0.37 0.42 0.47 0.52 0.57 BFEGHADC91011121386151617181920212223242552728293031 ROUGE-L 95% CL Lower 95% CL Higher Figure 3.
162:188	Unofficial ROUGE results.
163:188	Humans are labeled AH. Peer systems are labeled with numeric IDs.
164:188	86 is our system with 10-class biography classification.
165:188	Baseline is 5.
166:188	biographies on people-in-news since miscellaneous personal facts are often overlooked and told in conflicting reports.
167:188	Misrepresented biographical information may well be controversies and may never be clarified.
168:188	The scandal element from our corpus study (Section 3) is sufficient to identify information of the disputed kind.
169:188	Extraction-based MDS summarizers, such as this one, present the inherent problem of lacking the discourse-level fluency.
170:188	While sentence ordering for single document summarization can be determined from the ordering of sentences in the input article, sentences extracted by a MDS system may be from different articles and thus need a strategy on ordering to produce a fluent surface summary (Barzilay et al. , 2002).
171:188	Previous summarization systems have used temporal sequence as the guideline on ordering.
172:188	This is especially true in generating biographies where a person is represented by a sequence of events that occurred in his/her life.
173:188	Barzilay et al. also introduced a combinational method with an alternative strategy that approximates the information relatedness across the input texts.
174:188	We plan to use a fixed-form structure for the majority of answer construction, fitted for biographies only.
175:188	This will be a top-down ordering strategy, contrary to the bottom-up algorithm shown by Barzilay et al. 7 Conclusion and Future Work In this paper, we described a system that uses IR and text categorization techniques to provide summary-length answers to biographical questions.
176:188	The core problem lies in extracting biographyrelated information from large volumes of news texts and composing them into fluent, concise, multi-document summaries.
177:188	The summaries generated by the system address the question about the person, though not listing the chronological events occurring in this persons life due to the lack of background information in the news articles themselves.
178:188	In order to obtain a normal biography, one should consult other means of information repositories.
179:188	Question: Who is Sir John Gielgud?
180:188	Answer: Sir John Gielgud, one of the great actors of the English stage who enthralled audiences for more than 70 years with his eloquent voice and consummate artistry, died Sunday at his home Gielguds last major film role was as a surreal Prospero in Peter Greenaways controversial Shakespearean rhapsody.
181:188	Above summary does not directly explain who the person-in-question is, but indirectly does so in explanatory sentences.
182:188	We plan to investigate combining fixed-form and free-form structures in answer construction.
183:188	The summary would include an introductory sentence of the form x is <type/fame-category> , possibly through querying outside online resources.
184:188	A main body would follow the introduction with an assembly of checklist items generated from the 10-Class classifier.
185:188	A conclusion would contain open-ended items of special interest.
186:188	Furthermore, we would like to investigate compression strategies in creating summaries, specifically for biographies.
187:188	Our biography corpus was tailored for this purpose and will be the starting point for further investigation.
188:188	Acknowledgement We would like to thank Chin-Yew Lin from ISI for many insightful discussions on MDS, biography generation, and ROUGE.

