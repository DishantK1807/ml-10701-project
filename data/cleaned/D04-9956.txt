1:141	Efficient Decoding for Statistical Machine Translation with a Fully Expanded WFST Model Hajime Tsukada NTT Communication Science Labs.
2:141	2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan tsukada@cslab.kecl.ntt.co.jp Masaaki Nagata NTT Cyber Space Labs.
3:141	1-1 Hikari-no-Oka Yokosuka-shi Kanagawa 239-0847 Japan nagata.masaaki@lab.ntt.co.jp Abstract This paper proposes a novel method to compile statistical models for machine translation to achieve efficient decoding.
4:141	In our method, each statistical submodel is represented by a weighted finite-state transducer (WFST), and all of the submodels are expanded into a composition model beforehand.
5:141	Furthermore, the ambiguity of the composition model is reduced by the statistics of hypotheses while decoding.
6:141	The experimental results show that the proposed model representation drastically improves the efficiency of decoding compared to the dynamic composition of the submodels, which corresponds to conventional approaches.
7:141	1 Introduction Recently, research on statistical machine translation has grown along with the increase in computational power as well as the amount of bilingual corpora.
8:141	The basic idea of modeling machine translation was proposed by Brown et al.9:141	(1993), who assumed that machine translation can be modeled on noisy channels.
10:141	The source language is encoded from a target language by a noisy channel, and translation is performed as a decoding process from source language to target language.
11:141	Knight (1999) showed that the translation problem defined by Brown et al.12:141	(1993) is NPcomplete.
13:141	Therefore, with this model it is almost impossible to search for optimal solutions in the decoding process.
14:141	Several studies have proposed methods for searching suboptimal solutions.
15:141	Berger et al.16:141	(1996) and Och et al.17:141	(2001) proposed such depth-first search methods as stack decoders.
18:141	Wand and Waibel (1997) and Tillmann and Ney (2003) proposed breadth-first search methods, i.e. beam search.
19:141	Germann (2001) and Watanabe and Sumita (2003) proposed greedy type decoding methods.
20:141	In all of these search algorithms, better representation of the statistical model in systems can improve the search efficiency.
21:141	For model representation, a search method based on weighted finite-state transducer (WFST) (Mohri et al. , 2002) has achieved great success in the speech recognition field.
22:141	The basic idea is that each statistical model is represented by a WFST and they are composed beforehand; the composed model is optimized by WFST operations such as determinization and minimization.
23:141	This fully expanded model permits efficient searches.
24:141	Our motivation is to apply this approach to machine translation.
25:141	However, WFST optimization operations such as determinization are nearly impossible to apply to WFSTs in machine translation because they are much more ambiguous than speech recognition.
26:141	To reduce the ambiguity, we propose a WFST optimization method that considers the statistics of hypotheses while decoding.
27:141	Some approaches have applied WFST to statistical machine translation.
28:141	Knight and AlOnaizan (1998) proposed the representation of IBM model 3 with WFSTs; Bangalore and Riccardi (2001) studied WFST models in call-routing tasks, and Kumar and Byrne (2003) modeled phrase-based translation by WFSTs.
29:141	All of these studies mainly focused on the representation of each submodel used in machine translation.
30:141	However, few studies have focued on the integration of each WFST submodel to improve the decoding efficiency of machine translation.
31:141	To this end, we propose a method that expands all of the submodels into a composition model, reducing the ambiguity of the expanded model by the statistics of hypotheses while decoding.
32:141	First, we explain the translation model (Brown et al. , 1993; Knight and Al-Onaizan, 1998) that we used as a base for our decoding research.
33:141	Second, our proposed method is introduced.
34:141	Finally, experimental results show that our proposed method drastically improves decoding efficiency.
35:141	2 IBM Model For our decoding research, we assume the IBMstyle modeling for translation proposed in Brown et al.36:141	(1993).
37:141	In this model, translation from Japanese a0 to English a1 attempts to find the a1 that maximizes a2a4a3 a1a6a5 a0a8a7.
38:141	Using Bayes rule, a2a4a3 a1a6a5 a0a8a7 is rewritten as a9a11a10a13a12a15a14a16a9a18a17a20a19 a2a4a3 a1a6a5 a0a8a7a22a21 a9a11a10a13a12a15a14a16a9a18a17a20a19 a2a4a3 a0 a5a1 a7 a2a4a3 a1 a7a24a23 where a2a4a3 a1 a7 is referred to as a language model and a2a4a3 a0 a5a1 a7 is referred to as a translation model.
39:141	In this paper, we use word trigram for a language model and IBM model 3 for a translation model.
40:141	The translation model is represented as follows considering all possible word alignments.
41:141	a2a4a3 a0 a5a1 a7a25a21 a26 a2a4a3 a0a27a23 a9 a5a1 a7a24a28 The IBM model only assumes a one-to-many word alignment, where a Japanese word a0 in the a29 -th position connects to the English word a1 in the a9a31a30 -th position.
42:141	The IBM model 3 uses the following a2a4a3 a0a27a23 a9 a5a1 a7 . a2a4a3 a0a27a23 a9 a5a1 a7a22a21 a14a33a32a35a34a37a36 a34a37a36 a38a40a39a42a41a37a43a45a44a47a46 a36 a3a24a48 a32 a38 a36 a7 a44a15a46a50a49 a51 a52a54a53 a36 a34 a52a56a55a11a57 a3 a34 a52 a5a1 a52 a7 a49 a39 a30 a53a8a58 a59 a3 a0 a30 a5a1 a26a61a60 a7a24a62 a3 a29a8a5 a9a63a30 a23 a14 a23a61a64a54a7a24a28 (1) a34 a52 the a number of a0 words connecting to a1 a52, and it is called fertility.
43:141	Note, however, that a34a37a36 is the number of words connecting to null words.
44:141	a57 a3 a34 a5a1 a52 a7 is conditional probability where English word a1 a52 connects to a34 words in a0 . a57 a3 a34 a5a1 a52 a7 is called fertility probability.
45:141	a59 a3 a0 a30 a5a1 a52 a7 is conditional probability where English word a1 a52 is translated to Japanese word a0 a30 and called translation probability.
46:141	a62 a3 a29a65a5a66 a23a61a64a54a23 a14 a7 is conditional probability where the English word in the a66 -th position connects to the the Japanese word in the a29 -th position on condition that the length of the English sentence a1 and Japanese sentence a0 are a64 and a14, respectively.
47:141	a62 a3 a29a8a5a66 a23a61a64a67a23 a14 a7 is called distortion probability.
48:141	In our experiment, we used the IBM model 3 while assuming constant distortion probability for simplicity.
49:141	3 WFST Cascade Model WFST is a finite-state device in which output symbols and output weights are defined as well as input symbols.
50:141	Using composition (Pereira and Riley, 1997), we can obtain the combined WFST a68 a58a70a69 a68 a43by connecting each output of a68 a58 to an input of a68 a43 . If we assume that each submodel of Equation (1) is represented by a WFST, a conventional decoder can be considered to compose submodels dynamically.
51:141	kaku:each/ t(kaku|each) tekisuto:text/ t(tekisuto|text)ha:NULL/ t(ha|NULL) Figure 2: T Model NULL:/1-p0 :/p0 each:each/1.0 tex:text/1.0 Figure 3: NULL Model The main idea of the proposed approach is to compute the composition beforehand.
52:141	Figure 1 shows the translation process modeled by a WFST cascade.
53:141	This WFST cascade model (Knight and Al-Onaizan, 1998) represents the IBM model 3 described in the previous section.
54:141	Any possible permutations of the Japanese sentence are inputed to the cascade.
55:141	First, T model(a68 ) translates the Japanese word to an English word.
56:141	NULL model(a71 ) deletes special word NULL.
57:141	Fertility model(a72 ) merges the same continuous words into one word.
58:141	At each stage, the probability represented by the weight of a WFST is accumulated.
59:141	Finally, the weight of language model (a73 ) is accumulated.
60:141	If WFST a74 represents all permutations of the input sentence, decoding can be considered to search for the best path of a74 a69 a68 a69 a71 a69 a72 a69 a73 . Therefore, computing a68 a69 a71 a69 a72 a69 a73 in advance can improve the efficiency of the decoder.
61:141	For a68, a71, and a72, we adopt the representation of Knight and Al-Onaizan (1998).
62:141	For a73, we adopt the representation of Mohri et al.63:141	(2002).
64:141	Figures 2 5 show examples of submodel representation with WFSTs.
65:141	a75 a3 a17 a7 in Figure 5 stands for a back-off parameter.
66:141	Conditional branches are represented by nondeterministic paths in the WFST.
67:141	4 Ambiguity Reduction If we can determinize a fully-expanded WFST, we can achieve the best performance of the decoder.
68:141	kaku tekisuto SGMLdeko-do ka sareruha each text encoded in SGMLNULL encoded encoded T Model (T) NULL Model (N) Fertility Model (F) Language Model (L) each text encoded in SGMLencoded encoded each text is in SGMLencoded each text is in SGMLencoded Figure 1: Translation with WFST Cascade Model encoded:encoded/ n(1|encoded) :encoded/ n(0|encoded) encoded:encoded/ n(2|encoded) encoded:/ n(3|encoded)/n(2|encoded) encoded:/ n(4|encoded)/n(3|encoded) encoded:/1.0 encoded:/1.0 encoded:/1.0 Figure 4: Fertility Model However, the composed WFST for machine translation is not obviously determinizable.
69:141	The wordto-word translation model a68 strongly contributes to WFSTs ambiguity while the a76 transition of other submodels also contributes to ambiguity.
70:141	Mohri et al.71:141	(2002) proposed a technique that added special symbols allowing the WFST to be determinizable.
72:141	Determinization using this technique, however, is not expected to achieve efficient decoding in machine translation because the WFSTs of machine translation are inherently ambiguous.
73:141	To overcome this problem, we propose a novel WFST optimization approach that uses decoding information.
74:141	First, our method merges WFST states by considering the statistics of hypotheses while decoding.
75:141	After merging the states, redundant edges whose beginning states, end states, input symbols, and output symbols are the same are also reduced.
76:141	IBM models consider all possible alignments while a decoder searches for only the most appropriate alignment.
77:141	Therefore, there are many redundant states in the full-expansion WFST from the viewpoint of decoding.
78:141	We adopted a standard decoding algorithm in the speech recognition field, where the forward is beam-search and the backward is a77a79a78 search.
79:141	Since beam-search is adopted in the forward pass, the obtained results are not optimal but suboptimal.
80:141	All input permutations are represented by a finite-state acceptor (Figure 6), where each state corresponds to input positions that are already read.
81:141	In the forward search, hypotheses are maintained for each state of ab ca bc a b c  c:c/P(c|ab) b:b/P(b|ca) a:a/P(a|bc) :/b(ca) b:b/P(b|a) :/b(ab) c:c/P(c|b) :/b(bc)a:a/P(a|c) :/b(c) c:c/P(c) b:b/P(b) :/b(b):/b(a) a:a/P(a) Figure 5: Trigram Language Model the finite-state acceptor.
82:141	The WFST states that always appear together in the same hypothesis list of the forward beam-search should be equated if the states contribute to correct translation.
83:141	Let a80 be a full-expansion WFST model and a81a82a1 a0a61a83 be a WFST that represents the correct translation of an input sentence a0 . For each a0, the states of a80 that always appear together in the same hypothesis list in the course of decoding a0 with a80 a69 a81a82a1 a0a61a83 are merged in our method.
84:141	Simply merging states of a80 may increase model errors, but a81a82a1 a0a61a83 corrects the errors caused by merging states.
85:141	Unlike ordinary FSA minimization, states are merged without considering their successor states.
86:141	If the weight represents probability, thesum of the weights of output transitions may not be 1.0 after merging states, and then thecondition of probability may be destroyed.
87:141	Since the decoder does not sum up all possible paths but searches for the most appropriate paths, this kind of state merging does not pose a serious problem in practice.
88:141	In the following experiment, we measured the association between states by a34 a43 in Gale and Church (1991).
89:141	a34 a43 is a a84 a43 -like statistic that is bounded between 0 and 1.
90:141	If the a34 a43 of two states is higher than the specified threshold, these two states are merged.
91:141	The definition of a34 a43 is as follows, where a9 a21a85a0 a10 a1a31a86 a3 a86 a58 a23 a86 a43 a7, a75 a21a85a0 a10 a1a11a86 a3 a86 a58 a7 a32a87a9, a88 a21a89a0 a10 a1a11a86 a3 a86 a43 a7 a32a90a9, and a62a91a21 a71 a32a90a9a92a32 a75 a32a93a88 . a71 is the total number of hypothesis lists.
92:141	a0 a10 a1a11a86 a3 a86 a7 (a0 a10 a1a11a86 a3 a86 a58 a23 a86 a43 a7 ) is the number of hypothesis lists in which a86 appears (both a86 a58 and a86 a43 appear).
93:141	a34 a43 a21 a3 a9 a62 a32 a75 a88 a7 a43 a3 a9a79a94 a75 a7 a3 a9a95a94a96a88 a7 a3 a75 a94 a62a47a7 a3 a88a97a94 a62a40a7 a28 {} {1} {2} {3} {1,2} {2,3} {1,3} {1,2,3} Figure 6: FSA for All Input Permutations Merging the beginning and end states of a transition whose input is a76 (a76 transition for short) may cause a problem when decoding.
94:141	In our implementation, weight is basically minus a64a54a98 a12 probability, and its lower bound is 0 in theory.
95:141	However, there exists negative a76 transition that originated from the backoff value of n-gram.
96:141	If we merge the beginning and end states of the negative a76 transition, the search process will not stop due to the negative a76 loop.
97:141	To avoid this problem, we rounded the negative weight to 0 if the negative a76 loop appears during merging.
98:141	In the preliminary experiment, a weight-pushing operation (Mohri and Riley, 2001) was also effective for deleting negative a76 transition of our fullexpansion models.
99:141	However, pushing causes an imbalance of weights among paths if the WFST is not deterministic.
100:141	As a result of this imbalance, we cannot compare path costs when pruning.
101:141	In fact, our preliminary experiment showed that pushed fullexpansion WFST does not work well.
102:141	Therefore, we adopted a simpler method to deal with a negative a76 loop as described above.
103:141	5 Experiments 5.1 Effect of Full Expansion To clarify the effectiveness of a full-expansion approach, we compared the computational costs while using the same decoder with both dynamic composition and static composition, a full-expansion model in other words.
104:141	In the forward beam-search, any hypothesis whose probability is lower than a48a100a99a100a48a100a101 of the top of the hypothesis list is pruned.
105:141	In this experiment, permutation is restricted, and words can be moved 6 positions at most.
106:141	The translation model was trained by GIZA++ (Och and Ney, 2003), and the trigram was trained by the CMU-Cambridge Statistical Language Modeling Toolkit v2 (Clarkson and Rosenfeld, 1997).
107:141	For the experiment, we used a Japanese-toEnglish bilingual corpus consisting of example sentences for a rule-based machine translation system.
108:141	Each language sentence is aligned in the corpus.
109:141	The total number of sentence pairs is 20,204.
110:141	We used 17,678 pairs for training and 2,526 pairs for the test.
111:141	The average length of Japanese sentences was 8.4 words, and that of English sentences was 6.7 words.
112:141	The Japanese vocabulary consisted of 15,510 words, and the English vocabulary was 11,806 words.
113:141	Table 1 shows the size of the WFSTs used in the experiment.
114:141	In these WFSTs, special symbols that express beginning and end of sentence are added to the WFSTs described in the previous section.
115:141	The NIST score (Doddington, 2002) and BLEU Score (Papineni et al. , 2002) were used to measure translation accuracy.
116:141	Table 2 shows the experimental results.
117:141	The fullexpansion model provided translations more than 10 times faster than conventional dynamic composition submodels without degrading accuracy.
118:141	However, the NIST scores are slightly different.
119:141	In the course of composition, some paths that do not reach the final states are produced.
120:141	In the full-expansion model these paths are trimmed.
121:141	These trimmed paths may cause a slight difference in NIST scores.
122:141	5.2 Effect of Ambiguity Reduction To show the effect of ambiguity reduction, we compared the translation results of three different models.
123:141	Model a102 is the full-expansion model described above.
124:141	Model a81 is a reduced model by using our proposed method with a 0.9 a34 a43 threshold.
125:141	Model a81a82a103 is a reduced model with the statistics of the decoder without using the correct translation WFST.
126:141	In other words, a81a82a103 reduces the states of the fullexpansion model more roughly than a81 . The a34 a43 threshold for a81a82a103 is set to 0.85 so that the size of the produced WFST is almost the same as a81 . Table 3 shows the model size.
127:141	To obtain decoder statistics for calculating a34 a43, all of the sentence pairs in the training set were used.
128:141	When obtaining the statistics, any hypothesis whose probability is lower than a48a100a99a100a48a100a101 a36a31a104a105 of the top of the hypothesis list is pruned in the forward beam-search.
129:141	The translation experiment was conducted by successively changing the beam width of the forward search.
130:141	Figures 7 and 8 show the results of the translation experiments, revealing that our proposed model can reduce the decoding time by approximately half.
131:141	This model can reduce decoding time to a much greater extent than the rough reduction model, indicating that our state merging criteria are valid.
132:141	6 Conclusions We proposed a method to compile statistical models to achieve efficient decoding in a machine translation system.
133:141	In our method, each statistical submodel is represented by a WFST, and all submodels are composed beforehand.
134:141	To reduce the ambiguity of the composed WFST, the states are merged according to the statistics of hypotheses while decoding.
135:141	As a result, we reduced decoding time to approximately a48a100a99 a103 a101 of dynamic composition of submodels, which corresponds to the conventional approach.
136:141	In this paper, we applied the state merging method to a fully-expanded WFST and showed the effectiveness of this approach.
137:141	However, the state merging method itself is general and independent of the fully-expanded WFST.
138:141	We can apply this method to each submodel of machine translation.
139:141	More generally, we can apply it to all WFST-like models, including HMMs.
140:141	Acknowledgements We would like to thank F. J. Och for providing GIZA++ and mkcls toolkits, and P. R. Clarkson for the CMU-Cambridge statistical language modeling toolkit v2.
141:141	We also thank T. Hori for providing the n-gram conversion program for WFSTs and F. Bond and S. Fujita for providing the bilingual corpus.

