Furthermore, our model is not necessarily nativist; these biases may be innate, but they may also be the product of some other earlier learning algorithm, as the results of Ellison (1992) and Brown et al.(1992) suggest (see Section 5.2).
J92-4003
Ellison (1994), for example, has shown how to map the optimality constraints of Prince and Smolensky (1993) to finite-state automata; given this result, models of 1 Although our assumption of the simultaneous presentation of surface and underlying forms to the learner may seem at first glance to be unnatural as well, it is quite compatible with certain theories of word-based morphology.
C94-2163
The most popular method is the two-level formalism of Koskenniemi (1983), based on Johnson (1972) and the (belatedly published) work of Kaplan and Kay (1994), and various implementations and extensions (summarized and contrasted in Karttunen \[1993\]).
J94-3001
Such datadriven models include the stress acquisition models of Daelemans, Gillis, and Durieux (1994) (an application of Instance-based Learning \[Aha, Kibler, and Albert 1991\]) and Gupta and Touretzky (1994) (an application of Error Back-Propagation), as well as Ellison's (1992) Minimum-Description-Length-based model of the acquisition of the basic concepts of syllabicity and the sonority hierarchy.
J94-3007
More recently, Bird and Ellison (1994) show that a one-level finite-state automaton can model richer phonological structure, such as the multitier representations of autosegmental phonology.
C94-2163 J94-1003
Johnson (1984) gives one of the first computational algorithms for phonological rule induction.
P84-1070
Nonsegmental approaches include those of Daelemans, Gillis, and Durieux (1994) for learning stress systems, as well as approaches to learning morphology such as Gasser's (1993) system for inducing Semitic morphology, and Ellison's (1992) extensive work on syllabicity, sonority, and harmony.
J94-3007
Brown et al.(1992) used a purely data-driven greedy, incremental clustering algorithm to derive word-classes for n-gram grammars; their algorithm successfully induced classes like 514 Gildea and Jurafsky Learning Bias and Phonological-Rule Induction V(~t ..r(,~ r~,.~C V VcV C :tC "~J'"~ / t : ~ r :tr ~ 9.
J92-4003
This means that finite-state transducers (FSTs) can be used to represent phonological rules, greatly simplifying the problem of parsing the output of phonological rules in order to obtain the underlying, lexical forms (Koskenniemi 1983; Karttunen 1993; Pulman and Hepple 1993; Bird 1995; Bird and Ellison 1994).
C94-2163 J94-1003
