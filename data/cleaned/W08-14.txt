1:218	Coling 2008 22nd International Conference on Computational Linguistics Proceedings of the 2nd workshop on Multi-source, Multilingual Information Extraction and Summarization 23 August 2008 Manchester, UK c2008 The Coling 2008 Organizing Committee Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Nonported license http://creativecommons.org/licenses/by-nc-sa/3.0/ Some rights reserved Order copies of this and other Coling proceedings from: Association for Computational Linguistics (ACL) 209 N. Eighth Street Stroudsburg, PA 18360 USA Tel: +1-570-476-8006 Fax: +1-570-476-0860 acl@aclweb.org ISBN 978-1-905593-51-4 Design by Chimney Design, Brighton, UK Production and manufacture by One Digital, Brighton, UK ii Editors Foreword Information extraction (IE) and text summarization (TS) are key technologies aiming at extracting relevant information from texts and presenting the information to the user in a condensed form.
2:218	The ongoing information explosion makes IE and TS particularly critical for successful functioning within the information society.
3:218	These technologies, however, face new challenges with the adoption of the Web 2.0 paradigm (e.g., blogs, wikis) due to their inherent multi-source nature.
4:218	These technologies must no longer deal only with isolated texts or narratives, but with large-scale repositories or sourcespossibly in several languagescontaining a multiplicity of views, opinions, and commentaries on particular topics, entities and events.
5:218	There is thus a need to adapt and/or develop new techniques to deal with these new phenomena.
6:218	Recognising similar information across different sources and/or in different languages is of paramount importance in this multi-source, multi-lingual context.
7:218	In information extraction, merging information from multiple sources can lead to increased accuracy, as compared to extraction from a single source.
8:218	In text summarization, similar facts found across sources can inform sentence scoring algorithms.
9:218	In question answering, the distribution of answers in similar contexts can inform answer-ranking components.
10:218	Often, it is not the similarity of information that matters, but its complementary nature.
11:218	In a multi-lingual context, information extraction and text summarization can provide solutions for crosslingual access: key pieces of information can be extracted from different texts in one or many languages, merged, and then conveyed in natural language in concise form.
12:218	Applications need to be able to cope with the idiosyncratic nature of the new Web 2.0 media: mixed input, new jargon, ungrammatical and mixed-language input, emotional discourse, etc. In this context, synthesizing or inferring opinions from multiple sources is a new and exciting challenge for NLP.
13:218	On another level, profiling of individuals who engage in the new social Web, and identifying whether a particular opinion is appropriate/relevant in a given context are important topics to be addressed.
14:218	The objective of this second Multi-source Multilingual Information Extraction and Summarization (MMIES) workshop is to bring together researchers and practitioners in information-access technologies, to discuss recent approaches for dealing with multi-source and multi-lingual challenges.
15:218	Each paper submitted to the workshop was reviewed by three members of an international Programme Committee.
16:218	The selection process resulted in this volume of eight papers, covering the following key topics:  Multilingual Named Entity Recognition,  Automatic Construction of Multilingual Dictionaries for Information Retrieval,  Multi-document Summaries for Geo-referenced Images,  Keyword Extraction for Single-Document Summarization,  Recognizing Similar News over Time and across Languages,  Speech-to-Text Summarization,  Automatic Annotation of Bibliographical References.
17:218	iii We are grateful to the members of the programme committee for their invaluable work, as well as to Roger Evans, Mark Stevenson and Harold Somers for their support.
18:218	We thank Robert Gaizauskas for giving the invited talk at the workshop.
19:218	July 2008.
20:218	Sivaji Bandyopadhyay, Jadavpur University (India) Thierry Poibeau, CNRS / Universite Paris 13 (France) Horacio Saggion, University of Sheffield (UK) Roman Yangarber, University of Helsinki (Finland) iv Organizers  Sivaji Bandyopadhyay, Jadavpur University (India)  Thierry Poibeau, CNRS and University of Paris 13 (France)  Horacio Saggion, University of Sheffield (United Kingdom)  Roman Yangarber, University of Helsinki (Finland) Programme Committee  Javier Artiles, UNED (Spain)  Kalina Bontcheva, University of Sheffield (UK)  Nathalie Colineau, CSIRO (Australia)  Nigel Collier, NII (Japan)  Hercules Dalianis, KTH/Stockholm University (Sweden)  Thierry Declerk, DFKI (Germany)  Michel Genereux, LIPN-CNRS (France)  Julio Gonzalo, UNED (Spain)  Brigitte Grau, LIMSI-CNRS (France)  Ralph Grishman, New York University (USA)  Kentaro Inui, NAIST (Japan)  Min-Yen Kan, National University of Singapore (Singapore)  Guy Lapalme, University of Montreal (Canada)  Diana Maynard, University of Sheffield (UK)  Jean-Luc Minel, Modyco-CNRS (France)  Constantin Orasan, University of Wolverhampton (UK)  Cecile Paris, CSIRO (Australia)  Maria Teresa Pazienza, University of Roma Tor Vergata (Italy)  Bruno Pouliquen, European Commission  Joint Research Centre (Italy)  Patrick Saint-Dizier, IRIT-CNRS (France) v  Agnes Sandor, Xerox XRCE (France)  Satoshi Sekine, NYU (USA)  Ralf Steinberger, European Commission  Joint Research Centre (Italy)  Stan Szpakowicz, University of Ottawa (Canada)  Lucy Vanderwende, Microsoft Research (USA)  Jose Luis Vicedo, Universidad de Alicante (Spain) vi Table of Contents Generating Image Captions using Topic Focused Multi-document Summarization Robert Gaizauskas  1 Learning to Match Names Across Languages Inderjeet Mani, Alex Yeh and Sherri Condon  2 Automatic Construction of Domain-specific Dictionaries on Sparse Parallel Corpora in the Nordic languages Sumithra Velupillai and Hercules Dalianis10 Graph-Based Keyword Extraction for Single-Document Summarization Marina Litvak and Mark Last  17 MultiSum: Query-Based Multi-Document Summarization Mike Rosner and Carl Camilleri25 Mixed-Source Multi-Document Speech-to-Text Summarization Ricardo Ribeiro and David Martins de Matos33 Evaluating automatically generated user-focused multi-document summaries for geo-referenced images Ahmet Aker and Robert Gaizauskas  41 Story tracking: linking similar news over time and across languages Bruno Pouliquen, Ralf Steinberger and Olivier Deguernel49 Automatic Annotation of Bibliographical References with target Language Harald Hammarstrom  57 vii  Conference Programme Wednesday, August 23, 2008 Invited Talk 9:3010:30 Generating Image Captions using Topic Focused Multi-document Summarization Robert Gaizauskas 10:3011:00 Coffee break Session 1: Named Entity and Lexical Resources for IE and Summarization 11:0011:30 Learning to Match Names Across Languages Inderjeet Mani, Alex Yeh and Sherri Condon 11:3012:00 Automatic Construction of Domain-specific Dictionaries on Sparse Parallel Corpora in the Nordic languages Sumithra Velupillai and Hercules Dalianis 12:0012:30 Graph-Based Keyword Extraction for Single-Document Summarization Marina Litvak and Mark Last 12:3014:00 Lunch Session 2: Multi-document Summarization 14:0014:30 MultiSum: Query-Based Multi-Document Summarization Mike Rosner and Carl Camilleri 14:3015:00 Mixed-Source Multi-Document Speech-to-Text Summarization Ricardo Ribeiro and David Martins de Matos 15:0015:30 Evaluating automatically generated user-focused multi-document summaries for geo-referenced images Ahmet Aker and Robert Gaizauskas 15:3016:00 Coffee break ix Wednesday, August 23, 2008 (continued) Session 3: Applications 16:0016:30 Story tracking: linking similar news over time and across languages Bruno Pouliquen, Ralf Steinberger and Olivier Deguernel 16:3017:00 Automatic Annotation of Bibliographical References with target Language Harald Hammarstrom 17:0017:30 Open Discussion x Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, page 1 Manchester, August 2008 Generating Image Captions using Topic Focused Multi-document Summarization Robert Gaizauskas Natural Language Processing Group Department of Computer Science, University of Sheffield Regent Court, 211 Portobello, Sheffield, S1 4DP, UK R.Gaizauskas@sheffield.ac.uk In the near future digital cameras will come standardly equipped with GPS and compass and will automatically add global position and direction information to the metadata of every picture taken.
21:218	Can we use this information, together with information from geographical information systems and the Web more generally, to caption images automatically?
22:218	This challenge is being pursued in the TRIPOD project (http://tripod.shef.ac.uk/) and in this talk I will address one of the subchallenges this topic raises: given a set of toponyms automatically generated from geo-data associated with an image, can we use these toponyms to retrieve documents from the Web and to generate an appropriate caption for the image?
23:218	We begin assuming the toponyms name the principal objects or scene contents in the image.
24:218	Using web resources (e.g. Wikipedia) we attempt to determine the types of these things  is this a picture of church?
25:218	a mountain?
26:218	a city?
27:218	We have constructed a taxonomy of such image content types using on-line collections of captioned images and for each type in the taxonomy we have constructed several collections of texts describing that type.
28:218	For example, we have a collection of captions describing churches and a collection of Wiki pages describing churches.
29:218	The intuition here is that these collections are examples of, e.g. the sorts of things people say in captions or in descriptions of churches.
30:218	These collections can then be used to derive models of objects or scene types which in turn can be used to bias or focus multi-document summaries of new images of things of the same c2008.
31:218	Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
32:218	Some rights reserved.
33:218	type.
34:218	In the talk I report results of work we have carried out to explore the hypothesis underlying this approach, namely that brief multi-document summaries generated as image captions by using models of object/scene types to bias or focus content selection will be superior to generic multidocument summaries generated for this purpose.
35:218	I describe how we have constructed an image content taxonomy, how we have derived text collections for object/scene types, how we have derived object/scene type models from these collections and how these have been used in multi-document summarization.
36:218	I also discuss the issue of how to evaluate the resulting captions and present preliminary results from one sort of evaluation.
37:218	1 Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 29 Manchester, August 2008 Learning to Match Names Across Languages Inderjeet Mani The MITRE Corporation 202 Burlington Road Bedford, MA 01730, USA imani@mitre.org Alex Yeh The MITRE Corporation 202 Burlington Road Bedford, MA 01730, USA asy@mitre.org Sherri Condon The MITRE Corporation 7515 Colshire Drive McLean, VA 22102, USA scondon@mitre.org  Abstract We report on research on matching names in different scripts across languages.
38:218	We explore two trainable approaches based on comparing pronunciations.
39:218	The first, a cross-lingual approach, uses an automatic name-matching program that exploits rules based on phonological comparisons of the two languages carried out by humans.
40:218	The second, monolingual approach, relies only on automatic comparison of the phonological representations of each pair.
41:218	Alignments produced by each approach are fed to a machine learning algorithm.
42:218	Results show that the monolingual approach results in machine-learning based comparison of person-names in English and Chinese at an accuracy of over 97.0 F-measure.
43:218	1 Introduction The problem of matching pairs of names which may have different spellings or segmentation arises in a variety of common settings, including integration or linking database records, mapping from text to structured data (e.g., phonebooks, gazetteers, and biological databases), and text to text comparison (for information retrieval, clustering, summarization, coreference, etc.).
44:218	For named entity recognition, a name from a gazetteer or dictionary may be matched against text input; even within monolingual applications, the forms of these names might differ.
45:218	In multidocument summarization, a name may have different forms across different sources.
46:218	Systems   2008 The MITRE Corporation.
47:218	All rights reserved.
48:218	Licensed for use in the proceedings of the Workshop on Multi-source, Multilingual Information Extraction and Summarization (MIMIES2) at COLING2008.
49:218	that address this problem must be able to handle variant spellings, as well as abbreviations, missing or additional name parts, and different orderings of name parts.
50:218	In multilingual settings, where the names being compared can occur in different scripts in different languages, the problem becomes relevant to additional practical applications, including both multilingual information retrieval and machine translation.
51:218	Here special challenges are posed by the fact that there usually arent one-to-one correspondences between sounds across languages.
52:218	Thus the name Stewart, pronounced   / s t u w   r t / in IPA, can be mapped to Mandarin  , which is Pinyin si tu er te, pronounced /s i t  u a   t  e/, and the name Elizabeth / I l I z   b    / can map to , which is Pinyin yi li sha bai, pronounced /I l I     p aI/.
53:218	Further, in a given writing system, there may not be a one-to-one correspondence between orthography and sound, a well-known case in point being English.
54:218	In addition, there may be a variety of variant forms, including dialectical variants, (e.g., Bourguiba can map to Abu Ruqayba), orthographic conventions (e.g., Anglophone Wasim can map to Francophone Ouassime), and differences in name segmentation (Abd Al Rahman can map to Abdurrahman).
55:218	Given the high degree of variation and noise in the data, approaches based on machine learning are needed.
56:218	The considerable differences in possible spellings of a name also call for approaches which can compare names based on pronunciation.
57:218	Recent work has developed pronunciation-based models for name comparison, e.g., (Sproat, Tao and Zhai 2006) (Tao et al. 2006).
58:218	This paper explores trainable pronunciation-based models further.
59:218	2 Table 1: Matching Ashburton and  Consider the problem of matching Chinese script names against their English (Pinyin) Romanizations.
60:218	Chinese script has nearly 50,000 characters in all, with around 5,000 characters in use by the well-educated.
61:218	However, there are only about 1,600 Pinyin syllables when tones are counted, and as few as 400 when they arent. This results in multiple Chinese script representations for a given Roman form name and many Chinese characters that map to the same Pinyin forms.
62:218	In addition, one can find multiple Roman forms for many names in Chinese script, and multiple Pinyin representations for a Chinese script representation.
63:218	In developing a multilingual approach that can match names from any pair of languages, we compare an approach that relies strictly on monolingual knowledge for each language, specifically, grapheme-to-phoneme rules for each language, with a method that relies on cross-lingual rules which in effect map between graphemic and/or phonemic representations for the specific pair of languages.
64:218	The monolingual approach requires finding data on the phonemic representations of a name in a given language, which (as we describe in Section 4) may be harder than finding more graphemic representations.
65:218	But once the phonemic representation is found for names in a given language, then as one adds more languages to a system, no more work needs to be done in that given language.
66:218	In contrast, with the crosslingual approach, whenever a new language is added, one needs to  go over all the existing languages already in the system and compare each of them with the new language to develop cross-lingual rules for each such language pair.
67:218	The engineering of such rules requires bilingual expertise, and knowledge of differences between language pairs.
68:218	The cross-lingual approach is thus more expensive to develop, especially for applications which require coverage of a large number of languages.
69:218	Our paper investigates whether we can address the name-matching problem without requiring such a knowledge-rich approach, by carrying out a comparison of the performance of the two approaches.
70:218	We present results of large-scale machine-learning for matching personal names in Chinese and English, along with some preliminary results for English and Urdu.
71:218	2 Basic Approaches 2.1 Cross-Lingual Approach Our cross-lingual approach (called MLEV) is based on (Freeman et al. 2006), who used a modified Levenshtein string edit-distance algorithm to match Arabic script person names against their corresponding English versions.
72:218	The Levenshtein edit-distance algorithm counts the minimum number of insertions, deletions or substitutions required to make a pair of strings match.
73:218	Freeman et al.74:218	(2006) used (1) insights about phonological differences between the two languages to create rules for equivalence classes of characters that are treated as identical in the computation of edit-distance and (2) the use of normalization rules applied to the English and transliterated Arabic names based on mappings between characters in the respective writing systems.
75:218	For example, characters corresponding to low diphthongs in English are normalized as w, the transliteration for the Arabic  character, while high diphthongs are mapped to y, the transliteration for the Arabic   character.
76:218	Table 1 shows the representation and comparison of a Roman-Chinese name pair (shown in the title) obtained from the Linguistic Data Consortiums LDC Chinese-English name pairs corpus (LDC 2005T34).
77:218	This corpus provides name part pairs, the first element in English (Roman characters) and the second in Chinese characters, created by the LDC from Xinhua Newswire's proper name and who's who databases.
78:218	The name part can be a first, middle or last name.
79:218	We compare the English form of the name with a Pinyin Romanization of the Chinese.
80:218	(Since the Chinese is being compared with English, which is toneless, the tone part of Pinyin is being ignored throughout this paper.)
81:218	For this study, the Levenshtein edit-distance score (where a perfect match scores zero) is  Roman Chinese (Pinyin) Alignment Score LEV ashburton ashenbodu |   a   s   h   b   u   r   t   o   n   | |   a   s   h   e   n   b  o  d    u   | 0.67 MLEV ashburton ashenbodu |  a   s   h       b   u   r    t   o   n  | |  a   s   h   e   n   b   o     d   u    | 0.72 MALINE asVburton aseCnpotu |   a   sV    b   <   u   r   t   o   |   n |   a   s   eC  n   p   o     t   u   |   0.48 3 normalized to a similarity score as in (Freeman et al. 2006), where the score ranges from 0 to 1, with 1 being a perfect match.
82:218	This edit-distance score is shown in the LEV row.
83:218	The MLEV row, under the Chinese Name column, shows an Englishized normalization of the Pinyin for Ashburton.
84:218	Certain characters or character sequences in Pinyin are pronounced differently than in English.
85:218	We therefore apply certain transforms to the Pinyin; for example, the following substitutions are applied at the start of a Pinyin syllable, which makes it easier for an English speaker to see how to pronounce it and renders the Pinyin more similar to English orthography: u: (umlaut u) => u, zh => j, c => ts, and q => ch (so the Pinyin Qian is more or less pronounced as if it were spelled as Chian, etc.).
86:218	The MLEV algorithm uses equivalence classes that allow o and u to match, which results in a higher score than the generic score using the LEV method.
87:218	2.2 Monolingual Approach Instead of relying on rules that require extensive knowledge of differences between a language pair 2 , the monolingual approach first builds phonemic representations for each name, and then aligns them.
88:218	Earlier research by (Kondrak 2000) used dynamic programming to align strings of phonemes, representing the phonemes as vectors of phonological features, which are associated with scores to produce similarity values.
89:218	His program ALINE includes a skip function in the alignment operations that can be exploited for handling epenthetic segments, and in addition to 1:1 alignments, it also handles 1:2 and 2:1 alignments.
90:218	In this research, we made extensive modifications to ALINE to add the phonological features for languages like Chinese and Arabic and to normalize the similarity scores, producing a system called MALINE.
91:218	In Table 1, the MALINE row 3  shows that the English name has a palato-alveolar modification   2 As (Freeman et al., 2006) point out, these insights are not easy to come by: These rules are based on first author Dr. Andrew Freemans experience with reading and translating Arabic language texts for more than 16 years (Freeman et al., 2006, p. 474).
92:218	3 For the MALINE row in Table 1, the ALINE documentation explains the notation as follows: every phonetic symbol is represented by a single lowercase letter followed by zero or more uppercase letters.
93:218	The initial lowercase letter is the base letter most similar to the sound represented by the phonetic symbol.
94:218	The remaining uppercase letters stand for the feature modon the s (expressed as sV), so that we get the sound corresponding to sh; the Pinyin name inserts a centered e vowel, and devoices the bilabial plosive /b/ to /p/.
95:218	There are actually sixteen different Chinese pinyinizations of Ashburton, according to our data prepared from the LDC corpus.
96:218	3 Experimental Setup 3.1 Machine Learning Framework Neither of the two basic approaches described so far use machine learning.
97:218	Our machine learning framework is based on learning from alignments produced by either approach.
98:218	To view the learning problem as one amenable to a statistical classifier, we need to generate labeled feature vectors so that each feature vector includes an additional class feature that can have the value true or false. Given a set of such labeled feature vectors as training data, the classifier builds a model which is then used to classify unlabeled feature vectors with the right labels.
99:218	A given set of attested name pairs constitutes a set of positive examples.
100:218	To create negative pairs, we have found that randomly selecting elements that havent been paired will create negative examples in which the pairs of elements being compared are so different that they can be trivially separated from the positive examples.
101:218	The experiments reported here used the MLEV score as a threshold to select negatives, so that examples below the threshold are excluded.
102:218	As the threshold is raised, the negative examples should become harder to discriminate from positives (with the harder problems mirroring some of the confusable name characteristics of the real-world name-matching problems this technology is aimed at).
103:218	Positive examples below the threshold are also eliminated.
104:218	Other criteria, including a MALINE score, could be used, but the MLEV scores seemed adequate for these preliminary experiments.
105:218	Raising the threshold reduces the number of negative examples.
106:218	It is highly desirable to balance the number of positive and negative examples in training, to avoid the learning being  ifiers which alter the sound defined by the base letter.
107:218	By default, the output contains the alignments together with the overall similarity scores.
108:218	The aligned subsequences are delimited by '|' signs.
109:218	The '<' sign signifies that the previous phonetic segment has been aligned with two segments in the other sequence, a case of compression/expansion.
110:218	The '-' sign denotes a skip, a case of insertion/deletion. 4 biased by a skewed distribution.
111:218	However, when one starts with a balanced distribution of positive and negatives, and then excludes a number of negative examples below the threshold, a corresponding number of positive examples must also be removed to preserve the balance.
112:218	Thus, raising the threshold reduces the size of the training data.
113:218	Machine learning algorithms, however, can benefit from more training data.
114:218	Therefore, in the experiments below, thresholds which provided woefully inadequate training set sizes were eliminated.
115:218	One can think of both the machine learning method and the basic name comparison methods (MLEV and MALINE) as taking each pair of names with a known label and returning a system-assigned class for that pair.
116:218	Precision, Recall, and F-Measure can be defined in an identical manner for both machine learning and basic name comparison methods.
117:218	In such a scheme, a threshold on the similarity score is used to determine whether the basic comparison match is a positive match or not.
118:218	Learning the best threshold for a dataset can be determined by searching over different values for the threshold.
119:218	In short, the methodology employed for this study involves two types of thresholds: the MLEV threshold used to identify negative examples and the threshold that is applied to the basic comparison methods, MLEV and MALINE, to identify matches.
120:218	To avoid confusion, the term negative threshold refers to the former, while the term positive threshold is used for the latter.
121:218	The basic comparison methods were used as baselines in this research.
122:218	To be able to provide a fair basic comparison score at each negative threshold, we trained each basic comparison matcher at twenty different positive thresholds on the same training set used by the learner.
123:218	For each negative threshold, we picked the positive threshold that gave the best performance on the training data, and used that to score the matcher on the same test data as used by the learner.
124:218	3.2 Feature Extraction Consider the MLEV alignment in Table 1.
125:218	It can be seen that the first three characters are matched identically across both strings; after that, we get an e inserted, an n inserted, a b matched identically, a u matched to an o, a r deleted, a t matched to a d, an o matched to a u, and an n deleted.
126:218	The match unigrams are thus a:a, s:s, h:h, -:e, -:n, b:b, u:o, r:-, t:d, o:u, and n:-.
127:218	Match bigrams were generated by considering any insertion, deletion, and (non-identical) substitution unigram, and noting the unigram, if any, to its left, prepending that left unigram to it (delimited by a comma).
128:218	Thus, the match bigrams in the above example include h:h,-:e, -:e,-:n, b:b,u:o, u:o,r:-, r:-,t:d, t:d,o:u, o:u,n:-.
129:218	These match unigram and match bigram features are generated from just a single MLEV match.
130:218	The composite feature set is the union of the complete match unigram and bigram feature sets.
131:218	Given the composite feature set, each match pair is turned into a feature vector consisting of the following features: string1, string2, the match score according to each of the basic comparison matchers (MLEV and MALINE), and the Boolean value of each feature in the composite feature set.
132:218	3.3 Data Set Our data is a (roughly 470,000 pair) subset of the Chinese-English personal name pairs in LDC 2005T34.
133:218	About 150,000 of the pairs had more than 1 way to pronounce the English and/or Chinese.
134:218	For these, to keep the size of the experiments manageable from the point of view of training the learners, one pronunciation was randomly chosen as the one to use.
135:218	(Even with this restriction, a minimum negative threshold results in over half a million examples).
136:218	Chinese characters were mapped into Hanyu Pinyin representations, which are used for MLEV alignment and string comparisons.
137:218	Since the input to MALINE uses a phonemic representation that encodes phonemic features in one or more letters, both Pinyin and English forms were mapped into the MALINE notation.
138:218	There are a number of slightly varying ways to map Pinyin into an international pronunciation system like IPA.
139:218	For example, (Wikipedia 2006) and (Salafra 2006) have mappings that differ from each other and also each of these two sources have changed its mapping over time.
140:218	We used a version of Salafra from 2006 (but we ignored the ejectives).
141:218	For English, the CMU pronouncing dictionary (CMU 2008) provided phonemic representations that were then mapped into the MALINE notation.
142:218	The dictionary had entries for 12% of our data set.
143:218	For the names not in the CMU dictionary, a simple grapheme to phoneme script provided an approximate phonemic form.
144:218	We did not use a monolingual mapping of Chinese characters (Mandarin pronunciation) into IPA because we did not find any.
145:218	5 60 65 70 75 80 85 90 95 100 105 0 0.2 0.4 0.6 0.8 M X C MB XB CB Note that we could insist that all pairs in our dataset be distinct, requiring that there be exactly one match for each Roman name and exactly one match for each Pinyin name.
146:218	This in our view is unrealistic, since large corpora will be skewed towards names which tend to occur frequently (e.g., international figures in news) and occur with multiple translations.
147:218	We included attested match pairs in our test corpora, regardless of the number of matches that were associated with a member of the pair.
148:218	4 Results A variety of machine learning algorithms were tested.
149:218	Results are reported, unless otherwise indicated, using SVM Lite, a Support Vector Machine (SVM 4 ) classifier 5  that scales well to large data sets.
150:218	Testing with SVM Lite was done with a 90/10 train-test split.
151:218	Further testing was carried out with the weka SMO SVM classifier, which used built-in cross-validation.
152:218	Although the latter classifier didnt scale to the larger data sets we used, it did show that cross-validation didnt change the basic results for the data sets it was tried on.
153:218	4.1 Machine Learning with Different Feature Sets Figure 1:  F-measure with Different Feature Sets Figure 1 shows the F-measure of learning for monolingual features (M, based on MALINE), cross-lingual features (X, based on MLEV), and a combined feature set (C) of both types of features 6  at different negative thresholds (shown on the horizontal axis).
154:218	Baselines are shown with the suffix B, e.g., the basic MALINE without learning is MB.
155:218	When using both monolingual and cross-lingual features (C), the baseline (CB)   4 We used a linear kernel function in our SVM experiments; using polynomial or radial basis kernels did not improve performance.
156:218	5  From svmlight.joachims.org.
157:218	6 In Figure 1, the X curve is more or less under the C curve.
158:218	is set to a system response of true only when both the MALINE and MLEV baseline systems by themselves respond true.
159:218	Table 2 shows the number of examples at each negative threshold and the Precision and Recall for these methods, along with baselines using the basic methods shown in square brackets.
160:218	The results show that the learning method (i) outperforms the baselines (basic methods), and (ii) the gap between learning and basic comparison widens as the problem becomes harder (i.e., as the threshold is raised).
161:218	For separate monolingual and cross-lingual learning, the increase in accuracy of the learning over the baseline (non-learning) results 7  was statistically significant at all negative thresholds except 0.6 and 0.7.
162:218	For learning with combined monolingual and cross-lingual features (C), the increase over the baseline (non-learning) combined results was statistically significant at all negative thresholds except for 0.7.
163:218	In comparing the mono-lingual and crosslingual learning approaches, however, the only statistically significant differences were that the cross-lingual features were more accurate than the monolingual features at the 0 to 0.4 negative thresholds.
164:218	This suggests that (iii) the monolingual learning approach is as viable as the cross-lingual one as the problem of confusable names becomes harder.
165:218	However, using the combined learning approach (C) is better than using either one.
166:218	Learning accuracy with both monolingual and crosslingual features is statistically significantly better than learning with monolingual features at the 0.0 to 0.4 negative thresholds, and better than learning with cross-lingual features at the 0.0 to 0.2, and 0.4 negative thresholds.
167:218	7 Statistical significance between F-measures is not directly computable since the overall F-measure is not an average of the F-measures of the data samples.
168:218	Instead, we checked the statistical significance of the increase in accuracy (accuracy is not shown for reasons of space) due to learning over the baseline.
169:218	The statistical significance test was done by assuming that the accuracy scores were binomials that were approximately Gaussian.
170:218	When the Gaussian approximation assumption failed (due to the binomial being too skewed), a looser, more general bound was used (Chebyshevs inequality, which applies to all probability distributions).
171:218	All statistically significant differences are at the 1% level (2-sided).
172:218	6 4.2 Feature Set Analyses The unigram features reflect common correspondences between Chinese and English pronunciation.
173:218	For example, (Sproat, Tao and Zhai 2006) note that Chinese /l/ is often associated with English /r/, and the feature l:r is among the most frequent unigram mappings in both the MLEV and MALINE alignments.
174:218	At a frequency of 103,361, it is the most frequent unigram feature in the MLEV mappings, and it is the third most frequent unigram feature in the MALINE alignments (56,780).
175:218	Systematic correspondences among plosives are also captured in the MALINE unigram mappings.
176:218	The unaspirated voiceless Chinese plosives /p,t,k/ contrast with aspirated plosives /p ,t ,k /, whereas the English voiceless plosives (which are aspirated in predictable environments) contrast with voiced plosives /b,d,g/.
177:218	As a result, English /b,d,g/ phonemes are usually transliterated using Chinese characters that are pronounced /p,t,k/, while English /p,t,k/ phonemes usually correspond to Chinese /p ,t ,k /.
178:218	The examples of Stewart and Elizabeth in Section 1 illustrate the correspondence of English /t/ and Chinese / t / and of English /b/ with Chinese /p/ respectively.
179:218	All six of the unigram features that result from these correspondences occur among the 20 most frequent in the MALINE alignments, ranging in frequency from 23,602 to 53,535.
180:218	Negative Threshold Examples Monolingual  (M) Cross-Lingual (X) Combined (C)   P R P R P R 0 538,621 94.69 [90.6] 95.73 [91.0] 96.5 [90.0] 97.15 [93.4] 97.13 [90.8] 97.65 [91.0] 0.1 307,066 95.28 [87.1] 96.23 [83.4] 98.06 [89.2] 98.25 [89.9] 98.4 [87.6] 98.64 [84.1] 0.2 282,214 95.82 [86.2] 96.63 [84.4] 97.91 [88.4] 98.41 [90.3] 98.26 [86.7] 98.82 [84.7] 0.3 183,188 95.79 [80.6] 96.92 [85.3] 98.18 [86.3] 98.8 [90.7] 98.24 [80.6] 99.27 [84.8] 0.4 72,176 96.31 [77.1] 98.69 [82.3] 97.89 [91.8] 99.61 [86.2] 98.91 [77.1] 99.64 [80.9] 0.5 17,914 94.62 [64.6] 98.63 [84.3] 99.44 [89.4] 100.0 [91.9] 99.46 [63.8] 99.89 [84.7] 0.6 2,954 94.94 [66.1] 100 [77.0] 98.0 [85.2] 98.66 [92.8] 99.37 [61.3] 100.0 [73.1] 0.7 362 95.24 [52.8] 100 [100.0] 94.74 [78.9] 100.0 [78.9] 100.0 [47.2] 94.74 [100.0] Table 2:  Precision and Recall with Different Feature Sets (Baseline scores in square brackets)  4.3 Comparison with other Learners To compare with other machine learning tools, we used the WEKA toolkit (from www.weka.net.nz).
181:218	Table 3 shows the comparisons on the MLEV data for a fixed size at one threshold.
182:218	Except for SVM Light, the results are based on 10-fold cross validation.
183:218	The other classifiers appear to perform relatively worse at that setting for the MLEV data, but the differences in accuracy are not statistically significant even at the 5% level.
184:218	A large contributor to the lack of significance is the small test set size of 66 pairs (10% of 660 examples) used in the SVM Light test.
185:218	4.4 Other Language Pairs Some earlier experiments for Arabic-Roman comparisons were carried out using a Conditional Random Field learner (CRF), using the Carafe toolkit (from sourceforge.net/projects/carafe).
186:218	The method computes its own Levenshtein edit-distance scores, and learns edit-distance costs from that.
187:218	The scores obtained, on average, had only a .6 correlation with the basic comparison Levenshtein scores.
188:218	However, these experiments did not return accuracy results, as ground-truth data was not specified for this task.
189:218	7 Several preliminary machine learning experiments were also carried out on Urdu-Roman comparisons.
190:218	The data used were Urdu data extracted from a parallel corpus recently produced by the LDC (LCTL_Urdu.20060408).
191:218	The results are shown in Table 4.
192:218	Here a .55 MALINE score and a .85 MLEV score were used for selecting positive examples by basic comparison, and negative examples were selected at random.
193:218	Here the MALINE method (row 1) using the weka SMO SVM made use of a threshold based on a MALINE score.
194:218	In these earlier experiments, machine learning does not really improve the system performance (F-measure decreases with learning on one test and only increases by 0.1% on the other test).
195:218	However, since these earlier experiments did not benefit from the use of different negative thresholds, there was no control over problem difficulty.
196:218	5 Related Work While there is a substantial literature employing learning techniques for record linkage based on the theory developed by Fellegi and Sunter (1969), researchers have only recently developed applications that focus on name strings and that employ methods which do not require features to be independent (Cohen and Richman 2002).
197:218	Ristad and Yianilos (1997) have developed a generative model for learning string-edit distance that learns the cost of different edit operations during string alignment.
198:218	Bilenko and Mooney (2003) extend Ristads approach to include gap penalties (where the gaps are contiguous sequences of mismatched characters) and compare this generative approach with a vector similarity approach that doesnt carry out alignment.
199:218	McCallum et al.200:218	(2005) use Conditional Random Fields (CRFs) to learn edit costs, arguing in favor of discriminative training approaches and against generative approaches, based in part on the fact that the latter approaches cannot benefit from negative evidence from pairs of strings that (while partially overlapping) should be considered dissimilar.
201:218	Such CRFs model the conditional probability of a label sequence (an alignment of two strings) given a sequence of observations (the strings).
202:218	A related thread of research is work on automatic transliteration, where training sets are typically used to compute probabilities for mappings in weighted finite state transducers (Al-Onaizan and Knight 2002; Gao et al. 2004) or source-channel models (Knight and Graehl 1997; Li et al. 2004).
203:218	(Sproat et al. 2006) have compared names from comparable and contemporaneous English and Chinese texts, scoring matches by training a learning algorithm to compare the phonemic representations of the names in the pair, in addition to taking into account the frequency distribution of the pair over time.
204:218	(Tao et al. 2006) obtain similar results using frequency and a similarity score based on a phonetic cost matrix The above approaches have all developed special-purpose machine-learning architectures to address the matching of string sequences.
205:218	They take pairs of strings that havent been aligned, and learn costs or mappings from them, and once trained, search for the best match given the learned representation  Positive Threshold Examples Method P R F Accuracy .65 660 SVM Light 90.62 87.88 89.22 89.39 .65 660 WEKA SMO 80.6 83.3 81.92 81.66 .65 660 AdaBoost M1 84.9 78.5 81.57 82.27 Table 3: Comparison of Different Classifiers  Method Positive Threshold Examples P R F WEKA SMO .55 (MALINE) 206 (MALINE) 84.8 [81.5] 86.4 [93.3] 85.6 [87.0] WEKA SMO .85 (MLEV) 584 (MLEV) 89.9 [93.2] 94.7 [91.2] 92.3 [92.2] Table 4: Urdu-Roman Name Matching Results with Random Negatives (Baseline scores in square brackets)  8 Our approach, by contrast, takes pairs of strings along with an alignment, and using features derived from the alignments, trains a learner to derive the best match given the features.
206:218	This offers the advantage of modularity, in that any type of alignment model can be combined with SVMs or other classifiers (we have preferred SVMs since they offer discriminative training).
207:218	Our approach allows leveraging of any existing alignments, which can lead to starting the learning from a higher baseline and less training data to get to the same level of performance.
208:218	Since the learner itself doesnt compute the alignments, the disadvantage of our approach is the need to engineer features that communicate important aspects of the alignment to the learner.
209:218	In addition, our approach, as with McCallum et al.210:218	(2005), allows one to take advantage of both positive and negative training examples, rather than positive ones alone.
211:218	Our data generation strategy has the advantage of generating negative examples so as to vary the difficulty of the problem, allowing for more fine-grained performance measures.
212:218	Metrics based on such a control are likely to be useful in understanding how well a name-matching system will work in particular applications, especially those involving confusable names.
213:218	6 Conclusion The work presented here has established a framework for application of machine learning techniques to multilingual name matching.
214:218	The results show that machine learning dramatically outperforms basic comparison methods, with Fmeasures as high as 97.0 on the most difficult problems.
215:218	This approach is being embedded in a larger system that matches full names using a vetted database of full-name matches for evaluation.
216:218	So far, we have confined ourselves to minimal feature engineering.
217:218	Future work will investigate a more abstract set of phonemic features.
218:218	We also hope to leverage ongoing work on harvesting name pairs from web resources, in addition applying them to less commonly taught languages, as and when appropriate resources for them become available.

