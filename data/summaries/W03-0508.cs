Largescale conferences like SUMMAC (Mani et al., 1999) and DUC (2002) have unfortunately shown weak results in that current evaluation measures could not distinguish between automatic summaries { though they are efiective enough to distinguish them from human-written summaries.
E99-1011
The factoid approach can capture much flner shades of meaning difierentiations than DUC-style information overlap does { in an example from Lin and Hovy (2002), an assessor judged some content overlap between \Thousands of people are feared dead" and \3,000 and perhaps...
W02-0406
However, Lin and Hovy (2002) report low agreement for two tasks: producing the human summaries (around 40%), and assigning information overlap between them.
W02-0406
Lin and Hovy (2002) examine the use of a multiple gold standard for summarisation evaluation, and conclude \we need more than one model summary although we cannot estimate how many model summaries are required to achieve reliable automated summary evaluation".
W02-0406
In principle, the comparison can be done via coselection of extracted sentences (Rath et al., 1961; Jing et al., 1998; Zechner, 1996), by string-based surface measures (Lin and Hovy, 2002; Saggion et al., 2002), or by subjective judgements of the amount of information overlap (DUC, 2002).
C96-2166 W02-0406
In summarisation there appears to be no \one truth", as is evidenced by a low agreement between humans in producing gold standard summaries by sentence selection (Rath et al., 1961; Jing et al., 1998; Zechner, 1996), and low overlap measures between humans when gold standards summaries are created by reformulation in the summarisers’ own words (e.g.
C96-2166
In the past years, there has been quite a lot of summarisation work that has efiectively aimed at flnding viable evaluation strategies (Sp˜arck Jones, 1999; Jing et al., 1998; Donaway et al., 2000).
W00-0408
