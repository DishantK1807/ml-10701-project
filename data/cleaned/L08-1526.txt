<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>O Rambow</author>
<author>S Whittaker</author>
</authors>
<title>Evaluation metrics for generation</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st International Natural Language Generation Conference (INLG-2000</booktitle>
<pages>1--8</pages>
<location>Mitzpe Ramon, Israel</location>
<contexts>
<context>controlled, in many contexts there are simply too many valid ways of expressing the same content for a gold standard to be helpful. The use of a reference corpus, rather than a single reference text (Bangalore et al., 2000), addresses this problem of excessive stricture, although, as a result, the evaluator has less control over the quality and authorship of the text, and this may mean that the reference text contains </context>
<context>ere assembled was, as a necessity, re-used to run the test, and so the model itself did not fall under scrutiny. Hartley and Scott (2001) describe in detail how the Generation String Accuracy metric (Bangalore et al., 2000) was used to test the model through which the AGILE system expressed the desired content, by porting the metric so that it measured the correspondence between the desired model and the model produced</context>
</contexts>
<marker>Bangalore, Rambow, Whittaker, 2000</marker>
<rawString>S. Bangalore, O. Rambow, and S. Whittaker. 2000. Evaluation metrics for generation. In Proceedings of the 1st International Natural Language Generation Conference (INLG-2000), pages 1–8, Mitzpe Ramon, Israel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>E Reiter</author>
</authors>
<title>Comparing Automatic and Human Evaluation of NLG Systems</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the ACL (EACL-2006</booktitle>
<pages>313--320</pages>
<location>Trento, Italy</location>
<marker>Belz, Reiter, 2006</marker>
<rawString>A. Belz and E. Reiter. 2006. Comparing Automatic and Human Evaluation of NLG Systems. In Proceedings of the 11th Conference of the European Chapter of the ACL (EACL-2006), pages 313–320, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Semantic Distance in WordNet: An Experimental, Application-oriented Evaluation of Five Measures</title>
<date>2001</date>
<booktitle>In Proceedings of the Workshop on WordNet and Other Lexical Resources, Second Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-2001</booktitle>
<pages>29--34</pages>
<location>Pittsburgh, PA, USA</location>
<marker>Budanitsky, Hirst, 2001</marker>
<rawString>A. Budanitsky and G. Hirst. 2001. Semantic Distance in WordNet: An Experimental, Application-oriented Evaluation of Five Measures. In Proceedings of the Workshop on WordNet and Other Lexical Resources, Second Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-2001), pages 29–34, Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>M Osborne</author>
<author>P Koehn</author>
</authors>
<title>Reevaluating the Role of BLEU in Machine Translation Research</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the ACL (EACL-2006</booktitle>
<pages>249--256</pages>
<location>Trento, Italy</location>
<contexts>
<context>igned specifically for NLG and high quality reference texts are likely to be required. There is some controversy over the use of such metrics to measure quality, see for example Callison-Burch et al (Callison-Burch et al., 2006), but, in our view, the most significant challenge in using such metrics for evaluating NLG is the task of identifying appropriate reference texts to serve as gold standards – see also Scott and Moor</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Reevaluating the Role of BLEU in Machine Translation Research. In Proceedings of the 11th Conference of the European Chapter of the ACL (EACL-2006), pages 249– 256, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>C Mellish</author>
</authors>
<title>Towards Evaluation in Natural Language Generation</title>
<date>1998</date>
<booktitle>In Proceedings of the 1st International Conference on Language Resources and Evaluation (LREC-1998</booktitle>
<pages>555--562</pages>
<location>Granada, Spain</location>
<marker>Dale, Mellish, 1998</marker>
<rawString>R. Dale and C. Mellish. 1998. Towards Evaluation in Natural Language Generation. In Proceedings of the 1st International Conference on Language Resources and Evaluation (LREC-1998), pages 555–562, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
</authors>
<title>Matador: A Large-Scale SpanishEnglish GHMT System</title>
<date>2003</date>
<booktitle>In Proceedings of the 9th Machine Translation Summit (MT Summit IX</booktitle>
<pages>149--156</pages>
<location>New Orleans, USA</location>
<contexts>
<context>uation of the subjective notion of text quality. 1. Introduction Evaluations of NLG systems focus on a wide range of textual features, such as readability (Williams and Reiter, 2005), grammaticality (Habash, 2003), fluency (Mutton et al., 2007), and fidelity (Hartley and Scott, 2001), to cite just a small sample. While all of these measures are useful aids to development, none individually characterises text </context>
</contexts>
<marker>Habash, 2003</marker>
<rawString>N. Habash. 2003. Matador: A Large-Scale SpanishEnglish GHMT System. In Proceedings of the 9th Machine Translation Summit (MT Summit IX), pages 149– 156, New Orleans, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hallett</author>
<author>D Scott</author>
<author>R Power</author>
</authors>
<title>Composing queries through conceptual authoring</title>
<date>2007</date>
<journal>Computational Linguistics</journal>
<volume>33</volume>
<marker>Hallett, Scott, Power, 2007</marker>
<rawString>C. Hallett, D. Scott, and R. Power. 2007. Composing queries through conceptual authoring. Computational Linguistics, 33(1):105–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hardcastle</author>
</authors>
<title>Riddle posed by computer (6): The Computer Generation of Cryptic Crossword Clues</title>
<date>2007</date>
<tech>PhD thesis</tech>
<institution>University of London</institution>
<contexts>
<context>ppraisal of the features of a specific text in some context, and one that is more than just the sum of its parts. In this paper we relate the evaluation of a case study NLG application called ENIGMA (Hardcastle, 2007, 228-265) which automatically generates cryptic crossword clues. These are short texts, not dissimilar to newspaper headlines, which typically consist of a single clause and some ellipsis. Each clue </context>
</contexts>
<marker>Hardcastle, 2007</marker>
<rawString>D. Hardcastle. 2007. Riddle posed by computer (6): The Computer Generation of Cryptic Crossword Clues. PhD thesis, University of London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hartley</author>
<author>D Scott</author>
</authors>
<title>Evaluating text quality: judging output texts without a clear source</title>
<date>2001</date>
<booktitle>In Proceedings of the 8th European Workshop on Natural Language Generation (EWNLG-01</booktitle>
<pages>111--115</pages>
<location>Toulouse, France</location>
<contexts>
<context>tion Evaluations of NLG systems focus on a wide range of textual features, such as readability (Williams and Reiter, 2005), grammaticality (Habash, 2003), fluency (Mutton et al., 2007), and fidelity (Hartley and Scott, 2001), to cite just a small sample. While all of these measures are useful aids to development, none individually characterises text quality. As Dale and Mellish (1998, 3) point out “there is no agreed ob</context>
<context>as grammaticality (Mutton et al., 2007), is used to rate the intelligibility of the text, the text must also be analyzed to ensure fidelity – namely that the text “says what it [is] supposed to say” (Hartley and Scott, 2001, 1). The experiment conducted to check the fidelity of the clues generated by ENIGMA was rather circular, since the same model used to generate the plans from which the clues were assembled was, as a</context>
</contexts>
<marker>Hartley, Scott, 2001</marker>
<rawString>A. Hartley and D. Scott. 2001. Evaluating text quality: judging output texts without a clear source. In Proceedings of the 8th European Workshop on Natural Language Generation (EWNLG-01), pages 111–115, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparck Jones</author>
<author>J R Galliers</author>
</authors>
<title>Evaluating Natural Language Processing Systems: An Analysis and Review</title>
<date>1996</date>
<publisher>Springer-Verlag</publisher>
<location>New York</location>
<contexts>
<context>h this in mind we propose that the Turing-style test described in Section 2.5. may offer a way forward in the evaluation of text quality in NLG systems. The test has the benefit of an extrinsic test (Jones and Galliers, 1996, 19) in that the human subjects are forced to make decisions that can be measured and aggregated, but the evaluator does not need to find a task that fits the application, since the task is the same </context>
</contexts>
<marker>Jones, Galliers, 1996</marker>
<rawString>K. Sparck Jones and J. R. Galliers. 1996. Evaluating Natural Language Processing Systems: An Analysis and Review. Springer-Verlag New York Inc., New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL-03</booktitle>
<pages>423--430</pages>
<location>Morristown, New Jersey, USA</location>
<contexts>
<context>hecking in place. As a point of comparison a sample of 150 humanauthored clues taken from the Independent newspaper were also analysed. The parser used was a statistical parser – the Stanford parser (Klein and Manning, 2003) – and so a parse was returned for all clues in the experiment, including those with no syntactic constraints. The test involved checking whether the tags assigned by the parser matched the tags assi</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL-03), pages 423–430, Morristown, New Jersey, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>I Chander</author>
</authors>
<title>Automated postediting of documents</title>
<date>1994</date>
<booktitle>In Proceedings of the 12th National Conference on Artificial Intelligence</booktitle>
<pages>779--784</pages>
<location>Seattle, USA</location>
<marker>Knight, Chander, 1994</marker>
<rawString>K. Knight and I. Chander. 1994. Automated postediting of documents. In Proceedings of the 12th National Conference on Artificial Intelligence, pages 779–784, Seattle, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S Macnutt</author>
</authors>
<title>1966]. Ximenes on the Art of the Crossword</title>
<date>2001</date>
<publisher>Swallowtail Books, Claverly, UK</publisher>
<contexts>
<context>standard comparison with metrics was simply not viable. Figure 2 shows three cryptic clues for the word DAINTILY by way of example. Clue 1 is taken from a manual for cryptic crossword clue compilers (Macnutt, 2001) written by Ximenes, one of the best-known compilers in the UK – arguably a good candidate as a reference text – and it realizes a wordplay puzzle in which the word daily, defined as char, is written</context>
</contexts>
<marker>Macnutt, 2001</marker>
<rawString>D. S. Macnutt. 2001 [1966]. Ximenes on the Art of the Crossword. Swallowtail Books, Claverly, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mutton</author>
<author>M Dras</author>
<author>S Wan</author>
<author>R Dale</author>
</authors>
<title>GLEU: Automatic evaluation of sentence-level fluency</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL-07</booktitle>
<pages>344--351</pages>
<location>Prague, Czech Republic</location>
<contexts>
<context> notion of text quality. 1. Introduction Evaluations of NLG systems focus on a wide range of textual features, such as readability (Williams and Reiter, 2005), grammaticality (Habash, 2003), fluency (Mutton et al., 2007), and fidelity (Hartley and Scott, 2001), to cite just a small sample. While all of these measures are useful aids to development, none individually characterises text quality. As Dale and Mellish (1</context>
<context> the correct sub-corpus, the success or failure of this test only tells us part of the story on text quality. 4.2. Other Metrics Whether a reference text or some other metric, such as grammaticality (Mutton et al., 2007), is used to rate the intelligibility of the text, the text must also be analyzed to ensure fidelity – namely that the text “says what it [is] supposed to say” (Hartley and Scott, 2001, 1). The exper</context>
</contexts>
<marker>Mutton, Dras, Wan, Dale, 2007</marker>
<rawString>A. Mutton, M. Dras, S. Wan, and R. Dale. 2007. GLEU: Automatic evaluation of sentence-level fluency. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL-07), pages 344–351, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>T Ward S Roukos</author>
<author>W-J Zhu</author>
</authors>
<title>Bleu: A method for automatic evaluation of machine translation</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association of Computational Linguistics (ACL02</booktitle>
<pages>311--318</pages>
<location>Philadelphia, USA</location>
<marker>Papineni, Roukos, Zhu, 2002</marker>
<rawString>K. Papineni, S.Roukos, T. Ward, and W-J. Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association of Computational Linguistics (ACL02), pages 311–318, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>S Sripada</author>
</authors>
<title>Should Corpora Texts be Gold Standards for NLG</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Natural Language Generation Conference (INLG-2002</booktitle>
<pages>97--104</pages>
<location>New York, USA</location>
<contexts>
<context>lem of excessive stricture, although, as a result, the evaluator has less control over the quality and authorship of the text, and this may mean that the reference text contains undesirable features (Reiter and Sripada, 2002). Furthermore, although a corpus-based comparison can tell us that the output text has the right ‘feel’ to it, because we are essentially able to show that we can re-classify (Sebastiani, 2002) our o</context>
</contexts>
<marker>Reiter, Sripada, 2002</marker>
<rawString>E. Reiter and S. Sripada. 2002. Should Corpora Texts be Gold Standards for NLG? In Proceedings of the 2nd International Natural Language Generation Conference (INLG-2002), pages 97–104, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>R Robinson</author>
<author>A S Lennox</author>
<author>L Osman</author>
</authors>
<title>Using a randomised controlled clinical trial to evaluate an NLG system</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association of Computational Linguistics (ACL-01</booktitle>
<pages>442--449</pages>
<location>Toulouse, France</location>
<contexts>
<context>ith it, and even when it does it may be hard to disentangle the contribution of the quality of the text to the success or failure of the task. For example, the failure of the clinical trials of STOP (Reiter et al., 2001) might have resulted from a problem with the clinical assumptions behind them, rather than the quality of the texts generated by the system, and even if performance in the task is traced back to the </context>
</contexts>
<marker>Reiter, Robinson, Lennox, Osman, 2001</marker>
<rawString>E. Reiter, R. Robinson, A. S.Lennox, and L. Osman. 2001. Using a randomised controlled clinical trial to evaluate an NLG system. In Proceedings of the 39th Annual Meeting of the Association of Computational Linguistics (ACL-01), pages 442–449, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Scott</author>
<author>J Moore</author>
</authors>
<title>An NLG evaluation competition? Eight reasons to be cautious</title>
<date>2006</date>
<tech>Technical Report 2006/09</tech>
<institution>The Open University</institution>
<location>Milton Keynes, UK</location>
<marker>Scott, Moore, 2006</marker>
<rawString>D. Scott and J. Moore. 2006. An NLG evaluation competition? Eight reasons to be cautious. Technical Report 2006/09, The Open University, Milton Keynes, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization</title>
<date>2002</date>
<journal>ACM Computing Surveys</journal>
<volume>34</volume>
<contexts>
<context>eiter and Sripada, 2002). Furthermore, although a corpus-based comparison can tell us that the output text has the right ‘feel’ to it, because we are essentially able to show that we can re-classify (Sebastiani, 2002) our output into the correct sub-corpus, the success or failure of this test only tells us part of the story on text quality. 4.2. Other Metrics Whether a reference text or some other metric, such as</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>F. Sebastiani. 2002. Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sripada</author>
<author>E Reiter</author>
<author>L Hawizy</author>
</authors>
<title>Evaluation of an NLG System using Post-Edit Data: Lessons Learned</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th European Workshop on Natural Language Generation (EWNLG-05</booktitle>
<pages>133--139</pages>
<location>Helsinki, Finland</location>
<marker>Sripada, Reiter, Hawizy, 2005</marker>
<rawString>S. Sripada, E. Reiter, and L. Hawizy. 2005. Evaluation of an NLG System using Post-Edit Data: Lessons Learned. In Proceedings of the 10th European Workshop on Natural Language Generation (EWNLG-05), pages 133–139, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Turing</author>
</authors>
<title>Computing machinery and intelligence</title>
<date>1950</date>
<tech>Mind</tech>
<contexts>
<context>orrectly identify as many of the human-authored clues as possible, and they were also asked to provide comments on how they had reached their decision before being shown the answers. The Turing test (Turing, 1950) was designed to address a particular problem in AI, namely that there was no agreed definition of what constituted intelligence. Rather than try to define it, the researcher involves the test subjec</context>
<context>yle Test The final experiment in the evaluation of the ENIGMA system provides a solution of sorts to the problems raised by the other four approaches. The experiment consisted of a Turing-style test (Turing, 1950) in which subjects were presented with 30 pairs of clues, with each pair cluing the same answer word, and were told to choose the human-authored clue for each pair. Participants were also asked to de</context>
</contexts>
<marker>Turing, 1950</marker>
<rawString>A. Turing. 1950. Computing machinery and intelligence. Mind, (49):433–460.</rawString>
</citation>
</citationList>
</algorithm>

