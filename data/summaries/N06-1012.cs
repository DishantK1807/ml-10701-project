The bagged CRF performs significantly better than a single CRF (McNemarâ€™s test; p < 0.01), and equals the results of (Ando and Zhang, 2005), who use a large amount of unlabeled data.
P05-1001
These are the logarithmic opinion pools that have been applied to CRFs by (Smith et al., 2005).
P05-1003
This is same idea behind logarithmic opinion pools, used by Smith, Cohn, and Osborne (2005) to reduce overfitting in CRFs.
P05-1003
These methods include the logarithmic opinion pools used by Smith et al.(2005). We evaluate feature bagging on linear-chain conditional random fields for two natural-language tasks.
P05-1003
The first bag also includes part-of-speech tags generated by the Brill tagger and the conjunctions of those tags used by Sha and Pereira (2003).
N03-1028
Of these four combination methods, Method 2, the persequence product of experts, is originally due to Smith et al.(2005). The other three combination methods are as far as we know novel.
P05-1003
94.34 (Sha and Pereira, 2003) 94.38 (Kudo and Matsumoto, 2001) 94.39 (Ando and Zhang, 2005) 94.70 Combined CRF 94.77 Table 3: Results for the CoNLL 2000 Chunking Task.
N01-1025 N03-1028 P05-1001
Discriminative methods for training probabilistic models have enjoyed wide popularity in natural language processing, such as in part-of-speech tagging (Toutanova et al., 2003), chunking (Sha and Pereira, 2003), namedentity recognition (Florian et al., 2003; Chieu and Ng, 2003), and most recently parsing (Taskar et al., 2004).
N03-1028 N03-1033 W03-0423 W03-0425 W04-3201
On the chunking task, the bagged model also outperforms the models of Kudo and Matsumoto (2001) and Sha and Pereira (2003), and equals the currently-best results of (Ando and Zhang, 2005), who use a large amount of unlabeled data.
N01-1025 N03-1028 P05-1001
This is lower than the best reported results for this data set, which is 89.3% (Ando and Zhang, 2005), using a large amount of unlabeled data.
P05-1001
Most closely related to the present work is that on logarithmic opinion pools for CRFs (Smith et al., 2005), which we have called per-sequence mixture of experts in this paper.
P05-1003
Concurrently and independently, Smith and Osborne (2006) present similar experiments on the CoNLL-2003 data set, examining a per-sequence mixture of experts (that is, a logarithmic opinion pool), in which the lexicon features are trained separately.
W06-2918
