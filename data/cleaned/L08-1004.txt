<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>E Scott Adler</author>
<author>John Wilkerson</author>
</authors>
<title>Intended consequences: Juridictionalreformandissuecontrolintheu.s. house of representatives</title>
<date>2008</date>
<journal>Legislative Studies Quarterly</journal>
<volume>33</volume>
<pages>112</pages>
<marker>Adler, Wilkerson, 2008</marker>
<rawString>E. Scott Adler and John Wilkerson. 2008. Intended consequences: Juridictionalreformandissuecontrolintheu.s. house of representatives. Legislative Studies Quarterly, 33(1):85 – 112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jones Baumgartner</author>
<author>Macleod</author>
</authors>
<title>Lessons from the trenches: Quality, reliability, and usability in a new data source. The Political Methodologist</title>
<date>1998</date>
<marker>Baumgartner, Macleod, 1998</marker>
<rawString>Baumgartner, Jones, and Macleod. 1998. Lessons from the trenches: Quality, reliability, and usability in a new data source. The Political Methodologist.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Baumgartner</author>
<author>B Jones</author>
<author>J Wilkerson</author>
</authors>
<title>Policy Dynamics, chapter 2</title>
<date>2002</date>
<publisher>University of Chicago Press</publisher>
<contexts>
<context> topics changes over time. This condition is the most dangerous for managing automatedlabelingreliabilityandtemporalconsistencybecause it can be difficult for people to identify (Soroka et al., 2006; Baumgartner et al., 2002). While the Policy Agendas annotation scheme is designed to capture the primary topics of legislation in one sense, specific programs come and go. The result can be a problem for machine learning sys</context>
</contexts>
<marker>Baumgartner, Jones, Wilkerson, 2002</marker>
<rawString>F. Baumgartner, B. Jones, and J. Wilkerson, 2002. Policy Dynamics, chapter 2. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Jun Wu</author>
</authors>
<title>Classifier combination for improved lexical disambiguation</title>
<date>1998</date>
<booktitle>In Proc. ACL</booktitle>
<pages>191--195</pages>
<contexts>
<context> and Hillard (2006) 6.2. Classifiers and Parameters Existing research indicates that combining the decisions of multiple statistical systems (a.k.a. ensemble learning) usually improves final results (Brill and Wu, 1998; Dietterich, 2000; Curran, 2002). For the ensembles, we employ three modeling approaches that are freely available to the research community: a Support Vector Machine (SVM), a Maximum Entropy classif</context>
</contexts>
<marker>Brill, Wu, 1998</marker>
<rawString>Eric Brill and Jun Wu. 1998. Classifier combination for improved lexical disambiguation. In Proc. ACL, pages 191–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Cardie</author>
<author>Cynthia Farina</author>
<author>Matt Rawding</author>
<author>Adil Aijaz</author>
<author>Stephen Purpura</author>
</authors>
<title>A Study in Rule-Specific Issue Categorization for e-Rulemaking</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th Annual International Conference on Digital Government Research</booktitle>
<marker>Cardie, Farina, Rawding, Aijaz, Purpura, 2008</marker>
<rawString>Claire Cardie, Cynthia Farina, Matt Rawding, Adil Aijaz, and Stephen Purpura. 2008. A Study in Rule-Specific Issue Categorization for e-Rulemaking. In Proceedings of the 9th Annual International Conference on Digital Government Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>Weighted Kappa: Nominal Scale Agreement with Provision for Scaled Disagreement or Partial Credit</title>
<date>1968</date>
<journal>Psychological Bulletin</journal>
<volume>70</volume>
<contexts>
<context>ustpasswithhigh inter-rater agreement. Many hours of annotation by trained graduate and undergraduate students have been invested in the project, with observed inter-rater agreement of Cohen’s Kappa (Cohen, 1968) approaching 0.9 at the major topic level and 0.8 at the subtopic level. During a decade of human annotation, temporal inconsistencies in the annotation process have been found(Baumgartner et al., 19</context>
</contexts>
<marker>Cohen, 1968</marker>
<rawString>J. Cohen. 1968. Weighted Kappa: Nominal Scale Agreement with Provision for Scaled Disagreement or Partial Credit. Psychological Bulletin, 70(4):213–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Curran</author>
</authors>
<title>Ensemble methods for automatic thesaurus extraction</title>
<date>2002</date>
<booktitle>Proc. Empirical Methods in Natural Language Processing</booktitle>
<pages>222--229</pages>
<contexts>
<context>nd Parameters Existing research indicates that combining the decisions of multiple statistical systems (a.k.a. ensemble learning) usually improves final results (Brill and Wu, 1998; Dietterich, 2000; Curran, 2002). For the ensembles, we employ three modeling approaches that are freely available to the research community: a Support Vector Machine (SVM), a Maximum Entropy classifier, and a boosting classifier. </context>
</contexts>
<marker>Curran, 2002</marker>
<rawString>J. Curran. 2002. Ensemble methods for automatic thesaurus extraction. Proc. Empirical Methods in Natural Language Processing, pages 222–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dietterich</author>
</authors>
<title>Ensemble methods in machine learning</title>
<date>2000</date>
<journal>Lecture Notes in Computer Science</journal>
<pages>1857--1</pages>
<contexts>
<context>6.2. Classifiers and Parameters Existing research indicates that combining the decisions of multiple statistical systems (a.k.a. ensemble learning) usually improves final results (Brill and Wu, 1998; Dietterich, 2000; Curran, 2002). For the ensembles, we employ three modeling approaches that are freely available to the research community: a Support Vector Machine (SVM), a Maximum Entropy classifier, and a boostin</context>
</contexts>
<marker>Dietterich, 2000</marker>
<rawString>T. Dietterich. 2000. Ensemble methods in machine learning. Lecture Notes in Computer Science, 1857:1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Dumais</author>
<author>Hao Chen</author>
</authors>
<title>Hierarchical classification of web content</title>
<date>2000</date>
<booktitle>In SIGIR ’00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval</booktitle>
<pages>256--263</pages>
<publisher>ACM</publisher>
<location>New York, NY, USA</location>
<marker>Dumais, Chen, 2000</marker>
<rawString>Susan Dumais and Hao Chen. 2000. Hierarchical classification of web content. In SIGIR ’00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 256–263, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hillard</author>
<author>S Purpura</author>
<author>J Wilkerson</author>
</authors>
<title>An active learning framework for classifying political text</title>
<date>2007</date>
<booktitle>In Midwest Political Science Association 65th Annual National Conference. Dustin</booktitle>
<contexts>
<context>review. In past research, we have verified that the use of the bill title as a proxy for full bill content is reasonable for the purposes of assigning a primary topic with inter-temporal reliability (Hillard et al., 2007). The annotation teams are supervised by four project directors and many annotation team members have worked on the project over the years. Each is trained using a six week training protocol that beg</context>
</contexts>
<marker>Hillard, Purpura, Wilkerson, 2007</marker>
<rawString>D. Hillard, S. Purpura, and J. Wilkerson. 2007. An active learning framework for classifying political text. In Midwest Political Science Association 65th Annual National Conference. Dustin Hillard, Stephen Purpura, and John Wilkerson.</rawString>
</citation>
<citation valid="true">
<title>Computer assisted topic classification for mixed methods social science research</title>
<date>2008</date>
<journal>Journal of Information Technology and Politics</journal>
<volume>4</volume>
<contexts>
<context>acMullen, 2003). Examples of test corpora include the TREC data sets, Reuters RCV1 (Rose et al., 2002) and, more recently, Claire Cardie, Cynthia Farina, Matt Rawding, Adil Aijaz, and Stephen Purpura (2008).1 Using these prior works as a guide, this work describes the creation of a test corpora which includes the titles of all bills introduced in the United States Congress during a 50 year period. Each </context>
<context>ve in the use of ontologies for the standardization of semantic web services might see the parallel between defining the semantics, or intended meaning, of a category across time. Adler and Wilkerson (2008) use the Congressional Bills Project database to study the impact of congressional reforms. To 3See Adler, E. Scott and John Wilkerson, Congressional Bills Project: 1947-1998, NSF 00880066 and 0088006</context>
<context>advocated by Koller and Sahami (1997). 6http://www.congressionalbills.org Unlikeotherresearch,suchasDumaisandChen(2000)and Claire Cardie, Cynthia Farina, Matt Rawding, Adil Aijaz, and Stephen Purpura (2008), which shows that flat classificationusuallyexceedstheperformanceofhierarchicalclassification, we note that hierarchical classification was chosen over flat classification after empirical testing dem</context>
<context>Cohen’s Kappa measure is presented in parentheses. This experiment benefits from a few key aspects of the corpus that are worth noting. As reported in Stephen Purpura, Claire Cardie, and Jesse Simons (2008), 120,927 records of the 375,517 records in the data set are near duplicates. The relatively large number of near duplicates is caused by systemic factors in the United States Congress. First, multipl</context>
<context> historical research mode because they annotate performsthepre-processingstepsdescribedaboveandisavailable for download from www.stephenpurpura.com. 8These results are also reported in Hillard et al. (2008) which discussesthegeneralproblemofconductingtemporallyconsistent mixed-method social science research with quantitative and qualitative requirements and information retrieval or extraction methods. i</context>
</contexts>
<marker>2008</marker>
<rawString>2008. Computer assisted topic classification for mixed methods social science research. Journal of Information Technology and Politics, 4(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features</title>
<date>1998</date>
<booktitle>In Proc. European Conference on Machine Learning</booktitle>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In Proc. European Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Koller</author>
<author>M Sahami</author>
</authors>
<title>Hierarchically classifying documents using very few words</title>
<date>1997</date>
<booktitle>Proc. Int. Conf. on Machine Learning</booktitle>
<pages>170--178</pages>
<marker>Koller, Sahami, 1997</marker>
<rawString>D. Koller and M. Sahami. 1997. Hierarchically classifying documents using very few words. Proc. Int. Conf. on Machine Learning, pages 170–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W John MacMullen</author>
</authors>
<title>Requirements definition and design criteria for test corpora in information science</title>
<date>2003</date>
<tech>Technical report</tech>
<institution>University of North Carolina at Chapel Hill</institution>
<contexts>
<context>mbating topic drift in temporally grounded on-line corpora. 2. Related Work For decades, language researchers and information scientists have constructed test corpora (Robertson and Walker, 1997) in (MacMullen, 2003). These collections usually consist of documents (titles, abstracts, or full-text articles), a set of standardized queries made by experts and relevance judgments (MacMullen, 2003). Examples of test </context>
</contexts>
<marker>MacMullen, 2003</marker>
<rawString>W. John MacMullen. 2003. Requirements definition and design criteria for test corpora in information science. Technical report, University of North Carolina at Chapel Hill, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
</authors>
<title>Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering. http://www.cs.cmu.edu/ mccallum/bow</title>
<date>1996</date>
<contexts>
<context>pport Vector Machine (SVM), a Maximum Entropy classifier, and a boosting classifier. For SVM classification, we use SVMlight(Joachims, 1998); we use the Bow toolkit for Maximum Entropy classification(McCallum, 1996); and the Boostexter tool for the AdaBoost.MHalgorithm(SchapireandSinger,2000). Inadditiontotheclassifiersusedintheensemble,wealsocompare performance of our systems against the performance of the Nai</context>
</contexts>
<marker>McCallum, 1996</marker>
<rawString>A. McCallum. 1996. Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering. http://www.cs.cmu.edu/ mccallum/bow.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
</authors>
<title>Why inverse document frequency</title>
<date>2001</date>
<booktitle>In ProceedingsoftheNorthAmericanAssociationforComputational Linguistics, NAACL</booktitle>
<pages>25--32</pages>
<contexts>
<context>hting strategies such as tf-idf (i.e. term frequency multiplied by inverse document frequency) have been shown to be generally effective, but specialized weighting schemes often provide improvements (Papineni, 2001). After empirical testing of various weighting schemes on the training data, this work adopts a term weighting strategy related to mutual information, which is the ratio of sentencebasedwordfrequency</context>
</contexts>
<marker>Papineni, 2001</marker>
<rawString>K. Papineni. 2001. Why inverse document frequency? In ProceedingsoftheNorthAmericanAssociationforComputational Linguistics, NAACL, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping</title>
<date>1980</date>
<journal>Program</journal>
<volume>14</volume>
<pages>137</pages>
<marker>Porter, 1980</marker>
<rawString>M. F. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130 – 137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Purpura</author>
<author>Dustin Hillard</author>
</authors>
<title>Automated Classification of Congressional Legislation</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Annual International Conference on Digital Government Research</booktitle>
<marker>Purpura, Hillard, 2006</marker>
<rawString>Stephen Purpura and Dustin Hillard. 2006. Automated Classification of Congressional Legislation. In Proceedings of the 7th Annual International Conference on Digital Government Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>S Walker</author>
</authors>
<title>Laboratory experiments with okapi: participation in the trec programme</title>
<date>1997</date>
<journal>Journal of Documentation, 53:20</journal>
<volume>34</volume>
<contexts>
<context>a semi-automated strategy for combating topic drift in temporally grounded on-line corpora. 2. Related Work For decades, language researchers and information scientists have constructed test corpora (Robertson and Walker, 1997) in (MacMullen, 2003). These collections usually consist of documents (titles, abstracts, or full-text articles), a set of standardized queries made by experts and relevance judgments (MacMullen, 200</context>
</contexts>
<marker>Robertson, Walker, 1997</marker>
<rawString>S.E. Robertson and S. Walker. 1997. Laboratory experiments with okapi: participation in the trec programme. Journal of Documentation, 53:20 – 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Rose</author>
<author>Mark Stevenson</author>
<author>Miles Whitehead</author>
</authors>
<title>The reuters corpus volume 1 from yesterday’s news to tomorrow’s language resources</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation</booktitle>
<contexts>
<context>(titles, abstracts, or full-text articles), a set of standardized queries made by experts and relevance judgments (MacMullen, 2003). Examples of test corpora include the TREC data sets, Reuters RCV1 (Rose et al., 2002) and, more recently, Claire Cardie, Cynthia Farina, Matt Rawding, Adil Aijaz, and Stephen Purpura (2008).1 Using these prior works as a guide, this work describes the creation of a test corpora which</context>
</contexts>
<marker>Rose, Stevenson, Whitehead, 2002</marker>
<rawString>Tony Rose, Mark Stevenson, and Miles Whitehead. 2002. The reuters corpus volume 1 from yesterday’s news to tomorrow’s language resources. In Proceedings of the 3rd International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval</title>
<date>1983</date>
<publisher>McGraw-Hill</publisher>
<location>New York</location>
<contexts>
<context>ated its advantage when using the same features. 6.1. Text Pre-processing Input to text categorization systems is usually preprocessed to create word/term vectors for each training and test instance (Salton and McGill, 1983). In addition, the word-based feature vectors are associated with a corresponding weight vector that ascribes a different weight to each word. Before creating word vectors, we remove nonword tokens, </context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M.J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Schapire</author>
<author>Y Singer</author>
</authors>
<title>Boostexter: A boosting-based system for text categorization</title>
<date>2000</date>
<booktitle>Machine Learning</booktitle>
<pages>39--2</pages>
<marker>Schapire, Singer, 2000</marker>
<rawString>R. E. Schapire and Y. Singer. 2000. Boostexter: A boosting-based system for text categorization. Machine Learning, 39(2/3):135–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Soroka</author>
<author>C Wlezien</author>
<author>I McLean</author>
</authors>
<title>Public Expenditure in the UK: How Measures Matter</title>
<date>2006</date>
<journal>Journal of the Royal Statistical Society</journal>
<pages>255--271</pages>
<contexts>
<context>guage associated with topics changes over time. This condition is the most dangerous for managing automatedlabelingreliabilityandtemporalconsistencybecause it can be difficult for people to identify (Soroka et al., 2006; Baumgartner et al., 2002). While the Policy Agendas annotation scheme is designed to capture the primary topics of legislation in one sense, specific programs come and go. The result can be a proble</context>
</contexts>
<marker>Soroka, Wlezien, McLean, 2006</marker>
<rawString>S. Soroka, C. Wlezien, and I. McLean. 2006. Public Expenditure in the UK: How Measures Matter. Journal of the Royal Statistical Society, pages 255–271.</rawString>
</citation>
</citationList>
</algorithm>

