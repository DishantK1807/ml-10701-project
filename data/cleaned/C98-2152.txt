Prefix Probabilities from Stochastic Tree Adjoining Grammars* 
Mark-Jan Nederhof 
DFKI 
Stuhlsatzenhausweg 3, 
D-66123 Saarbriicken, 
Germany 
nederhof@dfki, de 
Anoop Sarkar 
Dept. of Computer and Info. Sc. 
Univ of Pennsylvania 
200 South 33rd Street, 
Philadelphia, PA 19104 USA 
ano op(~linc, c ± s. upenn, edu 
Giorgio Satta 
Dip. di Elettr. e Inf. 
Univ. di Padova 
via Gradenigo 6/A, 
35131 Padova, Italy 
satta~dei, unipd, it 
Abstract 
Language models for speech recognition typ
ically use a probability model of the form 
Pr(an\]al,a~,...,an-1). Stochastic grammars, 
on the other hand, are typically used to as
sign structure to utterances. A lm~guage model 
of the above form is constructed from such 
grammars by computing the prefix probabil
ity ~c>:* Pr(al... aT~w), where w represents 
all possible terminations of the prefix al "" an. 
The main result in this paper is an algorithm 
to compute such prefix probabilities given a 
stochastic Tree Adjoining Grammar (TAG). 
The algorithm achieves the required computa
tion in (9(n a) time. The probability of sub
derivations that do not derive any words in the 
prefix, but contribute structurally to its deriva
tion, are preeomputed to achieve termination. 
This algorithm enables existing corpus-based es
timation techniques for stochastic TAGs to be 
used for language modelling. 
1 Introduction

Given some word sequence al."aT~-l, speech 
recognition language models are used to hy
pothesize the next word an, which could be 
any word t~om the vocabulary E. This 
is typically done using a probability model 
Pr(a~Ial,... ,an-l). Based on the assumption 
that modelling the hidden structure of nat
* Part of this research was done while the first and the 
third authors were visiting the Institute for Research 
in Cognitive Science, University of Pennsylvania. The 
first author wa.a supported by the German Federal Min
istry of Education, Science, Research and Technology 
(BMBF) in the framework of the VErtBMOmL Project un
der Grant 01 IV 701 V0, and by the Priority Programme 
Language and Speech Technology, which is sponsored by 
NWO (Dutch Organization for Scientific Research). The 
second and third authors were partially supported by 
NSF grant SBR8920230 and ARO grant DAAH0404-94
G-0426. The authors wish to thank Aravind Joshi for 
his support in this research. 
ural language would improve performance of 
such language models, some researchers tried to 
use stochastic context-free grammars (CFGs) to 
produce language models (Wright and Wrigley, 
1989; Jelinek and Lafferty, 1991; Stolcke, 1995). 
The probability model used for a stochas
tic grammar was ~we~* Pr(at...aT~w). How
ever, language models that are based on tri
gram probability models out-perform stochastic 
CFGs. The common wisdom about this failure 
of CFGs is that trigram models are lexicalized 
models while CFGs are ,lot. 
q~'ee Adjoining Grammars (TAGs) are impor
tant in this respect since they are easily lexical
ized while capturing the constituent structure 
of language. More importantly, TAGs allow 
greater linguistic expressiveness. The trees as
sociated with words can be used to encode argu
ment and adjunct relations in various syntactic 
environments. This paper assumes some famil
iarity with the TAG tbrmalism. (Joshi, 1988) 
and (aoshi and Schabes, 1992) are good intro
ductions to the formalism and its linguistic rele
vance. TAGs have been shown to have relations 
with both phrase-structure grammars and de
pendency grammars (Rambow and Joshi, 1995), 
which is relevant because recent work on struc
tured language models (Chelba et al., 1997) have 
used dependency grammars to exploit their lex
icalization. We use stochastic TAGs as such a 
structured language model in contrast with ear
lier work where TAGs have been exploited in 
a class-based n-gram language model (Srinivas, 
1996). 
This paper derives an algorithm to compute 
prefix probabilities ~,,eE" Pr(al ... anw). The 
algorithm assumes as input a stochastic TAG G 
and a string which is a prefix of some string in 
L(G), the language generated by G. This algo
rithm enables existing corpus-based estimation 
techniques (Schabes, 1992) in stochastic TAGs 
to be used for language modelling. 
953 
2 Notation

A stochastic Tree Adjoining Grammar (STAG) 
is represented by a tuple (NT, E, Z, .A, ¢) where 
NT is a set of nonterminal symbols, E is a set 
of terminal symbols, Z is a set of initial trees 
and A is a set of auxiliary trees. Trees in 2;UA 
are also called elementary trees. 
We refer to the root of an elementary tree t as 
Rt. Each auxiliary tree has exactly one distin
guished leaf, which is called the foot. We refer 
to the foot of an auxiliary tree t as Ft. We let 
V denote the set of all nodes in the elementary 
trees. 
For each leaf N in an elementary tree, except 
when it is a foot, we define label(N) to be the 
label of the node, which is either a terminal from 
E or the empty string e. For each other node 
N, label(N) is an element from NT. 
At a node N in a tree such that label(N) E 
NT an operation called adjunct±on can be ap
plied, which excises the tree at N and inserts 
an auxiliary tree. 
Function ¢ assigns a probability to each ad
junction. The probability of adjunct±on of t E .A 
at node N is denoted by ¢(t, N). The probabil
ity that at N no adjunct±on is applied is denoted 
by ¢(nil, N). We assume that each STAG G 
that we consider is proper. That is, for each 
N such that label(N) E NT, 
¢(t, N) = 1. 
tEAO{nil} 
For each non-leaAf node N we construct the 
string cdn(N) = Nl""Nm from the (ordered) 
list of children nodes NI,..., Nm A by defining, 
for each d such that 1 < d < m, Nd = label(Nd) 
in case label(Ng) E E U {e}, and Nd = Nd oth
erwise. In other words, children nodes are re
placed by their labels unless the labels are non
terminal symbols. 
To simplify the exposition, we assume an ad
ditional node for each auxiliary tree t, which 
we denote by ±. This is the unique child of the 
actual foot node Ft. That is, we change the def
±nit±on of cdn such that cdn(Ft) = _1_ for each 
auxiliary tree t. We set 
V±= {N E V I label(N)E NT} OEO {_L}. 
We use symbols a, b,c,.., to range over E, 
symbols v,w,x,.., to range over E*, sym
bols N, M,... to range over V ±, and symbols 
a,/9,%.., to range over (V±) *. We use t,t',... 
to denote trees in Z0 ~A or subtrees thereof. 
We define the predicate d\]t on elements from 
V ± as dft(N) if and only if (i) N E V and N 
dominates ±, or (ii) N = ±. We extend dft 
to strings of the form N1... Nm E (V±) * by 
defining dft(N1 ... Nm) if and only if there is a 
d (1 < d < m) such that dft(Nd). 
For some logical expression p, we define 
5(p) = 1 iff p is true, 5(p) = 0 otherwise. 
3 Overview

The approach we adopt in the next section to 
derive a method for the computation of prefix 
probabilities for TAGs is based on transforma
tions of equations. Here we informally discuss 
the general ideas underlying equation transfor
mations. 
Let w = ala2...a,~ E N* be a string and let 
N E V ±. We use the following representation 
which is standard in tabular methods for TAG 
parsing. An item is a tuple IN, i, j, fl, f2\] rep
resenting the set of all trees t such that (i) t is a 
subtree rooted at N of some derived elementary 
tree; and (ii) t's root spans from position i to 
position j in w, t's foot node spans from posi
tion fl to position f2 in w. In case N does not 
dominate the foot, we set fl = f2 --~ --. We gen
eralize in the obvious way to items It, i,j, fl, f2\], 
where t is an elementary tree, and \[c~, i, j, fl, f2\], 
where cdn(N) = c~ for some N and/9. 
To introduce our approach, let us start with 
some considerations concerning the TAG pars
ing problem. When parsing w with a TAG G, 
one usually composes items in order to con
struct new items spanning a larger portion of 
the input string. Assume there are instances of 
auxiliary trees t and t ~ in G, where the yield of 
t', apart fl'om its foot, is the empty string. If 
¢(t, N) > 0 for some node N on the spine of t ~, 
and we have recognized an item \[Rt, i, j, fl, f2\], 
then we may adjoin t at N and hence deduce 
the existence of an item \[Rt,,i,j, fl, f2\] (see 
Fig. l(a)). Similarly, if t can be adjoined at 
a node N to the left of the spine of t I and 
ft = f2, we may deduce the existence of an item 
\[Rt, , i,j,j,j\] (see Fig. l(b)). Importantly, one 
or more other auxiliary trees with empty yield 
could wrap the tree t ~ before t adjoins. Adjunc
tions in this situation are potentially nontermi
nating. 
One may argue that situations where auxil
iary trees have empty yield do not occur in prac
tice, and are even by definition excluded in the 
954 
a) Rt~ 
pine 
\j 
t t ~ 
t ~ e .tl e 
Figure 1: Wrapping in auxiliary trees with 
empty yield 
case of lexicalized TAGs. However, in the corn
put,at,ion of the prefix probability we must take 
into account, trees with non-ernpty yield which 
behave like trees with empty yield because their 
lexical nodes fM1 to the right' of the right' bound
ary of the prefix string. For example, the two 
cases previously considered in Fig. 1 now gen
eralize to those in Fig. 2. 
l~t¢ Rt¢ 
rtl ¢ 
Figure 2: Wrapping of auxiliary trees when 
computing the prefix probability 
To derive a method for the computation of 
prefix probabilities, we give some simple recur
sive equations. Each equation decomposes an 
item into other items in all possible ways, in 
the sense that it expresses the probability of 
that item as a function of the probabilities of 
items associated with equal or smaller portions 
of the input. 
In specifying the equations, we exploit tech
niques used in the parsing of incomplete in
put (Lang, 1988). This allows us to compute 
the prefix probability as a by-product, of com
puting the inside probability. 
In order to avoid the problem of nontermi
nation outlined above, we transform our equa
l,ions to remove infinite recursion, while preserv
ing tile correctness of the probability computa
tion. The transformation of tile equations is 
explained as follows. For an item I, the span 
of 1, written a(I), is the 4-tuple representing 
the 4 input' posit,ions in I. We will define an 
equivalence relation oil spans that, relates to the 
port,ion of the input that is covered. The trans
format,ions that we apply to our equations pro
duce two new sets of equations. The first, set 
of equations are concerned with all possible de
compositions of a given item I into set of items 
of which one has a st)an equivalent to that of I 
and the others have an empty span. Equations 
in this set represent endless recursion. The sys
tem of all such equations can be solved indepen
dent,ly of the actual intmt w. This is done once 
for a given grammar. 
The second set of equations have the property 
that, when evaluated, recursion always termi
nates. The evaluation of these equations com
putes the probability of the input string modulo 
the comt)ut,at,ion of some parts of the derivat,ion 
that, do not contribute to the input' itself. Com
t)ination of the second set of equations with the 
solutions obtained ii'om the first set allows the 
effective conltmtation of the prefix prot)ability. 
4 Computing
Prefix Probabilities 
This section develops an algorithm for the com
putation of prefix probabilities for stochastic 
TAGs. 
4.1 General
equations 
The prefix probability is given by: 
Pr(al-..a,,w) = ~P(\[t, 0,~%-,-\]), 
wEE* tEZ 
where P is a Nnct,ion over items recursively de
fined as follows: 
P(\[t,i,j, fl,f2\]) = P(\[Rt, i,j,f~,f2\]); (1) 
-, -\]) = (2) 
}2 i, k, -, -\]). P(\[N, k, j, -, -1), 
k(i < k < j) 
if a ¢ c A -~dft(aN); 
P(\[~tN, i, j, fl, f2\]) = (3) 
P(\[o6 i, k, -, -\]). P(\[N, k, j, ft, f2\]), 
k(i ~ ~ ~ fl) 
if a ¢ ~ A dft(N); 
955 
P(\[aN, i,j, fl, f2\]) = (4) 
P(\[a,i,k, fl,f2\]) . P(\[N,k,j,-,-\]), 
k(I2 < k <_ j) 
if a ¢ e A dft (a); 
P(\[N,i,j, fl, f2\]) = (5) 
¢(nil, N) . P(\[cdn (N), i, j, fl, f2\]) + 
P(\[cdn(N), f\[, f~, fl, f2\]) • 
f\[,f~(i <_ f~ <_ fl A f2 <_ f~ < j) 
¢(t,N). P(\[t,i,j,f;,f~\]), 
tCA 
if N E V A dft(N); 
P(\[N,i,j, -, -\]) = (6) 
¢(nil, N) . P(\[cdn(N),i,j,-,-\]) + 
P(\[cdn(N), ' ' f*, f2, -, -1) 
fl,f~(i <_ f~ <_ f~ <_J) 
¢(t, N). P(\[t,i,j,f~,f~\]), 
tEA 
if N C V A -,dft(N); 
P(\[a,i,j,, \])= (7) 
5(i + 1 = j A aj = a) + 5(i = j = n); 
P(\[J-,i,j, fl,f2\]) = 5(i = fl A j = f2); (8) 
P(\[e, i,j, -,-\]) = 5(i = j). (9) 
Term P(\[t, i, j, fl, f2\]) gives the inside probabil
ity of all possible trees derived from elementary 
tree t, having the indicated span over the input. 
This is decomposed into the contribution of each 
single node of t in equations (1) through (6). 
In equations (5) and (6) the contribution of a 
node N is determined by the combination of 
the inside probabilities of N's children and by 
all possible adjunctions at N. In (7) we rec
ognize some terminal symbol if it occurs in the 
prefix, or ignore its contribution to the span if it 
occurs after the last symbol of the prefix. Cru
cially, this step allows us to reduce the compu
tation of prefix probabilities to the computation 
of inside probabilities. 
4.2 Terminating
equations 
In general, the recursive equations (1) to (9) 
are not directly computable. This is because 
the value of P(\[A, i, j, f, f'\]) might indirectly de
pend on itself, giving rise to nontermination. 
We therefore rewrite the equations. 
We define an equivalence relation over spans, 
that expresses when two items are associated 
with equivalent portions of the input: 
" " " (i, j, fl, f2) if and only if 
((i',j') = (i,j))A 
((f~, f~) = (fl, f2)V 
((f~ = f~ = iV f; = f~ = jV f\[ = f~ = -)A 
(fl = f2 = iV fl = f2 = iV fl = f2 = -))) 
We introduce two new functions Pto.o and 
P~p,t. When evaluated on some item I, I)~ow re
cursively calls itself as long as some other item 
I' with a given elementary tree as its first com
ponent can be reached, such that c~(I) ~ a(I'). 
P~o~. returns 0 if the actual branch of recursion 
cannot eventually reach such an item I', thus 
removing the contribution to the prefix proba
bility of that branch. If item I' is reached, then 
P~o,, switches to P~,,t. Complementary to P~o~, 
function Psp,t tries to decompose an argument 
item I into items I' such that :(I) ¢ a(I'). If 
this is not possible through the actual branch 
of recursion, P~pm returns O. If decomposition 
is indeed possible, then we start again with P~o,~ 
at items produced by tile decomposition. The 
effect of this intermixing of function calls is the 
simulation of the original flmction P, with Pto~ 
being called only on potentially nonterminating 
parts of the computation, and P,p,t being called 
on parts that are guaranteed to terminate. 
Consider some derivation tree spanning some 
portion of the input string, and the associated 
derivation tree z. There must be a unique ele
mentary tree which is represented by a node in 
r that is the "lowest" one that entirely spans 
the portion of the input of interest. (This node 
might be the root of 7 itself.) Then, for each 
t E A and for each i,j, fl,f2 such that i < j 
and i _< ft <_ f2 <_ j, we must have: 
P(\[t, i, j, fl, f2\]) = (10) 
P,o~(\[t,i,j, fl,f2\], \[t',f;,f;\]). 
t' E A,f\[,f'2((i,j,f~,f:2) ~ (i,j,f~,f2)) 
Similarly, for each t E Z and for each i, j such 
that i < j, we must have: 
P(\[t, i,j, -,-\]) = (11) 
~o~(\[t,i,j,-,-\], \[t',f,f\]). 
t' E {t}U.A, f C {-,i,j) 
The reason why P~o~ keeps a record of indices 
f{ and f~, i.e., the spanning of the foot node 
of the lowest tree (in the above sense) on which 
P~o~o is called, will become clear later, when we 
introduce equations (29) and (30). 
We define P~o~(\[t,i,j, ft,f2\], \[t',f~,f~\]) and 
Plo,,(\[a,i,j, fl,f2\], rt' ~' \[ ,dl, f2\]) for i < j and 
(i,j, fl,f2) ~ (i,j,f~,f~), as follows. 
956 
Prod(It, i, j, fl, f2\], \[t',f{,f~\]) = (12) 
Pto~(\[Rt, i, j, fl, f2\], \[t',fI,f~\]) + 
5((t, fl, f2) = (t', f{, f~)) • 
P.p.t(\[Rt, i, j, f~, f2\]); 
P,o,o(\[.N,i,j,-,-\], = (13) 
P~o~(\[.,i,j,-,-\], \[t,f{,f~\]). 
P(\[N,j,j,-,-\]) + 
i, i, -, -\]) • 
P~o~(\[N,i,j, , \], \[t,f{,f~\]), 
if a ¢ e A -~dft(t~N); 
P~o,o(\[aN, i,j, fl,f~\], \[t,f{,f~\]) = (14) 
5(f~ : j). Pto~(\[ce, i,j,-,-\], \[t, fI,f~\]) • 
P(\[N,j,j, fl,f2\]) + 
i, i, -, -\]). 
P,o~(\[N,i,j,f~,f~\], \[t,f{,f~\]), 
if ~ ~ e A dft(N); 
P,o~(\[aN, i,j,f~,f~\], \[t,f{,f~\]) = (15) 
P, ow(\[a,i,j,f~,f2\], \[t,f{,f~\]). 
P(\[N,j,j,-,-\]) + 
5(i = f2). I)(\[o6 i, i, fl, f2\]) " 
P~o~(\[N,i,j,-,-\], \[t,f~,f~\]), 
if c~ ¢ e A dft(a); 
~o,.(\[N,i,j, fl,f2\], \[t,f{,f6\]) = (16) 
¢(nil, N) • 
I'~o~(\[cdn(N),i,j, ft,fu\], \[t,f{,f~\]) + 
P~o~(\[cdn(g), i,j, fl, f2\], \[t, f~, f~\]) • 
~e~A ¢(t',N) . P(\[t',i,j,i,j\]) + 
P(\[cdn( N), f~, f2, f~, f2\]) • 
¢(t', N). P~o~(\[t', i,j, ft, f~\], \[t, f{, f~\]), 
if g ~ Y A rift(N); 
P~o~(\[N,i,j, , \], t ' ' = \[., f~, f~\]) (17) 
¢(nil, N) • 
P~o.(\[cdn(N),i,j,-,-\], \[t, ff,f~\]) + 
P~o,.(\[cdn(N), i,j,-,-\], \[t, f~; f2\])' ' 
Et, eA¢(t',g). P(\[t',i,j,i,j\]) + 
P(\[cdn(N),f~',f~',-,-\]) . 
/I If /I II II It /~,/~(f~ =/.~ =iv/~ =/~ =3) 
~ ¢(t', N)'P,o,~(\[t', i, J, r''J1 , J2¢tt~J, \[t, f{ , f~\]), 
t'EA 
if N ~ V A ~dft(N); 
l~o,~(\[a, i,j,-,-\], \[t, f{, f~\]) = 0; (18) 
Pto~(\[±,i,j, fl,f2\], t ' = \[', fl, f~\]) 0; (19) 
Pto~,(\[e, i, j, -, -\], \[t,f~,f~\]) = 0. (20) 
The definition of Pzo~ parallels the one of P 
given in §4.1. In (12), the second term in the 
right-hand side accounts for the case in which 
the tree we arc visiting is the "lowest" one on 
which Plow should be called. Note how in tile 
above equations Plow must be called also on 
nodes that do not dominate the footnodc of the 
elementary tree they belong to (cf. tile definition 
of ..~). Since no call to P~p,t is possible through 
the terms in (18), (19) and (20), we must set 
the right-hand side of these equations to 0. 
The specification of P~,m(\[~,i,j, fl, f2\]) is 
given below. Again, tile definition parallels the 
one of P given in §4.1. 
P~,m(\[aN, i,j,, \])= (21) 
P(\[N,k,j,--,-\]) + 
k(i < k < j) 
P~m(\[~,i,j,-,-\]) " P(\[N,j,j,-,-\]) + 
P(\[a,i,i,-,-l) . P~,,.t(\[N,i,j, , \]), 
if a ¢ e A ~dft(c~g); 
P~,m(\[aN, i,j, fl,/2\]) = (22) 
P(\[a,i,k,-,-\]) . P(\[N,k,j, fl,f~\]) + 
k(i < k < fl A k < j) 
5(fl = j) " P~p.t(\[a,i,j,-,-\]) • 
P(\[N,j,j, fl,f2\]) + 
P(\[a, i, i, -, -\]) • P~,,,t (\[N, i, j, fl, f2\]), 
if (~ ¢ e A dft(N); 
P~7,., (\[aN, i, j, fl, f2\]) = (23) 
P(\[a,i,k, fl,f2\]), r(\[N,k,j,-,-\]) + 
k(i < k^f~ _< k <j) 
t~,m(\[a,i,j, fl,f2\]) . P(\[Y,j,j, , \]) + 
5(i = f2)" P(\[a, i, i, fl, f~\])' 
P~pm(\[N,i,j, -, -\]), 
if ~ ~ e A dft(~); 
P~,,m(\[N, i, j, ft, f2\]) = (24) 
¢(nil, N) . P~,,,,t(\[cdn(N), i, j, fl, f2\]) + 
P(\[cdn(N), f~, f~, f~, f2\]) • 
' ' (i< ' <flAf2<f' <jA fl,f2 fl ~ 
(f\[,f~) ¢ (i,j)A (f~,f~) ¢ (f~,f',)) 
~¢(t,N). P(\[t,i,j,f~,f~\]) + 
tCA 
P~..t (\[cdn( N), i, j, f ~ , f2\]) 
¢(t,N). P(\[t,i,j,i,j\]), 
tGA 
957 
if N e V A rift(N); 
P~p,t (\[N, i, j, -, -\]) = (25) 
¢(nil, N). P~pm(\[cdn(N), i, j, -, -\]) + 
P(\[cdn( g), f~, f~, -, -\]) . 
f~,f'2 (i < < _< J A (f~,f~) ¢ (i,J) A 
-~(f~ -~ f~ i V f~ -' = I2 = J)) 
¢(t, N) . P(\[t, i,j, f~, f~\]) + 
tEA 
P~p,t (\[cdn (N), i, j, -, -\]) • 
¢(t, N). e(\[t, i, j, i, j\]), 
tEA 
if Y C Y A -~dft(Y); 
P.p,t(\[a,i,j,-,-\]) = 5(i + 1 = j A aj = a); (26) 
P, pm(\[-l-, i,j, fl, f2\]) = 0; (27) 
P,p,t (\[C i, j, -, -\]) = 0. (28) 
We can now separate those branches of re
cursion that terminate on the given input from 
the cases of endless recursion. We assume be
low that P~vm(\[Rt,i,j, f{, f~\]) > 0. Even if this 
is not always valid, for the purpose of deriving 
the equations below, this assumption does not 
lead to invalid results. We define a new function 
Po=t,,, which accounts for probabilities of sub
derivations that do not derive any words in the 
prefix, but contribute structurally to its deriva
tion: 
Po~t,~(\[t,i,j, fl,f2\], \[t',f~,f~\]) = (29) 
Pto~ (\[t, i, j, f~, f2\], It', fl, f~\]). 
p.p,,t(\[Rt,,i,j,:I,:~\]) ' 
Pout.~(\[a,i,j, fl, f2\], \[t', fl; f2\])' = (30) 
Pto~(\[ce, i,j, fl,f2\], \[t',fl,f~\]) 
P,,,t (\[Rt,, i, j, f~, f~\]) 
We can now eliminate the infinite recur
sion that arises in (10) and (11) by rewriting 
P(\[t, i,j, fl, f2\]) in terms of Po~t~: 
P(\[t, i, j, fl, f2\]) = (31) 
t' E A, ' ' .... (i,j, fl,f2)) fl' f2(( z'3' fl' f2) ~ 
• t I P, put(\[Rt', i,3, fl, f2\]); 
P(\[t, i,j,-,-\]) (32) 
Po~,..(\[t,i,j,-,-\], \[t',f,f\]). 
t' E {t}uA, f E {-,i,jl 
P.,m(\[Rt,, i,j, f, f\]). 
Equations for Po~t~ will be derived in the next 
subsection. 
In summary, terminating computation of pre
fix probabilities should be based on equa
tions (31) and (32), which replace (1), along 
with equations (2) to (9) and all the equations 
for P~pm. 
4.3 Off-line Equations 
In this section we derive equations for function 
Pouter introduced in §4.2 and deal with all re
maining cases of equations that cause infinite 
recursion. 
In some eases, function P can be computed 
independently of the actual input. For any 
i < n we can consistently define the following 
quantities, where t E ZU.4 and a E V ± or 
cdn (N) = o#~ for some N and/~: 
lit = P(\[t,i,i,f,f\]); 
Ha = P(\[o~,i,i,f',f'\]), 
where f = i if t E A, f = otherwise, and f~ = 
i if dft(v~), f = otherwise. Thus, Ht is the 
probability of all derived trees obtained from t, 
with no lexical node at their yields. Quantities 
Ht and Ha can be computed by means of a sys
tem of equations which can be directly obtained 
from equations (1) to (9). Similar quantities as 
above must be introduced for the case i = n. 
For instance, we can set H~ = P(\[t, n, n, f, f\]), 
f specified as above, which gives the probabil
ity of all derived trees obtained from t (with no 
restriction at their yields). 
Function Po.t~ is also independent of the 
actual input. Let us focus here on the case 
fl, f2 ~ {i,j, -} (this enforces (fl, f2) = (f~, f~) 
below). For any i,j, fl, f2 < n, we can consis
tently define the following quantities. 
Lt,t, = Po.t¢r(\[t,i,j, fl,f2\], \[t',f~,f~\]); 
La,t, = Po~t~(\[c~,i,j, fl,f2\], \[t',fi,f~\]). 
In the case at hand, Lt,t, is the probability of all 
derived trees obtained from t such that (i) no 
lexical node is found at their yields; and (ii) at 
some 'unfinished' node dominating the foot of 
t, the probability of tile adjunction of t ~ has al
ready been accounted for, but t t itself has not 
been adjoined. 
It is straightforward to establish a system of 
equations for the computation of Lt,e and La,t,, 
by rewriting equations (12) to (20) according 
to (29) and (30). For instance, combining (12) 
and (29) gives (using the above assumptions on 
fl and f2): 
Lt,t' = LRt,t' + 5(t = t'). 
Also, if a ~ e and dft(N), combining (14) 
and (30) gives (again, using previous assump
958 
~ions on fl and f2; note that the H~'s are known 
terms here): 
Lc~y,t' = H~ " LN,t'. 
For any i, fl,f2 < n and j = n, we also need to 
define: 
L~, t, = Po,te,(\[t,i,n, fl,f2\], \[t',f{,f~\]); 
fl, f2\])" L~,t, = Po~,.(\[a,i,n, fl,f~\], \[t', ' ' 
Here L~, t, is the probability of all derived trees 
obtained from t with a node dominating the 
foot node of t, that is an adjunction site for t ~ 
and is 'unfinished' in the same sense as above, 
and with lexical nodes only in the portion of 
the tree to the right of that node. When we 
drop our assumption on fl and f2, we must 
(pre)compute in addition terms of the form 
Po~t~(\[t,i,j,i,i\], \[t',i,i\]) and Po~t~(\[t,i,j,i,i\], 
\[t',j,j\]) tbr i < j < n, Po~(\[t,i,n, fl,n\], 
\[t',f~,f~\]) for i < fl < n, Po~t.~(\[t,i,n,n,n\], 
\[t', f{, f~\]) for i < n, and similar. Again, these 
are independent of the choice of i, j and fl. Full 
treatment is omitted due to length restrictions. 
5 Complexity
and concluding 
remarks 
We have presented a method for the computa
tion of the prefix probability when the underly
ing model is a Tree Adjoining Grammar. Func
tion P.,,~ is the core of the method. Its equa
tions can be directly translated into an effective 
algorithm, using standard functional memoiza
tion or other tabular techniques. It is easy to 
see that such an algorithm can be made to run 
in time (9(n6), where n is the length of the input 
prefix. 
All the quantities introduced in §4.3 (Ht, 
Lt,t, , etc.) are independent of the input and 
should be computed off-line, using the system of 
equations that can be derived as indicated. For 
quantities Ht we have a non-linear system, since 
equations (2) to (6) contain quadratic terms. 
Solutions can then be approximated to any de
gree of precision using standard iterative meth
ods, as for instance those exploited in (Stolcke, 
1995). Under the hypothesis that the grammar 
is consistent, that is Pr(L(G)) = 1, all quanti
ties H~ and H~ evaluate to one. For quantities 
Lt,t, and the like, §4.3 provides linear systems 
whose solutions can easily be obtained using 
standard methods. Note also that quantities 
L~,t, are only used in the off-line computation 
of quantities Lt,t,, they do not need to be stored 
for the computation of prefix probabilities (com
pare equations for Lt,t, with (31) and (32)). 
We can easily develop implementations of our 
method that can compute prefix probabilities 
incrementally. That is, after we have computed 
the prefix probability for a prefix al •. • an, on in
put an+l we can extend the calculation to prefix 
al...ana~+l without having to recompute all 
intermediate steps that do not depend on an+l. 
This step takes time O(nS). 
In this paper we have assumed that the pa
rameters of the stochastic TAG have been pre
viously estimated. In practice, smoothing to 
avoid sparse data problems plays an important 
role. Smoothing can be handled for prefix prob
ability computation in the following ways. Dis
counting methods for smoothing simply pro
duce a modified STAG model which is then 
treated as input to the prefix probability com
putation. Smoothing using methods such as 
deleted interpolation which combine class-based 
models with word-based models to avoid sparse 
data problems have to be handled by a cognate 
interpolation of prefix probability models. 

References 

C. Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khu
danpur, L. Mangu, H. Printz, E. Ristad, A. Stolcke, 
R. Rosenfeld, and D. Wu. 1997. Structure and per
formance of a dependency language model. In Proe. 
of Eurospeech 97, volume 5, pages 2775-2778. 

F. Jelinck and J. Latferty. 1991. Computation of thc 
probability of initial substring generation by stochas
tic context-free grammars. Computational Linguis
tics, 17(3):315-323. 

A. K. Joshi and Y. Schabes. 1992. Tree-adjoining gram
mars and lexicalized grammars. In M. Nivat and 

A. Podelski, editors, Tree automata and languages, 
pages 409-431. Elsevier Science. 

A. K. Joshi. 1988. An introduction to tree adjoining 
grammars. In A. Manaster-Ramer, editor, Mathemat
ics of Language. John Benjamins, Amsterdam. 

B. Lang. 1988. Parsing incomplete sentences. In Proc. of 
the 12th International Conference on Computational 
Linguistics, volume 1, pages 365-371, Budapest. 

O. Rambow and A. Joshi. 1995. A formal look at de
pendency grammars and phrase-structure grammars, 
with special consideration of word-order phenomena. 
In Leo Wanner, editor, Current Issues in Meaning
Text Theory. Pinter, London. 

Y. Schabes. 1992. Stochastic lexicalized tree-adjoining 
grammars. In Proc. of COLING '92, volume 2, pages 
426-432~ Nantes, France. 

B. Srinivas. 1996. "Ahnost Parsing" technique for lan
guage modeling. In P~oc. ICSLP '96, volume 3, pages 
1173-1176, Philadelphia, PA, Oct 3-6. 

A. Stolcke. 1995. An cfficient probabilistic context-free 
parsing algorithm that computes prefix probabilitics. 
Computational Linguistics, 21(2):165-201. 

J. H. Wright and E. N. Wrigley. 1989. Probabilistic LI~ 
parsing for speech recognition. In IWPT '89, pages 
105-114. 

