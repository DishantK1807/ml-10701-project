level (Wong, 2007). In contrast, the proposed tree
N07-1022
Kay (1996). Examples of such systems include
P96-1027
Following the work of Wong and Mooney (2007),
N07-1022
Franz J. Och. 2003. Minimum error rate training in
P03-1021
is assumed, as discussed in Lu et al. (2008)):
D08-1082
Yuk Wah Wong and Raymond J. Mooney. 2007.
N07-1022
2006; Lu et al., 2008), we consider MRs in the
D08-1082
(Miyao and Tsujii, 2008), which was originally
J08-1002
model of Wong and Mooney (2007). Further-
N07-1022
Yuk Wah Wong and Raymond J. Mooney. 2006.
N06-1056
posed in Lu et al. (2008) was shown to give state-
D08-1082
sented in Lu et al. (2008). The training corpus re-
D08-1082
The recent work by Wong and Mooney (2007)
N07-1022
the published paper of Wong and Mooney (2007).
N07-1022
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
P06-1115
been studied in Lu et al. (2008), its inverse task
D08-1082
in Lu et al. (2008) has several advantages over var-
D08-1082
error rate training (Och, 2003) to directly optimize
P03-1021
roll and Oepen, 2005; Nakanishi et al., 2005), and
W05-1510
John Carroll and Stephan Oepen. 2005. High ef-
I05-1015
Yuk Wah Wong. 2007. Learning for Semantic Parsing
N07-1022
Yusuke Miyao and Junâ€™ichi Tsujii. 2008. Feature for-
J08-1002
comparable to Wong and Mooney (2007), we used
N07-1022
Martin Kay. 1996. Chart generation. In Proceedings
P96-1027
Ruifang Ge and Raymond J. Mooney. 2005. A statis-
W05-0602
structure. In Lu et al. (2008), we introduced a
D08-1082
area (Kate et al., 2005; Ge and Mooney, 2005;
W05-0602
Kate and Mooney, 2006; Wong and Mooney,
P06-1115
