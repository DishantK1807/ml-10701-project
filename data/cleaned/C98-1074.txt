Efficient Linear Logic Meaning Assembly 
Vineet Gupta 
Caelum Research Corporation 
NASA Ames Research Center 
Moffett Field CA 94035 
vgupt a@pt olemy, arc. nasa. gov 
John Lamping 
Xerox PARC 
3333 Coyote Hill Road 
Palo Alto CA 94304 USA 
lamping~parc, xerox, com 
1 Introduction

The "glue" approach to semantic composition 
in Lexical-Functional Grammar uses linear logic 
to assemble meanings from syntactic analyses 
(Dalrymple et al., 1993). It has been compu
tationally feasible ill practice (Dalrymple et al., 
1997b). Yet deduction in linear logic is known 
to be intractable. Even the propositional ten
sor fragment is NP complete(Kanovich, 1992). 
In this paper, we investigate what has made 
the glue approach computationally feasible and 
show how to exploit that to efficiently deduce 
underspecified representations. 
In the next section, we identify a restricted 
pattern of use of linear logic in the glue analyses 
we are aware of, including those in (Crouch and 
Genabith, 1997; Dalrymple et al., 1996; Dal
rymple et al., 1995). And we show why that 
fragment is computationally feasible. In other 
words, while the glue approach could be used 
to express computationally intractable analyses, 
actual analyses have adhered to a pattern of use 
of linear logic that is tractable. 
The rest of the paper shows how this pat
tern of use can be exploited to efficiently cap
ture all possible deductions. We present a con
servative extension of linear logic that allows a 
reformulation of the semantic contributions to 
better exploit this pattern, almost turning them 
into Horn clauses. We present a deduction algo
rithm tbr this formulation that yields a compact 
description of the possible deductions. And fi
nally, we show how that description of deduc
tions can be turned into a compact underspeci
fled description of the possible meanings. 
Throughout the paper we will use the illus
trative sentence "every gray cat left". It has 
fu~('tional structure ,1,  pRED Liav 1 PRED ~CAT' f: SUBJ g: SPEC 'EVERY' 
and semantic contributions 
leave :Yx. go-,~x --o fo,,~ leave(x) 
cat :Vx. (go VAR)--~x --o (g, RESTR)-,~ cat(x) 
gray :VP. \[Vx. (go VAR)--~x --o (go RESTR)--~ P(x)\] 
--o \[Vx. (go vaR)~x 
-o (go RESTR)"--,gray(P)(x)\] 
every :VH~/~, S. 
\[Vx. (go vaR)~x -o (go R~STR)~(X)\] 
®\[Vx. go-.*x --o n-.~s(x)\] 
--o H',~ every(R, S) 
For our purposes, it is more convenient to fol
low (Dalrymple et al., 1997a) and separate the 
two parts of the semantic contributions: use a 
lambda term to capture the meaning formulas, 
and a type to capture the connections to the 
f-structure. In this form, the contributions are 
leave: Ax.leave(x) : gz --o f~ 
cat : /~x.cal(x): (go VAR) --o (ga RESTR) 
gray: A\]:'.Ax.gray(P)(x) : 
((go vaa) -~ (go nESTR)) 
-o (go vaR) -o (Jo RESTa) 
every : AR.AS.every(R, S) : 
VH. (((go Vaa) -o (go aESTa)) 
®(go --~ H)) 
--oH 
With this separation, the possible derivations 
are determined solely by the "types", the con
nections to the f-structure. The meaning is as
sembled by applying the lambda terms in ac
cordance with a proof of a type for the sen
tence. We give the formal system behind this 
approach, C in Figure 1 -this is a different 
presentation of the system given in (Dalrymple 
464 
et al., 1997a), adding the two standard rules 
for tensor, using pairing for meanings. For the 
types, the system merely consists of the linear 
logic rules for the glue fi'agment. 
We give the proof for our example in Figure 2, 
where we have written the types only, and have 
omitted the trivial proofs at the top of the tree. 
The meaning evc'ry(gray(cat),left ) may be as
sembled by putting the meanings back in ac
cording to the rules of O and rl-reduction. 
M :Abe A:I''A 
where l~:I ~,,j A~/~ 
I',M :A\[B/A'\] ~c 1? 
F, M : VX.A Pc R 
I" Pc N : A 
F,P,Q, At-c R 
F,Q,P, APc R 
r Po M: A\[Y/X\] ,n¢w) 
F bc M : VX.A (} 
l,:_k, fla'.A/ : A -o II Pc R 
lleW) F ~-c Aa:.M : A -o B 
I',M :.I,N :/3P R FP M :A A~-N :B 
r,<M,N> : A,> B le r,5 <M,N> : AGB 
Figure 1: The system C. M,N are meanings, 
and x, y are meaning variables. A,B are types, 
and X, Y are type variables. P, Q, R are fornm
las of the kind M : A. I',A are multisets of 
formulas. 
2 Skeleton
references and modifier ref
erences 
The terms that describe atomic types, terms like 
g~ and (g~ VAlt), are semantic structure refer
~nc~, the type atoms that connect the semantic 
assembly to the syntax. There is a pattern to 
how they occur ill glue analyses, which reflects 
their function in the semantics. 
Consider a particular type atom in the ex
ample, such as g~. It occurs once positively in 
the contribution of "every" and once negatively 
in the contribution of "leave". A slightly more 
complicated example, the type (go I~ESTR) oc
curs once positively in the contribution of "cat", 
once negatively in the contribution of "every", 
and once each positively and negatively in the 
contribution of "gray". 
The pattern is that every type atom occurs 
once positively in one contribution, once nega
tively in one contribution, and once each posi
tively and negatively in zero or more other con
tributions. (To make this generalization hold, 
we add a negative occurrence or "consumer" of 
re, the final meaning of the sentence.) This pat
tern holds in all the glue analyses we know of, 
with one exception that we will treat shortly. 
We call the independent occurrences the skele
ton occurrences, and the occurrences that occur 
paired in a contribution modifier occurrences. 
The pattern reflects the functions of the lex
ical entries in LEG. For the type that corre
sponds to a particular f-structure, the idea is 
that, the entry corresponding to the head mMces 
positive skeleton contribution, the entry that 
subcategorizes for the f-structure makes a neg
ative skeleton contribution, and modifiers on 
the f-structure make both positive and negative 
modifier contributions. 
Itere are the contributions for the example 
sentence again, with the occurrences classified. 
Each occurrence is marked positive or negative, 
and the skeleton occurrences are underlined. 
leave 
eat 
gray 
every 
g._Ga--o fc, + 
(qa VAR)--0 (ga I{~ESTR') + 
+ aES  a) -) 
--o (ga VAR)--o (go RESTR) + 
VH. (((ga VAR) + --o (g~ RF, STR)) 
--o H-)) 
---o H + 
This pattern explains the empirical tractabil
ity of glue inference. In the general case of 
multiplieative linear logic, there can be complex 
combinatorics in matching up positive and neg
ative occurrences of literals, which leads to NP
completeness (Kanovich, 1992). But in the glue 
fragment, on the other hand, the only combina
torial question is the relative ordering of modi
tiers. In the common case, each of those order
ings is legal and gives rise to a different mean
ing. So the combinatorics of inference tends to 
be proportional to the degree of semantic am
biguity. The complexity per possible reading is 
thus roughly linear in the size of the utterance. 
But, this simple eombinatoric structure sug
gests a better way to exploit the pattern. 
Rather than have inference explore all tile com
binatorics of different modifier orders, we can 
get a single underspecified representation that 
captures all possible orders, without having to 
465 
cat ~(g, van) -o (g. RESTa) (~, van) --o (a. nESTa) F(g. WR) --o (g, aEs'ra) 
(:at, ((ga VAR) --o (ga RESTR.)) --o (g~r VAIl.) --O (ga RESTR) 1"(go VAR) ---o (g~r RESTR) 
gray, eat i(gq VAIl,) ---o (g~, RESTR) leave Fgo ---o f~ 
gray, c.t,leave ~((g~ van) -o (go aESTk)) ® (g~ --0 to) f~ ~f~ 
gray, cat,leave, (((go VAn) --o (g~ RESTR)) ® (go --~ fo)) --o f~ Ff~ 
every, gray, eat, leave ~fo 
Figure 2: Proof of "Every gray cat left", omitting the lambda terms 
explore them. 
The idea is to do a preliminary deduction in
volving just the skeleton, ignoring the modifier 
occurrences. This will be completely determin
istic and linear in the total length of the for
mulas. Once we have this skeletal deduction, 
we know that the sentence is well-formed and 
has a meaning, since modifier occurrences es
sentially occur as instances of the identity ax
iom and do not contribute to the type of the 
sentence. Then the system can determine the 
meaning terms, and describe how the modifiers 
can be attached to get the final meaning term. 
That is the goal of the rest of the paper. 
3 Conversion
toward horn clauses 
The first hurdle is that the distinction between 
skeleton and modifier applies to atomic types, 
not to entire contributions. The contribution of 
"every", for example, has skeleton contributions 
for g~, (.q~ Via), and (g~ aESTR), but modifier 
contributions for H. Furthermore, the nested 
implication structure allows no nice way to dis
entangle the two kinds of occurrences. When a 
deduction interacts with the skeletal g¢ in the 
hypothetical it also brings in the modifier H. 
If the problematic hypothetical could be con
vetted to Horn clauses, then we could get a bet
ter separation of the two types of occurrences. 
We can approximate this by going to an in
dexed linear logic, a conservative extension of 
the system of Figure 1, similar to Hepple's sys
tem(Hepple, 1996). 
To handle nested implications, we introduce 
the type constructor A{B}, which indicates an 
A whose derivation made use of B. This is sim
ilar to Hepple's use of indices, except that we 
indicate dependence on types, rather than on in
dices. This is sufficient in our application, since 
each such type has a unique positive skeletal 
occurrence. 
We can eliminate problematic nested impli
cations by translating them into this construct, 
in accordance with the following rule: 
For a nested hypothetical at top level that has 
a mix of skeleton and modifier types: 
M :(A -o B) --o C 
replace it with 
x:d, M:(B{A}--oC) 
where x is a new variable, and reduce complex 
dependency formulas as follows: 
1. Replace A{B -oe} with A{C{Bi}. 
2. Replace (A -o B){C} with A -o 13{C}. 
The semantics of the new type constructors 
is captured by the additional proof rule: 
F,x:A~-M:B 
F,x : A ~)~x.M : B{A} 
The translation is sound with respect to this 
rule: 
Theorem 1 If r is a set of sentences in the 
unextended system of Figure 1, A is a sentence 
in that system, and F ~ results from r by applying 
the above conversion rules, then F tA in the 
system of Figure 1 iff F ~ ~A in the extended 
system. 
The analysis of pronouns present a different 
problem, which we discuss in section 5. For all 
other glue analyses we know of, these conver
sions are sufficient to separate items that mix 
interaction and modification into statements of 
466 
the form 8, M, or S --o M, where ,5 is pure 
skeleton and 3,4 is pure modifier. Furthermore, 
3,4 will be of the form A -o A, where A may be 
a formula, not just an atom. In other words, the 
type of the modifier will be an identity axiom. 
The modifier will consume some meaning and 
produce a modified meaning of the same type. 
In our example, the contribution of "every", 
can be transformed by two applications of the 
nested hypothetical rule to 
every :AR.AS'.every( g, 3) : 
vu. (g~ aESTR){(~ v.R)} 
-o H { go } ---o tt 
x :(go v,r¢) 
Y :go 
Here, the last two sentences are pure skele
ton, producing (g~ VAtt) and g~, respectively. 
The first is of the form ,5" --o jr4, consuming 
(g~ aESTR), to produce a pure modifier. 
While the rule for nested hypotheticals could 
be generalized to eliminate all nested implica
tions, as Hepple does, that is not our goal, be
cause that does remove the combinatorial com
bination of different modifier orders. We use the 
rule only to segregate skeleton atoms fl'om mod
ifier atoms. Since we want modifiers to end up 
looking like the identity axiom, we leave them 
in the A --o A form, even if A contains further 
ilnplications. For example, we would not apply 
the nested hypothetical rule to simplify the en
try for gray any further, since it is already in 
the form A-o A. 
Ifandling intensional verbs requires a more 
precise definition of skeleton and modifier. The 
type part of an intensional verb contribution 
looks like (VV.(h~ ---o F) --o F) -o g~ --o f~ 
(Dalrymple et al., 1996). 
First, we have to deal with the small 
technical problem that the VF gets in the 
way of the nested hypothetical translation 
rule. This is easily resolved by introducing 
a skolem constant, S, turning the type into 
((h~, --o S) --o ,5') -o g~ --o f~. Now, the 
nested hypothetical rule can be applied to yield 
(h~ --o S) and ,5'{S{h~}} --o g~ --o f~. 
But now we have the interesting question of 
whether the occurrences of the skolem constant, 
S, are skeleton or modifier. If we observe how S 
resources get produced and consumed in a de
duction involving the intensional verb, we find 
that (ho --o S) produces an S, which may be 
modified by quantifiers, and then gets consumed 
by S{S{h¢}} -o ga --o f~. So unlike a modifier, 
which takes an existing resource from the envi
ronment and puts it back, the intentional verb 
places the initial resource into the environment, 
allows modifiers to act on it, and then takes it 
out. In other words, the intensional verb is act
ing like a combination of a skeleton producer 
and a skeleton consumer. 
So just because an atom occurs twice in a 
contribution doesn't make the contribution a 
modifier. It is a modifier if its atoms must in
teract with the outside, rather than with each 
other. Roughly, paired modifier atoms function 
as f --o f, rather than as f ® f_k, as do the S 
atoms of intensional verbs. 
Stated precisely: 
Definition 2 Assume two occurrences of the 
same type atom occur in a single contribution. 
Convert the formula to a normal form consist
ing of just ®, ~ , and ± on atoms by converting 
subformulas A -o B to the equivalent A ± :2 B, 
and then using DeMowan's laws to push all ± 's 
down to atoms. Now, if the occurrences of the 
same type atom occur with opposite polarity and 
the connective between the two subexpressions in 
which they occur is ~ , then the occurrences are 
modifiers. All other occurrences are skeleton. 
For the glue analyses we are aware of, this def
inition identifies exactly one positive and one 
negative skeleton occurrence of each type among 
all the contributions for a sentence. 
4 Efficient
deduction of underspecified 
representation 
In the converted form, the skeleton deductions 
can be done independently of the modifier de
ductions. Furthermore, the skeleton deductions 
are completely trivial, they require just a lin
ear time algorithm: since each type occurs once 
positively and once negatively, the algorithm 
just resolves the matching positive and nega
tive skeleton occurrences. The result is several 
deductions starting from the contributions, that 
collectively use all of the contributions. One of 
the deductions produces a meaning for f~, for 
the whole f-structure. The others produce pure 
modifiers -these are of the form A -o A. For 
467 
Lexical contributions in indexed logic: 
leave : 
cat : 
gray : 
every~ : 
every2 : 
every3 : ,~x.leave(x) : ga -o fa 
)~x.cat(x): (ga VAR) --o (ga RESTR) 
: vAR) --o RESTR)) --0 VAR) --o RESTR) 
)~R.,~S.every(R, S): VZ. (y~ RESTR){(y~ VAR)} ---o H{g~} -o H 
x vAR) 
Y :g~ 
The following can now be proved using the extended system: 
gray F,\P.,~x.gray(P)(x) : ((gz VAR) --o (g~ RESTR)) ---O (ga VAR) -o (gcr RESTR) 
every2, cat,every1 t,~S.every()~x.cat(x), S): VH. II{go} -o H 
every3, leave ~leave(y) : f~ 
Figure 3: Skeleton deductions for "Every gray cat left". 
the example sentence, the results are shown in 
Figure 3. 
These skeleton deductions provide a compact 
representation of all possible complete proofs. 
Complete proofs can be read off from the skele
ton proofs by interpolating the deduced modi
fiers into the skeleton deduction. One way to 
think about interpolating the modifiers is in 
terms of proof nets. A modifier is interpolated 
by disconnecting the arcs of the proof net that 
connect the type or types it modifies, and recon
necting them through the modifier. Quantifiers, 
which turn into modifiers of type VF.F -o F, 
can choose which type they modify. 
Not all interpolations of modifiers are le
gal. however. For example, a quantifier must 
outscope its noun phrase. The indices of the 
modifier record these limitations. In the case 
of the modifier resulting from "every cat", 
VH.H{g~} --o H, it records that it must 
outscope "every cat" in the {g~}. The in
dices determine a partial order of what modi
fiers must outscope other modifiers or skeleton 
terms. 
In this particular example, there is no choice 
about where modifiers will act or what their rel
ative order is. In general, however, there will be 
choices, as in the sentence "someone likes every 
cat", analyzed in Figure 4. 
To summarize so far, the skeleton proofs pro
vide a compact representation of all possible de
ductions. Particular deductions are read off by 
interpolating modifiers into the proofs, subject 
to the constraints. But we are usually more in
terested in all possible meanings than in all pos
sible deductions. Fortunately, we can extract a 
compact representation of all possible meanings 
from the skeleton proofs. 
We do this by treating the meanings of the 
skeleton deductions as trees, with their arcs an
notated with the types that correspond to the 
types of values that flow along the arcs. Just as 
modifiers were interpolated into the proof net 
links, now modifiers are interpolated into the 
links of the meaning trees. Constraints on what 
modifiers must outscope become constraints on 
what tree nodes a modifier must dominate. 
Returning to our original example, the skele
ton deductions yield the following three trees: 
RESTR) / ~ tgaJ 
leavelf  a STR) I j RESTR) gray 
cat \[ga (go. VAR) ; \] t~: VAR) ''-ORESTR) 
y 
leave(y) AS.every(~x.cat(x), S) ~P.Ax.gray(P)(x) 
Notice that higher order arguments 
are reflected as structured types, like 
(g~ VAR) -o (go RESTR). These trees are 
a compact description of the possible meanings, 
in this case the one possible meaning. We 
believe it will be possible to translate this rep
resentation into a UDRS representation(Reyle, 
1993), or other similar representations for 
ambiguous sentences. 
We can also use the trees directly as an un
derspecified representation. To read out a par
ticular meaning, we just interpolate modifiers 
into the arcs they modify. Dependencies on a 
468 
The functional structure of "Someone likes every cat". 
P B, ED 
SUBJ 
f: 
OBJ 
The lexical entries after 
~LIKE' 
h:\[Pf~. '~OM~ONE'\] \[ 
t'RsD 'cAT' \] g: 
SPEC ~EVER,Y ~ 
conversion to indexed form: 
like : 
eat : 
someone 1 : 
someone2 : 
everyl : 
every2 : 
every3 : 
ax.ay.like(x,y) : (h~ ® g~) --o L 
Ax.~at(x) : (g~ vAR) --o (g~ aSSTR) 
z:h a 
AS.some(person, S) :VH. lI{ha} --o It 
)~R.~S.eve,'y(R, Z) : VII. (ga Rt~STR){(ga VAIt)} ---o II{g~} --o II 
X : (ga VAR) 
Y:ga 
From these we can prove: 
someone1, every3, like ~like(z, y) : fo 
someone2 ~AS.some(person, S) : VH. H{ha} -o H 
every2, cat, every1 ~AS.every(cat, S) : VII. H{g~} -o H 
Figure 4: Skeleton deductions for "Someone likes every cat" 
modifier's tyt)e indicate that a lambda abstrac
tion is also needed. So, when "every cat" mod
ifies the ,~entence meaning, its antecedent, in
stantiated to f~{g~} indicates that it lambda 
abstracts over the variable annotated with g~ 
and replaces the term annotated fa. So the re
sult is: 
cvery /g e VAR) --~o/ ~y 
~ ,~.~T%... \ .-~ f~ 
cat le(tve 
(gcr VAR) !, ;.(/cr 
Similarly "gray" can modify this by splicing 
it into the line labeled (g~ VAa) -o (g~ P~SSTR) 
to yield (after ~-reduction, and removing labels 
on the arcs). 
If~ every 
/ \ 
gray leave l 
eat 
This gets us the expected meaning every(gray(eat), leave). 
In some cases, the link called for by a higher 
order modifier is not directly present in the tree, 
and we need to do A-abstraction to support 
it. Consider the sentence "John read llamlet 
quickly". We get the following two trees fi'om 
the skeleton deductions: 
read g/ \h~ 
John Hamlet 
read(John, Hamlet) 
Ig ~ -o fo 
quickly 
Ig~-~ f~ 
AP.Ax.quickl y( P )( x ) 
There is no link labeled go --o f~ to be modi
fied. The left tree however may be converted by 
A-abstraction to the following tree, which has a 
required link. The @ symbol represents k ap
plication of the right subtree to the left. 
LI
go --° f~//~',.~¢ 
Ax. John 
tf, read 
g~ \h~ 
x Hamlet 
Now quickly can be interpolated into the 
link labeled g¢ -o f¢ to get the desired 
meaning quickly(read(Hamlet), John), after 71 
reduction. The cases where A~abstraction is re
quired can be detected by scanning the modi
tiers and noting whether the links to be mod
ified are present in the skeleton trees. If not, 
A-abstraction can introduce them into the un
469 
derspecified representation. Furthermore, the 
introduction is unavoidable, as the link will be 
present in any final meaning. 
5 Anaphora

As mentioned earlier, anaphoric pronouns 
present a different challenge to separating skele
ton and modifier. Their analysis yields types 
like f~ -o (f~ ® g~) where g~ is skeleton and f~ 
is modifier. We sketch how to separate them. 
We introduce another type constructor (B)A, 
informally indicating that A has not been fully 
used, but is also used to get B. 
This lets us break apart an implication whose 
right hand side is a product in accordance with 
the following rule: 
For an implication that occurs at top level, 
and has a product on the right hand side that 
mixes skeleton and modifier types: 
Ax.<2ig, N): d --o (B ® C) 
replace it with 
Ax.M : (C)A --o B, N : C 
The semantics of this constructor is captured 
by the two rules: 
M1 : AI~...,M~ : An ~M : A 
M1 : (B)A,,...,Mn: (B)An ~M: (B)A 
F, M1 :(B)A, Me :B~-N:C 
F I, M~:A, M~:B~-NI:C 
where the primed terms are obtained by 
replacing free x's with what was applied to 
the Ax. in the deduction of (B)A 
With these rules, we get the analogue of The
orem 1 for the conversion rule. In doing the 
skeleton deduction we don't worry about the 
(B)A constructor, but we introduce constraints 
on modifier positioning that require that a hy
pothetical dependency can't be satisfied by a 
deduction that uses only part of the resource it 
requires. 
6 Acknowledgements

We would like to thank Mary Dalrymple, John 
Fry, Stephan Kauffmann, and Hadar Shemtov 
for discussions of these ideas and for comments 
on this paper. 

References 

Richard Crouch and Josef van Genabith. 1997. How to glue a donkey to an f-structure, or porting a dynamic meaning representation into LFG's linear logic based glue-language semantics. Paper to be presented at the Second International Workshop on Computational Semantics, Tilburg, The Netherlands, January 1997. 

Mary Dalrymple, John Lamping, and Vijay Saraswat. 1993. LFG semantics via constraints. In Proceedings of the Sixth Meeting of the European ACL, pages 97-105, University of Utrecht. European Chapter of the Association for Computational Linguistics. 

Mary Dalrymple, John Lamping, Fernando C. N. Pereira, and Vijay Saraswat. 1995. Linear logic for meaning assembly. In Proceedings of CLNLP, Edinburgh. 

Mary Dalrymple, John Lamping, Fernando C. N. Pereira, and Vijay Saraswat. 1996. Intensional verbs without type-raising or lexical ambiguity. In Jerry Seligman and Dag Westerst£hl, editors, Logic, Language and Computation, pages 167-182. CSLI Publications, Stanford University. 

Mary Dalrymple, Vineet Gupta, John Lamping, and Vijay Saraswat. 1997a. Relating resource-based semantics to categorial semantics. In Proceedings of the Fifth Meeting on Mathematics of Language (MOLS), Schloss Dagstuhl, Saarbriicken, Germany. 

Mary Dalrymple, John Lamping, Fernando C. N. Pereira, and Vijay Saraswat. 1997b. Quantifiers, anaphora, and intensionality. Journal of Logic, Language, and Information, 6(3):219-273. 

Mark Hepple. 1996. A compilation-chart method for linear categorical deduction. In Proceedings of COLING-96, Copenhagen. 
		
Max I. Kanovich. 1992. Horn programming in linear logic is NP-complete. In Seventh Annual IEEE Symposium on Logic in Computer Science, pages 200-210, Los Alamitos, California. IEEE Computer Society Press. 

Uwe Reyle. 1993. Dealing with ambiguities by underspecification: Construction, representation, and deduction. Journal of Semantics, 10:123-179.

