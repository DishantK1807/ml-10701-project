Proceedings of BioNLP Shared Task 2011 Workshop, pages 102–111,
Portland, Oregon, USA, 24 June, 2011. c©2011 Association for Computational Linguistics
BioNLP 201 Task Bacteria Biotope – The Alvis system 
Zorana Ratkovic
1,2
   Wiktoria Golik
1
    Piere Warnier
1
   Philipe Veber
1
   Claire Nédellec
1
 
1 MIG
INRA UR1077, Domaine de Vilvert 
F-850 Jouy-en-Josas, France 
forename.name@jouy.inra.fr 
 
 
2 LaTiCe
UMR 8094 CNRS Univ. Paris 3 
1 rue Maurice Arnoux 
F-92120 MONTROUGE 
 
 
Abstract 
This paper describes the system of the 
INRA Bibliome research group aplied to 
the Bacteria Biotope (B) task of the Bi-
oNLP 2011 shared tasks. Bacteria, geo-
graphical locations and host entities were 
procesed by a patern-based approach and 
domain lexical resources. For the extraction 
of environment locations, we propose a 
framework based on semantic analysis sup-
ported by an ontology of the biotope do-
main. Domain-specific rules were devel-
oped for dealing with Bacteria anaphora. 
Official results show that our Alvis system 
achieves the best performance of participat-
ing systems. 
1 Introduction

Given a set of Web pages, the information extrac-
tion goal of the Bacteria Biotope (B) task is to 
precisely identify bacteria and their locations and 
to relate them. The type of the predicted locations 
has to be selected among eight types. Among them 
the host and host-part locations have to be related 
by the part-of relation. Thre teams participated in 
the challenge. 
 
BB task example 
Ureaplasma parvum is a mycoplasma and a pathogenic 
ureolytic molicute which colonises 
 the urogenital tracts of humans. 
 
One of the specificities of the B task is that the 
bacteria location vocabulary is very large and vari-
ous as opposed to protein subcelular locations in 
biology chalenges (Kim et al., 2010) and geo-
graphical locations (Zhou et al., 205). Locations 
include natural environments and hosts as wel as 
food and medical locations. In order to deal with 
this heterogeneity, we propose a framework based 
on a term analysis of the test corpus and a shalow 
maping of these terms to a bacteria biotope (B) 
termino-ontology. This mapping derives the type of 
location terms and filters out non-location terms. 
Large external dictionaries of host names (i.e. 
NCBI taxonomy) and geographical names (i.e. 
Agrovoc thesaurus) complete the lexical resources. 
The high frequency of bacteria anaphora and 
ambiguous antecedent candidates in the corpus was 
also a dificulty. Our Alvis system implements an 
anaphora resolution algorithm that takes into con-
sideration the anaphoric distance and the position 
of the antecedent in the sentence. Alvis predicts the 
bacteria names and their relation to the locations 
with the help of hand-made paterns based on lin-
guistic analysis and lexical resources. 
The methods for predicting and typing locations 
(section 2) and bacteria (section 3) are first de-
scribed. Section 4 details the method for relating 
them. Section 5 comments the experimental results. 
2 Location

Our system handles separately the recognition of 
host and geographical names by dictionary map-
pings, while the recognition of locations of the en-
vironment and host part types is based on linguistic 
analysis and ontology inference. 
Host names and geographical names appeared to 
be easier to predict by using a named-entity recog-
nition strategy than the other types of location. 
They are les subject to variation than environ-
mental locations, which can include any physical 
feature. For host name extraction, we used the 
NCBI taxonomy as the major source. Only the eu-
karyote subtre was considered for host detection. 
Localization 
Part-of 
102
Our system filters out the ambiguous names such 
as Indicator (honeyguides) or Dialysis (xylophage 
insect) by comparing them to a list of comon 
words in English. The host name list was enriched 
with aditional common names including non-
taxonomic host groups (e.g. herbivores), progeny 
names (e.g. calf) and human categories (e.g. pa-
tient). The resulting host name list contains more 
than 1,80,00 scientifc naes and 60,0 com-
mon names. The geographical name recognition 
component uses a small dictionary of all geo-
graphic terms from the Agrovoc thesaurus sub-
vocabularies. At first, we considered using the very 
rich resource GeoNames. However, it contains to 
many ambiguous names to be directly usable by 
short-term development. 
2.1 Location
of Environment type 
The identification of environment locations is done 
in two steps. First, the automatic extraction of all 
candidate terms from the test corpus, then the as-
signment of a location type to these terms with the 
help of the Bacteria Biotope (BB) termino-
ontology. The type asigned to a given term is the 
type of the closest concept label in the ontology. 
Since the B termino-ontology was originally not 
structured acording to the eight types, in order to 
be usable it first had to be enriched by the new 
concepts and then mapped to this topology. 
Corpus term extraction. The corpus terms were 
automatically extracted by the AlvisNLP/ML pipe-
line (Nedellec et al., 208) with BioYatea (Nedel-
lec et al., 2010). BioYatea is the version of Yatea 
(Hamon & Aubin, 206) adapted to the biology 
domain. We modified BioYatea seting acording 
to the training dataset study. We observed that 
most of the location terms in the training dataset 
are noun phrases with adjective modifiers (e.g. ro-
dent nests) while prepositional phrases are rather 
rare (e.g. breaks in the skin). We set the term 
boundaries of BioYatea to include al prepositions 
except the of preposition. Considering other prepo-
sitions such as with may yield syntactic atachment 
errors, thus we prefer the risk of incomplete terms 
to incorect prepositional attachments. 
Bacteria Biotope ontology. We used the Bacte-
ria Biotope (BB) termino-ontology for typing the 
extracted terms. It is under development for the 
study of bacteria phenotypes and habitats. The high 
level of the habitat part is structured in a manner 
similar to that proposed by the one level classifica-
tion by Floyd (Floyd et al., 205). It has a fine-
grained structure with the same goal as the general-
ist EnvO habitat ontology (Field et al., 208), but it 
focuses on bacteria phenotype and biotope model-
ing. It includes a terminological level that records 
lexical forms of the concepts including terms, 
synonyms and variations. 
For the purpose of the chalenge, the initial on-
tology was manually completed using location 
concepts. The training corpus, as well as the habitat 
and isolation site fields of the GOLD database on 
sequenced prokaryotes (Liolios et al., 209) are the 
main sources of location terms and synonyms. The 
analysis of the training corpus mainly led to the 
addition of adjectival forms of host parts (e.g. lym-
phatic, intracelular) and human references (e.g. 
patient, infant, progeny). 
The GOLD database isolation site field is a very 
rich source of bacteria location terms. It is filed by 
natural language descriptions of maters, natural 
habitats, hosts and geographical locations. For in-
stance, the isolation site of Anoxybacilus flavi-
thermus bacterium is waste water drain at the 
Wairakei geothermal power station in New Zea-
land. The term analysis of GOLD isolation site en-
tries yielded 3,415 location terms including 1,050 
geographical names. Hundreds of these terms were 
manualy aded to the B termino-ontology. The 
lack of time as well as the full sentence structure of 
the GOLD resource prevented us from corectly 
handling them in a fully automatic way. We are 
currently developing a method for the automatic 
alignment of the terms extracted from GOLD to the 
BB termino-ontology. Additionally, the GOLD 
habitat field provided around a hundred diferent 
terms that have ben directly integrated into the B 
termino-ontology. 
The curent version of the habitat subpart of the 
BB termino-ontology contains 1,247 concepts and 
266 synonyms.  
Location types in Bacteria Biotope ontology. 
The B termino-ontology has been developed pre-
vious to the B task and the structure of its habitat 
subpart does not reflect the eight location types of 
the task. In order to reuse the ontology for the B 
task, we assigned types to each location concept. 
We manualy asociated the high level nodes of the 
location hierarchies to the eight B task types. The 
types of the lower level concepts were then auto-
maticaly infered. For instance, the concept 
aquatic environment is taged Water in the ontol-
103
ogy and al of its descendants lake, sea, ocean are 
of type Water as wel. Local type exceptions were 
manualy taged. For instance, the waste tre in-
cludes water-carried wastes of type Water and solid 
industrial residues of type Environment. This way 
all concepts in the resulting typed ontology were 
assigned a unique type. The concept types are then 
propagated to their asociated term clases at the 
terminological level. For instance, underground 
water and its synonym subterranean water are both 
typed as Water. The resulting typed B termino-
ontology is then usable for deriving the types of the 
terms extracted from the test corpus. 
Derivation of location type. The B termino-
ontology scope is too limited for the correct predic-
tion of all candidate term types by Bolean and 
exact comparison. From the 2,290 candidate terms 
of the test corpus, only 152 belong as such to the 
BB termino-ontology. We propose a method based 
on the head comparison of the candidate and B 
terms for the derivation of the candidate term type. 
The quality of the ontology-based annotation 
depends to a large extent on an accurate match be-
twen the resource and the terms extracted from the 
corpus. Our method targets the syntactic structure 
of terms (candidate and B terms) in order to gath-
er the most of semantically similar terms. This 
approach differs from the ontology alignment and 
population methods that also use the information 
from the ontology structure in order to infer seman-
tic relationships (e.g. hyponyms, meronyms) (Eu-
zenat, 207). It also difers from semantic anota-
tion suported by context analysis such as distribu-
tional semantics (Grefenstette, 194) or Hearst pat-
terns (Hearst, 192). It belongs to the clas of 
methods that focus on the morphology of the cor-
pus terms, which use string-based (Levensthein, 
1966, Jaro, 1989) or linguistic-based methods (Jac-
quemin & Tzoukermann, 199). 
Even though the context-based approach should 
produce very good results, we chose a les time-
consuming method that is easier and faster to set 
up, which is based on morphosyntactic analysis.  In 
our case, string similarity measures turn out to be 
irrelevant (laboratory rat does not mean rat labo-
ratory). We observed that in candidate and BB 
terms, the head is very often the most informative 
element. Thus, the linguistic-based analysis of 
terms, in particular the head-similarity analysis 
(Hamon & Nazarenko, 201), represents a promis-
ing altertive. Our method i iied by 
MetaMap (Aronson, 2001). MetaMap tags bio-
medical corpora with the UMLS Metathesaurus by 
syntactic analysis that takes into acount lexical 
heads of terms. The similarity scores computed by 
linguistically-based metrics are higher for terms 
whose heads have previously ben analyzed.  
The MetaMap method includes a variant compu-
tation that maps acronyms, abbreviations, syno-
nyms as wel as derivational, inflectional and spell-
ing variants. Our term typing method is les sophis-
ticated and uses a few lexical variants due to the 
lack of a complete resource. Some ontology en-
richment aplications also use head-suported term 
matching, as in Desmontils (Desmontils et al, 
2003). In Desmontils, new concepts belonging to 
WordNet (Felbaum, 198) are automatically added 
to the ontology in order to improve the indexing 
proces. However, the analysis of the results shows 
that a great number of concepts found in the texts 
are not considered because they do not exist in 
WordNet. Our typing task uses a similar head-
based method, but only for type derivation. 
Our system derives the location type of candi-
date terms in several steps. First, if there is a term 
in the B termino-ontology that is strictly equal to 
the candidate term, it is asigned the same type. 
Then, the other candidate terms are assigned types 
according to the comparison of their heads to the 
BB term heads. We asume that in most of the 
cases the term head conveys the information about 
the type and is non-ambiguous. A given head H is 
non-ambiguous if all B terms with head H are of 
the same type. The location term head set is the set 
of al habitat term heads found in the B termino-
ontology. The curent version contains 693 difer-
ent heads. Let T
e
 
denote the extracted term to be 
typed. If the head of T
e
 does not belong to the BB 
term head set, then the type of T
e
 is simply not Lo-
cation (e.g. high metabolic diversity). If T
e
 head 
does belong to the BB term head set and the head is 
non-ambiguous, then T
e
 is asigned the associated 
type. For instance, the head of the extracted term 
stratified lake is lake. The type of al the BB terms 
with lake head is Water (e.g. meromictic lake). 
Stratified lake is therefore typed as Water.  
Specific procesing is aplied to terms with am-
biguous heads. The associative set of B term 
heads and types exhibits some cases of ambiguous 
heads with multiple types that we analyzed in de-
tail. There are two kinds of ambiguities that were 
104
procesed in diferent ways. In the first, multiple 
types reflect different roles of the same object. In 
the second, the head is non-informative with re-
spect to the type. In the later case the type is con-
veyed by the subterm (term after head removal). 
We qualify non-informative B term heads as neu-
tral. They mainly denote habitats (habitat, envi-
ronment, medium, zone) and extracts (sample, sur-
face, isolate, material, content). In this case, the 
type is derived from the subterm. For instance, the 
head isolate of the extracted term marine isolate is 
neutral. After head removal, it is assigned the type 
Water since marine is of type Water. Freshwater 
has the same type as freshwater medium or fresh-
water environment since medium and environment 
are neutral heads.  
Some heads have more than one type although 
they denote specific locations. Their multiple types 
reflect diferent uses or states. For instance, the 
head bottle has two types: Food and Medical. The 
type Food is derived from the B concept water 
bottle and the type Medical is derived from bedside 
water botles in a hospital environment. The correct 
type for the extracted terms is then selected by a set 
of paterns based on the context of the term in the 
document. For instance, many vegetables and 
meats could be either of type Host or Food. The 
type is Host by default. One pattern states that if a 
term includes or is preceded by a fod processing-
related word (e.g. cooked, griled, fermented), then 
the term is reasigned the type Food. Another pat-
tern states that if a host is preceded by a death-
related adjective (dead, decaying), then its type 
should be revised as Environment.  
Our system currently includes nine disambigua-
tion/retyping patterns. The first version of the type 
derivation method was automaticaly applied to the 
1,263 GOLD terms after head analysis. Manual 
examination of the results yielded an extension of 
the two lists of neutral heads and heads with am-
biguous types. There are 20 neutral heads and 21 
ambiguous heads in the current version of the B 
termino-ontology. The head-matching algorithm 
appears to be quite productive for the biotope 
terms. The procedure aplied to the test corpus 
yielded the following figures: BioYatea extracted 
2,290 terms. 416 terms matching th pos-
procesing filters were discarded. This includes 
terms which are too general (i.e. approach, diver-
sity), terms containing irrelevant or non desirable 
adjectives (i.e. numerous deficiencies, known spe-
cies) and terms containing forbidden words acord-
ing to the anotation location rules (i.e. bacteria, 
pathogen, contaminated, parasite). Finaly, 1,873 
candidate terms were kept. 
Among these figures: 
152  terms belong to the B termino-ontology 
90  terms were typed using the ontology heads 
6 terms with several types were handled by 
disambiguation paterns. 
We plan to extend the list of neutral heads and dis-
criminate adjectives for type disambiguation by 
machine learning clasification aplied to the B 
termino-ontology modifiers. 
Location entity boundary. The analysis of term 
extraction result from the training corpus shows 
that the predicted boundaries of locations were not 
fuly consistent with the task anotation guidelines. 
Post-procesing adjusts incorrect boundaries by 
filtering irelevant words, packing and merging 
terms. Irelevant words (e.g. contaminated, in-
fected, host species, disease, inflamation) were 
removed from the location candidate terms inde-
pendently of their types (e.g. contaminated Bach-
man Road site vs. Bachman Road ; host plant vs. 
plant). Note that BioYatea extracts not only the 
maximum terms (e.g. contaminated Bachman Road 
site), but also their constituents (Bachman Road 
site, Bachman Road and site). Boundary adjust-
ment often consists in selecting the relevant alter-
native among the subterms.  
Other boundary isues are handled by several 
paterns, which are applied after the typing stage. 
These paterns are type-dependent: each pattern 
only applies to one type or a subset of location 
types. When necessary, they shift the boundaries in 
order to include relevant modifiers. They also split 
location terms or join adjacent location terms. 
BioYatea may have mised relevant modifiers be-
cause of POS-taging erors. For instance, if a na-
tionality name precedes a location, then it is in-
cluded (e.g. German oil field). Also, it frequently 
happens that hosts are modifiers of host parts (e.g. 
insect gut). BioYatea extracts the whole term and 
its constituents. The term is corectly typed as 
Host-part and the host modifier as Host. In order to 
avoid embedded locations, a specific pattern is de-
voted to the spliting of these terms. In this way 
insect gut (Host-part) becomes insect (Host) and 
gut (Host-part). 
Most of these paterns involve several specific 
lexicons, including cardinal directions, relevant and 
105
irrelevant modifiers for each type of location, as 
wel as types, which can be merged and split. The 
curent resources were manually built by examin-
ing the location terms of the training set and GOLD 
isolation fields. The acquisition of relevant and 
irrelevant modifiers could be automated by ma-
chine learning. Some linguistic phenomena could 
be beter handled by the customization of BioYa-
tea. For instance BioYatea considers the preposi-
tion with as a term boundary so it cannot extract 
terms containing with, like areas with high sulfur 
and salt concentrations. 
3 Extraction
of Bacteria names 
We observed in the training corpus that not only 
were bacteria names taged, but also higher level 
taxa (families) and lower level taxa (strains). We 
used the NCBI taxonomy as the main bacteria tax-
on resource since it includes al organism levels 
and is kept up-to-date. This bacteria dictionary was 
enriched by taxa from the training corpus, in par-
ticular by non standard abbreviations (e.g. Chl. = 
Chlorobium, ssp. = subsp) and plurals, (Vibrios as 
the plural for Vibrio) that were hopefuly rather 
rare. 
Determining the boundaries of the bacteria 
names was one of the main isues because corpus 
strain names do not always folow conventional 
nomenclature rules. Also, the recognition of bacte-
ria name is evaluated using a strict exact match. 
Paterns were developed to account for such cases. 
They handle inversion (LB40 of Burkholderia xe-
novorans istad of Burkholderia xenovorans 
LB40) and parenthesis (Tropheryma whipplei (the 
Twist strain) instead of Tropheryma whiplei strain 
Twist).  The corpus also mentions names of bacte-
ria that contain modifiers not found in the NCBI 
dictionary, such as antimicrobial-resistant C. coli 
or L. pneumophila serogroup 1. Such cases, as well 
as abbreviations (e.g. GSB for gren sulfur bacte-
ria) and partial strain names (e.g. strain DSMZ 245 
T for Chlorobium limicola strain DSMZ 245 T) 
were also specifically handled. 
The main source of eror in bacteria name pre-
diction is due to the mixture of family names and 
strain name abreviations in the same text. It fre-
quently happens that the strain name is abbreviated 
into the first word of the name. For instance Bar-
tonella henselae is abreviated as Bartonela. Un-
fortunately, Bartonela is a genus mentioned in the 
same text, thus yielding ambiguities betwen the 
anaphora and the family name, which are identical. 
3.1 Bacteria
anaphora resolution 
Anaphors are frequent in the text, especially for 
bacteria reference and to a smaler extent for host 
reference. Our effort focused on bacteria anaphora 
resolution ignoring host anaphora. The extraction 
method of location relations (section 4) asumes 
that the relation arguments, location and bacterium 
(or anaphora of the bacterium) ocur in the same 
sentence. From a total of 2,296 sentences in the 
training corpus, only 363 sentences contain both 
the location and the explicit bacterium, while 574 
mention only the location. Two thirds of the loca-
tions do not co-occur with bacteria. This demon-
strates the importance of recovering the bacteria for 
these cases, which is potentially refered to by an 
explicit anaphora. 
The manual examination of the training corpus 
showed that the most frequent anaphora of bacteria 
are not pronouns but higher level taxa, often pre-
ceded by a demonstrative determinant, (i.e. This 
bacteria, This Clostridium) and sortal anaphora 
(i.e. genus, organism, species and strain), both of 
which are comonly found in biological texts 
(Torri & Vijay-Shanker, 207). The style of some 
of the documents is rather relaxed and the antece-
dent may be ambiguous even for a human reader. 
We observed thre types of anaphora in the corpus. 
First, the standard anaphora which includes both 
pronouns and sortal anaphora, which requires a 
unique bacterial antecedent. Second, bi-anaphora 
or an anaphora that requires two bacteria antece-
dents. This happens when the properties of two 
strains are compared in the document. Finaly, the 
case of a higher taxon being used to refer to a 
lower taxon, which we named name taxon anaph-
ora. 
 
Anaphora with a unique antecedent 
C. coli is pathogenic in animals and humans. Peo-
ple usually get infected by eating poultry that con-
tained the bacteria, eating raw fod, drinking raw 
milk, and drinking bottle water […]. 
 
Anaphora with two antecedents 
C. coli is usually found hand in hand with its bac-
teria relative, C. jejuni. These two organisms are 
recognized as the two most leading causes of acute 
inflammation of intestine in the United States and 
other nations. 
106
 
Name taxon anaphora 
Ticks become infected with Borelia dutoni while 
feeding on an infected rodent. Borelia then multi-
plies rapidly, causing a generalized infection 
throughout the tick. 
 
For anaphora detection and resolution a patern-
based approach was prefered to machine learning 
because the constraints for relating anaphora to 
antecedent candidates of the same taxonomy level 
were mainly semantic and domain-dependent and 
the anotation of anaphora was not provided in the 
training corpus.  
Anaphora detection consists of identifying po-
tential anaphora in the corpus, given a list of pro-
nouns, sortal anaphora and taxa and then filtering 
out irelevant cases (Segura-Bedmar et al., 2010, 
Lin & Lian, 204) before anaphora resolution. Not 
all the pronouns, sortal anaphora terms and higher 
taxon bacteria are anaphoric. For example, if a 
higher taxon is preceded or followed by the word 
genus, this signals that it is not anaphoric but that 
the text is actually about the higher taxon. 
 
Non-anaphoric higher taxon 
Burkholderia cenocepacia HI2424[…] 
The genus Burkholderia consists of some 35 bacte-
rial species, most of which are soil saprophytes 
and phytopathogens that occupy a wide range of 
environmental niches. 
 
The anaphora resolution algorithm takes into ac-
count two features: the distance to the antecedent 
candidate and its position in the sentence. The an-
tecedent is usually found in proximity to the ana-
phora, in order to maintain the coherence of the 
text. Therefore, our method ranks the antecedent 
candidates according to the anaphoric distance 
counted in sentences. 
If more than one bacterium is found in a given 
sentence, their position is discriminate. Centering 
theory states that in a sentence the most prominent 
entities and therefore the most probable antecedent 
candidates are in the order: subject > object > other 
position (Grosz et al., 195). In English, due to the 
SVO order of the language the subject is most of-
ten found at the beginning of the sentence, fol-
lowed by the object and the others. Therefore, the 
method retains the leftmost bacterium in the sen-
tence when searching for the best antecedent can-
didate. 
More precisely, the method selects the first ante-
cedent that it finds according to the folowing pre-
cedence list: 
First bacterium in the curent sentence (s) 
First bacterium in the previous sentence 
  (s-1) 
First bacterium in sentence s-2 
First bacterium in sentence s-3 
First bacterium in the curent paragraph 
Last bacterium in the previous paragraph 
First bacterium in the first sentence of the 
document 
The first bacterium ever mentioned. 
 
The method only relates anaphora to antecedents 
that are found before. It does not handle cataphors 
since they are rarely found in the corpus. For ana-
phors that require two antecedents we use the same 
criteria but search for two bacteria in each sentence 
or paragraph, instead of one. For taxon anaphora 
we lok for the presence of a lower taxon in the 
document found before the anaphora that is com-
patible acording to the species taxonomy. 
The counts of anaphora detected by the paterns are 
given in Table 1. 
 
Corpus Single ante Bi ante Taxon ante 
Train 933 4 129 
Dev 204 3 22 
Test 240 0 18 
Total 1,377 7 169 
 
Table 1. The count of the types of anaphora per corpus. 
 
The anaphora resolution algorithm alowed us to 
retrieve more sentences that contain both a bacte-
rium and a location. Out of the 574 sentences that 
contain only a location, 436 were found to contain 
an anaphora related to at least one bacterium. The 
remaining 138 sentences are cases where there is 
no bacterial anaphora or the bacterium name is im-
plicit. It frequently happens that the bacterium is 
referred to through its action. For example in the 
sentence below, the bacterium name could be de-
rived from the name of the disease that it causes. 
 
In the 160s anthrax was known as the "Black 
bane" and kiled over 60,000 cows. 
 
One of the questions we had about the resolution 
of anaphora is whether anaphora that are found in 
the same sentence together with a bacterium (there-
fore potentialy its antecedent) should be consid-
107
ered or not. We tested this on the development set. 
We found that removing such anaphora from con-
sideration improved the overal score. It yielded an 
F-score of 53.2% (precision: 46.17%, recal: 
62.81%), compared to the original F-score of 
50.15% (precision: 41.06%, recal: 64.44%). This 
improvement in F-score is solely due to an increase 
in precision, which shows that while resolving 
anaphora is important and required, the incorect 
recognition of terms as anaphora and incorrect 
anaphora resolution can introduce noise. 
4 Relation
extraction 
In this work we concentrated most of our efort on 
the prediction of entities. For the prediction of 
events we used a strategy based on the co-
occurrence of  arguments and triger words within 
a sentence: 
If a bacteria name, a location and a triger word 
are present in a sentence, then the system pre-
dicts a Localization event betwen the bacte-
rium and the location. 
If a bacteria anaphora, a location and a trigger 
word are present in a sentence, then the system 
predicts a Localization event betwen each ana-
phora antecedent and the location. 
If a host, a host part, a bacterium and at least 
one trigger word are present in a sentence, then 
the system predicts a PartOf event betwen the 
host and the host part. 
 
The list of triger words contains 20 verbs (e.g. 
inhabit, colonize, but also discover, isolate), 16 
disease markers (e.g. chronic, pathogen) and 19 
other relevant words (e.g. ingest, environment, 
niche). This list was designed by ranking words in 
the sentences of the training corpus containing both 
a bacteria name and a location. The ranking crite-
rion used was the information gain with respect to 
whether the sentence contained an event or not. 
The ranked list was adjusted by removing spurious 
words and ading domain knowledge words. 
By removing the constraint of the ocurrence of 
a triger word in the sentence, we can determine 
that the maximum recal the method can achieve 
with this strategy is 47% (precision: 41%, F-score: 
44%). The selected trigger word list yielded a re-
call close to the maximum, thus it sems that the 
trigger words do not afect the recall and are suit-
able for the task. 
5 Results

Table 2 sumarizes the oficial scores that the Bib-
liome Alvis system achieved for the Bacteria 
Biotope Task. It ranked first among thre partici-
pants. The first column gives the recal of entity 
prediction. The prediction of hosts and bacteria 
named-entities achieved a god recall of 84 and 82, 
respectively. 
 
 
Entity 
recal 
Event 
recal 
Event 
Precis. 
F-score 
Bacteria 84 
Host 82 61 48 53 
Host part 72 53 42 47 
Env. 53 29 24 26 
Geo. 29 13 38 19 
Food 29 41 
Medical 100 50 33 40 
Water 83 60 55 57 
Soil 86 69 59 63 
Total  45 45 45 
 
Table 2. Bibliome system scores at Bacteria Biotope 
Task in BioNLP shared tasks 201. 
 
However, geographical locations based on a similar 
strategy were porly predicted (29%). Our system 
predicted only 15 countries. A more apropriate 
resource of geographical names than the Agrovoc 
thesaurus would certainly increase the recall of 
geographical locations.  
The host parts, medical, water and soil locations 
predicted with the same ontology-based method 
were surprisingly god with a recal of 72, 10, 83 
and 86, respectively. The smal size of the ontology 
and the small number of diferent term heads (i.e. 
51 diferent heads) initialy appeared as a limitation 
factor for reuse on new corpora. The god recall 
shows that the location vocabulary of the test set 
has similarities with the training set compared to 
potential space of location names.  The potential 
space is reflected by the richness of the GOLD iso-
lation site field. This demonstrates the robustnes 
of the type derivation approach based on term 
heads. The correctnes of the derivation type can-
not be calculated without a corpus where al the 
locations and not only bacteria ones are anotated. 
The recal of the environment location prediction is 
a litle bit lower, 53%. The environment type in-
108
cludes many diferent types that cannot all be an-
ticipated. Therefore the coverage of the B ter-
mino-ontology environment part is limited except 
for water and soil, which are more focused topics.  
The localization event recal (column 2) is on 
average 20% lower for all types than the location 
entity recall. The regularity of the difference may 
sugest that once the argument is identified, the 
localization relation is equally harder to find by our 
method independently of the type. The localization 
event precision (column 3) is more dificult to ana-
lyze because many sources of eror may be in-
volved, such as an incorrect arguments, incorrect 
anaphora resolution, relation to the wrong bacte-
rium among several or the absence of a relation. 
The prediction precision of localization events 
involving soil, water and host is beter than envi-
ronment and fod. The manual analysis of the test 
corpus shows that in some cases environmental 
locations were mentioned as potential sources of 
industrial aplications without actually being bac-
teria isolation places. For instance, in Other fields 
of application for thermostable enzymes are starch-
procesing, organic synthesis, diagnostics, waste 
treatment, pulp and paper manufacture, and ani-
mal fed and human fod, the Alvis system erone-
ously predicted waste treatment, paper manufac-
ture, animal fed and human food.  This is due to 
the fact that the system does not handle modalities. 
Such hypotheses are specific to the B task text 
genre, i.e. Bacteria sequencing projects. Such pro-
jects contain details for potential industrial aplica-
tions, which are absent from academic literature. 
Ambiguous types are also a source of eror. De-
spite the host dictionary cleaning, some ambigui-
ties remained. For example, the head canal in tooth 
root canal is eroneously typed as water and should 
be disambiguated with its tooth host-part modifier. 
After test publication we measured the gain of 
anaphora resolution by using the on-line service. 
The anaphora resolution algorithm was found to 
have a strong impact on the final result.  Runing 
the test set using all of the modules except for the 
anaphora resolution algorithm yielded a decrease in 
the F-score by almost 13% (F-score: 32.5%, preci-
sion: 48.5%, 24.4%). This shows that the adition 
of an anaphora resolution algorithm significantly 
increases the precision and that a resolution algo-
rithm adapted to the Bacteria domain is necesary 
for the Biotope corpus. 
The part-of event prediction relies on the strict 
co-occurrence of a bacterium, triger word, host 
and host part within a sentence. An aditional run 
with the more relaxed constraint where the bacte-
rium can be denoted by an anaphora as wel 
yielded a gain of 6 recal points, a loss of 5 preci-
sion points with a net benefit of 1 F-measure point. 
6 Discusion

The use of triger words for the selection of sen-
tences for relation extraction does not take into ac-
count the structure or syntax of the sentence for the 
prediction of relation arguments. The system pre-
dicts all combinations of bacteria and locations as 
localization events and all combination of host and 
host parts as part-of event. This has a negative ef-
fect on the precision measure since some pairs are 
irrelevant as in the sentence below. 
 
Baumania cicadelinicola. This newly discovered or-
ganism is an obligate endosymbiont of the leafhopper 
insect Homalodisca coagulata (Say), also known as the 
Glasy-Winged Sharpshoter, which feeds on the xylem 
of plants. 
 
It has ben shown that the use of syntactic de-
pendencies to extract biological events (such as 
protein-protein interactions) improves the results of 
such systems (Erkan et al., 207, Manine et al., 
2008, Airola et al. 208). The use of syntactic de-
pendencies could offer a more in depth examina-
tion of the syntax and the semantics and therefore 
allow for a more refined extraction of bacteria-
localization and host-host part relations.   
Term extraction apears to be a god method for 
predicting locations including unsen terms, but it 
is limited by the typing strategy that filters out al 
terms with unknown heads (with respect to the BB 
termino-ontology). In the future, we wil study the 
effect of linguistic markers such as enumeration 
and exemplification structures for recovering addi-
tional location terms. For instance, in heated or-
ganic materials such as compost heaps, rotting 
hay, manure piles or mushroom growth medium, 
our system has correctly typed heated organic ma-
terials as environment but not the other examples 
because of their unknown heads. 
The promising performance of the Alvis system on 
the B task shows that a combination of seantic 
analysis and domain-adapted resources is a god 
strategy for information extraction in the biology 
domain. 
109
References 
Agrovoc: http:/aims.fao.org/website/AGROVOC-
Thesaurus 
Anti Airola, Sampo Pysalo, Jari Björne, Tapio Pah-
nikkala, Filp Ginter, and Tapio Salakoski. 2008. A 
Graph Kernel for Protein-Protein Interaction Extrac-
tion. BioNLP208: Curent Trends in Biomedical 
Natural Language Procesing, pages 1-9. 
Alan R. Aronson. 2001. Efective maping of biomedi-
cal text to the UMLS Metathesaurus: The MetaMap 
program. Procedings of AMIA Symposium 201, 
pages 17-21. 
Emmanuel Desmontils, Christine Jacquin and  Laurent 
Simon. 2003. Ontology enrichment and indexing 
proces. Research report R-IRIN-03.05, Institut de 
Recherche en Informatique de Nantes, Nantes, Fran-
ce. 
Jérôme Euzenat and Pavel Shvaiko. 2007. Ontology 
matching, Springer Verlag, Heidelberg (DE),page 
333. 
Dawn Field et al. 2008. Towards a richer description of 
our complete colection of genomes and metage-
nomes: the Minimum Information about a Genome 
Sequence (IGS) specification. Nature Biotechnol-
ogy 26, pages 541-547. 
GeoNames: htp:/www.geonames.org/ 
Gregory Grefenstete. 1994. Explorations in Automatic 
Thesaurus Discovery.  Natural Language Procesing 
and Machine Translation. London: Kluwer Academic 
Publishers. 
Barbara J. Grosz, Araving K. Joshi and Scot Weinstein. 
1995. Centering: A Framework for Modelling the Lo-
cal Coherence of Discourse.  University of Pennsyl-
vania Institute for Research in Cognitive Science 
Technical Reports Series. 
Güneş Erkan, Arzucan Özgür and Dragomir R. Radev. 
2007. Semi-Supervised Clasification for Extracting 
Protein Interaction Sentences using Dependency 
Parsing. Procedings of the 2007 Joint Conference on 
Empirical Methods in Natural Language Procesing 
and Computational Natural Language Learning, pag-
es 28-237. 
Thiery Hamon and Sophie Aubin. 2006. Improving 
term extraction with terminological resources. In 
Salakoski, T. et al., editors, Advances in Natural Lan-
guage Procesing 5th International Conference on 
NLP (FinTAL’06), pages 380–387. Springer. 
Thiery Hamon and Adeline Nazarenko. 2001. Detection 
of synonymy links betwen terms: experiment and re-
sults, Recent Advances in Computational Terminol-
ogy. Pages 185-208. John Benjamins. 
Marti A. Hearst. 1992. Automatic acquisition of hypo-
nyms from large text corpora. In Zampolli, A.(ed.), 
Procedings of the 14 th COLING, pages 539–545, 
Nantes, France. 
Christian Jacquemin and Evelyne Tzoukerman. 1999. 
NLP for term variant extraction: A synergy of mor-
phology, lexicon, and syntax. In Strzalkowski, T. 
(ed.), Natural language information retrieval, volume 
7 of Text, speech and language technology, chapter 2, 
pages  25–74. Dordrecht & Boston: Kluwer Aca-
demic Publishers. 
Mathew A. Jaro. 1989. Advances in record linkage me-
thodology as aplied to matching the 1985 census of 
Tampa, Florida. Journal of the American Statistical 
Association 84(406), pages 414-20. 
Jin-Dong Kim, Tomoko Ohta, Sampo Pysalo, Yoshi-
nobu Kano and Jun’ichi Tsuji. (to apear). Extract-
ing bio-molecular events from literature the Bi-
oNLP’09 shared task. Special isue of the Interna-
tional Journal of Computational Intelligence. 
Vladimir I. Levenshtein. 1966. Binary codes capable of 
corecting deletions, insertions and reversals. Dok-
lady akademii nauk SSR, 163(4):845-848, 1965. In 
Rusian. English translation in Soviet Physics Dok-
lady, 10(8), pages 707-710. 
Yu-Hsiang Lin and Tyne Liang. 2004. Pronomial and 
Sortal Anaphora Resolution for Biomedical Litera-
ture. In Procedings of ROCLING XVI: Conference 
on Computational Linguistics and Speech Procesing. 
Konstantinos Liolios, I-Min A. Chen., Konstantinos 
Mavromatis, Nektarios Tavernarakis, Philip Hugen-
holtz, Victor M. Markowitz and Nikos C. Kyrpides. 
2009. The Genomes On Line Database (GOLD) in 
2009: status of genomic and metagenomic projects 
and their associated metadata. NAR Epub. 
Alain-Piere Manine, Erick Alphonse and Philipe Bes-
sières. 208. Information extraction as an ontolgy 
population task and its aplication to genic interac-
tions, 20th IEE Intl. Conf. Tols with Artificial In-
telligence, ICTAI'08., vol. II, pp. 74-81. 
NCBI taxonomy: 
http:/ww.ncbi.nlm.nih.gov/Taxonomy/ 
 Claire Nédellec, Wiktoria Golik, Sophie Aubin and 
Robert Bosy. 2010. Building Large Lexicalized On-
tologies from Text: a Use Case in Indexing Biotech-
nology Patents, International Conference on Knowl-
edge Engineering and Knowledge Management 
(EKAW 2010), Lisbon, Portugal. 
110
Isabel Segura-Bedmar, Mario Crespo, César de De Pa-
blo-Sánchez and Paloma Martínez. 2010. Resolving 
anaphoras for the extraction of drug-drug interactions 
in pharmacological documents. BMC Bioinformatics 
11(Supl 2):S1. 
Manabu Tori and K. Vijay-Shanker. 2007. Sortal 
Anaphora Resolution in Medline Abstracts. Computa-
tional Intelligence 23, pages 15-27. 
Zhou GuoDong, Su Jian, Zhang Jie and Zhang Min. 
2005. Exploring Various Knowledge in Relation Ex-
traction. In Procedings of the 43rd Anual Meting 
of the ACL, pages 427-434, Ann Arbor. Asociation 
for Computational Linguistics. 
 
111

