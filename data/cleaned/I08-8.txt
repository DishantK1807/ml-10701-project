1:167	IJCNLP 2008 Workshop on Technologies and Corpora for Asia-Pacific Speech Translation (TCAST) Proceedings of the Workshop Organizer Asian Speech Translation Advanced Research Consortium (A-Star) Local Host International Institute of Information Technology, India January 11 2008 Hyderabad, India 2008 Asian Federation of Natural Language Processing Preface This volume contains the paper accepted for presentation at the 2008 Workshop on Technologies and Corpora for Asia-Pacific Speech Translation (TCAST), which is part of the The Third International Joint Conference on Natural Language Processing held on January 7-12, 2008, in Hyderabad, India (IJCNLP2008).
2:167	This workshop took place on January 11 2008.
3:167	In an age of global communication, information exchange by means of speech-to-speech technology is playing an increasingly important role.
4:167	This technology is vital in breaking down language barriers and facilitating better social interaction and exchange in business and other areas.
5:167	Research programs have been launched in many different countries and efforts have been made to develop successful speech-to-speech systems for several languages around the world.
6:167	In the Asia-Pacific region, extensive efforts are needed to develop the field.
7:167	In the region a large number of languages and dialects are spoken, some of these languages have a very rich cultural heritage.
8:167	However, many of these languages have been neglected and information resources are not available.
9:167	Given this background, the objective of the workshop was to present the research and development work currently in progress for the development of corpora, data tools and techniques for the processing of Asian languages and their standardisation for applications in speech translation between Asian languages.
10:167	The main aims of this workshop were to allow participants to interact and share knowledge of available resources and ongoing research, and to discuss possible avenues for future development in the field.
11:167	This workshop was a part of the activities of the expert group on Speech and Natural Language Processing created under the ASTAP program, APEC-TEL and the A-Star project.
12:167	We would like to acknowledge the exceptional cooperation of our organizing committee members during the organization of this workshop.
13:167	Andrew Finch Workshop Organizer November 2007 Organization Workshop Chair:  Satoshi Nakamura (NiCT-ATR, Japan) Organizing Committee:  Satoshi Nakamura (NiCT-ATR, Japan)  Andrew Finch (NiCT-ATR, Japan)  Sakriani Sakti (NiCT-ATR, Japan) Program Committee:  Satoshi Nakamura (NiCT-ATR, Japan)  S.S. Agrawal (CDAC, India)  Hammam Riza (BPPT, Indonesia)  Jun Park (ETRI, Korea)  Chai Wutiwiwatchai (NECTEC, Thailand)  Bo Xu (CAS, China)  Linshan Lee (NTU, Taipei) Workshop Website:  http://www.slc.atr.jp/TCAST/TCAST2008/TCAST_Home.html Workshop Program  09:00-09:30  Workshop Registration  09:30-10:00  Opening Speech     Satoshi Nakamura (NiCT-ATR, Japan)  Session 1: Machine Translation  10:00-10:30  Transformation-based Sentence Splitting method for     Statistical Machine Translation     Jonghoon Lee, Donghyeon Lee and Gary Geunbae Lee  10:30-11:00  Coffee Break  11:00-11:30  Speech-to-Speech Translation Activities in Thailand     Chai Wutiwiwatchai, Thepchai Supnithi and     Krit Kosawat  11:30-12:00  Phrase-based Machine Transliteration     Andrew Finch and Eiichiro Sumita  12:00-13:30  Lunch  Session 2: Speech Recognition  13:30-14:00  Development of Indonesian Large Vocabulary Continuous     Speech Recognition System within A-STAR Project     Sakriani Sakti, Eka Kelana, Hammam Riza, Shinsuke     Sakai, Konstantin Markov and Satoshi Nakamura  14:00-14:30  Using Confidence Vector in Multi-Stage Speech Recognition     Hyungbae Jeon, Kyuwoong Hwang, Hoon Chung,     Seunghi Kim, Jun Park and Yunkeun Lee  14:30-15:00  Toward Asian Speech Translation System: Developing     Speech Recognition and Machine Translation for     Indonesian Language     Hammam Riza and Oskar Riandi  15:00-15:45  Discussion and Closing Table of Contents Transformation-based Sentence Splitting method for Satistical Machine Translation Jonghoon Lee, Donghyeon Lee and Gary Geunbae Lee 1 Speech-to-Speech Translation Activities in Thailand Chai Wutiwiwatchai, Thepchai Supnithi and Krit Kosawat 7 Phrase-based Machine Transliteration Andrew Finch and Eiichiro Sumita 13 Development of Indonesian Large Vocabulary Continuous Speech Recognition System within A-STAR Project Sakriani Sakti, Eka Kelana, Hammam Riza, Shinsuke Sakai, Konstantin Markov and Satoshi Nakamura 19 Using Confidence Vector in Multi-Stage Speech Recognition Hyungbae Jeon, Kyuwoong Hwang, Hoon Chung, Seunghi Kim, Jun Park and Yunkeun Lee 25 Toward Asian Speech Translation System: Developing Speech Recognition and Machine Translation for Indonesian Language Hammam Riza and Oskar Riandi 35 Author Index Hoon Chung 25 Andrew Finch 13 Kyuwoong Hwang 25 Hyungbae Jeon 25 Eka Kelana 19 Seunghi Kim 25 Krit Kosawat 7 Donghyeon Lee 1 Geunbae Lee 1 Jonghoon Lee 1 Yunkeun Lee 25 Konstantin Markov 19 Satoshi Nakamura 19 Jun Park 25 Oskar Riandi 35 Hammam Riza 19,35 Shinsuke Sakai 19 Sakriani Sakti 19 Eiichiro Sumita 13 Thepchai Supnithi 7 Chai Wutiwiwatchai 7 A Transformation-based Sentence Splitting Method for Statistical Machine Translation Jonghoon Lee, Donghyeon Lee and Gary Geunbae Lee Department of Computer Science and Engineering Pohang University of Science & Technology (POSTECH) {jh21983, semko, gblee}@postech.ac.kr    Abstract We propose a transformation based sentence splitting method for statistical machine translation.
14:167	Transformations are expanded to improve machine translation quality after automatically obtained from manually split corpus.
15:167	Through a series of experiments we show that the transformation based sentence splitting is effective pre-processing to long sentence translation.
16:167	1 Introduction Statistical approaches to machine translation have been studied actively, after the formalism of statistical machine translation (SMT) is proposed by Brown et al.17:167	(1993).
18:167	Although many approaches of them were effective, there are still lots of problems to solve.
19:167	Among others, we have an interest in the problems occurring with long sentence decoding.
20:167	Various problems occur when we try to translate long input sentences because a longer sentence contains more possibilities of selecting translation options and reordering phrases.
21:167	However, reordering models in traditional phrase-based systems are not sufficient to treat such complex cases when we translate long sentences (Koehn et al, 2003).
22:167	Some methods which can offer powerful reordering policies have been proposed like syntax based machine translation (Yamada and Knight, 2001) and Inversion Transduction Grammar (Wu, 1997).
23:167	Although these approaches are effective, decoding long sentences is still difficult due to their computational complexity.
24:167	As the length of an input sentence becomes longer, the analysis and decoding become more complex.
25:167	The complexity causes approximations and errors inevitable during the decoding search.
26:167	In order to reduce this kind of difficulty caused by the complexity, a long sentence can be paraphrased by several shorter sentences with the same meaning.
27:167	Generally, however, decomposing a complex sentence into sub-sentences requires information of the sentence structures which can be obtained by syntactic or semantic analysis.
28:167	Unfortunately, the high level syntactic and semantic analysis can be erroneous and costs as expensive as SMT itself.
29:167	So, we dont want to fully analyze the sentences to get a series of sub-sentences, and our approach to this problem considers splitting only compound sentences.
30:167	In the past years, many research works were concerned with sentence splitting methods to improve machine translation quality.
31:167	This idea had been used in speech translation (Furuse et al, 1998) and example based machine translation (Doi and Sumita, 2004).
32:167	These research works achieved meaningful results in terms of machine translation quality.
33:167	Unfortunately, however, the method of Doi and Sumita using n-gram is not available if the source language is Korean.
34:167	In Korean language, most of sentences have special form of ending morphemes at the end.
35:167	For that reason, we should determine not only the splitting position but also the ending morphemes that we should replace instead of connecting morphemes.
36:167	And the Furuse et als method involves parsing which requires heavy cost.
37:167	In this paper we propose a transformation based splitting method to improve machine translation quality which can be applied to the translation tasks with Korean as a source language.
38:167	1 2 Methods Our task is splitting a long compound sentence into short sub-sentences to improve the performance of phrase-based statistical machine translation system.
39:167	We use a transformation based approach to accomplish our goal.
40:167	2.1 A Concept of Transformation The transformation based learning (TBL) is a kind of rule learning methods.
41:167	The formalism of TBL is introduced by Brill (1995).
42:167	In past years, the TBL approach was used to solve various problems in natural language processing such as part of speech (POS) tagging and parsing (Brill, 1993).
43:167	A transformation consists of two parts: a triggering environment and a rewriting rule.
44:167	And the rewriting rule consists of a source pattern and a target pattern.
45:167	Our consideration is how to get the right transformations and apply them to split the long sentences.
46:167	A transformation works in the following manner; some portion of the input is changed by the rewriting rule if the input meets a condition specified in the triggering environment.
47:167	The rewriting rule finds the source pattern in the input and replaces it with the target pattern.
48:167	For example, suppose that a transformation which have a triggering environment A, source pattern B and target pattern C. We can describe this transformation as a sentence: if a condition A is satisfied by an input sentence, then replace pattern B in the input sentence with pattern C. 2.2 A Transformation Based Sentence Splitting Method Normally, we have two choices when there are two or more transformations available for an input pattern at the same time.
49:167	The first choice is applying the transformation one by one, and the second choice is applying them simultaneously.
50:167	The choice is up to the characteristics of the problem that we want to solve.
51:167	In our problem, we choose the former strategy which is applying the transformations one by one, because it gives direct intuition about the process of splitting sentences.
52:167	By choosing this strategy, we can design splitting process as a recursive algorithm.
53:167	At first, we try to split an input sentence into two sub-sentences.
54:167	If the sentence has been split by some transformation, the result involves exactly two sub-sentences.
55:167	And then we try to split each sub-sentence again.
56:167	We repeat this process in recursive manner until no sub-sentences are split.
57:167	In the above process, a sentence is split into at most two sub-sentences through a single trial.
58:167	In a single trial, a transformation works in the following manner:  If an input sentence satisfies the environment, we substitute the source pattern into the target pattern.
59:167	That is, replace the connecting morphemes with the proper ending morphemes.
60:167	And then we split the sentence with pre-defined position in the transformation.
61:167	And finally, we insert the junction word that is also pre-defined in the transformation between the split sentences after the sub sentences are translated independently.
62:167	From the above process, we can notice easily that a transformation for sentence splitting consists of the four components: a triggering environment, a rewriting rule, a splitting position and a junction type.
63:167	The contents of each component are as follows.
64:167	(1) A triggering environment contains a sequence of morphemes with their POS tags.
65:167	(2) A rewriting consists of a pair of sequences of POS tagged morphemes.
66:167	(3) A junction type can have one of four types: and, or, but and NULL.
67:167	(4) A splitting position is a non-negative integer that means the position of starting word of second sub-sentence.
68:167	2.3 Learning the Transformation for Sentence Splitting At the training phase, TBL process determines the order of application (or rank) of the transformations to minimize the error-rate defined by a specific measure.
69:167	The order is determined by choosing the best rule for a given situation and applying the best rule for each situation iteratively.
70:167	In the sentence splitting task, we maximize the machine translation quality with BLEU score (Papineni et al., 2001) instead of minimizing the error of sentence splitting.
71:167	During the training phase, we determine the order of applying transformation after we build a set of transformations.
72:167	To build the set of transformations, we need manually split examples to learn the transformations.
73:167	Building a transformation starts from extracting a rewriting rule by calculating edit-distance matrix between an original sentence and its split form from the corpus.
74:167	We can easily extract the different parts from the matrix.
75:167	2 BaseBLEU :=  BLEU score of the baseline system S := Split example sentence T := Extracted initial transformation for each t T     for each sS         while true              try to split s with t              if mis-splitting is occurred                   Expand environment              else exit while loop              if environment cannot be expanded                   exit while loop S := apply t to S     Decode S     BLEU := measure BLEU     Discard t if BLEU < BaseBLEU sort  T w.r.t. BLEU From the difference pattern, we can make the source pattern of a rewriting rule by taking the different parts of the original sentence side.
76:167	Similarly, the target pattern can be obtained from the different parts of split form.
77:167	And the junction type and splitting position are directly obtained from the difference pattern.
78:167	Finally, the transformation is completed by setting the triggering environment as same to the source pattern.
79:167	The set of initial transformations is obtained by repeating this process on all the examples.
80:167	The Transformations for sentence splitting are built from the initial transformations through expanding process.
81:167	In the expanding process, each rule is applied to the split examples.
82:167	We expand the triggering environment with some heuristics (in section 2.4), if a sentence is a mis-split.
83:167	And finally, in order to determine the rank of each transformation, we sorted the extracted transformations by decreasing order of resulted BLEU scores after applying the transformation to each training sentence.
84:167	And some transformations are discarded if they decrease the BLEU score.
85:167	This process is different from original TBL.
86:167	The modified TBL learning process is described in figure 1.
87:167	2.4 Expanding Triggering Environments Expanding environment should be treated very carefully.
88:167	If the environment is too specific, the transformation cannot be used in real situation.
89:167	On the other hand, if it is too general, then the transformation becomes erroneous.
90:167	Our main strategy for expanding the environment is to increase context window size of the triggering environment one by one until it causes no error on the training sentences.
91:167	In this manner, we can get minimal error-free transformations on the sentence splitting corpus.
92:167	We use two different windows to define a triggering environment: one for morpheme and another for its part of speech (POS) tag.
93:167	Figure 2 shows this concept of two windows.
94:167	The circles correspond to sequences of morphemes and POS tags in a splitting example.
95:167	Window 1 represents a morpheme context and window 2 represents a POS tag context.
96:167	The windows are independently expanded from the initial environment which consists of a morpheme A and its POS tag.
97:167	In the figure, window 1 is expanded to one forward morpheme and one backward morpheme while window 2 is expanded to two backward POS tags.
98:167	In order to control these windows, we defined some heuristics by specifying the following three policies of expanding windows: no expansion, forward only and forward and backward.
99:167	From those three polices, we have 9 combinations of heuristics because we have two windows.
100:167	By observing the behavior of these heuristics, we can estimate what kind of information is most important to determine the triggering environment.
101:167	Figure 1.
102:167	Modified TBL for sentence splitting    Figure 2.
103:167	Window-based heuristics for triggering environments       3 Test No.
104:167	Window1 policy Window2 policy Test 1 No expansion No expansion Test 2 Forward only Test 3 Free expansion Test 4 Forward only No expansion Test 5 Forward only Test 6 Free expansion Test 7 Free expansion No expansion Test 8 Forward only Test 9 Free expansion  Table 2.Experimental setup     We have at most 4 choices for a single step of the expanding procedure: forward morpheme, backward morpheme, forward POS tag, and backward POS tag.
105:167	We choose one of them in a fixed order: forward POS tag, forward morpheme, backward POS tag and backward morpheme.
106:167	These choices can be limited by 9 heuristics.
107:167	For example, suppose that we use a heuristic with forward policy on morpheme context window and no expansion policy for POS tag context window.
108:167	In this case we have only one choice: forward morpheme.
109:167	3  Experiments We performed a series of experiments on Korean to English translation task to see how the sentence splitting affects machine translation quality and which heuristics are the best.
110:167	Our baseline system built with Pharaoh (Koehn, 2004) which is most popular phrase-based decoder.
111:167	And trigram language model with KN-discounting (Kneser and Ney, 1995) built by SRILM toolkit (Stolcke, 2002) is used.
112:167	Test No.
113:167	# of  affected sentences BLEU score Before splitting After splitting Test 1 209 0.1778 0.1838 Test 2 142 0.1564 0.1846 Test 3 110 0.1634 0.1863 Test 4 9 0.1871 0.2150 Test 5 96 0.1398 0.1682 Test 6 100 0.1452 0.1699 Test 7 8 0.2122 0.2433 Test 8 157 0.1515 0.1727 Test 9 98 0.1409 0.1664 Table 1 shows the corpus statistics used in the experiments.
114:167	The training corpus for MT system has been built by manually translating Korean sentences which are collected from various sources.
115:167	We built 123,425 sentence pairs for training SMT, 1,577 pairs for splitting and another 1,577 pairs for testing.
116:167	The domain of the text is daily conversations and travel expressions.
117:167	The sentence splitting corpus has been built by extracting long sentences from the source-side mono-lingual corpus.
118:167	The sentences in the splitting corpus have been manually split.
119:167	The experimental settings for comparing 9 heuristics described in the section 2.4 are listed in table 2.
120:167	Each experiment corresponds to a heuristic.
121:167	To see the effect of sentence splitting on translation quality, we evaluated BLEU score for affected sentenced by the splitting.
122:167	The results are shown in table 3.
123:167	Each test number shows the effect of transformation-based sentence splitting with different window selection heuristics listed in table 2.
124:167	The scores are consistently increased with significant differences.
125:167	After analyzing the results of table 3, we notice that we can expect some perfor SMT Splitting Korean English Before Split After Split Train # of Sentences 123,425 1,577 1,906 # of Words 1,083,912 916,950 19,918 20,243 Vocabulary 15,002 14,242 1,956 1,952 Test #of Sentences 1,577  Table 1.
126:167	Corpus statistics Table 3.
127:167	BLEU scores of affected sentences  4 mance gain when the average sentence length is long.
128:167	The human evaluation shows more promising results in table 4.
129:167	In the table, the superior change means that the splitting results in better translation and inferior means the opposite case.
130:167	Two ratios are calculated to see the effects of sentence splitting.
131:167	The ratio sup/inf shows the ratio of superior over inferior splitting.
132:167	And ratio trans/change shows how many sentences are affected by a transformation in an average.
133:167	In most of the experiments, the number of superior splitting is over three times larger than that of inferior ones.
134:167	This result means that the sentence splitting is a helpful pre-processing for machine translation.
135:167	We listed some example translations affected by sentence splitting in the table 5.
136:167	In the three cases, junction words dont appear in the results of translation after split because their junction types are NULL that involves no junction word.
137:167	Although several kinds of improvements are observed in superior cases, the most interesting case occurs in out-of-vocabulary (OOV) cases.
138:167	A translation result has a tendency to be a word salad when OOVs are included in the input sentence.
139:167	In this case, the whole sentence may lose its original meaning in the result of translation.
140:167	But after splitting the input sentence, the OOVs have a high chance to be located in one of the split subsentences.
141:167	Then the translation result can save at least a part of its original meaning.
142:167	This case occurs easily if an input sentence includes only one OOV.
143:167	The Superior change of table 5 is the case.
144:167	Although both baseline and split are far from the reference, split catches some portion of the meaning.
145:167	Test No.
146:167	# of transformations (rules) # of changes (sentences) # of superior changes # of inferior changes # of insignificant changes Ratio Sup/Inf Ratio trans/chang e 1 34 209 60 30 119 2.00 6.15 2 177 142 43 9 90 4.78 0.802 3 213 110 29 9 72 3.22 0.516 4 287 9 4 1 4 4.00 0.031 5 206 96 25 4 67 6.25 0.466 6 209 100 23 8 69 2.88 0.478 7 256 8 3 1 4 3.00 0.031 8 177 157 42 10 102 4.20 0.887 9 210 98 21 4 73 5.25 0.467 Table 4.
147:167	Human evaluation results Superior change Reference I saw that some items are on sale on window . what are they ? Baseline What kind of items do you have this item in OOV some discount, I get a discount ? Split You have this item in OOV some discount . what kind of items do I get a discount ? Insignificant change Reference What is necessary to be issued a new credit card?
148:167	Baseline I d like to make a credit card . What do I need?
149:167	Split I d like to make a credit card . What is necessary?
150:167	Inferior change  Reference I d like to make a reservation by phone and tell me the phone number please . Baseline I d like to make a reservation but can you tell me the phone number , please . Split I  d like to make a reservation . can you tell me the , please . Table 5.
151:167	Example translations (The sentences are manually re-cased for readability) 5 Most of the Inferior cases are caused by missplitting.
152:167	Mis-splitting includes a case of splitting a sentence that should not be split or splitting a sentence on the wrong position.
153:167	This case can be reduced by controlling the heuristics described in section 2.4.
154:167	But the problem is that the effort to reducing inferior cases also reduces the superior cases.
155:167	To compare the heuristics each other in this condition, we calculated the ratio of superior and inferior cases.
156:167	The best heuristic is test no. 5 in terms of the ratio of sup/inf.
157:167	The test no. 4 and 7 show that a trans-formation becomes very specific when lexical information is used alone.
158:167	Hence the ratio trans/change becomes below 0.01 in this case.
159:167	And test no. 1 shows that the transformations with no environment expansion are erroneous since it has the lowest ratio of sup/inf.
160:167	4 Conclusion We introduced a transformation based sentence splitting method for machine translation as a effective and efficient pre-processing.
161:167	A transformation consists of a triggering environment and a rewriting rule with position and junction type information.
162:167	The triggering environment of a transformation is extended to be error-free with respect to training corpus after a rewriting rule is extracted from manually split examples.
163:167	The expanding process for the transformation can be generalized by adding POS tag information into the triggering environment.
164:167	The experimental results show that the effect of splitting is clear in terms of both automatic evaluation metric and human evaluation.
165:167	The results consistently state that the statistical machine translation quality can be improved by transformation based sentence splitting method.
166:167	Acknowledgments This research was supported by the MIC (Ministry of Information and Communication), Korea, under the ITRC (Information Technology Research Center) support program supervised by the IITA (Institute of Information Technology Assessment) (IITA-2006-C1090-0603-0045).
167:167	The parallel corpus was courteously provided by Infinity Telecom, Inc.

