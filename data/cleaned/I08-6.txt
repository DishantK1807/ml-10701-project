1:510	CLIA 2008  2 nd  International Workshop on Cross Lingual Information Access (CLIA) Addressing the Information Need of Multilingual Societies  The Third International Joint Conference On Natural Language Processing IJCNLP 2008     Proceedings of the Workshop       11 January 2008, Hyderabad, India Copyright Transfer Agreement    Asian Federation of Natural Language Processing (AFNLP)  The copyright covers the non-exclusive right to reproduce and distribute the article, including the publication in printed and electronic forms (offline or online) and any other reproductions of similar nature, on a not-for-profit basis for academic purposes.
2:510	i Preface  Welcome to the second international workshop on Cross Lingual Information Access (CLIA 2008), with a focus on "Addressing the Information Need of Multilingual Societies".
3:510	In this workshop, like in the previous year, our aim was to bring together various trends in cross and multi-lingual information retrieval and access.
4:510	This year we have accepted eight papers after a careful review process and these accepted papers are included in the proceedings.
5:510	The workshop will have four sessions, each focusing on a specific theme: Cross Language Information Retrieval, Translations and Transliterations in CLIR, Information Extraction/Summarization in CLIR contexts, and, finally a session on the overview of the experiences of Indian research groups in the CLEF-2007 competition.
6:510	There are three papers in the first session on Cross Language Information Retrieval: The first paper explores the effects of language relatedness on multilingual Information retrieval.
7:510	This paper presents a case study with Indo-European and Semitic Languages and addresses some of the challenges posed by Semitic languages IR.
8:510	The paper on Identifying Similar and Co-referring Documents Across Languages, authors make use of Vector Space Model (VSM) and Named Entities in identifying the co-reference and similarity.
9:510	In the paper on finding parallel texts on the web using cross-language information retrieval, CLIR techniques are used in combination with structural features to retrieve candidate document pairs from the web.
10:510	These three papers are part of the session on Cross Language Information Retrieval.
11:510	In the second session on Translations and Transliterations in CLIR, we will again have three papers will be presented: The first paper presents results of some experiments in Mining Named Entity Transliteration Pairs from Comparable Corpora, employing English-Tamil named entity parallel comparable corpus texts.
12:510	The second paper on Domain-Specific Query Translation for Multilingual Information Access using Machine Translation Augmented with Dictionaries Mined from Wikipedia authors demonstrates that effective query translation for CLIA can be achieved in the domain of cultural heritage using a standard MT system, and that domain specific phrase dictionaries that are may be automatically mined from the online Wikipedia.
13:510	The paper Statistical Transliteration for Cross Language Information Retrieval using HMM alignment model and CRF, presents a technique that combines HMM and CRF for transliteration task in CLIR.
14:510	In the third session we have two papers.
15:510	The first paper is Script Independent Word Spotting in Multilingual Documents, which describes a system that accepts a query in the form of text from the user and returns a ranked list of word images from document image corpus based on similarity with the query word.
16:510	The second paper is about building a document graph based multi-document summarizer that makes use of a graph model at offline processing time as well as the query time.
17:510	ii  Finally, in addition to all the refereed papers, we have six invited presentations by various teams focusing on Indian language CLIR.
18:510	These presentations are based on the work done by these teams for Ad-hoc task in Cross Language Evaluation Forum (CLEF) in 2007.
19:510	Teams from IIT Bombay (focusing Marathi, Hindi), IIT Kharagpur (Bengali and Hindi), IIIT Hyderabad (Telugu and Hindi), Microsoft Research India (Tamil, Telugu and Hindi) and Jadhavpur University (Bengali, Telugu and Hindi) will present their work to achieve CLIR for queries in Indian languages and documents in English.
20:510	In this special session, a team from ISI, Kolkata will make a presentation on FIRE (Forum for Information Retrieval Evaluation), a proposed cross language evaluation forum, specifically for Indian languages.
21:510	Abstracts of these presentations are also included in these proceedings.
22:510	We would like to thank all authors for the hard word that they have put in, in submission, rework and presentation.
23:510	The workshop would not be possible without them.
24:510	We would also like to thank the program committee and all the reviewers for their valuable feedback.
25:510	We hope you would enjoy the workshop.
26:510	"We would like to thank Minhaj Babji for all his help in preparing these proceedings as well as supporting the organizing committee during all phases of the workshop."
27:510	Vasudeva Varma, Pushpak Bhattacharya, Sivaji Bandyopadhyay, A. Kumaran, Sudeshna Sarkar.
28:510	(Editors  CLIA 2008 Workshop)                iii    Committees  Organizing Committee Vasudeva Varma, IIIT Hyderabad, India Pushpak Bhattacharya, IIT Bombay, India Sudeshna Sarkar, IIT Kharagpur, India A Kumaran Microsoft Research, India Sivaji Bandyopadhyay, Jadavpur University, Kolkata, India  Program Committee Asanee Kawtrakul, Kasetsart University, Bangkok, Thailand Carol Peters, Istituto di Scienza e Tecnologie dellInformazione and CLEF campaign, Italy Gilles Serasset, GETALP-LIG, Grenoble, France Kumaran A, Microsoft Research, Bangalore, India Lucy Vanderwende, Microsoft Research, USA Mandar Mitra, ISI Kolkata, India Paolo Rosso, Universidad Politecnica de Valencia (UPV), Spain Patrick Saint Dizier, IRIT, Universite Paul Sabatier, Toulouse, France Paul McNamee, Johns Hopkins University, USA Petri Myllymaki, University of Helsinki, Finland Pushpak Bhattacharya, IIT Bombay, India Ralf Steinberger, European Commission Joint Research Centre, Italy Sivaji Bandyopadhyay, Jadavpur University, Kolkata, India Sobha L, AU-KBC, Chennai, India Sudeshna Sarkar, IIT Kharagpur, India Vasudeva Varma, IIIT Hyderabad, India    iv   Workshop Program 11 January 2008, Hyderabad, India   08:45-09:00   Workshop Introduction and Opening Remarks   09:00-10:30    Session-1                        Cross Language Information Retrieval                         The Effects of Language Relatedness on Multilingual  Information          Retrieval: A Case Study With Indo-European and  Semitic Languages Peter Chew and Ahmed Abdelali.
29:510	Identifying Similar and Co-referring Documents Across Languages Pattabhi R K Rao T and Sobha L.   Finding parallel texts on the web using cross-language information  retrieval  Achim Ruopp and Fei Xia.
30:510	10:30 11:00  Tea Break  11:00 12:30   Session II            Translation and Transliteration in CLIR  Some Experiments in Mining Named Entity Transliteration Pairs from Comparable Corpora K Saravanan and A Kumaran.
31:510	Domain-Specific Query Translation for Multilingual Information Access using Machine Translation Augmented With Dictionaries Mined from Wikipedia Gareth Jones, Fabio Fantino, Eamonn Newman and Ying Zhang.
32:510	Statistical Transliteration for Cross Language Information Retrieval using HMM alignment model and CRF Prasad Pingali, Suryaganesh, Sreeharsha Yella and Vasudeva Varma.
33:510	12:30 14:00   Lunch Break   v     14:00 15:00   Session III Cross Language Information Access and Evaluation  Script Independent Word Spotting in Multilingual Documents Anurag Bhardwaj, Damien Jose and Venu Govindaraju.
34:510	A Document Graph Based Query Focused Multi-Document Summarizer Sibabrata Paladhi and Sivaji Bandyopadhyay.
35:510	15:00 15:30   Tea Break  15:30 17:30   Session IV            CLIR in Indian Languages Invited Talks  Hindi and Marathi to English Cross Language Information Retrieval Manoj Kumar Chinnakotla, Sagar Ranadive, Om P. Damani and Pushpak Bhattacharyya  Bengali and Hindi to English CLIR Evaluation Debasis Mandal, Sandipan Dandapat, Mayank Gupta, Pratyush Banerjee, Sudeshna Sarkar  Bengali, Hindi and Telugu to English Ad-hoc Bilingual task Sivaji Bandyopadhyay, Tapabrata Mondal, Sudip Kumar Naskar, Asif Ekbal, Rejwanul Haque, Srinivasa Rao Godavarthy  Cross-Lingual Information Retrieval System for Indian Languages Jagadeesh Jagarlamudi and A Kumaran                          Hindi and Telugu to English CLIR using Query Expansion Prasad Pingali, Vasudeva Varma  FIRE: Forum for Information Retrieval Evaluation Mandar Mitra and Prosenjit Majumdar.
36:510	17:30 17:45   Conclusions and Closing Remarks       vi     Table of Contents   The Effects of Language Relatedness on Multilingual Information Retrieval: A Case Study With Indo-European and Semitic Languages Peter Chew and Ahmed Abdelali 01 Identifying Similar and Co-referring Documents Across Languages Pattabhi R K Rao T and Sobha L. 10 Finding parallel texts on the web using cross-language information retrieval Achim Ruopp and Fei Xia.
37:510	18 Some Experiments in Mining Named Entity Transliteration Pairs from Comperable Corpora K Saravanan and A Kumaran.
38:510	26 Domain-Specific Query Translation for Multilingual Information Access using Machine Translation Augmented With Dictionaries Mined from Wikipedia Gareth Jones, Fabio Fantino, Eamonn Newman and Ying Zhang.
39:510	34 Statistical Transliteration for Cross Language Information Retrieval using HMM alignment model and CRF Prasad Pingali, Suryaganesh Veeravalli, Sreeharsha Yella and Vasudeva Varma.
40:510	42 Script Independent Word Spotting in Multilingual Documents Anurag Bhardwaj, Damien Jose and Venu Govindaraju.
41:510	48 A Document Graph Based Query Focused Multi-Document Summarizer Sibabrata Paladhi and Sivaji Bandyopadhyay.
42:510	55  CLIR in Indian Languages Invited Talks  Hindi and Marathi to English Cross Language Information Retrieval Manoj Kumar Chinnakotla, Sagar Ranadive, Om P. Damani and Pushpak Bhattacharyya64  Bengali and Hindi to English CLIR Evaluation Debasis Mandal, Sandipan Dandapat, Mayank Gupta, Pratyush Banerjee, Sudeshna Sarkar65  vii  Bengali, Hindi and Telugu to English Ad-hoc Bilingual task Sivaji Bandyopadhyay, Tapabrata Mondal, Sudip Kumar Naskar, Asif Ekbal, Rejwanul Haque, Srinivasa Rao Godavarthy66   Cross-Lingual Information Retrieval System for Indian Languages Jagadeesh Jagarlamudi and A Kumaran 67  Hindi and Telugu to English CLIR using Query Expansion Prasad Pingali, Vasudeva Varma 68  FIRE: Forum for Information Retrieval Evaluation Mandar Mitra and Prosenjit Majumdar69        viii The Effects of Language Relatedness on Multilingual Information Retrieval: A Case Study With Indo-European and Semitic Languages Peter A. Chew Sandia National Laboratories P. O. Box 5800, MS 1012 Albuquerque, NM 87185-1012, USA pchew@sandia.gov Ahmed Abdelali New Mexico State University P.O. Box 30002, Mail Stop 3CRL Las Cruces, NM 88003-8001, USA ahmed@crl.nmsu.edu   Abstract We explore the effects of language relatedness within a multilingual information retrieval (IR) framework which can be deployed to virtually any language, focusing specifically on Indo-European versus Semitic languages.
43:510	The Semitic languages present unique challenges to IR for a number of reasons, so we set out to answer the question of whether cross-language IR for Semitic languages can be boosted by manipulation of the training data (which, in our framework, includes multilingual parallel text, some of which is morphologically analyzed).
44:510	We attempted three measures to achieve this: first, the inclusion of genetically related (i.e., other Semitic) languages in the training data; second, the inclusion of non-related languages sharing the same script, and third, the inclusion of morphological analysis for Semitic languages.
45:510	We find that language relatedness is a definite factor in boosting IR precision; script similarity can probably be ruled out as a factor; and morphological analysis can be helpful, but  perhaps paradoxically  not necessarily to the languages which are subjected to morphological analysis.
46:510	1 Introduction In this paper, we consider how related languages fit into a general framework developed for multilingual cross-language information retrieval (CLIR).
47:510	Although this framework can deal with virtually any language, there are some special considerations which make related languages more interesting for exploration.
48:510	Taking one example, Semitic languages are distinguished by their complex morphology, a characteristic which presents challenges to an information retrieval model in which terms (usually, separated by white space or punctuation) are implicitly treated as individual units of meaning.
49:510	We consider three possible methods for investigating the phenomena.
50:510	In all cases, we keep the overall framework the same but simply make changes to the training data.
51:510	One method we consider is to augment the training data with text from related languages; we compare results obtained from using Semitic languages with those obtained when non-Semitic languages are used.
52:510	The other two relate to morphological analysis: the second is to replace inflected forms (in just one language, Arabic) with just the root in the training data; and the third is to remove vowels (again in just one language, Hebrew).
53:510	The paper is organized as follows.
54:510	Section 2 describes our general framework, which is a standard one used for CLIR.
55:510	At a high level, section 3 outlines some of the challenges Semitic languages present within the context of our approach.
56:510	In section 4, we compare results from using a number of different combinations of training data with the same test data.
57:510	Finally, we conclude on our findings in section 5.
58:510	2 The Framework 2.1 General description The framework that we use for IR is multilingual Latent Semantic Analysis (LSA) as described by Berry et al.59:510	(1994:21, and used by Landauer and Littman (1990) and Young (1994).
60:510	A number of different approaches to CLIR have been proposed; generally, they rely either on the use of a parallel corpus for training, or translation of the IR query.
61:510	Either or both of these methods can be based on the use of dictionaries, although that is not the approach that we use.
62:510	In the standard multilingual LSA framework, a term-by-document matrix is formed from a parallel aligned corpus.
63:510	Each document consists of the concatenation of all the languages, so terms from all languages will appear in any given document.
64:510	Thus, if there are K languages, N documents (each of which is translated into each of the K languages), and T distinct linguistic terms across all languages, then the term-by-document matrix is of dimensions T by N. Each cell in the matrix represents a weighted frequency of a particular term t (in any language) in a particular document n. The weighting scheme we use is a standard log-entropy scheme in which the weighted frequency xt,n of a particular term t in a particular document n is given by:  W = log2 (F + 1)  (1 + Ht / log2 (N))  where F is the raw frequency of t in n, and Ht is a measure of the entropy of the term across all documents.
65:510	The last term in the expression above, log2 (N), is the maximum entropy that any term can have in the corpus, and therefore (1 + Ht / log2 (N)) is 1 for the most distinctive terms in the corpus, 0 for those which are least distinctive.
66:510	The log-entropy weighting scheme has been shown to outperform other schemes such as tf-idf in LSAbased retrieval (see for example Dumais 1991).
67:510	The sparse term-by-document matrix is subjected to singular value decomposition (SVD), and a reduced non-sparse matrix is output.
68:510	Generally, we used the output corresponding to the top 300 singular values in our experiments.
69:510	To evaluate the similarity of unseen queries or documents (those not in the training set) to one another, these documents are tokenized, the weighted frequencies are calculated in the same way as they were for the training set, and the results are multiplied by the matrices output by the SVD to project the unseen queries/documents into a semantic space, assigning (in our case) 300dimensional vectors to each document.
70:510	Again, our approach to measuring the similarity of one document to another is a standard one: we calculate the cosine between the respective vectors.
71:510	For CLIR, the main advantages of an approach like LSA are that it is by now quite wellunderstood; the underlying algorithms remain constant regardless of which languages are being compared; and there is wide scope to use different sets of training data, providing they exist in parallel corpora.
72:510	LSA is thus a highly generic approach to CLIR: since it relies only on the ability to tokenize text at the boundaries between words, or more generally semantic units, it can be generalized to virtually all languages.
73:510	2.2 Training and test data For our experiments, the training and test data were taken from the Bible and Quran respectively.
74:510	As training data, the Bible lends itself extremely well to multilingual LSA.
75:510	It is highly available in multiple languages1 (over 80 parallel translations in 50 languages, mostly public-domain, are available from a single website, www.unboundbible.org); and a very fine-grained alignment is possible (by verse) (Resnik et al 1999, Chew and Abdelali 2007).
76:510	Many purpose-built parallel corpora are biased towards particular language groups (for example, the European Union funds work in CLIR, but it tends to be biased towards European languages  for example, see Peters 2001).
77:510	This is not as true of the Bible, and the fact that it covers a wider range of languages is a reflection of the reasons it was translated in the first place.
78:510	The question which is most commonly raised about use of the Bible in this way is whether its coverage of vocabulary from other domains is sufficient to allow it to be used as training data for most applications.
79:510	Based on a variety of experiments we have carried out (see for example Chew et al. forthcoming), we believe this need not always be a drawback  it depends largely on the intended application.
80:510	However, it is beyond our scope to address this in detail here; it is sufficient to note that for the experiments we describe in this paper, we were able to achieve perfectly respectable CLIR results using the Bible as the training data.
81:510	1 It has proved hard to come by reliable statistics to allow direct comparison, but the Bible is generally believed to be the worlds most widely translated book.
82:510	At the end of 2006, it is estimated that there were full translations into 429 languages and partial translations into 2,426 languages (Bible Society 2007).
83:510	As test data, we used the 114 suras (chapters) of the Quran, which has also been translated into a wide variety of languages.
84:510	Clearly, both training and  test data have to be available in multiple languages to allow the effectiveness of CLIR to be measured in a meaningful way.
85:510	For the experiments reported in this paper, we limited the testing languages to Arabic, English, French, Russian and Spanish (the respective abbreviations AR, EN, FR, RU and ES are used hereafter).
86:510	The test data thus amounted to 570 (114  5) documents: a relatively small set, but large enough to achieve statistically significant results for our purposes, as will be shown.
87:510	In all tests described in this paper, we use the same test set: thus, although the test documents all come from a single domain, it is reasonable to suppose that the comparative results can be generalized to other domains.
88:510	The complete list of languages used for both testing and training is given in Table 1.
89:510	Language Bible -trainingQuran -testLanguage Family Sub-Family Afrikaans Yes No Indo-European Germanic-West Amharic Yes No Afro-Asiatic Semitic-South Arabic Yes Yes Afro-Asiatic Semitic-Central Aramaic Yes No Afro-Asiatic Semitic-North Czech Yes No Indo-European Slavic-West Danish Yes No Indo-European Germanic-North Dutch Yes No Indo-European Germanic-West English Yes Yes Indo-European Germanic-West French Yes Yes Indo-European Italic Hebrew Yes No Afro-Asiatic Semitic-Central Hungarian Yes No Uralic Finno-Ugric Japanese Yes No Altaic Latin Yes No Indo-European Italic Persian Yes No Indo-European Indo-Iranian Russian Yes Yes Indo-European Slavic-East Spanish Yes Yes Indo-European Italic Table 1.
90:510	Languages used for training and testing 2.3 Test method We tokenized each of the 570 test documents, applying the weighting scheme described above to obtain a vector of weighted frequencies of each term in the document, then multiplying that vector by U  S-1, also as described above.
91:510	The result was a set of projected document vectors in the 300dimensional LSA space.
92:510	For some of our experiments, we used a light stemmer for Arabic (Darwish 2002) to replace inflected forms in the training data with citation forms.
93:510	It is commonly accepted that morphology improves IR (Abdou et al. 2005, Lavie et al. 2004, Larkey et al. 2002, Oard and Gey 2002), and it will be seen that our results generally confirm this.
94:510	For Hebrew, we used the Westminster Leningrad Codex in the training data.
95:510	Since this is available for download either with vowels or without vowels, no morphological pre-processing was required in this case; we simply substituted one version for the other in the training data when necessary.
96:510	Various measurements are used for evaluating IR systems performance (Van Rijsbergen 1979).
97:510	However, since the aim of our experiments is to assess whether we could identify the correct translation for a given document among a set of possibilities in another language (i.e., given the language of the query and the language of the results), we selected precision at 1 document as our preferred metric.
98:510	This metric represents the proportion of cases, on average, where the translation was retrieved first.
99:510	3 Challenges of Semitic languages The features which make Semitic languages challenging for information retrieval are generally fairly well understood: it is probably fair to say that chief among them is their complex morphology (for example, ambiguity resulting from diacritization, root-and-pattern alternations, and the use of infix morphemes as described in Habash 2004).
100:510	These challenges can be illustrated by means of a statistical comparison of a portion of our training data (the Gospel of Matthew) as shown in Table 2.
101:510	Types Tokens Afrikaans 2,112 24,729 French 2,840 24,438 English 2,074 23,503 Dutch 2,613 23,099 Danish 2,649 21,816 Spanish 3,075 21,279 Persian 3,587 21,190 Hungarian 4,730 18,787 Czech 4,236 18,000 Russian 4,196 16,826 Latin 3,936 16,543 Hebrew (Modern) 4,337 14,153 Arabic 4,607 13,930 Japanese 5,741 13,130 Amharic 5,161 12,940 TOTAL 55,894 284,363 Table 2.
102:510	Statistics of parallel texts by language From Table 2, it should be clear that there is generally an inverse relationship between the number of types and tokens.
103:510	Modern Indo-European (IE) (and particularly Germanic or Italic languages) are at one end of the spectrum, while the Semitic languages (along with Japanese) are at the other.
104:510	The statistics separate analytic languages from synthetic ones, and essentially illustrate the fact that, thanks to the richness of their morphology, the Semitic languages pack more information (in the information-theoretic sense) into each term than the other languages.
105:510	Because this results in higher average entropy per word (in the information theoretic sense), a challenge is presented to information retrieval techniques such as LSA which rely on tokenization at word boundaries: it is harder to isolate each unit of meaning in a synthetic language.
106:510	The actual effect this has on information retrieval precision will be shown in the next section.
107:510	4 Results with LSA The series of experiments described in this section have the aims of:  clarifying what effect morphological analysis of the training data has on CLIR precision;  highlighting the effect on CLIR precision of adding more languages in training;  illustrating what the impact is of adding a partial translation (text in one language which is only partially parallel with the texts in the other languages) We choose Arabic as the language of focus in our experiment; specifically for these experiments, we intended to reveal the effect of adding languages from the same group (Semitic) compared with that of adding languages of different groups.
108:510	First, we present results in Table 3 which confirm that morphological analysis of the training data improves CLIR performance.
109:510	ES RU FR EN AR without morphological analysis of Arabic ES 1.0000 0.5614 0.8333 0.7368 0.2895 RU 0.4211 1.0000 0.5263 0.7632 0.2632 FR 0.7807 0.7018 1.0000 0.8158 0.4035 EN 0.7193 0.8158 0.8596 1.0000 0.4825 AR 0.5000 0.2807 0.6228 0.5526 1.0000 Average precision: Overall 0.677, within IE 0.783, IE-Semitic 0.488 with morphological analysis of Arabic ES 1.0000 0.6579 0.8772 0.7807 0.4123 RU 0.4912 1.0000 0.7193 0.8158 0.3947 FR 0.8421 0.7719 1.0000 0.8421 0.3772 EN 0.8070 0.8684 0.8947 1.0000 0.3684 AR 0.3947 0.3509 0.5614 0.4561 1.0000 Average precision: Overall 0.707, within IE 0.836, IE-Semitic 0.480 Table 3.
110:510	Effect of morphological analysis2 An important point to note first is that CLIR precision is generally much lower for pairs including Arabic than it is elsewhere, lending support to our assertion above that Arabic and other Semitic languages present special challenges in information retrieval.
111:510	It also emerges from Table 3 that when morphological analysis of Arabic was added, the overall average precisions increased from 0.677 to 0.707, a highly significant increase (p 6.7  10-8).
112:510	(Here and below, a chi-squared test is used to measure statistical significance.)
113:510	Given that the ability of morphological analysis to improve IR precision has been documented, this result in itself is not surprising.
114:510	However, it is interesting that the net benefit of adding morphological analysis  and just to Arabic within the training data  was more or less confined to pairs of nonSemitic languages.
115:510	We believe that the explanation is that by adding morphology more relations (liai 2 In this and the following tables, the metric used is precision at 1 document (discussed in section 2.3).
116:510	sons) are defined in LSA between the words from different languages.
117:510	For language pairs including Arabic, the average precision actually decreased from 0.488 to 0.480 when morphology was added (although this decrease is insignificant).
118:510	With the same five training languages as used in Table 3, we added Persian.
119:510	The results are shown in Table 4.
120:510	ES RU FR EN AR ES 1.0000 0.6140 0.8246 0.7632 0.3246 RU 0.5088 1.0000 0.6667 0.7982 0.2281 FR 0.8772 0.7368 1.0000 0.8158 0.3947 EN 0.8246 0.8333 0.8947 1.0000 0.4035 AR 0.4474 0.4386 0.6140 0.5526 1.0000 Average precision: Overall 0.702, within IE 0.822, IE-Semitic 0.489 Table 4.
121:510	Effect on CLIR of adding Persian First to note is that the addition of Persian (an IE language) led to a general increase in precision for pairs of IE languages (Spanish, Russian, French and English) from 0.783 to 0.822 but no significant change for pairs including Arabic (0.488 to 0.489).
122:510	Although Persian and Arabic share the same script, these results confirm that genetic relatedness is a much more important factor in affecting precision.
123:510	Chew and Abdelali (2007) show that the results of multilingual LSA generally improve as the number of parallel translations used in training increases.
124:510	Our next step here, therefore, is to analyze whether it makes any difference whether the additional languages are from the same or different language groups.
125:510	In Table 5 we compare the results of adding an IE language (Latin), an Altaic language (Japanese), and another Semitic language (Hebrew) to the training data.
126:510	In all three cases, no morphological analysis of the training data was performed.
127:510	Based on these results, cross-language precision yielded only very slightly improved results overall by adding Latin or Japanese.
128:510	With Japanese, the net improvement (0.677 to 0.680) was not statistically significant overall, neither was the change significant for pairs either including or excluding Arabic (0.488 to 0.485 and 0.783 to 0.789 respectively).
129:510	Note that this is even though Japanese shares some statistical (although of course not linguistic) properties with the Semitic languages, as shown in Table 2.
130:510	With Latin, the net overall improvement (0.677 to 0.699) was barely significant (p  0.01) and was insignificant for pairs including Arabic (0.488 to 0.496).
131:510	With Hebrew, however, the net improvement was highly significant in all cases (0.677 to 0.718, p  3.36  10-6 overall, 0.783 to 0.819, p  2.20  10-4 for non-Semitic pairs, and 0.488 to 0.538, p  1.45  10-3 for pairs including Arabic).
132:510	We believe that these results indicate that there is more value overall in ensuring that languages are paired with at least one other related language in the training data; our least impressive results (with Japanese) were when two languages in training (one Semitic and one Altaic language) were isolated.
133:510	ES RU FR EN AR Latin included in training data ES 1.0000 0.6140 0.8333 0.7456 0.2544 RU 0.4737 1.0000 0.6316 0.8246 0.3333 FR 0.8596 0.7368 1.0000 0.8333 0.4474 EN 0.7719 0.7982 0.8860 1.0000 0.4474 AR 0.5088 0.3509 0.6140 0.5088 1.0000 Average precision: Overall 0.699, within IE 0.813, IE-Semitic 0.496 Japanese included in training data ES 1.0000 0.5789 0.8333 0.7456 0.2895 RU 0.4298 1.0000 0.5526 0.7807 0.2719 FR 0.7719 0.7368 1.0000 0.8070 0.4035 EN 0.7193 0.807 0.8596 1.0000 0.4123 AR 0.5088 0.2982 0.614 0.5702 1.0000 Average precision: Overall 0.680, within IE 0.789, IE-Semitic 0.485 Modern Hebrew (no vowels) in training data ES 1.0000 0.6140 0.8596 0.7807 0.3509 RU 0.4561 1.0000 0.6667 0.7719 0.3684 FR 0.8509 0.7193 1.0000 0.8684 0.4298 EN 0.7632 0.8509 0.9035 1.0000 0.4298 AR 0.5263 0.4474 0.6491 0.6404 1.0000 Average precision: Overall 0.718, within IE 0.819, IE-Semitic 0.538 Table 5.
134:510	Effect of language relatedness on CLIR The next set of results are for a repetition of the previous three experiments, but this time with morphological analysis of the Arabic data.
135:510	These results are shown in Table 6.
136:510	As was the case without the additional languages, the overall effect of adding morphological analysis of Arabic is still to increase precision.
137:510	In all three cases, the net improvement for pairs excluding Arabic is highly significant (0.813 to 0.844 with Latin, 0.789 to 0.852 with Japanese, and 0.819 to 0.850 with Hebrew).
138:510	For pairs including Arabic, however, the change is again insignificant.
139:510	This was a consistent but surprising feature of our results, that morphological analysis of Arabic in fact appears to benefit non-Semitic languages more than it benefits Arabic itself, at least with this dataset.
140:510	The results might possibly have been different if we had included other Semitic languages in the test data, although this appears unlikely as we found the same phenomenon consistently occurring across a wide variety of tests, and regardless of which languages we used in training.
141:510	ES RU FR EN AR Latin included in training data ES 1.0000 0.6579 0.8684 0.7456 0.4211 RU 0.5614 1.0000 0.7456 0.8509 0.4386 FR 0.8421 0.8158 1.0000 0.8509 0.4211 EN 0.8421 0.8333 0.8947 1.0000 0.4123 AR 0.4123 0.3947 0.5351 0.4825 1.0000 Average precision: Overall 0.721, within IE 0.844, IE-Semitic 0.502 Japanese included in training data ES 1.0000 0.7544 0.8684 0.8070 0.4211 RU 0.4737 1.0000 0.7193 0.8509 0.4123 FR 0.8246 0.8596 1.0000 0.8772 0.4211 EN 0.8421 0.8596 0.8947 1.0000 0.4035 AR 0.3333 0.3509 0.5614 0.4649 1.0000 Average precision: Overall 0.720, within IE 0.852, IE-Semitic 0.485 Modern Hebrew (no vowels) in training data ES 1.0000 0.7018 0.9035 0.7982 0.4561 RU 0.5614 1.0000 0.7105 0.8070 0.4035 FR 0.8421 0.8246 1.0000 0.8596 0.4825 EN 0.8509 0.8509 0.8947 1.0000 0.4123 AR 0.3947 0.4298 0.5351 0.5175 1.0000 Average precision: Overall 0.729, within IE 0.850, IE-Semitic 0.514 Table 6.
142:510	Effect of language relatedness and morphology on CLIR For further verification, we explored what would happen if only the Arabic root were included in morphological analysis.
143:510	As already mentioned, for languages that combine affixes with the stem, there is a higher token-to-type ratio.
144:510	Omitting the affix from the morphological analysis of these languages reveals the importance of considering the affixes and their contribution to the semantics of a given sentence.
145:510	Although LSA is not sentence-structureaware (as it uses a bag-of-words approach), the importance of considering the affixes as part of the sentence is very crucial.
146:510	The results in Table 7 demonstrate clearly that ignoring or over-looking the word affixes has a negative effect on the overall performance of the CLIR system.
147:510	When including only the Arabic stem, a performance degradation is noticeable across all languages, with a larger impact on IE languages.
148:510	The results which illustrate can be seen by comparing Table 7 with Table 3.
149:510	ES RU FR EN AR morphological analysis of Arabic Stem onlyES 1.0000 0.5789 0.8070 0.7807 0.3421 RU 0.4912 1.0000 0.6842 0.8246 0.1842 FR 0.8421 0.7018 1.0000 0.8333 0.4211 EN 0.8333 0.8333 0.9211 1.0000 0.4211 AR 0.4561 0.4386 0.5702 0.4912 1.0000 Average precision: Overall 0.698, within IE 0.821, IE-Semitic 0.481 Table 7.
150:510	Effect of Using Stem only Next, we turn specifically to a comparison of the effect that different Semitic languages have on CLIR precision.
151:510	Here, we compare the results when the sixth language used in training is Hebrew, Amharic, or Aramaic.
152:510	However, since our Amharic and Aramaic training data were only partially parallel (we have only the New Testament in Amharic, and only portions of the New Testament in Aramaic), we first considered the effect that partial translations have on precision.
153:510	Table 8 shows the results we obtained when only the Hebrew Old Testament (with vowels) was used as the sixth parallel version.
154:510	No morphological analysis was performed.
155:510	ES RU FR EN AR without morphological analysis of Arabic ES 1.0000 0.6842 0.8421 0.8158 0.3947 RU 0.4211 1.0000 0.6228 0.7982 0.4737 FR 0.8509 0.7719 1.0000 0.8509 0.4737 EN 0.7895 0.8333 0.8684 1.0000 0.4649 AR 0.4561 0.3333 0.6404 0.4561 1.0000 Average precision: Overall 0.714, within IE 0.822, IE-Semitic 0.521 with morphological analysis of Arabic ES 1.0000 0.7105 0.9035 0.8333 0.4737 RU 0.4649 1.0000 0.7456 0.8333 0.4912 FR 0.8421 0.8070 1.0000 0.8860 0.4474 EN 0.8772 0.8421 0.9298 1.0000 0.4298 AR 0.2719 0.3684 0.5088 0.5000 1.0000 Average precision: Overall 0.727, within IE 0.855, IE-Semitic 0.499  Table 8.
156:510	Effect of partial translation on CLIR Although two or more parameters differ from those used for Hebrew in Table 5 (a fully-parallel text in modern Hebrew without vowels, versus a partial text in Ancient Hebrew with vowels), it is worth comparing the two sets of results.
157:510	In particular, the reductions in average precision from 0.718 to 0.714 and from 0.729 to 0.727 respectively are insignificant.
158:510	Likewise, the changes for pairs with and without Arabic were insignificant.
159:510	This appears to show that, at least up to a certain point, even only partially parallel corpora can successfully be used under our LSA-based approach.
160:510	We now turn to the results we obtained using Aramaic, with the intention of comparing these to our previous results with Hebrew.
161:510	ES RU FR EN AR no morphological analysis of Arabic ES 1.0000 0.4035 0.8070 0.7368 0.2632 RU 0.3509 1.0000 0.5965 0.6579 0.2281 FR 0.8421 0.6754 1.0000 0.8246 0.2719 EN 0.7018 0.6754 0.8947 1.0000 0.2719 AR 0.4825 0.2807 0.4649 0.3947 1.0000 Average precision: Overall 0.633, within IE 0.760, IE-Semitic 0.406 morphological analysis of Arabic ES 1.0000 0.5351 0.8684 0.7719 0.2895 RU 0.5175 1.0000 0.6930 0.7807 0.3421 FR 0.8947 0.7807 1.0000 0.8684 0.2807 EN 0.8070 0.8158 0.9035 1.0000 0.2982 AR 0.3509 0.2193 0.3772 0.2895 1.0000 Average precision: Overall 0.667, within IE 0.827, IE-Semitic 0.383 Table 9.
162:510	Effect of Aramaic on CLIR Here, there is a noticeable across-the-board decrease in precision from the previous results.
163:510	We believe that this may have more to do with the fact that the Aramaic training data we have is fairly sparse (2,957 verses of the Bible out of a total of 31,226, compared with 23,269 out of 31,226 for Ancient Hebrew).
164:510	It is likely that at some point as the parallel translations coverage drops (somewhere between the coverage of the Hebrew and the Aramaic), there is a severe hit to the performance of CLIR.
165:510	Accordingly, we discarded Aramaic for further tests.
166:510	Next, we considered the addition of two Semitic languages other than Arabic, Modern Hebrew and Amharic, to the training data.
167:510	In this case, we performed morphological analysis of Arabic.
168:510	The results appear to show a significant increase in precision for pairs of IE languages and a significant decrease for cross-language-group cases (those where an IE language is paired with Arabic), compared to when just Modern Hebrew was used in the training data (see the relevant part of Table 6).
169:510	It is not clear why this is the case, but in this case we believe that it is quite possible that the results would have been different if more than one Semitic language had been included in the test data.
170:510	ES RU FR EN AR ES 1.0000 0.6930 0.8860 0.7719 0.4649 RU 0.5000 1.0000 0.7456 0.8684 0.5175 FR 0.8772 0.7982 1.0000 0.8772 0.4649 EN 0.8684 0.8596 0.9298 1.0000 0.4386 AR 0.2632 0.2982 0.4386 0.3947 1.0000 Average precision: Overall 0.718, within IE 0.855, IE-Semitic 0.476 Table 10.
171:510	CLIR with 7 languages (including Modern Hebrew and Amharic) We now come to a rare example where we achieved a boost in precision specifically for Arabic.
172:510	In this case, we repeated the last experiment but removed the vowels from the Hebrew text.
173:510	The results are shown in Table 11.
174:510	ES RU FR EN AR ES 1.0000 0.7018 0.8772 0.8158 0.5088 RU 0.5175 1.0000 0.7632 0.8421 0.4825 FR 0.8596 0.8246 1.0000 0.8860 0.5351 EN 0.8947 0.8158 0.9298 1.0000 0.5088 AR 0.2895 0.3772 0.5526 0.5000 1.0000 Average precision: Overall 0.739, within IE 0.858, IE-Semitic 0.528 Table 11.
175:510	Effect of removing Hebrew vowels Average precision for pairs including Arabic increased from 0.476 to 0.528, an increase which was significant (p  7.33  10-4), but for other pairs the change was insignificant.
176:510	Since the Arabic text in training did not include vowels, we believe that the exclusion of vowels from Hebrew placed the two languages on a more common footing, allowing LSA, for example, to make associations between Hebrew and Arabic roots which otherwise might not have been made.
177:510	Although Hebrew and Arabic do not always share common stems, it can be seen from Table 2 that the type/token statistics of Hebrew (without vowels) and Arabic are very similar.
178:510	The inclusion of Hebrew vowels would change the statistics for Hebrew considerably, increasing the number of types (since previously indistinguishable wordforms would now be listed separately).
179:510	Thus, with the exclusion of Hebrew vowels, there should be more instances where Arabic tokens can be paired one-to-one with Hebrew tokens.
180:510	Finally, in order to confirm our conclusions and to eliminate any doubts about the results obtained so far, we experimented with more languages.
181:510	We added Japanese, Afrikaans, Czech, Danish, Dutch, Hungarian and Hebrew in addition to our 5 original languages.
182:510	Morphological analysis of the Arabic text in training was performed, as in some of the previous experiments.
183:510	The results of these tests are shown in Table 12.
184:510	ES RU FR EN AR 11 languages (original 5 + Japanese, Afrikaans, Czech, Danish, Dutch, and Hungarian) ES 1.0000 0.6754 0.9035 0.7719 0.5526 RU 0.4737 1.0000 0.7632 0.8772 0.5175 FR 0.8596 0.8070 1.0000 0.8947 0.5088 EN 0.8421 0.8684 0.9035 1.0000 0.4912 AR 0.3772 0.2632 0.6316 0.4912 1.0000 Average precision: Overall 0.739, within IE 0.853, IE-Semitic 0.537 12 languages (as above plus Hebrew) ES 1.0000 0.7018 0.8947 0.7719 0.6404 RU 0.6667 1.0000 0.7105 0.9123 0.6228 FR 0.8772 0.8333 1.0000 0.8421 0.6404 EN 0.6667 0.8684 0.9035 1.0000 0.6316 AR 0.5877 0.4386 0.5965 0.6491 1.0000 Average precision: Overall 0.778, within IE 0.853, IE-Semitic 0.645 Table 12.
185:510	Effect of further languages on CLIR Generally, these results confirm the finding of Chew and Abdelali (2007) about adding more languages; doing so enhances the ability to identify translations across language boundaries.
186:510	Across the board (for Arabic and other languages), the increase in precision gained by adding Afrikaans, Czech, Danish, Dutch and Hungarian is highly significant (compared to the part of Table 5 which deals with Japanese, overall average precision increased from 0.680 to 0.739, with p  1.17  10-11; for cross-language-group retrieval, from 0.485 to 0.537, with p  9.31  10-4; for pairs within IE, from 0.789 to 0.853 with p  2.81  10-11).
187:510	In contrast with most previous results, however, with the further addition of Hebrew, precision was boosted primarily for Arabic (0.537 to 0.645 with p  4.39  10-13).
188:510	From this and previous results, it appears that there is no clear pattern to when the addition of a Semitic language in training was beneficial to the Semitic language in testing.
189:510	5 Conclusion and future work Based on our results, it appears that although clear genetic relationships exist between certain languages in our training data, it was less possible than we had anticipated to leverage this to our advantage.
190:510	We had expected, for example, that by including multiple Semitic languages in the training data within an LSA framework, we would have been able to improve cross-language information retrieval results specifically for Arabic.
191:510	Perhaps surprisingly, the greatest benefit of including additional Semitic languages in the training data is most consistently to non-Semitic languages.
192:510	A clear observation is that any additional languages in training are generally beneficial, and the benefit of additional languages can be considerably greater than the benefits of linguistic pre-processing (such as morphological analysis).
193:510	Secondly, it is not necessarily the case that cross-language retrieval with Arabic is helped most by including other Semitic languages, despite the genetic relationship.
194:510	Finally, as we expected, we were able to rule out script similarity (e.g. between Persian and Arabic) as a factor which might improve precision.
195:510	Our results appear to demonstrate clearly that language relatedness is much more important in the training data than use of the same script.
196:510	Finally, to improve cross-language retrieval with Arabic  the most difficult case in the languages we tested  we attempted to prime the training data by including Arabic morphological analysis.
197:510	This did lead to a statistically significant improvement overall in CLIR, but  perhaps paradoxically  the improvement specifically for cross-language retrieval with Arabic was negligible in most cases.
198:510	The only two measures which were successful in boosting precision for Arabic significantly were (1) the inclusion of Modern Hebrew in the training data; and (2) the elimination of vowels in the Ancient Hebrew training data  both measures which would have placed the training data for the two Semitic languages (Arabic and Hebrew) on a more common statistical footing.
199:510	These results appear to confirm our hypothesis that there is value, within the current framework, of pairing genetically related languages in the training data.
200:510	In short, language relatedness does matter in cross-language information retrieval.
201:510	6 Acknowledgement Sandia is a multiprogram laboratory operated by Sandia Corporation, a Lockheed Martin Company, for the United States Department of Energys National Nuclear Security Administration under contract DE-AC04-94AL85000.
202:510	7 References Abdou, S., Ruck, P., and Savoy, J. 2005.
203:510	Evaluation of Stemming, Query Expansion and Manual Indexing Approaches for the Genomic Task.
204:510	In Proceedings of TREC 2005.
205:510	Berry, M. W., Dumais, S. T., and OBrien, G. W. 1994.
206:510	Using Linear Algebra for Intelligent Information Retrieval.
207:510	SIAM: Review, 37, 573-595.
208:510	Biola University.
209:510	2005-2006.
210:510	The Unbound Bible.
211:510	Accessed at http://www.unboundbible.com/ on February 27, 2007.
212:510	Chew, P. A., and Abdelali, A. 2007.
213:510	Benefits of the Massively Parallel Rosetta Stone: Cross-Language Information Retrieval with over 30 Languages, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, ACL 2007.
214:510	Prague, Czech Republic, June 2330, 2007.
215:510	pp.
216:510	872-879.
217:510	Chew, P. A., Kegelmeyer, W. P., Bader, B. W. and Abdelali, A. Forthcoming.
218:510	The Knowledge of Good and Evil: Multilingual Ideology Classification with PARAFAC2 and Maching Learning.
219:510	Chew, P. A., Verzi, S. J., Bauer, T. L., and McClain, J. T. 2006.
220:510	Evaluation of the Bible as a Resource for Cross-Language Information Retrieval.
221:510	Proceedings of the Workshop on Multilingual Language Resources and Interoperability, 6874.
222:510	Darwish, K. 2002.
223:510	Building a shallow Arabic morphological analyzer in one day.
224:510	In Proceedings of the Association for Computational Linguistics (ACL-02), 40th Anniversary Meeting.
225:510	pp.
226:510	47-54.
227:510	Dumais, S. T. 1991.
228:510	Improving the Retrieval of Information from External Sources.
229:510	Behavior Research Methods, Instruments, and Computers 23 (2), 229236.
230:510	Dumais, S. T., Furnas, G. W., Landauer, T. K., Deerwester, S. and Harshman, R. 1998.
231:510	Using Latent Semantic Analysis to Improve Access to Textual Information.
232:510	In CHI88: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 281-285.
233:510	ACM Press.
234:510	Frakes, W. B. and Baeza-Yates, R. 1992.
235:510	Information Retrieval: Data Structures and Algorithms.
236:510	PrenticeHall: New Jersey.
237:510	Habash, N. 2004.
238:510	Large Scale Lexeme Based Arabic Morphological Generation.
239:510	In Proc.
240:510	of Traitement Automatique du Langage Naturel.
241:510	Larkey, L., Ballesteros, L. and Connell, M. 2002.
242:510	Improving Stemming for Arabic Information Retrieval: Light Stemming and Co-Occurrence Analysis.
243:510	SIGIR 2002, Finland, pp.
244:510	275-282.
245:510	Larkey, L. and Connell, M. 2002.
246:510	Arabic Information Retrieval at Umass in TREC-10.
247:510	In Voorhees, E.M. and Harman, D.K.
248:510	(eds.): The Tenth Text Retrieval Conference, TREC 2001 NIST Special Publication 500-250, pp.
249:510	562-570.
250:510	Lavie, A., Peterson, E., Probst, K., Wintner, S., and Eytani, Y. 2004.
251:510	Rapid Prototyping of a Transfer-Based Hebrew-to-English Machine Translation System.
252:510	In Proceedings of the TMI-04.
253:510	Mathieu, B., Besanon, R. and Fluhr, C. 2004.
254:510	Multilingual Document Clusters Discovery.
255:510	Recherche dInformation Assiste par Ordinateur (RIAO) Proceedings, 1-10.
256:510	Oard, D. and Gey, F. 2002.
257:510	The TREC 2002 Arabic/English CLIR Track, NIST TREC 2002 Proceedings, pp.
258:510	16-26.
259:510	Peters, C.
260:510	(ed.).
261:510	2001.
262:510	Cross-Language Information Retrieval and Evaluation: Workshop of the CrossLanguage Evaluation Forum, CLEF 2000.
263:510	Berlin: Springer-Verlag.
264:510	Resnik, P., Olsen, M. B., and Diab, M. 1999.
265:510	The Bible as a Parallel Corpus: Annotating the "Book of 2000 Tongues".
266:510	Computers and the Humanities, 33, 129153.
267:510	Van Rijsbergen, C. 1979.
268:510	Information Retrieval (2nd edition).
269:510	Butterworth: London.
270:510	Identifying Similar and Co-referring Documents Across Languages Pattabhi R K Rao T AU-KBC Research Centre, MIT Campus, Anna University, Chennai-44, India.
271:510	pattabhi@au-kbc.org Sobha L AU-KBC Research Centre, MIT Campus, Anna University, Chennai-44, India.
272:510	sobha@au-kbc.org   Abstract This paper presents a methodology for finding similarity and co-reference of documents across languages.
273:510	The similarity between the documents is identified according to the content of the whole document and co-referencing of documents is found by taking the named entities present in the document.
274:510	Here we use Vector Space Model (VSM) for identifying both similarity and co-reference.
275:510	This can be applied in cross-lingual search engines where users get documents of very similar content from different language documents.
276:510	1 Introduction In this age of information technology revolution, the growth of technology and easy accessibility has contributed to the explosion of text data on the web in different media forms such as online news magazines, portals, emails, blogs etc in different languages.
277:510	This represents 80% of the unstructured text content available on the web.
278:510	There is an urgent need to process such huge amount of text using Natural Language Processing (NLP) techniques.
279:510	One of the significant challenges with the explosion of text data is to organize the documents into meaningful groups according to their content.
280:510	The work presented in this paper has two parts a) finding multilingual cross-document similarity and b) multilingual cross-document entity coreferencing.
281:510	The present work analyzes the documents and identifies whether the documents are similar and co-referring.
282:510	Two objects are said to be similar, when they have some common properties between them.
283:510	For example, two geometrical figures are said to be similar if they have the same shape.
284:510	Hence similarity is a measure of degree of resemblance between two objects.
285:510	Two documents are said to be similar if their contents are same.
286:510	For example a document D1 describes about a bomb blast incident in a city and document D2 also describes about the same bomb blast incident, its cause and investigation details, then D1 and D2 are said to be similar.
287:510	But if document D3 talks of terrorism in general and explains bomb blast as one of the actions in terrorism and not a particular incident which D1 describes, then documents D1 and D3 are dissimilar.
288:510	The task of finding document similarity differs from the task of document clustering.
289:510	Clustering is a task of categorization of documents based on domain/field.
290:510	In the above example, documents D1, D2, D3 can be said to be in a cluster of crime domain.
291:510	When documents are similar they share common noun phrases, verb phrases and named entities.
292:510	While in document clustering, sharing of named entities and noun phrases is not essential but still there can be some noun phrases and named entities in common.
293:510	Cross-document co-referencing of entities refers to the identification of same entities across the documents.
294:510	When the named entities present in the documents which are similar and also coreferencing, then the documents are said to be coreferring documents.
295:510	The paper is further organized as follows.
296:510	In section 2, the motivation behind this paper is explained and in 3 the methodology used is described.
297:510	Results and discussions are dealt in section 4 and conclusion in section 5.
298:510	2 Motivation Dekang Lin (1998) defines similarity from the information theoretic perspective and is applicable if the domain has probabilistic model.
299:510	In the past decade there has been significant amount of work done on finding similarity of documents and organizing the documents according to their content.
300:510	Similarity of documents are identified using different methods such as Self-Organizing Maps (SOMs) (Kohonen et al, 2000; Rauber, 1999), based on Ontologies and taxanomy (Gruber, 1993; Resnik, 1995), Vector Space Model (VSM) with similarity measures like Dice similarity, Jaccards similarity, cosine similarity (Salton, 1989).
301:510	Bagga (Bagga et al., 1998) have used VSM in their work for finding co-references across the documents for English documents.
302:510	Chung and Allan (2004) have worked on cross-document co-referencing using large scale corpus, where they have said ambiguous names from the same domain (here for example, politics) are harder to disambiguate when compared to names from different domains.
303:510	In their work Chung and Allan compare the effectiveness of different statistical methods in cross-document coreference resolution task.
304:510	Harabagiu and Maiorano (2000) have worked on multilingual co-reference resolution on English and Romanian language texts.
305:510	In their system, SWIZZLE they use a datadriven methodology which uses aligned bilingual corpora, linguistic rules and heuristics of English and Romanian documents to find co-references.
306:510	In the Indian context, obtaining aligned bilingual corpora is difficult.
307:510	Document similarity between Indian languages and English is tough since the sentence structure differs and Indian languages are agglutinative in nature.
308:510	In the recent years there has been some work done in the Indian languages, (Pattabhi et al, 2007) have used VSM for multilingual cross-document co-referencing, for English and Tamil, where no bilingual aligned corpora is used.
309:510	One of the methods used in cross-lingual information retrieval (CLIR) is Latent Semantic Analysis (LSA) in conjunction with multilingual parallel aligned corpus.
310:510	This approach works well for information retrieval task where it has to retrieve most similar document in one language to a query given in another language.
311:510	One of the drawbacks of using LSA in multilingual space for the tasks of document clustering, document similarity is that it gives similar documents more based on the language than by topic of the documents in different languages (Chew et al, 2007).
312:510	Another drawback of LSA is that the reduced dimension matrix is difficult to interpret semantically.
313:510	The examples in Table 1, illustrate this.
314:510	Before Reduction After Reduction 1 . {(car),(truck),(flower)} {(1.2810*car+0.5685*tr uck),(flower) 2 {(car),(bottle),(flower)} {(1.2810*car+0.5685*b ottle),(flower) Table 1.
315:510	LSA Example  In the first example the component (1.2810*car+0.5685*truck) can be inferred as Vehicle but in cases such as in second example, the component (1.2810*car+0.5685*bottle) does not have any interpretable meaning in natural language.
316:510	In LSA the dimension reduction factor k has very important role to play and the value of k can be found by doing several experiments.
317:510	The process of doing dimension reduction in LSA is computationally expensive.
318:510	When LSA is used, it reduces the dimensions statistically and when there is no parallel aligned corpus, this can not be interpreted semantically.
319:510	Hence, in the present work, we propose VSM which is computationally simple, along with cosine similarity measure to find document similarity as well as entity co-referencing.
320:510	We have taken English and three Dravidian languages viz.
321:510	Tamil, Telugu and Malayalam for analysis.
322:510	3 Methodology In VSM, each document is represented by a vector which specifies how many times each term occurs in the document (the term frequencies).
323:510	These counts are weighted to reflect the importance of each term and weighting is the inverse document frequency (idf).
324:510	If a term t occurs in n documents in the collection then the idf is the inverse of log n. This vector of weighted counts is called a "bag of words" representation.
325:510	Words such as "stop words" (or function words) are not included in the representation.
326:510	The documents are first pre-processed, to get syntactic and semantic information for each word in the documents.
327:510	The preprocessing of documents involves sentence splitting, morph analysis, partof-speech (POS) tagging, text chunking and named entity tagging.
328:510	The documents in English are preprocessed using Brills Tagger (Brill, 1994) for POS tagging and fn-TBL (Ngai and Florian, 2001) for text chunking.
329:510	The documents in Indian languages are preprocessed, using  a generic engine (Arulmozhi et al., 2006) for POS tagging, and text chunking based on TBL (Sobha and Vijay, 2006).
330:510	For both English and Indian language documents the named entity tagging is done using Named Entity Recognizer (NER) which was developed based on conditional random field (CRF).
331:510	The tagset used by the NER tagger is a hierarchical tagset, consists of mainly i) ENAMEX, ii) NUMEX and iii) TIMEX.
332:510	Inside the ENAMEX there are mainly 11 subtypes viz.
333:510	a) Person b) Organization c) Location d) Facilities e) Locomotives f) Artifacts g) Entertainment h) Cuisines i) Organisms j) Plants k) Disease.
334:510	For the task of multilingual crossdocument entities co-referencing, the documents are further processed for anaphora resolution where the corresponding antecedents for each anaphor are tagged in the document.
335:510	For documents in English and Tamil, anaphora resolution is done using anaphora resolution system.
336:510	For documents in Malayalam and Telugu anaphora resolution is done manually.
337:510	After the preprocessing of documents, the language model is built by computing the term frequency  inverse document frequency (tf-idf) matrix.
338:510	For the task of finding multilingual cross-document similarity, we have performed four different experiments.
339:510	They are explained below:  E1: The terms are taken from documents after removing the stop words.
340:510	These are raw terms where no preprocessing of documents is done; the terms are unique words in the document collection.
341:510	E2: The terms taken are the words inside the noun phrases, verb phrases and NER expressions after removing the stop words.
342:510	E3: The whole noun phrase/verb phrase/NER expression is taken to be a single term.
343:510	E4: The noun phrase/NER expression along with the POS tag information is taken as a single term.
344:510	The first experiment is the standard VSM implementation.
345:510	The rest three experiments differ in the way the terms are taken for building the VSM.
346:510	For building the VSM model which is common for all language document texts, it is essential that there should be translation/transliteration tool.
347:510	First the terms are collected from individual language documents and a unique list is formed.
348:510	After that, using the translation/transliteration tool the equivalent terms in language L2 for language L1 are found.
349:510	The translation is done using a bilingual dictionary for the terms present in the dictionary.
350:510	For most of the NERs only transliteration is possible since those are not present in the dictionary.
351:510	The transliteration tool is developed based on the phoneme match it is a rule based one.
352:510	All the Indian language documents are represented in roman notation (wx-notation) for the purpose of processing.
353:510	After obtaining equivalent terms in all languages, the VSM model is built.
354:510	Let S1 and S2 be the term vectors representing the documents D1 and D2, then their similarity is given by equation (1) as shown below.
355:510	Sim(S1,S2) =  (W 1j  x W 2j )                      -(1)   tj  Where,        tj is a term present in both vectors S1and S2.
356:510	W 1j  is the weight of term tj in S1 and        W 2j  is the weight of term tj in S2.
357:510	The weight of term tj in the vector S1 is calculated by the formula given by equation (2), below.
358:510	Wij=(tf*log(N/df))/[sqrt(S i1 2 +S i2 2 ++S in 2 )] --(2) Where,  tf = term frequency of term t j   N=total number of documents in the collection df = number of documents in the collection that the term t j   occurs in.
359:510	sqrt represents square root The denominator [sqrt(S i1 2 +S i2 2 ++S in 2 )] is the cosine normalization factor.
360:510	This cosine normalization factor is the Euclidean length of the vector S i,  where i is the document number in the collection and S in 2 is the square of the product of (tf*log(N/df)) for term t n  in the vector S i . For the task of multilingual cross-document entity co-referencing, the words with-in the anaphor tagged sentences are considered as terms for building the language model.
361:510	4 Results and Discussion The corpus used for experiments is collected from online news magazines and online news portals.
362:510	The sources in English include The Hindu, Times of India, Yahoo News, New York Times, Bangkok Post, CNN, WISC, The Independent.
363:510	The sources for Tamil include Dinamani, Dinathanthi, Dinamalar, Dinakaran, and Yahoo Tamil.
364:510	The work was primarily done using English and Tamil.
365:510	Later on this was extended for Malayalam and Telugu.
366:510	The data sources for Malayalam are Malayala Manorama, Mathrubhumi, Deshabhimani, Deepika and sources for Telugu include Eenadu, Yahoo Telugu and Andhraprabha.
367:510	First we discuss about English and Tamil and Later Telugu and Malayalam.
368:510	The domains of the news taken include sports, business, politics, tourism etc. The news articles were collected using a crawler, and hence we find in the collection, a few identical news articles because they appear in different sections of the news magazine like in Front page section, in state section and national section.
369:510	The dataset totally consists of 1054 English news articles, 390 Tamil news articles.
370:510	Here we discuss results in two parts; in the first part results pertaining to document similarity are explained.
371:510	In second part we discuss results on multilingual cross-document entity co-referencing.
372:510	4.1 Document Similarity The data collection was done in four instances, spread in a period of two months.
373:510	At the first instance two days news was crawled from different news sources in English as well as Tamil.
374:510	In the first set 1004 English documents and 297 Tamil documents were collected.
375:510	In this set when manually observed (human judgment) it was found that there are 90 similar documents forming 31 groups, rest of the documents were not similar.
376:510	This is taken as gold standard for the evaluation of the system output.
377:510	As explained in the previous section, on this set the four experiments were performed.
378:510	In the first experiment (E1), no preprocessing of the documents was done except that the stop words were removed and the language model was built.
379:510	In this it was observed that the number of similar documents is 175 forming 25 groups.
380:510	Here it was observed that along with actual similar documents, system also gives other not similar documents (according to gold standard) as similar ones.
381:510	This is due to the fact there is no linguistic information given to the system, hence having words alone does not tell the context, or in which sense it is used.
382:510	And apart from that named entities when split dont give exact meaning, for example in name of hotels Leela Palace and Mysore Palace, if split into words yields three words, Leela, Mysore, and Palace.
383:510	In a particular document, an event at hotel Leela Palace is described and the hotel is referred as Leela Palace or by Palace alone.
384:510	Another document describes about Dussera festival at Mysore Palace.
385:510	Now here the system identifies both these documents to be similar even though both discuss about different events.
386:510	The precision of the system was observed to be 51.4%, where as the recall is 100% since all the documents which were similar in the gold standard is identified.
387:510	Here while calculating the precision; we are considering the number of documents that are given by the system as similar to the number of documents similar according to the gold standard.
388:510	Hence to overcome the above discussed problem, we did the second experiment (E2) where only words which occur inside the noun phrases, verb phrases and named entities are considered as terms for building the language model.
389:510	Here it is observed that the number of similar documents is 140 forming 30 groups.
390:510	This gives a precision of 64.2% and 100% recall.
391:510	Even though we find a significant increase in the precision but still there are large number of false positives given by the system.
392:510	A document consists of noun phrases and verb phrases, when the individual tokens inside these phrases are taken; it is equivalent to taking almost the whole document.
393:510	This reduces the noise.
394:510	The problem of Leela Palace and Mysore Palace as explained in the previous paragraph still persists here.
395:510	In the third experiment (E3) the whole noun phrase, verb phrase and named entity is considered as a single term for building the language model.
396:510	Here the phrases are not split into individual tokens; the whole phrase is a single term for language model.
397:510	This significantly reduces the number of false positives given by the system.
398:510	The system identifies 106 documents as similar documents forming 30 groups.
399:510	Now the precision of the system is 84.9%.
400:510	In this experiment, the problem of Leela Palace and Mysore Palace is solved.
401:510	Though this problem was solved the precision of the system is low, hence we performed the fourth (E4) experiment.
402:510	In the fourth experiment (E4), the part-of-speech (POS) information is given along with the phrase for building the language model.
403:510	It is observed that the precision of the system increases.
404:510	The number of similar documents identified is 100 forming 31 groups.
405:510	This gives a precision of 90% and a recall of 100%.
406:510	Another important factor which plays a crucial role in implementation of language model or VSM is the threshold point.
407:510	What is the threshold point that is to be taken?
408:510	For obtaining an answer for this question, few experiments were performed by setting the threshold at various points in the range 0.75 to 0.95.
409:510	When the threshold was set at 0.75 the number of similar documents identified by the system was larger, not true positives but instead false positives.
410:510	Hence the recall was high and precision was low at 50%.
411:510	When the threshold was moved up and set at 0.81, the number of similar documents identified was more accurate and the number of false positives got reduced.
412:510	The precision was found to be 66%.
413:510	When the threshold was moved up still further and set at 0.90, it was found that the system identified similar documents which were matching with the human judgment.
414:510	The precision of the system was found to be 90%.
415:510	The threshold was moved up further to 0.95, thinking that the precision would further improve, but this resulted in documents which were actually similar to be filtered out by the system.
416:510	Hence the threshold chosen was 0.9, since the results obtained at this threshold point had matched the human judgment.
417:510	For the experiments E1, E2, E3 and E4 explained above, the threshold is fixed at 0.9.
418:510	A new set of data consisting of 25 documents from 5 days news articles is collected.
419:510	This is completely taken from single domain, terrorism.
420:510	These news articles describe specifically the Hyderabad bomb blast, which occurred on August 25 th  2007.
421:510	All these 25 documents were only English documents from various news magazines.
422:510	This data set was collected specifically to observe the performance of the system, when the documents belonging to single domain are given.
423:510	In the new data set, from terrorism domain, human judgment for document similarity was found to have 13 similar documents forming 3 groups.
424:510	While using this data set the noun phrases, verb phrases and named entities along with POS information were taken as terms to build the language model and the threshold was set at 0.9, it was observed that the system finds 14 documents to be similar forming 3 groups.
425:510	Here, out of 14 similar documents, only 12 documents match with the human judgment and one document which ought to be identified was not identified by the system.
426:510	The document which was not identified described about the current event, that is, bomb blast on 25 th  August in the first paragraph and then the rest of the document described about the similar events that occurred in the past.
427:510	Hence the similarity score obtained for this document with respect to other documents in the group was 0.84 which is lower than the threshold fixed.
428:510	Hence the recall of the system is 92.3% and the precision of the system is 85.7%.
429:510	Another data set consisting of 114 documents was taken from tourism domain.
430:510	The documents were both in Tamil and English, 79 documents in Tamil and 35 documents in English.
431:510	This data set describes various pilgrim places and temples in Southern India.
432:510	The human annotators have found 21 similar documents which form a group of three.
433:510	These similar documents describe about Lord Sivas and Lord Murugans temples.
434:510	The system obtained 25 documents as similar and grouped into three groups.
435:510	Out of 25 documents obtained as similar, four were dissimilar.
436:510	These dissimilar documents described non-Siva temples in the same place.
437:510	In these dissimilar documents the names of offerings, festivals performed were referred by the same names as in the rest of the documents of the group, hence these documents obtained similarity score of 0.96 with respect to other documents in the group.
438:510	Here we get a precision of 84% and a recall of 100%.
439:510	A new data set consisting of 46 documents was taken from various news magazines.
440:510	This set consists of 24 English documents, 11 Tamil documents, 7 Malayalam documents and 4 Telugu documents.
441:510	This data set describes the earthquake in Indonesia on 12 th  September 2007 and tsunami warning in other countries.
442:510	The news articles were collected on two days 13 th  and 14 th  September 2007.
443:510	The documents collected were in different font encoding schemes.
444:510	Hence before doing natural language processing such as morph-analysis, POS tagging etc, the documents were converted to a common roman notation (wx-notation) using the font converter for each encoding scheme.
445:510	Here we have used multilingual dictionaries of place; person names etc for translation.
446:510	The language model is built by taking noun phrases and verb phrases along with POS information were as terms.
447:510	In this set human annotators have found 45 documents to be similar and have grouped them into one group.
448:510	The document which was identified as dissimilar describes about a Tamil film shooting at Indonesia being done during the quake time.
449:510	The system had identified all the 46 documents including the film shooting document in the collection to be similar and put into one group.
450:510	The film shooting document consisted of two paragraphs about the quake incident, other two paragraphs consisted of statement by the film producer stating that the whole crew is safe and the shooting is temporarily suspended for next few days.
451:510	Since this document also contained the content describing the earthquake found in other documents of the group, the system identified this film shooting document to be similar.
452:510	Here one interesting point which was found was that all the documents gave a very high similarity score greater than 0.95.
453:510	Hence the precision of the system is 97.8% and recall 100%.
454:510	The summary of all these experiments with different dataset is shown in the table 2 below.
455:510	SNo Dataset Precision % Recall % 1 English 1004 and Tamil 297 documents 90.0 100.0 2 English 25  terrorism domain documents 85.7 92.3 3 35 English Docs and Tamil 79 docs Tourism domain 84.0 100.0 4 46 Docs on Earth Quake incident  24 English, 11 Tamil, 7 Malayalam, 4 Telugu 97.8 100.0 Average 89.3 % 98.07% Table 2.
456:510	Summary of Results for Document similarity for four different data sets 4.2 Document Co-referencing The documents that were identified as similar ones are taken for entity co-referencing.
457:510	In this work the identification of co-referencing documents is done for English and Tamil.
458:510	In this section first we discuss the co-referencing task for English documents in terrorism domain, then for documents in English and Tamil in Tourism domain.
459:510	In the end of this section we discuss about documents in English and Tamil, which are not domain specific.
460:510	In the first experiment, the document collection in terrorism domain is taken for co-referencing task.
461:510	This data set of 25 documents in terrorism domain consists of 60 unique person names.
462:510	In this work we consider only person names for entity coreferencing.
463:510	In this data set, 14 documents are identified as similar ones by the system.
464:510	These 14 documents consist of 26 unique person names.
465:510	The language model is built using only named entity terms and the noun, verb phrases occurring in the same sentence where the named entity occurs.
466:510	POS information is also provided with the terms.
467:510	Here we find that out of 26 entities, the system co-references correctly for 24 entities, even though the last names are same.
468:510	The results obtained for these named entities is shown in the below table Table 3.
469:510	Entity Name No.
470:510	of links c o ntaining th e en tit y Correct  Re sponse s obtained Total Re-spon se s obtained Pre c ision % Recall  % Y S Rajasekhar Reddy 7 7 7 100 100 Indrasena Reddy 1 1 1 100 100 K Jana Reddy 1 1 1 100 100 Shivaraj Patil 2 2 2 100 100 Manmohan Singh 4 4 4 100 100 Abdul Shahel Mohammad 1 1 2 50 100 Mohammad Abdullah 1 1 2 50 100 Mohammad Amjad 1 1 1 100 100 Mohammad Yunus 1 1 1 100 100 Ibrahim 1 1 1 100 100 Dawood Ibrahim 1 1 1 100 100 Madhukar Gupta 3 3 3 100 100 N Chandrababu Naidu 2 2 2 100 100 Tasnim Aslam 2 2 2 100 100 Mahender Agrawal 1 1 1 100 100 Somnath Chatterjee 2 2 2 100 100 Pervez Musharaff 2 2 2 100 100 Sonia Gandhi 2 2 2 100 100 Taslima 1 1 1 100 100 Nasrin Bandaru Dattatreya 1 1 1 100 100 L K Advani 2 2 2 100 100 Average 95.2 100 Table 3.
471:510	Results for entity co-referencing for English documents in terrorism domain  The system identifies the entity names ending with Reddy correctly.
472:510	These names in the documents occur along with definite descriptions which helps the system in disambiguating these names.
473:510	For example Y S Rajasekhar Reddy in most cases is referred to as Dr. Reddy along with the definite description chief minister.
474:510	Similarly the other name K Jana Reddy occurs with the definite description Home minister.
475:510	Since here we are taking full noun phrases as terms for building language model, this helps obtaining good results.
476:510	For entities such as Abdul Shahel Mohammad and Mohammad Abdullah, it is observed that the both names are referred in the documents as Mohammad and surrounding phrases do not have any distinguishing phrases such as definite descriptions, which differentiate these names.
477:510	Both these entities have been involved in masterminding of the Hyderabad bomb blast.
478:510	Hence the system couldnt disambiguate between these two named entities and identifies both to be same, hence it fails here.
479:510	In the second experiment, the data set in Tourism domain consisting of 79 Tamil Documents and 35 English documents is taken for the task of coreferencing.
480:510	In this data set 25 documents were identified as similar.
481:510	Now these similar documents of 25 are considered for entity co-referencing task.
482:510	There are 35 unique names of Gods.
483:510	Here in this domain, one of the interesting points is that, there are different names to refer to a single God.
484:510	For example Lord Murugan, is also referred by other names such as Subramanyan, Saravana, Karttikeyan, Arumukan etc. Simialrly for Lord Siva is referred by Parangirinathar, Dharbaraneswara etc. It is observed that in certain documents the alias names are not mentioned along with common names.
485:510	In these instances even human annotators found it tough for co-referencing, hence the system could not identify the co-references.
486:510	This problem of alias names can be solved by having a thesaurus and using it for disambiguation.
487:510	The results obtained for these named entities are shown in the table 4, below.
488:510	Entity Name No.
489:510	of links  c o ntaining the e n tity Correct  Re sponse s obtained  Total Re-spon se s obtained Pre c ision % Recall  % Murugan 7 7 8 87.5 100 Shiva 10 9 9 100 90 Parvathi 10 9 11 81.8 90 Nala 5 5 5 100 100 Damayanthi 2 2 2 100 100 Narada 3 3 3 100 100 Saneeswarar 6 6 7 85.7 100 Deivayani 4 4 4 100 100 Vishnu 2 2 2 100 100 Vinayaka 3 3 3 100 100 Indra 2 2 2 100 100 Thirunavukkarasar 1 1 1 100 100 Mayan 2 2 2 100 100 Average 96.5 98.4 Table 4.
490:510	Results for entity co-referencing for English and Tamil Documents in Tourism domain  The co-referencing system could disambiguate a document which was identified as similar by the system and dissimilar by the human annotator.
491:510	Another experiment is performed where both English and Tamil Documents are taken for entity co-referencing.
492:510	In this experiment we have taken the data set in which there are 1004 English documents and 297 Tamil documents.
493:510	The documents are not domain specific.
494:510	Here 100 documents are identified as similar ones, which contains of 64 English and 36 Tamil documents.
495:510	Now we consider these 100 similar documents for entity coreferencing.
496:510	In the 100 similar documents, there are 520 unique named entities.
497:510	The table (Table 5) below shows results of few interesting named entities in this set of 100 similar documents.
498:510	Entity Name No.
499:510	of links c o ntaining th e en tit y Correct  Re sponse s obtained Total Re-spon se s obtained Pre c ision % Recall   % Karunanidhi 7 7 7 100 100 Manmohan Singh 15 14 16 87.5 93.3 Sonia Gandhi 54 54 58 93.1 100 Shivaraj Patil 8 8 10 80 100 Prathibha Patil 24 24 26 92.3 100 Lalu Prasad 5 5 5 100 100 Atal Bihari Vajpayee 4 4 4 100 100 Abdul Kalam 22 22 22 100 100 Sania Mirza 10 10 10 100 100 Advani 8 8 8 100 100 Average 95.3 99.3 Table 5.
500:510	Results for entity co-referencing for English and Tamil Documents not of any specific domain 5 Conclusion The VSM method is a well known statistical method, but here it has been applied for multilingual cross-document similarity, which is a first of its kind.
501:510	Here we have tried different experiments and found that using phrases with its POS information as terms for building language model is giving good performance.
502:510	In this we have got an average precision of 89.3 and recall of 98.07% for document similarity.
503:510	Here we have also worked on multilingual cross-document entity co-referencing and obtained an average precision of 95.6 % and recall of 99.2 %.
504:510	The documents taken for multilingual cross-document co-referencing are similar documents identified by the similarity system.
505:510	Considering similar documents, helps indirectly in getting contextual information for co-referencing entities, because obtaining similar documents removes documents which are not in the same context.
506:510	Hence this helps in getting good precision.
507:510	Here we have worked on four languages viz.
508:510	English, Tamil, Malayalam and Telugu.
509:510	This can be applied for other languages too.
510:510	Multilingual document similarity and co-referencing, helps in retrieving similar documents across languages.

