<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>C C Chibelushi</author>
<author>S Gandon</author>
<author>J S D Mason</author>
</authors>
<title>Design issue for a digital audio-visual integrated database. In Integrated Audio-visual Processing for Recognition, Synthesis and Communication</title>
<date>1996</date>
<contexts>
<context>veral views. Following subdivision is by type of utterances. First type of database is for recognition of vowels and consonants. We can mention French database (Teissier, 1999), or english one David (Chibelushi et al., 1996). Another type are databases for recognition of isolated or connected digits. Database Tulip1 (Movellan, 1995) contain 12 speakers that are saying numbers one to four, French database M2VTS (Pigeon a</context>
</contexts>
<marker>Chibelushi, Gandon, Mason, 1996</marker>
<rawString>C.C Chibelushi, S. Gandon, and J. S. D. Mason. 1996. Design issue for a digital audio-visual integrated database. In Integrated Audio-visual Processing for Recognition, Synthesis and Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P C´ısaˇr</author>
<author>M ˇZelezn´y</author>
<author>Z Krˇnoul</author>
<author>J Kanis</author>
<author>J Zelinka</author>
<author>L M¨uller</author>
</authors>
<title>Design and recording of czech speech corpus for audio-visual continuous speech recognition</title>
<date>2005</date>
<marker>C´ısaˇr, ˇZelezn´y, Krˇnoul, Kanis, Zelinka, M¨uller, 2005</marker>
<rawString>P. C´ısaˇr, M. ˇZelezn´y, Z. Krˇnoul, J. Kanis, J. Zelinka, and L. M¨uller. 2005. Design and recording of czech speech corpus for audio-visual continuous speech recognition.</rawString>
</citation>
<citation valid="true">
<date>2005</date>
<booktitle>In Auditory-Visual Speech Processing Workshop</booktitle>
<marker>2005</marker>
<rawString>In Auditory-Visual Speech Processing Workshop 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P C´ısaˇr</author>
<author>M ˇZelezn´y</author>
<author>J Zelinka</author>
<author>J Trojanov´a</author>
</authors>
<title>Development and testing of new combined visual speech parametrization</title>
<date>2007</date>
<booktitle>In International Conference on Auditory-Visual Speech Processing. C.J.and Cooper D.H Cootes</booktitle>
<marker>C´ısaˇr, ˇZelezn´y, Zelinka, Trojanov´a, 2007</marker>
<rawString>P. C´ısaˇr, M. ˇZelezn´y, J. Zelinka, and J. Trojanov´a. 2007. Development and testing of new combined visual speech parametrization. In International Conference on Auditory-Visual Speech Processing. C.J.and Cooper D.H Cootes, T. F.and Taylor and J. Graham.</rawString>
</citation>
<citation valid="true">
<title>Active shape models their training and application</title>
<date>1995</date>
<booktitle>In Computer Vision and Image Understanding</booktitle>
<volume>61</volume>
<pages>38--59</pages>
<marker>1995</marker>
<rawString>1995. Active shape models their training and application. In Computer Vision and Image Understanding, volume 61, pages 38–59, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Goecke</author>
<author>J B Millar</author>
</authors>
<title>The Audio-Video Australian English Speech Data Corpus AVOZES</title>
<date>2004</date>
<booktitle>In Proceedings of the 8th International Conference on Spoken Language Processing ICSLP2004, volume III</booktitle>
<pages>2525--2528</pages>
<location>Jeju, Korea</location>
<contexts>
<context>2 was in the past. The distribution is also easy thanks to the internet connection. The list of some databases is as follows: LPFAV2(Pera et al., 2004),MANDARIN CHINESE (Liangi et al., 2004), AVOZES (Goecke and Millar, 2004), XM2VTS (Messer et al., 1999), IBM ViaVoice (Neti et al., 2000), UWB-05-HSCAVC (C´ısaˇr et al., 2005). There could be many criteria to compare the databases see Table 1 for other comparison. 3. Data</context>
</contexts>
<marker>Goecke, Millar, 2004</marker>
<rawString>R. Goecke and J.B. Millar. 2004. The Audio-Video Australian English Speech Data Corpus AVOZES. In Proceedings of the 8th International Conference on Spoken Language Processing ICSLP2004, volume III, pages 2525–2528, Jeju, Korea, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luhong Liangi</author>
<author>Yu Luo</author>
<author>Feiyue Huang</author>
<author>A V Nefian</author>
</authors>
<title>A multi-stream audio-video large-vocabulary mandarin chinese speech database</title>
<date>2004</date>
<booktitle>In IEEE International Conference on Multimedia and Expo</booktitle>
<volume>3</volume>
<pages>1787--1790</pages>
<contexts>
<context>ions IC 1-6 look at the table 2 was in the past. The distribution is also easy thanks to the internet connection. The list of some databases is as follows: LPFAV2(Pera et al., 2004),MANDARIN CHINESE (Liangi et al., 2004), AVOZES (Goecke and Millar, 2004), XM2VTS (Messer et al., 1999), IBM ViaVoice (Neti et al., 2000), UWB-05-HSCAVC (C´ısaˇr et al., 2005). There could be many criteria to compare the databases see Tab</context>
</contexts>
<marker>Liangi, Luo, Huang, Nefian, 2004</marker>
<rawString>Luhong Liangi, Yu Luo, Feiyue Huang, and A.V. Nefian. 2004. A multi-stream audio-video large-vocabulary mandarin chinese speech database. In IEEE International Conference on Multimedia and Expo, volume 3, pages 1787 – 1790.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K Messer</author>
<author>J Matas</author>
<author>J Kittler</author>
<author>J L¨uttin</author>
<author>G Maitre</author>
</authors>
<marker>Messer, Matas, Kittler, L¨uttin, Maitre, </marker>
<rawString>K. Messer, J. Matas, J. Kittler, J. L¨uttin, and G. Maitre.</rawString>
</citation>
<citation valid="true">
<title>Xm2vtsdb: The extended m2vts database</title>
<date>1999</date>
<booktitle>In Audioand Video-based Biometric Person Authentication, AVBPA’99</booktitle>
<pages>72--77</pages>
<location>Washington, D.C</location>
<marker>1999</marker>
<rawString>1999. Xm2vtsdb: The extended m2vts database. In Audioand Video-based Biometric Person Authentication, AVBPA’99, pages 72–77. Washington, D.C., March 1999. 16 IDIAP–RR 99-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Movellan</author>
</authors>
<title>Visual speech recognition with stochastic networks</title>
<date>1995</date>
<booktitle>Advances in Neutral Information Processing Systems</booktitle>
<volume>7</volume>
<editor>In G. Tesauro, D. Toruetzky, and Leen T., editors</editor>
<publisher>MIT PRESS</publisher>
<location>Cambridge</location>
<contexts>
<context>nsonants. We can mention French database (Teissier, 1999), or english one David (Chibelushi et al., 1996). Another type are databases for recognition of isolated or connected digits. Database Tulip1 (Movellan, 1995) contain 12 speakers that are saying numbers one to four, French database M2VTS (Pigeon and Vandendorpe, 1997) contain all digits for 37 speakers, English one David (Chibelushi et al., 1996) consist </context>
</contexts>
<marker>Movellan, 1995</marker>
<rawString>J.R. Movellan. 1995. Visual speech recognition with stochastic networks. In G. Tesauro, D. Toruetzky, and Leen T., editors, Advances in Neutral Information Processing Systems, volume 7. Cambridge, MIT PRESS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Neti</author>
<author>G Pontamianos</author>
<author>J Luettin</author>
<author>I Matthews</author>
</authors>
<title>Audio visual speech recognition. Technical report, Center for Language and Speech Processing</title>
<date>2000</date>
<contexts>
<context>t connection. The list of some databases is as follows: LPFAV2(Pera et al., 2004),MANDARIN CHINESE (Liangi et al., 2004), AVOZES (Goecke and Millar, 2004), XM2VTS (Messer et al., 1999), IBM ViaVoice (Neti et al., 2000), UWB-05-HSCAVC (C´ısaˇr et al., 2005). There could be many criteria to compare the databases see Table 1 for other comparison. 3. Data Collection The design criteria for the database is to record da</context>
</contexts>
<marker>Neti, Pontamianos, Luettin, Matthews, 2000</marker>
<rawString>C. Neti, G. Pontamianos, J. Luettin, and I. Matthews. 2000. Audio visual speech recognition. Technical report, Center for Language and Speech Processing.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Patterson</author>
<author>S Gurbuz</author>
<author>Z Tufekci</author>
<author>J N Gowdy</author>
</authors>
<marker>Patterson, Gurbuz, Tufekci, Gowdy, </marker>
<rawString>E. Patterson, S. Gurbuz, Z. Tufekci, and J.N. Gowdy.</rawString>
</citation>
<citation valid="true">
<title>Cuave: a new audio-visual database for multimodal human-computerinterface research</title>
<date>2002</date>
<booktitle>In Acoustics, Speech, and Signal Processing</booktitle>
<volume>2</volume>
<pages>pages</pages>
<marker>2002</marker>
<rawString>2002. Cuave: a new audio-visual database for multimodal human-computerinterface research. In Acoustics, Speech, and Signal Processing, volume 2, pages 2017 – 2020.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vitor Pera</author>
<author>Antonio Moura</author>
<author>Diamantino Freitas</author>
</authors>
<title>Lpfav2: a new multi-modal database for developing speech recognition systems for an assistive technology application</title>
<date>2004</date>
<booktitle>In SPECOM</booktitle>
<contexts>
<context>est snaps are for illumination conditions IC 1-6 look at the table 2 was in the past. The distribution is also easy thanks to the internet connection. The list of some databases is as follows: LPFAV2(Pera et al., 2004),MANDARIN CHINESE (Liangi et al., 2004), AVOZES (Goecke and Millar, 2004), XM2VTS (Messer et al., 1999), IBM ViaVoice (Neti et al., 2000), UWB-05-HSCAVC (C´ısaˇr et al., 2005). There could be many cr</context>
</contexts>
<marker>Pera, Moura, Freitas, 2004</marker>
<rawString>Vitor Pera, Antonio Moura, and Diamantino Freitas. 2004. Lpfav2: a new multi-modal database for developing speech recognition systems for an assistive technology application. In SPECOM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>St´ephane Pigeon</author>
<author>Luc Vandendorpe</author>
</authors>
<title>The m2vts multimodal face database (release 1.00</title>
<date>1997</date>
<booktitle>In AVBPA ’97: Proceedings of the First International Conference on Audioand Video-Based Biometric Person Authentication</booktitle>
<pages>403--409</pages>
<publisher>Springer-Verlag</publisher>
<location>London, UK</location>
<contexts>
<context>l., 1996). Another type are databases for recognition of isolated or connected digits. Database Tulip1 (Movellan, 1995) contain 12 speakers that are saying numbers one to four, French database M2VTS (Pigeon and Vandendorpe, 1997) contain all digits for 37 speakers, English one David (Chibelushi et al., 1996) consist of isolated words, newest one is CUAVE from University of Clemon that contains 36 speakers saying numbers(Patt</context>
</contexts>
<marker>Pigeon, Vandendorpe, 1997</marker>
<rawString>St´ephane Pigeon and Luc Vandendorpe. 1997. The m2vts multimodal face database (release 1.00). In AVBPA ’97: Proceedings of the First International Conference on Audioand Video-Based Biometric Person Authentication, pages 403–409, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Potamianos</author>
<author>C Neti</author>
<author>J Luettin</author>
<author>I Matthews</author>
</authors>
<title>Audio-Visual Automatic Speech Recognition: An Overview, chapter 10. Issue in Visual and Audio-Visual Speech Processing</title>
<date>2004</date>
<contexts>
<context>tely. Rather, it is used as a supplement for acoustic speech recognition. For the visual component of the speech, various parameterizations are used. The overview of existing methods can be found in (Potamianos et al., 2004). 3.1. Database Specification Database UWB-07-ICAVR (UWB stands for University of West Bohemia, 07 for year of recording, ICAVR for impaired conditions audio visual speech recognition) enlarges the p</context>
</contexts>
<marker>Potamianos, Neti, Luettin, Matthews, 2004</marker>
<rawString>G. Potamianos, C. Neti, J. Luettin, and I. Matthews, 2004. Audio-Visual Automatic Speech Recognition: An Overview, chapter 10. Issue in Visual and Audio-Visual Speech Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Radov´a</author>
<author>P Vop´alka</author>
</authors>
<title>Methods of sentences selection for read-speech corpus design</title>
<date>1999</date>
<booktitle>In TSD ’99: Proceedings of the Second International Workshop on Text, Speech and Dialogue</booktitle>
<pages>165--170</pages>
<publisher>Springer-Verlag</publisher>
<location>London, UK</location>
<marker>Radov´a, Vop´alka, 1999</marker>
<rawString>V. Radov´a and P. Vop´alka. 1999. Methods of sentences selection for read-speech corpus design. In TSD ’99: Proceedings of the Second International Workshop on Text, Speech and Dialogue, pages 165–170, London, UK. Springer-Verlag.</rawString>
</citation>
</citationList>
</algorithm>

