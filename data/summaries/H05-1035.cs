In our experiments we used two datasets: • FN – extracted from FrameNet II 1.1 (Baker et al., 1998) • TB2 – extracted from Penn Treebank-II Table 1 presents the datasets1.
P98-1013
Some of these features were proven efficient for semantic information labeling (Gildea and Jurafsky, 2002).
J02-3001
The difference in accuracy between a SVM model applied to RRR dataset (RRR-basic experiment) and the same experiment applied to TB2 dataset (TB2278 Description Accuracy Data Extra Supervision Always noun 55.0 RRR Most likely for each P 72.19 RRR Most likely for each P 72.30 TB2 Most likely for each P 81.73 FN Average human, headwords (Ratnaparkhi et al., 1994) 88.2 RRR Average human, whole sentence (Ratnaparkhi et al., 1994) 93.2 RRR Maximum Likelihood-based (Hindle and Rooth, 1993) 79.7 AP Maximum entropy, words (Ratnaparkhi et al., 1994) 77.7 RRR Maximum entropy, words & classes (Ratnaparkhi et al., 1994) 81.6 RRR Decision trees (Ratnaparkhi et al., 1994) 77.7 RRR Transformation-Based Learning (Brill and Resnik, 1994) 81.8 WordNet Maximum-Likelihood based (Collins and Brooks, 1995) 84.5 RRR Maximum-Likelihood based (Collins and Brooks, 1995) 86.1 TB2 Decision trees & WSD (Stetina and Nagao, 1997) 88.1 RRR WordNet Memory-based Learning (Zavrel et al., 1997) 84.4 RRR LexSpace Maximum entropy, unsupervised (Ratnaparkhi, 1998) 81.9 Maximum entropy, supervised (Ratnaparkhi, 1998) 83.7 RRR Neural Nets (Alegre et al., 1999) 86.0 RRR WordNet Boosting (Abney et al., 1999) 84.4 RRR Semi-probabilistic (Pantel and Lin, 2000) 84.31 RRR Maximum entropy, ensemble (McLauchlan, 2001) 85.5 RRR LSA SVM (Vanschoenwinkel and Manderick, 2003) 84.8 RRR Nearest-neighbor (Zhao and Lin, 2004) 86.5 RRR DWS FN dataset, w/o semantic features (FN-best-no-sem) 91.79 FN PR-WWW FN dataset, w/ semantic features (FN-best-sem) 92.85 FN PR-WWW TB2 dataset, best feature set (TB2-best) 93.62 TB2 PR-WWW Table 5: Accuracy of PP-attachment ambiguity resolution (our results in bold) basic experiment) is 2.9%.
C94-2195 H94-1048 J93-1005 P00-1014 P98-2177 W95-0103 W97-0109 W97-1016 W99-0606 W99-0628
Morphological processing applied to n1 and n2 was inspired by the algorithm described in (Collins and Brooks, 1995).
W95-0103
The best accuracy in cross-validation is 92.92%, which leads to an accuracy on test set of 93.62%. 7 Comparison with previous work Because we couldn’t use the standard dataset used in PP-attachment resolution (Ratnaparkhi’s), we implemented back-off algorithm developed by Collins and Brooks (1995) and applied it to our TB2 dataset.
W95-0103
• LexSpace Lexical Space – a method to measure the similarity of the words (Zavrel et al., 1997).
W97-1016
We have adopted the notation from (Collins and Brooks, 1995), where v is the verb, n1 is the head noun of object phrase, p is the preposition and n2 is the head noun of the prepositional phrase.
W95-0103
The performance of the system on Penn Treebank-II exceeds the reported human expert performance on Penn Treebank-I (Ratnaparkhi et al., 1994) by about 0.4%.
H94-1048
The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions.
A00-2018
Compared to our datasets, Ratnaparkhi’s dataset (Ratnaparkhi et al., 1994) contains only the lexical heads v, n1, p and n2.
H94-1048
Other acronyms used in this table: • AP – dataset of 13 million word sample of Associated Press news stories from 1999 (Hindle and Rooth, 1993).
J93-1005
But if one limits the information used for disambiguation of the PPattachment to include only the verb, the noun representing its object, the preposition and the main noun in the PP, the accuracy for human decision degrades from 93.2% to 88.2% (Ratnaparkhi et al., 1994) on a dataset extracted from Penn Treebank (Marcus et 273 al., 1993).
H94-1048 J93-2004
We adopted this feature from (Gildea and Jurafsky, 2002).
J02-3001
