Our word bits construction algorithm (Ushioda 1996) is a modification and an extension 29 of the mutual information (MI) clustering algorithm proposed by Brown et al.(1992). The reader is referred to (Ushioda 1996) and (Brown et al.1992) for details of MI clustering, but we will first briefly summarize the MI clustering and then describe our hierarchical clustering algorithm.
C96-2212 J92-4003
The ATR corpus is a comprehensive sampling of Written American English, displaying language use in a very wide range of styles and settings, and compiled from many different domains (Black et al.1996). Since the ATR corpus is still in the process of development, the size of the texts we have at hand for this experiment is rather minimal considering the large size of the tag set.
C96-1020
Most of the previously proposed methods to extract compounds or to measure word association using mutual information (MI) either ignore or penalize items with low co-occurrence counts (Church and Hanks 1990, Su, Wu and Chang 1994), because MI becomes unstable when the co-occurrence counts are very small.
J90-1003 P94-1033
We used *TH*=3 following "a very rough rule of thumb" used for word-based mutual information in (Church and Hanks, 1990).
J90-1003
An ideal type of clusters for NLP is the one which guarantees mutual substitutability, in terms of both syntactic and semantic soundness, among words in the same class (Harris 1951, Brill and Marcus 1992).
H92-1030
Clustering of Words Several algorithms have been proposed for automatically clustering words based on a large corpus (Jardino and Adda 91, Brown et al.1992, Kneser and Ney 1993, Martin et al.1995, Ueberla 1995).
J92-4003
While several methods have been proposed to automatically extract compounds (Smadja 1993, Suet al.1994), we know of no successful attempt to automatically make classes of compounds.
J93-1007 P94-1033
