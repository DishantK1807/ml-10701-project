Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 861–869,
Suntec, Singapore, 2-7 August 2009. c©2009 ACL and AFNLP
Semantic Taging of Web Search Queries 
 
 
Mehdi Manshadi Xiao Li 
University of Rochester 
Microsoft Research 
Rochester, NY Redmond, WA 
mehdih@cs.rochester.edu xiaol@microsoft.com 
  
 
 
 
 
Abstract 
We present a novel aproach to parse web 
search queries for the purpose of automatic 
taging of the queries. We wil define a set 
of probabilistic context-fre rules, which 
generates bags (i.e. multi-sets) of words. Us-
ing this new type of rule in combination 
with the traditional probabilistic phrase 
structure rules, we define a hybrid gramar, 
which treats each search query as a bag of 
chunks (i.e. phrases). A hybrid probabilistic 
parser is used to parse the queries. In order 
to take contextual information into acount, 
a discriminative model is used on top of the 
parser to re-rank the n-best parse tres gen-
erated by the parser. Experiments show that 
our aproach outperforms a basic model, 
which is based on Conditional Random 
Fields. 
1 Introduction

    Understanding users’ intent from web search 
queries is an important step in designing an intel-
ligent search engine.  While it remains a chal-
lenge to have a scientific definition of ''intent', 
many eforts have ben devoted to automaticaly 
maping queries into diferent domains i.e. topi-
cal clases such as product, job and travel 
(Broder et al. 207; Li et al. 2008). This work 
goes beyond query-level clasification. We as-
sume that the queries are already clasified into 
the corect domain and investigate the problem of 
semantic taging at the word level, which is to 
asign a label from a set of pre-defined semantic 
labels (specific to the domain) to every word in 
the query. For example, a search query in the 
product domain can be taged as: 
cheap    garmin  stretpilot  c340    gps 
    |              |              |             |           | 
SortOrder Brand   Model   Model  Type 
 
Many specialized search engines build their in-
dexes directly from relational databases, which 
contain highly structured information. Given a 
query taged with the semantic labels, a search 
engine is able to compare the values of semantic 
labels in the query (e.g., Brand = “garmin”) with 
its counterpart values in documents, thereby pro-
viding users with more relevant search results. 
    Despite this importance, there has ben rela-
tively litle published work on semantic taging 
of web search queries. Alan and Raghavan 
(202) and Bar et al. (208) study the linguistic 
structure of queries by performing part-of-spech 
taging.  Pasca et al. (207) use queries as a 
source of knowledge for extracting prominent 
atributes for semantic concepts.  
On the other hand, there has ben much work 
on extracting structured information from larger 
text segments, such as adreses (Kushmerick 
201), bibliographic citations (McCalum et al. 
1999), and clasified advertisements (Grenager 
et al. 205),  among many others. The most 
widely used aproaches to these problems have 
ben sequential models including hiden Markov 
models (HMs), maximum entropy Markov mod-
els (MEMs) (Mcalum 200), and conditional 
random fields (CRFs) (Laferty et al. 201) 
These sequential models, however, are not op-
timal for procesing web search queries for the 
folowing reasons.. The first problem is that the 
global constraints and long distance dependencies 
on state variables are dificult to capture using 
sequential models. Because of this limitation, 
Viola and Narasimhand (207) use a discrimina-
tive context-fre (phrase structure) gramar for 
extracting information from semi-structured data 
and report higher performances over CRFs. 
    Secondly, sequential models treat the input text 
as an ordered sequence of words. A web search 
query, however, is often formulated by a user as a 
bag of keywords. For example, if a user is lok-
861
ing for cheap garmin gps, it is posible that the 
query comes in any ordering of these thre 
words. We are loking for a model that, once it 
observes this query, asumes that the other per-
mutations of the words in this query are also 
likely. This model should also be able to handle 
cases where some local orderings have to be 
fixed as in the query buses from New York City to 
Boston, where the words in the phrases from New 
York city and to Boston have to come in the exact 
order. 
The third limitation is that the sequential mod-
els treat queries as unstructured (linear) se-
quences of words. The study by Bar et al. (208) 
on Yahoo! query logs sugests that web search 
queries, to some degre, cary an underlying lin-
guistic structure. As an example, consider a query 
about finding a local busines near some location 
such as: 
seatle wa drugstore 24/7 98109 
This query has two constituents: the Busines that 
the user is loking for (24/7 drugstore) and the 
Neighborhod (seatle wa 98109). The model 
should not only be able to recognize the two con-
stituents but it also neds to understand the struc-
ture of each constituent. Note that the arbitrary 
ordering of the words in the query is a big chal-
lenge to understanding the structure of the query. 
The problem is not only that the two constituents 
can come in either order, but also that a sub-
constituent such as 98109 can also be far from 
the other words belonging to the same constitu-
ent. We are loking for a model that is able to 
generate a hierarchical structure for this query as 
shown in figure (1). 
The last problem that we discus here is that 
the two powerful sequential models i.e. MEM 
and CRF are discriminative models; hence they 
are highly dependent on the training data. Prepar-
ing labeled data, however, is very expensive. 
Therefore in cases where there is no or a smal 
amount of labeled data available, these models do 
a por job.   
In this paper, we define a hybrid, generative 
gramar model (section 3) that generates bags of 
phrases (also caled chunks in this paper). The 
chunks are generated by a set of phrase structure 
(PS) rules. At a higher level, a bag of chunks is 
generated from individual chunks by a second 
type of rule, which we cal context-fre multiset 
generating rules. We define a probabilistic ver-
sion of this gramar in which every rule has a 
probability asociated with it. Our gramar 
model eliminates the local dependency asump-
tion made by sequential models and the ordering 
constraints imposed by phrase structure gram-
mars (PSG). This model beter reflects the under-
lying linguistic structure of web search queries. 
The model’s power, however, comes at the cost 
of increased time complexity, which is exponen-
tial in the length of the query. This, is les of an 
isue for parsing web search queries, as they are 
usualy very short (2.8 words/query in average 
(Xue et al., 204).  
Yet another drawback of our aproach is due 
to the context-fre nature of the proposed gram-
mar model. Contextual information often plays a 
big role in resolving taging ambiguities and is 
one of the key benefits of discriminative models 
such as CRFs. But such information is not 
straightforward to incorporate in our gramar 
model. To overcome this limitation, we further 
present a discriminative re-ranking module on top 
of the parser to re-rank the n-best parse tres gen-
erated by the parser using contextual features. As 
sen later, in the case where there is not a large 
amount of labeled data available, the parser part 
is the dominant part of the module and performs 
reasonably wel. In cases where there is a large 
amount of labeled data available, the discrimina-
tive re-ranking incorporates into the system and 
enhances the performance. We evaluate this 
model on the task of taging search queries in the 
product domain. As sen later, preliminary ex-
periments show that this hybrid genera-
tive/discriminative model performs significantly 
beter than a CRF-based module in both absence 
and presence of the labeled data. 
The structure of the paper is as folows. Sec-
tion 2 introduces a linguistic gramar formalism 
that motivates our gramar model. In section 3, 
we define our gramar model. In section 4 we 
adres the design and implementation of a 
parser for this kind of gramar. Section 5 gives 
an example of such a gramar designed for the 
purpose of automatic taging of queries. Section 
6 discuses motivations for and benefits of run-
ning a discriminative re-ranker on top of the 
parser. In section 7, we explain the evaluations 
 
Figure 1. A simple gramar for product domain 
 
862
and discus the results. Section 8 sumarizes this 
work and discuses future work. 
2 ID/LP Grammar 
Context-fre phrase structure gramars are 
widely used for parsing natural language. The 
adequate power of this type of gramar plus the 
eficient parsing algorithms available  for it has 
made it very popular. PSGs treat a sentence as an 
ordered sequence of words. There are however 
natural languages that are fre word order. For 
example, a thre-word sentence consisting of a 
subject, an object and a verb in Rusian, can 
ocur in al six posible orderings. PSGs are not 
a wel-suited model for this type of language, 
since six diferent PS-rules must be defined in 
order to cover such a simple structure. To adres 
this isue, Gazdar (1985) introduced the concept 
of ID/LP rules within the framework of 
Generalized Phrase Structure Gramar (GPSG). 
In this framework, Imediate Dominance or ID 
rules are of the form: 
(1) A→ B, C 
This rule specifies that a non-terminal A can be 
rewriten as B and C, but it does not specify the 
order. Therefore A can be rewriten as both BC 
and CB. In other words the rule in (1) is 
equivalent to two PS-rules: 
(2) A → BC 
A  CB 
Similarly one ID rule wil sufice to cover the 
simple subject-object-verb structure in Rusian: 
(3) S  Sub, Obj, Vrb 
However even in fre-word-order languages, 
there are some ordering restrictions on some of 
the constituents. For example in Rusian an 
adjective always comes before the noun that it 
modifies. To cover these ordering restrictions, 
Gazdar defined Linear Precedence (LP) rules. (4) 
gives an example of a linear precedence rule: 
(4) ADJ < N 
This specifies that ADJ always comes before N 
when both ocur on the right-hand side of a 
single rule. 
   Although very intuitive, ID/LP rules are not 
widely used in the area of natural language 
procesing. The main reason is the time-
complexity isue of ID/LP gramar. It has been 
shown that parsing ID/LP rules is an NP-
complete problem (Barton 1985). Since the 
length of a natural language sentence can easily 
reach 30-40 (and sometimes even up to 10) 
words, ID/LP gramar is not a practical model 
for natural language syntax. In our case, however, 
the time-complexity is not a botleneck as web 
search queries are usualy very short (2.8 words 
per query in average). Moreover, the nature of ID 
rules can be deceptive as it might apear that ID 
rules alow any reordering of the words in a valid 
sentence to ocur as another vaild sentence of the 
language. But in general this is not the case. For 
example consider a gramar with only two ID 
rules given in (5) and consider S as the start 
symbol: 
(5) S → B, c 
B   d, e 
It can be easily verified that dec is a sentence of 
the language but dce is not. In fact, although the 
permutation of subconstituents of a constituent is 
alowed, a subconstituent can not be puled out 
from its mother consitutent and frely move 
within the other constituents. This kind of 
movement however is a comon behaviour in 
web search queries as shown in figure (1). It 
means that even ID rules are not powerful enough 
to model the fre-word-order nature of web 
search queries.  This leads us to define to a new 
type of gramar model. 
3 Our
Grammar Model 
3.1 The
basic model 
We propose a set of rules in the form: 
(6) S → {B, c} 
B  {D, E} 
D → {d} 
E  {e} 
which can be used to generate multisets of words. 
For the notation convenience and consistancy, 
throughout this paper, we show terminals and 
non-terminals by lowercase and upercase leters, 
respectively and sets and multisets by bold font 
upercase leters. Using the rules in (6) a 
sentence of the language (which is a multiset in 
this model) can be derived as folows: 
(7) S ⇒ {B, c} ⇒ {D, E, c} ⇒ {D, e, c}⇒ {d, e, c} 
Once the set is generated, it can be realized as 
any of the six permutation of d, e, and c. 
Therefore a single sequence of derivations can 
lead to six diferent strings of words. As another 
example consider the gramar in (8). 
(8) Query →  {Busines, Location} 
Busines   {Atribute, Busines} 
Location → {City, State} 
Busines   {drugstore} | {Resturant} 
Atribute→ {Chinese} | {24/7} 
City→ {Seatle} | {Portland} 
State {WA} | {OR} 
863
where Query is the start symbol and by A → B|C 
we mean two difernet rules A → B and A  C. 
Figures (2) and (3) show the tre structures for 
the queries Restaurant Rochester Chinese MN, 
and Rochester MN Chinese Restaurant, 
respectively. As sen in these figures, no mater 
what the order of the words in the query is, the 
gramar always groups the words Resturant and 
Chinese together as the Busines and the words 
Rochester and MN together as the Location. It is 
important to notice that the above gramars are 
context-fre as every non-terminal A, which 
ocurs on the left-hand side of a rule r, can be 
replaced with the set of terminals and non-
terminals on the right-hand side of r, no mater 
what the context in which A ocurs is. 
More formaly we define a Context-Fre 
multiSet generating Gramar (CFSG) as a 4-
tuple G=(N, T, S, R) where 
• N is a set of non-terminals; 
• T is a set of terminals; 
• S ∈ N is a special non-terminal caled 
start symbol, 
• R is a set of rules {A
i
→ X
j
} where A
i
 is a 
non-terminal and X
j
 is a set of terminals 
and non-terminals. 
Given two multisets Y and Z over the set N ∪ T, 
we say Y dervies Z (shown as Y ⇒ Z) if there 
exists A, W, and X such that: 
Y =  + {A}
1
 
Z = W + X 
A→ X ∈ R 
Here ⇒
*
 is defined as the reflexive transitive 
closure of ⇒. Finaly we define the language of 
multisets generated by the gramar G (shown as 
L(G) as 
L = { X | X is a multiset over N∪T and S ⇒
*
X} 
The sequence of ⇒ used to derive X from S is 
caled a derivation of X. Given the above 
                                                
1
 If X and Y are two multisets, X+Y simply means apend-
ing X to Y. For example {a, b, a} + {b, c, d} = {a, b, a, b, c, 
d}. 
definitions, parsing a multiset X means to find al 
(if any) the derivations of X from S. 
2
 
3.2 Probabilisic
CFSG 
Very often a sentence in the language has more 
than one derivation, that is the sentence is 
syntacticaly ambiguous. One natural way of 
resolving the ambiguity is using a probabilistic 
gramar. Analogous to PCFG (Maning and 
Schütze 199), we define the probabilistic 
version of a CFSG, in which every rule A
i
→X
j
 has 
a probability P(A
i
→X
j
) and for every non-
terminal A
i, we have: 
(9) Σ
j
 P(A
i
→ X
j
) = 1 
Consider a sentence w
1
w
2
…w
n, a parse tre T of 
this sentence, and an interior node v in T labeled 
with A
v
 and asume that v
1, v
2, …v
k
 are the 
children of the node v in T. We define: 
(10) α(v) = P(A
v
→ {A
v1
… A
vk
})α(v
1
) … α(v
k
) 
with the initial conditions α(w
i
)=1. If u is the rot 
of the tre T we have: 
(1) P(w
1
w
2
…w
n
 , T) = α(u) 
The parse tre that the probabilistic model 
asigns to the sentence is defined as: 
(12) T
max
 = argmax
T
 (P(w
1
w
2
…w
n
 , T) 
where T ranges over al posible parse tres of the 
sentence. 
4 Parsing
Algorithm 
4.1 Deterministic
parser 
The parsing algorithm for the CFSG is straight-
forward. We used a modified version of the Bot-
tom-Up Chart Parser for the phrase structure 
gramars (Alen 195, se 3.4). Given the 
gramar G=(N,T,S,R) and the query 
q=w
1
w
2
…w
n, the algorithm in figure (4) is used to 
parse q. The algorithm is based on the concept of 
an active arc. An active arc is defined as a 3–
                                                
2
 Every sentence of a language coresponds to a vector of |T| 
integers where the k
th
 element represents how many times 
the k
th
 terminal ocurs in the multi-set. In fact, the languages 
defined by gramars are not interesting but the derivations 
are.  
 
Figure 2. A CFSG parse tre 
 
 
Figure 3. A CFSG parse tre 
 
864
tuple (r, U, I) where r is a rule A → X in R, U is a 
subset of X, and I is a subset of {1, 2 …n} (where 
n is the number of words in the query). This ac-
tive arc tries to find a match to the right-hand side 
of r (i.e. X) and sugests to replace it with the 
non-terminal A. U contains the part of the right-
hand side that has not ben matched yet. There-
fore when an arc is newly created U=X. Equiva-
lently, X\U
3
 is the part of the right hand side that 
has so far ben matched with a subset of words in 
the query, where I stores the positions of these 
words in q.  
An active arc is completed when U=Ø. Every 
completed active arc can be reduced to a tuple (A, 
I), which we cal a constituent. A constituent (A, 
I) shows that the non-terminal A matches the 
words in the query that are positioned at the 
numbers in I. Every constituent that is built by 
the parser is stored in a data structure caled chart 
and remains there throughout the whole proces. 
Agenda is another data structure that temporarily 
stores the constituents. At initialization step, the 
constituents (w
1, {1}), … (w
n, {n}) are aded to 
both chart and agenda. At each iteration, we pul 
out a constituent from the agenda and try to find a 
match to this constituent from the remaining list 
of terminals and non-terminals on the right-hand 
side of an active arc. More precisely, given a 
constituent c=(A, I) and an active arc γ = 
(r:BX, U, J), we check if A ∈ U and I ∩ J = 
Ø; if so, γ is extendable by c, therefore we extend 
γ by removing A from U and apending I to J. 
Note that the extension proces keps a copy of 
every active arc before it extends it. In practice 
every active arc and every constituent kep a set 
of pointers to its children constituents (stored in 
chart). This information is necesary for the ter-
mination step in order to print the parse tres. The 
algorithm suceds if there is a constituent in the 
chart that coresponds to the start symbol and 
covers al the words in the query, i.e. there is a 
constituent of the form (S, {1,2,….n}) in the 
chart. 
4.2 Probabilistic
Parser 
The algorithm given in figure (4) works for a de-
terministic gramar. As mentioned before, we 
use a probabilistic version of the gramar. 
Therefore the algorithm is modified for the prob-
abilistic case. The probabilistic parser keps a 
probability p for every active arc and every con-
stituent: 
γ = (r, U, J, p
γ 
) 
                                                
3
 A\B is defined as {x | x ∈ A & x ∉ B} 
c =(A, I, p
c 
) 
When extending γ using c, we have: 
(13) p
γ
 ← p
γ
 p
c
 
When creating c from the completed active arc γ : 
(14) p
c
 ← p
γ
 p(r) 
Although search queries are usualy short, the 
runing time is stil an isue when the length of 
the query exceds 7 or 8. Therefore a couple of 
techniques have ben used to make the naïve al-
gorithm more eficient. For example we have 
used pruning techniques to filter out structures 
with very low probability. Also, a dynamic pro-
graming version of the algorithm has ben 
used, where for every subset I of the word posi-
tions and every non-terminal A only the highest-
ranking constituent c=(A, I, p) is kept and the rest 
are ignored. Note that although more eficient, 
the dynamic programing version is stil expo-
nential in the length of the query. 
5 A
grammar for semantic tagging  
As mentioned before, in our system queries are 
already clasified into diferent domains like 
movies, boks, products, etc. using an automatic 
query clasifier. For every domain we have a 
schema, which is a set of pre-defined tags. For 
example figure (5) shows an example of a 
schema for the product domain. The task defined 
for this system is to automaticaly tag the words 
in the query with the tags defined in the schema: 
cheap    garmin  stretpilot  c340    gps 
|                |               |               |            | 
SortOrder Brand   Model   Model  Type 
 
Initialization: 
For each word w
i 
n q ad (w
i, {i}) to Chart and 
to Agenda 
For al r: A→X in R, create an active arc (r, X, 
{}) and ad it to the list of active arcs. 
 
Iteration 
Repeat 
Pul a constituent c = (A, I) from Agenda 
For every active arc γ =(r:BX, U, I) 
 Extend γ using c if extendable 
 If U=Ø ad (B, I) to Chart and to Agenda 
Until Agenda is empty 
 
Termination 
For every item c=(S, {1.n}) in Chart, return the 
tre roted at c. 
Figure 4. An algorithm for parsing deterministic 
CFSG 
865
We mentioned that one of the motivations of 
parsing search queries is to have a deper under-
standing of the structure of the query. The 
evaluation of such a dep model, however, is not 
an easy task. There is no Trebank available for 
web search queries. Furthermore, the definition 
of the tre structure for a query is quite arbitrary. 
Therefore even when human resources are avail-
able, building such a Trebank is not a trivial 
task. For these reasons, we evaluate our gramar 
model on the task of automatic taging of queries 
for which we have labeled data available. The 
other advantage of this evaluation is that there 
exists a CRF-based module in our system used 
for the task of automatic taging. The perform-
ance of this module can be considered as the 
baseline for our evaluation.  
We have manualy designed a gramar for 
the purpose of automatic taging. The resources 
available for training and testing were a set of 
search queries from the product domain. There-
fore a set of CFSG rules were writen for the 
product domain. We defined very simple and 
intuitive rules (shown in figure 6) that could eas-
ily be generalized to the other domains 
Note that Type, Brand, Model, … could be 
either pre-terminals generating word tokens, or 
non-terminals forming the left-hand side of the 
phrase structure rules. For the product domain, 
Type and Atribute are generated by a phrase 
structure gramar. Model and Atribute may also 
be generated by a set of manualy designed regu-
lar expresions. The rest of the tags are simply 
pre-terminals generating word tokens. Note that 
we have a lexicon, e.g.., a Brand lexicon, for al 
the tags except Type and Atribute. The model, 
however, extends the lexicon by including words 
discovered from labeled data (if available). The 
gray color for a non-terminal on the right-hand 
side (RHS) of some rule means that the non-
terminal is optional (se Query rule in figure (6). 
We used the optional non-terminals to make the 
task of defining the gramar easier. For example 
if we consider a rule with n optional non-
terminals on its RHS, without optional non-
terminals we have to define 2
n
 diferent rules to 
have an equivalent gramar. The parser can treat 
the optional non-terminals in diferent ways such 
as pre-compiling the rules to the equivalent set of 
rules with no optional non-terminal, or directly 
handling optional non-terminals during the pars-
ing. The first aproach results in exponentialy 
many rules in the system, which causes sparsity 
isues when learning the probability of the rules. 
Therefore in our system the parser handles op-
tional non-terminals directly. In fact, every non-
terminal has its own probability for not ocuring 
on the RHS of a rule, therefore the model learns 
n+1 probabilities for a rule with n optional non-
terminals on its RHS: one for the rule itself and 
one for every non-terminal on its RHS. It means 
that instead of learning 2
n
 probabilities for 2
n
 dif-
ferent rules, the model only learns n+1 probabili-
ties. That solves the sparsity problem, but causes 
another isue which we cal short length prefer-
ence. This ocurs because we have asumed that 
the probability of a non-terminal being optional is 
independent of other optional non-terminals. 
Since for almost al non-terminals on the RHS of 
the query rule, the probability that the non-
terminal does not exist in an instance of a query 
is higher than 0.5, a nul query is the most likely 
query that the model generates! We solve this 
problem by conditioning the probabilities on the 
length of queries. This brings a trade-of betwen 
the two other alternatives: ignoring sparsity prob-
lem to prevent making many independence as-
sumptions and making a lot of independence 
asumptions to adres the sparsity isue. 
 Unlike sequential models, the gramar 
model is able to capture critical global con-
straints. For example, it is very unlikely for a 
query to have more than one Type, Brand, etc. 
This is an important property of the product que-
ries that can help to resolve the ambiguity in 
many cases. In practice, the probability that the 
model learns for a rule like: 
Query → {Brand*, Product*, Model*, …} 
Brand* → {Brand} 
Brand*  {Brand*, Brand} 
Type* → {Type} 
Type*  {Type*, Type} 
Model* → {Model} 
Model*  {Model*, Model} 
… 
Figure 6. A simple gramar for product domain 
 
Type: Camera, Shoe, Cel phone, … 
Brand: Canon, Nike, At&t, … 
Model: dc170, powershot, ipod nano 
Atribute: 1GB, 7mpixel, 3X, … 
BuyingIntenet: Sale, deal, … 
ResearchIntent:  Review, compare, … 
SortOrder: Best, Cheap, … 
Merchant:  Walmart, Target, … 
 
Figure 5. Example of schema for product domain 
 
866
Type*  {Type*, Type} 
compared to the rule: 
Type*  Type 
is very smal; the model penalizes the ocurence 
of more than one Type in a query. Figure (7a) 
shows an example of a parse tre generated for 
the query “Canon vs Sony Camera” in which B, 
Q, and T are abreviations for Brand, Query, and 
Type, and U is a special tag for the words that 
does not fal into any other tag categories and 
have ben left unlabeled in our corpus such as a, 
the, for, etc. Therefore the parser asigns the tag 
sequence B U B T to this query. It is true that the 
word “vs” plays a critical role in this query, rep-
resenting that the user’s intention is to compare 
the two brands; but as mentioned above in our 
labeled data such words has left unlabeled. The 
general model, however, is able to easily capture 
these sorts of phenomena. 
A more careful lok at the gramar shows 
that there is another parse tre for this query as 
shown in figure (7b). These two tres basicaly 
represent the same structure and generate the 
same sequence of tags. The number of tres gen-
erated for the same structure increases exponen-
tialy with the number of equal tags in the tre. 
To prevent this over-generation we used rules 
analogous to GPSG’s LP rules such as: 
B* < B 
which alows only a unique way of generating a 
bag of the Brand tags.  Using this LP rule, the 
only valid tre for the above query is the one in 
figure (7a). 
6 Discriminative
re-ranking 
By using a context-fre gramar, we are mising 
a great source of clues that can help to resolve 
ambiguity. Discriminative models, on the other 
hand, alow us to define numerous features, 
which can coperate to resolve the ambiguities. 
Similar studies in parsing natural language sen-
tences (Colins and Ko 2005) have shown that 
if, instead of taking the most likely tre structure 
generated by a parser, the n-best parse tres are 
pased through a discriminative re-ranking mod-
ule, the acuracy of the model wil increase sig-
nificantly. We use the same idea to improve the 
performance of our model. We run a Suport 
Vector Machine (SVM) based re-ranking module 
on top of the parser. Several contextual features 
(such as bigrams) are defined to help in disam-
biguation. This combination provides a frame-
work that benefits from the advantages of both 
generative and discriminative models. In particu-
lar, when there is no or a very smal amount of 
labeled data, a parser could stil work by using 
unsupervised learning aproaches to learn the 
rules, or by simply using a set of hand-built rules 
(as we did above for the task of semantic tag-
ging). When there is enough labeled data, then a 
discriminative model can be trained on the la-
beled data to learn contextual information and to 
further enhance the taging performance. 
7 Evaluation

Our resources are a set of 2100 manualy la-
beled queries, a manualy designed gramar, a 
lexicon for every tag (except Type and Atribute), 
and a set of regular expresions defined for Mod-
els and Atributes. Note that with a gramar 
similar to the one in figure (6), generating a parse 
tre from a labeled query is straightforward. Then 
the parser is trained on the tres to learn the pa-
rameters of the model (probabilities in this case). 
We randomly extracted 300, out of 2100, 
queries as the test set and used the remaining 
1800 for training. We created training sets with 
diferent sizes to evaluate the impact of training 
data size on taging performance. 
Thre modules were used in the evaluation: 
the CRF-based model
4, the parser, and the parser 
plus the SVM-based re-ranking. Figure (8) shows 
the learning curve of the word-level F-score for 
al the thre modules. As sen in this plot, when 
there is a smal amount of training data, the 
parser performs beter than the CRF module and 
parser+SVM module performs beter than the 
other two. With a large amount of training data, 
the CRF and parser almost have the same per-
formance. Once again the parser+SVM module 
                                                
4
 The CRF module also uses the lexical resources and regu-
lar expresions. In fact, it aplies a deterministic context fre 
gramar to the query to find al the posible groupings of 
words into chunks and uses this information as a set of fea-
tures in the system. 
 
Figure 7. Two equivalent CFSG parse tres 
 
867
outperforms the other two. These results show 
that, as expected, the CRF-based model is more 
dependent on the training data than the parser. 
Parser+SVM always performs at least as wel as 
the parser-only module even with a very smal 
set of training data. This is because the rank 
given to every parse tre by the parser is used as 
a feature in the SVM module. When there is a 
very smal amount of training data, this feature is 
dominant and the output of the re-reranking 
module is basicaly the same as the parser’s 
highest-rank output. Table (1) shows the per-
formance of al thre modules when the whole 
training set was used to train the system. The first 
thre columns in the table show the word-level 
precision, recal, and F-score; and the last column 
represents the query level acuracy (a query is 
considered corect if al the words in the query 
have ben labeled corectly). There are two rows 
for the parser+SVM in the table: one for n=2 (i.e. 
re-ranking the 2-Best tres) and one for n=10. It 
is interesting to se that even with the re-ranking 
of only the first two tres generated by the 
parser, the diference betwen the acuracy of 
the parser+SVM module and the parser-only 
module is quite significant. Re-ranking with a 
larger number of tres (n>10) did not increase 
performance significantly. 
8 Summary

We introduced a novel aproach for dep parsing 
of web search queries. Our aproach uses a 
gramar for generating multisets caled a con-
text-fre multiset generating gramar (CFSG). 
We used a probabilistic version of this gramar. 
A parser was designed for parsing this type of 
gramar. Also a discriminative re-ranking mod-
ule based on a suport vector machine was used 
to take contextual information into acount. We 
have used this system for automatic taging of 
web search queries and have compared it with a 
CRF-based model designed for the same task.  
The parser performs much beter when there is 
a smal amount of training data, but an adequate 
lexicon for every tag. This is a big advantage of 
the parser model, because in practice providing 
labeled data is very expensive but very often the 
lexicons can be easily extracted from the struc-
tured data on the web (for example extracting 
movie titles from imdb or bok titles from Ama-
zon).  
Our hybrid model (parser plus discriminative 
re-ranking), on the other hand, outperforms the 
other two modules regardles of the size of the 
training data.  
The main drawback with our aproach is to 
completely ignore the ordering. Note that al-
though strict ordering constraints such as those 
imposed by PSG is not apropriate for modeling 
query structure, it might be helpful to take order-
ing information into acount when resolving am-
biguity. We leave this for future work. Another 
interesting and practicaly useful problem that we 
have left for future work is to design an unsuper-
vised learning algorithm for CFSG similar to its 
phrase structure counterpart: inside-outside algo-
rithm (Baker 1979). Having such a capability, we 
are able to automaticaly learn the underlying 
structure of queries by procesing the huge 
amount of available unlabeled queries. 
Acknowledgement 
We ned to thank Ye-Yi Wang for his helpful 
advices. We also thank Wiliam de Beaumont for 
his great coments on the paper. 
References  
Alan, J. and Raghavan, H. (202) Using Part-of-
spech Paterns to Reduce Query Ambiguity, Pro-
cedings of SIGIR 202, p. 307-314. 
Alen, J. F. (195) Natural Language Understanding, 
Benjamin Cumings. 
Baker, J. K. (1979) Trainable gramars for spech 
recognition. In Jared J. Wolf and Denis H. Klat, 
editors, Spech comunication papers presented at 
the 97th Meting of the Acoustical Society of 
America, IT, Cambridge, MA. 
Barton, E. (1985) On the complexity of ID/LP rules, 
Computational Linguistics, Volume 1, Pages 205-
218. 
 
Figure 8. The learning curve for the thre modules 
 
Train No = 1800 
Test No = 300  P  R  F  Q 
CRF  0.815  0.812  0.813  0.509 
Parser  0.808  0.814  0.81  0.494 
Parser+SVM (n = 2)  0.823  0.827  0.825  0.531 
Parser+SVM (n = 10)  0.832  0.835  0.83  0.55 
Table 1. The results of evaluating the thre modules 
868
Bar, C., Jones, R., Regelson, M., (208) The Linguis-
tic Structure of English Web-Search Queries, In 
Procedings of EMNLP-08: conference on Empiri-
cal Methods in Natural Language Procesing. 
Broder, A., Fontoura, M., Gabrilovich, E., Joshi, A., 
Josifovski, V., and Zhang, T. (207) Robust clasi-
fication of rare queries using web knowledge. In 
Procedings of SIGIR’07 
Colins, M., Ko, T., (205) Discriminative Reranking 
for Natural Language Parsing, Computational Lin-
guistics, v.31 p.25-70. 
Gazdar, G., Klein, E., Sag, I., Pulum, G., (1985) Gen-
eralized Phrase Structure Gramar, Harvard Uni-
versity Pres. 
Grenager, T., Klein, D., and Maning, C. (205) Un-
supervised learning of field segmentation models 
for information extraction, In Procedings of ACL-
05. 
Kushmerick, N., Johnston, E., and McGuines, S. 
(201). Information extraction by text clasifica-
tion, In Procedings of the IJCAI-01 Workshopon 
Adaptive Text Extraction and Mining. 
Li, X., Wang, Y., and Acero, A. (208) Learning 
query intent from regularized click graphs. In Pro-
cedings of SIGIR’08 
Maning, C., Schütze, H. (199) Foundations of Sta-
tistical Natural Language Procesing, The MIT 
Pres, Cambridge, MA. 
McCalum, A., Freitag, D., Pereira, F. (200) Maxi-
mum entropy markov models for information ex-
traction and segmentation, Procedings of the 
Sevententh International Conference on Machine 
Learning, Pages: 591 598 
McCalum, A., Nigam, K., Renie, J., and Seymore, 
K. (199) A machine learning aproach to building 
domain-specific search engines, In IJCAI-1999. 
Pasca, M., Van Durme, B., and Garera, N. (207) The 
Role of Documents vs. Queries in Extracting Clas 
Atributes from Text, ACM Sixtenth Conference 
on Information and Knowledge Management 
(CIKM 207). Lisboa, Portugal. 
Viola, P., Narasimhan, M., Learning to extract infor-
mation from semi-structured text using a discrimi-
native context fre gramar SIGIR 205: 30-337. 
Xue, GR, HJ Zeng, Z Chen, Y Yu, WY Ma, WS Xi, 
WG Fan, (204), Optimizing web search using web 
click-through data, Procedings of the thirtenth 
ACM international conference. 
 
 
869

