<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Bogdan Babych</author>
<author>Anthony Hartley</author>
</authors>
<title>Comparative Evaluation of Automatic Named Entity Recognition from Machine Translation Output</title>
<date>2004</date>
<booktitle>In IJCNLP Workshop on Named Entity Recognition for Natural Language Processing Applications</booktitle>
<contexts>
<context>ould reward a system for its ability to avoid such over-generation. However, the type of phenomena which can characterise MT quality for a given language pair can only be established experimentally. (Babych and Hartley 2004b) proposed an evaluation method based on Named Entity (NE) recognition in MT output, using the ANNIE open-source NE recognition system available in GATE (Cunningham et al. 2002). The idea that certai</context>
</contexts>
<marker>Babych, Hartley, 2004</marker>
<rawString>Babych, Bogdan and Hartley, Anthony. (2004). Comparative Evaluation of Automatic Named Entity Recognition from Machine Translation Output. In IJCNLP Workshop on Named Entity Recognition for Natural Language Processing Applications.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Bogdan Babych</author>
<author>Debbie Elliott</author>
<author>Anthony Hartley</author>
</authors>
<marker>Babych, Elliott, Hartley, </marker>
<rawString>Babych, Bogdan and Elliott, Debbie and Hartley, Anthony.</rawString>
</citation>
<citation valid="true">
<title>Extending MT evaluation tools with translation complexity metrics</title>
<date>2004</date>
<booktitle>In CoLing 2004: 20 th International Conference on Computational Linguistics</booktitle>
<marker>2004</marker>
<rawString>(2004). Extending MT evaluation tools with translation complexity metrics. In CoLing 2004: 20 th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bogdan Babych</author>
<author>Anthony Hartley</author>
<author>Debbie Elliott</author>
</authors>
<title>Estimating the predictive power of n-gram MT evaluation metrics across language and text types</title>
<date>2005</date>
<booktitle>In MT Summit X</booktitle>
<contexts>
<context>conditions: e.g., the regression figures which predict human scores using automated scores (the slope and the intercept of the fitted line) depend on the combination of text type and target language (Babych et al., 2005). Projecting automated scores onto human scores is important if we are interested in the acceptability of MT output at or above some known threshold given by human scores. Therefore, it is essential </context>
</contexts>
<marker>Babych, Hartley, Elliott, 2005</marker>
<rawString>Babych, Bogdan and Hartley, Anthony and Elliott, Debbie. (2005) Estimating the predictive power of n-gram MT evaluation metrics across language and text types. In  MT Summit X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bogdan Babych</author>
<author>Anthony Hartley</author>
<author>Serge Sharoff</author>
</authors>
<title>Translating from under-resourced languages: comparing direct transfer against pivot translation</title>
<date>2007</date>
<booktitle>In MT</booktitle>
<location>Summit XI, Copenhagen, Denmark</location>
<contexts>
<context>ave a relatively low correlation with human judgments at the level of smaller segments (sentences and individual texts), so they are not very useful for automating error analysis in MT. For example, (Babych et al. 2007b) show that BLEU converges with human scores only after the evaluated corpus reaches some 7,000 words in size. Task-based metrics can work without a human reference translation. Their principle was i</context>
<context>itself using a distance metric (e.g., BLEU) on texts with varying difficulty. Here a difficulty slope parameter is computed which relates performance of a tested system against some reference system (Babych et al. 2007b). This method was suggested for comparing different pivot MT architectures against a direct translation route (used as a reference); it shows how systems cope with increasing difficulty of segments </context>
</contexts>
<marker>Babych, Hartley, Sharoff, 2007</marker>
<rawString>Babych, Bogdan and Hartley, Anthony and Sharoff, Serge. (2007b). Translating from under-resourced languages: comparing direct transfer against pivot translation. In MT Summit XI, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philippe Koehn</author>
</authors>
<title>Re-evaluation the Role of Bleu in Machine Translation Research</title>
<date>2006</date>
<booktitle>In 11 th Conference of the European Chapter of the Association for Computational Linguistics) (EACL06</booktitle>
<location>Philadelphia</location>
<contexts>
<context>often does not hold. For example, for test sets which include both RBMT and SMT systems the correlation of BLEU/NIST type scores is lower, since these metrics overestimate the Adequacy of SMT output (Callison-Burch et al., 2006). Other features are also dependent on experimental conditions: e.g., the regression figures which predict human scores using automated scores (the slope and the intercept of the fitted line) depend </context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Callison-Burch, Chris and Osborne, Miles and Koehn, Philippe. Re-evaluation the Role of Bleu in Machine Translation Research. (2006). In 11 th Conference of the European Chapter of the Association for Computational Linguistics) (EACL06) Cunningham, H. and Maynard, D and Bontcheva, K. and Tablan V. (2002). GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications. In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics (ACL'02). Philadelphia. 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W John Hutchins</author>
<author>Harold Somers</author>
</authors>
<title>An introduction to machine translation</title>
<date>1992</date>
<publisher>Academic Press</publisher>
<location>London</location>
<contexts>
<context>ut a human reference translation. Their principle was initially suggested for human evaluation: “…can someone using the translation carry out the instructions as well as someone using the original?” (Hutchins and Somers, 1992: 163), and has been successfully applied in automated metrics. One of the first examples of automated task-based metrics is the X-score suggested in (Rajman and Hartley, 2001). It is computed by runn</context>
</contexts>
<marker>Hutchins, Somers, 1992</marker>
<rawString>Hutchins,W.John and Somers, Harold. (1992). An introduction to machine translation. London: Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Abdessamad Echihabi</author>
<author>Kevin Knight</author>
</authors>
<title>Morphosyntactic Lexical Text cohesion/ coherence Marcu, Daniel</title>
<date>2006</date>
<marker>Wang, Echihabi, Knight, 2006</marker>
<rawString>Morphosyntactic Lexical Text cohesion/ coherence Marcu, Daniel and Wang, Wei and Echihabi, Abdessamad and Knight, Kevin. (2006). SPMT: Statistical Machine Translation with Syntactified Target Language Phrases.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</booktitle>
<marker></marker>
<rawString>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Papineni Kishore</author>
<author>Roukos Salim</author>
<author>Ward Todd</author>
<author>Zhu WeiJing</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation</title>
<date>2002</date>
<booktitle>In ACL 2002: the 40th Annual Meeting of the Association for Computational Linguistics</booktitle>
<marker>Kishore, Salim, Todd, WeiJing, 2002</marker>
<rawString>Papineni Kishore and Roukos Salim and Ward Todd and Zhu WeiJing. (2002). Bleu: a method for automatic evaluation of machine translation. In ACL 2002: the 40th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Popescu-Belis</author>
</authors>
<title>The place of automatic evaluation metrics in external quality models for machine translation</title>
<date>2007</date>
<booktitle>In MT Summit XI Workshop: Automatic procedures in MT evaluation</booktitle>
<location>Copenhagen</location>
<contexts>
<context>tance-based evaluation, while a method based on Named-Entity recognition in degraded MT output exemplifies task-based evaluation. 2 Distance-based and task-based MT evaluation models As suggested in (Popescu-Belis, 2007), the majority of MT evaluation systems can be grouped around two central principles used to score MT output: distance-based metrics compute some sort of distance between MT output and a gold-standar</context>
</contexts>
<marker>Popescu-Belis, 2007</marker>
<rawString>Popescu-Belis, Andrei. (2007). The place of automatic evaluation metrics in external quality models for machine translation. In MT Summit XI Workshop: Automatic procedures in MT evaluation, Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hartley Anthony</author>
</authors>
<title>ual Long-distance dependencies Textual function Performance-based metrics Lower sensitivity on higher quality output Referenceproximity metrics Rajman Martin</title>
<date>2001</date>
<marker>Anthony, 2001</marker>
<rawString>ual Long-distance dependencies Textual function Performance-based metrics Lower sensitivity on higher quality output Referenceproximity metrics Rajman Martin and Hartley Anthony. (2001).</rawString>
</citation>
<citation valid="true">
<title>Automatically predicting MT systems ranking compatible with fluency adequacy and Informativeness scores. In</title>
<date>2007</date>
<booktitle>4th ISLE Workshop on MT Evaluation, MT Summit VIII Thurmair</booktitle>
<location>Gregor</location>
<marker>2007</marker>
<rawString>Automatically predicting MT systems ranking compatible with fluency adequacy and Informativeness scores. In: 4th ISLE Workshop on MT Evaluation, MT Summit VIII Thurmair, Gregor. (2007). Automatic evaluation in MT system production. In MT Summit XI Workshop: Automatic procedures in MT evaluation, Copenhagen.</rawString>
</citation>
</citationList>
</algorithm>

