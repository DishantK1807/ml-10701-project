For each such binomial, we approximate P(ai = ai\[S) as a trun3In the implementation we smooth the MLE by interpolation with a uniform probability distribution, following Merialdo (1994).
J94-2001
The methods we investi1This gives the Viterbi model (Merialdo, 1994), which we use here.
J94-2001
• In text categorization (Lewis and GMe, 1994; Iwayama and Tokunaga, 1994): P(tlC), where t is a term in the document to be categorized, and C is a candidate category label.
A94-1027
2This version of the method uses Bayes' theorem ~ (Church, 1988).
A88-1019
• In prepositional-phrase (PP) attachment (Hindle and Rooth, 1993): P(alf), where a is a possible attachment, such as an attachment to a head verb or noun, and f is a feature, or a combination of features, of the attachment.
J93-1005
• In statistical parsing (Black et al., 1993): P(rlh), the probability of applying the rule r at a certain stage of the top down derivation of the parse tree given the history h of the derivation process.
P93-1005
Parame6As noted above, we smooth the MLE estimates by interpolation with a uniform probability distribution (Merialdo, 1994).
J94-2001
In statistical NLP, probabilistic classifiers are often used to select a preferred analysis of the linguistic structure of a text (for example, its syntactic structure (Black et al., 1993), word categories (Church, 1988), or word senses (Gale, Church, and Yarowsky, 1993)).
A88-1019 P93-1005
The ultimate reduction in annotation cost is achieved by unsupervised training methods, which do not require an annotated corpus at all (Kupiec, 1992; Merialdo, 1994; Elworthy, 1994).
A94-1009 J94-2001
