ple, in headline generation (Dorr et al., 2003; Jin,
W03-0501
McDonald (2006) is that the latter employs â€œsoft syntactic ev-
E06-1038
Osborne, 2002) and multiple documents (Radev and
W02-0401
McDonaldet al., 2005a; McDonaldet al., 2005b).
H05-1066
(Lin and Hovy, 2002), a popular n-gram recall-
W02-0406
M. Osborne. 2002. Using maximum entropy for sen-
W02-0401
McDonald (2006) 71.40% 0.7444 0.7697 0.7568 0.7711 0.7852 0.7696
E06-1038
by Dorr et al. (2003) include removal of low content units, and
W03-0501
McDonald (2006) uses the outputs of two parsers
E06-1038
ning, 2002; McDonaldand Pereira, 2006). How-
E06-1038
resentationsovertheinput(McDonaldet al.,2005a).
H05-1066
C.-Y. Lin and E. Hovy. 2002. Manual and automatic
W02-0406
tem of McDonald (2006); however, none of our vari-
E06-1038
polynomialtime(McDonaldet al., 2005b).Thepri-
H05-1066
cording to linguistic criteria (Dorr et al., 2003; Zajic
W03-0501
mer, the headline generation system of Dorr et al.(2003) and Zajic et al. (2006),8 and the discrimina-
W03-0501
D. R. Radev, H. Jing, and M. Budzikowska. 2000.
W00-0403
pairs of consecutive words. As in McDonald (2006),
E06-1038
R. McDonald. 2006. Discriminative sentence compres-
E06-1038
C.-Y. Lin. 2003. Improving summarization performance
W03-1101
B. Dorr, D. Zajic, and R. Schwartz. 2003. Hedge trim-
W03-0501
Parser (McDonald et al., 2005).10
H05-1066
tive model described by McDonald (2006), which
E06-1038
T. Cohn and M. Lapata. 2008. Sentence compression
C08-1018
D. R. Radev and K. McKeown. 1998. Generating natural
J98-3005
study carried out by Lin (2003) suggests that sum-
W03-1101
Radev et al., 2000). Most of this work aims only
W00-0403
baseline systems (HedgeTrimmer and the system described in McDonald, 2006), and the two variants of our model,
E06-1038
of McDonaldet al. (2005b). In particular, we ap-
H05-1066
