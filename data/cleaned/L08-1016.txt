<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A framework for learning predictive structures from multiple tasks and unlabeled data</title>
<date>2005</date>
<journal>Journal of Machine Learning Research</journal>
<pages>6--1817</pages>
<contexts>
<context>weighting using raw data, can be seen as a novel version thereof. Theoretically speaking, successfull domain adaptation hinges on some sense of “overlap” between the source and target domains, e.g., (Ando and Zhang, 2005). The overlap between source and target domains can be seen as a (mix of) subdomain(s) of both. Naturally, instance weighting and its subdomain instantiation can be seen as a weighted version of self</context>
</contexts>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6:1817–1853.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
</authors>
<title>Design of a Multi-lingual, Parallel-processing Statistical Parsing Engine</title>
<date>2002</date>
<booktitle>Proceedings of HLT</booktitle>
<contexts>
<context>een the individual subdomains and weaken the biases in the model. The present paper describes a method for incorporating subdomain sensitivity into an existing state-of-the-art parser (Collins, 1997; Bikel, 2002) in order to improve its predictions. The main idea is to create and combine an ensemble of subdomain sensitive parsers (each for a different subdomain) without the need for further manual annotation</context>
<context>0,1] is an interpolation parameter set experimentally to balance the contribution of the DVM and the constituentweighting subterms. 3. Experiments All experiments were performed using Bikel’s parser (Bikel, 2002), an emulation of Collins’ Headlexicalized Probabilistic Context Free Grammar (PCFG) model (Collins, 1997; Collins, 2003). We use the Penn Treebank (PT) Wall Street Journal (WSJ) (Marcus et al., 1993</context>
</contexts>
<marker>Bikel, 2002</marker>
<rawString>Bikel, D. (2002). Design of a Multi-lingual, Parallel-processing Statistical Parsing Engine. Proceedings of HLT 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom M Mitchell</author>
</authors>
<title>Combining labeled and unlabeled sata with co-training</title>
<date>1998</date>
<booktitle>In COLT</booktitle>
<pages>92--100</pages>
<contexts>
<context>ain(s) of both. Naturally, instance weighting and its subdomain instantiation can be seen as a weighted version of self-training, e.g., (McClosky et al., 2006), which is again related to co-training (Blum and Mitchell, 1998). 5. Conclusions and Outlook This paper explores a particular instantiation for subdomain sensitive parsing. We exploit unlabeled subdomain corpora gathered from the web in order to create a set of s</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom M. Mitchell. 1998. Combining labeled and unlabeled sata with co-training. In COLT, pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>Context-sensitive spoken dialogue processing with the DOP model. In Natural Language Engineering</title>
<date>1999</date>
<publisher>Cambridge University Press</publisher>
<contexts>
<context>of experiments against the baseline state-of-the-art parser. 2. Subdomain Sensitive Parsing To exploit domain-information and create an ensemble of subdomain sensitive parsers, one approach, used in (Bod, 1999), is to partition the given training treebank into disjoint subtreebanks, each addressing one subdomain. We think that it is often not that straightforward (sometimes even impossible) to partition th</context>
<context> NW-DVM when instantiated with various values for λ. We can see that for λ ≥ 0.5 the technique slightly but constantly outperforms the baseline system, reaching a peak at λ = 0.6. 4. Related Work In (Bod, 1999), dialogue context/state is taken into account by splitting up the training treebank into four disjoint 88 88.5 89 89.5 90 90.5 0 0.2 0.4 0.6 0.8 1 F-score Lambda NW-DVM WSJ-40 NW-DVM WSJ-100 Baselin</context>
</contexts>
<marker>Bod, 1999</marker>
<rawString>Bod, R. (1999). Context-sensitive spoken dialogue processing with the DOP model. In Natural Language Engineering. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking</title>
<date>2005</date>
<booktitle>Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context>ngle, final result. There are various ways to combine parsers, including methods for combining the outputs of classifiers by voting schemes or even parse-reranking schemes based on log-linear models (Charniak and Johnson, 2005). For our preliminary exploration in parser combination, we choose here two simple methods: • parser pre-selection (choose a parser for a given input sentence) and • post-selection (choose any of the</context>
<context>rpora other than the WSJ, e.g., Brown corpus (see (Gildea, 2001)), thus looking at domain adaptation. Furthermore, we would like examine our approach with the current best-performing parsing systems (Charniak and Johnson, 2005). It consists of a generative n-best parsing system that employs a discriminative reranking scheme. We would like to gauge to what extent n-best parsing might benefit from subdomain information. 6. R</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Charniak, E. and Johnson, M. (2005). Coarse-to-Fine n-Best Parsing and MaxEnt Discriminative Reranking. Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three Generative, Lexicalised Models for Statistical Parsing</title>
<date>1997</date>
<booktitle>Proceedings of the 35th Annual Meeting of the ACL-EACL</booktitle>
<location>Madrid</location>
<contexts>
<context>ifferences between the individual subdomains and weaken the biases in the model. The present paper describes a method for incorporating subdomain sensitivity into an existing state-of-the-art parser (Collins, 1997; Bikel, 2002) in order to improve its predictions. The main idea is to create and combine an ensemble of subdomain sensitive parsers (each for a different subdomain) without the need for further manu</context>
<context>stituentweighting subterms. 3. Experiments All experiments were performed using Bikel’s parser (Bikel, 2002), an emulation of Collins’ Headlexicalized Probabilistic Context Free Grammar (PCFG) model (Collins, 1997; Collins, 2003). We use the Penn Treebank (PT) Wall Street Journal (WSJ) (Marcus et al., 1993) version 2, with the now ’standard division’ (Collins, 1997; Collins, 2003) into training (sections 02-21</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Collins, M. (1997). Three Generative, Lexicalised Models for Statistical Parsing. Madrid. Proceedings of the 35th Annual Meeting of the ACL-EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing. Association of Computational Linguistics</title>
<date>2003</date>
<contexts>
<context>ng subterms. 3. Experiments All experiments were performed using Bikel’s parser (Bikel, 2002), an emulation of Collins’ Headlexicalized Probabilistic Context Free Grammar (PCFG) model (Collins, 1997; Collins, 2003). We use the Penn Treebank (PT) Wall Street Journal (WSJ) (Marcus et al., 1993) version 2, with the now ’standard division’ (Collins, 1997; Collins, 2003) into training (sections 02-21), test (sectio</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Collins, M. (2003). Head-Driven Statistical Models for Natural Language Parsing. Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
</authors>
<title>Corpus variation and parser performance</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context>, extending the current approach to a Bayesian approach, exploring more sophisticated parser combination techniques, and examining the approach on corpora other than the WSJ, e.g., Brown corpus (see (Gildea, 2001)), thus looking at domain adaptation. Furthermore, we would like examine our approach with the current best-performing parsing systems (Charniak and Johnson, 2005). It consists of a generative n-best</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Gildea, D. (2001). Corpus variation and parser performance. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in nlp</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>264--271</pages>
<location>Prague, Czech Republic</location>
<contexts>
<context>a “domain-dependent parser”. Although different, work on domain adaptation is distantly related to our work. Recent research on adaptation is too numerous to discuss in a short paper. In particular, (Jiang and Zhai, 2007) suggest “instance weighting” as a method for adaptation. Our approach, subdomain instance weighting using raw data, can be seen as a novel version thereof. Theoretically speaking, successfull domain</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in nlp. In Proceedings of ACL 2007, pages 264–271, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>J Peters</author>
</authors>
<title>Semantic clustering for adaptive language modeling</title>
<date>1997</date>
<volume>02</volume>
<pages>779</pages>
<publisher>IEEE Computer Society</publisher>
<location>Los Alamitos, CA, USA</location>
<contexts>
<context>reebanks) and their parameters are estimated to reflect properties of the training data. Usually, a treebank/corpus consists of language use concerning a range of topics. For example, as observed by (Kneser and Peters, 1997), subdomains like ”politics, stock market, financial news etc. can be found“ in the Wall Street Journal (WSJ) Penn Treebank (PT) (Marcus et al., 1993). Hence, for a statistical parser trained on such</context>
</contexts>
<marker>Kneser, Peters, 1997</marker>
<rawString>Kneser, R. and Peters, J. (1997). Semantic clustering for adaptive language modeling. volume 02, page 779, Los Alamitos, CA, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A Multilingual Corpus for Evaluation of Machine Translation</title>
<date>2005</date>
<publisher>MT Summit</publisher>
<contexts>
<context>00) sets. As concepts constituting possible subdomains within the PT WSJ we assume: FINANCIAL, POLITICS and SPORTS. For the POLITICS subdomain we use the English part of the Europarl Parallel Corpus (Koehn, 2005). For the Figure 1: Summary of the Experimental Design for Sub-domain Aware Parsing FINANCIAL and SPORTS subdomains, to the best of our knowledge there were no ready-to-use corpora available. Hence, </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Koehn, P. (2005). Europarl: A Multilingual Corpus for Evaluation of Machine Translation. MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: the Penn Treebank</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>13</volume>
<contexts>
<context>range of topics. For example, as observed by (Kneser and Peters, 1997), subdomains like ”politics, stock market, financial news etc. can be found“ in the Wall Street Journal (WSJ) Penn Treebank (PT) (Marcus et al., 1993). Hence, for a statistical parser trained on such a treebank the statistics gathered are averages over different subdomains. By definition, averages smooth-out the statistical differences between the</context>
<context>parser (Bikel, 2002), an emulation of Collins’ Headlexicalized Probabilistic Context Free Grammar (PCFG) model (Collins, 1997; Collins, 2003). We use the Penn Treebank (PT) Wall Street Journal (WSJ) (Marcus et al., 1993) version 2, with the now ’standard division’ (Collins, 1997; Collins, 2003) into training (sections 02-21), test (section 23) and development/dev (section 00) sets. As concepts constituting possible </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, M., Santorini, B., and Marcinkiewicz, M. (1993). Building a Large Annotated Corpus of English: the Penn Treebank. Computational Linguistics, 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING-ACL</booktitle>
<contexts>
<context>ween source and target domains can be seen as a (mix of) subdomain(s) of both. Naturally, instance weighting and its subdomain instantiation can be seen as a weighted version of self-training, e.g., (McClosky et al., 2006), which is again related to co-training (Blum and Mitchell, 1998). 5. Conclusions and Outlook This paper explores a particular instantiation for subdomain sensitive parsing. We exploit unlabeled subd</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2006. Reranking and self-training for parser adaptation. In Proceedings of the COLING-ACL 2006. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wikimedia Foundation Inc</author>
</authors>
<title>Wikipedia, The Free Encyclopedia</title>
<date>2007</date>
<note>http://en.wikipedia.org</note>
<marker>Inc, 2007</marker>
<rawString>Wikimedia Foundation Inc. (2007). Wikipedia, The Free Encyclopedia. http://en.wikipedia.org/.</rawString>
</citation>
</citationList>
</algorithm>

