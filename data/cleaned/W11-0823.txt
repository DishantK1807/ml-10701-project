Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 137–144,
Portland, Oregon, USA, 23 June 2011. c©2011 Association for Computational Linguistics
How Many Multiword Expresions do People Know? 
 
Keneth Church 
HLT COE 
Johns Hopkins University 
Kenneth.Church@jhu.edu 
 
Abstract 
What is a multiword expresion (MWE) 
and how many are there? What is a WE? 
What is many?   Mark Liberman gave a 
great invited talk at ACL-89 titled “how 
many words do people know?” where he 
spent the entire hour questioning the 
question.  Many of these same questions 
apply to multiword expressions.  What is a 
word?  What is many?  What is a person?  
What does it mean to know?  Rather than 
answer these questions, this paper wil use 
these questions as Liberman did, as an 
excuse for surveying how such issues are 
addressed in a variety of fields: computer 
science, web search, linguistics, lexicogra-
phy, educational testing, psychology, statis-
tics, etc. 
1 How
many words do people know? 
One can find all sorts of answers on the web: 
• Very low: Apparently I only knew 7,00 
words when I was seven and 14,000 when 
I was fourten. I learned from exposure. 
Now things are not that easy in a second 
language, but it just shows that the brain 
can absorb information from sheer input.
1
 
• Low: 12,00 – 20,000 words
2
 
• Higher: 98,968
3
 
• Even higher: 13,58,391
4
 
                                                             
1
 
http:/thelinguist.blogs.com/how_to_learn_english_and/2009/0
2/how-many-words-do-you-know-how-many-have-you-
looked-up-in-a-dictionary.html 
2
 
http:/answers.yahoo.com/question/index?qid=200611052050
54AA5YL0B 
3
 http:/ww.independent.co.uk/news/world/americas/english-
language-nears-the-one-milionword-milestone-473935.html 
2 Motivation

As mentioned in the abstract, Liberman used his 
ACL-89 invited talk to survey how various fields 
approach these isues.  He started his ACL-89 
invited talk by questioning every word in the title 
of his talk: How many words do people know? 
 
1. What is a word?   Is a word defined in 
terms of meaning?  Sound?  Syntax? 
Speling?  White space?  Distribution? 
Etymology?  Learnability? 
2. What is a person? Child? Adult? Native 
speaker?  Language Learner? 
3. What does it mean to know something?  
Active knowledge is diferent from pasive 
knowledge.    What is (Artificial) Inteli-
gence?  Is vocabulary size a measure of in-
telligence?  (Terman, 1918) 
4. What do we mean by many?  Is there a 
limit like 20,000 or 1M or 13.6M or does 
vocabulary size (V) keep growing with 
experience (larger corpora à larger V)? 
The original motivation for Liberman’s talk came 
from a very practical busines concern.  At the 
time, Liberman was runing a spech synthesis 
effort at AT&T Bell Labs. As the manager of this 
effort, Liberman would receive questions from the 
busines asking how large the synthesizer’s dict-
ionary would have to be for such and such com-
mercial aplication. 
 
Vocabulary size was also a hot topic in many other 
engineering applications.  How big does the 
dictionary have to be for X?  X can be  anything 
from parsing, part of spech taging, speling 
corection, machine translation, word breaking for 
Chinese and Japanese (and English), spech recog-
                                                                                                
4
 Franz and Brants (206) 
137
nition, speech synthesis, web search or some other 
application.   
3 Dictionary
Estimates 
These questions reminded Liberman of similar 
questions that his colleagues in lexicography were 
receiving from their marketing departments.  Many 
dictionaries and other reference books lead with a 
marketing pitch such as: “Most comprehensive: 
more than 30,00 words and phrases [MWEs]…” 
(Kipfer, 201). 
 
The very smalest dictionaries are caled “gems.” 
They typicaly contain 20,00 words. Unabridged 
colegiate dictionaries have about 50,00 words.
5
 
The Oxford English Dictionary (OED) has 60,00 
entries.
6
 
 
All of these dictionaries limit themselves to what is 
known as general vocabulary, words that would be 
expected to be understod by a general audience. 
General vocabulary is typicaly contrasted with 
technical terminology, words that would only be 
understood by domain experts in a particular topic. 
There are reference boks that specialize on place 
names (Gazeters), surnames, technical termi-
nology, quotations, etc., but standard dictionaries 
of general vocabulary tend to avoid proper nouns, 
complex nominals (e.g., “staff meeting”), abbrevi-
ations, acronyms, technical terminology, digit 
sequences, stret adresses, trademarks, product 
numbers, etc.
7
   Even the largest dictionaries may 
not have al that much coverage because in prac-
tice, one often runs into texts that go wel beyond 
general vocabulary. 
4 Broder’s Taxonomy of Web Queries 
Obviously, the web goes wel beyond general 
vocabulary.  Web queries tend to be short phrases 
(MWEs), often a word or two such as a product 
number.  Broder (202) introduced a taxonomy of 
                                                             
5
 http:/ww.collinslanguage.com/shop/english-dictionary-
landing.aspx 
6
 http:/ww.oed.com/public/about 
7
 See Sproat (194) and references therein for more on 
complex nominals. See Coker et al (190) for coverage 
statistics on surnames. See Liberman and Church (191) for 
more on abreviations, acronyms, digit sequences and more. 
See Dagan and Church (194) for more on technical 
terminology. 
queries that has become widely acepted.   His 
percentage estimates were estimated from 
AltaVista query logs and could use updating. 
 
• Naviational (20%) 
• Informational (48%) 
• Transactional (30%) 
Navigational queries are extremely comon these 
days, perhaps even more common than 20%.  The 
user intent is to navigate to a particular url: 
 
• google à www.gogle.com 
• Greyhound Bus à www.greyhound.com 
• American Airlines à www.a.com 
Broder’s examples of informational queries are: 
cars, San Francisco, normocytic anemia, Scovile 
heat units.  The user intent is to research a 
particular information need.  The user expects to 
read one or more static web pages in order to 
address the information need.  Broder italicized 
“static” to distinguish informational queries from 
transactional queries.  Transactional queries are 
intended to reach a site where further (non-static) 
action wil take place: shoping, directions, web-
mediated services, medical advice, gaming, down-
loading music, pictures, videos, etc. 
5 User
Intent & One Sense Per Query 
I prefer a two-way distinction betwen 
 
1. Navigational queries: user knows where 
she wants to go, and 
2. Non-navigational queries: user is open to 
sugestions. 
Google, for example, ofers the folowing “related 
search” sugestions for “camera:” digital camera, 
video camera, history of the camera, sony camera, 
ritz camera, Nikon camera, camera brands, cam-
era reviews, camera store, beach camera, canon, 
photography, bestbuy, camara, cannon, cir-
cuit city. camero. Olympus, camcorder, b&h. 
These kinds of sugestions can be very sucesful 
when the user is open to sugestions, but not for 
navigational queries.   There are a number of other 
mechanisms for making sugestions such as ads 
and did-you-mean speling suggestions. 
138
Pitler and Church (209) used click logs to clasify 
queries by intent (CQI).   Consider five types of 
clicks. Some types of clicks are evidence that the 
user knows where she wants to go, and some are 
evidence that the user is open to sugestions. 
 
1. Algo: clicks on the so-called 10 blue links 
2. Paid: clicks on comercial ads 
3. Wikipedia: clicks on Wikipedia entries 
4. Speling Corections: did you mean …? 
5. Other sugestions from search engine 
Many queries are strongly asociated with one type 
of click (more than others). 
 
• Commercial queries à clicks on ads 
• Non-comercial queries à Wikipedia. 
There is a one-sense-per-X constraint (Gale et al, 
1992; Yarowsky, 1993).  It is unlikely that the 
same query wil be ambiguous with both 
comercial and non-comercial senses.  Inded, 
the click logs show that both ads and Wikipedia 
are effective, but they have complementary 
distributions.  There are few queries with both 
clicks on ads and clicks on Wikipedia entries.   For 
a comercial query like, “JC Peney,” it is ok for 
Google to return an ad and a store locator map, but 
Google shouldn’t return a Wikipedia discusion of 
the history of the company. 
 
Although the click logs are very large, they are 
never large enough.  How do we resolve the user 
intention when the click logs are to sparse to 
resolve the ambiguity directly?  Pitler sugested 
using word sense disambiguation methods.   For 
example, her method labels the ambiguous query 
“designer trench” as comercial because it is 
closer (in random walk distance) to a couple of 
stores than to a Wikipedia discussion of trench 
warfare during orld War I. 
 
More generaly, random walk methods (like word 
sense disambiguation) can be used to resolve all 
sorts of hiden variables such as gender, age, 
location, political orientation, user intent, etc.  Did 
the user mean X?   Does the user know what she 
wants, or is she open to sugestions? 
5.1 User
Intent & Speling Corection 
Speling corection is an extreme case where it is 
often relatively easy for the system to determine 
user intent.  On the web, speling corection has 
become synonymous with did-you-mean.  The 
synonymy makes it clear that the point of speling 
corection is to get at what users mean as oposed 
to what they say. 
 
Then you should say what you 
mean,' the March Hare went on. 
`I do,' Alice hastily replied; `at 
least--at least I mean what I say--
that's the same thing, you know.' 
`Not the same thing a bit!' said the 
Hater.  (Lewis Carrol, 1865) 
 
See Kukich (192) for a comprehensive survey on 
speling corection.  Boswel (204) is a nice 
research exam; it is short and crisp and recent. 
 
I’ve worked on Microsoft’s speling correction 
products in two diferent divisions: Ofice and 
Web Search.  One might think that corecting 
documents in Microsoft Word would be similar to 
corecting web queries, but in fact, the two 
applications have remarkably litle in common. A 
dictionary of general vocabulary is essential for 
corecting documents and nearly useles for 
corecting web queries.  General vocabulary is 
more important in documents than web queries. 
 
The surveys mentioned above are more apropriate 
for corecting documents than web queries. 
Cucerzan and Bril (2004) propose an iterative 
proces that is more appropriate for web queries. 
In Table 1, they show a number of (mis)spelings 
of Albert Einstein’s name from a query log, sorted 
by frequency: albert einstein (4834), albert 
einstien (525), albert einstine (149), albert einsten 
(27), albert einsteins (25), etc. Their method takes 
a web query that may or may not be mispelled and 
considers nearby corections with higher 
frequencies.   The method continues to iterate in 
this way until it converges at a fixed point.  The 
iteration makes it posible to corect multiple 
errors.  For example, anol scwarteger à arnold 
schwartzneger à arnold schwarznegger à arn-
old schwarzeneger.  They find that context is 
often very helpful. In general, it is easier to correct 
139
the combination of the first name and the last name 
together than separately.   So too, it is probably 
easier to corect MWEs as a combination than to 
corect each of the parts separately. 
5.2 User
Intent & Spoken Queries 
Queries often depend on context in complex and 
unexpected ways. It has been said that there is no 
there there on the web, but queries from cell 
phones are often looking for stuf that has a “there” 
(a location), and moreover the location is often 
near the user (e.g., restaurants, directions). 
 
Users now have the option to enter queries by 
voice in adition to the keyboard.  Kamvar and 
Beeferman (2010) found voice was relatively 
popular on mobile devices with “compresed” 
(hard-to-use) keyboards.  They also found some 
topics were relatively more likely to be spoken: 
 
• Food & Drink: Starbucks, tuna fish, Mex-
ican food 
• Busines Listings: Starbucks Holmdel NJ, 
Lake George 
• Properties relating to places: weather 
Holmdel NJ, best gas prices 
• Shoping & Travel: Rapids Water Park 
coupons, black Converse shoes, Costco, 
Walmart 
Other topics such as adult queries are relatively 
les likely to be spoken, presumably because users 
don’t want to be overheard.  Privacy is more of a 
concern for some topics and less for others. 
6 What
is “large”? 
The term “large vocabulary” has ben a moving 
target.  Vocabulary sizes have been increasing with 
advances in technology.  Around the time of 
Liberman’s ACL-89 talk, the speech recognition 
comunity was working really hard on a 20,000-
word task.   Since it was so hard at the time to 
scale up recognizers to such large vocabularies, 
some researchers were desperately hoping that 
20,000 words would be sufficient to achieve broad 
coverage of unrestricted language. 
 
At that time, I gave a workshop talk that used a 
much larger vocabulary of 400,000 words (Church 
and Gale, 1989).  A leading researcher pulled me 
aside and begged me to tell him that I had made a 
mistake and there was no need to go beyond 
20,000 words. 
 
Similar questions came up when Gogle released 
their ngram counts over a trilion word corpus 
(Franz and Brants, 206).  There was considerable 
pushback from the community over the size of the 
vocabulary (13,588,391).  Norvig (personal com-
munication) caled me up to ask if their estimate of 
13.6 milion seemed unreasonable. 
 
While I had no reason to question Gogle’s 
estimate, I was reluctant to make a strong 
statement, given Efron and Thisted (1976).   Efron 
and Thisted studied a similar question: How many 
words did Shakespeare know (but didn’t use)? 
They conclude that one can extrapolate corpus size 
a litle bit (e.g., a factor of two) but not to much 
(e.g., an order of magnitude).  Since Gogle is 
working with corpora that are many orders of 
magnitude larger than what I had the oportunity 
to work with, it would require way too much 
extrapolation to answer Norvig’s question based on 
my relatively limited experience. 
7 Vocabulary
Grows with Experience 
Many people share the (mistaken) intuition that 
there is an uper bound on the size of the 
vocabulary.  Marketing pitches such “330,000 
words” (above) suggest that there is a reasonable 
upper bound that a person could hope to master 
(something considerably more manageable than 
Google’s estimate of 13.6 milion). 
 
In fact, the story is probably worse than that.  At 
ACL-1989, Liberman showed plots like those 
below.
8
 These plots make it clear that vocabulary 
(V) is going up and up and up with corpus size (N). 
There apears to be no end in sight. It is unlikely 
that there is an uper bound.  20k isn’t enough.  
Nor is 40k, or even 13.6 milion… 
 
The diferent curves call out diferences in what 
counts as a word. Do we consider morphologically 
related forms to be one word or two? How about 
                                                             
8
 Plots borowed with permision from Language Log: 
http:/itre.cis.upenn.edu/~myl/languagelog/archives/005514.ht
ml 
140
upper and lower case?  MWEs?  The diferent 
curves corespond to diferent choices. 
 
No mater how we define a word, we find that 
vocabulary grows (rapidly) with corpus size for as 
far as we can se. This observation apears to hold 
acros a broad set of conditions (languages, 
definitions of word/ngram, etc.)   Vocabulary be-
comes larger and larger with experience.  Similar 
coments apply to ngrams and MWEs. 
 
 
 
 
 
There is wide agrement that there’s no data like 
more data (Mercer, 1985).
9
   Google quoted Mer-
cer in their anouncement of ngram counts (Franz 
and Brants, 206). 
 
Banko and Bril (201) observed that performance 
goes up and up and up with experience (data). In 
the plot below, they note that the differences be-
twen lines (learners) are small compared to the 
gains to be had by simply collecting more data. 
Based on this observation, Bril has sugested 
(probably in jest) that we should fire everyone and 
spend the money on colecting data. 
 
 
Another interpretation is that experience improves 
performance on a host of tasks. This patern might 
help acount for the large correlation (0.91) in 
Terman (1918).  Terman sugests that vocabulary 
size should not be viewed as a measure of 
intelligence but rather a measure of experience.  
He uses the term “mental age” for experience, and 
measures “mental age” by performance on a 
standardized test. After adjusting for the large cor-
relation betwen vocabulary size and experience, 
there is little evidence of a conection betwen 
vocabulary size and inteligence (or much of any-
thing else).  Terman also considers a number of 
other factors such as gender and the language 
spoken at home (and testing erors), but ultimately 
concludes that experience dominates all of these 
alternative factors. 
                                                             
9
 Jelinek (204) atributes this position to Mercer (1985) 
http:/ww.lrec-conf.org/lrec204/doc/jelinek.pdf. 
 
141
8 What
is a Word?  MWE? 
We tend to think that white space makes it prety 
easy to tokenize English text into words. Ob-
viously, white space makes the task much easier 
than it would be otherwise.  There is a considerable 
literature on word breaking in Chinese and Japan-
ese which is considerably more challenging than 
English largely because there is no white space in 
Chinese and Japanese.   There are a number of 
popular dictionary-based solutions such as Cha-
Sen
10
 and Juman.
11
  Sproat et al (196) proposed 
an alternative solution based on distributional 
statistics such as mutual information. 
 
The situation may not be al that diferent in 
English. English is ful of multiword expresions. 
An obvious example involves words that lok like 
prepositions: up, in, on, with.  A great example is 
often atributed to Winston Churchil: This is the 
sort of arant nonsense up with which I will not 
put.
12
  One could argue that “put up with” is a 
phrasal verb and therefore it should be treated 
more like a fixed expresion (or a word) than a 
stranded preposition. 
8.1 Preventing
Blopers 
Almost any high frequency verb (go, make, do, 
have, give, call) can form a phrase with almost any 
high frequency function word (it, up, in, on, with, 
out, down, around, over), often with non-
compositional (tabo) semantics. 
 
This fact led to a rather entertaining failure mode 
with a word sense program that was trained on a 
combination of Roget’s Thesaurus and Grolier’s 
Encyclopedia (Yarowsky, 192).  Yarowsky’s 
program had a tendency to label high frequency 
words incorectly with tabo senses due to a 
mismatch betwen Groliers and Roget’s. Groliers 
was written for the parents of middle-American 
schol children and therefore avoided tabo 
language, whereas Roget’s was edited by 
Chapman, an authority on American Slang (tabo 
language).  The mismatch was particularly nasty 
for high frequency words, which are very comon 
                                                             
10
 http:/chasen.naist.jp/hiki/ChaSen/ 
11
 http:/nlp.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html 
12
 For a discusion of the source of this quotation, se 
http:/itre.cis.upenn.edu/~myl/languagelog/archives/002
670.html . 
in Groliers, but unlikely to be mentioned in 
Roget’s, except when the semantics are non-
compositional (tabo).   Consequently, there was 
an embarrassingly high probability that Yarow-
sky’s program would find embarasing inter-
pretations of benign texts. 
 
While some of these mistakes are somewhat 
understandable and even somewhat amusing in a 
research prototype, such mistakes are no laughing 
mater in a commercial product.  The testers at 
Microsoft worked realy hard to make sure that 
their products don’t make inapropriate sugest-
ions.  Despite their best eforts, there have ben a 
few highly publicized mistakes
13
 and there wil 
probably be more unles we find beter ways to 
prevent bloopers. 
8.2 Complex
Nominals and What is a Word? 
Complex nominals are probably more comon 
than phrasal verbs.  Is “White House” one word or 
two?  Is a word defined in terms of speling? 
White space? 
 
These days, among computational linguists, there 
would be considerable sympathy for using distri-
butional statistics such as word frequency and 
mutual information to find MWEs.   Folowing 
Firth (1957), we know a word by the company that 
it keps.  In Church and Hanks (190), we sugest-
ed using pointwise mutual information as a heur-
istic to look for pairs of words that have non-com-
positional distributional statistics.  That is, if the 
joint probability, P(x,y), of seing two words 
together in a context (e.g., window of 5 words) is 
much higher than chance, P(x)P(y), then there is 
probably a hidden variable such as meaning that is 
causing the deviation from chance. In this way, we 
are able to discover lots of word associations (e.g., 
doctor…nurse), colocates, fixed expresions, etc. 
 
If the list of MWEs becomes too large and too 
unmanageable, one could turn to a method like 
Stolcke pruning to cut back the list as necesary. 
Stolcke pruning is designed to prune ngram models 
so they fit in a manageable amount of memory. 
Supose we have an ngram model with too many 
                                                             
13
 http:/ww.zdnet.co.uk/news/it-
strategy/199/07/01/microsoft-sued-for-racist-application-
2072468/ 
142
ngrams and we have to drop some of them. Which 
ones should we drop? Stolcke pruning computes a 
los in terms of relative entropy for droping each 
ngram in the model.  The method drops the ngram 
that minimizes los. 
 
When an ngram is droped from the model, that 
sequence is modeled with a backed of estimate 
from other ngrams.  Stolcke pruning can be 
thought of as introducing compositionality 
assumptions.   Supose, for example, that “nice 
house” has more compositional statistics than 
“white house.”  That is, Pr(nice house) ≈ Pr(nice) 
Pr(house) whereas Pr(white house) > Pr(white) 
Pr(house).  In this case, Stolcke pruning would 
drop “nice house” before it drops “white house.” 
8.3 Linguistic
Diagnostics 
Linguists would fel more comfortable with 
defining word in terms of sound (phonology) and 
meaning (semantics).  It is prety clear that “White 
House” has non-compositional sound and meaning. 
The “White House” does not refer to a house that 
happens to be white, which is what would be 
expected under compositional semantics.  It is 
accented on the left (the WHITE house) in contrast 
with the general patern where adjective-noun 
complex nominals are typically accented on the 
right (a nice HOUSE), though there are many 
exceptions to this rule (Sproat 1994).
14
 
 
Linguists would also fel comfortable with diag-
nostic tests based on paraphrases and trans-
formations.  Fixed expresions are fixed.  One 
can’t paraphrase a “red herring” as “*herring that 
is red.”  They resist regular inflection: “*two red 
herings.”  In Bergsma et al (201), we use a 
paraphrase diagnostic to distinguish [N & N] N 
from N & [ N ]: 
 
• [dairy and meat] production 
o meat and dairy production 
o production of meat and dairy 
o production de produits [laitiers et 
de viand] (French) 
                                                             
14
 Sproat has posted a list of 7831 English binary noun 
compounds with hand assigned accent labels at: 
http:/ww.cslu.ogi.edu/~sproatr/newindex/ap90nominals.txt 
• asbestos and [polyvinyl chloride] 
o polyvinyl chloride and asbestos 
o asbestos and chloride 
o l’asbesto e il [polivinilcloruro] 
(Italian) 
The first thre paraphrases make it clear that “dairy 
and meat” is a constituent whereas the last thre 
paraphrases make it clear that “polyvinyl chloride” 
is a constituent.  Comparable corpora can be 
viewed as a rich source of paraphrase data, as in-
dicated by the French and Italian examples above.  
9 Conclusions

How many multiword expresions (MWEs) do 
people know?  The question is related to how 
many words do people know. 20k?  400k?  1M? 
13M?  Is there a bound or does vocabulary size 
increase with experience (corpus size)?  Is vocab-
ulary size a measure of inteligence or just exper-
ience? 
 
Dictionary sizes are just a lower bound because 
they focus on general vocabulary and avoid much 
of what maters on the web. Speling correction is 
not the same for documents of general vocabulary 
and web queries. 
 
One can use Stolcke pruning and other composi-
tionality tricks to cut down on the number of the 
number of multiword units that people must know.  
But obviously, the number they must know is just a 
lower bound on the number they may know. 
 
There are lots of enginering motivations for want-
ing to know how many words and MWEs people 
know. How big does the dictionary have to be for 
X (where X is parsing, tagging, speling correction, 
machine translation, word breaking for Chinese 
and Japanese (and English), speech recognition, 
spech synthesis or some other application)? 
 
Rather than answer these questions, this paper used 
these questions as Liberman did, as an excuse for 
surveying how such issues are adressed in a 
variety of fields: computer science, web search, 
linguistics, lexicography, educational testing, psy-
chology, statistics, etc.  
143
References 
Harald Bayen (201) Word Frequency Distributions. 
Kluwer, Dordrecht. 
 
Michele Banko and Eric Bril. (201) “Scaling to very 
very large corpora for natural language 
disambiguation,” ACL. 
 
Shane Bergsma, David Yarowsky and Kenneth Church 
(201), “Using Large Monolingual and Bilingual 
Corpora to Improve Coordination Disambiguation,” 
ACL. 
 
Dustin Boswel (204) “Speling Korecksion: A Survey 
of Techniques from Past to Present,” 
http:/dustwel.com/PastWork/SpelingCorrectionRes
earchExam.pdf 
 
Andrei Broder. 202. A taxonomy of web search. SIGIR 
Forum 36, 2 (September 2002), 3-10. 
 
Lewis Carol, 1865, Alice’s Adventures in Wonder-
land. 
Keneth Church and Wiliam Gale (1989) “Enhanced 
Good-Turing and Cat-Cal: two new methods for 
estimating probabilities of English bigrams.” HLT. 
 
Keneth Church and Patrick Hanks. (190) “Word 
association norms, mutual information, and lexico-
graphy.” CL. 
 
Cecil Coker, Keneth Church and Mark Liberman 
(190) “Morphology and rhyming: two powerful 
alternatives to letter-to-sound rules for speech 
synthesis,” In ESCA Workshop on Spech Synthesis 
SSW1-1990, 83-86. 
 
Silviu Cucerzan and Eric Bril (204) “Speling Cor-
rection as an Iterative Proces that Exploits the 
Colective Knowledge of Web Users, EMNLP. 
 
Ido Dagan and Kenneth Church. (1994) “Termight: 
identifying and translating technical terminology,” 
ANLC. 
 
Wiliam Gale, Keneth Church and David Yarowsky. 
(192) “One sense per discourse,” HLT. 
 
Bradley Efron and Ronald Thisted, (1976) “Estimating 
the number of unsen species: How many words did 
Shakespeare know?” Biometrika, 63, 3, pp. 435-447. 
 
John Firth, (1957) “A Synopsis of Linguistic Theory 
1930-1955,” in Studies in Linguistic Analysis, 
Philological Society, Oxford; reprinted in Palmer, F. 
(ed.) 1968 Selected Papers of J. R. Firth, Longman, 
Harlow. 
 
Alex Franz and Thorsten Brants (2006) 
http:/googleresearch.blogspot.com/2006/08/al-our-
n-gram-are-belong-to-you.html. 
 
Fred Jelinek (204) “Some of my Best Friends are Lin-
guists,” LREC. 
 
Maryan Kamvar and Doug Beferman, (2010) “Say 
What? Why users chose t speak their web queries,” 
Interspech. 
Barbara Kipfer (ed.) (201) Roget’s Thesaurus, Sixth 
Edition, HarperColins, NY, NY, USA. 
 
Karen Kukich. 192. Techniques for automaticaly 
corecting words in text. ACM Comput. Surv. 24, 4. 
Mark Liberman (1989) “How many words do people 
know?” ACL. 
 
Mark Liberman and Keneth Church (191). "Text 
analysis and word pronunciation in text-to-spech 
synthesis." In Advances in Speech Signal Procesing, 
edited by S. Furui and M. Sondhi. 
 
Frederick Mosteler and David Walace. Inference and 
Disputed Authorship: the Federalist, Addison-Wes-
ley, 1964. 
 
Emily Pitler and Kenneth Church. (209) “Using word-
sense disambiguation methods to classify web 
queries by intent,” EMNLP. 
 
Richard Sproat, Wiliam Gale, Chilin Shih, and Nancy 
Chang. 196. A stochastic finite-state word-segment-
ation algorithm for Chinese, CL. 
 
Richard Sproat (194) “English noun-phrase acent 
prediction for text-to-spech,” Computer Spech and 
Language, 8, p. 79-94. 
 
Andreas Stolcke (198) “Entropy-based Pruning of 
Backof Language Models” Proc. DARPA News 
Transcription and Understanding Workshop. 
 
Lewis Terman, (1918) “The vocabulary test as a 
measure of inteligence,” Journal of Educational 
Psychology, Vol 9(8), pp. 452-466. 
 
David Yarowsky. 193. One sense per colocation, 
HLT. 
 
144

