Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 1–10,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics
EmotiBlog: a finer-grained and more precise learning of subjectivity 
expression models 
Ester Boldrini 
University of Alicante, Department of 
Software and Computing Systems 
eboldrini@dlsi.ua.es 
Alexandra Balahur 
University of Alicante, Department of 
Software and Computing Systems 
abalahur@dlsi.ua.es 
Patricio Martínez-Barco 
University of Alicante, Department of 
Software and Computing Systems 
patricio@dlsi.ua.es 
Andrés Montoyo 
University of Alicante, Department of 
Software and Computing Systems 
montoyo@dlsi.ua.es 
 
Abstract 
 
The exponential growth of the subjective in-
formation in the framework of the Web 2.0 
has led to the need to create Natural Language 
Procesing tols able to analyse and proces 
such dat for multiple practical aplications. 
They require training on specifically annotated 
corpora, whose level of detail must be fine 
enough to capture the phenomena involved. 
This paper presents EmotiBlog – a fine-
grained annotation scheme for subjectivity. 
We show the maner in which it is built and 
demonstrate the benefits it brings to the sys-
tems using it for training, through the experi-
ments we caried out on opinion mining and 
emotion detection. We employ corpora of dif-
ferent textual genres –a set of annotated re-
ported speech extracted from news articles, the 
set of news titles anotated with polarity and 
emotion from the SemEval 2007 (Task 14) 
and ISEAR, a corpus of real-life self-
expressed emotion. We also show how the 
model built from the EmotiBlog anotations 
can be enhanced with external resources. The 
results demonstrate that EmotiBlog, through its 
structure and anotation paradigm, ofers high 
quality training data for systems dealing both 
with opinion mining, as wel as emotion detec-
tion. 
1 Credits

This paper has been supported by Ministe-
rio de Ciencia e InnovaciónSpanish Gov-
ernment (grant no. TIN209-13391-C04-
01), and Conseleria d'Educación-
Generalitat Valenciana (grant no. PRO-
METEO/209/19 and A-
COMP/2010/28). 
2 Introduction

The exponential growth of the subjective infor-
mation with Web 2.0 created the ned to develop 
new Natural Language Procesing (NLP) tools to 
automatically process and manage the content 
available on the Internet. Apart from the tradi-
tional textual genres, at present we have new 
ones such as blogs, forums and reviews. The 
main diference betwen them is that the latter 
are predominantly subjective, containing per-
sonal judgments. At the moment, NLP tols and 
methods for analyzing objective information 
have a beter performance than the new ones the 
research comunity is creating for managing the 
subjective content. The survey caled “The State 
of the Blogosphere 2009”, published by Tech-
norati
1, demonstrates that users are bloging 
more than ever. Furthermore, in contrast to the 
general idea about bloggers, each day it is more 
and more the number of professionals who de-
cide to use this means of comunication, contra-
dicting the common belief about the predomi-
nance of an informal editing (Balahur et al., 
2009). Due to the growing interest in this text 
type, the subjective data of the Web is increasing 
on a daily basis, becoming a reflection of peo-
ple’s opinion about a wide range of topics. (Cui, 
Mital and Datar, 206). Blogs represent an im-
portant source of real-time, unbiased informa-
tion, useful for the development of many aplica-
tions for concret purposes. Given the proved 
importance of automatically procesing this data, 
a new task has appeared in NLP task, dealing 
with the treatment of subjective data: Sentiment 
Analysis (SA). The main objective of this paper 
is to present EmotiBlog (Boldrini et al., 209), a 
fine-grained annotation scheme for labeling sub-
jectivity in the new textual genres. Subjectivity 
                                                
1
 http:/technorati.com/ 
1
can be reflected in text through expresions of 
emotions beliefs, views (a way of considering 
something)
2
 and opinions, generally denomi-
nated “private states” (Uspensky, 1973), not 
open to verification (Wiebe, 1994). We per-
formed a series of experiments focused on dem-
onstrating that EmotiBlog represnts a step for-
ward to previous research in this field; its use 
allows a finer-grained and more precise learning 
of subjectivity expresion models. Starting form 
(Wiebe, Wilson and Cardie, 205) we created an 
annotation schema able to capture a wide range 
and key elements, which give subjectivity, mov-
ing a step forward the mere polarity recognition. 
In particular, the experiments concern expres-
sions of emotion, as a finer-grained analysis of 
affect in text and a subsequent task to opinion 
mining (OM) and clasification. To that aim, we 
employ corpora of different textual genres– a set 
of annotated reported speech extracted from 
news articles (denominated JRC quotes) (Bala-
hur et al., 2010) and the set of news titles anno-
tated with polarity and emotion from the SemE-
val 2007 Task No. 14 (Strapparava and Mihal-
cea, 207), as well as a corpus of real-life self-
expressed emotion entitled ISEAR (Scherer and 
Walbot, 199). We subsequently show, through 
the quality of the results obtained, that Emoti-
Blog, through its structure and anotation para-
digm, offers high quality training for systems 
dealing both with opinion mining, as wel as 
emotion detection.  
3 Motivation
and Contribution 
The main motivation of this research is the dem-
onstrated necesity to work towards the harmoni-
zation and interoperability of the increasingly 
large number of tools and frameworks that sup-
port the creation, instantiation, manipulation, 
querying, and exploitation of annotated resource. 
This necesity is stresed by the new tols and 
resources, which have ben recently created for 
processing the subjectivity in the new-textual 
genres born with the Web 2.0. Such predomi-
nantly subjective data is increasing at an expo-
nential rate (about 75000 new blogs are reported 
to be created every day) and contains opinions on 
the most diverse set of topics. Given its world-
wide availability, the subjective data on the Web 
has become a primary source of information 
(Balahur et al., 209). As a consequence, new 
mechanisms have to be implemented so that this 
                                                
2
 http:/dictionary.cambridge.org/ 
data is efectively analyzed and processed. The 
main challenge of the opinionated content is that, 
unlike the objective one, which presents facts, 
the subjective information is most of the times 
dificult and complex to extract and clasify us-
ing in gramatically static and fixed rules. Ex-
presion of subjectivity is more spontaneous and 
even if the majority is quite formal, new means 
of expresivity can be encountered, such as the 
use of colloquialisms, sayings, collocations or 
anomalies in the use of punctuation; this is moti-
vated by the fact that subjectivity expression is 
part of our daily life. For example, at the time of 
taking a decision, people search for information 
and opinions expressed on the Web on their mat-
ter of interest and base their final decision on the 
information found. At the same time, when using 
a product, people often write reviews on it, so 
that others can have a better idea of the perform-
ance of that product before purchasing it. There-
fore, on the one hand, the growing volume of 
opinion information available on the Web alows 
for beter and more informed decisions of the 
users. On the other hand, the amount of data to 
be analyzed requires the development of special-
ized NLP systems that automatically extract, 
classify and summarize the data available on the 
Web on diferent topics. (Esuli and Sebastiani, 
2006) define OM as a recent discipline at the 
crosroads of Information Retrieval and Compu-
tational Linguistics, which is concerned not with 
the topic a document is about, but with the opin-
ion it expreses. Research in this field has proven 
the task to be very dificult, due to the high se-
mantic variability of afective language. Difer-
ent authors have addressed the problem of ex-
tracting and clasifying opinion from different 
perspectives and at diferent levels, depending on 
a series of factors which can be level of interest 
(overall/specific), querying formula (“Nokia 
E65”/”Why do people buy Nokia E65?”), type of 
text (review on forum/blog/dialogue/pres arti-
cle), and manner of expression of opinion di-
rectly (using opinion statements, e.g. “I think this 
product is wonderful!”/”This is a bright initia-
tive”), indirectly (using afect vocabulary, e.g. “I 
love the pictures this camera 
takes!”/”Personally, I am shocked one can pro-
pose such a law!”) or implicitly (using adjectives 
and evaluative expresions, e.g. “It’s light as a 
feather and fits right into my pocket!”). While 
determining the overal opinion on a movie is 
suficient for taking the decision to watch it or 
not, when buying a product, people are interested 
in the individual opinions on the diferent prod-
2
uct characteristics. When discussing a person, 
one can judge and give opinion on the person’s 
actions. Moreover, the approaches taken can vary 
depending on the manner in which a user asks 
for the data (general formula such as “opinions 
on X” or a specific question “Why do people like 
X?” and the text source that needs to be queried). 
Retrieving opinion information in newspaper 
articles or blogs posts is more complex, because 
it involves the detection of different discusion 
topics, the subjective phrases present and subse-
quently their clasification acording to polarity. 
Especialy in the blog area, determining points of 
view expresed in dialogues together with the 
mixture of quotes and pastes from newspapers on 
a topic can, additionally, involve determining the 
persons present and whether or not the opinion 
expressed is on the required topic or on a point 
previously made by another speaker. This difi-
cult NLP problem requires the use of specialized 
data for system training and tuning, gathered, 
annotated and tested within the diferent text 
spheres. At the present moment, these 
specialized resources are scarce and when they 
exist, they are rather simplisticaly annotated or 
highly domain-dependent. Moreover, most of 
these resources created are for the English. The 
contribution we describe in this paper intends to 
propose solutions to the above-mentioned 
problems, and consists of the following points: 
first of al, we overcome the problem of corpora 
scarcity in other languages except English and 
also improve the English ones; we present the 
maner in which we compiled a multilingual 
corpus of blog posts on diferent topics of 
interest in thre languages-Spanish, Italian and 
English. The second isue we tried to solve was 
the coarse-grained annotation schemas employed 
in other anotation schema. Thus, we describe 
the new anotation model, EmotiBlog built up in 
order to capture the different 
subjectivity/objectivity, emotion/opinion/attitude 
aspects we are interested in at a finer-grained 
level. We justify the ned for a more detailed 
annotation model, the sources and the reasons 
taken into consideration when constructing the 
corpus and its annotation. Thirdly, we adres an 
aspect strongly related to blogs annotation: due 
the presence of “copy and pastes” from news 
articles or other blogs, the frequent quotes, we 
include the anotation of both the directly 
indicated source, as well as the anaphoric 
references at cros-document level. We discuss 
on the problems encountered at different stages 
and comment upon some of the conclusions we 
have reached while performing this research. 
this research. Finally, we conclude on our ap-
proach and propose the lines for future work. 
4 Related
Work 
In recent years, different researchers have ad-
dresed the needs and posible methodlogies 
from the linguistic, theoretical and practical 
points of view. Thus, the first step involved re-
sided in building lexical resources of afect, such 
as WordNet Affect (Straparava and Valituti, 
2004), SentiWordNet (Esuli and Sebastiani, 
2006), Micro-WNOP (Cerini et. Al, 207) or 
“emotion triggers” (Balahur and Montoyo, 
2009). All these lexicons contain single words, 
whose polarity and emotions are not necesarily 
the ones annotated within the resource in a larger 
context. We also employed the ISEAR corpus, 
consisting of phrases where people describe a 
situation when they felt a certain emotion. Our 
work, therefore, concentrates on anotating 
larger text spans, in order to consider the undeni-
able influence of the context. The starting point 
of research in emotion is represented by (Balahur 
and Montoyo, 208), who centered the idea of 
subjectivity around that of private states, and set 
the benchmark for subjectivity analysis as the 
recognition of opinion-oriented language in order 
to distinguish it from objective language and giv-
ing a method to anotate a corpus depending on 
these two aspects – MPQA (Wiebe, Wilson and 
Cardie, 205). Furthermore, authors show that 
this initial discrimination is crucial for the senti-
ment task, as part of Opinion Information Re-
trieval  (last thre editions of the TREC Blog 
tracks
3
 competitions, the TAC 208 competi-
tion
4
), Information Extraction (Rilof and Wiebe, 
2003) and Question Answering (Stoyanov et al., 
2004) systems. Once this discrimination is done, 
or in the case of texts containing only or mostly 
subjective language (such as e-reviews), opinion 
mining becomes a polarity clasification task. 
Our work takes into consideration this initial dis-
crimination, but we also ad a deper level of 
emotion annotation. Since expressions of emo-
tion are also highly related to opinions, related 
work also includes customer review clasifica-
tion at a document level, sentiment clasification 
using unsupervised methods (Turney, 202), 
Machine Learning techniques (Pang and Le, 
2002), scoring of features (Dave, Lawrence and 
Penock, 203), using PMI, syntactic relations 
                                                
3
 http:/trec.nist.gov/data/blog.html 
4
 http:/ww.nist.gov/tac/ 
3
and other attributes with SVM (Mulena and Col-
lier, 204), sentiment clasification considering 
rating scales (Pang and Le, 202), supervised 
and unsupervised methods (Chaovalit and Zhou, 
2005) ad sisuprvised learning (Goldberg 
and Zhou, 206). Research in clasification at a 
document level included sentiment clasification 
of reviews (Ng, Dasgupta and Arifin, 206), sen-
timent classification on customer fedback data 
(Gamon, Aue, Corston-Oliver, Ringer, 205), 
comparative experiments (Cui, Mital and Datar, 
2006). Other research has ben conducted in ana-
lysing sentiment at a sentence level using bot-
straping techniques (Rilof, Wiebe, 203), con-
sidering gradable adjectives (Hatzivasiloglou, 
Wiebe, 200), semisupervised learning with the 
initial training some strong paterns and then ap-
plying NB or self-training (Wiebe and Rilof, 
2005) finding strength of opinions (Wlsn, 
Wiebe, Hwa, 204) sum up orientations of opin-
ion words in a sentence (or within some word 
window) (Kim and Hovy, 204), (Wilson and 
Wiebe, 204), determining the semantic orienta-
tion of words and phrases (Turney and Littman, 
2003), identifying opinion holders (Stoyanov and 
Cardie, 206), comparative sentence and relation 
extraction and feature-based opinion mining and 
sumarization (Turny, 202). Finaly, fine-
grained, feature-based opinion summarization is 
defined in (Hu and Liu, 2004) and researched in 
(Turney, 202) or (Pang and Le, 202). All 
these aproaches concentrate on finding and 
classifying the polarity of opinion words, which 
are mostly adjectives, without taking into ac-
count modifiers or the context in general. Our 
work, on the other hand, represents the first step 
towards achieving a contextual comprehension 
of the linguistic roots of emotion expresion. 
5 Corpora

It is wel known that nowadays blogs are the 
second way of comunication most used after 
the e-mail. They are extremely useful and a poll 
for discusing about any topic with the world. 
For this reason, the first corpus object of our 
study is a colection of blog posts extracted from 
the Web. The texts we selected have distinctive 
features, extremely different from traditional tex-
tual ones. In fact people writing a post can use an 
informal language colloquialism, emoticons, etc. 
to expres their felings and it is not rare to find 
a mix of sources in the same post; people usualy 
mention some facts or discourses and then they 
give their opinion about them. As we can deduce, 
the source detection represents one of the most 
complex tasks. As we mentioned above, we car-
ried out a multilingual research, colecting texts 
in thre languages: Spanish, Italian, and English 
about three subjects of interest. The first one 
contains blog posts comenting upon the signing 
of the Kyoto Protocol against global warming, 
the second collection consists of blog entries 
about the Mugabe government in Zimbabwe, and 
finaly we selected a series of blog posts discuss-
ing the issues related to the 208 USA presiden-
tial elections. For each of the abovementioned 
topics, we have gathered 10 texts, suming up 
a total of 30.00 words approximately for each 
language. However in this research we start with 
English but consider as future work labeling the 
other languages we have. The second corpus we 
employed for this research is a collection of 1592 
quotes extracted from the news in April 208. As 
a consequence they are about many diferent top-
ics and in English (Balahur and Steinberg, 209). 
Both of these corpora have been annotated with 
EmotiBlog that is presented in the next section. 
6 EmotiBlog
Anotation Model 
Our anotation schema can be defined as a fine-
grained model for labeling subjectivity of the 
new-textual genres born with the Web 2.0. As 
mentioned above, it represents a step forward to 
previous research and it is focused on detecting 
the linguistic elements, which give subjectivity 
to the text. The EmotiBlog annotation is divided 
into different levels (Figure 1). 
 
 
Figure 1: General structure of EmotiBlog. 
 
As we can observe in Figure 1, the first distinc-
tion to be made is betwen objective and subjec-
tive spech. If we are labelling an objective sen-
tence, we insert the source element, while if we 
are annotating a subjective discourse, a list of 
elements with the coresponding attributes have 
to be aded. We select among the list of subjec-
tive elements and specify the element’s atrib-
4
utes. Table 1 presents the annotation model in 
detail. 
 
Elem. Description 
Obj. spech Confidence, coment, source, target. 
Subj. spech Confidence, coment, level, emotion, 
phenomenon, polarity, source and target. 
Adjectives Confidence, coment, level, emotion, 
phenomenon, modifier/not, polarity, source 
and target. 
Adverbs Confidence, coment, level, emotion, 
phenomenon, modifier/not, polarity, source 
and target. 
Verbs Confidence, coment, level, emotion, 
phenomenon, polarity, mode, source and 
target. 
Anaphora Confidence, coment, type, source and 
target. 
Capital letter Confidence, coment, level, emotion, 
phenomenon, modifier/not, polarity, source 
and target. 
Punctuation Confidence, coment, level, emotion, 
phenomenon, modifier/not, polarity, source 
and target. 
Names Confidence, coment, level, emotin, 
phenomenon, modifier/not, polarity, and 
source. 
Phenomenon Confidence, coment, type: colocation, 
saying, slang, title, and rhetoric. 
Reader Inter-
pretation 
Confidence, coment, level, emotion, 
phenomenon, polarity, source and target. 
Author Inter-
pretation 
Confidence, coment, level, emotion, 
phenomenon, polarity, source and target. 
Emotions Confidence, coment, acept, anger, 
anticipation, anxiety, appreciation, bad, 
bewilderment, comfort, … 
Table 1: EmotiBlog structure 
 
Each element of the discourse has its own atrib-
utes with a series of features, which have to be 
annotated. Due to space reasons it is imposible 
to detail each one of them, however we would 
like to underline the most innovative and rel-
evant. For each element we are labelling the an-
notator has to insert his level of confidence. In 
this way we will asign each label a weight that 
wil be computed for future evaluations. More-
over, the annotator has to insert the polarity, 
which can be positive or negative, the level 
(high, medium, low) and also the sentiment this 
element is expressing. Table 2 presnts a com-
plete list of the emotions we selected to be part 
of EmotiBlog. We grouped al sentiments into 
subgroups in order to help the evaluation pro-
cess. In fact emotions of the same subgroup wil 
have les impact when calculating the inter-
annotation agreement. In order to make this sub-
division proper and efective division, we were 
inspired by (Scherer, 205) who created an alter-
native dimensional structure of the semantic 
space for emotions. The graph below represents 
the maping of the term Rusell (1983) uses for 
his claim of an emotion circumflex in two-
dimensional valence by activity/arousal space 
(uper-case terms). As we can apreciate, the 
circle is divided by 4 axes. Moreover, Scherer 
distinguishes betwen positive and negative sen-
timents and after that betwen active and pasive. 
Furthermore emotions are grouped betwen ob-
structive and conductive, and finaly betwen 
high power and low power control. We started 
form this classification, grouping sentiments into 
positive and negative, but we divided them as 
high/low power control, obstructive/conductive 
and active/passive. Further on, we distributed the 
sentiments within our list into the Scherer slots 
creating other smaller categories included in the 
abovementioned general ones. The result of this 
division is shown in Table 2: 
Table 2: Alternative dimensional structures of the 
semantic space for emotions 
 
Folowing with the description of the model, we 
said that the first distinction to be made is be-
twen objective and subjective spech. Analys-
ing the texts we collected, we realised that even 
if the writer uses an objective spech, sometimes 
it is just aparently objective and for this reason 
we aded two elements: reader and author inter-
pretation. The first one is the impres-
sion/feling/reaction the reader has reading the 
intervention and what s/he can deduce from the 
piece of text and the author interpretation is what 
we can understand from the author (politic orien-
tation, preferences). All this information can be 
deduced form some linguistic elements that ap-
parently are not so objective as they may appear. 
Another inovative element we inserted in the 
model is the coreference but just at a cros-post 
level. It is necesary because blogs are composed 
by posts linked betwen them and thus cros-
Group Emotions 
Criticism Sarcasm, irony, incorrect, criticism, 
objection, opposition, scepticism. 
Hapines Joy, joke. 
Support Accept, corect, god, hope, support, 
trust, rapture, respect, patience, 
appreciation, excuse. 
Importance Important, interesting, wil, justice, 
longing, anticipation, revenge. 
Gratitude Thank. 
Guilt Guilt, vexation. 
Fear Fear, fright, troublednes, anxiety. 
Surprise Surprise, bewilderment, disappoint-
ment, consternation. 
Anger Rage, hatred, enmity, wrath, force, 
anger, revendication. 
Envy Envy, rivalry, jealousy. 
Indiference Unimportant, yield, slugishnes. 
Pity Compasion, shame, grief. 
Pain Sadnes, lament, remorse, mournig, 
depression, despondency. 
Shynes Timidity. 
Bad Bad, malice, disgust, gred. 
5
document coreference can help the reader to fol-
low the conversations. We also label the unusual 
usage of capital leters and repeated punctuation. 
In fact, it is very comon in blogs to find words 
writen in capital letter or with no conventional 
usage of punctuation; these features usualy 
mean shouts or a particular mood of the writer. 
Using EmotiBlog, we anotate the single ele-
ments, but we also mark sayings or colocations, 
representative of each language. A saying is a 
wel-known and wise statement, which often has 
a meaning, different from the simple meanings of 
the words it contains
5
; while a collocation is a 
word or phrase, which is frequently used with 
another word or phrase, in a way that sounds cor-
rect to native speakers, but might not be expected 
from the individual words’ meanings6. Finally 
we insert for each element the source and topic. 
An example of anotation can be:  <phenomenon 
target="Kyoto Protocol" category="phrase" degre="medium" 
source="w" polarity="positive" emotion="god">The Onion has a 
<adjective target="Kyoto Protocol" phenomenon="phrase" de-
gre="medium" polarity="positive" emotion="good" source="w" 
ismodifier="yes">great</adjective> story today titled “Bush Told 
to Sign Birthday Treaty for Someone Named Kyoto." 
</phenomenon> 
7 Experiments
and Evaluation 
In order to evaluate the apropriatenes of the 
EmotiBlog anotation scheme and to prove that 
the fine-grained level it aims at has a positive 
impact on the performance of the systems em-
ploying it as training, we performed several ex-
periments. Given that a) EmotiBlog contains an-
notations for individual words, as wel as for 
multi-word expresions and at a sentence level, 
and b) they are labeled with polarity, but also 
emotion, our experiments show how the anno-
tated elements can be used as training for the 
opinion mining and polarity clasification task, 
as well as for emotion detection. Moreover, tak-
ing into consideration the fact that EmotiBlog 
labels the intensity level of the anotated ele-
ments, we performed a brief experiment on de-
termining the sentiment intensity, measured on a 
thre-level scale: low, medium and high. In order 
to perform these thre different evaluations, we 
chose three diferent corpora. The first one is a 
colection of quotes (reported speech) from 
newspaper articles presented in (Balahur et al., 
2010), enriched with the manual fine-grained 
                                                
5
  Definition acording to the Cambridge Advanced Learner’s 
Dictionary 
6
   Definition acording to the Cambridge Advanced Learner’s 
Dictionary 
annotation of EmotiBlog
7
; the second one is the 
colection of newspaper titles in the test set of the 
SemEval 207 task number 14 – Afective Text. 
Finaly, the third one is a corpus of self-reported 
emotional response – ISEAR (Scherer and Wal-
bott, 1999). The intensity clasification task is 
evaluated only on the second corpus, given that it 
is the only one in which scores betwen -100 and 
0 and 0 and 100, respectively, are given for the 
polarity of the titles. 
 
6.1 Creation
of training models 
For the OM and polarity clasification task, we 
first extracted the Named Entities contained in 
the anotations using Lingpipe and united 
through a “_” all the tokens pertaining to the NE. 
All the anotations of punctuation signs that had 
a specific meaning together were also united un-
der a single punctuation sign. Subsequently, we 
procesed the annotated data, using Minipar. We 
compute, for each word in a sentence, a series of 
features (some of these features are used in (Choi 
et al., 205): 
• the part of spech (POS) 
• capitalization (if all letters are in capitals, if 
only the first leter is in capitals, and if it is a 
NE or not) 
• opinionatednes/intensity/emotion if the 
word is anotated as opinion word, its polar-
ity, i.e. 1 and -1 if the word is positive or 
negative, respectively and 0 if it is not an 
opinion word, its intensity (1.2 or 3) and 0 if 
it is not a subjective word, its emotion (if it 
has, none otherwise) 
• syntactic relatedness with other opinion 
word – if it is directly dependent of an opin-
ion word or modifier (0 or 1), plus the polar-
ity/intensity and emotion of this word (0 for 
all the components otherwise) 
•  role in 2-word, 3-word and 4-word anota-
tions: opinionatednes, intensity and emo-
tion of the other words contained in the an-
notation, direct dependency relations with 
them if they exist and 0 otherwise.  
We compute the length of the longest sentence in 
EmotiBlog. The feature vector for each of the 
sentences contains the feature vectors of each of 
its words and 0s for the coresponding feature 
vectors of the words, which the current sentence 
has les than the longest annotated sentence. Fi-
naly, we add for each sentence as feature binary 
features for subjectivity and polarity, the value 
corresponding to the intensity of opinion and the 
                                                
7
 Frely available on request to the authors. 
6
general emotion. These feature vectors are fed 
into the Weka
8
 SVM SMO ML algorithm and a 
model is created (EmotiBlog I). A second model 
(EmotiBlog I) is created by ading to the colec-
tion of single opinion and emotion words ano-
tated in EmotiBlog, the Opinion Finder lexicon 
and the opinion words found in MicroWordNet, 
the General Inquirer resource and WordNet Af-
fect. 
 
6.2 Evaluation
of models on test sets 
 
In order to evaluate the performance of the mod-
els extracted from the features of the annotations 
in EmotiBlog, we performed different tests. The 
first one regarded the evaluation of the polarity 
and intensity classification task using the Emoit-
blog I and II constructed models on two test sets 
– the JRC quotes colection and the SemEval 
2007 Task Number 14 test set. Since the quotes 
often contain more than a sentence, we consider 
the polarity and intensity of the entire quote as 
the most frequent result in each clas, corre-
sponding to its constituent sentences. Also, given 
the fact that the SemEval Afective Text head-
lines were given intensity values betwen -100 
and 10, we mapped the values contained in the 
Gold Standard of the task into three categories: [-
100, -67] is high (value 3 in intensity) and nega-
tive (value -1 in polarity), [-66, 34] medium 
negative and [33, 1] is low negative. The values 
betwen [1 and 100] are mapped in the same 
maner to the positive category. 0 was consid-
ered objective, so containing the value 0 for in-
tensity. The results are presented in Table 3 (the 
values I and I correspond to the models Emoti-
Blog I and EmotiBlog I): 
 
Test 
Corpus 
Evaluation 
type 
Precision Recal 
Polarity 32.13 54.09 JRC quotes I 
Intensity 36.00 53.2 
Polarity 36.4 51.00 JRC quotes 
II Intensity 38.7 57.81 
Polarity 38.57 51.3 SemEval I 
Intensity 37.39 50.9 
Polarity 35.8 58.68 SemEval I 
Intensity 32.3 50.4 
Table 3. Results for polarity and intensity clasifi-
cation using the models built from the EmotiBlog 
annotations 
The results shown in Table 2 show a signifi-
cantly high improvement over the results ob-
tained in the SemEval task in 207. This is ex-
plainable, on the one hand, by the fact that sys-
                                                
8
 http:/ww.cs.waikato.ac.nz/ml/weka/ 
tems performing the opinion task did not have at 
their disposal the lexical resources for opinion 
employed in the EmotiBlog II model, but also 
because of the fact that they did not use machine 
learning on a corpus comparable to EmotiBlog 
(as sen from the results obtained when using 
solely the EmotiBlog I corpus). Compared to the 
NTCIR 8 Multilingual Analysis Task this year, 
we obtained significant improvements in preci-
sion, with a recal that is comparable to most of 
the participating systems. In the second experi-
ment, we tested the performance of emotion clas-
sification using the two models built using Emo-
tiBlog on the thre corpora – JRC quotes, SemE-
val 2007 Task No.14 test set and the ISEAR cor-
pus. The JRC quotes are labeled using Emoti-
Blog; however, the other two are labeled with a 
smal set of emotions – 6 in the case of the Se-
mEval data (joy, surprise, anger, fear, sadnes, 
disgust) and 7 in ISEAR (joy, sadnes, anger, 
fear, guilt, shame, disgust). Moreover, the Se-
mEval data contains more than one emotion per 
title in the Gold Standard, therefore we consider 
as corect any of the classifications containing 
one of them. In order to unify the results and ob-
tain comparable evaluations, we asesed the 
performance of the system using the alternative 
dimensional structures defined in Table 1. The 
ones not overlapping with the category of any of 
the 8 different emotions in SemEval and ISEAR 
are considered as “Other” and are not included 
either in the training, nor test set. The results of 
the evaluation are presented in Table 4. Again, 
the values I and II correspond to the models 
EmotiBlog I and I. The “Emotions” category 
contains the folowing emotions: joy, sadness, 
anger, fear, guilt, shame, disgust, surprise. 
 
Test 
corpus 
Evaluation 
type 
Precision Recal 
JRC 
quotes I 
Emotions 
 
24.7 15.08 
JRC 
quotes I 
Emotions 
 
33.65 18.98 
SemEval I Emotions 29.03 18.89 
SemEval I Emotions 32.98 18.45 
ISEAR I Emotions 22.31 15.01 
ISEAR II Emotions 25.62 17.83 
Table 4. Results for emotion clasification using the 
models built from the EmotiBlog anotations. 
The best results for emotion detection were ob-
tained for the “anger” category, where the preci-
sion was around 35 percent, for a recal of 19 
percent. The worst results obtained were for the 
ISEAR category of “shame”, where precision 
was around 12 percent, with a recal of 15 per-
7
cent. We believe this is due to the fact that the 
latter emotion is a combination of more complex 
affective states and it can be easily misclassified 
to other categories of emotion. Moreover, from 
the analysis performed on the erors, we realized 
that many of the afective phenomena presented 
were more explicit in the case of texts expresing 
strong emotions such as “joy” and “anger”, and 
were mostly related to comon-sense interpreta-
tion of the facts presented in the weaker ones. As 
it can be sen in Table 3, results for the texts per-
taining to the news category obtain better results, 
most of al news titles. This is due to the fact that 
such texts, although they contain a few words, 
have a more direct and stronger emotional charge 
than direct spech (which may be biased by the 
need to be diplomatic, find the best suited words 
etc.). Finally, the error analysis showed that emo-
tion that is directly reported by the persons expe-
riencing is more “hiden”, in the use of words 
carrying special signification or related to gen-
eral human experience. This fact makes emotion 
detection in such texts a harder task. Neverthe-
les, the results in all corpora are comparable, 
showing that the aproach is robust enough to 
handle diferent text types. Al in al, the results 
obtained using the fine and coarse-grained anno-
tations in EmotiBlog increased the performance 
of emotion detection as compared to the systems 
in the SemEval competition. 
 
6.3 Discusion
on the overall results 
 
From the results obtained, we can se that this 
approach combining the features extracted from 
the EmotiBlog fine and coarse-grained annota-
tions helps to balance betwen the results ob-
tained for precision and recall. The impact of 
using additional resources that contain opinion 
words is that of increasing the recal of the sys-
tem, at the cost of a slight drop in precision, 
which proves that the aproach is robust enough 
so that aditional knowledge sources can be 
added. Although the corpus is small, the results 
obtained show that the phenoena it captures is 
relevant in the OM task, not only for the blog 
sphere, but also for other types of text (newspa-
per articles, self-reported afect). 
8 Conclusions
and future work 
Due to the exponential increase of the subjective 
information result of the high-level usage of the 
Internet and the Web 2.0, NLP able to process 
this data are required. In this paper we presented 
the procedure by which we compiled a multilin-
gual corpus of blog posts on diferent topics of 
interest in thre languages: Spanish, Italian and 
English. Further on, we explained the need to 
create a finer-grained annotation schema that can 
be used to improve the performance of subjectiv-
ity mining systems. Thus, we presented the new 
annotation model, EmotiBlog and justifed the 
benefits of this detailed anotation schema, pre-
senting the sources and the reasons taken into 
consideration when building up the corpus and 
its labeling. Furthermore, we adresed the pres-
ence of “copy and pastes” from news articles or 
other blogs, the frequent quotes. For solving this 
possible ambiguity we included the anotation of 
both the directly indicated source, as wel as the 
anaphoric references at cross-document level. 
We performed several experiments on thre dif-
ferent corpora, aimed at finding and clasifying 
both the opinion, as wel as the expressions of 
emotion they contained; we showed that the fine 
and coarse-grained levels of annotation that 
EmotiBlog contains ofers important information 
on the structure of afective texts, leading to an 
improvement of the performance of systems 
trained on it. Although the EmotiBlog corpus is 
smal, the results obtained are promising and 
show that the phenomena it captures are relevant 
in the OM task, not only for the blog sphere, but 
also for other textual-genres. It is wel known 
that OM is an extremely challenging task and a 
young discipline, thus there is room for im-
provement above al to solve linguistic phenom-
ena such as the coreference resolution at a cros 
document level, temporal expresion recognition. 
In adition to this, more experiments would ned 
to be done in order to verify the complete ro-
bustnes of EmotiBlog. Last but not least, our 
idea is to include the existing tools for a more 
effective semi-supervised anotation. After the 
training of the ML system we obtain automati-
cally some markables which have to be validated 
or not by the annotator and the ideal option 
would be to conect these terms the system de-
tects automatically with tools, such as the map-
ping with an opinion lexicon based on WordNet 
(SentiWordNet, WordNet Afect, MicroWord-
Net), in order to automaticaly anotate al the 
synonyms and antonyms with the same or the 
opposite polarity respectively and assigning them 
some other elements contemplated into the Emo-
tiBlog anotation schema. This would mean an 
important step forward for saving time during the 
annotation proces and it will also asure a high 
quality annotation due to the human supervision. 
8
References 
Balahur A., Steinberger R., Kabadjov M., Zavarela 
V., van der Got E., Halkia M., Pouliquen B., and 
Belyaeva J. 2010. Sentiment Analysis in the News. 
In Proceedings of LREC 2010. 
Balahur A., Boldrini E., Montoyo A., Martínez-Barco 
P. 2009. A Comparative Study of Open Domain 
and Opinion Question Answering Systems for Fac-
tual and Opinionated Queries. In Procedings of 
the Recent Advances in Natural Language Proc-
essing. 
Balahur A., Montoyo A. 208. Applying a Culture 
Dependent Emotion Trigers Database for Text 
Valence and Emotion Clasification. In Proced-
ings of the AISB 2008 Symposium on Afective 
Language in Human and Machine, Aberden, Scot-
land. 
Balahur A., Steinberger R., Rethinking Sentiment 
Analysis in the News: from Theory to Practice and 
back. In Proceding of WOMSA 209. Sevile. 
Balahur A., Boldrini E., Montoyo A., Martínez-Barco 
P. 209. Summarizing Threads in Blogs Using 
Opinion Polarity. In Procedings of ETS work-
shop. RANLP. 209. 
Boldrini E., Balahur A., Martínez-Barco P., Montoyo 
A. 209. EmotiBlog: a fine-grained model for 
emotion detection in non-traditional textual gen-
res. In Procedings of WOMSA. Seville, Spain. 
Boldrini E., Fernández J., Gómez J.M., Martínez-
Barco P. 209. Machine Learning Techniques for 
Automatic Opinion Detection in Non-Traditional 
Textual Genres. In Procedings of WOMSA 209. 
Sevile, Spain. 
Chaovalit P, Zhou L. 205. Movie Review Mining: a 
Comparison between Supervised and Unsupervised 
Clasification Aproaches. In Procedings of 
HICS-05. 
Carleta J. 196. Assesing agrement on 
clasification task: the kapa statistic. Computa-
tional Linguistics, 22(2): 249–254. 
Cui H., Mital V., Datar M. 206. Comparative Ex-
periments on Sentiment Classification for Online 
Product Reviews. In Procedings of the 21st Na-
tional Conference on Artificial Intelligence AAI. 
Cerini S., Compagnoni V., Demontis A., Formentelli 
M., and Gandini G. 207. Language resources and 
linguistic theory: Typology, second language ac-
quisition. English linguistics (Forthcoming), chap-
ter Micro-WNOp: A gold standard for the evalua-
tion of automatically compiled lexical resources for 
opinion mining. Franco Angeli Editore, Milano, IT. 
Choi Y., Cardie C., Rilof E., Padwardhan S. 205. 
Identifying Sources of Opinions with Conditional 
Random Fields and Extraction Paterns.  In Pro-
ceedings of the HLT/EMNLP. 
Dave K., Lawrence S., Penock, D. “Mining the Pea-
nut Galery: Opinion Extraction and Semantic 
Clasification of Product Reviews”. In Procedings 
of WW-03. 2003. 
Esuli A., Sebastiani F. 206. SentiWordNet: A Pub-
licly Available Resource for Opinion Mining. In 
Proceedings of the 6th International Conference on 
Language Resources and Evaluation, LREC 206, 
Genoa, Italy. 
Gamon M., Aue S., Corston-Oliver S., Ringer E. 
2005. Mining Customer Opinions from Free Text. 
Lecture Notes in Computer Science. 
Goldberg A.B., Zhu J. 206. Seeing stars when there 
aren’t many stars: Graph-based semi-supervised 
learning for sentiment categorization. In HLT-
NAACL 206 Workshop on Textgraphs: Graph-
based Algorithms for Natural Language Process-
ing. 
Hu M., Liu B. 204. Mining Opinion Features in Cus-
tomer Reviews. In Procedings of Nineteenth Na-
tional Conference on Artificial Intelligence AAI. 
Hatzivasiloglou V., Wiebe J. 200. Effects of adjec-
tive orientation and gradability on sentence subjec-
tivity. In Procedings of COLING.  
Kim S.M., Hovy E. 204. Determining the Sentiment 
of Opinions. In Procedings of COLING. 
Mulen T., Colier N. 206. Sentiment Analysis Using 
Support Vector Machines with Diverse Information 
Sources. In Procedings of EMNLP. 204. Lin, 
W.H., Wilson, T., Wiebe, J., Hauptman, A. “Which 
Side are You On? Identifying Perspectives at the 
Document and Sentence Levels”. In Procedings of 
the Tenth Conference on Natural Language Learn-
ing CoNL.206.  
Ng V., Dasgupta S. and Arifin S. M. 206. Examining 
the Role of Linguistics Knowledge Sources in the 
Automatic Identification and Clasification of Re-
views. In the procedings of the ACL, Sydney. 
Pang B., Le L., Vaithyanathan S. 202. Thumbs up? 
Sentiment classification using machine learning 
techniques. In Procedings of EMNLP-02, the 
Conference on Empirical Methods in Natural Lan-
guage Procesing. 
Rilof E., Wiebe J. 203. Learnig Extraction Pat-
terns for Subjective Expresions. In Procedings of 
the 203 Conference on Empirical Methods in 
Natural Language Procesing.  
Straparava C. Valituti A. 204. WordNet-Affect: an 
affective extension of WordNet. In Procedings 
ofthe 4th International  Conference on Language 
Resources and Evaluation, LREC. 
Rusel J.A. 1983. Pancultural aspects of the human 
conceptual organization of emotions. Journal of 
Personality and Social Psychology 45: 1281–8. 
Scherer K. R. 205. What are emotions? And how can 
they be measured? Social Sciece Ifrmatio, 
44(4), 693–727. 
Stoyanov V. and Cardie C. 206. Toward Opinion 
Summarization: Linking the Sources. COLING-
ACL. Workshop on Sentiment and Subjectivity in 
Text. 
Stoyanov V., Cardie C., Litman D., and Wiebe J. 
2004. Evaluating an Opinion Annotation Scheme 
Using a New Multi-Perspective Question and An-
9
swer Corpus. AAI Spring Symposium on Explor-
ing Attitude and Afect in Text.  
Straparava and Mihalcea, 207 SemEval 207 
Task 14: Afective Text. In  Procedings of the 
ACL.   
Turney P. 202. Thumbs Up or Thumbs Down? Se-
mantic Orientation Aplied to Unsupervised Clas-
sification of Reviews. ACL 202: 417-424. 
Turney P., Litman M. 203. Measuring praise and 
criticism: Inference of semantic orientation from 
association. ACM Transactions on Information 
Systems 21. 
Uspensky B. 1973. A Poetics of Composition. Univer-
sity of California Press, Berkeley, California. 
Wiebe J. M. 194. Tracking point of view in nara-
tive. Computational Linguistics, vol. 20, p. 23–
287. 
Wiebe J., Wilson T. and Cardie C. 205. Anotating 
expressions of opinions and emotions in language. 
Language Resources and Evaluation. 
Wilson T., Wiebe J., Hwa R. 204. Just how mad are 
you? Finding strong and weak opinion clauses. In: 
Procedings of AAI. 
Wiebe J., Wilson T. and Cardie C. 205. “Anotation 
Expresions of Opinions and Emotions in Lan-
guage. Language Resources and Evaluation.  
Wiebe J., Rilof E. 205. Creating Subjective and 
Objective Sentence Clasifiers from Unanotated 
Texts. In Procedings of the 6th International Con-
ference on Computational Linguistics and Inteli-
gent Text Procesing (CICLing). 
 
 
 
 
10

