Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, pages 12–19,
Columbus, Ohio, USA June 2008. c©2008 Association for Computational Linguistics
A Bayesian model of natural language phonology:
generating alternations from underlying forms
David Ellis
de@cs.brown.edu
Brown University
Providence, RI 02912
Abstract
A stochastic approach to learning phonology.
The model presented captures 7-15% more
phonologically plausible underlying forms
than a simple majority solution, because it
prefers  pure alternations. It could be use-
ful in cases where an approximate solution is
needed, or as a seed for more complex mod-
els. A similar process could be involved in
some stages of child language acquisition; in
particular, early learning of phonotactics.
1 Introduction
Sound changes in natural language, such as stem
variation in in ected forms, can be described as
phonological processes. These are governed by a
constraint hierarchy as in Optimality Theory (OT),
or by a set of ordered rules. Both rely on a sin-
gle lexical representation of each morpheme (i.e., its
underlying form), and context-sensitive transforma-
tions to surface forms. Phonological changes often
affect segments near morpheme boundaries, but can
also apply over an entire prosodic word, as in vowel
harmony.
It does not seem straightforward to incorporate
context into a Bayesian model of phonology, al-
though a clever solution may yet be found. A
standard way of incorporating conditioning envi-
ronments is to treat them as factors in a Gibbs
model (Liang and Klein, 2007), but such models
require an explicit calculation of the partition func-
tion. Unless the rule contexts possess some kind of
locality, we don’t know how to compute this par-
tition function ef ciently. Some context could be
captured by generating underlying phonemes from
an n-gram model, or by annotating surface forms
with neighborhood features. However, the effects of
autosegmental phonology and other long-range de-
pendencies (like vowel harmony) cannot be easily
Bayesianized.
1.1 Related
Work
In the last decade,  nite-state approaches to phonol-
ogy (Gildea and Jurafsky, 1996; Beesley and Kart-
tunen, 2000) have effectively brought theoretical lin-
guistic work on rewrite rules into the computational
realm. A  nite-state approximation of optimality
theory (Karttunen, 1998) was later re ned into a
compact treatment of gradient constraints (Gerde-
mann and van Noord, 2000).
Recent work on Bayesian models of morpholog-
ical segmentation (Johnson et al., 2007) could be
combined with phonological rule induction (Gold-
water and Johnson, 2004) in a variety of ways,
some of which will be explored in our discussion
of future work. Also, the Hierarchical Bayes Com-
piler (Daume III, 2007) could be used to generate a
model similar to the one presented here, but less con-
strained1 which makes correspondingly more ran-
dom, less accurate predictions.
1.2 Dataset
As we describe the model and its implementation in
this and subsequent sections, we will refer to a sam-
1Recent updates to HBC, inspired by discussions with the
author, have addressed some of these limitations.
12
ple dataset (in Figure 1), consisting of a paradigm2
of verb stems and person/number suf xes. The
head of each row or column is an /underlying/ form,
which in 3rd person singular is a phonologically null
segment (represented as /ł/). In [surface] forms, the
realization of each morpheme is affected by phono-
logical processes. For example, in the combination
of /tietcurrency1a/ + /vat/, the result is [tietcurrency1a+vcurrency1at], where the
3rd person plural /a/ becomes [currency1a] due to vowel har-
mony.
1.3 Bayesian
Approach
As a baseline model, we select the most frequently
occurring allophone as the underlying form. Our
goal is to outperform this baseline using a Bayesian
model. In other words, what patterns in phonologi-
cal processes can be inferred with such a statistical
model? This simple framework begins learning with
the assumption that the underlying forms are faithful
to the surface (i.e., without considering markedness
or phonotactics).
We model the generation of surface forms from
underlying ones on the segmental (character) level.
Input is an in ectional paradigm, with tokens of the
form stem+suffix. Morphology is limited to a
single suf x (no agglutination), and is already iden-
ti ed. Each character of an underlying stem or suf-
 x (ui) generates surface characters (sij) in an entire
row or column of the input.
To capture the phonology of a variety of lan-
guages with a single model, we need constraints
from linguistically plausible priors (universal gram-
mar). We prefer that underlying characters be pre-
served in surface forms, especially when there is no
alternation. It is also reasonable that there be fewer
underlying forms (phonemes) than surface forms
(phones, phonetic inventory), to account for allo-
phones. We expect to be able to capture a signi -
cant subset of phonological processes using a simple
model (only faithfulness constraints).
1.4 Pure
Generators
Our model has an advantage over the baseline in its
preference for  purity in underlying forms. Each
underlying segment should generate as few distinct
2The paradigm format lends itself to analysis of word types,
but if supplemented with surface counts, can also handle tokens.
surface segments as possible: if it generates non-
alternating (identical) segments, it will be less likely
to generate an alternation in addition. This means
that when two segments alternate, the underlying
form should be the one that appears less frequently
in other contexts, irrespective of the majority within
the alternation.
In the  rst stem of our Finnish verb conjugation
(Figure 1), we see a [t,d] alternation (a case of con-
sonant gradation), as well as unalternating [t]. If we
isolate three of the surface forms where /tietcurrency1a/ is in-
 ected (1st person singular, and 3rd person singular
and plural), and consider only the dental segments in
the stem of each, we have two underlying segments.
Here, we use question marks to indicate unknown
underlying segments.
/??/ [dt] [tt] [tt]
In this subset of the data, the reasonable candidate
underlying forms are /t/ and /d/. These two compete
to explain the observed data (surface forms). The na-
ture of the prior probability distribution determines
whether the majority is hypothesized for each under-
lying form, so /t/ produces both alternating and unal-
ternating surface segments, or /d/ is hypothesized as
the source of the alternation (and /t/ remains  pure ).
In a Bayesian setting, we impose a sparse prior over
underlying forms conditioned on the surface forms
they generate.
If u2 is hypothesized to be /t/, the posterior prob-
ability of u1 being /t/ goes down:
P(u1 = /t/|u2 = /t/) < P(u1 = /t/)
The probability of u1 being the competitor, /d/, cor-
respondingly increases:
P(u1 = /d/|u2 = /t/) > P(u1 = /d/)
Even though the majority in this case would be /t/,
the favored candidate for the alternating form was
/d/. This happened because of how we de ned the
model’s prior, in combination with the evidence that
/t/ (assigned to u2) generated the sequence of [t]. So
selection bias prefers /d/ as the source of an ambigu-
ous segment, leaving /t/ to always generate itself.
A similar effect can occur if there are both unal-
ternating [t]’s and [d]’s on the surface, in addition to
the [t,d] alternation. The candidate (/t/ or /d/) that is
13
a97a97a97
a97a97a97StemSuf x /n/ (1s) /t/ (2s) /ł/ (3s) /mme/ (1p) /tte/ (2p) /vat/ (3p)
/tietcurrency1a/ [tiedcurrency1a+n] [tiedcurrency1a+t] [tietcurrency1a+currency1a] [tiedcurrency1a+mme] [tiedcurrency1a+tte] [tietcurrency1a+vcurrency1at]
/aiko/ [aiło+n] [aiło+t] [aiko+o] [aiło+mme] [aiło+tte] [aiko+vat]
/luke/ [lułe+n] [lułe+t] [luke+e] [lułe+mme] [lułe+tte] [luke+vat]
/puhu/ [puhu+n] [puhu+t] [puhu+u] [puhu+mme] [puhu+tte] [puhu+vat]
/saa/ [saa+n] [saa+t] [saa+ł] [saa+mme] [saa+tte] [saa+vat]
/tule/ [tule+n] [tule+t] [tule+e] [tule+mme] [tule+tte] [tule+vat]
/pelkcurrency1acurrency1a/ [pelkcurrency1acurrency1a+n] [pelkcurrency1acurrency1a+t] [pelkcurrency1acurrency1a+ł] [pelkcurrency1acurrency1a+mme] [pelkcurrency1acurrency1a+tte] [pelkcurrency1acurrency1a+vcurrency1at]
Figure 1: Sample dataset (constructed by hand): Finnish verbs, with in ection for person and number.
generating fewer unalternating segments is preferred
to explain the alternation. For example, if there were
1000 cases of [t], 500 [d] and 500 [t,d], we would ex-
pect the following hypotheses: /t/ → [t], /d/ → [d]
and /d/ → [t,d]. This is because one of the two
candidates must be responsible for both unalternat-
ing and alternating segments, but we prefer to have
as much  ‘purity as possible, to minimize ambigu-
ity.
With this solution, we still have 1000 pure /t/ →
[t], and only the 500 /d/ → [d] are now indistinct
from /d/ → [t,d]. If we had selected /t/ as the
source of the alternation, there would be only 500
remaining  pure (/d/) segments, and 1500 ambigu-
ous /t/. Our Bayesian model should prefer the less
ambiguous ( purer ) solution, given an appropriate
prior.
2 Model
We will use boldface to indicate vectors, and sub-
scripts to identify an element from a vector or ma-
trix. The variable N(u) is a vector of observed
counts with the current underlying form hypothe-
ses. The notation we use for a vector u with one
element i removed is u−i, so we can exclude the
counts associated with a particular underlying form
by indicating that in the parenthesized variable (i.e.,
N(u−4) is all the counts except those associated with
the fourth underlying form). Ni(u) is the number of
times character i is used as an underlying form, and
Nij(u) is the number of times character i generated
surface character j.
The priors over surface s and underlying u seg-
ments in Figure 2 are captured by Dirichlet priors
α and β, which generate the multinomial distribu-
tions θ and φ, respectively (see Figure 3). The
prior over underlying form encourages sparse solu-
tions, so βu < 1 for all u. The prior over surface
form given underlying encourages identity mapping,
/x/ → [x], so αxx > 1, and discourages different
segments, /x/ → [y], so αxy < 1 for all x negationslash= y.
nc
θ
α
nu
mnu
s
u
φ
β
Figure 2: Bayesian network:  and  are vectors of hy-
perparameters, and  i (for i 2 f1; : : :; ncg) and  are
distributions. u is a vector of underlying forms, generated
from  , and si (for i 2 nu) is a set of observed surface
forms generated from the hidden variable ui according to
 i
Phones and phonemes are drawn from a set of
characters (e.g., IPA, unicode) C used to represent
them. φi is the probability of a character (Ci for
i ∈ nc) being an underlying form, irrespective of
current alignments or its position in the paradigm.
θij is the conditional probability of a surface char-
14
θc | α ∼ DIR(α),c = 1,... ,nc
φ | β ∼ DIR(β)
ui | φi ∼ MULTI(φi),i = 1,... ,nu
sij | ui, θui ∼ MULTI(θui),i = 1,... ,nu,
j = 1,... ,mi
Figure 3: Model parameters: nc is # different segments,
nu is # underlying segments
acter (skn = Cj for j ∈ nc, n ∈ mk) given the
underlying character it is generated from (uk = Ci
for i ∈ nc, k ∈ nu), which is determined by its po-
sition in the paradigm.
In our Finnish example (Figure 1), if k = 1, we
are looking at the  rst underlying character, which
is /t/ (from /tietcurrency1a/), so assuming our character set is
the Finnish alphabet, of which ‘t’ is the 20th char-
acter, u1 = C20 = t. It generates the  rst character
of each in ected form (1st, 2nd, 3rd person, singu-
lar and plural) of that stem, so m1 = 6, and since
there is no alternation s1n = t (for n ∈ {1,... ,6}).
Given the phonologically plausible (gold) underly-
ing forms, the probability of /t/ is φ20 = 7/41.
On the other hand, k = 33 identi es the 3rd per-
son singular /ł/, which in ects each of the seven
stems, so m33 = 7. Since we need our alpha-
bet to identify a null character, we’ll give it in-
dex zero (i.e., u33 = C0 = ø). For each of the
(underlying, surface) alignments in this alternation
(caused by vowel gemination), we can identify the
probability in θ. For 3rd person singular [tietcurrency1a+currency1a],
where s33,1 = C28 = currency1a, the conditional probability
θ0,28 = 1/7.
The prior hyperparameters can be understood as
follows. As βi gets smaller, an underlying form uk
is less likely to be Ci. As αij gets smaller, an un-
derlying uk = Ci is less likely to generate a surface
segment skn = Cj ∀n ∈ mk. In our experiments,
we will vary αi=j (prior over identity map from un-
derlying to surface) and αinegationslash=j.
Our implementation of this model uses Gibbs
sampling (c.f., (Bishop, 2006), pp 542-8), an algo-
rithm that produces samples from the posterior dis-
tribution. Each sample is an assignment of the hid-
den variables, u (i.e., a set of hypothesized underly-
ing forms). Our sampler initializes u from a uniform
distribution over segments in the training data, and
resamples underlying forms in a  xed order, as in-
put in the paradigm. Rather than reestimate θ and
φ at each iteration before sampling from u, we can
marginalize these intermediate probability distribu-
tions in order to ease implementation and speed con-
vergence.
Our search procedure tries to sample from the
posterior probability, according to Bayes’ rule.
posterior ∝ likelihood ∗ prior
P(u, s|β, α) ∝ P(u|β)P(s, u|α)
Each of these probabilities is drawn from a Dirichlet
distribution, which is de ned in terms of the multi-
variate Beta function, C. The prior β added to un-
derlying counts N(u) forms the posterior Dirichlet
corresponding to P(u|β). In P(s|u, α), each αi
vector is supplemented by the observed counts of
(underlying, surface) pairs N(si).
P(u, s|β, α) = C(β + N(u))C(β)
ncproductdisplay
c=1
C(αc + summationtexti:ui=c N(si))
C(α)
The collapsed update procedure consists of re-
sampling each underlying form, u, incorporating the
prior hyperparameters α, β and counts N over the
rest of the dataset. The relevant counts for a can-
didate k being the underlying form ui are Nk(u−i)
and Nksij(u−i) for j ∈ mi. P(ui = k|u−i, α, β) is
proportional to the probability of generating ui = k,
given the other u−i and all sij (for j ∈ mi), given
s−i and u−i.
P(ui = c|u−i, α, β) ∝ Nc(u−i) + βcn−1 + β
•
C(α + summationtextiprimenegationslash=i:u
iprime=c
N(sprimei) + N(si))
C(α + summationtextiprimenegationslash=i:u
iprime=c
N(sprimei))
Suppose we were updating this sampler running
on the Finnish verb in ections. If we had all seg-
ments as in Figure 1, but wanted to resample u31 (1st
person singular /n/), we would consider the counts
N excluding that form (i.e., under u−31). The prior
for /n/, β14, is  xed, and there are no other occur-
rences, so N14(u−31) = 0. Another potential un-
derlying form, like /t/, would have higher uncondi-
tioned posterior probability, because of the counts
15
(7, in this case) added to its prior from β. Then, we
have to multiply by the probability of each generated
surface segment (all are [n], so 7 ∗ P([n]|c, α) for a
given hypothesis u31 = c).
We select a given character c ∈ C for u31 from a
distribution at random. Depending on the prior, /n/
will be the most likely choice, but other values are
still possible with smaller probability. The counts
used for the next resampling, N(u−31), are affected
by this choice, because the new identity of u31 has
contributed to the posterior distribution. After un-
bounded iterations, Gibbs sampling is guaranteed to
converge and produce samples from the true poste-
rior (Geman and Geman, 1984).
3 Evaluation
This model provides a language agnostic solution to
a subset of phonological problems. We will  rst
examine performance on the sample Finnish data
(from Figure 1), and then look more closely at the is-
sue of convergence. Finally, we present results from
larger corpora 3.
3.1 Finnish
Output from a trial run on Finnish verbs (from Fig-
ure 1) follows, with hyperparameters αij{100 ⇐⇒
i = j,0.05 ⇐⇒ i negationslash= j} and βi = {0.1}.
In the paradigm (a sample after 1000 iterations),
each [sur+face] form is followed by its hypothesized
/under/ + /lying/ morphemes.
[tiedcurrency1a+n] : /tiedcurrency1a/ + /n/
[tiedcurrency1a+t] : /tiedcurrency1a/ + /t/
[tietcurrency1a+currency1a] : /tiedcurrency1a/ + /currency1a/
[tiedcurrency1a+mme] : /tiedcurrency1a/ + /mme/
[tiedcurrency1a+tte] : /tiedcurrency1a/ + /tte/
[tietcurrency1a+vcurrency1at] : /tiedcurrency1a/ + /vcurrency1at/
[aiło+n] : /aiło/ + /n/
...
[pelkcurrency1acurrency1a+vcurrency1at] : /pelkcurrency1acurrency1a/ + /vat/
With strong enough priors (faithfulness con-
straints), our sampler often selects the most com-
mon surface form aligned with an underlying seg-
ment. Although [vat] is more common than [vcurrency1at],
we choose the latter as the purer underlying form.
So /a/ is always [a], but /currency1a/ can be either [currency1a] or [a].
32.8 million word types from Morphochallenge2007 (Ku-
rimo et al., 2007)
3.2 Convergence
Testing convergence, we run again on the sample
data (Figure 1), using αij = 0.1 when i negationslash= j and
10 when i = j and β = 0.1, starting from different
initializations, we get the same solution.
0 10 20 30 40 50 60 70 80 90 1002.97
2.98
2.99
3
3.01
3.02
3.03
3.04
3.05 x 10
6
Iteration
−log Likelihood
Figure 4: Posterior likelihood at each of the  rst 100 iter-
ations, from 4 runs (with different random seeds) on 10%
of the Morphochallenge dataset ( inegationslash=j = 0:001;  i=j =
100;  = 0:1), indicating convergence within the  rst 15
iterations.
To con rm that the sampler has converged, we
output and plot trace statistics at each iteration, in-
cluding marginal probability, log likelihood, and
changes in underlying forms (i.e., variables resam-
pled). If the sampler has converged, there should no
longer be a trend (consistent slope) in any of these
statistics (as in Figure 4).
Examining the posterior probability of each se-
lected underlying form reveals interesting patterns
that also help explain the variation. In the above run,
the ambiguous segments (with surface alternations)
were drawn from the distributions (with improbable
segments elided) in Figure 5.
We expect this model to maximize the probabil-
ity of either the  majority solution or a solution
demonstrating selection bias. We compare likeli-
hood of the posterior sample with that of a  phono-
logically plausible solution (in which underlying
forms are determined by referring to formal lin-
guistic accounts of phonological derivation) and a
 majority solution (see Figure 6 for a log-log plot,
where lower is more likely).
The posterior sample has optimal likelihood with
each parameter setting, as expected. The majority
parse is selected with αinegationslash=j = 0.5 With lower val-
ues of αinegationslash=j, the  phonologically plausible parse is
16
u4=/d/ s4=[d,d,t,d,d,t]
P(ui = c)  
d 0.99968
t 0.00014
u8=/k/ s8=[ł,ł,k,ł,ł,k]
(same behavior as u12)
P(ui = c)  
ł 0.642
k 0.124
u33=/e/ s33=[currency1a,o,e,u,ł,e,ł]
P(ui = c)  
currency1a,o,u 0.0029
ł 0.215
a 0.0003
e 0.297
Figure 5: Resampling probabilities for alternations, after
1000 iterations.
10−2 10−1 100
104.24
104.25
104.26
alpha
−log likelihood
 
 
posterior sample
majority solution
phonologically plausible
Figure 6: Parse likelihood
more likely than the majority. However, the sam-
pler does not converge to this solution, because in
this [t,d] alternation, the  phonologically plausible 
solution identi es /t/, but neither selection bias nor
majority rules would lead to that with the given data.
3.3 Morphologically
segmented corpora
In our search for appropriate data for additional,
larger-scale experiments, we found several vi-
able alternatives. The correct morphological seg-
mentations for Finnish data used in Morphochal-
lenge2007 (Kurimo et al., 2007) provide a rich and
varied set of words, and are readily analyzable by
our sampler. Rather than associating each surface
form with a position in the paradigm, we use the an-
Majority Bayesian
types 50.84 69.53
tokens 65.23 72.11
Figure 7: Accuracy of underlying segment hypotheses.
notated morphemes.
For example, the word ajavalle is listed in the cor-
pus as follows:
ajavalle aja:ajaajV va:PCP1 lle:ALL The
word is segmented into a verb stem, ‘aja’ (drive),
a present participle marker ‘va’, and the allative suf-
 x ( for ). Each surface realization of a given mor-
pheme is identi ed by the same tag (e.g., PCP1).
However, in this corpus, insertion and deletion are
not explicitly marked (as they were in the paradigm,
by ł). Rather than introduce another component
to determine which segments in the form were
dropped, we ignore these cases.
The sampling algorithm proceeds as described in
section 2. To run on tokens (as opposed to types), we
incorporate another input  le that contains counts
from the original text (ajavalle appeared 8 times).
The counts of each morpheme’s surface forms then
re ect the number of times that form appeared in any
word in the corpus.
3.3.1 Type
or Token
In Finnish verb conjugation, 3rd person (esp. sin-
gular) forms have high frequency and tend to be un-
marked (i.e., closer to underlying). In types, un-
marked is a minority (one third), but incorporat-
ing token frequency shifts that balance, bene ting
the  majority learner. Among noun in ections, un-
marked has higher frequency in speech, but marked
tokens may still dominate in text. We might expect
that it is easier to learn from tokens than types, in
part because more data is often helpful.
Testing on half of the Morphochallenge 2007
Finnish data (1M word types, 5M morph types,
17.5M word tokens, 48M morph tokens), we ran
both our Bayesian model and a majority solver on
the morphological analyses, and compared against
phonologically plausible (gold) underlying forms.
Results are reported in Figure 7.
The Bayesian estimate consistently outperformed
the majority solution, and cases where the two differ
could often be ascribed to the preference for  pure 
17
analyses.
4 Conclusion
We have described a model where surface forms
are generated from underlying representations seg-
ment by segment. Taking this approach allowed us
to investigate the properties of a Bayesian statistical
learner, and how these can be useful in the context
of sound systems, a basic component of language.
Experiments with our implementation of a collapsed
sampler have produced results largely con rming
our hypotheses.
Without context, we can often learn about 60 to 80
percent of the mapping from underlying phonemes
to surface phones. Especially with lower values of
αinegationslash=j, closer to 0, our model does prefer pure alter-
nations. Gibbs sampling tends to select the major-
ity underlying form, particularly with αinegationslash=j relatively
high, closer to 1. So, a sparser prior leads us further
from the baseline, and often closer to a phonologi-
cally plausible solution.
4.1 Directions
In future research, we hope to integrate morpholog-
ical analysis into this sort of a treatment of phonol-
ogy. This is a natural approach for children learn-
ing their  rst language. They intuitively discover
phonotactics, and how it affects the prosodic shape
of each word, as they learn meaningful units and
compose them together. It is clear that many lay-
ers of linguistic information interact in the early
stages of child language acquisition (Demuth and
Ellis, 2005 in press), so they should also interact
in our models. As discussed above, the present
model should be applicable to analysis of language-
learners’ speech errors, and this connection should
be explored in greater depth.
It might be interesting to predispose the sampler
to select underlying forms from open syllables. That
is, set α to increase the probability of matching
one of the surface segments if its context (feature
annotations) includes a vocalic segment or a word
boundary immediately following. The probability
of phonological processes like assimilation could be
similarly modeled, with the prior higher for choos-
ing a segment that appears on the surface in a con-
trastive context (where it shares few features with
neighboring segments).
If we de ne a MaxEnt distribution over Optimal-
ity Theoretic constraints, we might use that to in-
form our selection of underlying forms. In (Gold-
water and Johnson, 2003), the learning algorithm
was given a set of candidate surface forms asso-
ciated with an underlying form, and tried to opti-
mize the constraint weights. In addition to the con-
straint weights, we must also optimize the underly-
ing form, since our goal is to take as input only ob-
servable data. Sampling from this type of complex
distribution is quite dif cult, but some approaches
(e.g., (Murray et al., 2006)) may help reduce the in-
tractability.
References
Kenneth R. Beesley and Lauri Karttunen. 2000. Finite-
state non-concatenative morphotactics. In Lauri Kart-
tunen, Jason Eisner, and Alain Th·eriault, editors, SIG-
PHON2000, August 6 2000. Proceedings of the Fifth
Workshop of the ACL Special Interest Group in Com-
putational Phonology., pages 1 12.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning (Information Science and Statis-
tics). Springer, August.
Hal Daume III. 2007. Hbc: Hierarchical bayes compiler.
Katherine Demuth and David Ellis, 2005 (in press). Re-
visiting the acquisition of Sesotho noun class pre xes.
Lawrence Erlbaum.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, gibbs distributions, and the bayesian restora-
tion of images. IEEE Trans. Pattern Anal. Machine
Intell., 6(6):721 741, Nov.
Dale Gerdemann and Gertjan van Noord. 2000. Approx-
imation and exactness in  nite state optimality theory.
Daniel Gildea and Daniel Jurafsky. 1996. Learning bias
and phonological-rule induction. Computational Lin-
guistics, 22(4):497 530.
Sharon Goldwater and Mark Johnson. 2003. Learning ot
constraint rankings using a maximum entropy model.
Sharon Goldwater and Mark Johnson. 2004. Priors in
Bayesian learning of phonological rules. In Proceed-
ings of the Seventh Meeting of the ACL Special Inter-
est Group in Computational Phonology, pages 35 42,
Barcelona, Spain, July. Association for Computational
Linguistics.
Mark Johnson, Thomas L. Grif ths, and Sharon Goldwa-
ter. 2007. Adaptor grammars: A framework for spec-
ifying compositional nonparametric Bayesian models.
In B. Schcurrency1olkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems 19,
pages 641 648. MIT Press, Cambridge, MA.
18
Lauri Karttunen. 1998. The proper treatment of optimal-
ity in computational phonology. In Lauri Karttunen,
editor, FSMNLP’98: International Workshop on Fi-
nite State Methods in Natural Language Processing,
pages 1 12. Association for Computational Linguis-
tics, Somerset, New Jersey.
Mikko Kurimo, Mathias Creutz, and Ville Turunen.
2007. Overview of morpho challenge in clef 2007.
In Working Notes for the CLEF 2007 Workshop, Bu-
dapest, Hungary.
Percy Liang and Dan Klein. 2007. Tutorial 1: Bayesian
nonparametric structured models, June.
Iain Murray, Zoubin Ghahramani, and David MacKay.
2006. MCMC for doubly-intractable distributions. In
UAI. AUAI Press.
19

