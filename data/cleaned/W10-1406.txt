Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 49–57,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics
FactorsAffectingtheAccuracyofKoreanParsing
TagyoungChung, MattPost and DanielGildea
DepartmentofComputerScience
UniversityofRochester
Rochester,NY14627
Abstract
We investigate parsing accuracy on the Ko-
rean Treebank 2.0 with a number of different
grammars. Comparisons among these gram-
marsandtotheirEnglishcounterpartssuggest
different aspects of Korean that contribute to
parsingdifficulty. Ourresultsindicatethatthe
coarseness of the Treebank’s nonterminal set
is a even greater problem than in the English
Treebank. We also find that Korean’s rela-
tivelyfreewordorderdoesnotimpactparsing
results as much as one might expect, but in
facttheprevalenceofzeropronounsaccounts
for a large portion of the difference between
KoreanandEnglishparsingscores.
1 Introduction
Korean is a head-final, agglutinative, and mor-
phologically productive language. The language
presents multiple challenges for syntactic pars-
ing. Like some other head-final languages such
as German, Japanese, and Hindi, Korean exhibits
long-distance scrambling (Rambow and Lee, 1994;
Kallmeyer and Yoon, 2004). Compound nouns are
formed freely (Park et al., 2004), and verbs have
wellover400paradigmaticendings(Martin,1992).
Korean Treebank 2.0 (LDC2006T09) (Han and
Ryu,2005)isasubsetofaKoreannewswirecorpus
(LDC2000T45) annotated with morphological and
syntactic information. The corpus contains roughly
5K sentences, 132K words, and 14K unique mor-
phemes. The syntactic bracketing rules are mostly
the same as the previous version of the treebank
(Han et al., 2001) and the phrase structure annota-
tion schemes used are very similar to the ones used
in Penn English treebank. The Korean Treebank is
constructedovertextthathasbeenmorphologically
analyzed; not only is the text tokenized into mor-
phemes,butallallomorphsareneutralized.
Toourknowledge,therehavebeenonlyafewpa-
persfocusingonsyntacticparsingofKorean. Herm-
jakob (2000) implemented a shift-reduce parser for
Korean trained on very limited (1K sentences) data,
and Sarkar and Han (2002) used an earlier version
of the Treebank to train a lexicalized tree adjoining
grammar. In this paper, we conduct a range of ex-
perimentsusingtheKoreanTreebank2.0(hereafter,
KTB)asourtrainingdataandprovideanalysesthat
revealinsightsintoparsingmorphologicallyrichlan-
guages like Korean. We try to provide comparisons
withEnglishparsingusingparserstrainedonasimi-
laramountofdatawhereverapplicable.
2 DifficultiesparsingKorean
ThereareseveralchallengesinparsingKoreancom-
paredtolanguageslikeEnglish. Attherootofmany
of these challenges is the fact that it is highly in-
flected and morphologically productive. Effective
morphological segmentation is essential to learning
grammar rules that can generalize beyond the train-
ingdatabylimitingthenumberofout-of-vocabulary
words. Fortunately,therearegoodtechniquesfordo-
ingso. ThesentencesinKTBhavebeensegmented
intobasicmorphologicalunits.
Second, Korean is a pro-drop language: subjects
andobjectsaredroppedwherevertheyarepragmati-
callyinferable,whichisoftenpossiblegivenitsrich
morphology. Zero pronouns are a remarkably fre-
quentphenomenoningeneral(Han,2006),occuring
49
an average of 1.8 times per sentence in the KTB.
The standard approach in parsing English is to ig-
noreNULLelementsentirelybyremovingthem(and
recursively removing unary parents of empty nodes
inabottom-upfashion). Thisislessofaproblemin
Englishbecausetheseemptynodesaremostlytrace
elements that denote constituent movement. In the
KTB, these elements are removed altogether and a
crucial cue to grammatical inference is often lost.
Later we will show the profound effect this has on
parsingaccuracy.
Third, word order in Korean is relatively free.
This is also partly due to the richer morphology,
since morphemes (rather than word order) are used
to denote semantic roles of phrases. Consider the
followingexample:
㩕㨋 uni3714uni36FDuni3982uni3433 㫾㧺 uni3A73uni397Duni35B0 .
John-NOM Mary-DAT book-ACC give-PAST .
In the example, any permutation of the first three
words produces a perfectly acceptable sentence.
This freedom of word order could potentially result
in a large number of rules, which could complicate
analysis with new ambiguities. However, formal
written Korean generally conforms to a canonical
wordorder(SOV).
3 Initialexperiments
There has been some work on Korean morphologi-
cal analysis showing that common statistical meth-
ods such as maximum entropy modeling and condi-
tional random fields perform quite well (Lee et al.,
2000;SarkarandHan,2002;HanandPalmer,2004;
LeeandRim,2005). Mostclaimaccuracyrateover
95%. Inlightofthis,wefocusontheparsingpartof
the problem utilizing morphology analysis already
presentinthedata.
3.1 Setup
For our experiments we used all 5,010 sentences in
theKoreanTreebank(KTB),whicharealreadyseg-
mented. Duetothesmallsizeofthecorpus,weused
ten-fold cross validation for all of our experiments,
unlessotherwisenoted. Sentenceswereassignedto
folds in blocks of one (i.e., fold 1 contained sen-
tences1,11,21,andsoon.). Withineachfold,80%
of the data was assigned to training, 10% to devel-
opment,and10%totesting. Eachfold’svocabulary
model F1 F1≤40 types tokens
Korean 52.78 56.55 6.6K 194K
English(§02–03) 71.06 72.26 5.5K 96K
English(§02–04) 72.20 73.29 7.5K 147K
English(§02–21) 71.61 72.74 23K 950K
Table 1: Parser scores for Treebank PCFGs in Korean
andEnglish. ForEnglish,wevarythesizeofthetraining
data to provide a better point of comparison against Ko-
rean. Types and tokens denote vocabulary sizes (which
forKoreanisthemeanoverthefolds).
wassettoallwordsoccurringmorethanonceinits
trainingdata,withahandfulofcountonetokensre-
placing unknown words based on properties of the
word’s surface form (all Korean words were placed
in a single bin, and English words were binned fol-
lowing the rules of Petrov et al. (2006)). We report
scoresonthedevelopmentset.
We report parser accuracy scores using the stan-
dardF1metric, whichbalancesprecisionandrecall
of the labeled constituents recovered by the parser:
2PR/(P + R). Throughout the paper, all evalua-
tion occurs against gold standard trees that contain
no NULL elements or nonterminal function tags or
annotations, which in some cases requires the re-
moval of those elements from parse trees output by
theparser.
3.2 Treebankgrammars
We begin by presenting in Table 1 scores for the
standard Treebank grammar, obtained by reading a
standardcontext-freegrammarfromthetreesinthe
training data and setting rule probabilities to rela-
tive frequency (Charniak, 1996). For these initial
experiments,wefollowstandardpracticeinEnglish
parsingandremoveall(a)nonterminalfunctiontags
and (b) NULL elements from the parse trees before
learningthegrammar. Forcomparisonpurposes,we
present scores from parsing the Wall Street Journal
portion of the English Penn Treebank (PTB), using
both the standard training set and subsets of it cho-
sen to be similar in size to the KTB. All English
scoresaretestedonsection22.
There are two interesting results in this table.
First, Korean parsing accuracy is much lower than
English parsing accuracy, and second, the accuracy
difference does not appear to be due to a difference
in the size of the training data, since reducing the
50
size of the English training data did not affect accu-
racyscoresverymuch.
Before attempting to explain this empirically, we
note that Rehbein and van Genabith (2007) demon-
stratethattheF1metricisbiasedtowardsparsetrees
with a high ratio of nonterminals to terminals, be-
cause mistakes made by the parser have a smaller
effect on the overall evaluation score.1 They rec-
ommend that F1 not be used for comparing parsing
accuracy across different annotation schemes. The
nonterminal to terminal ratio in the KTB and PTB
are 0.40 and 0.45, respectively. It is a good idea to
keepthisbiasinmind,butwebelievethatthissmall
ratio difference is unlikely to account for the huge
gapinscoresdisplayedinTable1.
The gap in parsing accuracy is unsurprising in
lightofthebasicknowndifficultiesparsingKorean,
summarizedearlierinthepaper. Hereweobservea
numberoffeaturesoftheKTBthatcontributetothis
difficulty.
Sentence length On average, KTB sentences are
much longer than PTB sentences (23 words versus
48 words, respectively). Sentence-level F1 is in-
versely correlated with sentence length, and the rel-
ativelylargerdropinF1scoregoingfromcolumn3
to 2 in Table 1 is partially accounted for by the fact
thatcolumn3represents33%oftheKTBsentences,
but92%oftheEnglishsentences.
Flat annotation scheme The KTB makes rela-
tivelyfrequentuseofveryflatandambiguousrules.
Forexample,considertheextremecasesofruleam-
biguity in which the lefthand side nonterminal is
present three or more times on its righthand side.
There are only three instances of such “triple+-
recursive”NPsamongthe∼40Ktreesinthetraining
portionofthePTB,eachoccurringonlyonce.
NP→NPNPNP,CCNP
NP→NPNPNPCCNP
NP→NPNPNPNP.
The KTB is an eighth of the size of this, but has
fifteen instances of such NPs (listed here with their
frequencies):
1Wethankoneofouranonymousreviewersforbringingthis
toourattention.
NP→NPNPNPNP(6)
NP→NPNPNPNPNP(3)
NP→NPNPNPNPNPNP(2)
NP→NPNPNPNPNPNPNP(2)
NP→SLQNPNPNPSRQPAD(1)
NP→SLQNPNPNPNPSRQPAN(1)
Similarrulesarecommonforothernonterminalsas
well. Generally,flatterrulesareeasiertoparsewith
because they contribute to parse trees with fewer
nodes (and thus fewer independent decision points).
However, the presence of a single nonterminal on
boththeleftandrighthandsideofarulemeansthat
the annotation scheme is failing to capture distribu-
tionaldifferenceswhichmustbepresent.
Nonterminalgranularity Thisbringsustoafinal
pointaboutthegranularityofthenonterminalsinthe
KTB. After removing function tags, there are only
43 nonterminal symbols in the KTB (33 of them
preterminals), versus 72 English nonterminals (44
of them preterminals). Nonterminal granularity is
awell-studiedprobleminEnglishparsing,andthere
is a long, successful history of automatically refin-
ing English nonterminals to discover distributional
differences. In light of this success, we speculate
that the disparity in parsing performance might be
explained by this disparity in the number of nonter-
minals. Inthenextsection,weprovideevidencethat
thisisindeedthecase.
4 Nonterminalgranularity
There are many ways to refine the set of nontermi-
nals in a Treebank. A simple approach suggested
by Johnson (1998) is to simply annotate each node
with its parent’s label. The effect of this is to re-
fine the distribution of each nonterminal over se-
quences of children according to its position in the
sentence;forexample,aVPbeneathanSBARnode
willhaveadifferentdistributionoverchildrenthana
VPbeneathanSnode. Thissimpletechniquealone
produces a large improvement in English Treebank
parsing. Klein and Manning (2003) expanded this
ideawithaseriesofexperimentswhereintheymanu-
allyrefinednonterminalstodifferentdegrees,which
resulted in parsing accuracy rivaling that of bilexi-
calized parsing models of the time. More recently,
Petrov et al. (2006) refined techniques originally
proposed by Matsuzaki et al. (2005) and Prescher
51
SBJ subjectwithnominativecasemarker
OBJ complementwithaccusativecasemarker
COMP complementwithadverbialpostposition
ADV NPthatfunctionasadverbialphrase
VOC nounwithvocativecasemaker
LV NPcoupledwith“light”verbconstruction
Table2: FunctiontagsintheKoreantreebank
model F1 F1≤40
Korean
coarse 52.78 56.55
w/functiontags 56.18 60.21
English(small)
coarse 72.20 73.29
w/functiontags 70.50 71.78
English(standard)
coarse 71.61 72.74
w/functiontags 72.82 74.05
Table 3: Parser scores for Treebank PCFGs in Korean
and English with and without function tags. The small
Englishresultswereproducedbytrainingon§02–04.
(2005)forautomaticallylearninglatentannotations,
resultinginstateoftheartparsingperformancewith
cubic-timeparsingalgorithms.
We begin this section by conducting some sim-
pleexperimentswiththeexistingfunctiontags,and
thenapplythelatentannotationlearningprocedures
ofPetrovetal.(2006)totheKTB.
4.1 Functiontags
The KTB has function tags that mark grammatical
functions of NP and S nodes (Han et al., 2001),
whichwelistalloftheminTable2. Thesefunction
tags are principally grammatical markers. As men-
tioned above, the parsing scores for both English
andKoreanpresentedinTable1wereproducedwith
grammars stripped of their function tags. This is
standardpracticeinEnglish,wheretheexistingtags
are known not to help very much. Table 3 presents
resultsofparsingwithgrammarswithnonterminals
that retain these function tags (we include results
from Section 3 for comparison). Note that evalua-
tion is done against the unannotated gold standard
parsetreesbyremovingthefunctiontagsafterpars-
ingwiththem.
The results for Korean are quite pronounced:
we see a nearly seven-point improvement when re-
taining the existing tags. This very strongly sug-
gests that the KTB nonterminals are too coarse
when stripped of their function tags, and raises the
question of whether further improvement might be
gainedfromlatentannotations.
TheEnglishscoresallowustomakeanotherpoint.
Retaining the provided function tags results in a
solid performance increase with the standard train-
ing corpus, but actually hurts performance when
training on the small dataset. Note clearly that this
does not suggest that parsing performance with the
grammar from the small English data could not be
improved with latent annotations (indeed, we will
show that they can), but only that the given annota-
tions do not help improve parsing accuracy. Taking
theKoreanandEnglishaccuracyresultsfromthista-
bletogetherprovidesanotherpieceofevidencethat
theKoreannonterminalsetistoocoarse.
4.2 Latentannotations
Weappliedthelatentannotationlearningprocedures
of Petrov et al.2 to refine the nonterminals in the
KTB. Thetrainerlearnsrefinementsoverthecoarse
versionoftheKTB(withfunctiontagsremoved). In
thisexperiment,ratherthandoing10-foldcrossvali-
dation,wesplittheKTBintotraining,development,
and test sets that roughly match the 80/10/10 splits
ofthefolds:
section fileIDs
training 302000to316999
development 317000to317999
testing 320000to320999
This procedure results in grammars which can then
beusedtoparsenewsentences. Table4displaysthe
parsing accuracy results for parsing with the gram-
mar(aftersmoothing)attheendofeachsplit-merge-
smooth cycle.3 The scores in this table show that,
justaswiththePTB,nonterminalrefinementmakes
ahugedifferenceinparserperformance.
Again with the caveat that direct comparison of
parsing scores across annotation schemes must be
taken loosely, we note that the KTB parsing accu-
racy is still about 10 points lower than the best ac-
2http://code.google.com/p/berkeleyparser/
3As described in Petrov et al. (2006), to score a parse tree
producedwitharefinedgrammar,wecaneithertaketheViterbi
derivationorapproximateasumoverderivationsbeforeproject-
ingbacktothecoarsetreeforscoring.
52
Viterbi max-sum
cycle F1 F1≤40 F1 F1≤40
1 56.93 61.11 61.04 64.23
2 63.82 67.94 66.31 68.90
3 69.86 72.83 72.85 75.63
4 74.36 77.15 77.18 78.18
5 78.07 80.09 79.93 82.04
6 78.91 81.55 80.85 82.75
Table 4: Parsing accuracy on Korean test data from the
grammarsoutputbytheBerkeleystate-splittinggrammar
trainer. For comparison, parsing all sentences of §22 in
the PTB with the same trainer scored 89.58 (max-sum
parsingwithfivecycles)withthestandardtrainingcorpus
and85.21whentrainedon§2–4.
curacy scores produced in parsing the PTB which,
in our experiments, were 89.58 (using max-sum to
parse all sentences with the grammar obtained after
fivecyclesoftraining).
An obvious suspect for the difference in parsing
accuracywithlatentgrammarsbetweenEnglishand
Korean is the difference in training set sizes. This
turnsoutnottobethecase. Welearnedlatentanno-
tations on sections 2–4 of the PTB and again tested
on section 22. The accuracy scores on the test set
peakat85.21(max-sum,allsentences,fivecyclesof
training). ThisisaboutfivepointslowerthantheEn-
glish grammar trained on sections 2–21, but is still
overfourpointshigherthantheKTBresults.
In the next section, we turn to one of the theoret-
ical difficulties with Korean parsing with which we
beganthepaper.
5 NULL
elements
Both the PTB and KTB include many NULL ele-
ments. For English, these elements are traces de-
noting constituent movement. In the KTB, there
are many more kinds of NULL elements, in includ-
ing trace markers, zero pronouns, relative clause re-
duction, verb deletions, verb ellipsis, and other un-
knowncategories. StandardpracticeinEnglishpars-
ing is to remove NULL elements in order to avoid
thecomplexityofparsingwith ǫ-productions. How-
ever, another approach to parsing that avoids such
productions is to retain the NULL elements when
readingthegrammar;attesttime,theparserisgiven
sentences that contain markers denoting the empty
elements. To evaluate, we remove these elements
model F1 F1≤40 tokens
English(standardtrainingcorpus)
coarse 71.61 72.74 950K
w/functiontags 72.82 74.05 950K
w/NULLs 73.29 74.38 1,014K
Korean
w/verbellipses 52.85 56.52 3,200
w/traces 55.88 59.42 3,868
w/r.c. markers 56.74 59.87 3,794
w/zeropronouns 57.56 61.17 4,101
latent(5)w/NULLs 89.56 91.03 22,437
Table 5: Parser scores for Treebank PCFGs in English
and Korean with NULL elements. Tokens denotes the
number of words in the test data. The latent grammar
wastrainedforfiveiterations.
from the resulting parse trees output by the parser
and compare against the stripped-down gold stan-
dard used in previous sections, in order to provide
afairpointofcomparison.
Parsinginthismannerhelpsustoanswertheques-
tion of how much easier or more difficult parsing
would be if the NULL elements were present. In
this section, we present results from a variety of ex-
periments parsing will NULL tokens in this manner.
These results can be seen in Table 5. The first ob-
servationfromthistableisthatinEnglish,retaining
NULL elementsmakesafewpointsdifference.
ThefirstfourrowsoftheKTBportionofTable5
contains results with retaining different classes of
NULL elements,oneatatime,accordingtotheman-
ner described above. Restoring deleted pronouns
and relative clause markers has the largest effect,
suggesting that the absence of these optional ele-
mentsremoveskeycuesneededforparsing.
In order to provide a more complete picture of
the effect of empty elements, we train the Berkeley
latent annotation system on a version of the KTB
in which all empty elements are retained. The fi-
nalrowofTable5containsthescoreobtainedwhen
evaluating parse trees produced from parsing with
thegrammarafterthefifthiteration(afterwhichper-
formance began to fall). With the empty elements,
we have achieved accuracy scores that are on par
with the best accuracy scores obtained parsing the
EnglishTreebank.
53
6 Treesubstitutiongrammars
Wehaveshownthatcoarselabelsandtheprevalence
ofNULL elementsinKoreanbothcontributetopars-
ing difficulty. We now turn to grammar formalisms
thatallowustoworkwithlargerfragmentsofparse
trees than the height-one rules of standard context-
free grammars. Tree substitution grammars (TSGs)
have been shown to improve upon the standard En-
glish Treebank grammar (Bod, 2001) in parser ac-
curacy, and more recently, techniques for inferring
TSGsubtreesinaBayesianframeworkhaveenabled
learning more efficiently representable grammars,
permitting some interesting analysis (O’Donnell et
al.,2009;Cohnetal.,2009;PostandGildea,2009).
In this section, we try parsing the KTB with TSGs.
We experiment with different methods of learning
TSGs to see whether they can reveal any insights
intothedifficultiesparsingKorean.
6.1 Headrules
TSGs present some difficulties in learning and rep-
resentation, but a simple extraction heuristic called
a spinal grammar has been shown to be very use-
ful(Chiang,2000;SangatiandZuidema,2009;Post
and Gildea, 2009). Spinal subtrees are extracted
from a parse tree by using a set of head rules to
maximallyprojecteachlexicalitem(awordormor-
pheme). Eachnodeintheparsetreehavingadiffer-
ent head from its parent becomes the root of a new
subtree, which induces a spinal TSG derivation in
the parse tree (see Figure 1). A probabilistic gram-
mar is derived by taking counts from these trees,
smoothing them with counts of all depth-one rules
fromthesametrainingset,andsettingruleprobabil-
itiestorelativefrequency.
This heuristic requires a set of head rules, which
wepresentinTable6. Asanevaluationofourrules,
we list in Table 7 the accuracy results for parsing
withspinalgrammarsextractedusingtheheadrules
we developed as well as with two head rule heuris-
tics(head-leftandhead-right). Asapointofcompar-
ison,weprovidethesameresultsforEnglish,using
the standard Magerman/Collins head rules for En-
glish (Magerman, 1995; Collins, 1997). Function
tagswereretainedforKoreanbutnotforEnglish.
We observe a number of things from Table 7.
First, the relative performance of the head-left and
NT RC rule
S SFN secondrightmostchild
VV EFN rightmostXSV
VX EFN rightmostVJorCO
ADJP EFN rightmostVJ
CV EFN rightmostVV
LV EFN rightmostVV
NP EFN rightmostCO
VJ EFN rightmostXSVorXSJ
VP EFN rightmostVX,XSV,orVV
⋆ ⋆ rightmostchild
Table 6: Head rules for the Korean Treebank. NT is the
nonterminal whose head is being determined, RC identi-
fiesthelabelofitsrightmostchild. Thedefaultistotake
therightmostchildasthehead.
head-right spinal grammars between English and
KoreancapturethelinguisticfactthatEnglishispre-
dominantly head-first and Korean is predominantly
head-final. In fact, head-finalness in Korean was so
strong that our head rules consist of only a handful
of exceptions to it. The default rule makes heads
of postpositions (case and information clitics) such
as dative case marker and topic marker. It is these
words that often have dependencies with words in
the rest of the sentence. The exceptions concern
predicates that occur in the sentence-final position.
As an example, predicates in Korean are composed
of several morphemes, the final one of which indi-
cates the mood of the sentence. However, this mor-
pheme often does not require any inflection to re-
flect long-distance agreement with the rest of the
sentence. Therefore, we choose the morpheme that
would be considered the root of the phrase, which
in Korean is the verbalization/adjectivization suf-
fix, verb, adjective, auxiliary predicate, and copula
(XSV,XSJ,VV,VJ,VX,CO).Theseitemsoftenin-
cludetheinformationaboutvalencyofthepredicate.
Second, in both languages, finer-grained specifi-
cation of head rules results in performance above
that of the heuristics (and in particular, the head-
leftheuristicforEnglishandhead-rightheuristicfor
Korean). The relative improvements in the two lan-
guages are in line with each other: significant, but
not nearly as large as the difference between the
head-leftandhead-rightheuristics.
Finally, we note that the test results together sug-
gest that parsing with spinal grammars may be a
54
(a) TOP
S
NP-SBJ
NPR
uni38D9uni39D7㭐
NNC
㞇uni3850
PAU
㧹
VP
NP-ADV
DAN
uni3494
NNC
uni360F
VP
VV
NNC
㲩uni344C
XSV
uni35BE㲠
EPF
uni397D
EFN
uni35B0
SFN
.
(b) S
NP-SBJ
NPR
uni38D9uni39D7㭐
NNC PAU
VP SFN
(c) S
NP-SBJ VP SFN
.
Figure1: (a)AKTBparsetree; theboldnodesdenotethetop-levelspinalsubtreeusingourheadselectionrules. (b)
The top-level spinal subtree using the head-left and (c) head-right extraction heuristics. A gloss of the sentence is
Doctor Schwartz was fired afterward.
model F1 F1≤40 size
Korean
spinal(headleft) 59.49 63.33 49K
spinal(headright) 66.05 69.96 29K
spinal(headrules) 66.28 70.61 29K
English
spinal(headleft) 77.92 78.94 158K
spinal(headright) 72.73 74.09 172K
spinal(headrules) 78.82 79.79 189K
Table7: SpinalgrammarscoresontheKTBandonPTB
section22.
goodevaluationofasetofheadselectionrules.
6.2 Inducedtreesubstitutiongrammars
Recent work in applying nonparametric machine
learningtechniquestoTSGinductionhasshownthat
the resulting grammars improve upon standard En-
glish treebank grammars (O’Donnell et al., 2009;
Cohn et al., 2009; Post and Gildea, 2009). These
techniquesuseaDirichletProcessprioroverthesub-
tree rewrites of each nonterminal (Ferguson, 1973);
this defines a model of subtree generation that pro-
duces new subtrees in proportion to the number of
times they have previously been generated. Infer-
ence under this model takes a treebank and uses
Gibbs sampling to determine how to deconstruct a
parse tree into a single TSG derivation. In this sec-
tion,weapplythesetechniquestoKorean.
ThisTSGinductionrequiresonetospecifyabase
measure, which assigns probabilities to subtrees be-
ing generated for the first time in the model. One
base measure employed in previous work scored a
subtree by multiplying together the probabilities of
the height-one rules inside the subtree with a ge-
ometric distribution on the number of such rules.
Since Korean is considered to be a free word-order
language,wemodifiedthisbasemeasuretotreatthe
childrenofaheight-oneruleasamultiset(insteadof
asequence). Thishastheeffectofproducingequiva-
lenceclassesamongthesetsofchildrenofeachnon-
terminal,concentratingthemassontheseclassesin-
stead of spreading it across their different instantia-
tions.
Tobuildthesampledgrammars,weinitializedthe
samplers from the best spinal grammar derivations
and ran them for 100 iterations (once again, func-
tion tags were retained). We then took the state of
the training data at every tenth iteration, smoothed
togetherwiththeheight-onerulesfromthestandard
Treebank. The best score on the development data
for a sampled grammar was 68.93 (all sentences)
and 73.29 (sentences with forty or fewer words):
well above the standard Treebank scores from ear-
liersectionsandabovethespinalheuristics,butwell
below the scores produced by the latent annotation
learning procedures (a result that is consistent with
English).
This performance increase reflects the results for
Englishdemonstratedintheaboveworks. Weseea
largeperformanceincreaseabovethebaselineTree-
bank grammar, and a few points above the best
spinal grammar. One nice feature of these induced
TSGs is that the rules learned lend themselves to
analysis,whichweturntonext.
6.3 Wordorder
In addition to the base measure mentioned above,
we also experimented with the standard base mea-
55
NP
NPR NNC
㨆uni39D9
NNU NNX
uni373C
Figure2: Exampleofalongdistancedependencylearned
byTSGinduction.
sureproposedbyCohnetal.andPost&Gildea,that
treats the children of a nonterminal as a sequence.
The grammars produced sampling under a model
withthisbasemeasurewerenotsubstantivelydiffer-
entfromthoseoftheunorderedbasemeasure. Apar-
tialexplanationforthisisthatalthoughKoreandoes
permitasignificantamountofreorderingrelativeto
English, the sentences in the KTB come from writ-
ten newswire text, where word order is more stan-
dardized. Koreansentencesarecharacterizedashav-
ing a subject-object-verb (SOV) word order. There
is some flexibility; OSV, in particular, is common
in spoken Korean. In formal writing, though, SOV
wordorderisoverwhelminglypreferred. Weseethis
reflectedintheKTB,whereSOVsentencesare63.5
times more numerous that OSV among sentences
thathaveexplicitlymarkedboththesubjectandthe
object. However,wordorderisnotcompletelyfixed
even in the formal writing. NP-ADV is most likely
to occur right before the VP it modifies, but can be
movedearlier. Forexample,
S→NP-SBJ NP-ADV VP
is 2.4 times more frequent than the alternative with
theorderoftheNPsreversed.
Furthermore, the notion of free(er) word order
does not apply to all constituents. An example is
nonterminalsdirectlyabovepreterminals. AKorean
verbmayhaveuptosevenaffixes;however,theyal-
waysagglutinateinafixedorder.
6.4 Longdistancedependencies
The TSG inference procedure can be thought of
as discovering structural collocations in parse trees.
The model prefers subtrees that are common in the
data set and that comprise highly probable height-
one rules. The parsing accuracy of these grammars
is well below state of the art, but the grammars are
smaller,andthesubtreeslearnedcanhelpusanalyze
the parse structure of the Treebank. One particular
classofsubtreeisonethatincludesmultiplelexical
items with intervening nonterminals, which repre-
sent long distance dependencies that commonly co-
occur. In Korean, a certain class of nouns must ac-
companyaparticularclassofmeasureword(amor-
pheme) when counting the noun. In the example
shown in Figure 2, (NNC 㨆uni39D9) (members of as-
sembly) is followed by NNU, which expands to in-
dicateordinal,cardinal,andnumeralnouns;NNUis
inturnfollowedby(NNXuni373C),thepolitenessneutral
measurewordforcountingpeople.
7 Summary&futurework
In this paper, we addressed several difficult aspects
ofparsingKoreanandshowedthatgoodparsingac-
curacyforKoreancanbeachieveddespitethesmall
sizeofthecorpus.
Analysis of different parsing results from differ-
ent grammatical formalisms yielded a number of
usefulobservations. Wefound,forexample,thatthe
set of nonterminals in the KTB is not differentiated
enoughforaccurateparsing;however,parsingaccu-
racy improves substantially from latent annotations
and state-splitting techniques that have been devel-
oped with English as a testbed. We found that freer
word order may not be as important as might have
been thought from basic a priori linguistic knowl-
edgeofKorean.
The prevalence of NULL elements in Korean is
perhaps the most interesting difficulty in develop-
ing good parsing approaches for Korean; this is
a key difference from English parsing that to our
knowledge is not addressed by any available tech-
niques. One potential approach is a special an-
notation of parents with deleted nodes in order to
avoid conflating rewrite distributions. For example,
S → VP is the most common rule in the Korean
treebankafterstrippingawayemptyelements; how-
ever,thisisaresultofcondensingtheruleS→(NP-
SBJ*pro*)VPandS→VP,whichpresumablyhave
different distributions. Another approach would be
toattemptautomaticrecoveryofemptyelementsas
apre-processingstep.
Acknowledgments We thank the anonymous re-
viewers for their helpful comments. This work
wassupportedbyNSFgrantsIIS-0546554andITR-
0428020.
56
References
Rens Bod. 2001. What is the minimal set of fragments
that achieves maximal parse accuracy? In Proc. ACL,
Toulouse,France,July.
Eugene Charniak. 1996. Tree-bank grammars. In Proc.
of the National Conference on Artificial Intelligence,
pages1031–1036.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proc. ACL,HongKong.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducingcompactbutaccuratetree-substitution
grammars. InProc. NAACL.
Michael Collins. 1997. Three penerative, lexicalised
modelsforstatisticalparsing. InProc. ACL/EACL.
Thomas S. Ferguson. 1973. A Bayesian analysis of
some nonparametric problems. Annals of Mathemat-
ical Statistics,1(2):209–230.
Chung-Hye Han and Martha Palmer. 2004. A mor-
phological tagger for Korean: Statistical tagging com-
bined with corpus-based morphological rule applica-
tion. Machine Translation,18(4):275–297.
Na-Rae Han and Shijong Ryu. 2005. Guidelines for
Penn Korean Treebank version 2.0. Technical report,
IRCS,UniversityofPennsylvania.
Chung-hye Han, Na-Rae Han, and Eon-Suk Ko. 2001.
Bracketing guidelines for Penn Korean Treebank.
Technicalreport,IRCS,UniversityofPennsylvania.
Na-RaeHan. 2006. Korean zero pronouns: analysis and
resolution. Ph.D. thesis, University of Pennsylvania,
Philadelphia,PA,USA.
Ulf Hermjakob. 2000. Rapid parser development: a ma-
chinelearningapproachforKorean. In Proc. NAACL,
pages118–123,May.
MarkJohnson. 1998. PCFGmodelsoflinguistictreerep-
resentations. Computational Linguistics, 24(4):613–
632.
Laura Kallmeyer and SinWon Yoon. 2004. Tree-local
MCTAG with shared nodes: Word order variation in
German and Korean. In Proc. TAG+7, Vancouver,
May.
Dan Klein and Chris Manning. 2003. Accurate unlexi-
calizedparsing. InProc. ACL.
Do-Gil Lee and Hae-Chang Rim. 2005. Probabilistic
models for Korean morphological analysis. In Com-
panion to the Proceedings of the International Joint
Conference on Natural Language Processing, pages
197–202.
Sang-zoo Lee, Jun-ichi Tsujii, and Hae-Chang Rim.
2000. Hidden markov model-based Korean part-of-
speechtaggingconsideringhighagglutinativity,word-
spacing,andlexicalcorrelativity. InProc. ACL.
David M. Magerman. 1995. Statistical decision-tree
modelsforparsing. InProc. ACL.
SamuelE.Martin. 1992. ReferenceGrammarofKorean:
A Complete Guide to the Grammar and History of the
Korean Language. TuttlePublishing.
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc. ACL,AnnArbor,Michigan.
TimothyJ.O’Donnell,NoahD.Goodman,andJoshuaB.
Tenenbaum. 2009. Fragment grammar: Exploring
reuse in hierarchical generative processes. Technical
report,MIT.
Seong-Bae Park, Jeong-Ho Chang, and Byoung-Tak
Zhang. 2004. Koreancompoundnoundecomposition
using syllabic information only. In Computational
Linguistics and Intelligent Text Processing (CICLing),
pages146–157.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretabletreeannotation. In Proc. COLING/ACL,Syd-
ney,Australia,July.
MattPostandDanielGildea. 2009. Bayesianlearningof
atreesubstitutiongrammar. In Proc. ACL,Singapore,
Singapore,August.
Detlef Prescher. 2005. Inducing head-driven PCFGs
with latent heads: Refining a tree-bank grammar for
parsing. Machine Learning: ECML 2005, pages 292–
304.
Owen Rambow and Young-Suk Lee. 1994. Word order
variation and tree-adjoining grammar. Computational
Intelligence,10:386–400.
Ines Rehbein and Josef van Genabith. 2007. Eval-
uating evaluation measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
(NODALIDA).
Federico Sangati and Willem Zuidema. 2009. Unsuper-
visedmethodsforheadassignments. InProc. EACL.
Anoop Sarkar and Chung-hye Han. 2002. Statistical
morphological tagging and parsing of Korean with an
LTAGgrammar. InProc. TAG+6.
57

