The prime public domain examples of such implementations include the Trigrams’n’Tags tagger (Brandts 2000), Xerox tagger (Cutting et al.1992) and LT POS tagger (Mikheev 1997).
A92-1018 J97-3003
Kappa coefficient is given in (1) (Carletta 1996) (1) )(1 )()( EP EPAP Kappa − − = where P(A) is the proportion of times the annotators actually agree and P(E) is the proportion of times the annotators are expected to agree due to chance 3. The Absolute Agreement is most informative when computed over several sets of labels and where one of the sets represents the “authoritative” set.
J96-2004
The LT POS tagger is reported to perform at 93.6-94.3% accuracy on known words and at 87.7-88.7% on unknown words using a cascading unknown word “guesser” (Mikheev, 1997).
J97-3003
Maximum Entropy (MaxEnt) based taggers also seem to perform very well (Ratnaparkhi 1996, Jason Baldridge, Tom Morton, and Gann Bierner http://maxent.sourceforge.net ).
W96-0213
An acceptable agreement for most NLP classification tasks lies between 0.7 and 0.8 (Carletta 1996, Poessio and Vieira 1988).
J96-2004
Currently, most of the POS tagger accuracy reports are based on the experiments involving Penn Treebank data (Marcus, 1993).
J93-2004
