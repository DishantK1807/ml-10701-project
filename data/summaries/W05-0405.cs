Mochizuki et al.(1998) use a combination of linguistic cues to segment Japanese text.
P98-2145
Reynar (1999) describes a maximum entropy model that combines hand selected features, including: broadcast news domain cues, number of content word bigrams, number of named entities, number of content words that are WordNet synonyms in the left and right regions, percentage of content words in the right segment that are rst uses, whether pronouns occur in the rst ve words, and whether a word frequency based algorithm predicts a boundary.
P99-1046
human performance (Hearst, 1994) Word Sent.
P94-1002
As text analysis and retrieval moves from retrieval of documents to retrieval of document passages, the ability to segment documents into smaller, coherent regions enables more precise retrieval of meaningful portions of text (Hearst, 1994) and improved question answering.
P94-1002
Hearst (1994) examined the task of identifying the paragraph boundaries in expository text.
P94-1002
Research projects on text segmentation have focused on broadcast news stories (Beeferman et al., 1999), expository texts (Hearst, 1994) and synthetic texts (Li and Yamanishi, 2000; Brants et al., 2002).
P94-1002 W00-1305
WindowDiff (Pevzner and Hearst, 2002) uses a sliding window over the data and measures the difference between the number of hypothesized boundaries and the actual boundaries within the window.
J02-1002
The cosine similarity measure between term vectors is used by Hearst (1994) to de ne the similarity between blocks.
P94-1002
Hearst. 1994.
P94-1002
Segmentation also has applications in other areas of information access, including document navigation (Choi, 2000), anaphora and ellipsis resolution, and text summarization (Kozima, 1993).
P93-1041
Boundary words Many feature-based methods rely on cues at the boundaries (Beeferman et al., 1999; Reynar, 1999).
P99-1046
However, Mochizuki et al.(1998) analyzed Japanese texts, and Reynar (1999) and Beeferman et al.(1999) evaluated on broadcast news stories, which have many cues that narrative texts do not.
P98-2145 P99-1046
Sentences are identi ed using the TnT tokenizer and parts of speech with the TnT part of speech tagger (Brants, 2000) with the standard English Wall Street Journal n-grams.
A00-1031
