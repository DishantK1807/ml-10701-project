Following the suggestions in (Carletta, 1996), Core et al.consider kappa scores above 0.67 to indicate significant agreement and scores above 0.8 reliable agreement.
J96-2004
(Di Eugenio et al., 1998; Shriberg et al., 2004) ).
J04-1005 P98-1052 W04-2319
(Di Eugenio and Glass, 2004)), one of which is the discrepancy between p0 and κ for skewed class distribution.
J04-1005
Ever since its introduction in general (Cohen, 1960) and in computational linguistics (Carletta, 1996), many researchers have pointed out that there are quite some problems in using κ (e.g.
J96-2004
Agreement is sometimes measured as percentage of the cases on which the annotators agree, but more often expected agreement is taken into account in using the kappa statistic (Cohen, 1960; Carletta, 1996), which is given by: κ = po − pe1 − p e (1) where po is the observed proportion of agreement and pe is the proportion of agreement expected by chance.
J96-2004
