Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 55–63,
Columbus, Ohio, USA, June 2008. c©2008 Association for Computational Linguistics
ThePaGe2008SharedTaskonParsingGerman
∗
SandraK¨ubler
DepartmentofLinguistics
IndianaUniversity
Bloomington,IN,USA
skuebler@indiana.edu
Abstract
TheACL2008WorkshoponParsingGerman
featuresasharedtaskonparsingGerman.The
goal of the shared task was to find reasons
for the radicallydifferentbehaviorof parsers
on the different treebanks and between con-
stituent and dependency representations. In
this paper, we describethe task and the data
sets. In addition,we providean overview of
thetestresultsandafirstanalysis.
1 Introduction
Germanis oneof the very few languagesfor which
more than one syntacticallyannotatedresource ex-
ists. Other languagesfor which this is the case in-
clude English (with the Penn treebank (Marcus et
al., 1993), the Susanne Corpus (Sampson, 1993),
and the British section of the ICE Corpus (Wallis
and Nelson, 2006)) and Italian (with ISST (Mon-
tegmagni et al., 2000) and TUT (Bosco et al.,
2000)). The three German treebanks are Negra
(Skutetal.,1998),TIGER (Brantset al.,2002),and
T¨uBa-D/Z(Hinrichset al., 2004). We will concen-
trate on TIGER and T¨uBa-D/Z here; Negra is an-
notated with an annotation scheme very similar to
TIGER butissmaller. Incontrasttootherlanguages,
these two treebanks are similar on many levels:
Both treebanks are based on newspaper text, both
use the STTS part of speech (POS) tagset (Thie-
len and Schiller, 1994), and both use an annotation
∗
I am very grateful to Gerald Penn, who suggested this
workshopandthesharedtask,tookover thebiggestpartof the
workshoporganizationandhelpedwiththesharedtask.
scheme based on constituent structure augmented
withgrammaticalfunctions.However,theydifferin
the choicesmadein the annotationschemes,which
makes them ideally suited for an investigation of
how these decisions influence parsing accuracy in
differentparsers.
On a different level, German is an interesting
language for parsing because of the syntactic phe-
nomenainwhichthelanguagediffersfromEnglish,
the undoubtedlymost studied language in parsing:
Germanis oftenlistedas a non-configurationallan-
guage. However, while the word order is freer
thaninEnglish,thelanguageexhibitsalessflexible
word order than more typical non-configurational
languages. Ashortoverviewof Germanwordorder
phenomenaisgiveninsection2.
Thestructureof this paperis as follows: Section
2discussesthreecharacteristicsofGermanwordor-
der,section3providesadefinitionofthesharedtask,
andsection4givesashortoverviewofthetreebanks
and their annotationschemesthat were used in the
sharedtask. Insection5,wegiveanoverviewofthe
participatingsystemsandtheirresults.
2 GermanWordOrder
In German,the order of non-verbal phrasesis rela-
tivelyfree,but theplacementoftheverbalelements
is determined by the clause type. Thus, we will
first describe the placementof the finite verb, then
wewillexplainphrasalordering,andfinallywewill
lookatdiscontinuousconstituents.
55
2.1 VerbPlacement
In German, the clause type determines the place-
ment of finite verbs: In non-embeddeddeclarative
clauses,as in (1a),the finite verb is in secondposi-
tion (V2). In yes/noquestions,as in (1b), the finite
verbistheclause-initialconstituent(V1),andinem-
bedded clauses, as in (1c), it appears clause finally
(Vn).
(1) a. Der
The
Mann
man
hat
has
das
the
Auto
car
gekauft.
bought
’Themanhasboughtthecar.’
b. Hat
Has
der
the
Mann
man
das
the
Auto
car
gekauft?
bought
’Hasthemanboughtthecar?’
a. dass
that
der
the
Mann
man
das
the
Auto
car
gekauft
bought
hat.
has
’...thatthemanhasboughtthecar.’
All non-finite verbs appearat the right periphery
of the clause (cf. 2), independently of the clause
type.
(2) Der
The
Mann
man
sollte
should
das
the
Auto
car
gekauft
bought
haben.
have
’Themanshouldhaveboughtthecar.’
2.2 FlexiblePhraseOrdering
Apartfromthefixed placementoftheverbs,theor-
derofthenon-verbalelementsisflexible.In(3),any
of the four complementsand adjuncts of the main
verb (ge)geben can be in sentence-initial position,
depending on the informationstructure of the sen-
tence.
(3) a. Das
The
Kind
child
hat
has
dem
the
Mann
man
gestern
yesterday
den
the
Ball
ball
gegeben.
given
’Thechildhasgiventheballtothemanyes-
terday.’
b. Dem Mann hat das Kind gestern den Ball
gegeben.
c. Gestern hat das Kind dem Mann den Ball
gegeben.
d. Den Ball hat das Kind gestern dem Mann
gegeben.
In addition,the orderingof the elementsthat oc-
cur betweenthe finiteandthe non-finite verb forms
is also free so that there are six possible lineariza-
tionsforeachoftheexamplesin(3a-d).
One exceptionto the free orderingof non-verbal
elements is the ordering of pronouns. If the pro-
nouns appear to the right of the finite verb in V1
and V2 clauses, they are adjacent to the finite verb
infixedorder.
(4) Gestern
Yesterday
hat
has
sie
she
sie
her/them
ihm
him
gegeben.
given.
’Yesterday, shegaveher/themtohim.’
In (4), three pronouns are present. Although
the pronoun sie is ambiguous between nomina-
tive/accusative singular and nominative/accusative
plural, the given example is unambiguouswith re-
spect to case since the nominative precedesthe ac-
cusative, whichinturnprecedesthedative.
Duetotheflexiblephraseordering,thegrammat-
ical functions of constituentsin German, unlike in
English, cannot be deduced from the constituents’
location in the constituenttree. As a consequence,
parsingapproachesto Germanneed to be based on
treebankdata whichcontain a combinationof con-
stituent structure and grammatical functions – for
parsing and evaluation. For English, in contrast,
grammatical functions are often used internally in
parsersbut suppressedinthefinalparseroutput.
2.3 DiscontinuousConstituents
Anothercharacteristicof Germanword orderis the
frequency of discontinuous constituents. The sen-
tencein(5)showsanextraposedrelative clausethat
isseparatedfromitsheadnoundasBuchbythenon-
finiteverbgelesen.
(5) Der
The
Mann
man
hat
has
das
the
Buch
book
gelesen,
read,
das
which
ich
I
ihm
him
empfohlen
recommended
habe.
have
’ThemanreadthebookthatIrecommendedto
him.’
56
In German, it is also possible to partially front
VPs,suchasin sentence(6). Thissentenceistaken
fromtheT¨uBa-D/Ztreebank.
(6) F¨ur
For
den
the
Berliner
Berlin
Job
job
qualifiziert
qualified
hat
has
sich
himself
Zimmermann
Zimmermann
auch
also
durch
by
seinen
his
Blick
view
f¨urs
forthe
finanziell
financially
Machbare.
doable
’Zimmermann qualified for the job in Berlin
partiallybecauseof his view for whatis finan-
ciallyfeasible.’
Here,thecanonicalwordorderwouldbeZimmer-
mannhatsichauchdurchseinenBlickf¨urs finanziell
Machbare f¨ur denBerlinerJob qualifiziert.
Suchdiscontinuousstructuresoccurfrequentlyin
theTIGER andT¨uBa-D/Ztreebanksandarehandled
differentlyinthetwoannotationschemes,aswillbe
discussedinmoredetailinsection4.
3 TaskDefinition
In this section, we give the definition of the shared
task. Weprovidedtwosubtasks:parsingconstituent
structure and parsing the dependency representa-
tions.Bothsubtasksinvolvedtrainingandtestingon
datafromthetwotreebanks,TIGER andT¨uBa-D/Z.
The dependency format was derived from the con-
stituent format so that the sentences were identical
in the two versions. The participants were given
training sets, developmentsets, and test sets of the
two treebanks. The training sets contained 20894
sentences per treebank, the development and test
set consistedof 2611 sentenceseach. The test sets
contained gold standard POS labels. In these sets,
sentencelengthwas restrictedto a maximumof 40
words. Sincefor somesentencesin bothtreebanks,
the annotation consists of more than one tree, all
treeswerejoinedunderavirtualrootnode,VROOT.
Since some parsers cannot assign grammatical
functions to part of speech tags, these grammati-
cal functions were provided for the test data as at-
tached to the POStags. Participantswere asked to
performatestwithoutthesefunctionsiftheirparser
wasequippedtoprovidethem.Twoparticipantsdid
submittheseresults,andinbothcases,theseresults
wereconsiderablylower.
Evaluation for the constituent version consisted
of the PARSEVAL measures precision, recall, and
F
1
measure. Allthese measureswerecalculatedon
combinationsof constituentlabelsandgrammatical
functions.Partofspeechlabelswerenotconsidered
in the evaluation. Evaluation for the dependency
version consisted of labeled and unlabeled attach-
mentscores.Forthisevaluation,weusedthescripts
providedbytheCoNLLsharedtask2007ondepen-
dencyparsing(Nivreetal.,2007).
4 TheTreebanks
The two treebanks used for the shared task were
the TIGER Corpus, (Brants et al., 2002) version
2, and the T¨uBa-D/Z treebank (Hinrichs et al.,
2004; Telljohann et al., 2006), version 3. Both
treebanks use German newspapers as their data
source: the Frankfurter Rundschau newspaper for
TIGER and the ’die tageszeitung’ (taz) newspaper
for T¨uBa-D/Z. The average sentence length is
very similar: In TIGER, sentenceshave an average
length of 17.0, and in T¨uBa-D/Z, 17.3. This can
be regarded as an indicationthat the complexity of
the two texts is comparable. Both treebanks use
the same POS tagset, STTS (Thielen and Schiller,
1994), and annotations based on phrase structure
grammar,enhancedbyalevelofpredicate-argument
structure.
4.1 TheConstituentData
Despite all the similarities presented above, the
constituent annotationsdiffer in four importantas-
pects: 1) TIGER does not allow for unary branch-
ing whereas T¨uBa-D/Z does; 2) in TIGER, phrase
internal annotation is flat whereas T¨uBa-D/Z uses
phrase internal structure; 3) TIGER uses crossing
branches to represent long-distance relationships
whereasT¨uBa-D/Zuses a pure tree structure com-
binedwithfunctionallabelstoencodethisinforma-
tion. Thetwotreebanksalsousedifferentnotionsof
grammaticalfunctions:T¨uBa-D/Zdefines36gram-
matical functions covering head and non-head in-
formation,as wellas subcategorizationfor comple-
ments and modifiers. TIGER utilizes 51 grammati-
calfunctions.Apartfromcommonlyacceptedgram-
maticalfunctions,such as SB (subject)or OA (ac-
cusative object), TIGER grammaticalfunctions in-
57
Figure1: TIGER annotationwithcrossingbranches.
Figure2: TIGER annotationwithresolvedcrossingbranches.
cludeothers,e.g. RE (repeatedelement)orRC (rel-
ativeclause).
(7) Beim
Atthe
M¨unchner
Munich
Gipfel
Summit
ist
is
die
the
sprichw¨ortliche
proverbial
bayerische
Bavarian
Gem¨utlichkeit
’Gem¨utlichkeit’
von
by
einem
a
Bild
picture
verdr¨angt
supplanted
worden,
been,
das
which
im
inthe
Wortsinne
literalsense
an
of
einen
a
Polizeistaat
policestate
erinnert.
reminds
’Atthe MunichSummit,the proverbial Bavar-
ian ’Gem¨utlichkeit’ was supplantedby an im-
agethatisevocative ofapolicestate.’
Figure 1 shows a typical tree from the TIGER
treebank for sentence (7). The syntacticcategories
are shown in circular nodes, the grammaticalfunc-
tions as edge labels in square boxes. A major
phrasal category that serves to structure the sen-
tence as a whole is the verb phrase (VP). It con-
tains non-finite verbs (here: verdr¨angt worden) as
wellastheircomplementsandadjuncts.Thesubject
NP (die sprichw¨ortliche bayerische Gem¨utlichkeit)
is outside the VP and, depending on its linear po-
sition,leads to crossingbrancheswiththe VP. This
happens in all cases where the subject follows the
finite verb as in Figure 1. Notice also that the PPs
are completely flat. An additional crossing branch
resultsfromthe directattachmentof the extraposed
relative clause(thelowerS nodewithfunctionRC)
tothenounthatitmodifies.
As mentioned in the previous section, TIGER
treesmustbetransformedintotreeswithoutcrossing
branchesprior to trainingPCFGparsers. The stan-
dardapproachforthistransformationis tore-attach
crossingnon-headconstituentsassistersofthelow-
estmothernodethatdominatesallthecrossingcon-
stituent and its sister nodes in the original TIGER
tree.Figure2showstheresultofthistransformation
58
Figure3: T¨uBa-D/Zannotationwithoutcrossingbranches.
of the tree in Figure1. Crossingbranchesnot only
arisewithrespecttothesubjectatthesentencelevel
butalsoincasesofextrapositionandfrontingofpar-
tialconstituents. Asa result,approximately30%of
allTIGER treescontainatleastonecrossingbranch.
Thus, tree transformationshave a major impact on
the type of constituent structures that are used for
trainingprobabilisticparsingmodels.
Figure3showstheT¨uBa-D/Zannotationforsen-
tence(8),asentencewithaverysimilarstructureto
the TIGER sentence shown in Figure 1. Crossing
branches are avoided by the introduction of topo-
logical structures(here: VF, LK, MF, VC, NF, and
C) into the tree. Notice also that compared to the
TIGER annotation,T¨uBa-D/Zintroducesmoreinter-
nalstructureintoNPsandPPs. InT¨uBa-D/Z,long-
distancerelationshipsarerepresentedbyapuretree
structure and specific functional labels. Thus, the
extraposed relative clause is attached to the matrix
clausedirectly,butitsfunctionallabelON-MODex-
plicatesthatitmodifiesthesubjectON.
(8) In
In
Bremen
Bremen
sind
are
bisher
sofar
nur
only
Fakten
facts
geschaffen
produced
worden,
been,
die
which
jeder
any
modernen
modern
Stadtplanung
cityplanning
entgegenstehen.
contradict
’In Bremen, so far only such attempts have
beenmadethatareopposedtoanymoderncity
planning.’
4.2 TheDependencyData
Theconstituentrepresentationsfrombothtreebanks
were converted into dependencies. The conver-
sion aimed at finding dependency representations
for both treebanksthat are as similar to each other
as possible. Complete identity is impossible be-
cause the treebanks contain different levels of dis-
tinctionfordifferentphenomena. Theconversionis
based on the original formats of the treebanks in-
cluding crossing branches. The target dependency
formatwas defined basedon the dependency gram-
mar by Foth (2003). For the conversion, we used
pre-existingdependency convertersfor TIGER trees
(Daumet al., 2004) and for T¨uBa-D/Ztrees (Vers-
ley, 2005). The dependency representationsof the
treesin Figures1 and3 are shown in Figures4 and
5. Notethatthelong-distancerelationshipsarecon-
vertedintonon-projective dependencies.
5 SubmissionsandResults
The shared task drew submissions from 3 groups:
the Berkeley group, the Stanford group, and the
V¨axj¨o group. Four more groups or individuals had
registeredbut didnot submitany data. Thesubmit-
tedsystemsandresultsaredescribedindetailinpa-
pers in this volume (Petrov and Klein, 2008; Raf-
fertyandManning,2008;HallandNivre,2008).All
three systems submitted results for the constituent
task. Forthedependency task,theV¨axj¨o grouphad
the only submission. For this reason, we will con-
centrateontheanalysisoftheconstituentresultsand
willmentionthedependency resultsonlyshortly.
59
BeimM.Gipfelist diesprichw. bayer. Gem.von einemBildverdr¨angtworden,dasim Worts.an einenP.staaterinnert.
PP
ATTR
PN
DET
ATTR
ATTR
SUBJ
PP
DET
PN
AUX
AUX SUBJ
PP
PN
OBJP
DET
PN
REL
Figure4: TIGER dependencyannotation.
In BremensindbishernurFaktengeschaffenworden,diejedermodernenStadtplanungentgegenstehen.
PN
PP ADV
ADV
SUBJ
AUX
AUX
SUBJ
DET
ATTR OBJD
REL
Figure5: T¨uBa-D/Zdependencyannotation.
5.1 ConstituentEvaluation
The results of the constituentanalysisare shown
in Table 1. The evaluationwas performedwith re-
gard to labels consisting of a combinationof syn-
tactic labels and grammaticalfunctions. A subject
nounphrase,forexample,isonlycountedascorrect
if it has the correct yield, the correct label (i.e. NP
for TIGER and NX for T¨uBa-D/Z),and the correct
grammatical function (i.e. SB for TIGER and ON
for T¨uBa-D/Z). The results show that the Berke-
leyparserreachesthebestresultsforbothtreebanks.
Theothertwoparserscompeteforsecondplace.For
TIGER, the V¨axj¨o parser outperformsthe Stanford
parser, but for T¨uBa-D/Z,the situation is reversed.
ThisgivesanindicationthattheV¨axj¨oparserseems
bettersuitedfortheflatannotationsin TIGER while
theStanfordparserisbettersuitedforthemorehier-
archicalstructureinT¨uBa-D/Z.Notethatallparsers
reachmuchhigherF-scoresforT¨uBa-D/Z.
Acomparisonofhowwellsuitedtwodifferentan-
notation schemes are for parsing is a surprisingly
difficult task. A first approach would be to com-
pare the parserperformancefor specific categories,
such as for noun phrases, etc. However, this is
not possiblefor TIGER and T¨uBa-D/Z. On the one
hand, the range of phenomena described as noun
phrases, for example, is different in the two tree-
banks. The most obvious difference in annotation
schemesis that T¨uBa-D/Zannotatesunary branch-
ing structures while TIGER does not. As a conse-
quence, in T¨uBa-D/Z, all pronouns and substitut-
ingdemonstrativesareannotatedasnounphrases;in
TIGER, they areattacheddirectlyto thenext higher
node(cf.the relative pronouns,POStagPRELS,in
Figures1 and 3). K¨ubler (2005) and Maier (2006)
suggest a method for comparingsuch different an-
notation schemes by approximatingthem stepwise
so that the decisionswhichresultin majorchanges
can be isolated. They come to the conclusion that
thedifferencesbetweenthetwoannotationschemes
is a leastpartiallydueto inconsistenciesintroduced
into TIGER style annotations during the resolution
of crossing branches. However, even this method
cannotgiveanyindicationwhichannotationscheme
provides more useful information for systems that
usesuchparsesasinput.Toanswerthisquestion,an
in vivo evaluation would be necessary. It is, how-
ever, rather difficult to find systems into which a
parser can be plugged in without too many modi-
ficationsofthesystem.
On the other hand, it is a well-known fact that
60
TIGER T¨uBa-D/Z
system precision recall F-score precision recall F-score
Berkeley 69.23 70.41 69.81 83.91 84.04 83.97
Stanford 58.52 57.63 58.07 79.26 79.22 79.24
V¨axj¨o 67.06 63.40 65.18 76.44 74.79 75.60
Table1: Theresultsoftheconstituentparsingtask.
TIGER T¨uBa-D/Z
system GF precision recall F-score precision recall F-score
Berkeley SB/ON 74.46 78.31 76.34 78.33 77.08 77.70
OA 60.08 66.61 63.18 58.11 65.81 61.72
DA/OD 49.28 41.72 43.19 59.46 44.72 51.05
Stanford SB/ON 64.40 63.11 63.75 71.16 77.76 74.31
OA 45.52 45.91 45.71 47.23 51.28 49.17
DA/OD 12.40 9.82 10.96 24.42 8.54 12.65
V¨axj¨o SB/ON 75.33 73.00 74.15 72.37 69.53 70.92
OA 57.01 57.65 57.33 58.07 57.55 57.81
DA/OD 55.45 37.42 44.68 63.75 20.73 31.29
Table2: Theresultsforsubjects,accusativeobjects,anddativeobjects.
thePARSEVALmeasuresfavor annotationschemes
with hierarchical structures, such as in T¨uBa-D/Z,
incomparisontoannotationschemeswithflatstruc-
tures (Rehbein and van Genabith, 2007). Here,
TIGERandT¨uBa-D/Zdiffersignificantly:inTIGER,
phrases receive a flat annotation. Prepositional
phrases, for example, do not contain an explicitly
annotatednoun phrase. T¨uBa-D/Zphrases,in con-
trast, are more hierarchical; prepositionphrases do
containa noun phrase, and non phrasesdistinguish
betweenpre-andpost-modification. Forthisreason,
the evaluation presented in Table 1 must be taken
withmorethanagrainofsaltasacomparisonofan-
notationschemes. However, it seemssafeto follow
K¨ubler et al. (K¨ubler et al., 2006) in the assump-
tion that the major grammatical functions, subject
(SB/ON),accusative object(OA),anddative object
(DA/OD) are comparable. Again, this is not com-
pletely true because in the case of one-word NPs,
these functions are attached to the POS tags and
thusaregiven in theinput. Anothersolution,which
was pursued by Rehbeinand van Genabith(2007),
istheintroductionofnewunarybranchingnodesin
the tree in cases wheresuch grammaticalfunctions
areoriginallyattachedtothePOStag. Werefrained
from using this solution because it introduces fur-
therinconsistencies(onlya subsetof unarybranch-
ing nodes are explicitly annotated), which make it
difficultforaparsertodecidewhethertogroupsuch
phrasesor not. The evaluationshown in Table 2 is
based on all nodes which were annotated with the
grammaticalfunctioninquestion.
The results presented in Table 2 show that the
differences between the two treebanks are incon-
clusive. While the Stanford parser performs con-
sistently better on T¨uBa-D/Z, the Berkeley parser
handlesaccusative objectsbetterin TIGER, and the
V¨axj¨oparsersubjectsanddativeobjects.Theresults
indicate that the Berkeley parser profits from the
TIGER annotation of accusative objects, which are
groupedin the verb phrasewhileT¨uBa-D/Zgroups
allobjectsintheirfieldsdirectlywithoutresortingto
a verb phrase. However, this does not explain why
the Berkeley parser cannot profit from the subject
attachmentontheclauselevelin TIGER tothesame
degree.
5.2 DependencyEvaluation
The results of the dependency evaluation for the
V¨axj¨o systemareshown in Table3. Theresultsare
61
TIGER T¨uBa-D/Z
UAS 92.63 91.45
LAS 90.80 88.64
precision recall precision recall
SUBJ 90.20 89.82 88.99 88.55
OBJA 77.93 82.19 77.18 82.71
OBJD 57.00 44.02 67.88 45.90
Table3: Theresultsofthedependencyevaluation.
importantforthecomparisonofconstituentandde-
pendency parsingsince in the conversion to depen-
dencies, most of the differences betweenthe anno-
tation schemes, and as a consequence, the prefer-
ence of the PARSEVAL measures have been neu-
tralized. Therefore, it is interesting to see that the
resultsfor TIGER are slightlybetterthanthe results
for T¨uBa-D/Z, both for unlabeled (UAS) and la-
beledattachmentscores. The reasonsfor these dif-
ferencesareunclear:eitherthe TIGER textsareeas-
iertoparse,orthe(originalannotationand)conver-
sion from TIGER is more consistent. Another sur-
prisingfactisthatthedependency resultsareclearly
better than the constituentones. This is partly due
to the fact that the dependency representationis of-
tenlessinformativethanthenconstituentrepresenta-
tion.Oneexampleforthiscanbefoundincoordina-
tions: Independency representations,thescopeam-
biguityinphraseslike youngmenandwomenisnot
resolved. This gives parsers fewer opportunitiesto
go wrong. However, this cannotexplainall the dif-
ferences. Especiallythe better performanceon the
majorgrammaticalfunctionscannotbeexplainedin
thisway.
Acloserlookatthegrammaticalfunctionsshows
that here, precision and recall are higher than for
constituentparses. Thisis a first indicationthat de-
pendency representation may be more appropriate
for languages with freer word order. A compari-
son betweenthe two treebanksis inconclusive: for
theaccusativeobject,theresultsaresimilarbetween
the treebanks. For subjects, the results for TIGER
are better while for dative objects, the results for
T¨uBa-D/Zare better. This issue requirescloser in-
vestigation.
6 Conclusion
This is the first shared task on parsing German,
whichprovidestrainingandtestsetsfrombothma-
jor treebanks for German, TIGER and T¨uBa-D/Z.
Forbothtreebanks,weprovidedaconstituentanda
dependency representation.It isourhopethatthese
datasetswillsparkmoreinterestin the comparison
of different annotationschemesand their influence
on parsingresults. The evaluationof the three par-
ticipatingsystemshasshownthatforbothtreebanks,
theuseofalatentvariablegrammarintheBerkeley
system is beneficial. However, many questions re-
main unansweredand require further investigation:
To whatextent do the evaluationmetricsdistortthe
results?Doesameasureexistthatisneutraltowards
thedifferencesinannotation?Isthedependencyfor-
mat better suited for parsingGerman? Are the dif-
ferencesbetweenthe dependency resultsof the two
treebanksindicatorsthat TIGER provides more im-
portantinformationfordependency parsing?Orcan
the differencesbe traced back to the conversion al-
gorithms?
Acknowledgments
Firstand foremost,we want to thankall the people
and organizationsthat generouslyprovided us with
treebank data and without whom the shared task
would have been literally impossible: Erhard Hin-
richs,UniversityofT¨ubingen(T¨uBa-D/Z),andHans
Uszkoreit,SaarlandUniversityandDFKI(TIGER).
Secondly,wewouldliketothankWolfgangMaier
and Yannick Versley who performed the data con-
versionsnecessaryforthesharedtask. Additionally,
Wolfgang provided the scripts for the constituent
evaluation.
References
Cristina Bosco, Vincenzo Lombardo, D. Vassallo, and
LeonardoLesmo. 2000. Buildinga treebankforItal-
ian: a data-drivenannotationscheme. In Proceedings
of the 2ndInternationalConferenceon Language Re-
sourcesandEvaluation,LREC-2000,Athens,Greece.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gangLezius, and George Smith. 2002. The TIGER
treebank. In Erhard Hinrichs and Kiril Simov, edi-
tors,Proceedingsof the First Workshopon Treebanks
62
andLinguisticTheories(TLT 2002),pages24–41,So-
zopol,Bulgaria.
Michael Daum, Kilian Foth, and Wolfgang Menzel.
2004. Automatictransformationof phrasetreebanks
to dependency trees. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation,LREC-2004,Lisbon,Portugal.
Kilian Foth. 2003. Eine umfassendeDependenzgram-
matik des Deutschen. Technicalreport, Fachbereich
Informatik,Universit¨atHamburg.
Johan Hall and Joakim Nivre. 2008. A dependency-
driven parser for German dependency and con-
stituency representations. In Proceedingsof the ACL
WorkshoponParsingGerman,Columbus,OH.
ErhardHinrichs,SandraK¨ubler, KarinNaumann,Heike
Telljohann, and Julia Trushkina. 2004. Recent de-
velopmentsinlinguisticannotationsof theT¨uBa-D/Z
treebank. In Proceedings of the Third Workshop
on Treebanks and Linguistic Theories, pages 51–62,
T¨ubingen,Germany.
SandraK¨ubler,ErhardW.Hinrichs,andWolfgangMaier.
2006. Is it really that difficult to parse German?
In Proceedings of the 2006 Conference on Empiri-
calMethodsinNaturalLanguageProcessing,EMNLP
2006,pages111–119,Sydney,Australia.
Sandra K¨ubler. 2005. How do treebank annotation
schemesinfluenceparsingresults?Orhownottocom-
pareapplesandoranges. In Proceedingsof the Inter-
national Conference on Recent Advances in Natural
Language Processing, RANLP 2005, pages 293–300,
Borovets,Bulgaria.
WolfgangMaier. 2006. Annotationschemesandtheirin-
fluenceonparsingresults. InProceedingsoftheACL-
2006StudentResearchWorkshop,Sydney,Australia.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Buildingalargeannotatedcor-
pus of English: The Penn Treebank. Computational
Linguistics,19(2):313–330.
S. Montegmagni,F. Barsotti, M. Battista, N. Calzolari,
O.Corazzari,A.Zampolli,F.Fanciulli,M.Massetani,
R. Raffaelli, R. Basili, M. T. Pazienza, D. Saracino,
F. Zanzotto, N. Mana, F. Pianesi, and R. Delmonte.
2000. The Italian syntactic-semantictreebank: Ar-
chitecture,annotation,tools and evaluation. In Pro-
ceedingsoftheWorkshoponLinguisticallyInterpreted
CorporaLINC-2000,pages18–27,Luxembourg.
JoakimNivre,JohanHall,SandraK¨ubler,RyanMcDon-
ald, Jens Nilsson,SebastianRiedel,andDenizYuret.
2007. The CoNLL2007 shared task on dependency
parsing. In Proceedings of the CoNLL 2007 Shared
Task.JointConferenceonEmpiricalMethodsinNatu-
ral Language Processingand ComputationalNatural
Language Learning, EMNLP-CoNLL 2007, Prague,
CzechRepublic.
Slav PetrovandDanKlein. 2008. ParsingGermanwith
languageagnosticlatent variablegrammars. In Pro-
ceedings of the ACL Workshop on Parsing German,
Columbus,OH.
AnnaRaffertyandChristopherManning. 2008. Parsing
three German treebanks: Lexicalized and unlexical-
ized baselines. In Proceedingsof the ACL Workshop
onParsingGerman,Columbus,OH.
Ines Rehbeinand Josef van Genabith. 2007. Treebank
annotationschemesandparserevaluationforGerman.
In Proceedingsof the 2007 Joint Conference on Em-
piricalMethodsin Natural Language Processingand
ComputationalNatural LanguageLearning, EMNLP-
CoNLL,pages630–639,Prague,CzechRepublic.
Geoffrey Sampson. 1993. The SUSANNE corpus.
ICAMEJournal,17:125–127.
Wojciech Skut, Thorsten Brants, Brigitte Krenn, and
Hans Uszkoreit. 1998. A linguisticallyinterpreted
corpus of German newspaper texts. In ESSLLI
Workshopon RecentAdvancesin CorpusAnnotation,
Saarbr¨ucken,Germany.
Heike Telljohann, Erhard W. Hinrichs, Sandra K¨ubler,
and Heike Zinsmeister, 2006. Stylebook for
the T¨ubingen Treebank of Written German (T¨uBa-
D/Z). Seminar f¨ur Sprachwissenschaft,Universit¨at
T¨ubingen,Germany.
ChristineThielenandAnneSchiller. 1994. Ein kleines
underweitertesTagsetf¨ursDeutsche.InHelmutFeld-
weg and Erhard Hinrichs, editors, Lexikon & Text,
pages215–226.Niemeyer,T¨ubingen.
Yannick Versley. 2005. Parser evaluation across text
types. InProceedingsoftheFourthWorkshoponTree-
banksandLinguisticTheories,TLT 2005,pages209–
220,Barcelona,Spain.
SeanWallisandGeraldNelson. 2006. TheBritishcom-
ponentoftheInternationalCorpusofEnglish.Release
2.CD-ROM.London:SurveyofEnglishUsage,UCL.
63

