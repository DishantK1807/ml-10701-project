Hearst developed a technique called TextTiling that automatically divides expository texts into multi-paragraph segments using the vector space model from IR (Hearst, 1994).
P94-1002
In previous work (Reynar, 1994), we described a method of finding topic boundaries using an optimisation algorithm based on word repetition that was inspired by a visualization technique known as dotplotting (Helfman, 1994).
P94-1050
coreference resolution), summarization, hypertext linking (Salton and Buckley, 1992), automated essay grading (Burstein et al., 1997) and topic detection and tracking (TDT program committee, 1998).
A97-1026
The row TextTiling shows the performance of the publicly available version of that algorithm (Hearst, 1994).
P94-1002
Bikel et al., 1997).
A97-1029
He identified topic boundaries where the LCP score was low (Kozima, 1993).
P93-1041
Hearst, 1994; Reynar, 1994; Beeferman et al., 1997).
P94-1002 P94-1050 W97-0304
Beeferman, Berger and Lafferty used the relative performance of two statistical language models and cue words to identify topic boundaries (Beeferman et al., 1997).
W97-0304
Beeferman et al., 1997), but the types of cue words used vary greatly.
W97-0304
Grosz and Sidner, 1986 or Hirschberg and Grosz, 1992).
H92-1089 J86-3001
Hirschberg and Litman, 1993) as well as some topic segmentation algorithms rely on cue words and phrases (e.g.
J93-3003
Richmond, Smith and Amitay designed an algorithm for topic segmentation that weighted words based on their frequency within a document and subsequently used these weights in a formula based on the distance between repetitions of word types (Richmond et al., 1997).
W97-0305
Our named entity recognizer used a maximum entropy model, built with Adwait Ratnaparkhi's tools (Ratnaparkhi, 1996) to label word sequences as either person, place, company or none of the above based on local cues including the surrounding words and whether honorifics (e.g.
W96-0213
Optimization is the algorithm we proposed in (Reynar, 1994).
P94-1050
see Hearst, 1994).
P94-1002
Both of these algorithms are applied to text following some preprocessing including tokenization, conversion to lowercase and the application of a lemmatizer (Karp et al., 1992).
C92-3145
Morris and Hirst developed an algorithm (Morris and Hirst, 1991) based on lexical cohesion relations (Halliday and Hasan, 1976).
J91-1002
First, most relevance metrics are based on word frequency, which can be viewed as a function of the topic being discussed (Church and Gale, 1995).
W95-0110
