Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 460–463, Prague, June 2007.
c©2007 Association for Computational Linguistics UTD-SRL: A Pipeline Architecture for Extracting Frame Semantic Structures Cosmin Adrian Bejan and Chris Hathaway Human Language Technology Research Institute The University of Texas at Dallas Richardson, TX 75083-0688, USA {ady,chris}@hlt.utdallas.edu Abstract This paper describes our system for the task of extracting frame semantic structures in SemEval 2007.
The system architecture uses two types of learning models in each part of the task: Support Vector Machines (SVM) and Maximum Entropy (ME).
Designed as a pipeline of classi ers, the semantic parsing system obtained competitive precision scores on the test data.
1 Introduction
The SemEval 2007 task for extracting frame semantic structures relies on the human annotated data available in the FrameNet (FN) database.
The Berkeley FrameNet project (Baker et al., 1998) is an ongoing effort of building a semantic lexicon for English based on the theory of frame semantics.
In frame semantics, the meaning of words or word expressions, also called target words (TW), comprises aspects of conceptual structures, or frames, that describe speci c situations.
The semantic roles, or frame elements (FE), associated with a target word are locally de ned in the frame evoked by the target word.
Currently, the FN lexicon includes more than 135,000 sentences extracted from the British National Corpus containing more than 6,100 target words that evoke more than 825 semantic frames.
For this task, we extended our previous work at Senseval-3 (Bejan et al., 2004) by (1) experimenting with additional features, (2) adding new classi cation sub-tasks to accomplish all the requirements, and (3) integrating these sub-tasks into a pipeline architecture.
2 System
Description Given a sentence, the frame semantic structure extraction task consists of recognizing the word expressions that evoke semantic frames, assigning the correct frame to them and, for each target word, detecting and labeling the corresponding frame elements properly.
The task also requires the determination of syntactic realizations associated to a frame element, such as grammatical function (GF) and phrase type (PT).
The following illustrates a sentence example annotated with frame elements together with their corresponding grammatical functions and phrase types for the target word “tie”: FE = Content2 GF = Dep PT = PP FE = Content1 GF = Ext PT = NP AEOI’s activities and facilities have been tied to several universities. Frame = Make_Cognitive_Connection evokes To extract semantic structures similar to those illustrated in the example we divide the SemEval 2007 task into four sub-tasks: (1) target word frame disambiguation (TWFD); (2) FE boundary detection (FEBD); (3) GF label classi cation (GFLC) and (4) FE label classi cation (FELC).
The sub-tasks TWFD and GFLC are natural extensions of the approach described in (Bejan et al., 2004) for the task of semantic role labeling at Senseval-03.
We design machine learning classi ers speci c for each of the four sub-tasks and arrange them in a pipeline architecture such that a classi er can use information predicted by its previous classi ers.
The system architecture is illustrated in Figure 1.
In the data processing step, we parse each sentence into a syntactic tree using the Collins parser and extract named entities using an in 460 Target word listTest data Named Entity Recognizer Test Data Target word Frame FE Boundary GF SVM model ME model SVM model ME model SVM model ME modelSVM model ME model ME trainSVM train Feature Extractor ME train one multi−class classifier SVM train Feature Extractor ME trainSVM train Feature Extractor one binary classifier ME trainSVM train Feature Extractor 556 multi−class classifiers 489 multi−class classifiers Test Data Train Data FeatureFeature Test Data Target word Frame FeatureFeature Test Data Target word Frame FE Boundary FN Annotation FE Boundary Predictor Frame Predictor Predictor FE Label Predictor Extractor Extractor Extractor Extractor GF Label Syntactic Parser GF Label Classification FE Label ClassificationFE Boundary DetectionFrame Disambiguation FrameNet Lexicon Data Processing Figure 1: System architecture.
house implementation of a named entity recognizer.
We also extract from the FN lexicon mappings of target words and the semantic frames they evoke.
Various features corresponding to constituents were extracted and passed to SVM and ME classi ers.
For example, in Figure 2, the frame disactivities NNS AEOI NNP POS ’s NP and CC facilities NNS NP JJ several Inhibit_movement Rope_manipulation Attaching Closure Activity_finish Finish_competition Immobilization Make_cognitive_connection Knot_creation Forming_relationships Frame Disambiguation positive negative FE Boundary Detection Head NULL Obj Quant Appositive Dep Ext Gen GF Classification VBP have VP VP VBN VBN been S tied NPto PP NNS universities VP Concept_1 Concept_2 Evidence Cognizer Concepts Time Place Circumstances Frequency FE Classif.
Figure 2: Classi cation examples for each sub-task.
ambiguation sub-task extracts features corresponding to the constituent tied in order to predict the right frame between the semantic frames that can be evoked by this target word.
In this gure, the correct categories for each sub-task are shown in boldface.
The complete set of features extracted for all the classi cation sub-tasks is illustrated in Figure 3.
These represent a subset of features used in previous works (Gildea and Jurafsky, 2002; Florian et al., 2002; Surdeanu et al., 2003; Xue and Palmer, 2004; Bejan et al., 2004; Pradhan et al., 2005) for automatic semantic role labeling and word sense disambiguation.
Figure 3 also indicates whether or not a feature is selected for a speci c classi cation task.
In the remaining part of this section we describe in detail each classi cation sub-task and the features that have the most salient effect on improving the corresponding classi ers.
2.1 Frame
Disambiguation In FrameNet, some target words can evoke multiple semantic frames.
In order to extract the semantic structure of an ambiguous target word, the rst step is to assign the correct frame to the target word in a given context.
This task is similar with the word sense disambiguation task.
We select from the FN lexicon 556 target words that evoke at least two semantic frames and have at least ve sentences annotated for each frame, and assemble a multi-class classi er for each ambiguous target word.
As described in Figure 3, for this task we extract features used in word sense disambiguation (Florian et al., 2002), lexical features of the target word, and NAMED ENTITY FLAGS associated with the root node in a syntactic parse tree.
For the rest of the ambiguous target words that have less than ve sentences annotated we randomly choose a frame as being the correct frame in a given context.
2.2 Frame
Element Identification The idea of splitting the automatic semantic role labeling task into FE boundary detection and FE label classi cation was rst proposed in (Gildea and Jurafsky, 2002) and then adopted by other works in this task.
The problem of detecting the FE boundaries is cast as the problem of deciding whether or not a constituent is a valid candidate for a FE.
461 TWFD GFLC Feature DescriptionNO NO Feature Description TWFD GFLCFEBD FEBD FELCFELC CW: The content word of the constituent computed as described in (Surdeanu et al., 2003); v20 CW POS: The POS corresponding to the content word;v21 CW STEM: Stemmed content word;v22 GOVERNING CATEGORY: Test whether the noun phrase constituents arevv23 dominated by verbal phrases or sentence phrases; SYNTACTIC DISTANCE: The length of the syntactic path;v24 PP FIRST WORD: If the constituent is a prepositional phrase, return the first word in the phrase; v25 HUMAN: Test whether the constituent phrase is either a personal pronoun or a hyponym of first sense of PERSON synset in WordNet; v26 CONSTITUENTS NUMBER: The number of candidate FEs;v27 CONSTITUENTS LIST: Constituents labels list of the candidate FEs;v28 SAME CLAUSE: Test whether the constituent is in the same clause withv29 the target word; GF: The grammatical function of a candidate frame element;v30 GF LIST: The list of grammatical functions associated to the candidate FEs;v31 FRAME: The name of the semantic frame that is evoked by the target word;vvv32 NP SISTER: Determine whether the constituent has a noun phrase sister;v33 FIRST/LAST WORD: Return the first/last word of the constituent phrase;v34 FIRST/LAST POS: Return the first/last POS in the constituent;vv35 LEFT/RIGHT SISTER LABEL: Return the left/right sibling constituent label;v36 LEFT/RIGHT SISTER HEAD: Return the left/right sibling head word;v37 LEFT/RIGHT SISTER STEM HEAD: Return the left/right sibling stemmedv38 head word; LEFT/RIGHT SISTER POS HEAD: Return the left/right sibling head POS;v39 HW POS: The syntactic head POS of the constituent; HW STEM: The stem word of the constituent’s head word; v v v 18 19 TW STEM & HW STEM: Join of TW STEM and HW STEM; TW STEM & PHRASE TYPE: Join of TW STEM and PHRASE TYPE; v v 40 41 VOICE & POSITION: Join of VOICE and POSITION.v42 TW UNIGRAMS: The words, stem words and part of speech (POS) unigramsv01 that are adjacent to target word expressions; TW BIGRAMS: The words, stem words and POS bigrams that are adjacent to02 target word expressions; TW WORD: The target word expression;03 TW STEM: The stem word(s) of the target word expression;v v04 v TW POS: The POS of the target word;v TW CLASS: The lexical class of the target word, e.g. verb, noun, adjective;vv06 05 NAMED ENTITY FLAGS: Set of binary features indicating whether a consti−vv07 tuent contains, is contained or exactly identifies a named entity; VERB WSD: If the target word is a verb, extract the head noun of the direct object and the prepositional object included in the verbal phrase; v08 v NOUN WSD: If the target word is a noun, extract the head word of the verbal phrase that is in a verb−subject or verb−object relation with the noun; 09 v ADJECTIVE WSD: If the target word is an adjective, extract the head noun that is modified by the adjective; 10 v PHRASE TYPE: The syntactic category of the constituent;vv11 DIRECTED PATH: Path in the syntactic parse tree between the constituent and the target word preserving the movement direction; vvv12 UNDIRECTED PATH: Same syntactic path as DIRECTED PATH without13 v preserving the movement direction; PARTIAL PATH: Path from the constituent to the earlier common ancestor of the target word and the constituent; v14 POSITION: Test whether the constituent contains the target word, or appears before or after the target word; vv v15 VOICE: Test if the verbal target word has active or passive construction;vv16 HW: The head word of the constituent;v vv17 Figure 3: Feature set for extracting frame semantic structures.
We consider a binary classi er over the entire FN data and extract features for each constituent from a syntactic parse tree.
Because this experimental setup allows training the binary classi er on a large set of examples, the best feature combination consists of a restrained number of features.
Most of these features are from the set proposed by (Gildea and Jurafsky, 2002).
Another feature that improved the prediction of FE boundaries in every feature selection experiment is the FRAME feature.
Since the frame disambiguation is executed before the FE boundary detection in the pipeline architecture, we can use the FRAME feature at this step.
This feature helps the binary classi er distinguish between frame element structures from different semantic frames.
2.3 Grammatical
Function Classification Once we identify the candidate boundaries for frame elements, the next step is to assign the grammatical functions to these boundaries.
In FrameNet, the grammatical functions represent the manner in which the frame elements satisfy grammatical constraints with respect to the target word.
For this task we train a multi-class classi er over the entire lexicon to predict seven categories of GFs that exist in FN.
In addition, we assign the NULL category for those FEs that double as target words.
The features are extracted only for the constituents that are identi ed as FEs in the previous FE boundary identi cation sub-task.
The best feature set in this phase includes the features proposed by (Gildea and Jurafsky, 2002) and the FRAME feature.
2.4 Frame
Element Classification The task of FE classi cation is to assign FE labels to every constituent identi ed as FE.
In order to predict the frame elements, which are locally de ned for each semantic frame, we built 489 multi-class classi ers, where each classi er corresponds to a frame in FrameNet.
This partitioning of the FN lexicon has the advantage of increasing the overall classi cation performance and ef ciently learning the frame elements labels.
On the other hand, this approach suffers from the lack of annotated data in some frames and hence it requires using a large set of features.
The advantage of designing the classi ers in a pipeline architecture is best illustrated in this subtask.
Some of the most effective features for FE classi cation are extracted using information from previous sub-tasks: FRAME feature is made available by the TWFD sub-task, CONSTITUENTS NUMBER and CONSTITUENTS LIST are made available by the FEBD sub-task, and GF and GF LIST are made available by the GFLC sub-task.
462 3 Experimental Results We report experimental results on all four classication sub-tasks.
In our experiments we trained two types of classi cation models for each sub-task: SVM and ME.
In order to optimize the performance measure of each sub-task and to nd the best con guration of classi cation models we used 20% of the sub-tasks training data as validation data.
Table 1 lists the best con guration of classi cation models as well as the best sub-task results when running the experiments on the validation data.
For frame disambiguation, we obtained 76.71% accuracy compared to a baseline of 60.72% accuracy that always predicts the most annotated frame for each of the 556 target words.
The results for GFLC and FELC sub-tasks listed in Table 1 were achieved by using gold FE boundaries.
Frame Disambiguation GF Label Classification FE Label Classification FE Boundary Detection Task SVM SVM 76.71 Best Model 96.00 88.93 ME ME Accuracy F1−measureRecall 73.65 Precision 87.08 79.80 Table 1: Task results on the validation set.
The SemEval 2007 organizers provided fully annotated training les, a scorer to evaluate these training les, and testing les containing at sentences.
In the evaluation process, a semantic dependency graph corresponding to a fully system annotated sentence is created and then matched with its gold dependency graph.
The matching process not only evaluates every semantic structure of a target word, but also considers frame-to-frame and FE-toFE graph relations between the semantic structures.
In addition, various scoring options were considered: exact or partial frame matching, partial credit for evaluating the named entities, evaluation of the at frame elements labels, and an option for matching only the frames in evaluation.
The evaluation for at frame elements labels is similar with the evaluation performed at Senseval-3.
The only difference is that for this scorer the FE boundaries must match exactly.
In Table 2, we present the averaged precision, recall and F1 measures for evaluating the semantic dependency graphs and detecting the semantic frames on the testing les.
The “Options” column represents the con guration parameters of the scorer: (E)xact/(P)artial frame matching, semantic (D)ependency or (L)abels only evaluation, and (Y)es/(N)o named entity evaluation.
E D N P D N E L Y P L Y E D Y P D Y E L N P L N Semantic Dependency Evaluation F1−measureRecallPrecision 51.10 50.29 54.78 51.85 51.38 56.13 55.56 56.59 27.74 27.05 29.48 27.59 26.95 29.45 30.19 30.14 35.88 35.11 38.26 35.94 35.29 38.57 39.04 39.25 69.16 71.69 80.35 69.16 71.69 80.35 77.82 77.82 42.73 44.43 49.79 42.73 44.43 49.79 48.09 48.09 52.71 54.74 61.35 52.71 54.74 61.35 59.32 59.32 Precision Recall F1−measureOptions Frame Detection Evaluation Table 2: System results on the test set.
Although the system achieved good precision scores on the test data, the recall values caused the system to obtain unsatisfactory F1-measure values.
We expect that the recall will increase by considering various heuristics for a better mapping of the frame elements to constituents in parse trees.
4 Conclusions
We described a system that participated in SemEval 2007 for the task of extracting frame semantic structures.
We showed that a pipeline architecture of the SVM and ME classi ers as well as an adequate selection of the classi cation models can improve the performance measures of each sub-task.
References Collin F.
Baker, Charles J.
Fillmore, and John B.
Lowe. 1998.
The Berkeley FrameNet project.
In Proceedings of the COLING-ACL, Montreal, Canada.
Cosmin Adrian Bejan, Alessandro Moschitti, Paul Mor arescu, Gabriel Nicolae, and Sanda Harabagiu.
2004. Semantic Parsing Based on FrameNet.
In Senseval-3: Workshop on the Evaluation of Systems for the Semantic Analysis of Text.
Radu Florian, Silviu Cucerzan, Charles Schafer, and David Yarowsky.
2002. Combining classi ers for word sense disambiguation.
Natural Language Engineering.
Daniel Gildea and Daniel Jurafsky.
2002. Automatic Labeling of Semantic Roles.
Computational Linguistic.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward, James H.
Martin, and Daniel Jurafsky.
2005. Support vector learning for semantic argument classi cation.
Journal of Machine Learning Research.
Mihai Surdeanu, Sanda M.
Harabagiu, John Williams, and Paul Aarseth.
2003. Using predicate-argument structures for information extraction.
In Proceedings of ACL.
Nianwen Xue and Marta Palmer.
2004. Calibrating features for semantic role labeling.
In Proceedings of EMNLP .

