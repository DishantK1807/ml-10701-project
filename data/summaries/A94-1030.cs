To alleviate the unknown word problem, we next experimented with augmenting the tokenizer's dictionary using CXtract, a statistical tool that finds morpheme sequences likely to be Chinese words (Fung & Wu 1994).
P94-1012
The unsupervised training procedure is described in detail in Fung & Wu (1994).
P94-1012
The text, drawn from the HKUST English-Chinese Parallel Bilingual Corpus (Wu 1994), consists of transcripts from the parliamentary proceedings of the Hong Kong Legislative Council.
P94-1012
Recent heavy activity in this area has shown the biggest stumbling block to be words that are absent from the lexicon, since successful tokenizers to date have been based on dictionary lookup (e.g., Chang &Chen 1993; Chiang et al.1992; Linet al.1993; Wu & Tseng 1993; Sproat et al.1994). We present empirical evidence for four points concerning tokenization of Chinese text: (I) More rigorous "blind" evaluation methodology is needed to avoid inflated accuracy measurements; we introduce the nk-blind method.
P94-1010 P94-1012 W93-0305
