H~ADING WITH A PURPOSE Michael Lebowitz Department of Computer Science, Yale University 1.
iNTRODUCTION A newspaper story about terrorism, war, politics or football is not likely to be read in the same way as a gothic novel, college catalog or physics textbook.
Similarly, tne process used to understand a casual conversation is unlikely to be the same as the process of understanding a biology lecture or TV situation comedy.
One of the primary differences amongst these various types of comprehension is that the reader or listener will nave different goals in each case.
The reasons a person nan for reading, or the goals he has when engaging in conversation wlll nave a strong affect on what he pays attention to, how deeply the input is processed, and what information is incorporated into memory.
The computer model of understanding described nere addresses the problem of using a reader's purpose to assist in natural language understanding.
This program, the Integrated Partial Parser (IPP) ~s designed to model the way people read newspaper stories in a robust, comprehensive, manner.
IPP nan a set of interests, much as a human reader does.
At the moment it concentrates on stories about International violence and terrorism.
IPP contrasts sharply wlth many other tecnniques which have been used in parslng.
Most models of language processing have had no purpose in reading.
They pursue all inputs with the same dillgence and create the same type of representation for all stories.
The key difference in IPP is that it maps lexlcal input into as high a level representation as possible, thereby performing the complete understanding process.
Other approaches have invariably first tried to create a preliminary representation, often a strictly syntactic parse tree, in preparation for real understandlng.
~ince high-level, semantic representations are ultimately necessary for understanding, there is no obvious need for creating a preliminary syntactic representation, which can be a very difficult task.
The isolation of the lexlcal level processing from more complete understanding processes makes it very difficult for hlgn level predictions to influence low-level processing, which is crucial in IPP.
One very popular technique for creating a low-level representation of sentences has been the Augmented Transition NetworX (ATN).
Parsers of this sort have been discussed by Woods \[ 11\] and Kaplan \[SJ.
An ATN-IiKe parser was developed by Winograd \[10\].
Most ATN parsers nave dealt primarily wltn syntax, occasionally checking a" few simple semantic properties of words.
A more recent parser wnicn does an isolated syntactic parse was created by Marcus \[4\].
TOe important thing to note about all of these parsers is that they view syntactic parsing as a process to be done prior to real understanding.
Even thougn systems of this sort at times make use of semantic information, they are driven by syntax.
Their ~oal of developing a syntactic parse tree is not an explicit part of the purpcse of human understanding.
the type of understanding done by IPP is in some sense a compromise between the very detailed understanding of This work was supported in part by the Advanced Research 8roJects A~enoy of the Department of Defense and monitored under the Office of Naval Research under contract N00014-75-C-1111.
SAM Ill and P~M \[9\], both of which operated in conjunction with ELI, Riesbeck's parser \[SJ, and the skimming, highly top-down, style of FRUMP \[2\].
EL1 was a semantically driven parser which maps English language sentences into the Conceptual Dependency \[6\] representations of their meanings, it made extensive use of the semantic properties of the words being processed, but interacted only slightly with the rest of the understanding processes it was a part of.
it would pass off a completed Conceptual Dependency representation of each sentence to SAM or PAM which would try to incorporate it into an overall story representation.
BOth these programs attempted to understand each sentence fully, SAM in terms of scripts, PAM in terms of plans and goals, before going onto the next sentence.
(In \[~\] Scnank and Abelson describe scripts, plans and goals).
SAM and PAM model the way people might read a story if they were expecting a detalied test on it, or the way a textbook might be read.
£acn program's purpose was to get out of a story every piece of informatlon possible, fney treated each piece of every story as being equally important, ~nd requiring total understanding.
Both of these programs are relatively fragile, requiring compiex dictionary entries for every word they might en0ounter, as well as extensive Knowledge of the appropriate scripts and plans.
FRÙMP, in contrast to SAM and rAM, is a robust system whlcn attempts to extract the amount of information from a newspaper story which a person gets when ne skims rapidly.
It does this by selecting a script to represent the story and then trying to fill in the various slots which are important to understand the story.
Its purpose is simply to obtain enough information from a story to produce a meaningful summary.
FRUMP is strongly top-down, and worries about incoming information from the story only insofar ~s it helps fill In the details of the script which it selected.
50 wnile FRUMP is robust, simply skipping over words it doesn't Know, it does miss interesting sections of stories which are not explained by its initial selection of a script.
18P attempts to model the way people normally read a newspaper story.
Unlike SAM and PAH, it does not care if it gets every last plece of information out of a story.
Dull, mundane information is gladly ignored.
But, In contrast with FRUMP, it does not want to miss interesting parts of stories simply because tney do not mesh with initial expectations.
It tries to create a representation which captures the important aspects of each story, but also tries to minimize extensive, unnecessary processing which does not contrlbute to the understanding of the story.
Thus IFP's purpose is to decide wnat parts of a story, if any, are interesting (in IPP's case, that means related to terrorism), and incorporate the appropriate information into its memory.
The concepts used to determine what is interesting are an extension of ideas presented by SctmnK \[7\].
2. How l~ EOA~s The ultimate purpose of reading a newspaper story is to incorporate new information into memory.
In order to do this, a number of different Kinds of Knowledge are needed.
The understander must Know the meanings of words, llngulatic rules about now words combine into sentences, the conventions used in writing newspaper 5g stories, and, crucially, have extensive knowledge about the "real world".
It is impossible to properly understand a story without applying already existing knowledge about the functioning of the world.
This means the use of long-term memory cannot be fruitfully separated from other aspects of the natural understandin~ problem.
The mana~emant of all this information by an understander is a critical problem In comprehension, since the application of all potentially relevant Knowledge all the time, would seriously degrade the understandin~ process, possibly to the point of halting It altogether.
In our model of understanding, the role played by the interests of the understander Is to allow detailed processing to occur only on the parts of the story which are Important to overall understanding, thereby conserving processing resources.
Central to any understandin~ system is the type of Knowledge structure used to represent stories.
At the present time, IPP represents stories in terms of scripts similar to, although simpler than, those used by SAM and FRUMP.
Most of the co--on events In IPP's area of Interest, terrorism, such as hiJaokings, kidnappings, and ambushes, are reasonanly stereotyped, although not necessarily wltn all the temporal sequencing present in the scripts SAM uses.
ZPP also represents some events directly In Conceptual Dependency.
The representations in IPP consist of two types of structures.
There are the event structures themselves, generally scripts such as $KIDNAP and SAMBUSH, which form the backbone of the story representations, and tokens which fill the roles in the event structures.
These tokens are basically the ?tcture Producers of \[6\], and represent the concepts underlying words such as "airliner," "machine-gun" and "Kidnapper".
The final story representation can also Include links between event structures indicating causal, temporal and script-scene relationships.
Due to IPP's limited repertoire of structures with which to represent events, it is currently unable to fully understand some stories which maXe sense only in terms of goals and plans, or other higher level representations.
However, the understanding techniques used in IPP should be applicable to stories which require the use of such knowledge structures.
This is a topic of current research.
It Is worth noting that the form of a story's representation may depend on the purpose behind its being read.
If the reader is only mildly Interested in the subject of the story, soriptal representation may well be adequate.
On the other hand, for an story of great interest to the reader, additional effort may be expended to allow the goals and plans of the actors In the story to be gorked out.
This Is generally more complex than simply representing a story in terms of stereotypical knowledge, and will only be attempted in cases of great interest.
In order to achieve its purpose, ~PP does extensive "top-down" processing.
That Is, It makes predlotions aOout what it is likely to see.
These predictions range from low-level, syntactic predictions ("the next noun phrase will be the person kidnapped," for instance) to quite high-level, global predictions, ("expect to see demands made by the terrorist").
Significantly, the program only makes predictions about things it would like to Know.
It doesn't mind skipping over unimportant parts of the text.
The top-down predictions made by IPP are implemented in terms of requests, similar to those used by RiesbecK \[5\], which are basically Just test-action pairs.
While such an implementation In theory allows arbitrary computations to ~e performed, the actions used in IPP are in fact quite limited.
IPP requests can build an event structure, link event structures together, use a token to fill a role in an event structure, activate new requests or de-activate other active requests.
The tests in IPP requests are also llmited in nature.
They can look for certain types of events or tokens, check for words with a specified property in their dictionary entry, or even check for specific lexical items.
The tests for lexical items are quite Important in Keeping IPP's processing efficient.
One advantage is that very specific top-down predictions will often allow an otherwise very complex word disa~biguation process to be bypassed.
For example, in a story about a hijacking, ZPP expects the word "carrying" to indicate that the passengers of the hijacked vehicle are to follow.
So it never has to consider An any detail the meaning of "carrying".
Many function words really nave no meaning by themselves, and the type of predictive processing used by IPP is crucial in handling them efficiently.
Despite its top-down orientation, IPP does not ignore unexpected Input.
Rather, If the new Information is interesting in itself the program will concentrate on it, makin~ new predictions In addition to, or instead of, the original ones.
The proper integration of top-down and bottom-up processing allows the program to be efficient, and yet not miss interesting, unexpected information.
The bottom-up processin~ of IPP is based around a ulassification of words that is done strictly on the basis of processing considerations.
IPP Is interested in the traditional syntactic classifications only when they help determine how worqs should be processed.
IPP's criteria for classification Involve the type of data structures words build, and when they should be processed.
Words can build either of the main data structures used in XPP, events and tokens.
The words bulldin~ events are usually verbs, but many syntactic nouns, such as • kidnapping," "riot," and "demonstration" also indicate events, and are handled in Just the same way as traditional verbs.
Some words, such as =oat adjectives and adverbs, do not build structures but rather modify structures built by other words.
These words are handled according to the type of structure they modify.
The second criteria for classifying words when they should be processed is crucial to 1PP's operation.
In order to model a rapid, normally paced reader, IPP attempts to avoid doin~ any processing which will not add to its overall understandin~ of a story.
To do this, it classifies words into three groups words which must be fully processed i--edlately, words which should be saved in short-ter~ memory, and then processed later, if ne,=essary, and words which should be skipped entirely.
Words which must be processed immediately include interesting words building either event structures or tokens.
"Gunmen," "kidnapped" and "exploded" are typical examples.
These words give us the overall framework of a story, indicate how much effort should 0e devoted to further analysis, and, most importantly, generate the predictions w~loh allow later processing to proceed efficiently.
The save and process later words are those which may become si~nifioant later, but are not obviously impor~cant when they are read.
This class is quite substantial, Including many dull nouns and nearly all adjectives and adverbs.
Zn a noun phrase sucn as "numerous Italian gunmen," there Is no point in processing tO any depth "numerous" or "Italian" until we ~now the word they modify is Important enou~n to be included in the final representation.
Zn the cases where further procesein~ is necessary, IPP has the proper information to easily incorporate the saved words Into the story representation, and In the many cases 60 where the word is not important, no effort above saving the word is required.
The processin~ strategy for these words is a Key to modei~n~ nom,al reading.
The final class of words are those IPP skips altogether.
Thls class includes very unlnterestln~ words whlch neither contribute processing clues, nor add to the story representation.
Many function words, adjectives and verbs irrelevant to the domain at hand, and most pronouns fall into this category.
These words can still be significant in cases where they are predlcted, but otherwise they are ignored by IPP and take no processln~ effort.
In addition to the processing techniques mentioned so far, IPP makes use of several very pragmatic heuristics.
These are particularly important in processlng noun ~roups properly.
An example of the type of heuristic used is IPP's assumption that the first actor in a story tends to be important, and is worth extra processing effort.
Other heurlst~cs can be seen in the example In section ~.
IP~'s basic strategy is to make reasonable guesses about the appropriate representation as qulcKly as possible, facilitating later processln~ and fix things later if its ~uesses are prove to be wrong.
~. ~ DETAILED ~XAMPLE ~n order to illustrate bow IPP operates, and how its purpose affects its process|n, an annotated run of IPP on a typical story, one taken from the Boston Globe is shown below.
The text between the rows of stars has been added to explain the operation of IPP.
Items beginning with a dollar sign, such as $rERRORISM, indicate scripts used by IPP to represent events.
\[PHOTO: Initiated Sun 24-Jun-79 3:36PM\] @RUN IPP *(PARSE $1) Input: $1 (3 I~ 79) IRELAND (GUNMEN FIRING FROM AMBUSH SERIOUSLY WOUNDED AN 8-YEAR-OLD GIRL AS SHE WAS BEING TAKEN TO SCHOOL YESTERDAY AT STEWARrSTOWN COUNTY r~RONNE) Processing: GUNMEN : InterestinE token GUNMEN Predictions SHOOTING-WILL-OCCUR ROBBERY-SCRIPT TERRORISM-SCRIPT HIJACKING-SCRIPT lll**lem*llllll*l*mli,lll,l,lll,l,mllll,mlm,lllilmm,illl GUNMEN is marked In the dlotionary as inherently interesting.
In humans this presumably occurs after a reader has noted that stories involving gunmen tend to be interesting.
Since it is interesting, IPP fully processes GUNMEN, Knowing that it Is important to its purpose of extracting the significant content of the story, it builds a token to represent the GUNMEN and makes several predlctlons to facilitate later processing.
There is a strong possibility that some verb conceptually equivalent to "shoot" will appear.
There are also a set of scripts, including SROBBERY, STERRORISM and $HIJACK wnlcn are likely to appear, so IPP creates predictions looking for clues indicating that one of these scripts sOould be activated and used to represent the story.
FIRING : Word satisfies prediction Prediction confirmed SHOOTING-WILL-OCCUR Instantiated $SHOOT script 61 Predictions ° $SHOOf-HUL::-FINUER REASON-FOR-SHOOtING $SHoor-scEN~S tJeiIJ~i~Jf~mmQll~l|l#~Oilm~i~Ome|J|i~|~i~iQltllliJIDI FIHING satisfies the predlction for a "shoot" verb.
Notice that tne prediction immediately dlsamblguates FIRING.
Other senses of the word, such as "terminate employment" are never considered.
Once IPP has confirmed an event, it builds a structure to represent it, in this case the $SHOOr script and the token for GUNMEN is filled in ss the actor.
Predictions are made trying to flnd the unknown roles of the script, VICTIM, in particular, the reason for the shooting, and any scenes of $SHOOT wnicn might be found.
JJJiJJJJJiJiJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJlJJJJJJJJJJJJJ instantiated $ATTACK-P~RSON script Predictions SAT rACK-PERSON-ROLE-FINDER.
SATrACK-PERSON-SC~N~S Im,*|i@m|li,I@Wm~#mI~@Igm#wIiII#mmimmIII|@milIIillJgimR@ IPP does not consider the $SHOOT script to be a total explanation of a snootin~ event.
It requires a representation wnlcn indicates the purpose of the various actors, in the absence of any other information, IPP assu~es people wno shoot are deliberately attacKin~ someone.
So the SATTACK-PERSON script is Inferred, and $SHOOT attacned to it as a scene.
The SATTACK-PERSON representation allows IPP to make inferences which are relevant to any case of a person being attacked, not just snootin~s.
IPP is still not able to Instantiate any of the high level scripts predicted by GUNMEN, since the SATTACK-PERSON script is associated with several of the~.
FROM : Function word Predictions FILL-FROM-SLOT Ji*JiJJeJ**JJJJiJJJJJJJlJJJJJJJJJ*JJJJ*JJJJ**J*JJJJJ*J*J FROM in s =ontext such as this normally indicates the location from which the attack was made is to follow, so IPP makes a prediction to that effect.
However, since a word building a token does not follow, the prediction is deactivated.
The fact that AMBUSH is syntactically a noun is not relevant, since iFP's prediction loo~s for a word which identifies a place.
li*JiJJ*Jll**J*lJli|iJl*lii|llll#*J**JiJJiJJ**iJil*iiJJ* AMBUSH : Scene word Predictions SAMBUSH-ROL~-FIND~R $AMBUSH-SCENKS Prediction confirmed TERRORISM-SCRIPT Instantlated $TERRORISM script Predictions TERRORIST-DEMANDS STERRORISM-ROLE-FINDER STERRORISM-SCENES COUNTER-MEASURES J*lJJJ*JiJJJJJJiJ*JJJJJJlJJJJJJJJJ*JJJi*JJ*JJJJ***JJJJ** IPP <nows the word AMBUSH to indicate an instance of the SAMBUSH scr|pt, and tn~t SAMBUSH can be a scene of $TERRORISM (i.e.
it is an activity w~Ich can be construed as a terrorist act).
This causes the prediction made by GUNMEN that $TERRORISM was a possible script tO be trlggerred.
Even if AMBUSH had other meanings, or could be associated with other higher level scripts, the prediction would enable quicK, accurate identification and incorporation of the word's meaning into the story representation.
IPP's purpose of associating the shooting with a nlgh level Knowledge structure which helps to expialn it, has been achieved.
At this point in the processing an Instance of STERRORISM is constructed to serve as the top level representation of the story.
The SAMBUSH and SATTACK-PERSON scripts are attached as scenes of STERRORISM.
SgRIOUSLY : SKip and save ~OUNO£D : Word satisfies prediction Prediction confirmed SWOUND-SCENE Predictions SWOUND-ROLE-FINDER SWOUND-SCENES t~e~eoeeeleleeeeeeelloeelem|eee|eoeeeeaoalenlo|eleeoeeee SWOUND is a Known scene of $ATTACK-PERSON, representin~ a common outcome of an attack.
It is instantlated and attached to $ATTACK-P~RSON.
IPP infers that the actor of SWOUND is probably the same as for $A~ACK-PERSON, i.e. the GUNMgN.
eleileleleeeelllllll|lllalllolsllieilllOlllelllel|oileil AN : SKip and save ~-YEAR-OLD : Skip and save GiRL : Normal token GIRL Prediction confirmed SWOUND-ROLE-FINDER-VICTIM eeee~eeeeeeme~eee~see~e~eee~m~ee~o~eeeeeeeeeee~aeeoee ~IRL Ouilds a toXen wnlch fllls t~e VICTIM role of the SWOUND script.
Since IPP has inferred that the VICTIM of the ~ATrACK-PERSON and SSHOOr scripts are the same as the VICTIM of SWOUND, it also fills in those roles.
Identifyin~ these roles is integral to IFP's purpose of understanding the story, since an attack on a person can only Oe properly understood if the victim is Known.
As t~is person is important to the understandln~ of the story, IPP wants to acquire as much information as possible about net.
Therefore, it looks baoK at the modifiers temporarily saved in short-term memory, 8-YEAR-OLD in this case, and uses them to modify the token ~uilt for GIRL.
The age of the ~Irl is noted as eight years.
This information could easily be crucial to appreciatin~ the interesting nature of the story.
@EeE~eeBe@~oeeEeeeeeeeE~e~aEeeoaeEsasee|eaeeeeeeeeEssee AS : SKip SHE : SKip WAS : SKip and save BEING : Dull verb skipped TAKEN : SKip TO : Function word SCHOOL : Normal token SCHOOL Y~ST~RDAY : Normal token YESTERDAY ~eee~ene~e~e~neeeeeaeeeeoeeeeeeeaeeeeeaeeeeeeeeeeeeeeee Nothin~ in this phrase is either inherently interesting or fulfills expectations made earlier in the processing of the story.
So it is all prc,:essed very superficially, addin~ nothing to the final representation.
It is important that IPP ma~es no attempt to dlsamOi~uate words such as TAKEN, an extremely complex process, since it knows none of the possible meanings will add significantly to its understanding.
@illIIIIIIIIIIIIIIIIIIIIIIIllOIIlllIIIIIiilIIIIIIIIilIII AT : Function word STEWARTSTOWN : Skip and save COUNTY : SKip and save TYRONNE : Normal token TYRONNE Prediction confirmed $T~RRORISH-ROLE-FIHDER-PLACE emmtu~u~eeeeteHeJ~eee~t~e~eeeeatteet~aaeaaeaeeesewaa ST£WARTSTOWN COUNTY rYRONNE satisfies the ?redlotlon for the place where the terrorism took plane.
IPP has inferred that all the scenes of the event took place at the same location.
IPP expends effort in identifying this role, as location is crucial to the understandln~ of most storles.
It is also important in the or~anizatlon of memories about stories.
A incidence of terrorism in Northern ireland is understood differently from one in New York or Geneva.
62 Story Representation: ee MAIN \[VENT ee SCRIPT $TERRORISM ACTOR GUNMEN PLACE $TEWARTSTOWN COUNTY TYRONNE TIHE ~ESTERDAY SCENES SCRIPT SAHBUSH ACTOR GUNMEN SCRIPT $ATTACK-PERSON ACTOR GUNMEN VICTIM 8 ~EAR OLD GIRL SCENES SCRIPT $SHOOT ACTOR GUNMEN VICTIM 8 XEAR OLD GIRL SCRIPT SWOUND ACTOR GUNMEN VICTIM 8 YEAR OLD GIRL EXTENT GREATERTHAN-nNORH e saesaeeeaeeeeseeeeeeeeeesseeesesesaeaeeoeeeeaeeeeeaeeeee IPP's final representation indicates that it has fulfilled its purpose in readimi the story.
It has extracted roughly the same information as a person reading the story quickly.
IPP has r~ognised an instance of terrorism oonststln8 of an ambush in whioh an eight year-old girl was wounded.
That seems to be about all a person would normally remember from suoha story.
eseeeeeeeeeae|eeeeeeesneeeeeaeeeeeeeeeeseeeeeeeaeeeeeese \[PHOTO: Terminated Sun 24-jun-79 3:38~\] As it pro~esses a story such as this one, IPF keeps track of how interesting it feels the story is.
Novelty and relevance tend to increase interestlngness, while redundancy and irrelevance dec?ease it.
For example, in the story shown moore, the faot that the victim of the shooting was an 8 year-old ingresses the interest of the story, and the the incident taMin~ place in Northern Ireland as opposed to a more unusual sate for terrorism decreases the interest.
The story's interest Is used to determine how much effort should be expended in tryin~ to fill in more details of t~e story.
If the level of lnterestingness decreases fax' enough, the program can stop processing the story, and look for a more interesting one, in the same way a person does when reading through a newspaper.
~. ANOTHER EXAMPLE The following example further illustrates the capabilities of IPP.
In this example only IPP's final story representation is snows.
This story was also taken from the Boston Globe.
\[PHOTO: Initiated Wed 27-Jun-79 I:OOPM\] @RUN IPP °(PARSE S2) Input: S2 (6 3 79) GUATEMA~t (THE SON OF FORMER PRESIDENT EUGENIC KJELL LAUGERUD WAS SHOT DEAD B~ UNIDENTIFIED ASSAILANTS LAST WEEK AND A BOMB EXPLODED AT THE HOME OF A GOVERNMENT OFFICIAL ~LICE SAID) Story Representation: am MAIN EVENF ea SCRIPT STERRORISM ACTOR UNKNOWN ASSAILANTS SCENES SCRIPT $ATTACK-PERSON ACTOR UNKNOWN ASSAILANTS VICTIM SON OF PREVIOUS PRESIDENT EUGENIC KJELL LAUG~RUD SCENES SCRIPT $SHOOT ACTOR UNKNOWN ASSAILANTS VICTIM SON OF PREVIOUS PRESIDENT EUGENIC KJELL LAUGERUD SCRIPT SKill ACTOR UNKNOWN ASSAILANTS VICTIM SON OF PREVIOUS PRESIDENT EUGENIC KJELh LAUG~RUD SCRIPT SATTACK-PLAC£ ACTOR UNKNOWN ASSAILANTS PLACE HOME OF GOVERNMENT OFFICIAL SC~NdS SCRIPT $BOHB ACTOR UNKNONN ASSAILANTS PLACE HOME OF GOVERNMENT OFFICIAL \[PHOTO: Terminated Wed 27-Jun-79 I:09PM\] Thls example maces several interesting points about the way IPP operates.
Notice that 1PP has jumped to a conclusion about the story,, which, while plausible, could easily be wrong, it assumes that the actor of the SBOMB and SATTACK-PLACE scripts is the same as the actor of the STERRORISM script, which was in turn inferred from the actor of the sbootln~ incident.
Tnls is plausible, as normally news stories are about a coherent set of events witn lo~Ical relations amongst them.
So it is reasonable for a story to De about a series of related acts of terrorism, committed by the same person or ~roup, and tnat is what IPP assumes here even though that may not be correct.
Uut this ~Ind of inference is exactly the Kind which IPP must make in order to do efficient top-down processln~, despite the possibility of errors.
The otner interesting point about tnis example is the way some of iPP's quite pragmatic heuristics for processln~ give positive results.
For instance, as mentioned earlier, the first actor mentioned has a stronz tendency to be important to the understandln~ of a story.
In thls story that means that the modlfyin~ prepositional phrase "of former President Su~enlo Kjell Lau~erud" is analyzed and attached to the token built for "son," usually not an interesting word.
Heur~stlcs of this sort ~ive IPP its power and robustness, rather than any single rule about language understandln~.
5. CONCLUSION IPP has been implemented on a DECsystem 20/50 at Yale.
It currently has a vocabulary of more than I~00 words wnlcn is oelng continually Increased in an attempt to make the program an expert underst~der of newspaper stories scout terrorism.
£t is also planned to add information about nigher level knowledge structures such as ~oals and plans and expand IPP's domain of interest.
To date, IPP has successfully processed over 50 stories taken directly from various newspapers, many sight unseen.
The difference between the powers of IPP and the syntactlcally driven parsers mentioned earller can cent be seen by the Kinds of sentences they handle.
Syntax-0ased parsers generally deal with relatively simple, syntactically well-formed sentences.
IPP handles sucn sentences, Out also accurately processes stories taken directly from newspapers, which often involve extremely convoluted syntax, and in many cases are not grammatical at all.
Sentences of this type are difficult, if not impossible for parsers relyln~ on syntax.
IPP is sole to process news stories quickly, on the order of 2 CPU seconds, and when done, it has achieved a complete understandln~ of the story, not Just a syntactic parse.
As shown in tne examples above, interest can provide a purpose for reading newspaper stories.
In other situations, other factors might provide the purpose.
But the purpose is never simply to create a representation especially a representation with no semantic content, such as a syntax tree.
This is not to say syntax is not important, obviously in many circumstances it provides crucial information, but it should not drive the understanding process.
Preliminary representations are needed only if they assist in the reader's ultimate purpose bulldln~ an appropriate, high-level representation which can be incorporated with already existing Knowledge.
The results achieved by IPP indicate that parsing directly into high-level knowledge structures is possible, and in many situations may well be more practical than first doin~ a low-level parse.
Its integrated approacn allows IPP to make use of all the various kinds of knowledge which people use when understandtn~ a story.
References \[1\] Cullin&ford, R.
(1978) Script application: Computer understanding of newspaper stories.
Research Report 116, Department of Computer Science, Yale University.
\[2\] DeJon~, G.F.
(19/9) Skimming stories in real time: An experiment in integrated understanding.
Research Report 158, Department of Computer Science, Yale University.
\[3\] Kaplan, R.M.
(1975) On process models for sentence analysis, in D.A.
Norman and D.
E. R~elhart, ads., Explorations in ~oanition.
W. H.
Freeman and Company, San Francisco.
\[~\] Marcus, M.P.
(1979) A Theory of Syntactic Recognition for Natural Language, in P H. Winston and R.H.
Brown (eds.), Artificial IntellJ~ence: an,~ Presnectlve, HIT Press, Cambridge, Massachusetts.
\[5\] Riesbeck, C.
K. (1975) Conceptual analysis.
In R.C.
ScnanK (ed.),.
~ Information Processing.
North Holland, Amsterdam.
\[6\] Scnank, R.C.
(1975) Conceotual Information Processln¢.
North Holland, Amsterdam.
\[7\] Scnank, R.
C. (1978) Interestlngness: Controlling inferences.
Research Report I~5, Department of Computer Science, Yale University.
\[8\] Scbank, R.
C. and Abelson, R.
P. (1977) Scrints.
Plans, Goals and Understanding.
Lawrence grlbaum Associates, Rlllsdale, New Jersey.
\[9\] dllensky, R.
(1978) Understanding goal-based stories.
Research Report I~0, Department of Computer Science, Yale University.
\[10\] Wtnograd, T.
(1972) Understandin~ Natural Lan:uafe.
Academic Press, New York.
\[11\] ~oods, W.
A. (1970) Transition network grammars for natural language analysis.
~of the ACH.
Vol. 13, p 591. 63

