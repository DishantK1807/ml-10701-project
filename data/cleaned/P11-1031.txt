Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 299–308,
Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
Word Maturity: Computational Modeling of Word Knowledge 
Kiril Kireyev   Thomas K Landauer 
 
Pearson Education, Knowledge Technologies 
Boulder, CO 
 
{kirill.kireyev, tom.landauer}@pearson.com
 
 
Abstract 
While computational estimation of dificulty 
of words in the lexicon is useful in many edu-
cational and assessment applications, the 
concept of scalar word dificulty and curent 
corpus-based methods for its estimation are 
inadequate. We propose a new paradigm 
called word meaning maturity which tracks 
the degre of knowledge of each word at dif-
ferent stages of language learning. We pre-
sent a computational algorithm for estimating 
word maturity, based on modeling language 
acquisition with Latent Semantic Analysis. 
We demonstrate that the resulting metric not 
only corelates well with external indicators, 
but captures deeper semantic efects in lan-
guage. 
1 Motivation

It is no surprise that through stages of language 
learning, different words are learned at different 
times and are known to different extents. For ex-
ample, a comon word like “dog” is familiar to 
even a first-grader, whereas a more advanced 
word like “focal” does not usually enter learners’ 
vocabulary until much later. Although individual 
rates of learning words may vary betwen high
and low-performing students, it has been observed 
that “children […] acquire word meanings in 
roughly the same sequence” (Biemiler, 208). 
The aim of this work is to model the degre of 
knowledge of words at diferent learning stages. 
Such a metric would have extremely useful apli-
cations in personalized educational technologies, 
for the purposes of acurate asesment and per-
sonalized vocabulary instruction. 
2 Rethinking
Word Difficulty 
Previously, related work in education and psy-
chometrics has been concerned with measuring 
word dificulty or clasifying words into diferent 
dificulty categories. 
Examples of such aproaches include creation 
of word lists for targeted vocabulary instruction at 
various grade levels that were compiled by educa-
tional experts, such as Nation (1993) or Biemiler 
(208). Such word dificulty asignments are also 
implicitly present in some readability formulas 
that estimate dificulty of texts, such as Lexiles 
(Stener, 196), which include a lexical difficulty 
component based on the frequency of occurence 
of words in a representative corpus, on the as-
sumption that word dificulty is inversely corelat-
ed to corpus frequency. Additionaly, research in 
psycholinguistics has atempted to outline and 
measure psycholinguistic dimensions of words 
such as age-of-acquisition and familiarity, which 
aim to track when certain words become known 
and how familiar they appear to an average per-
son. 
Importantly, al such word dificulty measures 
can be thought of as functions that assign a single 
scalar value to each word w: 
!"##"$%&’(∶!→ℝ (1) 
There are several important limitations to such 
metrics, regardles of whether they are derived 
from corpus frequency, expert judgments or other 
measures. 
First, learning each word is a continual proces, 
one that is interdependent with the rest of the vo-
cabulary. Wolter (201) writes: 
299
[…] Knowing a word is quite often not an either-or 
situation; some words are known well, some not at 
all, and some are known to varying degres. […] How 
wel a particular word is known may condition the 
conections made between that particular word and 
the other words in the mental lexicon. 
Thus, instead of modeling when a particular 
word wil become fuly known, it makes more 
sense to model the degre to which a word is 
known at diferent levels of language exposure. 
Second, word dificulty is inherently perspec-
tival: the degree of word understanding depends 
not only on the word itself, but also on the sophis-
tication of a given learner. Consider again the dif-
ference betwen “dog” and “focal”: a typical first-
grader wil have much more dificulty understand-
ing the latter word compared to the former, where-
as a well-educated adult wil be able to use these 
words with equal ease. Therefore, the degre, or 
maturity, of word knowledge is inherently a func-
tion of two parameters -word w and learner level 
l: 
!"#$%&#’∶!,!→ℝ (2) 
As the level l increases (i.e. for more advanced 
learners), we would expect the degre of under-
standing of word w to aproach its full value cor-
responding to perfect knowledge; this wil hapen 
at diferent rates for diferent words. 
Idealy, we would obtain maturity values by 
testing word knowledge of learners acros differ-
ent levels (ages or schol grades) for all the words 
in the lexicon. Such a procedure, however, is pro-
hibitively expensive; so instead we would like to 
estimate word maturity by using computational 
models. 
To sumarize: our aim is to model the devel-
opment of meaning of words as a function of in-
creasing exposure to language, and ultimately the 
degre to which the meaning of words at each 
stage of exposure resemble their “adult” meaning. 
We therefore define word meaning maturity to be 
the degre to which the understanding of the word 
(expected for the average learner of a particular 
level) resembles that of an ideal mature learner. 
3 Modeling
Word Meaning Acquisition 
with Latent Semantic Analysis 
3.1 Latent
Semantic Analysis (LSA) 
An apealing choice for quantitatively modeling 
word meanings and their growth over time is La-
tent Semantic Analysis (LSA), an unsupervised 
method for representing word and document 
meaning in a multi-dimensional vector space. 
The LSA vector representation is derived in an 
unsupervised manner, based on occurrence pat-
terns of words in a large corpus of natural lan-
guage documents. A Singular Value 
Decomposition on the high-dimensional matrix of 
word/document occurrence counts (A) in the cor-
pus, followed by zeroing all but the largest r ele-
ments
1
 of the diagonal matrix S, yields a lower-
rank word vector matrix (U). The dimensionality 
reduction has the efect of smothing out inci-
dental co-occurrences and preserving significant 
semantic relationships betwen words. The result-
ing word vectors
2
 in U are positioned in such a 
way that semanticaly related words vectors point 
in similar directions or, equivalently, have higher 
cosine values betwen them. For more details, 
please refer to Landauer et al. (2007) and others. 
 
 
Figure 1. The SVD proces in LSA ilustrated. The original 
high-dimensional word-by-document matrix A is decomposed 
into word (U) and document (V) matrices of lower dimen-
sionality. 
In adition to merely measuring semantic relat-
edness, LSA has ben shown to emulate the learn-
ing of word meanings from natural language (as 
can be evidenced by a broad range of applications 
from synonym tests to automated esay grading), 
at rates that resemble those of human learners 
(Laundauer et al, 197). Landauer and Dumais 
(197) have demonstrated empirically that LSA 
can emulate not only the rate of human language 
acquisition, but also more subtle phenomena, such 
as the effects of learning certain words on mean-
ing of other words. LSA can model meaning with 
                                                             
1
 Typicaly the first aprox. 30 dimensions are retained 
2
 UΣ is used to project word vectors into V-space 
SVD x x
Document	  VectorsWord	  Vectors
Original	  Matrix
word	  1
word	  2
word	  n
do
c
	  1
do
c
	  2
do
c
	  m.	  .	  .
.	  
.	  
.
r
r
r
r
 
A 
U 
S V 
Σ 
300
high acuracy, as atested, for example, by 90% 
corelation with human judgments on assessing 
the quality of student essay content (Landauer, 
2002). 
3.2 Using
LSA to Compute Word Maturity 
In this work, the general procedure behind 
computationally estimating word maturity of a 
learner at a particular intermediate level (i.e. age 
or schol grade level) is as folows: 
1. Create an intermediate corpus for the given 
level. This corpus approximates the amount 
and sophistication of language encountered 
by a learner at the given level. 
2. Build an LSA space on that corpus. The re-
sulting LSA word vectors model the mean-
ing of each word to the particular 
intermediate-level learner. 
3. Compare the meaning representation of each 
word (its LSA vector) to the coresponding 
one in a reference model. The reference 
model is trained on a much larger corpus 
and approximates the word meanings by a 
mature adult learner. 
 
We can repeat this proces for each of a num-
ber of levels. These levels may directly correspond 
to schol grades, learner ages or any other arbi-
trary gradations. 
In sumary, we estimate word maturity of a 
given word at a given learner level by comparing 
the word vector from an intermediate LSA model 
(trained on a corpus of size and sophistication 
comparable to that which a typical real student at 
the given level encounters) to the coresponding 
vector from a reference adult LSA model (trained 
on a larger corpus corresponding to a mature lan-
guage learner). A high discrepancy betwen the 
vectors would sugest that an intermediate mod-
el’s meaning of a particular word is quite different 
from the reference meaning, and thus the word 
maturity at the coresponding level is relatively 
low. 
3.3 Procrustes
Alignment (PA) 
Comparing vectors acros diferent LSA spaces 
is les straightforward, since the individual dimen-
sions in LSA do not have a meaningful interpreta-
tion, and are an artifact of the content and ordering 
of the training corpus used. Therefore, direct com-
parisons across two diferent spaces, even of the 
same dimensionality, are meaningless, due to a 
mismatch in their cordinate systems. 
Fortunately, we can employ a multivariate al-
gebra technique known as Procrustes Alignment 
(or Procrustes Analysis) (PA) typically used to 
align two multivariate configurations of a core-
sponding set of points in two different geometric 
spaces.  PA has ben used in conjunction with 
LSA, for example, in cros-language information 
retrieval (Litman, 198). 
The basic idea behind PA is to derive a rotation 
matrix that allows one space to be rotated into the 
other. The rotation matrix is computed in such a 
way as to minimize the diferences (namely: sum 
of squared distances) betwen corresponding 
points, which in the case of LSA can be common 
words or documents in the training set. 
For more details, the reader is advised to con-
sult chapter 5 of (Krzanowski, 200) or similar 
literature on multivariate analysis. In sumary, 
given two matrices containing coordinates of n 
coresponding points X and Y (and asuming 
mean-centering and equal number of dimensions, 
as is the case in this work), we would like to min-
imize the sum of squared distances betwen the 
points: 
!
!
= !
!"
−!
!"
!
!
!!
!
!!
 
We try to find an orthogonal rotation matrix Q, 
which minimizes M
2  
by rotating Y relative to X. 
That matrix can be obtained by solving the equa-
tion: 
!
!
=!"#$%(!
!
+!
!
−2!
!!
) 
It turns out that the solution to Q is given by VU’, 
where UΣV’ is the singular value decomposition 
of the matrix X’Y. 
In our situation, where there are two spaces, 
adult and intermediate, the alignment points are 
the coresponding document vectors corespond-
ing to the documents that the training corpora of 
the two models have in comon (recall that the 
adult corpus is a superset of each of the intermedi-
ate corpora).  The result of the Procrustes Align-
ment of the two spaces is efectively a joint LSA 
space containing two distinct word vectors for 
each word (e.g. “dog1”, “dog2”), coresponding to 
the vectors from each of the original spaces. After 
301
merging using Procrustes Alignment, the compari-
son of word meanings becomes a simple problem 
of comparing word vectors in the joint space using 
the standard cosine metric. 
4 Implementation
Details 
In our experiments we used pasages from the 
MetaMetrics Inc. 202 corpus
3, largely consisting 
of educational and literary content representative 
of the reading material used in American schools 
at diferent grade levels. The average length of 
each passage is approximately 135 words. 
The first-level intermediate corpus was com-
posed of 6,00 text pasages, intended for schol 
grade 1 or below. The grade level is approximated 
using the Coleman-Liau readability formula 
(Coleman, 1975), which estimates the US grade 
level necesary to comprehend a given text, based 
on its average sentence and word length statistics: 
!"#=0.0588!−0.296!−15.8 (4) 
where L is the average number of letters per 10 
words and S is the average number of sentences 
per 100 words. 
 Each subsequent intermediate corpus contains 
additional 6,00 new pasages of the next grade 
level, in adition to the previous corpus. In this 
way, we create 14 levels. The adult corpus is twice 
as large, and of same grade level range (0-14) as 
the largest intermediate corpus. 
In summary, the folowing describes the size 
and makeup of the corpora used: 
 
Corpus Size 
(pasages) 
Approx. Grade Level 
(Coleman-Liau Index) 
Intermediate 1 6,00 0.0 1.0 
Intermediate 2 12,00 0.0 2.0 
Intermediate 3 18,00 0.0 3.0 
Intermediate 4 24,00 0.0 4.0 
…   
Intermediate 14 84,000 0.0 14.0 
Adult 168,000 0.0 14.0 
Table 1. Size and makeup of corpora. used for LSA models. 
The particular choice of the Coleman-Liau 
readability formula (CLI) is not essential; our ex-
periments show that other wel-known readability 
formulas (such as Lexiles) work equaly wel. Al 
that is neded is some approximate ordering of 
                                                             
3
 We would like to acknowledge Jack Stenner and MetaMet-
rics for the use of their corpus. 
pasages by dificulty, in order to miic the way 
typical human learners encounter progresively 
more difficult materials at sucesive schol 
grades. 
After creating the corpora, we: 
1. Build LSA spaces on the adult and each of 
the intermediate corpora 
2. Merge the intermediate space for level l 
with the adult space, using Procrustes Alignment. 
This results in a joint space with two sets of vec-
tors: the versions from the intermediate space 
{vl
w
}, and adult space{va
w
}. 
3. Compute the cosine in the joint space be-
twen the two word vectors for the given word w 
!"!,!=!"#  (!"
!,!"
!
) (5) 
In the cases where a word w has not been encoun-
tered in a given intermediate space, or in the rare 
cases where the cosine value falls below 0, the 
word maturity value is set to 0. Hence, the range 
for the word maturity function fals in the closed 
interval [0.0, 1.0]. A higher cosine value means 
greater similarity in meaning betwen the refer-
ence and intermediate spaces, which implies a 
more mature meaning of word w at the level l, i.e. 
higher word meaning maturity. The scores be-
twen discrete levels are interpolated, resulting in 
a continuous word maturity curve for each word. 
Figure 1 below ilustrates resulting word ma-
turity curves for some of the words. 
!"
!#$"
!#%"
!#&"
!#'"
("
!"("$")"%"*"&"+"'","(!"(("($"()"(%"-."
!
"
#
$
%
&'
(
)
#
*
(
+
%,-.-/%
/01"
234567"
846/9204"
:0;9<"
Figure 2. Word maturity curves for selected words. 
Consistent with intuition, simple words like “dog” 
approach their adult meaning rather quickly, while 
“focal” takes much longer to become known to 
any degree. 
An interesting example is “turkey”, which has 
a noticeable plateau in the midle. This can be 
explained by the fact that this word has two dis-
tinct senses. Closer analysis of the corpus and the 
semantic near-neighbor word vectors at each in-
302
termediate space, shows that earlier meaning deal 
almost exclusively with the first sense (bird), 
while later readings with the other (country). 
Therefore, even though the word “turkey” is quite 
prevalent in earlier readings, its full meaning is not 
learned until later levels. This demonstrates that 
our method takes into acount the meaning, and 
not merely the frequency of occurrence. 
5 Evaluation

5.1 Time-to-maturity 
Evaluation of the word maturity metric against 
external data is not always straightforward be-
cause, to the best of our knowledge, data that con-
tains word knowledge statistics at diferent learner 
levels does not exist. Instead, we often have to 
evaluate against external data consisting of scalar 
dificulty values (se Section 2 for discusion) for 
each word, such as age-of-acquisition norms de-
scribed in the folowing subsection. 
There are two ways to make such comparisons 
possible. One is to compute the word maturity at a 
particular level, obtaining a single number for 
each word. Another is by computing time-to-
maturity: the minimum level (the value on the x-
axis of the word maturity graph) at which the word 
maturity reaches
4
 a particular threshold α: 
 !!"!=min!!.!.!"!,!>!       (6) 
Intuitively, this measure coresponds to the age 
in a learner’s development when a given word be-
comes suficiently understod. The parameter α 
can be estimated empirically (in practice α=0.45 
gives good correlations with external measures). 
Since the values of word maturity are interpolated, 
the ttm(w) can take on fractional values. 
It should be emphasized that such a colapsing 
of word maturity into a scalar value inherently 
results in los of information; we only perform it 
in order to allow evaluation against external data 
sources. 
As a baseline for these experiments we include 
word frequency, namely the document frequency 
of words in the adult corpus. 
                                                             
4
 Values between discrete levels are obtained using piecewise 
linear interpolation 
5.2 Age-of-Acquisition Norms 
Age-of-Acquisition (AoA) is a psycholinguistic 
property of words originaly reported by Carol & 
White (1973). Age of Acquisition aproximates 
the age at which a word is first learned and has 
been proposed as a significant contributor to lan-
guage and memory proceses. With some excep-
tions, AoA norms are collected by subjective 
measures, typicaly by asking each of a large 
number of participants to estimate in years the age 
when they have learned the word. AoA estimates 
have been shown to be reliable and provide a valid 
estimate for the objective age at which a word is 
acquired; see (Davis, in pres) for references and 
discussion. 
In this experiment we compute Spearman cor-
relations betwen time-to-maturity and two avail-
able colections of AoA norms: Gilholy et al., 
(1980) norms
5, and Bristol norms
6
 (Stadthagen-
Gonzalez et al., 2010). 
Measure Gilholy 
(n=1643) 
Bristol 
(n=1402) 
(-) Frequency 
0.59 0.59 
Time-to-Maturity (α=0.45) 0.72 0.64 
Table 2. Corelations with Age of Acquisition norms. 
5.3 Instruction
Word Lists 
In this experiment, we examine leveled lists of 
words, as created by Biemiler (208) in the bok 
entitled “Words Worth Teaching: Closing the Vo-
cabulary Gap”. Based on results of multiple-
choice word comprehension tests administered to 
students of different grades as wel as expert 
judgments, the author derives several word difi-
culty lists for vocabulary instruction in schols, 
including: 
o Words known by most children in grade 2 
o Words known by 40-80% of children in 
grade 2 
o Words known by 40-80% of children in 
grade 6 
o Words known by fewer than 40% of chil-
dren in grade 6 
One would expect the words in these four groups 
to increase in difficulty, in the order they are pre-
sented above. 
                                                             
5
 http:/ww.psy.uwa.edu.au/mrcdatabase/uwa_mrc.htm 
6
 http:/language.psy.bris.ac.uk/bristol_norms.html 
303
To verify how these word groups corespond to 
the word maturity metric, we asign each of the 
words in the four groups a dificulty rating 1-4 
respectively, and measure the correlation with 
time-to-maturity. 
 
 
Measure Corelation 
(-) Frequency 0.43 
Time-to-maturity (α=0.45) 0.49 
Table 3. Corelations with instruction word lists (n=4176). 
The word maturity metric shows higher corela-
tion with instruction word list norms than word 
frequency. 
5.4 Text
Complexity 
Another way in which our metric can be evaluated 
is by examining the word maturity in texts that 
have been leveled, i.e. have ben asigned ratings 
of difficulty. On average, we would expect more 
dificult texts to contain more dificult words. 
Thus, the corelation betwen text dificulty and 
our word maturity metric can serve as another val-
idation of the metric. 
For this purpose, we obtained a collection of 
readings that are used as reading comprehension 
tests by different state websites in the US
7
. The 
colection consists of 1,20 readings, each anno-
tated with a US schol grade level (in the range 
betwen 3-12) for which the reading is intended. 
The average length each pasage was aproxi-
mately 489 words. 
In this experiment we computed the correlation 
of the grade level with time-to-maturity, and two 
other measures, namely: 
• Time-to-maturity: average time-to-
maturity of unique words in text (excluding 
stopwords) with α=0.45. 
• Coleman-Liau. The Coleman-Liau reada-
bility index (Equation 4). 
• Frequency. Average of corpus log-
frequency for unique words in the text, ex-
cluding stopwords. 
 
 
 
 
                                                             
7
 The colection was created as part of the “Aspects of Text 
Complexity” project funded by the Bil and Melinda Gates 
Foundation, 2010. 
Measure Corelation 
Frequency 
(avg. of unique words) 
0.60 
Coleman-Liau 0.64 
Time-to-maturity (α=0.45) 
(avg. of unique non-stopwords) 
0.70 
Table 4. Corelations of grade levels with diferent metrics. 
6 Emphasis
on Meaning 
In this section, we would like to highlight certain 
properties of the LSA-based word maturity metric, 
particularly aiming to ilustrate the fact that the 
metric tracks acquisition of meaning from expo-
sure to language and not merely more shalow ef-
fects, such as word frequency in the training 
corpus. 
6.1 Maturity
based on Frequency 
For a baseline that does not take meaning into ac-
count, let us construct a set of maturity-like curves 
based on frequency statistics alone. More specifi-
cally, we define the frequency-maturity for a par-
ticular word at a given level as the ratio of the 
number of occurrences at the intermediate corpus 
for that level (l) to the number of ocurrences in 
the reference corpus (a): 
!"!,!=
!"#_!""#$
!
()
!"#_!""#$
!
()
 
Similarly to the original LSA-based word maturity 
metric, this ratio increases from 0 to 1 for each 
word as the amount of cumulative language expo-
sure increases. The corpora used at each interme-
diate level are identical to the original word 
maturity model, but instead of creating LSA spac-
es we simply use the corpora to compute word 
frequency. 
The folowing figure shows the Spearman cor-
relations betwen the external measures used for 
experiments in Section 5, and time-to-maturity 
computed based on the two maturity metrics: the 
new frequency-based maturity and the original 
LSA-based word maturity. 
304
 
Figure 3. Corelations of word maturity computed using fre-
quency (as wel as the original) against external metrics de-
scribed in Section 5. 
 
The results indicate that the original LSA-based 
word maturity corelates beter with real-world 
data than a maturity metric simply based on fre-
quency. 
 
6.2 Homographs

Another insight into the fact that the LSA-based 
word maturity metric tracks word meaning rather 
than mere frequency may be gained from analysis 
of words that are homographs: words that contain 
two or more unrelated meanings in the same writ-
ten form, such as the word “turkey” ilustrated in 
Section 4. (This is related to but distinct from the 
merely polysemous words that have several related 
meanings), 
Because of the conflation of several unrelated 
meanings into the same orthographic form, homo-
graphs implicitly contain more semantic content in 
a single word. Therefore, one would expect the 
meaning of homographs to mature more slowly 
than would be predicted by frequency alone: all 
things being equal, a learner has to learn the mean-
ings for all of the senses of a homograph word 
before the word can be considered fully known. 
More specificaly, one would expect the time-
to-maturity of homographs to have greater values 
than words of similar frequency. To test this hy-
pothesis, we obtained
8
 a list 174 comon English 
homographs. For each of them, we compared their 
time-to-maturity to the average time-to-maturity of 
words that have the same (+/1%) corpus fre-
quency. 
                                                             
8
 http:/en.wikipedia.org/wiki/List_of_English_homographs 
The results of a paired t-test confirms the hy-
pothesis that the time-to-maturity of homographs 
is greater than other words of the same frequency, 
with the p-value = 5.9e
-6
. This is consistent with 
the observation that homographs wil take longer 
to learn and serves as evidence that LSA-based 
word maturity aproximates efects related to 
meaning. 
6.3 Size
of the Reference Corpus 
Another area of investigation is the repercus-
sions of the choice of the corpus for the reference 
(adult) model. The size (and content) of the corpus 
used to train the reference model is potentialy 
important, since it afects the word maturity calcu-
lations, which are comparisons of the intermediate 
LSA spaces to the reference LSA space built on 
this corpus.  
It is interesting to investigate how the word 
maturity model would be afected if the adult cor-
pus were made significantly more sophisticated. If 
the word maturity metric were simply based on 
word frequency (including the frequency-based 
maturity baseline described in Section 6.1), one 
would expect the word maturity of the words at 
each level to decrease significantly if the reference 
model is made significantly larger, since each in-
termediate level will have encountered fewer 
words by comparison. Intuition about language 
learning, however, tells us that with enough lan-
guage exposure a learner learns virtually all there 
is to know about any particular word; after the 
word reaches its adult maturity, subsequent en-
counters of natural readings do litle to further 
change the knowledge of that word. Therefore, if 
word maturity were tracking something similar to 
real word knowledge, one would expect the word 
maturity for most words to plateau over time, and 
subsequently not change significantly, no mater 
how sophisticated the reference model becomes. 
To evaluate this inquiry we created a reference 
corpus that is twice as large as before (four times 
as large and of the same difficulty range as the 
corpus for the last intermediate level), containing 
roughly 329,000 pasages. We computed the word 
maturity model using this larger reference corpus, 
while keping al the original intermediate corpora 
of the same size and content. 
The results show that the average word maturi-
ty of words at the last intermediate level (14) de-
0.66	  
0.56	  
0.42	  
0.68	  
0.72	  
0.64	  
0.49	  
0.71	  
0.0	  
0.2	  
0.4	  
0.6	  
0.8	  
1.0	  
AoA	  
(Gilhooly)	  
AoA	  
(Bristol)	  
Word	  Lists	   Readings	  
freq-­‐WM	  (α=0.15)	  
LSA-­‐WM	  (α=0.45)	  
305
creases by less than 14% as a result of doubling 
the adult corpus. Furthermore, this number is as 
low as 6%, if one only considers more common 
words that ocur 50 times or more in the corpus. 
This relatively small difference, in spite of a two-
fold increase of the adult corpus, is consistent with 
the idea that word knowledge should aproach a 
plateau, after which further exposure to language 
does litle to change most word meanings. 
6.4 Integration
into Lexicon 
Another important consideration with respect to 
word learning mentioned in Wotler (201), is the 
“conections made between [a] particular word 
and the other words in the mental lexicon.” One 
implication of that is that measuring word maturity 
must take into acount the way words in the lan-
guage are integrated with other words. 
One way to test this efect is to introduce read-
ings where a large part of the important vocabu-
lary is not well known to learners at a given level. 
One would expect learning to be impeded when 
the learning materials are inapropriate for the 
learner level. 
This can be simulated in the word maturity 
model by rearranging the order of some of the 
training pasages, by introducing certain advanced 
pasages at a very early level. If the results of the 
word maturity metric were merely based on fre-
quency, such a reordering would have no efect on 
the maturity of important words (measured after 
all the passages containing these words have been 
encountered), since the total number of relevant 
word encounters does not change as a result of this 
reshufling. If, however, the metric reflected at 
least some degre of semantics, we would expect 
word maturities for important words in these read-
ings to be lower as a result of such rearanging, 
due to the fact that they are being introduced in 
contexts consisting of words that are not wel 
known at the early levels. 
To test this efect, we first colected all pasag-
es in the training corpus of intermediate models 
containing some advanced words from diferent 
topics, namely: “chromosome”, “neutron” and 
“filibuster” together with their plural variants. We 
changed the order of inclusion of these 89 passag-
es into the intermediate models in each of the two 
folowing ways: 
1. All the pasages were introduced at the first 
level (l=1) intermediate corpus 
2. All the pasages were introduced at the last 
level (l=14) intermediate corpus. 
This resulted in two new variants of word ma-
turity models, which were computed in al the 
same ways as before, except that al of these 89 
advanced passages were introduced either at the 
very first level or at the very last level. We then 
computed the word maturity at the levels they 
were introduced. The hypothesis consistent with a 
meaning-based maturity method would be that les 
learning (i.e. lower word maturity) of the relevant 
words wil ocur when pasages are introduced 
prematurely (at level 1). Table 5 shows the word 
maturities measured for each of those cases, at the 
level (1 or 14) when al of the pasages have ben 
introduced. 
Word Introduced at 
l=1 
(WM at l=1) 
Introduced at 
l=14 
(WM at l=14) 
chromosome 0.51 0.73 
neutron 0.51 0.72 
filibuster 0.58 0.85 
Table 5. Word maturity of words resulting when al the rele-
vant pasages are introduced early vs late. 
Inded, the results show lower word maturity val-
ues when advanced passages are introduced to 
early, and higher ones when the passages are in-
troduced at a later stage, when the rest of the sup-
porting vocabulary is known. 
7 Conclusion

We have introduced a new metric for estimating 
the degre of knowledge of words by learners at 
diferent levels. We have also proposed and evalu-
ated an implementation of this metric using Latent 
Semantic Analysis. 
The implementation is based on unsupervised 
word meaning acquisition from natural text, from 
corpora that resemble in volume and complexity 
the reading materials a typical human learner 
might encounter. 
The metric corelates beter than word frequen-
cy to a range of external measures, including vo-
cabulary word lists, psycholinguistic norms and 
leveled texts. Furthermore, we have shown that the 
metric is based on word meaning (to the extent 
that it can be aproximated with LSA), and not 
merely on shalow measures like word frequency. 
306
Many interesting research questions stil re-
main pertaining to the best way to select and parti-
tion the training corpora, align adult and 
intermediate LSA models, corelate the results 
with real schol grade levels, as wel as other fre 
parameters in the model. Nevertheless, we have 
shown that LSA can be employed to usefuly 
mimic model word knowledge. The models are 
curently used (at Pearson Education) to create 
state-of-the-art personalized vocabulary instruc-
tion and asesment tools. 
  
307
 
References 
Andrew Biemiler (208). Words Worth Teaching. Co-
lumbus, OH:  SRA/McGraw-Hil. 
John B. Carol and M. N. White (1973). Age of acquisi-
tion norms for 20 picturable nouns. Journal of Ver-
bal Learning & Verbal Behavior, 12, 563-576. 
Meri Coleman and T.L. Liau (1975). A computer read-
ability formula designed for machine scoring, Jour-
nal of Applied Psychology, Vol. 60, pp. 283-284. 
Ken J. Gilholy and R. H. Logie (1980). Age of acqui-
sition, imagery, concreteness, familiarity and ambi-
guity measures for 1944 words. Behaviour Research 
Methods & Instrumentation, 12, 395-427. 
Wojtek J. Krzanowski (200) Principles of Multivari-
ate Analysis: A User’s Perspective (Oxford Statisti-
cal Science Series). Oxford University Press, USA. 
Thomas K Landauer and Susan Dumais (197). A solu-
tion to Plato's problem: The Latent Semantic Analy-
sis Theory of the Acquisition, Induction, and 
Representation of Knowledge. Psychological Re-
view, 104, pp 21-240. 
Thomas K Landauer (202). On the Computation Basis 
of Learning and Cognition: Arguments from LSA. In 
N. Ros (Ed.), The Psychology of Learning and Mo-
tivation, 41, 43-84. 
Thomas K Landauer, Daniele S. McNamara, Simon 
Denis, and Walter Kintsch (207). Handbok of 
Latent Semantic Analysis. Lawrence Erlbaum. 
Paul Nation (193). Measuring readines for simplified 
material: a test of the first 1,00 words of English. In 
Simplification: Theory and Application M. L. Ticko 
(ed.), RELC Anthology Series 31: 193-203. 
Hans Stadthagen-Gonzalez and C. J. Davis (206). The 
Bristol Norms for Age of Acquisition, Imageability 
and Familiarity. Behavior Research Methods, 38, 
598-605. 
A. Jackson Stener (196). Measuring Reading Com-
prehension with the Lexile Framework. Forth North 
American Conference on Adolescent/Adult Literacy. 
Brent Wolter (201). Comparing the L1 and L2 Mental 
Lexicon. Studies in Second Language Acquisition. 
Cambridge University Pres. 
 
 
 
 
 
 
308

