CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 143–150
Manchester, August 2008
A Tree-to-StringPhrase-basedModelfor StatisticalMachineTranslation
ThaiPhuongNguyen
College of Technology
VietnamNationalUniversity, Hanoi
thainp@vnu.edu.vn
AkiraShimazu1, Tu-BaoHo2, MinhLe Nguyen1, and Vinh Van Nguyen1
1Schoolof InformationScience
2Schoolof KnowledgeScience
JapanAdvancedInstituteof Scienceand Technology
{shimazu,bao,nguyenml,vinhnv}@jaist.ac.jp
Abstract
Thoughphrase-basedSMThas achieved high
translationquality, it still lacks of generaliza-
tion abilityto captureword order differences
betweenlanguages.In this paperwe describe
a general method for tree-to-stringphrase-
based SMT. We study how syntactic trans-
formation is incorporatedinto phrase-based
SMTand its effectiveness. We designsyntac-
tic transformationmodelsusingunlexicalized
form of synchronouscontext-free grammars.
These models can be learned from source-
parsedbitext. Our systemcan naturallymake
use of both constituentand non-constituent
phrasaltranslationsin thedecodingphase.We
consideredvarious levels of syntacticanaly-
sis ranging from chunking to full parsing.
Our experimentalresultsof English-Japanese
and English-Vietnamese translation showed
a significantimprovement over two baseline
phrase-basedSMTsystems.
1 Introduction
Based on the kind of linguistic informationwhich
is made use of, syntactic SMT can be divided into
four types: tree-to-string,string-to-tree,tree-to-tree,
and hierarchicalphrase-based.The tree-to-stringap-
proach (Collins et al., 2005; Nguyen and Shimazu,
2006; Liu et al., 2006 and 2007) supposesthat syn-
tax of the source languageis known. This approach
can be applied when a source language parser is
available. The string-to-treeapproach(Yamada and
Knight,2001;Galley et al.,2006)focusesonsyntactic
modellingof the target languagein cases it has syn-
tactic resourcessuch as treebanksand parsers. The
tree-to-treeapproachmodelsthe syntax of both lan-
guages, thereforeextra cost is required. The fourth
approach (Chiang, 2005) constraintsphrases under
context-free grammarstructurewithout any require-
mentof linguisticannotation.
In this paper, we present a tree-to-stringphrase-
based methodwhichis based on synchronousCFGs.
This methodhas two importantproperties: syntactic
transformationis used in the decodingphase includ-
ing a word-to-phrasetree transformationmodel and
a phrasereorderingmodel;phrasesare the basic unit
of translation.Sincewe designsyntactictransforma-
tion modelsusing un-lexicalizedsynchronousCFGs,
the number of rules is small1. Previous studies on
tree-to-stringSMT are different from ours. Collins
et al. Collinset al. (2005)used hand craftedrules to
carry out word reorderingin the preprocessingphase
but not decodingphase. Nguyenand Shimazu(2006)
presenteda moregeneralmethodin whichlexicalized
syntacticreorderingmodelsbased on PCFGscan be
learnedfromsource-parsedbitext and then appliedin
thepreprocessingphase.Liuet al. (2006)changedthe
translationunit from phrases to tree-to-stringalign-
ment templates(TATs) while we do not. TATs was
representedas xRs rules while we use synchronous
CFG rules. In order to overcomethe limitationthat
TATs can not capturenon-constituentphrasaltransla-
tions,Liu et al. (2007)proposedforest-to-stringrules
whileour systemcan naturallymake use of suchkind
of phrasaltranslationby word-to-phrasetreetransfor-
mation.
We carried out experiments with two language
pairs English-Japaneseand English-Vietnamese.Our
system achieved significant improvements over
Pharaoh,a state-of-the-artphrase-basedSMTsystem.
We also analyzedthe dependenceof translationqual-
ity on thelevel of syntacticanalysis(shallow or deep).
Figure1 shows the architectureof our system.The
inputof this systemis a source-languagetree and the
output is a target-languagestring. This system uses
all featuresof conventionalphrase-basedSMT as in
(Koehn et al., 2003). There are two new featuresin-
cluding a word-to-phrasetree transformationmodel
and a phrase reorderingmodel. The decodingalgo-
1See Section6.2.
143
rithmis a tree-basedsearchalgorithm.
Figure1: A syntax-directedphrase-basedSMTarchi-
tecture.
2 TranslationModel
We usean exampleof English-Vietnamesetranslation
to demonstratethe translationprocessas in Figure2.
Now we describea tree-to-stringSMT model based
on synchronousCFGs.The translationprocessis:
Figure2: The translationprocess.
T1 →T2 →T3 →T4 (1)
whereT1 is a sourcetree, T2 is a sourcephrasetree,
T3 is a reorderedsourcephrasetree,andT4 is a target
phrasetree.
Usingthe first orderchainrule,the join probability
over variables(trees)in graphicalrepresentation1 is
approximatelycalculatedby:
P(T1,T2,T3,T4) =P(T1)×P(T2|T1)×P(T3|T2)×P(T4|T3)
(2)
P(T1) can be omittedsinceonly one syntactictree
is used. P(T2|T1) is a word-to-phrasetree transfor-
mation model we describe later. P(T3|T2) is a re-
orderingmodel. P(T4|T3) can be calculatedusing a
phrasetranslationmodeland a languagemodel. This
is the fundamentalequationof our study represented
in thispaper. In thenext section,wewilldescribehow
to transforma word-basedCFG tree into a phrase-
basedCFGtree.
3 Word-to-PhraseTree Transformation
3.1 Penn
Treebank’s Tree Structure
Accordingto this formalism,a tree is representedby
phrasestructure. If we extract a CFG from a tree or
set of trees,therewill be two possiblerule forms:
• A → α whereα is a sequenceof nonterminals
(syntacticcategories).
• B →γ whereγ is a terminalsymbol(or a word
in this case).
We consideran example of a syntactictree and a
simpleCFGextractedfromthat tree.
Sentence:”I am a student”
Syntactictree: (S (NP (NN I)) (VP (VBPam) (NP (DT a) (NN
student))))
Ruleset: S →NP VP; VP→VBPNP; NP→NN| DT NN; NN
→I | student;
VBP→am; DT→a
However, we are consideringphrase-basedtransla-
tion. Thereforethe right hand side of the secondrule
form must be a sequenceof terminalsymbols(or a
phrase) but not a single symbol (a word). Suppose
thatthe phrasetablecontainsa phrase”ama student”
whichleadsto the followingpossibletree structure:
Phrasesegmentation:”I| am a student”
Syntactictree: (S (NP (NN I)) (VP (VBPam a student)))
Ruleset: S →NP VP; VP→VBP;NP→NN; NN→I; VBP→
am a student
We have to find out someway to transforma CFG
treeintoa tree withphrasesat leaves. In the next sub-
sectionwe proposesuchan algorithm.
3.2 An
Algorithmfor Word-to-PhraseTree
Transformation
Table 1 representsour algorithmto transforma CFG
tree to a phraseCFG tree. Whendesigningthis algo-
rithm, our criterionis to preserve the originalstruc-
tureas muchas possible.Thisalgorithmincludestwo
steps. Thereare a numberof notionsconcerningthis
algorithm:
• A CFGrule has a headsymbolon the righthand
side. Using this information,head child of a
nodeon a syntactictree can be determined.
144
+ Input: A CFGtree,a phrasesegmentation
+ Output: A phraseCFGtree
+ Step1: Allocatephrasesto leaf nodesin a top-down manner:A phraseis allocatedto headword of a nodeif the
phrasecontainsthe headword. Thisheadword is thenconsideredas the phrasehead.
+ Step2: Transformthe syntactictree by replacingleaf nodesby theirallocatedphraseand removingall nodeswhose
spanis a substringof phrases.
Table1: An algorithmto transforma CFGtree to a phraseCFGtree.
• If a nodeis a pre-terminalnode(containingPOS
tag), its head word is itself. If a node is an in-
ner node (containingsyntacticconstituenttag),
its headword is retrieved throughthe headchild.
• Word spanof a nodeis a stringof its leaves. For
instance,word span of subtree(NP (PRP$your)
(NNclass))is ”yourclass”.
Now we consideran exampledepictedin Figure 3
and 4. Headchildrenare taggedwithfunctionallabel
H. Thereare two phrases: ”is a” and ”in your class”.
Afterthe Step1, the phrase”is a” is attachedto (VBZ
is). The phrase”in your class”is attachedto (IN in).
In Step2, the node(V is) is replacedby (V ”is a”) and
(DT a) is removed from its father NP. Similarly, (IN
in) is replacedby (IN ”in yourclass”)and the subtree
NP on the rightis removed.
S [is] 
NP 
[Fred] 
VP-H 
[is] 
VBZ-H NP [student] NNP-H 
is NP-H [student] 
DT NN-H 
PP 
[in] 
IN-H NP [class] 
PRP$ NN-H 
Fred 
a student in 
your class 
{is a} 
{in your class} 
Figure3: Tree transformationstep 1. Solid arrows
show the allocationprocessof ”is a”. Dottedarrows
demonstratethe allocationprocessof ”in yourclass”
The proposedalgorithmhas some properties. We
statethesepropertieswithoutpresentingproof2.
• Uniqueness:Given a CFGtreeanda phraseseg-
mentation,by applying Algorithm 1, one and
onlyone phrasetree is generated.
2Proofsare simple.
Figure4: Tree transformationstep 2.
• Constituentsubgraph: A phrase CFG tree is
a connectedsubgraphof input tree if leaves are
ignored.
• Flatness:A phraseCFGtree is flatterthaninput
tree.
• Outsidehead: The headof a phraseis always a
word whosehead outsidethe phrase. If there is
morethanonewordsatisfyingthiscondition,the
word at the highestlevel is chosen.
• Dependencysubgraph:Dependency graphof a
phraseCFG tree is a connectedsubgraphof in-
put tree’s dependency graphif thereexist no de-
tachednodes.
The meaningof uniquenesspropertyis that our al-
gorithmis a deterministicprocedure.Theconstituent-
subgraphpropertywill be employed in the next sec-
tion for an efficientdecodingalgorithm.Whena syn-
tactic tree is transformed,a number of subtrees are
replacedby phrases.Theheadword of a phraseis the
contact point of that phrase with the remainingpart
of a sentence. From the dependency point of view, a
headwordshoulddependonan outerwordratherthan
an innerword. Aboutdependency-subgraphproperty,
whenthereis a detachednode,an indirectdependency
will become a direct one. In any cases, there is no
145
changein dependency direction. We can observe de-
pendency trees in Figure 5. The first two trees are
source dependency tree and phrase dependency tree
of the previousexample. The last one correspondsto
the casein whicha detachednodeexists.
Fred is 
ROOT 
student in your class a 
Fred is  a 
ROOT 
student in  your class 
Fred is  a student 
ROOT 
in  your class 
Figure 5: Dependency trees. The third tree corre-
spondswithphrasesegmentation:”Fred| is a student
| in yourclass”
3.3 ProbabilisticWord-to-PhraseTree
Transformation
We have proposed an algorithm to create a phrase
CFG tree from a pair of CFG tree and phrase seg-
mentation. Two questionsnaturallyarise: ”is there
a way to evaluatehow gooda phrasetree is?” and ”is
such an evaluationvaluable?”Note that phrase trees
are the means to reorder the source sentencerepre-
sented as phrase segmentations. Thereforea phrase
tree is surely not good if no right order can be gen-
erated. Now the answer to the second question is
clear. We need an evaluationmethodto prevent our
programfrom generatingbad phrase trees. In other
words,goodphrasetreesshouldbe given a higherpri-
ority.
We definethe phrasetreeprobabilityas the product
of its rule probabilitygiven the originalCFGrules:
P(Tprime) =productdisplay
i
P(LHSi →RHSprimei|LHSi →RHSi)
(3)
where Tprime is a phrase tree whose CFG rules are
LHSi → RHSprimei. LHSi → RHSi are origi-
nal CFG rules. RHSprimei are subsequencesof RHSi.
Since phrasetree rules shouldcapturechangesmade
by the transformationfrom word to phrase, we use
’+’ to represent an expansion and ’-’ to show an
overlap. These symbol will be added to a nonter-
minal on the side having a change. In the previ-
ous example, since a head noun in the word tree
has been expanded on the right, the correspond-
ing symbol in phrase tree is NN-H+. A nonter-
minal X can become one of the following symbols
X,−X,+X,X−,X+,−X−,−X+,+X−,+X+.
Conditionalprobabilitiesare computedin a sepa-
rate trainingphase using a source-parsedand word-
alignedbitext. First, all phrase pairs consistentwith
the word alignmentare collected. Then using this
phrasesegmentationandsyntactictreeswecangener-
atephrasetreesby word-to-phrasetreetransformation
and extractrules.
4 PhraseReorderingModel
Reordering rules are represented as SCFG rules
whichcanbe un-lexicalizedor source-sidelexicalized
(Nguyenand Shimazu,2006). In this paper, we used
un-lexicalized rules. We used a learning algorithm
as in (Nguyenand Shimazu,2006)to learn weighted
SCFGs.Thetrainingrequirementsincludea bilingual
corpus,a word alignmenttool, and a broad coverage
parser of the source language. The parser is a con-
stituency analyzer which can produce parse tree in
Penn Tree-bank’s style. The model is applicableto
languagepairs in which the target languageis poor
in resources.We used phrasereorderruleswhose’+’
and ’-’ symbolsare removed.
5 Decoding
A sourcesentencecanhave many possiblephraseseg-
mentations.Eachsegmentationin combinationwitha
sourcetreecorrespondsto a phrasetree. A phrase-tree
forest is a set of those trees. A naive decodingalgo-
rithm is that for each segmentation,a phrase tree is
generatedand thenthe sentenceis translated.Thisal-
gorithm is very slow or even intractable. Based on
the constituent-subgraphproperty of the tree trans-
formationalgorithm, the forest of phrase trees will
be packed into a tree-structurecontainerwhoseback-
boneis the originalCFGtree.
5.1 TranslationOptions
A translationoptionencodesa possibilityto translate
a source phrase (at a leaf node of a phrase tree) to
anotherphrasein target language. Sinceour decoder
uses a log-lineartranslationmodel,it can exploitvar-
ious featuresof translationoptions. We use the same
featuresas (Koehnet al., 2003). Basicinformationof
a translationoptionincludes:
• sourcephrase
• target phrase
• phrasetranslationscore(2)
146
• lexicaltranslationscore(2)
• word penalty
Translationoptions of an input sentence are col-
lectedbeforeany decodingtakes place. Thisallows a
faster lookupthan consultingthe wholephrasetrans-
lation table during decoding. Note that the entire
phrase translation table may be too big to fit into
memory.
5.2 TranslationHypotheses
A translationhypothesis representsa partial or full
translationof an input sentence. Initial hypotheses
correspondto translationoptions. Each translation
hypothesisis associatedwith a phrase-treenode. In
other words, a phrase-treenode has a collectionof
translationhypotheses.Now we considerbasicinfor-
mationcontainedin a translationhypothesis:
• the cost so far
• list of childhypotheses
• left language model state and right language
modelstate
5.3 DecodingAlgorithm
First we considerstructureof a syntactictree. A tree
node containsfields such as syntacticcategory, child
list, and head child index. A leaf node has an ad-
ditionalfield of word string. In order to extend this
structureto store translationhypotheses,a new field
of hypothesis collection is appended. A hypothe-
sis collectioncontainstranslationhypotheseswhose
word spans are the same. Actually, it correspondsto
a phrase-treenode. A hypothesiscollectionwhose
word span is [i1,i2] at a node whose tag is X ex-
pressesthat:
• Thereis a phrase-treenode(X,i1,i2).
• Thereexist a phrase[i1,i2] or
• There exist a subsequence of X’s child list:
(Y1,j0,j1), (Y2,j1+1,j2), ...,(Yn,jn−1+1,jn)
wherej0 =i1 andjn =i2
• Supposethat [i,j] is X’s span, then [i1,i2] is a
valid phrasenode’s span if and only if: i1 <= i
or i <i1 <= j and thereexist a phrase[i0,i1 −
1] overlappingX’s span at [i,i1 −1]. A similar
conditionis requiredof j.
Table 2 shows our decodingalgorithm.Step1 dis-
tributes translationoptionsto leaf nodesusing a pro-
ceduresimilarto Step1 of algorithmin Table 1. Step
Corpus Size Training Development Testing
Conversation 16,809 15,734 403 672
Reuters 57,778 55,757 1,000 1,021
Table3: Corporaand data sets.
English Vietnamese
Sentences 16,809
Averagesent. len. 8.5 8.0
Words 143,373 130,043
Vocabulary 9,314 9,557
English Japanese
Sentences 57,778
Averagesent. len. 26.7 33.5
Words 1,548,572 1,927,952
Vocabulary 31,702 29,406
Table4: Corpusstatisticsof translationtasks.
2 helpscheckvalid subsequencesin Step 3 fast. Step
3 is a bottom-upprocedure,a node is translatedif all
of its childnodeshave been translated.Step 3.1 calls
syntactictransformationmodels. After reorderedin
Step 3.2, a subsequencewill be translatedin Step 3.3
usinga simplemonotonicdecodingprocedureresult-
ing in new translationhypotheses. We used a beam
pruningtechniqueto reducethe memorycost and to
acceleratethe computation.
6 ExperimentalResults
6.1 ExperimentalSettings
We usedReuters3, an English-Japanesebilingualcor-
pus,andConversation,an English-Vietnamesecorpus
(Table 4). Thesecorporawere split into data sets as
shown in Table 3. Japanesesentenceswereanalyzed
by ChaSen4, a word-segmentationtool.
A numberof tools were used in our experiments.
Vietnamesesentenceswere segmentedusinga word-
segmentationprogram (Nguyen et al., 2003). For
learning phrase translationsand decoding, we used
Pharaoh (Koehn, 2004), a state-of-the-artphrase-
based SMT system which is available for research
purpose. For word alignment,we used the GIZA++
tool (Och and Ney, 2000). For learning language
models,we used SRILMtoolkit(Stolcke, 2002). For
MT evaluation,we used BLEUmeasure(Papineniet
al., 2001) calculatedby the NIST script version11b.
For theparsingtask,weusedCharniak’s parser(Char-
niak,2000). For experimentswith chunking(or shal-
low parsing),we used a CRFs-basedchunkingtool 5
to split a sourcesentenceinto syntacticchunks.Then
a pseudoCFG rule over chunksis built to generatea
two-level syntactictree. This tree can be used in the
3http://www2.nict.go.jp/x/x161/members/mutiyama/index.html
4http://chasen.aist-nara.ac.jp/chasen/distribution.html.en
5http://crfpp.sourceforge.net/
147
+ Input: A sourceCFGtree,a translation-optioncollection
+ Output: The best target sentence
+ Step1: Allocatetranslationoptionsto hypothesiscollectionsat leaf nodes.
+ Step2: Computeoverlapvectorfor all nodes.
+ Step3: For eachnode,if all of its childrenhave beentranslated,thenfor eachvalid
sub-sequenceof childlist, carryout the followingsteps:
+ Step3.1: Retrieve transformationrules
+ Step3.2: Reorderthe sub-sequence
+ Step3.3: Translatethe reorderedsub-sequenceand updatecorresponding
hypothesiscollections
Table2: A bottom-updynamic-programmingdecodingalgorithm.
Corpus CFG PhraseCFG W2PTT Reorder
Conversation 2,784 2,684 8,862 2,999
Reuters 7,668 5,479 13,458 7,855
Table5: Ruleinductionstatistics.
Corpus Pharaoh PB system SD system SD system
(chunking) (full-parsing)
Conversation 35.47 35.66 36.85 37.42
Reuters 24.41 24.20 20.60 25.53
Table 6: BLEU score comparisonbetween phrase-
based SMT and syntax-directedSMT. PB=phrase-
based;SD=syntax-directed
sameway as treesproducedby Charniak’s parser.
We built a SMTsystemfor phrase-basedlog-linear
translationmodels. This system has two decoders:
beam searchand syntax-based.We implementedthe
algorithmin Section5 for the syntax-baseddecoder.
We also implementeda rule inductionmoduleand a
modulefor minimumerrorrate training.We usedthe
systemfor our experimentsreportedlater.
6.2 RuleInduction
In Table 5, we report statistics of CFG rules,
phraseCFGrules,word-to-phrasetreetransformation
(W2PTT) rules, and reorderingrules. All counted
ruleswerein un-lexicalizedform. Thosenumbersare
very smallin comparisonwith the numberof phrasal
translations(up to hundredsof thousandson our cor-
pora). Therewere a numberof ”un-seen”CFG rules
which did not have a correspondingreorderingrule.
A reasonis that those rules appearedonce or several
timesin the trainingcorpus;however, theirhierarchi-
cal alignmentsdid not satisfy the conditionsfor in-
ducinga reorderingrule since word alignmentis not
perfect(NguyenandShimazu,2006).Anotherreason
is that therewereCFGruleswhichrequirednonlocal
reordering.This may be an issue for futureresearch:
a Markovizationtechniquefor SCFGs.
6.3 BLEUScores
Table 6 shows a comparison of BLEU scores be-
tween Pharaoh, our phrase-basedSMT system, and
our syntax-directed(SD)SMTsystemwith chunking
and full parsing respectively. On both Conversation
corpus and Reuterscorpus: The BLEU score of our
phrase-basedSMT system is comparableto that of
Pharaoh;TheBLEUscoreof our SD systemwithfull
parsing is higher than that of our phrase-basedsys-
tem. On Conversation corpus, our SD system with
chunkinghas a higherperformancein termsof BLEU
score than our phrase-basedsystem. Using sign test
(Lehmann,1986), we verified the improvements are
statisticallysignificant.However, on Reuterscorpus,
performanceof the SD systemwithchunkingis much
lower than the phrase-basedsystem’s. The reasonis
that in English-Japanesetranslation,chunk is a too
shallow syntacticstructureto captureword order in-
formation. For example, a prepositionalchunk of-
ten includes only prepositionand adverb, therefore
such informationdoes not help reorderingpreposi-
tionalphrases.
6.4 The
Effectivenessof the W2PTTModel
Without this feature,BLEUscoresdecreasedaround
0.5 on bothcorpora.We now considera linguistically
motivatedexampleof English-Vietnamesetranslation
to show that phrase segmentationcan be evaluated
through phrase tree scoring. This example was ex-
tractedfromConversationtest set.
Englishsentence:for my wife ’s mother
Vietnameseword order: for mother’s wife my
Phrasesegmentation1: for my wife| ’s | mother
P1=P(PP→IN+-NP| PP→IN NP)xP(-NP→-NP NN| NP→NP
NN)xP(-NP→POS| NP→PRP$ NN
POS)=log(0.00001)+log(0.14)+log(0.048)=-5-0.85-1.32=-7.17
Phrasesegmentation2: for| my wife ’s | mother
P2=P(PP→IN NP| PP→IN NP)xP(NP→NP NN| NP→NP
NN) xP(NP→POS| NP→PRP$ NN POS)
=log(0.32)+log(0.57)+log(0.048)=-0.5-0.24-1.32=-2.06
The first phrase segmentationis bad (or even un-
acceptable) since the right word order can not be
achieved from this segmentationby phrase reorder-
ing and word reorderingwithinphrases. The second
phrase segmentationis much better. Source syntax
tree and phrasetrees are shown in Figure6. The first
phrasetreehasa muchsmallerprobability(P1=-7.17)
thanthe second(P2=-2.06).
148
Figure6: Two phrasetrees.
Corpus Level-1 Level-2 Level-3 Level-4 Full
Conversation 36.85 36.91 37.11 37.23 37.42
Reuters 20.60 22.76 24.49 25.12 25.53
Table 7: BLEU score with different syntacticlevels.
Level-imeanssyntactictransformationwas appliedto
tree nodeswhoselevel smallerthanor equalto i. The
level of a pre-terminalnode(POStag) is 0. The level
of an innernodeis the maximumof its children’s lev-
els.
6.5 Levels
of SyntacticAnalysis
Sincein practice,chunkingand full parsingare often
used,in Table 6, we showed translationqualityof the
two cases. It is interestingif we can find how syn-
tactic analysiscan affect BLEU score at more inter-
mediatelevels (Table7). On the Conversationcorpus,
usingsyntaxtreesof level-1is effective in comparison
withbaseline.Theincreaseof syntacticlevel makes a
steady improvement in translationquality. Note that
whenwe carriedoutexperimentswithchunking(con-
sidered as level-1 syntax) the translationspeed (in-
cluding chunking) of our tree-to-stringsystem was
muchfaster than baselinesystems’.This is an option
for developingapplicationswhichrequirehigh speed
suchas web translation.
7 RelatedWorks
7.1 A
Comparisonof SyntacticSMTMethods
To advance the state of the art, SMT systemdesign-
ers have experimentedwith tree-structuredtransla-
tion models. The underlyingcomputationalmodels
were synchronouscontext-free grammarsand finite-
state tree transducerswhichconceptuallyhave a bet-
ter expressive power than finite-statetransducers.We
create Tables 8 and 9 in order to comparesyntac-
tic SMT methodsincludingours. The first row is a
baselinephrasalSMT approach. The secondcolumn
in Table 8 onlydescribesinputtypesbecausethe out-
put is often string. SyntacticSMT methodsare dif-
ferent in many aspects. Methodswhich make use of
phrases (in either explicit or implicit way) can beat
the baseline approach(Table 8) in terms of BLEU
metric. Two mainproblemsthesemodelsaim to deal
with are word orderand word choice. In orderto ac-
complishthis purpose, the underlyingformal gram-
mars (includingsynchronouscontext-free grammars
and tree transducers)can be fully lexicalizedor un-
lexicalized(Table 9).
7.2 Non-constituentPhrasalTranslations
Liu et al. (2007) proposed forest-to-stringrules to
capturenon-constituentphrasaltranslationwhile our
systemcannaturallymake useof suchkindof phrasal
translationby using word-to-phrasetree transforma-
tion. Liu et al. (2007) also discussed about how
the phenomenonof non-syntacticbilingual phrases
is dealt with in other SMT methods. Galley et al.(2006)handlednon-constituentphrasaltranslationby
traversing the tree upwards until reachesa node that
subsumesthe phrase. Marcu et al. (2006) reported
that approximately28%of bilingualphrasesare non-
syntacticon their English-Chinesecorpus. They pro-
posed using a pseudo nonterminalsymbol that sub-
sumes the phrase and correspondingmulti-headed
syntacticstructure. One new xRs rule is requiredto
explainhow thenew nonterminalsymbolcanbe com-
bined with others. This techniquebrought a signif-
icant improvement in performanceto their string-to-
tree noisychannelSMTsystem.
8 Conclusions
We have presented a general tree-to-stringphrase-
based method. This methodemploys a syntax-based
reorderingmodel in the decodingphase. By word-
to-phrase tree transformation, all possible phrases
are considered in translation. Our method does
not suppose a uniform distribution over all possible
phrase segmentationsas (Koehn et al., 2003) since
each phrase tree has a probability. We believe that
other kinds of translationunit such as n-gram (Jos
et al., 2006),factoredphrasaltranslation(Koehn and
Hoang, 2007), or treelet (Quirk et al., 2005) can be
used in this method. We would like to considerthis
problemas a futurestudy. Moreover we wouldlike to
use n-besttreesas the inputof our system.A number
149
Method Input Theoretical Decodingstyle Linguistic Phrase Performance
model information usage
Koehnet al. (2003) string FSTs beamsearch no yes baseline
Yamadaand Knight(2001) string SCFGs parsing target no not better
Melamed(2003) string SCFGs parsing bothsides no not better
Chiang(2005) string SCFGs parsing no yes better
Quirket al. (2005) dep. tree TTs parsing source yes better
Galley et al. (2006) string TTs parsing target yes better
Liu et al. (2006) tree TTs tree transf. source yes better
Our work tree SCFGs tree transf. source yes better
Table8: A comparisonof syntacticSMTmethods(part1). FST=FiniteStateTransducer;SCFG=Synchronous
Context-FreeGrammar;TT=Tree Transducer.
Method Ruleform Rulefunction Rulelexicalizationlevel
Koehnet al. (2003) no no no
Yamadaand Knight(2001) SCFGrule reorderand function-word ins./del. unlexicalized
Melamed(2003) SCFGrule reorderand word choice full
Chiang(2005) SCFGrule reorderand word choice full
Quirket al. (2005) Treeletpair word choice full
Galley et al. (2006) xRs rule reorderand word choice full
Liu et al. (2006) xRs rule reorderand word choice full
Our work SCFGrule reorder unlexicalized
Table9: A comparisonof syntacticSMTmethods(part2). xRsis a kindof rulewhichmapsa syntacticpattern
to a string, for exampleVP(AUX(does),RB(not),x0:VB)→ ne, x0, pas. In the columnRule lexicalization
level: full=lexicalizationusingvocabulariesof bothsourcelanguageand target language.
of non-localreorderingphenomenasuch as adjunct
attachmentshouldbe handledin the future.
References
Charniak,E. 2000. A maximumentropy inspiredparser.
In Proceedingsof HLT-NAACL.
Galley, M., JonathanGraehl,Kevin Knight,DanielMarcu,
Steve DeNeefe,Wei Wang, IgnacioThayer 2006. Scal-
able Inferenceand Training of Context-RichSyntactic
TranslationModels.In Proceedingsof ACL.
Jos B. Mario,Rafael E. Banchs,Josep M. Crego, Adri de
Gispert,Patrik Lambert,Jos A. R. Fonollosa,MartaR.
Costa-juss. 2006. N-gram-basedMachineTranslation.
ComputationalLinguistics, 32(4): 527–549.
Koehn, P. 2004. Pharaoh: a beam search decoder for
phrase-basedstatisticalmachinetranslationmodels. In
Proceedingsof AMTA.
Koehn, P. and Hieu Hoang. 2007. Factored Translation
Models.In Proceedingsof EMNLP.
Koehn, P., F. J. Och, and D. Marcu. 2003. Statisti-
cal phrase-basedtranslation. In Proceedingsof HLT-
NAACL.
Lehmann,E. L. 1986. TestingStatisticalHypotheses(Sec-
ond Edition).Springer-Verlag.
Liu, Y., Qun Liu, Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Transla-
tion. In Proceedingsof ACL.
Liu, Y., Yun Huang, Qun Liu, and Shouxun Lin 2007.
Forest-to-StringStatisticalTranslationRules. In Pro-
ceedingsof ACL.
Marcu, D., Wei Wang, AbdessamadEchihabi,and Kevin
Knight. 2006. SPMT: StatisticalMachineTranslation
withSyntactifiedTarget LanguagePhrases.In Proceed-
ingsof EMNLP.
Melamed,I. D. 2004. Statisticalmachinetranslationby
parsing.In Proceedingsof ACL.
Nguyen,ThaiPhuongand AkiraShimazu.2006. Improv-
ing Phrase-BasedStatisticalMachineTranslationwith
MorphosyntacticTransformation.MachineTranslation,
20(3): 147–166.
Nguyen, Thai Phuong, Nguyen Van Vinh and Le Anh
Cuong. 2003. VietnameseWord SegmentationUsing
HiddenMarkov Model. In Proceedingsof International
Workshopfor Computer, Information,and Communica-
tion Technologies in Korea and Vietnam.
Och, F. J. and H. Ney. 2000. Improved statisticalalign-
mentmodels.In Proceedingsof ACL.
Papineni, K., S. Roukos, T. Ward, W.-J. Zhu. 2001.
BLEU: a method for automaticevaluationof machine
translation. TechnicalReport RC22176(W0109-022),
IBMResearchReport.
Quirk,C., A. Menezes,and C. Cherry. 2005. Dependency
treelettranslation:SyntacticallyinformedphrasalSMT.
In Proceedingsof ACL.
Stolcke, A. 2002. SRILMAnExtensibleLanguageMod-
eling Toolkit. In Proc. Intl. Conf. Spoken Language
Processing.
Yamada,K. and K. Knight. 2001. A syntax-basedstatisti-
cal translationmodel. In Proceedingsof ACL.
150

