Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 590–598,
Singapore, 6-7 August 2009. c 2009 ACL and AFNLP
Adapting a Polarity Lexiconusing Integer Linear Programming
for Domain-Specific Sentiment Classification
Yejin Choi and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14853
{ychoi,cardie}@cs.cornell.edu
Abstract
Polarity lexicons have been a valuable re-
source for sentiment analysis and opinion
mining. There are a number of such lexi-
cal resources available, but it is often sub-
optimal to use them as is, because general
purpose lexical resources do not reflect
domain-specific lexical usage. In this pa-
per, we propose a novel method based on
integer linear programming that can adapt
an existing lexicon into a new one to re-
flect the characteristics of the data more
directly. In particular, our method collec-
tivelyconsidersthe relationsamongwords
and opinionexpressionsto derive themost
likely polarity of each lexical item (posi-
tive, neutral, negative, or negator) for the
given domain. Experimental results show
that our lexicon adaptation technique im-
provestheperformance offine-grainedpo-
larity classification.
1 Introduction
Polaritylexiconshave beenavaluableresourcefor
sentimentanalysisandopinionmining. In particu-
lar, they have been an essentialingredientfor fine-
grained sentiment analysis (e.g., Kim and Hovy
(2004), Kennedy and Inkpen (2005), Wilson et al.(2005)). Even thoughthe polaritylexicon playsan
important role (Section 3.1), it has received rela-
tively less attention in previous research. In most
cases, polarity lexicon construction is discussed
onlybriefly asa preprocessingstepfor a sentiment
analysis task (e.g., Hu and Liu (2004), Moilanen
and Pulman (2007)), but the effect of different al-
ternative polarity lexicons is not explicitly inves-
tigated. Conversely, research efforts that focus
on constructinga general purpose polaritylexicon
(e.g., Takamura et al. (2005), Andreevskaia and
Bergler (2006), Esuli and Sebastiani (2006), Rao
and Ravichandran (2009)) generally evaluate the
lexicon in isolation from any potentially relevant
NLP task, and it is unclear how the new lexicon
might affect end-to-endperformance of a concrete
NLP application.
It might even be unrealistic to expect that there
can be a general-purpose lexical resource that
can be effective across all relevant NLP applica-
tions, as general-purpose lexicons will not reflect
domain-specific lexical usage. Indeed, Blitzer
et al. (2007) note that the polarity of a particu-
lar word can carry opposite sentiment depending
on the domain (e.g., Andreevskaia and Bergler
(2008)).
In this paper, we propose a novel method based
oninteger linearprogrammingtoadaptanexisting
polarity lexicon into a new one to reflect the char-
acteristics of the data more directly. In particular,
our method considers the relations among words
and opinion expressions collectively to derive the
most likelypolarityof each wordfor the givendo-
main.
Figure 1 depicts the key insightof our approach
usinga bipartitegraph. On the left handside, each
noderepresentsa word,andon therighthandside,
each noderepresentsanopinionexpression. There
is an edge between a word wi and an opinion ex-
pression ej, if the word wi appears in the expres-
sion ej. We assume the possible polarity of each
expression is one of the following three values:
{positive, neutral, negative}, while the possible
polarity of each word is one of:{positive, neutral,
negative or negator}. Strictly speaking,negator is
nota valuefor polarity,butwe includethem inour
lexicon, because valence shiftersor negators have
beenshowntoplayanimportantroleforsentiment
analysis (e.g., Polanyi and Zaenen (2004), Moila-
nen and Pulman (2007), Choi and Cardie (2008)).
Typically, the ultimate goal of the sentiment
analysis task is to determine the expression-level
(or sentiment/ document-level) polarities, rather
590
than the correct word-level polarities with respect
to the domain. Therefore, word-level polarities
can beconsideredas latentinformation. Inthispa-
per, we show how we can improve the word-level
polarities of a general-purpose polarity lexicon by
utilizing the expression-level polarities, and in re-
turn, how the adapted word-level polarities can
improve the expression-level polarities.
In Figure 1, there are two types of relations
we could exploitwhenadaptinga general-purpose
polarity lexicon into a domain-specific one. The
first are word-to-word relations within each ex-
pression. That is, if we are not sure about the
polarity of a certain word, we can still make a
guessbasedonthepolaritiesofotherwordswithin
the same expression and knowledge of the polar-
ity of the expression. The second type of relations
areword-to-expressionrelations: e.g., somewords
appear in expressions that take on a variety of po-
larities, while other words are associated with ex-
pressions of one polarity class or another.
In relation to previous research, analyz-
ing word-to-word (intra-expression) relations
is most related to techniques that determine
expression-level polarity in context (e.g., Wilson
et al. (2005)), while exploring word-to-expression
(inter-expression) relations has connections to
techniques that employ more of a global-view of
corpus statistics (e.g., Kanayama and Nasukawa
(2006)).1
While most previousresearch exploits only one
or the other type of relation, we propose a unified
method that can exploit both types of semantic re-
lation, while adapting a general purpose polarity
lexicon into a domain specific one. We formulate
our lexicon adaptation task using integer linear
programming (ILP), which has been shown to be
very effective when solving problems with com-
plex constraints (e.g., Roth and Yih (2004), Denis
and Baldridge (2007)). And the word-to-wordand
word-to-expression relations discussed above can
beencodedas softandhard constraintsinILP. Un-
fortunately, one class of constraint that we would
like to encode (see Section 2) will require an
exponentially many number of constraints when
grounded into an actual ILP problem. We there-
fore propose an approximation scheme to make
the problem more practically solvable.
We evaluate the effect of the adapted lex-
1In case of document-level polarity classification, word-
to-expression relations correspond to word-to-document re-
lations.
uni0065uni0078uni0070a0
uni0065uni0078uni0070a1
uni0065uni0078uni0070a2
uni0065uni0078uni0070a3uni0077a3 uni0077a2 uni0077a0
uni0077a3 uni0077a1 uni0077a0 uni0077a4
uni0077a3 uni0077a4
uni0077a4
uni002B
uni2212
uni0077a3
uni0077a2
uni0077a1
uni0077a0
uni0077a4
uni003D
uni002B
uni2212
uni2212
uni003D
uni2212
Figure 1: The relations among words and expres-
sions. + indicates positive, indicates negative, =
indicates neutral, and¬indicates a negator.
icon in the context of a concrete NLP task:
expression-level polarity classification. Experi-
mental results show that our lexicon adaptation
technique improves the accuracy of two com-
petitive expression-level polarity classifiers from
64.2% 70.4% to 67.0% 71.2%..
2 An
Integer Linear Programming
Approach
In this section, we describe how we formulate the
lexicon adaptation task using integer linear pro-
gramming. Before we begin, we assume that we
have a general-purpose polarity lexiconL, and a
polarity classification algorithm f(el,L), that can
determine the polarityof the opinionexpressionel
based on the words in el and the initial lexiconL.
The polarity classification algorithm f(·) can be
eithera heuristic-basedone, ora machine-learning
based one – we consider it as a black box for now.
Constraints for word-level polarities: For
each word xi, we define four binary variables:
x+i ,x=i ,x−i ,x¬i to represent positive, neutral, neg-
ative polarity, and negators respectively. If xδi = 1
for some δ ∈{+,=,−,¬}, then the word xi has
the polarity δ. The followinginequalityconstraint
states that at least one polarity value must be cho-
sen for each word.
x+i +x=i +x−i +x¬i >= 1 (1)
If we allowonly one polarityper word, then the
above inequality constraint should be modified as
an equality constraint. Although most words tend
to associate with a single polarity, some can take
on more than one polarity. In order to capture this
observation,we introduceanauxiliarybinaryvari-
able αi for each word xi. Then the next inequality
591
constraint states that at most two polarities can be
chosen for each word.
x+i + x=i + x−i + x¬i <= 1 + αi (2)
Next we introduce the initial part of our objec-
tive function.
maximize
summationdisplay
i
parenleftbigg
w+i x+i + w=i x=i
+ w−i x−i + w¬i x¬i
− wααi
parenrightbigg
+ ··· (3)
For the auxiliary variable αi, we apply a con-
stant weight wα to discourage ILP from choosing
more than one polarity for each word. We can al-
low more than two polarities for each word, by
adding extra auxiliary variables and weights. For
each variable xδi, we define its weight wδi, which
indicates how likely it is that word xi carries the
polarity δ. We define the value of wδi using two
different types of information as follows:
wδi := Lwδi + Cwδi
where Lwδi is the degree of polarity δ for word xi
determined by the general-purpose polarity lexi-
conL, and Cwδi is the degree of polarity δ deter-
mined by the corpus statisticsas follows:2
Cwδ
i :=
# of xi in expressions with polarity δ
# of xi in the corpusC
Note that the occurrence of word xi in an ex-
pression ej with a polarity δ does not necessar-
ily mean that the polarity of xi should also be
δ, as the interpretation of the polarity of an ex-
pression is more than just a linear sum of the
word-level polarities (e.g., Moilanen and Pulman
(2007)). Nonetheless, not all expressions require
a complicated inference procedure to determine
their polarity. Therefore, Cwδi still provides useful
information about the likely polarity of each word
based on the corpus statistics.
From the perspective of Chomskyan linguistics,
the weights Lwδi based on the prior polarity from
the lexicon can be considered as having a ”com-
petence” component , while Cwδi derived from
the corpus counts can be considered as a ”perfor-
mance” component (Noam Chomsky (1965)).
2If a word xi is in an expression that is not an opinion,
then we countit as an occurrencewith neutral polarity.
Constraints for content-word negators: Next
we describe a constraint that exploits knowledge
of the typical distribution of content-word nega-
tors in natural language. Content-word negators
are words that are not function words, but act se-
mantically as negators (Choi and Cardie, 2008).3
Although it is possible to artificially construct a
very convolutedsentence with lots of negations, it
is unlikely for multiple layers of negations to ap-
pear very often in natural language (Pickett et al.(1996)). Therefore, we allow at most one content-
word negator for each expression el. Because we
do not restrict the number of function-word nega-
tors, our constraint still gives room for multiple
layers of negations.
summationdisplay
i∈µ(el)
x¬i <= 1 (4)
In the above constraint, µ(el) indicates the set
of indices of content words appearing in el. For
instance, if i ∈ µ(el), then xi appears in el. This
constraintcan be polishedfurthertoaccommodate
longer expressions where multiple content-word
negators are more likely to appear, by adding a
separate constraintwith a slidingwindow.
Constraints for expression-level polarities:
Before we begin, we introduce pi(el) that will be
used often in the remaining section. For each ex-
pression el, we define pi(el) to be the set of con-
tent words appearing in el, together with the most
likely polarity proposed by a general-purpose po-
larity lexiconL. For instance, if x+i ∈pi(el), then
the polarity of word xi is + according toL.
Next we encode constraints that consider
expression-level polarities. If the polarity classifi-
cation algorithm f(el,L) makes an incorrect pre-
dictionfor el usingthe originallexiconL, then we
need to encourage ILP to fix the error by suggest-
ingdifferentword-levelpolarities. We capturethis
idea by the followingconstraint:
summationdisplay
xδi∈pi(el)
xδi <= |pi(el)|−1 +βl (5)
The auxiliary binary variable βl is introduced
for each el so that the assignment pi(el) does not
have to be changed if paying for the cost wβ in the
objective function. (See equation (10).) That is,
suppose the ILP solver assigns ‘1’ to all variables
3Examples of content-word negators are destroy, elimi-
nate, preventetc.
592
in φ(el), (which corresponds to keeping the orig-
inal lexicon as it is for all words in the given ex-
pressionel), then the auxiliary variable βl must be
also set as ‘1’ in order to satisfy the constraint(5).
Because βl is associated with a negative weight
in the objective function, doing so will act against
maximizing the objective function. This way, we
discourage the ILP solver to preserve the original
lexicon as it is.
To verify the constraint(5) further, supposethat
the ILP solver assigns‘1’ for all variables in φ(el)
except for one variable. (Notice that doing so cor-
responds to proposing a new polarity for one of
the words in the given expression el.) Then the
constraint (5) will hold regardless of whether the
ILP solver assigns ‘0’ or ‘1’ to βl. Because βl is
associated with a negative weight in the objective
function,theILPsolverwillthenassign‘0’toβl to
maximize the objective function. In other words,
weencouragetheILPsolvertomodifytheoriginal
lexicon for the given expression el.
We use this type of soft constraint in order to
cope with the following two noise factors: first, it
is possible that some annotations are noisy. Sec-
ond, f(el,L) is not perfect, and might not be able
to make a correct prediction even with the correct
word-level polarities.
Next we encode a constraint that is the oppo-
site of the previous one. That is, if the polarity
classification algorithm f(el,L) makes a correct
prediction on el using the original lexiconL, then
we encourage ILP to keep the original word-level
polarities for words in el.
summationdisplay
xδi∈pi(el)
xδi >= |pi(el)|−|pi(el)|βl (6)
Interpretation of constraint (6) with the auxil-
iary binary variable βl is similar to that of con-
straint (5) elaborated above.
Notice that in equation (5), we encouraged ILP
to fix the current lexicon L for words in el, but
we have not specified the consequence of a mod-
ified lexicon (Lprime) in terms of expression-level po-
larity classification f(el,Lprime). Certain changes to
L might not fix the prediction error for el, and
those might even cause extra incorrect predictions
for other expressions. Then it wouldseem that we
need to replicate constraints (5) & (6) for all per-
mutations of word-level polarities. However, do-
ing so wouldincur exponentiallymany number of
constraints (4|el|) for each expression.4
To make the problem more practically solv-
able, we only consider changes to the lexicon that
are within edit-one distance with respect to pi(el).
More formally, let us define piprime(el) to be the set of
content words appearing in el, together with the
most likelypolarityproposed by a modified polar-
ity lexiconLprime. Then we need to considerall piprime(el)
such that|piprime(el)∩pi(el)|=|pi(el)|−1. There are
(4−1)|el|numberofdifferentpiprime(el),andweindex
them as piprimek(el). We then add followingconstraints
similarly as equation(5) & (6):
summationdisplay
xδi∈piprimek(el)
xδi <= |piprimek(el)|−1 +β(l,k) (7)
if the polarity classification algorithm f(·) makes
an incorrect prediction based on piprimek(el). And,
summationdisplay
xδi∈piprimek(el)
xδi >= |piprimek(el)|−|piprimek(el)|β(l,k) (8)
if the polarity classification algorithm f(·) makes
a correct prediction based on piprimek(el). Remember
that none of the constraints (5) (8) enforces as-
signment pi(el) or piprimek(el) as a hard constraint. In
order to enforce at least one of them to be chosen,
we add the followingconstraint:
summationdisplay
xδi∈pi(el)
xδi >= |pi(el)|−1 (9)
This constraint ensures that the modified lexi-
conLprime is not drastically different fromL. Assum-
ing that the initial lexiconLis a reasonably good
one, constraining the search space forLprime will reg-
ulate thatLprime does not turn into a degenerative one
that overfits to the current corpusC.
Objective function: Finally, we introduce our
full objective function.
4For certain simple polarity classification algorithm
f(el,L), it is possibleto write polynomially manynumberof
constraints. However our approach intends to be more gen-
eral by treating f(el,L) as a black box, so that algorithms
that do not factor nicely can also be consideredas an option.
593
maximize
summationdisplay
i
parenleftbigg
w+i x+i + w=i x=i
+ w−i x−i + w¬i x¬i
− wααi
parenrightbigg
−
summationdisplay
l
wβρlβl
−
summationdisplay
l,k
wβρ(l,k)β(l,k) (10)
We have already described the first part of the
objectivefunction (equation(3)), thuswe onlyde-
scribe the last two terms here. wβ is defined simi-
larly as wα; it is a constant weight that applies for
any auxiliary binary variable βl and β(l,k).
We further define ρl and ρ(l,k) as secondary
weights,oramplifierstoadjusttheconstantweight
wβ. To enlighten the motivation behind the am-
plifiers ρl and ρ(l,k), we bring out the following
observations:
1. Among the incorrect predictions for
expression-level polarity classification,
some are more incorrect than the other.
For instance, classifying positive class to
negative class is more wrong thanclassifying
positive class to neutral class. Therefore, the
cost of not fixing very incorrect predictions
should be higher than the cost of not fixing
less incorrect predictions. (See [R2] and
[R3] in Table 1.)
2. Ifthecurrentassignmentpi(el)for expression
el yieldsa correct predictionusingthe classi-
fier y(el,L), then there is not much point in
changingLtoLprime, even if y(el,Lprime) alsoyields
a correct prediction. In this case, we would
liketoassignslightlyhigherconfidenceinthe
original lexiconLthen the new oneLprime. (See
[R1] in Table 1.)
3. Likewise, if the current assignment pi(el) for
expression el yields an incorrect prediction
using the classifier y(el,L), then there is not
much point in changingLtoLprime, if y(el,Lprime)
also yields an equally incorrect prediction.
Againwe assignslightlyhigherconfidencein
the original lexiconLthan the new oneLprime in
such cases. (Compare each row in [R2] with
a corresponding row in [R3] in Table 1.)
[R1] If pi(el) correct ρl ←1.5
If piprimek(el) correct ρ(l,k) ←1.0
[R2] If pi(el) very incorrect ρl ←1.0
If pi(el) less incorrect ρl ←0.5
[R3] If piprimek(el) very incorrect ρ(l,k) ←1.5
If piprimek(el) less incorrect ρ(l,k) ←1.0
Table 1: The value of amplifiers ρl and ρ(l,k).
To summarize, for correct predictions, the de-
gree of ρ determines the degree of cost of (unde-
sirably) altering the current lexicon for el. For in-
correct predictions,the degree of ρ determines the
degree of cost of not fixing the current lexicon for
el.
3 Experiments
In the experiment section, we seek for answers for
the followingquestions:
Q1 What is the effect of a polarity lexicon on the
expression-level polarity classification task?
In particular, is it useful when using a ma-
chinelearningtechniquethatmightbeableto
learn the necessary polarity information just
based on the wordsin the training data, with-
out consultinga dictionary? (Section 3.1)
Q2 What is the effect of an adapted polarity lex-
icon on the expression-level polarity classifi-
cation task? (Section 3.2)
Notice that we include the neutral polarity in the
polarity classification. It makes our task much
harder (e.g., Wilson et al. (2009)) than those that
assume inputs are guaranteed to be either strongly
positive or negative (e.g., Pang et al. (2002), Choi
and Cardie (2008)). But in practice, one can-
not expect that a given input is strongly polar, as
automatically extracted opinions are bound to be
noisy. Furthermore, Wiebe et al. (2005) discuss
that some opinion expressions do carry a neutral
polarity.
We experiment with the Multi-Perspective
Question Answering (MPQA) corpus (Wiebe et
al., 2005) for evaluation. It contains535 newswire
documents annotated with phrase-level subjectiv-
ity information. We evaluate on all opinion ex-
pressions that are known to have high level of
inter-annotator agreement. That is, we include
opinions with intensity marked as ‘medium’ or
594
higher, and exclude those with annotation confi-
dence marked as ‘uncertain’. To focus our study
onthe directinfluence of thepolaritylexicon upon
the sentiment classification task, we assume the
boundaries of the expressions are given. How-
ever, our approach can be readily used in tan-
dem with a system that extracts opinion expres-
sions (e.g., Kim and Hovy (2005), Breck et al.(2007)). Performance is reported using 10-fold
cross-validationon 400 documents, and a separate
135 documents were used as a development set.
For the general-purpose polarity lexicon, we ex-
pand the polarity lexicon of Wilson et al. (2005)
with General Inquirer dictionary as suggested by
Choi and Cardie (2008).
We reporttheperformance intwomeasures: ac-
curacy for 3-way classification, and average error
distance. The reasonwhy we consideraverageer-
ror distance is because classifying a positiveclass
into a negative class is worse than classifying a
positive class into a neutral one. We define the er-
ror distance between ‘neutral’ class and any other
class as 1, while the error distance between ‘posi-
tive’ class and ‘negative’ class as 2. If a predicted
polarity is correct, then the error distance is 0. We
compute the error distance of each prediction and
take the average over all predictions in the test
data.
3.1 Experiment-I: Effect of a Polarity
Lexicon
To verify the effect of a polarity lexicon on the
expression-level polarity classification task, we
experiment with simple classification-based ma-
chine learning technique. We use the Mallet
(McCallum, 2002) implementation of Conditional
Random Fields (CRFs) (Lafferty et al., 2001).5 To
highlight the influence of a polarity lexicon, we
compare the performance of CRFs with and with-
out features derived from polarity lexicons.
Features: We encode basic features as words
and lemmas for all content words in the given ex-
pression. Theperformance of CRFs usingonlythe
basic features are given in the first row of the Ta-
ble 2. Next we encode features derived from po-
larity lexicons as follows.
• The output of Vote & Flip algorithm. (Sec-
tion 3.2 & Figure 2.)
5We use the CRF implementation of Mallet (McCallum,
2002)with Markov-order0, whichis equivalentto Maximum
Entropy models (Berger et al. (1996)).
Accuracy Avg. Error Distance
Without Lexicon 63.9 0.440
With Lexicon 70.4 0.334
Table2: Effectofapolaritylexicononexpression-
level classification using CRFs
• Number of positive, neutral, negative, and
negators in the given expression.
• Number of positive (or negative) words in
conjunctionwith number of negators.
• (boolean) Whether the number of positive
words dominates negative ones.
• (boolean) Whether the number of negative
words dominates positive ones.
• (boolean) None of the above two cases
• Each of the above three boolean values in
conjunctionwith the number of negators.
Results: Table 2 shows the performance of
CRFs with and without features that consult the
general-purpose lexicon. As expected, CRFs can
perform reasonably well (accuracy = 63.9%) even
without consulting the dictionary, by learning di-
rectly from the data. However, having the polarity
lexicon boosts the performance significantly (ac-
curacy = 70.4%), demonstrating that lexical re-
sources are very helpfulforfine-grained sentiment
analysis. The difference in performance is statisti-
cally significant by paired t-test for both accuracy
(p < 0.01) and average error distance (p < 0.01).
3.2 Experiment-II: Adapting a Polarity
Lexicon
In thissection,weassessthequalityoftheadapted
lexiconinthe contextofan expression-levelpolar-
ity classification task. In order to perform the lex-
icon adaptation via ILP, we need an expression-
level polarity classification algorithm f(el,L) as
described in Section 2. According to Choi and
Cardie (2008), voting algorithms that recognize
content-word negators achieve a competitive per-
formance, so we will use a variant of it for sim-
plicity. Because none of the algorithms proposed
by Choi and Cardie (2008) is designed to handle
the neutral polarity, we invent our own version as
shown in Figure 2.
595
For eachexpression ei,
nPositive←# of positive words in ei
nNeutral←# of neutral words in ei
nNegative←# of negative words in ei
nNegator←# of negating words in ei
if (nNegator % 2 = 0)
then fFlipPolarity←false
else
then fFlipPolarity←true
if (nPositive > nNegative) &¬fFlipPolarity
then Polarity(ei)←positive
else if (nPositive > nNegative) & fFlipPolarity
then Polarity(ei)←negative
else if (nPositive < nNegative) &¬fFlipPolarity
then Polarity(ei)←negative
else if (nPositive < nNegative) & fFlipPolarity
then Polarity(ei)←neutral
else if nNeutral > 0
then Polarity(ei)←neutral
else
then Polarity(ei)←default polarity (the most
prominent polarity in the corpus)
Figure 2: Vote & Flip Algorithm
It might look a bit complex at first glance,
but the intuition is simple. The variable
fFlipPolarity determines whether we need to
fliptheoverallmajoritypolaritybasedonthenum-
ber of negators in the given expression. If the
positive (or negative) polarity words dominate the
given expression, and if there is no need to flip
the majority polarity, then we take the positive(or
negative) polarity as the overall polarity. If the
positive (or negative) polarity words dominate the
given expression, and if we need to flip the major-
ity polarity, then we take the negative (or neutral)
polarity as the overall polarity.
Noticethattheresultofflippingthenegativepo-
larityisneutral,notpositive. Inourpilotstudy,we
found that this strategy works better than flipping
the negative polarity to positive.6 Finally, if the
number of positive words and the negative words
tie, and there is any neutral word, then we assign
the neutral polarity. In thiscase, we don’t worry if
6This finding is not surprising. For instance, if we con-
sider the polarity of ”She did not get hurt much from the ac-
cident.”, it can be viewed as neutral; although it is good that
one did not hurt much, it is still bad that there was an acci-
dent. Hence it gives a mixed feeling, which corresponds to
the neutral polarity.
there isa negator, because flippinga neutral polar-
itywouldstillresultinaneutralpolarity. Ifnoneof
above conditionismet, thanwe defaulttothemost
prominent polarity of the data, which is the nega-
tive polarity in the MPQA corpus. We name this
simple algorithm as Vote & Flip algorithm. The
performance is shown in the first row in Table 2.
Nextwe describetheimplementationpartofthe
ILP. For 10fold-crossvalidation,we formulatethe
ILP problem using the training data (360 docu-
ments), and then test the effect of the adapted lex-
icon on the remaining 40 documents. We include
only those content words that appeared more than
3 times in the training data. From the pilottest us-
ing the development set, we picked the value of
wβ as 0.1. We found that having the auxiliary
variables αl which allow more than one polarity
per word does not necessarily help with the per-
formance, so we omitted them. We suspect it is
because the polarity classifiers we experimented
with is not highly capable of disambiguating dif-
ferent lexical usages and select the right polarity
for a given context. We use CPLEX integer pro-
gramming solver to solve our ILP problems. On a
machine with 4GHz CPU, it took several minutes
to solve each ILP problem.
In order to assess the effect of the adapted lex-
icon using CRFs, we need to first train the CRFs
model. Using the same training set used for the
lexicon adaptation would be suboptimal, because
the features generated from the adapted lexicon
will be unrealisticallygood in that particular data.
Therefore, weprepared a separatetrainingdata for
CRFs using135 documents from the development
set.
Results: Table 3 shows the comparison of the
original lexicon and the adapted lexicon in terms
of polarity classification performance using the
Vote & Flip algorithm. The adapted lexicon im-
proves the accuracy as well as reducing the aver-
age error distance. The difference in performance
is statistically significant by paired t-test for both
accuracy (p < 0.01) and average error distance
(p < 0.01).
Table 4 shows the comparison of the original
lexicon and the adapted lexicon using CRFs. The
improvement isnotas substantialasthatofVote&
Flipalgorithmbutthedifference inperformance is
alsostatisticallysignificantforbothaccuracy (p =
0.03) and average error distance (p = 0.04).
596
Accuracy Avg. Error Distance
Original Lexicon 64.2 0.395
Adapted Lexicon 67.0 0.365
Table 3: Effect of an adapted polarity lexicon on
expression-level classification using the Vote &
Flip Algorithm
Accuracy Avg. Error Distance
Original Lexicon 70.4 0.334
Adapted Lexicon 71.2 0.327
Table 4: Effect of an adapted polarity lexicon on
expression-level classificationusing CRFs
4 Related
Work
There are a number of previous work that focus
on building polarity lexicons (e.g., Takamura et
al. (2005), Kaji and Kitsuregawa (2007), Rao and
Ravichandran (2009)). But most of them evalu-
ated their lexicon in isolationfrom any potentially
relevant NLP task, and it is unclear how the new
lexicon might affect end-to-end performance of a
concrete NLPapplication. Ourworkdiffersinthat
we try to draw a bridge between general purpose
lexical resources and a domain-specific NLP ap-
plication.
Kim and Hovy (2005) and Banea et al. (2008)
present bootstrappingmethods to construct a sub-
jectivitylexiconand measure the effect of the new
lexicon for sentence-level subjectivity classifica-
tion. However, their lexicons only tell whether a
wordisa subjectiveone, butnotthe polarityof the
sentiment. Furthermore, the construction of lexi-
con is still an isolated step from the classification
task. Ourworkontheotherhandallowstheclassi-
fication task to directly influence the construction
of lexicon, enabling the lexicon to be adapted for
a concrete NLP application and for a specific do-
main.
Wilson et al. (2005) pioneered the expression-
level polarity classification task using the MPQA
corpus. The experimental results are not directly
comparable to ours, because Wilson et al. (2005)
limit the evaluation only for the words that ap-
peared in their polarity lexicon. Choi and Cardie
(2008) also focus on the expression-level polarity
classification, but their evaluation setting is not as
practical as ours in that they assume the inputsare
guaranteed to be either strongly positive or nega-
tive.
5 Conclusion
In this paper, we present a novel lexicon adapta-
tion technique based on integer linear program-
ming to reflect the characteristics of the domain
more directly. In particular, our method collec-
tively considers the relations among words and
opinion expressions to derive the most likely po-
larity of each lexical item for the given domain.
We evaluate the effect of our lexicon adaptation
technique in the context of a concrete NLP ap-
plication: expression-level polarity classification.
The positiveresults from our experiments encour-
agefurtherresearch forlexicalresourceadaptation
techniques.
Acknowledgments
This work was supported in part by National Sci-
ence Foundation Grant BCS-0624277 and by the
Department of Homeland Security under ONR
Grant N0014-07-1-0152. We also thank the
EMNLP reviewers for insightfulcomments.
References
Alina Andreevskaia and Sabine Bergler. 2008. When
Specialists and Generalists Work Together: Over-
coming Domain Dependence in Sentiment Tagging.
ACL
Alina Andreevskaia and Sabine Bergler. 2006. Min-
ing WordNet For a Fuzzy Sentiment: Sentiment Tag
Extraction From WordNet Glosses. EACL
Carmen Banea, Rada Mihalcea, and JanyceWiebe.
2008. A Bootstrapping Method for Building Sub-
jectivity Lexicons for Languages with Scarce Re-
sources. LREC
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. In Computational Lin-
guistics, 22(1)
John Blitzer, Mark Dredze, and Fernando Pereira.
2007. Biographies, Bollywood, Boom-boxes, and
Blenders: Domain Adaptation for Sentiment Classi-
fication. Association for Computational Linguistics
ACL 2007
Eric Breck, Yejin Choi and Claire Cardie. 2007. Iden-
tifyingExpressionsofOpinioninContext. InIJCAI.
Yejin Choi and Claire Cardie. 2008. Learning with
CompositionalSemantics as StructuralInferencefor
Subsentential Sentiment Analysis. EMNLP
Noam Chomsky. 1965. Aspects of the theory of syn-
tax. Cambridge, MA: MIT Press.
597
Pascal Denis and Jason Baldridge. 2007. Joint deter-
mination of anaphoricity and coreference resolution
using integer programming. NAACL
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
WordNet: A Publicly Available Lexical Resource
for Opinion Mining. In Proceedings of 5th Con-
ference on Language Resources and Evaluation
(LREC),.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
ACM SIGKDD International Conference on Knowl-
edge Discovery & Data Mining(KDD-2004).
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing Lexicon for Sentiment Analysis from Massive
Collection of HTML Documents. In EMNLP-
CoNLL.
Hiroshi Kanayama Tetsuya Nasukawa. 2006. Fully
Automatic Lexicon Expansion for Domain-oriented
Sentiment Analysis. In ACL.
Alistair Kennedy and Diana Inkpen. 2005. Sentiment
Classification of Movie and Product Reviews Us-
ing Contextual Valence Shifters. In Proceedings of
FINEXIN 2005, Workshop on the Analysis of Infor-
mal and Formal Information Exchange during Ne-
gotiations.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of COL-
ING.
Soo-MinKim and Eduard Hovy. 2005. AutomaticDe-
tection of OpinionBearing Wordsand Sentences. In
Companion Volume to the Proceedings of the Sec-
ond International Joint Conference on Natural Lan-
guage Processing (IJCNLP-05)
John Lafferty, Andrew Kachites McCallum and Fer-
nando Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. In ICML.
Andrew Kachites McCallum. 2002. MAL-
LET: A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Karo Moilanen and Stephen Pulman. 2007. Sentiment
Composition. In Proceedings of Recent Advances in
Natural Language Processing (RANLP 2007).
Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification using
Machine Learning Techniques. In EMNLP.
Joseph Pickett et al. 1996. The American heritage
book of English usage: A practical and authoritative
guide to contemporary English. Houghton Mifflin
Company.
Livia Polanyi and Annie Zaenen. 2004. Contextual
lexical valence shifters. In Exploring Attitude and
Affect in Text: Theories and Applications: Papers
from the 2004 Spring Symposium, AAAI.
Delip Rao and Deepak Ravichandran. 2009. Semi-
Supervised PolarityLexicon Induction. In EACL.
Dan Roth and Wen-tau Yih. 2004. A Linear Program-
ming Formulation for Global Inference in Natural
Language Tasks. In CoNLL.
HiroyaTakamura, TakashiInui,andManabu Okumura.
2005. Extracting semantic orientationsof words us-
ing spin model. In ACL.
Janyce Wiebe, Theresa Wilson and Claire Cardie.
2005. Annotatingexpressions of opinions and emo-
tionsinlanguage. In LanguageResources and Eval-
uation (formerly Computers and the Humanities),
39(2-3):165210.
Theresa Wilson, Janyce Wiebe and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of
HLT/EMNLP.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing Contextual Polarity: an explo-
ration of features for phrase-level sentiment analy-
sis. In ComputationalLinguistics 35(3).
598

