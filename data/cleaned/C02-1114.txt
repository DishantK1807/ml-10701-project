A Graph Model for Unsupervised Lexical Acquisition Dominic Widdows and Beate Dorow Center for the Study of Language and Information 210 Panama Street Stanford University Stanford CA 94305-4115 fdwiddows,beateg@csli.stanford.edu Abstract Thispaperpresentsanunsupervisedmethodfor assemblingsemanticknowledgefromapart-ofspeechtaggedcorpususinggraphalgorithms.
The graph model is built by linking pairs of wordswhichparticipateinparticularsyntactic relationships.
Wefocusonthesymmetricrelationshipbetweenpairsofnounswhichoccurtogetherinlists.Anincrementalcluster-building algorithmusingthispartofthegraphachieves 82%accuracyatalexicalacquisitiontask,evaluatedagainstWordNetclasses.Themodelnaturallyrealisesdomainandcorpusspeci cambiguitiesasdistinctcomponentsinthegraph surroundinganambiguousword.
1 Introduction
Semanticknowledgeforparticulardomainsis increasinglyimportantinNLP.ManyapplicationssuchasWord-SenseDisambiguation,InformationExtractionandSpeechRecognition all require lexicons.
The coverage of handbuiltlexicalresourcessuchasWordNet(Fellbaum,1998)hasincreaseddramaticallyinrecent years, but leaves several problems and challenges.
Coverage is poor in many critical,rapidlychangingdomainssuchascurrent a airs, medicineandtechnology, wheremuch timeisstillspentbyhumanexpertsemployed to recognise and classify new terms.
Most languages remain poorly covered in comparisonwithEnglish.
Hand-builtlexicalresources whichcannotbeautomaticallyupdatedcanoftenbesimplymisleading.
Forexample,using WordNettorecognisethatthewordapplerefers toafruitoratreeisagraveerrorinthemany situationswherethiswordreferstoacomputer manufacturer,asensewhichWordNetdoesnot cover.ForNLPtoreachawiderclassofapplicationsinpractice,theabilitytoassembleand updateappropriatesemantic knowledgeautomaticallywillbevital.
Thispaperdescribesamethodforarranging semantic information into a graph (Bollob as, 1998),wherethenodesarewordsandtheedges (also called links) represent relationships betweenwords.Thepaperisarrangedasfollows.
Section 2 reviews previous work on semantic similarityandlexicalacquisition.Section3describeshowthegraphmodelwasbuiltfromthe PoS-taggedBritishNationalCorpus.Section4 describesanewincrementalalgorithmusedto buildcategoriesofwordsstepbystepfromthe graphmodel.Section5demonstratesthisalgorithminactionandevaluatestheresultsagainst WordNetclasses,obtainingstate-of-the-artresults.Section6describeshowthegraphmodel canbeusedtorecognisewhenwordsarepolysemousandtoobtaingroupsofwordsrepresentativeofthedi erentsenses.
2 Previous
Work Mostworkonautomaticlexicalacquisitionhas been based at some point on the notion of semantic similarity.
The underlyingclaim is thatwordswhicharesemanticallysimilaroccur withsimilar distributionsandinsimilarcontexts(MillerandCharles,1991).
Themainresultstodateinthe eldofautomaticlexicalacquisitionareconcernedwith extractinglistsofwordsreckonedtobelongtogetherinaparticularcategory,suchasvehicles orweapons(Rilo andShepherd,1997)(Roark andCharniak,1998).
RoarkandCharniakdescribea\genericalgorithm"forextractingsuch listsofsimilarwordsusingthenotionofsemanticsimilarity,asfollows(RoarkandCharniak, 1998,x1).
1.Foragivencategory,chooseasmall setofexemplars(or‘seedwords’) 2.
Countco-occurrenceofwordsand seedwordswithinacorpus 3.
Usea gureofmeritbasedupon thesecountstoselectnewseedwords 4.Returntostep2anditeratentimes 5.Usea gureofmerittorankwords forcategorymembershipandoutputa rankedlist Algorithms of this type were used by Rilo andShepherd(1997)andRoarkandCharniak (1998), reporting accuracies of 17% and35% respectively.
Likethealgorithmwepresentin Section5,thesimilaritymeasure(or‘ gureof merit’) used inthese cases was based on cooccurrenceinlists.
Bothoftheseworksevaluatedtheirresults byaskinghumanstojudgewhetheritemsgeneratedwereappropriatemembersofthecategoriessought.
Rilo andShepherd(1997)also givesomecreditfor‘relatedwords’(forexample crashmightberegardedasbeingrelatedtothe categoryvehicles).
One problem with these techniques is the dangerof‘infections’|onceanyincorrector out-of-category word has been admitted, the neighboursofthiswordarealsolikelytobeadmitted.
InSection4wepresentanalgorithm whichgoessomewaytowardsreducingsuchinfections.
Theearlyresultshavebeenimproveduponby Rilo andJones(1999),wherea‘mutualbootstrapping’approachisusedtoextractwordsin particular semantic categories and expression patternsforrecognisingrelationshipsbetween thesewordsforthepurposesofinformationextraction.
Theaccuracyachievedinthisexperimentissometimesashighas78%andisthereforecomparabletotheresultsreportedinthis paper.
Anotherwaytoobtainword-sensesdirectly from corpora is to use clustering algorithms onfeature-vectors(Lin,1998; Sch utze, 1998).
Clusteringtechniquescanalsobeusedtodiscriminatebetweendi erentsensesofanambiguousword.
Ageneralproblemforsuchclusteringtechniquesliesinthequestionofhowmany clustersoneshouldhave,i.e.
howmanysenses areappropriateforaparticularwordinagiven domain(ManningandSch utze,1999,Ch14).
Lin’sapproachtothisproblem(Lin,1998)is tobuilda‘similaritytree’(usingwhatisineffectahierarchicalclusteringmethod)ofwords relatedtoatargetword(inthiscasetheword duty).Di erentsensesofdutycanbediscerned asdi erentsub-treesofthissimilaritytree.We presentanewmethodforword-sensediscriminationinSection6.
3 Building
a Graph from a PoS-tagged Corpus Inthissectionwedescribehowagraph|a collection of nodesand links | was built to representtherelationshipsbetweennouns.The modelwasbuiltusingtheBritishNationalCorpuswhichisautomaticallytaggedforpartsof speech.
Initially,grammaticalrelationsbetweenpairs ofwordswereextracted.
Therelationshipsextractedwerethefollowing: Noun(assumedtobesubject)Verb VerbNoun(assumedtobeobject) AdjectiveNoun NounNoun(oftenthe rstnounismodifyingthesecond) Nounand/orNoun Thelastoftheserelationshipsoftenoccurs whenthepairofnounsispartofalist.
Since listsareusuallycomprisedofobjectswhichare similar in some way, these relationships have beenusedtoextractlistsofnounswithsimilar properties(Rilo andShepherd,1997)(Roark andCharniak,1998).
Inthispaperwetoofocusonnounsco-occurringinlists.
Thisisbecausethenounand/ornounrelationshipisthe onlysymmetricrelationshipinourmodel,and symmetricrelationshipsaremucheasiertomanipulatethanasymmetricones.Ourfullgraph containsmanydirectedlinksbetweenwordsof di erent parts of speech.
Initial experiments withthismodelshowconsiderablepromisebut areattooearlyastagetobereporteduponyet.
Thusthegraphusedinmostofthispaperrepresentsonlynouns.Eachnoderepresentsanoun andtwonodeshavealinkbetweenthemifthey co-occurseparatedbytheconjunctionsandor or,andeachlinkisweightedaccordingtothe numberoftimestheco-occurrenceisobserved.
Variouscuto functionswereusedtodeterminehowmanytimesarelationshipmustbe observedtobecountedasalinkinthegraph.
Awell-behavedoptionwastotakethetop n neighboursofeachword,wherencouldbedetermined by the user.
In this way the linkweightingschemewasreducedtoalink-ranking scheme.
Oneconsequenceofthisdecisionwas thatlinkstomorecommonwordswerepreferred overlinkstorarerwords.
Thisdecisionmay havee ectivelyboostedprecisionattheexpense ofrecall,becausethepreferredlinksaretofairly commonand(probably)morestablewords.Researchisneedtorevealtheoreticallymotivated orexperimentallyoptimaltechniquesforselectingtheimportancetoassigntoeachlink|the choicesmadeinthisareasofarareoftenofan adhocnature.
Thegraphusedintheexperimentsdescribed has 99,454 nodes (nouns) and 587,475 links.
There were roughly 400,000 di erent types tagged as nouns in the corpus, so the graph model represents about one quarter of these nouns, including most of the more common ones.
4 An
Incremental Algorithm for Extracting Categories of Similar Words Inthissectionwedescribeanewalgorithmfor addingthe‘mostsimilarnode’toanexisting collection ofnodesinawaywhichincrementallybuildsastablecluster.
Werelyentirely uponthegraphtodeducetherelativeimportanceofrelationships.
Inparticular,ouralgorithmisdesignedtoreduceso-called‘infections’ (RoarkandCharniak,1998,x3)wheretheinclusionofanout-of-categorywordwhichhappens toco-occurwithoneofthecategorywordscan signi cantlydistortthe nallist.
Hereistheprocessweusetoselectandadd the‘mostsimilarnode’toasetofnodes: De nition1Let A be a set of nodes and let N(A), the neighbours of A, be the nodes whicharelinkedtoany a 2 A.
(So N(A)=S a2AN(a).) Thebestnewnodeistakentobethenode b2N(A)nAwiththehighestproportionoflinks toN(A).Moreprecisely,foreachu2N(A)nA, letthea nitybetweenuandAbegivenbythe ratio jN(u)\N(A)j jN(u)j : Thebestnewnode b 2 N(A)nA isthenode whichmaximisesthisa nityscore.
Thisalgorithmhasbeenbuiltintoanon-line demonstration where the user inputs a given seedwordandcanthenseetheclusterofrelatedwordsbeinggraduallyassembled.
The algorithm is particularly e ective at avoiding infections arising from spurious cooccurrencesandfromambiguity.
Consider,for example,thegraphbuiltaroundthewordappleinFigure6.Supposethatwestartwiththe seed-listapple,orange,banana.
Howevermany timesthestring\AppleandNovell"occursin thecorpus,thenovellnodewillnotbeadded tothislistbecauseitdoesn’thavealinktoorange,bananaoranyoftheirneighboursexcept forapple.
Onewaytosummarisethee ectof thisdecisionisthatthealgorithmaddswords toclustersdependingontypefrequencyrather thantokenfrequency.Thisavoidsspuriouslinks dueto(forexample)particularidiomsrather thangeniunesemanticsimilarity.
5 Examples
and Evaluation Inthissectionwegiveexamplesoflexicalcategoriesextractedbyourmethodandevaluate themagainstthecorrespondingclassesinWordNet.
5.1 Methodology
Our methodology is as follows.
Consider an intuitive category of objects such as musical instruments.
De ne the ‘WordNet class’ or ‘WordNetcategory’ofmusicalinstrumentsto bethecollectionofsynsetssubsumedinWordNetbythemusicalinstrumentssynset.
Takea ‘protypical example’ of a musical instrument, suchas piano.
Thealgorithm de nedin (1) givesawayof ndingthennodesdeemedtobe mostcloselyrelatedtothepianonode.
These canthenbechecked toseeiftheyaremembers of the WordNet class of musical instruments.Thismethodiseasiertoimplementand lessopentovariationthanhumanjudgements.
WhileWordNetoranyotherlexicalresourceis notaperfectarbiter,itishopedthatthisexperimentprocedureisbothreliableandrepeatable.
Thetenclassesofwordschosenwerecrimes, places, tools, vehicles, musical instruments, clothes,diseases,bodyparts,academicsubjects andfoodstu s.
Theclasseswerechosenbefore theexperimentwascarriedoutsothattheresultscouldnotbemassagedtoonlyusethose classeswhichgavegoodresults.(The rst4categoriesarealsousedby(Rilo andShepherd, 1997)and(RoarkandCharniak,1998)andso wereincludedforcomparison).
Havingchosen theseclasses,20wordswereretrievedusinga singleseed-wordchosenfromtheclassinquestion.
Thislistofwordsclearlydependsontheseed wordchosen.
Whilewehavetriedtooptimise thischoice,itdependsonthecorpusandthe themodel.
Thein uenceofsemantic PrototypeTheory(Rosch,1988)isapparentinthis process,alinkwewouldliketoinvestigatein moredetail.Itispossibletochooseanoptimal seedwordforaparticularcategory:itshouldbe possibletocomparetheseoptimalseedwords withthe‘prototypes’suggestedbypsychologicalexperiments(MervisandRosch,1981).
5.2 Results
TheresultsforalistoftenclassesandprototypicalwordsaregiveninTable1.Wordswhich arecorrectmembersoftheclassessoughtare in Roman type: incorrect results are in italics.
Thedecisionbetweencorrectnessandincorrectnesswasmadeonastrictbasisforthe sakeofobjectivityandtoenabletherepeatability ofthe experiment: wordswhichare in WordNetwerecountedascorrectresultsonlyif theyareactualmembersoftheWordNetclass inquestion.
Thusbrigandageisnotregarded asacrimeeventhoughitisclearlyanactof wrongdoing,orchestraisnotregardedasamusicalinstrumentbecauseitisacollectionofinstrumentsratherthanasingleinstrument,etc.
Theonlyexceptionswehavemadearetheterms wyndandplanetology(markedinbold),which arenotinWordNetbutarecorrect nonetheless.
Theseconditionsareatleastasstringent asthoseofpreviousexperiments,particularly thoseofRilo andShepherd(1997)whoalso givecreditforwordsassociatedwithbutnot belongingtoaparticularcategory.(Ithasbeen pointedoutthatmanypolysemouswordsmay occurinseveralclasses,makingthetaskeasier becauseformanywordsthereareseveralclasses whichouralgorithmwouldgivecreditfor.) With these conditions, our algorithm retrievesonly36incorrecttermsoutofatotal of200,givinganaccuracyof82%.
5.3 Analysis
Ourresultsareanorderofmagnitudebetter than those reported by Rilo and Shepherd (1997) and Roark andCharniak (1998), who reportaverageaccuraciesof17%and35%respectively.
(Ourresultsarealsoslightlybetter thanthosereportedbyRilo andJones(1999)).
Since the algorithms used are in many ways verysimilar,thisimprovementdemandsexplanation.
Someofthedi erenceinaccuracycanbeattributedtothecorporaused.Theexperiments in(Rilo andShepherd,1997)wereperformed onthe500,000wordMUC-4corpus,andthose of(RoarkandCharniak,1998)wereperformed usingMUC-4andtheWallStreetJournalcorpus(some30millionwords).
Ourmodelwas built using the British National Corpus (100 millionwords).
Ontheotherhand,ourmodel wasbuiltusingonlyapart-of-speechtaggedcorpus.Thehighaccuracyachievedthusquestions theconclusiondrawnbyRoarkandCharniak (1998)that‘parsingisinvaluable’.
Ourresults clearlyindicatethatalargePoS-taggedcorpus maybemuchbetterforautomaticlexicalacquisitionthanasmallfully-parsedcorpus.This claimcouldofcoursebetestedbycomparing techniquesonthesamecorpus.
ToevaluatetheadvantageofusingPoSinformation,wecomparedthegraphmodelwitha similaritythesaurusgeneratedusingLatentSemanticIndexing(ManningandSch utze,1999, Ch15),a‘bag-of-words’approach,onthesame corpus.
The same number of nouns was retrieved for each class using the graph model andLSI.TheLSIsimilaritythesaurusobtained anaccuracyof31%,muchlessthanthegraph model’s 82%.
This is because LSI retrieves wordswhicharerelatedbycontextbutarenot inthesameclass: forexample,theneighbours ofpianofoundusingLSIcosine-similarityonthe BNCcorpusincludewordssuchascomposer, music,Bach,concertoanddance,whicharerelated butcertainly not inthe same semantic class.
TheincrementalclusteringalgorithmofDefinition(1)workswellatpreventing‘infections’ Class SeedWord NeighboursProducedbyGraphModel crimes murder crimetheftarsonimportuningincestfraudlarcenyparricide burglaryvandalismindecencyviolenceo encesabusebrigandagemanslaughterpillageraperobberyassaultlewdness places park pathvillagelaneview eldchurchsquareroadavenuegarden castlewyndgaragehousechapeldrivecrescenthomeplace cathedralstreet tools screwdriver chiselnavillenailshoulderknifedrillmatchstickmorgenthau gizmohandkneeelbowmalletpenknifegallielegarmsickle bolsterhammer vehicle conveyance train tramcardriverpassengerscoachlorrytruckaeroplanecoons planetrailerboattaxipedestriansvansvehiclesjeepbusbuses helicopter musical instruments piano fortepianoorchestramarimbaclarsachviolincizekviolaoboe utehornbassoonculbonemandolinclarinetequiluzcontrabasssaxophoneguitarcello clothes shirt chapeaubrascardigantrousersbreechesskirtjeansbootspair shoesblousedresshatwaistcoatjumpersweatercoatcravat tieleggings diseases typhoid malariaaidspoliocancerdiseaseatelectasisillnessescholera hivdeathsdiphtheriainfectionshepatitistuberculosiscirrhosisdiptheriabronchitispneumoniameaslesdysentery bodyparts stomach headhipsthighsneckshoulderschestbackeyestoesbreasts kneesfeetfacebellybuttockshawsankleswaistlegs academic subjects physics astrophysicsphilosophyhumanitiesartreligionsciencepolitics astronomy sociology chemistry history theology economicsliteraturemathsanthropologyculturemathematics geographyplanetology foodstu s cake macaroonsconfectioneriescreamrollssandwichescroissant bunssconescheesebiscuitdrinkspastriesteadanishbutter lemonadebreadchocolateco eemilk Table1:Classesofsimilarwordsgivenbythegraphmodel.
andkeepingclusterswithinoneparticularclass. Thenotableexceptionisthetoolsclass,where thewordhandappearstointroduceinfection.
Inconclusion,itisclearthatthegraphmodel combinedwiththeincrementalclusteringalgorithmofDe nition1performsbetterthanmost previousmethodsatthetaskofautomaticlexicalacquisition.
6 Recognising
Polysemy Sofarwehavepresentedagraphmodelbuilt uponnounco-occurrencewhichperformsmuch betterthanpreviouslyreportedmethodsatthe taskofautomaticlexical acquisition.
Thisis animportanttask,becauseassemblingandtuninglexiconsforspeci cNLPsystemsisincreasingly necessary.
We nowtake astepfurther andpresentasimplemethodfornotonlyassemblingwordswithsimilarmeanings,butfor empiricallyrecognisingwhenawordhasseveral meanings.
Recognising and resolving ambiguity is an important task in semantic processing.
The traditional Word Sense Disambiguation (WSD)problemaddressesonlytheambiguityresolutionpartoftheproblem:compilingasuitablelistofpolysemouswordsandtheirpossible sensesisataskforwhichhumansaretraditionallyneeded(Kilgarri andRosenzweig,2000).
ThismakestraditionalWSDanintensivelysupervisedandcostlyprocess.
Breadthofcoveragedoesnotinitselfsolvethisproblem:general lexicalresourcessuchasWordNetcanprovide toomanysensesmanyofwhicharerarelyused inparticulardomainsorcorpora(Galeetal., 1992).
Thegraphmodelpresentedinthispapersuggestsanewmethodforrecognisingrelevantpolysemy.
Wewillneedasmallamountofterminologyfromgraphtheory(Bollob as,1998).
De nition2(Bollob as, 1998, Ch 1 x1) LetG=(V;E)beagraph,whereV istheset ofvertices(nodes)ofGandE V V isthe setofedgesofG.
Twonodesv1;vn aresaidtobeconnected ifthereexistsapathfv1;v2;:::;vn 1;vng suchthat(vj;vj+1)2Efor1 j<n.
Connectednessisanequivalencerelation. TheequivalenceclassesofthegraphGunderthisrelationarecalledthecomponents ofG.
Wearenowinapositiontode nethesenses ofawordasrepresentedbyaparticulargraph.
De nition3LetGbeagraphofwordsclosely relatedtoaseed-word w,andletGnw bethe subgraphwhichresultsfromtheremovalofthe seed-nodew.
The connected components of the subgraph Gnwarethesensesofthewordwwithrespect tothegraphG.
Asanillustrativeexample,considerthelocal graphgeneratedforthewordapple(6).Theremovaloftheapplenoderesultsinthreeseparate componentswhichrepresentthedi erentsenses ofapple:fruit,trees,andcomputers.De nition 3givesanextremelygoodmodelofthesenses ofapplefoundintheBNC.(Inthiscasebetter thanWordNetwhichdoesnotcontainthevery commoncorporatemeaning.) Theintuitivenotionofambiguitybeingpresentedisasfollows.
Anambiguouswordoften connectsotherwiseunrelatedareasofmeaning.
De nition3recognisestheambiguityofapple becausethiswordislinkedtobothbananaand novell,wordswhichotherwisehavenothingto dowithoneanother.
It is well-known that any graph can be thoughtofasacollectionoffeature-vectors,for examplebytakingtherow-vectorsintheadjacencymatrix(Bollob as,1998,Ch2x3).
There mightthereforebefundamentalsimilaritiesbetweenourapproachandmethodswhichrelyon similaritiesbetweenfeature-vectors.
Extra motivation for thistechnique is provided by Word-Sense Disambiguation.
The standardmethodforthistaskistousehandlabelled data to train a learning algorithm, whichwilloftenpickoutparticularwordsas Bayesianclassi erswhichindicateonesenseor theother.
(Soifmicrosoftoccursinthesame sentenceasapplewemighttakethisasevidence thatappleisbeingusedinthecorporatesense.) Clearly,thewordsinthedi erentcomponents inDiagram6canpotentiallybeusedasclassiersforjustthispurpose,obviatingtheneedfor time-consuminghumanannotation.
Thistechniquewillbeassessedandevaluatedinfuture experiments.
Demonstration Anonlineversionofthegraphmodelandtheincrementalclusteringalgorithmdescribedinthis paperarepubliclyavailable1fordemonstration purposesandtoallowuserstoobservethegeneralityofourtechniques.
Asampleoutputis includedinFigure6.
Acknowledgements Theauthorswouldliketothanktheanonymous reviewerswhosecommentswereagreathelpin makingthispapermorefocussed: anyshortcomingsremainentirelyourownresponsibility.
Thisresearchwassupportedinpartbythe ResearchCollaborationbetweentheNTTCommunicationScienceLaboratories,NipponTelegraph and Telephone Corporation and CSLI, StanfordUniversity,andbyEC/NSFgrantIST1999-11438fortheMUCHMOREproject.

