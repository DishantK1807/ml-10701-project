<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>R Drullman</author>
<author>J Festen</author>
<author>R Plomp</author>
</authors>
<title>Effect of reducing slow temporal modulations on speech reception</title>
<date>1994</date>
<contexts>
<context>rlier studies reported that the modulation frequency components from the range between 2 and 16 Hz, with dominant component at around 4 Hz , contain important linguistic information (Hermansky, 1996; Drullman et al., 1994; Kanedera et al., 1999). Dominant components represent strong rate of change of the vocal tract shape. This particular property, along with the other features has been used to discriminate speech and</context>
</contexts>
<marker>Drullman, Festen, Plomp, 1994</marker>
<rawString>R. Drullman, J. Festen, and R. Plomp. 1994. Effect of reducing slow temporal modulations on speech reception.</rawString>
</citation>
<citation valid="false">
<journal>Journal of the Acousic Society</journal>
<pages>95--2670</pages>
<marker></marker>
<rawString>Journal of the Acousic Society, 95:2670–2680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Grimm</author>
<author>K Kroschel</author>
<author>H Harris</author>
<author>C Nass</author>
<author>B Schuller</author>
<author>G Rigoll</author>
<author>T Mossmayr</author>
</authors>
<title>On the necessity and feasibility of detecting driver’s emotional state while driving</title>
<date>2007</date>
<booktitle>In Proceedings of Affective Computing and Intelligent Interaction</booktitle>
<pages>126--138</pages>
<contexts>
<context>ecordings and labeled each sentence using the previously described tool. In order to compare the evaluation results with previously published work the stress values were mapped to a scale of [+1;¡1] (Grimm et al., 2007). The average standard deviations in the human evaluator’s ratings of all the utterances is 0.32. After removing the unusable recordings, where for example no answer was given, 619 recordings remaine</context>
</contexts>
<marker>Grimm, Kroschel, Harris, Nass, Schuller, Rigoll, Mossmayr, 2007</marker>
<rawString>M. Grimm, K. Kroschel, H. Harris, C. Nass, B. Schuller, G. Rigoll, and T. Mossmayr. 2007. On the necessity and feasibility of detecting driver’s emotional state while driving. In Proceedings of Affective Computing and Intelligent Interaction, pages 126–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H L Hansen</author>
<author>S Bou-Ghazale</author>
</authors>
<title>Getting started with susas: a speech under simulated and actual stress database</title>
<date>1997</date>
<booktitle>In Proceedings of Eurospeech</booktitle>
<contexts>
<context>ch applications since data is difficult to obtain and the available datasets are mostly recordings made by the military. Furthermore, the vocabulary of the datasets is limited (Vlasenko et al., 2007; Hansen and Bou-Ghazale, 1997). The goal of this work is to obtain a large dataset comprising a large vocabulary at different stress levels. Therefore, an approach to record audio signals containing actual stress induced by an ai</context>
</contexts>
<marker>Hansen, Bou-Ghazale, 1997</marker>
<rawString>J. H. L. Hansen and S. Bou-Ghazale. 1997. Getting started with susas: a speech under simulated and actual stress database. In Proceedings of Eurospeech 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hermansky</author>
</authors>
<title>Auditory modeling in automatic recognition of speech</title>
<date>1996</date>
<booktitle>In Proceedings of Keele Workshop</booktitle>
<contexts>
<context>ditory system. Earlier studies reported that the modulation frequency components from the range between 2 and 16 Hz, with dominant component at around 4 Hz , contain important linguistic information (Hermansky, 1996; Drullman et al., 1994; Kanedera et al., 1999). Dominant components represent strong rate of change of the vocal tract shape. This particular property, along with the other features has been used to </context>
</contexts>
<marker>Hermansky, 1996</marker>
<rawString>H. Hermansky. 1996. Auditory modeling in automatic recognition of speech. In Proceedings of Keele Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hermansky</author>
</authors>
<title>The modulation spectrum in automatic recognition of speech</title>
<date>1997</date>
<booktitle>In Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding</booktitle>
<contexts>
<context> frames not more than several milliseconds, dominates speech processing for many years. However, these features are strongly influenced by environmental noise and renders them therefore unstable. In (Hermansky, 1997), it is suggested to use the so called modulation spectrum of speech to obtain information about the temporal dynamics of the speech signal to extract reliable cues for the linguistic context. Since </context>
</contexts>
<marker>Hermansky, 1997</marker>
<rawString>H. Hermansky. 1997. The modulation spectrum in automatic recognition of speech. In Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jaeger</author>
</authors>
<title>Tutorial on training recurrent neural networks, covering bppt, rtrl, ekf and the echo state network approach</title>
<date>2002</date>
<tech>Technical Report 159, FraunhoferGesellschaft</tech>
<location>St. Augustin Germany</location>
<contexts>
<context>perty. The echo state property says: “if the network has been run for a very long time, the current network state is uniquely determined by the history of the input and the (teacher-forced) output.” (Jaeger, 2002). According to experience, it is better to ensure that the internal weight matrix has maximum eingenvalue j‚maxj &lt; 1. Second, one attaches output neurons to the network and trains suitable output wei</context>
</contexts>
<marker>Jaeger, 2002</marker>
<rawString>H. Jaeger. 2002. Tutorial on training recurrent neural networks, covering bppt, rtrl, ekf and the echo state network approach. Technical Report 159, FraunhoferGesellschaft, St. Augustin Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kanedera</author>
<author>T Araib</author>
<author>H Hermansky</author>
<author>M Pavele</author>
</authors>
<title>On the relative importance of various components of the modulation spectrum for automatic speech recognition”. Speech Communications</title>
<date>1999</date>
<pages>28--43</pages>
<contexts>
<context>that the modulation frequency components from the range between 2 and 16 Hz, with dominant component at around 4 Hz , contain important linguistic information (Hermansky, 1996; Drullman et al., 1994; Kanedera et al., 1999). Dominant components represent strong rate of change of the vocal tract shape. This particular property, along with the other features has been used to discriminate speech and music (Scheirer and Sl</context>
</contexts>
<marker>Kanedera, Araib, Hermansky, Pavele, 1999</marker>
<rawString>N. Kanedera, T. Araib, H. Hermansky, and M. Pavele. 1999. On the relative importance of various components of the modulation spectrum for automatic speech recognition”. Speech Communications, 28:43–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H K Maganti</author>
<author>P Motlicek</author>
<author>D Gatica-Perez</author>
</authors>
<title>Unsupervised speech/non-speech detection for automatic speech recognition</title>
<date>2007</date>
<booktitle>In Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing</booktitle>
<contexts>
<context>.1.. An additional goal is the implementation of a stress recognizer that performs close to real-time. The recognition and evaluation parts of this proposed work are based on recently published work (Maganti et al., 2007b; Scherer et al., 2007; Scherer et al., 2008). For the recognition features extracted from the audio signal resembling the rate of change of frequency in time windows of 100 ms, called modulation spe</context>
<context>recordings. For the segmentation a novel and robust algorithm utilizing similar features as in the recognition process presented in Sect. 4.1. was applied to purge the recordings of pauses and noise (Maganti et al., 2007a). The subjects are then asked to assign a number between 0 (no stress) and 100 (extremely stressed) to each of the audio segments. To simplify the labeling process, a tool was implemented that helps</context>
<context>iment are described in Sect. 4.3.. 4. Automatic Stress Recognition The setup of the automatic stress recognition system will be very similar to previously published systems, and work to be published (Maganti et al., 2007b; Scherer et al., 2007; Scherer et al., 2008). Biologically motivated modulation spectrum features will be used as input for the artificial neural network. The utilized network architecture is a so c</context>
</contexts>
<marker>Maganti, Motlicek, Gatica-Perez, 2007</marker>
<rawString>H. K. Maganti, P. Motlicek, and D. Gatica-Perez. 2007a. Unsupervised speech/non-speech detection for automatic speech recognition. In Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H K Maganti</author>
<author>S Scherer</author>
<author>G Palm</author>
</authors>
<title>A novel feature for emotion recognition in voice based applications</title>
<date>2007</date>
<booktitle>In Proceedings of ACII</booktitle>
<pages>710--711</pages>
<contexts>
<context>.1.. An additional goal is the implementation of a stress recognizer that performs close to real-time. The recognition and evaluation parts of this proposed work are based on recently published work (Maganti et al., 2007b; Scherer et al., 2007; Scherer et al., 2008). For the recognition features extracted from the audio signal resembling the rate of change of frequency in time windows of 100 ms, called modulation spe</context>
<context>recordings. For the segmentation a novel and robust algorithm utilizing similar features as in the recognition process presented in Sect. 4.1. was applied to purge the recordings of pauses and noise (Maganti et al., 2007a). The subjects are then asked to assign a number between 0 (no stress) and 100 (extremely stressed) to each of the audio segments. To simplify the labeling process, a tool was implemented that helps</context>
<context>iment are described in Sect. 4.3.. 4. Automatic Stress Recognition The setup of the automatic stress recognition system will be very similar to previously published systems, and work to be published (Maganti et al., 2007b; Scherer et al., 2007; Scherer et al., 2008). Biologically motivated modulation spectrum features will be used as input for the artificial neural network. The utilized network architecture is a so c</context>
</contexts>
<marker>Maganti, Scherer, Palm, 2007</marker>
<rawString>H. K. Maganti, S. Scherer, and G. Palm. 2007b. A novel feature for emotion recognition in voice based applications. In Proceedings of ACII, pages 710–711.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Scheirer</author>
<author>M Slaney</author>
</authors>
<title>Construction and evaluation of a robust multifeature speech/music discriminator</title>
<date>1997</date>
<contexts>
<context>ra et al., 1999). Dominant components represent strong rate of change of the vocal tract shape. This particular property, along with the other features has been used to discriminate speech and music (Scheirer and Slaney, 1997). In this work, the proposed features are based on this specific characteristic of speech, to recognize the emotional state of the speaker. The block diagram for the feature extraction for a system t</context>
</contexts>
<marker>Scheirer, Slaney, 1997</marker>
<rawString>E. Scheirer and M. Slaney. 1997. Construction and evaluation of a robust multifeature speech/music discriminator.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of ICASSP</booktitle>
<volume>1</volume>
<pages>1331--1334</pages>
<marker></marker>
<rawString>In Proceedings of ICASSP, volume 1, pages 1331–1334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R Scherer</author>
<author>T Johnstone</author>
<author>G Klasmeyer</author>
</authors>
<title>Handbook of Affective Sciences Vocal expression of emotion, chapter 23</title>
<date>2003</date>
<pages>433--456</pages>
<publisher>Oxford University Press</publisher>
<contexts>
<context> the linguistic context. Since emotion in speech is often communicated by varying temporal dynamics in the signal the same features are used to classify emotional speech in the following experiments (Scherer et al., 2003). The proposed features are based on long-term modulation spectrum. In this work, the features based on slow temporal evolution of the speech are used to represent the emotional status of the speaker</context>
</contexts>
<marker>Scherer, Johnstone, Klasmeyer, 2003</marker>
<rawString>K. R. Scherer, T. Johnstone, and G. Klasmeyer, 2003. Handbook of Affective Sciences Vocal expression of emotion, chapter 23, pages 433–456. Affective Science. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Scherer</author>
<author>F Schwenker</author>
<author>G Palm</author>
</authors>
<title>Classifier fusion for emotion recognition from speech</title>
<date>2007</date>
<booktitle>In Proceedings of Intelligent Environments 07</booktitle>
<contexts>
<context> is the implementation of a stress recognizer that performs close to real-time. The recognition and evaluation parts of this proposed work are based on recently published work (Maganti et al., 2007b; Scherer et al., 2007; Scherer et al., 2008). For the recognition features extracted from the audio signal resembling the rate of change of frequency in time windows of 100 ms, called modulation spectrum features, will be</context>
<context>Sect. 4.3.. 4. Automatic Stress Recognition The setup of the automatic stress recognition system will be very similar to previously published systems, and work to be published (Maganti et al., 2007b; Scherer et al., 2007; Scherer et al., 2008). Biologically motivated modulation spectrum features will be used as input for the artificial neural network. The utilized network architecture is a so called echo state networ</context>
</contexts>
<marker>Scherer, Schwenker, Palm, 2007</marker>
<rawString>S. Scherer, F. Schwenker, and G. Palm. 2007. Classifier fusion for emotion recognition from speech. In Proceedings of Intelligent Environments 07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Scherer</author>
<author>M Oubbati</author>
<author>F Schwenker</author>
<author>G Palm</author>
</authors>
<title>Real-time emotion recognition from speech using echo state networks</title>
<date>2008</date>
<booktitle>In to be published in Proceedings of ANNPR</booktitle>
<contexts>
<context> of a stress recognizer that performs close to real-time. The recognition and evaluation parts of this proposed work are based on recently published work (Maganti et al., 2007b; Scherer et al., 2007; Scherer et al., 2008). For the recognition features extracted from the audio signal resembling the rate of change of frequency in time windows of 100 ms, called modulation spectrum features, will be used as input to a no</context>
<context>ic Stress Recognition The setup of the automatic stress recognition system will be very similar to previously published systems, and work to be published (Maganti et al., 2007b; Scherer et al., 2007; Scherer et al., 2008). Biologically motivated modulation spectrum features will be used as input for the artificial neural network. The utilized network architecture is a so called echo state network (ESN). ESNs utilizin</context>
<context>e network is robust towards noisy real world conditions as well as the features. The goal of this work is to recognize stress from the speech signal close to real-time, as in recently published work (Scherer et al., 2008), which is of great advantage in time sensitive applications. Additionally, it would be interesting to see the performance of the ESN under noisy conditions, such as in a car or office, by adding noi</context>
</contexts>
<marker>Scherer, Oubbati, Schwenker, Palm, 2008</marker>
<rawString>S. Scherer, M. Oubbati, F. Schwenker, and G. Palm. 2008. Real-time emotion recognition from speech using echo state networks. In to be published in Proceedings of ANNPR 2008.</rawString>
</citation>
</citationList>
</algorithm>

