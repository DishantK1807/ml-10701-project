<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>A Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization at the 43rd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<location>Ann Arbor, MI</location>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Banerjee, S., and A. Lavie (2005). METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization at the 43rd Annual Meeting of the Association for Computational Linguistics (ACL). Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales</title>
<date>1960</date>
<journal>Educational and Psychological Measurement</journal>
<volume>10</volume>
<pages>37--46</pages>
<contexts>
<context>n the January 2007 evaluation on which the judges had a high level of agreement. We used that set of exemplars to train the July 2007 judges. Despite this, for the July 2007 evaluation Cohen’s kappa (Cohen, 1960) for pairs of Arabic judges on the four possible semantic adequacy judgments ranged from 0.178 to 0.435 (median 0.294), which is quite low. If, however, one relaxes the criteria for a match so as to </context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, (10), pp. 37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Condon</author>
<author>J Phillips</author>
<author>C Doran</author>
<author>J Aberdeen</author>
<author>D Parvaz</author>
<author>B Oshika</author>
<author>G Sanders</author>
<author>C Schlenoff</author>
</authors>
<date>2008</date>
<contexts>
<context>ni et al., 2001), METEOR (Banerjee and Lavie, 2004; Lavie et al., 2005), and TER (Snover et al., 2005), with both reference and hypothesis texts normalized much like they were normalized for ASR (see Condon, et al., 2008). We calculated BLEU, using the IBM script that had been used for the DARPA Babylon project. We calculated Translation Edit Rate (TER) scores using TerCom, version 6b, developed by Matt Snover in col</context>
</contexts>
<marker>Condon, Phillips, Doran, Aberdeen, Parvaz, Oshika, Sanders, Schlenoff, 2008</marker>
<rawString>Condon, S., J. Phillips, C. Doran, J. Aberdeen, D. Parvaz, B. Oshika, G. Sanders, and C. Schlenoff (2008).</rawString>
</citation>
<citation valid="true">
<title>Applying Automated Metrics to Speech Translation Dialogs</title>
<booktitle>In Proceedings of LREC-2008</booktitle>
<marker></marker>
<rawString>Applying Automated Metrics to Speech Translation Dialogs. In Proceedings of LREC-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Frederking</author>
<author>S Nirenburg</author>
</authors>
<title>Three Heads are Better Than One</title>
<date>1994</date>
<booktitle>In Proceedings of the Fourth Conference on Applied Natural Language Processing (ANLP</booktitle>
<pages>95--100</pages>
<contexts>
<context>http://www.cs.cmu.edu/~alavie/METEOR/ There is, in fact, a long history of various automated metrics, as well as various human-mediated metrics for the evaluation of machine translation (for example, Frederking and Nirenburg, 1994; Knight and Chander, 1994; King 1996; Hogan and Frederking, 1998; Niessen et al., 2000; Frederking, et al., 2002; Melamed, Green, and Turian, 2003; Lita, Rogati, and Lavie, 2005; Russo-Lassner, Lin, </context>
</contexts>
<marker>Frederking, Nirenburg, 1994</marker>
<rawString>Frederking, R. and S. Nirenburg (1994). Three Heads are Better Than One. In Proceedings of the Fourth Conference on Applied Natural Language Processing (ANLP), pp. 95–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Frederking</author>
<author>A W Black</author>
<author>R D Brown</author>
<author>J Moody</author>
<author>E Steinbrecher</author>
</authors>
<title>Field Testing the Tongues Speech-to-Speech Machine Translation System</title>
<date>2002</date>
<booktitle>In Proceedings of LREC-2002 (Las Palmas, Gran Canaria, Canary Islands</booktitle>
<pages>160--164</pages>
<location>Spain</location>
<contexts>
<context>s human-mediated metrics for the evaluation of machine translation (for example, Frederking and Nirenburg, 1994; Knight and Chander, 1994; King 1996; Hogan and Frederking, 1998; Niessen et al., 2000; Frederking, et al., 2002; Melamed, Green, and Turian, 2003; Lita, Rogati, and Lavie, 2005; Russo-Lassner, Lin, and Resnik, 2005; Pozar and Charniak, 2006). 4. Discussion of the Results Between January 2007 and July 2007, the</context>
</contexts>
<marker>Frederking, Black, Brown, Moody, Steinbrecher, 2002</marker>
<rawString>Frederking, R., A. W. Black, R. D. Brown, J. Moody, and E. Steinbrecher (2002). Field Testing the Tongues Speech-to-Speech Machine Translation System. In Proceedings of LREC-2002 (Las Palmas, Gran Canaria, Canary Islands, Spain), pp. 160–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hogan</author>
<author>R E Frederking</author>
</authors>
<title>An Evaluation of the Multi-Engine MT Architecture</title>
<date>1998</date>
<booktitle>In Proceedings of AMTA-1998</booktitle>
<pages>113--124</pages>
<contexts>
<context>ry of various automated metrics, as well as various human-mediated metrics for the evaluation of machine translation (for example, Frederking and Nirenburg, 1994; Knight and Chander, 1994; King 1996; Hogan and Frederking, 1998; Niessen et al., 2000; Frederking, et al., 2002; Melamed, Green, and Turian, 2003; Lita, Rogati, and Lavie, 2005; Russo-Lassner, Lin, and Resnik, 2005; Pozar and Charniak, 2006). 4. Discussion of the</context>
</contexts>
<marker>Hogan, Frederking, 1998</marker>
<rawString>Hogan, C., and R. E. Frederking (1998). An Evaluation of the Multi-Engine MT Architecture. In Proceedings of AMTA-1998, pp. 113–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M King</author>
</authors>
<title>Evaluating Natural Language Processing Systems</title>
<date>1996</date>
<journal>In Communications of the ACM</journal>
<volume>39</volume>
<pages>73--79</pages>
<contexts>
<context> long history of various automated metrics, as well as various human-mediated metrics for the evaluation of machine translation (for example, Frederking and Nirenburg, 1994; Knight and Chander, 1994; King 1996; Hogan and Frederking, 1998; Niessen et al., 2000; Frederking, et al., 2002; Melamed, Green, and Turian, 2003; Lita, Rogati, and Lavie, 2005; Russo-Lassner, Lin, and Resnik, 2005; Pozar and Charniak,</context>
</contexts>
<marker>King, 1996</marker>
<rawString>King, M. (1996). Evaluating Natural Language Processing Systems. In Communications of the ACM (39)1, pp. 73–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>I Chander</author>
</authors>
<title>Automated Postediting of Documents</title>
<date>1994</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence (AAAI</booktitle>
<contexts>
<context>TEOR/ There is, in fact, a long history of various automated metrics, as well as various human-mediated metrics for the evaluation of machine translation (for example, Frederking and Nirenburg, 1994; Knight and Chander, 1994; King 1996; Hogan and Frederking, 1998; Niessen et al., 2000; Frederking, et al., 2002; Melamed, Green, and Turian, 2003; Lita, Rogati, and Lavie, 2005; Russo-Lassner, Lin, and Resnik, 2005; Pozar an</context>
</contexts>
<marker>Knight, Chander, 1994</marker>
<rawString>Knight, K. and I. Chander (1994). Automated Postediting of Documents. In Proceedings of the National Conference on Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>K Sagae</author>
<author>S Jayaraman</author>
</authors>
<title>The Significance of Recall in Automatic Metrics for MT Evaluation</title>
<date>2004</date>
<booktitle>In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas (AMTA</booktitle>
<location>Washington, DC</location>
<marker>Lavie, Sagae, Jayaraman, 2004</marker>
<rawString>Lavie, A., K. Sagae, and S. Jayaraman (2004). The Significance of Recall in Automatic Metrics for MT Evaluation. In Proceedings of the 6th Conference of the Association for Machine Translation in the Americas (AMTA). Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L V Lita</author>
<author>M Rogati</author>
<author>A Lavie</author>
</authors>
<title>BLANC: Learning Evaluation Metrics for MT</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP</booktitle>
<pages>740--747</pages>
<location>Vancouver, Canada</location>
<contexts>
<context>lation (for example, Frederking and Nirenburg, 1994; Knight and Chander, 1994; King 1996; Hogan and Frederking, 1998; Niessen et al., 2000; Frederking, et al., 2002; Melamed, Green, and Turian, 2003; Lita, Rogati, and Lavie, 2005; Russo-Lassner, Lin, and Resnik, 2005; Pozar and Charniak, 2006). 4. Discussion of the Results Between January 2007 and July 2007, the TRANSTAC developers made substantial performance improvements at</context>
</contexts>
<marker>Lita, Rogati, Lavie, 2005</marker>
<rawString>Lita, L. V., M. Rogati, and A. Lavie (2005). BLANC: Learning Evaluation Metrics for MT. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (HLT/EMNLP). (Vancouver, Canada, October 2005). pp. 740–747.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
<author>R Green</author>
<author>J P Turian</author>
</authors>
<title>Precision and Recall of Machine Translation</title>
<date>2003</date>
<contexts>
<context>or the evaluation of machine translation (for example, Frederking and Nirenburg, 1994; Knight and Chander, 1994; King 1996; Hogan and Frederking, 1998; Niessen et al., 2000; Frederking, et al., 2002; Melamed, Green, and Turian, 2003; Lita, Rogati, and Lavie, 2005; Russo-Lassner, Lin, and Resnik, 2005; Pozar and Charniak, 2006). 4. Discussion of the Results Between January 2007 and July 2007, the TRANSTAC developers made substant</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>Melamed, I. D., R. Green, and J. P. Turian (2003). Precision and Recall of Machine Translation.</rawString>
</citation>
<citation valid="false">
<booktitle>Proceedings of HLT/NAACL 2003</booktitle>
<pages>61--63</pages>
<location>Edmonton, Alberta, Canada</location>
<marker></marker>
<rawString>Proceedings of HLT/NAACL 2003 (Edmonton, Alberta, Canada), pp. 61–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation. (IBM research report available at http://domino.watson.ibm.com/library/cybergig.nsf</title>
<date>2001</date>
<contexts>
<context>on. MITRE normalized the non-English texts similarly, making use of their in-house expertise in those languages. For machine translation, we calculated three commonly used automated MT metrics: BLEU (Papineni et al., 2001), METEOR (Banerjee and Lavie, 2004; Lavie et al., 2005), and TER (Snover et al., 2005), with both reference and hypothesis texts normalized much like they were normalized for ASR (see Condon, et al.,</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Papineni, K., S. Roukos, T. Ward, and W. Zhu (2001). BLEU: A Method for Automatic Evaluation of Machine Translation. (IBM research report available at http://domino.watson.ibm.com/library/cybergig.nsf/).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Polzar</author>
<author>E Charniak</author>
</authors>
<title>Bllip: An Improved Evaluation Metric for Machine Translation. Brown University, Providence, Rhode Island. (Available as: http://www.cs.brown.edu/research/pubs/theses/masters /2006/mpozar.pdf) Russo-Lassner</title>
<date>2006</date>
<institution>University of Maryland</institution>
<location>College Park, MD</location>
<marker>Polzar, Charniak, 2006</marker>
<rawString>Polzar, M., and E. Charniak (2006). Bllip: An Improved Evaluation Metric for Machine Translation. Brown University, Providence, Rhode Island. (Available as: http://www.cs.brown.edu/research/pubs/theses/masters /2006/mpozar.pdf) Russo-Lassner, G., J. Lin, and P. Resnik (2005). A Paraphrase-based Approach to Machine Translation Evaluation. LAMP-TR-125 CS-TR-4754 UMIACS-TR-2005-57, University of Maryland, College Park, MD, August, 2005.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>J Makhoul</author>
<author>L</author>
</authors>
<marker>Snover, Dorr, Schwartz, Makhoul, L, </marker>
<rawString>Snover M., B. Dorr, R. Schwartz, J. Makhoul, L.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micciulla</author>
<author>R Weischedel</author>
</authors>
<title>A Study of Translation Error Rate with Targeted Human Annotation</title>
<date>2005</date>
<volume>126</volume>
<pages>2005--58</pages>
<institution>University of Maryland</institution>
<location>College Park, MD</location>
<marker>Micciulla, Weischedel, 2005</marker>
<rawString>Micciulla, and R. Weischedel (2005). A Study of Translation Error Rate with Targeted Human Annotation. LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-58, University of Maryland, College Park, MD, July, 2005 Weiss, B., C. Schlenoff, G. Sanders, M. P. Steves, S.</rawString>
</citation>
</citationList>
</algorithm>

