We use two measures, the first one (%Words) was proposed by Lin (1995) and was the one reported in (Eisner, 1996).
C96-1058
We generate our training data from the Wall Street Journal Section of the Penn Tree Bank (PTB), by transforming it to projective dependency structures, following (Collins, 1996), and extracting rules from the result.
P96-1025
All sentences containing CC tags are filtered out, following (Eisner, 1996).
C96-1058
N-grams have been used extensively for this purpose (Collins 1996, 1997; Eisner, 1996).
C96-1058 P96-1025 P97-1003
They are central to many parsing models (Charniak, 1997; Collins, 1997, 2000; Eisner, 1996), and despite their simplicity n-gram models have been very successful.
C96-1058 P96-1025 P97-1003
As a side product, we find empirical evidence to suggest that the effectiveness of rule lexicalization techniques (Collins, 1997; Sima’an, 2000) and parent annotation techniques (Klein and Manning, 2003) is due to the fact that both lead to a reduction in perplexity in the automata induced from training corpora.
P03-1054 P97-1003
In particular, Krotov et al.(1998) try to induce a CW-grammar from the PTB with the underlying assumption that some derivations that were supposed to be hidden were left visible.
P98-1115
A different approach (Klein and Manning, 2003) consists in adding head information to dependents; words tagged with VBP are then split into classes according to the words that dominate them in the training corpus.
P03-1054
Let GwL and GwR be the PCFGs equivalent to AwL and AwR, respectively, following (Abney et al., 1999), and let SwL and SwR be the starting symbols of GwL and GwR, respectively.
P99-1070
One attempt to implement this idea is lexicalization: increasing the information in the POS tag by adding the lemma to it (Collins, 1997; Sima’an, 2000).
P97-1003
We first transform the PTB into projective dependencies structures following (Collins, 1996).
P96-1025
