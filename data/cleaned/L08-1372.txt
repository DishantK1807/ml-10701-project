<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Herbert Clark</author>
</authors>
<title>Using Language</title>
<date>1996</date>
<publisher>Cambridge University Press</publisher>
<contexts>
<context>wo user ratings on a 5-point Likert scale. 1. The task was easy to solve. 2. I had no problems finding the information I wanted. We choose Task Ease as the ultimate measure to be optimised following (Clark, 1996)’s principle of the least effort which says: “All things being equal, agents try to minimize their effort in doing what they intend to do”. The PARADISE regression model is constructed from 3 differe</context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>Herbert Clark. 1996. Using Language. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus-Peter Engelbrecht</author>
<author>Sebastian M¨oller</author>
</authors>
<title>Pragmatic usage of linear regression models for the predictions of user judegments</title>
<date>2007</date>
<booktitle>In Proc. of SIGdial. James</booktitle>
<marker>Engelbrecht, M¨oller, 2007</marker>
<rawString>Klaus-Peter Engelbrecht and Sebastian M¨oller. 2007. Pragmatic usage of linear regression models for the predictions of user judegments. In Proc. of SIGdial. James Henderson, Oliver Lemon, and Kallirroi Georgila.</rawString>
</citation>
<citation valid="true">
<title>Hybrid reinforcement / supervised learning of dialogue policies from fixed datasets</title>
<date>2008</date>
<journal>Computational Linguistics</journal>
<note>to appear</note>
<marker>2008</marker>
<rawString>2008. Hybrid reinforcement / supervised learning of dialogue policies from fixed datasets. Computational Linguistics (to appear).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jiang Hu</author>
</authors>
<location>Andi Winterboer, Clifford Nass, Johanna D</location>
<marker>Hu, </marker>
<rawString>Jiang Hu, Andi Winterboer, Clifford Nass, Johanna D.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moore</author>
<author>Rebecca Illowsky</author>
</authors>
<title>Context &amp; usability testing: user-modeled information presentation in easy and difficult driving condition</title>
<date>2007</date>
<booktitle>In Proc. CHI</booktitle>
<pages>1343--1346</pages>
<marker>Moore, Illowsky, 2007</marker>
<rawString>Moore, and Rebecca Illowsky. 2007. Context &amp; usability testing: user-modeled information presentation in easy and difficult driving condition. In Proc. CHI, pages 1343–1346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck Jones</author>
<author>Julia Galliers</author>
</authors>
<title>Evaluating Natural Language Processing Systems: An Analysis and Review</title>
<date>1996</date>
<publisher>Springer Verlag</publisher>
<contexts>
<context>-anderror (Paek, 2007). In addition, user preferences are highly context dependent (Hu et al., 2007). This is why dialogue strategy design is often referred to as being more of an art than a science (Jones and Galliers, 1996; Pieraccini, 2002) Over recent years, data-driven statistical optimisation methods (e.g. Reinforcement Learning (RL)) for dialogue strategy design have become more and more popular (Lemon and Pietqui</context>
</contexts>
<marker>Jones, Galliers, 1996</marker>
<rawString>Karen Sparck Jones and Julia Galliers. 1996. Evaluating Natural Language Processing Systems: An Analysis and Review. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Lemon</author>
<author>Olivier Pietquin</author>
</authors>
<title>Machine learning for spoken dialogue systems</title>
<date>2007</date>
<booktitle>In Proc. ofInterspeech</booktitle>
<contexts>
<context>nd Galliers, 1996; Pieraccini, 2002) Over recent years, data-driven statistical optimisation methods (e.g. Reinforcement Learning (RL)) for dialogue strategy design have become more and more popular (Lemon and Pietquin, 2007). One major advantage of RL-based dialogue strategy development is that the dialogue strategy can be automatically trained and evaluated using the same objective function (Walker, 2005). In the conte</context>
</contexts>
<marker>Lemon, Pietquin, 2007</marker>
<rawString>Oliver Lemon and Olivier Pietquin. 2007. Machine learning for spoken dialogue systems. In Proc. ofInterspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Paek</author>
</authors>
<title>Reinforcement learning for spoken dialogue systems: Comparing strengths and weaknesses for practical deployment</title>
<date>2006</date>
<booktitle>In Proc. Dialog-on-Dialog Workshop, Interspeech</booktitle>
<contexts>
<context>rto, 1998). Despite its central aspect for RL, quality assurance for objective functions has received little attention so far. In fact, the reward function is one of the most handcoded aspects in RL (Paek, 2006). In this paper we propose a new method for meta-evaluation of the objective function. We bring together two strands of research: one strand uses Reinforcement Learning to automatically optimise dial</context>
</contexts>
<marker>Paek, 2006</marker>
<rawString>Tim Paek. 2006. Reinforcement learning for spoken dialogue systems: Comparing strengths and weaknesses for practical deployment. In Proc. Dialog-on-Dialog Workshop, Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Paek</author>
</authors>
<title>Toward evaluation that leads to best practices: Reconciling dialogue evaluation in research and industry</title>
<date>2007</date>
<booktitle>In Proc. of the NAACL-HLT Workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technologies</booktitle>
<contexts>
<context>rance for dialogue strategies is a non-trivial problem. In conventional dialogue design the dialogue often is designed following ‘best practises’ which are often obscure and emerge by trial-anderror (Paek, 2007). In addition, user preferences are highly context dependent (Hu et al., 2007). This is why dialogue strategy design is often referred to as being more of an art than a science (Jones and Galliers, 1</context>
<context>he other other focuses on automatic evaluation of dialogue strategies, e.g. the PARADISE framework (Walker et al., 1997), and meta-evaluation of dialogue metrics, e.g. (Engelbrecht and M¨oller, 2007; Paek, 2007). Clearly, automatic optimisation and evaluation of dialogue policies, as well as quality control of the objective function, are closely inter-related problems: how can we make sure that we optimise </context>
<context>e models obtained with PARADISE usually fit the data poorly (Engelbrecht and M¨oller, 2007). It is also not clear how general they are across different systems and user groups (Walker et al., 2000), (Paek, 2007). Furthermore, it is not clear how they perform when being used for automatic strategy optimisation within the RL framework. In the following we evaluate different aspects of an objective function ob</context>
</contexts>
<marker>Paek, 2007</marker>
<rawString>Tim Paek. 2007. Toward evaluation that leads to best practices: Reconciling dialogue evaluation in research and industry. In Proc. of the NAACL-HLT Workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Pieraccini</author>
</authors>
<title>The art and science of spoken dialog systems. Invited talk, The Center for Language and Speech Processing</title>
<date>2002</date>
<institution>Johns Hopkins University</institution>
<contexts>
<context> addition, user preferences are highly context dependent (Hu et al., 2007). This is why dialogue strategy design is often referred to as being more of an art than a science (Jones and Galliers, 1996; Pieraccini, 2002) Over recent years, data-driven statistical optimisation methods (e.g. Reinforcement Learning (RL)) for dialogue strategy design have become more and more popular (Lemon and Pietquin, 2007). One majo</context>
</contexts>
<marker>Pieraccini, 2002</marker>
<rawString>Roberto Pieraccini. 2002. The art and science of spoken dialog systems. Invited talk, The Center for Language and Speech Processing, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
</authors>
<title>Does this list contain what you were searching for? Learning adaptive dialogue strategies for interactive question answering. Natural Language Engineering</title>
<date>2008</date>
<note>to appear</note>
<contexts>
<context>ective function. We bring together two strands of research: one strand uses Reinforcement Learning to automatically optimise dialogue strategies, e.g. (Singh et al., 2002), (Henderson et al., 2008), (Rieser and Lemon, 2008a; Rieser and Lemon, 2008b); the other other focuses on automatic evaluation of dialogue strategies, e.g. the PARADISE framework (Walker et al., 1997), and meta-evaluation of dialogue metrics, e.g. (E</context>
<context>hey perform when being used for automatic strategy optimisation within the RL framework. In the following we evaluate different aspects of an objective function obtained from Wizard-of-Oz (WOZ) data (Rieser and Lemon, 2008b). We proceed as follows: The next Section shortly summarises the overall dialogue system design. In Section 3. we test the model stability in a test-retest comparison across different user populatio</context>
<context>ctually means depends on the dialogue context and the preferences of our users as reflected in the objective function. We therefore formulate dialogue learning as a hierarchical optimisation problem (Rieser and Lemon, 2008b). The applied objective function follows this structure as well. 2 66 66 66 66 66 66 66 64 info. acquisition: 2 66 66 66 66 66 66 4 User: I am searching for a song by Radiohead. System: Searching fo</context>
<context>77 5 3 77 77 77 77 77 77 77 75 Figure 1: Hierarchical dialogue structure for information seeking multimodal systems. 2.2. Method In the following the overall method is shortly summarised. Please see (Rieser and Lemon, 2008b; Rieser, 2008) for details. 1. We obtain an objective function from the WOZ data of (Rieser et al., 2005) according to the PARADISE framework. In PARADISE multivariate linear regression is applied t</context>
<context>in doing what they intend to do”. The PARADISE regression model is constructed from 3 different corpora: the SAMMIE WOZ experiment (Rieser et al., 2005), and the iTalk system used for the user tests (Rieser and Lemon, 2008b) running the supervised baseline policy and the RL-based policy. By replicating the regression model on different data sets we test whether the automatic estimate of Task Ease generalises beyond the</context>
</contexts>
<marker>Rieser, Lemon, 2008</marker>
<rawString>Verena Rieser and Oliver Lemon. 2008a. Does this list contain what you were searching for? Learning adaptive dialogue strategies for interactive question answering. Natural Language Engineering (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
</authors>
<title>Learning effective multimodal dialogue strategies from wizard-of-oz data: Bootstrapping and evaluation</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL/HLT-08</booktitle>
<contexts>
<context>alises well. 5. Error Analysis In previous work we showed that the RL-based policy significantly outperforms the supervised policy in terms of improved user ratings and dialogue performance measures (Rieser and Lemon, 2008b). Here, we test the relationship between improved user ratings and dialogue behaviour, i.e. we investigate which factors lead the users to give higher scores, and whether this was correctly reflecte</context>
<context>ned reward by displaying no more than 15 items. The SL policy, in contrast, did not learn an upper boundary for when to show items on the screen (since the wizards did not follow a specific pattern, (Rieser and Lemon, 2008b)). When relating number of items to user scores, the RL policy produces a linear (slightly declining) line between 7 and 6 (Table 3, bottom right), indicating that the applied policy reflected the u</context>
</contexts>
<marker>Rieser, Lemon, 2008</marker>
<rawString>Verena Rieser and Oliver Lemon. 2008b. Learning effective multimodal dialogue strategies from wizard-of-oz data: Bootstrapping and evaluation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL/HLT-08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Ivana Kruijff-Korbayov´a</author>
<author>Oliver Lemon</author>
</authors>
<title>A corpus collection and annotation framework for learning multimodal clarification strategies</title>
<date>2005</date>
<booktitle>In Proc. SIGdial</booktitle>
<marker>Rieser, Kruijff-Korbayov´a, Lemon, 2005</marker>
<rawString>Verena Rieser, Ivana Kruijff-Korbayov´a, and Oliver Lemon. 2005. A corpus collection and annotation framework for learning multimodal clarification strategies. In Proc. SIGdial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
</authors>
<title>Bootstrapping Reinforcement Learning-based Dialogue Strategies from Wizard-of-Oz data (to appear</title>
<date>2008</date>
<tech>Ph.D. thesis</tech>
<institution>International Research Training Group Language Technology and Cognitive Systems, Saarland University</institution>
<contexts>
<context>77 75 Figure 1: Hierarchical dialogue structure for information seeking multimodal systems. 2.2. Method In the following the overall method is shortly summarised. Please see (Rieser and Lemon, 2008b; Rieser, 2008) for details. 1. We obtain an objective function from the WOZ data of (Rieser et al., 2005) according to the PARADISE framework. In PARADISE multivariate linear regression is applied to experimental </context>
</contexts>
<marker>Rieser, 2008</marker>
<rawString>Verena Rieser. 2008. Bootstrapping Reinforcement Learning-based Dialogue Strategies from Wizard-of-Oz data (to appear). Ph.D. thesis, International Research Training Group Language Technology and Cognitive Systems, Saarland University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satinder Singh</author>
<author>Diane Litman</author>
<author>Micheal Kearns</author>
<author>Marilyn Walker</author>
</authors>
<title>Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system</title>
<date>2002</date>
<journal>JAIR</journal>
<volume>16</volume>
<contexts>
<context>pose a new method for meta-evaluation of the objective function. We bring together two strands of research: one strand uses Reinforcement Learning to automatically optimise dialogue strategies, e.g. (Singh et al., 2002), (Henderson et al., 2008), (Rieser and Lemon, 2008a; Rieser and Lemon, 2008b); the other other focuses on automatic evaluation of dialogue strategies, e.g. the PARADISE framework (Walker et al., 199</context>
</contexts>
<marker>Singh, Litman, Kearns, Walker, 2002</marker>
<rawString>Satinder Singh, Diane Litman, Micheal Kearns, and Marilyn Walker. 2002. Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system. JAIR, 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sutton</author>
<author>Andrew Barto</author>
</authors>
<title>Reinforcement Learning</title>
<date>1998</date>
<publisher>MIT Press</publisher>
<contexts>
<context>ent is that the dialogue strategy can be automatically trained and evaluated using the same objective function (Walker, 2005). In the context of RL the objective function is also called the “reward” (Sutton and Barto, 1998). Despite its central aspect for RL, quality assurance for objective functions has received little attention so far. In fact, the reward function is one of the most handcoded aspects in RL (Paek, 200</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Richard Sutton and Andrew Barto. 1998. Reinforcement Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Diane Litman</author>
<author>Candance Kamm</author>
<author>Alicia Abella</author>
</authors>
<title>PARADISE: a general framework for evaluating spoken dialogue agents</title>
<date>1997</date>
<booktitle>In ACL/EACL. Marilyn Walker, Jeanne Fromer, and Shrikanth Narayanan</booktitle>
<contexts>
<context>Singh et al., 2002), (Henderson et al., 2008), (Rieser and Lemon, 2008a; Rieser and Lemon, 2008b); the other other focuses on automatic evaluation of dialogue strategies, e.g. the PARADISE framework (Walker et al., 1997), and meta-evaluation of dialogue metrics, e.g. (Engelbrecht and M¨oller, 2007; Paek, 2007). Clearly, automatic optimisation and evaluation of dialogue policies, as well as quality control of the obj</context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1997</marker>
<rawString>Marilyn Walker, Diane Litman, Candance Kamm, and Alicia Abella. 1997. PARADISE: a general framework for evaluating spoken dialogue agents. In ACL/EACL. Marilyn Walker, Jeanne Fromer, and Shrikanth Narayanan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Candance Kamm Walker</author>
<author>Diane Litman</author>
</authors>
<title>Learning optimal dialogue strategies: A case study of a spoken dialogue agent for email</title>
<date>1998</date>
<booktitle>In Proceedings of ACL/COLING. Marilyn</booktitle>
<marker>Walker, Litman, 1998</marker>
<rawString>1998. Learning optimal dialogue strategies: A case study of a spoken dialogue agent for email. In Proceedings of ACL/COLING. Marilyn Walker, Candance Kamm, and Diane Litman.</rawString>
</citation>
<citation valid="true">
<title>Towards developing general models of usability with PARADISE</title>
<date>2000</date>
<journal>Natural Language Engineering</journal>
<volume>6</volume>
<marker>2000</marker>
<rawString>2000. Towards developing general models of usability with PARADISE. Natural Language Engineering, 6(3).</rawString>
</citation>
</citationList>
</algorithm>

