Although (Mihalcea and Strapparava, 2005) obtained much higher accuracies using lexical features alone, it might be due to the fact that our data is homogeneous in the sense that both humorous and non-humorous turns are extracted from the same source, and involve same speakers, which makes the two groups highly alike and hence challenging to distinguish.
H05-1067
Although this indirectly accounts for alliterations, in the future studies, we plan to use more stylistic lexical features like (Mihalcea and Strapparava, 2005).
H05-1067
Computational approaches to humor recognition so far primarily rely on lexical and stylistic cues such as alliteration, antonyms, adult slang (Mihalcea and Strapparava, 2005).
H05-1067
Nonetheless, recent studies have shown a feasibility of automatically recognizing (Mihalcea and Strapparava, 2005) (Taylor and Mazlack, 2004) and generating (Binsted and Ritchie, 1997) (Stock and Strapparava, 2005) humor in computer systems.
H05-1067 P05-3029
Speaker #Turns #Humor Male 685 347 (50.5% of Main) (50.6% of Male) Female 672 268 (49.5% Of Main) (39.9% of Female) Total 1357 615 Main (83.3% of Total) (86.1% of Humor) Table 2: Gender Distribution for Main Actors 210 5 Features Literature in emotional speech analysis (Liscombe et al., 2003)(Litman and Forbes-Riley, 2004) (Scherer, 2003)(Ang et al., 2002) has shown that prosodic features such as pitch, energy, speaking rate (tempo) are useful indicators of emotional states, such as joy, anger, fear, boredom etc.
P04-1045
There has been a considerable amount of research on incorporating affect (Litman and Forbes-Riley, 2004) (Alm et al., 2005) (Dâ€™Mello et al., 2005) (Shroder and Cowie, 2005) (Klein et al., 2002) and personality (Gebhard et al., 2004) in computer interfaces, so that, for instance, user frustrations can be recognized and addressed in a graceful manner.
H05-1073 P04-1045
