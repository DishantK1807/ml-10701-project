Book Review
Learning Machine Translation
Cyril Goutte
†, Nicola Cancedda
∗, Marc Dymetman
∗, and George Foster
†
(editors)
(
†
InstituteforInformationTechnology,NationalResearchCouncilCanada;
∗
Xerox
ResearchCentreEurope)
Cambridge,MA:TheMITPress,2009,xii+316pp;hardbound,ISBN978-0-262-07297-7,
$45.00,£29.95
Reviewedby
Phil Blunsom
The Universityof Edinburgh
Attending recent computational linguistics conferences, it is hard to ignore the phe-
nomenal amount of research devoted to statistical machine translation (SMT). Driven
by the wide availability of open-source translation systems, corpora, and evaluation
tools, a research area that was once the preserve of large research groups has become
accessibletothoseofmoremodestresources.Althoughthecurrentstate-of-the-artSMT
systemshavematuredintorobustcommercialsystems,capableofprovidingreasonable
quality translations for a variety of domains, they remain limited by naive modeling
assumptions and a heavy reliance on heuristics. These limitations have led researchers
to ask the question of whether the adoption of techniques from the machine learning
literature could allow more complex translations to be modeled effectively. As such,
thisbook,focusedontheapplicationofmachinelearningtoSMT,isparticularlytimely
incapturingthecurrentinterestofthemachinetranslationcommunity.
Learning Machine Translation is presented in two parts. The ﬁrst, titled “Enabling
Technologies,” focuses on research peripheral to machine translation. Topics covered
includetheacquisitionofparallelcorpora,cross-languagenamed-entityprocessing,and
language modeling. The second part covers core machine translation system building,
presenting a number of approaches applying discriminative machine learning tech-
niqueswithinaSMTdecoder.
MuchofthecontentofthebookarosefromtheMachineLearningforMultilingual
AccessWorkshopheldattheNeuralInformationProcessingconferencein2006.AsSMT
isnotafrequenttopicatthatconference,thebridgingofresearchfromthemainstream
machine learning community with research on MT is particularly promising. A ﬁne
exampleofthiscross-overisChapter9,“Kernel-BasedMachineTranslation,”inwhich
anovelapproachtoestimatingtranslationmodelsispresented.However,thispromise
isnotentirelyfulﬁlled,assomecontributionseitherfailtomakeuseofmachinelearning
oraresomewhatobscure,unlikelytoimpactonthemainstreamSMTcommunity.
1. Chapter 1: A Statistical Machine Translation Primer
In the ﬁrst chapter, “A Statistical Machine Translation Primer,” the editors seek to
both introduce the concept of the book as well as give a brief tutorial on current SMT
techniques.Intheseaimstheysucceed,describingtheelementsofcurrentapproachesto
SMTsuccinctly.Althoughthoseforeigntotheﬁeldwouldnotcomeawayfromreading
this chapter able to implement a translation model, pointers to research publications
ComputationalLinguistics Volume35,Number4
thatcontainthatlevelofdetailareprovidedandtheauthorsavoidhighlightingobscure
researchthatmaymislead.
The introduction also motivates machine translation as an instance of learning
withstructuredoutputs,anactiveareaofresearchinthemachinelearningcommunity.
However, I can’t help but feel an opportunity was missed to lay out a clear agenda for
research seeking to leverage machine learning techniques in SMT. The issue is not that
machine learning can be applied to SMT, but why we would want to do so. Here it is
necessarytoidentifyproblemswiththecurrentapproachwhichcouldbeaddressedby
a more rigorous statistical treatment: in particular, the lack of structural conditioning
andtheoretical analysis. Conversely itwouldseem prudent tohighlight theproperties
ofthecurrentapproachthathaveledtoitssuccess:theabilitytoscaletoverylargecor-
poraandrepresentphrasaltranslationunits.Tradingeitheroftheseformoreprincipled
learningframeworksisunlikelytoyieldimprovementsinperformance.
2. Part I (Chapters 2–6): Enabling Technologies
The ﬁrst section features a collection of ﬁve chapters dealing with technologies which
are related to, but not core, SMT. Chapter 2, “Mining Patents for Parallel Corpora,” by
Masao Utiyama and Hitoshi Isahara, describes the application of standard techniques
for collecting parallel corpora to a Japanese–English patent data corpus. Lacking any
particular novel insights or applications of machine learning, this will mostly be of
interest to those seeking data in that particular domain. The problem of constructing
dictionaries of named entities and their translation is tackled by Bruno Pouliquen and
Ralf Steinberger in Chapter 3, “Automatic Construction of Multilingual Name Dictio-
naries.”ThisisaninterestingproblemwithrelevanceforcommercialMTsystemswhich
must avoid nonsensical literal translations of named entities. However, the treatment
takes the form of a system description and fails to make use of any machine learning,
thusfeelingsomewhatoutofplaceinthisbook.
ThingspickupinChapter4,“NamedEntityTransliterationandDiscoveryinMul-
tilingualCorpora,”byAlexandreKlementievandDanRoth,againdealingwithnamed
entities but this time making novel use of their temporal occurrence distributions. The
authors are able to learn both entity alignments and transliterations with an iterative
procedure using the observation that, in temporally aligned parallel corpora, a named
entityanditstranslationwillappearco-locatedintime.Thisisaninterestingtechnique
andshouldbeequallyapplicabletothealignmentofotherwordtypes.
Chapter 5, “Combination of Statistical Word Alignments based on Multiple Pre-
processing Schemes,” byJakobElming, NizarHabash,andJosepM.Crego,tacklesthe
problemofwordalignmentformorphologicallyrichlanguagessuchasArabic.Toavoid
the issue of having to choose a single morphological tokenization, the authors create
alignmentsfromarangeoftokenizationswhicharethencombinedusingabinaryclas-
siﬁer trained on hand-aligned data. Although of particular interest for those working
with Arabic, this chapter fails to go beyond other works on supervised training for
word alignment which have consistently shown that it’s easy to achieve large gains
in alignment accuracy while much more difﬁcult to impact on end-to-end translation
performance(FraserandMarcu2007).
Part I ﬁnishes with a chapter that applies more-advanced machine learning than
those before. In “Linguistically Enriched Word-Sequence Kernels for Discriminative
LanguageModeling,”PierreMah´eandNicolaCanceddademonstratetheuseofstring
kernels for language modeling, evaluating a number of kernels including one able to
integrate a range of factors (surface form, lemma, part-of-speech). This is interesting
638
BookReview
work, showing that complex machine learning techniques can be brought to bear on
basicNLPtasks,althoughscalingissueslimittheevaluationtosmallartiﬁcialdatasets.
3. Part II (Chapters 7–13): Machine Translation
PartIIpresentsacollectionofworksmoredirectlyaddressingthetitleofthebook.Itis
often the case that research seeking to apply machine learning techniques to SMT can
neatlybedividedintotwocategories:thosethatsimplifyanddecomposethetranslation
problem into subtasks that ﬁt existing classiﬁcation models; and those that maintain
the structure of state-of-the-art models and develop new machine learning algorithms
speciﬁcallyforthem.
Chapters 7 and 10 ﬁt in the ﬁrst category. Both decompose the translation prob-
lemintosubproblems,particularlyfocusingonlexicalchoiceasclassiﬁcation.InChap-
ter7,“TowardPurelyDiscriminativeTrainingforTree-StructuredTranslationModels,”
Benjamin Wellington, Joseph Turian, and I. Dan Melamed seek to transduce source
syntax trees into target strings by learning local classiﬁers for the nodes in the trees.
Although such an approach allows SMT to be viewed as learning local classiﬁers,
the trade-offs made seem to signiﬁcantly limit the model, something encountered
in other works on local tree transduction (Yamada and Knight 2002). In Chapter 10,
“Statistical Machine Translation through Global Lexical Selection,” Srinivas Bangalore,
StephanKanthak,andPatrickHaffnertakeabag-of-wordsapproach,ignoringordering
information and learning classiﬁers that predict the presence of target lexical items
givenanentiresourcesentence.Thischaptertakesquiteanovelﬁnite–statetransducer
approachtoSMT;however,againthesimplifyingmodelingassumptionsseemlimiting.
Perhaps the most novel and interesting chapter of this book is Chapter 9, “Kernel-
Based Machine Translation,” by Zhuoran Wang and John Shawe-Taylor. This work
directly addresses the aim of the book: applying powerful state-of-the-art machine
learning approaches to machine translation. The authors describe a class of bilingual
string kernels capable of modeling phrase-based SMT without constraining phrase
extraction with word alignments, instead modeling unrestricted phrase co-occurrence.
The learning objective chosen is to minimize the squared loss of the n-gram overlap of
candidatetranslationsgiventhereference,acloseﬁttotheevaluationmetricBLEU.The
inevitablescalingproblemsaretackledwithanovelinformationretrievalapproach.For
each test sentence the algorithm sub-selects training samples based on lexical overlap
and decodes using a regression model based on this subset. The results achieved are
surprisinglycompetitivewithastandardphrase-basedmodel,anencouragingoutcome
giventhatnoexplicitlanguagemodelispresentinthekernel-baseddecoder.
Additionalchapters(8:“RerankingforLarge-ScaleStatisticalMachineTranslation”
by Kenji Yamada and Ion Muslea; 11: “Discriminative Phrase Selection for SMT” by
Jes´usGim´enezandLlu´ısM`arquez;12:“SemisupervisedLearningforMachineTransla-
tion”byNicolaUefﬁng,GholamrezaHaffari,andAnoopSarkar)coverrelativelywell-
troddenground,takingstandardSMTmodelsandapplyingcommonmachinelearning
algorithmstoasub-partofthesystem(re-ranking,discriminativephraseselection,and
semi-supervised learning, respectively). These chapters provide solid descriptions of
applying these techniques and the performance gains that can be achieved, a useful
contribution for anyone seeking to augment their existing decoder. However, a caveat
hereistheevaluationinChapter11.Althoughtheauthorsmustbecommendedontheir
thoroughness, the vast number of metrics used (one table includes 37!) provides more
confusionthanclaritywhenseekingtounderstandtheperformanceoftheirsystem.
639
ComputationalLinguistics Volume35,Number4
In the ﬁnal chapter, “Learning to Combine Machine Translation Systems,” Evgeny
Matusov, Gregor Leusch, and Hermann Ney introduce a novel approach to learning
systemcombinationmodelsbasedonconfusionnetworks.Thischapterprovidesanice
treatment of this topic with an evaluation demonstrating the consistent performance
gains that can be achieved; it will be of particular interest for those involved in multi-
siteevaluationcampaigns.
4. Summary
In an age in which most research publications can be readily accessed for free via the
Web, a collected-works publication such as this stands on its ability to bring together
articles which compactly summarize and deﬁne a direction of research. In this respect,
this book fallsshort of being amust-buy fortheSMT researcher, asmany of theworks
tendtowardstheesoteric,makingithardforsomeoneseekingfamiliaritywiththeﬁeld
toseparatecorecontributionsfromthoseunlikelytorepresentitsfuture.However,the
highdegreeofnoveltyandrangeinthecollectedarticles,withmanyauthorsproposing
new structures for translation models, still make this a worthwhile read with great
potentialtoinspirefutureresearch.
References
Fraser,AlexanderandDanielMarcu.2007.
Measuringwordalignmentqualityfor
statisticalmachinetranslation.
Computational Linguistics,33(3):293–303.
Yamada,KenjiandKevinKnight.2002.A
decoderforsyntax-basedstatisticalMT.
Proceedingsof the40thAnnual Meetingon
Association for ComputationalLinguistics,
pages303–310,Philadelphia,PA.
Phil Blunsom is a research fellow in the School of Informatics at the University of Edinburgh.
He conducts research focusing on the application of machine learning to complex structured
problemsinlanguageprocessing,suchasmachinetranslation,languagemodeling,parsing,and
grammar induction. Blunsom’s address is School of Informatics, The University of Edinburgh,
10CrichtonStreet,Edinburgh,EH89AB,UnitedKingdom;e-mail:pblunsom@inf.ed.ac.uk;URL:
http://homepages.inf.ed.ac.uk/pblunsom.
640

