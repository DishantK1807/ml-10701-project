<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<title>Inside-outside estimation of a lexicalized pcfg for german</title>
<date>1999</date>
<booktitle>In ACL 37</booktitle>
<marker>1999</marker>
<rawString>1999. Inside-outside estimation of a lexicalized pcfg for german. In ACL 37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
</authors>
<title>Automatic acquisition of subcategorization frames from untagged text</title>
<date>1991</date>
<booktitle>In ACL 29</booktitle>
<contexts>
<context>ation are important for various tasks like parsing, machine translation, etc. Creation of a resource containing such information from large corpora has received much attention in the community, with (Brent, 1991), (Ushioda et al., 1993), and (Manning, 1993) being early attempts at extracting frames from raw data. (Briscoe and Carroll, 1997) induce 163 pre-defined frame types, using apriori information about </context>
</contexts>
<marker>Brent, 1991</marker>
<rawString>M. Brent. 1991. Automatic acquisition of subcategorization frames from untagged text. In ACL 29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th ACL Conference on Applied NLP</booktitle>
<contexts>
<context>parseness of lexical distributions, computational lexicons derived from corpora should be based on very large corpus samples, much larger than the roughly 50,000-sentence Penn Treebank (for example, (Briscoe and Carroll, 1997)). (Beil et al., 1999; im Walde, 2002) demonstrated that PCFG grammars and lexicons with incorporated valence features could be improved by iterative EM estimation; however their grammar was not a tr</context>
<context> information from large corpora has received much attention in the community, with (Brent, 1991), (Ushioda et al., 1993), and (Manning, 1993) being early attempts at extracting frames from raw data. (Briscoe and Carroll, 1997) induce 163 pre-defined frame types, using apriori information about probabilities of particular frame types to filter the induced frames while (Korhonen, 2002) uses Levin classes to get better back-</context>
<context>ionaries, or on distributional similarity measures. Most are evaluated on testsets of high-frequency verbs (unlike the present work), in order to gauge the effectiveness of the acquisition strategy. (Briscoe and Carroll, 1997) report a token-based evaluation for seven verb types– their system gets an average recall of 80.9% for these verbs (which appear to be high-frequency verbs). This is slightly lower than the present </context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>Ted Briscoe and John Carroll. 1997. Automatic extraction of subcategorization from corpora. In Proceedings of the 5th ACL Conference on Applied NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chen</author>
<author>K Vijay-Shankar</author>
</authors>
<title>Automatic extraction of tags from the penn treebank</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th International Workshop on Parsing Technologies</booktitle>
<location>Trento, Italy. Stephen</location>
<contexts>
<context>r German to extract distributions for a large number of verbs from a German newspaper corpus. There has also been work to extract formalism-specific lexical resources from treebank data, for example (Chen and Vijay-Shankar, 2000) for LTAG, (Clark et al., 2002) for CCG, (Tsuruoka and Tsujii, 2004) for HPSG. 6.1. Evaluation In order to evaluate the models obtained from the insideoutside procedures, we focus on the task of dete</context>
</contexts>
<marker>Chen, Vijay-Shankar, 2000</marker>
<rawString>John Chen and K. Vijay-Shankar. 2000. Automatic extraction of tags from the penn treebank. In Proceedings of the 6th International Workshop on Parsing Technologies, Trento, Italy. Stephen Clark, Julia Hockenmaier, and Mark Steedman.</rawString>
</citation>
<citation valid="true">
<title>Building deep dependency structures using a wide-coverage ccg parser</title>
<date>2002</date>
<booktitle>In ACL40</booktitle>
<pages>00--00</pages>
<contexts>
<context>n (Carroll and Rooth, 1998) with a hand-written, head-lexicalised CFG and a raw corpus to iteratively estimate the distribution of subcategorization frames for particular predicates. Schulte im Walde (2002) also uses a head-lexicalised grammar for German to extract distributions for a large number of verbs from a German newspaper corpus. There has also been work to extract formalism-specific lexical res</context>
</contexts>
<marker>2002</marker>
<rawString>2002. Building deep dependency structures using a wide-coverage ccg parser. In ACL40, pages 00–00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>A subcategorisation lexicon for german verbs induced from a lexicalised pcfg</title>
<date>2002</date>
<booktitle>In LREC</booktitle>
<contexts>
<context>xicons derived from corpora should be based on very large corpus samples, much larger than the roughly 50,000-sentence Penn Treebank (for example, (Briscoe and Carroll, 1997)). (Beil et al., 1999; im Walde, 2002) demonstrated that PCFG grammars and lexicons with incorporated valence features could be improved by iterative EM estimation; however their grammar was not a treebank grammar, and therefore could no</context>
</contexts>
<marker>Walde, 2002</marker>
<rawString>Sabine Schulte im Walde. 2002. A subcategorisation lexicon for german verbs induced from a lexicalised pcfg. In LREC 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Pcfg models of linguistic tree representations</title>
<date>1998</date>
<journal>Computational Linguistics</journal>
<volume>24</volume>
<contexts>
<context>ation about the tree shapes associated with lexical items, using information implicit in the treebank. We also use features which are tree-geometric rather that linguistic in nature, in the style of (Johnson, 1998; Klein z.-.-.intransitive n.-.-.NP p.-.-.PP np.-.-.NP PP s.-.-.S b.-.-.SBAR t.-.-.-PRD (predicate complement) s.e.to control s.-.sc active small clause complement s.e.sc passive small c</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Mark Johnson. 1998. Pcfg models of linguistic tree representations. Computational Linguistics, 24(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Accurate unlexicalized parsing</title>
<date>2003</date>
<booktitle>In ACL 41</booktitle>
<location>Sapporo, Japan</location>
<contexts>
<context>es in the treebank PCFG 3. PCFG Compilation and Parsing application In treebank parsing applications, PCFGs are often created by incorporating features into context free grammar symbols (for example,(Klein and Manning, 2003)). We use a method which compiles a frequency table for a PCFG from the feature annotated treebank database (Privman, 2003). For each symbol, a list of attributes to be incorporated into the symbol i</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher Manning. 2003. Accurate unlexicalized parsing. In ACL 41, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
</authors>
<title>Subcategorization Acquisition</title>
<date>2002</date>
<tech>Ph.D. thesis</tech>
<institution>Univ. of Cambridge</institution>
<contexts>
<context>cting frames from raw data. (Briscoe and Carroll, 1997) induce 163 pre-defined frame types, using apriori information about probabilities of particular frame types to filter the induced frames while (Korhonen, 2002) uses Levin classes to get better back-off estimates for hypothesis selection at the filtering stage. An approach similar to ours is used in (Carroll and Rooth, 1998) with a hand-written, head-lexica</context>
</contexts>
<marker>Korhonen, 2002</marker>
<rawString>Anna Korhonen. 2002. Subcategorization Acquisition. Ph.D. thesis, Univ. of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language</title>
<date>1990</date>
<pages>4--35</pages>
<contexts>
<context> of the treebank for testing. On the task of identifying the valence of token occurrences of novel verbs, we get up to 23.38% error reduction following a standard inside-outside estimation procedure (Lari and Young, 1990). A modified inside-outside procedure which re-estimated lexical parameters while retaining syntactic parameters in the PCFG gives a reduction in error rate of 31.6%. In the sections to follow, we fi</context>
<context>2. Re-estimation using Inside-Outside Starting with the smoothed treebank model t and corpus C, two procedures are carried out. The first procedure is the standard iterative inside-outside procedure (Lari and Young, 1990). The second procedure has a frequency transformation step interleaved between the inside-outside iterations. In this transformation, lexical parameters from the re-estimated model and the original t</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>K. Lari and S. J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
</authors>
<title>Automatic acquisition of a large subcategorization dictionary from corpora</title>
<date>1993</date>
<booktitle>In ACL 31</booktitle>
<contexts>
<context>parsing, machine translation, etc. Creation of a resource containing such information from large corpora has received much attention in the community, with (Brent, 1991), (Ushioda et al., 1993), and (Manning, 1993) being early attempts at extracting frames from raw data. (Briscoe and Carroll, 1997) induce 163 pre-defined frame types, using apriori information about probabilities of particular frame types to fi</context>
</contexts>
<marker>Manning, 1993</marker>
<rawString>C. Manning. 1993. Automatic acquisition of a large subcategorization dictionary from corpora. In ACL 31. M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.</rawString>
</citation>
<citation valid="true">
<title>Building a large annotated corpus of english: The penn treebank</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>330</pages>
<marker>1993</marker>
<rawString>1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313– 330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R O’Donovan</author>
<author>M Burke</author>
<author>A Cahill</author>
<author>J van Genabith</author>
<author>A Way</author>
</authors>
<title>Large-scale induction and evaluation of lexical resources from the penn-ii and penn-iii treebanks</title>
<date>2005</date>
<journal>Computational Linguistics</journal>
<pages>31--329</pages>
<marker>O’Donovan, Burke, Cahill, van Genabith, Way, 2005</marker>
<rawString>R. O’Donovan, M. Burke, A. Cahill, J. van Genabith, and A. Way. 2005. Large-scale induction and evaluation of lexical resources from the penn-ii and penn-iii treebanks. Computational Linguistics, 31:329–365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lior Privman</author>
</authors>
<title>Yappffun: Java implemention of shared forest algorithms</title>
<date>2003</date>
<journal>Computational Linguistics</journal>
<institution>Lab, Cornell University</institution>
<contexts>
<context>rporating features into context free grammar symbols (for example,(Klein and Manning, 2003)). We use a method which compiles a frequency table for a PCFG from the feature annotated treebank database (Privman, 2003). For each symbol, a list of attributes to be incorporated into the symbol is stipulated. For instance, it may be stipulated that VP incorporates the attributes Vform and Slash, and that verbs incorp</context>
<context> yap-parser, (Schmid, 2000)) 5. Map feature shared forests to PCFG rules and lexical entries with incorporated features. Parameter files for several choices of incorporations are included. (yappffun (Privman, 2003), java) 6. Adapt PCFG lexicon to a test corpus by tagging the test corpus and smoothing the PCFG lexicon to include the word forms in the test corpus ((Schmid, 1994), perl). 7. PCFG Viterbi parsing w</context>
</contexts>
<marker>Privman, 2003</marker>
<rawString>Lior Privman. 2003. Yappffun: Java implemention of shared forest algorithms. Computational Linguistics Lab, Cornell University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing</booktitle>
<contexts>
<context>orations are included. (yappffun (Privman, 2003), java) 6. Adapt PCFG lexicon to a test corpus by tagging the test corpus and smoothing the PCFG lexicon to include the word forms in the test corpus ((Schmid, 1994), perl). 7. PCFG Viterbi parsing with labeled bracket and valence evaluation (bitpar (Schmid, 2004), evalb, perl) 8. Lexicon smoothing for modified inside-outside procedure and re-estimation on unsup</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of International Conference on New Methods in Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>YAP Parsing and Disambiguation With Feature-Based Grammars</title>
<date>2000</date>
<tech>Ph.D. thesis</tech>
<institution>University of Stuttgart</institution>
<contexts>
<context> treebank. First a feature constraint grammar is constructed by adding feature-constraints to a grammar obtained from the vanilla treebank. The formalism used is the Yap feature-constraint formalism (Schmid, 2000). The feature constraint annotations are similar in some respects to those used in LFG frameworks like (O’Donovan et al., 2005)– however, unlike (O’Donovan et al., 2005) who create a treebank LFG gra</context>
<context>ich solves constraints in the shared forest. This stage adds features and may split a tree into several solutions, writing out a feature shared forest. For solving constraints, we use the parser Yap (Schmid, 2000), and a feature-constraint grammar that we create using the vanilla treebank grammar as backbone. 2.1. Feature Constraint Grammar A context free grammar containing approximately 30,000 rules is obtai</context>
<context>d with context free categories. A constraint consists of a feature name, an equal sign and a value, followed by a semi-colon, and with possible feature values being variables, constants, lists, etc. (Schmid, 2000). VP -&gt; VB ADVP VP VP {Vform=base; Slash=sl;} -&gt; ‘VB { Val=aux; Prep=-; Vsel=vf; Prtcl=-; Sbj=-; } ADVP {} VP { Slash=sl; Vform=vf; } The above rule is for auxiliary VPs. A VP licensed by the rule ma</context>
<context>t free shared forest ti.cpf representing one tree. (lisp) 4. Solve feature constraints in each context-free shared forest ti.cpf to produce a feature shared forest ti.fpf. (yap-compiler, yap-parser, (Schmid, 2000)) 5. Map feature shared forests to PCFG rules and lexical entries with incorporated features. Parameter files for several choices of incorporations are included. (yappffun (Privman, 2003), java) 6. A</context>
</contexts>
<marker>Schmid, 2000</marker>
<rawString>Helmut Schmid. 2000. YAP Parsing and Disambiguation With Feature-Based Grammars. Ph.D. thesis, University of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Efficient parsing of highly ambiguous context-free grammars with bit vectors</title>
<date>2004</date>
<booktitle>In COLING</booktitle>
<contexts>
<context>ed treebank using standard PARSEVAL measures. We obtain maximum probability (Viterbi) parses for all sentences in the standard test section of the Penn Treebank (Section 23), using the parser Bitpar (Schmid, 2004). Table 2 shows the labeled bracketing scores for an optimal combination of features incorporated in the PCFG symbols. The labeled bracketing scores are comparable to state-of-the-art unlexicalized g</context>
<context>4 million words of unannotated Wall Street Journal text (year 1997), with sentence length restricted to less than 25 words. The re-estimation was carried out over a cluster of computers using Bitpar (Schmid, 2004) for inside-outside estimation. The parameter λ in Equation 3 was set to 0.5 for all τ and ι, giving equal weight to the treebank and the re-estimated lexicons. Starting from a smoothed treebank gram</context>
<context>agging the test corpus and smoothing the PCFG lexicon to include the word forms in the test corpus ((Schmid, 1994), perl). 7. PCFG Viterbi parsing with labeled bracket and valence evaluation (bitpar (Schmid, 2004), evalb, perl) 8. Lexicon smoothing for modified inside-outside procedure and re-estimation on unsupervised training corpus (perl, bitpar). 9. Constraint grammar files that are the output of 3 for tr</context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>Helmut Schmid. 2004. Efficient parsing of highly ambiguous context-free grammars with bit vectors. In COLING 2004. Yusuke Miyao Tsuruoka, Yoshimasa and Jun’ichi Tsujii.</rawString>
</citation>
<citation valid="true">
<title>Towards efficient probabilistic hpsg parsing: integrating semantic and syntactic preference to guide the parsing</title>
<date>2004</date>
<booktitle>In Proceedings of IJCNLP-04 Workshop: Beyond</booktitle>
<location>Hainan Island, China</location>
<marker>2004</marker>
<rawString>2004. Towards efficient probabilistic hpsg parsing: integrating semantic and syntactic preference to guide the parsing. In Proceedings of IJCNLP-04 Workshop: Beyond shallow analyses Formalisms and statistical modeling for deep analyses, Hainan Island, China.</rawString>
</citation>
</citationList>
</algorithm>

