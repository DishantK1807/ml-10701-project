Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 33–40
Manchester, August 2008
Improving Alignments for Better Confusion Networks
for Combining Machine Translation Systems
Necip Fazil Ayan and Jing Zheng and Wen Wang
SRI International
Speech Technology and Research Laboratory (STAR)
333 Ravenswood Avenue
Menlo Park, CA 94025
{nfa,zj,wwang}@speech.sri.com
Abstract
The state-of-the-art system combination
method for machine translation (MT) is
the word-based combination using confusion
networks. One of the crucial steps in
confusion network decoding is the alignment
of different hypotheses to each other
when building a network. In this paper, we
presentnewmethodstoimprovealignment
of hypotheses using word synonyms and a
two-pass alignment strategy. We demonstrate
that combination with the new alignment
technique yields up to 2.9 BLEU
point improvement over the best input system
and up to 1.3 BLEU point improvement
over a state-of-the-art combination
method on two different language pairs.
1 Introduction
Combining outputs of multiple systems performing
the same task has been widely explored in
various fields such as speech recognition, word
sense disambiguation, and word alignments, and it
had been shown that the combination approaches
yielded significantly better outputs than the individual
systems. System combination has also
been explored in the MT field, especially with
the emergence of various structurally different MT
systems. Various techniques include hypothesis
selection from different systems using sentencelevel
scores, re-decoding source sentences using
phrases that are used by individual systems (Rosti
et al., 2007a; Huang and Papineni, 2007) and
c©2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported license
(http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
word-based combination techniques using confusion
networks (Matusov et al., 2006; Sim et al.,
2007; Rosti et al., 2007b). Among these, confusion
network decoding of the system outputs has
been shown to be more effective than the others in
terms of the overall translation quality.
One of the crucial steps in confusion network
decoding is the alignment of hypotheses to each
other because the same meaning can be expressed
with synonymous words and/or with a different
word ordering in different hypotheses. Unfortunately,
all the alignment algorithms used in confusion
network decoding are insensitive to synonyms
of words when aligning two hypotheses to each
other. This paper extends the previous alignment
approaches to handle word synonyms more effectively
to improve alignment of different hypotheses.
We also present a two-pass alignment strategy
for a better alignment of hypotheses with similar
words but with a different word ordering.
We evaluate our system combination approach
using variants of an in-house hierarchical MT system
as input systems on two different language
pairs: Arabic-English and Chinese-English. Even
with very similar MT systems as inputs, we show
that the improved alignments yield up to an absolute
2.9 BLEU
point improvement over the best
input system and up to an absolute 1.3 BLEU
point improvement over the old alignments in a
confusion-network-based combination.
The rest of this paper is organized as follows.
Section 2 presents an overview of previous system
combination techniques for MT. Section 3 discusses
the confusion-network-based system combination.
In Section 4, we present the new hypothesis
alignment techniques. Finally, Section 5
presents our experiments and results on two language
pairs.
33
2 Related
Work
System combination for machine translation can
be done at three levels: Sentence-level, phraselevel
or word-level.
Sentence-level combination is done by choosing
onehypothesisamongmultipleMTsystemoutputs
(and possibly among n-best lists). The selection
criterioncanbeacombinationoftranslationmodel
and language model scores with multiple comparison
tests (Akiba et al., 2002), or statistical confidence
models (Nomoto, 2004).
Phrase-level combination systems assume that
the input systems provide some internal information
about the system, such as phrases used by the
system, and the task is to re-decode the source sentence
using this additional information. The first
exampleof thisapproachwasthe multi-engineMT
system (Frederking and Nirenburg, 1994), which
builds a chart using the translation units inside
each input system and then uses a chart walk algorithm
to find the best cover of the source sentence.
Rosti et al. (2007a) collect source-to-target correspondences
from the input systems, create a new
translation option table using only these phrases,
and re-decode the source sentence to generate better
translations. In a similar work, it has been
demonstrated that pruning the original phrase table
according to reliable MT hypotheses and enforcing
the decoder to obey the word orderings in
the original system outputs improves the performance
of the phrase-based combination systems
(Huang and Papineni, 2007). In the absence of
source-to-target phrase alignments, the sentences
can be split into simple chunks using a recursive
decomposition as input to MT systems (Mellebeek
et al., 2006). With this approach, the final output
is a combination of the best chunk translations that
are selected by majority voting, system confidence
scores and language model scores.
The word-level combination chooses the best
translation units from different translations and
combine them. The most popular method for
word-based combination follows the idea behind
theROVERapproachforcombiningspeechrecognition
outputs (Fiscus, 1997). After reordering
hypotheses and aligning to each other, the combination
system builds a confusion network and
chooses the path with the highest score. The following
section describes confusion-network-based
system combination in detail.
© 205 SRI nterationl
Confusion Network Example
Hypothesi 1:she wnt home
2sscol
3atom
she
schol
#eps home
at
wen
was
she
home#epswent
#eps
wa
#eps
went
at
#eps
hom #ps
schol
<s>
<s>
</s>
</s>
Figure 1: Alignment of three hypotheses to each
other using different hypotheses as skeletons.
3 System
Combination with Confusion
Networks
The general architecture of a confusion-networkbased
system combination is as follows:
1. Extract n-best lists from MT systems.
2. Pickaskeletontranslationforeachsegment.
3. Reorder all the other hypotheses by aligning
them to the skeleton translation.
4. Build a confusion network from the reordered
translations for each segment.
5. Decode the confusion network using various
arc features and sentence-level scores
such as LM score and word penalty.
6. Optimize feature weights on a held-out test
set and re-decode.
In this framework, the success of confusion networkdecodingforsystemcombinationdependson
workdecodingforsystemcombinationdependson
two important choices: Selection of the skeleton
hypothesis and alignment of other hypotheses to
the skeleton.
For selecting the best skeleton, two common
methodsarechoosingthehypothesiswiththeMinimum
Bayes Risk with translation error rate (TER)
(Snover et al., 2006) (i.e., the hypothesis with the
minimum TER score when it is used as the reference
against the other hypotheses) (Sim et al.,
2007) or choosing the best hypotheses from each
system and using each of those as a skeleton in
multiple confusion networks (Rosti et al., 2007b).
In this paper, we use the latter since it performs
slightly better than the first method in our experiments.
An example confusion network on three
translations is presented in Figure 1.1
The major difficulty when using confusion networks
for system combination for MT is aligning
different hypotheses to the skeleton since the word
1Inthispaper, weusemultipleconfusionnetworksthatare
attached to the same start and end node. Throughout the rest
of the paper, the term confusion network refers to one network
among multiple networks used for system combination.
34
order might be different in different hypotheses
and it is hard to align words that are shifted from
one hypothesis to another. Four popular methods
to align hypotheses to each other are as follows:
1. Multiple string-matching algorithm based
on Levenshtein edit distance (Bangalore et
al., 2001)
2. A heuristic-based matching algorithm (Jayaraman
and Lavie, 2005)
3. Using GIZA++ (Och and Ney, 2000) with
possibly additional training data (Matusov
et al., 2006)
4. Using TER (Snover et al., 2006) between
the skeleton and a given hypothesis (Sim et
al., 2007; Rosti et al., 2007b)
None of these methods takes word synonyms
into account during alignment of hypotheses.2 In
this work, we extend the TER-based alignment
to use word stems and synonyms using the publicly
available WordNet resource (Fellbaum, 1998)
when aligning hypotheses to each other and show
thatthisadditionalinformationimprovesthealignment
and the overall translation significantly.
4 Confusion
Networks with Word
Synonyms and Two-pass Alignment
When building a confusion network, the goal is to
put the same words on the same arcs as much as
possible. Matching similar words between two hypotheses
is necessary to achieve this goal.
When we align two different hypotheses using
TER, it is necessary that two words have the identical
spelling to be considered a match. However,
in natural languages, it is possible to represent the
same meaning using synonyms of words in possibly
different positions. For example, in the following
sentences, “at the same time” and “in the
meantime”, “waiting for” and “expect”, and “set”
and “established” correspond to each other, respectively:
spectively:
Skeleton: at the same time expect israel
to abide by the deadlines set by .
Hypothesis: in the meantime , we are
waiting for israel to abide by the
established deadlines .
Using TER, synonymous words might be
aligned to each other if they appear in the same po2Note
that the approach by Matusov et al. (2006) attempts
to align synonyms and different morphological forms
of words to each other but this is done implicitly, relying on
the parallel text to learn word alignments.
sition in two hypotheses but this is less likely when
two words appear in different positions. Without
knowing that two words are synonyms of each
other, they are considered two separate words during
TER alignment.
Our goal is to create equivalence classes for
each word in the given translations and modify the
alignment algorithm to give priority to the matching
of words that are in the same equivalence class.
In this paper, the equivalence classes are generated
using WordNet by extracting synonyms of each
word in the translations.
To incorporate matching of word synonyms into
the alignment, we followed three steps:
1. Use WordNet to extract synonyms of the
words that appear in all hypotheses.
2. Augment each skeleton word with all synonymous
words that appear in all the hypotheses.
potheses.
3. Modify TER script to handle words with
alternatives using an additional synonym
matching operation.
In the following subsections, we describe how
each of these tasks is performed.
4.1 Extracting
Synonyms from WordNet
The first step is to use WordNet to extract synonyms
of each word that appears in all hypotheses.
This is simply done using the publicly available
WordNet processing tools to extract all synonyms
of the given word. To allow matching words that
have the same stem or variations of the same word
with different part-of-the-speech (POS) tags, we
extract all synonyms of the given word regardless
of their POS tag in the given translation.3
In the example above, it is clear that the verbs
wait and expect have the same meaning but TER
is unable to align these two words to each other
because of different word positions. Using WordNet,
we get the following two entries for the words
waiting and expect, indicating that two words can
be considered as synonyms of each other:
expect: Sense 1: expect, anticipate ⇒ evaluate,
pass judgment, judge. Sense 2: ask, require,
expect ⇒ demand. Sense 3: expect, look, await,
wait. Sense 4: expect ⇒ see, consider, reckon,
3Our goal is to add as many synonyms as possible to increase
the chances of a word aligning to one of its possible
synonyms rather than to any other word. Therefore, we do
not distinguish between the synonyms of the same word according
to their confidence value or their POS tag.
35
ONE-PASS ALIGNMENT
khaddam receives italian house speaker
khaddam receives the head of the italian chamber of deputies
khaddam will meet with the president of the italian chamber of deputies
TWO-PASS ALIGNMENT
khaddam receives italian house speaker
khaddam receives of the head italian chamber of the deputies
khaddam will meet with president of the italian chamber of the deputies
Figure 2: Example alignment of three hypotheses with one-pass and two-pass alignment
view, regard. Sense 5: expect ⇒ expect, look,
await, wait. Sense 6: have a bun in the oven, bear,
carry, gestate, expect ⇒ give birth, deliver, bear,
birth, have.
waiting: Sense 1: wait. Sense 2: wait, hold
off, hold back ⇒ act, move. Sense 3: wait, expect,
look, await. Sense 4: wait, waitress ⇒ work.
We should note that WordNet contains only
open-class words, i.e., nouns, verbs, adjectives,
and adverbs. There are no entries for determiners,
prepositions, pronouns, conjunctions, and particles.
For better matching of these additional POS
tags, we manually created a different equivalence
class for each POS tag that is not included in the
WordNet so that words with the same POS tag can
be considered synonymous.

