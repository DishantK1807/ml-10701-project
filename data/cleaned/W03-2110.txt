
SpokenDialogueforVirtualAdvisersinasemi-imme rsiveCommand
andControlenvironment
DominiqueEstival,MichaelBroughton,AndrewZschor n,ElizabethPronger
HumanSystemsIntegrationGroup,CommandandContro lDivision
DefenceScienceandTechnologyOrganisation
POBox1500,EdinburghSA5111
AUSTRALIA
{Dominique.Estival,Michael.Broughton,AndrewZscho rn}@dsto.defence.gov.au


Abstract
We present the spoken dialogue system
designed and implemented for Virtual
AdvisersintheFOCALenvironment.Its
architectureisbasedon:DialogueAgents
using propositional attitudes, a Natural
Language Understanding component
using typed unification grammar, and a
commercial speaker-independent speech
recognition system. The current
application aims to facilitate the multi-
media presentation of military planning
information in a semi-immersive
environment.
1 Introduction
In this paper, we present the spoken dialogue
systemimplementedforcommunicatingwiththe
virtual advisers (VAs) in the Future Operations
Centre Analysis Laboratory (FOCAL) at the
Australian Defence Science and Technology
Organisation(DSTO).Weareexperimentingwith
the use of spoken dialogue with virtual
conversational characters to access multi-media
information during the conduct of military
operations and in particular to facilitate the
planningofsuchoperations.
Unlike telephone-based dialogue systems
(Estival,2002),whicharemainlycreatedfornew
commercial applications, dialogue systems for
CommandandControlapplications(Mooreetal.
1997) generally seek to simulate the military
domainandthereforerequireanunderstandingof
thatdomain.
2 UsingVirtualAdvisersinFOCAL
FOCAL was established to "pioneer a paradigm
shiftincommandenvironmentsthroughasuperior
useofcapabilityandgreatersituationawareness". 
The facility was designed to experiment with
innovativetechnologiestosupportthisgoal,andi t
hasnowbeenrunningfortwoyears.
FOCAL contains a large-screen, semi-
immersive virtual reality environment as its
primary display, allowing vast quantities of
informationtobedisplayed.OurcurrentVAscan
be described as 3-dimensional "Talking Heads",
i.e.onlytheheadandupperportionsofthebody
arerepresented.Theycandisplayexpression,lip-
synchronisation and head movement, along with
certain autonomous behaviours such as blinking
and gaze (Taplin et al., 2001). These factors all
combinetoaddlife-likenesstotheVAsandcreate
moreengaginginteractionwithusers.
PresentinginformationviaaTalkingHeadhas
been commercially demonstrated by the virtual
newscaster “Ananova” (Ananova, 2002).
Embodiedcharactersarealsobeingdevelopedand
include the PPP (Andre, Rist and Muller, 1998)
andRea(Cassell,2000).PPPisacartoonstyle
Personalized Plan-based Presenter that combines
pointing, headmovementsandfacial expressions
todrawtheviewer’sattentiontotheinformation
beingpresented.Reaisavirtualreal-estateagen t
thattakesanactiveroleinconversation,shenods 
herheadtoindicateunderstandingofspokeninput, 
orcanraiseherhandtoindicateadesiretospeak .
Several VAs have been implemented for
FOCAL, each having a particular role or
knowledge expertise.  For example, one adviser

may have specialist knowledge relating to legal
issues, another may have information relating to
the geography of a region.  Each VA has a
differentfacialappearance,voiceandmannerisms.
Todemonstrate andevaluatetheperformance
of VAs (and of the other FOCAL projects), a
fictitious scenario has been developed that
incorporateskeyelementsofmilitaryplanningat
the operational level (see section 8).  The VAs
provide information rich briefs through the
combineduseofspokenoutputviaText-to-Speech
(TTS)andmultimedia.Relevantquestionscanbe
askedattheendofthebriefsthroughtheuseof
spokendialogue.
3 Previousimplementation:Franco
Asdescribedin(Taplinetal.,2001)thefirstVA in
FOCAL,namedFranco,wasalsoananimated3-
dimensional "Talking Head" model, intended to
either deliver prepared information, such as a
briefing or slide show, or to interact
conversationally with users. To demonstrate the
conversational functionality (Broughton et al.,
2002), it was implemented with a commercial
speaker-dependent automated speech recogniser
(ASR),DragonNaturallySpeaking™.TheNatural
Language understanding component was
implemented in NatLink (Gould, 2001) and a
simpleuser-drivendialoguemanagement,basedon
key-word recognition and nesting of dialogue
statestoprovidecontext,wasalsoimplementedin
Python.
Franco has been successful in demonstrating
the proof-of-concept of a VA in the FOCAL
environment.Answeringspokenquestionsabout
specific military assets and platforms, it also
permitsthedisplayofothertypesofinformation
such as pictures, animated video clips, tabular
informationfromadatabase,andlocationdetails
ondigitalmaps.
4 Improvements
AlthoughFrancowassuccessfulindemonstrating
the potentialusefulnessofa VAina Command
andControlenvironmentforoperationalplanning,
it suffers from certain limitations which we are
nowaddressinginafollow-upproject.
Thefirstlimitation,andtheeasiesttoremedy,
was the unnaturalness of the synthetic voice we
hadgiven Franco.Forgreatereffectiveness,we
hadtoprovideourVAwithamorenaturalvoice
andwithanAustralianaccent.Wechosethenew
AustralianTTSvoicefromRhetorical,developed
byAppen(rVoice,2002).Thisrequiredmaking
somechanges,someofthemrelativelyimportant,
to the interface with the talking head model to
achievelip-synchronisation,butthataspectofthe 
workwillnotbeaddressedinthispaper.
Thesecondlimitationwastherelativerigidity
of the dialogue management strategy we were
using.  The alternative approach we have
developed is to create Dialogue Agents
implementedinA TTITUDE.Thisisdescribedin
section6.
The third limitation was due to the speaker-
dependentnatureoftheASR. Whileaspeaker-
dependent ASR allows greater flexibility in the
inputtowhichtheVAcanrespond,wewantedto
develop a system which could not only be
demonstratedbythefewpeoplewhohavetrained
the speech recogniser, but where visitors
themselvescouldbeparticipantsandcouldinteract 
withtheVA.Switchingtoaspeaker-independent
ASR led us to radically modify our Spoken
Language Understanding component, and this is
describedinsection7.
Thenewimplementationwedescribeherehas
allowed us not only to address those three
limitations, but also to alter fundamentally the
architectureofthesystem,openingupthedialogue 
managementcomponentstocontrolandinteraction
by other tools and agents in the FOCAL
environment. The resultingsystem is nowfully
modular and provides scalability as well as
flexibility.
Thisnewimplementationallowsustofocusour
research into dialogue management issue, to
investigate the use of A TTITUDE for dialogue
managementandtoexperimentwithmorenatural
languageinput.
5 Integration
Communication between the variouscomponents
ofthesystem(speechrecogniser,dialoguecontrol, 
virtualadvisercontrolandmultimediadisplay)is
nowachievedwiththeCoABS(ControlofAgent
Based Systems) Grid infrastructure (Global
InfoTek,2002).TheCoABSGridwasdesignedto
allowalargenumberofheterogeneousprocedural,

object-oriented and agent-based systems to
communicate.  Using the CoABS Grid as our
infrastructure has allowed us to integrate all the
components of the dialogue system and it will
provideaneasywaytointegrateotheragentsanda 
variety of input and output devices.
Communication between CoABS agents is
accomplishedviastringmessages.
6 DialogueManagementwithA
TTITUDE
ATTITUDEisamulti-agentarchitecturedeveloped
at DSTO, capable of representing and reasoning
both with uncertainty and about multiple
alternative scenarios (Lambert, 1999).  It is a
multi-agent extension of the MetaCon reactive
planner developed for control of phased array
radars on the Swedish Airborne Early Warning
aircraft(LambertandRelbe,1998).A TTITUDEhas
some similarities with Prolog and other logic
programming languages as well as with AI
research on blackboard and multi-agent
architectures.  Because A TTITUDE was designed
specificallytosupporttheprogrammingofreactive 
systems, it possesses powerful facilities for
handling interactions of the internal system
entities,bothwitheachotherandwiththeexterna l
world.
ATTITUDE isveryhigh-level,weakly-typed,and
thankstotheagentparadigm,itproducesloosely
coupled and modularised systems. For these
reasons, and because A TTITUDE implements
reasoningabout propositionalattitudes ,itprovides
averyattractiveframeworkinwhichtodevelop
and express dialogue management control
strategies.  It is worth emphasizing here that
ATTITUDE is not merely a notation to represent
speech acts or  communicative acts between
agents, but that it is actually the programming
language and environment in which both the
agents themselves and the control structure for
interaction between the agents are implemented
andexecuted.
BecauseA TTITUDEhasneverbeenusedforthis
purpose before, this is an interesting area of
research in itself, and one of the goals of the
projecthasbeentoseehowA TTITUDEneedstobe
extended to implement dialogue management.
Further, this allows us to investigate how far
attitude programming  (see section 6.2) can go
towardsexpressingspeechactsandcommunicative
acttype.However,wedonotclaimtoemploythe
full power of propositional attitudes in our
implementation yet. This is another area of
researchwhichwearenowexploring.Neitherare
we yet at the stage where we could perform
automatic detection of utterance type (Wright,
1998)orofdialogueact(Carberryand Lambert,
1999;PrasadandWalker,2002).
6.1 Propositionalattitudes
The A TTITUDE programming environment is so
named because it utilises propositional attitude
instructionsasprogramminginstructions(thishas
beendubbed attitudeprogramming ).  Propositional
attitudesareallegedmentalstatescharacterisedb y
propositional attitude expressions, which are the
means by which individuals relate their own
mentalbehaviourtoothers'.
Propositionalattitudeinstructionsareoftheform 
shownin(1).

(1)[subject][attitude][propositionalexpression]

In(1):
-[subject]denotestheindividualwhosemental
stateisbeingcharacterised;
[propositional expression] describes some
propositionalclaimabouttheworld;and
-[attitude]expressesthesubject'sdispositional
attitudetowardthatclaimabouttheworld.
6.2 ATTITUDEprogramming
When software agent Mary encounters the
propositionalattitudeinstruction" Fred desire[the
door is closed]", Mary will issue a message to
softwareagent Fredinstructing Fredtodesirethat
thedoorbeclosed.Similarly,whenencountering
thepropositionalattitudeinstruction" I believe[the
skyisblue]", Maryherselfwillattempttobelieve
thattheskyisblue.
An important characteristic of A TTITUDE
programming is that each propositional attitude
instructioneithersucceedsorfails,possiblywith 
sideeffects,dependinguponwhethertherecipient
agentisabletosatisfytheinstructionalrequest. As
each propositional attitude instruction either
succeeds or fails, the execution path selected
through a network of propositional attitude
instructions (routine) is determined by the
successesandfailuresofthepropositionalattitud e

instructionsattemptedalongtheway.Thecontrol
structureisthereforegovernedbya semanticsof
success.
Computational routines for a software agent
arise by linking together particular choices of
propositionalattitudeinstructions.Thesenetworks 
ofpropositionalattitudeinstructionsthenprescri be
recipesdefiningthepossiblementalbehaviourofa 
softwareagent.
6.3 The
ATTITUDEDialogueAgents
We have implemented a number of A TTITUDE
DialogueAgents.ThemainagentinourDialogue
Management architecture (shown in Figure 1) is
the Conductor.Itistheagentresponsibleforthe
flowofinformationbetweentheotheragentsandit 
manages multi-modal interactions. The other
agents,alsodescribedfurtherinthis section,are 
the Speaker, the NLG (Natural Language
Generator),the MMP (MultimediaPresenter)and
several IS(InformationSource)agents.Inaddition
totheseagents,eachdialoguestate(seesection8 )
isalsoimplementedasanA TTITUDEagent,withits
ownsetofroutines.
As explained in section 6.2, each A TTITUDE
agent’s behaviour is programmed as a set of
routines

Figure1.DialoguewithA TTITUDE
The interaction between the A TTITUDE
DialogueagentsisshowninFigure1,inwhichthe
frame around the A TTITUDE agents can be
interpretedasrepresentingtheCoABSgrid.
SpeakerAgent
Whenspeechfromtheuserhasbeendetectedand
recognised, the attribute-value pairs for that
utterance (see section 7) are sent to Speaker.
Speaker takes that information and produces a
correspondingA TTITUDEexpression,whichisthen
forwardedto Conductor.
The linguistic coverage of the system is
determinedbythegrammarswhichareavailableat
each dialogue state.  For now, the coverage is
limited toa setof utterancesappropriate forthe
briefing scenario described in section 8.  These
wereusedtodefinetheRegulus1grammarsfrom
whichtheNuancegrammarsarecompiled.Weare
nowplanningtomovefromRegulus1toRegulus2,
which will allow us to derive dialogue state
grammarsfromalargeEnglishgrammarusingthe
EBLstrategydescribedin(Rayneretal.,2002b)








Conductor
Thisagentisresponsiblefordialogueflowcontrol andallothe  rdialogue 
agentsmustregisterwithit. 
Conductor receivescommunicativeactsfrom  Speaker.Forexample: 
(whquestion (property mig-29flying -range?value?units)) 
Thisqueryisforwardedontoallregisteredagents .  Conductor choosesthe 
mostappropriateresponsereceivedandsendsthist o MMP topresentthe 
answer.
Speaker
Speakerreceivesspeechrecognitionresultsinthe form
ofattribute -valuepairs,andtranslatestheseinto 
Attitudeexpressionstosendto  Conductor . 
InformationSource
(IS)Thiscategoryofagentseachregisterwith  Conductor andinterfacewithabackgrounddatasource,forexampl e,a 
databaseofaircraftproperties. 
Eachusestheirdatasourcetorespondtoqueriesf rom
Conductor .
MultimediaPresenter
(MMP)Thisagentreceivesalistofexpressionsfrom  Conductor anddirectstheappropriateservicestopresent multimediadatatotheuser.Forexample: 
((whanswer (property mig-29flying -range810nautical -miles))(image mig-29))
Inthiscase,  MMP requestsanEnglishformofthe  whanswerexpressionandsendstheresulttotheTTS 
application.Similarly,anappropriateapplication isdirectedt  odisplaytherequestedimage. 
NaturalLanguageGenerator
(NLG)Receivesexpressionsfrom  MMP andusestemplatestoreturn correspondingEnglishsentences. 
Englishquestionfromuser 
Nuance/Regulus
Attitudeexpression
Query
Response
Presentationdirectives
NLGdirective
TTSEnglishstring
VirtualAdvisorspeaking 
Englishstring
Attribute/Valuepairs
Multimediadisplayed

ConductorAgent
Conductor takes an A TTITUDE expression from
Speakerandforwardsitontoallthe IS agentsthat
haveregisteredwithit. Itthenwaitsforallthe 
responsestocomebackfromthoseagents,inthe
formoflistsofexpressions.
Everyresponse Conductorreceivesisputinto
its knowledge base, along with some extra
information:
-Sender:whichISagentsenttheresponse.
-In-Reply-To:whichpreviouscommunicative
actthisisaresponseto.
Strength: whether every expression of the
response is 'strong' (the sender believes it is
eitherabsolutetruthorabsolutenegation)orif
oneormoreis'weak'(thesenderbelievesitis
neitherabsolutetruthnorabsolutenegation).
-Bound-State:ifthereareanyfreevariablesin
theresponse,orifitisfullyground.
Unifiability: whether one or more of the
expressionsintheresponseisofthesameform
as Speaker’s initial expression.
The final expression in Conductor’s knowledge
baseisasshownin(2).

(2)(response?in-reply-to?sender?strength
?bound_state?unifiability?content)

Giventheinitialexpressionfrom Speakerandthe
repliesitreceivesfromtheISagents, Conductor
chooses the 'best' response. For example, a
response that is strong, fully ground and unifies
with Speaker’sexpressionisdeemedtobemore
relevant and informative than a response that is
weak and contains free variables.  Conductor
forwardsthisresponseto MMP.
MultimediaPresenter(MMP)
MMPiteratesthroughthelistofexpressionssent
by Conductorandpresentseachexpressiontothe
user. MMP recognisesclassesofexpressionsand
choosestopresentthemusingcertainmedia.For
example, some expressions are instructions to
changetheVAheadmodel,whileothersaretobe
translatedintoEnglishsentencesandspokenbythe 
VA.Forthelatterfunction MMPuses NLG (see
below).
Othermediathroughwhich MMPcanchooseto
present the information contained in the
expressionsinclude:imageryfromadatabase(e.g.
pictures of military platforms, or of strategic
locations), video clips, images from weather or
radar information sources, virtual video, 3-
dimensional virtual battle space maps, textual
informationandaudio.
NaturalLanguageGenerator(NLG)
For now, NLG uses templates to transform
ATTITUDEexpressionsintoEnglish.Forexample,
the instruction in (3) provides two possible
responsesfortheA TTITUDEexpressionspecified: 1

(3)(property?assetoverview?valuetext)
whanswerpriority10
((response1("The"?asset"isa"?value".") )
((response2("Iunderstandthatthe"?asset "isa
"?value"."))))

When NLG is first requested to generate the
Englishoutputfortheexpressionin(4.a),intende d
tobea communicativeact oftype whanswer, it
usesthetemplategivenin(4.b),correspondingto
"response1"in(3),toproducetheEnglishanswer
givenin(4.c).

(4.a)(propertymig-29overview"Russianmulti-role 
fighter"text)
b.("The"?asset"isa"?value".")
c.TheMig-29isaRussianmulti-rolefighter.

When NLGisrequestedasecondtimetogenerate
the output for (3), it uses the template in (5.a),
correspondingto"response2"in(3),toproduce
theEnglishanswergivenin(5.b).

(5.a)("Iunderstandthatthe"?asset"isa"?valu e".")
b.IunderstandthattheMig-29isaRussianm ulti-
rolefighter.

Thus NLGcyclesthroughthelistoftemplatesfor
appropriateresponses.Prioritiescanalsobegive n
to templates, enabling NLG to use general
templatestogetherwithmorespecificandtailored
ones.
It is clear that template-based language
generationistoorigidforfullynaturaldialogues ,
andweintendtoexploremoreflexibletechniques
after we implement a wider coverage English
grammar;however,ithassofarbeensufficientfor 
  
1Variablesaredenotedwith"?",whiletextstrings (tobesent
tospeechsynthesis,ordisplayedonaslide)areb etween
doublequotes,"".

our purposes, namely to demonstrate and
investigateagent-baseddialoguemanagement.
InformationSourceAgent(IS)
The ISagents,e.g.aWeatherAgentoraPlatform
Capabilities Agent, can answer users' questions,
eitherbyusingtheirowninternalknowledgebase
orbyaccessingexternalInformationSources,such
asaweatherinformationserver,oradatabaseof
military assets.  All IS agents register with
Conductor, and when an expression is sent by
Speaker,all ISagentstrytorespondtoit.

ByusingtheCoABSGridastheinfrastructureand
implementingtheagentwithA TTITUDE,weleave
the architecture extremely flexible and scalable
(Kahn and Della Torre Cicalese, 2001). For
instance,itispossibletoincreasetheamountof
information at the system’s disposal during run-
timebylaunchinganew ISagentandbyadding
sometemplatesto NLG.
6.4 Dialoguedesign
Fornow,thedialogueisspecifiedasafinitestat e
machineandisstillverymuchsystemdirected.In 
thebriefingapplication(seesection8.1),theVAs 
first "push" the information that needs to be
presented, as briefing officers do in a normal
briefing.Someoftheinformationisalsopresente d
usingvisualaids,suchaspowerpointslidesand
maps for specifying location  information.  The
information to bepresentedandthemediatobe
usedaredeterminedbytheagentforthatparticula r
dialoguestate.
TheVAthenallowsuserstoaskquestionsto
repeat or clarify particular points, or to gain
additionalinformation. 
7 SpokenLanguageProcessing
7.1 Speaker-independentspeechrecognition
Asstatedinsection4,oneofthemainmotivations 
formovingfromaspeaker-dependenttoaspeaker-
independentASRwastoallowvisitorsinFOCAL
the possibility of using the system themselves,
rather than relying on a small set of trained
individualstorundemonstrations.  We chose to
usetheNuanceToolkit(Nuance,2002)forseveral
reasons:  besides its reliability as a speaker-
independent ASR for both telephone and
microphone speech, Nuance 8.0 provides
Australian-New Zealand English, as well as US
andUKEnglish,acousticlanguagemodels.Even
more importantly for our purposes, Nuance
grammarscanbecompiledfromRegulus,ahigher-
level language processing component which has
already been used to develop several spoken
dialoguesystemsindifferentdomains(Rayneret
al.,2001,RaynerandBouillon,2002).
7.2 SpokenLanguageUnderstanding
Followingourdecision tomovefromaspeaker-
dependent to a speaker-independent ASR, we
decidedtouseRegulustoimplementourNatural
LanguageUnderstandingcomponent.Regulusis
an Open Source environment which compiles
typed unification grammars into context-free
grammar language models compatible with the
NuanceToolkit.Itis"writteninaProlog-based
feature-value notation and compiles into Nuance
GSLgrammars."(Rayneretal.,2002a).Regulus
isalsodescribedindetailin(Rayneretal.,2001 ).
ThemainmotivationforusingRegulusisthe
usual one of greater efficiency due to the more
compact nature of a unification grammar
representation compared with a context-free
grammar.Inaddition,usingRegulustodefinea
higherlevelgrammar,weareabletoobtainasour
semantic representation a list of attribute-value
pairs, and this permits a more sophisticated
processingoftheinformationbytheotheragents.
Regulus also allows the development of bi-
directionalgrammars,andweintendtomakeuse
ofthisfunctionalityinlaterimplementationsoft he
NLGagent.However,fornow,thegrammarswe
havedevelopedhavebeenlimitedtorecognition
andunderstanding.
8 Currentapplicationimplementation
8.1 Dialoguescenario
The scenario for the current application was
developed by members of the Human Systems
Integration(HSI)groupandisgroundedontheir
experience with, and observations of, military
operational planning.  It is based on a fictitious
scenario developed for training (the examples
givenherehaveallbeenmodified)andexemplifies

theJointMilitaryAppreciationProcess(JMAP)for
militaryplanningacrossthethreeservices(Army,
NavyandAirForce).Asub-scenariowaschosen
forthedevelopmentofthespokendialoguewith
theVAs. 2
8.2 Dialogueflow
Thestructurednatureofamilitaryplanningtask
suchasthisonemakesitveryeasytopartitionit 
intodifferentstages,whichcanthenbemappedto
different dialogue states. In our dialogue script,
each top-level dialogue state corresponds to a
sectionoftheplanningexercise,givenin(6).

(6)Commander'sInitialGuidance 
-CDF(ChiefofDefenceForces)Intent
-PlanningGuidance
-Constraints
-Restrictions
-LegalIssues
-CommandandControl

These6topleveldialoguestatesarethenfollowed 
byanOverallQuestionTime.
Themixed-initiativenatureofthesystemcan
bemodelledinafinitestatediagram,allowingfor 
a) briefing-like system ‘pushes’, b) confirmation
queriesfromthesystemandc)questionsfromthe
user.However,becausethesystemisprimarily
agent-based, the dialogue can also evolve
dynamically.Forinstance,oncethesystemisina 
‘question’ state,  the dialogue flow then allows
userstoaskanumberofquestions,untiltheyare
satisfied,andthedialoguecanmovetoadifferent 
state.
Each of the top level dialogue states also
corresponds to an IS agent with its own set of
ATTITUDE routines.  These agents register with
Conductor and act as experts in their particular
fields(e.g.,theLegalIssuesadviser).Theagent s
contain knowledge which they use to answer
questionsposedtothemby Conductor.Allagents
havetheability tokeeptrackofwhichstate(or
topic)theyarein.Thisallowsnotonly Conductor,
butalsotheotherdialogueagents,todistinguish
betweenprovidingtheuserwithnewinformation
orinformationthathasalreadybeenpresented.
  
2ThisistheCommander’sinitialguidancetotheTh eatre
PlanningGroup(TPG),whichispartoftheMission Analysis
sectionofJMAP.
8.3 KnowledgeRepresentation
Thecurrentontologydevelopedforthisapplication 
is only a small part of the larger Knowledge
Representationontologytobeusedthroughoutthe
whole FOCAL system.  For now, we only
representtheconceptsneededinoursmalldomain,
and their relationships are translated into
ATTITUDE statements, allowing agents to draw
inferences.Forexample,ifausercanaskthe
questiongivenin(7.a),itwillbetranslatedint o
thelistofattributevaluepairsgivenin(7.b)a nd
sent to Speaker. Speaker then translates these
attributevaluepairsintotheA TTITUDEexpression
in(7.c)andforwardsitonto Conductor.

(7.a) Whatdepartmentoverseesnegotiations
withunionsandindustry?
b. [questionwhatquestion,concept
negotiation,attributeoversee,obj1department]
c. conductordesire(comm_act(negotiation
oversee?department)fromspeakertype
whatquestionin-response-tonull)

Asdescribedinsection6,when Conductorposes
thequestiontotheappropriateagents,theyrespon d
with the information in their knowledge base or
information they can extract from a database.
Agentsstoreknowledgeas believestatementssuch
astheoneshownin(8):

(8) Ibelieve(negotiationoversee“department
ofworkplacerelations”)

These believestatementsarethenunifiedwiththe
propositions translated by Speaker, and if
unification is successful, a reply is sent back to
Conductor.Finally, Conductorpassestheanswer
on to NLG to matchatemplateand producean
Englishanswer,forinstance(9).

(9) TheDepartmentofWorkplaceRelations
overseesnegotiationswithunionsandindustry.

Anagentwhichhasaccesstoadatabasecanalso
translate a user's question into the relevant
databasequerytoobtaintheanswer.Animportant
issue under research concerns the automatic
derivation of A TTITUDE statements from a pre-
existingdatabase.

8.4 SeveraldifferentVAs
As explained above, each stage of the planning
processispresentedtotheuserbyaparticularVA 
withitsassociated IS agentandtheVAthenallows
users to ask further questions. Besides their
specialisedknowledge,theVAsaredifferentiated
through different head models, different TTS
voices(maleorfemale,differentregionalaccents) 
anddifferentpersonalities.
Onceadialoguestateiscompletedandtheuser
has no further questions, the VA for that state
sendsamessageto Conductortomovetothenext
state. Conductorcantheninitiatethechangein
recognitiongrammar, voiceforthenextVAand
modelforthenextVAhead.
Having several VAs coming on at different
stagestopresentdifferentinformationallowsfor a
VAtobespecialisedinaparticulardomain,just
asrealbriefingofficersareduringarealmilitar y
planningexercise.
Fornow,weonlydisplayoneVAatatime,but
weintendtoexperimentwithhavingmultipleVAs
atthesametime.Thefinalstateofthedialogue
flowallowsuserstoaskquestionsaboutanyaspect 
of the planning process, and questions can be
posedtoalltheVAs,soitwouldbenaturalforth e
userstoseealltheVAsatthatstage.
8.5 RapidPrototypingandEvaluation
Thekeyword versiondevelopedpreviously(see
Broughtonetal.,2002)hasbeenmaintainedasa
rapidprototypingenvironmentforevaluatingnew
scriptsanddialogues.Itallowsnewdialoguesto
bequicklytestedbyenteringsuitablekeywords,
sufficient to discriminate one question from
another.Thissystemprovesfasterfortestingtha n
the more precise method of grammar building.
Multiple response strings can be generated,
providing more naturalness for those interacting
with the VAs on a regular basis.  By rapidly
prototypingquestionsandresponses,wecantest
the intuitiveness of expected questions and the
smoothness and timeliness of responses,
particularly when presented combined with
multimedia.
The implemented system described here has
sofaronlybeentestedwithothermembersofthe
group,butdemonstrationstovisitorsandpotential 
users will provide a more rigorous  form of
evaluation onanon-going basis.  An evaluation
phasefortheprojectisscheduledfor2003-2004,
during which time we will have access to more
usersandwillbeabletoconductmorestructured
experiments.
9 NaturalInteractionwithVAs
In addition to the ASR and TTS systems
previously discussed, other technologies can be
combined into the overall system to increase
naturalnessofinteraction,andweareinvestigatin g
speakerrecognitionaswellasarangeofpointing
technologies.
Theneedforaspeakerrecognitionsystemhas
emergedwiththemovetoaspeakerindependent
ASR.WithaspeakerdependentASR,userswould
load their individual profile before use, thus
enabling the system to know who was using it.
With a speaker-independent ASR, a speaker
recognition system would allow the VAs to
recognisewhoistalkingtothemandenablethem
to address known users by name.  We plan to
integrate within FOCAL the speaker recognition
system which has been developed at DSTO
(Roberts, 1998).  This system uses statistical
modelling techniques and is capable of both
speaker identification (recognising users from a
database of stored speech profiles) and speaker
verification(verifying theidentityofaparticula r
user).
We are also proposing to use pointing
techniques in combination with the speech and
language technologies to build a multimodal
system.  Multimodal systems were originally
demonstrated by Bolts (1980) and research is
continuingacrossvariedapplications(e.g.,Oviatt 
etal.,2000andGibbonetal.,2000).However,
unlikesystemssuchasMATCH(Johnstonetal.,
2002), where the issue is allowing multimodal
interaction on portable devices with very small
screens, in FOCAL we are concerned with
ensuringthatusersgetthefullbenefitofthever y
large screen and with allowing several users to
interactatadistancefromthescreen.Itisalso 
worth mentioning that, unlike the interactive
systemdescribedin(Rickeletal.,2002),whichi s
concernedwithtraininginamilitaryenvironment,
wearenottryingto simulate acomplete virtual
worldwithembodiedagents.
However, we propose to include traditional
pointingtechnologies,suchasthestandarddesktop 

mouse,throughto3-dimensionaltrackingsystems
for gaze, gesture and user tracking.  This will
involve integrating more complex language
understanding, as information will need to be
derived from both the user's utterance and from
whatisbeingpointedto.Forexample,tointerpre t
an utterance such as (10) uttered while the user
pointstoalocationonamap,weneedtoperform
reference resolution on "this region", and match
thatreferenttotheitembeingpointedat.

(10)Whatdoweknowaboutthisregion?

10 Conclusion
We have now implemented in FOCAL the
infrastructure needed to perform spoken and
multimodaldialoguewithseveralVAs.Thisisof
interestinitself,asitwillallowustocontinue our
research on spoken language understanding and
spokendialoguesystemsandalsotoaddressissues
oflanguagegenerationwhichhavefornowbeen
leftaside.  Already we havebeenableto move
fromarigiddialoguecontrolstructure,withvery
constrainedinput,toamoreflexibleandscalable
control structure allowing real connectivity
betweenagents.
Havingmovedtoaspeaker-independentASR,
andtakingadvantageoftheopensourcenatureof
Regulus, we intend to pursue research issues
regardingrobustprocessingofspokeninput,such
asusinggrammarspecialisationfromacorpusand
devisingtechniquesforignoringpartsoftheinput .
Wehaveimplementedadialoguemanagement
architecture based on A TTITUDE agents which
communicatewitheachotherusingpropositional
attitude expressions.  Other agents can now be
developed to  perform additional functions, in
particulartolaunchthedisplayofothertypesof
informationandtointerpretothertypesofinput. 
This will allow us to explore how spoken
dialogue with VAs can be combined with other
virtual interaction technologies (e.g., gesture,
pointing, gazetracking). Inthisrespect,thenext 
step in our project is the development of a full
fledge MMP agent based on the framework
describedin(ColineauandParis,2003).

However,theworkwehavereportedheremust
also be seen as part of the larger research
programmeundertakenwithinFOCAL.Fromthis
perspective, this work is of interest because it
allowsothermembersoftheHSIgrouptopursue
research in the usability of new technologies to
perform the paradigm shift in command
environments.  In particular, this project is
providing the support for further research into
whether this way of presenting information is
helpfulinan operationalcommandenvironment.
Itallowsustodeviseexperimentstoexplorethe
crucial issue of trust in the information being
presented,andhowthewaytheinformationbeing
presentedcanaffectthattrust.
Integratingspokendialoguewithplanningtools
willalsoallowustoexplorewhetherVAscanhelp
inmilitaryoperationplanning,andhowbesttouse 
thesetools.

Acknowledgements
We wish to thank the Chief of C2D, and the
Directorof InformationSciences Laboratory,for
sponsoring and funding this work. We wish to
acknowledge the work of Paul Taplin in
integrating speech synthesis and lip-
synchronisation, and the work of Benjamin Fry
from the University of South Australia in
developing the Regulus/Nuance grammars.
Finallywewishtothanktheothermembersofthe
HSIgroupinC2Dfortheirconstantandinvaluable
helpwiththeFOCALproject.

References

Ananova.2002. http://www.ananova.com.
E. Andre, T. Rist, and J. Muller. 1998. Integrating 
Reactive and Scripted Behaviours in a Life-Like
Presentation Agent, Proceedings of the Second
International Conference on Autonomous Agents ,
261-268.

Appen.2002. http://www.appen.com.au.
R.A.Bolt.1980."Put-that-there":voiceandgestu reat
the graphics interface . Proceedings of the
SIGGRAPH,July,262-270.

Michael Broughton, Oliver Carr, Dominique Estival,
Paul Taplin, Steven Wark, Dale Lambert.  2002.
"Conversing with Franco, FOCAL’s Virtual
Adviser". Conversation Characters Workshop, 
HumanFactors2002 ,Melbourne,Australia.

SandraCarberryandLynnLambert.1999."AProcess
Model for Recognizing Communicative Acts and
ModelingNegotiationSubdialogues". Computational
Linguistics.25,1,pp.1-53

Justine Cassell. 2000. Embodied Conversational
InterfaceAgents, CommunicationsoftheACM ,Vol.
43,No.4,70-78.

NathalieColineauandCécileParis.2003. Framework
fortheDesignofIntelligentMultimediaPresentati on
Systems: An architecture proposal for FOCAL.
CMISTechnicalReport03/92,CSIRO,May2003. 

Dominique Estival. 2002. "The Syrinx Spoken
LanguageSystem". InternationalJournalof  Speech
Technology.vol.5.no.1.pp.85-96.

Michael Johnston, Srinivas Bangalore, Gunaranjan
Vasireddy, Amanda Stent, Patrick Ehlen, Marilyn
Walker, Steve Whittaker, Preetam Maloor. 2002.
"MATCH:anArchitectureforMultimodalDialogue
Systems". Proceedingsofthe40thAnnualMeetingof
the Association for Computational Linguistics
(ACL'02).pp.376-383.Philadelphia..

DafyddGibbon,IngeMertins,RogerK.Moore(Eds.). 
2000.  Handbook of Multimodal and Spoken
Dialogue Systems: Resources, Terminology and
ProductEvaluation. KluwerAcademicPublishers.

Global InfoTek Inc. 2002. Control of Agent Based
Systems.  http://coabs.globalinfotek.com.

JoelGould.2001."ImplementationandAcceptanceof 
NatLink,aPython-BasedMacroSystemforDragon
NaturallySpeaking", TheNinthInternationalPython
Conference,March5-8,California 

Martha L. Kahn and Cynthia Della Torre Cicalese.
2001. "CoABS Grid Scalability Experiments".
ProceedingsoftheSecond International Workshop
on Infrastructure for Agents, MAS, and Scalable
MAS, AutonomousAgents2001Conference.

Dale A. Lambert and Mikael G. Relbe.  1998.
"Reasoning with Tolerance".  2nd International
Conference on Knowledge-Based Intelligent
ElectronicSystems .IEEE.pp.418-427.

DaleA.Lambert.1999."AdvisersWithA TTITUDEfor
Situation Awareness". Proceedings of the 1999 
Workshop on Defence Applications of Signal
Processing. pp.113-118, Edited A. Lindsey, B.
Moran, J. Schroeder, M. Smith and L. White.
LaSalle,Illinois.

Dale A. Lambert. 2003. "Automating Cognitive
Routines", accepted for publication in the 6th
InternationalConferenceonInformationFusion. 

R.Moore,J.Dowding,H.Bratt,J.Gawron,Y.Gorfu ,
A. Cheyer. 1997. "CommandTalk: A spoken-
language interface for battlefield simulations". In 
Proceedings of the Fifth Conference on Applied
NaturalLanguageProcessing,pp1-7. 
Nuance.2002.http://www.nuance.com/ .

Oviatt,S.,Cohen,P.,Wu, L.,Vergo,J.,Duncan, L .,
Suhm, B., Bers, J., Holzman, T., Winograd, T.,
Landay,J.,Larson,J.,Ferro,D.2000."Designing the
userinterfaceformultimodalspeechandpen-based
gesture applications: state-of-the-art systems and
future research directions". Human Computer
Interaction.

RashmiPrasadandMarilynWalker.2002."Traininga 
Dialogue Act Tagger for Human-Human and
Human-ComputerTravelDialogues". Proceedingsof
3rdSIGDIALWorkshop .Philadelphia.pp.162-173.

Manny Rayner, John Dowding, Beth Ann Hockey.
2001. "A Baseline method for compiling typed
unification grammars into context free language
models". In Proceedings of Eurospeech 2001, pp
729-732.Aalborg,Denmark.

Manny Rayner, John Dowding, Beth Ann Hockey.
2002a."RegulusDocumentation".
Manny Rayner, Beth Ann Hockey, John Dowding.
2002b.  "Grammar Specialisation meets Language
Modelling". ICSLP2002. Denver.

Manny Rayner and Pierrette Bouillon.  2002. "A
FlexibleSpeechtoSpeechPhrasebookTranslator".
Proceedings of the ACL-02 Speech-Speech
TranslationWorkshop ,pp69-76.

JeffRickel,StacyMarsella,JonathanGratch,Randa ll
Hill,DavidTraum,WilliamSwartout.2002.Toward
aNewGenerationofVirtualHumansforInteractive
Experiences. IEEE Intelligent Systems, 1094-7167,
pp.32-38.

William Roberts. 1998. "Automatic Speaker
Recognition Using Statistical Models". DSTO
ResearchReport,DSTO-RR-0131 ,DSTOElectronics
andSurveillanceResearchLaboratory.
rVoice. 2002. Rhetorical Systems,
http://www.rhetoricalsystems.com/rvoice.html.

Paul Taplin,Geoffrey Fox,Michael Coleman, Steven
Wark, Dale Lambert. 2001.  "Situation Awareness
UsingaVirtualAdviser", TalkingHeadWorkshop,
OzCHI2001 ,Fremantle,Australia.

Helen Wright. 1998. "Automatic utterance type
detection using suprasegmental features".
Proceedingsofthe5thInternationalConferenceon
SpokenLanguageProcessing(ICSLP'98).Sydney.

