In this work, the task is to select facial displays for an animated talking head to use while presenting output in the COMIC multimodal dialogue system (Foster et al., 2005), which generates spoken descriptions and comparisons of bathroom-tile options.
P05-3012
In another study (Foster and Oberlander, 2007), two different data-driven strategies were implemented that used the corpus data to select facial displays to accompany speech.
W07-1901
Foster. 2007b.
W07-1901
Similarly, van Deemter et al.(2006) created a corpus of multimodal referring expressions produced in specific pragmatic contexts and used it to compare several referring-expression generation algorithms to human performance.
W06-1420
Rather than adding this information to an existing corpus, we chose—like Stone et al.(2004) and van Deemter et al.(2006), for example—to create a corpus based on known contexts so that the full information for every sentence was known before the fact.
W06-1420
Foster. 2007a.
W07-1901
Two studies (Foster, 2007b) were carried out to test the generality of the characteristic positive and negative displays (Figure 4).
W07-1901
In a previous study using the same video recordings but a different, simpler scheme (Foster and Oberlander, 2006), facial displays could only be associated with single leaf nodes (i.e., words); that is, in the terminology of Ekman (1979), all motions were considered to be batons rather than underliners.
E06-1045
Two further human evaluation studies compared the weighted data-driven generation strategy from the preceding study to a rule-based strategy that selected the most characteristic displays based only on the user-model evaluation (Foster, 2007a).
W07-1901
