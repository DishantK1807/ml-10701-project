Another type of WSD method uses supervised machine learning (ML) technolo41 gies (Bruce and Wiebe, 1994; Lee and Ng, 2002; Liu et al., 2002).
W02-1006
For the fine-grained All Words sense tagging task, which has always used WordNet, the system performance has ranged from our 59% to 65.2 (Senseval3, (Decadt et al., 2004)) to 69% (Seneval2, (Chklovski and Mihalcea, 2002)).
W04-0827
Briefly, after ensuring the corpora were sentence-aligned, we tokenized the English texts and performed word segmentation on the Chinese texts (Low et al., 2005).
I05-3025 I05-3025
Och. 2000.
P00-1056
In (Och and Ney, 2000a), the word null is introduced to generate the French words that don't align to any English words.
P00-1056
Alignments of both directions are generated and then are combined by heuristic rules described in (Och and Ney 2000b).
P00-1056
System Following the approach of (Lee and Ng, 2002), we train an SVM classifier for each word using the knowledge sources of local collocations, parts-ofspeech (POS), and surrounding words.
W02-1006
The classical approaches to word alignment are based on IBM models 1-5 (Brown et al., 1994) and the HMM based alignment model (Vogel et al., 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax based approaches (Zhang and Gildea, 2005) for word alignment are also studied.
P00-1056
Text Research in (Ng et al., 2003; Chan and Ng, 2005) has shown that examples gathered from parallel texts are useful for WSD.
P03-1058
For both systems, we performed supervised word sense disambiguation based on the approach of (Lee and Ng, 2002) and using Support Vector Machines (SVM) as our learning algorithm.
W02-1006
Note that contrary to the specifications, this team returned WordNet 2.1 senses, so we had to map automatically to 1.6 senses (Daude et al., 2000).
P00-1064
To gather examples from these parallel corpora, we followed the approach in (Ng et al., 2003).
P03-1058
also tested "self-training with bagging", which Ng and Cardie (2003) used for co-reference resolution.
P03-1058
In order to improve transition models in the HMM based alignment, Och and Ney (2000a) extended the transition models to be word-class dependent.
P00-1056
Och and Ney (2000a) have suggested estimating word-class based transition models so as to provide more detailed transition probabilities.
P00-1056
(Radev et al., 2000; Ng et al., 2001; Harabagiu et al., 2003).
P03-1058
Lee, Y.K., & Ng, H.T. 2002.
W02-1006
Hence, besides gathering examples from the widely used SEMCOR corpus, we also gathered training examples from 6 English-Chinese parallel corpora and the DSO corpus (Ng and Lee, 1996).
P96-1006
Previous best results: FIJZ03 (Florian et al., 2003), CN03 (Chieu and Ng, 2003), KSNM03 (Klein et al., 2003).
P03-1058
For this, we rely primarily on the WordNet sense mappings automatically generated by the work of (Daude et al., 2000).
P00-1064
Finally, we further smooth transition probabilities with a uniform distribution as described in (Och and Ney, 2000a), __ 1 (|,) (1 )(|,) jj jj p aa I paa I I ??=??????.
P00-1056
Corpus Besides SEMCOR, the DSO corpus (Ng and Lee, 1996) also contains manually annotated examples for WSD.
P96-1006
Using this annotation, we report the word alignment performance in terms of alignment error rate (AER) as defined by Och and Ney (2000a): |||| 1 |||| A SAP AER AS ????
P00-1056
Further, the best performing system in SENSEVAL-3 English all-words task (Decadt et al., 2004) used training data gathered from multiple sources, highlighting the importance of having a large amount of training data.
W04-0827
and Test Data from Different Documents In our previous work (Ng et al., 2003), we conducted experiments on the nouns of SENSEVAL-2 English lexical sample task.
P03-1058
Briefly, training a Hiero model proceeds as follows: ??GIZA++ (Och and Ney, 2000) is run on the parallel corpus in both directions, followed by an alignment refinement heuristic that yields a many-to-many alignment for each parallel sentence.
P00-1056
Franz J. Och and Hermann Ney. 2000. Improved statis-
P00-1056
Tests sentencepairs were manually aligned and were marked with both sure and possible alignments (Och and Ney 2000a).
P00-1056
This transformation searches an adequate monotonic segmentation for each of the m source-target language pairs on the basis of bilingual alignments such as those given by GIZA++ (Och, 2000).
P00-1056
In this paper, we present improvements to the HMM based alignment model originally proposed by (Vogel et al., 1996, Och and Ney, 2000a).
P00-1056
The SEMCOR corpus (Miller et al., 1994) is one of the few currently available, manually senseannotated corpora for WSD.
H94-1046
In order to evaluate, we had to map automatically all WSD results to the respective WordNet version (using the mappings in (Daude et al., 2000) which are publicly available).
P00-1064
The idea of using supervised machine learning for WSD is not new and was used for example in (Ng and Lee, 1996).
P96-1006
It is widely used by various systems which participated in the English all-words task of SENSEVAL-2 and SENSEVAL-3, including one of the top performing teams (Hoste et al., 2001; Decadt et al., 2004) which had performed consistently well in both SENSEVAL allwords tasks.
W04-0827
Corpora We gathered training examples from parallel corpora, SEMCOR (Miller et al., 1994), and the DSO corpus.
H94-1046
Local Collocations We adopt the same 11 collocation features as (Lee and Ng, 2002), namely C??,??, C1,1, C??,??, C2,2, C??,??, C??,1, C1,2, C??,??, C??,1, C??,2, and C1,3.
W02-1006
Examples from Parallel Corpora To gather examples from parallel corpora, we followed the approach in (Ng et al., 2003).
P03-1058
In the next section we briefly review modeling of transition probabilities in a conventional HMM alignment model (Vogel et al., 1996, Och and Ney, 2000a).
P00-1056
HMM alignment model We briefly review the HMM based word alignment models (Vogel, 1996, Och and Ney, 2000a).
P00-1056
We also trained IBM model 4 using GIZA++ provided by Och and Ney (2000c), where 5 iterations of model 4 training was performed after 5 iterations of model 1 plus 5 iterations of HMM.
P00-1056
Fig. 1 only presents AER results that are calculated after combination of word alignments of both E??F and F??E directions based on a set of heuristics proposed by Och and Ney (2000b).
P00-1056
ISL Phrase-Based MT System 2.1 Word and Phrase Alignment Phrase-to-phrase translation pairs are extracted by training IBM Model-4 word alignments in both directions, using the GIZA++ toolkit (Och and Ney, 2000), and then extracting phrase pair candidates which are consistent with these alignments, starting from the intersection of both alignments.
P00-1056
Algorithm SVM is an effective learning algorithm to WSD (Lee and Ng, 2002).
W02-1006
Syntactic Relations We adopt the same syntactic relations as (Lee and Ng, 2002).
W02-1006
We then made use of the GIZA++ software (Och and Ney, 2000) to perform word alignment on the parallel corpora.
P00-1056 P00-1056
Our past research in (Ng et al., 2003; Chan and Ng, 2005) has shown that examples gathered from parallel texts are useful for WSD.
P03-1058
As described in (Ng et al., 2003), when several senses of an English word are translated by the same Chinese word, we can collapse these senses to obtain a coarser-grained, lumped sense inventory.
P03-1058
ZA++ (Och and Ney, 2000) on the training corpus 
P00-1056
Our SVM classifier (using SVMlight) follows that of (Lee and Ng, 2002), which ranked the third in Senseval-3 English lexical sample task.
W02-1006
The feature set used here is as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations.
W02-1006
