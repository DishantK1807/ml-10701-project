Modeling with Structures in Statistical Machine Translation 
Ye-Yi Wang and Alex Waibel 
School of Computer Science 
Carnegie Mellon University 
5000 l:brbes Avenue 
Pittsburgh, PA 152113, USA 
{yyw, waibel}©cs, cmu. edu 
Abstract 
Most statistical machine translation systems 
employ a word-based alignment model. In this 
paper we demonstrate that word-based align: 
ment is a major cause of translation errors. We 
propose a new alignment model based on shal
low phrase structures, and tile structures can 
be automatically acquired from parallel corpus. 
This new model achieved over 110% error reduc
tion for our st)oken language translation task. 
1 Introduction

Most (if not all) statistical machine translation 
systems employ a word-based alignment model 
(Brown et al., 1993; Vogel, Ney, and Tilhnan, 
1996: Wang and Waibel, 1997), which treats 
words in a sentence as independent entities and 
ignores the structural relationship among them. 
While this independence assumption works well 
in speech recognition, it poses a major problem 
ill our experiments with spoken language trails
lation between a language pair with very dif
ferent word orders. In this l)aper we propose a 
translation model that employs shallow phrase 
structures. It ha~s the following adwntages over 
word-based alignment: 
• Since tile tr~tnslation model can directly de
pict phrmse reordering in translation, it is 
more accurate for translation between lan
guages with different word (phrase) orders. 
• The decoder of the translation system can 
use the phrase information and extend 
hyf)othesis by phrases (multiple words), 
therefore it can speed ut) decoding. 
The paper is organized as follows. In sec
tion 2, tile problems o/' word-based alignment 
models are discussed. To alienate these prot)
lems, a new alignment model based on shal
low l)hrase structures is introduced in section 
3. In section d, a grammar inference algorithm 
is presented that can automatically acquire the 
t)hra.se structures used in the new model. Trans
l~tion performance is then evaluated in sec
tion 5, and conchtsions are presented in sec
tion 6. 
2 Word-based Alignment Model 
lit a word-based alignment translation model, 
tile transformation from a sentence at the source 
end of a. communica.tion channel to a sentence 
at the ta.rget end can be described with tile fol
lowing random process: 
1. Pick a length {'or the sentence at tile target 
end. 
2. For each word position in the target sen
temce, align it with a source word. 
3. Produce ~ word at each target word po
sition according to the source word with 
which the target word position has been 
aligned. 
IBM Alignment Model 2 is ~ typical example 
of word-based alignment. Assuming a sentence 
s = Sl,"', sl at the source of a channel, the 
model picks a length nz of the target sentence 
t according to the distribution P(m I s) = e, 
where e is a small, fixed number. Then for each 
posit;ion i (0 < i < m) in t, it finds its corre
sponding 1)osition a i ill S according to an aliqn
merit distribution I)(ai\[i, a~ -1, re, s)= a(a/ I 
i, ~n,l). Finally, it generates a word ti at the 
position i of t from the source word s~, at the 
aligned position ai, according to a translatio~ 
distribution l'(ti l t~-l,ai",s) = t(ti l s~,). 
1357 
waere denn Mot, tag der sech und zwanzigste Juli moeglich 
it "s going to difficuhy to find meeting time 1 think is Monday the twenty sixth of July possible 
waere denn Montag der sech und zwanzigste Juli moeglich 
C_ 
it's going to difficulty to find meeting time 1 think is Monday the twenty sixth of July possible 
Figure 1: Word Alignment with deletion in translation: the top alignment is the one made by IBM 
Alignment Model 2, the bottom one is the 'ideal' alignment. 
fiter der zweiten Terrain im Mai koennte ich den Mittwoch den fiwnf und zwanzigsten onbieten %. 
I could offer ~ou Wednesday the twenty fifth for the second date in May 
fi~er der zweiten Terrain im Mai koennte ich den Mittwoch den fitenf und zwanzigsten anbieten 
1 could offer you Wednesday the twenty fifth for the second date in May 
Figure 2: Word Alignment of translation with different phrase order: the top alignment is the one 
made by IBM Alignment Model 2, the bottom one is the 'ideal' alignment. 
filer der zweiten Terrain im Mai koennte ich den Mittwoch den fuenf und zwanzigsten anbieten 
J 
1 could offer you Wednesday the twenty fifth for the second date in May 
Figure 3: Word Alignment with Model 1 for one of the previous examples. Because no Mignment 
probability penalizes the long distance phrase reordering, it is much closer to the 'ideal' alignment. 
1358 
Therefore, P(tls ) is the sum of tile proba
bilities of generating t fi'om s over all possible 
alignments A, in which the position i in t is 
aligned with the position ai in s: 
P(t Is) 
l l 77l 
"'" Ht(tJ\[,%)a(ailj, l,m) 
al=O a,n=0 j=l 
fi' e ~ t(tj I si)a(ilj, l, m) (1) 
j=l i=0 
A word-ba.sed model may have severe prob
lems when there are deletions in translation 
(this may be a result of erroneous sentence 
alignment) or the two languages have different 
word orders, like English and German. Figure 1 
and Figure 2 show some problematic a.lignments 
between English/German sentences made by 
IBM Model 2, together with the 'ideal' align~ 
ments for the sentences, tIere the alignment 
paranleters penalize the alignment of English 
words with their German translation equiva
lents because the translation equiwdents are far 
away tYom the words. 
An ext)eriment reveals how often this ldnd 
of "skewed" alignment hat)pens in our 1.2n
glish/(~erman scheduling conversation parallel 
corpus (Wang and Waibel, 1997). The ex
periment was based on the following obser
vation: IBM translation Model 1 (where the 
alignment distribution is uniform) and Model 
2 found similar Viterbi alignments when there 
were no movements or deletions, and they pre
dicted very different Viterbi alignnlents when 
the skewness was severe in a sentence pair, since 
the alignment parameters in Model 2 penalize 
the long distance alignment. Figure 3 shows the 
Viterbi alignment discovered by Model 1 for the 
same sentences in Figure 21 . 
We memsured the distance of a Model 1 
alignment a 1 and a Model 2 alignment a 2 
as ~!g_l 1 \[a~a~\[. To estimate the skew
ness of the corpus, we collected the statistics 
about the percentage of sentence pairs (with at 
1The better alignment on a given pair of sentences 
does not nlean Model 1 is a better model. Non-uniform 
alignment distribution is desirable. Otherwise, language 
model would be the only factor that determines the 
source sentence word order in decoding. 
o 2 
0a 
30 25 
20 
15 0tl5 
0 .... 
0 0.5 
Alignment distance > x * target sentence length 
/ 
J 
k 
1 1.5 2 2.5 
Figure ~1: Skewness of Translations 
least live words in a sentence) with Model 1 
and Model 2 alignment distance greater than 
1/4,2/4,3/4,..., 10/4 of the target sentence 
length. By checking tile Viterbi alignments 
made by both models, it is almost certain that 
whenever the distance is greater that 3/4 of the 
target sentence length, there is either a move
ment or a deletion in the sentence pair. Fig
ure .'1 plots this statistic around 30% of the 
sentence l)airs in our training data have solne 
degree of skewness in aligulnents. 
3 Structure-based Alignment Model 
To solve tile problems with the word-based 
alignment models, we present a structure-based 
alignment model here. The idea is to di
rectly model the phrase movement with a rough 
alignment, and then model the word alignment 
within phrases with a detailed alignment. 
Given an English sentence e = ele2...¢l, its 
(~erman transla.tion g = .qlg2 "".(1,,, can be gen
erated by the following process: 
1. Parse e into a sequence of phrases, so 
\]': =: ((~11,C12,''',CIIt)(C. 21,C22,''',¢~212) "'" 
c,,n) 
= EoE1172 • • • t'2,~, 
where Fo is a null phrase. 
2. With tile probability P(q I e,E), deter
mine q < n + \], tile number of phrases in 
g. Let GI'..G, 1 denote these q phrases. 
Each source phrase can be aligned with at 
most one target; phrase. Unlike English 
phrases, words in a German phrase do not 
1359 
have to form a consecutive sequence. So 
g may be expressed with something like 
g = gllg12g21g13g22"", where gij repre
sents the j-th word in the i-th phrase. 
3. Foreach German phrase Gi, O << i < q, with 
the probability P(ri I i, r 0i-1, E, e), Mien it 
with an English phrase E~,. 
4. For each German phrase Gi, 0 <_ i < q, de
termine its beginning position bi in g with 
i-1FOq, e, E). tile distribution P(bi I i, b o , 
5. Now it is time to generate tile individual 
words in the German phrases through de
tailed alignment. It works like IBM Model 
4. For each word eij in the phrase El, 
its fertility ~)ij has the distribution r(¢ij I j-1 i-1 E). 
i,j,¢il ,¢o ,b~,rg, e, 
6. For each word eij in the phrase El, it gen
erates a tablet vii = {Tijl,Yij2," "'Tij¢i3 } 
by generating each of the words in vii 
in turn with the probability lP(rijk I 
r~;', r{(-', r(~ -1, Ct0, be, re, e, E) for the k-th 
word in the tablet. 
7. For each element r ijk in the tablet 
rij, the permutation 7rij k determines 
its position in the target sentence ac
cording to the distribution P(rrij k \] 
~k-lijl ,Tr'ilJ-l,7"/Oi-l,4,~/0,bq ?.q e,~). 
We made the following independence assump
tions: 
1. The number of target sentence phrases de
pends only on the number of phrases in the 
source sentence: 
P(q I e, E) = p, (q I n) ,i-1 E, e) 2. P(ri l i,*o , 
: I i) × 1-10<j< (1 ,'j) 
where 6(x,y) = 1 when x = y, and 
6(x, y) = 0 otherwise. 
This assumption states that P(ri I 
i-1 E,e) depends on i and rl. It also i, r 0 , 
depends on r~ -1 with the factor I-\[0<j<i(1
6(ri, rj)) to ensure that each Englis~ phrase 
is aligned with at most one German phrase. 
3. The beginning position of a target phrase 
depends on its distance from the beginning 
position of its preceding phrase, as well as 
. 
. 
the length of the source phrase aligned with 
the preceding phrase: 
P(bi I '," boi-l,r q,e,\]':) 
= oz(bi-bi-I I l) = I I) 
The fertility and translation tablet of a 
source word depend on the word only: 
q ' q E) P(¢ij { i, j, ¢~11, ¢~-1, b0 ' r0 ' e, 
= 
/9(T/J k I T{~71 , 7"J? 1 ' 4 -1' QSg, be, 7'g, e, 1~) 
= t( ijk levi) 
The leftmost position of the translations of 
a source word depends on its distance fi'om 
the beginning of the target phrase aligned 
with the source phrase that contains that 
source word. It also depends on the iden
tity of the phrase, and the position of the 
source word in the source phrase. 
J-l,Tr; 1,4 , l q ,'~),e,m) P(Trijl I 7ril "¢o, bo, 
= dl (TCijX -hi\[ Ei, j) 
For a target word rijv other than the left
most Tij 1 in tile translation tablet; of tile 
source eij, its position depends on its dis
tance from the position of another tablet 
word rij(k_ 0 closest to its left, the class of 
the target word rijk, and the fertility of the 
source word eij. 
i-1 l l I)(rcijk l Tr~jl 1 , 7/'31 1,71-0 , T6,~)o, bg , ,o,e,'q E) 
= d2( rijk %(k-,) I g(r jk), ¢gj) 
here G(g) is tile equivalent class for g. 
3.1 Parameter
Estimation 
EM algorithm was used to estimate the seven 
types of parameters: p~, a, a, ¢, r, dl and 
d2. We used a subset of probable alignments 
in the EM learning, since the total number of 
alignments is exponential to the target sentence 
length. The subset was the neighboring align
ments (Brown et al., 1993) of the Viterbi align
ments discovered by Model 1 and Model 2. We 
chose to include the Model 1 Viterbi alignment 
here because the Model 1 alignment is closer 
to the "ideal" when strong skewness exists in a 
sentence pair. 
4 Finding
the Structures 
It is of little interest for the structure-based 
alignment model if we }lave to manually find 
1360 
the language structures and write a grammar 
for them, since the primary merit of statistical 
machine translation is to reduce human labor. 
In this section we introduce a grammar infer
ence technique that finds the phrases used in the 
structure-based alignment model. It is based on 
the work in (Ries, Bu¢, and W~Lng, 1995), where 
the following two operators are used: 
. 
. 
Clustering: Clustering words/phrases 
with similar meanings/grammatical func
tions into equivalent classes. The mutual 
information clustering algorithm(Brown et 
el., 1992) were used for this. 
Phrasing: The equivalent class sequence 
cr, c2,'..ck forms a phrase if' 
\]9(CI,C2,'''Ck) log \])(C1'C2'''" ck) 2> O, 
where 0 is a threshold. By changing the 
threshold, we obtain a ditferent number of 
phrases. 
The two operators are iteratively apl)lied to 
the training corpus in alternative steps. This 
results in hierarchical phrases in the form of se
quences of equivalent classes of words/phrases. 
Since the algorithm only uses a monolin
gual corpus, it often introduces some language
specific structures resulting from biased usages 
of a specific language. In machine transla
tion we are more interested in cross-linguistic 
structures, similar to the case of using interlin
gua to represent cross-linguistic information in 
knowledge-based MT. 
To obtain structures that are common in both 
languages, a bilinguM mutuM information clus
tering algorithm (Wang, Lafferty, and Waibel, 
1996) was used as the clustering operator. It 
takes constraints from parallel corpus. We also 
introduced an additional constraint in cluster
ing, which requires that words in the same class 
must have at least one common potential part
of-speech. 
Bilingual constraints are also imposed on the 
phrasing operator. We used bilingual heuris
tics to filter out the sequences acquired by the 
phrasing operator that may not be common in 
multiple languages. The heuristics include: 
. 
. 
Average Translation Span: Given a 
phrase candidate, its average translation 
span is the distance between the left,nest 
and the rightmost target positions aligned 
with the words inside the candidate, av
eraged over all Model 1 Viterbi alignments 
of sample sentences. A candidate is filtered 
out if its ~werage tr~mslation span is greater 
than the length of the candidate multiplied 
by a threshold. This criterion states that 
the words in the translation of a phrase 
have to be close enough to form a phrase 
in another language. 
Ambiguity Reduction: A word occur
ring in a phrase should be less ambiguous 
than in other random context. Therefore 
a phrase should reduce the ambiguity (un
certainty) of the words inside it. -t'br each 
source language word class c, its translation 
entropy is defined as ~0t(glc)log(g I c). 
'Fire average I)er source class entropy re
duction induced by tire introduction of a 
phrase P is therefore 
1 \]P\[ ~-~" \[~-~" t(g \[ c)logt(g I c ) 
tED 9 
~_ t,(.q It., I )) logt(o \[ c, P)\] 
9 
A threshold was set up for minimum en
tropy reduction. 
By applying the clustering operator followed 
with the phrasing operator, we obtained shallow 
phrase structures partly shown in Figure 5. 
Given a set of phrases, we can deterministi
tally parse a. sentence into a sequence of phrases 
by replacing the leftmost unparsed substring 
with the longest matching phrase in the set. 
5 Evaluation
and Discussion 
We used the Janus English/German schedul
ing corpus (Sutnn et al., 1995) to train our 
phrase-based alignment model. Around 30,000 
parallel sentences (400,000 words Mtogether for 
both languages) were used for training. The 
same data were used to train Simplified Model 
2 (Wang and Waibel, i\[997) and IBM Model 
3 tbr performance comparison. A larger En
glish monolingual corpus with around 0.5 mil
lion words was used for the training of a bigram 
1361 
\[Sunday Monday .\] 
\[Sunday Monday .\] 
\[Sunday Monday .\] 
\[Sunday Monday .\] 
\[Sunday Monday .\] 
\[Sunday Monday .\] 
\[January February. 
\[January February. 
\[January February. 
\[I he she itself\] 
\[eleventh thirteenth...\] 
\[afternoon morning...\] 
\[at by...\] \[one two...\] 
\[the every each...\] \[first second third...\] 
\[the every each...\] \[twenty depending remaining\] 
\[the every each...\] \[eleventh thirteenth...\] 
\[in within...\] \[January February...\] 
..\] \[first second third...\] \[at by...\] 
..\] \[first second third...\] 
..\] \[the every each...\] \[first second third...\] 
\[have propose remember hate...\] 
\[after before around\] \[one two three...\] 
Figure 5: Example of Acquired Phrases. Words in a bracket form a cluster, phrases are cluster 
sequences. Ellipses indicate that a cluster has more words than those shown here. 
Model Correct 
Model 2 284 
Model 3 98 
S. Model 303 
OK Incorrect Accuracy 
87 176 59.9% 
45 57 60.3% 
96 148 64.2% 
Table 1: Translation Accuracy: a correct trans
lation gets one credit, an okay translation gets 
1/2 credit, an incorrect one gets 0 credit. Since 
the IBM Model 3 decoder is too slow, its per
formance was not measured on the entire test 
set. 
ity mass is more scattered in the structure-based 
model, reflecting the fact that English and Ger
man have different phrase orders. On the other 
hand, the word based model tends to align a 
target word with the source words at similar po
sitions, which resulted in many incorrect align
ments, hence made the word translation proba
bility t distributed over many unrelated target 
words, as to be shown in the next subsection. 
5.3 Model
Complexity 
language model. A preprocessor splited Ger
mall compound nouns. Words that occurred 
only once were taken as unknown words. This 
resulted in a lexicon of 1372 English and 2202 
German words. The English/German lexicons 
were classified into 250 classes in each language 
and 560 English phrases were constructed upon 
these classes with the grammar inference algo
rithm described earlier. 
We limited the maximum sentence length to 
be 20 words/15 phrases long, the maximum fer
tility for non-null words to be 3. 
5.1 Translation
Accuracy 
Table i shows the end-to-end translation perfor
mance. The structure-based model achieved an 
error reduction of around 12.5% over the word
based alignment models. 
5.2 Word
Order and Phrase Alignment 
Table 2 shows the alignment distribution for the 
first German word/phrase in Simplified Model 
2 and the structure-based model. The probabil
The structure-based model has 3,081,617 free 
parameters, an increase of about 2% over the 
3,022,373 free parameters of Simplified Model 2. 
This small increase does not cause over-fitting, 
as the performance on the test data suggests. 
On the other hand, the structure-based model 
is more accurate. This can be illustrated with 
an example of the translation probability distri
bution of the English word "I". Table 3 shows 
the possible translations of "I" with probability 
greater than 0.01. It is clear that the structure
based model "focuses" better on the correct 
translations. It is interesting to note that the 
German translations in Simplified Model 2 of
ten appear at the beginning of a sentence, the 
position where 'T' often appears ill English sen
tences. It is the biased word-based alignments 
that pull the unrelated words together and in
crease the translation uncertainty. 
We define the average translation entropy as 
P(ei) ~ -t(gj l e/)logt(gjlei ). 
i=0 j:l 
1362 
J 
aM2(j l l) 
aSM(j \[ 1) 
1 2 3 4 7 =1=s
o.86 0.054 0.025 o.o08 0.005 \] 0.004 \]0.00213.3×i0-'* 
000 a 0.29 0.% o.15 0.0r 0.11 10.05A 4 L0.02 2 
2.9×10 -4 
0.01 
Table 2: The alignment distribution for the first German word/phrase in Simplified Model 2 and 
in tile structure-based model. The second distribution reflects the higher possibility of phrase 
reordering in translation. 
tM2(*l I) tsM(*l I) 
ich 0.708 
da 0.104 
am 0.024 
das 0.022 
dann 0.022 
also 0.019 
es 0.011 
ich 0.988 
reich 0.010 
Table 3: The translation distribution of 'T'. it 
is more uncertain in the word-based alignment 
model because the biased alignment distribu
tion forced the associations between unrelated 
English/German words. 
(m,n are English and German lexicon size.) 
It is a direct measurement of word transla
tion uncertainty. The average translation en
tropy is 3.01 bits per source word in Sin> 
plified Model 2, 2.68 in Model 3, and 2.50 
in the structure(l-based model. 'l?herefore 
information-theoretically the complexity of the 
word-based alignment models is higher than 
that of tile structure-based model. 
6 Conclusions

Tile structure-based alignment directly models 
the word order difference between English and 
German, makes the word translation dist,'ibu
tion focus on the correct ones, hence improves 
translation performance. 
7 Acknowledgements

We would like to thank the anonymous COL
ING/ACL reviewers for valuat)le comments. 
This research was partly supported by ATR and 
the Verbmobil Project. The views and conclu
sions in this document are those of the authors. 
ematics of Statistical Machine Translation: 
Parameter Estimation. Computational Lin
g'uistic8, 19(2):263 311. 
Brown, P. F., V. J. l)ella-Pietra, P. V. deSouza, 
J. C. Lai, and R. I,. Mercer. 1992. Class
Based N-gram Models of Natural Language. 
Computational Linguistics, 18(4):467 479. 
Ries, Klaus, Finn Dag Btm, and Ye
Yi Wallg. 1995. hnproved Language 
Modelling by Unsupervised Acquisi
tion of Structure. In ICASS'I > '95. 
Ih;EE. corrected version available via 
http ://mew. cs. cmu. edu/~2ies/icas sp_9S, html. 
Suhm, B., P.Gcutner, T. Kelnp, A. l,avie, 
1,. Mayfield, A. McNair, 1. Rogina, T. Schultz, 
q'. Sloboda, W. Ward, M. \¥oszczyna, and 
A. Waibel. 1995..JANUS: Towards multilin
gual spoken language translation. In Proceed
in qs of the AHd>A 5);cech 5)~oken Language 
7~chnology H~%rkshop, Austin, ~ILV, 1995. 
Vogel, S., lI. Ney, and C. Tillman. 1996. 
IIMM-Based Word Alignment in Statistical 
Translation. In Proceedings of the Seven
tcenth International Conference on Compu
tational Linguistics: COLING-96, pages 836 
841, Copenhagen, l)enmaI'k. 
Wang, Y., J. LMf'erty, and A. Waibel. 11996. 
Word Clustering with Parallel Spoken Lan
guage Corpora. In Proceedings of the /~th In
ternational (7on fore, rice on Spoken Language 
Processing (IC,5'LP'96), Philadelphia, USA. 
Wang, Y. and A. Waibel. 1997. Decoding Al
gorithm in Statistical Machine Translation. 
In PTvcccdings of thc 35th Annual Meeting 
of thc Association for Computational Lin
guistics and 8th Confcrcncc of the lCuropca~ 
Chaptcr of the Association for Computational 
Linguistics (A CI;/EA CL '97), pages 366-372, 
Madrid, Spain. 

References 

Brown, P. F., S. h. Della-Pietra, V..J Della
Pietra, and R. L. Mercer. 1993. The Math

