Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 46–54,
Uppsala, Sweden, 16 July 2010. c©2010 Association for Computational Linguistics
n-Best Parsing Revisitednull
Matthias B¨uchse and Daniel Geisler and Torsten St¨uber and Heiko Vogler
Faculty of Computer Science
Technische Universit¨at Dresden
01062 Dresden
{buechse,geisler,stueber,vogler}@tcs.inf.tu-dresden.de
Abstract
We derive and implement an algorithm
similar to (Huang and Chiang, 2005) for
finding the n best derivations in a weighted
hypergraph. We prove the correctness and
termination of the algorithm and we show
experimental results concerning its run-
time. Our work is different from the afore-
mentioned one in the following respects:
we consider labeled hypergraphs, allowing
for tree-based language models (Maletti
and Satta, 2009); we specifically handle
the case of cyclic hypergraphs; we admit
structured weight domains, allowing for
multiple features to be processed; we use
the paradigm of functional programming
together with lazy evaluation, achieving
concise algorithmic descriptions.
1 Introduction
In statistical natural language processing, proba-
bilistic models play an important role which can
be used to assign to some input sentence a set of
analyses, each carrying a probability. For instance,
an analysis can be a parse tree or a possible trans-
lation. Due to the ambiguity of natural language,
the number of analyses for one input sentence can
be very large. Some models even assign an infinite
number of analyses to an input sentence.
In many cases however, the set of analyses can
in fact be represented in a finite and compact way.
While such a representation is space-efficient, it
may be incompatible with subsequent operations.
In these cases a finite subset is used as an approx-
imation, consisting of n best analyses, i. e. n anal-
yses with highest probability. For example, this
approach has the following two applications.
(1) Reranking: when log-linear models (Och
and Ney, 2002) are employed, some features may
null This research was financially supported by DFG VO
1101/5-1.
not permit an efficient evaluation during the com-
putation of the analyses. These features are com-
puted using individual analyses from said approx-
imation, leading to a reranking amongst them.
(2) Spurious ambiguity: many models produce
analyses which may be too fine-grained for further
processing (Li et al., 2009). As an example, con-
sider context-free grammars, where several left-
most derivations may exist for the same terminal
string. The weight of the terminal string is ob-
tained by summing over these derivations. The
n best leftmost derivations may be used to approx-
imate this sum.
In this paper, we consider the case where the
finite, compact representation has the form of a
weighted hypergraph (with labeled hyperedges)
and the analyses are derivations of the hypergraph.
This covers many parsing applications (Klein and
Manning, 2001), including weighted deductive
systems (Goodman, 1999; Nederhof, 2003), and
also applications in machine translation (May and
Knight, 2006).
In the nomenclature of (Huang and Chiang,
2005), which we adopt here, a derivation of a hy-
pergraph is a tree which is obtained in the follow-
ing way. Starting from some node, an ingoing hy-
peredge is picked and recorded as the label of the
root of the tree. Then, for the subtrees, one con-
tinues with the source nodes of said hyperedge in
the same way. In other words, a derivation can be
understood as an unfolding of the hypergraph.
The n-best-derivations problem then amounts
to finding n derivations which are best with re-
spect to the weights induced by the weighted hy-
pergraph.1 Among others, weighted hypergraphs
with labeled hyperedges subsume the following
two concepts.
(I) probabilistic context-free grammars (pcfgs).
1Note that this problem is different from the n-best-
hyperpaths problem described by Nielsen et al. (2005), as
already argued in (Huang and Chiang, 2005, Section 2).
46
In this case, nodes correspond to nonterminals,
hyperedges are labeled with productions, and the
derivations are exactly the abstract syntax trees
(ASTs) of the grammar (which are closely related
the parse trees). Note that, unless the pcfg is un-
ambiguous, a given word may have several cor-
responding ASTs, and its weight is obtained by
summing over the weights of the ASTs. Hence,
the n best derivations need not coincide with the
n best words (cf. application (2) above).
(II) weighted tree automata (wta) (Alexandrakis
and Bozapalidis, 1987; Berstel and Reutenauer,
1982; ´Esik and Kuich, 2003; F¨ul¨op and Vogler,
2009). These automata serve both as a tree-based
language model and as a data structure for the
parse forests obtained from that language model
by applying the Bar-Hillel construction (Maletti
and Satta, 2009). It is well known that context-free
grammars and tree automata are weakly equiv-
alent (Thatcher, 1967; ´Esik and Kuich, 2003).
However, unlike the former formalism, the latter
one has the ability to model non-local dependen-
cies in parse trees.
In the case of wta, nodes correspond to states,
hyperedges are labeled with input symbols, and
the derivations are exactly the runs of the automa-
ton. Since, due to ambiguity, a given tree may
have several accepting runs, the n best derivations
need not coincide with the n best trees. As for
the pcfgs, this is an example of spurious ambigu-
ity, which can be tackled as indicated by appli-
cation (2) above. Alternatively, one can attempt
to find an equivalent deterministic wta (May and
Knight, 2006; B¨uchse et al., 2009).
Next, we briefly discuss four known algorithms
which solve the n-best-derivations problem or
subproblems thereof.
null The Viterbi algorithm solves the 1-best-
derivation problem for acyclic hypergraphs. It is
based on a topological sort of the hypergraph.
null Knuth (1977) generalizes Dijkstra’s algorithm
(for finding the single-source shortest paths in a
graph) to hypergraphs, thus solving the case n = 1
even if the hypergraph contains cycles. Knuth as-
sumes the weights to be real numbers, and he re-
quires weight functions to be monotone and supe-
rior in order to guarantee that a best derivation ex-
ists. (The superiority property corresponds to Di-
jkstra’s requirement that edge weights—or, more
generally, cycle weights—are nonnegative.)
null Huang and Chiang (2005) show that the n-
best-derivations problem can be solved efficiently
by first solving the 1-best-derivation problem and
then extending that solution in a lazy manner.
Huang and Chiang assume weighted unlabeled hy-
pergraphs with weights computed in the reals, and
they require the weight functions to be monotone.
Moreover they assume that the 1-best-
derivation problem be solved using the Viterbi
algorithm, which implies that the hypergraph must
be acyclic. However they conjecture that their
second phase also works for cyclic hypergraphs.
null Pauls and Klein (2009) propose a variation
of the algorithm of Huang and Chiang (2005) in
which the 1-best-derivation problem is computed
via an Anull-based exploration of the 1-best charts.
In this paper, we also present an algorithm
for solving the n-best-derivations problem. Ulti-
mately it uses the same algorithmic ideas as the
one of Huang and Chiang (2005); however, it is
different in the following sense:
1. we consider labeled hypergraphs, allowing
for wta to be used in parsing;
2. we specifically handle the case of cyclic
hypergraphs, thus supporting the conjecture of
Huang and Chiang; for this we impose on the
weight functions the same requirements as Knuth
and use his algorithm;
3. by using the concept of linear pre-orders (and
not only linear orders on the set of reals) our ap-
proach can handle structured weights such as vec-
tors over frequencies, probabilities, and reals;
4. we present our algorithm in the framework
of functional programming (and not in that of im-
perative programming); this framework allows to
decribe algorithms in a more abstract and concise,
yet natural way;
5. due to the lazy evaluation paradigm often
found in functional programming, we obtain the
laziness on which the algorithm of Huang and Chi-
ang (2005) is based for free;
6. exploiting the abstract level of description
(see point 4) we are able to prove the correctness
and termination of our algorithm.
At the end of this paper, we will discuss experi-
ments which have been performed with an imple-
mentation of our algorithm in the functional pro-
gramming language HASKELL.
2 The
n-best-derivations problem
In this section, we state the n-best-derivations
problem formally, and we give a comprehensive
47
example. First, we introduce some basic notions.
Trees and hypergraphs The definition of
ranked trees commonly used in formal tree lan-
guage theory will serve us as the basis for defining
derivations.
A ranked alphabet is a finite set Σ (of symbols)
where every symbol carries a rank (a nonnegative
integer). By Σ(k) we denote the set of those sym-
bols having rank k. The set of trees over Σ, de-
noted by TΣ, is the smallest set T such that for
every k null N, σ null Σ(k), and ξ1,...,ξk null T,
also σ(ξ1,...,ξk) null T;2 for σ null Σ(0) we ab-
breviate σ() by σ. For every k null N, σ null
Σ(k) and subsets T1,...,Tk null TΣ we define
the top-concatenation (with σ) σ(T1,...,Tk) =
nullσ(ξ1,...,ξk) null ξ1 null T1,...,ξk null Tknull.
A Σ-hypergraph is a pair H = (V,E) where
V is a finite set (of vertices or nodes) and E null
V nullnullΣnullV is a finite set (of hyperedges) such that
for every (v1 ...vk,σ,v) null E we have that σ null
Σ(k).3 We interpret E as a ranked alphabet where
the rank of each edge is carried over from its label
in Σ. The family (Hv null v null V ) of derivations of H
is the smallest family (Pv null v null V ) of subsets
of TE such that e(Pv1,...,Pvk) null Pv for every
e = (v1 ...vk,σ,v) null E.
A Σ-hypergraph (V,E) is cyclic if there
are hyperedges (v11 ...v1k1,σ1,v1), . . . ,
(vl1 ...vlkl,σl,vl) null E such that vjnull1 occurs
in vj1 ...vjkj for every j null null2,...,lnull and vl occurs
in v11 ...v1k1. It is called acyclic if it is not cyclic.
Example 1 Consider the ranked alphabet Σ =
Σ(0)nullΣ(1)nullΣ(2) with Σ(0) = nullα,βnull, Σ(1) = nullγnull,
and Σ(2) = nullσnull, and the Σ-hypergraph H =
(V,E) where
null V = null0,1null and
null E = null(ε,α,1),(ε,β,1),(1,γ,1),(11,σ,0),
(1,γ,0)null.
A graphical representation of this hypergraph is
shown in Fig. 1. Note that this hypergraph is cyclic
because of the edge (1,γ,1).
We indicate the derivations of H, assuming that
e1,...,e5 are the edges in E in the order given
above:
2The term σ(ξ1,...,ξk) is usually understood as a string
composed of the symbol σ, an opening parenthesis, the
string ξ1, a comma, and so on.
3The hypergraphs defined here are essentially nondeter-
ministic tree automata, where V is the set of states and E is
the set of transitions.
γ α
0 1 γ
σ β
e5
e3
e4
e1
e2
Figure 1: Hypergraph of Example 1.
null H1 = nulle1,e2,e3(e1),e3(e2),e3(e3(e1)),...null
and
null H0 = e4(H1,H1) null e5(H1) where, e. g.,
e4(H1,H1) is the top-concatenation of H1,
H1 with e4, and thus
e4(H1,H1) = nulle4(e1,e1),e4(e1,e2),
e4(e1,e3(e1)),e4(e3(e1),e1),...null .
Next we give an example of ambiguity in hyper-
graphs with labeled hyperedges. Suppose that E
contains an additional hyperedge e6 = (0,γ,0).
Then H0 would contain the derivations e6(e5(e1))
and e5(e3(e1)), which describe the same Σ-tree,
viz. γ(γ(α)) (obtained by the node-wise projec-
tion to the second component). square
In the sequel, let H = (V,E) be a Σ-hypergraph.
Ordering Usually an ordering is induced on the
set of derivations by means of probabilities or,
more generally, weights. In the following, we will
abstract from the weights by using a binary rela-
tion precedesorequal directly on derivations, where we will in-
terpret the fact ξ1 precedesorequal ξ2 as “ξ1 is better than or
equal to ξ2”.
Example 2 (Ex. 1 contd.) First we show how an
ordering is induced on derivations by means of
weights. To this end, we associate an operation
over the set R of reals with every hyperedge (re-
specting its arity) by means of a mapping θ:
θ(e1)() = 4 θ(e2)() = 3
θ(e3)(x1) = x1 + 1 θ(e4)(x1,x2) = x1 + x2
θ(e5)(x1) = x1 + 0.5
The weight h(ξ) of a tree ξ null TE is obtained by
interpreting the symbols at each node using θ, e. g.
h(e3(e2)) = θ(e3)(θ(e2)()) = θ(e2)() + 1 = 4.
Then the natural order null on R induces the bi-
nary relation precedesorequal over TE as follows: for every
ξ1,ξ2 null TE we let ξ1 precedesorequal ξ2 iff h(ξ1) null h(ξ2),
meaning that trees with smaller weights are con-
sidered better. (This is, e. g., the case when calcu-
lating probabilites in the image of nulllogx.) Note
48
that we could just as well have defined precedesorequal with the
inverted order.
Since addition is commutative, we obtain
for every ξ1,ξ2 null TE that h(e4(ξ1,ξ2)) =
h(e4(ξ2,ξ1)) and thus e4(ξ1,ξ2) precedesorequal e4(ξ2,ξ1) and
vice versa. Thus, for two different trees (e4(ξ1,ξ2)
and e4(ξ2,ξ1)) having the same weight, precedesorequal should
not prefer any of them. That is, precedesorequal need not be
antisymmetric.
As another example, the mapping θ could as-
sign to each symbol an operation over real-valued
vectors, where each component represents one
feature of a log-linear model such as frequencies,
probabilities, reals, etc. Then the ordering could
be defined by means of a linear combination of the
feature weights. square
We use the concept of a linear pre-order to cap-
ture the orderings which are obtained this way.
Let S be a set. A pre-order (on S) is a binary
relation precedesorequal null S null S such that (i) s precedesorequal s for ev-
ery s null S (reflexivity) and (ii) s1 precedesorequal s2 and s2 precedesorequal s3
implies s1 precedesorequal s3 for every s1,s2,s3 null S (transi-
tivity). A pre-order precedesorequal is called linear if s1 precedesorequal s2
or s2 precedesorequal s1 for every s1,s2 null S. For instance, the
binary relation precedesorequal on TE as defined in Ex. 2 is a
linear pre-order.
We will restrict our considerations to a class
of linear pre-orders which admit efficient algo-
rithms. For this, we will always assume a lin-
ear pre-order precedesorequal with the following two properties
(cf. Knuth (1977)).4
SP (subtree property) For every e(ξ1,...,ξk) null
TE and i null null1,...,knull we have ξi precedesorequal
e(ξ1,...,ξk).5
CP (compatibility) For every pair e(ξ1,...,ξk),
e(ξnull1,...,ξnullk) null TE with ξ1 precedesorequal ξnull1, . . . ,
ξk precedesorequal ξnullk we have that e(ξ1,...,ξk) precedesorequal
e(ξnull1,...,ξnullk).
It is easy to verify that the linear pre-order precedesorequal of
Ex. 2 has the aforementioned properties.
In the sequel, letprecedesorequalbe a linear pre-order
on TE fulfilling SP and CP.
4Originally, these properties were called “superiority” and
“monotonicity” because they were viewed as properties of
the weight functions. We use the terms “subtree property”
and “compatibility” respectively, because we view them as
properties of the linear pre-order.
5This strong property is used here for simplicity. It suf-
fices to require that for every v ∈ V and pair ξ,ξnull ∈ Hv we
have ξ precedesorequal ξnull if ξ is a subtree of ξnull.
Before we state the n-best-derivations problem
formally, we define the operation minn, which
maps every subset T of TE to the set of all se-
quences of n best elements of T. To this end, let
T null TE and n null nullTnull. We define minn(T) to be
the set of all sequences (ξ1,...,ξn) null Tn of pair-
wise distinct elements such that ξ1 precedesorequal ... precedesorequal ξn and
for every ξ null T null nullξ1,...,ξknull we have ξn precedesorequal ξ.
For every n > nullTnull we set minn(T) = minnullTnull(T).
In addition, we set minnulln(T) = nullni=0 mini(T).
n-best-derivations problem The n-best-
derivations problem amounts to the following.
Given a Σ-hypergraph H = (V,E), a vertex v null
V , and a linear pre-order precedesorequal on TE fulfilling
SP and CP,
compute an element of minn(Hv).
3 Functional
Programming
We will describe our main algorithm as a func-
tional program. In essence, such a program is a
system of (recursive) equations that defines sev-
eral functions (as shown in Fig. 2). As a conse-
quence the main computational paradigm for eval-
uating the application (f a) of a function f to an
argument a is to choose an appropriate defining
equation f x = r and then evaluate (f a) to r’
which is obtained from r by substituting every oc-
currence of x by a.
We assume a lazy (and in particular, call-by-
need) evaluation strategy, as in the functional pro-
gramming language HASKELL. Roughly speak-
ing, this amounts to evaluating the arguments of
a function only as needed to evaluate the its body
(i. e. for branching). If an argument occurs multi-
ple times in the body, it is evaluated only once.
We use HASKELL notation and functions for
dealing with lists, i. e. we denote the empty list by
[] and list construction by x:xs (where an ele-
ment x is prepended to a list xs), and we use the
functions head (line 01), tail (line 02), and take
(lines 03 and 04), which return the first element in
a list, a list without its first element, and a prefix
of a list, respectively.
In fact, the functions shown in Fig. 2 will be
used in our main algorithm (cf. Fig. 4). Thus,
we explain the functions merge (lines 05–07) and
e(l1,...,lk) (lines 08–10) a bit more in detail.
The merge function takes a set null of pairwise
disjoint lists of derivations, each one in ascend-
ing order with respect to precedesorequal, and merges them into
49
-standard Haskell functions: list deconstructors, take operation
01 head (x:xs) = x
02 tail (x:xs) = xs
03 take n xs = [] if n == 0 or xs == []
04 take n xs = (head xs):take (n-1) (tail xs)
-merge operation (lists in null should be disjoint)
05 merge null = [] if nullnullnull[]null = null
06 merge null = m:merge (nulltail l null l null null, l != [], head l == mnull null
nulll null l null null, l != [], head l != mnull)
07 where m = minnullhead l null l null null, l != []null
-top concatenation
08 e(l1,...,lk) = [] if li == [] for some i null null1,...,knull
09 e(l1,...,lk) = e(head l1,...,head lk):merge nulle(li1,...,lik) null i null null1,...,knullnull
10 where lij =
nullnull
null
nullnull
lj if j < i
tail lj if j = i
[head lj] if j > i
Figure 2: Some useful functions specified in a functional programming style.
one list with the same property (as known from the
merge sort algorithm).
Note that the minimum used in line 07 is based
on the linear pre-order precedesorequal. For this reason, it
need not be uniquely determined. However, in an
implementation this function is deterministic, de-
pending on the the data structures.
The function e(l1,...,lk) implements the top-
concatenation with e on lists of derivations. It is
defined for every e = (v1 ...vk,σ,v) null E and
takes lists l1, . . . , lk of derivations, each in as-
cending order as for merge. The resulting list is
also in ascending order.
4 Algorithm
In this section, we develop our algorithm for solv-
ing the n-best-derivations problem. We begin by
motivating our general approach, which amounts
to solving the 1-best-derivation problem first and
then extending that solution to a solution of the n-
best-derivations problem.
It can be shown that for every m null n, the
set minn(Hv) is equal to the set of all prefixes of
length n of elements of minm(Hv). According
to this observation, we will develop a function p
mapping every v null V to a (possibly infinite) list
such that the prefix of length n is in minn(Hv)
for every n. Then, by virtue of lazy evaluation, a
solution to the n-best-derivations problem can be
obtained by evaluating the term
take n (p v)
where take is specified in lines 03–04 of Fig. 2.
Thus, what is left to be done is to specify p appro-
priately.
4.1 A
provisional specification of p
Consider the following provisional specification
of p:
p v = merge nulle(p v1,...,p vk) null
e = (v1 ...vk,σ,v) null Enull (null)
where the functions merge and e(l1,...,lk) are
specified in lines 05–07 and lines 08–10 of Fig. 2,
respectively. This specification models exactly the
trivial equation
Hv =
null
e=(v1...vk,σ,v)nullE
e(Hv1,...,Hvk)
for every v null V , where the union and the top-
concatenation have been implemented for lists via
the functions merge and e(l1,...,lk).
This specification is adequate if H is acyclic.
For cyclic hypergraphs however, it can not even
solve the 1-best-derivation problem. To illustrate
this, we consider the hypergraph of Ex. 2 and cal-
50
culate6
take 1 (p 1)
= (head (p 1)):take 0 (tail (p 1)) (04)
= head (p 1) (03)
= head (merge nulle1(),e2(),e3(p 1)null) (null)
= minnullhead e1(),head e2(),head e3(p 1)null
(01, 06, 07)
= minnullhead e1(),head e2(), e3(head (p 1))null.
(09)
Note that the infinite regress occurs because the
computation of the head element head (p 1) de-
pends on itself. This leads us to the idea of
“pulling” this head element (which is the solu-
tion to the 1-best-derivation problem) “out” of the
merge in (null). Applying this idea to our particular
example, we reach the following equation for p 1:
p 1 = e2: merge nulle1(),e3(p 1)null
because e2 is the best derivation in H1. Then, in
order to evaluate merge we have to compute
minnullhead e1(),head e3(p 1)null
= minnulle1, e3(head (p 1))null
= minnulle1, e3(e2)null.
Since h(e1) = h(e3(e2)) = 4, we can choose any
of them, say e1, and continue:
e2: merge nulle1(),e3(p 1)null
= e2: e1: merge nulltail e1(), e3(p 1)null
= e2: e1: e3(e2): merge nulltail e3(p 1)null
= ...
Generalizing this example, the function p could
be specified as follows:
p 1 = (b 1) : merge nullexpnull (nullnull)
where b 1 evaluates the 1-best derivation in H1
and exp “somehow” calculates the next best
derivations. In the following subsection, we elabo-
rate this approach. First, we develop an algorithm
for solving the 1-best-derivation problem.
4.2 Solving
the 1-best-derivation problem
Using SP and CP, it can be shown that for ev-
ery v null V such that Hv null= null there is a mini-
mal derivation in Hv which does not contain any
subderivation in Hv (apart from itself). In other
words, it is not necessary to consider cycles when
solving the 1-best-derivation problem.
6Please note that e1() is an application of the function in
lines 08–10 of Fig. 2 while e1 is a derivation.
We can exploit this knowledge in a program by
keeping a set U of visited nodes, taking care not to
consider edges which lead us back to those nodes.
Consider the following function:
b v U = minnulle(b v1 U’,..., b vk U’) null
e = (v1 ...vk,σ,v) null E,
nullv1,...,vknullnull U’ = nullnull
where U’ = U null nullvnull
The argument U is the set of visited nodes. The
term b v null evaluates to a minimal element of Hv,
or to minnull if Hv = null. The problem of this divide-
and-conquer (or top-down) approach is that man-
aging a separate set U for every recursive call in-
curs a big overhead in the computation.
This overhead can be avoided by using a
dynamic programming (or bottom-up) approach
where each node is visited only once, and nodes
are visited in the order of their respective best
derivations.
To be more precise, we maintain a family (Pv null
v null V ) of already found best derivations (where
Pv null minnull1(Hv) and initially empty) and a set C
of candidate derivations, where candidates for all
vertices are considered at the same time. In each
iteration, a minimal candidate with respect to precedesorequal is
selected. This candidate is then declared the best
derivation of its respective node.
The following lemma shows that the bottom-up
approach is sound.
Lemma 3 Let (Pv null v null V ) be a family such that
Pv null minnull1(Hv). We define
C = nulle=(v1...vk,σ,v)nullE,
Pv=null
e(Pv1,...,Pvk) .
Then (i) for every ξ null nullvnullV,Pv=null Hv there is a
ξnull null C such that ξnull null ξ, and (ii) for every v null V
and ξ null C null Hv the following implication holds:
if ξ null ξnull for every ξnull null C, then ξ null min1(Hv).
An algorithm based on this lemma is shown in
Fig. 3. Its key function iter uses the notion of ac-
cumulating parameters. The parameter q is a map-
ping corresponding to the family (Pv null v null V ) of
the lemma, i. e., q v = Pv; the parameter c is a
set corresponding to C. We begin in line 01 with
the function q0 mapping every vertex to the empty
list. According to the lemma, the candidates then
consist of the nullary edges.
As long as there are candidates left (line 04),
in a recursive call of iter the parameter q is up-
dated with the newly found pair (v,[ξ]) of ver-
tex v and (list of) best derivation ξ (expressed by
51
Require Σ-hypergraph H = (V,E), linear pre-
order precedesorequal fulfilling SP and CP.
Ensure b v null min1(Hv) for every v null V
such that if b v == [e(ξ1,...,ξk)] for some
e = (v1 ...vk,σ,v) null E, then b vi == [ξi] for
every i null null1,...,knull.
01 b = iter q0 null(ε,α,v) null E null α null Σ(0)null
02 q0 v = []
03 iter q null = q
04 iter q c = iter (q//(v,[ξ])) c’
05 where
06 ξ = min c and ξ null Hv
07 c’ = nulle=(v1...vk,σ,v)nullE
q v == []
e(q v1,...,q vk)
Figure 3: Algorithm solving the 1-best-derivation
problem.
q//(v,[ξ])) and the candidate set is recomputed
accordingly. When the candidate set is exhausted
(line 03), then q is returned.
Correctness and completeness of the algorithm
follow from Statements (ii) and (i) of Lemma 3,
respectively. Now we show termination. In every
iteration a new next best derivation is determined
and the candidate set is recomputed. This set only
contains candidates for vertices v null V such that
q v == []. Hence, after at most nullV null iterations
the candidates must be depleted, and the algorithm
terminates.
We note that the algorithm is very similar to that
of Knuth (1977). However, in contrast to the latter,
(i) it admits Hv = null for some v null V and (ii) it
computes some minimal derivation instead of the
weight of some minimal derivation.
Runtime According to the literature, the run-
time of Knuth’s algorithm is in O(nullEnull null lognullVnull)
(Knuth, 1977). This statement relies on a number
of optimizations which are beyond our scope. We
just sketch two optimizations: (i) the candidate set
can be implemented in a way which admits ob-
taining its minimum in O(lognullCnull), and (ii) for the
computation of candidates, each edge needs to be
considered only once during the whole run of the
algorithm.
4.3 Solving
the n-best-derivations problem
Being able to solve the 1-best-derivation problem,
we can now refine our specification of p. The re-
fined algorithm is given in Fig. 4; for the func-
tions not given there, please refer to Fig. 3 (func-
tion b) and to Fig. 2 (functions merge, tail, and
the top-concatenation). In particular, line 02 of
Fig. 4 shows the general way of “pulling out” the
head element as it was indicated in Section 4.1 via
an example. We also remark that the definition of
the top-concatenation (lines 08–09 of Fig. 2) cor-
responds to the way in which multnullk was sped up
in Fig. 4 of (Huang and Chiang, 2005).
Theorem 4 The algorithm in Fig. 4 is correct with
respect to its require/ensure specification and it
terminates for every input.
PROOF (SKETCH). We indicate how induction on n
can be used for the proof. If n = 0, then the statement
is trivially true. Let n > 0. If b v == [], then the
statement is trivially true as well. Now we consider the
converse case. To this end, we use the following three
auxiliary statements.
(1) take n (merge nulll1,...,lknull) =
take n (merge nulltake n l1,...,take n lknull),
(2) take n e(l1,...,lk) =
take n e(take n l1,...,take n lk),
(3) take n (tail l) = tail (take (n+1) l).
Using these statements, line 04 of Fig. 2, and line 02
of Fig. 4, we are able to “pull” the take of take n (p
v) “into” the right-hand side of p v, ultimately yield-
ing terms of the form take n (p vj) in the first line
of the merge application and take (n-1) (p vnullj) in
the second one.
Then we can show the following statement by induc-
tion on m (note that the n is still fixed from the outer
induction): for every m null N we have that if the tree
in b v has at most height m, then take n (p v) null
minn(Hv). To this end, we use the following two aux-
iliary statements.
(4) For every sequence of pairwise disjoint sub-
sets P1,...,Pk null nullvnullV Hv, sequence of nat-
ural numbers n1,...,nk null N, and lists l1 null
minn1(P1), . . . , lk null minnk(Pk) such that
nj null n for every j null null1,...,knull we have that
take n (merge nulll1,...,lknull) null minn(P1null...nullPk).
(5) For every edge e = (v1 ...vk,σ,v) null E, subsets
P1,...,Pk null nullvnullV Hv, and lists l1 null minn(P1), . . . ,
lk null minn(Pk) we have that take n e(l1,...,lk) null
minn(e(P1,...,Pk)).
Using these statements, it remains to show that
nulle(ξ1,...,ξk)null null minnnull1null(e(Hv1,...,Hvk) null
nulle(ξ1,...,ξk)null) null nullenullnull=e enull(Hvnull1,...,Hvnullk)null null
minn(Hv) where b v = [e(ξ1,...,ξk)] and null
denotes language concatenation. This can be shown by
using the definition of minn.
Termination of the algorithm now follows from the
fact that every finite prefix of p v is well defined. squaresolid
52
Require Σ-hypergraph H = (V,E), linear pre-order precedesorequal fulfilling SP and CP.
Ensure nulltake n (p v)null null minn(Hv) for every v null V and n null N.
01 p v = [] if b v == []
02 p v = e(ξ1,...,ξk):merge (nulltail e(p v1,...,p vk) null e = (v1 ...vk,σ,v) null Enull null
nullenull(p vnull1,...,p vnullk) null enull = (vnull1 ...vnullk,σnull,v) null E, enull null= enull)
if b v == [e(ξ1,...,ξk)]
Figure 4: Algorithm solving the n-best-derivations problem.
4.4 Implementation, Complexity, and
Experiments
We have implemented the algorithm (consisting
of Figs. 3 and 4 and the auxiliary functions of
Fig. 2) in HASKELL. The implementation is
rather straightforward except for the following
three points.
(1) Weights: we assume that precedesorequal is defined by
means of weights (cf. Ex. 2), and that comparing
these weights is in O(1) (which often holds be-
cause of limited precision). Hence, we store with
each derivation its weight so that comparison ac-
cording to precedesorequal is in O(1) as well.
(2) Memoization: we use a memoization tech-
nique to ensure that no derivation occurring in p v
is computed twice.
(3) Merge: the merge operation deserves some
consideration because it is used in a nested fash-
ion, yielding trees of merge applications. This
leads to an undesirable runtime complexity be-
cause these trees need not be balanced. Thus, in-
stead of actually computing the merge in p and in
the top-concatenation, we just return a data struc-
ture describing what should be merged. That data
structure consists of a best element and a list of
lists of derivations to be merged (cf. lines 06 and
09 in Fig. 2). We use a higher-order function to
manage these data structures on a heap, perform-
ing the merge in a nonnested way.
Runtime Here we consider the n-best part of the
algorithm, i. e. we assume the computation of the
mapping b to take constant time. Note however
that due to memoization, b is only computed once.
Then the runtime complexity of our implementa-
tion is in OnullnullEnull+nullV nullnullnnulllog(nullEnull+n)null. This can
be seen as follows.
By line 02 in Fig. 4, the initial heaps in the
higher-order merge described under (3) have a to-
tal of nullEnull elements. Building these heaps is thus
in O(nullEnull). By line 09 in Fig. 2, each newly found
derivation spawns at most as many new candidates
n total time [s] time for n-best part [s]
1 8.713 —
25 000 10.832 2.119
50 000 12.815 4.102
100 000 16.542 7.739
200 000 24.216 15.503
Table 1: Experimental results
on the heap as the maximum rank in Σ. We assume
this to be constant. Moreover, at most n deriva-
tions are computed for each node, that is, at most
nullVnullnulln in total. Hence, the size of the heap of a node
is in O(nullEnull+n). For each derivation we compute,
we have to pop the minimal element off the heap
(cf. line 07 in Fig. 2), which is in Onulllog(nullEnull+n)null,
and we have to compute the union of the remaining
heap with the newly spawned candidates, which
has the same complexity.
We give another estimate for the total number
of derivations computed by the algorithm, which
is based on the following observation. When pop-
ping a new derivation ξ off the heap, new next best
candidates are computed. This involves comput-
ing at most as many new derivations as the number
of nodes of ξ, because for each hyperedge occur-
ring in ξ we have to consider the next best alter-
native. Since we pop off at most n elements from
the heap belonging to the target node, we arrive at
the estimate dnulln, where d is the size of the biggest
derivation of said node.
A slight improvement of the runtime complex-
ity can be obtained by restricting the heap size to
n best elements, as argued by Huang and Chiang
(2005). This way, they are able to obtain the com-
plexity O(nullEnull + dnullnnull logn).
We have conducted experiments on an Intel
Core Duo 1200 MHz with 2 GB of RAM using
a cyclic hypergraph containing 671 vertices and
12136 edges. The results are shown in Table 1.
This table indicates that the runtime of the n-best
part is roughly linear in n.
53
References
Athanasios Alexandrakis and Symeon Bozapalidis.
1987. Weighted grammars and Kleene’s theorem.
Inform. Process. Lett., 24(1):1–4.
Jean Berstel and Christophe Reutenauer. 1982. Recog-
nizable formal power series on trees. Theoret. Com-
put. Sci., 18(2):115–148.
Matthias B¨uchse, Jonathan May, and Heiko Vogler.
2009. Determinization of weighted tree automata
using factorizations. Talk presented at FSMNLP 09
in Pretoria, South Africa.
Zolt´an ´Esik and Werner Kuich. 2003. Formal tree se-
ries. J. Autom. Lang. Comb., 8(2):219–285.
Zolt´an F¨ul¨op and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Handbook
of Weighted Automata, chapter 9. Springer.
Joshua Goodman. 1999. Semiring parsing. Comp.
Ling., 25(4):573–605.
Liang Huang and David Chiang. 2005. Better k-
best parsing. In Parsing ’05: Proceedings of the
Ninth International Workshop on Parsing Technol-
ogy, pages 53–64. ACL.
Dan Klein and Christopher D. Manning. 2001. Parsing
and hypergraphs. In Proceedings of IWPT, pages
123–134.
Donald E. Knuth. 1977. A Generalization of Dijkstra’s
Algorithm. Inform. Process. Lett., 6(1):1–5, Febru-
ary.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proc. ACL-IJCNLP ’09, pages 593–601.
ACL.
Andreas Maletti and Giorgio Satta. 2009. Parsing al-
gorithms based on tree automata. In Proc. 11th Int.
Conf. Parsing Technologies, pages 1–12. ACL.
Jonathan May and Kevin Knight. 2006. A better n-best
list: practical determinization of weighted finite tree
automata. In Proc. HLT, pages 351–358. ACL.
Mark-Jan Nederhof. 2003. Weighted deductive pars-
ing and Knuth’s algorithm. Comp. Ling., 29(1):135–
143.
Lars Relund Nielsen, Kim Allan Andersen, and
Daniele Pretolani. 2005. Finding the k shortest hy-
perpaths. Comput. Oper. Res., 32(6):1477–1497.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In ACL, pages 295–302.
Adam Pauls and Dan Klein. 2009. k-best a* parsing.
In Proc. ACL-IJCNLP ’09, pages 958–966, Morris-
town, NJ, USA. ACL.
J. W. Thatcher. 1967. Characterizing derivation trees
of context-free grammars through a generalization
of finite automata theory. J. Comput. Syst. Sci.,
1(4):317–322.
54

