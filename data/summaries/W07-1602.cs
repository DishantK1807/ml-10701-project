Finally, word similarity can be computed from structural features like head-modifier relationships (Grefenstette 1994b; Ruge 1992; Dagan, Marcus, and Markovitch 1993; Pereira, Tishby, and Lee 1993; Dagan, Pereira, and Lee 1994).
J93-2004
(Lapata and Keller, 2004) results also support this assertion.
N04-1016
Tag sets for English are derived from the Penn Treebank (Marcus et al., 1993).
J93-2004
annotation For manual syntactic annotation of collected data (see Section 4), we followed the syntactic annotation conventions of the well-known Penn Treebank (Marcusetal., 1993).
J93-2004
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313330.
J93-2004
on measures for inter-rater reliability (Carletta, 1996), on frameworks for evaluating spoken dialogue agents (Walker et al., 1998) and on the use of different corpora in the development of a particular system (The Carnegie-Mellon Communicator, Eskenazi et al.(1999)). The question we are addressing in this paper is how to collect and analyse relevant corpora.
J96-2004
Lapata and Keller?s (2004) unsupervised approach is a notable exception.
N04-1016
Penn treebank (PTB) (Marcus et al., 1993). We used
J93-2004
(Lapata and Keller, 2004) use the document counts returned by WWW search engines.
N04-1016
Proposals for computing acceptability (or preference) include raw frequency counts ((Evans and Zhai, 1996) and (Lapata and Keller, 2004)), Latent Semantic Indexing ((Buckeridge and Sutcliffe, 2002)) and statistical measures of association ((Lapata et al., 1999) and (Nakov and Hearst, 2005)).
N04-1016
Penn Treebank (Marcus et al., 1993), using a stan-
J93-2004
M.P. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. ComputationalLinguistics, 19(2):313330.
J93-2004
Intra-annotator agreement We measured pairwise agreement among annotators usingthekappacoefficient(K)whichiswidelyused in computational linguistics for measuring agreement in category judgments (Carletta, 1996).
J96-2004
Empirical evaluation has been done with the ERG on a small set of texts from the Wall Street Journal Section 22 of the Penn Treebank (Marcus et al., 1993).
J93-2004
For these classications, we calculated a kappa statistic of 0.528 (Carletta, 1996).
J96-2004
Lapata and Keller (2004) use Web counts for phrases Noun P Noun where P belongs to a predefined set of prepositions.
N04-1016
For this reason, each preposition and verb was assigned a weight based on the proportion of occurrences of that word in the Penn Treebank (Marcus et al., 1993) which are labelled with a spatial meaning.
J93-2004
However, the vast amount of data available can nevertheless give better results than more theoretically motivated techniques (Lapata and Keller, 2004).
N04-1016
In order to ensure accuracy of the paraphrases, we use statistics gathered from the Web, using a variation of the approaches presented in Lapata and Keller (2004) and Nakov and Hearst (2005).
N04-1016
Itisnaturaltoquestiontheappropriatenessofweb data for research purposes, because web data is inevitablynoisyandsearchenginesthemselvescanintroduce certain idiosyncracies which can distort results (Kilgarriff and Grefenstette, 2003).
J03-3001
