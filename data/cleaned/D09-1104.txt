Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 997–1006,
Singapore, 6-7 August 2009. c 2009 ACL and AFNLP
Person Cros Document Coreference with Name Perplexity Estimates 
 
 
Octavian Popescu 
popescu@racai.ro 
 
. 
 
 
  
 
Abstract 
The Person Cros Document Coreference sys-
tems depend on the context for making deci-
sions on the posible coreferences betwen 
person name mentions. The amount of context 
required is a parameter that varies from cor-
pora to corpora, which makes it dificult for 
usual disambiguation methods. In this paper 
we show that the amount of context required 
can be dynamicaly controled on the basis of 
the prior probabilities of coreference and we 
present a new statistical model for the compu-
tation of these probabilities. The experiment 
we caried on a news corpus proves that the 
prior probabilities of coreference are an impor-
tant factor for maintaining a god balance be-
twen precision and recal for cros document 
coreference systems. 
1 Introduction

The Person Cros Document Coreference 
(Grishman 194) task, which requires that al and 
only the textual mentions of an entity of type 
Person be individuated in a large colection of 
text documents, is one of the chalenging tasks 
for natural language procesing systems. In the 
most general case the corpus itself is the only 
available source of information regarding the 
persons mentioned and we consider that this is 
the case in this paper. A PCDC system must be 
able to use the information existing in the corpus 
in order to asign to each personal name mention 
(PNM) a piece of context. The coreference of 
any two PNMs is decided mainly on the basis of 
the similarity of the pieces of contexts asociated 
with them. A sucesful PCDC must acurately 
extract the relevant context for coreference. 
However, the context relevance is not abso-
lute. Whether the contextual information 
uniquely individuates a person is a mater of 
probability. This paper presents a statistical tech-
nique developed to provide a PCDC system with 
more information regarding the probability of a 
corect coreference. The reason for developing 
this technique is twofold: (i) the relevant corefer-
ence context depends on the corpus itself and (i) 
valid coreferences require a large amount of in-
formation, which is unavailable in the majority 
of cases. 
The first reason is linked to a particularity of 
the CDC task that makes it more complex than 
other NLP tasks. Unlike in other disambiguation 
tasks, in the CDC tasks the relevant coreference 
context depends on the corpus itself. In word 
sense disambiguation, for instance, the distribu-
tion of the relevant context is mainly regulated 
by strong syntactic and semantic rules. The exis-
tence of such rules makes it posible for the dis-
ambiguation decisions to be made considering 
the local context. On the other hand, the distribu-
tion of the PNMs in a corpus is rather random 
and the relevant coreference context is a dynamic 
variable depending on the diversity of the corpus, 
that is, on how many diferent persons with the 
same name share a similar context. To exem-
plify, consider the name “John Smith” and an 
organization, say “U.N.”.  The extent to which 
“works for U.N.” in “John Smith works for 
U.N.” is a relevant coreference context depends 
on the diversity of the corpus itself. If in that 
corpus, among al the “John Smiths” there is 
only one person who works for “U.N.” then 
“works for U.N.” is a relevant coreference con-
text, but if there are many “John Smiths” work-
ing for U.N., then “works for U.N.” is not a rele-
vant coreference system; in this last case, more 
contextual evidence is neded in order to cor-
rectly corefer the “John Smith” PNMs. The rele-
vance of a context for coreference also depends 
on the corpus, not only on the specific relation-
ship that exists betwen “John Smith” and 
997
“works for U.N.”. Thus, A PCDC system must 
have aces to global information regarding the 
PNMs. 
The second reason comes from practical con-
siderations. The amount of information required 
to corectly infer PNMs coreferences is not pre-
sent in corpus in a computationaly friendly way. 
In many cases the relevant coreference informa-
tion is embeded in semantic and ontological 
dep inferences, which are dificult to program 
In as much as 60% of the cases, two documents 
containing the same name, from a news corpus, 
lack contexts which are directly similar and big 
enough to corectly decide on the coreference.  
We propose a new method to control the 
amount of contextual coreference required for 
corect coreferences. Rather than having fixed 
rules deciding on the size of the context sur-
rounding a PNM, we propose a probabilistic ap-
proach that requires contextual evidence for 
coreference diferentialy, by considering the 
prior probability of the coreference of two 
PNMs; the higher this probability is, the les 
their corect coreference depends on the context 
and vice versa. We present a statistical model 
where the prior coreference probabilities are 
computed considering only the corpus itself, and 
we show how these probabilities are used by a 
PCDC system that dynamicaly revises the 
amount of context relevant for coreference. 
In Section 2 we review the CDC relevant lit-
erature. In section 3 we analyze the data from 
anotated coreference corpora and we individu-
ate a specific problem, seting up a working hy-
pothesis. In Section 4 we develop a statistical 
model for computing the prior coreference prob-
abilities and in Section 5 we present the results 
obtained by aplying it to a large news corpus. In 
section 6 a direct evaluation on CDC is caried 
on a test corpus. In Section 7 we show how the 
proposed techniques extends naturaly to a strat-
egy of construction relevant test corpora for 
CDC task. The paper ends with the Conclusion 
and the Future Research section. 
2 Related
Work 
In a clasical paper (Baga and Baldwin 198), a 
PCDC system based on the vector space model 
(VSM) is proposed. While there are many advan-
tages in representing the context as vectors on 
which a similarity function is aplied, it has ben 
shown that there are inherent limitations asoci-
ated with the vectorial model (Popescu 208). 
These problems, related to the density in the vec-
torial space (superposition) and to the discrimi-
native power of the similarity power (masking), 
become visible as more cases are considered. 
Testing the system on many names, (Goi and 
Alan, 204), it has ben noted empiricaly that 
the acuracy of the results varies significantly 
from name to name. Inded, considering just the 
sentence level context, which is a strong re-
quirement for establishing coreference, a PCDC 
system obtains a god score for “John Smith”. 
This hapens because the prior probability of 
coreference of any two “John Smiths” mentions 
is low, as this is a very comon name and none 
of the “John Smith” has an overwhelming num-
ber of mentions. But for other types of names the 
same system is not acurate. If it considers, for 
instance, “Barack Obama”, the same system ob-
tains a very low recal, as the probability of any 
two “Barack Obama” mentions to corefer is very 
high and the relevant coreference context is 
found very often beyond the sentence level. 
Without further adjustments, a vectorial model 
canot resolve the problem of considering to 
much or to litle contextual evidence in order to 
obtain a god precision for “John Smith” and 
simultaneously a god recal for “Barack 
Obama”. 
In an experiment using bigrams (Pederson et 
al. 205) on a news corpus, it has ben observed 
that the relationship betwen the amount of in-
formation given to a PCDC system and the per-
formances is not linear. If the system has re-
ceived in input the corect number of persons 
with the same name, the acuracy of the system 
has droped. A typical case for this situation is 
when there is a person that is very often men-
tioned, and few other persons having few men-
tions; when the number of clusters is pased in 
the input, the clusters representing the persons 
who are rarely mentioned are wrongly enriched. 
However, this situation can be avoided if there is 
a measure of how probable it is to have a certain 
number of diferent persons with the same name, 
each being mentioned very often in a newspaper. 
Recently, there has ben a major interest in the 
PCDC systems, and, in the last two years, thre 
important evaluation campaigns have ben orga-
nized: Web People Search-1 (Artiles et al. 207), 
ACE 208 (ww.nist.gov/spech/tests/ace/). It 
has ben noted that the data variance betwen 
training and test is very high (Lefever 207). 
Rather than being a particularity of those cor-
pora, the problem is general. The performances 
of a bag of words VSM depends to a very high 
extent on the corpus diversity (se Section 3). 
998
For reliable results, a PCDC system must have 
aces to global information regarding the 
coreference space. 
Rich biographic facts have ben shown to im-
prove the acuracy of PCDC (Man and 
Yarowsky 203). Inded, when available, the 
birth date, the ocupation etc. represent a rele-
vant coreference context because the probability 
that two diferent persons have the same name, 
the same birth date and the same ocupation is 
negligible. However, it is equaly unlikely to find 
this information in a news corpus a suficient 
number of times. Even for a web corpus, where 
the amount of this kind of information is higher 
than in a news corpus, the extended biographic 
facts, including e-mail adres, phones, etc., con-
tribute only with aproximately 3% to the total 
number of coreferences (Elmacioglu et al. 207). 
In order to improve the performances of the 
PCDC systems based on VSM, some authors 
have focused on methods that alow a beter 
analysis of the context by extracting the depend-
ency chains (Ng 207).  The special importance 
of pieces of context has ben exploited by im-
plementing a cascade clustering technique (Wei 
206). Other authors have relied on advanced 
clustering techniques (among others Han et al. 
205, Chen 206). However, these techniques 
rely on the precise analysis of the context, which 
is a time consuming proces. It has ben also 
noted that, in spite of dep analysis, the relevant 
coreference context is hard to find (Vu 207). 
The technique we present in the next sections 
is complementary to these aproaches. We pro-
pose a statistical model designed to ofer to the 
PCDC systems information regarding the distri-
bution of PNMs in the corpus. This information 
is used to reduce the contextual data variation 
and to atain a god balance betwen precision 
and recal. 
3 Data
Analysis 
In this Section we present the data analysis of the 
PNMs. We are interested in establishing a rela-
tionship betwen the distribution of the PNMs 
and the relevant context for coreference. As men-
tioned in the preceding sections, the amount of 
the relevant context for coreference canot be 
decided prior to the investigation of that particu-
lar corpus. The performances of a bag of words 
VSM with a prior defined context aproach wil 
vary greatly from corpus to corpus. We have run 
the folowing experiment: we have considered 
the training and test corpora used in Web People 
Search-1 (WePS-1), which are web page corpora, 
and we have implemented a bag of word ap-
proach with two variants of clustering: aglom-
erative (A), and hierarchic (H). We have ran-
domly chosen a set of seven names from training 
and test (14 names in total) and we have com-
pared the results aplying the two systems, A 
and H, on each set of names. In Figure 1 we pre-
sent the results obtained. The figures on the ver-
tical axes are computed using F
α=0.5 
formula.
 
Figure 1. Variation betwen training and test 
We have noticed a great variation in the be-
havior of the two systems. In order to search for 
an explanation for this diference we have loked 
at the distribution in the two corpora of the 
Named Entities, of the words denoting profes-
sions and of the meta-contextual information e-
mails, urls, phones, and adreses. It turns out 
that these types of contextual information are 
distributed betwen training and test aproxi-
mately evenly. (se Table 1a,b). 
Profesion training oc. test oc. 
Doctor 543 668 
Lawyer 277 385 
Profesor 523 490 
Researcher 340 166 
Teacher 617 569 
Coach 467 471 
Actor 998 790 
Table 1a. Profesion words in training and test 
 Adres training oc. test oc. 
Phone 1,109 1,169 
Fax 606 426 
e-mail 3,134 2,186 
Table 1b. Meta-Context in training and test 
By manualy investigating the training and test 
set of our experiment we have reached the con-
clusion that the reason for the diference is two 
fold: firstly, while the distribution of the words 
denoting profesion is similar, in the test set the 
modifiers, for example “internist”, “neurosur-
geon” for “doctor”, are more frequent. Secondly, 
the number of diferent persons having the same 
999
name is, on average, higher in test than in train-
ing. The results ploted in Figure 1 show that it is 
not a question of which algorithm is beter, but 
rather that there are diferent cases where one 
aproach is prefered over the other. The prob-
lem we face is deciding when it is apropriate to 
use one or the other. 
To induce from the corpus itself when a piece 
of context is or is not a relevant context requires 
dep ontological inferences and a very powerful 
tol of semantic analysis of the context. Consider 
for example two words denoting profesion, 
“doctor” and “researcher”, and their posible 
modifiers “internist”, “neurosurgeon”, and “pro-
fesor” and “PhD”. In the first case it is certain 
that the coreference is not posible, while in the 
second the coreference is very probable. To find 
out such relationships is computationaly very 
hard. However, the analysis caried out further 
shows that we can avoid making such computa-
tions in most of the cases. 
The number of diferent persons is a parame-
ter that canot be known beforehand. However, 
not al the names behave alike with respect to 
coreference. There are noticeable diferences 
betwen names; for example les than 5 00 first 
names cover aproximately 96% of the total of 
first names, while for the same percentage of 
coverage more than 70 00 of last names must be 
considered (Popescu et al. 207). Let us cal per-
plexity of a name the number of diferent persons 
that cary it. The search space depends directly 
on the name perplexity. The biger the perplex-
ity, the larger the amount of information required 
for the corect coreference must be. It sems 
natural that the amount of contextual evidence 
required by a PCDC depends on the name per-
plexity. 
In order to evaluate the relationships betwen 
the context and the name perplexity, we ned an 
anotated corpus. We have used the I-CAB cor-
pus (Magnini et al. 206), which is a four-day 
news corpus fuly anotated, coreference rela-
tionships included. The documents in this corpus 
are entire pieces of news. For each PNM we have 
counted how many contexts containing specific 
information about the person carying the respec-
tive name is present in that particular document. 
There are many types of contexts that refer to a 
person, but some of these types are very infre-
quent. We considered only those types of infor-
mation that are present at least 5% of the times in 
the context surounding a PNM. Table 2 presents 
the results of this investigation. 
 oc. dif oc entities 
First Names 2299 676 1592 
Last Names 4173 1906 2191 
Midle Name 110 44 41 
Activity 973 322 569 
Afiliation 566 399 409 
Role 531 211 317 
Family Relation 133 46 94 
Table 2. Name perplexity and context 
On the second column the total number of oc-
curences is listed, on the third column how 
many of these ocurences have diferent values 
(no case sensitive string match), and on the 
fourth column the number of diferent persons 
(Entities) having that information. The entries 
“activity”, “afiliation”, and “role” represent 
pieces of context where the respective informa-
tion is directly expresed (no inferences). We cal 
this type of context profesional context and for 
aproximately 30% of the PNMs, one of the 
above thre types of profesional contexts is pre-
sent. 
The perplexity of the first names, computed as 
the ratio betwen the fourth column and the third 
column is two times biger than the perplexity of 
the last names. The lowest name perplexity is 
obtained by the names having a midle name a 
name with at least thre tokens – and it is very 
close to 1 (1.07). Comparatively, the highest per-
plexity of two tokens name is 3. The relationship 
betwen the number of tokens of a name and its 
perplexity is straightforward: for names with 
more than four tokens the perplexity is 1 in 
9,6% of the cases (the name by itself is a rele-
vant context for coreference). 
In aproximately 74% of the cases there is just 
one entity coresponding to a two-token name. 
Considering any two PNMs of the same name 
the similarity of two of the profesional contexts 
guarantes the corect coreference. However, 
two profesional contexts are present in only 4% 
of the cases. There are just four cases when con-
sidering just one profesional atribute was mis-
leading, and al these cases are high perplexity 
names. Moreover, in the case of many low per-
plexity names, the contexts could be minimaly 
similar in order to corectly corefer any two 
PNMs of that respective name. 
This analysis shows that there is a direct rela-
tionship betwen the name perplexity and the 
relevant coreference context. However, the aver-
age figures are not very informative, as the vari-
ance of perplexity is very high. Rather than fo-
1000
cusing on the exact figure for name perplexity, 
we wil try to partition the names acording to 
their perplexity and to link each partition to a 
specific behavior with respect to coreference. 
The partitioning technique should ensure that the 
variance of the name perplexity within the same 
partition is low and that a specific amount of 
context should lead to the corect coreference 
decision for the great majority of names within 
that partition. 
Our working hypothesis is that we can esti-
mate the name perplexity within each partition 
and use this information to control the amount of 
contextual evidence required. Let us recal the 
“John Smith” and “Barack Obama” example 
from the previous section. Both “John” and 
“Smith” are American comon first and last 
names. The chance that many diferent persons 
cary this name is high. On the other hand, as 
both “Barack” and “Obama” are rare American 
first and last names respectively, almost surely 
many mentions of this name refer only to one 
person. The argument above does not depend on 
the context, but just on the prior estimation of the 
usage of those names. Having an estimation of a 
name’s perplexity, we may decrease/increase the 
amount of contextual evidence neded. 
4 (p, γ) Statistical Model 
Let D be the set of al PNMs from a given corpus 
C and let D
N
 be the set of coresponding names. 
We want to find a partition P of D
N
 such that 
within each partition the name perplexity varies 
only within predicted margins. Let X be a ran-
dom variable with uniform distribution over D
N 
and let Y be the random variable defined by X’s 
name perplexity. Let us supose that we want P 
= {p
1, p
2, …, p
m
} to be a partition of D
N, where 
the percentage of each partition clas is p
i
:
 
the 
first partition clas contains p
1
 percentage of the 
name population, the second partition clas con-
tains p
2
 percentage of the name population and 
the last partition clas contains p
m
 = 1 Σp
i
 per-
centage of the name population.  
If we knew the distribution function of Y, let’s 
cal it F, we would simply determine ξ
i
 from 
equation 1, where P
i
 =Σp
k , k ≤ i : 
ξ
i
 = F
-1
(P
i
)⇔ F(Y<ξ
i
) = P
i
       (1) 
and we would know that in each partition p
i
 the 
name perplexity is betwen ξ
i-1 
and
 
ξ
i, 
with ξ
0 
= 0. 
However we do not know F. Fortunately, we can 
estimate ξ
i
.  
There is no restriction that may impose a par-
ticular form for F; for example, the normal dis-
tribution hypothesis of name perplexity is ruled 
out by a χ
2 
test with 96.5% confidence for the 14 
names chosen from WePS-1 (se Section 3, first 
paragraph). 
We are going to present a distributional fre 
method for constructing the partition P. The ad-
vantage of this method is that it does not depend 
on any asumption about the PNMs distribution. 
Let us consider X
1, X
2,
 
…, X
n
 a sample of in-
dependent and identical distributed names from 
D
N. 
By rearanging the indexes, 
without losing the 
generality, let us supose that Y
1, Y
2, …, Y
n
 is 
ordered, that is Y
1
 ≤ Y
2
 ≤ … ≤ Y
n
. Even if we do 
not know what form F has, we can stil use equa-
tion (1) in order to estimate ξ
i
. The expected 
value of F(Y
i
) is (Hog, Mckean, Craig 206): 
E[F(Y
i
)] = i/(n+1)       (2) 
which is an estimation of how much mas prob-
ability is on the left of Y
i
. In our terms, we esti-
mate that E[F(Y
i
)] percentage of the name popu-
lation has a name perplexity lower than Y
i
.  
For a given number ξ, the percentage of the 
name population having the name perplexity at 
most ξ is determined by finding the smalest Y
i 
greater than
 
ξ and use the equation (2) to esti-
mate E[F(Y
i
)]. 
In order to build the partition P we are inter-
ested in the percentage of the name population 
that has the perplexity betwen two given values. 
Let (Y
i, Y
j
) be the smalest interval that includes 
these two values. We can estimate the percentage 
of the name population that has a perplexity be-
twen Y
i 
and Y
j
. This estimate is simply F(Y
j
) –
F(Y
i
). We can use directly equation (2) to esti-
mate this diference. However, it is more impor-
tant to have a confidence interval for this esti-
mate, that is we want to know what the probabil-
ity is that the interval (F(Yi), F(Yj) contains at 
least a given percentage of the population, p. The 
optimal partition P is the one that maximizes the 
confidence in the fact that within each of the par-
tition clases as many names as posible have the 
name perplexity in a given interval. 
Let p be a given real number betwen (0,1) 
representing the mas probability that goes into 
the interval (F(Y
i
), F(Y
j
). Let γ = P(F(Y
j
) – 
F(Y
i
) ≥ p). Fortunately γ has a distribution that 
does not depend on F. More precisely, γ has a 
beta distribution given by the function in formula 
(3): 
1001
γ = P(F(Y
j
) – F(Y
i
) ≥ p) =   
∫
p
1  
Γ(n+1)/( Γ(j-i) Γ(n-j+i+1)x
j-i-1
(1-x)
n-j+i
dx  (3) 
The Γ, caled the gama function, is the ex-
tension of the factorial, Γ(x) = ∫
0
∞ 
t
x-1
e
-t
dt. The 
gama function has the property that Γ(x) = Γ(x-
1) Γ(x-2)…. Γ(1); for x an integer, as the argu-
ments in the formula (3) are, Γ(x) = (x-1)! 
The formula (3) gives us a method of building 
the partition P. Let us start with a set of perplex-
ity intervals: (ξ
0, ξ
1
], (ξ
1,ξ
2
], …(ξ
m-1, ξ
m
]. We 
partition the names in D
N 
such that we maximize 
the confidence γ that at least p
i
 percentage of the 
name population has a name perplexity in (ξ
i-1, 
ξ
i
]. We chose an independent and identical dis-
tributed sample X of names to which the ordered 
sample Y of name perplexity values coresponds. 
We start with the lowest perplexity interval and 
determine p
1,γ
1
 and Y
0, Y
i1, such that Y
0
 ≤ ξ
0 
≤ ξ
1 
≤Y
i1
 and γ
1
 = P(F(Y
i1
) – F(Y
0
) ≥ p
1
). The ith in-
dex varies acording to the desired γ
1, 
when
 
p
1 
is 
given,
 
and vice-versa. We can chose i1 with m-
1 liberty grades. Once we are satisfied with the 
values (p
1,γ
1
), we search for the i2th index such 
that Y
i1
 ≤ ξ
1 
≤ ξ
2 
≤Y
i2 
and (p
i2,γ
i2
) have the de-
sired value. The proces continues til the penul-
timate (p
m-1,γ
m-1
). We have no liberty in chosing 
the (p
m,γ
m
).  
We can compute the size of the sample neded 
for guaranteing a minimum γ and p. 
Let us give an example. Supose that (ξ
0, ξ
1
] = 
(0,2]. Thus we are interested in finding p, the 
percentage of the name population such that we 
can be γ sure that at least p names have a per-
plexity betwen 1 and 2 inclusive. We take a 
random sample of n = 30 and supose the small-
est index i1 such that Y
i
 ≥ 3 for al i > i1 is 17 
. 
 
We want to compute the confidence γ that at 
least p = 60% of the name population has the 
name perplexity within (0,2]: 
γ = 1 ∫
0 
0.6 
30!/(16!15!)x
15
(1-x
14
)dx = 
= 1 – k(∫
0 
0.6 
x
15
dx + ∫
0 
0.6 
x
29
dx) = 
= 1 – k[(1/16)(6/10)
16 
+ (1/30)(6/10)
30
] 
≥ .965 
In practice we want to have optimal values for 
p and γ; a large p implies a smal γ and vice-
versa. The optimality is determined by the acu-
racy of the CDC system: we want to have the 
largest posible percentage of names into each 
partition such that our confidence that the names 
inside each partition have the same perplexity. 
It is useful to work the equation (1) back-
wards. Supose that we established the first par-
tition clas of P we have found the i1th index, 
p
1, and γ
1
. Now we refer only to the names in the 
partition clas. We can compute the probability 
that a certain percentage of the names within that 
particular partition clas have a given name per-
plexity. That is, we consider a random sample 
inside the partition clas, X, and its corespon-
dent random variable Y, as above. The confi-
dence that p
1inside
 percentage of names have the 
name perplexity ξ
p
 within the interval (Y
0, Y
ith
) 
is: 
P(Y
0
 < ξ
p 
<Y
i1
) = Σ
k
   (
k
n
) p
k
(1-p)
n-k      
(4)
 
(
k
n
) represents the k-combinations of size n. 
By taking advantage of the botstraping 
method (Efron and Tibshirani 193) we do not 
have to resample inside the partition clas. We 
use the Y
0, ., Y
i1 
values with replacement. Using 
(4) we obtain p
1inside
 which shows us which per-
centage inside the partition clas has the name 
perplexity within (Y
0, Y
i1
]. And consequently we 
can compute γ
1inside
. Finaly we are able to formu-
late the folowing statement about each partition 
clas: 
In the ith partition clas enter p
i
 percent-
age of the name population with a confi-
dence γ
i
.
 
Inside
 
this partition clas we are 
γ
inside 
confident that p
inside 
percentage of 
the names have a name perplexity within 
(Y
i1-1, Y
i1
]. 
The p and γ indicate the theoretical values 
that define the partition. In practice the exact 
distribution of the names into the subset is 
unknown, therefore each heuristics that 
computes the perplexity creates its own dis-
tribution. The values γ
inside 
and p
inside 
control 
how much a certain heuristics departs from 
the theoretical values. The optimal heuristics 
have very big figures for γ
inside 
and p
inside
. 
In the next section we present an experi-
ment carried on a news corpus. We show 
how the above model leads to a stable parti-
tion of names and that inside each partition 
class reliable (p,γ) values can be computed.  
1002
5 Name
Perplexity Partition 
For the experiment described in this section, we 
have used a two-year part of the seven-years Ital-
ian local newspaper corpus caled Adige50k 
Corpus (Magnini 206). 
We describe below how we compute the per-
plexity clas for the one-token names and two-
tokens names respectively. As mentioned in sec-
tion 3, the name perplexity decreases rapidly for 
tre-token or more names. If desired, the same 
technique could also be aplied for those names. 
In Adige50k there are 106, 187 diferent one-
token names; 429, 243 two-token names; 36, 73 
thre-token names; 5, 152 four-token names, 940 
four token names and les than 300 diferent 
four-token or more names. 
An estimate of the name perplexity of the one-
token names is the size of the diferent one-token 
names with which it forms a complete PNM in 
the corpus. For example for the first name 
“John” the estimation of its perplexity is the size 
of the one-token last names it combines with in 
forming PNMs, like “Smith, Travolta, Kenedy” 
etc. The biger the size of its complementary 
names, the higher is its name perplexity. In Table 
3 we present the figures of these estimates. 
ocurences (interval) average perplexity 
1-5 4.13 
6-20 8.34 
21-100 17.4 
101-1,00 68.54 
1,00-5,00 683.95 
5,00-31,091 478.23 
Table 3. Average perplexity one-token names 
We start with a five name perplexity clases: 
“very low” (VL) , “low” (L) , “medium”, (M) 
“high” (H) and “very high” (VH). The name per-
plexity of a two-token name is interpolated from 
the name perplexity of its components. We used 
the folowing heuristics: the name perplexity 
clas is the average name perplexity clases of its 
one-token name. If the name perplexity clases 
are the same then the name perplexity clas of 
the whole name is one clas les (if posible). 
In order to compute the borderline be-
tween two consecutive classes we apply the 
(p, γ) method. We selected 25 two-tokens 
names and we manually investigate their oc-
currence in order to know their real name 
perplexity. The perplexity classes obtained 
after applying the (p, γ) technique are listed 
in Tables 4a and 4b respectively. 
perplexity clas percentage 
very high (VH) 5.3% 
high (H) 8.7% 
medium (M) 20.9% 
low (L) 27.6% 
very low (VL) 37.5% 
Table 4a. First Name perplexity clases 
perplexity clas percentage 
very high (VH) 1.8% 
high (H) 3.36% 
medium (M) 17.51% 
low (L) 20.31% 
very low (VL) 57.02 
Table 4b. Last Name perplexity clases 
Tables 4a and 4b fuly describe the partition 
for one-token names. Ordering the one token 
names acording to their perplexity we chose the 
first ones acording to the  percentage listed 
above. The same proces aplies to the one-token 
last names. The values computed for two-token 
names are listed. 
 P 
γ 
p
inside
 γ
inside
 
VH 0.04% 70% 70% 80% 
H 2.53% 76% 70% 80% 
M 10.08% 87% 80% 82% 
L 27.97% 90% 99% 90% 
VL 59.38% 96.5% 99% 96.5% 
Table 5. (p, γ, p
inside,
 γ
inside
) values 
6 CDC
with Name Perplexity Estimates 
The working hypothesis is that using the name 
partition obtained with the (p, γ) procedure we 
can effectively improve the accuracy of a 
CDC system by reducing/increasing the 
amount of contextual evidence required for 
coreferencing according to the perplexity 
class to each the name belong. 
To construct a test corpus we have 
adopted the following strategy: we chose 20 
two-token names such that both sets of one 
token-names, the first names and the last 
names respectively, cover the whole space in 
the perplexity partition. In Table 6a and 6b 
we present 5 first and last names used in test. 
As not all the 25 names formed by combin-
1003
ing the names in 6a and 6b are found in the 
corpus, we consider 11 other two-tokens 
names having the same distribution. On the 
first column the names are listed, on the sec-
ond column the computed perplexity (P), on 
the third column the number of occurrences 
as one-token name (O), on the fourth the 
number of occurrences in a two-token name 
(T) and on the last column the computed 
perplexity class (PC). 
Name P 
O T PC 
Delai 7 31091 10722 VL 
Parolari 171 1,619 2207 H 
Prodi 52 9184 3382 M 
Ruini 15 554 203 L 
Rosi 753 7506 8356 VH 
Table 6a. Test Last Names 
Name P 
O T PC 
Camilo 276 664 1731 L 
Lorenzo 2088 2167 2198 H 
Paolo 5255 4001 51244 VH 
Romano 14 886 6414 M 
Varena 5 10 85 VH 
Table 6b. Test First Names 
We compare the results obtained by our 
CDC system using the name perplexity parti-
tion (S) against two baselines: one that con-
siders only the context at the sentence level 
and (BLS) one that considers the whole news 
(BLN). We obtain the following figures us-
ing the B-CUBED measure: S scores .72, 
BLS .59 and BLN .61. The gain in accuracy 
of more than 10% is due to the use of name 
perplexity classes. 
The great advantage of using the (p,γ) es-
timates can be seen in those case where the 
ratio between the number of mentions and 
the rank of the name is close to extremes: 
either big number of mentions and low name 
perplexity, or low number of mentions and 
high name perplexity. In the first case the 
contextual evidence for coreference may be 
very scarce and in the second case, the re-
quirement for strong contextual evidence is 
the best decision. Our results suggest that 
loosening the contextual requirements in the 
first case leads to an important gain in recall, 
up to 40%, while the lose in precision is less 
than 1.5%. The situation is best described by 
four panels of the five-number-summary 
plots of the test corpus. Panel A shows the 
distribution of the main five quantilies con-
sidering all the names together. Panel B 
shows the distribution for very low perplex-
ity class, Panel C for medium perplexity 
class and Panel D for the very high perplex-
ity class. The number of outliers in Panel A 
is high, which makes it difficult for any CDC 
system, but inside each perplexity class the 
variation is reduced. 
 
7 Constructing
an Evaluation Corpus 
The (p, γ) technique could be used for construct-
ing a test corpus for the CDC task. The main 
problem faced in the construction of the test cor-
pus is data variation. The number of diferent 
entities mentioned with the same name is a ran-
dom variable with a big variance. The distribu-
tion of the number of entities is very skew. The 
average perplexity is 2.01%, but les than 18% of 
the total number of names have a perplexity 
greater than 3. In Figure 2 we plot a modified 
Lorenz curve (the vertical axis is not divided in 
percentage, as the values are discrete). 
 
Figure 2. Lorenz Curve names/no. entities 
The direct consequence of this situation is the 
fact that constructing an evaluation corpus by 
taking random names wil result with a great 
probability in a very skew test corpus. Inded, 
1004
the expectation is that in such corpus, the aver-
age perplexity is very low, and consequently, the 
great majority of cases can be coreferenced by a 
simple algorithm. Therefore, this test corpus may 
be largely inefective in ranking the algorithms. 
In fact, we want to construct an evaluation cor-
pus that is able to promote the most efective 
algorithms. The discriminative power of a test 
corpus is directly related to the variance of the 
data. Moreover, if only certain names are consid-
ered for a test corpus, the variance can be very 
low; in particular, when the test corpus contains 
just one name the variance is zero. It is dificult 
to se the merits of diferent algorithms when 
tested on such corpora. 
In order to make more informative statements 
we ned to construct an evaluation corpus that is 
les dependent on the data variance. A posible 
solution is to form a partition of the set of the 
PNMs, that is, to split the whole set of PNMs in 
mutual disjunctive groups. This type of method-
ology is caled stratified sampling, mainly be-
cause each group is a stratum. The sampling 
strategy, the number of sampling elements, the 
variance and the sampling eror can be calculated 
independently for each strata. 
The main advantages of stratified sampling 
are that we can concentrate on the special 
groups, that in general this strategy improves the 
acuracy of the estimation, and that the number 
of elements in each stratum can be conveniently 
chosen. The main disadvantages are related to 
the dificulty in finding a suitable partition of the 
population. The strata should be chosen prior to 
the sampling time, but the homogeneity inside 
the stratum should be guaranted. 
Our proposal is to use the name perplexity in-
tervals. We argue that this proposal is four-fold 
sustainable. Firstly, the name perplexity is di-
rectly conected to the random variable whose 
distribution we estimate, namely the number of 
entities. Secondly, for fre names it can be com-
puted of line. Thirdly, it gives us an independ-
ent and formaly corect way to make a partition. 
Fourthly, it easily alows a separation betwen 
the important and unimportant cases. 
To begin with, let us supose we have a name 
that has n ocurences in the Adige 50K. If n is 
relatively large, than we can be sure that there 
are some dominant entities that may be repre-
sented by the majority of PNMs that have this 
name as value. However, it is unknown whether 
the n comes from the fact that there are inded 
some dominant entities or whether the name by 
itself is a frequently used name. 
In order to deal with the diferences betwen 
frequency vs. perplexity, we propose to build a 
matrix defined by the frequency clases as rows 
and perplexity clases as columns. In Figure 3 
we present this matrix. 
 
Figure 3. Frequency/Comones strata ma-
trix. 
The number of diferent names in each of the 
cels of the matrix may difer acording to the 
departure of the normal distribution of each stra-
tum. In general, if the real distribution is normal, 
then as much as ten examples are suficient. Oth-
erwise, for not very skew distributions, which 
we expect most of the strata to have, an average 
of 30 examples should sufice. In same cases, as 
the normal distribution can be apropriately 
sampled when both Np and N(1-p) are grater than 
five – where p is the ratio perplexity/frequency 
and N the sample dimension – the number of 
elements in the cel may be around 20, by a 
maximal rough estimation. 
8 Conclusion
and Further Research 
We have presented a distributional fre statistical 
method to design a name perplexity system, such 
that each perplexity clas maximizes the number 
of names for which the prior coreference prob-
ability belongs to the same interval. This infor-
mation helps the PCDC systems lower/increase 
adequately the amount of contextual evidence 
required for coreference. 
The aproach presented here is efective in 
dealing with the problems raised by using a simi-
larity metrics on contextual vectors improving 
the overal acuracy with more than 10%. 
We would like to increase the number of cases 
considered in the sample required to delimit the 
perplexity clases. Equation (3) may be devel-
oped further in order to obtain exactly the num-
ber of required cases. 
The (p, γ) procedure is efective is dealing 
with the problems regarding the construction of 
an evaluation corpus. The technique presented in 
the last section could be extended further and we 
are already working on a new series of experi-
ments whose results wil be made available in the 
near future.  
1005
Acknowledgments 
The corpus used in this paper is Adige50k, a 
seven-year news corpus from an Italian local 
newspaper. The author thanks to al the people 
involved in the construction of Adige50k. 
References 
J. Artiles, Gonzalo, J., S. Sekine. 207. Establishing 
a benchmark for WePS. In Procedings of Se-
mEval. 
A. Baga, B. Baldwin. 1998. Entity-based Cros-
Document Co-referencing using the Vector 
Space Model. In Procedings of ACL. 
J. Chen, D. Ji, C. Tan, Z. Niu. 206. Unsupervised 
Relation Disambiguation Using Spectral Cluster-
ing. In Procedings of COLING 
C. Goi, J. Alan. 204. Cros-Document Corefer-
ence on a Large Scale Corpus. In Procedings of 
ACL. 
R. Grishman. 194. Whither Writen Language 
Evaluation? In Procedings of Human Language 
Technology Workshop, p. 120-125. San Mateor. 
E. Elmacioglu, Y. M. F. M.Y.Khan, D. Le. 2007. 
PSNUS: Web People Name Disambiguation by 
Simple Clustering with Rich Features, in Proced-
ings of SemEval 
H. Han, W. Xu. 205. A Hierarchical Bayes Mix-
ture Model for Name Disambiguation in Author 
Citations, in Procedings of SAC’05 
R. Hog, J. McKean, A. Craig, 206. Introduction of 
Mathematical Statistics, ed. Prentice Hal 
E. Lefever, V. Hoste, F. Timur. 207. AUG: A Com-
bined Clasification and Clustering Aproach 
for Web People Disambiguation, In Procedings of 
SemEval 
B. Magnini, M. Speranza, M. Negri, L. Romano, R. 
Sprugnoli. 206. I-CAB – the Italian Content An-
notation Bank. LREC 206 
V., Ng. 207. Shalow Semantics for Coreference 
Resolution, In Procedings of IJCAI 
T. Pedersen, A. Purandare, A. Kulkarni. 205. Name 
Discrimination by Clustering Similar Contexts, in 
Proceding of CICLING 
O. Popescu, C. Girardi. 208. Improving Cros 
Document Coreference, in Procedings of JADT 
O. Popescu, B. Magnini. 207. Infering Corefer-
ence among Person Names in a Large Corpus of 
News Colection, in Procedings of AIIA 
Y. Wei, M. Lin, H. Chen. 206. Name Disambigua-
tion in Person Information Mining, in Procedings 
of IEE 
Q. Vu, T. Masada, A. Takasu, J. Adachi. 207. Us-
ing Knowledge Base to Disambiguate Personal 
names in Web Search Results, In Procedings of 
SAC 
1006

