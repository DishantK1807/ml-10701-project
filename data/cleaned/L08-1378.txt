<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Bias decreases in proportion to the number of annotators</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Formal Grammar – the Mathematics of Language</booktitle>
<pages>141--150</pages>
<contexts>
<context>ion studies, Vox Populi Annotation requires very little annotator training. However, are these intensity measures using the Vox Populi Annotation method reliable? The Kappa statistic (Carletta, 1996; Artstein and Poesio, 2005b) cannot adequately assess the reliability of Vox Populi Annotations because annotators are not expected to agree on the same sentence at all. Contrary to most annotation studies, the Vox Populi Anno</context>
<context>, that is, individual annotators may have different preferences to label one category more than the other category. Incidentally, the same number of 18 annotators as ours were recruited in the study (Artstein and Poesio, 2005a). We explicitly determined the number of annotators based on the analysis in Section 2.1., and not simply chose a big number. One seeming obstacle to the Vox Populi Annotation method is a large numb</context>
</contexts>
<marker>Artstein, Poesio, 2005</marker>
<rawString>Ron Artstein and Massimo Poesio. 2005a. Bias decreases in proportion to the number of annotators. In Proceedings of the Workshop on Formal Grammar – the Mathematics of Language, pages 141–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Kappa3 = Alpha (or Beta</title>
<date>2005</date>
<tech>Technical Report CSM-437</tech>
<institution>Department of Computer Science, University of Essex</institution>
<contexts>
<context>ion studies, Vox Populi Annotation requires very little annotator training. However, are these intensity measures using the Vox Populi Annotation method reliable? The Kappa statistic (Carletta, 1996; Artstein and Poesio, 2005b) cannot adequately assess the reliability of Vox Populi Annotations because annotators are not expected to agree on the same sentence at all. Contrary to most annotation studies, the Vox Populi Anno</context>
<context>, that is, individual annotators may have different preferences to label one category more than the other category. Incidentally, the same number of 18 annotators as ours were recruited in the study (Artstein and Poesio, 2005a). We explicitly determined the number of annotators based on the analysis in Section 2.1., and not simply chose a big number. One seeming obstacle to the Vox Populi Annotation method is a large numb</context>
</contexts>
<marker>Artstein, Poesio, 2005</marker>
<rawString>Ron Artstein and Massimo Poesio. 2005b. Kappa3 = Alpha (or Beta). Technical Report CSM-437, Department of Computer Science, University of Essex, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Barr</author>
<author>Luis Felipe Cabrera</author>
</authors>
<date>2006</date>
<note>AI gets a brain</note>
<contexts>
<context>little training, this paper offer guidelines on selecting number of annotators and assessing reliability. Recently there has been an annotation study on sentiment conducted on Amazon Mechanical Turk (Barr and Cabrera, 2006), a commercial web service that facilitates large number of annotators. 5. Conclusion We annotated the intensity of ideological perspectives expressed in 250 sentences extracted from articles on the </context>
</contexts>
<marker>Barr, Cabrera, 2006</marker>
<rawString>Jeff Barr and Luis Felipe Cabrera. 2006. AI gets a brain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Queue</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistics</title>
<date></date>
<journal>Computational Linguistics</journal>
<booktitle>4http://del.icio.us/ 5http://www.flickr.com/ 6http://www.youtube.com/ 7http://images.google.com/imagelabeler</booktitle>
<volume>22</volume>
<marker>Queue, </marker>
<rawString>Queue, 4(4):24–29, May. 4http://del.icio.us/ 5http://www.flickr.com/ 6http://www.youtube.com/ 7http://images.google.com/imagelabeler/ Jean Carletta. 1996. Assessing agreement on classification tasks: The kappa statistics. Computational Linguistics, 22(2). Lynn Carlson, Daniel Marcu, and mary Ellen Okurowski.</rawString>
</citation>
<citation valid="true">
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory</title>
<date>2001</date>
<booktitle>In Proceedings of the Second SIGDial Workshop on Discourse and Dialogue</booktitle>
<pages>1--10</pages>
<marker>2001</marker>
<rawString>2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In Proceedings of the Second SIGDial Workshop on Discourse and Dialogue, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Casella</author>
<author>Roger L Berger</author>
</authors>
<title>Statistical Inference</title>
<date>2001</date>
<publisher>Duxbury Press</publisher>
<note>second edition</note>
<contexts>
<context>x Populi Intensity is not related between different annotator groups. Correlation coefficients for the above two random cases will be zero because two groups of annotators make independent judgments (Casella and Berger, 2001). Therefore, the Vox Populi Annotation method is reliable if the correlation coefficients between two annotator groups are positive, high, and above zero. 3. Measuring Intensity of Ideological Perspe</context>
</contexts>
<marker>Casella, Berger, 2001</marker>
<rawString>George Casella and Roger L. Berger. 2001. Statistical Inference. Duxbury Press, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
</authors>
<title>MUC-7 named entity task definition</title>
<date>1997</date>
<booktitle>In Proceedings of the Seventh Message Understanding Conference</booktitle>
<contexts>
<context>nnotators becomes prohibitive. Besides, qualified candidates are very unlikely to be recruited from general public. Annotation tasks that have little ambiguities include, for example, named entities (Chinchor, 1997) and automatic speech recognition transcriptions (Fisher et al., 1986). Multiple annotators make little sense because they will label very similarly. Our annotation study is about labeling intensity </context>
</contexts>
<marker>Chinchor, 1997</marker>
<rawString>Nancy Chinchor. 1997. MUC-7 named entity task definition. In Proceedings of the Seventh Message Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William J Conover</author>
</authors>
<title>Practical Nonparametric Statistics</title>
<date>1971</date>
<publisher>John Wiley &amp; Sons</publisher>
<contexts>
<context>Populi Annotation method requires annotators to make forced binary choices for each sentence, and each choice is like flipping a coin, i.e., a Bernoulli experiment. We choose the exact Binomial test (Conover, 1971) to test the above hypothesis. The test procedure’s power depends on two factors: the number of annotators and a sentence’s ideological intensity (i.e., ). There is a trade-off between two factors. I</context>
<context> we are that Vox Populi Intensity is not random; the more intensely a sentence expresses an 1We only list the case for x &lt; n=2 and omit the case for x n=2 because the two cases are very similar. See (Conover, 1971) for more details. ideological perspective, the fewer annotators we need to assess whether annotators make random guesses. By checking Figure 1 researchers can decide how many annotators are needed a</context>
</contexts>
<marker>Conover, 1971</marker>
<rawString>William J. Conover. 1971. Practical Nonparametric Statistics. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip H DuBois</author>
</authors>
<title>Multivariate correlational analysis</title>
<date>1957</date>
<publisher>Harper</publisher>
<contexts>
<context>omial vector that assumes every category is equally likely. The correlation coefficient for assessing the reliability of the Vox Populi Annotation method will be replaced by multivariate correlation (DuBois, 1957). There has been annotation studies on measuring intensity, for example, the intensity of opinioned expressions (Wiebe et al., 2005). The annotation schemes in previous work mostly use the Likert Sca</context>
</contexts>
<marker>DuBois, 1957</marker>
<rawString>Philip H. DuBois. 1957. Multivariate correlational analysis. Harper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Michael Glass</author>
</authors>
<title>The kappa statistics: a second look</title>
<date>2004</date>
<journal>Computational Linguistics</journal>
<volume>30</volume>
<marker>Di Eugenio, Glass, 2004</marker>
<rawString>Barbara Di Eugenio and Michael Glass. 2004. The kappa statistics: a second look. Computational Linguistics, 30(1):95–101, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W M Fisher</author>
<author>G R Doddington</author>
<author>K M GoudieMarshall</author>
</authors>
<title>The DARPA speech recognition research database: specifications and status</title>
<date>1986</date>
<booktitle>In Proceedings of the DARPA Speech Recognition Workshop</booktitle>
<contexts>
<context>very unlikely to be recruited from general public. Annotation tasks that have little ambiguities include, for example, named entities (Chinchor, 1997) and automatic speech recognition transcriptions (Fisher et al., 1986). Multiple annotators make little sense because they will label very similarly. Our annotation study is about labeling intensity of bipolar ideological perspectives, but how about some annotation tas</context>
</contexts>
<marker>Fisher, Doddington, GoudieMarshall, 1986</marker>
<rawString>W.M. Fisher, G.R. Doddington, and K.M. GoudieMarshall. 1986. The DARPA speech recognition research database: specifications and status. In Proceedings of the DARPA Speech Recognition Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Hoew</author>
</authors>
<title>The rise of crowdsourcing</title>
<date>2006</date>
<journal>Wired Magazine</journal>
<volume>14</volume>
<contexts>
<context>tors? While most annotation studies in computational linguistics recruit few annotators, many “annotation” tasks in other fields have begun to “recruit” a huge number of people, i.e., Crowd-sourcing (Hoew, 2006). Millions of Internet users have constantly labeled web pages (e.g., Delicious4), photos (e.g., Flicker5), and videos (e.g., YouTube6) without being paid. ESP game (von Ahn and Dabbish, 2004) and Go</context>
</contexts>
<marker>Hoew, 2006</marker>
<rawString>Jeff Hoew. 2006. The rise of crowdsourcing. Wired Magazine, 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rensis Likert</author>
</authors>
<title>A technique for the measurement of attitudes</title>
<date>1932</date>
<journal>Archives of Psychology</journal>
<volume>140</volume>
<institution>Columbia University</institution>
<location>New York City</location>
<contexts>
<context>here has been annotation studies on measuring intensity, for example, the intensity of opinioned expressions (Wiebe et al., 2005). The annotation schemes in previous work mostly use the Likert Scale (Likert, 1932) and quantize intensity into discrete categories (e.g., low, medium, strong, and extreme). To have two annotators agree on each scale requires extensive training. Moreover, it is not trivial at all t</context>
</contexts>
<marker>Likert, 1932</marker>
<rawString>Rensis Likert. 1932. A technique for the measurement of attitudes. Archives of Psychology 140, Columbia University, New York City.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Hao Lin</author>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Alexander Hauptmann</author>
</authors>
<title>Which side are you on? identifying perspectives at the document and sentence levels</title>
<date>2006</date>
<booktitle>In Proceedings of Tenth Conference on Natural Language Learning (CoNLL</booktitle>
<contexts>
<context>In this paper we focus on annotating the intensity of ideological perspectives expressed at the sentence level. Annotations on the ideological perspectives at the document level are available (e.g., (Lin et al., 2006)); annotations at the sentence level, however, remain scarce. Not all sentences in a biased document express the overall ideological perspective to the same degree, and manual annotations are needed.</context>
<context>otator groups are positive, high, and above zero. 3. Measuring Intensity of Ideological Perspectives 3.1. Annotation corpus and procedure We randomly chose 250 sentences from the bitterlemons corpus (Lin et al., 2006), which consists of articles published on the website http://bitterlemons.org/. The website is set up to “contribute to mutual understanding [between Palestinians and Israelis] through the open excha</context>
</contexts>
<marker>Lin, Wilson, Wiebe, Hauptmann, 2006</marker>
<rawString>Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexander Hauptmann. 2006. Which side are you on? identifying perspectives at the document and sentence levels. In Proceedings of Tenth Conference on Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Nancy Ide</author>
<author>Ludovic Denoyer</author>
<author>Yusuke Shinyama</author>
</authors>
<title>The shared corpora working group report</title>
<date>2007</date>
<booktitle>In Proceedings of the Linguistic Annotation Workshop</booktitle>
<contexts>
<context>ther annotation tasks. As more computational linguistics are interested in more complex linguistic phenomena (e.g., intensity of subjectivity (Wiebe et al., 2005), political and social controversies (Meyers et al., 2007)), the Vox method Populi Annotation method can be a viable alternative for researchers to quantitatively measure these complex phenomena. However, the Vox Populi Annotation method is not applicable t</context>
<context>equire extensive linguistic knowledge or have little ambiguities: Annotation tasks that require extensive linguistic knowledge include, for example, predicate argument stricture in the Penn treebank (Meyers et al., 2007) and Rhetorical Structure Theory (RST) (Carlson et al., 2001). Because these annotation tasks require intensive training and constant monitoring, the cost of recruiting a large number of annotators b</context>
</contexts>
<marker>Meyers, Ide, Denoyer, Shinyama, 2007</marker>
<rawString>Adam Meyers, Nancy Ide, Ludovic Denoyer, and Yusuke Shinyama. 2007. The shared corpora working group report. In Proceedings of the Linguistic Annotation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rimothy R C Read</author>
<author>Noel A C Cressie</author>
</authors>
<title>Goodness-of-Fit Statistics for Discrete Multivariate Data</title>
<date>1988</date>
<publisher>Springer-Verlag</publisher>
<contexts>
<context> can extend our reliability assessment in Section 2. to more than two choices. The exact binomial test for determining the number of annotators in Section 2.1. will be replaced by a multinomial test (Read and Cressie, 1988). The null hypothesis will not be a simple = 0:5, and will be a multinomial vector that assumes every category is equally likely. The correlation coefficient for assessing the reliability of the Vox </context>
</contexts>
<marker>Read, Cressie, 1988</marker>
<rawString>Rimothy R.C. Read and Noel A.C. Cressie. 1988. Goodness-of-Fit Statistics for Discrete Multivariate Data. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling images with a computer game</title>
<date>2004</date>
<booktitle>In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</booktitle>
<pages>319--326</pages>
<marker>von Ahn, Dabbish, 2004</marker>
<rawString>Luis von Ahn and Laura Dabbish. 2004. Labeling images with a computer game. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Theresa Wilson</author>
<author>Claire Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation</title>
<date>2005</date>
<volume>39</volume>
<pages>3--165</pages>
<contexts>
<context>ethod is a general methodology and may be applicable to other annotation tasks. As more computational linguistics are interested in more complex linguistic phenomena (e.g., intensity of subjectivity (Wiebe et al., 2005), political and social controversies (Meyers et al., 2007)), the Vox method Populi Annotation method can be a viable alternative for researchers to quantitatively measure these complex phenomena. How</context>
<context>ox Populi Annotation method will be replaced by multivariate correlation (DuBois, 1957). There has been annotation studies on measuring intensity, for example, the intensity of opinioned expressions (Wiebe et al., 2005). The annotation schemes in previous work mostly use the Likert Scale (Likert, 1932) and quantize intensity into discrete categories (e.g., low, medium, strong, and extreme). To have two annotators a</context>
<context>output confidence scores. The mathematical relationship between annotation group sizes and a sentence’s intensity in Section 2.1. seems to be empirically observed. In a subjectivity annotation study (Wiebe et al., 2005), ... the difference between no subjectivity and a low-intensity private state might be highly debatable, but the difference between no subjectivity and a medium or high-intensity private sate is oft</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2– 3):165–210.</rawString>
</citation>
</citationList>
</algorithm>

