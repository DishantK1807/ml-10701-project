1:171	A Distributional Analysis of a Lexicalized Statistical Parsing Model Daniel M. Bikel Department of Computer and Information Science University of Pennsylvania 3330 Walnut Street Philadelphia, PA 19104 dbikel@cis.upenn.edu Abstract This paper presents some of the first data visualizations and analysis of distributions for a lexicalized statistical parsing model, in order to better understand their nature.
2:171	In the course of this analysis, we have paid particular attention to parameters that include bilexical dependencies.
3:171	The prevailing view has been that such statistics are very informative but su er greatly from sparse data problems.
4:171	By using a parser to constrain-parse its own output, and by hypothesizing and testing for distributional similarity with back-o distributions, we have evidence that finally explains that (a) bilexical statistics are actually getting used quite often but that (b) the distributions are so similar to those that do not include head words as to be nearly indistinguishable insofar as making parse decisions.
5:171	Finally, our analysis has provided for the first time an e ective way to do parameter selection for a generative lexicalized statistical parsing model.
6:171	1 Introduction Lexicalized statistical parsing models, such as those built by Black et al.7:171	(1992a), Magerman (1994), Collins (1999) and Charniak (2000), have been enormously successful, but they also have an enormous complexity.
8:171	Their success has often been attributed to their sensitivity to individual lexical items, and it is precisely this incorporation of lexical items into features or parameter schemata that gives rise to their complexity.
9:171	In order to help determine which features are helpful, the somewhat crude-bute ective method has been to compare a models overall parsing performance with and without a feature.
10:171	Often, it has seemed that features that are derived from linguistic principles result in higherperforming models (cf.
11:171	(Collins, 1999)).
12:171	While this may be true, it is clearly inappropriate to highlight ex post facto the linguistically-motivated features and rationalize their inclusion and state how e ective they are.
13:171	A rigorous analysis of features or parameters in relation to the entire model is called for.
14:171	Accordingly, this work aims to provide a thorough analysis of the nature of the parameters in a Collins-style parsing model, with particular focus on the two parameter classes that generate lexicalized modifying nonterminals, for these are where all a sentences words are generated except for the head word of the entire sentence; also, these two parameter classes have by far the most parameters and su er the most from sparse data problems.
15:171	In spite of using a Collins-style model as the basis for analysis, throughout this paper, we will attempt to present information that is widely applicable because it pertains to properties of the widely-used Treebank (Marcus et al. , 1993) and lexicalized parsing models in general.
16:171	This work also sheds light on the much-discussed bilexical dependencies of statistical parsing models.
17:171	Beginning with the seminal work at IBM (Black et al. , 1991; Black et al. , 1992b; Black et al. , 1992a), and continuing with such lexicalist approaches as (Eisner, 1996), these features have been lauded for their ability to approximate a words semantics as a means to override syntactic preferences with semantic ones (Collins, 1999; Eisner, 2000).
18:171	However, the work of Gildea (2001) showed that, with an approximate reimplementation of Collins Model 1, removing all parameters that involved dependencies between a modifier word and its head resulted in a surprisingly small decrease in overall parse accuracy.
19:171	The prevailing assumption was then that such bilexical statistics were not useful for making syntactic decisions, although it was not entirely clear why.
20:171	Subsequently, we replicated Gildeas experiment with a complete emulation of Model 2 and presented additional evidence that bilexical statistics were barely getting used during decoding (Bikel, 2004), appearing to confirm the original result.
21:171	However, the present work will show that such statistics do get frequently used for the highest-probability parses, but that when a Collinsstyle model generates modifier words, the bilexical parameters are so similar to their back-o distributions as to provide almost no extra predictive information.
22:171	2 Motivation A parsing model coupled with a decoder (an algorithm to search the space of possible trees for a given terminal sequence) is largely an engineering e ort.
23:171	In the end, the performance of the parser with respect to its evaluation criteriatypically accuracy, and perhaps also speedare all that matter.
24:171	Consequently, the engineer must understand what the model is doing only to the point that it helps make the model perform better.
25:171	Given the somewhat crude method of determining a features benefit by testing a model with and without the feature, a researcher can argue for the e cacy of that feature without truly understanding its e ect on the model.
26:171	For example, while adding a particular feature may improve parse accuracy, the reason may have little to do with the nature of the feature and everything to do with its canceling other features that were theretofore hurting performance.
27:171	In any case, since this is engineering, the rationalization for a feature is far less important than the models overall performance increase.
28:171	On the other hand, science would demand that, at some point, we analyze the multitude of features in a state-of-the-art lexicalized statistical parsing model.
29:171	Such analysis is warranted for two reasons: replicability and progress.
30:171	The first is a basic tenet of most sciences: without proper understanding of what has been done, the relevant experiment(s) cannot be replicated and therefore verified.
31:171	The second has to do with the idea that, when a discipline matures, it can be di cult to determine what new features can provide the most gain (or any gain, for that matter).
32:171	A thorough analysis of the various distributions being estimated in a parsing model allows researchers to discover what is being learned most and least well.
33:171	Understanding what is learned most well can shed light on the types of features or dependencies that are most e cacious, pointing the way to new features of that type.
34:171	Understanding what is learned least well defines the space in which to look for those new features.
35:171	3 Frequencies 3.1 Definitions and notation In this paper we will refer to any estimated distribution as a parameter that has been instantiated from a parameter class.
36:171	For example, in an ngram language model, p(wijwi 1) is a parameter class, whereas the estimated distribution p( jthe) is a particular parameter from this class, consisting of estimates of every word that can follow the word the.
37:171	For this work, we used the model described in (Bikel, 2002; Bikel, 2004).
38:171	Our emulation of Collins Model 2 (hereafter referred to simply as the model) has eleven parameter classes, each of which employs up to three back-o levels, where back-o level 0 is just the un-backed-o  maximal context history.1 In other words, a smoothed probability estimate is the interpolation of up to three di erent unsmoothed estimates.
39:171	The notation and description for each of these parameter classes is shown in Table 1.
40:171	3.2 Basic frequencies Before looking at the number of parameters in the model, it is important to bear in mind the amount of data on which the model is trained and on which actual parameters will be induced from parameter classes.
41:171	The standard training set for English consists of Sections 0221 of the Penn Treebank, which in turn consist of 39,832 sentences with a total of 950,028 word tokens (not including null elements).
42:171	There are 44,113 unique words (again, not including null elements), 10,437 of which occur 6 times or more.2 The trees consist of 904,748 brackets with 28 basic nonterminal labels, to which function tags such as -TMP and indices are added in the data to form 1184 observed nonterminals, not including preterminals.
43:171	After tree transformations, the model maps these 1184 nonterminals down to just 43.
44:171	There are 42 unique part of speech tags that serve as preterminals in the trees; the model prunes away three of these (, and.).
45:171	Induced from these training data, the model contains 727,930 parameters; thus, there are nearly as many parameters as there are brackets or word tokens.
46:171	From a history-based grammar perspective, there are 727,930 types of history contexts from which futures are generated.
47:171	However, 401,447 of these are singletons.
48:171	The average count for a history context is approximately 35.56, while the average diversity is approximately 1.72.
49:171	The model contains 1,252,280 unsmoothed maximum-likelihood probability estimates (727; 930 1:72 1; 252; 280).
50:171	Even when a given future was not seen with a particular history, it is possible that one of its associated 1Collins model splits out the PM and PM w classes into leftand right-specific versions, and has two additional classes for dealing with coordinating conjunctions and inter-phrasal punctuation.
51:171	Our emulation of Collins model incorporates the information of these specialized parameter classes into the existing PM and PMw parameters.
52:171	2We mention this statistic because Collins thesis experiments were performed with an unknown word threshold of 6.
53:171	Notation Description No.
54:171	of back-o levels PH Generates unlexicalized head child given lexicalized parent 3 PsubcatL Generates subcat bag on left side of head child 3 PsubcatR Generates subcat bag on right side of head child 3 PM (PM;NPB) Generates partially-lexicalized modifying nonterminal (withNPBparent) 3 PMw (PMw;NPB) Generates head word of modifying nonterminal (withNPBparent) 3 PpriorNT Priors for nonterminal conditioning on its head word and part of speech 2 Ppriorlex Priors for head word/part of speech pairs (unconditional probabilities) 0 PTOPNT Generates partially-lexicalized child of+TOP+y 1 PTOPw Generates the head word for children of+TOP+y 2 Table 1: All eleven parameter classes in our emulation of Collins Model 2.
55:171	A partially-lexicalized nonterminal is a nonterminal label and its head words part of speech (such asNP(NN)).
56:171	yThe hidden nonterminal +TOP+is added during training to be the parent of every observed tree.
57:171	PP(IN/with) IN(IN/with) {NPA} NPA(NN/: : : ) Figure 1: A frequent PMw history context, illustrated as a tree fragment.
58:171	The : : : represents the future that is to be generated given this history.
59:171	back-o contexts was seen with that future, leading to a non-zero smoothed estimate.
60:171	The total number of possible non-zero smoothed estimates in the model is 562,596,053.
61:171	Table 2 contains count and diversity statistics for the two parameter classes on which we will focus much of our attention, PM and PMw.
62:171	Note how the maximal-context back-o levels (level 0) for both parameter classes have relatively little training: on average, raw estimates are obtained with history counts of only 10.3 and 4.4 in the PM and PMw classes, respectively.
63:171	Conversely, observe how drastically the average number of transitions n increases as we remove dependence on the head word going from back-o level 0 to 1.
64:171	3.3 Exploratory data analysis: a common distribution To begin to get a handle on these distributions, particularly the relatively poorly-trained and/or highentropy distributions of the PMw class, it is useful to perform some exploratory data analysis.
65:171	Figure 1 illustrates the 25th-most-frequent PMw history context as a tree fragment.
66:171	In the top-down model, the following elements have been generated: a parent nonterminal PP(IN/with) (a PP headed by the word with with the part-ofspeech tagIN) the parents head childIN a right subcat bag containingNP-A(a single NP argument must be generated somewhere on the 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0 500 1000 1500 2000 2500 3000 3500 cummulative density rank Figure 2: Cumulative density function for the PMw history context illustrated in Figure 1.
67:171	right side of the head child) a partially-lexicalized right-modifying nonterminal At this point in the process, a PMw parameter conditioning on all of this context will be used to estimate the probability of the head word of the NP-A(NN), completing the lexicalization of that nonterminal.
68:171	If a candidate head word was seen in training in this configuration, then it will be generated conditioning on the full context that crucially includes the head wordwith; otherwise, the model will back o to a history context that does not include the head word.
69:171	In Figure 2, we plot the cumulative density function of this history context.
70:171	We note that of the 3258 words with non-zero probability in this context, 95% of the probability mass is covered by the 1596 most likely words.
71:171	In order to get a better visualization of the probability distribution, we plotted smoothed probability estimates versus the training-data frequencies of the words being generated.
72:171	Figure 3(a) shows smoothed estimates that make use of the full context (i.e. , include the head word with) wherever possible, and Figure 3(b) shows smoothed estimates that do not use the head word.
73:171	Note how the plot in Figure 3(b) appears remarkably similar to the true distribuBack-o PM PMw level c d n c d n 0 10:268 1:437 7:145 4:413 1:949 2:264 1 558:047 3:643 153:2 60:19 8:454 7:120 2 1169:6 5:067 230:8 21132:1 370:6 57:02 Table 2: Average counts and diversities of histories of the PM and PMw parameter classes.
74:171	c and d are average history count and diversity, respectively.
75:171	n = cd is the average number of transitions from a history context to some future.
76:171	1e-06 1e-05 0.0001 0.001 0.01 0.1 1 10 100 1000 10000smoothed probability estimate word frequency (a) prob.
77:171	vs. word freq.
78:171	, back-o level 1 1e-06 1e-05 0.0001 0.001 0.01 0.1 1 10 100 1000 10000 100000smoothed probability estimate word frequency (b) prob.
79:171	vs. word freq.
80:171	, back-o level 2 Figure 3: Probability versus word frequency for head words ofNP-A(NN)in thePPconstruction.
81:171	tion of 3(a).
82:171	3(b) looks like a slightly compressed version of 3(b) (in the vertical dimension), but the shape of the two distributions appears to be roughly the same.
83:171	This observation will be confirmed and quantified by the experiments of 5.3 4 Entropies A good measure of the discriminative e cacy of a parameter is its entropy.
84:171	Table 3 shows the average entropy of all distributions for each parameter class.4 By far the highest average entropy is for the PMw parameter class.
85:171	Having computed the entropy for every distribution in every parameter class, we can actually plot a meta-distribution of entropies for a parameter class, as shown in Figure 4.
86:171	As an example of one of the data points of Figure 4, consider the history context explored in the previous section.
87:171	While it may be one of the most frequent, it also has the highest entropy at 9.141 3The astute reader will further note that the plots in Figure 3 both look bizarrely truncated with respect to low-frequency words.
88:171	This is simply due to the fact that all words below a fixed frequency are generated as the+UNKNOWN+word.
89:171	4The decoder makes use of two additional parameter classes that jointly estimate the prior probability of a lexicalized nonterminal; however, these two parameter classes are not part of the generative model.
90:171	PH 0:2516 PTOPNT 2:517 PsubcatL 0:02342 PTOPw 2:853 PsubcatR 0:2147 PM 1:121 PMw 3:923 Table 3: Average entropies for each parameter class.
91:171	0 1 2 3 4 5 6 7 8 9 10 0 50000 100000 150000 200000 250000 entropy rank Figure 4: Entropy distribution for the PMw parameters.
92:171	bits, as shown by Table 4.
93:171	This value not only confirms but quantifies the long-held intuition that PP-attachment requires more than just the local phrasal context; it is, e.g., precisely why the PPspecific features of (Collins, 2000) were likely to be very helpful, as cases such as these are among the most di cult that the model must discriminate.
94:171	In fact, of the top 50 of the highest-entropy Back-o PM PMw level min max avg median min max avg median 0 3.080E-10 4.351 1.128 0.931 4.655E-8 9.141 3.904 3.806 1 4.905E-7 4.254 0.910 0.667 2.531E-6 9.120 4.179 4.224 2 8.410E-4 3.501 0.754 0.520 0.002 8.517 3.182 2.451 Overall 3.080E-10 4.351 1.121 0.917 4.655E-8 9.141 3.922 3.849 Table 4: Entropy distribution statistics for PM and PMw . Figure 5: Total modifier wordgeneration entropy broken down by parent-head-modifier triple.
95:171	distributions from PMw, 25 involve the configuration PP --> IN(IN/<prep>)NP-A(NN/: : :), where<prep>is some preposition whose tag isIN.
96:171	Somewhat disturbingly, these are also some of the most frequent constructions.
97:171	To gauge roughly the importance of these high-frequency, high-entropy distributions, we performed the following analysis.
98:171	Assume for the moment that every word-generation decision is roughly independent from all others (this is clearly not true, given head-propagation).
99:171	We can then compute the total entropy of word-generation decisions for the entire training corpus via HPMw = X c2PMw f (c) H(c) (1) where f (c) is the frequency of some history context c and H(c) is that contexts entropy.
100:171	The total modifier word-generation entropy for the corpus with the independence assumption is 3,903,224 bits.
101:171	Of these, the total entropy for contexts of the formPP ! IN NP-Ais 618,640 bits, representing a sizable 15.9% of the total entropy, and the single largest percentage of total entropy of any parenthead-modifier triple (see Figure 5).
102:171	On the opposite end of the entropy spectrum, there are tens of thousands of PMw parameters with extremely low entropies, mostly having to do with extremely low-diversity, low-entropy part-ofspeech tags, such asDT,CC,INorWRB.
103:171	Perhaps even more interesting is the number of distributions with identical entropies: of the 206,234 distributions, there are only 92,065 unique entropy values.
104:171	Distributions with the same entropy are all candidates for removal from the model, because most of their probability mass resides in the back-o distribution.
105:171	Many of these distributions are lowor one-count history contexts, justifying the common practice of removing transitions whose history count is below a certain threshold.
106:171	This practice could be made more rigorous by relying on distributional similarity.
107:171	Finally, we note that the most numerous low-entropy distributions (that are not trivial) involve generating right-modifier words of the head child of an SBAR parent.
108:171	The model is able to learn these constructions extremely well, as one might expect.
109:171	5 Distributional similarity and bilexical statistics We now return to the issue of bilexical statistics.
110:171	As alluded to earlier, Gildea (2001) performed an experiment with his partial reimplementation of Collins Model 1 in which he removed the maximal-context back-o level from PMw, which e ectively removed all bilexical statistics from his model.
111:171	Gildea observed that this change resulted in only a 0.5% drop in parsing performance.
112:171	There were two logical possibilities for this behavior: either such statistics were not getting used due to sparse data problems, or they were not informative for some reason.
113:171	The prevailing view of the NLP community had been that bilexical statistics were sparse, and Gildea (2001) adopted this view to explain his results.
114:171	Subsequently, we duplicated Gildeas experiment with a complete emulation of Collins Model 2, and found that when the decoder requested a smoothed estimate involving a bigram when testing on held-out data, it only received an estimate that made use of bilexical statistics a mere 1.49% of the time (Bikel, 2004).
115:171	The conclusion was that the minuscule drop in performance from removing bigrams must have been due to the fact that they were barely able to be used.
116:171	In other words, it appeared that bigram coverage was not nearly good enough for bigrams to have an impact on parsing performance, seemingly confirming the prevailing view.
117:171	But the 1.49% figure does not tell the whole story.
118:171	The parser pursues many incorrect and ultimately low-scoring theories in its search (in this case, using probabilistic CKY).
119:171	So rather than asking how many times the decoder makes use of bigram statistics on average, a better question is to ask how many times the decoder can use bigram statistics while pursuing the top-ranked theory.
120:171	To answer this question, we used our parser to constrain-parse its own output.
121:171	That is, having trained it on Sections 0221, we used it to parse Section 00 of the Penn Treebank (the canonical development test set) and then re-parse that section using its own highestscoring trees (without lexicalization) as constraints, so that it only pursued theories consistent with those trees.
122:171	As it happens, the number of times the decoder was able to use bigram statistics shot up to 28.8% overall, with a rate of 22.4% for NPB constituents.
123:171	So, bigram statistics are getting used; in fact, they are getting used more than 19 times as often when pursuing the highest-scoring theory as when pursuing any theory on average.
124:171	And yet there is no disputing the fact that their use has a surprisingly small e ect on parsing performance.
125:171	The exploratory data analysis of 3.3 suggests an explanation for this perplexing behavior: the distributions that include the head word versus those that do not are so similar as to make almost no di erence in terms of parse accuracy.
126:171	5.1 Distributional similarity A useful metric for measuring distributional similarity, as explored by (Lee, 1999), is the JensenShannon divergence (Lin, 1991): JS (p kq)= 12 h D p avgp;q +D q avgp;q i (2) where D is the Kullback-Leibler divergence (Cover and Thomas, 1991) and where avgp;q = 1 2 (p(A)+q(A)) for an event A in the event spaceof at least one of the two distributions.
127:171	One interpretation for the Jensen-Shannon divergence due to Slonim et al.128:171	(2002) is that it is related to the loglikelihood that the two sample distributions originate by the most likely common source, relating the quantity to the two-sample problem.
129:171	In our case, we have p = p(yjx1; x2) and q = p(yjx1), where y is a possible future and x1; x2 are elements of a history context, with q representing a back-o distribution using less context.
130:171	Therefore, whereas the standard JS formulation is agnosmin max avg.
131:171	median JS 0 1 2.729E-7 2.168 0.1148 0.09672 JS 1 2 0.001318 1.962 0.6929 0.6986 JS 0 2 0.001182 1.180 0.3774 0.3863 Table 5: Jensen-Shannon statistics for back-o parameters in PMw . tic with respect to its two distributions, and averages them in part to ensure that the quantity is defined over the entire space, we have the prior knowledge that one history context is a superset of the other, thathx1iis defined whereverhx1; x2iis.
132:171	In this case, then, we have a simpler, one-sided definition for the Jensen-Shannon divergence, but generalized to the multiple distributions that include an extra history component: JS (p kq)=X x2 p(x2) D (p(yjx1; x2) kp(yjx1)) = Ex2 D (p(yjx1; x2) kp(yjx1)) (3) An interpretation in our case is that this is the expected number of bits x2 gives you when trying to predict y.5 If we allow x2 to represent an arbitrary amount of context, then the Jensen-Shannon divergence JS b a = JS (pbjjpa) can be computed for any two back-o levels, where a; b are back-o levels s.t. b < a (meaning pb is a distribution using more context than pa).
133:171	The actual value in bits of the Jensen-Shannon divergence between two distributions should be considered in relation to the number of bits of entropy of the more detailed distribution; that is, JS b a should be considered relative to H(pb).
134:171	Having explored entropy in 4, we will now look at some summary statistics for JS divergence.
135:171	5.2 Results We computed the quantity in Equation 3 for every parameter in PMw that used maximal context (contained a head word) and its associated parameter that did not contain the head word.
136:171	The results are listed in Table 5.
137:171	Note that, for this parameter class with a median entropy of 3.8 bits, we have a median JS divergence of only 0.097 bits.
138:171	The distributions are so similar that the 28.8% of the time that the decoder uses an estimate based on a bigram, it might as well be using one that does not include the head word.
139:171	5Or, following from Slonim et al.s interpretation, this quantity is the (negative of the) log-likelihood that all distributions that include an x2 component come from a common source that does not include this component.
140:171	40 words 00 23 Model LR LP LR LP m3 n/a n/a 88.6 88.7 m2-emu 89.9 90.0 88.8 88.9 reduced 90.0 90.2 88.7 88.9 all sentences Model 00 23 m3 n/a n/a 88.0 88.3 m2-emu 88.8 89.0 88.2 88.3 reduced 89.0 89.0 88.0 88.2 Table 6: Parsing results on Sections 00 and 23 with Collins Model 3, our emulation of Collins Model 2 and the reduced version at a threshold of 0.06.
141:171	LR =labeled recall, LP=labeled precision.6 6 Distributional Similarity and Parameter Selection The analysis of the previous two sections provides a window onto what types of parameters the parsing model is learning most and least well, and onto what parameters carry more and less useful information.
142:171	Having such a window holds the promise of discovering new parameter types or features that would lead to greater parsing accuracy; such is the scientific, or at least, the forward-minded research perspective.
143:171	From a much more purely engineering perspective, one can also use the analysis of the previous two sections to identify individual parameters that carry little to no useful information and simply remove them from the model.
144:171	Specifically, if pb is a particular distribution and pb+1 is its corresponding back-o distribution, then one can remove all parameters pb such that JS (pbjjpb+1) H(pb) < t; where 0 < t < 1 is some threshold.
145:171	Table 6 shows the results of this experiment using a threshold of 0:06.
146:171	To our knowledge, this is the first example of detailed parameter selection in the context of a generative lexicalized statistical parsing model.
147:171	The consequence is a significantly smaller model that performs with no loss of accuracy compared to the full model.6 Further insight is gained by looking at the percentage of parameters removed from each parameter class.
148:171	The results of (Bikel, 2004) suggested that the power of Collins-style parsing models did not 6None of the di erences between the Model 2emulation results and the reduced model results is statistically significant.
149:171	PH 13:5% PTOPw 0:023% PsubcatL 0:67% PM 10:1% PsubcatR 1:8% PMw 29:4% Table 7: Percentage of parameters removed from each parameter class for the 0.06-reduced model.
150:171	lie primarily with the use of bilexical dependencies as was once thought, but in lexico-structural dependencies, that is, predicting syntactic structures conditioning on head words.
151:171	The percentages of Table 7 provide even more concrete evidence of this assertion, for whereas nearly a third of the PMw parameters were removed, a much smaller fraction of parameters were removed from the PsubcatL, PsubcatR and PM classes that generate structure conditioning on head words.
152:171	7 Discussion Examining the lower-entropy PMw distributions revealed that, in many cases, the model was not so much learning how to disambiguate a given syntactic/lexical choice, but simply not having much to learn.
153:171	For example, once a partially-lexicalized nonterminal has been generated whose tag is fairly specialized, such asIN, then the model has painted itself into a lexical corner, as it were (the extreme example isTO, a tag that can only be assigned to the word to).
154:171	This is an example of the label bias problem, which has been the subject of recent discussion (La erty et al. , 2001; Klein and Manning, 2002).
155:171	Of course, just because there is label bias does not necessarily mean there is a problem.
156:171	If the decoder pursues a theory to a nonterminal/partof-speech tag preterminal that has an extremely low entropy distribution for possible head words, then there is certainly a chance that it will get stuck in a potentially bad theory.
157:171	This is of particular concern when a head wordwhich the top-down model generates at its highest point in the treeinfluences an attachment decision.
158:171	However, inspecting the lowentropy word-generation histories of PMw revealed that almost all such cases are when the model is generating a preterminal, and are thus of little to no consequence vis-a-vis syntactic disambiguation.
159:171	8 Conclusion and Future Work With so many parameters, a lexicalized statistical parsing model seems like an intractable behemoth.
160:171	However, as statisticians have long known, an excellent angle of attack for a mass of unruly data is exploratory data analysis.
161:171	This paper presents some of the first data visualizations of parameters in a parsing model, and follows up with a numerical analysis of properties of those distributions.
162:171	In the course of this analysis, we have focused in on the question of bilexical dependencies.
163:171	By constrainparsing the parsers own output, and by hypothesizing and testing for distributional similarity, we have presented evidence that finally explains that (a) bilexical statistics are actually getting used with great frequency in the parse theories that will ultimately have the highest score, but (b) the distributions involving bilexical statistics are so similar to their back-o counterparts as to make them nearly indistinguishable insofar as making di erent parse decisions.
164:171	Finally, our analysis has provided for the first time an e ective way to do parameter selection with a generative lexicalized statistical parsing model.
165:171	Of course, there is still much more analysis, hypothesizing, testing and extrapolation to be done.
166:171	A thorough study of the highest-entropy distributions should reveal new ways in which to use grammar transforms or develop features to reduce the entropy and increase parse accuracy.
167:171	A closer look at the low-entropy distributions may reveal additional reductions in the size of the model, and, perhaps, a way to incorporate hard constraints without disturbing the more ambiguous parts of the model more suited to machine learning than human engineering.
168:171	9 Acknowledgements Thanks to Mitch Marcus, David Chiang and Julia Hockenmaier for their helpful comments on this work.
169:171	I would also like to thank Bob Moore for asking some insightful questions that helped prompt this line of research.
170:171	Thanks also to Fernando Pereira, with whom I had invaluable discussions about distributional similarity.
171:171	This work was supported in part by DARPA grant N66001-00-1-9815.

