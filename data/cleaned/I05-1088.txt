R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 1017
 
–
 
1028, 2005. 
© Springer-Verlag Berlin Heidelberg 2005 
Extracting Terminologically Relevant Collocations in the 
Translation of Chinese Monograph* 
Byeong-Kwu Kang, Bao-Bao Chang, Yi-Rong Chen, and Shi-Wen Yu 
The Institute of Computational Linguistics, Peking University, Beijing, 100871g14808China 
{kbg43, chbb, chenyr, yusw}@pku.edu.cn 
Abstract. This paper suggests a methodology which is aimed to extract the 
terminologically relevant collocations for translation purposes. Our basic idea is 
to use a hybrid method which combines the statistical method and linguistic 
rules. The extraction system used in our work operated at three steps: (1) 
Tokenization and POS tagging of the corpus; (2) Extraction of multi-word units 
using statistical measure; (3) Linguistic filtering to make use of syntactic 
patterns and stop-word list. As a result, hybrid method using linguistic filters 
proved to be a suitable method for selecting terminological collocations, it has 
considerably improved the precision of the extraction which is much higher 
than that of purely statistical method. In our test, hybrid method combining 
“Log-likelihood ratio” and “linguistic rules” had the best performance in the 
extraction. We believe that terminological collocations and phrases extracted in 
this way, could be used effectively either to supplement existing terminological 
collections or to be used in addition to traditional reference works. 
1   Introduction 
Communication between different individuals and nations is not always easy, 
especially when more than one language is involved. This kind of communication can 
include translation problems, which can be solved by the translators who bridge the 
gap between two different languages.  
Through the past decade, China and Korea have been undergoing large economic, 
cultural exchange, which invariably affects all aspects of communication, particularly 
translation. New international contacts, foreign investments as well as cross-cultural 
communication have caused an enormous increase in the volume of translations 
produced and required. But by now, most of all this translation work has been 
conducted by translators alone, which bears the burden of an enormous translation 
task to them.  
In order to accomplish these tasks with maximum efficiency and quality, a new 
translation method supported by computer technology has been suggested. MAHT, 
also known as computer-assisted translation involves some interaction between 
translator and the computer. It seems to be more suited for the needs of many 
                                                           
*  This work has been supported by The National Basic Research Program of China(973 
program, No. 2004CB318102) and the 863 program (No. 2001AA114210, 2002AA117010). 
1018 B.-K. Kang et al. 
organizations which have to handle the translation of the documents.  Computer-
assisted translation systems are based on “translation memory” and “terminology 
databases”. With translation memory tools, translators have immediate access to 
previous translations of the text, which they can then accept or modify.   
Terminology management systems also can prove very useful in supporting 
translator’s work [2, 11]. Most translators use some sort of glossary or terminology 
database, especially in the translation of the technical documents or academic 
monograph. Many translation bureaux have the collection of the terminology data 
bases. But time pressure and costs make it difficult to get glossary building task done 
fully manually. Thus there is a pressing need for the tool which is computationally 
supported. For Chinese, other than for English, terminology management tools are not 
so sophisticated that they could provide wide enough coverage to be directly usable 
for the translators.  
We are contemplating, in this article, situations where computational support is 
sought to extract the term candidate, construct or enhance such terminology 
databases. Our work will be more focused on the problem of terminologically relevant 
collocation extraction.  
In order to extract multiword terms from the domain corpus, three main strategies 
have been proposed in the literature. First, linguistic rule-based systems propose to 
extract relevant terms by making use of parts of speech, lexicons, syntax or other 
linguistic structure [2, 4]. This methodology is language dependent rather than 
language independent, and the system requires highly specialized linguistic 
techniques to identify the possible candidate terms. Second, purely statistical systems 
extract discriminating multiword terms from the text corpora by means of association 
measures [5, 6, 7]. As they use plain text corpora and only require the information 
appearing in texts, such systems are highly flexible and extract relevant units 
independently from the domain and the language of the input text. Finally, hybrid 
methodologies define co-occurrences of interest in terms of syntactical patterns and 
statistical regularities [1, 3, 9].  
There is no question that the term extraction work comes into play when the tools 
are parameterized in such a way as to provide as much relevant material (maximizing 
recall and precision), and as little “noise” as possible.  As seen in the literature, 
neither purely rule-based approach nor statistic based approach could bring an 
encouraging result alone[3, 4]. The main problem is the "noise".  So we need to find a 
combined technique for reducing this “noise”.  In this paper, we have taken a hybrid 
approach which combines the linguistic rules and statistical method. First, we applied 
a linguistic filter which selects candidates from the corpus. Second, the statistical 
method was used to extract the word class  combinations. And then, the results of 
several experiments were evaluated and compared with each other. 
2   Methodology Overview 
The basic idea in our work is that the extraction tool operates on pre-processed corpus 
which contains the results of tokenizing word and word class annotation (POS-
tagging). Figure1 contains an annotated sentence from one of the Chinese academic 
monograph[18]. 
 Extracting Terminologically Relevant Collocations in the Translation 1019 
<s id=2> 
g13520g8981/p  g9248g2739/n  g8610g7540/n  g8859/u  g6371g8905/d  g2878g5495g3451/v  g14808/w  g2673g18649/n  g12263g6634g12263/d  g31542
g32576/a  g4094/u  g5079g6584/v  g8618/p  g10564g8060/n  g20863g2018/n  g3627/p  g20787g4905g6869/n  g2660g7545/v  g2878g5495/n  g1941
/w 
Fig. 1. Sample annotated text (tagged by the Peking University Tagger) 
And the extraction routine used in our work operated at three steps: 
(1)Tokenization and POS Tagging; (2)Extraction of the candidates from the corpus; 
(3)Linguistic  filtering(making use of syntactic patterns and stop-word list). The 
schema in Figure2 summarizes the three steps of pre-processing and extracting the 
term candidate. The extraction is automatic once the appropriate templates are 
designed. 
 
Fig. 2. Simplified schema of term extraction from a corpus 
3   Statistical Method 
Statistical methods in computational linguistics generally share the fundamental 
approach to language viewed as a string of characters, tokens or other units, where 
patterns are discovered on the basis of their recurrence and co-occurrence. 
Accordingly, when we approach the extraction of multi-word terms from a statistical 
point of view, we initially retrieve the word sequences which are not only frequent in 
their occurrence but also collocating each other.  
Before a statistical methodology could be developed, some characteristics of terms 
in Chinese had to be established. In Chinese, the length of terms can vary from single 
word to multi-words(n-gram), with the majority of entries being less than 4-word 
items, usually two word items(bi-gram) (See in 4.3). The number of n-grams with n>4 
Raw Corpus 
Annotated  text 
List of extraction result 
List of Term candidate 
filtered
Step 2. Extraction based on statistical information
Step 3.    Linguistic  filtering  
Step 1.   Tokenization and POS Tagging 
1020 B.-K. Kang et al. 
is very small, and the occurrence of which is also rare. Therefore, the problems of bi-
grams, tri-grams and 4-grams are primarily taken into considerations in our work. 
Now let us consider the correlation between two neighboring words A and B. 
Assuming that these two words are terminologically relevant units, we can intuitively 
expect that they occur more often than random chance. From a statistical point of 
view, this probability can be measured by several statistical methods, such as “co-
occurrence frequency”, “Mutual Information”, “Dice coefficient”, “Chi-square test”, 
“log-likelihood”, etc[1, 6, 15]. 
Table 1 lists several statistical measures which have been widely used in extracting 
collocations. In table 1: XY represents any two word itemg19463 X stands for all words 
except Xg19463  N is the size of corpusg19463
X
f  and 
X
P  are frequency and probability of X 
respectivelyg19463
XY
f  and 
XY
P  are frequency and probability of XY respectivelyg19195And 
assuming that two words X and Y are independent of each other, the formulas are 
represented as follows: 
Table 1. Statistical methods used in multi word extraction 
Method Formula 
Frequency(Freq) 
XY
f  
Mutual Information 
(MI) 
2
log
XY
XY
P
PP
 
Dice Formula 
(Dice) 
2
XY
XY
f
ff+
 
Log-likelihood(Log-L) 
()
2log
()( )
Y
XY XY
f
XY
XY
f
f
XY
XY X Y XY
PPPP
PP PP
−  
Chi-squared(Chi) 
2
()
()()()()
XY
XY X Y XY
XY XY
XY XY XY XY XY XY
Nf f f f
ffffffff
−
++++
 
For the purposes of this work, we used these five statistics to measure the 
correlation of neighboring words. The statistical criterion of judgments is the value of 
measures which can judge the probability whether they belong to the rigid 
collocations or not. From a statistical point of view, we can say that if the value of 
measure is high, the two word combination is more likely to be a rigid collocation. 
And XY could be accepted as a collocation if its statistical value is larger than a given 
threshold. Those bi-gram candidates with correlation coefficient smaller than a pre-
defined threshold are considered to occur randomly and should be discarded. Others 
are sorted according to their correlation coefficient in descending order.  
Tri-gram and 4-gram candidates were processed in the same way. To compute the 
correlation coefficient of all tri-grams, we just considered a tri-gram as the 
 Extracting Terminologically Relevant Collocations in the Translation 1021 
combination of one bi-gram and one word, and then calculated their correlation 
coefficient. Similarly, a 4-gram was considered either as the combination of a tri-
gram and a word, or the combination of two bi-grams [12].  
As mentioned before, our methodology was tested on pre-processed corpus which 
contained the result of word class annotation. The extraction test was delivered on word 
sequence (POS tags) combinations. And the test corpus was a Chinese academic 
monograph [18]. The size of this corpus is 0.2 million Chinese characters, including 
about 5,000 sentences. In our test, the extraction of multi-word units was based on 
65,663 candidate bi-grams. Among these candidates, when their correlation coefficients 
were higher than a given threshold, they were considered as multi-word unit, and then 
sorted in descending order. The results of experiment are shown in Figure3. 
g19
g20g19
g21g19
g22g19
g23g19
g24g19
g25g19
g26g19
g27g19
g28g19
g20g19g19 g21g19g19 g22g19g19 g23g19g19 g24g19g19 g20g19g19g19
g43g76g74g75g79g92g3g89g68g79g88g72g71g3g70g68g81g71g76g71g68g87g72g86
g51
g85
g72
g70
g76
g86
g76
g82
g81
g11
g8
g12
g48g44
g39g44g38g40
g47g82g74g47
g38g75g76
 
Fig. 3. Comparison of Extraction Performance between different statistical measures 
Table 2. The sample result sorted by Chi Square value 
g20g86g87g58g82g85g71g3 g21g81g71g58g82g85g71g3 g38g75g76g3 g47g82g74g47g3 g39g44g38g40g3 g48g44g3
g2878g5495g3 g16801g3158g3 g26g27g21g21g17g20g23g3 g20g21g26g27g17g23g27g3 g24g20g26g17g24g27g20g3 g24g17g21g27g20g27g22g3
g7186g10628g3 g3252g6634g3 g23g21g22g22g17g23g22g3 g23g21g17g27g22g23g27g3 g21g24g21g19g3 g20g19g17g23g25g22g25g3
g13560g2773g3 g12930g16801g3 g22g19g27g24g17g25g23g3 g20g25g19g17g25g23g26g3 g23g24g25g19g3 g26g17g24g28g28g21g24g3
g4668g7313g3 g4271g3 g20g23g25g20g17g23g20g3 g23g21g23g17g27g20g27g3 g26g25g26g17g22g25g3 g22g17g28g19g27g28g20g3
g6990g5917g3 g4094g3 g27g19g28g17g20g25g27g3 g22g27g17g21g21g21g25g3 g27g23g23g3 g26g17g25g25g28g25g23g3
g17247g3632g3 g2172g16801g3 g26g24g21g17g25g22g26g3 g20g21g23g17g22g24g22g3 g26g27g26g17g21g23g22g3 g24g17g20g25g20g26g22g3
g4298g3359g3 g2636g3 g25g20g28g17g25g28g23g3 g20g20g20g17g24g21g26g3 g20g25g19g19g3 g24g17g19g21g20g28g23g3
g17728g6454g3 g1038g3 g24g27g21g17g23g21g24g3 g24g21g17g19g22g23g20g3 g24g20g25g17g23g23g23g3 g25g17g23g19g24g19g20g3
g2567g3627g3 g8859g3 g24g23g28g17g20g20g28g3 g21g27g25g17g27g27g23g3 g20g26g19g22g26g17g20g3 g21g17g25g25g28g19g25g3
g8257g5589g3 g16801g3 g22g22g25g17g21g27g22g3 g24g27g17g19g26g24g26g3 g21g20g25g25g17g25g26g3 g24g17g20g22g21g27g20g3
g7609g3 g16801g3158g3 g21g28g25g17g20g28g25g3 g24g21g17g27g24g23g20g3 g24g23g23g17g22g23g27g3 g23g17g28g25g26g23g23g3
g2621g3 g6426g3 g21g21g27g17g24g28g25g3 g20g21g21g17g20g20g28g3 g24g21g22g17g24g28g26g3 g21g17g21g25g25g26g3
1022 B.-K. Kang et al. 
An examination of the results first showed a significant difference in precision. 
Checked by hand, the precisions of Chi-square value and Log-likelihood ratio were 
relatively high. In contrast, the precisions of Mutual information and Dice formula 
were not so ideal.  
Considering the size of the corpus and the terminological richness of the texts, this 
result is not very encouraging. Regardless of any statistical measure, the precision and 
coverage of the extraction are not so high that could be directly used in the application 
system.  
More over, as shown in table 2, the purely statistical system extracts all multi-word 
units regardless of their types, so that we can also find sequences like “g4298g3359
[zengjia](add)g2636[le](auxiliary word)”, “g2567g3627[butong](different)g8859[de](auxiliary 
word)”, “g2621[ye](also)g6426[shi](be)”, “g21078g19368[zhuanhuan](change)g18592[wei](become)”, etc., 
for which we have no use in terminology. Clearly the output must be thoroughly 
filtered before the result can be used in any productive way. 
On the whole, the somewhat disappointing outcome of the statistical method 
provoked us to rethink the methodology and tried to include more linguistic 
information in the extraction of terminology. 
4   Hybrid Method Combining Statistical Method and Linguistic 
Rules 
To improve the precision and recall of the extraction system, it was decided to use 
two criteria determining whether a sequence was terminologically relevant or not. The 
first was to use the frequent syntactic patterns of terms. The idea underlying this 
method is that multi-word terms are constructed according to more or less fixed 
syntactic patterns, and if such patterns are identified for each language, it is possible 
to extract them from a POS tagged corpus. The second was to use a stop-word filter 
that a term can never begin or end in a stop-word. This would filter out things not 
relevant with the domain-specific collocation or term. 
4.1   Syntactic Patterns of Terms in Chinese 
Before a methodology for extracting the terminologically relevant word units could be 
developed, some characteristics of terms in Chinese had to be established. We were 
especially interested in the following: How many words do terms usually have in 
Chinese? What is the structure of multi-word units in terms of syntax and morphology? 
What kind of terms can be successfully retrieved by computational methods?  
To find answers to the above questions, an existing terminology database could be 
used as a sample. Because the source text to be tested in our work is related with 
computational or linguistic domain, we selected the terminology database of 
computational linguistics which was constructed by Peking University. This term 
bank currently contains over 6,500 entries in English and Chinese.  
An analysis of 6,500 term entries in Chinese showed that the length of terms can 
vary from 1 to over 6 words, with the majority of entries being two-word items, 
usually a “noun+noun” sequence. The second most frequent type is a single-word 
term. As less than 5% of all entries exceed 4 words and single word terms can be 
 Extracting Terminologically Relevant Collocations in the Translation 1023 
identified with the use of monolingual or bilingual dictionary
1, we decided that 
automatic extraction should be limited to sequences of 2-4 words. 
 
g19 g20g19g19g19 g21g19g19g19 g22g19g19g19 g23g19g19g19
g20g3g90g82g85g71
g21g3g90g82g85g71
g22g3g90g82g85g71
g23g3g90g82g85g71
g24g3g90g82g85g71
g25g3g90g82g85g71
g72g87g70
  
g19 g24g19g19 g20g19g19g19 g20g24g19g19
g81g14g81
g89g81g14g81
g81g14g89g81
g81g14g89
g68g14g81
g69g14g81
g89g14g89
g89g14g81
g89g81g14g49g74
g81g14g49g74
g71g14g89
g72g87g70
 
 Fig. 4. Length of Chinese terms     Fig. 5. Syntactic patterns of two word terms 
As the next step we manually analyzed the syntactic patterns of Chinese terms and 
ordered them according to frequency. These patterns were needed for the second part 
of the experiment, the “linguistically motivated” filtering. According to the analysis 
of the existing terms, multi-word terms have some kinds of fixed syntactic patterns. In 
many cases, these syntactic patterns are based on the combinations of different two 
word classes, such as “noun+noun”, “gerend verb+noun”, “adjective+noun”, 
“noun+suffix” etc.  We found that there were about 30 syntactic patterns which 
covered almost 95% in the two word combinations. Therefore, we decided that these 
patterns could be used filtering in the extraction. In figure 6, certain types of word 
combinations are more typical for technical vocabulary than for general language.  
More than three word combinations also can be divided into two small parts whose 
syntactic structures are the same as those of two word terms. For example: “(n+n)+n”, 
“(vn+n)+n”, “(v+n)+(n+vn)”, “(a+n)+(vn+n)”, etc. Therefore when we extracted 
three-word or four-word units, we didn’t set another syntactic rule for them. We just 
considered tri-gram as the combination of one bi-gram and one word. Similarly, 4-
gram was considered as the combination of different two bi-grams.  
Although we admit that these syntactic patterns are typical for certain type of 
technical prose only, we don’t think that they could filter out all the irrelevant units. If 
                                                           
1
  To extract a glossary of terms from a corpus, we must first identify single-word terms. But it 
might be slightly confusing for the computer to identify the single word terms alone. So we 
would like to set aside this problem for the sake of achieving efficiency. But we believe that 
the translator might not be troubled with single terms if he has some kind of dictionary in the 
translation of the source text. 
1024 B.-K. Kang et al. 
we extract all combinations of a certain POS-shape, additional filters are needed 
afterwards, to identify those combinations which are terminologically relevant. 
Char*Patterns={"n+n","vn+n","n+vn","n+v","a+n","b+n","v+v","v+n","vn+Ng","n+Ng","d+v","m+n","h+n","f+v","a+v","f+n","j+n","a+Ng
","vn+k","b+vn","b+Ng","Ag+n","v+Ng","a+nz","vn+v","nz+n","b+k
","v+k","j+n","nz+v",null}; 
Fig. 6. The syntactic patterns for filtering
2
 
4.2   Stop-Word Filter in Chinese 
When we examine multi word units regardless of their type, we can easily find some 
words which have no use in terminology. These irrelevant or meaningless data is a 
noise for extracting desired data. To resolve this problem, we can make use of the stop 
word list to be filtered. In the system, it would filter out things irrelevant with the 
domain-specific collocation or term. But how can we make the set of stop words? 
Indeed, the stop word list is rather flexible than firmly fixed in their usage. Whenever 
the words are frequent and meaningless in text, they can be stop words in a given task. 
For practical purposes, we used the word frequency data of the large technical 
domain corpora which was constructed by Beijing Language and Cultural University. 
In this data, we randomly selected the 2,000 words most highly frequent in their 
usage. And then we examined whether the frequent words were terminologically 
relevant or not. The analysis of the word data showed that 77.6% were domain 
dependent which could be the part of term, and 22.4% were general words. It means 
that terminologically irrelevant words amounted to about 450 words of the highly 
frequent 2000 words in technical corpora. The results are shown in Table 3. 
Table 3. The results of analysis on the high frequency words  
Frequency 
Terminologically
Relevant words 
Terminologically
Irrelevant words 
Example 
1-100 44(44%) 56(56%) 
g8859(aux), g6426(be), g3706(and), g4087
(at), g2587(middle), etc. 
101-200 58(58%) 42(42%) 
g6061g2831(provide),g20274(to),g5296(serve 
as),g3157g6573(possess), etc. 
201-500 229(76.3%) 71(23.7%) 
g4450(good),g18592g1329(for),g6705(some),
g3604(only),g3156g4697(other), etc. 
501-1000 408(81.6%) 92(18.4%) 
g8935g5296(quite), g8948 (see), g5249g12256
(arose), g5936g3252(indicate),etc. 
1001-2000 813(81.3%) 187(18.7%) 
g3252g18849(leave),g2684g2640(engage),g8607
g10571(even),g2567g5381(need not),etc 
Total 1552(77.6%) 448(22.4%)  
                                                           
2
 These POS patterns are based on the tag sets of Peking University. 
 Extracting Terminologically Relevant Collocations in the Translation 1025 
According to these analyzed data, we made the set of stop words which amounted 
to about 450 words. And we used them for filtering out the frequent, meaningless 
words in a given text before the output can be used in any productive way.  
5   Experiments  
The hybrid methods combining statistical measure and linguistic rules were tested on 
pre-processed corpus. Based on the statistical method, the extraction test was limited 
to the boundary of the frequent syntactic patterns first, and then filtered out by the 
stop word list. Three different statistical measures were used to enhance the precision 
of the extraction, such as Log-likelihood ratio, Chi-square test and Mutual 
information. Because of the poor performance in our first test, Dice formula was not 
used in hybrid method any more. Therefore, we have delivered three different 
experiments using like “LogL + Liguistic Filter”, “Chi + Liguistic Filter”, “MI + 
Liguistic Filter” methods.  
In Figure 7, we present the comparative results of precision rate among these 
different experiments. In order to measure the precision rate of the result, we used the 
grammatical criterion: A multi word n-gram could be considered as accurate result if 
it is grammatically appropriate. By grammatical appropriation, we refer to compound 
noun phrase or compound verb phrase, since with majority of multi-word terms have 
these structures.  
As a result, hybrid method using linguistic filters proved to be a suitable method for 
selecting terminological collocations, and it has considerably improved the precision of 
the extraction. The precision was much higher than that of purely statistical method, 
retrieving appropriate result almost 10%-20% higher than in the first experiment. In 
our test, hybrid method combining “Log-likelihood ratio” and “linguistic rules” had the 
best performance in the extraction. The precision was higher than 90%. According to 
their performance, the results of different experiments can be arranged like: 
LogL+Filter   >   Chi+Filter  >  MI+Filter  >  LogL  >  Chi  >  MI  >  Dice 
g19
g21g19
g23g19
g25g19
g27g19
g20g19g19
g20g21g19
g20g19g19 g21g19g19 g22g19g19 g23g19g19 g24g19g19 g20g19g19g19
g43g76g74g75g79g92g3g89g68g79g88g72g71g3g70g68g81g71g76g71g68g87g72g86
g51
g85
g72
g70
g76
g86
g76
g82
g81
g11
g8
g12
g48g44
g39g44g38g40
g47g82g74g47
g38g75g76
g48g44g14g41g76g79g87g72g85
g47g82g74g47g14g41g76g79g87g72g85
g38g75g76g14g41g76g79g87g72g85
 
Fig. 7. Comparison of Extraction Performance between statistical measures and hybrid measure 
1026 B.-K. Kang et al. 
In the analysis of the extraction data, we examined the precision of every 100 
multi-word candidates which sorted in descending order. Considering the size of 
corpus, we compared the results within the highly valued 1000 candidates. A sample 
of the highly valued output is seen in Table 4. 
Table 4. The sample result sorted by Log-likelihood ratio 
g20g86g87g58g82g85g71 g21g81g71g58g82g85g71 g47g82g74g47g14g41g76g79g87g72g85 g38g43g44g14g41g76g79g87g72g85 g48g44g14g41g76g79g87g72g85 
g16833g8873g3 g1461g5699g3 g20g19g21g25g17g22g27g3 g22g26g23g27g17g25g24g3 g23g17g21g19g20g27g28g3
g1461g5699g3 g3800g10714g3 g20g19g21g19g17g23g22g3 g24g20g19g21g17g28g27g3 g23g17g28g22g19g20g26g3
g1461g5699g3 g16801g1868g3 g28g27g20g17g22g21g22g3 g22g25g24g20g17g24g21g3 g23g17g21g22g25g26g21g3
g14270g9994g3 g16833g16340g3 g27g28g28g17g26g22g20g3 g26g27g19g24g17g24g28g3 g25g17g20g25g25g23g26g3
g8733g16833g3 g16833g8873g3 g26g22g23g17g21g20g22g3 g21g21g27g23g17g19g25g3 g22g17g26g25g28g25g23g3
g16757g12651g3 g16833g16340g4410g3 g26g20g27g17g19g20g25g3 g20g23g28g22g20g17g22g3 g26g17g27g19g23g19g20g3
g16833g16340g4410g3 g11752g12362g6164g3 g24g24g26g17g27g27g27g3 g20g22g24g25g28g17g23g3 g27g17g20g20g25g24g25g3
g16833g8873g3 g2163g14033g3 g24g22g26g17g20g28g25g3 g21g22g25g20g17g23g28g3 g23g17g25g19g19g19g27g3
g7424g3 g4395g8585g3 g24g19g19g17g19g20g20g3 g20g21g28g20g28g17g26g3 g27g17g21g25g26g26g25g3
g2081g6521g3 g6116g2010g3 g22g25g22g17g21g24g28g3 g22g24g22g24g17g19g23g3 g25g17g20g28g27g24g27g3
g11017g4388g3 g16801g1868g3 g22g24g24g17g23g28g28g3 g21g19g24g22g17g21g21g3 g24g17g21g28g20g20g26g3
g2345g3 g19911g14422g3 g22g23g24g17g24g24g20g3 g25g26g22g22g17g20g22g3 g26g17g24g24g24g22g28g3
g17247g2533g3 g15929g16833g3 g22g22g28g17g23g24g3 g25g19g28g21g17g26g22g3 g26g17g23g20g21g19g27g3
g16833g16340g3 g1461g5699g3 g22g21g28g17g24g22g25g3 g20g19g25g20g17g19g28g3 g22g17g26g25g28g23g23g3
g999g7389g3 g20045g11458g3 g22g20g25g17g26g28g21g3 g27g20g22g19g17g26g23g3 g27g17g19g27g26g22g22g3
As seen in Table 4, although not all these units would be considered terms in the 
traditional sense of the word, most of them either contain terms or include 
terminologically relevant collocations. Besides, our extraction started from these two 
word items, expanded to extract multi-word units like three word or four word units. 
Finally we could extract multi word units such as the following sample: 
Table 5. The sample of multi-word terms 
 Terminologically relevant units 
Two word units 
g20863g2308g1508g3176(grammatical function), g16261g1659g20690g20863(directional 
complement),  g20740g3025g20870g2260g18608(specification), g4744g3962g12930g20831(container 
classifier), g2806g8618g21666g2539(usage frequency), etc. 
Three word units 
g20863g2308g2430g2962g20831g2082(grammatical knowledge-base), g2587g6309g2878g5495g14474g3579
(Chinese Information Processing), g20863g2819g20824g14628g2003g20280(speech 
recognition system), etc. 
Four word units  
g6605g3962g10273g20835g2003g20280g20816g20787(MT system design), g20863g2018g2430g2962g14474g3579g1918g14492
(language information processing technology), g2564g2565g6309g6368g18724g20863g2308
(context free grammar), etc.  
On the whole, as we think that the performance of the extraction was quite good, 
this method could be applicable in the translation system.  
 Extracting Terminologically Relevant Collocations in the Translation 1027 
6   Conclusions and Future Work 
The paper presents a methodology for the extraction of terminological collocations 
from academic documents for translation purposes. It shows that statistical methods 
are useful because they can automatically extract all the possible multi word units 
according to the correlation coefficient. But the purely statistical system extracts all 
multi-word units regardless of their types, so that we also find sequences which are 
meaningless in terminology. Clearly the output must be thoroughly filtered before the 
result can be used in any productive way. To improve the precision of the extraction 
system, we decided to use linguistic rules determining whether a sequence was 
terminologically relevant or not.  The frequent syntactic patterns of terminology and 
the stop-word list were used to filter out the irrelevant candidates. As a consequence, 
hybrid method using linguistic filters proved to be a suitable method for selecting 
terminological collocations, and it has considerably improved the precision of the 
extraction. The precision was much higher than that of purely statistical method.  
We believe that terminological collocations and phrases extracted in this way, 
could be used effectively either to supplement existing terminological collections or 
to be used in addition to traditional reference works. 
In future we envisage the development of techniques for the alignment of exact 
translation equivalents of multi-word terms in Chinese and Korean, and one way of 
doing so is by finding correspondences between syntactic patterns in both languages. 
Translation memory systems already store translations in a format similar to a parallel 
corpus, and terminology tools already involve functions such as “auto-translate” that 
statistically calculate the most probable translation equivalent. By refining these 
functions and making them language specific, we could soon be facing a new 
generation of tools for translators. It remains to be seen, however, whether they can 
really be implemented into translation environments on broad scale. 
References 
1. Chang Bao-Bao, Extraction of Translation Equivalent Pairs from Chinese-English Parallel 
Corpus, Terminology Standardization and Information Technology, pp24-29, 2002. 
2. Bourigault, D. Lexter, A Natural Language Processing Tool for Terminology Extraction. 
In Proceedings of  7th EURALEX International Congress, 1996. 
3. Daille, B. Study and Implementation of Combined Techniques for Automatic Extraction of 
Terminology. In The balancing act combining symbolic and statistical approaches to 
language. MIT Press,  1995. 
4. Ulrich Heid, A linguistic bootstrapping approach to the extraction of term candidates from 
German text, http://www.ims.uni-stuttgart.de/~uli/papers.html, 2000 . 
5. Sayori Shimohata, Toshiyuki Sugio, JunjiI Nagata, Retrieving Domain-Specific 
Collocations By Co-Occurrences and Word Order Constraints, Computational 
Intelligence, Vol 15, pp92-100, 1999. 
6. Shengfen Luo, Maosong Sun Nation,Two-Character Chinese Word Extraction Based on 
Hybrid of Internal and Contextual Measures, 2003 
7. Smadja, F. Retrieving Collocations From Text: XTRACT. In Computational Linguistics, 
19(1) (pp 143--177).1993. 
1028 B.-K. Kang et al. 
8. David Vogel, Using Generic Corpora to Learn Domain-Specific Terminology, Workshop 
on Link Analysis for Detecting Complex Behavior, 2003 
9. Dias, G. & Guilloré, S. & Lopes, J.G.P. Multiword Lexical Units Extraction. In 
Proceedings of the International Symposium on Machine Translation and Computer 
Language Information Processing. Beijing, China. 1999. 
10. Feng Zhi-Wei, An Introduction to Modern Terminology, Yuwen press, China, 1997. 
11. Gaël Dias etc, Combining Linguistics with Statistics for Multiword Term Extraction, In 
Proc. of Recherche d'Informations Assistee par Ordinateur, 2000. 
12. Huang Xuan-jing & Wu Li-de & Wang Wen-xin, Statistical Acquisition of Terminology 
Dictionary, the Fifth Workshop on Very Large Corpora, 1997 
13. Jiangsheng Yug19448g19448g19448Automatic Detection of Collocation http://icl.pku.edu.cn/yujs/ 2003 
14. Jong-Hoon Oh, Jae-Ho Kim, Key-Sun Choi, Automatic Term Recognition Through EM 
Algorithm, http://nlplab.kaist.ac.kr/, 2003 
15. Patrick Schone and Daniel Jurafsky, Is Knowledge-Free Induction of Multiword Unit 
Dictionary Headwords a Solved Problem?, In proceedings of EMNLP, 2001. 
16. Philip Resnik, I. Dan Melamed, Semi-Automatic Acquisition of Domain-Specific 
Translation Lexicons, Proceedings of the fifth conference on Applied natural language 
processing, pp 340-347, 1997. 
17. Sui Zhi-Fang, Terminology Standardization using the NLP Technology, Issues in Chinese 
Information Processing,pp341-352, 2003. 
18. Yu Shi-wen, A Complete Specification on The Grammatical Knowledge-base of 
Contemporary Chinese, Qinghua Univ. Press, 2003 

