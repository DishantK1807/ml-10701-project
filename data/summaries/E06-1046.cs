The meaning is generated by concatenating the meaning symbols and replacing SEM with the appropriate specific content: a12 cmda13a14a12 infoa13a14a12 typea13 phone a12 /typea13a15a12 obja13a16a12 resta13 [r12,r15] a12 /resta13 a12 /obja13a17a12 /infoa13a18a12 /cmda13 . For use in our system, the multimodal grammar is compiled into a cascade of finite-state transducers(Johnston andBangalore, 2000; Johnston etal., 2002; Johnston and Bangalore, 2005).
C00-1054 P02-1048
To this end, we adopt techniques from statistical machine translation (Brown et al., 1993; Och and Ney, 2003) and use statistical alignment to learn the edit patterns.
J03-1002 J93-2003
a130 a60a131a66a18a68a36a69a57a70a38a71a128a127 a0 a132a113a133 a126 a122a44a126a84a129a134a122a44a135a137a136 a98a139a138 a129a131a122a44a123 (6) We first describe the edit machine introduced in (Bangalore and Johnston, 2004) (Basic Edit) then go on to describe a smaller edit machine with higher performance (4-edit) and an edit machine 1We note that the closest string according to the edit metric may not be the closest string in meaning 364 which incorporates additional heuristics (Smart edit).
N04-1005
Figure 2: MATCH Example 3 Finite-state Multimodal Understanding Our approach to integrating and interpreting multimodal inputs (Johnston et al., 2002) is an extension of the finite-state approach previously proposed in (Bangalore and Johnston, 2000; Johnston and Bangalore, 2005).
C00-1054 P02-1048
These tags are drawn from a tagset which is constructed by 363 extending each argument label by three additional symbols a80a44a81a83a82a84a81a86a85, following (Ramshaw and Marcus, 1995).
W95-0107
(Johnston et al., 2002).
P02-1048
4.1 Classification-based Approach In previous work (Bangalore and Johnston, 2004), we viewed multimodal understanding as a sequence of classification problems in order to determine the predicate and arguments of an utterance.
N04-1005
wjiw : /scost iw : /0wi iw: ε/dcost i w: ε /icost Figure 5: Basic Edit Machine 5.2 4-edit Basic edit is effective in increasing the number of strings that are assigned an interpretation (Bangalore and Johnston, 2004) but is quite large (15mb, 1 state, 978120 arcs) and adds an unacceptable amount of latency (5s on average).
N04-1005
The multiplicity of hypotheses is also required for exploiting the mutual compensation between the two modalities as shown in (Oviatt, 1999; Bangalore and Johnston, 2000).
C00-1054
For speech recognition, a corpus-driven stochastic language model (SLM) with smoothing or a combination of grammarbased and a0 -gram model (Bangalore and Johnston, 2004; Wang et al., 2002) can be built in order to overcome the brittleness of a grammar-based language model.
N04-1005 P02-1048
edit Our baseline, the edit machine described in (Bangalore and Johnston, 2004), is essentially a finitestate implementation of the algorithm to compute the Levenshtein distance.
N04-1005
Over the years, there have been several multimodal systems that allow input and/or output to be conveyed over multiple channels such as speech, graphics, and gesture, for example, put that there (Bolt, 1980), CUBRICON (Neal and Shapiro, 1991), QuickSet (Cohen et al., 1998), SmartKom (Wahlster, 2002), Match (Johnston et al., 2002).
P02-1048
a62 a60 a123 a66a18a68a36a69a57a70a38a71a72a68a38a73 a145a55a146 a75 a76 a62a142a143a27a81a83a62a124a123 a79 (7) We then use a Markov approximation (trigram for our purposes) to compute the joint probability a75 a76 a62 a143 a81a83a62 a123 a79 . a62 a60 a123 a66a18a68a36a69a57a70a38a71a72a68a36a73 a145 a146 a97 a75 a76 a62 a98 a143 a81a83a62 a98 a123 a77a147a62 a98 a104 a65 a143 a81a83a62 a98 a104a23a105 a143 a81a83a62 a98 a104 a65 a123 a81a83a62 a98 a104a23a105 a123 a79 (8) where a62a142a143 a66 a62 a65 a143 a62 a105 a143 a53a148a53a148a53 a62 a103 a143 and a62a124a123 a66 a62 a65 a123 a62 a105 a123 a53a148a53a148a53 a62a150a149 a123 . In order to compute the joint probability, we need to construct an alignment between tokens a76 a62 a98 a143 a81a83a62 a98 a123 a79 . We use the viterbi alignment provided by GIZA++ toolkit (Och and Ney, 2003) for this purpose.
J03-1002
Johnston. 2000.
C00-1054
In (Bangalore and Johnston, 2000; Johnston and Bangalore, 2005), we have shown that such grammars can be compiled into finite-state transducers enabling effective processing of lattice input from speech and gesture recognition and mutual compensation for errors and ambiguities.
C00-1054
For detailed explanation of our technique for abstracting over and then re-integrating specific gestural content and our approach to the representation of complex gestures see (Johnston et al., 2002).
P02-1048
Here we evaluate these different techniques on data from the MATCHmultimodal conversational system (Johnston et al., 2002) but the same techniques are more broadly applicable to spoken language systems in general whether unimodal or multimodal.
P02-1048
Multimodal integration and interpretation for such interfaces is elegantly expressed using multimodal grammars (Johnston and Bangalore, 2000).
C00-1054
The symbol SEM is used to abstract over specific content such as the set of points delimiting an area or the identifiers of selected objects (Johnston et al., 2002).
P02-1048
First, a parsing-based approach attempts to recover partial parses from the parse chart when the input cannot be parsed in its entirety due to noise, in order to construct a (partial) semantic representation (Dowding et al., 1993; Allen et al., 2001; Ward, 1991).
P93-1008
