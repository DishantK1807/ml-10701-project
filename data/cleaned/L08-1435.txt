<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>S E Brennan</author>
<author>M A Friedman</author>
<author>C J Pollard</author>
</authors>
<title>A Centering Approach to Pronouns</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics (ACL’87</booktitle>
<pages>155--162</pages>
<contexts>
<context>ining repetitions of entities across consecutive utterances, and the relationship between these repetitions. The main concepts and assumptions introduced in the earliest versions of Centering Theory (Brennan et al., 1987; Grosz et al., 1995) are presented in this section. Whilst the most popular application in the past has been anaphora resolution, the suitability of CT variations for other tasks has been shown in re</context>
</contexts>
<marker>Brennan, Friedman, Pollard, 1987</marker>
<rawString>S. E. Brennan, M. A. Friedman, and C. J. Pollard. 1987. A Centering Approach to Pronouns. In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics (ACL’87), pages 155–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Donaway</author>
<author>K Drummey</author>
<author>L Mather</author>
</authors>
<title>A Comparison of Rankings Produced by Summarization Evaluation Measures</title>
<date>2000</date>
<booktitle>In Proceedings of the NAACLANLP2000 Workshop on Automatic Summarization</booktitle>
<pages>69--78</pages>
<marker>Donaway, Drummey, Mather, 2000</marker>
<rawString>R. Donaway, K. Drummey, and L. Mather. 2000. A Comparison of Rankings Produced by Summarization Evaluation Measures. In Proceedings of the NAACLANLP2000 Workshop on Automatic Summarization, pages 69–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>A K Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Centering: a Framework for Modelling the Local Coherence of Discourse</title>
<date>1995</date>
<journal>Computational Linguistics</journal>
<volume>21</volume>
<contexts>
<context>he aim is to take a step in the right direction to find a more objective evaluation method for coherence than those currently available for this task. A metric is developed for Centering Theory (CT) (Grosz et al., 1995), a discourse theory of local coherence and salience, to measure the coherence of pairs of extracts and the abstracts created from them using summary production guidelines. 50 pairs of news text summ</context>
<context>ntities across consecutive utterances, and the relationship between these repetitions. The main concepts and assumptions introduced in the earliest versions of Centering Theory (Brennan et al., 1987; Grosz et al., 1995) are presented in this section. Whilst the most popular application in the past has been anaphora resolution, the suitability of CT variations for other tasks has been shown in recent years by its ap</context>
<context>(see Poesio et al. (2004) for a comprehensive overview). There are a wide variety of possible instantiations of CT, and parameters need to be specified before the theory can be used. In earlier work (Grosz et al., 1995), even the most basic notion of utterance is not defined, although an utterance is often considered to be a sentence because it is the simplest option. This view has been criticised by some researche</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995. Centering: a Framework for Modelling the Local Coherence of Discourse. Computational Linguistics, 21(2):203–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gunning</author>
</authors>
<title>The Technique of Clear Writing</title>
<date>1988</date>
<publisher>McGraw-Hill</publisher>
<location>New York</location>
<contexts>
<context>these which will almost always score the abstracts very well because the criteria are very similar to the guidelines used to produce them. Standard readability measures such as the Gunning-fog index (Gunning, 1988) and the Flesch-Kincaid index (Kincaid et al., 1975), which assess ease of reading based on average word and sentence length, can also be used to evaluate the quality of summaries. However, these hav</context>
</contexts>
<marker>Gunning, 1988</marker>
<rawString>R. Gunning. 1988. The Technique of Clear Writing. New York. McGraw-Hill.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Harnly</author>
<author>A Nenkova</author>
<author>R Passonneau</author>
<author>O Rambow</author>
</authors>
<marker>Harnly, Nenkova, Passonneau, Rambow, </marker>
<rawString>A. Harnly, A. Nenkova, R. Passonneau, and O. Rambow.</rawString>
</citation>
<citation valid="true">
<title>Automation of Summary Evaluation by the Pyramid Method</title>
<date>2005</date>
<booktitle>In Proceedings of Recent Advances in Natural Language Processing</booktitle>
<pages>226--232</pages>
<contexts>
<context> other tasks has been shown in recent years by its application in natural language generation (e.g. Karamanis (2003)) and automatic summarisation (Orasan, 2006). Hasler (2004) and Lapata and Barzilay (2005) also prove CT’s usefulness in evaluation. 3.1. Centers As CT is a theory of local coherence, only two consecutive utterances are considered at any one time (Un and Un+1). Each utterance in a text int</context>
</contexts>
<marker>2005</marker>
<rawString>2005. Automation of Summary Evaluation by the Pyramid Method. In Proceedings of Recent Advances in Natural Language Processing 2005 (RANLP’05), pages 226–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hasler</author>
<author>C Orasan</author>
<author>R Mitkov</author>
</authors>
<title>Building Better Corpora for Summarisation</title>
<date>2003</date>
<booktitle>In Proceedings of Corpus Linguistics</booktitle>
<pages>309--319</pages>
<contexts>
<context>readability. 4.2. Texts for CT Evaluation The CT evaluation uses summaries produced both by humans and automatically. Twenty two human-produced extracts of news texts were taken from the CAST corpus (Hasler et al., 2003). A further 3 texts from New Scientist which had previously been annotated for summarisation in another project were added to make 25 in total. The source texts of the 25 human-produced extracts were</context>
</contexts>
<marker>Hasler, Orasan, Mitkov, 2003</marker>
<rawString>L. Hasler, C. Orasan, and R. Mitkov. 2003. Building Better Corpora for Summarisation. In Proceedings of Corpus Linguistics 2003, pages 309–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hasler</author>
</authors>
<title>An Investigation into the Use of Centering Transitions for Summarisation</title>
<date>2004</date>
<booktitle>In Proceedings of the 7th Annual Colloquium of the UK Special Interest Group in Computational Linguistics (CLUK’04</booktitle>
<pages>100--107</pages>
<marker>Hasler, 2004</marker>
<rawString>L. Hasler. 2004. An Investigation into the Use of Centering Transitions for Summarisation. In Proceedings of the 7th Annual Colloquium of the UK Special Interest Group in Computational Linguistics (CLUK’04), pages 100–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hasler</author>
</authors>
<title>From Extracts to Abstracts: Human Summary Production Operations for Computer-Aided Summarisation</title>
<date>2007</date>
<tech>Ph.D. thesis</tech>
<institution>University of Wolverhampton</institution>
<contexts>
<context>ost-edit it to improve the summary. Guidelines, which aim to consistently improve the coherence and readability of extracts produced by CAS systems, have been developed to help users of such systems (Hasler, 2007). Summaries produced using these guidelines need to be evaluated to prove that such a resource is indeed useful. However, because the final summaries are produced via a mixture of extraction and huma</context>
</contexts>
<marker>Hasler, 2007</marker>
<rawString>L. Hasler. 2007. From Extracts to Abstracts: Human Summary Production Operations for Computer-Aided Summarisation. Ph.D. thesis, University of Wolverhampton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
<author>I Mani</author>
</authors>
<date>2003</date>
<booktitle>The Oxford Handbook of Computational Linguistics</booktitle>
<pages>414--429</pages>
<editor>Evaluation. In R. Mitkov, editor</editor>
<publisher>Oxford. Oxford University Press</publisher>
<contexts>
<context>c extraction as opposed to abstraction, with relatively little consideration for issues of coherence and readability. Evaluation in AS can be split into two main strands: informativeness and quality (Hirschman and Mani, 2003).1 Sections 2.1. and 2.2. below briefly describe existing evaluation methods in the field of automatic summarisation and explain why they are not wholly suitable for assessing coherence in computerai</context>
</contexts>
<marker>Hirschman, Mani, 2003</marker>
<rawString>L. Hirschman and I. Mani. 2003. Evaluation. In R. Mitkov, editor, The Oxford Handbook of Computational Linguistics, pages 414–429. Oxford. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kameyama</author>
</authors>
<title>Intrasentential Centering: A Case Study</title>
<date>1998</date>
<booktitle>Centering Theory in Discourse</booktitle>
<pages>89--112</pages>
<editor>In M. A Walker, A. K. Joshi, and E. F. Prince, editors</editor>
<publisher>Oxford. Oxford University Press</publisher>
<marker>Kameyama, 1998</marker>
<rawString>M. Kameyama. 1998. Intrasentential Centering: A Case Study. In M. A Walker, A. K. Joshi, and E. F. Prince, editors, Centering Theory in Discourse, pages 89–112. Oxford. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Karamanis</author>
<author>M Poesio</author>
<author>C Mellish</author>
<author>J Oberlander</author>
</authors>
<title>Evaluating Centering-based Metrics of Coherence Using a Reliably Annotated Corpus</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL’04</booktitle>
<pages>391--398</pages>
<contexts>
<context> NO TRANSITION (INDIRECT), which are due to the indirect realisation of an entity. Although NO CB is cited as a common transition, representing on average 20% of transitions found in various corpora (Karamanis et al., 2004), in terms of summaries it is the most damaging transition. In this type of short text, which should have one or perhaps two main topics over an average of 6 sentences, for even one pair of utterance</context>
</contexts>
<marker>Karamanis, Poesio, Mellish, Oberlander, 2004</marker>
<rawString>N. Karamanis, M. Poesio, C. Mellish, and J. Oberlander. 2004. Evaluating Centering-based Metrics of Coherence Using a Reliably Annotated Corpus. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL’04), pages 391–398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Karamanis</author>
</authors>
<title>Entity Coherence for Descriptive Text Structuring</title>
<date>2003</date>
<tech>Ph.D. thesis</tech>
<institution>University of Edinburgh</institution>
<marker>Karamanis, 2003</marker>
<rawString>N. Karamanis. 2003. Entity Coherence for Descriptive Text Structuring. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kibble</author>
</authors>
<title>A Reformulation of Rule 2 of Centering Theory</title>
<date>2001</date>
<journal>Computational Linguistics</journal>
<volume>27</volume>
<contexts>
<context>same as something which is judged more coherent in terms of an ’objective’ theory of local coherence. The comparison of the CT and human evaluations correlates with the findings of other researchers (Kibble, 2001; Poesio et al., 2004), who claim that CT alone is not always enough to account for the coherence of a text. Indeed, a reader does not assess a text solely on whether an entity is mentioned in consecu</context>
</contexts>
<marker>Kibble, 2001</marker>
<rawString>R. Kibble. 2001. A Reformulation of Rule 2 of Centering Theory. Computational Linguistics, 27(4):579–587.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Kincaid</author>
<author>R P Fishburne</author>
<author>R L Rogers</author>
<author>B S Chissom</author>
</authors>
<title>Derivation of new readability formulas (automated readability index, Fog count and Flesch reading ease formula) for navy enlisted personnel. Research Branch Report 8-75, Naval Air Station</title>
<date>1975</date>
<location>Memphis, TN, USA</location>
<contexts>
<context>racts very well because the criteria are very similar to the guidelines used to produce them. Standard readability measures such as the Gunning-fog index (Gunning, 1988) and the Flesch-Kincaid index (Kincaid et al., 1975), which assess ease of reading based on average word and sentence length, can also be used to evaluate the quality of summaries. However, these have been criticised as extremely coarse methods due to</context>
</contexts>
<marker>Kincaid, Fishburne, Rogers, Chissom, 1975</marker>
<rawString>J. P. Kincaid, R. P. Fishburne, R. L. Rogers, and B. S. Chissom. 1975. Derivation of new readability formulas (automated readability index, Fog count and Flesch reading ease formula) for navy enlisted personnel. Research Branch Report 8-75, Naval Air Station, Memphis, TN, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>R Barzilay</author>
</authors>
<title>Automatic Evaluation of Text Coherence: Models and Representations</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th International Conference on Artificial Intelligence (IJCAI’05</booktitle>
<pages>1085--1090</pages>
<marker>Lapata, Barzilay, 2005</marker>
<rawString>M. Lapata and R. Barzilay. 2005. Automatic Evaluation of Text Coherence: Models and Representations. In Proceedings of the 19th International Conference on Artificial Intelligence (IJCAI’05), pages 1085–1090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
</authors>
<title>SEE Summary Evaluation Environment</title>
<date>2001</date>
<note>http://haydn.isi.edu/SEE</note>
<contexts>
<context>use criteria such as good spelling and grammar, impersonal style, clear indication of the topic of the source document, and conciseness, as criteria for human judges to grade summaries. The SEE tool (Lin, 2001) allows humans to manually assess extracts for a variety of quality and informativeness phenomena, including coverage, completeness and grammatical fluency. Because the texts used in the evaluation e</context>
</contexts>
<marker>Lin, 2001</marker>
<rawString>C.-Y. Lin. 2001. SEE Summary Evaluation Environment. http://haydn.isi.edu/SEE/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of Summaries</title>
<date>2004</date>
<booktitle>In Proceedings of The ACL2004 Workshop Text Summarization Branches Out</booktitle>
<pages>74--81</pages>
<contexts>
<context> reading comprehension and the usefulness of the summary in completing other tasks are other common means of evaluation.2 Automatic evaluation has recently become popular, with methods such as ROUGE (Lin, 2004) and the Pyramid method (Nenkova and Passonneau, 2004; Harnly et al., 2005) being incorporated in the Document Understanding Conferences (DUC: http://duc.nist.gov/). Systems can also be automatically</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>C.-Y. Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Proceedings of The ACL2004 Workshop Text Summarization Branches Out, pages 74– 81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
</authors>
<title>Automatic Summarization</title>
<date>2001</date>
<publisher>Amsterdam/Philadelphia. John Benjamins</publisher>
<contexts>
<context>e of reading based on average word and sentence length, can also be used to evaluate the quality of summaries. However, these have been criticised as extremely coarse methods due to their simplicity (Mani, 2001): word and sentence length do not determine a ’good’ summary, and do not give many insights into how or why one summary is of a higher quality than another. Indeed, in the texts used for this evaluat</context>
</contexts>
<marker>Mani, 2001</marker>
<rawString>I. Mani. 2001. Automatic Summarization. Amsterdam/Philadelphia. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-L Minel</author>
<author>S Nugier</author>
<author>G Piat</author>
</authors>
<title>How to Appreciate the Quality of Automatic Text Summarization</title>
<date>1997</date>
<booktitle>Examples of FAN and MLUCE Protocols and their Results on SERAPHIN. In Proceedings of the ACL/EACL’97 Workshop on Intelligent Scalable Text Summarization (ISTS’97</booktitle>
<pages>25--31</pages>
<marker>Minel, Nugier, Piat, 1997</marker>
<rawString>J.-L. Minel, S. Nugier, and G. Piat. 1997. How to Appreciate the Quality of Automatic Text Summarization? Examples of FAN and MLUCE Protocols and their Results on SERAPHIN. In Proceedings of the ACL/EACL’97 Workshop on Intelligent Scalable Text Summarization (ISTS’97), pages 25–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>R Passonneau</author>
</authors>
<title>Evaluating Content Selection in Summarization: The Pyramid Method</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference/North American Chapter of the Association for Computational Linguistics (HLT/NAACL2004</booktitle>
<pages>145--152</pages>
<contexts>
<context>ulness of the summary in completing other tasks are other common means of evaluation.2 Automatic evaluation has recently become popular, with methods such as ROUGE (Lin, 2004) and the Pyramid method (Nenkova and Passonneau, 2004; Harnly et al., 2005) being incorporated in the Document Understanding Conferences (DUC: http://duc.nist.gov/). Systems can also be automatically compared against annotated corpora used as a gold sta</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>A. Nenkova and R. Passonneau. 2004. Evaluating Content Selection in Summarization: The Pyramid Method. In Proceedings of the Human Language Technology Conference/North American Chapter of the Association for Computational Linguistics (HLT/NAACL2004), pages 145–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Orasan</author>
<author>L Hasler</author>
</authors>
<title>Computer-aided Summarisation: What the User Really Wants</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC2006</booktitle>
<pages>1548--1551</pages>
<contexts>
<context>oduced 30% automatic extracts of them using the term weighting method. Previous experiments showed that a professional summariser using CAST selected this method to produce extracts for post-editing (Orasan and Hasler, 2006). This means that the automatic extracts used in the evaluation are more likely to be similar to those which a user of a CAS system would work with. These extracts were then transformed into abstract</context>
</contexts>
<marker>Orasan, Hasler, 2006</marker>
<rawString>C. Orasan and L. Hasler. 2006. Computer-aided Summarisation: What the User Really Wants. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC2006), pages 1548–1551.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Orasan</author>
<author>R Mitkov</author>
<author>L Hasler</author>
</authors>
<title>CAST: a Computer-Aided Summarisation Tool</title>
<date>2003</date>
<booktitle>In Proceedings of the 11th Conference of The European Chapter of the Association for Computational Linguistics (EACL’03</booktitle>
<pages>135--138</pages>
<contexts>
<context>oduction Computer-aided summarisation (CAS) is an alternative to fully automatic summarisation which accounts for the fact that fully automatic summaries are not always of a high standard of quality (Orasan et al., 2003). CAS combines methods from the fields of both human and automatic summarisation, allowing users of a system to access the output and post-edit it to improve the summary. Guidelines, which aim to con</context>
<context>ew Scientist which had previously been annotated for summarisation in another project were added to make 25 in total. The source texts of the 25 human-produced extracts were fed into the CAST system (Orasan et al., 2003), which produced 30% automatic extracts of them using the term weighting method. Previous experiments showed that a professional summariser using CAST selected this method to produce extracts for pos</context>
</contexts>
<marker>Orasan, Mitkov, Hasler, 2003</marker>
<rawString>C. Orasan, R. Mitkov, and L. Hasler. 2003. CAST: a Computer-Aided Summarisation Tool. In Proceedings of the 11th Conference of The European Chapter of the Association for Computational Linguistics (EACL’03), pages 135–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Orasan</author>
</authors>
<title>Comparative Evaluation of Modular Summarisation Systems Using CAST</title>
<date>2006</date>
<tech>Ph.D. thesis</tech>
<institution>University of Wolverhampton</institution>
<contexts>
<context>hora resolution, the suitability of CT variations for other tasks has been shown in recent years by its application in natural language generation (e.g. Karamanis (2003)) and automatic summarisation (Orasan, 2006). Hasler (2004) and Lapata and Barzilay (2005) also prove CT’s usefulness in evaluation. 3.1. Centers As CT is a theory of local coherence, only two consecutive utterances are considered at any one t</context>
</contexts>
<marker>Orasan, 2006</marker>
<rawString>C. Orasan. 2006. Comparative Evaluation of Modular Summarisation Systems Using CAST. Ph.D. thesis, University of Wolverhampton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>R Stevenson</author>
<author>B di Eugenio</author>
<author>J Hitzeman</author>
</authors>
<title>Centering: A Parametric Theory and its Instantiations</title>
<date>2004</date>
<tech>NLE Technical Note TN-02-01/CS Technical Report CSM-369</tech>
<institution>University of Essex, UK</institution>
<marker>Poesio, Stevenson, di Eugenio, Hitzeman, 2004</marker>
<rawString>M. Poesio, R. Stevenson, B. di Eugenio, and J. Hitzeman. 2004. Centering: A Parametric Theory and its Instantiations. NLE Technical Note TN-02-01/CS Technical Report CSM-369, University of Essex, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>G Lapalme</author>
</authors>
<title>Concept Identification and Presentation in the Context of Technical Text Summarization</title>
<date>2000</date>
<booktitle>In Proceedings of The NAACL-ANLP 2000 Workshop on Automatic Summarization</booktitle>
<pages>1--10</pages>
<marker>Saggion, Lapalme, 2000</marker>
<rawString>H. Saggion and G. Lapalme. 2000. Concept Identification and Presentation in the Context of Technical Text Summarization. In Proceedings of The NAACL-ANLP 2000 Workshop on Automatic Summarization, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Strube</author>
<author>U Hahn</author>
</authors>
<title>Functional Centering Grounding Referential Coherence in Information Structure</title>
<date>1999</date>
<journal>Computational Linguistics</journal>
<volume>25</volume>
<marker>Strube, Hahn, 1999</marker>
<rawString>M. Strube and U. Hahn. 1999. Functional Centering Grounding Referential Coherence in Information Structure. Computational Linguistics, 25(3):309–344.</rawString>
</citation>
</citationList>
</algorithm>

