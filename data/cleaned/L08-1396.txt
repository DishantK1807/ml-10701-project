<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>L Allison</author>
<author>C S Wallace</author>
<author>C N Yee</author>
</authors>
<title>When is a string like a string</title>
<date>1990</date>
<booktitle>In Proceedings of International Sym8PASSAGE is</booktitle>
<location>Ft. Lauderdale, Florida</location>
<contexts>
<context>scus, 1997) in a DARPA/NIST evaluation campaign on speech recognition. He found out that by aligning the output of the participating speech transcription systems with a dynamic programming algorithm (Allison et al., 1990) and by selecting the hypothesis which was proposed by the majority of the systems, he obtained better performances than with the best system. Since, the idea gained support, first in the speech proc</context>
</contexts>
<marker>Allison, Wallace, Yee, 1990</marker>
<rawString>L. Allison, C. S. Wallace, and C. N. Yee. 1990. When is a string like a string? In Proceedings of International Sym8PASSAGE is a project supported by ANR (ANR-06-MDCA013), the national research funding agency, in the subprogram Data Mass and Ambient knowledge (Masses de donn´ees et Connaissances Ambiantes). posium on Artificial Intelligence in Mathematics (AIM), Ft. Lauderdale, Florida, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chaudiron</author>
<author>J Mariani</author>
</authors>
<title>Techno-langue: The french national initiative for human language technologies (hlt</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Ressources and Evaluation (LREC</booktitle>
<pages>767--772</pages>
<location>Genoa, Italy</location>
<contexts>
<context>2004) was to organize a campaign with such characteristics for parsers of French. EASY is one of the 8 evaluation campaigns of the EVALDA platform, which itself is part of the TECHNOLANGUE 1 project (Chaudiron and Mariani, 2006). The aim of the EVALDA platform is to constitute a framework for the NLP system evaluation covering all domains of text or speech processing.. EASY gathered 5 corpus providers to collect and annotat</context>
</contexts>
<marker>Chaudiron, Mariani, 2006</marker>
<rawString>S. Chaudiron and J. Mariani. 2006. Techno-langue: The french national initiative for human language technologies (hlt). In Proceedings of the 5th International Conference on Language Ressources and Evaluation (LREC), pages 767–772, Genoa, Italy, may.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan G Fiscus</author>
</authors>
<title>A post-processing system to yield reduced word error rates: recognizer output voting error reduction (rover</title>
<date>1997</date>
<booktitle>In In proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding</booktitle>
<pages>347--357</pages>
<location>Santa Barbara, CA</location>
<contexts>
<context>t of the parsers using what now begins to be known in some circles as a ROVER (Reduced Output Voting Error Reduction) algorithm. The acronym and the first experiment of the kind are due to J. Fiscus (Fiscus, 1997) in a DARPA/NIST evaluation campaign on speech recognition. He found out that by aligning the output of the participating speech transcription systems with a dynamic programming algorithm (Allison et</context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>Jonathan G Fiscus. 1997. A post-processing system to yield reduced word error rates: recognizer output voting error reduction (rover). In In proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, pages 347–357, Santa Barbara, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hirschman</author>
</authors>
<title>Language understanding evaluations: lessons learned from muc and atis</title>
<date>1998</date>
<booktitle>In Proceedings of the 1st International Conference on Language Resources and Evaluation (LREC</booktitle>
<location>Granada, Spain</location>
<contexts>
<context> • we see that for relation annotations, the best systems have an average f-measure near 0.60, which is considered as a significant threshold in the MUC evaluations on natural language understanding (Hirschman, 1998), thus a campaign which measures results in a comparable domain. . • although the variability of results for relation annotation is high (the detailed performance profile according to text genre and </context>
</contexts>
<marker>Hirschman, 1998</marker>
<rawString>L. Hirschman. 1998. Language understanding evaluations: lessons learned from muc and atis. In Proceedings of the 1st International Conference on Language Resources and Evaluation (LREC), Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L¨o¨of</author>
<author>C Gollan</author>
<author>S Hahn</author>
<author>G Heigold</author>
<author>B Hoffmeister</author>
<author>C Plahl</author>
<author>D Rybach</author>
<author>R Schl¨uter</author>
</authors>
<title>The rwth 2007 tc-star evaluation system for european english and spanish</title>
<date>2007</date>
<booktitle>In In proceedings of the Interspeech Conference</booktitle>
<pages>2145--2148</pages>
<marker>L¨o¨of, Gollan, Hahn, Heigold, Hoffmeister, Plahl, Rybach, Schl¨uter, 2007</marker>
<rawString>J. L¨o¨of, C. Gollan, S. Hahn, G. Heigold, B. Hoffmeister, C. Plahl, D. Rybach, R. Schl¨uter, , and H. Ney. 2007. The rwth 2007 tc-star evaluation system for european english and spanish. In In proceedings of the Interspeech Conference, pages 2145–2148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewciz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank. Computational Linguistics</title>
<date>1993</date>
<contexts>
<context>ation, dialog systems or question-answering systems. Since 1992, syntactically annotated corpora have been built for English, e.g. the SUSANNE corpus (Sampson, 1995) or the much bigger Penn Treebank (Marcus et al., 1993). More recently one can observe the development of such corpora in many other languages (as of today there are 30 languages listed in the treebank page of Wikipedia). Despite the existence of these c</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewciz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewciz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Paroubek</author>
<author>Isabelle Robba</author>
<author>Anne Vilnat</author>
<author>Christelle Ayache</author>
</authors>
<title>Data, annotations and measures in EASY the evaluation campaign for parsers of French</title>
<date>2006</date>
<contexts>
<context>(modifier respectively of the noun, the adjective, the adverb or the proposition), • COORD (coordination), • APP (apposition), • JUXT (juxtaposition). To find details on this annotation process, see (Paroubek et al., 2006). The figure 1 gives an example of a literary sentence annotation. We carried out an estimation of the annotation error rate concerning the relations. For each sub-corpus of the reference, we asked a</context>
</contexts>
<marker>Paroubek, Robba, Vilnat, Ayache, 2006</marker>
<rawString>Patrick Paroubek, Isabelle Robba, Anne Vilnat, and Christelle Ayache. 2006. Data, annotations and measures in EASY the evaluation campaign for parsers of French.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>In proceedings of the fifth international conference on Language Resources and Evaluation (LREC 2006</booktitle>
<pages>315--320</pages>
<editor>In ELRA, editor</editor>
<publisher>ELRA</publisher>
<location>Genoa, Italy</location>
<marker></marker>
<rawString>In ELRA, editor, In proceedings of the fifth international conference on Language Resources and Evaluation (LREC 2006), pages 315–320, Genoa, Italy, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Paroubek</author>
</authors>
<title>Language resources as by-product of evaluation: the multitag example. In</title>
<date>2000</date>
<booktitle>In proceedings of the Second International Conference on Language Resources and Evaluation (LREC2000</booktitle>
<volume>1</volume>
<pages>151--154</pages>
<contexts>
<context>e such instance is for POS tagging, where the algorithm was applied to provide POS tags with confidence annotation to yield a validated language resource from data produced in an evaluation campaign (Paroubek, 2000). Since we are processing text the problem seems to be simpler than for speech, because we can use the words of the text to be annotated for realigning the different annotations, provided the parsers</context>
</contexts>
<marker>Paroubek, 2000</marker>
<rawString>Patrick Paroubek. 2000. Language resources as by-product of evaluation: the multitag example. In In proceedings of the Second International Conference on Language Resources and Evaluation (LREC2000), volume 1, pages 151–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sampson</author>
</authors>
<title>English for the Computer: The Susanne Corpus and Analytic Scheme</title>
<date>1995</date>
<publisher>Oxford University Press, USA</publisher>
<contexts>
<context>cessary step in many complex tasks such as translation, dialog systems or question-answering systems. Since 1992, syntactically annotated corpora have been built for English, e.g. the SUSANNE corpus (Sampson, 1995) or the much bigger Penn Treebank (Marcus et al., 1993). More recently one can observe the development of such corpora in many other languages (as of today there are 30 languages listed in the treeba</context>
</contexts>
<marker>Sampson, 1995</marker>
<rawString>G. Sampson. 1995. English for the Computer: The Susanne Corpus and Analytic Scheme. Oxford University Press, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Improved rover using language model information. In</title>
<date>2000</date>
<booktitle>In proceedings of the ISCA ITRW Workshop on Automatic Speech Recognition: Challenges for the new Millenium</booktitle>
<pages>47--52</pages>
<location>Paris</location>
<contexts>
<context>t speech recognizers as confidence weights in the hypothesis lattice obtained by combining the different outputs and by applying language models to guide the final stage of best hypothesis selection (Schwenk and Gauvain, 2000). In general better results are obtained with retaining only the output of the two or three best performing systems, in which case the relative improvement can go up to 20% with respect to the best p</context>
</contexts>
<marker>Schwenk, Gauvain, 2000</marker>
<rawString>Holger Schwenk and Jean-Luc Gauvain. 2000. Improved rover using language model information. In In proceedings of the ISCA ITRW Workshop on Automatic Speech Recognition: Challenges for the new Millenium, pages 47–52, Paris, September.</rawString>
</citation>
</citationList>
</algorithm>

