Proceedings of the 2010 Workshop on Companionable Dialogue Systems, ACL 2010, pages 13–18,
Uppsala, Sweden, 15 July 2010. c©2010 Association for Computational Linguistics
Is a Companion a distincve kind of relationship with a machine? 
Yorck Wils 
Florida Instiute of Human and Machine Cognition 
     ywilks@ihmc.u 
Abstract 
I start from a perspective close to that of the EC 
COMPANIOS project, and set out its aimto 
model a newkindof human-computer relation-
ship based on long-terminteraction, with some 
tasks involved althg a Copai should not 
be herently task-based, since there need be no 
stopping point to is conversation. Some demon-
ration of is functionality wl be given but the 
man purpose here is an naysis of whais 
people mightwantfrom such a relationship and 
what evidnc we hv for whatevr we con-
clude. Is politeness important?Is an attemptat 
emotional sympathy important or achievable? 
Does a ur wt a cnsistent personality n a 
Companion or variety of personalites? Should 
we btalkingmo rms of a "cgnivepros-
thesis(or orthsis)?"---something to extract, r-
ganize, and locate the user's knowledge or pe
sonal information---rather than attitudes? 
1. Introduction 
It is convenient to distinguish Companions from 
both (a) conversational internet agents that carry 
out specific tasks, such as the train and plane 
scheduling and ticket ordering speech dialogue 
applications back to the MIT AIS system (Zue 
et al., 1992), also from (b) descendants of the 
early chatbots PARYand ELIZA, the best 
which ompet anuly inthe Lobner competi-
tion(Leber). These ave essentiallymm-
ory or knowledge but are simple finite state re-
sponse sets, although ELIZA had primitive 
“scripts” giving some context,and PARRY 
(Colby, 1971) had parameters like FEAR and 
ANGER tha cnged with e conversationd 
determined which reply was selected at a given 
point. 
I take plausible distinguishing fu of a
Companio agent to be: 
 
1) that it has nocentral or ver-riding task 
and there is no point at which its conver-
sation is complete or has to stop, al-
though it may havsome tasks it carries 
out in the course of conversation;
2) That itshould be capable of a sustained 
discourse over a long-period, possibly 
ideally the whole life-time of itsprinci-
pal user;
3) It is essentially the Companion of a par-
ticular individual, its principal user, 
about whom it knows a great deal of per-
sonal knowledge, and whose interests it 
serves—it could, in principle, contain all 
the information associated ith a whole 
life; 
4) It establishes some form ofrelationship 
with at user, if thaisapropriate, 
whicwoldv ect ssocied 
t e trm “emotion”, and shared ini-
tiative is essential;  
5) It is not essentially an internet agent or 
interface, but since it will have to have 
access to the internet for information (in-
cluding whole-life information about 
its user—which ould bepulc dat like
Facebook, orlfeiforibuilup by 
the Companion ver long erios ofin-
teractio withthe user) and to act in the 
world, e.g. to reserve at a restaurant or 
call a doctor. But a Companion need not 
be robot to act in the world in this way, 
and wemayaswel asume internet 
agent status, with ace to ope itret 
knowledge sources. 
Given this arowing f focus inthis paer, what 
questons hen arise and whatchoices does hat
leave open? We  discus ome bvious qs-
tions that have arisen in the literature: 
 
i) Emotion, politeness and affection 
Cheepen and Monaghan (1997) presentd results 
some thirteen years ago thatcustomers of some 
automata, such as ATMs, are repelled by exces-
sive politeness and endless repetitons of ”thank 
you for using our service”, because they know 
they are dealing with a machine and such feigned
sincerity is nappropriate. This suggests that po-
liteness isvery uch a atter of jgment in 
certain situations, just as it is with humans,
wher inaproiate politens iften con-
tered. Wallis (Walls et al., 2001) has reported 
results that many find computer conversational-
ists “chippy” or “cocky” and sugests that is 
should be avoided as it breeds hostilty on the 
part of users; he beleves his ialways a major 
13
risk in human-machine interactions.  
 
Weknow, sine th orignal work of Nas 
(Reeves and Nass, 1996) and colleagues that 
people wil display some level of feeling for the 
simplestmachines, even PCs in his original ex-
periments, and Levy (2007) has argued persua-
sively that the trend seem tobe towards hig 
levels of“affectionate” relationship ithma-
chines in the next decades, as realistic hardware 
and sophisticated speech generation ake a-
chine interlocutors increasingly lifelike. How-
ever, much of this work is about human psychol-
ogy, faced ith entites known to be artificial, 
and does not bear directly on the issue of 
whethr Companios hould atempt odetc 
emotion in what they hear fromus, or attempt to 
generate it in what they say back. 
 
The AI area of “emotion and machines” is con-
fused and contradictory: thas established tself 
as more than an eccentric minority taste, but as 
yet has nothing concrete to show beyond some 
better han random algorihms for detecting “sen-
timent” in coing text (e.g. Wiebe et al., 
2005), buteven there its success is dependenton 
effective content extraction techniques. This 
work began s“conte alysi” (Kripendorff, 
2004) atthe Harvard psychology department
many yers go and, whil prose texts may offer 
enough length to enable a measure of senti
to be assessed, this inot always the case wth 
shortdialogue urns. Thattechnology rested al-
most entirly on the supposed sentiment value of 
individual words, hich ignores the fact that 
their value is content depent. “Cancer” may 
be marked as negative word but the utterance “I 
have found a cure for cancer” is presumably 
positive and detecting the appropriate rsponse to 
that uterance rests on the ability to do informa-
tion extraction beyond single terms. Failure to 
observe this ha le t may of the claic fol-
ishes ofchatbts such asconratulating peo-
ple on the death of their relatives, and so on.
At depr levls, ther a coflti thoris f 
emotion for automata, not all of which are con-
sistent and which apply n inlimited ranges f
discourse. So, for example, the classic theory that
emotion is a response to the failure and success 
of the machine’s plans (e.g. Marsella Gratch, 
2003) covers only those situations that are 
clearly plan driven and, as we noted, Compan-
ionship dialogue is not always cloely related to
plans and tasks. “Dimensional” theories (Cowie 
et al., 2001, following Wundt, 1913), display 
emotions along dimensions marked with opposed 
qualites (such as positive-negative) and nor-
maly distribute across the space emotion “primi-
tives”, such asFEAR, and these normally as-
signed by manual tagging.Allsuch asigments 
of tags re, lik thex-sentimenttheories 
above, on human pre-taggin.  Te problem with 
this that taggin for “COMPANY” o “TEM-
PERATURE” (in classi NLP) is a quit differ-
ent task fromtagging for “FEAR” and “AN-
GER”. Thes lter tms are not, d probaly 
cannot be, analyzed but rest on the commonsense 
intuitios ofthe tagger, which ay vary ery 
much from person to person—theyhave 
low cosilience between tagers. 
All th makes ny emotionteoris lok 
primiive in terms of developments n AI and 
NLP elswhr. Appraisl Thry (Schr etal, 
2008) seeks to explain why individuals can have 
quite different emotional reactions to similar 
siuations because they have appraised hem dif-
ferently, e.g. a death welcomed or regretted. Ap-
praisalcan also be of the performance of planned 
activities, in which case this theory approximates 
to the plan-based one mentioned above. The he-
ory itself, like all such theories, has a large-
commonsense component, and the issue for 
putational implementation is how, in assess-
ing the emotional state of the Companion’s user 
toaksuch coceptsquantitatively evaluable. 
If the Companio ndcts long coversatios 
witha ser abothisor elife,the on might
expect there to be ample opportunity to assess 
the user’s appraisal f, say, a funeral r weddin 
by means of the applicaton of the sentiment ex-
traction techiques towhat is said in tpresence 
of the relevant image. In so far as a Companio 
can be said to have over-arching goals, such as 
keeping the user happy then, to hat degree, it s 
not difficul o envisage methods (again based on 
estimates of the happiness, orotherwise, of the 
user’s uterances) for self-appraisal by the Cm-
panion of its own performance and some conse-
quent causal lnk to generated demonstrations of 
its own emotions of satisfaction or guilt. 
 
In speaking of “language” and Companions, we 
have so far ignored speech, although that is a 
communication mode in which a great deal has 
been done to identify and, more recently, gener-
ate, emotion-bearing components (Luneski etal., 
2008).  Elements of the above approaches can be 
found in the work Worgan and Moore (s 
figure below, from REFERENCE REMOVED), 
wher thre is the sam citen th cn-
14
trality ofemtion inthe comunication process, 
butin a form focusing on an ntegratiof 
speech and language  (rather than visual and de-
sign) technologies. Their argument is for a layer 
in a dialogue manager over andabove local e-
sponse management, but one whicul sk 
to navigate the whle corsaton across a two-
dimensional space onto which Companion and 
user are mapped using continuous values (rather 
than discrete values corresponding toprimitive 
butunexplained emotioal terms) but in sch a 
way asto bth respnd the aer’dmon-
strated emotion appropriately, butalso----again, 
if ppropriate or chosen by the userto raw 
the user back toother more psitive emotional 
areas of the tw-dimensional space. It is not yet 
clear what right echanismshould be for the 
integration of this “landscape” global emotion-
based dialogue manager should be with he local 
dialogue management that generates responses 
and alters the world context: in the Senior Com-
panion this last as sophisticated stack of net-
works (e Wikset al., inpres). Insome sen, 
weajutloingformodad feibl
interface o replace what PARRY had in simple 
form in 197 when the sumof tw emotion pa-
rameters determined hich response to select 
from a stack of alternatives. 
 
This last is a high level issue to be settled in a 
Companion’s architecture and also, perhaps, to 
be under the control of he user, namely: should a 
Companion invariably tro cheer auserup if 
miserable-----whch sying to“mov” the usr 
to the most naturallydesirable (i.e. the op-right) 
quadrant of the space----or, rather, o rack to he 
part tspace where the user is deemed be 
and stay there in roughly the same emotional lo-
cation—i.e. be sad ith a sad user and happy 
with apy one?Threinogeralnswerto
thisquestioand, ineed, in anideal Comio, 
whic trckig metod suld b used would 
itselfbe a conversationtopic e.g. “Dywant 
me to heer you up or would you rather stay mis-
erable?”. 
 
 
 
ii) What should aCompanio lk lie? 
A fcel Cmpni is a pusb candt
for Companionhood: the proverbial frry hand-
bag, warm and lightto carry, chatty butwitful 
internet access. Such a Comani could aways
take control of a nearby screen r a phne if it 
needed to showanthing. Ifthere is toba face, 
the questio fe “ucann valleyefect” al-
ways comesup, wher itsarued tha user  
more una th mo si i vy lik
ourselves (Mori, 1970). But many observes do 
not feel this, and, indeedit cannot in principle 
apply to an avatar so g that e cannt be 
sure it s artificial, as many fel abut the Emily 
from Manchester (Eily 2009). 
 
Onthe otr hand, if the quality snot gd, an 
in particular ifthe lipsynchist perfect, it may
be better o go for an abstract avatar ---the Com-
panions logo was chosen with hat in mind, and 
without amouth al. Non-human avatars seem 
to avidsme ofthe problems that arise with
valleys and mixed feelings generally, and the 
best REMOVED demonstration video so far fea-
tures REMD.
 
iii) Voice or Typing to communicate with a 
Companion? 
At the moment he limitaion the us of vice 
istwo-fold: first,although rained ASR fora sin-
gle user—such as a Companion’s user—is nw 
very good and up in the high 90%, it stilntro-
duces uncertainty o understanding an utter-
ance that is far greater than that of spelling 
rors. Secondly, it is currently not possible to 
store sufficient ASR software ocallon a mobil
phone to recognize a large vocabulary in real 
time; access toa remote server takes additional 
e andcanbe suject to fluctuationsan e-
lays. All of whic sget thaa web-based 
Companion may have to use yped input in the 
immediate future—thousing TSoutp—
15
which isno prblem for most bile phone us-
ers, w hav cm to find ty cat prfctly
natural. However, this almost certainly only a 
transitory delay asbile RAMincreses rap-
idly anthe prolem should nt determine e-
search decisions---there isoubthat voice 
wil move back toe ctre f cmmunication 
once storage and access size have grown by n-
other order of magnitude. 
 
iv) One Companio personality orseveral? 
Some (.g. Pulman, in Wilks, 2010) have argued 
that havinga consistent personalityisa conditio 
on Companionhood, but one could differ and 
argue that, although that is true of people—
multiple personalites being a clssi psycho-
sis—there is no reason why e should expect 
is of a Cmpanion. Perhaps a Companion 
should have personality adapted to is particu-
lar relationship toa user at a given ent: 
Lowe (in Wilks,2010) has pointed outthat one 
might want aCompanion to function as,say,a 
gym trainer, in which case a rather harsh attiude 
on he part of the Companimight well be he 
best one. If a Companio’s emotional attitude 
wereto (figuratively) moveacrs atwo imen-
sionalemotion space (see diagram above) mitat-
ing r correctin what itperceived to be he 
user’s state over time (as Worgan, see above, has 
proposed), then hat shift in attiude might well 
seem to be he productof different personalites, 
as it sometimes can with humans.
 
It might be better, pace Pulman, to give a user 
access to, and some control over, the display of a 
multiple-personality Companion, something one 
could think of as an “agency” of Companions, 
rather than a single “agent”, all of which shared 
access to the same knowledge of the orld and 
of the state and history of the user. 
 
v) Ethics and goals in the Companion 
The issue is very close to the question ofwhat 
goals a Companion can plausibly have, beyond 
something  general, sch as“keep the user 
happy and do what they ask if you can”, hich 
are goals and constraints that directly relate to 
the standard iscussions of he ethics a robot
could be considered to have, a discussion started 
long ago by Asimov (1975). Clearly, there will 
be need for a Companion to have goals to carry 
out specific tasks: if  isto place a restaurant 
table boking onthe phone fr uer ho as 
just said to i “Get me a table for two tonight at
Brancaround 8.30”---phone request ell 
within te bounds of the curently achievable 
techology-----and the Companion ill first ave 
to find the restaurant’s pe mber fore it 
phones and ask aboutavailabiity before choos-
ing a reservation time. This the standard con-
tent of al-driven behavior, with alternatives at 
every stage if unexpected replies are encountered
(such as the restaurant being fully boked to-
night).  But one does not need to consider such 
goals as “goals of its own” since they are in-
ferred from what was old and are simply as-
sumed, as an agentor slave of the user. But a 
Companion tha finds its user not respondig
after some minutes of conversation might well 
have to ake an independent decision to call a 
doctor urgently, based on a stored permanent
goal about danger to a user who is unable to an-
swer but is not asleep etc. 
 
vi) Safeguards for the information cotent of a 
Companion 
Data protecin, privacy, orwhatevr tm one 
prefers,now captures a crucial conceptin the 
new information society. A Companion tha d 
learnedin deals of a user’s lifeover 
months or years would crtinly have contents 
needing protection, and many forces-----
commercial, security, governmental, research---
might wel want acess to i, oreven to hose of 
all the Companions in a given society. If socie-
ties ove to a clear legal state where one’s per-
sonaldatis one’s own, wit te or origi-
nator having rights over sale and distributon of 
their data---which isot al th case the 
moment in mostountre----thene isue of the 
personaldata elicited by a Companion would 
automatically be covered.   
 
Ifwe ignor the isu of governmts and na-
tioal security---and a Companion would clearly 
be useful to he police when anting to know as 
much as possible about a murder suspect, so that
it might then banissue of hether talking to
one’s Companion constituted any kind of self-
incrimination, in contries were that form 
comunication is protected. Some might well 
want oe’s rlatioship toa Compani put n 
some basis ike hat of a relationship to a priest
or doctor, or even to spouse, who cannot al-
ays be forced to give evidence in common-law 
countries. 
 
16
More alistcly, auser might wel want opro-
tect parts ofhisr er Copanio’sinfrmation, 
or even an organized life-story based on that
from particular individuals: e.g. “this must never 
be told t my chldren, even when I am gone”. It
is nt hard toimagine a Companion deciding 
whm tivulge crtain thigs t,elcti be-
tween classes ofsprin, relatio, friens, col-
leaguesetc. There will almost certainly need to
be a new set of laws cvering the wnership,n-
heritance and destruction of Companion-objects 
in the future. 
 
vii) What must aCompanio kw? 
There is no clear answer to his question: dogs 
make exclnt Companions and know nothing. 
Morvanty, Colby’s PARRY program, the 
best conversationalist of its day(Colby, 197) 
and possibly since, famously “knew’ nothing: 
John McCarthy at Stanford dismissed PARRY’s 
performance by saying:”It doesn’t even know 
who the US Presidnt is”,forgetinashe aidt
that mst of wrld’spoulaton did not 
at, leasat the time.  O the oer an, it is
hard to relate over a long term to an interlocutor 
who knws lite ornhig and hs nmemory 
of whatitor you have said n the past. It is hard 
to attribute persality toan entiy with no m-
ory and litle or no knowledge.
 
Much ofwhat Companio kws tha isper-
sonalitshould elicit n conversation from its 
user; yet much could also be gained pub-
liclyavailable sources, just as the current Senior 
Companion demo goes off to Facebook, inde-
pendently of a conversation, find out who ts 
user’s friends are. Current nformation extraction 
technology (e.g. Ciravegna et al., 204) allos a 
reasonable job to be made of going to Wikipedia 
for general information when,say, a world city s
mentioned;the Companican theglean some-
thing about that city fro ikipedia an ask a 
relevant question such as “Did you see the Eiffel 
Tower when you were in Paris?” which again 
gives a plausible ilusion of general knowledge.  
 
Aconrete Companion paradigm: the 
Vitorian C 
 
The subsections above are mini-discussions of 
some of the constraints on what its to be a 
Companion, he subject of a recntbook collec-
tion (Wilks, 201). The upsot f those discus-
sions is that there are mny dimensions of 
choice, even within an agreed definition of what 
a Companion is to be, and they will depend on 
the user’s tastes and needs above all. In the sec-
tion at follos, I cut thugh thchoices and 
makeasemi-serious proposalfor a modelCom-
panion, one based on a once well-known social 
stereotype. 
 
More sriously, and ithe spirt of aprio 
thoughts(andwhat else canwe ave at this tech-
nological stage of development?) aboutwhata 
Companion should be, Iwould suggest we could 
profitably spend a fewmoments reminding our-
selves of the role of the Victorian lady’s Com-
panion. One could, and n no scientific manner, 
risk a listing of features of the ideal Victorian 
Companion: 
1. Politeness 
2. Dscretion
3. Knowng their place 
4. Dependence 
5. Emotions firmly under control 
6. Modesty 
7. Wit 
8. Cheerfulness 
9. Well-informed
10. Diverting 
11. Looks are irrelevant 
12. Long-term relationship if posible 
13. Trustworthy 
14. Limited socializaton between Com-
panios permitted of-duty. 
 
The Victorian virtue of discretion here brings to
mind the “confidant”conceptthaBoden (i
Wilks,201) explitly rjt asei aplu-
sible one for automaed Companions: 
Most ecrts ar scrt fom sme HBs [Human 
Beings]butnotothers.If two CCs Computer 
Companios] were tshareir -users’ se-
crets with each other, how would they know 
which other CCs (i. potntially, user) to ’trust’ 
in this ay? The HBcould of core ay "This 
notto be told to Tommy"...... but usuallwe re-
gard i as obvious tat our confidant(sic) knows 
whatshould not beold to Tommy -eite to 
avoid upsettingToy, or toavoidupsetting the 
original HB. How is a CC to emulate that? 
The HB could certainly say "Tel this to no-one" 
-wher "no-one" includes other Cs. But would
the HB always remmber to do that?  
How could secrt-sharing CC deal with family 
feus? Sme faily websites have special func-
17
tionalities todeal with this. E.g Robie is never 
shown inputposted by Bilie. Could similar, or 
more subtle, fnctioalities be given to CCs?” 
Boden brings up real difficulties in extending 
this notio toa computer Copanion, but the 
problems are not all where she thinks. I see no 
difficulty in programming the notion of explicit 
secrets for a Companion, or even things to be 
kept from specific individuals (“Never tellthis to 
Tommy”). Companions wil have less problems 
remembering to be discrete than people do, and I 
suspect people have less instnctive dscretion 
than Boden believes: they have to b tld explic-
itly whtosay what to, or not, in mt ca,
unlessthey are told tell n e. In any case, 
much of this wilbemoot becauseCompanions 
wil ormaly dea onlywithoe prson except 
when,sa,kigpe lstanofficial, 
friend or restaurant, where they can ry to keep 
the coversation to limited replies that ey can 
be sure to understand. The noton of a stored fact
that must not be disclosed is relatively simple to 
code. Nonetheless, the Lady’s Compani aal-
ogy foresees that Companions wil, n tme, gos-
sip among hemselves behind their owners’ 
backs. 
I wouldargue tha te “Lady’s Companion” list 
above an attractive and plausible one: itassumes 
emotion wil blrgly linguistic in expreion, 
it implies care for the mental and emotioal state 
of the user,and I would personally find it hard to 
abuse any computer with the characteristics 
listed above. Many ofthe situations discused 
above are, at the mment, wildly speculative:
that f a Cpanion acting as its owner’s agent, 
on the phone or World ide Web, perhaps hold-
ing power f attorneyincasof anwner’sinca-
pacity and, ith he owner’s advance permission, 
perhaps even being a source of conversational 
comfort for relatives after the oner’s death. 
Companions may notlbe nice orven frienly: 
to stop us faing aslp while driv-
ing may tell us jokes but will probablyshout at 
us and make us do stretchiexercises. Long-
voyage Companions in space il be indispensa-
ble cognitive prostheses (or, more correctly, or-
thoses) for running a huge vessel and experi-
ments above any beyond any personalservices---
Holywodlready knows all that.  
 
Acknowlegement:  
This work as funded by the Companions project 
sponsored by the European Commission as parof he 
Information Society Technologies (IST) program 
under EC grant number IST-FP6-034434. 
Refernces 
Colby, K.M."Artificial Paranoia." Artif. Intel. 2(1) 
(1971), pp 1-2 
Chen,C.andonagh, J.197, ‘Design 
Naturals iAutmated Diloues some prbles
and solutons’. In Proceedins ‘FirtIntnational 
Workshop nHumanCpt Cnvan’, Bl-
lagio, Italy.
Cowie, R., Dogls-Cowie, E., Tsaptsouli, N., Vot-
sis, G.Kollias, S.,Fellenz,W.and Taylor,JG.2001. 
Emotion rcognition in humn-computer interaction,
Signal Processing Magazine, IEEE, 18(1), pp. 32–80. 
ily,2009.http:/w.youtube./watch?v=UYgL
Ft5wfP4&feaur=player_embedded# 
http:/w.surrealaward.com/avatar/3ddigital12.shtm
l 
Kripendorf, K. 204.  Conte Analysi: An Intro-
ducton to Its Methodology. 2d eto, Thusand 
Oaks, CA: Sage. 
Levy,D.2007.Love and Sex with Robots: The Evo-
lution of Humn–Robot Relaonhip. Lond: 
Duckworth.
Luneski, A, Moore, R.K., &Bamidis, P.D. (2008). 
Affetivcomputingadcolabrtventworks: to-
wards moti-aware interaction. In L. M. Cama-
rinha-Mas &W. Pr (Es.),PervasiveCollabor
tive Ntwork (V 283 pp 315-322). Boston: 
Springer. 
sla,Snd Gratch, J.(20) Modeling Coping 
Behvior VirtulHumans: Do'tWory,BeHay.
2nd IntConf on Autonomous Agents and Multiagent 
Systems (AMS), elbourne, ustralia, July 2003.
Reevs, B., Nas,C.196, Themedia quation: how 
people treatcomputers, television, and new media like 
real peole and places, Cabridg: Cbridge Uni-
versity Press,1996. 
Schere, S., Schwenker, F.nd Palm, G. 208. Eo-
tion recognitio fromspech using multi-classifier 
systems and rbf-ensembles,in Speech,Audio, Iage 
and Biomedical Signal Processing using Neuralet-
works, pp. 49–70,Sprnger: Berlin. 
Wali, P.Mitchard, H.,O’Dea,.,and Ds J.201, 
Diaoguemoelingfor convrstiolagentI
‘AI-2001: Advances in Atificial Inteligence’, 
Stumptner,Corbet, and Brooks, (eds.),In Proceed-
ings 14th ustralianJoint Cnfrec onArtificial 
Intelligence, Adelid, Australi. 
Wieb, J., Wis, T., dCarie, C. 2005. 
Annotaig xprions fopins andmotionsi
lagu. LanguageReource nd Evaluaon, vol-
ume 39, issue 2-3, pp. 165-210.
Wilks,Y.(ed.)(01)Artifial Companios inSoci-
ety: scientific, economc, psychological and philoso-
phicalperspectives. John Benjamins: Asterdam. 
Wundt, W., 193.Grundis erPsycholgie,A. 
Kroer:Berlin.
Zue, V., as, J., oddeau, D., Goodine, D., 
HirschmaL.192The MIATISsytem, InProc
Wokop nsec andturl nguaHai, 
New Yor.
18

