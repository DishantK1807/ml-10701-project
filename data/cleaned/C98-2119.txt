Word Clustering and Disambiguation Based on Co-occurrence 
Data 
Hang Li and Naoki Abe 
Theory NEC Laboratory, Real World Computing Partnership 
c/o C&C Media Research Laboratories., NEC 
4-1-1 Miyazaki, Miyamae-ku, Kawasaki 216-8555, Japan 
{lihang,abe} @cem.cl.nec.co.jp 
Abstract 
We address the problem of clustering words (or con
structing a thesaurus) based on co-occurrence data, 
and using the acquired word classes to improve the 
accuracy of syntactic disambiguation. We view this 
problem as that of estimating a joint probability dis
tribution specifying the joint probabilities of word 
pairs, such as noun verb pairs. We propose an effi
cient algorithm based on the Minimum Description 
Length (MDL) principle for estimating such a prob
ability distribution. Our method is a natural ex
tension of those proposed in (Brown et al., 1992) 
and (Li and Abe, 1996), and overcomes their draw
backs while retaining their advantages. We then 
coinbined this clustering method with the disam
I)iguation method of (Li and Abe, 1995) to derive a 
disambiguation method that makes use of both auto
matically constructed thesauruses and a hand-made 
thesaurus. The overall disambiguation accuracy 
achieved by our method is 85.2%, which compares 
favorably against the accuracy (82.4%) obtained by 
the state-of-the-art disambiguation method of (Brill 
and Resnik, 1994). 
1 Introduction

We address the problem of clustering words, or that 
of constructing a thesaurus, based on co-occurrence 
data. We view this problem as that of estimating a 
joint probability distribution over word pairs, speci
fying the joint probabilitie~ of word pairs, such as 
noun verb pairs. In this paper, we assume that 
the joint distribution can be expressed in the fol
lowing manner, which is stated for noun verb pairs 
for the sake of readability: The joint probability of 
a noun and a verb is expressed as the product of the 
joint probability of the noun class and the verb class 
which the noun and the verb respectively belong to, 
and the conditional probabilities of the noun and the 
verb given their respective classes. 
As a method for estimating such a probability 
distribution, we propose an algorithm based on the 
Minimum Description Length (MDL) principle. Our 
clustering algorithm iteratively merges noun classes 
and verb classes in turn, in a bottom up foMfion. For 
each merge it performs, it calculates the increase 
in data description length resulting from merging 
any noun (or verb) class pair, and performs the 
merge having the least increase in data description 
length, provided that the increase in data descrip
tion length is less than the reduction in model de
scription length. 
There have been a number of methods proposed in 
the Literature to address the word clustering problem 
(e.g., (Brown et at., 1992; Pereira et al., 1993; Li and 
Abe, 1996)). The method proposed in this paper is 
a natural extension of t)oth Li & Abe's and Brown 
et al's methods, and is an attempt to overcome their 
drawbacks while retaining their advantages. 
The method of Brown et aI, which is based on the 
Maximum Likelihood Estimation (MLE), performs 
a merge which would result in the least reduction 
in (average) mutual information. Our method turns 
out to be equivalent to performing the merge with 
the least reduction in mutual information, provided 
that the reduction is below a certain threshold which 
depends on the size of the co-occurrence data and 
the number of classes in the current situation. This 
method, based on the MDL principle, takes into ac
count both the fit to data and the simplicity of a 
model, and thus can help cope with the over-fitting 
problem that the MLE-based method of Brown et al.faces. 
The model employed in (Li and Abe, 1996) is 
based on the assmnption that the word distribution 
within a class is a uniform distribution, i.e. every 
word in a same class is generated with an equal prob
ability. Employing such a model has the undesirable 
tendency of classifying into different classes those 
words that have similar co-occurrence patterns but 
have different absolute frequencies. The proposed 
method, in contrast, employs a model in which dif
ferent words within a same class can have different 
conditional generation probabilities, and thus can 
classify words in a way that is not affected by words' 
absolute fl'equencies and resolve the problem faced 
by the method of (Li and Abe, 1996). 
We evaluate our clustering method by using the 
word classes and the joint probabilities obtained by 
749 
it in syntactic disambiguation experiments. Our 
experimental results indicate that using the word 
classes constructed by our method gives better dis
ambiguation results than when using Li & Abe or 
Brown et al's methods. By combining thesauruses 
automatically constructed by our method and an 
existing hand-made thesaurus (WordNet), we were 
able to achieve the overall accuracy of 85.2% for pp
attachment disambiguation, which compares favor
ably against the accuracy (82.4%) obtained using the 
state-of-the-art method of (Brill and Resnik, 1994). 
2 Probability
Model 
Suppose available to us are co-occurrence data over 
two sets of words, such as the sample of verbs and 
the head words of their direct objects given in Fig. 1. 
Our goal is to (hierarchically) cluster the two sets 
of words so that words having similar co-occurrence 
patterns are classified in the same class, and output 
a thesaurus for each set of words. 
wine 
beer 
bread 
rice 
eat drink make 
0 3 t 
0 5 I 
4 0 2 
4 0 0 
Figure 1: Example co-occurrence data 
We can view this problem as that of estimating 
the best probability model from among a class of 
models of (probability distributions) which can give 
rise to the co-occurrence data. 
In this paper, we consider the following type of 
probability models. Assume without loss of gener
ality that the two sets of words are a set of nouns 
A/ and a set of verbs /2. A partition T~ of A/is a 
set of noun-classes satisfying UC.eT.G', = A/ and 
VCi,C 5 E Tn,CinCj = 0. A partition T, of)2 
can be defined analogously. We then define a proba
bility model of noun-verb co-occurrence by defining 
the joint probability of a noun n and a verb v as the 
product of the joint probability of the noun and verb 
classes that n and v belong to, and the conditional 
probabilities of n and v given their classes, that is, 
P(n, v) = P(C,, P(nIC,, ) â€¢ P(vlC,), (1) 
where C,~ and Cv denote the (unique) classes to 
which n and v belong. In this paper, we refer to 
this model as the 'hard clustering model,' since it is 
based on a type of clustering in which each word can 
belong to only one class. Fig. 2 shows an example of 
the hard clustering model that can give rise to the 
co-occurrence data in Fig. 1. 
P(vlCv) ~ I 0.5 
P(nlCn) 
wine 
0.4 beer 
bread 0,4 
rice 
make 
0.1 
0.1 
P(On.C'v) / 
Figure 2: Example hard clustering model 
3 Parameter
Estimation 
A particular choice of partitions for a hard clustering 
model is referred to as a 'discrete' hard-clustering 
model, with the probability parameters left to be 
estimated. The values of these parameters can be 
estimated based on the co-occurrence data by the 
Maximum Likelihood Estimation. For a given set of 
co-occurrence data 
s = {(nl, v2),..., vm)}, 
the maximum likelihood estimates of the parameters 
are defined as the values that maximize the following 
likelihood function with respect to the data: 
m m 
I-I p(,i, = 
i=l i=1 
It is easy to see that this is possible by setting the 
parameters as 
p(cn, f(c.,co), 
m 
Vx e az u v, P(xlc ) f(x) f(C,). 
tiere, m denotes the entire data size, f(Cn, Cv) the 
frequency of word pairs in class pair (Cn, Cv), f(x) 
the frequency of word x, and f(Cx) the frequency of 
words in class C~. 
4 Model
Selection Criterion 
The question now is what criterion should we employ 
to select the best model from among the possible 
models. Here we adopt the Minimum Description 
Length (MDL) principle. MDL (Rissanen, 1989) is 
a criterion for data compression and statistical esti
mation proposed in information theory. 
In applying MDL, we calculate the code length for 
encoding each model, referred to as the 'model de
scription length' L(M), the code length for encoding 
750 
the given data through the model, referred to as the 
'data description length' L(SIM ) and their sum: 
L(M, S) = L(M) + L(SIM ). 
The MDL principle stipulates thatt, for both data 
compression and statistical estimation, the best 
probability model with respect to given data is that 
which requires the least total description length. 
The data description length is calculated a~s 
L(,S\[M) = Z log /5(n, v), 
(n,v)e8 
where \[~ stands for the maxintum likelihood estimate 
of P (as defined in Section 3). 
We then calculate the model description length eus 
k L(M) := ~ logm, 
where k denotes the number of free parameters in the 
model, and m the entire data sizeJ In this paper, 
we ignore the code length for encoding a 'discrete 
model,' assuming implicitly that they are equal for 
all models and consider only the description length 
for encoding the parameters of at model as the model 
description length. 
If computation time were of no concern, we could 
in principle calculate the totatl description length for 
catch model and select the optimal model in terms of 
MDL. Since tile number of hard clustering models 
is of order O(N N. vv), where N and V denote the 
size of the noun set and the verb set, respectively, it 
would be infeasible to do so. We therefore need to 
devise an efficient atlgorithm that heuristically per
forms this task. 
5 Clustering
Algorithm 
The proposed algorithm, which we call '21)
Clustering,' iterattively selects at suboptimal MDL
model from among those hard clustering models 
which can be obtained from the current model by 
merging at nonn (or verb) class pair. As it turns out, 
the minimum description length criterion can be re
formalized in terms of (average) mutuatl information, 
and at greedy heuristic algorithm carl be formulated 
to calculate, in each iteration, the reduction of mu
tual information which would result from merging 
any noun (or verb) class pair, and perform the merge 
aWe note that there are alternative ways of calculating 
the parameter description length. For example, we can sep
arately encode the different types of probability parameters; 
the joint probabilities P(Cn, Cv), and the conditional prob
abilities P(nlCn ) and P(v{Cv). Since these alternatives are 
approximations of one another asymptotically, here we use 
only the simplest formulation. In the fifll paper, we plan to 
compm'c the eml)irical behavior of the alternatives. 
having the least mutual information reduction, pro
vided that the reduclion is below a variable threshold. 
2D-Clustering(S, b,,, b,) 
(S is the input co-occurrence data, and bn and b, 
are positive integers.) 
1. Initiatlize the set of noun classes T~ and the set 
of verb classes 7~ as: 
7;, = {{,,}1, H},T  = V}, 
where Af and V denote the noun set and the 
verb set, respectively. 
2. Repeatt the following three steps: 
(at) execute Merge(S, T~, ~/;, b,) to update T,~, 
(b) execute Merge(S, 7;,, T,~, b~) to update T~, 
(c) if 7;, and To are unchanged, go to Step 3. 
3. Construct and output at thesaurus for nouns 
based on the history of T,,, and one for verbs 
based on the history of T~. 
Next, we describe the procedure of 'Merge,' as it 
is being atpplicd to the set of noun classes with the 
set of verb classes fixed. 
Merge(S, 7;, b,,) 
1. For each class pair in T,~, calculate the reduc
tion of mutual informattion which would result 
from merging them. (The details will follow.) 
Discard those class pairs whose mutnatl informa
tion reduction (2) is not less than the threshold 
of 
(kB -kA) â€¢ logm 
2.m 
where m denotes the total data size, kn the 
number of free parameters in the model before 
the merge, and ka the number of free param
eters in the model after the merge. Sort the 
remaining class pairs in ascending order with 
respect to mutual information reduction. 
2. Merge the first bn class pairs in the sorted list. 
3. Output current 7;,. 
We perform (maximum of) bn merges at step 2 for 
improving efficiency, which will result in outputting 
an art-most bn-atry tree. Note that, strictly speatking, 
once we perform one merge, the model will change 
and there will no longer be at gtmratntee that the 
remaining merges still remain justifiable from the 
viewpoint of MDL. 
Next, we explain why the criterion in terms of 
description length can be reformalized in terms of 
mutual information. We denote the model before 
at merge as MB and the model after the merge as 
751 
MA. According to MDL, iliA should have the least 
increase in data description length 
~Ldat = L(S\[MA) n(s\[illB) > O, 
and at the same time satisfies 
(kB kA) log m ~Ldat < 
2 
This is due to the fact that the decrease in model 
description length equals 
L( MB) L(MA) = (kB leA)logm 2 >0, 
and is identical for each merge. 
In addition, suppose that MA is obtained by merg
ing two noun classes Ci and Cj in Me to a single 
noun class Cij. We in fact need only calculate the 
difference between description lengths with respect 
to these classes, i.e., 
6 Ldat
= --~C~eT~ ~,*eCij,,eC~ log/5( n, V) 
+ ~C, eT~ ~,~ec,,vec, log P(n, v) 
+  , eci,,ec  lÂ°g/5( n, v). 
Now using the identity 
P(n,v) = ~ ~ mr: c:~ p(C,,) " p(C,,) "--\~n,~v) 
P(C.~C~) n~ "~ r~," \ ~P(C.).P(C,,) 
" F'in) â€¢ YIV) 
we can rewrite the above as 
f'(c,,.c~) 6 L~at = ~c.eT. f(Cij, C~ ) log P(c,j).P(c~) 
-t-~C.eT~ f(Ci,C,)log b(c,,c~) ~(g,) fÂ°(c~) 
P(C i C,,) + Ec eT  f(6), C )log 
Thus, the quantity 6Ldat is equivalent to the mutual 
information reduction times the data size. 2 We con
clnde therefore that in our present context, a cluster
ing with the least data description length increase is 
equivalent to that with the least mutual information 
decrease. 
Canceling out /5(C~) and replacing the probabil
ities with their maximum likelihood estimates, we 
obtain 
I~L _1( w, "~ datm~-Z.c~er~(f(Ci,C.)+ f(Cj,Cv)) 
k log .t ( c,,c~ ) +.t ( c.~.c~) 
J(cd+I(c~) 
+ f(c , c,) log f(cd 
+ f(c , c,) log j(cj) ). 
(2) 
2 Average
mutual information between Tn and Tv is defined 
as 
'(Tn,Tv)= ~ ~ (P(Cn,Cv)logp(cn).p(cv)j 
CneTn CvET,j 
Therefore, we need calculate only this quantity for 
each possible merge at Step 1 of Merge. 
In our implementation of the algorithm, we first 
load the co-occurrence data into a matrix, with 
nouns corresponding to rows, verbs to columns. 
When merging a noun class in row i and that in 
row j (i < j), for each Cv we add f(Ci,C,,) and 
f(Cj, Cv) obtaining f(Cij, Cv), write f(C,~j, Cv) on 
row i, move f(Cla,t,C,,) to row j, and reduce the 
matrix by one row. 
By the above implementation, the worst case time 
complexity of the algorithm is O(N 3 â€¢ V + V 3 â€¢ N) 
where N denotes the size of the nouu set, V that of 
the verb set. If we can merge b, and b~ classes at 
each step, the algorithm will become slightly more 
g 3 . efficient with the time complexity of O(b~ â€¢ V+ 
Y7 
N). 
6 Related
Work 
6.1 Models

We can restrict the hard clustering model (1) by as
suming that words within a same class are generated 
with an equal probability, obtaining 
1 1 
P(,~, v) = P(C,, , C, ) . IC,, I IC,,l' 
which is equivalent to the model proposed by (Li and 
Abe, 1996). Employing this restricted model has the 
undesirable tendency to classify into different classes 
those words that have similar co-occurrence patterns 
but have different absolute frequencies. 
The hard clustering model defined in (1) can also 
be considered to be an extension of the model pro
posed by Brown et al. First, dividing (1) by P(v), 
we obtain 
= P(CnlC,,). P(nlC,~) â€¢ ( P(C~).PUlC.)'~ \ P(v) \] ' 
(3) P(C~).P(v\]C,,) 
Since hard clustering implies p@) = 1 
holds, we have 
P(nlv) : P(c,~Ic,) . P(nIC,~). 
In this way, the hard clustering model turns out to be 
a class-based bigram model and is similar to Brown 
et al's model. The difference is that the model of (3) 
assumes that the clustering for C,~ and the clustering 
for Cv can be different, while the model of Brown et 
al assumes that they are the same. 
A very general model of noun verb joint probabil
ities is a model of the following form: 
P(n,v)= ~ ~ P(C,~,Cv).P(n\]C,O.P(vlC,). 
C,~ E F. C~EF~ 
(4) 
752 
llere Fn denotes a set of noun classes satisfying 
L)c, ep C, = Af, but not necessarily disjoint. Sim
ilarly Fv is a set of not necessarily disjoint verb 
classes. We can view the problem of clustering words 
in general as estimation of such a model. This type 
of clustering in which a word can belong to several 
different classes is generally referred to as 'soft clus
tering.' If we assume in the above model that each 
verb fornrs a verb class by itself, then (4) becomes 
t'(,~,v) = ~ P(C,,,,,). P(niC,,), 
(5',~ E 1',~ 
which is equivalent to the model of Pereira et al. On 
the other hand, if we restrict the general model of (4) 
so that both noun classes and verb classes are dis
joint, then we obtain the hard clustering nrodel we 
propose here (1). All of these models, therefore, are 
some special cases of (4). Each specialization comes 
with its merit and demerit. For example, employing 
a model of soft clustering will make the clustering 
process more ttexible but also make the learning pro
eess more computationally demanding. Our choice 
of hard clustering obviously has the merits and de
merits of the soft clustering model reversed. 
6.2 Estilnation
criteria 
Our Inethod is also an extension of that proposed 
by Brown et al from the viewpoint of estimation cri
terion. Their method merges word classes so that 
the reduction in mutnal information, or equivalently 
the increase in data description length, is miuimized. 
Their method has the tendency to overlit the train
ing data, since it is based on MLE. Employing MDL 
can help solve this problem. 
7 Disambiguation
Method 
We apply the acquired word classes, or more specif
ically the probability model of co-occurrence, to the 
problem of structural disambiguation. In particular, 
we consider the problem of resolving pp-attachment 
ambiguities in quadruples, like (see, girl, with, tele
scope) and that of resolving ambiguities in com
pound noun triples, like (data, base, system). In 
the former, we determine to which of 'see' or 'girl' 
the phrase 'with telescope' should be attached. In 
the latter, we judge to which of 'base' or 'system' 
the word 'data' should be attached. 
We can perform pl)-attachment disambiguation by 
comparing the probabilities 
\[~ith (telescopelsee),/Swith (teleseopelgirl). (5) 
If the former is larger, we attach 'with telescope' 
to 'see;' if the latter is larger we attach it to 'girl;' 
otherwise we make no decision. (l)isambiguation on 
compound noun triples can be performed similarly.) 
Since the nuinber of probabilities to be estimated 
is extremely large, estimating all of these probabil
ities accurately is generally iufeasible (i.e., the data 
sparseness problem). Using our clustering model to 
calculate these conditional probabilities (by normal
izing tile joint probabilities with marginal probabil
ities) can solve this problem. 
We further enhance our disambiguation method 
by the following back-off procedure: We first esti
mate the two probabilities in question using hard 
clustering models constructed by our method. We 
also estimate the probabilities using an existing 
(hand-made) thesaurns with the 'tree cut' estima
tion method of (Li and Abe, 1995), and use these 
probability values when the probabilities estimated 
based on hard clustering models are both zero. Fi
nally, if both of them are still zero, we make a default 
decision. 
8 Experimental
Results 
8.1 Qualitative
ewduation 
In this experiment, we used heuristic rules to extract 
verbs and the head words of their direct objects from 
the tagged texts of the WSJ corpus (ACL/I)CI CD
ROMI) consisting of 126,084 sentences. 
--~llaro. umot. dutu 
stock, t~ond, socorlly 
â€¢ .m. ,coq)..co. 
Dank, 9ro~lp, firm 
pn,:o, mÃ— 
monoy, cash 
car, vohmlo 
pint*t, r,sk 
-~oltwaÂ¢o, nmwork 
procure, power 
Figure 3: A part of a constructed thesaurus 
We then constructed a number of thesauruses 
based on these data, using our method. Fig. 3 shows 
a part of a thesaurus for 100 raudomly selected 
nouns, based on their appearances as direct objects 
of 20 randomly selected verbs. The thesaurus seems 
to agree with human intuition to some degree, al
though it is constructed based oil a relatively small 
amount of co-occurrence data. For examt)le, 'stock,' 
'security,' and 'bond' are classified together, despite 
the fact that their absolute frequencies in the data 
vary a great deal (272, 59, and 79, respectively.) 
The results demonstrate a desirable feature of our 
method, namely, it classifies words based solely on 
the similarities in co-occurrence data, and is not af
fected by the absolute fi'equencies of the words. 
8.2 Colnpound
noun disamblguation 
We extracted compound noun doubles (e.g., 'data 
base') from the tagged texts of tile WSJ corpus and 
used them as training data, and then conducted 
753 
structural disambiguation on compound noun triples 
(e.g., 'data base system'). 
We first randomly selected 1,000 nouns from the 
corpus, and extracted compound noun doubles con
taining those nouns as training data and compound 
noun triples containing those nouns as test data. 
There were 8,604 training data and 299 test data. 
We hand-labeled the test data with the correct dis
ambiguation 'answers.' 
We performed clustering on the nouns on the 
left position and the nouns on the right position in 
the training data by using both our method ('2D
Clustering') and Brown et al's method ('Brown'). 
We actually implemented an extended version of 
their method, which separately conducts clustering 
for nouns on the left and those on the right (which 
should only improve the performance). 
u.v 
0,85 
0,8 
0.75 
0.7 o 
0.65 "Word,~\]ased" rown" ,2o-clu;,~o.nÂ¢ 
0,6 
0.55 
0...0 5 ~t_ 0.55 0.6 0,05 0,;' 0.15 0.8 0.85 0,0 Covorago 
Figure 4: Compound noun disambiguation results 
We next conducted structural disambiguation on 
the test data, using the probabilities estimated based 
on 2D-Clustering and Brown. We also tested the 
method of using the probabilities estimated based 
on word co-occurrences, denoted as 'Word-based.' 
Fig. 4 shows the results in terms of accuracy and 
coverage, where coverage refers to the percentage 
of test data for which the disambiguation method 
was able to make a decision. Since for Brown the 
number of classes finally created has to be designed 
in advance, we tried a number of alternatives and 
obtained results for each of them. (Note that, for 
2D-Clustering, the optimal number of classes is au
tomatically selected.) 
Table 1: Compound noun disambiguation results 
Method Ace.(%) 
Default 59.2 
Word-based + Default 73.9 
Brown + Default 77.3 
2D-Clustering + Default 78.3 
Tab. 1 shows the final results of all of the above 
methods combined with 'Default,' in which we at
tach the first noun to the neighboring noun when 
a decision cannot be made by each of the meth
ods. We see that 2D-Clustering+Default performs 
the best. These results demonstrate a desirable as
pect of 2D-Clustering, namely, its ability of automat
ically selecting the most appropriate level of clus
tering, resulting in neither over-generalization nor 
under-generalization. 
8.3 PP-attachment disambiguation 
We extracted triples (e.g., 'see, with, telescope') 
from the bracketed data of the WSJ corpus (Penn 
Tree Bank), and conducted PP-attachment disam
bignation on quadruples. We randomly generated 
ten sets of data consisting of different training and 
test data and conducted experiments through 'ten
fold cross validation,' i.e., all of the experimental 
results reported below were obtained by taking av
erage over ten trials. 
Table 2: PP-attachment disambiguation results 
Method Cov.(%) Ace.(%) 
Default 10O 56.2 
Word-based 32.3 95.6 
Brown 51.3 98.3 
2D-Clustering 51.3 98.3 
Li-Abe96 37.3 94.7 
WordNet 74.3 94.5 
NounClass-2DC 42.6 97.1 
We constructed word classes using our method 
('2D-Clustering') and the method of Brown et al.('Brown'). For both methods, following the pro
posal due to (Tokunaga et al., 1995), we separately 
conducted clustering with respect to each of the 10 
most frequently occurring prepositions (e.g., 'for,' 
'with,' etc). We did not cluster words for rarely 
occurring prepositions. We then performed disam
biguation based on 2D-Clustering and Brown. We 
also tested the method of using the probabilities es
timated based on word co-occurrences, denoted as 
'Word-based.' 
Next, rather than using the conditional probabili
ties estimated by our method, we only used the noun 
thesauruses constructed byour method, and applied 
the method of (Li and Abe, 1995) to estimate the 
best 'tree cut models' within the thesauruses a in 
order to estimate the conditional probabilities like 
those in (5). We call the disambiguation method 
using these probability values 'NounClass-2l)C.' We 
also tried the analogous method using thesauruses 
constructed by the method of (Li and Abe, 1996) 
3The method of (Li and Abe, 1995) outputs a 'tree cut 
model' in a given thesaurus with conditional probabilities at
tached to all the nodes in the tree cut. They use MDL to 
select the best tree cut model. 
754 
and estimating the best tree cut models (tiffs is ex
actly the disambiguation method proposed in that 
paper). Finally, we tried using a hand-made the
sanrus, WordNet (this is the same as the disam
l)iguation method used in (Li and Abe, 1995)). We 
denote these methods as qA-Abe96' and 'WordNet,' 
respectively. 
Tab. 2 shows the results tbr all these methods in 
terms of coverage and accuracy. 
Table 3: PP-attachment disambiguation results 
Method Ace.(%) 
Word-based + Default 
Brown + Default 
2D-Clustering + Default 
Li-Abe96 + Default 
WordNet + Default 
NounClass-2DC + Default 
69.5 
76.2 
76.2 
71.0 
82.2 
73.8 
2D-Clustering + WordNet + Default 85.2 
Brill-Resnik 82.4 
We. then enhanced each of these methods by using 
a default rule when a decision cannot be made, which 
is indicated as '+Default.' Tab. 3 shows the results 
of these experiments. 
We can make a number of observations from these 
results. (1) 2D-Clustering achieves a broader cover
age than NounClass-2DC. This is because in order 
to estimate the probabilities for disambiguation, the 
former exploits more iuformation than the latter. 
(2) For Brown, we show here only its best result, 
which happens to be the same as the result for 2D
Clustering, but in order to obtain this result we had 
to take the trouble of conducting a number of tests to 
tlnd the best level of clustering. For 2D-Clustering, 
this was done once and automatically. Compared 
with Li-Abe96, 2D-Clustering clearly performs bet
ter. Therefore we conclude that our method im
proves these previous clustering methods in one way 
or another. (3) 2D-Clustering outperforms WordNet 
in term of accuracy, but not in terms of coverage. 
This seems reasonable, since an automatically con
structed thesaurus is more domain dependent and 
therefore captures the domain dependent features 
better, and thus can help achieve higher accuracy. 
On the other hand, with the relatively small size of 
training data we had available, its coverage is smaller 
than that of a general purpose hand made thesaurus. 
The result indicates that it makes sense to combine 
automatically constructed thesauruses and a hand
made thesaurus, as we have proposed in Section 7. 
This method of combining both types of the
sauruses '2D-Clustering+WordNet+Default' was 
then tested. We see that this method performs the 
best. (See Tab. 3.) Finally, for comparison, we 
tested the 'transformation-based error-driven learn
ing' proposed in (Brill and Resnik, 1994), which is 
a state-of-the-art method for pp-attachment disam
biguation. Tab. 3 shows the result for this method 
as 'Brill-Resnik.' We see that our disambigua
tion method also peribrlns better than Brill-Resnik. 
(Note further that for Brill & Resnik's method, we 
need to use quadruples as training data, whereas 
ours only requires triples.) 
9 Conclusions

We have proposed a new method of clustering words 
based on co-occurrence data. Our method employs 
a probability model which naturally represents co
occurrence patterns over word pairs, and makes use 
of an etficient estimation algorithm based on the 
MDL principle. Our clustering method improves 
upon the previous methods proposed by Brown et al.and (Li and Abe, 1996), and filrthcrmore it can be 
used to derive a disambiguation method with overall 
disambiguation accuracy of 85.2%, which improves 
thc perforrnance of a state-of-the-art disambiguatiou 
method. 
The proposed algorithm, 2D-Clustering, can be 
used in practice, as long as the data size is at the 
level of the current Penn Tree Bank. Yet it is still 
relativcly computationally demanding, and thus an 
important future task is to fllrther improve on its 
computational efficiency. 
Acknowledgement 
We are grateful to Dr. S. Doi of NEC C&C Media 
Res. Labs. for his encouragement. We thank Ms. Y. 
Yamagnchi of NIS for her programming efforts. 

References 

E. Brill and P. Rcsnik. A rule-based approach to 
prepositional phrase attachment disambiguation. 
Proc. of COLING'94, pp. 1198-1204. 

P. F. Brown, V. J. Della Pietra, P. V. deSouza, J. 
C. Lai, and R. L. Mercer. 1992. Class-based n
gram models of natural language. Comp. Ling., 
18(4):283-298. 

H. Li and N. Abe. 1995. Generalizing case frames 
using a thesaurus and the MDL principle. Comp. 
Ling., (to appear). 

H. Li and N. Abe. 1996. Clustering words with the 
MDL principle. Proc. of COL1NG'96, pp. 4-9. 

F. Pcrcira, N. Tishby, and L. Lee. 1993. Distri
butional clustering of English words. Proc. of 
ACL'93, pp. 183--190. 

J. Rissanen. 1989. Slochaslic Complexily in ,5'tatisti
cal Inquiry. World Scientific Publishing Co., Sin
gapore. 

T. Tokunaga, M. lwayama, and II. Tanaka. Auto
matic thesaurus construction based-on grammat
ical relations. Proc. of IJCAI'95, pp. 1308-1313. 

