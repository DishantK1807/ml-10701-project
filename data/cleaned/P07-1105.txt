Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 832–839, Prague, Czech Republic, June 2007.
c©2007 Association for Computational Linguistics Grammar Approximation by Representative Sublanguage: A New Model for Language Learning Smaranda Muresan Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA smara@umiacs.umd.edu Owen Rambow Center for Computational Learning Systems Columbia University New York, NY 10027, USA rambow@cs.columbia.edu Abstract We propose a new language learning model that learns a syntactic-semantic grammar from a small number of natural language strings annotated with their semantics, along with basic assumptions about natural language syntax.
We show that the search space for grammar induction is a complete grammar lattice, which guarantees the uniqueness of the learned grammar.
1 Introduction
There is considerable interest in learning computational grammars.1 While much attention has focused on learning syntactic grammars either in a supervised or unsupervised manner, recently there is a growing interest toward learning grammars/parsers that capture semantics as well (Bos et al., 2004; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005).
Learning both syntax and semantics is arguably more difficult than learning syntax alone.
In formal grammar learning theory it has been shown that learning from “good examples,” or representative examples, is more powerful than learning from all the examples (Freivalds et al., 1993).
Haghighi and Klein (2006) show that using a handful of “proto1This research was supported by the National Science Foundation under Digital Library Initiative Phase II Grant Number IIS-98-17434 (Judith Klavans and Kathleen McKeown, PIs).
We would like to thank Judith Klavans for her contributions over the course of this research, Kathy McKeown for her input, and several anonymous reviewers for very useful feedback on earlier drafts of this paper.
types” significantly improves over a fully unsupervised PCFG induction model (their prototypes were formed by sequences of POS tags; for example, prototypical NPs were DT NN, JJ NN).
In this paper, we present a new grammar formalism and a new learning method which together address the problem of learning a syntactic-semantic grammar in the presence of a representative sample of strings annotated with their semantics, along with minimal assumptions about syntax (such as syntactic categories).
The semantic representation is an ontology-based semantic representation.
The annotation of the representative examples does not include the entire derivation, unlike most of the existing syntactic treebanks.
The aim of the paper is to present the formal aspects of our grammar induction model.
In Section 2, we present a new grammar formalism, called Lexicalized Well-Founded Grammars, a type of constraint-based grammars that combine syntax and semantics.
We then turn to the two main results of this paper.
In Section 3 we show that our grammars can always be learned from a set of positive representative examples (with no negative examples), and the search space for grammar induction is a complete grammar lattice, which guarantees the uniqueness of the learned grammar.
In Section 4, we propose a new computationally efficient model for grammar induction from pairs of utterances and their semantic representations, called Grammar Approximation by Representative Sublanguage (GARS).
Section 5 discusses the practical use of our model and Section 6 states our conclusions and future work.
832 2 Lexicalized Well-Founded Grammars Lexicalized Well-Founded Grammars (LWFGs) are a type of Definite Clause Grammars (Pereira and Warren, 1980) where: (1) the Context-Free Grammar backbone is extended by introducing a partial ordering relation among nonterminals (wellfounded) 2) each string is associated with a syntactic-semantic representation called semantic molecule; 3) grammar rules have two types of constraints: one for semantic composition and one for ontology-based semantic interpretation.
The partial ordering among nonterminals allows the ordering of the grammar rules, and thus facilitates the bottom-up induction of these grammars.
The semantic molecule is a syntactic-semantic representation of natural language strings a0a2a1a4a3a6a5a8a7a9a11a10 where a12 (head) encodes the information required for semantic composition, and a13 (body) is the actual semantic representation of the string.
Figure 1 shows examples of semantic molecules for an adjective, a noun and a noun phrase.
The representations associated with the lexical items are called elementary semantic molecules (I), while the representations built by the combination of others are called derived semantic molecules (II).
The head of the semantic molecule is a flat feature structure, having at least two attributes encoding the syntactic category of the associated string, cat, and the head of the string, head.
The set of attributes is finite and known a priori for each syntactic category.
The body of the semantic molecule is a flat, ontology-based semantic representation.
It is a logical form, built as a conjunction of atomic predicates a14a16a15a18a17a20a19a21a15a23a22a25a24a27a26a29a28a31a30a32a14a16a33a34a26a35a26a37a36a38a28 a3 a14a16a15a39a17a20a19a21a15a23a22a25a24a40a26a11a28, where variables are either concept or slot identifiers in an ontology.
For example, the adjective major is represented as a14a42a41a44a43a45a30a47a46a37a48a49a33 a3a51a50 a33a53a52a54a17a20a36a20a55a29a41a57a56a58a30a60a59 a3 a41a44a43a31a28, which says that the meaning of an adjective is a concept (a41a44a43a45a30a47a46a35a48a61a33 a3a62a50 a33a53a52a54a17a20a36 ), which is a value of a property of another concept (a41a63a56a53a30a60a59 a3 a41a44a43 ) in the ontology.
The grammar nonterminals are augmented with pairs of strings and their semantic molecules.
These pairs are called syntagmas, and are denoted by a64 a3 a65 a0 a55 a0 a1a67a66 a3 a65 a0 a55 a5 a7a9a11a10 a66. There are two types of constraints at the grammar rule level — one for semantic composition (defines how the meaning of a natural language expression is composed from the meaning I.
Elementary Semantic Molecules (major/adj)a68 = a69a70 a70 a70 a70 a70 a70a71a20a72a23a73 a74a75 a76 cat adj head a77 a73 mod a77a79a78 a80 a81 a82 a83 a73a85a84 a77 a73 .isa = major, a77a79a78 .Y=a77 a73a25a86 a87a42a88 a88 a88 a88 a88 a88 a89 (damage/noun)a68 = a69a70 a70 a70 a70 a70 a70a71 a72a18a90 a74a75 a76 cat noun nr sg head a77 a90 a80 a81 a82 a83 a90 a84 a77 a90 .isa = damagea86 a87 a88 a88 a88 a88 a88 a88 a89 II.
Derived Semantic Molecule (major damage)a68 = a69a70 a70 a70 a70 a70 a70a71 a72 a74a75 a76 cat n nr sg head X a80 a81 a82 a83 a84 a77 a73 .isa = major, X.Y= a77 a73, X.isa=damagea86 a87a42a88 a88 a88 a88 a88 a88 a89 III.
Constraint Grammar Rule a91a93a92a95a94a97a96a29a98a85a99a100a16a101a8a102a54a103a105a104a107a106a85a108a11a92a95a94 a73 a96a25a98 a99 a73 a100 a73 a101a8a102a16a96a67a91a93a92a95a94 a78 a96a29a98 a99 a78 a100 a78 a101a109a102a11a110a39a111a113a112a32a114a42a115a117a116a45a96a42a111a34a114a42a118a120a119a121a114a120a92a122a83a42a102 a111a113a112a32a114a42a115a117a116a45a92 a72 a96 a72a39a73 a96 a72 a78 a102a58a123a125a124 a72a49a126 a127a129a128a25a130 a123a132a131a53a96 a72a49a126 a72a39a133a109a128 a106a27a123 a72a39a73a29a126 a134a136a135 a106a31a96 a72a49a126 a72a39a133a109a128 a106a27a123 a72 a78 a126 a72a39a133a109a128 a106a31a96 a72a49a126 a131a45a137a40a123 a72 a78 a126 a131a45a137a35a96 a72 a73 a126 a127a129a128a29a130 a123 a128 a106a85a108a35a96 a72 a78 a126 a127a129a128a25a130 a123a132a131a61a138 a111 a114a42a118a120a119a121a114 a92a121a83a67a102 returns a77 a73 =MAJOR, a77 =DAMAGE, a139 =DEGREE from ontology Figure 1: Examples of two elementary semantic molecules (I), a derived semantic molecule (II) obtained by combining them, and a constraint grammar rule together with the constraints a140a142a141a109a143a37a144a146a145, a140a147a143a37a148a61a149a150a143 (III) . of its parts) and one for ontology-based semantic interpretation.
An example of a LWFG rule is given in Figure 1(III).
The composition constraints a140 a141a109a143a37a144a146a145 applied to the heads of the semantic molecules, form a system of equations that is a simplified version of “path equations” (Shieber et al., 1983), because the heads are flat feature structures.
These constraints are learned together with the grammar rules.
The ontology-based constraints represent the validation on the ontology, and are applied to the body of the semantic molecule associated with the left-hand side nonterminal.
They are not learned.
Currently, a140a151a143a37a148a61a149a150a143 is a predicate which can succeed or fail.
When it succeeds, it instantiates the variables of the semantic representation with concepts/slots in the ontology.
For example, given the phrase major damage, a140a142a143a37a148a61a149a150a143 succeeds and returns (a41a152a43 =MAJOR, a41 =DAMAGE, a59 =DEGREE), while given the phrase major birth it fails.
We leave the discussion of the ontology constraints for a future paper, since it is not needed for the main result of this paper.
We give below the formal definition of Lexical833 ized Well-Founded Grammars, except that we do not define formally the constraints due to lack of space (see (Muresan, 2006) for details).
Definition 1.
A Lexicalized Well-Founded Grammar (LWFG) is a 6-tuple, a0 a3 a14a2a1a151a55a3a1 a1 a55a5a4a7a6 a55a9a8 a55a5a10 a6 a55a3a11 a28, where: 1.
a1 is a finite set of terminal symbols.
2. a1 a1 is a finite set of elementary semantic molecules corresponding to the set of terminal symbols.
3. a4a7a6 is a finite set of nonterminal symbols.
4. a8 is a partial ordering relation among the nonterminals.
5. a10a12a6 is a set of constraint rules.
A constraint rule is written a13 a65 a64 a66 a14 a15 a43 a65 a64 a43 a66 a55a23a30a23a30a23a30a61a55 a15 a148 a65 a64 a148 a66a17a16 a140 a65a3a18 a64 a66, where a18 a64 a3 a65 a64a79a55a11a64 a43a49a55a23a30a85a30a85a30a85a55a11a64a40a148 a66 such that a64 a3 a65 a0 a55 a0 a1a67a66 a55a11a64a20a19 a3 a65 a0 a19a37a55 a0 a19 a1 a66 a55a9a21a23a22 a46a24a22 a19a136a55 a0 a3 a0 a43a26a25a27a25a27a25 a0 a148a27a55 a0 a1a57a3 a0 a1 a43a29a28 a25a27a25a27a25 a28 a0 a1 a148, and a28 is the semantic composition operator.
For brevity, we denote a rule by a13 a14 a30a31a16 a140, where a13 a32a33a4 a6 a55 a30 a32a33a4a35a34 a6 . For the rules whose left-hand side are preterminals, a13 a65 a64 a66a36a14, we use the notation a13 a14 a64 . There are three types of rules: ordered non-recursive, ordered recursive, and non-ordered rules.
A grammar rule a13 a65 a64 a66a37a14 a15 a43 a65 a64 a43 a66 a55a23a30a23a30a23a30a61a55 a15 a148 a65 a64a107a148 a66a17a16 a140 a65a3a18 a64 a66, is an ordered rule, if for all a15 a19, we have a13a38a8 a15 a19 . In LWFGs, each nonterminal symbol is a lefthand side in at least one ordered non-recursive rule and the empty string cannot be derived from any nonterminal symbol.
6. a11a39a32a23a4a7a6 is the start nonterminal symbol, and a40 a13a41a32a42a4a43a6a93a55a3a11a37a8a39a13 (we use the same notation for the reflexive, transitive closure of a8 ).
The relation a8 is a partial ordering only among nonterminals, and it should not be confused with information ordering derived from the flat feature structures.
This relation makes the set of nonterminals well-founded, which allows the ordering of the grammar rules, as well as the ordering of the syntagmas generated by LWFGs.
Definition 2.
Given a LWFG, a0, the ground syntagma derivation relation, a44 a6a45,2 is defined as: a46a48a47a50a49 a46a52a51 a53 a54 a49 (if a64 a3 a65 a0 a55 a0 a1 a66 a55 a0 a32 2The ground derivation (“reduction” in (Wintner, 1999)) can be viewed as the bottom-up counterpart of the usual derivation.
a1a151a55 a0 a1 a32 a1 a1, i.e., a13 is a preterminal), and a55a57a56 a51 a53 a54 a49 a56a59a58 a19a61a60a146a43 a58a63a62a63a62a63a62a58 a148 a58 a46a65a64a66a49a68a67a69a47 a55a26a70 a64a66a49 a70 a67 a58a63a62a63a62a63a62a58a55a57a71 a64a66a49 a71 a67a3a72a27a73a74a64a59a75a49a76a67 a46 a51 a53 a54 a49 . In LWFGs all syntagmas a64 a3 a65 a0 a55 a0a151a1 a66, derived from a nonterminal a13 have the same category of their semantic molecules a0 a1 .3 The language of a grammar a0 is the set of all syntagmas generated from the start symbol a11, i.e., a77 a65 a0 a66 a3 a78 a64a80a79a64 a3 a65 a0 a55 a0a142a1 a66 a55 a0 a32a81a1 a34 a55a3a11 a44 a6 a45 a64a83a82 . The set of all syntagmas generated by a grammar a0 is a77 a49 a65 a0 a66 a3a84a78 a64a80a79a64 a3 a65 a0 a55 a0 a1a67a66 a55 a0 a32a85a1 a34 a55a3a86a87a13a84a32 a4 a6 a55a5a13a88a44 a6 a45 a64a83a82 . Given a LWFG a0 we call a set a89 a49a91a90a77 a49 a65 a0 a66 a sublanguage of a0 . Extending the notation, given a LWFG a0, the set of syntagmas generated by a rule a65 a13 a14 a30a31a16 a140 a66 a32a92a10a12a6 is a77 a49 a65 a13 a14 a30a31a16 a140 a66 a3 a78 a64a80a79a64 a3 a65 a0 a55 a0 a1 a66 a55 a0 a32a93a1 a34 a55 a65 a13 a14 a30a31a16 a140 a66 a44 a6 a45 a64a83a82, where a65 a13 a14a94a30a31a16 a140 a66 a44 a6a45 a64 denotes the ground derivation a13a95a44 a6a45 a64 obtained using the rule a13 a14 a30a31a16 a140 in the last derivation step (we have bottom-up derivation).
We will use the short notation a77 a49 a65 a36 a66, where a36 is a grammar rule.
Given a LWFG a0 and a sublanguage a89 a49 (not necessarily of a0 ) we denote by a96 a65 a0 a66 a3 a77 a49 a65 a0 a66a98a97 a89 a49, the set of syntagmas generated by a0 reduced to the sublanguage a89 a49 . Given a grammar rule a36a23a32a99a10a100a6, we call a96 a65 a36 a66 a3 a77 a49 a65 a36 a66a101a97 a89 a49 the set of syntagmas generated by a36 reduced to the sublanguage a89 a49 . As we have previously mentioned, the partial ordering among grammar nonterminals allows the ordering of the syntagmas generated by the grammar, which allows us to define the representative examples of a LWFG.
Representative Examples.
Informally, the representative examples a89a103a102 of a LWFG, a0, are the simplest syntagmas ground-derived by the grammar a0, i.e., for each grammar rule there exist a syntagma which is ground-derived from it in the minimum number of steps.
Thus, the size of the representative example set is equal with the size of the set of grammar rules, a79a89a103a102a104a79 a3 a79a10a12a6a104a79.
This set of representative examples is used by the grammar learning model to generate the candidate hypotheses.
For generalization, a larger sublanguage a89 a49a35a105 a89a106a102 is used, which we call representative sublanguage.
3This property is used for determining the lhs nonterminal of the learned rule.
834 PSfrag replacements a0a2a1a4a3a5a1a7a6a8a3a10a9a12a11a14a13 a0a2a1a4a3a5a1a7a6a8a3a10a9a12a11a14a13 a0a15a1a16a3a17a1a7a6a8a3a10a9a18a11a19a13 a0a15a1a16a3a17a1a7a6a8a3a10a9a18a11a19a13 a0a21a20a22a3a17a1a23a6a8a24a26a25a28a27a29a3a17a1 a0a21a30a31a3a32a20a33a6a8a34a36a35a38a37a39a3a17a1 a0a40a20a22a3a17a1a23a6a8a24a26a25a28a27a33a3a5a1 a0a40a30a12a41a42a3a32a20a43a6a8a34a36a35a38a37a44a3a10a9a12a11a14a13 a0a40a20a12a41a29a3a17a1a7a6a45a24a26a25a28a27a42a3a10a9a12a11a14a13 a0a40a30a46a3a10a20a33a6a8a34a36a35a38a37a47a3a5a1 a0a21a20 a41 a3a17a1a7a6a45a24a26a25a28a27a42a3a10a9a12a11a14a13 a0a21a30 a41 a3a10a20a43a6a45a34a48a35a49a37a44a3a50a9a12a11a19a13 a51a53a52 a54 a51a53a52 a54 a51a53a52 a54 a51a53a52 a54 a55 a56 a57 a52 a57a42a58 a59 = a60 the, noise, loud, cleara61 a62a64a63 = a60 noise, loud noise, the noisea61 a62a66a65 =a62 a63a68a67 a60 clear loud noise, the loud noisea61 a69a19a70a72a71a74a73 =a62a75a65 a69a19a70a72a76a78a77a79a73 =a62a64a63 a67 a60 clear loud noisea61 a69a19a70a72a76a74a80a81a73 =a62 a63a16a67 a60 the loud noisea61 a69a19a70a72a82a74a73 =a62a83a63 Rule specialization steps a84a86a85a88a87 a73 a89 a84a86a90a85 a84a92a91a93a87 a73 a89 a84a86a90a91 Rule generalization steps a84a90a85 a87 a73 a94 a84a86a85 a84a81a90a91 a87 a73 a94 a84a91 Figure 2: Example of a simple grammar lattice.
All grammars generate a89a43a102, and only a95 generates a89 a49 (a1 is a common lexicon for all the grammars) 3 A Grammar Lattice as a Search Space for Grammar Induction In this section we present a class of Lexicalized Well-Founded Grammars that form a complete lattice.
This grammar lattice is the search space for our grammar induction model, which we present in Section 4.
An example of a grammar lattice is given in Figure 2, where for simplicity, we only show the context-free backbone of the grammar rules, and only strings, not syntagmas.
Intuitively, the grammars found lower in the lattice are more specialized than the ones higher in the lattice.
For learning, a89 a102 is used to generate the most specific hypotheses (grammar rules), and thus all the grammars should be able to generate those examples.
The sublanguage a89 a49 is used during generalization, thus only the most general grammar, a95, is able to generate the entire sublanguage.
In other words, the generalization process is bounded by a89 a49, that is why our model is called Grammar Approximation by Representative Sublanguage.
There are two properties that LWFGs should have in order to form a complete lattice: 1) they should be unambiguous, and 2) they should preserve the parsing of the representative example set, a89a31a102 . We define these two properties in turn.
Definition 3.
A LWFG, a0, is unambiguous w.r.t. a sublanguage a89 a49 a90 a77 a49 a65 a0 a66 if a40 a64 a32 a89 a49 there is one and only one rule that derives a64 . Since the unambiguity is relative to a set of syntagmas (pairs of strings and their semantic molecules) and not to a set of natural language strings, the requirement is compatible with modeling natural language.
For example, an ambiguous string such as John saw the man with the telescope corresponds to two unambiguous syntagmas.
In order to define the second property, we need to define the rule specialization step and the rule generalization step of unambiguous LWFGs, such that they are a89a103a102 -parsing-preserving and are the inverse of each other.
The property of a89a31a102 -parsingpreserving means that both the initial and the specialized/generalized rules ground-derive the same syntagma, a64 a46 a32 a89 a102 . Definition 4.
The rule specialization step: a46a65a64a66a49a38a96 a67a69a47a29a97 a55 a64a49 a51 a98 a67a100a99a48a72a27a73a15a96 a55 a64a49 a98 a67a47a42a101a52a72a9a73 a98 a46a26a64a49a38a96 a67a47a29a97a102a101a18a99a74a72a27a73 a90 a96 is a89 a102 -parsing-preserving, if there exists a64 a46 a32 a89 a102 and a36a104a103a86a105a8a148 a44 a6 a45 a64 a46 and a36a12a106a32a145a49a105a37a141 a44 a6 a90 a45 a64 a46, where a36a49a103a81a105a8a148 = a107 a70a100a108 a96 a73a23a109a111a110a2a112a7a70a100a108 a51 a98 a73a114a113a83a115a117a116 a96, a36 a55 = a112a118a70a100a108 a98 a73a7a109a120a119a74a115a121a116 a98, and a36a18a106a67a145a49a105a37a141 = a107 a70a100a108 a96 a73a122a109a123a110a121a119a124a113a64a115a102a116 a90 a96 . We write a36a104a103a86a105a35a148 a125a98 a126 a36a18a106a67a145a49a105a37a141 . The rule generalization step : a46a65a64a66a49a38a96a57a67a47a29a97a102a101a18a99a57a72a9a73 a90 a96 a55 a64a49 a98 a67a47a42a101a48a72 a73 a98 a46a65a64a66a49a38a96a57a67a47a29a97 a55 a64a49 a51 a98 a67a127a99a48a72a27a73a122a96 is a89 a102 -parsing-preserving, if there exists a64 a46 a32 a89 a102 and a36 a106a67a145a49a105a37a141 a44 a6 a90a45 a64 a46 and a36 a103a86a105a35a148 a44 a6a45 a64 a46 . We write a36 a106a32a145a49a105a37a141 a125a98 a128 a36a49a103a86a105a35a148 . Since a64 a46 is a representative example, it is derived in the minimum number of derivation steps, and thus the rule a36 a55 is always an ordered, non-recursive rule.
835 The goal of the rule specialization step is to obtain a new target grammar a0 a1 from a0 by modifying a rule of a0 . Similarly, the goal of the rule generalization step is to obtain a new target grammar a0 from a0 a1 by modifying a rule of a0 a1 . They are not to be taken as the derivation/reduction concepts in parsing.
The specialization/generalization steps are the inverse of each other.
From both the specialization and the generalization step we have that: a77 a49 a65 a36 a103a86a105a35a148 a66 a105 a77 a49 a65 a36 a106a67a145a49a105a37a141 a66 . In Figure 2, the specialization step a36a58a56 a125 a70 a126 a36 a1 a56 is a89 a102 -parsing-preserving, because the rule a36 a1 a56 groundderives the syntagma loud noise.
If instead we would have a specialization step a36a58a56 a125a85 a126 a36 a1 a1 a56 (a36 a1 a1 a56 a3 a4 a21 a14 a13a1a0a35a52 a13a2a0a37a52a7a4 a21 ), it would not be a89 a102 -parsingpreserving since the syntagma loud noise could no longer be ground-derived from the rule a36 a1 a1 a56 (which requires two adjectives).
Definition 5.
A grammar a0 a1 is one-step specialized from a grammar a0, a0 a125 a70 a126 a0 a1, if a86a113a36a61a55a29a36a54a43a24a32 a10a12a6 and a86a113a36 a1 a55a29a36a54a43 a32a92a10 a6 a90, s.t. a36 a125 a70 a126 a36 a1, and a40a4a3a6a5 a3 a36a20a55 a3 a32 a10 a6 iff a3 a32 a10 a6 a90.
A grammar a0 a1 is specialized from a grammar a0, a0 a44a126 a0 a1, if it is obtained from a0 in a19 -specialization steps: a0 a125 a70 a126 a25a27a25a27a25 a125 a71 a126 a0 a1, where a19 is finite.
We extend the notation so that we have a0 a44 a126 a0 . Similarly, we define the concept of a grammar a0 generalized from a grammar a0 a1, a0 a1 a44a128 a0 using the rule generalization step.
In Figure 2, the grammar a7 is one-step specialized from the grammar a0 a43, i.e., a0 a43 a125 a70 a126 a7, since a7 preserve the parsing of the representative examples a89a106a102 . A grammar which contains the rule a36 a1 a1 a56 a3 a4 a21 a14 a13a2a0a37a52a7a13a2a0a37a52 a4 a21 instead of a36 a1 a56 is not specialized from the grammar a0 a43 since it does not preserve the parsing of the representative example set, a89 a102 . Such grammars will not be in the lattice.
In order to define the grammar lattice we need to introduce one more concept: a normalized grammar w.r.t. a sublanguage.
Definition 6.
A LWFG a0 is called normalized w.r.t. a sublanguage a89 a49 (not necessarily of G), if none of the grammar rules a36a102a106a32a145a104a105a35a141 of a0 can be further generalized to a rule a36a104a103a86a105a35a148 by the rule generalization step such that a96 a65 a36a12a106a67a145a38a105a35a141 a66a9a8 a96 a65 a36a49a103a86a105a35a148 a66 . In Figure 2, grammar a95 is normalized w.r.t. a89 a49, while a7, a0 a43 and a0a132a56 are not.
We now define a grammar lattice a10 which will be the search space for our grammar learning model.
We first define the set of lattice elements a11 . Let a95 be a LWFG, normalized and unambiguous w.r.t. a sublanguage a89 a49 a90 a77 a49 a65 a95 a66 which includes the representative example set a89a50a102 of the grammar a95 (a89 a49 a105 a89a106a102 ).
Let a11 a3a95a78 a0 a79a95 a44 a126 a0 a82 be the set of grammars specialized from a95 . We call a95 the top element of a11, and a7 the bottom element of a11, if a40 a0 a32a12a11a142a55a81a95 a44 a126 a0a6a13 a0 a44 a126 a7 . The bottom element, a7, is the grammar specialized from a95, such that the right-hand side of all grammar rules contains only preterminals.
We have a96 a65 a95 a66 a3 a89 a49 and a96 a65 a7 a66 a105 a89 a102 . The grammars in a11 have the following two properties (Muresan, 2006): a14 For two grammars a0 and a0 a1, we have that a0 a1 is specialized from a0 if and only if a0 is generalized from a0 a1, with a77 a49 a65 a0 a66 a105 a77 a49 a65 a0 a1 a66 . a14 All grammars in a11 preserve the parsing of the representative example set a89a50a102 . Note that we have that for a0 a55a3a0 a1 a32a15a11, if a0 a44a126 a0 a1 then a96 a65 a0 a66 a105 a96 a65 a0 a1 a66 . The system a10 a3 a14a16a11 a55 a44a126 a28 is a complete grammar lattice (see (Muresan, 2006) for the full formal proof).
In Figure 2 the grammars a0 a43, a0 a56,a95, a7 preserve the parsing of the representative examples a89 a102 . We have that a95 a125 a70 a126 a0 a43, a95 a125 a70 a126 a0 a56, a0 a56 a125 a70 a126 a7, a0 a43 a125 a70 a126 a7 anda95 a44a126 a7 . Due to space limitation we do not define here the least upper bound (a17a19a18a97a13 ), a20 and the greatest lower bound (a21a22a17a129a13 ), a23 operators, but in this example a95 = a0 a43a24a20 a0 a56, a7 = a0 a43a25a23a24a0a132a56 . In oder to give a learnability theorem we need to show that a7 and a95 elements of the lattice can be built.
First, an assumption in our learning model is that the rules corresponding to the grammar preterminals are given.
Thus, for a given set of representative examples, a89 a102, we can build the grammar a7 using a bottom-up robust parser, which returns partial analyses (chunks) if it cannot return a full parse.
In order to soundly build thea95 element of the grammar lattice from the a7 grammar through generalization, we must give the definition of a grammar a0 conformal w.r.t. a89 a49 . 836 Definition 7.
A LWFG a0 is conformal w.r.t. a sublanguage a89 a49 a90 a77 a49 a65 a0 a66 iff a0 is normalized and unambiguous w.r.t. a89 a49 and the rule specialization step guarantees that a96 a65 a36a18a103a86a105a35a148 a66a1a0 a96 a65 a36a18a106a67a145a38a105a35a141 a66 for all grammars specialized from a0 . The only rule generalization steps allowed in the grammar induction process are those which guarantee the same relation a96 a65 a36a102a106a67a145a38a105a35a141 a66 a8 a96 a65 a36a49a103a86a105a35a148 a66, which ensures that all the generalized grammars belong to the grammar lattice.
In Figure 2, a95 is conformal to the given sublanguage a89 a49 . If the sublanguage were a89 a44 a49 a3 a89 a102a3a2 a78 clear loud noise a82 then a95 would not be conformal to a89 a44 a49 since a96 a65 a95 a66 a3 a96 a65 a0 a43 a66 a3 a89 a44 a49 and thus the specialization step would not satisfy the relation a96 a65 a4a5a4 a14a7a6 a22a45a26a12a4 a21 a66a8a0 a96 a65 a4a5a4 a14a7a6 a22a23a26a12a4 a17 a18a27a19 a66 . During learning, the generalization step cannot generalize from grammar a0 a43 to a95 . Theorem 1 (Learnability Theorem).
If a89a50a102 is the set of representative examples associated with a LWFG a0 conformal w.r.t. a sublanguage a89 a49 a105 a89a106a102, then a0 can always be learned from a89a50a102 and a89 a49 as the grammar lattice top element (a95 a3 a0 ).
The proof is given in (Muresan, 2006).
If the hypothesis of Theorem 1 holds, then any grammar induction algorithm that uses the complete lattice search space can converge to the lattice top element, using different search strategies.
In the next section we present our new model of grammar learning which relies on the property of the search space as grammar lattice.
4 Grammar
Induction Model Based on the theoretical foundation of the hypothesis search space for LWFG learning given in the previous section, we define our grammar induction model.
First, we present the LWFG induction as an Inductive Logic Programming problem.
Second, we present our new relational learning model for LWFG induction, called Grammar Approximation by Representative Sublanguage (GARS).
4.1 Grammar
Induction Problem in ILP-setting Inductive Logic Programming (ILP) is a class of relational learning methods concerned with inducing first-order Horn clauses from examples and background knowledge.
Kietz and Dˇzeroski (1994) have formally defined the ILP-learning problem as the tuple a14 a126 a55 a77a101a15 a55 a77 a89a57a55 a77a10a9 a28, where a126 is the provability relation (also called the generalization model), a77a80a15 is the language of the background knowledge, a77 a89 is the language of the (positive and negative) examples, and a77a10a9 is the hypothesis language.
The general ILP-learning problem is undecidable.
Possible choices to restrict the ILP-problem are: the provability relation, a126, the background knowledge and the hypothesis language.
Research in ILP has presented positive results only for very limited subclasses of first-order logic (Kietz and Dˇzeroski, 1994; Cohen, 1995), which are not appropriate to model natural language grammars.
Our grammar induction problem can be formulated as an ILP-learning problem a14 a126 a55 a77a80a15 a55 a77 a89 a55 a77a11a9 a28 as follows: a14 The provability relation, a126, is given by robust parsing, and we denote it by a126 a125 a145 . We use the “parsing as deduction” technique (Shieber et al., 1995).
For all syntagmas we can say in polynomial time whether they belong or not to the grammar language.
Thus, using the a126 a125 a145 as generalization model, our grammar induction problem is decidable.
a14 The language of background knowledge, a77a80a15, is the set of LWFG rules that are already learned together with elementary syntagmas (i.e., corresponding to the lexicon), which are ground atoms (the variables are made constants).
a14 The language of examples, a77 a89 are syntagmas of the representative sublanguage, which are ground atoms.
We only have positive examples.
a14 The hypothesis language, a77a10a9, is a LWFG lattice whose top element is a conformal grammar, and which preserve the parsing of representative examples.
4.2 Grammar
Approximation by Representative Sublanguage Model We have formulated the grammar induction problem in the ILP-setting.
The theoretical learning model, 837 called Grammar Approximation by Representative Sublanguage (GARS), can be formulated as follows: Given: a14 a representative example set a89a50a102, lexically consistent (i.e., it allows the construction of the grammar lattice a7 element) a14 a finite sublanguage a89 a49, conformal and thus unambiguous, which includes the representative example set, a89 a49 a105 a89a106a102 . We called this sublanguage, the representative sublanguage Learn a grammar a0, using the above ILP-learning setting, such that a0 is unique and a89 a49 a90 a77 a49 a65 a0 a66 . The hypothesis space is a complete grammar lattice, and thus the uniqueness property of the learned grammar is guaranteed by the learnability theorem (i.e., the learned grammar is the lattice top element).
This learnability result extends significantly the class of problems learnable by ILP methods.
The GARS model uses two polynomial algorithms for LWFG learning.
In the first algorithm, the learner is presented with an ordered set of representative examples (syntagmas), i.e., the examples are ordered from the simplest to the most complex.
The reader should remember that for a LWFG a0, there exists a partial ordering among the grammar nonterminals, which allows a total ordering of the representative examples of the grammar a0 . Thus, in this algorithm, the learner has access to the ordered representative syntagmas when learning the grammar.
However, in practice it might be difficult to provide the learner with the “true” order of examples, especially when modeling complex language phenomena.
The second algorithm is an iterative algorithm that learns starting from a random order of the representative example set.
Due to the property of the search space, both algorithms converge to the same target grammar.
Using ILP and theory revision terminology (Greiner, 1999), we can establish the following analogy: syntagmas (examples) are “labeled queries”, the LWFG lattice is the “space of theories”, and a LWFG in the lattice is “a theory.” The first algorithm learns from an “empty theory”, while the second algorithm is an instance of “theory revision”, since the grammar (“theory”) learned during the first iteration, is then revised, by deleting and adding rules.
Both of these algorithms are cover set algorithms.
In the first step the most specific grammar rule is generated from the current representative example.
The category name annotated in the representative example gives the name of the lhs nonterminal (predicate invention in ILP terminology), while the robust parser returns the minimum number of chunks that cover the representative example.
In the second step this most specific rule is generalized using as performance criterion the number of the examples in a89 a49 that can be parsed using the candidate grammar rule (hypothesis) together with the previous learned rules.
For the full details for these two algorithms, and the proof of their polynomial efficiency, we refer the reader to (Muresan, 2006).
5 Discussion
A practical advantage of our GARS model is that instead of writing syntactic-semantic grammars by hand (both rules and constraints), we construct just a small annotated treebank utterances and their semantic molecules.
If the grammar needs to be refined, or enhanced, we only refine, or enhance the representative examples/sublanguage, and not the grammar rules and constraints, which would be a more difficult task.
We have built a framework to test whether our GARS model can learn diverse and complex linguistic phenomena.
We have primarily analyzed a set of definitional-type sentences in the medical domain.
The phenomena covered by our learned grammar includes complex noun phrases (including noun compounds, nominalization), prepositional phrases, relative clauses and reduced relative clauses, finite and non-finite verbal constructions (including, tense, aspect, negation, and subject-verb agreement), copula to be, and raising and control constructions.
We also learned rules for wh-questions (including longdistance dependencies).
In Figure 3 we show the ontology-level representation of a definition-type sentence obtained using our learned grammar.
It includes the treatment of reduced relative clauses, raising construction (tends to persist, where virus is not the argument of tends but the argument of persist), and noun compounds.
The learned grammar together with a semantic interpreter targeted to terminological knowledge has been used in an acquisition-query experiment, where the answers are at the concept level (the querying is a graph 838 Hepatitis B is an acute viral hepatitis caused by a virus that tends to persist in the blood serum.
#hepatitis #acute #viral #cause #blood #virus sub kind_of th of duration ag prop locationth #tend #persist #serum #’HepatitisB’ Figure 3: A definition-type sentence and its ontology-based representation obtained using our learned LWFG matching problem where the “wh-word” matches the answer concept).
A detailed discussion of the linguistic phenomena covered by our learned grammar using the GARS model, as well as the use of this grammar for terminological knowledge acquisition, is given in (Muresan, 2006).
To learn the grammar used in these experiments we annotated 151 representative examples and 448 examples used as a representative sublanguage for generalization.
Annotating these examples requires knowledge about categories and their attributes.
We used 31 categories (nonterminals) and 37 attributes (e.g., category, head, number, person).
In this experiment, we chose the representative examples guided by the type of phenomena we wanted to modeled and which occurred in our corpus.
We also used 13 lexical categories (i.e., parts of speech).
The learned grammar contains 151 rules and 151 constraints.
6 Conclusion
We have presented Lexicalized Well-Founded Grammars, a type of constraint-based grammars for natural language specifically designed to enable learning from representative examples annotated with semantics.
We have presented a new grammar learning model and showed that the search space is a complete grammar lattice that guarantees the uniqueness of the learned grammar.
Starting from these fundamental theoretical results, there are several directions into which to take this research.
A first obvious extension is to have probabilisticLWFGs.
For example, the ontology constraints might not be “hard” constraints, but “soft” ones (because language expressions are more or less likely to be used in a certain context).
Investigating where to add probabilities (ontology, grammar rules, or both) is part of our planned future work.
Another future extension of this work is to investigate how to automatically select the representative examples from an existing treebank.
References Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier.
2004. Wide-coverage semantic representations from a CCG parser.
In Proceedings of COLING-04.
William Cohen.
1995. Pac-learning recursive logic programs: Negative results.
Journal of Artificial Intelligence Research, 2:541–573.
Rusins Freivalds, Efim B.
Kinber, and Rolf Wiehagen.
1993. On the power of inductive inference from good examples.
Theoretical Computer Science, 110(1):131–144.
R. Ge and R.J.
Mooney. 2005.
A statistical semantic parser that integrates syntax and semantics.
In Proceedings of CoNLL-2005.
Russell Greiner.
1999. The complexity of theory revision.
Artificial Intelligence Journal, 107(2):175–217.
Aria Haghighi and Dan Klein.
2006. Prototype-driven grammar induction.
In Proceedings of ACL’06.
J¨org-Uwe Kietz and Saˇso Dˇzeroski.
1994. Inductive logic programming and learnability.
ACM SIGART Bulletin., 5(1):22–32.
Smaranda Muresan.
2006. Learning Constraint-based Grammars from Representative Examples: Theory and Applications.
Ph.D. thesis, Columbia University.
http://www1.cs.columbia.edu/a0 smara/muresan thesis.pdf.
Fernando C.
Pereira and David H.D Warren.
1980. Definite Clause Grammars for language analysis.
Artificial Intelligence, 13:231–278.
Stuart Shieber, Hans Uszkoreit, Fernando Pereira, Jane Robinson, and Mabry Tyson.
1983. The formalism and implementation of PATR-II.
In Barbara J.
Grosz and Mark Stickel, editors, Research on Interactive Acquisition and Use of Knowledge, pages 39–79.
SRI International, Menlo Park, CA, November.
Stuart Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive parsing.
Journal of Logic Programming, 24(1-2):3–36.
Shuly Wintner.
1999. Compositional semantics for linguistic formalisms.
In Proceedings of the ACL’99.
Luke S.
Zettlemoyer and Michael Collins.
2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.
In Proceedings of UAI-05 .

