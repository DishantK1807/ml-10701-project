<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>P S Bayerl</author>
<author>K I Paul</author>
</authors>
<title>Identifying sources of disagreement: Generalizability theory in manual annotation studies</title>
<date>2007</date>
<journal>Computational Linguistics</journal>
<volume>33</volume>
<pages>3--8</pages>
<contexts>
<context>ristics, such as familiarity with the recorded material, amount of annotation training, motivation etc.; and on various external influences such as time pressure, changes in annotation protocol etc. (Bayerl &amp; Paul, 2007). It is clear then that a quality estimation of these annotations, i.e. an analysis of annotator agreement and consistency in segmentation, transcription, and labeling, is recommended (Cucchiarini, 1</context>
</contexts>
<marker>Bayerl, Paul, 2007</marker>
<rawString>Bayerl, P. S., &amp; Paul, K. I. (2007). Identifying sources of disagreement: Generalizability theory in manual annotation studies. Computational Linguistics, 33(1), 3–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D V Cicchetti</author>
</authors>
<title>A new measure of agreement between rank ordered variables</title>
<date>1972</date>
<booktitle>In Proceedings of the 80th Annual Convention of the American Psychological Association</booktitle>
<pages>17--18</pages>
<marker>Cicchetti, 1972</marker>
<rawString>Cicchetti, D. V. (1972). A new measure of agreement between rank ordered variables. In Proceedings of the 80th Annual Convention of the American Psychological Association (pp. 17–18).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Cleuren</author>
<author>J Duchateau</author>
<author>A Sips</author>
<author>P Ghesqui`ere</author>
<author>H Van hamme</author>
</authors>
<title>Developing an automatic assessment tool for children’s oral reading</title>
<date>2006</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<pages>817--820</pages>
<location>Pittsburgh, USA</location>
<marker>Cleuren, Duchateau, Sips, Ghesqui`ere, Van hamme, 2006</marker>
<rawString>Cleuren, L., Duchateau, J., Sips, A., Ghesqui`ere, P., &amp; Van hamme, H. (2006). Developing an automatic assessment tool for children’s oral reading. In Proceedings of Interspeech (pp. 817–820). Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A cofficient of agreement for nominal scales</title>
<date>1960</date>
<journal>Educational and Psychological Measurement</journal>
<volume>20</volume>
<pages>37--46</pages>
<contexts>
<context>one wants to correct for chance agreement, the unweighted kappa statistic (which varies between 0 and 1) is commonly used to evaluate annotator agreement at the label level (for the exact formula see Cohen, 1960). A kappa statistic of 0.6 or higher indicates a substantial agreement; a kappa of 0.8 or higher indicates an almost perfect agreement (Landis &amp; Koch, 1977). For each percentage agreement score and k</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Cohen, J. (1960). A cofficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1), 37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cucchiarini</author>
</authors>
<title>Assessing transcription agreement: methodological aspects</title>
<date>1996</date>
<journal>Clinical Linguistics and Phonetics</journal>
<volume>10</volume>
<pages>131--155</pages>
<contexts>
<context>l &amp; Paul, 2007). It is clear then that a quality estimation of these annotations, i.e. an analysis of annotator agreement and consistency in segmentation, transcription, and labeling, is recommended (Cucchiarini, 1996). To do so, different methods to assess the quality of corpus annotations have been proposed in the literature, such as pair wise transcriber percentage agreement and the kappa statistic (e.g. Dilley</context>
<context>ements between annotators for each word, by the total number of possible pair wise agreements (sum of total number of disagreements and total number of agreements), and then multiplying this by 100% (Cucchiarini, 1996). Cohen’s Kappa. When one wants to correct for chance agreement, the unweighted kappa statistic (which varies between 0 and 1) is commonly used to evaluate annotator agreement at the label level (for</context>
</contexts>
<marker>Cucchiarini, 1996</marker>
<rawString>Cucchiarini, C. (1996). Assessing transcription agreement: methodological aspects. Clinical Linguistics and Phonetics, 10(2), 131–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Dilley</author>
<author>M Breen</author>
<author>M Bolivar</author>
<author>J Kraemer</author>
<author>E Gibson</author>
</authors>
<title>A comparison of inter-transcriber reliability for two systems of prosodic annotation</title>
<date>2006</date>
<booktitle>RaP (Rhythm and Pitch) and ToBI (Tones and Break Indices). In Proceedings of Interspeech</booktitle>
<pages>317--320</pages>
<location>Pittsburgh, USA</location>
<contexts>
<context>, 1996). To do so, different methods to assess the quality of corpus annotations have been proposed in the literature, such as pair wise transcriber percentage agreement and the kappa statistic (e.g. Dilley et al., 2006; Kazemzadeh et al., 2005; Pitt et al., 2005; Yoon et al., 2004). In this paper, we present an analysis of interand intraannotator agreement in the transcription and labeling of recorded children’s </context>
</contexts>
<marker>Dilley, Breen, Bolivar, Kraemer, Gibson, 2006</marker>
<rawString>Dilley, L., Breen, M., Bolivar, M., Kraemer, J., &amp; Gibson, E. (2006). A comparison of inter-transcriber reliability for two systems of prosodic annotation: RaP (Rhythm and Pitch) and ToBI (Tones and Break Indices). In Proceedings of Interspeech (pp. 317–320). Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kazemzadeh</author>
<author>H You</author>
<author>M Iseli</author>
<author>B Jones</author>
<author>X Cui</author>
<author>M Heritage</author>
</authors>
<title>TBALL data collection: The making of a young children’s speech corpus</title>
<date>2005</date>
<booktitle>In ProceedingsofInterspeech</booktitle>
<pages>1581--1584</pages>
<location>Lisbon, Portugal</location>
<contexts>
<context>ch as the LISTEN project (Carnegie Mellon University) (Mostow &amp; Aist, 2001), the Foundations to Literacy Reading Tutor project (Colorado University) (Wise et al., 2005), and the TBALL project (UCLA) (Kazemzadeh et al., 2005). In Flanders, the SPACE project (SPeech Algorithms for Clinical and Educational applications) aims to automate the reading assessment process and to make it more objective. Additionally, the project</context>
<context>fferent methods to assess the quality of corpus annotations have been proposed in the literature, such as pair wise transcriber percentage agreement and the kappa statistic (e.g. Dilley et al., 2006; Kazemzadeh et al., 2005; Pitt et al., 2005; Yoon et al., 2004). In this paper, we present an analysis of interand intraannotator agreement in the transcription and labeling of recorded children’s oral readings. More speci</context>
<context>o account word, sentence and text features. 2Reading Tutor developed at the Department of Electrical Engineering (ESAT), K.U.Leuven, Belgium. they could listen to their own recordings afterwards (cf. Kazemzadeh et al., 2005). 2.1.4. Annotation Procedure The recordings were segmented, transcribed and labeled manually by means of a customized version of ’Praat’ (http://www.Praat.org/). This tool provides the possibility t</context>
</contexts>
<marker>Kazemzadeh, You, Iseli, Jones, Cui, Heritage, 2005</marker>
<rawString>Kazemzadeh, A., You, H., Iseli, M., Jones, B., Cui, X., Heritage, M., et al. (2005). TBALL data collection: The making of a young children’s speech corpus. In ProceedingsofInterspeech (pp. 1581–1584). Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data</title>
<date>1977</date>
<journal>Biometrics</journal>
<volume>33</volume>
<pages>159--174</pages>
<contexts>
<context>ement at the label level (for the exact formula see Cohen, 1960). A kappa statistic of 0.6 or higher indicates a substantial agreement; a kappa of 0.8 or higher indicates an almost perfect agreement (Landis &amp; Koch, 1977). For each percentage agreement score and kappa value,                </context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>Landis, J. R., &amp; Koch, G. G. (1977). The measurement of observer agreement for categorical data. Biometrics, 33, 159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mostow</author>
<author>G Aist</author>
</authors>
<title>Evaluating tutors that listen: An overview of project LISTEN. In</title>
<date>2001</date>
<pages>169--234</pages>
<contexts>
<context>ent research projects have made a great effort in implementing speech technology in the assessment (and intervention) of reading difficulties, such as the LISTEN project (Carnegie Mellon University) (Mostow &amp; Aist, 2001), the Foundations to Literacy Reading Tutor project (Colorado University) (Wise et al., 2005), and the TBALL project (UCLA) (Kazemzadeh et al., 2005). In Flanders, the SPACE project (SPeech Algorithm</context>
</contexts>
<marker>Mostow, Aist, 2001</marker>
<rawString>Mostow, J., &amp; Aist, G. (2001). Evaluating tutors that listen: An overview of project LISTEN. In K. D. Forbus &amp; P. J. Feltovich (Eds.), Smart machines in education: The coming revolution in educational technology (pp. 169–234).</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Mostow</author>
<author>J Beck</author>
<author>S Winter</author>
<author>S Wang</author>
<author>B Tobin</author>
</authors>
<marker>Mostow, Beck, Winter, Wang, Tobin, </marker>
<rawString>Mostow, J., Beck, J., Winter, S., Wang, S., &amp; Tobin, B.</rawString>
</citation>
<citation valid="true">
<title>Predicting oral reading miscues</title>
<date>2002</date>
<booktitle>In Proceedings of the Seventh International Conference on Spoken Language Processing</booktitle>
<pages>1221--1224</pages>
<location>Denver, CO</location>
<marker>2002</marker>
<rawString>(2002). Predicting oral reading miscues. In Proceedings of the Seventh International Conference on Spoken Language Processing (pp. 1221–1224). Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Pitt</author>
<author>K Johnson</author>
<author>E Hume</author>
<author>S Kiesling</author>
<author>W Raymond</author>
</authors>
<title>The Buckeye corpus of conversational speech: Labeling conventions and a test of transcriber reliability</title>
<date>2005</date>
<journal>Speech Communication</journal>
<volume>45</volume>
<pages>89--95</pages>
<contexts>
<context> the quality of corpus annotations have been proposed in the literature, such as pair wise transcriber percentage agreement and the kappa statistic (e.g. Dilley et al., 2006; Kazemzadeh et al., 2005; Pitt et al., 2005; Yoon et al., 2004). In this paper, we present an analysis of interand intraannotator agreement in the transcription and labeling of recorded children’s oral readings. More specifically, we investi</context>
</contexts>
<marker>Pitt, Johnson, Hume, Kiesling, Raymond, 2005</marker>
<rawString>Pitt, M. A., Johnson, K., Hume, E., Kiesling, S., &amp; Raymond, W. (2005). The Buckeye corpus of conversational speech: Labeling conventions and a test of transcriber reliability. Speech Communication, 45, 89–95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wise</author>
<author>R Cole</author>
<author>S van Vuuren</author>
<author>S Schwartz</author>
<author>L Snyder</author>
<author>N Ngampatipatpong</author>
</authors>
<title>Learning to read with a virtual tutor: Foundations to literacy. In</title>
<date>2005</date>
<marker>Wise, Cole, van Vuuren, Schwartz, Snyder, Ngampatipatpong, 2005</marker>
<rawString>Wise, B., Cole, R., van Vuuren, S., Schwartz, S., Snyder, L., Ngampatipatpong, N., et al. (2005). Learning to read with a virtual tutor: Foundations to literacy. In C. Kinzer &amp; L. Verhoeven (Eds.), Interactive literacy education: Facilitating literacy environments through technology.</rawString>
</citation>
</citationList>
</algorithm>

