1:188	A Phrase-Based HMM Approach to Document/Abstract Alignment Hal Daume III and Daniel Marcu Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 fhdaume,marcug@isi.edu Abstract We describe a model for creating word-to-word and phrase-to-phrase alignments between documents and their human written abstracts.
2:188	Such alignments are critical for the development of statistical summarization systems that can be trained on large corpora of document/abstract pairs.
3:188	Our model, which is based on a novel Phrase-Based HMM, outperforms both the Cut & Paste alignment model (Jing, 2002) and models developed in the context of machine translation (Brown et al. , 1993).
4:188	1 Introduction There are a wealth of document/abstract pairs that statistical summarization systems could leverage to learn how to create novel abstracts.
5:188	Detailed studies of such pairs (Jing, 2002) show that human abstractors perform a range of very sophisticated operations when summarizing texts, which include reordering, fusion, and paraphrasing.
6:188	Unfortunately, existing document/abstract alignment models are not powerful enough to capture these operations.
7:188	To get around directly tackling this problem, researchers in text summarization have employed one of several techniques.
8:188	Some researchers (Banko et al. , 2000) have developed simple statistical models for aligning documents and headlines.
9:188	These models, which implement IBM Model 1 (Brown et al. , 1993), treat documents and headlines as simple bags of words and learn probabilistic word-based mappings between the words in the documents and the words in the headlines.
10:188	As our results show, these models are too weak for capturing the operations that are employed by humans in summarizing texts beyond the headline level.
11:188	Other researchers have developed models that make unreasonable assumptions about the data, which lead to the utilization of a very small percent of available data.
12:188	For instance, the document and sentence compression models of Daume III, Knight, and Marcu (Knight and Marcu, 2002; Daume III and Marcu, 2002a) assume that sentences/documents can be summarized only through deletion of contiguous text segments.
13:188	Knight and Marcu found that from a corpus of 39;060 abstract sentences, only 1067 sentence extracts existed: a recall of only 2:7%.
14:188	An alternate techinque employed in a large variety of systems is to treat the summarization problem as a sentence extraction problem.
15:188	Such systems can be trained either on human constructed extracts or extracts generated automatically from document/abstract pairs (see (Marcu, 1999; Jing and McKeown, 1999) for two such approaches).
16:188	None of these techniques is adequate.
17:188	Even for a relatively simple sentence from an abstract, we can see that none of the assumptions listed above holds.
18:188	In Figure 1, we observe several phenomena: Alignments can occur at the granularity of words and at the granularity of phrases.
19:188	The ordering of phrases in an abstract can be different from the ordering in the document.
20:188	Some abstract words do not have direct correspondents in the document, and some document words are never used.
21:188	It is thus desirable to be able to automatically construct alignments between documents and their abstracts, so that the correspondences between the pairs are obvious.
22:188	One might be initially tempted to use readily-available machine translation systems like GIZA++ (Och and Ney, 2003) to perform such Connecting Point has become the single largest Mac retailer after tripling it s Macintosh sales since January 1989.
23:188	Connecting Point Systems tripled it s sales of Apple Macintosh systems since last January . It is now the single largest seller of Macintosh . Figure 1: Example abstract/text alignment.
24:188	alignments.
25:188	However, as we will show, the alignments produced by such a system are inadequate for this task.
26:188	The solution that we propose to this problem is an alignment model based on a novel mathematical structure we call the Phrase-Based HMM.
27:188	2 Designing a Model As observed in Figure 1, our model needs to be able to account for phrase-to-phrase alignments.
28:188	It also needs to be able to align abstract phrases with arbitrary parts of the document, and not require a monotonic, left-to-right alignment.1 2.1 The Generative Story The model we propose calculates the probability of an alignment/abstract pair in a generative fashion, generating the summary S = hs1 ::: smi from the document D = hd1 ::: dni.
29:188	In a document/abstract corpus that we have aligned by hand (see Section 3), we have observed that 16% of abstract words are left unaligned.
30:188	Our model assumes that these null-generated words and phrases are produced by a unique document word ?, called the null word. The parameters of our model are stored in two tables: a rewrite/paraphrase table and a jump table.
31:188	The rewrite table stores probabilities of producing summary words/phrases from document words/phrases and from the null word (namely, probabilities of the form rewrite s d and rewrite ( s ?)); the jump table stores the probabilities of moving within a document from one position to another, and from and to ?.
32:188	The generation of a summary from a document is assumed to proceed as follows: 1In the remainder of the paper, we will use the words summary and abstract interchangeably.
33:188	This is because we wish to use the letter s to refer to summaries.
34:188	We could use the letter a as an abbreviation for abstract; however, in the definition of the Phrase-Based HMM, we reuse common notation which ascribes a different interpretation to a. 1.
35:188	Choose a starting index i and jump to position di in the document with probability jump (i).
36:188	(If the first summary phrase is nullgenerated, jump to the null-word with probability jump (?).)
37:188	2.
38:188	Choose a document phrase of length k 0 and a summary phrase of length l 1.
39:188	Generate summary words sl1 from document words di+ki with probability rewrite sl1 di+ki .2 3.
40:188	Choose a new document index i0 and jump to position di0 with probability jump (i0 (i + k)) (or, if the new document position is the empty state, then jump (?)).
41:188	4.
42:188	Choose k0 and l0 as in step 2, and generate the summary words s1+l+l01+l from the document words di0+k0i0 with probability rewrite s1+l+l01+l di0+k0i0 . 5.
43:188	Repeat from step 3 until the entire summary has been generated.
44:188	6.
45:188	Jump to position dn+1 in the document with probability jump (n + 1 (i0 + k0)).
46:188	Note that such a formulation allows the same document word/phrase to generate many summary words: unlike machine translation, where such behavior is typically avoided, in summarization, we observe that such phenomena do occur.
47:188	However, if one were to build a decoder based on this model, one would need to account for this issue to avoid degenerate summaries from being produced.
48:188	The formal mathematical model behind the alignments is as follows: An alignment @ defines both a segmentation of the summary S and a mapping from the segments of S to the segments of the document D. We write si to refer to the ith segment of S, and M to refer to the total number of segments 2We write xb a for the subsequence hxa : : : xbi.
49:188	in S. We write d@(i) to refer to the words in the document which correspond to segment si.
50:188	Then, the probability of a summary/alignment pair given a document (Pr(S;@ D)), becomes: M+1Y i=1 jump (@(i) @(i 1)) rewrite s i d@(i) Here, we implicitly define sm+1 to be the end-ofdocument token h!i and d@(m+1) to generate this with probability 1.
51:188	We also define the initial position in the document, @(0) to be 0, and assume a uniform prior on segmentations.
52:188	2.2 The Mathematical Model Having decided to use this model, we must now find a way to efficiently train it.
53:188	The model is very much like a Hidden Markov Model in which the summary is the observed sequence.
54:188	However, using a standard HMM would not allow us to account for phrases in the summary.
55:188	We therefore extend a standard HMM to allow multiple observations to be emitted on one transition.
56:188	We call this model a Phrase-Based HMM (PBHMM).
57:188	For this model, we have developed equivalents of the forward and backward algorithms, Viterbi search and forward-backward parameter reestimation.
58:188	Our notation is shown in Table 1.
59:188	Here, S is the state space, and the observation sequences come from the alphabet K. j is the probability of beginning in state j. The transition probability ai;j is the probability of transitioning from state i to state j. bi;j; k is the probability of emitting (the non-empty) observation sequence k while transitioning from state i to state j. Finally, xt denotes the state after emitting t symbols.
60:188	The full derivation of the model is too lengthy to include; the interested reader is directed to (Daume III and Marcu, 2002b) for the derivations and proofs of the formulae.
61:188	To assist the reader in understanding the mathematics, we follow the same notation as (Manning and Schutze, 2000).
62:188	The formulae for the calculations are summarized in Table 2.
63:188	2.2.1 Forward algorithm The forward algorithm calculates the probability of an observation sequence.
64:188	We define j(t) as the probability of being in state j after emitting the first t 1 symbols (in whatever grouping we want).
65:188	2.2.2 Backward algorithm Just as we can compute the probability of an observation sequence by moving forward, so can we calculate it by going backward.
66:188	We define i(t) as the probability of emitting the sequence oTt given that we are starting out in state i. 2.2.3 Best path We define a path as a sequence P = hp1 ::: pLi such that pi is a tuple ht;xi where t corresponds to the last of the (possibly multiple) observations made, and x refers to the state we were coming from when we output this observation (phrase).
67:188	Thus, we want to find: argmax P Pr P oT1 ; = argmax P Pr P;oT1 To do this, as in a traditional HMM, we estimate the table.
68:188	When we calculate j(t), we essentially need to choose an appropriate i and t0, which we store in another table, so we can calculate the actual path at the end.
69:188	2.2.4 Parameter re-estimation We want to find the model which best explains observations.
70:188	There is no known analytic solution for standard HMMs, so we are fairly safe in assuming that we will not find an analytic solution for this more complex problem.
71:188	Thus, we also revert to an iterative hill-climbing solution analogous to BaumWelch re-estimation (i.e. , the Forward Backward algorithm).
72:188	The equations for the re-estimated values ^a and ^b are shown in Table 2.
73:188	2.2.5 Dirichlet Priors Using simple maximum likelihood estimation is inadequate for this model: the maximum likelihood solution is simply to make phrases as long as possible; unfortunately, doing so will first cut down on the number of probabilities that need to be multiplied and second make nearly all observed summary phrase/document phrase alignments unique, thus resulting in rewrite probabilities of 1 after normalization.
74:188	In order to account for this, instead of finding the maximum likelihood solution, we instead seek the maximum a posteriori solution.
75:188	The distributions we deal with in HMMs, and, in particular, PBHMMs, are all multinomial.
76:188	The Dirichlet distribution is in the conjugate family to the multinomial distribution3.
77:188	This makes Dirichlet priors very appealing to work with, so long as 3This effectively means that the product of a Dirichlet and multinomial yields a multinomial.
78:188	S set of states K output alphabet = f j : j 2 Sg initial state probabilities A = fai;j : i;j 2 Sg transition probabilities B = fbi;j; k : i;j 2 S; k 2 K+g emission probabilities Table 1: Notation used for the PBHMM j(t) = Pr ot 11 ;xt 1 = j = t 1X t0=0 X i2S i(t0 + 1) ai;j bi;j;ot t0+1 i(t) = Pr oTt ;xt 1 = i = TX t0=t X j2S ai;j bi;j;ot0 t j(t0 + 1) j(t) = max l;pl 11 Pr pl 11 ;ot 11 ;pl:t = t 1;pl:x = j = i(t0)ai;jbi;j;ot 1 t0 i;j(t0;t) = E # of transitions i; j emitting ott0 = i(t0)ai;jbi;j;ot t0 j(t + 1) Pr oT1 ^ai;j = E [# of transitions i ;j]E [# of transitions i;?]
79:188	= PT t0=1 PT t=t0 i;j(t 0;t) PT t0=1 PT t=t0 P j02S i;j0(t0;t) ^bi;j; k = E # of transitions i;j with k observed E [# of transitions i ;j] = PT+1 j kj t=1 ( k;o t+j kj 1 t ) i;j(t;t + j kj 1)P T t0=1 PT t=t0 i;j(t0;t) Table 2: Summary of equations for a PBHMM we can adequately express our prior beliefs in their form.
80:188	(See (Gauvain and Lee, 1994) for the application to standard HMMs.)
81:188	Applying a Dirichlet prior effectively allows us to add fake counts during parameter re-estimation, according to the prior.
82:188	The prior we choose has a form such that fake counts are added as follows: word-to-word rewrites get an additional count of 2; identity rewrites get an additional count of 4; stemidentity rewrites get an additional count of 3.
83:188	2.3 Constructing the PBHMM Given our generative story, we construct a PBHMM to calculate these probabilities efficiently.
84:188	The structure of the PBHMM for a given document is conceptually simple.
85:188	We provide values for each of the following: the set of possible states S; the output alphabet K; the initial state probabilities ; the transition probabilities A; and the emission probabilities B. 2.3.1 State Space The state set is large, but structured.
86:188	There is a unique initial state p, a unique final state q, and a state for each possible document phrase.
87:188	That is, for all 1 i i0 n, there is a state that corresponds to the document phrase beginning at position i and ending at position i0, di0i, which we will refer to as ri;i0.
88:188	There is also a null state for each document position ra0 ;i, so that when jumping out of a null state, we can remember what our previous position in the document was.
89:188	Thus, S = fp;qg [fri;i0 : 1 i i0 ng [ fra0 ;i : 1 i ng.
90:188	Figure 2 shows the schematic drawing of the PBHMM constructed for the document a b.
91:188	K, the output alphabet, consists of each word found in S, plus the token !.
92:188	2.3.2 Initial State Probabilities For initial state probabilities: since p is our initial state, we say that p = 1 and that r = 0 for all r 6= p. 2.3.3 Transition Probabilities The transition probabilities A are governed by the jump table.
93:188	Each possible jump type and its associated probability is shown in Table 3.
94:188	By these calculations, regardless of document phrase lengths, transitioning forward between two consecutive segments will result in jump (1).
95:188	When transitioning jump(2) jump(1) jump(1) jump(0) jump(2) jump(1) jump(2) jump(0) jump(1) ba ab b qp jump(1)jump( ) a Figure 2: Schematic drawing of the PBHMM (with some transition probabilities) for the document a b source target probability p ri;i0 jump (i) ri;i0 rj;j0 jump (j i0) ri;j0 q jump (m + 1 i0) p ra0 ;i jump ()?
96:188	jump (i) ra0 ;i rj;j0 jump (j i) ra0 ;i ra0 ;j jump ()?
97:188	jump (j i) ra0 ;i q jump (m + 1 i) ri;i0 ra0 ;j jump ()?
98:188	jump (j i0) Table 3: Jump probability decomposition from p to ri;i0, the value ap;ri;i0 = jump (i).
99:188	Thus, if we begin at the first word in the document, we incur a transition probability of jump (1).
100:188	There are no transitions into p. 2.3.4 Rewrite Probabilities Just as the transition probabilities are governed by the jump table, the emission probabilities B are governed by the rewrite table.
101:188	In general, we write bx;y; k to mean the probability of generating k while transitioning from state x to state y. However, in our case we do not need the x parameter, so we will refer to these as bj; k, the probability of generating k when jumping into state j. When j = ri;i0, this is rewrite k di0i . When j = ra0 ;i, this is rewrite k ? .
102:188	Finally, any state transitioning into q generates the phrase h!i with probability 1 and any other phrase with probability 0.
103:188	Consider again the document a b (the PBHMM for which is shown in Figure 2) in the case when the corresponding summary is c d.
104:188	Suppose the correct alignment is that c d is aligned to a and b is left unaligned.
105:188	Then, the path taken through the PBHMM is p ! a ! q. During the transition p ! a, c d is emitted.
106:188	During the transition a ! q, ! is emitted.
107:188	Thus, the probability for the alignment is: jump (1) rewrite (cd a) jump (2).
108:188	The rewrite probabilities themselves are governed by a mixture model with unknown mixing parameters.
109:188	There are three mixture component, each of which is represented by a multinomial.
110:188	The first is the standard word-for-word and phrase-for-phrase table seen commonly in machine translation, where rewrite s d is simply a normalized count of how many times we have seen s aligned to d. The second is a stem-based table, in which suffixes (using Porters stemmer) of the words in s and d are thrown out before a comparison is made.
111:188	The third is a simple identity function, which has a constant zero value when s and d are different (up to stem) and a constant non-zero value when they have the same stem.
112:188	The mixing parameters are estimated simultaneously during EM.
113:188	2.3.5 Parameter Initialization Instead of initializing the jump and rewrite tables randomly or uniformly, as it typically done with HMMs, we initialize the tables according to the distribution specified by the prior.
114:188	This is not atypical practice in problems in which a MAP solution is sought.
115:188	3 Evaluation and Results In this section, we describe an intrinsic evaluation of the PBHMM document/abstract alignment model.
116:188	All experiments in this paper are done on the ZiffDavis corpus (statistics are in Table 4).
117:188	In order to judge the quality of the alignments produced by a system, we first need to create a set of gold standard alignments.
118:188	Two human annotators manually constructed such alignments between documents and their abstracts.
119:188	Software for assisting this process was developed and is made freely available.
120:188	An annotation guide, which explains in detail the document/abstract alignment process was also prepared and is freely available.4 4Both the software and documentation are available on the first authors web page.
121:188	The alignments are also available; contact the authors for a copy.
122:188	Abstracts Extracts Documents 2033 Sentences 13k 41k Words 261k 1m Types 14k 26k 29k Sentences/Doc 6:28 21:51 Words/Doc 128:52 510:99 Words/Sent 20:47 23:77 Table 4: Ziff-Davis extract corpus statistics 3.1 Human Annotation From the Ziff-Davis corpus, we randomly selected 45 document/abstract pairs and had both annotators align them.
123:188	The first five were annotated separately and then discussed; the last 40 were done independently.
124:188	Annotators were asked to perform phrase-tophrase alignments between abstracts and documents and to classify each alignment as either possible P or sure S, where P S. In order to calculate scores for phrase alignments, we convert all phrase alignments to word alignments.
125:188	That is, if we have an alignment between phrases A and B, then this induces word alignments between a and b for all words a 2 A and b 2 B. Given an alignment A, we could calculate precision and recall as (see (Och and Ney, 2003)): Precision = jA\PjjAj Recall = jA\SjjSj One problem with these definitions is that phrasebased models are fond of making phrases.
126:188	That is, when given an abstract containing the man and a document also containing the man, a human may prefer to align the to the and man to man. However, a phrase-based model will almost always prefer to align the entire phrase the man to the man. This is because it results in fewer probabilities being multiplied together.
127:188	To compensate for this, we define soft precision (SoftP in the tables) by counting alignments where a b is aligned to a b the same as ones in which a is aligned to a and b is aligned to b. Note, however, that this is not the same as a aligned to a b and b aligned to b.
128:188	This latter alignment will, of course, incur a precision error.
129:188	The soft precision metric induces a new, soft F-Score, labeled SoftF.
130:188	Often, even humans find it difficult to align function words and punctuation.
131:188	A list of 58 function words and punctuation marks which appeared in the corpus (henceforth called the ignore-list) was assembled.
132:188	Agreement and precision/recall have been calculated both on all words and on all words that do not appear in the ignore-list.
133:188	Annotator agreement was strong for Sure alignments and fairly weak for Possible alignments (considering only the 40 independently annotated pairs).
134:188	When considering only Sure alignments, the kappa statistic (over 7:2 million items, 2 annotators and 2 categories) for agreement was 0:63.
135:188	When words from the ignore-list were thrown out, this rose to 0:68.
136:188	Carletta (1995) suggests that kappa values over 0:80 reflect very strong agreement and that kappa values between 0:60 and 0:80 reflect good agreement.
137:188	3.2 Machine Translation Experiments In order to establish a baseline alignment model, we used the IBM Model 4 (Brown et al. , 1993) and the HMM model (Stephan Vogel and Tillmann, 1996) as implemented in the GIZA++ package (Och and Ney, 2003).
138:188	We modified this slightly to allow longer inputs and higher fertilities.
139:188	Such translation models require that input be in sentence-aligned form.
140:188	In the summarization task, however, one abstract sentence often corresponds to multiple document sentences.
141:188	In order to overcome this problem, each sentence in an abstract was paired with three sentences from the corresponding document, selected using the techniques described by Marcu (1999).
142:188	In an informal evaluation, 20 such pairs were randomly extracted and evaluated by a human.
143:188	Each pair was ranked as 0 (document sentences contain little-to-none of the information in the abstract sentence), 1 (document sentences contain some of the information in the abstract sentence) or 2 (document sentences contain all of the information).
144:188	Of the twenty random examples, none were labeled as 0; five were labeled as 1; and 15 were labeled as 2, giving a mean rating of 1:75.
145:188	We ran experiments using the document sentences as both the source and the target language in GIZA++.
146:188	When document sentences were used as the target language, each abstract word needed to produce many document words, leading to very high fertilities.
147:188	However, since each target word is generated independently, this led to very flat rewrite tables and, hence, to poor results.
148:188	Performance increased dramatically by using the document as the source language and the abstract as the target language.
149:188	In all MT cases, the corpus was appended with one-word sentence pairs for each word where that word is translated as itself.
150:188	In the two basic models, HMM and Model 4, the abstract sentence is the source language and the document sentences are the target language.
151:188	To alleviate the fertility problem, we also ran experiments with the translation going in the opposite direction.
152:188	These are called HMMflipped and Model 4-flipped, respectively.
153:188	These tend to out-perform the original translation direction.
154:188	In all of these setups, 5 iterations of Model 1 were run, followed by 5 iterations of the HMM model.
155:188	In the Model 4 cases, 5 iterations of Model 4 were run, following the HMM.
156:188	3.3 Cut and Paste Experiments We also tested alignments using the Cut and Paste summary decomposition method (Jing, 2002), based on a non-trainable HMM.
157:188	Briefly, the Cut and Paste HMM searches for long contiguous blocks of words in the document and abstract that are identical (up to stem).
158:188	The longest such sequences are aligned.
159:188	By fixing a length cutoff of n and ignoring sequences of length less than n, one can arbitrarily increase the precision of this method.
160:188	We found that n = 2 yields the best balance between precision and recall (and the highest F-measure).
161:188	The results of these experiments are shown under the header Cut & Paste. It clearly outperforms all of the MT-based models.
162:188	3.4 PBHMM Experiments While the PBHMM is based on a dynamic programming algorithm, the effective search space in this model is enormous, even for moderately sized document/abstract pairs.
163:188	We selected the 2000 shortest document/abstract pairs from the Ziff-Davis corpus for training; however, only 12 of the hand-annotated documents were included in this set, so we additionally added the other 33 hand-annotate documents to this set, yielding 2033 document/abstract pairs.
164:188	We then performed sentence extraction on this corpus exactly as in the MT case, using the technique of (Marcu, 1999).
165:188	The relevant data for this corpus is in Table 4.
166:188	We also restrict the state-space with a beam, sized at 50% of the unrestricted state-space.
167:188	The PBHMM system was then trained on this abstract/extract corpus.
168:188	The precision/recall results are shown in Table 5.
169:188	Under the methodology for combining the two human annotations by taking the union, either of the human scores would achieve a System SoftP Recall SoftF Human1 0.727 0.746 0.736 Human2 0.680 0.695 0.687 HMM 0.120 0.260 0.164 Model 4 0.117 0.260 0.161 HMM-flipped 0.295 0.250 0.271 Model 4-flipped 0.280 0.247 0.262 Cut & Paste 0.349 0.379 0.363 PBHMM 0.456 0.686 0.548 PBHMM O 0.523 0.686 0.594 Table 5: Results on the Ziff-Davis corpus precision and recall of 1:0.
170:188	To give a sense of how well humans actually perform on this task (in addition to the kappa scores reported earlier), we compare each human against the other.
171:188	One common precision mistake made by the PBHMM system is to accidentally align words on the summary side to words on the document side, when the summary word should be null-aligned.
172:188	The PBHMMO system is an oracle system in which system-produced alignments are removed for summary words that should be null-aligned (according to the hand-annotated data).
173:188	Doing this results in a rather significant gain in SoftP score.
174:188	As we can see from Table 5, none of the machine translation models is well suited to this task, achieving, at best, an F-score of 0:298.
175:188	The Cut & Paste method performs significantly better, which is to be expected, since it is designed specifically for summarization.
176:188	As one would expect, this method achieves higher precision than recall, though not by very much.
177:188	Our method significantly outperforms both the IBM models and the Cut & Paste method, achieving a precision of 0:456 and a recall nearing 0:7, yielding an overall F-score of 0:548.
178:188	4 Conclusions and Future Work Despite the success of our model, its performance still falls short of human performance (we achieve an F-score of 0:548 while humans achieve 0:736).
179:188	Moreover, this number for human performance is a lower-bound, since it is calculated with only one reference, rather than two.
180:188	We have begun to perform a rigorous error analysis of the model to attempt to identify its deficiencies: currently, these appear to primarily be due to the model having a zeal for aligning identical words.
181:188	This happens for one of two reasons: either a summary word should be null-aligned (but it is not), or a summary word should be aligned to a different, non-identical document word.
182:188	We can see the PBHMMO model as giving us an upper bound on performance if we were to fix this first problem.
183:188	The second problem has to do either with synonyms that do not appear frequently enough for the system to learn reliable rewrite probabilities, or with coreference issues, in which the system chooses to align, for instance, Microsoft to Microsoft, rather than Microsoft to the company, as might be correct in context.
184:188	Clearly more work needs to be done to fix these problems; we are investigating solving the first problem by automatically building a list of synonyms from larger corpora and using this in the mixture model, and the second problem by investigating the possibility of including some (perhaps weak) coreference knowledge into the model.
185:188	Finally, we are looking to incorporate the results of this model into a real system.
186:188	This can be done either by using the word-for-word alignments to automatically build sentence-to-sentence alignments for training a sentence extraction system (in which case the precision/recall numbers over full sentences are likely to be much higher), or by building a system that exploits the word-for-word alignments explicitly.
187:188	5 Acknowledgments This work was partially supported by DARPA-ITO grant N66001-00-1-9814, NSF grant IIS-0097846, and a USC Dean Fellowship to Hal Daume III.
188:188	Thanks to Franz Josef Och and Dave Blei for discussions related to the project.

