Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 23–24,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics
Twiter in Mass Emergency:  
What NLP Techniques Can Contribute 
 
Wiliam J. Corvey
1, Sarah Vieweg
2, Travis Rood
1
 & Martha Palmer
1
 
1
Department of Linguistics, 
2
ATLAS Institute 
University of Colorado 
Boulder, CO 80309 
William.Corvey, Sarah.Vieweg, Travis.Rood, Martha.Palmer@colorado.edu 
Abstract 
We detail methods for entity span identifica-
tion and entity clas anotation of Twiter 
comunications that take place during times 
of mas emergency. We present our motiva-
tion, method and preliminary results. 
1 Introduction

During times of mass emergency, many turn to 
Twiter to gather and disperse relevant, timely in-
formation (Starbird et al. 2010; Vieweg et al. 
2010). However, the sheer amount of information 
now communicated via Twiter during these time
and safety-critical situations can make it difficult 
for individuals to locate personally meaningful and 
actionable information. In this paper, we discus 
natural language processing (NLP) techniques de-
signed for Twiter data that wil lead to the location 
and extraction of specific information during times 
of mass emergency. 
2 Twiter
Use in Mass Emergency 
Twiter communications are comprised of 140-
character messages called “tweets.”  During times 
of mass emergency, Twiter users send detailed 
information that may help those affected to better 
make critical decisions. 
 
Our goal is to develop techniques to automatically 
identify crucial pieces of information in these 
tweets. This process wil lead to the automatic ex-
traction of information that helps people under-
stand the situation “on the ground” during mass 
emergencies. Relevant information would include 
such things as warnings, road closures, and 
evacuations among other timely information. 
 
3 The
Anotation Proces 
A foundational level of linguistic annotation for 
many natural language processing tasks is Named 
Entity (or nominal entity) tagging (Bikel 199). 
Typical labeled entities that were included in the 
Automatic Content Extraction (ACE) guidelines 
(LDC 204) are: Person, Location, Organization, 
and Facility, the four maximal entity classes. Our 
preliminary annotation task consists of identifying 
the syntactic span and entity class for these four 
types of entities in a pilot set of Twiter data (20 
tweets from a data set generated during the 209 
Oklahoma grassfires). In future annotation, the 
ontology wil be expanded to include event and 
relation annotations, as well as additional sub-
classes of the entities now examined. Anotations 
are done using Knowtator (Ogren 206), a tol 
built within the Protégé framework 
(htp:/protege.stanford.edu/). The ontology devel-
opment is data-driven; as such it is likely that cer-
tain ACE annotations wil never emerge and other 
annotations (such as disaster-relevant materials) 
wil be necessary additions. 
 
Three annotators undertok pilot annotation as part 
of the construction of preliminary annotation 
guidelines; the top pairwise ITA score is reported 
below. Twiter data makes reference to numerous 
entity spans that are of specific interest to this an-
notation task, such as road intersections and multi-
word named entities. The example below, from the 
pilot annotation set, shows a relatively simple span 
delineation. 
 
[
PERSON
 Velma area residents]: [
PERSON
 
Officials] say to take [
FACILTY
 Old Hwy 
7] to [
FACILTY
 Speedy G] to safely 
evacuate. [
LOCATION 
Stephens Co Fair-
grounds] in [
LOCATION 
Duncan] for shel-
ter  
23
Because of the varying length of entities, annota-
tors cannot be given simple rules for deciding the 
spans for annotations. This difficulty is reflected in 
markedly lower rates for span identification inter-
annotator agreement (IA) rates than for simple 
class assignment. 
4Preliminary Results  
IAA calculations were performed using the Know-
tator IAA functionality. When annotations are re-
quired to be both the same span and class, the pilot 
annotation yielded an F-score of 56.27 (An addi-
tional 4% have exact span matches but different 
classes). However, when annotations are required 
to have the same class assignment but only over-
lapping spans, this F-score rises to 72.85. While 
Facility and Location are the most commonly con-
fused classes, span-matching remains a difficult 
issue for all entity classes. 
5 Discusion

While these ITA rates are significantly lower than 
published results from previous ACE annotation 
efforts (LDC 204), we believe that the crisis 
communications domain, particularly with regard 
to Twiter analysis, provides challenges not en-
countered in newswire, broadcast transcripts, or 
newspaper data. First, determining the maximal 
span of interest for a given class assignment is 
non-trivial. The constraint of 140 characters neces-
sarily results in very limited syntactic and semantic 
contexts, making spans and entity class assign-
ments much harder to determine.  
 
A large source of disagreement was on the treat-
ment of coordinated or listed noun phrases. In cer-
tain contexts, each entity (cities below) requires its 
own span (e.g. “Firestorms in Oklahoma. [Midwest 
City], [Lake Draper]. Some houses lost”), whereas 
in other contexts we find multiple entities per span 
(e.g. “Midwest City to evacuate between SE 15th 
and Rena and Anderson and Hiwassee also [Tur-
tlewood, Wingsong, and Oakwood aditions]”). 
Equally, class asignment cannot be a mechanistic 
process or accomplished by reference to lists, as it 
is important to distinguish between cases where 
terms have been elided due to limited space and 
cases where no elision has taken place. For in-
stance, the entity “Atorney General” (as oposed 
to “Atorney General’s Office”) might be anno-
tated ‘Person’ or ‘Organization’ depending on con-
text, or simply ambiguous, i.e. lacking sufficient 
context. It is primarily these unclear cases of class 
assignment that wil require careful discussion in 
the annotation guidelines and in future mappings to 
an ontology. 
 
In summary, this pilot study represents a new ap-
plication of ACE annotation practices to a uniquely 
challenging domain. We outline isues that place 
special demands on annotators and future direc-
tions for ongoing research. We are confident that 
as we refine our guidelines and provide more cues 
and examples for the annotators that the determina-
tion of spans and entity classes wil improve. 
Acknowledgments 
This work is suported by the US National Science 
Foundation IIS-0546315 and IIS-0910586 but does 
not represent the views of the NSF. This work was 
conducted using the Protégé resource, suported 
by grant LM00785 from the US NLM. 
References 
Daniel M. Bikel, Richard Schwartz and Ralph M. 
Weischedel. 199. An Algorithm that Learns What’s 
in a Name. In: the Machine Learning Journal Special 
Isue on Natural Language Learning. 
George Dodington, A. Mitchel, M. Przybocki, L. 
Ramshaw, S. Strasel, and R. Weischedel. (204). 
The Automatic Content Extraction (ACE) Program – 
Tasks, Data, and Evaluation. In: Procedings of Con-
ference on Language Resources and Evaluation 
(LREC 204). 
Kate Starbird, Leysia Palen, Amanda L. Hughes and 
Sarah Vieweg. 2010. Chater on The Red: What 
Hazards Threat Reveals About the Social Life of Mi-
crobloged Information. In: Proc. CSCW 2010. 
ACM Pres. 
LDC, 204, Automatic Content Extraction 
[ww.ldc.upen.edu/Projects/ACE/] 
Philip Ogren. 206. Knowtator: A Protégé plug-in for 
anotated corpus construction. In : Procedings of the 
206 Conference of the North American Chapter of 
the Asociation for Computational Linguistics on 
Human Language Technology 206. ACM Pres. 
Sarah Vieweg, Amanda L. Hughes, Kate Starbird and 
Leysia Palen. 2010. Microbloging During Two 
Natural Hazards Events: What Twiter May Contrib-
ute to Situational Awarenes. In: Proc. CHI 2010. 
ACM Pres. 
24

