Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp.
421–429, Prague, June 2007.
c©2007 Association for Computational Linguistics Bayesian Document Generative Model with Explicit Multiple Topics Issei Sato Graduate School of Information Science and Technology, The University of Tokyo sato@r.dl.itc.u-tokyo.ac.jp Hiroshi Nakagawa Information Technology Center, The University of Tokyo nakagawa@dl.itc.u-tokyo.ac.jp Abstract In this paper, we proposed a novel probabilistic generative model to deal with explicit multiple-topic documents: Parametric Dirichlet Mixture Model(PDMM).
PDMM is an expansion of an existing probabilistic generative model: Parametric Mixture Model(PMM) by hierarchical Bayes model.
PMM models multiple-topic documents by mixing model parameters of each single topic with an equal mixture ratio.
PDMM models multiple-topic documents by mixing model parameters of each single topic with mixture ratio following Dirichlet distribution.
We evaluate PDMM and PMM by comparing F-measures using MEDLINE corpus.
The evaluation showed that PDMM is more effective than PMM.
1 Introduction
Documents, such as those seen on Wikipedia and Folksonomy, have tended to be assigned with explicit multiple topics.
In this situation, it is importanttoanalyzealinguisticrelationshipbetweendocuments and the assigned multiple topics. We attempt to model this relationship with a probabilistic generative model.
A probabilistic generative model for documents with multiple topics is a probability model of the process of generating documents with multiple topics.
By focusing on modeling the generation process of documents and the assigned multiple topics, we can extract specific properties of documents and the assigned multiple topics.
The model can also be applied to a wide range of applications such as automatic categorization for multiple topics, keyword extraction and measuring document similarity, for example.
A probabilistic generative model for documents withmultipletopicsiscategorizedintothefollowing two models.
One model assumes a topic as a latent topic.
Wecallthismodelthelatent-topicmodel. The other model assumes a topic as an explicit topic.
We call this model the explicit-topic model.
In a latent-topic model, a latent topic indicates not a concrete topic but an underlying implicit topic of documents.
Obviously this model uses an unsupervised learning algorithm.
Representative examples of this kind of model are Latent Dirichlet Allocation(LDA)(D.M.Blei et al., 2001; D.M.Blei et al., 2003) and Hierarchical Dirichlet Process(HDP)(Y.W.Teh et al., 2003).
In an explicit-topic model, an explicit topic indicates a concrete topic such as economy or sports, for example.
A learning algorithm for this model is a supervised learning algorithm.
That is, an explicit topic model learns model parameter using a training data set of tuples such as (documents, topics).
Representative examples of this model are Parametric Mixture Models(PMM1 and PMM2)(Ueda, N.
and Saito, K., 2002a; Ueda, N.
and Saito, K., 2002b).
In the remainder of this paper, PMM indicates PMM1 because PMM1 is more effective than PMM2.
Inthispaper,wefocusontheexplicittopicmodel.
In particular, we propose a novel model that is based on PMM but fundamentally improved.
The remaining part of this paper is organized as follows.
Sections 2 explains terminology used in the 421 following sections.
Section 3 explains PMM that is most directly related to our work.
Section 4 points out the problem of PMM and introduces our new model.
Section 5 evaluates our new model.
Section 6 summarizes our work.
2 Terminology
This section explains terminology used in this paper.
K is the number of explicit topics.
V is the number of words in the vocabulary.
V = {1,2,···,V}is a set of vocabulary index.
Y ={1,2,···,K}is a set of topic index.
N is the number of words in a document.
w = (w1,w2,···,wN) is a sequence of N words where wn denotes the nth word in the sequence.
w is a document itself and is called words vector.
x = (x1,x2,···,xV ) is a word-frequency vector, that is, BOW(Bag Of Words) representation where xv denotes the frequency of word v.
wvn takes a value of 1(0) when wn is v ∈ V (is not v ∈ V ).
y = (y1,y2,···,yK) is a topic vector into which a document w is categorized, where yi takes a value of 1(0) when the ith topic is (not) assigned with a document w.
Iy ⊂Y is a set of topic index i, where yi takes a value of 1 in y.
summationtexti∈Iy and Πi∈Iy denote the sum and product for all i in Iy, respectively.
Γ(x) is the Gamma function and Ψ is the Psi function(Minka, 2002).
A probabilistic generative model for documents with multiple topicsmodelsaprobabilityofgeneratingadocumentw in multiple topics y using model parameter Θ, i.e., models P(w|y,Θ).
A multiple categorization problem is to estimate multiple topics y∗ of a document w∗ whose topics are unknown.
The model parameters are learned by documents D ={(wd,yd)}Md=1, where M is the number of documents.
3 Parametric
Mixture Model In this section, we briefly explain Parametric Mixture Model(PMM)(Ueda, N.
and Saito, K., 2002a; Ueda, N.
and Saito, K., 2002b).
3.1 Overview
PMM models multiple-topic documents by mixing model parameters of each single topic with an equal mixture ratio, where the model parameter θiv is the probability that word v is generated from topic i.
This is because it is impractical to use model parameter corresponding to multiple topics whose number is 2K −1(all combination of K topics).
PMM achieved more useful results than machine learning methods such as Naive Bayes, SVM, K-NN and Neural Networks (Ueda, N.
and Saito, K., 2002a; Ueda, N.
and Saito, K., 2002b).
3.2 Formulation
PMM employs a BOW representation and is formulated as follows.
P(w|y,θ) = ΠVv=1(ϕ(v,y,θ))xv (1) θ is a K × V matrix whose element is θiv = P(v|yi = 1).
ϕ(v,y,θ) is the probability that word v is generated from multiple topics y and is defined as the linear sum of hi(y) and θiv as follows: ϕ(v,y,θ) =summationtextKi=1 hi(y)θiv hi(y) is a mixture ratio corresponding to topic i and is formulated as follows: hi(y) = yisummationtextK j=1 yj,summationtextKi=1 hi(y) = 1.
(if yi = 0, then hi(y) = 0) 3.3 Learning Algorithm of Model Parameter The learning algorithm of model parameter θ in PMM is an iteration method similar to the EM algorithm.
Model parameter θ is estimated by maximizing ΠMd=1P(wd|yd,θ) in training documents D = {(wd,yd)}Md=1.
Function g corresponding to a document d is introduced as follows: gdiv(θ) = h(yd)θivsummationtextK j=1 hj(yd)θjv (2) Theparametersareupdatedalongwiththefollowing formula.
θ(t+1)iv = 1C( Msummationdisplay d xdvgdiv(θ(t)) + ζ−1) (3) xdv is the frequency of word v in document d.
C is the normalization term for summationtextVv=1 θiv = 1.
ζ is a smoothing parameter that is Laplace smoothing when ζ is set to two.
In this paper, ζ is set to two as the original paper.
4 Proposed
Model In this section, firstly, we mention the problem related to PMM.
Then, we explain our solution of the problem by proposing a new model.
422 4.1 Overview PMM estimates model parameter θ assuming that all of mixture ratios of single topic are equal.
It is our intuition that each document can sometimes be more weighted to some topics than to the rest of the assigned topics.
If the topic weightings are averaged over all biases in the whole document set, they could be canceled.
Therefore, model parameter θ learned by PMM can be reasonable over the whole of documents.
However, if we compute the probability of generating an individual document, a document-specific topic weight bias on mixture ratio is to be considered.
The proposed model takes into account this document-specific bias by assuming that mixture ratio vector pi follows Dirichlet distribution.
This is because we assume the sum of the element in vector pi is one and each element pii is nonnegative.
Namely, the proposed model assumes model parameter of multiple topics as a mixture of model parameter on each single topic with mixture ratio following Dirichlet distribution.
Concretely, given a document w and multiple topics y, it estimates a posterior probability distribution P(pi|x,y) by Bayesian inference.
For convenience, the proposed model is called PDMM(Parametric Dirichlet Mixture Model).
In Figure 1, the mixture ratio(bias) pi = (pi1,pi2,pi3),summationtext3i=1 pii = 1,pii > 0 of three topics is expressed in 3-dimensional real spaceR3.
The mixture ratio(bias)pi constructs 2D-simplex inR3.
One pointonthesimplexindicatesonemixtureratiopi of the three topics.
That is, the point indicates multiple topics with the mixture ratio.
PMM generates documents assuming that each mixture ratio is equal.
That is, PMM generates only documents with multiple topics that indicates the center point of the 2Dsimplex in Figure 1.
On the contrary, PDMM generates documents assuming that mixture ratio pi followsDirichletdistribution.
Thatis, PDMMcangenerate documents with multiple topics whose weights can be generated by Dirichlet distribution.
4.2 Formulation
PDMM is formulated as follows: P(w|y,α,θ) = integraldisplay P(pi|α,y)ΠVv=1(ϕ(v,y,θ,pi))xvdpi (4) Figure 1: Topic Simplex for Three Topics pi is a vector whose element is pii(i∈Iy).
pii is a mixture ratio(bias) of model parameter corresponding to single topic i where pii > 0,summationtexti∈Iy pii = 1.
pii can be considered as a probability of topic i, i.e., pii = P(yi = 1|pi).
P(pi|α,y) is a prior distribution of pi whose index i is an element of Iy, i.e., i∈Iy.
We use Dirichlet distribution as the prior.
α is a parameter vector of Dirichlet distribution corresponding to pii(i∈Iy).
Namely, the formulation is as follows.
P(pi|α,y) = Γ( summationtext i∈Iy αi) Πi∈IyΓ(αi) Πi∈Iypi αi−1 i (5) ϕ(v,y,θ,pi) is the probability that word v is generatedfrommultipletopicsy andisdenotedasalinear sum of pii(i∈Iy) and θiv(i∈Iy) as follows.
ϕ(v,y,θ,pi) = summationdisplay i∈Iy piiθiv (6) = summationdisplay i∈Iy P(yi = 1|pi)P(v|yi = 1,θ) (7) 4.3 Variational Bayes Method for Estimating Mixture Ratio This section explains a method to estimate the posterior probability distribution P(pi|w,y,α,θ) of a document-specific mixture ratio.
Basically, P(pi|w,y,α,θ) is obtained by Bayes theorem using Eq.(4).
However, that is computationally impractical because a complicated integral computation is needed.
Therefore we estimate an approximate distribution of P(pi|w,y,α,θ) using Variational Bayes Method(H.Attias, 1999).
The concrete explanation is as follows 423 Use Eqs.(4)(7).
P(w,pi|y,α,θ) = P(pi|α,y)ΠVv=1( summationdisplay i∈Iy P(yi = 1|pi)P(v|yi = 1,θ))xv Transform document expression of above equation into words vector w = (w1,w2,···,wN).
P(w,pi|y,α,θ) = P(pi|α,y)ΠNn=1 summationdisplay in∈Iy P(yin = 1|pi)P(wn|yin = 1,θ) By changing the order of summationtext and Π, we have P(w,pi|y,α,θ) = P(pi|α,y) summationdisplay i∈INy ΠNn=1P(yin = 1|pi)P(wn|yin = 1,θ) ( summationdisplay i∈INy ≡ summationdisplay i1∈Iy summationdisplay i2∈Iy ··· summationdisplay iN∈Iy ) Express yin = 1 as zn = i.
P(w|y,α,θ) =integraldisplay summationdisplay z∈INy P(pi|α,y)ΠNn=1P(zn|pi)P(wn|zn,θ)dpi ( summationdisplay z∈INy ≡ summationdisplay z1∈Iy summationdisplay z2∈Iy ··· summationdisplay zN∈Iy ) (8) Eq.(8) is regarded as Eq.(4) rewritten by introducing a new latent variable z = (z1,z2,···,zN).
P(w|y,α,θ) = integraldisplay summationdisplay z∈INy P(pi,z,w|y,α,θ)dpi (9) Use Eqs.(8)(9) P(pi,z,w|y,α,θ) = P(pi|α,y)ΠNn=1P(zn|pi)P(wn|zn,θ) (10) Hereafter, we explain Variational Bayes Method for estimating an approximate distribution of P(pi,z|w,y,α,θ) using Eq.(10).
This approach is the same as LDA(D.M.Blei et al., 2001; D.M.Blei et al., 2003).
The approximate distribution is assumed to be Q(pi,z|γ,φ).
The following assumptions are introduced.
Q(pi,z|γ,φ) = Q(pi|γ)Q(z|φ) (11) Q(pi|γ) = Γ( summationtext i∈Iy γi) Πi∈IyΓ(γi) Πi∈Iypi γi−1 i (12) Q(z|φ) = ΠNn=1Q(zn|φ) (13) Q(zn|φ) = ΠKi=1(φni)zin (14) Q(pi|γ) is Dirichlet distribution where γ is its parameter.
Q(zn|φ) is Multinomial distribution where φni is its parameter and indicates the probability that the nth word of a document is topic i, i.e.
P(yin = 1).
zin is a value of 1(0) when zn is (not) i.
According to Eq.(11), Q(pi|γ) is regarded as an approximate distribution of P(pi|w,y,α,θ) The log likelihood of P(w|y,α,θ) is derived as follows.
logP(w|y,α,θ) = integraldisplay summationdisplay z∈INy Q(pi,z|γ,φ)dpilogP(w|y,α,θ) = integraldisplay summationdisplay z∈INy Q(pi,z|γ,φ)log P(pi,z,w|y,α,θ)Q(pi,z|γ,φ) dpi + integraldisplay summationdisplay z∈INy Q(pi,z|γ,φ)log Q(pi,z|γ,φ)P(pi,z|w,y,α,θ)dpi logP(w|y,α,θ) = F[Q] + KL(Q,P) (15) F[Q] = integraltext summationtextz∈INy Q(pi,z|γ,φ)log P(pi,z,w|y,α,θ)Q(pi,z|γ,φ) dpi KL(Q,P) = integraltext summationtextz∈INy Q(pi,z|γ,φ)log Q(pi,z|γ,φ)P(pi,z|w,y,α,θ)dpi KL(Q,P) is the Kullback-Leibler Divergence that is often employed as a distance between probability distributions.
Namely, KL(Q,P) indicates a distance between Q(pi,z|γ,φ) and P(pi,z|w,y,α,θ).
logP(w|y,α,θ) is not relevant to Q(pi,z|γ,φ).
Therefore, Q(pi,z|γ,φ) that maximizes F[Q] minimizes KL(Q,P), and gives a good approximate distribution of P(pi,z|w,y,α,θ).
We estimate Q(pi,z|γ,φ), concretely its parameter γ and φ, by maximizingF[Q] as follows.
Using Eqs.(10)(11).
F[Q] = integraldisplay Q(pi|γ)logP(pi|α,y)dθ (16) + integraldisplay summationdisplay z∈INy Q(pi|γ)Q(z|φ)logΠNn=1P(zn|pi)dθ(17) + summationdisplay z∈INy Q(z|φ)logΠNn=1P(wn|zn,θ) (18) − integraldisplay Q(pi|γ)logQ(pi|γ)dθ (19) − summationdisplay z∈INy Q(z|φ)logQ(z|φ) (20) 424 = logΓ(summationtexti∈Iy αj)−summationtexti∈Iy logΓ(αi) +summationtexti∈Iy(αi−1)(Ψ(γi)−Ψ(summationtextj∈Iy γj))(21) + Nsummationdisplay n=1 summationdisplay i∈Iy φni(Ψ(γi)−Ψ( summationdisplay j∈Iy γj)) (22) + Nsummationdisplay n=1 summationdisplay i∈Iy Vsummationdisplay j=1 φniwjn logθij (23) − logΓ( summationdisplay j∈Iy γj) + summationdisplay i∈Iy logΓ( summationdisplay j∈Iy γj) − summationdisplay i∈Iy (γi−1)(Ψ(γi)−Ψ( summationdisplay j∈Iy γj))(24) − Nsummationdisplay n=1 summationdisplay i∈Iy φni logφni (25) F[Q] is known to be a function of γi and φni from Eqs.(21) through (25).
Then we only need to resolve the maximization problem of nonlinear function F[Q] with respect to γi and φni.
In this case, the maximization problem can be resolved by Lagrange multiplier method.
First, regard F[Q] as a function of γi, which is denoted as F[γi].
Then,γi does not have constraints.
Therefore we only need to find the following γi, where ∂F[γi]∂γi = 0.
The resultant γi is expressed as follows.
γi = αi + Nsummationdisplay n=1 φni (i∈Iy) (26) Second, regard F[Q] as a function of φni, which is denoted asF[φni].
Then, considering the constraint thatsummationtexti∈Iy φni = 1, Lagrange function L[φni] is expressed as follows: L[φni] = F[φni] + λ( summationdisplay i∈Iy φni−1) (27) λ is a so-called Lagrange multiplier.
We find the following φni where ∂L[φni]∂φni = 0.
φni = θiwnC exp(Ψ(γi)−Ψ( summationdisplay j∈Iy γj)) (i∈Iy))(28) C is a normalization term.
By Eqs.(26)(28), we obtain the following updating formulas of γi and φni.
γ(t+1)i = αi + Nsummationdisplay n=1 φ(t)ni (i∈Iy) (29) φ(t+1)ni = θiwnC exp(Ψ(γ(t+1)i )−Ψ( summationdisplay j∈Iy γ(t+1)j ))(30) Using the above updating formulas, we can estimate parameters γ and φ, which are specific to a document w and topics y.
Last of all, we show a pseudo code :vb(w,y) which estimates γ and φ.
In addition, we regard α, which is a parameter of a prior distribution of pi, as a vector whose elements are all one.
That is because Dirichlet distribution where each parameter is one becomes Uniform distribution.
•Variational Bayes Method for PDMM———— function vb(w,y): 1.
Initialize αi←1∀i∈Iy 2.
Compute γ(t+1),φ(t+1) using Eq.(29)(30) 3.
ifbardblγ(t+1) − γ(t)bardbl< epsilon1 &bardblφ(t+1) − φ(t)bardbl< epsilon1 4.
then return (γ(t+1),φ(t+1)) and halt 5.
else t←t + 1 and goto step (2) ———————————————————— 4.4 Computing Probability of Generating Document PMM computes a probability of generating a document w on topics y and a set of model parameter Θ as follows: P(w|y,Θ) = ΠVv=1(ϕ(v,y,θ))xv (31) ϕ(v,y,θ) is the probability of generating a word v on topics y that is a mixture of model parameter θiv(i ∈ Iy) with an equal mixture ratio.
On the otherhand, PDMMcomputestheprobabilityofgenerating a word v on topics y using θiv(i ∈ Iy) and an approximate posterior distribution Q(pi|γ) as follows: 425 ϕ(v,y,θ,γ) = integraldisplay ( summationdisplay i∈Iy piiθiv)Q(pi|γ)dpi (32) = summationdisplay i∈Iy integraldisplay piiQ(pi|γ)dpiθiv (33) = summationdisplay i∈Iy ˜piiθiv (34) ˜pii = integraltext piiQ(pi|γ)dpi = γisummationtext j∈Iy γj (C.M.Bishop, 2006) The above equation regards the mixture ratio of topics y of a document w as the expectation ˜pii(i∈ Iy) of Q(pi|γ).
Therefore, a probability of generating w P(w|y,Θ) is computed with ϕ(v,y,θ,γ) estimated in the following manner: P(w|y,Θ) = ΠVv=1(ϕ(v,y,θ,γ)))xv (35) 4.5 Algorithm for Estimating Multiple Topics of Document PDMM estimates multiple topics y∗ maximizing a probability of generating a document w∗, i.e., Eq.(35).
This is the 0-1 integer problem(i.e., NPhard problem), so PDMM uses the same approximate estimation algorithm as PMM does.
But it is different from PMM’s estimation algorithm in that it estimates the mixture ratios of topics y by Variational Bayes Method as shown by vb(w,y) at step 6 in the following pseudo code of the estimation algorithm: •Topics Estimation Algorithm———————– function prediction(w): 1.
Initialize S ← {1,2,···},yi ← 0 for i(1,2···,K) 2.
vmax←−∞ 3.
while S is not empty do 4.
foreach i∈S do 5.
yi←1,yj∈S\i←0 6.
Compute γ by vb(w,y) 7.
v(i)←P(w|y) 8.
end foreach 9.
i∗←argmax v(i) 10.
if v(i∗) > vmax 11.
yi∗ ←1,S←S\i∗,vmax←v(i∗) 12.
else 13.
return y and halt ———————————————————— 5 Evaluation WeevaluatetheproposedmodelbyusingF-measure of multiple topics categorization problem.
5.1 Dataset
We use MEDLINE1 as a dataset.
In this experiment, we use five thousand abstracts written in English.
MEDLINE has a metadata set called MeSH Term.
For example, each abstract has MeSH Terms such as RNAMessengerandDNA-BindingProteins.
MeSH Terms are regarded as multiple topics of an abstract.
In this regard, however, we use MeSH Terms whose frequency are medium(100-999).
We did that because the result of experiment can be overly affected by such high frequency terms that appear in almost every abstract and such low frequency terms that appear in very few abstracts.
In consequence, the number of topics is 88.
The size of vocabulary is 46,075.
Theproportionofdocumentswithmultipletopicson the whole dataset is 69.8%, i.e., that of documents with single topic is 30.2%.
The average of the number of topics of a document is 3.4.
Using TreeTagger2, we lemmatize every word.
We eliminate stop words such as articles and be-verbs.
5.2 Result
of Experiment We compare F-measure of PDMM with that of PMM and other models.
F-measure(F) is as follows: F = 2PRP+R, P = |Nr∩Ne||Ne|, R = |Nr∩Ne||Nr| . Nr is a set of relevant topics . Ne is a set of estimated topics.
A higher F-measure indicates a better ability to discriminate topics.
In our experiment, we compute F-measure in each document and average the F-measures throughout the whole document set.
We consider some models that are distinct in learning model parameter θ.
PDMM learns model parameter θ by the same learning algorithm as PMM.
NBM learns model parameter θ by Naive Bayes learning algorithm.
The parameters are updated according to the following formula: θiv = Miv+1 C . Miv is the number of training documentswhere a word v appears in topic i.
C is normalization term forsummationtextVv=1 θiv = 1.
1http://www.nlm.nih.gov/pubs/factsheets/medline.html 2http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/ 426 The comparison of these models with respect to F-measure is shown in Figure 2.
The horizontal axis is the proportion of test data of dataset(5,000 abstracts).
For example, 2% indicates that the number of documents for learning model is 4,900 and the numberofdocumentsforthetestis100.
Thevertical axis is F-measure.
In each proportion, F-measure is an average value computed from five pairs of training documents and test documents randomly generated from dataset.
F-measure of PDMM is higher than that of other methods on any proportion, as shown in Figure 2.
Therefore, PDMM is more effective than other methods on multiple topics categorization.
Figure 3 shows the comparison of models with respect to F-measure, changing proportion of multiple topic document for the whole dataset.
The proportion of document for learning and test are 40% and 60%, respectively.
The horizontal axis is the proportion of multiple topic document on the whole dataset.
For example, 30% indicates that the proportion of multiple topic document is 30% on the whole dataset and the remaining documents are single topic, that is, this dataset is almost single topic document.
In 30%.
there is little difference of Fmeasure among models.
As the proportion of multiple topic and single topic document approaches 90%, that is, multiple topic document, the differences of F-measure among models become apparent.
This result shows that PDMM is effective in modeling multiple topic document.
Figure 2: F-measure Results 5.3 Discussion In the results of experiment described in section 5.2, PDMM is more effective than other models in Figure 3: F-measure Results changing Proportion of Multiple Topic Document for Dataset multiple-topic categorization.
If the topic weightings are averaged over all biases in the whole of training documents, they could be canceled.
This cancellation can lead to the result that model parameter θ learned by PMM is reasonable over the whole of documents.
Moreover, PDMM computes the probability of generating a document using a mixture of model parameter, estimating the mixture ratio of topics.
This estimation of the mixture ratios, we think, is the key factor to achieve the results better than other models.
In addition, the estimation of a mixture ratio of topics can be effective from the perspective of extracting features of a document with multiple topics.
A mixture ratio of topics assigned to a document is specific to the document.
Therefore, the estimation of the mixture ratio of topics is regarded as a projection from a word-frequency space of QV where Q is a set of integer number to a mixture ratio space of topics [0,1]K in a document.
Since the size of vocabulary is much more than that of topics, the estimation of the mixture ratio of topics is regarded as a dimension reduction and an extraction of features in a document.
This can lead to analysis of similarity among documents with multiple topics.
For example, the estimated mixture ratio of topics [Comparative Study]C[Apoptosis] and [Models,Biological] in one MEDLINE abstract is 0.656C0.176 and 0.168, respectively.
This ratio can be a feature of this document.
Moreover, we can obtain another interesting results as follows.
The estimation of mixture ratios of topics uses parameter γ in section 4.3.
We obtain interesting results from another parameter φ that needstoestimateγ.
φni isspecifictoadocument.
A 427 Table1: WordListofDocumentXwhoseTopicsare [Female], [Male] and [Biological Markers] Ranking Top10 Ranking Bottom10 1(37) biomarkers 67(69) indicate 2(19) Fusarium 68(57) problem 3(20) non-Gaussian 69(45) use 4(21) Stachybotrys 70(75) % 5(7) chrysogenum 71(59) correlate 6(22) Cladosporium 72(17) population 7(3) mould 73(15) healthy 8(35) Aspergillus 7433) response 9(23) dampness 75(56) man 10(24) 1SD 76(64) woman φni indicates the probability that a word wn belongs to topic i in a document.
Therefore we can compute the entropy on wn as follows: entropy(wn) =summationtextKi=1 φni log(φni) We rank words in a document by this entropy.
For example, a list of words in ascending order of the entropy in document X is shown in Table 1.
A value inparenthesesisarankingofwordsindecendingorder of TF-IDF(= tf ·log(M/df),where tf is term frequency in a test document, df is document frequency andM is the number of documents in the set of doucuments for learning model parameters) (Y.
Yang and J.
Pederson, 1997) . The actually assigned topics are [Female], [Male] and [Biological Markers], where each estimated mixture ratio is 0.499, 0.460 and 0.041, respectively.
The top 10 words seem to be more technical than thebottom10wordsinTable1.
Whentheentropyof a word is lower, the word is more topic-specific oriented, i.e., more technical . In addition, this ranking of words depends on topics assigned to a document.
When we assign randomly chosen topics to the same document, generic terms might be ranked higher.
For example, when we rondomly assign the topics [Rats], [Child] and [Incidence], generic terms such as ”use” and ”relate” are ranked higher as shown in Table 2.
The estimated mixture ratio of [Rats], [Child] and [Incidence] is 0.411, 0.352 and 0.237, respectively.
For another example, a list of words in ascending order of the entropy in document Y is shown in Table 3.
The actually assigned topics are Female, Animals, Pregnancy and Glucose..
The estimated mixture ratio of [Female], [Animals],[Pregnancy] and Table2: WordListofDocumentXwhoseTopicsare [Rats], [Child] and [Incidence] Ranking Top10 Ranking Bottom10 1(69) indicate 67(56) man 2(63) relate 68(47) blot 3(53) antigen 69(6) exposure 4(45) use 70(54) distribution 5(3) mould 71(68) evaluate 6(4) versicolor 72(67) examine 7(35) Aspergillus 73(59) correlate 8(7) chrysogenum 74(58) positive 9(8) chartarum 75(1) IgG 10(9) herbarum 76(60) adult [Glucose] is 0.442, 0.437, 0.066 and 0.055, respectively In this case, we consider assigning sub topics of actual topics to the same document Y.
Table 4 shows a list of words in document Y assigned with the sub topics [Female] and [Animals].
The estimated mixture ratio of [Female] and [Animals] is 0.495 and 0.505, respectively.
Estimated mixture ratio of topics is chaged.
It is interesting that [Female] has higher mixture ratio than [Animals] in actual topics but [Female] has lower mixture ratio than [Animals] in sub topics [Female] and [Animals].
According to these different mixture ratios, the ranking of words in docment Y is changed.
Table 5 shows a list of words in document Y assigned with the sub topics [Pregnancy] and [Glucose].
The estimated mixture ratio of [Pregnancy] and [Glucose] is 0.502 and 0.498, respectively.
It is interesting that in actual topics, the ranking of gglucose-insulinhand”IVGTT”ishighindocument Y but in the two subset of actual topics, gglucoseinsulinh and ”IVGTT” cannot be find in Top 10 words.
The important observation known from these examplesisthatthisrankingmethodofwordsinadocument can be assosiated with topics assigned to the document.
φ depends on γ seeing Eq.(28).
This is because the ranking of words depends on assigned topics, concretely, mixture ratios of assigned topics.
TF-IDF computed from the whole documents cannot have this property.
Combined with existing the extraction method of keywords, our model has the potential to extract document-specific keywords using information of assigned topics.
428 Table 3: Word List of Document Y whose Actual Topics are [Femaile],[Animals],[Pregnancy] and [Glucose] Ranking Top 10 Ranking Bottom 10 1(2) glucose-insulin 94(93) assess 2(17) IVGTT 95(94) indicate 3(11) undernutrition 96(74) CT 4(12) NR 97(28) % 5(13) NRL 98(27) muscle 6(14) GLUT4 99(85) receive 7(56) pregnant 100(80) status 8(20) offspring 101(100) protein 9(31) pasture 102(41) age 10(32) singleton 103(103) conclusion Table4: WordListofDocumentYwhoseTopicsare [Femaile]and [Animals] Ranking Top 10 Ranking Bottom 10 1(31) pasture 94(65) insulin 2(32) singleton 95(76) reduced 3(33) insulin-signaling 96(27) muscle 4(34) CS 97(74) CT 5(35) euthanasia 98(68) feed 6(36) humane 99(100) protein 7(37) NRE 100(80) status 8(38) 110-term 101(85) receive 9(50) insert 102(41) age 10(11) undernutrition 103(103) conclusion 6 Concluding Remarks We proposed and evaluated a novel probabilistic generative models, PDMM, to deal with multipletopic documents.
We evaluated PDMM and other models by comparing F-measure using MEDLINE corpus.
The results showed that PDMM is more effective than PMM.
Moreover, we indicate the potential of the proposed model that extracts documentspecific keywords using information of assigned topics.
Acknowledgement This research was funded in part by MEXT Grant-in-Aid for Scientific Research on Priority Areas ”i-explosion” in Japan.
References H.Attias 1999.
Learning parameters and structure of latent variable models by variational Bayes.
in Proc of Uncertainty in Artificial Intelligence.
C.M.Bishop 2006.
Pattern Recognition And Machine Learning (Information Science and Statistics), p.687.
Springer-Verlag. D.M.
Blei, Andrew Y.
Ng, and M.I.
Jordan. 2001.
Latent Dirichlet Allocation.
Neural Information Processing Systems 14.
D.M. Blei, Andrew Y.
Ng, and M.I.
Jordan. 2003.
Latent Dirichlet Allocation.
Journal of Machine Learning Research, vol.3, pp.993-1022.
Minka 2002.
Estimating a Dirichlet distribution.
Technical Report.
Y.W.Teh, M.I.Jordan, M.J.Beal, and D.M.Blei. 2003.
Hierarchical dirichlet processes.
Technical Report 653, Department Of Statistics, UC Berkeley.
Ueda, N.
and Saito, K.
2002. Parametric mixture models for multi-topic text.
Neural Information Processing Systems 15.
Ueda, N.
and Saito, K.
2002. Singleshot detection of multi-category text using parametric mixture models.
ACM SIG Knowledge Discovery and Data Mining.
Y. Yang and J.
Pederson 1997.
A comparative study on feature selection in text categorization.
Proc. International Conference on Machine Learning .

