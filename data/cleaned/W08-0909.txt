Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 71–79,
Columbus, Ohio, USA, June 2008. c©2008 Association for Computational Linguistics
An Analysisof StatisticalModelsandFeatures for ReadingDifficulty
Prediction
MichaelHeilman,KevynCollins-Thompson and MaxineEskenazi
LanguageTechnologiesInstitute
Carnegie MellonUniversity
Pittsburgh, PA 15213,USA
{mheilman,kct,max}@cs.cmu.edu
Abstract
A readingdifficulty measurecan be described
as a function or model that maps a text to a
numerical value corresponding to a difficulty
orgradelevel. We describea measureofread-
ability that uses a combinationof lexical fea-
turesandgrammaticalfeaturesthatarederived
from subtrees of syntactic parses. We also
tested statistical models for nominal, ordinal,
and interval scales of measurement. The re-
sults indicate that a model for ordinal regres-
sion, such as the proportionalodds model,us-
ing a combination of grammaticaland lexical
featuresis mosteffective at predictingreading
difficulty.
1 Introduction
A reading difficulty, or readability, measure can be
described as a function or model that maps a text
to a numericalvalue correspondingto a difficultyor
gradelevel. Inputstothisfunctionareusuallystatis-
tics for various lexical and grammatical features of
the text. The output is one of a set of ordered dif-
ficulty levels, usually correspondingto grade levels
forelementaryschoolthroughhighschool. Assuch,
reading difficulty prediction can be viewed as a re-
gressionof gradelevel on a set of textual features.
Early work on readability measures employed
simpleproxiesforgrammaticalandlexicalcomplex-
ity, includingsentencelengthandthenumberofsyl-
lables in a word. Fairly simple features were often
employedbecauseofa lackofcomputationalpower.
Such featuresexhibithigh bias becausethey rely on
strongassumptionsaboutwhatmakesa textdifficult
to read. For example,the use of sentencelengthas a
measure of grammatical complexity assumes that a
longersentenceismoregrammaticallycomplexthan
a shorterone,whichisoftenbutnotalwaysthecase.
Inoneearlymodel,theDale-Challmodel (Daleand
Chall, 1948; Chall and Dale, 1995), reading diffi-
cultyisalinearfunctionofthemeansentencelength
andthepercentageof rarewords,as definedbya list
of 3,000 words commonly known by 4th grade. In
this paper, sentence length is defined as the mean
numberof words in the sentencesof a text.
Many early measures did not employ direct esti-
mates of word frequency due to computationallim-
itations (e.g., (Gunning, 1952; McLaughlin, 1969;
Kincaidetal.,1975)). Instead,thesemeasuresrelied
on the strong relationshipbetween the frequency of
and the number of syllables in a word. More fre-
quent words are more likely to have fewer syllables
(e.g., “the”) than less frequent words (e.g., “vocab-
ulary”), an association that is related to Zipf’s Law
(Zipf,1935). TheFlesch-Kincaidmeasure (Kincaid
et al., 1975) is probably the most common reading
difficultymeasurein use. It is implementedin com-
mon word processing programs. This measure is a
linear function of the mean number of syllables per
word and the mean number of words per sentence.
Klare (1974) provides a summary of other early
work on readability.
More recent approachesto reading difficulty em-
ploy moresophisticatedmodelsthatmake useof the
growth in computationalpower. The Lexile Frame-
work (e.g., (Stenner, 1996)) uses individual word
frequency estimates as a measure of lexical diffi-
culty. The word frequency estimates are derived
71
from a large, varied corpus of text. Lexile uses a
Raschmodel (Rasch,1980)withthemeanlogword
frequencyasalexicalfeatureandthelogofthemean
sentencelengthasagrammaticalfeature. TheRasch
model, related to logistic regression, is used to esti-
mate the level of a student that would comprehend
75% of a given text. The converted log odds ratio
calleda “Lexile” that is used as part of this measure
can be easilymappedto gradeschoollevels.
A reading difficulty measure developed by
Collins-Thompson and Callan (2005) uses
smoothed unigram language modeling to capture
the predictive ability of individual words based
on their frequency at each reading difficulty level.
Collins-Thompson and Callan found that certain
words were very predictive of certain levels. For
example, “grownup” was very predictive of grade
1, and “essay” was very predictive of grade 12. For
a given text, this measure estimates the likelihood
that the text was generatedby each level’s language
model. The predictionis the level of the modelwith
the highest likelihood of generating the text. There
are no grammaticalfeatures.
Natural language processing techniques enable
more sophisticated grammatical analysis for read-
ing difficulty measures. Rather than using sentence
lengthasa proxy, measurescanemploy toolsforau-
tomatic analysis of the syntactic structure of texts
(e.g., (Charniak, 2000)). A measure by Schwarm
and Ostendorf (2005) incorporatessyntactic analy-
ses, amonga varietyof othertypesof features. It in-
cludes four grammaticalfeatures derived from syn-
tactic parses of text: the mean parse tree height, the
meannumberofnounphrases,meannumberofverb
phrases, and mean number of “SBARs.” “SBARs”
are non-terminalnodesthat are associatedwith sub-
ordinate clauses. Their system led to better pre-
dictions than the Flesch-Kincaid and Lexile mea-
sures, but the predictive value of the grammatical
features is not entirely clear. In initial experiments
usingsuchcourse-graingrammaticalfeaturesalone,
rather than in conjunction with language modeling
and other features as in Schwarm and Ostendorf’s
system, we found relatively poor prediction perfor-
mance. Our final approach using subtrees of syn-
tactic parses allows for a finer level of discrimina-
tion that may supportthe detectionof differencesin
gradelevelsbetweentextsthatexhibitthesamehigh
level features.
A reading difficulty measure developed by Heil-
man, Collins-Thompson, Callan, and Eskenazi
(2007) uses the frequency of grammaticalconstruc-
tions as a measure of grammatical difficulty. A set
ofapproximatelytwentyconstructionswereselected
from English as a Second Language grammar text-
books. This set includes grammaticalconstructions
such as the passive voice, relative clauses, and vari-
ousverbtenses. Thefrequenciesareusedasfeatures
for a nearest neighbor classificationalgorithm. The
unigram language modeling approach of Collins-
Thompson and Callan (2005) is used to estimate
lexical difficulty in this measure. The final predic-
tion is a linear functionof the lexical and grammat-
ical components. That model assumes that gram-
matical difficulty is adequately captured by a small
number of constructions chosen according to de-
tailed knowledgeof Englishgrammar. In that work,
the constructions were selected from an English as
a SecondLanguagegrammartextbook,a laborand
knowledge-intensive task that may be less practical
for otherlanguages.
We aim to identify the appropriate scale of mea-
surementfor reading difficulty–nominal,ordinal, or
interval–bycomparingtheeffectivenessofstatistical
models for each type of data. We also extend pre-
vious work combininglexical and grammaticalfea-
tures (Heilman et al., 2007) by making it possible
to include a large number of grammatical features
derived from syntactic structures without requiring
significant linguistic or pedagogicalcontent knowl-
edge, such as a reference guide for the grammar of
the languageof interest.
2 Typesof
Features
2.1 LexicalFeatures
This section and the following section describe the
lexical and grammatical features used in our read-
ing difficulty models. The lexical features are the
relative frequencies of word unigrams. The use of
word unigrams is a standard approach in text clas-
sification (Yang and Pedersen, 1997), and has also
been successfully used to predict reading difficulty
(Collins-ThompsonandCallan,2005). Higherorder
n-gramssuchasbigramsandtrigramswerenotused
as featuresbecausethey didnotimprove predictions
72
in preliminarytests. The specific set of lexical fea-
tures was chosen based on the frequenciesof words
in the training corpus. The system performs mor-
phological stemming and stopword removal. The
remaining5000mostcommonwords comprisedthe
lexical featureset.
2.2 GrammaticalFeatures
Grammatical features are extracted from automatic
context-free grammarparses of sentences. The sys-
tem computes relative frequencies of partial syn-
tactic derivations, which will be called ’subtrees’
hereafter. The approach extends (Heilman et al.,
2007), where frequencies of manually defined syn-
tactic patterns were extracted from syntactic struc-
tures. Inthatapproach,thefeaturesaredefinedman-
ually using linguistic knowledge of the target lan-
guagetoimplementtreesearchpatterns,alabor-and
knowledge-intensive process. The approach advo-
catedinthispaper, however, extractsfrequenciesfor
anautomaticallydefinedsetof subtreepatterns. The
systemconsidersallsubtreesuptoagivendepththat
occurinthetrainingcorpus. Examplesofgrammati-
calfeaturesatlevels0through2areshowninFigure
1. The sentence for the parse tree shown was taken
from a third gradetext.
For depth 0, the system includesall subtreescon-
sisting of just nonterminal nodes. This includes all
parts of speech, as well as non-terminal nodes for
noun phrases, adjective phrases, clauses, etc. For
depth1, the systemincludessubtreescorresponding
to the application of a single context free grammar
rule in the derivation of the tree. An example of a
feature at this level would be a sentence node that
dominatesnodesfor nounphrasesand verb phrases.
For deeper levels, the system includes subtrees cor-
respondingto the successive applicationof rules on
non-terminals symbols until either a terminal sym-
bol is reached or the given depth is reached. An
example feature for level 2 is a subtree in which
a prepositional phrase node dominates a preposi-
tionnodeandnounphrasenode,andthe preposition
node in turn dominates a preposition, and the noun
phrase dominates determiner, adjective, and noun
nodes.
We used a maximum depth of 3 in our exper-
iments. Features of deeper levels occur less fre-
quently in general, and deeper levels were avoided
due to data sparseness. A depth first search algo-
rithm extracts candidate grammatical features from
the training corpus. First, a context-free grammar
parser (Klein and Manning, 2003) derives parse
trees for all texts in the training corpus. The algo-
rithm traverses these parses, at each node counting
all subtree features up to the given depth that are
rooted at that node. The subtree features are sorted
by their overall counts in the corpus. In our ex-
periments, frequencies of the most common 1000
subtrees were chosen as the final features. These
included 64 level 0 features corresponding to non-
terminal symbols, 334 level 1 features, 461 level 2
features, and 141 level 3 features. Deeper levels
have more possible features, but sparsity at level 3
resultedin fewer level 3 featuresbeingselected.
Inourexperiments,thesubtreesincludedterminal
symbols for stopwords. However, the system effec-
tively removed content word terminals from parses
before extracting features. The system could be
modified to include terminal symbols for content
words, or even to ignoreall nodesfor terminalsym-
bols. Subtree features including terminal symbols
for content words would, of course, occur with low
frequency andnotlikelybe includedin thefinalfea-
ture set. Terminal symbols for content words were
omittedso that lexicalinformationwas not included
inthesetofgrammaticalfeatures. Similartoleaving
higher order n-grams out of the lexical feature set,
omittingterminalsymbolsfor content words avoids
confoundinggrammaticalandlexicalinformationin
the grammaticalfeatureset. Subtreecountsare nor-
malizedbythenumberofwordsinatexttocompute
the relative frequencies. Normalizationby the num-
berof sentencesin a text is alsopossible,but didnot
perform as well in preliminary tests. The Stanford
Parser (KleinandManning,2003)version1.5.1was
usedto derive treestructuresfor sentences. We used
the unlexicalized model included in the distribution
whichwas trainedon Wall StreetJournaltexts.
3 StatisticalModels
3.1 Scalesof
Measurementfor Reading
Difficulty
Several statistical models were tested for effective-
ness at predicting reading difficulty. The appropri-
ateness of these models depends on the nature of
73
Figure1: Parse Tree for Sentence from ThirdGradeText with ExampleSubtreeFeatures.
readingdifficultydata,particularlythescaleofmea-
surement. The standardunit for readingdifficultyis
the grade level. First throughtwelfthgrade levels in
American schools have been used in previous work
(e.g., (Heilmanet al., 2007; Collins-Thompsonand
Callan, 2005)). English as a Second Language lev-
els have also been used (Heilman et al., 2007),
as well as grade levels for other languages such
as French (Collins-Thompson and Callan, 2005).
While these grades are assignedevenly spacedinte-
gers, the ranges of reading difficulty corresponding
to these grades are not necessarilyevenly spaced. It
is possible, of course, that assuming even spacing
between levels might produce more parsimonious
and accurate statistical models. A more reasonable
assumption is that the grade numbers assigned to
each difficulty level denote an ordering: for exam-
ple, that grade 1 is in some sense less than grade 2,
which is less than grade 3, etc. Different statistical
modelshandlethis assumptionmoreor less well.
Statisticsgenerallydistinguishfourscalesofmea-
surement,whichare, orderedby increasingassump-
tions about the relationshipsbetween values: nomi-
nal, ordinal, interval, and ratio (Stevens, 1946; Co-
hen et al., 2003). Nominal data involve no relation-
ships between the labels or classes of the data. An
example would be types of fruits, where a model
mightbeusedtomakedecisionsbetweenapplesand
oranges. This type of prediction is generally called
classificationin machinelearningand relatedfields.
Ordinal data have a natural ordering, but the val-
ues are not necessarily evenly spaced. For exam-
ple, data about the severity of illnesses might have
labels such as mild, moderate, severe, deceased, in
which the transitions between consecutive classes
all have the same direction but not the same mag-
nitude. Making predictions about such data is gen-
erally calledordinalregression (McCullagh,1980).
Interval data, however, are both orderedand evenly
spaced. An example would be temperatureas mea-
sured in Fahrenheit degrees. Such data have an ar-
bitrary zero point, and negative values may occur.
Ratio data, of which annual income is an example,
do have a meaningful zero point. We will not dis-
cuss ratio data further since its distinction from in-
terval datais notrelevantto thispaper. It is notclear
to which scale reading difficulty corresponds. The
assumption of an interval scale allows for simpler
modelswithfewerparameters.However, modelsfor
ordinal or even nominal data might be more appro-
priate if the strong assumption of an interval scale
does not hold.
We experimentedwith three linear and log-linear
modelscorrespondingto interval, ordinal,and nom-
inal data. Parameters were estimated using L2 reg-
ularization, which corresponds to a Gaussian prior
distribution with zero mean and a user-specified
variance over the parameters. We chose these mod-
els because they are commonly used in the statis-
tics, machinelearning,and behavioral sciencecom-
munities, and aimed to set up meaningful compar-
isons among the scales of measurement. Other ma-
chine learning algorithms might also be employed.
Infact,webrieflytestedthemaximummargin (Vap-
nik, 1995) approach, which led to comparable re-
sults and mightbe worth exploringin futurework.
74
3.2 LinearRegression
LinearRegression(LIN) producesa linear modelin
which the dependent, or outcome, variable is a lin-
ear function of the values for predictor variables,
or features. A prediction for a given text is the
inner product of a vector of feature values for the
text anda vectorof regressioncoefficientsestimated
fromtrainingdata. Forthecaseofreadingdifficulty,
the grade level is a linear combination of the lexi-
caland/orgrammaticalfeaturevalues. LINprovides
continuousestimatesof reading difficulty, such that
a prediction might fall between grade levels. The
estimateswerenot roundedto wholenumbersin the
experiments. For rare cases of an LIN prediction
fallingoutsidethe appropriaterangeof gradelevels,
thevaluewassettothemaximumorminimumgrade
level. LIN implicitly assumes that the data fall on
an interval scale, meaning that the levels are evenly
spaced. The LIN model has relatively few parame-
tersbut makes strongassumptionsaboutthescaleof
measurement.For details,see (Hastieet al., 2001).
3.3 ProportionalOddsModel
The Proportional Odds (PO) model, also called the
parallel regression model and the cumulative logit
model,is a formoflog-linear, or exponential,model
for ordinal data (McCullagh, 1980). Given a new
unlabeled instance as input, the model provides es-
timates of the probability that the instance belongs
to a class at or above a particularlevel. In Equation
(1), P(y ≥ j) is this estimatedprobability, αj is an
interceptparameterfor the given level j, β is vector
of regressioncoefficients,Xi is the vectorof feature
values for instancei, and yi is the predictedreading
difficultylevel.
P(yi ≥ j) = exp(αj +β
TXi)
1+exp(αj +βTXi) (1)
ln P(yi ≥ j)1−P(y
i ≥ j)
= αj +βTXi (2)
The PO model has a parameterαj for the thresh-
old,or intercept,at eachlevel j, but onlya singleset
β of parametersfor the features. Thesetwo typesof
parameterscorrespondto an implicit assumptionof
ordinality. Havinga singlesetof parametersforfea-
tures acrossthe levels meansthat changesin feature
valuesproportionallyaffecttheoddsoftransitioning
from any one class to another.
The estimated probability of an instance belong-
ingto a particularclassis the differencebetweenes-
timatesfor that class and the next highestclass. For
example, the estimated probability of a text being
at the eighth grade level would be the estimate for
being at or above eighth grade minus the estimate
for being at or above ninth grade. As in binary lo-
gistic regression, the PO model estimates log odds
ratios based on the values of features or predictor
variables. The numerator of the odds ratio is the
probabilityof being at or above a level, and the de-
nominatoris the probabilityof being below a level.
Equation (2) shows the form of the model that is
linearin the parameters.
3.4 Multi-classLogisticRegression
Multi-classLogisticRegression(LOG),or multino-
mial logit regression,is a log-linearmodelfor nom-
inal data. In contrast to the simpler PO model, the
model maintains parameters for all of the features
for every class except one category, which is used
for comparison. Thus, for reading difficulty, there
are about 11 times as many parameters to estimate
compared to LIN and PO. The increased difficulty
of parameter estimation for this model is offset for
domains in which assumptions of ordinality or lin-
earity do not hold. For more details, see (Hastie et
al., 2001).
4 Evaluation
4.1 Web
Corpus
The corpus of materials used for training and test-
ing the modelsconsistsof the contenttext extracted
from Web pages with readingdifficulty level labels.
Web pages were used because the system for pre-
dictingreadingdifficultyis beingusedas part of the
REAP tutoring system, which finds authentic and
appropriateWeb pagesfor Englishvocabularyprac-
tice (Brown and Eskenazi, 2004; Heilman et al.,
2006). Approximately half of these texts were au-
thored by students at the particular grade level, and
half were authoredby teachersor writersand aimed
at readers at a particular grade level. Texts were
found for grade levels 1 through 12. The twelfth
grade level also includedsome post-secondarylevel
75
texts. Variousgenresand subjectswererepresented.
In all cases, either the text itself or a link to it iden-
tified it as having a certain level. The content text
was manually extracted from these Web pages so
thatnoisyinformationsuchasnavigationmenusand
advertisements were not included. Automatic con-
tentextractionmay,however,beabletoremovesuch
noisy informationwithouthumanintervention(e.g.,
(Gupta et al., 2003)). This Web corpus is adapted
from the corpora used in prior work on reading dif-
ficulty predication (Collins-Thompsonand Callan,
2005; Heilman et al., 2007). We modified that cor-
pus because it contained a number of documents
pertaining to mathematics and vocabulary practice.
The majority of tokens in these texts were not part
of well-formed, grammatical sentences suitable for
reading practice. Since our goal is to measure the
difficulty of reading passages, we removed these
documents and added additional texts consisting of
more suitable reading material. The corpus con-
sisted of approximately 150,000 words, distributed
among289texts. Thenumberoftextsforeachgrade
level was approximately the same, with at least 28
texts at each level. The mean length in words of
thetextswasapproximately500words,whichcorre-
spondsto abouta page. Texts for lower gradeswere
necessarilyshorter. Weextractedexcerptsforhigher
leveltextssothattextswereotherwiseroughlyequal
in length across levels. For these excerpts, the first
500 or so words of text were extracted, while re-
spectingsentenceand paragraphboundaries.
4.2 EvaluationMetrics
Root mean squareerror (RMSE),Pearson’s correla-
tion coefficient, and accuracy within 1 grade level
served as metrics for evaluating the performance
of reading difficulty predictions. Multiple statistics
were used because it is not entirely clear what the
bestmeasureof predictionqualityis for readingdif-
ficulty. RMSE is the square root of the empirical
mean of the squared error of predictions. It more
stronglypenalizesthose errors that are furtheraway
fromthetruevalue. It canbeinterpretedas theaver-
age numberof gradelevels that predictionsmeasure
deviate from human-assignedlabels.
Pearson’s correlation coefficient measures the
strength of the linear relationship, or similarity of
trends,betweentworandomvariables. Ahighcorre-
lation would indicatethat difficult texts would more
likely receive high predicted difficulty values, and
easiertexts wouldbe morelikely to receive low pre-
dicted difficulty values. Correlations do not, how-
ever, measure the degree to which values match in
absoluteterms.
Adjacent accuracy is the proportion of predic-
tionsthat werewithinone gradelevel of the human-
assigned label for the given text. Exact accuracy is
toostringenta measurebecausethehuman-assigned
reading levels are not always perfect and consis-
tent. For example, one school might read “Romeo
and Juliet” in 9th grade while another school might
readit in 10thgrade. The drawbackof this accuracy
metric is that predictions that are two levels off are
treatedthesameaspredictionsthataretenlevelsoff.
4.3 Baselines
The performance of other algorithms for estimat-
ing reading difficulty was estimated using the same
data. These comparisoninclude Collins-Thompson
andCallan’s implementationof theirlanguagemod-
eling approach (2005), an implementation of the
Flesch-Kincaid reading level measure (Kincaid et
al., 1975), and a measureusing word frequency and
sentence length similar to Lexile (Stenner et al.,
1983). We did not directly test the approach de-
scribed by (Heilman et al., 2007). We observe
that its reported results for first language texts were
notsignificantlydifferentintermsofcorrelationand
only slightly better in terms of mean squared er-
ror than the language modeling approach. Finally,
a simple uniform baseline, which always chose the
middlevalue of 6.5, was tested.
The Lexile-like measure (LX) used the same two
features as the Lexile measure: mean log frequency
or words and log mean sentence length. Instead of
usinga Raschmodeland convertingscoresto “Lex-
iles,” however, the PO model was used to directly
predict grade levels. The log frequency values for
wordswereestimatedfromthesecondreleaseof the
American National Corpus (Reppen et al., 2005),
a 20 million word corpus with texts in American
English from different genres on a variety of sub-
jects. Using the proportional odds models is effec-
tively equivalent to using Lexile’s Rasch model and
mappingitsoutputtogradelevels. Themajordiffer-
encebetweentheLexilemeasureandtheimplemen-
76
tation used in these experimentsis the training data
sets used to estimated word frequencies and model
parameters.
4.4 Procedure
The Web Corpus was randomly split into training
and test sets. The test set consisted of 25% of the
individual texts at each level, a total of 84 texts.
Ten-fold stratified cross-validation on the training
set was employed to estimate the prediction per-
formance according to the evaluation metrics. In
cross-validation, data are partitioned randomly into
a given number of folds, and each fold is used for
testing while all others are used for training. For
more details and a discussion of validation meth-
ods, see (Hastie et al., 2001). The regularization
hyper-parametersweretunedonthetrainingsetdur-
ing cross-validation by a simple grid search. After
cross-validation, models were trained on the entire
training set, and then evaluated using the held-out
test data.
Wetestedwhethereachfeature-set,algorithmpair
or baseline performed significantly differently than
our hypothesized best model, the PO model with
the combined feature set. We employed the bias-
corrected and accelerated (BCa) Bootstrap (Efron
andTibshirani,1993)with50,000replicationsofthe
held-out test data to generate confidence intervals
for differencesin evaluationresults. If the(1−α)%
confidence intervals for the difference do not con-
tain zero, which is the value corresponding to the
null hypothesis,then that difference is significantat
the α level. For example,the 99% confidenceinter-
val for the difference in adjacent accuracy between
the language modeling baseline and the PO model
with the combined feature set was (-1.86, -0.336),
indicatingthatthisdifferenceis significantat the.01
level sinceit does not containzero.
5 Results
Table 1presentscorrelationcoefficients,RMSEval-
ues, and accuracy values for cross-validation and
held-outtest data. Statisticalsignificancewas tested
only for the held-out test data since the hyper-
parameterswere tuned during cross-validation. Our
discussionof the results pertainsmostly to the eval-
uationon the test-set.
Of the various statistical models, the PO model
for ordinal data appears to provide superior perfor-
mance over the LIN and LOG models. Compared
to the LOG model, the PO model performs sig-
nificantly better in terms of correlation and RMSE
and comparablywell in terms of adjacent accuracy.
ComparedtotheLINmodel,thePOmodelperforms
almost as well in terms of correlation, comparably
well in terms of RMSE, and far better in terms of
accuracy.
The performanceof the methods when using dif-
ferentfeaturesetsdoesnotclearlyindicatea bestset
of features to use for predicting reading difficulty.
For the PO model, none of the feature sets lead to
significant gains over the others in terms of any of
the metrics. However, the combined feature set led
to the best performance in terms of correlation and
adjacentaccuracy duringcross-validationas well as
RMSE on the test set, suggesting at the very least
that including the extra features does not degrade
performance.
The PO modelwith the combinedfeatureset out-
performed most of the baseline measures. LX had
the same accuracy value on the test set. The LX
method appears to perform the best in general of
the baselines models. Interestingly, LX uses pro-
portional odds logistic regression like PO, and thus
assumesanordinalbutnotintervalscaleofmeasure-
ment. RMSEvaluesweresignificantlylower for the
PO model than for LX and the language modeling
approach.
No statistically significant advantages are seen
for PO model when compared to Flesch-Kincaid.
We observe however, that for the sample of web
pages which constitutes the evaluation corpus the
PO model produced superior results across evalua-
tion metrics. That is, PO performed better in terms
of adjacentaccuracy, RMSE,and correlationcoeffi-
cients,bothincross-validationandtestingwithheld-
out data.
6 Discussion
In our tests, the PO model, which assumes ordinal
data, lead to the most effective predictions of read-
ingdifficultyingeneral. Thisresultindicatesthatthe
reading difficulty of texts, according to grade level,
lies on an ordinal scale of measurement. That is,
77
Method Features Cross-Validation Held-OutTest Set
Correl. RMSE Adj. Acc. Correl. RMSE Adj. Acc.
LIN Lexical .629 2.73 .242 .779 2.42 .167**
Grammatical .767 2.26 .294 .753 2.33 .274*
Combined .679 2.57 .284 .819** 2.21 .226**
PO Lexical .713 2.57 .498 .780 2.29 .464
Grammatical .762 2.22 .505 .734 2.42 .560
Combined .773 2.24 .519 .767 2.23 .440
LOG Lexical .517 3.24 .443 .619* 2.83* .548
Grammatical .632 2.87 .443 .506** 3.38** .464
Combined .582 2.94 .446 .652* 2.71* .556
LX .659 2.77 .467 .731 2.67* .464
Lang. Modeling .590 2.74 .370 .630 2.70** .381
Flesch-Kincaid .697 2.66 .388 .718 2.54 .369
Uniform .000 3.39 .170 .000** 3.45** .167**
Table 1: Results from Cross-Validation and Test Set Evaluations, as measured by CorrelationCoefficients (Correl.),
Root Mean Square Error (RMSE), and Adjacent Accuracy. The best result for each metric for each evaluation is
given in bold. Asterisksindicatesignificantdifferencescomparedto the PO modelwith a CombinedFeatureSet. * =
p < .05, ** = p < .01.
readingdifficultyappearstoincreasesteadilybutnot
linearlywithgradelevel. Assuch,theLINapproach
that produces linear models was less effective, par-
ticularly in terms of adjacent accuracy. The LOG
model, for nominal data, also led to inferior perfor-
mance comparedto the PO model, which can be at-
tributed to the difficulty of accurately estimating a
morecomplexmodelwithmanyparametersforeach
level.
Our tests found that grammatical features alone
can be effective predictorsof readability. This find-
ing disagreeswitha previousresultthat foundthat a
model using a combinationof lexical and manually
definedgrammaticalfeatures (Heilmanet al., 2007)
outperformed a model using grammatical features
alone. The superior predictive ability of the mod-
els wedescribethatusegrammaticalfeaturescanbe
attributed to the automatic derivation of a grammat-
ical feature set that is more than an order of magni-
tude larger than in the previous approach. Our ap-
proach enables the use of much larger grammatical
featuresets becauseit doesnot requirethe extensive
linguistic knowledge and effort to manually define
the grammatical features. The automatic approach
also enables an easier transition to other languages,
assuminga parser is available. Using the combined
feature set did not hurt performance, however, and
since regularized statistical models can avoid over-
fittinglargenumbersofparameters,a combinedfea-
ture set still seemsappropriate.
Acknowledgments
We thank Jamie Callan for his comments and sug-
gestions. This research was supported in part by
the Institute of Education Sciences, U.S. Depart-
ment of Education,through Grant R305B040063to
Carnegie Mellon University; Dept. of Education
grantR305G03123;thePittsburghScienceofLearn-
ing Center which is funded by the National Sci-
ence Foundation,award numberSBE-0354420;and
a National Science Foundation Graduate Research
Fellowship awarded to the first author. Any opin-
ions,findings,conclusions,or recommendationsex-
pressed in this material are the authors, and do not
necessarilyreflectthoseof the sponsors.
References
Jon Brown and MaxineEskenazi. 2004. Retrieval of au-
thentic documents for reader-specific lexical practice.
Proceedingsof InSTIL/ICALLSymposium2004.
78
J. S. Chall and E. Dale. 1995. Readability Revisited:
The New Dale-Chall ReadabilityFormula. Brookline
Books.Cambridge,MA.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. Proceedingsof the NAACL.
J. Cohen, P. Cohen, S. G. West, and L. S. Aiken. 2003.
Applied Multiple Regression/Correlation Analysis for
the Behavioral Sciences, 3rd Edition. Lawrence Erl-
baumAssociates,Inc.
Michael Collins and Nigel Duffy. 2002. Convolution
KernelsforNaturalLanguage. Advancesin Neural In-
formationProcessingSystems..
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical language
models. Journal of the AmericanSociety for Informa-
tion Scienceand Technology, 56(13). pp. 1448-1462..
E. Dale and J. S. Chall. 1948. A Formula for Predicting
Readability. Educational Research Bulletin Vol. 27,
No. 1.
Bradley Efron and Robert J. Tibshirani. 1993. An In-
troduction to the Bootstrap. Chapman and Hall, New
York.
R. Gunning. 1952. The technique of clear writing..
McGraw-Hill,New York.
S. Gupta, G. Kaiser, D. Neistadt, and P. Grimm. 2003.
DOM-based content extraction of HTML documents.
ACM Press,New York.
Trevor Hastie, Robert Tibshirani, Jerome Friedman.
2003. The Elements of StatisticalLearning:DataMin-
ing, Inference, and Prediction. Springer.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan,and MaxineEskenazi. 2007. CombiningLex-
ical andGrammaticalFeaturesto Improve Readability
Measures for First and Second Language Texts. Pro-
ceedingsof the Human Language Technology Confer-
ence. Rochester, NY.
Michael Heilman, Kevyn Collins-Thompson, Jamie
Callan, and Maxine Eskenazi. 2006. Classroom suc-
cessof an IntelligentTutoringSystemforlexicalprac-
tice and reading comprehension. Proceedings of the
Ninth International Conference on Spoken Language
Processing. Pittsburgh, PA.
J. Kincaid, R. Fishburne, R. Rodgers, and B. Chissom.
1975. Derivationof new readabilityformulasfor navy
enlisted personnel. Branch Report 8-75. Chief of
Naval Training,Millington,TN.
G. R. Klare. 1974. Assessing Readability. Reading Re-
search Quarterly, Vol. 10, No. 1. pp. 62-102..
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. Proceedings of the 41st Meet-
ing of the Association for Computational Linguistics,
pp. 423-430.
G. H. McLaughlin. 1969. SMOG grading: A new read-
abilityformula. Journalof Reading.
P. McCullagh. 1980. Regression Models for Ordinal
Data. Journal of the Royal Statistical Society. Series
B (Methodological),Vol. 42, No. 2. pp. 109-142.
G. Rasch. 1980. Probabilistic Models for Some Intelli-
gence and Attainment Tests. MESA Press, Chicago,
IL.
G. Rasch. 2005. AmericanNational Corpus (ANC) Sec-
ond Release.. Linguistic Data Consortium. Philadel-
phia, PA.
Sarah Schwarm and Mari Ostendorf. 2005. Read-
ing level assessment using support vector machines
and statistical language models. Proceedings of the
43rd Annual Meeting on Association for Computa-
tional Linguistics.
A. J. Stenner, M. Smith, and D. S. Burdick. 1983. To-
ward a Theory of ConstructDefinition. Journalof Ed-
ucationalMeasurement,Vol. 20, No. 4. pp. 305-316.
A. J. Stenner. 1996. Measuring reading comprehension
with the Lexile framework. Fourth North American
Conference on Adolescent/Adult Literacy.
S. S. Stevens. 1946. On the theoryof scales of measure-
ment. Science, 103, pp. 677-680.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer.
Y. Yang and J. P. Pedersen. 1997. A Comparative Study
on Feature Selectionin Text Categorization. Proceed-
ings of the Fourteenth International Conference on
Machine Learning(ICML’97), pp. 412-420.
G. K. Zipf. 1935. The Psychobiology off Language.
HoughtonMifflin, Boston,MA.
79

