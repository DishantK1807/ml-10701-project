Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 14–22,
Uppsala, July 2010.
TowardsABetterUnderstandingof
UncertaintiesandSpeculationsinSwedishClinicalText
– AnalysisofanInitialAnnotationTrial
SumithraVelupillai
Departmentof Computerand SystemsSciences(DSV)
StockholmUniversity
Forum 100
SE-16440 Kista, Sweden
sumithra@dsv.su.se
Abstract
ElectronicHealthRecords(EHRs)contain
a large amountof free text documentation
which is potentiallyvery useful for Infor-
mation Retrieval and Text Mining appli-
cations. We have, in an initial annotation
trial, annotated6 739 sentencesrandomly
extractedfrom a corpusof SwedishEHRs
for sentencelevel (un)certainty, and token
level speculative keywords and negations.
This set is split into differentclinicalprac-
tices and analyzed by means of descrip-
tive statisticsand pairwiseInter-Annotator
Agreement(IAA) measured by F1-score.
We identify geriatrics as a clinical prac-
tice with a low average amount of uncer-
tain sentences and a high average IAA,
and neurology with a high averageamount
of uncertainsentences. Speculative words
are oftenn-grams,anduncertainsentences
longer than average. The results of this
analysis is to be used in the creation of a
new annotatedcorpuswherewe will refine
and further develop the initial annotation
guidelinesand introducemorelevels of di-
mensionality. Once we have finalizedour
guidelinesand refined the annotationswe
plan to release the corpus for further re-
search, after ensuring that no identifiable
informationis included.
1 Introduction
ElectronicHealthRecords(EHRs)containa large
amount of free text documentationwhich is po-
tentiallyvery usefulfor InformationRetrieval and
Text Miningapplications.Clinicaldocumentation
is specific in many ways; there are many authors
in a document(e.g. physicians,nurses), there are
different situationsthat are documented(e.g. ad-
mission,currentstatus). Moreover, they mayoften
be written under time pressure, resulting in frag-
mented,brieftexts oftencontainingspelling errors
and abbreviations.With accessto EHRdata,many
possibilitiesto exploitdocumentedclinicalknowl-
edge and experiencearise.
One of the propertiesof EHRs is that they con-
tain reasoning about the status and diagnoses of
patients. Gathering such informationfor the use
in e.g. medical research in order to find rela-
tionships between diagnoses, treatmentsetc. has
great potential.However, in many situations,clin-
icians might describe uncertain or negated find-
ings, which is crucial to distinguishfrom positive
or asserted findings. Potentialfuture applications
includesearch engineswhere medicalresearchers
can search for particular diseases where negated
or speculative contexts are separatedfromasserted
contexts, or text mining systems where e.g. dis-
eases that seem to occur often in speculative con-
texts are presentedto the user, indicatingthat more
research is needed. Moreover, laymen may also
benefitfrominformationretrieval systemsthatdis-
tinguish diseases or symptoms that are more or
less certain given current medical expertise and
knowledge.
We have, in an initialannotationtrial, annotated
6 739 sentencesrandomlyextractedfrom a corpus
of SwedishEHRs for sentencelevel (un)certainty,
and token level speculative keywords and nega-
tions1. In this paper, a deeper analysis of the re-
sulting annotations is performed. The aims are
to analyze the results split into different clinical
practices by means of descriptive statistics and
pairwise Inter-Annotator Agreement (IAA) mea-
sured by F1-score, with the goal of identifyinga)
whether specific clinical practices contain higher
or lower amounts of uncertain expressions, b)
1This research has been carried out after approval
from the Regional Ethical Review Board in Stockholm
(Etikpr¨ovningsn¨amnden i Stockholm), permission number
2009/1742-31/5
14
whetherspecificclinicalpracticesresult in higher
or lower IAA indicatinga less or more difficult
clinical practice for judging uncertainties,and c)
identifyingthe characteristicsof the entitiesanno-
tated as speculative words, are they highly lexi-
cal or is a deeper syntactic and/or semantic anal-
ysis required for modeling? From this analysis,
we plan to conduct a new annotation trial where
we will refine and further develop the annotation
guidelinesand use domainexpertsfor annotations
in orderto be able to createa usefulannotatedcor-
pus modelinguncertainties,negations and specu-
lationsin Swedishclinicaltext, whichcan be used
to develop tools for the automaticidentificationof
these phenomenain, for instance, Text Miningap-
plications.
2 RelatedResearch
In recent years, the interest for identifying and
modelingspeculative languagein naturallanguage
text has grown. In particular, biomedical scien-
tific articles and abstractshave been the object of
several experiments. In Light et al. (2004), four
annotatorsannotated891 sentenceseach as either
highly speculative, low speculative, or definite,
in biomedical scientific abstracts extracted from
Medline. In total, they found 11 percent specula-
tive sentences, resultingin IAA results, measured
with kappa, between 0.54 and 0.68. One of their
main findings was that the majority of the specu-
lative sentences appeared towards the end of the
abstract.
Vincze et al. (2008)describethe creationof the
BioScope corpus, where more than 20 000 sen-
tences from both medical (clinical)free texts (ra-
diologyreports),biologicalfull papersand biolog-
ical scientific abstracts have been annotated with
speculative and negation keywords along with
their scope. Over 10 percent of the sentences
were either speculative or negated. In the clinical
sub-corpus,14 percentcontainedspeculative key-
words. Threeannotatorsannotatedthe corpus,and
the guidelineswere modifiedseveral times during
the annotation process, in order to resolve prob-
lematic issues and refine definitions. The IAA
results, measured with F1-score, in the clinical
sub-corpusfor negationkeywords rangedbetween
0.91 and 0.96, and for speculative keywords be-
tween 0.84 and 0.92. The BioScope corpus has
been used to train and evaluate automatic classi-
fiers (e.g. ¨Ozg¨ur and Radev (2009) and Morante
and Daelemans(2009))with promisingresults.
Five qualitative dimensions for characterizing
scientific sentences are defined in Wilbur et al.(2006), includinglevels of certainty. Here, guide-
lines are also developedover a long periodof time
(more than a year), testingand revising the guide-
lines consecutively. Their final IAA results, mea-
suredwithF1-score,rangebetween0.70 and 0.80.
Differentlevels of dimensionalityfor categorizing
certainty(in newspaper articles) is also presented
in Rubinet al. (2006).
Expressionsfor communicatingprobabilitiesor
levels of certainty in clinical care may be inher-
ently difficult to judge. Eleven observers were
asked to indicate the level of probabilityof a dis-
ease implied by eighteen expressionsin the work
presented by Hobby et al. (2000). They found
that expressionsindicatingintermediateprobabili-
ties were much less consistentlyrated than those
indicating very high or low probabilities. Sim-
ilarly, Khorasani et al. (2003) performed a sur-
vey analyzingagreementbetweenradiologistsand
non-radiologistsregarding phrasesused to convey
degrees of certainty. In this study, they found lit-
tle or no agreementamongthe survey participants
regarding the diagnosticcertaintyassociatedwith
these phrases. Althoughwe do not have access to
radiologyreports in our corpus, these findingsin-
dicatethat it is not trivial to classifyuncertainlan-
guage in clinical documentation,even for domain
experts.
3 Method
The annotation trial is based on sentences ran-
domly extracted from a corpus of Swedish EHRs
(see Dalianis and Velupillai (2010) for an initial
descriptionand analysis). These records contain
both structured (e.g. measure values, gender in-
formation)and unstructuredinformation(i.e. free
text). Each free text entry is written under a spe-
cific heading,e.g. Status, Current medication, So-
cial Background. For this corpus, sentenceswere
extractedonly from the free text entry Assessment
(Bed¨omning), with the assumptionthat these en-
tries contain a substantialamountof reasoningre-
gardinga patient’s diagnosisand situation.A sim-
ple sentence tokenizing strategy was employed,
based on heuristicregular expressions2. We have
used Knowtator (Ogren, 2006) for the annotation
2The performanceof the sentencetokenizer has not been
evaluatedin this work.
15
work.
One senior level student(SLS), one undergrad-
uatecomputerscientist(UCS),andoneundergrad-
uatelanguageconsultant(ULC)annotatedthe sen-
tences into the following classes; on a sentence
level: certain, uncertain or undefined, and on a
token level: speculativewords, negations, and un-
definedwords.
The annotators are to be considered naive
coders, as they had no prior knowledge of the
task, nor any clinical background. The annota-
tion guidelineswere inspiredby those created for
the BioScope corpus (Vincze et al., 2008), with
some modifications (see Dalianis and Velupillai
(2010)). The annotatorswere allowed to break a
sentence into subclausesif they found that a sen-
tence containedconflictinglevels of certainty, and
they wereallowedto markquestionmarksas spec-
ulative words. They did not annotate the linguis-
tic scopesof each token level instance. The anno-
tators worked independently, and met for discus-
sions in even intervals (in total seven), in order to
resolve problematicissues. No informationabout
the clinic, patient gender, etc. was shown. The
annotationtrial is consideredas a first step in fur-
ther work of annotatingSwedish clinical text for
speculative language.
Clinicalpractice # sentences # tokens
hematology 140 1 494
surgery 295 3 269
neurology 351 4 098
geriatrics 142 1 568
orthopaedics 245 2 541
rheumatology 384 3 348
urology 120 1 393
cardiology 128 1 242
oncology 550 5 262
ENT 224 2 120
infection 107 1 228
emergency 717 6 755
paediatrics 935 8 926
total, clinicalpractice 4 338 43 244
total, full corpus 6 739 69 495
Table 1: Numberof sentencesand tokens per clin-
icalpractice(#sentences>100),andin total. ENT
= Ear, Nose and Throat.
3.1 Annotationsandclinicalpractices
The resulting corpus consists of 6 739 sentences,
extracted from 485 unique clinics. In order to
be able to analyze possible similarities and dif-
ferences across clinical practices, sentences from
clinics belongingto a specific practice type were
groupedtogether. In Table 1, the resultinggroups,
along with the total amount of sentences and to-
kens, are presented3. Only groups with a total
amountof sentences> 100 were used in the anal-
ysis, resultingin 13 groups. A clinic was included
in a clinical practice group based on a priority
heuristics, e.g. the clinic ”Barnakuten-kir”(Pae-
diatric emergency surgery) was groupedinto pae-
diatrics.
The averagelength(in tokens) per clinicalprac-
tice and in total are given in Table 2. Clinical
documentationis often very brief and fragmented,
for most clinical practices (except urology and
cardiology)the minimum sentence length (in to-
kens) was one, e.g. ”basal”, ”terapisvikt” (ther-
apy failure), ”lymf¨odem” (lymphedema), ”viros”
(virosis), ”opanm¨ales” (reported to surgery, com-
pound with abbreviation). We see that the aver-
age sentencelengthis aroundten for all practices,
where the shortestare found in rheumatologyand
the longestin infection.
As the annotatorswereallowed to breakup sen-
tencesinto subclauses,but not requiredto, this led
to a considerabledifferencein the total amountof
annotationsper annotator. In order to be able to
analyze similarities and differences between the
resulting annotations, all sentence level annota-
tions were converted into one sentenceclass only,
the primary class (defined as the first sentence
level annotationclass, i.e. if a sentence was bro-
ken into two clauses by an annotator, the first be-
ing certain and the second being uncertain, the
final sentence level annotation class will be cer-
tain). The sentencelevel annotationclass certain
was in clear majority among all three annotators.
On both sentenceand token level, the class unde-
fined (a sentence that could not be classified as
certain or uncertain, or a token which was not
clearly speculative) was rarely used. Therefore,
all sentencelevel annotationsmarked as undefined
are converted to the majorityclass, certain, result-
ing in two sentence level annotationclasses (cer-
tain and uncertain) and two token level annotation
classes (speculativewords and negations, i.e. to-
3Whitespace tokenization.
16
kens annotatedas undefinedare ignored).
For the remaining analysis, we focus on the
distributions of the annotation classes uncertain
and speculativewords, per annotatorand annota-
tor pair, and per clinicalpractice.
Clinicalpractice Max Avg Stddev
hematology 40 10.67 7.97
surgery 57 11.08 8.29
neurology 105 11.67 10.30
geriatrics 58 11.04 9.29
orthopaedics 40 10.37 6.88
rheumatology 59 8.72 7.99
urology 46 11.61 7.86
cardiology 50 9.70 7.46
oncology 54 9.57 7.75
ENT 54 9.46 7.53
infection 37 11.48 7.76
emergency 55 9.42 6.88
paediatrics 68 9.55 7.24
total, full corpus 120 10.31 8.53
Table 2: Token statistics per sentenceand clinical
practice. All clinic groups except urology (min =
2) and cardiology(min = 2) have a minimumsen-
tence lengthof one token.
Figure 1: Sentence level annotation: uncertain,
percentageper annotatorand clinicalpractice.
4 Results
We have measuredthe proportions(in percent)per
annotator for each clinical practice and in total.
This enablesan analysisof whetherthere are sub-
stantial individual differencesin the distributions,
indicatingthat this annotationtask is highly sub-
jective and/or difficult. Moreover, we measure
IAA by pairwise F1-score. From this, we may
Figure 2: PairwiseF1-score, sentencelevel anno-
tation class uncertain.
draw conclusions whether specific clinical prac-
tices are harder or easier to judge reliably (i.e. by
high IAA results).
Figure 3: Average length in tokens, per annotator
and sentenceclass.
In Figure 1, we see that the average amount of
uncertainsentenceslies between9 and 12 percent
for each annotator in the full corpus. In general,
UCS has annotated a larger proportion of uncer-
tain sentencescomparedto ULC and SLS.
The clinicaldisciplinewith the highestaverage
amount of uncertainsentencesis neurology (13.7
percent), the lowest average amount is found in
cardiology (4.7 percent). Surgery and cardiology
show the largest individual differences in propor-
tions (from 9 percent(ULC)to 15 percent(UCS),
and from 2 percent(ULC)to 7 percent(UCS),re-
spectively).
However, in Figure 2, we see that the pairwise
IAA,measuredbyF1-score,is relatively low, with
an average IAA of 0.58, ranging between 0.54
(UCS/SLS) and 0.65 (UCS/ULC), for the entire
corpus. In general, the annotator pair UCS/ULC
have higherIAA results,with the highestfor geri-
atrics (0.78). The individual proportionsfor un-
17
certain sentences in geriatrics is also lower for
all annotators(see Figure 1), indicatinga clinical
practicewitha low amountof uncertainsentences,
and a slightlyhigheraverageIAA (0.64F1-score).
4.1 Sentencelengths
As the focuslies on analyzingsentencesannotated
as uncertain, one interestingpropertyis to look at
sentence lengths (measured in tokens). One hy-
pothesisis that uncertainsentencesare in general
longer. In Figure 3 we see that in general, for
all threeannotators,uncertainsentencesare longer
than certain sentences. This result is, of course,
highlyinfluencedby the skewness of the data (i.e.
uncertainsentencesare in minority),but it is clear
that uncertainsentences,in general,are longer on
average. It is interesting to note that the annota-
tor SLS has, in most cases, annotatedlonger sen-
tences as uncertain, comparedto UCS and ULC.
Moreover, geriatrics, with relatively high IAA but
relatively low amountsof uncertainsentences,has
well above average sentencelengthsin the uncer-
tain class.
4.2 Tokenlevelannotations
When it comes to the token level annotations,
speculative words and negations, we observed
very high IAA for negations(0.95F1-score(exact
match)on averagein the full corpus,the lowestfor
neurology, 0.94). These annotationswere highly
lexical (13 unique tokens) and unambiguous,and
spread evenly across the two sentence level anno-
tation classes(rangingbetween1 and 3 percentof
the total amount of tokens per class). Moreover,
all negationswere unigrams.
On the other hand,we observed large variations
in IAA results for speculative words. In Figure
4, we see that there are considerabledifferences
between exact and partial matches4 between all
annotator pairs, indicating individual differences
in the interpretationsof what constitutes a spec-
ulative word and how many tokens they cover,
and the lexicality is not as evident as for nega-
tions. The highest level of agreementwe find be-
tween UCS/ULCin orthopaedics(0.65 F1-score,
partial match) and neurology (0.64F1-score, par-
tial match),and the lowest in infection(UCS/SLS,
0.31F1-score).
4Partial matchesare measuredon a characterlevel.
Figure 4: F1-score, speculative words, exact and
partialmatch.
4.2.1 Speculative
words– mostcommon
The low IAA results for speculativewords invites
a deeper analysisfor this class. How is this inter-
pretedby the individualannotators?First,we look
at the most common tokens annotated as specu-
lative words, shared by the three annotators: ”?”,
”sannolikt”(likely), ”ev” (possibly, abbreviated),
”om” (if). The most common speculative words
are all unigrams, for all three annotators. These
tokens are similar to the most common specu-
lative words in the clinical BioScope subcorpus,
where if, may and likely are among the top five
most common. Those tokens that are most com-
mon per annotator and not sharedby the othertwo
(among the five most frequent)include ”bed¨oms”
(judged), ”kan” (could), ”helt” (completely) and
”st¨allningstagande” (standpoint).
Lookingat neurology andurology, witha higher
overall averageamountof uncertainsentences,we
find that the most common words for neurology
are similar to those most common in total, while
for urology we find moren-grams. In Table 3, the
five mostcommonspeculative wordsper annotator
for neurologyand urologyare presented.
When it comes to the unigrams,many of these
are also not annotated as speculative words. For
instance, ”om” (if), is annotatedas speculative in
only 9 percent on average of its occurrencein the
neurologicaldata (the same distribution holds, on
average, in the total set). In Morante and Daele-
mans (2009), if is also one of the words that are
subject to the majority of false positives in their
automatic classifier. On the other hand, ”sanno-
likt” (likely) is almostalways annotatedas a spec-
ulative word (over 90 percentof the time).
18
UCS ULC SLS
neurology ? ? ?
sannolikt(likely) kan (could) sannolikt(likely)
kan (could) sannolikt(likely) ev (possibly, abbr)
om (if) om (if) om (if)
pr¨ova (try) verkar (seems) st¨allningstagande (standpoint)
ter (seem) ev (possibly, abbr) m¨ojligen(possibly)
urology kan vara (could be) mycket (very) tyder p˚a(indicates)
tyder p˚a(indicates) inga tecken (no signs) i f¨orsta hand (primarily)
ev (possibly, abbr) kan vara (could be) misst¨ankt (suspected)
misst¨ankt (suspected) kan (could) kanske (perhaps)
kanske (perhaps) tyder (indicates) skall vi f¨ors¨oka (shouldwe try)
planerastydligen(apparently planned) misst¨ankt (suspected) kan vara (could be)
Table 3: Most commonspeculative words per annotatorfor neurology and urology.
4.2.2 Speculative
words–n-grams
Speculative words are, in Swedish clinical text,
clearly not simple lexical unigrams. In Figure 5
we see that the average length of tokens anno-
tated as speculative words is, on average, 1.34,
with the longest in orthopaedics(1.49) and urol-
ogy (1.46). We also see that SLS has, on aver-
age, annotatedlongersequencesof tokens as spec-
ulative words compared to UCS and ULC. The
longest n-grams range between three and six to-
kens, e.g. ”kan inte se n˚agra tydliga” (can’t see
any clear), ”kan r¨ora sig om” (could be about),
”inte helt har kunnatuteslutas”(has not been able
to completelyexclude), ”i f¨orstahand”(primarily).
In many of these cases, the strongest indicator is
actually a unigram (”kan” (could)), within a verb
phrase. Moreover, negations inside a speculative
word annotation,such as ”inga tecken” (no signs)
are annotateddifferentlyamongthe individualan-
notators.
Figure5: Average length,speculative words.
4.3 Examples
We have observed low average pairwise IAA for
sentence level annotationsin the uncertain class,
withmoreor lesslarge differencesbetweenthe an-
notatorpairs. Moreover, at the token level and for
the class speculativewords, we also see low av-
erage agreement,and indicationsthat speculative
words often aren-grams. We focus on the clinical
practices neurology, because of its average large
proportion of uncertain sentences, geriatrics for
its high IAA results for UCS/ULCand low aver-
age proportionof uncertainsentences,and finally
surgery, for its large discrepancy in proportions
and low average IAA results.
In Example1 we see a sentencewhere two an-
notators (ULC, SLS) have marked the sentence
as uncertain, also markinga unigram(”ospecifik”
(unspecific) as a speculativeword. This example
is interestingsince the utterance is ambiguous,it
can be judgedas certainas in the dizzinessis con-
firmed to be of an unspecifictype or uncertain as
in the type of dizziness is unclear, a type of ut-
terance which should be clearly addressed in the
guidelines.
<C> Yrsel av ospecifiktyp. </C>
<U> Yrsel av <S> ospecifik </S> typ.
</U>
<U> Yrsel av <S> ospecifik </S> typ.
</U>
Dizzinessof unspecifictype
Example1: Annotationexample,neurology. Am-
biguous sentence, unspecificas a possible specu-
lation cue. C = Certain,U = Uncertain,S = Spec-
ulative words.
An example of different interpretationsof the
minimumspan a speculativeword covers is given
in Example 2. Here, we see that ”inga egentliga
m¨arkbara” (no real apparent) has been annotated
in three different ways. It is also interesting to
19
note the role of the negation as part of ampli-
fying speculation. Several such instances were
marked by the annotators (for further examples,
see Dalianis and Velupillai (2010)), which con-
forms well with the findingsreportedin Kilicoglu
and Bergler (2008), where it is showed that ex-
plicit certaintymarkers togetherwith negation are
indicators of speculative language. In the Bio-
Scopecorpus(Vincze et al., 2008),such instances
are marked as speculationcues. This example, as
well as Example1, is also interestingas they both
clearlyare part of a longerpassageof reasoningof
a patient, with no particular diagnosis mentioned
in the current sentence. Instead of randomly ex-
tractingsentencesfrom the free text entry Assess-
ment, onepossibilitywouldbe to let the annotators
judgeall sentencesin an entry(or a full EHR).Do-
ing this, differencesin wherespeculative language
often occur in an EHR (entry) might become ev-
ident, as for scientificwritings,where it has been
showed that speculative sentences occur towards
the end of abstracts(Lightet al., 2004).
<U> <S><N> Inga </N> egentliga </S>
<S> m¨arkbara</S> minnessv˚arigheter under
samtal.</U>.
<U> <N> Inga </N> <S> egentliga </S>
m¨arkbaraminnessv˚arigheterunder samtal.</U>.
<U> <S><N> Inga </N> egentliga m¨arkbara
</S> minnessv˚arigheterunder samtal.</U>.
No real apparent memory difficulties during
conversation
Example2: Annotationexample,neurology. Dif-
ferentannotationcoverageover negationandspec-
ulation. C = Certain,U = Uncertain,S = Specula-
tive words, N = Negation
In geriatrics, we have observed a lower than
average amount of uncertain sentences, and high
IAAbetweenUCSand ULC.In Example3 we see
a sentence where UCS and ULC have matching
annotations,whereasSLShas judgedthissentence
as certain. This example shows the difficulty of
interpretingexpressionsindicatingpossible spec-
ulation – is ”ganska” (relatively) used here as a
marker of certainty (as certain as one gets when
diagnosingthis type of illness)?
The word ”sannolikt”(likely) is one of the most
common words annotated as a speculative word
in the total corpus. In Example 4, we see a sen-
<U> B˚ade anamnestiskt och testm¨assigt <S>
ganska </S> stabil vad det g¨aller Alzheimer
sjukdom.</U>.
<U> B˚ade anamnestiskt och testm¨assigt <S>
ganska </S> stabil vad det g¨ller Alzheimer
sjukdom.</U>.
<C> B˚ade anamnestiskt och testm¨assigt ganska
stabil vad det g¨aller Alzheimersjukdom.</C>.
Both anamnesis and tests relatively stabile
when it comesto Alzheimer’s disease.
Example3: Annotationexample, geriatrics. Dif-
ferent judgements for the word ”ganska” (rela-
tively). C = Certain, U = Uncertain, S = Specu-
lative words.
tence where the annotators UCS and SLS have
judged it to be uncertain, while UCS and ULC
have marked the word ”sannolikt” (likely) as a
speculative word. This is an interesting exam-
ple, through informal discussionswith clinicians
we were informed that this word might as well be
used as a marker of high certainty. Such instances
show the need for using domain experts in future
annotationsof similarcorpora.
<C>En 66-˚arig kvinna med <S>sannolikt</S>
2 synkronatum¨orer v¨anstercolon/sigmoideumoch
d¨ar till levermetastaser.</C>.
<U>En 66-˚arig kvinna med <S>sannolikt</S>
2 synkronatum¨orer v¨anstercolon/sigmoideumoch
d¨ar till levermetastaser.</U>.
<C>En 66-˚arig kvinna med sannolikt2 synkrona
tum¨orer v¨anster colon/sigmoideum och d¨ar till
levermetastaser.</C>.
A 66 year old woman likely with 2 synchronous
tumours left colon/sigmoideumin additionto liver
metastasis.
Example4: Annotationexample, surgery. Differ-
ent judgementsfor the word”sannolikt”(likely). C
= Certain,U = Uncertain,S = Speculative words.
5 Discussion
We have presentedan analysis of an initial anno-
tation trial for the identificationof uncertainsen-
tences as well as for token level cues (specula-
tive words) acrossdifferent clinicalpractices. Our
main findings are that IAA results for both sen-
tence level annotations of uncertainty and token
level annotationsfor speculative words are, on av-
20
erage, fairly low, with higher average agreement
in geriatricsand rheumatology (see Figures1 and
2). Moreover, by analyzing the individual distri-
butions for the classes uncertain and speculative
words, we find that neurology has the highestaver-
age amountof uncertainsentences,and cardiology
the lowest. On average, the amount of uncertain
sentencesrangesbetween9 and 12 percent,which
is in line with previous work on sentencelevel an-
notationsof uncertainty(see Section2).
We have also showed that the most common
speculativewords are unigrams,but thata substan-
tial amount are n-grams. The n-grams are, how-
ever, often part of verb phrases,where the head is
often the speculationcue. However, it is evident
that speculative words are not always simple lex-
ical units, i.e. syntacticinformationis potentially
very useful. Questionmarksare the mostcommon
entitiesannotatedas speculativewords. Although
these are not interesting indicatorsin themselves,
it is interestingto note that they are very common
in clinicaldocumentation.
Fromthe relatively low IAAresultswe draw the
conclusion that this task is difficult and requires
more clearly defined guidelines. Moreover, using
naivecoderson clinicaldocumentationis possibly
not very useful if the resulting annotationsare to
be usedin, e.g. a Text Miningapplicationfor med-
ical researchers. Clinicaldocumentation is highly
domain-specificand contains a large amount of
internal jargon, which requires judgements from
clinicians. However, we find it interestingto note
that we have identified differences between dif-
ferent clinical practices. A consensuscorpus has
beencreatedfromthe resultingannotations,which
has beenusedin an experimentfor automaticclas-
sification,see Dalianis and Skeppstedt (2010) for
initial resultsand evaluation.
Duringdiscussionsamongthe annotators,some
specific problems were noted. For instance, the
extractedsentenceswere not always about the pa-
tientor the currentstatusor diagnosis,andin many
casesan expressioncoulddescribe(un)certaintyof
someoneotherthan the author(e.g. anotherphysi-
cian or a family member), introducingaspects of
perspective. The sentences annotated as certain,
are difficult to interpret,as they are simplynot un-
certain. We believe that it is important to intro-
duce further dimensions, e.g. explicit certainty,
and focus (what is (un)certain?),as well as time
(e.g. current or past).
6 Conclusions
To our knowledge,thereis no previousresearchon
annotatingSwedish clinical text for sentence and
token level uncertainty together with an analysis
of the differencesbetweendifferent clinical prac-
tices. Althoughthe initial IAA results are in gen-
eral relatively low for all clinical practice groups,
we have identifiedindications that neurology is a
practice which has an above average amount of
uncertain elements, and that geriatrics has a be-
low average amount,as well as higher IAA. Both
these disciplineswould be interestingto continue
the work on identifyingspeculative language.
It is evidentthatclinicallanguagecontainsa rel-
atively high amount of uncertain elements, but it
is also clear that naive coders are not optimal to
use for interpretingthe contents of EHRs. More-
over, more care needs to be taken in the extrac-
tion of sentences to be annotated, in order to en-
sure that the sentences actually describe reason-
ing about the patient status and diagnosis. For in-
stance, instead of randomly extracting sentences
from within a free text entry, it might be better to
let the annotatorsjudgeall sentenceswithinan en-
try. This would also enablean analysisof whether
speculative language is more or less frequent in
specificparts of EHRs.
From our findings, we plan to further develop
the guidelines and particularly focus on specify-
ing the minimal entities that should be annotated
as speculative words (e.g. ”kan” (could)). We
also plan to introducefurtherlevels of dimension-
ality in the annotation task, e.g. cues that indi-
cate a high level of certainty, and to use domain
experts as annotators. Although there are prob-
lematic issues regarding the use of naive coders
for this task, we believe that our analysis has re-
vealed some propertiesof speculative languagein
clinical text which enables us to develop a useful
resourcefor furtherresearchin the areaof specula-
tive language.Judgingan instanceas beingcertain
or uncertain is, perhaps, a task which can never
excludesubjective interpretations.One interesting
way of exploitingthis fact would be to exploit in-
dividualannotationssimilarto the work presented
in Reidsma and op den Akker (2008). Once we
have finalized the annotatedset, and ensured that
no identifiableinformationis included,we plan to
make this resourceavailablefor furtherresearch.
21
References
Hercules Dalianis and Maria Skeppstedt. 2010. Cre-
ating and Evaluating a Consensusfor Negated and
Speculative Wordsin a SwedishClinicalCorpus. To
be publishedin the proceedingsof the Negation and
Speculationin Natural LanguageProcessingWork-
shop, July 10, Uppsala,Sweden.
Hercules Dalianis and Sumithra Velupillai. 2010.
How Certain are Clinical Assessments? Annotat-
ing SwedishClinicalText for (Un)certainties,Spec-
ulations and Negations. In Proceedings of the of
the Seventh InternationalConference on Language
Resources and Evaluation, LREC 2010, Valletta,
Malta,May 19-21.
J. L. Hobby, B. D. M. Tom, C. Todd, P. W. P. Bearcroft,
and A. K. Dixon. 2000. Communicationof doubt
and certainty in radiological reports. The British
Journalof Radiology, 73:999–1001,September.
R. Khorasani,D. W. Bates, S. Teeger, J. M. Rotschild,
D. F. Adams, and S. E. Seltzer. 2003. Is terminol-
ogy used effectively to convey diagnostic certainty
in radiologyreports? AcademicRadiology, 10:685–
688.
Halil Kilicogluand Sabine Bergler. 2008. Recogniz-
ing speculative languagein biomedicalresearch ar-
ticles: a linguisticallymotivated perspective. BMC
Bioinformatics, 9(S-11).
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The language of bioscience: Facts, spec-
ulations, and statements in between. In Lynette
Hirschman and James Pustejovsky, editors, HLT-
NAACL 2004 Workshop: BioLINK 2004, Linking
Biological Literature, Ontologies and Databases,
pages 17–24, Boston, Massachusetts,USA, May 6.
Associationfor ComputationalLinguistics.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts.
In BioNLP ’09: Proceedings of the Workshop on
BioNLP, pages 28–36, Morristown, NJ, USA. As-
sociationfor ComputationalLinguistics.
Philip V. Ogren. 2006. Knowtator: a prot´eg´e plug-in
for annotatedcorpusconstruction.In Proceedingsof
the 2006Conferenceof the NorthAmericanChapter
of the Associationfor ComputationalLinguisticson
HumanLanguage Technology, pages273–275,Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Arzucan ¨Ozg¨ur and Dragomir R. Radev. 2009. De-
tecting speculations and their scopes in scientific
text. In Proceedingsof the 2009 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1398–1407, Singapore, August. Association
for ComputationalLinguistics.
Dennis Reidsma and Rieks op den Akker. 2008. Ex-
ploiting ’subjective’ annotations. In HumanJudge
’08: Proceedingsof the Workshopon HumanJudge-
ments in Computational Linguistics, pages 8–16,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Victoria L. Rubin, Elizabeth D. Liddy, and Noriko
Kando. 2006. Certaintyidentificationin texts: Cat-
egorization model and manual tagging results. In
ComputingAffect and Attitutdein Text: Theory and
Applications. Springer.
Veronika Vincze, Gy¨orgy Szarvas, Rich´ard Farkas,
Gy¨orgy M´ora, and J´anos Csirik. 2008. The bio-
scope corpus: biomedicaltexts annotatedfor uncer-
tainty, negationand theirscopes. BMCBioinformat-
ics, 9(S-11).
J. W. Wilbur, A. Rzhetsky, and H. Shatkay. 2006. New
directionsin biomedicaltext annotation:definitions,
guidelinesand corpus construction. BMC Bioinfor-
matics, 7:356+,July.
22

