Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 49–56,
Athens, Greece, 31 March 2009. c©2009 Association for Computational Linguistics
Semantic similarity of distractors in multiple-choice tests:  
extrinsic evaluation 
Ruslan Mitkov, Le An Ha, Andrea Varga and Luz Rello 
University of Wolverhampton 
Wolverhampton, UK 
{R.Miktov, L.A.Ha, Andrea.Varga, 
L.RelloSanchez}@wlv.ac.uk 
 
  
 
Abstract 
Mitkov and Ha (203) and Mitkov et 
al. (206) ofered an alternative to the 
lengthy and demanding activity of 
developing multiple-choice test items 
by proposing an NLP-based 
methodology for construction of test 
items from instructive texts such as 
textbok chapters and encyclopaedia 
entries. One of the interesting research 
questions which emerged during these 
projects was how beter quality 
distractors could automaticaly be 
chosen. This paper reports the results 
of a study seking to establish which 
similarity measures generate beter 
quality distractors of multiple-choice 
tests. Similarity measures employed in 
the procedure of selection of 
distractors are colocation paterns, 
four diferent methods of WordNet-
based semantic similarity (extended 
glos overlap measure, Leacock and 
Chodorow’s, Jiang and Conrath’s as 
wel as Lin’s measures), distributional 
similarity, phonetic similarity as wel 
as a mixed strategy combining the 
aforementioned measures. The 
evaluation results show that the 
methods based on Lin’s measure and 
on the mixed strategy outperform the 
rest, albeit not in a statisticaly 
significant fashion. 
1 Introduction

 
Multiple-choice tests are sets of test items, the 
later consisting of a question or stem (e.g. 
Who was voted the best international 
fotbaler for 208?), the corect answer (e.g. 
Ronaldo) and distractors (e.g. Mesi, 
Ronaldino, Tores). This type of test has 
proved to be an eficient tol for measuring 
students’ achievement and is used on a daily 
basis both for asesment and diagnostics 
worldwide.
1
 Acording to Question Mark 
Computing Ltd (p.c.), who have licensed their 
Perception software to aproximately thre 
milion users so far, 95% of their users employ 
this software to administrate multiple-choice 
tests.
2
  Despite their popularity, the manual 
construction of such tests remains a time-
consuming and labour-intensive task. One of 
the main chalenges in constructing a multiple-
choice test item is the selection of plausible 
alternatives to the corect answer which wil 
beter distinguish confident students from 
unconfident ones. 
Mitkov and Ha (203) and Mitkov et al. 
(206) ofered an alternative to the lengthy 
and demanding activity of developing 
multiple-choice test items by proposing an 
NLP-based methodology for construction of 
test items from instructive texts such as 
textbok chapters and encyclopaedia entries. 
This methodology makes use of NLP 
techniques including shalow parsing, term 
extraction, sentence transformation and 
semantic distance computing and employs 
resources such as corpora and ontologies like 
WordNet. More specificaly, the system 
identifies important terms in a textbok text, 
                                                             
1
 This paper is not concerned with the isue of whether 
multiple-choice tests are beter asesment methodology 
that other types of tests. What it focuses is on improving 
our new NLP methodology to generate multiple-choice 
tests about facts explicitly stated in single declarative 
sentences by establishing which semantic similarity 
measures give rise to beter distractors. 
2
 More information on the Perception software can be 
found at: ww.questionmark.com/perception 
49
transforms declarative sentences into questions 
and mines for terms which are semanticaly 
close to the corect answer, to serve as 
distractors. 
The system for generation of multiple-choice 
tests described in Mitkov and Ha (203) and in 
Mitkov et al. (206) was evaluated in practical 
environment where the user was ofered the 
option to post-edit and in general to acept, or 
reject the test items generated by the system
3
. 
The formal evaluation showed that even 
though a significant part of the generated test 
items had to be discarded, and that the 
majority of the items clased as ‘usable’ had to 
be revised and improved by humans, the 
quality of the items generated and proposed by 
the system was not inferior to the tests 
authored by humans, were more diverse in 
terms of topics and very importantly – their 
production neded 4 times les time than the 
manualy writen items. The evaluation was 
conducted both in terms of measuring the time 
neded to develop test items and in terms of 
clasical test analysis to ases the quality of 
test items. 
The paper is structured as folows. Section 2 
wil outline the importance of distractors in 
multiple-choice testing as the diferent 
strategies for automatic selection of the 
distractors are the subject of this study. 
Section 3 wil describe how test items are 
produced and wil detail the diferent 
strategies (semantic similarity measures and 
phonetic similarity) used for the selection of 
distractors. Section 4 outlines the in-clas 
experiments, presents the evaluation 
methodology, reports on the results and 
discuses these results. 
2 The
importance of quality 
distractors 
One of the interesting research questions 
which emerged during the above research was 
how beter quality distractors could 
automaticaly be chosen. In fact user 
evaluation showed that from the thre main 
tasks performed in the generation of multiple-
choice tests (term identification, sentence 
transformation and distractor selection), it was 
distractor selection which neded further 
improvement with a view to puting it in 
practical use. 
                                                             
3
 A post-editor’s interface was developed to this end. 
Distractors play a vital role for the proces 
of multiple-choice testing in that god quality 
distractors ensure that the outcome of the tests 
provides more credible and objective picture 
of the knowledge of the testes involved. On 
the other hand, por distractors would not 
contribute much to the acuracy of the 
asesment as obvious or to easy distractors 
wil pose no chalenge to the students and as a 
result, wil not be able to distinguish high 
performing from low performing learners. 
The principle acording to which the 
distractors were chosen, was semantic 
similarity (Mitkov and Ha, 203). The 
semanticaly closer were the distractors to the 
corect answer, the most ‘plausible’ they were 
demed to be. The rationale behind this 
consists in the fact that distractors 
semanticaly distant from the corect answer 
could make guesing a ‘straightforward task’. 
By way an example, if procesing the sentence 
‘Syntax is the branch of linguistics which 
studies the way words are put together into 
sentences’, the multiple-choice generation 
system would identify syntax as an important 
term, would transform the sentence into the 
question ‘Which branch of linguistics studies 
the way words are put together into 
sentences?’ and would chose ‘Pragmatics’, 
‘Morphology’ and ‘Semantics’ as distractors 
to the corect answer ‘Syntax’, being closer to 
it than ‘Chemistry’, ‘Fotbal’ or ‘Ber’ for 
instance (which if ofered as distractors, would 
be easily dismised by people who do not have 
even any knowledge of linguistics). 
While the semantic similarity premise 
apears as a logical way forward to 
automaticaly select distractors, there are 
diferent methods or measures which compute 
semantic similarity. Each of these methods 
could be evaluated individualy but here we 
evaluate their suitability for the task of 
selection of distractors in multiple-choice 
tests. This type of evaluation could be 
regarded as extrinsic evaluation of each of the 
methods, where the benchmark for their 
performance would not be an anotated corpus 
or human judgement on acuracy, but to what 
extent a specific NLP aplication can benefit 
from employing a method. 
Another premise that this study seks to 
verify is whether orthographicaly close 
distractors, in adition to being semanticaly 
related, could yield even beter results. 
50
3 Production
of test items and 
selection of distractors 
Test items were constructed by a program 
based on the methodology described in the 
previous section. We ran the program on an 
on-line course materials in linguistics (Vajda, 
2001). A total of 14 items were initialy 
generated. 31 out of these 14 items were kept 
for further considerations as they either did not 
ned any or, only minor revision. The 
remaining 13 items were demed to require 
major post-editing revision. The 31 items kept 
for consideration were further revised by a 
second linguist and finaly, we narowed down 
the selection to 20 questions for the 
experiments
4
. These 20 questions gave a rise 
to a total of eight diferent asesments. Each 
asesment had the same 20 questions but they 
difered in the sets of distractors as these were 
chosen using diferent similarity measures
5
 
(sections 3.1-3.5). 
To generate a list of distractors for single-
word terms the function cordinate terms in 
WordNet is employed. For multi-word terms, 
noun phrases with the same head as the corect 
answers apearing in the source text as wel as 
entry terms from Wikipedia having the same 
head with the corect answers, are used to 
compile the list of distractors. This list of 
distractors is ofered to the user from which he 
or she could chose his/her prefered 
distractors. 
In this study we explore which is the best 
way to narow down the distractors to the 4 
most suitable ones. To this end, the folowing 
strategies for computing semantic (and in one 
case, phonetic) similarity were employed: (i) 
colocation paterns, (i-v) four diferent 
methods of WordNet-based semantic 
similarity (Extended glos overlap measure, 
Leacock and Chodorow’s, Jiang and Conrath’s 
and Lin’s measures), (vi) Distributional 
Similarity, and (vi) Phonetic similarity. 
                                                             
4
 The folowing is an example of an item generated of 
the program and then post-edited. 
"Which type of clause might contain verb and 
dependent words? i) verb clause i) adverb clause ii) 
adverbial clause 
  iv) multiple subordinate clause v) subordinate clause". 
5
 It should be noted that there were cases where the 
diferent selection/similarity strategies picked the same 
distractors. 
3.1 Colocation
paterns 
The colocation extraction strategy used in this 
experiment is based on the method reported in 
(Mitkov and Ha, 203). Distractors that apear 
in the source text are given preference. If there 
are not enough distractors, distractors are 
selected randomly from the list. 
For the other methods described below 
(sections 3.2-3.5), instead of giving preference 
to noun phrases apearing in the same text, 
and randomly pick the rest from the list, we 
ranked the distractors in the list based on the 
similarity scores betwen each distractor and 
the corect answer and chose the top 4 
distractors. 
We compute similarity for words rather than 
multi-word terms. When the corect answers 
and distractors are multi-word terms, we 
calculate the similarities betwen their 
modifier words. By way of example, in the 
case of "verb clause" and "adverbial clause", 
the similarity score betwen "verb" and 
"adverbial" is computed. When the corect 
answer or distractor contains more than one 
modifiers we compute the similarity for each 
modifier pairs and we chose the maximum 
score. (e.g. for "verb clause" and "multiple 
subordinate clause", similarity scores of "verb" 
and "multiple" and of "verb" and 
"subordinate" are calculated, the higher one is 
considered to represent the similarity score). 
3.2 Four
diferent methods for WordNet-
based similarity 
For computing WordNet-based semantic 
similarity we employed the package made 
available by Ted Pedersen
6
. Pedersen’s tol 
computes (i) extended glos overlap measure 
(Banerje and Pedersen, 203), (i) Leacock 
and Chodorow’s (198) measure, (ii) Jiang 
and Conrath’s (197) measure and (iv) Lin’s 
(197) measure. 
The extended glos overlap measure 
calculates the overlaps betwen not only the 
definitions of the two concepts measured but 
also among those concepts to which they are 
related. The relatednes score is the sum of the 
squares of the overlap lengths. 
Leacock and Chodorow’s measure uses the 
normalised path length betwen the two 
concepts c
1
 and c
2
 and is computed as folows: 
                                                             
6
 htp:/search.cpan.org/~tpederse/WordNet-Similarity 
51
 
(1) 
where len is the number of edges on the 
shortest path in the taxonomy betwen the two 
concepts and MAX is the depth of the 
taxonomy. 
Jiang and Conrath’s measure compares the 
sum of the information content of the 
individual concepts with that of their lowest 
comon subsumer: 
 
(2) 
where IC(c) is the information content 
(Patwardhan et al., 203) of the concept c, and 
lcs denotes the lowest comon subsumer, 
which represents the most specific concept 
that the two concepts have in comon. 
The Lin measure scales the information 
content of lowest comon subsumer with the 
sum of information content of two concepts. 
 
 
(3) 
 
3.3 Distributional
similarity 
For computing distributional similarity we 
made use of Viktor Pekar's implementation
7
 
based on Information Radius, which acording 
to a comparative study by Dagan et al. (197) 
performs consistently beter than the other 
similar measures. Information Radius (or 
Jensen-Shanon divergence) is a variant of 
Kulback-Leiber divergence measuring 
similarity betwen two words as the amount of 
information contained in the diference 
betwen the two coresponding co-ocurence 
vectors. Every word w
j
 is presented by the set 
of words w
i1..n 
with which it co-ocurs. The 
semantics of w
j
 are modeled as a vector in an 
n-dimensional space where n is the number of 
words co-ocuring with w
j, and the features of 
the vector are the probabilities of the co-
ocurences established from their observed 
frequencies, as in (4). In Pekar’s 
implementation, if one word is identified as 
dependent on another word by a dependency 
                                                             
7
 htp:/clg.wlv.ac.uk/demos/similarity/index.html 
parser, these two words are said to be “co-
ocuring”
8
. The corpus used to colect the co-
ocurance vector was the BNC and the 
dependency parsed used the FDG parser 
(Tapanainen and Järvinen, 197). The 
Information Radius (JS) is calculated using 
(5). 
 
(4) 
 
(5) 
where  
3.4 Phonetic
similarity 
For measuring phonetic similarity we use 
Soundex, phonetic algorithm for indexing 
words by sound. It operates on the principle of 
term based evaluation where each term is 
given a Soundex code. Each Soundex code 
itself consists of a leter and thre numbers 
betwen 0 and 6. By way of example the 
Soundex code of verb is V610 (the first 
character in the code is always the first leter 
of the word encoded). Vowels are not used and 
digits are based on the consonants as ilustrate 
by the folowing table: 
 
1. B, P, F, V 
2. C, S, K, G, J, Q, X, Z 
3. D, T 
4. L 
5. M, N 
6. R 
Table 1 Digits based on consonants 
First the Soundex code for each word is 
generated
9
. Then similarity is computed using 
the Diference method, returning an integer 
result ranging in value from 1 (least similar) to 
4 (most similar). 
3.5 Mixed
Strategy 
After items have ben generated by the above 
seven methods, we pick thre items from each 
method, except from Soundex, where only two 
items have ben picked, to compose an 
                                                             
8
 There are many other ways to construct the co-
ocurence vectors. This paper does not intend to exploit 
these diferent ways. 
9
 We adopt the phonetic representation used in MS SQL 
Server. As ilustrated above, each soundex code consists 
of a leter and thre numbers, such as A252. 
52
asesment of 20 items. This asesment is 
caled “mixed”, and used to ases whether or 
not an asesment with distractors generated 
by combining diferent methods would 
produce a diferent result from an asesment 
featuring distractors generated by a single 
method. 
4 In-class experiments, evaluation, 
results and discussion 
The tests (papers) generated with the help of 
our program with the distractors chosen 
acording the diferent methods described 
above, were taken by a total of 243 students 
from diferent European universities: 
University of Wolverhampton (United 
Kingdom), University Colege Ghent 
(Belgium), University of Sarbrücken 
(Germany), University of Cordoba (Spain), 
University of Sofia (Bulgaria). A prerequisite 
for the students taking the test was that they 
studied language and linguistics and that they 
had a god comand of English. Each test 
paper consisted of 20 questions and the 
students had 30 minutes to reply to the 
questions. The tests were ofered through the 
Questionmark Perception web-based testing 
software which in adition to providing a user-
friendly interface, computes diverse statistics 
related to the test questions answered. 
In order to evaluate the quality of the 
multiple-choice test items generated by the 
program (and subsequently post-edited by 
humans), we employed standard item analysis. 
Item analysis is an important procedure in 
clasical test theory which provides 
information as to how wel each item has 
functioned. The item analysis for multiple-
choice tests usualy consists of the folowing 
information (Gronlund, 1982): (i) the 
dificulty of the item, (i) the discriminating 
power and (ii) the usefulnes
10
 of each 
distractor. This information can tel us if a 
specific test item was to easy or to hard, 
how wel it discriminated betwen high and 
low scorers on the test and whether al of the 
alternatives functioned as intended. Such types 
of analysis help improve test items or discard 
defective items. 
                                                             
10
 Originaly caled ‘efectivenes’. We chose to term 
this type of analysis ‘usefulnes’ to distinguish it from 
the (cost/time) ‘efectivenes’ of the semi-automatic 
procedure as oposed to the manual construction of 
tests. 
Whilst this study focuses on the quality of 
the distractors generated, we believe that the 
distractors are esential for the quality of the 
overal test and hence the dificulty of an item 
and its discriminating power are demed 
apropriate to ases the quality of distractors, 
even though the quality of the test stem also 
pays in important part. On the other hand 
usefulnes is a completely independent 
measure as it loks at distractors only and not 
only the combination of stems and distractors. 
In order to conduct this type of analysis, we 
used a simplified procedure, described in 
(Gronlund, 1982). We aranged the test papers 
in order from the highest score to the lowest 
score. We selected one third of the papers and 
caled this the uper group. We also selected 
the same number of papers with the lowest 
scores and caled this the lower group. For 
each item, we counted the number of students 
in the uper group who selected each 
alternative; we made the same count for the 
lower group. 
(i) Item Dificulty 
We estimated the Item Dificulty (ID) by 
establishing the ratio of students from the two 
groups who answered the item corectly (ID = 
C/T, where C is the number who answered the 
item corectly and T is the total number of 
students who atempted the item). As Table 2 
shows, from the items featuring distractors 
generated using the colocation method
11, there 
were 4 to easy and 0 to dificult items.
12
 The 
average Item Dificulty was 0.61. From the 
items with distractors generated using 
WordNet-based similarity
13, the results were 
the folowing. When employing the extended 
glos overlap measure there were 2 to easy 
and 0 to dificult items, showing an average 
ID of 0.58. Leacock and Chodorow’s measure 
produced 1 to easy and 3 to dificult items 
with item average dificulty of 0.54. The use 
of Jiang and Conrath’s measure resulted in 3 
to easy and 1 to dificult items; the average 
item dificulty observed was 0.57. Lin’s 
measure delivered the best results from the 
                                                             
11
 Henceforth refered to as ‘colocation items’; the 
distractors generated are refered to as ‘colocation 
distractors’. 
12
 For experimental purposes, we consider an item to be 
‘to dificult’ if ID ≤ 0.15 and an item ‘to easy’ if ID ≥ 
0.85. 
13
 Henceforth refered to as ‘WordNet items’; the 
distractors are refered to as ‘WordNet distractors’. 
53
point of item dificulty with an almost ideal 
average item dificulty of 0.51 (the 
recomended item dificult is 0.5; se also 
fotnote 16); there were 2 to easy and 1 to 
dificult items. 
The items constructed on the basis of 
distractors selected via the distributional 
similarity metric
14, scored an average ID of 
0.64 with 6 items being to easy and 1 ― to 
dificult.  From the items with distractors 
produced using the phonetic similarity 
algorithm
15, there were 4 to easy and 0 to 
dificult questions with overal average 
dificult of 0.60. Finaly, a mixed strategy 
produced test items with average dificulty of 
0.53, 1 of them being to easy and 0 ― to 
dificult. 
The results showed that almost al items 
produced after selecting distractors using the 
strategies described above, featured very 
reasonable ID values. In many cases the 
average values were close to the recomended 
ID value of 0.5 with Lin’s measure delivering 
the best ID of 0.51. Runers-up are the mixed 
strategy delivering items with average ID 0.53 
Leacock and Chodorow’s measure 
contributing to the generation of items with 
average ID of 0.54. 
(i) Discriminating Power 
We estimated the item's Discriminating Power 
(DP) by comparing the number students in the 
uper and lower groups who answered the 
item corectly. It is desirable that the 
discrimination is positive which means that the 
item diferentiates betwen students in the 
same way that the total test score does.
16
 The 
formula for computing the Discriminating 
Power is as folows: DP = (C
U
 – C
L
) : T/2, 
where C
U
 is the number of students in the 
uper group who answered the item corectly 
and  C
L
 the number of the students in the 
lower group that did so. Here again T is the 
                                                             
14
 Henceforth refered to as ‘distributional items’; the 
distractors are refered to as ‘distributional distractors’. 
15
 Henceforth refered to as ‘phonetic items’; the 
distractors are refered to as ‘phonetic distractors’. 
16
 Zero DP is obtained when an equal number of 
students in each group respond to the item corectly. On 
the other hand, negative DP is obtained when more 
students in the lower group than the uper group answer 
corectly. Items with zero or negative DP should be 
either discarded or improved. 
total number of students included in the item 
analysis.
17
 
The average Discriminating Power for the 
colocation items was 0.3 and there were no 
negative discriminating colocation test 
items.
18
 The figures asociated to the WordNet 
items were as folows. The average DP for 
items produced with the extended glos 
overlap measure was 0.32, and there were 2 
items with negative discrimination. Leacock 
and Chodorow’s measure did not produce any 
items with negative discrimination and the 
average DP of these was 0.38. Jiang and 
Conrath’s measure gave rise to 2 negatively 
discriminating items and the average DP of the 
items based on this measure was 0.29. The 
selection of distractors with Lin’s measure 
resulted in items with average DP of 0.37; 
none of them had a negative discrimination. 
The average discrimination power for the 
distributional items was 0.29 (1 item with 
negative discrimination) and for phonetic 
items – 0.34 (0 item with negative 
discrimination). The employment of mixed 
strategy when selecting distractors which 
resulted in items with average DP of 0.39 (0 
items with negative discrimination). 
The figures related to the Discriminating 
Power of the items generated showed that 
whereas the DP was not of the desired high 
level, as a whole the proportion of items with 
negative discrimination was fairly low (Table 
2). The items did not difer substantialy in 
terms of the values of DP, the top performer 
being the items where the distractors were 
selected on the basis of the mixed strategy, 
folowed by those selected by Leacock and 
Chodorow’s measure and phonetic similarity. 
(ii) Usefulnes of the distractors 
 The usefulnes of the distractors is estimated 
by comparing the number of students in the 
uper and lower groups who selected each 
incorect alternative. A god distractor should 
atract more students from the lower group 
than the uper group. 
The evaluation of the distractors estimated 
the average diference betwen students in the 
                                                             
17
 Maximum positive DP is obtained only when al 
students in the uper group answer corectly and no one 
in the lower group does. An item that has a maximum 
DP (1.0) would have an ID 0.5; therefore, test authors 
are advised to construct items at the 0.5 level of 
dificulty. 
18
 Obviously a negative discriminating test item is not 
regarded as a god one. 
54
lower and uper groups to be 0.74 for the sets 
of distractors generated using colocations. 
For the WordNet distractors the results were as 
folows. The average distance betwen the 
students in the lower and uper groups was 
found to be 0.71 for the extended glos overlap 
distractors, 0.76 for the Leacock and 
Chodorow distractors, 0.71 for the Jiang and 
Conrath distractors and 0.83 for the Lin 
distractors. For the distractors selected by way 
of distributional similarity the average 
diference betwen students in the lower and 
uper groups was 0.79, for the phonetic 
distractors ― 0.6 and for those selected by a 
mixed strategy ― 0.89. 
In our evaluation we also used the notions 
of por distractors as wel as not-useful 
distractors. Distractors are clased as poor if 
they atract more students from the uper 
group than from the lower group. There were 2 
(2.5%) por distractors from the colocation 
distractors. The WordNet distractors fared as 
folows with regard to the number of por 
distractors. There were altogether 9 (1%) 
por distractors from the extended glos 
overlap distractors, 9 (11%) from the Leacock 
and Chodorow distractors, 10 (12%) from the 
Jiang and Conrath distractors and 10 (12%) 
from the Lin ones. There were 6 (7.5%) from 
the distributional similarity which were 
clased as por, 5 (6%) from the phonetic 
similarity ones were clased as por and 5 
(6%) from the distractors selected through a 
mixed strategy were clased as such (Table 2). 
On the other hand, distractors are termed 
not useful if they are not selected by any 
students at al. The evaluation showed (se 
Table 2) that there were 24 (30%) distractors 
demed not useful from the colocation 
distractors. The figures for not useful 
distractors for those selected by way of 
WordNet similarity were as folows: 17 (21%) 
for extended glos overlap distractors, 20 
(25%) for the Leacock and Chodorow 
distractors, 19 (24%) for the Jiang and Conrath 
distractors and 16 (20%) for the Lin ones. 
From the distributional distractors, 27 (34%) 
emerged as not useful, whereas 31 (39%) 
phonetic similarity and 14 (18%) mixed 
strategy distractors were found not useful. 
The overal figures sugest that the ‘most 
useful’ distractors are those chosen with mixed 
strategy (highest average diference 0.89; 
lowest number of not useful distractors, 
second lowest number of por distractors), 
folowed by those chosen with Lin’s WordNet 
measure (second highest average distance of 
0.83; second lowest number of not useful 
distractors). 
Sumarising the results of the item 
analysis, it is clear that there is not a method 
that outperforms the rest in terms of producing 
best quality items or distractors. At the same 
time it is also clear that in general the mixed 
strategy and Lin’s measure consistently 
perform beter than the rest of 
methods/measures. Phonetic similarity did not 
deliver as expected. 
 
Item Dificulty Item Discriminating Power Usefulnes of distractors 
 
average 
item 
dificulty 
to 
easy 
to 
dificult 
average 
discriminating 
power 
negative 
discriminating 
power 
poor 
not 
useful 
average 
diference 
Colocation items 0.61 4 0 0.3 0 2 24 0.74 
WordNet items 
Extended glos overlap 
Leacock and Chodorow 
Jiang and Conrath 
Lin 
 
0.58 
0.54 
0.57 
0.51 
 
2 
1 
3 
2 
 
0 
3 
1 
1 
 
0.32 
0.38 
0.29 
0.37 
 
2 
0 
2 
0 
 
9 
9 
10 
10 
 
17 
20 
19 
16 
 
0.71 
0.76 
0.71 
0.83 
Distributional items 0.64 6 1 0.29 1 6 27 0.79 
Phonetic items 0.60 4 0 0.34 0 5 31 0.6 
Mixed strategy items 0.53 1 0 0.39 0 5 14 0.89 
Table 2: Item analysis 
55
Although the results indicate that the Lin 
items have the best average item dificulty, 
none of the diference (betwen item dificulty 
of Lin and other methods, or betwen any pair 
of methods) is statisticaly significant. From 
the DP point of view, only the diference 
betwen mixed strategy (0.39) and 
distributional items (0.29) is statisticaly 
significant (p<0.05). For the distractor 
usefulnes measure, none of the diference is 
statisticaly significant (p<0.05). 
5 Conclusion

In this study we conducted extrinsic evaluation 
of several similarity methods (colocation 
paterns; four diferent methods of WordNet-
based semantic similarity: extended glos 
overlap measure, Leacock and Chodorow’s, 
Jiang and Conrath’s as wel as Lin’s measures; 
distributional similarity; phonetic similarity; 
mixed strategy) by seking to establish which 
one would be most suitable for the task of 
selection of distractors in multiple-choice 
tests. The evaluation results based on item 
analysis sugests that whereas there is not a 
method that clearly outperforms in terms of 
delivering beter quality distractors, mixed 
strategy and Lin’s measure consistently 
perform beter than the rest of 
methods/measures. However, these two 
methods do not ofer any statisticaly 
significant improvement over their closest 
competitors. 
Acknowledgments 
We would like to expres our gratitude to 
Kathelijne Denturck, Johan Haler, 
Veronique Hoste, Constantin Orasan, Miriam 
Seghiri, Andrea Stockero and Irina Temnikova 
for helping us in the organisation of the in-
clas experiments. 
References 
Banerje, S. and Perderson, T. 203. Extended 
glos overlaps as a measure of semantic 
relatednes. Procedings of the Eightenth 
International Joint Conference on Artificial 
Inteligence, 805-810. 
Dagan I., Le L., and Pereira F. 197. Similarity-
based methods for word sense disambiguation. 
Procedings of the 35th Anual Meting of 
the Asociation for Computational 
Linguistics. Madrid, Spain, 56-63. 
Gronlund, N. 1982. Constructing achievement 
tests. New York: Prentice-Hal Inc. 
Jiang, J. and Conrath, D. 197. Semantic similarity 
based on corpus statistics and lexical taxonomy. 
Procedings of the International 
Conference on Research in Computational 
Linguistics. Taiwan, 19-33. 
Lin, D. 197. Using syntactic dependency as a 
local context to resolve word sense ambiguity. 
Procedings of the 35th Anual Meting of 
the Asociation for Computational 
Linguistics. Madrid, Spain, 4-71. 
Leacock, C., Chodorow, M. 198. Combining local 
context and WordNet similarity for word sense 
identification. In: Felbaum, C., 198, 
WordNet: An Electronic Lexical Database. 
MIT Pres, Cambridge, MA, 265–283. 
Mitkov R. and Ha L.A. 203. Computer-aided 
generation of multiple-choice tests. 
Procedings of the HLT/NACL 203 
Workshop on Building educational 
aplications using Natural Language 
Procesing. Edmonton, Canada, 17-22. 
Mitkov, R., An, L.A. and Karamanis, N. 206. "A 
computer-aided environment for generating 
multiple-choice test items". Journal of Natural 
Language Enginering, 12 (2): 17-194. 
Patwardhan, S, Banerje, S. and Pedersen, T. 203. 
Using Measures of Semantic Relatednes for Word 
Sense Disambiguation. Procedings of the Third 
International Conference on Inteligent Text 
Procesing and Computational Linguistics. 
Mexico City, 241-257. 
Resnik, P. 195. Using information content to 
evaluate semantic similarity in a taxonomy. 
Procedings of the 14th International Joint 
Conference on Artificial Inteligence. 
Montreal, 48–453. 
Tapanainen, P. and Järvinen, T. 1997. A non-
projective dependency parser. Procedings of 
the 5th Conference of Aplied Natural 
Language Procesing, Washington, 64-71. 
Vajda, E.J. 2001 Course Materials from the module 
of Introduction to Linguistics. Profesor Edward 
J. Vajda Homepage, Washington, Western 
Washington University, Modern and Clasical 
Languages.htp:/pandora.ci.wu.edu/vajda/ling
201/ling201home.htm
 
56

