<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>R Miller</author>
</authors>
<title>Just-in-time language modelling</title>
<date>1998</date>
<booktitle>In Proceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP</booktitle>
<volume>2</volume>
<pages>705--708</pages>
<marker>Berger, Miller, 1998</marker>
<rawString>A. Berger and R. Miller. 1998. Just-in-time language modelling. In Proceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP), volume 2, pages 705 708.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Bulyko</author>
<author>M Ostendorf</author>
<author>M Siu</author>
<author>T Ng</author>
<author>A Stolcke</author>
<author>currency1Ozgcurrency1ur C‚ etin</author>
</authors>
<title>Web resources for language modeling in conversational speech recognition</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing</journal>
<volume>5</volume>
<pages>25</pages>
<contexts>
<context> the topic at hand must be considered. 6.2. Filtering documents In many works, relevant texts are selected based on their accordance with topic-speci c LMs (Nisimura et al., 2001; Sethy et al., 2005; Bulyko et al., 2007) already available. However, in our application framework, no initial topic speci c LM is available. We therefore propose to make use of the tf idf scores to measure the similarity between documents </context>
</contexts>
<marker>Bulyko, Ostendorf, Siu, Ng, Stolcke, etin, 2007</marker>
<rawString>I. Bulyko, M. Ostendorf, M. Siu, T. Ng, A. Stolcke, and currency1Ozgcurrency1ur C‚ etin. 2007. Web resources for language modeling in conversational speech recognition. ACM Transactions on Speech and Language Processing, 5(1):1 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Callan</author>
<author>M Connell</author>
<author>A Du</author>
</authors>
<title>Automatic discovery of language models for text databases</title>
<date>1999</date>
<journal>SIGMOD Record</journal>
<volume>28</volume>
<pages>490</pages>
<contexts>
<context>h text collections. The problem here is to sample a subset of a text collection using queries in order to build a LM as close as possible to the one that would be obtained from the entire collection (Callan et al., 1999; Monroe et al., 2002). Though not directly related to the problem of LM adaptation, these works interestingly raise questions about the best way to build queries and sample documents which best repre</context>
<context>00 hits with 5 keywords. Thus, in practice, the set of keywords selected is limited to the ve words used to represent the ve lemmas with the highest scores (‘). As in query-based sampling techniques (Callan et al., 1999; Monroe et al., 2002), various simple queries are formed based on subsets on the selected keywords. For example, one query is composed of the two best keywords while another one is composed of the rs</context>
<context>ere 200 pages, about 800,000 words, seem like a good compromise between speed and accuracy. If the number of documents considered is close to what is reported in the information retrieval literature (Callan et al., 1999; Monroe et al., 2002), it is more unusual within the ASR domain. For example, in Suzuki et al. (2006), several thousands Web pages are retrieved. However, even if broader experiments should be carrie</context>
</contexts>
<marker>Callan, Connell, Du, 1999</marker>
<rawString>J. Callan, M. Connell, and A. Du. 1999. Automatic discovery of language models for text databases. SIGMOD Record, 28(2):479 490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Chen</author>
<author>J-L Gauvain</author>
<author>L Lamel</author>
<author>G Adda</author>
</authors>
<date>2004</date>
<contexts>
<context>ion, most current works aim at reestimating n-gram probabilities from topic speci c texts selected automatically. In several works, texts are selected among static collections of texts (Klakow, 2000; Chen et al., 2004). However, since these collections are limited, there are always topics for which no, or few, adaptation data can be found. These methods are therefore rather dedicated to cases where all the topics </context>
</contexts>
<marker>Chen, Gauvain, Lamel, Adda, 2004</marker>
<rawString>L. Chen, J.-L. Gauvain, L. Lamel, and G. Adda. 2004.</rawString>
</citation>
<citation valid="true">
<title>Dynamic language modeling for broadcast news</title>
<booktitle>In Proceedings of International Conference on Speech and Language Processing (ICSLP</booktitle>
<pages>1281--1284</pages>
<marker></marker>
<rawString>Dynamic language modeling for broadcast news. In Proceedings of International Conference on Speech and Language Processing (ICSLP), pages 1281 1284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Daille</author>
</authors>
<title>Conceptual structuring through term variations</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>9--16</pages>
<contexts>
<context>in a segment dealing with the 70’s, the two words ower and power describe much more the topic when considered together than when considered separately. To do this, complex term extraction techniques (Daille, 2003) could be adapted in order to handle automatic transcriptions. On the other hand, despite the fact that words are grouped into word classes, the length and the style of some segments still imply only</context>
</contexts>
<marker>Daille, 2003</marker>
<rawString>B. Daille. 2003. Conceptual structuring through term variations. In Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment, pages 9 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Federico</author>
</authors>
<title>Ef cient language model adaptation through MDI estimation</title>
<date>1999</date>
<booktitle>In Proceedings of Eurospeech</booktitle>
<volume>4</volume>
<pages>1583--1586</pages>
<contexts>
<context>containing ordinary words. Even if using larger corpora may solve the problem, we rather think that a different adaptation method should be used instead of linear interpolation, e.g., MDI adaptation (Federico, 1999). 8. Discussion In this paper, we have demonstrated that Web resources and natural language processing techniques can be successfully used to improve an ASR system by adapting the language model. We </context>
</contexts>
<marker>Federico, 1999</marker>
<rawString>M. Federico. 1999. Ef cient language model adaptation through MDI estimation. In Proceedings of Eurospeech, volume 4, pages 1583 1586.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Galliano</author>
<author>E Geoffrois</author>
<author>D Mostefa</author>
<author>K Choukri</author>
<author>J-F</author>
</authors>
<marker>Galliano, Geoffrois, Mostefa, Choukri, J-F, </marker>
<rawString>S. Galliano, E. Geoffrois, D. Mostefa, K. Choukri, J.-F.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonastre</author>
<author>G Gravier</author>
</authors>
<title>The ESTER phase II evaluation campaign for the rich transcription of French broadcast news</title>
<date>2005</date>
<booktitle>In Proceedings of Eurospeech</booktitle>
<pages>1149--1152</pages>
<marker>Bonastre, Gravier, 2005</marker>
<rawString>Bonastre, and G. Gravier. 2005. The ESTER phase II evaluation campaign for the rich transcription of French broadcast news. In Proceedings of Eurospeech, pages 1149 1152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Corpus-derived rst, second and third-order word af nities</title>
<date>1994</date>
<booktitle>In Proceedings of EURALEX</booktitle>
<pages>279--290</pages>
<contexts>
<context>rouped into word classes, the length and the style of some segments still imply only a few repetitions. This problem could be overcome with the help of information about semantic links between words (Grefenstette, 1994). Finally, a deeper and better use of the thematic corpora is necessary. Given the sparseness of general-purpose ngrams in the adaptation corpora, it would be interesting to attach different importan</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>G. Grefenstette. 1994. Corpus-derived  rst, second and third-order word af nities. In Proceedings of EURALEX, pages 279 290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Huet</author>
<author>G Gravier</author>
<author>P S·ebillot</author>
</authors>
<title>Morphosyntactic processing of N-best lists for improved recognition and con dence measure computation</title>
<date>2007</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<pages>1741--1744</pages>
<marker>Huet, Gravier, S·ebillot, 2007</marker>
<rawString>S. Huet, G. Gravier, and P. S·ebillot. 2007. Morphosyntactic processing of N-best lists for improved recognition and con dence measure computation. In Proceedings of Interspeech, pages 1741 1744.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klakow</author>
</authors>
<title>Selecting articles from the language model training corpus</title>
<date>2000</date>
<booktitle>In Proceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP</booktitle>
<volume>3</volume>
<pages>1695--1698</pages>
<contexts>
<context>peech recognition, most current works aim at reestimating n-gram probabilities from topic speci c texts selected automatically. In several works, texts are selected among static collections of texts (Klakow, 2000; Chen et al., 2004). However, since these collections are limited, there are always topics for which no, or few, adaptation data can be found. These methods are therefore rather dedicated to cases wh</context>
</contexts>
<marker>Klakow, 2000</marker>
<rawString>D. Klakow. 2000. Selecting articles from the language model training corpus. In Proceedings of International Conference on Acoustics, Speech, and Signal Processing (ICASSP), volume 3, pages 1695 1698.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Monroe</author>
<author>J French</author>
<author>A Powell</author>
</authors>
<title>Obtaining language models of Web collections using query-based sampling techniques</title>
<date>2002</date>
<booktitle>In Proceedings of Hawaii International Conference on System Sciences (HICSS</booktitle>
<volume>3</volume>
<contexts>
<context>he problem here is to sample a subset of a text collection using queries in order to build a LM as close as possible to the one that would be obtained from the entire collection (Callan et al., 1999; Monroe et al., 2002). Though not directly related to the problem of LM adaptation, these works interestingly raise questions about the best way to build queries and sample documents which best represent a topic or a dom</context>
<context>ds. Thus, in practice, the set of keywords selected is limited to the ve words used to represent the ve lemmas with the highest scores (‘). As in query-based sampling techniques (Callan et al., 1999; Monroe et al., 2002), various simple queries are formed based on subsets on the selected keywords. For example, one query is composed of the two best keywords while another one is composed of the rst and third keywords.</context>
<context>800,000 words, seem like a good compromise between speed and accuracy. If the number of documents considered is close to what is reported in the information retrieval literature (Callan et al., 1999; Monroe et al., 2002), it is more unusual within the ASR domain. For example, in Suzuki et al. (2006), several thousands Web pages are retrieved. However, even if broader experiments should be carried out with larger num</context>
</contexts>
<marker>Monroe, French, Powell, 2002</marker>
<rawString>G. Monroe, J. French, and A. Powell. 2002. Obtaining language models of Web collections using query-based sampling techniques. In Proceedings of Hawaii International Conference on System Sciences (HICSS), volume 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nisimura</author>
<author>K Komatsu</author>
<author>Y Kuroda</author>
<author>K Nagatomo</author>
<author>A Lee</author>
<author>H Saruwatari</author>
<author>K Shikano</author>
</authors>
<title>Automatic n-gram language model creation from Web resources</title>
<date>2001</date>
<booktitle>In Proceedings of Eurospeech</booktitle>
<pages>2127--2130</pages>
<contexts>
<context>d the quality of a document with respect to the topic at hand must be considered. 6.2. Filtering documents In many works, relevant texts are selected based on their accordance with topic-speci c LMs (Nisimura et al., 2001; Sethy et al., 2005; Bulyko et al., 2007) already available. However, in our application framework, no initial topic speci c LM is available. We therefore propose to make use of the tf idf scores to </context>
</contexts>
<marker>Nisimura, Komatsu, Kuroda, Nagatomo, Lee, Saruwatari, Shikano, 2001</marker>
<rawString>R. Nisimura, K. Komatsu, Y. Kuroda, K. Nagatomo, A. Lee, H. Saruwatari, and K. Shikano. 2001. Automatic n-gram language model creation from Web resources. In Proceedings of Eurospeech, pages 2127 2130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic text processing: the transformation, analysis, and retrieval of information by computer</title>
<date>1989</date>
<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
<contexts>
<context> the topic of a segment in order to retrieve texts from the same topic on the Internet. To do this, our idea is to extract keywords based on the tf idf criterion widely used in information retrieval (Salton, 1989). However, this criterion has been designed for regular texts rather than for automatic transcriptions, the latter containing misrecognized words, being case insensitive and lacking punctuation. The </context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>G. Salton. 1989. Automatic text processing: the transformation, analysis, and retrieval of information by computer. Addison-Wesley Longman Publishing Co., Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sethy</author>
<author>P G Georgiou</author>
<author>S Narayanan</author>
</authors>
<title>Building topic speci c language models from Webdata using competitive models</title>
<date>2005</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<pages>1293--1296</pages>
<contexts>
<context>ment with respect to the topic at hand must be considered. 6.2. Filtering documents In many works, relevant texts are selected based on their accordance with topic-speci c LMs (Nisimura et al., 2001; Sethy et al., 2005; Bulyko et al., 2007) already available. However, in our application framework, no initial topic speci c LM is available. We therefore propose to make use of the tf idf scores to measure the similari</context>
</contexts>
<marker>Sethy, Georgiou, Narayanan, 2005</marker>
<rawString>A. Sethy, P. G. Georgiou, and S. Narayanan. 2005. Building topic speci c language models from Webdata using competitive models. In Proceedings of Interspeech, pages 1293 1296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Suzuki</author>
<author>Y Kajiura</author>
<author>A Ito</author>
<author>S Makino</author>
</authors>
<title>Unsupervised language model adaptation based on automatic text collection from WWW</title>
<date>2006</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<pages>2202--2205</pages>
<marker>Suzuki, Kajiura, Ito, Makino, 2006</marker>
<rawString>M. Suzuki, Y. Kajiura, A. Ito, and S. Makino. 2006. Unsupervised language model adaptation based on automatic text collection from WWW. In Proceedings of Interspeech, pages 2202 2205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vaufreydaz</author>
<author>M Akbar</author>
<author>J Rouillard</author>
</authors>
<title>Internet documents: A rich source for spoken language modeling</title>
<date>1999</date>
<booktitle>In Proceedings of Workshop on Automatic Speech Recognition and Understanding (ASRU</booktitle>
<pages>277--280</pages>
<contexts>
<context>s resource is all the more interesting to process speech as it has been shown that texts coming from the Internet are more suited for spoken language modeling than classical texts such as newspapers (Vaufreydaz et al., 1999). Among the recent Web-based approaches, Sethy et al. (2005) propose to collect topic-speci c texts from the Internet using both stochastic and information retrieval methods in order to update iterat</context>
</contexts>
<marker>Vaufreydaz, Akbar, Rouillard, 1999</marker>
<rawString>D. Vaufreydaz, M. Akbar, and J. Rouillard. 1999. Internet documents: A rich source for spoken language modeling. In Proceedings of Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 277 280.</rawString>
</citation>
</citationList>
</algorithm>

