Proceedings of the 6th Workshop on Statistical Machine Translation, pages 1–11,
Edinburgh, Scotland, UK, July 30–31, 2011. c©2011 Association for Computational Linguistics
A Grainof Saltfor the WMTManualEvaluation∗
Ondˇrej Bojar, Miloˇs Ercegovˇcevi´c, MartinPopel
CharlesUniversityin Prague
Facultyof MathematicsandPhysics
Instituteof FormalandAppliedLinguistics
{bojar,popel}@ufal.mff.cuni.cz
ercegovcevic@hotmail.com
OmarF. Zaidan
Departmentof ComputerScience
JohnsHopkinsUniversity
ozaidan@cs.jhu.edu
Abstract
The Workshop on Statistical Machine
Translation (WMT) has become one of
ACL’s flagshipworkshops,held annually
since 2006. In additionto solicitingpa-
pers from the researchcommunity, WMT
also featuresa sharedtranslationtask for
evaluatingMT systems. This shared task
is notablefor havingmanualevaluationas
its cornerstone.TheWorkshop’s overview
paper, playinga descriptive and adminis-
trative role,reportsthe mainresultsof the
evaluationwithoutdelvingdeep into ana-
lyzingthoseresults.Theaim of this paper
is to investigate andexplainsomeinterest-
ing idiosyncrasiesin the reportedresults,
which only become apparent when per-
forminga more thoroughanalysis of the
collectedannotations.Our analysissheds
some light on how the reported results
should(andshouldnot)beinterpreted,and
alsogives riseto somehelpfulrecommen-
dationfor the organizersof WMT.
1 Introduction
The Workshopon StatisticalMachineTranslation
(WMT) has become an annual feast for MT re-
searchers.Of particularinterestis WMT’s shared
translationtask, featuringa componentfor man-
ual evaluationof MT systems.The friendlycom-
petitionis a sourceof inspirationfor participating
teams, and the yearly overview paper (Callison-
Burchet al.,2010)providesa concisereportof the
state of the art. However, the amountof interest-
ing data collectedevery year (the systemoutputs
∗ This work has beensupportedby the grantsEuroMa-
trixPlus(FP7-ICT-2007-3-231720of the EUand7E09003of
the CzechRepublic),P406/10/P259,MSM0021620838,and
DARPA GALE programunder ContractNo.HR0011-06-2-
0001. We are gratefulto our students,colleagues,and the
threereviewersfor variousobservationsandsuggestions.
and, most importantly, the annotatorjudgments)
is quitelarge, exceedingwhatthe WMToverview
papercan affordto analyzewithmuchdepth.
In this paper, we take a closerlook at the data
collectedin last year’s workshop,WMT101, and
delve a bit deeperintoanalyzingthe manualjudg-
ments. We focusmainlyon the English-to-Czech
task, as it includeda diverse portfolioof MT sys-
tems,was a heavilyjudgedlanguagepair, andalso
illustratesinteresting “contradictions”in the re-
sults. We try to explain such points of interest,
andanalyzewhatwebelieve to be thepositive and
negative aspectsof the currentlyestablishedeval-
uationprocedureof WMT.
Section2 examinesthe primarystyle of man-
ual evaluation: systemranking. We discusshow
theinterpretationof collectedjudgments,thecom-
putation of annotator agreement,and document
thatannotators’individualpreferencesmayrender
two systemseffectively incomparable.Section3
is devoted to the impactof embeddingreference
translations,whileSection4 andSection5 discuss
some idiosyncrasiesof other WMT shared tasks
andmanualevaluationin general.
2 TheSystemRankingTask
At the core of the WMTmanualevaluationis the
system ranking task. In this task, the annotator
is presentedwith a source sentence,a reference
translation,and the outputs of five systemsover
that source sentence. The instructionsare kept
minimal: the annotatoris to rank the presented
translationsfrom best to worst. Ties are allowed,
but thescaleprovidesfive ranklabels,allowingthe
annotatorto give a totalorderif desired.
The five assignedrank labels are submittedat
once, makingthe 5-tuplea unit of annotation.In
the following, we will call this unit a block. The
blocksdiffer from each otherin the choiceof the
1http://www.statmt.org/wmt10
1
LanguagePair Systems Blocks Labels Comparisons Ref≥others Intra-annot.κ Inter-annot.κ
German-English 26 1,050 5,231 10,424 0.965 0.607 0.492
English-German 19 1,407 6,866 13,694 0.976 0.560 0.512
Spanish-English 15 1,140 5,665 11,307 0.989 0.693 0.508
English-Spanish 17 519 2,591 5,174 0.935 0.696 0.594
French-English 25 837 4,156 8,294 0.981 0.722 0.452
English-French 20 801 3,993 7,962 0.917 0.636 0.449
Czech-English 13 543 2,691 5,375 0.976 0.700 0.504
English-Czech 18 1,395 6,803 13,538 0.959 0.620 0.444
Average 19 962 4,750 9,471 0.962 0.654 0.494
Table 1: Statisticson the collectedrankings,qualityof referencesand kappasacross languagepairs. In
general,a blockyieldsa set of five rank labels,whichyieldsa set of parenleftbig52parenrightbig = 10 pairwisecomparisons.
Dueto occasionalomittedlabels,the Comparisons/Blocksratiois not exactly10.
sourcesentenceandthe choiceof the five systems
beingcompared.A coupleof tricksareintroduced
in the samplingof the source sentences, to en-
sure that a large enoughnumberof judgmentsis
repeatedacross different screens for meaningful
computationof interand intra-annotatoragree-
ment. As for the samplingof systems,it is done
uniformly– no effortis madeto oversampleor un-
dersamplea particularsystem (or a particularpair
of systemstogether)at any pointin time.
In termsof the interface, the evaluationutilizes
the infrastructureof Amazon’s Mechanical Turk
(MTurk)2, witheachMTurkHIT3 containingthree
blocks,correspondingto threeconsecutive source
sentences.
Table1 provides a briefcomparisonof the vari-
ous languagepairsin termsof numberof MTsys-
tems compared(includingthe reference),number
of blocks ranked, the number of pairwise com-
parisonsextractedfrom the rankings(one block
with 5 systemsranked gives 10 pairwisecompar-
isons, but occasionalunranked systems are ex-
cluded),the qualityof the reference (the percent-
ageof comparisonswherethereferencewas better
or equalthan anothersystem),and theκ statistic,
whichis a measureof agreement(see Section2.2
for moredetails).4
We seethatEnglish-to-Czech,thelanguagepair
on whichwe focus,is not far fromthe averagein
all those characteristicsexcept for the numberof
collectedcomparisons(andblocks),makingit the
secondmostevaluatedlanguagepair.
2http://www.mturk.com/
3“HIT”is an acronym for humanintelligencetask, which
is the MTurk term for a singlescreenpresentedto the anno-
tator.
4We onlyuse the “expert”annotationsof WMT10,ignor-
ing the data collectedfrompaid annotatorson MTurk, since
they werenot partof the officialevaluation.
2.1 Interpretingthe
RankLabels
Thedescriptionin the WMToverview papersays:
“Relative ranking is our official evaluation met-
ric. [Systems]areranked basedon how frequently
they were judged to be better than or equal to
anyothersystem.” (Emphasisadded.)TheWMT
overview paper refers to this measureas “≥oth-
ers”,witha variantofit called“>others”thatdoes
not reward ties.
We first note that this description is somewhat
ambiguous,and an uninformedreader might in-
terpretit in one of two differentways. For some
systemA, eachblockin whichAappearsincludes
four implicit pairwise comparisons(against the
otherpresentedsystems).How is A’s scorecom-
putedfromthosecomparisons?
The correct interpretation is that A is re-
wardedonce for each of the four comparisonsin
whichAwins(or ties).5 In otherwords,A’s score
is the numberof pairwisecomparisonsin which
A wins (or ties), divided by the total numberof
pairwisecomparisonsinvolvingA. We will use
“≥others”(resp. “>others”)to referto thisinter-
pretation,in keepingwith the terminologyof the
overview paper.
Theotherinterpretationis thatAis rewarded
only if A wins (or ties) all four comparisons.In
otherwords,A’s scoreis the numberof blocksin
whichAwins(orties)all comparisons,dividedby
thenumberof blocksin whichAappears.We will
use “≥all in block” (resp. “> all in block”)to
referto thisinterpretation.6
5PersonalcommunicationwithWMTorganizers.
6There is yet a third interpretation,due to a literal read-
ing of the description,whereAis rewardedat mostonceper
blockif it wins(orties)anyoneof its fourcomparisons.This
is probablyless useful: it might be good at identifyingthe
bottomtier of systems,but wouldfail to distinguishbetween
all othersystems.
2
REF
CU
-BO
JA
R
CU
-TE
CT
O
EU
RO
TR
AN
S
ON
LI
NE
B
PC
-TR
AN
S
UE
DI
N
≥others 95.9 65.6 60.1 54.0 70.4 62.1 62.2
>others 90.5 45.0 44.1 39.3 49.1 49.4 39.6
≥all in block 93.1 32.3 30.7 23.4 37.5 32.5 28.1
>all in block 81.3 13.6 19.0 13.3 15.6 18.7 10.6
Table 2: Sentence-level ranking scores for the
WMT10 English-Czechlanguagepair. The “≥
others” and “> others” scores reproducedhere
exactlymatchnumberspublishedin the WMT10
overview paper. A boldfacedscoremarks the best
systemin a given row (besidesthe reference).
For qualitycontrolpurposes,theWMTorganiz-
ers embedthe referencetranslationsas a ‘system’
alongsidethe actualentries(theideabeingthatan
annotatorclickingrandomlywouldbe easyto de-
tect, since they would not consistentlyrank the
reference‘system’highly). This means that the
referenceis as likely as any other system to ap-
pearin a block,andwhenthescorefor a systemA
is computed,pairwisecomparisonswiththe refer-
enceare included.
We usethepublicly releasedhumanjudgments7
to computethe scoresof systemsparticipatingin
the English-Czechsubtask,underboth interpreta-
tions. Table2 reportsthe scores,withour “≥oth-
ers” (resp. “>others”)scores reproducedexactly
matchingthosereportedin Table 21 of the WMT
overview paper. (Forclarity, Table2 is abbreviated
to includeonlythe top six systemsof twelve.)
Ourfirstsuggestionis thatbothmeasurescould
be reportedin futureevaluations,since each tells
us somethingdifferent. The first interpretation
givespartialcreditforanMTsystem,hencedistin-
guishingsystemsfrom each otherat a finer level.
This is especially importantfor a languagepair
with relatively few annotations,since “≥others”
wouldproducea largernumberofdatapoints(four
per systemper block)than “≥all in block”(one
per systemper block). Anotheradvantageof the
official “≥ others” is greater robustness towards
variousfactorslike the numberof systemsin the
competition,the numberof systemsin one block
or the presenceof the referencein the block(how-
ever, see Section3).
As for the secondinterpretation,it helps iden-
tify whether or not a single system (or a small
group of systems)is stronglydominantover the
other systems. For the systemslisted in Table 2,
7http://statmt.org/wmt10/results.html
-10 0 10 20 30 40 50 60  10
 20
 30
 40
 50
 60
 70
 80
>= All in Block
>= Others
Czech-English English-Czech English-French English-German English-Spanish French-English German-English Spanish-English
a*x+b
Figure1: “≥all in block”and “≥others”provide
very similarorderingof systems.
“> all in block”suggestsits potentialin the con-
text of systemcombination:CU-TECTO and PC-
TRANS winalmostonefifthof theblocksin which
they appear, despite the fact that either a refer-
ence translationor a combinationsystemalready
appearsalongsidethem.(SeealsoTable4 below.)
Also,notethatif therankingtaskweredesigned
specificallyto cater to the “≥all in block”inter-
pretation,it wouldonlyhave two ‘rank’labels(ba-
sically, “top” and “non-top”). In that case, an-
notatorswould spend considerably less time per
blockthan they do now, sinceall they need to do
is identifythetopsystem(s)perblock,withoutdis-
tinguishingnon-topsystemsfromeachother.
Even for thoseinterestedin distinguishingnon-
state-of-the-artsystemsfromeachother, we point
out that the “≥ all in block” interpretationulti-
matelygives a systemorderingthat is very simi-
lar to that of the official“≥others”interpretation,
even for the lower-tiersystems(Figure1).
2.2 AnnotatorAgreement
The WMT10 overview paper reports interand
intra-annotatoragreement over the pairwisecom-
parisons, to show the validity of the evaluation
setup and the “≥ others” metric. Agreementis
quantifiedusingthe followingformula:
κ= P(A)−P(E)1−P(E) (1)
whereP(A) is the proportionof timestwo anno-
tatorsare observed to agree,andP(E) is the ex-
pectedproportionof times two annotatorswould
agreebychance. Notethatκhasa valueof at most
1, with higherκ values indicatinghigherrates of
agreement. The κ measureis more meaningful
3
 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  0
 5
 10
 15
 20
 25
 30
 35
Kappa
Source length
Intra. incl. ref. Intra. excl. ref. Inter. incl. ref. Inter. excl. ref.
Moderate agreement
Figure 2: Intra-/inter-annotator agreement
with/without references, across various source
sentencelengths(lengthsofnandn+1 are used
to plotthe pointatx=n). Thisfigureis basedon
all languagepairs.
than reportingP(A) as is, since it takes into ac-
count,viaP(E), how ‘surprising’it is for annota-
torsto agreein the firstplace.
In the context of pairwise comparisons, an
agreementbetween two annotatorsoccurs when
they comparethe same pair of systems(S1,S2),
and both agree on their relative ranking: either
S1 >S2, S1 =S2, orS1 <S2. P(E) is then:
P(E) =P2(S1>S2)+P2(S1=S2)+P2(S1<S2) (2)
In the WMT overview paper, all three cate-
goriesare assumedequallylikely, givingP(E) =
1
9 +
1
9 +
1
9 =
1
3. For consistency with the WMToverview paper, and unless otherwisenoted, we
also use P(E) = 13 whenever a κ value is re-
ported.(ThoughseeSection2.2.2for a discussion
aboutP(E).)
2.2.1 Observed
Agreementfor Different
SentenceLengths
In Figure2 we plot the κ values across different
source sentencelengths. We see that the inter-
annotatoragreement(whenexcludingreferences)
is reasonablyhigh only for sentences up to 10
words in length – accordingto Landisand Koch
(1977),and as cited by the WMToverview paper,
not even ‘moderate’agreementcan be assumedif
κis lessthan0.4. Anotherpopular(andcontrover-
sial) rule of thumb(Krippendorff, 1980) is more
strict and says thatκ < 0.67 is not suitableeven
for tentative conclusions.
For thisreason,andgiventhata majorityof sen-
tences are indeed more than 10 words in length
(the medianis 20 words), we suggestthat future
evaluationseitherincludefeweroutputsperblock,
or divide longer sentencesinto shorter segments
(e.g. on clauseboundaries),so thesesegmentsare
more easily and reliablycomparable. The latter
suggestionsassumesword alignmentas a prepro-
cessingand presentingthe annotatorsthe context
of the judgedsegment.
2.2.2 EstimatingP(E), the Expected
Agreementby Chance
Several agreementmeasures(usually called kap-
pas) were designedbased on the Equation1 (see
ArtsteinandPoesio(2008)andEugenioandGlass
(2004)for an overview and a discussion).Those
measures differ from each other in how to de-
finethe individualcomponentsof Equation2, and
hence differ in what the expected agreementby
chance(P(E)) wouldbe:8
• TheSmeasure(Bennettet al.,1954)assumes
a uniformdistributionover the categories.
• Scott’spi(Scott,1955)estimatesthe distribu-
tionempiricallyfromactualannotation.
• Cohen’s κ (Cohen,1960) estimatesthe dis-
tribution empiricallyas well, and furtheras-
sumesa separatedistributionforeach anno-
tator.
Given that the WMT10overview paper assumes
that the threecategories(S1 >S2, S1 = S2, and
S1 <S2) are equallylikely, it is usingtheS mea-
sure versionof Equation1, thoughit doesnot ex-
plicitlysay so – it simplycalls it “thekappacoef-
ficient”(K).
Regardless of what the measure should be
called,we believe that the uniformdistributionit-
self is not appropriate,even though it seems to
model a “randomclicker” adequately. In partic-
ular, and given the design of the ranking inter-
face, 13 is an overestimateof P(S1 = S2) for
a randomclicker, and should in fact be 15: each
system receives one of five rank labels, and for
two systemsto receive the same rank label, there
are only five (out of 25) label pairs that satisfy
S1 = S2. Therefore,with P(S1 = S2) = 15,
8Thesethreemeasureswerelatergeneralizedto morethan
two annotators(Fleiss, 1971; Bartko and Carpenter, 1976),
Thus, withoutloss of generality, our examplesinvolve two
annotators.
4
“≥Others” S pi
Inter incl. ref. 0.487 0.454excl. ref. 0.439 0.403
Intra incl. ref. 0.633 0.609excl. ref. 0.601 0.575
Table 3: Summaryof two variants of kappa: S
(orK as it is reportedin the WMT10paper)and
our proposedScott’s pi. We reportintervs. intra-
annotatoragreementand collectedfrom all com-
parisons (“incl. ref.”) vs. collected only from
comparisonswithoutthe reference(“excl. ref.”)
becauseit is generallyeasierto agreethat the ref-
erenceis betterthan the othersystems.Thistable
is basedon all languagepairs.
we have P(S1 > S2) = P(S1 < S2) = 25, and
thereforeP(E) = 0.36 ratherthan0.333.
Takingthediscussiona stepfurther, weactually
advocatefollowingthe ideaof Scott’s pi, whereby
the distributionof each category is estimatedem-
piricallyfrom the actualannotation, rather than
assuminga randomannotator– thesefrequencies
are easy to compute,and reflecta moremeaning-
fulP(E).9
Under this interpretation,P(S1 = S2) is cal-
culatedto be 0.168, reflectingthe fractionof pair-
wise comparisonsthat correspondto a tie. (Note
that this further supports the claim that setting
P(S1 = S2) = 13 for a randomclicker, as used
in the WMToverview paper, is an overestimate.)
This results in P(E) = 0.374, yielding,for in-
stance,pi = 0.454 for “≥others”inter-annotator
agreement,somewhat lower thanκ = 0.487 (re-
portedin Table3).
We do note that the differenceis rather small,
and that our aim is to be mathematicallysound
above all. CarefullydefiningP(E) would be im-
portant when comparingkappas across different
tasks with different P(E), or when attempting
to satisfycertainthresholds(as the cited 0.4 and
0.67). Furthermore,if one is interestedin mea-
suring agreementfor individual annotators,such
as identifyingthose who have unacceptablylow
intra-annotatoragreement, thequestionofP(E) is
quite important,since annotationbehavior varies
noticeablyfromone annotatorto another. A ‘con-
servative’ annotatorwho prefersto rank systems
as beingtied most of the time would have a high
9We believe thatP(E) shouldnot reflectthe chancethat
two randomannotatorswouldagree,but the chancethat two
actualannotatorswouldagreerandomly. Thetwo soundsub-
tly relatedbut are actuallyquitedifferent.
P(E), whereasan annotatorusingtiesmoderately
would have a low P(E). Hence,two annotators
with equalagreementrates(P(A)) are not neces-
sarily equallyproficient,since theirP(E) might
differ considerably.10
2.3 The≥variantvs. the>variant
Even within the same interpretationof how sys-
tems could be scored, there is a question of
whetheror not to reward ties. Theoverview paper
reportsboth variantsof its measure,but does not
notethat thereare non-trivial differencesbetween
the two orderings. Comparefor examplethe “≥
others” orderingvs. the “> others” orderingof
CU-BOJAR and PC-TRANS (Table 2), showing an
unexpectedswingof 7.9%:
≥others >others
CU-BOJAR 65.6 45.0
PC-TRANS 62.1 49.4
CU-BOJAR seemsbetterunderthe≥variant,but
losesoutwhenonlystrictwinsarerewarded.The-
oretically, this couldbe purelydue to chance,but
the total numberof pairwisecomparisonsin “≥
others” is relatively large (about 1,500 pairwise
comparisonsfor each system),and ought to can-
cel sucheffects.
A similarpatterncouldbe seenunderthe“allin
block”interpretationas well (e.g. for CU-TECTO
and ONLINEB). Table 4 documentsthis effect by
lookingat how often a systemis the sole winner
of a block. ComparingPC-TRANS and CU-BOJAR
again, we see that PC-TRANS is up therewith CU-
TECTO andDCU-COMBO as themostfrequentsole
winners,winning71 blocks, whereasCU-BOJAR
is the sole winner of only 53 blocks. This is in
spiteof the fact that PC-TRANS actuallyappeared
in slightlyfewer blocksthan CU-BOJAR (385 vs.
401).
One possibleexplanationis that the two vari-
ants (“≥” and “>”) measuretwo subtlydifferent
thingsaboutMTsystems.DiggingdeeperintoTa-
ble 2’s values,we findthat CU-BOJAR is tied with
anothersystem65.6−45.0 = 20.4%of the time,
whilePC-TRANS is tied with anothersystemonly
62.1−49.4 = 12.7%of the time. So it seemsthat
PC-TRANS’s output is noticeablydifferent from
anothersystemmore frequentlythan CU-BOJAR,
whichreducesthenumberof timesthatannotators
10Who’s more impressive: a psychicwho correctlypre-
dicts the result of a coin toss 50% of the time, or a psychic
whocorrectlypredictstheresultof a dieroll50%of thetime?
5
Blocks SoleWinner
305 Reference
73 CU-TECTO
71 PC-TRANS
70 DCU-COMBO
57 RWTH-COMBO
54 ONLINEB
53 CU-BOJAR
46 EUROTRANS
41 UEDIN
41 UPV-COMBO
175 Oneof eightothersystems
409 No solewinner
1395 TotalEnglish-to-CzechBlocks
Table 4: A breakdown of the 1,395blocksfor the
English-Czechtask,accordingto whichsystem(if
any) is the sole winner. On average,a systemap-
pearsin 388blocks.
markPC-TRANS as tied withanothersystem.11 In
that sense,the “≥” rankingis hurtingPC-TRANS,
sinceit does not benefitfromits smallnumberof
ties. On the otherhand,the “>” variantwouldnot
reward CU-BOJAR for its large numberof ties.
The“≥others”scoremaybeartificiallyboosted
if several very similar systems (and therefore
likely to be “tied”) take part in the evaluation.12
One possiblesolutionis to completelydisregard
ties and calculatethe finalscoreas winswins+losses. We
recommendto usethisscoreinsteadof “≥others”
( wins+tieswins+ties+losses) whichis biasedtoward oftentied
systems,and “>others”( winswins+ties+losses) whichis
biasedtoward systemswithfew ties.
2.4 Surprise?Doesthe Numberof
EvaluationsAffecta System’s Score?
When examining the system scores for the
English-Czechtask, we noticeda surprisingpat-
tern: it seemed that the more times a system is
sampled to be judged, the lower its “≥ others”
score (“≥ all in block” behaving similarly). A
scatterplot of a system’s scorevs. the numberof
blocks in which it appears(Figure3) makes the
patternobvious.
We immediatelywonderedif the patternholds
in other languagepairs. We measuredPearson’s
correlationcoefficient withineach language pair,
reported in Table 5. As it turns out, English-
11Indeed, PC-TRANS is a commercialsystem (manually)
tunedover a longperiodof timeandbasedon resourcesvery
differentfromwhatotherparticipantsin WMTuse.
12In the preliminaryWMT11results, this seems to hap-
pen to four Moses-like systems (UEDIN, CU-BOJAR, CU-
MARECEK and CU-TAMCHYNA) which have better “≥oth-
ers”scorebut worse“>others”scorethan CU-TECTO.
Correlationof BlockCount
Source Target vs. “≥Others”
English Czech -0.558
English Spanish -0.434
Czech English -0.290
Spanish English -0.240
English French -0.227
English German -0.161
French English -0.024
German English 0.146
Overall -0.092
Table 5: Pearson’s correlationbetweenthe num-
ber of blockswherea systemwas ranked and the
system’s “≥others”score.(Thereferenceitselfis
not includedamongthe consideredsystems).
 30 35 40 45 50 55 60 65 70 75 80  350
 360
 370
 380
 390
 400
 410
 420
>= Others
Number of judgmentscmu-heafield-combo
cu-bojar
cu-tecto
cu-zeman dcu
dcu-combo
eurotrans
koc
koc-combo
onlineA
onlineB
pc-trans
potsdam
rwth-combo
sfu
uedin
upv-combo
a*x+b
Figure 3: A plot of “≥others”systemscore vs.
timesjudged,for English-Czech.
Czechhappenedto be theonelanguagepairwhere
the ‘correlation’is strongest,with only English-
Spanish also having a somewhat strong correla-
tion. Overall, though,there is a consistenttrend
that can be seen acrossthe languagepairs. Could
it reallybe the casethatthe moreoftena systemis
judged,the worseits scoregets?
Examiningplots for the other language pairs
makes thingsa bit clearer. Considerfor example
the plot for English-Spanish(Figure4). As one
wouldhope,thedatapointsactuallycometogether
to forma cloud,indicatinga lackof correlation.
Thereasonthat a hintof a correlationexists is the
presenceof two outliersin the bottomright cor-
ner. In other words, the very worst systemsare,
indeed,the onesjudgedquiteoften. We observed
thispatternin severalotherlanguagepairsas well.
The correlationnaturallydoes not imply cau-
sation. We are still not sure how to explain the
artifact. A subtle possibilitylies in the MTurk
interface: annotatorshave the choiceto accept a
HIT or skip it before actuallyproviding their la-
6
 10 20 30 40 50 60 70 80  130
 135
 140
 145
 150
 155
 160
 165
 170
>= Others
Number of judgments
cambridge
cmu-heafield-combo
cu-zeman
dcu dfki
jhu
koc
koc-combo
onlineA
onlineB
rwth-combo
sfu
uedin upb-combo
upv
upv-nnlm
a*x+b
Figure 4: A plot of “≥others”systemscore vs.
timesjudged,for English-Spanish.
bels. It mightbe the casethatsomeannotatorsare
morewillingto acceptHITswhenthereis an ob-
viouslypoor system(sincethat would make their
task somewhat easier), and who are more prone
to skippingHITswherethe systemsseemhard to
distinguishfrom each other. So there mightbe a
causationeffect after all, but in the reverse order:
a systemgets judgedmoreoftenif it is a bad sys-
tem.13 A suggestionfromthe reviewersis to run a
pilotannotationwithdeliberateinclusionof a poor
systemamongthe ranked ones.
2.5 Issuesof
PairwiseJudgments
The WMToverview paperalso providespairwise
systemcomparisons:eachcellin Table6 indicates
the percentageof pairwisecomparisonsbetween
the two systemswherethe systemin the column
was ranked better(>) than the systemin the row.
For instance,thereare81 rankingresponseswhere
both CU-TECTO and CU-BOJAR were presentand
indeedranked14 amongthe5 systemsin theblock.
In 37 (45.7%)of thecases,CU-TECTO was ranked
better, in 29(35.8%),CU-BOJAR wasrankedbetter
and there was a tie in the remaining15 (18.5%)
cases.Theties are not explicitlyshown in Table6
but they areimpliedby thetotalof 100%.Thecell
is in bold where there was a win in the pairwise
comparison,so 45.7is boldin our example.
An interesting“discrepancy” in Table 6 is that
CU-TECTO wins pairwisecomparisonswith CU-
BOJAR and UEDIN but it scoresworse than them
in the official “≥ others”, cf. Table 2. Simi-
larly, UEDIN outperformedONLINEB in the pair-
13No punintended!
14The users sometimes did not fill any rank for a system.
Suchcasesare ignored.
RE
F
CU
-BO
JA
R
CU
-TE
CT
O
EU
RO
TR
AN
S
ON
LI
NE
B
PC
-TR
AN
S
UE
DI
N
REF 4.3 4.3 5.1 3.8 3.6 2.3
CU-BOJAR 87.1 45.7 28.3 44.4 39.5 41.1
CU-TECTO 88.2 35.8 38.0 55.8 44.0 36.0
EUROTRANS 88.5 60.9 46.8 50.7 53.8 48.6
ONLINEB 91.2 31.1 29.1 32.8 43.8 39.3
PC-TRANS 88.0 45.3 42.9 28.6 49.3 36.6
UEDIN 94.3 39.3 44.2 31.9 32.1 49.5 -
Table 6: Pairwise comparisonsextracted from
sentence-level rankingsof the WMT10English-
CzechNews Task. Re-evaluatedto reproducethe
numbers published in WMT10 overview paper.
Bold in columnA and row B meansthat system
A is pairwisebetterthansystemB.
wisecomparisonsbut it was ranked worsein both
>and≥officialcomparison.
In the following, we focus on the CU-BOJAR
(B) and CU-TECTO (T) pair becausethey are in-
terestingcompetitorson theirown. They bothuse
the same parallelcorpus for lexical mappingbut
operate very differently: CU-BOJAR is based on
Moses while CU-TECTO transfersat a deep syn-
tacticlayerandgeneratestargettext whichis more
or lessgrammaticallycorrectbut suffersin lexical
choice.
2.5.1 Different
Set of Sentences
The mismatchin the outcomesof “≥others”and
pairwisecomparisonscouldbe causedbydifferent
set of sentences.Thepairwiserankingis collected
fromthe set of blockswhereboth CU-BOJAR and
CU-TECTO appeared (and were indeed ranked).
Each of the systemshowever competesin other
blocksas well,whichcontributesto theofficial“≥
others”.
Theset of sentencesunderlyingthe comparison
is very differentandmoreimportantlythatthe ba-
sis for pairwisecomparisonsis muchsmallerthan
the basis of the official “≥others”interpretation.
Theoutcomeof theofficialinterpretationhowever
dependson therandomsetof systemsyoursystem
was comparedto. In our case, it is impossibleto
distinguish,whetherCU-TECTO had just bad luck
onsentencesandsystemsit wascomparedto when
CU-BOJAR was notin theblockand/orwhetherthe
81 blocksdo not providea reliablepicture.
2.5.2 PairwiseJudgmentsUnreliable
To complementWMT10rankingsfor the two sys-
tems and avoid the possiblelower reliabilitydue
to 5-fold ranking instead of a targeted compari-
7
Authorof B says:
both both
B>T T>B fine wrong Total
Tsays:
B>T 9 1 1 11
T>B 2 13 3 18
bothfine 2 2 3 7
bothwrong 10 5 1 11 27
Total 23 18 4 18 63
Table 7: Additionalannotationof 63 CU-BOJAR
(B) vs. CU-TECTO (T) sentencesby two annota-
tors.
Better Both
Annotator B T fine wrong
A 24 23 5 11
C 10 12 5 36
D 32 20 2 9
M 11 18 7 27
O 23 18 4 18
Z 25 27 2 9
Total 125 118 25 110
Table 8: Blurry picture of pairwiserankingsof
CU-BOJAR vs. CU-TECTO. Wins in bold.
son,we asked themainauthorsof bothCU-BOJAR
and CU-TECTO to carryout a blindpairwisecom-
parisonon the exactset of 63 sentencesappearing
acrossthe 81 blocksin whichboth systemswere
ranked. As the totals in Table 7 would suggest,
eachauthorunwittinglyrecognizedhissystemand
slightlypreferredit. The detailshowever reveal a
subtlerreasonfor the low agreement:one of the
annotatorswas less picky about MT quality and
accepted10+5 sentencescompletelyrejected by
the otherannotator. In total, thesetwo annotators
agreedon 9 + 13+ 2 + 11 = 35 (56%)of cases
andtheirpairwiseκis 0.387.
A further annotationof these 63 sentencesby
four more people completesthe blurry picture:
the pairwiseκ for each pair of our five annota-
tors ranges from 0.242 to 0.615 with the aver-
age 0.407±0.106. The multi-annotatorκ (Fleiss,
1971) is 0.394 and all six annotatorsagree on a
single label only in 24% of cases. The agree-
mentis not bettereven if we merge the categories
“Both fine” and “Both wrong”into a single one:
The pairwiseκ ranges from 0.212 to 0.620 with
the average0.405±0.116,the multi-annotatorκis
0.391.Individualannotationsaregiven in Table8.
Naturally, the set of these63 sentencesis not a
representative sample.Even if one of the systems
SRC It’s not completelyideal.
REF Nen´ı to ´uplnˇe ide´aln´ı. Ranks
PC-TRANS To nen´ı ´uplnˇe ide´aln´ı. 2 5
CU-BOJAR To nen´ı ´uplnˇe ide´aln´ı. 5 4
Table9: Two rankingsby the sameannotator.
SRC FCCawardeda tunnelin Sloveniafor 64 million
REF FCCbyl pˇridˇelentunelve Slovinskuza 64 milion˚u
GlossFCCwasawardeda tunnelin Sloveniafor 64 million
HYP1 FCCpˇridˇelil tunelve Slovinskuza 64 mili´on˚u
HYP2 FCCpˇridˇelilatunelve Slovinskuza 64 milion˚u
Gloss FCCawardedmasc/fem a tunnelin Sloveniafor 64 million
Figure 5: A poor referencetranslationconfuses
humanjudges.TheSRCandREFdiffer in the ac-
tive/passive form,attributingcompletelydifferent
rolesto “FCC”.
actuallywon, such an observation couldnot have
been generalizedto other test sets. The purpose
of the exercisewas to checkwhetherwe areatall
able to agree whichof the systemstranslatesthis
specific set of sentencesbetter. As it turns out,
even a simple pairwise ranking can fail to pro-
vide an answerbecausedifferent annotatorssim-
ply have differentpreferences.
Finally, Table 9 illustrates how poor the
WMT10rankingscan be. The exact same string
producedby two systemswas ranked differently
eachtime– by the sameannotator. (Thehypothe-
sis is a plausibletranslation,only the information
structureof thesentenceis slightlydistortedso the
translationmaynotfit wellit thesurroundingcon-
text.)
3 TheImpactof
the Reference
Translation
3.1 BadReferenceTranslations
Figure 5 illustratesthe impact of poor reference
translationon manual ranking as carried out in
Section2.5.2.Of oursix independentannotations,
three annotatorsmarked the hypothesesas “both
fine” given the match with the source and three
annotatorsmarked them as “both wrong”due to
the mismatchwith the reference. Given the con-
structionof the WMTtest set, this particularsen-
tence comes from a Spanishoriginaland it was
mostlikely translateddirectlyto bothEnglishand
Czech.
8
Correlationof
Source Target Referencevs. “≥others”
Spanish English 0.341
English French 0.164
French English 0.098
German English 0.088
Czech English -0.041
English Czech -0.145
English Spanish -0.411
English German -0.433
Overall -0.107
Table10: Pearson’s correlationof the relative per-
centage of blocks where the reference was in-
cluded in the ranking and the final “≥ others”
of the system(the referenceitself is not included
amongthe consideredsystems).
 25 30 35 40 45 50 55 60 65 70 75  0.19
 0.2
 0.21
 0.22
 0.23
 0.24
 0.25
 0.26
>= Others
Relative presence of the reference
cmu-heafield-combo
cu-zeman
dfki
fbk
jhu
kit
koc
koc-combo
limsi
liu
onlineA
onlineB
rwth
rwth-combo
sfu
uedin
uppsala
upv-combo
a*x+b
Figure 6: Correlationof the presenceof the ref-
erence and the official “≥ others” for English-
Germanevaluation.
3.2 ReferenceCanSkew
Pairwise
Comparisons
Theexactset of competingsystemsin each5-fold
rankingin WMT10evaluationis random.The“≥
others”however is affectedby this: a systemmay
suffer more lossesif often comparedto the refer-
ence,andsimilarlyit maybenefitfrombeingcom-
paredto a poorcompetitor.
To check this, we calculatethe correlationbe-
tweentherelative presenceof thereferenceamong
the blocks where a system was judged and the
system’s official “≥ others” score. Across lan-
guage, there is almost no correlation(Pearson’s
coefficient:−0.107). However, forsomelanguage
pairs, the correlationis apparent,as listed in Ta-
ble 10. Negative correlationmeans: the moreof-
ten the systemwas comparedalongwiththe refer-
ence,the worsethe scoreof the system.
Figure 6 plots the extreme case of English-
Germanevaluation.
Source Target Min Avg±StdDev Max
English Czech 40 65±19 115
English French 40 66±17 110
English German 10 40±16 80
English Spanish 30 54±15 85
Czech English 5 38±13 60
French English 5 37±15 70
German English 10 32±12 65
Spanish English 35 56±11 70
Table11: Thenumberof post-editsper systemfor
eachlanguagepair to complementFigure3 (page
12) of the WMT10overview paper.
4 OtherWMT10Tasks
4.1 BlindPost-EditingUnreliable
WMToften carriesout one more type of manual
evaluation:“Editingtheoutputof systemswithout
displayingthe source or a referencetranslation,
and then later judgingwhethereditedtranslations
were correct.” (Callison-Burchet al., 2010). We
callthe evaluation“blindpost-editing”for short.
We feel that blind post-editingis more infor-
mative than system ranking. First, it constitutes
a uniquecomprehensibilitytest, and afterall, MT
should aim at comprehensibleoutput in the first
place. Second, blind post-editingcan be further
analyzedto search for specific errors in system
output,see Bojar(2011)for a preliminarystudy.
Unfortunately, the amount of post-edits col-
lectedin WMT10varieda lot acrosssystemsand
languagepairs. Table 11 provides the minimum,
average and maximumnumber of post-edits of
outputsof a particularMT system. We see that
e.g. whileEnglish-to-Czechhas many judgments
of thiskindpersystem,Czech-to-Englishis oneof
the worstsupporteddirections.
It is not surprisingthat conclusionsbasedon 5
observationscan be extremelydeceiving. For in-
stance CU-BOJAR seems to produce60% of out-
putscomprehensible(andthuswinsin Figure3 on
page 12 in the WMT overview paper), far better
than CMU. Thisis not in line withthe rankingre-
sultswherebothrankequally(Table5 on page10
in the WMToverview paper). In fact, CU-BOJAR
was post-edited5 times and 3 of these post-edits
were acceptablewhile CMU was post-edited30
timesand5 of thesepost-editswereacceptable.
4.2 A
Remarkon SystemCombinationTask
One resultsof WMT10not observed in previous
years was that system combinationsindeed per-
formed better than individual systems. Previous
9
Dev Set Test Set
Sententes 455 2034 Diff
GOOGLE 17.32±1.25 16.76±0.60 arrowsoutheast
BOJAR 16.00±1.15 16.90±0.61 arrownortheast
TECTOMT 11.48±1.04 13.19±0.58 arrownortheast
PC-TRANS 10.24±0.92 10.84±0.46 arrownortheast
EUROTRAN 9.64±0.92 11.04±0.48 arrownortheast
Table 12: BLEUscoresof samplefive systemsin
English-to-Czechcombinationtask.
years failed to show this clearly, becauseGoogle
Translateusedto beincludedamongthecombined
systems,makingit hard to improve. In WMT10,
GoogleTranslatewas excludedfromsystemcom-
bination task (except for translations involving
Czech,whereit was accidentallyincluded).
OurTable12providesanadditionalexplanation
why thepresenceof Googleamongcombinedsys-
tems leads to inconclusive results. Whilethe test
set was easier(basedon BLEU)thanthe develop-
mentset for mostsystems,it was muchharderfor
Google.All systemcombinationswerethuslikely
to overfit and select Googlen-gramsmost often.
Withoutaccessto Googlepowerfullanguagemod-
els,the combinationsystemswerelikely to under-
performGooglein finalfluency of the output.
5 FurtherIssuesof
ManualEvaluation
We have alreadyseen that the comprehensibility
test by blindpost-editingprovidesa differentpic-
tureof thesystemsthan theofficialranking.Berka
et al. (2011)introduceda third“quiz-basedevalu-
ation”.The quiz-like evaluationusedthe English-
to-Czech WMT10 systems, applied to different
texts: short text snippetswere translatedand an-
notatorswere asked to answerthree yes/noques-
tions complementingeach snippet. The order of
the systemswas rather different from the official
WMT10results: CU-TECTO won the quiz-based
evaluationdespitebeingthe fourthin WMT10.
Becausethetextsweredifferentin WMT10and
the quiz-basedevaluation,we asked a smallgroup
of annotatorsto applytherankingtechniqueonthe
text snippets.Whilenot exactlycomparableto the
WMT10ranking,the WMT10rankingwas con-
firmed: CU-TECTO was again amongthe lowest-
scoringsystemsandGooglewon the ranking.
Bojar(2011)appliesthe error-flaggingmanual
evaluationby Vilar et al. (2006) to four systems
of WMT09 English-to-Czechtask. Again, the
overall order of the systemsis somewhat differ-
ent whenranked by the numberof errorsflagged.
MireiaFarr´us and Fonollosa(2010)use a coarser
but linguisticallymotivatederrorclassificationfor
Catalan-Spanishand suggest that differences in
ranking are caused by annotatorstreating some
typesof errorsas moreserious.
In short, different types of manualevaluations
lead to different resultseven when identicalsys-
temsandtexts are evaluated.
6 Conclusion
We tooka deeperlookat theresultsof theWMT10
manualevaluation,andbasedon ourobservations,
we have somerecommendationsfor futureevalu-
ations:
• We propose to use a score which ignores
ties insteadof the official “≥others”metric
whichrewardsties and “>others”whichpe-
nalizesties. Anotherscore,“≥all in block”,
could help identifywhich systemsare more
dominant.
• Inter-annotatoragreementdecreasesdramat-
ically with sentencelength; we recommend
includingfewer sentencesper block,at least
for longersentences.
• We suggestagreementbe measuredbasedon
an empiricalestimateofP(E), or at leastus-
ing a morecorrectrandomclickingP(E) =
0.36.
• There is evidence of a negative correlation
between the number of times a system is
judgedandits score;werecommenda deeper
analysisof thisissue.
• We recommendthe referencebe sampledat
a lower rate thanothersystems,so as to play
a smallerrole in the evaluation.We also rec-
ommendbetter qualitycontrolover the pro-
ductionof the references.
And to the readersof the WMT overview paper,
we pointout:
• Pairwise comparisonsderived from 5-fold
rankingsare sometimesunreliable. Even a
targetedpairwisecomparisonof two systems
can shedlittlelightas to whichis superior.
• The acceptabilityof post-editsis sometimes
very unreliabledue to the low numberof ob-
servations.
10
References
R. Artsteinand M. Poesio. 2008. Inter-coder agree-
ment for computationallinguistics. Computational
Linguistics, 34(4):555–596.
JohnJ. Bartko andWilliamT. Carpenter. 1976. On the
methodsand theory of reliability. Journalof Ner-
vousandMentalDisease, 163(5):307–317.
E. M. Bennett,R. Alpert,and A. C. Goldstein. 1954.
Communicationsthroughlimited questioning.Pub-
licOpinionQuarterly, 18(3):303–308.
Jan Berka, Martin ˇCern´y, and Ondˇrej Bojar. 2011.
Quiz-Based Evaluation of Machine Translation.
PragueBulletinofMathematicalLinguistics, 95:77–
86, March.
Ondˇrej Bojar. 2011. Analyzing Error Types in
English-CzechMachineTranslation. Prague Bul-
letinofMathematicalLinguistics, 95:63–76,March.
Chris Callison-Burch,PhilippKoehn, ChristofMonz,
Kay Peterson,Mark Przybocki,and Omar Zaidan.
2010. Findingsof the 2010 joint workshopon sta-
tisticalmachinetranslationand metricsfor machine
translation.In Proceedingsof theJointFifthWork-
shoponStatisticalMachineTranslationandMetric-
sMATR, pages17–53,Uppsala,Sweden,July. Asso-
ciationfor ComputationalLinguistics.
Jacob Cohen. 1960. A coefficient of agreement
for nominalscales. EducationalandPsychological
Measurement, 20(1):37–46.
BarbaraDi Eugenioand MichaelGlass. 2004. The
kappastatistic: A secondlook. Computationallin-
guistics, 30(1):95–101.
J. L. Fleiss. 1971. Measuringnominalscale agree-
ment among many raters. Psychological Bulletin,
76(5):378–382.
KlausKrippendorff. 1980. ContentAnalysis:An In-
troductionto Its Methodology. Sage Publications,
BeverlyHills,CA. Chapter12.
J. RichardLandisand GaryG. Koch. 1977. The mea-
surementof observer agreementforcategoricaldata.
Biometrics, 33:159–174.
Jos´e B. Mari˜no Mireia Farr´us, Marta R. Costa-juss`a
and Jos´e A. R. Fonollosa. 2010. Linguistic-based
evaluation criteria to identify statistical machine
translationerrors. InProceedingsofthe14thAnnual
Conference of the EuoropeanAssociationfor Ma-
chineTranslation(EAMT’10), pages167–173,May.
WilliamA. Scott. 1955. Reliabilityof contentanaly-
sis: Thecaseof nominalscalecoding.PublicOpin-
ionQuarterly, 19(3):321–325.
David Vilar, Jia Xu, Luis FernandoD’Haro,and Her-
mannNey. 2006. ErrorAnalysisof MachineTrans-
lationOutput. In InternationalConferenceonLan-
guage Resources and Evaluation, pages 697–702,
Genoa,Italy, May.
11

