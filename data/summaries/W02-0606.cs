Notice anyway that, for both languages, the precision rate is still around 50% at the 5000-pair cutoff.7 7Yarowsky and Wicentowski (2000) report an accuracy of over 99% for their best model and a test set of 3888 pairs.
P00-1027
and Jurafsky (2000) Schone and Jurafsky (2000) present a knowledgefree unsupervised model in which orthographybased distributional cues are combined with semantic information automatically extracted from word co-occurrence patterns in the input corpus.
W00-0712
This differs from the (more general) approach of Schone and Jurafsky (2000), who look for words that tend to occur in the same context.
W00-0712
From the point of view of the evaluation of the algorithm, we should design an assessment scheme that would make our experimental results more directly comparable to those of Yarowsky and Wicentowski (2000), Schone and Jurafsky (2000) and others.
P00-1027 W00-0712
Mutual information (first introduced to computational linguistics by Church and Hanks (1989)) is one of many measures that seems to be roughly correlated to the degree of semantic relatedness between words.
P89-1010
and Wicentowski (2000) Yarowsky and Wicentowski (2000) propose an algorithm that extracts morphological rules relating roots and inflected forms of verbs (but the algorithm can be extended to other morphological relations).
P00-1027
A similar pattern emerges among the examples of German words clustered in a similar manner by Baroni et alii (2002).
C02-1096
See, for example, Goldsmith (2001) for a very different (possibly complementary) approach, and for a review of other relevant work.
J01-2001
Our procedure could also be used to replace the first step of algorithms, such as those of Goldsmith (2001) and Snover and Brent (2001), where heuristic methods are employed to generate morphological hypotheses, and then an informationtheoretically/probabilistically motivated measure is used to evaluate or improve such hypotheses.
J01-2001 P01-1063
Schone and Jurafsky (2000) report a maximum precision of 92%.
W00-0712
