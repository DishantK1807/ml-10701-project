Book Review
Statistical MachineTranslation
PhilippKoehn
(UniversityofEdinburgh)
CambridgeUniversityPress,2010,xii+433pp;ISBN978-0-521-87415-1,$60.00
Reviewedby
ColinCherry
NationalResearchCouncilCanada
Statistical Machine Translation provides a comprehensive and clear introduction to the
most prominent techniques employed in the ﬁeld of the same name (SMT). This text-
book is aimed at students or researchers interested in a thorough entry-point to the
ﬁeld,anditdoesanexcellentjobofprovidingbasicunderstandingforeachofthemany
piecesofastatisticaltranslationsystem.Iconsiderthisbooktobeanessentialaddition
toanyadvancedundergraduatecourseorgraduatecourseonSMT.
Thebookisdividedintothreeparts:Foundations,CoreMethods,andAdvancedTopics.
Foundations (75 pages) covers an introduction to translation, working with text, and
probabilitytheory.CoreMethods(170pages)coversthemaincomponentsofastandard
phrase-based SMT system. Advanced Topics (125 pages) covers discriminative training
and linguistics in SMT, including an in-depth discussion of syntactic SMT. The text as
a whole assumes a certain familiarity with natural language processing; though the
Foundations section provides an effort to ﬁll in the gaps, the book’s focus is decidedly
translation. As such, students unfamiliar with NLP may sometimes need to consult a
generalNLPtext.
Thebookaimstoprovideathoroughintroductiontoeachcomponentofastatistical
translation system, and it deﬁnitely succeeds in doing so. Supplementing this core
material for each chapter is a highly inclusive Further Reading section. These sections
providebriefnarrativeshighlightingmanyrelevantpapersandalternativetechniques
foreachtopicaddressedinthechapter.Isuspectmanyreaderswillﬁndtheseliterature
pointerstobequitevaluable,fromstudentswishingtodivedeeper,toexperiencedSMT
researchers wishing to get started in a new sub-ﬁeld. Each chapter also closes with a
short list of exercises. Many of these are very challenging (accurately indicated by a
star-ratingsystem),andinvolvegettingyourhandsdirtywithtoolsdownloadedfrom
theWeb.Theusefulnessoftheseexerciseswilldependlargelyontheinstructor’stastes;
Iviewthemasabonusratherthanacorefeatureofthebook.
1. Chapters 1–3: Foundations
The ﬁrst three chapters provide foundational knowledge for the rest of the book. In-
troduction provides an overview of the book and a brief history of machine transla-
tion, along with a discussion of applications and an expansive list of resources. The
overview’s structure takes the form of a summary of each chapter. This structure pro-
vides an effective preview of what will be covered and in what order, but it does not
focusontypicalintroductionmaterial;forexample,thereisnooneplacesetasidetocon-
vincethereaderthatSMTisagoodidea,ortointroduceconciselythemainphilosophies
behind the ﬁeld. The history section is enjoyable, and I was glad to see a cautionary
ComputationalLinguistics Volume36,Number4
note regarding machine translation’s history of high hopes and disappointments. The
applicationssectionprovidesanexcellentoverviewofwhereSMTseesactualuse,and
helpsthereaderunderstandwhytranslationsdonotalwaysneedtobeprefect.
Words,Sentences,CorporaprovidesawhirlwindtourofNLPbasics,brieﬂytouching
onabroadsetoftopicsincludingZipf’slaw,parts-of-speech,morphology,andanum-
berofgrammarformalisms.Togiveanideaofjusthowbriefcoveragecanbe,thesection
on grammar covers four formalisms in ﬁve pages. Nonetheless, these descriptions
should be helpful when the concepts re-appear later in the book. This chapter closes
with a discussion of parallel corpora and sentence alignment. As these are central to
thebusinessofSMT,Ifeeltheymighthavebeenbetterplacedinatranslation-focused
chapter.
ProbabilityTheorycoversthebasicstatisticsneededtounderstandtheideasthrough-
out the book. This chapter is clear, and provides strong intuitions on important issues
suchasconditionalprobability.Thereisasurprisinglylargeemphasisonbinomialand
normal distributions, considering SMT’s heavy reliance on categorical distributions;
however,theseareneededtodiscusssigniﬁcancetestingandsomelanguagemodeling
techniquescoveredlater.
2. Chapters 4–8: Core Methods
The next ﬁve chapters provide detailed descriptions of each of the major components
of a phrase-based SMT system. Word-based Methods discusses the ﬁve IBM translation
models, with a brief detour to discuss the noisy channel model that motivates the
IBM approach. This chapter is best taken as a complement to Brown et al. (1993)
and Kevin Knight’s (1999) tutorial on the same subject, rather than a replacement. It
providesstrongintuitionsonwhateachIBMmodelcoversandhoweachmodelworks,
includingtheclearestdescriptionsIhaveseenofIBM:3–5.However,itdoessometimes
make them seem a little mysterious. For example, there is no attempt to explain why
IBM:1 always arrives at a global maximum, or to generalize when one can apply the
mathematical simpliﬁcation that reduces IBM:1’s exponential sum over products to a
polynomialproductoversums.Oneglaringomissionfromthischapterisadiscussion
ofthealignment HMM(Vogel,Ney,andTillmann 1996).Thiselegantmodel iswidely
usedandwidelyextended,andIhadexpectedtoseeitcoveredindetail.
Chapters5and6onPhrase-basedModelsandDecodingcoverthemajoralgorithmsin
thepopularphrase-basedSMTparadigm.Theyareclearandfairlycomplete;thisbook
couldeasilyserveasaneffectivereferenceforthesetopics.Phrase-basedModelsmotivates
the use of phrases, and then covers phrase extraction along with the calculation of
phrasefeatures,suchaslexicalweightingandlexicalizedre-orderingmodels.Thischap-
teralsomarksthebeginningofacarefuldance,wherelog-linearmodelsareintroduced
without having yet covered SMT evaluation or discriminative training. These topics
are covered in Chapters 8 and 9, respectively. This division of modeling and training
is a reasonable strategy, given the amount of material required to understand the full
pipeline, but a student may need some extra guidance to understand the complete
picture. The Decoding chapter focuses on stack decoding, and it is extremely well-
written,withgreatexplanationsofsearchandpruningstrategies.Alternativedecoding
formalisms,suchasA*orﬁnite-statedecoding,aregivenshortbuteffectivesummaries.
ThischaptermakesphrasalSMTdecodingfeeleasy.
The next chapter covers Language Models. Considering that this topic is given in-
depth coverage in other NLP texts, I was surprised to see it covered quite thoroughly
here as well. This chapter covers a number of smoothing techniques, as well as some
774
BookReview
practicaltipsforhandlinglargemodels.Asusual,theexpositionisexceptionallyclear,
andeachnewmethod’sadvantagesaredemonstratedwithpredictedcountsorperplex-
ity scores on Europarl data, which I found to be very useful. For many SMT courses,
thischapterwillbesufﬁcienttostandaloneasbothanintroductionandareferencefor
languagemodeling.
Finally,theCoreMethodssectioncloseswithadiscussionofEvaluation.Thischapter
discusseshumanevaluation,motivatesautomaticevaluation,andthencoversthemajor
contenders:worderrorrate,BLEU,andMETEOR.ThediscussionofBLEU’sshortcom-
ingsisveryeven-handed,perhapsalittlepessimistic,andacknowledgesallofthemajor
concernsregardingthemetric.
3. Chapters 9–11: Advanced Topics
Theﬁnalthreechapterscoveradvancedtopics,whichincluderecentornotuniversally
adoptedadvances.Soatthispoint,onemightexpectthatallofthemajorcomponentsof
abaselinephrase-basedSMTsystemhavebeencovered,buttheﬁnalpieceofthepuzzle
does not come until Discriminative Training, which includes a discussion of minimum
error rate training (MERT) for the log-linear models introduced in Chapter 5. This
chapter also covers n-best list extraction, n-best list re-ranking, and posterior methods
suchasMinimumBayesRiskDecoding.Italsodevotesasurprisinglylargeamountof
time to large-scale discriminative training, where thousands of parameter values can
be learned. There is a lot of ground to cover here; consequently, much of the material
willneedtobesupplementedwithresearchpapersorothertextsiftheinstructorwants
to cover any one topic in depth. The sections covering the learning methods used in
parameter tuning (maximum entropy, MERT) did not feel as clear as the rest of the
book.Isuspectthatanewcomertotheﬁeldwillrequiresomeguidancetopickoutthe
essentialparts.
Chapter 10 is on Integrating Linguistic Information, which is kind of a grab bag,
covering linguistic pre-processing, syntactic features, and factored translation models.
The pre-processing discussion includes transliteration, morphological normalization,
compound splitting, and even syntactically motivated re-ordering of the input sen-
tence.Thesyntacticfeaturessectionmostlycoversn-bestlistre-rankingasdoneinthe
Smorgasbordpaper(Ochetal.2004).Eachofthesetopicsiswellmotivated,andthetext
providesacleardescriptionofaprominent,recentsolution.
Finally,thebookcloseswithTree-basedModels.Thischaptercoversalotofground:
ﬁrst describing synchronous context-free grammars, and then describing both for-
mally syntactic hierarchical grammars and linguistically syntactic synchronous-tree-
substitution grammars in terms of this common formalism. This is a very nicely
presented chapter. It draws a lot of interesting connections between formalisms; for
example, tree-to-tree rule extraction and tree-to-string rule extraction are presented as
simple constraints on hierarchical phrase extraction. The description of chart parsing
for decoding is also very clear, and it draws many useful analogies to the material
presentedearlierforphrasaldecoding.Igettheimpressionthatmanyinsightsgained
while adding syntactic SMT into the Moses translation system have found their way
intothischapter.
4. Summary
This book’s existence indicates that the ﬁeld of SMT has reached a point of maturity
whereitmakessensetodiscusscoreandfoundationaltechniques.Thisbookprovides
775
ComputationalLinguistics Volume36,Number4
a clear and comprehensive introduction to word, phrase, and tree-based translation
modeling,alongwiththedecoding,training,andevaluationalgorithmsthatmakethese
modelswork.Thetext’sstatedgoalistoprovideathoroughintroduction,butIwould
also recommend it as an effective reference for anyone interested in writing their own
SMTdecoder,beitphrasalorsyntactic.Mostimportantly,thisbookmakestheprospect
of teaching a course devoted to SMT much less daunting, and it should provide a
valuableresourcetoresearchersorstudentslookingtoteachthemselves.
References
Brown,PeterF.,StephenA.DellaPietra,
VincentJ.DellaPietra,andRobertL.
Mercer.1993.Themathematicsof
statisticalmachinetranslation:
Parameterestimation.Computational
Linguistics,19(2):263–312.
Knight,Kevin1999.AstatisticalMT
tutorialworkbook.Availableat:
http://www.isi.edu/∼knight/.
Och,FranzJosef,DanielGildea,Sanjeev
Khudanpur,AnoopSarkar,KenkiYamada,
AlexFraser,ShankarKumar,LibinShen,
DavidSmith,KatherineEng,VirenJain,
ZhengJin,andDragomirRadev.2004.
Asmorgasbordoffeaturesforstatistical
machinetranslation.InProceedingsofthe
HumanLanguageTechnologyConference
oftheNorthAmericanChapterofthe
AssociationforComputationalLinguistics:
HLT-NAACL2004,pages161–168,
Boston,MA.
Vogel,Stephan,HermannNey,and
CristophTillmann.1996.HMM-based
wordalignmentinstatisticaltranslation.
InProceedings,16thInternationalConference
onComputationalLinguistics(COLING),
pages836–841,Copenhagen.
ColinCherryisaresearchofﬁcerattheNationalResearchCouncilCanada.Hisresearchinterests
includestructurepredictionandinduction,withapplicationtoparsing,morphology,pronuncia-
tion,andmachinetranslation.Cherry’saddressisNRCInstituteforInformationTechnology,1200
MontrealRoad,M50:C-318,Ottawa,Ontario,CanadaK1A0R6;e-mail:Colin.Cherry@nrc-cnrc.
gc.ca;URL:https://sites.google.com/site/colinacherry/.
776

