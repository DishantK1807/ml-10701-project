Coling 2008: Proceedings of 3rd Textgraphs workshop on Graph-Based Algorithms in Natural Language Processing, pages 57–60
Manchester, August 2008
Random Graph Model Simulations of Semantic Networks for 
Asociative Concept Dictionaries 
Hiroyuki Akama 
Tokyo Institute of Technology 
2-12-1 O-okayama Meguro-ku 
Tokyo 152-8550, Japan 
akama@dp.hum.titech.ac.jp 
Tery Joyce 
Tama University 
802 Engyo Fujisawa-shi 
Kanagawa-ken 252-0805, Japan 
terry@tama.ac.jp 
Jaeyoung Jung 
Tokyo Institute of Technology 
2-12-1 O-okayama Meguro-ku 
Tokyo 152-8550, Japan 
catherina@dp.hum.titech.ac.jp 
Maki Miyake 
Osaka University 
1-8 Machikaneyama-cho Toyonaka-shi 
Osaka 560-0043, Japan 
mmiyake@lang.osaka-u.ac.jp 
 
Abstract 
Word asociation data in dictionary form 
can be simulated through the combina-
tion of three components: a bipartite 
graph with an imbalance in set sizes; a 
scale-free graph of the Barabási-Albert 
model; and a normal distribution con-
necting the two graphs.  Such a model 
makes it posible to simulate the complex 
features in degree distributions and the 
interesting graph clustering results that 
are typically observed for real data. 
1 Modeling
background 
Asociative Concept Dictionaries (ACDs) consist 
of word pair data based on psychological ex-
periments where the participants are typically 
asked to provide the semantically-related re-
sponse word that comes to mind on presentation 
of a stimulus word. Two well-known ACDs for 
English are the University of South Florida word 
asociation, rhyme and word fragment norms 
(Nelson et al., 198) and the Edinburgh Word 
Asociation Thesaurus of English (EAT; Kis et 
al., 1973). Two ACDs for Japanese are Ishizaki’s 
Asociative Concept Dictionary (IACD) (Oka-
moto and Ishizaki, 201) and the Japanese Word 
Asociation Database (JWAD) (Joyce, 205, 
206, 207). 
While there are a number of practical applica-
tions for ACDs, three are singled out for mention 
                                                
© 208. Licensed under the Creative Comons Atri-
bution-Noncomercial-Share Alike 3.0 Unported 
license (htp:/creativecomons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
 
here. The first is in the area of artificial intelli-
gence, where ACDs can contribute to the devel-
opment of intelligent information retrieval sys-
tems for societies requiring increasingly sophisti-
cated navigation methods. A second application 
is in the field of medicine, where ACDs could be 
used in developing systems that seek to prevent 
dementia by checking higher brain functions 
with a brain dock. Finally, within educational 
settings, ACDs can greatly facilitate language 
learning through the manifestation of inherent 
cultural modes of thinking. 
The typical format of an ACD is to list the 
stimulus words (cue words) and their response 
words together with some statistics relating to the 
word pairing. The stimulus words are generally 
basic words determined in advance by the ex-
perimenter, while the response words are seman-
tically associated words provided by respondents 
on presentation of the stimulus word. The statis-
tics for the word pairing include, for example, 
measured or calculated indices of distance or 
perhaps some classification of the semantic rela-
tionship between the pair of words. 
In order to mathematically analyze the struc-
ture of ACDs, the raw association data is often 
transformed into some form of graph or complex 
network representation, where the vertices stand 
for words and the edges indicate an associative 
relationship (Joyce and Miyake, 207). However, 
to our knowledge, there have been no attempts at 
mathematically simulating an ACD as a way of 
determining in advance the architectural design 
of a dictionary. One reason is that it is a major 
challenge to compute maximum likelihod esti-
mations (MLEs) or Monte-Carlo simulations for 
graph data (Snijder, 205). Thus, it is extremely 
difficult to predict dependences for unknown 
57
factors such as the lexical distribution acros a 
predetermined and controlable dictionary 
framework starting simply from a list of basic 
words. Accordingly, we propose an easier and 
more basic approach to constructing an ACD 
model by combining random graph models to 
simulate graph features in terms of degree distri-
butions and clustering results. 
 
2 Degre
distributions for ACDs 
2.1 Typical
local skew 
It is widely known that Barabási and Albert 
(199) have sugested that the degree 
distributions of scale-free network structures 
correspond to a power law, expressed as 
r
dxP
!
=)(
 (where 
d
stands for degree and 
r
 is a small number, such as 2 or 3). This type of 
distribution is also known as Zipf's law 
describing the typical frequency distribution of 
words in a document and plots on a log scale as a 
falling diagonal stroke. However, in the degree 
distribution of ACDs, there is always a local 
skew, as a local peak or bump with a low 
hemline. Figure 1 presents two degree 
distributions; for the IACD (uper) (
r
 = 1.8) and 
the JWAD (lower) (
r
 = 2.3). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Degree distributions for actual data 
 
The plots indicate a combination of heteroge-
neous distributions, consisting of a single degree 
distribution represented as a bell form with a 
steep slope on the right side. However, what is 
most interesting here is that throughout the dis-
tribution range the curves remain regular and 
continuous, with an absence of any ruptures or 
fractures both before and after the local peaks. 
When actual ACD data is examined, one finds 
that as response words are not linked together, 
almost all the words located in the skewed part 
are stimulus words (which we refer to as peak 
words in this study), while the items before the 
local peak are les frequent response words that 
have a strong tendency to conform to a decaying 
distribution. It is therefore relatively natural to 
divide all word pairs into two types of graph: 
either a bipartite graph for new response words 
that are not already part of the stimulus list and a 
graph that conforms to Zipf's law for the fre-
quencies of response words that are already pre-
sent in the stimulus list. For the first type, new 
response words are represented as nodes only 
with incoming links, generating a bipartite graph 
with two sets of different sizes. This bipartite 
graph would exhibit the decaying distribution 
due to low-frequency response words prior to the 
local peak. In the second type of graph, response 
words are represented as nodes with both incom-
ing and outgoing links. This second type is simi-
lar to a scale-free graph, such as that incorpo-
rated within the Barabási-Albert (BA) model. 
2.2 Bipartite
Graph and BA Model 
A bipartite graph is a graph consisting of vertices 
that are divided into two independent sets, S and 
R, such that every edge conects to one S vertex 
and one R vertex. The graph can be represented 
by an adjacency matrix with diagonal zero sub-
matrices, where the values of the lower right sub-
matrices would all be zero were it not for the 
appearances of some stimulus words as response 
words. The lower right section is exactly where 
the extremely high degrees of hubs are produced, 
which far exceed the average numbers of 
response words. 
Thus, we adopt an approach to generating a 
scale-free graph that reflects Zipf's law for fre-
quency distributions. According to the BA model, 
the probability that a node receives an additional 
link is proportional to its degree. Here, we im-
plement the principle of preferential atachment 
formulated by Bolobás (203): 
!
=
+
"
t
T
tx
dmNP
1
1
)(/)()(
  (1), 
0.00001
0.0001
0.001
0.01
0.1
1
1 10 100 1000
k
P
(
k
)
data
k^(-r)
0.00001
0.0001
0.001
0.01
0.1
1
1 10 100 1000
k
P
(
k
)
data
k^(-r)
58
with the addition of one condition that is specific 
to ACDs, which we explain below. The BA 
model starts with a small number, 
0
m
 of vertices, 
and at each time step, 
T, a new vertex with 
m
 
edges is added and linked to  different vertices 
that already exist in the graph. 
1+t
N
 represents a 
random set of 
m
 early vertices, 
)(id
 the degree 
of vertex 
i
 in the process at time 
t
. The 
probability that a new vertex wil be conected to 
a vertex  depends on the conectivity of that 
vertex, as expressed by Equation (1). However, 
we specifically assume that  is a random 
natural number that is smaller than 
0
m, because 
in actual data the ratio of stimulus words among 
all responses words for each stimulus word is 
obviously far from constant. 
Moreover, the graph for the BA model here 
should be regarded as being a directed graph, 
because the very reason that hubs emerge within 
semantic network representations of ACDs is 
that the number of incoming edges is much larger 
than the expected number of nodes for each 
posible in-degree. In contrast, out-degre is 
limited by the number of responses for each 
stimulus word 
i, which is represented as
)(ic
. 
Let 
)(ic
 folow a normal distribution with a 
mean c
m
 and a small variance value 
2
!
 (which is 
not constant but nearly so) to smoothly combine 
the distribution of the bipartite graph and the 
power distribution. If a directed adjacency matrix 
for the network exclusively between stimulus 
words is expressed as 
)(
ij
BD, then the sum of the 
non-zero values for each row in a random 
bipartite graph introducing new response words 
wil be 
!
"
#
i
j
DC)()
 (The vertices of stimulus 
words with the subscript  are linked with the 
vertex of the stimulus word i). Thus, new 
response words—words that are not stimulus 
words—wil be randomly allocated within a 
bipartite graph according to Equation (2): 
"
#
=
j
BcrP)()()1,(
      (2), 
where 
r
is the approximate number of such 
words. Equation (2) wil yield the lower left and 
the uper right sections of the complete adja-
cency matrix 
A
 for the ACD model. The subse-
quent sub-matrix 
t
P
 refers to the transposition 
of the prior sub-matrix . The adjacency matrix 
in Equation (3) represents a pseudo bipartite 
graph structure where the uper left section is a 
zero sub-matrix (because there are no intercon-
nections among new response words), but the 
lower right section is not. Here, 
ij
B
 (not 
)(
ij
D, 
but the undirected counterpart to it), which core-
sponds to the BA model, is taken as a subsection 
of the adjacency matrix that must be non-
directed for the whole composition. 
!
"
#
$
%
&
=
ij
t
BP
O
A
     (3) 
The key to understanding Equation (3) is to real-
ize that 
P
 is conditionally dependent on 
ij
B, 
because we assume a normal distribution for the 
number of non-zero values at each row in the 
lower section of 
A
. 
2.3 Simulation
Results 
Taking into account the approximate numbers of 
posible new response words, in other words, the 
balance in sizes between the two sets in the 
bipartite graph, we built a composition of partial 
random graphs that could represent an adjacency 
matrix of the ACD model. Figure 2 presents one 
of the results obtained for the folowing 
conditions:
3000,15,310,90
0
===rcm!
. 
As the Figure shows, the local peak and the 
accompanying hemline in the degree distribution 
are clearly simulated by the complex 
combination of random graphs. 
10 20 30 40
-8
-6
-4
-2
 
 
Figure 2. Degree distribution of an ACD model 
 
The degree distribution for the artificial net-
work is consistent with the features observed for 
actual ACD data, where more than 96% of the 
stimulus words in each data set are distributed 
acros the peak section of the degree distribution, 
which is why we have referred to them as peak 
words. Moreover, it is easy to verify that without 
the assumption of a normal distribution for 
)(c, 
distinct fractures emerge in the artificial curve 
where new response words in the bipartite struc-
Local peak 
59
ture would be distinguished from stimulus words 
located at initial points of the local peak. 
3 Markov
Clustering of ACDs 
3.1 MCL

This section introduces the graph clustering 
method that is applied to both the real and artifi-
cial ACD data in order to compare them. Markov 
Clustering (MCL) proposed by Van Dongen 
(201) is well known as a scalable unsupervised 
cluster algorithm for graphs that decomposes a 
whole graph into small coherent groups by simu-
lating the probability movements of a random 
walker acros the graph. It is believed that when 
MCL is applied to semantic networks, it yields 
clusters of words that share certain similarities in 
meaning or appear to be related to common con-
cepts. 
3.2 MCL
Results 
The clustering results for the ACD model created 
by combining random graphs reveal that each of 
the resultant clusters contains only one stimulus 
word surrounded by several response words. This 
result is somewhat strange because there are 
dense conections between stimulus words, 
which would lead us to assume that clusters 
would have multiple stimulus word. However, 
the results of applying MCL clustering to the 
graph for the ACD model are in reality highly 
influenced by the sub-structure of the bipartite 
graph and less dependent on the scale-free 
structure. 
Nevertheless, the result is quite similar to 
results observed with real data. On examining 
MCL clustering results for different ACD 
semantic networks, we have observed that MCL 
clusters tend to consist of one word node with a 
relatively high degree and some other words with 
relatively low degrees. On closer inspection of 
the graph, it is posible to see several suporter 
nodes that gather around one leader node, 
forming a kind of small conceptual community. 
This sugests that the highest degree word for 
each cluster becomes a representative for that 
particular cluster consisting of some other low 
degree words. In short, MCL clustering is 
executed based on such high degree words that 
tend to have relatively low curvature values 
(Dorow, 205) compared to their high average 
degree values. 
4 Conclusion

In this paper, we have proposed a basic approach 
to simulating word association dictionary data 
through the application of graph methodologies. 
This modeling is expected not only to provide 
insights into the structures of real ACD data, but 
also to predict, by manipulating the model pa-
rameters, posible forms for future ACDs. Future 
research wil focus on constructing an exponen-
tial random graph model for ACDs based on 
Markov Chain Monte Carlo (MCMC) methods. 
References 
Barabási, Albert-László and Réka Albert. 199. 
Emergence of scaling in random networks, Science. 
286:509-512. 
Bolobás, Béla. 203. Mathematical Results on Scale-
fre Random Graphs, htp:/ww.stat.berk 
eley.edu/~aldous/Networks/bol1.pdf 
Dorow, Beate et al. 205. Using Curvature and 
Markov Clustering in Graphs for Lexical Acquisi-
tion and Word Sense Discrimination, MEANING-
205,2nd orkshop organized by the EANING 
Project, February,3rd-4th. 
Joyce, Tery and Maki Miyake. 207. Capturing the 
Structures in Asociation Knowledge: Aplication 
of Network Analyses to Large-Scale Databases of 
Japanese Word Asociations, Large-Scale Knowl-
edge Resources. Construction and Aplication, 
Springer Verlag:16-131. 
Kis, G.R., Armstrong, C., Milroy, R., and Piper, J. 
1973. An asociative thesaurus of English and its 
computer analysis, In Aitken, A.J., Bailey, R.W. 
and Hamilton-Smith, N. (Eds.), The Computer and 
Literary Studies, Edinburgh University Pres. 
Nelson, Douglas L., Cathy L. McEvoy, & Thomas A. 
Schreiber. 198. The University of South Florida 
word asociation, rhyme, and word fragment 
norms, Retrieved August 31, 205, from 
htp:/ww.usf.edu/FreAsociation 
Okamato, Jun and Shun Ishizaki. 201. Asociative 
Concept Dictionary and its Comparison Electronic 
Concept Dictionaries. PACLING201-4th Confer-
ence of the Pacific Asociation for Computational 
Linguistics:214-20. 
Snijders, Tom A.B., Philipa E. Patison, Gary 
L.Robins, Mark S. Handcock, 205. New Specifi-
cations for Exponential Random Graph Models, 
htp:/stat.gama.rug.nl/SnijdersPatisonRobinsHa
ndcock206.pdf 
Steyvers, Mark and Josh Tenenbaum. 205. The 
Large-Scale Structure of Semantic Networks, Sta-
tistical Analyses and a Model of Semantic Growth, 
Cognitive Science. 29 (1):41-78. 
60
 
61

