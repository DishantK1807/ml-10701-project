Generative Left-Corner Probability Model As with several previous statistical parsers (Collins, 1999; Charniak, 2000), we use a generative history-based probability model of parsing.
A00-2018
The input features for these loglinear models are the real-valued vectors computed by h(d1;:::; di 1), as explained in more detail in (Henderson, 2003).
N03-1014
We determined appropriate training parameters and network size based on intermediate validation 1We used a publicly available tagger (Ratnaparkhi, 1996) to provide the tags.
W96-0213
A moderately larger vocabulary version (4215 tag-word pairs) of this parser achieves 89.8% F-measure on section 0, where the best current result on the testing set is 90.7% (Bod, 2003).
E03-1005
As has been previously proposed by Brants and Crocker (2000), we take a corpus-based approach to this empirical investigation, using a previously de ned statistical parser (Henderson, 2003).
C00-1017 N03-1014
P(dijd1;:::; di 1) P(dijh(d1;:::; di 1)) Of the previous work on using neural networks for parsing natural language, by far the most empirically successful has been the work using Simple Synchrony Networks (Henderson, 2003).
N03-1014
Experiments To investigate the e ects of lookahead on our family of deterministic parsers, we ran empirical experiments on the standard the Penn Treebank (Marcus et al., 1993) datasets.
J93-2004
This provides the neural network with a linguistically appropriate inductive bias when it learns the history representations, as explained in more detail in (Henderson, 2003).
N03-1014
