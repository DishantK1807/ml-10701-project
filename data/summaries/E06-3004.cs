We used bootstrapping (Abney, 2002) which refers to a problem setting in which one is given a small set of labeled data and a large set of unlabeled data, and the task is to induce a classifier.
P02-1046
Resource limitation, directed NER research (Collins and Singer, 1999), (Carreras et al., 2003), (Kozareva et al., 2005a) toward the usage of semi-supervised techniques.
E03-1038 W99-0613
end for Bootstrapping was previously used by (Carreras et al., 2003), who were interested in recognizing Catalan names using Spanish resources.
E03-1038
(Collins and Singer, 1999) classified NEs through co-training, (Kozareva et al., 2005a) used self-training and cotraining to detect and classify named entities in news domain, (Shen et al., 2004) conducted experimentswithmulti-criteria-basedactivelearning for biomedical NER.
P04-1075 W99-0613
(Yarowsky, 1995) and (Mihalcea and Moldovan, 2001) utilized bootstrapping for word sense disambiguation.
P95-1026
Abney. 2002.
P02-1046
This scheme was initially introduced in CoNLLâ€™s (Tjong Kim Sang, 2002a) and (Tjong Kim Sang and De Meulder, 2003) NER competitions, and we decided to adapt it for our experimental work.
W02-2024 W02-2025 W03-0419
(Klein et al., 2003), (Mayfield et al., 2003), (Wu et al., 2003), (Kozareva et al., 2005c) among others, combined several classifiers to obtain better named entity coverage rate.
W03-0428 W03-0429 W03-0433
Various state-of-the-art machine learning algorithms such as Maximum Entropy (Borthwick, 1999), AdaBoost(Carreras et al., 2002), Hidden Markov Models (Bikel et al., ), Memory-based Based learning (Tjong Kim Sang, 2002b), have been used1.
W02-2004 W02-2024 W02-2025
Early NER systems (Fisher et al., 1997), (Black et al., 1998) etc., participating in Message Understanding Conferences (MUC), used linguistic tools and gazetteer lists.
M98-1014
