Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 301–306,
Portland, Oregon, June 17-18, 2011. c©2011 Association for Computational Linguistics
A Robotic World Model Framework Designed to Facilitate 
Human-robot Comunication 
 
 
Meghan Lomas, E. Vincent Cros I, Jonathan Darvil, R. Christopher Garet, 
Michael Kopack, and Keneth Whitebread 
Lockhed Martin Advanced Technology Laboratories 
3 Executive
Campus, Suite 60, Chery Hil, NJ 0802 
1 856.792.9681 
{mlomas, ecros, jdarvil, rgaret, mkopack, kwhitebr}@atl.lmco.com 
 
 
 
 
 
 
Abstract 
We describe a novel world model frame-
work designed to suport situated human-
robot comunication through improved 
mutual knowledge about the physical 
world. This work focuses on enabling a 
robot to store and use semantic information 
from a human located in the same envi-
ronment as the robot and respond using 
human-understandable terminology. This 
facilitates information sharing betwen a 
robot and a human and subsequently pro-
motes team-based operations. Herein, we 
present motivation for our world model, an 
overview of the world model, a discusion 
of prof-of-concept simulations, and future 
work. 
1 Introduction

As robots become more ubiquitous, their interac-
tions with humans must become more natural and 
intuitive for humans. One of the main chalenges to 
natural human-robot interaction is the “language 
barier” betwen humans and robots. While a con-
siderable amount of work has gone into making 
robot dialogue more human-like (Fong et al., 
205), the content of the conversation is frequently 
highly scripted. 
An esential precondition to intuitive human-
robot dialogue is the establishment of a comon 
ground of understanding betwen humans and ro-
bots (Kiesler, 205). Operators expect information 
to be presented in a way such that they can conect 
it with their own world information. This implies a 
ned for robots to be capable of expresing infor-
mation in human-understandable terms. By shifting 
some responsibility for establishing comon 
ground to robots, interactions betwen humans and 
robots become considerably more natural for 
humans by reducing the ned for humans to “trans-
late” the robot’s information. 
Ultimately, the robot’s world model is a key 
contributor to the “language barier.” Because 
humans and robots view and think about the world 
diferently (having diferent “sensors” and “pro-
cesing algorithms”), they subsequently have 
diferent world representations (Figure 1). Humans 
tend to think of the world as objects in space, while 
robotic representations vary based on sensors, but 
are typicaly cordinate-based representations of 
 
 
 
Figure 1. Humans and robots think and subse-
quently comunicate about the world using 
diferent terminology. 
301
fre and ocupied space. This presents a consider-
able chalenge when humans want to comunicate 
naturaly with robots. For robots to become active 
partners for humans, they must be beter able to 
share the information they have gathered about the 
world. To that end, we have begun to adres the 
“language barier” by focusing on how information 
is stored by the robot. 
We have developed a novel world model repre-
sentation that wil enable a robot to merge 
information comunicated by its human team-
mates with its own situational awarenes data and 
use the resulting “operating picture” to drive plan-
ning and decision-making for navigation in 
unfamiliar environments. The ultimate aim of this 
research is to enable robots to comunicate with 
humans and maintain an “actionable awarenes” of 
the environment. This provides a number of bene-
fits: 
• Increased robot situational awarenes. The robots 
wil be able to learn about, store, and recal envi-
ronmental information obtained from humans (or 
other robots). This can include information the 
robot would be incapable of geting on its own, 
either because it has not visited that region of the 
environment or because it is not capable of sens-
ing that information. 
• Increased human situational awarenes. Humans 
wil be able to receive information from robots in 
human-understandable terms. 
• Reduced workload and training for human-robot 
interaction. Because robots wil be able to com-
municate in human-understandable terms, people 
wil be able to interact with robots in ways that are 
more natural to humans. As a result, people wil 
ned fewer specialized interfaces to interact with 
robots and subsequently les training. 
• Improved colaboration. Because people and ro-
bots wil be able to share information, the team 
wil be able to operate more eficiently. Each team 
member wil be able to contribute to team 
knowledge, which wil alow for beter planing. 
2 World
Model Overview 
Our world model framework was designed using 
several key principles: that information must be 
stored in both human-understandable terms and in 
a format usable by the robot; that information must 
be capable of being aded, deleted, or modified 
during operations; and that the world model 
framework should be capable of integrating with a 
wide variety of external systems including pre-
existing perception and planing systems. 
To met these principles, we have developed a 
layered framework that has internal functions for 
managing the world model and can integrate with 
external systems that use the world model, such as 
systems that populate it (perception systems) or 
use it to govern robotic actions (planing systems) 
(Figure 2). 
 
 
 
Figure 2. We have developed a two-layer world 
model that integrates with external functions via 
translation functions to suport the use of a variety 
of robotic capabilities. 
 
Layered world models have shown promise for 
both robot navigation (Kuipers and Byun, 191; 
Mataric, 190) and for comunication with 
humans (Kenedy et al., 207; Zender et al., 
208). Aditionaly, work in symbol grounding has 
suported robotic actions based on natural lan-
guage interactions (Jacobson et al., 208, Hsiao et 
al., 208). We leverage this research and extend it 
with the aim of suporting human-robot infor-
mation sharing, robot navigation, and use by 
external systems. 
The botom layer stores a spatiotemporal de-
scription of the environment expresed in metrical 
terms. While there are several diferent posibili-
ties for how this location-based information could 
be stored, we use a grid-based representation be-
cause it is comonly used by existing planers 
(e.g., a cost map-based planer) and it alows for 
flexibility of information storage. While our 
framework suports the inclusion of an arbitrary 
number of grids, our experimental prototype uses 
thre: an ocupancy grid that stores fre and ocu-
pied space, an “object” grid, and a “terain” grid. 
The object grid stores the types of objects in each 
 
302
cel in ascending order of vertical position (e.g., 
“table, plate, aple”). The terain grid stores terain 
type in each cel and may also have multiple 
entries per cel (e.g., “sand, boulders” or “gras”). 
The top layer stores a relational description of 
the situation in semantic terms compatible with 
typical human descriptions of the physical envi-
ronment. We use node-atribute structures in which 
objects (e.g., chairs, keys, tres, people, buildings) 
are represented as nodes that have a list of core-
sponding atributes (e.g., type, color, GPS 
cordinates, last time sensed, source of infor-
mation, etc.). The nodes are conected by their 
relationships, which are human-understandable 
concepts (e.g., “near” or “above”). The graph form 
of the semantic layer suports the many, varied 
types of relationships betwen objects. There are 
many ways to expres the physical relationships 
betwen objects, and humans often use ambiguous 
terms (Crangle et al., 1987). By establishing the 
semantic layer as a conected graph, we aim to 
suport these ambiguous terms and ultimately pro-
vide a way for the robot to proces their meaning. 
In the top layer of the world model, we use an 
ontological representation to model the world, and 
include both an “uper ontology” that provides a 
template for what information can be included in 
the world as wel as an instantiated world built 
from experience. In adition to providing a frame-
work that stores the list of al objects that could be 
present in the world, their asociated atributes, and 
the posible relationship betwen the objects, this 
uper layer includes other information such as the 
robot’s goals and curent high level plans and adi-
tional information the robot has about itself or the 
world (e.g., domain theory or object afordances). 
An aditional benefit of an ontology-based repre-
sentation is that it suports the inclusion of objects 
despite uncertainty. If a perception algorithm can-
not confidently identify an object but can clasify 
it, this clas of object can be stored in the semantic 
layer of the world model and refined as more 
information is made available. 
To suport a consistent, complete view of the 
world, translation functions translate the infor-
mation betwen the layers and asimilation 
functions merge information within layers. These 
translation functions suport symbol grounding 
and enable the robot to use both semanticaly-
described information along with sensed data. The 
translation functions are a set of functions, each of 
which translates an atribute, for example, a color 
translation function that translates betwen RGB 
values and a semantic label. More interesting are 
the location-based translation functions, for exam-
ple “near A” translates to “within 2 meters of A’s 
position.” This introduces uncertainty into the po-
sition of the object and so we use a probabilistic 
aproach for placing any unsensed (but described) 
object in the botom layer. The location of the 
object is updated once the object is sensed by the 
robot. 
The asimilation algorithms, which are also stil 
in development, are built upon data fusion ideas 
because they merge data from multiple sources. 
Because a considerable amount of existing work 
has ben done on integrating (asimilating) infor-
mation at the sensor level, to date we have focused 
on asimilation in the semantic layer of our world 
model. We have developed heuristic-based algo-
rithms that compare information stored in the 
world model with actively sensed information 
(esentialy creating a temporary world model of 
the area curently being sensed by the robot). Dur-
ing operation, the robot’s sensor detects an object 
and outputs a vector of posible object clasifica-
tions. Each object clasification has an asociated 
confidence along with atributes of the object 
including size, color, etc. The asimilation compo-
nent puls al objects within a prescribed radius of 
the newly sensed object’s location from the world 
model to compare them with the newly sensed 
object. The asimilation algorithm starts with the 
object closest in position to the newly sensed ob-
ject and stops comparing objects if an object is 
determined to be “same as” the newly sensed 
object or if al objects with the prescribed radius 
are compared and none match. 
To compare our newly sensed object with one 
of the objects already in the world model, the 
asimilation algorithm compares the object vectors, 
which contain the list and confidence in each ob-
ject type and object atributes such as color, size, 
and location. Some atributes (like source of 
information) are ignored in this calculation. To 
compare two objects, we compute the distance 
betwen the object vectors. This distance is com-
puted through a pairwise comparison of atributes 
in the vector lists. These distances are then 
weighted acording to “importance” in asimila-
tion proces, for example objects with similar type 
should be more likely to be merged than objects 
303
that only have similar color. We then sum the 
weighted distances; if sum is les than a prescribed 
threshold, we asume the objects are the same and 
then merge them. If not the same, the algorithm 
checks this object against the other objects within 
the radius and if none are found, ads the object as 
a new object. To merge objects, the algorithm 
merges the atribute vectors of the temporary ob-
ject and the original object. Some parts of the 
vectors are averaged (e.g., color), some amalga-
mated (e.g., data source), and some pick one of the 
values (e.g., pick most recent time). Aditionaly, 
because it is stored in the world model, we can 
incorporate logic about the world to facilitate 
asimilation (e.g., “this object is imovable so it 
must not have changed position”). While this algo-
rithm has served as an initial asimilation 
algorithm, we wil continue researching and 
designing asimilation algorithms to beter suport 
the uncertainty present in the sensing outputs (e.g., 
false positives). 
One of the key requirements of our world mod-
el is that it be able to integrate with external 
robotic systems. To acomplish this, the world 
model layers integrate with external functions that 
serve as translators to existing (or future) func-
tions. These external translation functions pul 
relevant information from the world model and 
present it in a form usable by a planer. For exam-
ple, we have created a planing translator that 
takes the grids from the physical layer and produc-
es a cost map for a ground robot (with set 
parameters), which can then be used by any cost 
map-based planer. 
3 Proof-of-Concept Simulations 
To evaluate the feasibility of our world model 
framework, we performed several prof-of-concept 
simulations designed to both demonstrate and test 
the capabilities of our world model and subse-
quently to help the design proces. We created 
diferent environments using Player/Stage and ran 
the robot through two scenarios. In both scenarios, 
humans neded robotic asistance to escape from a 
burning building and comunicated with the robot 
using natural language. In the first scenario, a 
mobile robot was asked by a group of traped peo-
ple to unlock a dor and alert them when the dor 
was open. In the second scenario, two mobile 
robots were tasked with searching for traped 
people and cordinating with first responders. Be-
cause the focus of the simulations was on 
evaluating the world model itself, we made the 
asumption that the robot had both camera and 
LIDAR sensors and had procesing algorithms 
capable of outputing an object clasification and a 
confusion matrix. We asumed the robot had both 
a spech procesing and synthesis mechanism with 
which it could comunicate verbaly with people 
in the environment. We asumed the robot had a 
comon A* planer that used a cost map represen-
tation for planing. 
The first scenario highlighted the ability for the 
robot to understand and use human-comunicated 
information by ading a human-described object to 
its world model and planing based on this asimi-
lated information. At the begining of the scenario, 
a human described the location of a key (“near the 
desk in the rom with one table and one desk”) and 
told the robot to open the locked east dor. The 
human did not tel the robot to use the key to 
unlock the dor, instead the robot used object 
afordances stored in its world model to establish a 
high-level plan of geting the key, then unlocking 
the dor. When the human told the robot about the 
location of the key, the robot stored this location in 
the top layer and translated the object’s position 
down to the botom layer using a probabilistic 
translation algorithm that placed the key in the bot-
tom layer at the most likely position within a 
certain region (whose size and position core-
sponded to “nearnes”). The robot used a simple 
cost map-based planer to plan its movements and 
so the system created a cost map from al the rele-
vant botom layer information in a format used by 
a clasic A* planer. As a result, this scenario 
showed that our world model enabled the robot to 
use information gathered by a human teamate 
and expresed in semantic terminology without a 
specialy designed planer. 
The second scenario ilustrated the merits of our 
world model for responding to humans. In this 
scenario, once the robot had searched the environ-
ment, it was asked a series of questions by a first 
responder including: “How many people did you 
find?” and “How do I get to the fire extinguisher?” 
The later question was particularly interesting 
because it forced the robot to describe a path in 
semantic terminology (as oposed to a list of way-
points). The robot used information from its top 
layer to describe the path from the first responder’s 
304
curent position to the fire extinguisher. This sce-
nario highlighted the ability for the robot to 
produce human-understandable and useful infor-
mation despite having gathered the information 
using its low-level sensors and planer. 
In both of the scenarios, the robot was given 
both instructions and information verbaly from 
one or more of the people in the robot’s environ-
ment. The robot stored this described information 
in the world model and merged it with the infor-
mation the robot had gathered with its own sensors 
to form a cohesive view of the world. The robot 
then used both the described and sensed infor-
mation to formulate a plan to acomplish its goals. 
At the end of the mision, the robot was asked 
questions about the environment and was able to 
answer using human understandable terminology. 
In these simulations we were able to show the 
robot formulating a plan based on information it 
had not sensed by itself. Because the robot had on-
ly a simple cost map-based planer, it was 
esential that the semantic information be translat-
ed to the grid representations in the botom layer. 
This alowed the planing translator to produce a 
cost map in the form expected by the planer. 
We used these simulations to inform key design 
decisions including the ned to have multiple grids 
in the botom layer of the world model and to 
incorporate object afordances in the semantic lay-
er. Another key insight was that uncertainty must 
be included in the semantic layer and that it is an 
important element in semantic layer asimilation. 
4 Conclusions
and Future Work 
We have designed and developed a world model 
framework that suports situated information shar-
ing betwen robots and humans. By integrating 
semantic and sensor-based terminology, we have 
enabled a robot to integrate information described 
in natural human terms with its own sensed infor-
mation. In adition, we have shown how a robot 
with a standard A* planing algorithm can thereby 
plan and respond apropriately using information 
obtained in semantic terms. 
Because this world model framework was 
designed to suport a variety of robotic operations 
and capabilities, there are many areas of potential 
future work. These include facilitating robotic dia-
logue systems, developing reasoning systems that 
can use the semantic level information to predict 
certain aspects of the world model (such as how an 
event wil afect the physical layout of the world or 
where an object wil be in a certain amount of 
time), and enabling semantic-level planers that 
can perform high-level planing. 
To further improve the functionality suported 
by this world model framework, there are a num-
ber of areas of future work within the framework 
itself. We are exploring the design changes neded 
to suport modeling of dynamic objects and the 
types of asimilation algorithms that exist or ned 
to be developed to truly integrate tracks generated 
by external perception systems into our world 
model. We are also loking into how to beter 
reason about spatial relationships, particularly 
those that are only true when described from a spe-
cific vantage point. Aditionaly, we would like to 
improve the translation algorithms by exploring 
aditional scenarios and determining what mecha-
nisms are neded. In the area of multi-robot 
cordination, we want to explore physical layer as-
similation, which includes the ability to align 
reference frame for heterogeneous robots. Finaly, 
we would also like to aply our world model on 
multiple real robots with spech systems and eval-
uate the world model in a series of real-world 
operations. 
References  
Terence W. Fong, Ilah Nourbakhsh, Robert Ambrose, 
Reid Simons, Alan Schultz, and Jean Scholtz. The 
per-to-per human-robot interaction project. AIA 
Space, 205. 
S. Kiesler. Fostering comon ground in human-robot 
interaction. Robot and Human Interactive Comuni-
cation Procedings. ROMAN 205. The 14th IEE 
International Workshop. Nashvile, TE. Aug 205. 
Benjamin Kuipers and Yung-Tai Byun. A robot explo-
ration and maping strategy based on a semantic 
hierarchy of spatial representation. Journal of Robot-
ics and Autonomous Systems, 8:47–63, 191. 
Maja Mataric. A distributed model for mobile robot 
environment-learning and navigation. Technical Re-
port, MIT Artificial Inteligence Laboratory, 190. 
Wiliam G. Kenedy, Magdalena D. Bugajska, Mathew 
Marge, Wiliam Adams, Benjamin R. Fransen, 
Denis Perzanowski, Alan C. Schultz, and J. Gregory 
Trafton. Spatial representation and reasoning for 
human-robot colaboration. In Procedings of the 
305
Twenty-Second Conference on Artificial Inteli-
gence, 207. 
C. Crangle, P. Supes, and S. Michalowski. Types of 
verbal interaction with instructable robots. In Pro-
cedings of the Workshop on Space Telerobotics, 
Vol 2, 1987. 
H. Zender, O. Martinez Mozos, P. Jenselt, G.-J. M. 
Kruijf, and W. Burgard. Conceptual Spatial Repre-
sentations for Indor Mobile Robots. Robotics and 
Autonomous Systems, Special Isue “From Sensors 
to Human Spatial Concepts.” Vol. 56, Isue 6. p. 
493-502. Elsevier. June 208. 
H. Jacobson, N. Hawes, G-J. Kruijf, J. Wyat, Cross-
modal Content Binding in Information-Procesing 
Architectures. Procedings of the 3rd ACM/IEE 
International Conference on Human-Robot Interac-
tion (HRI). March 208. Amsterdam, The 
Netherlands. 
Kai-yuh Hsiao, Soroush Vosoughi, Stefanie Telex, 
Rony Kubat, Deb Roy. (208). Object Schemas for 
Responsive Robotic Language Use. Procedings of 
the 3rd ACM/IEE International Conference on 
Human-Robot Interaction, pages 23-240. 
306

