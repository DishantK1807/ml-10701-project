Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 467–473,
Edinburgh, Scotland, UK, July 27–31, 2011. c 2011 Association for Computational Linguistics
RankingHumanandMachineSummarizationSystems
PeterRankel
Universityof Maryland
College Park,Maryland
rankel@math.umd.edu
JohnM.Conroy
IDA/Centerfor ComputingSciences
Bowie,Maryland
conroyjohnm@gmail.com
EricV. Slud
Universityof Maryland
College Park,Maryland
evs@math.umd.edu
DianneP. O’Leary
Universityof Maryland
College Park,Maryland
oleary@cs.umd.edu
Abstract
The Text Analysis Conference (TAC) ranks
summarizationsystemsby theiraveragescore
over a collection of document sets. We in-
vestigate the statisticalappropriatenessof this
score and propose an alternative that better
distinguishes between human and machine
evaluationsystems.
1 Introduction
For the past several years, the National Institute of
Standards and Technology (NIST) has hosted the
Text AnalysisConference(TAC) (previouslycalled
the Document Understanding Conference (DUC))
(Nat, 2010). A major theme of this conference is
multi-document summarization: machine summa-
rization of sets of related documents, sometimes
query-focused and sometimes generic. The sum-
marizers are judged by how well the summaries
match human-generatedsummaries in either auto-
maticmetricssuchasROUGE(LinandHovy,2003)
or manual metrics such as responsiveness or pyra-
midevaluation(Nenkova et al.,2007). Typicallythe
systems are ranked by their average score over all
documentsets.
Rankingbyaveragescoreis quiteappropriateun-
dercertainstatisticalhypotheses,forexample,when
each sample is drawn from a distribution which
differs from the distribution of other samples only
througha locationshift (Randlesand Wolfe, 1979).
However, a non-parametric(rank-based)analysisof
variance on the summarizers’scores on each docu-
ment set revealed an impossiblysmallp-value (less
Figure 1: Confidence Intervals from a non-parametric
Tukey’s honestly significant difference test for 46 TAC
2010updatedocumentsets. Theblueconfidenceinterval
(for documentset d1032)does not overlap any of the 30
redintervals. Hence, thetestconcludesthat30document
sets have mean significantlydifferent from the mean of
d1032.
467
Figure2: OverallResponsivenessscores.
Figure3: Linguisticscores.
Figure4: Pyramidscores.
Figure 5: ROUGE-2 scores for the TAC 2010 update
summary task, organized by document set (y-axis) and
summarizer(x-axis). The 51 summarizersfall into two
distinctgroups: machinesystems(first 43 columns)and
humans (last 8 columns). Note that each human only
summarizedhalf of the document sets, thus creating 23
missing values in each of the last 8 columns. Black is
usedto indicatemissingvaluesin the last 8 columnsand
low scoresin the first43 columns.
than 10−12 using Matlab’s kruskalwallis1),
providing evidence that a summary’s score is not
independent of the document set. This effect can
be seen in Figure 1, showing the confidencebands,
as computedby a Tukey honestlysignificantdiffer-
ence test for each documentset’s difficulty as mea-
sured by the mean rank responsiveness score for
TAC 2010. The test clearly shows that the summa-
rizer performanceson different documentsets have
differentaverages.
We further illustratethis in Figures 2 – 5, which
show the scores of various summarizers on vari-
ous document sets using standard human and au-
tomatic evaluation methods (Dang and Owczarzak,
2008) of overall responsiveness, linguistic quality,
pyramid scores, and ROUGE-2using color to indi-
cate the value of the score. Some rows are clearly
darker, indicatingoverall lower scores for the sum-
1The Kruskal-Wallis test performs a one-way analysis of
variance of document-setdifferences after first converting the
summaryscoresforeachsampletotheirrankswithinthepooled
sample. Computed from the converted scores, the Kruskal-
Wallistest statisticis essentiallythe ratioof the between-group
sumof squaresto the combinedwithin-groupsumof squares.
468
mariesof thesedocuments,and the variancesof the
scoresdiffer row-by-row. Theseplotsshow qualita-
tively what the non-parametricanalysisof variance
demonstratesstatistically. While the data presented
wasfortheTAC2010updatedocumentsets,similar
results hold for all the TAC 2008, 2009, and 2010
data. Hence, it may be advantageous to measure
summarizerqualityby accountingfor heterogeneity
ofdocumentswithineachtestset. Anon-parametric
pairedtestlike theWilcoxonsigned-rankisoneway
to do this. Anotherway wouldbe pairedt-tests.
Inthepaper(Conroy andDang,2008)theauthors
noted that while there is a significantgap in perfor-
mance between machine systems and human sum-
marizers when measured by average manual met-
rics, this gap is not present when measured by the
averagesof the best automaticmetric(ROUGE).In
particular,intheDUC2005-2007datasomesystems
have ROUGE performance within the 95% confi-
dence intervals of several human summarizers,but
their pyramid, linguistic,and responsiveness scores
do not achieve this level of performance. Thus,
the inexpensive automaticmetrics,as currentlyem-
ployed,donotpredictwellhow machinesummaries
compareto humansummaries.
In this work we explore the use of document-
pairedtestingforsummarizercomparison.Ourmain
approach is to consider each pair of two summa-
rizers’ sets of scores (over all documents)as a bal-
anced two-sample dataset, and to assess that pair’s
mean difference in scores through a two-sample T
or Wilcoxon test, paired or unpaired. Our goal has
been to confirm that human summarizerscores are
uniformlydifferent and better on average than ma-
chine summarizerscores, and to rate the quality of
the statistical method (T or W, paired or unpaired)
by the consistency with which the human versus
machinescores show superiorhuman performance.
Ourhopeis thatpairedtesting,usingeitherthestan-
dard paired two-sample t-test or the distribution-
free Wilcoxonsigned-ranktest, can provide greater
powerinthestatisticalanalysisofautomaticmetrics
suchas ROUGE.
2 SizeandPowerof
Tests
Statisticaltestsare generallycomparedby choosing
rejectionthresholdsto achieve a certainsmallprob-
ability of Type I error (usuallyas α = .05). Given
multipletestswiththesameTypeIerror, oneprefers
thetestwiththesmallestprobabilityofTypeIIerror.
Since power is definedto be one minus the Type II
error probability, we prefer the test with the most
power. Recall that a test-statistic S depending on
available data-samples gives rise to a rejection re-
gion by definingrejectionof the null hypothesis H0
astheevent{S ≥ c}fora cutoff orrejectionthresh-
old c chosenso that
P(S ≥ c) ≤ α
for all probabilitylaws compatible withthe null hy-
pothesis where the (nominal) significance level α
is chosen in advance by the statistician, usually as
α = .05. However, in many settings, the null hy-
pothesiscomprisesmany possibleprobabilitylaws,
as here where the null hypothesisis that the under-
lying probabilitylaws for the score-samplesof two
separatesummarizersare equal, without specifying
exactly what that probabilitydistribution is. In this
case,thesignificancelevel is anupper boundforthe
attainedsizeof thetest,defined as supP∈H0 P(S ≥
c), the largest rejection probability P(S ≥ c)
achievedbyany probabilitylawcompatiblewiththe
null hypothesis. The power of the test then depends
on the specific probability law Q from the consid-
eredalternatives in HA. For eachsuchQ, andgiven
a thresholdc, the power for the test at Q is the re-
jectionprobabilityQ(S ≥ c). These definitionsre-
flect the fact that the null and alternative hypothe-
ses are composite, that is, each consistsof multiple
probability laws for the data. One of the advan-
tages of consideringa distribution-free two-sample
test statisticsuch as the Wilcoxonis that the proba-
bilitydistributionfor the statisticS is thenthe same
forall(continuous,ornon-discrete)probabilitylaws
P ∈ H0, so that one cutoff c serves for all of H0
withall rejectionprobabilitiesequalto α. 2
Two test statistics, say S and ˜S, are generally
comparedin terms of their powers at fixed alterna-
tives Q in thealternative hypothesisHA, whentheir
respective thresholdsc, c∗ have beendefinedso that
the sizes of the respective tests, supP∈H0 P(S ≥
2TheWilcoxontestis not distribution-freefor discretedata.
However, the discrete TAC data can be thought of as rounded
continuousdata,ratherthanas trulydiscretedata.
469
c) and supP∈H0 P(˜S ≥ c∗), are approximately
equal. In this paper, the test statisticsunder consid-
eration are – in one-sidedtesting — the (unpaired)
two-samplet test with pooled sample variance (T),
the paired two-sample t test (Tp), and the (paired)
signed-rank Wilcoxon test (W); and for two-sided
testing, S is defined by the absolute value of one
of these statistics. The thresholds c for the tests
can be definedeitherby theoreticaldistributions,by
large-sampleapproximations,orbydata-resampling
(bootstrap) techniques, and (only) in the last case
are thesethresholdsdata-dependent,or random. We
explainthesenotionswithrespecttothetwo-sample
data-structureinwhichthescoresfromthefirstsum-
marizer are denoted X1,...,Xn, where n is the
number of documents with non-missing scores for
both summarizers, and the scores from the second
summarizer are Y1,...,Yn. Let Zk = Xk − Yk
denote the document-wisedifferences between the
summarizers’ scores, and ¯Z = n−1summationtextnk=1Zk be
their average. Then the paired statisticsare defined
as
Tp = radicalbign(n−1) ¯Z/(
nsummationdisplay
k=1
(Zk − ¯Z)2)1/2
and
W =
nsummationdisplay
k=1
sgn(Zk)R+k
where R+k is the rank of |Zk| among
|Z1|, ..., |Zn|. Notethat underboth null and alter-
native hypotheses, the variates Zk are assumed in-
dependentidenticallydistributed (iid), while under
H0, therandomvariablesZk aresymmetricabout0.
The t-statisticTp is ‘parametric’in the sensethat
exacttheoreticalcalculationsofprobabilitiesP(a <
Tp < b) dependon the assumptionof normalityof
the differences Zk, and when that holds, the two-
sided cutoff c = c(Tp) is defined as the 1 − α/2
quantileof the tn−1 distributionwithn−1 degrees
of freedom. However, when n is moderately or
very large, the cutoff is well approximated by the
standard-normal1 −α/2 quantilezα/2, and Tp be-
comes approximatelynonparametrically valid with
this cutoff, by the Central Limit Theorem. The
Wilcoxon signed-rank statistic W has theoretical
cutoff c = c(W) which depends only on n, when-
everthedataZk arecontinuouslydistributed;butfor
largen,thecutoffisgivensimplyas radicalbign3/12·zα/2.
Whenthereareties(asmightbecommonindiscrete
data), the calculation of cutoffs and p-values for
Wilcoxon becomes slightly more complicated and
is no longer fully nonparametricexcept in a large-
sampleapproximatesense.
The situation for the two-sample unpaired t-
statistic T currently used in TAC evaluation is not
so neat. Even whenthe two samplesX= {Xk}nk=1
and Y = {Yk}nk=1 are independent,exact theoret-
ical distribution of cutoffs is known only under the
parametricassumptionthat the scores are normally
distributed (and in the case of the pooled-sample-
variance statistic, that Var(Xk) = Var(Yk).) How-
ever, an essential elementof thesummarizationdata
is the heterogeneityof documents. This means that
while {Xk}nk=1 can be viewed as iid scores when
documentsare selected randomly – and not neces-
sarily equiprobably– from the ensembleof all pos-
sible documents, the Yk and Xk samples are de-
pendent. Still, the pairs {(Xk,Yk)}nk=1, and there-
fore the differences{Zk}nk=1, are iid which is what
makespairedtestingvalid. However, thereisnothe-
oretical distribution for T from which to calculate
validquantilesc forcutoffs, andthereforetheuseof
the unpaired t-statistic cannot be recommendedfor
TAC evaluation.
What can be done in a particulardataset,like the
TAC summarization score datsets we consider, to
ascertain the approximate validity of theoretically
derived large-sample cutoffs for test statistics? In
the age of plentiful and fast computers,quite a lot,
through the powerful computational machinery of
the bootstrap (EfronandTibshirani,1993).
The idea of bootstrap hypothesis testing (Efron
and Tibshirani,1993), (Bickel and Ren, 2001) is to
randomly sample with replacement (the rows with
non-missing data in) the dataset {(Xk,Yk)}nk=1 in
such a way as to generate representative data that
plausiblywould have beenseenif two-samplescore
data had been generated from two equally effec-
tive summarizers with score distributional charac-
teristics like the pooled scores from the two ob-
served summarizers. We have done this in two dis-
tinctways,eachcreating2000datasetswithnpaired
scores:
MC Monte Carlo Method. For each of many it-
470
erations (in our case 2000), define a new
dataset{(Xprimek,Y primek)}nk=1 byindependentlyswap-
ping Xk and Yk with probability1/2. Hence,
(Xprimek,Y primek) = (Xk,Yk) with probability 1/2 and
(Yk,Xk) withprobability1/2.
HB Hybrid MC/Bootstrap. For each of 2000
iterations, create a re-sampled dataset
{(Xprimeprimek,Y primeprimek )}nk=1 in the following way. First,
sample n pairs (Xk,Yk) with replacement
from the original dataset. Then, as above,
randomly swap the components of each pair,
eachwith1/2 probability.
Both of these two methods can be seen to gener-
ate two-sampledata satisfyingH0, witheachscore-
sample’s distribution obtained as a mixture of the
distributions actually generatingthe X and Y sam-
ples. The empirical qth quantiles for a statistic
S = S(X,Y) such as |W| or |Tp| are estimated
from the resampleddata as ˆF−1S (q), where ˆFS(t) is
simply the fraction of times (out of 2000) that the
statistic S applied to the constructed dataset had a
value less than or equal to t. The upshot is that the
1 − α empirical quantile for S based on either of
thesesimulationmethodsservesasadata-dependent
cutoff c attaining approximate size α for all H0-
generated data. The MC and HB methods will be
employed in Section 4 to check the theoretical p-
values.
3 Relative
EfficiencyofW versusTp
Statisticaltheory does have somethingto say about
the comparative powers of paired W versus Tp
statistics. These statistics have been studied (Ran-
dles and Wolfe, 1979), in terms of their asymp-
toticrelativeefficiencyforlocation-shiftalternatives
basedonsymmetricdensities(f(z−ϑ)isalocation-
shift of f(z)). For many pairs of parametric and
rank-basedstatisticsS, ˜S, includingW and Tp, the
following assertion has been proved for testing H0
at significancelevel α.
First assume the Zk are distributed according to
some density f(z − ϑ), where f(z) is a symmet-
ric function (f(−z) = f(z)). Next assume ϑ = 0
under H0. When n gets large the powers at any al-
ternatives with very small ϑ = γ/√n, γ negationslash= 0, can
be made asymptoticallyequal by using samples of
sizen withstatisticS and of sizeρ·n withstatistic
˜S. Here ρ = ARE(S, ˜S) is a constantnot depend-
ing on n or γ but definitelydependingon f, called
asymptoticrelativeefficiencyofS withrespectto ˜S.
(The smaller ρ < 1 is, the more statistic ˜S is pre-
ferredamongthe two.)
Using this definition, it is known (Randles and
Wolfe 1979, Sec. 5.4 leading up to Table 5.4.7 on
p. 167) that the Wilcoxon signed-rank statistic W
provides greater robustness and often much greater
efficiencythanthepairedT, withAREwhichis0.95
withf astandardnormaldensity, andwhichisnever
lessthan0.864foranysymmmetricdensityf. How-
ever, in ourcontext, continuousscoressuchas pyra-
mid exhibitdocument-specificscoredifferencesbe-
tweensummarizerswhichoftenhave approximately
normal-lookinghistograms,and although the alter-
natives perhaps cannot be viewed as pure location
shifts, it is unsurprisingin view of the ARE theory
cited above that the W and T pairedtests have very
similar performance. Nevertheless,as we found by
statisticalanalysisof the TAC data, both are far su-
periortotheunpairedT-statistic,witheithertheoret-
icalor empiricalbootstrappedp-values.
4 TestingSetupandResults
To evaluate our ideas, we used the TAC data from
2008-2010 and focused on three manual metrics
(overall responsiveness, pyramid score, and lin-
guistic quality score) and two automatic metrics
(ROUGE-2 and ROUGE-SU4). We make the as-
sumption,backedbyboththescoresgivenandcom-
mentsmadeby NISTsummaryassessors3, that au-
tomatic summarization systems do not perform at
thehumanlevel ofperformance.Assuch,if a statis-
tic based on an automaticmetric, such as ROUGE-
2, weretoshow fewersystemsperformingat human
level of performancethan the statistic of averaging
scores, such a statisticwould be preferablebecause
3Assessors have commentedprivately at the Text Analysis
Conference2008, that while the origin of the summaryis hid-
den from them,“we know whichones are machinegenerated.”
Thus,automaticsummarizationfailstheTuringtestof machine
intelligence (Turing, 1950). This belief is also supported by
(ConroyandDang,2008)and(DangandOwczarzak,2008). Fi-
nally, ourownresultsshow no matterhow youcomparehuman
and machine scores all machines systems score significantly
worsethanhumans.
471
2008: 2145 =parenleftbig662parenrightbigpairs 2009: 1830 =parenleftbig612parenrightbigpairs 2010: 1275 =parenleftbig512parenrightbigpairs
Metric Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc.
Linguistic 1234 1416 1410 1000 1182 1173 841 939 934
Overall 1202 1353 1342 982 1149 1146 845 894 889
Pyramid 1263 1417 1418 1075 1238 1216 875 933 926
ROUGE-2 1243 1453 1459 1016 1182 1193 812 938 939
ROUGE-SU4 1333 1493 1507 1059 1241 1254 894 983 976
Table1: Numberofsignificantdifferencesfoundwhentestingforthedifferenceofallpairsofsummarizationsystems
(includinghumans).
2008: 464 = 58×8 pairs 2009: 424 = 53×8 pairs 2010: 344 = 43×8 pairs
Metric Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc.
Linguistic 464 464 464 424 424 424 344 344 344
Overall 464 464 464 424 424 424 344 344 344
Pyramid 464 464 464 424 424 424 344 344 344
ROUGE-2 375 409 402 323 350 341 275 309 305
ROUGE-SU4 391 418 414 354 378 373 324 331 328
Table 2: Number of significantdifferences resulting from 8 × (N − 8) tests for human-machinesystem means or
signed-rankcomparisons.
of its greaterpower in the machinevs. humansum-
marizationdomain.
For eachof thesemetrics, we first createda score
matrix whose (i,j)-entry represents the score for
summarizerj ondocumentseti(thesematricesgen-
eratedthe colorplotsin Figures2 – 5). We thenper-
formeda Wilcoxonsigned-ranktest on certainpairs
ofcolumnsofthismatrix(anypairconsistingofone
machinesystemand one humansummarizer). As a
baseline,we did the same testing with a paired and
an unpairedt-test. Each of these tests resulted in a
p-value, and we counted how many were less than
.05 andcalledthesethe significantdifferences.
The results of these tests (shown in Table 2),
were somewhat surprising. Although we expected
thenonparametricsigned-ranktestto performbetter
thananunpairedt-test,weweresurprisedtoseethat
a pairedt-test performedeven better. All three tests
always rejectthe null hypotheseswhenhumanmet-
rics are used. This is whatwe’d like to happenwith
automaticmetrics as well. As seen from the table,
thepairedt-testandWilcoxonsigned-ranktestoffer
a goodimprovementover the unpairedt-test.
Theresultsin Table1 are lessclear, but still posi-
tive. Inthiscase,wearecomparingpairsofmachine
summarizationsystems.Incontrasttothehumanvs.
machinecase,we do not know the truthhere. How-
ever, since the numberof significantdifferencesin-
creases with paired testing here as well, we believe
thisalsoreflectsthegreaterdiscriminatorypowerof
pairedtesting.
WenowapplytheMonteCarloandHybridMonte
Carlo to check the theoretical p-values reported in
Tables 1 and 2. The empirical quantiles found
by these methods generally confirm the theoreti-
cal p-value test results reported there, especially
in Table 2. In the overall tallies of all compar-
isons (Table 1), it seems that the bootstrap results
(comparing only W and the un-paired T) make
W look still stronger for linguistic and overall re-
sponsiveness versus the T; but for the pyramid
and ROUGEscores, the bootstrapp-values bring T
slightlycloserto W althoughit still remainsclearly
inferior, achievingroughly10%fewerrejections.
5 ConclusionsandFuture
Work
In this paper we observed that summarization sys-
tems’ performancevaried significantly across doc-
ument sets on the Text AnalysisConference(TAC)
data. This variance in performancesuggested that
paired testing may be more appropriate than the
t-test currently employed at TAC to compare the
472
performance of summarization systems. We pro-
posed a non-parametrictest, the Wilcoxon signed-
rank test, as a robust more powerful alternative to
the t-test. We estimatedthe statisticalpower of the
t-test and the Wilcoxon signed-rank test by calcu-
lating the number of machine systems whose per-
formancewas significantlydifferentthanthatof hu-
man summarizers. As human assessors score ma-
chine systemsas not achieving humanperformance
in either content or responsiveness, automaticmet-
ricssuchasROUGEshouldideallyindicatethisdis-
tinction. We found that the paired Wilcoxon test
significantlyincreases the number of machine sys-
tems that score significantly different than humans
when the pairwise test is performed on ROUGE-2
and ROUGE-SU4 scores. Thus, we demonstrated
that the Wilcoxonpairedtest shows more statistical
power than the t-test for comparingsummarization
systems.
Consequently, theuseofpairedtestingshouldnot
onlybe usedin formalevaluationssuchas TAC, but
also should be employed by summarizationdevel-
opers to more accuratelyassess whetherchangesto
an automatic system give rise to improved perfor-
mance.
Furtherstudy needs to analyzemore summariza-
tion metrics such as those proposed at the recent
NIST evaluation of automatic metrics, Automati-
callyEvaluatingSummariesofPeers(AESOP)(Nat,
2010). As metrics become more sophisticatedand
aim to more accurately predict human judgements
such as overall responsiveness and linguistic qual-
ity, paired testing seems likely to be a more power-
ful statisticalprocedurethan the unpaired t-test for
head-to-headsummarizercomparisons.
Throughoutour researchin this paper, we treated
each separate kind of scores on a document set as
data for one summarizer to be compared with the
same kind of scores for other summarizers. How-
ever, it might be more fruitfulto treat all the scores
as multivariate data and compare the summarizers
that way. Multivariate statisticaltechniquessuch as
PrincipalComponentAnalysismayplaya construc-
tive role in suggesting highly discriminating new
compositescores, perhaps leading to statisticswith
even morepowerto measurea summary’s quality.
ROUGE was inspired by the success of the
BLEU (BiLingual Evaluation Understudy), an n-
gram based evaluation for machine translation(Pa-
pineni et al., 2002). It is likely that paired testing
may also be appropriatefor BLEU as well and will
give additional discriminating power between ma-
chinetranslationsandhumantranslations.
References
Peter J. Bickel and Jian-JianRen. 2001. The Bootstrap
in HypothesisTesting. In State of the Art in Statistics
and Probability Theory, Festschrift for Willem R. van
Zwet,volume36ofLecture Notes–MonographSeries,
pages91–112.Instituteof MathematicalStatistics.
John M. Conroy and Hoa Trang Dang. 2008. Mind the
Gap: Dangers of Divorcing Evaluations of Summary
Content from Linguistic Quality. In Proceedings of
the 22nd InternationalConference on Computational
LinguisticsVolume1, COLING’08, pages145–152,
Stroudsburg,PA,USA.AssociationforComputational
Linguistics.
Hoa T. Dang and KarolinaOwczarzak. 2008. Overview
of the tac 2008 update summarizationtask. In Pro-
ceedings of the 1st Text Analysis Conference (TAC),
Gaithersburg, Maryland,USA.
B. Efron and R. J. Tibshirani. 1993. An Introductionto
the Bootstrap. Chapman& Hall,New York.
Chin-Yew LinandEduardHovy. 2003. AutomaticEval-
uation of Summaries Using N-gram Co-Occurrences
Statistics. In Proceedings of the Conference of the
North AmericanChapter of the Associationfor Com-
putationalLinguistics, Edmonton,Alberta.
National Institute of Standards and Technology. 2010.
Text AnalysisConference, http://www.nist.gov/tac.
Ani Nenkova, RebeccaPassonneau,and KathleenMcK-
eown. 2007. The Pyramid Method: Incorporating
HumanContentSelectionVariationin Summarization
Evaluation. ACM Transactions on Speech and Lan-
guage Processing, 4(2).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluationof MachineTranslation. In Proceedingsof
the 40thAnnualMeetingon Associationfor Computa-
tional Linguistics, ACL ’02, pages 311–318,Strouds-
burg, PA, USA. Association for ComputationalLin-
guistics.
R.H. Randles and D.A. Wolfe. 1979. Introduction to
the Theory of Nonparametric Statistics. Wiley series
in probabilityand mathematicalstatistics.Probability
andmathematicalstatistics.Wiley.
Alan Turing. 1950. ComputingMachineryand Intelli-
gence. Mind, 59(236):433–460.
473

