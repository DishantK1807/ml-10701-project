<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Y Bernstein</author>
<author>J Zobel</author>
</authors>
<title>A scalable system for identifying co-derivative documents</title>
<date>2004</date>
<booktitle>Proc. String Processing and Information Retrieval Symp</booktitle>
<pages>55--67</pages>
<marker>Bernstein, Zobel, 2004</marker>
<rawString>Y. Bernstein and J. Zobel. 2004. A scalable system for identifying co-derivative documents. Proc. String Processing and Information Retrieval Symp., pages 55–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Z Broder</author>
</authors>
<title>Identifying and filtering near-duplicate documents</title>
<date>2000</date>
<booktitle>Proceedings of the 11th Annual Symposium on Combinatorial Pattern Matching</booktitle>
<pages>1--10</pages>
<marker>Broder, 2000</marker>
<rawString>A.Z. Broder. 2000. Identifying and filtering near-duplicate documents. Proceedings of the 11th Annual Symposium on Combinatorial Pattern Matching, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Finn</author>
<author>N Kushmerick</author>
<author>B Smyth</author>
</authors>
<title>Fact or fiction: Content classification for digital libraries</title>
<date>2001</date>
<booktitle>In DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries</booktitle>
<contexts>
<context>20 billion words. In order to collect such a large amount of data we use a web crawler. In the first stage of the project we managed to download 13.86 million web pages. We applied the BTE algorithm (Finn et al., 2001) to remove HTML markup and boilerplate from the pages. After filtering out fullduplicates using MD5 checksums we ended up with 6.73 million of plain text documents. This collection contained 9.29 bil</context>
</contexts>
<marker>Finn, Kushmerick, Smyth, 2001</marker>
<rawString>A. Finn, N. Kushmerick, and B. Smyth. 2001. Fact or fiction: Content classification for digital libraries. In DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T C Hoad</author>
<author>J Zobel</author>
</authors>
<title>Methods for identifying versioned and plagiarized documents</title>
<date>2003</date>
<journal>Journal of the American Society for Information Science and Technology</journal>
<volume>54</volume>
<contexts>
<context>rements of this technique, different algorithms have been described for selecting small n-gram samples from documents, which can be used for estimating the resemblance – an overview is given in e.g. (Hoad and Zobel, 2003). Intuitively, if two documents share a significant number of n-grams then if we take a sample of each according to the same criteria, such as the k lowest in an arbitrary ordering of the n-grams, it</context>
</contexts>
<marker>Hoad, Zobel, 2003</marker>
<rawString>T.C. Hoad and J. Zobel. 2003. Methods for identifying versioned and plagiarized documents. Journal of the American Society for Information Science and Technology, 54(3):203–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N J Larsson</author>
<author>K Sadakane</author>
</authors>
<title>Faster suffix sorting</title>
<date>1999</date>
<tech>Technical Report LU-CS-TR:99-214</tech>
<institution>Lund University, Sweden</institution>
<contexts>
<context>size of the input text. Creating a suffix array of tokenized text (sequence of word IDs) is used to generate sorted list of n-grams for each input chunk. We use a linear algorithm for suffix sorting (Larsson and Sadakane, 1999) which requires only 8 bytes per token. Another 4 bytes per token is used for the original token sequence to output the list of n-grams. Both techniques and the final join are very fast, which we sho</context>
</contexts>
<marker>Larsson, Sadakane, 1999</marker>
<rawString>N.J. Larsson and K. Sadakane. 1999. Faster suffix sorting. Technical Report LU-CS-TR:99-214, Lund University, Sweden.</rawString>
</citation>
</citationList>
</algorithm>

