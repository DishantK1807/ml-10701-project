Proceedings of the 6th Workshop on Statistical Machine Translation, pages 135–139,
Edinburgh, Scotland, UK, July 30–31, 2011. c©2011 Association for Computational Linguistics
MANYimprovementsfor WMT’11
Lo¨ıc Barrault
LIUM,Universityof Le Mans
Le Mans,France.
FirstName.LastName@lium.univ-lemans.fr
Abstract
This paperdescribes the developmentop-
erated into MANY for the 2011 WMT
systemcombinationevaluationcampaign.
Hypothesesfrom French/Englishand En-
glish/FrenchMT systemswere combined
with a new version of MANY, an open
sourcesystemcombinationsoftwarebased
on confusionnetworksdecodingcurrently
developedat LIUM.MANYhas beenup-
dated in order to optimize decoder pa-
rameters with MERT, which proves to
find better weights. The system combi-
nationyieldedsignificantimprovementsin
BLEUscorewhenappliedonsystemcom-
binationdatafromtwo languages.
1 Introduction
This year, the LIUMcomputersciencelaboratory
participatedin the French-Englishsystemcombi-
nationtaskat WMT’11evaluationcampaign.The
system used for this task is MANY1 (Barrault,
2010), an open source system combinationsoft-
ware basedon ConfusionNetworks(CN).
For this year evaluation,rather more technical
than scientificimprovementshave been added to
MANY. The tuning process has been improved
by using MERT (Och, 2003) as a replacement
of the numericaloptimizerCondor(Berghen and
Bersini,2005). The impactof such changeis de-
tailedin section3.
After the evaluationperiod, some experiments
have been performedon the English-Frenchsys-
tem combinationtask. The results are presented
in the section5. Beforethat, a quick description
of MANY, includingrecentdevelopments,can be
foundin section2.
1MANYis availableat the following addresshttp://
www-lium.univ-lemans.fr/˜barrault/MANY
2 Systemdescription
MANY is a system combinationsoftware (Bar-
rault, 2010) based on the decoding of a lattice
made of several ConfusionNetworks (CN). This
is a widespreadapproachin MT systemcombina-
tion(Rostiet al., 2007; Shenet al., 2008; Karakos
et al., 2008; Rosti et al., 2009). MANYcan be
decomposedin two main modules. The first one
is the alignmentmodulewhichactually is a modi-
fiedversionof TERp(Snover et al., 2009). Its role
is to incrementallyalign the hypothesesagainst a
backbonein order to createa confusionnetwork.
Thoseconfusionnetworks are then connectedto-
gether to create a lattice. This module uses dif-
ferentcosts(whichcorrespondsto a match,an in-
sertion, a deletion,a substitution,a shift, a syn-
onym and a stem) to computethe best alignment
and incrementallybuild a confusionnetwork. In
the case of confusionnetwork, the match(substi-
tution,synonyms,and stems)costsare considered
whenthewordin thehypothesismatches(isa sub-
stitution,a synonyms or a stems of) at least one
word of the consideredconfusionsetsin the CN.
System 0
System 1
TERp 
alignment LM
output
1-best 
output
1-best 
output
TERp 
alignment
DECODEMerge
System M
1-best 
output
TERp 
alignment
{
best hypo
nbest list
Lattice
CN
CN
CN
Figure1: Systemcombinationbasedon confusion
network decoding.
Thesecondmoduleis thedecoder. Thisdecoder
is basedon thetoken passalgorithmandit accepts
as inputthe latticepreviouslycreated.The proba-
bilitiescomputedin the decodercan be expressed
as follow :
135
log(PW) = summationdisplay
i
αi log
parenleftBig
hi(t)
parenrightBig
(1)
wheretis thehypothesis,theαi aretheweights
of the featurefunctionshi. Thefollowingfeatures
are consideredfor decoding:
• The languagemodelprobability:the proba-
bilitygiven by a 4-gramlanguagemodel.
• The word penalty:penaltydependingon the
size(in words)of the hypothesis.
• The null-arcpenalty: penaltydependingon
the numberof null-arcscrossedin the lattice
to obtainthe hypothesis.
• Systemweights:eachword receive a weight
correspondingto thesumof theweightsof all
systemswhichproposedit.
3 Tuning
Asmentionedbefore,MANYis madeof two main
modules:the alignmentmodulebasedon a modi-
fiedversionofTERpandthedecoder. Considering
a maximumof 24 systemsforthisyearevaluation,
33 parametersin total have to be optimized. By
default, TERp costs are set to 0.0 for match and
1.0foreverythingelse. Thesecostsarenotcorrect,
since a shift in that case will hardly be possible.
TERp costs are tuned with Condor(a numerical
optimizerbased on Powell’s algorithm,(Berghen
and Bersini, 2005)). Decoder feature functions
weightsare optimizedwith MERT (Och, 2003).
The 300-bestlist createdat each MERT iteration
is appendedto the n-bestlists createdat previous
iterations.Thisproves to be a morereliabletuning
as shown in the followingexperiments.
Duringexperiments,data from WMT’09eval-
uation campaignare used for testing the tuning
approach.news-dev2009ais usedas development
set, andnews-dev2009bas internaltest,thesecor-
poraare describedin Table1.
NAME #sent. #words #tok
news-dev2009a 1025 21583 24595
news-dev2009b 1026 21837 24940
Table1: WMT’09corpora: numberof sentences,
wordsandtokens calculatedon the reference.
For the sake of simplicity, the five best systems
(ranking given by score on dev) are considered
only. Baselinesystemsperformances on dev and
test are presentedin Table2.
Corpus Sys0 Sys1 Sys2 Sys3 Sys4
Dev 18.20 17.83 20.14 21.06 17.72
Test 18.53 18.33 20.43 21.35 18.15
Table 2: Baseline systems performance on
WMT’09data(%BLEU).
The 2-step tuning protocol applied on news-
dev2009a, whenusingMERT to optimizedecoder
featurefunctionsweightsprovides the set of pa-
rameterspresentedin Table3.
Costs: Del Stem Syn Ins Sub Shift
0.87 0.91 0.94 0.90 0.98 1.21
Dec.: LMweight Word pen. Nullpen.
0.056 0.146 0.042
Wghts.: Sys0 Sys1 Sys2 Sys3 Sys4
-0.03 -0.21 -0.23 -0.28 -0.02
Table3: Parametersobtainedwithtuningdecoder
parameterswithMERT.
Results on development corpus of WMT’09
(usedas test set) are presentedin Table4. We can
System Dev Test
Bestsingle 21.06 21.35
MANY(2010) 22.08 22.28
MANY-2steps(2010) 21.94 22.09
MANY-2steps/MERT (2011) 23.05 23.07
Table4: SystemCombinationresultsonWMT’09
data(%BLEU-cased).
observe that 2-step tuning provides almost +0.9
BLEUpointimprovementon developmentcorpus
which is well reflectedon test set with a gain of
more than 0.8 BLEU. By using MERT, this im-
provementis increasedto reachalmost+2 BLEU
pointon dev corpusand+1.7BLEUon test.
There are two main reasons for this improve-
ment. The first one is the use of MERT which
make use of specific heuristics to better opti-
mize toward BLEUscore. The secondone is the
fullylog-linearinterpolationof featuresfunctions
scores operatedinto the decoder(previously, the
word andnullpenaltieswereappliedlinearly).
136
4 2011evaluationcampaign
A development corpus, newssyscombtune2011,
and a test set, newssyscombtest2011, describedin
Table5, wereprovidedto participants.
NAME #sent. #words #tok
newssyscombtune20111003 23108 26248
newssyscombtest2011 2000 42719 48502
Table5: Descriptionof WMT’11corpora.
Languagemodel: TheEnglish target language
modelshas been trainedon all monolingualdata
provided for the translationtasks. In addition,
LDC’s Gigawordcollectionwas usedforbothlan-
guages. Data correspondingto the development
andtestperiodswereremoved fromthe Gigaword
collections.
Sys. # BLEU TER Sys. # BLEU TER
Sys0 29.86 52.46 Sys11 27.23 53.48
Sys1 29.74 51.74 Sys12* 26.82 54.23
Sys2 29.73 52.90 Sys13 26.25 55.60
Sys3 29.58 52.73 Sys14* 26.13 55.65
Sys4* 29.39 52.91 Sys15 25.90 55.69
Sys5 28.89 53.74 Sys16 25.45 56.92
Sys6 28.53 53.27 Sys17 25.23 56.09
Sys7* 28.31 54.22 Sys18 23.63 60.25
Sys8* 28.08 54.47 Sys19 21.90 63.65
Sys9* 27.98 53.92 Sys20 21.77 60.78
Sys10 27.46 54.60 Sys21 20.97 64.00
Sys22 16.63 65.83
MANY-5sys 31.83 51.27
MANY-10sys 31.75 51.91
MANY-allsys 30.75 54.33
Table 6: Systemsperformanceon newssyscomb-
tune2011developmentdata (%BLEU-cased).(*
indicatea contrastive run)
Choosingthe rightnumberof systemsto com-
bine: Table6 shows the performanceof the in-
putsystems(orderedby BLEUscorecomputedon
newssyscombtune2011) and the resultof 3 system
combinationsetups. The difference in these se-
tupsonlyresideon the numberof inputsto usefor
combination(5,10andall systemoutputs).Notice
that the contrastive runs have not beenusedwhen
combining5 and 10 systems. The motivation for
this is to benefit from the multi-sitesystemsde-
velopmentwhichmore likely provide varied out-
puts (i.e. differentngramsand word choice).The
resultsshow that combining5 systemsis slightly
betterthan 10, but give more than 1 BLEUpoint
improvementcomparedto combiningall systems.
Still,the combinationalways providean improve-
ment,whichwas not the case in last year evalua-
tion.
Theresultsobtainedbycombining5 and10sys-
temsare presentedin Table7.
Sys. # BLEU TER Sys. # BLEU TER
Sys0 29.43 52.01 Sys6 28.08 53.19
Sys1 29.15 51.30 Sys11 27.24 53.74
Sys2 28.87 52.82 Sys13 26.74 52.92
Sys3 28.82 52.57 Sys15 26.31 54.61
Sys5 28.08 53.19 Sys16 25.23 55.38
MANY(5sys) 30.74 51.17
MANY(10sys) 30.60 51.39
Table 7: Baseline systems performance on
WMT’11syscombtest data(%BLEU-cased).
OptimizingMANY on newssyscombtune2011
corpusproducedtheparametersetpresentedin Ta-
ble8. We canseethattheweightsof allsystemare
not proportionalto the BLEU score obtainedon
the developmentcorpus. This suggestthat a bet-
ter systemselection couldbe found. This is even
moreprobablesincethe weightof systemSys2is
positive (whichimply a negative impacton each
word proposedby this system),whichmeansthat
whenan hypothesiscontainsa word comingfrom
thissystem,thenits scoreis decreased.
Costs: Del Stem Syn Ins Sub Shift
0.90 0.88 0.96 0.97 1.01 1.19
Dec.: LMweight Nullpen. Lenpen.
0.0204 0.26 0.005
Wghts.:Sys0 Sys1 Sys2 Sys3 Sys5
-0.16 -0.30 0.008-0.16 -0.09
Table8: Parametersobtainedaftertuningthe sys-
temparameterusing5 hypotheses.
Table9 containstheBLEUscorescomputedbe-
tweenthe outputsof the five systemsused during
combination.Aninterestingobservationis thatthe
systemwhichreceive the biggerweightis the one
which”distance”2 againstall othersystemoutputs
2This ”distance”is expressedin terms of ngramsagree-
ment
137
Sys0 Sys1 Sys2 Sys3 Sys5 mean
Sys0 53.59 62.67 64.60 62.50 60.84
Sys1 53.51 54.19 52.42 51.69 52.95
Sys2 62.72 54.28 65.49 63.09 61.40
Sys3 64.63 52.51 65.47 61.35 60.99
Sys5 62.55 51.78 63.10 61.37 59.70
mean 60.85 53.04 61.36 60.97 59.66
Table 9: Cross-systemBLEU scores computed
on WMT’11French-Englishtest corpus outputs
(%BLEU-cased).
is thehighest,whereasthe”closest”systemgetthe
smallestweight.Thissuggeststhatsystemscloser
to other systemstends to be less useful for sys-
tem combination.Thisis an interestingbehaviour
whichhas to be exploreddeeperand validatedon
othertasksandcorpora.
5 MANYfor
frenchoutputs
After the evaluation period, some experiments
have been conductedin order to combinefrench
outputs. The main differencelie in the fact that
linguisticresourcesare not easilyor freelyavail-
able for that kind of language.Therefore,instead
of using TERp with relax3 shift constraint, the
strictconstraintwas used(shiftsoccuronlywhen
a matchis found).
Theavailabledataare detailedin the Table10.
NAME #sent. #words #tok
syscombtune 1003 24659 29171
syscombtest 2000 45372 53970
Table 10: Descriptionof WMT’11corpora for
systemcombinationin french.
The resultsobtainedare presentedin Table 11.
The BLEUscoreincreaseby morethan 0.8 point
but the TER score decreaseby 0.58. The metric
targeted during tuning is BLEU, which can ex-
plainthe improvementin that metric. Whendeal-
ingwithenglishtext, theonlycasewheresuchbe-
haviouris observedis whencombiningallsystems
(seeTable6.
6 MANYtechnicalnews
Several improvements have been performed on
MANY. The decoderis now basedon a fullylog-
3Shiftscan occurwhena match, a stem, a synonym or a
paraphraseis found.
Corpus syscombtune2011 syscombtest2011
BLEU TER BLEU TER
Sys0 35.99 49.16 34.36 49.78
Sys1 32.99 51.90 30.73 52.52
Sys2 32.41 52.77 29.85 53.61
Sys3 32.40 51.26 30.48 52.20
Sys4 32.30 52.21 31.02 52.49
MANY 36.81 49.74 34.51 50.54
Table 11: Systemsand combinationperformance
on WMT’11frenchdata(%BLEU-cased).
linear model (whereasbefore, the word and null
penaltieswere appliedlinearly). UsingMERT to
tune the decoderparametersis thereforepossible
andallowsto reachbiggerimprovementcompared
to usingCondor. This is probablydue to the fact
that MERT uses several heuristicsusefulfor tun-
ing on BLEUscore.
In order to facilitatethe use of MANY, it has
been integrated in the ExperimentManagement
System,EMS(Koehn,2010). Anexperimentcan
now be setup/modified/re-runeasilyby modifying
a singleconfigurationfile. Thedefaultbehaviorof
this framework is to perform3 runs of MERT in
parallel(usingtorque)and take the bestoptimiza-
tionrun. Apartfromavoidinglocalmaximum,the
procedureallows to see the variabilityof the opti-
mizationprocessand reportmore realisticresults
(forexample,by takingthe average).
7 Conclusionandfuture
work
ForWMT’11systemcombinationevaluationcam-
paign,severalrathertechnicalimprovementshave
been performedinto MANY. By homogenizing
the log-linearmodelused by the decoderand uti-
lizing MERT for tuning, MANY achieves im-
provements of more than 2 BLEU points on
WMT’09 data and about 1.3 BLEU point on
newssyscombtest2011relatively to the best single
system. Moreover, a dry-runoperatedon french
data shows a promisingresult with an improve-
mentof morethan0.8 BLEUpoints. Thiswill be
furtherexploredin the future.
MANY can benefit from various information.
At the moment,the decisiontaken by the decoder
mainlydependson a target languagemodel. This
is clearly not enough to achieve greater perfor-
mances. The next issueswhichwill be addressed
withinthe MANYframework is to estimategood
confidencemeasureto use in placeof the systems
138
priors.Theseconfidencesmeasureshave to be re-
lated to the systemperformances,but also to the
complementarityof the systemsconsidered.
8 Acknowledgement
This work has been partiallyfunded by the Eu-
ropeanUnion under the EuroMatrixPlus project
(http://www.euromatrixplus.net, IST-2007.2.2-
FP7-231720)
References
[Barrault,2010] Barrault, L. (2010). MANY :
Open sourcemachine translationsystemcom-
bination.PragueBulletinofMathematicalLin-
guistics,SpecialIssueonOpenSourceToolsfor
MachineTranslation, 93:147–155.
[BerghenandBersini,2005] Berghen, F. V. and
Bersini,H. (2005). CONDOR,a new parallel,
constrainedextension of Powell’s UOBYQA
algorithm: Experimentalresults and compari-
son with the DFOalgorithm.Journalof Com-
putationalandAppliedMathematics, 181:157–
175.
[Karakos et al., 2008] Karakos, D., Eisner, J.,
Khudanpur, S., and Dreyer, M. (2008). Ma-
chine translation system combination using
ITG-basedalignments.In46thAnnualMeeting
of the Associationfor ComputationalLinguis-
tics: Human Language Technologies., pages
81–84,Columbus, Ohio,USA.
[Koehn,2010] Koehn,P. (2010).Anexperimental
managementsystem. The Prague Bulletinof
MathematicalLinguistics, 94:87–96.
[Och,2003] Och, F. (2003). Minimumerror rate
training in statisticalmachinetranslation. In
ACL, Sapporo,Japan.
[Rostiet al., 2007] Rosti, A.-V., Matsoukas, S.,
and Schwartz,R. (2007). Improved word-level
system combinationfor machine translation.
In Associationfor ComputationalLinguistics,
pages312–319.
[Rostiet al., 2009] Rosti, A.-V., Zhang,B., Mat-
soukas, S., , and Schwartz, R. (2009). In-
cremental hypothesis alignment with flexi-
ble matchingfor buildingconfusionnetworks:
BBN system descriptionfor WMT09 system
combinationtask. In EACL/WMT, pages 61–
65.
[Shenet al., 2008] Shen, W., Delaney, B., An-
derson, T., and Slyh, R. (2008). The MIT-
LL/AFRLIWSLT-2008MT System. In Inter-
nationalWorkshoponSpokenLanguageTrans-
lation, Hawaii, U.S.A.
[Snover et al., 2009] Snover, M., Madnani, N.,
Dorr, B., and Schwartz, R. (2009). TER-Plus:
Paraphrase,semantic,and alignmentenhance-
mentsto translationedit rate. MachineTrans-
lationJournal.
139

