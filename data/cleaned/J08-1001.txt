ModelingLocalCoherence:
AnEntity-BasedApproach
ReginaBarzilay
∗
MassachusettsInstituteofTechnology
MirellaLapata
∗∗
UniversityofEdinburgh
Thisarticleproposesanovelframeworkforrepresentingandmeasuringlocalcoherence.Central
to this approach is the entity-grid representation of discourse, which captures patterns of entity
distribution in a text. The algorithm introduced in the article automatically abstracts a text
into a set of entity transition sequences and records distributional, syntactic, and referential
information about discourse entities. We re-conceptualize coherence assessment as a learning
task and show that our entity-based representation is well-suited for ranking-based generation
andtextclassiﬁcationtasks.Usingtheproposedrepresentation,weachievegoodperformanceon
textordering,summarycoherenceevaluation,andreadabilityassessment.
1.Introduction
Akeyrequirementforanysystemthatproducestextisthecoherenceofitsoutput.
Notsurprisingly,avarietyofcoherencetheorieshavebeendevelopedovertheyears
(e.g., Mann and Thomson 1988; Grosz et al. 1995) and their principles have found
applicationinmanysymbolictextgenerationsystems(e.g.,ScottanddeSouza1990;
KibbleandPower2004).Theabilityofthesesystemstogeneratehighqualitytext,
almostindistinguishablefromhumanwriting,makestheincorporationofcoherence
theories in robust large-scale systems particularly appealing. The task is, however,
challengingconsideringthatmostpreviouseffortshavereliedonhandcraftedrules,
validonlyforlimiteddomains,withnoguaranteeofscalabilityorportability(Reiter
andDale2000).Furthermore,coherenceconstraintsareoftenembeddedincomplex
representations(e.g.,AsherandLascarides2003)whicharehardtoimplementina
robustapplication.
Thisarticlefocusesonlocalcoherence,whichcapturestextrelatednessatthelevel
ofsentence-to-sentencetransitions.Localcoherenceisundoubtedlynecessaryforglobal
coherenceandhasreceivedconsiderableattentionincomputationallinguistics(Foltz,
Kintsch,andLandauer1998;Marcu2000;Lapata2003;Althaus,Karamanis,andKoller
∗ ComputerScienceandArtiﬁcialIntelligenceLaboratory,MassachusettsInstituteofTechnology,32Vassar
Street,32-G468Cambridge,MA02139.E-mail:regina@csail.mit.edu.
∗∗ SchoolofInformatics,UniversityofEdinburgh,EH89LW,Edinburgh,UK.E-mail:mlap@inf.ed.ac.uk.
Submissionreceived:29November2005;revisedsubmissionreceived:6March2007;acceptedfor
publication:5May2007.
©2008AssociationforComputationalLinguistics
ComputationalLinguistics Volume34,Number1
2004;Karamanisetal.2004).Itisalsosupportedbymuchpsycholinguisticevidence.For
instance,McKoonandRatcliff(1992)arguethatlocalcoherenceistheprimarysourceof
inference-makingduringreading.
Thekeypremiseofourworkisthatthedistributionofentitiesinlocallycoher-
enttextsexhibitscertainregularities.Thisassumptionisnotarbitrary—someofthese
regularitieshavebeenrecognizedinCenteringTheory(Grosz,Joshi,andWeinstein
1995)andotherentity-basedtheoriesofdiscourse(e.g.,Givon1987;Prince1981).The
algorithmintroducedinthearticleautomaticallyabstractsatextintoasetofentitytran-
sitionsequences,arepresentationthatreﬂectsdistributional,syntactic,andreferential
informationaboutdiscourseentities.
We argue that the proposed entity-based representation of discourse allows us
tolearnthepropertiesofcoherenttextsfromacorpus,withoutrecoursetomanual
annotationorapredeﬁnedknowledgebase.Wedemonstratetheusefulnessofthisrep-
resentationbytestingitspredictivepowerinthreeapplications:textordering,automatic
evaluationofsummarycoherence,andreadabilityassessment.
Weformulatetheﬁrsttwoproblems—textorderingandsummaryevaluation—as
rankingproblems,andpresentanefﬁcientlylearnablemodelthatranksalternativeren-
deringsofthesameinformationbasedontheirdegreeoflocalcoherence.Suchamecha-
nismisparticularlyappropriateforgenerationandsummarizationsystemsastheycan
producemultipletextrealizationsofthesameunderlyingcontent,eitherbyvaryingpa-
rametervalues,orbyrelaxingconstraintsthatcontrolthegenerationprocess.Asystem
equipped with a ranking mechanism could compare the quality of the candidate
outputs,inmuchthesamewayspeechrecognizersemploylanguagemodelsatthe
sentencelevel.
Inthetext-orderingtaskouralgorithmhastoselectamaximallycoherentsen-
tenceorderfromasetofcandidatepermutations.Inthesummaryevaluationtask,
wecomparetherankingsproducedbythemodelagainsthumancoherencejudgments
elicitedforautomaticallygeneratedsummaries.Inbothexperiments,ourmethodyields
improvementsoverstate-of-the-artmodels.Wealsoshowthebeneﬁtsoftheentity-
basedrepresentationinareadabilityassessmenttask,wherethegoalistopredictthe
comprehensiondifﬁcultyofagiventext.Incontrasttoexistingsystemswhichfocuson
intra-sententialfeatures,weexplorethecontributionofdiscourse-levelfeaturestothis
task.Byincorporatingcoherencefeaturesstemmingfromtheproposedentity-based
representation,weimprovetheperformanceofastate-of-the-artreadabilityassessment
system(SchwarmandOstendorf2005).
Inthefollowingsection,weprovideanoverviewofentity-basedtheoriesoflo-
calcoherenceandoutlinepreviousworkonitscomputationaltreatment.Then,we
introduceourentity-basedrepresentation,anddeﬁneitslinguisticproperties.Inthe
subsequentsections,wepresentourthreeevaluationtasks,andreporttheresultsofour
experiments.Discussionoftheresultsconcludesthearticle.
2.RelatedWork
Ourapproachisinspiredbyentity-basedtheoriesoflocalcoherence,andiswell-suited
fordevelopingacoherencemetricinthecontextofaranking-basedtextgeneration
system.Weﬁrstsummarizeentity-basedtheoriesofdiscourse,andoverviewprevious
attemptsfortranslatingtheirunderlyingprinciplesintocomputationalcoherencemod-
els.Next,wedescriberankingapproachestonaturallanguagegenerationandfocuson
coherencemetricsusedincurrenttextplanners.
2
BarzilayandLapata ModelingLocalCoherence
2.1Entity-BasedApproachestoLocalCoherence
Linguistic Modeling. Entity-based accounts of local coherence have a long tradition
withinthelinguisticandcognitivescienceliterature(Kuno1972;Chafe1976;Halliday
andHasan1976;Karttunen1976;ClarkandHaviland1977;Prince1981;Grosz,Joshi,
andWeinstein1995).Aunifyingassumptionunderlyingdifferentapproachesisthat
discoursecoherenceisachievedinviewofthewaydiscourseentitiesareintroduced
anddiscussed.Thisobservationiscommonlyformalizedbydevisingconstraintsonthe
linguisticrealizationanddistributionofdiscourseentitiesincoherenttexts.
At any point in the discourse, some entities are considered more salient than
others, and consequently are expected to exhibit different properties. In Centering
Theory(Grosz,Joshi,andWeinstein1995;Walker,Joshi,andPrince1998;Strubeand
Hahn1999;Poesioetal.2004),salienceconcernshowentitiesarerealizedinanutterance
(e.g.,whethertheyaretheypronominalizedornot).Inothertheories,salienceisdeﬁned
intermsoftopicality(Chafe1976;Prince1978),predictability(Kuno1972;Hallidayand
Hasan1976),andcognitiveaccessibility(Gundel,Hedberg,andZacharski1993).More
reﬁnedaccountsexpandthenotionofsaliencefromabinarydistinctiontoascalarone;
examplesincludePrince’s(1981)familiarityscale,andGivon’s(1987)andAriel’s(1988)
givenness-continuum.
Thesaliencestatusofanentityisoftenreﬂectedinitsgrammaticalfunctionand
thelinguisticformofitssubsequentmentions.Saliententitiesaremorelikelytoap-
pearinprominentsyntacticpositions(suchassubjectorobject),andtobeintroduced
in a main clause. The linguistic realization of subsequent mentions—in particular,
pronominalization—issotightlylinkedtosaliencethatinsometheories(e.g.,Givon
1987)itprovidesthesolebasisfordeﬁningasaliencehierarchy.Thehypothesisisthat
thedegreeofunderspeciﬁcationinareferringexpressionindicatesthetopicalstatusof
itsantecedent(e.g.,pronounsrefertoverysaliententities,whereasfullNPsrefertoless
salientones).InCenteringTheory,thisphenomenoniscapturedinthePronoun Rule,
andGivon’sScale of TopicalityandAriel’sAccessibility Marking Scaleproposeagraded
hierarchyofunderspeciﬁcationthatrangesfromzeroanaphoratofullnounphrases,
andincludesstressedandunstressedpronouns,demonstrativeswithmodiﬁers,and
deﬁnitedescriptions.
Entity-basedtheoriescapturecoherencebycharacterizingthedistributionofen-
titiesacrossdiscourseutterances,distinguishingbetweensaliententitiesandtherest.
Theintuitionhereisthattextsaboutthesamediscourseentityareperceivedtobe
morecoherentthantextsfraughtwithabruptswitchesfromonetopictothenext.The
patterneddistributionofdiscourseentitiesisanaturalconsequenceoftopiccontinuity
observedinacoherenttext.CenteringTheoryformalizesﬂuctuationsintopiccontinuity
intermsoftransitionsbetweenadjacentutterances.Thetransitionsareranked,that
is,textsdemonstratingcertaintypesoftransitionsaredeemedmorecoherentthantexts
wheresuchtransitionsareabsentorinfrequent.Forexample, CONTINUEtransitions
require that two utterances have at least one entity in common and are preferred
overtransitionsthatrepeatedlySHIFTfromoneentitytotheother.Givon’s(1987)and
Hoey’s (1991) accounts of discourse continuity complement local measurements by
consideringglobalcharacteristicsofentitydistribution,suchasthelifetimeofanentity
indiscourseandthereferentialdistancebetweensubsequentmentions.
Computational Modeling.Animportantpracticalquestionishowtotranslateprinciples
oftheselinguistictheoriesintoarobustcoherencemetric.Agreatdealofresearch
hasbeendevotedtothisissue,primarilyinCenteringTheory(MiltsakakiandKukich
3
ComputationalLinguistics Volume34,Number1
2000;Hasler2004;Karamanisetal.2004).Suchtranslationischallenginginseveral
respects:onehastodeterminewaysofcombiningtheeffectsofvariousconstraintsand
toinstantiateparametersofthetheorythatareoftenleftunderspeciﬁed.Poesioetal.
(2004)notethatevenforfundamentalconceptsofCenteringTheorysuchas“utterance,”
“realization,”and“ranking,”multiple—andoftencontradictory—interpretationshave
beendevelopedovertheyears,becauseintheoriginaltheorytheseconceptsarenot
explicitlyﬂeshedout.Forinstance,insomeCenteringpapers,entitiesarerankedwith
respecttotheirgrammaticalfunction(Brennan,Friedman,andPollard1987;Walker,
Iida,andCote1994;Grosz,Joshi,andWeinstein1995),andinotherswithrespecttotheir
positioninPrince’s(1981)givennesshierarchy(StrubeandHahn1999)ortheirthematic
role(Sidner1979).Asaresult,two“instantiations”ofthesametheorymakedifferent
predictionsforthesameinput.Poesioetal.(2004)explorealternativespeciﬁcations
proposedintheliterature,anddemonstratethatthepredictivepowerofthetheoryis
highlysensitivetoitsparameterdeﬁnitions.
Acommonmethodologyfortranslatingentity-basedtheoriesintocomputational
modelsistoevaluatealternativespeciﬁcationsonmanuallyannotatedcorpora.Some
studiesaimtoﬁndaninstantiationofparametersthatismostconsistentwithobservable
data(StrubeandHahn1999;Karamanisetal.2004;Poesioetal.2004).Otherstudies
adoptaspeciﬁcinstantiationwiththegoalofimprovingtheperformanceofametricon
atask.Forinstance,MiltsakakiandKukich(2000)annotateacorpusofstudentessays
withentitytransitioninformation,andshowthatthedistributionoftransitionscorre-
lateswithhumangrades.Analogously,Hasler(2004)investigateswhetherCentering
Theorycanbeusedinevaluatingthereadabilityofautomaticsummariesbyannotating
humanandmachinegeneratedextractswithentitytransitioninformation.
Thepresentworkdiffersfromtheseapproachesingoalandmethodology.Although
ourworkbuildsuponexistinglinguistictheories,wedonotaimtodirectlyimplement
orreﬁneanyoftheminparticular.Weprovideourmodelwithsourcesofknowledge
identiﬁed as essential by these theories, and leave it to the inference procedure to
determinetheparametervaluesandanoptimalwaytocombinethem.Fromadesign
viewpoint,weemphasizeautomaticcomputationforboththeunderlyingdiscourse
representationandtheinferenceprocedure.Thus,ourworkiscomplementarytocom-
putationalmodelsdevelopedonmanuallyannotateddata(MiltsakakiandKukich2000;
Hasler2004;Poesioetal.2004).Automatic,albeitnoisy,featureextractionallowsus
toperformalargescaleevaluationofdifferentlyinstantiatedcoherencemodelsacross
genresandapplications.
2.2RankingApproachesinNaturalLanguageGeneration
Ranking approaches have enjoyed an increasing popularity at all stages in the
generation pipeline, ranging fromtext planning to surface realization (Knight and
Hatzivassiloglou1995;LangkildeandKnight1998;Mellishetal.1998;Walker,Rambow,
and Rogati 2001; Karamanis 2003; Kibble and Power 2004). In this framework, an
underlyingsystemproducesapotentiallylargesetofcandidateoutputs,withrespect
tovarioustextgenerationrulesencodedashardconstraints.Notalloftheresulting
alternativeswillcorrespondtowell-formedtexts,andofthosewhichmaybejudgedac-
ceptable,somewillbepreferabletoothers.Thecandidategenerationphaseisfollowed
byanassessmentphaseinwhichthecandidatesarerankedbasedonasetofdesirable
properties encoded in a ranking function. The top-ranked candidate is selected for
presentation.Atwo-stagegenerate-and-rankarchitecturecircumventsthecomplexity
4
BarzilayandLapata ModelingLocalCoherence
oftraditionalgenerationsystems,wherenumerous,oftenconﬂictingconstraints,have
tobeencodedduringdevelopmentinordertoproduceasinglehigh-qualityoutput.
Because the focus of our work is on text coherence, we discuss here rank-
ing approaches applied to text planning (see Walker et al. [2001] and Knight and
Hatzivassiloglou[1995]forrankingapproachestosentenceplanningandsurfacere-
alization,respectively).Thegoaloftextplanningistodeterminethecontentofatext
byselectingasetofinformation-bearingunitsandarrangingthemintoastructurethat
yieldswell-formedoutput.Dependingonthesystem,textplansarerepresentedasdis-
coursetrees(Mellishetal.1998)orlinearsequencesofpropositions(Karamanis2003).
Candidatetextstructuresmaydifferintermsoftheselectedpropositions,thesequence
inwhichfactsarepresented,thetopologyofthetree,ortheorderinwhichentitiesare
introduced.Asetofplausiblecandidatescanbecreatedviastochasticsearch(Mellish
etal.1998)orbyasymbolictextplannerfollowingdifferenttext-formationrules(Kibble
andPower2004).Thebestcandidateischosenusinganevaluationorrankingfunction
oftenencodingcoherenceconstraints.Althoughthetypeandcomplexityofconstraints
varygreatlyacrosssystems,theyarecommonlyinspiredbyRhetoricalStructureTheory
orentity-basedconstraintssimilartotheonescapturedbyourmethod.Forinstance,
therankingfunctionusedbyMellishetal.givespreferencetoplanswhereconsecutive
factsmentionthesameentitiesandissensitivetothesyntacticenvironmentinwhich
the entity is ﬁrst introduced (e.g., in a subject or object position). Karamanis ﬁnds
thatarankingfunctionbasedsolelyontheprincipleofcontinuityachievescompeti-
tive performance against more sophisticated alternatives when applied to ordering
shortdescriptionsofmuseumartifacts.
1
Inotherapplications,therankingfunctionis
morecomplex,integratingrulesfromCenteringTheoryalongwithstylisticconstraints
(KibbleandPower2004).
Acommonfeatureofcurrentimplementationsisthatthespeciﬁcationoftherank-
ingfunction—featureselectionandweighting—isperformedmanuallybasedonthe
intuitionofthesystemdeveloper.However,eveninalimiteddomainthistaskhas
provendifﬁcult.Mellishetal.(1998;page100)note:“Theproblemisfartoocomplex
andourknowledgeoftheissuesinvolvedsomeagerthatonlyatokengesturecanbe
madeatthispoint.”Moreover,theserankingfunctionsoperateoversemanticallyrich
inputrepresentationsthatcannotbecreatedautomaticallywithoutextensiveknowl-
edgeengineering.Theneedformanualcodingimpairstheportabilityofexistingmeth-
odsforcoherencerankingtonewapplications,mostnotablytotext-to-textgeneration
applications,suchassummarization.
Inthenextsection,wepresentamethodforcoherenceassessmentthatovercomes
theselimitations:Weintroduceanentity-basedrepresentationofdiscoursethatisauto-
maticallycomputedfromrawtext;wearguethattheproposedrepresentationreveals
entitytransitionpatternscharacteristicofcoherenttexts.Thelattercanbeeasilytrans-
latedintoalargefeaturespacewhichlendsitselfnaturallytotheeffectivelearningofa
rankingfunction,withoutexplicitmanualinvolvement.
3.TheCoherenceModel
Inthissectionwedescribeourentity-basedrepresentationofdiscourse.Weexplainhow
itiscomputedandhowentitytransitionpatternsareextracted.Wealsodiscusshow
1Eachutteranceinthediscoursereferstoatleastoneentityintheutterancethatprecedesit.
5
ComputationalLinguistics Volume34,Number1
thesepatternscanbeencodedasfeaturevectorsappropriateforperformingcoherence-
relatedrankingandclassiﬁcationtasks.
3.1TheEntity-GridDiscourseRepresentation
Each text is represented by an entity grid, a two-dimensional array that captures
thedistributionofdiscourseentitiesacrosstextsentences.WefollowMiltsakakiand
Kukich(2000)inassumingthatourunitofanalysisisthetraditionalsentence(i.e.,a
mainclausewithaccompanyingsubordinateandadjunctclauses).Therowsofthe
gridcorrespondtosentences,andthecolumnscorrespondtodiscourseentities.By
discourseentitywemeanaclassofcoreferentnounphrases(weexplaininSection3.3
howcoreferententitiesareidentiﬁed).Foreachoccurrenceofadiscourseentityinthe
text,thecorrespondinggridcellcontainsinformationaboutitspresenceorabsence
inasequenceofsentences.Inaddition,forentitiespresentinagivensentence,grid
cellscontaininformationabouttheirsyntacticrole.Suchinformationcanbeexpressed
inmanyways(e.g.,usingconstituentlabelsorthematicroleinformation).Because
grammaticalrelationsﬁgureprominentlyinentity-basedtheoriesoflocalcoherence(see
Section2),theyserveasalogicalpointofdeparture.Eachgridcellthuscorrespondsto
astringfromasetofcategoriesreﬂectingwhethertheentityinquestionisasubject(S),
object (O), or neither (X). Entities absent froma sentence are signaled by gaps ( –).
Grammaticalroleinformationcanbeextractedfromtheoutputofabroad-coverage
dependencyparser(Lin2001;BriscoeandCarroll2002)oranystate-of-theartstatistical
parser(Collins1997;Charniak2000).Wediscusshowthisinformationwascomputed
forourexperimentsinSection3.3.
Table1illustratesafragmentofanentitygridconstructedforthetextinTable2.
Becausethetextcontainssixsentences,thegridcolumnsareoflengthsix.Consider
forinstancethegridcolumnfortheentity trial, [O ––––X].Itrecordsthat trial is
presentinsentences1and6(asOandX,respectively)butisabsentfromtherestofthe
sentences.AlsonotethatthegridinTable1takescoreferenceresolutionintoaccount.
Eventhoughthesameentityappearsindifferentlinguisticforms,forexample,Microsoft
Corp., Microsoft,andthe company,itismappedtoasingleentryinthegrid(seethe
columnintroducedbyMicrosoftinTable1).
Table1
Afragmentoftheentitygrid.Nounphrasesarerepresentedbytheirheadnouns.Gridcells
correspondtogrammaticalroles:subjects(S),objects(O),orneither(X).
Department T
r
ial
Micr
osoft
Evidence Competitors Markets Pr
oducts
Brands Case Netscape Softwar
e
T
a
ctics
Government Suit Earnings
1 SOSXO––––––––––1
2 ––O ––XSO–––––––2
3 ––SO––––SOO––––3
4 ––S ––––––––S –––4
5 ––––––––––––SO– 5
6 – XS–––––––––––O 6
6
BarzilayandLapata ModelingLocalCoherence
Table2
Summaryaugmentedwithsyntacticannotationsforgridcomputation.
1 [TheJusticeDepartment]
S
isconductingan[anti-trusttrial]
O
against[MicrosoftCorp.]
X
with[evidence]
X
that[thecompany]
S
isincreasinglyattemptingtocrush[competitors]
O
.
2 [Microsoft]
O
isaccusedoftryingtoforcefullybuyinto[markets]
X
where[itsown
products]
S
arenotcompetitiveenoughtounseat[establishedbrands]
O
.
3 [Thecase]
S
revolvesaround[evidence]
O
of[Microsoft]
S
aggressivelypressuring
[Netscape]
O
intomerging[browsersoftware]
O
.
4 [Microsoft]
S
claims[itstactics]
S
arecommonplaceandgoodeconomically.
5 [Thegovernment]
S
mayﬁle[acivilsuit]
O
rulingthat[conspiracy]
S
tocurb[competition]
O
through[collusion]
X
is[aviolationoftheShermanAct]
O
.
6 [Microsoft]
S
continuestoshow[increasedearnings]
O
despite[thetrial]
X
.
Whenanounisattestedmorethanoncewithadifferentgrammaticalroleinthe
samesentence,wedefaulttotherolewiththehighestgrammaticalranking:subjectsare
rankedhigherthanobjects,whichinturnarerankedhigherthantherest.Forexample,
theentityMicrosoftismentionedtwiceinSentence1withthegrammaticalrolesx(for
Microsoft Corp.)ands(for the company),butisrepresentedonlybysinthegrid(see
Tables1and2).
3.2EntityGridsasFeatureVectors
Afundamentalassumptionunderlyingourapproachisthatthedistributionofentities
incoherenttextsexhibitscertainregularitiesreﬂectedingridtopology.Someofthese
regularities are formalized in Centering Theory as constraints on transitions of the
localfocusinadjacentsentences.Gridsofcoherenttextsarelikelytohavesomedense
columns(i.e.,columnswithjustafewgaps,suchas Microsoft inTable1)andmany
sparsecolumnswhichwillconsistmostlyofgaps(seemarketsandearningsinTable1).
Onewouldfurtherexpectthatentitiescorrespondingtodensecolumnsaremoreoften
subjectsorobjects.Thesecharacteristicswillbelesspronouncedinlow-coherencetexts.
InspiredbyCenteringTheory,ouranalysisrevolvesaroundpatternsoflocalentity
transitions.A local entity transition isasequence {S,O,X,–}
n
thatrepresentsentity
occurrencesandtheirsyntacticrolesinnadjacentsentences.Localtransitionscanbe
easilyobtainedfromagridascontinuoussubsequencesofeachcolumn.Eachtransition
will have a certain probability in a given grid. For instance, the probability of the
transition [S –]inthegridfromTable1is0 .08(computedasaratioofitsfrequency
[i.e.,six]dividedbythetotalnumberoftransitionsoflengthtwo[i.e.,75]).Eachtext
canthusbeviewedasadistributiondeﬁnedovertransitiontypes.
Wecannowgoonestepfurtherandrepresenteachtextbyaﬁxedsetoftransition
sequencesusingastandardfeaturevectornotation.Eachgridrenderingjofadocument
d
i
correspondstoafeaturevector Φ(x
ij
)=(p
1
(x
ij
),p
2
(x
ij
),...,p
m
(x
ij
)),where m isthe
numberofallpredeﬁnedentitytransitions,and p
t
(x
ij
)theprobabilityoftransition t
ingridx
ij
.Thisfeaturevectorrepresentationisusefullyamenabletomachinelearning
algorithms(seeourexperimentsinSections4–6).Furthermore,itallowstheconsid-
erationoflargenumbersoftransitionswhichcouldpotentiallyuncovernovelentity
distributionpatternsrelevantforcoherenceassessmentorothercoherence-relatedtasks.
Notethatconsiderablelatitudeisavailablewhenspecifyingthetransitiontypesto
beincludedinafeaturevector.Thesecanbealltransitionsofagivenlength(e.g.,two
orthree)orthemostfrequenttransitionswithinadocumentcollection.Anexampleof
7
ComputationalLinguistics Volume34,Number1
afeaturespacewithtransitionsoflengthtwoisillustratedinTable3.Thesecondrow
(introducedbyd
1
)isthefeaturevectorrepresentationofthegridinTable1.
3.3GridConstruction:LinguisticDimensions
Oneofthecentralresearchissuesindevelopingentity-basedmodelsofcoherenceis
determiningwhatsourcesoflinguisticknowledgeareessentialforaccurateprediction,
andhowtoencodethemsuccinctlyinadiscourserepresentation.Previousapproaches
tend to agree on the features of entity distribution related to local coherence—the
disagreementliesinthewaythesefeaturesaremodeled.
Our study of alternative encodings is not a mere duplication of previous ef-
forts(Poesioetal.2004)thatfocusonlinguisticaspectsofparameterization.Becausewe
areinterestedinanautomaticallyconstructedmodel,wehavetotakeintoaccountcom-
putationalandlearningissueswhenconsideringalternativerepresentations.Therefore,
ourexplorationoftheparameterspaceisguidedbythreeconsiderations:thelinguistic
importanceofaparameter,theaccuracyofitsautomaticcomputation,andthesizeofthe
resultingfeaturespace.Fromthelinguisticside,wefocusonpropertiesofentitydistri-
butionthataretightlylinkedtolocalcoherence,andatthesametimeallowformultiple
interpretationsduringtheencodingprocess.Computationalconsiderationspreventus
fromconsideringdiscourserepresentationsthatcannotbecomputedreliablybyexist-
ingtools.Forinstance,wecouldnotexperimentwiththegranularityofanutterance—
sentenceversusclause—becauseavailableclauseseparatorsintroducesubstantialnoise
intoagridconstruction.Finally,weexcluderepresentationsthatwillexplodethesizeof
thefeaturespace,therebyincreasingtheamountofdatarequiredfortrainingthemodel.
EntityExtraction.Theaccuratecomputationofentityclassesiskeytocomputingmean-
ingfulentitygrids.Inpreviousimplementationsofentity-basedmodels,classesofcoref-
erentnounshavebeenextractedmanually(MiltsakakiandKukich2000;Karamanis
etal.2004;Poesioetal.2004),butthisisnotanoptionforourmodel.Anobvious
solutionforidentifyingentityclassesistoemployanautomaticcoreferenceresolution
toolthatdetermineswhichnounphrasesrefertothesameentityinadocument.
Currentapproachesrecastcoreferenceresolutionasaclassiﬁcationtask.Apair
ofNPsisclassiﬁedascoreferringornotbasedonconstraintsthatarelearnedfrom
anannotatedcorpus.Aseparateclusteringmechanismthencoordinatesthepossibly
contradictorypairwiseclassiﬁcationsandconstructsapartitiononthesetofNPs.In
ourexperiments,weemployNgandCardie’s(2002)coreferenceresolutionsystem.
ThesystemdecideswhethertwoNPsarecoreferentbyexploitingawealthoflexical,
grammatical,semantic,andpositionalfeatures.ItistrainedontheMUC(6–7)datasets
andyieldsstate-of-the-artperformance(70.4F-measureonMUC-6and63.4onMUC-7).
Table3
Exampleofafeature-vectordocumentrepresentationusingalltransitionsoflengthtwogiven
syntacticcategoriesS,O,X,and–.
SS SO SX S– OS OO OX O– XS XO XX X––S – O – X ––
d
1
.01 .01 0 .08 .01 0 0 .09 0 0 0 .03 .05 .07 .03 .59
d
2
.02 .01 .01 .02 0 .07 0 .02 .14 .14 .06 .04 .03 .07 0.1 .36
d
3
.02 0 0 .03 .09 0 .09 .06 0 0 0 .05 .03 .07 .17 .39
8
BarzilayandLapata ModelingLocalCoherence
Althoughmachinelearningapproachestocoreferenceresolutionhavebeenrea-
sonably successful—state-of-the-art coreference tools today reach an F-measure
2
of
70%whentrainedonnewspapertexts—itisunrealistictoassumethatsuchtoolswill
bereadilyavailablefordifferentdomainsandlanguages.Wethereforeconsideran
additionalapproachtoentityextractionwhereentityclassesareconstructedsimplyby
clusteringnounsonthebasisoftheiridentity.Inotherwords,eachnouninatextcor-
respondstoadifferententityinagrid,andtwonounsareconsideredcoreferentonlyif
theyareidentical.UnderthisviewMicrosoftCorp.fromTable2(Sentence1)corresponds
totwoentities, Microsoft and Corp.,whichareinturndistinctfrom the company.This
approachisonlyaroughapproximationtofullyﬂedgedcoreferenceresolution,butit
issimplefromanimplementationalperspectiveandproducesconsistentresultsacross
domainsandlanguages.
GrammaticalFunction.Severalentity-basedapproachesassertthatgrammaticalfunction
isindicativeofanentity’sprominenceindiscourse(Hudson,Tanenhaus,andDell1986;
Kameyama1986;Brennan,Friedman,andPollard1987;Grosz,Joshi,andWeinstein
1995).Mosttheoriesdiscriminatebetweensubject,object,andtheremaininggrammati-
calroles:subjectsarerankedhigherthanobjects,andthesearerankedhigherthanother
grammaticalfunctions.
In our framework, we can easily assess the impact of syntactic knowledge by
modifyinghowtransitionsarerepresentedintheentitygrid.Insyntacticallyaware
grids,transitionsareexpressedbyfourcategories:s,o,xand–,whereasinsimpliﬁed
grids,weonlyrecordwhetheranentityispresent(x)orabsent(–)inasentence.
Weemployarobuststatisticalparser(Collins1997)todeterminetheconstituent
structureforeachsentence,fromwhichsubjects(s),objects(o),andrelationsotherthan
subjectorobject(x)areidentiﬁed.Thephrase-structureoutputofCollins’sparseris
transformedintoadependencytreefromwhichgrammaticalrelationsareextracted.
Passiveverbsarerecognizedusingasmallsetofpatterns,andtheunderlyingdeep
grammaticalroleforargumentsinvolvedinthepassiveconstructionisenteredinthe
grid(seethegridcelloforMicrosoft,Sentence2,Table2).Formoredetailsonthegram-
maticalrelationsextractioncomponentwerefertheinterestedreadertoBarzilay(2003).
Salience. Centeringandotherdiscoursetheoriesconjecturethatthewayanentityis
introducedandmentioneddependsonitsglobalroleinagivendiscourse.Weevaluate
theimpactofsalienceinformationbyconsideringtwotypesofmodels:Theﬁrstmodel
treatsallentitiesuniformly,whereasthesecondonediscriminatesbetweentransitions
ofsaliententitiesandtherest.Weidentifysaliententitiesbasedontheirfrequency,
3
fol-
lowingthewidelyacceptedviewthatfrequencyofoccurrencecorrelateswithdiscourse
prominence(Givon1987;Ariel1988;Hoey1991;MorrisandHirst1991).
Toimplementasalience-basedmodel,wemodifyourfeaturegenerationproce-
durebycomputingtransitionprobabilitiesforeachsaliencegroupseparately,andthen
2Whenevaluatingtheoutputofcoreferencealgorithms,performanceistypicallymeasuredusinga
model-theoreticscoringschemeproposedinVilainetal.(1995).Thescoringalgorithmcomputesthe
recallerrorbytakingeachequivalenceclassSinthegoldstandardanddeterminingthenumberof
coreferencelinksmthatwouldhavetobeaddedtothesystem’soutputtoplaceallentitiesinSinto
thesameequivalenceclassproducedbythesystem.Recallerrorthenisthesumofmsdividedbythe
numberoflinksinthegoldstandard.Precisionerroriscomputedbyreversingtherolesofthegold
standardandsystemoutput.
3Thefrequencythresholdisempiricallydeterminedonthedevelopmentset.SeeSection4.2forfurther
discussion.
9
ComputationalLinguistics Volume34,Number1
combiningthemintoasinglefeaturevector.Forntransitionswithk salienceclasses,
thefeaturespacewillbeofsizen×k.Whilewecaneasilybuildamodelwithmultiple
salienceclasses,weoptforabinarydistinction(i.e., k=2).Thisismoreinlinewith
theoretical accounts of salience (Chafe 1976; Grosz, Joshi, and Weinstein 1995) and
resultsinamoderatefeaturespaceforwhichreliableparameterestimationispossible.
Consideringalargenumberofsalienceclasseswouldunavoidablyincreasethenumber
offeatures.Parameterestimationinsuchaspacerequiresalargesampleoftraining
examplesthatisunavailableformostdomainsandapplications.
Differentclassesofmodelscanbedeﬁnedalongthelinguisticdimensionsjustdis-
cussed.Ourexperimentswillconsiderseveralmodelswithvaryingdegreesoflinguistic
complexity,whileattemptingtostrikeabalancebetweenexpressivityofrepresentation
andeaseofcomputation.Inthefollowingsectionsweevaluatetheirperformanceon
threetasks:sentenceordering,summarycoherencerating,andreadabilityassessment.
3.4Learning
Equippedwiththefeaturevectorrepresentationintroducedherein,wecanviewco-
herenceassessmentasamachinelearningproblem.Whenconsideringtextgeneration
applications,itisdesirabletorankratherthanclassifyinstances:Thereisoftennosingle
coherentrenderingofagiventextbutmanydifferentpossibilitiesthatcanbepartially
ordered.Itisthereforenotsurprisingthatsystemsoftenemployscoringfunctionsto
selectthemostcoherentoutputamongalternativerenderings(seethediscussionin
Section2.2).Inthisarticlewearguethatencodingtextsasentitytransitionsequences
constitutesanappropriatefeaturesetfor learning (ratherthanmanuallyspecifying)
sucharankingfunction(seeSection4fordetails).Wepresenttwotask-basedexper-
imentsthatputthishypothesistothetest:informationordering(Experiment1)and
summarycoherencerating(Experiment2).Bothtaskscanbenaturallyformulatedas
ranking problems; the learner takes as input a set of alternative renderings of the
samedocumentandranksthembasedontheirdegreeoflocalcoherence.Examples
ofsuchrenderingsareasetofdifferentsentenceorderingsofthesametextandaset
ofsummariesproducedbydifferentsystemsforthesamedocument.Notethatinboth
rankingexperimentsweassumethatthealgorithmisprovidedwithalimitednumber
ofalternatives.Inpractice,thespaceofcandidatescanbevast,andﬁndingtheoptimal
candidatemayrequirepairingourrankingalgorithmwithadecodersimilartotheones
usedinmachinetranslation(Germannetal.2004).
Althoughthemajorityofourexperimentsfallwithinthegenerate-and-rankframe-
workpreviouslysketched,nothingpreventstheuseofourfeaturevectorrepresentation
forconventionalclassiﬁcationtasks.WeofferanillustrationinExperiment3,where
featuresextractedfromentitygridsareusedtoenhancetheperformanceofareadability
assessmentsystem.Here,thelearnertakesasinputasetofdocumentslabeledwith
discreteclasses(e.g.,denotingwhetheratextisdifﬁcultoreasytoread)andlearnsto
makepredictionsforunseeninstances(seeSection6fordetailsonthemachinelearning
paradigmweemploy).
4.Experiment1:SentenceOrdering
Textstructuringalgorithms(Lapata2003;BarzilayandLee2004;Karamanisetal.2004)
arecommonlyevaluatedbytheirperformanceatinformation-ordering.Thetaskcon-
cernsdeterminingasequenceinwhichtopresentapre-selectedsetofinformation-
10
BarzilayandLapata ModelingLocalCoherence
bearingitems;thisisanessentialstepinconcept-to-textgeneration,multi-document
summarization,andothertext-synthesisproblems.Theinformationbearingitemscan
bedatabaseentries(Karamanisetal.2004),propositions(Mellishetal.1998)orsen-
tences(Lapata2003;BarzilayandLee2004).Insentenceordering,adocumentisviewed
asabagofsentencesandthealgorithm’staskistotrytoﬁndtheorderingwhich
maximizescoherenceaccordingtosomecriterion(e.g.,theprobabilityofanorder).
Asexplainedpreviously,weuseourcoherencemodeltorankalternativesentence
orderingsinsteadoftryingtoﬁndanoptimalordering.Wedonotassumethatlocal
coherenceissufﬁcienttouniquelydetermineamaximallycoherentordering—other
constraintsclearlyplayarolehere.Itisneverthelessakeypropertyofwell-formed
text(documentslackinglocalcoherencearenaturallygloballyincoherent),andamodel
which takes it into account should be able to discriminate coherent from incoher-
enttexts.Inoursentence-orderingtaskwegeneraterandompermutationsofatest
documentandmeasurehowoftenapermutationisrankedhigherthantheoriginal
document.Anon-deﬁcientmodelshouldprefertheoriginaltextmorefrequentlythan
itspermutations(seeSection4.2fordetails).
Webeginbyexplaininghowarankingfunctioncanbelearnedforthesentence
orderingtask.Next,wegivedetailsregardingthecorpususedforourexperiments,
describethemethodsusedforcomparisonwithourapproach,andnotetheevaluation
metricemployedforassessingmodelperformance.OurresultsarepresentedinSec-
tion4.3.
4.1Modeling
Ourtrainingsetconsistsoforderedpairsofalternativerenderings(x
ij,x
ik
)ofthesame
documentd
i,wherex
ij
exhibitsahigherdegreeofcoherencethanx
ik
(wedescribein
Section4.2howsuchtraininginstancesareobtained).Withoutlossofgenerality,we
assumej > k.Thegoalofthetrainingprocedureistoﬁndaparametervectorwthat
yieldsa“rankingscore”functionwhichminimizesthenumberofviolationsofpairwise
rankingsprovidedinthetrainingset
∀(x
ij,x
ik
)∈ r
∗
:w·Φ(x
ij
)> w·Φ(x
ik
)
where(x
ij,x
ik
)∈ r
∗
if x
ij
isrankedhigherthan x
ik
fortheoptimalranking r
∗
(inthe
training data), and Φ(x
ij
)andΦ(x
ik
) are a mapping onto features representing the
coherencepropertiesofrenderings x
ij
and x
ik
.Inourcasethefeaturescorrespondto
theentitytransitionprobabilitiesintroducedinSection3.2.Thus,theidealranking
function,representedbytheweightvectorwwouldsatisfythecondition
w·(Φ(x
ij
)−Φ(x
ik
))>0∀j,i,ksuchthatj > k
TheproblemistypicallytreatedasaSupportVectorMachineconstraintoptimization
problem,andcanbesolvedusingthesearchtechniquedescribedinJoachims(2002).
Thisapproachhasbeenshowntobehighlyeffectiveinvarioustasksrangingfrom
collaborativeﬁltering(Joachims2002)toparsing(Toutanova,Markova,andManning
2004).Otherdiscriminativeformulationsoftherankingproblemarepossible(Collins
2002;Freundetal.2003);however,weleavethistofuturework.
11
ComputationalLinguistics Volume34,Number1
Table4
ThesizeofthetrainingandtestinstancesfortheEarthquakesandAccidentscorpora(measured
bythenumberofpairsthatcontaintheoriginalorderandarandompermutationofthisorder).
Training Testing
Earthquakes 1,896 2,056
Accidents 2,095 2,087
Oncetherankingfunctionislearned,unseenrenderings(x
ij,x
ik
)ofdocumentd
i
canberankedsimplybycomputingthevaluesw
∗
Φ(x
ij
)andw
∗
Φ(x
ik
)andsortingthem
accordingly.Here,w
∗
istheoptimizedparametervectorresultingfromtraining.
4.2Method
Data. Toacquirealargecollectionfortrainingandtesting,wecreatesyntheticdata,
whereinthecandidatesetconsistsofasourcedocumentandpermutationsofitssen-
tences.Thisframeworkfordataacquisitionenableslarge-scaleautomaticevaluation
andiswidelyusedinassessingorderingalgorithms(Karamanis2003;Lapata2003;
Althaus,Karamanis,andKoller2004;BarzilayandLee2004).Theunderlyingassump-
tionisthattheoriginalsentenceorderinthesourcedocumentmustbecoherent,and
soweshouldprefermodelsthatrankithigherthanotherpermutations.Becausewe
donotknowtherelativequalityofdifferentpermutations,ourcorpusincludesonly
pairwiserankingsthatcomprisetheoriginaldocumentandoneofitspermutations.
Givenkoriginaldocuments,eachwithnrandomlygeneratedpermutations,weobtain
k·n(trivially)annotatedpairwiserankingsfortrainingandtesting.
Usingthetechniquedescribedherein,wecollecteddata
4
intwodifferentgenres:
newspaperarticlesandaccidentreportswrittenbygovernmentofﬁcials.Theﬁrstcol-
lectionconsistsofAssociatedPressarticlesfromtheNorthAmericanNewsCorpus
onthetopicofearthquakes(Earthquakes).Thesecondincludesnarrativesfromthe
NationalTransportationSafetyBoard’saviationaccidentdatabase(Accidents).Both
corporahavedocumentsofcomparablelength—theaveragenumberofsentencesis10.4
and11.5,respectively.Foreachset,weused100sourcearticleswithupto20randomly
generatedpermutationsfortraining.
5
Asimilarmethodwasusedtoobtainthetestdata.
Table4showsthesizeofthetrainingandtestcorporausedinourexperiments.Weheld
out10documents(i.e.,200pairwiserankings)fromthetrainingdatafordevelopment
purposes.
Features and Parameter Settings. In order to investigate the contribution oflinguistic
knowledgeonmodelperformanceweexperimentedwithavarietyofgridrepresenta-
tionsresultingindifferentparameterizationsofthefeaturespacefromwhichourmodel
islearned.Wefocusedonthreesourcesoflinguisticknowledge—syntax,coreference
resolution,andsalience—whichplayaprominentroleinentity-basedanalysesofdis-
4Thecollectionsareavailablefromhttp://people.csail.mit.edu/regina/coherence/.
5Shorttextsmayhavelessthan20permutations.ThecorpusdescribedintheoriginalACLpublication
(BarzilayandLapata2005)containedanumberofduplicatepermutations.Thesewereremovedfrom
thecurrentversionofthecorpus.
12
BarzilayandLapata ModelingLocalCoherence
coursecoherence(seeSection3.3fordetails).Anadditionalmotivationforourstudy
wastoexplorethetrade-offbetweenrobustnessandrichnessoflinguisticannotations.
NLPtoolsaretypicallytrainedonhuman-authoredtexts,andmaydeteriorateinper-
formancewhenappliedtoautomaticallygeneratedtextswithcoherenceviolations.
Wethuscomparedalinguisticallyrichmodelagainstmodelsthatusemoreim-
poverishedrepresentations.Moreconcretely,ourfullmodel(Coreference+Syntax+
Salience+)usescoreferenceresolution,denotesentitytransitionsequencesviagram-
matical roles, and differentiates between salient and non-salient entities. Our less-
expressive models (seven in total) use only a subset of these linguistic features
during the grid construction process. We evaluated the effect of syntactic knowl-
edgebyeliminatingtheidentiﬁcationofgrammaticalrelationsandrecordingsolely
whether an entity is present or absent in a sentence. This process created a class
of four models of the form Coreference[+/−]Syntax−Salience[+/−]. The effect of
fully ﬂedged coreference resolution was assessed by creating models where entity
classes were constructed simply by clustering nouns on the basis of their identity
(Coreference−Syntax[+/−]Salience[+/−]). Finally, the contribution of salience was
measuredbycomparingthefullmodelwhichaccountsseparatelyforpatternsofsalient
andnon-saliententitiesagainstmodelsthatdonotattempttodiscriminatebetween
them(Coreference[+/−]Syntax[+/−]Salience−).
Wewouldliketonotethatinthisexperimentweapplyacoreferenceresolutiontool
totheoriginaltextandthengeneratepermutationsforthepairwiserankingtask.An
alternativedesignistoapplycoreferenceresolutiontopermutedtexts.Becauseexisting
methodsforcoreferenceresolutiontakeintoconsiderationtheorderofnounphrasesin
atext,theaccuracyofthesetoolsonpermutedsentencesequencesisclosetorandom.
Therefore,weopttoresolvecoreferencewithintheoriginaltext.Althoughthisdesign
hasanoraclefeeltoit,itisnotuncommoninpracticalapplications.Forinstance,intext
generationsystems,contentplannersoftenoperateoverfullyspeciﬁedsemanticrep-
resentations,andcanthustakeadvantageofcoreferenceinformationduringsentence
ordering.
Besidesvariationsintheunderlyinglinguisticrepresentation,ourmodelisalso
speciﬁedbytwofreeparameters:thefrequencythresholdusedtoidentifysalienten-
titiesandthelengthofthetransitionsequence.Theseparametersweretunedseparately
foreachdatasetonthecorrespondingheld-outdevelopmentset.Optimalsalience-
basedmodelswereobtainedforentitieswithfrequency ≥2.Theoptimaltransition
lengthwas≤3.
6
Inourorderingexperiments,weusedJoachims’s(2002)SVM
light
packagefortrain-
ingandtestingwithallparameterssettotheirdefaultvalues.
Comparison with State-of-the-Art Methods. Wecomparedtheperformanceofouralgo-
rithmagainsttwostate-of-the-artmodelsproposedbyFoltz,Kintsch,andLandauer
(1998)andBarzilayandLee(2004).Thesemodelsrelylargelyonlexicalinformation
forassessingdocumentcoherence,contrarytoourmodelswhichareinessenceun-
lexicalized.RecallfromSection3thatourapproachcaptureslocalcoherencebymod-
elingpatternsofentitydistributionindiscourse,withouttakingnoteoftheirlexical
instantiations.Inthefollowingwebrieﬂydescribethelexicalizedmodelsweemployed
inourcomparativestudyandmotivatetheirselection.
6Themodelsweusedinourexperimentsareavailablefromhttp://people.csail.mit.edu/
regina/coherence/andhttp://homepages.inf.ed.ac.uk/mlap/coherence/.
13
ComputationalLinguistics Volume34,Number1
Foltz,Kintsch,andLandauer(1998)modelmeasurescoherenceasafunctionof
semanticrelatednessbetweenadjacentsentences.Theunderlyingintuitionhereisthat
coherent texts will contain a high number of semantically related words. Semantic
relatednessiscomputedautomaticallyusingLatentSemanticAnalysis(LSA;Landauer
andDumais1997)fromrawtextwithoutemployingsyntacticorotherannotations.In
thisframework,aword’smeaningiscapturedinamulti-dimensionalspacebyavector
representingitsco-occurrencewithneighboringwords.Co-occurrenceinformationis
collectedinafrequencymatrix,whereeachrowcorrespondstoauniqueword,andeach
columnrepresentsagivenlinguisticcontext(e.g.,sentence,document,orparagraph).
Foltz,Kintsch,andLandauer’smodelusesingularvaluedecomposition(SVD;Berry,
Dumais,andO’Brien1994)toreducethedimensionalityofthespace.Thetransforma-
tionrenderssparsematricesmoreinformativeandcanbethoughtofasameansof
uncoveringlatentstructureindistributionaldata.Themeaningofasentenceisnext
representedasavectorbytakingthemeanofthevectorsofitswords.Thesimilarity
betweentwosentencesisdeterminedbymeasuringthecosineoftheirmeans:
sim(S
1,S
2
)=cos(µ(
vector
S
1
),µ(
vector
S
2
))
=
n
summationtext
j=1
µ
j
(
vector
S
1
)µ
j
(
vector
S
2
)
radicalBigg
n
summationtext
j=1
(µ
j
(
vector
S
1
))
2
radicalBigg
n
summationtext
j=1
(µ
j
(
vector
S
2
))
2
(1)
where µ(
vector
S
i
)=
1
|S
i
|
summationtext
vectoru∈S
i
vectoru,andvectoru isthevectorforword u.Anoveralltextcoherence
measurecanbeeasilyobtainedbyaveragingthecosinesforallpairsofadjacentsen-
tencesS
i
andS
i+1
:
coherence(T)=
n−1
summationtext
i=1
cos(S
i,S
i+1
)
n−1
(2)
Thismodelisagoodpointofcomparisonforseveralreasons:(a)itisfullyautomatic
andhasrelativelyfewparameters(i.e.,thedimensionalityofthespaceandthechoiceof
similarityfunction),(b)itcorrelatesreliablywithhumanjudgmentsandhasbeenused
toanalyzediscoursestructure,and(c)itmodelsanaspectoflocalcoherencewhichis
orthogonaltoours.TheLSAmodelislexicalized:coherenceamountstoquantifyingthe
degreeofsemanticsimilaritybetweensentences.Incontrast,ourmodeldoesnotincor-
porateanynotionofsimilarity:coherenceisencodedintermsoftransitionsequences
thataredocument-speciﬁcratherthansentence-speciﬁc.
Our implementation of the LSA model followed closely Foltz, Kintsch, and
Landauer (1998).Weconstructed vector-based representations forindividual words
fromalemmatizedversionoftheNorthAmericanNewsCorpus
7
(350millionwords)
usingaterm–document matrix.WeusedSVDtoreducethesemanticspaceto100
dimensionsobtainingthusaspacesimilartoLSA.Weestimatedthecoherenceofadoc-
umentusingEquations(1)and(2).Arankingcanbetriviallyinferredbycomparingthe
7Ourselectionofthiscorpuswasmotivatedbytwofactors:(a)thecorpusislargeenoughtoyielda
reliablesemanticspace,and(b)itconsistsofnewsstoriesandisthereforesimilarinstyle,vocabulary,
andcontenttomostofthecorporaemployedinourcoherenceexperiments.
14
BarzilayandLapata ModelingLocalCoherence
coherencescoreassignedtotheoriginaldocumentagainsteachofitspermutations.Ties
areresolvedrandomly.
BothLSAandourentity-gridmodelarelocal—theymodelsentence-to-sentence
transitionswithoutbeingawareofglobaldocumentstructure.Incontrast,thecontent
modelsdevelopedbyBarzilayandLee(2004)learntorepresentmoreglobaltextprop-
ertiesbycapturingtopicsandtheorderinwhichthesetopicsappearintextsfromthe
samedomain.Forinstance,atypicalearthquakenewspaperreportcontainsinformation
aboutthequake’sepicenter,howmuchitmeasured,thetimeitwasfelt,andwhether
therewereanyvictimsordamage.Byencodingconstraintsontheorderingofthese
topics,contentmodelshaveapronouncedadvantageinmodelingdocumentstructure
becausetheycanlearntorepresenthowdocumentsbeginandend,butalsohowthe
discourseshiftsfromonetopictothenext.LikeLSA,thecontentmodelsarelexicalized;
however,unlikeLSA,theyaredomain-speciﬁc,andwouldexpectedlyyieldinferior
performanceonout-of-domaintexts.
BarzilayandLee(2004)implementedcontentmodelsusinganHMMwhereinstates
correspondtodistincttopics(forinstance,theepicenterofanearthquakeorthenumber
ofvictims),andstatetransitionsrepresenttheprobabilityofchangingfromonetopic
toanother,therebycapturingpossibletopic-presentationorderingswithinadomain.
Topicsrefertotextspansofvaryinggranularityandlength.BarzilayandLeeused
sentencesintheirexperiments,butclausesorparagraphswouldalsobepossible.
BarzilayandLee(2004)employedtheircontentmodelstoﬁndahigh-probability
orderingforadocumentwhosesentenceshadbeenrandomlyshufﬂed.Here,weuse
contentmodelsforthesimplercoherencerankingtask.Giventwotextpermutations,
weestimatetheirlikelihoodaccordingtotheirHMMmodelandselectthetextwiththe
highestprobability.Becausethetwocandidatescontainthesamesetofsentences,the
assumptionisthatamoreprobabletextcorrespondstoanorderingthatismoretypical
forthedomainofinterest.
Inourexperiments,webuilttwocontentmodels,onefortheAccidentscorpusand
onefortheEarthquakecorpus.Althoughthesemodelsaretrainedinanunsupervised
fashion,anumberofparametersrelatedtothemodeltopology(i.e.,numberofstates
andsmoothingparameters)affecttheirperformance.Theseparametersweretunedon
thedevelopmentsetandchosensoastooptimizethemodels’performanceonthe
pairwiserankingtask.
Evaluation Metric.Givenasetofpairwiserankings(anoriginaldocumentandoneof
itspermutations),wemeasureaccuracyastheratioofcorrectpredictionsmadebythe
modeloverthesizeofthetestset.Inthissetup,randompredictionresultsinanaccuracy
of50%.
4.3Results
Impact of Linguistic Representation.Weﬁrstinvestigatehowdifferenttypesoflinguistic
knowledgeinﬂuenceourmodel’sperformance.Table5showstheaccuracyontheor-
deringtaskwhenthemodelistrainedondifferentgridrepresentations.Ascanbeseen,
inbothdomains,thefullmodelCoreference+Syntax+Salience+signiﬁcantlyoutper-
formsalinguisticallynaivemodelwhichsimplyrecordsthepresence(andabsence)
ofentitiesindiscourse(Coreference−Syntax−Salience−).Moreover,weobservethat
linguistically impoverished models consistently perform worse than their linguisti-
callyelaboratecounterparts.Weassesswhetherdifferencesinaccuracyarestatistically
15
ComputationalLinguistics Volume34,Number1
Table5
Accuracymeasuredasafractionofcorrectpairwiserankingsinthetestset.Coreference[+/−]
indicateswhethercoreferenceinformationhasbeenusedintheconstructionoftheentitygrid.
Similarly,Syntax[+/−]andSalience[+/−]reﬂecttheuseofsyntacticandsalienceinformation.
Diacritics
**
(p<.01)and
*
(p<.05)indicatewhetherdifferencesinaccuracybetweenthefull
model(Coreference+Syntax+Salience+)andallothermodelsaresigniﬁcant(usingaFisher
Signtest).
Model Earthquakes Accidents
Coreference+Syntax+Salience+ 87.2 90.4
Coreference+Syntax+Salience− 88.3 90.1
Coreference+Syntax−Salience+ 86.6 88.4
∗∗
Coreference−Syntax+Salience+ 83.0
∗∗
89.9
Coreference+Syntax−Salience− 86.1 89.2
Coreference−Syntax+Salience− 82.3
∗∗
88.6
∗
Coreference−Syntax−Salience+ 83.0
∗∗
86.5
∗∗
Coreference−Syntax−Salience− 81.4
∗∗
86.0
∗∗
HMM-basedContentModels 88.0 75.8
∗∗
LatentSemanticAnalysis 81.0
∗∗
87.3
∗∗
signiﬁcantusingaFisherSignTest.Speciﬁcally,wecomparethefullmodelagainsteach
ofthelessexpressivemodels(seeTable5).
Let us ﬁrst discuss in more detail how the contribution of different knowl-
edge sources varies across domains. On the Earthquakes corpus every model that
doesnotusecoreferenceinformation(Coreference−Syntax[+/−]Salience[+/−])per-
forms signiﬁcantly worse than models augmented with coreference (Coreference+
Syntax[+/−]Salience[+/−]).ThiseffectislesspronouncedontheAccidentscorpus,
especially for model Coreference−Syntax+Salience+ whose accuracy drops only
by 0.5% (the difference between Coreference−Syntax+Salience+ and Coreference+
Syntax+Salience+isnotstatisticallysigniﬁcant).Thesamemodel’sperformancede-
creasesby4.2%ontheEarthquakescorpus.Thisvariationcanbeexplainedbydiffer-
encesinentityrealizationbetweenthetwodomains.Inparticular,thetwocorporavary
intheamountofcoreferencetheyemploy;textsfromtheEarthquakescorpuscontain
manyexamplesofreferringexpressionsthatoursimpleidentity-basedapproachcannot
possiblyresolve.ConsiderforinstancethetextinTable6.Here,theexpressions the
same area,the remote region,andsiteallrefertoMenglian county.Incomparison,thetext
fromtheAccidentscorpuscontainsfewerreferringexpressions,infactentitiesareoften
repeatedverbatimacrossseveralsentences,andthereforecouldbestraightforwardly
resolvedwithashallowapproach(seethepilot,thepilot,thepilotinTable6).
Theomissionofsyntacticinformationcausesadropinaccuracyformodelsapplied
totheAccidentscorpus.ThiseffectislessnoticeableontheEarthquakescorpus(com-
paretheperformanceofmodelCoreference+Syntax−Salience+onthetwocorpora).
Weexplainthisvariationbythesubstantialdifferenceinthetype/tokenratiobetween
thetwodomains—12.1forEarthquakesversus5.0forAccidents.Thelowtype/token
ratioforAccidentsmeansthatmostsentencesinatexthavesomewordsincommon.
Forexample,theentitiespilot,airplane,andairportappearinmultiplesentencesinthe
textfromTable6.Becausethereissomuchrepetitioninthisdomain,thesyntax-free
gridswillberelativelysimilarforbothcoherent(original)andincoherenttexts(permu-
tations).Infact,inspectionofthegridsfromtheAccidentscorpusrevealsthattheyhave
manysequencesoftheform[XXX],[X −−X],[XX−−],and[−−XX]incommon,
16
BarzilayandLapata ModelingLocalCoherence
Table6
TwotextsfromtheEarthquakesandAccidentscorpus.Oneentityclassforeachdocumentis
showntodemonstratethedifferenceinreferringexpressionsusedinthetwocorpora.
ExampleTextfromEarthquakes
A strong earthquake hit the China-Burma border early Wednesday morning, but there
were no reports of deaths, according to China’s Central Seismology Bureau. The 7.3 quake
hit
✞
✝
a4
✆
Mengliancounty at5:46am.
✄
✂
a0
✁
Thesamearea wasstruckbya6.2temblorearlyMonday
morning,thebureausaid.ThecountyisontheChina-Burmaborder,andisasparselypopulated,
mountainousregion.Thebureau’sXuWeisaidsomebuildingssustaineddamageandtherewere
someinjuries,buthehadnofurtherdetails.Communicationwith
✞
✝
a4
✆
theremoteregion isdifﬁcult,
andsatellitephonessentfromtheneighboringprovinceofSichuanhavenotyetreached
✄
✂
a0
✁
thesite.
However,hesaidthelikelihoodofdeathswaslowbecauseresidentsshouldhavebeenevacuated
from
✄
✂
a0
✁
thearea followingMonday’squake.
ExampleTextfromAccidents
When
✞
✝
a4
✆
thepilot failedtoarriveforhisbrother’scollegegraduation,concernedfamilymembers
reportedthatheandhisairplaneweremissing.Asearchwasinitiated,andtheCivilAirPatrol
locatedtheairplaneontopofPineMountain.Accordingto
✞
✝
a4
✆
thepilot’sﬂightlog,theintended
destinationwasPensacola,FL,withintermediatestopsforfuelatThomson,GA,andGreenville,
AL.AirportpersonalatThomsonconﬁrmedthattheairplanelandedabout1630on11/6/97.
Theyreportedthat
✞
✝
a4
✆
thepilot purchased26.5gallonsof100LLfuelanddepartedabout1700.
WitnessesattheThomsonAirportstatedthatwhenhetookoff,theweatherwasmarginalVFR
anddeterioratingrapidly.WitnessesnearPineMountainstatedthatthevisibilityatthetimeof
theaccidentwasabout1/4mileinhaze/fog.
whereassuchsequencesaremorecommonincoherentEarthquakesdocumentsand
moresparseintheirpermutations.Thisindicatesthatsyntax-freeanalysiscansufﬁ-
cientlydiscriminatecoherentfromincoherenttextsintheEarthquakesdomain,while
amorereﬁnedrepresentationofentitytransitiontypesisrequiredfortheAccidents
domain.
The contribution of salience is less pronounced in both domains—the differ-
enceinperformancebetweenthefullmodel(Coreference+Syntax+Salience+)and
its salience-agnostic counterpart (Coreference+Syntax+Salience+) is not statisti-
cally signiﬁcant. Salience-based models do deliver some beneﬁts for linguistically
impoverishedmodels—forinstance, Coreference−Syntax−Salience+ improvesover
Coreference−Syntax−Salience−(p<0.06)ontheEarthquakescorpus.Wehypothesize
thatthesmallcontributionofsalienceisrelatedtothewayitiscurrentlyrepresented.
Additionofthisknowledgesourcetoourgridrepresentation,doublesthenumber
of features that serve as input to the learning algorithm. In other words, salience-
awaremodelsneedtolearntwiceasmanyparametersassalience-freemodels,while
havingaccesstothesameamountoftrainingdata.Achievinganyimprovementinthese
conditionsischallenging.
ComparisonwithState-of-the-ArtMethods.WenextdiscusstheperformanceoftheHMM-
basedcontentmodels(BarzilayandLee2004)andLSA(Foltz,Kintsch,andLandauer
1998)incomparisontoourmodel(Coreference+Syntax+Salience+).
17
ComputationalLinguistics Volume34,Number1
First,notethattheentity-gridmodelsigniﬁcantlyoutperformsLSAonbothdo-
mains(p<.01usingaSigntest,seeTable5).Incontrasttoourmodel,LSAisnei-
therentity-basednorunlexicalized:Itmeasuresthedegreeofsemanticoverlapacross
successivesentences,withouthandlingdiscourseentitiesinaspecialway(allcontent
wordsinasentencecontributetowardsitsmeaning).Weattributeourmodel’ssuperior
performance,despitethelackoflexicalization,tothreefactors:(a)theuseofmore
elaboratelinguisticknowledge(coreferenceandgrammaticalroleinformation);(b)a
moreholisticrepresentationofcoherence(recallthatourentitygridsoperateovertexts
ratherthanindividualsentences;furthermore,entitytransitionscanspanmorethan
twoconsecutivesentences,somethingwhichisnotpossiblewiththeLSAmodel);and
(c)exposuretodomainrelevanttexts(theLSAmodelusedinourexperimentswasnot
particularlytunedtotheEarthquakesorAccidentscorpus).Oursemanticspacewas
createdfromalargenewscorpus(seeSection4.2)coveringawidevarietyoftopics
andwritingstyles.Thisisnecessaryforconstructingrobustvectorrepresentationsthat
arenotextremelysparse.Wethusexpectthegridmodelstobemoresensitivetothe
discourseconventionsofthetraining/testdata.
TheaccuracyoftheHMM-basedcontentmodesiscomparabletothegridmodelon
theEarthquakescorpus(thedifferenceisnotstatisticallysigniﬁcant)butissigniﬁcantly
lowerontheAccidentstexts(seeTable5).Althoughthegridmodelyieldssimilar
performanceonthetwodomains,contentmodelsexhibithighvariability.Theseresults
arenotsurprising.TheanalysispresentedinBarzilayandLee(2004)showsthatthe
Earthquakestextsarequiteformulaicintheirstructure,followingtheeditorialstyleof
theAssociatedPress.Incontrast,theAccidentstextsaremorechallengingforcontent
models—reportsinthissetdonotundergocentralizededitingandthereforeexhibit
morevariabilityinlexicalchoiceandstyle.TheLSAmodelalsosigniﬁcantlyoutper-
formsthecontentmodelontheEarthquakesdomain(p<.01usingaSigntest).Beinga
localmodel,LSAislesssensitivetothewaydocumentsarestructuredandistherefore
morelikelytodeliverconsistentperformanceacrossdomains.
ThecomparisoninTable5coversabroadspectrumofcoherencemodels.Atone
endofthespectrumisLSA,alexicalizedmodeloflocaldiscoursecoherencewhichis
fairlyrobustanddomainindependent.Inthemiddleofthespectrumliesourentity-
gridmodel,whichisunlexicalizedbutlinguisticallyinformedandgoesbeyondsim-
plesentence-to-sentencetransitionswithout,however,fullymodelingglobaldiscourse
structure.AttheotherendofthespectrumaretheHMM-basedcontentmodels,which
arebothglobalandlexicalized.Ourresultsindicatethatthesemodelsarecomplemen-
taryandthattheircombinationcouldyieldimprovedresults.Forexample,wecould
lexicalizeourentitygridsorsupplythecontentmodelswithlocalinformationeitherin
thestyleofLSAorasentitytransitions.However,weleavethistofuturework.
TrainingRequirements.Wenowexamineinmoredetailthetrainingrequirementsforthe
entity-gridmodels.Althoughforourorderingexperimentsweobtainedtrainingdata
cheaply,thiswillnotgenerallybethecaseandsomeeffortwillhavetobeinvested
incollectingappropriatedatawithcoherenceratings.Wethusaddresstwoquestions:
(1)Howmuchtrainingdataisrequiredforachievingsatisfactoryperformance?(2)How
domainsensitivearetheentity-gridmodels?Inotherwords,doestheirperformance
degradegracefullywhenappliedtoout-of-domaintexts?
Figure 1 shows learning curves for the best performing model (Coreference+
Syntax+Salience+)ontheEarthquakesandAccidentscorpora.Weobservethatthe
amountofdatarequireddependsonthedomainathand.TheAccidentstextsaremore
repetitiveandthereforelesstrainingdataisrequiredtoachievegoodperformance.The
18
BarzilayandLapata ModelingLocalCoherence
Figure1
Learningcurvesfortheentity-basedmodelCoreference+Syntax+Salience+onthe
EarthquakesandAccidentscorpora.
learningcurveissteeperfortheEarthquakesdocuments.Irrespectiveofthedomain
differences,themodelreachesgoodaccuracieswhenhalfofthedatasetisused(1,000
pairwiserankings).Thisisencouraging,becauseforsomeapplications(e.g.,summa-
rization)largeamountsoftrainingdatamaybenotreadilyavailable.
Table 7 illustrates the accuracy of the best performing model Coreference+
Syntax+Salience+whentrainedontheEarthquakescorpusandtestedonAccidents
textsandreverselywhentrainedontheAccidentcorpusandtestedonEarthquakes
documents.Wealsoillustratehowthismodelperformswhentrainedandtestedon
adatasetthatcontainstextsfrombothdomains.Forthelatterexperimentthetrain-
ing data set was created by randomly sampling 50 Earthquakes and 50 Accidents
documents.
Table7
Accuracyofentity-basedmodel(Coreference+Syntax+Salience+)andHHM-basedcontent
modelonout-of-domaintexts.Diacritics
**
(p<.01)and
*
(p<.05)indicatewhether
performancesonin-domainandout-of-domaindataaresigniﬁcantlydifferentusingaFisher
SignTest.
Coreference+Syntax+Salience
a80
a80
a80
a80
a80
a80
Train
Test Earthquakes Accidents
Earthquakes 87.3 67.0
∗∗
Accidents 69.7
∗∗
90.4
EarthAccid 86.7 88.5
∗
HMM-BasedContentModels
a80
a80
a80
a80
a80
a80
Train
Test Earthquakes Accidents
Earthquakes 88.0 31.7
∗∗
Accidents 60.3
∗∗
75.8
19
ComputationalLinguistics Volume34,Number1
As can be seen from Table 7, the model’s performance degrades considerably
(approximately by 20%) when tested on out-of-domain texts. On the positive side,
themodel’sout-of-domainperformanceisbetterthanchance(i.e.,50%).Furthermore,
oncethemodelistrainedondatarepresentativeofbothdomains,itperformsalmost
aswellasamodelwhichhasbeentrainedexclusivelyonin-domaintexts(seethe
rowEarthAccidinTable7).Toputtheseresultsintocontext,wealsoconsideredthe
cross-domainperformanceofthecontentmodels.AsTable7shows,thedecreasein
performanceismoredramaticforthecontentmodels.Infact,themodeltrainedon
theEarthquakesdomainplummetsbelowtherandombaselinewhenappliedtothe
Accidentsdomain.Theseresultsareexpectedforcontentmodels—thetwodomains
havelittleoverlapintopicsanddonotsharestructuralconstraints.NotethattheLSA
modelisnotsensitivetocross-domainissues.Thesemanticspaceisconstructedover
manydifferentdomainswithouttakingintoaccountstyleorwritingconventions.
Thecross-trainingperformanceoftheentity-basedmodelsissomewhatpuzzling:
thesemodelsarenotlexicalized,andonewouldexpectthatvalidentitytransitions
arepreservedacrossdomains.Althoughtransitiontypesarenotdomain-speciﬁc,their
distributioncouldvaryfromonedomaintoanother.Togiveasimpleexample,some
domainswillhavemoreentitiesthanothers(e.g.,descriptivetexts).Inotherwords,
entitytransitionscapturenotonlytextcoherenceproperties,butalsoreﬂectstylistic
andgenre-speciﬁcdiscourseproperties.Thishypothesisisindirectlyconﬁrmedbythe
observeddifferencesinthecontributionofvariouslinguisticfeaturesacrossthetwo
domainsdiscussedabove.Cross-domaindifferencesinthedistributionandoccurrence
ofentitieshavebeenalsoobservedinotherempiricalstudiesoflocalcoherence.For
instance,Poesioetal.(2004)showdifferencesintransitiontypesbetweeninstructional
textsanddescriptionsofmuseumtexts.InSection6,weshowthatfeaturesderived
fromthe entity grid help determine the readability level for a given text, thereby
verifyingmoredirectlythehypothesisthatthegridrepresentationcapturesstylistic
discoursefactors.
Theresultspresentedsofarsuggestthatadaptingtheproposedmodeltoanew
domainwouldinvolvesomeeffortincollectingrepresentativetextswithassociated
coherence ratings. Thankfully, the entity grids are constructed in a fully automatic
fashion,withoutrequiringmanualannotation.Thiscontrastswithtraditionalimple-
mentationsofCenteringTheorythatoperateoverlinguisticallyricherrepresentations
thataretypicallyhand-coded.
5.Experiment2:SummaryCoherenceRating
Wefurthertesttheabilityofourmethodtoassesscoherencebycomparingmodel
inducedrankingsagainstrankingselicitedbyhumanjudges.Admittedly,thesynthetic
datausedintheorderingtaskonlypartiallyapproximatescoherenceviolationsthat
human readers encounter in machine generated texts. A representative example of
suchtextsareautomaticallygeneratedsummarieswhichoftencontainsentencestaken
outofcontextandthusdisplayproblemswithrespecttolocalcoherence(e.g.,dan-
glinganaphors,thematicallyunrelatedsentences).Amodelthatexhibitshighagree-
ment with human judges not only accurately captures the coherence properties of
thesummariesinquestion,butultimatelyholdspromisefortheautomaticevaluation
of machine-generated texts. Existing automatic evaluation measures such as BLEU
(Papineni et al. 2002) and ROUGE (Lin and Hovy 2003) are not designed for the
coherenceassessmenttask,becausetheyfocusoncontentsimilaritybetweensystem
outputandreferencetexts.
20
BarzilayandLapata ModelingLocalCoherence
5.1Modeling
Summarycoherenceratingcanbealsoformulatedasarankinglearningtask.Weare
assumingthatthelearnerhasaccesstoseveralsummariescorrespondingtothesame
documentordocumentcluster.Suchsummariescanbeproducedbyseveralsystems
thatoperateoveridenticalinputsorbyasinglesystem(e.g.,byvaryingthecompression
lengthorbyswitchingonoroffindividualsystemmodules,forexampleasentence
compressionoranaphoraresolutionmodule).Similarlytothesentenceorderingtask,
our training data includes pairs of summaries (x
ij,x
ik
) of the same document(s) d
i,
wherex
ij
ismorecoherentthanx
ik
.Anoptimallearnershouldreturnarankingr
∗
that
ordersthesummariesaccordingtotheircoherence.AsinExperiment1weadoptan
optimizationapproachandfollowthetrainingregimeputforwardbyJoachims(2002).
5.2Method
Data.OurevaluationwasbasedonmaterialsfromtheDocumentUnderstandingCon-
ference(DUC2003),whichincludemulti-documentsummariesproducedbyhuman
writers and by automatic summarization systems. In order to learn a ranking, we
requireasetofsummaries,eachofwhichhasbeenratedintermsofcoherence.One
stumblingblocktoperformingthiskindofevaluationisthecoherenceratingsthem-
selves,whicharenotroutinelyprovidedbyDUCsummaryevaluators.InDUC2003,the
qualityofautomaticallygeneratedsummarieswasassessedalongseveraldimensions
rangingfromgrammatically,tocontentselection,ﬂuency,andreadability.Coherence
wasindirectlyevaluatedbynotingthenumberofsentencesindicatinganawkward
time sequence, suggesting a wrong cause–effect relationship, or being semantically
incongruentwiththeirneighboringsentences.
8
Unfortunately,theobservedcoherence
violationswerenotﬁne-grainedenoughtobeofuseinourratingexperiments.In
themajorityofcasesDUCevaluatorsnotedeither0or1violations;however,without
judgingthecoherenceofthesummaryasawhole,wecannotknowwhetherasingle
violationdisruptscoherenceseverelyornot.
Wethereforeobtainedjudgmentsforautomaticallygeneratedsummariesfromhu-
mansubjects.
9
Werandomlyselected16inputdocumentclustersandﬁvesystemsthat
hadproducedsummariesforthesesets,alongwithreferencesummariescomposedby
humans.Coherenceratingswerecollectedduringanelicitationstudyby177unpaid
volunteers,allnativespeakersofEnglish.Thestudywasconductedremotelyoverthe
Internet.Participantsﬁrstsawasetofinstructionsthatexplainedthetask,anddeﬁned
thenotionofcoherenceusingmultipleexamples.Thesummarieswererandomizedin
listsfollowingaLatinsquaredesignensuringthatnotwosummariesinagivenlist
weregeneratedfromthesamedocumentcluster.Participantswereaskedtouseaseven-
point-scaletoratehowcoherentthesummarieswerewithouthavingseenthesource
texts.Theratings(approximately23persummary)givenbyoursubjectswereaveraged
toprovidearatingbetween1and7foreachsummary.
Thereliabilityofthecollectedjudgmentsiscrucialforouranalysis;wetherefore
performedseveralteststovalidatethequalityoftheannotations.First,wemeasured
howwellhumansagreeintheircoherenceassessment.Weemployedleave-one-out
8Seequestion12inhttp://duc.nist.gov/duc2003/quality.html.
9Theratingsareavailablefromhttp://homepages.inf.ed.ac.uk/mlap/coherence/.
21
ComputationalLinguistics Volume34,Number1
resampling
10
(WeissandKulikowski1991),bycorrelatingthedataobtainedfromeach
participantwiththemeancoherenceratingsobtainedfromallotherparticipants.The
inter-subjectagreementwasr=.768(p<.01.)Second,weexaminedtheeffectofdiffer-
enttypesofsummaries(human-vs.machine-generated.)AnANOVArevealedareliable
effectofsummarytype:F(1;15)=20.38,p<.01indicatingthathumansummariesare
perceivedassigniﬁcantlymorecoherentthansystem-generatedones.Finally,wealso
comparedtheelicitedratingsagainsttheDUCevaluationsusingcorrelationanalysis.
Thehumanjudgmentswerediscretizedtotwoclasses(i.e.,0or1)usingentropy-based
discretization(WittenandFrank2000).Wefoundamoderatecorrelationbetweenthe
humanratingsandDUCcoherenceviolations(r=.41,p<.01).Thisisexpectedgiven
thatDUCevaluatorswereusingadifferentscaleandandwerenotexplicitlyassessing
summarycoherence.
The summaries used in our rating elicitation study form the basis of a corpus
usedforthedevelopmentofourentity-basedcoherencemodels.Toincreasethesize
ofourtrainingandtestsets,weaugmentedthematerialsusedintheelicitationstudy
withadditionalDUCsummariesgeneratedbyhumansforthesameinputsets.We
assumedthatthesesummariesweremaximallycoherent.Asmentionedpreviously,our
participantstendtoratehuman-authoredsummarieshigherthanmachine-generated
ones.Toensurethatwedonottuneamodeltoaparticularsystem,weusedtheoutput
summariesofdistinctsystemsfortrainingandtesting.Oursetoftrainingmaterials
contained6×16summaries(averagelength4.8),yielding
parenleftbig
6
2
parenrightbig
×16=240pairwiserank-
ings.Becausehumansummariesoftenhaveidentical(high)scores,weeliminatedpairs
ofsuchsummariesfromthetrainingset.Consequently,theresultingtrainingcorpus
consistedof144summaries.Inasimilarfashion,weobtained80pairwiserankingsfor
thetestset.Sixdocumentsfromthetrainingdatawereusedasadevelopmentset.
Features, Parameter Settings, and Training Requirements.Weexaminetheinﬂuenceoflin-
guisticknowledgeonmodelperformancebycomparingmodelswithvaryingdegrees
oflinguisticcomplexity.Tobeabletoassesstheperformanceofourmodelsacrosstasks
(e.g.,sentenceorderingvs.summarization),weexperimentedwiththesamemodel
typesintroducedinthepreviousexperiment(seeSection4.3).Wealsoinvestigatethe
trainingrequirementsforthesemodelsonthesummarycoherencetask.
Experiment1differsfromthepresentstudyinthewaycoreferenceinformation
wasobtained.InExperiment1,acoreferenceresolutiontoolwasappliedtohuman-
writtentexts,whicharegrammaticalandcoherent.Here,weapplyacoreferencetool
toautomaticallygeneratedsummaries.Becausemanysummariesinourcorpusare
fraught with coherence violations, the performance of a coreference resolution tool
islikelytodrop.Unfortunately,resolvingcoreferenceintheinputdocumentswould
requireamulti-documentcoreferencetool,whichiscurrentlyunavailabletous.
AsinExperiment1,thefrequencythresholdandthelengthofthetransitionse-
quencewereoptimizedonthedevelopmentset.Optimalsalience-basedmodelswere
obtainedforentitieswithfrequency ≥2.Theoptimaltransitionlengthwas ≤2.All
modelsweretrainedandtestedusingSVM
light
(Joachims2002).
Comparison with State-of-the-Art Methods.OurresultswerecomparedtotheLSAmodel
introducedinExperiment1(seeSection4.2fordetails).Unfortunately,wecouldnot
10WecannotapplythecommonlyusedKappastatisticformeasuringagreementbecauseitisappropriate
fornominalscales,whereasoursummariesareratedonanordinalscale.
22
BarzilayandLapata ModelingLocalCoherence
employBarzilayandLee’s(2004)contentmodelsforthesummaryrankingtask.Being
domain-dependent,thesemodelsrequireaccesstodomainrepresentativetextsfortrain-
ing.Oursummarycorpus,however,containstextsfrommultipledomainsanddoesnot
provideanappropriatesampleforreliablytrainingcontentmodels.
5.3Results
Impact of Linguistic Representation. Ourresults aresummarized inTable8. Similarly
tothesentenceorderingtask,weobservethatthelinguisticallyimpoverishedmodel
Coreference−Syntax−Salience−exhibitsdecreasedaccuracywhencomparedagainst
modelsthatoperateovermoresophisticatedrepresentations.However,thecontribution
ofindividualknowledgesourcesdiffersinthistask.Forinstance,coreferenceresolu-
tionimprovedmodelperformanceinordering,butitcausesadecreaseinaccuracy
in summary evaluation (compare the models Coreference+Syntax+Salience+ and
Coreference−Syntax+Salience+inTables5and8).Thisdropinperformancecanbe
attributedtotwofactorsbothrelatedtothefactthatoursummarycorpuscontains
manymachine-generatedtexts.First,anautomaticcoreferenceresolutiontoolwillbe
expectedtobelessaccurateonourcorpus,becauseitwastrainedonwell-formed
human-authoredtexts.Second,automaticsummarizationsystemsdonotuseanaphoric
expressionsasoftenashumansdo.Therefore,asimpleentityclusteringmethodismore
suitableforautomaticsummaries.
Bothsalienceandsyntacticinformationcontributetotheaccuracyoftheranking
model.Theimpactofeachoftheseknowledgesourcesinisolationisnotdramatic—
droppingeitherofthemyieldssomedecreaseinaccuracy,butthedifferenceisnotsta-
tisticallysigniﬁcant.However,eliminatingbothsalienceandsyntacticinformationsig-
niﬁcantlydecreases performance (compare Coreference−Syntax+Salience+ against
Coreference+Syntax−Salience−andCoreference−Syntax−Salience−inTable8).
Figure 2 shows the learning curve for our best model Coreference−Syntax+
Salience+.Althoughthemodelperformspoorlywhentrainedonasmallfractionofthe
data,itstabilizesrelativelyfast(with80pairwiserankings),anddoesnotimproveafter
Table8
Summaryrankingaccuracymeasuredasfractionofcorrectpairwiserankingsinthetestset.
Coreference[+/−]indicateswhetheranaphoricinformationhasbeenusedwhenconstructing
theentitygrid.Similarly,Syntax[+/−]andSalience[+/−]reﬂecttheuseofsyntacticand
salienceinformation.Diacritics
**
(p<.01)and
*
(p<.05)indicatewhetherCoreference−
Syntax+Salience+issigniﬁcantlydifferentfromallothermodels(usingaFisherSignTest).
Model Accuracy
Coreference+Syntax+Salience+ 80.0
Coreference+Syntax+Salience− 75.0
Coreference+Syntax−Salience+ 78.8
Coreference−Syntax+Salience+ 83.8
Coreference+Syntax−Salience− 71.3
∗
Coreference−Syntax+Salience− 78.8
Coreference−Syntax−Salience+ 77.5
Coreference−Syntax−Salience− 73.8
∗
LatentSemanticAnalysis 52.5
∗∗
23
ComputationalLinguistics Volume34,Number1
Figure2
Learningcurvefortheentity-basedmodelCoreference−Syntax+Salience+appliedtothe
summaryrankingtask.
acertainpoint.Theseresultssuggestthatfurtherimprovementstosummaryranking
areunlikelytocomefromaddingmoreannotateddata.
Comparison with the State-of-the-Art. AsinExperiment1,wecomparedthebestper-
forminggridmodel(Coreference−Syntax+Salience+)againstLSA(seeTable8).The
formermodelsigniﬁcantlyoutperformsthelatter(p<.01)byawidemargin.LSAisper-
hapsatadisadvantageherebecauseithasbeenexposedonlytohuman-authoredtexts.
Machine-generated summaries are markedly distinct from human texts even when
theseareincoherent(asinthecaseofourorderingexperiment).Forexample,manual
inspectionofoursummarycorpusrevealedthatlow-qualitysummariesoftencontain
repetitiveinformation.Insuchcases,simplyknowingabouthighcross-sententialover-
lapisnotsufﬁcienttodistinguisharepetitivesummaryfromawell-formedone.
Furthermore,notethatincontrasttothedocumentsinExperiment1,thesummaries
beingrankedheredifferinlexicalchoice.Somearewrittenbyhumans(andarethus
abstracts),whereasothershavebeenproducedbysystemsfollowingdifferentsumma-
rizationparadigms(somesystemsperformrewritingwhereasothersextractsentences
verbatimfromthesourcedocuments).ThismeansthatLSAmayconsiderasummary
coherentsimplybecauseitsvocabularyisfamiliar(i.e.,itcontainswordsforwhich
reliablevectorshavebeenobtained).Analogously,asummarywithalargenumber
ofout-of-vocabularylexicalitemswillbegivenlowsimilarityscores,irrespectiveof
whetheritiscoherentornot.Thisisnotuncommoninsummarieswithmanyproper
names.TheseoftendonotoverlapwiththepropernamesfoundintheNorthAmerican
NewsCorpususedfortrainingtheLSAmodel.Lexicaldifferencesexertmuchless
inﬂuenceontheentity-gridmodelwhichabstractsawayfromalternativeverbalizations
ofthesamecontentandcapturescoherencesolelyonthebasisofgridtopology.
6.Experiment3:ReadabilityAssessment
Sofar,ourexperimentshaveexploredthepotentialoftheproposeddiscourserepre-
sentationforcoherencemodeling.Wehavepresentedseveralclassesofgridmodels
24
BarzilayandLapata ModelingLocalCoherence
achievinggoodperformanceindiscerningcoherentfromincoherenttexts.Ourexperi-
mentsalsorevealasurprisingpropertyofgridmodels:Eventhoughthesemodelsare
notlexicalized,theyaredomain-andstyle-dependent.Inthissection,weinvestigatein
detailthisfeatureofgridmodels.Here,wemoveawayfromthecoherenceratingtask
andputtheentity-gridrepresentationfurthertothetestbyexaminingwhetheritcan
beusefullyemployedinstyleclassiﬁcation.Speciﬁcally,weembedourentitygridsinto
asystemthatassessesdocumentreadability.Thetermdescribestheeasewithwhich
adocumentcanbereadandunderstood.Thequantitativemeasurementofreadability
hasattractedconsiderableinterestanddebateoverthelast70years(seeMitchell[1985]
andChall[1958]fordetailedoverviews)andhasrecentlybeneﬁtedfromtheuseofNLP
technology(SchwarmandOstendorf2005).
Anumberofreadabilityformulashavebeendevelopedwiththeprimaryaimof
assessingwhethertextsorbooksaresuitableforstudentsatparticulargradelevels
orages.Manyreadabilitymethodsfocusonsimpleapproximationsofsemanticfactors
concerningthewordsusedandsyntacticfactorsconcerningthelengthorstructureof
sentences(Gunning1952;Kincaidetal.1975;ChallandDale1995;Stenner1996;Katz
and Bauer 2001). Despite their widespread applicability in education and technical
writing (Kincaid et al. 1981), readability formulas are often criticized for being too
simplistic;theysystematicallyignoremanyimportantfactorsthataffectreadabilitysuch
asdiscoursecoherenceandcohesion,layoutandformatting,useofillustrations,the
natureofthetopic,thecharacteristicsofthereaders,andsoforth.
SchwarmandOstendorf(2005)developedamethodforassessingreadabilitywhich
addressessomeoftheshortcomingsofpreviousapproaches.Byrecastingreadability
assessmentasaclassiﬁcationtask,theyareabletocombineseveralknowledgesources
rangingfromtraditionalreadinglevelmeasures,tostatisticallanguagemodels,and
syntacticanalysis.Evaluationresultsshowthattheirsystemoutperformstwocom-
monlyusedreadinglevelmeasures(theFlesch-KincaidGradeLevelindexandLexile).
Inthefollowingwebuildontheirapproachandexaminewhethertheentity-gridrep-
resentationintroducedinthisarticlecontributestothereadabilityassessmenttask.The
incorporationofcoherence-basedinformationinthemeasurementoftextreadabilityis,
toourknowledge,novel.
6.1Modeling
WefollowSchwarmandOstendorf(2005)intreatingreadabilityassessmentasaclassiﬁ-
cationtask.Theunitofclassiﬁcationisasinglearticleandthelearner’staskistopredict
whetheritiseasyordifﬁculttoread.Avarietyofmachinelearningtechniquesare
amenabletothisproblem.BecauseourgoalwastoreplicateSchwarmandOstendorf’s
systemascloselyaspossible,wefollowedtheirchoiceofsupportvectormachines
(SVMs)(Joachims1998b)forourclassiﬁcationexperiments.Ourtrainingsamplethere-
foreconsistedofndocumentssuchthat
(vectorx
1,y
1
),...,(vectorx
n,y
n
) vectorx
i
∈Rfractur
N,y
i
∈{−1,+1}
where vectorx
i
is afeature vector for the ithdocument in thetraining sample and y
i
its
(positiveornegative)classlabel.InthebasicSVMframework,wetrytoseparatethe
positiveandnegativeinstancesbyahyperplane.Thismeansthatthereisaweight
25
ComputationalLinguistics Volume34,Number1
Table9
ExcerptsfromtheBritannicareadabilitycorpus
TheLemmaVallettainBritannica
AlsospelledValletta,seaportandcapitalofMalta,onthenortheastcoastoftheisland.The
nucleusofthecityisbuiltonthepromontoryofMountSceberrasthatrunslikeatongueinto
themiddleofabay,whichitthusdividesintotwoharbours,GrandHarbourtotheeastand
Marsamxett(Marsamuscetto)Harbourtothewest.BuiltaftertheGreatSiegeofMaltain1565,
whichcheckedtheadvanceofOttomanpowerinsouthernEurope,itwasnamedafterJeanParisot
delaValette,grandmasteroftheorderofHospitallers(KnightsofSt.JohnofJerusalem),and
becametheMaltesecapitalin1570.TheHospitallersweredrivenoutbytheFrenchin1798,and
aMalteserevoltagainsttheFrenchgarrisonledtoValletta’sseizurebytheBritishin1800.
TheLemmaVallettainBritannicaElementary
Aportcity,VallettaisthecapitaloftheislandcountryofMaltaintheMediterraneanSea.Valletta
islocatedontheeasterncoastofthelargestisland,whichisalsonamedMalta.Vallettaliesona
peninsula—alandmasssurroundedbywateronthreesides.ItbordersMarsamxettHarbortothe
northandGrandHarbortothesouth.TheeasternendofthecityjutsoutintotheMediterranean.
Vallettawasplannedinthe16thcenturybytheItalianarchitectFrancescoLaparelli.Tomake
travelingthroughVallettaeasier,Laparellidesignedthecityinagridpatternwithstraightstreets
thatcrossedeachotherandrantheentirewidthandlengthofthetown.Vallettawasoneofthe
ﬁrsttownstobelaidoutinthisway.
vectorwandathresholdb,sothatallpositivetrainingexamplesareononesideofthe
hyperplane,whileallnegativeoneslieontheotherside.Thisisequivalenttorequiring
y
i
[(w· vectorx
i
)+b]>0
Findingtheoptimalhyperplaneisanoptimizationproblemwhichcanbesolved
efﬁcientlyusingtheproceduredescribedinVapnik(1998).SVMshavebeenwidely
usedformanyNLPtasksrangingfromtextclassiﬁcation(Joachims1998b),tosyntactic
chunking(KudoandMatsumoto2001),andshallowsemanticparsing(Pradhanetal.
2005).
6.2Method
Data.ForourexperimentsweusedacorpuscollectedbyBarzilayandElhadad(2003)
fromthe Encyclopedia BritannicaandBritannica Elementary.Thelatterisanewversion
targetedatchildren.Thecorpuscontains107articlesfromthefullversionoftheencyclo-
pediaandtheircorrespondingsimpliﬁedarticlesfromBritannicaElementary(214articles
intotal).Althoughthesetextsarenotexplicitlyannotatedwithgradelevels,theystill
representtwobroadreadabilitycategories,namely,easyanddifﬁcult.
11
Examplesof
thesetwocategoriesaregiveninTable9.
11TheBritannicacorpuswasalsousedbySchwarmandOstendorf(2005);inadditiontheymakeuseofa
corpuscompiledfromtheWeeklyReader,aneducationalnewspaperwithdocumentstargetedatgrade
levels2–5.Unfortunately,thiscorpusisnotpubliclyavailable.
26
BarzilayandLapata ModelingLocalCoherence
FeaturesandParameterSettings.Wecreatedtwosystemversions:theﬁrstoneusedsolely
SchwarmandOstendorf(2005)features;
12
thesecondoneemployedaricherfeature
space—weaddedtheentity-basedrepresentationproposedheretotheiroriginalfeature
set.Wewillbrieﬂydescribethereadability-relatedfeaturesusedinoursystemsand
directtheinterestedreadertoSchwarmandOstendorfforamoredetaileddiscussion.
SchwarmandOstendorf(2005)usethreebroadclassesoffeatures:syntactic,se-
mantic,andtheircombination.Theirsyntacticfeaturesareaveragesentencelengthand
featuresextractedfromparsetreescomputedusingCharniak’s(2000)parser.Thelatter
includeaverageparsetreeheight,averagenumberofNPs,averagenumberofVPs,and
averagenumberofsubordinateclauses(SBARs).Wecomputedaveragesentencelength
bymeasuringthenumberoftokenspersentence.
Theirsemanticfeaturesincludetheaveragenumberofsyllablesperword,and
languagemodelperplexityscores.Aunigram,bigram,andtrigrammodelwasesti-
matedforeachclass,andperplexityscoreswereusedtoassesstheirperformanceon
testdata.FollowingSchwarmandOstendorf(2005)weusedinformationgaintoselect
wordsthatweregoodclassdiscriminants.Allremainingwordswerereplacedbytheir
partsofspeech.Thevocabularythusconsistedof300wordswithhighinformation
gainand36PennTreebankpart-of-speechtags.Thelanguagemodelswereestimated
using maximum likelihood estimation and smoothed with Witten-Bell discounting.
ThelanguagemodelsdescribedinthisarticlewereallbuiltusingtheCMUstatistical
languagemodelingtoolkit(ClarksonandRosenfeld1997).Ourperplexityscoreswere
sixintotal(2classes×3languagemodels).
Finally,theFlesch-KincaidGradeLevelscorewasincludedasafeaturethatcap-
turesbothsyntacticandsemantictextproperties.TheFlesch-Kincaidformulaestimates
readabilityasacombinationofthetheaveragenumberofsyllablesperwordandthe
averagenumberofwordspersentence:
0.39
parenleftBig
totalwords
totalsentences
parenrightBig
+11.8
parenleftbigg
totalsyllables
totalwords
parenrightbigg
−15.59 (3)
WealsoenrichedSchwarmandOstendorf’s(2005)featurespacewithcoherence-
basedfeatures.Eachdocumentwasrepresentedasafeaturevectorusingtheentitytran-
sitionnotationintroducedinSection3.Weexperimentedwithtwomodelsthatyielded
goodperformancesinourpreviousexperiments:Coreference+Syntax+Salience+(see
Experiment1)andCoreference−Syntax+Salience+(seeExperiment2).Thetransition
lengthwas≤2andentitieswereconsideredsalientiftheyoccurred≥2times.Asinour
previousexperiments,wecomparedtheentity-basedrepresentationagainstLSA.The
latterisameasureofthesemanticrelatednessacrosspairsofsentences.Wecouldnot
applytheHMM-basedcontentmodels(BarzilayandLee2004)tothereadabilitydata
set.Theencyclopedialemmasarewrittenbydifferentauthorsandconsequentlyvary
considerablyinstructureandvocabularychoice.Recallthatthesemodelsaresuitable
formorerestricteddomainsandtextsthataremoreformulaicinnature.
12SchwarmandOstendorf(2005)deﬁneout-of-vocabulary(OOV)scoresrelativetothemostcommon
wordsingrade2,thelowestgradelevelintheircorpus;itwasnotpossibletoestimateOOVscores,
becausewedidnothaveaccesstograde2texts.
27
ComputationalLinguistics Volume34,Number1
Table10
Thecontributionofcoherence-basedfeaturestotheautomaticreadabilityassessmenttask.
Diacritics
**
(p<.01)and
*
(p<.05)indicatewhetherdifferencesinaccuracybetween
SchwarmandOstendorfandallothermodelsaresigniﬁcant(usingaFisherSigntest).
Model Accuracy
Schwarm&Ostendorf 78.56
Schwarm&Ostendorf,Coreference+Syntax+Salience+ 88.79
∗
Schwarm&Ostendorf,Coreference−Syntax+Salience+ 79.49
Schwarm&Ostendorf,LatentSemanticAnalysis 78.56
Coreference+Syntax+Salience+ 50.90
∗∗
Coreference−Syntax+Salience+ 49.55
∗∗
LatentSemanticAnalysis 48.58
∗∗
ThedifferentsystemsweretrainedandtestedontheBritannicacorpususingﬁve-
foldcross-validation.
13
Thelanguagemodelswerecreatedanewforeveryfoldusingthe
documentsinthetrainingdata.WeuseJoachims’(1998a)SVM
light
packagefortraining
andtestingwithallparameterssettotheirdefaultvalues.
Evaluation Metric. Wemeasureclassiﬁcationaccuracy(i.e.,thenumberofclassesas-
signedcorrectlybytheSVMoverthesizeofthetestset).Wereportaccuracyaveraged
overfolds.Achancebaseline(selectingoneclassatrandom)yieldsanaccuracyof50%.
Ourtrainingandtestsetshavethesamenumberofdocumentsforthetworeadability
categories.
6.3Results
Table 10 summarizes our results on the readability assessment task. We ﬁrst com-
pared Schwarmand Ostendorf’s (2005) systemagainst a systemthat incorporates
entity-basedcoherencefeatures(seerows3–4inTable10).Ascanbeseen,thesys-
tem’s accuracy signiﬁcantly increases by 10% when the full feature set is included
(Coreference+Syntax+Salience+).Entity-gridfeaturesthatdonotincorporatecorefer-
enceinformation(Coreference−Syntax+Salience+)performnumericallybetter(com-
parerow1and3inTable10);however,thedifferenceisnotstatisticallysigniﬁcant.
ThesuperiorperformanceoftheCoreference+Syntax+Salience+featuresetisnot
entirelyunexpected.Inspectionofourcorpusrevealedthateasyanddifﬁculttextsdiffer
intheirdistributionofpronounsandcoreferencechainsingeneral.Easytextstendto
employlesscoreferenceandtheuseofpersonalpronounsisrelativelysparse.Togive
aconcreteexample,thepronountheyisattested173timesinthedifﬁcultcorpusand
only73intheeasycorpus.Thisobservationsuggeststhatcoreferenceinformationisa
goodindicatorofthelevelofreadingdifﬁcultyandexplainswhyitsomissionfromthe
entity-basedfeaturespaceyieldsinferiorperformance.
13Thedatafortheexperimentsreportedherecanbefoundathttp://homepages.inf.ed.ac.uk/
mlap/coherence/.
28
BarzilayandLapata ModelingLocalCoherence
Furthermore,notethatdiscourse-levelinformationisabsentfromSchwarmand
Ostendorf’s(2005)originalmodel.Thelatteremploysalargenumberoflexicaland
syntacticfeatureswhichcapturesententialdifferencesamongdocuments.Ourentity-
basedrepresentationsupplementstheirfeaturespacewithinformationspanningtwoor
moresuccessivesentences.Wethusareabletomodelstylisticdifferencesinreadability
thatgobeyondsyntaxandlexicalchoice.Besidescoreference,ourfeaturerepresenta-
tioncapturesimportantinformationaboutthepresenceanddistributionofentitiesin
discourse.Forexample,difﬁculttextstendtohavetwiceasmanyentitiesaseasyones.
Consequently,easyanddifﬁculttextsarerepresentedbyentitytransitionsequences
withdifferentprobabilities(e.g.,thesequences [SS] and [SO] aremoreprobablein
difﬁculttexts).Interestingly,whencoherenceisquantiﬁedusingLSA,weobserveno
improvementtotheclassiﬁcationtask.TheLSAscorescapturelexicalorsemantictext
propertiessimilartothoseexpressedbytheFleschKincaidindexandtheperplexity
scores(e.g.,wordrepetition).Itisthereforenotsurprisingthattheirinclusioninthe
featuresetdoesnotincreaseperformance.
Wealsoevaluatedthetrainingrequirementsforthereadabilitysystemdescribed
herein.Figure3showsthelearningcurveforSchwarmandOstendorf’s(2005)model
enhancedwiththeCoreference+Syntax+Salience+featurespaceandonitsown.As
can be seen, both models perform relatively well when trained on small data sets
(e.g.,20–40documents)andreachpeakaccuracywithhalfofthetrainingdata.The
inclusionofdiscourse-basedfeaturesconsistentlyincreasesaccuracyirrespectiveofthe
amountoftrainingdataavailable.Figure3thussuggeststhatbetterfeatureengineering
islikelytobringfurtherperformanceimprovementsonthereadabilitytask.
Ourresultsindicatethattheentity-basedtextrepresentationintroducedherecap-
turesaspectsoftextreadabilityandcanbesuccessfullyincorporatedintoapractical
system.Coherenceisbynomeansthesolepredictorofreadability.Infact,onitsown,
itperformspoorlyonthistaskasdemonstratedwhenusingeitherLSAortheentity-
basedfeaturespacewithoutSchwarmandOstendorf’s(2005)features(seerows5–7in
Table10).Rather,weclaimthatcoherenceisoneamongmanyfactorscontributingto
textreadabilityandthatourentity-gridrepresentationiswell-suitedfortextclassiﬁca-
tiontaskssuchasreadinglevelassessment.
Figure3
LearningcurveforSchwarmandOstendorf’s(2005)modelonitsownandenhancedwiththe
Coreference+Syntax+Salience+featurespace.
29
ComputationalLinguistics Volume34,Number1
7.DiscussionandConclusions
Inthisarticleweproposedanovelframeworkforrepresentingandmeasuringtextco-
herence.Centraltothisframeworkistheentity-gridrepresentationofdiscourse,which
wearguecapturesimportantpatternsofsentencetransitions.Were-conceptualizeco-
herenceassessmentasalearningtaskandshowthatourentity-basedrepresentation
is well-suited for ranking-based generation and text classiﬁcation tasks. Using the
proposedrepresentation,weachievegoodperformanceontextordering,summary
coherenceevaluation,andreadabilityassessment.
The entity grid is a ﬂexible, yet computationally tractable, representation. We
investigated three important parameters for grid construction: the computation of
coreferringentityclasses,theinclusionofsyntacticknowledge,andtheinﬂuenceof
salience. All these knowledge sources ﬁgure prominently in theories of discourse
(seeSection2)andareconsideredimportantindeterminingcoherence.Ourresults
empiricallyvalidatetheimportanceofsalienceandsyntacticinformation(expressed
by S, O, X,and–)forcoherence-basedmodels.Thecombinationofbothknowledge
sources(Syntax+Salience)yieldsmodelswithconsistentlygoodperformanceforall
ourtasks.
Thebeneﬁtsoffullcoreferenceresolutionarelessuniform.Thisispartlydueto
mismatchesbetweentrainingandtestingconditions.Thesystemweemploy(Ngand
Cardie2002)wastrainedonhuman-authorednewspapertexts.Thecorporaweused
inoursentenceorderingandreadabilityassessmentexperimentsaresomewhatsimilar
(i.e.,human-authorednarratives),whereasoursummarycoherenceratingexperiment
employedmachinegeneratedtexts.Itisthereforenotsurprisingthatcoreferencereso-
lutiondeliversperformancegainsontheﬁrsttwotasksbutnotonthelatter(seeTable5
inSection4andTable10inSection6.3).Ourresultsfurthershowthatinlieuofan
automaticcoreferenceresolutionsystem,entityclassescanbeapproximatedsimply
bystringmatching.Thelatterisagoodindicatorofnominalcoreference;itisoften
includedasafeatureinmachinelearningapproachestocoreferenceresolution(Soon,
Ng,andLim2001;NgandCardie2002)andisrelativelyrobust(i.e.,likelytodeliver
consistentresultsinthefaceofdifferentdomainsandgenres).
Itisimportanttonotethat,althoughinspiredbyentity-basedtheoriesofdiscourse
coherence,ourapproachisnotadirectimplementationofanytheoryinparticular.
Rather,wesacriﬁcelinguisticfaithfulnessforautomaticcomputationandbreadthof
coverage.Despiteapproximationsandunavoidableerrors(e.g.,intheparser’soutput),
ourresultsindicatethatentitygridsareausefulrepresentationalframeworkacross
tasksandtextgenres.InagreementwithPoesioetal.(2004)weﬁndthatpronomi-
nalizationisagoodindicatorofdocumentcoherence.Wealsoﬁndthatcoherenttexts
arecharacterizedbytransitionswithparticularpropertieswhichdonotholdforall
discourses.ContrarytoCenteringTheory,weremainagnostictothetypeoftransi-
tionsthatourmodelscapture(e.g.,CONTINUE,SHIFT).Wesimplyrecordwhetheran
entityismentionedinthediscourseandinwhatgrammaticalrole.Ourexperiments
quantitativelymeasuredthepredictivepowerofvariouslinguisticfeaturesforseveral
coherence-relatedtasks.Crucially,weﬁndthatourmodelsaresensitivetothedomainat
handandthetypeoftextsunderconsideration(human-authoredvs.machinegenerated
texts).Thisisanunavoidableconsequenceofthegridrepresentation,whichisentity-
speciﬁc.Differencesinentitydistributionindicatenotonlydifferencesincoherence,but
alsoinwritingconventionsandstyle.Similarobservationshavebeenmadeinother
workwhichiscloserinspirittoCentering’sclaims(Hasler2004;Karamanisetal.2004;
Poesioetal.2004).
30
BarzilayandLapata ModelingLocalCoherence
Animportantfuturedirectionliesinaugmentingourentity-basedrepresentation
withmoreﬁne-grainedlexico-semanticknowledge.Onewaytoachievethisgoalisto
clusterentitiesbasedontheirsemanticrelatedness,therebycreatingagridrepresen-
tationoverlexicalchains(MorrisandHirst1991).Anentirelydifferentapproachisto
developfullylexicalizedmodels,akintotraditionallanguagemodels.Cachelanguage
models(KuhnandDeMori1990)seemparticularlypromisinginthiscontext.The
granularityofsyntacticinformationisanothertopicthatwarrantsfurtherinvestigation.
Sofarwehaveonlyconsideredthecontributionof“core”grammaticalrelationsto
thegridconstruction.Expandingourgrammaticalcategoriestomodiﬁersandadjuncts
mayprovideadditionalinformation,inparticularwhenconsideringmachinegenerated
texts.Wealsoplantoinvestigatewhethertheproposeddiscourserepresentationand
modelingapproachesgeneralizeacrossdifferentlanguages.Forinstancetheidentiﬁ-
cationandextractionofentitiesposesadditionalchallengesingridconstructionfor
Chinesewherewordboundariesarenotdenotedorthographically(byspace).Similar
challengesariseinGerman,alanguagewithalargenumberofinﬂectedformsand
productivederivationalprocesses(e.g.,compounding)notindicatedbyorthography.
Inthediscourseliterature,entity-basedtheoriesareprimarilyappliedatthelevel
oflocalcoherence,whilerelationalmodels,suchasRhetoricalStructureTheory(Mann
andThomson1988;Marcu2000),areusedtomodeltheglobalstructureofdiscourse.
Weplantoinvestigatehowtocombinethetwoforimprovedpredictiononbothlocal
andgloballevels,withtheultimategoalofhandlinglongertexts.
Acknowledgments
Theauthorsacknowledgethesupportof
theNationalScienceFoundation(Barzilay;
CAREERgrantIIS-0448168andgrant
IIS-0415865)andEPSRC(Lapata;grant
GR/T04540/01).WearegratefultoClaire
CardieandVincentNgforprovidingus
theresultsoftheircoreferencesystem
onourdata.ThankstoEliBarzilay,Eugene
Charniak,MichaelElhadad,Noemie
Elhadad,NikiforosKaramanis,FrankKeller,
AlexLascarides,IgorMalioutov,Smaranda
Muresan,MartinRinard,KevinSimler,
CarolineSporleder,ChaoWang,Bonnie
Webber,andthreeanonymousreviewers
forhelpfulcommentsandsuggestions.
Anyopinions,ﬁndings,andconclusionsor
recommendationsexpressedhereinarethose
oftheauthorsanddonotnecessarilyreﬂect
theviewsoftheNationalScienceFoundation
orEPSRC.

References

Althaus,Ernst,Nikiforos Karamanis,and AlexanderKoller.2004.Computing locally coherent discourses. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,pages399–406, Barcelona,Spain.

Ariel,Mira.1988.Referringandaccessibility. JournalofLinguistics,24:65–87.

Asher,NicholasandAlexLascarides.2003. LogicsofConversation.Cambridge UniversityPress,Cambridge,England.

Barzilay,Regina.2003.Information Fusion forMulti-DocumentSummarization: PraphrasingandGeneration.Ph.D.thesis, ColumbiaUniversity,NewYork.

Barzilay,Regina and Noemie El hadad. 2003. Sentence alignment for monolingual comparable corpora. In Proceedings of the 8th Conference on Empirical Methods in Natural Language Processing, pages25–32 Sapporo,Japan.

Barzilay,Regina and Mirella Lapata.2005. Modeling local coherence: Anentity-based approach.In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,pages141–148,AnnArbor,MI.

Barzilay,Regina and Lillian Lee.2004. Catching the drift: Probabilistic content models,withapplications to generation and summarization.In Proceedings of the 2nd Human Language Technology Conference and Annual Meeting of the North American Chapter of the Association for Computational Linguistics,pages113–120,Boston,MA.

Berry,MichaelW.,SusanT.Dumais,and GavinW.O’Brien.1994.Usinglinear algebraforintelligentinformation retrieval.SIAMReview,37(4):573–595.

Brennan,SusanE.,MarilynW.Friedman, and Charles J.Pollard.1987.A centering approach to pronouns.In Proceedings of the 31 Computational Linguistics Volume 34,Number1 

Briscoe,TedandJohnCarroll.2002.Robust accurate statistical annotation of general text.In Proceedings of the 3rd International Conference on Language Resources and Evaluation, pages1499–1504,LasPalmas, CanaryIslands.

Chafe,WallaceL.1976.Givenness, contrastiveness,deﬁniteness,subjects, topics,andpointofview.InCharlesN.Li, editor,SubjectandTopic.AcademicPress, NewYork,pages25–55.

Chall,JeanneS.1958.Readability:An AppraisalofResearchandApplication. Number34inBureauofEducational ResearchMonographs.OhioState UniversityPress,Columbus.

Chall,JeanneS.andEdgarDale.1995. ReadabilityRevisited:TheNewDale-Chall ReadabilityFormula.BrooklineBooks, Cambridge,MA.

Charniak,Eugene.2000.A maximum-entropy-inspired parser. In Proceedings of the 1st Annual Meeting of the North American Chapter of the Association for Computational Linguistics,pages132–139, Seattle,WA.

Clark,HerbertH.andSusanE.Haviland. 1977.Comprehensionandthegiven-new contract.InRoyO.Freedle,editor, DiscourseProductionandComprehension.Ablex,Norwood,NJ,pages1–39. 

Clarkson,PhilipandRonaldRosenfeld. 1997.Statisticallanguagemodeling. InProceedingsofESCAEuroSpeech’97, pages2707–2710,Rhodes,Greece.

Collins,Michael.1997.Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, pages16–23, Madrid,Spain.

Collins,Michael.2002.Discriminative reranking for natural language parsing. In Proceedings of the 17th International ConferenceonMachineLearning, pages175–182,PaloAlto,CA.

Foltz,PeterW.,WalterKintsch,and ThomasK.Landauer.1998.Textual coherenceusinglatentsemanticanalysis. DiscourseProcesses,25(2&3):285–307.

Freund,Yovav,RajIyer,RobertE.Schapire, andYoramSinger.2003.Anefﬁcient boostingalgorithmforcombining preferencs.MachineLearning,4:933–969.

Germann,Ulrich,MichaelJahr,Kevin Knight,DanielMarcu,andKenjiYamada. 2004.Fastandoptimaldecodingfor machinetranslation.ArtiﬁcialIntelligence, 154(1–2):127–143.

Givon,Talmy.1987.Beyondforegroundand background.InRussellS.Tomlin,editor, CoherenceandGroundinginDiscourse. Benjamins,Amsterdam/Philadelphia, pages175–188.

Grosz, Barbara, Aravind K. Joshi, and Scott Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse.Computational Linguistics, 21(2):203–225.

Gundel,JaenetteK.,NancyHedberg,andRonZacharski.1993.Cognitivestatus andtheformofreferringexpressions indiscourse.Language,69(2):274–307. 

Gunning,Robert.1952.TheTechniqueofClear Writing.McGrawHill,NewYork. 

Halliday,M.A.K.andRuqaiyaHasan.1976. CohesioninEnglish.Longman,London. 

Hasler,Laura.2004.Aninvestigationinto theuseofcenteringtransitionsfor summarisation.InProceedingsofthe 7thAnnualCLUKResearchColloquium, pages100–107,Birmingham,UK.

Hoey,Michael.1991.Patterns of Lexis in Text. OxfordUniversityPress,Oxford,England.

Hudson,S.B.,M.K.Tanenhaus,andG.S. Dell.1986.Theeffectofthediscourse centeronthelocalcoherenceofa discourse.InProceedingsofthe8thAnnual MeetingoftheCognitiveScienceSociety, pages96–101,Amherst,MA.

Joachims,Thorsten.1998a.Making large-scalesupportvectormachine learningpractical.InBernardSch¨olkopf, ChristopherBurges,andAlexanderSmola, editors,AdvancesinKernelMethods: SupportVectorMachines.MITPress, Cambridge,MA.

Joachims,Thorsten.1998b.Text categorizationwithsupportvector machines:Learningwithmanyrelevant features.InProceedingsoftheEuropean ConferenceonMachineLearning, pages137–142,Berlin,Springer. 

Joachims,Thorsten.2002.Optimizingsearch enginesusingclickthroughdata.In ProceedingsofACMConferenceonKnowledge DiscoveryandDataMining,pages133–142, Chicago,IL.

Kameyama,Megumi.1986.A property-sharing constraint in centering. In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, pages 200–206,NewYork.

BarzilayandLapata ModelingLocalCoherence Karamanis,Nikiforos.2003.EntityCoherence forDescriptiveTextStructuring.Ph.D. thesis,UniversityofEdinburgh,Edinburgh, Scotland. 

Karamanis, Nikiforos, Massimo Poesio, Chris Mellish, and Jon Oberlander. 2004. Evaluating centering-based metrics of coherence for text structuring using a reliably annotated corpus.In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages391–398,Barcelona,Spain.

Karttunen,Lauri.1976.Discoursereferents. InJamesD.McCawley,editor,Syntax andSemantics:NotesfromtheLinguistic Underground,volume7.AcademicPress, NewYork,pages363–386.

Katz,IrvinR.andMalcolmI.Bauer.2001. Sourceﬁnder:Coursepreparationvia linguisticallytargetedwebsearch.Journal ofEducationalTechnologyandSociety, 4(3):45–49.

Kibble, Rodger and Richard Power. 2004. Optimising referential coherence in text generation.Computational Linguistics, 30(4):401–416.

Kincaid,J.Peter,JamesAagard,JohnO’Hara, andLarryCottrell.1981.Computer readabilityeditingsystem.IEEE TransactionsonProfessionalCommunication, 1(24):34–81.

Kincaid,PeterJ.,RobertP.Fishburne, RichardL.Rodgers,andBradS.Chissom. 1975.Derivationofnewreadability formulasforNavyenlistedpersonnel. ResearchBranchReport8-75,U.S. NavalAirStation,Memphis,TN.

Knight,Kevin and Vasileios Hatzivassiloglou. 1995. Two-level, many-path generation.In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages252–260,Cambridge,MA.

Kudo,TakuandYujiMatsumoto.2001. Chunkingwithsupportvectormachines. InThorstenJoachims,editor,Proceedings ofthe2ndAnnualMeetingoftheNorth AmericanChapteroftheAssociationfor ComputationalLinguistics,pages192–199, Pittsburgh,PA.

Kuhn,R.andR.DeMori.1990.A cache-basednaturallanguagemodelfor speechrecognition.IEEETransactionson PAMI,12(6):570–583. Kuno,Susumu.1972.Functionalsentence perspective.LinguisticInquiry,3:269–320. 

Landauer,ThomasK.andSusanT.Dumais. 1997.AsolutiontoPlato’sproblem: Thelatentsemanticanalysistheoryof acquisition,inductionandrepresentation ofknowledge.PsychologicalReview, 104(2):211–240.

Langkilde,IreneandKevinKnight.1998. Generationthatexploitscorpus-based statisticalknowledge.InProceedings ofthe17thInternationalConferenceon ComputationalLinguisticsand36th AnnualMeetingoftheAssociationfor ComputationalLinguistics,pages704–710, Montr´eal,Canada.

Lapata,Mirella.2003.Probabilistictext structuring:Experimentswithsentence ordering.InProceedingsofthe41stAnnual MeetingoftheAssociationforComputational Linguistics,pages545–552,Sapporo,Japan.

Lin,Chin-YewandEduardH.Hovy.2003. Automaticevaluationofsummariesusing n-gramco-occurrencestatistics.In Proceedingsofthe2ndHumanLanguage TechnologyConferenceandAnnualMeetingof theNorthAmericanChapteroftheAssociation forComputationalLinguistics,pages71–78, Boston,MA.

Lin,Dekang.2001.LaTaT:Languageand textanalysistools.InProceedingsofthe1st InternationalConferenceonHumanLanguage TechnologyResearch,pages222–227, SanFrancisco,CA.

Mann,WilliamC.andSandraA.Thomson. 1988.Rhetoricalstructuretheory.Text, 8(3):243–281.

Marcu,Daniel.2000.TheTheoryandPractice ofDiscourseParsingandSummarization. MITPress,Cambridge,MA. 

McKoon,GailandRogerRatcliff.1992. Inferenceduringreading.Psychological Review,99(3):440–446.

Mellish,Chris,MickO’Donnell,Jon Oberlander,andAlistairKnott.1998. Experimentsusingstochasticsearch fortextplanning.InProceedingsofthe 9thInternationalWorkshoponNatural LanguageGeneration,pages98–107, NewBrunswick,NJ.

Miltsakaki,EleniandKarenKukich.2000. Theroleofcenteringtheory’srough-shift intheteachingandevaluationofwriting skills.InProceedingsofthe38thAnnual MeetingoftheAssociationforComputational Linguistics,pages408–415,HongKong. Mitchell,JamesV.1985.TheNinthMental MeasurementsYearbook.Universityof NebraskaPress,Lincoln.

Morris,JaneandGraemeHirst.1991.Lexical cohesioncomputedbythesauralrelations asanindicatorofthestructureoftext. ComputationalLinguistics,1(17):21–43. 

Ng,VincentandClaireCardie.2002. Improvingmachinelearningapproaches 33 ComputationalLinguistics Volume34,Number1 

tocoreferenceresolution.InProceedingsof the40thAnnualMeetingoftheAssociationfor ComputationalLinguistics,pages104–111, Philadelphia,PA.

Papineni,Kishore,SalimRoukos,Todd Ward,andWei-JingZhu.2002.Bleu:A methodforautomaticevaluationof machinetranslation.InProceedingsofthe 40thAnnualMeetingoftheAssociationfor ComputationalLinguistics,pages311–318, Philadelphia,PA.

Poesio,Massimo,RosemaryStevenson, BarbaraDiEugenio,andJanetHitzeman. 2004.Centering:Aparametrictheoryand itsinstantiations.ComputationalLinguistics, 30(3):309–363.

Pradhan,Sameer,KadriHacioglu,Valerie Krugler,WayneWard,JamesH.Martin,andDanJurafsky.2005.Supportvector learningforsemanticargument classiﬁcation.MachineLearning, 60(1):11–39.

Prince,Ellen.1978.Acomparisonofwh-clefts andit-cleftsindiscourse.Language, 54:883–906.

Prince,Ellen.1981.Towardataxonomyof given-newinformation.InPeterCole, editor,RadicalPragmatics.AcademicPress, NewYork,pages223–255.

Reiter,EhudandRobertDale.2000. BuildingNatural-LanguageGenerationSystems.CambridgeUniversityPress,Cambridge,England.

Schwarm,SarahE.andMariOstendorf.2005. Readinglevelassessmentusingsupport vectormachinesandstatisticallanguage models.InProceedingsofthe43rdAnnual MeetingoftheAssociationforComputational Linguistics,pages523–530,AnnArbor,MI.

Scott,DoniaandClarisseSieckenius deSouza.1990.Gettingthemessageacross inRST-basedtextgeneration.InRobert Dale,ChrisMellish,andMichaelZock, editors,CurrentResearchinNatural LanguageGeneration.AcademicPress, NewYork,pages47–73. 

Sidner,CandaceL.1979.Towardsa ComputationalTheoryofDeﬁniteAnaphora ComprehensioninEnglishDiscourse.Ph.D. thesis,MIT.

Soon,W.M.,HweeTouNg,andD.C.Y.Lim. 2001.Amachinelearningapproachto coreferenceresolutionofnounphrases. ComputationalLinguistics,27(4):521–544. 

Stenner,A.Jackson.1996.Measuring readingcomprehensionwiththe lexileframework.Presentedatthe CaliforniaComparabilitySymposium, Burlingame,CA.

Strube, Michael and Udo Hahn.1999. Functional centering —Grounding referential coherence in information structure. Computational Linguistics, 25(3):309–344.

Toutanova, Kristina,Penka Markova,and Christopher D. Manning.2004. The leaf projection path view of parse trees: Exploring string kernels for HPSG parse selection.In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages166–173,Barcelona,Spain.

Vapnik,Vladimir.1998.StatisticalLearning Theory.Wiley,Chichester,UK. Vilain,Marc,JohnBurger,JohnAberdeen,

Dennis Connolly,and Lynette Hirschman. 1995.A model-theoretic coreference scoring scheme. In Proceedings of the 6th Message Understanding Conference (MUC-6), pages45–52,SanFrancisco,CA.

Walker,Marilyn,Masayo Iida,and Sharon Cote.1994. Japanese discourse and the process of centering. Computational Linguistics,20(2):193–232.

Walker,Marilyn,AravindJoshi,and EllenPrince,editors.1998.Centering TheoryinDiscourse.ClarendonPress, Oxford,UK.

Walker, MarilynA.,Owen Rambow,and Monica Rogati.2001. Spot:A trainable sentence planner. In Proceedingso f the 2nd Annual Meeting of the North American Chapter of the Association for Computational Linguistics,pages17–24,Pittsburgh,PA.

Weiss,SholomM.andCasimirA. Kulikowski.1991.ComputerSystemsthat Learn:ClassiﬁcationandPredictionMethods from,Statistics,NeuralNets,Machine Learning,andExpertSystems.Morgan Kaufmann,SanMateo,CA.

Witten,IanH.andEibeFrank.2000.Data Mining:PracticalMachineLearningTools andTechniqueswithJavaImplementations. MorganKaufman,SanMateo,CA.

