SurveyArticle
Inter-CoderAgreementfor
ComputationalLinguistics
RonArtstein
∗
UniversityofEssex
MassimoPoesio
∗∗
UniversityofEssex/Universit`adiTrento
This article is a survey of methods for measuring agreement among corpus annotators. It exposes
the mathematics and underlying assumptions of agreement coefﬁcients, covering Krippendorff’s
alpha as well as Scott’s pi and Cohen’s kappa;discusses the use of coefﬁcients in several annota-
tion tasks;and argues that weighted, alpha-like coefﬁcients, traditionally less used than kappa-
like measures in computational linguistics, may be more appropriate for many corpus annotation
tasks—but that their use makes the interpretation of the value of the coefﬁcient even harder.
1.IntroductionandMotivations
Since the mid 1990s, increasing effort has gone into putting semantics and discourse
researchonthesameempiricalfootingasotherareasofcomputationallinguistics(CL).
This soon led to worries about the subjectivity of the judgments required to create
annotatedresources,muchgreaterforsemanticsandpragmaticsthanfortheaspectsof
languageinterpretationofconcerninthecreationofearlyresourcessuchastheBrown
corpus (Francis and Kucera 1982), the British National Corpus (Leech, Garside, and
Bryant1994),orthePennTreebank(Marcus,Marcinkiewicz,andSantorini1993).Prob-
lems with early proposals for assessing coders’ agreement on discourse segmentation
tasks(suchasPassonneauandLitman1993)ledCarletta(1996)tosuggesttheadoption
oftheKcoefﬁcientofagreement,avariantofCohen’sκ(Cohen1960),asthishadalready
beenusedforsimilarpurposesincontentanalysisforalongtime.
1
Carletta’sproposals
∗ NowattheInstituteforCreativeTechnologies,UniversityofSouthernCalifornia,13274FijiWay,Marina
DelRey,CA90292.
∗∗ AttheUniversityofEssex:DepartmentofComputingandElectronicSystems,UniversityofEssex,
WivenhoePark,Colchester,CO43SQ,UK.E-mail:poesio@essex.ac.uk.AttheUniversityofTrento:
CIMeC,Universit`adegliStudidiTrento,PalazzoFedrigotti,CorsoBettini,31,38068Rovereto(TN),Italy.
E-mail:massimo.poesio@unitn.it.
1 Theliteratureisfullofterminologicalinconsistencies.Carlettacallsthecoefﬁcientofagreementshe
arguesfor“kappa,”referringtoKrippendorff(1980)andSiegelandCastellan(1988),andusingSiegel
andCastellan’sterminologyanddeﬁnitions.However,SiegelandCastellan’sstatistic,whichtheycallK,
isactuallyFleiss’sgeneralizationtomorethantwocodersofScott’s π,notoftheoriginalCohen’sκ;to
confusemattersfurther,SiegelandCastellanusetheGreekletterκ toindicatetheparameterwhichis
estimatedbyK.Inwhatfollows,weuseκ toindicateCohen’soriginalcoefﬁcientanditsgeneralization
tomorethantwocoders,andKforthecoefﬁcientdiscussedbySiegelandCastellan.
Submissionreceived:26August2005;revisedsubmissionreceived:21December2007;acceptedfor
publication:28January2008.
©2008AssociationforComputationalLinguistics
ComputationalLinguistics Volume34,Number4
wereenormouslyinﬂuential,andKquicklybecamethedefactostandardformeasuring
agreement in computational linguistics not only in work on discourse (Carletta et al.1997;CoreandAllen1997;Hearst1997;PoesioandVieira1998;DiEugenio2000;Stolcke
et al. 2000; Carlson, Marcu, and Okurowski 2003) but also for other annotation tasks
(e.g.,V´eronis1998;BruceandWiebe1998;StevensonandGaizauskas2000;Craggsand
McGee Wood 2004; Mieskes and Strube 2006). During this period, however, a number
of questions have also been raised about K and similar coefﬁcients—some already in
Carletta’s own work (Carletta et al. 1997)—ranging from simple questions about the
way the coefﬁcient is computed (e.g., whether it is really applicable when more than
two coders are used), to debates about which levels of agreement can be considered
‘acceptable’(DiEugenio2000;CraggsandMcGeeWood2005),totherealizationthatK
is not appropriate for all types of agreement (Poesio and Vieira 1998; Marcu, Romera,
and Amorrortu 1999; Di Eugenio 2000; Stevenson and Gaizauskas 2000). Di Eugenio
raisedtheissueoftheeffectofskeweddistributionsonthevalueofKandpointedout
that the original κ developed by Cohen is based on very different assumptions about
coderbiasfromtheKofSiegelandCastellan(1988),whichistypicallyusedinCL.This
issueofannotatorbiaswasfurtherdebatedinDiEugenioandGlass(2004)andCraggs
andMcGeeWood(2005).DiEugenioandGlasspointedoutthatthechoiceofcalculating
chance agreement by using individual coder marginals (κ) or pooled distributions (K)
can lead to reliability values falling on different sides of the accepted 0.67 threshold,
andrecommendedreportingbothvalues.CraggsandMcGeeWoodargued,following
Krippendorff (2004a,b), that measures like Cohen’s κ are inappropriate for measur-
ing agreement. Finally, Passonneau has been advocating the use of Krippendorff’s α
(Krippendorff 1980, 2004a) for coding tasks in CL which do not involve nominal and
disjoint categories, including anaphoric annotation, wordsense tagging, and summa-
rization(Passonneau2004,2006;NenkovaandPassonneau2004;Passonneau,Habash,
andRambow2006).
Now that more than ten years have passed since Carletta’s original presentation
at the workshopon Empirical Methods in Discourse, it is time to reconsider the use
of coefﬁcients of agreement in CL in a systematic way. In this article, a survey of
coefﬁcientsofagreementandtheiruseinCL,wehavethreemaingoals.First,wediscuss
insomedetailthemathematicsandunderlyingassumptionsofthecoefﬁcientsusedor
mentioned in the CL and content analysis literatures. Second, we also cover in some
detailKrippendorff’sα,oftenmentionedbutneverreallydiscussedindetailinprevious
CLliteratureotherthaninthepapersbyPassonneaujustmentioned.Third,wereview
thepasttenyearsofexperiencewithcoefﬁcientsofagreementinCL,reconsideringthe
issuesthathavebeenraisedalsofromamathematicalperspective.
2
2.CoefﬁcientsofAgreement
2.1Agreement,Reliability,andValidity
Webeginwithaquickrecapofthegoalsofagreementstudies,inspiredbyKrippendorff
(2004a, Section 11.1). Researchers who wish to use hand-coded data—that is, data in
which items are labeled with categories, whether to support an empirical claim or to
developand test a computational model—need to show that such data are reliable.
2 Onlypartofourmaterialcouldﬁtinthisarticle.Anextendedversionofthesurveyisavailablefrom
http://cswww.essex.ac.uk/Research/nle/arrau/.
556
ArtsteinandPoesio Inter-CoderAgreementforCL
Thefundamentalassumptionbehindthemethodologiesdiscussedinthisarticleisthat
dataarereliableifcoderscanbeshowntoagreeonthecategoriesassignedtounitsto
an extent determined by the purposes of the study (Krippendorff 2004a; Craggs and
McGeeWood2005).Ifdifferentcodersproduceconsistentlysimilarresults,thenwecan
inferthattheyhaveinternalizedasimilarunderstandingoftheannotationguidelines,
andwecanexpectthemtoperformconsistentlyunderthisunderstanding.
Reliability is thus a prerequisite for demonstrating the validity of the coding
scheme—that is, to show that the coding scheme captures the “truth” of the phenom-
enonbeingstudied,incasethismatters:Iftheannotatorsarenotconsistenttheneither
some of them are wrong or else the annotation scheme is inappropriate for the data.
(Justasinreallife,thefactthatwitnessestoaneventdisagreewitheachothermakesit
difﬁcultforthirdpartiestoknowwhatactuallyhappened.)However,itisimportantto
keepinmindthatachievinggoodagreementcannotensurevalidity:Twoobserversof
thesameeventmaywellsharethesameprejudicewhilestillbeingobjectivelywrong.
2.2ACommonNotation
It is useful to think of a reliability study as involving a set of items (markables), a
set of categories, and a set of coders (annotators) who assign to each item a unique
categorylabel.Thediscussionsofreliabilityintheliteratureoftenusedifferentnotations
toexpresstheseconcepts.Weintroduceauniformnotation,whichwehopewillmake
therelationsbetweenthedifferentcoefﬁcientsofagreementclearer.
•Thesetofitemsis{i | i ∈ I }andisofcardinalityi.
•Thesetofcategoriesis{k | k ∈ K }andisofcardinalityk.
•Thesetofcodersis{c | c ∈ C }andisofcardinalityc.
Confusionalsoarisesfromtheuseoftheletter P,whichisusedintheliteraturewithat
least three distinct interpretations, namely “proportion,” “percent,” and “probability.”
Wewillusethefollowingnotationuniformlythroughoutthearticle.
•A
o
isobservedagreementandD
o
isobserveddisagreement.
•A
e
andD
e
areexpectedagreementandexpecteddisagreement,
respectively.Therelevantcoefﬁcientwillbeindicatedwithasuperscript
whenanambiguitymayarise(forexample,A
π
e
istheexpectedagreement
usedforcalculating π,andA
κ
e
istheexpectedagreementusedfor
calculatingκ).
•P(·) isreservedfortheprobabilityofavariable,and
ˆ
P(·) isanestimateof
suchprobabilityfromobserveddata.
Finally,weusen withasubscripttoindicatethenumberofjudgmentsofagiventype:
• n
ik
isthenumberofcoderswhoassigneditem i tocategory k;
• n
ck
isthenumberofitemsassignedbycoder c tocategory k;
• n
k
isthetotalnumberofitemsassignedbyallcoderstocategory k.
557
ComputationalLinguistics Volume34,Number4
2.3AgreementWithoutChanceCorrection
Thesimplestmeasureofagreementbetweentwocodersispercentageofagreementor
observedagreement,deﬁnedforexamplebyScott(1955,page323)as“thepercentageof
judgmentsonwhichthetwoanalystsagreewhencodingthesamedataindependently.”
This is the number of items on which the coders agree divided by the total number
of items. More precisely, and looking ahead to the following discussion, observed
agreementisthearithmeticmeanoftheagreementvalueagr
i
forallitems i ∈ I,deﬁned
asfollows:
agr
i
=
braceleftbigg
1ifthetwocodersassign i tothesamecategory
0ifthetwocodersassign i todifferentcategories
Observedagreementoverthevaluesagr
i
forallitems i ∈ I isthen:
A
o
=
1
i
∑
i∈I
agr
i
For example, let us assume a very simple annotation scheme for dialogue acts in
information-seekingdialogueswhichmakesabinarydistinctionbetweenthecategories
statement and info-request, as in the DAMSL dialogue act scheme (Allen and Core
1997).Twocodersclassify100utterancesaccordingtothisschemeasshowninTable1.
Percentage agreement for this data set is obtained by summing upthe cells on the
diagonalanddividingbythetotalnumberofitems:A
o
=(20+50)/100 = 0.7.
Observedagreemententersinthecomputationofallthemeasuresofagreementwe
consider, but on its own it does not yield values that can be compared across studies,
because some agreement is due to chance, and the amount of chance agreement is
affectedbytwofactorsthatvaryfromonestudytotheother.Firstofall,asScott(1955,
page 322) points out, “[percentage agreement] is biased in favor of dimensions with a
small number of categories.” In other words, given two coding schemes for the same
phenomenon,theonewithfewercategorieswillresultinhigherpercentageagreement
just by chance. If two coders randomly classify utterances in a uniform manner using
theschemeofTable1,wewouldexpectanequalnumberofitemstofallineachofthe
fourcellsinthetable,andthereforepurechancewillcausethecoderstoagreeonhalfof
theitems(thetwocellsonthediagonal:
1
4
+
1
4
).Butsupposewewanttoreﬁnethesimple
binarycodingschemebyintroducinganewcategory,check,asintheMapTaskcoding
scheme (Carletta et al. 1997). If two coders randomly classify utterances in a uniform
manner using the three categories in the second scheme, they would only agree on a
thirdoftheitems(
1
9
+
1
9
+
1
9
).
Table1
Asimpleexampleofagreementondialogueacttagging.
CODERA
STAT IREQ TOTAL
STAT 20 20 40
CODERBIREQ 10 50 60
TOTAL 30 70 100
558
ArtsteinandPoesio Inter-CoderAgreementforCL
The second reason percentage agreement cannot be trusted is that it does not
correct for the distribution of items among categories: We expect a higher percentage
agreement when one category is much more common than the other. This problem,
already raised by Hsu and Field (2003, page 207) among others, can be illustrated
using the following example (Di Eugenio and Glass 2004, example 3, pages 98–99).
Suppose95%ofutterancesinaparticulardomainarestatement,andonly5%areinfo-
request. We would then expect by chance that 0.95×0.95 = 0.9025 of the utterances
would be classiﬁed as statement by both coders, and 0.05×0.05 = 0.0025 as info-
request, so the coders would agree on 90.5% of the utterances. Under such circum-
stances,aseeminglyhighobservedagreementof90%isactuallyworsethanexpectedby
chance.
Theconclusionreachedintheliteratureisthatinordertogetﬁguresthatarecompa-
rableacrossstudies,observedagreementhastobeadjustedforchanceagreement.These
arethemeasureswewillreviewintheremainderofthisarticle.Wewillnotlookatthe
variantsofpercentageagreementusedinCLworkondiscoursebeforetheintroduction
ofkappa,suchaspercentageagreementwithanexpertandpercentageagreementwith
themajority;seeCarletta(1996)fordiscussionandcriticism.
3
2.4Chance-CorrectedCoefﬁcientsforMeasuringAgreementbetweenTwoCoders
All of the coefﬁcients of agreement discussed in this article correct for chance on the
basisofthesameidea.Firstweﬁndhowmuchagreementisexpectedbychance:Letus
callthisvalueA
e
.Thevalue1−A
e
willthenmeasurehowmuchagreementoverand
abovechanceisattainable;thevalueA
o
−A
e
willtellushowmuchagreementbeyond
chancewasactuallyfound.TheratiobetweenA
o
−A
e
and1−A
e
willthentelluswhich
proportion of the possible agreement beyond chance was actually observed. This idea
isexpressedbythefollowingformula.
S,π,κ =
A
o
−A
e
1−A
e
The three best-known coefﬁcients, S (Bennett, Alpert, and Goldstein 1954), π (Scott
1955), and κ (Cohen 1960), and their generalizations, all use this formula; whereas
Krippendorff’s α is based on a related formula expressed in terms of disagreement
(see Section 2.6). All three coefﬁcients therefore yield values of agreement between
−A
e
/1−A
e
(noobservedagreement)and1(observedagreement=1),withthevalue0
signifying chance agreement (observed agreement = expected agreement). Note also
that whenever agreement is less than perfect (A
o
<1), chance-corrected agreement
will be strictly lower than observed agreement, because some amount of agreement
isalwaysexpectedbychance.
Observed agreement A
o
is easy to compute, and is the same for all three
coefﬁcients—the proportion of items on which the two coders agree. But the notion
of chance agreement, or the probability that two coders will classify an arbitrary item
asbelongingtothesamecategorybychance,requiresamodelofwhatwouldhappen
if coders’ behavior was only by chance. All three coefﬁcients assume independence of
the two coders—that is, that the chance of c
1
and c
2
agreeing on any given category k
3 Theextendedversionofthearticlealsoincludesadiscussionofwhy
χ
2
andcorrelationcoefﬁcientsare
notappropriateforthistask.
559
ComputationalLinguistics Volume34,Number4
Table2
ThevalueofdifferentcoefﬁcientsappliedtothedatafromTable1.
Coefﬁcient Expectedagreement Chance-correctedagreement
S 2×(
1
2
)
2
= 0.5 (0.7−0.5)/(1−0.5)=0.4
π 0.35
2
+0.65
2
= 0.545 (0.7−0.545)/(1−0.545) ≈ 0.341
κ 0.3×0.4+0.6×0.7 = 0.54 (0.7−0.54)/(1−0.54) ≈ 0.348
Observed agreement for all the coefﬁcients is 0.7.
is the product of the chance of each of them assigning an item to that category:
P(k|c
1
) ·P(k|c
2
).
4
Expected agreement is then the probability of c
1
and c
2
agreeing on
anycategory,thatis,thesumoftheproductoverallcategories:
A
S
e
= A
π
e
= A
κ
e
=
∑
k∈K
P(k|c
1
)·P(k|c
2
)
Thedifferencebetween S, π,andκ liesintheassumptionsleadingtothecalculationof
P(k|c
i
),thechancethatcoder c
i
willassignanarbitraryitemtocategory k (Zwick1988;
HsuandField2003).
S: Thiscoefﬁcientisbasedontheassumptionthatifcoderswereoperating
bychancealone,wewouldgetauniformdistribution:Thatis,foranytwo
coders c
m,c
n
andanytwocategories k
j,k
l,P(k
j
|c
m
)=P(k
l
|c
n
).
π: Ifcoderswereoperatingbychancealone,wewouldgetthesame
distributionforeachcoder:Foranytwocoders c
m,c
n
andanycategory k,
P(k|c
m
)=P(k|c
n
).
κ: Ifcoderswereoperatingbychancealone,wewouldgetaseparate
distributionforeachcoder.
Additionally, the lack of independent prior knowledge of the distribution of items
amongcategoriesmeansthatthedistributionofcategories(forπ)andthepriorsforthe
individualcoders(for κ)havetobeestimatedfromtheobserveddata.Table2demon-
stratestheeffectofthedifferentchancemodelsonthecoefﬁcientvalues.Theremainder
ofthissectionexplainshowthethreecoefﬁcientsarecalculatedwhenthereliabilitydata
comefromtwocoders;wewilldiscussavarietyofproposedgeneralizationsstartingin
Section2.5.
2.4.1 All
Categories Are Equally Likely: S. The simplest way of discounting for chance
is the one adopted to compute the coefﬁcient S (Bennett, Alpert, and Goldstein 1954),
alsoknownintheliteratureas C, κ
n, G,andRE (seeZwick1988;HsuandField2003).
As noted previously, the computation of S is based on an interpretation of chance as
a random choice of category from a uniform distribution—that is, all categories are
equally likely. If coders classify the items into k categories, then the chance P(k|c
i
) of
4 Theindependenceassumptionhasbeenthesubjectofmuchcriticism,forexamplebyJohnS.Uebersax.
http://ourworld.compuserve.com/homepages/jsuebersax/agree.htm.
560
ArtsteinandPoesio Inter-CoderAgreementforCL
anycoderassigninganitemtocategory k undertheuniformityassumptionis
1
k
;hence
thetotalagreementexpectedbychanceis
A
S
e
=
∑
k∈K
1
k
·
1
k
= k·
parenleftbigg
1
k
parenrightbigg
2
=
1
k
Thecalculationofthevalueof S fortheﬁguresinTable1isshowninTable2.
The coefﬁcient S is problematic in many respects. The value of the coefﬁcient can
beartiﬁciallyincreasedsimplybyaddingspuriouscategorieswhichthecoderswould
neveruse(Scott1955,pages 322–323).Inthecase ofCL,forexample, S wouldreward
designing extremely ﬁne-grained tagsets, provided that most tags are never actually
encounteredinrealdata.AdditionallimitationsarenotedbyHsuandField(2003).Ithas
beenarguedthatuniformityisthebestmodelforachancedistributionofitemsamong
categoriesifwehavenoindependentpriorknowledgeofthedistribution(Brennanand
Prediger1981).However,alackofpriorknowledgedoesnotmeanthatthedistribution
cannotbeestimatedposthoc,andthisiswhattheothercoefﬁcientsdo.
2.4.2 A
Single Distribution: π. Alloftheothermethodsfordiscountingchanceagreement
we discuss in this article attempt to overcome the limitations of S’s strong uniformity
assumptionusinganideaﬁrstproposedbyScott(1955):Usetheactualbehaviorofthe
coderstoestimatethepriordistributionofthecategories.Asnotedearlier,Scottbased
his characterization of π on the assumption that random assignment of categories to
items, by any coder, is governed by the distribution of items among categories in the
actualworld.Thebestestimateofthisdistributionis
ˆ
P(k),theobservedproportion of
itemsassignedtocategory k bybothcoders.
P(k|c
1
)=P(k|c
2
)=
ˆ
P(k)
ˆ
P(k), the observed proportion of items assigned to category k by both coders, is the
total number of assignments to k by both coders n
k, divided by the overall number of
assignments,whichforthetwo-codercaseistwicethenumberofitemsi:
ˆ
P(k)=
n
k
2i
Giventheassumptionthatcodersactindependently,expectedagreementiscomputed
asfollows.
A
π
e
=
∑
k∈K
ˆ
P(k)·
ˆ
P(k)=
∑
k∈K
parenleftBig
n
k
2i
parenrightBig
2
=
1
4i
2
∑
k∈K
n
2
k
It is easy to show that for any set of coding data, A
π
e
≥ A
S
e
and therefore π ≤ S,with
the limiting case (equality) obtaining when the observed distribution of items among
categoriesisuniform.
2.4.3 Individual
Coder Distributions: κ. The method proposed by Cohen (1960) to calcu-
late expected agreement A
e
in his κ coefﬁcient assumes that random assignment of
categories to items is governed by prior distributions that are unique to each coder,
andwhichreﬂectindividualannotatorbias.Anindividualcoder’spriordistributionis
561
ComputationalLinguistics Volume34,Number4
estimatedbylookingatheractualdistribution:P(k|c
i
),theprobabilitythatcoder c
i
will
classify anarbitrary itemintocategory k,isestimatedbyusing
ˆ
P(k|c
i
),theproportion
ofitemsactually assigned bycoder c
i
tocategory k;thisisthenumberofassignments
to k by c
i,n
c
i
k,dividedbythenumberofitemsi.
P(k|c
i
)=
ˆ
P(k|c
i
)=
n
c
i
k
i
Asinthecaseof S andπ,theprobabilitythatthetwocoders c
1
and c
2
assignanitemto
aparticularcategory k ∈ K isthejointprobabilityofeachcodermakingthisassignment
independently.Forκthisjointprobabilityis
ˆ
P(k|c
1
)·
ˆ
P(k|c
2
);expectedagreementisthen
thesumofthisjointprobabilityoverallthecategories k ∈ K.
A
κ
e
=
∑
k∈K
ˆ
P(k|c
1
)·
ˆ
P(k|c
2
)=
∑
k∈K
n
c
1
k
i
·
n
c
2
k
i
=
1
i
2
∑
k∈K
n
c
1
k
n
c
2
k
Itiseasytoshowthatforanysetofcodingdata,A
π
e
≥ A
κ
e
andthereforeπ ≤ κ,withthe
limitingcase(equality)obtainingwhentheobserveddistributionsofthetwocodersare
identical.Therelationshipbetween κ and S isnotﬁxed.
2.5MoreThanTwoCoders
In corpus annotation practice, measuring reliability with only two coders is seldom
consideredenough,exceptforsmall-scalestudies.Sometimesresearchersrunreliability
studies with more than two coders, measure agreement separately for each pair of
coders,andreporttheaverage.However,abetterpracticeistousegeneralizedversions
of the coefﬁcients. A generalization of Scott’s π is proposed in Fleiss (1971), and a
generalization of Cohen’s κ is given in Davies and Fleiss (1982). We will call these
coefﬁcients multi-π and multi-κ, respectively, dropping the multi-preﬁxes when no
confusionisexpectedtoarise.
5
2.5.1 Fleiss’s Multi-π. With more than two coders, the observed agreement A
o
can no
longer be deﬁned as the percentage of items on which there is agreement, because
inevitably there will be items on which some coders agree and others disagree. The
solution proposed in the literature is to measure pairwise agreement (Fleiss 1971):
Deﬁne the amount of agreement on a particular item as the proportion of agreeing
judgmentpairsoutofthetotalnumberofjudgmentpairsforthatitem.
Multiple coders also pose a problem for the visualization of the data. When the
number of coders c is greater than two, judgments cannot be shown in a contingency
table like Table 1, because each coder has to be represented in a separate dimension.
5 Duetohistoricalaccident,theterminologyintheliteratureisconfusing.Fleiss(1971)proposeda
coefﬁcientofagreementformultiplecodersandcalleditκ,eventhoughitcalculatesexpectedagreement
basedonthecumulativedistributionofjudgmentsbyallcodersandisthusbetterthoughtofasa
generalizationofScott’s π.Thisunfortunatechoiceofnamewasthecauseofmuchconfusionin
subsequentliterature:Often,studieswhichclaimtogiveageneralizationofκ tomorethantwocoders
actuallyreportFleiss’scoefﬁcient(e.g.,BartkoandCarpenter1976;SiegelandCastellan1988;DiEugenio
andGlass2004).SinceCarletta(1996)introducedreliabilitytotheCLcommunitybasedonthedeﬁnitions
ofSiegelandCastellan(1988),theterm“kappa”hasbeenusuallyassociatedinthiscommunitywith
SiegelandCastellan’sK,whichisineffectFleiss’scoefﬁcient,thatis,ageneralizationofScott’s π.
562
ArtsteinandPoesio Inter-CoderAgreementforCL
Fleiss(1971)thereforeusesadifferenttypeoftablewhichlistseachitemwiththenum-
berofjudgmentsitreceivedforeachcategory;SiegelandCastellan(1988)useasimilar
table,whichDiEugenioandGlass(2004)callanagreementtable.Table3isanexample
of an agreement table, in which the same 100 utterances from Table 1 are labeled by
three coders instead of two. Di Eugenio and Glass (page 97) note that compared to
contingencytableslikeTable1,agreementtableslikeTable3loseinformationbecause
they do not say which coder gave each judgment. This information is not used in the
calculationofπ,butisnecessaryfordeterminingtheindividualcoders’distributionsin
thecalculationof κ.(Agreementtablesalsoaddinformationcomparedtocontingency
tables, namely, the identity of the items that make upeach contingency class, but this
informationisnotusedinthecalculationofeitherκ or π.)
Let n
ik
stand for the number of times an item i is classiﬁed in category k (i.e., the
number of coders that make such a judgment): For example, given the distribution in
Table3,n
Utt
1
Stat
= 2andn
Utt
1
IReq
= 1.Eachcategoryk contributes(
n
ik
2
)pairsofagreeing
judgmentsforitem i;theamountofagreementagr
i
foritem i isthesumof (
n
ik
2
) overall
categories k ∈ K,dividedby (
c
2
),thetotalnumberofjudgmentpairsperitem.
agr
i
=
1
(
c
2
)
∑
k∈K
parenleftbigg
n
ik
2
parenrightbigg
=
1
c(c−1)
∑
k∈K
n
ik
(n
ik
−1)
Forexample,giventheresultsinTable3,weﬁndtheagreementvalueforUtterance1
asfollows.
agr
1
=
1
(
3
2
)
parenleftbiggparenleftbigg
n
Utt
1
Stat
2
parenrightbigg
+
parenleftbigg
n
Utt
1
IReq
2
parenrightbiggparenrightbigg
=
1
3
(1+0) ≈ 0.33
Theoverallobservedagreementisthemeanofagr
i
forallitems i ∈ I.
A
o
=
1
i
∑
i∈I
agr
i
=
1
ic(c−1)
∑
i∈I
∑
k∈K
n
ik
(n
ik
−1)
(Notice that this deﬁnition of observed agreement is equivalent to the mean of the
two-coderobservedagreementvaluesfromSection2.4forallcoderpairs.)
If observed agreement is measured on the basis of pairwise agreement (the pro-
portionofagreeingjudgmentpairs),itmakessensetomeasureexpectedagreementin
termsofpairwisecomparisonsaswell,thatis,astheprobabilitythatanypairofjudg-
mentsforanitemwouldbeinagreement—or,saidotherwise,theprobabilitythattwo
Table3
Agreementtablewiththreecoders.
STAT IREQ
Utt
1
21
Utt
2
03
.
.
.
Utt
100
12
TOTAL 90(0.3) 210(0.7)
563
ComputationalLinguistics Volume34,Number4
arbitrarycoderswouldmakethesamejudgmentforaparticularitembychance.Thisis
theapproachtakenbyFleiss(1971).LikeScott,Fleissinterprets“chanceagreement”as
theagreementexpectedonthebasisofasingledistributionwhichreﬂectsthecombined
judgmentsofallcoders,meaningthatexpectedagreementiscalculatedusing
ˆ
P(k),the
overall proportion of items assigned to category k, which is the total number of such
assignmentsbyallcodersn
k
dividedbytheoverallnumberofassignments.Thelatter,
inturn,isthenumberofitemsimultipliedbythenumberofcodersc.
ˆ
P(k)=
1
ic
n
k
As in the two-coder case, the probability that two arbitrary coders assign an item to a
particular category k ∈ K is assumed to be the joint probability of each coder making
this assignment independently, that is (
ˆ
P(k))
2
. The expected agreement is the sum of
thisjointprobabilityoverallthecategories k ∈ K.
A
π
e
=
∑
k∈K
parenleftbig
ˆ
P(k)
parenrightbig
2
=
∑
k∈K
parenleftbigg
1
ic
n
k
parenrightbigg
2
=
1
(ic)
2
∑
k∈K
n
2
k
Multi-π isthecoefﬁcientthatSiegelandCastellan(1988)callK.
2.5.2 Multi-κ. It is fairly straightforward to adapt Fleiss’s proposal to generalize
Cohen’s κ proper to more than two coders, calculating expected agreement based on
individual coder marginals. A detailed proposal can be found in Davies and Fleiss
(1982),orintheextendedversionofthisarticle.
2.6Krippendorff’sα andOtherWeightedAgreementCoefﬁcients
A serious limitation of both π and κ is that all disagreements are treated equally. But
especiallyforsemanticandpragmaticfeatures,disagreementsarenotallalike.Evenfor
the relatively simple case of dialogue act tagging, a disagreement between an accept
andarejectinterpretationofanutteranceisclearlymoreseriousthanadisagreement
between an info-request and a check. For tasks such as anaphora resolution, where
reliabilityisdeterminedbymeasuringagreementonsets(coreferencechains),allowing
for degrees of disagreement becomes essential (see Section 4.4). Under such circum-
stances, π andκ arenotveryuseful.
In this section we discuss two coefﬁcients that make it possible to differentiate
between types of disagreements: α (Krippendorff 1980, 2004a), which is a coefﬁcient
deﬁned in a general way that is appropriate for use with multiple coders, different
magnitudesofdisagreement,andmissingvalues,andisbasedonassumptionssimilar
tothoseof π;andweightedkappaκ
w
(Cohen1968),ageneralizationofκ.
2.6.1 Krippendorff’s α. The coefﬁcient α (Krippendorff 1980, 2004a) is an extremely ver-
satile agreement coefﬁcient based on assumptions similar to π, namely, that expected
agreement is calculated by looking at the overall distribution of judgments without
regard to which coders produced these judgments. It applies to multiple coders, and
it allows for different magnitudes of disagreement. When all disagreements are con-
sidered equal it is nearly identical to multi-π, correcting for small sample sizes by
using an unbiased estimator for expected agreement. In this section we will present
564
ArtsteinandPoesio Inter-CoderAgreementforCL
Krippendorff’s α and relate it to the other coefﬁcients discussed in this article, but we
willstartwith α’soriginsasameasureofvariance,followingalongtraditionofusing
variancetomeasurereliability(seecitationsinRajaratnam1960;Krippendorff1970).
A sample’s variance s
2
is deﬁned as the sum of square differences from the mean
SS = ∑(x − ¯x)
2
divided by the degrees of freedom df. Variance is a useful way of
looking at agreement if coders assign numerical values to the items, as in magnitude
estimation tasks. Each item in a reliability study can be considered a separate level
in a single-factor analysis of variance: The smaller the variance around each level, the
higherthereliability.Whenagreementisperfect,thevariancewithinthelevels(s
2
within
)
is zero; when agreement is at chance, the variance within the levels is equal to the
variancebetweenthelevels,inwhichcaseitisalsoequaltotheoverallvarianceofthe
data: s
2
within
= s
2
between
= s
2
total
.Theratios s
2
within
/s
2
between
(thatis,1/F)ands
2
within
/s
2
total
are
therefore0whenagreementisperfectand1whenagreementisatchance.Additionally,
the latter ratio is bounded at 2: SS
within
≤ SS
total
by deﬁnition, and df
total
<2df
within
becauseeachitemhasatleasttwojudgments.Subtractingtheratio s
2
within
/s
2
total
from1
yieldsacoefﬁcientwhichrangesbetween−1and1,where1signiﬁesperfectagreement
and0signiﬁeschanceagreement.
α = 1−
s
2
within
s
2
total
= 1−
SS
within
/df
within
SS
total
/df
total
Wecanunpacktheformulafor α tobringittoaformwhichissimilartotheother
coefﬁcients we have looked at, and which will allow generalizing α beyond simple
numericalvalues.Theﬁrststepistogetridofthenotionofarithmeticmeanwhichliesat
theheartofthemeasureofvariance.Weobservethatforanysetofnumbers x
1,...,x
N
with a mean ¯x =
1
N
∑
N
n=1
x
n, the sum of square differences from the mean SS can be
expressedasthesumofsquareofdifferencesbetweenallthe(ordered)pairsofnumbers,
scaledbyafactorof1/2N.
SS =
N
∑
n=1
(x
n
− ¯x)
2
=
1
2N
N
∑
n=1
N
∑
m=1
(x
n
− x
m
)
2
For calculating α we considered each item to be a separate level in an analysis of
variance; the number of levels is thus the number of items i, and because each coder
marks each item, the number of observations for each item is the number of coders c.
Within-levelvarianceisthesumofthesquaredifferencesfromthemeanofeachitem,
SS
within
= ∑
i
∑
c
(x
ic
− ¯x
i
)
2, divided by the degrees of freedom df
within
= i(c−1).We
canexpressthisasthesumofthesquaresofthedifferencesbetweenallofthejudgment
pairsforeachitem,summedoverallitemsandscaledbytheappropriatefactor.Weuse
the notation x
ic
for the value given by coder c to item i,and¯x
i
for the mean of all the
valuesgiventoitem i.
s
2
within
=
SS
within
df
within
=
1
i(c−1)
∑
i∈I
∑
c∈C
(x
ic
− ¯x
i
)
2
=
1
2ic(c−1)
∑
i∈I
c
∑
m=1
c
∑
n=1
(x
ic
m
− x
ic
n
)
2
Thetotalvarianceisthesumofthesquaredifferencesofalljudgmentsfromthegrand
mean, SS
total
= ∑
i
∑
c
(x
ic
− ¯x)
2,dividedbythedegreesoffreedom df
total
= ic−1.This
565
ComputationalLinguistics Volume34,Number4
can be expressed as the sum of the squares of the differences between all of the judg-
mentspairswithoutregardtoitems,againscaledbytheappropriatefactor.Thenotation
¯x istheoverallmeanofallthejudgmentsinthedata.
s
2
total
=
SS
total
df
total
=
1
ic−1
∑
i∈I
∑
c∈C
(x
ic
− ¯x)
2
=
1
2ic(ic−1)
i
∑
j=1
c
∑
m=1
i
∑
l=1
c
∑
n=1
(x
i
j
c
m
− x
i
l
c
n
)
2
Nowthatwehaveremovedreferencestomeansfromourformulas,wecanabstractover
themeasureofvariance.Wedeﬁneadistancefunctiondwhichtakestwonumbersand
returnsthesquareoftheirdifference.
d
ab
=(a − b)
2
We also simplify the computation by counting all the identical value assignments
together. Each unique value used by the coders will be considered a category k ∈ K.
We use n
ik
for the number of times item i is given the value k, that is, the number of
codersthatmakesuchajudgment.Forevery(ordered)pairofdistinctvalues k
a,k
b
∈ K
there are n
ik
a
n
ik
b
pairs of judgments of item i, whereas for non-distinct values there
aren
ik
a
(n
ik
a
−1) pairs.Weusethisnotationtorewritetheformulaforthewithin-level
variance. D
α
o, the observed disagreement for α, is deﬁned as twice the variance within
the levels in order to get rid of the factor 2 in the denominator; we also simplify the
formulabyusingthemultipliern
ik
a
n
ik
a
foridenticalcategories—thisisallowedbecause
d
kk
= 0forall k.
D
α
o
= 2s
2
within
=
1
ic(c−1)
∑
i∈I
k
∑
j=1
k
∑
l=1
n
ik
j
n
ik
l
d
k
j
k
l
We perform the same simpliﬁcation for the total variance, where n
k
stands for the
total number of times the value k is assigned to any item by any coder. The expected
disagreementfor α,D
α
e,istwicethetotalvariance.
D
α
e
= 2s
2
total
=
1
ic(ic−1)
k
∑
j=1
k
∑
l=1
n
k
j
n
k
l
d
k
j
k
l
Because both expected and observed disagreement are twice the respective vari-
ances, the coefﬁcient α retains the same form when expressed with the disagreement
values.
α = 1−
D
o
D
e
Nowthatαhasbeenexpressedwithoutexplicitreferencetomeans,differences,and
squares,itcanbegeneralizedtoavarietyofcodingschemesinwhichthelabelscannot
beinterpretedasnumericalvalues:Allonehastodoistoreplacethesquaredifference
functiondwithadifferentdistancefunction.Krippendorff(1980,2004a)offersdistance
metricssuitablefornominal,interval,ordinal,andratioscales.Ofparticularinterestis
566
ArtsteinandPoesio Inter-CoderAgreementforCL
thefunctionfornominalcategories,thatis,afunctionwhichconsidersalldistinctlabels
equallydistantfromoneanother.
d
ab
=
braceleftbigg
0ifa = b
1ifa negationslash= b
Itturnsoutthatwiththisdistancefunction,theobserveddisagreementD
α
o
isexactlythe
complement of the observed agreement of Fleiss’s multi-π,1−A
π
o, and the expected
disagreement D
α
e
differs from 1−A
π
e
by a factor of (ic−1)/ic; the difference is due
to the fact that π uses a biased estimator of the expected agreement in the population
whereas α uses an unbiased estimator. The following equation shows that given the
correspondence between observed and expected agreement and disagreement, theco-
efﬁcientsthemselvesarenearlyequivalent.
α = 1−
D
α
o
D
α
e
≈ 1−
1−A
π
o
1−A
π
e
=
1−A
π
e
−(1−A
π
o
)
1−A
π
e
=
A
π
o
−A
π
e
1−A
π
e
= π
Fornominaldata,thecoefﬁcients π and α approacheachotheraseitherthenumberof
itemsorthenumberofcodersapproachesinﬁnity.
Krippendorff’s α will work with any distance metric, provided that identical cat-
egories always have a distance of zero (d
kk
= 0 for all k). Another useful constraint is
symmetry (d
ab
= d
ba
for all a,b). This ﬂexibility affords new possibilities for analysis,
whichwewillillustrateinSection4.Weshouldalsonote,however,thattheﬂexibility
also creates new pitfalls, especially in cases where it is not clear what the natural dis-
tancemetricis.Forexample,therearedifferentwaystomeasuredissimilaritybetween
sets, and any of these measures can be justiﬁably used when the category labels are
setsofitems(asintheannotationofanaphoricrelations).Thedifferentdistancemetrics
yielddifferentvaluesof α forthesameannotationdata,makingitdifﬁculttointerpret
theresultingvalues.WewillreturntothisprobleminSection4.4.
2.6.2 Cohen’s κ
w
. A weighted variant of Cohen’s κ is presented in Cohen (1968). The
implementation of weights is similar to that of Krippendorff’s α—each pair of cate-
gories k
a,k
b
∈ K isassociatedwithaweightd
k
a
k
b,wherealargerweightindicatesmore
disagreement(Cohenusesthenotationv;hedoesnotplaceanygeneralconstraintson
theweights—notevenarequirementthatapairofidenticalcategorieshaveaweightof
zero, or that the weights be symmetric across the diagonal). The coefﬁcient is deﬁned
for two coders: The disagreement for a particular item i is the weight of the pair of
categories assigned to it by the two coders, and the overall observed disagreement is
the (normalized) mean disagreement of all the items. Let k(c
n,i) denote the category
assignedbycoder c
n
toitem i;thenthedisagreementforitem i isdisagr
i
= d
k(c
1,i)k(c
2,i)
.
TheobserveddisagreementD
o
isthemeanofdisagr
i
forallitems i,normalizedtothe
interval [0,1] throughdivisionbythemaximalweightd
max
.
D
κ
w
o
=
1
d
max
1
i
∑
i∈I
disagr
i
=
1
d
max
1
i
∑
i∈I
d
k(c
1,i)k(c
2,i)
Ifwetakealldisagreementstobeofequalweight,thatisd
k
a
k
a
= 0forallcategories k
a
andd
k
a
k
b
= 1forall k
a
negationslash= k
b,thentheobserveddisagreementisexactlythecomplement
oftheobservedagreementascalculatedinSection2.4:D
κ
w
o
= 1−A
κ
o
.
567
ComputationalLinguistics Volume34,Number4
Like κ,thecoefﬁcient κ
w
interpretsexpecteddisagreementastheamountexpected
by chance from a distinct probability distribution for each coder. These individual
distributions are estimated by
ˆ
P(k|c), the proportion of items assigned by coder c to
category k,thatisthenumberofsuchassignmentsn
ck
dividedbythenumberofitemsi.
ˆ
P(k|c)=
1
i
n
ck
The probability that coder c
1
assigns an item to category k
a
and coder c
2
assigns it to
category k
b
isthejointprobabilityofeachcodermakingthisassignmentindependently,
namely,
ˆ
P(k
a
|c
1
)
ˆ
P(k
b
|c
2
). The expected disagreement is the mean of the weights for
all (ordered) category pairs, weighted by the probabilities of the category pairs and
normalizedtotheinterval [0,1] throughdivisionbythemaximalweight.
D
κ
w
e
=
1
d
max
k
∑
j=1
k
∑
l=1
ˆ
P(k
j
|c
1
)
ˆ
P(k
l
|c
2
)d
k
j
k
l
=
1
d
max
1
i
2
k
∑
j=1
k
∑
l=1
n
c
1
k
j
n
c
2
k
l
d
k
j
k
l
If we take all disagreements to be of equal weight then the expected disagreement is
exactly the complement of the expected agreement for κ as calculated in Section 2.4:
D
κ
w
e
= 1−A
κ
e
.
Finally, the coefﬁcient κ
w
itself is the ratio of observed disagreement to expected
disagreement,subtractedfrom1inordertoyieldaﬁnalvalueintermsofagreement.
κ
w
= 1−
D
o
D
e
2.7AnIntegratedExample
We end this section with an example illustrating how all of the agreement coefﬁcients
justdiscussedarecomputed.Tofacilitatecomparisons,allcomputationswillbebased
on the annotation statistics in Table 4. This confusion matrix reports the results of an
experimentwheretwocodersclassifyasetofutterancesintothreecategories.
2.7.1 The
Unweighted Coefﬁcients. Observed agreement for all of the unweighted coefﬁ-
cients (S, κ,andπ) is calculated by counting the items on which the coders agree (the
Table4
Anintegratedcodingexample.
CODERA
STAT IREQ CHCK TOTAL
STAT 46 6 0 52
IREQ 032 0 32
CODERB
CHCK 0 6 10 16
TOTAL 46 44 10 100
568
ArtsteinandPoesio Inter-CoderAgreementforCL
ﬁgures on the diagonal of the confusion matrix in Table 4) and dividing by the total
numberofitems.
A
o
=
46+32+10
100
= 0.88
Theexpectedagreementvaluesandtheresultingvaluesforthecoefﬁcientsareshownin
Table5.Thevaluesofπ andκ areverysimilar,whichistobeexpectedwhenagreement
ishigh,becausethisimpliessimilarmarginals.NoticethatA
κ
e
<A
π
e,hence κ > π;this
reﬂectsageneralpropertyof κ and π,alreadymentionedinSection2.4,whichwillbe
elaboratedinSection3.1.
2.7.2 Weighted
Coefﬁcients. Suppose we notice that whereas Statement and Info-
Request are clearly distinct classiﬁcations, Check is somewhere between the two. We
therefore opt to weigh the distances between the categories as follows (recall that
1 denotes maximal disagreement, and identical categories are in full agreement and
thushaveadistanceof0).
Statement Info-Request Check
Statement 0 1 0.5
Info-Request 1 0 0.5
Check 0.5 0.5 0
Theobserveddisagreementiscalculatedbysummingup all thecellsinthecontingency
table, multiplying each cell by its respective weight, and dividing the total by the
numberofitems(inthefollowingcalculationweignorecellswithzeroitems).
D
o
=
46×0+6×1+32×0+6×0.5+10×0
100
=
6+3
100
= 0.09
TheonlysourcesofdisagreementinthecodingexampleofTable4arethesixutterances
marked as Info-Requests by coder A and Statements by coder B, which receive the
maximalweightof1,andthesixutterancesmarkedasInfo-RequestsbycoderAand
ChecksbycoderB,whicharegivenaweightof0.5.
Thecalculationofexpecteddisagreementfortheweightedcoefﬁcientsisshownin
Table6,andisthesumoftheexpecteddisagreementforeachcategorypairmultiplied
Table5
UnweightedcoefﬁcientsforthedatafromTable4.
Expectedagreement Chance-correctedagreement
S 3×(
1
3
)
2
=
1
3
(0.88−
1
3
)/(1−
1
3
)=0.82
π
0.46+0.52
2
+
0.44+0.32
2
+
0.10+0.16
2
= 0.4014 (0.88−0.4014)/(1−0.4014) ≈ 0.7995
κ .46×.52+.44×.32+.1×.16 = 0.396 (0.88−0.396)/(1−0.396) ≈ 0.8013
569
ComputationalLinguistics Volume34,Number4
Table6
ExpecteddisagreementoftheweightedcoefﬁcientsforthedatafromTable4.
D
α
e
(46+52)×(46+52)
2×100×(2×100−1)
×0+
(44+32)×(46+52)
2×100×(2×100−1)
×1 +
(10+16)×(46+52)
2×100×(2×100−1)
×
1
2
+
(46+52)×(44+32)
2×100×(2×100−1)
×1 +
(44+32)×(44+32)
2×100×(2×100−1)
×0 +
(10+16)×(44+32)
2×100×(2×100−1)
×
1
2
+
(46+52)×(10+16)
2×100×(2×100−1)
×
1
2
+
(44+32)×(10+16)
2×100×(2×100−1)
×
1
2
+
(10+16)×(10+16)
2×100×(2×100−1)
×0
0.4879
D
κ
w
e
46×52
100×100
×0+
44×52
100×100
×1 +
10×52
100×100
×
1
2
+
46×32
100×100
×1 +
44×32
100×100
×0 +
10×32
100×100
×
1
2
+
46×16
100×100
×
1
2
+
44×16
100×100
×
1
2
+
10×16
100×100
×0
0.49
byitsweight.Thevalueoftheweightedcoefﬁcientsisgivenbytheformula1−
D
o
D
e,so
α ≈ 1−
0.09
0.4879
≈ 0.8156,andκ
w
= 1−
0.09
0.49
≈ 0.8163.
3.BiasandPrevalence
Two issues recently raised by Di Eugenio and Glass (2004) concern the behavior of
agreementcoefﬁcientswhentheannotationdataareseverelyskewed.Oneissue,which
Di Eugenio and Glass call the bias problem,isthatπ and κ yield quite different
numerical values when the annotators’ marginal distributions are widely divergent;
the other issue, the prevalence problem, is the exceeding difﬁculty in getting high
agreementvalueswhenmostoftheitemsfallunderonecategory.Lookingatthesetwo
problemsindetailisusefulforunderstandingthedifferencesbetweenthecoefﬁcients.
3.1AnnotatorBias
The difference between π and α on the one hand and κ on the other hand lies in the
interpretation of the notion of chance agreement, whether it is the amount expected
fromthetheactualdistributionofitemsamongcategories(π)orfromindividualcoder
priors (κ). As mentioned in Section 2.4, this difference has been the subject of much
debate(Fleiss1975;Krippendorff1978,2004b;Byrt,Bishop,andCarlin1993;Zwick1988;
HsuandField2003;DiEugenioandGlass2004;CraggsandMcGeeWood2005).
A claim often repeated in the literature is that single-distribution coefﬁcients like
π and α assume that different coders produce similar distributions of items among
categories,withtheimplicationthatthesecoefﬁcientsareinapplicablewhentheanno-
tatorsshowsubstantiallydifferentdistributions.Recommendationsvary:Zwick(1988)
suggeststestingtheindividualcoders’distributionsusingthemodiﬁedχ
2
testofStuart
(1955), and discarding the annotation as unreliable if signiﬁcant systematic discrepan-
cies are observed. In contrast, Hsu and Field (2003, page 214) recommend reporting
the value of κ even when the coders produce different distributions, because it is “the
only [index] ... that could legitimately be applied in the presence of marginal hetero-
geneity”;likewise,DiEugenioandGlass(2004,page96)recommendusingκin“thevast
majority ... of discourseand dialogue-tagging efforts” where the individual coders’
distributions tend to vary. All of these proposals are based on a misconception: that
570
ArtsteinandPoesio Inter-CoderAgreementforCL
single-distribution coefﬁcients require similar distributions by the individual annota-
tors in order to work properly. This is not the case. The difference between the coefﬁ-
cientsisonlyintheinterpretationof“chanceagreement”: π-stylecoefﬁcientscalculate
the chance of agreement among arbitrary coders, whereas κ-style coefﬁcients calcu-
latethechanceofagreementamongthecoderswhoproducedthereliabilitydata.There-
fore, the choice of coefﬁcient should not depend on the magnitude of the divergence
betweenthecoders,butratheronthedesiredinterpretationofchanceagreement.
Another common claim is that individual-distribution coefﬁcients like κ “reward”
annotatorsfordisagreeingonthemarginaldistributions.Forexample,DiEugenioand
Glass(2004,page99)saythat κ suffersfromwhattheycallthebiasproblem,described
as “the paradox that κ
Co
[our κ] increases as the coders become less similar.” Similar
reservations about the use of κ have been noted by Brennan and Prediger (1981) and
Zwick (1988). However, the bias problem is less paradoxical than it sounds. Although
it is true that for a ﬁxed observed agreement, a higher difference in coder marginals
impliesalowerexpectedagreementandthereforeahigher κ value,theconclusionthat
κ penalizes coders for having similar distributions is unwarranted. This is because A
o
and A
e
are not independent: Both are drawn from the same set of observations. What
κdoesisdiscountsomeofthedisagreementresultingfromdifferentcodermarginalsby
incorporatingitintoA
e
.Whetherthisisdesirabledependsontheapplicationforwhich
thecoefﬁcientisused.
ThemostcommonapplicationofagreementmeasuresinCListoinferthereliability
of a large-scale annotation, where typically each piece of data will be marked by just
one coder, by measuring agreement on a small subset of the data which is annotated
by multiple coders. In order to make this generalization, the measure must reﬂect the
reliability of the annotation procedure, which is independent of the actual annotators
used.Reliability,orreproducibilityofthecoding,isreducedbyalldisagreements—both
random and systematic. The most appropriate measures of reliability for this purpose
are therefore single-distribution coefﬁcients like π and α, which generalize over the
individual coders and exclude marginal disagreements from the expected agreement.
ThisargumenthasbeenpresentedrecentlyinmuchdetailbyKrippendorff(2004b)and
reiteratedbyCraggsandMcGeeWood(2005).
At the same time, individual-distribution coefﬁcients like κ provide important in-
formationregardingthetrustworthiness(validity)ofthedataonwhichtheannotators
agree. As an intuitive example, think of a person who consults two analysts when
decidingwhethertobuyorsellcertainstocks.Ifoneanalystisanoptimistandtendsto
recommend buying whereas the other is a pessimist and tends to recommend selling,
theyarelikelytoagreewitheachotherlessthantwomoreneutralanalysts,sooverall
theirrecommendationsarelikelytobelessreliable—lessreproducible—thanthosethat
comefromapopulationoflike-mindedanalysts.Thisreproducibilityismeasuredbyπ.
But whenever the optimistic and pessimistic analysts agree on a recommendation for
a particular stock, whether it is “buy” or “sell,” the conﬁdence that this is indeed the
right decision is higher than the same advice from two like-minded analysts. This is
why κ “rewards”biasedannotators:itisnotamatterofreproducibility(reliability)but
ratheroftrustworthiness(validity).
Havingsaidthis,weshouldpointoutthat,ﬁrst,inpracticethedifferencebetween
π and κ doesn’t often amount to much (see discussion in Section 4). Moreover, the
differencebecomessmallerasagreementincreases,becauseallthepointsofagreement
contributetowardmakingthecodermarginalssimilar(ittookalotofexperimentation
tocreatedataforTable4sothatthevaluesof π and κ wouldstraddletheconventional
cutoffpointof0.80,andevensothedifferenceisverysmall).Finally,onewouldexpect
571
ComputationalLinguistics Volume34,Number4
thedifferencebetweenπandκtodiminishasthenumberofcodersgrows;thisisshown
subsequently.
6
We deﬁne B, the overall annotator bias in a particular set of coding data, as the
difference between the expected agreement according to (multi)-π and the expected
agreementaccordingto(multi)-κ.Annotatorbiasisameasureofvariance:Ifwetake cto
be a random variable with equal probabilities for all coders, then the annotator bias B
isthesumofthevariancesofP(k|c) forallcategories k ∈ K,dividedbythenumberof
codersclessone(seeArtsteinandPoesio[2005]foraproof).
B = A
π
e
−A
κ
e
=
1
c−1
∑
k∈K
σ
2
ˆ
P(k|c)
Annotatorbiascanbeusedtoexpressthedifferencebetweenκ and π.
κ −π =
A
o
−(A
π
e
−B)
1−(A
π
e
−B)
−
A
o
−A
π
e
1−A
π
e
= B·
(1−A
o
)
(1−A
κ
e
)(1−A
π
e
)
This allows us to make the following observations about the relationshipbetween
π andκ.
Observation 1. The difference between κ and π grows as the annotator bias grows: For a
constant A
o
and A
π
e, a greater B implies a greater value for κ −π.
Observation 2. The greater the number of coders, the lower the annotator bias B, and hence
the lower the difference between κ and π, because the variance of
ˆ
P(k|c) does not increase in
proportion to the number of coders.
In other words, provided enough coders are used, it should not matter whether a
single-distributionorindividual-distributioncoefﬁcientisused.Thisisnottoimplythat
multiplecodersincreasereliability:Thevarianceoftheindividualcoders’distributions
can be just as large with many coders as with few coders, but its effect on the value
of κ decreases as the number of coders grows, and becomes more similar to random
noise.
Thesameholdsforweightedmeasurestoo;seetheextendedversionofthisarticle
fordeﬁnitionsandproof.Inanannotationstudywith18subjects,wecompared α with
a variant which uses individual coder distributions to calculate expected agreement,
and found that the values never differed beyond the third decimal point (Poesio and
Artstein2005).
We conclude with a summary of our views concerning the difference between π-
style and κ-style coefﬁcients. First of all, keepin mind that empirically the difference
is small, and gets smaller as the number of annotators increases. Then instead of
reportingtwocoefﬁcients,assuggestedbyDiEugenioandGlass(2004),theappropriate
coefﬁcientshouldbechosenbasedonthetask(not ontheobserveddifferencesbetween
codermarginals).Whenthecoefﬁcientisusedtoassessreliability,asingle-distribution
coefﬁcientlike π or α shouldbeused;thisisindeedalreadythepracticeinCL,because
Siegel and Castellan’s K is identical with (multi-)π. It is also good practice to test
6 CraggsandMcGeeWood(2005)alsosuggestincreasingthenumberofcodersinordertoovercome
individualannotatorbias,butdonotprovideamathematicaljustiﬁcation.
572
ArtsteinandPoesio Inter-CoderAgreementforCL
reliabilitywithmorethantwocoders,inordertoreducethelikelihoodofcoderssharing
adeviantreadingoftheannotationguidelines.
3.2Prevalence
WetoucheduponthematterofskeweddatainSection2.3whenwemotivatedtheneed
forchancecorrection:Ifadisproportionateamountofthedatafallsunderonecategory,
then the expected agreement is very high, so in order to demonstrate high reliability
anevenhigherobservedagreementisneeded.Thisleadstotheso-calledparadoxthat
chance-correctedagreementmaybeloweventhoughA
o
ishigh(CicchettiandFeinstein
1990; Feinstein and Cicchetti 1990; Di Eugenio and Glass 2004). Moreover, when the
data are highly skewed in favor of one category, the high agreement also corresponds
to high accuracy: If, say, 95% of the data fall under one category label, then random
coding would cause two coders to jointly assign this category label to 90.25% of the
items,andonaverage95%oftheselabelswouldbecorrect,foranoverallaccuracyofat
least85.7%.Thisleadstothesurprisingresultthatwhendataarehighlyskewed,coders
may agree on a high proportion ofitems while producing annotations that are indeed
correcttoahighdegree,yetthereliabilitycoefﬁcientsremainlow.(Foranillustration,
seethediscussionofagreementresultsoncodingdiscoursesegmentsinSection4.3.1.)
This surprising result is, however, justiﬁed. Reliability implies the ability to dis-
tinguish between categories, but when one category is very common, high accuracy
and high agreement can also result from indiscriminate coding. The test for reliabil-
ity in such cases is the ability to agree on the rare categories (regardless of whether
these are the categories of interest). Indeed, chance-corrected coefﬁcients are sensitive
to agreement on rare categories. This is easiest to see with a simple example of two
codersandtwocategories,onecommonandtheotheronerare;tofurthersimplifythe
calculationwealsoassumethatthecodermarginalsareidentical,sothat π and κ yield
thesamevalues.Wecanthusrepresentthejudgmentsinacontingencytablewithjust
two parameters: epsilon1 is half the proportion of items on which there is disagreement, and
δ is the proportion of agreement on the Rare category. Both of these proportions are
assumedtobesmall,sothebulkoftheitems(aproportionof1−(δ +2epsilon1))arelabeled
with the Common category by both coders (Table 7). From this table we can calculate
A
o
= 1−2epsilon1 andA
e
= 1−2(δ + epsilon1)+2(δ + epsilon1)
2,aswellas π andκ.
π,κ =
1−2epsilon1−(1−2(δ + epsilon1)+2(δ + epsilon1)
2
)
1−(1−2(δ + epsilon1)+2(δ + epsilon1)
2
)
=
δ
δ + epsilon1
−
epsilon1
1−(δ + epsilon1)
Whenepsilon1andδarebothsmall,thefractionaftertheminussignissmallaswell,soπandκ
are approximately δ/(δ + epsilon1): the value we get if we take all the items marked by one
Table7
Asimpleexampleofagreementondialogueacttagging.
CODERA
COMMON RARE TOTAL
COMMON 1−(δ +2epsilon1) epsilon1 1−(δ + epsilon1)
CODERB
RARE epsilon1δ+ epsilon1
TOTAL 1−(δ + epsilon1) δ + epsilon1 1
573
ComputationalLinguistics Volume34,Number4
particularcoderasRare,andcalculatewhatproportionofthoseitemswerelabeledRare
bytheothercoder.Thisisameasureofthecoders’abilitytoagreeontherarecategory.
4.UsingAgreementMeasuresforCLAnnotationTasks
In this section we review the use of intercoder agreement measures in CL since
Carletta’s original paper in light of the discussion in the previous sections. We begin
with a summary of Krippendorff’s recommendations about measuring reliability
(Krippendorff 2004a, Chapter 11), then discuss how coefﬁcients of agreement have
beenusedinCLtomeasurethereliabilityofannotationschemes,focusinginparticular
on the types of annotation where there has been some debate concerning the most
appropriatemeasuresofagreement.
4.1MethodologyandInterpretationoftheResults:GeneralIssues
Krippendorff(2004a,Chapter11)noteswithregretthefactthatreliabilityisdiscussedin
onlyaround69%ofstudiesincontentanalysis.InCLaswell,notallannotationprojects
include a formal test of intercoder agreement. Some of the best known annotation
efforts,suchasthecreationofthePennTreebank(Marcus,Marcinkiewicz,andSantorini
1993)andtheBritishNationalCorpus(Leech,Garside,andBryant1994),donotreport
reliability results as they predate the Carletta paper; but even among the more recent
efforts, many only report percentage agreement, as for the creation of the PropBank
(Palmer, Dang, and Fellbaum 2007) or theongoing OntoNotes annotation (Hovy etal.
2006). Even more importantly, very few studies apply a methodology as rigorous as
that envisaged by Krippendorff and other content analysts. We therefore begin this
discussion of CL practice with a summary of the main recommendations found in
Chapter 11 of Krippendorff (2004a), even though, as we will see, we think that some
oftheserecommendationsmaynotbeappropriateforCL.
4.1.1 Generating
Data to Measure Reproducibility. Krippendorff’s recommendations were
developed for the ﬁeld of content analysis, where coding is used to draw conclusions
fromthetexts.Acodedcorpusisthusakintotheresultofascientiﬁcexperiment,and
it can only be considered valid if it is reproducible—that is, if the same coded results
canbereplicatedinanindependentcodingexercise.Krippendorffthereforearguesthat
any study using observed agreement as a measure of reproducibility must satisfy the
followingrequirements:
• Itmustemployanexhaustivelyformulated,clear,andusablecoding
schemetogetherwithstep-by-stepinstructionsonhowtouseit.
• Itmustuseclearlyspeciﬁedcriteriaconcerningthechoiceofcoders
(sothatothersmayusesuchcriteriatoreproducethedata).
• Itmustensurethatthecodersthatgeneratethedatausedtomeasure
reproducibilityworkindependentlyofeachother.
Some practices that are common in CL do not satisfy these requirements. The ﬁrst
requirement is violated by the practice of expanding the written coding instructions
and including new rules as the data are generated. The second requirement is often
574
ArtsteinandPoesio Inter-CoderAgreementforCL
violatedbyusingexpertsascoders,particularlylong-termcollaborators,assuchcoders
may agree not because they are carefully following written instructions, but because
theyknowthepurposeoftheresearchverywell—whichmakesitvirtuallyimpossible
for others to reproduce the results on the basis of the same coding scheme (the prob-
lems arising when using experts were already discussed at length in Carletta [1996]).
Practiceswhichviolatethethirdrequirement(independence)includeaskingcodersto
discusstheirjudgmentswitheachotherandreachtheirdecisionsbymajorityvote,or
toconsultwitheachotherwhenproblemsnotforeseeninthecodinginstructionsarise.
Anyofthesepracticesmaketheresultingdataunusableformeasuringreproducibility.
Krippendorff’s own summary of his recommendations is that to obtain usable
data for measuring reproducibility a researcher must use data generated by three or
morecoders,chosenaccordingtosomeclearlyspeciﬁedcriteria,andworkingindepen-
dentlyaccordingtoawrittencodingschemeandcodinginstructionsﬁxedinadvance.
Krippendorffalsodiscussesthecriteriatobeusedintheselectionofthesample,from
the minimum number of units (obtained using a formula from Bloch and Kraemer
[1989], reported in Krippendorff [2004a, page 239]), to how to make the sample rep-
resentative of the data population (each category should occur in the sample often
enoughtoyieldatleastﬁvechanceagreements),tohowtoensurethereliabilityofthe
instructions (the sample should contain examples of all the values for the categories).
These recommendations are particularly relevant in light of the comments of Craggs
and McGee Wood (2005, page 290), which discourage researchers from testing their
coding instructions on data from more than one domain. Given that the reliability of
thecodinginstructionsdependstoagreatextentonhowcomplicationsaredealtwith,
and that every domain displays different complications, the sample should contain
sufﬁcient examples from all domains which have to be annotated according to the
instructions.
4.1.2 Establishing
Signiﬁcance. In hypothesis testing, it is common to test for the sig-
niﬁcance of a result against a null hypothesis of chance behavior; for an agreement
coefﬁcientthiswouldmeanrejectingthepossibilitythatapositivevalueofagreement
is nevertheless due to random coding. We can rely on the statement by Siegel and
Castellan(1988,Section9.8.2)thatwhensamplesizesarelarge,thesamplingdistribu-
tion of K (Fleiss’s multi-π) is approximately normal and centered around zero—this
allowstestingtheobtainedvalueofKagainstthenullhypothesisofchanceagreement
byusingthe z statistic.ItisalsoeasytotestKrippendorff’s α withtheintervaldistance
metricagainstthenullhypothesisofchanceagreement,becausethehypothesisα = 0is
identicaltothehypothesis F = 1inananalysisofvariance.
However,anullhypothesisofchanceagreementisnotveryinteresting,anddemon-
strating that agreement is signiﬁcantly better than chance is not enough to establish
reliability.ThishasalreadybeenpointedoutbyCohen(1960,page44):“toknowmerely
that κ isbeyondchanceistrivialsinceoneusuallyexpectsmuchmorethanthisinthe
way of reliability in psychological measurement.” The same point has been repeated
and stressed in many subsequent works (e.g., Posner et al. 1990; Di Eugenio 2000;
Krippendorff2004a):Thereasonformeasuringreliabilityisnottotestwhethercoders
performbetterthanchance,buttoensurethatthecodersdonotdeviatetoomuchfrom
perfectagreement(Krippendorff2004a,page237).
The relevant notion of signiﬁcance for agreement coefﬁcients is therefore a conﬁ-
dence interval. Cohen (1960, pages 43–44) implies that when sample sizes are large,
the sampling distribution of κ is approximately normal for any true population value
of κ,and therefore conﬁdence intervals for the observed value of κ can be determined
575
ComputationalLinguistics Volume34,Number4
using the usual multiples of the standard error. Donner and Eliasziw (1987) propose
a more general form of signiﬁcance test for arbitrary levels of agreement. In contrast,
Krippendorff (2004a, Section 11.4.2) states that the distribution of α is unknown, so
conﬁdenceintervalsmustbeobtainedbybootstrapping;asoftwarepackagefordoing
thisisdescribedinHayesandKrippendorff(2007).
4.1.3 Interpreting
the Value of Kappa-Like Coefﬁcients. Even after testing signiﬁcance and
establishing conﬁdenceintervalsforagreementcoefﬁcients,wearestillfacedwiththe
problemofinterpretingthemeaningoftheresultingvalues.Suppose,forexample,we
establishthatforaparticulartask,K = 0.78±0.05.Isthisgoodorbad?Unfortunately,
deciding what counts as an adequate level of agreement for a speciﬁc purpose is still
little more than a black art: As we will see, different levels of agreement may be
appropriateforresourcebuildingandformorelinguisticpurposes.
Theproblemisnotunlikethatofinterpretingthevaluesofcorrelationcoefﬁcients,
andintheareaofmedicaldiagnosis,thebestknownconventionsconcerningthevalue
of kappa-like coefﬁcients, those proposed by Landis and Koch (1977) and reported in
Figure 1, are indeed similar to those used for correlation coefﬁcients, where values
above 0.4 are also generally considered adequate (Marion 2004). Many medical re-
searchersfeelthattheseconventionsareappropriate,andinlanguagestudies,asimilar
interpretation of the values has been proposed by Rietveld and van Hout (1993). In
CL, however, most researchers follow the more stringent conventions from content
analysis proposed by Krippendorff (1980, page 147), as reported by Carletta (1996,
page 252): “content analysis researchers generally think of K>.8 as good reliability,
with.67<K<.8allowingtentativeconclusionstobedrawn”(Krippendorffwasdis-
cussing values of α rather than K, but the coefﬁcients are nearly equivalent for cate-
gorical labels). As a result, ever since Carletta’s inﬂuential paper, CL researchers have
attemptedtoachieveavalueofK(moreseldom,ofα)abovethe0.8threshold,or,failing
that, the 0.67 level allowing for “tentative conclusions.” However, the description of
the0.67boundaryinKrippendorff(1980)wasactually“highlytentativeandcautious,”
and in later work Krippendorff clearly considers 0.8 the absolute minimum value of
α to accept for any serious purpose: “Even a cutoff point of α = .800 ... is a pretty
low standard” (Krippendorff 2004a, page 242). Recent content analysis practice seems
to have settled for even more stringent requirements: A recent textbook, Neuendorf
(2002, page 3), analyzing several proposals concerning “acceptable” reliability, con-
cludes that “reliability coefﬁcients of .90 or greater would be acceptable to all, .80
or greater would be acceptable in most situations, and below that, there exists great
disagreement.”
Thisisclearlyafundamentalissue.Ideallywewouldwanttoestablishthresholds
whichareappropriatefortheﬁeldofCL,butaswewillseeintherestofthissection,a
decade of practical experience hasn’t helped in settling the matter. In fact, weighted
coefﬁcients, while arguably more appropriate for many annotation tasks, make the
issue of deciding when the value of a coefﬁcient indicates sufﬁcient agreement even
K = 0.0 0.2 0.4 0.6 0.8 1.0
Poor Slight Fair Moderate Substantial Perfect
Figure1
KappavaluesandstrengthofagreementaccordingtoLandisandKoch(1977).
576
ArtsteinandPoesio Inter-CoderAgreementforCL
more complicated because of the problem of determining appropriate weights (see
Section 4.4). We will return to the issue of interpreting the value of the coefﬁcients at
theendofthisarticle.
4.1.4 Agreement
and Machine Learning. In a recent article, Reidsma and Carletta (2008)
pointoutthatthegoalsofannotationinCLdifferfromthoseofcontentanalysis,where
agreement coefﬁcients originate. A common use of an annotated corpus in CL is not
toconﬁrmorrejectahypothesis,buttogeneralizethepatternsusingmachine-learning
algorithms. Through a series of simulations, Reidsma and Carletta demonstrate that
agreement coefﬁcients are poor predictors of machine-learning success: Even highly
reproducibleannotationsaredifﬁculttogeneralizewhenthedisagreementscontainpat-
ternsthatcanbelearned,whereashighlynoisyandunreliabledatacanbegeneralized
successfully when the disagreements do not contain learnable patterns. These results
show that agreement coefﬁcients should not be used as indicators of the suitability of
annotateddataformachinelearning.
However, the purpose of reliability studies is not to ﬁnd out whether annotations
can be generalized, but whether they capture some kind of observable reality. Even if
thepatternofdisagreementallowsgeneralization,weneedevidencethatthisgeneral-
ization would be meaningful. The decision whether a set of annotation guidelines are
appropriateormeaningfulisultimatelyaqualitativeone,butabaselinerequirementis
an acceptable level of agreement among the annotators, who serve as the instruments
of measurement. Reliability studies test the soundness of an annotation scheme and
guidelines,whichisnottobeequatedwiththemachine-learnabilityofdataproduced
bysuchguidelines.
4.2LabelingUnitswithaCommonandPredeﬁnedSetofCategories:TheCase
ofDialogueActTagging
The simplest and most common coding in CL involves labeling segments of text with
a limited number of linguistic categories: Examples include part-of-speech tagging,
dialogue act tagging, and named entity tagging. The practices used to test reliability
forthistypeofannotationtendtobebasedontheassumptionthatthecategoriesused
in the annotation are mutually exclusive and equally distinct from one another; this
assumptionseemstohaveworkedoutwellinpractice,butquestionsaboutithavebeen
raisedevenfortheannotationofpartsofspeech(Babarczy,Carroll,andSampson2006),
letalonefordiscoursecodingtaskssuchasdialogueactcoding.Weconcentratehereon
this latter type of coding, but a discussion of issues raised for POS, named entity, and
prosodiccodingcanbefoundintheextendedversionofthearticle.
Dialogue act tagging is a type of linguistic annotation with which by now the CL
community has had extensive experience: Several dialogue-act-annotated spoken lan-
guage corpora now exist, such as MapTask (Carletta et al. 1997), Switchboard (Stolcke
et al. 2000), Verbmobil (Jekat et al. 1995), and Communicator (e.g., Doran et al. 2001),
amongothers.Historically,dialogueactannotationwasalsooneofthetypesofannota-
tionthatmotivatedtheintroductioninCLofchance-correctedcoefﬁcientsofagreement
(Carletta et al. 1997) and, as we will see, it has been the type of annotation that has
generated the most discussion concerning annotation methodology and measuring
agreement.
A number of coding schemes for dialogue acts have achieved values of K over
0.8 and have therefore been assumed to be reliable: For example, K = 0.83 for the
577
ComputationalLinguistics Volume34,Number4
13-tagMapTaskcodingscheme(Carlettaetal.1997),K = 0.8forthe42-tagSwitchboard-
DAMSLscheme(Stolckeetal.2000),K = 0.90forthesmaller20-tagsubsetoftheCSTAR
scheme used by Doran et al. (2001). All of these tests were based on the same two
assumptions: that every unit (utterance) is assigned to exactly one category (dialogue
act),andthatthesecategoriesaredistinct.Therefore,again,unweightedmeasures,and
inparticularK,tendtobeusedformeasuringinter-coderagreement.
However, these assumptions have been challenged based on the observation that
utterances tend to have more than one function at the dialogue act level (Traum and
Hinkelman1992;AllenandCore1997;Bunt2000);forausefulsurvey,seePopescu-Belis
(2005).Anassertionperformedinanswertoaquestion,forinstance,typicallyperforms
atleasttwofunctionsatdifferentlevels:assertingsomeinformation—thedialogueact
thatwecalledStatementinSection2.3,operatingatwhatTraumandHinkelmancalled
the“corespeechact”level—andconﬁrmingthatthequestionhasbeenunderstood,adi-
alogueactoperatingatthe“grounding”levelandusuallyknownasAcknowledgment
(Ack). In older dialogue act tagsets, acknowledgments and statements were treated as
alternativelabelsatthesame“level”,forcingcoderstochooseoneortheotherwhenan
utteranceperformedadualfunction,accordingtoawell-speciﬁedsetofinstructions.By
contrast,intheannotationschemesinspiredfromthesenewertheoriessuchasDAMSL
(AllenandCore1997),codersareallowedtoassigntagsalongdistinct“dimensions”or
“levels”.
Two annotation experiments testing this solution to the “multi-tag” problem with
the DAMSL scheme were reported in Core and Allen (1997) and Di Eugenio et al.(1998). In both studies, coders were allowed to mark each communicative function
independently: That is, they were allowed to choose for each utterance one of the
Statementtags(orpossiblynone),oneoftheInfluencing-Addressee-Future-Action
tags,andsoforth—andagreementwasevaluatedseparatelyforeachdimensionusing
(unweighted) K. Core and Allen found values of K ranging from 0.76 for answer
to 0.42 for agreement to 0.15 for Committing-Speaker-Future-Action. Using differ-
ent coding instructions and on a different corpus, Di Eugenio et al. observed higher
agreement, ranging from K = 0.93 (for other-forward-function) to 0.54 (for the tag
agreement).
These relatively low levels of agreement led many researchers to return to “ﬂat”
tagsets for dialogue acts, incorporating however in their schemes some of the in-
sights motivating the work on schemes such as DAMSL. The best known example
of this type of approach is the development of the SWITCHBOARD-DAMSL tagset
by Jurafsky, Shriberg, and Biasca (1997), which incorporates many ideas from the
“multi-dimensional”theoriesofdialogueacts,butdoesnotallowmarkinganutterance
as both an acknowledgment and a statement; a choice has to be made. This tagset
results in overall agreement of K = 0.80. Interestingly, subsequent developments of
SWITCHBOARD-DAMSL backtracked on some of these decisions. For instance, the
ICSI-MRDA tagset developed for the annotation of the ICSI Meeting Recorder corpus
reintroducessomeoftheDAMSLideas,inthatannotatorsareallowedtoassignmulti-
ple SWITCHBOARD-DAMSL labels to utterances (Shriberg et al. 2004). Shriberg et al.achievedacomparablereliabilitytothatobtainedwithSWITCHBOARD-DAMSL,but
onlywhenusingatagsetofjustﬁve“class-maps”.
Shribergetal.(2004)alsointroducedahierarchicalorganizationoftagstoimprove
reliability. The dimensions of the DAMSL scheme can be viewed as “superclasses” of
dialogue acts which share some aspect of their meaning. For instance, the dimension
of Influencing-Addressee-Future-Action (IAFA) includes the two dialogue acts
Open-option (used to mark suggestions) and Directive, both of which bring into
578
ArtsteinandPoesio Inter-CoderAgreementforCL
consideration a future action to be performed by the addressee. At least in principle,
an organization of this type opens up the possibility for coders to mark an utterance
withthesuperclass(IAFA)incasetheydonotfeelconﬁdentthattheutterancesatisﬁes
the additional requirements for Open-option or Directive. This, in turn, would do
away with the need to make a choice between these two options. This possibility
wasn’t pursued in the studies using the original DAMSL that we are aware of (Core
and Allen 1997; Di Eugenio 2000; Stent 2001), but was tested by Shriberg et al. (2004)
and subsequent work, in particular Geertzen and Bunt (2006), who were speciﬁcally
interestedintheideaofusinghierarchicalschemestomeasurepartialagreement,and
inadditionexperimentedwithweightedcoefﬁcientsofagreementfortheirhierarchical
taggingscheme,speciﬁcallyκ
w
.
Geertzen and Bunt tested intercoder agreement with Bunt’s DIT++ (Bunt 2005),
a scheme with 11 dimensions that builds on ideas from DAMSL and from Dynamic
Interpretation Theory (Bunt 2000). In DIT++, tags can be hierarchically related: For
example, the class information-seeking is viewed as consisting of two classes, yes-
noquestion(ynq)andwh-question(whq).Thehierarchyisexplicitlyintroducedinorder
to allow coders to leave some aspects of the coding undecided. For example, check is
treatedasasubclassofynqinwhich,inaddition,thespeakerhasaweakbeliefthatthe
propositionthatformsthebeliefistrue.Acoderwhoisnotcertainaboutthedialogue
actperformedusinganutterancemaysimplychoosetotagitasynq.
The distance metric d proposed by Geertzen and Bunt is based on the crite-
rion that two communicative functions are related (d(c
1,c
2
) <1) if they stand in an
ancestor–offspringrelationwithinahierarchy.Furthermore,theyargue,themagnitude
ofd(c
1,c
2
) shouldbeproportionaltothedistancebetweenthefunctionsinthehierar-
chy.Alevel-dependentcorrectionfactorisalsoproposedsoastoleaveopentheoption
tomakedisagreementsathigherlevelsofthehierarchymattermorethandisagreements
at the deeper level (for example, the distance between information-seeking and ynq
mightbeconsideredgreaterthanthedistancebetweencheckandpositive-check).
The results of an agreement test with two annotators run by Geertzen and Bunt
show that taking into account partial agreement leads to values of κ
w
that are higher
thanthevaluesofκ forthesamecategories,particularlyforfeedback,aclassforwhich
CoreandAllen(1997)gotlowagreement.Ofcourse,evenassumingthatthevaluesofκ
w
andκ weredirectlycomparable—weremarkonthedifﬁcultyofinterpretingthevalues
ofweightedcoefﬁcientsofagreementinSection4.4—itremainstobeseenwhetherthese
highervaluesareabetterindicationoftheextentofagreementbetweencodersthanthe
valuesofunweightedκ.
This discussion of coding schemes for dialogue acts introduced issues to which
we will return for other CL annotation tasks as well. There are a number of well-
established schemes for large-scale dialogue act annotation based on the assumption
ofmutualexclusivitybetweendialogueacttags,whosereliabilityisalsowellknown;if
oneoftheseschemesisappropriateformodelingthecommunicativeintentionsfound
in a task, we recommend to our readers to use it. They should also realize, however,
thatthemutualexclusivityassumptionissomewhatdubious.Ifamulti-dimensionalor
hierarchical tagset is used, readers should also be aware that weighted coefﬁcients do
capture partial agreement, and need not automatically result in lower reliability or in
an explosion in the number of labels. However, a hierarchical scheme may not reﬂect
genuineannotationdifﬁculties:Forexample,inthecaseofDIT++,onemightarguethat
itismoredifﬁculttoconfuseyes-noquestionswith wh-questionsthanwithstatements.
Wewillalsoseeinamomentthatinterpretingtheresultswithweightedcoefﬁcientsis
difﬁcult.Wewillreturntobothoftheseproblemsinwhatfollows.
579
ComputationalLinguistics Volume34,Number4
4.3MarkingBoundariesandUnitizing
Before labeling can take place, the units of annotation, or markables, need to be
identiﬁed—aprocessKrippendorff(1995,2004a)callsunitizing.ThepracticeinCLfor
theformsofannotationdiscussedintheprevioussectionistoassumethattheunitsare
linguisticconstituentswhichcanbeeasilyidentiﬁed,suchaswords,utterances,ornoun
phrases, and therefore there is no need to check the reliability of this process. We are
awareoffewexceptionstothisassumption,suchasCarlettaetal.(1997)onunitization
for move coding and our own work on the GNOME corpus (Poesio 2004b). In cases
such as text segmentation, however, the identiﬁcation of units is as important as their
labeling,ifnotmoreimportant,andthereforecheckingagreementonunitidentiﬁcation
isessential.InthissectionwediscusscurrentCLpracticewithreliabilitytestingofthese
types of annotation, before brieﬂy summarizing Krippendorff’s proposals concerning
measuringreliabilityforunitizing.
4.3.1 Segmentation
and Topic Marking. Discourse segments are portions of text that con-
stitute a unit either because they are about the same “topic” (Hearst 1997; Reynar
1998)orbecausetheyhavetodowithachievingthesameintention(GroszandSidner
1986) or performing the same “dialogue game” (Carletta et al. 1997).
7
The analysis
ofdiscoursestructure—and especiallytheidentiﬁcationofdiscoursesegments—isthe
type of annotation that, more than any other, led CL researchers to look for ways of
measuring reliability and agreement, as it made them aware of the extent of disagree-
ment on even quite simple judgments (Kowtko, Isard, and Doherty 1992; Passonneau
and Litman 1993; Carletta et al. 1997; Hearst 1997). Subsequent research identiﬁed a
number of issues with discourse structure annotation, above all the fact that segmen-
tation,thoughproblematic,isstillmucheasierthanmarkingmorecomplexaspectsof
discoursestructure,suchasidentifyingthemostimportantsegmentsorthe“rhetorical”
relationsbetweensegmentsofdifferentgranularity.Asaresult,manyeffortstoannotate
discoursestructureconcentrateonlyonsegmentation.
The agreement results for segment coding tend to be on the lower end of the
scale proposed by Krippendorff and recommended by Carletta. Hearst (1997), for
instance, found K = 0.647 for the boundary/not boundary distinction; Reynar (1998),
measuring agreement between his own annotation and the TREC segmentation of
broadcast news, reports K = 0.764 for the same task; Ries (2002) reports even lower
agreement of K = 0.36. Teufel, Carletta, and Moens (1999), who studied agreement on
the identiﬁcation of argumentative zones, found high reliability (K = 0.81) for their
three main zones (own, other, background), although lower for the whole scheme
(K = 0.71). For intention-based segmentation, Passonneau and Litman (1993) in the
pre-Kdaysreportedanoverallpercentageagreementwithmajorityopinionof89%,but
the agreement on boundaries was only 70%. For conversational games segmentation,
Carletta et al. (1997) reported “promising but not entirely reassuring agreement on
where games began (70%),” whereas the agreement on transaction boundaries was
K = 0.59. Exceptions are two segmentation efforts carried out as part of annotations
of rhetorical structure. Moser, Moore, and Glendening (1996) achieved an agreement
7 Thenotionof“topic”isnotoriouslydifﬁculttodeﬁneandmanycompetingtheoreticalproposalsexist
(Reinhart1981;Vallduv´ı1993).Asitisoftenthecasewithannotation,fairlysimpledeﬁnitionstendto
beusedindiscourseannotationwork:Forexample,inTDTtopicisdeﬁnedforannotationpurposes
as“aneventoractivity,alongwithalldirectlyrelatedeventsandactivities”(TDT-2AnnotationGuide,
http://projects.ldc.upenn.edu/TDT2/Guide/label-instr.html).
580
ArtsteinandPoesio Inter-CoderAgreementforCL
of K = 0.9 for the highest level of segmentation of their RDA annotation (Poesio,
Patel, and Di Eugenio 2006). Carlson, Marcu, and Okurowski (2003) reported very
highagreement overtheidentiﬁcation oftheboundaries ofdiscourse units,thebuild-
ing blocks of their annotation of rhetorical structure. (Agreement was measured sev-
eral times; initially, they obtained K = 0.87, and in the ﬁnal analysis K = 0.97.) This,
however, was achieved by employing experienced annotators, and with considerable
training.
One important reason why most agreement results on segmentation are on the
lowerendofthereliabilityscaleisthefact,knowntoresearchersindiscourseanalysis
fromasearlyasLevinandMoore(1978),thatalthoughanalystsgenerallyagreeonthe
“bulk”ofsegments,theytendtodisagreeontheirexactboundaries.Thisphenomenon
wasalsoobservedinmorerecentstudies:SeeforexamplethediscussioninPassonneau
and Litman (1997), the comparison of the annotations produced by seven coders of
the same text in Figure 5 of Hearst (1997, page 55), or the discussion by Carlson,
Marcu,andOkurowski(2003),whopointoutthattheboundariesbetweenelementary
discourseunitstendtobe“veryblurry.”SeealsoPevznerandHearst(2002)forsimilar
commentsmadeinthecontextoftopicsegmentationalgorithms,andKlavans,Popper,
andPassonneau(2003)forselectingdeﬁnitionphrases.
This “blurriness” of boundaries, combined with the prevalence effects discussed
in Section 3.2, also explains the fact that topic annotation efforts which were only
concerned with roughly dividing a text into segments (Passonneau and Litman 1993;
Carlettaetal.1997;Hearst1997;Reynar1998;Ries2002)generallyreportloweragree-
mentthanthestudieswhosegoalistoidentifysmallerdiscourseunits.Whendisagree-
mentismostlyconcentratedinoneclass(‘boundary’inthiscase),ifthetotalnumberof
unitstoannotateremainsthesame,thenexpectedagreementonthisclassislowerwhen
agreaterproportionoftheunitstoannotatebelongstothisclass.Wheninadditionthis
classismuchlessnumerousthantheotherclasses,overallagreementtendstodepend
mostlyonagreementonthisclass.
For instance, suppose we are testing the reliability of two different segmentation
schemes—intobroad“discoursesegments”andintoﬁner“discourseunits”—onatext
of50utterances,andthatweobtaintheresultsinTable8.Case1wouldbeasituation
inwhichCoderAandCoderBagreethatthetextconsistsoftwosegments,obviously
agreeonitsinitialandﬁnalboundaries,butdisagreebyonepositionontheintermediate
boundary—say,oneofthemplacesitatutterance25,theotheratutterance26.Never-
theless, because expected agreement is so high—the coders agree on the classiﬁcation
of98%oftheutterances—thevalueofKisfairlylow.Incase2,thecodersdisagreeon
threetimesasmanyutterances,butKishigherthanintheﬁrstcasebecauseexpected
agreementissubstantiallylower(A
e
= 0.53).
The fact that coders mostly agree on the “bulk” of discourse segments, but tend
to disagree on their boundaries, also makes it likely that an all-or-nothing coefﬁcient
like K calculated on individual boundaries would underestimate the degree of agree-
ment, suggesting low agreement even among coders whose segmentations are mostly
similar. A weighted coefﬁcient of agreement like α might produce values more in
keepingwithintuition,butwearenotawareofanyattemptsatmeasuringagreement
onsegmentationusingweightedcoefﬁcients.Weseetwomainoptions.Wesuspectthat
the methods proposed by Krippendorff (1995) for measuring agreement on unitizing
(see Section 4.3.2, subsequently) may be appropriate for the purpose of measuring
agreementondiscoursesegmentation.Asecondoptionwouldbetomeasureagreement
not on individual boundaries but on windows spanning several units, as done in the
methods proposed to evaluate the performance of topic detection algorithms such as
581
ComputationalLinguistics Volume34,Number4
Table8
Fewerboundaries,higherexpectedagreement.
Case1:Broadsegments
A
o
= 0.96,A
e
= 0.89,K = 0.65
CODERA
BOUNDARY NO BOUNDARY TOTAL
BOUNDARY 213
CODERBNO BOUNDARY 14647
TOTAL 34750
Case2:Finediscourseunits
A
o
= 0.88,A
e
= 0.53,K = 0.75
CODERA
BOUNDARY NO BOUNDARY TOTAL
BOUNDARY 16 3 19
CODERBNO BOUNDARY 3283
TOTAL 19 31 50
P
k
(Beeferman,Berger,andLafferty1999)or WINDOWDIFF(PevznerandHearst2002)
(whichare,however,rawagreementscoresnotcorrectedforchance).
4.3.2 Unitizing
(Or, Agreement on Markable Identiﬁcation). ItisoftenassumedinCLanno-
tation practice that the units of analysis are “natural” linguistic objects, and therefore
there is no need to check agreement on their identiﬁcation. As a result, agreement is
usuallymeasuredonthelabelingofunitsratherthanontheprocessofidentifyingthem
(unitizing,Krippendorff1995).Wehavejustseen,however,twocodingtasksforwhich
the reliability of unit identiﬁcation is a crucial part of the overall reliability, and the
problemofmarkableidentiﬁcationismorepervasivethanisgenerallyacknowledged.
For example, when the units to be labeled are syntactic constituents, it is common
practicetouseaparserorchunkertoidentifythemarkablesandthentoallowthecoders
tocorrecttheparser’soutput.Insuchcasesonewouldwanttoknowhowreliablethe
coders’correctionsare.Wethusneedageneralmethodoftestingrelibilityonmarkable
identiﬁcation.
Theoneproposalformeasuringagreementonmarkableidentiﬁcationweareaware
of is the α
U
coefﬁcient, a non-trivial variant of α proposed by Krippendorff (1995). A
fullpresentationoftheproposalwouldrequiretoomuchspace,sowewilljustpresent
thecoreidea.Unitizingisconceivedofasconsistingoftwoseparatesteps:identifying
boundariesbetweenunits,andselectingtheunitsofinterest.Ifaunitidentiﬁedbyone
coder overlaps a unit identiﬁed by the other coder, the amount of disagreement is the
squareofthelengthsofthenon-overlappingsegments(seeFigure2);ifaunitidentiﬁed
by one coder does not overlapany unit of interest identiﬁed by the other coder, the
amount of disagreement is the square of the length of the whole unit. This distance
metric is used in calculating observed and expected disagreement, and α
U
itself. We
referthereadertoKrippendorff(1995)fordetails.
Krippendorff’s α
U
is not applicable to all CL tasks. For example, it assumes that
units may not overlap in a single coder’s output, yet in practice there are many
582
ArtsteinandPoesio Inter-CoderAgreementforCL
CoderA
CoderB
s
−
✛ ✲
s
✛ ✲
s
+
✛ ✲
Figure2
Thedifferencebetweenoverlappingunitsisd(A,B)=s
2
−
+ s
2
+
(adaptedfromKrippendorff
1995,Figure4,page61).
annotation schemes which require coders to label nested syntactic constituents. For
continuous segmentation tasks, α
U
may be inappropriate because when a segment
identiﬁedbyoneannotatoroverlapswithtwosegmentsidentiﬁedbyanotherannotator,
the distance is smallest when the one segment is centered over the two rather than
alignedwithoneofthem.Nevertheless,wefeelthatwhenthenon-overlapassumption
holds, and the units do not cover the text exhaustively, testing the reliabilty of unit
identiﬁcationmayprovebeneﬁcial.Toourknowledge,thishasneverbeentestedinCL.
4.4Anaphora
Theannotationtasksdiscussedsofarinvolveassigningaspeciﬁclabeltoeachcategory,
whichallowsthevariousagreementmeasurestobeappliedinastraightforwardway.
Anaphoricannotationdiffersfromtheprevioustasksbecauseannotatorsdonotassign
labels, but rather create links between anaphors and their antecedents. It is therefore
not clear what the “labels” should be for the purpose of calculating agreement. One
possibility would be to consider the intended referent (real-world object) as the label,
as in named entity tagging, but it wouldn’t make sense to predeﬁne a set of “labels”
applicable to all texts, because different objects are mentioned in different texts. An
alternative is to use the marked antecedents as “labels”. However, we do not want to
count as a disagreement every time two coders agree on the discourse entity realized
by a particular noun phrase but just happen to mark different words as antecedents.
Consider the reference of the underlined pronoun it in the following dialogue excerpt
(TRAINS1991[Gross,Allen,andTraum1993],dialogued91-3.2).
8
1.1 M: ....
1.4 first thing I’d like you to do
1.5 is send engine E2 off with a boxcar to Corning to
pick up oranges
1.6 as soon as possible
2.1 S: okay
3.1 M: and while it’s there it should pick up the tanker
Some of the coders in a study we carried out (Poesio and Artstein 2005) indicated the
noun phrase engine E2 as antecedent for the second it in utterance 3.1, whereas others
indicated the immediately preceding pronoun, which they had previously marked as
having engine E2 asantecedent.Clearly,wedonotwanttoconsiderthesecoderstobein
disagreement.AsolutiontothisdilemmahasbeenproposedbyPassonneau(2004):Use
the emerging coreference sets as the ‘labels’ for the purpose of calculating agreement.
This requires using weighted measures for calculating agreement on such sets, and
8 ftp://ftp.cs.rochester.edu/pub/papers/ai/92.tn1.trains 91 dialogues.txt.
583
ComputationalLinguistics Volume34,Number4
consequentlyitraisesseriousquestionsaboutweightedmeasures—inparticular,about
theinterpretabilityoftheresults,aswewillseeshortly.
4.4.1 Passonneau’s Proposal. Passonneau (2004) recommends measuring agreement on
anaphoric annotation by using sets of mentions of discourse entities as labels, that is,
the emerging anaphoric/coreference chains. This proposal is in line with the meth-
ods developed to evaluate anaphora resolution systems (Vilain et al. 1995). But using
anaphoric chains as labels would not make unweighted measures such as K a good
measure for agreement. Practical experience suggests that, except when a text is very
short, few annotators will catch all mentions of a discourse entity: Most will forget to
mark a few, with the result that the chains (that is, category labels) differ from coder
to coder and agreement as measured with K is always very low. What is needed is
a coefﬁcient that also allows for partial disagreement between judgments, when two
annotatorsagreeonpartofthecoreferencechainbutnotonallofit.
Passonneau(2004)suggestssolvingtheproblembyusing α withadistancemetric
thatallowsforpartialagreementamonganaphoricchains.Passonneauproposesadis-
tancemetricbasedonthefollowingrationale:Twosetsareminimallydistantwhenthey
areidenticalandmaximallydistantwhentheyaredisjoint;betweentheseextremes,sets
that stand in a subset relation are closer (less distant) than ones that merely intersect.
Thisleadstothefollowingdistancemetricbetweentwosets A and B.
d
P
=







0ifA = B
1
/
3
if A ⊂ B or B ⊂ A
2
/
3
if A ∩ B negationslash= ∅,butA negationslash⊂ B and B negationslash⊂ A
1ifA ∩ B = ∅
Alternative distance metrics take the size of the anaphoric chain into account, based
on measures used to compare sets in Information Retrieval, such as the coefﬁcient of
community of Jaccard (1912) and the coincidence index of Dice (1945) (Manning and
Sch¨utze1999).
Jaccard:d
J
= 1−
|A ∩ B|
|A ∪ B|
Dice:d
D
= 1−
2|A ∩ B|
|A|+|B|
Inlaterwork,Passonneau(2006)offersareﬁneddistancemetricwhichshecalledMASI
(Measuring Agreement on Set-valued Items), obtained by multiplying Passonneau’s
originalmetricd
P
bythemetricderivedfromJaccardd
J
.
d
M
= d
P
×d
J
4.4.2 Experience
with α for Anaphoric Annotation. Intheexperimentmentionedpreviously
(PoesioandArtstein2005)weused18coderstotest α andKunderavarietyofcondi-
tions.Wefoundthateventhoughourcodersbyandlargeagreedontheinterpretationof
anaphoricexpressions,virtuallynocodereveridentiﬁedallthementionsofadiscourse
entity. As a result, even though the values of α and K obtained by using the ID of
theantecedentaslabelwereprettysimilar,thevaluesobtainedwhenusinganaphoric
chains as labels were drastically different. The value of α increased, because examples
where coders linked a markable to different antecedents in the same chain were no
584
ArtsteinandPoesio Inter-CoderAgreementforCL
longerconsideredasdisagreements.However,thevalueofKwasdrasticallyreduced,
becausehardlyanycoderidentiﬁedallthementionsofdiscourseentities(Figure3).
Thestudyalsolookedatthematterofindividualannotatorbias,andasmentioned
inSection3.1,wedidnotﬁnddifferencesbetween α anda κ-styleversionof α beyond
thethirddecimalpoint.Thissimilarityiswhatonewouldexpect,giventheresultabout
annotatorbiasfromSection3.1andgiventhatinthisexperimentweused18annotators.
These very small differences should be contrasted with the differences resulting from
the choice of distance metrics, where values for the full-chain condition ranged from
α = 0.642 using Jaccard as distance metric, to α = 0.654 using Passonneau’s metric, to
thevalueforDicereportedinFigure3, α = 0.691.Thesedifferencesraiseanimportant
issueconcerningtheapplicationof α-likemeasuresforCLtasks:Using α makesitdifﬁ-
culttocomparetheresultsofdifferentannotationexperiments,inthata“poor”valueor
a“high”valuemightresultfrom“toostrict”or“toogenerous”distancemetrics,making
it even more important to develop a methodology to identify appropriate values for
thesecoefﬁcients.Thisissueisfurtheremphasizedbythestudyreportednext.
4.4.3 Discourse
Deixis. A second annotation study we carried out (Artstein and Poesio
2006) shows even more clearly the possible side effects of using weighted coefﬁcients.
This study was concerned with the annotation of the antecedents of references to
abstractobjects,suchastheexampleofthepronoun that inutterance7.6(TRAINS1991,
dialogued91-2.2).
7.3 : so we ship one
7.4 : boxcar
7.5 : of oranges to Elmira
7.6 : and that takes another 2 hours
Previous studies of discourse deixis annotation showed that these are extremely difﬁ-
cult judgments to make (Eckert and Strube 2000; Navarretta 2000; Byron 2002), except
perhapsforidentifyingthetypeofobject(PoesioandModjeska2005),sowesimpliﬁed
the task by only requiring our participants to identify the boundaries of the area of
text in which the antecedent was introduced. Even so, we found a great variety in
how these boundaries were marked: Exactly as in the case of discourse segmentation
discussedearlier,ourparticipantsbroadlyagreedontheareaoftext,butdisagreedon
Chain K α
None 0.628 0.656
Partial 0.563 0.677
Full 0.480 0.691
0.4
0.5
0.6
0.7
α
α
α
K
K
K
no partial full
chain chain chain
Figure3
Acomparisonofthevaluesof α andKforanaphoricannotation(PoesioandArtstein2005).
585
ComputationalLinguistics Volume34,Number4
itsexactboundary.Forinstance,inthisexample,nineoutoftenannotatorsmarkedthe
antecedentof that asatextsegmentendingwiththeword Elmira,butsomestartedwith
theword so,somestartedwith we,somewithship,andsomewith one.
Wetestedanumberofwaystomeasurepartialagreementonthistask,andobtained
widelydifferentresults.Firstofall,wetestedthreeset-baseddistancemetricsinspired
bythePassonneauproposalsthatwejustdiscussed:Weconsidereddiscoursesegments
to be sets of words, and computed the distance between them using Passonneau’s
metric,Jaccard,andDice.Usingthesethreemetrics,weobtained α valuesof0.55(with
Passonneau’s metric), 0.45 (with Jaccard), and 0.55 (with Dice). We should note that
becauseantecedentsofdifferentexpressionsrarelyoverlapped,theexpecteddisagree-
mentwascloseto1(maximal),sothevalueof α turnedouttobeveryclosetothecom-
plementoftheobserveddisagreementascalculatedbythedifferentdistancemetrics.
Next, we considered methods based on the position of words in the text. The ﬁrst
method computed differences between absolute boundary positions: Each antecedent
wasassociatedwiththepositionofitsﬁrstorlastwordinthedialogue,andagreement
was calculated using α with the interval distance metric. This gave us α values of
0.998forthebeginnings oftheantecedent-evoking areaand0.999fortheends.Thisis
becauseexpecteddisagreementisexceptionallylow:Coderstendtomarkdiscoursean-
tecedentsclosetothereferringexpression,sotheaveragedistancebetweenantecedents
of the same expression is smaller than the size of the dialogue by a few orders of
magnitude. The second method associated each antecedent with the position of its
ﬁrst or last word relative to the beginning of the anaphoric expression.Thistimewefound
extremely low values of α = 0.167 for beginnings of antecedents and 0.122 for ends—
barelyinthepositiveside.Thisshowsthatagreementamongcodersisnotdramatically
betterthanwhatwouldbeexpectediftheyjustmarkeddiscourseantecedentsataﬁxed
distancefromthereferringexpression.
Thethreerangesofαthatweobserved(middle,high,andlow)showagreementon
theidentityofdiscourseantecedents,theirpositioninthedialogue,andtheirposition
relativetoreferringexpressions,respectively.Themiddlerangeshowsvariabilityofup
to 10 percentage points, depending on the distance metric chosen. The lesson is that
once we start using weighted measures we cannot anymore interpret the value of α
usingtraditionalrulesofthumbsuchasthoseproposedbyKrippendorfforbyLandis
andKoch.Thisisbecausedependingonthewaywemeasureagreement,wecanreport
α valuesrangingfrom0.122to0.998fortheverysameexperiment!Newinterpretation
methods have to be developed, which will be taskand distance-metric speciﬁc. We’ll
returntothisissueintheconclusions.
4.5WordSenses
Wordsensetaggingisoneofthehardestannotationtasks.Whereasinthecaseofpart-
of-speechanddialogueacttaggingthesamecategoriesareusedtoclassifyallunits,in
thecaseofwordsensetaggingdifferentcategoriesmustbeusedforeachword,which
makeswritingasinglecodingmanualspecifyingexamplesforallcategoriesimpossible:
The only option is to rely on a dictionary. Unfortunately, different dictionaries make
different distinctions, and often coders can’t make the ﬁne-grained distinctions that
trained lexicographers can make. The problem is particularly serious for verbs, which
tendtobepolysemousratherthanhomonymous(Palmer,Dang,andFellbaum2007).
These difﬁculties, and in particular the difﬁculty of tagging senses with a ﬁne-
grained repertoire of senses such as that provided by dictionaries or by WordNet
(Fellbaum 1998), have been highlighted by the three SENSEVAL initiatives. Already
586
ArtsteinandPoesio Inter-CoderAgreementforCL
during the ﬁrst SENSEVAL, V´eronis (1998) carried out two studies of intercoder
agreementonwordsensetaggingintheso-calledROMANSEVALtask.Onestudywas
concerned with agreement on polysemy—that is, the extent to which coders agreed
that a word was polysemous in a given context. Six naive coders were asked to make
thisjudgmentabout600Frenchwords(200nouns,200verbs,200adjectives)usingthe
repertoireofsensesinthe Petit Larousse.Onthistask,a(pairwise)percentageagreement
of 0.68 for nouns, 0.74 for verbs, and 0.78 for adjectives was observed, corresponding
to K values of 0.36, 0.37, and 0.67, respectively. The 20 words from each category
perceivedbythecodersinthisﬁrstexperimenttobemostpolysemouswerethenused
in a second study, of intercoder agreement on the sense tagging task, which involved
six different naive coders. Interestingly, the coders in this second experiment were
allowedtoassignmultipletagstowords,althoughtheydidnotmakemuchuseofthis
possibility;soκ
w
wasusedtomeasureagreement.Inthisexperiment,V´eronisobserved
(weighted)pairwiseagreementof0.63forverbs,0.71foradjectives,and0.73fornouns,
corresponding to κ
w
values of 0.41, 0.41, and 0.46, but with a wide variety of values
when measured per word—ranging from 0.007 for the adjective correct to 0.92 for the
noun d´etention. Similarly mediocre results for intercoder agreement between naive
coders were reported in the subsequent editions of SENSEVAL. Agreement studies
for SENSEVAL-2, where WordNet senses were used as tags, reported a percentage
agreement for verb senses of around 70%, whereas for SENSEVAL-3 (English Lexical
SampleTask),Mihalcea,Chklovski,andKilgarriff(2004)reportapercentageagreement
of67.3%andaverageKof0.58.
Two types of solutions have been proposed for the problem of low agreement on
sensetagging.ThesolutionproposedbyKilgarriff(1999)istouseprofessionallexicog-
raphers and arbitration. The study carried out by Kilgarriff does not therefore qualify
asatruestudyofreplicabilityinthesenseofthetermsusedbyKrippendorff,butitdid
show that this approach makes it possible to achieve percentage agreement of around
95.5%.Analternativeapproachhasbeentoaddresstheproblemoftheinabilityofnaive
coders to make ﬁne-grained distinctions by introducing coarser-grained classiﬁcation
schemes which grouptogether dictionary senses (Bruce and Wiebe, 1998; Buitelaar
1998; V´eronis 1998; Palmer, Dang, and Fellbaum 2007). Hierarchical tagsets were also
developed,suchasHECTOR(Atkins1992)or,indeed,WordNetitself(wheresensesare
relatedbyhyponymylinks).InthecaseofBuitelaarandPalmer,Dang,andFellbaum,
the“supersenses”wereidentiﬁedbyhand,whereasBruceandWiebeandV´eronisused
clusteringmethodssuchasthosefromBruceandWiebe(1999)tocollapsesomeofthe
initial sense distinctions.
9
Palmer, Dang, and Fellbaum (2007) illustrate this practice
with the example of the verb call, which has 28 ﬁne-grained senses in WordNet 1.7:
They conﬂate these senses into a small number of groups using various criteria—for
example, four senses can be grouped in a group they call Group 1 on the basis of
subcategorizationframesimilarities(Table9).
Palmer,Dang,andFellbaum(2007)achievedfortheEnglishVerbLexicalSensetask
ofSENSEVAL-2apercentageagreementamongcodersof82%withgroupedsenses,as
opposedto71%withtheoriginalWordNetsenses.BruceandWiebe(1998)foundthat
collapsingthesensesoftheirtestword(interest)onthebasisoftheirusebycodersand
merging the two classes found to be harder to distinguish resulted in an increase of
9 ThemethodologyproposedinBruceandWiebe(1999)isinourviewthemostadvancedtechniqueto
“makesense”oftheresultsofagreementstudiesavailableintheliterature.Theextendedversionofthis
articlecontainsafullerintroductiontothesemethods.
587
ComputationalLinguistics Volume34,Number4
Table9
Group1ofsensesof call inPalmer,Dang,andFellbaum(2007,page149).
SENSE DESCRIPTION EXAMPLE HYPERNYM
WN1 name,call “Theynamed
a
theirsonDavid” LABEL
WN3 call,giveaquality “Shecalledherchildrenlazy LABEL
andungrateful”
WN19 call,consider “Iwouldnotcallherbeautiful” SEE
WN22 address,call “Callmemister” ADDRESS
a
Theverb named appearsintheoriginalWordNetexamplefortheverb call.
thevalueofKfrom0.874to0.898.Usingarelatedtechnique,V´eronis(1998)foundthat
agreementonnounwordsensetaggingwentupfromaKofaround0.45toaKof0.86.
We should note, however, that the post hoc merging of categories is not equivalent to
runningastudywithfewercategoriestobeginwith.
Attemptswerealsomadetodeveloptechniquestomeasurepartialagreementwith
hierarchical tagsets. A ﬁrst proposal in this direction was advanced by Melamed and
Resnik (2000), who developed a coefﬁcient for hierarchical tagsets that could be used
in SENSEVAL for measuring agreement with tagsets such as HECTOR. Melamed and
Resnikproposedto“normalize”thecomputationofobservedandexpectedagreement
by taking each label which is not a leaf in the tag hierarchy and distributing it down
totheleavesinauniformway,andthenonlycomputingagreementontheleaves.For
example, with a tagset like the one in Table 9, the cases in which the coders used the
label ‘Group1’ would be uniformly “distributed down” and added in equal measure
to the number of cases in which the coders assigned each of the four WordNet labels.
The method proposed in the paper has, however, problematic properties when used
tomeasureintercoderagreement.Forexample,supposetagAdominatestwosub-tags
A1andA2,andthattwocodersmarkaparticularitemasA.Intuitively,wewouldwant
toconsiderthisacaseofperfectagreement,butthisisnotwhatthemethodproposed
by Melamed and Resnik yields. The annotators’ marks are distributed over the two
sub-tags, each with probability 0.5, and then the agreement is computed by summing
thejointprobabilitiesoverthetwosubtags(Equation(4)ofMelamedandResnik2000),
withtheresultthattheagreementovertheitemturnsouttobe0.5
2
+0.5
2
= 0.5instead
of 1. To correct this, Dan Melamed (personal communication) suggested replacing the
productinEquation(4)withaminimumoperator.However,thecalculationofexpected
agreement(Equation(5)ofMelamedandResnik2000)stillgivestheamountofagree-
mentwhichisexpectedifcodersareforcedtochooseamongleafnodes,whichmakes
thismethodinappropriateforcodingschemesthatdonotforcecoderstodothis.
One way to use Melamed and Resnik’s proposal while avoiding the discrepancy
between observed and expected agreement is to treat the proposal not as a new co-
efﬁcient, but rather as a distance metric to be plugged into a weighted coefﬁcient
like α.LetA and B be two nodes in a hierarchical tagset, let L be the set of all leaf
nodes in the tagset, and let P(l|T) be the probability of selecting a leaf node l given
an arbitrary node T when the probability mass of T is distributed uniformly to all the
nodes dominated by T. We can reinterpret Melamed’s modiﬁcation of Equation (4) in
MelamedandResnik(2000)asametricmeasuringthedistancebetweennodes A and B.
d
M+R
= 1−
∑
l∈L
min(P(l|A),P(l|B))
588
ArtsteinandPoesio Inter-CoderAgreementforCL
This metric has the desirable properties—it is 0 when tags A and B are identical,
1 when the tags do not overlap, and somewhere in between in all other cases. If we
use this metric for Krippendorff’s α we ﬁnd that observed agreement is exactly the
sameasinMelamedandResnik(2000)withtheproductoperatorreplacedbyminimum
(Melamed’smodiﬁcation).
We can also use other distance metrics with α. For example, we could associate
with each sense an extended sense—a set es(s) including the sense itself and its
grouped sense—and then use set-based distance metrics from Section 4.4, for ex-
ample Passonneau’s d
P
. To illustrate how this approach could be used to measure
(dis)agreementonwordsenseannotation,supposethattwocodershavetoannotatethe
useof call inthefollowingsentence(fromtheWSJpartofthePennTreebank,section02,
textw0209):
Thisgene,called“gametocide,”iscarriedintotheplantbyavirusthat
remainsactiveforafewdays.
The standard guidelines (in SENSEVAL, say) require coders to assign a WN sense to
words.Undersuchguidelines,ifcoderAclassiﬁestheuseofcalledintheaboveexample
asaninstanceofWN1,whereascoderBannotatesitasaninstanceofWN3,wewould
ﬁndtotaldisagreement(d
k
a
k
b
= 1)whichseemsexcessivelyharshasthetwosensesare
clearly related. However, by using the broader senses proposed by Palmer, Dang, and
Fellbaum (2007) in combination with a distance metric such as the one just proposed,
it is possible to get more ﬂexible and, we believe, more realistic assessments of the
degreeofagreementinsituationssuchasthis.Forinstance,incasethereliabilitystudy
had already been carried out under the standard SENSEVAL guidelines, the distance
metric proposed above could be used to identify post hoc cases of partial agreement
by adding to each WN sense its hypernyms according to the groupings proposed by
Palmer, Dang, and Fellbaum. For example, A’s annotation could be turned into a new
set label {WN1,LABEL} and B’s mark into the set table {WN3,LABEL}, which would
give a distance d = 2/3, indicating a degree of overlap. The method for computing
agreement proposed here could could also be used to allow coders to choose either a
more speciﬁc label or one of Palmer, Dang, and Fellbaum’s superlabels. For example,
supposeAstickstoWN1,butBdecidestomarktheuseaboveusingPalmer,Dang,and
Fellbaum’sLABELcategory,thenwewouldstillﬁndadistanced = 1/3.
Analternativewayofusingαforwordsenseannotationwasdevelopedandtested
by Passonneau, Habash, and Rambow (2006). Their approach is to allow coders to
assignmultiplelabels(WordNetsynsets)forwordsenses,asdonebyV´eronis(1998)and
more recently by Rosenberg and Binkowski (2004) for text classiﬁcation labels and by
Poesio and Artstein (2005) for anaphora. These multi-label sets can then be compared
usingtheMASIdistancemetricfor α (Passonneau2006).
5.Conclusions
Thepurposeofthisarticlehasbeentoexposethereadertothemathematicsofchance-
correctedcoefﬁcientsofagreementaswellasthecurrentstateoftheartofusingthese
coefﬁcients in CL. Our hope is that readers come to view agreement studies not as an
additional chore or hurdle for publication, but as a tool for analysis which offers new
insightsintotheannotationprocess.Weconcludebysummarizingwhatinourvieware
themainrecommendationsemergingfromtenyearsofexperiencewithcoefﬁcientsof
agreement. These can be grouped under three main headings: methodology, choice of
coefﬁcients,andinterpretationofcoefﬁcients.
589
ComputationalLinguistics Volume34,Number4
5.1Methodology
Ourﬁrstrecommendationisthatannotationeffortsshouldperformandreportrigorous
reliability testing. The last decade has already seen considerable improvement, from
the absence of any tests for the Penn Treebank (Marcus, Marcinkiewicz, and Santorini
1993) or the British National Corpus (Leech, Garside, and Bryant 1994) to the central
roleplayedbyreliabilitytestinginthePennDiscourseTreebank(Miltsakakietal.2004)
and OntoNotes (Hovy et al. 2006). But even the latter efforts only measure and report
percent agreement. We believe that part of the reluctance to report chance-corrected
measuresisthedifﬁcultyininterpretingthem.However,ourexperienceisthatchance-
corrected coefﬁcients of agreement do provide a better indication of the quality of the
resultingannotationthansimplepercentagreement,andmoreover,thedetailedcalcu-
lations leading to the coefﬁcients can be very revealing as to where the disagreements
arelocatedandwhattheirsourcesmaybe.
Arigorousmethodologyforreliabilitytestingdoesnot,inouropinion,excludethe
useofexpertcoders,andherewefeeltheremaybeamotivateddifferencebetweenthe
ﬁelds of content analysis and CL. There is a clear tradeoff between the complexity of
the judgments that coders are required to make and the reliability of such judgments,
and we should strive to devise annotation schemes that are not only reliable enough
to be replicated, but also sophisticated enough to be useful (cf. Krippendorff 2004a,
pages 213–214). In content analysis, conclusions are drawn directly from annotated
corpora, so the emphasis is more on replicability; whereas in CL, corpora constitute a
resourcewhichisusedbyotherprocesses,sotheemphasisismoretowardsusefulness.
Thereisalsoatradeoffbetweenthesophisticationofjudgmentsandtheavailabilityof
coders who can make such judgments. Consequently, annotation by experts is often
the only practical way to get useful corpora for CL. Current practice achieves high
reliability either by using professionals (Kilgarriff 1999) or through intensive training
(Hovyetal.2006;Carlson,Marcu,andOkurowski2003);thismeansthatresultsarenot
replicable across sites, and are therefore less reliable than annotation by naive coders
adheringtowritteninstructions.Wefeelthatinter-annotatoragreementstudiesshould
still be carried out, as they serve as an assurance that the results are replicable when
the annotators are chosen from the same population as the original annotators. An
important additional assurance should be provided in the form of an independent
evaluationofthetaskforwhichthecorpusisused(cf.Passonneau2006).
5.2ChoosingaCoefﬁcient
One of the goals of this article is to helpauthors make an informed choice regarding
the coefﬁcients they use for measuring agreement. While coefﬁcients other than K,
speciﬁcallyCohen’sκ andKrippendorff’sα,haveappearedintheCLliteratureasearly
asCarletta(1996)andPassonneauandLitman(1996),theyhadn’tsprungintogeneral
awarenessuntilthepublicationofDiEugenioandGlass(2004)andPassonneau(2004).
Regarding the question of annotator bias, there is an overwhelming consensus in CL
practice:Kandαareusedinthevastmajorityofthestudieswereported.Weagreewith
theviewthatKand α aremoreappropriate,astheyabstractawayfromthebiasofspe-
ciﬁccoders.Butwealsobelievethatultimatelythisissueofannotatorbiasisoflittlecon-
sequence because the differences get smaller and smaller as the number of annotators
grows(ArtsteinandPoesio2005).Webelievethatincreasingthenumberofannotators
isthebeststrategy,becauseitreducesthechancesofaccidentalpersonalbiases.
590
ArtsteinandPoesio Inter-CoderAgreementforCL
However, Krippendorff’s α is indispensable when the category labels are not
equally distinct from one another. We think there are at least two types of coding
schemes in which this is the case: (i) hierarchical tagsets and (ii) set-valued interpre-
tations such as those proposed for anaphora. At least in the second case, weighted
coefﬁcientsarealmostunavoidable.Wethereforerecommendusing α,notinghowever
thatthespeciﬁcchoiceofweightswillaffecttheoverallnumericalresult.
5.3InterpretingtheValues
Weviewthelackofconsensusonhowtointerpretthevaluesofagreementcoefﬁcients
as a serious problem with current practice in reliability testing, and as one of the
mainreasonsforthereluctanceofmanyinCLtoembarkonreliabilitystudies.Unlike
signiﬁcancevalueswhichreportaprobability(thatanobservedeffectisduetochance),
agreement coefﬁcients report a magnitude, and it is less clear how to interpret such
magnitudes. Our own experience is consistent with that of Krippendorff: Both in our
earlier work (Poesio and Vieira 1998; Poesio 2004a) and in the more recent efforts
(PoesioandArtstein2005)wefoundthatonlyvaluesabove0.8ensuredanannotation
ofreasonablequality(Poesio2004a).Wethereforefeelthatifathresholdneedstobeset,
0.8isagoodvalue.
That said, we doubt that a single cutoff point is appropriate for all purposes.
For some CL studies, particularly on discourse, useful corpora have been obtained
while attaining reliability only at the 0.7 level. We agree therefore with Craggs and
McGee Wood (2005) that setting a speciﬁc agreement threshold should not be a pre-
requisiteforpublication.Instead,asrecommendedbyDiEugenioandGlass(2004)and
others, researchers should report in detail on the methodology that was followed in
collecting the reliability data (number of coders, whether they coded independently,
whethertheyreliedexclusivelyonanannotationmanual),whetheragreementwassta-
tisticallysigniﬁcant,andprovideaconfusionmatrixoragreementtablesothatreaders
canﬁndoutwhetheroverallﬁguresofagreementhidedisagreementsonlesscommon
categories.Foranexampleofgoodpracticeinthisrespect,seeTeufelandMoens(2002).
Thedecisionwhetheracorpusisgoodenoughforpublicationshouldbebasedonmore
than the agreement score—speciﬁcally, an important consideration is an independent
evaluationoftheresultsthatarebasedonthecorpus.
Acknowledgments
ThisworkwassupportedinpartbyEPSRC
grantGR/S76434/01,ARRAU.Wewishto
thankfouranonymousreviewersandJean
Carletta,MarkCore,BarbaraDiEugenio,
RuthFilik,MichaelGlass,GeorgeHripcsak,
AdamKilgarriff,DanMelamed,Becky
Passonneau,PhilResnik,TonySanford,
PatrickSturt,andDavidTraumforhelpful
commentsanddiscussion.Specialthanksto
KlausKrippendorffforanextremelydetailed
reviewofanearlierversionofthisarticle.We
arealsoextremelygratefultotheBritish
LibraryinLondon,whichmadeaccessibleto
usvirtuallyeverypaperweneededforthis
research.
References
Allen,JamesandMarkCore.1997. DAMSL:
Dialogueactmarkupinseverallayers.
DraftcontributionfortheDiscourse
ResourceInitiative,Universityof
Rochester.Availableat
http://www.cs.rochester.edu/
research/cisd/resources/damsl/.
Artstein,RonandMassimoPoesio.2005.
Biasdecreasesinproportiontothenumber
ofannotators. In Proceedings of FG-MoL
2005,pages141–150,Edinburgh.
Artstein,RonandMassimoPoesio.2006.
Identifyingreferencetoabstractobjects
indialogue. In brandial 2006: Proceedings
of the 10th Workshop on the Semantics and
591
ComputationalLinguistics Volume34,Number4
Pragmatics of Dialogue,pages56–63,
Potsdam.
Atkins,Sue.1992. Toolsforcomputer-aided
corpuslexicography:TheHectorproject.
Acta Linguistica Hungarica,41:5–71.
Babarczy,Anna,JohnCarroll,andGeoffrey
Sampson.2006. Deﬁnitional,personal,
andmechanicalconstraintsonpart
ofspeechannotationperformance.
Natural Language Engineering,12(1):77–90.
Bartko,JohnJ.andWilliamT.Carpenter,Jr.
1976. Onthemethodsandtheoryof
reliability. Journal of Nervous and Mental
Disease,163(5):307–317.
Beeferman,Doug,AdamBerger,and
JohnLafferty.1999. Statisticalmodels
fortextsegmentation. Machine Learning,
34(1–3):177–210.
Bennett,E.M.,R.Alpert,andA.C.
Goldstein.1954. Communications
throughlimitedquestioning. Public
Opinion Quarterly,18(3):303–308.
Bloch,DanielA.andHelenaChmura
Kraemer.1989. 2×2kappacoefﬁcients:
Measuresofagreementorassociation.
Biometrics,45(1):269–287.
Brennan,RobertL.andDaleJ.Prediger.
1981. Coefﬁcientkappa:Someuses,
misuses,andalternatives. Educational
and Psychological Measurement,
41(3):687–699.
Bruce,RebeccaandJanyceWiebe.1998.
Word-sensedistinguishabilityand
inter-coderagreement. In Proceedings
of EMNLP,pages53–60,Granada.
Bruce,RebeccaF.andJanyceM.Wiebe.1999.
Recognizingsubjectivity:Acasestudyin
manualtagging. Natural Language
Engineering,5(2):187–205.
Buitelaar,Paul.1998. CoreLex : Systematic
Polysemy and Underspeciﬁcation.Ph.D.
thesis,BrandeisUniversity,Waltham,MA.
Bunt,HarryC.2000. Dynamicinterpretation
anddialoguetheory. InMartinM.Taylor,
Franc¸oiseN´eel,andDonG.Bouwhuis,
editors, The Structure of Multimodal
Dialogue II.JohnBenjamins,Amsterdam,
pages139–166.
Bunt,HarryC.2005. Aframeworkfor
dialogueactspeciﬁcation. In Proceedings
of the Joint ISO-ACL Workshop on the
Representation and Annotation of Semantic
Information,Tilburg.Availableat:
http://let.uvt.nl/research/ti/
sigsem/wg/discussionnotes4.htm.
Byron,DonnaK.2002. Resolving
pronominalreferencetoabstractentities.
In Proceedings of the 40th Annual Meeting of
the ACL,pages80–87,Philadelphia,PA.
Byrt,Ted,JanetBishop,andJohnB.Carlin.
1993. Bias,prevalenceandkappa. Journal
of Clinical Epidemiology,46(5):423–429.
Carletta,Jean.1996. Assessingagreementon
classiﬁcationtasks:Thekappastatistic.
Computational Linguistics,22(2):249–254.
Carletta,Jean,AmyIsard,StephenIsard,
JacquelineC.Kowtko,Gwyneth
Doherty-Sneddon,andAnneH.
Anderson.1997. Thereliabilityofa
dialoguestructurecodingscheme.
Computational Linguistics,23(1):13–32.
Carlson,Lynn,DanielMarcu,and
MaryEllenOkurowski.2003. Buildinga
discourse-taggedcorpusintheframework
ofrhetoricalstructuretheory. InJanC.J.
vanKuppeveltandRonnieW.Smith,
editors, Current and New Directions in
Discourse and Dialogue.Kluwer,Dordrecht,
pages85–112.
Cicchetti,DomenicV.andAlvanR.
Feinstein.1990. Highagreementbutlow
kappa:II.Resolvingtheparadoxes. Journal
of Clinical Epidemiology,43(6):551–558.
Cohen,Jacob.1960. Acoefﬁcientof
agreementfornominalscales. Educational
and Psychological Measurement,20(1):37–46.
Cohen,Jacob.1968. Weightedkappa:
Nominalscaleagreementwithprovision
forscaleddisagreementorpartialcredit.
Psychological Bulletin,70(4):213–220.
Core,MarkG.andJamesF.Allen.1997.
CodingdialogswiththeDAMSL
annotationscheme. In Working Notes of the
AAAI Fall Symposium on Communicative
Action in Humans and Machines,AAAI,
Cambridge,MA.Availableat:http://www.
cs.umd.edu/∼traum/CA/fpapers.html.
Craggs,RichardandMaryMcGeeWood.
2004. Atwo-dimensionalannotation
schemeforemotionindialogue. In Papers
from the 2004 AAAI Spring Symposium on
Exploring Attitude and Affect in Text:
Theories and Applications,Stanford,
pages44–49.
Craggs,RichardandMaryMcGeeWood.
2005. Evaluatingdiscourseanddialogue
codingschemes. Computational Linguistics,
31(3):289–295.
Davies,MarkandJosephL.Fleiss.1982.
Measuringagreementformultinomial
data. Biometrics,38(4):1047–1051.
DiEugenio,Barbara.2000. Ontheusageof
Kappatoevaluateagreementoncoding
tasks. In Proceedings of LREC,volume1,
pages441–444,Athens.
DiEugenio,BarbaraandMichaelGlass.
2004. Thekappastatistic:Asecondlook.
Computational Linguistics,30(1):95–101.
592
ArtsteinandPoesio Inter-CoderAgreementforCL
DiEugenio,Barbara,PamelaW.Jordan,
JohannaD.Moore,andRichmondH.
Thomason.1998. Anempirical
investigationofproposalsincollaborative
dialogues. In Proceedings of 36th Annual
Meeting of the ACL,pages325–329,
Montreal.
Dice,LeeR.1945. Measuresoftheamount
ofecologicassociationbetweenspecies.
Ecology,26(3):297–302.
Donner,AllanandMichaelEliasziw.
1987. Samplesizerequirementsfor
reliabilitystudies. Statistics in Medicine,
6:441–448.
Doran,Christine,JohnAberdeen,Laurie
Damianos,andLynetteHirschman.
2001. Comparingseveralaspectsof
human-computerandhuman-human
dialogues. In Proceedings of the 2nd
SIGdial Workshop on Discourse and
Dialogue,Aalborg,Denmark.Availableat:
http://www.sigdial.org/workshops/
workshop2/proceedings.
Eckert,MiriamandMichaelStrube.2000.
Dialogueacts,synchronizingunits,
andanaphoraresolution. Journal of
Semantics,17(1):51–89.
Feinstein,AlvanR.andDomenicV.Cicchetti.
1990. Highagreementbutlowkappa:
I.Theproblemsoftwoparadoxes. Journal
of Clinical Epidemiology,43(6):543–549.
Fellbaum,Christiane,editor.1998. WordNet:
An Electronic Lexical Database. MITPress,
Cambridge,MA.
Fleiss,JosephL.1971. Measuringnominal
scaleagreementamongmanyraters.
Psychological Bulletin,76(5):378–382.
Fleiss,JosephL.1975. Measuringagreement
betweentwojudgesonthepresenceor
absenceofatrait. Biometrics,31(3):651–659.
Francis,W.NelsonandHenryKucera.
1982. Frequency Analysis of English Usage:
lexicon and grammar. HoughtonMifﬂin,
Boston,MA.
Geertzen,JeroenandHarryBunt.2006.
Measuringannotatoragreementina
complexhierarchicaldialogueact
annotationscheme. In Proceedings of the
7th SIGdial Workshop on Discourse and
Dialogue,pages126–133,Sydney.
Gross,Derek,JamesF.Allen,andDavidR.
Traum.1993. TheTrains91dialogues.
TRAINSTechnicalNote92-1,University
ofRochesterComputerScience
Department,Rochester,NY.
Grosz,BarbaraJ.andCandaceL.Sidner.
1986. Attention,intentions,andthe
structureofdiscourse. Computational
Linguistics,12(3):175–204.
Hayes,AndrewF.andKlausKrippendorff.
2007. Answeringthecallforastandard
reliabilitymeasureforcodingdata.
Communication Methods and Measures,
1(1):77–89.
Hearst,MartiA.1997. TextTiling:
Segmentingtextintomulti-paragraph
subtopicpassages. Computational
Linguistics,23(1):33–64.
Hovy,Eduard,MitchellMarcus,Martha
Palmer,LanceRamshaw,andRalph
Weischedel.2006. OntoNotes:The90%
solution. In Proceedings of HLT–NAACL,
Companion Volume: Short Papers,
pages57–60,NewYork.
Hsu,LouisM.andRonaldField.2003.
Interrateragreementmeasures:Comments
onkappa
n,Cohen’skappa,Scott’s π,
andAickin’s α. Understanding Statistics,
2(3):205–219.
Jaccard,Paul.1912. Thedistributionofthe
ﬂoraintheAlpinezone. New Phytologist,
11(2):37–50.
Jekat,Susanne,AlexandraKlein,Elisabeth
Maier,IlonaMaleck,MarionMast,and
J.JoachimQuantz.1995. Dialogueactsin
VERBMOBIL. VM-Report65,Universit¨at
Hamburg,DFKIGmbH,Universit¨at
Erlangen,andTUBerlin.
Jurafsky,Daniel,ElizabethShriberg,and
DebraBiasca.1997. Switchboard
SWBD-DAMSLshallow-discourse-
functionannotationcodersmanual,
draft13. TechnicalReport97-02,
UniversityofColoradoatBoulder,
InstituteforCognitiveScience.
Kilgarriff,Adam.1999. 95%replicability
formanualwordsensetagging. In
Proceedings of the Ninth Conference
of the European Chapter of the Association
for Computational Linguistics,
pages277–278,Bergen,Norway.
Klavans,JudithL.,SamuelPopper,and
RebeccaPassonneau.2003. Tackling
theinternetglossaryglut:Automatic
extractionandevaluationofgenus
phrases. In Proceedings of the
SIGIR-2003 Workshop on the Semantic Web,
Toronto.
Kowtko,JacquelineC.,StephenD.Isard,
andGwynethM.Doherty.1992.
Conversationalgameswithindialogue.
ResearchPaperHCRC/RP-31,Human
CommunicationResearchCentre,
UniversityofEdinburgh.
Krippendorff,Klaus.1970. Estimatingthe
reliability,systematicerrorandrandom
errorofintervaldata. Educational and
Psychological Measurement,30(1):61–70.
593
ComputationalLinguistics Volume34,Number4
Krippendorff,Klaus.1978. Reliabilityof
binaryattributedata. Biometrics,
34(1):142–144. Lettertotheeditor,
withareplybyJosephL.Fleiss.
Krippendorff,Klaus.1980. Content Analysis:
An Introduction to Its Methodology,
chapter12.Sage,BeverlyHills,CA.
Krippendorff,Klaus.1995. Onthereliability
ofunitizingcontiguousdata. Sociological
Methodology,25:47–76.
Krippendorff,Klaus.2004a. Content Analysis:
An Introduction to Its Methodology,
secondedition,chapter11.Sage,
ThousandOaks,CA.
Krippendorff,Klaus.2004b. Reliability
incontentanalysis:Somecommon
misconceptionsandrecommendations.
Human Communication Research,
30(3):411–433.
Landis,J.RichardandGaryG.Koch.1977.
Themeasurementofobserveragreement
forcategoricaldata. Biometrics,
33(1):159–174.
Leech,Geoffrey,RogerGarside,andMichael
Bryant.1994. CLAWS4:Thetaggingofthe
BritishNationalCorpus. In Proceedings of
COLING 1994: The 15th International
Conference on Computational Linguistics,
Volume 1,pages622–628,Kyoto.
Levin,JamesA.andJamesA.Moore.1978.
Dialogue-games:Metacommunication
structuresfornaturallanguage
interaction. Cognitive Science,1(4):395–420.
Manning,ChristopherD.andHinrich
Schuetze.1999. Foundations of Statistical
Natural Language Processing. MITPress,
Cambridge,MA.
Marcu,Daniel,MagdalenaRomera,and
EstibalizAmorrortu.1999. Experimentsin
constructingacorpusofdiscoursetrees:
Problems,annotationchoices,issues. In
Workshop on Levels of Representation in
Discourse,pages71–78,Universityof
Edinburgh.
Marcus,MitchellP.,MaryAnn
Marcinkiewicz,andBeatriceSantorini.
1993. Buildingalargeannotatedcorpusof
English:thePennTreebank. Computational
Linguistics,19(2):313–330.
Marion,Rodger.2004. Thewholeartof
deduction. Unpublishedmanuscript.
Melamed,I.DanandPhilipResnik.2000.
Taggerevaluationgivenhierarchical
tagsets. Computers and the Humanities,
34(1–2):79–84.Availableat:
http://www.sahs/utmb.edu/PELLINORE/
Intro to research/wad/wad/ home.htm.
Mieskes,MargotandMichaelStrube.2006.
Part-of-speechtaggingoftranscribed
speech. In Proceedings of LREC,
pages935–938,Genoa.
Mihalcea,Rada,TimothyChklovski,and
AdamKilgarriff.2004. TheSENSEVAL-3
Englishlexicalsampletask. In Proceedings
of SENSEVAL-3,pages25–28,Barcelona.
Miltsakaki,Eleni,RashmiPrasad,Aravind
Joshi,andBonnieWebber.2004.
Annotatingdiscourseconnectives
andtheirarguments. In Proceedings
of the HLT-NAACL Workshop on
Frontiers in Corpus Annotation,pages9–16,
Boston,MA.
Moser,MeganG.,JohannaD.Moore,and
ErinGlendening.1996. Instructions
forCodingExplanations:Identifying
Segments,RelationsandMinimalUnits.
TechnicalReport96-17,Universityof
Pittsburgh,DepartmentofComputer
Science.
Navarretta,Costanza.2000. Abstract
anaphoraresolutioninDanish. In
Proceedings of the 1st SIGdial Workshop on
Discourse and Dialogue,HongKong,
pages56–65.
Nenkova,AniandRebeccaPassonneau.
2004. Evaluatingcontentselectionin
summarization:Thepyramidmethod.
In Proceedings of HLT-NAACL 2004,
pages145–152,Boston,MA.
Neuendorf,KimberlyA.2002. The
Content Analysis Guidebook.Sage,
ThousandOaks,CA.
Palmer,Martha,HoaTrangDang,and
ChristianeFellbaum.2007. Making
ﬁne-grainedandcoarse-grainedsense
distinctions,bothmanuallyand
automatically. Natural Language
Engineering,13(2):137–163.
Passonneau,RebeccaJ.2004. Computing
reliabilityforcoreferenceannotation.
In Proceedings of LREC,volume4,
pages1503–1506,Lisbon.
Passonneau,RebeccaJ.2006. Measuring
agreementonset-valueditems(MASI)
forsemanticandpragmaticannotation.
In Proceedings of LREC,Genoa,
pages831–836.
Passonneau,RebeccaJ.,NizarHabash,and
OwenRambow.2006. Inter-annotator
agreementonamultilingualsemantic
annotationtask. In Proceedings of LREC,
Genoa,pages1951–1956.
Passonneau,RebeccaJ.andDianeJ.Litman.
1993. Intention-basedsegmentation:
Humanreliabilityandcorrelationwith
linguisticcues. In Proceedings of 31st
Annual Meeting of the ACL,pages148–155,
Columbus,OH.
594
ArtsteinandPoesio Inter-CoderAgreementforCL
Passonneau,RebeccaJ.andDianeJ.Litman.
1996. Empiricalanalysisofthree
dimensionsofspokendiscourse:
Segmentation,coherenceandlinguistic
devices. InEduardH.HovyandDonia
R.Scott,editors, Computational and
Conversational Discourse: Burning Issues –
An Interdisciplinary Account,volume151
of NATO ASI Series F: Computer and
Systems Sciences.Springer,Berlin,
chapter7,pages161–194.
Passonneau,RebeccaJ.andDianeJ.Litman.
1997. Discoursesegmentationbyhuman
andautomatedmeans. Computational
Linguistics,23(1):103–139.
Pevzner,LevandMartiA.Hearst.2002.
Acritiqueandimprovementofan
evaluationmetricfortextsegmentation.
Computational Linguistics,28(1):19–36.
Poesio,Massimo.2004a. Discourse
annotationandsemanticannotationinthe
GNOMEcorpus. In Proceedings of the 2004
ACL Workshop on Discourse Annotation,
pages72–79,Barcelona.
Poesio,Massimo.2004b. The
MATE/GNOMEproposalsforanaphoric
annotation,revisited. In Proceedings of the
5th SIGdial Workshop on Discourse and
Dialogue,pages154–162,Cambridge,MA.
Poesio,MassimoandRonArtstein.2005.
Thereliabilityofanaphoricannotation,
reconsidered:Takingambiguityinto
account. In Proceedings of the Workshop on
Frontiers in Corpus Annotation II: Pie in the
Sky,pages76–83,AnnArbor,MI.
Poesio,MassimoandNataliaN.Modjeska.
2005. Focus,activation,and this-noun
phrases:Anempiricalstudy. In
Ant´onioBranco,TonyMcEnery,and
RuslanMitkov,editors, Anaphora
Processing,volume263of Current Issues
in Linguistic Theory.JohnBenjamins,
pages429–442,Amsterdamand
Philadelphia.
Poesio,Massimo,A.Patel,andBarbara
DiEugenio.2006. Discoursestructure
andanaphoraintutorialdialogues:An
empiricalanalysisoftwotheoriesofthe
globalfocus. Research in Language and
Computation,4(2–3):229–257.
Poesio,MassimoandRenataVieira.1998.
Acorpus-basedinvestigationofdeﬁnite
descriptionuse. Computational Linguistics,
24(2):183–216.
Popescu-Belis,Andrei.2005. Dialogueacts:
Oneormoredimensions? Working
Paper62,ISSCO,UniversityofGeneva.
Posner,KarenL.,PaulD.Sampson,RobertA.
Caplan,RichardJ.Ward,andFrederickW.
Cheney.1990. Measuringinterrater
reliabilityamongmultipleraters:An
exampleofmethodsfornominaldata.
Statistics in Medicine,9:1103–1115.
Rajaratnam,Nageswari.1960. Reliability
formulasforindependentdecisiondata
whenreliabilitydataarematched.
Psychometrika,25(3):261–271.
Reidsma,DennisandJeanCarletta.2008.
Reliabilitymeasurementwithoutlimits.
Computational Linguistics,34(3):319–326.
Reinhart,T.1981. Pragmaticsandlinguistics:
Ananalysisofsentencetopics.
Philosophica,27(1):53–93.
Reynar,JeffreyC.1998. Topic Segmentation:
Algorithms and Applications. Ph.D.thesis,
UniversityofPennsylvania,Philadelphia.
Ries,Klaus.2002. Segmentingconversations
bytopic,initiativeandstyle. InAnniR.
Coden,EricW.Brown,andSavitha
Srinivasan,editors, Information Retrieval
Techniques for Speech Applications,
volume2273of Lecture Notes in Computer
Science.Springer,Berlin,pages51–66.
Rietveld,ToniandRoelandvanHout.
1993. Statistical Techniques for the Study
of Language and Language Behaviour.
MoutondeGruyter,Berlin.
Rosenberg,AndrewandEdBinkowski.
2004. Augmentingthekappastatistic
todetermineinterannotatorreliability
formultiplylabeleddatapoints. In
Proceedings of HLT-NAACL 2004: Short
Papers,pages77–80,Boston,MA.
Scott,WilliamA.1955. Reliabilityofcontent
analysis:Thecaseofnominalscale
coding. Public Opinion Quarterly,
19(3):321–325.
Shriberg,Elizabeth,RajDhillon,Sonali
Bhagat,JeremyAng,andHannahCarvey.
2004. TheICSImeetingrecorderdialogact
(MRDA)corpus. In Proceedings of the 5th
SIGdial Workshop on Discourse and Dialogue,
pages97–100,Cambridge,MA.
Siegel,SidneyandN.JohnCastellan,Jr.1988.
Nonparametric Statistics for the Behavioral
Sciences,2ndedition,chapter9.8.
McGraw-Hill,NewYork.
Stent,AmandaJ.2001. Dialogue Systems
as Conversational Partners: Applying
Conversation Acts Theory to Natural
Language Generation for Task-Oriented
Mixed-Initiative Spoken Dialogue.Ph.D.
thesis,DepartmentofComputerScience,
UniversityofRochester.
Stevenson,MarkandRobertGaizauskas.
2000. Experimentsonsentenceboundary
detection. In Proceedings of 6th ANLP,
pages84–89,Seattle,WA.
595
ComputationalLinguistics Volume34,Number4
Stolcke,Andreas,NoahCoccaro,Rebecca
Bates,PaulTaylor,CarolVanEss-Dykema,
KlausRies,ElizabethShriberg,Daniel
Jurafsky,RachelMartin,andMarieMeteer.
2000. Dialogueactmodelingforautomatic
taggingandrecognitionofconversational
speech. Computational Linguistics,
26(3):339–373.
Stuart,Alan.1955. Atestforhomogeneityof
themarginaldistributionsinatwo-way
classiﬁcation. Biometrika,42(3/4):412–416.
Teufel,Simone,JeanCarletta,andMarc
Moens.1999. Anannotationscheme
fordiscourse-levelargumentationin
researcharticles. In Proceedings of Ninth
Conference of the EACL,pages110–117,
Bergen.
Teufel,SimoneandMarcMoens.2002.
Summarizingscientiﬁcarticles:
Experimentswithrelevanceand
rhetoricalstatus. Computational
Linguistics,28(4):409–445.
Traum,DavidR.andElizabethA.
Hinkelman.1992. Conversation
actsintask-orientedspokendialogue.
Computational Intelligence,8(3):575–599.
Vallduv´ı,Enric.1993. Information
packaging:Asurvey. ResearchPaper
RP-44,UniversityofEdinburgh,HCRC.
V´eronis,Jean.1998. Astudyofpolysemy
judgmentsandinter-annotatoragreement.
In Proceedings of SENSEVAL-1,
HerstmonceuxCastle,England.Available
at:http://www.itri.brighton.ac.uk/
events/senseval/ARCHIVE/PROCEEDINGS/.
Vilain,Marc,JohnBurger,JohnAberdeen,
DennisConnolly,andLynetteHirschman.
1995. Amodel-theoreticcoreference
scoringscheme. In Proceedings of the Sixth
Message Understanding Conference,
pages45–52,Columbia,MD.
Zwick,Rebecca.1988. Anotherlookat
interrateragreement. Psychological
Bulletin,103(3):374–378.
596

