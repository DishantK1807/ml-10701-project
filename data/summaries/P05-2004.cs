A promising approach is to learn the mappings using decision trees or random forests, which has recently achieved good results in a similar problem in language modeling (Xu and Jelinek, 2004).
W04-3242
Finally, we plan to integrate the tagger/chunker in an endto-end system, such as a Factored Language Model (Bilmes and Kirchhoff, 2003), to measure the overall merit of joint labeling.
N03-2002
See, for example, NP chunkers utilizing conditional random fields (Sha and Pereira, 2003) and support vector machines (Kudo and Matsumoto, 2001).
N01-1025 N03-1028
This segmentation task can be achieved by assigning words in a sentence to one of three tokens: B for “Begin-NP”, I for “Inside-NP”, or O for “OutsideNP” (Ramshaw and Marcus, 1995).
W95-0107
Florian and Ngai (2001) extends transformationbased learning tagger to a joint tagger/chunker by modifying the objective function such that a transformation rule is evaluated on the classification of all simultaneous subtasks.
W01-0701
The data comes from the CoNLL 2000 shared task (Sang and Buchholz, 2000), which consists of sentences from the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).
J93-2004
The most notable of these include the trigram HMM tagger (Brants, 2000), maximum entropy tagger (Ratnaparkhi, 1996), transformation-based tagger (Brill, 1995), and cyclic dependency networks (Toutanova et al., 2003).
A00-1031 J95-4004 N03-1033 W96-0213
