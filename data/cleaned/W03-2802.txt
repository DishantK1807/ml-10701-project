1:192	The PEACE SLDS understanding evaluation paradigm of the French MEDIA campaign Laurence Devillers, Hel`ene Maynard, Patrick Paroubek, Sophie Rosset LIMSI-CNRS Bt 508 University of Paris XI BP 133 F-91403 ORSAY Cedex, France CUdevil,hbm,pap,rossetCV@limsi.fr Abstract This paper presents a paradigm for evaluating the context-sensitive understanding capability of any spoken language dialog system: PEACE (French acronym for Paradigme dEvaluation Automatique de la Comprehension hors et En-contexte).
2:192	This paradigm will be the basis of the French Technolangue MEDIA project, in which dialog systems from various academic and industrial sites will be tested in an evaluation campaign coordinated by ELRA/ELDA (over the next two years).
3:192	Despite previous efforts such as EAGLES,DISC, AUPELF ARCB2 or the ongoing American DARPA COMMUNICATOR project, the spoken dialog community still lacks common reference tasks and widely agreed upon methods for comparing and diagnosing systems and techniques.
4:192	Automatic solutions are nowadays being sought both to make possible the comparison of different approaches by means of reliable indicators with generic evaluation methodologies and also to reduce system development costs.
5:192	However achieving independence from both the dialog system and the task performed seems to be more and more a utopia.
6:192	Most of the evaluations have up to now either tackled the system as a whole, or based the measurements on dialog-context-free information.
7:192	The PEACE proposal aims at bypassing some of these shortcomings by extracting, from real dialog corpora, test sets that synthesize contextual information.
8:192	1 Introduction Generally speaking common reference tasks (Whittaker et al., 2002) and methods to compare and diagnose spoken language dialog systems (SLDS) and spoken dialog techniques are lacking despite previous efforts futher discussed in the next section such as EAGLES, DISC,AUPELF ARCB2 or the ongoing American project DARPA COMMUNICATOR.
9:192	Without an objective assessment of dialog systems, it is difficult to reuse previous work and to advance theories.
10:192	The assessment of a dialog system is complex in part to the high integration factor and tight coupling between the various modules present in any SLDS, for which unfortunately today, no common accepted reference architecture exists.
11:192	Nevertheless, a major problem remains the dynamic nature of dialog.
12:192	Consequently to these shortcomings, researchers are often unable to provide principled design and system capabilities for technology transfer.
13:192	In other research areas, such as speech recognition and information retrieval, common reference tasks have been highly effective in sharing research costs and efforts.
14:192	A similar development is highly needed in the dialog community.
15:192	In this contribution which addresses only a part of the SLDS evaluation problem, a paradigm for evaluating the context-sensitive understanding capability of any spoken language dialog system is proposed.
16:192	PEACE (Devillers et al., 2002a) described in section 3, is based on test sets extracted from real corpora, and has three main aspects: it is generic, contextual and it offers diagnostic capabilities.
17:192	Here genericity is envisaged in a context of information dialogs access.
18:192	The diagnostic aspect is important in order to determine the different qualities of the systems under test.
19:192	The contextual aspect of evaluation is a crucial point since dialog is dynamic by nature.
20:192	We propose to simulate/synthesize the contextual information.
21:192	The PEACE paradigm will be tested in the French Technolangue MEDIA project and will serve as basis in the comparison and diagnostic evaluation of systems presented by various academic and industrial sites (section 4).
22:192	ELRA/ELDA is the coordinator of the larger scope evaluation campaign EVALDA, which includes the MEDIA campaign that began in January 2003.
23:192	2 Overview of SLDS evaluation Without an attempt to be exhaustive, we overview some recent efforts for evaluation of SLDS.
24:192	The objective of the European DISC project was to write the best-practice guidelines for SLDS development and evaluation of its time.
25:192	DISC has collected a systematic list of bottom-up evaluation criteria, each corresponding to a partially ordered list of properties likely to be encountered in any SLDS.
26:192	This properties are positioned on a grid defining an SLDS abstract architecture and relate to various phases of the generic DISC SLDS development life-cycle (Dybkjr and al., 1998).
27:192	They are complemented by a standard evaluation pattern made of 10 generic questions (e.g. Which symptoms need to be observed? ) which has been instantiated for all the evaluation criteria.
28:192	If the DISC results are quite extensive and presented in an homogeneous way, they do not provide a direct answer to the question of SLDS evaluation.
29:192	Its contribution lies more at the specification level.
30:192	Although the approach and the goals of the European EAGLES project were different, one could forward the same remark about the results of the speech evaluation work group (D. Gibbon, 1997).
31:192	In (Fraser, 1998), one find a set of evaluation criteria for voice oriented products and services, organized in four broad categories.: 1) voice command, 2) document generation, 3) phone services 4) other.
32:192	To the best of our knowledge, the MADCOW (Multi Site Data COllection Working group) coordination group set up in the USA by ARPA in the context of the ATIS (Air Travel Information Services) task to collect corpora, was the first to propose a common infrastructure for SLDS automatic evaluation (MADCOW, 1992), which also addressed the problem of language understanding evaluation, based on system answer comparison.
33:192	Unfortunately no direct diagnostic information can be produced, since understanding is appreciated by gauging the distance from the answer to a pair of minimal and a maximal reference answers.
34:192	In ATIS, the protocol was only been applied to context free sentences.
35:192	Up to now it has been one of the most used by the community since it is relatively objective and generic because it relies on counts of explicit information and allows for a certain variation in the answers.
36:192	On the other hand, the method displays a bias toward silence and does not give the means to appreciate error severity.
37:192	In ARISE (Automatic Railway Information Systems for Europe) (Lamel, 1998), a corpus of roughly 10,000 calls has been used in conjunction with user debriefing questionnaire analysis to diagnose different versions of a phone information server.
38:192	The hand-tagging objective measures of the corpus include understanding error counts (glass box methodology).
39:192	Although it provides fine grained diagnostic information, this procedure cannot be easily generalized since it requires handannotated corpus and access to the internal representation of the system.
40:192	Two metrics have been developped at MIT (Glass et al., 2000): the Query Density (QD) and the Concept Efficiency (CE), which measure respectively over the course of a dialogue: the mean number of new concepts introduced per user query, and the number of turns necessary for each concept to be understood by the system.
41:192	Concepts are generated automatically for each utterance with a parsable orthographic transcription as a series of keyword-value pairs.
42:192	The higher the QD, the more effectively a user is able to communicate information to the system.
43:192	The CE is an indicator of recognition or understanding errors; the higher it is, the fewer times a user needs to repeat himself.
44:192	These metrics were evaluated on single systems (JUPITER and and MERCURY); to compare different systems of the same type, one would need a common ontology.
45:192	In (Glass et al., 2000), the authors believe that CE should be related to user frustation, but to show it they would need to use the PARADISE framework.
46:192	PARADISE (Walker et al., 1998) can be seen as a sort of meta-paradigm which correlates objective and subjective measurements.
47:192	Its grounding hypothesis states that the goal of any SLDS is to achieve user-satisfaction, which in turn can be predicted through task success and various interaction costs.
48:192	With the help of the kappa coefficient (Carletta, 1996) proposes to represent the dialog success independently from the task intrinsic complexity, thus opening the way to task generic comparative evaluation.
49:192	PARADISE has been tested in the COMMUNICATOR project (Walker et al., 2001) with 9 systems working on the same task over different databases.
50:192	With four basic measures (e.g. task completion) the protocol has been able to predict 37% of user satisfaction variation, and 42% with the help of a few extra measurements on dialog acts and subtasks.
51:192	One critic, one can make about PARADISE concern its cost (real user tests are costly) and the use of subjective assessment.
52:192	The adaption of the DQR text understanding evaluation methodology (Sabatier et al., 2000) to speech resulted in a generic and qualitative procedure.
53:192	Each element of its test set holds three parts, the Declaration to define the context, a Question which bears on point present in the context and the Response.
54:192	The test set is organized through seven levels of test, from basic explicit understanding to semantic interpretation and reply pertinence assessment.
55:192	This protocol is task and system generic but test set construction is not straightforward and the bias introduced by the wording of the question is difficult to assess.
56:192	Recently the GDR-13 work group of CNRS on spoken dialog understanding, has proposed an evaluation methodology for literal understanding.
57:192	According to (Antoine and al., 2002), DEFI tries to remedy two important weaknesses of the MADCOW methodology, namely the lack of genericity and the lack of diagnostic information, by crafting system specific test sets from a primary set of enunciations representative of the task (provided by the developers).
58:192	Secondary enunciations are then derived from the primary ones in order to exhibit particular language phenomena.
59:192	Afterwards, the systems are evaluated by their developers using specific test set and their own metrics.
60:192	The various results can be mapped over a generic abstract architecture for comparison (although this mapping is still unspecified at the time of writing).
61:192	DEFI has already been used in one evaluation campaign, with 5 systems presented by 4 laboratories.
62:192	(Antoine and al., 2002) has reported the following weaknesses of the protocol: how to control the bias introduced by the derivation of enunciations, how to guaranty that derived enunciation will remain in the task scope (this prevented some system from being evaluated over the complete test set) and finally how to restrict and organize the language phenomena used in the test set.
63:192	3 The PEACE paradigm We first describe the paradigm and relate preliminary experiments with PEACE.
64:192	This paradigm which is as basement for the MEDIA project will be refined by all the partners and use for an evaluation campaign between seven systems of industrial and academic sites.
65:192	3.1 Description The PEACE paradigm relies on the idea that for database querying tasks, it is possible to define a common semantic representation, onto which all the systems are able to convert their own representation (Moore, 1994).
66:192	The paradigm based on data extracted from real corpus, includes both literal and contextual understanding test sets.
67:192	More precisely, it provides: AF the definition of a semantic representation (see 3.1.1), AF the definition of a model for dialogic contexts (see 3.1.2), AF the definition and typology of linguistic phenomena and dialogic functions used to selectively diagnoze the system language capabilities (anaphora resolution, constraints relaxation, etc.)
68:192	(see 3.1.3), AF a data structuring method.
69:192	The format of the annotated data will be adapted to language resource standard annotations implemented (see 3.1.4), AF and evaluation metrics with the corresponding evaluation tool (see 3.1.5).
70:192	3.1.1 Generic semantic representation The difficulty of choosing a semantic representation lies in finding a complete and simple representation of a user utterance meaning in a unified format.
71:192	A frame Attribute Value Representation (AVR) has been chosen, allowing a fast and reliable annotation.
72:192	The values are either numeric units, proper names, or semantic classes, that group together lexical units which are synonyms for the task.
73:192	The order of the (attribute, value) pairs in the semantic representation matches their respective position in the utterance.
74:192	A modal information (positive (+) and negative(-)) is also assigned to each (attribute, value) pair.
75:192	The semantic representation of an utterance consists then in a list of triplets of the form (mode, attribute, normalized value).
76:192	An example is given in figure 1.
77:192	In order to take into account for long-time dependencies or to allow multiple referenced objects, the semantic representation may be enriched by adding a reference value to each triplet for the representation of links between 2 attributes of the utterance.
78:192	Attributes can grouped into different classes: AF the database attributes (the most frequent) correspond to the attributes of the database tables (e.g. categoryfor an hotel); AF the modifier attributes are associated to the database concepts.
79:192	Their values are used to modify the database concept interpretation values (e.g. the attribute category-modifier with possible values: BQ; BO, BP, Max, Min); AF the discursive attributes are introduced to handle various aspects of dialogic interaction User cest pas Paris cest Passy Query it is not Paris it is Passy (LU) AVR (-, place, Paris) (+, place, Passy) Figure 1: Example of a semantic representation of an utterance with positive and negative information for the ARISE task.
80:192	Place is an database attribute,Paris and Passy are values and +/modal markers.
81:192	(e.g. commandwith values cancelation, correction, error specificationBMBMBM,orresponse with values yes or no); AF the argument attribute which represents the topic at the focus of the utterance.
82:192	When dealing with information retrieval applications, defining the database and modifier attributes and the appropriate values can be done in a rather straightforward way.
83:192	Most of those attributes are derived directly from the information stored in the database.
84:192	Furthermore, most of the discursive attributes are domain-independent.
85:192	Some database attributes remain unchanged across many tasks, such as those dealing with dates or prices.
86:192	This semantic representation has been used at LIMSI for PARIS-SITI TASK (touristic information) and ARISE TASK (traintable information) both with triplet representation.
87:192	More recently in the context of the AMITIES project, quadruplets were used.
88:192	3.1.2 Contextual understanding modeling Contextual understanding evaluation provides information about the capability of the system to take into account the dialog history in order to properly interpret the user query.
89:192	Contextual understanding evaluation is rarely performed because of the dynamic nature of the dialog make the dialog context depend on the systems dialog strategy.
90:192	Nevertheless PEACE proposes a systemindependent way to evaluate local contextual interpretation.
91:192	Given CD BD BMBMBMCD D8 the user interactions, and CB BD BMBMBMCB D8 the answers of the agent or system, the context a time D8 is a function CUB4CD BD BNCB BD BNCD BE BNCB BE BNBMBMBMCD D8 BNCB D8 B5.
92:192	In the PEACE paradigm, a paraphrase of the context is derived from the semantic representation (BonneauMaynard et al., 2000).
93:192	The dialog contexts are extracted from real dialogs in three steps.
94:192	First, the internal semantic frames representing the dialog contexts are automatically extracted from the log files of the session recordings.
95:192	Secondly, the semantic frames are converted into AVR format and then handcorrected to faithfully represent the dialog history.
96:192	The last step consists in the writing of a sentence for each context (the context paraphrase), which results in the same AVR representation as the one of the dialog context.
97:192	Two possibilities may be investigated for building the paraphrase from the internal semantic representation of the dialog context.
98:192	A rule-based or template-based natural language generation module can be used to automatically produce the paraphrase.
99:192	The paraphrase can also be obtained by concatenating the sentences preceding the extracted dialog state.
100:192	In both cases, a manual verification is needed.
101:192	3.1.3 A typology of linguistic phenomena and dialogic functions For dialog system evaluation, it is essential to build test sets randomly extracted from real corpus.
102:192	For dialog system diagnosis, it is also crucial to build test sets labeled with the linguistic phenomena and dialogic functions.
103:192	Thus, the capabilities of systems contextual understanding can be assessed for the main linguistic and dialogic difficulties such as, for instance, anaphora or ellipsis resolution.
104:192	3.1.4 A data structuring method Two types of units, one for literal understanding (LU), the other for contextual understanding (CU) are defined.
105:192	The format of the annotated data will be adapted to language resource standard annotations implemented in XML, e.g.
106:192	(Geoffrois et al., 2000), (Ide and Romary, 2002).
107:192	Each unit is extracted from a real dialog corpus.
108:192	LU units are composed of the user query, the corresponding audio signal, an automatic transcription obtained with a recognition system, and finally the literal semantic representation of the utterance (see Figure 1).
109:192	CU units are composed of Context je voudrais un hotel 4 paraphrase etoiles dans le neuvi`eme I would like a 4 category hotel in the ninth (LU) AVR (+, argument, hotel) (+, district, 9) (+, category, 4) User la meme categorie dans query un autre arrondissement the same category in another district (LU) AVR (+, other, district) (+, same, category) (CU) AVR (+, argument, hotel) (-, district, 9) (+, category, 4) Figure 2: Example of a contextual understanding unit composed of a context paraphrase, a user query and the resulting AVR.
110:192	AVR of context paraphrase and user query are given in TYPEWRITING MODE.
111:192	Ellipsis (in the ninth) and anaphora (same category, another district) may be observed.
112:192	the dialog context (given by the paraphrase), the user query and the resulting AVR of the user query in the given context (see Figure 2).
113:192	Those units are also labeled with linguistic and dialogic phenomena.
114:192	3.1.5 Evaluation metrics and scoring tool Common evaluation metrics are essential for analyzing the system capabilities.
115:192	The scoring tool for AVR comparison is able to compare between two AVR frame representation sets.
116:192	For evaluation, system outputs translated in AVR format composed one set, the other one contains the AVR references which are manually annotated.
117:192	Both frame sets have the form of a list of AVRs (fixed length records).
118:192	Each record is composed of three or four fields (mode, attribute, value, reference).
119:192	The comparison consists in applying a set of predefined operators each assigned with a cost value.
120:192	The comparison process looks for operator lists to be applied to the test frame in order to obtain the reference frame that minimizes the final cost value.
121:192	For a global evaluation, the classical operators from speech evaluation (DELetion, INSertion and SUBstitution) may be used (as used for first two values of Accuracy percentage in Table 1).
122:192	With our scoring tool the definition of new operators is quite easy.
123:192	It is then also possible to distinguish between different types of errors by defining specific operators (as used to estimate Topic identification in Table 1), or by using different cost values (for example a substitution is often considered more costly for dialog management).
124:192	3.2 Example use of PEACE In order to validate the evaluation paradigm, a set of approximatively 1,700 literal units and a set of 100 contextual units has been used for the PARIS-SITI task (Bonneau-Maynard and Devillers, 2000).
125:192	Results for both literal and contextual understanding test sets are given in Table 1.
126:192	In order to observe the ability of the systems to deal with recognition errors, each literal understanding unit also contains the ASR transcription of the original user utterance.
127:192	The various measures of understanding accuracy are computed as the ratio between the sum of the number of deleted, inserted and substituted attributes, and the total number of AVR attributes in the test set.
128:192	The possibility of an automatic evaluation of the LU accuracy and the ability of the scoring tool to point out the errors allowed us to easily improve the literal understanding accuracy from 89.0% to 93.5%.
129:192	Due to a 26.5% ASR error rate, the LU accuracy goes down from 93.5% to 72% after ASR transcription.
130:192	The contextual understanding accuracy on the 100 test units is 82.6% on exact transcription.
131:192	For instance, anaphoric references are relatively well solved, with 80.4% accuracy on the 50 units containing at least one anaphoric reference.
132:192	For each example, the anaphoric referenced object is generally correctly identified and remaining errors are often due to a bad history constraint management.
133:192	3.3 Discussing the PEACE paradigm The PEACE paradigm enables automatic evaluation of literal and contextual dialog understanding.
134:192	The evaluation paradigm makes the distinction between different types of errors, allowing a qualitative and diagnostic analysis of the performances of a speech understanding module.
135:192	Very few evaluation paradigms propose automatic diagnosis of contextual interpretation (Glass et al., 2000).
136:192	The proposed methodology is based on #Units #Attr.
137:192	%Acc.
138:192	Prec.
139:192	LU exact 1 681 3 991 93.5% 0.7 LU ASR . 1 681 3 991 72.0% 1.4 Topic id. 680 833 94.3% 1.6 Modifier id. 323 445 95.7% 1.9 CU exact 100 430 86.8% 3.2 Anaphoric 50 245 84.4% 4.5 resolution Ellipsis 25 106 85.3% 6.7 resolution Table 1: Literal understanding (LU) accuracy on both exact and ASR transcription, and contextual understanding (CU) accuracy.
140:192	Second column indicates the number of units included in the test set (i.e # of user utterances), third column gives the total number of attributes in the correct AVR test sets.
141:192	Details, using specific operators, are given for argument (topic) and modifier identification for LU on exact transcription, and for anaphoric reference and ellipsis resolution for CU.
142:192	Last column gives the 95% precision of the accuracy estimation (Montacie and Chollet, 1997) . semi-automatically built reference test sets, and therefore is much more time effective than manual evaluation.
143:192	Furthermore, it provides reproducible tests.
144:192	Although the semantic representation is task dependent, the example described above shows the feasibility of the paradigm for any dialog system interfacing to a database.
145:192	Robustness to many linguistic phenomena such as repetitions, hesitations or auto-corrections may be evaluated with this method.
146:192	XML coding will facilitate the genericity and the reusability of the test sets, by allowing the selection of the dialogic contexts to be studied.
147:192	The representation of the dialog context with a single paraphrase, derived from a flat structured AVR, may have some limitations in case of longtime dialog dependencies.
148:192	It does not allow for memorizing all the steps of the dialog.
149:192	For example, if the speaker says first I would like a 2 star hotel, then no I prefer 3 stars and finally says give me again my first choice, the CU unit cannot take into account this succession of queries.
150:192	However, this kind of interaction is rarely observed in dialogue corpora: the user usually repeats the constraint value (give me again a 2 star hotel).
151:192	To represent more precisely the dialog state, the representation of the dialog context should incorporate some meta-information inspired for example from the DAMSL annotation standard 1 (Devillers et al., 2002b).
152:192	Another point is the representativity of the test sets.
153:192	This may be considered as a limitation as far as PEACE paradigm is built on the idea that the test units are extracted from real dialogs.
154:192	Obviously, the larger the test sets are, the better.
155:192	A diagnostic evaluation may need a very large test corpora to validate system performance against the wide range of phenomena present in spontaneous dialog.
156:192	The ability to automatically diagnose the performances of contextual understanding modules on local difficulties such as ellipsis, negations, anaphoric reference or constraint relaxation is one of the major advantages of the PEACE paradigm, which has not been investigated by other methodologies.
157:192	This is why it has been chosen for the MEDIA project described in the next section.
158:192	4 The MEDIA project The MEDIA project proposes a paradigm based on a reference task and on test sets extracted from real corpora for evaluating literal and contextual understanding in dialog systems.
159:192	The PEACE paradigm will serve as basis for the MEDIA project.
160:192	The consortium is composed of IRIT, LIA, LIMSI, LORIA, VALORIA for the French academic sites and France Telecom R&D and TELIP for the industrial sites.
161:192	The scientific committee contains representatives of AT&T (USA), Tilburg University (Netherlands), IBM, IMAG, LIUM and VECSYS (France).
162:192	The project has four main parts.
163:192	First, the selection of reference task such as for example a task of web-based travel agency.
164:192	The reference task has to correspond to a real-life application allowing real user tests.
165:192	Secondly, multi-level representation such as the semantic representation, the typology of linguistic phenomena and dialogic functions, the dialog context model will be commonly refined and adapted to the reference task.
166:192	The third part deals with the recording and labeling of a dialog corpus which will be used for 1 http://www.cs.rochester.edu/research/trains/annotation both system adaptation and test set selection.
167:192	The last part is the organisation of the evaluation campaigns by ELRA/ELDA for the participating sites.
168:192	ELRA/ELDA is the coordinator of a larger scope project: EVALDA which includes among others, the MEDIA project.
169:192	ELDA with VECSYS will provide transcribed and annotated corpora and evaluation tools according to consortium specifications.
170:192	The recording of 1200 French dialogs (240 speakers, 5 dialogs each, 15k user queries) is planned.
171:192	Three sets of LU and CU units will be built from this corpus.
172:192	A large size adaptation set will be used by the participants to adapt their system to the task and the semantic representation.
173:192	The development set (around 1K LU (resp.
174:192	CU) units) will be used to validate the evaluation protocole.
175:192	The size of the test set is planned to be around 3K LU (resp.
176:192	CU) units.
177:192	Various approaches are currently used at the participating sites; stochastic or syntactic and semantic rule-based modeling.
178:192	The project started in January 2003 and will last two years.
179:192	5 Conclusion Assessing the dialog system understanding capabilities requires to evaluate the transition between successive states of the dialog.
180:192	At least, we must be able to test a sequence of two states at any point in the dialog.
181:192	The dynamic and interactive nature of the dialog makes construction and reuse of test sets difficult.
182:192	Furthermore, to evaluate one particular dialog transition, the system has to be put in a particular state corresponding to the original dialog context.
183:192	The variable describing the dialog state can be composed of complex information such as the current semantic frame (list of triplets (mode,attribute,value) or quadruples (mode, attribute, value, reference)), the dialog history semantic frame and potentially other information like recognition scores, dialog acts, etc. The PEACE paradigm allows the evaluation of two successive simplified dialog states.
184:192	It has been successfully tested with test samples focusing on linguistic difficulties of literal and contextual understanding.
185:192	For these tests, the dialog state is the dialog history semantic frame.
186:192	The contextual understanding modeling in PEACE is system independent since the context is given by a paraphrase of queries.
187:192	PEACE allows a diagnostic evaluation of specific semantic attributes and particular linguistic phenomena.
188:192	In our opinion, it is crucial for the dialog community to agree on a common reference task and reference test sets in order to be able to compare and diagnose dialog systems.
189:192	Both evaluation with real users and artificial simulation of successive dialog states using test sets extracted from real corpora have to be carried out in parallel.
190:192	The use of test sets reduces the global cost of dialog system evaluation, moreover such tests are reproducible.
191:192	The PEACE protocol will be used as basis for the French Technolangue MEDIA project in a two year evaluation campaign where dialog systems from both academia and industry will be evaluated.
192:192	In other domains, it could be related with (Hirschman, 2000) propositions for Question Answering evaluation.

