Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 170–179,
Portland, Oregon, 24 June 2011. c©2011 Association for Computational Linguistics
Bilingual Random Walk Models for Automated Grammar Correction of
ESL Author-Produced Text
Randy West and Y. Albert Park
Department of Computer Science & Engineering
University of California, San Diego
La Jolla, CA 92093-0533
nullrdwest,yaparknull@cs.ucsd.edu
Roger Levy
Department of Linguistics
University of California, San Diego
La Jolla, CA 92093-0533
rlevy@ucsd.edu
Abstract
We present a novel noisy channel model for
correcting text produced by English as a sec-
ond language (ESL) authors. We model the
English word choices made by ESL authors as
a random walk across an undirected bipartite
dictionary graph composed of edges between
English words and associated words in an au-
thor’s native language. We present two such
models, using cascades of weighted finite-
state transducers (wFSTs) to model language
model priors, random walk-induced noise, and
observed sentences, and expectation maxi-
mization (EM) to learn model parameters af-
ter Park and Levy (2011). We show that such
models can make intelligent word substitu-
tions to improve grammaticality in an unsu-
pervised setting.
1 Introduction
How do language learners make word choices as
they compose text in a language in which they are
not fluent? Anyone who has attempted to learn a for-
eign language can attest to spending a great deal of
time leafing through the pages of a bilingual dictio-
nary. However, dictionaries, especially those with-
out a wealth of example sentences or accompany-
ing word sense information, can often lead even the
most scrupulous of language learners in the wrong
direction. Consider an example: the English noun
“head” has several senses, e.g. the physical head and
the head of an organization. However, the Japanese
atama can only mean the physical head or mind, and
likewise shuchou, meaning “chief,” can only map to
the second sense of head. A native English speaker
and Japanese learner faced with the choice of these
two words and no additional explanation of which
Japanese word corresponds to which sense is liable
to make a mistake on the flip of a coin.
One could of course conceive of more subtle ex-
amples where the semantics of a set of choices are
not so blatantly orthogonal. “Complete” and “en-
tire” are synonyms, but they are not necessarily in-
terchangeable. “Complete stranger” is a common
two-word phrase, but “entire stranger” sounds com-
pletely strange, if not entirely ungrammatical, to the
native English speaker, who will correct “entire”
to “complete” in a surprisingly automatic fashion.
Thus, correct word choice in non-native language
production is essential not only to the preservation
of intended meaning, but also to fluent expression of
the correct meaning.
The development of software to correct ESL text
is valuable for both learning and communication.
A language learner provided instant grammatical-
ity feedback during self-study is less likely to fall
into patterns of misuse, and the comprehension diffi-
culties one may encounter when corresponding with
non-native speakers would be ameliorated by an au-
tomated system to improve text fluency. Addition-
ally, since machine-translated text is often ungram-
matical, automated grammar correction algorithms
can be deployed as part of a machine translation sys-
tem to improve the quality of output.
We propose that word choice production errors
on the part of the language learner can be mod-
eled as follows. Given an observed word and an
undirected bipartite graph with nodes representing
170
words in one of two languages, i.e. English and the
sentence author’s native tongue, and edges between
words in each language and their dictionary trans-
lation in the other (see Figure 1 for an example),
there exists some function null null [0null1] that defines
the parameters of a random walk along graph edges,
conditioned on the source word. By composing this
graph with a language model prior such as an null-
gram model or probabilistic context-free grammar,
we can “correct” an observed sentence by inferring
the most likely unobserved sentence from which it
originated.
More concretely, given that we know null, we can
compute argmaxwnull null(wnullnullwnullnullnullnull), where w is the
observed sentence, null is the language model, and wnull
is the “corrected,” unobserved sentence. Under this
view, some wnull drawn from the distribution null is sub-
jected to some noise process null, which perturbs the
sentence author’s intended meaning and outputs w.
We perform this computation in the standard way
from the statistical machine translation (SMT) liter-
ature (Brown et al., 1993), namely by using Bayes’
theorem to write
null(wnullnullwnullnullnullnull) = null(w
nullnullnull)null(wnullwnullnullnullnullnull)
null(wnullnull)
Since the denominator of the RHS is independent of
wnull, we can rewrite our argmax as
argmax
wnull
null(wnullnullnull)null(wnullwnullnullnullnullnull)
We have now decomposed our original equation into
two manageable parts, a prior belief about the gram-
maticality of an unobserved sentence wnull, which we
can compute using a language model null learned sepa-
rately using standard supervised techniques (in par-
ticular, null-gram estimation), and the probability of
the observed sentence w given wnull, null, and null. To-
gether, these constitute a noisy channel model from
information theory (Shannon, 1948). All that re-
mains is to learn an appropriate null, for which we will
employ unsupervised methods, namely expectation
maximization.
The rest of this paper is organized as follows. In
Section 2, we will discuss related work. In Section
3, we will present the implementation, methodology
and results of two experiments with different null. In
Section 4, we will discuss our experimental results,
and we will conclude in Section 5.
2 Related
Work
The literature on automated grammar correction
is mostly focused on rule-based methods and er-
ror identification rather than correction. However,
there has been a recent outgrowth in the applica-
tion of machine translation (MT) techniques to ad-
dress the problem of single-language grammar cor-
rection. Park and Levy (2011) propose a noisy chan-
nel model for learning to correct various types of er-
rors, including article and preposition errors, word-
form errors, and spelling mistakes, to which this pa-
per is an extension. As the present work builds on
Park and Levy’s basic model, we will reserve a more
detailed discussion of their work for Section 3.
Brockett et al. (2006) use phrasal SMT techniques
to identify and correct mass noun errors of ESL stu-
dents with some success, but they correct no other
production error classes to our knowledge.
Lee and Seneff (2006) learn a method to aid ESL
students in language acquisition by reducing sen-
tences to their canonical form, i.e. a lemmatized
form devoid of articles, prepositions, and auxil-
iaries, and then building an over-specified lattice by
reinserting all word inflections and removed word
classes. They then score this lattice using a trigram
model and PCFG. While this method has many ad-
vantages, it does not take into account the full con-
text of the original sentence.
Kok and Brockett (2010) use random walks over
biand multilingual graphs generated by aligning
English sentences with translations in 10 other Eu-
ropean languages to learn paraphrases, which they
then evaluate in the context of the original sentence.
While their approach shares many high-level simi-
larities with ours, both their task, paraphrasing cor-
rect sentences, and the details of their methodology
are divergent from the present work.
D´esilets and Hermet (2009) employ round-trip
machine translation from L1 to L2 and back again
to correct second language learner text by keep-
ing track of the word alignments between transla-
tions. They operate on a very similar hypothesis
to that of this work, namely that language learners
make overly-literal translations when the produce
text in their second language. However, they go
about correcting these errors in a very different way
than the present work, which is novel to the best of
171
Figure 1: Example English-Korean dictionary graph for a subset of the edges out of the English head, leader, and
chief.
nullnullnullnull
nullnullnullnullnullnullnull nullnullnullnullnullnull nullnullnullnullnullnull
nullnullnullnullnullnullnullnullnullnullnull
our knowledge, and their technique of using error-
annotated sentences for evaluation makes a compar-
ison difficult.
3 Model
Implementation and Experiments
We present the results of two experiments with dif-
ferent random walk parametrizations. We begin by
describing our dataset, then proceed to an overview
of our model and experimental procedures, and fi-
nally detail the experiments themselves.
3.1 Dataset
We use the dataset of Park and Levy (2011), a col-
lection of approximately 25,000 essays comprised of
478,350 sentences scraped from web postings made
by Korean ESL students studying for the Test of En-
glish as a Foreign Language (TOEFL). Of these, we
randomly select 10,000 sentences for training, 504
as a development set, and 1017 held out for final
model evaluation.
Our English-Korean dictionary is scraped from
http://endic2009.naver.com, a widely-
used and trusted online dictionary source in South
Korea. We are unfortunately unaware of any freely
available, downloadable English-Korean dictionary
databases.
3.2 Model
and Experimental Procedures
3.2.1 Overview
The bulk of our experimental methodology and
machinery is borrowed from Park and Levy (2011),
so we will summarize that portion of it only briefly
here. At a high level, there are three major compo-
nents to the model of a sentence: a language prior,
a noise model, and an observed sentence. Each
of these is implemented as a wFST and composed
together into a single transducer whose accepting
paths represent all possibilities of transducing from
an (unobserved) input sentence to the (observed)
output sentence, with the path weight being associ-
ated probability. See Figure 2 for an example.
3.2.2 Language
Model
For our language model, we use a Kneser-Ney
smoothed trigram model learned from a version
of the British National Corpus modified to use
Americanized spellings (Chen and Goodman, 1996;
Burnard, 1995). The implementation of an null-gram
model as a wFST requires that each state represent a
context, and so one must necessarily instantiate arcs
for all words in the alphabet from each state. In order
to reduce model size and minimize memory usage, it
is standard practice to remove relatively uninforma-
tive higher-order null-grams from the model, but under
the wFST regime one cannot, for example, remove
some trigrams from a bigram context without re-
moving all of them. Instead, we retain only the 1,000
most informative bigram contexts, as measured by
the Kullback-Leibler divergence between each bi-
gram context and its unigram counterpart. This is
in contrast to standard cutoff models, which remove
null-grams occurring less than some cutoff number of
times in the corpus.
3.2.3 Noise
Models
The structure of the noise wFST differs for each
noise model; for our model of word-choice error, we
can use a single initial/final state with arcs labeled
with unobserved words as input, observed words as
output, and a weight defined by the function null that
governs the parameters of a random walk across our
dictionary graph (again, see Figure 2 for an exam-
ple). We will reserve the definition of null, which is
172

