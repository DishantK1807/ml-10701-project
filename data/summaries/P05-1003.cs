Malouf (2002) and Sha and Pereira (2003) show that gradient-based algorithms, particularly limited memory variable metric (LMVM), require much less time to reach convergence, for some NLP tasks, than the iterative scaling methods (Della Pietra et al., 1997) previously used for log-linear optimisation problems.
N03-1028 W02-2018
Usually these experts are designed to 4 See (Cohn et al., 2005) for a scaling method allowing the full POS tagging task with CRFs.
P05-1002
In order to make training time manageable4, we collapse the number of POS tags from 48 to 5 following the procedure used in (McCallum et al., 2003).
W03-0430
Comparison with previous best results: KM01 (Kudoh and Matsumoto, 2001), CM03 (Carreras and Marquez, 2003), SP03 (Sha and Pereira, 2003), ZDJ02 (Zhang et al., 2002).
N03-1028
These are based on those found in (Curran and Clark, 2003).
W03-0424
In recent empirical studies on maximum entropy models and CRFs, limited memory variable metric (LMVM) has proven to be the most efficient method (Malouf, 2002; Wallach, 2002); accordingly, we have used LMVM for CRF estimation.
W02-2018
Recently, there have been a number of sophisticated approaches to reducing overfitting in CRFs, including automatic feature induction (McCallum, 2003) and a full Bayesian approach to training and inference (Qi et al., 2005).
W03-0430
Those results which are significantly better than the corresponding multiclass MLE or regularised model are flagged with a, and those which are significantly worse with a . These results show that error-correcting CRF training achieves quite similar performance to the multiclass CRF on the task (which incidentally exceeds McCallum (2003)'s result of 89.0 using feature induction).
W03-0430
All our results for NER are reported on the CoNLL-2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003).
W03-0419
For our experiments we use the CoNLL-2000 shared task dataset (Tjong Kim Sang and Buchholz, 2000).
W00-0726
McCallum (2003) describes a technique for greedily adding those feature conjuncts to a CRF which significantly improve the model's log-likelihood.
W03-0430
CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003), simplified part-of-speech (POS) tagging (Lafferty et al., 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of tabular data (Pinto et al., 2003), among other tasks.
N03-1028 W03-0430
entity recognition CRFs have been used with strong results on the CoNLL 2003 NER task (McCallum, 2003) and thus this task is included here as a benchmark.
W03-0430
The concept of combining the distributions of a set of expert models via a weighted product has previously been used in a range of different application areas, including economics and management science (Bordley, 1982), and NLP (Osborne and Baldridge, 2004).
N04-1012
Vijay-Shanker. 2003.
N03-1028
In recent years, conditional random fields (CRFs) (Lafferty et al., 2001) have shown success on a number of natural language processing (NLP) tasks, including shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003) and information extraction from research papers (Peng and McCallum, 2004).
N03-1028 N04-1042 W03-0430
