(Radev et al., 2000; Ng et al., 2001; Harabagiu et al., 2003).
A00-1021 W01-0509
W S J 8 80923 0163 DocumentIDs Un i g r a m  C o o cc u rre n c e  Sc o r e s B1 MaxH T CMPRH1 CMPRH2 CMPRH3 FT Linear(B1) Linear(MaxH) Linear(T) Linear(FT) Figure9.DUC2001singledocinter-human,baseline,andsystemunigramco-occurrencescoreversus compressionratio.DocumentIDsaresortedbyincreasingcompressionratioCMPRH1.  much.Theunigramco-occurrencescoresforB1,T,and MaxHwerenoisybuthad ageneraltrend(Linear B1, Linear T, and Linear MaxH) of drifting into lower performance when compression ratio increased (i.e. when summaries became shorter); while the performance of FT did not exhibit a similar trend. This confirms our earlier hypothesis that humans are less likely to agree at high compression ratio and system performancewillalsosufferathighcompressionratio. TheconstancyofFTacrossdifferentcompressionratios is reasonable since FT scores should only depend on how well the unigram co-occurrence scoring method capturescontentoverlapbetweenafulltextanditsreference summaries and how likely humans use vocabularyoutsidetheoriginaldocument. 6 Conclusions In this paper we presented an empirical study of the potential and limitations of sentence extraction as a method of automatic text summarization. We showed thefollowing: (1) Howtouseoracleextractstoestimatetheperformance upper bound of sentence extraction methodsatdifferentextractlengths.Weunderstandthatsummariesoptimizedusingunigram co-occurrence score do not guarantee good quality in terms of coherence, cohesion, and overallorganization.However,wewouldargue thatagoodsummarydoesrequiregoodcontent andwewillleavehowtomakethecontentcohesive, coherent, and organized to future research. (2) Inter-humanagreementvariedalotandthedifferencebetweenmaximumagreement(MaxH) and minimum agreement (MinH) was about 18%ontheDUC2001data.Tominimizethe gap,weneedtodefinethesummarizationtask better. This has been addressed by providing guided summarization tasks in DUC 2003 (DUC 2002). We guesstimate the gap should besmallerinDUC2003data. (3) State-of-the-artsystemsperformedatthesame levelasthebaselinesystembutwerestillabout 10% away from the average human performance. (4) The potential performance gains (15% from E100 to E150 and 24% to FT) estimated by oracleextractsofdifferentsizesindicatedthat sentence compression or sub-sentence extractionarepromisingfuturedirections. (5) Therelativeperformanceofhumansandoracle extracts at three inter-human agreement intervalsshowedthatitwasonlymeaningfultooptimize sentence extraction algorithms if interhuman agreement was high. Although overall highinter-humanagreementwaslowbutsubsets of high inter-human agreement did exist. For example, about human achieved at least 60% agreement in 59 out of 303 (~19%) documentsof30sentencesorless. (6) We also studied how compression ratio affectedinter-humanagreementandsystemperformance, and the results supported our hypothesis that humans tend to agree less at high compression ratio, and similar between humansandsystems.Howtotakeintoaccount thisfactorinfuturesummarizationevaluations isaninterestingtopictopursuefurther. Usingexhaustivesearchtoidentifyoracleextractionhas beenstudiedbyotherresearchersbutindifferentcontexts.Marcu(1999a)suggestedusingexhaustivesearch tocreatetrainingextractsfromabstracts.Donawayetal. (2000)usedexhaustivesearchtogenerateallthreesentencesextractstoevaluatedifferentevaluationmetrics. Themaindifferencebetweentheirworkandoursisthat we searched for extracts of a fixed number of words whiletheylookedforextractsofafixednumberofsentences. Inthefuture,wewouldliketoapplyasimilarmethodologytodifferenttextunits,forexample,sub-sentence unitssuchaselementarydiscourseunit(Marcu1999b). We wanttostudyhowtoconstrainthesummarization task to achieve higher inter-human agreement, train sentence extraction algorithms using oracle extracts at different compression sizes, and explore compression techniquestogobeyondsimplesentenceextraction. References Donaway, R.L., Drummey, K.W., and Mather, L.A. 2000. A Comparison of Rankings Produced by Summarization Evaluation Measures. In Proceeding oftheWorkshoponAutomaticSummarization,postconferenceworkshopofANLP-NAACL-2000,Seattle,WA,USA,6978. DUC.2002.TheDocumentUnderstandingConference. http://duc.nist.gov. Edmundson, H.P. 1969. New Methods in Automatic Abstracting.JournaloftheAssociationforComputingMachinery.16(2). Goldstein, J., M. Kantrowitz, V. Mittal, and J. Carbonell. 1999. Summarizing Text Documents: Sentence Selection and Evaluation Metrics. In Proceedingsofthe22ndInternationalACMConference on Research and Development in Information Retrieval(SIGIR-99),Berkeley,CA,USA,121128. Hovy,E.andC.-Y.Lin.1999.AutomaticTextSummarization in SUMMARIST. In I. Mani and M. Maybury (eds), Advances in Automatic Text Summarization,8194.MITPress. Kupiec,J.,J.Pederson,andF.Chen.1995.ATrainable Document Summarizer. In Proceedings of the 18th InternationalACMConferenceonResearchandDevelopmentinInformationRetrieval(SIGIR-95),Seattle,WA,USA,6873. Lin,C.-Y.and E.Hovy.2002. Manualand Automatic Evaluations of Summaries. In Proceedings of the Workshop on Automatic Summarization, postconference workshop of ACL-2002, pp. 45-51, Philadelphia,PA,2002. Lin,C.-Y.andE.H.Hovy.2003.AutomaticEvaluation of Summaries Using N-gram Co-occurrence Statistics. In Proceedings of the 2003 Human Language Technology Conference (HLT-NAACL 2003), Edmonton,Canada,May27June1,2003. Luhn,H.P.1969.TheAutomaticCreationofLiterature Abstracts. IBM Journal of Research and Development.2(2),1969. Marcu,D.1999a.Theautomaticconstructionoflargescale corpora for summarization research. Proceedings of the 22nd International ACM Conference on ResearchandDevelopmentinInformationRetrieval (SIGIR-99),Berkeley,CA,USA,137144. Marcu,D.1999b.Discoursetreesaregoodindicatorsof importanceintext.InI.ManiandM.Maybury(eds), Advances in Automatic Text Summarization, 123 136.MITPress. McKeown, K., R. Barzilay, D. Evans, V. Hatzivassiloglou, J. L. Klavans, A. Nenkova, C. Sable, B. Schiffman,S.Sigelman.2002.TrackingandSummarizingNewsonaDailyBasiswithColumbiasNewsblaster. In Proceedings of Human Language Technology Conference 2002 (HLT 2002). San Diego,CA,USA. NIST.2002.AutomaticEvaluationofMachineTranslationQualityusingN-gramCo-OccurrenceStatistics. Over, P. and W. Liggett. 2002. Introduction to DUC2002:anIntrinsicEvaluationofGenericNewsText Summarization Systems. In Proceedings of Workshop on Automatic Summarization (DUC 2002), Philadelphia,PA,USA. http://www-nlpir.nist.gov/projects/duc/pubs/ 2002slides/overview.02.pdf Papineni, K., S. Roukos, T. Ward, W.-J. Zhu. 2001. Bleu:aMethodforAutomaticEvaluationofMachine Translation. IBM Research Report RC22176 (W0109-022). Radev, D.R. and K.R. McKeown. 1998. Generating NaturalLanguageSummariesfromMultipleOn-line Sources.ComputationalLinguistics,24(3):469500. Strzalkowski,T,G.Stein,J.Wang,andB,Wise.ARobustPracticalTextSummarizer.1999.InI.Maniand M. Maybury (eds), Advances in Automatic Text Summarization,137154.MITPress. White, M., T. Korelsky, C. Cardie, V. Ng, D. Pierce, andK.Wagstaff.2001.MultidocumentSummarization via Information Extraction. In Proceedings of Human Language Technology Conference 2001 (HLT2001),SanDiego,CA,USA.
W01-0509
