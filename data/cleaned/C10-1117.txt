Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1038–1046,
Beijing, August 2010
Modeling Socio-Cultural Phenomena in Discourse 
Tomek Strzalkowski
1,2, George Aaron Broadwell
1, Jennifer Stromer-Galley
1, 
Samira Shaikh
1, Sarah Taylor
3
 and Nick Webb
1
 
1
ILS Institute, University at Albany, SUNY 
2
IPI, Polish Academy of Sciences 
3
Lockheed Martin Corporation 
tomek@albany.edu 
 
Abstract 
In this paper, we describe a novel ap-
proach to computational modeling 
and understanding of social and cul-
tural phenomena in multi-party dia-
logues. We developed a two-tier ap-
proach in which we first detect and 
clasify certain social language uses, 
including topic control, disagrement, 
and involvement, that serve as first 
order models from which presence the 
higher level social constructs such as 
leadership, may be infered. 
1. Introduction 
We investigate the language dynamics in 
smal group interactions acros various set-
tings. Our focus in this paper is on English 
online chat conversations; however, the mod-
els we are developing are more universal and 
aplicable to other conversational situations: 
informal face-to-face interactions, formal 
metings, moderated discusions, as wel as 
interactions conducted in languages other 
than English, e.g., Urdu and Mandarin.  
Multi-party online conversations are particu-
larly interesting because they become a per-
vasive form of comunication within virtual 
comunities, ubiquitous acros al age groups. 
In particular, a great amount of comunica-
tion online ocurs in virtual chat-roms, typi-
caly conducted using a highly informal text 
dialect. At the same time, the reduced-cue 
environment of online interaction necesitates 
more explicit linguistic devices to convey 
social and cultural nuances than is typical in 
face-to-face or even voice conversations. 
Our objective is to develop computational 
models of how certain social phenomena such 
as leadership, power, and conflict are signaled 
and reflected in language through the choice 
of lexical, syntactic, semantic and conversa-
tional forms by discourse participants. In this 
paper we report the results of an initial phase 
of our work during which we constructed a 
prototype system caled DSARMD-1 (De-
tecting Social Actions and Roles in Multi-
party Dialogue). Given a representative seg-
ment of multiparty task-oriented dialogue, 
DSARMD-1 automaticaly clasifies al dis-
course participants by the degre to which 
they deploy selected social language uses, 
such as topic control, task control, involve-
ment, and disagrement. These are the 
mid-level social phenomena, which are de-
ployed by discourse participants in order to 
achieve or asert higher-level social con-
structs, including leadership. In this work we 
adopted a two-tier empirical aproach where 
social language uses are modeled through 
observable linguistic features that can be 
automaticaly extracted from dialogue. The 
high-level social constructs are then infered 
from a combination of language uses atrib-
uted to each discourse participant; for exam-
ple, a high degre of influence and a high de-
gre of involvement by the same person may 
indicate a leadership role. In this paper we 
limit our discusion to the first tier only: how 
to efectively model and clasify social lan-
guage uses in multi-party dialogue. 
2. Related Research 
Isues related to linguistic manifestation of 
social phenomena have not ben systemati-
caly researched before in computational lin-
guistics; inded, most of the efort thus far 
was directed towards the comunicative di-
mension of discourse. While the Spech Acts 
theory (Austin, 1962; Searle, 1969) provides 
a generalized framework for multiple levels 
of discourse analysis (locution, ilocution and 
perlocution), most curent aproaches to dia-
logue focus on information content and 
structural components (Blaylock, 202; Car-
bery & Lambert, 199; Stolcke, et al., 200) 
in dialogue; few take into acount the efects 
that spech acts may have upon the social 
1038
roles of discourse participants. Also relevant 
is research on modeling sequences of dia-
logue acts – to predict the next one (Samuel et 
al. 198; Ji & Bilmes, 206 inter alia) – or to 
map them onto subsequences or “dialogue 
games” (Carlson 1983; Levin et al., 198), 
which are atempts to formalize participants’ 
roles in conversation (e.g., Linel, 190; Poe-
sio & Mikhev, 198; Field et al., 208). 
There is a body of literature in anthropology, 
linguistics, sociology, and comunication on 
the relationship betwen language and power, 
as wel as other social phenomena, e.g., con-
flict, leadership; however, existing ap-
proaches typicaly lok at language use in 
situations where the social relationships are 
known, rather than using language predic-
tively. For example, conversational analysis 
(Sacks et al., 1974) is concerned with the 
structure of interaction: turn-taking, when 
interuptions ocur, how repairs are signaled, 
but not what they reveal about the speakers. 
Research in anthropology and comunication 
has concentrated on how certain social norms 
and behaviors may be reflected in language 
(e.g., Scolon and Scolon, 201; Agar, 194) 
with few systematic studies atempting to ex-
plore the reverse, i.e., what the linguistic 
phenomena tel us about social norms and 
behaviors. 
3. Data & Annotation 
Our initial focus has ben on on-line chat 
dialogues. While chat data is plentiful on-line, 
its adaptation for research purposes presents a 
number of chalenges that include users’ pri-
vacy isues on the one hand, and their com-
plete anonymity on the other. Furthermore, 
most data that may be obtained from public 
chat-roms is of limited value for the type of 
modeling tasks we are interested in due to its 
high-level of noise, lack of focus, and rapidly 
shifting, chaotic nature, which makes any 
longitudinal studies virtualy imposible. To 
derive complex models of conversational be-
havior, we ned the interaction to be reasona-
bly focused on a task and/or social objectives 
within a group. 
Few data colections exist covering multiparty 
dialogue, and even fewer with on-line chat. 
Moreover, the few colections that exist were 
built primarily for the purpose of training 
dialogue act taging and similar linguistic 
phenomena; few if any of these corpora are 
suitable for deriving pragmatic models of 
conversation, including socio-linguistic phe-
nomena. Existing resources include a 
multi-person meting corpus ICSI-MRDA 
and the AMI Meting Corpus (Carleta, 207), 
which contains 10 hours of metings cap-
tured using synchronized recording devices. 
Stil, al of these resources lok at spoken 
language rather than on-line chat. There is a 
paralel interest in the online chat environ-
ment, although the development of useful re-
sources has progresed les. Some corpora 
exist such as the NPS Internet chat corpus 
(Forsyth and Martel, 207), which has ben 
hand-anonymized and labeled with 
part-of-spech tags and dialogue act labels. 
The StrikeCom corpus (Twitchel et al., 207) 
consists of 32 multi-person chat dialogues 
betwen players of a strategic game, where in 
50% of the dialogues one participant has ben 
asked to behave ‘deceptively’. 
It is thus more typical that those interested in 
the study of Internet chat compile their own 
corpus on an as neded basis, e.g., Wu et al. 
(202), Khan et al. (202), Kim et al. (207). 
Driven by the ned to obtain a suitable dataset 
we designed a series of experiments in which 
recruited subjects were invited to participate 
in a series of on-line chat sesions in a spe-
cialy designed secure chat-rom. The ex-
periments were carefuly designed around 
topics, tasks, and games for the participants to 
engage in so that apropriate types of behav-
ior, e.g., disagrement, power play, persuasion, 
etc. may emerge spontaneously. These ex-
periments and the resulting corpus have ben 
described elsewhere (Shaikh et al., 2010b), 
and we refer the reader to this source. Ulti-
mately a corpus of 50 hours of English chat 
dialogue was colected comprising more than 
20,00 turns and 120,00 words. In adition 
we also asembled a corpus of 20 hours of 
Urdu chat. 
A subset of English language dataset has ben 
anotated at four levels: comunication links, 
dialogue acts, local topics and meso-topics 
(which are esentialy the most persistent lo-
cal topics). Although ful details of these an-
notations are imposible to explain within the 
scope of this article, we briefly describe them 
below. Anotated datasets were used to de-
velop and train automatic modules that detect 
and clasify social uses of language in dis-
course. It is important to note that the anota-
1039
tion has ben developed to suport the objec-
tives of our project and does not necesarily 
conform to other similar anotation systems 
used in the past. 
• Comunicative links. In a multi-party dia-
logue an uterance may be directed towards 
a specific participant, a subgroup of par-
ticipants or to everyone. 
• Dialogue Acts. We developed a hierarchy 
of 15 dialogue acts for anotating the func-
tional aspect of the uterance in discusion. 
The tagset we adopted is based on DAMSL 
(Alen & Core, 197) and SWBD (Jurafsky 
et al., 197), but compresed to 15 tags 
tuned significantly towards dialogue prag-
matics and away from more surface char-
acteristics of uterances (Shaikh et al., 
2010a).  
• Local topics. Local topics are defined as 
nouns or noun phrases introduced into dis-
course that are subsequently mentioned 
again via repetition, synonym, or pronoun. 
• Topic reference polarity. Some topics, 
which we cal meso-topics, persist through 
a number of turns in conversation. A selec-
tion of meso-topics is closely asociated 
with the task in which the discourse par-
ticipants are engaged. Meso-topics can be 
distinguished from the local topics because 
the speakers often make polarized state-
ments about them. 
4. Socio-linguistic Phenomena 
We are interested in modeling the social phe-
nomena of Leadership and Power in discourse. 
These high-level phenomena (or Social Roles, 
SR) wil be detected and atributed to dis-
course participants based on their deployment 
of selected Language Uses (LU) in 
multi-party dialogue. Language Uses are 
mid-level socio-linguistic devices that link 
linguistic components deployed in discourse 
(from lexical to pragmatic) to social con-
structs obtaining for and betwen the partici-
pants. The language uses that we are curently 
studying are Agenda Control, Disagrement, 
and Involvement (Broadwel et al., 2010). 
Our research so far is focused on the analysis 
of English-language synchronous chat, and 
we are loking for corelations betwen vari-
ous metrics that can be used to detect LU in 
multiparty dialogue. We expect that some of 
these corelations may be culturaly specific 
or language-specific, as we move into the 
analysis of Urdu and Mandarin discourse in 
the next phase of this project. 
4.1 Agenda
Control in Dialogue 
Agenda Control is defined as eforts by a 
member or members of the group to advance 
the group’s task or goal. This is a complex 
LU that we wil model along two dimensions: 
(1) Topic Control and (2) Task Control. Topic 
Control refers to atempts by any discourse 
participants to impose the topic of conversa-
tion. Task Control, on the other hand, is an 
efort by some members of the group to de-
fine the group’s project or goal and/or ster 
the group towards that goal. We believe that 
both behaviors can be detected using scalar 
measures per participant based on certain 
linguistic features of their uterances. 
For example, one hypothesis is that topic 
control is indicated by the rate of local topic 
introductions (LTI) per participant (Givon, 
1983). Local topics may be defined quite 
simply as noun phrases introduced into dis-
course, which are subsequently mentioned 
again via repetition, synonym, pronoun, or 
other form of co-reference. Thus, one meas-
ure of topic control is the number of local 
topics introduced by each participant as per-
centage of al local topics in a discourse. 
Using an LTI index we can construct aser-
tions about topic control in a discourse. For 
example, supose the folowing information 
is discovered about the speaker LE in a 
multi-party discusion dialogue-1
1
 where 90 
local topics are identified: 
1. LE introduces 23/90 (25.6%) of local top-
ics in this dialogue. 
2. The mean rate of local topic introductions 
is this dialogue is 14.29%, and standard 
deviation is 8.01. 
3. LE is in the top quintile of participants for 
introducing new local topics 
We can now claim the folowing, with a de-
gre of confidence (to be determined): 
TopicControl
LTI
 (LE, 5, dialogue-1) 
We read this as folows: speaker LE exerts the 
highest degre of topic control in dialogue-1. 
Of course, LTI is just one source of evidence 
and we developed other metrics to comple-
ment it. We mention thre of them here: 
                                                
1
 Dialogue-1 refers to an actual dataset of 90-minute chat 
among 7 participants, covering aproximately 70 turns. The 
task is to select a candidate for a job given a set of resumes. 
1040
• SMT Index. This is a measure of topic con-
trol sugested in (Givon, 1983) and it is 
based on subsequent mentions of already 
introduced local topics. Speakers who in-
troduce topics that are discused at length 
by the group tend to control the topic of the 
discusion. The subsequent mentions of lo-
cal topics (SMT) index calculates the per-
centage of second and subsequent refer-
ences to the local topics, by repetition, 
synonym, or pronoun, relative to the 
speakers who introduced them.  
• Cite Score. This index measures the extent 
to which other participants discus topics 
introduced by that speaker. The diference 
betwen SMT and CiteScore is that the lat-
ter reflect to what degre a speaker’s eforts 
to control the topic are asented to by other 
participants in a conversation. 
• TL Index (TL). This index stipulates that 
more influential speakers take longer turns 
than those who are les influential. The TL 
index is defined as the average number of 
words per turn for each speaker. Turn 
length also reflects the extent to which 
other participants are wiling to ‘yield the 
flor’ in conversation. 
Like LTI, al the above indices are maped 
into a degre of topic control, based on quin-
tiles in normal distribution (Table 1). 
 
 
 
LTI SMT CS TL AVG 
LE 5 5 5 5 5.0 
JR 4 4 4 3 3.75 
KI 4 3 3 1 2.75 
KN 3 5 4 4 4.0 
KA 2 2 2 4 2.50 
CS 2 2 2 2 2.0 
JY 1 1 1 2 1.25 
Table 1: Topic Control distribution in dialogue-1. Each 
row represents a speaker in the group (LE, JR, etc.). 
Columns show indices used, with degres per speaker 
on 5-point scale based on quintiles in normal distribu-
tion, and the average value. 
Idealy, al the above indices (and others yet 
to be defined) should predict the same out-
come, i.e., for each dialogue participant they 
should asign the same degre of topic control, 
relative to other speakers. This is not always 
the case, and where the indices divert in their 
predictions, our level of confidence in the 
generated claims decreases. We are curently 
working on how these diferent metrics cor-
relate to each other and how they should be 
weighted to maximize acuracy of making 
Topic Control claims. Nonetheles, we can 
already output a Topic Control map (shown in 
Table 1) that captures a sense of internal so-
cial dynamics within the group. 
The other aspect of Agenda Control phe-
nomenon is Task Control. It is defined as an 
efort to determine the group's goal and/or 
ster the group towards that goal. Unlike 
Topic Control, which is imposed by influenc-
ing the subject of conversation, Task Control 
is gained by directing other participants to 
perform certain tasks or acept certain opin-
ions. Consequently, Task Control is detected 
by observing the usage of certain dialogue 
acts, including Action-Directive, 
Agre-Acept, Disagre-Reject, and related 
categories. Here again, we define several in-
dices that alow us to compute a degre of 
Task Control in dialogue for each participant: 
• Directive Index (DI). The participant who 
directs others is atempting to control the 
course of the task that the group is per-
forming. We count the number of directives, 
i.e., uterances clasified as Ac-
tion-Directive, made by each participant as 
a percentage of al directives in discourse. 
• Directed Topic Shift Index (DTSI). When a 
participant who controls the task ofers a 
directive on the task, then the topic of con-
versation shifts. In order to detect this con-
dition, we calculate the ratio of coincidence 
of directive dialogue acts by each partici-
pant with topic shifts folowing them. 
• Proces Management index (PMI). Another 
measure of Task Control is the proportion 
of turns each participant has that explicitly 
adres the problem solving proces. This 
includes uterances that involve cordinat-
ing the activities of the participants, plan-
ning the order of activities, etc. These fal 
into the category of Task (or Proces) 
Management in most DA taging systems.  
• Proces Management Suces Index 
(PMSI). This index measures the degre of 
suces by each speaker at controling the 
task. A credit is given to the speaker whose 
sugested curse of action is suported by 
other speakers for each response that sup-
ports the sugestion. Conversely, a credit is 
taken away for each response that rejects or 
1041
qualifies the sugestion. PMSI is computed 
as distribution of task management credits 
among the participants over al dialogue 
uterances clasified as Task/Proces Man-
agement.
 2
 
As an example, let’s consider the folowing 
information computed for the PMI index over 
dialogue-1:  
1. Dialogue-1 contains 246 uterances clasi-
fied as Task/Proces Management rather 
than doing the task. 
2. Speaker KI makes 65 of these uterances 
for a PMI of 26.4%. 
3. Mean PMI for participants is 14.3%; 80
th
 
percentile is >21.2%. PMI for KI is in the 
top quintile for al participants. 
Based on this evidence we may claim (with 
yet to be determined confidence) that: 
TaskControl
PMI
(KI, 5, dialogue-1) 
This may be read as folows: speaker KI ex-
erts the highest degre of Task Control in 
dialogue-1. We note that Task Control and 
Topic Control do not coincide in this dis-
course, at least based on the PMI index. Other 
index values for Task Control may be com-
puted and tabulated in a way similar to LTI in 
Table 1. We omit these here due to space 
limitations. 
4.2 Disagrement
in Dialogue 
Disagrement is another language use that 
corelates with speaker’s power and leader-
ship. There are two ways in which disagree-
ment is realized: expresive disagrement and 
topical disagrement (Stromer-Galey, 207; 
Price, 202). Both can be detected using sca-
lar measures aplied to subsets of participants, 
typicaly any two participants. In adition, we 
can also measure for each participant the rate 
with which he or she generates disagrement 
(with any and al other speakers). Expresive 
Disagrement is normaly understod at the 
level of dialogue acts, i.e., when discourse 
participants make explicit uterances of dis-
agrement, disaproval, or rejection in re-
sponse to a prior speaker’s uterance. Here is 
an example (KI and KA are two speakers in a 
multiparty dialogue in which participants 
                                                
2
 The exact structure of the credit function is stil being deter-
mined experimentaly. For example, more credit may be given 
to first suporting response and les for subsequent responses; 
more credit may be given for unprompted sugestions than for 
those that were responding to questions from others. 
discus candidates for a youth counselor job): 
KA: CARLA.. women are always beter with 
kids 
KI: That’s not true! 
KI: Men can be god with kids to 
While such exchanges are vivid examples of 
expresive disagrement, we are interested in 
more sustained phenomenon where two 
speakers repeatedly disagre, thus revealing a 
social relationship betwen them. Therefore, 
one measure of Expresive Disagrement that 
we consider is the number of Disagre-Reject 
dialogue acts betwen any two speakers as a 
percentage of al uterances exchanged be-
twen these two speakers. This becomes a 
basis for the Disagre-Reject Index (DRX). In 
dialogue-1 we have: 
1. Speakers KI and KA have 47 turns betwen 
them. Among these there are 8 turns clasi-
fied as Disagre-Reject, for the DRX of 
15.7%. 
2. The mean DRX for speakers who make any 
Disagre-Reject uterances is 9.5%. The 
pair of speakers KI-KA is in the top quin-
tile (>13.6%). 
Based on this evidence we can conclude the 
folowing: 
 ExpDisagrement
DRX
 (KI,KA, 5, dialogue-1) 
which may be read as folows: speakers KI 
and KA have the highest level of expresive 
disagrement in dialogue-1. This measure is 
complemented by a Cumulative Disagrement 
Index (CDX), which is computed for each 
speaker as a percentage of al Disagre-Reject 
uterances in the discourse that are made by 
this speaker. Unlike DRX, which is computed 
for pairs of speakers, the CDX values are as-
signed to each group participant and indicate 
the degre of disagrement that each person 
generates. 
While Expresive Disagrement is based on 
the use of more overt linguistic devices, 
Topical Disagrement is defined as a difer-
ence in referential valence in uterances 
(statements, opinions, questions, etc.) made 
on a topic. Referential valence of an uterance 
is determined by the type of statement made 
about the topic in question, which can be 
positive (+), negative (−), or neutral (0). A 
positive statement is one in favor of (expres 
advocacy) or in suport of (suporting infor-
mation) the topic being discused. A negative 
statement is one that is against or negative on 
1042
the topic being discused. A neutral statement 
is one that does not indicate the speaker’s po-
sition on the topic. Here is an example of op-
posing polarity statements about the same 
topic in discourse: 
Sp-1: I like that he mentions “Volunterism 
and Leadership” 
Sp-2: but if they’re loking for someone who 
is experienced then I’d cros him of 
Detecting topical disagrement in discourse is 
more complicated because its strength may 
vary from one topic in a conversation to the 
next. A reasonable aproach is thus to meas-
ure the degre of disagrement betwen two 
speakers on one topic first, and then extrapo-
late over the entire discourse. Acordingly, 
our measure of topical disagrement is valua-
tion diferential betwen any two speakers as 
expresed in their uterances about a topic. 
Here, the topic (or an “isue”) is understod 
more narowly than the local topic defined in 
the previous section (as used in Topic Control, 
for example), and may be asumed to cover 
only the most persistent local topics, i.e., top-
ics with the largest number of references in 
dialogue, or what we cal the meso-topics. For 
example, in a discusion of job aplicants, 
each of the aplicants becomes a meso-topic, 
and there may be aditional meso-topics pre-
sent, such as qualifications required, etc. 
The resulting Topical Disagrement Metric 
(TDM) captures the degre to which any two 
speakers advocate the oposite sides of a 
meso-topic. TDM is computed as an average 
of P-valuation diferential for one speaker 
(advocating for a meso-topic) and 
(−P)-valuation diferential for the other 
speaker (advocating against the meso-topic). 
Using TDM we can construct claims related 
to disagrement in a given multiparty dia-
logue of suficient duration (exactly what 
constitutes a suficient duration is stil being 
researched). Below is an example based on a 
90-minute chat dialogue-1 about several job 
candidates for a youth counselor. The discus-
sion involved 7 participants, including KI and 
KA. Topical disagrement is measured on 5 
points scale (coresponding to quintiles in 
normal distribution): 
TpDisAgre
TDM
(KI,KA,“Carla”,4,dialogue-1) 
This may be read as folows: speakers KI and 
KA topicaly disagre to degre 4 on topic 
[job candidate] “Carla” in dialogue-1. In or-
der to calculate this we compute the value of 
TDM index betwen these two speakers. We 
find that KA makes 30% of al positive uter-
ances made by anyone about Carla (40), while 
KI makes 45% of al negative uterances 
against Carla. This places these two speakers 
in the top quintiles in the “for Carla” polarity 
distribution and “against Carla” distribution, 
respectively. Taking into acount any opos-
ing polarity statements made by KA against 
Carla and any statements made by KI for 
Carla, we calculate the level of topical dis-
agrement betwen KA and KI to be 4 on the 
1-5 scale. 
TDM alows us to compute topical disagree-
ment betwen any two speakers in a discourse, 
which may also be represented in a 
2-dimensional table revealing another inter-
esting aspect of internal group dynamics. 
4.3 Involvement
in Dialogue 
The third type of social language use that we 
discus in this paper is Involvement. In-
volvement is defined as a degre of engage-
ment or participation in the discusion of a 
group. It is an important element of leader-
ship, although its importance is expected to 
difer betwen cultures; in Western cultures, 
high involvement and influence (topic control) 
often corelates with group leadership. 
In order to measure Involvement we designed 
several indices based on turn characteristics 
for each speaker. Four of the indices are 
briefly explained below:  
• The NP index (NPI) is a measure of gros 
informational content contributed by each 
speaker in discourse. NPI counts the ratio 
of third-person nouns and pronouns used 
by a speaker to the total number of nouns 
and pronouns in the discourse. 
• The Turn index (TI) is a measure of inter-
actional frequency; it counts the ratio of 
turns per participant to the total number of 
turns in the discourse. 
• The Topic Chain Index (TCI) counts the 
degre to which participants discus of the 
most persistent topics. In order to calculate 
TCI values, we define a topic chains for al 
local topics. We compute frequency of 
mentions of these longest topics for each 
participant. 
• The Alotopicality Index (ATP) counts the 
number of mentions of local topics that 
were introduced by other participants. An 
1043
ATP value is the proportion of a speaker's 
alotopical mentions, i.e., excluding 
“self-citations”, to al alotopical mentions 
in a discourse. 
As an example, we may consider the folow-
ing situation in dialogue-1: 
1. Dialogue-1 contains 796 third person 
nouns and pronouns, excluding mentions of 
participants’ names. 
2. Speaker JR uses 180 nouns and pronouns 
for an NPI of 2.6%. 
3. The median NPI is 14.3%; JR are in the 
uper quintile of participants (> 19.9%). 
From the above evidence we can draw the 
folowing claim: 
Involvement
NPI
(JR, 5, dialogue-1) 
This may be read as: speaker JR is the most 
involved participant in dialogue-1. 
As with other language uses, multiple indices 
for Involvement can be combined into a 
2-dimensional map capturing the group in-
ternal dynamics. 
5. Implementation & Evaluation 
We developed a prototype automated 
DSARMD system that comprises a series of 
modules that create automated anotation of 
the source dialogue for al the language ele-
ments discused above, including comuni-
cative links, dialogue acts, local/meso topics, 
and polarity. Automaticaly anotated dia-
logue is then used to generate language use 
degre claims. In order to evaluate acuracy 
of the automated proces we conducted a pre-
liminary evaluation comparing the LU claims 
generated from automaticaly anotated data 
to the claims generated from manualy coded 
dialogues. Below we briefly describe the 
methodology and metrics used. 
Each language use is aserted per a partici-
pant in a discourse (or per each pair of par-
ticipants, e.g., for Disagrement) on a 5-point 
“strength” scale. This can be represented as 
an ordered sequence LU
X
(d
1, d
2, … d
n
), where 
LU is the language use being aserted, X is 
the index used, d
i
 is the degre of LU atrib-
uted to speaker i. This asignment is therefore 
a 5-way clasification of al discourse par-
ticipants and its corectnes is measured by 
dividing the number of corect asignments 
by the total number of elements to be clasi-
fied, which gives the micro-averaged preci-
sion. The acuracy metric is computed with 
several variants as folows: 
1. Strict maping: each complete match is 
counted as 1; al mismatches are counted as 
0. For example, the outputs LU
X
 (5,4,3,2,1) 
and LU
X
 (4,5,3,1,1) produce two exact 
matches (for the third and the last speaker) 
for a precision of 0.4. 
2. Weighted maping: since each degre value 
d
i 
in LU
X
(d
1, d
2, … d
n
) represents a quintile 
in normal distribution, we consider the po-
sition of the value within the quintile. If 
two mismatched values are les than ½ 
quintile apart we asign a partial credit 
(curently 0.5). 
3. Highest – Rest: we measure acuracy with 
which the highest LU degre (but not nec-
esarily the same degre) is asigned to the 
right speaker vs. any other score. This re-
sults in binary clasification of scores. The 
sequences in (1) produce 0.6 match score. 
4. High – Low: An alternative binary clasifi-
cation where scores 5 and 4 are considered 
High, while the remaining scores are con-
sidered Low. Under this metric, the se-
quences in (1) match with 10% precision. 
The proces of automatic asignment of lan-
guage uses derived from automaticaly proc-
esed dialogues was evaluated against the 
control set of asignments based on hu-
man-anotated data. In order to obtain a reli-
able “ground truth”, each test dialogue was 
anotated by at least thre human coders 
(linguistics and comunication graduate stu-
dents, trained). Since human anotation was 
done at the linguistic component level, a strict 
inter-anotator agrement was not required; 
instead, we were interested whether in each 
case a comparable statistical distribution of 
the coresponding LU index was obtained. 
Anotations that produced index distributions 
disimilar from the majority were eliminated. 
Automated dialogue procesing involved the 
folowing modules: 
• Local topics detection identifies first men-
tions by tracking ocurences of noun 
phrases. Subsequent mentions are identi-
fied using fairly simple pronoun resolution 
(based mostly on lexical features), with 
Wordnet used to identify synonyms, etc. 
• Meso-topics are identified as longest-chain 
local topics. Their polarity is asesed at 
the uterance level by noting presence of 
positive or negative cue words and phrases. 
• Dialogue acts are taged based on presence 
1044
of certain cue phrases derived from a train-
ing corpus (Web et al., 208). 
• Comunicative links are maped by com-
puting inter-uterance similarity based on 
n-gram overlap. 
Preliminary evaluation results are shown in 
Tables 3-5 with average performance over 3 
chat sesions (aprox 4.5 hours) involving 
thre groups of speakers and diferent tasks 
(job candidates, political isues). Topic Con-
trol and Involvement tables show average 
acuracy per index. For example, the LTI in-
dex, computed over automaticaly extracted 
local topics, produces Topic Control asign-
ments with the average precision of 80% 
when compared to asignments derived from 
human-anotated data using the strict accu-
racy metric. However, automated prediction 
of Involvement based on NPI index is far les 
reliable, although we can stil pick the most 
involved speaker with 67% acuracy. We omit 
the indices based on turn length (TL) and turn 
count (TI) because their values are trivialy 
computed. At this time we do not combine 
indices into a single LU prediction. Adi-
tional experiments are neded to determine 
how much each of these indices contributes to 
LU prediction. 
Topic 
Control 
LTI  SMT  CS 
Strict  0.80  0.40  0.40 
Weighted  0.90  0.53  0.53 
Highest‐Rest  0.90  0.67  0.67 
High‐Low  1.00  0.84  0.90 
Table 3: Topic Control LU asignment performance 
averages of selected indices over a subset of data cov-
ering thre dialogues with combined duration of 4.5 
hours with total of 19 participants (7, 5, 7 per sesion). 
Involvement NPI  TCI  ATP 
Strict  0.31  0.42  0.39 
Weighted  0.46  0.49  0.42 
Highest‐Rest  0.67  0.7  0.68 
High‐Low  0.58  0.74  0.48 
Table 4: Involvement LU asignment performance av-
erages for selected indices over the same subset of data 
as in Table 3. 
Topical Disagrement performance is shown 
in Table 5. We calculated precision and recal 
of asigning a corect degre of disagrement 
to each pair of speakers who are members of 
a group. Precision and recal averages are 
then computed over al meso-topics identified 
in the test dataset, which consists of thre 
separate 90-minute dialogues involving 7, 5 
and 7 speakers, respectively. Our calculation 
includes the cases where diferent sets of 
meso-topics were identified by the system 
and by the human coder. A strict maping of 
levels of disagrement betwen speakers is 
hard to compute acurately; however, finding 
the speakers who disagre the most, or the 
least, is significantly more robust. 
Topical 
Disagrement 
Prec.  Recal 
Strict  0.33  0.32 
Weighted  0.54  0.54 
Highest‐Rest  0.89  0.85 
High‐Low  0.77  0.73 
Table 5: Topical Disagrement LU asignment per-
formance averages over 13 meso-topics discused in 
thre dialogues with combined duration of 4.5 hours 
with total of 19 participants (7, 5, and 7 per sesion). 
6. Conclusion 
In this paper we presented a preliminary 
design for modeling certain types of social 
phenomena in multi-party on-line dialogues. 
Initial, limited-scale evaluation indicates that 
the model can be efectively automated. 
Much work lies ahead, including large scale 
evaluation, testing index stability and 
resilience to NL component level eror. 
Curent performance of the system is based 
on only preliminary versions of linguistic 
modules (topic extraction, polarity 
asignments, etc.) which perform at only 
70-80% acuracy, so these ned to be 
improved as wel. Research on Urdu and 
Chinese dialogues is just starting. 
Acknowledgements 
This research was funded by the Ofice of the 
Director of National Inteligence (ODNI), 
Inteligence Advanced Research Projects 
Activity (IARPA), through the U.S. Army 
Research Lab. Al statements of fact, opinion 
or conclusions contained herein are those of 
the authors and should not be construed as 
representing the oficial views or policies of 
IARPA, the ODNI or the U.S. Government. 
1045
References 
Agar, Michael. 194. Language Shock, Under-
standing the Culture of Conversation. Quil, 
Wiliam Morow, New York. 
Alen, J. . Core. 197. Draft of DAMSL: Dialog 
Act Markup in Several Layers. ww.cs. roch-
ester.edu/research/cisd/resources/damsl/ 
Anderson, A., et al. 1991. The HCRC Map Task 
Corpus. Language and Spech 34(4), 351--366. 
Austin, J. L. 1962. How to do Things with Words. 
Clarendon Pres, Oxford. 
Bird, Steven, et al. 2009. Natural Language Proc-
esing with Python: Analyzing Text with the 
Natural Language Tolkit. O'Reily Media. 
Blaylock, Nate. 202. Managing Comunicative 
Intentions in Dialogue Using a Colaborative 
Problem-Solving Model. Technical Report 74, 
University of Rochester, CS Dept. 
Broadwel, G. A et al. (2010). Social Phenomena 
and Language Use. ILS Technical report. 
Carbery, Sandra and Lyn Lambert. 199. A 
Proces Model for Recognizing Comunicative 
Acts and Modeling Negotiation Dialogue. 
Computational Linguistics, 25(1), p. 1-53. 
Carleta, J. (207). Unleashing the kiler corpus: 
experiences in creating the multi-everything 
AMI Meting Corpus. Language Resources and 
Evaluation Journal 41(2): 181-190 
Carlson, Lauri. 1983. Dialogue Games: An Ap-
proach to Discourse Analysis. D. Reidel. 
Eric N. Forsyth and Craig H. Martel. 2007. Lexi-
cal and Discourse Analysis of Online Chat Dia-
log. First IEE International Conference on 
Semantic Computing (ICSC 207), p. 19-26. 
Field, D., et al. 208. Automatic Induction of Dia-
logue Structure from the Companions Dialogue 
Corpus, 4th Int. Workshop on Human-Computer 
Conversation, Belagio. 
Givon, Talmy. 1983. Topic continuity in discourse: 
A quantitative cros-language study. Amster-
dam: John Benjamins. 
Ivanovic, Edward. 205. Dialogue Act Taging for 
Instant Mesaging Chat Sesions. In Proced-
ings of the ACL Student Research Workshop. 
79–84. An Arbor, Michigan. 
Ji, Gang Jef Bilmes. 206. Backof Model Train-
ing using Partialy Observed Data: Aplication 
to Dialog Act Taging. HLT-NACL 
Jurafsky, Dan, Elizabeth Shriberg, and Debra Bi-
asca. 197. Switchboard SWBD-DAMSL Shal-
low-Discourse-Function Anotation Coders 
Manual. htp:/stripe.colorado.edu/~jurafsky/ 
manual.august1.html 
Jurafsky, D., et al. 1997. Automatic detection of 
discourse structure for spech recognition and 
understanding. IEE Workshop on Spech 
Recognition and Understanding, Santa Barbara. 
Khan, Faisal M., et al. 2002. Mining Chat-rom 
Conversations for Social and Semantic Interac-
tions. Computer Science and Enginering, Le-
high University. 
Kim, Jihie., et al. 207. An Inteligent Discus-
sion-Bot for Guiding Student Interactions in 
Threaded Discusions. AAI Spring Sympo-
sium on Interaction Chalenges for Inteligent 
Asistants 
Levin, L., et al. (198). A discourse coding 
scheme for conversational Spanish. Interna-
tional Conference on Spech and Language 
Procesing. 
Levin, L., et al. (2003). Domain specific spech 
acts for spoken language translation. 4th SIG-
dial Workshop on Discourse and Dialogue. 
Linel, Per. 190. The power of dialogue dynamics. 
In Ivana Markov´a and Klaus Fopa, editors, 
The Dynamics of Dialogue. Harvester, 147–177. 
Poesio, Masimo and Andrei Mikhev. 198. The 
predictive power of game structure in dialogue 
act recognition. International Conference on 
Spech and Language Procesing (ICSLP-98). 
Price, V., Capela, J. N., & Nir, L. (202). Does 
disagrement contribute to more deliberative 
opinion? Political Comunication, 19, 95-12. 
Sacks, H. and Scheglof, E., Jeferson, G. 1974. A 
simplest systematic for the organization of 
turn-taking for conversation. In: Language 
50(4), 696-735. 
Samuel, K. et al. 198. Dialogue Act Taging with 
Transformation-Based Learning. 36th Anual 
Meting of the ACL. 
Scolon, Ron and Suzane W. Scolon. 201. 
Intercultural Comunication, A Discourse Ap-
proach. Blackwel Publishing, Second Edition. 
Searle, J. R. 1969. Spech Acts. Cambridge Uni-
versity Pres, London-New York. 
Shaikh, S. et al. 2010. DSARMD Anotation 
Guidelines, V. 2.5. ILS Technical Report.  
Shaikh S. et al. 2010. MPC: A Multi-Party Chat 
Corpus for Modeling Social Phenomena in 
Discourse, Proc. LREC-2010, Malta. 
Stolcke, Andreas et al. 200. Dialogue Act Mod-
eling for Automatic Taging and Recognition of 
Conversational Spech. Computational Linguis-
tics, 26(3). 
Stromer-Galey, J. 207. Measuring deliberation’s 
content: A coding scheme. Journal of Public 
Deliberation, 3(1). 
Tianhao Wu, et al. 2002. Posting Act Taging Us-
ing Transformation-Based Learning. Founda-
tions of Data Mining and Discovery, IEE In-
ternational Conference on Data Mining 
Twitchel, Douglas P., Jay F. Nunamaker Jr., and 
Jude K. Burgon. 204. Using Spech Act 
Profiling for Deception Detection. Inteligence 
and Security Informatics, LNCS, Vol. 3073 
Web, N., T. Liu, M. Heple and Y. Wilks. 208. 
Cros-Domain Dialogue Act Taging. 6th In-
ternational Conference on Language Resources 
and Evaluation (LREC-208), Marakech. 
1046

