<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>S Babi´c</author>
<author>B Finka</author>
<author>M Moguˇs</author>
</authors>
<title>Hrvatski pravopis. ˇSkolska knjiga</title>
<date>2002</date>
<marker>Babi´c, Finka, Moguˇs, 2002</marker>
<rawString>S. Babi´c, B. Finka, and M. Moguˇs. 2002. Hrvatski pravopis. ˇSkolska knjiga.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Bekavac</author>
<author>M Tadi´c</author>
</authors>
<title>Implementation of croatian nerc system</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing</booktitle>
<location>Prague, Czech Republic</location>
<marker>Bekavac, Tadi´c, 2007</marker>
<rawString>B. Bekavac and M. Tadi´c. 2007. Implementation of croatian nerc system. In Proceedings of the Workshop on Balto-Slavonic Natural Language Processing 2007, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>Tnt – a statistical part-of-speech tagger</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied NLP Conference, ANLP2000</booktitle>
<location>Seattle, WA</location>
<contexts>
<context>distributing a part of the probability mass to unseen n-grams (Dagan et al., 1997). The method applied in this research uses linear interpolation. It was introduced in (Samuelsson, 1996) and used in (Brants, 2000) and is called linear successive abstraction. The values calculated in the model are conditional probabilities of a specific tag t given the last m letters of an n letter word. The algorithm combines</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>T. Brants. 2000. Tnt – a statistical part-of-speech tagger. In Proceedings of the 6th Applied NLP Conference, ANLP2000, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
</authors>
<title>Empirical methods in information extraction</title>
<date>1997</date>
<journal>AI Magazine</journal>
<volume>18</volume>
<contexts>
<context>lace manual coding efforts with automatically trainable components that make it increasingly faster and easier to build accurate and robust information extraction systems in new domains or languages (Cardie, 1997). NEI is still most often based on deterministic methods combining lists of named entities with finite state automata, i.e. transducers. Considering the NLG task, there are two main approaches: the t</context>
</contexts>
<marker>Cardie, 1997</marker>
<rawString>C. Cardie. 1997. Empirical methods in information extraction. AI Magazine, 18(4):65–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>L Lee</author>
<author>F Pereira</author>
</authors>
<title>Similarity-based methods for word sense disambiguation</title>
<date>1997</date>
<booktitle>Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics</booktitle>
<pages>56--63</pages>
<editor>In Philip R. Cohen and Wolfgang Wahlster, editors</editor>
<publisher>Association for Computational Linguistics</publisher>
<location>Somerset, New Jersey</location>
<contexts>
<context>need for combining evidence from different size n-grams. Two basic techniques are commonly used: linear interpolation and smoothing by redistributing a part of the probability mass to unseen n-grams (Dagan et al., 1997). The method applied in this research uses linear interpolation. It was introduced in (Samuelsson, 1996) and used in (Brants, 2000) and is called linear successive abstraction. The values calculated </context>
</contexts>
<marker>Dagan, Lee, Pereira, 1997</marker>
<rawString>I. Dagan, L. Lee, and F. Pereira. 1997. Similarity-based methods for word sense disambiguation. In Philip R. Cohen and Wolfgang Wahlster, editors, Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, pages 56–63, Somerset, New Jersey. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gaizauskas</author>
<author>Y Wilks</author>
</authors>
<title>Information extraction: Beyond document retrieval</title>
<date>1998</date>
<journal>Journal of Documentation</journal>
<volume>54</volume>
<contexts>
<context>y in forthcoming research. The single-word lexicon is capable of handling both tasks. 1. Introduction Information extraction (IE) is the task of deriving structured factual information from the text (Gaizauskas and Wilks, 1998). Natural language generation (NLG) is the task of generating natural language utterances from structured information representing the reverse process to information extraction. One of information ex</context>
</contexts>
<marker>Gaizauskas, Wilks, 1998</marker>
<rawString>R. Gaizauskas and Y. Wilks. 1998. Information extraction: Beyond document retrieval. Journal of Documentation, 54(1):70–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Krˇzak</author>
<author>D Boras</author>
</authors>
<title>Lexical database of the croatian literary language. Informatologia Yugoslavica</title>
<date>1985</date>
<pages>17--3</pages>
<marker>Krˇzak, Boras, 1985</marker>
<rawString>M. Krˇzak and D. Boras. 1985. Lexical database of the croatian literary language. Informatologia Yugoslavica, 17(3-4):223–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Nadeau</author>
<author>S Sekine</author>
</authors>
<title>A survey of named entity recognition and classification</title>
<date>2007</date>
<journal>Linguisticae Investigationes</journal>
<volume>30</volume>
<contexts>
<context>s activity) which is called named entity recognition (NER). NER locates and classifies atomic elements in the text into predefined categories such as names of persons, organizations, locations, etc. (Nadeau and Sekine, 2007). There are two main research approaches in the field of NER: the approach based on stochastic methods and deterministic approach. In stochastic approaches the named entity models are trained on a la</context>
</contexts>
<marker>Nadeau, Sekine, 2007</marker>
<rawString>D. Nadeau and S. Sekine. 2007. A survey of named entity recognition and classification. Linguisticae Investigationes, 30(1):3–26, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>R Dale</author>
</authors>
<title>Building Natural Language Generation Systems</title>
<date>2000</date>
<publisher>Cambridge University Press</publisher>
<contexts>
<context>sed approach is used more often since it is simpler and generates more accurate, but less diverse results. It is based on predefined templates with gaps that are filled based on database information (Reiter and Dale, 2000). When dealing with highly inflected languages such as Croatian, tasks relating to NEI and template-based NLG become more complicated (Bekavac and Tadi´c, 2007). In the NEI task all possible forms fo</context>
</contexts>
<marker>Reiter, Dale, 2000</marker>
<rawString>E. Reiter and R. Dale. 2000. Building Natural Language Generation Systems. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Samuelsson</author>
</authors>
<title>Handling sparse data by successive abstraction</title>
<date>1996</date>
<booktitle>In Proceedings of COLING-96, Kopenhagen</booktitle>
<location>Denmark</location>
<contexts>
<context>terpolation and smoothing by redistributing a part of the probability mass to unseen n-grams (Dagan et al., 1997). The method applied in this research uses linear interpolation. It was introduced in (Samuelsson, 1996) and used in (Brants, 2000) and is called linear successive abstraction. The values calculated in the model are conditional probabilities of a specific tag t given the last m letters of an n letter w</context>
<context>::ln) C(ln i+1;:::ln) (3) where C() is the count function. The weights proven to get best results are standard deviations of unconditioned maximum likelihood estimates of n-grams in the training set (Samuelsson, 1996) by i = vu ut 1 N NX j=1 ( ^P(ln i+1;:::ln) P)2 (4) P = 1 N NX j=1 ^P(ln i+1;:::ln) (5) Besides the model, morphological lexica of general language, personal names and settlements are used in the dec</context>
</contexts>
<marker>Samuelsson, 1996</marker>
<rawString>C. Samuelsson. 1996. Handling sparse data by successive abstraction. In Proceedings of COLING-96, Kopenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tadi´c</author>
<author>S Fulgosi</author>
</authors>
<title>Building the croatian morphological lexicon</title>
<date>2003</date>
<booktitle>In Proceedings of the EACL2003 Workshop on Morphological Processing of Slavic Languages</booktitle>
<pages>41--46</pages>
<publisher>ACL</publisher>
<location>Budapest</location>
<marker>Tadi´c, Fulgosi, 2003</marker>
<rawString>M. Tadi´c and S. Fulgosi. 2003. Building the croatian morphological lexicon. In Proceedings of the EACL2003 Workshop on Morphological Processing of Slavic Languages, pages 41–46, Budapest. ACL.</rawString>
</citation>
</citationList>
</algorithm>

