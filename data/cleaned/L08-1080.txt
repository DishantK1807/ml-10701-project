<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Anna Babarczy</author>
<author>John Carroll</author>
<author>Geoffrey Sampson</author>
</authors>
<title>Definitional, personal, and mechanical constraints on part of speech annotation performance</title>
<date>2006</date>
<journal>Journal of Natural Language Engineering</journal>
<volume>12</volume>
<pages>77--90</pages>
<contexts>
<context>the data. Further, we want it to tell us something about what the categories denote across tagsets, both to pinpoint the most problematic cases for taggers and to provide feedback to humans (cf. also Babarczy et al., 2006). A simple question that provides some guidance in exploring these issues is: for cases where two annotation schemes could have made the same tagging decision, do they in fact make the same distincti</context>
</contexts>
<marker>Babarczy, Carroll, Sampson, 2006</marker>
<rawString>Babarczy, Anna, John Carroll, and Geoffrey Sampson (2006). Definitional, personal, and mechanical constraints on part of speech annotation performance. Journal of Natural Language Engineering 12, 77–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>Tagset Reduction Without Information Loss</title>
<date>1995</date>
<booktitle>In Proceedings of ACL-95</booktitle>
<location>Cambridge, MA</location>
<marker>Brants, 1995</marker>
<rawString>Brants, Thorsten (1995). Tagset Reduction Without Information Loss. In Proceedings of ACL-95. Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>Internal and External Tagsets in Part-of-Speech Tagging</title>
<date>1997</date>
<booktitle>In Proceedings of Eurospeech</booktitle>
<location>Rhodes, Greece</location>
<contexts>
<context>us annotation, the distinctions made in corpus annotation can have a great impact on accuracy. For example, which partof-speech (POS) distinctions are made affects the success of a tagger (cf., e.g., Brants, 1997), especially the amount of non-locality in tagging distinctions across tagsets. Tagging algorithms thus sometimes use tagset-specific features or features optimized for a particular annotation scheme</context>
</contexts>
<marker>Brants, 1997</marker>
<rawString>Brants, Thorsten (1997). Internal and External Tagsets in Part-of-Speech Tagging. In Proceedings of Eurospeech. Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herv´e D´ejean</author>
</authors>
<title>How to Evaluate and Compare Tagsets? A Proposal</title>
<date>2000</date>
<booktitle>In Proceedings of LREC-00</booktitle>
<location>Athens</location>
<marker>D´ejean, 2000</marker>
<rawString>D´ejean, Herv´e (2000). How to Evaluate and Compare Tagsets? A Proposal. In Proceedings of LREC-00. Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
</authors>
<title>Error detection and correction in annotated corpora</title>
<date>2005</date>
<tech>Ph.D. thesis</tech>
<institution>The Ohio State University</institution>
<contexts>
<context>llow for the same tagging decision to be made. But what are comparable contexts? To find a suitable definition, we start with the variation n-gram error detection method (Dickinson and Meurers, 2003; Dickinson, 2005), which detects errors by looking for items occurring multiple times in a corpus with varying annotation. These variation nuclei are predicted to be unambiguous when a part of the same variation n-gr</context>
<context> and across corpora, and more fully evaluating the variation n-gram method gets at this question. Previously testing the method on the BNC-sampler (Leech, 1997) and finding much lower accuracy (52%) (Dickinson, 2005) did not take into account the differences between a news corpus (WSJ) and a balanced corpus (BNC-sampler). In other words, we do not know whether the difference in precision is more attributable to </context>
<context>their shared history. Again, the coarser-grained tagset winds up having more variation in context. This is also likely related to annotation quality (cf. the comparison between the WSJ and SUSANNE in Dickinson, 2005) PTB SUSANNE Count JJ/NN JJ 25 IN/RP RP 23 IN/RB RP 21 DT/PDT DBa 17 JJR/RBR DAR 15 JJ/NNP JJ 13 VBD/VBN VVNv 12 IN/RB/RP RP 8 WDT/WP DDQ 6 VBD/VBN VVDv 6 JJS/RBS DAT 6 DT/RB ATn 6 PRP/PRP$ APPGf 5 J</context>
<context>genres and different languages, in order to see how general localized variation nuclei are in their prediction of the same basic category. In parallel with the development of error detection methods (Dickinson, 2005), the methods explored here can also potentially be extended to more complex forms of annotation, where there is an even greater need for comparison, such as syntactic annotation (cf. Rehbein and van</context>
</contexts>
<marker>Dickinson, 2005</marker>
<rawString>Dickinson, Markus (2005). Error detection and correction in annotated corpora. Ph.D. thesis, The Ohio State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dickinson</author>
<author>W Detmar Meurers</author>
</authors>
<title>Detecting Errors in Part-of-Speech Annotation</title>
<date>2003</date>
<booktitle>In Proceedings of EACL-03</booktitle>
<pages>107--114</pages>
<location>Budapest, Hungary</location>
<contexts>
<context>elying on simple but accurate corpus properties, is annotation error detection. Techniques for detecting inconsistencies predict areas of consistent annotation. Furthermore, the method we build from (Dickinson and Meurers, 2003) uses the local contextual information shown to be relevant for human category learning (Mintz, 2003). Thus, as described below, we have a simple and cognitively plausible method to use, which gives </context>
<context>compare across corpora that allow for the same tagging decision to be made. But what are comparable contexts? To find a suitable definition, we start with the variation n-gram error detection method (Dickinson and Meurers, 2003; Dickinson, 2005), which detects errors by looking for items occurring multiple times in a corpus with varying annotation. These variation nuclei are predicted to be unambiguous when a part of the sa</context>
</contexts>
<marker>Dickinson, Meurers, 2003</marker>
<rawString>Dickinson, Markus and W. Detmar Meurers (2003). Detecting Errors in Part-of-Speech Annotation. In Proceedings of EACL-03. Budapest, Hungary, pp. 107–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W N Francis</author>
<author>H Kucera</author>
</authors>
<title>Brown Corpus Manual</title>
<date>1979</date>
<tech>Tech. rep</tech>
<institution>Department of Linguistics, Brown University</institution>
<location>Providence, Rhode Island, US</location>
<contexts>
<context>ficult to disambiguate and fraught with errors; as it turns out, the notion of adverb and particle is more lexicalized in Brown, which leads to such correspondences. As it states in the Brown manual (Francis and Kucera, 1979), It was decided instead to consider this a syntactic and semantic rather than a taxonomic problem, and to give the “portmanteau” tag RP (for “adverb or particle”) to the ten words about, across, dow</context>
</contexts>
<marker>Francis, Kucera, 1979</marker>
<rawString>Francis, W. N. and H. Kucera (1979). Brown Corpus Manual. Tech. rep., Department of Linguistics, Brown University, Providence, Rhode Island, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry Kucera</author>
<author>W Nelson Francis</author>
</authors>
<title>Computational Analysis of Present-Day American English</title>
<date>1967</date>
<publisher>Brown University Press</publisher>
<location>Providence, RI</location>
<contexts>
<context>orpus. On the other hand, to control for differences between corpora and focus on differences in the tagset, we would at first like the text to be highly similar across annotations. The Brown corpus (Kucera and Francis, 1967) is thus an ideal test case: this balanced corpus contains its original annotation, but it was also re-annotated with a related, but simpler, tagset as part of the Penn Treebank (Marcus et al., 1993)</context>
</contexts>
<marker>Kucera, Francis, 1967</marker>
<rawString>Kucera, Henry and W. Nelson Francis (1967). Computational Analysis of Present-Day American English. Providence, RI: Brown University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Leech</author>
</authors>
<title>A Brief Users’ Guide to the GrammaticalTaggingoftheBritishNationalCorpus. UCREL</title>
<date>1997</date>
<institution>Lancaster University</institution>
<contexts>
<context>termine how comparable localized contexts are within and across corpora, and more fully evaluating the variation n-gram method gets at this question. Previously testing the method on the BNC-sampler (Leech, 1997) and finding much lower accuracy (52%) (Dickinson, 2005) did not take into account the differences between a news corpus (WSJ) and a balanced corpus (BNC-sampler). In other words, we do not know whet</context>
</contexts>
<marker>Leech, 1997</marker>
<rawString>Leech, Geoffrey (1997). A Brief Users’ Guide to the GrammaticalTaggingoftheBritishNationalCorpus. UCREL, Lancaster University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing</booktitle>
<publisher>The MIT Press</publisher>
<location>Cambridge, MA</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Manning, Christopher D. and Hinrich Sch¨utze (1999). Foundations of Statistical Natural Language Processing. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>313--330</pages>
<contexts>
<context>annotation. These variation nuclei are predicted to be unambiguous when a part of the same variation n-gram. For example, in the Wall Street Journal (WSJ) corpus, part of the Penn Treebank 3 release (Marcus et al., 1993), the string in (1) is a variation 12-gram since off is a variation nucleus that in one corpus occurrence is tagged as a preposition (IN), while in another it is tagged as a particle (RP). Dickinson </context>
<context>ra and Francis, 1967) is thus an ideal test case: this balanced corpus contains its original annotation, but it was also re-annotated with a related, but simpler, tagset as part of the Penn Treebank (Marcus et al., 1993). We refer to these corpora as Brown and Brown-PTB, respectively. Due to tokenization differences, Brown contains 1,161,192 tokens, and Brown-PTB has 1,170,811 tokens. In addition to slight textual d</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, Mitchell P., Beatrice Santorini and Mary Ann Marcinkiewicz (1993). Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics 19(2), 313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toben H Mintz</author>
</authors>
<title>Category induction from distributional cues in an artificial language</title>
<date>2002</date>
<journal>Memory &amp; Cognition</journal>
<volume>30</volume>
<pages>678--686</pages>
<marker>Mintz, 2002</marker>
<rawString>Mintz, Toben H. (2002). Category induction from distributional cues in an artificial language. Memory &amp; Cognition 30, 678–686.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toben H Mintz</author>
</authors>
<title>Frequent frames as a cue for grammatical categories in child directed speech</title>
<date>2003</date>
<journal>Cognition</journal>
<volume>90</volume>
<pages>91--117</pages>
<contexts>
<context>ies predict areas of consistent annotation. Furthermore, the method we build from (Dickinson and Meurers, 2003) uses the local contextual information shown to be relevant for human category learning (Mintz, 2003). Thus, as described below, we have a simple and cognitively plausible method to use, which gives an indication about the quality of the categories. The representation of data for error detection, th</context>
</contexts>
<marker>Mintz, 2003</marker>
<rawString>Mintz, Toben H. (2003). Frequent frames as a cue for grammatical categories in child directed speech. Cognition 90, 91–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toben H Mintz</author>
</authors>
<title>Finding the verbs: distributional cues to categories available to young learners</title>
<date>2006</date>
<booktitle>Action Meets Word: How Children Learn Verbs</booktitle>
<pages>31--63</pages>
<editor>In K. Hirsh-Pasek and R. M. Golinkoff (eds</editor>
<publisher>University Press</publisher>
<location>New York: Oxford</location>
<marker>Mintz, 2006</marker>
<rawString>Mintz, Toben H. (2006). Finding the verbs: distributional cues to categories available to young learners. In K. Hirsh-Pasek and R. M. Golinkoff (eds.), Action Meets Word: How Children Learn Verbs, New York: Oxford University Press, pp. 31–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar</title>
<date>1994</date>
<publisher>University of Chicago Press</publisher>
<location>Chicago</location>
<contexts>
<context>n Brown—a distinction collapsed into RB in Brown-PTB—as shown in (6). Determining these cases not only often requires more context to disambiguate, but some knowledge of linguistic theory (see, e.g., Pollard and Sag, 1994, sec. 9.4). (6) a. not nearly/rb as complex/jj b. not nearly/ql as much/ap Manually examining the error detection output thus uncovers tag distinctions which are problematic in one way or another in </context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Pollard, Carl and Ivan A. Sag (1994). Head-Driven Phrase Structure Grammar. Chicago: University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ines Rehbein</author>
<author>Josef van Genabith</author>
</authors>
<title>Treebank Annotation Schemes and Parser Evaluation for German</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL-07</booktitle>
<pages>630--639</pages>
<marker>Rehbein, van Genabith, 2007</marker>
<rawString>Rehbein, Ines and Josef van Genabith (2007). Treebank Annotation Schemes and Parser Evaluation for German. In Proceedings of EMNLP-CoNLL-07. pp. 630–639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Sampson</author>
</authors>
<title>English for the Computer: The SUSANNE Corpus and Analytic Scheme</title>
<date>1995</date>
<publisher>Clarendon Press</publisher>
<location>Oxford</location>
<contexts>
<context>rison. For a more thorough analysis, we additionally explore a third corpus—which happens also to be based on the Brown data—for the tagset discrepancy calculation (section 4.3.). The SUSANNE corpus (Sampson, 1995) has approximately 151,600 tokens and provides a much more fine-grained annotation scheme, with 424 lexical categories.1 Comparing it to the other two tagsets will provide insights into whether and h</context>
</contexts>
<marker>Sampson, 1995</marker>
<rawString>Sampson, Geoffrey (1995). English for the Computer: The SUSANNE Corpus and Analytic Scheme. Oxford: Clarendon Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Santorini</author>
</authors>
<title>Part-Of-Speech Tagging Guidelines for the Penn Treebank Project (3rd Revision, 2nd printing</title>
<date>1990</date>
<tech>Tech. Rep. MS-CIS-90-47</tech>
<institution>The University of Pennsylvania</institution>
<location>Philadelphia, PA</location>
<contexts>
<context>here were also several cases which were unclear, such as the distinction between adjective (JJ) and noun (NN) in Brown-PTB, which is perhaps not adequately explicated for compounds in the guidelines (Santorini, 1990), leading to variation as in (4). (4) a. ... through most of the liquid/JJ helium . b. ... to isolate the tube ... from the liquid/NN helium surface Finally, many of the variations reveal constructio</context>
</contexts>
<marker>Santorini, 1990</marker>
<rawString>Santorini, Beatrice (1990). Part-Of-Speech Tagging Guidelines for the Penn Treebank Project (3rd Revision, 2nd printing). Tech. Rep. MS-CIS-90-47, The University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>D Christopher</author>
</authors>
<date>2000</date>
<marker>Toutanova, Christopher, 2000</marker>
<rawString>Toutanova, Kristina and Christopher D. Manning (2000).</rawString>
</citation>
<citation valid="true">
<title>Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger</title>
<booktitle>In Proceedings of EMNLP/VLC-2000. Hong Kong</booktitle>
<marker></marker>
<rawString>Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger. In Proceedings of EMNLP/VLC-2000. Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>Morphological features help POS tagging of unknown words across language varieties</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing</booktitle>
<contexts>
<context>, being unsure of its precision for even one corpus, we use localized variation nuclei. For other languages, other contextual features might be more appropriate, such as affix information (cf., e.g., Tseng et al., 2005). The claim is further that these contexts only predict the same basic category, such as verb; more fine-grained distinctions may or may not be present across corpora, and thus by predicting the same</context>
</contexts>
<marker>Tseng, Jurafsky, Manning, 2005</marker>
<rawString>Tseng, Huihsin, Daniel Jurafsky and Christopher Manning (2005). Morphological features help POS tagging of unknown words across language varieties. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
</citationList>
</algorithm>

