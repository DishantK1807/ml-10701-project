We trained an IBM style translation model (Brown et al., 1990) using GIZA++ (Och and Ney, 2000) on the 500 test lines used in our experiments paired with corresponding English lines from an online Bible.
J90-2002 P00-1056
(Resnik and Melamed, 1997)).
A97-1050
We model each subsequence a0 a10 as being transformed into an OCR subsequence a0 a10, so a2a4a3 a0 a5 a37 a9 a7 a5 a0 a5 a1 a7 a10 a2a4a3 a11 a0 a12 a5a15a14a15a14a16a14a15a5 a0 a35 a20 a9 a7 a5 a0 a5a17a1 a7 a14 and we assume each a0 a10 is transformed independently, allowing a2a4a3 a11 a0 a12 a5a15a14a15a14a16a14 a5 a0 a35 a20 a9 a7 a5 a0 a5a17a1 a7a76a75 a6 a77 a10a79a78 a12 a2a4a3 a0 a10 a9a0 a10 a7 a14 Any character-level string error model can be used to define a2a4a3 a0 a10 a9a0 a10 a7 ; for example Brill and Moore (2000) or Kolak and Resnik (2002).
P00-1037
translation lexicon entries were scored according to the log likelihood ratio (Dunning, 1993) (cf.
J93-1003
We anticipate applying the technique in order to retarget cross-language IR technology â€” the results of Resnik et al.(2001) demonstrate that even noisy extensions to dictionary-based translation lexicons, acquired from parallel text, can have a positive impact on cross language information retrieval performance.
H01-1033
