Yarowsky, David (1993).
H93-1052
While the strategy is universally applicable to any tokenization ambiguity resolution, here we will only examine its performance in the resolution of critical ambiguities (Guo 1997), for ease of direct comparison with works in the literature.
J97-4004
Note, the emphasis here is not on producing a unique "correct" tokenization but on managing and minimizing tokenization inconsistencyL 3 One Tokenization per Source Noticing that all the fragments studied in the preceding section are critical fragments (Guo 1997) from the same source, it becomes reasonable to accept the following hypothesis.
J97-4004
The actual implementation of the weighted finitestate transducer by Sproat et al.(1996) can be taken as an evidence that the hypothesis of one tokenization per source has already in practical use.
J96-3004
Guo, Jin (1997).
J97-4004
Consequently, the mainstream research in the literature has been focused on the modeling and utilization of local and sentential contexts, either linguistically in a rule-based framework or statistically in a searching and optimization set-up (Gan, Palmer and Lua 1996; Sproat, Shih, Gale and Chang 1996; Wu 1997; Gut 1997).
J96-3004 J96-4004 J97-3002
utilizing local and sentential constraints, what Sproat et al.(1996) implemented was simply a token unigram scoring function.
J96-3004
Wu, Dekai (1997).
J97-3002
While we have not been able to specify the notion of source used in the hypothesis to the same clarity as that of critical fragment and critical tokenization in (Guo 1997), the above empirical test has made us feel comfortable to believe that the scope of the source can be sufficiently large to cover any single domain of practical interest.
J97-4004
We admit here that, while we have been aware of the fact for long time, only after the dissemination of the closely related hypotheses of one sense per discourse (Gale, Church and Yarowsky 1992)" and one sense per collocation (Yarowsky 1993), we are able to articulate the hypothesis of one tokenization per source.
H92-1045 H93-1052
The linguistic object here is a critical fragment, i.e., the one in between two adjacent critical points or unambiguous token boundaries (Guo 1997), but not an arbitrary sentence segment.
J97-4004
(3) Maximum Tokenization: The tokenization is a critical tokenization (Guo 1997).
J97-4004
Gan, Kok-Wee; Palmer, Martha; and Lua, Kim-Teng (1996).
J96-4004
Sproat, Richard, Chilin Shih, Villiam Gale, and Nancy Chang (1996).
J96-3004
