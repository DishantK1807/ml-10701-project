This cubing concept was introduced in another context by (Knight and Yamada, 1999).
J99-4005 W99-0906
Machine translation has code-like characteristics, and indeed, the initial models of (Brown et al., 1993) took a word-substitution/transposition approach, trained on a parallel text.
J93-2003
Such methods have also been a key driver of progress in statistical machine translation, which depends heavily on unsupervised word alignments (Brown et al., 1993).
J93-2003
(This result is equivalent to Knight & Yamada [1999]’s 4% error, which did not count extra incorrect phonemes produced by decipherment, such as pronunciations of silent letters).
J99-4005 W99-0906
Figures 1 and 2 show standard EM algorithms (Knight, 1999) for the case in which we have a bigram P(p) model (driven by a two-dimensional b table of bigram probabilities) and a one-for-one P(c a0 p) model (driven by a two-dimensional s table of substitution probabilities).
J99-4005 W99-0906
This is similar to Model 3 of (Brown et al., 1993), but without null-generated elements or re-ordering.
J93-2003
We follow (Knight and Yamada, 1999) in using Spanish as an example.
J99-4005 W99-0906
In cases like (Yarowsky, 1995), unsupervised methods offer accuracy results than rival supervised methods (Yarowsky, 1994) while requiring only a fraction of the data preparation effort.
P94-1013 P95-1026
Decipherment This section expands previous work on phonetic decipherment (Knight and Yamada, 1999).
J99-4005 W99-0906
The model bootstrapping is good for dealing with too many parameters; we see a similar approach in Brown et al’s (1993) march from Model 1 to Model 5.
J93-2003
