Automatic Acquisition of Language Model 
based on Head-Dependent Relation between Words 
Seungmi Lee ~nd Key-Sun Choi 
Department of Computer Science 
Center for Artificial Intelligence Research 
Korea Advanced Institute of Science a.nd Technology 
e-mail: {leesm, kschoi}©world.kaist.ac.kr 
Abstract 
Language modeling is to associate a sequence 
of words with a priori probability, which is a 
key part of many natural language applications 
such as speech recognition and statisticM lna
chine translation. In this paper, we present a 
language modeling based on a kind of simple 
dependency grammar. Tile grammar consists 
of hea.d-dependellt relations between words and 
cau be learned automatically from a raw corpus 
using the reestimation algorithm which is also 
introduced in this paper. Our experiments show 
that the proposed model performs better than 
u-gram models at 11% to 11.5% reductions in 
test corpus entropy. 
1 Introduction

Language modeling is to associate a priori prob
ttbility to a sentence. It is a key part of many 
natural language applications such a,s speech 
recognition a.nd sta, tistica, l machine translation. 
Previous works for lallguage modeling Call be 
broadly divided into two approaches; oue is 11
gram-based and the other is grammar-I)ased. 
N-gram model estinlates the probability of a 
sentence a.s the product of tile probability of 
each word in tile sentence. It, assumes that 
probM)ility of the nth word is dependent on 
the previous n 1 words. The n-gram prob
abilities are estimated by simply counting the 
n-gram frequencies in a trMnillg corpus. In 
some cases, class (or part of speech) 11-grams 
are used instead of word n-grams(Brown et al., 
1992; Chang and Cheu, 1996). N-gram model 
has beeu widely used so far, but; it has always 
been clear that n-gram can not represent long 
distance dependencies. 
In contrast with n-gram model, grammar
based approa.ch assigns syntactic structures to 
a sentence and computes the l)robability of tile 
sentence using the probabilities of the struc
tures. Lollg distance dependencies can be rep
resented well by means of the structures. Tile 
approach usuMly makes use of phrase struc
ture grammars such as probabilistic context-free 
grammar and recursive transition 11etwork(Lari 
a11d Young, 1.991; Snefl', 1992; Chen, 1996). In 
the approach, however, a sentence which is not 
accepted by the grammar is assigned zero prob
M)ility. Thus, the grammar nlust have broad
coverage so that any sentence will get non-zero 
probability. But acquisition of such a. robust 
grammar has been ktlown to be very difficult. 
Due to tile difficulty, some works try to use an 
integrated model of grammar and l>gram com
I)ensa.tillg each other(McCandless, 1994; Meteer 
and R.ohlicek, 1993). Given a robust grammar, 
gramnmr-based language nmdeling is expected 
to be more powerful and compact in model size 
than n-gram-based one. 
In this paper we present a lallguage modeling 
based on a kind of simple dependency gram
ma.r. The grammar consists of head-delmndent 
relatiolls between words and ca11 be learned au
tomatically from a ra.w corpus using the rees
tima.tion algorithm which is Mso introduced in 
this paper. Based on the dependencies, a sen
tellce is anMyzed and assigned syntactic struc
tures by which long distance dependences are 
represented. Because the model can be thought 
of as a linguistic bi-gram lnodel, the snmothillg 
functions of n-gram models can be applied to it. 
Thus, the model can be robust, adapt easily to 
new domains, a11d be effective. 
The paper is organized as follows. We intro
d11ce some definitions and notations for the de
pelldency grammar aud the reestimation algo
rithm in section 2, a.ud explain the algorithm in 
section 3. hi section 4, we show the experhne11
tal results for the suggested nlodel compared to 
n-gram models. Finally, section 5 concludes this 
paper. 
2 A
Simple Dependency Grammar 
Ill this paper, we assunm a kind of simt)le de
I)elldency gramnlar which describes a, language 
723 
by a set of head-dependent relations between 
words. A sentence is analyzed by establishing 
dependency links between individual words in 
the sentence. A dependency analysis, D, of a 
sentence can be represented with arrows point
ing from head to dependent as depicted in Fig
ure 1. For structural generality, we assume that 
there is always a marking tag, "EOS"(End of 
Sentence), at the end of a sentence and it has 
the head word of the sentence as its own depen
dent("gave" in Figure 1). 
I gave him a book EOS 
Figure 1: An example dependency analysis 
A D is a set of inter-word dependencies which 
satisfy the following conditions: (1) every word 
in the sentence has its head in the sentence ex
cept the head word of the sentence. (2) every 
word can have only one head. (3) there is nei
ther crossing nor cycle of dependencies. 
The probabilistic model of the simple depen
dency grammar is given by 
p(sentence) = 
D 
=Ell 
D x-~yED 
v(x + y), 
where p(x-+y) = p( lx) 
freq(x --+ y) 
E~ freq(m --+ z)" 
Complete-Link and Complete-Sequence 
Here, we define complete-link and complete
sequence which represent partial Ds for sub
strings. They are used to construct overall 
Ds and used as the basic structures for the rees
timation algorithm in section 3. 
A set of dependency relations on a word se
quence, wi,j 1, is a complete-link when the fol
lowing conditions are satisfied: 
• there is (wi -+ wj) or (wl e-wj) exclu
sively. 
• Every inner word has a, head in the word 
sequence. 
• Neither crossing nor cycle of dependency 
relations is allowed. 
1We use wi for ith word in a sentence and wi,j for the 
word sequence from wi to ws(i < j). 
' her second child the bus 
Figure 2: Example complete-links 
A complete-link has direction. A complete-link 
on wi5 is said to be "rightward" if the outermost 
relation is (wi ~ wj), and "leftward" if the rela
tion is (wi +-wj). Unit complete-link is defined 
on a string of two adjacent words, wi,i+l. In 
Figure 2, (a} is a rightward complete-link, and 
both of (b) and (c) are leftward ones. 
bird in the cage the bus book 
Figure 3: Example complete-sequences 
A complete-sequence is a sequence of 0 or 
more adjacent complete-links that have the 
same direction. A unit complete-sequence is de
fned on a string of one word. It is 0 sequence 
of complete-links. The direction of a complete
sequence is determined by the direction of the. 
component complete-links. In Figure 3, (a) is a. 
rightward complete-sequence composed of two 
complete-links, and (b) is a leftward one. (c) is a 
complete-sequence composed of zero complete
links, and it can be both leftwa.rd a.nd rightwa.rd. 
The word of "complete" means that the de
pendency relations on the inner words are com
pleted and that consequently there is no need 
to process further on them. From now on, 
we use L~(i,j)/Lt(i,j) for rightward/leftward 
complete-links and S~(i,j)/&(i,j) for right
ward/leftward complete-sequences on wi, j. 
Any complete-link on wi, j can be viewed as 
the following combination. 
• L~(i,j): {(wi --+ wj), S,.(i, m), Sl(m+l,j)} 
• Ll(i,j): {(wi e-wj), S,.(i, m), .S't(m+l,j)} 
for am(i<m<j). 
Otherwise, the set of dependencies does not sat
isfy the conditions of no crossing, no cycle and 
no multiple heads and is not a complete-link any 
more. 
Similarly, any complete-sequence on wi,j can 
be viewed as the following combination. 
• Sr(i,j): {Sr(i,m), Lr(m,j)} 
• &(i,j): {Lt(i,m), St(re,j)} 
for a m(i <_ m < j) 
hi the case of conlp\[ete-sequence, we can 
prevent mnltiple constructions of lhe same 
724 
complete-sequence by tile above combina.tional 
restriction. 
....... ~._SW..E_ON ...... -"" 
Figure 4: Abstract representation of D 
Figure 4 shows an abstract representation of 
a '/9 of an n-word sentence. When wk(1 _< k < 
n) is the hea.d of the sentence, a.ny D of ttm 
sentence can be represented by a SI(1, EOS) 
uniquely by the assumption that there is always 
the dependency relation, (wk +-wEos). 
3 Reestimation
Algorithm 
The reestimation a.lgorithm is a varia.tion of 
Inside-Outside a, lgorithm(aelinek et a.l., 1990) 
a(h~pted to dependency gra.mula.r. In this sec
tion we first define the inside-outside probabili
ties of complete-links and complete-sequences, 
and then describe the reestimation a.lgorithm 
based on them 2. 
In the followings, fl indicates inside probabil
ity and a, is for outside probability. The su
perscripts, 1 and s, a.re used for "complete-link" 
a.nd "complete-sequence'" respectively. The sub
scripts indica, te direction: r for "rightward" and 
1 for "leftward". 
The inside probabilities of cmnplete-links 
(L,.(i,j), Lt(i,j)) and complete-sequences 
(S~ (i, j), ,5\] (i, j)) a.re as follows. 
j-1 
\[~.(i,j) = ~ p(wi -4 ,tj)/J,.(, m)flg(m + 1,j). 
7tz~--i 
j-1 
fl\[(i,j) ~ p(wi +-,,j)\[~,. (i, m)fl/~(m + i,j). 
j--1 
o;(i,j) = ~ ;~;(~,,O/~\[.(,,,,,J). 
j 
fli'(i,j) = ~ fl\[(i,m)fli~(m,J) • 
m=i+l 
The basis t)robabilities are: 
f4l~(i,i + 1)= p(wi -4 wi+l) 
fl\[(i,i + 1)=p(wi ¢--wi+,) 
fl,1(i, i) = fli'(i, i) = 1 
/Jis(i, ~'(),S') = p(Wl,n) 
2A lit't'le nlore detailed explanation of the expressions 
can be fmmd in (Lee and Choi, 1997}. 
fl,~(i,i+ 1) = p(L,.(i, i+ 1)) = p(wi ~ wi+l) 
fliP(i, i+ 1)= p(Lt(i, i+ l)) = p(wi +-wi+~). 
fli~(1,EOS) is the sentence probability be
cause every dependency analysis, 79, is repre
sented by a St(l, EO,5') a.nd fl}~(1, HOS) is sum 
of tile probability of every St(l, EO, ). 
Ttle outside probabilities for complete
links (r,. (i, j), Lt(i, j)) and con, plete-sequences 
(S,.(i,j), St(i,j)) are a.s follows. 
i s s 
a,l.(i,j ) = ~ %(v,j)fl,.(~,i). 
v= l 
a,l(i,j ) = ~-\]a,~(i,h)\[~i~(j,h). 
h=j 
~,,,.(,,j) = ,_, ,~.,.(,,h)y~(j,h) 
h=j+l 
+a.\[(i, h)fl}~(j + 1, h)p(wi --+ w,~) 
+4(/, h)Og(j + l, h)p(w~ ~w,O. 
i-I ,tr(i,j) = ~ ,tt(~,j)fl\[(,,,i) 
+(tl.(v, j)fl;?(v, il)p(wv -4 wj) 
+o'l(v,j)fl;~.(v, il)p(wv +wj). 
The basis probability is 
o'7(1, EO,9) = 1. 
Given a training corpus, the initial grammar 
is just a. list. of all pairs of unique words in 
the corpus. Tile initial pairs represent the ten
ta, tive head-dependent relations of the words. 
And the initial probabilities of the pairs can 
be given randomly. Tile training starts with 
tile initia.l grammar. The train corl>us is an: 
alyzed with the grammar a.nd the occurrence 
frequency of ea,ch dependency rela,tion is cal
culated. Based on the frequen(:ies, probabili
ties of dependency rela£ions are recah:ulated by 
C(w v -4 wq) Tile l)rocess p~(w,, -4 w~) = Y:w, c(w,, -4 w,)' 
continues until the entropy of the tra.ining cot-
pus becomes the nfinimunl. The frequency of 
occurrence, C(wi -4 wj), is ca.h:ulated t)y 
c(,,~f -~ w) = y\]v(r'l,-,,.)o..(.,, -~ w, ~, ,,;,,,,) 
"D 
1 a'~(i,j)fl~.(i,j) 
p(wl,,,) 
where Occ(Wi -4 Wj, "19, 'u.'l,n) is 1 if the (tet)en
dency rela, tion, (wi -4 wj), is used in the "D, 
725 
and 0 otherwise. Similarly, the occurrence fre
quency of the dependency relation, (wi ~ w5), 
is computed by P(!,,dw, °~l(i'J)fll(i'J)" 
4 Preliminary
experiments 
We have experimented with three language 
models, tri-gram model (TRI), bi-gram model 
(BI), and the proposed model (DEP) on a raw 
corpus extracted from KAIST corpus a. The raw 
corpus consists of 1,589 sentences with 13,139 
words, describing animal life in nature. We 
randomly divided the corpus into two parts: a 
training set of 1,445 sentences and a test set of 
144 sentences. And we made 15 partial training 
sets which include the first s sentences in the 
whole training set, for s ranging from 100 to 
1,445 sentences. We trained the three language 
models for each partial training set, and tested 
the training and the test corpus entropies. 
TRI and BI was trained by counting the oc
currence of tri-grams and N-grams respectively. 
DEP was trained by running the reestimation 
algorithm iteratively until it converges to an op
timal dependency grammar. On the average, 26 
iterations were done for the training sets. 
Smoothing is needed for language modeling 
due to the sparse data problem. It is to com
pensate for the overestimated and the under
estimated probabilities. Smoothing method it
self is an important factor. But our goal is not 
to find out a better smoothing method. So we 
fixed on an interpolation method and applied it 
for the three models. It can be represented as 
(McCandless, 1994) 
\[~,,(wi\[wi-,,+l, ..., wi-1) = AP,(wilwi-,,+l, ..., wi-1) 
where ,X = C(wl, ..., wn-l) 
C(w,,..., Wn-1) + 
The K, is the global smoothing factor. Tile big
ger the K~, the larger the degree of smoothing. 
For the experiments we used 2 for K.,. 
We take the performance of a language model 
to be its cross-entropy on test corpus, 
s 1 
IVt i=1 
3KAIST (Korean Advanced Institute of Science and 
Technology) corpus has been under construction since 
1994. It consists of raw text collection(45,000,000 
words), POS-tagged collection(6,750,000 words), and 
tree-tagged collection(30,000 sentences) at present. 
where tile test corpus contains a total of IV\[ 
words and is composed of S sentences. 
3.4 , , , , , , , 
2.8 
>',~ 2.6 
2.4 e" 
tu 2.2 (TRI model) 2 
1.8 
1.6 
1.4 
200 400 600 800 1000 1200 1400 1600 
No. of training sentences 
Figure 5: Training corpus entropies 
Figure 5 shows tile training tort)us entropies 
of the three models. It is not surprising that 
DEP performs better than BI. DEP can be 
thought of as a kind of linguistic bi-granl model 
in which long distance dependencies can be rep
resented through the head-dependent relations 
between words. TRI shows better performance 
than both BI and DEP. We think it. is because 
TRI overfits the training corpus, judging from 
the experimental results for the test corpus. 
9.5 ! i ! i i ! i 
8.5 
ua 7.5 
Jg O'RI model) -+-
7 / (DEP model) .-B-
13 
6.5 i i i i i t i 
0 200 400 600 800 1000 1200 1400 600 
No. of training sentences 
Figure 6: Test corpus entropies 
For the test corpus, BI shows slightly bet
ter performance than TRI as depicted in Fig
ure 6. Increase in tile order of n-gram from 
two to three shows no gains in entropy reduc
tion. DEP, however, Shows still better per
formance than the n-gram models. It shows 
about 11.5% entropy reduction to BI and at)out 
11% entropy reduction to TRI. Figure 7 shows 
the entropies for the mixed corpus of training 
and test sets. From tile results, we can see 
that head-dependent relations between words 
are more useful information than the naive n
gram sequences, for language modeling. We can 
see also that the reestimation algorithm can find 
out properly the hidden head-dependent rela
tions between words, from a raw corpus. 
726 
13. g 
t
UJ 
Ill 
"5 
O 
Z 
10 
7 
6 
5 
4 
3 
0 
i i i i i i i 
f (BI model) (TRI model) .4--
(DEP model) 
200 400 600 800 1000 1200 1400 1600 
No. of training sentences 
Figure 7: Mixed corpus entropies 
60000 
50000 
40000 
30000 
20000 
10000 
0 
0 
i ! i i i ! i (DEP model) ~ 
(I"RI model) -+--
~'T I | I I I 1 
200 400 600 800 1000 1200 1400 1600 
No. of training sentences 
Figure 8: Model size 
Related to the size of model, however, DEP 
has much more parameters than TRI and BI 
as depicted in Figure 8. This can be a serious 
l)roblem when we create a language model from 
a large body of text. In the experiments, how
ever, DEP used the grammar acquired automat
ically as it is. In the grammar, many inter-word 
dependencies have probabilities near 0. If we 
exclude su(:h dependencies as was ext)erimented 
for n-grams by Seymore and Rosenfeld (1996), 
we \[nay get much more compact DEP model 
with very slight increase in entropy. 
5 Conclusions

In this paper, we t)resented a language model 
based on a, kind of simple dependency gram
mar. The grammar consists of head-dependent 
relations between words and can be learned au
tomatically from a raw corpus by the reestima
tion algorithtn which is also introduced in this 
paper. By the preliminary experiments, it was 
shown that the proposed language model per
forms better than n-gram models in test cor
pus entropy. This means tha,t the reestimatiou 
Mgorithm can find out the hidden information 
of head-del)endent relation between words in a 
raw corl)us, and the information is more useful 
than the naive word sequences of u-gram, for 
language modeling. 
We are planning to experiment the perfor
mance of the proposed language model for large 
corpus, for various domains, and with various 
smoothing methods. For the size of the mode.l, 
we are planning to test the efl'ects of excluding 
the dependency relations with near zero proba
bilities. 

References 

P. F. Brown, V. J. Della. Pietra, P. V. (leSouza., 
J. C. Lai, and R. L. Mercer. 1992. "Class
Based n-gram Models of Natural Language". 
Computational Linguistics, 18(4):467-480. 

C. Chang and C. Chen. 1996. "Application Is
sues of SA-class Bigram Language Models". 
Computer Processing of Oriental LaTtguagcs, 10(1):1-15. 

S. F. Chen. 1996. "Building Probabilistic 
Models for Natural Language". Ph.D. the
sis, Havard University, Carat)ridge, Mas
sachusetts. 

F. Jelinek, J. D. Lafferty, and R. L. Mercer. 
1990. "Basic Methods of Probabilisti(: Con
text Free Grammars". Technical report, IBM 
T.J. Watson Research Center. 

K. Lari and S. J. Young. 1991. "Applications 
of stochastic context-free grammars using the 
inside-outside algorithm". 6'o'mputer Speech 
and Language, 5:237-257. 

S. Lee and K. Choi. 1997. "Reestimation and 
Best-First Parsing Algorithm fi)r Probabilis
tic Dependency Grammar". h! WVLC-5, 
pages 11-21. 

M. K. McCandless. 19!)4. "Automatic Acquisi
tion of Langua.ge Models tbr St)eech Recog~ 
nition". Master's thesis, Massa, chusetts Insti
tute of Technology. 

M. Meteer and J.R. Rohlicek. 1993. "Statis
tical Language Modeling Combining N-gram 
and Context-free Grammars". In ICAS,S'P
93, volume II, pages 37-40, January. 

K. Seymore and R. Rosenfeld. 1,996. "ScMable 
Trigram Backoff Language Models". Techni
ca.l Report CMU-CS-96-139, C~mmgie Mellon 
University. 

S. Sneff. 1992. "TINA: A natural language sys
tem \['or Sl)oken language at)f)lications". Co'm
putational Linguistics, 18(1):61-86. 

