periments on a English-French subset of the Europarl corpus used for the ACL 2005 SMT evaluations (Koehn and Monz, 2005).
W05-0820
For comparison, we use the MT training program, GIZA++ (Och and Ney, 2003), the phrase-base decoder, Pharaoh (Koehn et al., 2003), and the wordbased decoder, Rewrite (Germann, 2003).
J03-1002 N03-1010 N03-1017
We attribute the difference in M3/4 scores to the fact we use a Viterbi-like training procedure (i.e., we consider a single configuration of the hidden variables in EM training) while GIZA uses pegging (Brown et al., 1993) to sum over a set of likely hidden variable configurations in EM.
J93-2003
Representation of IBM MT Models In this section we present a GM representation for IBM model 3 (Brown et al., 1993) in fig.
J93-2003
We perform MT ex1We refer to the HMM MT model in (Vogel et al., 1996) as M-HMM to avoid any confusion.
C96-2141
