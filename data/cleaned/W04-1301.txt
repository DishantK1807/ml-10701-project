1 Aquantitativeevaluationofnaturalisticmodelsoflanguageacquisition;the ef ciencyoftheTriggeringLearningAlgorithmcomparedtoaCategorial GrammarLearner PaulaButtery NaturalLanguageandInformationProcessingGroup, ComputerLaboratory,CambridgeUniversity, 15JJThomsonAvenue,Cambridge,CB30FD,UK paula.buttery@cl.cam.ac.uk Abstract Naturalistictheoriesoflanguageacquisitionassume learnerstobeendowedwithsomeinnatelanguage knowledge.
Thepurposeofthisinnateknowledge istofacilitatelanguageacquisitionbyconstrainingalearner’shypothesisspace.
Thispaperdiscussesanaturalisticlearningsystem(aCategorial GrammarLearner(CGL))thatdiffersfromprevious learners(suchastheTriggeringLearningAlgorithm (TLA)(GibsonandWexler,1994))byemployinga dynamicde nitionofthehypothesis-spacewhich isdrivenbytheBayesianIncrementalParameter Settingalgorithm(Briscoe, 1999).
Wecompare theef ciencyoftheTLAwiththeCGLwhenacquiringanindependentlyandidenticallydistributed English-likelanguageinnoiselessconditions.
We showthatwhenconvergence tothetargetgrammaroccurs(whichisnotguaranteed),theexpected number of steps toconvergence for the TLAis shorterthanthatfortheCGLinitializedwithuniformpriors.
However,theCGLconvergesmore reliablythantheTLA.Wediscussthetrade-offof ef ciencyagainstmorereliableconvergencetothe targetgrammar.
1 Introduction
Anormalchildacquiresthelanguageofherenvironmentwithoutanyspeci ctraining.
Chomsky (1965)claimsthat,giventhe relativelyslightexposure toexamplesand remarkablecomplexity oflanguage,itwouldbe anextraordinaryintellectualachievement forachildtoacquirealanguage ifnotspeci callydesignedtodoso.HisArgument fromthePovertyoftheStimulussuggeststhatifwe knowX,andXisundeterminedbylearningexperiencethenXmustbeinnate.Foranexampleconsiderstructuredependencyinlanguagesyntax: AquestioninEnglishcanbeformedbyinvertingtheauxiliaryverbandsubjectnoun-phrase:(1a) Dinahwasdrinkingasaucerofmilk ;(1b) was Dinahdrinkingasaucerofmilk?
Uponexposuretothisexample,achildcouldhypothesizein nitelymanyquestion-formationrules, suchas:(i)swapthe rstandsecondwordsinthe sentence;(ii)frontthe rstauxiliaryverb;(iii)front wordsbeginningwithw.
The rsttwooftheserulesarerefutedifthechild encountersthefollowing: (2a) thecatwhowas grinningatAlicewasdisappearing ;(2b) wasthe catwhowasgrinningatAlicedisappearing?
Ifachildistoconvergeuponthecorrecthypothesisunaidedshemustbeexposedtosuf cientexamplessothatallfalsehypothesesarerefuted.Unfortunatelysuchexamplesarenotreadilyavailable inchild-directedspeech;eventheconstructionsin examples(2a)and(2b)arerare(Legate,1999).To compensateforthislackofdataChomskysuggests thatsomeprinciplesoflanguagearealreadyavailableinthechild’smind.
Forexample,ifthechild hadinnately known thatallgrammarrulesare structurally-dependentuponsyntaxshewouldnever havehypothesizedrules(i)and(iii).
Thus,ChomskytheorizesthatahumanmindcontainsaUniversalGrammarwhichde nesahypothesis-spaceof legal grammars.1 Thishypothesis-spacemustbe bothlargeenoughtocontaingrammar’sforallof theworld’slanguagesandsmallenoughtoensure successful acquisition giventhesparsity ofdata.
Languageacquisitionistheprocessofsearchingthe hypothesis-spaceforthegrammarthatmostclosely describesthelanguageoftheenvironment.
With estimatesofthenumberoflivinglanguagesbeing around6800(Ethnologue,2004)itisnotsensibleto modelthehypothesis-spaceofgrammarsexplicitly, ratheritmustbemodeledparametrically.Language acquisitionisthentheprocessofsettingtheseparameters.
Chomsky(1981)suggestedthatparametersshouldrepresentpointsofvariationbetween languages, howevertheonlyrequirementforparametersisthattheyde nethecurrenthypothesisspace.
1DiscussionofstructuraldependenceasevidenceoftheArgumentfromthePovertyofStimulusisillustrative,thesigni cancebeingthatinnateknowledgeinanyformwillplace constraintsonthehypothesis-space 2 Theproperties oftheparameters usedbythis learner(theCGL)areasfollows:(1)Parametersare lexical;(2)Parametersareinheritancebased;(3)Parametersettingisstatistical.
1-LexicalParameters TheCGLemploysparametersettingasameans toacquirealexicon;differingfromotherparametriclearners,(suchastheTriggeringLearningAlgorithm(TLA)(GibsonandWexler,1994)andthe StructuralTriggersLearner(STL)(Fodor,1998b), (SakasandFodor, 2001)) whichacquire general syntacticinformationratherthanthesyntacticpropertiesassociatedwithindividualwords.2 Inparticular,acategorialgrammarisacquired.
Thesyntacticpropertiesofawordarecontainedin itslexicalentryintheformofasyntacticcategory.
Awordthatmaybeusedinmultiplesyntacticsituations(orsub-categorizationframes)willhavemultipleentriesinthelexicon.
Syntacticcategoriesareconstructedfroma nite setofprimitivecategoriescombinedwithtwooperators(=andn)andarede nedbytheirmembers abilitytocombinewithotherconstituents;thusconstituentsmaybethoughtofaseitherfunctionsor arguments.
Thearguments of afunctional constituent are showntotherightoftheoperatorsandtheresult totheleft.Theforwardslashoperator(=)indicates thattheargumentmustappeartotherightofthe functionandabackwardslash(n)indicatesthatit mustappearontheleft.
Considerthefollowing CFGstructurewhichdescribesthepropertiesofa transitiveverb: s ! npvp vp ! tvnp tv ! gets, nds,...
Assumethatthereisasetofprimitivecategories fs;npg.
A vp mustbeinthecategoryoffunctionalconstituentsthattakesanpfromtheleftand returnsans.
Thiscanbewritten snnp.
Likewise a tv takesan np fromtherightandreturnsa vp (whosetypewealreadyknow).Atvmaybewritten(snnp)=np.
Rulesmaybeusedtocombinecategories. We assumethatourlearnerisinnatelyendowedwiththe rulesoffunctionapplication,functioncomposition andgeneralizedweakpermutation(Briscoe,1999) (see gures1and2).
ForwardApplication(>) X=Y Y ! X 2Theconceptoflexicalparametersandthelexical-linking ofparametersistobeattributedtoBorer(1984).
BackwardApplication(<) Y XnY ! X ForwardComposition(> B) X=Y Y=Z ! X=Z BackwardComposition(< B) Y nX ZnY ! XnZ GeneralizedWeakPermutation(P) ((X j Y1)::: j Yn) ! ((X j Yn)::: j Y1) wherejisavariableovernand=.
Alice np may (snnp)=(snnp) eat (snnp)=np > B (snnp)=np the cake np > snnp < s Figure1:Illustrationofforward/backwardapplication(>,<)andforwardcomposition(> B) the np=n rabbit n that (nnn)=(s=np) she np saw (snnp)=np P (s=np)nnp < (s=np) > nnn < n > np Figure2:Illustrationofgeneralizedweakpermutation(P) Thelexiconforalanguagewillcontaina nite subsetofallpossiblesyntacticcategories,thesizeof whichdependsonthelanguage.
Steedman(2000) suggeststhatforEnglishthelexicalfunctionalcategoriesneverneedmorethan veargumentsandthat theseareneededonlyinalimitednumberofcases suchasfortheverbbetinthesentenceIbetyou ve poundsforEnglandtowin.
ThecategorialgrammarparametersoftheCGL are concerned withde ning the set of syntactic categoriespresentinthelanguageoftheenvironment.Convergingonthecorrectsetaidsacquisition byconstrainingthelearner’shypothesizedsyntactic categoriesforanunknownword.Aparameter(with 3 valueofeitherACTIVEorINACTIVE)isassociatedwitheverypossiblesyntacticcategorytoindicatewhetherthelearnerconsidersthecategorytobe partofthetargetgrammar.
Some previous parametric learners (TLA and STL)havebeenprimarilyconcernedwithoverall syntacticphenomenaratherthanthesyntacticpropertiesofindividualwords.
Movementparameters (suchastheV 2parameteroftheTLA)maybecapturedbytheCGLusinginnaterulesormultiplelexicalentries.Forinstance,DutchandGermanword orderiscapturedbyassumingthatverbsinthese languagessystematicallyhavetwocategories,one determiningmainclauseorderandtheothersubordinateclauseorders.
2-InheritanceBasedParameters Thecomplexsyntactic categories ofacategorial grammarareasub-categorizationofsimplercategories;consequentlycategoriesmaybearrangedin ahierarchywithmorecomplexcategoriesinheriting fromsimplerones.Figure3showsafragmentofa possiblehierarchy.Thishierarchicalorganizationof parametersprovidesthelearnerwithseveralbenets:(1)Thehierarchycanenforceanorderonlearning;constraintsmaybeimposedsuchthataparent parametermustbeacquiredbeforeachildparameter(forexample,inFigure3,thelearnermustacquireintransitiveverbsbeforetransitiveverbsmay behypothesized).
(2)Parametervaluesmaybeinheritedasamethodofacquisition.(3)Theparametersarestoredef ciently.
s-ACTIVE‘‘ ‘‘‘‘ s=s snnp-ACTIVEXX XXX [snnp]=np-ACTIVE [snnp]=[snnp] Figure3: Partialhierarchyofsyntacticcategories.
EachcategoryisassociatedwithaparameterindicatingeitherACTIVEorINACTIVEstatus.
3-StatisticalParameterSetting Thelearnerusesastatisticalmethodtotrackrelativefrequenciesofparameter-setting-utterancesin theinput.3 WeusetheBayesianIncrementalParameterSetting(BIPS)algorithm(Briscoe,1999) tosetthecategorialparameters.
Suchanapproach setstheparameterstothevaluesthataremostlikely givenalltheaccumulatedevidence.Thisrepresents 3OtherstatisticalparametersettingmodelsincludeYang’s Variationalmodel(2002)andtheGuessingSTL(Fodor,1998a) acompromisebetweentwoextremes:implementationsoftheTLAarememorylessallowingaparametervaluestooscillate;someimplementationsofthe STLsetaparameteronce,foralltime.
UsingtheBIPSalgorithm,evidencefromaninpututterancewilleitherstrengthenthecurrentparametersettingsorweakenthem.Eitherway,there isre-estimationoftheprobabilitiesassociatedwith possibleparametervalues.Valuesareonlyassigned whensuf cientevidencehasbeenaccumulated,i.e.
oncetheassociatedprobabilityreachesathreshold value.
Byemployingthismethod,itbecomesunlikelyforparameterstoswitchbetweensettingsas theconsequenceofanerroneousutterance.
AnotheradvantageofusingaBayesianapproach isthatwemaysetdefaultparametervaluesbyassigningBayesianpriors; ifaparameter’sdefault valueisstronglybiasedagainsttheaccumulatedevidencethenitwillbedif culttoswitch.Also,weno longerneedtoworryaboutambiguityinparametersetting-utterances(Clark,1992)(Fodor,1998b):the Bayesianapproachallowsustosolvethisproblem forfree sinceindeterminacyjustbecomesanother caseoferrorduetomisclassi cationofinputdata (ButteryandBriscoe,2004).
2 OverviewoftheCategorialGrammar
Learner Thelearningsystemiscomposedofathreemodules:asemanticslearningmodule,syntaxlearning moduleandmemorymodule.
Foreachutterance heardthelearnerreceivesaninputstreamofword tokenspairedwithpossiblesemantichypotheses.
Forexample,onhearingtheutterance Dinahdrinks milk thelearnermayreceivethepairing:(fdinah, drinks,milkg,drinks(dinah,milk)).
2.1 TheSemanticModule
Thesemanticmoduleattemptstolearnthemapping betweenwordtokensandsemanticsymbols,building a lexicon containing the meaning associated witheachwordsense.
Thisisachievedbyanalyzingeachinpututteranceanditsassociatedsemantic hypothesesusingcross-situationaltechniques(followingSiskind(1996)).
Foratrivialexampleconsidertheutterances Alicelaughs and Aliceeatscookies ; theymight havewordtokenspairedwithsemanticexpressions asfollows:(falice,laughsg,laugh(alice)),(falice, eats,cookiesg,eat(alice,cookies)).
Fromthesetwoutterancesitispossibletoascertainthatthemeaningassociatedwiththewordtoken alicemustbealicesinceitistheonlysemanticelementthatiscommontobothutterances.
4 2.2 TheSyntacticModule Thelearningsystemlinksthesemanticmoduleand syntacticmodulebyusingatypingassumption:the semanticarityofawordisusuallythesameasits numberofsyntacticarguments.Forexample,ifitis knownthatlikesmapstolike(x;y),thenthetypingassumptionsuggeststhatitssyntacticcategory willbeinoneofthefollowingforms:anbnc,a=bnc, anb=c,a=b=cormoreconciselya j b j c(wherea,b andcmaybebasicorcomplexsyntacticcategories themselves).
Byemployingthetypingassumptionthenumber ofargumentsinaword’ssyntacticcategorycanbe hypothesized.
Thus,theobjectiveofthesyntactic moduleistodiscoverthearguments’categorytypes andlocations.
Themoduleattemptstocreatevalidparsetrees startingfromthesyntacticinformationalreadyassumedbythetypingassumption(followingButtery(2003)).
Avalidparseisonethatadheres totherulesofthecategorialgrammaraswellas theconstraintsimposedbythecurrentsettingsof theparameters.
Ifavalidparsecannotbefound thelearnerassumesthetypingassumptiontohave failedandbacktrackstoallowtyperaising.
2.3 MemoryModule
Thememorymodulerecordsthecurrentstateof thehypothesis-space.
Thesyntacticmodulerefers tothisinformationtoplaceconstraintsuponwhich syntactic categories may be hypothesized.
The moduleconsistsoftwohierarchiesofparameters whichmaybesetusingtheBIPSalgorithm: CategorialParametersdeterminewhetheracategoryisinusewithinthelearner’scurrentmodel oftheinputlanguage.
Aninheritancehierarchyof allpossiblesyntacticcategories(forupto vearguments)isde nedandaparameterassociatedwith eachone(Villavicencio,2002).
Everyparameter (exceptthoseassociatedwithprimitivecategories suchasS)isoriginallysettoINACTIVE,i.e.
no categories(exceptprimitives)areknownuponthe commencementoflearning.Acategorialparameter mayonlybesettoACTIVEifitsparentcategory isalreadyactiveandtherehasbeensatisfactoryevidencethattheassociatedcategoryispresentinthe languageoftheenvironment.
WordOrderParametersdeterminetheunderlyingorderinwhichconstituentsoccur.Theymaybe settoeitherFORWARDorBACKWARDdependingonwhethertheconstituentsinvolvedaregenerallylocatedtotherightorleft.
Anexampleis theparameterthatspeci esthedirectionofthesubjectofaverb: ifthelanguageoftheenvironment isEnglishthisparameterwouldbesettoBACKWARDsincesubjectsgenerallyappeartotheleftof theverb.Evidenceforthesettingofwordorderparametersiscollectedfromwordorderstatisticsof theinputlanguage.
3 TheacquisitionofanEnglish-type language TheEnglish-likelanguageofthethree-parameter system studied by Gibson and Wexler has the parameter settings and associated unembedded surface-stringsasshowninFigure4.
Forthistask weassumethatthesurface-stringsoftheEnglishlikelanguageareindependentandidenticallydistributedintheinputtothelearner.
Speci er Complement V2 0(Left) 1(Right) 0(off) 1.
SubjVerb 2.
SubjVerbObj 3.
SubjVerbObjObj 4.
SubjAuxVerb 5.
SubjAuxVerbObj 6.
SubjAuxVerbObjObj 7.
AdvSubjVerb 8.
AdvSubjVerbObj 9.
AdvSubjVerbObjObj 10.
AdvSubjAuxVerb 11.
AdvSubjAuxVerbObj 12.
AdvSubjAuxVerbObjObj Figure4:Parametersettingsandsurface-stringsof GibsonandWexler’sEnglish-likeLanguage.
3.1 Ef
ciencyofTriggerLearningAlgorithm FortheTLAtobesuccessfulitmustconvergeto thecorrectparametersettingsoftheEnglish-like language.BerwickandNiyogi(1996)modeledthe TLAasaMarkovprocess(seeFigure5).
Usingthismodelitispossibletocalculatethe probabilityofconvergingtothetargetfromeach startinggrammarandtheexpectednumberofsteps beforeconvergence.
ProbabilityofConvergence: ConsiderstartingfromGrammar3,aftertheprocess nishesloopingithasa 3=5 probabilityofmovingtoGrammar 4 (fromwhichitwillneverconverge)anda2=5probabilityofmovingtoGrammar 7(fromwhichitwillde nitelyconverge),therefore thereisa40%probabilityofconvergingtothetarget grammarwhenstartingatGrammar3.
5 ExpectednumberofStepstoConvergence: LetSn betheexpectednumberofstepsfromstate ntothetargetstate.Forstartinggrammars6,7and 8,whichde nitelyconverge,weknow: S6 = 1 + 56S6 (1) S7 = 1 + 23S7 + 118S8 (2) S8 = 1 + 112S6 + 136S7 + 89S8 (3) andforthetimeswhenwedoconvergefromgrammars3and1wecanexpect: S1 = 1 + 35S1 (4) S3 = 1 + 3133S3 (5) Figure6showstheprobabilityofconvergenceand expectednumberofstepstoconvergenceforeach ofthestartinggrammars.Theexpectednumberof stepstoconvergencerangesfromin nity(forstartinggrammars 2 and 4)downto 2:5 forGrammar 1.
Ifthedistributionoverthestartinggrammarsis uniformthentheoverallprobabilityofconverging isthesumoftheprobabilitiesofconvergingfrom eachstatedividedbythetotalnumberofstates: 1:00 + 1:00 + 1:00 + 1:00 + 0:40 + 0:66 8 = 0:63(6) andtheexpectednumberofstepsgiventhatyou convergeistheweightedaverageofthenumberof stepsfromeachpossiblyconvergingstate: 5:47 + 14:87 + 6 + 21:98 0:4 + 2:5 0:66 1:00 + 1:00 + 1:00 + 1:00 + 0:40 + 0:66 = 7:26(7) 3.2 Ef ciencyofCategorialGrammarLearner TheinputdatatotheCGLwouldusuallybeanutteranceannotatedwithalogicalform;theonlydata availableherehowever,issurface-stringsconsistingofwordtypes.Hence,forthepurposeofcomparisonwiththeTLAthesemanticmoduleofour learnerisby-passed;weassumethatmappingsto semanticformshavepreviouslybeenacquiredand thatthesubjectandobjectsofsurface-stringsare known.
Forexample,givensurface-string1(Subj Verb) weassume the mapping Verb 7! verb(x), whichprovidesVerbwithasyntacticcategoryofthe formajbbythetypingassumption(wherea, bare unknownsyntacticcategoriesand j isanoperator overnand=);wealsoassumeSubjtomaptoaprimitivesyntacticcategorySB,sinceitisthesubjectof Verb.
ThecriteriaforsuccessfortheCGLwhenacquiringGibsonandWexler’sEnglish-likelanguageisa lexiconcontainingthefollowing:4 Adv S=S Aux [SnSB]=[SnSB] Obj OB Verb SnSB Subj SB [SnSB]=OB [[SnSB]=OB]=OB where S (sentence), SB (subject)and OB (object)areprimitivecategorieswhichareinnatetothe learnerwith SB and OB assumedtobederivable fromthesemanticmodule.
DuringthelearningprocesstheCGLwillhave constructedacategoryhierarchybysettingappropriatecategorialparameterstotrue(seeFigure7).
Thelearnerwillhavealsoconstructedaword-order hierarchy(Figure8), settingparameterstoFORWARDorBACKWARD.Thesehierarchiesareused duringthelearningprocesstoconstrainhypothesizedsyntacticcategories.
Forthistaskthesettingoftheword-orderparametersbecomestrivial andtheirroleinconstraininghypothesesnegligible; consequently,therestofourargumentwillrelateto categorialparametersonly.Forthepurposeofthis gendir=/a aa!!! subjdir=n vargdir=/ Figure8:Word-orderparametersettingsrequiredto parseGibsonandWexler’sEnglish-likelanguage.
analysisparametersareinitializedwithuniformpriorsandareoriginallysetINACTIVE.Sincetheinputisnoiseless,theswitchingthresholdissetsuch thatparametersmaybesetACTIVEupontheevidencefromonesurface-string.
Itisarequirementoftheparametersettingdevicethattheparent-typesofhypothesizedsyntax categoriesareACTIVEbeforenewparametersare set.
Thus,thelearnerisnotallowedtohypothesizethesyntactic category foratransitive verb [[SnSB]=OB]beforeithaslearntthecategoryfor anintransitiveverb [SnSB]; thisbehaviourconstrainsover-generation.
Additionally,itisusually notpossibletoderiveaword’sfullsyntacticcategory(i.e.withoutanyremainingunknowns)unless itistheonlynewwordintheclause.
Asaconsequenceoftheseissues,theorderin whichthesurface-stringsappeartothelearneraf4Notethatthelexiconwouldusuallycontainorthographic entriesforthewordsinthelanguageratherthanwordtypeentries.
6 fectsthespeedofacquisition.
Forinstance, the learnerpreferstoseethesurface-stringSubjVerb before Subj VerbObjsothat it can acquire the maximuminformationwithoutwastinganystrings.
FortheEnglish-typelanguagedescribedbyGibsonandWexlerthelearnercanoptimallyacquire thewholelexiconafterseeingonly5surface-strings (onestringneededforeachnewcomplexsyntactic categorytobelearnt).However,thestringsappear tothelearnerinarandomordersoitisnecessaryto calculatetheexpectednumberofstrings(orsteps) beforeconvergence.
ThelearnermustnecessarilyseethestringSubj Verbbeforeitcanlearnanyotherinformation.With 12 surface-strings the probability of seeing Subj Verbis1=12andtheexpectednumberofstringsbeforeitisseenis12.Thelearnercannowlearnfrom 3surface-strings:SubjVerbObj,SubjAuxVerband AdvSubjVerb.Figure9showsaMarkovstructure oftheprocess.Fromthemodelwecancalculatethe expectednumberofstepstoconvergetobe24:53.
4 Conclusions
TheTLAandCGLwerecomparedforef ciency (expectednumberofstepstoconvergence)when acquiringtheEnglish-typegrammarofthethreeparametersystemstudiedbyGibsonandWexler.
TheexpectednumberofstepsfortheTLAwas foundtobe7:26butthealgorithmonlyconverged 63%ofthetime.Theexpectednumberofstepsfor theCGLis24:53butthelearnerconvergesmorereliably;atradeoffbetweenef ciencyandsuccess.
WithnoiselessinputtheCGLcanonlyfailifthere isinsuf cientinputstringsorifBayesianpriorsare heavilybiasedagainstthetarget.
Furthermore,the CGLcanbemaderobusttonoisebyincreasingthe probabilitythresholdatwhichaparametermaybe setACTIVE;theTLAhasnomechanismforcoping withnoisydata.
TheCGLlearnsincrementally; thehypothesisspacefromwhichitcanselectpossiblesyntactic categoriesexpandsdynamicallyand, asaconsequenceofthehierarchicalstructureofparameters, thespeedofacquisitionincreasesovertime.
For instance,inthestartingstatethereisonlya 1=12 probabilityoflearningfromsurface-stringswhereas instatek(whenallbutonecategoryhasbeenacquired)thereisa 1=2 probability.
Itislikelythat withamorecomplexlearningtaskthebene tsof thisincrementalapproachwilloutweightheslow startingcosts.RelatedworkontheeffectsofincrementallearningonSTLperformance(Sakas,2000) drawssimilarconclusions.
Futureworkhopesto comparetheCGLwithotherparametriclearners (suchastheSTL)inlargerdomains.
References RBerwickandPNiyogi.1996.Learningfromtriggers.LinguisticInquiry,27(4):605 622.
HBorer. 1984.
ParametricSyntax: CaseStudies inSemiticandRomanceLanguages.Foris,Dordrecht.
EBriscoe.1999.Theacquisitionofgrammarinan evolvingpopulationoflanguageagents.Machine Intelligence,16.
PButteryandTBriscoe.2004.Thesigni canceof errorstoparametricmodelsoflanguageacquisition.
TechnicalReportSS-04-05,AmericanAssociationofArti cialIntelligence,March.
PButtery. 2003.
Acomputationalmodelfor rst languageacquisition.InCLUK-6,Edinburgh.
NChomsky.1965.AspectsoftheTheoryofSyntax. MITPress.
NChomsky. 1981.
LecturesonGovernmentand Binding.ForisPublications.
RClark. 1992.
Theselectionofsyntacticknowledge.LanguageAcquisition,2(2):83 149.
Ethnologue. 2004.
Languages of the world, 14th edition.
SIL International.
http://www.ethnologue.com/. JFodor.
1998a. Parsingtolearn.
JournalofPsycholinguisticResearch,27(3):339 374.
JFodor.1998b.Unambiguoustriggers.Linguistic Inquiry,29(1):1 36.
EGibsonandKWexler.1994.Triggers.Linguistic Inquiry,25(3):407 454.
JLegate. 1999.
Wastheargumentthatwasmade empirical?
Ms,MassachusettsInstituteofTechnology.
WSakasandJFodor.2001.Thestructuraltriggers learner.
InSBertolo,editor,LanguageAcquisitionandLearnability,chapter5.CambridgeUniversityPress,Cambridge,UK.
WSakas.2000.AmbiguityandtheComputational FeasibilityofSyntaxAcquisition.
Ph.D.thesis, CityUniversityofNewYork.
J Siskind.
1996. A computational study of crosssituationaltechniquesforlearningword-tomeaning mappings.
Cognition, 61(1-2):39 91, Nov/Oct.
MSteedman. 2000.
TheSyntacticProcess. MIT Press/BradfordBooks.
A Villavicencio.
2002. The acquisition of a uni cation-based generalised categorial grammar.Ph.D.thesis,UniversityofCambridge.
CYang.2002.KnowledgeandLearninginNatural Language.OxfordUniversityPress.

