<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<title>The GRACE French Part-of-Speech Tagging Evaluation Task</title>
<date>1998</date>
<booktitle>In Proc. LREC’98</booktitle>
<marker>1998</marker>
<rawString>(1998). The GRACE French Part-of-Speech Tagging Evaluation Task. In Proc. LREC’98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bosco</author>
<author>A Mazzei</author>
<author>V Lombardo</author>
<author>G Attardi</author>
<author>A Corazza</author>
<author>A Lavelli</author>
<author>L Lesmo</author>
<author>G Satta</author>
<author>M Simi</author>
</authors>
<title>Comparing Italian parsers on a common treebank: the EVALITA experience</title>
<date>2008</date>
<booktitle>In Proc. LREC’08</booktitle>
<marker>Bosco, Mazzei, Lombardo, Attardi, Corazza, Lavelli, Lesmo, Satta, Simi, 2008</marker>
<rawString>Bosco, C., Mazzei, A., Lombardo, V., Attardi, G., Corazza, A., Lavelli, A., Lesmo, L., Satta, G., Simi, M. (2008) Comparing Italian parsers on a common treebank: the EVALITA experience. In Proc. LREC’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bosco</author>
<author>A Mazzei</author>
<author>V Lombardo</author>
</authors>
<title>EVALITA parsing task: an analysis of the first parsing system contest for Italian. Intelligenza Artificiale</title>
<date>2007</date>
<volume>4</volume>
<marker>Bosco, Mazzei, Lombardo, 2007</marker>
<rawString>Bosco, C., Mazzei, A., Lombardo, V. (2007) EVALITA parsing task: an analysis of the first parsing system contest for Italian. Intelligenza Artificiale, 4(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bosco</author>
<author>V Lombardo</author>
<author>D Vassallo</author>
<author>L Lesmo</author>
</authors>
<date>2000</date>
<contexts>
<context>Monachini, 1995). Several research groups have been working on PoS annotation to develop Italian treebanks, such as VIT (Venice Italian Treebank – Delmonte, 2004) and TUT (Turin University Treebank – Bosco et al., 2000) and morphological analysers such as the one by XEROX. A comparison of the tagsets used by these groups with EAGLES guidelines reveals that, although there is general agreement on the main parts of s</context>
</contexts>
<marker>Bosco, Lombardo, Vassallo, Lesmo, 2000</marker>
<rawString>Bosco C., Lombardo V., Vassallo D., Lesmo L. (2000).</rawString>
</citation>
<citation valid="true">
<title>Building a treebank for Italian: a data-driven annotation schema</title>
<booktitle>In Proc. LREC’2000</booktitle>
<marker></marker>
<rawString>Building a treebank for Italian: a data-driven annotation schema. In Proc. LREC’2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT A Statistical Part-of-Speech Tagger</title>
<date>2000</date>
<booktitle>In Proc. 6th Conf. on Applied Natural Language Processing</booktitle>
<pages>224--231</pages>
<marker>Brants, 2000</marker>
<rawString>Brants T. (2000). TnT A Statistical Part-of-Speech Tagger. In Proc. 6th Conf. on Applied Natural Language Processing, pp. 224-231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Some Advances in Transformation-Based Part of Speech Tagging</title>
<date>1994</date>
<booktitle>In Proc. 12th National Conference on Artificial Intelligence</booktitle>
<pages>722--727</pages>
<marker>Brill, 1994</marker>
<rawString>Brill E. (1994). Some Advances in Transformation-Based Part of Speech Tagging. In Proc. 12th National Conference on Artificial Intelligence, pp. 722-727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>J Hajic</author>
<author>L Ramshaw</author>
<author>C Tillmann</author>
</authors>
<title>A statistical parser of Czech</title>
<date>1999</date>
<booktitle>In Proc. of ACL’99</booktitle>
<contexts>
<context> it has left open several questions on parsers’ portability. While strong empirical evidences demonstrate that results obtained on a particular treebank are unportable on other corpora (Gildea, 2001; Collins et al., 1999; Corazza et al., 2004), the validation of existing parsing models depends on the possibility of generalizing their results on corpora other than those on which they have been trained, tuned and teste</context>
</contexts>
<marker>Collins, Hajic, Ramshaw, Tillmann, 1999</marker>
<rawString>Collins, M., Hajic, J., Ramshaw, L. and Tillmann, C. (1999) A statistical parser of Czech. In Proc. of ACL’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Corazza</author>
<author>A Lavelli</author>
<author>G Satta</author>
<author>R Zanoli</author>
</authors>
<date>2004</date>
<contexts>
<context>ral questions on parsers’ portability. While strong empirical evidences demonstrate that results obtained on a particular treebank are unportable on other corpora (Gildea, 2001; Collins et al., 1999; Corazza et al., 2004), the validation of existing parsing models depends on the possibility of generalizing their results on corpora other than those on which they have been trained, tuned and tested. The aim of the EVAL</context>
</contexts>
<marker>Corazza, Lavelli, Satta, Zanoli, 2004</marker>
<rawString>Corazza, A., Lavelli, A., Satta, G. and Zanoli, R. (2004).</rawString>
</citation>
<citation valid="true">
<title>Analyzing an Italian treebank with state-of-the-art statistical parser</title>
<booktitle>In Proc. of TLT-2004</booktitle>
<marker></marker>
<rawString>Analyzing an Italian treebank with state-of-the-art statistical parser. In Proc. of TLT-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>S Berck</author>
</authors>
<title>MBT: A Memory Based Part of Speech Tagger-Generator</title>
<date>1996</date>
<booktitle>In Proc. 4th Workshop on Very Large Corpora</booktitle>
<pages>14--27</pages>
<marker>Daelemans, Zavrel, Berck, 1996</marker>
<rawString>Daelemans W., Zavrel J., Berck S. (1996). MBT: A Memory Based Part of Speech Tagger-Generator. In Proc. 4th Workshop on Very Large Corpora, pp. 1427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Delmonte</author>
</authors>
<title>Strutture sintattiche dall’analisi computazionale di corpora di italiano. In</title>
<date>2004</date>
<pages>187--220</pages>
<location>Milano</location>
<contexts>
<context>been developed in the context of the EAGLES project (Monachini, 1995). Several research groups have been working on PoS annotation to develop Italian treebanks, such as VIT (Venice Italian Treebank – Delmonte, 2004) and TUT (Turin University Treebank – Bosco et al., 2000) and morphological analysers such as the one by XEROX. A comparison of the tagsets used by these groups with EAGLES guidelines reveals that, a</context>
</contexts>
<marker>Delmonte, 2004</marker>
<rawString>Delmonte R. (2004). Strutture sintattiche dall’analisi computazionale di corpora di italiano. In A. Cardinaletti, A. and F. Frasnedi (Eds.), Intorno all’italiano contemporaneo. Tra linguistica e didattica. F. Angeli, Milano, pp. 187-220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ferro</author>
<author>L Gerber</author>
<author>I Mani</author>
<author>B Sundheim</author>
<author>G Wilson</author>
</authors>
<title>TIDES 2005 Standard for the Annotation of Temporal Expressions</title>
<date>2005</date>
<note>http://timex2.mitre.org/annotation_guidelines/2005_ti mex2_standard_v1.1.pdf</note>
<contexts>
<context>alian texts. Our work refers to the Automatic Content Extraction (ACE) program that in 2004 adopted the TERN Task with respect to the “TIDES 2005 Standard for the Annotation of Temporal Expressions” (Ferro et al., 2005). TEs to be marked include both absolute (17 luglio 2007/July 17th, 2007) and relative expressions (ieri/yesterday). Also durations (un’ora/one hour), sets of times (ogni settimana/every week), under</context>
</contexts>
<marker>Ferro, Gerber, Mani, Sundheim, Wilson, 2005</marker>
<rawString>Ferro, L., Gerber, L., Mani, I., Sundheim, B., and Wilson, G. (2005). TIDES 2005 Standard for the Annotation of Temporal Expressions. http://timex2.mitre.org/annotation_guidelines/2005_ti mex2_standard_v1.1.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gildea</author>
</authors>
<title>Corpus variation and parser performance</title>
<date>2001</date>
<booktitle>In Proc. of EMNLP’01</booktitle>
<contexts>
<context>ocalization on it has left open several questions on parsers’ portability. While strong empirical evidences demonstrate that results obtained on a particular treebank are unportable on other corpora (Gildea, 2001; Collins et al., 1999; Corazza et al., 2004), the validation of existing parsing models depends on the possibility of generalizing their results on corpora other than those on which they have been tr</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Gildea, G. (2001). Corpus variation and parser performance. In Proc. of EMNLP’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Guazzini</author>
<author>M Ulivieri</author>
<author>F Bertagna</author>
<author>N Calzolari</author>
</authors>
<title>Senseval-3: the Italian All-Words Task</title>
<date>2004</date>
<booktitle>In Proc. SENSEVAL-3</booktitle>
<contexts>
<context>e resource but also its lemma and the Part of Speech (PoS) tag. 4.1. Data Description The data used for the current task corresponds mostly to the set already presented in the occasion of Senseval 3 (Guazzini et al., 2004). 10 ISST is an Italian treebank (Montemagni et al., 2003) that implements a syntactic annotation distributed on a constituent structure and a relation level including a smaller set of relations than</context>
</contexts>
<marker>Guazzini, Ulivieri, Bertagna, Calzolari, 2004</marker>
<rawString>Guazzini, E., Ulivieri, M., Bertagna, F., Calzolari., N., (2004). Senseval-3: the Italian All-Words Task. In Proc. SENSEVAL-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>A Cappelli</author>
</authors>
<date>2007</date>
<journal>Intelligenza Artificiale</journal>
<booktitle>Proc. of EVALITA</booktitle>
<volume>4</volume>
<contexts>
<context>ation campaigns mentioned above, participants were provided with training data and had the chance to test their systems with the evaluation metrics and procedures to be used in the formal evaluation (Magnini &amp; Cappelli, 2007). For EVALITA 2007, we received a total number of 55 expressions of interest for the five tasks. In the end, 30 participants actually submitted their results, with the following distribution: 11 for </context>
</contexts>
<marker>Magnini, Cappelli, 2007</marker>
<rawString>Magnini, B., Cappelli, A. (Eds.) (2007). Proc. of EVALITA 2007. Intelligenza Artificiale, 4(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>M Negri</author>
<author>E Pianta</author>
<author>M Speranza</author>
<author>Bartalesi Lenzi</author>
<author>V</author>
<author>R Sprugnoli</author>
</authors>
<title>Italian Content Annotation Bank (I-CAB): Temporal Expressions</title>
<date>2007</date>
<tech>Technical Report, FBK-irst</tech>
<note>http://evalita.itc.it/tasks/I-CAB-Report-Temporal-Expr essions.pdf</note>
<contexts>
<context>ires cultural or domain-specific knowledge (anno accademico/academic year) are to be annotated. The TERN Task consisted of two subtasks based on the TIMEX2 standards with some adaptations to Italian (Magnini et al., 2007a): (i) Temporal Expression Recognition only, in which systems are required to recognize the TEs occurring in the source data by identifying their extension; (ii) Temporal Expression Recognition + Nor</context>
<context>. The task was based on the ACE-LDC standards for the ACE Entity Recognition and Normalization Task12, with appropriate adaptations needed to limit it to the recognition of Named Entities (NEs) only (Magnini et al. 2007b). 6.1. Data Description and Evaluation Metrics As a dataset, we used the I-CAB corpus, developed within the Ontotext project and described in Section 5.1. Training and test data contained respective</context>
</contexts>
<marker>Magnini, Negri, Pianta, Speranza, Lenzi, V, Sprugnoli, 2007</marker>
<rawString>Magnini, B., Negri, M., Pianta, E., Speranza, M., Bartalesi Lenzi, V., and Sprugnoli, R. (2007a). Italian Content Annotation Bank (I-CAB): Temporal Expressions. Technical Report, FBK-irst. http://evalita.itc.it/tasks/I-CAB-Report-Temporal-Expr essions.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>E Pianta</author>
<author>M Speranza</author>
<author>Bartalesi Lenzi</author>
<author>V</author>
<author>R Sprugnoli</author>
</authors>
<title>Italian Content Annotation Bank (I-CAB): Named Entities</title>
<date>2007</date>
<tech>Technical Report FBK-irst. http://evalita.itc.it/tasks/I-CAB-Report-Named-Entitie s.pdf</tech>
<contexts>
<context>ires cultural or domain-specific knowledge (anno accademico/academic year) are to be annotated. The TERN Task consisted of two subtasks based on the TIMEX2 standards with some adaptations to Italian (Magnini et al., 2007a): (i) Temporal Expression Recognition only, in which systems are required to recognize the TEs occurring in the source data by identifying their extension; (ii) Temporal Expression Recognition + Nor</context>
<context>. The task was based on the ACE-LDC standards for the ACE Entity Recognition and Normalization Task12, with appropriate adaptations needed to limit it to the recognition of Named Entities (NEs) only (Magnini et al. 2007b). 6.1. Data Description and Evaluation Metrics As a dataset, we used the I-CAB corpus, developed within the Ontotext project and described in Section 5.1. Training and test data contained respective</context>
</contexts>
<marker>Magnini, Pianta, Speranza, Lenzi, V, Sprugnoli, 2007</marker>
<rawString>Magnini, B., Pianta, E., Speranza, M., Bartalesi Lenzi, V., and Sprugnoli, R. (2007b). Italian Content Annotation Bank (I-CAB): Named Entities, Technical Report FBK-irst. http://evalita.itc.it/tasks/I-CAB-Report-Named-Entitie s.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>A Cappelli</author>
<author>E Pianta</author>
<author>M Speranza</author>
<author>Bartalesi Lenzi</author>
<author>V Sprugnoli</author>
<author>R Romano</author>
<author>C L Girardi</author>
<author>M Negri</author>
</authors>
<title>Annotazione di contenuti concettuali in un corpus italiano: I-CAB</title>
<date>2006</date>
<booktitle>In Proc. of SILFI 2006</booktitle>
<location>Florence, Italy</location>
<contexts>
<context> TEs by assigning values to a pre-defined set of attributes. 5.1 Data Description Both training data and test data are part of the Italian Content Annotation Bank (I-CAB), developed by FBK and CELCT (Magnini et al., 2006). I-CAB consists of 525 news stories taken from different sections (e.g. Cultural, Economic, Sports and Local News) of the local newspaper “L’Adige”, for a total of around 180,000 words (the ratio be</context>
</contexts>
<marker>Magnini, Cappelli, Pianta, Speranza, Lenzi, Sprugnoli, Romano, Girardi, Negri, 2006</marker>
<rawString>Magnini, B., Cappelli, A., Pianta, E., Speranza, M., Bartalesi Lenzi, V., Sprugnoli, R., Romano, L., Girardi C., Negri, M. (2006). Annotazione di contenuti concettuali in un corpus italiano: I-CAB. In Proc. of SILFI 2006, Florence, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Monachini</author>
</authors>
<title>ELM-IT: EAGLES Specification for Italian morphosintax Lexicon Specification and Classification Guidelines</title>
<date>1996</date>
<journal>EAGLES Document EAG CLWG ELM IT/F</journal>
<marker>Monachini, 1996</marker>
<rawString>Monachini M. (1996). ELM-IT: EAGLES Specification for Italian morphosintax Lexicon Specification and Classification Guidelines. EAGLES Document EAG CLWG ELM IT/F.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Montemagni</author>
<author>F Francesco</author>
<author>M Battista</author>
<author>N Calzolari</author>
<author>O Corazzari</author>
<author>A Lenci</author>
<author>V Pirrelli</author>
<author>A Zampolli</author>
<author>F Fanciulli</author>
<author>M Massetani</author>
<author>R Raffaelli</author>
<author>R Basili</author>
<author>M T Pazienza</author>
<author>D Saracino</author>
<author>F Zanzotto</author>
<author>N Mana</author>
<author>F Pianesi</author>
<author>R Delmonte</author>
</authors>
<title>The syntactic-semantic Treebank of Italian. An Overview. Linguistica Computazionale a Pisa, vol</title>
<date>2003</date>
<journal>I</journal>
<contexts>
<context>S) tag. 4.1. Data Description The data used for the current task corresponds mostly to the set already presented in the occasion of Senseval 3 (Guazzini et al., 2004). 10 ISST is an Italian treebank (Montemagni et al., 2003) that implements a syntactic annotation distributed on a constituent structure and a relation level including a smaller set of relations than TUT. A corpus of about 13,600 word tokens extracted from </context>
</contexts>
<marker>Montemagni, Francesco, Battista, Calzolari, Corazzari, Lenci, Pirrelli, Zampolli, Fanciulli, Massetani, Raffaelli, Basili, Pazienza, Saracino, Zanzotto, Mana, Pianesi, Delmonte, 2003</marker>
<rawString>Montemagni, S., Francesco, F., Battista, M., Calzolari, N., Corazzari, O, Lenci, A., Pirrelli, V., Zampolli, A., Fanciulli, F., Massetani, M., Raffaelli, R., Basili, R., Pazienza, M.T., Saracino, D., Zanzotto, F., Mana, N., Pianesi, F., Delmonte, R. (2003). The syntactic-semantic Treebank of Italian. An Overview. Linguistica Computazionale a Pisa, vol. I.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S Kübler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>The CoNLL-2007 shared task on dependency parsing</title>
<date>2007</date>
<booktitle>In Proc. of EMNLPCoNLL</booktitle>
<contexts>
<context> from Penn mainly because of the PoS tagset. The evaluation of results is performed separately for dependency and constituency. For dependency results it is based on the three CoNLL standard metrics (Nivre et al., 2007): • Labeled Attachment Score (LAS), the percentage of tokens with correct head and relation label; • Unlabeled Attachment Score (UAS), the percentage of tokens with correct head; • Label Accuracy (LA</context>
</contexts>
<marker>Nivre, Hall, Kübler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Nivre, J., Hall, J., Kübler, S., McDonald, R., Nilsson, J., Riedel, S. and Yuret, D. (2007). The CoNLL-2007 shared task on dependency parsing. In Proc. of EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pianta</author>
<author>R Zanoli</author>
</authors>
<title>Exploiting SVM for Italian Named Entity Recognition</title>
<date>2007</date>
<journal>Intelligenza Artificiale</journal>
<booktitle>In Proc. of EVALITA</booktitle>
<volume>4</volume>
<marker>Pianta, Zanoli, 2007</marker>
<rawString>Pianta, E., Zanoli, R. (2007). Exploiting SVM for Italian Named Entity Recognition. In Proc. of EVALITA 2007. Intelligenza Artificiale, 4(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-of-Speech Tagging. In</title>
<date>1996</date>
<booktitle>Proc. EMNLP’96</booktitle>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Ratnaparkhi A. (1996). A Maximum Entropy Model for Part-of-Speech Tagging. In Proc. EMNLP’96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Alonge Roventini</author>
<author>A Bertagna</author>
<author>F Calzolari</author>
<author>N Girardi</author>
<author>C Magnini</author>
<author>B Marinelli</author>
<author>R Speranza</author>
<author>M Zampolli</author>
<author>A</author>
</authors>
<title>ItalWordNet: Building a Large Semantic Database for the Automatic Treatment of Italian</title>
<date>2003</date>
<booktitle>In Linguistica Computazionale, Istituti Editoriali e Poligrafici Internazionali, Pisa-Roma, ISSN</booktitle>
<pages>0392--6907</pages>
<contexts>
<context>d expressions (97). The reference lexical resource, provided to participants, was the ItalWordNet computational lexicon, which contains about 64,000 word senses corresponding to about 50,000 synsets (Roventini et al., 2003). 4.2. Evaluation Metrics Results were evaluated by taking into account the standard measures: Precision, Recall and F-Measure (a0 =1). Moreover, two different scores were taken into account: a) Fine</context>
</contexts>
<marker>Roventini, Bertagna, Calzolari, Girardi, Magnini, Marinelli, Speranza, Zampolli, A, 2003</marker>
<rawString>Roventini, A. Alonge, A., Bertagna, F., Calzolari, N., Girardi, C., Magnini, B., Marinelli, R., Speranza, M., Zampolli, A. (2003). ItalWordNet: Building a Large Semantic Database for the Automatic Treatment of Italian. In Linguistica Computazionale, Istituti Editoriali e Poligrafici Internazionali, Pisa-Roma, ISSN 0392-6907.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Speranza</author>
</authors>
<title>EVALITA 2007: The Named Entity Recognition task</title>
<date>2007</date>
<journal>Intelligenza Artificiale</journal>
<booktitle>In Proc. of EVALITA</booktitle>
<volume>4</volume>
<contexts>
<context>eight points when used without external resources. The recognition of PER NEs turned out to be the easiest subtask, as all participants obtained their highest F-Measure values, ranging from 75 to 92 (Speranza, 2007). The recognition of NEs of type GPE did not constitute a problem for most participant systems either, with F-Measure values ranging between 65 and 86. System results dropped significantly in the rec</context>
</contexts>
<marker>Speranza, 2007</marker>
<rawString>Speranza, M. (2007). EVALITA 2007: The Named Entity Recognition task. In Proc. of EVALITA 2007. Intelligenza Artificiale, 4(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Tamburini</author>
<author>C Seidenari</author>
</authors>
<title>EVALITA 2007. The Italian Part-of-Speech Tagging Evaluation Task Guidelines</title>
<date>2007</date>
<note>http://evalita.itc.it/tasks/pos.html</note>
<contexts>
<context>rent subtasks for the PoS-tagging evaluation campaign, the first using a traditional tagset (EAGLES-like), the second using a structurally different tagset (DISTRIB). We refer to the task guidelines (Tamburini &amp; Seidenari, 2007) for an in-depth discussion of the two proposed tagsets. 2.3 Tokenisation issues The problem of text segmentation (tokenisation) is a central issue in PoS-taggers comparison and evaluation. In princi</context>
</contexts>
<marker>Tamburini, Seidenari, 2007</marker>
<rawString>Tamburini F., Seidenari C. (2007). EVALITA 2007. The Italian Part-of-Speech Tagging Evaluation Task Guidelines. http://evalita.itc.it/tasks/pos.html.</rawString>
</citation>
</citationList>
</algorithm>

