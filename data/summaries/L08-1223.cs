<context>on. MITRE normalized the non-English texts similarly, making use of their in-house expertise in those languages. For machine translation, we calculated three commonly used automated MT metrics: BLEU (Papineni et al., 2001), METEOR (Banerjee and Lavie, 2004; Lavie et al., 2005), and TER (Snover et al., 2005), with both reference and hypothesis texts normalized much like they were normalized for ASR (see Condon, et al.,</context>
N01-1004
<rawString>Papineni, K., S. Roukos, T. Ward, and W. Zhu (2001). BLEU: A Method for Automatic Evaluation of Machine Translation. (IBM research report available at http://domino.watson.ibm.com/library/cybergig.nsf/).</rawString>
N01-1004
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
N01-1004
