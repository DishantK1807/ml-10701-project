UnsupervisedType andTokenIdentiﬁcation
ofIdiomaticExpressions
AfsanehFazly
∗
UniversityofToronto
PaulCook
∗∗
UniversityofToronto
SuzanneStevenson
†
UniversityofToronto
Idiomaticexpressionsareplentifulineverydaylanguage,yettheyremainmysterious,asit
isnotclearexactlyhowpeoplelearnandunderstandthem.Theyareofspecialinterestto
linguists,psycholinguists,andlexicographers,mainlybecauseoftheirsyntacticandsemantic
idiosyncrasiesaswellastheirunclearlexicalstatus.Despiteagreatdealofresearchonthe
propertiesofidiomsinthelinguisticsliterature,thereisnotmuchagreementonwhichproperties
arecharacteristicoftheseexpressions.Becauseoftheirpeculiarities,idiomaticexpressionshave
mostlybeenoverlookedbyresearchersincomputationallinguistics.Inthisarticle,welook
intotheusefulnessofsomeoftheidentiﬁedlinguisticpropertiesofidiomsfortheirautomatic
recognition.Speciﬁcally,wedevelopstatisticalmeasuresthateachmodelaspeciﬁcproperty
ofidiomaticexpressionsbylookingattheiractualusagepatternsintext.Weusethesesta-
tisticalmeasuresinatype-basedclassiﬁcationtaskwhereweautomaticallyseparateidiomatic
expressions(expressionswithapossibleidiomaticinterpretation)fromsimilar-on-the-surface
literalphrases(forwhichnoidiomaticinterpretationispossible).Inaddition,weusesomeof
themeasuresinatokenidentiﬁcationtaskwherewedistinguishidiomaticandliteralusagesof
potentiallyidiomaticexpressionsincontext.
1. Introduction
Idiomsformaheterogeneousclass,withprototypicalexamplessuchasbyandlarge,kick
thebucket,andletthecatoutofthebag. It is hard to ﬁnd a single agreed-upon deﬁnition
that covers all members of this class (Glucksberg 1993; Cacciari 1993; Nunberg, Sag,
andWasow1994),buttheyareoftendeﬁnedassequencesofwordsinvolvingsomede-
greeofsemanticidiosyncrasyornon-compositionality.Thatis,anidiomhasadifferent
∗ DepartmentofComputerScience,UniversityofToronto,6King’sCollegeRd.,Toronto,ONM5S3G4,
Canada.E-mail:afsaneh@cs.toronto.edu.
∗∗ DepartmentofComputerScience,UniversityofToronto,6King’sCollegeRd.,Toronto,ONM5S3G4,
Canada.E-mail:pcook@cs.toronto.edu.
† DepartmentofComputerScience,UniversityofToronto,6King’sCollegeRd.,Toronto,ONM5S3G4,
Canada.E-mail:suzanne@cs.toronto.edu.
Submissionreceived:12September2007;revisedsubmissionreceived:29February2008;acceptedfor
publication:6May2008.
©2009AssociationforComputationalLinguistics
ComputationalLinguistics Volume35,Number1
meaning from the simple composition of the meaning of its component words. Idioms
arewidelyandcreativelyusedbyspeakersofalanguagetoexpressideascleverly,eco-
nomically,orimplicitly,andthusappearinalllanguagesandinalltextgenres(Sagetal.
2002).Manyexpressionsacquireanidiomaticmeaningovertime(Cacciari1993);conse-
quently, new idioms come into existence on a daily basis (Cowie, Mackin, and McCaig
1983; Seaton and Macaulay 2002). Automatic tools are therefore necessary for assisting
lexicographers in keeping lexical resources up to date, as well as for creating and ex-
tendingcomputationallexiconsforuseinnaturallanguageprocessing(NLP)systems.
Though completely frozen idioms, such as by and large, can be represented as
words with spaces (Sag et al. 2002), most idioms are syntactically well-formed phrases
that allow some variability in expression, such asshootthebreezeand holdﬁre (Gibbs
and Nayak 1989; d’Arcais 1993; Fellbaum 2007). Such idioms allow a varying degree
of morphosyntactic ﬂexibility—for example, heldﬁre and holdone’sﬁre allow for an
idiomatic reading, whereas typically only a literal interpretation is available forﬁrewas
held and heldﬁres. Clearly, a words-with-spaces approach does not work for phrasal
idioms.Hence,inadditiontorequiringNLPtoolsforrecognizingidiomaticexpressions
(types) to include in a lexicon, methods for determining the allowable and preferred
usages (a.k.a. canonical forms) of such expressions are also needed. Moreover, in many
situations, an NLP system will need to distinguish a usage (token) of a potentially
idiomaticexpressionaseitheridiomaticorliteralinordertohandleagivensequenceof
wordsappropriately.Forexample,amachinetranslationsystemmusttranslateheldﬁre
differentlyinThearmyheldtheirﬁreandTheworshippersheldtheﬁreuptotheidol.
Previousstudiesfocusingontheautomaticidentiﬁcationofidiomtypeshaveoften
recognized the importance of drawing on their linguistic properties, such as their se-
mantic idiosyncrasy or their restricted ﬂexibility, pointed out earlier. Some researchers
have relied on a manual encoding of idiom-speciﬁc knowledge in a lexicon (Copestake
et al. 2002; Odijk 2004; Villavicencio et al. 2004), whereas others have presented ap-
proaches for the automatic acquisition of more general (hence less distinctive) knowl-
edge from corpora (Smadja 1993; McCarthy, Keller, and Carroll 2003). Recent work
that looks into the acquisition of the distinctive properties of idioms has been limited,
both in scope and in the evaluation of the methods proposed (Lin 1999; Evert, Heid,
and Spranger 2004). Our goal is to develop unsupervised means for the automatic
acquisition of lexical, syntactic, and semantic knowledge about a broadly documented
classofidiomaticexpressions.
Speciﬁcally, we focus on a cross-linguistically prominent class of phrasal idioms
whicharecommonlyandproductivelyformedfromthecombinationofafrequentverb
and a noun in its direct object position (Cowie, Mackin, and McCaig 1983; Nunberg,
Sag, and Wasow 1994; Fellbaum 2002), for example, shootthebreeze, makeaface,and
pushone’sluck. We refer to these as verb+noun idiomatic combinations or VNICs.
1
We present a comprehensive analysis of the distinctive linguistic properties of phrasal
idioms,includingVNICs(Section2),andproposestatisticalmeasuresthatcaptureeach
property(Section3).Weprovideamulti-facetedevaluationofthemeasures(Section4),
showing their effectiveness in the recognition of idiomatic expressions (types)—that is,
separatingthemfromsimilar-on-the-surfaceliteralphrases—aswellastheirsuperiority
to existing state-of-the-art techniques. Drawing on these statistical measures, we also
propose an unsupervised method for the automatic acquisition of an idiom’s canonical
1 WeusetheabbreviationVNICandthetermexpressiontorefertoaverb+nountypewithapotential
idiomaticmeaning.Weusethetermsinstanceandusagetorefertoatokenoccurrenceofanexpression.
62
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
forms(e.g.,shootthebreezeasopposedtoshootabreeze),andshowthatitcansuccessfully
accomplishthetask(Section5).
It is possible for a single VNIC to have both idiomatic and non-idiomatic (literal)
meanings. For example,makeafaceis ambiguous between an idiom, as inThelittlegirl
madeafunnyfaceathermother, and a literal combination, as in Shemadeafaceonthe
snowmanusingacarrotandtwobuttons. Despite the common perception that phrases
that can be idioms are mainly used in their idiomatic sense, our analysis of 60 idioms
has shown otherwise. We found that close to half of these also have a clear literal
meaning; and of those with a literal meaning, on average around 40% of their usages
areliteral.Distinguishingtokenphrasesasidiomaticorliteralcombinationsofwordsis
thus essential for NLP tasks, such as semantic parsing and machine translation, which
requiretheidentiﬁcationofmultiwordsemanticunits.
Most recent studies focusing on the identiﬁcation of idiomatic and non-idiomatic
tokens either assume the existence of manually annotated data for a supervised clas-
siﬁcation (Patrick and Fletcher 2005; Katz and Giesbrecht 2006), or rely on manually
encoded linguistic knowledge about idioms (Uchiyama, Baldwin, and Ishizaki 2005;
Hashimoto, Sato, and Utsuro 2006), or even ignore the speciﬁc properties of non-
literal language and rely mainly on general purpose methods for the task (Birke and
Sarkar 2006). We propose unsupervised methods that rely on automatically acquired
knowledge about idiom types to identify their token occurrences as idiomatic or literal
(Section6).Morespeciﬁcally,weexplorethehypothesisthatthetype-basedknowledge
we automatically acquire about an idiomatic expression can be used to determine
whether an instance of the expression is used literally or idiomatically (token-based
knowledge). Our experimental results show that the performance of the token-based
idiomidentiﬁcationmethodsproposedhereiscomparabletothatofexistingsupervised
techniques(Section7).
2. Idiomaticity,Semantic Analyzability, and Flexibility
Althoughsyntacticallywell-formed,phrasalidioms(includingVNICs)involveacertain
degree of semantic idiosyncrasy. This means that phrasal idioms are to some extent
nontransparent;thatis,evenknowingthemeaningoftheindividualcomponentwords,
the meaning of the idiom is hard to determine without special context or previous ex-
posure.Thereismuchevidenceinthelinguisticsliteraturethatidiomaticcombinations
also have idiosyncratic lexical and syntactic behavior. Here, we ﬁrst deﬁne semantic
analyzability and elaborate on its relation to semantic idiosyncrasy or idiomaticity. We
thenexpoundonthelexicalandsyntacticbehaviorofVNICs,pointingoutasuggestive
relation between the degree of idiomaticity of a VNIC and the degree of its lexicosyn-
tacticﬂexibility.
2.1Semantic Analyzability
Idioms have been traditionally believed to be completely non-compositional (Fraser
1970; Katz 1973). This means that unlike compositional combinations, the meaning
of an idiom cannot be solely predicted from the meaning of its parts. Nonetheless,
many linguists and psycholinguists argue against such a view, providing evidence
from idioms that show some degree of semantic compositionality (Nunberg, Sag, and
Wasow 1994; Gibbs 1995). The alternative view suggests that many idioms in fact do
63
ComputationalLinguistics Volume35,Number1
haveinternalsemanticstructure,whilerecognizingthattheyarenotcompositionalina
simplistic or traditional sense. To explain the semantic behavior of idioms, researchers
who take this alternative view thus use new terms such as semantic decomposability
and/orsemantic analyzabilityinplaceofcompositionality.
To say that an idiom is semantically analyzable to some extent means that the
constituentscontributesomesortofindependentmeaning—notnecessarilytheirliteral
semantics—to the overall idiomatic interpretation. Generally, the more semantically
analyzable an idiom is, the easier it is to map the idiom constituents onto their cor-
responding idiomatic referents. In other words, the more semantically analyzable an
idiom is, the easier it is to make predictions about the idiomatic meaning from the
meaningoftheidiomparts.Semanticanalyzabilityisthusinverselyrelatedtosemantic
idiosyncrasy.
Many linguists and psycholinguists conclude that idioms clearly form a heteroge-
neous class, not all of them being truly non-compositional or unanalyzable (Abeill´e
1995; Moon 1998; Grant 2005). Rather, semantic analyzability in idioms is a matter of
degree. For example, the meaning ofshootthebreeze(“to chat idly”), a highly idiomatic
expression, has nothing to do with eithershootorbreeze. A less idiomatic expression,
such as spillthebeans (“to reveal a secret”), may be analyzed as spill metaphorically
corresponding to “reveal” andbeansreferring to “secret(s).” An idiom such aspopthe
questionis even less idiomatic because the relations between the idiom parts and their
idiomaticreferentsaremoredirectlyestablished,namely,popcorrespondsto“suddenly
ask” and question refers to “marriage proposal.” As we will explain in the following
section, there is evidence that the difference in the degree of semantic analyzability of
idiomaticexpressionsisalsoreﬂectedintheirlexicalandsyntacticbehavior.
2.2 Lexical
and Syntactic Flexibility
Mostidiomsareknowntobelexicallyﬁxed,meaningthatthesubstitutionofanearsyn-
onym (or a closely related word) for a constituent part does not preserve the idiomatic
meaningoftheexpression.Forexample,neithershootthewindnorhitthebreezearevalid
variationsoftheidiomshootthebreeze.Similarly,spillthebeanshasanidiomaticmeaning,
whilespillthepeasandspreadthebeanshave only literal interpretations. There are, how-
ever, idiomatic expressions that have one (or more) lexical variants. For example,blow
one’sowntrumpetandtootone’sownhornhavethesameidiomaticinterpretation(Cowie,
Mackin, and McCaig 1983); alsokeepone’scoolandloseone’scoolhave closely related
meanings (Nunberg, Sag, and Wasow 1994). Nonetheless, it is not the norm for idioms
to have lexical variants; when they do, there are usually unpredictable restrictions on
thesubstitutionstheyallow.
Idiomatic combinations are also syntactically distinct from compositional combi-
nations. Many VNICs cannot undergo syntactic variations and at the same time retain
their idiomatic interpretations. It is important, however, to note that VNICs differ with
respect to the extent to which they can tolerate syntactic operations, that is, the degree
of syntactic ﬂexibility they exhibit. Some are syntactically inﬂexible for the most part,
whereasothersaremoreversatile,asillustratedinthesentencesinExamples(1)and(2):
1.(a) SamandAzinshotthebreeze.
(b) ??SamandAzinshotabreeze.
(c) ??SamandAzinshotthebreezes.
(d) ??SamandAzinshotthecasualbreeze.
64
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
(e) ??ThebreezewasshotbySamandAzin.
(f) ??ThebreezethatSamandAzinshotwasquiterefreshing.
(g) ??WhichbreezedidSamandAzinshoot?
2.(a) Azinspilledthebeans.
(b) ?Azinspilledsomebeans.
(c) ??Azinspilledthebean.
(d) AzinspilledtheEnronbeans.
(e) ThebeanswerespilledbyAzin.
(f) ThebeansthatAzinspilledcausedSamalotoftrouble.
(g) WhichbeansdidAzinspill?
Linguists have often explained the lexical and syntactic ﬂexibility of idiomatic
combinations in terms of their semantic analyzability (Fellbaum 1993; Gibbs 1993;
Glucksberg 1993; Nunberg, Sag, and Wasow 1994; Schenk 1995). The common belief
isthatbecausetheconstituentsofasemanticallyanalyzableidiomcanbemappedonto
theircorrespondingreferentsintheidiomaticinterpretation,analyzable(lessidiomatic)
expressions are often more open to lexical substitution and syntactic variation. Psy-
cholinguistic studies also support this hypothesis: Gibbs and Nayak (1989) and Gibbs
et al. (1989), through a series of psychological experiments, demonstrate that there is
variation in the degree of lexicosyntactic ﬂexibility of idiomatic combinations. (Both
studies narrow their focus to verb phrase idiomatic combinations, mainly of the form
verb+noun.) Moreover, their ﬁndings provide evidence that the lexical and syntactic
ﬂexibilityofVNICsisnotarbitrary,butrathercorrelateswiththesemanticanalyzability
oftheseidiomsasperceivedbythespeakersparticipatingintheexperiments.
Corpus-based studies such as those by Moon (1998), Riehemann (2001), and Grant
(2005) conclude that idioms are not as ﬁxed as most have assumed. These claims are
often based on observing certain idiomatic combinations in a form other than their so-
called canonical forms. For example, Moon mentions that she has observed bothkick
thepail and kickthecan as variations of kickthebucket. Also, Grant ﬁnds evidence of
variations such aseatone’sheart(out)andeatone’shearts(out)in the BNC. Riehemann
concludes that in contrast to non-idiomatic combinations of words, “idioms have a
strongly preferred canonical form, but at the same time the occurrence of lexical and
syntacticvariationsofidiomsistoocommontobeignored”(page67).Ourunderstand-
ingofsuchﬁndingsisthatidiomaticcombinations arenotinherentlyfrozenandthatit
is possible for them to appear in forms other than their agreed-upon canonical forms.
However, it is important to note that most such observed variations are constrained,
oftenwithunpredictablerestrictions.
Wearewellawarethatsemanticanalyzabilityisneitheranecessarynorasufﬁcient
condition for an idiomatic combination to be lexically or syntactically ﬂexible. Other
factors, such as communicative intentions and pragmatic constraints, can motivate a
speaker to use a variant in place of a canonical form (Glucksberg 1993). For exam-
ple, journalism is well known for manipulating idiomatic expressions for humor or
cleverness (Grant 2005). The age and the degree of familiarity of an idiom have also
been shown to be important factors that affect its ﬂexibility (Gibbs and Nayak 1989).
Nonetheless, linguists often use observations about lexical and syntactic ﬂexibility of
VNICs in order to make judgments about their degree of idiomaticity (Kyt¨o 1999;
Tanabe 1999). We thus conclude that lexicosyntactic behavior of a VNIC, although
affected by historical and pragmatic factors, can be at least partially explained in terms
ofsemanticanalyzabilityoridiomaticity.
65
ComputationalLinguistics Volume35,Number1
3. Automatic Acquisition ofType-Based Knowledge about VNICs
We use the observed connection between idiomaticity and (in)ﬂexibility to devise sta-
tistical measures for automatically distinguishing idiomatic verb+noun combinations
(types) from literal phrases. More speciﬁcally, we aim to identify verb–noun pairs such
as 〈keep, word〉 as having an associated idiomatic expression (keep one’s word), and
also distinguish these from verb–noun pairs such as 〈keep, ﬁsh〉 which do not have
an idiomatic interpretation. Although VNICs vary in their degree of ﬂexibility (cf.
Examples (1) and (2)), on the whole they contrast with fully compositional phrases,
whicharemorelexicallyproductiveandappearinawiderrangeofsyntacticforms.We
thus propose to use the degree of lexical and syntactic ﬂexibility of a given verb+noun
combinationtodeterminethelevelofidiomaticityoftheexpression.
Note that our assumption here is in line with corpus-linguistic studies on idioms:
we do not claim that it is inherently impossible for VNICs to undergo lexical sub-
stitution or syntactic variation. In fact, for each given idiomatic combination, it may
well be possible to ﬁnd a speciﬁc situation in which a lexical or a syntactic variant of
the canonical form is perfectly plausible. However, the main point of the assumption
here is that VNICs are more likely to appear in ﬁxed forms (known as their canonical
forms), more so than non-idiomatic phrases. Therefore, the overall distribution of a
VNIC in different lexical and syntactic forms is expected to be notably different from
thecorrespondingdistributionofatypicalverb+nouncombination.
Thefollowingsubsectionsdescribeourproposedstatisticalmeasuresforidiomatic-
ity, which quantify the degree of lexical, syntactic, and overall ﬁxedness of a given
verb+nouncombination(representedasaverb–nounpair).
3.1 Measuring
Lexical Fixedness
A VNIC is lexically ﬁxed if the replacement of any of its constituents by a semantically
(and syntactically) similar word does not generally result in another VNIC, but in
an invalid or a literal expression. One way of measuring lexical ﬁxedness of a given
verb+noun combination is thus to examine the idiomaticity of its variants, that is,
expressions generated by replacing one of the constituents by a similar word. This
approachhastwomainchallenges:(i)itrequirespriorknowledgeabouttheidiomaticity
of expressions (which is what we are developing our measure to determine); (ii) it can
onlymeasurethelexicalﬁxednessofidiomaticcombinations,andsocouldnotapplyto
literal combinations. We thus interpret this property statistically in the following way:
We expect a lexically ﬁxed verb+noun combination to appear much more frequently
thanitsvariantsingeneral.
Speciﬁcally, we examine the strength of association between the verb and the
noun constituent of a combination (the target expression or its lexical variants) as
an indirect cue to its idiomaticity, an approach inspired by Lin (1999). We use the
automatically built thesaurus of Lin (1998) to ﬁnd words similar to each constituent,
in order to automatically generate variants.
2
Variants are generated by replacing either
2 WealsoreplicatedourexperimentswithanautomaticallybuiltthesauruscreatedfromtheBritish
NationalCorpus(BNC)inasimilarfashion,andkindlyprovidedtousbyDianaMcCarthy.Results
weresimilar,hencewedonotreportthemhere.
66
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
thenounortheverbconstituentofapairwithasemantically(andsyntactically)similar
word.
3
Examples of automatically generated variants for the pair 〈spill,bean〉 are 〈pour,
bean〉,〈stream,bean〉,〈spill,corn〉,and〈spill,rice〉.
Let S
sim
(v)={v
i
|1≤i≤K
v
} be the set of theK
v
most similar verbs to the verbv
of the target pair 〈v,n〉,andS
sim
(n)=
braceleftbig
n
j
|1≤j≤K
n
bracerightbig
be the set of theK
n
most similar
nounstothenounn(accordingtoLin’sthesaurus).Thesetofvariantsforthetargetpair
isthus:
S
sim
(v,n)={〈v
i,n〉|1≤i≤K
v
}∪
braceleftbig
〈v,n
j
〉|1≤j≤K
n
bracerightbig
.
We calculate the association strength for the target pair and for each of its variants
using an information-theoretic measure called pointwise mutual information or PMI
(Churchetal.1991):
PMI(v
r,n
t
) = log
P(v
r,n
t
)
P(v
r
)P(n
t
)
= log
N
v+n
f(v
r,n
t
)
f(v
r, ∗)f(∗,n
t
)
(1)
where〈v
r,n
t
〉∈{〈v,n〉}∪S
sim
(v,n);N
v+n
isthetotalnumberofverb–objectpairsinthe
corpus;f(v
r,n
t
) is the frequency ofv
r
andn
t
co-occurring as a verb–object pair;f(v
r, ∗)
is the total frequency of the target (transitive) verb with any noun as its direct object;
andf(∗,n
t
)isthetotalfrequencyofthenounn
t
inthedirectobjectpositionofanyverb
inthecorpus.
Inhiswork,Lin(1999)assumesthatatargetexpressionisnon-compositionalifand
only if its PMI value is signiﬁcantly different from that of all the variants. Instead, we
propose a novel technique that brings together the association strengths (PMI values)
of the target and the variant expressions into a single measure reﬂecting the degree of
lexical ﬁxedness for the target pair. We assume that the target pair is lexically ﬁxed to
theextentthatitsPMIdeviatesfromtheaveragePMIofitsvariants.Byourmeasure,the
target pair is considered lexically ﬁxed (i.e., is given a high ﬁxedness score) only if the
differencebetweenitsPMIvalueandthatofmostofitsvariants—notnecessarilyall,as
inthemethodofLin(1999)—ishigh.
4
Ourmeasurecalculatesthisdeviation,normalized
usingthesample’sstandarddeviation:
Fixedness
lex
(v,n)
.
=
PMI(v,n)−PMI
s
(2)
3 Inanearlyversionofthiswork(FazlyandStevenson2006),onlythenounconstituentwasvaried
becauseweexpectedreplacingtheverbconstituentwitharelatedverbtobemorelikelytoyieldanother
VNIC,asinkeep/loseone’scool,give/getthebird,crack/breaktheice(Nunberg,Sag,andWasow1994;Grant
2005).Laterexperimentsonthedevelopmentdatashowedthatvariantsgeneratedbyreplacingboth
constituents,oneatatime,producebetterresults.
4 Thisway,evenifanidiomhasafewfrequentlyusedvariants(e.g.,breaktheiceandcracktheice),itmay
stillbeassignedahighﬁxednessscoreifmostothervariantsareuncommon.Notealsothatitispossible
thatsomevariantsofagivenidiomarefrequentlyusedliteralexpressions(e.g.,makebiscuitfortake
biscuit).Itisthusimportanttouseaﬂexibleformulationthatreliesonthecollectiveevidence(e.g.,
averagePMI)andhenceislesssensitivetoindividualcases.
67
ComputationalLinguistics Volume35,Number1
wherePMIisthemeanandsthestandarddeviationofthefollowingsample:
braceleftbig
PMI(v
r,n
t
)|〈v
r,n
t
〉∈{〈v,n〉}∪S
sim(v,n)
bracerightbig
PMIcanbenegative,zero,orpositive;thusFixedness
lex
(v,n) ∈ [−∞,+∞],wherehigh
positivevaluesindicatehigherdegreesoflexicalﬁxedness.
3.2 Measuring
Syntactic Fixedness
Compared to literal (non-idiomatic) verb+noun combinations, VNICs are expected to
appearinmorerestrictedsyntacticforms.Toquantifythesyntacticﬁxednessofatarget
verb–noun pair, we thus need to: (i) identify relevant syntactic patterns, namely, those
thathelpdistinguishVNICsfromliteralverb+nouncombinations;and(ii)translatethe
frequency distribution of the target pair in the identiﬁed patterns into a measure of
syntacticﬁxedness.
3.2.1IdentifyingRelevantPatterns.Determiningauniquesetofsyntacticpatternsappro-
priatefortherecognitionofallidiomaticcombinationsisdifﬁcultindeed:Exactlywhich
formsanidiomaticcombinationcanoccurinisnotentirelypredictable(Sagetal.2002).
Nonetheless,therearehypothesesaboutthedifferenceinbehaviorofVNICsandliteral
verb+noun combinations with respect to particular syntactic variations (Nunberg, Sag,
and Wasow 1994). Linguists note that semantic analyzability of VNICs is related to
the referential status of the noun constituent (i.e., the process of idiomatization of a
verb+noun combination is believed to be accompanied by a change from concreteness
to abstractness for the noun). The referential status of the noun is in turn assumed to
be related to the participation of the combination in certain morpho-syntactic forms.
In what follows, we describe three types of syntactic variation that are assumed to be
mostlytoleratedbyliteralcombinations,butlesstoleratedbymanyVNICs.
Passivization.There is much evidence in the linguistics literature that VNICs often do
notundergopassivization.Linguistsmainlyattributethistothefactthatinmostcases,
only referential nouns appear as the surface subject of a passive construction (Gibbs
and Nayak 1989). Due to the non-referential status of the noun constituent in most
VNICs, we expect that they do not undergo passivization as often as literal verb+noun
combinations do. Another explanation for this assumption is that passives are mainly
used to put focus on the object of a clause or sentence. For most VNICs, no such
communicative purpose can be served by topicalizing the noun constituent through
passivization (Jackendoff 1997). The passive construction is thus considered as one of
thesyntacticpatternsrelevanttomeasuringsyntacticﬂexibility.
5
Determinertype.A strong correlation has been observed between the ﬂexibility of the
determiner preceding the noun in a verb+noun combination and the overall ﬂexibility
of the phrase (Fellbaum 1993; Kearns 2002; Desbiens and Simon 2003). It is however
5 Notethatthereareidiomsthatappearprimarilyinapassivizedform,forexample,thedieiscast(“the
decisionismadeandwillnotchange”).Ourmeasurecaninprinciplerecognizesuchidiomsbecausewe
donotrequirethatanidiomappearsmainlyinactiveform;rather,weincludevoice(passiveoractive)as
animportantpartofthesyntacticpatternofanidiomaticcombination.
68
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
important to note that the nature of the determiner is also affected by other factors,
such as the semantic properties of the noun. For this reason, determiner ﬂexibility is
sometimesarguednottobeagoodpredictoroftheoverallsyntacticﬂexibilityofanex-
pression.Nonetheless,manyresearchersconsideritasanimportantpartintheprocess
ofidiomatizationofaverb+nouncombination(Akimoto1999;Kyt¨o1999;Tanabe1999).
WethusexpectaVNICtomainlyappearwithonetypeofdeterminer.
Pluralization.Although the verb constituent of a VNIC is morphologically ﬂexible, the
morphologicalﬂexibilityofthenounrelatestoitsreferentialstatus(Grant2005).Again,
oneshouldnotethattheuseofasingularorpluralnouninaVNICmayalsobeaffected
by the semantic properties of the noun. Recall that during the idiomatization process,
the noun constituent may become more abstract in meaning. In this process, the noun
may lose some of its nominal features, including number (Akimoto 1999). The non-
referential noun constituent of a VNIC is thus expected to mainly appear in just one of
thesingularorpluralforms.
Mergingthethreetypesofvariationresultsinapatternset,P,of11distinctsyntac-
tic patterns that are displayed in Table 1 along with examples for each pattern. When
developingthissetofpatterns,wehavetakenintoaccountthelinguistictheoriesabout
the syntactic constraints on idiomatic expressions; for example, our choice of patterns
isconsistentwiththeidiomtypologydevelopedbyNicolas(1995).Notethatwemerge
some of the individual patterns into one; for example, we include only one passive
pattern independently of the choice of the determiner or the number of the noun. The
motivation here is to merge low frequency patterns (i.e., those that are expected to
be less common) in order to acquire more reliable evidence on the distribution of a
particular verb–noun pair over the resulting pattern set. In principle, however, the set
can be expanded to include more patterns; it can also be modiﬁed to contain different
patternsfordifferentclassesofidiomaticcombinations.
3.2.2DevisingaStatisticalMeasure. The second step is to devise a statistical measure
that quantiﬁes the degree of syntactic ﬁxedness of a verb–noun pair, with respect to
Table1
Patternsusedinthesyntacticﬁxednessmeasure,alongwithexamplesforeach.Apattern
signatureiscomposedofaverbvinactive(v
act
)orpassive(v
pass
)voice;adeterminer(det)that
canbe NULL,indeﬁnite(a/an),deﬁnite(the),demonstrative(DEM),orpossessive(POSS);anda
nounnthatcanbesingular(n
sg
)orplural(n
pl
).
PatternNo. PatternSignature Example
1 v
act
det:NULL n
sg
givemoney
2 v
act
det:a/an n
sg
giveabook
3 v
act
det:the n
sg
givethebook
4 v
act
det:DEM n
sg
givethisbook
5 v
act
det:POSS n
sg
givemybook
6 v
act
det:NULL n
pl
givebooks
7 v
act
det:the n
pl
givethebooks
8 v
act
det:DEM n
pl
givethosebooks
9 v
act
det:POSS n
pl
givemybooks
10 v
act
det:OTHER n
sg,pl
givemanybooks
11 v
pass
det:ANY n
sg,pl
a/the/this/mybook/bookswas/weregiven
69
ComputationalLinguistics Volume35,Number1
the selected set of patterns, P. We propose a measure that compares the syntactic
behavior of the target pair with that of a “typical” verb–noun pair. Syntactic behav-
ior of a typical pair is deﬁned as the prior probability distribution over the patterns in
P. The maximum likelihood estimate for the prior probability of an individual pattern
pt∈ P iscalculatedas
P(pt) =
summationdisplay
v
i
∈V
summationdisplay
n
j
∈N
f(v
i,n
j,pt)
summationdisplay
v
i
∈V
summationdisplay
n
j
∈N
summationdisplay
pt
k
∈P
f(v
i,n
j,pt
k
)
=
f(∗, ∗,pt)
f(∗, ∗, ∗)
(3)
whereV isthesetofallinstancesoftransitiveverbsinthecorpus,andN isthesetofall
instancesofnounsappearingasthedirectobjectofsomeverb.
Thesyntacticbehaviorofthetargetverb–nounpair〈v,n〉isdeﬁnedastheposterior
probabilitydistributionoverthepatterns,giventheparticularpair.Themaximumlike-
lihoodestimatefortheposteriorprobabilityofanindividualpatternptiscalculatedas
P(pt|v,n) =
f(v,n,pt)
summationdisplay
pt
k
∈P
f(v,n,pt
k
)
=
f(v,n,pt)
f(v,n, ∗)
. (4)
The degree of syntactic ﬁxedness of the target verb–noun pair is estimated as
the divergence of its syntactic behavior (the posterior distribution over the patterns)
from the typical syntactic behavior (the prior distribution). The divergence of the two
probability distributions is calculated using a standard information-theoretic measure,
theKullbackLeibler(KL-)divergence(CoverandThomas1991):
Fixedness
syn
(v,n)
.
=D(P(pt|v,n)||P(pt))
=
summationdisplay
pt
k
∈P
P(pt
k
|v,n)log
P(pt
k
|v,n)
P(pt
k
)
(5)
KL-divergence has proven useful in many NLP applications (Resnik 1999; Dagan,
Pereira, and Lee 1994). KL-divergence is always non-negative and is zero if and only
if the two distributions are exactly the same. Thus, Fixedness
syn
(v,n) ∈ [0,+∞], where
largevaluesindicatehigherdegreesofsyntacticﬁxedness.
3.3 A
Uniﬁed Measure of Fixedness
VNICsarehypothesizedtobe,inmostcases,bothlexicallyandsyntacticallymoreﬁxed
than literal verb+noun combinations (see Section 2). We thus propose a new measure
70
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
of idiomaticity to be a measure of the overall ﬁxedness of a given pair. We deﬁne
Fixedness
overall
(v,n)asaweightedcombinationofFixedness
lex
andFixedness
syn
:
Fixedness
overall
(v,n)
.
= α Fixedness
syn
(v,n) + (1−α) Fixedness
lex
(v,n)(6)
whereαweightstherelativecontributionofthemeasuresinpredictingidiomaticity.
Recall that Fixedness
lex
(v,n) ∈ [−∞,+∞], and Fixedness
syn
(v,n) ∈ [0,+∞].To
combine them in the overall ﬁxedness measure, we rescale them, so that they fall in
the range [0, 1]. Thus, Fixedness
overall
(v,n) ∈ [0, 1], where values closer to 1 indicate a
higherdegreeofoverallﬁxedness.
4. VNICType Recognition: Evaluation
To evaluate our proposed ﬁxedness measures, we analyze their appropriateness for
determiningthedegreeofidiomaticityofasetofexperimentalexpressions(intheform
ofverb–nounpairs,extractedasdescribedinSection4.1).Morespeciﬁcally,weﬁrstuse
eachmeasuretoassignscorestotheexperimentalpairs.Wethenusethescoresassigned
by each measure to perform two different tasks, and assess the overall goodness of the
measurebylookingatitsperformanceinboth.
First, we look into the classiﬁcation performance of each measure by using the
scores to separate idiomatic verb–noun pairs from literal ones in a mixed list. This is
done by setting a threshold, here the median score, where all pairs with scores higher
thanthethresholdarelabeledasidiomaticandtherestasliteral.
6
Forclassiﬁcation,we
report accuracy (Acc), as well as the relative error rate reduction (ERR) over a random
(chance) baseline, referred to as Rand. Second, we examine the retrieval performance
of our ﬁxedness measures by using the scores to rank verb–noun pairs according to
their degree of idiomaticity. For retrieval, we present the precision–recall curves, as
well as the interpolated three-point average precision orIAP—that is, the average of
the interpolated precisions at the recall levels of 20%, 50%, and 80%. The interpolated
average precision and precision–recall curves are commonly used for the evaluation of
information retrieval systems (Manning and Sch¨utze 1999), and reﬂect the goodness of
a measure in placing the relevant items (here, idioms) before the irrelevant ones (here,
literals).
Idioms are often assumed to exhibit collocational behavior to some extent, that is,
the components of an idiom are expected to appear together more often than expected
bychance.Hence,someNLPsystemshaveusedcollocationalmeasurestoidentifythem
(Smadja 1993; Evert and Krenn 2001). However, as discussed in Section 2, idioms have
distinctive syntactic and semantic properties that separate them from simple colloca-
tions.Forexample,althoughcollocationsinvolvesomedegreeofsemanticidiosyncrasy
(strongteavs.?powerfultea),comparedtoidioms,theytypicallyhaveamoretransparent
meaning, and their syntactic behavior is more similar to that of literal expressions. We
thus expect our ﬁxedness measures that draw on the distinctive linguistic properties
of idioms to be more appropriate than measures of collocation for the identiﬁcation of
idioms. To verify this hypothesis, in both the classiﬁcation and retrieval tasks, we com-
pare the performance of the ﬁxedness measures with that of two collocation extraction
measures:aninformedbaseline,PMI,andaposition-basedﬁxednessmeasureproposed
6 Weadoptthemedianforthisparticular(balanced)dataset,understandingthatinpracticeasuitable
thresholdwouldneedtobedetermined,e.g.,basedondevelopmentdata.
71
ComputationalLinguistics Volume35,Number1
by Smadja (1993), which we refer to as Smadja. Next, we provide more details on PMI
andSmadja.
PMI is a widely used measure for extracting statistically signiﬁcant combinations
of words or collocations. It has also been used for the recognition of idioms (Evert and
Krenn 2001), warranting its use as an informed baseline here for comparison.
7
As in
Equation (1), our calculation of PMI here restricts the counts of the verb–noun pair to
thedirectobjectrelation.Smadja(1993)proposesacollocationextractionmethodwhich
measures the ﬁxedness of a word sequence (e.g., a verb–noun pair) by examining the
relativepositionofthecomponentwordsacrosstheiroccurrencestogether.Wereplicate
Smadja’smethod,wherewemeasureﬁxednessofatargetverb–nounpairasthespread
(variance) of the co-occurrence frequency of the verb and the noun over 10 relative
positionswithinaﬁve-wordwindow.
8
Recall from Section 3.1 that our Fixedness
lex
measure is intended as an improve-
mentoverthenon-compositionalitymeasureofLin(1999).Forthesakeofcompleteness,
we also compare the classiﬁcation performance of our Fixedness
lex
with that of Lin’s
(1999)measure,whichwerefertoasLin.
9
We ﬁrst elaborate on the methodological aspects of our experiments in Section 4.1,
andthenpresentadiscussionoftheexperimentalresultsinSection4.2.
4.1 Experimental
Setup
4.1.1CorpusandDataExtraction. We use the British National Corpus (BNC; Burnard
2000);toextractverb–nounpairs,alongwithinformationonthesyntacticpatternsthey
appearin.Weautomatically parsetheBNCusingtheCollinsparser(Collins1999),and
augment it with information about verb and noun lemmas, automatically generated
using WordNet (Fellbaum 1998). We further process the corpus using TGrep2 (Rohde
2004) in order to extract syntactic dependencies. For each instance of a transitive verb,
we use heuristics to extract the noun phrase (NP) in either the direct object position
(if the sentence is active), or the subject position (if the sentence is passive). We then
automatically ﬁnd the head noun of the extracted NP, its number (singular or plural),
andthedeterminerintroducingit.
4.1.2ExperimentalExpressions. We select our development and test expressions from
verb–noun pairs that involve a member of a predeﬁned list of transitive verbs, referred
to as basic verbs. Basic verbs, in their literal use, refer to states or acts that are central
tohumanexperience. Theyarethusfrequent,highlypolysemous, andtendtocombine
withotherwordstoformidiomaticcombinations(Cacciari1993;Claridge2000;Gentner
and France 2004). An initial list of such verbs was selected from several linguistic and
psycholinguistic studies on basic vocabulary (Ogden 1968; Clark 1978; Nunberg, Sag,
andWasow1994;Goldberg1995;Pauwels2000;Claridge2000;NewmanandRice2004).
Wefurtheraugmentedthisinitiallistwithverbsthataresemanticallyrelatedtoanother
7 PMIhasbeenshowntoperformbetterthanorcomparabletomanyotherassociationmeasures(Inkpen
2003;MohammadandHirst,submitted).Inourexperiments,wealsofoundthatPMIconsistently
performsbetterthantwootherassociationmeasures,theDicecoefﬁcientandthelog-likelihoodmeasure.
ExperimentsbyKrennandEvert(2001)showedcontradictingresultsforPMI;however,these
experimentswereperformedonsmall-sizedcorpora,andondatawhichcontaineditemswithverylow
frequency.
8 WeimplementthemethodasexplainedinSmadja(1993),takingintoaccountthepart-of-speechtagsof
thetargetcomponentwords.
9 WeimplementthemethodasexplainedinLin(1999),using95%conﬁdenceintervals.Wethusneedto
ignorevariantswithfrequencylowerthan4forwhichnoconﬁdenceintervalcanbeformed.
72
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
verb already in the list; for example,loseis added in analogy withﬁnd. Here is the ﬁnal
listofthe28verbsinalphabeticalorder:
blow,bring,catch,cut,ﬁnd,get,give,have,hear,hit,hold,keep,kick,lay,lose,make,move,
place,pull,push,put,see,set,shoot,smell,take,throw,touch
From the corpus, we extract all the verb–noun pairs (lemmas) that contain any
of these listed basic verbs, and that appear at least 10 times in the corpus in a direct
object relation (irrespective of any intervening determiners or adjectives). From these,
we select a subset that are idiomatic, and another subset that are literal, as follows: A
verb–noun pair is considered idiomatic if it appears in an idiom listed in a credible
dictionary such as the OxfordDictionaryofCurrentIdiomaticEnglish (ODCIE; Cowie,
Mackin, and McCaig 1983), or the Collins COBUILD Idioms Dictionary (CCID; Seaton
and Macaulay 2002).
10
To decide whether a verb–noun pair has appeared in an idiom,
we look for all idioms containing the verb and the noun in a direct-object relation,
irrespectiveofanyinterveningdeterminersoradjectives,and/oranyotherarguments.
Thepairisconsideredliteralifitinvolvesaphysicalactorstate(i.e.,thebasicsemantics
of the verb) and does not appear in any of the mentioned dictionaries as an idiom (or
part of an idiom). From the set of idiomatic pairs, we then randomly pull out 80 de-
velopment pairs and 100 test pairs, ensuring that we have items of both low and high
frequency. We then double the size of each data set (development and test) by adding
equalnumbersofliteralpairs,withsimilarfrequencydistributions.Someoftheidioms
correspondingtotheexperimentalidiomaticpairsare:kickthehabit,movemountains,lose
face,andkeepone’sword. Examples of literal pairs include:movecarriage,loseticket,and
keepﬁsh.
Development expressions are used in devising the ﬁxedness measures, as well as
in determining the values of their parameters as explained in the next subsection. Test
expressionsaresavedasunseendatafortheﬁnalevaluation.
4.1.3ParameterSettings. Our lexical ﬁxedness measure in Equation (2) involves two
parameters,K
v
andK
n, which determine the number of lexical variants considered in
measuring the lexical ﬁxedness of a given verb–noun pair. We make the least-biased
assumption on the proportion of variants generated by replacing the verb (K
v
)and
thosegeneratedbyreplacing thenoun(K
n
)—thatis,weassumeK
v
=K
n
.
11
Weperform
experiments on the development data, where we set the total number of variants (i.e.,
K
v
+K
n
) from 10 to 100 by steps of 10. (For simplicity, we refer to the total number
of variants as K). Figure 1(a) shows the change in performance of Fixedness
lex
as a
function ofK. Recall thatAccis the classiﬁcation accuracy, andIAPreﬂects the average
precisionofameasureinrankingidiomaticpairsbeforenon-idiomaticones.According
to these results, there is not much variation in the performance of the measure for
10 Ourdevelopmentdataalsocontainsitemsfromseveralotherdictionaries,suchasChambersIdioms
(KirkpatrickandSchwarz1982).However,ourtestdata,whichisalsousedinthetoken-based
experiments,however,onlycontainsidiomsfromthetwodictionariesODCIEandCCID.Results
reportedinthisarticleareallontestpairs;developmentpairsaremainlyusedforthedevelopmentofthe
methods.
11 Wealsoperformedexperimentsonthedevelopmentdatainwhichwedidnotrestrictthenumberof
variants,andhencedidnotenforcetheconditionK
v
=K
n
.Instead,wetriedusingavarietyofthresholds
onthesimilarityscores(fromthethesaurus)inordertoﬁndthesetofmostsimilarwordstoagivenverb
ornoun.Wefoundthatﬁxingthenumberofmostsimilarwordsismoreeffectivethanusingasimilarity
threshold,perhapsbecausetheactualscorescanbeverydifferentfordifferentwords.
73
ComputationalLinguistics Volume35,Number1
Figure1
%IAPand%AccofFixedness
lex
andFixedness
overall
overdevelopmentdata.
K≥ 20. We thus choose an intermediate value forK that yields the highest accuracy
andareasonablyhighprecision;speciﬁcally,wesetKto50.
The overall ﬁxedness measure deﬁned in Equation (6) also uses a parameter, α,
which determines the relative weights given to the individual ﬁxedness measures in
the linear combination. We experiment on the development data with different values
ofαrangingfrom0to1bystepsof.02;resultsareshowninFigure1(b).Ascanbeseen
in the ﬁgure, the accuracy of Fixedness
overall
is not affected much by the change in the
value of α. The average precision (IAP), however, shows that the combined measure
performsbestwhensomewhatequalweightsaregiventothetwoindividualmeasures,
and performs worst when the lexical ﬁxedness component is completely ignored (i.e.,
α is close to 1). These results also reinforce that a complete evaluation of our ﬁxedness
measures should include both metrics, accuracy, and average precision, as they reveal
different aspects of performance. Here, for example, Fixedness
syn
(α=1) has compa-
rable accuracy to Fixedness
lex
(α=0), reﬂecting that the two measures generally give
higher scores to idioms. However, the ranking precision of the latter is much higher
thanthatoftheformer,showingthatFixedness
lex
ranksmanyoftheidiomsatthevery
top of the list. In all our experiments reported here, we set α to .6, a value for which
Fixedness
overall
showsreasonablygoodperformanceaccordingtobothAccandIAP.
4.2 Experimental
Results and Analysis
In this section, we report the results of evaluating our measures on unseen test expres-
sions, with parameters set to the values determined in Section 4.1.3. (Results on devel-
opment data have similar trends to those on test data.) We analyze the classiﬁcation
performance of the individual lexical and syntactic ﬁxedness measures in Section 4.2.1,
anddiscusstheireffectivenessforretrievalinSection4.2.2.Section4.2.3thenlooksinto
theperformanceoftheoverallﬁxednessmeasure,andSection4.2.4presentsasummary
anddiscussionoftheresults.
4.2.1ClassiﬁcationPerformance. Here, we look into the performance of the individual
ﬁxedness measures, Fixedness
lex
and Fixedness
syn, in classifying a mixed set of verb–
nounpairsintoidiomaticandliteralclasses.Wecomparetheirperformanceagainstthe
74
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
Table2
Accuracyandrelativeerrorreductionforthetwoﬁxednessmeasures,thetwobaseline
measures,andSmadja,overalltestpairs(TEST
all
),andtestpairsdividedbyfrequency
(TEST
f
low
and TEST
f
high
).
TEST
all
TEST
f
low
TEST
f
high
Measure %Acc (%ERR)%Acc (%ERR)%Acc (%ERR)
Rand 50 50 50
PMI 63 (26) 56 (12) 70 (40)
Smadja 54 (8) 64 (28) 62 (24)
Fixedness
lex
68 (36) 70 (40) 70 (40)
Fixedness
syn
71 (42) 72 (44) 82 (64)
two baselines, Rand and PMI, as well as the two state-of-the-art methods, Smadja and
Lin. For analytical purposes, we further divide the set of all test expressions, TEST
all,
into two sets corresponding to two frequency bands: TEST
f
low
contains 50 idiomatic
and 50 literal pairs, each with total frequency (across all syntactic patterns under
consideration) between 10 and 40; TEST
f
high
consists of 50 idiomatic and 50 literal pairs,
each with total frequency of 40 or greater. Classiﬁcation performances of all measures
except Lin are given in Table 2. Lin does not assign scores to the test verb–noun pairs,
hence we cannot calculate its classiﬁcation accuracy the same way we do for the other
methods (i.e., using median as the threshold). A separate comparison between Lin and
Fixedness
lex
isprovidedattheendofthissection.
AscanbeseenintheﬁrsttwocolumnsofTable2,theinformedbaseline,PMI,shows
a large improvement over the random baseline (26% error reduction) on TEST
all
.This
showsthatmanyVNICshaveturnedintoinstitutionalized(i.e.,statisticallysigniﬁcant)
co-occurrences.Hence,onecangetrelativelygoodperformancebytreatingverb+noun
idiomatic combinations as collocations. Fixedness
lex
performs considerably better than
theinformedbaseline(36%vs.26%errorreductionon TEST
all
).Fixedness
syn
hasthebest
performance (shown in boldface), with 42% error reduction over the random baseline,
and21.6%errorreductionoverPMI.Theseresultsdemonstratethatlexicalandsyntactic
ﬁxedness are good indicators of idiomaticity, better than a simple measure of colloca-
tion such as PMI. On TEST
all, Smadja performs only slightly better than the random
baseline (8% error reduction), reﬂecting that a position-based ﬁxedness measure is not
sufﬁcientforidentifyingidiomaticcombinations.Theseresultssuggestthatlookinginto
deep linguistic properties of VNICs is necessary for the appropriate treatment of these
expressions.
12
PMI is known to perform poorly on low frequency items. To examine the effect of
frequency on the measures, we analyze their performance on the two divisions of the
12 Performingtheχ
2
testofstatisticalsigniﬁcance,weﬁndthatthedifferencesbetweenSmadjaandour
lexicalandsyntacticﬁxednessmeasuresarestatisticallysigniﬁcantatp<0.05.However,thedifferences
inperformancebetweenﬁxednessmeasuresandPMIarenotstatisticallysigniﬁcant.Notethatthisdoes
notimplythatthedifferencesarenotsubstantial,ratherthatthereisnotenoughevidenceintheobserved
datatorejectthenullhypothesis(thattwomethodsperformthesameingeneral)withhighconﬁdence.
Moreover,χ
2
isanon-parametric(distributionfree)testandhenceithaslesspowertorejectanull
hypothesis.Later,whenwetakeintoaccounttheactualscoresassignedbythemeasures,weﬁndthatall
differencesarestatisticallysigniﬁcant(seeSections4.2.2–4.2.3formoredetails).Allsigniﬁcancetestsare
performedusingtheR(2004)package.
75
ComputationalLinguistics Volume35,Number1
test data, corresponding to the two frequency bands, TEST
f
low
and TEST
f
high
.Resultsare
given in the four rightmost columns of Table 2, with the best performance shown in
boldface. As expected, the performance of PMI drops substantially for low frequency
items. Interestingly, although it is a PMI-based measure, Fixedness
lex
has comparable
performance on all data sets. The performance of Fixedness
syn
improves quite a bit
when it is applied to high frequency items, while maintaining similar performance on
the low frequency items. These results show that the lexical and syntactic ﬁxedness
measuresperformreasonablywellonbothlowandhighfrequencyitems.
13
Hencethey
can be used with a higher degree of conﬁdence, especially when applied to data that is
heterogeneous with regard to frequency. This is important because, while some VNICs
are very common, others have very low frequency, as noted by Grant (2005). Smadja
shows a notable improvement in performance when data is divided by frequency. This
effect is likely due to the fact that ﬁxedness is measured as the spread of the position-
based (raw) co-occurrence frequencies. Nonetheless, on both data sets the performance
of Smadja remains substantially worse than that of our two ﬁxedness measures (the
differencesarestatisticallysigniﬁcantinthreeoutofthefourcomparisonsatp<.05).
Collectively,theseresultsshowthatourlinguisticallymotivatedﬁxednessmeasures
areparticularlysuitedforidentifyingidiomaticcombinations,especiallyincomparison
with more general collocation extraction techniques, such as PMI or the position-based
ﬁxedness measure of Smadja (1993). Especially, our measures tend to perform well on
lowfrequencyitems,perhapsduetotheirrelianceondistinctivelinguisticpropertiesof
idioms.
WenowcomparetheclassiﬁcationperformanceofFixedness
lex
tothatofLin.Unlike
Fixedness
lex, Lin does not assign continuous scores to the verb–noun pairs, but rather
classiﬁes them as idiomatic or non-idiomatic. Thus, we cannot use the same threshold
(e.g., median) for the two methods to calculate their classiﬁcation accuracies in a com-
parable way. Recall also from Section 3.1 that the performance of both these methods
depends on the value ofK(the number of variants). We thus measure the classiﬁcation
precision of the methods at equivalent levels of recall, using the same number of
variantsKateachrecalllevelforthetwomeasures. VaryingKfrom2to100bystepsof
4, Lin and Fixedness
lex
achieve an average classiﬁcation precision of 81.5% and 85.8%,
respectively. Performing a t-test on the precisions of the two methods conﬁrms that
the difference between the two is statistically signiﬁcant at p < .001. In addition, our
methodhastheadvantageofassigningascoretoatargetverb–nounreﬂectingitsdegree
of lexical ﬁxedness. Such information can help a lexicographer decide whether a given
verb–nounshouldbeplacedinalexicon.
4.2.2RetrievalPerformance.The classiﬁcation results suggest that the individual ﬁxed-
ness measures are overall better than a simple measure of collocation at separating
idiomatic pairs from literal ones. Here, we have a closer look at their performance
by examining their goodness in ranking verb–noun pairs according to their degree
of idiomaticity. Recall that the ﬁxedness measures are devised to reﬂect the degree of
ﬁxedness and hence the degree of idiomaticity of a target verb–noun pair. Thus, the
resultofapplyingeachmeasuretoalistofmixedpairsisalistthatisrankedintheorder
13 Infact,theresultsshowthattheperformanceofbothﬁxednessmeasuresisbetterwhendataisdivided
byfrequency.Althoughweexpectbetterperformanceoverhighfrequencyitems,moreinvestigationis
neededtoverifywhethertheimprovementinperformanceoverlowfrequencyitemsisameaningful
effectormerelyanaccidentofthedataathand.
76
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
of idiomaticity. For a measure to be considered good at retrieval, we expect idiomatic
pairs to be very frequent near the top of the ranked list, and to become less frequent
towards the bottom. Precision–recall curves are very indicative of this trend: The ideal
measurewillhaveaprecisionof100%forallvaluesofrecall,namely,themeasureplaces
all idiomatic pairs at the very top of the ranked list. In reality, although the precision
drops as recall increases, we expect a good measure to keep high precision at most
levelsofrecall.
Figure 2 depicts the interpolated precision–recall curves for PMI and Smadja, and
for the lexical, syntactic, and overall ﬁxedness measures, over TEST
all
. Note that the
minimuminterpolatedprecisionis50%duetotheequalnumberofidiomaticandliteral
pairs in the test data. In this section, we discuss the retrieval performance of the two
individualﬁxednessmeasures;thenextsectionanalyzestheperformanceoftheoverall
ﬁxednessmeasure.
The precision–recall curves of Smadja and PMI are nearly ﬂat (with PMI consis-
tently higher than Smadja), showing that the distribution of idiomatic pairs in the lists
ranked by these two measures is only slightly better than random. A close look at the
precision–recall curve of Fixedness
lex
reveals that, up to the recall level of 50%, the
precision of this measure is substantially higher than that of PMI. This means that,
compared to PMI, Fixedness
lex
places more idiomatic pairs at the very top of the list.
Athigherrecalllevels(50%andhigher),Fixedness
lex
stillconsistentlyoutperformsPMI.
Nonetheless,attheserecallvalues,thetwomeasureshaverelativelylowprecision(com-
pared to the other measures), suggesting that both measures also put many idiomatic
pairs near the bottom of the list. In contrast, the precision–recall curve of Fixedness
syn
shows that its performance is consistently much better than that of PMI: Even at the
recalllevelof90%,itsprecisioniscloseto70%(cf.55%precisionofPMI).
A comparison of the precision–recall curves of the two individual ﬁxedness mea-
sures reveals their complementary nature. Compared to Fixedness
lex, Fixedness
syn
maintains higher precision at very high levels of recall, suggesting that the syntactic
ﬁxedness measure places fewer idiomatic pairs at the bottom of the ranked list. In con-
trast,Fixedness
lex
hasnotablyhigherprecisionthanFixedness
syn
atrecalllevelsofupto
40%,suggestingthattheformerputsmoreidiomaticpairsatthetopoftherankedlist.
Statistical signiﬁcance tests conﬁrm these observations: Using the Wilcoxon Signed
Rank test (1945), we ﬁnd that both Fixedness
lex
and Fixedness
syn
produce signiﬁcantly
different rankings from PMI and Smadja (p lessmuch .001). Also, the rankings of the items
Figure2
Precision–recallcurvesforPMI,Smadja,andfortheﬁxednessmeasures,over TEST
all
.
77
ComputationalLinguistics Volume35,Number1
Table3
Classiﬁcationandretrievalperformanceoftheoverallﬁxednessmeasureover TEST
all
.
Measure %Acc (%ERR)%IAP
PMI 63 (26) 63.5
Smadja 54 (8) 57.2
Fixedness
lex
68 (36) 75.3
Fixedness
syn
71 (42) 75.9
Fixedness
overall
74 (48) 84.7
producedbythetwoindividualﬁxednessmeasuresarefoundtobesigniﬁcantlydiffer-
entatp<.01.
4.2.3PerformanceoftheOverallFixednessMeasure. We now look at the classiﬁcation
and retrieval performance of the overall ﬁxedness measure. Table 3 presents %Acc,
%ERR,and%IAPof Fixedness
overall, repeating that of PMI, Smadja, Fixedness
lex,and
Fixedness
syn,forcomparison.Hereagaintheerrorreductionsarerelativetotherandom
baseline of 50%. Looking at classiﬁcation performance (expressed in terms of %Acc
and %ERR), we can see that Fixedness
overall
notably outperforms all other measures,
includinglexicalandsyntacticﬁxedness(18.8%errorreductionrelativetoFixedness
lex,
and 10% error reduction relative to Fixedness
syn
). According to the classiﬁcation
results, each of the lexical and syntactic ﬁxedness measures are good at separating
idiomatic from literal combinations, with syntactic ﬁxedness performing better. Here
we demonstrate that combining them into a single measure of ﬁxedness, while giving
more weight to the better measure, results in a more effective classiﬁer.
14
The overall
behaviorofthismeasureasafunctionofαisdisplayedinFigure3.
As can be seen in Table 3, Fixedness
lex
and Fixedness
syn
have comparable IAP:
75.3% and 75.9%, respectively. In comparison, Fixedness
overall
has a much higherIAP
of 84.7%, reinforcing the claim that combining evidence from both lexical and syntac-
tic ﬁxedness is beneﬁcial. Recall from Section 4.2.2 that the two individual ﬁxedness
measures exhibit complementary behavior, as observed in their precision–recall curves
shown in Figure 2. The precision–recall curve of the overall ﬁxedness measure shows
thatthismeasureinfactcombinesadvantagesofthetwoindividualmeasures:Atmost
recall levels, Fixedness
overall
has a higher precision than both individual measures. Sta-
tisticalsigniﬁcanceteststhatlookattheactualscoresassignedbythemeasuresconﬁrm
thattheobserveddifferencesinperformancearesigniﬁcant.TheWilcoxonSignedRank
test shows that the Fixedness
overall
measure produces a ranking that is signiﬁcantly
differentfromthoseoftheindividualﬁxednessmeasures,thebaselinePMI,andSmadja
(atplessmuch.001).
4.2.4SummaryandDiscussion.Overall, the worst performance belongs to the two collo-
cationextractionmethods,PMIandSmadja,bothinclassifyingtestpairsasidiomaticor
14 Usingaχ
2
test,weﬁndastatisticallysigniﬁcantdifferencebetweentheclassiﬁcationperformanceof
Fixedness
overall
andthatofSmadja(p < 0.01),andalsoamarginallysigniﬁcantdifferencebetweenthe
performanceofFixedness
overall
andthatofPMI(p <.1).Recallfromfootnote12(page15)thatnone
oftheindividualmeasures’performancessigniﬁcantlydifferedfromthatofPMI.Nonetheless,no
signiﬁcantdifferencesarefoundbetweentheclassiﬁcationperformanceofFixedness
overall
andthat
oftheindividualﬁxednessmeasures.
78
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
Figure3
ClassiﬁcationperformanceofFixedness
overall
ontestdataasafunctionofα.
literal, and in ranking the pairs according to their degree of idiomaticity. This suggests
that although some VNICs are institutionalized, many do not appear with markedly
high frequency, and hence only looking at their frequency is not sufﬁcient for their
recognition.Moreover,aposition-basedﬁxednessmeasuredoesnotseemtosufﬁciently
capture the syntactic ﬁxedness of VNICs in contrast to the ﬂexibility of literal phrases.
Fixedness
overall
isthebestperformerofall,supportingthehypothesisthatmanyVNICs
arebothlexicallyandsyntacticallyﬁxed,moresothanliteralverb+nouncombinations.
Inaddition,theseresultsdemonstratethatincorporatingsuchlinguisticpropertiesinto
statisticalmeasuresisbeneﬁcialfortherecognitionofVNICs.
Althoughwefocusonexperimentalexpressionswithfrequencyhigherthan10,PMI
still shows great sensitivity to frequency differences, performing especially poorly on
items with frequency between 10 and 40. In contrast, none of the ﬁxedness measures
are as sensitive to such frequency differences. Especially interesting is the consistent
performanceofFixedness
lex,whichisaPMI-basedmeasure,onlowandhighfrequency
items. These observations put further emphasis on the importance of devising new
methods for extracting multiword expressions with particular syntactic and semantic
properties,suchasVNICs.
To further analyze the performance of the ﬁxedness measures, we look at the top
and bottom 20 pairs (10%) in the lists ranked by each ﬁxedness measure. Interestingly,
the list ranked by Fixedness
overall
contains no false positives (fp) in the top 20 items,
andnofalsenegatives(fn)inthebottom20items,onceagainreinforcingtheusefulness
of combining evidence from the individual lexical and syntactic ﬁxedness measures.
False positive and false negative errors found in the top and bottom 20 ranked pairs,
respectively,forthesyntacticandlexicalﬁxednessmeasuresaregiveninTable4.(Note
thatfperrors are the non-idiomatic pairs ranked at the top, whereasfnerrors are the
idiomaticpairsrankedatthebottom.)
We ﬁrst look at the errors made by Fixedness
syn
.Theﬁrstfp error, throwhat,is
an interesting one: even though the pair is not an idiomatic expression on its own,
it is part of the larger idiomatic phrase throwone’shatinthering, and hence exhibits
syntactic ﬁxedness. This shows that our methods can be easily extended to identify
other types of verb phrase idiomatic combinations which exhibit syntactic behavior
similar to VNICs. Looking at the frequency distribution of the occurrence of the other
twofperrors,touchﬁngerandlosehome,inthe11patternsfromTable1,weobservethat
both pairs tend to appear mainly in the patterns “v
act
det:POSSn
sg
”(touchone’sﬁnger,
loseone’shome)and/or“v
act
det:POSS n
pl
”(touchone’sﬁngers). These examples show
79
ComputationalLinguistics Volume35,Number1
Table4
Errorsfoundinthetopandbottom20pairsinthelistsrankedbythetwoindividualﬁxedness
measures;fpstandsforfalsepositive,fnstandsforfalsenegative.
Measure: Fixedness
syn
Fixedness
lex
ErrorType: fp fn fp fn
throwhat makepile pushbarrow havemoment
touchﬁnger keepsecret blowbridge giveway
losehome keephand
that syntactic ﬁxedness is not a sufﬁcient condition for idiomaticity. In other words,
it is possible for non-idiomatic expressions to be syntactically ﬁxed for reasons other
than semantic idiosyncrasy. In these examples, the nouns ﬁnger and home tend to be
introducedbyapossessivedeterminer,becausetheyoftenbelongtosomeone.Itisalso
important to note that these two patterns have a low prior (i.e., verb–noun pairs do
not typically appear in these patterns). Hence, an expression with a strong tendency to
appearinsuchpatternswillbegivenahighsyntacticﬁxednessscore.
ThefrequencydistributionofthetwofnerrorsforFixedness
syn
revealsthattheyare
givenlowscoresmainlybecausetheirdistributionsaresimilartotheprior.Eventhough
makepilepreferably appears in the two patterns “v
act
det:a/ann
sg
”and“v
act
det:NULL
n
pl,” both patterns have reasonably high prior probabilities. Moreover, because of the
low frequency ofmakepile(< 40), the evidence is not sufﬁcient to distinguish it from a
typicalverb–nounpair.Thepairkeepsecrethasahighfrequency,butitsoccurrencesare
scatteredacrossall11patterns,closelymatchingthepriordistribution.Thelatterexam-
pleshowsthatsyntacticﬁxednessisnotanecessaryconditionforidiomaticityeither.
15
Analyzing the errors made by Fixedness
lex
is more difﬁcult as many factors may
affect scores given by this measure. Most important is the quality of the automatically
generated variants. We ﬁnd that in one case,pushbarrow, the ﬁrst 25 distributionally
similarnouns(takenfromtheautomaticallybuiltthesaurus)arepropernouns,perhaps
becauseBarrowis a common last name. In general, it seems that the similar verbs and
nouns for a target verb–noun pair are not necessarily related to the same sense of the
target word. Another possible source of error is that in this measure we use PMI as an
indirect clue to idiomaticity. In the case ofgivewayandkeephand, many of the variants
are plausible combinations with very high frequency of occurrence, for example,give
opportunity,giveorder,ﬁndwayfor the former, andholdhand,puthand,keepeyefor the
latter. Whereas some of these high-frequency variants are literal (e.g., holdhand)or
idiomatic (e.g.,keepeye), many have metaphorical interpretations (e.g.,giveopportunity,
ﬁndway).Inourongoingwork,weuselexicalandsyntacticﬁxednessmeasures,incom-
bination with other linguistically motivated features, to distinguish such metaphori-
cal combinations from both literal and idiomatic expressions (Fazly and Stevenson,
toappear).
One way to decrease the likelihood of making any of these errors is to combine
evidencefromthelexicalandsyntacticﬁxednessofidioms.AscanbeseeninTable4,the
twoﬁxednessmeasuresmakedifferenterrors,andcombiningthemresultsinameasure
15 Onemightarguethatkeepsecretismoresemanticallyanalyzableandhencelessidiomaticthanan
expressionsuchasshootthebreeze.Nonetheless,itisstillsemanticallymoreidiosyncraticthanafully
literalcombinationsuchaskeepapen,andhenceshouldnotberankedattheverybottomofthelist.
80
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
(theoverallﬁxedness)thatmakesfewererrors.Inthefuture,weintendtoalsolookinto
otherpropertiesofidioms,suchastheirsemanticnon-compositionality,asextrasources
ofinformation.
5. DeterminingtheCanonical Forms of VNICs
Our evaluation of the ﬁxedness measures demonstrates their usefulness for the au-
tomatic recognition of VNICs. Recall from Section 2 that idioms appear in restricted
syntacticforms,oftenreferredtoastheircanonicalforms(Glucksberg1993;Riehemann
2001; Grant 2005). For example, the idiompullone’sweightmainly appears in this form
(when used idiomatically). The lexical representation of an idiomatic combination thus
mustcontaininformationaboutitscanonicalforms.Suchinformationisnecessaryboth
for automatically generating appropriate forms (e.g., in a natural language generation
system or a machine translation system), and for inclusion in dictionaries for learners
(e.g.,inthecontextofcomputationallexicography).
Because VNICs are syntactically ﬁxed, they are mostly expected to have a small
number of canonical forms. For example,shootthebreezeis listed in many idiom dictio-
naries as the canonical form for〈shoot,breeze〉.Also,holdﬁreandholdone’sﬁreare listed
in CCID as canonical forms for 〈hold,ﬁre〉. We expect a VNIC to occur in its canonical
form(s) with substantially higher frequency than in any other syntactic patterns. We
thus devise an unsupervised method that discovers the canonical form(s) of a given
idiomatic verb–noun pair by examining its frequency of occurrence in each syntactic
pattern under consideration. Speciﬁcally, the set of the canonical form(s) of the target
pair〈v,n〉isdeﬁnedas
C(v,n)={pt
k
∈ P|z(v,n,pt
k
) >T
z
} (7)
Here,P isthesetofpatterns(seeTable1),andtheconditionz(v,n,pt
k
) >T
z
determines
whether the frequency of the target pair 〈v,n〉 in pt
k
is substantially higher than its
frequency in other patterns; z(v,n,pt
k
) is calculated using the statistic z-score as in
Equation(8),andT
z
isapredeﬁnedthreshold.
z(v,n,pt
k
) =
f(v,n,pt
k
)−f
s
(8)
wheref isthesamplemeanandsthesamplestandarddeviation.
The statisticz(v,n,pt
k
) indicates how far and in which direction the frequency of
occurrence of the target pair 〈v,n〉 in a particular patternpt
k
deviates from the sample
mean, expressed in units of the sample standard deviation. To decide whetherpt
k
is a
canonicalpatternforthetargetpair,wecheckwhetheritsz-score,z(v,n,pt
k
),isgreater
than a thresholdT
z
. Here, we setT
z
to 1, based on the distribution of z and through
examiningthedevelopmentdata.
We evaluate our unsupervised canonical form identiﬁcation method by verifying
itspredictedformsagainstODCIEandCCID.Speciﬁcally,foreachofthe100idiomatic
pairs in TEST
all, we calculate the precision and recall of its predicted canonical forms
(those whose z-scores are aboveT
z
), compared to the canonical forms listed in the two
dictionaries. The average precision across the 100 test pairs is 81.2%, and the average
recallis88%(with68ofthepairshaving100%precisionand100%recall).Moreover,we
81
ComputationalLinguistics Volume35,Number1
ﬁndthatfortheoverwhelmingmajorityofthepairs,86%,thepredictedcanonicalform
withthehighestz-scoreappearsinthedictionaryentryofthepair.
According to the entries in ODCIE and CCID, 93 out of the 100 idiomatic pairs in
TEST
all
haveonecanonicalform.Ourcanonicalformextractionmethodonaverageﬁnds
1.2 canonical forms for these 100 pairs (one canonical form for 79 of them, two for 18,
and three for 3 of these). Generally, our method tends to extract more canonical forms
than listed in the dictionaries. This is a desired property, because idiom dictionaries
oftendonotexhaustivelylistallcanonicalforms,butthemostdominantones.Examples
ofsuchcasesinclude:seethesightsforwhichourmethodalsoﬁndsseesightsasacanon-
ical form, andcatchone’sattentionfor which our method also ﬁndscatchtheattention.
Therearealsocaseswhereourmethodﬁndscanonicalformsforagivenexpressiondue
tonoiseresultingfromtheuseoftheexpressioninanon-idiomaticsense.Forexample,
forholdone’shorses, our method also ﬁndsholdthehorseandholdthehorsesas canonical
forms.Similarly,forgetthebird,ourmethodalsoﬁndsgetabird.
In a few cases (4 out of 100), our method ﬁnds fewer canonical forms than listed
in the dictionaries. These are catchthe/one’simagination, havea/one’sﬂing, makea/one’s
mark,andhavea/thenerve. For the ﬁrst two of these, the z-score of the missed pattern
is only slightly lower than our predeﬁned threshold. In other cases (8 out of 100), none
of the canonical forms extracted by our method match those in a dictionary. Some of
theseexpressionsalsohaveanon-idiomaticsensewhichmightbemoredominantthan
the idiomatic usage. For example, forgivethepushandgivetheﬂick, our method ﬁnds
giveapushandgiveaﬂick, respectively, perhaps due to the common use of the latter
formsaslightverbconstructions.Formakeone’speace,ourmethodﬁndsadifferentform,
makepeace, which seems a plausible canonical form; and moreover, the canonical form
listed in the dictionaries (makeone’speace) has a z-score which is only slightly lower
than our threshold. There is also one case where our method ﬁnds a canonical form
that corresponds to a different idiom using the same verb+noun: we ﬁndlosetouchas
a canonical form, whereas the dictionaries list an idiom with a different canonical form
(loseone’stouch)astheidiomwithloseandtouch.
In general, canonical forms extracted by our method are reasonably accurate, but
may need to be further analyzed by a lexicographer to ﬁlter out incorrectly found
patterns. Moreover, our method extracts new canonical forms for some expressions,
whichcouldbeusedtoaugmentdictionaries.
6. Automatic Identiﬁcation of VNIC Tokens
Inprevioussections,wehaveprovidedananalysisofthelexicalandsyntacticbehavior
of idiomatic expressions. We have shown that our proposed techniques that draw on
such properties can successfully distinguish an idiomatic verb+noun combination (a
VNICtype)suchasgetthesackfromanon-idiomatic(literal)onesuchasgetthebag.Itis
important, however, to note that a potentially idiomatic expression such asgetthesack
can also have a literal interpretation in a given context, as inJoegotthesackfromthetop
shelf. This is true of many potential idioms, although the relative proportion of literal
usages may differ from one expression to another. For example, an expression such as
seestarsismuchmorelikelytohavealiteralinterpretationthangetthesack(accordingto
ourﬁndingsintheBNC).Identiﬁcationofidiomatictokensincontextisthusnecessary
forafullunderstandingoftext,andthiswillbethefocusofSections6and7.
Recent studies addressing token identiﬁcation for idiomatic expressions mainly
performthetaskasoneofwordsensedisambiguation,anddrawonthelocalcontextof
82
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
atokentodisambiguateit.Suchtechniqueseitherdonotuseanyinformationregarding
the linguistic properties of idioms (Birke and Sarkar 2006), or mainly focus on the
property of non-compositionality (Katz and Giesbrecht 2006). Studies that do make
use of deep linguistic information often handcode the knowledge into the systems
(Uchiyama,Baldwin,andIshizaki2005;Hashimoto,Sato,andUtsuro2006).Ourgoalis
to develop techniques that draw on the speciﬁc linguistic properties of idioms for their
identiﬁcation, without the need for handcoded knowledge or manually labelled train-
ing data. Such unsupervised techniques can also help provide automatically labelled
(noisy)trainingdatatobootstrap(semi-)supervisedmethods.
In Sections 3 and 4, we showed that the lexical and syntactic ﬁxedness of idioms
is especially relevant to their type-based recognition. We expect such properties to also
berelevantfortheirtokenidentiﬁcation.Moreover,wehaveshownthatitispossibleto
learnabouttheﬁxednessofidiomsinanunsupervisedmanner.Here,weproposeunsu-
pervisedtechniquesthatdrawonthesyntacticﬁxednessofidiomstoclassifyindividual
tokens of a potentially idiomatic phrase as literal or idiomatic. We also put forward a
classiﬁcation technique that combines such information (in the form of noisy training
data) with evidence from the local context of usages of an expression. In Section 6.1,
we elaborate on the underlying assumptions of our token identiﬁcation techniques.
Section 6.2 then describes our proposed methods that draw on these assumptions to
performthetask.
6.1Underlying Assumptions
Although there may be ﬁne-grained differences in meaning across the idiomatic us-
ages of an expression, as well as across its literal usages, we assume that the idiomatic
and literal usages correspond to two coarse-grained senses of the expression. We will
refer then to each of the literal and idiomatic designations as a (coarse-grained) mean-
ing of the expression, while acknowledging that each may have multiple ﬁne-grained
senses.
Recall from Section 2 that idioms tend to be somewhat ﬁxed with respect to the
syntactic conﬁgurations in which they occur. For example, pullone’sweight tends to
mainly appear in this form when used idiomatically. Other forms of the expression,
such aspulltheweights, typically are only used with a literal meaning. In other words,
anidiomtendstohaveone(orasmallnumberof)canonicalform(s),whichareitsmost
preferred syntactic patterns.
16
Here we assume that, in most cases, idiomatic usages of
an expression tend to occur in its canonical form(s). We also assume that, in contrast,
the literal usages of an expression are less syntactically restricted, and are expressed
in a greater variety of patterns. Because of their relative unrestrictedness, literal usages
may occur in a canonical form for that expression, but usages in a canonical form are
more likely to be idiomatic. Usages in alternative syntactic patterns for the expression,
which we refer to as the non-canonical forms of the expression, are more likely to be
literal.
Drawing on these assumptions, we develop unsupervised methods that deter-
mine, for each verb+noun token in context, whether it has an idiomatic or a literal
16 Asnotedpreviously,93outofthe100idiomaticpairsin TEST
all
haveonecanonicalform,accordingtothe
entriesinODCIEandCCID.Also,ourcanonicalformextractionmethodonaverageﬁnds1.2canonical
formsforthe100testidioms.
83
ComputationalLinguistics Volume35,Number1
interpretation.Clearly,thesuccessofourmethodsdependsontheextenttowhichthese
assumptionshold(wewillreturntotheseassumptionsinSection7.2.3).
6.2 Proposed
Methods
This section elaborates on our proposed methods for identifying the idiomatic and
literal usages of a verb+noun combination: the CFORM method that uses knowledge
ofcanonicalformsonly,andthe CONTEXTmethodthatalsoincorporatesdistributional
evidence about the local context of a token. Both methods draw on our assumptions
described herein, that usages in the canonical form(s) for a potential idiom are more
likely to be idiomatic, and those in other forms are more likely to be literal. Because
our methods need information about canonical forms of an expression, we use the
unsupervisedmethoddescribedinSection5toﬁndtheseautomatically.Inthefollowing
discussion,wedescribeeachmethodinmoredetail.
CFORM.This method classiﬁes an instance (token) of an expression as idiomatic if it
occurs in one of the automatically determined canonical form(s) for that expression
(e.g.,pullone’sweight), and as literal otherwise (e.g.,pullaweight,pulltheweights). The
underlyingassumptionofthismethodisthatinformationaboutthecanonicalform(s)of
anidiomtypecanprovideareasonablyaccurateclassiﬁcationofitsindividualinstances
asliteraloridiomatic.
CONTEXT.Recallourassumptionthattheidiomaticandliteralusagesofanidiomcorre-
spondtotwocoarse-grainedmeaningsoftheexpression.Itisnaturaltofurtherassume
that the literal and idiomatic usages have more in common semantically within each
group thanbetween thetwogroups. Adopting adistributional approach tomeaning—
where the meaning of an expression is approximated by the words with which it co-
occurs (Firth 1957)—we would expect the literal and idiomatic usages of an expression
totypicallyoccurwithdifferentsetsofwords.
Indeed, in a supervised setting, Katz and Giesbrecht (2006) show that the local
context of an idiom usage is useful in identifying its sense. Inspired by this work, we
proposeanunsupervisedmethodthatincorporatesdistributionalinformationaboutthe
local context of the usages of an idiom, in addition to the (syntactic) knowledge about
its canonical forms, in order to determine if its token usages are literal or idiomatic.
To achieve this, the method compares the context surrounding a test instance of an
expressionto“gold-standard”contextsfortheidiomaticandliteralusagesoftheexpres-
sion, which are taken from noisy training data automatically labelled using canonical
forms.
17
For each test instance of an expression, the CONTEXT method thus compares its
co-occurring words to two sets of gold-standard co-occurring words: one typical of
idiomatic usages and one typical of literal usages of the expression (we will shortly
explain precisely how we ﬁnd these). If the test token is determined to be (on aver-
age) more similar to the idiomatic usages, then it is labelled as idiomatic. Other-
wise, it is labelled as literal. To measure similarity between two sets of words, we use
17 Thetwo
CONTEXTmethodsinourearlierwork(Cook,Fazly,andStevenson2007)werebiasedbecause
theyusedinformationaboutthecanonicalformofatesttoken(inadditiontocontextinformation).
Wefoundthatwhenthebiaswasremoved,thesimilaritymeasureusedinthosetechniqueswasnot
aseffective,andhencewehavedevelopedadifferentmethodhere.
84
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
a standard distributional similarity measure, Jaccard, deﬁned subsequently.
18
In the
followingequationAandBrepresentthetwosetsofwordstobecompared:
Jaccard(A,B)=
A∩B
A∪B
(9)
NowweexplainhowtheCONTEXTmethodﬁndstypicallyco-occurringwordsforeach
oftheidiomaticandliteralmeaningsofanexpression.Notethatunlikeinasupervised
setting,herewedonotassumeaccesstomanuallyannotatedtrainingdata.Wethususe
knowledgeofautomaticallyacquiredcanonicalformstoﬁndthese.
The CONTEXT method labels usages of an expression in a leave-one-out strategy,
where each test token is labelled by using the other tokens as noisy training (gold-
standard) data. Speciﬁcally, to provide gold-standard data for each instance of an
expression, we ﬁrst divide the other instances (of the same expression) into likely-
idiomaticandlikely-literalgroups,wheretheformergroupcontainsusagesincanonical
form(s) and the latter contains usages in non-canonical form(s). We then pick represen-
tative usages from each group by selecting theKinstances that are most similar to the
instancebeinglabelled(thetesttoken)accordingtotheJaccardsimilarityscore.
Recall that we assume canonical form(s) are predictive of the idiomatic usages and
non-canonical form(s) are indicative of the literal usages of an expression. We thus
expect the co-occurrence sets of the selected canonical and non-canonical instances to
reﬂect the idiomatic and literal meanings of the expression, respectively. We take the
averagesimilarityofthetesttokentotheKnearestcanonicalinstances(likelyidiomatic)
andtheKnearestnon-canonicalinstances(likelyliteral),andlabelthetesttokenaccord-
ingly.
19
In the event that there are less thanKcanonical or non-canonical form usages
ofanexpression,wetaketheaveragesimilarityoverhowevermanyinstancesthereare
of this form. If we have no instances of one of these forms, we classify each token as
idiomatic,thelabelweexpecttobemorefrequent.
7. VNICToken Identiﬁcation: Evaluation
To evaluate the performance of our proposed token identiﬁcation methods, we use
each in a classiﬁcation task, in which the method indicates for each instance of a given
expression whether it has an idiomatic or a literal interpretation. Section 7.1 explains
thedetailsofourexperimentalsetup.Section7.2thenpresentstheexperimentalresults
aswellassomediscussionandanalysis.
7.1Experimental Setup
7.1.1ExperimentalExpressionsandAnnotation. In our token classiﬁcation experiments,
we use a subset of the 180 idiomatic expressions in the development and test data sets
used in the type-based experiments of Section 4. From the original 180 expressions, we
discard those whose frequency in the BNC is lower than 20, to increase the likelihood
that there are both literal and idiomatic usages of each expression. We also discard any
18 Itispossibletoincorporateextraknowledgesources,suchasWordNet,formeasuringsimilarity
betweentwosetsofwords.However,ourintentionistofocusonpurelyunsupervised,knowledge-lean
approaches.
19 Wealsotriedusingtheaveragesimilarityofthetesttokentoallinstancesineachgroup.However,
wefoundthatfocusingonthemostsimilarinstancesfromeachgroupperformsbetter.
85
ComputationalLinguistics Volume35,Number1
expression that is not from the two dictionaries ODCIE and CCID (see Section 4.1.2
for more details on the original data sets). This process results in the selection of
60candidateverb–nounpairs.
Foreachoftheselectedpairs,100sentencescontainingitsusagewererandomlyex-
tractedfromtheautomaticallyparsedBNC,usingthemethoddescribedinSection4.1.1.
For apair which occurs less than100 timesintheBNC,allofitsusages were extracted.
Twojudgeswereaskedtoindependentlylabeleachuseofeachcandidateexpressionas
literal, idiomatic, or unknown. When annotating a token, the judges had access to only
thesentenceinwhichitoccurred,andnotthesurroundingsentences.Ifthiscontextwas
insufﬁcient to determine the class of the expression, the judge assigned the unknown
label.Inanefforttoassurehighagreementbetweenthejudges’annotations,thejudges
were also provided with the dictionary deﬁnitions of the idiomatic meanings of the
expressions.
Idiomaticity is not a binary property; rather it is known to fall on a continuum
from completely semantically transparent, or literal, to entirely opaque, or idiomatic.
The human annotators were required to pick the label, literal or idiomatic, that best ﬁt
the usage in their judgment; they were not to use the unknown label for intermediate
cases. Figurative extensions of literal meanings were classiﬁed as literal if their overall
meaning was judged to be fairly transparent, as inYouturnrightwhenwehit the road
attheendofthistrack(takenfromtheBNC).Sometimesanidiomaticusage,suchashave
wordinAtthemomenttheyonly had the word ofNicola’shusbandforwhathadhappened
(also taken from the BNC), is somewhat directly related to its literal meaning, which
is not the case for more semantically opaque idioms such ashittheroof. This sentence
wasclassiﬁedasidiomaticbecausetheidiomaticmeaningismuchmoresalientthanthe
literalmeaning.
First, our primary judge, a native English speaker and an author of this paper,
annotatedeachuseofeachcandidateexpression.Basedonthisjudge’sannotations,we
removed the 25 expressions with fewer than 5 instances of either of their literal or idi-
omaticmeanings, leaving28expressions.
20
(Wewillrevisitthe25removedexpressions
inSection7.2.4.)Theremainingexpressionswerethensplitintodevelopment(DEV)and
test (TEST) sets of 14 expressions each. The data was divided such that DEV and TEST
would be approximately equal with respect to the frequency of their expressions, as
wellastheirproportionofidiomatic-to-literalusages(accordingtotheprimaryjudge’s
annotations). At this stage, DEV and TEST contained a total of 813 and 743 tokens,
respectively.
Our second judge, also a native English-speaking author of this paper, then anno-
tated DEV and TEST sentences. The observed agreement and unweighted kappa score
(Cohen 1960) on TEST were 76% and 0.62, respectively. The judges discussed tokens on
whichtheydisagreedtoachieveaconsensusannotation.Finalannotationsweregener-
ated by removing tokens that received theunknown label as theconsensus annotation,
leaving DEVand TESTwithatotalof573and607tokens,andanaverageof41and43to-
kens per expression, respectively. Table 5 shows the DEV and the TEST verb–noun pairs
used in our experiments. The table also contains information on the number of tokens
consideredforeachpair,aswellasthepercentageofitsusageswhichareidiomatic.
20 Fromtheoriginalsetof60expressions,sevenwereexcludedbecauseourprimaryannotatordidnot
provideanyannotationsforthem.Theseincludecatchone’sbreath,cutone’slosses,andpushone’sluck(for
whichourannotatordidnothaveaccesstoaliteralinterpretation);andblowone’s(own)horn,pullone’s
hair,givealift,andgetthebird(forwhichourannotatordidnothaveaccesstoanidiomaticmeaning).
86
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
Table5
Experimental DEVand TESTverb–nounpairs,theirtokenfrequency(FRQ),andthepercentageof
theirusagesthatareidiomatic(%IDM),orderedindecreasing%IDM.
DEV TEST
verb–noun FRQ %IDM verb–noun FRQ %IDM
ﬁndfoot 52 90 haveword 89 90
makeface 30 90 losethread 20 90
getnod 26 89 getsack 50 86
pullweight 33 82 makemark 85 85
kickheel 38 79 cutﬁgure 43 84
hitroad 31 77 pullpunch 22 82
takeheart 79 73 blowtop 28 82
pullplug 65 69 makescene 48 58
blowtrumpet 29 66 makehay 17 53
hitroof 17 65 getwind 29 45
losehead 38 55 makehit 14 36
makepile 25 32 blowwhistle 78 35
pullleg 51 22 holdﬁre 23 30
seestar 61 8 hitwall 61 11
7.1.2Baselines,Parameters,andPerformanceMeasures. We compare the performance of
our proposed methods, CFORM and CONTEXT, with the baseline of always predicting
an idiomatic interpretation, the most frequent meaning in our development data. We
also compare the unsupervised methods against a supervised method, SUP, which is
similar to CONTEXT, except that it forms the idiomatic and literal co-occurrence sets
from manually annotated data (instead of automatically labelled data using canonical
forms).Like CONTEXT,SUPalsoclassiﬁestokensinaleave-one-outmethodologyusing
theK idiomatic and literal instances which are most similar to a test token. For both
CONTEXT and SUP, we set the value of K (the number of similar instances used as
gold-standard)to5,sinceexperimentson DEVindicatedthatperformancedidnotvary
substantiallyusingarangeofvaluesofK.
For all methods, we report the accuracy macro-averaged over all expressions in
TEST. We use the individual accuracies (accuracies for the individual expressions) to
perform t-tests for verifying whether different methods have signiﬁcantly different
performance. To further analyze the performance of the methods, we also report their
recallandprecisiononidentifyingusagesfromeachoftheidiomaticandliteralclasses.
7.2Experimental Results and Analysis
We ﬁrst discuss the overall performance of our proposed unsupervised methods in
Section 7.2.1. Results reported in Section 7.2.1 are on TEST (results on DEV have similar
trends, unless noted otherwise). Next, we look into the performance of our methods
on expressions with different proportions of idiomatic-to-literal usages in Section 7.2.2,
which presents results on TEST and DEV combined, as explained subsequently. Sec-
tion7.2.3providesananalysisoftheerrorsmadebecauseofusingcanonicalforms,and
identiﬁes some possible directions for future work. In Section 7.2.4, we present results
on a new data set containing expressions with highly skewed proportion of idiomatic-
to-literalusages.
87
ComputationalLinguistics Volume35,Number1
Table6
Macro-averagedaccuracy(%Acc)andrelativeerrorratereduction(%ERR)onTESTexpressions.
Method %Acc (%ERR)
Baseline 61.9
Unsupervised CONTEXT 65.8 (10.2)
CFORM 72.4 (27.6)
Supervised SUP 82.7 (54.6)
7.2.1OverallPerformance.Table 6 shows the macro-averaged accuracy on TEST of our
two unsupervised methods, as well as that of the baseline and the supervised method
forcomparison.Thebestunsupervisedperformanceisindicatedinboldface.
As the table shows, both of our unsupervised methods as well as the supervised
method outperform the baseline, conﬁrming that the canonical forms of an expression,
and local context, areboth informative indistinguishing literaland idiomatic instances
of the expression.
21
Moreover, CFORMoutperforms CONTEXT(difference is marginally
signiﬁcant at p < .06), which is somewhat unexpected, as CONTEXT was proposed
as an improvement over CFORM in that it combines contextual information along
with the syntactic information provided by CFORM. We return to these results later
(Section 7.2.3) to offer some reasons as to why this might be the case. However, the
results using CFORM conﬁrm our hypothesis that canonical forms—which reﬂect the
overall behavior of a verb+noun type—are strongly informative about the class of a
token. Importantly, this is the case even though the canonical forms that we use are
imperfectknowledgeobtainedautomaticallythroughanunsupervisedmethod.
Comparing CFORM with SUP, we observe that even though on average the latter
outperforms the former, the difference is not statistically signiﬁcant (p > .1). A close
look at the performance of these methods on the individual expressions reveals that
neitherconsistentlyoutperformstheotheronall(orevenmost)expressions.Moreover,
aswewillseeinSection7.2.2, SUPseemstogainmostofitsadvantageover CFORMon
expressions with a low proportion of idiomatic usages, for which canonical forms tend
tohavelesspredictivevalue(seeSection7.2.3fordetails).
Recall that both CONTEXT and SUP label each token by comparing its local context
to those of itsKnearest “idiomatic” and itsKnearest “literal” usages. The difference is
that CONTEXT uses noisy (automatically) labelled data to identify these nearest usages
foreachtoken,whereas SUPusesmanuallylabelleddata.Onepossibledirectionforfu-
tureworkisthustoinvestigatewhetherprovidingsubstantiallylargeramountsofdata
alleviatestheeffectofnoise,asisoftenfoundtobethecasebyresearchersintheﬁeld.
7.2.2PerformanceBasedonClassDistribution.Recall from Section 6 that both of our un-
supervised techniques fortokenidentiﬁcation dependonhowaccurately thecanonical
formsofanexpressioncanbeacquired.Thecanonicalformacquisitiontechniquewhich
weusehereworkswelliftheidiomaticmeaningofanexpressionissufﬁcientlyfrequent
compared to its literal usage. In this section, we thus examine the performance of the
21 Performingapairedt-test,weﬁndthatthedifferencebetweenthebaselineand CFORMismarginally
signiﬁcant,p<.06,whereasthedifferencebetweenbaselineand CONTEXTisnotstatisticallysigniﬁcant.
Thedifferencebetweenthebaselineand SUPissigniﬁcantatp<.01.Thetrendon DEVissomewhat
similar:baselineand CFORMaresigniﬁcantlydifferentatp<.05; SUPismarginallydifferentfrom
baselineatp<.06.
88
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
Table7
Macro-averagedaccuracy(%Acc)andrelativeerrorratereduction(%ERR)onthe28expressions
in DT(DEVand TESTcombined),dividedaccordingtotheproportionofidiomatic-to-literal
usages(highandlow).
DT
I
high
DT
I
low
Method %Acc (%ERR)%Acc (%ERR)
Baseline 81.4 35.0
Unsupervised CONTEXT 80.6 (−4.3) 44.6 (14.8)
CFORM 84.7 (17.7) 53.4 (28.3)
Supervised SUP 84.4 (16.1) 76.8 (64.3)
tokenidentiﬁcationmethodsforexpressionswithdifferentproportionsofidiomatic-to-
literalusages.
We merge DEV and TEST (referring to the new set as DT), and then divide the re-
sultingsetof28expressions according totheirproportionofidiomatic-to-literal usages
(as determined by the human annotations) as follows.
22
Looking at the proportion of
idiomatic usages of our expressions in Table 5, we can see that there are gaps between
55% and 65% in DEV, and between 58% and 82% in TEST, in terms of proportion
ofidiomaticusages.Thevalueof65%thusservesasanaturallowerboundfordominant
idiomatic usage, and the value of 58% as a natural upper bound for non-dominant
idiomaticusage.Wethereforesplit DTintotwosets: DT
I
high
contains17expressionswith
65–90%oftheirusagesbeingidiomatic(i.e.,theiridiomaticusageisdominant),whereas
DT
I
low
contains11expressionswith8–58%oftheiroccurrencesbeingidiomatic(i.e.,their
idiomaticusageisnotdominant).
Table 7 shows the average accuracy of all the methods on these two groups of
expressions, with the best performance on each group shown in boldface. We ﬁrst look
attheperformanceofourmethodsonDT
I
high
.Ontheseexpressions,CFORMoutperforms
both the baseline (difference is not statistically signiﬁcant) and CONTEXT (difference is
statisticallysigniﬁcantatp <.05).CFORMalsohasacomparableperformancetothesu-
pervisedmethod,reinforcingthatfortheseexpressionsaccuratecanonicalformscanbe
acquiredandthatsuchknowledgecanbeusedwithhighconﬁdencefordistinguishing
idiomaticandliteralusagesincontext.
We now look into the performance on expressions in DT
I
low
. On these, both CFORM
and CONTEXT outperform the baseline, showing that even for expressions whose idi-
omaticmeaningisnotdominant,automaticallyacquiredcanonicalformscanhelpwith
theirtokenclassiﬁcation.Nonetheless,boththesemethodsperformsubstantiallyworse
than the supervised method, reinforcing that the automatically acquired canonical
formsarenoisier,andhencelesspredictive,thantheyareforexpressionsin DT
I
high
.
The poor performance of the unsupervised methods on expressions in DT
I
low
(com-
pared to the supervised performance) is likely to be mostly due to the less predictive
canonical forms extracted for these expressions. In general, we can conclude that when
canonical forms can be extracted with a high accuracy, the performance of the CFORM
method is comparable to that of a supervised method. One possible way of improving
theperformanceofunsupervisedmethodsisthustodevelopmoreaccuratetechniques
fortheautomaticacquisitionofcanonicalforms.
22 Wecombinethetwosetsinordertohaveasufﬁcientnumberofexpressionsineachgroupafterdivision.
89
ComputationalLinguistics Volume35,Number1
Table8
Confusionmatrixfor CF ORMonexpressionblowtrumpet.idm=idiomaticclass;lit=literalclass;
tp=truepositive;fp=falsepositive;fn=falsenegative;tn=truenegative.
True Class
idm lit
Predicted idm 17=tp 6=fp
Class lit 2=fn 4=tn
Table9
FormulasforcalculatingSensandPPV(recallandprecisionfortheidiomaticclass),andSpec
andNPV(recallandprecisionfortheliteralclass)fromaconfusionmatrix.
recall(R)precison(P)
idm Sens =
tp
tp+fn
PPV =
tp
tp+fp
lit Spec =
tn
tn+fp
NPV =
tn
tn+fn
Accuracyisoftennotasufﬁcientmeasurefortheevaluationofabinary(two-class)
classiﬁer, especially when the number of items in the two classes (here, idiomatic and
literal) differ. Instead, one can have a closer look at the performance of a classiﬁer by
examining its confusion matrix, which compares the labels predicted by the classiﬁer
for each item with its true label. As an example, the confusion matrix of the CFORM
methodfortheexpressionblowtrumpetisgiveninTable8.
Note that the choice of idiomatic as the positive class (and literal as the negative
class) is arbitrary; however, because our ultimate goal is to identify idiomatic usages,
thereisanaturalreasonforthischoice.Tosummarizeaconfusionmatrix,fourstandard
measuresareoftenused,whicharecalculatedfromthecellsinthematrix.Themeasures
are sensitivity (Sens), positive predictive value (PPV), speciﬁcity (Spec), and negative
predictive value (NPV), and are calculated as in Table 9. As stated in the table, Sens
and PPV are equivalents of recall and precision for the positive (idiomatic) class, also
referredtoasR
idm
andP
idm
laterinthearticle.Similarly,SpecandNPVareequivalents
ofrecallandprecisionforthenegative(literal)class,alsoreferredtoasR
lit
andP
lit
.
23
Table 10 gives the trimmed mean values of these four performance measures over
expressionsin DT
I
high
and DT
I
low
forthebaseline,thetwounsupervisedmethods,andthe
supervised method.
24
(The performance measures on individual expressions are given
inTables12,13,and14intheAppendix.)Table10showsthat,asexpected,thebaseline
hasveryhighSens(100%recallonidentifyingidiomaticusages),butverylowSpec(0%
23 Wemainlyrefertothesemeasuresusingtheirstandardnamesintheliterature:Sens,PPV,Spec,and
NPV.Alongsidethestandardnames,weusethemoreexpressivenamesR
idm,P
idm,R
lit,andP
lit,to
remindthereaderaboutthesemanticsofthemeasures.
24 Whenaveraginginterdependentmeasures,suchasprecisionandrecall,oneneedstomakesurethat
theobservedtrendintheaveragesisconsistentwiththatintheindividualvalues.Trimmed meanisa
standardstatisticusedinsuchcases,whichisequivalenttothemeanafterdiscardingapercentage(often
between5and25)ofthesampledataatthehighandlowends.Here,wereporta14%-trimmedmean,
whichinvolvesremovingtwodatapointsfromeachend.Theanalysispresentedhereisbasedonthe
trimmedmeans,aswellastheindividualvaluesoftheperformancemeasures.
90
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
Table10
Detailedclassiﬁcationperformanceofallmethodsover DT
I
high
and DT
I
low
.Performanceisgiven
usingfourmeasures:SensorR
idm,PPVorP
idm,SpecorR
lit,andNPVorP
lit,macro-averaged
using14%-trimmedmean.
DataSet Method Sens(R
idm
) PPV(P
idm
)Spec(R
lit
)NPV(P
lit
)
Baseline 1.00 .82 0.00 0.00
DT
I
high
CONTEXT .97 .84 .11 .18
CFORM .95 .92 .61 .71
SUP .99 .86 .22 .53
DataSet Method Sens(R
idm
) PPV(P
idm
)Spec(R
lit
)NPV(P
lit
)
Baseline 1.00 .36 0.00 0.00
DT
I
low
CONTEXT .89 .37 .22 .63
CFORM .86 .43 .36 .86
SUP .44 .62 .88 .80
recall on identifying literal usages). We thus expect a well-performing method to have
lowerSensthanthebaseline,buthigherSpecandalsohigherPPVandNPV(i.e.,higher
precisiononbothidiomaticandliteralusages).
Looking at performance on DT
I
high, we ﬁnd that all three methods have reasonably
high Sens and PPV, revealing that the methods are good at labeling idiomatic usages.
Performanceonliteralusages,however,differsacrossthethreemethods. CONTEXThas
verylowSpecandNPV,showingthatittendstolabelmosttokens—includingtheliteral
ones—as idiomatic. A close look at the performance of this method on the individual
expressions also conﬁrms this tendency: on many expressions (10 out of 17) the Spec
and NPV of CONTEXT are both zero (see Table 13 in the Appendix). As we will see in
Section 7.2.3, this tendency is partly due to the distribution of the idiomatic and literal
usages in canonical and non-canonical forms; because literal usages can also appear in
a canonical form, for many expressions there are often not many non-canonical form
instances. (Recall that, for training, CONTEXTuses instances in canonical form as being
idiomatic and those in non-canonical form as being literal.) Thus, in many cases, it
is a priori more likely that a token is more similar to the K most similar canonical
form instances. Interestingly, CFORM is the method with the highest Spec and NPV,
even higher than those of the supervised method. Nonetheless, even CFORM is overall
much better at identifying idiomatic tokens than literal ones (see Section 7.2.3 for more
discussiononthis).
We now turn to performance on DT
I
low
.CFORM has a high Sens, but a low PPV,
indicating that most idiomatic usages are identiﬁed correctly, but many literal usages
are also misclassiﬁed as idiomatic (hence a low Spec). CONTEXT shows the same trend
as CFORM,thoughoverallithaspoorerperformance.Performanceof SUPvariesacross
the expressions in this group: SUP is very good at identifying literal usages of these
expressions (high Spec and NPV for all expressions). Nonetheless, SUP has a low recall
inidentifyingidiomaticusages(lowSens)formanyoftheseexpressions.
7.2.3DiscussionandErrorAnalysis.Inthissection,weexaminetwomainissues.First,we
look into the plausibility of our original assumptions regarding the predictive value of
canonicalforms(andnon-canonicalforms).Second,weinvestigatetheappropriateness
ofourautomaticallyextractedcanonicalforms.
91
ComputationalLinguistics Volume35,Number1
To learn more about the predictive value of canonical forms, we examine the per-
formance of CFORM on the 28 expressions under study. More speciﬁcally, we look at
the values of Sens, PPV, Spec, and NPV on these expressions, as shown in Table 12
in the Appendix. On expressions in DT
I
high,CFORM hasbothhighSensandhighPPV.
The formulas in Table 9 indicate that if both Sens and PPV are high, thentpgreatermuchfnand
tpgreatermuchfp.Thus,mostidiomaticusagesofexpressionsin DT
I
high
appearinacanonicalform,
and most usages in a canonical form are idiomatic. The values of Spec and NPV on the
same expressions are in general lower (compared to Sens and PPV), showing thattnis
notmuchhigherthanfporfn.
On expressions in DT
I
low,CFORM generally has high Sens but low-to-medium PPV.
This indicates that for these expressions, most idiomatic usages appear in a canonical
form,butnotallusagesinacanonicalformareidiomatic.Ontheseexpressions,CFORM
has generally high NPV, but mostly low Spec. These indicate thattngreatermuchfn, that is, most
usagesinanon-canonicalformareliteral,andthattnisoftenlowerthanfp,thatis,many
literalusagesalsoappearinacanonicalform.Forexample,almostallusagesofhitwall
in a non-canonical form are literal, but most of its literal usages appear in a canonical
form.
Generally, it seems that, as we expected, literal usages are less restricted in terms
of the syntactic form they appear in; they can appear in both canonical form(s) and
in non-canonical form(s). For an expression with a low proportion of literal usages,
we can thus acquire canonical forms that are both accurate and have high predictive
value for identifying idiomatic usages in context. On the contrary, for expressions
with a relatively high proportion of literal usages, automatically acquired canonical
forms are less accurate and also have low predictive value (i.e., they are not speciﬁc
to idiomatic usages). We expected that using contextual information would help in
suchcases.However, our CONTEXTmethodreliesonnoisytrainingdataautomatically
labelled using information about canonical forms. Given these ﬁndings, it is not sur-
prisingthatthismethodperformssubstantiallyworsethanacorrespondingsupervised
methodthatusessimilarcontextualinformation,butmanuallylabelledtrainingdata.It
remainstobetestedinthefuturewhetherprovidingmorenoisydatawillhelp.Another
possible future direction is to develop context methods that can better exploit noisy
labelleddata.
Nowwe look ata fewcases where our automatically extracted canonical forms are
not sufﬁciently accurate. For a verb+noun such asmakepile(i.e.,makeapileofmoney),
we correctly identify only some of the canonical forms. The automatically determined
canonical forms formakepilearemakeapileandmakepiles. However, we ﬁnd that idi-
omaticusagesofthisexpressionaresometimesoftheformmakeone’spile.Furthermore,
we ﬁnd that the frequency of this form is much higher than that of the non-canonical
forms,andnotsubstantiallylowerthanthefrequencycut-offforselectionasacanonical
form.Thisindicatesthatourheuristicforselectingpatternsascanonicalformscouldbe
ﬁne-tunedtoyieldanimprovementinperformance.
Fortheexpressionpullplug,weidentifyitscanonicalformaspulltheplug,butﬁnda
mixtureofliteralandidiomaticusagesinthisform.However,manyoftheliteralusages
areverb-particleconstructions usingout(pulltheplugout),whilemanyoftheidiomatic
usages occur with a prepositional phrase headed by on (pulltheplugon). This indi-
cates that incorporating information about particles and prepositions could improve
the quality of the canonical forms. Other syntactic categories, such as adjectives, may
also be informative in determining canonical forms for expressions which are typically
used idiomatically with words of a particular syntactic category, as inblowone’sown
trumpet.
92
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
Table11
Macro-averagedaccuracy(%Acc)andrelativeerrorratereduction(%ERR)onthe23expressions
in SKEWED-IDMandonthe37expressionsinthecombinationof TESTand SKEWED-IDM(ALL).
SKEWED-IDM ALL
Method %Acc (%ERR)%Acc (%ERR)
Baseline 97.9 84.3
Unsupervised CONTEXT 94.2 (−176.2) 83.3 (−6.4)
CFORM 86.7 (−533.3) 81.3 (−19.1)
Supervised SUP 97.9 (0.0) 92.1 (49.7)
7.2.4PerformanceonExpressionswithSkewedDistribution.Recall from Section 7.1.1 that,
fromtheoriginalsetof60candidateexpressions,weexcludedthosethathadfewerthan
5instancesofeitheroftheirliteraloridiomaticmeanings.Itisnonethelessimportantto
see how well our methods perform on such expressions. In this section, we thus report
the performance of our measures on the set of 23 expressions with mostly idiomatic
usages, referred to as SKEWED-IDM. Table 11 presents the macro-averaged accuracy of
ourmethodsontheseexpressions.Thistablealsoshowstheaccuracyonallunseentest
expressions, that is, the combination of SKEWED-IDM and TEST, referred to as ALL,for
comparison.
25
On SKEWED-IDM,thesupervisedmethodperformsaswellasthebaseline,whereas
bothunsupervisedmethodsperformworse.
26
Notethatfor19outofthe23expressions
in SKEWED-IDM,allinstancesareidiomatic,andthebaselineaccuracy isthus100%.On
these, SUP also has 100% accuracy because no literal instances are available, and thus
SUP labels every token as idiomatic (same as the baseline). As for the unsupervised
methods, we can see that, unlike on TEST,theCONTEXT method outperforms CFORM
(the difference is statistically signiﬁcant at p < .001). We saw previously that CONTEXT
tends to label usages as idiomatic. This bias might be partially responsible for the
better performance of CONTEXT on this data set. Moreover, we ﬁnd that many of these
expressionstendtoappearinahighlyfrequentcanonicalform,butalsoinlessfrequent
syntacticformswhichwe(perhapsincorrectly)considerasnon-canonicalforms.When
consideringtheperformanceonallunseentestexpressions(ALL),neitherunsupervised
methodperformsaswellasthebaseline,butthesupervisedmethodoffersasubstantial
improvementoverthebaseline.
27
Our annotators pointed out that for many of the expressions in SKEWED-IDM,
either a literal interpretation was almost impossible (as for catchone’simagination),
or extremely implausible (as for kickthehabit). Hence, the annotators could predict
beforehand that the expression would be mainly used with an idiomatic meaning. A
semi-supervised approach that combines expert human knowledge with automatically
extracted corpus-drawn information can thus be beneﬁcial for the task of identifying
25 Theresultsobtainedonthetwoexcludedexpressionswhicharepredominantlyusedliterallyinterms
ofpercentaccuracyusingthevariousmethodsareasfollows.Baseline:4.2,Unsupervised CONTEXT:6.5,
Unsupervised CFORM:16.2,Supervised:43.5.However,becausethereareonlytwosuchexpressions,
itisdifﬁculttodrawconclusionsfromtheseresults,andwedonotfurtherconsidertheseexpressions.
26 Accordingtoapairedt-test,on SKEWED-IDM,alltheobserveddifferencesarestatisticallysigniﬁcantat
p<.05.
27 Accordingtoapairedt-test,on ALL,thedifferencesbetweenthesupervisedmethodandthethreeother
methodsarestatisticallysigniﬁcantatp<.01;noneoftheotherdifferencesarestatisticallysigniﬁcant.
93
ComputationalLinguistics Volume35,Number1
idiomatic expressions in context. A human expert (e.g., a lexicographer) could ﬁrst
ﬁlter out expressions for which a literal interpretation is highly unlikely. For the rest
of the expressions, a simple unsupervised method such as CFORM—that relies only on
automaticallyextractedinformation—canbeusedwithreasonableaccuracy.
8. Related Work
8.1 Type-Based Recognition of Idiomsand Other MultiwordExpressions
Our work relates to previous studies on determining the compositionality (the inverse
of idiomaticity) of idioms and other multiword expressions (MWEs). Most previous
workonthecompositionalityofMWEseithertreatsthemascollocations(Smadja1993),
or examines the distributional similarity between the expression and its constituents
(Baldwin et al. 2003; Bannard, Baldwin, and Lascarides 2003; McCarthy, Keller, and
Carroll 2003). Others have identiﬁed MWEs by looking into speciﬁc linguistic cues,
suchasthelexicalﬁxednessofnon-compositionalMWEs(Lin1999;WermterandHahn
2005), or the lexical ﬂexibility of productive noun compounds (Lapata and Lascarides
2003). Venkatapathy and Joshi (2005) combine aspects of this work, by incorporating
lexical ﬁxedness, distributional similarity, and collocation-based measures into a
set of features which are used to rank verb+noun combinations according to their
compositionality. Our work differs from such studies in that it considers various kinds
of ﬁxedness as surface behaviors that are tightly related to the underlying semantic
idiosyncrasy (idiomaticity) of expressions. Accordingly, we propose novel methods
for measuring the degree of lexical, syntactic, and overall ﬁxedness of verb+noun
combinations,andusetheseasindirectwaysofmeasuringdegreeofidiomaticity.
Earlier research on the lexical encoding of idiom types mainly relied on the exis-
tence of human annotations, especially for detecting which syntactic variations (e.g.,
passivization)anidiomcanundergo(Odijk2004;Villavicencioetal.2004).Evert,Heid,
and Spranger (2004) and Ritz and Heid (2006) propose methods for automatically
determiningmorphosyntacticpreferencesofidiomaticexpressions.However,theytreat
individual morphosyntactic markers (e.g., the number of the noun in a verb+noun
combination) as independent features, and rely mainly on the relative frequency of
eachpossiblevalueforafeature(e.g.,pluralfornumber)asanindicatorofapreference
for that value. If the relative frequency of a particular value of a feature for a given
combination (or the lower bound of the conﬁdence interval, in the case of Evert, Heid,
and Spranger’s approach) is higher than a certain threshold, then the expression is
said to have a preference for that value. These studies recognize that morphosyntactic
preferences can be employed as clues to the identiﬁcation of idiomatic combinations;
however, none proposes a systematic approach for such a task. Moreover, only subjec-
tiveevaluationsoftheproposedmethodsarepresented.
Others have also drawn on the notion of syntactic ﬁxedness for the detection
of idioms and other MWEs. Widdows and Dorow (2005), for example, look into the
ﬁxedness of a highly constrained type of idiom, namely, those of the form “X conj X”
whereXisanounoranadjective,and conj isaconjunctionsuchasand,or,but.Smadja
(1993)alsonotestheimportanceofsyntacticﬁxednessinidentifyingstronglyassociated
multiword sequences, including collocations and idioms. Nonetheless, in both these
studies, the notion of syntactic ﬁxedness is limited to the relative position of words
withinthesequence.Suchageneralnotionofﬁxednessdoesnottakeintoaccountsome
of the important syntactic properties of idioms (e.g., the choice of the determiner), and
hencecannotdistinguishamongdifferentsubtypesofMWEswhichmaydifferonsuch
94
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
grounds. Our syntactic ﬁxedness measure looks into a set of linguistically informed
patternsassociatedwithacoherent,thoughlarge,classofidiomaticexpressions.Results
presented in this article show that the ﬁxedness measures can successfully separate
idioms from literal phrases. Corpus analysis of the measures proves that they can also
be used to distinguish idioms from other MWEs, such as light verb constructions and
collocations(FazlyandStevenson2007;FazlyandStevenson,toappear).Bannard(2007)
proposes an extension of our syntactic ﬁxedness measure—which ﬁrst appeared in
Fazly and Stevenson (2006)—where he uses different prior distributions for different
syntacticvariations.
Work on the identiﬁcation of MWE types has also looked at evidence from another
language. For example, Melamed (1997a) assumes that non-compositional compounds
(NCCs) are usually not translated word-for-word to another language. He thus pro-
poses to discover NCCs by maximizing the information-theoretic predictive value of
a translation model between two languages. The sample extracted NCCs reveal an
important drawback of the proposed method: It relies on a translation model only,
withouttakingintoaccountanypriorlinguisticknowledgeaboutpossibleNCCswithin
alanguage.Nonetheless,suchatechniqueiscapableofidentifyingmanyNCCsthatare
relevantforatranslationtask.VilladaMoir´onandTiedemann(2006)proposemeasures
for distinguishing idiomatic expressions from literal ones (in Dutch), by examining
their automatically generated translations into a second language, such as English or
Spanish. Their approach is based on the assumptions that idiomatic expressions tend
to have fewer predictable translations and fewer compositional meanings, compared
to the literal ones. The ﬁrst property is measured as the diversity in the translations
for the expression, estimated using an entropy-based measure proposed by Melamed
(1997b).Thenon-compositionalityofanexpressionismeasuredastheoverlapbetween
themeaningofanexpression(i.e.,itstranslations)andthoseofitscomponentwords.
General approaches (such as those explained in the previous paragraph) may be
more easily extended to different domains and languages. Our measures incorporate
language-speciﬁc information about idiomatic expressions, thus extra work may be
required to extend and apply them to other languages and other expressions. (Though
see Van de Cruys and Villada Moir´on [2007] for an extension of our measures to Dutch
idiomsoftheformverbplusprepositionalphrase.)Nonetheless,becauseourmeasures
capture deep linguistic information, they are also expected to acquire more detailed
knowledge—forexample,theycanbeusedforidentifyingotherclassesofMWEs(Fazly
andStevenson2007).
8.2Token-Based Identiﬁcation of Idiomsand Other MultiwordExpressions
A handful of studies have focused on identifying idiomatic and non-idiomatic usages
(tokens) of words or MWEs. Birke and Sarkar (2006) propose a minimally supervised
algorithm for distinguishing between literal and non-literal usages of verbs in context.
Their algorithm uses seed sets of literal and non-literal usages that are automatically
extractedfromonlineresourcessuchasWordNet.Thesimilaritybetweenthecontextof
atargettokenandthatofeachseedsetdeterminestheclassofthetoken.Theapproachis
generalinthatitusesaslightlymodiﬁedversionofanexistingwordsensedisambigua-
tion algorithm. This is both an advantage and a drawback: The algorithm can be easily
extendedtootherpartsofspeechandotherlanguages;however,suchageneralmethod
ignoresthespeciﬁcpropertiesofnon-literal(metaphoricaland/oridiomatic)language.
Similarly, the supervised token classiﬁcation method of Katz and Giesbrecht (2006)
relies primarily on the local context of a token, and fails to exploit speciﬁc linguistic
95
ComputationalLinguistics Volume35,Number1
properties of non-literal language. Our results suggest that such properties are often
moreinformativethanthelocalcontext,indeterminingtheclassofanMWEtoken.
ThesupervisedclassiﬁerofPatrickandFletcher(2005)distinguishesbetweencom-
positional and non-compositional usages of English verb-particle constructions. Their
classiﬁerincorporateslinguisticallymotivatedfeatures,suchasthedegreeofseparation
between the verb and particle. Here, we focus on a different class of English MWEs,
namely, the class of idiomatic verb+noun combinations. Moreover, by making a more
direct use of their syntactic behavior, we develop unsupervised token classiﬁcation
methods that perform well. The unsupervised token classiﬁer of Hashimoto, Sato, and
Utsuro (2006) uses manually encoded information about allowable and non-allowable
syntactic transformations of Japanese idioms, which are roughly equivalent to our
notions of canonical and non-canonical forms. The rule-based classiﬁer of Uchiyama,
Baldwin, and Ishizaki (2005) incorporates syntactic information about Japanese com-
poundverbs(JCVs),atypeofMWEcomposedoftwoverbs.Inbothcases,althoughthe
classiﬁers incorporate syntactic information about MWEs, their manual development
limitsthescalabilityoftheapproaches.
Uchiyama, Baldwin, and Ishizaki (2005) also propose a statistical token classiﬁca-
tion method for JCVs. This method is similar to ours, in that it also uses type-based
knowledge to determine the class of each token in context. However, their method is
supervised, whereas our methods are unsupervised. Moreover, Uchiyama, Baldwin,
andIshizakionlyevaluatetheirmethodsonasetofJCVsthataremostlymonosemous.
Our main focus here is on MWEs that are harder to disambiguate, that is, those that
have two clear idiomatic and literal meanings, and that are frequently used with either
meaning.
9. Conclusions
Thesigniﬁcanceoftheroleidiomsplayinlanguagehaslongbeenrecognized;however,
due to their peculiar behavior, they have been mostly overlooked by researchers in
computational linguistics. In this work, we focus on a broadly documented and cross-
linguistically frequent class of idiomatic MWEs: those that involve the combination
of a verb and a noun in its direct object position, which we refer to as verb+noun
idiomatic combinations or VNICs. Although a great deal of research has focused on
non-compositionalityofMWEs,lessattentionhasbeenpaidtootherpropertiesrelevant
totheirsemanticidiosyncrasy,suchaslexicalandsyntacticﬁxedness.Drawingonsuch
properties,wehavedevelopedtechniquesfortheautomaticrecognitionofVNICtypes,
aswellasmethodsfortheirtokenidentiﬁcationincontext.
We propose techniques for the automatic acquisition and encoding of knowledge
about the lexicosyntactic behavior of idiomatic combinations. More speciﬁcally, we
propose novel statistical measures that quantify the degree of lexical, syntactic, and
overall ﬁxedness of a verb+noun combination. We demonstrate that these measures
can be successfully applied to the task of automatically distinguishing idiomatic ex-
pressions (types) from non-idiomatic ones. Our results show that the syntactic and
overall ﬁxedness measures substantially outperform existing measures of collocation
extraction, even when they incorporate some syntactic information. We put forward
an unsupervised means for automatically discovering the set of syntactic variations
that are preferred by a VNIC type (its canonical forms) and that should be included
in its lexical representation. In addition, we show that the canonical form extraction
methodcaneffectivelybeusedinidentifyingidiomaticandliteralusages(tokens)ofan
expressionincontext.
96
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
We have annotated a total of 2,465 tokens for 51 VNIC types according to whether
they are a literal or idiomatic usage. We found that for 28 expressions (1,180 tokens),
approximately 40% of the usages were literal. For the remaining 23 expressions (1,285
tokens), almost all usages were idiomatic. These ﬁgures indicate that automatically
determining whether a particular instance of an expression is used idiomatically or lit-
erallyisofgreatimportanceforNLPapplications.Wehaveproposedtwounsupervised
methodsthatperformsuchatask.
Our proposed methods incorporate automatically acquired knowledge about the
overall syntactic behavior of a VNIC type, in order to do token classiﬁcation. More
speciﬁcally, our methods draw on the syntactic ﬁxedness of VNICs—a property which
has been largely ignored in previous studies of MWE tokens. Our results conﬁrm the
usefulness of this property as incorporated into our methods. On the 23 expressions
whose usages are predominantly idiomatic, because the baseline is very high none
of the methods outperform it. Nonetheless, as pointed out by our human annotators,
for many of these expressions it can be predicted beforehand that they are mainly
idiomatic and that a literal interpretation is impossible or highly implausible. On the
28 expressions with frequent literal usages, all our methods outperform the baseline of
alwayspredictingthemostdominantclass(idiomatic).Moreover,onthese,theaccuracy
of our best unsupervised method is not substantially lower than the accuracy of a
standardsupervisedapproach.
Appendix: Performance on theIndividual Expressions
This Appendixcontains the values of the four performance measures, Sens, PPV, Spec,
andNPV,forourtwounsupervisedmethods(i.e., CFORMand CONTEXT)aswellasfor
thesupervisedmethod, SUP,onindividualexpressionsin DT
I
high
and DT
I
low
.Expressions
(verb–nounpairs)ineachdatasetareorderedalphabetically.
Table12
Performanceof CFORMonindividualexpressionsin DT
I
high
and DT
I
low
.
DataSet verb–noun Sens(R
idm
) PPV(P
idm
)Spec(R
lit
)NPV(P
lit
)
blowtop 1.00 0.92 0.60 1.00
blowtrumpet 0.89 0.89 0.80 0.80
cutﬁgure 0.97 0.97 0.86 0.86
ﬁndfoot 0.98 0.92 0.20 0.50
getnod 0.96 1.00 1.00 0.75
getsack 1.00 0.96 0.71 1.00
haveword 0.56 0.96 0.78 0.17
hitroad 1.00 0.80 0.14 1.00
DT
I
high
hitroof 1.00 0.65 0.00 0.00
kickheel 1.00 0.81 0.12 1.00
losethread 0.94 0.94 0.50 0.50
makeface 0.74 0.95 0.67 0.22
makemark 0.85 1.00 1.00 0.54
pullplug 0.89 0.77 0.40 0.62
pullpunch 0.83 0.94 0.75 0.50
pullweight 1.00 0.93 0.67 1.00
takeheart 1.00 0.97 0.88 1.00
97
ComputationalLinguistics Volume35,Number1
Table12
(continued)
DataSet verb–noun Sens(R
idm
) PPV(P
idm
)Spec(R
lit
)NPV(P
lit
)
blowwhistle 0.93 0.44 0.37 0.90
getwind 0.85 0.73 0.75 0.86
hitwall 0.86 0.11 0.09 0.83
holdﬁre 1.00 0.37 0.25 1.00
losehead 0.76 0.62 0.41 0.58
DT
I
low
makehay 1.00 0.56 0.12 1.00
makehit 1.00 0.71 0.78 1.00
makepile 0.25 0.14 0.29 0.45
makescene 0.82 0.68 0.45 0.64
pullleg 0.64 0.23 0.40 0.80
seestar 0.80 0.10 0.38 0.95
Table13
Performanceof CONTEXTonindividualexpressionsin DT
I
high
and DT
I
low
.
DataSet verb–noun Sens(R
idm
) PPV(P
idm
)Spec(R
lit
)NPV(P
lit
)
blowtop 1.00 0.85 0.20 1.00
blowtrumpet 0.89 0.74 0.40 0.67
cutﬁgure 1.00 0.84 0.00 0.00
ﬁndfoot 1.00 0.90 0.00 0.00
getnod 1.00 0.88 0.00 0.00
getsack 1.00 0.86 0.00 0.00
haveword 0.70 0.95 0.67 0.20
hitroad 1.00 0.77 0.00 0.00
DT
I
high
hitroof 1.00 0.65 0.00 0.00
kickheel 0.97 0.78 0.00 0.00
losethread 1.00 0.90 0.00 0.00
makeface 0.85 0.88 0.00 0.00
makemark 1.00 0.91 0.46 1.00
pullplug 0.96 0.69 0.05 0.33
pullpunch 0.94 0.89 0.50 0.67
pullweight 1.00 0.82 0.00 0.00
takeheart 0.90 0.85 0.38 0.50
blowwhistle 0.89 0.36 0.18 0.75
getwind 0.85 0.65 0.62 0.83
hitwall 1.00 0.11 0.00 0.00
holdﬁre 1.00 0.30 0.00 0.00
losehead 0.90 0.56 0.12 0.50
DT
I
low
makehay 0.78 0.50 0.12 0.33
makehit 0.60 0.38 0.44 0.67
makepile 0.50 0.25 0.29 0.56
makescene 0.96 0.66 0.30 0.86
pullleg 0.82 0.22 0.20 0.80
seestar 1.00 0.12 0.32 1.00
98
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
Table14
Performanceof SUPonindividualexpressionsin DT
I
high
and DT
I
low
.
DataSet verb–noun Sens(R
idm
) PPV(P
idm
)Spec(R
lit
)NPV(P
lit
)
blowtop 1.00 0.85 0.20 1.00
blowtrumpet 0.95 0.72 0.30 0.75
cutﬁgure 1.00 0.84 0.00 0.00
ﬁndfoot 1.00 0.90 0.00 0.00
getnod 0.91 0.91 0.33 0.33
getsack 1.00 0.86 0.00 0.00
haveword 1.00 0.90 0.00 0.00
hitroad 1.00 0.80 0.14 1.00
DT
I
high
hitroof 0.82 0.64 0.17 0.33
kickheel 0.97 0.78 0.00 0.00
losethread 1.00 0.95 0.50 1.00
makeface 1.00 0.96 0.67 1.00
makemark 1.00 0.91 0.46 1.00
pullplug 0.98 0.90 0.75 0.94
pullpunch 1.00 0.90 0.50 1.00
pullweight 1.00 0.82 0.00 0.00
takeheart 0.93 0.83 0.25 0.50
blowwhistle 0.52 0.78 0.92 0.78
getwind 0.77 0.71 0.75 0.80
hitwall 0.00 0.00 1.00 0.89
holdﬁre 0.00 0.00 0.88 0.67
losehead 0.48 0.62 0.65 0.50
DT
I
low
makehay 0.89 0.80 0.75 0.86
makehit 0.40 1.00 1.00 0.75
makepile 0.38 0.75 0.94 0.76
makescene 0.89 0.69 0.45 0.75
pullleg 0.55 0.75 0.95 0.88
seestar 0.00 0.00 1.00 0.92
99
ComputationalLinguistics Volume35,Number1
Acknowledgments
Thisarticleisanextendedandupdated
combinationoftwopapersthatappeared,
respectively,intheproceedingsofEACL
2006andtheproceedingsoftheACL2007
WorkshoponABroaderPerspectiveon
MultiwordExpressions.Wewishtothank
theanonymousreviewersofthosepapers
fortheirhelpfulrecommendations.Wealso
thanktheanonymousreviewersofthis
articlefortheirinsightfulcommentswhich
webelievehavehelpedusimprovethe
qualityofthework.WearegratefultoEric
JoanisforprovidinguswiththeNP-head
extractionsoftware,andtoAfraAlishahi
andVivianTsangforproofreadingthe
manuscript.Ourworkisﬁnancially
supportedbytheNaturalSciencesand
EngineeringResearchCouncilofCanada,
theOntarioGraduateScholarshipprogram,
andtheUniversityofToronto.
References
Abeill´e,Anne.1995.TheﬂexibilityofFrench
idioms:Arepresentationwithlexicalized
TreeAdjoiningGrammar.InEveraert
etal.,editors,Idioms:Structuraland
PsychologicalPerspectives.LEA,Mahwah,
NJ,pages15–42.
Akimoto,Minoji.1999.Collocationsand
idiomsinLateModernEnglish.InL.J.
BrintonandM.Akimoto.Collocationaland
IdiomaticAspectsofCompositePredicatesin
theHistoryofEnglish.JohnBenjamins
PublishingCompany,Amsterdam,
pages207–238.
Baldwin,Timothy,ColinBannard,Takaaki
Tanaka,andDominicWiddows.2003.An
empiricalmodelofmultiwordexpression
decomposability.InProceedingsofthe
ACL-SIGLEXWorkshoponMultiword
Expressions:Analysis,Acquisitionand
Treatment,pages89–96,Sapporo.
Bannard,Colin.2007.Ameasureofsyntactic
ﬂexibilityforautomaticallyidentifying
multiwordexpressionsincorpora.In
ProceedingsoftheACL’07Workshopona
BroaderPerspectiveonMultiword
Expressions,pages1–8,Prague.
Bannard,Colin,TimothyBaldwin,and
AlexLascarides.2003.Astatistical
approachtothesemanticsof
verb-particles.InProceedingsofthe
ACL-SIGLEXWorkshoponMultiword
Expressions:Analysis,Acquisitionand
Treatment,pages65–72,Sapporo.
Birke,JuliaandAnoopSarkar.2006.A
clusteringapproachforthenearly
unsupervisedrecognitionofnonliteral
language.InProceedingsofthe11th
ConferenceoftheEuropeanChapterofthe
AssociationforComputationalLinguistics
(EACL’06),pages329–336,Trento.
Burnard,Lou.2000.ReferenceGuideforthe
BritishNationalCorpus(WorldEdition),
secondedition.Availableatwww.natcorp.
ox.ac.uk.
Cacciari,Cristina.1993.Theplaceofidioms
inaliteralandmetaphoricalworld.InC.
CacciariandP.Tabossi,Idioms:Processing,
Structure,andInterpretation.LEA,Mahwah,
NJ,pages27–53.
Church,Kenneth,WilliamGale,Patrick
Hanks,andDonaldHindle.1991.Using
statisticsinlexicalanalysis.InUriZernik,
editor,LexicalAcquisition:Exploiting
On-LineResourcestoBuildaLexicon.LEA,
Mahwah,NJ,pages115–164.
Claridge,Claudia.2000.Multi-wordVerbsin
EarlyModernEnglish:ACorpus-basedStudy.
EditionsRodopiB.V.,Amsterdam.
Clark,EveV.1978.Discoveringwhatwords
cando.PapersfromtheParasessiononthe
Lexicon,14:34–57.
Cohen,Jacob.1960.Acoefﬁcientof
agreementfornominalscales.Educational
andPsychologicalMeasurement,20:37–46.
Collins,Michael.1999.Head-DrivenStatistical
ModelsforNaturalLanguageParsing.Ph.D.
thesis,UniversityofPennsylvania.
Cook,Paul,AfsanehFazly,andSuzanne
Stevenson.2007.Pullingtheirweight:
Exploitingsyntacticformsforthe
automaticidentiﬁcationofidiomatic
expressionsincontext.InProceedingsofthe
ACL’07WorkshoponaBroaderPerspectiveon
MultiwordExpressions,pages41–48,
Prague.
Copestake,Ann,FabreLambeau,Aline
Villavicencio,FrancisBond,Timothy
Baldwin,IvanA.Sag,andDanFlickinger.
2002.Multiwordexpressions:Linguistic
precisionandreusability.InProceedingsof
the4thInternationalConferenceonLanguage
ResourcesandEvaluation(LREC’02),
pages1941–47,LasPalmas.
Cover,ThomasM.andJoyA.Thomas.1991.
ElementsofInformationTheory.JohnWiley
andSons,Inc.,NewYork.
Cowie,AnthonyP.,RonaldMackin,and
IsabelR.McCaig.1983.OxfordDictionaryof
CurrentIdiomaticEnglish,volume2.Oxford
UniversityPress.
Dagan,Ido,FernandoPereira,andLillian
Lee.1994.Similarity-basedestimationof
wordco-occurrenceprobabilities.In
Proceedingsofthe32ndAnuualMeetingofthe
100
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
AssociationforComputationalLinguistics
(ACL’94),pages272–278,LasCruces,NM.
d’Arcais,GiovanniB.Flores.1993.The
comprehensionandsemantic
interpretationofidioms.InC.Cacciariand
P.Tabossi,Idioms:Processing,Structure,and
Interpretation.LEA,Mahwah,NJ,
pages79–98.
Desbiens,MargueriteChampagneandMara
Simon.2003.D´eterminantsetlocutions
verbales.Manuscript.Availableat
www.er.uqam.ca/nobel/scilang/cesla02/
mara margue.pdf.
Evert,Stefan,UlrichHeid,andKristina
Spranger.2004.Identifying
morphosyntacticpreferencesin
collocations.InProceedingsofthe4th
InternationalConferenceonLanguage
ResourcesandEvaluation(LREC’04),
pages907–910,Lisbon.
Evert,StefanandBrigitteKrenn.2001.
Methodsforthequalitativeevaluationof
lexicalassociationmeasures.InProceedings
ofthe39thAnnualMeetingoftheAssociation
forComputationalLinguistics(ACL’01),
pages188–195,Toulouse.
Fazly,AfsanehandSuzanneStevenson.2006.
Automaticallyconstructingalexiconof
verbphraseidiomaticcombinations.In
Proceedingsofthe11thConferenceofthe
EuropeanChapteroftheAssociationfor
ComputationalLinguistics(EACL’06),
pages337–344,Trento.
Fazly,AfsanehandSuzanneStevenson.2007.
Distinguishingsubtypesofmultiword
expressionsusinglinguistically-motivated
statisticalmeasures.InProceedingsofthe
ACL’07WorkshoponaBroaderPerspective
onMultiwordExpressions,pages9–16,
Prague.
Fazly,AfsanehandSuzanneStevenson.A
distributionalaccountofthesemanticsof
multiwordexpressions.Toappearinthe
ItalianJournalofLinguistics.
Fellbaum,Christiane.1993.Thedeterminer
inEnglishidioms.InC.Cacciariand
P.Tabossi,Idioms:Processing,Structure,
andInterpretation.LEA,Mahwah,NJ,
pages271–295.
Fellbaum,Christiane,editor.1998.WordNet,
AnElectronicLexicalDatabase.MITPress,
Cambridge,MA.
Fellbaum,Christiane.2002.VPidiomsinthe
lexicon:Topicsforresearchusingavery
largecorpus.InProceedingsofthe
KONVENS2002Conference,pages7–11,
Saarbruecken,Germany.
Fellbaum,Christiane.2007.Theontological
lonelinessofidioms.InAndreaSchalley
andDietmarZaefferer,editors,
Ontolinguistics.MoutondeGruyter,Berlin,
pages419–434.
Firth,JohnR.1957.Asynopsisoflinguistic
theory1930–1955.InStudiesinLinguistic
Analysis(specialvolumeofthePhilological
Society).ThePhilologicalSociety,Oxford,
pages1–32.
Fraser,Bruce.1970.Idiomswithina
transformationalgrammar.Foundationsof
Language,6:22–42.
Gentner,DedreandIleneM.France.2004.
Theverbmutabilityeffect:Studiesofthe
combinatorialsemanticsofnounsand
verbs.InStevenL.Small,GarrisonW.
Cottrell,andMichaelK.Tanenhaus,
editors,LexicalAmbiguityResolution:
PerspectivesfromPsycholinguistics,
Neuropsychology,andArtiﬁcialIntelligence.
Kaufmann,SanMateo,CA,pages343–382.
Gibbs,RaymondW.Jr.1993.Whyidiomsare
notdeadmetaphors.InC.Cacciariand
P.Tabossi,Idioms:Processing,Structure,and
Interpretation.LEA,Mahwah,NJ,
pages57–77.
Gibbs,RaymondW.Jr.1995.Idiomaticity
andhumancognition.InEveraertetal.,
editors,Idioms:StructuralandPsychological
Perspectives.LEA,Mahwah,NJ,
pages97–116.
Gibbs,RaymondW.Jr.andNandiniP.
Nayak.1989.Psychololinguisticstudieson
thesyntacticbehaviorofidioms.Cognitive
Psychology,21:100–138.
Gibbs,RaymondW.Jr.,NandiniP.Nayak,
J.Bolton,andM.Keppel.1989.Speaker’s
assumptionsaboutthelexicalﬂexibility
ofidioms.MemoryandCognition,
17:58–68.
Glucksberg,Sam.1993.Idiommeaningsand
allusionalcontent.InC.CacciariandP.
Tabossi,Idioms:Processing,Structure,and
Interpretation.LEA,Mahwah,NJ,
pages3–26.
Goldberg,AdeleE.1995.Constructions:A
ConstructionGrammarApproachto
ArgumentStructure.TheUniversityof
ChicagoPress.
Grant,LynnE.2005.Frequencyof‘core
idioms’intheBritishNationalCorpus
(BNC).InternationalJournalofCorpus
Linguistics,10(4):429–451.
Hashimoto,Chikara,SatoshiSato,and
TakehitoUtsuro.2006.Japaneseidiom
recognition:Drawingalinebetween
literalandidiomaticmeanings.In
Proceedingsofthe17thInternational
ConferenceonComputationalLinguistics
andthe36thAnnualMeetingofthe
101
ComputationalLinguistics Volume35,Number1
AssociationforComputationalLinguistics
(COLING-ACL’06),pages353–360,Sydney.
Inkpen,Diana.2003.BuildingaLexical
Knowledge-BaseofNear-SynonymDifferences.
Ph.D.thesis,UniversityofToronto.
Jackendoff,Ray.1997.TheArchitectureofthe
LanguageFaculty.MITPress,Cambridge,
MA.
Katz,GrahamandEugenieGiesbrecht.2006.
Automaticidentiﬁcationof
non-compositionalmulti-word
expressionsusingLatentSemantic
Analysis.InProceedingsoftheACL’06
WorkshoponMultiwordExpressions:
IdentifyingandExploitingUnderlying
Properties,pages12–19,Sydney.
Katz,JerroldJ.1973.Compositionality,
idiomaticity,andlexicalsubstitution.In
S.AndersonandP.Kiparsky,editors,A
FestschriftforMorrisHalle.Holt,Rinehart
andWinston,NewYork,pages357–376.
Kearns,Kate.2002.LightverbsinEnglish.
Manuscript.Availableatwww.ling.
canterbury.ac.nz/people/kearns.html.
Kirkpatrick,E.M.andC.M.Schwarz,
editors.1982.ChambersIdioms.W&R
ChambersLtd,Edinburgh.
Krenn,BrigitteandStefanEvert.2001.Can
wedobetterthanfrequency?Acasestudy
onextractingPP-verbcollocations.In
ProceedingsoftheACL’01Workshopon
Collocations,pages 39–46,Toulouse.
Kyt¨o,Merja.1999.Collocationaland
idiomaticaspectsofverbsinEarlyModern
English.InL.J.BrintonandM.Akimoto.
CollocationalandIdiomaticAspectsof
CompositePredicatesintheHistoryof
English.JohnBenjaminsPublishing
Company,Amsterdam,pages167–206.
Lapata,MirellaandAlexLascarides.2003.
Detectingnovelcompounds:Theroleof
distributionalevidence.InProceedingsof
the11thConferenceoftheEuropeanChapterof
theAssociationforComputationalLinguistics
(EACL’03),pages235–242,Budapest.
Lin,Dekang.1998.Automaticretrievaland
clusteringofsimilarwords.InProceedings
ofthe17thInternationalConferenceon
ComputationalLinguisticsandthe36th
AnnualMeetingoftheAssociationfor
ComputationalLinguistics
(COLING-ACL’98),pages768–774,
Montreal.
Lin,Dekang.1999.Automaticidentiﬁcation
ofnon-compositionalphrases.In
Proceedingsofthe37thAnnualMeetingofthe
AssociationforComputationalLinguistics
(ACL’99),pages317–324,CollegePark,
Maryland.
Manning,ChristopherD.andHinrich
Sch¨utze.1999.FoundationsofStatistical
NaturalLanguageProcessing.TheMIT
Press,Cambridge,MA.
McCarthy,Diana,BillKeller,andJohn
Carroll.2003.Detectingacontinuum
ofcompositionalityinphrasalverbs.
InProceedingsoftheACL-SIGLEX
WorkshoponMultiwordExpressions:
Analysis,AcquisitionandTreatment,
pages73–80,Sapporo.
Melamed,I.Dan.1997a.Automatic
discoveryofnon-compositional
compoundsinparalleldata.InProceedings
ofthe2ndConferenceonEmpiricalMethodsin
NaturalLanguageProcessing(EMNLP’97),
pages97–108,Providence,RI.
Melamed,I.Dan.1997b.Measuringsemantic
entropy.InProceedingsoftheACL-SIGLEX
WorkshoponTaggingTextwithLexical
Semantics:Why,WhatandHow,
pages41–46,Washington,DC.
Mohammad,SaifandGraemeHirst.
Distributionalmeasuresasproxiesfor
semanticrelatedness.Submitted.
Moon,Rosamund.1998.FixedExpressionsand
IdiomsinEnglish:ACorpus-BasedApproach.
OxfordUniversityPress.
Newman,JohnandSallyRice.2004.Patterns
ofusageforEnglishSIT,STAND,andLIE:
Acognitivelyinspiredexplorationin
corpuslinguistics.CognitiveLinguistics,
15(3):351–396.
Nicolas,Tim.1995.Semanticsofidiom
modiﬁcation.InEveraertetal.,editors,
Idioms:StructuralandPsychological
Perspectives.LEA,Mahwah,NJ,
pages233–252.
Nunberg,Geoffrey,IvanA.Sag,andThomas
Wasow.1994.Idioms.Language,
70(3):491–538.
Odijk,Jan.2004.Aproposedstandardforthe
lexicalrepresentationsofidioms.In
ProceedingsofEuralex’04,pages153–164,
Lorient.
Ogden,CharlesKay.1968.BasicEnglish,
InternationalSecondLanguage.Harcourt,
Brace,andWorld,NewYork.
Patrick,JonandJeremyFletcher.2005.
Classifyingverb-particleconstructions
byverbarguments.InProceedingsof
theSecondACL-SIGSEMWorkshoponthe
LinguisticDimensionsofPrepositionsand
theirUseinComputationalLinguistics
FormalismsandApplications,pages200–209,
Colcheter.
Pauwels,Paul.2000.Put,Set,LayandPlace:A
CognitiveLinguisticApproachtoVerbal
Meaning.LINCOMEUROPA,Munich.
102
Fazly,Cook,andStevenson UnsupervisedIdiomIdentiﬁcation
R2004.NotesonR:AProgramming
EnvironmentforDataAnalysisandGraphics.
Availableatwww.r-project.org.
Resnik,Philip.1999.Semanticsimilarityina
taxonomy:Aninformation-basedmeasure
anditsapplicationtoproblemsof
ambiguityinnaturallanguage.Journalof
ArtiﬁcialIntelligenceResearch(JAIR),
(11):95–130.
Riehemann,Susanne.2001.AConstructional
ApproachtoIdiomsandWordFormation.
Ph.D.thesis,StanfordUniversity.
Ritz,JuliaandUlrichHeid.2006.Extraction
toolsforcollocationsandtheir
morphosyntacticspeciﬁcities.In
Proceedingsofthe5thInternational
ConferenceonLanguageResourcesand
Evaluation(LREC’06),pages1925–30,
Genoa.
Rohde,DouglasL.T.2004.TGrep2User
Manual.Availableathttp://tedlab.mit.
edu/∼dr/Tgrep2.
Sag,IvanA.,TimothyBaldwin,Francis
Bond,AnnCopestake,andDanFlickinger.
2002.Multiwordexpressions:Apaininthe
neckforNLP.InProceedingsofthe3rd
InternationalConferenceonIntelligentText
ProcessingandComputationalLinguistics
(CICLing’02),pages1–15,MexicoCity.
Schenk,Andr´e.1995.Thesyntacticbehavior
ofidioms.InEveraertetal.,editors,Idioms:
StructuralandPsychologicalPerspectives.
LEA,Mahwah,NJ,chapter10,
pages253–271.
Seaton,MaggieandAlisonMacaulay,
editors.2002.CollinsCOBUILDIdioms
Dictionary.HarperCollinsPublishers,
secondedition,NewYork.
Smadja,Frank.1993.Retrievingcollocations
fromtext:Xtract.ComputationalLinguistics,
19(1):143–177.
Tanabe,Harumi.1999.Compositepredicates
andphrasalverbsinThePastonLetters.In
L.J.BrintonandM.Akimoto.Collocational
andIdiomaticAspectsofCompositePredicates
intheHistoryofEnglish.JohnBenjamins
PublishingCompany,Amsterdam,
pages97–132.
Uchiyama,Kiyoko,TimothyBaldwin,and
ShunIshizaki.2005.Disambiguating
Japanesecompoundverbs.Computer
SpeechandLanguage,19:497–512.
VandeCruys,TimandBego˜na
VilladaMoir´on.2007.Semantics-based
multiwordexpressionextraction.In
ProceedingsoftheACL’07Workshopona
BroaderPerspectiveonMultiword
Expressions,pages25–32,Prague.
Venkatapathy,SriramandAravidJoshi.2005.
Measuringtherelativecompositionalityof
verb-noun(V-N)collocationsby
integratingfeatures.InProceedingsofJoint
ConferenceonHumanLanguageTechnology
andEmpiricalMethodsinNaturalLanguage
Processing(HLT-EMNLP’05),
pages 899–906,Vancouver.
VilladaMoir´on,Bego˜naandJ¨orgTiedemann.
2006.Identifyingidiomaticexpressions
usingautomaticword-alignment.In
ProceedingsoftheEACL’06Workshopon
MultiwordExpressionsinaMultilingual
Context,pages33–40,Trento.
Villavicencio,Aline,AnnCopestake,
BenjaminWaldron,andFabreLambeau.
2004.Lexicalencodingofmultiword
expressions.InProceedingsofthe2ndACL
WorkshoponMultiwordExpressions:
IntegratingProcessing,pages80–87,
Barcelona.
Wermter,JoachimandUdoHahn.2005.
Paradigmaticmodiﬁabilitystatisticsfor
theextractionofcomplexmulti-word
terms.InProceedingsofJointConferenceon
HumanLanguageTechnologyandEmpirical
MethodsinNaturalLanguageProcessing
(HLT-EMNLP’05),pages 843–850,
Vancouver.
Widdows,DominicandBeateDorow.2005.
Automaticextractionofidiomsusing
graphanalysisandasymmetric
lexicosyntacticpatterns.InProceedingsof
ACL’05WorkshoponDeepLexical
Acquisition,pages48–56,AnnArbor,MI.
Wilcoxon,Frank.1945.Individual
comparisonsbyrankingmethods.
BiometricsBulletin,1(6):80–83.
103


