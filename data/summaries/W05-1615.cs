[Su et al., 1992] Keh-Yih Su, Ming-Wen Wu and Jing-Shin Chang, 1992, A new quantitative quality measure for machine translation systems.
C92-2067
SUMTIME-MOUSAM uses rules for this that are derived from corpus analysis and other knowledge acquisition activities [Reiter et al, 2003; Sripada et al, 2003].
W03-0611
[Lester and Porter, 1997] James Lester and Bruce Porter.
J97-1004
We have discussed lexical replacement errors in detail elsewhere [Reiter and Sripada, 2002].
J02-4007
Sripada, Ehud Reiter, and Ian Davy, 2003.
W03-0611
Cost of add and replace operations is set to 5 and cost of delete is set to 1 as used in Su et al [1992].
C92-2067
However, many lexical replacement errors simply reflected the lexical preferences of individual forecasters [Reiter and Sripada, 2002].
J02-4007
As described in [Reiter et al, 2003], some forecasters elide the last time phrase in simple sentences such as this one, and some do not.
W03-0611
• Background information: includes date, location, and forecaster We do not currently use the NWP data (other than for reconstructing SUMTIME-MOUSAM texts), although we hope in the future to include it in our analyses, in a manner roughly analogous to Reiter and Sripada [2003].
W03-0611
The only previous use of post-edit evaluation in NLG that we are aware of is Mitkov and An Ha [2003], but their evaluation is relatively small, and they give little information about it.
W03-0203
The cost-effectiveness of post-edit evaluation is less clear if the evaluators must organize and pay for the post-editing, as Mitkov and An Ha [2003] did.
W03-0203
[Reiter et al., 2003] Ehud Reiter, Somayajulu G.
W03-0611
[Reiter and Sripada, 2003] Ehud Reiter and Somayajulu G.
W03-0611
[Mitkov and An Ha, 2003] Ruslan Mitkov and Le An Ha, 2003.
W03-0203
2.1 Evaluating NLG Systems Common evaluation techniques for NLG systems [Mellish and Dale, 1998] include: • Showing generated texts to users, and measuring how effective they are at achieving their goal, compared to some control text (for example, [Young, 1999]) • Asking experts to rate computer-generated texts in various ways, and comparing this to their rating of manually authored texts (for example, [Lester and Porter, 1997]) • Automatically comparing generated texts to a corpus of human authored texts (for example, [Bangalore et al, 2000]).
J97-1004 W00-1401
[Reiter and Sripada, 2002] Ehud Reiter and Somayajulu G.
J02-4007
In [Reiter et al, 2003] we explained that we found that analysis of human-written corpora was more useful if it was combined with directly working with domain experts; and essentially this (perhaps not surprisingly) is our conclusion about post-edit data as well.
W03-0611
Our analysis of manually written forecasts [Reiter and Sripada, 2002] had highlighted a number of “noise” elements that made it more difficult to extract information from such corpora.
J02-4007
References [Bangalore et al., 2000] Srinivas Bangalore, Owen Rambow, and Steve Whittaker.
W00-1401
