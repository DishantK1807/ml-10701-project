<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>ACE</author>
</authors>
<title>The NIST ACE evaluation website</title>
<date>2005</date>
<note>http://www.nist.gov/speech/tests/ace</note>
<contexts>
<context>e clusters to key (gold standard) clusters, we selected Luo’s (2005) CEAF score (short for Constrained Entity-Alignment F-Measure). The CEAF score is similar to a simplified version of the ACE score (ACE, 2005). It works by computing an optimal mapping of response clusters to key clusters, summing the scores for each pair of mapped clusters and dividing by the maximum score (i.e. the score for mapping the </context>
</contexts>
<marker>ACE, 2005</marker>
<rawString>ACE. 2005. The NIST ACE evaluation website. http://www.nist.gov/speech/tests/ace/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains. In</title>
<date>1998</date>
<booktitle>In Proceedings of MUC7</booktitle>
<contexts>
<context>in general, it is not strict enough for for responses that link too many clusters together. 5.1.3. B-Cubed B-Cubed (B3) is another commonly used noun phrase coreference resolution evaluation measure (Bagga and Baldwin, 1998). It is computed as the precision and recall for each item (in our case, each opinion) and is then averaged for each document. The precision (recall) for an item i is computed as the proportion of it</context>
<context>lusters. This means that there are very few “nonlinks” to be recognized. The MUC score has some welldocumentedproblemsinnotbeingstrictenoughforpunishing clusterings that fail to identify “non-links” (Bagga and Baldwin, 1998). As a result, when comparing two opinion topic clusterings, it is very difficult to score better than the simple baseline of putting all opinions in the same cluster, which achieves perfect recall a</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Algorithms for scoring coreference chains. In In Proceedings of MUC7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bethard</author>
<author>H Yu</author>
<author>A Thornton</author>
<author>V Hativassiloglou</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic extraction of opinion propositions and their holders</title>
<date>2004</date>
<booktitle>In 2004 AAAI Spring Symposium on Exploring Attitude and Affect in Text</booktitle>
<contexts>
<context> research in the area has been further facilitated by the creation of several corpora that have been manually annotated with fine-grained expressions of opinions, including their source and polarity (Bethard et al., 2004; Wiebe et al., 2005). Notably missing from the corpora, however, are annotations for the topics of opinions. Despite the desire and motivation for creating such corpora, topic annotation has proven a</context>
</contexts>
<marker>Bethard, Yu, Thornton, Hativassiloglou, Jurafsky, 2004</marker>
<rawString>S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou, and D. Jurafsky. 2004. Automatic extraction of opinion propositions and their holders. In 2004 AAAI Spring Symposium on Exploring Attitude and Affect in Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Choi</author>
<author>C Cardie</author>
<author>E Riloff</author>
<author>S Patwardhan</author>
</authors>
<title>Identifying sources of opinions with conditional random fields and extraction patterns</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<marker>Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005. Identifying sources of opinions with conditional random fields and extraction patterns. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Coglianese</author>
</authors>
<title>E-rulemaking: Information technology and regulatory policy: New directions in digital government research</title>
<date>2004</date>
<tech>Technical report</tech>
<institution>Harvard University</institution>
<marker>Coglianese, 2004</marker>
<rawString>C. Coglianese. 2004. E-rulemaking: Information technology and regulatory policy: New directions in digital government research. Technical report, Harvard University, J. F. Kennedy School of Government.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Das</author>
<author>M Chen</author>
</authors>
<title>Yahoo for amazon: Extracting market sentiment from stock message boards</title>
<date>2001</date>
<booktitle>In Proceedings of APFAAC</booktitle>
<marker>Das, Chen, 2001</marker>
<rawString>S. Das and M. Chen. 2001. Yahoo for amazon: Extracting market sentiment from stock message boards. In Proceedings of APFAAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Dave</author>
<author>S Lawrence</author>
<author>D Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semanticclassification of product reviews</title>
<date>2003</date>
<booktitle>In Proceedings of IWWWC</booktitle>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>K. Dave, S. Lawrence, and D. Pennock. 2003. Mining the peanut gallery: Opinion extraction and semanticclassification of product reviews. In Proceedings of IWWWC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining opinion features in customer reviews</title>
<date>2004</date>
<booktitle>In AAAI</booktitle>
<pages>755--760</pages>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining opinion features in customer reviews. In AAAI, pages 755–760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>E Hovy</author>
</authors>
<title>Extracting opinions, opinion holders, and topicsexpressed inonlinenews mediatext</title>
<date>2006</date>
<booktitle>In Proceedings of ACL/COLING Workshop on Sentiment and Subjectivity in Text</booktitle>
<marker>Kim, Hovy, 2006</marker>
<rawString>S. Kim and E. Hovy. 2006. Extracting opinions, opinion holders, and topicsexpressed inonlinenews mediatext. In Proceedings of ACL/COLING Workshop on Sentiment and Subjectivity in Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kobayashi</author>
<author>K Inui</author>
<author>Y Matsumoto</author>
<author>K Tateishi</author>
<author>T Fukushima</author>
</authors>
<title>Collecting evaluative expressions for opinion extraction</title>
<date>2004</date>
<booktitle>In Proceedings of IJCNLP</booktitle>
<marker>Kobayashi, Inui, Matsumoto, Tateishi, Fukushima, 2004</marker>
<rawString>N. Kobayashi, K. Inui, Y. Matsumoto, K. Tateishi, and T. Fukushima. 2004. Collecting evaluative expressions for opinion extraction. In Proceedings of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology. Sage Publications</title>
<date>1980</date>
<location>Beverly Hills, CA</location>
<contexts>
<context>noun phrase coreference resolution. We present these metrics in the next subsections. 5.1.1. Krippendorff’s α As one evaluation measure, we use Passonneau’s (2004) generalization of Krippendorff’s α (Krippendorff, 1980) — a standard metric employed for inter-annotator reliability studies. Krippendorff’s α is a theoretically-founded measure with a nice probabilistic interpretation. It is designed to measure the reli</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>K. Krippendorff. 1980. Content Analysis: An Introduction to Its Methodology. Sage Publications, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
</authors>
<title>Oncoreferenceresolutionperformancemetrics</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<marker>Luo, 2005</marker>
<rawString>X.Luo. 2005. Oncoreferenceresolutionperformancemetrics. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts</title>
<date>2004</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Pang, Lee, 2004</marker>
<rawString>B. Pang and L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Passonneau</author>
</authors>
<title>Computing reliability for coreference annotation</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context>r inter-annotator agreement studies is that it is hardtotranslateabsolutescorestoqualityofagreement. Of the four metrics, only Krippendorff’s α attempts to incorporate a probabilistic interpretation (Passonneau, 2004). It is generally agreed that an α score above 0.66 indicates reliable agreement. Our inter-annotator agreement exhibits a score under that threshold when computed over all opinions (0.54) and a scor</context>
</contexts>
<marker>Passonneau, 2004</marker>
<rawString>R. Passonneau. 2004. Computing reliability for coreference annotation. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Popescu</author>
<author>O Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP</booktitle>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>A. Popescu and O. Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quirk</author>
<author>S Greenbaum</author>
<author>G Leech</author>
<author>J Svartvik</author>
</authors>
<title>A comprehensive grammar of the English language</title>
<date>1985</date>
<publisher>Longman</publisher>
<location>New York</location>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985. A comprehensive grammar of the English language. Longman, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions</title>
<date>2003</date>
<booktitle>In Proceesings of EMNLP</booktitle>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>E. Riloff and J. Wiebe. 2003. Learning extraction patterns for subjective expressions. In Proceesings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stoyanov</author>
<author>C Cardie</author>
<author>J Wiebe</author>
</authors>
<title>Multi-Perspective question answering using the OpQA corpus</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<marker>Stoyanov, Cardie, Wiebe, 2005</marker>
<rawString>V. Stoyanov, C. Cardie, and J. Wiebe. 2005. Multi-Perspective question answering using the OpQA corpus. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Turney, 2002</marker>
<rawString>P. Turney. 2002a. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Turney, 2002</marker>
<rawString>P. Turney. 2002b. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>J Burger</author>
<author>J Aberdeen</author>
<author>D Connolly</author>
<author>L Hirschman</author>
</authors>
<title>A model-theoretic coreference scoring scheme</title>
<date>1995</date>
<booktitle>In Proceedings of MUC6</booktitle>
<contexts>
<context>ormulation the measure does not carry the original probabilistic interpretation. 5.1.2. MUC score The MUC score is a model-theoretic coreference scoring metric for noun phrase coreference resolution (Vilain et al., 1995). The MUC recall score is computed as the ratio of correct non-repetitive links in the response (i.e. the system’s output) as compared to the minimum number of nonrepetitive links required to constru</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and L. Hirschman. 1995. A model-theoretic coreference scoring scheme. In Proceedings of MUC6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
<author>L Buckland</author>
</authors>
<title>Overview of the TREC 2003 Question Answering Track</title>
<date>2003</date>
<booktitle>In Proceedings of TREC 12</booktitle>
<marker>Voorhees, Buckland, 2003</marker>
<rawString>E. Voorhees and L. Buckland. 2003. Overview of the TREC 2003 Question Answering Track. In Proceedings of TREC 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>E Riloff</author>
</authors>
<title>Creating subjective and objective sentence classifiers from unannotated texts</title>
<date>2005</date>
<booktitle>In Proceedings of CICLing</booktitle>
<marker>Wiebe, Riloff, 2005</marker>
<rawString>J. Wiebe and E. Riloff. 2005. Creating subjective and objective sentence classifiers from unannotated texts. In Proceedings of CICLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson J Wiebe</author>
<author>andC Cardie</author>
</authors>
<title>Annotatingexpressions of opinions and emotions in language</title>
<date>2005</date>
<journal>Language Resources and Evaluation</journal>
<volume>1</volume>
<marker>Wiebe, Cardie, 2005</marker>
<rawString>J.Wiebe, T.Wilson, andC.Cardie. 2005. Annotatingexpressions of opinions and emotions in language. Language Resources and Evaluation, 1(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
</authors>
<date>2005</date>
<note>Personal communications</note>
<contexts>
<context>missing from the corpora, however, are annotations for the topics of opinions. Despite the desire and motivation for creating such corpora, topic annotation has proven a difficult task (Wilson, 2005; Wiebe, 2005). Nonetheless, topics remain an important component of an opinion, and topic extraction remains a critical step for sentiment analysis systems. In this paper, we describe a methodology for performing</context>
<context>hould be noted that Wiebe et al. (2005) initially intended to include topic annotations in the MPQA corpus, but postponed the task, discovering that topic annotation was very difficult (Wilson, 2005; Wiebe, 2005). Currently, Wiebe et al. are adding target spans to their annotations. While useful, target spans are insufficient for many applications that use fine-grained opinion information: they neither conta</context>
</contexts>
<marker>Wiebe, 2005</marker>
<rawString>Janyce Wiebe. 2005. Personal communications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>R Hwa</author>
</authors>
<title>Just how mad are you? Finding strong and weak opinion clauses</title>
<date>2004</date>
<booktitle>In Proceedings of AAAI</booktitle>
<marker>Wilson, Wiebe, Hwa, 2004</marker>
<rawString>T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad are you? Finding strong and weak opinion clauses. In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
</authors>
<date>2005</date>
<note>Personal communications</note>
<contexts>
<context>005). Notably missing from the corpora, however, are annotations for the topics of opinions. Despite the desire and motivation for creating such corpora, topic annotation has proven a difficult task (Wilson, 2005; Wiebe, 2005). Nonetheless, topics remain an important component of an opinion, and topic extraction remains a critical step for sentiment analysis systems. In this paper, we describe a methodology f</context>
<context>A corpus. It should be noted that Wiebe et al. (2005) initially intended to include topic annotations in the MPQA corpus, but postponed the task, discovering that topic annotation was very difficult (Wilson, 2005; Wiebe, 2005). Currently, Wiebe et al. are adding target spans to their annotations. While useful, target spans are insufficient for many applications that use fine-grained opinion information: they </context>
</contexts>
<marker>Wilson, 2005</marker>
<rawString>Theresa Wilson. 2005. Personal communications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yi</author>
<author>T Nasukawa</author>
<author>R Bunescu</author>
<author>W Niblack</author>
</authors>
<title>Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques</title>
<date>2003</date>
<booktitle>In Proceedings of ICDM</booktitle>
<marker>Yi, Nasukawa, Bunescu, Niblack, 2003</marker>
<rawString>J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003. Sentiment analyzer: Extracting sentiments about a given topic using natural language processing techniques. In Proceedings of ICDM.</rawString>
</citation>
</citationList>
</algorithm>

