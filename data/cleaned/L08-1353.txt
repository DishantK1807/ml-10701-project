<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<title>Resolution for Natural Language Processing</title>
<date>2002</date>
<location>Alicante, Spain</location>
<marker>2002</marker>
<rawString>Resolution for Natural Language Processing. Alicante, Spain, June 3-4, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
<author>Alexis Mitchell</author>
<author>Mark Przybocki</author>
<author>Lance Ramshaw</author>
<author>Stephanie Strassel</author>
<author>Ralph Weischedel</author>
</authors>
<title>The Automatic Content Extraction (ACE) Program: Tasks, data, &amp; evaluation</title>
<date>2004</date>
<booktitle>In LREC 2004 – Proceedings of the 4th International Conference on Language Resources and Evaluation</booktitle>
<volume>3</volume>
<pages>837--840</pages>
<location>Lisbon, Portugal</location>
<contexts>
<context> relation extraction. The annotation of semantic relations between entities is already in the focus of many annotation initiatives. This holds for newspaper corpora such as MUC7 (MUC-7, 1998) or ACE (Doddington et al., 2004), but also for life science corpora such as BIOINFER (Pyysalo et al., 2007) or BIOCREATIVE II’s protein-protein interaction corpus (Hirschman et al., 2007). We aim to provide a corpus suited for the </context>
<context>ions (coreferences, basically) for information extraction (IE) has been acknowledged and embedded as a special task in recent IE challenges such as MUC-6 (MUC-6, 1995), MUC-7 (MUC7, 1998) or ACE2005 (Doddington et al., 2004)). Also for an information extraction system for court opinions, AlKofahi et al. (1999), for instance, show that the resolution of referential expressions has a positive impact on the overall system </context>
</contexts>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The Automatic Content Extraction (ACE) Program: Tasks, data, &amp; evaluation. In LREC 2004 – Proceedings of the 4th International Conference on Language Resources and Evaluation. Vol. 3, pages 837–840. Lisbon, Portugal, 26-28 May 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udo Hahn</author>
<author>Joachim Wermter</author>
</authors>
<title>High-performance tagging on medical texts</title>
<date>2004</date>
<booktitle>In COLING Geneva 2004 – Proceedings of the 20th International Conference on Computational Linguistics</booktitle>
<volume>2</volume>
<pages>973--979</pages>
<contexts>
<context> etc. trained on newspaper annotations. In effect, their performance degraded significantly when ported to domains such as biology and medicine, or to genres such as scientific articles or abstracts (Hahn and Wermter, 2004). In the life sciences, this observation gave rise to the development of domain-specific corpora — GENIA being the first, most prominent example of these efforts (Kim et al., 2003) (approximately 500</context>
</contexts>
<marker>Hahn, Wermter, 2004</marker>
<rawString>Udo Hahn and Joachim Wermter. 2004. High-performance tagging on medical texts. In COLING Geneva 2004 – Proceedings of the 20th International Conference on Computational Linguistics, volume 2, pages 973–979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Switzerland Geneva</author>
</authors>
<date>2004</date>
<marker>Geneva, 2004</marker>
<rawString>Geneva, Switzerland, August 23-27, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Martin Krallinger</author>
<author>Alfonso Valencia</author>
<author>editors</author>
</authors>
<date>2007</date>
<booktitle>Proceedings of the Second BioCreative Challenge Evaluation Workshop. Madrid: CNIO Centro Nacional de Investigaciones Oncol´ogicas</booktitle>
<contexts>
<context> corpora such as MUC7 (MUC-7, 1998) or ACE (Doddington et al., 2004), but also for life science corpora such as BIOINFER (Pyysalo et al., 2007) or BIOCREATIVE II’s protein-protein interaction corpus (Hirschman et al., 2007). We aim to provide a corpus suited for the extraction of semantic relations between entities in the biomedical domain, especially for the domain of gene regulation. While literal relation mentions (</context>
</contexts>
<marker>Hirschman, Krallinger, Valencia, editors, 2007</marker>
<rawString>Lynette Hirschman, Martin Krallinger, and Alfonso Valencia, editors. 2007. Proceedings of the Second BioCreative Challenge Evaluation Workshop. Madrid: CNIO Centro Nacional de Investigaciones Oncol´ogicas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Corpora and their annotation</title>
<date>2006</date>
<booktitle>In Sophia Ananiadou and John McNaught, editors, Text Mining for Biology and Biomedicine</booktitle>
<pages>179--211</pages>
<publisher>Artech House</publisher>
<location>Norwood, MA</location>
<contexts>
<context>or three entity categories (‘cytokine and growth factor receptors’, ‘organisms and organism attributes’, and ‘transcription regulators and ligands’) in terms of the “authoritative annotator” F-score (Kim and Tsujii, 2006). Given two human annotators, this interannotator agreement metrics basically frames the annotations of one annotator to be the “gold standard” against which the other annotator’s annotations are eva</context>
</contexts>
<marker>Kim, Tsujii, 2006</marker>
<rawString>Jin-Dong Kim and Jun’ichi Tsujii. 2006. Corpora and their annotation. In Sophia Ananiadou and John McNaught, editors, Text Mining for Biology and Biomedicine, pages 179–211. Norwood, MA: Artech House.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoka Ohta</author>
<author>Yuka Teteisi</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>GENIA corpus: A semantically annotated corpus for bio-textmining</title>
<date>2003</date>
<journal>Bioinformatics</journal>
<booktitle>In Proceedings of the HLT-NAACL 2004 Workshop ‘Linking Biological Literature, Ontologies and Databases: Tools for Users – BioLink</booktitle>
<volume>19</volume>
<pages>61--68</pages>
<location>Boston, MA, USA</location>
<contexts>
<context>or abstracts (Hahn and Wermter, 2004). In the life sciences, this observation gave rise to the development of domain-specific corpora — GENIA being the first, most prominent example of these efforts (Kim et al., 2003) (approximately 500,000 tokens annotated with several entity types), and PENNBIOIE (Kulick et al., 2004) (approximately 500,000 tokens with 22 distinct entity types) being another example built with </context>
</contexts>
<marker>Kim, Ohta, Teteisi, Tsujii, 2003</marker>
<rawString>Jin-Dong Kim, Tomoka Ohta, Yuka Teteisi, and Jun’ichi Tsujii. 2003. GENIA corpus: A semantically annotated corpus for bio-textmining. Bioinformatics, 19(1):i180–2 Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel, Ryan McDonald, Martha Palmer, Andrew Schein, Lyle Ungar, Scott Winters, and Pete White. 2004. Integrated annotation for biomedical information extraction. In Proceedings of the HLT-NAACL 2004 Workshop ‘Linking Biological Literature, Ontologies and Databases: Tools for Users – BioLink 2004’, pages 61–68. Boston, MA, USA, May 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The PENN TREEBANK</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<contexts>
<context>lly ranging from POS tags over syntactic structure information up to named entities, semantic relations, and discourse structures) has been one of the true success stories for NLP. The PENN TREEBANK (Marcus et al., 1993) and the PENN PROPBANK (Palmer et al., 2005), e.g., have become a de facto standard for the coverage of the general newspaper language of English (though with a slant towards the economic domain). Ba</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The PENN TREEBANK. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-6</author>
</authors>
<date>1995</date>
<booktitle>Proceedings of the 6th Message Understanding Conference</booktitle>
<publisher>Morgan Kaufmann</publisher>
<location>Columbia, Maryland</location>
<marker>MUC-6, 1995</marker>
<rawString>MUC-6. 1995. Proceedings of the 6th Message Understanding Conference. Columbia, Maryland, November 6-8, 1995. San Mateo, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-7</author>
</authors>
<date>1998</date>
<booktitle>Proceedings of the 7th Message Understanding Conference, NYU</booktitle>
<marker>MUC-7, 1998</marker>
<rawString>MUC-7. 1998. Proceedings of the 7th Message Understanding Conference, NYU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles</title>
<date>2005</date>
<journal>Computational Linguistics</journal>
<volume>31</volume>
<contexts>
<context>ucture information up to named entities, semantic relations, and discourse structures) has been one of the true success stories for NLP. The PENN TREEBANK (Marcus et al., 1993) and the PENN PROPBANK (Palmer et al., 2005), e.g., have become a de facto standard for the coverage of the general newspaper language of English (though with a slant towards the economic domain). Based on the human-supplied annotations they c</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Poprat</author>
<author>Udo Hahn</author>
</authors>
<title>An investigation into the reusability of biomedical terminologies for the resolution of referential expressions</title>
<date>2007</date>
<booktitle>In BioLINK 2007 – Proceedings of the BioLINK SIG 2007. The Annual Meeting of the ISMB BioLINK Special Interest Group on Text Data Mining, in Association with ISMB</booktitle>
<pages>39--42</pages>
<location>Vienna, Austria</location>
<contexts>
<context>iers have not been sufficiently addressed yet. Therefore, we decided to define more comprehensive guidelines and to annotate referential expressions in biomedical abstracts in a more detailed manner (Poprat and Hahn, 2007b). Basically, our guidelines define as annotations mostly the heads of base nominal phrases (NPs). However, in some complex base NPs, we also have to annotate parts of base NPs (e.g., “IL-2-dependent</context>
</contexts>
<marker>Poprat, Hahn, 2007</marker>
<rawString>Michael Poprat and Udo Hahn. 2007a. An investigation into the reusability of biomedical terminologies for the resolution of referential expressions. In BioLINK 2007 – Proceedings of the BioLINK SIG 2007. The Annual Meeting of the ISMB BioLINK Special Interest Group on Text Data Mining, in Association with ISMB 2007, pages 39–42. Vienna, Austria, July 19, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Poprat</author>
<author>Udo Hahn</author>
</authors>
<title>Quantitative data on referring expressions in biomedical abstracts</title>
<date>2007</date>
<booktitle>In BioNLP at ACL 2007 – Proceedings of the Workshop on Biological, Translational, and Clinical Language Processing</booktitle>
<pages>193--194</pages>
<location>Prague, Czech Republic</location>
<contexts>
<context>iers have not been sufficiently addressed yet. Therefore, we decided to define more comprehensive guidelines and to annotate referential expressions in biomedical abstracts in a more detailed manner (Poprat and Hahn, 2007b). Basically, our guidelines define as annotations mostly the heads of base nominal phrases (NPs). However, in some complex base NPs, we also have to annotate parts of base NPs (e.g., “IL-2-dependent</context>
</contexts>
<marker>Poprat, Hahn, 2007</marker>
<rawString>Michael Poprat and Udo Hahn. 2007b. Quantitative data on referring expressions in biomedical abstracts. In BioNLP at ACL 2007 – Proceedings of the Workshop on Biological, Translational, and Clinical Language Processing, pages 193–194. Prague, Czech Republic, June 29, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Jos´e Casta˜no</author>
<author>Maciej Kotecki</author>
<author>Brent Cochran</author>
</authors>
<date>2007</date>
<note>An annotated biological corpus</note>
<marker>Pustejovsky, Casta˜no, Kotecki, Cochran, 2007</marker>
<rawString>James Pustejovsky, Jos´e Casta˜no, Maciej Kotecki, and Brent Cochran. 2007. An annotated biological corpus.</rawString>
</citation>
<citation valid="false">
<booktitle>In BioLINK 2007 – Proceedings of the BioLINK SIG 2007. The Annual Meeting of the ISMB BioLINK Special Interest Group on Text Data Mining</booktitle>
<pages>51--54</pages>
<marker></marker>
<rawString>In BioLINK 2007 – Proceedings of the BioLINK SIG 2007. The Annual Meeting of the ISMB BioLINK Special Interest Group on Text Data Mining, pages 51–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Austria Vienna</author>
</authors>
<title>Sampo Pyysalo, Filip Ginter, Juho Heimonen</title>
<date>2007</date>
<journal>Jari Bjorne, Jorma Boberg, Jouni Jarvinen, and Tapio Salakoski</journal>
<marker>Vienna, 2007</marker>
<rawString>Vienna, Austria, July 19, 2007. Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari Bjorne, Jorma Boberg, Jouni Jarvinen, and Tapio Salakoski.</rawString>
</citation>
<citation valid="true">
<title>BioInfer: A corpus for information extraction in the biomedical domain</title>
<date>2007</date>
<journal>Bioinformatics</journal>
<volume>8</volume>
<marker>2007</marker>
<rawString>2007. BioInfer: A corpus for information extraction in the biomedical domain. Bioinformatics, 8(50).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Tomanek</author>
<author>Joachim Wermter</author>
<author>Udo Hahn</author>
</authors>
<title>An approach to text corpus construction which cuts annotation costs and maintains reusability of annotated data</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL 2007 – Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</booktitle>
<pages>486--495</pages>
<location>Prague, Czech Republic</location>
<contexts>
<context>ncology and CYP450 proteins only. Again, it was shown that porting named entity recognizers trained on GENIA or PENNBIOIE data underperformed in biological fields other than the ones already covered (Tomanek et al., 2007b). Obviously, new fields (in the life sciences) not already covered by existing annotated corpora need new annotations unless one is willing to pay a high price, cashing in the performance-degrading </context>
<context>vision of new annotations is rather costly in terms of acquiring, training and supervising (life science) expert staff, our solution was to develop an annotation methodology based on Active Learning (Tomanek et al., 2007a). It turned out that applying this approach to the corpus annotation task, manpower expenses could be lowered by up to 75% for entity annotations, while keeping almost the same level of annotation q</context>
<context> and events, as well as referential discourse phenomena. 1www.bootstrep.eu 2www.stemnet.de 2.1. Annotation Environment All our annotations were performed using the Jena ANnotation Environment (JANE) (Tomanek et al., 2007b). It supports the whole annotation workflow, including the set-up of annotation projects, user management, annotation itself via an external editor, monitoring of the annotation process, and deploym</context>
<context>in an iterative manner those examples (in our case, sentences) for manual annotation which are expected to be the most informative for classifier learning. JANE employs a committee-based AL approach (Tomanek et al., 2007a) where in every iteration an ensemble of classifiers is trained on the already annotated material. Each of these classifiers make a prediction on the unlabeled examples. Examples on which the commit</context>
</contexts>
<marker>Tomanek, Wermter, Hahn, 2007</marker>
<rawString>Katrin Tomanek, Joachim Wermter, and Udo Hahn. 2007a. An approach to text corpus construction which cuts annotation costs and maintains reusability of annotated data. In EMNLP-CoNLL 2007 – Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 486–495. Prague, Czech Republic, June 28-30, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Tomanek</author>
<author>Joachim Wermter</author>
<author>Udo Hahn</author>
</authors>
<date>2007</date>
<contexts>
<context>ncology and CYP450 proteins only. Again, it was shown that porting named entity recognizers trained on GENIA or PENNBIOIE data underperformed in biological fields other than the ones already covered (Tomanek et al., 2007b). Obviously, new fields (in the life sciences) not already covered by existing annotated corpora need new annotations unless one is willing to pay a high price, cashing in the performance-degrading </context>
<context>vision of new annotations is rather costly in terms of acquiring, training and supervising (life science) expert staff, our solution was to develop an annotation methodology based on Active Learning (Tomanek et al., 2007a). It turned out that applying this approach to the corpus annotation task, manpower expenses could be lowered by up to 75% for entity annotations, while keeping almost the same level of annotation q</context>
<context> and events, as well as referential discourse phenomena. 1www.bootstrep.eu 2www.stemnet.de 2.1. Annotation Environment All our annotations were performed using the Jena ANnotation Environment (JANE) (Tomanek et al., 2007b). It supports the whole annotation workflow, including the set-up of annotation projects, user management, annotation itself via an external editor, monitoring of the annotation process, and deploym</context>
<context>in an iterative manner those examples (in our case, sentences) for manual annotation which are expected to be the most informative for classifier learning. JANE employs a committee-based AL approach (Tomanek et al., 2007a) where in every iteration an ensemble of classifiers is trained on the already annotated material. Each of these classifiers make a prediction on the unlabeled examples. Examples on which the commit</context>
</contexts>
<marker>Tomanek, Wermter, Hahn, 2007</marker>
<rawString>Katrin Tomanek, Joachim Wermter, and Udo Hahn. 2007b.</rawString>
</citation>
<citation valid="true">
<title>Efficient annotation with the Jena ANnotation Environment (JANE</title>
<booktitle>In The LAW at ACL 2007 – Proceedings of the Linguistic Annotation Workshop</booktitle>
<pages>9--16</pages>
<marker></marker>
<rawString>Efficient annotation with the Jena ANnotation Environment (JANE). In The LAW at ACL 2007 – Proceedings of the Linguistic Annotation Workshop, pages 9–16.</rawString>
</citation>
</citationList>
</algorithm>

