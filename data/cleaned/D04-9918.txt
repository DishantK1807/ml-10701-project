1:189	Automatic Analysis of Plot for Story Rewriting Harry Halpin School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh, EH8 9LW Scotland, UK H.Halpin@ed.ac.uk Johanna D. Moore School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh, EH8 9LW Scotland, UK J.Moore@ed.ac.uk Judy Robertson School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh EH8 9LW Scotland, UK judyr@inf.ed.ac.uk Abstract A method for automatic plot analysis of narrative texts that uses components of both traditional symbolic analysis of natural language and statistical machine-learning is presented for the story rewriting task.
2:189	In the story rewriting task, an exemplar story is read to the pupils and the pupils rewrite the story in their own words.
3:189	This allows them to practice language skills such as spelling, diction, and grammar without being stymied by content creation.
4:189	Often the pupil improperly recalls the story.
5:189	Our method of automatic plot analysis enables the tutoring system to automatically analyze the students story for both general coherence and specific missing events.
6:189	1 Introduction StoryStation is an intelligent tutoring system created to provide personalized attention and detailed feedback to children ages 10-12 on their writing (Roberston and Wiemar-Hastings, 2002).
7:189	Writing is viewed as a skill-based task, with skills being elements of writing such as spelling, diction, and plot development.
8:189	Each writing skill is associated with an animated agent that provides online help.
9:189	Evaluations of StoryStation show that children enjoy the personalized encouragement and constructive comments that StoryStation provides (Robertson and Cross, 2003).
10:189	StoryStation was designed by researchers in conjunction with two teachers and a group of students.
11:189	However, both students and teachers indicated StoryStation would be significantly improved if it were enhanced with an agent that could give feedback about the plot of a story.
12:189	Here we describe how techniques from symbolic natural language processing and statistical machinelearning were used to tackle the problem of automated plot analysis for StoryStation.
13:189	2 The Story Rewriting Task In the story rewriting task, pupils rewrite a story in their own words, allowing them to focus on their writing ability instead of plot formulation.
14:189	This task is currently used in Scottish schools and thus it was chosen to be the first feature of the plot analysis agent.
15:189	We collected a corpus of 103 stories rewritten by children from classes at primary schools in Scotland.
16:189	Pupils were told a story, an exemplar story, by a storyteller and were asked to rewrite the story in their own words.1 The automated plot analysis program must be able to give a general rating of the quality of the rewritten storys plot and be able to determine missing or incorrect events.
17:189	The general rating can be used by the teacher to find out which pupils are in need of attention, while the more specific details can be used by an animated agent in StoryStation to remind the student of specific events and characters they have forgotten or misused.
18:189	3 Plot Ratings The stories were rated for plot by three different raters.
19:189	A story-teller (Rater B) ranked all of the stories.
20:189	Two others (Rater A, a teacher, and Rater C) ranked the stories as well, although Rater A ranked only half.
21:189	The following scale, devised by a teacher with over forty years of experience, was used.
22:189	1.
23:189	Excellent: An excellent story shows that the reader understands the point of the story and should demonstrate some deep understanding of the plot.
24:189	The pupil should be able to retrieve all the important links and, not all the details, but the right details.
25:189	2.
26:189	Good: A good story shows that the pupil was listening to the story, and can recall the main 1The exemplar story used in our corpus was Nils Adventure, a story from The Wonderful Adventures of Nils (Lagerloff, 1907).
27:189	Class Probability Number of Class 1 (Excellent) 0.175 18 2 (Good) 0.320 33 3 (Fair) 0.184 19 4 (Poor) 0.320 33 Table 1: Distribution of Story Ratings events and links in the plot.
28:189	However, the pupil shows no deeper understanding of the plot, which can often be detected by the pupil leaving out an important link or emphasizing the wrong details.
29:189	3.
30:189	Fair: A fair story shows that the pupil is missing more than one link or chunk of the story, and not only lacks an understanding of the point but also lacks recall of vital parts of the story.
31:189	A fair story does not really flow.
32:189	4.
33:189	Poor: A poor story has definite problems with recall of events, and is missing substantial amount of the plot.
34:189	Characters will be misidentified and events confused.
35:189	Often the child writes on the wrong subject or starts off reciting only the beginning of the story.
36:189	Rater B and Rater A had an agreement of 39% while Rater B and Rater C had an agreement of 77%.
37:189	However, these numbers are misleading as the rating scale is ordinal and almost all the disagreements were the result of grading a story either one rank better or worse.
38:189	In particular Rater A usually marked incomplete stories as poor while the other raters assigned partial credit.
39:189	To evaluate the reliability of the grades both Cronbachs and Kendalls b were used, since these statistics take into account ordinal scales and inter-rater reliability.
40:189	Between Rater A and B there was a Cronbachs statistic of.86 and a Kendalls b statistic of .72.
41:189	Between Rater B and C there was a Cronbachs statistic of .93 and Kendalls b statistic of .82.
42:189	These statistics show our rating scheme to be fairly reliable.
43:189	As the most qualified expert to rate all the stories, Rater Bs ratings were used as the gold standard.
44:189	The distribution of plot ratings are given in Table 1.
45:189	4 A Minimal Event Calculus The most similar discourse analysis program to the one needed by StoryStation is the essay-grading component of Criterion by ETS technologies (Burstein et al. , 2003), which is designed to annotate parts of an essay according to categories such as Thesis, Main Points, Support, and Conclusion. Burstein et.
46:189	al.
47:189	(2003) uses Rhetorical Structure Theory to parse the text into discourse relations based on satellites and nuclei connected by rhetorical relations.
48:189	Moore and Pollack (1992) note that Rhetorical Structure Theory conflates the informational (the information being conveyed) and intentional (the effects on the readers beliefs or attitudes) levels of discourse.
49:189	Narratives are primarily informational, and so tend to degenerate to long sequences of elaboration or sequence relations.
50:189	Since in the story rewriting task the students are attempting to convey information about the narrative, unlike the primarily persuasive task of an essay, our system focuses on the informational level as embodied by a simplified event calculus.
51:189	Another tutoring system similar to ours is the WHY physics tutoring system (Rose et al. , 2002).
52:189	We formulate only three categories to describe stories: events, event names, and entities.
53:189	This formulation keeps the categories from being arbitrary or exploding in number.
54:189	Entities are both animate characters, such as elves and storks, and inanimate objects like sand and weather. Nouns are the most common type of entities.
55:189	Events are composed of the relationships among entities, such as the boy becomes an elf, which is composed of a boy and elf interacting via becoming, which we call the event name.
56:189	This is because the use of such verbs is an indicator of the presence of an event in the story.
57:189	In this manner events are relationships labeled with an event name, and entities are arguments to these relationships as in propositional logic.
58:189	Together these can form events such as become(boy,elf), and this formulation maps partially onto Shanahans event calculus which has been used in other story-understanding models (Mueller, 2003).
59:189	The key difference between an event calculus and a collection of propositions is that time is explicitly represented in the event calculus.
60:189	Each story consists of a group of events that are present in the story, e1:::eh.
61:189	Each event consists of an event name, a time variable t, and a set of entities arranged in an ordered set n1:::na.
62:189	An event must contain one and only one event name.
63:189	The event names are usually verbs, while the entities tend to be, but are not exclusively, nouns.
64:189	Time is made explicit through a variable t. Normally, the Shanahan event calculus has a series of predicates to deal with relations of achievements, accomplishments, and other types of temporal relations (Shanahan, 1997), however our calculus does not use these since it is difficult to extract these from ungrammatical raw text automatically.
65:189	A storys temporal order is a partial ordering of events as denoted by their time variable t. When incorporating a set of entities into an event, a superscript is used to keep the entities distinct, as n13 is entity 1 in event 3.
66:189	An entity may appear in multiple events, such as entity 1 appearing in event 3 (n13) and in event 5 (n15).
67:189	The plot of a story can then be considered an event structure of the following form if it has h events: e1(t1; (n11; n21; :::na1)); ::::; eh(th; (n2h; n4h:::nch)) Where time t1 t2 :::th.
68:189	An example from a rewritten story is Nils found a coin and he walked round a sandy beach.
69:189	He talked to the stork.
70:189	Asked a question. This is represented by an event structure as: find(t = 1(Nils; coin)); walk(t = 1; (Nils; sand; beach)); talk(t = 2; (stork; Nils)); ask(t = 3; (question)) Note that the rewritten stories are often ungrammatical.
71:189	A sentence may map onto one, multiple, or no events.
72:189	Two stories match if they are composed of the same ordering of events.
73:189	5 Extracting the Event Calculus The event calculus can be extracted from raw text by layering NLP modules using an XML-based pipeline.
74:189	Our main constraint was that the text of the pupil was rarely grammatical, restricting our choice of NLP components to those that did not require a correct parse or were in any other ways dependent on grammatical sentences.
75:189	At each level of processing, an XML-enabled natural language processing component can add mark-up to the text, and use any mark-up that the previous components made.
76:189	All layers in the pipeline are fully automatic.
77:189	For our pipeline we used LT-TTT (Language Technology Text Tokenization Toolkit) (Grover et al. , 2000).
78:189	Once words are tokenized and sentence boundaries detected by LT-TTT, LT-POS tags the words using the Penn Treebank tag-set without parsing the sentences.
79:189	While a full parse could be generated by a statistical parser, such parses would likely be incorrect for the ungrammatical sentences often generated by the pupils (Charniak, 2000).
80:189	Pronouns are resolved using a cascading rule-based approach directly inspired by the CogNIAC algorithm (Baldwin, 1997) with two variations.
81:189	First, it resolves in distinct cascades for singular and then plural pronouns.
82:189	Second, it resolves using only the CogNIAC rules that can be determined using Penn Treebank tags.
83:189	The words are lemmatized using an augmented version of the SCOL Toolset and sentences are chunked using the Cass Chunker (Abney, 1995).
84:189	There is a trade-off between this chunking approach that works on ungrammatical sentences and one that requires a full parse such as those using dependency grammars.
85:189	The Cass Chunker is highly precise, but often inaccurate and misses relations and entities that are not in a chunk.
86:189	In its favor, those tuples in chunks that it does identify are usually correct.
87:189	SCOL extracts tuples from the chunks to determine the presence of events, and the remaining elements in the chunk are inspected via rules for entities.
88:189	Time is explicitly identified using a variation of the now point algorithm (Allen, 1987).
89:189	We map each events time variable to a time-line, assuming that events occur in the order in which they appear in the text.
90:189	While temporal ordering of events is hard (Mani and Wilson, 2003), given that children of this age tend to use a single tense throughout the narrative and that in narratives events are presented in order (Hickmann, 2003), this simple algorithm should suffice for ordering in the domain of childrens stories.
91:189	6 Plot Comparison Algorithm Since the story rewriting task involves imperfect recall, story events will likely be changed or left out by the pupil.
92:189	The story rewriting task involves the students choosing their own diction and expressing their own unique mastery of language, so variation in how the fundamental elements of the story are rewritten is to be expected.
93:189	To deal with these issues, an algorithm had to be devised that takes the event structure of the rewritten story and compares it to the event structure of the exemplar story, while disregarding the particularities of diction and grammar. The problem is one of credit allocation for the similarity of rewritten events to the exemplar event.
94:189	The words used in the events of the two story models may differ.
95:189	The exemplar story model might use the event see(Nils,stork), but a rewritten story may use the word bird instead of the more precise word stork. However, since the bird is referring to the stork in the exemplar story, partial credit should be assigned.
96:189	A plot comparison algorithm was created that uses abstract event calculus representations of plot and the text of the rewritten story, taking into account temporal order and word similarity.
97:189	The exemplar storys event structure is created by applying the event extraction pipeline to the storytellers transcript.
98:189	The Plot Comparison Algorithm is given in Figure 1.
99:189	In the pseudo-code, E of size h and R of size j are the event structures of the exemplar story and rewritten story respectively, with the names of each of their events denoted as e and r. The set of entities of each event are denoted as Ne and Nr respectively.
100:189	T is the lemmatized tokens of the rewritten storys raw text.
101:189	WordNet(x) denotes the synset of x. The now point of the rewritten story is t, and feature set is f, which has an index of i. The index i is incremented every time f is assigned a value.
102:189	1 denotes an exact match, 2 a WordNet synset match, 3 a match in the text, and 0 a failure to find any match.
103:189	The Plot Comparison Algorithm essentially iterates through the exemplar story looking for matches of the events in the rewritten story.
104:189	To find if two events are in or out of order the rewritten story has a now point that serves as the beginning of its iteration.
105:189	Each event of the event structure of the exemplar story is matched against each event of the rewritten story starting at the now point and using the exact text of the event name.
106:189	If that match fails a looser match is attempted by giving the event names of the rewritten story to WordNet and seeing if a match to the resultant synset succeeds (Fellbaum, 1998).
107:189	If either match attempt succeeds, the algorithm attempts to match entities in the same fashion and the now point of the rewritten story is incremented.
108:189	Thus the algorithm does not looks back in the rewritten story for a match.
109:189	If the event match fails, one last attempt is made by checking the event name or entity against every lemmatized token in the entire rewritten text.
110:189	If this fails, a failure is recorded.
111:189	The results of the algorithm are can be used as a feature set for machine-learning.
112:189	The event calculus extraction pipeline and the Plot Comparison Algorithm can produce event calculus representations of any English text and compare them.
113:189	They have been tested on other stories that do not have a significant corpus of rewritten stories.
114:189	The number of events for an average rewritten story in our corpus was 26, with each event having an average of 1 entity.
115:189	Included in Figure 2 is sample output from our algorithm given the exemplar story model ea and a rewritten story rb whose text is as follows: Nils took the coin and tossed it away, cause it was worthless.
116:189	A city appeared and so he walked in.
117:189	Everywhere was gold and the merchant said Buy this Only one coin Nils has no coin.
118:189	So he went to get the coin he threw away but the city vanished just like that right behind him.
119:189	Nils asked the bird Hey where the city go?
120:189	Lets go home.
121:189	Due to space limitations, we only display selected events from the transcript and their most likely match from the rewritten story in Figure 2.
122:189	The output of the feature set would be the concatenation in order of every value of fe.
123:189	Algorithm 6.1: PLOTCOMPARE(E; R; T) t 1 i 0 for ex e1 to eh do for ry rt to rj do 8> >>> >>>> >>>> >>> >>>> >>>> >>> >>>< >>> >>>> >>>> >>> >>>> >>>> >>> >>>> : if ex = ry then fi 1 and t t + 1 else if ex 2WORDNET(ry) then fi 2 and t t + 1 if fi = 1 or 2 then 8> >>>> >>>> >< >>>> >>>> >>: for each n2Ne if n2Nr then fi 1 else if n2WORDNET(Nr) then fi 2 else if n2T then fi 3 else fi 0 else if ex 2T then fi 3 else fi 0 Figure 1: Plot Comparison Algorithm ea rb fe throw(Nils, coin) toss(coin) 2; 3; 1 see(Nils, city) appear(city) 0; 3; 3 enter(Nils, city) walk(Nils) 0; 3; 3 ask(Nils, merchant) say(merchant) 0; 3; 3 say(Nils) say(merchant) 1; 3 leave(Nils) go(Nils) 2; 1 disappear(city) vanish(city) 2; 1 inquire(Nils, stork) ask(Nils, bird) 2; 1; 2 fly(stork) go(home) 0; 3 Figure 2: Example of Plot Algorithm 7 Learning the Significance of Events Machine-learning is crucial to our experiment, as it will allow our model to discriminate what events and words in a rewritten story are good predictors of plot quality as rated by a human expert.
124:189	We have restricted our feature set to the results of the Plot Comparison Algorithm and LSA scores, as we describe below.
125:189	Other possible features, such as the grammatical correctness and the number of conjunctives, are dealt with by other agents in StoryStation.
126:189	We are focusing on plot recall quality as opposed to general writing quality.
127:189	Two different machine-learning algorithms with differing assumptions were used.
128:189	These are by no means exhaustive of the options, and extensive tests have been done with other algorithms.
129:189	Further experiments are needed to understand the precise nature of the relations between the feature set and machine learning algorithms.
130:189	All results were created by ten-fold cross validation over the rated stories, which is especially important given our small corpus size.
131:189	7.1 Nearest Neighbors using LSA We can classify the stories without using the results of the Plot Comparison Algorithm, and instead use only their statistical attributes.
132:189	Latent Semantic Analysis (LSA) provides an approximation of semantic similarity based on the hypothesis that the semantics of a word can be deduced from its context in an entire document, leading to useful coherency scores when whole documents are compared (Foltz et al. , 1998).
133:189	LSA compares the text of each rewritten story in the corpus for similarity to the transcript of the exemplar story in a subspace produced by reducing the dimensionality of the TASA 12 grade USA reading-level to 200.
134:189	This dimensionality was discovered through experimentation to be our problems optimal parameters for LSA given the range of choices originally used by Landauer (1997).
135:189	The stories can be easily classified by grouping them together based on LSA similarity scores alone, and this technique is embodied in the simple K-Nearest Neighbors (K-NN) learner.
136:189	K-NN makes no parametric assumptions about the data and uses no formal symbolic features other than an LSA similarity score.
137:189	For K-NN k = 4 gave the best results over a large range of k, and we expect this k would be ideal for stories of similar length.
138:189	As shown in Table 2, despite its simplicity this algorithm performs fairly well.
139:189	It is not surprising that features based primarily on word distributions such as LSA could correctly discriminate the non-poor from the poor rewritten stories.
140:189	Some good rewritten stories closely resemble the exemplar story almost word for word, and so share the same word distribution with the exemplar story.
141:189	Poor rewritten stories usually have little resemblance to the exemplar story, and so have a drastically different word distribution.
142:189	The high spread of error in classifying stories is shown in the confusion matrix in Table 3.
143:189	This leads to unacceptable errors such as excellent stories being classified as poor stories.
144:189	7.2 Hybrid Model with Naive Bayes By using both LSA scores and event structures as features for a statistical machine learner, a hybrid model of plot rating can be created.
145:189	In hybrid modClass Precision Recall F-score 1 (Excellent) 0.11 0.17 0.13 2 (Good) 0.42 0.46 0.44 3 (Fair) 0.30 0.16 0.21 4 (Poor) 0.83 0.76 0.79 Table 2: K-Nearest Neighbors Precision and Recall Class 1 2 3 4 1 (Excellent) 3 10 4 1 2 (Good) 13 15 2 3 3 (Fair) 9 6 3 1 4 (Poor) 2 5 1 25 Table 3: K-Nearest Neighbors: Confusion Matrix els a formal symbolic model (the event calculusbased results of a Plot Comparison Algorithm) enters a mutually beneficial relationship with a statistical model of the data (LSA), mediated by a machine learner (Naive Bayes).
146:189	One way to combine LSA similarity scores and the results of the event structure is by using the Naive Bayes (NB) machine learner.
147:189	NB makes the assumptions of both parametrization and Conditional Independence.
148:189	The recall and precision per rank is given in Table 4, and it is clear that while no stories are classified as excellent at all, the majority of good and poor stories are identified correctly.
149:189	As shown by the confusion matrix in Table 5, NB does not detect excellent stories and it collapses the distinction between good and excellent stories.
150:189	Compared to K-NN with LSA, NB shows less spread in its errors, although it does confuse some poor stories as good and one excellent story as fair.
151:189	Even though it mistakenly classifies some poor stories as good, for many teachers this is better than misidentifying a good story as a poor story.
152:189	The raw accuracy results over all classes of the machine learning algorithms are summarized in Table 6.
153:189	Note that average human rater agreement is the average agreement between Rater A and C (whose agreement ranged from 39% to 77%), since Rater Bs ratings were used as the gold standard.
154:189	This average also assumes Rater A would have continued marking at the same accuracy for the comClass Precision Recall F-Score 1 (Excellent) 0.00 0.00 0.00 2 (Good) 0.43 0.88 0.58 3 (Fair) 0.45 0.26 0.33 4 (Poor) 0.92 0.67 0.77 Table 4: Naive Bayes Precision and Recall Class 1 2 3 4 1 (Excellent) 0 17 1 0 2 (Good) 1 29 2 1 3 (Fair) 0 13 5 1 4 (Poor) 0 8 3 22 Table 5: Naive Bayes Confusion Matrix Machine Learner Percentage Correct K-NN (LSA) 44.66% ID3 DT (Events) 40.78% NB (LSA + Events) 54.37% Rater Agreement 58.37% Table 6: Machine Learner Comparison plete corpus.
155:189	DT refers to an ID3 Decision Tree algorithm that creates a purely symbolic machinelearner whose feature set was only the results of the Plot Comparison Algorithm (Quinlan, 1986).
156:189	It performed worse than K-NN and thus the details are not reported any further.
157:189	Using NB and combining the LSA scores with the results of the Plot Comparison Algorithm produces better raw performance than K-NN.
158:189	Recall of 54% for NB may seem disappointing, but given that the raters only have an average agreement of 58%, the performance of the machine learner is reasonable.
159:189	So if the machinelearner had a recall of 75% it would be suspect.
160:189	Statistics to compare the results given the ordinal nature of our rating scheme are shown in Table 7.
161:189	8 Discussion From these experiments as shown in Table 6 we see that the type of machine learner and the particular features are important to correctly classify childrens stories.
162:189	Inspection of the results shows that separating good and excellent stories from poor stories is best performed by Naive Bayes.
163:189	For our application, teachers have indicated that the classification of an excellent or good story as a poor one is considered worse than the classifying of a fair or even poor story as good.
164:189	Moreover, it uses the event-based results of the Plot Comparison Algorithm so that the agent in StoryStation may use these results to inform the student what precise events and entities are missing or misused.
165:189	NB is fast enough to provide possible feedback in real time and its ability to separate poor stories from good and excellent stories would allow it to be used in classrooms.
166:189	It also has comparable raw accuracy to average human agreement as shown in Table 6, although it makes more errors than humans in classifying a story off by more than one class off as shown by the statistics Machine Learner Cronbachs Kendalls b NB to Rater B .78 .59 Rater A to Rater B .86 .72 Rater C to Rater B .93 .82 Table 7: Statistical Comparison in Table 7.
167:189	The results most in its favor are shown highlighted in Table 5.
168:189	It separates with few errors both excellent and good stories from the majority of poor stories.
169:189	While the event calculus captures some of the relevant defining characteristics of stories, it does not capture all of them.
170:189	The types of stories that give the machine learners the most difficulty are those which are excellent and fair.
171:189	One reason is that these stories are less frequent in the training data than poor and good stories.
172:189	Another reason is that there are features particular to these stories that are not accounted for by an event structure or LSA.
173:189	Both excellent stories and fair stories rely on very subtle features to distinguish them from good and poor stories.
174:189	Good stories were characterized in the rating criteria as parroting off of the main events, and the event calculus naturally is good at identifying this.
175:189	Poor stories have definite problems with the recall of events, and so are also easily identified.
176:189	However, fair stories show both a lack of understanding of the point and do not really flow while the excellent story shows an understanding of the point. These characteristics involve relations such as the point of the story and connections between events.
177:189	These ideas of flow and point are much more difficult to analyze automatically.
178:189	9 Conclusion Due to its practical focus, the plot analysis of our system is very limited in nature, focusing on just the story rewriting task.
179:189	Traditionally deep representation systems have attempted to be powerful general-purpose story understanding or generation systems.
180:189	A general plot analysis agent would be more useful than our current system, which is successful by virtue of the story rewriting task being less complex than full story understanding.
181:189	However, our system fulfills an immediate need in the StoryStation application, in contrast to more traditional story-understanding and story-generation systems, which are usually used as testing grounds for theoretical ideas in artificial intelligence.
182:189	The system was tested and developed using a small manually collected corpus of a single rewritten story.
183:189	While previous researchers who worked on this problem felt that the small size of the corpus made machine-learning unusable, the results shows that with careful feature selection and relatively simple algorithms empirical methods can be made to work.
184:189	We expect that our technique can be generalized to larger corpora of diverse types.
185:189	Our hybrid system uses both LSA and event structures to classify plot quality.
186:189	The use of event structures in classifying stories allows us to detect whether particular crucial characters and events have been left out of the rewritten story.
187:189	Separating the students who have written good plots from those who have done so poorly is a boon to the teachers, since often it is the students who have the most difficulty with plot that are least likely to ask a teacher for help.
188:189	StoryStation is now being used in two schools as part of their classroom writing instruction over the course of the next year.
189:189	Results from this study will be instrumental in shaping the future of the plot analysis system in StoryStation and the expansion of the current system into a general purpose plot analysis system for other writing tasks.

