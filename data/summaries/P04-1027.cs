Previous studies have shown that it is feasible to evaluate the output of summarization systems automatically (Lin and Hovy, 2003).
N03-1020
From this point of view, some of the measures used in the evaluation of Machine Translation systems, such as BLEU (Papineni et al., 2002), have been imported into the summarization task.
P02-1040
There are, at least, two reasons to explain this: (Khandelwal et al., 2001) work on an average of 43 documents, half the size of the topics in our corpus.
H01-1022
We have compared several similarity metrics, including a few baseline measures (based on document, sentence and vocabulary overlap) and a stateof-the-art measure to evaluate summarization systems, ROUGE (Lin and Hovy, 2003).
N03-1020
Topic-oriented multi-document summarization has already been studied in other evaluation initiatives which provide testbeds to compare alternative approaches (Over, 2003; Goldstein et al., 2000; Radev et al., 2000).
W00-0403
For our experiment, we have only considered unigrams (lemmatized words, excluding stop words), which gives good results with standard summaries (Lin and Hovy, 2003).
N03-1020
(Lin and Hovy, 2003) tried to apply BLEU as a measure to evaluate summaries, but the results were not as good as in Machine Translation.
N03-1020
(Khandelwal et al., 2001) propose a task, called “Temporal Summarization”, that combines summarization and topic tracking.
H01-1022
