Keller and Lapata (2003) demonstrated a high degree of correlation between n-gram estimates fromsearch enginehitcountsand n-gramfrequencies obtained from traditional corpora such as the British National Corpus (BNC).
J03-3005
The vast amount of text available on the web is a major advantage, with Keller and Lapata (2003) estimating that over 98 billion words were indexed by Google in 2003.
J03-3005
Many NLP tasks have successfully utilised web data, including machine translation (Grefenstette, 1999), prepositional phrase attachment (Volk, 2001), and other-anaphora resolution (Modjeska et al., 2003).
W03-1023
To demonstrate the utility of a large collection of web data on a disambiguation problem, we implemented the simple memory-based learner from Banko and Brill (2001).
P01-1005
Thememory-basedlearnerwastestedusing the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus.
W95-0104
Banko and Brill (2001) showed that for contextsensitive spelling correction, increasing the training data size increases the accuracy, for up to 1 billion words in their experiments.
P01-1005
Golding. 1995.
W95-0104
Ravichandran et al.(2005) used a collection of 31 million web pages to produce noun similarity lists.
P05-1077
Clarke et al.(2002) and Terra and Clarke (2003) used this corpus for their question answering system.
N03-1032
Keller and Lapata (2003) needed to obtain the frequency counts of 26,271 test adjective pairs from the web and from the BNC for the task of prenominal adjective ordering.
J03-3005
