1:189	Monolingual Machine Translation for Paraphrase Generation Chris QUIRK, Chris BROCKETT and William DOLAN Natural Language Processing Group Microsoft Research One Microsoft Way Redmond, WA 90852 USA {chrisq,chrisbkt,billdol}@microsoft.com Abstract We apply statistical machine translation (SMT) tools to generate novel paraphrases of input sentences in the same language.
2:189	The system is trained on large volumes of sentence pairs automatically extracted from clustered news articles available on the World Wide Web.
3:189	Alignment Error Rate (AER) is measured to gauge the quality of the resulting corpus.
4:189	A monotone phrasal decoder generates contextual replacements.
5:189	Human evaluation shows that this system outperforms baseline paraphrase generation techniques and, in a departure from previous work, offers better coverage and scalability than the current best-of-breed paraphrasing approaches.
6:189	1 Introduction The ability to categorize distinct word sequences as meaning the same thing is vital to applications as diverse as search, summarization, dialog, and question answering.
7:189	Recent research has treated paraphrase acquisition and generation as a machine learning problem (Barzilay & McKeown, 2001; Lin & Pantel, 2002; Shinyama et al, 2002, Barzilay & Lee, 2003, Pang et al. , 2003).
8:189	We approach this problem as one of statistical machine translation (SMT), within the noisy channel model of Brown et al.9:189	(1993).
10:189	That is, we seek to identify the optimal paraphrase T* of a sentence S by finding: (){} )P()|P(maxarg |Pmaxarg* TTS STT T T = = T and S being sentences in the same language.
11:189	We describe and evaluate an SMT-based paraphrase generation system that utilizes a monotone phrasal decoder to generate meaning-preserving paraphrases across multiple domains.
12:189	By adopting at the outset a paradigm geared toward generating sentences, this approach overcomes many problems encountered by task-specific approaches.
13:189	In particular, we show that SMT techniques can be extended to paraphrase given sufficient monolingual parallel data.
14:189	1 We show that a huge corpus of comparable and alignable sentence pairs can be culled from ready-made topical/temporal clusters of news articles gathered on a daily basis from thousands of sources on the World Wide Web, thereby permitting the system to operate outside the narrow domains typical of existing systems.
15:189	2 Related work Until recently, efforts in paraphrase were not strongly focused on generation and relied primarily on narrow data sources.
16:189	One data source has been multiple translations of classic literary works (Barzilay & McKeown 2001; Ibrahim 2002; Ibrahim et al. 2003).
17:189	Pang et al.18:189	(2003) obtain parallel monolingual texts from a set of 100 multiply-translated news articles.
19:189	While translation-based approaches to obtaining data do address the problem of how to identify two strings as meaning the same thing, they are limited in scalability owing to the difficulty (and expense) of obtaining large quantities of multiply-translated source documents.
20:189	Other researchers have sought to identify patterns in large unannotated monolingual corpora.
21:189	Lin & Pantel (2002) derive inference rules by parsing text fragments and extracting semantically similar paths.
22:189	Shinyama et al.23:189	(2002) identify dependency paths in two collections of newspaper articles.
24:189	In each case, however, the information extracted is limited to a small set of patterns.
25:189	Barzilay & Lee (2003) exploit the metainformation implicit in dual collections of news1 Barzilay & McKeown (2001), for example, reject the idea owing to the noisy, comparable nature of their data.
26:189	wire articles, but focus on learning sentence-level patterns that provide a basis for generation.
27:189	Multisequence alignment (MSA) is used to identify sentences that share formal (and presumably semantic) properties.
28:189	This yields a set of clusters, each characterized by a word lattice that captures n-grambased structural similarities between sentences.
29:189	Lattices are in turn mapped to templates that can be used to produce novel transforms of input sentences.
30:189	Their methodology provides striking results within a limited domain characterized by a high frequency of stereotypical sentence types.
31:189	However, as we show below, the approach may be of limited generality, even within the training domain.
32:189	3 Data collection Our training corpus, like those of Shinyama et al. and Barzilay & Lee, consists of different news stories reporting the same event.
33:189	While previous work with comparable news corpora has been limited to just two news sources, we set out to harness the ongoing explosion in internet news coverage.
34:189	Thousands of news sources worldwide are competing to cover the same stories, in real time.
35:189	Despite different authorship, these stories cover the same events and therefore have significant content overlap, especially in reports of the basic facts.
36:189	In other cases, news agencies introduce minor edits into a single original AP or Reuters story.
37:189	We believe that our work constitutes the first to attempt to exploit these massively multiple data sources for paraphrase learning and generation.
38:189	3.1 Gathering aligned sentence pairs We began by identifying sets of pre-clustered URLs that point to news articles on the Web, gathered from publicly available sites such as http://news.yahoo.com/, http://news.google.com and http://uk.newsbot.msn.com.
39:189	Their clustering algorithms appear to consider the full text of each news article, in addition to temporal cues, to produce sets of topically/temporally related articles.
40:189	Story content is captured by downloading the HTML and isolating the textual content.
41:189	A supervised HMM was trained to distinguish story content from surrounding advertisements, etc. 2 Over the course of about 8 months, we collected 11,162 clusters, comprising 177,095 articles and averaging 15.8 articles per cluster.
42:189	The quality of 2 We hand-tagged 1,150 articles to indicate which portions of the text were story content and which were advertisements, image captions, or other unwanted material.
43:189	We evaluated several classifiers on a 70/30 test train split and found that an HMM trained on a handful of features was most effective in identifying content lines (95% F-measure).
44:189	these clusters is generally good.
45:189	Impressionistically, discrete events like sudden disasters, business announcements, and deaths tend to yield tightly focused clusters, while ongoing stories like the SARS crisis tend to produce very large and unfocused clusters.
46:189	To extract likely paraphrase sentence pairs from these clusters, we used edit distance (Levenshtein 1966) over words, comparing all sentences pairwise within a cluster to find the minimal number of word insertions and deletions transforming the first sentence into the second.
47:189	Each sentence was normalized to lower case, and the pairs were filtered to reject:  Sentence pairs where the sentences were identical or differed only in punctuation;  Duplicate sentence pairs;  Sentence pairs with significantly different lengths (the shorter is less than two-thirds the length of the longer);  Sentence pairs where the Levenshtein distance was greater than 12.0.
48:189	3 A total of 139K non-identical sentence pairs were obtained.
49:189	Mean Levenshtein distance was 5.17; mean sentence length was 18.6 words.
50:189	3.2 Word alignment To this corpus we applied the word alignment algorithms available in Giza++ (Och & Ney, 2000), a freely available implementation of IBM Models 1-5 (Brown, 1993) and the HMM alignment (Vogel et al, 1996), along with various improvements and modifications motivated by experimentation by Och & Ney (2000).
51:189	In order to capture the many-to-many alignments that identify correspondences between idioms and other phrasal chunks, we align in the forward direction and again in the backward direction, heuristically recombining each unidirectional word alignment into a single bidirectional alignment (Och & Ney 2000).
52:189	Figure 1 shows an example of a monolingual alignment produced by Giza++.
53:189	Each line represents a uni-directional link; directionality is indicated by a tick mark on the target side of the link.
54:189	We held out a set of news clusters from our training data and extracted a set of 250 sentence pairs for blind evaluation.
55:189	Randomly extracted on the basis of an edit distance of 5  n  20 (to allow a range of reasonably divergent candidate pairs while eliminating the most trivial substitutions), the gold-standard sentence pairs were checked by an independent human evaluator to ensure that 3 Chosen on the basis of ablation experiments and optimal AER (discussed in 3.2).
56:189	they contained paraphrases before they were hand word-aligned.
57:189	To evaluate the alignments, we adhered to the standards established in Melamed (2001) and Och & Ney (2000, 2003).
58:189	Following Och & Neys methodology, two annotators each created an initial annotation for each dataset, subcategorizing alignments as either SURE (necessary) or POSSIBLE (allowed, but not required).
59:189	Differences were highlighted and the annotators were asked to review their choices on these differences.
60:189	Finally we combined the two annotations into a single gold standard: if both annotators agreed that an alignment should be SURE, then the alignment was marked as SURE in the gold-standard; otherwise the alignment was marked as POSSIBLE.
61:189	To compute Precision, Recall, and Alignment Error Rate (AER) for the twin datasets, we used exactly the formulae listed in Och & Ney (2003).
62:189	Let A be the set of alignments in the comparison, S be the set of SURE alignments in the gold standard, and P be the union of the SURE and POSSIBLE alignments in the gold standard.
63:189	Then we have: || || precision A PA  = || || recall S SA  = || || AER SA SAPA + + = Measured in terms of AER 4, final interrater agreement between the two annotators on the 250 sentences was 93.1%.
64:189	4 The formula for AER given here and in Och & Ney (2003) is intended to compare an automatic alignment against a gold standard alignment.
65:189	However, when comparing one human against another, both comparison and reference distinguish between SURE and POSSIBLE links.
66:189	Because the AER is asymmetric (though each direction Table 1 shows the results of evaluating alignment after trainng the Giza++ model.
67:189	Although the overall AER of 11.58% is higher than the best bilingual MT systems (Och & Ney, 2003), the training data is inherently noisy, having more in common with analogous corpora than conventional MT parallel corpora in that the paraphrases are not constrained by the source text structure.
68:189	The identical word AER of 10.57% is unsurprising given that the domain is unrestricted and the alignment algorithm does not employ direct string matching to leverage word identity.
69:189	5 The non-identical word AER of 20.88% may appear problematic in a system that aims to generate paraphrases; as we shall see, however, this turns out not to be the case.
70:189	Ablation experiments, not described here, indicate that additional data will improve AER.
71:189	3.3 Identifying phrasal replacements Recent work in SMT has shown that simple phrase-based MT systems can outperform more sophisticated word-based systems (e.g. Koehn et al. 2003).
72:189	Therefore, we adopt a phrasal decoder patterned closely after that of Vogel et al.73:189	(2003).
74:189	We view the source and target sentences S and T as word sequences s 1s m and t 1 t n . A word alignment A of S and T can be expressed as a function from each of the source and target tokens to a unique cept (Brown et al. 1993); isomorphically, a cept represents an aligned subset of the source and target tokens.
75:189	Then, for a given sentence pair and word alignment, we define a phrase pair as a subset of the cepts in which both the source and target tokens are contiguous.
76:189	6 We gathered all phrase differs by less than 5%), we have presented the average of the directional AERs.
77:189	5 However, following SMT practice of augmenting data with a bilingual lexicon, we did append an identity lexicon to the training data.
78:189	6 While this does preclude the usage of gapped phrase pairs such as or  either  or, we found such mapTraining Data Type: L12 Precision 87.46% Recall 89.52% AER 11.58% Identical word precision 89.36% Identical word recall 89.50% Identical word AER 10.57% Non-identical word precision 76.99% Non-identical word recall 90.22% Non-identical word AER 20.88% Table 1.
79:189	AER on the Lev12 corpus Figure 1.
80:189	An example Giza++ alignment pairs (limited to those containing no more than five cepts, for reasons of computational efficiency) occurring in at least one aligned sentence somewhere in our training corpus into a single replacement database.
81:189	This database of lexicalized phrase pairs, termed phrasal replacements, serves as the backbone of our channel model.
82:189	As in (Vogel et al. 2003), we assigned probabilities to these phrasal replacements via IBM Model 1.
83:189	In more detail, we first gathered lexical translation probabilities of the form P(s | t) by running five iterations of Model 1 on the training corpus.
84:189	This allows for computing the probability of a sequence of source words S given a sequence of target words T as the sum over all possible alignments of the Model 1 probabilities: () ( ) ()    = = TtSs A tsP TASPTSP | |,| (Brown et al.85:189	(1993) provides a more detailed derivation of this identity).
86:189	Although simple, this approach has proven effective in SMT for several reasons.
87:189	First and foremost, phrasal scoring by Model 1 avoids the sparsity problems associated with estimating each phrasal replacement probability with MLE (Vogel et al. 2003).
88:189	Secondly, it appears to boost translation quality in more sophisticated translation systems by inducing lexical triggering (Och et al. 2004).
89:189	Collocations and other non-compositional phrases receive a higher probability as a whole than they would as independent single word replacements.
90:189	One further simplification was made.
91:189	Given that our domain is restricted to the generation of monolingual paraphrase, interesting output can be produced without tackling the difficult problem of inter-phrase reordering.
92:189	7 Therefore, along the lines of Tillmann et al.93:189	(1997), we rely on only monotone phrasal alignments, although we do allow intra-phrasal reordering.
94:189	While this means certain common structural alternations (e.g. , active/passive) cannot be generated, we are still able to express a broad range of phenomena: pings to be both unwieldy in practice and very often indicative of poor a word alignment.
95:189	7 Even in the realm of MT, such an assumption can produce competitive results (Vogel et al. 2003).
96:189	In addition, we were hesitant to incur the exponential increase in running time associated with those movement models in the tradition of Brown el al (1993), especially since these offset models fail to capture important linguistic generalizations (e.g. , phrasal coherence, headedness).
97:189	 Synonymy: injured  wounded  Phrasal replacements: Bush administration  White House  Intra-phrasal reorderings: margin of error  error margin Our channel model, then, is determined solely by the phrasal replacements involved.
98:189	We first assume a monotone decomposition of the sentence pair into phrase pairs (considering all phrasal decompositions equally likely), and the probability P(S | T) is then defined as the product of the each phrasal replacement probability.
99:189	The target language model was a trigram model using interpolated Kneser-Ney smoothing (Kneser & Ney 1995), trained over all 1.4 million sentences (24 million words) in our news corpus.
100:189	3.4 Generating paraphrases To generate paraphrases of a given input, a standard SMT decoding approach was used; this is described in more detail below.
101:189	Prior to decoding, however, the input sentence underwent preprocessing: text was lowercased, tokenized, and a few classes of named-entities were identified using regular expressions.
102:189	To begin the decoding process, we first constructed a lattice of all possible paraphrases of the source sentence based on our phrasal translation database.
103:189	Figure 2 presents an example.
104:189	The lattice was realized as a set of |S| + 1 vertices v 0 v |S| and a set of edges between those vertices; each edge was labeled with a sequence of words and a real number.
105:189	Thus a edge connecting vertex v i to v j labeled with the sequence of words w 1 w k and the real number p indicates that the source words s i+1 to s j can be replaced by words w 1 w k with probability p. Our replacement database was stored as a trie with words as edges, hence populating the lattice takes worst case O(n 2 ) time.
106:189	Finally, since source and target languages are identical, we added an identity mapping for each source word s i : an edge from v i-1 to v i with label s i and a uniform probability u. This allows for handling unseen words.
107:189	A high u value permits more conservative paraphrases.
108:189	We found the optimal path through the lattice as scored by the product of the replacement model and the trigram language model.
109:189	This algorithm reduces easily to the Viterbi algorithm; such a dynamic programming approach guarantees an efficient optimal search (worst case O(kn), where n is the maximal target length and k is the maximal number of replacements for any word).
110:189	In addition, fast algorithms exist for computing the n-best lists over a lattice (Soong & Huang 1991).
111:189	Finally the resultant paraphrases were cleaned up in a post-processing phase to ensure output was not trivially distinguishable from other systems during human evaluation.
112:189	All generic named entity tokens were re-instantiated with their source values, and case was restored using a model like that used in Vita et al.113:189	(2003).
114:189	3.5 Alternate approaches Barzilay & Lee (2003) have released a common dataset that provides a basis for comparing different paraphrase generation systems.
115:189	It consists of 59 sentences regarding acts of violence in the Middle East.
116:189	These are accompanied by paraphrases generated by their Multi-Sequence Alignment (MSA) system and a baseline employing WordNet (Fellbaum 1998), along with human judgments for each output by 2-3 raters.
117:189	The MSA WordNet baseline was created by selecting a subset of the words in each test sentenceproportional to the number of words replaced by MSA in the same sentenceand replacing each with an arbitrary word from its most frequent WordNet synset.
118:189	Since our SMT approach depends quite heavily on a target language model, we presented an alternate WordNet baseline using a target language model.
119:189	8 In combination with the language model described in section 3.4, we used a very simple replacement model: each appropriately inflected member of the most frequent synset was proposed as a possible replacement with uniform probability.
120:189	This was intended to isolate the contribution of the language model from the replacement model.
121:189	Given that our alignments, while aggregated into phrases, are fundamentally word-aligned, one question that arises is whether the information we learn is different in character than that learned 8 In contrast, Barzilay and Lee (2003) avoided using a language model for essentially the same reason: their MSA approach did not take advantage of such a resource.
122:189	from much simpler techniques.
123:189	To explore this hypothesis, we introduced an additional baseline that used statistical clustering to produce an automated, unsupervised synonym list, again with a trigram language model.
124:189	We used standard bigram clustering techniques (Goodman 2002) to produce 4,096 clusters of our 65,225 vocabulary items.
125:189	4 Evaluation We have experimented with several methods for extracting a parallel sentence-aligned corpus from news clusters using word alignment error rate, or AER, (Och & Ney 2003) as an evaluation metric.
126:189	A brief summary of these experiments is provided in Table 1.
127:189	To evaluate the quality of generation, we followed the lead of Barzilay & Lee (2003).
128:189	We started with the 59 sentences and corresponding paraphrases from MSA and WordNet (designated as WN below).
129:189	Since the size of this data set made it difficult to obtain statistically significant results, we also included 141 randomly selected sentences from held-out clusters.
130:189	We then produced paraphrases with each of the following systems and compared them with MSA and WN:  WN+LM: WordNet with a trigram LM  CL: Statistical clusters with a trigram LM  PR: The top 5 sentence rewrites produced by Phrasal Replacement.
131:189	For the sake of consistency, we did not use the judgments provided by Barzilay and Lee; instead we had two raters judge whether the output from each system was a paraphrase of the input sentence.
132:189	The raters were presented with an input sentence and an output paraphrase from each system in random order to prevent bias toward any particular judgment.
133:189	Since, on our first pass, we found inter-rater agreement to be somewhat low (84%), we asked the raters to make a second pass of judgments on those where they disagreed; this significantly improved agreement (96.9%).
134:189	The results of this final evaluation are summarized in Table 2.
135:189	Figure 2.
136:189	A simplified generation lattice: 44 top ranked edges from a total 4,140 5 Analysis Table 2 shows that PR can produce rewordings that are evaluated as plausible paraphrases more frequently than those generated by either baseline techniques or MSA.
137:189	The WordNet baseline performs quite poorly, even in combination with a trigram language model: the language model does not contribute significantly to resolving lexical selection.
138:189	The performance of CL is likewise abysmalagain a language model does nothing to help.
139:189	The poor performance of these synonymbased techniques indicates that they have little value except as a baseline.
140:189	The PR model generates plausible paraphrases for the overwhelming majority of test sentences, indicating that even the relatively high AER for non-identical words is not an obstacle to successful generation.
141:189	Moreover, PR was able to generate a paraphrase for all 200 sentences (including the 59 MSA examples).
142:189	The correlation between acceptability and PR sentence rank validates both the ranking algorithm and the evaluation methodology.
143:189	In Table 2, the PR model scores significantly better than MSA in terms of the percentage of paraphrase candidates accepted by raters.
144:189	Moreover, PR generates at least five (and often hundreds more) distinct paraphrases for each test sentence.
145:189	Such perfect coverage on this dataset is perhaps fortuitous, but is nonetheless indicative of scalability.
146:189	By contrast Barzilay & Lee (2003) report being able to generate paraphrases for only 59 out of 484 sentences in their training (test)?
147:189	set, a total of 12%.
148:189	One potential concern is that PR paraphrases usually involve simple substitutions of words and short phrases (a mean edit distance of 2.9 on the top ranked sentences), whereas MSA outputs more complex paraphrases (reflected in a mean edit distance of 25.8).
149:189	This is reflected in Table 3, which provides a breakdown of four dimensions of interest, as provided by one of our independent evaluators.
150:189	Some 47% of MSA paraphrases involve significant reordering, such as an active-passive alternation, whereas the monotone PR decoder precludes anything other than minor transpositions within phrasal replacements.
151:189	Should these facts be interpreted to mean that MSA, with its more dramatic rewrites, is ultimately more ambitious than PR?
152:189	We believe that the opposite is true.
153:189	A close look at MSA suggests that it is similar in spirit to example-based machine translation techniques that rely on pairing entire sentences in source and target languages, with the translation step limited to local adjustments of the target sentence (e.g. Sumita 2001).
154:189	When an input sentence closely matches a template, results can be stunning.
155:189	However, MSA achieves its richness of substitution at the cost of generality.
156:189	Inspection reveals that 15 of the 59 MSA paraphrases, or 25.4%, are based on a single high-frequency, domain-specific template (essentially a running tally of deaths in the Israeli-Palestinian conflict).
157:189	Unless one is prepared to assume that similar templates can be found for most sentence types, scalability and domain extensibility appear beyond the reach of MSA.
158:189	In addition, since MSA templates pair entire sentences, the technique can produce semantically different output when there is a mismatch in information content among template training sentences.
159:189	Consider the third and fourth rows of Table 3, which indicate the extent of embellishment and lossiness found in MSA paraphrases and the topranked PR paraphrases.
160:189	Particularly noteworthy is the lossiness of MSA seen in row 4.
161:189	Figure 3 illustrates a case where the MSA paraphrase yields a significant reduction in information, while PR is more conservative in its replacements.
162:189	While the substitutions obtained by the PR model remain for the present relatively modest, they are not trivial.
163:189	Changing a single content word is a legitimate form of paraphrase, and the ability to paraphrase across an arbitrarily large sentence set and arbitrary domains is a desideratum of paraphrase research.
164:189	We have demonstrated that the SMT-motivated PR method is capable of generating acceptable paraphrases for the overwhelming majority of sentences in a broad domain.
165:189	Method B&L59 B&L59 + 141 PR #1 54 / 59 = 91.5% 177 / 200 = 89.5% PR #2 53 / 59 = 89.8% 168 / 200 = 84.0% PR #3 46 / 59 = 78.0% 164 / 200 = 82.0% PR #4 49 / 59 = 83.1% 163 / 200 = 81.5% MSA 46 / 59 = 78.0% 46 / 59 = 78.0% PR #5 44 / 59 = 74.6% 155 / 200 = 77.5% WN 23 / 59 = 39.0% 25 / 59 = 37.9% WN+LM 30 / 59 = 50.9% 53 / 200 = 27.5% CL 14 / 59 = 23.7% 26 / 200 = 13.0% Table 2.
166:189	Human acceptability judgments MSA PR#1 Rearrangement 28 / 59 = 47% 0 / 100 = 0% Phrasal alternation 11 / 59 = 19% 3 / 100 = 3% Info added 19 / 59 = 32% 6 / 100 = 6% Info lost 43 / 59 = 73% 31 / 100 = 31% Table 3.
167:189	Qualitative analysis of paraphrases 6 Future work Much work obviously remains to be done.
168:189	Our results remain constrained by data sparsity, despite the large initial training sets.
169:189	One major agenda item therefore will be acquisition of larger (and more diverse) data sets.
170:189	In addition to obtaining greater absolute quantities of data in the form of clustered articles, we also seek to extract aligned sentence pairs that instantiate a richer set of phenomena.
171:189	Relying on edit distance to identify likely paraphrases has the unfortunate result of excluding interesting sentence pairs that are similar in meaning though different in form.
172:189	For example: The Cassini spacecraft, which is en route to Saturn, is about to make a close pass of the ringed planet's mysterious moon Phoebe On its way to an extended mission at Saturn, the Cassini probe on Friday makes its closest rendezvous with Saturn's dark moon Phoebe.
173:189	We are currently experimenting with data extracted from the first two sentences in each article, which by journalistic convention tend to summarize content (Dolan et al. 2004).
174:189	While noisier than the edit distance data, initial results suggest that these can be a rich source of information about larger phrasal substitutions and syntactic reordering.
175:189	Although we have not attempted to address the issue of paraphrase identification here, we are currently exploring machine learning techniques, based in part on features of document structure and other linguistic features that should allow us to bootstrap initial alignments to develop more data.
176:189	This will we hope, eventually allow us to address such issues as paraphrase identification for IR.
177:189	To exploit richer data sets, we will also seek to address the monotone limitation of our decoder that further limits the complexity of our paraphrase output.
178:189	We will be experimenting with more sophisticated decoder models designed to handle reordering and mappings to discontinuous elements.
179:189	We also plan to pursue better (automated) metrics for paraphrase evaluation.
180:189	7 Conclusions We presented a novel approach to the problem of generating sentence-level paraphrases in a broad semantic domain.
181:189	We accomplished this by using methods from the field of SMT, which is oriented toward learning and generating exactly the sorts of alternations encountered in monolingual paraphrase.
182:189	We showed that this approach can be used to generate paraphrases that are preferred by humans to sentence-level paraphrases produced by other techniques.
183:189	While the alternations our system produces are currently limited in character, the field of SMT offers a host of possible enhancementsincluding reordering modelsaffording a natural path for future improvements.
184:189	A second important contribution of this work is a method for building and tracking the quality of large, alignable monolingual corpora from structured news data on the Web.
185:189	In the past, the lack of such a data source has hampered paraphrase research; our approach removes this obstacle.
186:189	Acknowledgements We are grateful to Mo Corston-Oliver, Jeff Stevenson, Amy Muia, and Orin Hargraves of the Butler Hill Group for their work in annotating the data used in the experiments.
187:189	This paper has also benefited from discussions with Ken Church, Mark Johnson, and Steve Richardson.
188:189	We greatly appreciate the careful comments of three anonymous reviewers.
189:189	We remain, however, solely responsible for this content.

