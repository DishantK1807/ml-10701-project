<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Breck Baldwin</author>
<author>Bob Carpenter</author>
</authors>
<date>2003</date>
<note>Lingpipe. http://www.alias-i.com/lingpipe/index.html</note>
<contexts>
<context>ific language data are provided. To apply data and text mining algorithms to natural language processing and related product development, LingPipe offers information extraction and data mining tools (Baldwin and Carpenter, 2003). In order to combine such tools, GATE (Cunningham et al., 2002) provides infrastructure tools like IBM’s Unstructured Information Management Architecture (UIMA). The original design principles behin</context>
</contexts>
<marker>Baldwin, Carpenter, 2003</marker>
<rawString>Breck Baldwin and Bob Carpenter. 2003. Lingpipe. http://www.alias-i.com/lingpipe/index.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Biemann</author>
<author>S Teresniak</author>
</authors>
<title>Disentangling from babylonian confusion – unsupervised language identification</title>
<date>2005</date>
<booktitle>In Proceedings of CICLing-2005, Computational Linguistics and Intelligent Text Processing, Mexico City, Mexico and Springer LNCS</booktitle>
<pages>3406</pages>
<contexts>
<context>omplexity in the number of edges, which is especially beneficial for sparse graphs, i.e. where the number of egdes is far below the possible number of edges. It has been used for language separation (Biemann and Teresniak, 2005), unsupervised POS tagging (Biemann, 2006b) and word sense induction (Biemann, 2006a). CW is a clustering algorithm that clusters undirected, weighted graphs. The output is a nonhierarchical fuzzy pa</context>
<context>lso be visualized in an interactive manner, where for instances subgraphs can be chosen and extracted for a detailed view. See Figure 2 for the German subgraph from a seven languages separation task (Biemann and Teresniak, 2005). 3.2. Multi-purpose Word Classifier: Pretree Tool This implementation of Compact Patricia Tree (CPT, pretree) classifiers proved to be useful for morphology-related tasks in e.g. (Witschel and Biema</context>
<context>nce Level This state-of-the-art word-based language identification program allows identifying the language at sentence level using frequency word lists of the languages which are to be distinguished (Biemann and Teresniak, 2005). It can be used to identify chunks of foreign language in a corpus. At the moment, 25 languages are supported; the number of languages is easily extensible by providing frequency lists either from a</context>
</contexts>
<marker>Biemann, Teresniak, 2005</marker>
<rawString>C. Biemann and S. Teresniak. 2005. Disentangling from babylonian confusion – unsupervised language identification. In Proceedings of CICLing-2005, Computational Linguistics and Intelligent Text Processing, Mexico City, Mexico and Springer LNCS 3406.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Biemann</author>
</authors>
<title>Chinese whispers – an efficient graph clustering algorithm and its application to natural language processing problems</title>
<date>2006</date>
<booktitle>In Proceedings of the HLTNAACL-06 Workshop on Textgraphs</booktitle>
<contexts>
<context>ons are cited that describe the corresponding algorithm in more detail. 3. General Purpose Tools 3.1. Chinese Whispers Multi-purpose Graph Clustering Chinese Whispers is a graph clustering algorithm (Biemann, 2006a) which has linear time complexity in the number of edges, which is especially beneficial for sparse graphs, i.e. where the number of egdes is far below the possible number of edges. It has been used</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>C. Biemann. 2006a. Chinese whispers – an efficient graph clustering algorithm and its application to natural language processing problems. In Proceedings of the HLTNAACL-06 Workshop on Textgraphs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Biemann</author>
</authors>
<title>Unsupervised part-of-speech tagging employing efficient graph clustering</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL-06 Student Research Workshop 2006</booktitle>
<location>Sydney, Australia</location>
<contexts>
<context>ons are cited that describe the corresponding algorithm in more detail. 3. General Purpose Tools 3.1. Chinese Whispers Multi-purpose Graph Clustering Chinese Whispers is a graph clustering algorithm (Biemann, 2006a) which has linear time complexity in the number of edges, which is especially beneficial for sparse graphs, i.e. where the number of egdes is far below the possible number of edges. It has been used</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>C. Biemann. 2006b. Unsupervised part-of-speech tagging employing efficient graph clustering. In Proceedings of the COLING/ACL-06 Student Research Workshop 2006, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Edward Loper</author>
</authors>
<title>Nltk: The natural language toolkit</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (Demonstration Track</booktitle>
<pages>214--217</pages>
<contexts>
<context> modular construction of language processing systems and to make it easy to swap components in and out. Another collection of natural language processing tools is the Natural Language Toolkit (NLTK) (Bird and Loper, 2004). It provides several modules all written in Python for performing different kinds of NLP tasks – namely tokenization, tagging, parsing and formal semantic analysis – and comes with corpus data, too.</context>
</contexts>
<marker>Bird, Loper, 2004</marker>
<rawString>Steven Bird and Edward Loper. 2004. Nltk: The natural language toolkit. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (Demonstration Track), pages 214–217, http://nltk.sourceforge.net/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bordag</author>
</authors>
<title>Two-step approach to unsupervised morpheme segmentation</title>
<date>2006</date>
<booktitle>In Proceedings of the Unsupervised segmentation of words into morphemes Challenge</booktitle>
<location>Venice, Italy</location>
<contexts>
<context>lti-purpose Word Classifier: Pretree Tool This implementation of Compact Patricia Tree (CPT, pretree) classifiers proved to be useful for morphology-related tasks in e.g. (Witschel and Biemann, 2005; Bordag, 2006) and is used by various other tools in ASV Toolbox. The tool provides possibilities to train and evaluate classifiers that use beginnings or endings of strings as features. An important property of t</context>
</contexts>
<marker>Bordag, 2006</marker>
<rawString>S. Bordag. 2006. Two-step approach to unsupervised morpheme segmentation. In Proceedings of the Unsupervised segmentation of words into morphemes Challenge 2005, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bordag</author>
</authors>
<title>Unsupervised and knowledgefree morpheme segmentation and analysis</title>
<date>2007</date>
<booktitle>In Proceedings of the Working Notes for the CLEF Workshop</booktitle>
<location>Budapest, Hungary</location>
<contexts>
<context>es F-values in the high 90% range. CPTs have sucessfullt applied to other tasks including name gender classifications or as a generalization module in an unsupervised morpheme segmentation algorithm (Bordag, 2007). Unsupervised approaches of learning compound splits are described in (Larson et al., 2000; Monz and de Rijke, 2002; Holz and Biemann, 2008). In (Holz and Biemann, 2008) CPTs are built up for splitt</context>
</contexts>
<marker>Bordag, 2007</marker>
<rawString>S. Bordag. 2007. Unsupervised and knowledgefree morpheme segmentation and analysis. In Proceedings of the Working Notes for the CLEF Workshop 2007, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bourigault</author>
</authors>
<title>Surface grammatical analysis for the extraction of terminological noun phrases</title>
<date>1992</date>
<booktitle>In Proceedings of Coling92</booktitle>
<pages>977--981</pages>
<contexts>
<context>rns. Currently, it is available for English, Finnish and German. There are several approaches to extract index terms from documents, e.g. statistical (Salton et al., 1975; Cohen, 1995), linguistical (Bourigault, 1992) and hybrid (Daille et al., 1994). One important statistical method is the so-called differential analysis which measures the extent to which the frequency of a word w in the given text deviates from</context>
</contexts>
<marker>Bourigault, 1992</marker>
<rawString>D. Bourigault. 1992. Surface grammatical analysis for the extraction of terminological noun phrases. In Proceedings of Coling92, pages 977–981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Braschler</author>
<author>B Ripplinger</author>
</authors>
<title>Stemming and decompounding for german text retrieval</title>
<date>2003</date>
<booktitle>In Proceedings of ECIR, LLNCS 2633</booktitle>
<pages>177--192</pages>
<contexts>
<context>sults in sparse data, challenging a number of NLP applications. For IR experiments with German, Braschler et al. report that decompounding results in higher text retrieval improvements than stemming (Braschler and Ripplinger, 2003). Most string classification problems can be solved by using a Compact Patricia Tree (CPT, see Sect. 3.2.). Accordingly, this module, designed to perform baseform reduction and compound splitting, co</context>
</contexts>
<marker>Braschler, Ripplinger, 2003</marker>
<rawString>M. Braschler and B. Ripplinger. 2003. Stemming and decompounding for german text retrieval. In Proceedings of ECIR, LLNCS 2633, pages 177–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Cohen</author>
</authors>
<title>Highlights: language and domain independent automatic indexing terms for abstracting</title>
<date>1995</date>
<journal>Journal of the American Society for Information Science</journal>
<volume>46</volume>
<contexts>
<context>orpus and given POStag patterns. Currently, it is available for English, Finnish and German. There are several approaches to extract index terms from documents, e.g. statistical (Salton et al., 1975; Cohen, 1995), linguistical (Bourigault, 1992) and hybrid (Daille et al., 1994). One important statistical method is the so-called differential analysis which measures the extent to which the frequency of a word </context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>J.D. Cohen. 1995. Highlights: language and domain independent automatic indexing terms for abstracting. Journal of the American Society for Information Science, 46(3):162–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cunningham</author>
<author>D Maynard</author>
<author>K Bontcheva</author>
<author>V Tablan</author>
</authors>
<title>GATE: A framework and graphical development environment for robust NLP tools and applications</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics</booktitle>
<location>http://gate.ac.uk/documentation.html</location>
<contexts>
<context>ithms to natural language processing and related product development, LingPipe offers information extraction and data mining tools (Baldwin and Carpenter, 2003). In order to combine such tools, GATE (Cunningham et al., 2002) provides infrastructure tools like IBM’s Unstructured Information Management Architecture (UIMA). The original design principles behind the system have been to avoid tying developers into any partic</context>
</contexts>
<marker>Cunningham, Maynard, Bontcheva, Tablan, 2002</marker>
<rawString>H. Cunningham, D. Maynard, K. Bontcheva, and V. Tablan. 2002. GATE: A framework and graphical development environment for robust NLP tools and applications. In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics, http://gate.ac.uk/documentation.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Daille</author>
<author>E Gaussier</author>
<author>J Lang´e</author>
</authors>
<title>Towards automatic extraction of monolingual and bilingual terminology</title>
<date>1994</date>
<booktitle>In Proceedings of Coling94</booktitle>
<pages>515--521</pages>
<marker>Daille, Gaussier, Lang´e, 1994</marker>
<rawString>B. Daille, E. Gaussier, and J. Lang´e. 1994. Towards automatic extraction of monolingual and bilingual terminology. In Proceedings of Coling94, pages 515–521.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramon Ferrer i Cancho</author>
<author>Ricard V Sole</author>
</authors>
<title>The small world of human language</title>
<date>2001</date>
<booktitle>In Proceedings of The Royal Society of London. Series B, Biological Sciences</booktitle>
<pages>268--1482</pages>
<contexts>
<context>e algorithm can partition arbitrary undirected, weighted graphs of any sizes. Best results are obtained on graphs with the recently described small world structure (Watts and Strogatz, 1998; Ferrer i Cancho and Sole, 2001; Steyvers and Tenenbaum, 2005). The implementation of this algorithm is suitable to be applied on very large data sets (graphs with several millions nodes were tested successfully). For smaller data </context>
</contexts>
<marker>Cancho, Sole, 2001</marker>
<rawString>Ramon Ferrer i Cancho and Ricard V. Sole. 2001. The small world of human language. In Proceedings of The Royal Society of London. Series B, Biological Sciences., pages 268(1482): 2261–2265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard Heyer</author>
<author>Uwe Quasthoff</author>
<author>Thomas Wittig</author>
</authors>
<date>2006</date>
<booktitle>Text Mining: Wissensrohstoff Text – Konzepte, Algorithmen, Ergebnisse. W3L-Verlag</booktitle>
<contexts>
<context>mats. The algorithms it comprises are based on publications by researchers at the Natural Language Processing Division (ASV) at the Computer Science Department of the University of Leipzig (cf. also (Heyer et al., 2006)). The modular software design is kept open for adding new algorithms. Its main use at present is to share processing know-how and to support the teaching of natural language processing, helping stud</context>
<context>. Hierarchical Agglomerative Clustering This basic implementation of hierarchical agglomerative clustering allows clustering elements represented as vectors using various norms and distance measures (Heyer et al., 2006). As example configuration, words can be clustered by their common significant co-occurrences (available from LCC in 15 languages). The result can be exported as XML file or in dendrogram picture for</context>
</contexts>
<marker>Heyer, Quasthoff, Wittig, 2006</marker>
<rawString>Gerhard Heyer, Uwe Quasthoff, and Thomas Wittig. 2006. Text Mining: Wissensrohstoff Text – Konzepte, Algorithmen, Ergebnisse. W3L-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Holz</author>
<author>C Biemann</author>
</authors>
<title>Unsupervised and knowledge-free learning of compound splits and periphrases</title>
<date>2008</date>
<booktitle>Proceedings of the CICLing 2008, LNCS 4919</booktitle>
<pages>117--127</pages>
<editor>In A. Gelbukh, editor</editor>
<publisher>Springer</publisher>
<contexts>
<context>alization module in an unsupervised morpheme segmentation algorithm (Bordag, 2007). Unsupervised approaches of learning compound splits are described in (Larson et al., 2000; Monz and de Rijke, 2002; Holz and Biemann, 2008). In (Holz and Biemann, 2008) CPTs are built up for splitting and for periphrasing compounds. To facilitate experimentation with training or unclassified data, the GUI is split into two parts: base f</context>
</contexts>
<marker>Holz, Biemann, 2008</marker>
<rawString>F. Holz and C. Biemann. 2008. Unsupervised and knowledge-free learning of compound splits and periphrases. In A. Gelbukh, editor, Proceedings of the CICLing 2008, LNCS 4919, pages 117–127. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitar Kazakov</author>
</authors>
<title>Unsupervised learning of na¨ıve morphology with genetic algorithms</title>
<date>1997</date>
<booktitle>Workshop Notes of the ECML/MLnet Workshop on Empirical Learning of Natural Language Processing Tasks</booktitle>
<pages>105--112</pages>
<editor>In A. van den Bosch, W. Daelemans, and A. Weijters, editors</editor>
<location>Prague, Czech Republic</location>
<contexts>
<context>words for English Figure 9: The Genetomorph panel 3.5. Genetic Morphology Analysis: Genetomorph This tool provides a genetic algorithm that is able to detect morphological regularities in word lists (Kazakov, 1997; Kazakov, 2001). A fitness function that minimizes the cost of describing morphological rules is optimized, individual solutions can be browsed and the progress until convergence is visualized in a p</context>
</contexts>
<marker>Kazakov, 1997</marker>
<rawString>Dimitar Kazakov. 1997. Unsupervised learning of na¨ıve morphology with genetic algorithms. In A. van den Bosch, W. Daelemans, and A. Weijters, editors, Workshop Notes of the ECML/MLnet Workshop on Empirical Learning of Natural Language Processing Tasks, pages 105–112, Prague, Czech Republic, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitar Kazakov</author>
</authors>
<title>Unsupervised learning of word segmentation rules with genetic algorithms and inductive logic programming</title>
<date>2001</date>
<booktitle>Machine Learning, 43:121– 162, April-May</booktitle>
<contexts>
<context>sh Figure 9: The Genetomorph panel 3.5. Genetic Morphology Analysis: Genetomorph This tool provides a genetic algorithm that is able to detect morphological regularities in word lists (Kazakov, 1997; Kazakov, 2001). A fitness function that minimizes the cost of describing morphological rules is optimized, individual solutions can be browsed and the progress until convergence is visualized in a plot. Sample dat</context>
</contexts>
<marker>Kazakov, 2001</marker>
<rawString>Dimitar Kazakov. 2001. Unsupervised learning of word segmentation rules with genetic algorithms and inductive logic programming. Machine Learning, 43:121– 162, April-May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Larson</author>
<author>D Willett</author>
<author>J K¨ohler</author>
</authors>
<title>Compound splitting and lexical unit recombination for improved performance of a speech recognition system for german parliamentary speeches</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th International Conference on Spoken Language Processing (ICSLP</booktitle>
<marker>Larson, Willett, K¨ohler, 2000</marker>
<rawString>M. Larson, D. Willett, J. K¨ohler, , and G. Rigoll. 2000. Compound splitting and lexical unit recombination for improved performance of a speech recognition system for german parliamentary speeches. In Proceedings of the 6th International Conference on Spoken Language Processing (ICSLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals</title>
<date>1965</date>
<booktitle>Doklady Akademii Nauk SSSR</booktitle>
<pages>163--4</pages>
<contexts>
<context>Levenshtein Distance Based on a Directed Acyclic Word Graph (DAWG) implementation, this tool allows efficient basic spell checking by offering words from a given word list with Levenshtein distances (Levenshtein, 1965). As resources, we provide the top frequent 50.000 words for 15 languages. Building DAWGs from custom word lists is possible. For example, the Italian wordlist returns for the misspelled input word “</context>
</contexts>
<marker>Levenshtein, 1965</marker>
<rawString>Vladimir I. Levenshtein. 1965. Binary codes capable of correcting deletions, insertions, and reversals. Doklady Akademii Nauk SSSR, 163(4):845–848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monz</author>
<author>M de Rijke</author>
</authors>
<title>Shallow morphological analysis in monolingual information retrieval for dutch, german, and italian</title>
<date>2002</date>
<booktitle>In CLEF 2001: Revised Papers from the Second Workshop of the Cross-Language Evaluation Forum on Evaluation of Cross-Language Information Retrieval Systems</booktitle>
<pages>262--277</pages>
<marker>Monz, de Rijke, 2002</marker>
<rawString>Ch. Monz and M. de Rijke. 2002. Shallow morphological analysis in monolingual information retrieval for dutch, german, and italian. In CLEF 2001: Revised Papers from the Second Workshop of the Cross-Language Evaluation Forum on Evaluation of Cross-Language Information Retrieval Systems, pages 262–277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Quasthoff</author>
<author>C Biemann</author>
<author>C Wolff</author>
</authors>
<title>Named entity learning and verification: Expectation maximisation in large corpora</title>
<date>2002</date>
<booktitle>In Proceedings of CoNNL-2002</booktitle>
<location>Taipei, Taiwan</location>
<contexts>
<context>ing. In the case of person names, this search-and verification methodology is able to extract e.g. some 40,000 names starting from a list of 20 with high precision from large plain text corpora, see (Quasthoff et al., 2002). For using the tool a database which contains the following three tables is needed: a table with words, a table with sentences and table which connect the words and sentences tables. The bootstrappi</context>
</contexts>
<marker>Quasthoff, Biemann, Wolff, 2002</marker>
<rawString>U. Quasthoff, C. Biemann, and C. Wolff. 2002. Named entity learning and verification: Expectation maximisation in large corpora. In Proceedings of CoNNL-2002, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Quasthoff</author>
<author>M Richter</author>
<author>C Biemann</author>
</authors>
<title>Corpus portal for search in monolingual corpora</title>
<date>2006</date>
<booktitle>In Proceedings of the LREC</booktitle>
<contexts>
<context>data and features as well as adding new algorithms. At present, the toolbox is set up to use data of the Leipzig Corpora Collection (LCC), a collection of text corpora based on a set size and format (Quasthoff et al., 2006), but can be easily adapted to other data formats. The algorithms it comprises are based on publications by researchers at the Natural Language Processing Division (ASV) at the Computer Science Depar</context>
</contexts>
<marker>Quasthoff, Richter, Biemann, 2006</marker>
<rawString>U. Quasthoff, M. Richter, and C. Biemann. 2006. Corpus portal for search in monolingual corpora. In Proceedings of the LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden markov models and selected applications in speech recognition</title>
<date>1989</date>
<booktitle>In Proceedings of the IEEE</booktitle>
<volume>77</volume>
<pages>257--286</pages>
<contexts>
<context>meters to be specified. 4.7. POS-tagging with a ViterbiTagger This simple tagger implementation is based on tag trigrams and tag distributions for words. Not as powerful as a full HMM implementation (Rabiner, 1989), it also uses the viterbi algorithm (Viterbi, 1967) and comes with a morphological back-off component (realized with Pretree, see Sect. 3.2.) and is capable of training tagger models on very large a</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence R. Rabiner. 1989. A tutorial on hidden markov models and selected applications in speech recognition. In Proceedings of the IEEE, volume 77 (2), pages 257– 286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A vector space model for automatic indexing</title>
<date>1975</date>
<journal>Communications of the ACM</journal>
<volume>18</volume>
<contexts>
<context>hem to a background corpus and given POStag patterns. Currently, it is available for English, Finnish and German. There are several approaches to extract index terms from documents, e.g. statistical (Salton et al., 1975; Cohen, 1995), linguistical (Bourigault, 1992) and hybrid (Daille et al., 1994). One important statistical method is the so-called differential analysis which measures the extent to which the frequen</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, and C.S. Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18(11):613–620.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Josh Tenenbaum</author>
</authors>
<title>The large scale structure of semantic networks: Statistical analyses and a model of semantic growth</title>
<date>2005</date>
<journal>Cognitive Science</journal>
<volume>29</volume>
<pages>78</pages>
<contexts>
<context>on arbitrary undirected, weighted graphs of any sizes. Best results are obtained on graphs with the recently described small world structure (Watts and Strogatz, 1998; Ferrer i Cancho and Sole, 2001; Steyvers and Tenenbaum, 2005). The implementation of this algorithm is suitable to be applied on very large data sets (graphs with several millions nodes were tested successfully). For smaller data sets a force-directed graph la</context>
</contexts>
<marker>Steyvers, Tenenbaum, 2005</marker>
<rawString>Mark Steyvers and Josh Tenenbaum. 2005. The large scale structure of semantic networks: Statistical analyses and a model of semantic growth. Cognitive Science, 29(1):41– 78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew J Viterbi</author>
</authors>
<title>Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory</journal>
<volume>13</volume>
<contexts>
<context>rbiTagger This simple tagger implementation is based on tag trigrams and tag distributions for words. Not as powerful as a full HMM implementation (Rabiner, 1989), it also uses the viterbi algorithm (Viterbi, 1967) and comes with a morphological back-off component (realized with Pretree, see Sect. 3.2.) and is capable of training tagger models on very large annotated texts in various formats. Further, it allow</context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>Andrew J. Viterbi. 1967. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Transactions on Information Theory, 13(2):260–269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Watts</author>
<author>S H Strogatz</author>
</authors>
<title>Collective dynamics of ’small-world’ networks</title>
<date>1998</date>
<journal>Nature</journal>
<pages>393--440</pages>
<contexts>
<context>n is not bound to language data; the algorithm can partition arbitrary undirected, weighted graphs of any sizes. Best results are obtained on graphs with the recently described small world structure (Watts and Strogatz, 1998; Ferrer i Cancho and Sole, 2001; Steyvers and Tenenbaum, 2005). The implementation of this algorithm is suitable to be applied on very large data sets (graphs with several millions nodes were tested </context>
</contexts>
<marker>Watts, Strogatz, 1998</marker>
<rawString>D. J. Watts and S. H. Strogatz. 1998. Collective dynamics of ’small-world’ networks. Nature, 393:440–442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Witschel</author>
<author>C Biemann</author>
</authors>
<title>Rigorous dimensionality reduction through linguistically motivated feature selection for text categorisation</title>
<date>2005</date>
<booktitle>In Proceedings of NODALIDA</booktitle>
<contexts>
<context>nd Teresniak, 2005). 3.2. Multi-purpose Word Classifier: Pretree Tool This implementation of Compact Patricia Tree (CPT, pretree) classifiers proved to be useful for morphology-related tasks in e.g. (Witschel and Biemann, 2005; Bordag, 2006) and is used by various other tools in ASV Toolbox. The tool provides possibilities to train and evaluate classifiers that use beginnings or endings of strings as features. An important</context>
</contexts>
<marker>Witschel, Biemann, 2005</marker>
<rawString>F. Witschel and C. Biemann. 2005. Rigorous dimensionality reduction through linguistically motivated feature selection for text categorisation. In Proceedings of NODALIDA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Friedrich Witschel</author>
</authors>
<title>Text, W¨orter, Morpheme – M¨oglichkeiten einer automatischen TerminologieExtraktion</title>
<date>2004</date>
<publisher>Ergon Verlag</publisher>
<contexts>
<context>ying a POS-independent base form reducer. Figure 11: The base form reduction panel Figure 12: The compound noun decomposition panel 4.3. Terminology Extraction Comprising the non-interactive part of (Witschel, 2004)’s terminology extraction method, this tool extracts terminologically relevant terms and phrases from documents by comparing them to a background corpus and given POStag patterns. Currently, it is av</context>
<context>). One important statistical method is the so-called differential analysis which measures the extent to which the frequency of a word w in the given text deviates from its frequency in general usage (Witschel, 2004). The latter frequency is determined using a reference corpus, i.e. a large and well-balanced collection of documents in the given language. The termhood of w is quantified using a statistical signif</context>
</contexts>
<marker>Witschel, 2004</marker>
<rawString>Hans Friedrich Witschel. 2004. Text, W¨orter, Morpheme – M¨oglichkeiten einer automatischen TerminologieExtraktion. Ergon Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<title>Weka</title>
<date>2000</date>
<booktitle>Practical Machine Learning: Tools and Techniques with Java Implementations</booktitle>
<publisher>Morgan Kaufmann</publisher>
<location>http://www.cs.waikato.ac.nz/˜ml/weka</location>
<contexts>
<context>alysis, tagging and chunking as well as clustering (http://www.ims.uni-stuttgart.de/tcl/). For the data and text mining algorithms in general, WEKA offers a collection of machine learning algorithms (Witten and Frank, 2000). WEKA contains tools for data preprocessing, classification, regression, clustering, association rules, and visualization. However, apart from very constrained examples, no specific language data ar</context>
</contexts>
<marker>Witten, Frank, 2000</marker>
<rawString>I.H. Witten and E. Frank. 2000. Weka: Practical Machine Learning: Tools and Techniques with Java Implementations. Morgan Kaufmann, http://www.cs.waikato.ac.nz/˜ml/weka/.</rawString>
</citation>
</citationList>
</algorithm>

