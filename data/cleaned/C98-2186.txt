Maximum Entropy Model Learning of the Translation Rules 
Kengo Sato and Masakazu Nakanishi 
Department of Computer Science 
Keio University 
3-14-1, Hiyoshi, Kohoku, Yokohama 223-8522, Japan 
e-mail: {satoken, czl}@nak. ±cs .ke±o. ac. jp 
Abstract 
This paper proposes a learning method of 
translation rules from parallel corpora. This 
method applies the maximum entropy prin
ciple to a probabilistic model of translation 
rules. First, we define feature fimctions 
which express statistical properties of this 
model. Next, in order to optimize the nmdel, 
the system iterates following steps: (1) se
lects a feature function which maximizes log
likelihood, and (2) adds this function to the 
model incrementally. As computational cost 
associated with this model is too expensive, 
we propose several methods to suppress the 
overhead in order to realize the system. The 
result shows that it attained 69.54% recall 
rate. 
1 Introduction

A statistical natural language modeling can 
be viewed as estimating a combinational dis
tribution X x Y -+ \[0, 1\] using training data 
(x~,y,),..., (XT, YT) e X x Y observed in 
corpora. For this topic, Baum (1972) pro
posed EM algorithm, which was basis of 
Forward-Backward algorithm for the hidden 
Markov model (HMM) and Inside-Outside 
algorithm (Lafferty, 1993) for the pr0babilis
tic context free grammar (PCFG). However, 
these methods have problems such as in
creasing optimization costs which is due to 
a lot of parameters. Therefore, estimating a 
natural language model based on the max
imum entropy (ME) method (Pietra et al., 
1995; Berger et al., 1996) has been high
lighted recently. 
On the other hand, dictionaries for multi
lingual natural language processing such as 
the machine translation has been made by 
human hand usually. However, since this 
work requires a great deal of labor and it 
is ditficult to keep description of dictionar
ies consistent, the researches of automatical 
dictionaries making for machine translation 
(translation rules) fi'om corpora become ac
tive recently (Kay and RSschesen, 1993; Kaji 
and Aizono, 1996). 
In this paper, we notice that estimating 
a language model based on ME method is 
suitable for learning the translation rules, 
and propose several methods to resolve, prob
lems in adapting ME method to learning the 
translation rules. 
2 Problem
Setting 
If there exist (xl,yl),..., (xT, y~.) c X × Y 
such that each xi is translated into Yi in 
the parallel corpora X,Y, then its empiri
cal probability distribution/~ obtained fi'om 
observed training data is defined by: 
(1) Ex,yc(x,y) 
where c(x, y) is the number of times that x 
is translated into y in the training data. 
However, since it is difficult to observe 
translating between words actually, c(x, y) is 
approximated with equation (2) for sentence 
aligned parallel corpora. 
c(x, = l ~ xi ~ ~ ~ ~ ---( (2) 
where Xi is i-th sentence in X. We denote 
that sentence Xi is translated into sentence 
I:// in aligned parallel corpora. And c{(x, y) 
1171 
is the number of times that x and y appear 
in the i-th sentence. 
Our task is to learn the translation rules 
by estimating probability distribution p(yIx) 
that x E X is translated into y E Y from 
/5(x, y) given above. 
3 Maximum
Entropy Method 
3.1 Feature
Function 
We define binary-valued indicator function 
f : X x Y --~ {0,1} which divide X x Y 
into two subsets. This is called feature func
tion, which expresses statistical properties of 
a language model. 
The expected value of f with respected to 
~(x, y) is defined such as: 
~(f) = y~f(x,y)f(x,y) (3) 
x,y 
Thus training data are summarized as the 
expected value of feature function f. 
The expected value of a feature function 
f with respected to p(ylx) which we would 
like to estimate is defined such as: 
p(f) = ~-~(x)p(ylx)f(x,y) (4) 
x~y 
where/5(x) is the empirical probability dis
tribution on X. Then, the model which we 
would like to estimate is under constraint to 
satisfy an equation such as: 
p(f) =~(f) (5) 
This is called the constraint equation. 
3.2 Maximum
Entropy Principle 
When there are feature functions fi(i E 
{1, 2,..., n}) which are important to model
ing processes, the distribution p we estimate 
should be included in a set of distributions 
defined such as: 
C = {p e 7 9 I P(fi) =/5(f/) for i G {1,2,...,n}} (6) 
where T' is a set of all possible distributions 
onXxY. 
For the distribution p, there is no assump
tion except equation (6), so it is reason
able that the most uniform distribution is 
the most suitable for the training corpora. 
The conditional entropy defined in equa
tion (7) is used as the mathematical measure 
of the uniformity of a conditional probability 
p(ylx). 
H(p) = ~_,~(x)p(ylx ) logp(ylx ) (7) 
x,y 
That is, the model p. which maximizes the 
entropy H should be selected fi'om C. 
p. = argmax H(p) (8) 
peg 
This heuristic is called the maximum entropy 
principle. 
3.3 Parameter
Estimation 
In simple cases, we can find the solution 
to the equation (8) analytically. Unfortu
nately, there is no analytical solution in gen
eral cases, and we need a numerical algo
rithm to find the solution. 
By applying the Lagrange multiplier to 
equation (7), we can introduce the paramet
ric form of p. 
p~(ylx)Z~(x) exp Aifi(x,y) (9) 
Z:~(x) = ~exp (~i Aifi(x,Y) ) 
Y 
where each ki is the parameter for the fea
ture fi. P~ is known as Gibbs distribution. 
Then, to solve p, C C in equation (8) is 
equivalent to solve A. that maximize the log
likelihood: 
= E (x)log &(x) + Z A f(f ) 
x i 
(10) 
k* = argmax ~(k) 
Such A. can be solved by one of the nu
merical algorithm called the Improved Itera
tire Scaling Algorithm (Berger et al., 1996). 
1. Start with ki = 0 for alli E {1,2,...,n} 
2. Do for each i E {1,2,...,n}: 
1172 
(a) Let AAi be the solution to 
~(x)p(yl x ) fi (x, y) exp (AAif # (x, y) ) = P(fi) 
x~y (11) 
where f#(x,V) = k(z,y) 
(b) Update the value of Ai according to: 
Ai~-Ai+AAi 
3. Go to step 2 if not all the Ai have con
verged 
To solve AAi in the step (2a), the Newton's 
method is applied to equation (11). 
3.4 Feature
Selection 
In general cases, there exist a large collec
tion ~" of candidate features, and because 
of the limit of machine resources, we can
not expect to obtain all /?(f) estimated in 
real-life. However, the Maximum Entropy 
Principle does not explicitly state how to se
lect those particular constraints. We build a 
subset ,5 C 3 c incrementally by iterating to 
adjoin a feature f C 3 c which maximizes log
likelihood of the model to ,5. This algorithm 
is called the Basic Feature Selection (Berger 
et al., 1996). 
1. Start with ,5 = 0 
. Do for each candidate feature f c f: 
Compute the model PSuI using Improve 
Iterative Scaling Algorithm and the 
gain in the log-likelihood from adding 
this feature 
3. Check the termination condition 
4. Select the feature f with maximal gain 
5. Adjoin f to S 
6. Compute Ps using hnprove Iterative Al
gorithm 
7. Go to Step 2 
1173 
4 Maximum
Entropy Model 
Learning of the Translation 
Rules 
The art of modeling with the maximum en
tropy method is to define an informative 
set of computationally feasible feature func
tions. In this section, we define two models 
of feature functions tbr learning the transla
tion rules. 
Model 1: Co-occurrence Information 
The first model is defined with co-occurrence 
information between words appeared in the 
corpus X. 
{ J (~ c W(d,,w)) (12) f~(x,y) 
= 0 (otherwise) 
where W(d,w) is a set of words which ap
peared within d words from w E X (in our 
experiments, d = 5). f~,(x, y)expresses the 
information on w for predicting that :r is 
translated into y (Figure 1). 
..... W ............ X .......... ~-X 
prechctive'~"~ r nsl 1¢ n power" ~/tra ,' at') rule 
............... y ............... ~. y 
Figure 1: co-occurance information 
Model 2: Morphological Information 
The second model is defined with morpho
logical information such as part-of-speech. 
f,,,(x, v) = 
( pos(x) = t ) 
1 and 
POS(y)=s 
0 (otherwise) 
(13) 
where POS(x) is a part-of-speech tag for x. 
ft,u(x, y) expresses tile information on part
of-speech t, s for predicting that x is trans
lated into y (Figure 2). If part-of-speech tag? 
t -~pos 
................ predictive ~k"/x .......... ,'-X 
power = " / 
f~, ./translation rule s-%le 
............... y ............... ~. y 
Figure 2: morphological information 
gers for each language work extremely ac
curate, then these feature functions can be 
generated automatically. 
5 Implementation

Computational cost associated with the 
model described above is too expensive to 
realize the system for learning the transla
tion rules. We propose several methods to 
suppress the overhead. 
An estimated probability p~,(ylx) for a pair 
of (x,y) E X x Y which has not been ob
served as the sample data in the parallel 
corpora X,Y should be kept lower. Ac
cording to equation (9), we can allow to let 
fi(x,y) = 0 (for all i e {1,...,n}) for non
observed (x, y). Therefore, we will accept 
observed (x,y) only instead of all possible 
(x, y) in summation in equation (11), so that p~(ylx) 
can be calculated much more effi
ciently. 
Suppose that a set of (x, y) such that each 
member activates a feature fimction f is de
fined by: 
D(f) = {(x,y) e X × Y\[f(x,y) = 1} (14) 
Shirai et al. (1996) showed that if D(fi) and 
D(fj) were exclusive to each other, that is D(k) N D(fj) 
= 0, then ki and /~j could 
be estimated independently. Therefore, we 
can split a set of candidate feature fimctions 
b e into several exclusive subsets, and calcu
late p~ (y\[x) more efficiently by estimating on 
each subset independently. 
6 Experiments
and Results 
As the training corpora, we used 6,057 pairs 
of sentences included in Kodansya Japanese
English Dictionary, a machine-readable dic
tionary made by the Electrotechnical Lab
oratory. By applying morphological anal
ysis for the corpora, each word was trans
formed to the infinitive form. We excluded 
words which appeared below 3 times or over 
1,000 times from the target of learning. Con
sequently, our target for the experiments 
included 1,375 English words and 1,195 
Japanese words, and we prepared 1,375 fea
ture functions for model 1 and 2,744 for 
model 2 (56 part-of-speech for English and 
49 part-of-speech for Japanese). 
We tried to learn the translation rules 
from English to Japanese. We had two ex
periments: one of model 1 as the set of fea
ture functions, and one of model 1 + 2. For 
each experiment, 500 feature functions were 
selected according to the feature selection 
algorithm described in section 3.4, and we 
calculated p(y\[x) in equation (9), that is, 
the probability that English word x is trans
lated into Japanese word y. For each English 
word, all Japanese word were ordered by es
timated probability p(ylx), and we evaluated 
the recall rates by comparing the dictionary. 
Table 1 shows the recall rates for each ex
periment. The numbers for 15(x,y) are the 
Ta )le \]: rec~ 1st 
/~(x, y) 44.55% 
model 1 41.58% 
model 1 +2 58.29% 
dl rates 3rd 
53.47% 
63.37% 
69.54% 
lOth 
58.42% 
76.24% 
80.13% 
recall rates when the empirical probability 
defined by equation (1) was used instead of 
the estimated probability. It is showed that 
the model 1 + 2 attains higher recall rates 
than the model 1 and 15(x, y). 
Figure 3 shows the log-likelihood for each 
model plotted by the number of feature func
tions in the feature selection algorithm. No
tice that the log-likelihood for the model 1+2 
is always higher than the model 1. 
Thus, the model 1 + 2 is more effective 
than the model 1 for learning the translation 
rules. 
However, the result shows that the recall 
1174 
-8.02 
-9.04 
10. ~ 
.O.O~ 
.0,10 
-9.12 
-9,~4 
• 9.'1 El 
-O, IE 
'modall'-
I I \[ I I \[ I l 1 
50 160 Ir~l ~ ~ 300 350 4(~ 450 5\[~ 
tba +~ ~ I+al~a* 
Figure 3: log-likelihood 
rates of the '1st' for all models are not fa
vorable. We consider that it is the reason 
for this to assume word-to-word translation 
rules implicitly. 
7 Conclusions

We have described an approach to learn the 
translation rules from parallel corpora based 
on the maximum entropy method. As fea
ture flmctions, we have defined two mod
els, one with co-occurrence information and 
the other with morphological information. 
As computational cost associated with this 
method is too expensive, we have proposed 
several methods to suppress the overhead in 
order to realize the system. We had experi
ments for each model of features, and the re
sult showed the effectiveness of this method, 
especially for the model of features with co
occurrence and morphological information. 
Acknowledgments 
We would like to thank the Electrotechni
cal Laboratory for giving us the machine
readable dictionary which was used as the 
training data. 

References 

L. E. Baum. 1972. An inequality and associ
ated maximumization technique in statis
tical estimation of probabilistic fimctions 
of a markov process. Inequalities, 3:1-8. 

Adam L. Berger, Stephen A. Della Pietra, 
and Vincent J. Della Pietra. 1996. A max
imum entropy approach to natural lan
guage processing. Computational Linguis
tics, 22(1):39-71. 

Hiroyuki Kaji and Toshiko Aizono. 1996. 
Extracting word correspondences from 
bilingual corpora based on word co
occurrence information. In Proceedings 
of the 16th International Conference on 
Computational Linguistics, pages 23-28. 

M. Kay and M. RSschesen. 1993. Text 
translation alignment. Computational 
Linguistics, 19(1):121-142. 

J. D. Lafferty. 1993. A derivation of the 
inside-outside algorithm from tile EM al
gorithm. IBM Research Report. IBM T.J. 
Watson Research Center. 

Stephen Della Pietra, Vincent Della Pietra, 
and John Lafferty. 1995. Inducing fea
tures of random fields. Technical Report 
CMU-CS-95-144, Carnegie Mellon Univer
sity, May. 

Adwait Ratnaparkhi. 1997. A linear ob
served time statistical parser based on 
maximum entropy models. In Proceedings 
of Second Conference On Empirical Meth
ods in Natural Language Processing. 

Jeffrey C. Reynar and Adwait Ratnaparkhi. 
1997. A maximuin entropy approach to 
identifying sentence boundaries. It, Pro
ceedings of the 5th Applied Natural Lan
guage Processing Conference. 

Ronald Rosenfeld. 1996. A lnaxinmm en
tropy approach to adaptive statistical lan
guage modeling. Computer, Speech and 
Language, (10):187-228. 

Kiyoaki Shirai, Kentaro Inui, Takenobu 
Tokunaga, and Hozumi Tanaka. 1996. 
A maximum entropy model for estimat
ing lexical bigrams (in Japanese). In SIG 
Notes of the Information Processing Soci
ety of Japan, number 96-NL-116. 

Takehito Utsuro, Takashi Miyata, and Yuji 
Matsumoto. 1997. Maximum entropy 
model learning of subcategorizatoin pref
erence. In Proceedings of the 5th Work
shop on Very Large Corpora, pages 246
260, August. 

