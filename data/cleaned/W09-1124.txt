Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 192–200,
Boulder, Colorado, June 2009. c 2009 Association for Computational Linguistics
A simple feature-copying aproach 
for long-distance dependencies 
Marc Vilain, Jonathan Huggins, and Ben Wellner 
The MITRE Corporation 
202 Burlington Rd 
Bedford, MA 01730 (USA) 
{mbv,jhuggins,wellner}@mitre.org 
 
 
Abstract 
This paper is concerned with statistical meth-
ods for treating long-distance dependencies. 
We focus in particular on a case of substantial 
recent interest: that of long-distance depend-
ency efects in entity extraction.  We intro-
duce a new aproach to capturing these efects 
through a simple feature copying preproces, 
and demonstrate substantial performance 
gains on several entity extraction tasks. 
1 Long-distance dependencies 
The linguistic phenomena known as long-distance 
dependencies have a long history in computational 
linguistics.  Originaly arising in phrase-structure 
gramar, the term aptly describes phenomena that 
are not strictly gramatical, and has thus gained 
curency in other endeavors, including that of con-
cern to us here: entity extraction.  The comon 
thread, however, is simply that the treatment of a 
linguistic constituent α might be influenced by the 
treatment of a non-local constituent β. 
In phrase-structure gramar, dependencies 
arise betwen matrix phrases and the gaped 
phrases that they dominate, as in “the cake that I 
hope you’l serve ε”.  The idea that these are long-
distance dependencies arises from the fact that the 
separation betwen linked constituents can be arbi-
trarily increased while their dependency continues 
to hold (as in “the cake that I hope you’l ask Fred 
to tel Joan to beg Maryane to serve ε”). 
With entity extraction, long-distance dependen-
cies typicaly ocur betwen mentions of the same 
entity. Consider, for example, the italicized refer-
ences to Thomas White in this newswire excerpt: 
Bank of America on Friday named Thomas 
White head of global markets.  White has 
ben global head of credit products. 
The fact that the first of these mentions is easily 
understod as person-denoting has substantial 
bearing on interpreting the second mention as per-
son-denoting as wel. But while local evidence for 
personhod is abundant for the first instance (e.g., 
the given name “Thomas” or the verb “named”), 
the evidence local to the second instance is weak, 
and it is highly unlikely that a learning procedure 
would on its own acquire the relevant 5-gram con-
text (α has ben β
JJ
 γ
title
). The dependency betwen 
these instances of White is thus a significant factor 
in interpreting both as names. 
It is wel known that capturing this kind of de-
pendency can dramaticaly improve the perform-
ance of entity extraction systems. In this paper, we 
pursue a very simple method that enables statistical 
models to exploit these long-distance dependencies 
for entity extraction.  The method obtains compa-
rable or beter results than those achieved by more 
elaborate techniques, and while we focus here on 
the specific case of entity extraction, we believe 
that the method is simple and reliable enough to 
aply generaly to other long-distance phenomena. 
2 Approaches
to name dependencies 
The problem of capturing long-distance dependen-
cies betwen names has a traditional heuristic solu-
tion.  This method, which goes back to systems 
participating in the original MUC-6 evaluation 
(Sundheim, 195), is based on a found names list. 
The method requires two pases through the input. 
A first pas captures named entities based on local 
192
evidence, and enters these names into a found 
names registry. A second pas identifies candidate 
entities that were mised by the first pas, and 
compares them to entries in the registry.  Where 
there is string overlap betwen the candidate and a 
previously found name, the entity type asigned to 
the existing entry is copied to the candidate. 
Overal, this is an efective strategy, and we 
used it ourselves in a rule-based name tager from 
the MUC-6 era (Vilain and Day, 196).  The strat-
egy’s Achiles hel, however, is what hapens 
when eroneous entries are aded to the found 
names list.  These can get copied wily-nily, 
thereby drasticaly increasing the scope of what 
may originaly have started as a single local eror. 
Clearly, the aproach is beging to be given a 
firmer evidence-weighing foundation. 
2.1 A
statistical hybrid 
An early such atempt at reformulating the ap-
proach is due to Minkhev et al (199).  As with 
previous aproaches, Mikhev and his coleagues 
use a rule-based first pas to populate a found-
names list. The second pas, however, is based on 
a maximum entropy clasifier that labels non-first-
pased candidates based on evidence acrued from 
matching entries on the found-names list. The sta-
tistical nature of the decision eliminates some of 
the failure modes of the heuristic found-names 
strategy, and in particular, prevents the copying of 
single erors comited in the first pas.  The ma-
jor weaknes of the aproach, however, is the heu-
ristic first pas.  Minkhev et al note that their 
method is most efective with a high-precision 
found-names list, implemented as a tightly con-
troled (but incomplete) rule-based first pas. 
2.2 Fuly-statistical models 
Several more recent eforts have atempted to re-
move the ned for a heuristic first-pas tager, and 
have thus cast the problem as one-pas statistical 
models (Bunescu and Money, 204; Suton and 
McCalum, 204; Finkel et al, 205).  While the 
technical details difer, al thre methods aproach 
the problem through conditional random fields 
(CRFs).  In order to capture the long-distance de-
pendencies betwen name instances, these ap-
proaches extend the linear-chain sequence models 
that are typicaly used for extracting entities with a 
CRF (Sha and Pereira, 203). The resulting models 
consist of sentence-length sequences interlinked on 
those words that might potentialy have long-
distance interactions.  Because of the graph-like 
nature of these models, the simplifying asump-
tions of linear-chain CRFs no longer hold.  Since 
complete parameter estimation is intractable under 
these conditions, these thre aproaches introduce 
aproximate methods for parameter estimation or 
decoding (Perceptron training for the first, lopy 
belief propagation for the first two, Gibs sampling 
and simulated anealing for the third). 
Krishnan and Maning (206) provide a lucid 
critique of these extended models and of their 
computational ramifications.  In a nutshel, their 
critique centers on the complexity of constructing 
the linked graphs (which they demed high), the 
stability of Perceptron training (potentialy unsta-
ble), and the run-time cost of simulated anealing 
(undesirably high).  Since these undesirable prop-
erties are directly due to the treatment of long-
distance dependencies through graphical models, it 
is natural to ask whether graphical models are ac-
tualy required to capture these dependencies. 
2.3 Avoiding
non-sequential dependencies 
In point of fact, Krishnan and Maning (206) pre-
sent an alternative to these graph-based methods. 
In particular, they break the explicit links that mu-
tualy condition non-adjacent lexemes, and instead 
rely on separate pases in a way that is reminiscent 
of earlier methods.  A first-pas CRF is used to 
identify entities based solely on local information. 
The entity labels asigned by this first CRF are 
sumarized in terms of lexeme-by-lexeme major-
ity counts; these counts are then pased to a second 
CRF in the form of lexical features. 
Consider, for example, a financial news source, 
where we would expect that a term like “Bank” 
might be asigned a preponderance of ORG labels 
by the first-pas CRF.  This would be signaled to 
the second-pas CRF through a token majority fea-
ture that would take on the value ORG for al in-
stances of the lexeme “Bank”. This efectively 
agregates local first-pas labeling decisions that 
aply to this lexeme, and makes the second-pas 
CRF sensitive to these first-pas decisions. Further 
refinements capture cases where a lexeme’s label 
diverges from the token majority, for example: 
“Left Bank,” where “Bank” wil be asigned a 
LOC-valued entity majority feature whenever it ap-
193
pears in that particular word sequence. By captur-
ing long-distance dependencies through lexical 
features, Krishnan and Maning avoid the ned for 
graphical models, thus regaining tractability. 
How wel does this work?  Returning to our 
earlier example, the idea behind these majority 
count features is that a term like “White” might be 
asigned the PER label by the first CRF when it ap-
pears in the context “Thomas White.” Say, for the 
sake of argument, that suficiently many instances 
of “White” are labeled PER by the first pas to sum 
to a majority. The second-stage CRF might then be 
expected to exploit the majority count features for 
“White” to PER-label any instances of White that 
were left unlabeled in the first pas (or that were 
given eroneous first-pas labels). 
The method would be expected to fail, how-
ever, in cases where the first pas yields a majority 
of eroneous labels.  Krishnan and Maning sug-
gest that this is a fairly unlikely scenario, and dem-
onstrate that their aproach efectively captures 
long-distance name dependencies for the CoNL 
English name-taging task. They measured a best-
in-clas eror reduction of 13.3% betwen their 
two-pas method and a single-stage CRF equiped 
with comparable features. 
3 A
contradictory data set 
Just how unlikely, however, is the majority-eror 
scenario that Krishnan and Maning discount? As 
it turns out, we encountered precisely this scenario 
while working with a corpus that is closely related 
to the CoNL data used by Krishnan and Maning. 
The corpus in question was drawn from the on-
line edition of Reuters busines news. The articles 
cover a range of busines topics: mergers and ac-
quisitions (M+A), stock valuations, management 
change, and so forth.  This corpus is highly perti-
nent to this discusion, as the CoNL English data 
are also Reuters news stories, drawn from the gen-
eral news distribution. Our busines data thus rep-
resent a natural branch of the overal CoNL data. 
A characteristic of these Reuters busines sto-
ries that distinguishes them from general news is 
the prevalence of organization names, in particular 
company names.  In these data, instances of com-
pany names significantly outnumber the next-
most-comon entities (money, dates, and the like). 
Even state-of-the-art CRFs trained on these data 
therefore er on the side of generating companies, 
meaning that in the absence of countermanding 
evidence (such as the presence of a person’s given 
name), an entity wil tend to be labeled ORG by 
default. Our earlier “Thomas White” example is a 
case in point: where the ful name would typicaly 
be labeled PER, last-name-only instances (“White”) 
might go unlabeled or be marked ORGs. 
Table 1, above, shows a qualitative analysis of 
this phenomenon for PER entities in our M+A test 
set.  The table considers person-denoting entities 
with thre or more instances in the test set (n=35), 
and sumarizes the majority acuracy of the labels 
asigned to them by a feature-rich 1-pas CRF. Of 
these thirty-five cases, we eliminate from consid-
eration six trivial test cases that are present unam-
biguously in the training data (e.g., “Carl Icahn”), 
since the CRF wil efectively memorizes these 
cases during training.  Of the remaining twenty-
nine non-trivial cases, not quite half of them (45%) 
were acurately labeled by the CRF for the majority 
of their instances.  A larger number of entities ei-
ther received an incorect majority label (38%) or 
were equivocaly labeled, receiving an equal num-
ber of corect and incorect tags (17%). 
For this data set then, majority count features 
are por models of the long-distance dependencies 
betwen person names, as they are just about as 
likely to predict the wrong label as the corect one. 
4 A
feature-copying alternative 
A further analysis of our busines news test sample 
revealed an intriguing fact.  While in the absence 
of compeling evidence, the CRF might label a 
mention of a person entity as an org (or leave it 
unlabeled), for those mentions where compeling 
evidence existed, the CRF generaly got it right. By 
compeling evidence, we mean such linguistic cues 
as the presence of a given name, contextual prox-
imity to agentive verbs (e.g. “said”), and so forth. 
This sugests an alternative aproach to captur-
ing these kinds of long-distance dependencies be-
Label acuracy count % test cases 
Trivialy corect (present in 
both test and training) 
6 — 
Majority corect, test only 13 45% 
ajority incorect, test only 11 38% 
Equivocal, test only 5 17% 
Table 1: efectivenes of majority counts as predictors 
of entity type, Reuters busines news sample 
194
twen names.  In contrast to previous aproaches, 
what is neded is not so much a way of cordinat-
ing non-local decisions about an entity’s label, as a 
way of cordinating non-local evidence pertinent 
to the labeling decision. That is, instead of condi-
tioning the labeling decision of a lexeme on the 
labeling decisions for that lexeme elsewhere in the 
corpus, we ought to condition the decision on the 
key evidence suporting those decisions. 
4.1 Displaced
features 
Our aproach operates by identifying those fea-
tures of a CRF that are most predictive over a cor-
pus. Each of those features is then duplicated: for 
a given token α, one version of the feature aplies 
directly to α, while the other version aplies to al 
other instances where α’s word form apears in the 
curent document. In particular, what we duplicate 
is the indicator function for a feature.  The local 
version of an indicator Φ signals true if it aplies 
localy to α, while the displaced version Φ
d
 signals 
true if it aplies to any token α’ that is an instance 
of the same word from as α. 
To make this concrete, consider our opening 
example, now indexed with word positions: 
Thomas
7
 White
8
 … White
13
 has
14
 ben
15
 … 
Say that Φ is a feature indicator that is true of a 
token α
i
 just in case the token to its left, α
i-1, is a 
given name. In this instance, Φ(White
8
) is true and 
Φ(White
13
) is false. Then Φ
d, the displaced version 
of Φ, wil be true of α
i
 just in case there is some 
token α
j
 with the same word form such that Φ(α
j
) is 
true.  In this instance Φ
d
(White
8
) and Φ
d
(White
13
) 
are both true by virtue of Φ being true of White
8
. 
This feature displacement scheme introduces 
non-local evidence into labeling decisions, efec-
tively capturing the long-distance dependencies 
exhibited by name-taging tasks.  The method dif-
fers from previous aproaches in that the models 
are not made conditional on non-local decisions (as 
in the case of graphical models), nor are they made 
conditional on agregated first-pas decisions (as 
in Krishnan & Maning), but rather are made con-
ditional on non-local evidence (displaced features). 
4.2 Identifying
features to displace 
Because a typical entity extraction model can use 
tens or hundreds of thousands of features, it is not 
practical to displace every one of them.  Though 
technicaly this only doubles the number of fea-
tures under consideration, the lexical indexing rap-
idly gets out of hand. In adition, training and run 
times increase and, in our experience, a risk of 
over-fiting emerges.  In point of fact, however, 
capturing long-distance name dependencies does 
not require us to replicate every last bit of feature-
borne evidence. Instead, we only ned to displace 
the evidence that is most reliably predictive. 
To select predictive features to displace, we’ve 
had most suces with a method based on informa-
tion gain.  Specificaly, we use a one-time pre-
proces that measures feature gain relative to a 
corpus.  The pre-proces considers the same com-
plement of feature schemas as are used by the ac-
tual CRF, and grounds the schemas on a training 
corpus to instantiate fre lexical and P-O-S parame-
ters. Gain for the instantiated features is measured 
through K-L divergence, and the n features with 
highest gain are then selected for displacement 
(with n typicaly ranging from 1,00 to 10,00). 
As in (Schneider, 204), gain for a given fea-
ture Φ, is found through a variant of the familiar 
Kulback-Leibler divergence formula, 
€ 
D
KL
(P||Q)=p(x
i
)log
2
p(x
i
)
q(x
i
)
i
∑
 
For our purposes, the x
i
 are the non-nul entity 
labels defined for the training set (PER, ORG, etc.), 
P is the probability distribution of the labels over 
the training set, Q is the distribution of the labels 
over tokens for which Φ aplies, and p and q are 
their respective smothed probability estimates 
(Laplace smothing).  Note in particular that this 
formulation excludes the nul label (“not an en-
tity”). This efectively means that K-L divergence 
is giving us a measure of the degre to which a 
feature predicts one or more non-nul entity labels. 
Because the nul label is generaly the dominant 
label in named-entity tasks, including the nul label 
in the calculation of K-L divergence tends to 
overwhelm the statistics, and leads to the selection 
of uninformative features that predict non-entities. 
Figure 1 demonstrates the efectivenes of this 
feature selection method, along with sensitivity to 
the threshold parameter. The figure charts F-score 
on a Reuters busines news task (M+A) as a func-
tion of the number of displaced features.  From a 
baseline of F=89.3, performance improves rapidly 
with the adition of displaced features to the CRF 
model, reaching a maximum of F=91.4 with the 
195
adition of 1,00 displaced features. Performance 
then fluctuates asymptoticaly around this level. 
The chart also shows comparable growth curves 
for two alternative feature selection methods. The 
feature count method is similar to feature gain, but 
instead of ranking features with K-L divergence, it 
ranks them acording to the number of times they 
match against the corpus. Feature weight does not 
use a schema-grounding first pas to generate can-
didate features, but trains a CRF model on the cor-
pus, and then ranks features acording to the 
weight asigned to them in the model. In prelimi-
nary experiments, neither of these methods yielded 
as high-performing a set of displaced features as 
feature gain. Aditionaly their growth curves ex-
hibit sensitivity to parameter seting, which sug-
gests a risk of over-fiting.  For these reasons, we 
did not pursue these aproaches further. 
Note finaly that the feature schemas we con-
sider for displacement only encode local evidence 
(se Table 2 below). In particular, they do not en-
code the asigned label of a word form, as this 
would efectively introduce the kind of graphical 
conditional dependencies that lie outside the scope 
of linear-chain CRF methods. 
4.3 Training
and decoding 
Aside from two pre-procesing steps, training or 
decoding a CRF with displaced features is no dif-
ferent from training or decoding one with only 
conventional features.  As to the pre-procesing 
steps, the first aplies to the corpus overal, as we 
must initialy select a colection of localy predic-
tive features to displace.  The second step aplies 
on a per-document basis and consists of the crea-
tion of the inverted lexical indices that are used to 
triger indicator functions for displaced features. 
While these aditional steps complicate training 
and decoding somewhat, they have litle efect on 
actual decoding run times. Most importantly, they 
retain the linear-chain properties of the CRF, and 
therefore do not require the graphical modeling 
and involved parameter estimation caled for by 
most previous aproaches. In adition, the training 
logistics are of a leser magnitude than those re-
quired by Krishnan and Maning’s aproach, since 
training their second-stage model first requires 
round-robin training of one-fold-left-out clasifiers 
that estimate first-stage majority counts. 
5 Experimental
design 
To evaluate the efectivenes of feature copying 
with long-distance dependencies, we undertok a 
number of information extraction experiments. 
We focused on the traditional name-taging task, 
relying on both curent and archival data sets. For 
each data set, we trained entity-extraction models 
that coresponded to thre diferent strategies for 
capturing long-distance dependencies. 
• Baseline model: a feature-rich CRF trained 
with only local features and no long-distance 
dependency features; 
• Feature-copying model: a CRF trained with 
the same local features, along with displaced 
versions of high-gain features; 
• Majority model: a re-implementation of the 
Krishnan and Maning strategy, using the 
same feature set as the baseline CRF as wel 
as their majority count features. 
We used held-out development test sets to tune 
the selection of displaced features, in particular, 
the number of features to displace. 
5.1 CRF
configurations 
We used the Carafe open-source implementation of 
sequence-based conditional random fields.
1
 Carafe 
has achieved competitive results for standard se-
quence modeling tasks (Welner & Vilain, 206, 
Welner et al, 207), and alows for flexible feature 
design. Carafe provides several learning methods, 
including a fast gradient descent method using pe-
riodic step-size adjustment (Huang et al, 207). 
Preliminary trials, however, produced beter results 
                                                        
1
 htp:/sourceforge.net/projects/carafe 
Figure 1: F score on the Reuters M+A task, as a 
 function of number of displaced features 
196
with conditional log-likelihod learning (L-BFGS 
optimization).  We used this later method here, 
L2-regularized by a spherical Gausian prior with 
variance set to 10.0 (based on preliminary trials). 
Our baseline CRF was given a feature set that 
has proven its metle in the literature (se Table 2). 
Along with contextual n-grams and the like, these 
features capture linguistic regularities through 
membership in vocabulary lists, e.g., first names, 
major geographical names, honorifics, etc.  They 
also include hand-enginered lists from our legacy 
rule-based tager, e.g., head word lists for organi-
zation names, lists of agentive verbs that reliably 
aply to persons, date atoms, and more. For part-
of-spech features, we either acepted the parts of 
spech provided with a data set, or generated them 
with our implementation of Bril’s method (Bril, 
194).  For the majority count features, we used 
document and corpus versions the token and entity 
features described by Krishnan and Maning, but 
did not re-implement their super-entity feature. 
5.2 Experimental
data 
We evaluated our aproach on five diferent data 
sets: our curent corpus of Web-harvested Reuters 
busines news, as wel as four archival data sets 
that have ben reported on by other researchers.  
The busines news data consist of a training corpus 
of mergers and acquisition stories (M+A), devel-
opment and evaluation test sets for M+A and test 
sets for thre aditional topics: hot stocks (HS), 
new initiatives (NI), and general busines news 
(BN).   Table 3 provides an overview of our data 
sets and of some salient distinctions betwen them. 
Al five extraction tasks require the reporting of 
thre core entity types: persons, organizations, and 
locations; aditional required types are noted in the 
table.  The reporting guidelines for the first four 
tasks are closely related: Reuters busines and 
MUC-6 were anotated to the same original MUC-6 
standard, while MUC-7 and MNET extend the MUC-
6 standard slightly.  The CoNL standard alone 
cals for a catch-al (and troublesome) MISC entity. 
5.3 Scoring
metrics 
Previous results on these data sets have ben re-
ported using one of two scoring methods: strict 
match (CoNL) or match with partial credit, as cal-
culated by the MUC scorer (MUC-6, MUC-7, and 
MNET). To enable comparisons to previously pub-
lished work, we report our results with the metric 
apropriate to each data set (we use the MUC scorer 
for Reuters).  These scoring distinctions are perti-
nent only to comparisons of absolute performance. 
In this paper, the interest is with relative compari-
sons acros aproaches to long-distance dependen-
cies, for which the scorers are kept constant. 
6 Experimental
results 
Table 4 sumarizes our experimental results for 
the seven test sets anotated to the MUC-6 standard 
or its close variants (we wil consider the CoNL 
task separately). Along with F scores for our base-
line CRF, the table presents F scores and baseline-
relative eror reduction (Δ
E
) for two aproaches to 
long-distance name dependencies: feature dis-
placement (disp) and the Krishnan and Maning 
strategy (K+M).  We were pleased to se that fea-
ture displacement proved efective for al of the 
extraction tasks.  As the table shows, the adition 
of displaced features consistently reduced the re-
sidual eror term left by the baseline CRF trained 
only with local features. For the English-language 
corpora, the eror reduction ranged from a low of 
11 % for the Reuters NI task to a high of 39% for 
the MUC-6 task. The eror reduction for the Span-
ish-language MNET task was lowest of al, at 8.9%. 
For al the English tasks, we consistently 
achieved beter results with feature displacement 
lexical unigrams w
-2
 … w
+2 
lexical bigrams w
-2,w
-1
 … w
+1,w
+2 
P-O-S unigrams p
-2
 … p
+2
 
P-O-S bigrams p
-2,p
-1
  p
+1,p
+2
 
substrings .*s or s.* |s|≤4 
linguistic word lists gazeters, date atoms, … 
regular expresions caps., digits, … 
“corp.” nearby also “ltd.”  
Table 2: Baseline features; w
i
 and p
i
 respectively de-
note lexeme and P-O-S in relative position i. 
Corpus Language NU TM MI Topics 
MUC-6 English ✓ ✓  mostly politics 
MUC-7 English ✓ ✓r  mostly politics 
MNET Spanish ✓ ✓r  mostly politics 
Reuters English ✓ ✓  busines 
CoNL 
English   ✓ al news 
Table 3: Data set characteristics. Al include persons, 
organizations, and locations; some have nu-
meric forms (NU), dates and times (TM) 
where r indicates relative dates, or misc (MI). 
197
than with our version of Krishnan and Maning’s 
aproach (we were not able to obtain Spanish K+M 
results by publication time).  In each case, dis-
placement produced a greater reduction in baseline 
eror than did majority counts.  Furthermore, be-
cause both aproaches start from the same baseline 
CRF, the resulting raw performance was conse-
quently also higher for displacement. Note in par-
ticular the Reuters M+A test set: these are the data 
for which Table 1 sugests that majority counts 
would be por predictors of long-distance efects. 
This prediction is in fact borne out by our results. 
6.1 Efects
of linguistic enginering 
 We were interested to note that the feature dis-
placement method achieved both highest perform-
ance and highest eror reduction for the MUC-6 
corpus (F=92.8, ∆
E
=39.3%) and for two of the 
Reuters test sets: M+A (F=91.4, ∆
E
=20.0%) and BN 
(F=91.8, ∆
E
=21.6%).  The MUC-6 F-score, in par-
ticular, is comparable to those of hand-built MUC-
era systems; in fact, it exceds the score of our own 
hand-built MUC-6 system (Aberden et al, 195). 
What is aparently hapening is that these thre 
data sets are wel matched to a group of linguisti-
caly inspired lexical features with which we 
trained our baseline CRF.  In particular, our base-
line features include gazeters and word lists 
hand-selected for identifying entities based on lo-
cal context: first names, agentive verbs, date at-
oms, etc. This played out in two significant ways. 
First, these linguistic features tended to elevate 
baseline performance (se Table 4). Second, these 
same features also proved efective when dis-
placed, as demonstrated by the substantial eror 
reduction with displacement. Feature displacement 
thus further rewards sound feature enginering. 
6.2 Other
MUC-related results 
The MUC-7 and Reuters hot stocks data (HS) pro-
vide informative contrasts. For these data, feature 
displacement provided eror reduction of 
∆
E
=13.9% and 13.4% respectively, which is les 
than for the top thre data sets. It is interesting to 
note that in both cases, the baseline score is also 
lower, sugesting again that the performance of 
feature copying folows the performance of base-
line taging. In the case of Reuters HS, the evalua-
tion data contained many out-of-training references 
to stock indices, which depresed baseline scores. 
Similar development-to-evaluation divergences 
have also ben noted with the MUC-7 corpus. 
6.3 The
CoNL task 
Our results for the CoNL task, reported in Table 5 
below, provide a diferent point of contrast.  The 
midle two rows of the table present the same ex-
perimental configurations as have ben discused 
so far.  For this data set, we note that feature dis-
placement does not perform as wel as our re-
implementation of Krishnan and Maning’s strat-
egy in terms of both absolute score and eror re-
duction.  Likewise, published results for other 
aproaches mostly outperform displacement (se 
the first thre rows in Table 5). 
One posible explanation lies with the linguistic 
features with which we aproached CoNL: these 
are the same ones we originaly developed for 
MUC-6.  As noted earlier the CoNL standard di-
verges in several ways from MUC-6. In particular, 
CoNL cals for a MISC entity that covers a range of 
name-like entities, e.g., events. MISC also, how-
ever, captures names that are traped by tokeniza-
tion (“London-based”), as wel as some MUC 
organizations (sports leagues).  This sugests that 
adapting our features to the CONL task might help. 
MUC-6 MUC-7 MNET Reuters M+A Reuters BN Reuters HS Reuters NI  
F Δ
E
 F Δ
E
 F Δ
E
 F Δ
E
 F Δ
E
 F Δ
E
 F Δ
E
 
baseline 8.2 — 84.0 — 8.9 — 89.3 — 89.5 — 85.4 — 8.8 — 
disp. 92.8 39% 86.2 14% 89.9 8.9% 91.4 20% 91.8 22% 87.3 13% 90.1 11% 
K+M 91.5 28% 85.2 7.4% — — 90.4 11% 91.0 14% 86.3 6.2% 89.2 2.8% 
 
Table 4: Performance on seven test sets anotated to variants of the MUC-6 standard (MUC scorer). 
 
 base F LD F Δ
E
 
Bunescu + Money 204 80.09 82.30 1.1% 
Finkel et al 205 85.51 86.86 9.3% 
Krishnan + Maning 206 85.29 87.34 13.3% 
K+M (re-impl, MUC feats.) 84.3 86.0 10.7% 
displacement (MUC feats.) 84.3 85.8 9.6% 
displ. (CoNL feats.) 85.24 86.5 8.9% 
displ. (CoNL feats. + DS) 86.57 87.39 6.1% 
Table 5: Performance on the CoNL task; LD designates 
 use of long-distance dependency method. 
198
The final two rows in Table 5 present atempts 
to tune our features to CoNL. This includes some 
features (the “CoNL feats” in Table 5) indicating 
story topic, al-caps headline contexts, presence in 
a sporting result table, and similar idiosyncrasies. 
In adition, we also used features based on dis-
tributional similarity word lists (DS in the table) 
provided with the Stanford NER package.
2
 
While these feature enginering eforts proved 
efective, what we found surprised us. As Table 5 
shows, the CoNL features do substantialy raise 
baseline performance, with the ful set of new fea-
tures producing a baseline (F=86.6) that outper-
forms previously published baselines by over a 
point of F score. In keping with our observations 
for the MUC-anotated text, we would then have 
expected to se a comparable increase in the per-
formance of displaced features, i.e., a jump in eror 
reduction relative to the baseline.  Instead, we 
found just the reverse.  Whereas displacement ac-
counts for a 1.5 point gain in F (∆
E
=9.6%) with the 
MUC baseline features, with the beter CoNL fea-
tures, the gain due to displacement fals to 0.82 
points of F (∆
E
=6.1%). While the final result with 
displacement (F=87.39) slightly edges out the pre-
vious high water mark of F=87.35 (Krishnan and 
Maning, 205), the patern is puzling and not in 
keping with our seven other data sets. 
One posible explanations lies again with the 
CoNL standard.  The standard cals explicitly for 
inconsistent anotation of the same entity when 
used in diferent contexts. Along with place names 
being caled MISC in hyphenated contexts (noted 
above), some places must be caled ORG when used 
to refer to sports teams – except in results tables, 
where they are sometimes LOC.  Such inconsisten-
cies subvert the notion of long-distance dependen-
cies by making these dependencies contradictory, 
thereby reducing the potential value of displace-
ment as a means for improving performance. 
7 Conclusions

Earlier in this paper, we introduced the notion of 
long-distance dependencies through their original 
codification in the context of phrase-structure 
gramars.  By an interesting historical twist, the 
original solution to these gramatical long-
distance efects, known as gap threading (Pereira, 
                                                        
2
 htp:/nlp.stanford.edu/software/CRF-NER.shtml 
1981), involved what is esentialy a feature-
copying operation, namely unification of constitu-
ent features. It is gratifying to note that the method 
presented here has ilustrious predecesors. 
Regarding the particular task of interest here, 
entity extraction, this paper conclusively shows 
that a simple feature-copying method provides an 
efective method for capturing long-distance de-
pendencies betwen names. For the MUC-6 task, in 
particular, this eror reduction is enough to lift a 
midle-of-the-pack performance from our baseline 
CRF to a level that would have placed it among the 
handful of top performers at the MUC-6 evaluation. 
As noted, the method is also substantialy more 
manageable than earlier aproaches. It avoids the 
intractability of graphical models and also avoids 
the aproximations required by methods that rely 
on these models.  It also ads only minimal proc-
esing time at training and run times.  This pro-
vides a practical alternative to the method of 
Krishnan and Maning, who require twelve sepa-
rate training runs to create their models, and fur-
ther require a time-consuming run-time proces to 
mediate betwen their first and second stage CRFs. 
We intend to take this work in two directions. 
First, we would like to get to the botom of why the 
method did not do beter with the CoNL and MNET 
tasks.  As noted earlier, our hypothesis is that we 
would expect greater exploitation of long-distance 
dependencies if we first improved the performance 
of the baseline CRF, especialy by improving the 
acuity of task-related features.  While it is not a 
key interest of ours to achieve best-in-clas per-
formance on historical evaluations, it is the case 
that we sek a beter understanding of the range of 
aplication of the feature copying method. 
Another direction of interest is to consider other 
problems that exhibit long-distance dependencies 
that might be adresed by feature copying. Word 
sense disambiguation is one such case, especialy 
given Yarowsky’s maxim regarding one sense per 
discourse, a consistency notion that sems tailor-
made for treatment as long-distance dependencies 
(Yarowsky, 195). Likewise, we are curious about 
the aplicability of the method to reference resolu-
tion, another key task with long-distance efects. 
Meanwhile, we believe that this method pro-
vides a practical aproach for capturing long-
distance efects in one of the most practical and 
useful aplication of human language technologies, 
entity extraction. 
199
References 
John Aberden, John Burger, David Day, Lynete 
Hirschman, Patricia Robinson, and Marc Vilain. 
195. Description of the Alembic system as used for 
MUC-6. Pcdgs of the 6
th
 Mesage Understanding 
Conference (MUC-6). 
Eric Bril. 194. Some advances in rule-based part-of-
spech taging. Pcdgs. AAI-94. 
Razvan Bunescu and Raymond J. Money. 204. Col-
lective information extraction with relational Markov 
networks. Pcdgs. of the 42
nd
 ACL. Barcelona. 
Jeny Rose Finkel, Trond Grenager, and Christopher 
Maning. 205. Incorporating Non-local Information 
into Information Extraction Systems by Gibs Sam-
pling. Pcdgs. of the 43
rd
 ACL. An Arbor, MI. 
Han-Shen Huang, Yu-Ming Chang, and Chun-Nan Hsu. 
207. Training conditional random fields by periodic 
step size adaptation for large-scale text mining. 
Pcdgs. 7
th
 Intl. Conf. on Data Mining (ICDM-207). 
Vijay Krishnan and Christopher D. Maning. 206. An 
Efective Two-Stage Model for Exploiting Non-
Local Dependencies in Named Entity Recognition. 
Pcdgs. of the 21st COLING and 4th ACL. Sidney. 
Andrei Mikhev, Marc Moens, and Claire Grover. 199. 
Named entity recognition without gazeters. Pcdgs. 
of the 9
th
 EACL. Bergen. 
Fernando Pereira. 1981. Extraposition gramars. 
American Jnl. of Computational Linguistics, 4(7). 
Karl-Michael Schneider. 204. A new feature selection 
score for multinomial naive Bayes text clasification 
based on KL-divergence. In Companion to the Pcdgs. 
of the 42
nd
 ACL. Barcelona. 
Fei Sha and Fernando Pereira. 203. Shalow parsing 
with conditional random fields. Pcdgs. of NACL-HLT 
2003. Edmonton, CA. 
Beth Sundheim, ed. 195.  Pcdgs. of the 6
th
 Mesage 
Understanding Conference (MUC-6). Columbia, MD. 
Charles Suton and Andrew cCalum. 204. Colec-
tive segmentation and labeling of distant entities in 
information extraction.  Pcdgs. ICML Workshop on 
Statistical Relational Learning. 
Marc Vilain and David Day. 196. Finite-state phrse 
parsing by rule sequences. Pcdgs. of the 16
th
 Confer-
ence on Computational Lingusitics (COLING-96). 
Ben Welner, Mat Huyck, Scot Mardis, John Aber-
den, Alex organ, Leon Peskin, Alex Yeh, Janet 
Hitzeman, and Lynete Hirschman. 207. Rapidly re-
targetable aproaches to de-identification.  Journal of 
the Americal Medical Informatics Asociation; 14(5). 
Ben Welner and arc Vilain. (206). Leveraging ma-
chine-readable dictionaries in discriminative se-
quence models. In Pcdgs. of the 5
th
 Language 
Resources and Evaluation Conf. (LREC 2006). Genoa. 
David Yarowsky. 195.  Unsupervised word sense dis-
ambiguation rivaling supervised methods. Pcdgs. Of 
33
rd
 ACL. Cambridge, MA. 
200

