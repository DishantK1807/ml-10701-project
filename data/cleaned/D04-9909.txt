1:183	Mining Very-Non-Parallel Corpora: Parallel Sentence and Lexicon Extraction via Bootstrapping and EM Pascale Fung and Percy Cheung Human Language Technology Center, University of Science & Technology (HKUST), Clear Water Bay, Hong Kong {pascale,eepercy}@ee.ust.hk Abstract We present a method capable of extracting parallel sentences from far more disparate very-non-parallel corpora than previous comparable corpora methods, by exploiting bootstrapping on top of IBM Model 4 EM.
2:183	Step 1 of our method, like previous methods, uses similarity measures to find matching documents in a corpus first, and then extracts parallel sentences as well as new word translations from these documents.
3:183	But unlike previous methods, we extend this with an iterative bootstrapping framework based on the principle of find-one-get-more, which claims that documents found to contain one pair of parallel sentences must contain others even if the documents are judged to be of low similarity.
4:183	We re-match documents based on extracted sentence pairs, and refine the mining process iteratively until convergence.
5:183	This novel find-one-get-more principle allows us to add more parallel sentences from dissimilar documents, to the baseline set.
6:183	Experimental results show that our proposed method is nearly 50% more effective than the baseline method without iteration.
7:183	We also show that our method is effective in boosting the performance of the IBM Model 4 EM lexical learner as the latter, though stronger than Model 1 used in previous work, does not perform well on data from very-non-parallel corpus.
8:183	Figure1.
9:183	Parallel sentence and lexicon extraction via Bootstrapping and EM The most challenging task is to extract bilingual sentences and lexicon from very-non-parallel data.
10:183	Recent work (Munteanu et al. , 2004, Zhao and Vogel, 2002) on extracting parallel sentences from comparable data, and others on extracting paraphrasing sentences from monolingual corpora (Barzilay and Elhadad 2003) are based on the find-topic-extract-sentence principle which claims that parallel sentences only exist in document pairs with high similarity.
11:183	They all use lexical information (e.g. word overlap, cosine similarity) to match documents first, before extracting sentences from these documents.
12:183	1.
13:183	Introduction Parallel sentences are important resources for training and improving statistical machine translation and cross-lingual information retrieval systems.
14:183	Various methods have been previously proposed to extract parallel sentences from multilingual corpora.
15:183	Some of them are described in detail in (Manning and Sch tze, 1999, Wu, 2001, Veronis 2001).
16:183	The challenge of these tasks varies by the degree of parallel-ness of the input multilingual documents.
17:183	However, the non-parallel corpora used so far in the previous work tend to be quite comparable.
18:183	Zhao and Vogel (2002) used a corpus of Chinese and English versions of news stories from the Xinhua News agency, with roughly similar sentence order of content.
19:183	This corpus can be more accurately described as noisy parallel corpus.
20:183	Barzilay and Elhadad (2003) mined paraphrasing sentences from weather reports.
21:183	Munteanu et al. , (2004) used news articles published within the same 5-day window.
22:183	All these corpora have documents in the same, matching topics.
23:183	They can be described as on-topic documents.
24:183	In fact, both Zhao and Vogel (2002) and Barzilay and Elhadad (2003) assume similar sentence orders and applied dynamic programming in their work.
25:183	In our work, we try to find parallel sentences from far more disparate, very-non-parallel corpora than in any previous work.
26:183	Since many more multilingual texts available today contain documents that do not have matching documents in the other language, we propose finding more parallel sentences from off-topic documents, as well as on-topic documents.
27:183	An example is the TDT corpus, which is an aggregation of multiple news sources from different time periods.
28:183	We suggest the find-one-get-more principle, which claims that as long as two documents are found to contain one pair of parallel sentence, they must contain others as well.
29:183	Based on this principle, we propose an effective Bootstrapping method to accomplish our task (Figure 1).
30:183	We also apply the IBM Model 4 EM lexical learning to find unknown word translations from the extracted parallel sentences from our system.
31:183	The IBM models are commonly used for word alignment in statistical MT systems.
32:183	This EM method differs from some previous work, which used a seed-word lexicon to extract new word translations or word senses from comparable corpora (Rapp 1995, Fung & McKeown 1997, Grefenstette 1998, Fung and Lo 1998, Kikui 1999, Kaji 2003).
33:183	2.
34:183	Bilingual Sentence Alignment There have been conflicting definitions of the term comparable corpora in the research community.
35:183	In this paper, we contrast and analyze different bilingual corpora, ranging from the parallel, noisy parallel, comparable, to very-non-parallel corpora.
36:183	A parallel corpus is a sentence-aligned corpus containing bilingual translations of the same document.
37:183	The Hong Kong Laws Corpus is a parallel corpus with manually aligned sentences, and is used as a parallel sentence resource for statistical machine translation systems.
38:183	There are 313,659 sentence pairs in Chinese and English.
39:183	Alignment of parallel sentences from this type of database has been the focus of research throughout the last decade and can be accomplished by many off-the-shelf, publicly available alignment tools.
40:183	A noisy parallel corpus, sometimes also called a comparable corpus, contains non-aligned sentences that are nevertheless mostly bilingual translations of the same document.
41:183	(Fung and McKeown 1997, Kikui 1999, Zhao and Vogel 2002) extracted bilingual word senses, lexicon and parallel sentence pairs from such corpora.
42:183	A corpus such as Hong Kong News contains documents that are in fact rough translations of each other, focused on the same thematic topics, with some insertions and deletions of paragraphs.
43:183	Another type of comparable corpus is one that contains non-sentence-aligned, non-translated bilingual documents that are topic-aligned.
44:183	For example, newspaper articles from two sources in different languages, within the same window of published dates, can constitute a comparable corpus.
45:183	Rapp (1995), Grefenstette (1998), Fung and Lo (1998), and Kaji (2003) derived bilingual lexicons or word senses from such corpora.
46:183	Munteanu et al. , (2004) constructed a comparable corpus of Arabic and English news stories by matching the publishing dates of the articles.
47:183	Finally, a very-non-parallel corpus is one that contains far more disparate, very-non-parallel bilingual documents that could either be on the same topic (in-topic) or not (off-topic).
48:183	The TDT3 Corpus is such a corpus.
49:183	It contains transcriptions of various news stories from radio broadcasting or TV news report from 1998-2000 in English and Chinese.
50:183	In this corpus, there are about 7,500 Chinese and 12,400 English documents, covering more around 60 different topics.
51:183	Among these, 1,200 Chinese and 4,500 English documents are manually marked as being in-topic.
52:183	The remaining documents are marked as off-topic as they are either only weakly relevant to a topic or irrelevant to all topics in the existing documents.
53:183	From the in-topic documents, most are found to have high similarity.
54:183	A few of the Chinese and English passages are almost translations of each other.
55:183	Nevertheless, the existence of a considerable amount of off-topic document gives rise to more variety of sentences in terms of content and structure.
56:183	Overall, the TDT 3 corpus contains 110,000 Chinese sentences and 290,000 English sentences.
57:183	Some of the bilingual sentences are translations of each other, while some others are bilingual paraphrases.
58:183	Our proposed method is a first approach that can extract bilingual sentence pairs from this type of very-non-parallel corpus.
59:183	3.
60:183	Comparing bilingual corpora To quantify the parallel-ness or comparability of bilingual corpora, we propose using a lexical matching score computed from the bilingual word pairs occurring in the bilingual sentence pairs.
61:183	Matching bilingual sentence pairs are extracted from different corpora using existing and the proposed methods.
62:183	We then identify bilingual word pairs that appear in the matched sentence pairs by using a bilingual lexicon (bilexicon).
63:183	The lexical matching score is then defined as the sum of the mutual information score of a known set of word pairs that appear in the corpus:  = = ),( ),( )()( ),( ),( ec WWall ec ec ec ec WWSS WfWf WWf WWS where f(Wc,We) is the co-occurrence frequency of bilexicon pair (Wc,We) in the matched sentence pairs.
64:183	f(Wc) and f(We) are the occurrence frequencies of Chinese word Wc and English word We, in the bilingual corpus.
65:183	Corpus Parallel Comparable QuasiComparable Lexical matching score 359.1 253.8 160.3 Table 1: Bilingual lexical matching scores of different corpora Table 1 shows the lexical matching scores of the parallel corpus (Hong Kong Law), a comparable noisy parallel corpus (Hong Kong News), and a very-non-parallel corpus (TDT 3).
66:183	We can see that the more parallel or comparable the corpus, the higher the overall lexical matching score is. 4.
67:183	Comparing alignment principles It is well known that existing work on sentence alignment from parallel corpus makes use of one or multiple of the following principles (Manning and Sch tze, 1999, Somers 2001):  A bilingual sentence pair are similar in length in the two languages;  Sentences are assumed to correspond to those roughly at the same position in the other language;  A pair of bilingual sentences which contain more words that are translations of each other tend to be translations themselves.
68:183	Conversely, the context sentences of translated word pairs are similar.
69:183	For noisy parallel corpora, sentence alignment is based on embedded content words.
70:183	The word alignment principles used in previous work are as follows:  Occurrence frequencies of bilingual word pairs are similar;  The positions of bilingual word pairs are similar;  Words have one dominant sense/translation per corpus.
71:183	Different sentence alignment algorithms based on the above principles can be found in Manning and Sch tze (1999), Somers (2001), Wu (2000), and 1.
72:183	Initial document matching For all documents in the comparable corpus D: Gloss Chinese documents using the bilingual lexicon (Bilex); For every pair of glossed Chinese document and English documents, compute document similarity =>S(i,j); Obtain all matched bilingual document pairs whose S(i,j)> threshold1=>D2 2.
73:183	Sentence matching For each document pair in D2: For every pair of glossed Chinese sentence and English sentence, compute sentence similarity =>S2(i,j); Obtain all matched bilingual sentence pairs whose S2(i,j)> threshold2=>C1 3.
74:183	EM learning of new word translations For all bilingual sentences pairs in C1, do: Compute translation lexicon probabilities of all bilingual word pairs =>S3(i,j); Obtain all bilingual word pairs previously unseen in Bilex and whose S3(i,j)> threshold3=>L1, and update Bilex; Compute sentence alignment scores=>S4; if (S4 does not change) return C1 and L1, otherwise continue; 4.
75:183	Document re-matching Find all pairs of glossed Chinese and English documents which contain parallel sentences (anchor sentences) from C1=>D3; Expand D2 by finding documents similar to each of the document in D2; D2:=D3; Goto 2; Figure 2.
76:183	Bootstrapping with EM Veronis (2002).
77:183	These methods have also been applied recently in a sentence alignment shared task at NAACL 2003 1.
78:183	We have also learned that as bilingual corpora become less parallel, it is better to rely on lexical information rather than sentence length and position information.
79:183	For comparable corpora, the alignment principle made in previous work is as follows:  Parallel sentences only exist in document pairs with high similarity scores  find-topic-extract-sentence We take a step further and propose a new principle for our task:  Documents that are found to contain at least one pair of parallel sentences are likely to contain more parallel sentences  find-one-get-more 5.
80:183	Extracting Bilingual Sentences from Very-Non-Parallel Corpora Existing algorithms such as Zhao and Vogel, (2002), Barzilay and Elhadad, (2003), Munteanu et al. , (2004) for extracting parallel or paraphrasing sentences from comparable documents, are based on the find-topic-extract-sentence principle which looks for document pairs with high similarities, and then look for parallel sentences only from these documents.
81:183	Based on our proposed find-one-get-more principle, we suggest that there are other, dissimilar documents that might contain more parallel sentences.
82:183	We can iterate this whole process for improved results using a Bootstrapping method.
83:183	Figure 2 outlines the algorithm in more detail.
84:183	In the following sections 5.1-5.5, we describe the document pre-processing step followed by the four subsequent iterative steps of our algorithm.
85:183	5.1.
86:183	Document preprocessing The documents are word segmented with the Language Data Consortium (LDC) Chinese-English dictionary 2.0.Then the Chinese document is glossed using all the dictionary entries.
87:183	When a Chinese word has multiple possible translations in English, it is disambiguated by a method extended from (Fung et al. 1999).
88:183	5.2.
89:183	Initial document matching This initial step is based on the same find-topic-extract-sentence principle as in earlier works.
90:183	The aim of this step is to roughly match the Chinese-English documents pairs that have the same topic, in order to extract parallel sentences from 1 http://www.cs.unt.edu/~rada/wpt/ them.
91:183	Similar to previous work, comparability is defined by cosine similarity between document vectors.
92:183	Both the glossed Chinese document and English are represented in word vectors, with term weights.
93:183	We evaluated different combinations of term weighting of each word in the corpus: term frequency (tf); inverse document frequency (idf); tf.idf; and the product of a function of tf and idf.
94:183	The documents here are sentences.
95:183	We find that using idf alone gives the best sentence pair rank.
96:183	This is probably due to the fact that frequencies of bilingual word pairs are not comparable in a very-non-parallel corpus.
97:183	Pair-wise similarities are calculated for all possible Chinese-English document pairs, and bilingual documents with similarities above a certain threshold are considered to be comparable.
98:183	For very-non-parallel corpora, this document-matching step also serves as topic alignment.
99:183	5.3.
100:183	Sentence matching Again based on the find-topic-extract-sentence principle, we extract parallel sentences from the matched English and Chinese documents.
101:183	Each sentence is again represented as word vectors.
102:183	For each extracted document pair, pair-wise cosine similarities are calculated for all possible Chinese-English sentence pairs.
103:183	Sentence pairs above a set threshold are considered parallel and extracted from the documents.
104:183	Sentence similarity is based on the number of words in the two sentences that are translations of each other.
105:183	The better our bilingual lexicon is, the more accurate the sentence similarity will be.
106:183	In the following section, we discuss how to find new word translations.
107:183	5.4.
108:183	EM lexical learning from matched sentence pairs This step updates the bilingual lexicon according to the intermediate results of parallel sentence extraction.
109:183	New bilingual word pairs are learned from the extracted sentence pairs based on an EM learning method.
110:183	We use the GIZA++ (Och and Ney, 2000) implementation of the IBM statistical translation lexicon Model 4 (Brown et al. , 1993) for this purpose.
111:183	This model is based on the conditional probability of a source word being generated by the target word in the other language, based on EM estimation from aligned sentences.
112:183	Zhao and Vogel (2002) showed that this model lends itself to adaptation and can provide better vocabulary coverage and better sentence alignment probability estimation.
113:183	In our work, we use this model on the intermediate results of parallel sentence extraction, i.e. on a set of aligned sentence pairs that may or may not truly correspond to each other.
114:183	We found that sentence pairs with high alignment scores are not necessarily more similar than others.
115:183	This might be due to the fact that EM estimation at each intermediate step is not reliable, since we only have a small amount of aligned sentences that are truly parallel.
116:183	The EM learner is therefore weak when applied to bilingual sentences from very-non-parallel corpus.
117:183	We decided to try using parallel corpora to initialize the EM estimation, as in Zhao and Vogel (2002).
118:183	The results are discussed in Section 6.
119:183	5.5.
120:183	Document re-matching: find-one-get-more This step augments the earlier matched documents by the find-one-get-more principle.
121:183	From the set of aligned sentence pairs, we look for other documents, judged to be dissimilar in the first step, that contain one or more of these sentence pairs.
122:183	We further find other documents that are similar to each of the monolingual documents found.
123:183	This new set of documents is likely to be off-topic, yet contains segments that are on-topic.
124:183	Following our new alignment principle, we believe that these documents might still contain more parallel sentence candidates for subsequent iterations.
125:183	The algorithm then iterates to refine document matching and parallel sentence extraction.
126:183	5.6.
127:183	Convergence The IBM model parameters, including sentence alignment score and word alignment scores, are computed in each iteration.
128:183	The parameter values eventually stay unchanged and the set of extracted bilingual sentence pairs also converges to a fixed size.
129:183	The system then stops and gives the last set of bilingual sentence pairs as the final output.
130:183	6.
131:183	Evaluation We evaluate our algorithm on a very-non-parallel corpus of TDT3 data, which contains various news stories transcription of radio broadcasting or TV news report from 1998-2000 in English and Chinese Channels.
132:183	We compare the results of our proposed method against a baseline method that is based on the conventional, find-topic-extract-sentence principle only.
133:183	We investigate the performance of the IBM Model 4 EM lexical learner on data from very-non-parallel corpus, and evaluate how our method can boost its performance.
134:183	The results are described in the following sub-sections.
135:183	6.1.
136:183	Baseline method Since previous works were carried out on different corpora, in different language pairs, we cannot directly compare our method against them.
137:183	However, we implement a baseline method that follows the same find-topic-extract-sentence principle as in earlier work.
138:183	The baseline method shares the same preprocessing, document matching and sentence matching steps with our proposed method.
139:183	However, it does not iterate to update the comparable document set, the parallel sentence set, or the bilingual lexicon.
140:183	Human evaluators manually check whether the matched sentence pairs are indeed parallel.
141:183	The precision of the parallel sentences extracted is 42.8% for the top 2,500 pairs, ranked by sentence similarity scores.
142:183	6.2.
143:183	Bootstrapping performs much better There are 110,000 Chinese sentences and 290,000 English sentences in TDT3, which lead to more than 30 billion possible sentence pairs.
144:183	Few of the sentence pairs turn out to be exact translations of each other, but many are bilingual paraphrases.
145:183	For example, in the following extracted sentence pair, the English sentence has the extra phrase under the agreement, which is missing from the Chinese sentence:    (Hun Sen becomes Cambodia ' s sole prime minister)  Under the agreement, Hun Sen becomes Cambodia ' s sole prime minister.
146:183	Another example of translation versus bilingual paraphrases is as follows:   (The Chinese president Jiang Zemin arrived in Japan today for a state visit) (Translation) Chinese president Jiang Zemin arrived in Japan today for a landmark state visit.
147:183	 (This is a first visit by a Chinese head of state to Japan) (Paraphrase) Mr Jiang is the first Chinese head of state to visit the island country.
148:183	The precision of parallel sentences extraction is 65.7% for the top 2,500 pairs using our method, which has a 50% improvement over the baseline.
149:183	In addition, we also found that the precision of parallel sentence pair extraction increases steadily over each iteration, until convergence.
150:183	6.3.
151:183	Bootstrapping can boost a weak EM lexical learner 6.4.
152:183	Bootstrapping is significantly more useful than new word translations for mining parallel sentences In this section, we discuss experimental results that lead to the claim that our proposed method can boost a weak IBM Model 4 EM lexical learner.
153:183	It is important for us to gauge the effects of the two main ideas in our algorithm, Bootstrapping and EM lexicon learning, on the extraction parallel sentences from very-non-parallel corpora.
154:183	The baseline experiment shows that without iteration, the performance is at 42.8%.
155:183	We carried out another set of experiment of using Bootstrapping where the bilingual lexicon is not updated in each iteration.
156:183	The bilingual sentence extraction accuracy of the top 2500 sentence pairs in this case dropped to 65.2%, with only 1% relative degradation.
157:183	6.3.1.
158:183	EM lexical learning is weak on bilingual sentences from very-non-parallel corpora We compare the performances of the IBM Model 4 EM lexical learning on parallel data (130k sentence pairs from Hong Kong News) and very-non-parallel data (7200 sentence pairs from TDT3) by looking at a common set of source words and their top-N translation candidates extracted.
159:183	We found that the IBM Model 4 EM learning performs much worse on TDT3 data.
160:183	Figure 3 shows that the EM learner performs about 30% worse on average on the TDT3 data.
161:183	Based on the above, we conclude that EM lexical learning has little effect on the overall parallel sentence extraction output.
162:183	This is probably due to the fact that whereas EM does find new word translations (such as /Pinochet), this has little effect on the overall glossing of the Chinese document since such new words are rare.
163:183	7.
164:183	Conclusion Previous work on extracting bilingual or monolingual sentence pairs from comparable corpora has only been applied to documents that are within the same topic, or have very similar publication dates.
165:183	One principle for previous methods is find-topic-extract-sentence which claims that parallel or similar sentences can only be found in document pairs with high similarity.
166:183	We propose a new, find-one-get-more principle which claims that document pairs that contain at least one pair of matched sentences must contain others, even if these document pairs do not have high similarity scores.
167:183	Based on this, we propose a novel Bootstrapping method that successfully extracts parallel sentences from a far more disparate and very-non-parallel corpus than reported in previous work.
168:183	This very-non-parallel corpus, TDT3 data, includes documents that are off-topic, i.e. documents with no corresponding topic in the other language.
169:183	This is a completely unsupervised method.
170:183	Evaluation results show that our approach achieves 65.7% accuracy and a 50% relative improvement from baseline.
171:183	This shows that the proposed method is promising.
172:183	We also find that the IBM Model 4 lexical learner is weak on data from very-non-parallel corpus, and that its performance can be boosted by our Multilevel Bootstrapping method, whereas using parallel corpus for adaptation is not nearly as useful.
173:183	Figure 3.
174:183	EM lexical learning performance 6.3.2.
175:183	Multilevel Bootstrapping is significantly better than adaptation data in boosting the weak EM lexical learner Since the IBM model parameters can be better estimated if the input sentences are more parallel, we have tried to add parallel sentences to the extracted sentence pairs in each iteration step, as proposed by Zhao and Vogel (2002).
176:183	However, our experiments showed that adding parallel corpus gives no improvement on the final output.
177:183	This is likely due to (1) the parallel corpus is not in the same domain as the TDT corpus; and (2) there are simply not enough parallel sentences extracted at each step for the reliable estimation of model parameters.
178:183	In contrast, Figure 3 shows that when we apply Bootstrapping to the EM lexical learner, the bilingual lexicon extraction accuracy is improved by 20% on the average, evaluated on top-N translation candidates of the same source words, showing that our proposed method can boost a weak EM lexical learner even on data from very-non-parallel corpus.
179:183	In addition, we compare and contrast a number of bilingual corpora, ranging from the parallel, to comparable, and to very-non-parallel corpora.
180:183	The parallel-ness of each type of corpus is quantified by a lexical matching score calculated for the bi-lexicon pair distributed in the aligned bilingual sentence pairs.
181:183	We show that this scores increases as the parallel-ness or comparability of the corpus increases.
182:183	Finally, we would like to suggest that Bootstrapping can in the future be used in conjunction with other sentence or word alignment learning methods to provide better mining results.
183:183	For example, methods for learning a classifier to determine sentence parallel-ness such as that proposed by Munteanu et al. , (2004) can be incorporated into our Bootstrapping framework.

