<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Eric Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>26--33</pages>
<location>Toulouse, France</location>
<contexts>
<context>words of English, German, French, and many other languages. NLP researchers sometimes use “large and messy” training corpora in the hope that errors will somehow cancel out in the statistical models (Banko and Brill, 2001). Web pages are even messier than other text sources, though, and interesting linguistic regularities may easily be lost among the countless duplicates, index and directory pages, Web spam, open or d</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>Michele Banko and Eric Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 26–33, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>editors</author>
</authors>
<date>2006</date>
<booktitle>Wacky! Working papers on the Web as Corpus. GEDIT, Bologna. Online version: http://wackybook. sslmit.unibo.it</booktitle>
<marker>Baroni, Bernardini, editors, 2006</marker>
<rawString>Marco Baroni and Silvia Bernardini, editors. 2006. Wacky! Working papers on the Web as Corpus. GEDIT, Bologna. Online version: http://wackybook. sslmit.unibo.it/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Bauer</author>
<author>Judith Degen</author>
</authors>
<title>Xiaoye Deng, Priska Herger, Jan Gasthaus, Eugenie Giesbrecht</title>
<date>2007</date>
<location>Lina Jansen, Christin</location>
<marker>Bauer, Degen, 2007</marker>
<rawString>Daniel Bauer, Judith Degen, Xiaoye Deng, Priska Herger, Jan Gasthaus, Eugenie Giesbrecht, Lina Jansen, Christin Kalina,ThorbenKr¨uger,RobertM¨artin,MartinSchmidt, Simon Scholler, Johannes Steger, Egon Stemle, and StefanEvert. 2007. FIASCO:FilteringtheInternetbyautomatic subtree classification, Osnabr¨uck. In this volume.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Gregory Grefenstette</author>
</authors>
<title>Introduction to the special issue on the Web as corpus</title>
<date>2003</date>
<journal>Computational Linguistics</journal>
<volume>29</volume>
<contexts>
<context>ible and very convenient source of authentic natural language data. Automatically compiled Web corpora have become increasingly popular in recent years and have been used for many different purposes (Kilgarriff and Grefenstette, 2003; Baroni and Bernardini, 2006). In particular, Web corpora offer the NLP community an opportunity to train statistical models on, and mine information from, much larger amounts of text than was previo</context>
</contexts>
<marker>Kilgarriff, Grefenstette, 2003</marker>
<rawString>Adam Kilgarriff and Gregory Grefenstette. 2003. Introduction to the special issue on the Web as corpus. Computational Linguistics, 29(3):333–347.</rawString>
</citation>
</citationList>
</algorithm>

