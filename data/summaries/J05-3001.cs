Di Eugenio and Glass (2004) conclude with the proposal that these three forms of agreement measure collectively provide better means with which to judge agreement than any individual test.
J04-1005
For example, Table 1 shows an example taken from Di Eugenio and Glass (2004) showing the classification of the utterance Okay as an acceptance or acknowledgment.
J04-1005
2.2 Chance-Corrected Agreement: Unequal Coder Category Distribution The second class of agreement measure recommended in Di Eugenio and Glass (2004) is that of chance-corrected tests that do not assume an equal distribution of categories between coders.
J04-1005
Di Eugenio and Glass (2004) identify three general classes of agreement statistics and suggest that all three should be used in conjunction in order to accurately evaluate coding schemes.
J04-1005
For example, the coding manual for the Switchboard DAMSL dialogue act annotation scheme (Jurafsky, Shriberg, and Biasca 1997, page 2) states that kappa is used to “assess labelling accuracy,” and Di Eugenio and Glass (2004) relate reliability to “the objectivity of decisions,” whereas Carletta (1996) regards reliability as the degree to which we understand the judgments that annotators are asked to make.
J04-1005 J96-2004
The prevalent use of this criterion despite repeated advice that it is unlikely to be suitable for all studies (Carletta 1996; Di Eugenio and Glass 2004; Krippendorff 2004a) is probably due to a desire for a simple system that can be easily applied to a scheme.
J04-1005 J96-2004
Di Eugenio and Glass (2004) perceive this as an “unpleasant behavior” of chancecorrected tests, one that prevents us from concluding that the example given in Table 1 shows satisfactory levels of agreement.
J04-1005
The justification given for using percentage agreement is that it does not suffer from what Di Eugenio and Glass (2004) referred to as the “prevalence problem.” Prevalence refers to the unequal distribution of label use by coders.
J04-1005
Since Jean Carletta (1996) exposed computational linguists to the desirability of using chance-corrected agreement statistics to infer the reliability of data generated by applying coding schemes, there has been a general acceptance of their use within the field.
J96-2004
This is an unsuitable measure for inferring reliability, and it was the use of this measure that prompted Carletta (1996) to recommend chance-corrected measures.
J96-2004
