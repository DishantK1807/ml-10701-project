1:180	On Log-Likelihood-Ratios and the Significance of Rare Events Robert C. MOORE Microsoft Research One Microsoft Way Redmond, WA 90052 USA bobmoore@microsoft.com Abstract We address the issue of judging the significance of rare events as it typically arises in statistical naturallanguage processing.
2:180	We first define a general approach to the problem, and we empirically compare results obtained using log-likelihood-ratios and Fishers exact test, applied to measuring strength of bilingual word associations.
3:180	1 Introduction Since it was first introduced to the NLP community by Dunning (1993), the G 2 log-likelihood-ratio statistic 1 has been widely used in statistical NLP as a measure of strength of association, particularly lexical associations.
4:180	Nevertheless, its use remains controversial on the grounds that it may be unreliable when applied to rare events.
5:180	For instance Pedersen, et al.6:180	(1996) present data showing that significance values for rare bigrams estimated with G 2 can differ substantially from the true values as computed by Fishers exact test.
7:180	Although Dunning argues that G 2 is superior to the chi-square statistic 2 X 2 for dealing with rare events, Agresti (1990, p. 246) cites studies showing X 2 is valid with smaller sample sizes and more sparse tables than G 2, and either X 2 or G 2 can be unreliable when expected frequencies of less than 5 are involved, depending on circumstances.
8:180	The problem of rare events invariably arises whenever we deal with individual words because of the Zipfian phenomenon that, typically, no matter how large a corpus one has, most of the distinct words in it will occur only a small number of times.
9:180	For example, in 500,000 English sentences sampled from the Canadian Hansards data supplied for the bilingual word alignment workshop held at HLTNAACL 2003 (Mihalcea and Pedersen, 2003), there are 52,921 distinct word types, of which 60.5% oc1 Dunning did not use the name G 2, but this appears to be its preferred name among statisticians (e.g. , Agresti, 1990).
10:180	2 Following Agresti, we use X 2 to denote the test statistic and  2 to denote the distribution it approximates.
11:180	cur five or fewer times, and 32.8% occur only once.
12:180	The G 2 statistic has been most often used in NLP as a measure of the strength of association between words, but when we consider pairs of words, the sparse data problem becomes even worse.
13:180	If we look at the 500,000 French sentences corresponding to the English sentences described above, we find 19,460,068 English-French word pairs that occur in aligned sentences more often than would be expected by chance, given their monolingual frequencies.
14:180	Of these, 87.9% occur together five or fewer times, and 62.4% occur together only once.
15:180	Moreover, if we look at the expected number of occurrences of these word pairs (which is the criteria used for determining the applicability of the X 2 or G 2 significance tests), we find that 93.2% would be expected by chance to have fewer than five occurrences.
16:180	Pedersen et al.17:180	(1996) report similar proportions for monolingual bigrams in the ACL/DCI Wall Street Journal corpus.
18:180	Any statistical measure that is unreliable for expected frequencies of less than 5 would be totally unusable with such data.
19:180	2 How to Estimate Significance for Rare Events A wide variety of statistics have been used to measure strength of word association.
20:180	In one paper alone (Inkpen and Hirst, 2002), pointwise mutual information, the Dice coefficient, X 2, G 2, and Fishers exact test statistic were all computed and combined to aid in learning collocations.
21:180	Despite the fact that many of these statistics arise from significance testing, the usual practice in applying them in NLP is to choose a threshold heuristically for the value of the statistic being used and discard all the pairs below the threshold.
22:180	Indeed, Inkpen and Hirst say (p. 70) there is no principled way of choosing these thresholds. This may seem an odd statement about the measures that arise directly from significance testing, but it is clear that if standard statistical tests are used naively, the results make no sense in these applications.
23:180	One might suppose that this is merely the result of the statistics in question not being applicable to the rare events that predominate in NLP, but it is easy to show this is not so.
24:180	2.1 When is Something Seen Only Once Signficant?
25:180	Consider the case of two words that each occur only once in a corpus, but happen to co-occur.
26:180	Conventional wisdom strongly advises suspicion of any event that occurs only once, yet it is easy to see that applying standard statistical methods to this case will tend to suggest that it is highly significant, without using any questionable approximations at all.
27:180	The question that significance tests for association, such as X 2, G 2, and Fishers exact test, are designed to answer is, given the sample size and the marginal frequencies of the two items in question, what is the probability (or p-value) of seeing by chance as many or more joint occurrences as were observed?
28:180	In the case of a joint occurrence of two words that each occur only once, this is trivial to calculate.
29:180	For instance, suppose an English word and a French word each occur only once in our corpus of 500,000 aligned sentence pairs of Hansard data, but they happen to occur together.
30:180	What is the probability that this joint occurrence happened by chance alone?
31:180	We can suppose that the English word occurs in an arbitrary sentence pair.
32:180	The probability that the French word, purely by chance, would occur in the same sentence pair is clearly 1 in 500,000 or 0.000002.
33:180	Since it is impossible to have more than one joint occurrence of two words that each have only a single occurrence, 0.000002 is the exact p-value for the question we have asked.
34:180	Clearly, however, one cannot assume that the association of these two words is 0.999998 certain on this basis alone.
35:180	The problem is that there are so many possible singleton-singleton pairs, it is very likely that some of them will occur jointly, purely by chance.
36:180	This, too, is easy to calculate.
37:180	In our 500,000 sentence pairs there are 17,379 English singletons and 22,512 French singletons; so there are 391,236,048 possible singleton-singleton pairs.
38:180	For each pair, the probability of having a joint occurrence by chance is 0.000002, so the expected number of chance joint occurrences of singletonsingleton pairs is 391,236,048  0.000002, or approximately 782.5.
39:180	The question of whether a singleton-singleton pair is signficant or not then turns on how many singleton-singleton pairs we observe.
40:180	If we see only about 800, then they are not signficant, because that is just about the number we would expect to see by chance.
41:180	In our corpus, however, we see far more than that: 19,312.
42:180	Thus our best estimate of the proportion of the singleton-singleton pairs that are due to chance is 782.5/19312 = 0.0405, which we can think of as the expected noise in the singletonsingleton pairs.
43:180	Looked at another way, we can estimate that at least 95.9% of the observed singletonsingleton pairs are not due to chance, which we can think of as expected precision.
44:180	3 So, we conclude that, for this data, seeing two singletons together is significant at the 0.05 level, but this is more than five orders of magnitude less significant than naive use of standard p-values would suggest.
45:180	2.2 Generalizing the Method In the previous section, we used the p-value for the observed joint frequency given the marginal frequencies and sample size as a our base statistical measure.
46:180	We used this in an indirect way, however, that we could apply to any other measure of association.
47:180	For example, for a joint occurrence of two singletons in 500,000 samples, G 2 is approximately 28.24.
48:180	Therefore, if we wanted to use G 2 as our measure of association, we could compare the number of word pairs expected by chance to have a G 2 score greater than or equal to 28.24 with the number of word pairs observed to have a G 2 score greater than or equal to 28.24, and compute expected noise and precision just as we did with p-values.
49:180	In principle, we can do the same for any measure of association.
50:180	The worst that can happen is that if the measure of association is not a good one (i.e. , if it assigns values randomly), the expected precision will not be very good no matter how high we set the threshold.
51:180	This means that we can, if we wish, use two different statistics to estimate expected noise and precision, one as a measure of association and one to estimate the number of word pairs expected by chance to have a given level or higher of the association measure.
52:180	In our experiments, we will use a likelihood-ratio-based score as the measure of association, and contrast the results obtained using either a likelihood-ratio-based test or Fishers exact test to estimate expectations.
53:180	Computing the expected number of pairs with a given association score or higher, for a large collec3 Using the binomial distribution we can calculate that there is a 0.99 probability that there are no more than 848 singletonsingleton pairs by chance, and hence that there is a 0.99 probability that at least 95.6% of the observed singleton-singleton pairs are not due to chance.
54:180	Since this differs hardly at all from the expected precision, and there is no a priori reason to think overestimating precision is any worse than underestimating it, we will use expected values of noise and precision as our primary metrics in the rest of the paper.
55:180	for each observed C(x) { for each observed C(y) { possible pairs = |values of x with frequency C(x)| |values of y with frequency C(y)| ; C 0 (x,y)=int(C(x)C(y)/N )+1; i =1; loop: for each C(x,y) such that C 0 (x,y)  C(x,y)  min(C(x),C(y)) { score = assoc(C(x,y),C(x),C(y),N) ; if (score  threshold[i]) { prob = p-value(C(x,y),C(x),C(y),N) ; expected pairs = prob  possible pairs ; while (score  threshold[i]) { expected count[i] += expected pairs ; if (i<number of thresholds) { i++ ; } else { exit loop ; } } } } } } Figure 1: Algorithm for Expected Counts.
56:180	tion of word pairs having a wide range of marginal frequencies, turns out to be somewhat tricky.
57:180	We must first compute the p-value for an association score and then multiply the p-value by the appropriate number of word pairs.
58:180	But if the association score itself does not correlate exactly with p-value, the relationship between association score and pvalue will vary with each combination of marginal frequencies.
59:180	4 Furthermore, even for a single combination of marginal frequencies, there is in general no way to go directly from an association score to the corresponding p-value.
60:180	Finally, until we have computed all the expected frequencies and observed frequencies of interest, we dont know which association score is going to correspond to a desired level of expected precision.
61:180	These complications can be accomodated as follows: First compute the distinct marginal frequencies of the words that occur in the corpus (sepa4 We assume that for fixed marginals and sample size, and joint frequencies higher than the expected joint frequency, the association score will increase monotonically with the joint frequency.
62:180	It is hard to see how any function without this property could be considered a measure of association (unless it decreases monotonically as joint frequency increases).
63:180	rately for English and French), and how many distinct words there are for each marginal frequency.
64:180	Next, choose a set of association score thresholds that we would like to know the expected precisions for.
65:180	Accumulate the expected pair counts for each threshold by iterating through all possible combinations of observed marginals.
66:180	For each combination, compute the association score for each possible joint count (given the marginals and the sample size), starting from the smallest one greater than the expected joint count C(x)C(y)/N (where C(x) and C(y) are the marginals and N is the sample size).
67:180	Whenever the first association score greater than or equal to one of the thresholds is encountered, compute the associated p-value, multiply it by the number of possible word pairs corresponding to the combination of marginals (to obtain the expected number of word pairs with the given marginals having that association score or higher), and add the result to the accumluators for all the thresholds that have just been passed.
68:180	Stop incrementing the possible joint frequency when either the smaller of the two marginals is reached or the highest association threshold is passed.
69:180	(See Figure 1 for the details.)
70:180	At this point, we have computed the number of word pairs that would be expected by chance alone to have an association score equal to or greater than each of our thresholds.
71:180	Next we compute the number of word pairs observed to have an association score equal to or greater than each of the thresholds.
72:180	The expected noise for each threshold is just the ratio of the expected number of word pairs for the threshold to the observed number of word pairs for the threshold, and the expected precision is 1 minus the expected noise.
73:180	What hidden assumptions have we made that could call these estimates into question?
74:180	First, there might not be enough data for the estimates of expected and observed frequencies to be reliable.
75:180	This should seldom be a problem in statistical NLP.
76:180	For our 500,000 sentence pair corpus, the cumulative number of observed word pairs is in the tens of thousands for for any association score for which the estimated noise level approaches or exceeds 1%, which yields confidence bounds that should be more than adequate for most purposes (see footnote 3).
77:180	A more subtle issue is that our method may overestimate the expected pair counts, resulting in excessively conservative estimates of precision.
78:180	Our estimate of the number of pairs seen by chance for a particular value of the association measure is based on considering all possible pairs as nonassociated, which is a valid approximation only if the number 2log bracketleftBigg p(y|x) C(x,y)  p(y|x) C(x,y)  p(y|x) C(x,y)  p(y|x) C(x,y) p(y) C(y)  p(y) C(y) bracketrightBigg (1) 2log bracketleftBigg p(y|x) C(x,y)  p(y|x) C(x,y)  p(y|x) C(x,y)  p(y|x) C(x,y) p(y) C(x,y)  p(y) C(x,y)  p(y) C(x,y)  p(y) C(x,y) bracketrightBigg (2) 2log productdisplay x?{x,x} productdisplay y?{y,y} parenleftbigg p(y?|x?) p(y?) parenrightbigg C(x?,y?)
79:180	(3) 2   summationdisplay x?{x,x} summationdisplay y?{y,y} C(x?,y?)log p(y?|x?) p(y?)   (4) 2N   summationdisplay x?{x,x} summationdisplay y?{y,y} p(x?,y?)log p(x?,y?) p(x?)p(y?)   (5) Figure 2: Alternative Formulas for G 2.
80:180	of pairs having a significant positive or negative association is very small compared to the total number of possible pairs.
81:180	For the corpus used in this paper, this seems unlikely to be a problem.
82:180	The corpus contains 52,921 distinct English words and 66,406 distinct French words, for a total of 3,514,271,926 possible word pairs.
83:180	Of these only 19,460,068 have more than the expected number of joint occurrences.
84:180	Since most word pairs have no joint occurrences and far less than 1 expected occurrence, it is difficult to get a handle on how many of these unseen pairs might be negatively associated.
85:180	Since we are measuring association on the sentence level, however, it seems reasonable to expect fewer word pairs to have a significant negative association than a positive association, so 40,000,000 seems likely to be a upper bound on how many word pairs are significantly nonindependent.
86:180	This, however, is only about 1% of the total number of possible word pairs, so adjusting for the pairs that might be significantly related would not make an appreciable difference in our estimates of expected noise.
87:180	In applications where the significantly nonindependent pairs do make up a substantial proportion of the total possible pairs, an adjustment should be made to avoid overly conservative estimates of precision.
88:180	3 Understanding G 2 Dunning (1993) gives the formula for the statistic we are calling G 2 in a form that is very compact, but not necessarily the most illuminating: 2[logL(p 1,k 1,n 1 )+logL(p 2,k 2,n 2 )  log L(p,k 1,n 1 )  log L(p,k 2,n 2 )], where L(p,k,n)=p k (1  p) nk . The interpretation of the statistic becomes clearer if we re-express it in terms of frequencies and probabilities as they naturally arise in association problems, as shown in a number of alternative formulations in Figure 2.
89:180	In these formulas, x and y represent two words for which we wish to estimate the strength of association.
90:180	C(y) and C(y) are the observed frequencies of y occurring or not occurring in the corpus; C(x,y),,C(x,y) are the joint frequencies of the different possible combinations of x and y occuring and not occuring; and p(y),p(y),p(y|x),,p(y|x) are the maximum likelihood estimates of the corresponding marginal and conditional probabilities.
91:180	Formula 1 expresses G 2 as twice the logarithm of a ratio of two estimates of the probability of a sequence of observations of whether y occurs; one estimate being conditioned on whether x occurs, and the other not.
92:180	The estimate in the numerator is conditioned on whether x occurs, so the numerator is a product of four factors, one for each possible combination of x occuring and y occuring.
93:180	The overall probability of the sequence is the product of each conditional probability of the occurrence or nonoccurrence of y conditioned on the occurrence or nonoccurrence of x, to the power of the number of times the corresponding combination occurs in the sequence of observations.
94:180	The denominator is an estimate of the probability of the same sequence, based only on the marginal probability of y. Hence the denominator is simply the product of the probabilty of y occuring, to the power of the number of times y occurs, and the probabilty of y not occuring, to the power of the number of times y fails to occur.
95:180	5 The rest of Figure 2 consists of a sequence of minor algebraic transformations that yield other equivalent formulas.
96:180	In Formula 2, we simply factor the denominator into four factors corresponding to the same combinations of occurrence and nonoccurrence of x and y as in the numerator.
97:180	Then, by introducing x?
98:180	and y?
99:180	as variables ranging over the events of x and y occurring or not occurring, we can re-express the ratio as a doubly nested product as shown in Formula 3.
100:180	By distributing the log operation over all the products and exponentiations, we come to Formula 4.
101:180	Noting that C(x?,y)?
102:180	= N  p(x?,y)?
103:180	(where N is the sample size), and p(y?|x?)/p(y)?
104:180	times p(x?)/p(x)?
105:180	is p(x?,y?)/p(x?)p(y?), we arrive at Formula 5.
106:180	This can be immediately recognized as 2N times the formula for the (average) mutual information of two random variables, 6 using the maximum likelihood estimates for the probabilities involved.
107:180	The near equivalence of G 2 and mutual information is important for at least two reasons.
108:180	First, it gives us motivation for using G 2 as a measure of word association that is independent of whether it usable for determining significance.
109:180	Mutual information can be viewed as a measure of the information gained about whether one word will occur by knowing whether the other word occurs.
110:180	A priori, this is at least as plausible a measure of strength of association as is the degree to which we should be surprised by the joint frequency of the two words.
111:180	Thus even if G 2 turns out to be bad for estimating significance, it does not follow that it is therefore a bad measure of strength of association.
112:180	The second benefit of understanding the relation between G 2 and mutual information is that it an5 The interpretation of G 2 in terms of a likelihood ratio for a particular sequence of observations omits the binomial coefficients that complicate the usual derivation in terms of all possible sequences having the observed joint and marginal frequencies.
113:180	Since all such sequences have the same probability for any given probability distributions, and the same number of possible sequences are involved in both the numerator and denominator, the binomial coefficients cancel out, yielding the same likelihood ratio as for a single sequence.
114:180	6 After discovering this derivation, we learned that it is, in fact, an old result (Attneave, 1959), but it seems to be almost unknown in statistical NLP.
115:180	The only reference to it we have been able to find in the statistical NLP literature is a comment in Pedersons publically distributed Perl module for computing mutual information (http://search.cpan.org/src/TPEDERSE/Text-NSP-0.69/ Measures/tmi.pm).
116:180	It can also be seen as a special case of a more general result presented by Cover and Thomas (1991, p. 307, 12.18712.192), but otherwise we have not found it in any contemporary textbook.
117:180	swers the question of how to compare G 2 scores as measures of strength of association, when they are obtained from corpora of different sizes.
118:180	Formula 5 makes it clear that G 2 scores will increase linearly with the size of the corpus, assuming the relevant marginal and conditional probabilities remain the same.
119:180	The mutual information score is independent of corpus size under the same conditions, and thus offers a plausible measure to be used across corpora of varying sizes.
120:180	4 Computing Fishers Exact Test In Section 2 we developed a general method of estimating significance for virtually any measure of association, given a way to estimate the expected number of pairs of items having a specified degree of association or better, conditioned on the marginal frequencies of the items composing the pair and the sample size.
121:180	We noted that for some plausible measures of association, the association metric itself can be used to estimate the p-values needed to compute the expected counts.
122:180	G 2 is one such measure, but it is questionable whether it is usable for computing p-values on the kind of data typical of NLP applications.
123:180	We will attempt to answer this question empirically, at least with respect to bilingual word association, by comparing p-values and expected noise estimates derived from G 2 to those derived from a gold standard, Fishers exact test.
124:180	In this test, the hypergeometric probability distribution is used to compute what the exact probability of a particular joint frequency would be if there were no association between the events in question, given the marginal frequencies and the sample size.
125:180	The only assumption made is that all trials are independent.
126:180	The formula for this probability in our setting is: C(x)!
127:180	C(x)!
128:180	C(y)!
129:180	C(y)!
130:180	N! C(x,y)!
131:180	C(x,y)!
132:180	C(x,y)!
133:180	C(x,y)!
134:180	The p-value for a given joint frequency is obtained by summing the hypergeometric probability for that joint frequency and every more extreme joint frequency consistent with the marginal frequencies.
135:180	In our case more extreme means larger, since we are only interested in positive degrees of association.
136:180	7 Because it involves computing factorials of potentially large numbers and summing over many possible joint frequencies, this test has traditionally been considered feasible only for relatively small sample sizes.
137:180	However, a number op7 The null hypothesis that we wish to disprove is that a pair of words is either negatively associated or not associated; hence, a one-sided test is appropriate.
138:180	timizations enable efficient estimation of p-values by Fishers exact test for sample sizes up to at least 10 11 on current ordinary desktop computers, where the limiting factor is the precision of 64-bit floating point arithmetic rather than computation time.
139:180	Some keys to efficient computation of Fishers exact test are:  The logarithms of factorials of large numbers can be efficiently computed by highly accurate numerical approximations of the gamma function (Press et al. , 1992, Chapter 6.1), based on the identity n!=(n +1).
140:180	 The following well-known recurrence relation for the hypergeometric distribution: P k = C k1 (x,y) C k1 (x,y) C k (x,y) C k (x,y) P k1 makes it easy to calculate probabilities for a sequence of consecutive joint frequencies, once the first one is obtained.
141:180	(The subscript k indicates parameters associated with the kth joint frequency in the sequence.)
142:180	 The highest possible joint frequency will be the smaller of the two marginal frequencies, so if one of the marginals is small, few terms need to be summed.
143:180	 If we iterate from less extreme joint frequencies to more extreme joint frequencies, each probability in the summation will be smaller than the one before.
144:180	If both the marginals are large, the summation will often converge to a constant value, given limited arithmetic precision, long before the smaller marginal is reached, at which point we can stop the summation.
145:180	By taking advantage of these observations, plus a few other optimizations specific to our application, we are able to estimate the necessary expected joint frequencies for our 500,000 sentence pair corpus in 66.7 minutes using Fishers exact test, compared to 57.4 minutes using an approximate estimate based on likelihood ratios, a time penalty of only 16% for using the exact method.
146:180	5 Estimating P-Values with Log-Likelihood-Ratios The usual way of estimating p-values from loglikelihood-ratios is to rely on the fact that the pvalues for G 2 asymtotically approach the wellunderstood  2 distribution, as the sample size increases.
147:180	This is subject to the various caveats and conditions that we discussed in Section 1, however.
148:180	Since we have the ability to compute all of the exact p-values for our corpus, we do not need to rely on the  2 approximation to test whether we can use log-likelihood-ratios to estimate p-values.
149:180	We can empirically measure whether there is any consistent relationship between log-likelihood-ratios and p-values, and if so, use it empirically to estimate pvalues from log-likelihood-ratios without resorting to the  2 approximation.
150:180	For all we know at this point, it may be possible to empirically predict pvalues from G 2 under conditions where the correspondence with  2 breaks down.
151:180	This means we can drop the heretofore mysterious factor of 2 that has appeared in all the formulas for G 2, since this factor seems to have been introduced just to be able to read p-values directly from standard tables for the  2 distribution.
152:180	To make it clear what we are doing, from this point on we will use a statistic we will call LLR which we define to be G 2 /2.
153:180	To look for a relationship between LLR and pvalues as computed by Fishers exact test, we first computed both statistics for a various combinations of joint frequency, marginal frequencies, and sample sizes.
154:180	Exploratory data analysis suggested a near-linear relationship between LLR scores and the negative of the natural logarithm of the p-values.
155:180	To make sure the apparent relationship held for a real dataset, we computed the LLR scores and negative log p-values for all 19,460,068 English-French word pairs in our corpus with more joint occurrences than expected by chance, and carried out a least-squares linear regression, treating LLR score as the independent variable and negative log p-value as the dependent variable, to see how well we can predict p-values from LLR scores.
156:180	The results are as follows: slope: 1.00025 intercept: 1.15226 Pearsons r 2 : 0.999986 standard deviation: 0.552225 With an r 2 value that rounds off to five nines, LLR score proves to be a very good predictor of the negative log p-values over the range of values considered.
157:180	Moreover, with a slope of very close to 1, the LLR score and negative log p-values are not merely correlated, they are virtually the same except for the small delta represented by the intercept.
158:180	In other words, p-value  e (LLR+1.15) would seem to be not too bad an approximation.
159:180	The standard deviation of 0.55, however, is at least slightly worrying.
160:180	As a range of differences in the logarithms of the predicted and actual p-values, it corresponds to a range of ratios between the predicted and actual p-values from about 0.57 to 1.7.
161:180	6 Estimating Noise in Bilingual Word Association For our final experiment, we estimated the noise in the bilingual word associations in our data by the method of Section 2, using both Fishers exact test and LLR scores via our regression equation to estimate expected pair counts.
162:180	In both cases, we use LLR scores as the measure of association.
163:180	We computed the cumulative expected noise for every integral value of the LLR score from 1 through 20.
164:180	To try to determine the best results we could get by using LLR scores to estimate expected noise in the region where we would be likely to set a cutoff threshold, we recomputed the least-squares fit of LLR and negative log p-value, using only data with LLR scores between 5 and 15.
165:180	8 We obtained the following values for the parameters of the regression equation from the re-estimation: slope: 1.04179 intercept: 0.793324 Note that the re-estimated value of the intercept makes it closer to the theoretical value, which is log(0.5)  0.69, since independence corresponds to an LLR score of 0 and a p-value of 0.5.
166:180	The results of these experiments are summarized in Table 1.
167:180	The first column shows the potential LLR association score cut-offs, the second column is the expected noise for each cut-off estimated by Fishers exact test, the third column gives the noise estimates derived from p-values estimated from LLR scores, and the fourth column shows the ratio between the two noise estimates.
168:180	If we look at the noise estimates based on our gold standard, Fishers exact test, we see that the noise level is below 1% above an LLR score of 11, and rises rapidly below that.
169:180	This confirms previous annecdotal experience that an LLR score above 10 seems to be a reliable indicator of a significant association.
170:180	The comparison between the two noise estimates indicates that the LLR score underestimates the amount of noise except at very high noise levels.
171:180	It is worst when the LLR score cut-off equals 14, 8 This constitutes training on the test data for the sake of obtaining an upper bound on what could be achieved using LLR scores.
172:180	Should we conclude that LLR scores look promising for this use, one would want to re-run the test training the regression parameters on held-out data.
173:180	Fisher LLR Cut-Off Noise Est Noise Est Ratio 1 0.624 0.792 1.27 2 0.516 0.653 1.27 3 0.423 0.384 0.91 4 0.337 0.274 0.81 5 0.256 0.183 0.71 6 0.181 0.114 0.63 7 0.119 0.0650 0.55 8 0.0713 0.0338 0.47 9 0.0394 0.0159 0.40 10 0.0205 0.00695 0.34 11 0.00946 0.00260 0.27 12 0.00432 0.000961 0.22 13 0.00136 0.000221 0.16 14 0.00137 0.000166 0.12 15 3.52e-005 2.00e-005 0.57 16 1.56e-005 8.02e-006 0.51 17 6.82e-006 3.19e-006 0.47 18 2.94e-006 1.24e-006 0.42 19 1.24e-006 4.65e-007 0.38 20 5.16e-007 1.72e-007 0.33 Table 1: Word Association Noise Estimates.
174:180	which happens to be just below the LLR score (14.122) for singleton-singleton pairs.
175:180	Since, for a given sample size, singleton-singleton pairs have the lowest possible expected joint count, this is probably the effect of known problems with estimating p-values from likelihood ratios when expected counts are very small.
176:180	7 Conclusions When we use Fishers exact test to estimate pvalues, our new method for estimating noise for collections of rare events seems to give results that are quite consistent with our previous annecdotal experience in using LLR scores as a measure of word association.
177:180	Using likelihood ratios to estimate p-values introduces a substantial amount of error, but not the orders-of-magnitude error that Dunning (1993) demonstrated for estimates that rely on the assumption of a normal distribution.
178:180	However, since we have also shown that Fishers exact test can be applied to this type of problem without a major computational penalty, there seems to be no reason to compromise in this regard.
179:180	8 Acknowledgements Thanks to Ken Church, Joshua Goodman, David Heckerman, Mark Johnson, Chris Meek, Ted Pedersen, and Chris Quirk for many valuable discussions of the issues raised in this paper.
180:180	Thanks especially to Joshua Goodman for pointing out the existence of fast numerical approximations for the factorial function, and to Mark Johnson for helping to track down previous results on the relationship between log-likelihood-ratios and mutual information.

