1:162	Learning to Classify Email into "Speech Acts" William W. Cohen1 Vitor R. Carvalho2 Tom M. Mitchell1,2 1Center for Automated Learning & Discovery Carnegie Mellon University Pittsburgh, PA 15213 2Language Technology Institute Carnegie Mellon University Pittsburgh, PA 15213 Abstract It is often useful to classify email according to the intent of the sender (e.g. , "propose a meeting", "deliver information").
2:162	We present experimental results in learning to classify email in this fashion, where each class corresponds to a verbnoun pair taken from a predefined ontology describing typical email speech acts.
3:162	We demonstrate that, although this categorization problem is quite different from topical text classification, certain categories of messages can nonetheless be detected with high precision (above 80%) and reasonable recall (above 50%) using existing text-classification learning methods.
4:162	This result suggests that useful task-tracking tools could be constructed based on automatic classification into this taxonomy.
5:162	1 Introduction In this paper we discuss using machine learning methods to classify email according to the intent of the sender.
6:162	In particular, we classify emails according to an ontology of verbs (e.g. , propose, commit, deliver) and nouns (e.g. , information, meeting, task), which jointly describe the email speech act intended by the email sender.
7:162	A method for accurate classification of email into such categories would have many potential benefits.
8:162	For instance, it could be used to help an email user track the status of ongoing joint activities.
9:162	Delegation and coordination of joint tasks is a time-consuming and error-prone activity, and the cost of errors is high: it is not uncommon that commitments are forgotten, deadlines are missed, and opportunities are wasted because of a failure to properly track, delegate, and prioritize subtasks.
10:162	The classification methods we consider methods which could be used to partially automate this sort of activity tracking.
11:162	A hypothetical example of an email assistant that works along these lines is shown in Figure 1.
12:162	Bill, Do you have any sample scheduling-related email we could use as data?
13:162	-Steve Assistant announces: new email request, priority unknown. Sure, Ill put some together shortly.
14:162	-Bill Assistant: should I add this new commitment to your todo list? Fred, can you collect the msgs from the CSPACE corpora tagged w/ the meeting noun, ASAP?
15:162	-Bill Assistant: notices outgoing request, may take action if no answer is received promptly.
16:162	Yes, I can get to that in the next few days.
17:162	Is next Monday ok? -Fred Assistant: notices incoming commitment.
18:162	Should I send Fred a reminder on Monday? Figure 1 Dialog with a hypothetical email assistant that automatically detects email speech acts.
19:162	Dashed boxes indicate outgoing messages.
20:162	(Messages have been edited for space and anonymity).
21:162	2 Related Work Our research builds on earlier work defining illocutionary points of speech acts (Searle, 1975), and relating such speech acts to email and workflow tracking (Winograd, 1987, Flores & Ludlow, 1980, Weigant et al, 2003).
22:162	Winograd suggested that research explicating the speech-act based language-action perspective on human communication could be used to build more useful tools for coordinating joint activities.
23:162	The Coordinator (Winograd, 1987) was one such system, in which users augmented email messages with additional annotations indicating intent.
24:162	While such systems have been useful in limited contexts, they have also been criticized as cumbersome: by forcing users to conform to a particular formal system, they constrain communication and make it less natural (Schoop, 2001); in short, users often prefer unstructured email interactions (Camino et al. 1998).
25:162	We note that these difficulties are avoided if messages can be automatically annotated by intent, rather than soliciting a statement of intent from the user.
26:162	Murakoshi et al.27:162	(1999) proposed an email annotation scheme broadly similar to ours, called a deliberation tree, and an algorithm for constructing deliberation trees automatically, but their approach was not quantitatively evaluated.
28:162	The approach is based on recognizing a set of hand-coded linguistic clues.
29:162	A limitation of their approach is that these hand-coded linguistic clues are language-specific (and in fact limited to Japanese text).
30:162	Prior research on machine learning for text classification has primarily considered classification of documents by topic (Lewis, 1992; Yang, 1999), but also has addressed sentiment detection (Pang et al. , 2002; Weibe et al. , 2001) and authorship attribution (e.g. , Argamon et al, 2003).
31:162	There has been some previous use of machine learning to classify email messages (Cohen 1996; Sahami et al. , 1998; Rennie, 2000; Segal & Kephart, 2000).
32:162	However, to our knowledge, none of these systems has investigated learning methods for assigning email speech acts.
33:162	Instead, email is generally classified into folders (i.e. , according to topic) or according to whether or not it is spam.
34:162	Learning systems have been previously used to automatically detect acts in conversational speech (e.g. Finke et al. , 1998).
35:162	3 An Ontology of Email Acts Our ontology of nouns and verbs covering some of the possible speech acts associated with emails is summarized in Figure 2.
36:162	We assume that a single email message may contain multiple acts, and that each act is described by a verb-noun pair drawn from this ontology (e.g. , "deliver data").
37:162	The underlined nodes in the figure indicate the nouns and verbs for which we have trained classifiers (as discussed in subsequent sections).
38:162	To define the noun and verb ontology of Figure 2, we first examined email from several corpora (including our own inboxes) to find regularities, and then performed a more detailed analysis of one corpus.
39:162	The ontology was further refined in the process of labeling the corpora described below.
40:162	In refining this ontology, we adopted several principles.
41:162	First, we believe that it is more important for the ontology to reflect observed linguistic behavior than to reflect any abstract view of the space of possible speech acts.
42:162	As a consequence, the taxonomy of verbs contains concepts that are atomic linguistically, but combine several illocutionary points.
43:162	(For example, the linguistic unit "let's do lunch" is both directive, as it requests the receiver, and commissive, as it implicitly commits the sender.
44:162	In our taxonomy this is a single 'propose' act).
45:162	Also, acts which are abstractly possible but not observed in our data are not represented (for instance, declarations).
46:162	Noun Activity Information Meeting Logistics Data Opinion Ongoing Activity Data Single Event Meeting Other Short Term Task Other Data Committee <Verb><Noun> Verb Remind Propose Deliver Commit Request Amend Refuse Greet Other Negotiate Initiate Conclude Figure 2  Taxonomy Second, we believe that the taxonomy must reflect common non-linguistic uses of email, such as the use of email as a mechanism to deliver files.
47:162	We have grouped this with the linguistically similar speech act of delivering information.
48:162	The verbs in Figure 1 are defined as follows.
49:162	A request asks (or orders) the recipient to perform some activity.
50:162	A question is also considered a request (for delivery of information).
51:162	A propose message proposes a joint activity, i.e., asks the recipient to perform some activity and commits the sender as well, provided the recipient agrees to the request.
52:162	A typical example is an email suggesting a joint meeting.
53:162	An amend message amends an earlier proposal.
54:162	Like a proposal, the message involves both a commitment and a request.
55:162	However, while a proposal is associated with a new task, an amendment is a suggested modification of an already-proposed task.
56:162	A commit message commits the sender to some future course of action, or confirms the senders' intent to comply with some previously described course of action.
57:162	A deliver message delivers something, e.g., some information, a PowerPoint presentation, the URL of a website, the answer to a question, a message sent "FYI, or an opinion.
58:162	The refuse, greet, and remind verbs occurred very infrequently in our data, and hence we did not attempt to learn classifiers for them (in this initial study).
59:162	The primary reason for restricting ourselves in this way was our expectation that human annotators would be slower and less reliable if given a more complex taxonomy.
60:162	The nouns in Figure 2 constitute possible objects for the email speech act verbs.
61:162	The nouns fall into two broad categories.
62:162	Information nouns are associated with email speech acts described by the verbs Deliver, Remind and Amend, in which the email explicitly contains information.
63:162	We also associate information nouns with the verb Request, where the email contains instead a description of the needed information (e.g. , "Please send your birthdate".
64:162	versus "My birthdate is ".
65:162	The request act is actually for a 'deliver information' activity).
66:162	Information includes data believed to be fact as well as opinions, and also attached data files.
67:162	Activity nouns are generally associated with email speech acts described by the verbs Propose, Request, Commit, and Refuse.
68:162	Activities include meetings, as well as longer term activities such as committee memberships.
69:162	Notice every email speech act is itself an activity.
70:162	The <verb><noun> node in Figure 1 indicates that any email speech act can also serve as the noun associated with some other email speech act.
71:162	For example, just as (deliver information) is a legitimate speech act, so is (commit (deliver information)).
72:162	Automatically constructing such nested speech acts is an interesting and difficult topic; however, in the current paper we consider only the problem of determining toplevel the verb for such compositional speech acts.
73:162	For instance, for a message containing a (commit (deliver information)) our goal would be to automatically detect the commit verb but not the inner (deliver information) compound noun.
74:162	4 Categorization Results 4.1 Corpora Although email is ubiquitous, large and realistic email corpora are rarely available for research purposes.
75:162	The limited availability is largely due to privacy issues: for instance, in most US academic institutions, a users email can only be distributed to researchers if all senders of the email also provided explicit written consent.
76:162	The email corpora used in our experiments consist of four different email datasets collected from working groups who signed agreements to make their email accessible to researchers.
77:162	The first three datasets, N01F3, N02F2, and N03F2 are annotated subsets of a larger corpus, the CSpace email corpus, which contains approximately 15,000 email messages collected from a management course at Carnegie Mellon University.
78:162	In this course, 277 MBA students, organized in approximately 50 teams of four to six members, ran simulated companies in different market scenarios over a 14-week period (Kraut et al.).
79:162	N02F2, N01F3 and N03F2 are collections of all email messages written by participants from three different teams, and contain 351, 341 and 443 different email messages respectively.
80:162	The fourth dataset, the PW CALO corpus, was generated during a four-day exercise conducted at SRI specifically to generate an email corpus.
81:162	During this time a group of six people assumed different work roles (e.g. project leader, finance manager, researcher, administrative assistant, etc) and performed a number of group activities.
82:162	There are 222 email messages in this corpus.
83:162	These email corpora are all task-related, and associated with a small working group, so it is not surprising that they contain many instances of the email acts described abovefor instance, the CSpace corpora contain an average of about 1.3 email verbs per message.
84:162	Informal analysis of other personal inboxes suggests that this sort of email is common for many university users.
85:162	We believe that negotiation of shared tasks is a central use of email in many work environments.
86:162	All messages were preprocessed by removing quoted material, attachments, and non-subject header information.
87:162	This preprocessing was performed manually, but was limited to operations which can be reliably automated.
88:162	The most difficult step is removal of quoted material, which we address elsewhere (Carvalho & Cohen, 2004).
89:162	4.2 Inter-Annotator Agreement Each message may be annotated with several labels, as it may contain several speech acts.
90:162	To evaluate inter-annotator agreement, we doublelabeled N03F2 for the verbs Deliver, Commit, Request, Amend, and Propose, and the noun, Meeting, and computed the kappa statistic (Carletta, 1996) for each of these, defined as R RA  = 1 where A is the empirical probability of agreement on a category, and R is the probability of agreement for two annotators that label documents at random (with the empirically observed frequency of each label).
91:162	Hence kappa ranges from -1 to +1.
92:162	The results in Table 1 show that agreement is good, but not perfect.
93:162	Email Act Kappa Meeting 0.82 Deliver 0.75 Commit 0.72 Request 0.81 Amend 0.83 Propose 0.72 Table 1 Inter-Annotator Agreement on N03F2.
94:162	We also took doubly-annotated messages which had only a single verb label and constructed the 5-class confusion matrix for the two annotators shown in Table 2.
95:162	Note kappa values are somewhat higher for the shorter one-act messages.
96:162	Req Prop Amd Cmt Dlv kappa Req 55 0 0 0 0 0.97 Prop 1 11 0 0 1 0.77 Amd 0 1 15 0 0 0.87 Cmt 1 3 1 24 4 0.78 Dlv 1 0 2 3 135 0.91 Table 2 Inter-annotator agreement on documents with only one category.
97:162	4.3 Learnability of Categories Representation of documents.
98:162	To assess the types of message features that are most important for prediction, we adopted Support Vector Machines (Joachims, 2001) as our baseline learning method, and a TFIDF-weighted bag-of-words as a baseline representation for messages.
99:162	We then conducted a series of experiments with the N03F2 corpus only to explore the effect of different representations.
100:162	NF032 Cmt Dlv Directive Baseline SVM 25.0 49.8 75.2 no tfidf 47.3 58.4 74.6 +bigrams 46.1 66.1 76.0 +times 43.6 60.1 73.2 +POSTags 48.6 61.8 75.4 +personPhrases 41.2 61.1 73.4 NF02F2 and NF01F3 Cmt Dlv Directive Baseline SVM 10.1 56.3 66.1 All useful features 42.0 64.0 73.3 Table 3  F1 for different feature sets.
101:162	We noted that the most discriminating words for most of these categories were common words, not the low-to-intermediate frequency words that are most discriminative in topical classification.
102:162	This suggested that the TFIDF weighting was inappropriate, but that a bigram representation might be more informative.
103:162	Experiments showed that adding bigrams to an unweighted bag of words representation slightly improved performance, especially on Deliver.
104:162	These results are shown in Table 4 on the rows marked no tfidf and bigrams.
105:162	(The TFIDF-weighted SVM is shown in the row marked baseline, and the majority classifier in the row marked default; all numbers are F1 measures on 10-fold crossvalidation).
106:162	Examination of messages suggested other possible improvements.
107:162	Since much negotiation involves timing, we ran a hand-coded extractor for time and date expressions on the data, and extracted as features the number of time expressions in a message, and the words that occurred near a time (for instance, one such feature is the word before appears near a time).
108:162	These results appear in the row marked times.
109:162	Similarly, we ran a part of speech (POS) tagger and added features for words appearing near a pronoun or proper noun (personPhrases in the table), and also added POS counts.
110:162	To derive a final representation for each category, we pooled all features that improved performance over no tfidf for that category.
111:162	We then compared performance of these document representations to the original TFIDF bag of words baseline on the (unexamined) N02F2 and N01F3 corpora.
112:162	As Table 3 shows, substantial improvement with respect to F1 and kappa was obtained by adding these additional features over the baseline representation.
113:162	This result contrasts with previous experiments with bigrams for topical text classification (Scott & Matwin, 1999) and sentiment detection (Pang et al. , 2002).
114:162	The difference is probably that in this task, more informative words are potentially ambiguous: for instance, will you and I will are correlated with requests and commitments, respectively, but the individual words in these bigrams are less predictive.
115:162	Learning methods.
116:162	In another experiment, we fixed the document representation to be unweighted word frequency counts and varied the learning algorithm.
117:162	In these experiments, we pooled all the data from the four corpora, a total of 9602 features in the 1357 messages, and since the nouns and verbs are not mutually exclusive, we formulated the task as a set of several binary classification problems, one for each verb.
118:162	The following learners were used from the Based on the MinorThird toolkit (Cohen, 2004).
119:162	VP is an implementation of the voted perceptron algorithm (Freund & Schapire, 1999).
120:162	DT is a simple decision tree learning system, which learns trees of depth at most five, and chooses splits to maximize the function ( ) 00112 ++ + WWWW suggested by Schapire and Singer (1999) as an appropriate objective for weak learners.
121:162	AB is an implementation of the confidence-rated boosting method described by Singer and Schapire (1999), used to boost the DT algorithm 10 times.
122:162	SVM is a support vector machine with a linear kernel (as used above).
123:162	Act VP AB SVM DT Request (450/907) Error F1 0.25 0.58 0.22 0.65 0.23 0.64 0.20 0.69 Proposal (140/1217) Error F1 0.11 0.19 0.12 0.26 0.12 0.44 0.10 0.13 Delivery (873/484) Error F1 0.26 0.80 0.28 0.78 0.27 0.78 0.30 0.76 Commitment (208/1149) Error F1 0.15 0.21 0.14 0.44 0.17 0.47 0.15 0.11 Directive (605/752) Error F1 0.25 0.72 0.23 0.73 0.23 0.73 0.19 0.78 Commissive (993/364) Error F1 0.23 0.84 0.23 0.84 0.24 0.83 0.22 0.85 Meet (345/1012) Error F1 0.187 0.573 0.17 0.62 0.14 0.72 0.18 0.60 Table 4  Learning on the entire corpus.
124:162	Table 4 reports the results on the most common verbs, using 5-fold cross-validation to assess accuracy.
125:162	One surprise was that DT (which we had intended merely as a base learner for AB) works surprisingly well for several verbs, while AB seldom improves much over DT.
126:162	We conjecture that the bias towards large-margin classifiers that is followed by SVM, AB, and VP (and which has been so successful in topic-oriented text classification) may be less appropriate for this task, perhaps because positive and negative classes are not clearly separated (as suggested by substantial inter-annotator disagreement).
127:162	Class: Commisive (Total: 1357 msgs) 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Recall Pr ec isi on Voted Perceptron AdaBoost SVM Decision Tree Figure 3 Precision/Recall for Commissive act Further results are shown in Figure 3-5, which provide precision-recall curves for many of these classes.
128:162	The lowest recall level in these graphs corresponds to the precision of random guessing.
129:162	These graphs indicate that high-precision predictions can be made for the top-level of the verb hierarchy, as well as verbs Request and Deliver, if one is willing to slightly reduce recall.
130:162	Class: Directive (Total: 1357 msgs) 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Recall Pr ec isi on VotedPerceptron AdaBoost SVM DecisionTree Figure 4 Precision/Recall for Directive act 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Recall Pr ec isi on Meet Dlv Req AdaBoost Learner (Total: 1357 messages) Figure 5 Precision/Recall of 3 different classes using AdaBoost Transferability.
131:162	One important question involves the generality of these classifiers: to what range of corpora can they be accurately applied?
132:162	Is it possible to train a single set of email-act classifiers that work for many users, or is it necessary to train individual classifiers for each user?
133:162	To explore this issue we trained a DT classifier for Directive emails on the NF01F3 corpus, and tested it on the NF02F2 corpus; trained the same classifier on NF02F2 and tested it on NF01F3; and also performed a 5-fold crossvalidation experiment within each corpus.
134:162	(NF02F2 and NF01F3 are for disjoint sets of users, but are approximately the same size).
135:162	We then performed the same experiment with VP for Deliver verbs and SVM for Commit verbs (in each case picking the top-performing learner with respect to F1).
136:162	The results are shown in Table 5.
137:162	Test Data DT/Directive 1f3 2f2 Train Data Error F1 Error F1 1f3 25.1 71.6 16.4 72.8 2f2 20.1 68.8 18.8 71.2 VP/Deliver 1f3 30.1 55.1 21.1 56.1 2f2 35.0 25.4 21.1 35.7 SVM/Commit 1f3 23.4 39.7 15.2 31.6 2f2 31.9 27.3 16.4 15.1 Table 5 Transferability of classifiers If learned classifiers were highly specific to a particular set of users, one would expect that the diagonal entries of these tables (the ones based on cross-validation within a corpus) would exhibit much better performance than the offdiagonal entries.
138:162	In fact, no such pattern is shown.
139:162	For Directive verbs, performance is similar across all table entries, and for Deliver and Commit, it seems to be somewhat better to train on NF01F3 regardless of the test set.
140:162	4.4 Future Directions None of the algorithms or representations discussed above take into account the context of an email message, which intuitively is important in detecting implicit speech acts.
141:162	A plausible notion of context is simply the preceding message in an email thread.
142:162	Exploiting this context is non-trivial for several reasons.
143:162	Detecting threads is difficult; although email headers contain a reply-to field, users often use the reply mechanism to start what is intuitively a new thread.
144:162	Also, since email is asynchronous, two or more users may reply simultaneously to a message, leading to a thread structure which is a tree, rather than a sequence.
145:162	Finally, most sequential learning models assume a single category is assigned to each instancee.g. , (Ratnaparkhi, 1999)whereas our scheme allows multiple categories.
146:162	Classification of emails according to our verbnoun ontology constitutes a special case of a general family of learning problems we might call factored classification problems, as the classes (email speech acts) are factored into two features (verbs and nouns) which jointly determine this class.
147:162	A variety of real-world text classification problems can be naturally expressed as factored problems, and from a theoretical viewpoint, the additional structure may allow construction of new, more effective algorithms.
148:162	For example, the factored classes provide a more elaborate structure for generative probabilistic models, such as those assumed by Nave Bayes.
149:162	For instance, in learning email acts, one might assume words were drawn from a mixture distribution with one mixture component produces words conditioned on the verb class factor, and a second mixture component generates words conditioned on the noun (see Blei et al (2003) for a related mixture model).
150:162	Alternatively, models of the dependencies between the different factors (nouns and verbs) might also be used to improve classification accuracy, for instance by building into a classifier the knowledge that some nouns and verbs are incompatible.
151:162	The fact that an email can contain multiple email speech acts almost certainly makes learning more difficult: in fact, disagreement between human annotators is generally higher for longer messages.
152:162	This problem could be addressed by more detailed annotation: rather than annotating each message with all the acts it contains, human annotators could label smaller message segments (say, sentences or paragraphs).
153:162	An alternative to more detailed (and expensive) annotation would be to use learning algorithms that implicitly segment a message.
154:162	As an example, another mixture model formulation might be used, in which each mixture component corresponds to a single verb category.
155:162	5 Concluding Remarks We have presented an ontology of email speech acts that is designed to capture some important properties of a central use of email: negotiating and coordinating joint activities.
156:162	Unlike previous attempts to combine speech act theory with email (Winograd, 1987; Flores and Ludlow, 1980), we propose a system which passively observes email and automatically classifies it by intention.
157:162	This reduces the burden on the users of the system, and avoids sacrificing the flexibility and socially desirable aspects of informal, natural language communication.
158:162	This problem also raises a number of interesting research issues.
159:162	We showed that entity extraction and part of speech tagging improves classifier performance, but leave open the question of whether other types of linguistic analysis would be useful.
160:162	Predicting speech acts requires context, which makes classification an inherently sequential task, and the labels assigned to messages have non-trivial structure; we also leave open the question of whether these properties can be effectively exploited.
161:162	Our experiments show that many categories of messages can be detected, with high precision and moderate recall, using existing textclassification learning methods.
162:162	This suggests that useful task-tracking tools could be constructed based on automatic classifiersa potentially important practical application.

