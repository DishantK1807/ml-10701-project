<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Anna Korhonen</author>
<author>Aline Villavicencio</author>
<author>editors</author>
</authors>
<date>2005</date>
<booktitle>ACL-SIGLEX Workshop on Deep Lexical Acquisition</booktitle>
<editor>Computational Linguistics. Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor</editor>
<publisher>Association for</publisher>
<location>Ann Arbor, Michigan</location>
<contexts>
<context>f the target textual elements and the space of the contexts, defined by the above three working hypotheses. Finally, its principles and algorithms are used in many fields: lexical acquisition (e.g., (Baldwin et al., 2005)), terminology extraction and structuring (e.g., (Jacquemin, 2001)), ontology learning in the area of the semantic web (e.g., (Medche, 2002)), and learning knowledge for knowledge-intensive applicati</context>
</contexts>
<marker>Baldwin, Korhonen, Villavicencio, editors, 2005</marker>
<rawString>Timothy Baldwin, Anna Korhonen, and Aline Villavicencio, editors. 2005. ACL-SIGLEX Workshop on Deep Lexical Acquisition, Ann Arbor, Michigan, June. Association for Computational Linguistics. Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor.</rawString>
</citation>
<citation valid="true">
<title>The II PASCAL RTE challenge</title>
<date>2006</date>
<booktitle>In PASCAL Challenges Workshop</booktitle>
<location>Venice, Italy</location>
<marker>2006</marker>
<rawString>2006. The II PASCAL RTE challenge. In PASCAL Challenges Workshop, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Basili</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Parsing engineering and empirical robustness</title>
<date>2002</date>
<journal>Natural Language Engineering</journal>
<pages>8--2</pages>
<contexts>
<context>ature space and the working hypothesis. 5.2. The eXtended Dependency Graph To model both target textual forms and context we use also the computational version of the eXtended Dependency Graph (XDG) (Basili and Zanzotto, 2002). An XDG is a dependency graph whose nodes C are constituents and whose edges D are the grammatical relations among the constituents, i.e. XDG = (C,D). Constituents (i.e. c ∈ C) are classical syntact</context>
</contexts>
<marker>Basili, Zanzotto, 2002</marker>
<rawString>Roberto Basili and Fabio Massimo Zanzotto. 2002. Parsing engineering and empirical robustness. Natural Language Engineering, 8/2-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Burger</author>
<author>Lisa Ferro</author>
</authors>
<title>Generating an entailment corpus from news headlines</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment</booktitle>
<pages>49--54</pages>
<location>Ann Arbor, Michigan</location>
<contexts>
<context>omputing the similarity using the basic hypothesis have been proposed when the forms are word sequences such as terms (e.g., as in (Jacquemin, 2001)) or complete sentences (e.g., (Dolan et al., 2004; Burger and Ferro, 2005)). 2.2. The Distributional Hypothesis The well-known Distributional Hypothesis (DH) (Harris, 1964) allows to determine whether or not two forms are in relation by looking at their contexts. These lat</context>
</contexts>
<marker>Burger, Ferro, 2005</marker>
<rawString>John Burger and Lisa Ferro. 2005. Generating an entailment corpus from news headlines. In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 49–54, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information and lexicography</title>
<date>1989</date>
<booktitle>In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<location>Vancouver, Canada</location>
<contexts>
<context>been found (the set will be called pat as the pattern), the feature values of the I(pat) are computed as follows: I(s,w)(pat) = mi(pat,s,w) (3) where mi(pat,s,w) is the point-wise mutual information (Church and Hanks, 1989): mi(pat,s,w) = log p(pat,s,w)p(pat,s)p(s,w) (4) Probabilities are estimated with the maximum likelihood principle over the corpus. We define the vectors Is (where s is the slot X or Y ) as the vecto</context>
</contexts>
<marker>Church, Hanks, 1989</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1989. Word association norms, mutual information and lexicography. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics (ACL), Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>350--356</pages>
<publisher>COLING</publisher>
<location>Geneva, Switzerland</location>
<contexts>
<context>re complex ways of computing the similarity using the basic hypothesis have been proposed when the forms are word sequences such as terms (e.g., as in (Jacquemin, 2001)) or complete sentences (e.g., (Dolan et al., 2004; Burger and Ferro, 2005)). 2.2. The Distributional Hypothesis The well-known Distributional Hypothesis (DH) (Harris, 1964) allows to determine whether or not two forms are in relation by looking at t</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of Coling 2004, pages 350–356, Geneva, Switzerland, Aug 23–Aug 27. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Glickman</author>
<author>Ido Dagan</author>
</authors>
<title>Identifying lexical paraphrases from a single corpus: A case study for verbs</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference Recent Advances of Natural Language Processing (RANLP-2003</booktitle>
<location>Borovets, Bulgaria</location>
<contexts>
<context> algorithm has been largely employed. As we already discussed in the previous sections, the methods differ in the target of the analysis and the feature space in which the similarity is computed. In (Glickman and Dagan, 2003), the direct learning has been used to extract similarity between verbs from a single corpus. The elements represented in the feature space F were then intensional representations of verb contexts. T</context>
</contexts>
<marker>Glickman, Dagan, 2003</marker>
<rawString>Oren Glickman and Ido Dagan. 2003. Identifying lexical paraphrases from a single corpus: A case study for verbs. In Proceedings of the International Conference Recent Advances of Natural Language Processing (RANLP-2003), Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Distributional structure</title>
<date>1964</date>
<booktitle>The Philosophy of Linguistics</booktitle>
<editor>In Jerrold J. Katz and Jerry A. Fodor, editors</editor>
<publisher>Oxford University Press</publisher>
<location>New York</location>
<contexts>
<context>1)theBasicHypothesisstatingthatsimilaritiesorrelations between textual elements can be derived only looking at these textual elements (e.g., (Jacquemin, 2001)); (2) Harris’ Distributional Hypothesis (Harris, 1964) that gives a way to induce similarity between textual elements looking at their contexts in a corpus; (3) the Point-wise Assertion Pattern Exploitation Hypothesis saying that specific textual patter</context>
<context> as terms (e.g., as in (Jacquemin, 2001)) or complete sentences (e.g., (Dolan et al., 2004; Burger and Ferro, 2005)). 2.2. The Distributional Hypothesis The well-known Distributional Hypothesis (DH) (Harris, 1964) allows to determine whether or not two forms are in relation by looking at their contexts. These latter are found inacorpus. Thehypothesisstatesthattwoformsaresimilar if these are found in similar c</context>
</contexts>
<marker>Harris, 1964</marker>
<rawString>Zellig Harris. 1964. Distributional structure. In Jerrold J. Katz and Jerry A. Fodor, editors, The Philosophy of Linguistics, New York. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora</title>
<date>1992</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics (CoLing-92</booktitle>
<location>Nantes, France</location>
<contexts>
<context>ts. 2.3. The Point-wise Assertion Pattern Exploitation Hypothesis Finally, point-wise assertion pattern exploitation hypothesis (PAP) (first used in (Robison, 1970) and frequently used afterwards in (Hearst, 1992a; Szpektor et al., 2004; Pantel and Pennacchiotti, 2006)) allow to determine relations among relevant words using textual patterns, e.g. X is constituted of Y may be used to determine the part-of rel</context>
<context>ght in the context space (e.g., (Lin and Pantel, 2001)) (Sec. 4.1.). In the indirect strategy, contexts are previously selected and, then, equivalenttextualelementsemergeusingclusteringmethods(e.g., (Hearst, 1992a)) (Sec. 4.2.). In the iterative strategy, the algorithms use seeds in one of the spaces and, going back and forward in the two spaces, incrementally augment equivalent textual elements (e.g., (Szpek</context>
<context>he equivalence classes of elements in the space of the target forms. The first and important example of the indirect approach is the extraction of equivalent forms that express the relation isa (see (Hearst, 1992b)). The method is very intuitive. It assumes a pre-existing isa hierarchy H among words, e.g., a dictionary or a lexical knowledge base. Word pairs (wi,wj) ∈ H indicate that wi isa wj. Then, if we fi</context>
<context>the corpus. The pursued idea is the following. Given a seeding set of words or linguistic structures WS considered similar or realising a unique semantic relation (e.g., the is-a relation such as in (Hearst, 1992b)), a set CS of prototypical contexts are extracted. Each element c in the set CS that has some important property is called anchor. An anchor highly characterises the contexts where the set of words</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992a. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 15th International Conference on Computational Linguistics (CoLing-92), Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora</title>
<date>1992</date>
<booktitle>In Proc. of the 15th CoLing</booktitle>
<location>Nantes, France</location>
<contexts>
<context>ts. 2.3. The Point-wise Assertion Pattern Exploitation Hypothesis Finally, point-wise assertion pattern exploitation hypothesis (PAP) (first used in (Robison, 1970) and frequently used afterwards in (Hearst, 1992a; Szpektor et al., 2004; Pantel and Pennacchiotti, 2006)) allow to determine relations among relevant words using textual patterns, e.g. X is constituted of Y may be used to determine the part-of rel</context>
<context>ght in the context space (e.g., (Lin and Pantel, 2001)) (Sec. 4.1.). In the indirect strategy, contexts are previously selected and, then, equivalenttextualelementsemergeusingclusteringmethods(e.g., (Hearst, 1992a)) (Sec. 4.2.). In the iterative strategy, the algorithms use seeds in one of the spaces and, going back and forward in the two spaces, incrementally augment equivalent textual elements (e.g., (Szpek</context>
<context>he equivalence classes of elements in the space of the target forms. The first and important example of the indirect approach is the extraction of equivalent forms that express the relation isa (see (Hearst, 1992b)). The method is very intuitive. It assumes a pre-existing isa hierarchy H among words, e.g., a dictionary or a lexical knowledge base. Word pairs (wi,wj) ∈ H indicate that wi isa wj. Then, if we fi</context>
<context>the corpus. The pursued idea is the following. Given a seeding set of words or linguistic structures WS considered similar or realising a unique semantic relation (e.g., the is-a relation such as in (Hearst, 1992b)), a set CS of prototypical contexts are extracted. Each element c in the set CS that has some important property is called anchor. An anchor highly characterises the contexts where the set of words</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992b. Automatic acquisition of hyponyms from large text corpora. In Proc. of the 15th CoLing, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Jacquemin</author>
</authors>
<title>Spotting and Discovering Terms through Natural Language Processing. Massachusetts Institue of Technology, Cambrige</title>
<date>2001</date>
<location>Massachussetts, USA</location>
<contexts>
<context>alized patterns. Its main working hypotheses are stable: (1)theBasicHypothesisstatingthatsimilaritiesorrelations between textual elements can be derived only looking at these textual elements (e.g., (Jacquemin, 2001)); (2) Harris’ Distributional Hypothesis (Harris, 1964) that gives a way to induce similarity between textual elements looking at their contexts in a corpus; (3) the Point-wise Assertion Pattern Expl</context>
<context> the above three working hypotheses. Finally, its principles and algorithms are used in many fields: lexical acquisition (e.g., (Baldwin et al., 2005)), terminology extraction and structuring (e.g., (Jacquemin, 2001)), ontology learning in the area of the semantic web (e.g., (Medche, 2002)), and learning knowledge for knowledge-intensive applications (e.g., Question Answering, Information Extraction, Textual Ent</context>
<context> the two words and an external resource only. More complex ways of computing the similarity using the basic hypothesis have been proposed when the forms are word sequences such as terms (e.g., as in (Jacquemin, 2001)) or complete sentences (e.g., (Dolan et al., 2004; Burger and Ferro, 2005)). 2.2. The Distributional Hypothesis The well-known Distributional Hypothesis (DH) (Harris, 1964) allows to determine wheth</context>
</contexts>
<marker>Jacquemin, 2001</marker>
<rawString>Christian Jacquemin. 2001. Spotting and Discovering Terms through Natural Language Processing. Massachusetts Institue of Technology, Cambrige, Massachussetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT: discovery of inference rules from text</title>
<date>2001</date>
<booktitle>In Knowledge Discovery and Data Mining</booktitle>
<pages>323--328</pages>
<contexts>
<context> divided in three main classes: direct, indirect, and iterative model. In the direct strategy, textual elements are previously selected and, then, similarities are sought in the context space (e.g., (Lin and Pantel, 2001)) (Sec. 4.1.). In the indirect strategy, contexts are previously selected and, then, equivalenttextualelementsemergeusingclusteringmethods(e.g., (Hearst, 1992a)) (Sec. 4.2.). In the iterative strateg</context>
<context>arguments (e.g., the subject,object,modifier − from,modifier − in,...) and ArgFiller is a word sequence. The value of this feature was related to the frequency and the inverse document frequency. In (Lin and Pantel, 2001), the algorithm has been used to discoverequivalencerelationsbetweenverballinguisticexpressions connecting two arguments X and Y, e.g. X solved Y ≈ X found a solution to Y. Each of these verbal lingu</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT: discovery of inference rules from text. In Knowledge Discovery and Data Mining, pages 323–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Medche</author>
</authors>
<title>Ontology Learning for the Semantic Web</title>
<date>2002</date>
<volume>665</volume>
<publisher>Kluwer International</publisher>
<institution>of Engineering and Computer Science</institution>
<contexts>
<context>re used in many fields: lexical acquisition (e.g., (Baldwin et al., 2005)), terminology extraction and structuring (e.g., (Jacquemin, 2001)), ontology learning in the area of the semantic web (e.g., (Medche, 2002)), and learning knowledge for knowledge-intensive applications (e.g., Question Answering, Information Extraction, Textual Entailment Recognition, etc.). Despite all this, frontiers and challenges of </context>
</contexts>
<marker>Medche, 2002</marker>
<rawString>Alexander Medche. 2002. Ontology Learning for the Semantic Web, volume 665 of Engineering and Computer Science. Kluwer International.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A lexical database for English</title>
<date>1995</date>
<journal>Communications of the ACM</journal>
<volume>38</volume>
<contexts>
<context>ic Hypothesis (BH) is applied when the relation or the similarity between the textual forms, wi and wj, is determined looking only to wi and wj and to external knowledge repositories such as WordNet (Miller, 1995). The similarity or oriented relation between textual forms may be defined as rBH(wi,wj). The rBH(wi,wj) function works only in the space of the target textual forms. For example, an rBH(wi,wj) may d</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A lexical database for English. Communications of the ACM, 38(11):39–41, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</booktitle>
<tech>TedPedersen,SiddharthPatwardhan,andJasonMichelizzi</tech>
<pages>113--120</pages>
<location>Sydney, Australia</location>
<contexts>
<context>ern Exploitation Hypothesis Finally, point-wise assertion pattern exploitation hypothesis (PAP) (first used in (Robison, 1970) and frequently used afterwards in (Hearst, 1992a; Szpektor et al., 2004; Pantel and Pennacchiotti, 2006)) allow to determine relations among relevant words using textual patterns, e.g. X is constituted of Y may be used to determine the part-of relation among X and Y . These point-wise assertion pattern</context>
<context>n the iterative strategy, the algorithms use seeds in one of the spaces and, going back and forward in the two spaces, incrementally augment equivalent textual elements (e.g., (Szpektor et al., 2004; Pantel and Pennacchiotti, 2006)) (Sec. 4.3.). 4. Basic notations In this section we give the basic notations we are using. Trying to formalize, given a set of target words W in the target word space and a set of contexts Ctx in th</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 113–120, Sydney, Australia, July. Association for Computational Linguistics. TedPedersen,SiddharthPatwardhan,andJasonMichelizzi.</rawString>
</citation>
<citation valid="true">
<title>Wordnet::similarity measuring the relatedness of concepts</title>
<date>2004</date>
<booktitle>In Proc. of 5th NAACL</booktitle>
<location>Boston, MA</location>
<marker>2004</marker>
<rawString>2004. Wordnet::similarity measuring the relatedness of concepts. In Proc. of 5th NAACL, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th ACL Meeting</booktitle>
<location>Philadelphia, Pennsilvania</location>
<contexts>
<context>ferent as they can vary the similarity function sim(k,c) and the way to extract the recurrent forms from C−1(k). 4.3. The iterative approaches: Anchor-based algorithms The iterative approaches (e.g. (Ravichandran and Hovy, 2002; Szpektor et al., 2004)) have been proposed to apply the Distributional Hypothesis and the Point-wise assertion pattern when the corpus C is a-priori not known (e.g. the Web that is potentially infin</context>
<context>ime = I(C(Wprime)) iii. extract Wprimeprime = C−1({cprime ∈ CTX|simctx(c,IWprime) &gt; α}) iv. select the most relevant Wprimeprimeprime ⊆ Wprimeprime v. Wi = Wi ∪Wprimeprimeprime Ravichandran and Hovy (Ravichandran and Hovy, 2002) exploredthepowerofsurfacetextpatternsforopen-domain Question Answering systems. They recognized that in several Q/A systems specific types of answer are expressed by using characteristic phrases (th</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of the 40th ACL Meeting, Philadelphia, Pennsilvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold R Robison</author>
</authors>
<title>Computer-detectable semantic structures</title>
<date>1970</date>
<journal>Information Storage and Retrieval</journal>
<volume>6</volume>
<pages>288</pages>
<contexts>
<context>ts in a corpus; (3) the Point-wise Assertion Pattern Exploitation Hypothesis saying that specific textual patterns can be used to extract relevant relations between textual elements (firstly used in (Robison, 1970)). Its main strategies for realizing algorithms are stable: there are three strategies for going backward and forward between the two spaces, the space of the target textual elements and the space of</context>
<context>) similarity is defined in the space of the contexts. 2.3. The Point-wise Assertion Pattern Exploitation Hypothesis Finally, point-wise assertion pattern exploitation hypothesis (PAP) (first used in (Robison, 1970) and frequently used afterwards in (Hearst, 1992a; Szpektor et al., 2004; Pantel and Pennacchiotti, 2006)) allow to determine relations among relevant words using textual patterns, e.g. X is constitu</context>
</contexts>
<marker>Robison, 1970</marker>
<rawString>Harold R. Robison. 1970. Computer-detectable semantic structures. Information Storage and Retrieval, 6(3):273– 288.</rawString>
</citation>
</citationList>
</algorithm>

