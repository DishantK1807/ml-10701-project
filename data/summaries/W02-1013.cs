At different thresholds, a score of agreement (with each of Resnik’s (1999) two judges and their 10Tiedemann trained these weights using a list of known cognates; I use a noisy list of weighted translation pairs (speci cally, TMTL) Hence the resources required to extract cognates in this way are no di erent from those required for the translation model.
P99-1068
These were the same pairs for which human evaluations were carried out by Resnik (1999).11 Note that this is not a matching task; the documents are presented as candidate pairs, and there is no competition among pages for matches in the other language.
P99-1068
I have demonstrated the performance of this technique on English-Chinese and EnglishFrench.13 It is capable of pulling parallel texts out of a large multilingual collection, and it rivals the performance of structure-based approaches to pair classi cation (Resnik, 1999), having better agreement with human judges.
P99-1068
Melamed. 2000.
J00-2004
The STRAND system (Resnik, 1999), for example, uses structural markup information from the pages, without looking at their content, to attempt to align them.
P99-1068
A word-to-word translation model (Melamed, 2000) trained on a verse-aligned Bible using MWBM (15,548 verses, averaging 25.5 English words, 23.4 French words after tokenization).
J00-2004
Finally, I show that this algorithm performs competitively with the approach of Resnik (1999), in which only Association for Computational Linguistics.
P99-1068
Examples include mining the World-Wide Web for parallel text (Resnik, 1999; Nie et al., 1999; Ma and Liberman, 1999) and building parallel corpora from comparable corpora such as multilingual collections of news reports.
P99-1068
As in most areas of natural language processing, recent approaches to machine translation have turned increasingly to statistical modeling of the phenomenon (translation models) (Berger et al., 1994).
H94-1028
As noted by Melamed (2000), under the assumption that the quality of a link collection is the sum of the quality of the links, then this problem of nding the best set of links is equivalent to the maximum-weighted bipartite matching (MWBM) problem: Given a weighted bipartite graph G = (V1[V2;E) with jV1j=jV2jand edge weights ci;j(i2V1;j2V2), 1I use the term \text" to refer to a piece of text of any length.
J00-2004
As computational resources have become more powerful and less expensive, the task of training translation models has become feasible (Al-Onaizan et al., 1999), as has the task of translating (or \decoding") text using such models (Germann et al., 2001).
P01-1030
English-French cognate pairs, identi ed using the method of Tiedemann (1999).
W99-0626
For example, consider the task of aligning NP chunks (and perhaps also the extra-NP material) in an NP-bracketed parallel corpus; a chunk-level similarity score (Fluhr et al., 2000) built from a word-level model could be incorporated into a framework that involves bootstrapping more complex models of translation from simpler ones (Berger et al., 1994).
H94-1028
Similarity This section shows how to compute a crosslingual similarity score, tsim, for two texts.1 Suppose parallel texts are generated according to Melamed’s (2000) symmetric word-to-word model (Model A).
J00-2004
One source of raw text in this task is the World-Wide Web, for which several parallel text search systems currently exist (Resnik, 1999; Nie et al., 1999; Ma and Liberman, 1999).
P99-1068
Performance results are presented as (precision, recall) pairs as is lowered.3 Melamed (2000) used a greedy approximation to MWBM called competitive linking, which iteratively selects the edge with the highest weight, links those two vertices, then removes them from the graph.
J00-2004
The test set is 294 of the 326 pairs in Resnik’s (1999) test set.
P99-1068
6In parameter estimation, I used the aforementioned MWBM algorithm (instead of Melamed’s (2000) competitive linking), which is the maximum posterior approximation to EM.
J00-2004
The second translation lexicon, TMTL, is automatically generated by training a symmetric word-to-word translation model (Model A, (Melamed, 2000)) on the training corpus.6 All word pairs with nonzero probability were added to the translation lexicon (no smoothing or thresholding was applied).
J00-2004
The STRAND scores are similar to those published by Resnik (1999).
P99-1068
