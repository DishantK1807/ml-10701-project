Combining labelled and unlabelled data: a case study on Fisher kernels and transductive inference for biological entity recognition Cyril Goutte, HerveDejean, Eric Gaussier, Nicola Cancedda and Jean-Michel Renders Xerox Research Center Europe 6, chemin de Maupertuis 38240 Meylan, France Abstract We address the problem of using partially labelled data, eg large collections were only little data is annotated, for extracting biological entities.
Our approach relies on a combination of probabilistic models, whichwe use to model the generation ofentities and their context, and kernel machines, which implementpowerful categorisers based on a similarity measure and some labelled data.
This combination takes the form of the so-called Fisher kernels which implement asimilarity based on an underlying probabilistic model.
Suchkernels are compared with transductive inference, an alternative approachto combining labelled and unlabelled data, again coupled with Support Vector Machines.
Experiments are performed on a database of abstracts extracted from Medline.
1 Introduction
The availability of electronic databases of rapidly increasing sizes has encouraged the developmentofmethodsthatcantapinto these databases to automatically generate knowledge, for example by retrieving relevant information or extracting entities and their relationships.
Machine learning seems especially relevantin this context, because it helps performing these tasks with a minimum of user interaction.
Anumber of problems likeentity extraction or ltering can be mapped to supervised techniques like categorisation.
In addition, modern supervised classi cation methods like Support Vector Machines haveproven to be ecientand versatile.
They do, however, rely on the availability of labelled data, where labels indicate eg whether a document is relevant or whether a candidate expression is an interesting entity.
This causes two important problems that motivate our work: 1) annotating data is often a dicult and costly task involving a lot of human work 1, such that large collections of labelled data are dicult to obtain, and 2) interannotator agreement tends to be lowineggenomics collections (Krauthammer et al., 2000), thus calling for methods that are able to deal with noise and incomplete data.
On the other hand, unsupervised techniques do not require labelled data and can thus be applied regardless of the annotation problems.
Unsupervised learning, however, tend to be less data-ecient than its supervised counterpart, requiring many more examples to discover signi cant features in the data, and is incapable of solving the same kinds of problems.
For example, an ecient clustering technique maybe able to distribute documents in a number of well-de ned clusters.
However, it will be unable to decide which clusters are relevant without a minimum of supervision.
This motivates our study of techniques that rely on a combination of supervised and unsupervised learning, in order to leverage the availability oflargecollections ofunlabelled dataand use a limited amount of labelled documents.
The focus of this study is on a particular application to the genomics literature.
In genomics, a vast amountofknowledge still resides in large collections of scienti c papers suchas Medline, and several approaches have been proposed toextract,(semi-)automatically, information from such papers.
These approaches range from purely statistical ones to symbolic ones relying on linguistic and knowledge processing tools (Ohta et al., 1997;; Thomas et al., 2000;; Proux et al., 2000, for example).
Furthermore, due to the nature of the problem at hand, meth1 If automatic annotation was available, wewould basically have solved our Machine Learning problem ods derived from machine learning are called for, (Craven and Kumlien, 1999), whether supervised, unsupervised or relying on a combination of both.
Let us insist on the fact that our work is primarily concerned with combining labelled and unlabelled data, and entity extraction is used as an application in this context.
As a consequence, it is not our purpose at this pointto compare our experimental results to those obtained by speci c machine learning techniques applied to entity extraction (Cali, 1999).
Although we certainly hope that our work can be useful for entity extraction, we rather think of it as a methodological study which can hopefully be applied to dierent applications where unlabelled data may be used to improve the results of supervised learning algorithms.
In addition, performing a fair comparison of our work on standard information extraction benchmarks is not straightforward: either wewould need to obtain a large amount of unlabelled data that is comparable tothe benchmark, orwewould need to \un-label" a portion of the data.
In both cases, comparing to existing results is dicult as the amount of information used is dierent.
2 Classi
cation for entity extraction We formulate the following (binary) classi cation problem: given an input space X, and from a dataset of N input-output pairs (x k ;;y k ) 2 Xf;1;;+1g,wewant to learn a classi er h : X!f;1;;+1g so as to maximise the probability P (h(x)=y)over the xed but unknown joint input-output distribution of (x;;y) pairs.
In this setting, binary classi cation is essentially a supervised learning problem.
In order to map this to the biological entity recognition problem, we consider for each candidate term, the following binary decision problem: is the candidate a biological entity 2 (y =1)ornot(y = ;1).
The input space is a high dimensional feature space containing lexical, morpho-syntactic and contextual features.
In order to assess the validityofcombining labelled and unlabelled data for the particular task of biological entity extraction, we use the following tools.
First we rely on Suport Vector Machines together with transductive infer2 In our case, biological entities are proteins, genes and RNA, cf.
section 6.
ence (Vapnik, 1998;; Joachims, 1999), a training technique that takes both labelled and unlabelled data into account.
Secondly,we develop a Fisher kernel (Jaakkola and Haussler, 1999), whichderives the similarity from an underlying (unsupervised) model ofthe data, used as asimilarity measure (akakernel) within SVMs.
The learning process involves the following steps:  Transductive inference: learn aSVM classier h(x) using the combined (labelled and unlabelled) dataset, using traditional kernels.
 Fisher kernels: 1.
Learn aprobabilistic model of the data P(xj) using combined unlabelled and labelled data;; 2.
Derive the Fisher kernel K(x;;z) expressing the similarityinX-space;; 3.
Learn a SVM classi er h(x) using this Fisher kernel and inductive inference.
3 Probabilistic
models for co-occurence data In (Gaussier et al., 2002) we presented a general hierarchical probabilistic model whichgeneralises several established models likeNave Bayes (Yang and Liu, 1999),probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) or hierarchical mixtures (Toutanova et al., 2001).
In this model, data result from the observation of co-occuring objects.
For example, a document collection is expressed as co-occurences between documents and words;; in entity extraction, co-occuring objects may be potential entities and their context, for example.
For cooccuring objects i and j, the model is expressed as follows: P(i;;j)= X  P()P(ij) X  P(j)P(jj) (1) where  are latent classes for co-occurrences (i;;j)and arelatentnodes in ahierarchy generating objects j.
In the case where no hierarchy is needed (ie P(j)=( = )), the model reduces to PLSA: P(i;;j)= X  P()P(ij)P(jj) (2) where  are now latent concepts overboth i and j.Parameters of the model (class probabilities P() and class-conditional P(ij)andP(jj)) are learned using a deterministic annealing version of the expectation-maximisation (EM) algorithm (Hofmann, 1999;; Gaussier et al., 2002).
4 Fisher
kernels Probabilistic generative models like PLSA and hierarchical extensions (Gaussier et al., 2002) provide a natural way to model the generation of the data, and allow the use of well-founded statistical tools to learn and use the model.
In addition, they may be used to derivea model-based measure of similaritybetween examples, using the so-called Fisher kernels proposed byJaakkola and Haussler (1999).
The idea behind this kernel is that using the structure implied by the generative model will give a more relevant similarity estimate, and allow kernel methods like the support vectormachines or nearest neighbours to leverage the probabilistic model and yield improved performance (Hofmann, 2000).
The Fisher kernel is obtained using the loglikelihood of the model and the Fisher information matrix.
Let us consider our collection of documents fx k g k=1:::N, and denote by `(x)= logP(xj) the log-likelihood of the model for data x.
The expression of the Fisher kernel (Jaakkola and Haussler, 1999) is then: K(x 1 ;;x 2 )=r`(x 1 ) > I F ;1 r`(x 2 ) (3) The Fisher information matrix I F can be seen as a waytokeep the kernel expression independent of parameterisation and is de ned as I F = E  r`(x)r`(x) > , where the gradient is w.r.t.
 and the expectation is taken over P(xj).
With a suitable parameterization, the information matrixIis usually approximated by the identity matrix (Hofmann, 2000), leading to the simpler kernel expression: K(x 1 ;;x 2 )= r`(x 1 ) > r`(x 2 ).
Depending on the model, the various loglikelihoods and their derivatives will yield different Fisher kernel expressions.
For PLSA (2), the parameters are  =[P();;P(ij);;P(jj)].
From the derivatives of the likelihood `(x)= P (i;;j)2x logP(i;;j), we derive the following similarity (Hofmann, 2000): K(x 1 ;;x 2 )= X  P(jd i )P(jd j ) P() (4) + X w b P wd i b P wd j X  P(jd i ;;w)P(jd j ;;w) P(wj) with b P wd i, b P wd j theempirical worddistributions in documents d i, d j. 5 Transductive inference In standard, inductive SVM inference, the annotated data is used to infer a model, whichis then applied to unannotated test data.
The inference consists in a trade-o between the size of the margin (linked to generalisation abilities) and the number of training errors.
Transductive inference (Gammerman et al., 1998;; Joachims, 1999) aims at maximising the margin between positives and negatives, while minimising not only the actual number of incorrect predictions on labelled examples, but also the expected number of incorrect predictions on the set of unannotated examples.
This is done by including the unknown labels as extra variables in the original optimisation problem.
In the linearly separable case, the new optimisation problem amounts now to nd a labelling of the unannotated examples and a hyperplane which separates all examples (annotatedand unannotated) with maximum margin.
In the non-separable case, slackvariables are also associated to unannotated examples and the optimisation problem is now to nd a labelling and a hyperplane which optimally solves the trade-o between maximising the margin and minimising the number of misclassi ed examples (annotated and unannotated).
With the introduction of unknown labels as supplementary optimisation variables, the constraints of the quadratic optimisation problem are now nonlinear, whichmakes solving more dicult.
However, approximated iterative algorithms exist which can eciently train Transductive SVMs.
They are based on the principle ofgradually improving the solution byswitching the labels of unnannotated examples whichare misclassi ed at the current iteration, starting from an initial labelling given by the standard (inductive) SVM.
WUp Is the word capitalized?
WAllUp Is the word alls capitals?
WNum Does the word contain digits?
Table 1: Spelling features 6 Experiments Forourexperiments, weused 184abstractsfrom the Medline site.
In these articles, genes, proteins and RNAs were manually annotated bya biologist as part of the BioMIRE project.
These articles contain 1405occurrences of gene names, 792 of protein names and 81 of RNA names.
All these entities are considered relevant biological entities.
We focus here on the task of identifying names corresponding to suchentities in running texts, without dierentiating genes from proteins or RNAs.
Once candidates for biological entity names have been identi ed, this task amounts to a binary categorisation, relevant candidates corresponding to biological entity names.
We divided these abstracts in a training and development set (122 abstracts), and a test set (62 abstracts).
We then retained dierent portions of the training labels, to be used as labelled data, whereas the rest of the data is considered unlabelled.
6.1 De
nition of features First of all, the abstracts are tokenised, tagged and lemmatized.
Candidates for biological entity names are then selected on the basis of the following heuristics: a token is consideredacandidate if it appears in one of the biological lexicons we have at our diposal, or if it does not belong to our general English lexicon.
This simple heuristics allows us to retain 93% (1521 out of 1642) of biological names in the training set (90% in the test set), while considering only 21% of all possible candidates (5845 out of 27350 tokens).
It thus provides a good prelter which significantly improves the performance, in terms of speed, of our system.
The biological lexicons we use were provided by the BioMIRE project, and were derived from the resources available at: http://iubio.bio.indiana.edu/.
For each candidate, three types of features were considered.
We rst retained the part-ofspeech and some spelling information (table 1).
These features were chosen based on the inspection of gene and protein names in our lexicons.
LexPROTEIN Protein lexicon LexGENE Gene lexicon LexSPECIES Biological species lexicon LEXENGLISH General English lexicon Table 2: Features provided by lexicons.
The second type of features relates to the presence of the candidate in our lexical resources 3 (table 2).
Lastly, the third type of features describes contextual information.
The context we consider contains the four preceding and the four following words.
However, we did not take into account the position of the words in the context, but only their presence in the rightor left context, and in addition we replaced, whenever possible, eachword by a feature indicating (a) whether the word was part of the gene lexicon, (b) if not whether it was part of the protein lexicon, (c) if not whether it was part of the species lexicon, (d) and if not, whenever the candidate was neither a noun, an adjective nor averb, we replaced it by its part-of-speech.
For example, the word hairless is associated with the features given in Table 3, when encountered in the following sentence: Inhibition of the DNA-binding activity of Drosophila suppressor of hairless and of its human homolog, KBF2/RBP-J kappa, by direct proteinprotein interaction with Drosophila hairless.
The word hairless appears in the gene lexicon and is wrongly recognized as an adjectiveby our tagger.
4 The
word human, the fourth word of the rightcontext of hairless, belongs to the species lexicon, ans is thus replaced by the feature RC SPECIES.Neither Drosophila nor suppressor belong to the specialized lexicons we use, and, since they are both tagged as nouns, they are left unchanged.
Prepositions and conjunctions are replaced by their part-of-speech, and pre xes LC and RC indicate whether they were found in left or rightcontext.
Note that since twoprepositions appear in the left context of hairless, the value of the LC PREP feature is 2.
Altogether, this amounts to a total of 3690 possible features in the input space X.
3 Using
these lexicons alone, the same task with the same test data, yields: precision = 22%, recall = 76%.
4 Note
that no adaptaion work has been conducted on our tagger, which explains this error.
Feature Value LexGENE 1 ADJ 1 LC drosophila 1 LC suppressor 1 LC PREP 2 RC CONJ 1 RC SPECIES 1 RC PRON 1 RC PREP 1 Table 3: Featuresof hairless in \...of Drosophila suppressor of hairless and of its human...".
6.2 Results
In our experiments, wehave used the following methods:  SVM trained with inductive inference, and using a linear kernel, a polynomial kernel of degree d = 2 and the so-called \radial basis function" kernel (Scholkopf and Smola, 2002).
 SVM trained with transductive inference, and using a linear kernel or a polynomial kernel of degree d =2.
 SVM trained with inductive inference using Fisher kernels estimated fromthe whole training data (without using labels), with dierentnumber of classes c in the PLSA model (4).
The proportion of labelled data is indicated in the tables of results.
For SVM with inductive inference, only the labelled portion is used.
For transductive SVM (TSVM), the remaining, unlabelled portion is used (without the labels).
For the Fisher kernels (FK), an unsupervised model is estimated on the full dataset using PLSA, and a SVM is trained with inductive inference on the labelled data only, using the Fisher kernel as similarity measure.
6.3 Transductive
inference Table 4 gives interesting insightinto the effect of transductive inference.
As expected, in the limit where little unannotated data is used (100% in the table), there is little to gain from using transductive inference.
Accordingly,performance is roughly equivalent 5 for SVM and % annotated: 1.5% 6% 24% 100% SVM (lin) 41.22 45.34 49.67 62.97 SVM (d=2) 40.97 46.78 52.12 62.69 SVM (rbf) 42.51 49.53 51.11 63.96 TSVM (lin) 38.63 51.64 61.84 62.91 TSVM (d=2) 43.88 52.38 55.36 62.72 Table 4: F 1 scores(in %) using dierent proportionsofannotateddataforthefollowing models: SVM with inductive inference (SVM) and linear (lin) kernel, second degree polynomial kernel (d=2), and RBF kernel (rbf);; SVM with transductive inference (TSVM) and linear (lin) kernel or second degree polynomial (d=2) kernel.
TSVM, with a slightadvantage for RBF kernel trained with inductive inference.
Interestingly, in the other limit, ie when very little annotated data is used, transductive inference does not seem to yield a marked improvementover inductive learning.
This nding seems somehow at odds with the results reported by Joachims (1999) on a dierent task (text categorisation).
Weinterpret this result as a side-eect of the search strategy, where one tries to optimise both the size of the margin and the labelling of the unannotated examples.
In practice, an exact optimisation over this labelling is impractical, and when a large amount of unlabelled data is used, there is a risk that the approximate, sub-optimal search strategy described by Joachims (1999)mayfail toyield a solution that is markedly better that the result of inductive inference.
For the twointermediate situation, however, transductive inference seems to provide a sizeable performanceimprovement.
Using only 24% of annotated data, transductive learning is able totrain alinear kernel SVM thatyields approximately the same performance as inductive inference on the full annotated dataset.
This means that we get comparable performance using only what corresponds to about 30 abstracts, compared to the 122 of the full training set.
6.4 Fisher
kernels The situation is somewhat dierent for SVM trained with inductive inference, but using 5 Performance is not strictly equivalent because SVM and TSVM use the data dierently when optimising the trade-o parameter C over a validation set.
% annotated: 1.5% 6% 24% 100% SVM (lin) 41.22 45.34 49.67 62.97 SVM (d=2) 40.97 46.78 52.12 62.69 lin+FK8 46.08 42.83 54.59 63.92 lin+FK16 44.43 40.92 55.70 63.76 lin+combi 46.38 38.10 52.74 63.08 Table 5: F 1 scores(in %) using dierent proportions of annotated data for the following models: standard SVM with linear (lin) and second degree polynomial kernel (d=2);; Combination of linear kernel and Fisher kernel obtained from a PLSA with 4 classes (lin+FK4) or 8 classes (lin+FK8), and combination of linear and all Fisher kernels obtained from PLSA using 4, 8, 12 and 16 classes (lin+combi).
Fisher kernels obtained from a model of the entire (non-annotated) dataset.
As the use of Fisher kernels alone was unable to consistently achieve acceptable results, the similarity we used is a combination of the standard linear kernel and the Fisher kernel (a similar solution was advocate by Hofmann (2000)).
Table 5 summarises the results obtained using several types of Fisher kernels, depending on howmany classes were used in PLSA.
FK8 (resp.
FK16) indicates the model using 8 (resp.
16) classes, while combi is a combination of the Fisher kernels obtained using 4, 8, 12 and 16 classes.
The eect of Fisher kernels is not as clear-cut as that of transductive inference.
For fully annotated data, we obtain results that are similar to the standard kernels, although often better than the linear kernel.
Results obtained using 1.5%and 6%annotateddataseem somewhatinconsistent, whith a large improvement for 1.5%, but a marked degradation for 6%, suggesting that in that case, adding labels actually hurts performance.
We conjecture that this maybe an artifact of the speci c annotated set we selected.
For 24% annotated data, the Fisher kernel provides results that are inbetween inductive and transductive inference using standard kernels.
7 Discussion
The results of our experiments are encouraging in that they suggest that both transductive inference and the use of Fisher kernels are potentially eectiveway of taking unannotated data into account to improve performance.
These experimental results suggestthe following remark.
Note that Fisher kernels can be implemented by a simple scalar product (linear kernel) between Fisher scores r`(x) (equation 3).
The question arises naturally as to whether using non-linear kernels may improve results.
One one hand, Fisher kernels are derived from information-geometric arguments (Jaakkola and Haussler, 1999) which require that the kernel reduces to an inner-product of Fisher scores.
On the other hand, polynomial and RBF kernels often display better performance than a simple dot-product.
In order to test this, wehave performed experiments using the same features as in section 6.4, but with a second degree polynomial kernel.
Overall, results are consistently worse than before, which suggest that the expression of the Fisher kernel as the inner product of Fisher scores is theoretically well-founded and empirically justi ed.
Among possible future work, let us mention the following technical points: 1.
Optimising the weight of the contributions of the linear kernel and Fisher kernel, eg as K(x;;y)=hx;;yi +(1; )FK(x;;y),  2 [0;;1].
2. Understanding why the Fisher kernel alone (ie without interpolation with the linear kernel) is unable to provide a performance boost, despite attractive theoretical properties.
In addition, the performance improvement obtained by both transductive inference and Fisher kernels suggest to use both in cunjunction.
To our knowledge, the question of whether this would allow to \bootstrap" the unlabelled data by using them twice (once for estimating the kernel, once in transductive learning) is still an open research question.
Finally, regarding the application that we have targeted, namely entity recognition, the use of additional unlabelled data may help us to overcome the current performance limit on our database.
None of the additional experiments conducted internally using probabilisitc models and symbolic, rule-based methods have been able to yield F 1 scores higher than 63-64% on the same data.
In order to improve on this, wehave collected several hundred additional abstracts by querying the MedLine database.
After pre-processing, this yields more than a hundred thousand (unlabelled) candidates that wemay use with transductive inference and/or Fisher kernels.
8 Conclusion
In this paper, we presented a comparison between two state-of-the-art methods to combine labelled and unlabelled data: Fisher kernels and transductive inference.
Our experimental results suggest that both method are able to yield a sizeable improvement in performance.
For example transductive learning yields performance similar to inductive learning with only about a quarter of the data.
These results are very encouraging for tasks where annotation is costly while unannotated data is easy to obtain, like our task of biological entity recognition.
In addition, it provides a way to bene t from the availability of large electronic databases in order to automatically extract knowledge.
9 Acknowledgement
We thank Anne Schiller,  Agnes Sandor and Violaine Pillet for help with the data and related experimental results.
This researchwas supported by the European Commission under the KerMIT project no.
IST-2001-25431 and the French Ministry of Research under the BioMIRE project, grant 00S0356.
References M.
E. Cali, editor.
1999. Proc.
AAAI Workshop on Machine Learning for Information Extraction.
AAAI Press.
M. Craven and J.
Kumlien. 1999.
Constructing biological knowledge bases by extracting information from text sources.
In Proc.
ISMB'99. A.Gammerman, V.Vovk, and V.
Vapnik. 1998.
Learning by transduction.
In Cooper and Morla, eds, Proc.
Uncertainty in Arti cial Intelligence,pages145155.MorganKaufmann.
Eric Gaussier, Cyril Goutte, Kris Popat, and Francine Chen.
2002. A hierarchical model for clustering and categorising documents.
In Crestani, Girolami, and van Rijsbergen, eds, Advances in Information Retrieval| Proc.
ECIR'02, pages 229247.
Springer. Thomas Hofmann.
1999. Probabilistic latent semantic analysis.
In Proc.
Uncertainty in Arti cial Intelligence,pages289296.Morgan Kaufmann.
Thomas Hofmann.
2000. Learning the similarity of documents: An information-geometric approach to document retrieval and categorization.
In NIPS*12, page 914.
MIT Press.
Tommi S.
Jaakkola and David Haussler.
1999. Exploiting generative models in discriminative classi ers.
In NIPS*11, pages 487493.
MIT Press.
Thorsten Joachims.
1999. Transductive inference for text classi cation using support vector machine.
In Bratko and Dzeroski, eds, Proc.
ICML'99, pages 200209.
Morgan Kaufmann.
M. Krauthammer, A.
Rzhetsky,P.
Morozov, and C.
Friedman. 2000.
Using blast for identifying gene and protein names in journal articles.
Gene. Y.
Ohta, Y.
Yamamoto, T.
Okazaki, I.
Uchiyama, andT.Takagi. 1997.
Automatic constructing of knowledge base from biological papers.
In Proc.
ISMB'97. D.
Proux, F.
Reichemann, and L.
Julliard. 2000.
A pragmatic information extraction strategy for gathering data on genetic interactions.
In Proc.
ISMB'00. Bernhard Scholkopf and Alexander J.
Smola. 2002.
Learning with Kernels.
MIT Press.
J. Thomas, D.
Milward, C.
Ouzounis, S.
Pulman, and M.
Caroll. 2000.
Automatic extraction of protein interactions from scienti c abstracts.
In Proc.
PSB 2000.
Kristina Toutanova,Francine Chen, Kris Popat, and Thomas Hofmann.
2001. Text classi cation in a hierarchical mixture model for small training sets.
In Proc.
ACM Conf.
Information and Knowledge Management.
Vladimir N.
Vapnik. 1998.
Statistical Learning Theory.Wiley.
Yiming Yang and Xin Liu.
1999. A reexamination of text categorization methods.
In Proc.
22nd ACM SIGIR, pages 4249 .

