F. J. Och. 2003. Minimum error rate training for sta-
J03-1002 P03-1021
mum error-rate (MER) training (Och, 2003) was ap-
J03-1002 P03-1021
F. Och and H. Ney. 2003. A systematic comparison of
J03-1002 P03-1021
Y. Chan, H. Ng, and D. Chiang. 2007. Word sense dis-
P07-1005
gual, target-language corpora (Brants et al., 2007).
D07-1090
METEOR (Banerjee and Lavie, 2005, version 0.6).5
W05-0909
P. Koehn. 2004. Statistical significance tests for machine
W04-3250
D.Vickrey, L.Biewald, M.Teyssier, andD.Koller. 2005.
H05-1097
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
P02-1040
Bikel, 2004).
W04-3224
using numerical search (Och, 2003).
J03-1002 P03-1021
We follow the approach of Koehn et al. (2003),
N03-1017
D. M. Bikel. 2004. A distributional analysis of a lexical-
W04-3224
(Och, 2003) to obtain interpolation weights λm.
J03-1002 P03-1021
model (Chan et al., 2007). The context fea-
P07-1005
ing beam search (Koehn et al., 2003). The weights
N03-1017
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
W05-0909
D. Gildea. 2001. Corpus variation and parser perfor-
W01-0521
icalized grammar rules in statistical parsing (Gildea, 2001;
W01-0521
(Och and Ney, 2003), apply the “grow-diag-final-
J03-1002 P03-1021
corpora (Koehn et al., 2003) and, like the origi-
N03-1017
M. Carpuat and D. Wu. 2007. Improving statistical ma-
D07-1007
from freely-available tools (Koehn et al., 2007).
P07-2045
and Wu (2007) and Chan et al. (2007) embedded
P07-1005
data (e.g., the BLEU score; Papineni et al., (2002))
P02-1040
ward our approach. For example, Vickrey et al.(2005) built classifiers inspired by those used in
H05-1097
paired bootstrap (Koehn, 2004) with 1000 samples
W04-3250
translation system (Brants et al., 2007) is likely due
D07-1090
nal IBM models (Brown et al., 1990), benefit from
J90-2002
D. Chiang. 2005. A hierarchical phrase-based model for
P05-1033
(“minimum error-rate training,” Och, 2003), so that
J03-1002 P03-1021
P. Koehn, F. J. Och, and D. Marcu. 2003. Statisti-
J03-1002 N03-1017 P03-1021
