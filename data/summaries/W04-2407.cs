However, since most previous studies instead use the mean attachment score per word (Eisner, 1996; Collins et al., 1999), we will give this measure as well.
C96-1058 P99-1065
Model Labeled Unlabeled MCLE 74.7 (72.3) 81.5 (79.7) MBL non-lexical 76.5 (74.7) 82.9 (81.7) MBL lexical 81.7 (80.6) 85.7 (84.7) Table 4: Parsing accuracy for MCLE and MBL models, attachment score per sentence (per word in parentheses) If we compare the results concerning parsing accuracy to those obtained for other languages (given that there are no comparable results available for Swedish), we note that the best unlabeled attachment score is lower than for English, where the best results are above 90% (attachment score per word) (Collins et al., 1999; Yamada and Matsumoto, 2003), but higher than for Czech (Collins et al., 1999).
P99-1065
Thus, the Penn Treebank of American English (Marcus et al., 1993) has been used to train and evaluate the best available parsers of unrestricted English text (Collins, 1999; Charniak, 2000).
A00-2018 J93-2004 P99-1065
This is different from the original IB1 algorithm, as described in Aha et al.(1991). 1999; Charniak, 2000).
A00-2018
Eisner. 1996.
C96-1058
More precisely, parsing accuracy is measured by the attachment score, which is a standard measure used in studies of dependency parsing (Eisner, 1996; Collins et al., 1999).
C96-1058 P99-1065
Charniak. 2000.
A00-2018
Unlike most previous work on data-driven dependency parsing (Eisner, 1996; Collins et al., 1999; Yamada and Matsumoto, 2003; Nivre, 2003), we assume that dependency graphs are labeled with dependency types, although the evaluation will give results for both labeled and unlabeled representations.
C96-1058 P99-1065
