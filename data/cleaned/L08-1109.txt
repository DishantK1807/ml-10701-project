<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Christelle Ayache</author>
<author>Brigitte Grau</author>
<author>Anne Vilnat</author>
</authors>
<title>Equer : the french evaluation campaign of question answering system equer/evalda</title>
<date>2006</date>
<booktitle>In 5th international Conference on Language Resources and Evaluation (LREC</booktitle>
<pages>1157--1160</pages>
<location>Genoa, Italy</location>
<contexts>
<context>NII Test Collection for IR systems) includes a QA track in three languages and the French Technolangue EQueR4(Evaluation en Questions Réponses) evaluation focused on 500 questions made from the data (Ayache et al., 2006). But the questions asked in those campaigns are manually checked for being well formed and complete enough. We aim to test QA systems on their ability to answer questions spontaneously typed by peop</context>
</contexts>
<marker>Ayache, Grau, Vilnat, 2006</marker>
<rawString>Christelle Ayache, Brigitte Grau, and Anne Vilnat. 2006. Equer : the french evaluation campaign of question answering system equer/evalda. In 5th international Conference on Language Resources and Evaluation (LREC 2006), pages 1157–1160, Genoa, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Crestani</author>
</authors>
<title>Effects of word recognition in spoken query processing</title>
<date>2000</date>
<booktitle>In Proceedings of the IEEE Advances in Digital Libraries</booktitle>
<location>Washington, D.C, USA</location>
<contexts>
<context>stem robustness in more real life uses. Related experiments already done in document retrieval field have been testing the robustness of search engines with automatic transcription of spoken queries (Crestani, 2000) or with automatically degraded text entries (Ruch, 2002). The Confusion track (Kantor and Voorhees, 1997) of TREC evaluation campaing focuses on difficult queries for document retrieval. The idea of</context>
</contexts>
<marker>Crestani, 2000</marker>
<rawString>Fabio Crestani. 2000. Effects of word recognition in spoken query processing. In Proceedings of the IEEE Advances in Digital Libraries, Washington, D.C, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurent Gillard</author>
<author>Laurianne Sitbon</author>
<author>Eric Blaudez</author>
<author>Patrice Bellot</author>
<author>Marc El-Bèze</author>
</authors>
<title>Relevance measures for question answering, the lia at qa@clef2006. LNCS special issue</title>
<date>2007</date>
<pages>440--449</pages>
<contexts>
<context>of the process has been proposed by (Moldovan et al., 2003) where they analyse the causes of failure with standard questions only. The work we present here focuses on the evaluation of our QA system (Gillard et al., 2007) through a corpus created on purpose. This paper will first present the objectives, the protocol and some observations made on this corpus of semi-spontenaously typed questions. The second section wi</context>
<context>ten restraining the amount of data in this area. The experiment is composed of 20 questions selected from EQUER French evaluation campaign. The selected questions were some right answered by SQuALIA (Gillard et al., 2007). 8 of them contain proper nouns. 2 of them contain foreign low frequency proper nouns. The covered focuses are: person name (5 questions), number (5 questions), date (3 questions), location (2 quest</context>
</contexts>
<marker>Gillard, Sitbon, Blaudez, Bellot, El-Bèze, 2007</marker>
<rawString>Laurent Gillard, Laurianne Sitbon, Eric Blaudez, Patrice Bellot, and Marc El-Bèze. 2007. Relevance measures for question answering, the lia at qa@clef2006. LNCS special issue, pages 440–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abi James</author>
<author>EA Draffan</author>
</authors>
<title>The accuracy of electronic spell checkers for dyslexic learners. PATOSS bulletin</title>
<date>2004</date>
<contexts>
<context>ersonalised spell checking and make the system more robust by providing better entries. If such systems can be used in order to correct typos, this may not work as well for users with special needs. (James and Draffan, 2004) highlights the low accuracy of standard spellcheckers for dyslexics. The evaluation of the whole QA process including rewriting with classical spell checkers or with advanced rewriting scheme still </context>
</contexts>
<marker>James, Draffan, 2004</marker>
<rawString>Abi James and EA Draffan. 2004. The accuracy of electronic spell checkers for dyslexic learners. PATOSS bulletin, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul B Kantor</author>
<author>Ellen Voorhees</author>
</authors>
<title>Report on the trec-5 confusion track</title>
<date>1997</date>
<booktitle>In TREC-5 Text REtrieval Conference No5</booktitle>
<pages>65--74</pages>
<location>Gaithersburg, MD</location>
<contexts>
<context>field have been testing the robustness of search engines with automatic transcription of spoken queries (Crestani, 2000) or with automatically degraded text entries (Ruch, 2002). The Confusion track (Kantor and Voorhees, 1997) of TREC evaluation campaing focuses on difficult queries for document retrieval. The idea of analysing the performance of a QA system at the different steps of the process has been proposed by (Mold</context>
</contexts>
<marker>Kantor, Voorhees, 1997</marker>
<rawString>Paul B. Kantor and Ellen Voorhees. 1997. Report on the trec-5 confusion track. In TREC-5 Text REtrieval Conference No5,, pages 65–74, Gaithersburg, MD , ETATSUNIS. Bernard Lété, Liliane Sprenger-Charolles, and P. Colé.</rawString>
</citation>
<citation valid="true">
<title>Manulex : A grade-level lexical database from french elementary-school readers</title>
<date>2004</date>
<journal>Behavior Research Methods, Instruments, and Computers</journal>
<pages>36--156</pages>
<marker>2004</marker>
<rawString>2004. Manulex : A grade-level lexical database from french elementary-school readers. Behavior Research Methods, Instruments, and Computers, 36:156–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Marius Pasca</author>
<author>Sanda Harabagiu</author>
<author>Mihai Surdeanu</author>
</authors>
<title>Performance issues and error analysis in an open-domain question answering system</title>
<date>2003</date>
<journal>ACM Transactions On Information Systems</journal>
<volume>21</volume>
<contexts>
<context>1997) of TREC evaluation campaing focuses on difficult queries for document retrieval. The idea of analysing the performance of a QA system at the different steps of the process has been proposed by (Moldovan et al., 2003) where they analyse the causes of failure with standard questions only. The work we present here focuses on the evaluation of our QA system (Gillard et al., 2007) through a corpus created on purpose.</context>
</contexts>
<marker>Moldovan, Pasca, Harabagiu, Surdeanu, 2003</marker>
<rawString>Dan Moldovan, Marius Pasca, Sanda Harabagiu, and Mihai Surdeanu. 2003. Performance issues and error analysis in an open-domain question answering system. ACM Transactions On Information Systems, 21(2):133–154, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Ruch</author>
</authors>
<title>Using contextual spelling correction to improve retrieval effectiveness in degraded text collections</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational Linguistics</booktitle>
<volume>1</volume>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics</institution>
<location>Taipei, Taiwan</location>
<contexts>
<context>lready done in document retrieval field have been testing the robustness of search engines with automatic transcription of spoken queries (Crestani, 2000) or with automatically degraded text entries (Ruch, 2002). The Confusion track (Kantor and Voorhees, 1997) of TREC evaluation campaing focuses on difficult queries for document retrieval. The idea of analysing the performance of a QA system at the differen</context>
<context>egories of users. The document retrieval step has not been studied here since documents were furnished with evaluation campaign data. The robustness of this specific task has already been evaluated. (Ruch, 2002) shows that the mean average precision of the document retrieval system SMART drops by 18.7% on a document retrieval task when at 15% of the words of the queries are automatically corrupted. 2.1. Eva</context>
</contexts>
<marker>Ruch, 2002</marker>
<rawString>Patrick Ruch. 2002. Using contextual spelling correction to improve retrieval effectiveness in degraded text collections. In Proceedings of the 19th international conference on Computational Linguistics, volume 1, pages 1–7, Taipei, Taiwan. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Kiyoshi Sudo</author>
<author>Chikashi Nobata</author>
</authors>
<date>2002</date>
<contexts>
<context> wrong or underspecified (assigned a too general category), the question answering system is likely to fail (Sitbon et al., 2006). SQuaLIA bases the categorisation on Sekine’s named entity hierarchy (Sekine et al., 2002) that identifies 150 different categories. For most of our users, the expected answer type computed by the system is incorrect or missing for 20% to 40% of typed questions. On average the system was </context>
</contexts>
<marker>Sekine, Sudo, Nobata, 2002</marker>
<rawString>Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata. 2002.</rawString>
</citation>
<citation valid="true">
<title>Extended named entity hierarchy</title>
<date>2002</date>
<booktitle>In Actes de LREC</booktitle>
<location>Spain</location>
<marker>2002</marker>
<rawString>Extended named entity hierarchy. In Actes de LREC 2002, Las Palmas, Canary Islands, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurianne Sitbon</author>
<author>Laurent Gillard</author>
<author>Jens Grivolla</author>
<author>Patrice Bellot</author>
<author>Philippe Blache</author>
</authors>
<title>Vers une prédiction automatique de la difficulté d’une question en langue naturelle</title>
<date>2006</date>
<booktitle>In 13ième conférence Traitement Automatique des Langues Naturelles (TALN</booktitle>
<pages>337--346</pages>
<location>Louvain, Belgique</location>
<contexts>
<context> ...) to very specific (Inventor, River, ...). If the expected answer type is not detected, wrong or underspecified (assigned a too general category), the question answering system is likely to fail (Sitbon et al., 2006). SQuaLIA bases the categorisation on Sekine’s named entity hierarchy (Sekine et al., 2002) that identifies 150 different categories. For most of our users, the expected answer type computed by the s</context>
</contexts>
<marker>Sitbon, Gillard, Grivolla, Bellot, Blache, 2006</marker>
<rawString>Laurianne Sitbon, Laurent Gillard, Jens Grivolla, Patrice Bellot, and Philippe Blache. 2006. Vers une prédiction automatique de la difficulté d’une question en langue naturelle. In 13ième conférence Traitement Automatique des Langues Naturelles (TALN), pages 337–346, Louvain, Belgique, 10-13 April. 8OpenEphyra QA system is now available into open source (http://www.ephyra.info/) Laurianne Sitbon, Patrice Bellot, and Philippe Blache.</rawString>
</citation>
<citation valid="true">
<title>Phonetic based sentence level rewriting of questions typed by dyslexic spellers in an information retrieval context</title>
<date>2007</date>
<booktitle>In Proceedings of Interspeech Eurospeech</booktitle>
<location>Antwerp, Belgium</location>
<marker>2007</marker>
<rawString>2007. Phonetic based sentence level rewriting of questions typed by dyslexic spellers in an information retrieval context . In Proceedings of Interspeech Eurospeech 2007, Antwerp, Belgium.</rawString>
</citation>
</citationList>
</algorithm>

