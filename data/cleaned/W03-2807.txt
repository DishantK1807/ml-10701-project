1:102	Adaptation of the F-measure to Cluster Based Lexicon Quality Evaluation   Angelo Dalli NLP Research Group Department of Computer Science University of Sheffield a.dalli@dcs.shef.ac.uk   Abstract An external lexicon quality measure called the L-measure is derived from the F-measure (Rijsbergen, 1979; Larsen and Aone, 1999).
2:102	The typically small sample sizes available for minority languages and the evaluation of Semitic language lexicons are two main factors considered.
3:102	Large-scale evaluation results for the Maltilex Corpus are presented (Rosner et al., 1999).
4:102	1 Introduction Computational Lexicons form a fundamental component of any NLP system.
5:102	Unfortunately, good quality lexicons are hard to create and maintain.
6:102	The labour intensive process of lexicon creation is further compounded when minority languages are concerned.
7:102	Inevitably, computational lexicons for minor languages tend to be quite small when compared to computational lexicons available for more common languages such as English.
8:102	The Maltilex Corpus is used in this paper to evaluate a cluster based lexicon quality measure adapted from the F-measure.
9:102	The Maltilex Corpus is the first large-scale computational lexicon for Maltese (Rosner et al., 1999).
10:102	The choice of Maltese as the evaluation language presented some additional problems due to the Semitic morphology and grammar of Maltese (Mifsud, 1995).
11:102	An innovative approach to lexicon creation using an automated technique called the Lexicon Structuring Technique (LST) was used to create an initial computational lexicon from a wordlist (Dalli, 2002a).
12:102	LST decreased the amount of work that is normally required to create a lexicon from scratch by adapting a number of clustering, alignment, and approximate matching techniques to produce a set of clusters containing related wordforms.
13:102	Lexicon clusters are thus analogous to lemmas in more traditional lexicons.
14:102	This approach has many advantages for a language having a Semitic morphology and grammar due to the large number of wordforms that can be derived for a single lemma.
15:102	Instead of processing every wordform individually, the whole cluster can be treated as a single entity, reducing processing requirements significantly.
16:102	The close relationship of this lexicon definition and standard clustering systems (with lemmas corresponding to clusters), enabled the reuse of cluster quality evaluation measures to the task of lexicon quality evaluation.
17:102	There are two main ways of evaluating cluster quality which are summarised in (Steinbach et al., 1999 pg.
18:102	6) as follows:   Internal Quality Measure  Clusters are compared without reference to external knowledge against some predefined set of desirable qualities.
19:102	 External Quality Measure  Clusters are compared to known external classes.
20:102	Internal quality measures are not always desirable, since their very existence implies that better quality can be achieved by applying an internal quality measure in conjunction with some optimisation technique.
21:102	An internal quality measure for cluster-based lexicons was not available either.
22:102	The two main external quality measures applicable lexicon quality evaluation tasks are entropy (Shannon, 1948) and the F-measure (van Rijsbergen, 1979; Larsen and Aone, 1999).
23:102	Entropy based quality measures assert that the best entropy that can be obtained is when each cluster contains the optimal number of members.
24:102	In our context this corresponds to having clusters (corresponding to lemmas) that contain exactly all the wordforms associated with that cluster.
25:102	The class distribution of the data is calculated by considering the probability of every member belonging to some class.
26:102	The entropy of every cluster j is calculated using the standard entropy formula () () Ej p p ij ij i =  log  where p ij  denotes the probability that a member of cluster j belongs to class i. The total entropy is then calculated as  ()E n nEj j j m * = =  1 1  where n j  is the size of cluster j, m the number of clusters, and n the total number of data points.
27:102	The F-measure treats every cluster as a query and every class as the desired result set for a query.
28:102	The recall and precision values for each given class are then calculated using information retrieval concepts.
29:102	The F-measure of cluster j and class i is given by () ()() () () Fi j ri j pi j ri j pi j , ,, ,, =  + 2  where r denotes recall and p the precision.
30:102	Recall is defined as  ()ri j n n ij i , =  and precision is defined as  ()pi j n n ij j , =  where n ij  is the number of class i members in cluster j, while n j  and n i  are the sizes of cluster j and class i respectively.
31:102	The overall F-measure for the entire data set of size n is given by () [] F n n Fi j i i * max ,=  . 2 Lexicon Quality Measure Computational lexicons have an additional domain-specific external quality measure available in the form of existing non-computational language dictionaries.
32:102	Dictionaries can be used to compare the results generated by the automated system against those produced by human experts.
33:102	Generally it can be assumed that reputable printed dictionaries are of a very high quality and thus provide a gold standard for comparison.
34:102	For some languages, especially minority languages, the only available quality data would be in printed dictionary form.
35:102	Unfortunately most noncomputational dictionaries are not amenable to automated analysis techniques since the process of re-inputting and re-structuring data into a computational dictionary format is generally so labour intensive that it becomes too expensive.
36:102	Additionally, since every cluster and class correspond to a lemma, the number of classes to be considered is expected to number in the thousands.
37:102	This would make a straightforward application of the F-measure an overly long process.
38:102	A modified statistical sampling technique based on the F-measure that gives results that are approximately as good as the full application of the F-measure and that caters for the particular nuances of lexicon quality evaluation is thus needed.
39:102	The L-measure is such a new measure based on the F-measure that attempts to measure the quality of a given lexicon in relation to other existing lexicons that are possibly noncomputational lexicons (i.e. human compiled language dictionaries), taking into consideration that a full population analysis may not be practical under most circumstances.
40:102	2.1 Lexicon Extraction from Dictionaries The L-Measure works by comparing two lexicons, one derived from a gold standard representation in the form of human compiled dictionaries and the other being a computational lexicon whose quality is being assessed.
41:102	In order to avoid confusion, formal definitions of the terms dictionary, lexicon and wordlists are now presented.
42:102	A dictionary D is formally modeled as a sequence <t 1   t h > of tuples of the form (l, def) where l denotes a lemma (i.e. a dictionary headword in a more traditional sense) and def is a 5tuple (m, r, c, i, o) with m containing morphological information that enables members of the lemma to be inferred or generated, r a set of relations to other lemmas, c a description of the different contexts where the lemma may be normally used, i containing meta-information about lemma l itself, and o an object containing additional information (such as etymology, examples of common use, etc.) Since multiple entries of the same headword may be present in D the sequence is not injective, i.e. the sequence can contain duplicate elements.
43:102	The main two differences between a dictionary and a lexicon are that different types of information are stored about every lemma in the def component, and secondly, that a lexicon has an injective sequence of tuples (i.e. a sequence that does not have duplicates and where the exact order is important) while a dictionary does not (since a dictionary does not need to force a headword to have one unique entry, especially in the case of printed dictionaries that often have the same headword appearing in multiple top-level entries).
44:102	A dictionary D can be thus transformed into a lexicon L, denoted by L = lex(D), by filtering the tuple sequence <t 1   t h > making up D to include only the l components of every tuple.
45:102	The filtered sequence is then transformed into an injective sequence of unique lemmas <l 1   l u >, satisfying the requirements for a lexicon.
46:102	Appropriate transformations have to be defined to transform the def component from dictionary to lexicon format.
47:102	The sequence of lemmas is then expanded to a canonical wordlist W. A canonical wordlist W is a sequence <w 1   w u > of sets of strings generated from a lexicon L, denoted by W = can(L), by listing all possible instances of every lemma in the lexicon (i.e. all possible wordforms of a particular lemma), in effect creating a full form lexicon.
48:102	The canonical wordlist W thus has u sets of strings corresponding to u lemmas in the lexicon.
49:102	The particular lemma used to generate a wordform w is obtained by the operator lem(w).
50:102	The sequence of lemmas used to generate W is denoted as lemmas(W).
51:102	The union of two wordlists W 1   W 2  is defined to be the union of all sets of strings in both wordlists, i.e. jiji yxWWWyWx = 2121 , provided that lem(x i ) = lem(y j )  lem(x j )  lemmas(W 2 )  lem(y j )  lemmas(W 1 ) holds.
52:102	This definition ensures maximum coverage of the resulting canonical wordlist.
53:102	An empty or null canonical wordlist results if no pair of strings obey the previously stated condition while the union of a wordlist with a null wordlist is the original wordlist itself.
54:102	Similarly the intersection of two wordlists W 1   W 2  is defined to be the union of all sets of strings in both wordlists that have corresponding lemmas appearing in both wordlists, i.e.     = xWyWWW x y ij ij1212 , provided that lem(x i ) = lem(y j ) holds.
55:102	Note that this definition is concerned mainly with the lemmas and their associated wordforms themselves.
56:102	Since lexicons are not just a list of lemmas and wordforms, other linguistic annotations will have to be evaluated using other techniques appropriate to the particular linguistic annotations added to the lemma entries.
57:102	2.2 L-Measure Definition Given a lexicon L and a set of dictionaries D = {D 1   D k } transform the set of dictionaries D into a set of lexicons L' = {L 1   L k } using the lex transformation on every dictionary, thus () U k i DlexL 1 '= . Define W as the canonical wordlist obtained from L, W = can(L) and W' as the canonical wordlist obtained from L', () U k i LcanW 1 '=  under canonical wordlist union.
58:102	Define Y to be the canonical wordlist of words common to both W and W', Y = W  W'.
59:102	The sample size S used for the L-measure is defined as .|lemmas(Y)| where  is some value in the range (01) that controls the random sample size.
60:102	Typically  should be set to somewhere between 0.01 and 0.1.
61:102	It is expected that the sample size will be large enough to assume that the sample is representative of the whole population.
62:102	The L-measure of a lemma j in lemmas(W) and lemma i in lemmas(Y) is given by () ()() () () Li j ri j pi j ri j pi j , ,, ,, =  + 2  where r denotes recall and p is the precision.
63:102	Recall is defined as ()ri j n n ij i , =  and precision is defined as ()pi j n n ij j , =  where n ij  is the number of lemma i members in lemma j, while n j  and n i  are the sizes of lemma j and lemma i respectively.
64:102	The overall L-measure for the entire sample of size n is given by () [] L n n Li j i i * max ,=  . L *  is always in the range [01] and is proportional to the lexicon quality, with an L *   score of 1 representing a perfect quality lexicon with respect to the lexicon being used as a standard.
65:102	Y is used instead of W' since lexical word coverage is largely determined by the quality of the corpus used to create the lexicon.
66:102	While this kind of analysis might be useful in determining the coverage of a lexicon the L-measure is oriented towards measuring quality rather than quantity, independently of the corpus that was used to create the lexicon.
67:102	3 Results The L-measure has been used to measure the quality of the Maltilex Computational Lexicon in relation to existing paper based dictionaries.
68:102	The most comprehensive dictionary of Maltese was used to produce L', the comparison standard lexicon (Aquilina, 1987-1990).
69:102	The capability of the L-measure to work with a statistical sample made a manual analysis of results possible without having L' in digital form.
70:102	The value for the sample size S was determined through a parameter  that was set to 0.01, meaning that 1% of all lemmas in the Maltilex Computational Lexicon were covered by the statistical sample.
71:102	Since around 63,000 lemmas exist in the combined lexicon the sample size S was determined to be 630.
72:102	The set of 630 lemmas chosen at random from the Maltilex Corpus contained a total of 5,887 wordforms taken from the combined lexicon.
73:102	The precision and recall for the samples were calculated individually to obtain the individual Lmeasure for a range of lemmas.
74:102	A fully worked out example of the calculation of the L-measure for the lemma missier (father) is given.
75:102	Lemmas in the Maltilex Computational Lexicon are aligned automatically using a technique adopted from bioinformatics and hence the presentation of the wordforms in their aligned format (Dalli, 2000b; Gusfield, 1997).
76:102	The lemma missier (the Maltese word for father with the cluster showing different forms like my father, your father, etc.) taken from the Maltilex Computational Lexicon, which represents lemma i, contains seven members as displayed below:  m i s s ie r _ _ _ _ _ _  _ _ _ m i s s ie r e k _ _ _ _  _ _ _ m i s s ie r _ _ _ n _ a  _ _ _ m i s s ie r _ k o m _ _  _ _ _ m i s s i  r i _ _ _ j ie t n a m i s s ie r i _ _ _ _ _  _ _ _ m i s s ie r _ h o m _ _  _ _ _  The lemma missier, taken from Aquilinas Dictionary, which represents lemma j, can be used to generate the following ten members as displayed below:  m i s s ie r _ _ _ _ _ _  _ _ _ m i s s ie r e k _ _ _ _  _ _ _ m i s s ie r _ _ _ n _ a  _ _ _ m i s s ie r _ k o m _ _  _ _ _ m i s s i  r i _ _ _ j ie t n a m i s s ie r i _ _ _ _ _  _ _ _ m i s s ie r a _ _ _ _ _  _ _ _ m i s s ie r _ _ u _ _ _  _ _ _ m i s s ie r _ h o m _ _  _ _ _ m i s s i  r i _ _ _ j ie t _ _  For this example, n j  and n i  are thus equal to 10 and 7 respectively.
77:102	Recall and precision values are calculated as ()1 7 ', =missiermissierr  ()7.0 10 7 ', ==missiermissierp respectively.
78:102	The L-measure for the lemma missier is () 8235.0 7.1 4.1 7.01 7.012 ', == +  =missiermissierL The overall L-measure for the entire sample of 5,887 wordforms is given by ()[]  = i i jiL n L ,max 5887 * . The contribution of the lemma missier to the final L *  score is thus given by 8235.0 5887 7 = 0.000979226.
79:102	A high precision floating point library was used to represent the individual contribution values since these are generally very small.
80:102	Figures 1 and 2 show the precision and recall curves for the whole sample respectively.
81:102	0 0.2 0.4 0.6 0.8 1 1.2 1 32 63 94 125 156 187 218 249 280 311 342 373 404 435 466 497 528 559 590 621Lemmas  Figure 1 Precision 0 0.2 0.4 0.6 0.8 1 1.2 1 25 49 73 97 121 145 169 193 217 241 265 289 313 337 361 385 409 433 457 481 505 529 553 577 601 625Lemmas  Figure 2 Recall 0 0.2 0.4 0.6 0.8 1 1.2 1 32 63 94 125 156 187 218 249 280 311 342 373 404 435 466 497 528 559 590 621Lemmas  Figure 3 Precision and Recall Trends   Figure 3 shows moving average trendlines for precision and recall (precision is shown in a bold line on top, recall is the fainter line underneath).
82:102	The average precision was 0.91748 and the average rate of recall was 0.661359.
83:102	0 0.2 0.4 0.6 0.8 1 1.2 1 55 109 163 217 271 325 379 433 487 541 595 Figure 4 Individual L-Measure Values 0 0.2 0.4 0.6 0.8 1 1.2 1 55 109 163 217 271 325 379 433 487 541 595 Figure 5 Individual L-Measure Values Trend  Figure 4 shows the individual L-measure values for the sample.
84:102	The values displayed in Figure 4 are those used to calculate the final L *  value.
85:102	Figure 5 shows the moving average trendline for the individual L-measure values.
86:102	The average individual L-measure was 0.707256882 while the average individual contribution of a lemma to the L *  value was 0.000748924.
87:102	The variance in the L-measure individual values was 0.065504369.
88:102	The correlation between the L-measure and precision was 0.163665769 while the correlation between the L-measure and recall was 0.922214452.
89:102	The overall L *  score for the Maltilex Computational Lexicon was 0.4718.
90:102	This score is quite intuitive when the various problems in the existing Maltese corpus used to create the Computational Lexicon are considered.
91:102	This score means that the number of wordforms that are stored or that can be generated by the current lexicon needs to be expanded by around 53% in order to match the quality of the lexicon underlying Aquilinas dictionary (Aquilina, 1987-1990).
92:102	4 Conclusion The L-measure is a useful evaluation metric that can be used to measure the quality of a computational lexicon based on clustering concepts.
93:102	The small data sample required by L-measure to give meaningful results makes it a practical measure to use in a variety of situations where massive amounts of data might not be available.
94:102	This makes L-measure ideal for use in the evaluation of Language Resources for minority languages and also for quick benchmark studies that evaluate the quality of a computational lexicon as it is being created.
95:102	Compared with the F-measure, the L-measure will give highly similar results using less data.
96:102	Naturally the validity of the L-measure results depends on the choice of the  value, which in turn determines the sample size.
97:102	The lemma/cluster based approach of the Lmeasure is suitable for the evaluation of Semitic language lexicons that often prove problematic to evaluation techniques based on English or Romance languages.
98:102	The L-measure also has potential future applications in the comparison and evaluation of different lexicons.
99:102	The individual L-measure scores can also be used to identify areas of similarities and differences between different lexicons quickly.
100:102	The L-measure can also be adapted to other areas of Computational Linguistics as long as the concept of a cluster and some means of determining its precision and recall exist.
101:102	Minimal changes are needed to adapt the L-measure to other domains making future adaptations likely.
102:102	Acknowledgment This work has been made possible with the collaboration of the Maltilex Project at the University of Malta.

