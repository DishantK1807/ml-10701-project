<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>O Artemenko</author>
<author>T Mandl</author>
<author>M Shramko</author>
<author>C Womser-Hacker</author>
</authors>
<title>Evaluation of a language identification system for monoand multilingual text documents</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 ACM Symposium on Applied Computing</booktitle>
<pages>859--860</pages>
<location>Dijon, France</location>
<marker>Artemenko, Mandl, Shramko, Womser-Hacker, 2006</marker>
<rawString>O. Artemenko, T. Mandl, M. Shramko, and C. Womser-Hacker. 2006. Evaluation of a language identification system for monoand multilingual text documents. In Proceedings of the 2006 ACM Symposium on Applied Computing, pages 859–860, Dijon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W B Cavnar</author>
<author>J M Trenkle</author>
</authors>
<title>N-gram based text categorization</title>
<date>1994</date>
<booktitle>In Proceedings of the Third Annual Symposium on Document Analysis and Information Retrieval</booktitle>
<pages>161--169</pages>
<contexts>
<context>ords and their absolute frequencies of every of the 34 used languages. N-Gram-Based Approach The third type of language models is generated by the n-gram-based approach and uses n-grams of different (Cavnar and Trenkle, 1994) or fixed (Grefenstette, 1995; Prager, 1999) lengths from tokenized words. In contrast, Dunning (1994) generates n-grams of sequences of Bytes. An n-gram is a sequence of n characters. Before creatin</context>
<context> have better results for the language identification. For language models of more than 300 n-grams very good results of 99,8% were achieved. Figure 1: Out-of-Place-Measure Computation. (adapted from (Cavnar and Trenkle, 1994)) The approach of Dunning (1994) is quite similar to the one from Cavnar and Trenkle (1994). However, Dunning’s approach does not use the tokenization of the text to build the n-grams, the language m</context>
<context>e Model (Prager, 1999), Monte Carlo sampling (Poutsma, 2001), Markov Chains in combination with Bayesian Decision Rules (Dunning, 1994), Relative Entropy (Sibun and Reynar, 1996), and Ad-Hoc Ranking (Cavnar and Trenkle, 1994). Poutsma (2001) involved Monte Carlo sampling on language identification to compare it to the Ad-Hoc Ranking from Cavnar and Trenkle (1994) and the Mutual Information Statistics from Sibun and Reyna</context>
<context>a. All features are sorted by their descending frequency (rank). The features contained in the document model are successively searched in the language models. First, the single out-of-place measure (Cavnar and Trenkle, 1994) is computed by comparing the entities of the two models and their rank. Then, a value is assigned. The resulting total distance of the out-of-place measures is used for assigning the language to the</context>
<context>128 Table 3: Extract of short words contained in the language models 2.3. The Influence of the Out-of-Place Measure For comparing the language models, we used the AdHoc Ranking classification method (Cavnar and Trenkle, 1994). But analyzing the value of the out-of-place measure used in Cavnar and Trenkle (1994), we can notice that the authors do not specify the maximal value of the out-of-place measure, while Cowie et al</context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>W.B. Cavnar and J.M. Trenkle. 1994. N-gram based text categorization. In Proceedings of the Third Annual Symposium on Document Analysis and Information Retrieval, pages 161–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cowie</author>
<author>Y Ludovic</author>
<author>R Zacharski</author>
</authors>
<title>Language recognition for monoand multilingual documents</title>
<date>1999</date>
<booktitle>In Proceedings of the Vextal Conference</booktitle>
<location>Venice</location>
<marker>Cowie, Ludovic, Zacharski, 1999</marker>
<rawString>J. Cowie, Y. Ludovic, and R. Zacharski. 1999. Language recognition for monoand multilingual documents. In Proceedings of the Vextal Conference, Venice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Statistical identification of language</title>
<date>1994</date>
<tech>Technical report mccs 94-273</tech>
<institution>Computing Research Laboratory, New Mexico State University</institution>
<contexts>
<context>ssification approaches can be used for language identification like Vector Space Model (Prager, 1999), Monte Carlo sampling (Poutsma, 2001), Markov Chains in combination with Bayesian Decision Rules (Dunning, 1994), Relative Entropy (Sibun and Reynar, 1996), and Ad-Hoc Ranking (Cavnar and Trenkle, 1994). Poutsma (2001) involved Monte Carlo sampling on language identification to compare it to the Ad-Hoc Ranking</context>
</contexts>
<marker>Dunning, 1994</marker>
<rawString>T. Dunning. 1994. Statistical identification of language. Technical report mccs 94-273, Computing Research Laboratory, New Mexico State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Comparing two language identification schemes</title>
<date>1995</date>
<booktitle>In 3rd International conference On Statistical Analysis of Textual Data</booktitle>
<contexts>
<context>of every of the 34 used languages. N-Gram-Based Approach The third type of language models is generated by the n-gram-based approach and uses n-grams of different (Cavnar and Trenkle, 1994) or fixed (Grefenstette, 1995; Prager, 1999) lengths from tokenized words. In contrast, Dunning (1994) generates n-grams of sequences of Bytes. An n-gram is a sequence of n characters. Before creating n-grams of a word, its begin</context>
</contexts>
<marker>Grefenstette, 1995</marker>
<rawString>G. Grefenstette. 1995. Comparing two language identification schemes. In 3rd International conference On Statistical Analysis of Textual Data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Martino</author>
<author>R C Paulsen</author>
</authors>
<title>Natural language determination using partial words</title>
<date>2001</date>
<journal>U.S. Patent No</journal>
<volume>6216102</volume>
<pages>1</pages>
<contexts>
<context>uage model using a specific amount of the most frequent words. These words describe a set of words having the highest frequency of all words occurring in a text. Different work has been presented in (Martino and Paulsen, 2001; Souter et al., 1994) who use the most frequent one hundred words, while Cowie et al. (1999) uses the most frequent one thousand words to generated a language model. Souter et al. (1994) takes into a</context>
</contexts>
<marker>Martino, Paulsen, 2001</marker>
<rawString>M.J. Martino and R.C. Paulsen. 2001. Natural language determination using partial words. U.S. Patent No. 6216102 B1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Poutsma</author>
</authors>
<title>Applying monte carlo techniques to language identification</title>
<date>2001</date>
<booktitle>In Proceedings of Computational Linguistics in the</booktitle>
<location>Netherlands</location>
<contexts>
<context>erated document model as input for the classification method. Different classification approaches can be used for language identification like Vector Space Model (Prager, 1999), Monte Carlo sampling (Poutsma, 2001), Markov Chains in combination with Bayesian Decision Rules (Dunning, 1994), Relative Entropy (Sibun and Reynar, 1996), and Ad-Hoc Ranking (Cavnar and Trenkle, 1994). Poutsma (2001) involved Monte Ca</context>
</contexts>
<marker>Poutsma, 2001</marker>
<rawString>A. Poutsma. 2001. Applying monte carlo techniques to language identification. In Proceedings of Computational Linguistics in the Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Prager</author>
</authors>
<title>Linguini: Language identification for multilingual documents</title>
<date>1999</date>
<booktitle>In Proceedings of the 32nd Hawaii International Conference on System Sciences</booktitle>
<contexts>
<context>sed languages. N-Gram-Based Approach The third type of language models is generated by the n-gram-based approach and uses n-grams of different (Cavnar and Trenkle, 1994) or fixed (Grefenstette, 1995; Prager, 1999) lengths from tokenized words. In contrast, Dunning (1994) generates n-grams of sequences of Bytes. An n-gram is a sequence of n characters. Before creating n-grams of a word, its beginning and the e</context>
<context> document is identified using the generated document model as input for the classification method. Different classification approaches can be used for language identification like Vector Space Model (Prager, 1999), Monte Carlo sampling (Poutsma, 2001), Markov Chains in combination with Bayesian Decision Rules (Dunning, 1994), Relative Entropy (Sibun and Reynar, 1996), and Ad-Hoc Ranking (Cavnar and Trenkle, 1</context>
</contexts>
<marker>Prager, 1999</marker>
<rawString>J.M. Prager. 1999. Linguini: Language identification for multilingual documents. In Proceedings of the 32nd Hawaii International Conference on System Sciences.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Quasthoff</author>
<author>C Biemann</author>
<author>M Richter</author>
</authors>
<title>Corpus portal for search in monolingual corpora</title>
<date>2006</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<contexts>
<context>ntities of the document and language model are compared. The first five entities of the document model are also contained in the Figure 2: Overview of Document Coverage in Leipzig Corpora Collection (Quasthoff et al., 2006) language model. The out-of-place measures shown on the left side define the spread of their ranks in both models. In contrast, the entity ED is not included in the language model. Thus, the out-of-p</context>
<context> been tested with different parameter values for word length, n-gram length or word frequency. 2.1. Data Collection The data for our experiments are derived from the Leipzig Corpora Collection (LCC) (Quasthoff et al., 2006) and from randomly selected Wikipedia articles. Leipzig Corpora Collection The Leipzig Corpora Collection contains corpora in different languages intended for natural language processing use. These c</context>
</contexts>
<marker>Quasthoff, Biemann, Richter, 2006</marker>
<rawString>U. Quasthoff, C. Biemann, and M. Richter. 2006. Corpus portal for search in monolingual corpora. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Sibun</author>
<author>J C Reynar</author>
</authors>
<title>Language identification: Examining the issues</title>
<date>1996</date>
<booktitle>In 5th Symposium on Document Analysis and Information Retrieval</booktitle>
<pages>125--135</pages>
<location>Las Vegas, Nevada, U.S.A</location>
<contexts>
<context> for language identification like Vector Space Model (Prager, 1999), Monte Carlo sampling (Poutsma, 2001), Markov Chains in combination with Bayesian Decision Rules (Dunning, 1994), Relative Entropy (Sibun and Reynar, 1996), and Ad-Hoc Ranking (Cavnar and Trenkle, 1994). Poutsma (2001) involved Monte Carlo sampling on language identification to compare it to the Ad-Hoc Ranking from Cavnar and Trenkle (1994) and the Mut</context>
</contexts>
<marker>Sibun, Reynar, 1996</marker>
<rawString>P. Sibun and J.C. Reynar. 1996. Language identification: Examining the issues. In 5th Symposium on Document Analysis and Information Retrieval, pages 125–135, Las Vegas, Nevada, U.S.A.</rawString>
</citation>
</citationList>
</algorithm>

