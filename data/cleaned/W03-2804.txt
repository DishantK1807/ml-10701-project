1:186	A Quantitative Method for Machine Translation Evaluation  Jess Toms Escola Politcnica Superior de Gandia Universitat Politcnica de Valncia jtomas@upv.es Josep ngel Mas Departament dIdiomes Universitat Politcnica de Valncia jamas@idm.upv.es Francisco Casacuberta Institut Tecnolgic dInformtica Universitat Politcnica de Valncia fcn@iti.upv.es  Abstract Accurate evaluation of machine translation (MT) is an open problem.
2:186	A brief survey of the current approach to tackle this problem is presented and a new proposal is introduced.
3:186	This proposal attempts to measure the percentage of words, which should be modified at the output of an automatic translator in order to obtain a correct translation.
4:186	To show the feasibility of the method we have assessed the most important SpanishCatalan translators in comparing the results obtained by the various methods.
5:186	1 Introduction Research in automatic translation lacks an appropriate, consistent and easy to use criterion for evaluating the results (White et al., 1994; Niessen et al., 2000).
6:186	However, it turns out to be indispensable to have some tool that may allow us to compare two translation systems or to elicit how any variation of our system may affect the quality of the translations.
7:186	This is important in the field of research as well as when a user has to choose between two or more translators.
8:186	The evaluation of a translation system shows a number of inherent difficulties.
9:186	First of all we are dealing with a subjective process, which is even difficult to define.
10:186	This paper is circumscribed to the project SISHITRA (SIStemas Hbridos para la TRAduccin valenciano-castellano supported by the Spanish Government), whose aim is the construction of an automatic translator between Spanish and Catalan texts using hybrid methods (both deductive and inductive).
11:186	In the following section we discuss some of the most important translation quality metrics.
12:186	After that, we introduce a semiautomatic methodology for MT evaluation and we show a tool to facilitate this kind of evaluation.
13:186	Finally, we present the results obtained on the evaluation of several Spanish-Catalan translators.
14:186	2 Metrics in MT Evaluation 2.1    Automatic Evaluation Criteria Within the scope of inductive translation, the use of objective metrics, which can be evaluated automatically, is quite frequent.
15:186	These metrics take as their starting point a possible reference translation for each of the sentences we want to translate.
16:186	This reference will be compared with the proposed sentences by the translation system.
17:186	The most important metric systems are:  Word Error Rate (WER): WER is the percentage of words, which are to be inserted, deleted or replaced in the translation in order to obtain the sentence of reference (Vidal, 1997; Tillmann et al., 1997).
18:186	WER can be obtained automatically by using the editing distance between both sentences.
19:186	This metric is computed efficiently and is reproducible (successive applications to the same data produce the same results).
20:186	However, the main drawback is its dependency on the sentences of reference.
21:186	There is an almost unlimited number of correct translations for one and the same sentence and, however, this metric considers only one to be correct.
22:186	Sentence Error Rate (SER): SER indicates the percentage of sentences, whose translations have not matched in an exact manner those of reference.
23:186	It shows similar advantages and shortcomings as WER.
24:186	Some variations on WER have been defined, which can also be obtained automatically:  Multi reference WER (mWER): Identical approach to WER, but it considers several references for each sentence to be translated, i.e., for each sentence the editing distance will be calculated with regard to the various references and the smallest one is chosen (Niessen et al., 2000).
25:186	It presents the drawback of requiring a great human effort before actually being able to use it.
26:186	However, the effort is worthwhile, if it can be later used for hundreds of evaluations.
27:186	BLEU Score: BLEU is an automatic metric designed by IBM, which uses several references (Papineni et al., 2002).
28:186	The main problem of mWER is that all possible reference translations cannot be introduced.
29:186	The BLEU score try to solve this problem by combining the available references.
30:186	In a simplified manner we could say that it measures how many word sequences in the sentence under evaluation match the word sequences of some reference sentence.
31:186	The BLEU score also includes a penalty for translations whose length differs significantly from that of the reference translation.
32:186	2.2     Subjective Evaluation Criteria Other kinds of metrics have been developed, which require human intervention in order to obtain an evaluation.
33:186	Among the most widely used we could stand out:  Subjective Sentence Error Rate (SSER) Each sentence is scored from 0 to 10, according to its translation quality (Niessen et al., 2000).
34:186	An example of these categories is:  0  nonsensical 1  some aspects of the content are conveyed  5  comprehensible, but with important syntactic errors  9  OK. Only slight style errors.
35:186	10  perfect.
36:186	The biggest problem shown by this technique is its subjective nature.
37:186	Two people who may evaluate the same experiment could obtain quite different results.
38:186	To solve this problem several evaluations can be performed.
39:186	Another drawback is that the different sentence lengths have not been taken into account.
40:186	The score of a 100 word-long sentence has the same impact on the total score as that of a word-long sentence.
41:186	Information Item Error Rate (IER) An unclear question is how to evaluate long sentences consisting of correct and wrong parts.
42:186	IER attempts to find a solution to this question.
43:186	In order to solve the problem the concept of information items is introduced.
44:186	The sentences are divided into word segments.
45:186	Each item of the sentence is marked with OK, error, syntax, meaning or others, as shown in the translation.
46:186	The metric IER (Information Item Error Rate) can then be calculated as the percentage of badly translated items (not marked as OK) (Niessen et al., 2000).
47:186	2.3 New Evaluation Criteria  Automatic metrics are especially useful, since their cost is practically null.
48:186	However, they are very dependent on the used references.
49:186	In some cases they can yield misleading results, for instance, if we want to compare an inductive translation system with some deductive one which, in principle, should produce translations of a similar quality.
50:186	If we extract the references from the same source as the training material of the inductive translator, the inductive translator will have an advantage over the deductive translator, since it has learned to translate by using a vocabulary and structures that are similar to those appearing in the references.
51:186	The non-automatic evaluation metrics described above presents various constraints: When an SSER is used, it may be very difficult to decide the score to be assigned to one sentence.
52:186	For example, if in one sentence a small syntactic error appears, we can assign an 8.
53:186	If in the following sentence two similar errors appear, what score should we assign?
54:186	The same or half the score?
55:186	To solve these kinds of matters, IER introduces the concept of information item.
56:186	This proposal has the drawback of being quite costly, both during the initial stage of deciding the word segments which form each item as well as when classifying the correction for each item.
57:186	After having seen the previous drawbacks the following metric has been introduced:  All references WER (aWER): It measures the number of words, which are to be inserted, deleted or replaced in the sentence under evaluation in order to obtain a correct translation.
58:186	It can also be seen as a particular case of the mWER, but taking for granted that all the possible references are at our disposal.
59:186	Since it is impossible to have a priori all possible references, the evaluator will be able to propose new references, if needed.
60:186	The evaluation process can be carried out very quickly, if one takes as the starting point the result obtained by the WER or the mWER.
61:186	The idea consists of visualising the incorrect words detected by one of these methods (editing operations).
62:186	The evaluator just needs to indicate whether each of the marked items is an actual error or whether it can rather be considered as an alternative translation This metric resembles very much the one proposed in (Brown et al, 1990).
63:186	That work suggested for measuring the translation quality counting the number of times an evaluator would have to press the keyboard keys in order to make the proposed sentence correct.
64:186	All references Sentence Error Rate (aSER): The SER metric presents the drawback of working with only one reference.
65:186	Therefore, it does not really measure the number of wrong sentences, but rather those that do not match exactly the reference.
66:186	For this reason we thought it would be interesting to introduce a metric that could indicate the percentage of sentences whose acronym name  on references description WER Word Error Rate word 1 % of words which are to be inserted, deleted or replaced in order to obtain the reference.
67:186	SER Sentence Error Rate sent.
68:186	1 % of sentences different from reference.
69:186	mWER Multi reference WER word various The same as WER, but with several reference sentences.
70:186	BLEU Bilingual Evaluation Understudy objective sent.
71:186	various The number of word groups that match the reference groups.
72:186	SSER Subjective Sentence Error Rate sent.
73:186	To each sentence a score from 0 to 10 is assigned.
74:186	Later on, it is converted into %.
75:186	IER Information Item Error Rate item The sentence is segmented into information items.
76:186	IER = % of badly translated items.
77:186	aWER All references WER word % of words to be inserted, deleted or replaced in order to obtain a correct translation.
78:186	aSER All references SER subjective sent.
79:186	% of incorrect sentences.
80:186	Table 1.
81:186	Some metrics in MT evaluation translations are incorrect.
82:186	This metric can be obtained as a by-product of the aWER.
83:186	3 Evaluation Tool for MT In order to facilitate the evaluation of automatic translators a graphic user interface has been implemented.
84:186	The metrics provided by the program are: WER, mWER, aWER, SER, SSER and aSER.
85:186	Figure 1 shows how it is displayed.
86:186	Next, the way the program works is described: On the editing window from top to bottom the following items are displayed: the source sentence, the sentence to be evaluated, the new sentences proposed by the user, the four most similar references to the sentence under evaluation (according to editing distance).
87:186	The new sentence proposed by the user will be in principle the same as that of the most similar reference.
88:186	In the sentence being evaluated using different colours, depending on whether they are considered insertions, replacements or deletions, the words that may be wrong are highlighted.
89:186	The user can click with the mouse on those words that may be considered correct.
90:186	As a result, this action will modify the new reference.
91:186	In the example (figure 1), if the user clicks on the highlighted words -, Diagram and locate, he will obtain the new reference Diagram shows the scan procedure to locate the archives..
92:186	This new reference reduces the editing distance from 5 to 2.
93:186	The user will also be able to click directly on some word of new reference to modify it.
94:186	The aim of this is to allow the evaluator the introduction of any new reference which may be a correct translation of the source sentence and which, furthermore, may resemble most closely the sentence being evaluated.
95:186	This tool can be obtained for free on (http://ttt.gan.upv.es/~jtomas/eval), both in the Linux version as wells as in Windows.
96:186	3.1 Evaluation Database Format A format in XML has been defined to store the reference files.
97:186	For each evaluation sentence we store: the source sentence, the target reference sentences and the target sentences proposed by the different MT with their subjective Figure 1.
98:186	The Graphic User Interface.
99:186	The system highlights the non-matching words between the evaluation sentence and the nearest reference.
100:186	evaluations.
101:186	Should during an aWER evaluation a new reference be proposed, this one is also stored.
102:186	An example of a file with a sentence under evaluation is shown as follows:  <evalTrans> <sentence>   <source>     La figura muestra el mtodo.
103:186	</source>   <eval translator="first reference">     <target>       This figure shows the procedure.
104:186	</target>   </eval>   <eval translator="multi reference">     <target>       This figure shows the method.
105:186	</target>   </eval>   <eval translator="Statistical"    evaluator="JM" sser="8" awer="1/5">     <target>       Chart represent the method.
106:186	</target>     <newRef>       Chart represents the method.
107:186	</newRef>   </eval> </sentence>  </evalTrans> 4 Example of Evaluation 4.1 Spanish-Catalan Translators The tool described in the previous section has been applied to the most important SpanishCatalan translators.
108:186	The Catalan language receives more or less intense institutional support in all territories of the Spanish state, where it is co-official with Spanish (Balearic Islands, Catalonia and Valencian Community).
109:186	This makes it compulsory from an administrative standpoint to publish a bilingual edition of all official documents.
110:186	For that purpose the use of a Machine Translator becomes almost indispensable.
111:186	But the official scope is not the only one where we can find the need to write bilingual documents in a short period of time.
112:186	The most obvious example can be the bilingual edition of some newspapers, such as El Pas or El Peridico de Catalunya, both in their editions for the autonomous community of Catalonia.
113:186	In the following section there is a brief description of each of the programs we have reviewed: Salt: an automatic translation program of the Valencian local government, which also includes a text corrector.
114:186	It can be downloaded for free from http://www.cultgva.es.
115:186	It has an interactive option for solving doubts (subjective ambiguity resolution) and is executed with the OS Microsoft Windows.
116:186	Incyta: the translation business web-site Incyta (http://www.incyta.com) was adding at the time of this evaluation example review a free online automatic translator for short texts.
117:186	Internostrum: an on-line automatic translation program, available at http://www.torsimany.ua.es, designed by the Language and Computational Systems Department of the University of Alicante.
118:186	It marks the doubtful words or segments as a review helping aid.
119:186	It uses finite-state technology (Canals et al., 2001).
120:186	Statistical: An experimental translator developed at the Computer Technology Institute of the Polytechnic University of Valencia.
121:186	All components have been inferred automatically from training pairs using statistical methods (Toms & Casacuberta, 2001).
122:186	It is accessible at http://ttt.gan.upv.es/~jtomas/trad.
123:186	4.2 Setting up the evaluation experiment In order to carry out our evaluation, we have translated 120 sentences (2456 words) with the different MT. These sentences have been taken from different media: a newspaper, a technical manual, legal text  The references used by the WER were also taken from the Catalan version of the same documents.
124:186	In mWER and in BLEU we used three additional references.
125:186	These new references have been introduced by a human translator modifying the initial reference.
126:186	Before applying the metrics shown in point 2, a human expert carries out a detailed analysis in order to establish the quality of the translations.
127:186	The experiment consists of sorting out the four outputs obtained by each translator for each test sentence, according to its quality.
128:186	If the expert does no find any quality difference between the sentences proposed by two translators, he assigns the same rank to them.
129:186	Table 2 shows the results obtained.
130:186	After this sentence by sentence analysis, the expert concludes that Salt is the better translator, followed closely by Incyta.
131:186	Statistical is in an intermediate position and the worst is Internostrum.
132:186	4.3 Results The results of our experiment can be observed in Figure 2.
133:186	Table 3 shows the evaluation time for the 120 sentences.
134:186	The first thing we can point out is that the Salt translator obtains the best results from all used metrics and Internostrum is the worst of all metrics.
135:186	The other two translators obtain different results depending on the used method.
136:186	Next we will discuss the results obtained by the different methods: The WER metric shows a strong dependence on the used reference.
137:186	If the translator employs a similar style or vocabulary with regard to those of the reference, it clearly achieves better results.
138:186	This fact determines that the obtained results do not show faithfully the quality of the translations.
139:186	Specifically, for Incyta it obtains bad results, although that does not coincide with the conclusions of the expert.
140:186	The main advantage of this method is that it is a totally automatic measurement without any evaluation cost.
141:186	These conclusions can also be extended to the SER.
142:186	mWER solves in part the problem posed by the WER.
143:186	To attempt to introduce a priori all possible translations turns out to be impossible, so that it has to choose a subset of these giving thus the method a certain subjective nature.
144:186	In the case of our evaluation, the references were introduced by using certain dialectal variants.
145:186	That worked slightly against some automatic translator, which preferred some other dialectal variants.
146:186	The BLEU metric tries to combine the available references in order to improve the mWER metric.
147:186	In our experiment the use of several references, in mWER and BLEU, does not solve the deficiency of WER.
148:186	It continues being most detrimental to Incyta.
149:186	The use of the mWER and BLEU required a great initial effort, when the references were written, by even choosing only three new references for each translation.
150:186	However, these methods had a big advantage: each evaluation is done without any additional cost.
151:186	When we applied the SSER, we faced the following dilemma: Which criteria should we use for applying the scoring scale?
152:186	We decided that the latter had to be related with the global understanding of the sentence and the number of errors in correspondence with the sentence length.
153:186	Since this criterion is not made explicit in the method the choice of a different criterion would have produced very diverse results.
154:186	Regarding the evaluation effort, it was the most costly method.
155:186	In order to evaluate each sentence it was necessary to read and understand both the source sentence and the target sentence to try to score at the end the translation.
156:186	The aWER metric breaks with the dependence on the used references, which displayed the WER, mWER and BLEU.
157:186	Moreover, it turned out to be much more objective and clearer to apply than the SSER.
158:186	The metric achieved by this method provides us with clear and intuitive information.
159:186	If we use the Salt translator we will have to correct 3% of the words in order to obtain a correct translation.
160:186	Interpret the metrics supplied by the other methods it becomes unavoidable to know the conditions under which the evaluation has been carried out (references used, criteria ).
161:186	The evaluation effort for the aWER is significantly less than the mWER and the SSER.
162:186	Translator first second thrid fourth Salt 69% 13% 13% 4% Incyta 63% 11% 13% 13% Statistical 60% 13% 7% 20% Internostrum 48% 12% 20% 20%  Table 2.
163:186	Comparative classification sentence by sentence.
164:186	The discussion on the aWER method can be extended to the aSER.
165:186	Considering the expert evaluation, the subjective metrics reflect better the quality of the evaluated translations than the automatic ones.
166:186	The Incyta translator works quite appropriately, but it proposes translations that deviate from the references.
167:186	Thus, the automatic measures (WER, mWER and BLEU), based on these references, do not evaluate correctly this translator.
168:186	On the other hand, the Statistical Translator works worse, even though its translations are more similar to the references.
169:186	It is an example-based translator, and the training and test sentences have been obtained from the same sources.
170:186	This can benefit the evaluation of the Statistical translator using automatic measures.
171:186	5 Conclusions In this paper we present a criterion (aWER) for the evaluation of translation systems.
172:186	The evaluation of the translations can be carried out quickly thanks to the use of a computer tool developed for this purpose.
173:186	We have compared this criterion with other criteria (WER, mWER, SER, BLEU and SSER) using the translations obtained by several Spanish-Catalan translators.
174:186	It is our understanding that automatic measures (WER, mWER and BLEU) do not evaluate correctly the translators (specifically, they affect Incyta negatively).
175:186	Translator WER mWER aWER SER SSER BLEU aSER  Salt   9.9 1 6.6 1 3.0 1 68.3 1 10.3 1 0.866 1 40.0 1  Incyta 10.9 2 7.6 2 3.1 1 74.2 2 11.2 1 0.855 2 41.7 1  Statistical 10.7   2 7.8 2 3.8 2 70.8 2 12.8 2 0.857 2 45.8 2  Internostrum 11.9  3 8.5 3 4.9 3 80.0 3 15.8 3 0.837 3 58.3 3 9 9,5 10 10,5 11 11,5 WER 5,6 6,1 6,6 7,1 7,6 8,1 mWER 2 2,5 3 3,5 4 4,5 aWER 62 65 68 71 74 77 80 SER 7,5 9 10,5 12 13,5 15 SSER 18 28 38 48 58 BLEU 30 35 40 45 50 55 aSER Figure 2.
176:186	Comparative evaluation results using 7 different metrics for the 4 Spanish-Catalan translators.
177:186	In order to interpret quickly the results obtained in each metric, we have classified each translator using the following ranking: 1better 2intermediate 3worse.
178:186	mWER / BLEU SSER aWER / aSER Set-up time*  210 0 0 Internostrum 0 70 40 Salt 0 60 25 Incyta 0 55 30 Statistical 0 60 25 Total: 210 245 120  Table 3.
179:186	Comparative evaluation time (minutes) of the 120 sentences using  the different metrics.
180:186	*Time spent to introduce the proposed references.
181:186	0.84 0.85 0.86 0.87 0.88 The scores produced by human experts (SSER and aWER) are the metrics that best capture the translation quality among the different systems.
182:186	As its most important aWER feature we would stand out that, in spite of being a subjective method which requires the intervention of a human evaluator, the latter will not have to take too subjective decisions.
183:186	We believe that the aWER tool could be used in another domain, for the evaluation of other natural language processing systems, e.g. summarizing systems.
184:186	In a future our aim is to add to this comparative study other score methods, in addition to comparing the variability introduced by different human evaluators in each of the methods.
185:186	Acknowledgement This work was partially funded by the Spanish CICYT under grant TIC2000-1599-C02 and the IST Programme of the European Union under grant IST-2001-32091.
186:186	The authors wish to thank the anonymous reviewers for their criticism and suggestions.

