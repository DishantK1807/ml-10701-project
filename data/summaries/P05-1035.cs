Let us imagine, for instance, that the best metric turns out to be a ROUGE (Lin and Hovy, 2003a) variant that only considers unigrams to compute similarity.
N03-1020 W03-0510
Turian and Melamed, 2003; Lin and Hovy, 2003a).
N03-1020 W03-0510
BLEU (Papineni et al., 2001) and ROUGE (Lin and Hovy, 2003a) are the standard similarity metrics used in Machine Translation and Text Summarisation.
N03-1020 W03-0510
6.2 Meta-evaluation of similarity metrics The question of how to know which similarity metric is best to evaluate automatic summaries/translations has been addressed by â€¢ comparing the quality of automatic items with the quality of manual references (Culy and Riehemann, 2003; Lin and Hovy, 2003b).
N03-1020 W03-0510
We have used ROUGE-1 (only unigrams with lemmatization and stop word removal), which gives good results with standard summaries (Lin and Hovy, 2003a).
N03-1020 W03-0510
The methodology which is closest to our framework is ORANGE (Lin, 2004), which evaluates a similarity metric using the average ranks obtained by reference items within a baseline set.
C04-1072
