1:194	Statistical Significance Tests for Machine Translation Evaluation Philipp Koehn Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology The Stata Center, 32 Vassar Street, Cambridge, MA 02139 koehn@csail.mit.edu Abstract If two translation systems differ differ in performance on a test set, can we trust that this indicates a difference in true system quality?
2:194	To answer this question, we describe bootstrap resampling methods to compute statistical significance of test results, and validate them on the concrete example of the BLEU score.
3:194	Even for small test sizes of only 300 sentences, our methods may give us assurances that test result differences are real.
4:194	1 Introduction Recently, the field of machine translation has been changed by the emergence both of effective statistical methods to automatically train machine translation systems from translated text sources (so-called parallel corpora) and of reliable automatic evaluation methods.
5:194	Machine translation systems can now be built and evaluated from black box tools and parallel corpora, with no human involvement at all.
6:194	The evaluation of machine translation systems has changed dramatically in the last few years.
7:194	Instead of reporting human judgment of translation quality, researchers now rely on automatic measures, most notably the BLEU score, which measures n-gram overlap with reference translations.
8:194	Since it has been shown that the BLEU score correlates with human judgment, an improvement in BLEU is taken as evidence for improvement in translation quality.
9:194	Building the tools for any translation system involves many iterations of changes and performance testing.
10:194	It is important to have a method at hand that gives us assurances that the observed increase in the test score on a test set reflects true improvement in system quality.
11:194	In other words, we need to be able to gauge, if the increase in score is statistically significant.
12:194	Since complex metrics such as BLEU do not lend themselves to an analytical technique for assessing statistical significance, we propose bootstrap resampling methods.
13:194	We also provide empirical evidence that the estimated significance levels are accurate by comparing different systems on a large number of test sets of various sizes.
14:194	In this paper, after providing some background, we will examine some properties of the widely used BLEU metric, discuss experimental design, introduce bootstrap resampling methods for statistical significance estimation and report on experimental results that demonstrate the accuracy of the methods.
15:194	2 Background 2.1 Statistical Machine Translation Statistical machine translation was introduced by work at IBM [Brown et al. , 1990, 1993].
16:194	Currently, the most successful such systems employ so-called phrase-based methods that translate input text by translating sequences of words at a time [Och, 2002; Zens et al. , 2002; Koehn et al. , 2003; Vogel et al. , 2003; Tillmann, 2003] Phrase-based machine translation systems make use of a language model trained for the target language and a translation model trained from a parallel corpus.
17:194	The translation model is typically broken down into several components, e.g., reordering model, phrase translation model, and word translation model.
18:194	2.2 Automatic Evaluation To adequately evaluate the quality of any translation is difficult, since it is not entirely clear what the focus of the evaluation should be.
19:194	Surely, a good translation has to adequately capture the meaning of the foreign original.
20:194	However, pinning down all the nuances is hard, and often differences in emphasis are introduced based on the interpretation of the translator.
21:194	At the same time, it is desirable to have fluent output that can be read easily.
22:194	These two goals, adequacy and fluency, are the main criteria in machine translation evaluation.
23:194	System 1-gram 4-gram %BLEU Spanish 62.6% 14.7% 28.9% Portuguese 60.9% 13.4% 27.4% Danish 60.8% 13.3% 26.9% Greek 59.4% 12.1% 25.3% German 58.3% 9.8% 22.6% Finnish 56.1% 7.8% 20.2% Table 1: Translation quality of three systems, measured by the BLEU score and n-gram precision Human judges may be asked to evaluate the adequacy and fluency of translation output, but this is a laborious and expensive task.
24:194	Papineni et al. [2002] addressed the evaluation problem by introducing an automatic scoring metric, called BLEU, which allowed the automatic calculation of translation quality.
25:194	The system output is compared against a reference translation of the same source text.
26:194	2.3 BLEU: A Closer Look Formally, the BLEU metric is computed as follows.
27:194	Given the precision a0a2a1 of n-grams of size up to a3 (usually a3 a4a6a5 ), the length of the test set in words (a7 ) and the length of the reference translation in words (a8 ), BLEU a4 BP a9 expa10 a11 a12 a1a14a13a16a15 log a0 a1a18a17 (1) BP a4 mina10a20a19a22a21a24a23 a15a26a25a28a27a30a29a20a31 a17 (2) The effectiveness of the BLEU metric has been demonstrated by showing that it correlates with human judgment.
28:194	Let us highlight two properties of the BLEU metric: the reliance on higher n-grams and the brevity penalty BP.
29:194	First, look at Table 1.
30:194	Six different systems are compared here (we will get later into the nature of these systems).
31:194	While the unigram precision of the three systems hovers around 60%, the difference in 4-gram precision is much larger.
32:194	The Finnish system has only roughly half (7.8%) of the 4-gram precision of the Spanish system (14.7%).
33:194	This is the cause for the relative large distance in overall BLEU (28.9% vs. 20.2%)1.
34:194	Higher n-grams (and we could go beyond 4), measure not only syntactic cohesion and semantic adequacy of the output, but also give discriminatory power to the metric.
35:194	The other property worth noting is the strong influence of the brevity penalty.
36:194	Since BLEU is a precision based method, the brevity penalty assures that a 1We use in this paper the %BLEU notation: a BLEU score of 0.289 is reported as 28.9% BLEU 23 24 25 26 27 28 29 30 31 32 0.85 0.9 0.95 1 1.05 1.1 1.15 1.2 BLEU / GTM score length ration (output/reference) BLEU Spanish BLEU Portuguese GTM Spanish GTM Portuguese Figure 1: Effect of output length on BLEU and GTM score: brevity penalty in BLEU causes score to drop dramatically with shorter output system does not only translate fragments of the test set of which it is confident, resulting in high precision.
37:194	Is has become common practice to include a word penalty component in statistical machine translation system that gives bias to either longer or shorter output.
38:194	This is especially relevant for the BLEU score that harshly penalizes translation output that is too short.
39:194	To illustrate this point, see Figure 1.
40:194	BLEU scores for both Spanish and Portuguese system drop off when a large word penalty is introduced into the translation model, forcing shorter output.
41:194	This is not the case for a similar metric, GTM, an n-gram precision/recall metric proposed by Melamed et al. [2003] that does not have an explicit brevity penalty.
42:194	The BLEU metric also works with multiple reference translations.
43:194	However, we often do not have the luxury of having multiple translations of the same source material.
44:194	Fortunately, it has not been shown so far that having only a single reference translation causes serious problems.
45:194	While BLEU has become the most popular metric for machine translation evaluation, some of its short-comings have become apparent: It does not work on single sentences, since 4-gram precision is often 0.
46:194	It is also hard to interpret.
47:194	What a BLEU score of 28.9% means is not intuitive and depends, e.g., on the number of reference translation used.
48:194	Some researchers have recently used relative human BLEU scores, by comparing machine BLEU scores with high quality human translation scores.
49:194	However, the resulting numbers are unrealistically high.
50:194	3 Experimental Design In this section, we describe the experimental framework of our work.
51:194	We also report on a number of 20 25 30 35 40 10 20 30 40 50 60 70 80 90 100 %BLEU sample Figure 2: BLEU score of the Spanish-English system on 100 blocks of 300 sentences each preliminary experiments that give us some intuition on variance of test scores on different test sets.
52:194	3.1 System and Corpus We carry out experiments using a phrase-based statistical machine translation system [Koehn et al. , 2003; Koehn, 2004].
53:194	We train this system on the Europarl corpus, a parallel corpus in 11 European languages of 20-30 million words per language [Koehn, 2002].
54:194	Since the focus of this paper is the comparison of the performance of different systems, we need a set of translation systems.
55:194	Here, we resort to a trick: Instead of comparing different machine translation methods trained on the same training data, we train the same machine translation method on different parallel copora: language pairs with English as the target language and any of the 10 other languages as the source language.
56:194	Then, we assemble a test set that is sentence aligned across all 11 languages.
57:194	During evaluation, the, say, Spanish-English and Danish-English system each translate a sentence that correspond to the same English reference translation.
58:194	Hence, we can compare how well the English output of the Spanish-English translation system matches the reference sentence vs. how well the English output of the Portuguese-English matches the reference sentence.
59:194	We would like to stress that comparing performance of a method that translates text from different languages instead of using different translation methods on the same input text is irrelevant for the purpose of this paper.
60:194	We are comparing output of different systems in either case.
61:194	As an alternative, we may also use a single language pair and different systems, say, by using different parameter settings.
62:194	But we feel that this would leave us with many arbitrary choices that we would like to avoid for the 20 25 30 35 40 10 20 30 40 50 60 70 80 90 100 %BLEU sample Figure 3: BLEU score on 100 broad samples of 300 sentences: smaller variance in test results sake of clarity of our argument.
63:194	3.2 Selecting a Test Set We already reported some BLEU scores earlier.
64:194	These were computed on a 30,000 sentence test set (about 900,000 words).
65:194	Such a huge test set is very uncommon2, since translating such a large amount of text is computationally very expensive.
66:194	It may take hours or even days, which does not contribute to a fast turn-around system development process.
67:194	Therefore, we would like to be able to work with smaller test sets.
68:194	The trade-off between fast testing and having meaningful results is at the very heart of this paper: The statistical significance tests we propose give us the means to assess the significance of test results.
69:194	Let us start with the following experiment: We break up the test set into 100 blocks of 300 consecutive sentences each, translate each block, and compute the BLEU score for each block.
70:194	We plot in Figure 2 the BLEU scores for each of the 100 blocks for the Spanish system.
71:194	The BLEU scores for the 100 blocks vary from 21% to 37%.
72:194	Many factors influence why some sentences are easier to translate than others: unknown words, sentence length, degree of syntactic and semantic divergence, and how the input and reference translation were generated  in case of Europarl they may be both translations from a third language.
73:194	3.3 Broad Sampling Factors that influence translation difficulty may be clustered.
74:194	For instance, the original language, or the topic and style (and hence vocabulary) usually stays the same over many sentences in sequence.
75:194	When we collect only 300 sentences in sequence, certain 2The annual DARPA/NIST evaluations use test sets of size 1000-2000 sentences with 4 reference translations -1 0 1 2 3 4 5 10 20 30 40 50 60 70 80 90 100 difference in %BLEU sample Figure 4: Difference in BLEU between Spanish and Danish system on 100 broad samples factors may very strongly affect some samples, but not others.
76:194	Therefore, better test sets of 300 sentences may be constructed by sampling these sentences from different parts of the corpus.
77:194	In an effort to collect better test sets, we now place into test set a0 the sentences a0 a21a1a0 a2 a19a4a3a5a3a18a21a1a0 a2a7a6 a3a5a3a18a21a4a8a9a8a9a8 a21a1a0 a2a7a6a5a10a5a10 a3a5a3.
78:194	Let us call this a broad sample.
79:194	Again, we measure translation quality on each of these 100 broad samples.
80:194	Figure 3 shows that the BLEU scores are now closer together  ranging from 27% to 31%.
81:194	3.4 Comparison of Translation Performance Obtaining a BLEU score for a translation system on a given test set is not very meaningful by itself.
82:194	We want to use the metric to compare different translation systems.
83:194	In a third preliminary experiment, we compared for each of the 300 broad samples, the BLEU score for the Spanish system against the BLEU score for the Danish system.
84:194	Results are in Figure 4.
85:194	The Spanish system is better by up to 4%.
86:194	In only one sample, sample no. 10, the Danish system outperforms the Spanish system by 0.07%.
87:194	Let us stress in conclusion at this point that  when working with small test sets  it is important to obtain a representative sample, as much as this is possible.
88:194	Translation quality of neighboring sentences correlates positively, therefore we want to chose test sentences from different parts of a larger set.
89:194	4 Statistical Significance The purpose of experimental testing is to assess the true translation quality of a system on text from a certain domain.
90:194	However, even with the qualifier from a certain domain, this is an abstract concept, since it has to be computed on all possible sentences in that domain.
91:194	In practice, we will always just be able to measure the performance of the system on a specific sample.
92:194	From this test result, we would like to conclude what the true translation performance is. Statistical significance is an estimate of the degree, to which the true translation quality lies within a confidence interval around the measurement on the test sets.
93:194	A commonly used level of reliability of the result is 95%, also written as a0 a4 a3a11a8a12a3a14a13, called p-level.
94:194	Let us explore this notion on a simpler metric of translation quality than BLEU.
95:194	A common metric in older machine translation papers is a score from 1 to 5 for each sentence, or even simpler: a human judge deemed a translation either as correct or wrong.
96:194	If, say, 100 sentence translations are evaluated, and 30 are found correct, what can we say about the true translation quality of the system?
97:194	Our best guess is 30%, but that may be a few percent off.
98:194	How much off, is the question to be answered by statistical significance tests.
99:194	Given a set of a0 sentences, we can compute the sample mean and variance of the individual sentence scores a15a17a16 : a18 a15 a4 a19 a0 a1 a12 a16 a13a16a15 a15a17a16 (3) a19a21a20 a4 a19 a0a23a22 a19 a1 a12 a16 a13a16a15 a10a24a15a25a16a26a22 a18 a15 a17 a20 (4) What we are really interested in, however is the true mean a27 . Let us assume that the sentence scores are distributed according to the normal distribution.
100:194	This implies that a sentence score is independent from other sentence scores.
101:194	Since we do not know the true mean a27 and variance a28 a20, we can not model the distribution of sentence scores with the normal distribution.
102:194	However, we can use Students t-distribution, which approximates the normal distribution for large a0 . See Figure 5 for an illustration: Given the sample mean a18a15 and sample variance a19 a20, we estimate the probability distribution for true translation quality.
103:194	We are now interested in a confidence interval a29 a18a15a23a22a31a30 a21 a18a15 a2 a30a14a32 around the mean sentence score.
104:194	The true translation quality (or the true mean a27 lies within in the confidence interval with a probability a33 . Note the relationship between the degree of statistical significance and the confidence interval: The degree of statistical significance is indicated by the fraction of the area under the curve that is shaded.
105:194	Figure 5: With probability a33 a4 a3a11a8 a10 a13 (shaded area), the number of correct translations lies in an interval a29 a18 a15 a22 a30a28a21 a18 a15 a2 a30a5a32 around the mean sentence score a18 a15 The confidence interval is indicated by the boundaries on the x-axis.
106:194	The functional mapping between a confidence interval a29 a18a15 a22 a30a28a21 a18a15 a2 a30a5a32 and the probability a33 can be obtained by integrating over the distribution.
107:194	However, in case of Students t-distribution, the solution to this does not exist in closed form, but we can use numerical methods.
108:194	The size of the confidence interval can be computed by a30 a4 a1 a9 a19 a2 a0 (5) The factor a1 depends on the desired p-level of statistical significance and the sample size.
109:194	See Table 2 for typical values.
110:194	We described the standard method to compute statistical significance for some machine translation evaluation metrics.
111:194	Unfortunately, this method to compute confidence intervals does not work for the BLEU metric, since the BLEU metric is not the mean of single sentence scores.
112:194	5 Bootstrap Resampling Recall that we want to answer the following question: Given a test result of a3 BLEU, we want to compute with a confidence a33 (or p-level a0 a4 a19 a22 a33 ) that the true BLEU score lies in an interval a29 a3 a22 a30a28a21a4a3 a2 a30a5a32 . Instead of using an analytical method to compute confidence intervals for the BLEU score, we resort to a randomized method, called bootstrap resampling.
113:194	Bootstrap resampling has a long tradition in the field of statistics, refer to Efron and Tibshirani [1994] for a general introduction and Press et al. [2002] for a typical implementation.
114:194	Some recent papers on statistical machine translation hint on the use of bootstrap resampling for Significance Test Sample Size Level 100 300 600 a5 99% 2.6259 2.5923 2.5841 2.5759 95% 1.9849 1.9679 1.9639 1.9600 90% 1.6602 1.6499 1.6474 1.6449 Table 2: Values for a1 for different sizes and significance levels (Formula 5) assessing statistical significance of test results [Germann, 2003; Och, 2003; Kumar and Byrne, 2004], but do not lay out their methodology.
115:194	The intuition behind bootstrap resampling goes as follows: Assume that we can only test translation performance on a test set of a0 a4 a6 a3a5a3 sentences.
116:194	These 300 sentences are randomly drawn from the world.
117:194	Given a test set, we can compute a BLEU score.
118:194	Then, we draw a second test set of 300 sentences, and compute its BLEU score.
119:194	If we do this repeatedly, we get a number of different BLEU scores, not unlike the BLEU scores displayed in Figure 3.
120:194	If we do this for a large number a19 of test sets (say 1000 times), we can sort the corresponding BLEU scores.
121:194	If we drop the top 2.5% and bottom 2.5% of BLEU scores, we have the remaining 95% of BLEU scores in an interval a29 a7 a21a9a8 a32 . The law of large numbers dictates, that with an increasingly large number a19 of BLEU scores, the interval a29 a7 a21a9a8 a32 approaches the 95% confidence interval for scores of test sets of size 300.
122:194	Of course, having to translate and score sets of 300 sentences repeatedly, does not save anything in terms of computational translation cost and the need for a large set of potential sentences.
123:194	We therefore, take the following leap: Instead of the selected the 300 sentences in each test set from an infinite set of test sentences, we draw them from the same set of 300 sentences with replacement.
124:194	Let us clearly state this assumption: Assumption: Estimating the confidence interval from a large number of test sets with a0 test sentences drawn from a set of a0 test sentences with replacement is as good as estimating the confidence interval for test sets size a0 from a large number of test sets with a0 test sentences drawn from an infinite set of test sentences.
125:194	The benefit of this assumption is clear: We only have to translate a0 (say, 300) sentences.
126:194	We will now provide empirical evidence that we can make this assumption.
127:194	Figure 6 reports on an experiment with the Spanish-English system.
128:194	It displays the 95% con24 25 26 27 28 29 30 31 32 33 34 0 10 20 30 40 50 60 70 80 90 100 95% range for %BLEU sample Figure 6: Bootstrapped 95% confidence intervals of true BLEU on 100 broad samples of 300 sentences: Assuming the 30,000 sentence BLEU as true score, three mistakes (3%) are made: on test set no. 10, 81, and 88 fidence intervals computed for 100 test sets of 300 sentences.
129:194	For each of the 100 test sets, we compute the confidence interval as described before: We repeatedly (1000 times) generate BLEU scores on test sets of 300 sentences drawn from this one test set of 300 sentences.
130:194	We use the 1000 BLEU scores to estimate the confidence interval.
131:194	We drop the top 25 and bottom 25 BLEU scores, so that the displayed interval ranges from the 26th best BLEU score to the 975th best.
132:194	For the first test set, we obtain a confidence interval of [26.5,30.7], for the second test set a confidence interval of [27.5,33.0], and so on.
133:194	We do not know the true BLEU score for this system, which has to be computed on a near-infinite number of test sentences.
134:194	But the BLEU score computed on 30,000 test sentences is as good as any (assuming 30,000 is close to infinite).
135:194	It is, as you recall from Table 1, 28.9%.
136:194	For all but three test set, this near-true test score lies within the estimated confidence interval.
137:194	Loosely speaking, the 95% confidence level is actually 97% correct.
138:194	6 Paired Bootstrap Resampling As stated earlier, the value of scoring metrics comes from being able to compare the quality of different translation systems.
139:194	Typically, we want to compare two systems.
140:194	We translate the same test set with the two systems, and measure the translation quality using an evaluation metric.
141:194	One system will fare better than the other, with some difference in score.
142:194	Can we conclude that the better scoring system is truly better?
143:194	If the differences in score are small, we intuitively have some doubt.
144:194	40 50 60 70 80 90 100 10 20 30 40 50 60 70 80 90 100 statistical significance sample Figure 7: Paired bootstrap resampling results on 100 broad samples of 300 sentences: For 65 samples we make a 95% statistically significant conclusion that the Spanish system is better than the Danish (dots above the 95% line We would like measure the reliability of the conclusion that one system is better than the other, or in other words, that the difference in test scores is statistical significant.
145:194	As in the previous section, we use a bootstrap resampling method for this: Given a small collection of translated sentences, we repeatedly (say, 1000 times) create new virtual test sets by drawing sentences with replacement from the collection.
146:194	For each set, we compute the evaluation metric score for both systems.
147:194	We note, which system performs better.
148:194	If, say, one system outperforms the other system 95% of the time, we draw the conclusion that it is better with 95% statistical significance.
149:194	We call this method paired bootstrap resampling, since we compare a pair of systems.
150:194	We carry out experiments using this method using the BLEU score to compare the Spanish-English system with the Danish-English system.
151:194	Results are displayed in Figure 7.
152:194	We estimate statistical significance for 100 different test sets with 300 sentences each (the same broad samples used in previous experiments).
153:194	For 65 samples we draw the conclusion the Spanish system is better than the Danish with at least 95% statistical significance.
154:194	Recall that the BLEU score difference on the huge 30,000 sentence test set is 2.0% (refer to Table 1).
155:194	In this particular case, a small 300 sentence test set is often sufficient to detect the superiority of one of the systems with statistical significance.
156:194	If the true difference in translation performance is closer, we are less likely to draw conclusions from such a small test set.
157:194	See Figure 8, which compares the Portuguese and the Danish system.
158:194	Their BLEU score difference on the huge 30,000 sentence test set 0 20 40 60 80 100 10 20 30 40 50 60 70 80 90 100 statistical significance sample Figure 8: Paired bootstrap resampling results on 100 broad samples of 300 sentences: For 12 samples we make a 95% statistically significant conclusion that the Portuguese system is better than the Danish, and 1 conclusion to the opposite is only 0.5%.
159:194	Using paired bootstrap resampling, we drawn 13 conclusions: 12 correct conclusions that the Portuguese system is better, and one wrong conclusion that the Danish system is better.
160:194	That we draw one wrong conclusion, is unfortunate, but should not come as a surprise, when we talk about 95% statistical singificance levels.
161:194	At this level, 1 conclusion in 20 is expected to be wrong.
162:194	7 Validation Experiments We introduced two methods using bootstrap resampling.
163:194	One method estimates bounds for the true performance level of a system.
164:194	The other method, paired bootstrap resampling, estimates how confidently we can draw the conclusion from a test result that one system outperforms another.
165:194	We would now like to provide experimental evidence that these estimates are indeed correct at the specified level of statistical significance.
166:194	7.1 95% Statistical Significance We carried out a large number of experiments to estimate statistical significance for system comparisons.
167:194	We compared 9 different pairs of systems, with different test set sizes.
168:194	Detailed results can be found in Table 3.
169:194	The test set sizes vary from 100 sentences to 3000 sentences.
170:194	As described earlier, the systems translate aligned test sentences from different source languages into English.
171:194	First, we want to answer the questions: How often can we draw conclusions with 95% statistical significance?
172:194	How often are we correct?
173:194	Even for small test sets of size 300 sentences (about 9000 words), we can reliably draw the right conclusion, if the true BLEU score difference is at Significance Level Conclusions Correct 100% 1042 100% 99%-99.9% 738 100% 98%-98.9% 245 99% 95%-97.9% 394 98% 90%-94.9% 363 95% 80%-89.9% 520 88% 70%-79.9% 324 77% 60%-69.9% 253 72% 50%-59.9% 261 52% Table 4: Validation of the statistical significance estimations: Number of conclusions drawn at a certain level and accuracy of the conclusions.
174:194	least 2-3%.
175:194	Note that we make no general statements about relation of test set size and statistical significance BLEU score differences, this particular finding is specific to our test scenario and depends highly on how similar the systems are.
176:194	Only one conclusion is wrong for test sets of size 300  the already mentioned conclusion that the Danish system outperforms the Portuguese system.
177:194	For test sets with only 100 sentences, we observer more false conclusions, which suggests that this might be too small for a test set.
178:194	7.2 Other Significance Levels While the 95% statistical significance level is the most commonly used for historical reasons, we want to validate as well the accuracy of the bootstrap resampling method at different statistical significance levels.
179:194	Table 4 displays our findings.
180:194	For each conclusion, we check into what statistical significance range it falls, e.g., 90-94.9%).
181:194	Then, we check for all conclusions with an interval, how many are correct, i.e., consistent with the conclusion drawn from the much larger 30,000 sentence test set.
182:194	The numbers suggest, that the method is fairly accurate and errs on the side of caution.
183:194	For instance, when we conclude a statistical significance level of 90%-94.9%, we are actually drawing the right conclusion 95% of the time.
184:194	8 Summary and Outlook Having a trusted experimental framework is essential for drawing conclusions on the effects of system changes.
185:194	For instance: do not test on train, do not use the same test set repeatedly, etc. We stressed the importance of assembling test sets from different parts of a larger pool of sentences (Figure 2 vs. Figure 3).
186:194	We discussed some properties of the widely used BLEU score, especially the effect of the brevity System BLEU Sample Size (Sentences) Comparison Difference 100 300 600 3000 Spanish better than Portuguese 1.5% 33%/1% 60% 84% 100% Spanish better than Danish 2.0% 31% 65% 96% 100% Portuguese better than Danish 0.5% 7%/2% 12%/1% 10% 30% Portuguese better than Greek 2.1% 38% 68% 92% 100% Danish better than Greek 1.6% 24% 48% 74% 100% Danish better than German 4.3% 85% 100% 100% 100% Greek better than German 2.7% 65% 97% 100% 100% Greek better than Finnish 5.1% 97% 100% 100% 100% German better than Finnish 2.4% 53% 91% 100% 100% Table 3: The table displays how often a conclusion with 95% statistical significance is made for different system comparisons and different sample sizes.
187:194	12%/1% means 12% correct and 1% wrong conclusions.
188:194	30,000 test sentences are divided into 300, 100, 50, and 10 samples, each the size of 100, 300, 600, and 3000 sentences respectively.
189:194	penalty and the role of larger n-grams.
190:194	One important element of a solid experimental framework is a statistical significance test that allows us to judge, if a change in score that comes from a change in the system, truly reflects a change in overall translation quality.
191:194	We applied bootstrap resampling to machine translation evaluation and described methods to compute statistical significance intervals and levels for machine translation evaluation metrics.
192:194	We described how to compute statistical significance intervals for metrics such as BLEU for small test sets, using bootstrap resampling methods.
193:194	We provided empirical evidence that the computed intervals are accurate.
194:194	Aided by the proposed methods, we hope that it becomes common practice in published machine translation research to report on the statistical significance of test results.

