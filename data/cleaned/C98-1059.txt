A structure-sharing parser for lexicalized grammars 
Roger Evans 
Information Technology Research Institute 
University of Brighton 
Brighton, BN2 4G J, UK 
Roger. Evans @it ri. bright on. ac. uk 
David Weir 
Cognitive and Computing Sciences 
University of Sussex 
Brighton, BN1 9QH, UK 
David. Weir@cogs.susx.ae. uk 
Abstract 
In wide-coverage lexicalized grammars many of 
the elementary structures have substructures in 
common. This means that in conventional pars
ing algorithms some of the computation associ
ated with different structures is duplicated. In 
this paper we describe a precompilation tech
nique for such grammars which allows some of 
this computation to be shared. In our approach 
the elementary structures of the grammar are 
transformed into finite state automata which 
can be merged and minimised using standard al
gorithms, and then parsed using an automaton
based parser. We present algorithms for con
structing automata from elementary structures, 
merging and minimising them, and string recog
nition and parse recovery with the resulting 
grammar. 
1 Introduction

It is well-known that fully lexicalised grammar 
formalisms such as LTAG (Joshi and Schabes, 
1991) are difficult to parse with efficiently. Each 
word in the parser's input string introduces an 
elementary tree into the parse table for each 
of its possible readings, and there is often a 
substantial overlap in structure between these 
trees. A conventional parsing algorithm (Vijay
Shanker and Joshi, 1985) views the trees as in
dependent, and so is likely to duplicate tile pro
cessing of this common structure. Parsing could 
be made more efficient (empirically if not for
mally), if the shared structure could be identi
fied and processed only once. 
Recent work by Evans and Weir (1997) and 
Chen and Vijay-Shanker (1997) addresses this 
problem from two different perspectives. Evans 
and Weir (1997) outline a technique for com
piling LTAG grammars into automata which are 
then merged to introduce some sharing of struc
ture. Chen and Vijay-Shanker (1997) use un
derspecified tree descriptions to represent sets 
of trees during parsing. Tile present paper takes 
the former approach, but extends our previous 
work by: 
• showing how merged automata can be min
irnised, so that they share as much struc
ture as possible; 
• showing that by precompiling additional 
information, parsing can be broken down 
into recognition followed by parse recovery; 
• providing a formal treatment of the algo
rithms for transforming and minimising the 
grammar, recognition and parse recovery. 
In the following sections we outline the basic 
approach, and describe informally our improve
ments to the previous account. We then give a 
formal account of the optimisation process and 
a possible parsing algorithm that makes use of 
it 1 . 
2 Automaton-based parsing 
Conventional LTAG parsers (Vijay-Shanker and 
Joshi, 1985; Schabes and Joshi, 1988; Vijay
Shanker and Weir, 1993) maintain a parse ta
ble, a set of items corresponding to complete 
and partial constituents. Parsing proceeds by 
first seeding the table with items anchored on 
the input string, and then repeatedly scanning 
the table for parser actions. Parser actions 
introduce new items into the table licensed by 
one or more items already in the table. The 
main types of parser actions are: 
1. extending a constituent by incorporating 
a complete subconstituent (on the left or 
1However, due to lack of space, no proofs and only 
minimal informal descriptions are given in this paper. 
372 
right); 
2. extending a constituent by adjoining a sur
rounding complete auxiliary constituent; 
3. predicting the span of the foot node of an 
auxiliary constituent (to the left or right). 
Parsing is complete when all possible parser ac
tions have been executed. 
In a completed parse table it is possible to 
trace the sequence of items corresponding to the 
recognition of an elementary tree from its lexi
cal anchor upwards. Each item in the sequence 
corresponds to a node in the tree (with the se
quence as a whole corresponding to a complete 
traversal of the tree), and each step corresponds 
to the parser action that licensed the next item, 
given the current one. From this perspective, 
parser actions can be restated relative to the 
items in such a sequence as: 
1. substitute a complete subconstituent (on 
the left or right); 
2. adjoin a surrounding complete auxiliary 
constituent; 
3. predict the span of the tree's foot node (to 
the left or right). 
The recognition of the tree can thus be viewed 
as the computation of a finite state atttomaton, 
whose states correspond to a traversal of the 
tree and whose input symbols are these rela
tivised parser actions. 
This perspective suggests a re-casting of the 
conventional LTAG parser in terms of such au
tomata 2. For this automaton-based parser, the 
grammar structures are not trees, but automata 
corresponding to tree traversals whose inputs 
are strings of relativised parser actions. Items 
in the parse table reference automaton states 
instead of tree addresses, and if the automa
ton state is final, the item represents a complete 
constituent. Parser actions arise as before, but 
are executed by relativising them with respect 
to the incomplete item participating in the ac
tion, and passing this relativised parser action 
as the next input symbol for the automaton ref
erenced by that item. The resulting state of 
that automaton is then used as the referent of 
the newly licensed item. 
On a first pass, this re-casting is exactly that: it 
does nothing new or different from the original 
~Evans and Weir (1997) provides a longer informal 
introduction to this approach. 
parser on the original grammar. However there 
are a number of subtle differences3: 
• the automata are more abstract than the 
trees: the only grammatical information 
they contain are the input symbols and the 
root node labels, indicating the category of 
the constituent the automaton reeognises; ,, automata for several trees can be merged 
together and optimised using standard 
well-studied techniques, resulting in a sin
gle automaton that recognises many trees 
at once, sharing as many of the common 
parser actions as possible. 
It is this final point which is the focus of this 
paper. By representing trees as automata, we 
can merge trees together and apply standard 
optinfisation techniques to share their common 
structure. The parser will remain unchanged, 
but will operate more efficiently where struc
ture has been shared. Additionally, because 
the automata are more abstract than the trees, 
capturing precisely the parser's view of the 
trees, sharing may occur between trees which 
are structurally quite different, but which hap
pen to have common parser actions associated 
with them. 
3 Merging
and minimising automata 
Combining the automata for several trees can 
be achieved using a variety of standard algo
rithms (Huffman, 1954; Moore, 1956). How
ever any transformations must respect one im
portant feature: once the parser reaches a fi
nal state it needs to know what tree it has just 
recognised 4. When automata for trees with dif
ferent root categories are merged, the resulting 
automaton needs to somehow indicate to the 
parser what trees are associated with its final 
states. 
In Evans and Weir (1997), we combined au
tomata by introducing a new initial state with 
e-transitions to each of the original initial states, 
3A further difference is that the traversal encoded 
in the automaton captures part of the parser's control 
strategy. However for simplicity we assume here a fixed 
parser control strategy (bottom-up, anchor-out) and do 
not pursue this point further Evans and Weir (1997) 
offers some discussion. 
4For recognition alone it only needs to know the root 
category of the tree, but to recover the parse it needs to 
identify the tree itself. 
373 
and then determinising the resulting automa
ton to induce some sharing of structure. To 
recover trees, final automaton states were an
notated with the number of the tree the final 
state is associated with, which the parser can 
then readily access. 
However, the drawback of this approach is that 
differently annotated final states can never be 
merged, which restricts the scope for structure 
sharing (minirnisation, for example, is not pos
sible since all the final states are distinct). To 
overcome this, we propose an alternative ap
proach as follows: 
• each automaton transition is annotated 
with the set of trees which pass through 
it: when transitions are merged in au
tomaton optimisation, their annotations 
are unioned; 
• the parser maintains for each item in the 
table the set of trees that are valid for the 
item: initially this is all the valid trees for 
the automaton, but gets intersected with 
the annotation of any transition followed; 
also if two paths through the automaton 
meet (i.e., an item is about to be added 
for a second time), their annotations get 
unioned. 
This approach supports arbitrary merging of 
states, including merging all the final states into 
one. The parser maintains a dynamic record of 
which trees are valid for states (in particular fi
nal states) in the parse table. This means that 
we can minimise our automata as well as deter
minising them, and so share more structure (for 
example, common processing at the end of the 
recognition process as well as the beginning). 
4 Recognition
and parse recovery 
We noted above that a parsing algorithm 
needs to be able to access the tree that 
an automaton has recognised. The algo
rithm we describe below actually needs rather 
more information than this, because it uses a 
two-phase recognition/parse-recovery approach. 
The recognition phase only needs to know, for 
each complete item, what the root label of the 
tree recognised is. This can be recovered from 
the ~valid tree' annotation of the complete item 
itself (there may be more than one valid tree, 
corresponding to a phrase which has more than 
one parse which happen to have been merged to
gether). Parse recovery, however, involves run
ning the recogniser 'backwards' over the com
pleted parse table, identifying for each item, the 
items and actions which licensed it. 
A complication arises because the automata, es
pecially the merged automata, do not directly 
correspond to tree structure. The recogniser re
turns the tree recognised, and a search of the 
parse table reveals the parser action which com
pleted its recognition, but that information in 
itself may not be enough to locate exactly where 
in the tree the action took place. However, the 
additional information required is static, and 
so can be pre-compiled as the automata them
selves are built up. For each action transition 
(the action, plus the start and finish states) 
we record the tree address that the transition 
reaches (we catl this the action-site, or just 
a-site for short). During parse recovery, when 
the parse table indicates an action that licensed 
an item, we look up the relevant transition to 
discover where in the tree (or trees, if we are 
traversing several simultaneously) the present 
item must be, so that we can correctly construct 
a derivation tree. 
5 Technical
details 
5.1 Constructing
the automata 
We identify each node in an elementary tree 7 
with an elementary address 7/i. The root 
of 7 has the address 7/e where e is the empty 
string. Given a node v/i, its n children are ad
dressed from left to right with the addresses 7/il,...v/in, 
respectively. For convenience, 
let anchor (7) and foot (7) denote the elemen
tary address of the node that is the anchor and 
footnode (if it has one) of 7, respectively; and 
label (7/i) and parent (7/i) denote the label of 7/i 
and the address of the parent of v/i, respec
tively. 
In this paper we make the following assumup
tions about elementary trees. Each tree has a 
single anchor node and therefore a single spine 5. 
In the algorithms below we assume that nodes 
not on the spine have no children. In practice, 
not all elementary LTAG trees meet these con
ditions, and we discuss how the approach de
scribed here might be extended to the more gen
5The path from the root to the anchor node. 
374 
eral case in Section 6. 
Let "y/i be an elementary address of a 
node on the spine of 7 with n children 
~f/il,...,',/ilk,...,~/in for n > 1, where k is 
such that "~ilk dominates anchor ("/). 
7/ik+l ifj=l&n>k 
7~i j-1 if2_<j_<k next(3'/ij)= 3"/ij+l ifk<j <n 
7/i otherwise 
next defines a function that traverses a spine, 
starting at the anchor. Traversal of an elemen
tary tree during recognition yields a sequence of 
parser actions, which we annotate as follows: 
the two actions A and ~ indicate a substitu
tion of a tree rooted with A to the left or right, 
respectively; __A, and ~_A_ indicate the presence 
of the foot node, a node labelled A, to the left 
or right, respectively; Finally A indicates an 
adjunetion of a tree with root and foot labelled 
A. These actions constitute the input language 
of the automaton that traverses the tree. This 
automaton is defined as follows (note that we 
use e-transitions between nodes to ease the con
struction we assume these are removed using 
a standard algorithm). 
Let ~' be an elementary tree with terminal and 
nonterminal alphabets VT and VN, respectively. 
Each state of the following automaton specifies 
the elementary address 7/i being visited. When 
the node is first visited we use the state -\[-\[7/i\]; 
when ready to move on we use the state T\[~/i\]. 
Define as follows the finite state automaton 
M = (Q, E, _l_ \[anchor ('y)\], 6, F). Q is the set 
of states, E is the input alphabet, q0 is the ini
tial state, 5 is the transition relation, and F is 
the set of final states. 
Q = { T\['Ui \], ±\[3"/i\] \[3"/i is an address in 7 }; 
--,, ,~+ IA e VN ; 
F = { T\[3"/e\] }; and 
6 includes the following transitions: 
(±\[foot ('7)\], __A,, T\[foot (3')\]) if foot (3') is to the right 
of anchor (3") 
(_t_\[foot (3')\], ÷A, T\[foot (3')\]), if foot (3') is to the left 
of anchor (3") 
{ (T\[3"/i\], e, ±\[next (3'/i)\]) I 3"/i is an address in 3' 
{ (±\[3'/i\],_~, -r\[3'/i\]) I 3"/i substitution node, 
label (3"/i) = A, 
7/i to right of anchor (3') } 
{ (±\[^//il, ,d_, "r\[~/i\]) I 3"/i substitution node, 
label (3"/i) = A, 
?/i to left of anchor (~r') } 
( (±\[3'/i\], ~, T\[3'/i\]) I ~//i adjunction node 
label (3"/i) = A } 
{ (_1_\[3'/i\], ~, T\[3'/i D I ?/i adjunction node } 
{ (T\[7/i\], ~, T\[3"/i\]) I 3"/i adjunction node, 
label (3"/i) = d } 
In order to recover derivation trees, we also 
define the partial function a-site(q,a,q ~) for 
(q, a, q') E 5 which provides information about 
the site within the elementary tree of actions 
occurring in the automaton. 
q,) = ~ 3"/i ifa¢(~; q' = T\[~//i\] a-site(q, a~ 
undefined otherwise ( 
5.2 Combining
Automata 
Suppose we have a set of trees F = 
{3'1,...,7n }. Let M.n,... ,M.Y~ be the e-free 
automata that are built from members of the 
set F using the above construction, where for 
1 < k < n, Mk = (Qk,Ek,qk,bk, Fk). 
Construction of a single automaton for F is a 
two step process. First we build an automa
ton that accepts all elementary computations 
for trees in F; then we apply the standard au
tomaton determinization and minimization al
gorithms to produce an equivalent, compact au
tomaton. The first step is achieved simply by 
introducing a new initial state with e-transitions 
to each of the qk: 
Let M = (Q, E, q0, 5, F) where 
Q = ( q0 } u Ul<k<  Oi; 
Z = I.Jl<k<, Ek 
F = Ul<k<,~ F~ 
5 = Ul<~<,~(q0, e, qk) U U~<k<~ 5k. 
We determinize and then minimize M using 
the standard set-of-states constructions to pro
duce Mr = (Q', E, Q0,5',F'). Whenever two 
states are merged in either the determinizing 
or minimizing algorithms the resulting state is 
named by the union of the states from which it 
is formed. 
For each transition (QI,a, Q2) E 5' we define 
the function a-sites(Q1, a, Q2) to be a set of el
ementary nodes as follows: 
a-sites(Q1, a, Q2) = \[.Jq~ eQ~,q2eQ2 a-site(ql, a, q2) 
Given a transition in Mr, this function returns 
all the nodes in all merged trees which that tran
375 
sition reaches. 
Finally, we define: 
cross(Q1, a, Q2) = { 3' 17/i E a-sites(Q1, a, Q2) } 
This gives that subset of those trees whose el
ementary computations take the Mr through 
state Q1 to Q2. These are the transition an
notations referred to above, used to constrain 
the parser's set of valid trees. 
5.3 The
Recognition Phase 
This section illustrates a simple bottom-up 
parsing algorithm that makes use of minimized 
automata produced from sets of trees that an
chor the same input symbol. 
The input to the parser takes the form of a se
quence of minimized automata, one for each of 
the symbols in the input. Let the input string 
be w = al...an and the associated automata 
be MI,... Mn where Mk = (Qk, Ek, qk, 5k, Fk) 
for 1 < k < n. Let treesof(Mk) = Fk where Fk 
is a set of the names of those elementary trees 
that were used to construct the automata Mk. 
During the recognition phase of the algorithm, 
a set I of items are created. An item has 
the form (T, q, \[l, r, l', r'\]) where T is a set of 
elementary tree names, q is a automata state 
and l,r,l',r' E {0,... ,n,-} such that either 
l<l'<r'<rorl<randl'=r'=-. The in
dices l, l', #, r are positions between input sym
bols (position 0 is before the first input symbols 
and position n is after the final input symbol) 
and we use Wp,p, to denote that substring of the 
input w between positions p and p'. I can be 
viewed as a four dimensional array, each entry 
of which contains a set of pairs comprising of a 
set of nonterminals and an automata state. 
Roughly speaking, an item (T, q, \[I, r, l', r\]) is in
eluded in I when for every 7 E T, anchored 
by some ak (where I < k _< r and if l' # 
then k < l' or r' _< k); q is a state in Qk, such 
that some elementary subcomputation reaching 
q from the initial state, qk, of Mk is an ini
tial substring of the elementary computation for 
7 that reaches the elementary address 7/i, the 
subtree rooted at 7/i spans Wt,r, and ifT/i dom
inates a foot node then that foot node spans 
We,r,, otherwise l' = r' = 
The input is accepted if an item 
(T, qf,\[O,n, , \]) is added to I where T 
contains some initial tree rooted in the start 
symbol S and ql E Fk for some k. 
When adding items to I we use the procedure 
add(T, q, \[l, r, l', r'\]) which is defined such that 
if there is already an entry (T', q, \[l, r, l', r'\]) E 
I for some T' then replace this with the entry 
(T U T', q, \[l, r,l', rq)6; otherwise add the new 
entry (T, q, \[/, r, l', r'\]) to I. 
I is initialized as follows. For each k E 
{ 1,... ,n } call add(T, qk,\[k1,k,-,-\]) where 
T = treesof(Mk) and qk is the initial state of 
the automata Mk. 
We now present the rules with which the com
plete set I is built. These rules correspond 
closely to the familiar steps in existing bottom
up LTAG parser, in particular, the way that 
we use the four indices is exactly the same as 
in other approaches (Vijay-Shanker and Joshi, 
1985). As a result a standard control strategy 
can be used to control the order in which these 
rules are applied to existing entries of I. 
1. If (T,q,\[l,r,l',r'\]),(T',qs,\[r,r", -, -\])El, 
qf E Fk for some k, (q,A,q~) E 5k, for 
some U, label (~,l/e) = A from some ~/i E 
T ~ ~z T" = T VI cross(q,A,q~) then call 
add(T", q', \[1, r", l', r'\]). 
2. If <T, q, \[l, r, l', r'\]), iT', q$, \[l", l, -, -\]> e I, 
qf E Fk for some k, (q,A,q') E 5k, for 
some k', label(7'/e) = A from some 71 E 
T' gz T" = T A cross(q,a,q') then call 
add(T", q', \[l", r, l', r'\]>. 
3. If(T,q,\[1, r,-,-\]) EI, (q, A,q,) eSk for 
some k & T' = T A cross(q,_A,,q') then 
for each r ~ such that r < r / < n call 
add(T', q', \[/, r', r, r'\]). 
4. If (T, q, \[l, r, -, -\]) E I, (q,÷A__,q') E ~k 
for some k & T' = T M cross(q,÷A,q~) 
then for each l' such that 0 < l' < l call 
add(T', q', \[/', r, l', l\]). 
5. If (T,q,\[l,r,l',r'\]),(T',qf,\[l",r",l,r\]) G I, 
qf E Fk for some k, (q,A,q') E 5k' for 
some k', label (~//e) = A from some ~/ E 
T' & T" = T V~ eross(q,~,q') then call 
add(T", q', \[l", r", l', r'\]). 
°This replacement is treated as a new entry in the 
table. If the old entry has already licenced other entries, 
this may result in some duplicate processing. This could 
be eliminated by a more sophisticated treatment of tree 
sets. 
376 
The running time of this algorithm is O(rfi) 
since the last rule must be embedded within six 
loops each of which varies with n. Note that 
although the third and fourth rules both take 
O(n) steps, they need only be embedded within 
the 1 and r loops. 
5.4 Recovering
Parse Trees 
Once the set of items I has been completed, the 
final task of the parser is to a recover derivation 
tree 7. This involves retracing the steps of the 
recognition process in reverse. At each point, 
we look for a rule that would have caused the 
inclusion of item in I. Each of these rules in
volves some transition (q, a, q') ~ 5k for some k 
where a is one of the parser actions, and from 
this transition we consult the set of elementary 
addresses in a-sites(q, a, q') to establish how to 
build the derivation tree. We eventually reach 
items added during the initialization phase and 
the process ends. Given the way our parser has 
been designed, some search will be needed to 
find the items we need. As usual, the need for 
such search can be reduced through the inclu
sion of pointers in items, though this is at the 
cost of increasing parsing time. There are var
ious points in the following description where 
nondeterminism exists. By exploring all possi
ble paths, it would be straightforward to pro
duce an AND/OR derivation tree that encodes 
all derivation trees for the input string. 
We use the procedure der((T, q, \[l, r, l', r'\]), r) 
which completes the partial derivation tree r by 
backing up through the moves of the automata 
in which q is a state. 
A derivation tree for the input is returned 
by the call der(<T, qf, \[O, n, -, -\]), ~-) where 
(T, qf , \[O, n, -, -\]) E I such that T contains 
some initial tree 7 rooted with the start non
terminal S and qI is the final state of some au
tomata Mk, 1 < k <_ n. r is a derivation tree 
containing just one node labelled with name 7. 
In general, on a call to der(<T, q, \[l, r, l', r'\]), T) 
we examine I to find a rule that has caused this 
item to be included in I. There are six rules 
to consider, corresponding to the five recogniser 
rules, plus lexical introduction, as follows: 
1. If (T',q',\[l,r",I',r'\]>,(T",qf,\[r",r,-,-\]l e 
TDerivation trees are labelled with tree names and 
edges are labelled with tree addresses. 
I, qI E Fk for some k, (q',A,q) E 6k, for 
some k', 7 is the label of the root of % 
3' E T', label (-'//e) = A from some 7' E T" 
& ~//i E a-sites(q', A,q), then let z' be the 
derivation tree containing a single node 
labelled 7', and let T" be the result of at
taching der((T", q/, \[r", r, -,-\]), ~-') under 
the root of T with an edge labelled the tree 
address i. We then complete the derivation 
tree by calling der((T', q', \[l, r", l', r'\]}, T"). 
2. If (T', q', \[r", r, l', r'\]>, <T", qf, \[l, r", -, -\]) e 
I, qf E Fk for some k, (q',A,q) ~ @ for 
some U 7 is the label of the root of y, 
7 E
T', label (7'/e) = A from some 7' E T" 
& 7/i E a-sites(q', +__a, q), then let r' be the 
derivation tree containing a single node 
labelled 7', and let r" be the result of at
taching der((T", qs, \[l, r", -, -\]), r') under 
the root of r with an edge labelled the tree 
address i. We then complete the derivation 
tree by calling der((T', q', \[r", r, l', r'\]>, r"). 
3. If r = r', <T',q',\[l,l',-,-\]) ~ I and 
(q,,_A,,q) 6 c~ k for some k, 7 is the 
label of the root of T, 7 E T' and 
foot (7) E a-sites(q',_A,, q) then make the 
call der((T', q', \[l, l', -, -\]), r). 
4. If l = l', <T',q',\[r',r,-,-\]) E I and 
(q,,÷Aq,) E 6k tbr some k, 3' is the 
label of the root of % 7 E T' and 
foot (3') E a-sites(q', ÷A q) then make the 
call der(<T', q', \[r', r, -, -\]>, ~-). 
5. If <T',q',\[l",r",l',r'\]), <T",qf,\[l,r,l",r"\]) E 
I, qs E Fk for some k, (q',A,q) E @ for 
some k', 7 is the label of the root of "r, 
3' E T', label (7'/e) = A from some 7' E T" 
and "y/i E a-sites(q', A,q), then let T' be 
the derivation tree containing a single node 
labelled 7% and let "r" be the result of at
taching der(<T",qi, \[1, r,l",r"\]), r') under 
the root of -r with an edge labelled the tree 
address i. We then complete the derivation 
tree by calling der(<T', q', \[/", r", l', r'\]), ~-"). 
6. If I + 1 = r, r' = l' = q is the initial state 
of M~, 3' is the label of the root ofT-, 7 E T, 
then return the final derivation tree 7-. 
6 Discussion

The approach described here offers empirical 
rather than formal improvements in perfor
mance. In the worst case, none of the trees 
377 
word 
come 
break 
give 
no. of trees automaton no. of states no. of transitions trees per state 
133 merged 898 1130 1 
minimised 50 130 11.86 
177 merged 1240 1587 1 
minimised 68 182 12.13 
337 merged 2494 3177 1 
minimised 83 233 20.25 
Table 1: DTG compaction results (from Carroll et al. (1998)). 
in the grammar share any structure so no op
timisation is possible. However, in the typi
cal case, there is scope for substantial structure 
sharing among closely related trees. Carroll et 
al. (1998) report preliminary results using this 
technique on a wide-coverage DTG (a variant 
of LTAG) grammar. Table 1 gives statistics for 
three common verbs in the grammar: the total 
number of trees, the size of the merged automa
ton (before any optimisation has occurred) and 
the size of the minimised automaton. The fi
nal column gives the average of the number of 
trees that share each state in the automaton. 
These figures show substantial optimisation is 
possible, both in the space requirements of the 
grammar and in the sharing of processing state 
between trees during parsing. 
As mentioned earlier, the algorithms we have 
presented assume that elementary trees have 
one anchor and one spine. Some trees, how
ever, have secondary anchors (for example, a 
subcategorised preposition). One possible way 
of including such cases would be to construct 
automata from secondary anchors up the sec
ondary spine to the main spine. The automata 
for both the primary and secondary anchors 
associated with a lexieal item could then be 
merged, minimized and used for parsing as 
above. 
Using automata for parsing has a long his
tory dating back to transition networks (Woods, 
1970). More recent uses include Alshawi (1996) 
and Eisner (1997). These approaches differ from 
the present paper in their use of automata as 
part of the grammar formalism itself. Here, 
automata are used purely as a stepping-stone 
to parser optimisation: we make no linguistic 
claims about them. Indeed one view of this 
work is that it frees the linguistic descriptions 
from overt computational considerations. This 
work has perhaps more in common with the 
technology of LR parsing as a parser optimi
sation technique, and it would be interesting to 
compare our approach with a direct application 
of LR ideas to LTAGs. 

References 

H. Alshawi. 1996. Head automata and bilingual tilings: Translation with minimal representations. In ACL96, pages 167-176. 

J. Carroll, N. Nicolov, O. Shaumyan, M. Smets, and D. Weir. 1998. Grammar compaction and computation sharing in automaton-based parsing. In Proceedings of the First Workshop on Tabulation in Parsing and Deduction, pages 16-25. 

J. Chen and K. Vijay-Shanker. 1997. Towards a reduced-commitment D-theory style TAG parser. In IWPT97, pages 18-29. 

J. Eisner. 1997. Bilexical grammars and a cubic-time probabilistic parser. In IWPT97, pages 54-65. 

R. Evans and D. Weir. 1997. Automaton-based parsing for lexicalized grammars. In IWPT97, pages 66-76. 

D. A. Huffman. 1954. The synthesis of sequential switching circuits. J. Franklin Institute. 

A. K. Joshi and Y. Schabes. 1991. Tree-adjoining grammars and lexicalized grammars. In Maurice Nivat and Andreas Podelski, editors, Definability and Recognizability of Sets of Trees. Elsevier. 

E. F. Moore, 1956. Automata Studies, chapter Gedanken experiments on sequential machines, pages 129-153. Princeton University Press, N.J. 

Y. Schabes and A. K. Joshi. 1988. An Earley-type parsing algorithm for tree adjoining grammars. In ACL88. 

K. Vijay-Shanker and A. K. Joshi. 1985. Some computational properties of tree adjoining grammars. In A CL85, pages 82-93. 

K. Vijay-Shanker and D. Weir. 1993. Parsing some constrained grammar formalisms. Computational Linguistics, 19(4):591-636. 

W. A. Woods. 1970. Transition network grammars for natural language analysis. Commun. ACM, 13:591-606. 378 

