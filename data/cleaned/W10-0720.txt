Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 131–138,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics
Not-So-LatentDirichletAllocation:
CollapsedGibbsSamplingUsingHumanJudgments
JonathanChang
Facebook
1601 S. CaliforniaAve.
Palo Alto, CA 94304
jonchang@facebook.com
Abstract
Probabilistictopic models are a popular tool
for the unsupervisedanalysisof text, providing
both a predictive model of future text and a la-
tent topic representationof the corpus. Recent
studies have found that while there are sugges-
tive connectionsbetweentopic models and the
way humansinterpretdata, these two often dis-
agree. In this paper, we explore this disagree-
ment from the perspective of the learningpro-
cess rather than the output. We present a novel
task, tag-and-cluster, which asks subjects to
simultaneouslyannotatedocumentsand cluster
those annotations. We use these annotations
as a novel approach for constructing a topic
model, groundedin human interpretationsof
documents. We demonstratethat these topic
models have featureswhich distinguishthem
from traditionaltopic models.
1 Introduction
Probabilistictopic modelshave becomepopulartools
for the unsupervised analysis of large document
collections (Deerwester et al., 1990; Griffiths and
Steyvers, 2002; Blei and Lafferty, 2009). These mod-
els posit a set of latenttopics, multinomialdistribu-
tions over words, and assume that each document
can be describedas a mixtureof these topics. With
algorithmsfor fast approximateposteriorinference,
we can use topic models to discover both the topics
and an assignment of topics to documents from a
collectionof documents.(See Figure 1.)
These modeling assumptions are useful in the
sense that, empirically, they lead to good models
of documents(Wallach et al., 2009). However, re-
cent work has explored how these assumptionscorre-
spond to humans’understandingof language(Chang
et al., 2009; Griffiths and Steyvers, 2006; Mei et al.,
2007). Focusing on the latent space, i.e., the inferred
mappings between topics and words and between
documentsand topics, this work has discovered that
althoughthere are some suggestive correspondences
betweenhuman semanticsand topic models,they are
often discordant.
In this paper we build on this work to further
explore how humans relate to topic models. But
whereas previous work has focused on the results
of topic models, here we focus on the process by
which these modelsare learned. Topic modelslend
themselves to sequential proceduresthroughwhich
the latent space is inferred; these proceduresare in
effect programmaticencodingsof the modelingas-
sumptions. By substitutingkey steps in this program
with human judgments,we obtain insights into the
semanticmodel conceived by humans.
Here we present a novel task, tag-and-cluster,
which asks subjectsto simultaneouslyannotatea doc-
umentand clusterthat annotation.Thistask simulates
the samplingstep of the collapsedGibbs sampler(de-
scribed in the next section),except that the posterior
defined by the model has been replaced by human
judgments. The task is quick to complete and is
robust against noise. We report the results of a large-
scale humanstudy of this task, and show that humans
are indeedable to constructa topic modelin this fash-
ion, and that the learned topic model has semantic
propertiesdistinct from existing topic models. We
also demonstratethat the judgmentscan be used to
guidecomputer-learnedtopic modelstowards models
which are more concordantwith human intuitions.
2 Topicmodelsandinference
Topic models posit that each documentis expressed
as a mixture of topics. These topic proportionsare
drawn once per document,and the topics are shared
across the corpus. In this paper we focus on Latent
Dirichletallocation(LDA) (Blei et al., 2003) a topic
model which treats each document’s topic assign-
ment as a multinomialrandomvariable drawn from
a symmetricDirichletprior. LDA, when applied to a
collectionof documents,will build a latent space: a
collectionof topics for the corpus and a collectionof
topic proportionsfor each of its documents.
131
computer, 
technology, 
system, 
service, site, 
phone, 
internet, 
machine
play, film, 
movie, theater, 
production, 
star, director, 
stage
sel, sale, 
store, product, 
business, 
advertising, 
market, 
consumer
TOPIC 1
TOPIC 2
TOPIC 3
(a) Topics
Forget the 
Bootleg, Just 
Download the 
Movie Legaly
Multiplex Heralded 
As Linchpin To 
Growth
The Shape of 
Cinema, 
Transformed At 
the Click of a 
Mouse
A Peaceful Crew 
Puts Mupets 
Where Its Mouth Is
Stock Trades: A 
Beter Deal For 
Investors Isn't 
Simple
Internet portals 
begin to distinguish 
among themselves 
as shoping mals
Red Light, Green 
Light: A 
2-Tone L.E.D. to 
Simplify Screens
TOPIC 2
"BUSINESS"
TOPIC 3
"ENTERTAINMENT"
TOPIC 1
"TECHNOLOGY"
(b) DocumentAssignmentsto Topics
Figure 1: The latent space of a topic model consistsof topics, which are distributions over words, and a distribution
over these topics for each document. On the left are three topics from a fifty topic LDA model trained on articles from
the New York Times. On the right is a simplex depictingthe distribution over topics associatedwith seven documents.
The line from each document’s title shows the document’s positionin the topic space.
LDA can be describedby the following generative
process:
1. For each topick,
(a) Draw topicβk∼Dir(η)
2. For each documentd,
(a) Draw topic proportionsθd∼Dir(α)
(b) For each wordwd,n,
i. Draw topic assignmentzd,n∼Mult(θd)
ii. Draw wordwd,n∼Mult(βzd,n)
This process is depictedgraphicallyin Figure 2. The
parameters of the model are the number of topics,
K, as well as the Dirichletpriors on the topic-word
distributions and document-topicdistributions,αand
η. The only observed variables of the model are
the words, wd,n. The remaining variables must be
learned.
There are several techniquesfor performingpos-
terior inference, i.e., inferring the distribution over
hidden variablesgiven a collection of documents,in-
cludingvariationalinference(Blei et al., 2003) and
Gibbs sampling(Griffiths and Steyvers, 2006). In the
sequel, we focus on the latter approach.
CollapsedGibbssamplingfor LDA treatsthe topic-
word and document-topicdistributions,θd andβk, as
nuisancevariables to be marginalizedout. The poste-
rior distribution over the remaininglatent variables,
D
N
θ
d
α
β
k
η
z
d,n
w
d,n
K
Figure 2: A graphicalmodel depictionof latent Dirichlet
allocation(LDA). Plates denote replication.The shaded
circle denotesan observed variable and unshadedcircles
denote hidden variables.
the topic assignmentszd,n, can be expressedas
p(z|α,η,w)∝
productdisplay
d
productdisplay
k
bracketleftbigg
Γ(nd,k +αk)Γ(ηwd,n +nwd,n,k)Γ(summationtext
wnw,k +ηw)
bracketrightbigg,
wherend,k denotesthe numberof words in document
d assignedto topick andnw,k the number of times
word w is assigned to topic k. This leads to the
samplingequations,
p(zd,i = k|α,η,w,zy)∝
(n¬d,id,k +αk)
ηwd,i +n¬d,iwd,i,k
summationtext
wn
¬d,i
w,k +ηw, (1)
where the superscript¬d,i indicatesthat these statis-
132
tics should exclude the current variable under consid-
eration,zd,i.
In essence,the model performsinference by look-
ing at each word in succession,and probabilistically
assigningit to a topic accordingto Equation1. Equa-
tion 1 is derived throughthe modelingassumptions
and choice of parameters. By replacing Equation1
with a different equationor with empiricalobserva-
tions, we may construct new models which reflect
different assumptionsabout the underlyingdata.
3 Constructing
topics using human judg-
ments
In this section we propose a task which creates a
formal setting where humans can create a latent
space representationof the corpus. Our task,tag-and-
cluster, replacesthe collapsedGibbs samplingstep
of Equation 1 with a human judgment. In essence,
we are constructinga gold-standardseries of samples
from the posterior.1
Figure 3 shows how tag-and-clusteris presented
to users. The user is shown a documentalong with its
title; the documentis randomlyselectedfrom a pool
of available documents. The user is asked to select
a word from the documentwhich is discriminative,
i.e, a word which would help someone looking for
the document find it. Once the word is selected,the
user is then asked to assign the word to the topic
which best suits the sense of the word used in the
document. Users are specificallyinstructedto focus
on the meaningsof words, not their syntacticusage
or orthography.
The user assigns a word to a topic by selectingan
entry out of a menu of topics. Each topic is repre-
sented by the five words occuringmost frequentlyin
that topic. The order of the topics presentedto the
user is determinedby the number of words in that
documentalready assigned to each topic. Once an
instanceof a word in a documenthas been assigned,
it cannot be reassigned and will be marked in red
when subsequentusers encounterthis document. In
practice,we also prohibitusers from selectinginfre-
quentlyoccurringwords and stop words.
1Additionally, since Gibbs sampling is by nature stochastic,
we believe that the task is robust against small perturbationsin
the quality of the assignments,so long as in aggregate they tend
toward the mode.
4 Experimentalresults
We conductedour experimentsusing Amazon Me-
chanical Turk, which allows workers (our pool of
prospective subjects)to performsmall jobs for a fee
through a Web interface. No specialized training
or knowledge is typically expected of the workers.
AmazonMechanicalTurk has been successfullyused
in the past to develop gold-standarddata for natural
languageprocessing(Snow et al., 2008).
We prepare two randomly-chosen,100-document
subsets of EnglishWikipedia. For convenience,we
denote these two sets of documents as set1 and set2.
For each document, we keep only the first 150 words
for our experiments. Because of the encyclopedic
nature of the corpus, the first 150 words typically
provides a broadoverview of the themesin the article.
We also removed from the corpus stop words and
words whichoccurinfrequently2, leading to a lexicon
of 8263 words. After this pruning set1 contained
11614 words and set2 contained11318 words.
Workers were asked to performtwenty of the tag-
gings describedin Section 3 for each task; workers
were paid $0.25 for each such task. The number of
latent topics,K, is a free parameter. Here we explore
two values of this parameter, K = 10 andK = 15,
leadingto a total of four experiments— two for each
set of documentsand two for each value ofK.
4.1 Taggingbehavior
For each experimentwe issued 100 HITs, leading to
a total of 2000 tags per experiment. Figure 4 shows
the number of HITs performed per person in each
experiment. Between12 and 30 distinct workers par-
ticipated in each experiment. The number of HITs
performed per person is heavily skewed, with the
most active participantscompletingan order of mag-
nitude more HITs than other particpants.
Figure 5 shows the amountof time taken per tag,
in log seconds. Each color represents a different
experiment. The bulk of the tags took less than a
minute to performand more than a few seconds.
4.2 ComparisonwithLDA
Learnedtopics As describedin Section 3, the tag-
and-clustertask is a way of allowing humansto con-
2Infrequentlyoccurring words were identified as those ap-
pearing fewer than eight times on a larger collection of 7726
articles.
133
Figure 3: Screenshotsof our task. In the center, the documentalong with its title is shown. Words which cannot be
selected,e.g., distractors and words previously selected,are shown in red. Once a word is selected,the user is asked to
find a topic in which to place the word. The user selects a topic by clickingon an entry in a menu of topics, where each
topic is expressedby the five words which occur most frequentlyin that topic.
0 5 10 15 20 25 30
0
5
10
15
20
25
(a) set1, K = 10
0 2 4 6 8 10 12
0
10
20
30
40
50
60
(b) set1, K = 15
0 5 10 15 20
0
10
20
30
40
50
(c) set2, K = 10
0 5 10 15 20 25 30
0
5
10
15
20
(d) set2, K = 15
Figure 4: The numberof HITs performed(y-axis)by each participant(x-axis). Between12 and 30 people participated
in each experiment.
(a) K = 10
0.050.1 0.2 0.5 1 2 set1set2
0
0.5
1
1.5
2
2.5
3
entrop
y
α
(b) K = 15
Figure 6: A comparisonof the entropy of distributions drawn from a Dirichletdistribution versus the entropy of the
topic proportionsinferredby workers. Each columnof the boxplotshows the distribution of entropiesfor 100 draws
from a Dirichletdistribution with parameterα. The two rightmostcolumns show the distribution of the entropy of the
topic proportionsinferredby workers on set1 and set2. Theα of workers typicallyfalls between0.2 and 0.5.
134
Table 1: The five words with the highest probabilitymass in each topic inferredby humansusing the task describedin
Section3. Each subtableshows the results for a particular experimentalsetup. Each row is a topic; the most probable
words are orderedfrom left to right.
(a) set1, K = 10
railway lighthouse rail huddersfield station
school college education history conference
catholic church film music actor
runners team championships match racing
engine company power dwight engines
university london british college county
food novel book series superman
november february april august december
paint photographs american austin black
war history army american battle
(b) set2, K = 10
president emperor politician election government
american players swedish team zealand
war world navy road torpedo
system pop microsoft music singer
september 2007 october december 1999
television dog name george film
people malay town tribes cliff
diet chest enzyme hair therapy
british city london english county
school university college church center
(c) set1, K = 15
australia knee british israel set
catholic roman island village columbia
john devon michael austin charles
school university class community district
november february 2007 2009 2005
lighthouse period architects construction design
railway rail huddersfield ownership services
cyprus archdiocese diocese king miss
carson gordon hugo ward whitney
significant application campaign comic considered
born london american england black
war defense history military artillery
actor film actress band designer
york michigan florida north photographs
church catholic county 2001 agricultural
(d) set2, K = 15
music pop records singer artist
film paintings movie painting art
school university english students british
drama headquarters chess poet stories
family church sea christmas emperor
dog broadcast television bbc breed
champagne regular character characteristic common
election government parliament minister politician
enzyme diet protein hair oxygen
war navy weapons aircraft military
september october december 2008 1967
district town marin america american
car power system device devices
hockey players football therapy champions
california zealand georgia india kolkata
0 10 20 30 40
−70000
−65000
−60000
−55000
−50000
log lik
elihood
iteration
(a) set1, K = 10
0 10 20 30 40
−70000
−65000
−60000
−55000
−50000
−45000
log lik
elihood
iteration
(b) set2, K = 10
0 10 20 30 40
−70000
−65000
−60000
−55000
−50000
log lik
elihood
iteration
(c) set1, K = 15
0 10 20 30 40
−70000
−65000
−60000
−55000
−50000
−45000
log lik
elihood
iteration
(d) set2, K = 15
Figure 7: The log-likelihood achieved by LDA as a function of iteration. There is one series for each value of
α∈{0.05,0.1,0.2,0.5,1.0,2.0}from top to bottom.
135
Table 2: The five words with the highest probabilitymass in each topic inferredby LDA, withα = 0.2. Each subtable
shows the results for a particularexperimentalsetup. Each row is a topic; the most probablewords are orderedfrom left
to right.
(a) set1, K = 10
born 2004 team award sydney
regiment army artillery served scouting
line station main island railway
region street located site knee
food february conference day 2009
pride greek knowledge portland study
catholic church roman black time
class series film actor engine
travel human office management defense
school born war world university
(b) set2, K = 10
september english edit nord hockey
black hole current england model
training program war election navy
school university district city college
family word international road japan
publication time day india bridge
born pop world released march
won video microsoft project hungary
film hair bank national town
people name french therapy artist
(c) set1, K = 15
time michael written experience match
line station railway branch knowledge
film land pass set battle
william florida carson virginia newfoundland
war regiment british army south
reaction terminal copper running complex
born school world college black
food conference flight medium rail
township scouting census square county
travel defense training management edges
series actor engine november award
pride portland band northwest god
team knee 2004 sydney israel
catholic located site region church
class february time public king
(d) set2, K = 15
family protein enzyme acting oxygen
england producer popular canadian sea
system death artist running car
character series dark main village
english word publication stream day
training program hair students electrical
district town city local kolkata
september edit music records recorded
black pop bank usually hole
people choir road diet related
war built navy british service
center million cut champagne players
born television current drama won
school university college election born
film nord played league hockey
0 500 1000 1500 2000
−2
0
2
4
6
8
10
12
log(seconds betw
een tags)
Figure 5: The distribution of times taken per HIT. Each se-
ries representsa different experiment. The bulk of the tags
took less than one minute and more than a few seconds.
structa topicmodel. One way of visualizinga learned
topic modelis by examiningits topics. Table 1 shows
the topics constructedby human judgments. Each
subtable shows a different experimental setup and
each row shows an individual topic. The five most
frequentlyoccurringwords in each topic are shown,
orderedfrom left to right.
Many of the topics inferred by humans have
straightforward interpretations. For example, the
{november, february, april, august,
december}topic for the set1 corpus withK = 10
is simply a collection of months. Similar topics
(with years and months combined) can be found
in the other experimental configurations. Other
topics also cover specific semantic domains,
such as {president, emperor, politican,
election, government} or {music, pop,
records, singer, artist}. Several of the
topics are combinations of distinct concepts,
such as {catholic, church, film, music,
actor}, which is often indicative of the numberof
clusters,K, being too low.
136
Table 2 shows the topics learned by LDA un-
der the same experimental conditions, with the
Dirichlet hyperparameter α = 0.2 (we justify
this choice in the following section). These
topics are more difficult to interpret than the
ones created by humans. Some topics seem
to largely make sense except for some anoma-
lous words, such as {district, town, city,
local, kolkata} or {school, university,
college, election, born}. But the small
amountof data means that it is difficult for a model
which does not leverage prior knowledge to infer
meaningfultopic. In contrast,several humans,even
workingindependently, can leveragepriorknowledge
to constructmeaningfultopics with little data.
There is another qualitative difference between
the topics found by the tag-and-cluster task and
LDA. Whereas LDA must rely on co-occurrence,
humans can use ontological information. Thus, a
topic which has ontologicalmeaning,such as a list
of months,may rarely be discovered by LDA since
the co-occurrencepatternsof monthsdo not form a
strong pattern. But users in every experimentalcon-
figurationconstructedthis topic, suggestingthat the
users were consistentlyleveraginginformationthat
would not be available to LDA, even with a larger
corpus.
Hyperparameter values A persistent question
among practitionersof topic models is how to set or
learnthe value of the hyperparameterα. αis a Dirich-
let parameterwhich acts as a control on sparsity —
smaller values of α lead to sparser document-topic
distributions. By comparingthe sparsitypatternsof
human judgmentsto those of LDA for different set-
tings of α, we can infer the value of α that would
best match human judgments.
Figure 6 shows a boxplot comparisonof the en-
tropy of draws from a Dirichlet distribution (the gen-
erative process in LDA), versus the observed en-
tropy of the models learned by humans. The first
six columnsshow the distributions for the Dirichlet
draws for various values ofα; the last two columns
show the observed entropy distributions on the two
corpora,set1 and set2.
The empiricalentropy distributions across the cor-
pora are comparable to those of a Dirichlet distri-
bution with α between approximately0.2 and 0.5.
Table 3: The five words with the highest probabilitymass
in each topic inferred by LDA on set1 with α = 0.2,
K = 10, and initialized using human judgments. Each
row is a topic; the most probablewords are orderedfrom
left to right.
line station lighthouse local main
school history greek knowledge university
catholic church city roman york
team club 2004 scouting career
engine knee series medium reaction
located south site land region
food film conference north little
february class born august 2009
pride portland time northwest june
war regiment army civil black
Thesesettingsofαare slightlyhigherthan, but still in
line with a commonrule-of-thumbofα = 1/K. Fig-
ure 7 shows the log-likelihood, a measureof model
fit, achieved by LDA for each value of α. Higher
log-likelihoods indicate better fits. Commensurate
with the rule of thumb, using log-likelihoods to se-
lectα would encouragevalues smaller than human
judgments.
However, while the entropy of the Dirichletdraws
increasessignificantlywhen the numberof clusters
is increasedfromK = 10 toK = 15, the entropies
assigned by humans does not vary as dramatically.
This suggeststhat for any given document,humans
are likely to pull words from only a small numberof
topics, regardless of how many topics are available,
whereasa model will continueto spread probability
mass across all topics even as the number of topics
increases.
ImprovingLDAusinghumanjudgments The re-
sults of the previous sectionssuggeststhat human be-
havior differs from that of LDA, and that humanscon-
ceptualizedocumentsin ways LDA does not. This
motivates using the humanjudgmentsto augmentthe
informationavailable to LDA. To do so, we initialize
the topic assignmentsused by LDA’s Gibbs sampler
to those made by humans. We then run the LDA
samplertill convergence. This provides a method to
weaklyincorporatehumanknowledgeinto the model.
Table 3 shows the topics inferred by LDA when
initializedwith human judgments. These topics re-
semble those directlyinferredby humans,although
as we predicted in the previous sections, the topic
consistingof months has largely disappeared.Other
semantically coherent topics, such as {located,
137
Figure 8: The log likelihood achieved by LDA on set1
with α = 0.2, K = 10, and initialized using human
judgments(blue). The red line shows the log likelihood
without incorporatinghuman judgments. LDA with hu-
man judgmentsdominates LDA withouthumanjudgments
and helps the model converge more quickly.
south, site, land, region}, have appeared
in its place.
Figure 8 shows the log-likelihoodcourse of LDA
when initializedby human judgments(blue), versus
LDA without human judgments (red). Adding hu-
man judgmentsstrictly helps the model converge to
a higher likelihood and converge more quickly. In
short,incorporatinghumanjudgmentsshows promise
at improving both the interpretability and conver-
gence of LDA.
5 Discussion
We presented a new method for constructingtopic
models using human judgments. Our approach re-
lies on a novel task, tag-and-cluster, which asks
users to simultaneouslyannotate a document with
one of its words and to cluster those annotations.We
demonstrateusing experimentson AmazonMechan-
ical Turk that our method constructs topic models
quickly and robustly. We also show that while our
topic models bear many similaritiesto traditionally
constructedtopic models, our human-learnedtopic
models have unique features such as fixed sparsity
and a tendency for topics to be constructedaround
conceptswhich models such as LDA typicallyfail to
find.
We also underscorethat the collapsedGibbs sam-
pling framework is expressive enough to use as the
basis for human-guidedtopic model inference. This
may motivate, as future work, the constructionof dif-
ferent modelingassumptionswhich lead to sampling
equationswhich more closely match the empirically
observed samplingperformedby humans. In effect,
our method constructsa series of samplesfrom the
posterior, a gold standardwhich future topic models
can aim to emulate.
Acknowledgments
The author would like to thank Eytan Bakshy and
ProfessorJordan Boyd-Graber-Ying for their helpful
commentsand discussions.
References
David Blei and John Lafferty, 2009. Text Mining:Theory
and Applications, chapter Topic Models. Taylor and
Francis.
David Blei, Andrew Ng, and MichaelJordan. 2003. La-
tent Dirichletallocation. JMLR, 3:993–1022.
Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,
Chong Wang, and David M. Blei. 2009. Reading
tea leaves: How humans interpret topic models. In
NIPS.
Scott Deerwester, Susan Dumais, Thomas Landauer,
George Furnas,and RichardHarshman. 1990. Index-
ing by latent semanticanalysis. Journalof theAmeri-
canSocietyof InformationScience, 41(6):391–407.
Thomas L. Griffiths and Mark Steyvers. 2002. A prob-
abilistic approachto semanticrepresentation. In Pro-
ceedingsof the24thAnnualConferenceof theCogni-
tiveScienceSociety.
T. Griffiths and M. Steyvers. 2006. Probabilistictopic
models. In T. Landauer, D. McNamara,S. Dennis,and
W. Kintsch,editors,LatentSemanticAnalysis:A Road
to Meaning. LaurenceErlbaum.
QiaozhuMei, Xuehua Shen, and ChengXiangZhai. 2007.
Automatic labeling of multinomialtopic models. In
KDD.
R. Snow, Brendan O’Connor, D. Jurafsky, and A. Ng.
2008. Cheap and fast—but is it good? Evaluating
non-expert annotationsfor natural languagetasks. In
EMNLP.
Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,
and David Mimno. 2009. Evaluationmethodsfor topic
models. In ICML.
138

