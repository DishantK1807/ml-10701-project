The design of the sparse features was inspired by Zens and Ney (2006).
W06-3108
Liang et al.(2006) presented a similar updating strategy in which parameters were updated toward an oracle translation found inCt, but ignored potentially better translations discovered in the past iterations.
P06-1096
Such a quasi-syntactic structure can naturally capture the reordering of phrases that is not directly modeled by a conventional phrase-based approach (Koehn et al., 2003).
N03-1017
BLEU. Tillmann and Zhang (2006) avoided the problem by precomputing the oracle translations in advance.
P06-1091
An experiment has been undertaken using a small development set together with sparse features for the reranking of a k-best translation (Watanabe et al., 2006a).
P06-1098
A single oracle with 1-best translation is analytically solved without a QP-solver and is represented as the following perceptron-like update (Shimizu and Haas, 2006): α = max   0, min   C, L(ˆe, e′; et)− parenleftBig si( f t, ˆe)−si( f t, e′) parenrightBig ||h( f t, ˆe)−h( f t, e′)||2       Intuitively, the update amount is controlled by the margin and the loss between the correct and incorrect translations and by the closeness of two translations in terms of feature vectors.
P06-2098
BLEU We used the BLEU score (Papineni et al., 2002) as the loss function computed by: BLEU(E; E) = exp    1N Nsummationdisplay n=1 log pn(E, E)   ·BP(E, E) (7) where pn(·) is the n-gram precision of hypothesized translations E ={et}Tt=1 given reference translations E ={et}Tt=1 and BP(·)≤1 is a brevity penalty.
P02-1040
The algorithm is slightly different from other online training algorithms (Tillmann and Zhang, 2006; Liang et al., 2006) in that we keep and update oracle translations, which is a set of good translations reachable by a decoder according to a metric, i.e.
P06-1091 P06-1096
Tillmann and Zhang (2006), Liang et al.(2006) and Bangalore et al.(2006) introduced sparse binary features for statistical machine translation trained on a large training corpus.
P06-1091 P06-1096
The translation quality is evaluated by case-sensitive NIST (Doddington, 2002) and BLEU (Papineni et al., 2002)2.
P02-1040
Phrase-based SMT Chiang (2005) introduced the hierarchical phrasebased translation approach, in which non-terminals are embedded in each phrase.
P05-1033
Liang et al.(2006) employed an averaged perceptron algorithm.
P06-1096
In Watanabe et al.(2006a), binary features were trained only on a small development set using a variant of voted perceptron for reranking k-best translations.
P06-1098
Our method directly employs the k-best list generated by the fast decoding method (Watanabe et al., 2006b) at every iteration.
P06-1098
Second, the word alignment is refined by a grow-diag-final heuristic (Koehn et al., 2003).
N03-1017
Bigram Features Target side bigram features are also included to directly capture the fluency as in the n-gram language model (Roark et al., 2004).
P04-1007
Liang et al.(2006) introduced an averaged perceptron algorithm, but employed only 1-best translation.
P06-1096
We applied an Earley-style top-down parsing approach (Wu and Wong, 1998; Watanabe et al., 2006b; Zollmann and Venugopal, 2006).
P06-1098 P98-2230 W06-3119
The hierarchical phrase translation pairs are extracted in a standard way (Chiang, 2005): First, the bilingual data are word alignment annotated by running GIZA++ (Och and Ney, 2003) in two directions.
J03-1002 P03-1021 P05-1033
Normalized Form In Chiang (2005), each production rule is restricted to a rank-2 or binarized form in which each rule contains at most two non-terminals.
P05-1033
Based on hierarchical phrase-based modeling, we adopted the left-to-right target generation method (Watanabe et al., 2006b).
P06-1098
Tillmann and Zhang (2006) trained their feature set using an online discriminative algorithm.
P06-1091
Tillmann and Zhang (2006) used a different update style based on a convex loss function: α = ηL(ˆe, e′; et)·max parenleftBig 0, 1− parenleftBig si( f t, ˆe)−si( f t, e′) parenrightBigparenrightBig 768 Table 1: Experimental results obtained by varying normalized tokens used with surface form.
P06-1091
BLEU (Papineni et al., 2002).
P02-1040
MIRA is successfully employed in dependency parsing (McDonald et al., 2005) or the joint-labeling/chunking task (Shimizu and Haas, 2006).
P05-1012 P06-2098
Thus, we use an approximated BLEU score that basically computes BLEU for a sentence set, but accumulates the difference for a particular sentence (Watanabe et al., 2006a).
P06-1098
Online discriminative training has already been studied by Tillmann and Zhang (2006) and Liang et al.(2006). In their approach, training was performed on a large corpus using the sparse features of phrase translation pairs, target n-grams and/or bagof-word pairs inside phrases.
P06-1091 P06-1096
As a baseline SMT system, we use the hierarchical phrase-based translation with an efficient left-to-right generation (Watanabe et al., 2006b) originally proposed by Chiang (2005).
P05-1033 P06-1098
The recent advances in statistical machine translation have been achieved by discriminatively training a small number of real-valued features based either on (hierarchical) phrase-based translation (Och and Ney, 2004; Koehn et al., 2003; Chiang, 2005) or syntax-based translation (Galley et al., 2006).
J03-1002 J04-4002 N03-1017 P03-1021 P05-1033 P06-1121
The baseline hierarchical phrase-based system is trained using standard max-BLEU training (MERT) without sparse features (Och, 2003).
J03-1002 P03-1021
Tillmann and Zhang (2006) and Liang et al.(2006) solved the problem by introducing a sentence-wise BLEU.
P06-1091 P06-1096
The features are designed by considering the decoding efficiency and are based on the word alignment structure preserved in hierarchical phrase translation pairs (Zens and Ney, 2006).
W06-3108
When updating model parameters, we employ a memorizationvariant of a local updating strategy (Liang et al., 2006) in which parameters are optimized toward a set of good translations found in the k-best list across iterations.
P06-1096
The target normalized form (Watanabe et al., 2006b) further imposes a constraint whereby the target side of the aligned right-hand side is restricted to a Greibach Normal Form like structure: X → angbracketleftBig γ, ¯bβ,∼ angbracketrightBig (2) where X is a non-terminal, γ is a source side string of arbitrary terminals and/or non-terminals.
P06-1098
Infused Relaxed Algorithm The Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006) is an online version of the large-margin training algorithm for structured classification (Taskar et al., 2004) that has been successfully used for dependency parsing (McDonald et al., 2005) and joint-labeling/chunking (Shimizu and Haas, 2006).
P05-1012 P06-2098 W04-3201
Indeed, Liang et al.(2006) employed an averaged perceptron algorithm in which α value was always set to one.
P06-1096
In this method, each training sentence is decoded and weights are updated at every iteration (Liang et al., 2006).
P06-1096
Machine Translation We use a log-linear approach (Och, 2003) in which a foreign language sentence f is translated into another language, for example English, e, by seeking a maximum solution: ˆe = argmax e wT ·h( f, e) (1) where h( f, e) is a large-dimension feature vector.
J03-1002 P03-1021
Following McDonald et al.(2005), only k-best translations are used to form the margins in order to reduce the number of constraints in Eq.
P05-1012
The objective function is an approximated BLEU (Watanabe et al., 2006a) that scales the loss of a sentence BLEU to a documentwise loss.
P06-1098
• Backtrack-based penalties inspired by the distortion penalties in phrase-based modeling (Watanabe et al., 2006b).
P06-1098
