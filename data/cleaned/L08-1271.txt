<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>S Ananiadou</author>
</authors>
<title>A methodology for Automatic Term Recognition</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics (COLING94</booktitle>
<pages>1034--1038</pages>
<location>Kyoto, Japan</location>
<contexts>
<context>certain syntactic structures are filtered from the annotated text by using pattern matching techniques. Intrinsic methods try to filter TCs according to their internal (i.e morphological) structures (Ananiadou 1994). Extrinsic 1 It should be noted that other tools such as ParaConc and Déjà Vu can also be used for this task. methods, on the other hand, try to identify TCs by analysing the syntactic structure of </context>
</contexts>
<marker>Ananiadou, 1994</marker>
<rawString>Ananiadou, S. 1994. A methodology for Automatic Term Recognition. In Proceedings of the 15th International Conference on Computational Linguistics (COLING94), pp. 1034-1038. Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<date>2001</date>
<booktitle>Recent Advances in Computational Terminology</booktitle>
<editor>Bourigault, D., C. Jacquemin, and M. C L'Homme (ed</editor>
<publisher>John Benjamins Publishing Company</publisher>
<location>Amsterdam</location>
<marker>2001</marker>
<rawString>Bourigault, D., C. Jacquemin, and M. C L'Homme (ed.) 2001. Recent Advances in Computational Terminology. Amsterdam: John Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chambers</author>
</authors>
<title>Automatic Bilingual Terminology Extraction: A Practical Approach</title>
<date>2000</date>
<booktitle>In Proceedings of Translating and the Computer 22</booktitle>
<location>Aslib/IMI</location>
<marker>Chambers, 2000</marker>
<rawString>Chambers, D. 2000. Automatic Bilingual Terminology Extraction: A Practical Approach. In Proceedings of Translating and the Computer 22, Aslib/IMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Daille</author>
<author>E Gaussier</author>
<author>J-M Lange</author>
</authors>
<title>Towards Automatic Extraction of Monolingual and Bilingual Terminology</title>
<date>1994</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context> computational resources such as minority languages (cf. Streiter et al., 2003). More recently, approaches to automatic TE and TR have moved towards using both statistical and linguistic information (Daille et al., 1994; Justeson &amp; Katz, 1996; Frantzi, 1998). Generally the main part of the algorithm is the statistical part, but shallow linguistic information is incorporated in the form of a syntactic filter which on</context>
</contexts>
<marker>Daille, Gaussier, Lange, 1994</marker>
<rawString>Daille, B., E. Gaussier; J.-M. Lange. 1994. Towards Automatic Extraction of Monolingual and Bilingual Terminology. In Proceedings of COLING 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Dejean</author>
<author>E Gaussier</author>
<author>C Goutte</author>
<author>K Yamada</author>
</authors>
<date>2003</date>
<marker>Dejean, Gaussier, Goutte, Yamada, 2003</marker>
<rawString>Dejean, H., E. Gaussier, C. Goutte, and K. Yamada. 2003.</rawString>
</citation>
<citation valid="true">
<title>Reducing parameter space for word alignment</title>
<booktitle>In Proceedings of HLT-NAACL 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond</booktitle>
<location>Edmonton, Alberta</location>
<marker></marker>
<rawString>Reducing parameter space for word alignment. In Proceedings of HLT-NAACL 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond, Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K T Frantzi</author>
</authors>
<title>Automatic Recognition of Multi-Word Terms. PhD Thesis</title>
<date>1998</date>
<institution>Manchester Metropolitan University, UK</institution>
<contexts>
<context>nguages (cf. Streiter et al., 2003). More recently, approaches to automatic TE and TR have moved towards using both statistical and linguistic information (Daille et al., 1994; Justeson &amp; Katz, 1996; Frantzi, 1998). Generally the main part of the algorithm is the statistical part, but shallow linguistic information is incorporated in the form of a syntactic filter which only permits phrases having certain synt</context>
</contexts>
<marker>Frantzi, 1998</marker>
<rawString>Frantzi, K. T. 1998. Automatic Recognition of Multi-Word Terms. PhD Thesis. Manchester Metropolitan University, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gaussier</author>
</authors>
<title>Flow Network Models for Word Alignment and Terminology Extraction from Bilingual Corpora</title>
<date>1998</date>
<booktitle>In Proceedings of Thirty-Sixth Annual Meeting of the Association for Computational Linguistics and Seventeenth International Conference on Computational Linguistics</booktitle>
<pages>444--450</pages>
<location>San Francisco, California</location>
<marker>Gaussier, 1998</marker>
<rawString>Gaussier, E. 1998. Flow Network Models for Word Alignment and Terminology Extraction from Bilingual Corpora. In Proceedings of Thirty-Sixth Annual Meeting of the Association for Computational Linguistics and Seventeenth International Conference on Computational Linguistics, pp. 444--450. San Francisco, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gaussier</author>
<author>D Hull</author>
<author>S At-Mokthar</author>
</authors>
<title>Term alignment in use: Machine-aided human translation</title>
<date>2000</date>
<booktitle>Parallel text processing: Alignment and use of translation corpora</booktitle>
<pages>253--274</pages>
<editor>In J. Veronis (ed</editor>
<publisher>Kluwer Academic Publishers</publisher>
<location>Dordrecht</location>
<marker>Gaussier, Hull, At-Mokthar, 2000</marker>
<rawString>Gaussier, E., D. Hull, and S. At-Mokthar. 2000. Term alignment in use: Machine-aided human translation. In J. Veronis (ed.). Parallel text processing: Alignment and use of translation corpora, pp. 253--274. Dordrecht: Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hiemstra</author>
</authors>
<title>Deriving a bilingual lexicon for cross language information retrieval</title>
<date>1997</date>
<booktitle>In Proceedings of Gronics</booktitle>
<pages>21--26</pages>
<marker>Hiemstra, 1997</marker>
<rawString>Hiemstra, D. 1997. Deriving a bilingual lexicon for cross language information retrieval. In Proceedings of Gronics 1997, pp. 21-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hull</author>
</authors>
<title>A practical approach to terminology alignment</title>
<date>1998</date>
<booktitle>In Proceedings of CompuTerm</booktitle>
<pages>1--7</pages>
<marker>Hull, 1998</marker>
<rawString>Hull, D. 1998. A practical approach to terminology alignment. In Proceedings of CompuTerm 1998, pp. 1-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Justeson</author>
<author>S L Katz</author>
</authors>
<title>Technical Terminology: some linguistic properties and an algorithm for identification in text</title>
<date>1996</date>
<journal>Natural Language Engineering</journal>
<volume>3</volume>
<pages>259--289</pages>
<contexts>
<context>ces such as minority languages (cf. Streiter et al., 2003). More recently, approaches to automatic TE and TR have moved towards using both statistical and linguistic information (Daille et al., 1994; Justeson &amp; Katz, 1996; Frantzi, 1998). Generally the main part of the algorithm is the statistical part, but shallow linguistic information is incorporated in the form of a syntactic filter which only permits phrases havi</context>
</contexts>
<marker>Justeson, Katz, 1996</marker>
<rawString>Justeson, J. S., and S. L. Katz. 1996. Technical Terminology: some linguistic properties and an algorithm for identification in text. Natural Language Engineering 3(2): 259-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schütze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing</title>
<date>1999</date>
<publisher>MIT Press</publisher>
<contexts>
<context> use of certain lexical units or morpho-syntactic constructions. TETs based on statistics try to filter out words and phrases having a certain frequency-based statistic higher than a given threshold (Manning &amp; Schütze 1999). Another common method is to compare the frequency of words and phrases in a specialised text to their frequency in general language texts assuming that terms tend to appear more often in specialise</context>
<context>as good as frequency. As a result, in this paper we use term frequency as the statistical score. 3.2 Term candidate alignment To align term candidates, we use a contingency table, and log-likelihood (Manning &amp; Schütze 1999) to measure how likely a pair of English and Spanish term candidates is to be a correct pair. The contingency table is built using a parallel corpus manually aligned at sentence level (see Section 4.</context>
</contexts>
<marker>Manning, Schütze, 1999</marker>
<rawString>Manning, C. D., and H. Schütze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Models of translational equivalence among words</title>
<date>2000</date>
<journal>Computational Linguistics</journal>
<volume>26</volume>
<pages>221--249</pages>
<marker>Melamed, 2000</marker>
<rawString>Melamed, I. D. 2000. Models of translational equivalence among words. Computational Linguistics 26(2): 221-249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pearson</author>
</authors>
<title>Terms in context</title>
<date>1999</date>
<location>Amsterdam: John Benjamins</location>
<marker>Pearson, 1999</marker>
<rawString>Pearson, J. 1999. Terms in context. Amsterdam: John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V A Sauron</author>
</authors>
<title>Tearing out the terms: evaluating terms extractors</title>
<date>2002</date>
<booktitle>In Proceedings of Translating and the Computer 24</booktitle>
<location>London, Britain</location>
<contexts>
<context>in stages in terminology work can be summarised as: extraction of term candidates from a corpus, validation of the term candidates found, and organisation of validated terms by domain and sub-domain (Sauron, 2002). In this respect, a number of projects have been able to create automatic extraction tools, which identify term candidates from a corpus in electronic form. Some projects go one step further: on the</context>
</contexts>
<marker>Sauron, 2002</marker>
<rawString>Sauron, V. A. 2002. Tearing out the terms: evaluating terms extractors. In Proceedings of Translating and the Computer 24, London, Britain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Streiter</author>
<author>D Zielinski</author>
<author>I Ties</author>
<author>L Voltmer</author>
</authors>
<title>Term extraction for Ladin: An example-based approach</title>
<date>2003</date>
<contexts>
<context> as linguistic, statistical, or hybrid. Linguistic and statistical approaches can be further subdivided into term-based (intrinsic) and context-based (extrinsic) methods (cf. Bourigault et al., 2001; Streiter et al., 2003). Terminology Extraction tools (TETs) following a linguistic approach try to identify terms by their linguistic (morphological and syntactic) structure. For this purpose, texts are annotated with lin</context>
<context>ependent and thus only available for major languages. Statistical TETs, on the other hand, can also be used for lesser-used languages that lack computational resources such as minority languages (cf. Streiter et al., 2003). More recently, approaches to automatic TE and TR have moved towards using both statistical and linguistic information (Daille et al., 1994; Justeson &amp; Katz, 1996; Frantzi, 1998). Generally the main</context>
</contexts>
<marker>Streiter, Zielinski, Ties, Voltmer, 2003</marker>
<rawString>Streiter, O., D. Zielinski, I. Ties, and L. Voltmer. 2003. Term extraction for Ladin: An example-based approach.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of TANL 2003 Workshop on Natural Language Processing of Minority Languages with few computational linguistic resources, Batz-sur la Mer</booktitle>
<marker></marker>
<rawString>In Proceedings of TANL 2003 Workshop on Natural Language Processing of Minority Languages with few computational linguistic resources, Batz-sur la Mer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tsuji</author>
<author>K Kageura</author>
</authors>
<title>Extracting low-frequency translation pairs from Japanese-English bilingual corpora</title>
<date>2004</date>
<booktitle>In Proceedings of CompuTerm</booktitle>
<pages>23--30</pages>
<marker>Tsuji, Kageura, 2004</marker>
<rawString>Tsuji, K., and K. Kageura. 2004. Extracting low-frequency translation pairs from Japanese-English bilingual corpora. In Proceedings of CompuTerm 2004, pp. 23-30.</rawString>
</citation>
</citationList>
</algorithm>

