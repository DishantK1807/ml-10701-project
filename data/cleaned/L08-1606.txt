<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>L Bentivogli</author>
<author>P Forner</author>
<author>B Magnini</author>
<author>E Pianta</author>
</authors>
<date>2004</date>
<contexts>
<context>the client of the web service to search for both literals and synsets. As the example above shows, the Romanian WordNet contains not only the Princeton WordNet specific data but also the IRST DOMAIN (Bentivogli et al, 2004), SUMO (Niles &amp; Pease, 2001) and SentiWordnet (Esuli &amp; Sebastiani, 2006) annotations. Currently these annotations can be visualized on the web browser available at http://nlp.racai.ro/wnbrowser/. A c</context>
</contexts>
<marker>Bentivogli, Forner, Magnini, Pianta, 2004</marker>
<rawString>Bentivogli, L., Forner, P., Magnini, B., Pianta, E. (2004).</rawString>
</citation>
<citation valid="true">
<title>Revising WordNet Domains Hierarchy: Semantics, Coverage, and Balancing</title>
<booktitle>In Proceedings of COLING 2004 Workshop on &amp;quot;Multilingual Linguistic Resources</booktitle>
<pages>101--108</pages>
<location>Geneva, Switzerland</location>
<marker></marker>
<rawString>Revising WordNet Domains Hierarchy: Semantics, Coverage, and Balancing. In Proceedings of COLING 2004 Workshop on &amp;quot;Multilingual Linguistic Resources&amp;quot;. Geneva, Switzerland, pp. 101--108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Berners-Lee</author>
</authors>
<title>Web Services and Semantic Web: Integrating Applications. Talk at the Twelfth International World Wide Web Conference</title>
<date>2003</date>
<location>Budapest, Hungary</location>
<contexts>
<context>est and use loop is provided by the Semantic Web idea of web services. According to Tim Berners-Lee, a web service provides for “program integration across application and organizational boundaries” (Berners-Lee, 2003). The integration is achieved via a standardized RPC call interface implemented with SOAP which is essentially built over XML. Apart from that, the software API of the web services can be formally de</context>
</contexts>
<marker>Berners-Lee, 2003</marker>
<rawString>Berners-Lee, T. (2003). Web Services and Semantic Web: Integrating Applications. Talk at the Twelfth International World Wide Web Conference. Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
</authors>
<title>TnT – A Statistical Part-Of-Speech Tagger</title>
<date>2000</date>
<booktitle>In Proceedings of the 6th Applied NLP Conference ANLP-2000</booktitle>
<pages>224--231</pages>
<location>Seattle, WA</location>
<contexts>
<context>rts of an NE string (i.e. a period may be a part of an abbreviation). POS tagging is achieved through the HMM tagging technology. The POS tagger of TTL follows the description of HMM tagger given in (Brants, 2000) but it extends it in several ways allowing for tiered tagging, for a more accurate processing of unknown words and also for tagging of named entities (which are practically labeled by the NER module</context>
</contexts>
<marker>Brants, 2000</marker>
<rawString>Brants, T. (2000). TnT – A Statistical Part-Of-Speech Tagger. In Proceedings of the 6th Applied NLP Conference ANLP-2000. Seattle, WA, pp 224--231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>SentiWordNet: A publicly Available Lexical Resourced for Opinion Mining</title>
<date>2006</date>
<booktitle>In Proceedings of LREC2006</booktitle>
<pages>417--422</pages>
<location>Genoa, Italy</location>
<contexts>
<context> As the example above shows, the Romanian WordNet contains not only the Princeton WordNet specific data but also the IRST DOMAIN (Bentivogli et al, 2004), SUMO (Niles &amp; Pease, 2001) and SentiWordnet (Esuli &amp; Sebastiani, 2006) annotations. Currently these annotations can be visualized on the web browser available at http://nlp.racai.ro/wnbrowser/. A common usage scenario for the current wordnet web service is to translate</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Esuli, A., Sebastiani, F. (2006). SentiWordNet: A publicly Available Lexical Resourced for Opinion Mining, In Proceedings of LREC2006. Genoa, Italy, pp. 417--422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ion</author>
<author>Barbu Mititelu</author>
<author>V</author>
</authors>
<title>Constrained Lexical Attraction Models</title>
<date>2006</date>
<booktitle>In Proceedings of the Nineteenth International Florida Artificial Intelligence Research Society Conference. Menlo Park, Calif</booktitle>
<pages>297--302</pages>
<publisher>AAAI Press</publisher>
<marker>Ion, Mititelu, V, 2006</marker>
<rawString>Ion, R., Barbu Mititelu, V. (2006). Constrained Lexical Attraction Models. In Proceedings of the Nineteenth International Florida Artificial Intelligence Research Society Conference. Menlo Park, Calif.: AAAI Press, pp. 297--302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ion</author>
</authors>
<title>Word Sense Disambiguation Methods Applied to English and Romanian. PhD thesis (in Romanian). Romanian Academy</title>
<date>2007</date>
<location>Bucharest</location>
<contexts>
<context>entification, diacritics insertion (for Romanian) and Romanian Wikipedia indexing and searching. 2. General Architecture The POS tagging, lemmatization and chunking operations are implemented by TTL (Ion, 2007), a Perl module which was intended as an API for NLP applications requiring these operations. Word linking is achieved through LexPar (Ion 2006; Ion 2007), a lexical attraction model linker with synt</context>
<context>at follows, we will describe each of the web services concentrating on their programming interfaces. For the details of their algorithms, the reader may consult the references. 3. TTL and LexPar TTL (Ion, 2007) is a text preprocessing module developed in Perl. Its functions are: Named Entity Recognition (by means of regular expressions defined over sequences of characters), sentence splitting, tokenization</context>
<context>emma). In the case of out-of-lexicon word forms the lemmatization is performed by a statistical module which automatically learns normalization rules from the existing lexical stock (for details see (Ion, 2007)). Finally, chunking is implemented with regular expressions over sequences of POS tags. It is not recursive and it does not perform attachments (PPs to NPs for instance). The TTL web service offers </context>
<context>&amp;quot; chunk=&amp;quot;Vp#1&amp;quot;&gt; is&lt;/w&gt; &lt;w lemma=&amp;quot;a&amp;quot; ana=&amp;quot;Ti-s&amp;quot; chunk=&amp;quot;Np#1&amp;quot;&gt; a&lt;/w&gt; &lt;w lemma=&amp;quot;simple&amp;quot;ana=&amp;quot;Afp&amp;quot;chunk=&amp;quot;Np#1,Ap#1&amp;quot;&gt; simple&lt;/w&gt; &lt;w lemma=&amp;quot;example&amp;quot; ana=&amp;quot;Ncns&amp;quot;chunk=&amp;quot;Np#1&amp;quot;&gt; example&lt;/w&gt; ... Lexpar (Ion 2006; Ion 2007) is a word linker. A link between two syntactico-semantic related words in a 7 http://www.cs.vassar.edu/XCES/ sentence is an approximation of a dependency relation as described in (Mel’čuk, 1988) wit</context>
</contexts>
<marker>Ion, 2007</marker>
<rawString>Ion, R. (2007). Word Sense Disambiguation Methods Applied to English and Romanian. PhD thesis (in Romanian). Romanian Academy, Bucharest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Mel’čuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice</title>
<date>1988</date>
<publisher>Press</publisher>
<institution>State University of New York</institution>
<location>Albany, NY</location>
<marker>Mel’čuk, 1988</marker>
<rawString>Mel’čuk, I.A. (1988). Dependency Syntax: Theory and Practice. Albany, NY: State University of New York Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Niles</author>
<author>A Pease</author>
</authors>
<title>Towards a Standard Upper Ontology</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd International Conference on Formal Ontology in Information Systems (FOIS-2001). Ogunquit</booktitle>
<pages>2--9</pages>
<location>Maine</location>
<contexts>
<context>o search for both literals and synsets. As the example above shows, the Romanian WordNet contains not only the Princeton WordNet specific data but also the IRST DOMAIN (Bentivogli et al, 2004), SUMO (Niles &amp; Pease, 2001) and SentiWordnet (Esuli &amp; Sebastiani, 2006) annotations. Currently these annotations can be visualized on the web browser available at http://nlp.racai.ro/wnbrowser/. A common usage scenario for the</context>
</contexts>
<marker>Niles, Pease, 2001</marker>
<rawString>Niles I., Pease, A. (2001) Towards a Standard Upper Ontology. In Proceedings of the 2nd International Conference on Formal Ontology in Information Systems (FOIS-2001). Ogunquit, Maine, pp. 2--9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Steinberger</author>
<author>B Pouliquen</author>
<author>A Widiger</author>
<author>C Ignat</author>
<author>T Erjavec</author>
<author>D Tufiş</author>
<author>D Varga</author>
</authors>
<title>The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC'2006</booktitle>
<pages>2142--2147</pages>
<location>Genoa, Italy</location>
<contexts>
<context>ces in a given, presumably, monolingual corpus where indeed written in the respective language. This is, for instance, how we clean-up the Romanian part of the 22-language parallel corpus JRC-Acquis (Steinberger et al., 2006). Currently, the web service distinguishes among the 22 languages of the European Union., present in the JRC-Acquis parallel corpus. The function takes as its sole parameter a string consisting of a </context>
</contexts>
<marker>Steinberger, Pouliquen, Widiger, Ignat, Erjavec, Tufiş, Varga, 2006</marker>
<rawString>Steinberger, R., Pouliquen, B., Widiger, A., Ignat, C., Erjavec, T., Tufiş, D., Varga D. (2006). The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC'2006). Genoa, Italy, pp.2142--2147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tufiş</author>
</authors>
<title>Tiered Tagging and Combined Classifiers. In</title>
<date>1999</date>
<booktitle>Nth (Eds.), Text, Speech and Dialogue, Lecture Notes in Artificial Intelligence</booktitle>
<pages>28--33</pages>
<publisher>Springer</publisher>
<contexts>
<context>hich are practically labeled by the NER module before actual POS tagging). The TTL’s tag-set is the MSD 6 with its smaller superset CTAG. (TTL tagging methodology follows the tiered tagging approach (Tufiş, 1999) where MSDs are recovered from an initial CTAG annotation). Lemmatization is achieved after POS tagging by lexicon lookup (in general, a word form and its POS tag uniquely identify the lemma). In the</context>
</contexts>
<marker>Tufiş, 1999</marker>
<rawString>Tufiş, D. (1999). Tiered Tagging and Combined Classifiers. In F. Jelinek, E. Nth (Eds.), Text, Speech and Dialogue, Lecture Notes in Artificial Intelligence. Springer, pp. 28--33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Tufiş</author>
<author>A Ceauşu</author>
</authors>
<title>DIAC+: A Professional Diacritics Recovering System. In this volume</title>
<date>2008</date>
<contexts>
<context>xPar (Ion 2006; Ion 2007), a lexical attraction model linker with syntactic filtering of possible links also written in Perl. WordNet lookup, language identification and diacritics insertion (DIAC+, (Tufiş &amp; Ceauşu, 2008)) are implemented in C# and were developed as standalone applications. The indexing and search engines were developed for the CLEF series 3 of QA evaluation exercises. All applications were adapted a</context>
<context>However, this rarely happens because in the vast majority of cases the rejected links are indeed incorrect. 4. DIAC + The main task of the DIAC + web service is diacritics recovery in Romanian texts (Tufiş &amp; Ceauşu, 2008). For Romanian, automatic restoration of the diacritics is a real challenge, both because of their frequency (every third word might contain at least one diacritical character) and due to their signi</context>
<context>y change the meaning of the sentence. There are also other approaches, most of them based on a character language model, but they cannot provide the same degree of precision (for further details see (Tufiş &amp; Ceauşu, 2008) in this volume). The web service description is available at http://nlp.racai.ro/WebServices/TextProcessingWebServi ce.asmx?WSDL. The web service exposes only one function process that takes three</context>
</contexts>
<marker>Tufiş, Ceauşu, 2008</marker>
<rawString>Tufiş, D., Ceauşu, A. (2008). DIAC+: A Professional Diacritics Recovering System. In this volume.</rawString>
</citation>
</citationList>
</algorithm>

