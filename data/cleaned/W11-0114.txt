Concrete Sentence Spaces for Compositional Distributional
Models of Meaning
Edward Grefenstettenull, Mehrnoosh Sadrzadehnull, Stephen Clarknull, Bob Coeckenull, Stephen Pulmannull
nullOxford University Computing Laboratory, nullUniversity of Cambridge Computer Laboratory
firstname.lastname@comlab.ox.ac.uk, stephen.clark@cl.cam.ac.uk
Abstract
Coecke, Sadrzadeh, and Clark [3] developed a compositional model of meaning for distributional
semantics, in which each word in a sentence has a meaning vector and the distributional meaning of the
sentence is a function of the tensor products of the word vectors. Abstractly speaking, this function is the
morphismcorrespondingtothegrammaticalstructureofthesentenceinthecategoryoffinitedimensional
vector spaces. In this paper, we provide a concrete method for implementing this linear meaning map,
by constructing a corpus-based vector space for the type of sentence. Our construction method is based
on structured vector spaces whereby meaning vectors of all sentences, regardless of their grammatical
structure, live in the same vector space. Our proposed sentence space is the tensor product of two noun
spaces, in which the basis vectors are pairs of words each augmented with a grammatical role. This
enables us to compare meanings of sentences by simply taking the inner product of their vectors.
1 Background
Coecke, Sadrzadeh, and Clark [3] develop a mathematical framework for a compositional distributional
model of meaning, based on the intuition that syntactic analysis guides the semantic vector composition.
The setting consists of two parts: a formalism for a type-logical syntax and a formalism for vector space
semantics. Each word is assigned a grammatical type and a meaning vector in the space corresponding to
its type. The meaning of a sentence is obtained by applying the function corresponding to the grammatical
structure of the sentence to the tensor product of the meanings of the words in the sentence. Based on the
type-logic used, some words will have atomic types and some compound function types. The compound
types live in a tensor space where the vectors are weighted sums (i.e. superpositions) of the pairs of bases
from each space. Compound types are “applied” to their arguments by taking inner products, in a similar
manner to how predicates are applied to their arguments in Montague semantics.
For the type-logic we use Lambek’s Pregroup grammars [7]. The use of pregoups is not essential, but
leads to a more elegant formalism, given its proximity to the categorical structure of vector spaces (see [3]).
A Pregroup is a partially ordered monoid where each element has a right and left cancelling element, referred
to as an adjoint. It can be seen as the algebraic counterpart of the cancellation calculus of Harris [6]. The
operational difference between a Pregroup and Lambek’s Syntactic Calculus is that, in the latter, the monoid
multiplication of the algebra (used to model juxtaposition of the types of the words) has a right and a left
adjoint, whereas in the pregroup it is the elements themselves which have adjoints. The adjoint types are
used to denote functions, e.g. that of a transitive verb with a subject and object as input and a sentence as
output. In the Pregroup setting, these function types are still denoted by adjoints, but this time the adjoints
of the elements themselves.
As an example, consider the sentence “dogs chase cats”. We assign the type null (for noun phrase) to “dog”
and “cat”, and nullrnullnulll to “chase”, where nullr and nulll are the right and left adjoints of null and null is the type of a
125
(declarative) sentence. The type nullrnullnulll expresses the fact that the verb is a predicate that takes two arguments
of type null as input, on its right and left, and outputs the type null of a sentence. The parsing of the sentence is
the following reduction:
null(nullrnullnulll)null null 1null1 = null
This parse is based on the cancellation of null and nullr, and also nulll and null; i.e. nullnullr null 1 and nulllnull null 1 for 1
the unit of juxtaposition. The reduction expresses the fact that the juxtapositions of the types of the words
reduce to the type of a sentence.
On the semantic side, we assign the vector space null to the type null, and the tensor space null nullnullnullnull to the
type nullrnullnulll. Very briefly, and in order to introduce some notation, recall that the tensor space nullnullnull has as a
basis the cartesian product of a basis of null with a basis of null. Recall also that any vector can be expressed as
a weighted sum of basis vectors; e.g. if (nullnull1nullnullnullnullnullnullnulln) is a basis of null then any vector nullnull null null can be written as
nullnull =null
i nulli
nullnulli where each nulli null Ris a weighting factor. Now for (nullnull1nullnullnullnullnullnullnulln) a basis of null and (nullnullnull
1nullnullnullnullnull
nullnullnull
n)
a basis of null, a vector nullnull in the tensor space null null null can be expressed as follows:
null
ij
nullij (nullnulli nullnullnullnullj)
where the tensor of basis vectors nullnulli null nullnullnullj stands for their pair (nullnullinullnullnullnullj). In general nullnull is not separable into
the tensor of two vectors, except for the case when nullnull is not entangled. For non-entangled vectors we can
write nullnull = nullnull nullnullnull for nullnull =nulli nullinullnulli and nullnull =nullj nullnulljnullnullnullj; hence the weighting factor of nullnull can be obtained
by simply multiplying the weights of its tensored counterparts, i.e. nullij = nulli null nullnullj. In the entangled case
these weights cannot be determined as such and range over all the possibilities. We take advantage of this
fact to encode meanings of verbs, and in general all words that have compound types and are interpreted as
predicates, relations, or functions. For a brief discussion see the last paragraph of this section. Finally, we
use the Dirac notation to denote the dot or inner product of two vectorsnullnullnull null nullnull null null Rdefined bynulli nullinullnullnulli.
Returning to our example, for the meanings of nouns we have nullnullnulldogsnullnullnullcats null null, and for the meanings of
verbs we have nullnullnullnullchase null null null null null null, i.e. the following superposition:
null
ijk
nullijk (nullnulli nullnullnullj nullnullnullk)
Here nullnulli and nullnullk are basis vectors of null and nullnullj is a basis vector of null. From the categorical translation method
presented in [3] and the grammatical reduction null(nullrnullnulll)null null null, we obtain the following linear map as the
categorical morphism corresponding to the reduction:
nullN null 1s null nullN : null null (null null null null null) null null null null
Using this map, the meaning of the sentence is computed as follows:
nullnullnullnullnullnullnullnullnullnullnulldogs chase cats = (null
N null 1s null nullN)
nullnullnullnull
dogs nullnullnullnullnullchase nullnullnullcats
null
= (nullN null 1s null nullN)
null
nullnullnullnulldogs null
null
nullnull
ijk
nullijk(nullnulli nullnullnullj nullnullnullk)
null
nullnullnullnullcats
null
null
=
null
ijk
nullijknullnullnullnulldogs null nullnullinullnullnulljnullnullnullk null nullnullcatsnull
The key features of this operation are, first, that the inner-products reduce dimensionality by ‘consuming’
tensored vectors and by virtue of the following component function:
nullN : null null null null R :: nullnull nullnullnull null nullnullnull null nullnull null
126
Thus the tensored word vectors nullnullnulldogs null nullnullnullnullchase null nullnullcats are mapped into a sentence space null which is common
to all sentences regardless of their grammatical structure or complexity. Second, note that the tensor productnullnullnull
dogsnullnullnullnullnullchasenullnullnullcats does not need to be calculated, since all that is required for computation of the sentence
vector are the noun vectors and the nullijk weights for the verb. Note also that the inner product operations
are simply picking out basis vectors in the noun space, an operation that can be performed in constant
time. Hence this formalism avoids two problems faced by approaches in the vein of [9, 2], which use
the tensor product as a composition operation: first, that the sentence meaning space is high dimensional
and grammatically different sentences have representations with different dimensionalities, preventing them
from being compared directly using inner products; and second, that the space complexity of the tensored
representation grows exponentially with the length and grammatical complexity of the sentence. In constrast,
the model we propose does not require the tensored vectors being combined to be represented explicitly.
Note that we have taken the vector of the transitive verb, e.g. nullnullnullnullchase, to be an entangled vector in the
tensor space null nullnull nullnull. But why can this not be a separable vector, in which case the meaning of the verb
would be as follows:
nullnullnullnullchase = null
i
nullinullnulli null
null
j
nullnulljnullnullj null
null
k
nullnullnullknullnullk
The meaning of the sentence would then become null1null2nullj nullnulljnullnullj for null1 = nulli nullinullnullnullnulldogs null nullnullinull and null2 =null
k null
nullnull
knull
nullnullcats null nullnull
knull. The problem is that this meaning only depends on the meaning of the verb and is
independent of the meanings of the subject and object, whereas the meaning from the entangled case,
i.e. null1null2nullijk nullijknullnullj, depends on the meanings of subject and object as well as the verb.
2 From
Truth-Theoretic to Corpus-based Meaning
The model presented above is compositional and distributional, but still abstract. To make it concrete, null and
null have to be constructed by providing a method for determining the nullijk weightings. Coecke, Sadrzadeh,
and Clark [3] show how a truth-theoretic meaning can be derived in the compositional framework. For
example, assume that null is spanned by all animals and null is the two-dimensional space spanned by nullnulltrue andnullnullnull
false. We use the weighting factor to define a model-theoretic meaning for the verb as follows:
nullijknullnullj =
nullnullnull
true nullnullnullnullnull(nullnullinullnullnullk) = truenullnullnullnull
false nullnullnullnull
The definition of our meaning map ensures that this value propagates to the meaning of the whole sentence.
So nullnullnullnullnull(nullnullnullnullnullnullnullnullnullnullnullnullnullnullnull) becomes true whenever “dogs chase cats” is true and false otherwise. This is exactly
how meaning is computed in the model-theoretic view on semantics. One way to generalise this truth-
theoretic meaning is to assume that nullnullnullnullnull(nullnullinullnullnullk) has degrees of truth, for instance by defining nullnullnullnullnull as a
combination of nullnullnull and nullnullnullnullnull, such as:
nullnullnullnullnull = 23nullnullnull + 13nullnullnullnullnull
Again, the meaning map ensures that these degrees propagate to the meaning of the whole sentence. For a
worked out example see [3]. But neither of these examples provide a distributional sentence meaning.
Here we take a first step towards a corpus-based distributional model, by attempting to recover a meaning
for a sentence based on the meanings of the words derived from a corpus. But crucially this meaning goes
beyond just composing the meanings of words using a vector operator, such as tensor product, summation
or multiplication [8]. Our computation of sentence meaning treats some vectors as functions and others as
127
function arguments, according to how the words in the sentence are typed, and uses the syntactic structure
as a guide to determine how the functions are applied to their arguments. The intuition behind this approach
is that syntactic analysis guides semantic vector composition.
The contribution of this paper is to introduce some concrete constructions for a compositional distri-
butional model of meaning. These constructions demonstrate how the mathematical model of [3] can be
implemented in a concrete setting which introduces a richer, not necessarily truth-theoretic, notion of natural
language semantics which is closer to the ideas underlying standard distributional models of word meaning.
We leave full evaluation to future work, in order to determine whether the following method in conjunction
with word vectors built from large corpora leads to improved results on language processing tasks, such as
computing sentence similarity and paraphrase evaluation.
Nouns and Transitive Verbs. We take null to be a structured vector space, as in [4, 5]. The bases of null are
annotated by ‘properties’ obtained by combining dependency relations with nouns, verbs and adjectives. For
example, basis vectors might be associated with properties such as “arg-fluffy”, denoting the argument of
the adjective fluffy, “subj-chase” denoting the subject of the verb chase, “obj-buy” denoting the object of the
verb buy, and so on. We construct the vector for a noun by counting how many times in the corpus a word
has been the argument of ‘fluffy’, the subject of ‘chase’, the object of ‘buy’, and so on.
The framework in [3] offers no guidance as to what the sentence space should consist of. Here we take
the sentence space null to be null null null, so its bases are of the form nullnullj = (nullnullinullnullnullk). The intuition is that, for a
transitive verb, the meaning of a sentence is determined by the meaning of the verb together with its subject
and object.1 The verb vectors nullijk(nullnullinullnullnullk) are built by counting how many times a word that is nulli (e.g. has
the property of being fluffy) has been subject of the verb and a word that is nullk (e.g. has the property that it’s
bought) has been its object, where the counts are moderated by the extent to which the subject and object
exemplify each property (e.g. how fluffy the subject is). To give a rough paraphrase of the intuition behind
this approach, the meaning of “dog chases cat” is given by: the extent to which a dog is fluffy and a cat is
something that is bought (for the null null null property pair “arg-fluffy” and “obj-buy”), and the extent to which
fluffy things chase things that are bought (accounting for the meaning of the verb for this particular property
pair); plus the extent to which a dog is something that runs and a cat is something that is cute (for the null nullnull
pair “subj-run” and “arg-cute”), and the extent to which things that run chase things that are cute (accounting
for the meaning of the verb for this particular property pair); and so on for all noun property pairs.
Adjective Phrases. Adjectives are dealt with in a similar way. We give them the syntactic type nullnulll and
build their vectors in null null null. The syntactic reduction nullnulllnull null null associated with applying an adjective to a
noun gives us the map 1N null nullN by which we semantically compose an adjective with a noun, as follows:
nullnullnullnullnullred fox = (1
N null nullN)(
nullnullred nullnullnullfox) =null
ij
nullijnullnullinullnullnullj null nullnullfoxnull
We can view the nullij counts as determining what sorts of properties the arguments of a particular adjective
typically have (e.g. arg-red, arg-colourful for the adjective “red”).
Prepositional Phrases. We assign the type nullrnull to the whole prepositional phrase (when it modifies a noun),
for example to “in the forest” in the sentence “dogs chase cats in the forest”. The pregroup parsing is as
follows:
null(nullrnullnulll)null(nullrnull) null 1nullnulll1null null nullnulllnull null null1 = null
The vector space corresponding to the prepositional phrase will thus be the tensor space null null null and the
categorificationofthe parsewillbethecomposition oftwomorphisms: (1SnullnulllN)null(nullrNnull1Snull1NnullnullrNnull1N).
1Intransitive and ditransitive verbs are interpreted in an analagous fashion; see §4.
128

