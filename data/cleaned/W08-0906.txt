Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 44–52,
Columbus, Ohio, USA, June 2008. c©2008 Association for Computational Linguistics
AnsweringLearners’Questionsby RetrievingQuestionParaphrases
fromSocialQ&ASites
DelphineBernhard and IrynaGurevych
UbiquitousKnowledgeProcessingLab
ComputerScienceDepartment
TechnischeUniversit¨at Darmstadt,Hochschulstraße10
D-64289Darmstadt,Germany
{delphine|gurevych}@tk.informatik.tu-darmstadt.de
Abstract
Information overload is a well-known prob-
lem which can be particularly detrimental to
learners. In this paper, we propose a method
to support learners in the information seek-
ing process which consists in answering their
questions by retrieving question paraphrases
and their corresponding answers from social
Q&A sites. Given the novelty of this kind of
data, it is crucial to get a better understand-
ing of how questions in social Q&A sites can
be automatically analysed and retrieved. We
discuss and evaluate several pre-processing
strategies and question similarity metrics, us-
ing a new question paraphrase corpus col-
lected from the WikiAnswersQ&A site. The
results show that viable performancelevels of
more than 80% accuracy can be obtained for
the task of questionparaphraseretrieval.
1 Introduction
Question asking is an important component of effi-
cient learning. However, instructors are often over-
whelmedwith students’questions and are therefore
unabletoprovidetimelyanswers(Fengetal.,2006).
Informationseekingis also rendereddifficult by the
sheer amount of learning material available, espe-
cially online. The use of advanced information re-
trieval and natural language processing techniques
to answer learners’ questions and reduce the diffi-
culty of information seeking is henceforth particu-
larly promising. QuestionAnswering(QA) systems
seem well suited for this task since they aim at gen-
erating precise answers to natural language ques-
tions instead of merely returning documents con-
taining answers. However, QA systems have to be
adapted to meet learners’ needs. Indeed, learners
do not merely ask concreteor factoid questions, but
rather open-ended, explanatory or methodological
questionswhichcannotbeansweredbyasinglesen-
tence (Baram-Tsabari et al., 2006). Despitea recent
trendtorenderthetasksmorecomplex atlargescale
QA evaluation campaigns such as TREC or CLEF,
current QA systems are still ill-suited to meet these
requirements.
A first alternative to full-fledged QA consists in
makinguseofalreadyavailablequestionandanswer
pairs extracted from archived discussions. For in-
stance,Fenget al. (2006)describean intelligentdis-
cussion bot for answering student questions in fo-
rums which relies on answers retrieved from an an-
notated corpus of discussions. This renders the task
of QA easiersince answersdo not have to be gener-
ated from heterogeneousdocuments by the system.
Thescopeofsucha discussionbotishoweverinher-
ently limited since it relies on manually annotated
data, taken from forumswithina specificdomain.
We proposea different solutionwhich consistsin
tapping into the wisdom of crowds to answer learn-
ers’ questions. This approach provides the com-
pelling advantage that it utilises the wealth of al-
ready answered questions available in online social
Q&A sites. The task of Question Answering can
then be boileddown to the problemof findingques-
tion paraphrases in a database of answered ques-
tions. Question paraphrases are questions which
haveidenticalmeaningsandexpectthesameanswer
while presenting alternate wordings. Several meth-
ods have alreadybeen proposedto identifyquestion
44
paraphrases mostly in FAQs (Tomuro and Lytinen,
2004)or searchenginelogs (Zhaoet al., 2007).
Inthispaper, wefocusontheproblemofquestion
paraphraseidentificationin socialQ&Asites within
arealisticinformationseekingscenario: givenauser
question,wewanttoretrievethebestmatchingques-
tion paraphrase from a database of previously an-
sweredquestionsin orderto displaythecorrespond-
ing answer. The use of social Q&A sites for ed-
ucational applications brings about new challenges
linkedtothevariablequalityofsocialmediacontent.
As opposed to questionsin FAQs, whichare subject
to editorial control, questions in social Q&A sites
are often ill-formed or contain spelling errors. It is
thereforecrucialtogetabetterunderstandingofhow
they can be automaticallyanalysedand retrieved. In
thiswork,wefocusonseveralpre-processingstrate-
gies and questionsimilaritymeasuresappliedto the
task of identifying question paraphrases in a social
Q&A site. We chose WikiAnswerswhich has been
ranked by comScoreas the first fastest growing do-
main of the top 1,500in the U.S. in 2007.
The remainder of the paper is organised as fol-
lows. Section 2 first discusses related work on
paraphraseidentificationandquestionparaphrasing.
Section 3 then presents question and answer repos-
itories with special emphasis on social Q&A sites.
Ourmethodstoidentifyquestionparaphrasesarede-
tailed in section 4. Finally, we present and analyse
the experimental results obtained in section 5 and
concludein section6.
2 RelatedWork
The identification of question paraphrases in ques-
tion and answer repositories is related to research
focusingon sentenceparaphraseidentification(sec-
tion 2.1) and query paraphrasing(section 2.2). The
specific features of question paraphrasinghave also
alreadybeen investigated (section2.3).
2.1 SentenceParaphraseIdentification
Paraphrasesare alternative ways to convey the same
information (Barzilay and McKeown, 2001). Para-
phrases can be found at different levels of lin-
guistic structure: words, phrases and whole sen-
tences. While word and phrasal paraphrases can
be assimilated to the well-studied notion of syn-
onymy, sentencelevel paraphrasingis moredifficult
to grasp and cannot be equated with word-for-word
or phrase-by-phrase substitution since it might en-
tail changes in the structure of the sentence (Barzi-
lay and Lee, 2003). In practice, sentence para-
phrases are identified using various string and se-
mantic similarity measures which aim at captur-
ing the semanticequivalence of the sentencesbeing
compared. String similarity metrics, when applied
to sentences, consist in comparing the words con-
tained in the sentences. There exist many different
string similarity measures: word overlap (Tomuro
and Lytinen, 2004), longest common subsequence
(Islamand Inkpen,2007), Levenshteinedit distance
(Dolan et al., 2004), word n-gramoverlap (Barzilay
and Lee, 2003) etc. Semantic similarity measures
are obtained by first computing the semantic simi-
larity of the words containedin the sentencesbeing
compared. Mihalcea et al. (2006) use both corpus-
basedand knowledge-basedmeasuresof the seman-
tic similarity between words. Both string similarity
and semantic similarity might be combined: for in-
stance, Islam and Inkpen (2007) combine semantic
similarity with longest common subsequencestring
similarity, whileLi et al.(2006)make additionaluse
of word ordersimilarity.
2.2 QueryParaphrasing
In Information Retrieval, research on paraphrasing
is dedicatedto queryparaphrasingwhichconsistsin
identifying semantically similar queries. The over-
all objective is to discover frequently asked ques-
tions and popular topics (Wen et al., 2002) or sug-
gest related queries to users (Sahami and Heilman,
2006). Traditionalstring similaritymetrics are usu-
ally deemed inefficient for such short text snip-
petsandalternative similaritymetricshave therefore
been proposed. For instance, Wen et al. (2002) rely
onuserclicklogs,basedontheideathatqueriesand
questions which result in identical document clicks
are boundto be similar.
2.3 QuestionParaphrasing
Following previous research in this domain, we de-
fine question paraphrases as questions which have
all the following properties: (a) they have identi-
cal meanings, (b) they have the same answers, and
(c) they present alternate wordings. Question para-
45
phrases differ from sentence paraphrasesby the ad-
ditional condition (b). This definition encompasses
the following questions, taken from the WikiAn-
swers web site: How many ounces are there in a
pound?, What’s the number of ounces per pound?,
How many oz. in a lb.?
Question paraphrasesshare some properties both
with declarative sentence paraphrases and query
paraphrases. On the one hand, questions are com-
plete sentences which differ from declarative sen-
tences by their specific word order and the presence
ofquestionwordsandaquestionfocus. Ontheother
hand,questionsareusuallyassociatedwithanswers,
whichmakesthemsimilartoqueriesassociatedwith
documents. Accordingly, research on the identifi-
cation of question paraphrasesin Q&A repositories
builds upon both sentenceand queryparaphrasing.
Zhao et al. (2007) propose to utilise user click
logs from the Encarta web site to identify question
paraphrases. Jeon et al. (2005) employ a related
method, in that they identify similar answers in the
Naver Questionand Answerdatabaseto retrieve se-
mantically similar questions, while Jijkoun and de
Rijke (2005)includethe answerin the retrieval pro-
cess to return a ranked list of QA pairs in response
to a user’s question. Lytinen and Tomuro (2002)
suggestyetanotherfeaturetoidentifyquestionpara-
phrases,namelyquestiontypesimilarity, whichcon-
sistsin determininga question’s categoryin orderto
matchquestionsonly if they belongto the samecat-
egory.
Ourfocusis on questionparaphraseidentification
in social Q&A sites. Previous research was mostly
basedon questionparaphraseidentificationin FAQs
(Lytinen and Tomuro, 2002; Tomuro and Lytinen,
2004; Jijkoun and de Rijke, 2005). In FAQs, ques-
tions and answers are edited by expert information
suppliers, which guaranteesstricter conformanceto
conventional writing rules. In social Q&A sites,
questionsand answersare writtenby users and may
hence be error-prone. Question paraphrase identi-
fication in social Q&A sites has been little investi-
gated. To our knowledge, only Jeon et al. (2005)
have useddatafroma Q&Asite,namelythe Korean
Naver portal, to find semanticallysimilar questions.
Our work is related to the latter since it employs a
similar dataset, yet in English and from a different
socialQ&Asite.
3 QuestionandAnswerRepositories
3.1 Propertiesof
Q&ARepositories
Questionand answer repositorieshave existed for a
long time on the Internet. Their form has evolved
from Frequently Asked Questions (FAQs) to Ask-
an-expert services(Baram-Tsabariet al., 2006)and,
even more recently, social Q&A sites. The latest,
which include web sites such as Yahoo! Answers
and AnswerBag, provide portals where users can
ask their own questions as well as answer ques-
tions from other users. Social Q&A sites are in-
creasinglypopular. For instance,in December2006
Yahoo! Answers was the second-most visited edu-
cation/referencesite on the Internet after Wikipedia
accordingto the Hitwise company (Prescott, 2006).
Even more strikingly, the Q&A portal Naver is the
leaderof Internetsearchin SouthKorea, well ahead
of Google(Sang-Hun,2007).
Severalfactorsmightexplainthesuccessofsocial
Q&Asites:
• they provide answers to questions which are
difficulttoanswerwithatraditionalWebsearch
or using static reference sites like Wikipedia,
for instanceopinionsor adviceabouta specific
familysituationor a relationshipproblem;
• questionscan be asked anonymously;
• usersdo not have to browsea list of documents
but ratherobtaina completeanswer;
• the answers are almost instantaneous and nu-
merous,due to the large numberof users.
Social Q&A sites record the questions and their
answers online, and thus constitute a formidable
repository of collective intelligence, including an-
swers to complex questions. Moreover, they make
it possible for learners to reach other people world-
wide. TherelevanceofsocialQ&Asitesforlearning
has beenlittleinvestigated. To our knowledge,there
has been only one study which has shown that Ko-
rean users of the Naver Question and Answer plat-
form consider that social Q&A sites can satisfacto-
rily and reliablysupportlearning(Lee,2006).
3.2 WikiAnswers
For our experimentswe collecteda dataset of ques-
tions and their paraphrases from the WikiAnswers
46
website. WikiAnswers1 isasocialQ&Asitesimilar
to Yahoo! Answers and AnswerBag. As of Febru-
ary2008,it contained1,807,600questions,sortedin
2,404categories(AnswersCorporation,2008).
Compared with its competitors, the main origi-
nality of WikiAnswers is that it relies on the wiki
technologyusedin Wikipedia,whichmeansthatan-
swers can be edited and improved over time by all
contributors. Moreover, the Answers Corporation,
which owns the WikiAnswers site, explicitly tar-
gets educationaluses and even providesan educator
toolkit.2 Another interesting property of WikiAn-
swers is that users might manually tag question re-
formulations in order to prevent the duplication of
questions asking the same thing in a different way.
When a user enters a question which is not already
part of the question repository, the web site dis-
plays a list of questions already existing on the site
and similar to the one just asked by the user. The
user may then freelyselectthe questionwhichpara-
phrasesher question,if available, or choose to view
one of the proposedalternatives without labelling it
as a paraphrase. The user-labelled question refor-
mulations are stored in order to retrieve the same
answerwhenthequestionrephrasingis asked again.
The wiki principle holds for the stored reformula-
tions too, since they can subsequently be edited by
other users if they consider that they correspond to
another existing questionor actually ask an entirely
new question. It should be noted that contributors
get not reward in terms of trust points for providing
or editingalternatewordingsfor questions.
We use the wealth of question paraphrasesavail-
able on the WikiAnswers website as the so called
user generated gold standard in our question para-
phrasing experiments. User generated gold stan-
dards have been increasingly used in recent years
for research evaluation purposes, since they can be
easily created from user annotated content. For
instance, Mihalcea and Csomai (2007) use manu-
ally annotated keywords (links to other articles) in
Wikipedia articles to evaluate their automatic key-
word extraction and word sense disambiguational-
gorithms. Similarly, quality assessments provided
by users in social media have been used as gold
1http://wiki.answers.com/
2http://educator.answers.com/
standardsfor the automaticassessmentof postqual-
ity in forum discussions (Weimer et al., 2007). It
should however be kept in mind that user generated
gold standardsare not perfect,as alreadynoticedby
(Mihalceaand Csomai, 2007), and thus constitutea
trade-off solution.
For the experiments described hereafter, we ran-
domly extracted a collection of 1,000 questions
along with their paraphrases (totalling 7,434 ques-
tion paraphrases)from 100 randomly selected FAQ
files in the Education category of the WikiAnswers
web site. In what follows, the corpusof 1,000ques-
tions is called the target questions collection, while
the 7,434 question paraphrases constitute the input
questions collection. The objective of the task is to
retrieve the corresponding target question for each
input question. The target question selected is the
one which maximises the question similarity value
(see section4.2).
4 Method
In order to rate the similarity of input and target
questions, we have first pre-processed both the in-
put andtarget questionsandthenexperimentedwith
several questionsimilaritymeasures.
4.1 Pre-processing
We employthefollowingstepsinpre-processingthe
questions:
Stop words elimination however, we keep ques-
tion words such as how, why, what, etc. since these
make it possible to implicitly identify the question
type (Lytinenand Tomuro,2002;Jijkoun and de Ri-
jke, 2005)
Stemming usingthe PorterStemmer3
Lemmatisation usingthe TreeTagger4
Spelling correction using a statistical system
basedon languagemodelling(Norvig,2007).5
3http://snowball.tartarus.org/
4http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/
5We used a Java implementation of the system, jSpell-
Correct available at http://developer.gauner.org/
jspellcorrect/, trained with the default English training
data, to whichwe appendedthe myspellEnglishdictionaries.
47
Stop words were eliminated in all the experi-
mental settings, while stemming and lemmatisation
were optionally performed to evaluate the effects
of these pre-processing steps on the identification
of question paraphrases. We added spelling correc-
tion to the conventional pre-processingsteps, since
we target paraphrasing of questions which often
contain spelling errors, such as When was indoor
pluming invented? or What is the largest countery
in the western Hemipher? Other related endeav-
ours at retrieving question paraphrases have identi-
fied spelling mistakes in questions as a significant
sourceof errorsin theretrieval process,but have not
attemptedto solve this problem (Jijkoun and de Ri-
jke, 2005;Zhao et al., 2007).
4.2 QuestionSimilarityMeasures
We have experimented with several kinds of ques-
tion similarity measures, belongingto two different
familiesof measures: stringsimilaritymeasuresand
vector spacemeasures.
4.3 StringSimilarityMeasures
Basic string similaritymeasurescomparethe words
contained in the questions without taking word fre-
quency into account.
Matchingcoefficient The matchingcoefficient of
two questions q1 and q2 represented by the set of
distinct words Q1 and Q2 they contain is computed
as follows (Manningand Sch¨utze, 1999):
matchingcoefficient =| Q1 ∩ Q2 |
Overlap coefficient The overlap coefficient is
computedaccordingto thefollowingformula(Man-
ning and Sch¨utze, 1999):
overlapcoefficient = | Q1 ∩ Q2 |min(| Q
1 |,| Q2 |)
Normalised Edit Distance The edit distance of
two questionsisthenumberofwordsthatneedtobe
substituted,inserted,or deleted,to transformq1 into
q2. In order to be able to compare the edit distance
with the other metrics, we have used the follow-
ing formula(Wen et al., 2002)whichnormalisesthe
minimum edit distance by the length of the longest
questionand transformsit into a similaritymetric:
normalisededitdistance = 1− edit dist(q1,q2)max(| q
1 |,| q2 |)
Word Ngram Overlap This metric compares the
word n-gramsin both questions:
ngramoverlap = 1N
Nsummationdisplay
n=1
| Gn(q1) ∩ Gn(q2) |
min(| Gn(q1) |,| Gn(q2) |)
where Gn(q) is the set of n-grams of length n in
question q and N usually equals 4 (Barzilay and
Lee, 2003;Cordeiroet al., 2007).
4.4 VectorSpaceBasedMeasures
Vector space measures represent questions as real-
valued vectors by taking word frequency into ac-
count.
Term Vector Similarity Questions are repre-
sented as term vectors V1 and V2. The feature val-
ues of the vectors are the tf.idf scores of the corre-
spondingterms:
tf.idf = (1 + log(tf)) ∗ log N + 1df
where tf is equal to the frequency of the term in
the question, N is the number of target questions
and df is the number of target questions in which
the term occurs, computed by considering the in-
putquestionaspartofthetargetquestionscollection
(Lytinenand Tomuro,2002).
The similarity of an input question vector and a
target question vector is determined by the cosine
coefficient:
cosinecoefficient = V1 · V2| V
1 | · | V2 |
Lucene’s Extended Boolean Model The prob-
lem of question paraphrase identification can be
cast as an Information Retrieval problem, since in
real-world applications the user posts a question
and the system returns the best matching questions
from its database. We have therefore tested the re-
sults obtained using an Information Retrieval sys-
tem, namely Lucene6, which combines the Vector
Space Model and the Boolean model. Lucene has
alreadybeensuccessfullyusedbyJijkounanddeRi-
jke (2005) to retrieve answersfrom FAQ web pages
by combining several fields: question text, answer
text and the whole FAQ page. The target questions
are indexed as documents and retrieved by trans-
formingthe input questionsinto queries.
6http://lucene.apache.org/java/docs/
48
T
-SW
T
-SW
+SC
S
-SW
S
-SW
+SC
L
-SW
L
-SW
Preprocessing
50
60
70
80
90
100
Accuracy
T
-SW
T
-SW
+SC
S
-SW
S
-SW
+SC
L
-SW
L
-SW
Preprocessing
0.5
0.6
0.7
0.8
0.9
1.0
MRR Matching coefficient
Overlap coefficient
Normalised edit distance
Ngram Overlap
Term vector similarity
Lucene
Figure 1: Accuracy (%) and Mean Reciprocal Rank obtained for different question similarity measures and pre-
processing strategies: tokens (T), stemming (S), lemmatisation (L), stop words removal (-SW), spelling correction
(+SC).
5 EvaluationandExperimentalResults
5.1 EvaluationMeasures
We usethefollowingevaluationmeasuresforevalu-
ating the results:
MeanReciprocalRank For a question,the recip-
rocal rank RR is 1r wherer is the rank of the correct
targetquestion,or zeroif thetargetquestionwas not
found. The Mean Reciprocal Rank (MRR) is the
meanof the reciprocalranksover all the inputques-
tions.
Accuracy We define accuracy as Success@1,
whichis thepercentageof inputquestionsforwhich
the correcttarget questionhas beenretrieved at rank
1.
5.2 ExperimentalResults
Figure 1 displays the accuracy and the mean recip-
rocalranksobtainedwiththedifferentquestionsim-
ilarity measures and pre-processing strategies. As
could be expected, vector space based similarity
measures are consistently more accurate than sim-
ple string similarity measures. Moreover, both the
accuracy and the MRR are rather high for vector
space metrics (accuracy around 80-85% and MRR
around0.85-0.9),whichshows that goodresults can
be obtainedwith these retrieval mechanisms. Addi-
tional pre-processing, i.e. stemming, lemmatisation
and spelling correction, does not ameliorate the to-
kens minusstop words (T -SW)baseline.
5.3 DetailedError
Analysis
Stemming and lemmatisation Morphological
pre-processing brings about mitigated improve-
ments over the tokens-only baseline. On the one
hand, it improves paraphrase retrieval for ques-
tions containingmorphologicalvariantsof the same
wordssuchasWhatareanalogiesformitochondria?
and What is an analogy for mitochondrion? On the
other hand, it also leads to false positives, such has
How was calculus started?, stemmed as How was
calculus start? and lemmatisedas How be calculus
start?, which is mapped by Lucene to the question
How could you start your MA English studies?
instead of Who developed calculus?. The negative
effect of stemming has already been identified by
(Jijkoun and de Rijke, 2005) and our results are
consistentwith this previousfinding.
Spelling correction We expected that spelling
correction would have a positive impact on the re-
sults. There are indeed cases when spelling correc-
tion helps. For instance,given the questionHow do
you become an anestesiologist?, it is impossible to
retrieve the target questionHowmanyyears of med-
ical school do you need to be an anesthesiolgist?
withoutspellingcorrectionsince anesthesiologist is
ill-spelledbothintheparaphraseandthetargetques-
tion.
49
Lemma + Stop words + Spelling correction
Lemma + Stop words
Stem + Stop words + Spelling correction
Stem + Stop words
Token + Stop words + Spelling correction
Token + Stop words
(a)
Lucene
Term Vector similarity
Word Ngram overlap
Overlap coefficient
Matching coefficient
Edit distance
(b)
Figure 2: Comparisonof the different pre-processingstrategies 2(a) and methods2(b) for 50 input questions. For the
pre-processingcomparison,the Luceneretrieval methodhas been used,whilethe methodshave been comparedusing
baselinepre-processing(tokensminusstopwords). A filledsquareindicatesthatthetargetquestionhasbeenretrieved
at rank 1, whilea blanksquareindicatesthat the target questionhas not been retrieved at rank 1.
Therearehowever caseswhenspellingcorrection
inducesworseresults,sinceit is accuratein onlyap-
proximately 70% of the cases (Norvig, 2007). A
majorsourceof errorslies in namedentitiesand ab-
breviations, which are recognised as spelling errors
when they are not part of the training lexicon. For
instance, the question What are the GRE score re-
quired to get into top100 US universities? (where
GRE stands for Graduate Record Examination) is
badly corrected as What are the are score required
to get into top100US universities?.
Spelling correction also induces an unexpected
side effect, when the spelling error does not affect
the question’s focus. For instance, consider the fol-
lowing question, with a spelling error: What events
occured in 1919?, which gets correctly mapped to
the target questionWhat importantevents happened
in 1919? by Lucene;however, after spellingcorrec-
tion (What events occurred in 1919?), it has a big-
geroverlapwithanentirelydifferentquestion: What
events occurred in colonial South Carolina 1674-
1775?.
The latter example also points at another limita-
tion of the evaluatedmethods,whichdo not identify
semantically similar words, such as occurred and
happened.
Errors in the gold standard Some errors can ac-
tuallybetracedbacktoinaccuraciesinthegoldstan-
dard: some question pairs which have been flagged
as paraphrasesby the WikiAnswerscontributors are
actuallydistantlyrelated. Forinstance,thequestions
When was the first painting made? and Where did
leanardo da vinci live? are marked as reformula-
tions of the question What is the secret about mona
lisa? Though these questions all share a common
broad topic, they cannot be considered as relevant
paraphrases.
We can deduce several possible improvements
from what precedes. First, named entities and ab-
breviations play an important role in questions and
shouldthereforebe identifiedand treateddifferently
from other kinds of tokens. This could be achieved
by using a named entity recognition component
during pre-processing and then assigning a higher
weight to named entities in the retrieval process.
This shouldalso improve the resultsof spellingcor-
rectionsincenamedentitiesandabbreviationscould
be excluded from the correction. Second, seman-
tic errors could be dealt with by using a semantic
similaritymetricsimilarto those used in declarative
sentence paraphrase identification (Li et al., 2006;
Mihalceaet al., 2006;Islamand Inkpen,2007).
5.4 ComparisonandCombinationof
the
Methods
In a second part of the experiment, we investigated
whether the evaluated methods display independent
50
error patterns, as suggested by our detailed results
analysis. Figure 2 confirms that the pre-processing
techniques as well as the methods employed result
in dissimilarerror patterns. We therefore combined
several methods and pre-processing techniques in
orderto verify if we couldimprove accuracy.
We obtainedthe best results by performinga ma-
jority vote combination of the following methods
and pre-processingstrategies: Lucene, Term Vector
Similarity with stemming and Ngram Overlap with
spellingcorrection. The combinationyieldedan ac-
curacy of 88.3%, that is 0.9% over the best Lucene
resultswith an accuracy of 87.4%.
6 ConclusionandOutlook
In thispaper, we have shown thatit is feasibleto an-
swerlearners’questionsbyretrievingquestionpara-
phrases from social Q&A sites. As a first step to-
wards this objective, we investigated several ques-
tionsimilaritymetricsandpre-processingstrategies,
usingWikiAnswersas inputdataandusergenerated
gold standard. The approachis however not limited
to this dataset and can be easily applied to retrieve
questionparaphrasesfrom other socialQ&Asites.
We also performed an extended failure analysis
whichprovidedusefulinsightson how resultscould
be further improved by performing named entity
analysisand usingsemanticsimilaritymetrics.
AnotherimportantchallengeinusingsocialQ&A
sites for educational purposes lies in the quality of
the answers retrieved from such sites. Previous re-
searchontheidentificationofhighqualitycontentin
socialQ&Asiteshasdefinedanswerqualityinterms
of correctness, well-formedness,readability, objec-
tivity, relevance, utility and interestingness(Jeon et
al., 2006; Agichtein et al., 2008). It is obvious that
all these elements play an important role in the ac-
ceptance of the answers by learners. We therefore
plan to integrate quality measures in the retrieval
process and to perform evaluations in a real educa-
tionalsetting.
Acknowledgments
ThisworkwassupportedbytheEmmyNoetherPro-
grammeof theGermanResearchFoundation(DFG)
undergrant No. GU 798/3-1.
References
EugeneAgichtein,CarlosCastillo,DeboraDonato,Aris-
tides Gionis, and Gilad Mishne. 2008. Finding
high-quality content in social media. In WSDM ’08:
Proceedings of the international conference on Web
search and web data mining, pages183–194.
Answers Corporation. 2008. WikiAnswers Jour-
nalist Quick Guide. [Online; visited March
4, 2008]. http://site.wikianswers.com/
resources/WikiAnswers_1-pager.pdf.
Ayelet Baram-Tsabari, Ricky J. Sethi, Lynn Bry, and
AnatYarden. 2006. UsingquestionssenttoanAsk-A-
Scientistsite to identifychildren’s interestsin science.
ScienceEducation, 90(6):1050–1072.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervisedapproachusing multiple-
sequence alignment. In Proceedings of NAACL-HLT
2003, pages 16–23. Association for Computational
Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In ACL
’01: Proceedings of the 39th Annual Meeting of the
Associationfor ComputationalLinguistics, pages 50–
57. Associationfor ComputationalLinguistics.
Jo˜ao Cordeiro, Ga¨el Dias, and Pavel Brazdil. 2007.
Learning Paraphrases from WNS Corpora. In David
Wilson and Geoff Sutcliffe, editors, Proceedings of
the Twentieth International Florida Artificial Intelli-
gence Research Society Conference (FLAIRS), pages
193–198, Key West, Florida, USA, May 7-9. AAAI
Press.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In COL-
ING ’04: Proceedings of the 20th international con-
ferenceonComputationalLinguistics, pages350–356.
Associationfor ComputationalLinguistics.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard Hovy.
2006. An Intelligent Discussion-Bot for Answering
StudentQueriesinThreadedDiscussions. InProceed-
ingsof the11thinternationalconferenceonIntelligent
user interfaces(IUI’06), pages171–177.
Aminul Islam and Diana Inkpen. 2007. Semantic Sim-
ilarity of Short Texts. In Proceedings of the Interna-
tionalConferenceonRecentAdvancesinNaturalLan-
guage Processing(RANLP2007), Borovets, Bulgaria,
September.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Findingsimilarquestionsin largequestionandanswer
archives. In CIKM ’05: Proceedingsof the 14th ACM
international conference on Information and knowl-
edge management, pages84–90.
51
Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon
Park. 2006. A framework to predict the quality of
answerswith non-textual features. In SIGIR’06: Pro-
ceedingsof the 29th annualinternationalACM SIGIR
conference on Research and development in informa-
tion retrieval, pages228–235.
ValentinJijkoun and Maartende Rijke. 2005. Retrieving
answersfrom frequentlyasked questionspages on the
web. In CIKM ’05: Proceedings of the 14th ACM in-
ternationalconference on Informationand knowledge
management, pages76–83.
Yu Sun Lee. 2006. Toward a New Knowledge Shar-
ing Community: Collective Intelligence and Learn-
ing through Web-Portal-BasedQuestion-AnswerSer-
vices. Masters of arts in communication, culture &
technology, Faculty of the Graduate School of Arts
and Sciences of Georgetown University, May. [On-
line; visited February 15, 2008], http://hdl.
handle.net/1961/3701.
Yuhua Li, David McLean, Zuhair A. Bandar, James D.
O’Shea, and Keeley Crockett. 2006. Sentence Simi-
larity Based on Semantic Nets and Corpus Statistics.
IEEETransactionson Knowledge andDataEngineer-
ing, 18(8):1138–1150.
Steven L. Lytinen and Noriko Tomuro. 2002. The Use
of Question Types to Match Questionsin FAQFinder.
In Proceedings of the 2002 AAAI Spring Symposium
on Mining Answers from Texts and Knowledge Bases,
pages46–53.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundationsof StatisticalNatural Language Process-
ing. The MIT Press,Cambridge,Massachusetts.
RadaMihalceaandAndrasCsomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In CIKM
’07: Proceedingsof the sixteenth ACM conference on
information and knowledge management, pages 233–
242.
RadaMihalcea,Courtney Corley, and CarloStrapparava.
2006. Corpus-basedand Knowledge-basedMeasures
of Text Semantic Similarity. In Proceedings of AAAI
2006, Boston,July.
Peter Norvig. 2007. How to Write a Spelling Correc-
tor. [Online; visited February 22, 2008]. http:
//norvig.com/spell-correct.html.
Lee Ann Prescott. 2006. Yahoo! Answers Cap-
tures 96% of Q and A Market Share. Hit-
wise Intelligence [Online; visited February
26, 2008]. http://weblogs.hitwise.
com/leeann-prescott/2006/12/yahoo_
answers_captures_96_of_q.html.
MehranSahamiand Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In WWW ’06: Proceedings of
the 15th internationalconference on World Wide Web,
pages377–386.
Choe Sang-Hun. 2007. To outdo Google,
Naver taps into Korea’s collective wis-
dom. International Herald Tribune, July 4.
http://www.iht.com/articles/2007/
07/04/technology/naver.php.
Noriko Tomuro and Steven Lytinen. 2004. Retrieval
Models and Q&A Learning with FAQ Files. In
Mark T. Maybury, editor, New Directions in Question
Answering, pages183–194.AAAIPress.
Markus Weimer, Iryna Gurevych, and Max M¨uhlh¨auser.
2007. Automatically Assessing the Post Quality in
OnlineDiscussionsonSoftware. InProceedingsofthe
Demo and Poster Sessions of the 45th Annual Meet-
ing of the Association for ComputationalLinguistics,
pages 125–128, Prague, Czech Republic, June. Asso-
ciationfor ComputationalLinguistics.
Ji-Rong Wen, Jian-Yun Nie, and Hong-Jiang Zhang.
2002. Query clustering using user logs. ACM Trans.
Inf. Syst., 20(1):59–81.
Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learn-
ing Question Paraphrases for QA from Encarta Logs.
In Proceedingsof the 20th InternationalJoint Confer-
ence on Artificial Intelligence, pages 1795–1801,Hy-
derabad,India,January6-12.
52

