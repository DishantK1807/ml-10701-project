The tree chooser associates a supertag (Bangalore and Joshi, 1999) from a treeadjoining grammar (TAG) with each node in the underspeci ed dependency tree.
J99-2004
We partition the set of all features used by (Walker et al., 2001) to train SPoT into three subsets according to their level of domain and task dependence.
N01-1003
From this raw text, an LDA parser (Bangalore and Joshi, 1999) trained using the XTAG-based Penn Treebank creates a partially-parsed, non-hand-checked treebank.
J99-2004
(Chiang, 2000), (Xia, 1999)).
P00-1058
Test data is obtained by output strings that are produced by the combination of SPoT and the RealPro surface realizer (Lavoie and Rambow, 1998).
P98-1118
The other kind of treebank is the BLLIP corpus (Charniak, 2000).
A00-2018
(Bangalore et al., 2000) nds this metric to correlate well with human judgments of understandability and quality.
C00-1007 W00-1401
Here, we select as our metric understandability accuracy, de ned in (Bangalore et al., 2000) as quantifying the di erPTB TM HH TM PTB LM 0.30 0.38 HH LM 0.37 0.41 Table 2: Average understandability accuracies using XTAG-Based FERGUS for various kinds of training data PTB TM PTB LM 0.39 HH LM 0.33 Table 3: Average understandability accuracies using automatically-extracted grammar based FERGUS for various kinds of training data ence between the generator output, in terms of both dependency tree and surface string, and the desired reference output.
C00-1007 W00-1401
\Albuquerque." We have trained and tested SPoT with these di erent feature subsets using the air-travel domain corpus of 100 text plans borrowed from (Walker et al., 2001), using ve fold crossvalidation.
N01-1003
(Bangalore et al., 2001) perform all of their experiments on the same domain of Wall Street Journal news articles.
W01-0520
The sentence planning stage is embodied by the SPoT sentence planner (Walker et al., 2001), while the surface realization stage is embodied by the FERGUS surface realizer (Bangalore and Rambow, 2000).
C00-1007 N01-1003 W00-1401
Disadvantages of using XTAG are the considerable amount of human labor expended in its development and the lack of a treebank based on XTAG|the only way to estimate parameters in the TM is to rely on a heuristic mapping of XTAG tree frames onto a pre-existing treebank (Bangalore and Joshi, 1999).
J99-2004
We extend the work of (Walker et al., 2001) and (Bangalore and Rambow, 2000) in various ways.
C00-1007 N01-1003 W00-1401
But (Bangalore et al., 2001) show that automatically generated version of these resources can be used by FERGUS to obtain quality output.
W01-0520
One kind is the Penn Treebank (Marcus et al., 1993).
J93-2004
The best result is obtained by using HH to train both the TM and the LM; this result (0.41) is comparable to the result obtained by using matched PTB training and test data (0.43) that is used in (Bangalore et al., 2001).
W01-0520
Two kinds of TAG grammar are used in (Bangalore et al., 2001).
W01-0520
Stochastic methods for NLG may provide such automaticity, but most previous work (Knight and Hatzivassiloglou, 1995), (Langkilde and Knight, 1998), (Oh and Rudnicky, 2000), (Uchimoto et al., 2000), (Bangalore and Rambow, 2000) concentrate on the speci cs of individual stochastic methods, ignoring other issues such as integrability, portability, and e ciency.
C00-1007 C00-2126 P95-1034 W00-0306 W00-1401
Two kinds of treebank are used in (Bangalore et al., 2001).
W01-0520
(Bangalore et al., 2001) experimentally determine how the quality and quantity of the resources used in training FERGUS a ect the output quality of the generator.
W01-0520
