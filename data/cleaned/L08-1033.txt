<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<title>The Second PASCAL Recognising Textual Entailment Challenge</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop</booktitle>
<contexts>
<context>l’s Italian and Swiss companies. Hypothesis: Alfred Nobel is the inventor of dynamite. Table 1: Examples of Textual Entailment Following the success of the PASCAL RTE evaluations (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007), (Harabagiu et al., 2006) introduced a complementary form of inference, known as textual contradiction (TC). In (Harabagiu et al., 2006)’s framework, a t is considered to t</context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The Second PASCAL Recognising Textual Entailment Challenge. In Proceedings of the Second PASCAL Challenges Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blackburn</author>
<author>J Bos</author>
</authors>
<title>Representation and inference for natural language : a rst course in computational semantics. Center for the Study of Language and Information</title>
<date>2005</date>
<location>Stanford, Calif</location>
<contexts>
<context>ee http://www.pascal-network.org/Challenges/. formal, logic-based methods (such as automatic theorem proving(Tatu et al., 2006)) or model-based approaches (such as model building or model checking (Blackburn and Bos, 2005)). However, a considerable amount of recent work including many of the top-performing systems at the past PASCAL RTE Challenges (Hickl and Bensley, 2007; Hickl et al., 2006; Haghighi et al., 2005) ha</context>
</contexts>
<marker>Blackburn, Bos, 2005</marker>
<rawString>P. Blackburn and J. Bos. 2005. Representation and inference for natural language : a  rst course in computational semantics. Center for the Study of Language and Information, Stanford, Calif.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katya Markert</author>
</authors>
<title>When logical inference helps in determining textual entailment (and when it doesn’t</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Recognizing Textual Entailment Conference</booktitle>
<location>Venice, Italy</location>
<contexts>
<context>ent types of features in order to perform this classi cation (including syntactic heuristics (Vanderwende et al., 2006), graph matching techniques (Raina et al., 2005), or output from model checking (Bos and Markert, 2006) or paraphrasing (Hickl et al., 2006) applications), access to sources of training data has continued to be a limiting factor. This paper follows initial work done by (Burger and Ferro, 2005; Brocket</context>
</contexts>
<marker>Bos, Markert, 2006</marker>
<rawString>Johan Bos and Katya Markert. 2006. When logical inference helps in determining textual entailment (and when it doesn’t). In Proceedings of the Second PASCAL Recognizing Textual Entailment Conference, Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
<author>William Dolan</author>
</authors>
<title>Support Vector Machines for Paraphrase Identi cation and Corpus Construction</title>
<date>2005</date>
<booktitle>In Proceedings of the Third International Workshop on Paraphrasing, Jeju, Korea</booktitle>
<contexts>
<context>t, 2006) or paraphrasing (Hickl et al., 2006) applications), access to sources of training data has continued to be a limiting factor. This paper follows initial work done by (Burger and Ferro, 2005; Brockett and Dolan, 2005; Dolan and Quirk, 2004) in exploring how a battery of unsupervised techniques can be used in order to create large, high-quality corpora for textual inference applications. We show that it is possibl</context>
</contexts>
<marker>Brockett, Dolan, 2005</marker>
<rawString>Chris Brockett and William Dolan. 2005. Support Vector Machines for Paraphrase Identi cation and Corpus Construction. In Proceedings of the Third International Workshop on Paraphrasing, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Burger</author>
<author>Lisa Ferro</author>
</authors>
<title>Generating an Entailment Corpus from News Headlines</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment</booktitle>
<pages>49--54</pages>
<contexts>
<context>checking (Bos and Markert, 2006) or paraphrasing (Hickl et al., 2006) applications), access to sources of training data has continued to be a limiting factor. This paper follows initial work done by (Burger and Ferro, 2005; Brockett and Dolan, 2005; Dolan and Quirk, 2004) in exploring how a battery of unsupervised techniques can be used in order to create large, high-quality corpora for textual inference applications. </context>
<context>tions 3 and 4 presents the techniques we used to create training corpora for our RTE and RTC systems: Section 3 discusses how we extended extractionbased techniques (similar to those rst proposed in (Burger and Ferro, 2005)) for this task, while Section 4 examines how a generative approach can be used to create training pairs which can be used with either a textual entailment or textual contradiction system. Section 5 </context>
<context>of TE and TC. These methods are derived from analyzing the ways that elaboration, contrast, and paraphrase are generally presented in professional journalism texts. Our rst extractive method follows (Burger and Ferro, 2005) in creating positive textual entailment t-h pairs by pairing the rst sentence of a newswire document (assumed to be the t) with its corresponding headline (assumed to be the h). Since the rst senten</context>
</contexts>
<marker>Burger, Ferro, 2005</marker>
<rawString>John Burger and Lisa Ferro. 2005. Generating an Entailment Corpus from News Headlines. In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 49 54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognizing Textual Entailment Challenge</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop</booktitle>
<contexts>
<context>y the fusion of Nobel’s Italian and Swiss companies. Hypothesis: Alfred Nobel is the inventor of dynamite. Table 1: Examples of Textual Entailment Following the success of the PASCAL RTE evaluations (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007), (Harabagiu et al., 2006) introduced a complementary form of inference, known as textual contradiction (TC). In (Harabagiu et al., 2006)’s framework,</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL Recognizing Textual Entailment Challenge. In Proceedings of the PASCAL Challenges Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Dolan</author>
<author>Chris Quirk</author>
</authors>
<title>Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources</title>
<date>2004</date>
<booktitle>In Proceedings of COLING 2004</booktitle>
<location>Geneva, Switzerland</location>
<contexts>
<context>Hickl et al., 2006) applications), access to sources of training data has continued to be a limiting factor. This paper follows initial work done by (Burger and Ferro, 2005; Brockett and Dolan, 2005; Dolan and Quirk, 2004) in exploring how a battery of unsupervised techniques can be used in order to create large, high-quality corpora for textual inference applications. We show that it is possible to automatically gene</context>
</contexts>
<marker>Dolan, Quirk, 2004</marker>
<rawString>William Dolan and Chris Quirk. 2004. Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources. In Proceedings of COLING 2004, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
</authors>
<title>The third pascal recognizing textual entailment challenge</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</booktitle>
<volume>1</volume>
<pages>pages</pages>
<location>Prague</location>
<contexts>
<context>ompanies. Hypothesis: Alfred Nobel is the inventor of dynamite. Table 1: Examples of Textual Entailment Following the success of the PASCAL RTE evaluations (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007), (Harabagiu et al., 2006) introduced a complementary form of inference, known as textual contradiction (TC). In (Harabagiu et al., 2006)’s framework, a t is considered to textually contradict a h if</context>
</contexts>
<marker>Giampiccolo, Magnini, Dagan, Dolan, 2007</marker>
<rawString>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 1 9, Prague, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Glickman</author>
<author>Ido Dagan</author>
</authors>
<title>A Probabilistic Setting and Lexical Co-occurrence Model for Textual Entailment</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment</booktitle>
<location>Ann Arbor, USA</location>
<contexts>
<context> entailment and textual contradiction has underscored the need for large sources of training data which can be used to construct accurate models for recognizing textual inference. First described in (Glickman and Dagan, 2005), the task of recognizing textual entailment (RTE) requires systems to determine whether a short statement (conventionally known as a hypothesis (or h)) can be conventionally inferred from a longer p</context>
<context>and RTC systems along two dimensions: accuracy and average precision. We de ne accuracy as the percentage of inference pairs correctly classi ed by an RTE/RTC system. Average precision, is de ned by (Glickman and Dagan, 2005) as: 1 n Pn i=1 Pi j=1(correctj) i : correctj 2 0; 1This scoring metric assumes a sorted output based on con dence weights, with the highest con dence judgments appearing at the top of the sorted ord</context>
</contexts>
<marker>Glickman, Dagan, 2005</marker>
<rawString>Oren Glickman and Ido Dagan. 2005. A Probabilistic Setting and Lexical Co-occurrence Model for Textual Entailment. In Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Gunlogson</author>
</authors>
<title>True to Form: Rising and Falling Declaratives as Questions in English</title>
<date>2001</date>
<tech>Ph.D. thesis</tech>
<institution>University of California</institution>
<location>Santa Cruz</location>
<contexts>
<context>edge for recognizing textual entailment and textual contradiction, we have developed a novel framework which depends on the extraction of discourse commitments from a text-hypothesis pair. Following (Gunlogson, 2001; Stalnaker, 1979), we assume discourse commitments represent the set of propositions which can necessarily be inferred to be true given a conventional reading of a text. Formally, we assume that give</context>
</contexts>
<marker>Gunlogson, 2001</marker>
<rawString>Christine Gunlogson. 2001. True to Form: Rising and Falling Declaratives as Questions in English. Ph.D. thesis, University of California, Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Andrew Ng</author>
<author>Christopher Manning</author>
</authors>
<title>Robust textual inference via graph matching</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</booktitle>
<pages>387--394</pages>
<contexts>
<context> (Blackburn and Bos, 2005)). However, a considerable amount of recent work including many of the top-performing systems at the past PASCAL RTE Challenges (Hickl and Bensley, 2007; Hickl et al., 2006; Haghighi et al., 2005) has demonstrated the effectiveness of using shallow statistical classi ers in order to recognize TE (or TC) relations. While individual systems have exploited a wide range of different types of feat</context>
</contexts>
<marker>Haghighi, Ng, Manning, 2005</marker>
<rawString>Aria Haghighi, Andrew Ng, and Christopher Manning. 2005. Robust textual inference via graph matching. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 387 394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda M Harabagiu</author>
<author>Andrew Hickl</author>
<author>Finley Lacatusu</author>
</authors>
<date>2006</date>
<contexts>
<context> Nobel is the inventor of dynamite. Table 1: Examples of Textual Entailment Following the success of the PASCAL RTE evaluations (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007), (Harabagiu et al., 2006) introduced a complementary form of inference, known as textual contradiction (TC). In (Harabagiu et al., 2006)’s framework, a t is considered to textually contradict a h if there exists any proposit</context>
<context>rces of generated training data can have on the performance of state-of-the-art systems for recognizing textual entailment (RTE) (Hickl and Bensley, 2007) and recognizing textual contradiction (RTC) (Harabagiu et al., 2006). Our results con rm the hypothesis ( rst suggested in (Hickl et al., 2006)) that the performance of classi cation-based systems for RTE increases with the amount of available training data. In our e</context>
<context> way. Section 2 provides an overview of the general learningbased framework for recognizing instances of textual entailment and textual contradiction previously described in (Hickl and Bensley, 2007; Harabagiu et al., 2006; Hickl et al., 2006). Sections 3 and 4 presents the techniques we used to create training corpora for our RTE and RTC systems: Section 3 discusses how we extended extractionbased techniques (similar </context>
<context>03). Given a syntactically and 2Full details of our framework for recognizing instances of TE can be found in (Hickl and Bensley, 2007). Full details of our system for recognizing TC can be found in (Harabagiu et al., 2006). semantically-parsed input string, our system returns a series of output representations which can be mapped (given a set of generation heuristics) to natural language sentences which represent each</context>
<context>provide sources of training data for RTE and RTC systems which are as good if not better than the manually-created sources of training data provided by the PASCAL RTE organizers or by the authors of (Harabagiu et al., 2006)3. It is our expectation 3The PASCAL RTE organizers have released a collection of Preprocessing Hyp Text Commitment Selection Commitment Extraction NO YES Inference Classification a0 a0a1 a1 a2 a2a3 </context>
</contexts>
<marker>Harabagiu, Hickl, Lacatusu, 2006</marker>
<rawString>Sanda M. Harabagiu, Andrew Hickl, and Finley Lacatusu. 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Negation</author>
</authors>
<title>contrast and contradiction in text processing</title>
<booktitle>In Proceedings, The Twenty-First National Conference on Arti cial Intelligence (AAAI-2006</booktitle>
<marker>Negation, </marker>
<rawString>Negation, contrast and contradiction in text processing. In Proceedings, The Twenty-First National Conference on Arti cial Intelligence (AAAI-2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>Jeremy Bensley</author>
</authors>
<title>Recognizing Textual Entailment with LCC’s Groundhog System</title>
<date>2007</date>
<booktitle>In Proceedings of the Third PASCAL Challenges Workshop</booktitle>
<note>to appear</note>
<contexts>
<context>tive Resource Creation Methods In this section we describe how we experimented with methods for generating inference pairs which leverages the discourse commitment extraction framework introduced in (Hickl and Bensley, 2007) in order to generate candidate hypotheses from text passages retrieved from a document collection. Under this approach, the texts included in the PASCAL RTE-1, RTE-2, and RTE-3 test sets (as well as</context>
</contexts>
<marker>Hickl, Bensley, 2007</marker>
<rawString>Andrew Hickl and Jeremy Bensley. 2007. Recognizing Textual Entailment with LCC’s Groundhog System. In Proceedings of the Third PASCAL Challenges Workshop (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>John Williams</author>
<author>Jeremy Bensley</author>
<author>Kirk Roberts</author>
<author>Bryan Rink</author>
<author>Ying Shi</author>
</authors>
<title>Recognizing Textual Entailment with LCC’s Groundhog System</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop</booktitle>
<contexts>
<context>ng or model checking (Blackburn and Bos, 2005)). However, a considerable amount of recent work including many of the top-performing systems at the past PASCAL RTE Challenges (Hickl and Bensley, 2007; Hickl et al., 2006; Haghighi et al., 2005) has demonstrated the effectiveness of using shallow statistical classi ers in order to recognize TE (or TC) relations. While individual systems have exploited a wide range of </context>
<context>m this classi cation (including syntactic heuristics (Vanderwende et al., 2006), graph matching techniques (Raina et al., 2005), or output from model checking (Bos and Markert, 2006) or paraphrasing (Hickl et al., 2006) applications), access to sources of training data has continued to be a limiting factor. This paper follows initial work done by (Burger and Ferro, 2005; Brockett and Dolan, 2005; Dolan and Quirk, 2</context>
<context> systems for recognizing textual entailment (RTE) (Hickl and Bensley, 2007) and recognizing textual contradiction (RTC) (Harabagiu et al., 2006). Our results con rm the hypothesis ( rst suggested in (Hickl et al., 2006)) that the performance of classi cation-based systems for RTE increases with the amount of available training data. In our experiments, increases in accuracy are observed when training on as many as </context>
<context> an overview of the general learningbased framework for recognizing instances of textual entailment and textual contradiction previously described in (Hickl and Bensley, 2007; Harabagiu et al., 2006; Hickl et al., 2006). Sections 3 and 4 presents the techniques we used to create training corpora for our RTE and RTC systems: Section 3 discusses how we extended extractionbased techniques (similar to those rst propose</context>
<context>lenges, we used a decision tree (C5.0 (Quinlan, 1998)) to estimate the likelihood that a commitment pair represented a valid instance of textual entailment or textual contradiction. In previous work (Hickl et al., 2006), we showed that performance on the RTE task could be increased by more than 10% when a baseline classi cation-based system was allowed to train on more than 200,000 examples of textual entailment th</context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Rink, Shi, 2006</marker>
<rawString>Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts, Bryan Rink, and Ying Shi. 2006. Recognizing Textual Entailment with LCC’s Groundhog System. In Proceedings of the Second PASCAL Challenges Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>Kirk Roberts</author>
<author>Bryan Rink</author>
<author>Jeremy Bensely</author>
<author>Tobias Jungen</author>
<author>Ying Shi</author>
<author>John Williams</author>
</authors>
<date>2007</date>
<booktitle>Question Answering with LCC’s Chaucer-2 at TREC 2007. In Proceedings of the Sixteenth Text REtrieval Conference</booktitle>
<contexts>
<context>ommitments Extracted from a Positive Instance of TE. named entity recognition, and syntactic dependency parsing. Keywords extracted and expanded from these preprocessed texts by methods described in (Hickl et al., 2007) were used to retrieve passages from the 2 GB AQUAINT2 newswire collection. Passages were then ranked based on the density of keywords and submitted to a sentence decomposition module, which uses a s</context>
</contexts>
<marker>Hickl, Roberts, Rink, Bensely, Jungen, Shi, Williams, 2007</marker>
<rawString>Andrew Hickl, Kirk Roberts, Bryan Rink, Jeremy Bensely, Tobias Jungen, Ying Shi, and John Williams. 2007. Question Answering with LCC’s Chaucer-2 at TREC 2007. In Proceedings of the Sixteenth Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Quinlan</author>
</authors>
<title>C5.0: An Informal Tutorial</title>
<date>1998</date>
<publisher>RuleQuest</publisher>
<contexts>
<context>der until a positive judgment is returned, or until no more commitments above a threshold remain. Following work done by many participants in the PASCAL RTE Challenges, we used a decision tree (C5.0 (Quinlan, 1998)) to estimate the likelihood that a commitment pair represented a valid instance of textual entailment or textual contradiction. In previous work (Hickl et al., 2006), we showed that performance on t</context>
</contexts>
<marker>Quinlan, 1998</marker>
<rawString>R. Quinlan. 1998. C5.0: An Informal Tutorial. RuleQuest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajat Raina</author>
<author>Andrew Y Ng</author>
<author>Chris Manning</author>
</authors>
<title>Robust textual inference via learning and abductive reasoning</title>
<date>2005</date>
<booktitle>In Proceedings of the Twentieth National Conference on Arti cial Intelligence (AAAI</booktitle>
<contexts>
<context>vidual systems have exploited a wide range of different types of features in order to perform this classi cation (including syntactic heuristics (Vanderwende et al., 2006), graph matching techniques (Raina et al., 2005), or output from model checking (Bos and Markert, 2006) or paraphrasing (Hickl et al., 2006) applications), access to sources of training data has continued to be a limiting factor. This paper follow</context>
</contexts>
<marker>Raina, Ng, Manning, 2005</marker>
<rawString>Rajat Raina, Andrew Y. Ng, and Chris Manning. 2005. Robust textual inference via learning and abductive reasoning. In Proceedings of the Twentieth National Conference on Arti cial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Stalnaker</author>
</authors>
<date>1979</date>
<journal>Assertion</journal>
<volume>9</volume>
<pages>315--332</pages>
<contexts>
<context>ing textual entailment and textual contradiction, we have developed a novel framework which depends on the extraction of discourse commitments from a text-hypothesis pair. Following (Gunlogson, 2001; Stalnaker, 1979), we assume discourse commitments represent the set of propositions which can necessarily be inferred to be true given a conventional reading of a text. Formally, we assume that given a commitment se</context>
</contexts>
<marker>Stalnaker, 1979</marker>
<rawString>Robert Stalnaker, 1979. Assertion, volume 9, pages 315 332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simone Lacoste-Julien</author>
<author>Michael Jordan</author>
</authors>
<title>Structured prediction via the extragradient method</title>
<date>2005</date>
<booktitle>In Proceedings of Neural Information Processing Systems</booktitle>
<contexts>
<context> a maximum weighted matching problem in which each pair of words (ti,hj) in an commitment pair (ct,ch) is assigned a score sij(t; h) corresponding to the likelihood that ti is aligned to hj. As with (Taskar et al., 2005b), we use the large-margin structured prediction model introduced in (Taskar et al., 2005a) in order to compute a set of parameters w (computed with respect to a set of features f) which maximize the</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Jordan, 2005</marker>
<rawString>Ben Taskar, Simone Lacoste-Julien, and Michael Jordan. 2005a. Structured prediction via the extragradient method. In Proceedings of Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simone Lacoste-Julien</author>
<author>Dan Klein</author>
</authors>
<date>2005</date>
<contexts>
<context> a maximum weighted matching problem in which each pair of words (ti,hj) in an commitment pair (ct,ch) is assigned a score sij(t; h) corresponding to the likelihood that ti is aligned to hj. As with (Taskar et al., 2005b), we use the large-margin structured prediction model introduced in (Taskar et al., 2005a) in order to compute a set of parameters w (computed with respect to a set of features f) which maximize the</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>Ben Taskar, Simone Lacoste-Julien, and Dan Klein. 2005b.</rawString>
</citation>
<citation valid="true">
<title>A discriminative matching approach to word alignment</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Empirical Methods in Natural Language Processing (HLT/EMNLP</booktitle>
<marker>2005</marker>
<rawString>A discriminative matching approach to word alignment. In Proceedings of Human Language Technology Conference and Empirical Methods in Natural Language Processing (HLT/EMNLP 2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Tatu</author>
<author>Brandon Iles</author>
<author>John Slavick</author>
<author>Adrian Novischi</author>
<author>Dan Moldovan</author>
</authors>
<title>COGEX at the Second Recognizing Textual Entailment Challenge</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop</booktitle>
<contexts>
<context>aken from the PASCAL RTE-3 Test Set. For more information on the PASCAL RTE Challenges, see http://www.pascal-network.org/Challenges/. formal, logic-based methods (such as automatic theorem proving(Tatu et al., 2006)) or model-based approaches (such as model building or model checking (Blackburn and Bos, 2005)). However, a considerable amount of recent work including many of the top-performing systems at the pas</context>
</contexts>
<marker>Tatu, Iles, Slavick, Novischi, Moldovan, 2006</marker>
<rawString>Marta Tatu, Brandon Iles, John Slavick, Adrian Novischi, and Dan Moldovan. 2006. COGEX at the Second Recognizing Textual Entailment Challenge. In Proceedings of the Second PASCAL Challenges Workshop.</rawString>
</citation>
</citationList>
</algorithm>

