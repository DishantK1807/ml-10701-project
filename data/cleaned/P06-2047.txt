Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 361–368, Sydney, July 2006.
c©2006 Association for Computational Linguistics Graph Branch Algorithm: An Optimum Tree Search Method for Scored Dependency Graph with Arc Co-occurrence Constraints Hideki Hirakawa Toshiba R&D Center 1 Komukai Toshiba-cho, Saiwai-ku, Kawasaki 210, JAPAN hideki.hirakawa@toshiba.co.jp Abstract Various kinds of scored dependency graphs are proposed as packed shared data structures in combination with optimum dependency tree search algorithms.
This paper classifies the scored dependency graphs and discusses the specific features of the “Dependency Forest” (DF) which is the packed shared data structure adopted in the “Preference Dependency Grammar” (PDG), and proposes the “Graph Branch Algorithm” for computing the optimum dependency tree from a DF.
This paper also reports the experiment showing the computational amount and behavior of the graph branch algorithm.
1 Introduction
The dependency graph (DG) is a packed shared data structure which consists of the nodes corresponding to the words in a sentence and the arcs showing dependency relations between the nodes.
The scored DG has preference scores attached to the arcs and is widely used as a basis of the optimum tree search method.
For example, the scored DG is used in Japanese Kakari-uke analysis1 to represent all possible kakari-uke(dependency) trees(Ozeki, 1994),(Hirakawa, 2001).
(McDonald et al., 2005) proposed a dependency analysis method using a scored DG and some maximum spanning tree search algorithms.
In this method, scores on arcs are computed from a set of features obtained from the dependency trees based on the 1Kakari-uke relation, widely adopted in Japanese sentence analysis, is projective dependency relation with a constraint such that the dependent word is located at the left-hand side of its governor word.
optimum parameters for scoring dependency arcs obtained by the discriminative learning method.
There are various kinds of dependency analysis methods based on the scored DGs.
This paper classifies these methods based on the types of the DGs and the basic well-formed constraints and explains the features of the DF adopted in PDG(Hirakawa, 2006).
This paper proposes the graph branch algorithm which searches the optimum dependency tree from a DF based on the branch and bound (B&B) method(Ibaraki, 1978) and reports the experiment showing the computational amount and behavior of the graph branch algorithm.
As shown below, the combination of the DF and the graph branch algorithm enables the treatment of non-projective dependency analysis and optimum solution search satisfying the single valence occupation constraint, which are out of the scope of most of the DP(dynamic programming)based parsing methods.
2 Optimum
Tree Search in a Scored DG 2.1 Basic Framework Figure 1 shows the basic framework of the optimum dependency tree search in a scored DG.
In general, nodes in a DG correspond to words in the sentence and the arcs show some kind of dependency relations between nodes.
Each arc has a0a1a2 Scored Dependency Graph Dependency Tree Set of Scored Wellformed Dependency Trees Well-formed dependency tree constraint Optimum Tree Search Algorithm Well-formed Dependency Tree with the highest score s1 s2 s3 s4 s5 (score=s1+s2+s3+s4+s5 ) Figure 1: The optimum tree search in a scored DG 361 a preference score representing plausibility of the relation.
The well-formed dependency tree constraint is a set of well-formed constraints which should be satisfied by all dependency trees representing sentence interpretations.
A DG and a wellformed dependency tree constraint prescribe a set of well-formed dependency trees.
The score of a dependency tree is the sum total of arc scores.
The optimum tree is a dependency tree with the highest score in the set of dependency trees.
2.2 Dependency
Graph DGs are classified into some classes based on the types of nodes and arcs.
This paper assumes three types of nodes, i.e. word-type, WPP-type2 and concept-type3.
The types of DGs are called a word DG, a WPP DG and a concept DG, respectively.
DGs are also classified into non-labeled and labeled DGs.
There are some types of arc labels such as syntactic label (ex.
“subject”,“object”) and semantic label (ex.
“agent”,“target”).
Various types of DGs are used in existing systems according to these classifications, such as non-label word DG(Lee and Choi, 1997; Eisner, 1996; McDonald et al., 2005)4, syntactic-label word DG (Maruyama, 1990), semantic-label word DG(Hirakawa, 2001), non-label WPP DG(Ozeki, 1994; Katoh and Ehara, 1989), syntactic-label WPP DG(Wang and Harper, 2004), semantic-label concept DG(Harada and Mizuno, 2001).
2.3 Well-formedness Constraints and Graph Search Algorithms There can be a variety of well-formedness constraints from very basic and language-independent constraints to specific and language-dependent constraints.
This paper focuses on the following four basic and language-independent constraints which may be embedded in data structure and/or the optimum tree search algorithm.
(C1) Coverage constraint: Every input word has a corresponding node in the tree (C2) Single role constraint(SRC): No two nodes in a dependency tree occupy the same input position 2WPP is a pair of a word and a part of speech (POS).
The word “time” has WPPs such as “time/n” and “time/v”.
3One WPP (ex.
“time/n”) can be categorized into one or more concepts semantically (ex.
“time/n/period time” and “time/n/clock time”).
4This does not mean that these algorithms can not handle labeled DGs.
(C3) Projectivity constraint(PJC): No arc crosses another arc5 (C4) Single valence occupation constraint(SVOC): No two arcs in a tree occupy the same valence of a predicate (C1) and (C2), collectively referred to as “covering constraint”, are basic constraints adopted by almost all dependency parsers.
(C3) is adopted by the majority of dependency parsers which are called projective dependency parsers.
A projective dependency parser fails to analyze non-projective sentences.
(C4) is a basic constraint for valency but is not adopted by the majority of dependency parsers.
Graph search algorithms, such as the ChuLiu-Edmonds maximum spanning tree algorithm (Chu and Liu, 1965; Edmonds, 1967), algorithms based on the dynamic programming (DP) principle (Ozeki, 1994; Eisner, 1996) and the algorithm based on the B&B method (Hirakawa, 2001), are used for the optimum tree search in scored DGs.
The applicability of these algorithms is closely related to the types of DGs and/or well-formedness constraints.
The Chu-Liu-Edmonds algorithm is very fast (C7B4D2BEB5 for sentence length D2), but it works correctly only on word DGs.
DP-based algorithms can satisfy (C1)-(C3) and run efficiently, but seems not to satisfy (C4) as shown in 2.4.
(C2)-(C4) can be described as a set of cooccurrence constraints between two arcs in a DG.
As described in Section 2.6, the DF can represent (C2)-(C4) and more precise constraints because it can handle co-occurrence constraints between two arbitrary arcs in a DG.
The graph branch algorithm described in Section 3 can find the optimum tree from the DF.
2.4 SVOC
and DP (Ozeki and Zhang, 1999) proposed the minimum cost partitioning method (MCPM) which is a partitioning computation based on the recurrence equation where the cost of joining two partitions is the cost of these partitions plus the cost of combining these partitions.
MCPM is a generalization of (Ozeki, 1994) and (Katoh and Ehara, 1989) which compute the optimum dependency tree in a scored DG.
MCPM is also a generalization of the probabilistic CKY algorithm and the Viterbi algo5Another condition for projectivity, i.e. “no arc covers top node” is equivalent to the crossing arc constraint if special root node, which is a governor of top node, is introduced at the top (or end) of a sentence. 362

