<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>M Adda-Decker</author>
</authors>
<title>De la reconnaissance automatique de la parole à l’analyse linguistique de corpus oraux</title>
<date>2006</date>
<booktitle>In Proceedings of JEP</booktitle>
<contexts>
<context>(LM) and contextdependent acoustic phone models. Among the frequent words, et and est are the most error-prone items: 25% of et “and” and 20% of est “is” (verb “to be”) occurrences are misrecognized (Adda-Decker, 2006). 3. Automatic transcription errors Several reasons may be enumerated for ASR errors: OOV (out of vocabulary) words, words and word sequences which seldom occur in training data, acoustic confusabili</context>
</contexts>
<marker>Adda-Decker, 2006</marker>
<rawString>Adda-Decker, M. (2006). De la reconnaissance automatique de la parole à l’analyse linguistique de corpus oraux, In Proceedings of JEP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Boersma</author>
<author>D Weenink</author>
</authors>
<title>Praat: doing phonetics by computer</title>
<date>1999</date>
<pages>www.praat.org.</pages>
<contexts>
<context>nal, France Info, Radio-television of Morocco) from the Technolangue-ESTER corpus. Several acoustic and prosodic parameters have been defined and automatically extracted thanks to the Praat software (Boersma and Weenink, 1999) and to the LIMSI automatic speech alignment system (Gauvain et al., 2005). Selected parameters concern duration, fundamental frequency, formants (F1, F2, and F3) and surrounding context (pauses prec</context>
</contexts>
<marker>Boersma, Weenink, 1999</marker>
<rawString>Boersma, P., Weenink, D. (1999-2007). Praat: doing phonetics by computer, www.praat.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Deshmuk</author>
<author>R J Dunca</author>
<author>A Ganapathiraju</author>
<author>J Picone</author>
</authors>
<title>Benchmarking human performance for continous speech recognition, in</title>
<date>1996</date>
<booktitle>Proc. of ICSLP</booktitle>
<marker>Deshmuk, Dunca, Ganapathiraju, Picone, 1996</marker>
<rawString>Deshmuk, N., Dunca, R.J., Ganapathiraju, A., Picone, J. (1996). Benchmarking human performance for continous speech recognition, in Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Galliano</author>
<author>E Geoffrois</author>
<author>D Mostefa</author>
<author>K Choukri</author>
<author>J-F Bonastre</author>
<author>G Gravier</author>
</authors>
<title>The ESTER phase II evaluation campaign for the rich transcription of French broadcast news</title>
<date>2005</date>
<booktitle>In Proceedings of EUROSPEECH, Lisbonne</booktitle>
<contexts>
<context>ech recognition system developed for the 2005 ESTER evaluation (Gauvain et al., 2005). This system achieved the best results with an overall performance of 11.9 WER for the speech transcription task (Galliano et al., 2005). The ASR system made use of 4-gram language models (LM) and contextdependent acoustic phone models. Among the frequent words, et and est are the most error-prone items: 25% of et “and” and 20% of es</context>
</contexts>
<marker>Galliano, Geoffrois, Mostefa, Choukri, Bonastre, Gravier, 2005</marker>
<rawString>Galliano, S., Geoffrois, E., Mostefa, D., Choukri, K., Bonastre, J.-F., Gravier, G. (2005). The ESTER phase II evaluation campaign for the rich transcription of French broadcast news, In Proceedings of EUROSPEECH, Lisbonne.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Gauvain</author>
<author>G Adda</author>
<author>M Adda-Decker</author>
<author>A Allauzen</author>
<author>V Gendner</author>
<author>L Lamel</author>
<author>H Schwenk</author>
</authors>
<title>Where Are We in Transcribing French Broadcast News</title>
<date>2005</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<location>Lisbon</location>
<contexts>
<context>ne (French and Moroccan) radio stations. Transcription errors were extracted from the automatic transcriptions produced by the LIMSI speech recognition system developed for the 2005 ESTER evaluation (Gauvain et al., 2005). This system achieved the best results with an overall performance of 11.9 WER for the speech transcription task (Galliano et al., 2005). The ASR system made use of 4-gram language models (LM) and c</context>
<context>us. Several acoustic and prosodic parameters have been defined and automatically extracted thanks to the Praat software (Boersma and Weenink, 1999) and to the LIMSI automatic speech alignment system (Gauvain et al., 2005). Selected parameters concern duration, fundamental frequency, formants (F1, F2, and F3) and surrounding context (pauses preceding/following the target word). For pitch and formant values, measures h</context>
</contexts>
<marker>Gauvain, Adda, Adda-Decker, Allauzen, Gendner, Lamel, Schwenk, 2005</marker>
<rawString>Gauvain, J.L., Adda, G., Adda-Decker M., Allauzen A., Gendner V., Lamel, L., Schwenk, H. (2005). Where Are We in Transcribing French Broadcast News? In Proceedings of Interspeech, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gravier</author>
<author>J-F Bonastre</author>
<author>E Geoffrois</author>
<author>S Galliano</author>
<author>Mc Tait</author>
<author>K Choukri</author>
<author>K</author>
</authors>
<title>ESTER, une campagne d’´evaluation des systèmes d’indexation automatique d’émissions radiophoniques en français</title>
<date>2004</date>
<booktitle>In Proceedings of JEP-TALN</booktitle>
<marker>Gravier, Bonastre, Geoffrois, Galliano, Tait, Choukri, K, 2004</marker>
<rawString>Gravier, G., Bonastre, J.-F., Geoffrois, E., Galliano, S., Mc Tait, K., Choukri, K. (2004). ESTER, une campagne d’´evaluation des systèmes d’indexation automatique d’émissions radiophoniques en français, In Proceedings of JEP-TALN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lacheret-Dujour</author>
<author>F Beaugendre</author>
</authors>
<title>La prosodie du français. CNRS</title>
<date>1999</date>
<location>Paris</location>
<marker>Lacheret-Dujour, Beaugendre, 1999</marker>
<rawString>Lacheret-Dujour, A., Beaugendre, F. (1999). La prosodie du français. CNRS, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Lippmann</author>
</authors>
<title>Speech recognition by machines and humans: benchmarking human performance for continous speech recognition</title>
<date>2003</date>
<booktitle>In Speech communication</booktitle>
<volume>22</volume>
<pages>1--15</pages>
<marker>Lippmann, 2003</marker>
<rawString>Lippmann, N. (2003). Speech recognition by machines and humans: benchmarking human performance for continous speech recognition, In Speech communication, 22(99): 1-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nemoto</author>
<author>M Adda-Decker</author>
<author>I Vasilescu</author>
</authors>
<title>Fouille de données audio pour la classification automatique de mots homophones</title>
<date>2008</date>
<booktitle>In EGC’2008, Sophia-Antipolis</booktitle>
<location>France</location>
<contexts>
<context>dic parameters to model differences between the two words. In this section we address the matter of the automatic separability of the two words thanks to appropriate acoustic and prosodic attributes (Nemoto et al., 2008). 41 acoustic and prosodic attributes have been selected for the automatic classification. They were chosen in order to model both the target word (intra-phonemic attributes) and its relation to the </context>
</contexts>
<marker>Nemoto, Adda-Decker, Vasilescu, 2008</marker>
<rawString>Nemoto, R., Adda-Decker, M., Vasilescu, I. (2008). Fouille de données audio pour la classification automatique de mots homophones. In EGC’2008, Sophia-Antipolis, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Scharenborg</author>
</authors>
<title>Reaching over the gap: A review of efforts to link human and automatic speech recognition research</title>
<date>2007</date>
<journal>In Speech Communication</journal>
<volume>49</volume>
<pages>336--347</pages>
<marker>Scharenborg, 2007</marker>
<rawString>Scharenborg, O. (2007). Reaching over the gap: A review of efforts to link human and automatic speech recognition research, In Speech Communication, 49(5): 336-347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Selkir</author>
</authors>
<title>The prosodic structure of function words. In Signal to syntax: bootstrapping from speech to grammar in early acquisition. Mahwah, Lawrence Erlbaum</title>
<date>1996</date>
<pages>187--214</pages>
<marker>Selkir, 1996</marker>
<rawString>Selkir, E. (1996). The prosodic structure of function words. In Signal to syntax: bootstrapping from speech to grammar in early acquisition. Mahwah, Lawrence Erlbaum, pp 187-214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Shinozaki</author>
<author>S Furui</author>
</authors>
<title>An assessment of automatic recognition techniques for spontaneous speech in comparison with human performance</title>
<date>2003</date>
<booktitle>Proceedings of ISCA &amp; IEEE Workshop on Spontaneous Speech Processing and Recognition</booktitle>
<marker>Shinozaki, Furui, 2003</marker>
<rawString>Shinozaki, T., Furui, S. (2003). An assessment of automatic recognition techniques for spontaneous speech in comparison with human performance. Proceedings of ISCA &amp; IEEE Workshop on Spontaneous Speech Processing and Recognition.</rawString>
</citation>
</citationList>
</algorithm>

