Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
P06-2124
extraction (Koehn et al., 2003; Chiang, 2007; Galley
J07-2003 N03-1017 P07-2045
(Blunsom et al., 2009) and learning phrase align-
P09-1088
In Model 1 (Brown et al., 1993), each target word
J93-2003
GIZA++ (Och and Ney, 2003) for EM.
J03-1002
model tuning) using Moses (Koehn et al., 2007),
P07-2045
2007; Johnson et al., 2007) as well as to other tasks
N07-1018
ing the output of EM (Chiang et al., 2010). Once the
N10-1068
els, such as IBM Models (Brown et al., 1993), HMM
J93-2003
Tugba Bodrumlu, Kevin Knight, and Sujith Ravi. 2009.
W09-1804
ning GIZA++ (Xu et al., 2008) or by local maxi-
C08-1128
(Vogel et al., 1996), and the jointly-trained symmet-
C96-2141
phrase extraction (Koehn et al., 2003), we select for
N03-1017
ments directly (DeNero et al., 2008).
D08-1033
IBM Model 1 was pointed out by Moore (2004) and
P04-1066
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
N06-1014
Robert C. Moore. 2004. Improving IBM word alignment
P04-1066
jointly with segmentation learning in Xu et al.(2008), Nguyen et al. (2010), and Chung and Gildea
C08-1128
ported. Zhao and Xing (2006) note that the param-
P06-2124
Sharon Goldwater and Tom Griffiths. 2007. A fully
P07-1094
ric HMM (Liang et al., 2006), contain a large num-
N06-1014
Franz Josef Och and Hermann Ney. 2003. A system-
J03-1002
Chung and Gildea (2009) apply a sparse Dirichlet
D09-1075
David Chiang. 2007. Hierarchical phrase-based transla-
J07-2003
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
D09-1075
lectors‚Äù (Och and Ney, 2003) and get assigned ex-
J03-1002
