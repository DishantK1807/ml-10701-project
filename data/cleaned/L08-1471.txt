<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>B David</author>
</authors>
<title>The Optimization of Discourse Anaphora</title>
<date>2004</date>
<journal>Linguistics and Philosophy</journal>
<volume>27</volume>
<pages>3--56</pages>
<marker>David, 2004</marker>
<rawString>David, B. (2004) The Optimization of Discourse Anaphora. Linguistics and Philosophy 27(1), pp. 3-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cristea</author>
</authors>
<title>The relationship between discourse structure and referentiality in Veins Theory</title>
<date>2003</date>
<booktitle>glyph817atural Language Processing between Linguistic Inquiry and System Engineering, „Al.I.Cuza</booktitle>
<editor>in W. Menzel and C. Vertan (eds</editor>
<publisher>University Publishing House</publisher>
<location>Iaşi</location>
<contexts>
<context>rence are latent semantic analysis (Dumais et al., 1988), lexical chains (Hirst &amp; St.-Onge, 1997), centering theory (Beaver, 2004), discourse representation theory (Kamp &amp; Reyle, 1993), veins theory (Cristea, 2003), etc. Nevertheless, because of the lack of appropriate tools for Romanian language, we had to choose a quantitative approach for automatically categorizing short Romanian text into coherent /compreh</context>
</contexts>
<marker>Cristea, 2003</marker>
<rawString>Cristea, D. (2003): The relationship between discourse structure and referentiality in Veins Theory, in W. Menzel and C. Vertan (eds.) glyph817atural Language Processing between Linguistic Inquiry and System Engineering, „Al.I.Cuza” University Publishing House, Iaşi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristianini</author>
<author>J Shawe-Taylor</author>
</authors>
<title>An Introduction to Support Vector Machines</title>
<date>2000</date>
<publisher>Cambridge University Press</publisher>
<location>Cambridge, UK</location>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>Cristianini and J. Shawe-Taylor (2000) An Introduction to Support Vector Machines. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software avalable at http://ww.csie.ntu.edu.tw/ cjlin/libsvm</title>
<date>2001</date>
<contexts>
<context>directly on the data, needing no ratios. Its l.o.o. accuracy was 85.12%, with 17.64% higher then the baseline. All machine learning experiments were performed in Matlab, or using Matlab as interface (Chang and Lin, 2001). We summarized these results in the next table. Learning method type Accuracy Regression 67.48% linear Support Vector Classifier 77.34% quadratic Support Vector Machine 81.13% polynomial Kernel Fish</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin (2001) LIBSVM: a library for support vector machines. Software avalable at http://ww.csie.ntu.edu.tw/ cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Chen</author>
<author>P Ender</author>
<author>M Mitchell</author>
<author>C Wells</author>
</authors>
<date>2003</date>
<note>Regression with SPSS http://www.ats.ucla.edu/stat/ spss/webbooks/reg/default.htm</note>
<contexts>
<context>onsists of holding each example out, training on all the other examples and testing on the hold out example. The first and the simplest technique we used was the linear regression (Duda et al., 2001; Chen et al., 2003; Schroeder et al., 1986), not for its accuracy as a classifier, but because, being a linear method, it allows us to analyze the importance of each feature and so determine some of the most prominent </context>
</contexts>
<marker>Chen, Ender, Mitchell, Wells, 2003</marker>
<rawString>Chen, X., Ender, P., Mitchell, M. and Wells, C. (2003). Regression with SPSS http://www.ats.ucla.edu/stat/ spss/webbooks/reg/default.htm .</rawString>
</citation>
<citation valid="true">
<authors>
<author>R O Duda</author>
<author>P E Hart</author>
<author>D G Stork</author>
</authors>
<title>Pattern Classification</title>
<date>2001</date>
<editor>2nd ed.). Wiley-Interscience Publication. Susan T. Dumais, George W. Furnas, Thomas K. Landauer, Scott Deerwester, and Richard Harshman</editor>
<contexts>
<context>one out technique consists of holding each example out, training on all the other examples and testing on the hold out example. The first and the simplest technique we used was the linear regression (Duda et al., 2001; Chen et al., 2003; Schroeder et al., 1986), not for its accuracy as a classifier, but because, being a linear method, it allows us to analyze the importance of each feature and so determine some of </context>
</contexts>
<marker>Duda, Hart, Stork, 2001</marker>
<rawString>Duda, R.O., Hart, P.E., Stork, D.G. (2001) Pattern Classification (2nd ed.). Wiley-Interscience Publication. Susan T. Dumais, George W. Furnas, Thomas K. Landauer, Scott Deerwester, and Richard Harshman.</rawString>
</citation>
<citation valid="true">
<title>Using Latent Semantic Analysis to improve access to textual information</title>
<date>1988</date>
<booktitle>In Human Factors in Computing Systems, in CHI'88 Conference Proceedings (Washington, D.C</booktitle>
<pages>281--285</pages>
<publisher>ACM</publisher>
<location>New York</location>
<marker>1988</marker>
<rawString>(1988) Using Latent Semantic Analysis to improve access to textual information. In Human Factors in Computing Systems, in CHI'88 Conference Proceedings (Washington, D.C.), pages 281285, New York, May. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efron</author>
<author>R J Tibshirani</author>
</authors>
<title>Improvements on cross-validation: the .632+ bootstrap method</title>
<date>1997</date>
<journal>J. Amer. Statist. Assoc</journal>
<pages>92--548</pages>
<contexts>
<context> of 5 types of machines we employed (the other two needed no such pre-processing). Because of the relative small number of examples in our experiment, we used leave one out cross validation (l.o.o.) (Efron &amp; Tibshirani, 1997; Tsuda, 2001), which is considered an almost unbiased estimator of the generalization error. Leave one out technique consists of holding each example out, training on all the other examples and testi</context>
</contexts>
<marker>Efron, Tibshirani, 1997</marker>
<rawString>Efron and R.J. Tibshirani (1997) Improvements on cross-validation: the .632+ bootstrap method. J. Amer. Statist. Assoc, 92:548–560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
<author>David St-Onge</author>
</authors>
<title>Lexical chains as representation of context for the detection and correction of malapropisms</title>
<date>1997</date>
<pages>305--332</pages>
<editor>In Christiane Fellbaum, editor</editor>
<publisher>MIT Press</publisher>
<location>Cambridge</location>
<marker>Hirst, St-Onge, 1997</marker>
<rawString>Hirst, Graeme and David St.-Onge (1997) Lexical chains as representation of context for the detection and correction of malapropisms. In Christiane Fellbaum, editor, Wordnet: An electronic lexical database and some of its applications. MIT Press, Cambridge, pages 305-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lascarides</author>
<author>N Asher</author>
</authors>
<title>Segmented Discourse Representation Theory: Dynamic Semantics with Discourse Structure</title>
<date>2007</date>
<booktitle>Computing Meaning: Volume 3</booktitle>
<pages>87--124</pages>
<editor>in H. Bunt and R. Muskens (eds</editor>
<publisher>Springer</publisher>
<marker>Lascarides, Asher, 2007</marker>
<rawString>Lascarides, A., Asher, N. (2007) Segmented Discourse Representation Theory: Dynamic Semantics with Discourse Structure, in H. Bunt and R. Muskens (eds.) Computing Meaning: Volume 3, pp87--124, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kamp</author>
<author>U Reyle</author>
</authors>
<title>From Discourse to Logic. An Introduction to Modeltheoretic Semantics of glyph817atural Language, Formal Logic and Discourse Representation Theory</title>
<date>1993</date>
<publisher>Kluwer Academic Publishers</publisher>
<location>Dordrecht Netherlands</location>
<contexts>
<context>litative approaches related to coherence are latent semantic analysis (Dumais et al., 1988), lexical chains (Hirst &amp; St.-Onge, 1997), centering theory (Beaver, 2004), discourse representation theory (Kamp &amp; Reyle, 1993), veins theory (Cristea, 2003), etc. Nevertheless, because of the lack of appropriate tools for Romanian language, we had to choose a quantitative approach for automatically categorizing short Romani</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>Kamp, H. and Reyle, U. (1993) From Discourse to Logic. An Introduction to Modeltheoretic Semantics of glyph817atural Language, Formal Logic and Discourse Representation Theory, Kluwer Academic Publishers, Dordrecht Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Miller</author>
</authors>
<title>Essay Assessment with Latent Semantic Analysis</title>
<date>2004</date>
<journal>Journal of Educational Computing Research</journal>
<volume>28</volume>
<contexts>
<context>ication, authorship identification), by expressed opinion (opinion mining, sentiment classification), etc. Very few approaches consider the problem of categorizing text by degree of coherence, as in (Miller, 2004). We created a small corpus of representative texts from 6 Romanian alternative manuals. We manually classified the chosen paragraphs from such manuals into two categories: comprehensible/coherent te</context>
</contexts>
<marker>Miller, 2004</marker>
<rawString>T. Miller (2004) Essay Assessment with Latent Semantic Analysis. Journal of Educational Computing Research 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mika</author>
<author>G Rätsch</author>
<author>B Schölkopf J Weston</author>
<author>K-R Müller</author>
</authors>
<title>Fisher discriminant analysis with kernels</title>
<date>1999</date>
<booktitle>glyph817eural glyph817etworks for Signal Processing IX</booktitle>
<pages>41--48</pages>
<editor>In Y.-H. Hu, J. Larsen, E. Wilson, and S. Douglas, editors</editor>
<publisher>IEEE</publisher>
<contexts>
<context>he art machine learning techniques. Next, we tested two kernel methods (Müller et al., 2001; Schölkopf &amp; Smola, 2002): ν support vector machine (Saunders et al., 1998) and Kernel Fisher discriminant (Mika et al., 1999; Mika et al.,2001), both with linear and polynomial kernel. Kernel-based learning algorithms work by embedding the data into a feature space (a Hilbert space), and searching for linear relations in t</context>
</contexts>
<marker>Mika, Rätsch, Weston, Müller, 1999</marker>
<rawString>S. Mika, G. Rätsch, J.Weston, B. Schölkopf, and K.-R. Müller (1999) Fisher  discriminant analysis with kernels. In Y.-H. Hu, J. Larsen, E. Wilson, and S. Douglas, editors, glyph817eural glyph817etworks for Signal Processing IX, pages 41–48. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Smola Mika</author>
<author>B Schölkopf</author>
</authors>
<title>An improved training algorithm for kernel Fisher discriminants. In T</title>
<date>2001</date>
<marker>Mika, Schölkopf, 2001</marker>
<rawString>Mika, A.J. Smola, and B. Schölkopf (2001) An improved training algorithm for kernel Fisher discriminants. In T.</rawString>
</citation>
<citation valid="false">
<booktitle>Proceedings AISTATS 2001</booktitle>
<pages>98--104</pages>
<editor>Jaakkola and T. Richardson, editors</editor>
<location>San Francisco, CA</location>
<marker></marker>
<rawString>Jaakkola and T. Richardson, editors, Proceedings AISTATS 2001, pages 98–104, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-R Müller</author>
<author>S Mika</author>
<author>G Rätsch</author>
<author>K Tsuda</author>
<author>B Schölkopf</author>
</authors>
<title>An introduction to kernel-based learning algorithms</title>
<date>2001</date>
<journal>IEEE Transactions on glyph817eural glyph817etworks</journal>
<volume>12</volume>
<pages>2--181</pages>
<contexts>
<context>e linear regression to analyze the importance of different features in the discrimination process and as baseline for state of the art machine learning techniques. Next, we tested two kernel methods (Müller et al., 2001; Schölkopf &amp; Smola, 2002): ν support vector machine (Saunders et al., 1998) and Kernel Fisher discriminant (Mika et al., 1999; Mika et al.,2001), both with linear and polynomial kernel. Kernel-based </context>
</contexts>
<marker>Müller, Mika, Rätsch, Tsuda, Schölkopf, 2001</marker>
<rawString>K.-R. Müller, S. Mika, G. Rätsch, K. Tsuda, and B. Schölkopf (2001) An introduction to kernel-based learning algorithms. IEEE Transactions on glyph817eural glyph817etworks, 12 (2):181–201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Saunders</author>
<author>M O Stitson</author>
<author>J Weston</author>
<author>L Bottou</author>
<author>B Schölkopf</author>
<author>A J Smola</author>
</authors>
<title>Support vector machine reference manual</title>
<date>1998</date>
<tech>Technical Report CSD-TR-98-03</tech>
<institution>Royal Holloway University</institution>
<location>London</location>
<contexts>
<context>e discrimination process and as baseline for state of the art machine learning techniques. Next, we tested two kernel methods (Müller et al., 2001; Schölkopf &amp; Smola, 2002): ν support vector machine (Saunders et al., 1998) and Kernel Fisher discriminant (Mika et al., 1999; Mika et al.,2001), both with linear and polynomial kernel. Kernel-based learning algorithms work by embedding the data into a feature space (a Hilb</context>
</contexts>
<marker>Saunders, Stitson, Weston, Bottou, Schölkopf, Smola, 1998</marker>
<rawString>C. Saunders, M.O. Stitson, J. Weston, L. Bottou, B. Schölkopf, and A.J. Smola (1998) Support vector machine reference manual. Technical Report CSD-TR-98-03, Royal Holloway University, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Schölkopf</author>
<author>A J Smola</author>
</authors>
<title>Learning with Kernels</title>
<date>2002</date>
<publisher>MIT Press</publisher>
<location>Cambridge, MA</location>
<contexts>
<context>o analyze the importance of different features in the discrimination process and as baseline for state of the art machine learning techniques. Next, we tested two kernel methods (Müller et al., 2001; Schölkopf &amp; Smola, 2002): ν support vector machine (Saunders et al., 1998) and Kernel Fisher discriminant (Mika et al., 1999; Mika et al.,2001), both with linear and polynomial kernel. Kernel-based learning algorithms work </context>
</contexts>
<marker>Schölkopf, Smola, 2002</marker>
<rawString>Schölkopf and A.J. Smola (2002) Learning with Kernels. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Larry D Schroeder</author>
<author>David L Sjoquist</author>
<author>E Paula</author>
</authors>
<marker>Schroeder, Sjoquist, Paula, </marker>
<rawString>Schroeder, Larry D., David L. Sjoquist, and Paula E.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan</author>
</authors>
<title>Understanding regression analysis: An introductory guide. Thousand Oaks, CA: Sage Publications. Series: Quantitative Applications</title>
<date>1986</date>
<booktitle>in the Social Sciences, glyph817o. 57 John S. Taylor and Nello Cristianini</booktitle>
<publisher>Cambridge University Press</publisher>
<location>New York, NY, USA</location>
<marker>Stephan, 1986</marker>
<rawString>Stephan (1986) Understanding regression analysis: An introductory guide. Thousand Oaks, CA: Sage Publications. Series: Quantitative Applications in the Social Sciences, glyph817o. 57 John S. Taylor and Nello Cristianini (2004) Kernel Methods for Pattern Analysis. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tsuda</author>
<author>G Rätsch</author>
<author>S Mika</author>
<author>K-R Müller</author>
</authors>
<title>Learning to predict the leave-oneout error of kernel based classifiers. In</title>
<date>2001</date>
<marker>Tsuda, Rätsch, Mika, Müller, 2001</marker>
<rawString>K. Tsuda, G. Rätsch, S. Mika, and K.-R. Müller (2001) Learning to predict the leave-oneout error of kernel based classifiers. In G. Dorffner, H. Bischof, and K.</rawString>
</citation>
</citationList>
</algorithm>

