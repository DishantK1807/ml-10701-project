This would cause the second term on the right to increase with the sentence number, and since H(Xi|Ci,Li) must remain constant (by our assumption), the first term should increase with sentence number, and it had been shown to do so (Genzel and Charniak, 2002).
P02-1026
It is possible to distinguish between a set of earlier sentences and a set of later sentences without any direct comparison by computing certain local statistics of individual sentences, such as their entropy (Genzel and Charniak, 2002).
P02-1026
Setup We use the whole Penn Treebank corpus (Marcus et al., 1993) as our data set.
J93-2004
We observe, however, that the effect is smaller than reported in our previous work (Genzel and Charniak, 2002) for Wall Street Journal articles.
P02-1026
In our previous work (Genzel and Charniak, 2002) we propose that entropy rate is indeed constant in human communications.
P02-1026
2 Within-Paragraph Effects 2.1 Implications of Entropy Rate Constancy Principle We have previously demonstrated (see Genzel and Charniak (2002) for detailed derivation) that the conditional entropy of the ith word in the sentence (Xi), given its local context Li (the preceding words in the same sentence) and global context Ci (the words in all preceding sentences) can be represented as H(Xi|Ci,Li) = H(Xi|Li)âˆ’I(Xi,Ci|Li) where H(Xi|Li) is the conditional entropy of the ith word given local context, and I(Xi,Ci|Li) is the conditional mutual information between the ith word and out-of-sentence context, given the local context.
P02-1026
