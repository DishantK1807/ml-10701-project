Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives.
N03-1020 W02-0406
Radev et al.(2003) also exploits relative importance of information.
P03-1048
n average r s # pyramids 1 0.41 15 2 0.65 30 3 0.77 30 4 0.87 15 5 1.00 3 Table 4: Spearman correlation coefficient average for pyramids of order n 5 3.4 Rank-correlation with unigram overlap scores Lin and Hovy (2003) have shown that a unigram cooccurrence statistic, computed with stop words ignored, between a summary and a set of models can be used to assign scores for a test suite that highy correlates with the scores assigned by human evaluators at DUC.
N03-1020
However, in contrast to translation, where the evaluation criterion can be defined fairly precisely it is difficult to elicit stable human judgments for summarization (Rath et al., 1961) (Lin and Hovy, 2002).
W02-0406
In machine translation, the rankings from the automatic BLEU method (Papineni et al., 2002) have been shown to correlate well with human evaluation, and it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003).
N03-1020 P02-1040
Additionally, the task of determining the percentage overlap between two text units turns out to be difficult to annotate reliably â€“ (Lin and Hovy, 2002) report that humans agreed with their own prior judgment in only 82% of the cases.
W02-0406
