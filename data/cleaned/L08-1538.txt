<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>S F Adafre</author>
<author>M de Rijke</author>
<author>E T K Sang</author>
</authors>
<title>Entity Retrieval</title>
<date>2007</date>
<booktitle>In Proceedings of RANLP</booktitle>
<location>Bulgaria</location>
<marker>Adafre, de Rijke, Sang, 2007</marker>
<rawString>S. F. Adafre, M. de Rijke, and E. T. K. Sang. 2007. Entity Retrieval. In Proceedings of RANLP, Bulgaria, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ahn</author>
<author>J Bos</author>
<author>J R Curran</author>
<author>D Kor</author>
<author>M Nissim</author>
<author>B Webber</author>
</authors>
<title>Question Answering with QED at TREC-2005</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th Text Retrieval Conference (TREC-14</booktitle>
<publisher>NIST</publisher>
<location>Gaithersburg, USA</location>
<contexts>
<context>similarities or occur in different sentences that are partially or totally semantically equivalent. The latter case requires a system to deal with the difficulties of semantic analysis. For example, (Ahn et al., 2005) uses lexical similarities among sentences to expand the initial candidate lists. It actually exploits the idea that sentences containing answer instances share similar words. We hypothesized that th</context>
<context>factoid question answering system. List questions are also answered by exploiting the relationships between each pair of answers and/or relationships between question terms and answers. For example, (Ahn et al., 2005) and (Kor, 2005) propose an approach that identifies the common context shared by two or more candidate answers and uses this common context to expand the candidate list. To validate and identify rel</context>
</contexts>
<marker>Ahn, Bos, Curran, Kor, Nissim, Webber, 2005</marker>
<rawString>K. Ahn, J. Bos, J. R. Curran, D. Kor, M. Nissim, and B. Webber. 2005. Question Answering with QED at TREC-2005. In Proceedings of the 14th Text Retrieval Conference (TREC-14), Gaithersburg, USA, November. NIST.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Furnas</author>
</authors>
<title>Faculty profile: George furnas, university of michigan, school of information. [Online</title>
<date>2008</date>
<note>accessed 01April-2008</note>
<contexts>
<context> Statistical Semantics defined as the study of “how the statistical patterns of human word usage can be used to figure out what people mean, at least to a level sufficient for information access” by (Furnas, 2008). Following this view, we hypothesized that the instances of the answer to a list question tend to co-occur within the sentences of the documents related to the target and the question. In addition, </context>
</contexts>
<marker>Furnas, 2008</marker>
<rawString>George Furnas. 2008. Faculty profile: George furnas, university of michigan, school of information. [Online; accessed 01April-2008].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Ghahramani</author>
<author>K A Heller</author>
</authors>
<title>Bayesian Sets</title>
<date>2005</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS</booktitle>
<contexts>
<context>a topic in natural language and also a List Completion task, to complete a partial list of answers, given a topic text and a number of examples. The List Completion task is inspired by Google Sets3. (Ghahramani and Heller, 2005) and (Adafre et al., 2007) use several approaches based on these tasks to expand and validate the candidate list. 3. Our Overall Approach This section describes our approach to answering list questio</context>
</contexts>
<marker>Ghahramani, Heller, 2005</marker>
<rawString>Z. Ghahramani and K. A. Heller. 2005. Bayesian Sets. In Advances in Neural Information Processing Systems (NIPS), December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zelling Harris</author>
</authors>
<title>The Structure of Language, chapter Distributional structure</title>
<date>1954</date>
<pages>33--49</pages>
<publisher>Prentice-Hall</publisher>
<contexts>
<context>s section describes our approach to answering list questions. Our approach is based on Distributional Hypothesis, which states that words occurring in the same contexts tend to have similar meanings (Harris, 1954). DistributionalHypothesis is the basis of Statistical Semantics defined as the study of “how the statistical patterns of human word usage can be used to figure out what people mean, at least to a le</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zelling Harris, 1954. The Structure of Language, chapter Distributional structure, pages 33–49. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ko</author>
<author>L Si</author>
<author>E Nyberg</author>
</authors>
<title>A Probabilistic Graphical Model for Joint Answer Ranking in Question Answering</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<location>Amsterdam</location>
<contexts>
<context>rom a list of extracted candidates, several approaches have been used. Some systems use WordNet, gazetteers or ontologies for this purpose ( e.g. (Moldovan et al., 2003) and (Xu et al., 2002)) while (Ko et al., 2007) uses a probabilistic graphical model. INEX 2007 2 proposed an Entity Ranking track to reduce the difficulty of answering list questions. This track con2“INitiative for the Evaluation of XML retrieva</context>
</contexts>
<marker>Ko, Si, Nyberg, 2007</marker>
<rawString>J. Ko, L. Si, and E. Nyberg. 2007. A Probabilistic Graphical Model for Joint Answer Ranking in Question Answering. In Proceedings of SIGIR, Amsterdam, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Kor</author>
</authors>
<title>Improving answer precision and recall of list questions</title>
<date>2005</date>
<tech>Master’s thesis</tech>
<institution>School of Informatics, University of Edinburgh</institution>
<contexts>
<context>ing system. List questions are also answered by exploiting the relationships between each pair of answers and/or relationships between question terms and answers. For example, (Ahn et al., 2005) and (Kor, 2005) propose an approach that identifies the common context shared by two or more candidate answers and uses this common context to expand the candidate list. To validate and identify relevant answers fr</context>
</contexts>
<marker>Kor, 2005</marker>
<rawString>K. W. Kor. 2005. Improving answer precision and recall of list questions. Master’s thesis, School of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing. The</booktitle>
<publisher>MIT Press</publisher>
<location>Cambridge, Massachusetts</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>D Clark</author>
<author>S Harabagiu</author>
<author>S Maiorano</author>
</authors>
<title>Cogex: A logic prover for question answering</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<location>Edmonton, May-June</location>
<contexts>
<context>ate list. To validate and identify relevant answers from a list of extracted candidates, several approaches have been used. Some systems use WordNet, gazetteers or ontologies for this purpose ( e.g. (Moldovan et al., 2003) and (Xu et al., 2002)) while (Ko et al., 2007) uses a probabilistic graphical model. INEX 2007 2 proposed an Entity Ranking track to reduce the difficulty of answering list questions. This track con</context>
</contexts>
<marker>Moldovan, Clark, Harabagiu, Maiorano, 2003</marker>
<rawString>D. Moldovan, D. Clark, S. Harabagiu, and S. Maiorano. 2003. Cogex: A logic prover for question answering. In Proceedings of HLT-NAACL, Edmonton, May-June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Razmara</author>
<author>A Fee</author>
<author>L Kosseim</author>
</authors>
<title>Concordia University at the TREC 2007 QA track</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th Text Retrieval Conference (TREC-16</booktitle>
<location>Gaithersburg, USA</location>
<marker>Razmara, Fee, Kosseim, 2007</marker>
<rawString>M. Razmara, A. Fee, and L. Kosseim. 2007. Concordia University at the TREC 2007 QA track. In Proceedings of the 16th Text Retrieval Conference (TREC-16), Gaithersburg, USA, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Whittaker</author>
<author>J Novak</author>
<author>P Chatain</author>
<author>S Furui</author>
</authors>
<title>TREC2006 Question Answering Experiments at Tokyo Institute of Technology</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th Text Retrieval Conference (TREC-15</booktitle>
<location>Gaithersburg, USA</location>
<contexts>
<context>jection), which is required at TREC QA, is not dealt with in our system. 2. Related Work Several approaches to answering list questions are applied. Some systems, for example (Zhou et al., 2006) and (Whittaker et al., 2006), treat list questions as an expanded version of factoid questions that requires one answer, as opposed to a list of distinct answers. These systems answer a list question by simply returning the top</context>
</contexts>
<marker>Whittaker, Novak, Chatain, Furui, 2006</marker>
<rawString>E. Whittaker, J. Novak, P. Chatain, and S. Furui. 2006. TREC2006 Question Answering Experiments at Tokyo Institute of Technology. In Proceedings of the 15th Text Retrieval Conference (TREC-15), Gaithersburg, USA, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>A Licuanan</author>
<author>J May</author>
<author>S Miller</author>
<author>R Weischedel</author>
</authors>
<date>2002</date>
<contexts>
<context>entify relevant answers from a list of extracted candidates, several approaches have been used. Some systems use WordNet, gazetteers or ontologies for this purpose ( e.g. (Moldovan et al., 2003) and (Xu et al., 2002)) while (Ko et al., 2007) uses a probabilistic graphical model. INEX 2007 2 proposed an Entity Ranking track to reduce the difficulty of answering list questions. This track con2“INitiative for the E</context>
</contexts>
<marker>Xu, Licuanan, May, Miller, Weischedel, 2002</marker>
<rawString>J. Xu, A. Licuanan, J. May, S. Miller, and R. Weischedel. 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>TREC</author>
</authors>
<title>QA at BBN: Answer Selection and Confidence Estimation</title>
<date>2002</date>
<booktitle>In Proceedings of the 14th Text Retrieval Conference (TREC-11</booktitle>
<publisher>NIST</publisher>
<location>Gaithersburg, USA</location>
<marker>TREC, 2002</marker>
<rawString>TREC 2002 QA at BBN: Answer Selection and Confidence Estimation. In Proceedings of the 14th Text Retrieval Conference (TREC-11), Gaithersburg, USA, November. NIST.</rawString>
</citation>
</citationList>
</algorithm>

