Repair Work in Human-Computer Dialogue Alison Cawsey* Department of Artificial Intelligence, University of Edinburgh, Scotland ajc@uk.ae.ed.aipna Pirkko Raudaskoski English Department, University of Oulu, Finland ekl-pr@finfou.bitnet Abstracl;: If human-computer interaction is to be effective, it is vi~al that there are opportunities to check on understanding, and repair that understanding when it fails.
This paper discusses this idea of repair in human-computer interaction, and provides a number of examples of different types of repair work in an interactive explanation system.
1. Introduction The importance of repair in human interaction is increasingly recognised.
If a dialogue is to proceed smoothly it is vital that there are opportanities for checking understanding and providing clarification when misunderstanding does occur.
Everyday interaction is full of such checks and repairs, though these may be so au~;omatic as to be almost transparent, rarely disturbing ~he flow of the interaction.
In human-computer interaction, providing opportunities for clarification may be even more important.
If the communication is to be robust and effective, then there must be opportunities for both parties to 'repair' the interaction when it fails.
If the user is communicatÂ° ing in natural language (or even in a complex command language) then there are many cases where the system may not 'understand'.
If the system is giving complex instructions or explanations, then there are many cases where the user may not understand.
These checks and repairs have been studied by people working in the field of conversation analysis (CA) for many years.
For example, people have analysed preferences for different types of repair \[7\], and typical sequences of repair moves.
Recently, there has been some interest in checks and repafi" within Cognitive Science (though the approach *Supported by a post-doctoral fellowship from the Science and Engineering Research Council to the subject is often very different from that of CA).
This includes work by Ringle and Bruce \[5\], who analyse checking moves and conversation failure, and Clark and Schaefer \[2\], who have recently proposed a model of dialogae based on contributions rather than single communicative acts.
These are the sections of discourse through which the participants arrive at the mutual knowledge that the conveyed message is understood, and may involve checking and repair work.
Despite the prevalence of checks and repairs in human interaction, there has been very little work within computational linguistics on these essential components of conversation.
The rest of this paper will discuss the problem in more detail, and present some examples of different types of repair work in an implemented interactive explanation system.
2. Repair in Human Interaction In human conversation there are continual implicit acknowledgements that communication is proceeding smoothly.
The speaker is monitoring the hearer in different ways to see if they understand (for example, using checking moves such as 'Do you know what I mean?'), and the 'hearer' is often giving verbal acknowledgement to the speaker (e.g., 'yes', 'uhuh').
If the hearer takes over the conversation, she may acknowledge the last utterance implicitly by, for example, continuing the topic \[2\].
However, if the utterance is not understood, ~ repair may be initiated.
We can examine this repair from several perspectives: Sequencing: A typicalrepair sequence may consist of a repair initiator by the hearer, a repair by the original speaker, and an acknowledgement by the hearer.
However, repair sequences in general may be much more complex.
For example, the speaker may do a third turn 327 repair on realising, from the hearer's response that her original utterance was not understood.
These different types of repair have been discussed in \[7\].
ttepalr Initiators: Repair initiators may take many forms in human interaction, including: facial expression; verbal signals ('huh')? and clarification questions.
In human dialogue, the speaker may frequently selfcorrect without hearer intervention.
Source of Trouble: Communication may break down for many reasons, such as from lack of hearing, reference failure or from general misunderstanding of complex material.
Although the form of the repair initiator may indicate the source of the trouble, this is not always the case.
It may therefore be necessary to guess at the likely source of trouble, possibly using discourse context or assumptions about the hearer's knowledge to reason about likely problems \[3\].
Repair work both relies on, and shapes the context of the interaction.
However, whatever the source of the problem, the basic interactional mechanism is the same \[6\].
3. Example Repair Work In order to illustrate some of these different aspects of repair, this section will give a number of examples of types of repair work in an interactive explanation system (the EDGE system, described further in \[1\]).
These include repairs when the user fails to understand the explanation, as well as repairs when the system fails to understand the user.
These latter are adapted from \[4\].
The EDGE system plans explanations of the behaviour of simple circuits, depending on assumptions about the user's knowledge.
These are interactive, with many opportunities for repair work when the user fails to understand the explanation.
The user input to the system consists of one or two word commands or questions, rather than arbitrary natural language utterances.
However, even with this restricted input there in an obvious need for repair work which addresses the systems lack of 'understanding' as well as the users.
3.1 User
Misunderstandings First, we will illustrate how the system may repair user misunderstandings.
We must consider both how to structure the dialogue, and how to plan the content of a specific repair sequence.
Clarification Questions: Whenever the system pauses the user may ask a clarification question (using a restricted command language).
The system will normally reply to this question, then try and get back to what it was in the middle of explaining.
This is achieved using discourse 'plans' to structure the clarification sub-dialogue, and a simple notion of focus to attempt to resume the previous discussion in such a way that it follows on from the topic introduced by the user.
The following example illustrates this: S: The light detector circuit's components are: A light-dependent-resistor and a fixed-resistor.
U: What-is-a light-dependent-resistor?
S: A light-dependent-resistor is a kind of resistor.
Its function is to provide an output resistance which depends on the input light intensity.
S: Anyway, when its input light intensity is high, its output resistance is quite low....
In this example the system was planning to describe the detailed behaviour of the light detector circuit's components.
Because of the interruption/clarification, the system chooses to first describe the behaviour of the light dependent resistor.
Signalling Misunderstanding: The user may also signal that they are not following without mentioning the exact problem -maybe they don't know why they don't understand.
The system must then 'guess' at the likely source of trouble.
This is done by maintaining a model of the discourse so far which includes any assumptions made about the user's knowledge.
In attempting a repair the system identifies an assumption which may have been mistaken, then tries to fill in missing knowledge or explain something another way.
For example: U: Huh?
S: Don't you know what sort of circuit a lightdetector-circuit is?
U: No.
S: OK, A light-detector-circuit is a kind of potential-divider circuit.
Anyway.. System initiated remediation: Sometimes the system can deduce that the user has a misunderstanding from the user's utterances (e.g., replies to questions).
Then a remediation sequence is initiated by the system using strategies based on work on tutorial dialogues (e.g., \[8\]).
For example: S: What's the output voltage of the light-detectorcircuit?
U: High.
328 S: No, a light-detector-circuit is like a heatdetector.circuit except its output voltage depends on the input \]ight-intentity instead of hearintensity.
So, what's the output voltage of the light-detector-circuit?
'Fhese examples illustrate different ways repairs may be iniated, how repair sequences may be structured w:ithin an ongoing dialogue, and how the system may tress at problems or use standm'd remediation strategies.
3.2 System
~Misunderstandlng ~ Within the dialogue there are also places where the user may ask a question, but the system may not be able to in~;erpret it.
(\['he system must then choose an app,'opri.at~ repair initiator.
In these examples the user's input is in the form of a simple command hmguage --for naturnl language input it is even more important to give helpful repair initiators.
Object mimmderstood: If the object of the question is misunderstood, the repair initiator stmuld direct attention to that ~missing' object: U: What-is-a light-circuit ? S: What.d,~-~ what?
U: light-detector-circuit.
S: OK, A light-detector-circuit is a kind of ..
Ql:mstion type misunderstood: If the question type is misunderstood, attention should be directed to that: U: Whatisa light-detector-circuit ? S: What about the light-detector-circuit?
U: What-is-a light-detector-circuit.
Both misunderstood: If the question is of the right form but both parts are not understood, the system simply says 'what?'.
Wrong form: If the utter~nce is not of a recognisable form (e.g., it cannot be.
'parsed')j the system informs the user of acceptable forms (e.g., question-type questionobj).
Repeated errors: Repair initiators for repeated errors give: further information, such ~ lists of relevant object and question types.
These simple examples illustrate tile importance of nshtg an appropriate repair initiator when the system fails to understand.
This is important for both command and natural language based input.
4. Conclusion This paper has illustrated ~tle importance, and some of the problems of repair work in human-computer dialogues, hnportant issues include repair sequencing, se~ lecting and responding to different repair iniators, and reasoning about the possible source of the problem and helpful 'remediation' strategies.
The example system is fairly simple, though in an evaluation of an early version with menu-based user input, the interactive/repair based approach to explanation generation proved useful.
Future work on any practical natural language diMogue system should consider these issues.
References \[1\] A.
Cawsey. Generating explanatory discom'se.
I11 R.
Dale, C.
Mellish, and M.
Zock, editors, Current Research in Natural Language Generation, Academic Press, 1990.
\[2\] tI.
Clark and E.
Schaefer. Contributing to discourse.
Cognitive Science, 13:259-294, 1989.
\[3\] J.
D. Moore.
A Reactive Approach to Ezplanation in Expert and Advice-Giving Systems.
PhD thesis, Information Sciences Institute, University of Southern California, 1989.
(published as ISI-SR-90-251).
\[4\] P.
Raudaskoski. Repair work in human-computer interaction.
In P.
Luff, D.
Frohlich, and N.
Gilbert, editors, Computers and Conversation, Academic Press, 1990.
\[51 M.
Ringle and B.
Bruce. Conversation failure.
In W.
Lehnert and M.
Ringle, editors, Strategies for Natural Language Processing, Lawrence Earlbaum, Hillsdale, New Jersey, 1981.
\[6\] E.
Schegloff. Some som'ces of misunderstanding in talk-in-interaction.
Lecture to the Cognitive Sciences program, University of California, Berkeley.
(71 E.
Schegloff, G.
Jefferson, and H.
Sacks. The preference for self-correction in the organisation of repair in conversation.
Language, 53:361-382, 1977.
\[8\] B.
Woolf and T.
Murray. A framework for representing tutorial discourse.
In Proceedings of the lOth International Conference on Artificial fntelligence~ pages 189-192, 1987 .

