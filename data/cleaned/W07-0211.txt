TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 73–80, Rochester, April 2007 c©2007 Association for Computational Linguistics DLSITE-2:SemanticSimilarityBasedon Syntactic DependencyTrees Appliedto TextualEntailment DanielMicol, ´OscarFerr´andez,RafaelMu˜noz,and ManuelPalomar NaturalLanguageProcessingand InformationSystemsGroup Departmentof ComputingLanguagesand Systems Universityof Alicante San Vicentedel Raspeig,Alicante03690,Spain {dmicol, ofe, rafael, mpalomar}@dlsi.ua.es Abstract In this paper we attempt to deduce textual entailment based on syntactic dependencytreesofagiventext-hypothesispair.
The goals of this projectare to provide an accurate and fast system, which we have called DLSITE-2, that can be applied in software systems that require a near-realtime interaction with the user.
To accomplish this we use MINIPAR to parse the phrases and construct their corresponding trees.
Later on we apply syntacticbased techniques to calculate the semantic similarity between text and hypothesis.
Tomeasureourmethod’sprecisionwe used the test text corpus set from Second PASCAL Recognising Textual Entailment Challenge(RTE-2),obtaininganaccuracy rate of 60.75%. 1 Introduction There are several methods used to determine textual entailmentfor a given text-hypothesispair.
The one described in this paper uses the information contained in the syntactic dependency trees of such phrases to deduce whether there is entailment or not.
Inaddition,semanticknowledgeextractedfrom WordNet (Miller et al., 1990) has been added to achieve higheraccuracy rates.
It has been proven in several competitions and otherworkshopsthattextualentailmentisacomplex task.
One of these competitionsis PASCAL Recognising Textual Entailment Challenge (Bar-Haim et al.,2006),whereeachparticipatinggroupdevelopsa textual entailmentrecognizingsystemattemptingto accomplishthebestaccuracy rateofallcompetitors.
Such complexity is the reasonwhy we use a combination of various techniques to deduce whether entailmentis produced.
Currently there are few research projects related to the topic discussed in this paper.
Some systems use syntactictree matchingas the textualentailment decision core module, such as (Katrenko and Adriaans, 2006).
It is based on maximalembeddedsyntactic subtrees to analyze the semantic relation between text and hypothesis.
Other systems use syntactic trees as a collaborative module, not being the core, such as (Herreraet al., 2006).
The application discussedin thispaperbelongsto thefirstsetof systems,since syntacticmatchingis its main module.
The remainder of this paper is structured as follows.
In the second section we will describe the methods implementedin our system.
The third one containstheexperimentalresults,andthefourthand last discussessuch resultsand proposesfuture work basedon our actualresearch.
2 Methods
The system we have built aims to provide a good accuracy rate in a short lapse of time, making it feasible to be included in applications that require near-real-timeresponsesduetotheirinteractionwith the user.
Such a system is composed of few modules that behave collaboratively.
These include tree construction,filtering,embeddedsubtreesearchand graphnodematching.
Aschematicrepresentationof the systemarchitectureis shown in Figure1.
73 Figure1: DLSITE-2systemarchitecture.
Each of the steps or modules of DLSITE-2 is described in the following subsections, that are numbered sequentially according to their execution order.
2.1 Tree
generation The first module constructs the corresponding syntactic dependency trees.
For this purpose, MINIPAR (Lin, 1998) output is generatedand afterwards parsed for each text and hypothesis of our corpus.
Phrase tokens, along with their grammatical information, are stored in an on-memory data structure thatrepresentsatree,whichisequivalenttothementionedsyntacticdependency tree.
2.2 Tree
filtering Once the tree has been constructed, we may want to discard irrelevant data in order to reduce our system’s response time and noise.
For this purpose we have generated a database of relevant grammatical categories, represented in Table 1, that will allow us to remove from the tree all those tokens whose category does not belong to such list.
The resulting tree will have the same structureas the original, butwillnotcontainany stopwordsnorirrelevanttokens, such as determinants or auxiliary verbs.
The whole list of ignored grammaticalcategories is representedin Table 2.
We have performed tests taking into account and discardingeachgrammaticalcategory, whichhasallowed us to generate both lists of relevant and ignoredgrammaticalcategories.
Verbs, verbs with one argument, verbs with two arguments, verbs taking clause as complement, verb Have, verb Be Nouns Numbers Adjectives Adverbs Noun-nounmodifiers Table 1: Relevant grammaticalcategories.
2.3 Graphembeddingdetection
The next step of our system consists in determining whether the hypothesis’ tree is embedded into the text’s.
Let us first define the concept of embedded tree (Katrenko and Adriaans,2006).
Definition 1: Embedded tree A tree T1 = (V1,E1) is embedded into another oneT2 = (V2,E2) iff 1.
V1 ⊆V2, and 2.
E1 ⊆E2 where V1 and V2 represent the vertices, andE1 andE2 the edges.
In other words, a tree, T1, is embedded into another one, T2, if all nodes and branches of T1 are presentinT2.
Webelievethatitmakessensetoreducethestrictness of such a definition to allow the appearance of intermediatenodes in the text’s branchesthat are 74 Determiners Pre-determiners Post-determiners Clauses Inflectionalphrases Prepositionand prepositionphrases Specifiersof prepositionphrases Auxiliaryverbs Complementizers Table 2: Ignoredgrammaticalcategories.
notpresentinthecorrespondinghypothesis’branch, whichmeansthat we allow partialmatching.
Therefore, a match between two branches will be produced if all nodes of the first one, namelyθ1 ∈ E1, are presentin the second,namelyθ2 ∈E2, andtheir respective order is the same, allowing the possibility of appearanceof intermediatenodes that are not present in both branches.
This is also described in (Katrenko and Adriaans,2006).
To determinewhether the hypothesis’tree is embedded into the text’s, we perform a top-down matchingprocess.
For thispurposewefirstcompare therootsofbothtrees.
Iftheycoincide,wethenproceed to comparetheir respective child nodes, which are the tokens that have some sort of dependency with their respective root token.
In order to add more flexibility to our system, we do not require the pair of tokens to be exactly the same, but rather set a threshold that representstheminimumsimilarityvaluebetweenthem.
This is a difference between our approach and the one described in (Katrenko and Adriaans, 2006).
Such a similarity is calculated by using the WordNet::Similarity tool (Pedersen et al., 2004), and, concretely, the Wu-Palmer measure, as defined in Equation1 (Wu and Palmer, 1994).
Sim(C1,C2) = 2N3N 1 +N2 +2N3 (1) where C1 and C2 are the synsets whose similarity we want to calculate,C3 is their least common superconcept,N1 is the number of nodes on the path from C1 to C3, N2 is the number of nodes on the path fromC2 toC3, andN3 is the number of nodes on the path from C3 to the root.
All these synsets and distancescan be observed in Figure2.
Figure2: Distancebetweentwo synsets.
If the similarity rate is greater or equal than the establishedthreshold,whichwehavesetempirically to 80%, we will considerthe correspondinghypothesis’ token as suitable to have the same meaning as the text’s token, and will proceed to compare its child nodes in the hypothesis’ tree.
On the other hand, if such similarity value is less than the corresponding threshold, we will proceed to compare the children of such text’s tree node with the actual hypothesis’node that was beinganalyzed.
The comparison between the syntactic dependency treesof bothtext and hypothesiswill be completed when all nodes of either tree have been processed.
If we have been able to find a match for all the tokens within the hypothesis, the corresponding treewillbe embeddedintothetext’s andwewillbelieve that there is entailment.
If not, we will not be able to assure that such an implication is produced and will proceed to execute the next module of our system.
Next, we will present a text-hypothesispair sample where the syntactic dependency tree of the hypothesis (Figure 3(b)) is embedded into the text’s (Figure 3(a)).
The mentioned text-hypothesis pair is the following: Text: Mossad is one of the world’s most well-known intelligence agencies, and is oftenviewedinthesameregardastheCIA and MI6.
Hypothesis: Mossad is an intelligence agency.
75 (a) Mossadisoneoftheworld’smostwell-knownintelligenceagencies,andisoftenviewed in the same regard as the CIA and MI6.
(b) Mossad is an intelligence agency.
Figure3: Representationof a hypothesis’syntacticdependencytree that is embeddedinto the text’s.
As one can see in Figure 3, the hypothesis’ syntacticdependency treerepresentedis embeddedinto the text’s because all of its nodes are present in the text in the same order.
There is one exception though, that is the word an.
However, since it is a determinant, the filtering module will have deleted it before the graph embedding test is performed.
Therefore, in this example the entailment would be recognized.
2.4 Graphnodematching
Once the embedded subtree comparison has finished,andif itsresultis negative, weproceedto perform a graph node matching process, termed alignment,betweenboththetextandthehypothesis.
This operation consists in finding pairs of tokens in both treeswhoselemmasareidentical,nomatterwhether they are in the same position within the tree.
We would like to point out that in this step we do not use the WordNet::Similaritytool.
Some authors have already designed similar matching techniques, such as the ones described in (MacCartney et al., 2006) and (Snow et al., 2006).
However, theseincludesemanticconstraintsthatwe have decided not to consider.
The reason of this decision is that we desired to overcome the textual entailmentrecognitionfromanexclusivelysyntactic perspective.
Therefore,we did not want this module to includeany kind of semanticknowledge.
The weight given to a token that has been found in bothtreeswilldependon the depth in the hypothesis’ tree and the token’s grammatical relevance.
The first of thesefactorsdependson an empiricallycalculated weight that assigns less importance to a node the deeperit is locatedin the tree.
This weight is defined in Equation 2.
The second factor gives different relevance depending on the grammatical category and relationship.
For instance, a verb will have the highest weight, while an adverb or an adjective willhave lessrelevance.
Thevaluesassigned to each grammatical category and relationship are also empirically-calculatedand are shown in Tables 3 and 4, respectively.
Grammaticalcategory Weight Verbs,verbswithoneargument,verbs with two arguments, verbs taking clauseas complement 1.0 Nouns,numbers 0.75 Be used as a linkingverb 0.7 Adjectives, adverbs, noun-nounmodifiers 0.5 Verbs Have and Be 0.3 Table 3: Weights assigned to the grammaticalcategories.
76 Grammaticalrelationship Weight Subject of verbs, surface subject, object of verbs, second object of ditransitive verbs 1.0 The rest 0.5 Table 4: Weights assigned to the grammaticalrelationships.
Let τ and λ represent the text’s and hypothesis’ syntactic dependency trees, respectively.
We assumewehavefoundmembersofasynset,namelyβ, presentin bothτ andλ.
Now letγ be the weight assignedtoβ’sgrammaticalcategory(definedinTable 3),σtheweightofβ’sgrammaticalrelationship(defined in Table 4), µ an empirically-calculatedvalue that represents the weight difference between tree levels, and δβ the depth of the node that contains the synset β in λ.
We define the function φ(β) as representedin Equation2.
φ(β) =γ·σ·µ−δβ (2) The value obtained by calculating the expression of Equation 2 would represent the relevance of a synset in our system.
The experiments performed reveal that the optimalvalue forµis 1.1.
For a given pair (τ, λ), we define the setξ as the one that containsthe synsetspresentin both trees: ξ =τ ∩λ ∀α∈τ,β ∈λ (3) Therefore,thesimilarityratebetweenτ andλ,denotedby the symbolψ, would be definedas: ψ(τ,λ) = summationdisplay ν∈ξ φ(ν) (4) One should note that a requirement of our system’ssimilaritymeasurewouldbetobeindependent of the hypothesis length.
Thus, we must define the normalizedsimilarityrate, as shown in Equation5.
ψ(τ,λ) = ψ(τ,λ)summationdisplay β∈λ φ(β) = summationdisplay ν∈ξ φ(ν) summationdisplay β∈λ φ(β) (5) Once the similarity value, ψ(τ,λ), has been calculated,it will be provided to the user togetherwith the correspondingtext-hypothesis pair identifier.
It will be his responsibility to choose an appropriate thresholdthatwillrepresentthe minimumsimilarity rate to be consideredas entailmentbetweentext and hypothesis.
All values that are under such a threshold will be marked as not entailed.
For this purpose, we suggest using a development corpus in order to obtain the optimal threshold value, as it is done in the RTE challenges.
3 Experimentalresults
The experimental results shown in this paper were obtained processing a set of text-hypothesis pairs from RTE-2.
The organizers of this challenge provide development and test corpora to the participants, both of them containing 800 pairs manually annotated for logical entailment.
It is composed of four subsets, each of them corresponding to typical true and false entailmentsin different tasks,such as InformationExtraction(IE), Information Retrieval (IR), Question Answering (QA), and Multi-document Summarization (SUM).
For each task,theannotatorsselectedthesameamountoftrue entailmentsas negative ones (50%-50%split).
The organizershave also definedtwo measuresto evaluate the participating systems.
All judgments returned by the systems will be compared to those manually assigned by the human annotators.
The percentage of matching judgments will provide the accuracy of the system, i.e. the percentage of correct responses.
As a second measure, the average precisionwill be computed.
This measureevaluates the ability of the systems to rank all the pairs in the corpus according to their entailment confidence, in decreasingorderfromthemostcertainentailmentto the least.
Average precisionis a commonevaluation measureforsystemrankingsthatisdefinedasshown in Equation6.
AP = 1R nsummationdisplay i=1 E(i)#correct up to pair ii (6) wherenis theamountof thepairsin thetestcorpus, Ris the total numberof positive pairs in it,iranges over the pairs, orderedby their ranking,andE(i) is definedas follows: 77 E(i) =   1 if thei−thpair is positive, 0 otherwise.
(7) As we previously mentioned, we tested our system against RTE-2 development corpus, and used the test one to evaluateit.
First, Table 5 shows the accuracy (ACC) and averageprecision(AP),bothas a percentage,obtained processingthe development corpus from RTE-2 for a threshold value of 68.9%, which corresponds to the highest accuracy that can be obtained using our system for the mentioned corpus.
It also provides the rate of correctly predicted true and false entailments.
Task ACC AP TRUE FALSE IE 52.00 51.49 54.00 50.00 IR 55.50 58.99 32.00 79.00 QA 57.50 54.72 53.00 62.00 SUM 65.00 81.35 39.00 91.00 Overall 57.50 58.96 44.50 70.50 Table 5: Results obtained for the development corpus.
Next, let us show in Table 6 the results obtained processing the test corpus, which is the one used to comparethe differentsystemsthat participatedin RTE-2,with the same thresholdas before.
Task ACC AP TRUE FALSE IE 50.50 47.33 75.00 26.00 IR 64.50 67.67 59.00 70.00 QA 59.50 58.16 80.00 39.00 SUM 68.50 75.86 49.00 88.00 Overall 60.75 57.91 65.75 55.75 Table 6: Resultsobtainedfor the test corpus.
As one can observe in the previous table, our system provides a high accuracy rate by using mainly syntactical measures.
The number of texthypothesis pairs that succeeded the graph embedding evaluation was three for the development corpus and one for the test set, whichreflectsthe strictness of such module.
However, we would like to point out that the amount of pairs affected by the mentioned module will depend on the corpus nature, so it can vary significantly between different corpora.
Let us now compareour resultswiththe onesthat were achieved by the systems that participated in RTE-2.
One should note that the criteria for such ranking is based exclusively on the accuracy, ignoring the average precision value.
In addition, each participating group was allowed to submit two different systems to RTE-2.
We will consider here the bestresultofbothsystemsforeachgroup.
Thementionedcomparisonis shown in Table7, andcontains only the systemsthat had higheraccuracy rates than our approach.
Participant Accuracy (Hicklet al., 2006) 75.38 (Tatu et al., 2006) 73.75 (Zanzottoet al., 2006) 63.88 (Adams,2006) 62.62 (Bos and Markert, 2006) 61.62 DLSITE-2 60.75 Table 7: Comparisonof some of the teams that participatedin RTE-2.
As it is reflected in Table 7, our system would have obtained the sixth position out of twenty-four participants, which is an accomplishmentconsidering the limitednumberof resourcesthat it has builtin.
Since one of our system’s modules is based on (Katrenko and Adriaans, 2006), we will compare their results with ours to analyze whetherthe modifications we introduced perform correctly.
In RTE2, they obtained an accuracy rate of 59.00% for the test corpus.
The reason why we believe we have achieved better results than their system is due to the fact that we added semantic knowledge to our graph embeddingmodule.
In addition,the syntactic dependency trees to which we have applied such a module have been previously filtered to ensure that they do not contain irrelevant words.
This reduces the system’s noise and allows us to achieve higher accuracy rates.
In the introduction of this paper we mentioned that one of the goals of our system was to provide 78 a high accuracy rate in a short lapse of time.
This is one of the reasonswhy we choseto constructa light systemwhereone of the aspectsto minimizewas its response time.
Table 8 shows the execution times1 ofoursystemforbothdevelopmentandtesttextcorpora from RTE-2.
These include total and average2 responsetimes.
Development Test Total 1045 1023 Average 1.30625 1.27875 Table 8: DLSITE-2responsetimes(in seconds).
As we can see, accurate results can be obtained using syntactic dependency trees in a short lapse of time.
However, there are some limitations that our system does not avoid.
For instance, the tree embedding test is not applicable when there is no verb entailment.
This is reflectedin the following pair: Text: Tony Blair, the BritishPrimeMinister, met JacquesChirac in London.
Hypothesis: Tony Blair is the British PrimeMinister.
The root node of the hypothesis’ tree would be the one correspondingto the verb is.
Since the entailment here is implicit, there is no need for such a verb to appearin the text.
However, this is not compatible with our system, since is would not match any node of the text’s tree, and thus the hypothesis’ tree would not be foundembeddedinto the text’s.
The graph matching process would not behave correctlyeither.
This is due to the fact that the main verb, which has the maximum weight because it is the root of the hypothesis’ tree and its grammatical category has the maximumrelevance, is not present inthetext,sotheoverallsimilarityscorewouldhave a considerablehandicap.
The example of limitation of our system that we have presented is an apposition.
To avoid this specifickindofsituationsthatproduceanundesiredbehavior in our system, we could add a preprocessing modulethat transformsthe phrasesthat have the 1Themachineweusedtomeasuretheresponsetimeshadan Intel Core 2 Duo processorat 2GHz.
2Average response times are calculated diving the totals by the numberof pairs in the corpus.
structureX,Y,ZintoXisY,andZ.
Fortheshown example,the resultingtext and hypothesiswould be as follows: Text: TonyBlairis theBritishPrimeMinister, and met JacquesChirac in London.
Hypothesis: Tony Blair is the British PrimeMinister.
The transformed text would still be syntactically correct, and the entailmentwould be detected since the hypothesis’syntacticdependency tree is embedded into the text’s.
4 Conclusionsand
future work Theexperimentalresultsobtainedfromthisresearch demonstrate that it is possible to apply a syntacticbased approachto deduce textual entailmentfrom a text-hypothesis pair.
We can obtain good accuracy rates using the discussedtechniqueswith very short response times, which is very useful for assisting different kinds of tasks that demand near-real-time responsesto user interaction.
Thebaselineweset foroursystemwas to achieve betterresultsthantheonesweobtainedwithourlast participationin RTE-2.
As it is stated in (Ferr´andez et al., 2006), the maximumaccuracy value obtained by then was 55.63% for the test corpus.
Therefore, our systemis 9.20% more accuratecomparedto the one that participated in RTE-2, which represents a considerableimprovement.
Theauthorsof thispaperbelieve thatif higheraccuracyratesaredesired,astep-basedsystemmustbe constructed.
This would have several preprocessing units,suchas negationdetectors,multi-wordassociators and so on.
The addition of these units would definitely increase the response time preventing the systemfrom beingused in real-timetasks.
Future work can be related to the cases where no verb entailment is produced.
For this purpose we propose to extract a higher amount of semantic informationthat wouldallow us to constructa characterizedrepresentationbasedontheinputtext,sothat we can deduce entailmenteven if there is no apparent structuresimilaritybetweentext and hypothesis.
This would mean to create an abstract conceptualization of the information containedin the analyzed phrases, allowing us to deduce ideas that are not 79 explicitly mentioned in the parsed text-hypothesis pairs.
In addition, the weights and thresholds defined in our system have been established empirically.
It would be interesting to calculate those values by means of a machine learning algorithm and compare them to the ones we have obtainedempirically.
Some authorshave alreadyperformedthis comparison,beingoneexampletheworkdescribedin (MacCartney et al., 2006).
Acknowledgments The authors of this paper would like to thank professors Borja Navarro and Rafael M.
Terol for their help and criticalcomments.
This research has been supported by the undergraduateresearchfellowshipsfinancedby the Spanish Ministry of Education and Science, the project TIN2006-15265-C06-01financed by such ministry, and the project ACOM06/90 financed by the Spanish GeneralitatValenciana.
References Rod Adams.
2006. Textual Entailment Through Extended Lexical Overlap.
In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,Venice,Italy.
RoyBar-Haim,IdoDagan,BillDolan,LisaFerro,Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The Second PASCAL Recognising Textual Entailment Challenge.
In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,Venice,Italy.
JohanBos,andKatjaMarkert.
2006. When logical inference helps determining textual entailment (and when it doesnt).
In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice,Italy.
´Oscar Ferr´andez, Rafael M.
Terol, Rafael Mu˜noz, Patricio Mart´ınez-Barco, and Manuel Palomar.
2006. An approach based on Logic Forms and Word Net relationships to Textual Entailment performance.
In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice,Italy.
Jes´us Herrera, Anselmo Pe˜nas, ´Alvaro Rodrigo, and Felisa Verdejo.
2006. UNED at PASCAL RTE-2 Challenge.
In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice,Italy.
Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts, Bryan Rink, and Ying Shi.
2006. Recognizing Textual Entailment with LCC’s GROUNDHOG System.
In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice,Italy.
Sophia Katrenko, and Pieter Adriaans.
2006. Using Maximal Embedded Syntactic Subtrees for Textual Entailment Recognition.
In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,Venice,Italy.
Dekang Lin.
1998. Dependency-based Evaluation of MINIPAR.
In Workshop on the Evaluation of Parsing Systems,Granada,Spain.
Bill MacCartney, Trond Grenager, Marie-Catherine de Marneffe, Daniel Cer, and Christopher D.
Manning. 2006.
Learning to recognize features of valid textual entailments.
In Proceedings of the North American Association of Computational Linguistics (NAACL06),New York City, New York, United States of America.
George A.
Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J.
Miller. 1990.
Introduction to WordNet: An On-line Lexical Database.
International Journal of Lexicography 1990 3(4):235-244.
Ted Pedersen, Siddhart Patwardhan, and Jason Michelizzi.
2004. WordNet::SimilarityMeasuring the Relatedness of Concepts.
In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL-04), Boston, Massachussets, United States of America.
Rion Snow, Lucy Vanderwende, and Arul Menezes.
2006. Effectively using syntax for recognizing false entailment.
In Proceedings of the North American Association of Computational Linguistics (NAACL-06), New York City, New York, United Statesof America.
Marta Tatu, Brandon Iles, John Slavick, Adrian Novischi, and Dan Moldovan.
2006. COGEX at the Second Recognizing Textual Entailment Challenge.
In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice,Italy.
Zhibiao Wu, and Martha Palmer.
1994. Verb Semantics and LexicalSelection.
In Proceedingsof the 32nd Annual Meeting of the Associations for Computational Linguistics, pages 133-138, Las Cruces, New Mexico, United States of America.
Fabio M.
Zanzotto, Alessandro Moschitti, Marco Pennacchiotti, and Maria T.
Pazienza. 2006.
Learning textual entailment from examples.
In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy. 80

