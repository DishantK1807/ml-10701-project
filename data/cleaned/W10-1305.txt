Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 37–43,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics
A Platform for Automated Acoustic Analysis for Assistive Technology 
 
 
Suzanne Boyce Harriet Fel 
Department of Comunication Sciences and 
Disorders 
Colege of Computer and Information 
Science 
University of Cincinati Northeastern University 
Cincinati, Ohio, 45267, USA Boston, Massachusets, 0215, USA 
boycese@ucmail.uc.edu fell@ccs.neu.edu 
  
Joel MacAuslan Lorin Wilde 
Spech Technology and Aplied Research Boston University 
54 Middlesex Turnpike  
Bedford, assachusetts,  , USA Boston, Masachusets, 02180, USA 
joelm@staranalyticalservices.com wildercom@gmail.com 
 
 
 
 
Abstract 
The use of spech production data has ben 
limited by a steep learning curve and the ned 
for laborious hand measurement. We are 
building a tool set that provides sumary sta-
tistics for measures designed by clinicians to 
scren, diagnose or provide training to assis-
tive technology users.  This will be achieved 
by extending an existing shareware software 
platform with “plug-ins” that perform specific 
measures and report results to the user. The 
comon underlying basis for this tool set is a 
Stevens’ paradigm of landmarks, points in an 
utterance around which information about ar-
ticulatory events can be extracted. 
1 Introduction

 
To date, the use of speech production data has been 
limited by a steep learning curve and the ned for 
laborious hand measurement. Many speech-related 
studies result in voluminous acoustic data. Many 
clinicians who design and use assistive technology 
would like to incorporate acoustic analysis, but 
have been discouraged because of these technical 
challenges. We are in the process of developing a 
set of tols that considerably streamlines the proc-
ess of analyzing speech production details. 
 
We are building a tol set to provide sumary sta-
tistics for measures designed by clinicians to 
scren, diagnose or provide training to patients. 
This wil be achieved by extending an existing 
shareware software platform with “plug-ins” that 
perform specific measures and report results to the 
user. At present, our goal is to use the existing 
shareware software tol Wavesurfer (Wavesurfer, 
2005). The new modules wil be set up to report 
data from a single audio file, or groups of audio 
files in a standard table format, for easy input to 
statistical or other analysis software. For example, 
the data may be imported into a program that cor-
relates spech data with scalp electrode and medi-
cation data.  
 
Our tol wil include alternative and independently 
tested algorithms for clinically relevant measures, 
as well as guidance as to what the speech data may 
mean. 
 
The comon underlying basis for this tol set is a 
focused set of landmarks derived from Stevens’ 
Lexical Aces from Features (LAF) paradigm 
(Stevens, 192, 202; Liu, 195; Slifka et al., 
2004).  In this approach, landmarks are points in an 
utterance around which information about articula-
tory events can be extracted.   
 
37
In what folows, we wil describe (1) the theoreti-
cal rationale of landmarks, (2) the general utility of 
landmark processing and several examples of clin-
ically related measures, and (3) our current work 
on developing tools to make landmark analysis 
more widely available.  
   
2 Landmarks
reflect articulation 
Landmark analysis is based on the fact that difer-
ent sounds produce diferent paterns of abrupt 
changes in the acoustic signal simultaneously 
acros wide frequency ranges.   For instance, the 
abrupt increase in amplitude for a broad range of 
frequencies above 3 kHz can be used to indicate 
the onset of bursts.  Likewise, an abrupt decrease 
in the same frequency bands can be used to indi-
cate the end of frication. The use of onset and off-
set data in other frequency bands can be used to 
indicate sonorancy; i.e., intervals when the oral 
cavity is relatively unconstricted.  Examples based 
on Liu [1995] are listed below. 
 
g(lottis): marks the onset (+g) or ofset (-g) of 
voicing. 
 
s(ylabicity): marks the onset (+s) or ofset (-s) of 
sylabicity, i.e. onsets and releases of voiced sono-
rant consonants such as /l/ or /r/, vocal tract clo-
sures due to voiced stop consonants such as /b/ or 
/d/. 
 
b(urst): marks the onset (+b) of the burst of air 
following stop or africate consonant release, or the 
onset of frication noise for fricative consonants. 
Offsets (-b) mark points where aspiration or frica-
tion noise ends abruptly due to a stop closure. 
 
V(owel):  marks points of peak amplitude in a so-
norant region—that is, a region where voicing is 
evident [Howit, 200].   
 
Although much of the past work using landmark 
procesing has been focused on employing a wide 
variety of landmarks to recognize the lexical con-
tent of spech [Juneja and Espy-Wilson 203, Slif-
ka, et al. 204], the power of thes measures is 
even more apparent when applied to non-lexical 
attributes. 
3 Aplications
of Landmark Analysis to 
Asistive Technology 
3.1 Tracking
Articulatory Precision 
Measuring articulatory precision is important to 
evaluating efficacy of a treatment or in monitoring 
disease progresion, e.g. in Parkinson’s disease. 
 
Given that landmarks reflect articulation, tols 
based on landmarks may be useful for measuring 
and monitoring articulatory precision [Boyce et al. 
2005, 2007]. The technique relies on seting em-
piricaly derived thresholds for the detection of 
abrupt acoustic changes in specified frequency 
bands.  Recal that changes in the acoustic signal 
occur simultaneously across wide frequency 
ranges. When the onset of energy does not exced 
threshold in a particular frequency band, i.e., not 
quite abrupt enough to triger the detection of a 
landmark, then no landmark may be asigned. 
However, since diferent sounds produce diferent 
paterns, changes detected in other bands at that 
point in time are either a) assigned to a different 
landmark, or b) considered to be extraneous. Thus, 
smal acoustic diferences in the way spech is 
produced can be tracked as diferent patterns of 
landmarks.  
 
In adition to requirements that a tol for general 
clinical use must be fast and robust, it must be able 
to handle a wide variety of speaking styles, dia-
lects, and voices.  By focusing on landmarks that 
specify sylable structure and broad phoneme 
classes, distinctive diferences betwen phonemes 
can be ignored.  Therefore, the tol is les likely to 
break down due to problems recognizing specific 
vocabulary while remaining sensitive to changes in 
the acoustic signal that reflect articulatory preci-
sion of spech. 
3.2 Evaluating
phonological complexity 
Development of spech in early infancy includes 
the ability to produce increasingly compx
phonological structure.  Patterns of syllable struc-
ture in spech output can be tracked using land-
marks, again withot rfrn to specif 
phonemes or words.   In Fel et al. [202], land-
marks were grouped into standard sylable paterns 
and syllables were grouped into uterances.   Statis-
38
tics based on these patterns were then reported to 
the clinician for various uses in training, screning 
or diagnosis.  Patterns of syllable complexity were 
used to compute a "vocalization age."  This was 
used in turn to derive screning rules that clinically 
distinguish infants who may be at risk for later 
comunication or other developmental problems 
from typicaly developing infants. 
 
3.3 Measuring
and Evaluating “Clear 
Speech” 
“Clear Spech” is an inteligibility-enhancing style 
of speech that is used to improve communication 
outcomes.  Listeners with hearing impairment de-
rive significant benefit from being adresed with 
clearly articulated speech. Speech that is more 
clearly articulated contains more abrupt acoustic 
changes. The result is that spech with difernt 
levels of intelligibility shows different numbers 
and combinations of landmarks [Boyce et al. 2005, 
2007]. 
3.4 Other
Aplications 
In the UCARE project [1995], Cres reported ana-
lyzing 40 hours of pre-existing [2005] videotaped 
sessions of children with physical or neurological 
impairments using landmark-based tools. 
 
Fel et al. [2004] reported using landmark analysis 
to follow the progres of several children with se-
vere speech delays. In this project, 10-minute, in-
home audio recordings were procesed in real-time 
on a 2002-era PC laptop. 
 
Wade and Möbius [207] used automated land-
mark analysis to study speaking rate efects as a 
measure of disease progresion in Parkinson's dis-
ease. 
 
DiCico and Patel [208] used automatic landmark 
analysis on dysarthric speech. This study provides 
quantitative support for the hypothesis [Deler 
1991] that dysarthric speech includes eroneous 
additional acoustic cues, not only malformed or 
mising ones. 
4 Potential
Benefits of Landmark Apli-
cations 
In a smal study, Warner-Czyz and Davis [2010] 
compared consonant–vowel syllable acuracy in 
early words of children with normal hearing and 
children with hearing los who received cochlear 
implantation.  They found and evaluated, via man-
ual coding, approximately 4000 syllables from 48 
hours of recordings.  This is a project where auto-
matic landmark analysis might have greatly re-
duced the efort. 
 
Similarly, in a study on tongue-twisters, Matthew 
Goldrick (Northwestern University) colected 10 
hours of data comprising 20,000 tokens in les than 
thre weks, but found that it required another 60 
hours merely to segment and label the data for fur-
ther analysis.  In personal corespondence about 
another study on single words, he stated:  
A major ‘choke point’ for spech production 
research is the ned to manualy analyze 
spech data. Given that many thousands of 
data points are typically required to gain accu-
rate estimates of probability density functions 
along phonetic dimensions, hundreds of per-
son-hours are typically required to analyze da-
ta from a single simple experiment…. If we 
could gain access to reliable, highly accurate 
automated tols, we could change the sped of 
research by an order of magnitude. 
 
Researchers who curently want to use spech 
analysis as a tol must accept long periods of hand 
measurements. This discourages resarchers who 
may be more interested in a particular neurological 
disease or proces than in speech research per se.  
It is notoriously dificult to quantify projects not 
undertaken, or papers not writen, but it is telling 
that, although each of the studies cited above re-
ported positive results from a study of speech ar-
ticulation, they exist as relative islands in their 
respective disciplines. We contend that this situa-
tion exists largely because of barriers to entry; that 
is, we believe that many scientists would like to 
use speech asesment as part of their research, but 
elect not to do for lack of a convenient tool. The 
existence of a convenient tol to detect, measure 
and track subtle changes in speech articulation 
would constitute an enabling technology.  
 
39
5 Tools

5.1 Description

In our own work, we have developed an automatic 
tool for detecting, counting and analyzing acoustic 
events in the spech signal that are comonly used 
by scientists to measure diferences in speech ar-
ticulation. 
 
We are now integrating our system with Wave-
surfer for certain researchers (linguists, spech-
language pathologists, certain enginering and 
cognitive-science researchers) with a primary in-
terest in inspecting and interpreting the articula-
tion-related features in the waveforms of a corpus: 
e.g., the placement of landmarks of each type, pat-
terns of clustering, or identification of non-spech 
sounds to be excised. (See Figure 1).) 
 
For this version, we are implementing user controls 
(“widgets”) to produce automated measures or 
types of analyses for spech research such as: 
• Voice-onset time, VOT. 
• Detection of non-harmonic (and harmonic) 
voicing. 
• Identification and supresion or removal of 
stray sounds, i.e., non-spech. 
• Grouping of landmarks into sylable-like clus-
ters. 
 (Note that Wavesurfer already provides a general 
pitch-tracking capability for harmonic voicing.) 
 
The Wavesurfer plug-in will also allow the user to 
output information about an audio file or a direc-
tory of audio files, e.g. al the recordings of a child. 
This information wil be in a tab-delimited text file 
or a spreadsheet. This wil alow the speech scien-
tist
 
 
 
Figure 1: Wavesurfer with landmarks/waveform pane filtered to show only +/-g landmarks, and tran-
scription pane (top) with +/-g and +/-s landmarks 
40
This information wil be in a tab-delimited text file 
or a spreadsheet. This wil alow the speech scien-
tist to analyze the output and, for example, to 
sumarize and compare the typically developing 
children to those diagnosed with autism. 
 
5.2 User
Testing 
We are curently recruiting potential users to test 
the system including graduate students and senior 
researchers in neurosciences and spech-related 
sciences. So that these users can test the system on 
a realistic problem, we will provide them with a 
corpus of annotated, de-identified recordings of 
children with and without a diagnosis of autism. 
This wil provide context for specific training tasks 
that we ask of the users and enable them to formu-
late their own apropriate, if small, research ques-
tions that the system can help to answer.  We will 
probe their experiences by logging the questions 
they have about the system, watching their actions 
as they attempt to answer the research questions, 
and asking their opinions of the experience after-
ward. 
6 Requested
Features 
In an early trial of our Waversurfer plug-in, a user 
requested the VOT (voice-onset time) measure. In 
response to this request, we are now adding a 
VOT-transcription pane to display the automati-
cally computed voice onset times aligned with the 
waveform, spectrogram, and displayed informa-
tion.  The information in this pane is also auto-
matically saved to a text file that can be analyzed 
with other software. 
 
This request also led us to include a popup window 
to show the vowel-space in a recording. Vowel-
space measures are conventionally labor-intensive, 
thus limited to a few instances of specific vowels, 
and require that the researcher first identify spe-
cific instances of these vowels.  On the other hand, 
vocalic landmarks identify the instants where for-
mant frequencies may be reliably estimated, so our 
tools can quickly and automatically evaluate the 
ful vowel space of a passage. (Se Figure 2.) 
 
Figure 2: Automatic Vowel-Space Evaluation.  Com-
puting the resonant frequencies (formants) at vowel 
landmarks alows ploting the vowel space, i.e., the scat-
ter of the first two formants against each other.  In this 
case, a female read the complete Rainbow Pasage (a 
standard passage of 3 paragraphs, aprox. 90 sec of 
reading).  The system automatically identified all the 
consonantal and vocalic landmarks, evaluated the for-
mants at ~ 140 stresed vowels, and computed the con-
vex hull (“ruber-band”) area, 0.88 kHz
2
. Total 
computation time on a comodity 3 GHz PC was 143 
sec (and is directly proportional to the duration of the 
pasage). 
7 Challenges
for Software development, 
Chalenges for availability 
Our algorithms are implemented in MATLAB. 
Though tolkits that run in MATLAB might be 
available free, or for a modest price, the MATLAB 
platform itself is costly, especially for non-
academic users. On the other hand, shareware or 
freware may have minimal documentation; sup-
port that depends entirely on the presence (or ab-
sence!) of a knowledgeable user comunity; and 
variable standards for testing, correctnes, and per-
formance.   
 
A critical hiden cost for any system is the learn-
ing curve.  For those systems with litle documen-
tation and training, this can dwarf the overt costs. 
Our goal is to make learning easier by creating 
landmark-procesing plug-ins that people can use 
within software that they already employ. 
 
41
Such a plan requires a careful balance betwen the 
flexibility of a general, extensible system and the 
simplicity of a smal, fixed set of easily docu-
mented plug-in capabilities.  Our project therefore 
includes both a small set of simple functions, such 
as VOT, and software design centered on the neds 
identified by users from the apropriate research 
comunities.  Our design relis on an iterative 
proces of structured interviews and web-based 
surveys, combined with observations of user expe-
riences with our plug-ins. 
 
This user study extends beyond the mater of func-
tionality and documentation.  It also addreses the 
expectations or requirements for convenient avail-
ability, training, and suport, and the costs that 
these imply. 
8 Future
work 
8.1 R
– statistical analysis system 
We wil integrate our software with R 
(http:/ww.r-project.org/) for those with a pri-
mary interest instead in the derived articulatory-
precision information: e.g., syllable production 
rate, fraction of sylables of a given complexity, or 
range of vowels. 
 
For this platform, we will implement further user-
level functions, with coresponding graphical user 
interfaces as apropriate, to produce: 
• Number of landmarks, optionaly excluding 
those that are automatically detected as noise-
related. 
• Sylable complexity and statistics of same. 
• Utterance complexity. 
• Sylable production rate. 
• Articulatory precision. 
• Vowel space measures. 
8.2 Other
Platforms 
We plan to expand our work to include plugins or 
packages for integration with a wider (and more 
powerful) collection of research tools, for example 
PRAT, CSL, or even Excel. 
8.3 Other
Features 
We are soliciting input from user comunities 
about the features they would like to see in these 
tools.   
Acknowledgments 
This work was funded in part by NIH grant R43 
DC010104. 
References 
Suzane Boyce, Joel MacAuslan, An Bradlow, and 
Rajka Smiljanič. 2007. Automatic Detection of Dif-
ferences Betwen Clear & Conversational Spech, 
poster presented at American Spech-Language-
Hearing Convention. 
Suzane Boyce, An Bradlow, and Joel MacAuslan. 
2005. Landmark analysis of clear and conversational 
speaking styles, 150th meting of the Acoustical So-
ciety of America. 
Thomas DiCico and Rupal Patel. 208. Automatic 
Landmark Analysis of Dysarthric Spech, Journal of 
Medical Spech-Language Pathology, 16(4):213-
219. 
Cynthia J. Cres, S. Unrein, A. Weber, S. Krings, H. 
Fel, J. MacAuslan, and J. Gong. 205. Vocal Devel-
opment Paterns in Children at Risk for Being Non-
speaking. ASHA 205. 
Cynthia J. Cres. 195. Comunicative and symbolic 
precursors of AC, Unpublished NIH CIDA Grant: 
University of Nebraska-Lincoln. 
Jack R. Deler, D. Hsu, and Linda J. Ferier. 191. On 
the Use of Hidden Markov Modeling for Recognition 
of Dysarthric Speech, Computer Methods and Pro-
grams in Biomedicine. (35)2:125-139. 
Hariet J. Fel, Joel MacAuslan, Linda J. Ferier, Susan 
G. Worst, and Karen Chenausky. 202. Vocalization 
Age as a Clinical Tol. Procrocedings of the Inter-
national Conference on Speech and Language Proc-
essing. 
Hariet J. Fel, Joel MacAuslan, Cynthia. Cres, Linda J. 
Ferier. 204. visiBabble for Reinforcement of Early 
Vocalization, Procedings of ASSETS 204. 161-168. 
Wilson Howit. 200. Unpublished Ph.D. disertation, 
Masachusets Institute of Technology. 
Amit Juneja and Carol Espy-Wilson. 203, Spech 
Segmentation Using Probabilistic Phonetic Feature 
Hierarchy and Suport Vector Machines. Proced-
ings of the International Joint Conference on Neural 
Networks. 
Sharlene A. Liu. 195. Landmark Detection for Distinc-
tive Feature-Hyphen Based Spech Recognition, 
M.I.T. Doctoral Thesis. 
R, http:/ww.r-project.org/ 
42
Janet Slifka, Keneth N. Stevens, Sharon Manuel, and 
Stefanie Shatuck-Hufnagel. 204. A Landmark-
Based Model of Spech Perception: History and Re-
cent Developments. From Sound to Sense, 85-90. 
Keneth N. Stevens, 200. Acoustic Phonetics, The 
MIT Pres, Cambridge, Massachusetts. 
Keneth N. Stevens. 202. Toward a model for lexical 
access based on acoustic landmarks and distinctive 
features, Journal of the Acoustic Society of America. 
111(4):1872-1891. 
Keneth N. Stevens, Sharon Manuel, Stefanie Shatuck-
Hufnagel, and Sharlene Liu. 192. Implementation of 
a model for lexical access based on features,  Proced-
ings ICSLP (Int. Conf. on Spech & Language Proc-
essing). 49-502. 
Travis Wade, Bernd Möbius. 207. Speaking rate ef-
fects in a landmark-based phonetic exemplar model, 
Interspech 207. 402-405. 
Wavesurfer.205. http:/ww.speech.kth.se/wavesurfer/ 
 
43

