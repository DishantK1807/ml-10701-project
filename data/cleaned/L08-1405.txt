<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Inderjit S Dhillon</author>
<author>Yuqiang Guan</author>
<author>J Kogan</author>
</authors>
<title>Iterative Clustering of High Dimentional Text Data Augmented by Local Search</title>
<date>2002</date>
<booktitle>In The 2002 IEEE International Conference on Data Mining</booktitle>
<pages>131--138</pages>
<contexts>
<context>e will investigate the proper reduction degree, and improve the similarity definition. Lastly, we note that viewing our method as method to improve the given clustering result, “first validation” of (Dhillon et al., 2002) and “Link based refinement” of (Ding et al., 2001) are informative to refine our method. 6. Conclusion In this paper, we proposed the method to reduce the similarity matrix size in order to use spec</context>
</contexts>
<marker>Dhillon, Guan, Kogan, 2002</marker>
<rawString>Inderjit S. Dhillon, Yuqiang Guan, and J. Kogan. 2002. Iterative Clustering of High Dimentional Text Data Augmented by Local Search. In The 2002 IEEE International Conference on Data Mining, pages 131–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjit Dhillon</author>
<author>Yuqiang Guan</author>
<author>Brian Kulis</author>
</authors>
<title>A Unified View of Kernel k-means, Spectral Clustering and Graph Cuts</title>
<date>2005</date>
<tech>Technical Report TR04-25</tech>
<institution>In The University of Texas at Austin, Department of Computer Sciences</institution>
<contexts>
<context>ve the eigenvalue problem of the Laplacian matrix converted from the similarity matrix corresponding to the given data set. Therefore, we cannot use spectral clustering for a large document data set (Dhillon et al., 2005)(Liu et al., 2007). In this paper, we propose the method to reduce the similarity matrix size. First, using k-means, we obtain a clustering result for the given data set. From each cluster, we pick u</context>
</contexts>
<marker>Dhillon, Guan, Kulis, 2005</marker>
<rawString>Inderjit Dhillon, Yuqiang Guan, and Brian Kulis. 2005. A Unified View of Kernel k-means, Spectral Clustering and Graph Cuts. In The University of Texas at Austin, Department of Computer Sciences. Technical Report TR04-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Ding</author>
<author>Xiaofeng He</author>
<author>Hongyuan Zha</author>
<author>Ming Gu</author>
<author>Horst Simon</author>
</authors>
<title>Spectral Min-max Cut for Graph Partitioning and Data Clustering. In Lawrence Berkeley National Lab</title>
<date>2001</date>
<tech>Tech. report 47848</tech>
<contexts>
<context> retrieved documents are clustered, is actively researched (Hearst and Pedersen, 1996) (Kummamuru et al., 2004) etc. Spectral clustering is a powerful clustering method using partitioning of a graph (Ding et al., 2001). It uses an object function to find the optimal partition of the graph. The optimal solution of the object function corresponds to a solution of an eigenvalue problem. Using this solution, spectral </context>
<context> fact that an optimum solution of the object function corresponds to the solution of an eigenvalue problem. Different object functions are proposed. In this paper, we use the object function of Mcut (Ding et al., 2001). First, we define the similarity CRD9D8B4BTBNBUB5 between the subgraph BTand BUas follows: CRD9D8B4BTBNBUB5BPCFB4BTBNBUB5BM The function CFB4BTBNBUB5 is the sum of the weights of the edges between B</context>
<context>prove the similarity definition. Lastly, we note that viewing our method as method to improve the given clustering result, “first validation” of (Dhillon et al., 2002) and “Link based refinement” of (Ding et al., 2001) are informative to refine our method. 6. Conclusion In this paper, we proposed the method to reduce the similarity matrix size in order to use spectral clustering for a large data set. Our method ne</context>
</contexts>
<marker>Ding, He, Zha, Gu, Simon, 2001</marker>
<rawString>Chris Ding, Xiaofeng He, Hongyuan Zha, Ming Gu, and Horst Simon. 2001. Spectral Min-max Cut for Graph Partitioning and Data Clustering. In Lawrence Berkeley National Lab. Tech. report 47848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
<author>Jan O Pedersen</author>
</authors>
<title>Reexamining the cluster hypothesis: Scatter/gather on retrieval results</title>
<date>1996</date>
<booktitle>In Proceedings of SIGIR-96</booktitle>
<pages>76--84</pages>
<contexts>
<context>ent procedure, and is important in text mining systems (Michael W. Berry, 2003). As the specific application, relevant feedback in IR, where retrieved documents are clustered, is actively researched (Hearst and Pedersen, 1996) (Kummamuru et al., 2004) etc. Spectral clustering is a powerful clustering method using partitioning of a graph (Ding et al., 2001). It uses an object function to find the optimal partition of the g</context>
</contexts>
<marker>Hearst, Pedersen, 1996</marker>
<rawString>Marti A. Hearst and Jan O. Pedersen. 1996. Reexamining the cluster hypothesis: Scatter/gather on retrieval results. In Proceedings of SIGIR-96, pages 76–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krishna Kummamuru</author>
</authors>
<title>Rohit Lotlikar, Shourya Roy, Karan Singal, and Raghu Krishnapuram</title>
<date>2004</date>
<booktitle>In Proceedings of WWW-04</booktitle>
<pages>658--665</pages>
<marker>Kummamuru, 2004</marker>
<rawString>Krishna Kummamuru, Rohit Lotlikar, Shourya Roy, Karan Singal, and Raghu Krishnapuram. 2004. A Hierarchical Monothetic Document Clustering Algorithm for Summarization and Browsing Search Results. In Proceedings of WWW-04, pages 658–665.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tie-Yan Liu</author>
<author>Huai-Yuan Yang</author>
<author>Xin Zheng</author>
<author>Tao Qin</author>
<author>Wei-Ying Ma</author>
</authors>
<title>Fast Large-Scale Spectral Clustering by Sequential Shrinkage Optimization</title>
<date>2007</date>
<booktitle>In ECIR</booktitle>
<pages>319--330</pages>
<contexts>
<context>lem of the Laplacian matrix converted from the similarity matrix corresponding to the given data set. Therefore, we cannot use spectral clustering for a large document data set (Dhillon et al., 2005)(Liu et al., 2007). In this paper, we propose the method to reduce the similarity matrix size. First, using k-means, we obtain a clustering result for the given data set. From each cluster, we pick up some data, which</context>
</contexts>
<marker>Liu, Yang, Zheng, Qin, Ma, 2007</marker>
<rawString>Tie-Yan Liu, Huai-Yuan Yang, Xin Zheng, Tao Qin, and Wei-Ying Ma. 2007. Fast Large-Scale Spectral Clustering by Sequential Shrinkage Optimization. In ECIR, pages 319–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael W Berry</author>
<author>editor</author>
</authors>
<date>2003</date>
<booktitle>Survey of Text Mining: Clustering, Classification, and Retrieval</booktitle>
<publisher>Springer</publisher>
<marker>Berry, editor, 2003</marker>
<rawString>Michael W. Berry, editor. 2003. Survey of Text Mining: Clustering, Classification, and Retrieval. Springer.</rawString>
</citation>
</citationList>
</algorithm>

