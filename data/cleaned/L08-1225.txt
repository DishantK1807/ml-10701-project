<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Makoto Iwayama</author>
<author>Noriko Kando</author>
</authors>
<title>Overview of patent retrieval task at NTCIR-5</title>
<date>2005</date>
<booktitle>In Proceedings of the Fifth NTCIR Workshop Meeting</booktitle>
<pages>269--277</pages>
<publisher>John</publisher>
<contexts>
<context>ate, test collections for information retrieval research have been constructed from such sources as newspaper articles (Kitani et al., 1998), Web documents (Oyama et al., 2005), and patent documents (Fujii et al., 2005). Test collections for cross-language retrieval (Gey and Oard, 2001; Kishida et al., 2005), open-domain question answering (Voorheesand Tice, 1999; Kato et al., 2005),and text summarization (Hirao et</context>
</contexts>
<marker>Fujii, Iwayama, Kando, 2005</marker>
<rawString>Atsushi Fujii, Makoto Iwayama, and Noriko Kando. 2005. Overview of patent retrieval task at NTCIR-5. In Proceedings of the Fifth NTCIR Workshop Meeting, pages 269–277. John S. Garofolo, Cedric G. P. Auzanne, and Ellen M.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Voorhees</author>
</authors>
<title>The TREC spoken document retrieval track: A success story</title>
<date>1999</date>
<booktitle>In Proceedingsof TREC-9</booktitle>
<pages>107--129</pages>
<marker>Voorhees, 1999</marker>
<rawString>Voorhees. 1999. The TREC spoken document retrieval track: A success story. In Proceedingsof TREC-9, pages 107–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fredric C Gey</author>
<author>Douglas W Oard</author>
</authors>
<date>2001</date>
<booktitle>The TREC2001 cross-languageinformationretrieval track: Searchingarabicusingenglish,frenchorarabicqueries. InProceedings of TREC-10</booktitle>
<pages>16--25</pages>
<contexts>
<context>onstructed from such sources as newspaper articles (Kitani et al., 1998), Web documents (Oyama et al., 2005), and patent documents (Fujii et al., 2005). Test collections for cross-language retrieval (Gey and Oard, 2001; Kishida et al., 2005), open-domain question answering (Voorheesand Tice, 1999; Kato et al., 2005),and text summarization (Hirao et al., 2004) have also been constructed. A test collection for Spoken</context>
</contexts>
<marker>Gey, Oard, 2001</marker>
<rawString>Fredric C. Gey and Douglas W. Oard. 2001. The TREC2001 cross-languageinformationretrieval track: Searchingarabicusingenglish,frenchorarabicqueries. InProceedings of TREC-10, pages 16–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsutomu Hirao</author>
<author>Manabu Okumura</author>
<author>Takahiro Fukusima</author>
<author>Hidetsugu Nanba</author>
</authors>
<title>Text summarization challenge3 – text summarizationevaluationat NTCIR workshop 4</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth NTCIR Workshop. Tsuneaki Kato, Jun’ichi Fukumoto, and Fumito Masui</booktitle>
<contexts>
<context>l., 2005). Test collections for cross-language retrieval (Gey and Oard, 2001; Kishida et al., 2005), open-domain question answering (Voorheesand Tice, 1999; Kato et al., 2005),and text summarization (Hirao et al., 2004) have also been constructed. A test collection for Spoken Document Retrieval (SDR) is usually based on a broadcast news corpus. Compared to broadcast news, lectures are more challenging for speech re</context>
</contexts>
<marker>Hirao, Okumura, Fukusima, Nanba, 2004</marker>
<rawString>Tsutomu Hirao, Manabu Okumura, Takahiro Fukusima, and Hidetsugu Nanba. 2004. Text summarization challenge3 – text summarizationevaluationat NTCIR workshop 4. In Proceedings of the Fourth NTCIR Workshop. Tsuneaki Kato, Jun’ichi Fukumoto, and Fumito Masui.</rawString>
</citation>
<citation valid="true">
<title>An overview of NTCIR-5 QAC3</title>
<date>2005</date>
<booktitle>In Proceedings of the Fifth NTCIR Workshop Meeting</booktitle>
<pages>361--372</pages>
<marker>2005</marker>
<rawString>2005. An overview of NTCIR-5 QAC3. In Proceedings of the Fifth NTCIR Workshop Meeting, pages 361–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>HiroakiNanjo TatsuyaKawahara</author>
<author>TakahiroShinozaki</author>
<author>Sadaoki Furui</author>
</authors>
<title>Benchmark test for speech recognition using the corpus of spontaneous Japanese</title>
<date>2003</date>
<booktitle>In ISCA &amp; IEEE Workshop on Spontaneous Speech Processing and Recognition</booktitle>
<pages>135--138</pages>
<marker>TatsuyaKawahara, TakahiroShinozaki, Furui, 2003</marker>
<rawString>TatsuyaKawahara,HiroakiNanjo,TakahiroShinozaki,and Sadaoki Furui. 2003. Benchmark test for speech recognition using the corpus of spontaneous Japanese. In ISCA &amp; IEEE Workshop on Spontaneous Speech Processing and Recognition, pages 135–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuaki Kishida</author>
<author>Kuang hua Chen</author>
<author>Sukhoon Lee</author>
</authors>
<title>Kazuko Kuriyama, Noriko Kando, Hsin-Hsi Chen, and Sung Hyon Myaeng</title>
<date>2005</date>
<booktitle>In Proceedings of the Fifth NTCIR Workshop Meeting</booktitle>
<pages>1--38</pages>
<contexts>
<context> sources as newspaper articles (Kitani et al., 1998), Web documents (Oyama et al., 2005), and patent documents (Fujii et al., 2005). Test collections for cross-language retrieval (Gey and Oard, 2001; Kishida et al., 2005), open-domain question answering (Voorheesand Tice, 1999; Kato et al., 2005),and text summarization (Hirao et al., 2004) have also been constructed. A test collection for Spoken Document Retrieval (S</context>
</contexts>
<marker>Kishida, Chen, Lee, 2005</marker>
<rawString>Kazuaki Kishida, Kuang hua Chen, Sukhoon Lee, Kazuko Kuriyama, Noriko Kando, Hsin-Hsi Chen, and Sung Hyon Myaeng. 2005. Overview of CLIR task at the fifth NTCIR workshop. In Proceedings of the Fifth NTCIR Workshop Meeting, pages 1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsuyoshi Kitani</author>
</authors>
<title>Yasushi Ogawa, Tetsuya Ishikawa, Haruo Kimoto, Ikuo Keshi, Jun Toyoura, Toshikazu Fukushima, Kunio Matsui, Yoshihiro Ueda, Tetsuya Sakai, Takenobu Tokunaga,Hiroshi Tsuruoka, Hidekazu Nakawatase, and Teru Agata</title>
<date>1998</date>
<booktitle>In Proceedings of ACM SIGIR</booktitle>
<pages>345--346</pages>
<marker>Kitani, 1998</marker>
<rawString>Tsuyoshi Kitani, Yasushi Ogawa, Tetsuya Ishikawa, Haruo Kimoto, Ikuo Keshi, Jun Toyoura, Toshikazu Fukushima, Kunio Matsui, Yoshihiro Ueda, Tetsuya Sakai, Takenobu Tokunaga,Hiroshi Tsuruoka, Hidekazu Nakawatase, and Teru Agata. 1998. Lessons from BMIR-J2: A test collection for Japanese IR systems. In Proceedings of ACM SIGIR, pages 345–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akinobu Lee</author>
<author>Tatsuya Kawahara</author>
<author>K Shikano</author>
</authors>
<title>Julius — an open source real-time large vocabulary recognition engine</title>
<date>2001</date>
<booktitle>In Proceedings of European Conference on Speech Communication and Technology</booktitle>
<pages>1691--1694</pages>
<contexts>
<context> four relevant passages in the target collection, 39 queries were selected for our test collection. Table 2 shows some statistics of the result. 2.4. Automatic Transcription A Japanese LVCSR decoder (Lee et al., 2001) was used to obtain automatic transcriptions of the target spoken documents. Because the target spoken documents of lecture speech are more spontaneousthan those of broadcastnews, the speech recognit</context>
</contexts>
<marker>Lee, Kawahara, Shikano, 2001</marker>
<rawString>Akinobu Lee, Tatsuya Kawahara, and K. Shikano. 2001. Julius — an open source real-time large vocabulary recognition engine. In Proceedings of European Conference on Speech Communication and Technology, pages 1691–1694,Sept.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kikuo Maekawa</author>
</authors>
<title>Hanae Koiso, Sadaoki Furui, and Hitoshi Isahara</title>
<date>2000</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>947--952</pages>
<marker>Maekawa, 2000</marker>
<rawString>Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hitoshi Isahara. 2000. Spontaneous speech corpus of Japanese. In Proceedings of LREC, pages 947–952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keizo Oyama</author>
</authors>
<title>Masao Takaku, Haruko Ishikawa, Akiko Aizawa, and Hayato Yamana</title>
<date>2005</date>
<booktitle>In Proceedings of the Fifth NTCIR Workshop Meeting</booktitle>
<pages>423--442</pages>
<marker>Oyama, 2005</marker>
<rawString>Keizo Oyama, Masao Takaku, Haruko Ishikawa, Akiko Aizawa, and Hayato Yamana. 2005. Overview of the NTCIR-5 WEB navigational retrieval subtask 2. In Proceedings of the Fifth NTCIR Workshop Meeting, pages 423–442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amit Singhal</author>
<author>Chris Buckley</author>
<author>Mandar Mitra</author>
</authors>
<title>Pivoted document length normalization</title>
<date>1996</date>
<booktitle>In Proceedings of ACM SIGIR</booktitle>
<pages>21--29</pages>
<contexts>
<context>words, their character bi-grams, or a combination of the two. The vector space model was used as the retrieval model and TF–IDF (Term Frequency–Inverse Document Frequency) with pivoted normalization (Singhal et al., 1996)was used forterm weighting. We comparedthree representations of the pseudopassages: the 1-best automatically transcribed text, the union of the 10-best automatically transcribed texts, and the refere</context>
</contexts>
<marker>Singhal, Buckley, Mitra, 1996</marker>
<rawString>Amit Singhal, Chris Buckley, and Mandar Mitra. 1996. Pivoted document length normalization. In Proceedings of ACM SIGIR, pages 21–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
</authors>
<title>An overview of evaluation methods in TREC ad hoc information retrieval and TREC question answering</title>
<date>2007</date>
<booktitle>In Laila Dybkj¨ar, Holmer Hemsen, and WolfgangMinker,editors,EvaluationofTextandSpeech Systems, number37in Text,Speech andLanguageTechnology</booktitle>
<pages>163--186</pages>
<contexts>
<context>-best automatically transcribed text, the union of the 10-best automatically transcribed texts, and the reference manually transcribed text. 3.3. Evaluation Metric We used 11-point average precision (Teufel, 2007) as our evaluation metric, which is obtained by averaging the following AP over the queries. IP(x)=max x≤R i P i Table 3: A comparison between TREC-9 SDR and our CSJ SDR test collections. TREC9 SDR C</context>
</contexts>
<marker>Teufel, 2007</marker>
<rawString>Simone Teufel. 2007. An overview of evaluation methods in TREC ad hoc information retrieval and TREC question answering. In Laila Dybkj¨ar, Holmer Hemsen, and WolfgangMinker,editors,EvaluationofTextandSpeech Systems, number37in Text,Speech andLanguageTechnology, pages 163–186.Springer.</rawString>
</citation>
</citationList>
</algorithm>

