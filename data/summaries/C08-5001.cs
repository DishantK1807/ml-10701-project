in (Huang and Chiang, 2005). See Section 6 for the alternative notion of
W05-1506
Mohri, Mehryar and Brian Roark. 2006. Probabilistic context-free grammar
N06-1040
Charniak, Eugene and Mark Johnson. 2005. Coarse-to-fine-grained n-best parsing
P05-1022
McDonald, Ryan, Koby Crammer, and Fernando Pereira. 2005. Online
P05-1012
Och, Franz Joseph. 2003. Minimum error rate training in statistical machine
P03-1021
Graehl, Jonathan and Kevin Knight. 2004. Training tree transducers. In
N04-1014
modeling flexibility than the weighted deductive systems of Nederhof (2003):
J03-1006
Huang, Liang. 2005. k-best Knuth algorithm and k-best A* parsing. Unpublished
W05-1506
Huang, Liang and David Chiang. 2005. Better k-best Parsing. In Proceedings of
W05-1506
son, 2005; Huang and Chiang, 2005). The k-best list is also frequently used
W05-1506
tions of this algorithm include k-best parsing (McDonald et al., 2005; Mohri
P05-1012
mented as part of Dyna (Eisner et al., 2005), a generic langauge for dynamic
H05-1036
can apply (Nederhof, 2003). So which one is better in this case?
J03-1006
Huang (2005). A separate problem, k-shortest hyperpaths, has been studied
W05-1506
derivations (Jim´enez and Marzal, 2000; Huang and Chiang, 2005). Applica-
W05-1506
Nederhof, Mark-Jan. 2003. Weighted deductive parsing and Knuth’s algorithm.
J03-1006
is usually exponentially large (Och, 2003; McDonald et al., 2005).
P03-1021 P05-1012
Eisner, Jason, Eric Goldlust, and Noah A. Smith. 2005. Compiling comp ling:
H05-1036
Klein, Dan and Chris Manning. 2003. A* parsing: Fast exact Viterbi parse
N03-1016
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic labeling of semantic roles.
J02-3001
candidates (Collins, 2000; Gildea and Jurafsky, 2002; Charniak and John-
J02-3001
