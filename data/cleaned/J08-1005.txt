Incremental Conceptualization for Language Production
Markus Guhe
(UniversityofEdinburgh)
Mahwah,NJ:LawrenceErlbaumAssociates(distributedbyPsychologyPress),2007,
xii+260pp;hardbound,ISBN978-0-8058-5624-8,$75.00
Reviewedby
PaulPiwek
TheOpenUniversity
For the past ten years or more, most work in the ﬁeld of Natural Language Gener-
ation (NLG) has shied away from considerations regarding the processes underlying
humanlanguageproduction.Rather,thefocushasbeenonsystemsthatautomatically
produce language—usually text—from non-linguistic representations, with the main
objective being generation of a text that faithfully captures the meaning of those non-
linguistic representations (see, e.g., Reiter and Dale’s 2000 textbook on NLG). There
is, however, also a different take on NLG “as not just competent performance by a
computer but the development of a computational theory of the human capacity for
language and processes that engage it” (McDonald 1987, page 642). Guhe’s research
monograph, based on his 2003 Ph.D. thesis, is ﬁrmly situated in the latter tradition.
One of his main goals is to work out a computational architecture for Levelt’s (1989)
psycholinguistically motivated model of language production. According to Levelt’s
model,speakinginvolvesthreemainactivities:conceptualizing(decidingwhattosay),
formulating(decidinghowtosayit),andarticulating(sayingit).Guhe’sbookfocuses
onthementalactivityofconceptualizing.
Conceptualizing is a recalcitrant object of study, partly because of the problem of
the “initial spark”; the decision to say something appears to be the result of volitional
consciousdecisions,whichlargelyeludescientiﬁcstudy.Guheavoidsthisproblemby
investigating conceptualization in settings where the main intention is already ﬁxed:
a speaker witnesses several events unfold and is instructed to describe what happens
(whileithappens).Theresearchchallengethenistoﬁgureouthow“subintentions”for
individualspeechactscomeabout.Thebeneﬁtofusinganon-linegenerationsettingis
that it provides information on both what a speaker says at a given point in time and
whatisbeingreported,thatis,thedatathatdrivethespeaker’sutterances.
The book consists of the usual preface and introduction, followed by four parts
(A, B, C, and Results), a list of the book’s theses, and an appendix that includes,
among other things, a glossary, bibliography, name index, and subject index. Part A
of the book is titled “Conceptualization.” It starts with an introduction to the ﬁeld of
languageproduction,withparticularreferencetoLevelt’s(1989)model.Thenotionof
conceptualizationasa“quasi-module,”partlyusingFodor’s(1983)criteria,ispresented
andfoursubtasksofconceptualizationarediscussed:
1. construction(mainlymappingwhatisperceivedtoconceptsfrom
long-termmemory)
2. selection(ofeventsthataretobeverbalized)
3. linearization(orderingselectedeventsappropriatetothegoalof
thediscourse)
ComputationalLinguistics Volume34,Number1
4. generationofpreverbalmessages(mappingtheconceptual
representationsthathavebeenhandledsofartosemantic
contentthatcaninterfacewiththelinguisticformulator)
This chapter also introduces referential nets, the formalism that is used to represent
conceptualcontent.
PartB(“Incrementality”)tracestherootsofthenotionofincrementalityincomputer
science,andprovidesanextensiveoverviewofvariousnotionsofincrementality.Guhe
settles on a deﬁnition of incrementality whose crux is the piecemeal processing of
informationandproductionofoutputbeforeallinputhasbeenseen.Hedistinguishes
betweenincrementalprocesses,algorithms,andmodels;roughlyspeaking,incremental
models contain a strictly uni-directional cascade of incremental processes that recur-
sivelycallincrementalalgorithms.ForGuhe,anessentialcharacteristicofincremental
algorithms is that they use only a local context, as opposed to all available knowl-
edge,fortheircomputations.Healsoadoptsthecommondistinctionbetweenworking
memoryandlong-termmemory.Theformermediatestheﬂowofinformationbetween
incremental processes. “Increments,” the small pieces of information that incremental
processesoperatewith,canbereadfromitandwrittentoit.Itcontains“situationand
discourseknowledge,” whereaslong-termmemorystoresstatic“encyclopedic knowl-
edge.” This “blueprint for incrementality” is accompanied by a useful discussion of
various dimensions of incrementality, such as monotonicity, lookahead, feedback, and
discreteness.
Part C focuses on INC, the incremental conceptualizer, which is an implemented
“working model” of the blueprint for incrementality. INC is offered as a framework,
that is, a model which has been ﬂeshed out in detail in some respects and left under-
speciﬁedinothers.Acentralroleisplayedbyfourparametersof INC whichinﬂuence
itsbehavior.Forexample,twooftheseconcernthestorageofeventrepresentationsina
bufferinworkingmemorywhichmediatestheﬂowofinformationbetweenincremental
processes. One parameter, length of traverse buffer (LOTB), concerns the size of this
buffer, whereas the other, latency (LT), determines for how long an element is kept in
thebufferuntilitispickedupbypreverbalmessagegeneration.SmallvaluesforLOTB
incombinationwithalargevalueforLTcanleadtothe“forgetting”ofinformation:If
the buffer has ﬁlled up and new information is added, the ﬁrst element on the buffer
isdiscardedandneverreachespreverbalmessagegeneration.Thebookpresentssome
evidencethatvariationoftheparametersettingscanaccountforsomeofthevariation
foundamonghumanspeakers.Thispartofthebookconcludeswithadiscussionofthe
output of INC for two domains and output of human speakers for the same domains.
It concerns a visual scene, from a bird’s eye perspective, of two moving planes on a
runway,andthereplayofthedrawingofasimplelinedrawingconsistingofeightlines
thatrepresentsacrossing.
The “Results” summarizes the main contributions of the book, makes some com-
parisonswithLevelt’s(1989)model,andproposesanumberoffutureextensions,such
astheadditionofLevelt’smonitor.Themonitortakesasinputtheoutputofthespeech-
comprehensionsystemandusesthistoinﬂuencetheprocessingoftheconceptualizer.
Finally,thereareagoodnumberofsuggestionsforfurtherwaystoparameterize INC.
The book is a rich source of information on language production, both from a
computational and a cognitive point of view. It includes a good introduction to con-
ceptualizing,andprovidesaninsightfuldiscussionofmanyvarietiesofincrementality.
INCisanexcellentstartingpointforothersinterestedinon-linedata-drivengeneration
to both build on and respond to. The breadth of the work means that one gets a truly
130
BookReviews
holistic view of the problem and is given a good impression of the many debates that
cross the boundaries of different disciplines. In this respect, the book goes against a
recenttrendincomputationallinguisticstoshowlessinterestinotherlanguage-related
researchcommunities(seeReiter2007).
Although the wide scope of this book is in many ways what makes it attractive,
it also leads to some of its weaknesses. In particular, the way INC is presented in this
broad context did not feel optimal to me. Although the proper description of INCis
delayed until Part C, there are numerous forward references to INC in the preceding
parts.Thereaderwillﬁndseveralinstanceswhereacertainaspectofconceptualization
or incrementality is discussed with reference to INC, only to ﬁnd out later that this
particular feature “is not implemented yet (apart from a dummy function).” It would
havebeenfairertothereadertoseparateacleardescriptionofthecurrentstateof INC
from the wider discussion surrounding it. Another presentational issue concerns the
tight integration of locality and incrementality in the book’s deﬁnitions. In particular,
thevirtualidentiﬁcationofincrementalalgorithmswithcomputationonalocalcontext
makes one question why the book speaks of incremental rather than local algorithms.
A more substantive point relates to Part C on INC. This part includes the description
oftwosimulations thatwererunwith INC.Somewhat frustratingly, bothdescriptions
areincomplete.Forinstance,whereasfortheﬁrstsimulationtheappendixcontainsthe
textsproducedbyhumanparticipantsforthesametask,thereisnosystematicanalysis
of the structural (dis)similarities between the output of INC and that of the human
speakers.Forthesecondsimulation,therearesomeanalysesofthesimilaritiesbetween
thestructureof INC’sandthehumanspeakers’output,butnotranscriptsofcomplete
human outputs are provided. In both cases, there is also no detail about how long-
term memory, referred to as the concept storage (CS), was populated for the relevant
domains,eventhoughtheCSmusthavehadasigniﬁcantinﬂuenceontheoutputthat
INCproduces.
This book will be useful to research students and researchers in natural language
generation who are interested in the study of generation systems as a computational
model of human language production. Part B of the book, on incrementality, might
also prove useful to those approaching NLG as an engineering problem. The main
reason to consult this book is that it brings together in a single place information on
conceptualization,incrementality,andvariousdebatesinphilosophy,cognitivescience,
andcomputerscienceaffectingthesetopics.INC,theincrementalconceptualizerwhich
is described in part C of the book, presents an ambitious attempt to implement a
computational model of incremental conceptualization. The verdict on its adequacy is
stillout,giventhelimitedempiricalevaluationtowhichithasbeensubjectedthusfar.
Onlinegeneration,acentralthemeofthisbook,wasadoptedin2007attheInterna-
tionalConferenceonIntelligentVirtualAgentsasatask(automatedreal-timereporting
on simulated horse races) in the GALA competition for Embodied Lifelike Agents.
1
Work on embodiment and conceptualization, new insights into societal grounding of
conceptualrepresentations(e.g.,DeVault,Oved,andStone2006),empiricalandcompu-
tationalstudiesongeneration(bothincrementalandnon-incremental,fromnumerical
data;e.g.,vanDeemter2006),andrecentexperimentaltechniquesforstudyinglanguage
production(seeRoelofs2004foranoverview)giveasensethatthisbookcouldbepart
ofanexcitingrevivalofcognitivelymotivatedNLG.
1Seehttp://hmi.ewi.utwente.nl/gala/.
131
ComputationalLinguistics Volume34,Number1

References

vanDeemter,Kees.2006.Generating referringexpressionsthatinvolvegradable properties.ComputationalLinguistics, 32(2):195–222.
DeVault,David,IrisOved,andMatthew
Stone.2006.Societalgroundingis
essentialformeaningfullanguageuse.In
Proceedingsofthe21stNationalConferenceon
ArtiﬁcialIntelligence(AAAI-06),Boston,
MA,pages747–754.
Fodor,JerryA.1983.Themodularityofmind.
MITPress,Cambridge,MA.
Levelt,WillemJ.M.1989.Speaking:From
IntentiontoArticulation.MITPress,
Cambridge,MA.
McDonald,David.1987.Naturallanguage
generation.InStuartC.Shapiro,editor,
EncyclopediaofArtiﬁcialIntelligence,
Volume1,JohnWiley&Sons,NewYork,
pages642–654.
Reiter,Ehud.2007.Theshrinking
horizonsofcomputationallinguistics.
ComputationalLinguistics,
33(2):283–287.
Reiter,EhudandRobertDale.2000.
BuildingNaturalLanguageGeneration
Systems.CambridgeUniversityPress,
Cambridge,UK.
Roelofs,Ardi.2004.Theseducedspeaker:
Modelingofcognitivecontrol.
InAnjaBelz,RogerEvans,and
PaulPiwek,editors,NaturalLanguage
Generation,ThirdInternational
Conference,LNCS3123,Springer,
Berlin,pages1–10.
PaulPiwekislecturerincomputingattheOpenUniversity.Hisresearchinterestsare(multimodal)
natural language generation and dialogue modeling. Piwek’s address is: Centre for Research in
Computing,TheOpenUniversity,WaltonHall,MiltonKeynes,UK;e-mail:p.piwek@open.ac.uk.
132

