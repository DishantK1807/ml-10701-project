Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 24–31,
Boulder, Colorado, June 2009. c 2009 Association for Computational Linguistics
Automaticaly Extracting Word Relationships 
as Templates for Pun Generation 
 
 
Bryan Anthony Hong and Ethel Ong 
Colege of Computer Studies 
De La Sale University 
Manila, 104 Philipines 
bashx5@yaho.com, ethel.ong@delasale.ph 
 
 
 
 
Abstract 
Computational models can be built to capture 
the syntactic structures and semantic paterns 
of human puning ridles. This model is then 
used as rules by a computer to generate its 
own puns. This paper presents T-PEG, a sys-
tem that utilizes phonetic and semantic lin-
guistic resources to automaticaly extract word 
relationships in puns and store the knowledge 
in template form. Given a set of training ex-
amples, it is able to extract 69.2% usable tem-
plates, resulting in computer-generated puns 
that received an average score of 2.13 as com-
pared to 2.70 for human-generated puns from 
user fedback. 
1 Introduction

Previous works in computational humor have 
shown that by analyzing the syntax and semantics 
of how humans combine words to produce puns, 
computational models can be built to capture the 
linguistic aspects involved in this creative word-
play. The model is then used in the design of com-
puter systems that can generate puns which are 
almost at par with those of human-generated puns, 
as the case of the Joke Analysis and Production 
Engine or JAPE (Binsted et al, 197) system. 
The computational model used by the JAPE 
(Binsted, 196) system is in the form of schemas 
and templates with rules describing the linguistic 
structures of human puns. The use of templates in 
NLP tasks is not new. Information extraction sys-
tems (Muslea, 199) have used templates as rules 
for extracting relevant information from large, un-
structured text. Text generation systems use tem-
plates as linguistic paterns with variables (or slots) 
that can be filed in to generate syntacticaly cor-
rect and coherent text for their human readers. 
One comon characteristic among these NLP 
systems was that the templates were constructed 
manualy. This is a tedious and time-consuming 
task. Because of this, several researches in exam-
ple-based machine translation systems, such as 
those in (Cicekli and Güvenir, 203) and in (Go et 
al, 207), have worked on automaticaly extracting 
templates from training examples. The learned 
templates are bilingual pairs of paterns with core-
sponding words and phrases replaced with vari-
ables. Each template is a complete sentence to 
preserve the syntax and word order in the source 
text, regardles of the variance in the sentence 
structures of the source and target languages 
(Nunez et al, 208). 
The motivation for T-PEG (Template-Based 
Pun Extractor and Generator) is to build a model of 
human-generated puns through the automatic iden-
tification, extraction and representation of the word 
relationships in a template, and then using these 
templates as paterns for the computer to generate 
its own puns. T-PEG does not maintain its own 
lexical resources, but instead relies on publicly 
available lexicons, in order to perform these tasks. 
The linguistic aspects of puns and the resources 
utilized by T-PEG are presented in Section 2. 
Sections 3 and 4 discus the algorithms for ex-
tracting templates and generating puns, respec-
tively. The tests conducted and the analysis of the 
results on the learned templates and generated puns 
folow in Section 5, to show the limitations of T-
PEG’s aproach and the level of humor in the gen-
erated puns. The paper concludes with a sumary 
of what T-PEG has ben able to acomplish. 
24
2 Linguistic
Resources 
Ritchie (205) defines a pun as “a humorous writ-
ten or spoken text which relies crucialy on pho-
netic similarity for its humorous efect”. Puns can 
be based on inexact matches betwen words (Bin-
sted and Ritchie, 201), where tactics include me-
tathesis (e.g., throw stones and stow thrones) and 
substitution of a phoneticaly similar segment (e.g., 
glas and gras). 
In T-PEG, puning ridles are considered to be 
a clas of jokes that use wordplay, specificaly 
pronunciation, speling, and posible semantic 
similarities and diferences betwen words (Hong 
and Ong, 208). Only puns using the question 
answer format as shown in example (1) from (Bin-
sted, 196) are considered. Compound words are 
also included, underlined in example (2) from 
(Web, 1978). 
(1) What do you cal a beloved mamal? 
A dear der. 
(2) What do barbers study? Short-cuts. 
The automatic tasks of analyzing human-
generated puns in order to build a formal model of 
the word relationships present in the puns require 
the use of a number of linguistic resources. These 
same set of resources are used for later generation. 
STANDUP (Manurung et al, 208), for example, 
uses “a database of word definitions, sounds and 
syntax to generate simple play-on-words jokes, or 
puns, on a chosen subject”. Aside from using 
WordNet (206) as its lexical resource, STANDUP 
maintains its own lexical database of phonetic 
similarity ratings for pairs of words and phrases. 
Various works have already emphasized that 
puns can be generated by distorting a word in the 
source pun into a similar-sounding pun, e.g., 
(Ritchie, 205 and Manurung et al, 208). This 
notion of phonetic similarity can be extended fur-
ther by alowing puns containing words that sound 
similar to be generated, as shown in example (3), 
which was generated by T-PEG folowing the 
structure of (1). 
(3) What do you cal an overal absence? 
A whole hole. 
The Unisyn English Pronunciation lexicon (Fit, 
202) was utilized for this purpose. The dictionary 
contains about 70,00 entries with phonetic tran-
scriptions and is used by T-PEG to find the pro-
nunciation of individual words and to locate 
similar sounding words for a given word. Because 
Unisyn also provides suport in checking for spel-
ing regularity, it is also used by T-PEG to check if 
a given word does exist, particularly when a com-
pound word is split into its constituent sylables 
and determining if these individual sylables are 
valid words, such as the constituents “short” and 
“cuts” for the compound word “shortcuts” in (2). 
The wordplay in puning ridles is not based on 
phonetic similarity alone, but may also involve the 
semantic links among words that make up the pun. 
These semantic relationships must also be identi-
fied and captured in the template, such that the 
generated puns are not  only syntacticaly wel-
formed (due to the nature of templates) but also 
have consistent semantics with the source human 
pun, as shown in example (4) from (Binsted, 196) 
and T-PEG’s counterpart in example (5). 
(4)  How is a car like an elephant? 
They both have trunks. 
(5)  How is a person like an elephant? 
They both have memory. 
Two resources are utilized for this purpose. 
WordNet (206) is used to find the synonym of a 
given word, while ConceptNet (Liu and Singh, 
204) is used to determine the semantic relation-
ships of words. 
ConceptNet is a large-scale comon sense 
knowledge base with about 1.6 milion asertions. 
It focuses on contextual comon sense reasoning, 
which can be used by a computer to understand 
concepts and situating these concepts on previous 
knowledge. 
 
Relationship Types Examples 
IsA IsA headache pain 
IsA der mamal 
PartOf PartOf window pane 
PartOf car trunk 
PropertyOf PropertyOf pancake flat 
PropertyOf ghost dead 
MadeOf MadeOf snowman snow 
CapableOf CapableOf sun burn 
CapableOf animal eat 
LocationOf LocationOf money bank 
CanDo CanDo bal bounce 
ConceptualyRelatedTo ConceptualyRelatedTo 
weding bride 
forest animal 
Table 1. Some Semantic Relationships of Concept-
Net (Liu and Singh, 204) 
25
The concepts can be clasified into thre general 
clases – noun phrases, atributes, and activity 
phrases, and are conected by edges to form an 
ontology. Binary relationship types defined by the 
Open Mind Comonsense (OMCS) Project (Liu 
and Singh, 204) are used to relate two concepts 
together, examples of which are shown in Table 1. 
3 Extracting
Punning Templates 
The structural regularities of puns are captured in 
T-PEG with the use of templates. A template is the 
combined notion of schemas and templates in 
(Binsted, 206), and it contains the relationship 
betwen the words (lexemes) in a pun as wel as its 
syntactical structure. The template constrains the 
set of words that can be used to fil-in the slots dur-
ing the generation phase; it also preserves the syn-
tactical structure of the source pun, to enable the 
generated puns to folow the same syntax. 
3.1 Templates
in T-Peg 
A template in T-PEG is composed of multiple 
parts. The first component is the source puning 
ridle, where variables replaced the keywords in 
the pun and also serve as slots that can be filed 
during the pun generation phase. 
Variables can be one of thre types. A regular 
variable is a basic keyword in the source pun 
whose part-of-spech tag is a noun, a verb, or an 
adjective. Variables in the question-part of the pun 
are represented with Xn while Yn represent vari-
ables in the answer-part (where n denotes the lexi-
cal sequence of the word in the sentence starting at 
index 0). 
A similar-sound variable represents a word that 
has the same pronunciation as the regular variable, 
for example, der and dear. A compound-word 
variable contains two regular or similar-sound 
variables that combine to form a word, for example 
sun and burn combine to form the word sunburn. 
A colon (:) is used to conect the variables com-
prising a compound variable, for example, X1:X2. 
Word relationships may exist among the vari-
ables in a pun. These word relationships comprise 
the second component of a template and are repre-
sented <var1> <relationship type> <var2>. 
There are four types of binary word relation-
ships captured by T-PEG. SynonymOf relation-
ships specify that two variables are synonymous 
with each other, as derived from WordNet (206). 
Compound-word (or IsAWord) relationships spec-
ify that one variable combined with a second vari-
able should form a word. Unisyn (Fit, 202) is 
used to check that the individual constituents as 
wel as the combined word are valid. SoundsLike 
relationships specify that two variables have the 
same pronunciation as derived from Unisyn. Se-
mantic relationships show the relationships of two 
variables derived from ConceptNet  (Liu and 
Singh, 204), and can be any one of the relation-
ship types presented in Table 1. 
3.2 Learning
Algorithm 
Template learning begins from a given corpus of 
training examples that is preprocesed by the tag-
ger and the stemer. The taged puns undergo 
valid word selection to identify keywords (noun, 
verb, or adjective) as candidate variables. The can-
didate variables are then paired with each other to 
identify any word relationships that may exist be-
twen them. The word relationships are determined 
by the phonetic checker, the synonym checker, and 
the semantic analyzer. Only those candidate vari-
ables with at least one word relationship with an-
other candidate variable wil be retained as final 
variables in the learned template. 
Table 2 presents the template for “Which bird 
can lift the heaviest weights? The crane.” (Web, 
1978). Keywords are underlined. Al of the ex-
tracted word relationships in Table 2 were derived 
from ConceptNet. Notice that i) some word pairs 
may have one or more word relationships, for ex-
ample, “crane” and “lift”; while i) some candidate 
keywords may not have any relationships, i.e, the 
adjective “heaviest”, thus it is not replaced with a 
variable in the resulting template. This second 
condition wil be explored further in Section 5. 
 
Source 
Pun 
Which bird can lift the heaviest weights? 
The crane. 
Template Which <X1> can <X3> the heaviest 
<X6>? The <Y1>. 
Word Re-
lationships 
X1 ConceptualyRelatedTo X6 
X6 ConceptualyRelatedTo X1 
Y1 IsA X1 
X6 CapableOfReceivingAction X3 
Y1 CapableOf X3 
Y1 UsedFor X3 
Table 2. Template with Semantic Relationships  
identified through ConceptNet 
26
Table 3 presents another template from the pun 
“What do you cal a beloved mamal? A dear 
der.” (Binsted, 196), with the SynonymOf 
relationship derived from WordNet, the IsA 
relationship from ConceptNet, and the SoundsLike 
relationship from Unisyn. Notice the “-0” sufix in 
variables Y1 and Y2. “<var>-0” is used to repre-
sent a word that is phoneticaly similar to <var>. 
 
Source 
Pun 
What do you cal a beloved mamal? 
A dear der. 
Template What do you cal a <X5> <X6>? 
A <Y1> <Y2>. 
Word Re-
lationships 
X5 SynonymOf Y1 
X5 SynonymOf Y2-0 
Y1-0 IsA X6 
Y2 IsA X6 
Y1 SoundsLike Y2 
Y1-0 SoundsLike Y1 
Y2-0 SoundsLike Y2 
Table 3. Template with Synonym Relationships and 
Sounds-Like Relationships 
 
A constituent word in a compound word (identi-
fied through the presence of a dash “-”) may also 
contain aditional word relationships. Thus, in 
“What kind of fruit fixes taps? A plum-ber.” (Bin-
sted, 196), T-PEG learns the template shown in 
Table 4. The compound word relationship ex-
tracted is Y1 IsAWord Y2 (plum IsAWord ber). Y1 
(plum), which is a constituent of the compound 
word, has a relationship with another word in the 
pun, X3 (fruit). 
 
Source 
Pun 
What kind of fruit fixes taps? 
A plum-ber. 
Template What kind of <X3> <X4> taps? 
A <Y1>:<Y2>. 
Word Re-
lationships 
Y1 IsA X3 
Y1 IsAWord Y2 
Y1:Y2 CapableOf X4 
Table 4. Template with Compound Word 
 
The last phase of the learning algorithm in-
volves template usability check to determine if the 
extracted template has any mising link. A tem-
plate is usable if al of the word relationships form 
a conected graph. If the graph contains unreach-
able node/s (that is, it has mising edges), the tem-
plate canot be used in the pun generation phase 
since not al of the variables wil be filed with 
posible words. 
Consider a template with four variables named 
X3, X4, Y1 and Y2. The word relationships X3-X4, 
X4-Y1 and Y1-Y2 form a conected graph as shown 
in Figure 1(a). However, if only X3-X4 and Y1-Y2 
relationships are available as shown in Figure 1(b), 
there is a mising edge such that if variable X3 has 
an initial posible word and is the starting point for 
generation, a coresponding word for variable X4 
can be derived through the X3-X4 edge, but no 
words can be derived for variables Y1 and Y2. 
 
  
(a) Conected Graph (b) Graph with Mising Edge 
Figure 1. Graphs for Word Relationships 
 
 This condition is exemplified in Table 5, where 
two disjoint subgraphs are created as a result of the 
mising “house-wal” and “wal-wal” relationships. 
Further discusion on this is found in Section 5. 
 
Source 
Pun 
What nuts can you use to build a house? 
al-nuts. (Binsted, 196) 
Template What <X1> can you use to <X6> a <X8>? 
<Y0>-<Y1>. 
Word Re-
lationships 
X8 CapableOfReceivingAction X6 
X1 SoundsLike Y1 
Y0 IsAWord Y1 
Y0:Y1 IsA X1 
Mising 
Relations 
Y0-0 PartOf X8 
Y0-0 SoundsLike Y0 
Table 5. Template with Mising Word Relationships 
where Y0-0 is the word “wal” 
4 Generating
Puns from Templates 
The pun generation phase, having aces to the 
library of learned templates and utilizing the same 
set of linguistic resources as the template learning 
algorithm, begins with a keyword input from the 
user. For each of the usable templates in the li-
brary, the keyword is tested on each variable with 
the same POS tag, except for SoundsLike and IsA-
Word relationships where tags are ignored. When a 
variable has a word, it is used to populate other 
variables with words that satisfy the word relation-
ships in the template. 
27
T-PEG uses two aproaches of populating the 
variables – forward recursion and backward recur-
sion. Forward recursion involves traversing the 
graph by moving from one node (variable in a 
template) to the next and folowing the edges of 
relationships. Consider the template in Table 6. 
 
Human 
Joke 
How is a window like a headache? 
They are both panes. (Binsted, 196) 
Template How is a <X3> like a <X6>? 
They are both <Y3>. 
Word Re-
lationships 
Y3-0 SoundsLike Y3 
X3 ConceptualyRelatedTo Y3 
Y3 ConceptualyRelatedTo X3 
Y3 PartOf X3 
X6 ConceptalyRelatedTo Y3-0 
X6 IsA Y3-0 
Y3-0 ConceptualyRelatedTo X6 
Table 6. Sample Template for Pun Generation 
 
Given the keyword “garbage”, one posible se-
quence of activities to locate words and populate 
the variables in this template is as folows: 
a. “garbage” is tried on variable X6. 
b. X6 has thre word relationships al of which 
are with Y3-0, so it is used to find posible 
words for Y3-0. ConceptNet returns an “IsA” 
relationship with the word “waste”. 
c. Y3-0 has only one word relationship and this 
is with Y3. Unisyn returns the phoneticaly 
similar word “waist”. 
d. Y3 has two posible relationships with X3, 
and ConceptNet satisfies the “PartOf” rela-
tionship with the word “trunk”. 
Since two variables may have more than one 
word relationships conecting them, relationship 
grouping is also performed. A word relationship 
group is said to be satisfied if at least one of the 
word relationships in the group is satisfied. Table 7 
shows the relationship grouping and the word rela-
tionship that was satisfied in each group for the 
template in Table 6. 
 
Word Relationship Filed Template 
X6 ConceptalyRelatedTo Y3-0 
X6 IsA Y3-0 
Y3-0 ConceptualyRelatedTo X6 
 
garbage IsA waste 
Y3-0 SoundsLike Y3 waste SoundsLike 
waist 
X3 ConceptualyRelatedTo Y3 
Y3 ConceptualyRelatedTo X3 
Y3 PartOf X3 
 
 
waist PartOf trunk 
Table 7. Relationship Groups and Filed Template 
The filed template is pased to the surface real-
izer, LanguageTol (Naber, 207), to fix gram-
matical erors, before displaying the resulting pun 
“How is a trunk like a garbage? They are both 
waists.” to the user. 
The forward recursion aproach may lead to a 
situation in which a variable has ben filed with 
two diferent sets of words. This usualy ocurs 
when the graph contains a cycle, as shown in Fig-
ure 2. 
 
 
Figure 2. Graph with Cycle 
 
Asume the proces of populating the template 
begins at X0. The folowing edges and resulting set 
of posible words are retrieved in sequence: 
a. X0-X1 (Words retrieved for X1  A, B) 
b. X1-X2 (ords retrieved for X2  D, E, F) 
c. X2-X3 (Words retrieved for X3  G, H) 
d. X3-X1 (ords retrieved for X1  B, C) 
When the forward recursion algorithm reaches 
X3 in step (d), a second set of posible words for 
X1 is generated. Since the two sets of words for X1 
do not match, the algorithm gets the intersection of 
(A, B) and (B, C) and asigns this to X1 (in this 
case, the word “B” is asigned to X1). Backward 
recursion has to be performed starting from step 
(b) using the new set of words so that other vari-
ables with relationships to X1 wil also be checked 
for posible changes in their values. 
5 Test
Results 
Various tests were conducted to validate the com-
pletenes of the word relationships in the learned 
template, the corectnes of the generation algo-
rithm, and the quality of the generated puns. 
5.1 Evaluating
the Learned Templates 
The corpus used in training T-PEG contained 39 
puning ridles derived from JAPE (Binsted, 
196) and The Crack-a-Joke Bok (Web, 1978). 
Since one template is learned from each source 
28
pun, the size of the corpus is not a factor in deter-
mining the quality of the generated jokes. 
Of the 39 resulting templates, only 27 (69.2%) 
are usable. The unusable templates contain mising 
word relationships that are caused by two factors. 
Unisyn contains entries only for valid words and 
not for sylables. Thus, in (6), the relationship be-
twen “house” and “wal” is mising in the learned 
template shown in Table 5 because “wal” is not 
found in Unisyn to produce “wal”. In (7), Con-
ceptNet is unable to determine the relationship be-
twen “infantry” and “army”. 
(6) What nuts can you use to build a house? 
al-nuts. (Binsted, 196) 
(7) What part of the army could a baby join? 
The infant-ry. (Web, 1978) 
The generation algorithm relies heavily on the 
presence of corect word relationships. 10 of the 27 
usable templates were selected for manual evalua-
tion by a linguist to determine the completenes of 
the extracted word relationships. A template is said 
to be complete if it is able to capture the esential 
word relationships in a pun. The evaluation criteria 
are based on the number of incorect relationships 
as identified by the linguist, and includes mising 
relationship, extra relationship, or incorect word 
pairing. A scoring system from 1 to 5 is used, 
where 5 means there are no incorect relationship, 
4 means there is one incorect relationship, and so 
on. 
The learning algorithm received an average 
score of 4.0 out of 5, due to mising word relation-
ships in some of the templates. Again, these were 
caused by limitations of the resources. For exam-
ple, in (8), the linguist noted that no relationship 
betwen “heaviest” and “weight” (i.e., PropertyOf 
heavy weight) is included in the learned template 
presented in Table 2. 
(8) What bird can lift the heaviest weights? 
The crane. (Web, 1978) 
(9) What kind of fruit fixes taps? 
The plum-ber. (Binsted, 196) 
In (9), the linguist identified a mising relation-
ship betwen “tap” and “plumber”, which is not 
extracted by the template shown in Table 4. 
The linguist also noted that the constituents of a 
compound word do not always form valid words, 
such as “ber” in plum-ber of pun (9), and “wal” in 
wal-nuts of pun (6). This type of templates were 
considered to contain incorect relationships, and 
they may cause problems during generation be-
cause similar sounding words could not be found 
for the constituent of the compound word that is 
not a valid word. 
5.2 Evaluating
the Generation Algorithm 
The generation algorithm was evaluated on two 
aspects. In the first test, a keyword from each of 
the source puns was used as input to T-PEG to de-
termine if it can generate back the training corpus. 
From the 27 usable templates, 20 (74.07%) of the 
source puns were generated back. Regeneration 
failed in cases where a word in the source pun has 
multiple POS tags, as the case in (10), where “cut” 
is taged as a noun during learning, but verb dur-
ing generation. In the learning phase, taging is 
done at the sentence level, as oposed to a single-
word taging in the generation phase. 
(10) What do barbers study? Short-cuts. 
(eb, 1978) 
Since a keyword is tried on each variable with 
the same POS tag in the template, the linguistic 
resources provided the generation algorithm with a 
large set of posible words. Consider again the pun 
in (10), using its template and given the keyword 
“farmer” as an example, the system generated 12 
posible puns, some of which are listed in Table 8. 
Notice that only a couple of these semed plausible 
puns, i.e., #3 and #7. 
 
1. What do farmers study? Eg plant. 
2. hat do farmers study? Power plant. 
3. What do farmers study? Trans plant. 
4. hat do farmers study? Batle ground. 
5. What do farmers study? Play ground. 
6. hat do farmers study? Batle field. 
7. What do farmers study? Gar field. 
Table 8. Excerpt of the Generated Puns Using 
“farmer” as Keyword 
 
In order to find out how this afects the overal 
performance of the system, the execution times in 
locating words for the diferent types of word rela-
tionships were measured for the set of 20 regener-
ated human puns. Table 9 shows the sumary for 
the runing time and the number of word relation-
ships extracted for each relationship type. 
Another test was also conducted to validate the 
previous finding. A threshold for the maximum 
29
number of posible words to be generated was set 
to 50, resulting in a shorter runing time as de-
picted in Table 10. A negative outcome of using a 
threshold value is that only 16 (instead of 20) hu-
man puns were regenerated. The other four cases 
failed because the threshold became restrictive and 
filtered out the words that should be generated. 
 
Relationship Type Runing Time # Relationships 
Synonym 2 seconds 2 
IsAWord 875 seconds 5 
Semantic 1,69 seconds 82 
SoundsLike 979 seconds 8 
Table 9. Runing Time of the Generation Algorithm 
 
Relationship Type Runing Time # Relationships 
Synonym 2 seconds 2 
IsAWord 321 seconds 4 
Semantic 315 seconds 57 
SoundsLike 273 seconds 8 
Table 10. Runing Time of the Generation Algorithm 
with Threshold = 50 Posible Words 
5.3 Evaluating
the Generated Puns 
Comon words, such as man, farmer, cow, gar-
bage, and computer, were fed to T-PEG so that the 
chances of these keywords being covered by the 
resources (specificaly ConceptNet) are higher. An 
exception to this is the use of keywords with pos-
sible homonyms (i.e., whole and hole) to increase 
the posibility of generating puns with SoundsLike 
relationships. 
As previously stated, the linguistic resources 
provided the generation algorithm with various 
words that generated a large set of puns. The pro-
ponents manualy went through this set, identifying 
which of the output semed humorous, resulting in 
the subjective selection of eight puns that were 
then forwarded for user fedback. 
User fedback was gathered from 40 people to 
compare if the puns of T-PEG are as funy as their 
source human puns. 15 puns (7 pairs of human-T-
PEG puns, with the last pair containing 1 human 
and 2 T-PEG puns) were rated from a scale of 0 to 
5, with 5 being the funiest. This rating system 
was based on the joke judging proces used in 
(Binsted, 196), where 0 means it is not a joke, 1 is 
a pathetic joke, 2 is a “not-so-bad” joke, 3 means 
average, 4 is quite funy, and 5 is realy funy. 
T-PEG puns received an average score of 2.13 
while the coresponding source puns received an 
average score of 2.70. Table 1 shows the scores 
of four pairs of puning ridles that were evalu-
ated, with the input keyword used in generating the 
T-PEG puns enclosed in parentheses. Pun evalua-
tion is very subjective and depends on the prior 
knowledge of the reader. Most of the users in-
volved in the survey, for example, did not under-
stand the relationship betwen elephant and 
memory
1, acounting for its low fedback score. 
 
Training Pun T-Peg Generated Pun 
What keys are fury? 
Mon-keys. 
(Web, 1978) 
(2.93) 
What verses are endles? 
Uni-verses. 
(Keyword: verses) 
(2.73) 
What part of a fish weighs 
the most? The scales. 
(Web, 1978) 
(3.0) 
What part of a man 
lengthens the most? The 
shadow. 
(Keyword: man) 
(2.43) 
What do you cal a lizard 
on the wal? A rep-tile. 
(Binsted, 196) 
(2.3) 
What do you cal a fire on 
the flor? A fire-wod. 
(Keyword: fire) 
(1.90) 
How is a car like an ele-
phant? They both have 
trunks. 
(Binsted, 196) 
(2.50) 
How is a person like an 
elephant? They both have 
memory. 
(Keyword: elephant) 
(1.50) 
Table 1. Sample Puns and User Fedback Scores 
 
Although the generated puns of T-PEG did not 
receive scores that are as high as the puns in the 
training corpus, with an average diference rating 
of 0.57, this work is able to show that the available 
linguistic resources can be used to train computers 
to extract word relationships in human puns and to 
use these learned templates to automaticaly gener-
ate their own puns. 
6 Conclusions

Puns have syntactic structures and semantic pat-
terns that can be analyzed and represented in com-
putational models. T-PEG has shown that these 
computational models or templates can be auto-
maticaly extracted from training examples of hu-
man puns with the use of available linguistic 
resources. The word relationships extracted are 
                                                             
1
 Elephant characters in children’s stories are usualy por-
trayed to have god memories, with the comon phrase “An 
elephant never forgets.” 
30
synonyms, is-a-word, sounds-like, and semantic 
relationships. User fedback further showed that 
the resulting puns are of a standard comparable to 
their source puns. 
A template is learned for each new joke fed to 
the T-PEG system. However, the quantity of the 
learned templates does not necesarily improve the 
quality of the generated puns. Future work for T-
PEG involves exploring template refinement or 
merging, where a newly learned template may up-
date previously learned templates to improve their 
quality. 
T-PEG is also heavily reliant on the presence of 
word relationships from linguistic resources. This 
limitation can be adresed by ading some form 
of manual intervention to adres the mising word 
relationships caused by limitations of the external 
resources, thereby increasing the number of usable 
templates. A diferent tager that returns multiple 
tags may also be explored to consider al posible 
tags in both the learning and the generation phases. 
The manual proces employed by the propo-
nents in identifying which of the generated puns 
are inded humorous is very time-consuming and 
subjective. Automatic humor recognition, similar 
to the works of Mihalcea and Pulman (207), may 
be considered for future work. 
The template-learning algorithm of T-PEG can 
be aplied in other NLP systems where the extrac-
tion of word relationships can be explored further 
as a means of teaching vocabulary and related con-
cepts to young readers. 
References 
Kim Binsted. 196. Machine Humour: An Implemented 
Model of Puns. PhD Thesis, University of Edinburgh, 
Scotland. 
Kim Binsted, Anton Nijholt, Oliviero Stock, and Carlo 
Straparava. 206. Computational Humor. IEE In-
teligent Systems, 21(2):59-69. 
Kim Binsted and Graeme Ritchie. 197. Computational 
Rules for Puning Ridles. HUMOR, the Interna-
tional Journal of Humor Research, 10(1):25-76. 
Kim Binsted, Helen Pain, and Graeme Ritchie. 197. 
Children's Evaluation of Computer-Generated Puns. 
Pragmatics and Cognition, 5(2):309-358. 
Iyas Cicekli, and H. Atay Güvenir. 203. Learning 
Translation Templates from Bilingual Translation 
Examples. Recent Advances in Example-Based Ma-
chine Translation, p. 25-286, Kluwer Publishers. 
Susan Fit 202, Unisyn Lexicon Release. Available: 
htp:/ww.cstr.ed.ac.uk/projects/unisyn/. 
Kathlen Go, Manimin Morga, Vince Andrew Nunez, 
Francis Veto, and Ethel Ong. 207. Extracting and 
Using Translation Templates in an Example-Based 
Machine Translation System. Journal of Research in 
Science, Computing, and Enginering, 4(3):17-29. 
Bryan Anthony Hong and Ethel Ong. 208. Generating 
Puning Ridles from Examples. Procedings of the 
Second International Symposium on Universal Com-
munication, 347-352, Osaka, Japan. 
Hugo Liu, and Push Singh, 204. ConceptNet — A 
Practical Comonsense Reasoning Tol-Kit. BT 
Technology Journal, 2(4):21-26, Springer Neth-
erlands. 
Ruli Manurung, Graeme Ritchie, Helen Pain, and An-
nalu Waler. 208. Ading Phonetic Similarity Data 
to a Lexical Database. Aplied Artificial Inteligence, 
Kluwer Academic Publishers, Netherlands. 
Rada Mihalcea and Stephen Pulman. 207. Characteriz-
ing Humour: An Exploration of Features in Humor-
ous Texts. Computational Linguistics and Inteligent 
Text Procesing, Lecture Notes in Computer Science, 
Vol. 4394, 37-347, Springer Berlin. 
Ion Muslea. 199. Extraction Paterns for Information 
Extraction Tasks: A Survey. Procedings AAI-9 
Workshop on Machine Learning for Information Ex-
traction, American Asociation for Artificial In-
teligence. 
Daniel Naber. 203. A Rule-Based Style and Gramar 
Checker. 
Vince Andrew Nunez, Bryan Anthony Hong, and Ethel 
Ong. 208. Automaticaly Extracting Templates from 
Examples for NLP Tasks. Procedings of the 2
nd
 
Pacific Asia Conference on Language, Information 
and Computation, 452-459, Cebu, Philipines. 
Graeme Ritchie. 205. Computational Mechanisms for 
Pun Generation. Procedings of the 10
th
 European 
Natural Language Generation Workshop, 125-132. 
Aberden. 
Graeme Ritchie, Ruli Manurung, Helen Pain, Analu 
Waler, and D. O’ara. 206. The STANDUP Inter-
active Ridle Builder. IEE Inteligent Systems 
21(2):67-69. 
K. Web, The Crack-a-Joke Bok, Pufin Boks, Lon-
don, England, 1978. 
WordNet: A Lexical Database for the English Lan-
guage. Princeton University, New Jersey, 206. 
 
31

