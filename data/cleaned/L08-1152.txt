<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Ella Bingham</author>
<author>Heikki Mannila</author>
</authors>
<title>Random projection in dimensionality reduction: applications to image and text data</title>
<date>2001</date>
<journal>ACM. ISBN</journal>
<booktitle>In KDD ’01: Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining</booktitle>
<pages>245--250</pages>
<location>New York, NY, USA</location>
<marker>Bingham, Mannila, 2001</marker>
<rawString>Ella Bingham and Heikki Mannila. Random projection in dimensionality reduction: applications to image and text data. In KDD ’01: Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, pages 245–250, New York, NY, USA, 2001. ACM. ISBN 1-58113-391-X. doi: http://doi.acm.org/10.1145/502512.502546.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation</title>
<date>2003</date>
<journal>Journal of Machine Learning Research</journal>
<volume>3</volume>
<contexts>
<context>ic models [Papadimitriou et al., 2000], and there are several related models built from distributional principles that are based on probabilistic rather than geometric insights (e.g., [Hofmann, 1999, Blei et al., 2003]). A thorough comparison of probabilistic and geometric points of view is beyond the scope of this paper: van Rijsbergen [2004] points out that quantum mechanics is already a clearly extant framework</context>
<context>s over geometric LSA. It is hard to evaluate the computational complexity of the family of EM algorithms generally, though they are often computationally demanding. Latent Dirichlet Allocation (LSA) [Blei et al., 2003]. Building on Hofmann’s work, LDA provides a probabilistic generative model that accounts for documents as being probabilistic mixtures of underlying topics, these being the latent variables of the m</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Brand</author>
</authors>
<title>Incremental singular value decomposition of uncertain data with missing values</title>
<date>2002</date>
<booktitle>In Proceedings of the European Conference on Computer Vision (ECCV</booktitle>
<marker>Brand, 2002</marker>
<rawString>Matthew Brand. Incremental singular value decomposition of uncertain data with missing values. In Proceedings of the European Conference on Computer Vision (ECCV), May 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>Francine Chen, and Ioannis Tsochantaridis. Topic-based document segmentation with probabilistic latent semantic analysis</title>
<date>2002</date>
<booktitle>In Conference on Information and Knowledge Management (CIKM</booktitle>
<pages>211--218</pages>
<marker>Brants, 2002</marker>
<rawString>Thorsten Brants, Francine Chen, and Ioannis Tsochantaridis. Topic-based document segmentation with probabilistic latent semantic analysis. In Conference on Information and Knowledge Management (CIKM), pages 211–218, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edgar Ch´avez</author>
<author>Gonzalo Navarro</author>
<author>Ricardo Baeza-Yates</author>
<author>Jos´e Luis Marroqu´ın</author>
</authors>
<title>Searching in metric spaces</title>
<date>2001</date>
<journal>ACM Comput. Surv</journal>
<volume>33</volume>
<pages>0360--0300</pages>
<marker>Ch´avez, Navarro, Baeza-Yates, Marroqu´ın, 2001</marker>
<rawString>Edgar Ch´avez, Gonzalo Navarro, Ricardo Baeza-Yates, and Jos´e Luis Marroqu´ın. Searching in metric spaces. ACM Comput. Surv., 33(3):273–321, 2001. ISSN 0360-0300. doi: http://doi.acm.org/10.1145/502807.502808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Dean</author>
<author>Sanjay Ghemawat</author>
</authors>
<title>Mapreduce: Simplified data processing on large clusters</title>
<date>2004</date>
<booktitle>In OSDI’04: Sixth Symposium on Operating System Design and Implementation</booktitle>
<location>San Francisco</location>
<contexts>
<context> orthogonal basic vectors that are generated independently, parts of the model can be created independently of one another. This enables distributed model creation using techniques such as MapReduce [Dean and Ghemawat, 2004], and incremental addition of new terms and documents to models without rebuilding them from scratch. This is crucial for any large-scale industrial application. Related to distributed or paralleliza</context>
</contexts>
<marker>Dean, Ghemawat, 2004</marker>
<rawString>Jeffrey Dean and Sanjay Ghemawat. Mapreduce: Simplified data processing on large clusters. In OSDI’04: Sixth Symposium on Operating System Design and Implementation, San Francisco, December 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Deerwester</author>
<author>Susan Dumais</author>
<author>George Furnas</author>
<author>Thomas Landauer</author>
<author>Richard Harshman</author>
</authors>
<title>Indexing by latent semantic analysis</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science</journal>
<volume>41</volume>
<contexts>
<context> have been examined and evaluated in the performance of several tasks of importance to natural language processing. Such applications of semantic vector models to date include: Information retrieval [Deerwester et al., 1990]. This was the original motivation for applying dimension reduction to a term-document matrix, in the hope of creating a more semantically aware search engine (e.g., a search engine that can locate d</context>
<context>nly yields an approximation to the original matrix A, the projection onto the basis given by the first m columns of V . Singular value decomposition is the algorithm used in latent semantic indexing [Deerwester et al., 1990] or latent semantic analysis Landauer and Dumais [1997]. For an m n matrix A, the full singular value decomposition usually takes time O(mn2 + m2n + n3) to compute Brand [2002]. This can be reduced i</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Scott Deerwester, Susan Dumais, George Furnas, Thomas Landauer, and Richard Harshman. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391–407, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas L Griffiths</author>
<author>Mark Steyvers</author>
<author>Joshua B Tenenbaum</author>
</authors>
<title>Topics in semantic representation</title>
<date>2007</date>
<journal>Psychological Review</journal>
<volume>114</volume>
<marker>Griffiths, Steyvers, Tenenbaum, 2007</marker>
<rawString>Thomas L. Griffiths, Mark Steyvers, and Joshua B. Tenenbaum. Topics in semantic representation. Psychological Review, 114(2):211–244, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Customizing a lexicon to better suit a computational task</title>
<date>1993</date>
<booktitle>In ACL SIGLEX Workshop</booktitle>
<location>Columbus, Ohio</location>
<marker>Hearst, Sch¨utze, 1993</marker>
<rawString>Marti Hearst and Hinrich Sch¨utze. Customizing a lexicon to better suit a computational task. In ACL SIGLEX Workshop, Columbus, Ohio, 1993. Thomas Hofmann. Probabilistic latent semantic analysis.</rawString>
</citation>
<citation valid="true">
<date>1999</date>
<booktitle>In Uncertainty in Artificial Intelligence (UAI’99</booktitle>
<location>Stockholm, Sweden</location>
<marker>1999</marker>
<rawString>In Uncertainty in Artificial Intelligence (UAI’99), Stockholm, Sweden, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pentti Kanerva</author>
</authors>
<title>Sparse Distributed Memory</title>
<date>1988</date>
<publisher>MIT Press</publisher>
<contexts>
<context>ingham and Mannila [2001]. The ease with which very simple distributed memory units can collaboratively learn and represent a semantic vectors has been noted for its potential cognitive significance [Kanerva, 1988]. Being strongly distributional and associative in character, they have complementary strengths to some of the more traditional formalist and symbolic semantic techniques such as those based on propo</context>
</contexts>
<marker>Kanerva, 1988</marker>
<rawString>Pentti Kanerva. Sparse Distributed Memory. MIT Press, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition</title>
<date>1997</date>
<journal>Psychological Review</journal>
<volume>104</volume>
<contexts>
<context> 1971, Salton and McGill, 1983]. Semantic vector models include a family of related models for representing concepts with vectors in a high dimensional vector space, such as Latent Semantic Analysis [Landauer and Dumais, 1997], Hyperspace Analogue to Language [Lund and Burgess, 1996], and WORDSPACE [Sch¨utze, 1998, Widdows, 2004, Sahlgren, 2006]. The main attractions of semantic vector models include: They can be built us</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas Landauer and Susan Dumais. A solution to Plato’s problem: The latent semantic analysis theory of acquisition. Psychological Review, 104(2):211–240, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lund</author>
<author>C Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior research methods, instruments and computers</title>
<date>1996</date>
<volume>28</volume>
<pages>203--208</pages>
<contexts>
<context>de a family of related models for representing concepts with vectors in a high dimensional vector space, such as Latent Semantic Analysis [Landauer and Dumais, 1997], Hyperspace Analogue to Language [Lund and Burgess, 1996], and WORDSPACE [Sch¨utze, 1998, Widdows, 2004, Sahlgren, 2006]. The main attractions of semantic vector models include: They can be built using entirely unsupervised distributional analysis of free </context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>K Lund and C Burgess. Producing high-dimensional semantic spaces from lexical co-occurrence. Behavior research methods, instruments and computers, 28(22): 203–208, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing</title>
<date>1999</date>
<publisher>The MIT Press</publisher>
<location>Cambridge, Massachusetts</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos H Papadimitriou</author>
</authors>
<title>Hisao Tamaki, Prabhakar Raghavan, and Santosh Vempala. Latent semantic indexing: A probabilistic analysis</title>
<date>2000</date>
<journal>J. Comput. Syst. Sci</journal>
<volume>61</volume>
<marker>Papadimitriou, 2000</marker>
<rawString>Christos H. Papadimitriou, Hisao Tamaki, Prabhakar Raghavan, and Santosh Vempala. Latent semantic indexing: A probabilistic analysis. J. Comput. Syst. Sci., 61(2):217–235, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Plate</author>
</authors>
<title>Holographic Reduced Representations: Distributed Representation for Cognitive Structures</title>
<date>2003</date>
<publisher>CSLI Publications</publisher>
<contexts>
<context>, sum, normalization), convolution products, and orthogonalization routines for vector negation and disjunction. Background on these mathematical operations and their linguistic uses can be found in [Plate, 2003, Widdows, 2003a, 2004, 2008] Vector File Formats SemanticVectors currently supports two different file formats for vector storage and I/O. The default is an optimized binary format that uses Lucene’s</context>
</contexts>
<marker>Plate, 2003</marker>
<rawString>Tony Plate. Holographic Reduced Representations: Distributed Representation for Cognitive Structures. CSLI Publications, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>An introduction to random indexing</title>
<date>2005</date>
<booktitle>In Proceedings of the Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering (TKE</booktitle>
<institution>SICS, Swedish Institute of Computer Science</institution>
<location>Copenhagen, Denmark</location>
<note>URL http://www. sics.se/˜mange/papers/RI_intro.pdf</note>
<contexts>
<context>“nearly orthogonal”, in a way that can be formally characterized. Thus it achieves a result that is for many purposes comparable to orthogonalization methods such as Singular Value Decomposition [see Sahlgren, 2005], but spends none of the computational resources. The basic procedure for creating a random basic document vector is extremely terse and easy to implement. Consider two vectors which contain mainly z</context>
</contexts>
<marker>Sahlgren, 2005</marker>
<rawString>Magnus Sahlgren. An introduction to random indexing. In Proceedings of the Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering (TKE), Copenhagen, Denmark, 2005. SICS, Swedish Institute of Computer Science. URL http://www. sics.se/˜mange/papers/RI_intro.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces</title>
<date>2006</date>
<tech>PhD thesis</tech>
<institution>Department of Linguistics, Stockholm University</institution>
<contexts>
<context> a high dimensional vector space, such as Latent Semantic Analysis [Landauer and Dumais, 1997], Hyperspace Analogue to Language [Lund and Burgess, 1996], and WORDSPACE [Sch¨utze, 1998, Widdows, 2004, Sahlgren, 2006]. The main attractions of semantic vector models include: They can be built using entirely unsupervised distributional analysis of free text. While they involve some nontrivial mathematical machinery</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces. PhD thesis, Department of Linguistics, Stockholm University, 2006.</rawString>
</citation>
<citation valid="true">
<date>1971</date>
<booktitle>The Smart Retrieval System – Experiments in Automatic Document Processing. Prentice-Hall</booktitle>
<editor>Gerard Salton, editor</editor>
<location>Englewood Cliffs, NJ</location>
<marker>1971</marker>
<rawString>Gerard Salton, editor. The Smart Retrieval System – Experiments in Automatic Document Processing. Prentice-Hall, Englewood Cliffs, NJ, 1971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael McGill</author>
</authors>
<title>Introduction to modern information retrieval</title>
<date>1983</date>
<publisher>McGraw-Hill</publisher>
<location>New York, NY</location>
<contexts>
<context>rchers in natural language processing over the past 15 years, though their invention can be traced at least to Salton’s introduction of the Vector Space Model for information retrieval [Salton, 1971, Salton and McGill, 1983]. Semantic vector models include a family of related models for representing concepts with vectors in a high dimensional vector space, such as Latent Semantic Analysis [Landauer and Dumais, 1997], Hy</context>
<context>classes for subsequently building document vectors as a weighted sum of the term vectors of their constituent terms, as has been done since early vector model information retrieval systems [see e.g., Salton and McGill, 1983]. These derived document vectors should not be confused with the basic random vectors for each document that are used to create the term vectors in the first place. Searching Models Vectors are read </context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Gerard Salton and Michael McGill. Introduction to modern information retrieval. McGraw-Hill, New York, NY, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Ambiguity resolution in language learning</title>
<date>1997</date>
<publisher>CSLI Publications</publisher>
<location>Stanford CA</location>
<marker>Sch¨utze, 1997</marker>
<rawString>Hinrich Sch¨utze. Ambiguity resolution in language learning. CSLI Publications, Stanford CA, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination</title>
<date>1998</date>
<journal>Computational Linguistics</journal>
<volume>24</volume>
<pages>124</pages>
<note>URL citeseer.nj.nec.com/ schutze98automatic.html</note>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. Automatic word sense discrimination. Computational Linguistics, 24(1):97– 124, 1998. URL citeseer.nj.nec.com/ schutze98automatic.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lloyd N Trefethen</author>
<author>David Bau</author>
</authors>
<title>Numerical Linear Algebra</title>
<date>1997</date>
<publisher>S.I.A.M</publisher>
<contexts>
<context>to thank Prof. Trevor Cohen of Arizona State University for making this contribution. There are several other ways of reducing dimensions, including the following: Singular value decomposition (SVD) [Trefethen and Bau, 1997]. Taking a matrix A, an eigenvalue decomposition of the symmetric matrix AAT yields a factorization A = ^U V , where U is an orthonormal matrix and is a diagonal matrix with non-decreasing leading va</context>
</contexts>
<marker>Trefethen, Bau, 1997</marker>
<rawString>Lloyd N. Trefethen and David Bau. Numerical Linear Algebra. S.I.A.M., 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<title>The Geometry of Information Retrieval</title>
<date>2004</date>
<publisher>Cambridge University Press</publisher>
<marker>van Rijsbergen, 2004</marker>
<rawString>C.J. van Rijsbergen. The Geometry of Information Retrieval. Cambridge University Press, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Geometry and Meaning. CSLI publications</title>
<date>2004</date>
<location>Stanford, California</location>
<contexts>
<context>with vectors in a high dimensional vector space, such as Latent Semantic Analysis [Landauer and Dumais, 1997], Hyperspace Analogue to Language [Lund and Burgess, 1996], and WORDSPACE [Sch¨utze, 1998, Widdows, 2004, Sahlgren, 2006]. The main attractions of semantic vector models include: They can be built using entirely unsupervised distributional analysis of free text. While they involve some nontrivial mathem</context>
<context>onals, without increasing complexity ever further with yet more places for yet more information. Potential ways to explore data offered by semantic vector models include clustering and visualization [Widdows, 2004, Ch 6], and there is some excitement about using these technologies to enable the linking and bundling of complementary technologies. 6. Conclusion It is by now clear that a variety of distributional</context>
</contexts>
<marker>Widdows, 2004</marker>
<rawString>Dominic Widdows. Geometry and Meaning. CSLI publications, Stanford, California, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Orthogonal negation in vector spaces for modelling word-meanings and document retrieval</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<location>Sapporo, Japan</location>
<contexts>
<context>ly aware search engine (e.g., a search engine that can locate documents based on synonyms and related terms as well as matching keywords). Lexical and ontology acquisition [Hearst and Sch¨utze, 1993, Widdows, 2003b]. The core principle here is that knowledge of a few seed words and their relationships can help to infer analogous relationships for other similar words that are nearby in the semantic vector space</context>
<context>ization), convolution products, and orthogonalization routines for vector negation and disjunction. Background on these mathematical operations and their linguistic uses can be found in [Plate, 2003, Widdows, 2003a, 2004, 2008] Vector File Formats SemanticVectors currently supports two different file formats for vector storage and I/O. The default is an optimized binary format that uses Lucene’s very fast seri</context>
</contexts>
<marker>Widdows, 2003</marker>
<rawString>Dominic Widdows. Orthogonal negation in vector spaces for modelling word-meanings and document retrieval. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), Sapporo, Japan, 2003a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Unsupervised methods for developing taxonomies by combining syntactic and statistical information</title>
<date>2003</date>
<booktitle>In Proceedings of Human Langauge Technology / North American Chapter of the Association for Computational Linguistics</booktitle>
<location>Edmonton, Canada</location>
<contexts>
<context>ly aware search engine (e.g., a search engine that can locate documents based on synonyms and related terms as well as matching keywords). Lexical and ontology acquisition [Hearst and Sch¨utze, 1993, Widdows, 2003b]. The core principle here is that knowledge of a few seed words and their relationships can help to infer analogous relationships for other similar words that are nearby in the semantic vector space</context>
<context>ization), convolution products, and orthogonalization routines for vector negation and disjunction. Background on these mathematical operations and their linguistic uses can be found in [Plate, 2003, Widdows, 2003a, 2004, 2008] Vector File Formats SemanticVectors currently supports two different file formats for vector storage and I/O. The default is an optimized binary format that uses Lucene’s very fast seri</context>
</contexts>
<marker>Widdows, 2003</marker>
<rawString>Dominic Widdows. Unsupervised methods for developing taxonomies by combining syntactic and statistical information. In Proceedings of Human Langauge Technology / North American Chapter of the Association for Computational Linguistics, Edmonton, Canada, 2003b.</rawString>
</citation>
</citationList>
</algorithm>

