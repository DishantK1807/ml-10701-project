For more information on these models, please refer to Brown et al.[1993]. Again, we use the heuristics from the Section 4.5 to reconcile the mono-directional alignments obtained through training parameters using models of increasing complexity.
J93-2003
The extraction heuristic is similar to the one used in the alignment template work by Och et al.[1999]. A number of researchers have proposed to focus on the translation of phrases that have a linguistic motivation [Yamada and Knight, 2001; Imamura, 2002].
P01-1067 W99-0604
Note that phrase translation with a lexical weight is a special case of the alignment template model [Och et al., 1999] with one word class for each word.
W99-0604
We recombine search hypotheses as done by Och et al.[2001]. While this reduces the number of hypotheses stored in each stack somewhat, stack size is exponential with respect to input sentence length.
W01-1408
Och et al.[1999]’s alignment template model can be reframed as a phrase translation system; Yamada and Knight [2001] use phrase translation in a syntaxbased translation system; Marcu and Wong [2002] introduced a joint-probability model for phrase translation; and the CMU and IBM word-based statistical machine translation systems1 are augmented with phrase translation capability.
P01-1067 W02-1018 W99-0604
from Phrase Alignments Marcu and Wong [2002] proposed a translation model that assumes that lexical correspondences can be established not only at the word level, but at the phrase level as well.
W02-1018
The base heuristic [Och et al., 1999] proceeds as follows: We start with intersection of the two word alignments.
W99-0604
The third method for comparison is the joint phrase model proposed by Marcu and Wong [2002].
W02-1018
In fact, imposing syntactic restrictions on phrases, as used in recently proposed syntax-based translation models [Yamada and Knight, 2001], proves to be harmful.
P01-1067
As the first method, we learn phrase alignments from a corpus that has been word-aligned by a training toolkit for a word-based translation model: the Giza++ [Och and Ney, 2000] toolkit for the IBM models [Brown et al., 1993].
J93-2003 P00-1056
To identify these, we use a word-aligned corpus annotated with parse trees generated by statistical syntactic parsers [Collins, 1997; Schmidt and Schulte im Walde, 2000].
C00-2105 P97-1003
We also included in the figure the performance of an IBM Model 4 wordbased translation system (M4), which uses a greedy decoder [Germann et al., 2001].
P01-1030
We collect all aligned phrase pairs that are consistent with the word alignment: The words in a legal phrase pair are only aligned to each other, and not to words outside [Och et al., 1999].
W99-0604
Expectation Maximization learning in Marcu and Wong’s framework yields both (i) a joint probability distribution a9 a5 a1 a8 a0 a1 a2 a10, which reflects the probability that phrases a1 a8 and a1 a2 are translation equivalents; (ii) and a joint distribution a10 a5a47a19 a0a7a6 a10, which reflects the probability that a phrase at position a19 is translated into a phrase at position a6 . To use this model in the context of our framework, we simply marginalize to conditional probabilities the joint probabilities estimated by Marcu and Wong [2002].
W02-1018
We then parse both sides of the corpus with syntactic parsers [Collins, 1997; Schmidt and Schulte im Walde, 2000].
C00-2105 P97-1003
Another motivation to evaluate the performance of a phrase translation model that contains only syntactic phrases comes from recent efforts to built syntactic translation models [Yamada and Knight, 2001; Wu, 1997].
J97-3002 P01-1067
