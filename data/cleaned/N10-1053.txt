Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 353–356,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics
TheEffectofAmbiguityontheAutomatedAcquisitionofWSDExamples
MarkStevensonandYikunGuo
Departmentof ComputerScience,
Universityof Sheffield,
RegentCourt,211 Portobello,
Sheffield,S1 4DP
UnitedKingdom
m.stevenson@dcs.shef.ac.ukandg.yikun@dcs.shef.ac.uk
Abstract
Several methods for automatically gen-
erating labeled examples that can be
used as training data for WSD systems
have been proposed, including a semi-
supervised approach based on relevance
feedback(Stevenson et al., 2008a). This
approachwas shown to generateexamples
that improved the performanceof a WSD
systemfor a set of ambiguoustermsfrom
the biomedicaldomain.However, we find
thatthisapproachdoesnotperformas well
on other data sets. The levels of ambigu-
ity in these data sets are analysedand we
suggestthis is the reasonfor this negative
result.
1 Introduction
Several studies, for example (Mihalcea et al.,
2004; Pradhanet al., 2007), have shown that su-
pervisedapproachesto Word Sense Disambigua-
tion (WSD) outperformunsupervisedones. But
these rely on labeledtrainingdata which is diffi-
cult to createand not always available(e.g. (Wee-
ber et al., 2001)). Varioustechniquesfor creating
labeledtrainingdataautomaticallyhave beensug-
gested in the literature. Stevenson et al. (2008a)
describea semi-supervisedapproachthatusedrel-
evance feedback(Rocchio, 1971) to analyse ex-
isting labeled examples and use the information
producedto generatefurtherones. The approach
was testedon the biomedicaldomainandthe addi-
tionalexamplesfoundto improve performanceof
a WSD system. However, biomedicaldocuments
represent a restricted domain. In this paper the
same approachis testedagainst two data sets that
are not limitedto a singledomain.
2 ApplicationtoaRangeofDataSets
In this paperthe relevance feedbackapproachde-
scribedbyStevensonet al.(2008a)is evaluatedus-
ing three data sets: the NLM-WSDcorpus(Wee-
ber et al., 2001) which Stevenson et al. (2008a)
used for their experiments,the Senseval-3lexical
sampletask(Mihalceaet al., 2004)andthecoarse-
grained version of the SemEval English lexical
sampletask (Pradhanet al., 2007).
2.1 GeneratingExamples
To generateexamplesfor a particularsense of an
ambiguous term all of the examples where the
term is used in that sense are consideredto be
“relevantdocuments”whiletheexamplesin which
any othersenseof the term is used are considered
to be “irrelevant documents”. Relevance feed-
back (Rocchio,1971) is used to generatea set of
query terms designed to identify relevant docu-
ments, and thereforeinstancesof the sense. The
top five query terms are used to retrieve docu-
ments and these are used as labeled examplesof
the sense. Further details of this processare de-
scribedby Stevensonet al. (2008a).
Thisprocessrequiresa collectionof documents
that can be queried to generate the additional
examples. For the NLM-WSD data set we
used PubMed, a database of biomedicaljournal
abstracts queried using the Entrez retrieval sys-
tem (http://www.ncbi.nlm.nih.gov/
sites/gquery). The British NationalCorpus
(BNC) was used for Senseval-3 and SemEval.1
Lucene(http://lucene.apache.org) was
usedto index the BNCand retrieve examples.
1We also experimentedwith the English WaCky corpus
(Baroniet al., 2009) which containsnearly 2 billion words
automaticallyretrieved fromthe web. However, resultswere
not as goodas whenthe BNCwas used.
353
2.2 WSDSystem
We use a WSD system that has been shown to
performwell when evaluatedagainst ambiguities
foundin both generaltext and the biomedicaldo-
main (Stevenson et al., 2008b). MedicalSubject
Headings(MeSH), a controlledvocabulary used
fordocumentindexing,areobtainedfromPubMed
andusedas additionalfeaturesfor the NLM-WSD
data set since they have been shown to improve
performance. The features are combined using
the Vector Space Model,a simplememory-based
learningalgorithm.
2.3 Experiment
Experimentswere carried out comparingperfor-
mance when the WSD system was trained using
either the examplesin the originaldata set (orig-
inal), the examples generated from these using
the relevancefeedbackapproach(additional) or a
combinationof these (combined). The Senseval-
3 and SemEval corporaare split into trainingand
test portionsso the trainingportionis used as the
original data set and the WSD system evaluated
against the held-backdata. As there is no such
recognisedstandardsplit for the NLM-WSDcor-
pus, 10-foldcross-validationwas used. For each
foldthetrainingportionis usedas theoriginaldata
set and automaticallygeneratedexamplescreated
by examiningjust that part of the data. Evaluation
is carried out against the fold’s test data and the
averageresultacrossthe 10 foldsreported.
Table 1 shows the results of this experiment.2
Examplesgeneratedusingthe relevance feedback
approachonlyimprove resultsfor onedataset, the
NLM-WSDcorpus. In this case there is a sig-
nificantimprovement(Mann-Whitney, p < 0.01)
whenthe originaland automaticallygeneratedex-
amplesare combined. There is no such improve-
ment for the othertwo data sets: WSDresultsus-
ing the additionaldata are noticeablyworse than
whenthe originaldatais usedaloneand,although
performanceimproves when these examples are
combinedwith the original data, results are still
lower than using the originaldata. When exam-
ples are combinedthere is a drop in performance
of 1.2%and 2.9%for SemEval and Senseval-3re-
2Results reported here for the NLM-WSD corpus are
slightly different from those reported by (Stevenson et al.,
2008a). We used an additionalfeature (MeSH headings),
which improved the baselineperformance,and more query
termswhichimproved the qualityof the additionalexamples
for all threedatasets.
spectively.
Corpus Original Additional Combined
NLM-WSD 87.9 87.6 89.2
SemEval 83.7 74.6 82.5
Senseval-3 68.8 56.3 65.9
Table 1: Resultsof relevance feedbackapproach
appliedto threedatasets
These results indicate that the relevance feed-
back approach described by Stevenson et al.(2008a)is notableto generateusefulexamplesfor
the Senseval-3 and SemEval data sets, althoughit
can for the NLM-WSDdata set. We hypothesise
thatthesecorporacontaindifferentlevels of ambi-
guitywhicheffect suitabilityof the approach.
3 AnalysisofAmbiguities
The three data sets are comparedusing measures
designedto determinethe level of ambiguitythey
contain. Section3.1 reportsresultsusing various
widelyusedmeasures basedon the distributionof
senses. Section 3.2 introducesa measure based
on the semantic similarity between the possible
sensesof ambiguousterms.
3.1 SenseDistributions
Threemeasuresfor characterisingthe difficultyof
WSD data sets based on their sense distribution
were used. The first is the widely applied most
frequentsense (MFS) baseline (McCarthy et al.,
2004),i.e. the proportionof examplesfor an am-
biguoustermthat are labeledwiththe commonest
sense. The second is number of senses per am-
biguousterm. The final measure,the entropy of
thesensedistribution,hasbeenshownto be a good
indicationof disambiguationdifficulty(Kilgarriff
and Rosenzweig,2000). For two of these mea-
sures(numberof sensesand entropy) a higherfig-
ure indicatesgreaterambiguitywhilefor the MFS
measurea lower figure indicates a more difficult
dataset.
Table 2 shows the results of computingthese
measures averaged across all terms in the cor-
pus. For two measures(numberof senses and en-
tropy) the NLM-WSDcorpusis least ambiguous,
Senseval-3 the mostambiguouswithSemEval be-
tweenthem. The MFS scoresare very similarfor
two data sets (NLM-WSDand SemEval), both of
whichare muchhigherthanfor Senseval-3.
354
These measures suggest that the NLM-WSD
corpus is less ambiguousthan the other two and
also that the Senseval-3 corpus is the most am-
biguousof the three.
Corpus MFS Senses Entropy
NLM-WSD 78.0 2.63 0.73
SemEval 78.4 3.60 0.91
Senseval-3 53.8 6.43 1.75
Table2: Propertiesof DataSetsusingsensedistri-
butionmeasures
3.2 SemanticSimilarity
We also developed a measurethat takes into ac-
countthe similarityin meaningbetweenthe possi-
ble sensesfor an ambiguousterm. Thismeasureis
similarto the one used by Passoneauet al. (2009)
to analyse levels of inter-annotatoragreementin
word sense annotation. Our measureis shown in
equation 1 where Senses is the set of possible
sensesfor an ambiguousterm, |Senses| = n andparenleftbig
Senses
2
parenrightbigis thesetof allsubsetsofSensescontain-
ing two of its members(i.e the set of unordered
pairs). The similarity between a pair of senses,
sim(x,y), canbe computedusingany lexicalsim-
ilaritymeasure,see Pedersenet al. (2004).Essen-
tiallythis measurecomputesthe meanof the sim-
ilaritiesbetweeneachpair of sensesfor the term.
simmeasure =
summationtext
{x,y}epsilon1(Senses2 )sim(x,y)parenleftbig
n
2
parenrightbig (1)
Oneproblemwithcomparingthe datasets used
here is that they use a range of sense invento-
ries. Although lexical similarity measures have
been applied to WordNet (Pedersenet al., 2004)
and UMLS(Pedersenet al., 2007), it is not clear
that the scoresthey producecan be meaningfully
compared. To avoid this problemwe mappedthe
senseinventoriesontoa single resource:WordNet
version3.0.
The mapping was most straightforward for
Senseval-3 which uses WordNet 1.7.1 and could
be automaticallymappedontoWordNet3.0senses
using publiclyavailable mappings(Daud´e et al.,
2000). The SemEval data contains a mapping
from the OntoNotessenses to groupsof WordNet
2.1 senses. The first sense from this group was
mappedto WordNet3.0 usingthe samemappings.
Mapping the NLM-WSD corpus was more
problematicandhadto be carriedout manuallyby
comparingsense definitionsin UMLSand Word-
Net 3.0. We had expectedthis processto be diffi-
cult but found clear mappingsfor the majority of
senses. Therewereeven foundcasesin whichthe
sensedefinitionswere identicalin both resources.
(The most likely reason for this is that some of
the resourcesthat are includedin the UMLSwere
usedto compileWordNet.)Another, moreserious,
problemis relatedto the annotationschemeused
in the NLM-WSDcorpus. If none of the possi-
ble senses in UMLSwere judgedto be appropri-
ate the annotatorscouldlabelthe senseas “None”.
We didnotmapthesesensessinceit wouldrequire
examiningeachinstanceto determinethemostap-
propriatesense or senses in WordNet and we ex-
pectedthis to be error prone. In addition,there is
noguaranteethatallof theinstancesof a particular
termlabeledwith“None”referto the samemean-
ing. All of the “None”senseswere removed from
theNLM-WSDdataset andany termswherethere
were more than ten instancesmarked as “None”
were also rejected from the similarity analysis.
This allowed us to compute the similarityscore
for just 20 examples(40% of the total) although
we felt thatthis was a large enoughsampleto pro-
videinsightinto the dataset.
TheWordNet::Similaritypackage(Ped-
ersen et al., 2004) was used to computesimilar-
ity scores. Results are reported for three of the
measuresin this package. (Other measurespro-
duced similarresults.) The simplepath measure
computesthe similaritybetweena pairof nodesin
WordNetas the reciprocalof the numberof edges
in the shortestpath betweenthem, the LChmea-
sure (Leacocket al., 1998) also uses information
aboutthelengthof theshortestpathbetweena pair
of nodesandcombinesthiswith informationabout
themaximumdepthin WordNetandtheJCnmea-
sure (Jaing and Conrath,1997) makes use of in-
formationtheoryto assignprobabilitiesto eachof
the nodesin the WordNethierarchy and computes
similaritybasedon thesescores.
Table 3 shows the values of equation 1 for
the threesimilaritymeasureswithscoresaveraged
across terms. These results indicate that for all
measurestheSenseval-3datasetcontainsthemost
ambiguityandNLM-WSDthe least. Thisanalysis
is consistentwith the one carried out using mea-
sures based on sense distributions (Section 3.1)
355
MeasureCorpus
Path JCn LCh
NLM-WSD 0.074 0.032 1.027
SemEval 0.136 0.061 1.292
Senseval-3 0.159 0.063 1.500
Table 3: Semanticsimilarityfor each data set us-
ing a varietyof measures
andsuggest thatthe sensesin the NLM-WSDdata
set are more clearly distinguishedthan the other
two.
4 Conclusion
This paper has explored a semi-supervisedap-
proach to the generationof labeled training data
for WSD that is based on relevance feedback
(Stevenson et al., 2008a). It was tested on three
datasets but was onlyfoundto generateexamples
that were accurateenoughto improve WSD per-
formancefor one of these. The data set in which
a performanceimprovement was observed repre-
sented a limited domain (biomedicine)while the
othertwo werenotrestrictedin thisway. Measures
designedto quantifythe level of ambiguitywere
appliedto thesedata sets includingones basedon
the distribution of sensesand anotherdesignedto
quantifysimilarities betweensenses. These mea-
suresprovidedevidencethat the corpusfor which
the relevance feedback approach was successful
containedless ambiguitythan the other two and
thissuggeststhatthe relevancefeedbackapproach
is mostappropriatewhenthe level of ambiguityis
low.
The experiments describedin this paper high-
light the importanceof the level of ambiguityon
the relevance feedbackapproach’s ability to gen-
erate useful labeled examples. Since it is semi-
supervisedthe ambiguitylevel can be checked us-
ing the measures used in this paper (Section 3)
and the performanceof any automaticallygener-
ated examples can be comparedwith the manu-
allylabeledones(seeSection2.3)beforedeciding
whetheror not they shouldbe applied.
References
M. Baroni, S. Bernardini, A. Ferraresi, and
E. Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed
web-crawled corpora. Language Resources and
Evaluation, 43(3):209–226.
J. Daud´e, L. Padr´o, and G. Rigau. 2000. Mapping
wordnetsusing structuralinformation. In Proceed-
ingsofACL’00, pages504–511,HongKong.
J. Jaing and D. Conrath. 1997. Semantic similar-
ity based on corpusstatisticsand lexical taxonomy.
In Proceedingsof InternationalConferenceon Re-
search inComputationalLinguistics, Taiwan.
A. Kilgarriff and J. Rosenzweig. 2000. Framework
andresultsfor EnglishSENSEVAL. Computersand
theHumanities, 34(1-2):15–48.
C. Leacock, M. Chodorow, and G. Miller. 1998.
Using corpus statistics and WordNet relations for
sense identification. ComputationalLinguistics,
24(1):147–165.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Findingpredominantwordsensesin untagged
text. In Proceedingsof ACL’04, pages 279–286,
Barcelona,Spain.
R. Mihalcea,T. Chklovski, and A. Kilgarriff. 2004.
The Senseval-3 English lexical sample task. In
ProceedingsofSenseval-3, pages25–28,Barcelona,
Spain.
R. Passoneau, A. Salleb-Aouissi,and N. Ide. 2009.
Makingsense of word sense variation. In Proceed-
ingsofSEW-2009, pages2–9,Boulder, Colorado.
T. Pedersen, S. Patwardhan, and Michelizzi. 2004.
Wordnet::similaritymeasuringthe relatednessof
concepts. In Proceedingsof AAAI-04, pages1024–
1025,San Jose,CA.
T. Pedersen, S. Pakhomov, S. Patwardhan, and
C. Chute. 2007. Measuresof semanticsimilarity
and relatenessin the biomedicaldomain.Journalof
BiomedicalInformatics, 40(3):288–299.
S. Pradhan, E. Loper, D. Dligach, and M. Palmer.
2007. SemEval-2007 Task-17: English Lexical
Sample, SRL and All Words. In Proceedingsof
SemEval-2007, pages87–92,Prague,CzechRepub-
lic.
J. Rocchio. 1971. Relevance feedback in Informa-
tion Retrieval. In G. Salton, editor, The SMART
Retrieval System– Experimentsin AutomaticDoc-
umentProcessing. PrenticeHall, Englewood Cliffs,
NJ.
M. Stevenson, Y. Guo, and R. Gaizauskas. 2008a.
AcquiringSenseTaggedExamplesusingRelevance
Feedback.InProceedingsoftheColing2008, pages
809–816,Manchester, UK,August.
M.Stevenson,Y. Guo,R. Gaizauskas,andD. Martinez.
2008b. Disambiguationof biomedicaltext usingdi-
verse sourcesof information.BMCBioinformatics,
9(Suppl11):S7.
M. Weeber, J. Mork, and A. Aronson. 2001. Devel-
opinga Test Collectionfor BiomedicalWord Sense
Disambiguation. In Proceedingsof AMIASympo-
sium, pages746–50,Washington,DC.
356

