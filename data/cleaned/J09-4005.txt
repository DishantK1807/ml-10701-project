Squibs
From Annotator Agreement to Noise Models
BeataBeigmanKlebanov
∗
NorthwesternUniversity
EyalBeigman
∗∗
NorthwesternUniversity
Thisarticlediscussesthetransitionfromannotateddatatoagoldstandard,thatis,asubset
thatissufﬁcientlynoise-freewithhighconﬁdence.Unlessappropriatelyreinterpreted,agreement
coefﬁcientsdonotindicatethequalityofthedatasetasabenchmarkingresource:Highoverall
agreementisneithersufﬁcientnornecessarytodistillsomeamountofhighlyreliabledatafrom
theannotatedmaterial.Amathematicalframeworkisdevelopedthatallowsestimationofthe
noiseleveloftheagreedsubsetofannotateddata,whichhelpspromotecautiousbenchmarking.
1. Introduction
Byandlarge,thereasonacomputationallinguistengagesinanannotationprojectisto
build areliable data set for the eventual testing, and possibly training, of an algorithm
performing the task. Hence, the crucial question regarding the annotated data set is
whetheritisgoodforbenchmarking.
For classiﬁcation tasks, the current practice is to infer this information from the
value of an inter-annotator agreement coefﬁcient such as the κ statistic (Cohen 1960;
SiegelandCastellan1988;Carletta1996).Ifagreementishigh,thewholeofthedataset
isgoodfortrainingandtesting;theremainingdisagreementsaretypicallyadjudicated
by an expert (Snyder and Palmer 2004; Palmer, Kingsbury, and Gildea 2005; Girju,
Badulescu,andMoldovan2006)orthroughdiscussion(Litman,Hirschberg,andSwerts
2006), or, in case of more than two annotators, the majority label is chosen (Vieira and
Poesio2000).
1
Therearesomestudieswherecasesofdisagreementwereremovedfrom
testdata(MarkertandNissim2002;Dagan,Glickman,andMagnini2006).Ifagreement
islow,thewholedatasetisdiscardedasunreliable.Thethresholdofacceptabilityseems
tohavestabilizedaroundκ=0.67(Carletta1996;DiEugenioandGlass2004).
Thereislittleunderstanding,however,ofexactlyhowandhowwellthevalueof κ
reﬂects the quality of the data for benchmarking purposes. We develop a model of an-
notationgenerationthatallowsestimationofthelevelofnoiseinaspeciallyconstructed
gold standard. A gold standard with a noise ﬁgure supports cautious benchmarking,
∗ KelloggSchoolofManagement,NorthwesternUniversity,Evanston,IL,beata@northwestern.edu.
∗∗ KelloggSchoolofManagement,NorthwesternUniversity,Evanston,IL,e-beigman@northwestern.edu.
1 Inmanystudies,theprocedureforhandlingdisagreementsisnotclearlyspeciﬁed.Forexample,Gildea
andJurafsky(2002)mentiona“consistencycheck”;inLapata(2002),twoannotatorsattainedκ=0.78on
200testinstances,butitisnotclearhowcasesofdisagreementsweresettled.
Submissionreceived:30June2008;revisedsubmissionreceived:3December2008;acceptedforpublication:
26January2009.
©2009AssociationforComputationalLinguistics
ComputationalLinguistics Volume35,Number4
byrequiringthattheperformanceofanalgorithmbebetterthanbaselinebymorethan
thatwhichcanbeattributedtonoise.Articulatinganannotationgenerationmodelalso
allowsustoshedlightontheinformationκcancontributetobenchmarking.
2. Annotation Noise
We are interested in ﬁnding out which parts of the annotated data are sufﬁciently
reliable. This question presupposes a division of instances into two types: reliable
and unreliable, or, as we shall call them, easy and hard, under the assumption that
items that are easy are reliably annotated, whereas items that are hard display con-
fusion and disagreement. The plausibility of separation into easy and hard instances
is supported by researchers conducting annotation projects: “With many judgments
that characterize natural language, one would expect that there are clear cases as well
as borderline cases that are more difﬁcult to judge” (Wiebe, Wilson, and Cardie 2005,
page200).
Thissuggestsamodelofannotationgenerationwithlatentvariablesfortypes,thus,
foreveryinstancei,thereisavariablel
i
withvaluesE(easy)andH(hard).Letnbethe
numberofinstances,kthenumberofannotators,andX
ij
theclassiﬁcationoftheithin-
stancebythejthannotator.Anannotationgenerationmodelassignsafunctionalformto
thejointdistributionconditionedonthelatentvariableP(X
i1,...,X
ik
|l
i
).Similarmodels
have been studied in biometrics (Aickin 1990; Hui and Zhou 1998; Albert, McShane,
andShih2001;AlbertandDodd2004).Themainassumptionisthat,conditionedonthe
type, annotators agree on easy instances and independently ﬂip a coin on hard ones.
Thejointdistributionsatisﬁes:
P(X
i1
=...=X
ik
|l
i
=E)=1; P(X
i1
=b
1,...,X
ik
=b
k
|l
i
=H)=
productdisplay
k
j=1
P(X
ij
=b
j
|l
i
=H)
We want to take only easy instances into the gold standard, so that it contains
only settled, trustworthy judgments.
2
The problem is that the fact of being easy or
hard is not directly observable, but has to be inferred from the observed annotations.
In particular, some of the observed agreements will in fact be hard instances, since
coin-ﬂipscouldoccasionallycomeoutall-headsorall-tails.Ourobjectiveistoestimate,
withagivendegreeofconﬁdence(α),theproportion γ ofhardinstancesintheagreed
annotations,basedonthenumberofobserveddisagreements.Thevalueofγisthelevel
of annotation noise inthegoldstandardcomprisingagreedannotations.
Let p be the probability that the annotators agree on a hard instance in a binary
classiﬁcationtask:
p=P(X
i1
=...=X
ik
|l
i
=H)=
productdisplay
k
j=1
P(X
ij
=0|l
i
=H)+
productdisplay
k
j=1
P(X
ij
=1|l
i
=H)
DenotebyA
d
theeventthatthereareddisagreedinstances;thesearehard,andare
assumed to be labeled by coin-ﬂips. Let B
h
be the event that there are overall h hard
2 Onthestatusofhardinstances,seeSection5.1.
496
BeigmanKlebanovandBeigman FromAnnotatorAgreementtoNoiseModels
instances;someofthesemaybeunobservedastheysurfaceasrandomagreements.We
notethatP(A
d
|B
h
)=
parenleftbig
h
d
parenrightbig
·(1−p)
d
·p
h−d
ford≤h,hence:
P(B
h
|A
d
)=
P(A
d
∩B
h
)
P(A
d
)
=
P(A
d
|B
h
)·P(B
h
)
summationtext
n
i=d
P(A
d
|B
i
)·P(B
i
)
=
parenleftbig
h
d
parenrightbig
·p
h−d
·P(B
h
)
summationtext
n
i=d
parenleftbig
i
d
parenrightbig
·p
i−d
·P(B
i
)
LetXbearandomvariabledesignatingthenumberofcoin-ﬂips.Itfollowsthat
P(X>t|A
d
)=
summationtext
n
i=t+1
parenleftbig
i
d
parenrightbig
·p
i−d
·P(B
i
)
summationtext
n
i=d
parenleftbig
i
d
parenrightbig
·p
i−d
·P(B
i
)
(1)
Let t
0
be the smallest integer for which P(X>t
0
|A
d
) < 1−α.Givend observed dis-
agreements, we estimate the noise level of the agreed subset of the annotations as at
mostγ=
t
0
−d
n−d,withconﬁdenceα.
3. Relation to κ Statistic
3.1 The
Case ofHigh κ with Two Annotators
Suppose1,000instanceshavebeenannotatedbytwopeople,suchthat900areinstances
of agreement. Both in the 900 agreed instances and in the 100 disagreed ones, the
categories were estimated to be equiprobable for both annotators.
3
In this casep=0.5,
κ =0.8,
4
which is usually taken to be an indicator of sufﬁciently agreeable guidelines,
and, by implication, of a high quality data set. Our candidate gold standard is the 900
instancesofagreement.Whatisits95%conﬁdencenoiserate?Weﬁnd,usingourmodel,
thatwithmorethan5%probabilityupto125agreementsareduetocoin-ﬂipping,hence
γ =13.8%.
5
Thisscenarioisnothypothetical.InPoesioandVieira(1998)Experiment1,
the classiﬁcation of deﬁnite descriptions into Anaphoric-or-Associative versus Unfa-
miliarhasn=992,d=121,p=0.47,which,with95%conﬁdence,yieldsγ=15%.
Let us reverse the question: For a two-annotator project with 1,000 instances, how
many disagreements could we tolerate, so that the agreed part is 95% noise-free with
95% conﬁdence? Only 33 disagreements, corresponding to κ = 0.93. In practice, this
means that a two-annotator project of this size is unlikely to produce a high-quality
goldstandard,thehighκnotwithstanding.
3.2 The
Case ofLow κ with Five Annotators
Suppose now 1,000 instances are annotated by ﬁve people, with 660 agreements. With
categories equiprobable in both hard and easy instances, p = 0.0625. The exact value
of κ depends on the distribution of votes in the 340 disagreed cases, from κ =0.73
when all disagreements are split 4-to-1, to κ = 0.52 when all disagreements are split
3-to-2. Assuming disagreements are coin-ﬂips, the most likely measurement would be
aboutκ=0.637,wherethe340observedcoin-ﬂipsyieldedthemostlikelypattern.
6
This
valueof κ isconsideredlow,yetthe660agreeditemsmakeagoldstandardwithinthe
3 WeestimateP(X
ij
=1|l
i
=H)bytheproportionofdisagreedinstancesthatannotatorjputincategory1.
4 Forcalculatingκ,weusetheversionshowninEquation(2).
5 InallourcalculationsP(B
1
)=...=P(B
n
),thatis,apriori,anynumberofhardinstancesisequiprobable.
6 Thatis,therearetwiceasmany3-to-2casesthan4-to-1,correspondingto
parenleftbig
5
3
parenrightbig
asopposedto
parenleftbig
5
4
parenrightbig
.
497
ComputationalLinguistics Volume35,Number4
noise rateof γ =5%with95%conﬁdence, according toourmodel. Henceitispossible
for the overall annotation to have low-ish κ, but the agreement of all ﬁve annotators,
if observed sufﬁciently frequently, is reliable, and can be used to build a clean gold
standard.
3.3 Interpreting
the κ Statistic in the Annotation Generation Model
Theκstatisticisdeﬁnedasκ=
P
A
−P
E
1−P
E
whereP
A
istheobservedagreementandP
E
isthe
agreement expected by chance, calculated from the marginals. We use the Siegel and
Castellan(1988)version,referredtoasKinArtsteinandPoesio(2008):
P
E
=
m
summationdisplay
j=1
p
2
j
; p
j
=
summationtext
n
i=1
a
ij
nk
; P
A
=
1
n
n
summationdisplay
i=1
P
A
i
; P
A
i
=
summationtext
m
j=1
parenleftbig
a
ij
2
parenrightbig
parenleftbig
k
2
parenrightbig (2)
wherenisthenumberofitems;misthenumberofcategories;kisthenumberofanno-
tators;anda
ij
isthenumberofannotatorswhoassignedtheithitemtothejthcategory.
Suppose there are h hard instances and e easy ones, and m = 2. Suppose further
thatallannotatorsﬂipthesamecoinonhardinstances,andthatthedistributionofthe
categories in easy and hard instances is the same and is given by q
1,...,q
m
. Then the
probabilityforchanceagreementbetweentwoannotatorsisq=
summationtext
m
j=1
q
2
j,ofwhichP
E
is
an estimator. Agreement on a particular instance P
A
i
is measured by the proportion
of agreeing pairs of annotators out of all such pairs, and P
A
is an estimator of the
expectedagreementacrossallinstances.Ourmodelassumesperfectagreementoneasy
instances and agreement with probability q on hard ones, so we expect to see e+q·h
agreed instances, hence P
A
is an estimator of
e+qh
e+h
. Putting these together, κ=
P
A
−P
E
1−P
E
is an estimator of
e+qh
e+h
−q
1−q
=
e
e+h, the proportion of easy instances.
7
In fact, Aickin (1990)
showsthatκisveryclosetothisratiowhenthemarginaldistributionoverthecategories
isuniform,withamoresubstantialdivergenceforskewedcategorydistributions.
8
Thecorrespondencebetween κ andtheproportionofeasyinstancesmakesitclear
why κ is not a sufﬁcient indicator of data quality for benchmarking. For when κ =0.8,
20% of the data are hard cases. Using all data, especially for testing, is thus potentially
hazardous,andthecrucialquestionis:Canwezeroinontheeasyinstanceseffectively,
withoutadmittingmuchnoise?Thisisexactlythequestionansweredbythemodel.
When the distribution of categories is the same in easy and hard instances and
uniform,κcanbeusedtoaddressthisquestionaswell.Recallthatinthetwo-annotator
case in Section 3.1, κ = 0.8, that is, 80% of instances are estimated to be easy. Because
easycasesareasubsetofagreedonesinourmodel,800oftheagreed900instancesare
easy,givinganestimateof11%noiseinthegoldstandard.Requiring95%conﬁdencein
noiseestimation,wefoundγ=13.8%,usingourmodel.Similarly,intheﬁve-annotator
7 Theproportionofeasycasesispositive,whereastheestimatorκcanbenegativewithnon-negligible
probabilitywhene=O(
√
h).
8 InAickin(1990),categorydistributiononeasycasesisderivedfromthatinthehardcases.Thecloserthe
categoriesaretouniformdistributioninthehardcases,theclosertheirdistributioninhardcasesisto
thatineasycases.Forexample,ifthecategoriesaredistributeduniformlyinhardcases,theyarealsoso
distributedintheeasyones.Ifthecategoriesaredistributed(
1
3,
2
3
)inthehardcases,theyaredistributed
(
1
5,
4
5
)intheeasycases.Forthisreason,inAickin’smodel,itisnotpossibletodistinguishbetween
categoryimbalance(manymore0sthan1s)anddifferencesincategorydistributionsineasyandhard
cases.Hissimulationsshowthatincasesofcategoryimbalance(whichimply,inhismodel,differencesin
categorydistributionsineasyandhardcases),κtendstounderestimatetheproportionofeasyinstances.
498
BeigmanKlebanovandBeigman FromAnnotatorAgreementtoNoiseModels
scenario in Section 3.2, κ = 0.637 tells us that about 637 out of 1,000 instances are easy;
they are captured quite precisely by the 660 agreements, yielding a noise estimate of
3.5%,againsomewhatlowerthanthehighconﬁdenceonewegaveusingthemodel.
4. Training and Testing in the Presence ofAnnotation Noise
We discuss two uses of a gold standard within the benchmarking enterprise. The data
couldbeusedfortesting,and,ifthereisenoughofitandafteranappropriatepartition,
fortrainingaswell.Weconsidereachcaseseparatelyinthefollowingsections.
4.1 Testing
with Annotation Noise
Thetwoquestionsonewantstoanswerusingthedataare:Howwelldoesanalgorithm
capture the phenomenon? For any two algorithms, which one is better? Consider the
algorithm comparison situation. Suppose we have a gold standard with L items of
whichuptoRarenoise(γ=
R
L
).Twoalgorithmsmightdifferinperformanceontheeasy
cases, the hard ones, or both. Because we cannot distinguish between easy and hard
instancesinthegoldstandard,weareunabletoattributethedifferenceinperformance
correctly.Moreover,astheannotationsofthehardinstancesarerandomcoin-ﬂips,there
isanexpecteddifferenceinperformancethatisaresultofpurechance.
Supposetwoalgorithmsperformequallywelloneasyinstances;theirperformance
on the hard ones is as good as agreement-by-coin-ﬂipping would allow. Thus, the
difference in the number of “correct” answers on hard instances for algorithms A and
BisarandomvariableSsatisfyingS=
summationtext
R
i=1
X
i
whereX
1,...,X
R
areindependentand
identicallydistributedrandomvariableswhichobtainvalues−1(A“right”,B“wrong”)
and 1 (A “wrong”, B “right”) with probability
1
4
and 0 with probability
1
2,thusµ
S
=0;
σ
S
=
radicalbig
R
2
. By Chebyshev’s inequality Pr(|S| >kσ) ≤
1
k
2
: that is, the chance difference
between the algorithms will be within 4.5σ with 95% probability.
9
In our example,L=
900 and R = 125, hence a difference of up to 35 “correct” answers (3.9% of the gold
standard)canbeattributedtochance.
10
Thisexample showsthatevenifgettingacleandatasetisnotfeasible,itisimpor-
tant to report the noise rate of the data set that has been produced. This would allow
calibrating the benchmarking procedure by requiring the difference between the two
competingalgorithmstobelargerthanthechancedifferencescale.
Someperilsoftestingonnoisydatawerediscussedinarecentarticleinthisjournal
by Reidsma and Carletta (2008). They showed that a machine-learning classiﬁer is
sensitive to the type of noise in the data. Speciﬁcally, if the noise is in the form of
category over-use (an annotator disproportionately favors a certain category), when
algorithmperformanceismeasuredagainstthenoisydata,accuracyestimatesareoften
inﬂatedrelativetoperformanceontherealdata,uncorruptedbynoise(seeFigure3(b)
therein).Thisisbecause “whentheobserveddataisusedtotestperformance,someof
9ForlargeR,normalapproximationcanbeusedwiththetighter2σboundfor95%conﬁdence.
10 Wenotethatbecausethedifferenceattributabletocoin-ﬂippingisO(
radicalBig
γ
L
),andassumingnoiserate
isconstant,thescaleofchancedifferencediminisheswithlargerdatasets(seealsofootnote9).
Theissueismoreimportantwhendealingwithsmall-to-moderatedatasets.However,evenfor
a130Ktestset(Sections22–24oftheWallStreetJournalcorpus,standardlyusedasatestsetin
POS-taggingbenchmarks),itisusefultoknowtheestimatednoiserate,asitisnotclearthatall
reportedimprovementsinperformancewouldcomeoutsigniﬁcant.Forexample,Shen,Satta,and
Joshi(2007)summarizeperformanceofﬁvepreviouslypublishedandthreenewlyreportedalgorithms,
allbetween97.10%and97.33%.
499
ComputationalLinguistics Volume35,Number4
thesamplesmatchnotbecausetheclassiﬁergetsthelabelright,butbecauseitoveruses
the same label as the human coder” (Reidsma and Carletta 2008, page 232). On the
other hand, if disagreements are random classiﬁcation noise (the label of any instance
can be ﬂipped with a certain probability), a performance estimate based on observed
data would often be lower than performance on the real data, because the noise that
corrupteditwasignoredbytheclassiﬁer(seeFigure2(d)therein).
Reidsma and Carletta (2008) suggest that the community develops methods to
investigatethepatternsofdisagreementsbetweenannotatorstogaininsightintothepo-
tentialofincorrectperformanceestimation.Althoughweagreeonthegeneralpointthat
humanagreementsanddisagreementsshouldbeardirectlyonthepracticeofestimating
the performance of an algorithm, we focus on improving the quality of performance
estimation. We suggest (1) mitigating the effect of annotation noise on performance
estimationbyusingtheleastnoisypartofthedatasetfortesting,thatis,agoldstandard
with agreed items; (2) providing an estimate of the level of noise in the gold standard,
whichcanbeusedtogaugethedivergencebetweentheestimateofperformanceusing
thegoldstandardfromtherealperformanceﬁgureontheeasyinstances(i.e.,onnoise-
freedata),similarlytothealgorithmcomparisonscenarioprovidedherein.
4.2 Learning
with Annotation Noise
Theproblemwithnoiseinthetrainingdataisthepotentialformisclassiﬁcationofeasy
instances in the test data as a result of hard instances in the training data, the problem
wecall hard case bias.
Learning in the presence of noise is an active research area in machine learning.
However, annotation noise is different from existing well-understood noise models.
Speciﬁcally,randomclassiﬁcationnoise,whereeachinstancehasthesameprobabilityof
havingitslabelﬂipped,isknowntobetolerableinsupervisedlearning(Blumetal.1996;
Cohen1997;ReidsmaandCarletta2008).Inannotationnoise,coin-ﬂippingisconﬁned
tohardinstances,whichshouldnotbeassumedtobeuniformlydistributedacrossthe
feature space. Indeed, there is reason to believe that they form clusters; certain feature
combinationstendtogiverisetohardinstances.TheﬁndingreportedbyReidsmaand
opdenAkker(2008)thataclassiﬁertrainedondatafromoneannotatortendedtoagree
muchbetterwithtestdatafromthesameannotatorthanwiththatofanotherannotator
exempliﬁes a situation where observed hard cases (i.e., cases where the annotators
disagree)constituteapatterninthefeaturespacethataclassiﬁerpicksup.
In a separate article, we establish a number of properties of learning under anno-
tation noise (Beigman and Beigman Klebanov 2009). We show that the 0-1 loss model
may be vulnerable to annotation noise for small data sets, but becomes increasingly
robust the larger the data set, with worst-case hard case bias of θ(
1
√
n
). We also show
thatlearningwiththepopularvoted-perceptronalgorithm(FreundandSchapire1999)
couldsufferaconstantrateofhardcasebiasirrespectiveofthesizeofthedataset.
5. Discussion
5.1 The
Status ofHard Instances
Wesuggestedthatonlytheeasyinstancesshouldbetakenintothegoldstandard.This
is not to say that hard cases should be eliminated from the researcher’s attention; we
merely argue that they should not be used for testing algorithms for benchmarking
500
BeigmanKlebanovandBeigman FromAnnotatorAgreementtoNoiseModels
purposes.Hardcasesareinterestingfortheorydevelopment,becausethisiswherethe
theory might have a difﬁculty, but they do not allow for a fair comparison, as their
correctlabelcannotbedeterminedunderthecurrenttheory.Theagreeddataembodies
the well-articulated parts of the theory, which are ready for deployment as a gold
standard for machine learning. Once the theory is improved to a stage where some of
thepreviouslyhardcasesreceiveanunproblematictreatment,thoseitemscanbeadded
to the data set, which can make the task more challenging for the machine. Linguistic
theories-in-the-making can have limited coverage; they do not immediately attain the
status of medical conditions, for example, where there presumably exists a true label
evenforthehardest-to-diagnosecases.
11
5.2 Plausibility
ofthe Model
Beyond the separation into easy and hard instances, our model prescribes certain an-
notator behavior for each type. In our work on metaphor, we observed that certain
metaphor markups were retracted by their authors, when asked after 4–8 weeks to
revisit the annotations (Beigman Klebanov, Beigman, and Diermeier 2008). These were
apparently hard cases, with people resolving their doubts inconsistently on the two
occasions; coin-ﬂipping is a reasonable ﬁrst-cut model for such cases. The model also
accommodatescategoryover-usebias(DiEugenioandGlass2004;ArtsteinandPoesio
2008;ReidsmaandCarletta2008),asP(X
ij
=b
j
|l
i
=H)mayvaryacrossannotators.
Still, this model is clearly a simpliﬁcation. For example, it is possible that there
is more than one degree of hardness, and annotator behavior changes accordingly.
Anotherextensionismodelingimperfectannotators,allowedtocommitrandomerrors
oneasycases;thisextensionwouldbeneededifalargenumberofannotatorsisused.
Such extensions, as well as methods for estimating these more complex models,
should clearly be put on the community’s research agenda. The main contribution
of the simple model is in outlining the trajectory from agreement to gold standard
with a noise estimate, and indicating the potential beneﬁt of the latter to data uti-
lization (low overall agreement does not preclude the existence of a reliable subset)
and to prudent benchmarking. Furthermore, the simple model helps us improve the
understanding of the information provided by the κ statistic, and to appreciate its
limitations.Italsoallowsustoseethebeneﬁtofaddingannotators,asdiscussedinthe
nextsection.
5.3 Adding
Annotators
If we want the test data to be able to detect small advances in machines’ handling of
the task, we need to produce gold standards with low noise levels. The level of noise
inagreeddatadependsontwoparameters:(a)thenumberofagreeditems,and(b)the
probability of chance agreement between annotators. Although the ﬁrst is not under
the researcher’s control once the data set is chosen, the second is, by changing the
numberofannotators.Obviously,themoreannotatorsarerequiredtoagree,thelower
p will be, and the smaller the number of agreements that can be attributed to coin-
ﬂipping. If indeed 800 out of 1,000 items are easy, agreement between two annotators
canonlydetectthemwithupto13.8%noise.Addingathirdannotatormeansp=0.25.
11 Asoneoftheanonymousreviewerspointedout,somemedicalconditions,suchasautism,arealsoonly
partiallyunderstood.
501
ComputationalLinguistics Volume35,Number4
We are most likely to observe 850 agreed instances, which would not contain more
than 7.7% noise, with 95% conﬁdence. Effectively, we got rid of about half the random
agreements.
Acknowledgments
WethankEliShamirandBeiYuforreading
earlierdraftsofthisarticle,aswellasthe
editorandtheanonymousreviewersfor
commentsthathelpedusimprovethe
articlesigniﬁcantly.
References
Aickin,Mikel.1990.Maximumlikelihood
estimationofagreementintheconstant
predictiveprobabilitymodel,andits
relationtoCohen’skappa.Biometrics,
46(2):293–302.
Albert,PaulandLoriDodd.2004.A
cautionarynoteontherobustnessoflatent
classmodelsforestimatingdiagnostic
errorwithoutagoldstandard.Biometrics,
60(2):427–435.
Albert,Paul,LisaMcShane,andJoannaShih.
2001.Latentclassmodelingapproaches
forassessingdiagnosticerrorwithouta
goldstandard:Withapplicationstop53
immunohistochemicalassaysinbladder
tumors.Biometrics,57(2):610–619.
Artstein,RonandMassimoPoesio.2008.
Inter-coderagreementforcomputational
linguistics.ComputationalLinguistics,
34(4):555–596.
Beigman,EyalandBeataBeigmanKlebanov.
2009.Learningwithannotationnoise.In
Proceedingsofthe47thAnnualMeetingofthe
AssociationforComputationalLinguistics,
Singapore.
BeigmanKlebanov,Beata,EyalBeigman,
andDanielDiermeier.2008.Analyzing
disagreements.InCOLING2008Workshop
onHumanJudgmentsinComputational
Linguistics,pages2–7,Manchester.
Blum,Avrim,AlanFrieze,RaviKannan,
andSantoshVempala.1996.A
polynomial-timealgorithmforlearning
noisylinearthresholdfunctions.In
Proceedingsofthe37thAnnualIEEE
SymposiumonFoundationsofComputer
Science,pages330–338,Burlington,VT.
Carletta,Jean.1996.Assessingagreementon
classiﬁcationtasks:Thekappastatistic.
ComputationalLinguistics,22(2):249–254.
Cohen,Edith.1997.Learningnoisy
perceptronsbyaperceptroninpolynomial
time.InProceedingsofthe38thAnnual
SymposiumonFoundationsofComputer
Science,pages514–523,MiamiBeach,FL.
Cohen,Jacob.1960.Acoefﬁcientof
agreementfornominalscales.Educational
andPsychologicalMeasurement,20(1):37–46.
Dagan,Ido,OrenGlickman,andBernardo
Magnini.2006.ThePASCALrecognising
textualentailmentchallenge.InThe
PASCALRecognisingTextualEntailment
Challenge,Springer,Berlin,pages177–190.
DiEugenio,BarbaraandMichaelGlass.
2004.Thekappastatistic:Asecondlook.
ComputationalLinguistics,30(1):95–101.
Freund,Y.andR.E.Schapire.1999.Large
marginclassiﬁcationusingtheperceptron
algorithm.MachineLearning,37(3):277–296.
Gildea,DanielandDanielJurafsky.2002.
Automaticlabelingofsemanticroles.
ComputationalLinguistics,28(3):245–288.
Girju,Roxana,AdrianaBadulescu,andDan
Moldovan.2006.Automaticdiscovery
ofpart-wholerelations.Computational
Linguistics,32(1):83–135.
Hui,SiuandXiaoZhou.1998.Evaluationof
diagnostictestswithoutgoldstandards.
StatisticalMethodsinMedicalResearch,
7(4):354–370.
Lapata,Maria.2002.Thedisambiguationof
nominalizations.ComputationalLinguistics,
28(3):357–388.
Litman,Diane,JuliaHirschberg,andMarc
Swerts.2006.Characterizingand
predictingcorrectionsinspokendialogue
systems.ComputationalLinguistics,
32(3):417–438.
Markert,KatjaandMalvinaNissim.2002.
Metonymyresolutionasaclassiﬁcation
task.InProceedingsoftheEmpiricalMethods
inNaturalLanguageProcessingConference,
pages204–213,Philadelphia,PA.
Palmer,Martha,PaulKingsbury,andDaniel
Gildea.2005.Thepropositionbank:An
annotatedcorpusofsemanticroles.
ComputationalLinguistics,31(1):71–106.
Poesio,MassimoandRenataVieira.1998.
Acorpus-basedinvestigationofdeﬁnite
descriptionuse.ComputationalLinguistics,
24(2):183–216.
Reidsma,DennisandJeanCarletta.2008.
Reliabilitymeasurementwithoutlimit.
ComputationalLinguistics,34(3):319–326.
Reidsma,DennisandRieksopdenAkker.
2008.Exploitingsubjectiveannotations.
InCOLING2008WorkshoponHuman
JudgmentsinComputationalLinguistics,
pages8–16,Manchester.
502
BeigmanKlebanovandBeigman FromAnnotatorAgreementtoNoiseModels
Shen,Libin,GiorgioSatta,andAravind
Joshi.2007.Guidedlearningfor
bidirectionalsequenceclassiﬁcation.In
Proceedingsofthe45thAnnualMeeting
oftheAssociationofComputational
Linguistics,pages760–767,Prague.
Siegel,SidneyandN.JohnCastellanJr.
1988.NonparametricStatisticsforthe
BehavioralSciences.McGraw-Hill,
2ndedition.
Snyder,BenjaminandMarthaPalmer.2004.
TheEnglishall-wordstask.InSenseval-3:
3rdInternationalWorkshopontheEvaluation
ofSystemsfortheSemanticAnalysisofText,
pages41–43,Barcelona.
Vieira,RenataandMassimoPoesio.2000.An
empiricallybasedsystemforprocessing
deﬁnitedescriptions.Computational
Linguistics,26(4):539–593.
Wiebe,Janyce,TeresaWilson,andClaire
Cardie.2005.Annotatingexpressionsof
opinionsandemotionsinlanguage.
LanguageResourcesandEvaluation,
39(2):165–210.
503


