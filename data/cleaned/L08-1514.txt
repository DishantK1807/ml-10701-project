<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics</title>
<date>2002</date>
<booktitle>In Proc. ARPA Workshop on Human Language Technology</booktitle>
<contexts>
<context>gth used. The original IBM-BLEU used the length of the reference which was closest in length to the translation hypothesis. This is the variant that we use here. 3.5. NIST The NIST precision measure (Doddington, 2002) was intended as an improved version of BLEU. Unwanted effects of the brevity penalty of BLEU should be reduced and ngram occurrences are weighted by their importance. The importance is computed by t</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proc. ARPA Workshop on Human Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Mauser</author>
<author>Richard Zens</author>
<author>Evgeny Matusov</author>
<author>Sasa Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>The RWTH Statistical Machine Translation System for the IWSLT 2006 Evaluation</title>
<date>2006</date>
<booktitle>In Proc. of the International Workshop on Spoken Language Translation</booktitle>
<pages>103--110</pages>
<location>Kyoto, Japan</location>
<contexts>
<context>ls are: a phrase translation model, a word-based translation model, word and phrase penalty, a target language model and a reordering model. A detailed description of the models used can be found in (Mauser et al., 2006). 2. Minimum Error Rate Training Phrase table probabilies themselves already give a fairly good represetation of our training data. Translating unseen data however, requires the system to be more fle</context>
</contexts>
<marker>Mauser, Zens, Matusov, Hasan, Ney, 2006</marker>
<rawString>Arne Mauser, Richard Zens, Evgeny Matusov, Sasa Hasan, and Hermann Ney. 2006. The RWTH Statistical Machine Translation System for the IWSLT 2006 Evaluation. In Proc. of the International Workshop on Spoken Language Translation, pages 103–110, Kyoto, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>295--302</pages>
<location>Philadelphia, PA</location>
<contexts>
<context>mong all possible target language sentences, we will choose the sentence with the highest probability: ˆeˆI1 = argmax I,eI1 braceleftbigPr(eI 1|f J 1 ) bracerightbig (1) (2) Using a log-linear model (Och and Ney, 2002), we obtain: Pr(eI1|fJ1 ) = exp parenleftBigsummationtextM m=1 λmhm(e I1,fJ1 ) parenrightBig summationtext e′I′1 exp parenleftBigsummationtextM m=1 λmhm(e′ I′ 1 ,fJ1 ) parenrightBig (3) The denominat</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 295–302, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation</title>
<date>2003</date>
<booktitle>In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan</location>
<contexts>
<context>function is an automatic evaluation measure, we can use an automated procedure that will find a good solution four our parameters. This procedure is referred to as Minimum Error Rate Training (MERT) (Och, 2003). In the experiments, we used the downhill simplex algorithm (Press et al., 2002) to optimize the system weights for a specific evaluation measure. 3. Evaluation Measures This sections briefly descri</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wie-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA</location>
<contexts>
<context>nted as one edit with equal costs to insertions, deletions and substitutions of single words. The number of edit operations is divided by the average number of reference words. 3.4. BLEU Proposed by (Papineni et al., 2002), the BLEU criterion measures the similarity of n-grams count vectors of the reference translations in the candidate translation. If multiple references are present the counts are collected of all tr</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wie-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Saul A Teukolsky</author>
<author>William T Vetterling</author>
<author>Brian P Flannery</author>
</authors>
<title>Numerical Recipes in C</title>
<date>2002</date>
<publisher>Cambridge University Press</publisher>
<location>Cambridge, UK</location>
<contexts>
<context>cedure that will find a good solution four our parameters. This procedure is referred to as Minimum Error Rate Training (MERT) (Och, 2003). In the experiments, we used the downhill simplex algorithm (Press et al., 2002) to optimize the system weights for a specific evaluation measure. 3. Evaluation Measures This sections briefly describes the Evaluation measures considered in this work. We selected the most commonl</context>
</contexts>
<marker>Press, Teukolsky, Vetterling, Flannery, 2002</marker>
<rawString>William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. 2002. Numerical Recipes in C++. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas</booktitle>
<contexts>
<context>ures the difference in the count of the words occurring in hypothesis and reference. The resulting number is divided by the number of words in the reference. 3.3. Translation Edit Rate (TER) The TER (Snover et al., 2006) is an error measure counts the number of edits required to change a system output into one of the given translation references. The background is to measure the amount of human work that would be re</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the Americas, August.</rawString>
</citation>
</citationList>
</algorithm>

