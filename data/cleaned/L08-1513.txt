<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Harald Baayen</author>
<author>Richard Piepenbrock</author>
<author>Hedderik van Rijn</author>
</authors>
<date>1993</date>
<booktitle>The CELEX Lexical Database (Release 1) [CD-ROM</booktitle>
<institution>Linguistic Data Consortium, University of Pennsylvania</institution>
<location>Philadelphia, PA</location>
<marker>Baayen, Piepenbrock, van Rijn, 1993</marker>
<rawString>Harald Baayen, Richard Piepenbrock, and Hedderik van Rijn. 1993. The CELEX Lexical Database (Release 1) [CD-ROM]. Philadelphia, PA: Linguistic Data Consortium, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of lexical semantic relatedness</title>
<date>2006</date>
<journal>Computational Linguistics</journal>
<volume>35</volume>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based measures of lexical semantic relatedness. Computational Linguistics, 35(1):13–47.</rawString>
</citation>
<citation valid="true">
<title>WordNet. An Electronic Lexical Database</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor</editor>
<publisher>The MIT Press</publisher>
<location>Cambridge, MA</location>
<contexts>
<context>related, but not similar. Leacock and Chodorow Of course, a lexical hierarchy like EuroWordNet contains more relevant information than just the number of steps between two words. Leacock and Chodorow (1998) formalize the intuition that the semantic similarity between two words does not only depend on the number of steps between them, but also on the maximum depth of the hierarchy, D. They moreover trans</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet. An Electronic Lexical Database. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<title>Distributional structure</title>
<date>1954</date>
<journal>Word</journal>
<volume>10</volume>
<contexts>
<context>ructed on the basis of a large corpus of data in order to model the distribution of the target words. 3.1. Word Space Models Word Space Models are inspired by the so-called distributional hypothesis (Harris, 1954), which states that words that occur in similar contexts will also be semantically similar. The semantic similarity between two words is thus operationalized as their distributional similarity in a c</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Research in Computational Linguistics</booktitle>
<pages>19--33</pages>
<location>Taiwan</location>
<contexts>
<context>e of manually compiled resources, like machinereadable dictionaries or thesauri, in order to determine the similarity or dissimilarity between two terms (Rada and Bicknell, 1989; Wu and Palmer, 1994; Jiang and Conrath, 1997; Leacock and Chodorow, 1998). The second relies on large corpora of texts, and calculates the semantic similarity between two words on the basis of their distributional similarity in such a corpus (L</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the International Conference on Research in Computational Linguistics, pages 19–33, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Landauer</author>
<author>Susan T Dumais</author>
</authors>
<title>A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge</title>
<date>1997</date>
<journal>Psychological Review</journal>
<pages>104--211</pages>
<contexts>
<context>7; Leacock and Chodorow, 1998). The second relies on large corpora of texts, and calculates the semantic similarity between two words on the basis of their distributional similarity in such a corpus (Landauer and Dumais, 1997; Sch¨utze, 1998; Lin, 1998; Pad´o and Lapata, 2007). In this paper, we will focus on the evaluation of these two approaches, which has so far been suffering from a number of weaknesses. We will use e</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas. K. Landauer and Susan T. Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge. Psychological Review, 104:211–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification</title>
<date>1998</date>
<booktitle>In Christiane Fellbaum, editor, WordNet. An Electronic Lexical Database</booktitle>
<pages>265--283</pages>
<publisher>The MIT Press</publisher>
<location>Cambridge, MA</location>
<contexts>
<context>sources, like machinereadable dictionaries or thesauri, in order to determine the similarity or dissimilarity between two terms (Rada and Bicknell, 1989; Wu and Palmer, 1994; Jiang and Conrath, 1997; Leacock and Chodorow, 1998). The second relies on large corpora of texts, and calculates the semantic similarity between two words on the basis of their distributional similarity in such a corpus (Landauer and Dumais, 1997; Sc</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and WordNet similarity for word sense identification. In Christiane Fellbaum, editor, WordNet. An Electronic Lexical Database, pages 265–283. Cambridge, MA: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph P Levy</author>
<author>John A Bullinaria</author>
</authors>
<title>Learning lexical properties from word usage patterns: Which context words should be used</title>
<date>2001</date>
<booktitle>Connectionist Models of Learning, Development and Evolution: Proceedings of the Sixth Neural Computation and Psychology Workshop</booktitle>
<pages>273--282</pages>
<editor>In R.F. French and J.P. Sougne, editors</editor>
<publisher>Springer</publisher>
<location>London</location>
<contexts>
<context> documents a target word appears in (e.g., Latent Semantic Analysis, Landauer and Dumais (1997)), others look at the context words of the target within a window of a pre-defined size (Sch¨utze, 1998; Levy and Bullinaria, 2001), still others rely on the syntactic relationships in which the target takes part (Lin, 1998; Pad´o and Lapata, 2007). They all, however, compute the similarity between two words by comparing their c</context>
</contexts>
<marker>Levy, Bullinaria, 2001</marker>
<rawString>Joseph P. Levy and John A. Bullinaria. 2001. Learning lexical properties from word usage patterns: Which context words should be used. In R.F. French and J.P. Sougne, editors, Connectionist Models of Learning, Development and Evolution: Proceedings of the Sixth Neural Computation and Psychology Workshop, pages 273–282. London: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL98</booktitle>
<pages>768--774</pages>
<location>Montreal, Canada</location>
<contexts>
<context>relies on large corpora of texts, and calculates the semantic similarity between two words on the basis of their distributional similarity in such a corpus (Landauer and Dumais, 1997; Sch¨utze, 1998; Lin, 1998; Pad´o and Lapata, 2007). In this paper, we will focus on the evaluation of these two approaches, which has so far been suffering from a number of weaknesses. We will use each of the two methods to a</context>
<context> at the context words of the target within a window of a pre-defined size (Sch¨utze, 1998; Levy and Bullinaria, 2001), still others rely on the syntactic relationships in which the target takes part (Lin, 1998; Pad´o and Lapata, 2007). They all, however, compute the similarity between two words by comparing their context vectors. In general, the more features the two words share, and the more similar their</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLING-ACL98, pages 768–774, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes</booktitle>
<pages>6--1</pages>
<marker>Miller, Charles, 1991</marker>
<rawString>George A. Miller and Walter G. Charles. 1991. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependencybased construction of semantic space models</title>
<date>2007</date>
<journal>Computational Linguistics</journal>
<volume>33</volume>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependencybased construction of semantic space models. Computational Linguistics, 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Peirsman</author>
<author>Kris Heylen</author>
<author>Dirk Speelman</author>
</authors>
<title>Finding semantically related words in Dutch. Cooccurrences versus syntactic contexts</title>
<date>2007</date>
<booktitle>In Proceedings of the CoSMO workshop, held in conjunction with CONTEXT-07</booktitle>
<pages>9--16</pages>
<location>Roskilde, Denmark</location>
<marker>Peirsman, Heylen, Speelman, 2007</marker>
<rawString>Yves Peirsman, Kris Heylen, and Dirk Speelman. 2007. Finding semantically related words in Dutch. Cooccurrences versus syntactic contexts. In Proceedings of the CoSMO workshop, held in conjunction with CONTEXT-07, pages 9–16, Roskilde, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Peirsman</author>
<author>Kris Heylen</author>
<author>Dirk Speelman</author>
</authors>
<date>2008</date>
<marker>Peirsman, Heylen, Speelman, 2008</marker>
<rawString>Yves Peirsman, Kris Heylen, and Dirk Speelman. 2008.</rawString>
</citation>
<citation valid="true">
<title>Putting things in order. First and second order context models for the calculation of semantic similarity</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th Journ´ees internationales d’Analyse statistique des Donn´ees Textuelles (JADT</booktitle>
<pages>907--916</pages>
<location>Lyon, France</location>
<contexts>
<context>ilar their values for those features, the higher the estimate of semantic similarity will be. For a comparison of the results given by different context definitions, see Peirsman, Heylen and Speelman (2007; 2008). 3.2. Experimental setup In our second series of experiments, we compare two bagof-word approaches, which model a word on the basis of its context words. They are called bag-of-word because they trea</context>
</contexts>
<marker>2008</marker>
<rawString>Putting things in order. First and second order context models for the calculation of semantic similarity. In Proceedings of the 9th Journ´ees internationales d’Analyse statistique des Donn´ees Textuelles (JADT 2008), pages 907–916, Lyon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Rada</author>
<author>Ellen Bicknell</author>
</authors>
<title>Ranking documents with a thesaurus</title>
<date>1989</date>
<journal>Journal of the American Society for Information Science</journal>
<volume>40</volume>
<contexts>
<context>of addressing this problem. The first makes use of manually compiled resources, like machinereadable dictionaries or thesauri, in order to determine the similarity or dissimilarity between two terms (Rada and Bicknell, 1989; Wu and Palmer, 1994; Jiang and Conrath, 1997; Leacock and Chodorow, 1998). The second relies on large corpora of texts, and calculates the semantic similarity between two words on the basis of their</context>
</contexts>
<marker>Rada, Bicknell, 1989</marker>
<rawString>Roy Rada and Ellen Bicknell. 1989. Ranking documents with a thesaurus. Journal of the American Society for Information Science, 40(5):304–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence</booktitle>
<pages>448--453</pages>
<location>Montreal, Canada</location>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using information content to evaluate semantic similarity. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 448–453, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy</title>
<date>1965</date>
<journal>Communications of the ACM</journal>
<volume>8</volume>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Communications of the ACM, 8(10):627–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wim Ruts</author>
</authors>
<title>Simon De Deyne, Eef Ameel, Wolf Vanpaemel, Timothy Verbeemen, and Gert Storms</title>
<date>2004</date>
<journal>Computers</journal>
<volume>36</volume>
<marker>Ruts, 2004</marker>
<rawString>Wim Ruts, Simon De Deyne, Eef Ameel, Wolf Vanpaemel, Timothy Verbeemen, and Gert Storms. 2004. Dutch norm data for 13 semantic categories and 338 exemplars. Behavior Research Methods, Instruments, &amp; Computers, 36(3):506–515.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination</title>
<date>1998</date>
<journal>Computational Linguistics</journal>
<volume>24</volume>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
</authors>
<title>The application of Singular Value Decomposition to Dutch noun-adjective matrices</title>
<date>2006</date>
<booktitle>In Piet Mertens, C´edrick Fairon, Anne Dister, and Patrick Watrin, editors, Verbum Ex Machina. Actes de la 13e Conf´erence sur le Traitement Automatique des Langues Naturelles (TALN</booktitle>
<pages>767--772</pages>
<location>Leuven, Belgium</location>
<marker>Van de Cruys, 2006</marker>
<rawString>Tim Van de Cruys. 2006. The application of Singular Value Decomposition to Dutch noun-adjective matrices. In Piet Mertens, C´edrick Fairon, Anne Dister, and Patrick Watrin, editors, Verbum Ex Machina. Actes de la 13e Conf´erence sur le Traitement Automatique des Langues Naturelles (TALN), pages 767–772, Leuven, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke Van der Plas</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Finding synonyms using automatic word alignment and measures of distributional similarity</title>
<date>2006</date>
<booktitle>In Proceedings of ACL/COLING-2006</booktitle>
<pages>866--873</pages>
<marker>Van der Plas, Tiedemann, 2006</marker>
<rawString>Lonneke Van der Plas and J¨org Tiedemann. 2006. Finding synonyms using automatic word alignment and measures of distributional similarity. In Proceedings of ACL/COLING-2006, pages 866–873.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>At last parsing is now operational</title>
<date>2006</date>
<booktitle>In Piet Mertens, C´edrick Fairon, Anne Dister, and Patrick Watrin, editors, Verbum Ex Machina. Actes de la 13e Conf´erence sur le Traitement Automatique des Langues Naturelles (TALN</booktitle>
<pages>20--42</pages>
<marker>van Noord, 2006</marker>
<rawString>Gertjan van Noord. 2006. At last parsing is now operational. In Piet Mertens, C´edrick Fairon, Anne Dister, and Patrick Watrin, editors, Verbum Ex Machina. Actes de la 13e Conf´erence sur le Traitement Automatique des Langues Naturelles (TALN), pages 20–42.</rawString>
</citation>
<citation valid="true">
<title>EuroWordNet: a multilingual database with lexical semantic networks for European Languages</title>
<date>1998</date>
<editor>Piek Vossen, editor</editor>
<publisher>Kluwer</publisher>
<location>Dordrecht</location>
<contexts>
<context>related, but not similar. Leacock and Chodorow Of course, a lexical hierarchy like EuroWordNet contains more relevant information than just the number of steps between two words. Leacock and Chodorow (1998) formalize the intuition that the semantic similarity between two words does not only depend on the number of steps between them, but also on the maximum depth of the hierarchy, D. They moreover trans</context>
</contexts>
<marker>1998</marker>
<rawString>Piek Vossen, editor. 1998. EuroWordNet: a multilingual database with lexical semantic networks for European Languages. Kluwer, Dordrecht.</rawString>
</citation>
</citationList>
</algorithm>

