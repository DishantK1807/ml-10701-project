Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment.
P01-1067
In our experiments, we will use 4 different kinds of feature combinations: a157 Baseline: The 6 baseline features used in (Och, 2003), such as cost of word penalty, cost of aligned template penalty.
P03-1021
The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years.
J90-2002
SMT Team (2003) also used minimum error training as in Och (2003), but used a large number of feature functions.
P03-1021
Six features from (Och, 2003) were used as baseline features.
P03-1021
This approach used the same set of features as the alignment template approach in (Och and Ney, 2002).
P02-1038
Models for MT Och and Ney (2002) proposed a framework for MT based on direct translation, using the conditional modela1a15a2a16a4a7a6a14a8a11a10 estimated using a maximum entropy model.
P02-1038
Models for MT The seminal IBM models (Brown et al., 1990) were the first to introduce generative models to the MT task.
J90-2002
In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation.
P98-2162 W99-0604
Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements.
P02-1038 P03-1021
Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form.
J97-3002
The minimum error training (Och, 2003) was used on the development data for parameter estimation.
P03-1021
Wang and Waibel (1998) proposed an SMT model based on phrase-based alignments.
P98-2221
(Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank.
J97-3002
By reranking a 1000-best list generated by the baseline MT system from Och (2003), the BLEU (Papineni et al., 2001) score on the test dataset was improved from 31.6% to 32.9%. 2 Ranking and Reranking 2.1 Reranking for NLP tasks Like machine translation, parsing is another field of natural language processing in which generative models have been widely used.
P03-1021
Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages.
P03-1011
Och (2003) described the use of minimum error training directly optimizing the error rate on automatic MT evaluation metrics such as BLEU.
P03-1021
For example, SVMs even cannot be used to train on the whole Penn Treebank in parse reranking (Shen and Joshi, 2003).
W03-0402
Various machine learning algorithms have been employed in parse reranking, such as Boosting (Collins, 2000), Perceptron (Collins and Duffy, 2002) and Support Vector Machines (Shen and Joshi, 2003).
P02-1034 W03-0402
Och. 2003.
P03-1021
