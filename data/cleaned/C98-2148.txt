Hypertext Authoring for Linking Relevant Segments of 
Related Instruction Manuals 
Hiroshi Nakagawa and Tatsunori Mort and Nobuyuki Omori and Jun Okalnura 
I)el)artment of Computer and Electronic Engineering, Yokohama National University 
Tokiwadai 79-5, ltodogaya, Yokohama, 240-8501, JAPAN 
E-mail: nakagawa~.} n aklab.dnj .ynu.ac.j p, {mort ,ohmori,j un } C(} fores t .dnj .ynu.ac.j t) 
Abstract 
Recently manuals of industrial products become 
large and often consist of separated volumes. In 
reading such individual but related manuals, we 
nmst consider the relation among seglnents, which 
contain explanations of sequences of operation. In 
this paper, we propose methods for linking relevant 
segments in hypertext authoring of a set of related 
manuals. Our method is based on the similarity 
calculation between two segments. Our experimen
tal results show that the proposed method improves 
both recall and precision comparing with the con
ventional tf. idf based method. 
1 Introduction

In reading traditional paper based manuals, we 
should use their indices and table of contents in or
der to know where the contents we want to know are 
written. In fact, it is not an easy task especially for 
novices. Recent years, electronic nlalmals in a form 
of hypertext like Help of Microsoft Windows became 
widely used. Unfortunately it. is very expensive to 
make a hypertext manual by hand especially in case 
of a large volume of manual which consists of sev
eral separated volumes. In a case of such a large 
manual, the same topic appears at several places in 
different volumes. One of them is an introductory 
explanation for a novice. Another is a precise ex
planation for an advanced user. It is w~'ry useful to 
jmnp from one of them to another of them directly 
by just clicking a button of mouse in reading a man
ual text on a browser like NctScape. This type of 
access is realized by linking them in hypertext for
mat by hyl)ertext authoring. 
Autonmtic hypertext authoring has been focused 
on in these years, and much work has been clone. For 
instance, Basili et al. (1994) use document struc
tnres and semantic information by means of natural 
language processing technique to set hyl)erlinks on 
plain texts. 
The essential point in the research of automatic 
hypertext authoring is the way to find semantically 
relevant parts where each part is characterized by 
a nmnbcr of key words. Actually it is very similar 
with information retrieval, IR henceforth, especially 
with the so called passage retrieval (Salton et al., 
1993). J.Green (1996) does hypertext authoring of 
newspaper articles by word's lexical chains which are 
calculated using WordNet. Kurohashi et al. (1992) 
made a hypertext dictionary of tile field of inlbr
mation science. They use linguistic patterns that 
are used for definition of terminology as well ~Ls the
saurus based on words' similarity. Furner-llines and 
WiIlett (1994) experimentally evaluate and compare 
tile performance of several hnman hyper linkers. In 
general, however, we have not yet paid enough at
tention to a flfll-automatic hyper linker system, that 
is what we pursue in this paper. 
The new ideas in our system are tile following 
points: 
1. Our target is a lmdti-volmne manual that de
scribes the same hardware or soft.ware but is dif
flu'rent in their granularity of descriptions from 
volume to volume. 
2. In our system, hyper links are set not b(~tween 
an anchor word anti a certain part. of text but 
between two segments, where a segment is a 
smallest formal unit in document, like a sub
subsection of \[~'I\],~X if no smaller units like 
subsubsubsection are used. 
3. We lind pairs of relevant segments over two 
vohunes, for instance, between an introductory 
manual for novices and a reference manual for 
advanced level users about the same software or 
hardware. 
4. We use not only tf.idf based vector space model 
but also words' co-occurrence information to 
measure the similarity 1)etween segments. 
2 Similarity
Calculation 
We need to calculate a semantic similarity between 
two segments in order to decide whether two of them 
are linked, automatically. The most well known 
method to calculate similarity in IR is a vector space 
model based on tf. idf value. As for idf, namely 
inverse document frequency, we adopt a segment in
929 
stead of document in the definition of idf. The def
inition of idf in our system is the following. 
of segments in the manual 
idf(t) = log ~ of segments in which t occurs + 1 
Then a segment is described as a vector in a vector 
space. Each dimension of the vector space consists 
of each term used in the manual. A vector's value 
of each dimension corresponding to the term t is 
its tf • idf value. The similarity of two segments is 
a cosine of two vectors corresponding to these two 
segments respectively. Actually the cosine measure 
similarity based on tf. idf is a baseline in evaluation 
of similarity measures we propose in the rest of this 
section. 
As the first expansion of definition of tf • idf, we 
use case information of each noun. In Japanese, case 
information is easily identified by the case particle 
like .qa( nominal marker ), o( accusative marker ), 
hi( dative marker ) etc. which are attached just af
ter a noun. As the second expansion, we use not only 
nouns (+ case information) but also verbs because 
verbs give important information about an action a 
user does in operating a system. As the third expan
sion, we use co-occurrence information of nouns and 
verbs in a sentence because combination of nouns 
and a verb gives us an outline of what the sentence 
describes. The problem at this moment is the way 
to reflect co-occurrence information in tf. idf based 
vector space model. We investigate two methods for 
this, namely, 
1. Dimension expansion of vector space, and 
2. Modification of tf value within a segment. 
In the following, we describe the detail of these two 
methods. 
2.1 Dimension
Expansion 
This method is adding extra-dimensions into the 
vector space in order to express co-occurrence in
formation. It is described more precisely as the fol
lowing procedure. 
1. Extracting a case information (ease particle in 
Japanese) from each noun phrase. Extracting a 
verb from a clause. 
2. Suppose be there n noun phrases with a case 
particle in a clause. Enumerating every combi
nation of 1 to n noun phrases with case particle. 
~t 
Then we have E"C'k combinations. 
k=l 
3. Calculating tf • idf for every combination with 
the corresponding verb. And using them as new 
extra dimensions of the original vector space. 
For example, suppose a sentence "An end user 
learns tile programming language." Then in ad
dition to dimensions corresponding to every noun 
phrase like "end user", we introduce tile new di
mensions corresponding to co-occurrence informa
tion such as: 
• (VERB, learn) (NOMNINAL end user) (AC
CUSATIVE programming language) 
• (VERB, learn) (NOMNINAL end user) 
• (VERB, learn) (ACCUSATIVE programming 
language) 
We calculate if. idf of each of these combinations 
that is a value of vector corresponding to each of 
these combinations. The similarity calculation based 
on cosine measure is done on this expanded vector 
space. 
2.2 Modification
of tf value 
Another method we propose for reflecting co
occurrence information to similarity is modification 
of tf value within a segment. (Takaki and Kitani, 
1996) reports that co-occurrence of word pairs con
tributes to the IR performance for Japanese news 
paper articles. 
In our method, we modify tf of pairs of co
occurred words that occur in both of two segments, 
say dA and dt~, in the following way. Suppose that a 
term tk, namely noun or verb, occurs f times in the 
segment dA. Then the modified tff(da, tk) is defined 
as the following formula. 
tf'(dA, tk) = tf(da, tk) 
\] 
+ 
t~ET~(tk,da,du) p=l 
\] 
+ 
tcETc( tt, ,d a ,dB ) P =1 
where cw and ew' are scores of importance for co
occurrence of words, tk and t~. Intuitively, ew and 
ew I are counter parts of tf • idf for co-occurrence of 
words and co-occurrence of (noun case-information), 
respectively, ew is defined by the following formula. 
ew(da,tk,p, tc) 
a(da,tk,p,G) x fl(tk,tc) x 7(tk,t¢) x C 
M(dA) 
where a(da, tk,p, G) is a function expressing how 
near /kand tc occur, p denotes that pth tk's occur
rence in the segment da, and /3(tk, G) is a normal
ized frequency of co-occurrence of tk and t~. Each 
of them is defined as follows. 
c~(dA, lk, p, to) = d(da, tk, p) dist(da, tk, p, tc) 
d(da, tk, p) 
930 
rtf(tk,tc) fl(tk,t~) 
atf(tk) 
where the function disl(da, tk,p, t~) is a distance 
between pth tk within dA and t~ counted by word. 
d(dA,tk,p) shows the threshold of distance within 
which two words are regarded as a co-occurrence. 
Since, in our system, we only focus on co-occurrences 
within a sentence, ct(da,tk,p,t~) is calculated for 
pairs of word occurrences within a sentence. As a 
result, d(da,h~,p) is a number of words in a sen
tence we focus on. atf(tk) is a total number of 
tk's occurrences within the manual we deal with. 
rtf(tk,tc) is a total number of co-occurrences of tk 
and tc within a sentence. 7(/~, l~) is an inverse doc
ument fl:equency ( in this case "inverse segment fre
quency") of te which co-occurs with tk, and defined 
as follows. 
~(lk, t~) = log(~) 
where N is a number of segments in a manual, 
and dr(to) is a number segments in which tc occurs 
with tk. 
M(dA) is a length of segment dA counted in mor
phological unit, and used to normalize cw. C is a 
weight parameter for ew. Actually we adopt the 
wdue of C which optimizes 1 lpoint precision as de
scribed later. 
The other modification factor cu/ is defined in al
most the same way as cw is. The difference between 
cw and cw' is the following, cw is calculated for 
each noun. On tile other hand, cw ~ is calculated for 
each combination of noun and its case information. 
Therefore, cw' is calculated for each ( noun, case ) 
like (user, NOMINAL). In other words, in calcula
tion of cw', only when ( noun-l, case-1 ) and ( noun
2, case-2 ), like (user NOMINAl,) and (program AC
CUSATIVE), occur within the same sentence, they 
are regarded as a co-occurrence. 
Now we have defined cw and cu/. Then back to 
the formula which defines tf'. hi the definition of 
tJ", 7~(h,, dA, d~) is a set of word which occur in 
both of da and dz~. Therefore cws and cu/s are 
summcd up for all occurrences of tk in da. Namely 
we add up all cws and cw% whose t~ is included in 
7'~( t k , d a , de) to calculate if'. 
3 Implementation
and Experimental 
Results 
Our system has the following inputs and outputs. 
Input is an electronic mannal text which can be 
written in plain text,l~'l~Xor IITML) 
Output is a hypertext in It'FML format. 
Electronic Manuals manual A manual B 
text Generator 
and Case Information 
..... \[ t f i~l~'culatlon 
Stmllanly Calculation 
\[ based on Vector Sjp~ce Model 
~H~pertext Link Genarator 
Morphdogical Analyms ----~ 
ystern 
HYPERTEXT 
I manual A manual B 
OUTPUT 
Figure 1: Overview of our hypertext generator 
We need a browser like NctScapc that can display 
a text written in WI'ML. Our system consists of four 
sub-systems showrl in Figure 1. 
Keyword Extva(-tion Sub-System In this sub
system, a morphological analyzer segments out 
the input text, and extract all nouns and verbs 
that are to I)e keywords. We use Chasen 1.04b 
(Matsumoto et al., 1996) as a morphological 
analyzer for Japanese texts. Noun and Case
information pairs are also made in this sub
system. If you use the dimension expansion de
scribed in 2.1, you introdnce new dimensions 
here. 
tf. idf Calculation Sub-System 
This sub-system calculates tf • idf of extracted 
keywords by Keyword Extraction Sub-System. 
Similarity Calculation Sub-System This sub
system calculates the similarity that is repre
sented by cosine of every pair of segments based 
on tf • idf values calculated above. If you use 
modilicatious of tf values described in 2.2, you 
calculated modified t f, namely tff in ttLis sub
system. 
Hypertext Generator This sub-system trans
lates the given input text into a hypertext in 
which pairs of segments having high similarity, 
say high cosine value, are linked. The similarity 
of those pairs are associated with their links for 
user friendly display described in the following 
We show an example of display on a browser in 
Figure 2. The display screen is divided into four 
parts. Tim npper left. and upl)er right parts show 
a distinct part of manual text respectively. In the 
lower left, (right) part, the tMe of segments that 
are relevant to tile segment displayed on the upper 
left (right) part are displayed in descending order of 
931 
................... . ............ nnn ~.l 
FiOJe Edit Vie-~ CJo ~oo~M Optiot~ D~'ectorY W~lc\[~ IleJtp 1\] 
Loz,tion: ~/~. forest, d,~£ ynLL ~. 5p/-d~ JJ~,~tmfr~re. htaa 
I 
1£ JUMAN 2.0 ~',¢) rsli I ¢ JUMAN :2.0 ~',@ \[x i ChaSenl.0",-a)~,~ Ill aUMANa'°''c°~'R'e* |l 
il /I 
o~fljx~r~. III ? *~l~z~J~o~fll L I 
3 ~I~J\[/'~'~JXT-Z*OOJ~rL,~'~Wt~'L ~ I11 >O~ip:~\[~ / I 
• illfi_~r~;'~.t:. ~L~t~ I-II / I --, -'-, -, " _,l .... ..... 
II II 
Figure 2: The use of this system 
similarity. Since these titles are linked to the cor
responding segment text, if we click one of them in 
the lower left (right) part, the hyperlinked segment's 
text is instantly displayed on the upper right (left) 
part, and its relewmt segments' title are displayed 
on the lower right (left) part. By this type of brows
ing along with links displayed on the lower parts, 
if a user wants to know relevant information about 
what she/he is reading on the text displayed on the 
upper part, a user can easily access the segments in 
which what she/he wauts to know might be written 
in high probability. 
Now we describe the evaluation of our proposed 
methods with recall and precision defined as follows• 
recall = ~ of retrieved pairs of relevant segments 
1~ of pairs of relevant segments 
~I of retrieved pairs of relevant segments precision = 
of retrieved pairs of segments 
The first experiment is done for a large manual 
of APPGALLARY(Ilitachi, 1995) which is 2.SMB 
large. This manual is divided into two volumes. One 
is a tutorial manual for novices that contains 65 seg
ments. The other is a help manual for advanced 
users that contains 2479 segments. If we try to find 
the relevant segments between ones in the tutorial 
manual and ones in the help manual, the number of 
possible pairs of segments is 161135. This number 
is too big for human to extract all relevant segment 
manually. Then we investigate highest 200 pairs of 
segments by hand, actually by two students in the 
engineering department of our university to extract 
pairs of relevant segments. The guideline of selection 
of pairs of relevant segments is: 
1 
09 
08 
o/ 
06 
05 
04 
03 
02 
01 
0 
Procision • 
20 40 60 SO 100 120 140 160 180 200 
Rank~ 
Figure 3: Recall and precision of generated hyper
links on large-scale manuals 
Table 1: Manual combinations and number of right 
correspondences of segments 
pair of manuals AC~z B A ¢~C B ¢:~C 
of all pairs 1056 896 924 
of relevant pairs 65 60 47 
1. Two segments explain the same operation or the 
same terminology. 
2. One segment exl)lains an abstract concept and 
the other explains that concept in concrete op
eration. 
Figure 3 shows tile recall and precision for nmn
bets of selected pairs of segments where those pairs 
are sorted in descending order of cosine similarity 
value using normal tf • idf of all norms. This result 
indicates that pairs of relevant segments are concen
trated in high similarity area. In fact, tile pairs of 
segments within top 200 pairs are almost all relevant 
ones, 
Tile second experiment is done for three 
small manuals of three models of video cas
sette recorder(MITSUBISHI, 1995e; MITSUBISHI, 
1995a; MITSUBISHI, 1995b) produced by the same 
company. We investigate all pairs of segments 
that appear in tlle distinct manuals respectively, 
and extract relevant pairs of segment according 
to the same guideline we did in the first experi
ment by two students of the engineering depart
ment of our university. The numbers of segments 
are 32 for manual A(MITSUBIStII, 1995c), 33 for 
manual B(MITSUBISHI, 1995a) and 28 for manual 
C(MITSUBISHI, 1995b), respectively. Tile number 
of relevant pairs of segments are shown ill Table 1. 
We show tile 11 points precision averages for these 
methods in Table 2. Each recall-precision curve, 
say Keyword, dimension N, cw+cw' tf, and Normal 
Query, corresponds to the methods described in the 
previous section. We describe the more precise deft
nition of each ill the following. 
932 
Table 2: 11 point average of precision for each 
method and combination 
Method A¢~ B AevC B ¢~C 
Keyword 0.678 0.589 0.549 
cw+cw' tf 0.683 0.625 0.582 
C 0.1 0.6 1.3 
~lim'et{sioni 0.684 0.597 0.556 
Normal Query 0.692 0.532 0.395 
Keyword: Using tf • idf for all nouns and verbs 
occuring in a pair of manuals. This is the baseline 
data. 
dimension N: Dimension Expansion method de
scribed in section 2.1. In this experiment, we use 
only noun-noun co-occurrences. 
cwTcw' tf: Modification of tf value method de
scribed in section2.2. In this experiment, we use 
only noun-verb co-occurrences. 
Normal Query: This is the same as Keyword ex
cept that vector values in one manual are all set to 
0 Mr
l, and vector values of the other manual are 
t f . idf . 
In the rest of this section, we consider the results 
shown above point by point. 
The ettbct of using if. idf information of both 
segment s 
We consider the effect of using tf • idf of two seg
ments that we calculate similarity. For coml)arison, 
we did the experiment Normal Query where tf.idf 
is used as vector value for one segment and 1 or 0 
is used as vector value for the other segment. This 
is a typical situation in IR. In our system, we calcu
late similarity of two segments.already given. That 
makes us possible using tf • idf for both segments. 
As shown in Table 2, Keyword outperforms Nor
nml Query. 
The effect of using co-occurrence information 
The same types of operation are generally de
scribed in relevant segments. The same type of op
eration consists of the same action and equipment 
in high probability. This is why using co-occurrence 
information in similarity calculation magnifies sim
ilarities between relevant segments. Comparing di
mension expansion and modification of if, the latter 
outperforms the former in precision for ahnost all 
recall rates. Modification of tf value method also 
shows better results than dimension expansion in 11 
point precision average shown in Table 2 for A-C 
and B-C manual pairs. As for normalization factor 
C of modification of tf value method, the smaller 
(.: becomes, the less tf value changes and the more 
similar the result becomes with the baseline case in 
which only if is used. On the contrary, the bigger C 
becomes, the more incorrect pairs get high similar
ity and the precision deteriorates in low recall area. 
As a result, there is an optimum C value, which we 
selected experimentally for each pair of manuals and 
is shown in 'fable 2 respectively. 
4 Conclusions

We prol)osed two methods for calculating similarity 
of a pair of seglnents appearing in distinct manuals. 
One is Dimension Expansion method, and the other 
is Modification of tf value method. Both of them 
improve the recall and precision in searching pairs of 
relevant segment.This type of calculation of similar
ity between two seglnents is useful in implementing 
a user friendly manual browsing system that is also 
proposed and implemented ill this research. 

References 

Robcrto Basili, Fabrizio (\]risoli, and Maria Teresa 
Pazienza. 1994. Might a semantic lexicon support 
hypertextual authoring? In 4th ANLP, pages 
174--179. 

David Elhs. Jonathan Furner-Hines and Peter Wil
lett. 1994. On the measurement of inter-linker 
consistency and retrieval effectiveness in hyper
text databases. In ,5'IGIR '94, pages 51-60. 

Ititachi, 1995. Allow to use the APPGALLERY, 
A PPGALLEIt Y On-Line llelp, tlitachi l,imited. 

Stephen J.Green. 1996. Using lexcal chains to build 
hypertext links in newspaper articles. In Proceed
ings of AAAI Workshop on Knowledge Discovery 
in. Databases, Portland, Oregon. 

S. Kurohashi, M. Nagao, S. Sat(), and M. Murakami. 
1992. A method of automatic hypertcxt construc
tion from an encyclopedic dictionary of a specific 
field. In 3rd ANLP, pages 239 240. 

Yuji Matsumoto, Osamu Imaichi, Tatsuo Ya
mashita, Akira Kitauchi, and Tomoaki Ilnallltlra. 
1996. Japanese morphological analysis system 
ChaSen manual (version t.0b4). Nara Institute of 
Science and Technology, Nov. 

MITSUBISHI, 1995a. MIT'SUBISHI Video Tape 
Recorder HV-BZ66 Instruction Manual. 

MITSUBISIII, 1995b. MITSUBLS'ItI Video Tape 
Recorder HV-F93 Instruction Manual. 

MITSUBISItl, 1995c. MITSUB1SIll Video 7'ape 
Recorder HV-FZ62 Instruction Manual. 

Gerard Salton, J. Allan, and Chris Buckley. \]993. 
Apl)roaches to passage retrieval in fifll text infor
mation systems. In SIGIR '93, pages 49-58. 

q'orn Takaki and Tsuyoshi Kitani. 1996. Rele
vance ranking of documents using query word co
occurrences (iu Japanese). I1)SJ SIG Notes 96-F1
41-8, IPS Japan, April. 

