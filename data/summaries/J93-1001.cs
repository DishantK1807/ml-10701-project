This is a particularly exciting area in computational linguistics as evidenced by the large number of contributions in these special issues: Biber (1993), Brent (1993), Hindle and Rooth (this issue), Pustejovsky et al.(1993), and Smadja (this issue).
J93-2005
Pustejovsky et al.(1993) use an EDA approach to investigate certain questions in lexical semantics.
J93-2005
As a result, the empirical approach has been adopted by almost all contemporary part-of-speech programs: Bahl and Mercer (1976), Leech, Garside, and Atwell (1983), Jelinek (1985), Deroualt and Merialdo (1986), Garside, Leech, and Sampson (1987), Church (1988), DeRose (1988), Hindle (1989), Kupiec (1989, 1992), Ayuso et al.(1990), deMarcken (1990), Karlsson (1990), Boggess, Agarwal, and Davis (1991), Merialdo (1991), and Voutilainen, Heikkila, and Anttila (1992).
A88-1019 C90-3030 H89-2014 H90-1069 J88-1003 P89-1015
Linguistics Volume 19, Number 1 has tended to favor rationalism, though there are some important exceptions, such as example-based MT (Sato and Nagao 1990).
C90-3044
paper by Brown et al.(1990) revives Weaver's information theoretic approach to MT.
J90-2002
There are three papers in these special issues on aligning bilingual texts such as the Canadian Hansards (parliamentary debates) that are available in both English and French: Brown et al.(1993), Gale and Church (this issue), and Kay and R6senschein (this issue).
J93-2003
Karlsson, E (1990).
C90-3030
Recently, most work in MT 2 In fact, the trigram model might be even better than suggested in Table 5, since the estimate for the trigram model in Brown et al.(1992) is computed over a 256-character alphabet, whereas the estimate for human performance in Shannan (1951) is computed over a 27-character alphabet.
J92-1002
Model Bits / Character ASCII Huffman code each char Lempel-Ziv (Unix TM compress) Unigram (Huffman code each word) Trigram Human Performance 8 5 4.43 2.1 (Brown, personal communication) 1.76 (Brown et al.1992) 1.25 (Shannon 1951) The cross entropy, H, of a code and a source is given by: H(source, code) = ~ ~ Pr(s, h I source) log 2 Pr(s I h, code) s h where Pr(s, h I source) is the joint probability of a symbol s following a history h given the source.
J92-1002
See Brown et al.(1993) for more details on the estimation of the parameters.
J93-2003
