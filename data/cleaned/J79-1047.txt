American Journal of Computational Litrguis tics Mi crof i che 4 7 A SURVEY OF SYNTACTIC ANALYSIS PROCEDURES FOR NATURAL LANGUAGE Computer Science Department Courant-Institute of Matheniatical Sciences New York University 251 Mercer Street, New York.
10012 FYris survey was prepared under contract No.
N00014-67A-0467-0032 with the Office of Naval Research, and was originally issued as Report No.
NSO-8 of the Courant Institute of Mathematical Sciences, New York ~niversity.
Copyright 0 1976 Association for Computational Linguistics k SURVEY OF SYNTACTIC ANN-YS IS PROCEDURES FOR NATURAL LANGUAGE RALPH GRXSHMAN Computer Science Department Courant Institute of Mathematfical Sciences New York University This survdy was prepared under contract No.
N00014-E7A-0467-0032 wCth the OffZce of NauaZ Research, and was originaZZy issued as Report No.
NSO-8 of the Couraat Institute of MathematicaZ Sciences, New York University.
SUMMARY This report includes a brief discussion of the role of automatic syntactic analysis, a survey of parsing procedures used in the analysis of natural language, and a discussion of the approaches taken to a number of difficult linguistic problems, such as conjunction and graded acceptability.
It also conta'ins precise.
specifications in the programming language SETL of a number of parsing algorithms, includinq several context-free parsers, a unrestricted rewriting rul e Parser, and a transformational parser.
Table of Contents Page The Role of Syntactic Analysis..................... i, Computational and Theoretical Lir:guistics..
........ I ...............
GENEFtAL SURVEY OF PARSING PROCEDITmS 1, 1 Early Systems : Context-Free and Context-Sensitive Parsers-......... ....I..........................L.
11 .........
Transformational Analyzers : First Sys terns.
lq Transformational Analyzers : Subsequent Developmepts 1::,>I ...............
Other Syntactic Analysis Procedures.
..-I -3 Parsing with Probability and Graded Acceytability ..
.--V 7s .......................
Conjunction and Adjunction..
L Tl ..........................
ALGORITHM SPECIFICATIONS.
...... Parsing Algorithms for Context-Free Grammars.
3 1 A Parser for Unrestricted Rewriting Rule Grammars..
54 . Parsing Procedures for Transformational Grammars..
60 APPENDIXA Very Short Introduction to SETL......
. 34 .......................................
BIBLIOGRAPHY 92 1.
INTRODUCTION The Computer Science Department of dew YorK University, under contract to the Office of Naval Research, has prepared a series of reports examining selected areas of artificial intelligence.
We hope in these aritical surveys to place in perspective the main lines of past research and thereby perhaps to suggest fruitful directions for future work.
As part of these surveys we have prepared precise specificatibns, in the programing language SETL, of some of the basic algorithms in each area.
These specifications are intended to provide clear points of reference for structuring our review of past work.
***** This first report is concerned with natural language processing sys tems, systems which are able to accept instructions 'or data in natural language.
In Very general terms, these systems process the text through several stages: (1) syntactic: analyzes the structure of each sentence (or other text unit) and rearranges the elements of the sentence to simplify its structures; this stage should recognize paraphrases due to altexnati~~e arra~lgements of words in a sentence (2) semahtic: restructures the sentences into the form used for internal processing, such as inference or data retrieval; depending on the application, the output may be a command in an information retrieval language, a structure based on some set of semantic "primitives", or a tabular structure suitable as a data base; this stage should recognize some of the paraphrases due to alternative choices of words.
(3) pracjmatic: interprets the text based on particular context (problem situation or data base) ; this stage should recognize sentences which are equivalent in effect, (such as "Thr~w that switch".
and "Turn on the light").
. The reader will note that these stages are very-.vaguely characteri zed.
Current language processing sys tems differ very greatly in their structure and not oven these general divisions can be identified in all systems.
Ihe pragmatic stage is the most heterogeneous and the common threads which do appear are based more on general problem-solving methods than on specifically linguistic techniques.
Since the semhntic stage maps into the notation required by the pragmatics, it is correspondingly varied.
There is, however, a fair amount of current research on the selection of semantic primitives or selnantic classes; some of this research is reviewed in the proceedi~~gs of a recent Courant Institute symposium on Directions in Artificial Ir~llciligence (Courant ComputPr Science Report No.
7) . The syntactic stage is by far the best established a'rrd-most clearly defined.
There is a general (although far from total) agreement on the most; basic u~lderlying principles, and thare are a number of widely-used procedures.
For this stage, therefore, it seems possible to present a survey of current research in some organized fashion.
In the report ~dlich follows, we have endeavored to show th.e relation between the various syntactic analyzers in terms of their historical development, linguistic assumptions, and a lalysis procedures.
For a broader survey of automated language processing, readers are referred to [Jalker 19731 1.1 The Role of Syntactic Analysis The systems we shall be describing are all motivated by particular applications requiring natural language input, rather than by purely linguistic considerations.
Consequently, the pjarsing of a text (determining its structure) w-ill be viewed as ari essential step preliminary to processing the information in the text, rather than as an end in itself.
There are a wide variety of applications involving natural language input, such as machine translation, information retrieval, question answering, conunand systems, and data collection.
It may therefore seem at first that there would be little text processing which wa id' be generally usefu,i beyond the determination of a structurjal description (e.
g. a par'se tree) for each sentence.
There are, however, a numbet of operations whfch can reqularize sentence structure, and thereby simplifythe subsequen vA application-specif ic processing.
For example, Some material in sentences (enclosed in brackets in the ekamples below) can be omitted or "zeroed": John ate cake and Mary [ate] cookies.
. . five or more [than five] radishes i3e talks faster than John [taLks].
. . . the man [whom] I met . Sentence structure can be regularized by restoring such zeroed information.
Other transformations can relate sentences with normal word order (I crushed those grapes.
That I like wine is evident).
to passive (Those grapes were crushed by me).
and cleft It is evident that I like wine).
constructions, and c= relate nominal (the barbarians' destruction of Rome) and verbal (the barbarians destroyed Rome) constructions.
Such transformations will permit further (e.
g., semantic) processing to concern itself with a much smaller nurnper of structures.
In addition, if the structures are appropriately chosen, operator-operand relations should be clearly evident in the output of the syntactic stage.
Some lexical processes, such as nominalization and lexical decomposition, are considered syntactic by some and semantic by others.
Whether a clear division between the syntactic md semantic stages is ~ossible at all has been a major point of controversy in linguistics -between interpretive and generative semanticists -over the past decade.
We may therefore expect that, while some transformations will clearly be the province of the syntactic stage and others the province of the semantic stage, there will be a considerable fuzzy areain between.
This, hmever, should not disqualify automatic syntactic analysis as an area of separate research; there is hardly a field of science or enqineering which is clearly deIineaked from its neighbors.
The last few years have seen most work in language processing devoted to the development of integrated sys terns, combining syntactic, semantic, pragmatic, and generative components.
This was a healthy and p~dicthle reaction to the earlier research, which had largely approached syntactic processing in isolation froin these other areas.
Pt produced some systcms whose modest successes dispelled the skepticism that natural language proccssors would ever be able to do anything.
These systcms indicated how syntactic, semantic, and praglnn tic informL~t ion IIIUS~ interact to select the correct sentence analysis.
It is now generally understood that syntactic processing by itself is inadequate to select the intended analysis of a sentence.
e should not conclude from this, however, that it is impossible to study the processes of syntax analysis separately from the other components.
Rather, it means that syntax analysis must be studied with an undarstanding of its role in a larger system and the info-rmation it should be able to call upcjn from other components (i ee., the processing which the subsequent con~ponen ts must do to select among the analyses produced by the syntactic component) . While recognizing the importa~~ce of total systems in insuring that none of the problems has fallen in the gaps between stages and been forgotten, it still seers that more specialized research projects are essential if the field-of natural lal~yuage processing is to mature.
The development of another total system will not advance the field unless it endeavors to perform some particular processing task better than its predecessors; the problems are too vast for each research project to usefully attack the problems involved in all the phases of processing at once.
Some researchers have asserted recently that hatural language processing can be done without syntax analysis.
It seems to US that such claims are exaggerated, but they do arise out of some observations that are not without validity: (1) For the rekitively simple, sentences whose slemantics is within the scope of current artificial dnt~lligence systems, sophisticated syntactic processing is unnecessary.
This Was certainly true of some early questipn-answering sys teqs, whose syntax was limited to a few fixed imperative structures, into which adjective and prepositional phrabe modifiers could be inserted.
It is questionable whether this is true of the most syntactically sophisticated of today's sys terns (such as Pdtrick's) In any case, it is hard to imagine how sentences of the complexity typical in technical writing c~uld be undetstood without utilizing syntactic (as well as semantic) restrictio$s to select the correct analysis.
(2) Syntactic analysis may appear in guisks other than the traditional parsing procedures ; it can be interwoven with other components of the system and cqh be embedded into the analysis programs themselves.
This will often increase the parsing speed considexably . The "grammar in program" approach which characterized many of the early machine translation efforts is still employed in some of today's systems.
Its primary justification seems to be parsing efficiency, but this should be a secondary, consideration for research purposes at present, sinoe most current systems are able to parse (or, as often, reject as unanalyzable) a sentence in under a minute.
More important as research goals should be the ability to manage grammatical complexity and the ability to communicate successful methods to others.
In both these regards, a syntactic analyzer using attmified, semiformal.
set of rules is bound to be more effective.
(3) Syntax analvsis can be driven by semantic analysis (instead of being a separate, earlier stage), and, in particular, can be done by logking for semantic patterns in the sentence.
Syntax analysis is done separately because there are rules of sentence formation and %ransformation which can be stated in terms of the relatively broad syntactic categories (tensed verb, count noun, etc.
) . If the semantic classes are subcategorizations of the syntactic ones then clearly the tr2i.p~ for~nntions could be stated in terms of sequepces of semantic classes.
For those trnnsfor~nations which are properly syntactic, however, we would find that several transformations at the semantic stage would be required in place of one at the syntactic stage; certain useful generalizations would bc last.
The strongest axgument of those advocating a scm~ntiss-dri.\.cn syntax is the ability of people to interpret sentcncos from semantic clues in the face of syntactic errors or missing infor11 mation ("I want to xx to the inovies tonight.
) . This argument works both ways, however -people can also use syntactic rules when semantics is lackirlg; Sox esample, to understand the function of a word in a sentence without knowing its meaning ("Isn't that man wearing a very frimple coat")?
. Ultimately, we want an analyzer which can work from partial information of either kind, and research in that directi~n is to be welcomed (sone work on parsing in the face of uncertainty has been done by speech-understanding groups).
At the same time, since sczcessful processing of "perfect" sentences is presumably a prerequisite for processing imperfect sentences, it seems reasonable to continue devoting substantial effort to the rrmsibrable pl-oblems which remain in analyzing perfect sentences.
1.2 ComputationaJ
and Theoretical Linguistics Theodtical linguists and the sort of computational linguists we have been considering espouse quite different research objectives . A primary interest of transformational linguists is expl;.ining grammatical competence -how people come to accept some sentences as grammatical and reject others as ungraunatical.
In parti6ular, they are concerned trri th language universals -principles of grammar which apply to all natural languages.
Computational linguists, in contrast, are usually delighted if they can manage to handle one language (two, if they 're translating).
Their primary concern lies ih transforming sentences -often assumed to be gramrnatioal -into a form acceptable to some particular application s ys tern.
They are concerned with the efficiency of such processing, whereas theoretical linguists generally don ' t worry about the recogni tion problem at all.
I.Ionetheless, the two specialties should have many common areas of interest.
Questions of grammaticality are important, because exuerience has shown that a grammatical constraint which+ one case determines if h sentence is or is not acceptable will in other cases be needed to choose between correct and incorrect analyses of a sentence.
The relations between sets of sentences, which are a prime focus of transformational grammar, particularly in the Harrisian f famework, are crucial to the success of syntactic analysis procedures, since they enable a large variety of sentences to be reduced to a relatively small number of structu'res . More generally, both specialties seek to understand a particular mode'of communication.
Traditional linguists axe interested in a mode whioh has evolved as an efficient means of communicating ideas between people; ultimately, we may hope that they will understand not only the principles of language structure, but also some of tlle reasons why language has developed in this way.
Computational linguists, in studying how language can be wed for man-machine communication, are really asking much the same questions.
They want to develop a mode of communication for which people are naturally suited and they want to understand the principles for designing languages which are efficient for comn~micating ideas.
PROCEDURES We can impose several rough groupings on the set of parsers in order to structure the follawing survey.
To begin withr we may try to separate those sys tcms davclobed with solme reference to transformational theory from the nont-ransforlnational sys terns.
This turns out slso to be an approximate historical di+vision, since most systems written since 1965 have made soir~c connection with transformational theory, even though their ihcthods of analysis nmy ke only di%tzantly related to transfol-mational mc ch an i s IIIS . Z'hc tr~~nsformatianal systcms may in turn be divided into those parsers which have bccn systematically clerivca from a specific transformational g~ncrative gracunar and those which have "sacrificed" this direct connection rrii th a generative grammar in order to obtain a more direct and efficient algorithm for recovering base struckures.
This division appears to be in part a result of our inadequate theoretical understanding of tran'sformational grammars, and may be reduced by some recent theore tical work on transformational grammar's.
2.1 Early
Systems : ContextLFree . and Context-Sensi tive Parsers The prctransformational systems, developed mostly between 1959 and 1965, were, with a few esccptions, parsers for contextfree languages, although cloaked in a nur~ber of different guises.
These sys terns were based on immediate constituent analysis, dependency tlieory, linguistic string theory, or sometimes no theory at all.
The largest and probably the most important of these early projects was the Harvard Predictive Analyzer [Runo 19621.
A predictive analyzer is a top-down parser for contest-free grammars written in Greibach normal form; this formulation of the grammar was adopted from earlier work by Ida Rhodes for her Russian-English translation project.
The size of the grammar was staggering: a 1963 report [Kuno 19631 quotes figures of 133 Uord classes 'and about 2100 productions.
Even with a grammar of this size, the system did not incorporate simple agreement restrictions of English syntax Since the program was designed to produce parses for sentences which were presumed to be grammatical (and not to differentiate between grammatical, and nongrammatica1 septences), it was at first hoped that it could operate without these restrictions.
It was soon discoveredr however, that these restrictions were required to eliminate invalid analyses of grammatical sentencds . Because the direct inclusion of, say, sub ject-verb number agreement would cause a large increase in an already very large grammar, the Harvarq group chose instead to include a special mechanism in the parsing program to perform a rudimentary check on number agreement.
Thus the Harvard Predictive Analyzer, though probably the most successful of the context4 ree analyzers, clearly indicated the inadequacy of a contextfree formulation of natural languqge grammar.
The ~arvard Predictive Analyzer parsing algorithm progressed through several stages.
The first version of the predictive analyzer produced only one analysis of a sentence.
The next version introdtxed an automatic backup mechanism in order to produce all analyses of a sentence.
This is an exponential time algorithm, hence very slow for long sentences; a 1962 report gives typical times as 1 minute for an 18 word sentence and 12 minutes for a 35 word sentence.
An improvement of more than an 0of magnitude was obtained in the final version of the program by using a bit matrix for a path-elimination technique [Kuno 19651.
When an attempt was made to match a nonterminal symbol to the sentence beginning at a particular word and no match was found; the corresponding bit was turned on; if the same symbol came up again later in the ~arsing at the same point in the sentence, the program would not have to try to match it again.
Another important early parser was the immediate constituent anal-yzer used at RAND.
his system used a grammar in Chornsky normal form.
and a parsing algorithm designed by John Cocke, which produced all ~nalyses bottom-up in a single left-toright scan of -the sentence [Hays 1967 f.
This was a fqst algorithm but because all parses were dcve loped si~~~ult-nncous ly it nccdcd a lot of space for long sentences; the Rand system appears therefore to have bccn limited to sentences of about 30 words.
A differe~k bottom-up ~IIA~YS~S PI-occadure was ust2d ill the first linguistic string t~n~lh*sis pqogram dcval~pcd at the University of Pennsylvania [Harris 19651.
This proctxlure, called a cycling cancelling.
automaton, makes n stvies leftto-right passes through.
the sentence; in each p~~ss one type of reduction was performed.
The string parser recognized two classes of strihgs : first o~der, pot containing verb-object, and second order, containing verb-ob ject ; the reduction i,f the sentences was correspondingly done in two stages.
In addition & to these reductions, which corresponded to context-f ree rules the parsipg program also included some syntactic restricti~ns which were checked when second order strings were reduced.
A system incorpo;t-ating this &lrclinq automaton ~chcme was later used by Bross at Rdsewll Park for the analysis of rrlcdical reports [Bross 196 8, ~hapiro 19711.
As far as we k~low, onJy one major parsing system has been developed using a context-sensitive phrase structure graimar, This was DZACON, Direct English Access and Control, which was designed as a natural laqguaqe interface to a co~r~isnd, contml, and information retrieval system for the Army and was developed at General Electric [Craig 1'3661.
DZACOiJ was one of the first systems to provide flexible, systematic interaction between the parser and the semantic con~ponent.
Associgated with each production in the grammar was a semantic rule.
~hese rules operated on a ring-structured data base and had the functions of locating, adding, and changing information in the data base.
The parsing was done bottom-up, developing all analyqes of the sentence in parallel.
As each reduction was performed, the associated semantic rule was invoked.
In the Case of a query, the sequence of rules associated with the correct analysis was Q supposed to Locate the desired answer in the data base.
In some cases a rule could not be applied to the data base (e.g., a particqlar relation between two items did not exist) ; the rule then returned a fail-ure signal to the parser, indicating that the analysis was semantically anomalous, and this analysis was aborted.
Woods has noted [Woods 197Ql that the parser used in the Dmm4 pro jecf may produce redundant parses, and has given a parsing algorithm for context-sensitive languages which repedies this deficiency . 2-2 Transformational Analyzers: First Systems When the theory of transformational grammar was elaborated in the early l.9601s there was considerable interest in finding a corresponding recognition procedure.
Because the grammar is stated in a generative form, however, this is no simple matter.
A &hornsky) tree transformational grammar consists of a set of context-sensitive phrase structure rules, which generate a set of base treesa, and a set of transformations, which act on base trees to produce the surf ace trees.
A (Harris) string transfo~tional grammar consists of a finite set of sequences of word categories, called kernel sentences, and a set of transformations which combine and modify these kernel sentences t6 e the other sentences of the language.
There are at least three basic problems in reversing the generative process: (1) for a tree transformational grammar, assigning to a given sentence a set of parse trees which includes all the surface trees which would be assigned by the trans formational grammar (2) giveq a tree not in the base, determining which sequences of transf ormatigns might have applied to generate this tree (3) having decided on a transformation whose result may be the present tree, undoing this transformation If we attack each of these problems in the most straightforward manner, we are likely to try many false paths which will not lead to an analysis.
For the first problem, we could use a context-£ ree gramrnar which will give all the surface trees assigned by the transformational granunar, and probably lots more The superabundance of ''false" surface trees is aggravated by the fact that most E~~glish words have more than one word category (play more than one syntactic role), although normally only one is used in any given sen'tcnce.
F the second and third prpb?ems, rrie can mnswuct a set of reverse tYans formations ; however, since we are probably unable to determine uniquely in, advance the transformations which produced a given tree, we will have to try many sequences of reverse transfolrrnations which will not yield a base tree.
Because of these problems, the earliest recognition procedure, ctlggested by Matthews, was based on the idea of synthesizing trees to match a given sentence.
Although some checks were to have been made against the sentence during the generation procedure, it was still an inherently aery ir~efficient procedure and was never implemented.
Two. major systems were developed in the mid-GO 's, however, which did have limited success: the system of Zwicky et al.at MITRE and that of Petrick.
The transf orrnational generative grammar from which the MITRE group worked had a base component with about 275 rules and a set of 54 transformations [Zwicky 19651.
For the recognition proceIt dure they developed manually a contextf ree covering" grammar with about 550 productions to produce the surface trees and a set of 134 reverse transformational rules.
heir recognition procedure had four phases: (1) analysis of the sentence using the context-free covering grammar (with a bottom-up parser) (2) application of the revexse trans rorrnational rules 'i3) for each candidate base tree produced by steps (1) and (2), a check whether it can in fact be generated by the base component (4) for each base tree and sequence of transformations which passes the test in step (3), the (forward) transfoZmations are applied to verify that the original sentence can in fact be generated (The final check in step (4) is required bccause the covering grammar may lead.
to spurious matches of a transformation to the sentence in the reverse transformational process and because the reverse transformations may not incorporate' all the constraints included in the forward transformations.
) The covering grammar produced a large number of spurious surface analyses which the parser must process.
The 1965 report for example, cites a 12 word sentence which produced 48 parses with the covering grammar; each must be followed through steps (2) and (3) before most can be eliminated.
The system was therefore very slow; 36 minutes were required to analyze one 11 word sentence.
TWO measures were taken Sy the MITRE group to speed up the program: "super-trees" and rejection rules [Walker 196 61 . "Super-trees" was the MITRE term for a nodal span representation, in which several parse trees were represented in a single structure.
They intended to apply the reverse transformations to these super-trees, thus processing several possible surf ace trees simultaneously; it is not clear if they succeeded in implementing this idea.
Rejection rules were tests which were applied to the tree during the reverse transformational process (step (2) above), in order, to eliminate some trees as early as possible in the parsing.
The rejection rules incorporated some constraints wi~j-ch previously were only in the forward transformational component, and so eliminated some trees in step 12) which before had survived to step (4) . The rejection rules had a signific:ant effect on parsing times the 11 word 1 sentence which took 36 minutes hefore now took only 6 The system developed by Petrick [Petrick 1965, 1966; Keyser 19671 is similar in outline: applying a series oE reverse transformations, checking if the resulting tree can be generated by the base component, and the: verifying the analysis by applying the forward transfo~mations ta the base tree . There are, however, several diifere~ces from the M'TTRE system, motivated by the desire to have a parser which could be produced automatically from the generative formulation of the grammar.
Petrick devised a procedure to generate, from the base component and trans formations, an enlarged contextfree granunar sufficient to analyze the surface sentence structures.
He also automatict*J.ly converted a set of forward transformations mi?ting certain conditions into pseudo-inverse (reverse) transformations.
HAS parsing procedure also oif fered from the dITRE algorithm in the way in which the reverse transformations are applied.
In the MITRE: program reverse transformations operated on a sentence tree, just like foqvard trarlsformations in a Chonlsky grammar.
Petrick, on the other hand, did riot construct a surface tree in the analysis phase; when a particular reverse transformation came up for consideration, he built just enough structure above the sentence (using the enlarged context-£ ree grammar) to determine if the transformation was applicable.
If it was, the transformation was applied and the structure above the sentence then torn down again; what was passed from one reverse transformation to the next was only the string of word categories.
In the verifying phase, of course, Pexrick had to follow the rules of Chon~ky grammar and apply the forward transformations to a sentence tree.
The price for generality was paid in efficiency.
Petrick's problems were more severe than MITRZ's for two reasons.
~irst, the ~bsence of a sentence tree during the application of the reverse trans formational rules meant that many sequences of re-rse transformations were tried which did not correspond to any sequence of tree transformations and hence would eventually be rejected.
Second, if several rever se transformations Could apply at some point in the analysis, the procedure could not tell in advance which would lead to a valid deep structure.
Consequently, each one had to be tried and the resulting structure followed to a deep structure of a "dead end" (where no more transformations apply).
This produces a growth in the number of analysis paths which is exponential in the number of reverse transformations applied.
This explssion ntn be avoided only if the reverse transformations include tests of the current analysis tree to d6terrnine which transformations applied to generate .this tree.
Such tests were included in the manually prepared reverse transformations of the MITRE group, but it would have been fax too complicated for Pe trick to produce such tests automatlcally when inverting the trans formations.
Potrick's system has been significantly revised over the past decade [Petrick 1973, Plath 1974aI.
In the current system the covering grammar and reverse trans formations are both prepared manually.
The trans formational decomposition process works on a tree (as did MITRE 's), and considerably flexibility has been .,provided in stating the trankformations and the conditions of applicability . The transformations and conditions may be stated either in the traditional dorm (used by linguists) or in terms of elementary operations combined in LISP procedures.
The resulting system is fast enough to be used in an information retrieval system with a grammar of moderate size; most requests are processed in less than one minute.
2.3 Transformational
Analyzers: Subsequent Developmi!ints One result of the early transforrnati~nal systems was a recognition of the importance sf finding an efficient parsing procedure if traiisformationa.l analysis was ever to be a useful 19 techni~ue.
As the systems indicated, there are two main obstacles to an efficient procedure.
First, there is the problem of refini'ng the surface analysis, so Chat each sentence produces fewer troes for which transformational decomposition must be atten~pted.
This has generally been approached by using a inore 1% powerful mechanism than a context-.free parser for the surface analysis.
Second, there is the problem of dctcrrnining the base structure (or kernel sentences) from the surface struc.t:ure in a relatively direct fashion.
This has generally been done by associaEing particular rules for building the deep structure with rules of the surface structure analysis.
The approach here has generally been ad hoc, developing a reverse mapping without mplicit reference to a evrresponding set of fo~ward trans format ions.
Several groups which have played a significant role in the development of current parsing systems have been tied together by their comofl use c>f recursive transition networks.
Althouqh their use of these transition networks is not central to their basic contribution, it is frequently referred to and so deserves a few words of explanation.
A transition network is a set of nodes (including one initial and at least one terminal node) and a set of directed arcs between the nodes, labeled with symbols from the language; it is a standard representation for regular languages.
A recursive transition network is a set of transition networks in which the arcs of one network may also be labeled with the names of other networks; it is a form of representation of context-f ree languages.
In contrast to the usual context-f ree phrase structure grammars, this is equivalent to allowing regular expressions in place of finite sequences of elements in productions.
This does not increase the weak generative capactity of the grammars, but allows nonrccursive formulations for otherwise recursive const-ructions.
The first system using such a network was developed by Thorne, Bratley, and Dewar at Edinburgh [Thorne 1968, Dewar 19691.
They started with a regular base grammar, i-e., a transition network.
The importake of using a regular base lies in their claim that some transformations are equAvalent in effect to changing the base to a recursive transition network.
Transformations which could not be handled in this fasion, such as conjunction, were incorporated in^^ the parsing program.
Parsing a sentence with this surface grammar should then also give some indication of the associated base and transfsLTnational structure.
Their published papers do not describe, however, thee process by which the surface grammar is constructed and so it is not clear just how the transformation and base structure is extracted, from their parse.
The recutsive transition network was developed into an augmented recursive transition network grammar in the system of Bobrow and Frasex mw 9 An ausmented network is one in which an arbitrary predicate, written in some general purpose language (in this case, LISP).
may be associated uith each arc in the network.
A transition in the netw~rk is not allowed if the predicate associated with the arc fails.
These predicates perform two functions in the grammar.
First, they are used to incorporate restrictions in the language which would be difficult or impossible to state within the amtext-free mechanisms of the recursive network, e.
g., agreement restrictions.
Second, they are used to construct the deep structure tree as the sentence is being parsed.
The augmented transition network was further developed by Woods at Bolt Beranek and Newman.
In order to regularize the predicates, he introduced a standard set of operations for building and testing the deep structure lWoods 1970bl.
He considerably enla~ged the scope of the grammar and added a semantic component fbr translating the deep structure into information retrieval commands.
With these additions, the system served as a moderately successful natural language input interface to a retrieval system for data about moon rocks [Woods 1972, 19731.
The augmented transition network, and in particular the formalism developed bv Woods, has proven ta be an effective instrument for constructing natural language front-ends which is relatively simple to implement and use; it is probably the most widely used procedure today.
Like several of the systems described above, Proto-RELADES, developed 1 IBM Cambridge [Culicover 19691, tried to obtain a11 efficient tr~nsfor~national decomposition algorithm by linking the rules for building the deep structure to the productions of the surface granmar.
Their surface granunar was also augmented by restrictions (in PL/I this time).
Aorvover, their system differed from those mcntioncd earlier in several important respects : ~irst, the surf ace grammar allowed context-sensitive as well as contest-free rules.
Second, the rulcs which built the deep structure during the parse were in the form af reverse transformations acting or! an (incomplete) sentence tree (in contrast to the rules used by V400dsr for example, which first putwer& -8nto registers labeled "subject", "verb", and "object" and later build a tree out of them).
Proto-RELADES was tested as a restricted English language preprocessor for a library card catalog retrieval system [Loveman 19711.
One drawback of these procedures was the relatively ad hoe methodsr from a linguistic point of view, used to construct the surface grammars and to tie them in to the appropriate reverse trans formations.
A more principled approach to trans formational decomposition was proposed by Joshi and Hiz 1962, Biz 19671.
In contrast to the systems described above, their procedure was based on Harris' string transformational granunar.
One advantage of the Harrisian theory over that of Chomsky is the theoretical basis it provides for the segmentation of the sentence into "linguistic strings" (Chomsky 's theory, in contrast, makes no general assertions about the surface structure of sentences).
The procedure of ~oshi and Hiz was predicated on the claim that, from an analysis of the sentence into linguistic strings, one could directly determine the transformations which acted to produce the sentence, without having to try many sequences sf reverse transformations.
Their proposed sys tern therefore consisted of a procedure for linguistic string analysis (a context-free parsing problem at the level,of simpli fication of their original proposal) and a set of rules which constructed from each string a corresponding kernel-like sentence.
Their original proposal was a simplified scheme which accounted for only a limited set of trans.formations.
It has been followed by a good deal of theoretical work on adjunct grammars and trace conditions [Joshi 19731 which has laid a fbrrnal basis for their procedures.
These studaies indicate kow it may be possible, starting from a transformational grammar not specifically oriented towards recognition, to dkterrnine the features of a sentence which indicate that a particu1a.c transformation applied in generating it, and hence to produce an eff icient analysis procedure* Another group which has used linguistic string analysis is the Linguistic Strinq Project at New York University, led by Sager [Sager 1967, 1973; Grishman 1973a, 1973131.
Their system, which has gone through ssveral versions since 1965, is based on a contextfree grammar augmented with restrictions.
Because they were conce ned with processing scientific text, rather than commands or queries, they were led to develop a grammar of particularly broad coverage.
The present gramrr~ar has about 250 context-free rules and about 200 restrickions; although not as swift as some of the smaller systems, the parser is able to analyze most sentences in less than one minute.
Because of the large size of their grammar, this group *has been particularly concerned with techniques for organizing and specifying the grammar which will facilitate further development.
In particular, the most recent implementation of their system has added a special language designed for the economical and perspicuous staement of the restrictions [Sager 19751.
One of the earlier versions of this system, with a much more restricted grammar, was used as the front end for an information retrieval system developed by Cautin at the University of Pennsylvania [Cautin 1369 ] . The ~inguistic String Project system has recently been extended to include a transformational decomposition phase; this phase follows the linguistic? string analysis [IIobbs 19751.
As in the case of the Joshi-IIiz parser, thc strings ide~~tificd in the sentence generally indicate which rcvcrsc transfrsrmatisns must be applied.
'?he trans'forrnations are written in an extension of the 1-anguage which was used fur writing the restrictions.
The systelus of Woods, Petrick, and Sayer exhibit a range of approaches to the prob J.m of tral.lsf or~naticanal deceinpos i tion.
Their parsing p~octzclurcs are si filar in,nany respects: they have a contcxt-£see granunar as the Ertt~uowsrk for their surface analysis, and they use procedures both to cspress gral1~11rltical co~~straints and to effect the reverse transformations.
Petrick's system differs from the others in two primary respects : the restrictions on the context-free grammar are imposed by filtering trans forn~ations which act early in the transformatipnal phase to reject ill-formed trees, rather than by procerlures operating during the surface analysis.
This wol ld seem to be disadvantageous from the point of view of efficicncy, since erroneous parses which might be aborted at the 'beginning of the suxface analysis must be followed through the entire surface analysis and part of the transformational deconpssi tion.
Second, the trans formations are not associated wi.m particular productions of the surface grarxnar, but rather with particular patterns in the tree ("structural descriptions"), so pattern match; ng operations are required to determine 1;rhich transformations to apply.
These differences reflect Petrick 's c'iesire to remain as close as is practical to the formalism of transformational linguistics.
The primary distinction of the Woods system is that the deep structure tree is built during the surface analysis.
Consequently, his "transformati.ona1" procedures consist of tree building rather than tree transforming operations.
The tradeoffs between this approach and the two-stage analyzers of petrick and Sager are difficult tg weigh at this time.
They are part of the more general problem of parallel vs.
serial processing; e.g. should semantic analysis be done concurrently with Syntactic analysis.
Parallel processing is preferred if the added time required by the deeper analysis is outweighed by the fraction of incorrect analyses which can be eliminated early in the parsing erocess.
In the case of s ?mantic analysis, it clearly depends on the relative complexity of the syntactic and semantic components.
In the case of transformational analysis, it depends on the fraction of grammatical and selecti~nal constraints which can be expressed at: the surface level (if most of these can only be realized through transformational allalysis, concurrent trans formational analysis is probably more efficient).
This may depend in Lurn on the type of surface analysis ; for example, the relationships exhibited by linguistic string analysis axe suitable for expressing many of these constraints, so there is less motivation in the Linguistic String Project system for concurrent trans formational decomposition.
2.4. Other Syntactic Analysis Pxocedures The system developed by Winograd at M.
I. T.
[Winograd 19711 for accepting English commands and questions about a "block world" also uses a context-free grammar augmented by restrictions.
Winograd's context-free grammar was encoded as a set of procedures instead of a data structure to be interpreted, but this is not a material difference.
His grammar is based I1 on Halliday 's systemic grammar" to the extent that it extracts from a sentence the set of features described by Halliday; however, Halliday 's grammar (at least in its present stage of' development) is essentially descriptive rather than generative, so most of the detailed grammatical structure had to be supplied by Winograd.
His parser does not construct a deep structure; rather, it builds semantic structures directly during parsing.
The primary distinctive feature of his system is the integration of the syntactic component with senantics and pragmatics (the manipulation of objects in the block world) ; hi$ parser is thus able to use not only syntactic constraints but also semantic and pragmatic information in selecting a proper sentence analysis.
With regard .to the serial vs.
parallel distinction drawn in the previous section, his system would be characterized as highly patallel.
A number of natural language systen~s ?lave used granunars composed of finrestricted phrase-structure rewriting rulcs.
Since unrestricted rewriting rules, like transformational grammars, can be used to define any recursively enumerable language, they may be suf ficicnt for analyzing both surface and deep structure.
As with transformational.
grallunars, it will in practice be necessary to inlpose some cons tralnt (such as ordering) on the rules, so that the language defined is recursive; otherwise a parser will never be able to determine whzther some sentences are grammatical or not.
One parser for unrestricted rewriting rules was described by Kay [Kay 19671.
This parser includqd a number of mechanisms for restricting the application of rules, such as rule ordering, specifying part of the structure dominated by one elernentkof the rule, or requirihg the equality of the structures don~inatcd by two elements.
These mechanisms do not increase the gcnerative power of the grammars, but are designed to make granunars easier to write.
Kay described how his parser could be used to effect some gfeverse transformations.
Kay's parser was incorporated into a system called REL (Rapidly Extensible Langllage) developed by Thompson, Dostert, et al.at the California Institute of Technoloqy [Thompson 1969, Dostert 19711 . Kay's original parsor was auqmented by allowing a set of binary features to be associated with each node, including feature tests as park of the rewrite rules, and permitting more general restrictions where the features were inadequate.
The REL system was designez to support a number of graimnars, each interfaced to its own data base.
One of these is REL English, which analyzes a subset of English into a set of sub ject-verb-ob ject-time modifier deep structures ; this grammarhas 239 rules.
In support of the use of general rewrite rules with features, they note that only 29 of the 239 rules required constraints which could not be conveniently stated in terms of feature tests.
This is also a factor in efficiency, since binary feature tests can be performed very quickly.
Another system which uses unrestricfed rewriting rules with optional conditions on the elements is the "Q" system developed by Colmerauer [Colmerauer 19701.
*his system is presently being used in a machine translation prqject af the Universitv of Montreal [Kittredge 19731.
Colmerauer and de Chastellier [de Chastellier 196 9 ] have also investigated the possibility of using Wijngaarden grammars (as were developed for specifying ALGOL 6 8) for :tr.ansformational decomposition and machine translation.
Like unrestricted rewriting rules, W-grammars can define every recursively enumerable laquage, and so can perform the functions of the syrface and reverse transformational components.
They show how portions of transformational grammars of English and French may be rewritten as W-grammarsr with the pseudo-rules in the W-grammar taking the place of the transfol'mations~ 2.5 Parsinq with Prop-ability and Graded Acceptability In all the systems dqscribed above, a sharp line was drawn between correct and incomect parses: a teminal node either did or did not match the next word in the sentence: an analysis of a phrase was either acceptable or unacceptable.
There are circumstances under which we would want to relax these requirements.
For one thing, in analyzing connected speech, We segmentation and identification of words can never be done with complete certainty.
At best, one can say that a certain sound has some probability of being one phoneme and son= other probabiJi ty of being another phoneme; some expected phonemes may be lost entirely in the sound received.
consequently, one will associate some nufier with each terminal node, indicating the probability or quality of match; noflcrminal nodes will be assigned some value based on the values of the terminal nodes )xneath.
Another circuinstance arises in natural language systems which aro sophisticated mough to realize that syntactic and semantic rcs trictions are rarely all-or-nothing affairs, and that some restrictions arc stronger than others# Fur rsan~pla, the nominative-accusative distinction has become qui tc !tqc,ak for relative pronouns (?The man ~17x1 I met 1'ester;Flay.
) but remains strong for personal pronouns (*The man whom, me ~nct yesterday.).
As a result, a parser which wants to get the best analysis even if every analysis violates some constraint must associate a measure of grammaticality or acceptability with the analyses of portions of the sentence, and ultimately with the analyses of the entire sentence.
In principle, one could generate every sentence analysis with a nonzero acceptability or probability of n~atch, and then select the best analysis obtained.
IIobbs [1974] has described a modification to the bottom-up nodal spans parsing algorithm which uses this approach.
WPlks [1975] uses an essentially similar technique in his lLuguage analyzer based on "preference semantics " A more efficient approach, called "best-first" parsing, has been developed by Paxton and Robinson of the Stanford Research Institute as part of a speech understanding system axto ton 19731.
Their procedure involves a modi fication of the standard top-down serial parsing algorithm for ccntext-free grammars.
The standaqd algorithm generates one possible parse tree until it gets stuck (generates a terminal node which does not match the next sentence word) ; it then "backs up" to try another alternative.
The best-first procedure instead tries all alternatives in IleL A measure is associated with each alternative path, fadicating the likelihood that this analysis matches the sentence pwssed so far and that it can be extended to a let@ s@xtence analysis.
At each moment, the path with the higbsst likelihood is extended; if its measure falls below that otner path, the parser shifts its attention to that 2-6 ur~lon4uhction and Adjunction mere am$ certain pervasive natural language c~nstructivns rch ds not fit naturally into the standard syntax analysis dues, such as augmented context-free parsers.
Two of se are coordinate conjunctions and adjuncts.
Special reastAres have Deen developed to handle these constructions; these measur&s deserve brief mention here.
allowed patterns of occwl-rence of con joinings in a sentence are quite regular.
Loosely speaking, a sequence of e nts in the sentence tree may be followed by a con junction aad by same or all of the elements immediately preceding the jrmction.
For example, allowed patterns of con joining subject-verb-ob ject-and-sub ject-verb-ob ject (I drank dw and nary ate cake.
), sub ject-verb-ob ject-and-verb-object a milk and ate cake.
) and sub ject-verb-ob ject-and-ob ject [I ikank dlk and seltzer.
) . There are certain exceptions, known as ing phenomena, in which one of the elements following the amjlmction may be omitted; for example, sub ject-verb-object-andject-object (I drank milk and Mary seltzer).
. trouble with coordinate conjunctions is that they can t anywhere in the structure of a sentence.
Thus, it would be possible tr, extend a context-free surface to allow for all*possible conjoinings, such an extension increase the size of the grammar by perhaps an order of . The alternative scheme which has therefore been developed involves the automat'ic generation of productions which allow for conjunction as required during the parsing process.
When a conjunction is encountered in the sentence, the normal parsing procedure is interrupted and a special co~ljunction node is inserted in the parse tree.
The alternative values of this node provide for the various conjoi~~cd element 'sequences allowed at this point.
An interrupt mechanism of this sort including provision for gapping, is part of the Linguistic String Project parser [Sager 19671.
A similar mechanism is included in Woods ' augnlen ted transition network parser [Woods 19731 and a number of other sys terns.
This solves the problem of correcting the context-frcc granmar for conjunctions, but the contest-f rcc granw~lar is generally only a small part of the total system.
The task remains of modifying the routines which enforce granxmatical constraints and the transformations to account for con junctions.
Since practically every routine which examines a parse tree is someWow affected by conjunction, this can be a large job, but fortunately the changes are very regular for most routines.
The Linguistic String Project grammar; by performing a&l operations on the parse tree through a s~nall nunbcr of lowlevel routines, was able to localize the changes to these routines and a srngll number of restrictions (such as nun&er agrcerneJt) which are specially af fectcd by con junction [Raze 19741.
Certain classes of adjuncts or modifiers give rise to a different kind of problem: a high degree of syntactic ambiguity.
For instance, in the sentence) "I fixed the-pipe under the sink in the bathroom tzri th a wrench.
" there is no syntactic basis for deciding whether the pipe had a wrench the sink had a wrenoh, the bathroom had a wrench, or the tixing was done with a wrench.
If semantic and pragmatic restrictions are invoked during the syntactic analysis, the parser will have to generate several analyses, all but one of which will (hopefully) be rejected by the restrictions ; this is moderately inefficient.
If syntactic analysis precedes semantic processing i the ambiguities of the various adjuncts wi 11 be multiplied producing dozens of analyses for a sentence of moderate size ; this is hopelessly inefficientA more efficient solution has the parser identify the adjuncts and list for each adjunct the words it could be modifying, without generating a complee separate arp-alysis for each possibilfty.
The ambiguities associated with the adjuncts are thus factored out.
The semantic and pragmatic components may then choose for each adjunct its most likely or acceptable host (modified word).
This may be done either during the syntactic analysis [woods 1973, Simmons 19751 or after the syntax phase is complete [~orgida 1975, Hobbs 19751.
3. ALGORITHM SPECIFICATIOtJS We present below precise specifications for some of the parsing algorithms which have been discussed.
These algorithms are presented in SETL, a programming language which is based on concepts Prom set theory and has been developed at New York University by a group led by Jack Schwartz.
The large variety of data types, operators, and control structures in SETL makes it possible to specify the algorithms in a relatively compact and natural fashion.
An implementation is available which includes most of the fcaturcs of the specification language, so that algorithms can be tested in essentially the form in which they are published.
A description of the subset of SETL whici~ has been used in this report is given in the appendix.
3.1 Parsing
Alqorithms for Context-Free Grammars _.
Context-free grammars played a major role i the early stages of automatic natural language analysis.
A1 though they have now generally been superceded by more con~plex and powerful grammars, many of these grammars are based on or have as one of their components a context-f ree grammar.
The selection of an efficient context-free parser theref ore remains an i~nportant considerat ion in natural Language analysis.
Jecause so many different context-free parsers have been proposed, a comprehensive survey would be impracti cable.
We si~all rather present a taxonomy according t.u which most context-free parsers can be classified, and illustrate this classification with five of the possible basic zlgorithms.
At the end we shall mention which 6f these are beinq used in current natural language systems . T?L~ first division we shall make is according to the amount of memory space requLred by the parser.
Type 0 parsers store only the parse tree currently being built.
The other parsers grabally accumulate data from which all parses of a sentence can be extracted; types 1, 2 and 3 store this data in decreasingly compact representations.
The four types are : (0) Develops a single parse tree at a time; at any instant the store holds a set of nodes corresponding to the nodes of an incomplete potential parse tree ft) The store holds a set of nodes, each of which represents the fact that some substring of the sentence, from word f to word R, can be analyz~d as some symbol N.
(2) The store holds a set of nodes, each of which represents an analysis of some substring of the sentence, from word f to word a, as some symbol N (if there are several different analyses of words f to 11 as some symbol N, there will be several nodes corresponding to a single node in a type 1 parser).
(3) The store holds a set of nodes, each of which corresponds to an analysis of? some substring of the sentence; from word f to word a, as some symbol N appearing as part of some incomplete potential parse tree (if symbol N, spanning words f to !L, appears in several of the incomplete potential parse trees, there will be several nodes corresponding to each node in a type 2 parser) . Type (0) parsers require only an qmount of storage proportional to the length of the input sentence.
The storage requirements of type (1) parsers grow as the cube of the length, while the requirements for types (2) and (3) grow exponentially.
A second division can be made between top-down and bottom-up parsers.
A third criterion for classification is whether alternative parses of a sentence are all produced together (parallel parser) or are generated sequentially (serial parser) ; this division doe$ not apply to type (0) parsers.
Finer divisions can be made of some of these categories.
For example, among bottom-up parsers we can distinguish those which perform a reduction only when all required elements have been found from those which make a tentative reduction when the first element of a production is found (so-called " left-corner parsers").
Parallel parsers can be classified according to the orderin(strategy they use in building nodes: by leftmost or rightmast word subsumed i . e, spanned) by the node or by levell.
In addition, we shall not consider a number of optimization strategies, such as selectivity matrices and shaper and generalized shaper tests for top-down parsers.
We shall naw describe algorithms in five of the cateyorics: A Type 0 Top-down serial B Type 2 Bottom-up parallel C Type 1 Bottom-up parallel D ~y~e' 2 Top-down seri a1 E Type 1 Top-down serial We have not included any type 3 parsers bccausc, despite their profligate use nE storage, they do not operate inuch Easter than type 0 parsers.
The only reported use of such a parser of which we are aware is the "Error-Correcting Parse Algorithm" of Irons (Cornm.
ACM 6, 669 (1963) ) . A top-down left-to-right parallel strategy was employed so that the parser could make a suitable modification to the sentence when it "got stuck1' because of an error in the input.
STSITL ~rocedures are given for these five parsers.
The input data structures are the same in all cases: The sentence, passed through parameter SENTENCE, is a tuple.
The elements 0 the tuple, the words of :he sentence, are to be matched by terminal symbols from the grammar.
The context-f ree grammar, passed through parameter GRAMMAR, is a set each of whose elements corresponds to a production* The production a -+ al a2 ...
0 a r? is transformed into the [ntl) -tuple <ao, alra2, ...,a, > The root symbol sf the grammar is passed to the parser in parameter ROOT.
Algorithm A, Type 0 T-op-down Serial This procedure builds the parse trees for the input sentence sequentially in a two-dimen~iona$ arrap TREE.
The first subscript of TREE specifies the number of the node, the second selects t component of the node as follows: TREE(n,'NAME1) name of node n TREE (n, ' PARENT ' ) number of Barent node of node n (= 0 far root node) TEE ( n, ' DAUGHTERS ' ) tuple of numbers of daughter nodes of node n TREE (n, ' CURRENT OPTIOY ) tuple of current production used to expand this node TREE (n, 'ALTEEJATIVE OPTIONS~') set of tuples representing productions not yet! tried for this node TRlEE (n, 'FW1) TREE (n, 'LW+~') number of first sentence word subsumed by node n (number of last s~ltence word subsumed by node n) + 1 As each analysis of the sentence is completed, it is added to the skt PARSES.
When parsing is finished, this ~t of trees is returned as the value of the function PARSE.
The variable NODES holds a count of the number of npdes in the parse tree; this is also the number af the node most rqcently added to the tree.
WORD holds the number of the next word in fie sentence to be matched.
The heart of the parser is the recursive proced~re EXPAND.
EXPLtD is passed one argument, the number of a node in the parse tree.
IY EXPAND has not been called for this node before, it will try to expand the node, ffbe., build a parse tree below the node which matches part of the remainder of the sentence.
If EXPAND has already been called once for this node -so that a tree already exists below this node -EXPAND tries to find an alternate tree below the node which will match up with part of the remainder of the sentence.
If EXPAND is successful -an (alternate) tree below the node was found -it returns the value true; if it is unsueeessful, it returns false.
In the case where the node corresponds do a terminal sy~nbol, EXPAND will return true on the first call only if the synbol matches the next word in the sentence; it will always return false on the second call.
definef PARSE (6'11~filhlAR~ ROOT, SENTENCE) ; local PARSES, TREE, NODES, WORD ; TREE = nR; PARSES = nz; WORD = 1; 'LIJODES = 1; /* set up root node (node 1) */ TREE(^, 'NAME') = ROOT; TREE(1, 'FW') = WORD; /* loop until a11 parse trees have been formed .*/ (while EXPAND (1 ) ),/* if tree spans entire sentence, add to set */ if WORD eq ( ( Km?TENCE) +1J then PARSES = PARSES U {'I'REE); end if WORD; end while EXPAND; re turn PARSES ; end PARSE; definef EXPAND(X) ; local I, OPT; if GRAMMARITREE(X~~NAME~H eq nR then /* terminal symbol */ if TREE (X, 'ALTERNATE OPTIONS ' ) eq 0 then /* first call -test for match with sentence */ TREE (X, 'ALTERNATE OPTIONS ' ) = nR; if WORD le #SENTENCE then if SENTENCE(W0RD) eq TREE(x, 'NAME') then WORD = WORD + 1; TREE(X, 'LW+lf) = WORD; return true ; end if SENTENCE; end if WORD; else /* second call */ WORD = WORD-1; end if TREE; return false; end if GRAMMAR; /* nonterrninal symbol */ if TREE (x, 'ALTEFWATE OPTIONS ' ) eq Sl then /* first call, retrieve options from grammar */ TREE (XI 'ALTERNATE OPTIONS ' ) = GRAMMARTREE (x, 'NAME ' ) ; TIREB(X, 'DAUGHTERS') = nult; OPT = 52; else /* second or subsequent call */ OPT = TREE(XI 'CURRENT OPTION'); I = #OPT; end if TREE; /* select next option to try */ GE3'OPT: if OPT eq 0 th\en OPT from TEIEE(X, 'ALTERNATE OPTIONS1) ; TREE (x, 'CURRENT OPTION' ) -OPT; I = 1; end if OPT; /* ekpand node */ kvhile I ge 1) /* work on lth element of current option */ /* if corresponding node not in parse tree, add it */ i\f TREE (XI 'DAUGIITERS ' ) (I) eq Q then NODES A NODES + 1: TREE (NODES, 'NAME ) =, OPT (I) ; TREE= (NODES, ' FW ' ) = WORD ; TREE (NODES, 'PARENT ' ) = X; TREE (XI ' DAUGHTERS ' ) (I) = NODES ; end if TFCEE; /* kry for an(other) expansion of this node */ b if EXPAND (TREE (X, ' DAUGIlTERS ' ) (I) ) then /* expansion found..
. if this is last element, return successfully, else advance to next element */ if I eq #OPT then TREE (X, ' LW-1' ) = \J@RD; return true; else I=I+l; end if I; else /* no expansion found . . . erase this node and examine previous erernent */ TREE (NODES) = n; NODES = NODES-1; TREE (x~ 'DAUGHTERS ' ) (I) = R; I = 1-1; end if EXPAND; end while I; /* all expansions for this option have been generated; if more options, loop, else return falsa, */ OPT = Q; if TREE (X, 'ALTERNATE OPTIONS ' ) ne nR then go to GETOPT; ; re turn false ; end EXPAND; One way of viewing this procedure is to consider each node as a separate process.
Each process creates and invokes the processes corresponding to its daughter nodes.
In SETL, the algorithm cannot be represented directly in this way, since there are no meckgmisms for creating and suspending processes.
Instead, the data which would correspond to the local vaxiab1.e~ of the process are stored as components of each node in the parse tree.
In languages which provide for the suspension of processes, such as SIMULA, the algorithm can be represented even more succinctly (see, for example, a version of this algorithm in "Hierarchical Program Structures" by 0.-J, Dahl and C.
A. R.
Hoare, in Structured Progrminq by 0.-J.
Dahl et al., page 201).
Algorithm BType 2 Bottom-up Parallel This algorithm is sometimes called the "Immediate Co~stituent Analysis" (IcA) alqorithm, because it was used quite early in parsing natural langqage with ICA grammars, 1%constructs all nodes in a single left-to-right pass over the sentence.
As each word is scanned, the parser builds all nodes which subsume a portion of the sentence ending at that word.
The nodes ( "spans" ) are accumualted in a two-dimensional array SPAN, whose first subscript specifies the number of the span and whose second subscript selects a component of the span, as follows: SPAN (n, 'NAME ' ) = name of span n SPAN (n, 'FW' ) = number of first sentence word subsumed by span n SPAN (n, 'LW+ll) = (number of last sentence word subsumed by span n) + 1 SPAN (n, 'D'AUGHTERS') = tuple of numbers of daughter spans of span n.
At the end of the routine is some code to convert SPAN, a graph structure with each span potentially a part of many parses, into a set of parse trees.
This code has two parts: a loop to find all root odes created in the immediate constituent analysis, and a recursive routine EXPAND which makes copies of all descendants of the root node and puts them in TREE.
Each node in the tree has the following comr,onents: TREE(n, 'NAME') = name of node n TREE(n, 'FW') = number of first sentence word suksumed by node n TREE(n, 'LW+ll) = (nunher of last sentence word subsumed by node n) + 1 TREE (n, 'DAUGHTERS') = tuple of numbers of daughter nodes of node n TREE (11, 'PARENT')= number of parent node of node n.
The set of parse trees is accun\ulated i.n PARSES and finally returned as the value of the function PARSE.
definef PARSE(GRAMMAR,ROOT,SENTENCE) ; local TODO, WORR, CURRENT, DEF, DEFNAME, DEFELIST, REM, SPAN,, SPANS!
TREE, NODES, PARSES, MS, I; /* initialization */ SPAN = n1; SPANS = 0; TODO = nl; /* iterate over WORD=last word subsumed by spans being constructed */ (1 <= VWORD <= #SENTENCE) /* add span whose name is sentence word */ ADDSPAN (SENTENCE (WORD), WORD, WORD+l, nult) ; /* TODO contains the numbers of spans which were just created and for which ye have not yet checked whether they can be used as the last daughter span in building some more spans */ (while TODO ne nl) /* select a span from TODO */ CURRENT from TODO; /* loop over all productions whose last element = name of current span */ (VDEF E GRAMMAR(DEF(#DEF) eq SPAN(CURRENT, 'NAME')) /* separate left and right sides of production */ DEETIAME = hd DEF; DEFELIST = tl DEF; /* if elements preceding last element of production can be matched by spans, add a new span whose name = left-hand side of prodncision for each match*/ (~REM E MATCH (DEFELIST (1: (#DEFELIST) -1), a SPAN (CURmNT, 'FW') ) ) ADDSPAN (DEFNAME, hd REM, SPAN (CUIXWNT, 'LW+l'), (tl REM) -t <CURRENT>) ; end VREM; end VDEF; end while TODO; end 1 \= VWORD; /* eXtract trees from set of spans */ PARSES = nl; (1 <= \tI <= SPANS I (SP.W(I;'NAMG') aq ROOT) and (SPAN (1, ' FW' ) eq 1) and (SPAN(I,'LW+lt) eq SE SENTENCE)+^))) NODES = 1; TREE = nl; TREEC~) = SPANIII; TREE: (1, 'DAUGHTERS ' ) = EXPAND (TREE (I, ' DAUGEITERS ' ), 1) ; end 3, <= VI; return PARSES ; cnd PARSE; define rVLATCII (ELTST, ENDWDPI) ; local I, NTUP; * MATCH finds all n-tuples of spans whose names match the elen~ents of the n-tuple ELIST md .cqhich span a portion of the sentence whose last word +l = ENDFVDPI; returns a set, each element of which is an (ntl) -tuple, whose tail is one of the n-tupleof spans and whose head is the rlunher of the first word spanned by the n-tuple of spans */ if ELIST cq nu9t then return ((ENDwDP~~~); else return [U: 1 <= 1 <= NODESJ (if (SPAN (I, 'NAME ' ) eq ELXST (#ELIST) ) and (SPAN (I, 'LFJ+ll ) eq ENDWDP1) then CNTUP + <I>, NTUP E bylTCH (ELIST (1: ($ELIST),SPAN (1, 'FlY' ) ) 1 else nl) ; ; end MATCH ; &fix%@ ADDSPAN (NAME, FW r LWP1, DAUGHTERS ) I* ADDSPAN builds a span whose components are passed in the four parameters */ mms = SPANS+l; %Pm[SP.SrgNAME') = NAME:; S~hWiiI(SPANL:,'E'W') = FW; SP~(SPANS,'LW+~') = LWP1; S3Pm [SPANS, ' DAUGHTERS ' ) = DAUGHTERS ; SmS h TOM); *~bef EZ~PAND (DAW, PAR) ; /* creates a node for each span in DAW and each descendant thereof, and returns a tuple with the numbers of the nodes (in TREE) corresponding to the spans in DAW */ Dr Ni eq Q then return 52; ; n = zBtplt; 1 S = NODES +; 1; S = BODES; ei~3 = SPANCS); E(Nr*PARENT1) = PAR; (N,'DAqGHTERS') = EXPAND(TREE(N, 'DAUGHTERS'),N) ; p = D + <N>; end YS; Algorithm C Type 1 Bottom-up Parallel Algorithm C is the basic "nodal spansr' parsing algorithm [Cocke 19701.
The sequencing logic is identical to that for Algorithm 3.
The ~nly difference in the tree representation ia that all spans in Algorithm B with conunon values in the NIIME, FW, and LW+1 components are joined into a single span in Algorithm C.
The DAUGHTERS component now becomes a set, each of hose elements correspands to the value of the DAUGHTERS compbnent of one of the spans in Algorithm B (this set is called the "division list" .in the nodal spans algorithm): sp~~(n, 'DAUGHTERS ') = a set each of whose elements is a tuple of numbers of daughter nodes of span n In order to effect this change in the trce, it is necessary only to modify the procedure ADDSPN~ to check whether a span with the specified value of NAME, FW, and LW+1 aIlr?i?aay exists: define ADDSPAiJ (NFr).IE, FW, LWP1, DAUGHTERS) ; local S; if 1 <= 3s <= SPANS' I (SPA;J(S,'NtUIET) eq NUE) and (SPNI (S, ' FW' ) eq FIJ) and SP, L) ec, LWP1) then DAUGHTERS in SPALJ (S, 'DAUGHTERS') ; else SP=\JS = SPRNSSl; SPAN(SPm~,'NM~') = NAME;,SPAN (SPANS, ' FW' ) = FW; SPAN(SBANS,~LW+S.~) = LWP~; SPAN (SPANS, DAUGHTERS 1 = {DAUGHTERS 1 ; SPANS in TODO; end if; re turn ; end ADDNODE; The procedure for converting the spans into a set of trees is now more complicated than for ~lgorithm B; see, for example, Wens 1:1975], Sec.
7. Algorithm D.
Type 2 Top-down Serial We now seek to combine the advantages of algorithm A with those of algorithms B and C.
Algorithms B and C would construct any given tree over a portion of the sentence only once, whereas algorithm A might construct some trees many times during the course of a parse.
On the ~ther hand, B and C would construct many trees which A would never try to build.
More precisl~ly, B and C would build trees while processing word n+l which could not enter into any parse for any sentence whose first n words were those processed so far.
To combine these algorithms, we shall return to the basic framewdrk provided by algorithm A.
To this we add a mechanism for recording "well formed substrings".
The first time the parser tries to analyze a portion of the sentence beginning at word f as an instance of symbol N, this mechanism records any and all trees constructed below node N.
The next time the parser tries symbol N at word f, the saving mechanism retrieves this information so mat the trees below N need not actually be rebuilt.
The previously-completed trees are stored in the two-dimensional array WFS, whose structure is identical to that of SFAN in algorithmBc WFS (n, 'NAME' ) = name of well-formed substririg n WFS (n, 'FW') = first word of well-formed substring n WFS (n,'LW+lr ) = (last word of well-formed substring n) + 1 WFS (n, 'DAUGHTERS ' ) = tuple of numbers of daughter substrings of substring n WFSS sholds the numbeiof substrings in WFS.
When the parsing operation is complete, WFS will contain a subset of the elements which were in TREE at the end of algorithm B.
The tree used by the top-down parser must be augmented to allow for the possibility that khe parser is not building a trce below a given node but rather consulting the table of well-formed substrings for that node.
In that case the node will have, instcad of a tuple of daughters and a set of alternative options, the nun.lber of the well-formed substring currently being used in the taree and the set of alternative weil-formed substrings.
The structure of a node is thus: TRl?Lr;-(n,'NrW1) = name of node n TREE (n, 'PARENT') = number of psrcnt node of node n (= 0 for root node) TREE (n, 'UAUGHTERS ') = tuple of nunhers of daughter nodes of node n (= nult if node is r,latched by well-formed substring) TREE (n, 'WFS1) =*number of well-formed substring matched to node n (= Q if not matched to a substring) TREE (n, 'CURRENT 0PTIC3iJ1 ) = tuple of current production used to expand node n (= SZ if node is matched by a well-formed substring) TREE (n, 'ALTERNATE 0PTI:ONS ' ) = set of tdplcs rcprcsenting productions not yet tried for node n TREE (n, 'ALTERNATE WFS ' ) = set of numbers of well-form6d substrings not yet tried for node n 'TREE (n,'FW1) = number of first word subsum~d by node n TREE (n, 'LW+lt) = (number of last word subsumed by node n) + 1 Finally, we require a table which indicates, for each symbol N and sentence word f, whether a11 the well-formed substrings for N starting at f have been recorded in WFS.
For this the parser uses the two-dimensional array EXPANDED: EXPANDED(N,f) = true if all substrings have been recorded, Gi if not.
The text of procedure D is given below; comments arE included only for those statements added to procedure A.
def inef PARSE (GRAMMAR, ROOT, SENTENCE) ; local PARSES, TREE, NDDEG, WORD, WFS, WFSS,.
EXPANDED $ TREE = nR; PARSES = nR ; WFS = rlk; WFSS = 0; EXPANDED = nR; WORD = 1; NODES = I; TR'TE (1, 'NW ' ) = ROOT ; TREE(1,'EW1) = WORD; (while EXPAND (1) ) if WORD eq (#SENT+ENCE + 1) THEN PARSES = PARSES U {TREE 1 ; end if WORD; end while; return <PARSES,WFS> ; end PARSE; definef EXPAND(X) ; local I, S, LAST, OPT; if EXPANDED (TREE (X, 'NAME ' ), TREE (X, ' FW' ) ) eq true then /* the expansions for this symbol have been computed before */ /* if this is a new node, get its WFS entries */ if TREE (x, 'ALTERNATE WFS ' ) eq Q then TREE(X,'ALTERNATE WFS') = IS, 1 < S < WFSS I (WFS (S, 'NAME ) eq TREE a, 'NAME ' ) ) and (WFS(S,'FWi) eq TREE(X, 'FWr))) end if TREE; if TREE (x, 'ALTERNATE WFS ' ) eq na then /* all WFSs tried for this node */ worn = TREE(X,'FW') ; rsturn false; else GETOPT: if OPT eq R then OPT from TREE (-XI 'ALTERNATE OPTIONS ' ) ; TREE: (XI 'CURRELJT OPTION I)= OPT; I = 1; end if OPT; (while I ge 1) if TREE(x,'DAUGHTERS') (I) eq Q then NODES = NODES + 1; TREE (NODES, 'NAME') = OPT(1) ; TREE (NODES, ' FW ' ) = WORD ; TREE (NODES, ' PARENT ' ) = X; TREE(x,'DAUGI~TERS') (I) = NODES; end if TREE; if EXPAND (TREE (X, DAUGHTERS ) (I) ) then if I eq #OPT then TREE (X, 'LW+ll ) -= WORD; /* record substring matched by node X */ ADDWFS (TREE {X ].
) ; TREE(X,'WFSI) = WFSS; return true ; se I = I+1 end if I; else TRSE (JODES) = a; NODES = NODES-1; TREE (X, ' DAUGHTERS ' ) (I) = 52; I = 1-1; end if EXPAND; end while I; OPT = Q; if TREE (X, 'ALTERHATE OPTIONS ' ) ne nR then go to GETOPT; ; /* all expansions tried */ EXPANDED (TREE (X, 'NAME ' 1,WORD) = true ; return false; end EXPAND; define ADDTUFS (NODEX) /* add an entry to WFS */ local I; WFSS = WFSS+l; WFS (WFSS, 'NAME') = NODEX('NAME') ; WFS(WFSS,~FW~) = NODEX(~FN~); WFS (IVPSS, LW+~ ) = NODEX ( LW+L ) if NODEX('DAUGHTERS1) ne Q then WFS(NFSS,'DAUGHTERS') = [+: I E NODEX('DAUGHTERS1) ] <TREE (I, T~~~f 1 >; end if NODEX; return ; end AQDWFS; Note that this parser returns an ordered pair consisting of the set of trees and the set of well-formed sSstrings, since the trees alone do not contain complete information about the sentence analysis.
Algorithm E Type 1 Top-Down Serial To complete our set of algorithms, we shall apply to Algorithm D the same change we made to convert Algorithm B to Algorithm C.
That is, where in Algorithm D we may have had SSevc~.~.,~e11 formed substrings with the same values of XAP?, l?W, LIJ4-1, we shall combine these into a single substring ir, Algorithm E: The component DAUGHTERS 'becomes a set, each o: whose elements is a tuple corresponding to the value of DAUGHTERS of one of ti,e substrings in Aigorithm D.
Just as we only had to change AD7idODE in Algorithm B, we only have to change ADD~FS in Algorithm D.
define ADCWF$ (NODEX) ; local W, I, DAUGHTERS; /* conpdte DAUGHTERS for substring */ DAUGHTERS = Q; if NODEX ( ' DAUGHTERS ' ) ne SZ then DAUGHTERS = : I E NODEX ( ' DAUGHTERS ' ) ] <TREE (I, ' WFS ' ) > ; end $f NODEX; /* &arch for well formed substring with identical ;.IMil3, m, LW+l */ if 1 <= 3 W <= WFSS 1 ( (WFS (W,INAME ' ) eq NODEX ( 'NAME ' ) ) and (WFS (W, 'FW') eq NQDEX('FW') 1 and (WFS(W, 'LiV+1') eq NODEX('LW+l') ) /* found one, add daughter to set */ then if DAUGHTERS ne 52 then DAUGHTERS in WFS (W, ' DAUG:ITERS ' ) ; end if DAUGHTERS ; else WFSS = WFSS+I : WFS(WFSStlNAM, 1 = NOIT'~.~(.('NAME'); WFS(WFSS,'r;bW1) = NOUEY(~FWI); WFS(WFSP~~LW~-~~) .= NODEX('~;W+~'); 0 WFS[WFSS,'U~,'JG~L hs = Lf DAUGrlTF., eq R +hen nR else DAUGHTERS ; end if 3~; 2e turn ; end ADDWFS; Use of the Varioub Algo~ithms in Natural Language Systems The type 0 top-down algorithm (algorithm A) is one of the simplest and lnost frequently used.
For example, a special version of this algorithm (for Greibach normal form grammars) was used in the original Harvard predictive Analyzer [ICuno 19621.
The later version of the ~arvard system, incorpornti~~g a "path elimination" technique, was a type 1 top-down serial parser, a variant of Algorithm E; instead of saving all daughters in WFS during the parse, they were recomputed later for those nodes appearing in a parse tree [Xqo 19651.
Several current sys.t;cms use nug111&11t;ed context-f ree grammars : grammars to which have bccn uddcd restrictions on the parse tree typically in the form of LISP predicates, which must be true if the tree is to be accepted as a sentence L~~~alysis.
The lBinocjrcld [197l] svs tern uses an acy~ented contest-f ree grarmar with the context-f ree component encoded as a program rather than, data.
The parsing strategy is essentially that of !a type 0 top-down algorf-thm, except that back-up is explicitly controlled rather than automatic.
Woods ' system [1970b] also uses a type 0 top-down algorithm, although somewhat different from the one presented here since his grammar is a recursive transition network.
The LincJuistic String Project system [Sager 19671 started out with a parser based on a type 0 top-down algorithm; for efficiency it later progressed to a type 2 top-down algorithm.
A type 2 rather than a type 1 algorithm was used because the restrictions can o11e analysis of a portion of the sentence as a particular symbol while accepting another analysis of the same portion of the sentence as the same syxbol.
For a type 2 algorithm, this means simply el-ininating some nodes i'n WFS; for a type 1 algorithm, whe~e a single node may represent several ttrees, a complicated procedure which could create new nodes would have been required in general.
The Linguistic String Project parser is considerably more complex than the type 1 top-down serial parser shown above (algorithm D), in part because of the restrictions which must be evaluated during the parse, in part because (for reasons of skoroge ecorloiny) the system makes it possible to save only selected nodes in WFS.
The type 2 bottom-up parallel algorithm also saw early use in 5% natural language processing.
The parser designed by Cocke for the Rand system was a special version of this algorithm for Chomsky normal form grammars.
A thorough survey of the different ordering strategies possible with this algorithm was given by Hays [1967].
This algorithm was subsequently developed by Cocke (among others) into a type 1 bottom-up paralleL algorithm named "nodal spans" and subsequently into a type .1 top-down parallel algorithm called "improved nodal spans" (see Cocke [1970] for a description of these algorithms).
The latter is very similar to a parsing algorithm described by Earley [19 701 . +These type 1 algorithms have, to &he best of our knowledge, not yet been used in natural language parsing.
In closing a few remarks are in order on the practical importance of the differences between the various algorithms.
How significant is the difference between type 2 and type 1, between top-down and bottom-up, be tween serial and parallel?
There has been no systematic study of these questions, and the answers to them are in all likelihood qyite grammar specific.
For example, the advantage of the tcjp-down parser is that, in working on word n+l, it eliminates from consideration those symbols which could not occur in any parse tree whose first n terminal symbols are the first n words of the sentence.
Is this a large effect?
Although I am not aware of any measurement of this quantity, the factor seems to be relatively small for large-coverage English grammars -perhaps reducing the number of symbols in half.
The advantage of type 1 over type 2 algorithms depends on the degree of ambiguity of the grammar.
How frequently can a portion of i+ sentence be analyzed as a particular symbol in several ways?
For unaugmented context-free grammars the answer in general has been very frequently -this was one of the problems of the context-free systems.
For such grammars, type 1 algorithms would be much more efficient.
When restrictions are added, however, they discriminate some of the analyses from others.
L 53 A rich set of syntactic, semantic, and pragmatic restrictions (available so far only for small subsets of English in limited areas of di~coursa) would presumabl~~ eliminate almost all ambiguity, so that the advantage of a type 1 algorithm would then be small.
Finally, We should mentian the difference butwecn serial and parallel parsers.
Since serial and parallel algorithms wi'll have created the same nupber of nodes by the time parsing is con~plete, the difference in time is probably quite small.
The parallel algorithm pay have the edge because "bookkeeping" is simpl'er.
Also; the parallel algorithm can handle left rccursio:~ naturlaily, whereas a special mechanism is requirca for top-down serial parsers.
On the other hand, a serial algorithm may be preferable if only the first parse of a sentence is required.
In addition, the serial algorithms can more sim~ly handle the situation where memory space is marginal.
Normally most of the space in algoritnms D and E is used by the set WFS, not by TREE.
Consequently a type 1 or 2 serial parser can "rescue itself" when memory is almost exhausted by reverting to a type 0 algorithm; this simply means tnat it stops saving nodes in WFS.
In terms of the SETL programs given above this requires, in addition to a change to ADDWFS, only that elements of EXPANDED no longer be set t6 true qnce -savTng has been tern~inated.
3.2. A Parser for Unrestricted Rewritina Rule Grammars A number of natural language systems, such as REL (at the California Institute of Technology) and the Q-system (at the University of Montreal) have used unrestricted phrase structure grammars.
In such grammars, each rule specifies that some sequence of symbols be rewri'tten as some other sequence of symbols.
The parsing algorithm used in these system was described by Kay in 1967 ("Experiments with a Powerful.
Parser," Martin Kay, ih 26me Conference Internationale sur le Traitmcnt Automatique des Langues, Grenoble) . Kay added qudte a few features to the basic parsing procedure to create his "powerful parser".
These included rule ordering and conditions on rule application.
Other unrestricted rewriting rule sys terns have also included some such features.
to ~ermit the parsimonious description of c?~mplex natural language grammars.
In this newsletter, however, we shall not be concerned with these additional features ; only the basic parsing procedure will be described below.
The parser to be presented represents only a small modification to the context-free parser B (the "immediate constituent analyzer") given earlier . To understand this modification, consider the following example.
We are given a context-Free grammar which includes the productions a -+ def and x+ya ad the sentence ydef We shall create a diagram for the sentence by making eath word into an arc connecting twa nodes, which are labeled with the number of the word in the sentence and the number +1: Context-free parser B would fifst apply the product a -+ do£ in reverse, to obtain a span 'a", we can indicate this thus: Note that the arc for thc spa connects nodas corresponding to the first word and the last word + 1 subsurnod by I span.
The parscr would then apply x -c y a in rcvcrse: getting a span which subsumes the entire sentence.
Now consider analyzing the same sentence with the unrestricted phrase structure! grammar z+ya x+zb We begin by ilsing the first production tc reduce the se~~tcnce to y a b.
This raises the problem of how to label tthc no~ic between arcs a and b.
~lthough a and b together subs~~a the last three words of the sentence, no fraction ofthis C~II be assigned individually to a or to b; hence we cannot label this new node with the number of a sentence word, hlstead, we assign a nzw, unique label (here, v1) to the node: PiCri can thea reverse the second production to get a span z Fhallly we reverse the third production to get q span x subsuming the entire sentence: In the progzam below, new node names are created by cans oal the SWL function newat, which returns a different Ucpe symbol (a "blank atom" in SETL terminology) each time it is called.
We have retained the span component names EW and LW+l for the labels of the nodes at the ends of the arc, moxagh their values may now be blank atoms instead of numbers.
A production of the form al ...
a + bl ...
b n n is represented by th8 structure <<al, ...'a >, bl,...,b > n n GRAMPlZSR is a set of such structures.
For example, the mnlclestsicted rewriting rule grammar given above would be encOded4ar;r C < < a, b>, d, e, fz, < < zQ>, y, a>, < < x>, Z, b > 3 fact that spans numbered s.
s, were formed by lying an inverse production to spans numbered dl . . . d n Zs mxxm3ed by assigning the component CONSTITUENTS with value define URRP&SSE (GRAMMAR, SENTENCE) local SPAN, SPANS, WORD, TODO, CUFtRENT, DEF, DEFNAME, DEFELIST, MS ; /* initialization */ SPAN = nl; SPANS = 0; TODO = nult; I* iterate over WORD=last word subsumed by spans being cons tructed */ (1 <= VWORD <= #SENTENCE) /* add span whose name is sentence word */ RDDSPAN (SENTENCE (WORD), WORD, F?ORD+l, nu1 t) ; * TODO co~ltains the nunhers of spans which were created and for which we have not yet checked just whether they can be used as the last daughter span in building some more spans */ (while TODO ne nult) /* select a span from TODO */ CURRENT = hd TODO; TODO = tl TODO; /* loop over all productions whose last element = name of current span */ (VDEF E GRAMMAR~DEF(#DEF) eq SPAN (CURRENT, ') ) /* separate left and right sides of production */ DEFNkW = hd DEF; DEFELIST = tl DEF; /* if elements preceding last element of production can be matched by spans, add new spans whose names = left-hand side of production for each match*/ (VFSM E MATCH (DEFELIST (1: (#DEFELIST) -1), SFX~ (CURRENT, 'FW' ) ) ) ADDSPANS (DEFNAMEet hd REM, SPAN (CURRENT, 'LW+l ), (tl REM) + <CURRENT>) ; end VREM; end VDEF; end while TODO; end 1 <= VWORD; return SPAN; end URRPARSE; define MATCH (ELIST, ENDWDP~) ; local I, NTUP; /* MATCHfinds all n-tuples sf spans whose names match the elements of the n-tuple ELIST and which span a portion of the se~ltence whose last word + 1 = ENDWDP1; returns a set, each element of which is an (n+l) -tuple, whose tail is one of the n-tuples of spans and whose head is the number of the first word spanned by the n-tuple of spans */ if ELIST eq nult then return ( <ENDWDPl> ; else return [U: 1 <= I .f= NODES] (if (SPAN (I, 'NAME ' ) eq ELIST (#ELIST) ) and (SPAN ( I, 1 ' ) eq ENDWDP~) then {NTUP + <I>, NTUP E MATCH (ELIST (1: (#ELIST) -1),SPAN (I, 'FW' ) ) I else nl) ; ; end MATCH; define ADDSPANS (LHS, FW, LWP1, CONSTIT~JENTS ) ; /* builds a sequence of spans whose names are given by tuple LHS, with the first span beginning at FW and the last one ending at LWPl */ local W, I, TUP; W = FW; TUP ="nuLt; (1 <= VI <= #LHS) SPANS = SPANS + I; TUP = TUP -+ <SPANS>; SPAN(SPANS,'NAME1) = LHS(1)-; SPAN (SPANS, TW') = W; W = if I eq #LHS then LXPl else newat; SPAN (SPANS, 'LW+l ' ) = W; end 1 <= VI; SPAN (SPANS, 'CONSTITUENTS ' ) = <TUP, CONSTITUENTS>; TODO = TUP -t TODO; return; end ADDSPANS; The unrestricted rewriting rule parser has the power of a Turing machine.
The user is afforded great flexibility in the manipulation of sentence strings.
One drawback ~f such power, however, is the absence of a decision procedure -no parser can determine, for an arbitrary grammar of this type, that a given sentence string is ungrqmrnatical.
The user must tllcrefore be careful to design grammars so that the parser will terminate (in a reasonable amount of time) for any input sentence.
3.3. Parsinq Procedures for Transformational Grammars Most linguistic research over the past fifteen years has been conducted within the framework of transformational grammar developed by Chom'sky and ~arris.
In the early 19601sr a few years after the blossoming of transforma~ional grammar, several efforts were begun to develop parsers which could operate fairly directly from a transformational grammar.
Two of these achieved some measure of success: a project at MITRE led by Donald Walker [Zwicky 1965, Walker 19661 and work at MIT by Stanley Petrick [19651.
These twa efforts had quite different objectives.
The MITRE group was concerned with a specific practical application: development of a natural language interface for a military information retrieval sys tex.
They developed a grammar for a subset of English meeting their requirements and were primarily concerned with designing a parser which could handle this particular grammar.
Petrick, in contrast, developed a general parsing procedure which would work with any member of a class of transformational grammars.
This difference in objective affeeked a number of design decisions regarding the parsing procedure, as we shall see later on.
Petrick and his coworkers, at the Air Force Cambridge Research Laboratory and now at XBM, Yorktown Heights, have modified the parser to reflect changes in transformational grammar and to adapt it for use as the front-end in an information retrieval system.
IPetrick 1966,1973,1975; Keyser 1967; Plath 1974a, 1974bl.
Interestingly enough, these modifications have brought Petrick's parser much closer to the original MITRE design.
Since the structure of trans fom'iational grammar has varied in time and between different schools of linguistic theory, the notion of a transformational parser is not we11 defined.
In order to present a parsing algorithm, we have selected a particularly simple granular formulation.
This Eorinulation corresponds a~proxirnate~ly to the early work of Chomsky (e.g., Syntactic Structztres) and the theory used in the early versions of the MITRE and Petrick systems, Complicating factors, such as features and content-sensitive rules for lexical insertion, have been on~itted.
The grammar consists of a bass curnps~t8nt and a ~~~Lz~~s~oI*~*:LI$.iunnZ conty~?t~?zd.
The base componant is a context-free granunar which produces a set of dcsp s trnc-t;u~.a trees.
The t:-ansfor~natiorla1 coi~~ponent is a set of tree-rewriting rulcs which, when applied to a deep structure tree, produces one or more su~fucc s t~ucture trees.
The frontiers (tern~inal node sequences) of Lhe surface structure trees are the sentences of the language.
The root symbol of the base component is named S.
The base component also contains a distinguished symbol COMP which appears on the left side of only one production: COrviP -+ # s # # is referred to as the sentence boundary marker, With the exclusion of this production, the CralImIar is not recursive.
Each transformation consists primarily of a s +~ztctu~a 2 index and a strztctuz~at change.
The strllctural irldex is a tuple (vector), <si . . . s > I each of whose components 1' n is either a symbol (name of a node) or "XuThe structural change is a tuple <sclr.
. .,sc of the same length as the n structural index.
Each of its components is ir turn a tuple SC =<SC i il,.
.,sc >, possibly empty (ni = 0).
Each of the scij in, is either a terminal symbol or an integer between 1 and n.
The application of fransfomational rules is based dh the notion of a proper anatysis, which is in turn based on the concept of a cut of a tree.
Roughly speaking, a cut is defined by drawing a line from left to right through a tree, passing only through nodes (hot through the lines connecting nodes) ; the nodes thus passed through form the cut.
For example, for the tree --<> I1 PRO I like turnips the sequence of nodes NP, VERB, M form a cut.
More formally (Aho and Ullman, The Theory of Parsing, Trans Zation, and Compiling, Vol.
I, pg.140), a cut is a subset C of the nodes D of the tree such that I* no node in C is on a successor path from some other node in C 2.
no other node of D can be added to C without violating rule 1 If the na&nes of the nodes in the cut, arranged in sequence from left to right, match the structural index of the transformation, the cut is a proper analysis of the tree with Y respect to this transformatiofl.
A structural index matches the sequence of node names if there exists a substitution of sequences of symbols (possibly null and not incldding #) for the occurrences of "X" in the structural index which will make the structural index identical to the sequence of names.
For example, the cut NP VERB N would be matched by any of the structural indices NP VERB N The proper analysis associates with each element of the structural index (except possib~y "XX"s) a nods in thc tree and hence a subtree, the trce dominated by that node.
The? structural change indicates how these subtrees are to be shuffled to effect the trans fornation.
sc specifies what i is to go into the position occupied by the node matching sc i If sci is a 1-tuple, we simply have a case of one 11ode (and the sabtrce it dominates) replacing another; if sc is an i n tuple, n > 1, we first substitute sc for the or~yinal node and then insert sci2 1 . . ., sc . il lni as right siblings of that node: If scij is an integer,between 1 and n, the new node is the node matched to the scij-th element of the structural index; if SCij is a terminal symbol, the new node is a terminal node with that name.
Because the value of sci may be the null tuple < >, it is possible for a node in the tree to be left with no successors.
We therefore "clean up" the tree after applying the transformation by deleting any nonteminal node not dominating at least one terminal node.
The prescription just given is inadequate for components ~f the structural index equal to "X", since these may match zero or moxe than one node.
ide shall cons train the transfornations so that nodes in the cut which are matched by "~"'s do not take part in the transformation.
In terms of the structural change, if sik = "x" then sc = <k> and no other scij k = k.
As an example (3 siqplfication of the example in Keyser and Petrick, Syntactic AnaZysis, p.
9), consider the passive transformation.
Its structural index is <PIPt AUX, Vt XI NP, XI BY, PASS> and its structural change Applied to the tree ART the crocodi it produces the proper analysis indicated by the dotted line Applying the transformation yields the tree ART PREDP AUX the girl BE EN frighten BY the crocodile In additian to the structural index and structural change, some trans formations may have an identity condition, requiring that the subtrees matched by two elen~ents of the structural index be identical for the transEorma.t=ion to ~PP 1~ The rule COMP -+ # S #, which maes the base conponcnt recursive, also plays a special role in the transformations.
If the structure appears in the parse trce, we call the tree dominated by that S a c?3)istitltdnt (or embedded) sentence, and the tsce &~ninated by the next S above COMP the rnadl?,ix sentence parse tree.
The transformations are of h~o types, s and binarii (or embedding).
In a singulary transformation, the structurar index does not contain the symbol #.
In a bhary transformation, the structural index is of the form a #.
B B y, whete a, 8, and y are strings of symbols not containing #.
The binary transformation deEetes these boundary markers (if # are the ith and jth con~yonents of the structural index, then none of the SC~.~ = i or j, thus combining a constituent sentence with its matrix sentence..
The transformations are also cl ssed as optio~wl or ob lt gatory.
Just like the generation of sentences with a context-free grap.uilar, the application of t.ransfornations to a base structure may be viewed.
as a ndndeterninistic process.
Depending on the choices made, one of several possible surface structures may be obtained from a single deep structu-e.
The transformations are considered in a fixed order to be described momentarily.
If there exists no proper analysis for a transformation, the transfqrhation is skipped.
If there exist several proper analyses, one is chosen.
If the transformation is obligatory, it is then applied; if it 1s optional, a choice is made to apply it or not.
The singulary and binary trans formations are separately ordered.
The tr ~sformationol process begins by selecting an embedded sentence tree not including any other embedded sentence.
The singulary transformations are applied in sequence to this tree; structural indices are matched against the embedded tree, not the entire parse tree.
The binary transfokmations are then applied to this tree and its matrix sentence tree; one of these should actually transform the tree, deleting the embedded sentences (if none applied, we would eventually be left with a surface structure containing # ' s, which would be rejected) . Another deepest ernbedded sentence is selected and the process repeats until no embedded sentences remain.
The singulary trans formations are then applied to the entire tree, completing the generating process (if the base structure contained no embedded sentences, this would be the only step) . In order to parse a sentence -obtain its deep structures -we would like to reverse the process just describedv.
First build one or more ;.>otential surface structure parse trees for a sentence and $hen, by applying the transformations in reverse, try to obtain a valid deep structure from each of these.
We shall deal with these two steps in turn.
The surface structure parse tree will, in general, contain many structures which could not be directly generated by the base component.
If we want to produce all the surface structure trees for a sentence using a context-free grammar, it will be necessary to augment the base component.
For example, if the base component contains the production A+XY and there is a transformation which interchanges x and Y, the rule must be included in the grammar which is used to produce the surface structure trees.
Petrick has described (in his P21.
D. thesis) a procedure which can determine fro111 the base and transfor~national components, how the base must be augmented in order to obtain all surface struc.turt3 trees.
Because a trnnsformntion can replace one node with two, it is possible for rcgcated application of such a transformation to produce a node in the surface.
structure with an arbiarbitrary n~unbes of inmediate descendants.
Tllis cans th~t.
an infinite n~ur~el" of rules must be added to 1 base component.
Petrick noted, however, that; if a limit is placed on the length of sentences to be analyzed (and certain minimal assumptions are made about the grammar), only a finite number of rules are required.
(Alternatively, it seems, a recursive transition network could be used tu obtain the surface structure, since such a device allows a node to have an arbitrary number of immediate descendants . ) This augmented grcnlmar will produce all the valid surface structure parse trees but it will a! so produce, in general, many spurious trees (trees not derivable fl-om deep structures) This is unavoidable, since a context-free yranuuar is a ri~uch weaker computational device than a tra~:sfosmati on5.1 grannaar.
Because the language defined by the augmented base component is larger than that defined by the tranxEormationa1 graminar, the augmented base component is called a coda19ing gramrzar.
Since each spurious surface analysis will have to undergo a len'gthy reverse tra~sformational process before it !S recognized as invalid, it is important to minimize the number of such parses.
The seriousness of this problem is indicatedby some early results obtained by the MITRZ group.
The MITRE system did not have a procedure for automatically augmenting the base component; theirs was assembled manually.
Using a small grmar., one of their 12-word test sentences obtained 48 surface analyses, almost all of thorn spurious.
Petrick had similar experience: he found that the covering grammars produced by his procedure-were too broad, producing too many surface parses.
He has instead, like the MITRE group, produced his surface grammars manually, by analyzing oonstructions which appear in the surf ace structure of input sentences to determine which productions are required.
In tYlis way, he has been able to produce a practically useful covering grammar for a limited area of discourse.
These practical difficulties do n~t negate the value of an automatic procedure, such as that described by Petrick, which will produce a covering grammar we can be surc is complete (will produce all valid surface analyses) . They do indicate, however, the value of developing procedures which produce "tighter" surface grammars, perhaps by observing that certain sequences of transEormations are impossible and hence suppressing the corresponding surface grammar productions.
They also suggest that a more powerful device than acontextfree grammar -such as an "augmented* context-free grammar" should perhaps be used to generate the surface analyses.
This view is held by a number of workers, such as Sager and Woods, who are also aiming at a transformational decomposition.
Armed with a covering grammar, we turn now to the construction of a reverse transforrnafional component.
This component should produce, from a surface structure, all base structures which can generate the surface structure (and for a spurious surface structure, indicate that there are no base structures) . * "augmented" meaning here that the grammar may contain predicates which are arbitrary computable fiu7ctions.
The first problem is that it is not always possible to construct such a component.
If a (fonvard) transformation simply deletes a portion of the parse tree, it will in general be impossible to reconstruct that portion when working backwards from the sur'facc structure: there may be an infinite number ~f deep structures which produce one surface structure.
Such a situation is called i.z*z*e~?sur:~l~zb It1 .JG tt.t%'cv~ ('Phis is in contrast to recoverable delcti~ns, which make use of a compo~~ent af forward transformations briefly iiontionod oarlier: identity conditions.
An idcntiCy condition specifics two or more colnpar~cnts of the struotura3.
index; tlle tuL~nsformst.iun may be applied only if the trccs doit~inated by the nodes matched by these eleinents aiue identical.
If soc~e -but not a11 -sf thcse trces are deleted by a transformation, the deletion is recoverable: the reverse transfs~-matisnal.
component may restore the deletion by.
copying another part of the tree.
) So, to be able to construct a reverse component at all, the grammar may contain no irrecoverable de leci ons . Life would be relatively easy if, for each transformation, one could produce an inverse tra~~sfsrmation which undoes the change wrought in the tree.
Unfort:unatcly, for the £om of trar~sforrnation we have chosen, this is nut possible.
Cor~s ider, for exangle; a transformatior1 with structural index and structural change Suppsse that the only structure to which it ever applies in the generation of a sentence is producing bg as transformation seenp straight forward enough.
xse transformation need not be a true inverse ; it mt hawe to be able to reconstruct any input given to transformation.
It need only be able to reponSenputs which occur in the derivation of sentences.
this case, it must insert a B dominating a C !&!# m* This operation cannot be performed in the transformationaL sm dasc?r%bed above (unless a B dominating a C is present in the tree) . In terms of elementary changes to a parse tree, tHis formalism permits only deletion (of 1.
replacement (of 6ne node by another), and sister adjunction (insertion of one node "next" to another, with both natedqby the same notie); it does not allow insertion oS node' below another.
This formalism was used in Petrick's original system.
Most more recent systems, including the system and Petrick's later systems, have allowed a r set of elementary operations, capable of making an ftrary change to a tree.
Even if the set of operations is sufficient to form a set oE rehxse transformations, their formation is not trivial.
a such as the one just considered, the reverse transon cannot be genexated from an examination of the transformatian alone One must examine the entire to see how the transformation is used in sentence on, This is a complex process which has (to the oras knowledge) nevert been programmed.
In the MITRE, the reverse transfofdn'ations were all produced I Petrick, seeking orj yinally a procedure which w~uld work cal2y from the transf~rmational grammar took a actifberent tack.
He developed a reverse transformational & which mapped the surface strirrg (the senkence) hto a set of potential deep structure strings; the latter -me ~~ parsed by the base component.
The individual ions of the reverse component are str&ng and not kme mewr5,ting sules . The advantage of this approach lies in the simplicity of forming the individual reverse transformations.
The reverse transformations will be in one-to-one corresponAence with the forward ones, and each reverse transformation T' can be computed on the basis of the corresponding forward transformation T alone.
These reverse transformations will satisfy the following property: for any tree t with frontier s, if T maps t into t1 with frontier sl, then T' maps s1 into s.
Suppose we are given a fonvard transformatisr~ with structural index si and structural change sc.
since we are interested only in the frontier and not in the internal structure of the tree, we shall use a rcd~xccd structural change rsc = [+: 1 <= i <= #SC] sc(i) obtained by concatenating the elements.
of the structural change.
The fact that a proper analysis exists for a tree with frontier s implies that s can be givided into substrings s1 t -,Sn such that, for all j from 1 to n, a tree can be * built with root si and frontier s j (unless si = 'X1, j j in which case there is np, restridtion on s ).
: 3 The transformation rearranges the string into a set of 1 substrings s given by (for j from 1 to r, r = #rsc) j S' (1) = if rsc( j) is an integer then $(rsc(j)) else rsc(j),s (rsc (r) ) * using the, covering grammar.
How can this shuffle be reversed?
We begin by creating an .inverse structuraZ index isi (1 <= j <= r) according to.
j isi(j) = if rsc(j) is an integer then si(rsc(j)) else rsc(j) and an invrrae structura2 change isc (1 = j C= n) j according to isc(j) = if 3k I rsc(k) eq j then k else si(j) TQen giTlen *a string s ', we divide it into r substrings, requiring that the? j-th substring be the frontier of some tree with root isi (again unless isi = ' X' ) . One of j j these divisions will be the s . produced 'by the forward 3 transformation (there may be others) . These substrings are then rearranged according to thB isc, producing the original string s j s (j) = if isc( j) is an integer then s' (kc( j) ) else iscl j) If there are several matches to the isi, the transformation must be applied to all; we can only be sure that one of the resulting strings will be s.
If the forward transformation 1 is a recoverable deletion involvi3s identity conditions, the formulas given above are somewhat more complicated.
Given a set of reverse transformations, we must finally specify the sequencing among them.
The reverse transformations should be considered in precisely the reverse order fro11 that of the corresponding forward transformations.
The sequericihg is again cyclic, with each iteration now creating an embedded sentence.
Even if a reverse transformation matches the sentence being decomposed, one cannot be sure that the corresponding forward transformation was involved in the generation of the sentence.
Uridoing the transformation may lead to a dead end (no other reverse transformations apply), and arkother transformation may also have produced the current structure.
Consequently, both possibilities -unsdoing and not undoing the transformation -must Lie followed.
In analogy with the fonvard transformations, one can say that all reverse transformations are optional.
This implies, unfortunately, that parsing ti~ne can increase exponentially with the nuinber of applicable transformations.
Such a procedure has therefore proved impracticabLe for a11 but the smallest grammars and sentcn~cs.
To avoid this exponential growth, ths parser must have some way of dctermining directly from a tree the last transformation which applied to produce the trce.
An analysis must be made of the possible intermediate structures which can arise in sentence generation, and the resulting information trans1 ated into conditions on the reverse transformations.
Such an anaLysis has not been automated, but it: is norrr.al2y a straightforward and integral part of the manual construuti~n of a reverse transformational component.
The MITRE group was able to specify the appropriate conditions for ail their reverse trans formations ; their sys tern provided for optional reverse transformations but their grammar did not utilize this facility.
Eliminating optional reverse transformations is more di f ficulC in a reverse zornponen t using string ~cwriting rules, not retaining any tree structure between transforn~akions.
* Most of the information which is needed to determine which transformation to undo is not available.
In any case, the original impetus for using string rewriting rules -providing a procedure which can operate directly from the transformational grammar -is lost when we seek to add, for reasons of efficiency, re~erictdons which are not automatically generated from the grammar..
* Petrick's original system, using s~ring rewriting rules, did retain some low-level tree structures between transforrnations, but his later sys terns did not . Petrick' s current parser, part of the REQUEST sys tern, is mu& closer in overaL1 structure to the MITRE design, A set of potential surface structure trees are operated upon by a reverse transformational component consis*ing of tree rewriting rules.
The reverse transformations are prepared manually, not obtained automatically from corresponding forward transformations.
The conditions on the reveree tra,nsformations are sufficiently tight to obviate the need for optional reverse transformations.
As a result, they are able to operate efficiently with a moderately large set of reverse transl?ormations (about " 130) . Once a11 reverse transformations have been applied, the resulting structures nust be checked to determine which are valid deep structures.
If the reverse transforn~ations work on trees, each tree must be examined for productions not in the base component.
If the reverse transformations work on strings, each string musk be parsed using the base component.
The original Petrick and MITRE procedures envisioned a final synt.hesis phase.
This phase would apply to each deep structure produced by the reverse transformational component.
It would apply the corresponding forward transf ormations to determine whether the original sentence can be recovered; if it cannot, the deep structure is rejected.
Such a .check is necessary if the reverse transformations can produce deep structures which do not lead-back to the original sentence and perhaps do not lead to any sentence at all.
This is certainly the case with the reverse transformations applied to strings; such transformations are unable to capture many constraints present when applying the fomard transformations to trees.
It can also be true with reverse transformation's working on trees, if the constraint9 on the reverse transformations are too loose.
With revera transforinations on trees, however, it should be possible to formulate constraints sufficiently tight as to obviate the need for a synthesiq phase.
A synthesis check is optional in the current Petrick-Plath system.
Instead of applying the forward transformations in a sepsrate synLhesis phase after a deep structure is obtained, however, they are applied during analysis after each corrcspond jng inverse transformation is applied.
THE PROCE,D,UKE We present below a SETL verllion of one of Petrick's early trans farnational parsers.
As was noted earlier, this algorithm is of i~nportance because it is the only procedure which can work dircctly from a forward transformational gramlear.
The SETL program has bcen adapted from the LISP prvyr~m developed by Petrick at the Air Force Cambridge Research 1,aboratories (1966), and is somewhat simpler than the version presented in Petrick's thesis.
In particular, the 1966 version preserves no tree structure between reverse transformations.
Considerable liberty has bcen taken in rewriting the program for presentation here.
Features which were not deemed essential to an understanding of the basic procedure were deleted.
Specifically, the procedure for converting Iolwirr-d to reverse trans formations was not included; identity conditions in transformations were not prsvided; optional denrents in structural indices twrc not allowed.
On the other hand, the gross flow of control of the LISP program has bcen preserved.
The main procedure of the parser, XFPARSE, takes five arguments : SENTENCE the string to be analyzed XFMiiI the set of reverse transformations iq UEVXFMN S the number of transformations BASEGR the (contextfree) base component AUXRULES the additional rules which must be added to the base component to form the covering grammar The context-free grammars have the form described in Sec.
3.1. Each trans formation has the fbllowing components : XFMN (i,'ISIf ) inverse structural index XFNY (i, 'NAME') name of transformation XFMH (i", ' TYPE ' ) type of transformation : 'UNARY' or ' BINARY' if the transformation is unary, it aiso'has the component XFMN (it 'ISC') inverse structural change if the transformation is binary, the inverse structural change will contain a pair of sentence boundary markers, with the component sentence inside the markers and the matrix sentence outside (the general form is m m m # c c c #.m m m, with the m's part of the matrix sentence and the c's part of the component) . For the parser, it is more convenient to represent this as two separate tuples, one with the boundary markers and the elements between them replaced by the symbol 'COMP' ( m m m 'COMP' m m m ) the other with the elements between the markers (c c c) In the transformation, these are components XFm (i, 'ISC-MATRIX' ) inverse structural change for matrix sentence inverse structural change for component: sentence The value of XFPARSE is a set, with each element giving one possible deep Structure frontier for the sentence, and the reverse transformations which were applied to obtain that deep structure.
To determine whether these are in fact deep structures for the sentence, it would be necessary to go through the generative transformational phase for each potential deep structure and verify that the original sentence can be obtained.
If the deep structure contains no embedded sentences, the structure of an element in the returned set is <deep-structure-frontier, trans formations-applied> where deep-s tructure-f rontier is a tuple whose elcmcnts are the symbols in the frontier of we possible deep structure.
Transformations-applied is a li,~llplo whose clcmcnts are the transformations applicd to obtain 'this deep structure ; the first elenlent gives the lust transformation applied in the decomposition, and hence the first which would bc applicd in gcrleration.
If tihe deep structure colitains an cmbcdded sen tcnce, the element will still be a pair as just described; deep-structure-f rontier, however, will not incltida as elements two boundary markers and the intervening cl~beddcd suntence.
Instead at that point in the tuple will bc an elenlent which Is itself a pair, with the first element the frontier cf the embedded sentence and the second the transf~rmations applied to decompose the enbedded sentence.
The transformations-applied element of the top-level pair will include only the embedding transformation and the transfor~nations applied to the sentence before it was divided into matrix and constituent.
Since each reverse transformation is optional and may apply in several ways, there will usually be n~dny paths to follow during the decon~position process.
In XFPARSE, each such path is recorded as an elenlent in the set TODO; the element is a f ronbtier,'history pair, j ust like those produced as output.
The main loop of* the parser runs over the trsnsformations.
For each clement TODO, if the transformation applies all possible transforms of the element are added to TODO; since the transformation is optional, the ori~inal element remains as well.
When an inverse embedding transformation applies, XFPARSE is called recursively to deco~~~pose the embedded sentence.
de fine f XFPARSE (SENTENCE, XFMN, NUMXFMNS, BASEGR, AUXRULES ) ; local TODO, DONE, PARSES, SGRAMMAR, XFMNNO, CONT, SEiOT, XFAPPLD, MATCHSET, MATCH, COMPPARS,, ROMP MATRIX, P ; WNE = { <SENTENCE, nult> 1 ; PARSES = nk; /* compute covering grammar */b SGRAMMAR = BASEGR CJ AUXRULES ; /* iterate over transformations */ ( 1 <= YXF~NO <= NUMXFMNS) TODO 1= DONE ; DONE = nR; /* iterate over active continuations */ (WONT E TODO) SENT = CONT (1) ; XFAPPLD = CONT (2) ; MATCHSET=PROCES (SENT, XFMN (XFMNNO, ISI ),1, SGRAMMAR) ; /* iterate over matches to structural index */ (YMATCH E MATCHSET) if XFMN (XFMtJNO, ' TYPE ' ) eq ' BINARY' then /* for binary transformatibns, first try to analyze embedded sentence */ COMl?PARS=XFPARSE (IMPOSE (MATCH, XFMI ( XFJ!GJNQ, ' ISC-COMP ' ) ), XFMN, XFMNJO, BASEGR, AUXRULES) ; /* iterate over analyses of embedded sentence, adding matrlx with analyzed embedded sentence to continuations */ CVKOMP E COMPPARS) MATRIX=IMPOSE (MATCH,XFMN (XFMNNO, ' ISC-MATRIX' ) ) ; <MATRIX, <XFMiWO> -t XFAPPLD> in DONE ; end YKOMP; else /* unary transformation */ /* add transformed sentence to continuations*/ NEWSENT = IMPOSE (MATCH, XFNJ (XFK~L\I~, ' ISC ' ) ) ; <NEWSENT, <XFMiVNO> + XFAPPLD> in DONE ; end VMATCH; /"include untrans formed sentence in continuations, since reverse transformation is optional */ CONT in DONE; end VCONT; end 1 c= V XFMNdO ; /"fi lraransfo~.mations haw bcon triod */ /* select and return those strings Which can be analyzcd by tho base component */ t -1 return {PCDONE~ PARSE (DASRGR, b,P (1) ) ne n$) ; cnd XFPARSE; Most of the work of the parser is done by the two Eoutines PROCES and IMPOSE, PROCES matches cv-rant string against the inverse struct~iral index, an1 IMPOSE con~putes the e'flfect of the inverse structural change.
PROCES takes four arguments : SENTENCE STARTWD the string to be matched the structural indax the number o the fixst word in the string to be matcl~ed by the structural index (,this argument is required because the procedure operates recursively; its value is 1 for calls from XFPARSE) the context-free gl-anmar used in matching the structuxal index to the string.
The value of PRQCES is a set, with each element giving one match of the structural index to the string.
Each element is a forest, ..i.e., a tuple of trees, where each tree is represented as described in Sec.
3.1~. The 11th tree of the forest h~s as its root Symbol the nth element of the structural index.
Successive trees subsume contiguous segments of the string being matched.
The following xoutine differs from Petrick's routine of thg same name in using recursion instead of iteration.
def inef PROCES (SENTENCE, SI,STARTWD, GRAMMAR) ; local R, RMDRSI, MATCHES, EI\;IDWD, P, PI, RMDRMATCH, PARSES; /* split off first element of structural index */ R = hd SI; WRSI = tl SI; /* parse part of remainaer of sentence with this element*/ PARSES = PARTPARSE (GRAMMAR I R, SENTENCE STARTWD) ; MATCHES = nR; (VP E PARSES) * set ENDWD = next word in sentence to be matched */ ENDWD = P (1, ' LW+ll ) ; if RMDRSI eg nult then /* if at end of swim, accept parse tree for last element of s.i. only if it extends to end of sentence */ if ENDWD eq ( (#SENTEiJCE) + 1) then <P> in MATCHES; end if ENDWD; else /* not ak end of s.
i., call PROCES recursively to process next element */ RMDRMTCH = PROCESS (SEIITENEE,RMDRSI:, ENDWD, GRAMMAR) ; /* concatenate parse tree for current element to each forest of matches to succeeding elements*/ ( YM E RMDRMTCH) <P> + M in WCHES; end RMDRMTCH ; end if RMDRSI; end VP PARSES; re turn MATCHES ; end PROCESS, The transformational parser uses two varieties of context-free parser.
The first, called simply PARSE, has the external specifications givcn in Sec.
3J.A. Tho other, PARTPARSE, differs in three respects : 1.
the routine takes a fourth aryumant, STARTWD, specifying the first word in the sentence to be matched by the parser (ptccodiny words am ignored) ; tho returned value includes parse trccs which do not extend to the end of the sentence.
2. if tho specified koot symbol is ' X' tbc routine creates a set of parse trees, each containi~qj a single node, named X, spa~lning all possible substrings with first w6rd = STARTWD 3.
the sfmbol COMP will match an element of the string which is a tuple (since, as noted earlier, an embedded sentence is represented as a tuple).
The first two changes are effected by +ilodifyir~g the main routine of the first parser presented in Sec.
3.1. de fine f PARTPARSE (GIIA~AR, ROOT, SEW'PEACE sTARTII'D) ; local PARSES, TREE, iJODES, IVORD, LW; if ROOT eq 'X' then return 1, 'NAME', XI>, <1,'FW1,STARTWD>, <l,'~~+l',~\d+l>}, (STARTWD~~) <= LW <= !!SENTENCE} ; end if ROOT; if STARTWD gt #SE~JQENCE then return nP,; end if STARTWD; TREE = nR; PARSES = nR; WORD = SqhRTWD; NOUES = 1; TREE(~,~NAME~) = ROOT; TGE(1,'FW') = 'rJORV; (while @XPh?D (1,GRAMMAR,SENTENCE) ) PARSES = PARSES u {TREE 1 : end while EXPAND; recurn PARSES ; end PAR'fPARSE ; The third change is accomplished by modifying one line in the EXPEL\ID routine of the first parser in Sec.
3.1. The line which tests for a match against the current sentence word is changed from.
i f SENTENCE (WOW) eq TREE (X, 'NAME '9 to if (SE:JTEtJCs (WORD) eq TREE (X, ' NAME ' ) ) or ( (type SENTENCE (YJORD) aq tupl) and (TREE (x,'NAME1).
eg 'COMP') ) IMPOSE, which computes the effect of the structural change, is a routine with two arguments.
FQREST is a tuple of trees representing the match to the structural index (one element of the value returned byL PROCES) SC is the structural change component of a transformation In addition, KOMP, which (for binary trans formations) holds the embedded sentence and its transformational history, is passed from XFPARSE to IMPOSE as a global variable.
IMPOSE returns a tuple, the frontiers of the trees in the forest as rearranged in accordance with the structural change.
definef IMPOSE (FOREST,SC) local I; return [+: I E SC] if (type I) eq int then FRONTIER(FOREST (I) ) else if I eq 'COMP' then CKObJP, G~SC?
<I>; and IMPOSE; The function FRONTIER takes as its argun~ent a tree and returns the frontier of the tree.
Since the root= node of tha trcc specifies tho words spn~rncd in the sentence (variable Si7J4T, t?~6~~3a~?d ~;ZI XFPAWE) this is trivial: de fin& FRQNrTLER(TREE) ; local.
FNs L'CVPJ,; ~:CJ = TREE (1, 'FI~J') ; ES~P~ = TREE(~,.~LW+~~) ; return SENT (FF.7: LWP1-FW); end FROLITIER; 84 Rppendix.
A Very Short Intrahction to SETL (Prepared in collaboration with Norman ~uhin, CIMS . ) SETL is a programming lwguage designed aro set-theoretic structions and developed at NYU by a group led by Jack2Schwartz.
rich set of operators and control structures provided in SEZL are intended to make the specification of algorithms coneiderably easier in SETL than in other higher-level languages.
To facilitate the reading of our artificial inteliigerlce surveys, ua have used only a subset af SETL; this subset is described in pages which follow.
Those few %points where we haw deviated a true SETL subset are marked on the left with asterisks and mated at the end of this section.
mer information on SETL is available in J.
T. Schwartz, On Programming: An Interim Report on the 6 Project, Parts I and XI.
Courant Computer Science Notes, Cowant Institute of Mathematical Sciences, New York Univ.
X. Kennedy and& Schwartz, "An Introduction to the Set retical Language SET& " Comp.
and Math.
with Appl . 1 9 7.
Wuch of the expression semantics of SZTL is modeled on that wed @in the mathemakical theory of sets, and many of the syntactic ca~~entlans used reflect notions which are standard in that theory.
However, for programmin~urposes a set theory including atoms which tnemselves have no nemrs but may freely be @ers of sets is more conveniegt ban pure set theory.
Thus SETL contains both a general dath vb ject (the set) and other operationally more efficient structures known as atoms ; among these is a vector-like object known as a tuple, Atoms (1) 12) (3) (4) (5) integers (.1276) character strings ('hel.10, pally ' ) bit strings (10101b) boolean constants (true lbr false E Ob) blank atoms (created by the function NEWAT, the SETL equivalent of the LISP GENSYM) Q, the undefined atom labels (labels precede s tatcments, separated by a colon) subroutines and functions tuples (one-dimensional arrays of variable length; i.
e., ordsred lists; written as <1r 2, ' threeJ> ; the null tuple is designated nult) Sets are unordered collections of elements, written as {1,2, ' three' ) :he null set is designated nR Oper at OPS Integers arithmetic + *, comparison (eq,ne, it, gt, lc, gel max,min, abs Booleans and, or, hot (abbreviated n), cq, ne Character and bit strings concatenatiton *(+I, Length #, substriqq (S(1:K).
is the string of length X starting with the Ith element, like tb~ PL/I s&string function) Tuples A tuple is a sequence of components Cl,,C2,C3,.
. ., all but a finite number of which are equal to R, the undefined atom(1) T (K) is the Kth component of T (2) T(1:K) is the tuple of length K starting with the Ith element (3) #T ds the index of the last defined component of T (4) hd T E T(1) tR T I T(2:(#T-1)) T -+ U is the concatenation of tuples T and U Tuples are often used as push-down stacks, using T(#Ttl) = X to push X on the stack and T(#T) = 8 to pop the stack.
Sets X E.
S (true if X is an element of X, otherwise false) arb S (an arbitrary element of set S) 4 u B (union of sets A and B) A (7 B (intersection of sets A and B) A' / B (symmetric difference of se$s A and B).
A R (difference, set) #S (numbernof elements in set S) A with B 2 A U (B) A less B = Aw{B} sets may be compared using eq, ne, incs (in'cludes) Precedence There.
are ;three levels of precedencz: (1).
(highest built-in binary operators proiucing Boolean from non-Boolean values (eq, ne, lt, gt, le, ge, incs, E) (2) unary operators.
(3) other binary operators Within each level, evaluation is le f t-to-right . Set ~efinition A set may be defined by enumeration IA,B,c} or by a set-former: {EXPR(X~, . . ., X ), range-restriction I c (X . .,-X ) } n l* n which forms the set of values EXPR(,X1,.
. .,Xn) for those X1, . ., within the range-restriction for which C (X . . .,X ) n 1 ' n is true.
The range-restriction is a series of items, one for each of the X of the form i For example, Two abbreviated forms are allowed: {X E S I C(X)] foe, Ix,,X E S I 'C(X)I and IEXPR(X), XES} for {E?xPR'(x),,~~~$tipe} Conditional Operators if BOOL then EXPRl e~.je EXPR2 has the value EXPR1 if BOOt is true, the value EXPR2 if.BOOL i.s false.
The else is considered a unary operator, so tha-t;.
if X gt 0 then Y else X + Y is analyzed as (if X gt 0 then Y else X) + Y Functional Application and Sets If F is a set and A any set or atom then (1) FA] (if #P gt 2 then tR P else P (2), P E F I ("P is a tuple") and (#P gt 2) and (P (1) eq A) 1 e.g., if F is a set of pairs, FA) is the set of all X such that <A,X> E F.
(2) F(A) if #FA} eq 1 then grb FA) else $2.
i . e ., the unique image of A under F.
(3) F [A] z the union of the elements of FIA}.
Sets are often used to define complex mappings.
For instance, F = {<111>, <2,4>, <3,9>Ir or alternatively F(1) = 1; F(2) = 4; F(3) = 9; defines a set mapping the first three integers into their squares.
In either case, F(1) is 4 and F(4) is Q.
Quantified Boolean Expressions The basic forms for quantified boolean expressions are The first: is true if C (x) is true for some x in s, and furthermore sets x to the first value found far which C (x) is true; the second is true if C(x) is true for all x in s.
Several range restrictions may be combined: The alternate, numerical, forms of range restrictions, such as min < 3 x < max and min < Vx < max, . may also be used.
Compound Operators Co-mound operatogs 'hdve the form lop: range-restrictions I C (Xlr . . ., X,) ] EXPR (x~, . .,X ) n where op is a binary operator and the range-restrictions have the form described earlier.
The value of this expression is the value of variable VALUE after executing: PILE = ~EXPR~X~, . . ., X ), range-res trictions I C (XI,.
. ., X ) 1 ; n n VALUE from P1.W; (while PILE ne nR) X from PILE; 'VALW = VALUE op X; end while P~LE; For example, [max: X E 11r2t311 (X+1) is 4 K Statements __/ "" All statements are terminated by a semicolon.
Assignment A = EXPR; <ArB,C> = EXPR; is the same as A=EXPR(l) ;B=EXPR(2) ;C=EXPR(3) t Assignment may also be done with the operator "is": "EXPR is V" is an expression with value &XPR and the side effect of assigning this value to V X in S; is S = S with X; X from S; is X=avb S; S = S lcss X; X out S; is S = S less X; Transfer go to LABEL; Conditional if BOOLl then BLOCK, else if BOLL2 then ...
else BLOCK ; n ~teration (while BOOL) BLOCK; (vxl Sit X2 E S2(X1),.-.I~(~1t-.-t X n )) BLOCK; (M < YX < N) BLOCK; (N > VX > M) BLOCK; etc.
Scope of Conditionals and Iterators The BLOCK indicated above as the scope of a SETL conditional statement,or iterator is any sequence of SETL statements.
Note that the semicolon terminatir~g the block is in addition to the semicolon terminating the final statement of that block.
This semicolon may be replaced by an end statement such as end V; end VX; end while; end while BOOL; to indicate to the reader which scope is being closed.
Ex.: (1 VX < 100 I P(X)) Y(#Y+l) = X; end VX; if Y gt O then S = S with X; else N = Ni-1; end if Y; Output print EXPR1,EXPR2,.
. .,EXPRn; Subroutines and Functions Subroutines defined by define SUB(X,Y,Z); BLOCK end SUB; invoked by SUB (A,B, C) ; exit from subroutine by return;!
Functions defined by definef FCN (X,Y, Z) ; BLOCK end FCNr invoked by FCN(A,B,C) exit from function by return EXPR; Infix operator definition defined by X infixop Y; BLOCK end A infixop B; Scoping local X,Y, Z; defines X,Y, and Z as local to the current subroutine or function (these variables are allocated on entry to the routine).
Name scoping is dynamic, as in LISP; a variable declared loca 1 by procedure p is availale to all procedures invoked by p which do not themselves declare the variable local.
Thus define P; local X; Q(1) ; print X; return; end $; define Q (Y) ; x = 7; return; end Q; will print a 1.
Differences from Standard --SE'rL Standard SETL uscs a somewhat smaller character set than we have adopted in this survey.
Thus -t (addition), -+ (co~~catcnation) and U (union) Are all written as + in standard SETL; * (multiplication) and n (intersection) are written as *.
We have adopted the simple and familiar name-scoping rules of the current SETL irnplen~entatiorl in place of the relatively con~plex ones of the SETL standard.
iVe have written all variables function ni;mes, and s~broutin names in upper case, operator names and other tokens in lower case.
BIBLIOGRAPHY [Aho 19721 A.
V. Aho and J.
D. Ullrnan.
The Theory of Parsing, Translation, and Compiling, VolI.
Prentice-Hall, Englewood Cliffs, N.
J. [Bobrw 19691 D.
Bobruw and B.
Fraser, "An Augmented State Transition Network Analysis Procedure, " Proc.
International Joint Conference on Artificial Intelligence [Borgida 19 751 Alexander Borgida, Topics in the Understanding of English Sentences by Computer.
Tech. Rep.
78, Dept.
of Computer Science, Univ.
of Torsnto.
[Bross 19681 Irwin Bross; "A Syntactic Formula for English Sentences : Application to Scientific Narrative, 1 t Cornpu2ers and Biomedical Research 1, 565.
[Cautin 19691 Harvey Cautin, Real English: A Translator to Enable Natural Language Man-Machine Conversation.
Thesis, Moore School of ~lectrical Engineering, Univ, of Pennsylvania . [Cocke 1970 1 J.
Cocke and J.
T. Schwartz, Programming Languages and their Compilers.
Lecture Note series, Courant Institute of Mathematical Sciences, New York Univ; [Culmerauex 19 70 1 Alain Colrnerauer, " ~es Sys terns-Q ou un formalisme pour analyser et synthe tiser des phrases sur ordinateur.
Publ. interne no.
43, ~aculte' des sciences, ~niversi te' de ~ontre'al.
[Craig 19661 J.
ACraig, S.
C. Berenzner, H.
C. Carney, and C.
RLongyeart "DEACON: Direct English Access and Control.
" Proc1966 Fall Joint Computer Conf., Thompson Books, Washington, D.
C. ICuliQgver 19691 P.
Culicover, J.
Kimball, C.
Lewis, D.
Loveman, J.
Moyne, An Automated Recognition Grammar for English, IBM Technical Repor?
ZSC 69-5007.
Cde Chastellier 1969 ] G.
ae Chastellie~ and A.
Colrnerauer-, "W-Grarpmar, I I Proc.
24 flafional ~6nf.
Assn. for Comp . Mach.
. [Dewar 19691 HI Dewar, P.
Bratley, and J.
P. Thorne, "A Program for the Syntactic Analysis of English Sentences, " Cornm.
Assn. Comp.
Mach. 12, 476.
[Dostert 19711 B.
Dostert and F.
Thompson, "How Features Resolve Syntactic Ambiguity," Proc* Symposium on Information Storage and Retrieval.
[Earley 19 701 J.
Earleyr "An Efficient Context-Free Parsing Algorithm.
Camm* AssnCornpMach.
13, 94.
i~rishman 1973a] Ralph Grishman, "~mplementation of the String Parser of English, " in Natural Language Processing, ed.
R. Rustin, Alqorithmics Press, New York.
[Grishman 1973bJ R.
Grishman, N.
Sager, C.
Raze, and B.
Bookchin, "The Linguistic String Parser.
" Proc.
1973 Natl; Computer CConf., AFIPS Press, Montvale, N . J.
[Xarris AS) 651 Zellig Harris, String ~nalysis of Sentence Structure.
Mouton, The Hague.
[Hays 19671 David Hays Introduction to Computa+ional Linguistf cs.
American Elsetrier, NeiJ York.
[Hi2 19671 D.
Hiz and A.
Joshi, "Transfomati'onal Decomposition: A Simple Description of an Algorithm for Transformational Analysis of English Sentences" 2'eme Conf.
Ipternatidnale sur le Traitement Automatique des Langues, Gsenoble . [liobbs 19 $4 ] Jerry Hobbs, A Metalanguage for ~xpressi~g Grammatical Restrictions -in Nodal Spans Parsing ~f Natural.
Language. Courant Computer Science Report $2, Courant Inst.
Rath. Sci._, New York ~niv, [Eiobbs 19751 J.
Hobbs and R.
Grishman, "The Automatic.
Transformational Analysis of English Sentences: An 1mplkmentation".
Submitted to Xnt'l J.
Conputer Matn.
[~rons 19631 N.
Irons, "Error-Correcting Parse Algorithm..
=om. Assn.
Comp. Mach.
6, 669.
[~oshi. 19621 Aravind Joshi, A procedure for transformational decoinposition.
Transformations and Discc~rse Analysis Pape-rs $42, UnitT.
of Pennsylvania, [Joshi 19 331 Atavind Joshi, "A Class of Tr,-;ulsformational Grammars " In The Formal Analysis of Natural Languages* ed.
MI Gross, M.
Halle, and M.
-P. Schiitzenberger, Mouton, The Hague.
[Kay 19671 Martin Kay, Experiments with a powerful Parser..
PI In 2&me Conf.
Internationale sur le ~raitement Automatique des Langues, Grenoble.
[Keyser 19671 S.
5. Keyser and S.
R. petrick, syntactic Analysis * Air Force Cambirdge Research ~aboratories, -WCRL-6 7-0 305.
[Kittredge 19731 Richard Kittredge et al., TAUM 73.
A report of the Projet de Traduction Automatique de I'universit6 de ~ontrgal.
[Kuno 19621 S.
Kuno and A.
G. Oettinger, "~ultiple-Path Syntactic Analyzer".
Information Processing 1962, North-Holland, Amstetdam.
[Kuno 19631 Su$umo Kuno, "The Multiple-path Syntactic Analyzer for English.
" Report No.
NSF-9 in Mathematical Linguistics and Automatic Translation of the Computation Lab., Harvard Univ.
[Kmo 19651 Susumo KWO, "The Predictive Analyzer and a Path Elimination Technique".
Comm. Assn.
Comp. Mach.
8, 453.
[Loveman 19711 D.
Loveman, J.
Moyne, and R.
Tobey, "CUE: A Preprocessor System for Restricted, Natural English.
In Proc.
Symposium on Information Storage and ~etrieval.
[Owens 1'9751 Phillip Owens, A Comprehensive Survey of Parsing Algorithms for Programming Languages, Courant Computer Science Report #4, Courant Inst.
Math, Sci ., New York Univ.
(Forthcoming) . [Paxton 19731 W; H.
Paxton and A.
E. Robinson, "A Parser for a Speech ~ndeistanding System.
" Advance Papers of the Third Intl.
Joint Conf.
on ~rtif iciai Intelligence, S'tanford Research Instjtute, California.
[Petrick 1965 ] S tqley Re Petrick, A Recognition Proceddre for Trans formational Grammars.
Doctoral Disseztatf on . [petrick 19661 Stanley R.
Fetrick, A irogram for Transformational Syntactic Analysis, Air Force Cambridge Research Laboratories, AFCRL--66-698.
[Petrick 19731 Stanley R.
Petrick, Transformational Analysis, II In Natural Language Processing, ed.
R. Rustin, Algorithmics Press, N.
Y. [Petrick 19 75 I Stanley R.
Petrick, "Design of the Underlying Structure for a Data Base Retrieval.
System. " In Directions in Artificial Intelligence: Natural Language Processing, ed.
R. Gsishman Courant Computer Science Report #7, Courant Institute of Mathematical Sciences, New York nit', [Plath 19 74a] Warren J, Plath, "Transformational Gramcar and Trans formational Parsing in the REQUEST Sys tern.
" In Computational and Mathematical Linguistics, Proc, Intl.
Gonf. on Computational Linguistics, edA.
Zampolli, [Plath 1974b] Warren J*.
Plath, String Transformations in the REQmST System.
IBM TI J.
Watson Research Center, RC 4947 (#21963).
[~aze 19741 Carol Raze, A computatidnal treatment of c~osdinate conjunctions.
Talk at 12 Ann.
Meeting of Assn.
of Computational Linguistics, Amherst, Mass., July 26, 1974.
[Sager 19 671 Naomi Sager, "Syntacti~ analysis of natural language.
n In Advances in Computers, No.
8, ed.
Fa Alt and M.
Rubinoff, Academic Press, N.
Y. [Sager 19731 Naomi Sager, "The string parser for scientific literature . " In N%tural Language Processing, ed.
R. Rustin, Algorithmics Press, N.
Y. [Sager 19751 N.
Sager and R.
Grishman, "The Restriction Language for Co~mputer Gr&mars of Natural Language.
" Comrn.
AssnComp.
Mach. 18,390.
[Shapiro 19711 PShapiro and Dw Stermole, "ACOEUl (Automatic Coder Report Narrative) : An Automatied NaturalLanguage Question-Answering System for Surgicm Reports, " Computers and Automation, Feb.
1971, p.
13. [Simmons 19751 R.
Simmons and G.
Bennett-Wovak, "Semantically Analyzing an English Subset for the Clowns Microworld, " Tech.
Report NL-24, Dept.
of Computer Sciences, Univ.
of Texas at Austin.
[Thompson 19691 F.
9. Thompson, P.
C. Lockenan, B.
Dostert, and R.
S. Deverill, "REL: A Rapidly Extensible Language Sys tern.
" Proc.
24 Natl.
Conf. Assn.
Cornp. Mach.
[Thorne 13681 J.
P. Thorne, P.
Bratley, and H.
Dewar, h he Syntactic Analysis of English by ~achine.
it ~achine Intelligence 3.
[walker 19661 D.
Walker, P.
Chapin, M.
~eis, and Lo Gross, Recent Developments in the MITRE Syntactic Analysis Procedure.
MITRE Report MTP-11.
11 [Walker 19 73 ] Donald Walker, "Automated Language Processing.
In Annual Review of Informaticjn Science and Techn'olog.~, Vo1.
8, ed.
C. Cuadra, American Society for Information Science, Washington, Do C.
[Wilks 19 751 Yorick Wilks, "An Intelligent Analyzer and Understander of English".
Comrn. Assn.
Cornp. Mach.
18, 264.
[Winograd 19 711 Terry Winograd, Procedures as.
a Representation for Data in a Computer Program for Understanding Natural Language.
MIT Report MAC TR-48.
[Woods 19 70a)William A.
Woods, "Con@ext-Sensitive Parsing.
" Cum.
Assn. Cornp.
Mach. 13, 437.
[Woods 1970bJ William A.
Woods, "Transition Network Grammars tI for Natural Language Analysis.
Corn. Assn.
Cornp. Mach.
13, 591.
[Woods 19721 W.
A. Woods, .RI M.
Kaplan, B.
Nash-Webber, The Lunar Sciences Natural Language Information Syste'm: Final Report.
Report tf2378, Bolt Beranek and Newman, Cambridge, Mass.
[woods 19731 ~illiam A.
Woods, "An Eqerirnental Parsing Sy\stem for "Transition Network Grammars.
' In Natural Language Processing, ed.
R. Rus tin, Algorithmics Press, New York.
[Zwicky 19 651 A.
Zwicky, J.
Friedman, B.
Hall, and D.
Walker, h he MITRE Syntactic Analyqis Procedure for Trans formational Grammars.
" Proc.
1965 Fall Joint Computer CConf., Thompson Books, Washington, D. C .

