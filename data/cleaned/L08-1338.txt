<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Ahmed Abbasi</author>
<author>Hsinchun Chen</author>
</authors>
<title>Applying authorship analysis to extremist-group web forum messages</title>
<date>2005</date>
<journal>IEEE Intelligent Systems</journal>
<volume>20</volume>
<contexts>
<context>n Milton (Tweedie et al., 1998), and the Federalist papers (Mosteller and Wallace, 1964). In other cases, efforts have concentrated on identifying the authors of modern media, such as forum postings (Abbasi and Chen, 2005), newspaper articles (Diederich et al., 2000) and newsgroup postings. Because of email’s ubiquity as a communication tool, attribution of the authorship of email messages is also a critical problem. </context>
<context>of these classifiers are relatively standard: an SVM, which has been shown to be highly effective as a tool for authorship attribution and document classification more generally (Dumais et al., 1998; Abbasi and Chen, 2005), and a multinomial probabilistic classifier similar to those used in (McCallum and Nigam, 1998; Guthrie et al., 1994). The third classifier is based upon ongoing work to refine the multinomial class</context>
</contexts>
<marker>Abbasi, Chen, 2005</marker>
<rawString>Ahmed Abbasi and Hsinchun Chen. 2005. Applying authorship analysis to extremist-group web forum messages. IEEE Intelligent Systems, 20(5):67–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Allison</author>
</authors>
<title>A Bayesian Model of Word Counts for Document Classification (in preparation</title>
<date>2008</date>
<tech>Ph.D. thesis</tech>
<institution>Department of Computer Science, University of Sheffield</institution>
<contexts>
<context>thrie et al., 1994). The third classifier is based upon ongoing work to refine the multinomial classifier by hypothesising an alternate (and more expressive) distribution of word counts in documents (Allison, 2008), in a similar vein to (Madsen et al., 2005). The rest of this paper is organised as follows: Section 2. describes the original corpus and the manner in which we create our corpus for email attributi</context>
<context> hierarchical probabilistic classifier, but make several notable modifications to the method in that work—these modifications constitute ongoing work, and a more complete description can be found in (Allison, 2008). The hierarchical model we propose still assumes that all documents are samples from some multinomial, but rather than all being samples from the same multinomial Words Bigrams Trigrams Stems Closed</context>
</contexts>
<marker>Allison, 2008</marker>
<rawString>Ben Allison. 2008. A Bayesian Model of Word Counts for Document Classification (in preparation). Ph.D. thesis, Department of Computer Science, University of Sheffield.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Baayen</author>
<author>H van Halteren</author>
<author>F Tweedie</author>
</authors>
<title>Outside the cave of shadows: using syntactic annotation to enhance authorship attribution</title>
<date>1996</date>
<journal>Lit Linguist Computing</journal>
<volume>11</volume>
<marker>Baayen, van Halteren, Tweedie, 1996</marker>
<rawString>H Baayen, H van Halteren, and F Tweedie. 1996. Outside the cave of shadows: using syntactic annotation to enhance authorship attribution. Lit Linguist Computing, 11(3):121–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll Briscoe</author>
<author>E</author>
<author>R Watson</author>
</authors>
<title>The second release of the RASP system</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions</booktitle>
<location>Sydney, Australia</location>
<marker>Briscoe, E, Watson, 2006</marker>
<rawString>J. Carroll Briscoe, E. and R. Watson. 2006. The second release of the RASP system. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Burrows</author>
</authors>
<title>Word-patterns and story-shapes: The statistical analysis of narrative style</title>
<date>1987</date>
<booktitle>Literary and Linguistic Computing</booktitle>
<pages>2--2</pages>
<contexts>
<context>ds, where the stemming is done using an implementation of the popular Porter stemmer (Porter, 1980). From a more stylistically enlightened perspective, and following on from the work initial work of (Burrows, 1987) and many subsequent studies, we also consider using purely closed–class words to represent a text, since the use of these words is believed to be far more invariant across topic. Finally, we conside</context>
</contexts>
<marker>Burrows, 1987</marker>
<rawString>J. Burrows. 1987. Word-patterns and story-shapes: The statistical analysis of narrative style. Literary and Linguistic Computing, 2(2):61–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Diederich</author>
<author>J¨org Kindermann</author>
<author>Edda Leopold</author>
<author>Gerhard Paass</author>
</authors>
<title>Authorship attribution with support vector machines</title>
<date>2000</date>
<journal>Applied Intelligence</journal>
<volume>19</volume>
<contexts>
<context>eralist papers (Mosteller and Wallace, 1964). In other cases, efforts have concentrated on identifying the authors of modern media, such as forum postings (Abbasi and Chen, 2005), newspaper articles (Diederich et al., 2000) and newsgroup postings. Because of email’s ubiquity as a communication tool, attribution of the authorship of email messages is also a critical problem. However, substantial collections of email hav</context>
</contexts>
<marker>Diederich, Kindermann, Leopold, Paass, 2000</marker>
<rawString>Joachim Diederich, J¨org Kindermann, Edda Leopold, and Gerhard Paass. 2000. Authorship attribution with support vector machines. Applied Intelligence, 19(12):109–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Dumais</author>
<author>John Platt</author>
<author>David Heckerman</author>
<author>Mehran Sahami</author>
</authors>
<title>Inductive learning algorithms and representations for text categorization</title>
<date>1998</date>
<booktitle>In CIKM ’98</booktitle>
<pages>148--155</pages>
<contexts>
<context>ent classifiers. Two of these classifiers are relatively standard: an SVM, which has been shown to be highly effective as a tool for authorship attribution and document classification more generally (Dumais et al., 1998; Abbasi and Chen, 2005), and a multinomial probabilistic classifier similar to those used in (McCallum and Nigam, 1998; Guthrie et al., 1994). The third classifier is based upon ongoing work to refin</context>
<context> 4.3. Support Vector Machine Classifier Our final classifier is a linear Support Vector Machine, shown in several comparative studies to be the best performing classifier for document categorization (Dumais et al., 1998; Yang and Liu, 1999). Briefly, the support vector machine seeks the hyperplane which maximises the separation between two classes while minimising the magnitude of errors committed by this hyperplane</context>
</contexts>
<marker>Dumais, Platt, Heckerman, Sahami, 1998</marker>
<rawString>Susan Dumais, John Platt, David Heckerman, and Mehran Sahami. 1998. Inductive learning algorithms and representations for text categorization. In CIKM ’98, pages 148–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Louise Guthrie</author>
<author>Elbert Walker</author>
<author>Joe Guthrie</author>
</authors>
<title>Document classification by machine: theory and practice</title>
<date>1994</date>
<booktitle>In Proceedings COLING ’94</booktitle>
<pages>1059--1063</pages>
<contexts>
<context>hip attribution and document classification more generally (Dumais et al., 1998; Abbasi and Chen, 2005), and a multinomial probabilistic classifier similar to those used in (McCallum and Nigam, 1998; Guthrie et al., 1994). The third classifier is based upon ongoing work to refine the multinomial classifier by hypothesising an alternate (and more expressive) distribution of word counts in documents (Allison, 2008), in</context>
<context>ssifier The multinomial probabilistic classifier has been widely used, and in many cases has been shown to perform robustly (see (Lewis, 1998) for an overview of early probabilistic classifiers, and (Guthrie et al., 1994; McCallum and Nigam, 1998; McCallum et al., 1998) for examples of the multinomial classifier applied to real problems). In terms of notation, we use ~c to represent a random variable and c to represe</context>
</contexts>
<marker>Guthrie, Walker, Guthrie, 1994</marker>
<rawString>Louise Guthrie, Elbert Walker, and Joe Guthrie. 1994. Document classification by machine: theory and practice. In Proceedings COLING ’94, pages 1059–1063.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Jansche</author>
</authors>
<title>Parametric models of linguistic count data</title>
<date>2003</date>
<booktitle>In ACL ’03</booktitle>
<pages>288--295</pages>
<contexts>
<context>ot so characteristic of an author) while truly stylistic words should have low variance in their parameter and be reliable indicators of authorship. To overcome these problems, and following on from (Jansche, 2003; Lowe, 1999) who use similar models for individual words, we use a classifier which decomposes the term p(djc) into a sequence of independent terms of the form p(djjc), and hypothesises that conditio</context>
<context>en training documents from fixed class, we look to estimate the parameters of interest, in this case the ( j; j) since the j are integrated out, using a method–of–moments estimate similar to that in (Jansche, 2003). 4.3. Support Vector Machine Classifier Our final classifier is a linear Support Vector Machine, shown in several comparative studies to be the best performing classifier for document categorization</context>
</contexts>
<marker>Jansche, 2003</marker>
<rawString>Martin Jansche. 2003. Parametric models of linguistic count data. In ACL ’03, pages 288–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale svm learning practical</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods Support Vector Learning</booktitle>
<contexts>
<context>ses the linear kernel such that (x1;x2) = x1 x2. Nevertheless, the linear SVM has been shown to perform extremely well, and so we present results using the the linear kernel from theSVMlight toolkit (Joachims, 1999) (we note that experimentation with non–linear kernels made little difference, with no consistent trends in performance). We use default parameter values for the SVM, and the most typical method for </context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale svm learning practical. Advances in Kernel Methods Support Vector Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Klimt</author>
<author>Yiming Yang</author>
</authors>
<title>The enron corpus: A new dataset for email classification research</title>
<date>2004</date>
<booktitle>In Proceedings of ECML</booktitle>
<pages>217--226</pages>
<contexts>
<context>tains email from approximately 160 employees. The data were originally released by the FERC, and this was followed by substantial work correcting integrity issues in the data, described in detail in (Klimt and Yang, 2004). The full dataset is available at http://www.cs.cmu.edu/˜enron/. From this raw dataset, we create a corpus using nine employees’ mail, and select those nine on the basis of the largest outgoing mail</context>
</contexts>
<marker>Klimt, Yang, 2004</marker>
<rawString>Bryan Klimt and Yiming Yang. 2004. The enron corpus: A new dataset for email classification research. In Proceedings of ECML 2004, pages 217–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
</authors>
<title>Na¨ıve (Bayes) at forty: The independence assumption in information retrieval</title>
<date>1998</date>
<booktitle>In Proceedings of ECML-98</booktitle>
<pages>4--15</pages>
<contexts>
<context>t standard method for this purpose. 4.1. Multinomial Probabilistic Classifier The multinomial probabilistic classifier has been widely used, and in many cases has been shown to perform robustly (see (Lewis, 1998) for an overview of early probabilistic classifiers, and (Guthrie et al., 1994; McCallum and Nigam, 1998; McCallum et al., 1998) for examples of the multinomial classifier applied to real problems). </context>
</contexts>
<marker>Lewis, 1998</marker>
<rawString>David D. Lewis. 1998. Na¨ıve (Bayes) at forty: The independence assumption in information retrieval. In Proceedings of ECML-98, pages 4–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lowe</author>
</authors>
<title>The beta-binomial mixture model and its application to tdt tracking and detection</title>
<date>1999</date>
<booktitle>In Proceedings of the DARPA Broadcast News Workshop. Rasmus</booktitle>
<contexts>
<context>istic of an author) while truly stylistic words should have low variance in their parameter and be reliable indicators of authorship. To overcome these problems, and following on from (Jansche, 2003; Lowe, 1999) who use similar models for individual words, we use a classifier which decomposes the term p(djc) into a sequence of independent terms of the form p(djjc), and hypothesises that conditional on known</context>
</contexts>
<marker>Lowe, 1999</marker>
<rawString>S. Lowe. 1999. The beta-binomial mixture model and its application to tdt tracking and detection. In Proceedings of the DARPA Broadcast News Workshop. Rasmus E. Madsen, David Kauchak, and Charles Elkan.</rawString>
</citation>
<citation valid="true">
<title>Modeling word burstiness using the Dirichlet distribution</title>
<date>2005</date>
<booktitle>In ICML ’05</booktitle>
<pages>545--552</pages>
<marker>2005</marker>
<rawString>2005. Modeling word burstiness using the Dirichlet distribution. In ICML ’05, pages 545–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Nigam</author>
</authors>
<title>A comparison of event models for na¨ıve bayes text classification</title>
<date>1998</date>
<booktitle>In Proceedings AAAI-98 Workshop on Learning for Text</booktitle>
<contexts>
<context>tive as a tool for authorship attribution and document classification more generally (Dumais et al., 1998; Abbasi and Chen, 2005), and a multinomial probabilistic classifier similar to those used in (McCallum and Nigam, 1998; Guthrie et al., 1994). The third classifier is based upon ongoing work to refine the multinomial classifier by hypothesising an alternate (and more expressive) distribution of word counts in documen</context>
<context>l probabilistic classifier has been widely used, and in many cases has been shown to perform robustly (see (Lewis, 1998) for an overview of early probabilistic classifiers, and (Guthrie et al., 1994; McCallum and Nigam, 1998; McCallum et al., 1998) for examples of the multinomial classifier applied to real problems). In terms of notation, we use ~c to represent a random variable and c to represent an outcome. We use roma</context>
</contexts>
<marker>McCallum, Nigam, 1998</marker>
<rawString>A. McCallum and K. Nigam. 1998. A comparison of event models for na¨ıve bayes text classification. In Proceedings AAAI-98 Workshop on Learning for Text Categorization. Andrew K. McCallum, Ronald Rosenfeld, Tom M.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving text classification by shrinkage in a hierarchy of classes</title>
<date>1998</date>
<booktitle>In ICML-98</booktitle>
<pages>359--367</pages>
<marker>Mitchell, Ng, 1998</marker>
<rawString>Mitchell, and Andrew Y. Ng. 1998. Improving text classification by shrinkage in a hierarchy of classes. In ICML-98, pages 359–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Merriam</author>
</authors>
<title>Heterogeneous Authorship in Early Shakespeare and the Problem of Henry V</title>
<date>1998</date>
<journal>Lit Linguist Computing</journal>
<volume>13</volume>
<contexts>
<context>p attribution is an area of study which has attracted great interest, and for many different reasons: in some cases, scholars have sought identification of historic texts such as Shakespeare’s plays (Merriam, 1998), the works of John Milton (Tweedie et al., 1998), and the Federalist papers (Mosteller and Wallace, 1964). In other cases, efforts have concentrated on identifying the authors of modern media, such </context>
</contexts>
<marker>Merriam, 1998</marker>
<rawString>Thomas Merriam. 1998. Heterogeneous Authorship in Early Shakespeare and the Problem of Henry V. Lit Linguist Computing, 13(1):15–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Mosteller</author>
<author>D L Wallace</author>
</authors>
<title>Inference and Disputed Authorship: The Federalist Papers</title>
<date>1964</date>
<publisher>AddisonWesley</publisher>
<contexts>
<context> reasons: in some cases, scholars have sought identification of historic texts such as Shakespeare’s plays (Merriam, 1998), the works of John Milton (Tweedie et al., 1998), and the Federalist papers (Mosteller and Wallace, 1964). In other cases, efforts have concentrated on identifying the authors of modern media, such as forum postings (Abbasi and Chen, 2005), newspaper articles (Diederich et al., 2000) and newsgroup posti</context>
</contexts>
<marker>Mosteller, Wallace, 1964</marker>
<rawString>F. Mosteller and D. L. Wallace. 1964. Inference and Disputed Authorship: The Federalist Papers. AddisonWesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Porter</author>
</authors>
<title>An algorithm for suffix stripping</title>
<date>1980</date>
<journal>Program</journal>
<volume>14</volume>
<contexts>
<context> than single words, we also consider a bag–of–bigrams and a bag–of–trigrams. We also consider a bag of stemmed words, where the stemming is done using an implementation of the popular Porter stemmer (Porter, 1980). From a more stylistically enlightened perspective, and following on from the work initial work of (Burrows, 1987) and many subsequent studies, we also consider using purely closed–class words to re</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D M Rennie</author>
<author>Ryan Rifkin</author>
</authors>
<title>Improving multiclass text classification with the Support Vector Machine</title>
<date>2001</date>
<tech>Technical report</tech>
<institution>Massachusetts Insititute of Technology, Artificial Intelligence Laboratory. Fiona</institution>
<contexts>
<context>nce). We use default parameter values for the SVM, and the most typical method for transforming the SVM into a multi-class classifier, the One-Vs-All method, shown to perform extremely competitively (Rennie and Rifkin, 2001). All vectors are also normed to unit length. 5. Experiments We prepare the corpus as for experiments as follows: we define contiguous alpha-numeric strings to be words and case is normalised. For al</context>
</contexts>
<marker>Rennie, Rifkin, 2001</marker>
<rawString>Jason D. M. Rennie and Ryan Rifkin. 2001. Improving multiclass text classification with the Support Vector Machine. Technical report, Massachusetts Insititute of Technology, Artificial Intelligence Laboratory. Fiona J. Tweedie, David I. Holmes, and Thomas N. Corns.</rawString>
</citation>
<citation valid="true">
<title>The Provenance of De Doctrina Christiana, attributed to John Milton: A Statistical Investigation. Lit Linguist Computing</title>
<date>1998</date>
<marker>1998</marker>
<rawString>1998. The Provenance of De Doctrina Christiana, attributed to John Milton: A Statistical Investigation. Lit Linguist Computing, 13(2):77–87.</rawString>
</citation>
</citationList>
</algorithm>

