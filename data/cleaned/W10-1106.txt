Proceedings of the NAACL HLT 2010 Second Louhi Workshop on Text and Data Mining of Health Documents, pages 38–45,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics
Reliability and Type of Consumer Health Documents on the World Wide 
Web: an Anotation Study  
 
 
Melanie J. Martin 
California State University, Stanislaus 
One University Circle 
Turlock, CA 95382 
mmartin@cs.csustan.edu 
 
 
 
Abstract 
In this paper we present a detailed scheme 
for annotating medical web pages de-
signed for health care consumers.  The 
annotation is along two axes: first, by re-
liability or the extent to which the medical 
information on the page can be trusted, 
second, by the type of page (patient leaf-
let, commercial, link, medical article, tes-
timonial, or support). We analyze inter-
rater agreement among three judges for 
each category. Inter-rater agreement was 
moderate (0.7 acuracy, 0.62 F-measure, 
0.49 Kappa) on the reliability axis and 
good (0.81 acuracy, 0.72 F-measure, 
0.73 Kappa) along the type axis. 
1 Introduction

With the explosive growth of the World Wide Web 
has come, not just an explosion of information, but 
also the explosion of false, misleading and unsup-
ported information. At the same time, the web is 
increasingly used for tasks where information qual-
ity and reliability are vital, from legal and medical 
research by both professionals and lay people, to 
fact checking by journalists and research by gov-
ernment policy makers. 
   In particular, there has been a proliferation of 
web pages in the medical domain for health care 
consumers. At the first sign of illness or injury 
more and more people go to the web before con-
sulting medical professionals. The quality and reli-
ability of the information on consumer medical 
web pages has ben of concern for some time to 
medical profesionals and policy makers. (For ex-
ample see Eysenbach et al., 2002, Impicciatore et 
al., 197.) 
Our goal is to create a system that can automati-
cally measure the reliability of web pages in the 
medical domain (Martin, 204). More specifically, 
given a web page resulting from a user query on a 
medical topic, we would like to automaticaly pro-
vide an estimate of the extent to which the infor-
mation on the page can be trusted. In order to make 
use of supervised natural language processing and 
machine learning algorithms to create such a sys-
tem, and to ultimately evaluate the performance of 
the system, it is necessary to have human anno-
tated data. 
It is important to note the varied uses of the term 
“reliability” in the computer and information sci-
ences. In the current context we use it to refer to an 
intrinsic property of a web page: essentially the 
trustworthiness of the information it contains. This 
sense of reliability is distinct from its meaning in 
measurement theory as an indicator of repeatabil-
ity. It also excludes measures such as credibility 
that are based on user beliefs or understanding. 
In this paper we report results of an annotation 
study of medical web pages designed for health 
care consumers. Three humans annotated a corpus 
of web pages along two axes. The first axis is the 
reliability of the information contained in the page. 
The second axis is the type, or kind, of page. Inter-
coder agreement was moderate (0.7 accuracy, 
0.62 F-measure, 0.49 Kapa) on the reliability axis 
and god (0.81 accuracy, 0.72 F-measure, 0.73 
Kapa) along the type axis. 
In our materials and methods section we discus 
the data, definitions, annotation study and the re-
sults. We follow with a discusion section and a 
conclusion. 
38
2 Materials
and Methods 
In this section we wil discus the data and definitions 
for the anotation task.  We also describe the annotation 
study and the testing and analysis. 
2.1 Data

The data to be anotated consists of two corpora of 
web pages created by the author: IBS70 and 
MMED10. The MMED10 corpus is a subset of a 
larger corpus (MED1000). Both corpora are de-
scribed below. 
2.1.1 IBS70
Corpus 
The IBS70 corpus was created as an exploratory 
corpus for use in system development. It was 
originally the top 50 Google hits for "irritable 
bowel syndrome" downloaded automatically 
though the Google API on July 1, 2004. The query 
was chosen to provide a range of quality and types 
of pages which one would expect to see more gen-
erally in the medical domain on the web: patient 
information from both traditional and alternative 
sources, support groups, medical articles, commer-
cial pages from drug companies and quacks. Dur-
ing system development we determined that it 
would be useful to have additional pages at both 
ends of the reliability spectrum, possibly to use as 
seeds for clustering. 
On September 15, 204, twenty documents were 
added to the corpus to create the IBS70. Ten highly 
reliable documents were added based on web 
searches to find documents judged as meeting the 
standards of Evidence Based Medicine. Ten docu-
ments judged unreliable were added by taking the 
first ten relevant “Sponsored Links” resulting from 
a Gogle search on “irritable bowel syndrome”. 
There are two important things to note about this 
process: first, the high quality pages added were 
disproportionately from the U.K.; second, the low 
quality pages tend toward the crasly comercial 
and are more extreme than one would likely find in 
this proportion of the top 100 (or even 20) of the 
results of a Gogle query for a medical condition. 
2.1.2 MMED10
Corpus 
The MED100 corpus was created on November 
5th and 8th, 2004 by automatically downloading 
from Gogle the top 10 search results for each of 
the following 10 queries: 
 
• Adrenoleukodystrophy 
• Alzheimer's 
• Endometriosis 
• Fibromyalgia 
• Obesity 
• Pancreatic cancer 
• Coloidal Silver 
• Irritable Bowel Syndrome 
• Late Lyme Disease 
• Lower Back Pain 
 
The queries were chosen to provide a broad range 
of what might be typical queries for health con-
sumers on the web and the types of pages that 
would result from these queries.  
Coloidal Silver was chosen in the hopes of pro-
viding a sufficient number of pages of questionable 
reliability. Adrenoleukodystrophy, Pancreatic Can-
cer, Alzheimer’s and Obesity were chosen because 
there is general agrement in the medical comu-
nity that these are diseases or health issues, and on 
diagnostic techniques. They also cover a spectrum 
of occurrence rates, with Adrenoleukodystrophy 
being relatively rare and Obesity being relatively 
comon. The other five queries were chosen be-
cause there is less agreement in both the medical 
comunity and the general population about the 
existence, frequency, severity and treatment of 
these conditions. In particular, Fibromyalgia and 
IBS can be exclusionary diagnoses without clear 
and successful treatment options, which can open 
the door to web pages with a range of questionable 
treatments. 
For anotation purposes a subset of this corpus, 
MMED10, with 10 pages, was created by ran-
domly selecting ten documents from each of the 
ten queries. 
At this time neither corpus is publicly available. 
However they can be provided on request and it is 
anticipated that they wil be made publicly avail-
able once a viable standard is established for the 
anotations. 
2.2 Definitions

The primary clasification task is to clasify pages 
based on their reliability (quality or trustworthines 
of the information they contain). The secondary 
39
classification task is to classify pages based on 
their type (e.g. commercial, patient leaflet, link). 
The clasification by type emerged from the hy-
pothesis that different types of pages may need to 
be treated differently to classify them based on 
their reliability. For example, if the primary pur-
pose of a page is to provide links to information, 
determining the reliability of the page may require 
determining the reliability of the pages to which it 
links. However, in the current study, annotators are 
provided only the given web page and not allowed 
to follow links, so their reliability determination 
was made based on the apparent balance and ob-
jectivity of the links on the page. 
For both tasks, only one tag was alowed, so an-
notators were instructed to consider the main pur-
pose or intent of the page. 
2.2.1 Reliability

Reliability of web pages is annotated based on a 
five level scale. 
 
Probably Reliable (PrR) 
The information on these pages apears to be com-
plete and corect, meting the standards of Evi-
dence-Based Medicine where apropriate. 
Information is presented in a balanced and objec-
tive manner, with the full range of options dis-
cused (where appropriate). The page and author 
appear reputable, with no obvious conflicts of in-
terest. The appropriate disclaimers, policies, and 
contact information are present. Where appropri-
ate, sources are cited. An example of a page in this 
category would be a patient leaflet from a reputa-
ble source that adheres to the standards of Evi-
dence-Based Medicine. 
 
Posibly Reliable (PoR) 
The information on the page is generaly god and 
without obvious false or outdated statements, but 
may not be suficiently complete and balanced or 
may not conform to evidence-based standards. An 
example of a page in this category would be a pa-
tient leaflet that contains only a brief description of 
diagnostic procedures or suggests a treatment op-
tion that is generally acepted, but not supported 
by evidence. 
 
 
 
Unable to determine (N) 
For these pages it is dificult or imposible to de-
termine the reliability, generally because there is 
not enough information. For example, the page 
may be blank, only contain login information, or 
be the front page of a medical journal. 
 
Posibly Unreliable (PoU) 
These pages may contain some reliable informa-
tion, but either have some that is outdated, false or 
misleading, or the information is suficiently un-
balanced so as to be somewhat misleading. An ex-
ample of a page that might fall into this category is 
a practitioner comercial pages, which has valid 
information about an illness, but only discuss the 
prefered treatment offered by the practitioner. 
 
Probably Unreliable (PrU) 
These pages contain false or misleading informa-
tion, or present an unbalanced or biased viewpoint 
on the topic. Examples of pages in this category 
would include: testimonials (unsuported view-
points or opinions of a single individual) or pages 
that are clearly promoting and selling a single 
treatment option. 
2.2.2 Type
of Page 
We found six types of pages that frequently come 
up in search results for queries in the medical do-
main: Commercial, Patient Leaflet, Link, Medical 
Articles, Suport, and Testimonials. There are also 
pages which are not relevant, or do not contain 
sufficient information to make a determination. 
Below we discus each of these types. When a 
page seems to overlap categories the annotation is 
based on the primary purpose of the page. 
 
Comercial (C) 
The primary purpose of these pages is to sel some-
thing, for example, pages about an ailment spon-
sored by a drug (also more general treatment or 
equipment) company, which sells a drug to treat it. 
Given the desire to sell, these pages might not pre-
sent complete or balanced information (making 
them less likely to be reliable). Practitioner pages 
with no real (substantial) information, which are 
designed to get people to make an appointment, as 
opposed to patient leaflets (designed to suplement 
information that patients receive in the ofice or 
clinic), might also fall into this category 
40
Link (L) 
The primary purpose of these pages is to provide 
links to other pages or sites (external), which will 
provide information about a certain ilness or 
medical condition. These links may or may not be 
annotated, and the degree of annotation may vary 
considerably. Since the reliability of these pages 
depends on the reliability of the pages they link to 
(posibly also on the text in the anotations), with-
out following the links a reliability estimate can be 
based on the range and apparent objectivity of the 
links. 
 
Patient Leaflet, Brochure, Fact Shet or FAQ 
(P) 
The primary purpose of these pages is to provide 
information to patients about a specific illness or 
medical condition. Generaly, these pages wil be 
produced by a clinic, medical center, physician, or 
government agency, etc. The primary purpose is to 
provide information. This class needs to be distin-
guished from medical articles, especially in ency-
clopedias or the Merck Manual, etc. These pages 
wil tend to have headings like: symptoms, diagno-
sis, treatment, etc. These headings can take the 
form of links to specific parts of the same page or 
to other pages on the same site (internal). The reli-
ability of these pages is based on their content and 
determined by factors including Evidence-Based 
Medicine, completenes, and the presence of incor-
rect or outdated information. 
 
Medical Article (practitioner or consumer) 
(MA) 
The primary purpose of these pages is to discuss an 
aspect of a specific ilness or medical condition, or 
a specific ilness or medical condition. These can 
be divided into two main categories: articles aimed 
at consumers and articles aimed at health practitio-
ners. 
    Articles aimed at health practitioners, particu-
larly doctors, may be scientific research articles. 
The reliability of these pages is based on their con-
tent and determined by factors including Evidence 
Based Medicine, completeness, and the presence of 
incorrect or outdated information. Note: Medline 
search results may be considered a links page to 
medical articles.  
    Articles aimed at consumers may come from a 
variety of sources including mainstream and alter-
native media sources. Reliability is determined 
based on the content as with articles for practitio-
ners. 
 
Testimonial (T) 
The primary purpose of these pages is to provide 
testimonial(s) of individuals about their experience 
with an ilnes, condition, or treatment. While in-
dividuals may be considered reliable when discuss-
ing their own personal experiences, these pages 
tend to be unreliable, because they are generally 
not objective or balanced. There is a tendency for 
readers to generalize from very specific informa-
tion or experiences provided by the testimonial, 
which can be misleading.  
 
Support (S) 
The primary purpose of these pages is to provide 
support of sufferers (or their loved ones or care-
givers) of a particular illness or condition. The 
pages may contain information, similar to that 
found in a patient leaflet; links to other sites, simi-
lar to a links page; and testimonials. In addition 
they may contain facilities such as chat rooms, 
newsletters, and email lists. Activities may include 
lobbying for funding for research, generally put up 
by individuals or non-profit organizations. For re-
liability, one may need to look at the agenda of the 
authors or group. It may be in their interest (politi-
cally) to overstate the problem or make things out 
to be worse then they are to secure increased fund-
ing or sympathy for their cause. 
 
Not Relevant (N) 
These pages are blank or not relevant and include: 
login pages, conditions of use pages, and medical 
journal front pages. 
2.3 Anotation
Study 
In order to get started with system development, a 
single annotator, M, who was involved with devel-
opment of both the classifications and the system, 
tagged the IBS70 and MED100. Then in Spring 
2008 two senior undergraduate science majors 
(chemistry and biology), L and E, were hired for 
the annotation study. The annotation study con-
sisted to two primary phases: training and testing. 
Each phase is described below. 
 
 
41
2.3.1 Training
Phase 
The two student anotators, L and E, received cop-
ies of the draft annotation instructions. They each 
met individualy with M to discus the instructions 
and any questions they had.  
For each of thre training runs, ten randomly 
chosen web pages from the IBS70 corpus were 
posted on a private web site. The students anno-
tated the pages for reliability and type and then met 
individually to discuss their annotations with M. 
As questions and isues arose, the instructions 
were amended to reflect clarifications. For exam-
ple, L needed additional instructions on the distinc-
tion betwen Link and Patient Leaflet pages; a 
separate category for FAQs was collapsed into the 
Patient Leaflet category. 
2.3.2 Testing
Phase 
Once the student anotators semed to be achiev-
ing reasonable levels of agrement (Cohen’s 
Kapa above 0.4) on each task, there was a thre-
part testing phase. The remaining 40 pages in the 
IBS70 corpus were randomly divided into two test 
corpora and finally the MED10 corpus was an-
notated. 
During the testing phase, one of the students, L, 
seemed to annotate less carefully. (Possibly be-
cause the timing coincided with graduation and 
summer vacation.) For example, on the MED100 
corpus L tagged 30% as N (unable to determine the 
reliability, compared to 12% for E and 10% for M. 
L was asked to go back and reconsider the web 
pages tagged as N. We report results with L’s re-
considered tags here for completeness, but further 
discussion will focus on agrement between M and 
E. 
2.4 Testing
and Analysis 
We report inter-rater agreement using acuracy, 
Cohen’s Kapa statistic (Cohen, 1960) for chance 
corrected agreement and F-Measure (Hripcsak and 
Rothschild, 205). We consider each annotation 
axis separately. 
2.4.1 Page
Reliability 
We can estimate a baseline distribution of the cate-
gories R (reliable), N (unable to determine), and U 
(unreliable) based on an average of the tags acros 
all training and test sets to be approximately: 68% 
R; 13% N; 19% U. 
Table 1 shows the results for the Accuracy (per-
cent agreement) and Kappa statistic on the five 
reliability classes acros all the corpora. It became 
immediately clear the annotators were not able to 
make the more fine-grained distinctions betwen 
“probably” and “posibly” for either the reliable or 
unreliable classes, given the current instructions 
and timeline. The classes were then collapsed to 
thre: R (reliable), N (unable to determine) and U 
(unreliable) and the results are shown in Table 2. 
 
Accuracy/ 
Kapa 
5 Clases
Reliability  
Set\Raters M-E M-L E-L 
IBS train 0.47/0.30 0.33/0.12 0.40/0.19 
IBS test 0.33/0.1 0.40/0.25 0.43/0.28 
MMed10 0.51/0.32 0.35/0.12 0.38/0.14 
Table 1. Inter-rater agreement for 5-class reliability. 
 
Accuracy/ 
Kapa 
 3 Clases Reliability 
Set\Raters M-E M-L E-L 
IBS train 0.70/0.4 0.60/0.25 0.67/0.3 
IBS test 0.70/0.43 0.65/0.42 0.75/0.59 
MMed10 0.77/0.49 0.66/0.30 0.62/0.2 
Table 2. Inter-rater agreement for 3-class reliability. 
 
The results in Table 2 for M-E show improved 
agreement after training and consistent moderate 
agreement on the test corpora based on the Kappa 
statistic. Acuracy (percent agreement) for M-E is 
70% for both IBS testing and training and 77% for 
the MED100.  
Further analysis of L’s reliability tags showed a 
bias toward the  “U” tag. For example, in the 
MMED10 corpus, L taged 28% as U, compared 
to 19% and 17% for M and E, respectively. 
Hripcsak and Rothschild (205) sugest use of 
the F-measure (harmonic average of precision – 
equivalent to positive predictive value and recal 
– equivalent to sensitivity comonly used in In-
formation Retrieval) to calculate inter-rater agree-
ment in the absence of a gold standard. In Table 3 
we report the average F-measure betwen each pair 
of raters and the F-measure by clas. A higher F-
measure indicates beter agrement, so these re-
sults show that the “Can’t Tell” class is the most 
42
difficult to agre on, followed by the “Unreliable” 
class. 
 
MMED10 
F-Measure 
 3 Clases Reliability 
Class\Raters M-E M-L E-L 
Reliable 0.87 0.78 0.76 
Can’t Tel 0.45 0.22 0.30 
Unreliable 0.55 0.46 0.36 
Average 0.62 0.49 0.47 
Table 3. F-measure by clas for 3-class reliability. 
 
In order to lok for patterns of agreement be-
twen the raters we looked at agrement by query 
in the MED100 corpus. In Table 4 we show the 
agreement for M and E by query. Although it ap-
pears that some queries were easier to annotate 
than others, since there are only 10 pages per 
query, the sample may be too small to draw defi-
nite conclusions. 
 
Query Accuracy Kapa 
Endometriosis 1 1 
Pancreatic Cancer 1 1 
Late Lyme 1 1 
Adrenoleukodystrophy 0.8 0.412 
Obesity 0.8 0.655 
Alzheimer’s 0.7 -0.154 
Fibromyalgia 0.7 0.444 
Lower Back Pain 0.7 -0.154 
Coloidal Silver 0.6 0.13 
Irritable Bowel Syn-
drome 
0.4 -0.053 
Table 4. Inter-rater reliability agreement for M-E by 
query. 
 
Posible ways to improve these results are pre-
sented in the “Discussion” section. 
2.4.2 Page
Type 
The dominant page types are P (patient leaflets), L 
(link), C (comercial) and MA (medical article). 
The baseline distribution based on averages acros 
the training and test sets is approximately: 39% P; 
15% L; 18% C; and 13% MA. The other thre 
classes S (suport), T (testimonial), and N (unable 
to determine) making up only 15% of the pages in 
the corpus. 
Table 5 shows the results for Accuracy and the 
Kapa statistic on the seven type classes across all 
the corpora. Collapsing categories for the type an-
notation task did not appreciably increase Kappa 
scores (M-E Kapa was 0.742 on the MED100 
corpus when the P and MA classes were col-
lapsed), so it seems preferable to keep the original 
classes. 
 
Accuracy/ 
Kapa 
 Type  
Set\Raters M-E M-L E-L 
IBS train 0.57/0.42 0.83/0.78 0.47/0.28 
IBS test 0.73/0.64 0.65/0.5 0.73/0.64 
MMed10 0.81/0.73 0.48/0.29 0.50/0.31 
Table 5. Inter-rater agreement for type anotation. 
Again we se with anotators M and E, the im-
proved agrement from training to testing, as dis-
tinctions betwen classes were clarified (for 
example, between Link and Patient Leaflets, and 
betwen Patient Leaflets and Medical Articles). 
We also computed F-measure by type for the 
MMED10 corpus, as shown in Table 6. Of the 
thre most common types of pages (Patient Leaflet, 
Link, Comercial), the Link type was the most 
difficult for M-E to agre on.  
 
MMED10 
F-Measure 
 Type 
Clas\Raters M-E M-L E-L 
P 0.893 0.593 0.625 
L 0.625 0.480 0.435 
C 0.727 0.323 0.414 
S 0.769 0.222 0.250 
T 0.500 0.000 0.800 
MA 0.667 0.593 0.455 
N 0.857 0.143 0.118 
Average 0.720 0.336 0.442 
Table 6. F-measure by clas for page type. 
 
We further analyzed the page type anotations 
by query for raters M and E (Table 7). We found a 
negative correlation betwen the variance of the 
types in a query to the Kappa statistic of agrement 
for the query (r
2
 = -0.62). 
 
 
 
 
 
43
Query Accuracy Kapa 
Endometriosis 0.9 0.851 
Fibromyalgia 0.9 0.846 
Alzheimer’s 0.8 0.75 
Irritable Bowel Syn-
drome 
0.8 0.73 
Obesity 0.8 0.697 
Pancreatic Cancer 0.8 0.63 
Coloidal Silver 0.8 0.63 
Adrenoleukodystrophy 0.8 0.512 
Lower Back Pain 0.8 0.512 
Late Lyme 0.7 0.483 
Table 7. Inter-rater type agreement for M-E by query. 
3 Discusion

Librarians, scholars, and information scientists 
have done significant work on the quality (reliabil-
ity) of print, and more recently, web information 
(for example, see Cok 201, Alexander and Tate 
1999). It is important to distinguish quality (reli-
ability) from credibility (e.g. Danielson 205), 
which is based on the users view of the informa-
tion. Here we are interested in the quality of the 
information itself. 
In a relatively early study, Impicciatore et al. 
(197) sampled web documents relating to fever in 
children and found the quality of the information 
provided to be very low. In 202, Eysenbach et al. 
conducted a review of studies assessing the quality 
of consumer health information on the web. Of the 
79 studies meting their inclusion criteria (essen-
tially appropriate scope and quantitative analysis), 
they found that 70% of the studies concluded that 
reliability of medical information on the Web is a 
problem. 
To adres the question of how to determine the 
quality of medical information on the web, Fallis 
and Frické (202) empirically tested several pro-
posed indicators and found that the standard indi-
cators of quality for print media could not be 
directly translated to consumer medical informa-
tion on the Web. Price and Hersh (1999) developed 
a semi-automated system to filter out low quality 
consumer medical web pages based on approxi-
mately 30 criteria. 
Annotation studies have ben discused and 
conducted in the computational linguistics com-
munity for a variety of anotation tasks, including 
subjectivity (e.g. Weibe et al. 1999) and opinion 
(e.g. Somasundaran et al. 208). Artstein and Poe-
sio (2008) surveyed inter-coder agreement in com-
putational linguistics, including Cohen’s Kappa. 
To ensure a “gold standard” for training ma-
chine learning algorithms to do automatic classifi-
cation a number of approaches could be pursued: 
the production of bias-corrected tags as described 
by Weibe et al. (1999); a new study with “expert” 
annotators – having a stronger medical background 
– and aditional training; ask anotators to use ex-
isting web tools (e.g. American Accreditation 
HealthCare Comision) to ases the page qual-
ity; systematically assess whether the noise intro-
duced by moderate agreement levels wil create 
problems for machine learning with this data 
(Beigman Klebanov and Beigman 2009). 
The agrement on the type anotation task could 
still be improved, possibly by additional clarifica-
tion to the definitions. However, it is still to be de-
termined if noise levels are low enough and 
sufficiently random to be used successfully in su-
pervised learning. This task is easier than the reli-
ability task and requires less expertise of the 
annotators. 
4 Conclusion
 
There is a demonstrated ned to provide tols to 
health care consumers to automatically filter web 
pages by the reliability, quality, or trustworthiness 
of the medical information the pages contain. We 
have shown promising results in this study that 
appropriate classes of pages can be developed. 
These clases can be used by human anotators to 
annotate web pages with reasonable to god 
agreement. 
Thus we have laid a foundation for future ano-
tation studies to create a gold standard data set of 
consumer medical web pages. The corpora in this 
study are currently being used to create an auto-
mated system to estimate the reliability of medical 
web pages. 
Acknowledgments 
This work was suported in part by a CSU Stanis-
laus Naraghi Faculty Research Enhancement 
Grant. I am grateful to Elizabeth Jimenez and Luis 
Adalco for participating in the anotation study 
and to the anonymous reviews for their coments 
and sugestions. I would also like to thank Roger 
Hartley my disertation advisor and Peter Foltz for 
discussions during the formulation and develop-
44
ment of the system, and Tom Carter for helpful and 
insightful comments leading to the improvement of 
this paper. 
References  
Janet E. Alexander and Marsha An Tate. 199. Web 
Wisdom: How to Evaluate and Create Information 
Quality on the Web. Lawrence Erlbaum and Asoci-
ates, New Jersey. 
American Acreditation HealthCare Comision. 
Health information on the internet:  A checklist to 
help you judge which websites to trust. Retrieved 
February 28, 2010, from http:/ww.urac.org 
R. Artstein and M. Poesio. 208. Inter-coder agrement 
for computational linguistics. Comput. Linguist. 34, 4 
(Dec. 208), 55-596. 
B. Beigman Klebanov, and E. Beigman. 209. From 
annotator agreement to noise models. Comput. Lin-
guist. 35, 4 (Dec. 2009), 495-503. 
J. Cohen. 1960. A coeficient of agrement for nominal 
scales. Educational and Psychological Measurement, 
20, pages 34-46. 
Alison Coke. 201. A Guide to Finding Quality Infor-
mation on the Internet: Selection and Evaluation 
Strategies, Second Edition. Library Asociation Pub-
lishing, London. 
D.R. Danielson. 205. Web credibility. C. Ghaoui (Ed.), 
Encyclopedia of Human-Computer Interaction. Her-
shey, PA: Idea Group, 713-721. 
Gunther Eysenbach, John Powel, Oliver Kus, and 
Eun-Ryoung Sa. 202. Empirical Studies Asesing 
the Quality of Health Information for Consumers on 
the World Wide Web: A Systematic Review. JAMA, 
May 2, 202; 287(20): 2691 270.  
Don Falis and Martin Frické. 202. Indicators of Accu-
racy of Consumer Health Information on the Internet. 
Journal of the American Medical Informatics Aso-
ciation, 9, 1, (202): 73-79. 
George Hripcsak and Adam S. Rothschild. 205. 
Agrement, the F-Measure, and Reliability in Infor-
mation Retrieval. J Am Med Inform Asoc. 205 
May–Jun; 12(3): 296–298. 
Piero Impiciatore, Chiara Pandolfini, Nicola Casela, 
and Maurizio Bonati. 197. Reliability of Health In-
formation for the Public on the World Wide Web: 
Systematic Survey of Advice on Managing Fever in 
Children at Home. BMJ 197; 314:1875 (28 June). 
Melanie J. Martin. 204. Reliability and Verification of 
Natural Language Text on the World Wide Web. Pa-
per at ACM-SIGIR Doctoral Consortium, July 25, 
2004, Shefield, England. 
Susan L. Price and Wiliam R. Hersh. 199. Filtering 
Web Pages for Quality Indicators: An Empirical Ap-
proach to Finding High Quality Consumer Health In-
formation. American Medical Informatics 
Association 199. 
Swapna Somasundaran, Josef Rupenhofer and Janyce 
Wiebe. 208. Discourse Level Opinion Relations: An 
Annotation Study. Procedings of the 9th SIGdial 
Workshop on Discourse and Dialogue Columbus, 
Ohio, June 19-20, 2008, pp. 129–137. 
Janyce M. Wiebe, Rebeca F. Bruce, and Thomas P. 
O'Hara. 199. Development and use of a gold-
standard data set for subjectivity classifications. In 
Proceedings of the 37th Anual Meeting of the Aso-
ciation For Computational Linguistics on Computa-
tional Linguistics (Colege Park, Maryland, June 20 
26, 1999). Annual Meting of the ACL. Asociation 
for Computational Linguistics, Moristown, NJ, 246-
253. 
45

