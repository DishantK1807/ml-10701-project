1:226	Unsupervised Semantic Role Labelling Robert S. Swier and Suzanne Stevenson Department of Computer Science University of Toronto Toronto, Ontario, Canada M5S 3G4 a0 swier,suzanne a1 @cs.toronto.edu Abstract We present an unsupervised method for labelling the arguments of verbs with their semantic roles.
2:226	Our bootstrapping algorithm makes initial unambiguous role assignments, and then iteratively updates the probability model on which future assignments are based.
3:226	A novel aspect of our approach is the use of verb, slot, and noun class information as the basis for backing off in our probability model.
4:226	We achieve 5065% reduction in the error rate over an informed baseline, indicating the potential of our approach for a task that has heretofore relied on large amounts of manually generated training data.
5:226	1 Introduction Semantic annotation of text corpora is needed to support tasks such as information extraction and question-answering (e.g. , Riloff and Schmelzenbach, 1998; Niu and Hirst, 2004).
6:226	In particular, labelling the semantic roles of the arguments of a verb (or any predicate), as in (1) and (2), provides crucial information about the relations among event participants.
7:226	1.
8:226	Kivaa2a4a3a6a5a8a7a10a9a12a11a14a13a4a9a12a15a17a16a18a9a19a11a8a20 admires Matsa2a4a21a23a22a25a24a27a26a19a9a14a20 2.
9:226	Joa2a29a28a31a30a25a9a12a15a27a32a33a20 returned to Londona2a4a34a35a9a14a26a18a32a36a13a4a15a17a22a8a32a36a13a36a37a38a15a17a20 Because of the importance of this task, a number of recent methods have been proposed for automatic semantic role labelling (e.g. , Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Fleischman et al. , 2003; Hacioglu et al. , 2003; Thompson et al. , 2003).
10:226	These supervised methods are limited by their reliance on the manually roletagged corpora of FrameNet (Baker et al. , 1998) or PropBank (Palmer et al. , 2003) as training data, which are expensive to produce, are limited in size, and may not be representative.
11:226	We have developed a novel method of unsupervised semantic role labelling that avoids the need for expensive manual labelling of text, and enables the use of a large, representative corpus.
12:226	To achieve this, we take a bootstrapping approach (e.g. , Hindle and Rooth, 1993; Yarowsky, 1995; Jones et al. , 1999), which initially makes only the role assignments that are unambiguous according to a verb lexicon.
13:226	We then iteratively: create a probability model based on the currently annotated semantic roles, use this probability model to assign roles that are deemed to have sufficient evidence, and add the newly labelled arguments to our annotated set.
14:226	As we iterate, we gradually both grow the size of the annotated set, and relax the evidence thresholds for the probability model, until all arguments have been assigned roles.
15:226	To our knowledge, this is the first unsupervised semantic role labelling system applied to general semantic roles in a domain-general corpus.
16:226	In a similar vein of work, Riloff and colleagues (Riloff and Schmelzenbach, 1998; Jones et al. , 1999) used bootstrapping to learn case frames for verbs, but their approach has been applied in very narrow topic domains with topic-specific roles.
17:226	In other work, Gildea (2002) has explored unsupervised methods to discover role-slot mappings for verbs, but not to apply this knowledge to label text with roles.
18:226	Our approach also differs from earlier work in its novel use of classes of information in backing off to less specific role probabilities (in contrast to using simple subsets of information, as in Gildea and Jurafsky, 2002).
19:226	If warranted, we base our decisions on the probability of a role given the verb, the syntactic slot (syntactic argument position), and the noun occurring in that slot.
20:226	For example, the assignment to the first argument of sentence (1) above may be based on a39a41a40 Experiencera42 a43a45a44a47a46a49a48a18a50a52a51a54a53 subjecta53a8a55a56a48a18a57a45a43a59a58.
21:226	When backing off from this probability, we use statistics over more general classes of information, such as conditioning over the semantic class of the verb instead of the verb itselffor this example, psychological state verbs.
22:226	Our approach yields a very simple probability model which emphasizes classbased generalizations.
23:226	The first step in our algorithm is to use the verb lexicon to determine the argument slots and the roles available for them.
24:226	In Section 2, we discuss the lexicon we use, and our initial steps of syntactic frame matching and unambiguous role assignment.
25:226	This unambiguous data is leveraged by using those role assignments as the basis for the initial estimates for the probability model described in Section 3.
26:226	Section 4 presents the algorithm which brings these two components together, iteratively updating the probability estimates as more and more data is labelled.
27:226	In Section 5, we describe details of the materials and methods used for the experiments presented in Section 6.
28:226	Our results show a large improvement over an informed baseline.
29:226	This kind of unsupervised approach to role labelling is quite new, and we conclude with a discussion of limitations and on-going work in Section 7.
30:226	2 Determining Slots and Role Sets Previous work has divided the semantic role labelling task into the identification of the arguments to be labelled, and the tagging of each argument with a role (Gildea and Jurafsky, 2002; Fleischman et al. , 2003).
31:226	Our algorithm addresses both these steps.
32:226	Also, the unsupervised nature of the approach highlights an intermediate step of determining the set of possible roles for each argument.
33:226	Because we need to constrain the role set as much as possible, and cannot draw on extensive training data, this latter step takes on greater significance in our work.
34:226	We first describe the lexicon that specifies the syntactic arguments and possible roles for the verbs, and then discuss our process of argument and role set identification.
35:226	2.1 The Verb Lexicon In semantic role labelling, a lexicon is used which lists the possible roles for each syntactic argument of each predicate.
36:226	Supervised approaches to this task have thus far used the predicate lexicon of FrameNet, or the verb lexicon of PropBank, since each has an associated labelled corpus for training.
37:226	We instead make use of VerbNet (Kipper et al. , 2000), a manually developed hierarchical verb lexicon based on the verb classification of Levin (1993).
38:226	For each of 191 verb classes, including around 3000 verbs in total, VerbNet specifies the syntactic frames along with the semantic role assigned to each slot of a frame.
39:226	Throughout the paper we use the term frame to refer to a syntactic framethe set of syntactic arguments of a verbpossibly labelled with roles, as exemplified in the VerbNet entry in Table 1.
40:226	While FrameNet uses semantic roles specific to a particular situation (such as Speaker, Message, admire Frames: Experiencer V Cause Experiencer V Cause Prep(in) Oblique Experiencer V Oblique Prep(for) Cause Verbs in same (sub)class: [admire, adore, appreciate, cherish, enjoy, ] Table 1: A portion of a VerbNet entry.
41:226	Addressee), and PropBank uses roles specific to a verb (such as Arg0, Arg1, Arg2), VerbNet uses an intermediate level of thematic roles (such as Agent, Theme, Recipient).
42:226	These general thematic roles are commonly assumed in linguistic theory, and have some advantages in terms of capturing commonalities of argument relations across a wide range of predicates.
43:226	It is worth noting that although there are fewer of these thematic roles than the more situation-specific roles of FrameNet, the role labelling task is not necessarily easier: there may be more data per role, but possibly less discriminating data, since each role applies to more general relations.
44:226	(Indeed, in comparing the use of FrameNet roles to general thematic roles, Gildea and Jurafsky (2002) found very little difference in performance.)
45:226	2.2 Frame Matching We devise a frame matching procedure that uses the verb lexicon to determine, for each instance of a verb, the argument slots and their possible thematic roles.
46:226	The potential argument slots are subject, object, indirect object, and PP-object, where the latter is specialized by the individual preposition.1 Given chunked sentences with our verbs, the frame matcher uses VerbNet both to restrict the list of candidate roles for each slot, and to eliminate some of the PP slots that are likely not arguments.
47:226	To initialize the candidate roles precisely, we only choose roles from frames in the verbs lexical entry (cf.
48:226	Table 1) that are the best syntactic matches with the chunker output.
49:226	We align the slots of each frame with the chunked slots, and compute the portion %Frame of frame slots that can be mapped to a chunked slot, and the portion %Chunks of chunked slots that can be mapped to the frame.
50:226	The score for each frame is computed by %Framea0 %Chunks, and only frames having the highest score contribute candidate roles to the chunked slots.
51:226	An example 1As in VerbNet, we assume that when a verb takes a PP argument, the slot receiving the thematic role from the verb is the NP object of the preposition.
52:226	Also, VerbNet has few verbs that take sentence complements, and for now we do not consider them.
53:226	Extracted Slots Possible Frames for V SUBJ OBJ POBJ %Frame %Chunks Score Agent V Agent 100 33 133 Agent V Theme Agent Theme 100 67 167 Instrument V Theme Instrument Theme 100 67 167 Agent V Theme P Instrument Agent Theme Instrument 100 100 200 Agent V Recipient Theme Agent Recipient 67 67 133 Table 2: An example of frame matching.
54:226	scoring is shown in Table 2.
55:226	This frame matching step is very restrictive and greatly reduces potential role ambiguity.
56:226	Many syntactic slots receive only a single candidate role, providing the initial unambiguous data for our bootstrapping algorithm.
57:226	Some slots receive no candidate roles, which is an error for argument slots but which is correct for adjuncts.
58:226	The reduction of candidate roles in general is very helpful in lightening the load on the probability model, but note that it may also cause the correct role to be omitted.
59:226	In future work, we plan to explore other possible methods of selecting roles from the frames, such as choosing candidates from all frames, or setting a threshold value on the matching score.
60:226	3 The Probability Model Once slots are initialized as above, our algorithm uses an iteratively updated probability model for role labelling.
61:226	The probability model predicts the role for a slot given certain conditioning information.
62:226	We use a backoff approach with three levels of specificity of probabilities.
63:226	If a candidate role fails to meet the threshold of evidence (counts towards that probability) for a given level, we backoff to the next level.
64:226	For any given slot, we use the most specific level that reaches the evidence threshold for any of the candidates.
65:226	We only use information at a single level to compare candidates for a single slot.
66:226	We assume the probability of a role for a slot is independent of other slots; we do not ensure a consistent role assignment across an instance of a verb.
67:226	3.1 The Backoff Levels Our most specific probability uses the exact combination of verb, slot, and noun filling that slot, yielding a39 a40 a50 a42a57a31a53a1a0a52a53a3a2 a58 .2 2We use only the head noun of potential arguments, not the full NP, in our probability model.
68:226	Our combination of slot plus head word provides similar information (head of argument and its syntactic relation to the verb) to that captured by the features of Gildea and Jurafsky (2002) or Thompson et al.69:226	(2003).
70:226	For our first backoff level, we introduce a novel way to generalize over the verb, slot, and noun information of a39 a40 a50 a42a57a31a53a1a0a52a53a3a2 a58 . Here we use a linear interpolation of three probabilities, each of which: (1) drops one source of conditioning information from the most specific probability, and (2) generalizes a second source of conditioning information to a class-based conditioning event.
71:226	Specifically, we use the following probability formula: a4a6a5 a39 a5 a40 a50 a42a57 a53a1a0a8a7a10a58 a0 a4a10a9 a39 a9 a40 a50 a42a57a31a53a3a2a11a7 a58 a0 a4a13a12 a39 a12 a40 a50 a42a57a14a7 a53a1a0 a58 where a0a15a7 is slot class, a2a11a7 is noun class, a57a16a7 is verb class, and the individual probabilities are (currently) equally weighted (i.e. , all a4 a13 s have a value of a17a15a18a20a19 ).
72:226	Note that all three component probabilities make use of the verb or its class information.
73:226	In a39 a5, the noun component is dropped, and the slot is generalized to the appropriate slot class.
74:226	In a39 a9, the slot component is dropped, and the noun is generalized to the appropriate noun class.
75:226	Although it may seem counterintuitive to drop the slot, this helps us capture generalizations over alternations, in which the same semantic argument may appear in different syntactic slots (as in The ice melted and The sun melted the ice).
76:226	In a39 a12, again the noun component is dropped, but in this case the verb is generalized to the appropriate verb class.
77:226	Each type of class is described in the following subsection.
78:226	The last backoff level simply uses the probability of the role given the slot class, a39a41a40 a50 a42 a0a8a7a10a58 . The backoff model is summarized in Figure 1.
79:226	We use maximum likelihood estimates (MLE) for each of the probability formulas.
80:226	3.2 Classes of Information For slots, true generalization to a class only occurs for the prepositional slots, all of which are mapped to a single PP slot class.
81:226	All other slots subject, object, and indirect objecteach form their own singleton slot class.
82:226	Thus, a39 a5 differs from a39a41a40 a50 a42a57 a53a1a0a47a53a3a2 a58 by dropping the noun, and by treating all prepositional slots as the same slot.
83:226	This formula allows us to generalize over a slot regardless of the + + a0a2a1 a11a4a3a26a19a16a6a5 a7a9a8 a0 a8 a1 a11a4a3a10a4a11 a26a19a16a6a5 a7a13a12 a0 a12 a1 a11a4a3a10a25a16a14a11 a26a15a5 a7a17a16 a0 a16 a1 a11a4a3a10a4a11 a15 a16a6a5 a0a2a1 a11a4a3a10a4a11 a26a18a11a33a15a19a5 Figure 1: The backoff model.
84:226	particular noun, and preposition if there is one, used in the instance.
85:226	Classes of nouns in the model are given by the WordNet hierarchy.
86:226	Determining the appropriate level of generalization for a noun is an open problem (e.g. , Clark and Weir, 2002).
87:226	Currently, we use a cut through WordNet including all the top categories, except for the category entity; the latter, because of its generality, is replaced in the cut by its immediate children (Schulte im Walde, 2003).
88:226	Given a noun argument, all of its ancestors that appear in this cut are used as the class(es) for the noun.
89:226	(Credit for a noun is apportioned equally across multiple classes).
90:226	Unknown words placed in a separate category.
91:226	This yields a noun classification system that is very coarse and that does not distinguish between senses, but which is simple and computationally feasible.
92:226	a39 a9 thus captures consistent relations between a verb and a class of nouns, regardless of the slot in which the noun occurs.
93:226	Verb classes have been shown to be very important in capturing generalizations across verb behaviour in computational systems (e.g. , Palmer, 2000; Merlo and Stevenson, 2001).
94:226	In semantic role labelling using VerbNet, they are particularly relevant since the classes are based on a commonality of role-labelled syntactic frames (Kipper et al. , 2000).
95:226	The class of a verb in our model is its VerbNet class that is compatible with the current frame.
96:226	When multiple classes are compatible, we apportion the counts uniformly among them.
97:226	For probability a39 a12, then, we generalize over all verbs in a class of the target verb, giving us much more extensive data over relevant role assignments to a particular slot.
98:226	4 The Bootstrapping Algorithm We have described the frame matcher that produces a set of slots with candidate role lists (some unambiguous), and our backoff probability model.
99:226	All that remains is to specify the parameters that guide the iterative use of the probability model to assign roles.
100:226	The evidence count for each of the conditional probabilities refers to the number of times we have observed the conjunction of its conditioning events.
101:226	For example, for a39 a40 a50 a42a57a31a53a1a0a52a53a3a2 a58, this is the number of times the particular combination of verb, slot, and noun have been observed.
102:226	For a probability to be used, its evidence count must reach a given threshold, a46a49a48 a2a21a20 a57a47a48a19a44 a51 a2a11a7 a51 . The goodness of a role assignment is determined by taking the log of the ratio between the probabilities of the top two candidates for a slot (when the evidence of both meet a46a49a48 a2a21a20 a57a45a48a19a44a45a51 a2a11a7 a51 ) (e.g. , Hindle and Rooth, 1993).
103:226	A role is only assigned if the log likelihood ratio is defined and meets a threshold; in this case, the candidate role with highest probability is assigned to the slot.
104:226	(Note that in the current implementation, we do not allow re-labelling: an assigned label is fixed).
105:226	In the algorithm, the log ratio threshold is initially set high and gradually reduced until it reaches 0.
106:226	In the case of remaining ties, we assign the role for which a39a41a40 a50 a42 a0a8a7 a58 is highest.
107:226	Because our evidence count and log ratio restrictions may not be met even when we have a very good candidate for a slot, we reduce the evidence count threshold to the minimum value of 1 when the log ratio threshold reaches 1.3 By this point, we assume competitor candidates have been given sufficient opportunity to amass the relevant counts.
108:226	Algorithm 1 shows the bootstrapping algorithm.
109:226	Algorithm 1 Bootstrapping Algorithm Frame Matching, Slot Initialization: 1: Perform Frame Matching to determine the slots to be labelled, along with their candidate lists of roles.
110:226	2: Let a22 be the set of annotated slots; a22a24a23a24a25 . Let a26 be the set of unannotated slots, initially all slots.
111:226	Let a27 be the set of newly annotated slots; a27a28a23a29a25 . 3: Add to a27 each slot whose role assignment is unambiguouswhose candidate list has one element.
112:226	Set a26 to a26a31a30a24a27 and set a22 to a22a33a32a33a27 (where a30 and a32 remove/add elements of the second set from/to the first).
113:226	Probability Model Application: repeat repeat (Re)compute the probability model, using counts over the items in a22 . Add to a27 all slots in a26 for which: at least two candidates meet the evidence count threshold for a given probability level (see Figure 1); and the log ratio between the two highest probability candidates meets the log ratio threshold.
114:226	For each slot in a27, assign the highest probability role.
115:226	Set a26 to a26a34a30a35a27 and set a22 to a22a36a32a37a27 . until a27a38a23a24a25 Decrement the log ratio threshold.
116:226	Adjust evidence count threshold if log ratio threshold is 1.
117:226	until log ratio threshold = 0 Resolve ties and terminate.
118:226	3We also allow cases in which the log ratio is undefined to be assigned at this pointthis occurs when only one of multiple candidates has evidence.
119:226	5 Materials and Methods 5.1 Verbs, Verb Classes and Roles For the initial set of experiments, we chose 54 target verbs from three top-level VerbNet classes: preparing-26.3, transfer mesg-37.1, and contribute13.2.
120:226	We looked for classes that contained a large number of medium to high frequency verbs displaying a variety of interesting properties, such as having ambiguous (or unambiguous) semantic roles given certain syntactic constructions, or having ambiguous semantic role assignments that could (or alternatively, could not) be distinguished by knowledge of verb class.
121:226	From the set of target verbs, we derived an extended verb set that comprises all of the original target verbs as well as any verb that shares a class with one of those target verbs.
122:226	This gives us a set of 1159 verbs to observe in total, and increases the likelihood that some verb class information is available for each of the possible classes of the target verbs.
123:226	Observing the entire extended set also provides more data for our probability estimators that do not use verb class information.
124:226	We have made several changes to the semantic roles as given by VerbNet.
125:226	First, selectional restrictions such as [+Animate] are removed since our coarse model of noun class does not allow us to reliably determine whether such restrictions are met.
126:226	Second, a few semantic distinctions that are made in VerbNet appeared to be too fine-grained to capture, so we map these to a more coarse-grained subset of the VerbNet roles.
127:226	For instance, the role Actor is merged with Agent, and Patient with Theme.
128:226	We are left with a set of 16 roles: Agent, Amount, Attribute, Beneficiary, Cause, Destination, Experiencer, Instrument, Location, Material, Predicate, Recipient, Source, Stimulus, Theme, Time.
129:226	Of these, 13 actually occur in our target verb classes.
130:226	5.2 The Corpus and Preprocessing Our corpus consists of a random selection of 20% of the sentences in the British National Corpus (BNC Reference Guide, 2000).
131:226	This corpus is processed by the chunker of Abney (1991), from whose output we can identify the probable head words of verb arguments with some degree of error.
132:226	For instance, distant subjects are often not found, and PPs identified as arguments are often adjuncts.
133:226	To reduce the number of adjuncts, we ignore dates and any PPs that are not known to (possibly) introduce an argument to one of the verbs in our extended set.
134:226	5.3 Validation and Test Data We extracted two sets of sentences: a validation set consisting of 5 random examples of each target verb, and a test set, consisting of 10 random examples of each target verb.
135:226	The data sets were chunked as above, and the role for each potential argument slot was labelled by two human annotators, choosing from the simplified role set allowed by each verb according to VerbNet.
136:226	A slot could also be labelled as an adjunct, or as bad (incorrectly chunked).
137:226	Agreement between the two annotators was high, yielding a kappa statistic of 0.83.
138:226	After performing the labelling task individually, the annotators reconciled their responses (in consultation with a third annotator) to yield a set of human judgements used for evaluation.
139:226	5.4 Setting the Bootstrapping Parameters In our development experiments, we tried an evidence count threshold of either the mean or median over all counts of a particular conjunction of conditioning events.
140:226	(For example, for a39 a40 a50 a42a57a31a53a1a0a52a53a3a2 a58, this is the mean or median count across all combinations of verb, slot, and noun).
141:226	The more lenient median setting worked slightly better on the validation set, and was retained for our test experiments.
142:226	We also experimented with initial starting values of 2, 3, and 8 for the log likelihood ratio threshold.
143:226	An initial setting of 8 showed an improvement in performance, as lower values enabled too many early role assignments, so we used the value of 8 in our test experiments.
144:226	In all experiments, a decrement of .5 was used to gradually reduce the log likelihood ratio threshold.
145:226	6 Experimental Results Of over 960K slots we extracted from the corpus, 120K occurred with one of 54 target verbs.
146:226	Of these, our validation data consisted of 278 slots, and our test data of 554 slots.
147:226	We focus on the analysis of test data; the pattern on the validation data was nearly identical in all respects.
148:226	The target slots fall into several categories, depending on the human judgements: argument slots, adjunct slots, and bad slots (chunking errors).
149:226	We report detailed analysis over the slots identified as arguments.
150:226	We also report overall accuracy if adjunct and bad slots are included in the slots to be labelled.
151:226	This comparison is similar to that made by Gildea and Jurafsky (2002) and others, either using arguments as delimited in the FrameNet corpus, or having to automatically locate argument boundaries.4 Furthermore, we report results over individ4The comparison is not identical: in the case of manually ual slot classes (subject, object, indirect object, and PP object), as well as over all slots.
152:226	6.1 Evaluation Measures and Comparisons We report results after the unambiguous data is assigned, and at the end of the algorithm, when no more slots can be labelled.
153:226	At either of these steps it is possible for some slots to have been assigned and some to remain unassigned.
154:226	Rather than performing a simple precision/recall analysis, we report a finer grained elaboration that gives a more precise picture of the results.
155:226	For the assigned slots, we report percent correct (of total, not of assigned) and percent incorrect.
156:226	For the unassigned slots, we report percent possible (i.e. , slots whose candidate list contains the correct role) and percent impossible (i.e. , slots whose candidate list does not contain the correct roleand which may in fact be empty).
157:226	All these percent figures are out of all argument slots (for the first set of results), and out of all slots (for the second set); see Table 3.
158:226	Correctness is determined by the human judgements on the chunked slots, as reported above.
159:226	Using our notion of slot class, we compare our results to a baseline that assigns all slots the role with the highest probability for that slot class, a39a41a40 a50 a42 a0a15a7a10a58 . When using general thematic roles, this is a more informed baseline than a39a41a40 a50 a42a57 a58, as used in other work.
160:226	We are using a very different verb lexicon, corpus, and human standard than in previous research.
161:226	The closest work is that of Gildea and Jurafsky (2002) which maps FrameNet roles to a set of 18 thematic roles very similar to our roles, and also operates on a subset of the BNC (albeit manually rather than randomly selected).
162:226	We mention the performance of their method where appropriate below.
163:226	However, our results are compared to human annotation of chunked data, while theirs (and other supervised results) are compared to manually annotated full sentences.
164:226	Our percentage correct values therefore do not take into account argument constituents that are simply missed by the chunker.
165:226	6.2 Results on Argument Slots Table 3 summarizes our results.
166:226	In this section, we focus on argument slots as identified by our human judges (the first panel of results in the table).
167:226	There are a number of things to note.
168:226	First, our performance on these slots is very high, 90.1% correct at the end of the algorithm, with 7.0% incorrect, and delimited arguments, others train, as well as test, only on such arguments.
169:226	In our approach, all previously annotated slots are used in the iterative training of the probability model.
170:226	Thus, even when we report results on argument slots only, adjunct and bad slots may have induced errors in their labelling.
171:226	only 2.9% left unassigned.
172:226	(The latter have null candidate lists).
173:226	This is a 56% reduction in error rate over the baseline.
174:226	Second, we see that even after the initial unambiguous role assignment step, the algorithm achieves close to the baseline percent correct.
175:226	Furthermore, over 96% of the initially assigned roles are correct.
176:226	This means that much of the work in narrowing down the candidate lists is actually being preformed during frame matching.
177:226	It is noteworthy that such a simple method of choosing the initial candidates can be so useful, and it would seem that even supervised methods might benefit from employing such an explicit use of the lexicon to narrow down role candidates for a slot.
178:226	After unambiguous role assignment, about 21% of the test data remains unassigned (116 slots).
179:226	Of these 116 slots, 100 have a non-null candidate list.
180:226	These 100 are assigned by our iterative probability model, so we are especially interested in the results on them.
181:226	We find that 76 of these 100 are assigned correctly (accounting for the 13.7% increase to 90.1%), and 24 are assigned incorrectly, yielding a 76% accuracy for the probability model portion of our algorithm on identified argument slots.
182:226	Moreover, we also find that all specificity levels of the probability model (see Figure 1) are employed in making these decisionsabout a third of the decisions are made by each level.
183:226	This indicates that while there is sufficient data in many cases to warrant using the exact probability formula a39a41a40 a50 a42a57 a53a1a0a47a53a3a2 a58, the class-based generalizations we propose prove to be very useful to the algorithm.
184:226	As a point of comparison, the supervised method of Gildea and Jurafsky (2002) achieved 82.1% accuracy on identified arguments using general thematic roles.
185:226	However, they had a larger and more varied target set, consisting of 1462 predicates from 67 FrameNet frames (classes), which makes their task harder than ours.
186:226	We are aware that our test set is small compared to supervised approaches, which have a large amount of labelled data available.
187:226	However, our almost identical results across the validation and test sets indicates consistent behaviour that may generalize to a larger test set, at least on similar classes of verbs.
188:226	6.3 Differences Among Slot Classes When using general thematic roles with a small set of verb classes, the probability used for the baseline, a39a41a40 a50 a42 a0a15a7a10a58, works very well for subjects and objects (which are primarily Agents and Themes, respectively, for our verbs).
189:226	Indeed, when we examine each of the slot classes individually, we find that, for subjects and objects, the percent correct Identified Arguments All Target Slots Algorithm Algorithm Role Assignments Baseline Unambig Final Baseline Unambig Final Assigned Correct 77.3 76.4 90.1 63.7 75.9 87.2 Incorrect 22.7 2.7 7.0 36.3 6.8 10.4 Unassigned Possible 0 17.1 0 0 14.1 0 Impossible 0 3.8 2.9 0 3.1 2.4 Table 3: Evaluation of test data on 554 identified arguments (see Section 6.2) and on all 672 target slots (see Section 6.4).
190:226	achieved by the algorithm is indistinguishable from the baseline (both are around 93%, for both subjects and objects).
191:226	For PP objects, on the other hand, the baseline is only around 11% correct, while we achieve 78.5% correct, a 76% reduction in error rate.
192:226	Clearly, when more roles are available, even a39a41a40 a50 a42 a0a8a7 a58 becomes a weak predictor.
193:226	5 We could just assign the default role for subjects and objects when using general thematic roles, but we think this is too simplistic.
194:226	First, when we broaden our range of verb classes, subjects and objects will have more possible roles.
195:226	As we have seen with PPs, when more roles are available, the performance of a default role degrades.
196:226	Second, although we achieve the same correctness as the baseline, our algorithm does not simply assign the dominant role in these cases.
197:226	Some subjects are assigned Theme, while some objects are assigned Recipient or Source.
198:226	These roles would never be possible in these slots if a default assignment were followed.
199:226	6.4 Results Including All Target Slots We also consider our performance given frame matching and chunking errors, which can lead to adjuncts or even bad constituents being labelled.
200:226	Only arguments should be labelled, while nonarguments should remain unlabelled.
201:226	Of 98 slots judged to be adjuncts, 19 erroneously are given labels.
202:226	Including the adjunct slots, our percent correct goes from 90.1% to 88.7%.
203:226	Of the 20 bad slots, 12 were labelled.
204:226	Including these, correctness is reduced slightly further, to 87.2%, as shown in the second panel of results in Table 3.
205:226	The error rate reduction here of 65% is higher than on arguments only, because the baseline always labels (in error) adjuncts and bad slots.
206:226	(Gildea and Jurafsky (2002) achieved 63.6% accuracy when having to identify arguments for thematic roles, though note again that this is on a much larger and more 5Due to the rarity of indirect object slots in the chunker output, the test data included no such slots.
207:226	The validation set included one, which the algorithm correctly labelled.
208:226	general test set.
209:226	Also, although we take into account errors on identified chunks that are not arguments, we are are not counting chunker errors of missing arguments.)
210:226	As others have shown (Gildea and Palmer, 2002), semantic role labelling is more accurate with better preprocessing of the data.
211:226	However, we also think our algorithm may be extendable to deal with many of the adjunct cases we observed.
212:226	Often, adjuncts express time or location; while not argument roles, these do express generalizable semantic relations.
213:226	In future work, we plan to explore the notion of expanding our frame matching step to go beyond VerbNet by initializing potential adjuncts with appropriate roles.
214:226	7 Conclusions and Future Work Using an unsupervised algorithm for semantic role labelling, we have achieved 90% correct on identified arguments, well over an informed baseline of 77%, and have achieved 87% correct on all slots (64% baseline).
215:226	On PP objects, our conservative role assignment shows promise at leaving adjuncts unlabelled.
216:226	However, PP objects also have the lowest performance (of 78% correct on identified arguments, compared to 93% for subjects or objects).
217:226	More work is required on our frame matching approach to determine appropriate roles for PP objects given the specification in the lexicon, which (in the case of VerbNet) often over-constrains the allowable prepositions for a slot.
218:226	Although these results are promising, they are only a first step in demonstrating the potential of the approach.
219:226	We need to test more verbs, from a wider variety of verb classes (or even a different kind of predicate classification, such as FrameNet), to determine the generalizability of our findings.
220:226	Using FrameNet would also have the advantage of providing large amounts of labelled test data for our evaluation.
221:226	We also hope to integrate some processing of adjunct roles, rather than limiting ourselves to the specified arguments.
222:226	A unique aspect of our method is the probability model, which is novel in its generalizations over verb, slot, and noun classes for role labelling.
223:226	However, these have room for improvementour noun classes are coarse, and prepositions clearly have the potential to be divided into more informative subclasses, such as spatial or time relations.
224:226	Our ongoing work is investigating better class models to make the backoff process even more effective.
225:226	Acknowledgments We gratefully acknowledge the support of NSERC of Canada.
226:226	We also thank Martha Palmer for providing us with the VerbNet data, Eric Joanis for help with the chunker, Vivian Tsang and Ryan North for helpful discussion, and two anonymous reviewers.

