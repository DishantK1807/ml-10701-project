A Stochastic Language Model using Dependency 
and Its Improvement by Word Clustering 
Shinsuke Mori * 
Tokyo Research Labolatory, 
IBM Japan~ Ltd. 
1623-14 Shimotsuruma 
Yamatoshi, Japan 
Makoto Nagao 
Kyoto University 
Yoshida-honmachi Sakyo 
Kyoto, Japan 
Abstract 
In this paper, we present a stochastic language 
model for Japanese using dependency. The predic
tion unit in this model is au attribute of"bunsetsu'. 
This is represented by the product of the head of con
tent words and that of function words. TILe relation 
between the attributes of "bunsetsu" is ruled by a 
context-free grammar. The word sequences axe pre
dicted from the attribute using word n-gram model. 
The spell of Unknow word is predicted using charac
ter n-grain model. This model is robust in that it can 
compute the probability of art arbitrary string aild 
is complete in that it models from unknown word to 
dependency at tile saine time. 
1 Introduction

An effectiveness of stochastic lailguage modeling as 
a methodology of naturM language processing has 
been attested by various applications to tile recog
nition system such as speech recognition and to the 
analysis system such as part-of-speech (POS) tagger. 
In this methodology a stochastic lailguage model 
with some paraineters is built and they are estimated 
in order to maximize its prediction power (minimize 
the cross entropy) on an unknown input. Consid
ering a single application, it might be better to es
timate the parameters taking account of expected 
accuracy of recognition or analysis. This method is, 
however, heavily dependent on the problem azld of
fers no systematic solution, as far as we know. TILe 
methodology of stochastic language modeling, how
ever, allows us to separate, from various fraineworks 
of natural larlguage processing, the language descrip
tion model common to them and enables us a sys
tematic improvement of each application. 
In this framework a description on a language is 
represented as a map from a sequence of alphabetic 
characters to a probability value. The first model 
is C. E. Shannon's n-gram model (Shannon, 1951). 
The parameters of the model &re estimated from the 
frequency of n character sequences of the alphabet 
(n-gram) on a corpus containing a large number of 
sentences of a language. This is the same model as 
0 This
work is done when the auther was at Kyoto Univ. 
used in almost all of the recent practical applications 
in that it describes only relations between sequential 
elements. Some linguistic phenomena, however, are 
better described by assuming relations between sep
arated elements. And modeling this kind of phenom
ena, the accuracies of various application axe gener
ally augmented. 
As for English, there have been researches in 
which a stochastic context-free grammar (SCFG) 
(Fujisaki et al., 1989) is used for model descrip
tion. Recently some researchers have pointed out the 
importa~lce of the lexicon and proposed lexicMized 
models (Jelinek et al., 1994; Collins, 1997). In these 
models, every headword is propagated up through 
the derivation tree such that every parent receives a 
headword from tile head-child. This kind of speciaL
ization may, however, be excessive if tile criterion is 
predictive power of the model. Research aimed at 
estimating the best specialization level for 2-grarn 
model (Mori et al., 1997) shows a class-based model 
is more predictive than a word-based 2-gram model, 
a completely lexicaiized model, comparing cross en
tropy of a POS-based 2-grain model, a word-based 
2-gram model aztd a class-based 2-gram model, es
timated from information theoretical point of view. 
As for a parser based on a class-based SCFG, Char
niak (1997) reports better accuracy than tile above 
lexicalized models, but tile clustering method is not 
clear enough azld, in addition, there is no report 
on predictive power (cross entropy or perplexity). 
\]~\[ogenhout mid Matsumato (1997) propose a word
clustering method based on syntactic behavior, but 
no language model is discussed. As the experiments 
in the present paper attest, word-class relation is 
dependent on language model. 
In this paper, taking Japanese as tile object lan
guage, we propose two complete stochastic laitguage 
models using dependency between bunsetsu, a se
quence of one or more content words followed by 
zero, one or more function words, and evaluate their 
predictive power by cross entropy. Since tile number 
of sorts of bunsetsu is enormous, considering it as a 
symbol to be predicted would surely invoke the data
sparseness problem. To cope with this problem we 
898 
use the concept of class proposed for a word n-graaal 
model (Brown et al., 1992). Each bunsetsu is repre
sented by the class calculated from the POS of its 
last content word and that of its last flmction word. 
The relation between bunsetsu, called dependency, is 
described by a stochastic context-free grammar (Fu, 
1.974) on the classes. From the class of a bunsetsu, 
the content word sequence and the function word se
quence are independently predicted by word n-gra~m 
inodels equipped with unknown word models (Mori 
aztd Yamaji, 1997). 
The above model assumes that the syntactic be
havior of each bunsetsu depends only on POS. The 
POS system invented by greanmarians may not al
ways be the best in terms of stochastic language 
modeling. This is experimentally attested by the 
paper (Mori et al., 1997) reporting comparisons be
tween a POS-t)ased n-grasn model and a class-based 
n-graan model induced automatically. We now pro
pose, based on this report,, a word-clustering method 
on the model we have mentioned above to success
fully improve the predictive power. In axtdition, we 
discuss a parsing method as aat application of the 
model. 
We also report the result of experiments con
ducted on EDR corpus (Jap, 1993) The corpus is di
vided into text parts a~td the models estimated from 
nine of them are tested on the rest in terms of cross 
entropy. As the result, the cross entropy of the POS
based dependency model is 5.3536 bits asld that of 
the class-based dependency model estimated by our 
method is 4.9944 bits. This shows that the clus
tering method we I)ropose irnproves the predictive 
power of tile POS-based model notably. Addition
ally, a parsing experiment proved that the paxser 
based on the improved model has a higher accuracy 
than the POS-based one. 
2 Stochastic
Language Model based 
on Dependency 
In this section, we proI)ose a stochastic language 
model based on dependency. Formally this model is 
based on a stochastic context-free grammar (SCFG). 
The terminal symbol is the attribute of a bunsetsu, 
represented by the product of the head of the con
tent part and that of the function part. From the 
attribute, a word sequence that matches the bun
setsu is predicted by a word-based 2-gram model, 
and unknown words are predicted from POS by a 
character-based 2-gram model. 
2.1 Sentence
Model 
A Japanese sentence is considered as a sequence of 
units called bunsetsu composed of one or more con
tent words and fimction words. Let Cont be a set 
of content words, Func a set of function words and 
Sign a set of punctuation symbols. Then bunsetsu 
is defined as follows: 
Bnst = Cont+ Func * U Cont+ Func* Sign, 
where the signs "+" and "*" mean positive closure 
aatd Kleene closure respectively. Since the relations 
between bunnetsu known as dependency are not al
ways between sequential ones, we use SCFG to de
scribe them (Fu, 1974). The first problem is how 
to choose terminal symbols. The simplest way is to 
select each bunsetsu as a terminM symbol. In this 
case, however, the data-spaxseness problem would 
surely be invoked, since the number of possible bun
setsu is enormous. To avoid this problem we use the 
concept of class proposed for a word n-gram model 
(Brown et al., 1992). All bunsetsu axe grouped by 
the attribute defined a.s follows: 
attrib(b) (1) 
= (last(cont(b)), last(func(b)), last(sign(b))), 
where the functions cont, func and sign t',fl~e a 
bunsetsu as their argument aald return its content 
word sequence, its function word sequence and its 
punctuation respectively. In addition, the function 
last(m) returns the POS of the last element of word 
sequence ~rt or NULL if the sequence has lio word. 
Given the attribute, the content word seqnence aald 
the fnnction word sequence of the bunsetsu are inde
pendently generated by word-based 2-gram models 
(Mori zatd Yaanaji, 1997). 
2.2 Dependency
Model 
In order to describe the relation between bunsetsu 
called deI)endency , we make the generally accepted 
assumption that no two dei)endency relations cross 
each other, aald we introduce a SCFG with the at
tribute of bunsetsu as terminals. It is known, as a 
characteristic of the Japa~mse laIlguage, that each 
bunsetsu depends on the single bunsetsu appearing 
just before it. We say of two sequential bunsetsu 
that the first to appear is the zatterior and the sec
ond is the posterior. We assume, in addition, that 
tile dependency relation is a binary relation that 
each relation is independent of the others. Then 
this relation is representing by the following form of 
rewriting rule of CFG: B ~ AB, where A is the at
tribute of the a~tterior bunsetsu oaLd B is that of the 
posterior. 
Similarly to terminal symbols, non-terminal sym
bols caal be defined as the attribute of bunsetsu. Also 
they caal be defined as the product of the attribute 
aItd some additional information to reflect the char
acteristics of the dependency. It is reported that the 
dependency is more frequent between closer bunsetsu 
in terms of the 1)osition in tim sentence (Maruyaana 
and Ogino, 1992). IIt order to model these char
acteristics, we add to the attribute of bunsetsu an 
899 
S I 
<verb. ending, period. 2.0) 
<noun, NULL. comma, O, O> 
kyou/noun ,/sign 
(today) 
<noun. postp., NULL. 0. O> 
Kynto ! noun daigaku/noun he/postp. 
(Kyoto) (university) (to) 
I 
<verb. ending, period. 0.0> -
i/verb ku/ending ./sign 
(go) 
SCFG 
n-gram 
Figure h Dependency model based on bunsetsu 
additional information field holding the number of 
bunsetsu depending on it. Also the fact that a bun
setsu has a tendency to depend on a bunsetsu with 
comma. For this reasov the number of bunsetsu with 
comma depending on it is also added. To avoid 
data-sparseness problem we set an upper bound for 
these numbers. Let d be the number of bunsetsu de
pending on it and v be the number of bunsetsu with 
comma depending on it, the set of terminal symbols 
T and that of non-terminal symbols V is represented 
as follows (see Figure 1): 
T= attrib(b) x {0} x {0} 
V=attrib(b) x {1, 2, "'dm~=} x {0, 1, ""vm,z}. 
It should be noted that terminal symbols have no 
bunsetsu depending on them. It follows that all 
rewriting rules are in the following forms: 
S~ 
(al, dl, Vl):=> 
a 1 = 
dl ---
d, v) (2) 
(a2, d2, v2)(aa, da, v3) (3) 
a3 
min(da + 1, dma~) 
min(va + 1, vm,,) 
V 1 = if sign(a~) = comma 
va otherwise 
where a is the attribute of bunsetsu. 
The attribute sequence of a sentence is generated 
through applications of these rewriting rules to tile 
start symbol S. Each rewriting rule has a probability 
and the probability of the attribute sequence is the 
product of those of tile rewriting rules used for its 
generation. Taking tile example of Figure 1, this 
value is calculated as follows: 
P((noun, NULL, comma, O, O) 
(noun, postp., NULL, O, O) 
(verb, ending, period, 0, 0)) 
= P(S =#(verb, ending, period, 2, 0)) 
X/)((verb, ending, period, 2, O) 
=~ (noun, NULL, comma, O, O) 
(verb, ending, period, i, 0)) 
x P(<verb, ending, period, I, O) 
(noun, postp., NULL, O, O) 
(verb, ending, period, O, 0)). 
The probability value of each rewriting rule is esti
mated from its frequency N in a syntactically anno
tated corpus as follows: 
P(S =~ (al, dl, vl)) 
_ N(S =~ (al, dl, Vl)) 
N(S) 
P((al, dl, Vl> :=~ (a2, d2, v2)<aa, da, va)) 
N((al, dl, vl)=:~ (a2, d2, v2>(a3, d~, v3)) 
N((al, dl, Vl)) 
In a word n-graan model, in order to cope with 
data-spaxsevess problem, tile interpolation tech
nique is applicable to SCFG. The probability of the 
interpolated model of grammaxs G1 aald G2, whose 
900 
probabilities are Pl and /)2 respectively, is repre
sented as follows: 
r(A ~ ~) = ~P~(A ~ ~) + ~P2(A ~ ~) 
O<aj<l(j=l,e)~,d ~+a2=1 (4) 
where A E V and c~ E (VUT)*. Tile coefficients are 
estimated 1)y held-ont method or deleted interpola
tion method (Jelinek et at., 1991). 
3 Word
Clustering 
The model we have mentioned above uses the POS 
given ma~tually for the attribute of bur~etsu. Chang
ing it into some class may intprove the predictive 
power of the model. This change needs only a slight 
replacement in the model rei)resenting formula (1): 
the function last returns the class of tile last word of 
a word sequence m instead of the POS. The prol)lem 
we have to solve here is how to obtain such classes 
i.e. word clustering. In this section, we propose 
axt objective function and a search algorithm of the 
word clustering. 
3.1 Objective
Function 
The aim of word clustering is to build a language 
model with less cross entropy without referring to 
the test corpus. Similar reseaxch has been success
ful, aiming at az~ improvement of a word n-grain 
model both in English and Japanese (Mort et aJ., 
1997). So we have decided to extend this research 
to obtain a~ optimM word-class relation. The only 
difference from the previous reseaxch is the lautguage 
model. In this (:as% it is a SCFG in stead of a n
gram model. Therefore the objective flmction, called 
average cross entropy, is defined as follows: 
-H = __1 ~ H(Li, Mi), (5) 
i=l 
where Li is the i-th learning corpus aa~d Mi is tile 
la~tguetge model estimated from the learning corpus 
excluding the i-th leaxning corpus. 
3.2 Algorithm

The sohltion space of the word clustering is the set of 
• all possible word-class relations. The cardinality of 
the set, however, is too enormous for the dependency 
model to calculate tile average cross entropy for all 
word-class relations and select the best one. So we 
abandoned tim best solution and adopted a greedy 
algorithm as shown in Figure 2. 
4 Syntactic
Analysis 
Syntactic AnMysis is defined as a function which 
receives a character sequence as aa~ input, divides 
it into a buusetsu sequence and deterinines depen
dency relations among them, where the concatena
tion of character sequences of all the bunsetsu must 
Let "ml, 7/~2~ ... ~ Tit n be Ad sorted 
in the descending order of frequency. 
~1 := {ml, ,n2, ..., m.} 
C = {CI} 
foreach i (1, 2, ..., n) 
f(m,) := cl 
foreach i (1, 2, --., n) 
c := argmin~ecu ..... -H(move(f, mi, c)) 
if (-H(move(f, mi, c)) < H(f)) then 
f := move(f, mi, e) 
update interpolation coefficients. 
if (c = c.,.,) then 
C := C u {c.0~} 
i=1 
i=2 
i = 3 
i=4 
~~@@@lnOLl~(\],pl~jlC ~C~ ........................................................ ..................... ;' 
~ : update Interpulatloll ¢oe|ficlents 
................................ 
.. ...................... ::...:.:::,.. update interpolation coefficients 
/ '.., . 
f., ......................... 
update interpnlation coefficients 
Figure 2: The clustering algorithm. 
I)e equal to tile input. Generally there are one or 
more solutions for azly input. A syntactic aztalyzer 
chooses tile structure which seems tile most similax 
to the human decision. There axe two kinds of an
alyzer: one is called a rule-based aJr, dyzer, which is 
based oil rules described according to the intuition 
of graanmariaam; the other is called a corpus-based 
a~talyzer, because it is based on a laxge number of 
analyzed examples. In this section, we describe a 
stochastic syntactic analyzer, which belongs to the 
second category. 
4.1 Stochastic
Syntactic Analyzer 
A stochastic syntactic anaJyzer, based on a stochas
tic lazlguage model inclndlng the concept of depen
dency, calculates the syntactic tree (see Figure 1) 
with the highest probability for a given intmt m ac
cording to the following formula: 
~t : argmax P(TIx ) W(T)=X 
901 
Table 1: Corpus. Table 2: Predictive power. 
learning 
test 
#sentences #bunsetsu #word 
174,524 
19,397 
1,610,832 4,251,085 
178,415 471,189 
#non-terminal cross 
language model +#terminal entropy 
POS-based model 576 5.3536 
class-based model 10,752 4.9944 
= argma× P(TI:r)P(:r ) W(T)=X 
= argmax P(xlT)P(T) (v Bayes' formula) 
W(T)=X 
=argmaxP(T) ('." P(xlT ) = 1), W(T)=X 
where w(T) represents tlle character sequence of the 
syntactic tree T. P(T) in the last line is a stochas
tic lazlguage model including the concept of depen
dency. We use, as such a model, the POS-based de
pendency model described in section 2 or tile class
based dependency model described in section 3. 
4.2 Solution
Search Algorithm 
The stochastic context-free grammar used for sy~l
tactic analysis consists of rewriting rules (see for
mula (3)) in Chomsky normal form (Hopcroft and 
Ullma~l, 1979) except for the derivation from the 
start symbol (formula (2)). It follows that a CKY 
method extended to SCFG, a dynamic-programming 
method, is applicable to calculate tile best solution 
in O(n a) time, where n is the number of input char
acters. It should be noted that it is necessary to 
multiply the probability of the derivation from the 
start symbol at the end of the process. 
5 Evaluation

We constructed the POS-based dependency model 
and tile class-based dependency model to evaluate 
their predictive power. In addition, we implemented 
parsers based on them which calculate tile best syn
tactic tree from a given sequence of bun~etsu to ob
serve their accuracy. In this section, we present the 
experimental results and discuss them. 
5.1 Conditions
on the Experiments 
As a syntactically annotated corpus we used EDR 
corpus (Jap, 1993). The corpus was divided into 
ten parts and the models estimated from nine of 
them were tested on the rest in terms of cross en
tropy (see Table 1). The number of characters in 
the Japanese writing system is set to 6,879. Two 
parameters which have not been determined yet in 
tile expla~lation of tile models (dma~ and Vmaz) are 
both set to 1. Although the best value for each of 
them ca~l also be estimated using the average cross 
entropy, they are fixed through the experiments. 
5.2 Evaluation
of Predictive Power 
For the purpose of evaluating the predictive power 
of the models, we calculated their cross entropy on 
the test corpus. In this process the a~motated tree 
is used as the structure of tile sentences in the test 
corpus. Therefore the probability of each sentence 
in the test corpus is not the summation over all its 
possible derivations. In order to compare the POS
based dependency model and the class-based depen
dency model, we constructed these models from the 
same learning corpus and calculated their cross en
tropy on tile same test corpus. They are both inter
polated with the SCFG with uniform distribution. 
The processes for their construction are as follows: 
• POS-based dependency model 
1. estimate the interpolation coefficients in 
Formula (4) by the deleted interpolation 
method 
2. count the frequency of each rewriting rule 
on the whole learning corpus 
• class-based dependency model 
1. estimate the interpolation coefficients in 
Formula (4) by the deleted interpolation 
method 
2. calculate a~l optimal word-class relation by 
the method proposed in Section 3. 
3. count the frequency of each rewriting rule 
on the whole learning corpus 
The word-based 2-gra~l model for bunsetsu gener
ation a~ld the character-based 2-gram model as an 
unknown word model (Mori and Yamaji, 1997) are 
common to the POS-based model and class-based 
model. Their contribution to the cross entropy is 
constant on the condition that the dependency mod
els contain the prediction of the last word of the con
tent word sequence trod that of tile function word 
sequence. 
Table 2 shows the cross entropy of each model 
on the test corpus. The cross entropy of tile class
based dependency model is lower than that of the 
POS-based dependency model. This result attests 
experimentally that the class-based model estimated 
by our clustering method is more predictive than 
the POS-ba.sed model azld that our word clustering 
902 
Table 3: Accuracy of each nmdel. 
laalguage model cross entropy accuracy 
POS-based model 5.3536 68.77% 
class-based model 4.9944 81.96% 
select always 53.10% the next bunsetsu 
naethod is efficient at improvement of a dependency 
model. 
~,Ve also c',dculated the cross entropy of the class
t)ased model which we estimated with a word 2-graJn 
model as the model M in tile Formula (5). The num
ber of termin',ds and non-ternfin',ds is 1,148,916 and 
the cross entropy is 6.3358, which is nmch higher 
thazl that of the POS-base model. This result indi
cates that the best word-class relation for the depen
dency model is quite different from the best word
class relation for ttle n-gram model. Comparing the 
number of tile terminals said non-terminals, the best 
word-class relation for n-gram model is exceedingly 
specialized fi)r a dependency model. We cast con
clude that word-class relation depends on tile laal
guage model. 
5.3 Evaluation
of Syntactic Analysis 
We implemented a parser based o\[t the dependency 
models. Since our models, equipped with a word
1)ase(t 2-gram model for bunsetau generation said the 
character-based 2-graan as an unknown word model, 
can retnru the probability for any input, we cast 
tmild a parser, based on our model, receiving a char
acter sequence as input. Its evaluation is zlot easy, 
however, because errors \[nay occur in bunsetsu gen
eration or in POS estimatio\[, of unknown words. For 
this reason, in tile following description, we assume 
a bunsctsu sequence as the input. 
The criterion we adopted is the accuracy (if depen
dency relation, but the last bunsetsu, which has no 
bunsetsu to depend on, and the second-to-last bun
setsu, which depends always on the last bunsetsu, 
are excluded from consideration. 
Table 3 shows cross entropy a~ld parsing accuracy 
of the POS-based dependency model a~ld tile class
based dependency model. This result tells us our 
word clustering method increases parsing accuracy 
considerably. This is quite natural in the light of the 
(tecreaze of cross entropy. 
Tile relation between tile leaxnlng corpus size and 
cross entropy or parsing accuracy is shown in Fig
ure 3. The lower bound of cross entrol)y is the en
tropy of Japanese, which is estimated to be 4.3033 
lilt (Mori and Yzanaji, 1997). Talcing this fact into 
consideration, tile cross entropy of both of the mod
els has stronger tendency to decrease. As for ac
121 
10 
.q 8 
4 
2 
°io~ 
rlt~l b~ell depemdeacy m*dd 
c~*, e~r*py 
~00% 
8O 
60 ~" 
4O 
20 
L t A i . ~ l 0 
101 102 103 10 ~' 10 s 10 ~ 107 
%characters in learning cnrpus 
Figure 3: Relation between cross entropy a~l(1 pars
ing accuracy. 
curacy, there also is a tendency to get more accu
rate as the leaxning corpus size increases, but it is a 
strong tendency for the class-based model tha~l for 
tile POS-based model. It follows that tile class-based 
model t)rofits more greatly from an increase of the 
leaxning corlms size. 
6 Conclusion

hi this paper we have presented dependency mod
els fl)r Japanese based on the attribute of bunsetsu. 
They axe the first fully stochastic dependency mod
els for Japaalese which describes from character se
quellce to syntax:tic tree. Next we have proposed 
a word clustering method, au extension of deleted 
interpolation technique, which has been proven to 
be efficient in terms of improvement of the pre
dictive power. Finally we have discussed paxsers 
based on our model which demonstrated a remaxk
able improvement in parsing accuracy by our word
clustering method. 

References 

Peter F. Brown, Vincent .1. Della Pietra, Peter V. 
deSouza, Jennifer C. Lal, azld Robert L. Mercer. 
1992. Class-based n-graJn models of natural lazt
guage. Corn, putational Linguistics, 18(4):467 479. 

Eugene Chaxniak. 1997. Statistical parsing with a 
context-free graznmar a~ld word statistics. In Pro
ceedings of the 14th National Conference on Arti
ficial Intelligence, pages 598-603. 

Michael Collins. 1997. Three generative, lexicalised 
models for statistical parsing. In Proceedings of 
the S5th Annual Meeting of the Association for 
Computational Linguistics, pages 16-23. 

King Sun t711. 1974. Syntactic Methods in Pattern 
Recognition, volume 12 of Mathematics in Science 
and Engineering. Accademic Press. 

T. Fujisaki, F. Jelinek, J. Cocke, E. Black, and 
T. Nishino. 1989. A probabilistic parsing method 
for sentence disasnbiguation. In Proceedings of the 
International Parsing Workshop. 

Wide R. Hogenhout and Yuji Matsumoto. 1997. A 
preliminary study of word clustering based on syn
tactic behavior. In Proceedings of the Computa
tional Natural Language Learning, pages 16-24. 

John E. Hopcroft and Jeffrey D. Ullman. 1979. In
troduction to Automata Theory, Languages and 
Computation. Addison-~,Vesley Publishing. 
Japan Electronic Dictionary Research Institute, 
Ltd., 1993. EDR Electronic Dictionary Technical 
Guide. 

Fredelick Jelinek, Robert L. Mercer, and Salim 
Roukos. 1991. Principles of lexical language 
modeling for speech recognition. In Advances in 
Speech Signal Processing, chapter 21, pages 651
699. Dekker. 

F. Jelinek, J. Lafferty, D. Magermazl, R. Mercer, 
A. Raaltnaparkhi, and S. Roukos. 1994. Decision 
tree parsing using a hidden derivation model. In 
Proceedings of the ARPA Workshop on Human 
Language Technology, pages 256-261. 

Hiroshi Maruyama and Shiho Ogino. 1992. A staffs
ticaJ property ofjapaamse phrase-to-phrase modifi
cations. Mathematical Linguistics, 18(7):348-352. 

Shinsuke Mori and Osa~m Yamaji. 1997. An 
estimate of an upper bound for the entropy 
of japanese. Transactions of Information Pro
cessing Society of Japan, 38(11):2191-2199. (In 
Japa~mse). 

Shinsuke Mori, Masafumi Nishimura, and Nobuyuki 
Ito. 1997. Word clustering for class-based la~l
guage models. Transactions of Information Pro
cessing Society of Japan, 38(11):2200-2208. (In 
Japanese). 

C. E. Shazmon. 1951. Prediction and entropy of 
printed english. Bell System Technical Journal, 
30:50~54. 

