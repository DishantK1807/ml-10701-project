Data The sense-tagged text and feature set used in these experiments are the same as in (Bruce et al., 1996).
W96-0210
Because their joint distributions have such closed-form expressions, the parameters can be estimated directly from the training data without the need for an iterative fitting procedure (as is required, for example, to estimate the parameters of maximum entropy models; (Berger et al., 1996)).
J96-1002
In order to utilize models with more complicated interactions among feature variables, (Bruce and Wiebe, 1994b) introduce the use of sequential model selection and decomposable models for word-sense disambiguation.
H94-1047 P94-1020
2An alternative feature set for this data is utilized with an exemplar-based learning algorithm in (Ng and Lee, 1996).
P96-1006
Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., (Berger et al., 1996)), but within this framework no systematic study of interactions has been proposed.
J96-1002
They are characterized by the following properties (Bruce and Wiebe, 1994b): 1.
H94-1047 P94-1020
~ Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., (Brown et al., 1991), (Dagan et al., 1991), and (Yarowsky, 1993) present techniques for identifying the optimal feature to use in disambiguation).
H93-1052 P91-1017 P91-1034
However, the Naive Bayes classifier has been found to perform well for word-sense disambiguation both here and in a variety of other works (e.g., (Bruce and Wiebe, 1994a), (Gale et al., 1992), (Leacock et al., 1993), and (Mooney, 1996)).
H93-1051 H94-1047 P94-1020 W96-0208 W96-0210
(Black, 1988) and (Mooney, 1996)) but, while it is a type of model selection, the models are not parametric.
W96-0208
