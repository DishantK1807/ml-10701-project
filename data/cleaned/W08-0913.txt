Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 107–115,
Columbus, Ohio, USA, June 2008. c©2008 Association for Computational Linguistics
Diagnosingmeaningerrors in
shortanswersto readingcomprehensionquestions
StaceyBailey
Departmentof Linguistics
The Ohio State University
1712 Neil Avenue
Columbus, Ohio 43210,USA
s.bailey@ling.osu.edu
DetmarMeurers
Seminarf¨ur Sprachwissenschaft
Universit¨at T¨ubingen
Wilhelmstrasse19
72074T¨ubingen,Germany
dm@sfs.uni-tuebingen.de
Abstract
A common focus of systems in Intelli-
gent Computer-Assisted Language Learning
(ICALL) is to provide immediatefeedback to
languagelearnersworkingon exercises. Most
ofthisresearchhasfocusedonprovidingfeed-
back on the form of the learnerinput. Foreign
languagepracticeand secondlanguageacqui-
sitionresearch,on the other hand, emphasizes
the importance of exercises that require the
learnerto manipulatemeaning.
The ability of an ICALL system to diag-
nose and provide feedback on the mean-
ing conveyed by a learner response depends
on how well it can deal with the response
variation allowed by an activity. We focus
on short-answerreadingcomprehensionques-
tions which have a clearly defined target re-
sponse but the learner may convey the mean-
ing of the target in multiple ways. As empiri-
cal basis of our work, we collectedan English
as a Second Language (ESL) learner corpus
of short-answerreading comprehensionques-
tions, for which two graders provided target
answers and correctness judgments. On this
basis, we developed a Content-Assessment
Module (CAM), which performs shallow se-
manticanalysisto diagnosemeaningerrors. It
reachesan accuracy of 88%forsemanticerror
detection and 87% on semantic error diagno-
sis on a held-outtest data set.
1 Introduction
Languagepracticethat includesmeaningfulinterac-
tion is a critical component of many current lan-
guage teaching theories. At the same time, exist-
ing researchon intelligentcomputer-aidedlanguage
learning(ICALL)systemshas focusedprimarilyon
providing practice with grammatical forms. For
mostICALLsystems,althoughformassessmentof-
ten involves the use of natural language processing
(NLP) techniques, the need for sophisticated con-
tent assessment of a learner response is limited by
restricting the kinds of activities offered in order to
tightly control the variation allowed in learner re-
sponses,i.e.,onlyoneorveryfewformscanbeused
by the learner to express the correct content. Yet
many of the activities that language instructors typ-
ically use in real language-learningsettings support
a significant degree of variation in correct answers
and in turn require both form and content assess-
ment for answer evaluation. Thus, there is a real
need for ICALL systems that provide accurate con-
tent assessment.
While some meaningful activities are too unre-
strictedforICALLsystemstoprovideeffective con-
tent assessment, where the line should be drawn on
a spectrum of language exercises is an open ques-
tion. Different language-learning exercises carry
different expectations with respect to the level and
type of linguistic variation possible across learner
responses. In turn, theseexpectationsmay be linked
to the learning goals underlyingthe activity design,
the cognitive skills required to respond to the ac-
tivity, or other properties of the activity. To de-
velop adequate processingstrategies for content as-
sessment, it is important to understand the connec-
tion between exercises and expected variation, as
conceptualized by the exercise spectrum shown in
Figure 1, because the level of variation imposes re-
107
Tightly Restricted Responses Loosely Restricted Responses
Decontextualized 
grammar fill-in-
the-blanks
Short-answe r reading 
comprehension 
questions
Essa ys on 
individualized 
topics
The Middle Ground
Viable Processing Ground
Figure1: LanguageLearningExerciseSpectrum
quirements and limitations on different processing
strategies. At oneextremeof the spectrum,thereare
tightly restrictedexercises requiringminimalanaly-
sis in order to assess content. At the other extreme
are unrestricted exercises requiring extensive form
and contentanalysisto assess content. In this work,
we focus on determining whether shallow content-
analysis techniques can be used to perform content
assessment for activities in the space between the
extremes. A good test case in this middle ground
are loosely restricted reading comprehension (RC)
questions. From a teaching perspective, they are a
task that is common in real-life learning situations,
they combine elements of comprehension and pro-
duction, and they are a meaningful activity suited
to an ICALL setting. From a processing perspec-
tive, responsesexhibitlinguisticvariationonlexical,
morphological, syntactic and semantic levels – yet
the intendedcontentsof the answeris predictableso
that an instructorcan definetarget responses.
Since variation is possible across learner re-
sponses in activities in the middle ground of the
spectrum,we propose a shallow content assessment
approach which supports the comparison of target
and learnerresponseson several levels includingto-
ken, chunk and relation. We present an architec-
ture for a content assessmentmodule(CAM)which
providesthisflexibilityusingmultiplesurface-based
matching strategies and existing language process-
ing tools. For an empiricalevaluation, we collected
a corpus of language learner data consisting exclu-
sively of responsesto short-answerreadingcompre-
hensionquestionsby intermediateEnglishlanguage
learners.
2 TheData
The learner corpus consists of 566 responses to
short-answer comprehension questions. The re-
sponses, written by intermediate ESL students as
part of their regular homework assignments, were
typically 1-3 sentences in length. Students had ac-
cess to their textbooks for all activities. For devel-
opmentand testing,the corpuswas dividedinto two
sets. The development set contains 311 responses
from 11 students answering 47 different questions;
the test set contains255 responsesfrom 15 students
to 28 questions. The developmentand test sets were
collected in two different classes of the same inter-
mediatereading/writingcourse.
Two graders annotated the learner answers with
a binary code for semantic correctness and one of
several diagnosis codes to be discussed below. Tar-
get responses (i.e., correct answers) and keywords
from the target responses were also identified by
the graders.1 Because we focus on content assess-
ment, learner responses containing grammatical er-
rors were only marked as incorrect if the grammat-
ical errors impactedthe understandingof the mean-
ing.
The graders did not agree on correctness judg-
ments for 31 responses (12%) in the test set. These
wereeliminatedfromthe test set in orderto obtaina
gold standardfor evaluation.
The remainingresponses in the development and
test sets showed a rangeof variationfor many of the
prompts. As the following examplefrom the corpus
illustrates, even straightforward questions based on
1Keywords refer to terms in the target response essential to
a correctanswer.
108
an explicit short reading passage yield both linguis-
tic and contentvariation:
CUE: What are the methods of propaganda men-
tionedin the article?
TARGET: The methods include use of labels, visual
images, and beautiful or famous people promoting
the idea or product. Also used is linkingthe product
toconceptsthatareadmiredordesiredandtocreate
theimpressionthateveryonesupportstheproductor
idea.
LEARNER RESPONSES:
• A number of methods of propaganda are used
in the media.
• Bositiveor negative labels.
• Giving positive or negative labels. Using vi-
sualimages. Havinga beautifulor famousper-
son to promote. Creating the impression that
everyonesupportsthe product or idea.
While the third answer was judged to be correct,
the syntacticstructures,word order, forms,and lexi-
cal items used (e.g., famous person vs. famous peo-
ple) vary from the string provided as target. Of the
learner responsesin the corpus, only one was string
identical with the teacher-provided target and nine
wereidenticalwhentreatedasbags-of-words. Inthe
test set, none of the learner responses was string or
bag-of-word identical with the correspondingtarget
sentence.
To classify the variation exhibited in learner re-
sponses, we developed an annotationscheme based
on target modification, with the meaning error la-
bels being adapted from those identified by James
(1998) for grammatical mistakes. Target modifica-
tion encodes how the learner response varies from
the target, but makes the sometimes incorrect as-
sumption that the learner is actually trying to “hit”
the meaning of the target. The annotation scheme
distinguishes correct answers, omissions (of rele-
vant concepts), overinclusions (of incorrect con-
cepts), blends (both omissions and overinclusions),
and non-answers. These error types are exempli-
fied below with examples from the corpus. In ad-
dition, the graders used the label alternate answer
for responses that were correct given the question
and reading passage, but that differed significantly
in meaning from what was conveyed by the target
answer.2
1. Necessaryconceptsleftoutoflearnerresponse.
CUE: Name the features that are used in the
designof advertisements.
TARGET: The features are eye contact, color,
famous people, language and cultural refer-
ences.
RESPONSE: Eye contact,color
2. Responsewith extraneous,incorrect concepts.
CUE: Which form of programming on TV
shows that highestlevel of violence?
TARGET: Cartoonsshow the most violentacts.
RESPONSE: Television drama, children’s pro-
grams and cartoons.
3. An incorrect blend/substitution (correct con-
cept missing,incorrectone present).
CUE: What is alliteration?
TARGET: Alliteration is where sequential
words begin with the same letter or sound.
RESPONSE: The worlds are often chosen to
make some pattern or play on works. Sequen-
tial worksbeginswiththe sameletteror sound.
4. Multipleincorrectconcepts.
CUE: What was the major moral question
raised by the Clintonincident?3
TARGET: The moral question raised by the
Clinton incident was whether a politician’s
personal life is relevant to their job perfor-
mance.
RESPONSE: The scandal was about the rela-
tionshipbetweenClintonand Lewinsky.
3 Method
The CAM design integrates multiple matching
strategies at different levels of representation and
various abstractions from the surface form to com-
pare meanings across a range of response varia-
tions. Theapproachisrelatedtothemethodsusedin
2We use the term concept to refer to an entity or a relation
between entities in a representation of the meaning of a sen-
tence. Thus,a responsegenerallycontainsmultipleconcepts.
3Note the incorrect presupposition in the cue provided by
the instructor.
109
machine translation evaluation (e.g., Banerjee and
Lavie, 2005; Lin and Och, 2004),paraphraserecog-
nition (e.g., Brockett and Dolan, 2005; Hatzivas-
siloglou et al., 1999), and automatic grading (e.g.,
Leacock,2004;Mar´ın, 2004).
To illustrate the general idea, consider the exam-
ple from our corpusin Figure2.
Figure2: Basicmatchingexample
Wefindonestringidenticalmatchbetweenthetoken
was occurringin the target and the learnerresponse.
At the noun chunk level we can match home with
his house. And finally, after pronounresolutionit is
possibleto matchBob Hope with he.
The overall architectureof CAMis shown in Fig-
ure 3. Generally speaking, CAM compares the
learner response to a stored target response and de-
cides whetherthe two responsesare possiblydiffer-
ent realizations of the same semantic content. The
design relies on a series of increasingly complex
comparisonmodulesto “align”or matchcompatible
concepts. Aligned and unaligned concepts are used
to diagnose content errors. The CAM design sup-
portsthe comparisonof target and learnerresponses
on token, chunk and relation levels. At the token
level, the natureof the comparisonincludesabstrac-
tions of the stringto its lemma(i.e., uninflectedroot
formof a word),semantictype(e.g.,date,location),
synonyms, and a more general notion of similarity
supportingcomparisonacrosspart-of-speech.
Thesystemtakesasinputthelearnerresponseand
one or more target responses, along with the ques-
tion and the source reading passage. The compari-
son of the target and learnerinputpair proceedsfirst
with an analysis filter, which determines whether
linguistic analysis is required for diagnosis. Essen-
tially, thisfilteridentifieslearnerresponsesthatwere
copieddirectlyfrom the sourcetext.
Then, for any learner-target response pair that
requires linguistic analysis, CAM assessment pro-
ceeds in three phases – Annotation, Alignment and
Diagnosis. The Annotationphase uses NLP tools to
enrich the learner and target responses, as well as
the question text, with linguistic information, such
as lemmas and part-of-speech tags. The question
text is used for pronoun resolution and to eliminate
conceptsthat are “given” (cf. Halliday, 1967, p.204
and many others since). Here “given” information
refers to conceptsfrom the questiontext that are re-
used in the learner response. They may be neces-
sary for forming complete sentences, but contribute
no new information. For example, if the questionis
What is alliteration? and the response is Allitera-
tion is the repetitionof initialletters or sounds, then
the concept represented by the word alliteration is
given and the rest is new. For CAM, responses are
neither penalizednor rewarded for containinggiven
information.
Table 1 contains an overview of the annotations
and the resources, tools or algorithms used. The
choiceoftheparticularalgorithmorimplementation
wasprimarilybasedonavailabilityandperformance
on our developmentcorpus– otherimplementations
could generallybe substituted without changingthe
overall approach.
AnnotationTask LanguageProcessingTool
SentenceDetection, MontyLingua(Liu,2004)
Tokenization,
Lemmatization
Lemmatization PC-KIMMO(Antworth,1993)
SpellChecking Editdistance(Levenshtein,1966),
SCOWLwordlist(Atkinson,2004)
Part-of-speechTagging TreeTagger(Schmid,1994)
NounPhraseChunkingCASS(Abney,1997)
LexicalRelations WordNet(Miller,1995)
SimilarityScores PMI-IR(Turney,2001;
Mihalceaetal.,2006)
DependencyRelations StanfordParser
(KleinandManning,2003)
Table 1: NLP Tools used in CAM
Afterthe Annotationphase,Alignmentmapsnew
(i.e., not given) concepts in the learner response to
concepts in the target response using the annotated
information. The final Diagnosis phase analyzes
the alignment to determine whether the learner re-
110
Annotation Alignment Diagnosis
Punctuation
Input
Learner Response
Target Response(s)
Question
Output
Source Text
Activity Model
Settings
Sentence Detection
Tokenization
Lemmatization
POS Tagging
Chunking
Dependency Parsing
Spelling Correction
Similarity Scoring
Pronoun Resolution
Type Recognition
Analysis Filter
Givenness
Pre-Alignment Filters
Token-level 
Alignment
Chunk-level 
Alignment
Relation-level 
Alignment
Error
Reporting
Detection
Classification
Diagnosis
Classification
Figure3: Architectureof the ContentAssessmentModule(CAM)
sponsecontainscontenterrors. If multipletarget re-
sponses are supplied, then each is compared to the
learner response and the target response with the
most matches is selected as the model used in di-
agnosis. The output is a diagnosis of the input pair,
whichmightbe usedin a numberof waysto provide
feedbackto the learner.
3.1 Combiningthe
evidence
To combine the evidence from these different lev-
els of analysis for content evaluation and diagno-
sis, we tried two methods. In the first, we hand-
wrote rules and set thresholds to maximize perfor-
mance on the developmentset. On the development
set, the hand-tuned method resulted in an accuracy
of 81% for the semantic error detection task, a bi-
nary judgment task. However, performance on the
test set (which was collected in a later quarter with
a different instructor and different students) made
clearthattherulesandthresholdsthusobtainedwere
overly specific to the development set, as accuracy
dropped down to 63% on the test set. The hand-
written rules apparentlywere not general enough to
transferwellfromthedevelopmentsettothetestset,
i.e., they relied on propertiesof the developmentset
thatwherenot sharedacrossdatasets. Given the va-
riety of features and the many different options for
combiningand weighingthem that might have been
explored, we decided that rather than hand-tuning
therulesto additionaldata,wewouldtryto machine
learn the best way of combining the evidence col-
lected. We thus decided to explore machine learn-
ing, even though the set of development data for
trainingclearlyis very small.
Machine learning has been used for equivalence
recognitionin relatedfields. For instance,Hatzivas-
siloglou et al. (1999) trained a classifier for para-
phrase detection, though their performance only
reached roughly 37% recall and 61% precision. In
a different approach, Finch et al. (2005) found that
MT evaluation techniques combined with machine
learning improves equivalence recognition. They
usedtheoutputofseveralMTevaluationapproaches
based on matching concepts (e.g., BLEU) as fea-
tures/values for training a support vector machine
(SVM)classifier. Matchedconceptsand unmatched
111
conceptsalike were used as featuresfor trainingthe
classifier. Tested against the Microsoft Research
Paraphrase(MSRP) Corpus, the SVM classifierob-
tained 75% accuracy on identifying paraphrases.
But it does not appear that machine learning tech-
niqueshave so far been appliedto or even discussed
inthecontextoflanguagelearnercorpora,wherethe
availabledata sets typicallyare very small.
To begin to address the application of machine
learning to meaning error diagnosis, the alignment
data computedby CAMwas converted into features
suitablefor machinelearning. For example,the first
feature calculated is the relative overlap of aligned
keywords from the target response. The full list of
featuresare listed in Table 2.
Features Description
1.KeywordOverlap Percentofkeywordsaligned
(relativetotarget)
2.TargetOverlap Percentofalignedtargettokens
3.LearnerOverlap Percentofalignedlearnertokens
4.T-Chunk Percentofalignedtargetchunks
5.L-Chunk Percentofalignedlearnerchunks
6.T-Triple Percentofalignedtargettriples
7.L-Triple Percentofalignedlearnertriples
8.TokenMatch Percentoftokenalignments
thatweretoken-identical
9.SimilarityMatch Percentoftokenalignments
thatweresimilarity-resolved
10.TypeMatch Percentoftokenalignments
thatweretype-resolved
11.LemmaMatch Percentoftokenalignments
thatwerelemma-resolved
12.SynonymMatch Percentoftokenalignments
thatweresynonym-resolved
13.VarietyofMatch Numberofkindsoftoken-level
(0-5) alignments
Table 2: Featuresused for MachineLearning
Features1-7reflectrelativenumbersofmatches(rel-
ative to length of either the target or learner re-
sponse). Features2, 4, and 6 are relatedto the target
responseoverlap. Features3, 5, and 7 are related to
overlap in the learner response. Features 8–13 re-
flect the natureof the matches.
Thevaluesforthe13featuresinTable2wereused
totrainthedetectionclassifier. Fordiagnosis,afour-
teenthfeature–adetectionfeature(1or0depending
onwhetherthedetectionclassifierdetectedan error)
– was added to the developmentdata to train the di-
agnosisclassifier. Given that token-level alignments
are usedin identifyingchunk-and triple-level align-
ments,thatkindsof alignmentsarerelatedto variety
of matches,etc., thereis clearredundancy and inter-
dependence among features. But each feature adds
some new information to the overall diagnosis pic-
ture.
The machine learning suite used in all the devel-
opmentandtestingrunsisTiMBL(Daelemansetal.,
2007). AswiththeNLPtoolsused,TiMBLwascho-
senmainlytoillustratetheapproach. Itwasnoteval-
uated against several learning algorithms to deter-
mine the best performingalgorithm for the task, al-
thoughthisiscertainlyanavenueforfutureresearch.
In fact, TiMBL itself offers several algorithms and
options for training and testing. Experiments with
these optionson the developmentset includedvary-
inghow similaritybetweeninstanceswas measured,
how importance (i.e., weight) was assigned to fea-
tures and how many neighbors(i.e., instances)were
examined in classifying new instances. Given the
very small development set available, making em-
pirical tuning on the development set difficult, we
decided to use the default learning algorithm (k-
nearest neighbor) and majority voting based on the
top-performingtraining runs for each available dis-
tance measure.
4 Results
Turning to the results obtained by the machine-
learning based CAM, for the binary semantic error
detectiontask,thesystemobtainsanoverall87%ac-
curacy on the developmentset (usingthe leave-one-
out option of TiMBL to avoid training on the test
item). Interestingly, even forthissmalldevelopment
set, machinelearningthus outperformsthe accuracy
obtained for the manual method of combining the
evidence reported above. On the test set, the final
TiMBL-based CAM performance for detection im-
proved slightly to 88% accuracy. These results sug-
gest that detection using the CAM design is viable,
though more extensive testing with a larger corpus
is needed.
Balancedsets Both the developmentand test sets
contained a high proportion of correct answers –
71% of the development set and 84% of the test set
weremarkedas correctbythehumangraders. Thus,
112
we also sampled a balanced set consisting of 50%
correct and 50% incorrect answers by randomly in-
cluding correct answers plus all the incorrect an-
swers to obtain a set with 152 cases (development
subset)and 72 (test subset)sentences. The accuracy
obtained for this balanced set was 78% (leave-one-
out-testingwithdevelopmentset)and67%(testset).
The fact that the results for the balanced develop-
mentset usingleave-one-out-testingare comparable
to the generalresultsshows thatthe machinelearner
was not biased towards the ratio of correct and in-
correct responses, even though there is a clear drop
from developmentto test set, possiblyrelated to the
small size of the data sets available for training and
testing.
Alternate answers Another interesting aspect to
discuss is the treatmentof alternateanswers. Recall
that alternate answers are those learner responses
that are correct but significantlydissimilarfrom the
given target. Of the developmentset responsepairs,
15 were labeled as alternate answers. One would
expectthat given that theseresponsesviolatethe as-
sumption that the learner is trying to hit the given
target,usingtheseitemsintrainingwouldnegatively
effect the results. This turns out to be the case; per-
formanceon the trainingset dropsslightlywhenthe
alternateanswerpairs are included. We thus did not
include them in the development set used for train-
ing the classifier. In other words, the diagnosisclas-
sifier was trained to label the data with one of five
codes – correct, omissions (of relevant concepts),
overinclusions(of incorrect concepts), blends (both
omissions and overinclusions), and non-answers.
Because it cannot be determined beforehand which
itemsinunseendataarealternateanswerpairs,these
pairs were not removed from the test set in the final
evaluation. Were these items eliminated, the detec-
tion performancewould improve slightlyto 89%.
Form errors Interestingly, the form errors fre-
quently occurring in the student utterances did not
negatively impact the CAM results. On average, a
learner response in the test set contained 2.7 form
errors. Yet, 68% of correctly diagnosed sentences
included at least one form error, but only 53% of
incorrectly diagnosed ones did so. In other words,
correct responses had more form errors than incor-
rect responses. Looking at numbers and combina-
tions of form errors, no clear pattern emerges that
would suggest that form errors are linked to mean-
ing errors in a clear way. One conclusion to draw
based on these data is that form and content assess-
ment can be treated as distinct in the evaluation of
learner responses. Even in the presence of a range
of form-basederrors,humangraderscan clearlyex-
tract the intendedmeaningto be able to evaluate se-
manticcorrectness. The CAM approachis similarly
able to provide meaning evaluation in the presence
of grammaticalerrors.
Diagnosis For diagnosis with five codes, CAM
obtained overall 87% accuracy both on the devel-
opmentandon thetestset. Given thatthenumberof
labelsincreasesfrom2to5,theslightdropinoverall
performancein diagnosisas comparedto the detec-
tion of semantic errors (from 88% to 87%) is both
unsurprising in the decline and encouraging in the
smallnessof thedecline. However, given thesample
sizeandfew numbersofinstancesofany givenerror
in the test (and development)set, additionalquanti-
tative analysis of the diagnosisresults would not be
particularlymeaningful.
5 RelatedWork
The need for semantic error diagnosis in previous
CALL work has been limited by the narrow range
of acceptable response variation in the supported
language activity types. The few ICALL systems
that have been successfully integrated into real-life
language teaching, such as German Tutor (Heift,
2001) and BANZAI (Nagata, 2002), also tightly
control expected response variation through delib-
erate exercise type choices that limit acceptable re-
sponses. Content assessment in the German Tutor
is performed by string matching against the stored
targets. Because of the tightly controlled exercise
types and lack of variation in the expected input,
the assumption that any variation in a learner re-
sponse is due to form error, rather than legitimate
variation, is a reasonable one. The recently de-
veloped TAGARELA system for learners of Por-
tuguese(Amaraland Meurers, 2006; Amaral,2007)
liftssomeof therestrictionsonexercisetypes,while
relying on shallow semantic processing. Using
strategies inspired by our work, TAGARELA in-
corporatessimplecontentassessmentfor evaluating
113
learnerresponsesin short-answerquestions.
ICALL system designs that do incorporate more
sophisticated content assessment include FreeText
(L’Haire and Faltin, 2003), the Military Language
Tutor (MILT) Program (Kaplan et al., 1998), and
Herr Kommissar (DeSmedt, 1995). These systems
restrictboththeexercisetypesand domainsto make
content assessment feasible using deeper semantic
processingstrategies.
Beyond the ICALL domain, work in automatic
grading of short answers and essays has addressed
whether the students answers convey the correct
meaning, but these systems focus on largely scor-
ing rather than diagnosis (e.g., E-rater, Burstein
and Chodorow, 1999), do not specifically address
language learning contexts and/or are designed to
work specifically with longer texts (e.g., AutoTu-
tor, Wiemer-Hastingset al., 1999). Thus,the extent
to which ICALL systems can diagnose meaning er-
rors in languagelearnerresponseshas beenfar from
clear.
As far as we are aware, no directly comparable
systems performing content-assessment on related
languagelearner data exist. The closest related sys-
tem that does a similar kind of detection is the C-
rater system (Leacock, 2004). That system obtains
85%accuracy. However, thetestsetandscoringsys-
tem were different, and the system was applied to
responsesfromnative Englishspeakers. In addition,
theirwork focusedon detectionof errorsratherthan
diagnosis. So, the results are not directly compara-
ble. Nevertheless, theCAMdetectionresultsclearly
are competitive.
6 Summary
After motivating the need for content assessmentin
ICALL,inthispaperwehavediscussedanapproach
for content assessment of English language learner
responses to short answer reading comprehension
questions, which is worked out in detail in Bailey
(2008). Wediscussedanarchitecturewhichrelieson
shallow processingstrategies and achieves an accu-
racy approaching90%forcontenterrordetectionon
alearnercorpuswecollectedfromlearnerscomplet-
ing the exercises assigned in a real-life ESL class.
Even for the small data sets available in the area of
language learning, it turns out that machine learn-
ingcanbeeffectiveforcombiningtheevidencefrom
variousshallowmatchingfeatures. Thegoodperfor-
mance confirms the viability of using shallow NLP
techniques for meaning error detection. By devel-
oping and testing this model, we hope to contribute
to bridging the gap between what is practical and
feasible from a processing perspective and what is
desirablefrom the perspective of current theoriesof
languageinstruction.
References
Steven Abney, 1997. Partial ParsingviaFinite-StateCas-
cades. Natural Language Engineering, 2(4):337–344.
http://vinartus.net/spa/97a.pdf.
Luiz Amaral, 2007. DesigningIntelligentLanguageTu-
toring Systems: Integrating Natural Language Pro-
cessing Technology into Foreign Language Teaching.
Ph.D.thesis,The Ohio State University.
Luiz Amaral and Detmar Meurers, 2006. Where
does ICALL Fit into Foreign Language Teaching?
Presentation at the 23rd Annual Conference of the
Computer Assisted Language Instruction Consortium
(CALICO), May 19, 2006. University of Hawaii.
http://purl.org/net/icall/handouts/
calico06-amaral-meurers.pdf.
Evan L. Antworth, 1993. Glossing Text with the PC-
KIMMO Morphological Parser. Computers and the
Humanities, 26:475–484.
Kevin Atkinson, 2004. Spell Checking Oriented
Word Lists (SCOWL). http://wordlist.
sourceforge.net/.
Stacey Bailey, 2008. Content Assessment in Intelligent
Computer-Aided Language Learning: Meaning Error
Diagnosis for English as a Second Language. Ph.D.
thesis,The Ohio State University.
Satanjeev Banerjee and Alon Lavie, 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedingsof Workshopon Intrinsicand ExtrinsicEval-
uationMeasuresforMachineTranslationand/orSum-
marization at the 43th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL-2005). Ann
Arbor, Michigan, pp. 65–72. http://aclweb.
org/anthology/W05-0909.
ChrisBrockettandWilliamB.Dolan,2005.SupportVec-
tor Machines for Paraphrase Identification and Cor-
pus Construction. In Proceedings of the Third In-
ternational Workshop on Paraphrasing (IWP2005).
pp. 1–8. http://aclweb.org/anthology/
I05-5001.
Jill Burstein and Martin Chodorow, 1999. Automated
Essay Scoring for Nonnative English Speakers. In
Proceedings of a Workshop on Computer-Mediated
Language Assessmentand Evaluationof Natural Lan-
guage Processing, Joint Symposium of the Asso-
ciation of Computational Linguistics (ACL-99) and
the International Association of Language Learning
Technologies. pp. 68–75. http://aclweb.org/
anthology/W99-0411.
114
Walter Daelemans, Jakub Zavrel, Kovan der Sloot and
Antal van den Bosch,2007. TiMBL: Tilburg Memory-
Based Learner Reference Guide, ILK Technical Re-
port ILK 07-03. Induction of Linguistic Knowledge
Research Group Department of Communication and
Information Sciences, Tilburg University, P.O. Box
90153, NL-5000 LE, Tilburg, The Netherlands, ver-
sion 6.0 edition.
William DeSmedt, 1995. Herr Kommissar: An ICALL
Conversation Simulator for Intermediate German. In
V. Melissa Holland, Jonathan Kaplan and Michelle
Sams (eds.), Intelligent Language Tutors: Theory
Shaping Technology, Lawrence Erlbaum Associates,
pp. 153–174.
Andrew Finch,Young-SookHwangand EiichiroSumita,
2005. Using Machine Translation Evaluation Tech-
niques to DetermineSentence-level SemanticEquiva-
lence. In Proceedingsof the Third InternationalWork-
shop on Paraphrasing (IWP2005). pp. 17–24. http:
//aclweb.org/anthology/I05-5003.
MichaelHalliday,1967.NotesonTransitivityandTheme
in English.Part 1 and 2. Journal of Linguistics, 3:37–
81, 199–244.
Vasileios Hatzivassiloglou, Judith Klavans and Eleazar
Eskin,1999. DetectingText Similarityover ShortPas-
sages: ExploringLinguisticFeatureCombinationsvia
MachineLearning. InProceedingsofEmpiricalMeth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP’99). College Park, Maryland, pp.
203–212. http://aclweb.org/anthology/
W99-0625.
Trude Heift, 2001. Intelligent Language Tutoring
Systems for Grammar Practice. Zeitschrift f¨ur In-
terkulturellen Fremdsprachenunterricht, 6(2). http:
//www.spz.tu-darmstadt.de/projekt_
ejournal/jg-06-2/beitrag/heift2.htm.
CarlJames,1998. ErrorsinLanguageLearningandUse:
ExploringError Analysis. LongmanPublishers.
JonathanKaplan,MarkSobol,RobertWisherandRobert
Seidel, 1998. The Military Language Tutor (MILT)
Program: An AdvancedAuthoringSystem. Computer
AssistedLanguage Learning, 11(3):265–287.
Dan Klein and Christopher D. Manning, 2003. Accu-
rate UnlexicalizedParsing. In Proceedingsof the 41st
MeetingoftheAssociationforComputationalLinguis-
tics(ACL2003).Sapporo,Japan,pp.423–430.http:
//aclweb.org/anthology/P03-1054.
Claudia Leacock, 2004. Scoring Free-Responses Auto-
matically: ACaseStudyofa Large-ScaleAssessment.
Examens, 1(3).
VladimirI. Levenshtein,1966. BinaryCodesCapableof
CorrectingDeletions,Insertions,andReversals. Soviet
PhysicsDoklady, 10(8):707–710.
S´ebastien L’Haire and Anne Vandeventer Faltin, 2003.
Error Diagnosis in the FreeText Project. CALICO
Journal, 20(3):481–495.
Chin-Yew Lin and Franz Josef Och, 2004. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statistics. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-04). pp. 605–612. http://aclweb.org/
anthology/P04-1077.
Hugo Liu, 2004. MontyLingua: An End-to-
End Natural Language Processor with Common
Sense. http://web.media.mit.edu/˜hugo/
montylingua, accessedOctober30, 2006.
DianaRosarioP´erez Mar´ın, 2004. Automatic Evaluation
of Users’ Short Essays by Using Statistical and Shal-
low Natural Language Processing Techniques. Mas-
ter’sthesis,UniversidadAut´onomadeMadrid.http:
//www.ii.uam.es/˜dperez/tea.pdf.
Rada Mihalcea, Courtney Corley and Carlo Strapparava,
2006. Corpus-basedand Knowledge-basedMeasures
of Text SemanticSimilarity. In Proceedingsof the Na-
tional Conference on ArtificialIntelligence. American
Association for Artificial Intelligence (AAAI) Press,
MenloPark, CA, volume21(1),pp. 775–780.
George Miller, 1995. WordNet: A Lexical Database for
English. Communicationsof the ACM, 38(11):39–41.
Noriko Nagata, 2002. BANZAI:An Applicationof Nat-
ural Language Processing to Web-Based Language
Learning. CALICOJournal, 19(3):583–599.
HelmutSchmid,1994. ProbabilisticPart-of-SpeechTag-
ging Using Decision Trees. In International Con-
ference on New Methods in Language Processing.
Manchester, UnitedKingdom,pp. 44–49.
PeterTurney,2001.MiningtheWebforSynonyms: PMI-
IR Versus LSA on TOEFL. In Proceedings of the
Twelfth European Conference on Machine Learning
(ECML-2001). Freiburg, Germany, pp. 491–502.
Peter Wiemer-Hastings, Katja Wiemer-Hastings and
Arthur Graesser, 1999. Improving an Intelligent Tu-
tor’s Comprehensionof Students with Latent Seman-
tic Analysis. In Susanne Lajoie and Martial Vivet
(eds.), Artificial Intelligence in Education, IOS Press,
pp. 535–542.
115

