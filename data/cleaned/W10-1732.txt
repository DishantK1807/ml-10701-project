Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 212–215,
Uppsala, Sweden, 15-16 July 2010. c©2010 Association for Computational Linguistics
Hierarchical Phrase-Based MT at the Charles University
for the WMT 2010 Shared Task
Daniel Zeman
CharlesUniversityinPrague,InstituteofFormalandAppliedLinguistics(ÚFAL)
UniverzitaKarlovavPraze,Ústavformálníaaplikovanélingvistiky(ÚFAL)
Malostranskénáměstí25,Praha,CZ-11800,Czechia
zeman@ufal.mff.cuni.cz
Abstract
We describe our experiments with hier-
archical phrase-based machine translation
for WMT 2010 Shared Task. We provide
adetaileddescriptionofourconfiguration
and data so the results are replicable. For
English-to-Czech translation, we experi-
mentwithseveraldatasetsofvarioussizes
andwithvariouspreprocessingsequences.
For the other 7 translation directions, we
justpresentthebaselineresults.
1 Introduction
Czech is a language with rich morphology (both
inflectional and derivational) and relatively free
wordorder. Infact, thepredicate-argumentstruc-
ture,oftenencodedbyfixedwordorderinEnglish,
is usually captured by inflection (especially the
system of 7 grammatical cases) in Czech. While
the free word order of Czech is a problem when
translating to English (the text should be parsed
first in order to determine the syntactic functions
andtheEnglishwordorder),generatingcorrectin-
flectionalaffixesisindeedachallengeforEnglish-
to-Czech systems. Furthermore, the multitude
of possible Czech word forms (at least order of
magnitudehigherthaninEnglish)makesthedata
sparseness problem really severe, hindering both
directions.
There are numerous ways how these issues
could be addressed. For instance, parsing and
syntax-aware reordering of the source-language
sentences can help with the word order differ-
ences (same goal could be achieved by a reorder-
ingmodelorasynchronouscontext-freegrammar
in a hierarchical system). Factored translation, a
secondary language model of morphological tags
orevenamorphologicalgeneratoraresomeofthe
possiblesolutionstothepoor-to-richtranslationis-
sues.
Oursubmissiontothesharedtaskshouldreveal
whereapurehierarchicalsystemstandsinthisjun-
gleandwhatoftheabovementionedideasmatch
thephenomenathesystemsuffersfrom. Although
ourprimaryfocusliesonEnglish-to-Czechtrans-
lation, we also report the accuracy of the same
system on moderately-sized corpora for the other
threelanguagesandseventranslationdirections.
2 The
Translation System
Our translation system belongs to the hierarchi-
calphrase-basedclass(Chiang,2007),i.e. phrase
pairs with nonterminals (rules of a synchronous
context-free grammar) are extracted from sym-
metrized word alignments and subsequently used
bythedecoder. WeuseJoshua,aJava-basedopen-
sourceimplementationofthehierarchicaldecoder
(Lietal.,2009),release1.1. 1
Word alignment was computed using the first
three steps of thetrain-factored-phrase-
model.perlscriptpackedwithMoses 2 (Koehnet
al.,2007). Thisincludestheusualcombinationof
word clustering usingmkcls3 (Och, 1999), two-
way word alignment using GIZA++ 4 (Och and
Ney, 2003), and alignment symmetrization using
the grow-diag-final-and heuristic (Koehn et al.,
2003).
For language modeling we use the SRILM
toolkit 5 (Stolcke, 2002) with modified Kneser-
Neysmoothing(KneserandNey,1995;Chenand
Goodman,1998).
We use the Z-MERT implementation of mini-
mum error rate training (Zaidan, 2009). The fol-
lowing settings have been used for Joshua and Z-
MERT:
1http://sourceforge.net/projects/joshua/
2http://www.statmt.org/moses/
3http://fjoch.com/mkcls.html
4http://fjoch.com/GIZA++.html
5http://www-speech.sri.com/projects/srilm/
212
• Grammarextraction:
--maxPhraseLength=5
• Decoding:span_limit=10fuzz1=0.1
fuzz2=0.1max_n_items=30rela-
tive_threshold=10.0max_n_rules=50
rule_relative_threshold=10.0
• N-best decoding:use_unique_nbest=true
use_tree_nbest=false
add_combined_cost=truetop_n=300
• Z-MERT:-mBLEU4closest-maxIt5
-ipi20
3 Data
and Pre-processing Pipeline
3.1 Baseline
Experiments
Weappliedoursystemtoalleightlanguagepairs.
However,forallbutoneweranonlyabaselineex-
periment. Fromthedatapointofviewthebaseline
experimentswereevenmoreconstrainedthanthe
organizersofthesharedtasksuggested. Wedidnot
use the Europarl corpus, we only used the News
Commentarycorpus 6 fortraining. Thetargetside
oftheNewsCommentarycorpuswasalsotheonly
sourcetotrainthelanguagemodel. Table1shows
thesizeofthecorpus.
Corpus SentPairs Tokensxx Tokensen
cs-en 94,742 2,077,947 2,327,656
de-en 100,269 2,524,909 2,484,445
es-en 98,598 2,742,935 2,472,860
fr-en 84,624 2,595,165 2,137,407
Table1: Numberofsentencepairsandtokensfor
everylanguagepairintheNewsCommentarycor-
pus. Unlike the organizers of the shared task, we
stickwiththestandardISO639languagecodes: cs
=Czech,de=German,en=English,es=Spanish,
fr=French.
Notethatinsomecasesthegrammarextraction
algorithminJoshuafailsifthetrainingcorpuscon-
tains sentences that are too long. Removing sen-
tencesof100ormoretokens(peradvicebyJoshua
developers)effectivelyhealedallfailures. Unfor-
tunately,forthebaselinecorporathelossoftrain-
ingmaterialwasstillconsiderableandresultedin
dropofBLEUscore,thoughusuallyinsignificant. 7
6 Available
for download athttp://www.statmt.org/
wmt10/translation-task.htmlusing the link “Parallel
corpustrainingdata”.
7 Table1andTable2presentstatistics
before removingthe
longsentences.
The News Test 2008 data set (2051 sentences
in each language) was used as development data
for MERT. BLEU scores reported in this paper
were computed on the News Test 2009 set (2525
sentences each language). The official scores on
NewsTest2010aregivenonlyinthemainWMT
2010paper.
Onlylowercaseddatawereusedforthebaseline
experiments.
3.2 English-to-Czech
Aseparatesetofexperimentshasbeenconducted
fortheEnglish-to-Czechdirectionandlargerdata
were used. We used CzEng 0.9 (Bojar and
Žabokrtský, 2009) 8 as our main parallel corpus.
FollowingCzEngauthors’request,wedidnotuse
sections 8* and 9* reserved for evaluation pur-
poses.
Asthebaselinetrainingdataset(“Small”inthe
following) only the news section of CzEng was
used. Forlarge-scaleexperiments(“Large”inthe
following), we used all CzEng together with the
EMEAcorpus 9 (Tiedemann,2009). 10
As our monolingual data we use the mono-
lingual data provided by WMT10 organizers for
Czech. Table2showsthesizesofthesecorpora.
Corpus SentPairs Tokenscs Tokensen
Small 126,144 2,645,665 2,883,893
Large 7,543,152 79,057,403 89,018,033
Mono 13,042,040 210,507,305
Table 2: Number of sentences and tokens in the
Czech-Englishcorpora.
Again, the official WMT 2010 11 development
set (News Test 2008, 2051 sentences each lan-
guage) and test set (News Test 2009, 2525 sen-
tenceseachlanguage)areusedforMERTandeval-
uation, respectively. The official scores on News
Test 2010 are given only in the main WMT 2010
paper.
We use a slightly modified tokenization rules
compared to CzEng export format. Most notably,
we normalize English abbreviated negation and
auxiliary verbs (“couldn’t” → “could not”) and
8http://ufal.mff.cuni.cz/czeng/
9http://urd.let.rug.nl/tiedeman/OPUS/EMEA.php
10 Unfortunately, the EMEA corpus is badly tokenized on
the Czech side with fractional numbers split into several to-
kens(e.g. “3,14”). Weattemptedtoreconstructtheoriginal
detokenizedformusingasmallsetofregularexpressions.
11http://www.statmt.org/wmt10
213
attempt at normalizing quotation marks to distin-
guishbetweenopeningandclosingonefollowing
propertypesettingrules.
Therestofourpre-processingpipelinematches
the processing employed in CzEng (Bojar and
Žabokrtský,2009). 12 Weuse“supervisedtruecas-
ing”, meaning that we cast the case of the lemma
totheform,relyingonourmorphologicalanalyz-
ersandtaggerstoidentifypropernames,allother
wordsarelowercased.
4 Experiments
All BLEU scores were computed directly by
Joshua on the News Test 2009 set. Note that
theydifferfromwhattheofficialevaluationscript
wouldreport,duetodifferenttokenization.
4.1 Baseline
Experiments
Thesetofbaselineexperimentswithalltranslation
directions involved running the system on lower-
cased News Commentary corpora. Word align-
ments were computed on 4-character stems (in-
cludingtheen-csandcs-endirections). Atrigram
language model was trained on the target side of
theparallelcorpus.
Direction BLEU
en-cs 0.0905
en-de 0.1114
cs-en 0.1471
de-en 0.1617
en-es 0.1966
en-fr 0.2001
fr-en 0.2020
es-en 0.2025
Table3: LowercasedBLEUscoresofthebaseline
experimentsonNewsTest2009data.
4.2 English-to-Czech
Theextended(non-baseline)English-to-Czechex-
periments were trained on larger parallel and
monolingual data, described in Section 3.2. Note
that the dataset denoted as “Small” still falls into
the constrained task because it only uses CzEng
0.9andtheWMT2010monolingualdata.
12 Duetothesubsequentprocessing,incl. parsing,thetok-
enizationofEnglishfollowsPennTreebenkstyle. Therather
unfortunate convention of treating hyphenated words as sin-
gletokensincreasesourout-of-vocabularyrate.
Wordalignmentswerecomputedonlemmatized
versionoftheparallelcorpus. Hexagramlanguage
modelwastrainedonthemonolingualdata. True-
cased data were used for training, as described
above; the BLEU scores of these experiments in
Table4arecomputedontruecasedsystemoutput.
Setup BLEU
Baseline 0.0905
Small 0.1012
Large 0.1300
Table4: BLEUscores(lowercasedbaseline,true-
cased rest) of the English-to-Czech experiments,
including the baseline experiment with News
Commentary,mentionedearlier.
As for the official evaluation on News Test
2010,weusedtheSmallsetupasour primary sub-
mission ,andtheLargesetupas secondary despite
its better results. The reason was that it was not
clearwhethertheexperimentwouldbefinishedin
timefortheofficialevaluation. 13
An interesting perspective on the three en-cs
models is provided by the feature weights opti-
mizedduringMERT.WecanseeinTable5thatthe
small and relatively weak baseline LM is trusted
less than the most influential translation feature
whileforlargeparalleldataandevenmuchlarger
LMtheweightsaredistributedmoreevenly.
Setup LM Pt0 Pt1 Pt2 WP
Baseline 1.0 1.55 0.51 0.63 −2.63
Small 1.0 1.03 0.72 −0.09 −0.34
Large 1.0 0.98 0.97 −0.02 −0.82
Table5: Featureweightsarerelativetotheweight
of LM, the score by the language model. Then
there are the three translation features: Pt0 =
P(e|f), Pt1 = Plex(f|e) and Pt2 = Plex(e|f).
WP isthewordpenalty.
4.3 Efficiency
Themachinesonwhichtheexperimentswerecon-
ducted are 64bit Intel Xeon dual core 2.8 GHz
CPUswith32GBRAM.
Word alignment of each baseline corpus took
about 1 hour, time needed for data preprocessing
13 In
fact, it was not finished in time. Due to a failure of
aMERTrun,weusedfeatureweightsfromtheprimarysub-
missionforthesecondaryone,too.
214
andtrainingofthelanguagemodelwasnegligible.
Grammar extraction took about four hours but it
could be parallelized. For decoding the test data
were split into 20 chunks that were processed in
parallel. OneMERTiteration,includingdecoding,
tookfrom30minutesto1hour.
Training the large en-cs models requires more
carefulengineering. Thegrammarextractioneas-
ily consumes over 20 GB memory so it is impor-
tant to make sure Java really has access to it. We
parallelized the extraction in the same way as we
haddonewiththedecoding;evenso,about5hours
were needed to complete the extraction. The de-
coder now must use the SWIG-linked SRILM li-
brarybecauseJava-basedlanguagemodelingistoo
slowandmemory-consuming. Otherwise,thede-
codingtimesarecomparabletothebaselineexper-
iments.
5 Conclusion
We have described the hierarchical phrase-based
SMT system we used for the WMT 2010 shared
task. For English-to-Czech translation, we dis-
cussedexperimentswithlargedatafromthepoint
of view of both the translation accuracy and effi-
ciency.
Thishasbeenourfirstattempttoswitchtohier-
archicalSMTandwehavenotgonetoofarbeyond
just putting together the infrastructure and apply-
ingittotheavailabledata. Nevertheless,ouren-cs
experimentsnotonlyconfirmthatmoredatahelps;
intheSmallandLargesetup,thedatawasnotonly
larger than in Baseline, it also underwent a more
refined preprocessing. In particular, we took ad-
vantageoftheCzengcorpusbeinglemmatizedto
produce better word alignment; also, the truecas-
ing technique helped to better target named enti-
ties.
Acknowledgements
The work on this project was supported by the
grantMSM0021620838bytheCzechMinistryof
Education.
References
Ondřej Bojar and Zdeněk Žabokrtský. 2009. Czeng
0.9: Large parallel treebank with rich annotation.
The Prague Bulletin of Mathematical Linguistics ,
92:63–83.
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. In Technical report TR-10-98, Computer
Science Group , Harvard, MA, USA, August. Har-
vardUniversity.
David Chiang. 2007. Hierarchical Phrase-
Based Translation. Computational Linguistics ,
33(2):201–228.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Proceedings of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing ,
pages 181–184, Los Alamitos, California, USA.
IEEEComputerSocietyPress.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ’03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology , pages 48–54, Morristown, NJ, USA.
AssociationforComputationalLinguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch,MarcelloFederico,NicolaBertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
RichardZens, ChrisDyer, OndřejBojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics Compan-
ionVolumeProceedingsoftheDemoandPosterSes-
sions ,pages177–180,Praha,Czechia,June.Associ-
ationforComputationalLinguistics.
Zhifei Li, Chris Callison-Burch, Sanjeev Khudanpur,
and Wren Thornton. 2009. Decoding in Joshua:
Open Source, Parsing-Based Machine Translation.
The Prague Bulletin of Mathematical Linguistics ,
91:47–56,1.
FranzJosefOchandHermannNey. 2003. Asystematic
comparison of various statistical alignment models.
Computational Linguistics ,29(1):19–51.
FranzJosefOch. 1999. Anefficientmethodfordeter-
miningbilingualwordclasses. In Proceedings of the
NinthConferenceoftheEuropeanChapteroftheAs-
sociation for Computational Linguistics (EACL’99) ,
pages71–76,Bergen,Norway,June.Associationfor
ComputationalLinguistics.
AndreasStolcke. 2002. Srilm–anextensiblelanguage
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing , Den-
ver,Colorado,USA.
JörgTiedemann. 2009. Newsfromopus–acollection
ofmultilingualparallelcorporawithtoolsandinter-
faces. In RecentAdvancesinNaturalLanguagePro-
cessing (vol. V) ,pages237–248.JohnBenjamins.
Omar F. Zaidan. 2009. Z-mert: A fully configurable
open source tool for minimum error rate training of
machinetranslationsystems. The Prague Bulletin of
Mathematical Linguistics ,91:79–88.
215

