<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>S Bannerjee</author>
<author>A Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments</title>
<date>2005</date>
<booktitle>In Proceedings of Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, 43 rd Annual Meeting of the Association of Computational Linguistics (ACL-2005</booktitle>
<pages>65--73</pages>
<contexts>
<context>gnition (Information Access Division, 2007). BLEU scores (Papineni et al, 2002) were calculated for MT. M T performance was also measured by calculating METEOR and Translation Edit Rate (TER) scores (Bannerjee &amp; Lavie, 2005; Snover et al., 2005). TER was calculated using TerCom version 6b. METEOR normalization was modified to handle Arabic text. For all three languages, METEOR was run in the mode where it scores only ex</context>
</contexts>
<marker>Bannerjee, Lavie, 2005</marker>
<rawString>Bannerjee, S. and A. Lavie (2005). METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, 43 rd Annual Meeting of the Association of Computational Linguistics (ACL-2005), pp. 65-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Belvin</author>
<author>S Riehemann</author>
<author>K Precoda</author>
</authors>
<title>A Fine-Grained Evaluation Method for Speech-to-Speech Machine Translation Using Concept Annotations</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>1427--1430</pages>
<marker>Belvin, Riehemann, Precoda, 2004</marker>
<rawString>Belvin, R., Riehemann, S., and K. Precoda. (2004). A Fine-Grained Evaluation Method for Speech-to-Speech Machine Translation Using Concept Annotations. In Proceedings of LREC 2004, pp. 1427-1430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Condon</author>
<author>J Phillips</author>
<author>C Doran</author>
<author>J Aberdeen</author>
<author>D Parvaz</author>
<author>B Oshika</author>
<author>Sanders</author>
</authors>
<title>Applying automated metrics to speech translation dialogs</title>
<date>2008</date>
<booktitle>In Proceedings of LREC</booktitle>
<marker>Condon, Phillips, Doran, Aberdeen, Parvaz, Oshika, Sanders, 2008</marker>
<rawString>Condon, S., Phillips, J., Doran, C., Aberdeen, J., Parvaz, D., Oshika, B.,  Sanders , G. and C. Schlenoff. (2008) Applying automated metrics to speech translation dialogs.  In Proceedings of LREC 2008.</rawString>
</citation>
<citation valid="true">
<title>Information Access Division (2007). Tools – Evaluation T o o l s</title>
<note>http://www.nist.gov/speech/tools</note>
<marker></marker>
<rawString>Information Access Division (2007). Tools – Evaluation T o o l s .  http://www.nist.gov/speech/tools/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Gates Levin</author>
<author>D Lavie</author>
<author>A Pianesi</author>
<author>F Wallace</author>
<author>D Watanabe</author>
<author>T</author>
<author>M Woszczyna</author>
</authors>
<title>Evaluation of a practical interlingua for task-oriented dialogue</title>
<date>2000</date>
<booktitle>In Proceedings of the NAACL-ANLP</booktitle>
<volume>2</volume>
<pages>18--23</pages>
<marker>Levin, Lavie, Pianesi, Wallace, Watanabe, T, Woszczyna, 2000</marker>
<rawString>Levin, L. Gates, D., Lavie, A., Pianesi, F., Wallace, D., Watanabe, T., and M. Woszczyna. (2000). Evaluation of a practical interlingua for task-oriented dialogue. In Proceedings of the NAACL-ANLP 2000 Workshop on Applied interlinguas: practical applications of interlingual approaches to NLP, Volume 2, pp. 18-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<date>2002</date>
<contexts>
<context>nce texts, thus giving English WER values that should be directly comparable to previous large-scale NIST evaluations of automatic speech recognition (Information Access Division, 2007). BLEU scores (Papineni et al, 2002) were calculated for MT. M T performance was also measured by calculating METEOR and Translation Edit Rate (TER) scores (Bannerjee &amp; Lavie, 2005; Snover et al., 2005). TER was calculated using TerCom</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K., Roukos, S., Ward, T., and W-J. Zhu. (2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bleu</author>
</authors>
<title>A method for automatic evaluation of machine translation</title>
<date>2002</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>311--318</pages>
<marker>Bleu, 2002</marker>
<rawString>Bleu: A method for automatic evaluation of machine translation. In Proceedings of ACL 2002, pp. 311-318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sanders</author>
<author>S Bronsart</author>
<author>S Condon</author>
<author>C Schlenoff</author>
</authors>
<title>Odds of successful transfer of low-level concepts: A key metric for bidirectional speech-to-speech machine translation in DARPA’s TRANSTAC program</title>
<date>2008</date>
<booktitle>In Proceedings of LREC</booktitle>
<marker>Sanders, Bronsart, Condon, Schlenoff, 2008</marker>
<rawString>Sanders, G., Bronsart, S., Condon, S., and C. Schlenoff. (2008).  Odds of successful transfer of low-level concepts:  A key metric for bidirectional speech-to-speech machine translation in DARPA’s TRANSTAC program.  In Proceedings of LREC 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Schlenoff</author>
<author>M Steves</author>
<author>B Weiss</author>
<author>M Shneier</author>
<author>A Virts</author>
</authors>
<title>Applying SCORE to field-based performance evaluations of soldier worn sensor technologies</title>
<date>2007</date>
<journal>Journal of Field Robotics, Volume</journal>
<volume>24</volume>
<pages>8--9</pages>
<contexts>
<context> the premise that, in order to get a holistic picture of how a system performs in the field, it must be evaluated at the component level, the system level, and in operationally-relevant environments (Schlenoff et al., 2007). Each of these evaluation types provides insight into different aspects of the performance and value of the test systems. It is only by looking at the results of all of the evaluations that one can </context>
</contexts>
<marker>Schlenoff, Steves, Weiss, Shneier, Virts, 2007</marker>
<rawString>Schlenoff, C., Steves, M., Weiss, B., Shneier, M., and A. Virts. (2007). Applying SCORE to field-based performance evaluations of soldier worn sensor technologies. Journal of Field Robotics, Volume 24, (8-9), pp. 671-698.</rawString>
</citation>
</citationList>
</algorithm>

