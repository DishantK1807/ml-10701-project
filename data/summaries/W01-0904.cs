For example, the Penn Treebank (Marcus et al., 1993; Marcus et al., 1994; Bies et al., 1994) provides a large corpus of syntactically annotated examples mostly from the Wall Street Journal.
H94-1020 J93-2004
While the documentation is clear that the complement/adjunct structure is not explicitly marked (Marcus et al., 1994), the annotation includes a set of labels that relate to the role of a particular constituent in the sentence.
H94-1020
Corpus The systems are applied to examples from the Penn Treebank (Marcus et al., 1993; Marcus et al., 1994; Bies et al., 1994) a corpus of over 4.5 million words of American English annotated with both part-of-speech and syntactic tree information.
H94-1020 J93-2004
the categories assigned to the words) of the output of an unsupervised CG learner that annotated the words of the examples with CG categories and then extracts a probabilistic lexicon (see Watkinson and Manandhar (Watkinson and Manandhar, 2001) for details).
W01-0720
A slightly different approach has been followed by Krotov et al (Krotov et al., 1998), where they extract the grammar from the Penn Treebank like Charniak, but then compact it.
P98-1115
These modifications are made for use with the unsupervised learner (Watkinson and Manandhar, 2000; Watkinson and Manandhar, 2001) to simplify the learning process.
W01-0720
Similarly, Johnson (Johnson, 1998) modifies the labelling of the Penn Treebank, but remains within a CFG framework.
J98-4004
To be exact, we are using the Treebank II version (Bies et al., 1994; Marcus et al., 1994), which attempts to address the problem of complement/adjunct distinction, which previous versions had ignored.
H94-1020
without movement) from the Penn Treebank, so that these could be used to evaluate the results produced by an unsupervised CG lexicon learner (Watkinson and Manandhar, 2000; Watkinson and Manandhar, 2001).
W01-0720
