result included, for example, varlons "chips" phrases as shown m Table 1 The word "ch~ps" occurred 1143 times m this corpus, and the table shows that thin word m semantically very amblguons In word associatmns, It can refer to food, computer components, abstract concepts, etc By incorporating these conocatlons, we can dlsamblguate dtfferent meamngs of "chips" Secondly, as the recent Message Understanding Conference (MUC-6) showed (Adv, 1995), the accuracy and robustness of name extraction has reached a mature level, equahng the level of human performance m accuracy (lind-90%) and exceeding human speed by many thousands of times We employed SRA's NameTag TM (Krupka, 1995) to tag the aforementioned corpus with names of people, entztIes, and places, and derived a baseline database for tffIdf calculation In the database, we not only treated multi-word names (e g, "Ball Clinton") as single tokens but also dmamblguated the semantic types of names so that, for instance, the company "Ford" 67 ts treated separately from President "Ford" Our approach is thus different from (Kupiec, Pedersen, and Chen, !995) where only capitalization reformation was used to identify and group various types of proper names 2.1.2 Acquiring Knowledge of the Domain In knowledge-based summarization approaches, the biggest challenge ts to acquire enough domain knowledge to create conceptual representations for a text Though summarization from conceptual representation has many advantages (as discussed m Section 1), extracting such representations constrains a system to domain dependency and is too knowledgeintensive for our approach Instead, we took an automatic and robust approach where we acqmre some domain knowledge from a large corpus and incorporate that knowledge as summarization features m the system We incorporated corpus knowledge m three ways, that is, by using a large corpus baseline to calculate'ldf values for selecting signature words, by denying collocations statistically from a large corpus, and by creating a word association index derived from a large corpus (Jmg and Croft, 1994) With thin method, the system can automatically adapt to each dmtmct domain, hke newspapers vs legal documents without manually developing domain knowledge Domain knowledge is embraced in szgnature words, which indicate key concepts of a given-document, in colIocat:on phrases, which provide richer key concepts than single-word key concepts (e g "appropriations bill," "ommbus bill," "brady ball," "reconciliation bill," "crime bill," "stopgap bdl,", etc ), and in their assoczated words, which are clusters of dommn related terms (e g, "Bayer" and "aspirin," "Columbia Raver" and "gorge," "Dead Sea" and "scrolls") 2.i.3 Recognizing sources of Discourse Cohesion and Coherence Past research (Pmce, 1990) has described the negative impact on abstract quality of fathng to perform some type of discourse processing Since dincourse knowledge (e g, effective anaphora resolution and text segmentation) cannot currently be automatlcally acquired easily wlth high accuracy and robustness, heuristic techniques are often employed in summarization systems to suppress sentences with interdependent cohesive markers However, there are several shallower but robust methods we can employ now to acquire some discourse knowledge Namely, we exploit the dmcourse features of lexlcal items within a text by using name aliases, synonyms, and morphological variants Within a document, subsequent references to full names are often aliases Thus, linking name aliases provides some indication as to which sentences are interrelated, as shown below The Institutional Revolutionary Parry, or PRI, capped sis landmark assembly to reform,tself w,th a.Nourish of pomp and prom,ses Among the measures coming out of the assembly's fiercest pubhc debate, zn which party members rose up agaznst the,r leadership Saturday nlght, are new requsrements for future PRI pres-,denttal cand, dates, quahficatwns that netther ~eddlo nor any of Mezzco's prevzous four pres,dents would have met The NameTag name extractxon tool discussed m the previous section performs hnkmg of name aliases within a document such as "Albnght" to "Madeleme Albnght," "U S" to "Umted States," and "IBM" for "International Business Machine" We used tlus tool to link full names and.
M95-1018
of the works of (Kuplec, Pedersen, and Chen, 1995) and (Brandow, Mltze, .and Ran, 1995), and advances summarmatlon technology by applynag corpus-based statistical NLP teehmques, robust information extraction, and readily avaalable on-hne resources Our prehxmnary experiments with combining different summarization features have been reported, and our current effort to learn to combine these features to produce the best summaries has been described The features derived by these robust NLP techmques were also utihzed m presentmg multiple summary.vtews to the user m a novel way References Advanced Research Projects Agency 1995 Proceed:rigs of S:zth Message Understanding Conference (MUC-6) Morgan Kanfmann Pubhshers Brandow, Ron, Karl Mltze, and Lisa Ran 1995 Automatic condensation of electromc pubhcatlous by sentence selection Information Processing and • Management, 31, forthcoming .Bull, Eric 1993 A Comps-based Approach to Language Learning Ph D thesm, Umverslty of Pennsylvania Church, Kenneth and Patrick Hanks 1990 Word • Aesoclatlon Norrns, Mutual Information, and Lexicography Computational Lmgmstscs, 16(1) Church, Kenneth W 1995 One term or two 9 In Proceedings of the 17th Annual International SIGIR Conference on Research and Development In Informatzon Retrzeral, pages 310-318 Edmundson, H P 1969 New methods m automatic abstracting Journal of the ACM, 16(2) 264-228 Fum, Dando, Glovanm Gmda, and Carlo Tasso 1985 Evalutatmg Importance A step towards text surnmarlzatlon In I3CAI85, pages 840-844IJCAi, AAAI Hahn, Udo 1990 Topic parsing Accounting for text macro structures m full-text analysm In format:on Processing and Management, 26(1)135170 Harman, Donna 1991 How effective is suttixang ~ Journal of the Amerlcan Sot:cry for Informatwn Sc:ence, 42(1) 7-15 Harman, Donna 1996 Overview of the fifth text retrieval conference (tree-5) In TREC-5 Conference Proceedings Jmg, Y and B Croft 1994 An Assoc:atwn Thesaurns for Informatzon Retrseval Umass Techmcal Report 94-I7 Center for Intelligent Information Retrieval, University of Massachusetts Johnson, F C, C D Prate, W J Black, and A P Neal 1993.
J90-1003
In Knorz, Krause, and WomserHacker, edttors, Informatwn Retrieval 'g3, pages 9-26 Jones, Karen Sparck 1995 Dmcourse modeling for automahc surnmarms In E Hajlcova, M Cervenka, O Leskn, and P Sgall, editors, Prague ~ : gmsttc Circle Papers, volume 1, pages 201-227 Krupka, George 1995 SRA Descnphon of the SRA System as Used for MUC-6 In Proceedrags of $:z'th Message Understanding Conference (MUC-~) Kuplec, Juhem, Jan Pedersen, and Francme Chert 1995 A trmnable document summarizer In Procee&ngs of the 18th Annual Internatwnai SIGIR Conference on Research and Development :n Informatwn Retrzeval, pages 68-73 Luhn, H P 1958 The automatic creation of hterature abstracts In IBM J Research Development, volume 2, pages 159-165 Maybury, Mark T 1995 Automated even summarxzatlon techmques In B Endres-Nlggemeyer, J Hobbs, and Karen Sparck Jones, editors, Summarizing Text for Intelhgent Commun:cat,on, pages 101-149 McKeown, Kathleen and Dragomar Radev 1995 Generating summaries of mulhple news articles In Proceedings of the 18th Annual Internat:onal SIGIR Conference on Research and Development :n In format:on, pages 74-78 Make, Seljl, Etsuo Itho, Kenj10no, and Kazuo Surmta 1994 A full text retrieval system with a dynannc abstract generation function In Proceed:ngs of 17th Annual Internatwnal ACM S1GIR Conference on Research and Development :n Informatwn Retrieval, pages 152-161 Miller, George, Richard Beekwith, Chnshane Fell" baum, Derek Gross, and Katherine Miller 1990 Five papers on WordNet Technical Report CSL Report 43, Cogmhve Science Laboratory, Princeton Umverslty . Pmce, C 1990 Constructing hterature abstracts by computer Techmques and prospects Informa::on Processing and Management, 26(1) 171-186 Prate, C and P A Jones 1993 The ldenhficahon of important concepts m lughly structured techmeal papers In Proceedings of the 16th Annual Internatwnal A CM SIGIR Conference of Research and Development m Inforraatzon Retrieval, pages 6978 Qumlan, J Ross 1993 C4 5 Programs for Machine Learmng Morgan Kaufmann Publmhers Rath, G J, A Restock, and T R Savage 1961 The formation of abstracts by the selechon of sentenees Amer:can Doeumentatwn, 12(2) 139-143 Rau, Lma F, Paul S Jacobs, and Un Zermk 1989 Information extraction and text surnmanzatlon umng hngmstlc knowledge acqmslhon Informa:son Processing and Management, 25(4) 419-428 Relmer, Ulrich and Udo Hahn 1988 Text condensahon as knowledge base ahstractmn In IEEE • Conference on AI Apphcatwns, pages 338-344 Rxlotf, Ellen 1995 A corpus-based approach to dommn-speclfic text surnmanzatlon In B Endres-Nlggemeyer, J Hobbs, and K Sparek Jones, e&tors, Summarizing TeE/or lntelhgent Commumcatwn, pages 69-84 Rush, J E, R Salvador, and A Zamora 1971 Automatic abstracting and mdemng Produchon of m&eahve abstracts by.
M95-1018
robust mforrmatlon extractlon, and readlly-avmlable on-hne NLP resources These techtuques and resources allow us to create a richer indexed source of Imgmstlc and domain knowledge than other frequency approaches Our approach attempts to apprommate text dlscourse structure through these multlple layers of mformatlon, ohtinned from automated methods m contrast to labor-lntenslve, discourse-based approaches Moreover, our planned training methodology will also allow us to explmt thin productlve infrastructure m ways whlch model human performance whde avoidmg hand-crafting domain-dependent rules of the knowledge-based approaches Our ultlmate goal m to make our summarlzatlon system scalable and portable by learning summarization rules from easily extractable text features 2 System Description Our summarization system DlmSum consmts of the Summarization Server and the Summarlzatzon Chent The Server extracts features (the Feature Extractor) from a document using various robust NLP techmques, described In Sectzon 2 1, and combines these features (the Feature Combiner) to basehne multiple combinations of features, as described m Section 2 2 Our work m progress to automattcally tram the Feature Combiner based upon user and apphcatlon needs m presented in Section 2 2 2 The Java-based Chent, which wdl be dmcnssed In Section 4, provides a graphical user interface (GUI) for the end user to cnstomlze the summamzatlon preferences and see multiple views of generated sumInarles 2.1 Extracting Stlmmarization Features In this section, we describe how we apply robust NLP technology to extract summarization features Our goal IS to add more mtelhgence to frequencybased approaches, to acqmre domain knowledge In a more automated fashion, and to apprommate text structure by recogmzing sources of dmcourse cohesion and coherence 2.1.1 Going Beyond a Word Frequency-based summarization systems typically use a single word stnng as a umt for counting frequencies Whde such a method IS very robust, it ignores the semantic content of words and their potential membership m multi-word phrases For example, zt does not dmtmgumh between "bill" m "Bdl Table 1 Collocations with "chlps" {potato tortdla corn chocolate b~gle} chips {computer pentmm Intel macroprocessor memory} chips {wood oak plastlc} cchlps bsrgmmng clups blue clups mr chips Clmton" and "bill" in "reform bill" This may introduce noise m frequency counting as the same strmgs are treated umformly no matter how the context may have dmamblguated the sense or regardless of membership in multl-word phrases For DlrnSum, we use term frequency based on tf*Idf (Salton and McGdl, 1983, Brandow, Mitze, and Rau, 1995) to derive ssgnature words as one of the summarization features If single words were the sole basra of countmg for our summarization application, nome would be introduced both m term frequency and reverse document frequency However, recent advances in statmtlcal NLP and information extraction make it possible to utilize features which go beyond the single word level Our approach is to extract multi-word phrases automatlcally with high accuracy and use them as the basic unit in the summarization process, including frequency calculation Ftrst, just as word association methods have proven effective m lemcal analysis, e g (Church and Hanks, 1990), we are exploring whether frequently occurring Collocatlonal reformation can improve on simple word-based approaches We have preprocessed about 800 MB of LA tlmes/Wastnngton Post newspaper articles nsmg a POS tagger (Bnll, 1993) and derived two-word noun collocations using mutual information The.
J90-1003
