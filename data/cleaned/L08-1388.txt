<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>F Burkhardt</author>
<author>A Paeschke</author>
<author>M Rolfes</author>
<author>W Sendlmeier</author>
<author>B Weiss</author>
</authors>
<title>A database of german emotional speech</title>
<date>2005</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<contexts>
<context> the face of the avatar is shown to the users for the first time. In general, it is to say that the emotions expressed by the users are more moderate than artificial emotions played by actors, as in (Burkhardt et al., 2005). However, weconsiderthesemoderateemotionsasmorerealistic and common in human computer interaction and therefore very useful for affective computing tasks. 5. Collected Data The corpus consists of 75</context>
</contexts>
<marker>Burkhardt, Paeschke, Rolfes, Sendlmeier, Weiss, 2005</marker>
<rawString>F. Burkhardt, A. Paeschke, M. Rolfes, W. Sendlmeier, and B. Weiss. 2005. A database of german emotional speech. In Proceedings of Interspeech 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
<author>S Ashby</author>
<author>S Bourban</author>
<author>M Flynn</author>
<author>M Guillemot</author>
<author>T Hain</author>
<author>J Kadlec</author>
<author>V Karaiskos</author>
<author>W Kraaij</author>
<author>M Kronenthal</author>
<author>G Lathoud</author>
<author>M Lincoln</author>
<author>A Lisowska</author>
<author>I McCowan</author>
<author>W Post</author>
<author>D Reidsma</author>
<author>P Wellner</author>
</authors>
<title>The AMI meetings corpus</title>
<date>2005</date>
<booktitle>In Proceedings of the Measuring Behavior</booktitle>
<contexts>
<context>At present, research on multi-party interaction is very popular especially in the context of the meeting scenario. Various corpora have been published, e.g. the ICSI (Janin et al., 2003) and the AMI (Carletta et al., 2005) corpus. The meeting scenario requires intelligent computer systems to enhance and assist the human communication during meetings, however, our aim is to integrate the computer system asanequaldialog</context>
</contexts>
<marker>Carletta, Ashby, Bourban, Flynn, Guillemot, Hain, Kadlec, Karaiskos, Kraaij, Kronenthal, Lathoud, Lincoln, Lisowska, McCowan, Post, Reidsma, Wellner, 2005</marker>
<rawString>J. Carletta, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, W. Kraaij, M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska, I. McCowan, W. Post, D. Reidsma, and P. Wellner. 2005. The AMI meetings corpus. In Proceedings of the Measuring Behavior 2005 symposium on ”Annotating and measuring Meeting Behavior”.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cusano</author>
<author>G Ciocca</author>
<author>R Schettini</author>
</authors>
<title>Image annotation using SVM</title>
<date>2004</date>
<booktitle>In Internet Imaging IV</booktitle>
<volume>volume</volume>
<contexts>
<context>ames where the person (U1) attends to the system and video frames where the person attends to the human dialog partner (U2). This problem is closely linked to the field of automatic image annotation (Cusano et al., 2004), (Jeon and Manmatha, 2004), where a system automatically assigns metadata in the form of keywords to an image. Here, we train an adaboost classifier (Viola and Jones, 2004) with a small subset of ma</context>
</contexts>
<marker>Cusano, Ciocca, Schettini, 2004</marker>
<rawString>C. Cusano, G. Ciocca, and R. Schettini. 2004. Image annotation using SVM. In Internet Imaging IV, volume SPIE. M. Hassenzahl, M. Burmester, and F. Koller.</rawString>
</citation>
<citation valid="true">
<title>AttrakDiff: Ein Fragebogen zur Messung wahrgenommener hedonischer und pragmatischer Qualit¨at</title>
<date>2003</date>
<journal>In J. Ziegler &amp; G. Szwillus (Hrsg.), Mensch &amp; Computer</journal>
<booktitle>Interaktion in Bewegung</booktitle>
<pages>187--196</pages>
<marker>2003</marker>
<rawString>2003. AttrakDiff: Ein Fragebogen zur Messung wahrgenommener hedonischer und pragmatischer Qualit¨at. In J. Ziegler &amp; G. Szwillus (Hrsg.), Mensch &amp; Computer 2003. Interaktion in Bewegung , pages 187–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K S Hone</author>
<author>R Graham</author>
</authors>
<title>Towards a tool for the subjective assessment of speech system interfaces (sassi</title>
<date>2000</date>
<journal>Nat. Lang. Eng</journal>
<pages>6--3</pages>
<contexts>
<context>ctivenessaswellasthepragmaticandhedonicqualityofthesystem. To evaluate the direct interaction between the human dialogue partner U1 and the computer system, a short version (n=16 items) of the SASSI (Hone and Graham, 2000) questionnaire was selected. Furthermore, data on participants’ technical self assessment was collected. The results show that the evaluation of the system significantly improved from recording sessi</context>
</contexts>
<marker>Hone, Graham, 2000</marker>
<rawString>K. S. Hone and R. Graham. 2000. Towards a tool for the subjective assessment of speech system interfaces (sassi). Nat. Lang. Eng., 6(3-4):287–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Janin</author>
<author>D Baron</author>
<author>J Edwards</author>
<author>D Ellis</author>
<author>D Gelbart</author>
<author>N Morgan</author>
<author>B Peskin</author>
<author>T Pfau</author>
<author>E Shriberg</author>
<author>A Stolcke</author>
<author>C Wooters</author>
</authors>
<title>The ICSI meeting corpus</title>
<date>2003</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP</booktitle>
<pages>364--367</pages>
<contexts>
<context>nication over networked systems. At present, research on multi-party interaction is very popular especially in the context of the meeting scenario. Various corpora have been published, e.g. the ICSI (Janin et al., 2003) and the AMI (Carletta et al., 2005) corpus. The meeting scenario requires intelligent computer systems to enhance and assist the human communication during meetings, however, our aim is to integrate</context>
</contexts>
<marker>Janin, Baron, Edwards, Ellis, Gelbart, Morgan, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters. 2003. The ICSI meeting corpus. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), pages 364–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jeon</author>
<author>R Manmatha</author>
</authors>
<title>Using maximum entropy for automatic image annotation</title>
<date>2004</date>
<booktitle>In CIVR</booktitle>
<pages>24--32</pages>
<contexts>
<context>U1) attends to the system and video frames where the person attends to the human dialog partner (U2). This problem is closely linked to the field of automatic image annotation (Cusano et al., 2004), (Jeon and Manmatha, 2004), where a system automatically assigns metadata in the form of keywords to an image. Here, we train an adaboost classifier (Viola and Jones, 2004) with a small subset of manually annotated image fram</context>
</contexts>
<marker>Jeon, Manmatha, 2004</marker>
<rawString>J. Jeon and R. Manmatha. 2004. Using maximum entropy for automatic image annotation. In CIVR, pages 24–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Scherer</author>
<author>P-M Strauß</author>
</authors>
<title>A Flexible Wizard-ofOz Environment for Rapid Prototyping</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC</booktitle>
<location>Marrakech, Morocco</location>
<contexts>
<context>wizard simulatestheenvisionedfinalsystem’sbehaviourasclosely as possible, replacing the system components that are not yet functional (such as e.g. the speech recogniser) using the tool described in (Scherer and Strauß, 2008). The setup of the system is shown in Figure 2. The human dialogue partners U1 and U2 interact with the system S which is operated by the wizard situated in a different room. U1 is directly translate</context>
</contexts>
<marker>Scherer, Strauß, 2008</marker>
<rawString>S. Scherer and P.-M. Strauß. 2008. A Flexible Wizard-ofOz Environment for Rapid Prototyping. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC), Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hoffmann</author>
<author>W Minker</author>
<author>H Neumann</author>
<author>G Palm</author>
<author>S Scherer</author>
<author>F Schwenker</author>
<author>H Traue</author>
<author>W Walter</author>
<author>U Weidenbacher</author>
</authors>
<title>Wizard-of-Oz Data Collection for Perception and Interaction in Multi-User Environments</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<location>Genova, Italy</location>
<marker>Hoffmann, Minker, Neumann, Palm, Scherer, Schwenker, Traue, Walter, Weidenbacher, 2006</marker>
<rawString>P.-M. Strauß , H. Hoffmann, W. Minker, H. Neumann, G. Palm, S. Scherer, F. Schwenker, H. Traue, W. Walter, and U. Weidenbacher. 2006. Wizard-of-Oz Data Collection for Perception and Interaction in Multi-User Environments. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC), Genova, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Hoffmann</author>
<author>S Scherer</author>
</authors>
<title>Evaluation and User Acceptance of a Dialogue System Using Wizard-of-Oz Recordings</title>
<date>2007</date>
<booktitle>In 3rd IET International Conference on Intelligent Environments</booktitle>
<location>Ulm, Germany</location>
<marker>Hoffmann, Scherer, 2007</marker>
<rawString>P.-M. Strauß , H. Hoffmann, and S. Scherer. 2007. Evaluation and User Acceptance of a Dialogue System Using Wizard-of-Oz Recordings. In 3rd IET International Conference on Intelligent Environments, Ulm, Germany.</rawString>
</citation>
</citationList>
</algorithm>

