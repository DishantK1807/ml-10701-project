Cohn et al. (2009) – – 84.0 –
N09-1062
(2009). Zuidema (2007) incorporates deterministic refine-
D07-1058
c≡(A,i,j) and e≡(A→B C,i,k,j). Also, tmax is the highest scoring parse. Adapted from Petrov and Klein (2007).
N07-1051
tent by Johnson (2002). Later, Zollmann and Sima’an (2005)
J02-1005
ous phrases (Koehn et al., 2003) or syntactic trees
N03-1017
such subcategories (Henderson, 2004; Matsuzaki
P04-1013
to accelerate parsing. Charniak et al. (2006) in-
N06-1022
been explored in Bod (2001), Collins and Duffy
P01-1010
Klein and Manning (2003) describe a broad set
P03-1054
more complex Petrov and Klein (2008) work. For
D08-1012 D08-1091
(Bod, 1993) is a particular instance of the general
E93-1006
4The difference is that Goodman (1996a) collapses our
P96-1024 W96-0214
Zuidema (2007) – – 83.8star 26.9star
D07-1058
Johnson (2005), Petrov and Klein (2007)).9
N07-1051
tion (Zuidema, 2007; Cohn et al., 2009; Post and
D07-1058 N09-1062
system results obtained by Zuidema (2007), Cohn
D07-1058
et al., 2006). Cohn et al. (2009) also uses this lexicon.
N09-1062
mar. Petrov and Klein (2007) propose a multi-
N07-1051
Rens Bod. 1993. Using an Annotated Corpus as a
E93-1006
Mark Johnson. 2002. The DOP Estimation Method Is
J02-1005
Post and Gildea (2009) 82.6 – – –
P09-2012
Rens Bod. 2001. What is the Minimal Set of Frag-
P01-1010
Philip Resnik. 1992. Probabilistic Tree-Adjoining
C92-2065
head) information. Charniak and Johnson (2005)
P05-1022
Eugene Charniak and Mark Johnson. 2005. Coarse-
P05-1022
et al. (2009) and Post and Gildea (2009).
P09-2012
Johnson (1998) presents the particularly simple
J98-4004
parser. Zuidema (2007) showed a slight improve-
D07-1058
2Zuidema (2007), Cohn et al. (2009), Post and Gildea
D07-1058 N09-1062
(Galley et al., 2004; Chiang, 2005; Deneefe and
N04-1035 P05-1033
(Resnik, 1992; Bod, 1993) wherein each subgraph
C92-2065 E93-1006
et al., 2005; Petrov et al., 2006). In this paper,
P06-1055
(2007), Petrov et al. (2008)), that a certain amount
D08-1012 D08-1091
David Chiang. 2005. A Hierarchical Phrase-Based
P05-1033
Bod, 1993; Goodman, 1996a; Chiang, 2003) and
E93-1006 P96-1024 W96-0214
Eugene Charniak, Mark Johnson, et al. 2006. Multi-
N06-1022
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
N03-1017
tic refinement of Klein and Manning (2003) (Section 5.3).
P03-1054
Michael Collins and Nigel Duffy. 2002. New Ranking
P02-1034
Joshua Goodman. 1996a. Efficient Algorithms for
P96-1024 W96-0214
Mark Johnson. 1998. PCFG Models of Linguistic
J98-4004
80.1% by Petrov and Klein (2008).14
D08-1012 D08-1091
markovization. starResults from Klein and Manning (2003).
P03-1054
3Including Collins (1999), Charniak and Johnson (2005),
P05-1022
Petrov and Klein (2007).
N07-1051
Slav Petrov and Dan Klein. 2007. Improved Infer-
N07-1051
Willem Zuidema. 2007. Parsimonious Data-Oriented
D07-1058
man (1996b), Petrov and Klein (2007), and Mat-
N07-1051
tree kernel approaches (Collins and Duffy, 2002).
P02-1034
Petrov and Klein (2007) 90.6 39.1 90.1 37.1
N07-1051
James Henderson. 2004. Discriminative Training of
P04-1013
Joshua Goodman. 1996b. Parsing Algorithms and
P96-1024 W96-0214
Matt Post and Daniel Gildea. 2009. Bayesian Learning
P09-2012
Eugene Charniak. 2000. A Maximum-Entropy-
A00-2018
or lexicalization (Collins, 1999; Charniak, 2000).
A00-2018
ous work (Charniak et al. (1998), Petrov and Klein
W98-1115
tation (Johnson, 1998; Klein and Manning, 2003)
J98-4004 P03-1054
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
D08-1012 D08-1091
Slav Petrov and Dan Klein. 2008. Sparse Multi-Scale
D08-1012 D08-1091
ments inspired by Klein and Manning (2003).
P03-1054
(Zuidema, 2007; Cohn et al., 2009; Post and
D07-1058 N09-1062
Dan Klein and Christopher Manning. 2003. Accurate
P03-1054
