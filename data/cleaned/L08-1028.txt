<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>D Abercrombie</author>
</authors>
<title>Studies in phonetics and linguistics</title>
<date>1963</date>
<publisher>University Press</publisher>
<location>London: Oxford</location>
<contexts>
<context>bridge and Nottingham Corpus of Spoken Discourse, and spoken elements of the BNC; British National Corpus). Due to the fact that ‘we speak with our vocal organs, but we converse with our whole body’ (Abercrombie, 1963: 55), it is appropriate to call for a new generation of corpora to be developed, to allow for a more comprehensive view of the characteristics of language ‘beyond the text’ to be rendered. So whilst </context>
</contexts>
<marker>Abercrombie, 1963</marker>
<rawString>Abercrombie D. (1963). Studies in phonetics and linguistics. London: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Adolphs</author>
</authors>
<title>Corpus and Context: investigating pragmatic functions in spoken discourse</title>
<date>2008</date>
<location>Amsterdam: John Benjamins</location>
<contexts>
<context>er step it will then be possible to develop an integrated coding scheme which includes both verbal and gestural properties. This research is on-going (for preliminary results please refer to Carter &amp; Adolphs, 2008; Adolphs, 2008; Knight et al., 2008). In an attempt to mark-up the visual ‘mode’ of the data within DRS we have used a novel technique by which gestures are recognised and annotated by the system in </context>
</contexts>
<marker>Adolphs, 2008</marker>
<rawString>Adolphs, S. (In press, 2008) Corpus and Context: investigating pragmatic functions in spoken discourse. Amsterdam: John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Adolphs</author>
<author>R Carter</author>
</authors>
<title>Beyond the word: New challenges in analysing corpora of spoken English</title>
<date>2007</date>
<journal>European Journal of English Studies</journal>
<volume>11</volume>
<marker>Adolphs, Carter, 2007</marker>
<rawString>Adolphs, S. &amp; Carter, R. (2007). Beyond the word: New challenges in analysing corpora of spoken English. European Journal of English Studies 11(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Argyle</author>
</authors>
<title>Bodily Communication</title>
<date>1975</date>
<location>London: Methuen</location>
<contexts>
<context>tate the definition of such gestures across the corpus (for indepth discussions on iconics and other forms of gesticulation, also see studies by Ekman &amp; Friesen, 1969; Kendon, 1979, 1982, 1990, 1994; Argyle, 1975; McNeill, 1985, 1992; Chalwa &amp; Krauss, 1994 and Beattie &amp; Shovelton, 2002). Obviously the analyst would be required to ‘teach’ the tracking system be means of pre-defining the combination of movement</context>
</contexts>
<marker>Argyle, 1975</marker>
<rawString>Argyle, M. (1975). Bodily Communication. London: Methuen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Badre</author>
<author>M Guzdial</author>
<author>S Hudson</author>
<author>P Santos</author>
</authors>
<title>A user interface evaluation using synchronized video, visualizations and event trace data</title>
<date>1995</date>
<journal>Software Quality Journal</journal>
<volume>4</volume>
<contexts>
<context>ata. One final commercial tool supporting real time coding of video is Studiocode (http://www.studiocodegroup.com), this time developed for Apple’s OSX platform. Academic offerings include I-Observe (Badre et al., 1995: 101-113), an early project which used a video tape based system and made use of captured event streams to synchronise time-stamped events with the time-code on a video. The ever poular ANVIL, Develo</context>
</contexts>
<marker>Badre, Guzdial, Hudson, Santos, 1995</marker>
<rawString>Badre, A., Guzdial, M., Hudson, S. and Santos, P. (1995). A user interface evaluation using synchronized video, visualizations and event trace data. Software Quality Journal, 4(2):101–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Beattie</author>
<author>H Shovelton</author>
</authors>
<title>What properties of talk are associated with the generation of spontaneous iconic hand gestures</title>
<date>2002</date>
<journal>British Journal of Social Psychology</journal>
<volume>41</volume>
<pages>403--417</pages>
<contexts>
<context>ndepth discussions on iconics and other forms of gesticulation, also see studies by Ekman &amp; Friesen, 1969; Kendon, 1979, 1982, 1990, 1994; Argyle, 1975; McNeill, 1985, 1992; Chalwa &amp; Krauss, 1994 and Beattie &amp; Shovelton, 2002). Obviously the analyst would be required to ‘teach’ the tracking system be means of pre-defining the combination of movements to be coded as ‘iconic gesture 1’, for example (so perhaps a sequence of</context>
</contexts>
<marker>Beattie, Shovelton, 2002</marker>
<rawString>Beattie, G. &amp; Shovelton, H. (2002). What properties of talk are associated with the generation of spontaneous iconic hand gestures? British Journal of Social Psychology 41, 3: 403-417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Brugman</author>
<author>A Russel</author>
</authors>
<title>Annotating multimedia / multi-modal resources with elan</title>
<date>2004</date>
<booktitle>In LREC</booktitle>
<pages>2065--2068</pages>
<contexts>
<context> for annotating or coding several simultaneous videos on a timeline representation (Burr, 2006: 622-627). Now in its third version ELAN was developed at the Max-PlanckInstitute for Psycholinguistics (Brugman &amp; Russel, 2004: 2065-2068). It is a fairly comprehensive tool for the annotation of video data, primarily in the field of linguistic research. It supports annotation in tiers, what other projects might call tracks,</context>
</contexts>
<marker>Brugman, Russel, 2004</marker>
<rawString>Brugman, H and Russel, A. (2004). Annotating multimedia / multi-modal resources with elan. In LREC 2004, pages 2065–2068.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Burr</author>
</authors>
<title>Vaca: a tool for qualitative video analysis</title>
<date>2006</date>
<booktitle>In CHI ’06: CHI ’06 extended abstracts on Human factors in computing systems</booktitle>
<pages>622--627</pages>
<publisher>ACM</publisher>
<location>New York, NY, USA</location>
<contexts>
<context>ming, panning and rotation of the original video data. Also developed at Stanford University VACA provides a toolkit for annotating or coding several simultaneous videos on a timeline representation (Burr, 2006: 622-627). Now in its third version ELAN was developed at the Max-PlanckInstitute for Psycholinguistics (Brugman &amp; Russel, 2004: 2065-2068). It is a fairly comprehensive tool for the annotation of vi</context>
</contexts>
<marker>Burr, 2006</marker>
<rawString>Burr, B. (2006). Vaca: a tool for qualitative video analysis. In CHI ’06: CHI ’06 extended abstracts on Human factors in computing systems, pages 622– 627, New York, NY, USA, 2006. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Carter</author>
<author>S Adolphs</author>
</authors>
<title>Linking the verbal and visual: New directions for Corpus Linguistics</title>
<date>2008</date>
<journal>Language and Computers special issue ‘Language, People, Numbers</journal>
<volume>64</volume>
<pages>275--291</pages>
<contexts>
<context>n a further step it will then be possible to develop an integrated coding scheme which includes both verbal and gestural properties. This research is on-going (for preliminary results please refer to Carter &amp; Adolphs, 2008; Adolphs, 2008; Knight et al., 2008). In an attempt to mark-up the visual ‘mode’ of the data within DRS we have used a novel technique by which gestures are recognised and annotated by the system in </context>
</contexts>
<marker>Carter, Adolphs, 2008</marker>
<rawString>Carter, R. and Adolphs, S. (2008).  Linking the verbal and visual: New directions for Corpus Linguistics. Language and Computers special issue ‘Language, People, Numbers’, 64; 275-291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Chawla</author>
<author>R M Krauss</author>
</authors>
<title>Gesture and speech in spontaneous and rehearsed narratives</title>
<date>1994</date>
<journal>Journal of Experimental Social Psychology</journal>
<volume>30</volume>
<pages>580--601</pages>
<marker>Chawla, Krauss, 1994</marker>
<rawString>Chawla, P. &amp; Krauss, R. M. (1994). Gesture and speech in spontaneous and rehearsed narratives. Journal of Experimental Social Psychology 30: 580-601.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Conrad</author>
</authors>
<title>Corpus linguistic approaches for discourse analysis</title>
<date>2002</date>
<journal>Annual Review of Applied Linguistics</journal>
<volume>22</volume>
<pages>75--95</pages>
<contexts>
<context>asures of the strength of word associations’ in order to provide the impetus for exploring the characteristics of specific tokens, phrases and patterns of language usage in their co-text and context (Conrad, 2002: 77-83). Examples of such tools appear both commercially and academically. Transana (http://www.transana.org) is perhaps the most widely used, and focuses primarily on transcription of both audio and</context>
</contexts>
<marker>Conrad, 2002</marker>
<rawString>Conrad, S. (2002). Corpus linguistic approaches for discourse analysis. Annual Review of Applied Linguistics 22: 75-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
<author>W Friesen</author>
</authors>
<title>The repertoire of nonverbal behavior: Categories, origins, usage and coding</title>
<date>1969</date>
<journal>Semiotica</journal>
<volume>1</volume>
<pages>49--98</pages>
<contexts>
<context> it would be possible to use the hand tracker to facilitate the definition of such gestures across the corpus (for indepth discussions on iconics and other forms of gesticulation, also see studies by Ekman &amp; Friesen, 1969; Kendon, 1979, 1982, 1990, 1994; Argyle, 1975; McNeill, 1985, 1992; Chalwa &amp; Krauss, 1994 and Beattie &amp; Shovelton, 2002). Obviously the analyst would be required to ‘teach’ the tracking system be mea</context>
</contexts>
<marker>Ekman, Friesen, 1969</marker>
<rawString>Ekman, P. &amp; Friesen, W. (1969). The repertoire of nonverbal behavior: Categories, origins, usage and coding. Semiotica 1, 1: 49-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Evans</author>
<author>A Naeem</author>
</authors>
<title>Using visual tracking to link text and gesture in studies of natural discourse</title>
<date>2007</date>
<booktitle>Online Proceedings of the Cross Disciplinary Research Group Conference ‘Exploring Avenues to Cross-Disciplinary Research</booktitle>
<volume>7</volume>
<institution>University of Nottingham</institution>
<contexts>
<context>r are seen in figures 2 and 3. The tracking algorithm reports the position of, for example, the speaker’s mouth in relation to their eyes, in each frame (as seen in figure 2, for more information see Evans &amp; Naeem, 2007). The circular nodes seen in figure 2 are the tracking targets (with a pre-defined granularity), which have the flexibility to allow the user to adjust the size of the tracked locations in relation t</context>
</contexts>
<marker>Evans, Naeem, 2007</marker>
<rawString>Evans, D. and Naeem, A. (2007). “Using visual tracking to link text and gesture in studies of natural discourse”, Online Proceedings of the Cross Disciplinary Research Group Conference ‘Exploring Avenues to Cross-Disciplinary Research’, November 7, University of Nottingham.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Gu</author>
</authors>
<title>Multimodal text analysis: A corpus linguistic approach to situated discourse</title>
<date>2006</date>
<journal>Text and Talk</journal>
<volume>26</volume>
<pages>127--167</pages>
<marker>Gu, 2006</marker>
<rawString>Gu, Y. (2006). Multimodal text analysis: A corpus linguistic approach to situated discourse. Text and Talk 26, 2: 127-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kendon</author>
</authors>
<title>Some emerging features of face-toface interaction studies</title>
<date>1979</date>
<journal>Sign Language Studies</journal>
<volume>22</volume>
<pages>722</pages>
<contexts>
<context>o use the hand tracker to facilitate the definition of such gestures across the corpus (for indepth discussions on iconics and other forms of gesticulation, also see studies by Ekman &amp; Friesen, 1969; Kendon, 1979, 1982, 1990, 1994; Argyle, 1975; McNeill, 1985, 1992; Chalwa &amp; Krauss, 1994 and Beattie &amp; Shovelton, 2002). Obviously the analyst would be required to ‘teach’ the tracking system be means of pre-defi</context>
</contexts>
<marker>Kendon, 1979</marker>
<rawString>Kendon, A. (1979). Some emerging features of face-toface interaction studies. Sign Language Studies 22: 722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kendon</author>
</authors>
<title>The organisation of behaviour in face-to-face interaction: observations on the development of a methodology</title>
<date>1982</date>
<booktitle>In Scherer, K.R. &amp; Ekman, P. (eds) Handbook of Methods in Nonverbal Behaviour Research</booktitle>
<publisher>Cambridge University Press</publisher>
<location>Cambridge</location>
<marker>Kendon, 1982</marker>
<rawString>Kendon, A. (1982). The organisation of behaviour in face-to-face interaction: observations on the development of a methodology. In Scherer, K.R. &amp; Ekman, P. (eds) Handbook of Methods in Nonverbal Behaviour Research. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kendon</author>
</authors>
<title>Conducting Interaction. Cambridge</title>
<date>1990</date>
<publisher>Cambridge University Press</publisher>
<contexts>
<context> function in discourse, and how different, complex facets of meaning in discourse are constructed through the interplay of text, gesture and prosody (building on the seminal work of McNeill, 1992 and Kendon, 1990, 1994). 7. Acknowledgements The research on which this paper is based is funded by the UK Economic and Social Research Council (ESRC), e-Social Science Research Node DReSS (www.ncess.ac.uk/nodes/digi</context>
</contexts>
<marker>Kendon, 1990</marker>
<rawString>Kendon, A. (1990) Conducting Interaction. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kendon</author>
</authors>
<title>Do gestures communicate? A review</title>
<date>1994</date>
<journal>Research on Language and Social Interaction</journal>
<volume>27</volume>
<pages>175--200</pages>
<marker>Kendon, 1994</marker>
<rawString>Kendon, A. (1994). Do gestures communicate? A review. Research on Language and Social Interaction 27, 3: 175-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kipp</author>
</authors>
<title>Anvil a generic annotation tool for multimodal dialogue</title>
<date>2001</date>
<booktitle>In Proceedings of the 7th European Conference on Speech Communication and Technology (Eurospeech</booktitle>
<pages>1367--1370</pages>
<location>Aalborg</location>
<contexts>
<context>ideo. The ever poular ANVIL, Developed in 2001 by Michael Kipp at the University of the Saarland was designed as a video annotation tool specifically for the purpose of analysisng multimodal corpora (Kipp, 2001: 13671370). The Diver Project, developed at Stanford University is another tool to support video annotation (Pea et al, 2004: 54-61). Designed to work with a single video, it nevertheless has a uniqu</context>
</contexts>
<marker>Kipp, 2001</marker>
<rawString>Kipp, M. (2001) Anvil a generic annotation tool for multimodal dialogue. In Proceedings of the 7th European Conference on Speech Communication and Technology (Eurospeech), pages 1367–1370, Aalborg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Knight</author>
</authors>
<title>Corpora: The Next Generation’, Part of the AHRC funded online Introduction to Corpus Investigative Techniques, The University of Birmingham. http://www.humcorp.bham.ac.uk</title>
<date>2006</date>
<contexts>
<context> lies is the notion that current spoken corpora have a fundamental shortcoming: the fact that they represent all features of communication in the same format, that of transcribed textual records (see Knight, 2006). Since ‘the reflexivity of gesture, movement and setting is difficult to express in a transcript’ (Saferstein, 2004: 213), current spoken corpora therefore provide the linguist with little utility f</context>
</contexts>
<marker>Knight, 2006</marker>
<rawString>Knight, D. (2006). ‘Corpora: The Next Generation’, Part of the AHRC funded online Introduction to Corpus Investigative Techniques, The University of Birmingham. http://www.humcorp.bham.ac.uk/ Knight, D., Evans, D., Carter, R. and Adolphs, S.</rawString>
</citation>
<citation valid="true">
<title>Redrafting corpus development methodologies: Blueprints for 3rd generation “multimodal, multimedia’’ corpora</title>
<date>2008</date>
<journal>Corpora Journal</journal>
<marker>2008</marker>
<rawString>(Forthcoming, 2008). Redrafting corpus development methodologies: Blueprints for 3rd generation “multimodal, multimedia’’ corpora. Corpora Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McNeill</author>
</authors>
<title>So you think gestures are nonverbal</title>
<date>1985</date>
<journal>Psychological Review</journal>
<volume>92</volume>
<pages>350--371</pages>
<contexts>
<context>ition of such gestures across the corpus (for indepth discussions on iconics and other forms of gesticulation, also see studies by Ekman &amp; Friesen, 1969; Kendon, 1979, 1982, 1990, 1994; Argyle, 1975; McNeill, 1985, 1992; Chalwa &amp; Krauss, 1994 and Beattie &amp; Shovelton, 2002). Obviously the analyst would be required to ‘teach’ the tracking system be means of pre-defining the combination of movements to be coded a</context>
</contexts>
<marker>McNeill, 1985</marker>
<rawString>McNeill, D. (1985). So you think gestures are nonverbal? Psychological Review 92, 3: 350-371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McNeill</author>
</authors>
<title>Hand and Mind</title>
<date>1992</date>
<publisher>The University of Chicago Press</publisher>
<location>Chicago</location>
<contexts>
<context>inguistic form and function in discourse, and how different, complex facets of meaning in discourse are constructed through the interplay of text, gesture and prosody (building on the seminal work of McNeill, 1992 and Kendon, 1990, 1994). 7. Acknowledgements The research on which this paper is based is funded by the UK Economic and Social Research Council (ESRC), e-Social Science Research Node DReSS (www.ncess</context>
</contexts>
<marker>McNeill, 1992</marker>
<rawString>McNeill, D. (1992). Hand and Mind. Chicago: The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Newton</author>
<author>L Sweeney</author>
<author>B Malin</author>
</authors>
<title>Preserving Privacy by De-Identifying Face Images</title>
<date>2005</date>
<journal>IEEE Transactions on Knowledge and Data Engineering</journal>
<volume>17</volume>
<pages>232--243</pages>
<contexts>
<context> difficult to conceal the identities of participants. One traditional method would be to pixellate the video data, obscuring the faces of those involved using a technique such as the one proposed in (Newton et al., 2005). However, anything which obscures the features of the individual consequently reduces the usefulness of the video for understanding non-verbal communication. A more heavyweight approach might be to </context>
</contexts>
<marker>Newton, Sweeney, Malin, 2005</marker>
<rawString>Newton, E.M., Sweeney, L. and Malin, B. (2005). ‘Preserving Privacy by De-Identifying Face Images’, IEEE Transactions on Knowledge and Data Engineering , 17 (2),  pp. 232-243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Pea</author>
<author>M Mills</author>
<author>J Rosen</author>
<author>K Dauber</author>
<author>W Effelsberg</author>
<author>E Hoffert</author>
</authors>
<title>The diver project: Interactive digital video repurposing</title>
<date>2004</date>
<journal>IEEE MultiMedia</journal>
<volume>11</volume>
<contexts>
<context> annotation tool specifically for the purpose of analysisng multimodal corpora (Kipp, 2001: 13671370). The Diver Project, developed at Stanford University is another tool to support video annotation (Pea et al, 2004: 54-61). Designed to work with a single video, it nevertheless has a unique feature, that of socalled ‘dives’, where users can manipulate the viewpoint of a video using a virtual camera viewfinder, a</context>
</contexts>
<marker>Pea, Mills, Rosen, Dauber, Effelsberg, Hoffert, 2004</marker>
<rawString>Pea, R., Mills, M., Rosen, J., Dauber, K., Effelsberg, W., and Hoffert, E. (2004). The diver project: Interactive digital video repurposing. IEEE MultiMedia, 11(1):54–61, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Rodrigues</author>
<author>W Puech</author>
<author>P Meuel</author>
<author>J C Bajard</author>
<author>M Chaumont</author>
</authors>
<title>Face protection by fast selective encryption in a video</title>
<date>2006</date>
<booktitle>The Institution of Engineering and Technology Conference on Crime and Security</booktitle>
<contexts>
<context> of the individual consequently reduces the usefulness of the video for understanding non-verbal communication. A more heavyweight approach might be to apply a selective encryption algorithm such as (Rodrigues et al., 2006) across the faces which can be decrypted to show the original video by those with the appropriate decryption key, while those without it will see only the obscured video. This would allow the videos </context>
</contexts>
<marker>Rodrigues, Puech, Meuel, Bajard, Chaumont, 2006</marker>
<rawString>Rodrigues, J. M., Puech, W., Meuel, P., Bajard, J.C. and Chaumont, M. (2006). ‘Face protection by fast selective encryption in a video’, The Institution of Engineering and Technology Conference on Crime and Security 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Saferstein</author>
</authors>
<title>Digital technology and methodological adaptation: Text on video as a resource for analytical reflexivity</title>
<date>2004</date>
<journal>Journal of Applied Linguistics</journal>
<volume>1</volume>
<pages>197--223</pages>
<contexts>
<context>features of communication in the same format, that of transcribed textual records (see Knight, 2006). Since ‘the reflexivity of gesture, movement and setting is difficult to express in a transcript’ (Saferstein, 2004: 213), current spoken corpora therefore provide the linguist with little utility for the exploration of the nonverbal, gestural aspects of interaction (for examples of spoken corpora see the 5 millio</context>
</contexts>
<marker>Saferstein, 2004</marker>
<rawString>Saferstein, B. (2004). ‘Digital technology and methodological adaptation: Text on video as a resource for analytical reflexivity’, Journal of Applied Linguistics 1 (2), pp 197–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Scott</author>
<author>T Johns</author>
</authors>
<date>1993</date>
<publisher>University Press</publisher>
<location>MicroConcord. Oxford: Oxford</location>
<contexts>
<context>ract with both types of annotation in the same manner, applying the same skills and techniques appropriate to the use of traditional corpora when performing an analysis (for examples see Scott, 1999; Scott &amp; Johns, 1993). This automated approach to tracking hand gestures has two significant benefits over the manual analysis undertaken by McNeill and colleagues. Firstly, there is the potential to save a great deal of</context>
</contexts>
<marker>Scott, Johns, 1993</marker>
<rawString>Scott, M. &amp; Johns, T. (1993) MicroConcord. Oxford: Oxford University Press.</rawString>
</citation>
</citationList>
</algorithm>

