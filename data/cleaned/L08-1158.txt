<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>A-L Barab´asi</author>
<author>R Albert</author>
</authors>
<title>Emergence of scaling in random networks</title>
<date>1999</date>
<journal>Science</journal>
<pages>286--509</pages>
<marker>Barab´asi, Albert, 1999</marker>
<rawString>A.-L. Barab´asi and R. Albert. 1999. Emergence of scaling in random networks. Science, 286:509–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Biemann</author>
</authors>
<title>Chinese whispers an efficient graph clustering algorithm and its application to natural language processing problems</title>
<date>2006</date>
<booktitle>In Proceedings of HLTNAACL’06 workshop on TextGraphs</booktitle>
<pages>73--80</pages>
<contexts>
<context>ted by several work on unsupervised induction of POS based on the distributional hypothesis (Finch and Chater, 1992; Sch¨utze, 1993; Sch¨utze, 1995; Gauch and Futrelle, 1994; Clark, 2000; Rapp, 2005; Biemann, 2006b), our main contributions reside in – (a) a comparative study of various approaches to POS tagset induction on Bengali, (b) rigorous linguistic analysis of the word classes and suggestions for a Beng</context>
<context>d to describe syntactic contexts are further called feature words2. The general methodology (Finch and Chater, 1992; Sch¨utze, 1993; Sch¨utze, 1995; Gauch and Futrelle, 1994; Clark, 2000; Rapp, 2005; Biemann, 2006b) for inducing word class information can be outlined as follows, (a) collect global context vectors of target words by counting how often feature words appear in the neighboring positions, and, (b) </context>
<context>esting facts about the nature of word interactions and syntactic patterns. 3. Word Networks The definition and the construction of the word networks presented here are primarily based on the work by (Biemann, 2006b). Nevertheless, we also explore some variations while defining the network as well as their construction for Bengali data. Moreover, we study the topological properties of these networks, which prov</context>
<context>of dimension 4m in which the entries (4i + 1),(4i + 2),(4i + 3) and (4i + 4) correspond to the number of occurrences of the (i − 1)th feature word at the w−2,w−1,w1 and w2 positions respectively. In (Biemann, 2006b), the distributional similarity between two words w and v is defined as sim(w,v) = 11−cos(vectorw,vectorv), where vectorw and vectorv represent the context vectors of the words w and v respectively,</context>
<context> of the similarity could be simply the cosine of the angle between vectorw and vectorv, that is sim(w,v) = cos(vectorw,vectorv). We shall denote the networks constructed using the metric proposed in (Biemann, 2006b) by a prefixed superscript b (for Biemann) and the latter ones by another prefixed superscript c (for cosine). 3.2. Definition and Construction of the Networks The word network is a weighted undirec</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>C. Biemann. 2006a. Chinese whispers an efficient graph clustering algorithm and its application to natural language processing problems. In Proceedings of HLTNAACL’06 workshop on TextGraphs, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Biemann</author>
</authors>
<title>Unsupervised part-of-speech tagging employing efficient graph clustering</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL’06 Student Research Workshop</booktitle>
<pages>7--12</pages>
<contexts>
<context>3.4. Community Structure In order to gain insight into the topology of the network we cluster them using the following two different approaches. Chinese Whispers: The Chinese Whispers (CW) algorithm (Biemann, 2006a) is a non-parametric random-walk based clustering algorithm, where initially each node is in a separate cluster. In every iteration, the nodes propagate information about their current cluster to al</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>C. Biemann. 2006b. Unsupervised part-of-speech tagging employing efficient graph clustering. In Proceedings of COLING/ACL’06 Student Research Workshop, pages 7– 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
</authors>
<title>Inducing syntactic categories by context distribution clustering</title>
<date>2000</date>
<booktitle>Proceedings of CoNLL/LLL’00</booktitle>
<pages>91--94</pages>
<editor>In Claire Cardie, Walter Daelemans, Claire N´edellec, and Erik Tjong Kim Sang, editors</editor>
<contexts>
<context>novel and has been motivated by several work on unsupervised induction of POS based on the distributional hypothesis (Finch and Chater, 1992; Sch¨utze, 1993; Sch¨utze, 1995; Gauch and Futrelle, 1994; Clark, 2000; Rapp, 2005; Biemann, 2006b), our main contributions reside in – (a) a comparative study of various approaches to POS tagset induction on Bengali, (b) rigorous linguistic analysis of the word classes</context>
<context>les, 1991). The words used to describe syntactic contexts are further called feature words2. The general methodology (Finch and Chater, 1992; Sch¨utze, 1993; Sch¨utze, 1995; Gauch and Futrelle, 1994; Clark, 2000; Rapp, 2005; Biemann, 2006b) for inducing word class information can be outlined as follows, (a) collect global context vectors of target words by counting how often feature words appear in the neigh</context>
<context>tze, 1995) use the cosine of the angle between the vectors and Buckshot clustering, (Gauch and Futrelle, 1994) uses cosine on Mutual Information vectors for hierarchical agglomerative clustering and (Clark, 2000) applies Kullback-Leibler divergence. Slightly different variations of the above generic scheme can be found in (Clark, 2003), (Freitag, 2004) and (Dhillon et al., 2003). For small size raw corpora, </context>
</contexts>
<marker>Clark, 2000</marker>
<rawString>A. Clark. 2000. Inducing syntactic categories by context distribution clustering. In Claire Cardie, Walter Daelemans, Claire N´edellec, and Erik Tjong Kim Sang, editors, Proceedings of CoNLL/LLL’00, pages 91–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
</authors>
<title>Combining distributional and morphological information for part of speech induction</title>
<date>2003</date>
<booktitle>In Proceedings of EACL’03</booktitle>
<pages>59--66</pages>
<contexts>
<context> Mutual Information vectors for hierarchical agglomerative clustering and (Clark, 2000) applies Kullback-Leibler divergence. Slightly different variations of the above generic scheme can be found in (Clark, 2003), (Freitag, 2004) and (Dhillon et al., 2003). For small size raw corpora, Bayesian approaches are known to be capable of producing good results (Haghighi and Klein, 2006; Goldwater and Griffiths, 200</context>
</contexts>
<marker>Clark, 2003</marker>
<rawString>A. Clark. 2003. Combining distributional and morphological information for part of speech induction. In Proceedings of EACL’03, pages 59–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L da F Costa</author>
<author>O N Oliveira Jr</author>
<author>G Travieso</author>
<author>F A Rodrigues</author>
<author>P R V Boas</author>
<author>L Antiqueira</author>
<author>M P Viana</author>
<author>L E C da Rocha</author>
</authors>
<title>Analyzing and modeling realworld phenomena with complex networks: A survey of applications</title>
<date>2007</date>
<pages>0711--3199</pages>
<contexts>
<context> to explain the structure, function and evolutionary dynamics of a variety of natural systems found in the domains of biology, economics, physics, social sciences and information sciences. See (da F. Costa et al., 2007) for a survey on applications of networks in various areas. In the context ofsyntax, studieson word collocation networks and syntactic dependency networks have revealed several interesting cross-ling</context>
</contexts>
<marker>Costa, Jr, Travieso, Rodrigues, Boas, Antiqueira, Viana, Rocha, 2007</marker>
<rawString>L. da F. Costa, O. N. Oliveira Jr., G. Travieso, F. A. Rodrigues, P. R. V. Boas, L. Antiqueira, M. P. Viana, and L. E. C. da Rocha. 2007. Analyzing and modeling realworld phenomena with complex networks: A survey of applications. arXiv:0711.3199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sarkar S Dandapat</author>
<author>andA Basu</author>
</authors>
<title>Ahybridmodel for parts-of-speech tagging and its application to Bengali</title>
<date>2004</date>
<journal>International Journal of Information Technology</journal>
<volume>1</volume>
<marker>Dandapat, Basu, 2004</marker>
<rawString>S.Dandapat, S.Sarkar, andA.Basu. 2004. Ahybridmodel for parts-of-speech tagging and its application to Bengali. International Journal of Information Technology, 1(4):169–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S DasguptaandV Ng</author>
</authors>
<title>Unsupervisedpart-of-speech acquisition for resource-scarce languages</title>
<date>2007</date>
<booktitle>In EMNLPCoNLL’07</booktitle>
<pages>218--227</pages>
<contexts>
<context> and Klein, 2006; Goldwater and Griffiths, 2007). However, these approaches rely on a predefined set of tags and a small annotated corpus or a partial lexicon. A further related work is (Dasgupta and Ng, 2007), which proposes an unsupervised morphological analysis to create a soft clustering on word classes in their weakly supervised word class induction system for English and Bengali. 1A tagging dictiona</context>
<context>y standard task completion based evaluation strategy for the current work, we compare the clusters against two gold standard tagsets for Bengali described in (Dandapat et al., 2004) and (Dasgupta and Ng, 2007). 4.1. Tag Entropy Given a word w, a morphological analyzer returns all the possible segmentation of the word w along with the corresponding lexical categories4. For example, the Bengali word kare ha</context>
<context> However, for the purpose at hand, it suffices to have a lexicon with all the inflected forms of the root words and their categories. This is what we perform for the tagset presented in (Dasgupta and Ng, 2007). entropy of the cluster c. For a perfectly cohesive cluster, pi(c) is 1 or 0 for all i, and therefore, TE(c) = 0. For a perfectly incohesive cluster, TE(c) is T. This happens when pi(c) = 0.5 for al</context>
<context>example clusters can be found at http://banglaposclusters.googlepages.com/home tagset defined in (Dandapat et al., 2004). In the third set of experiments, we use the tagset described in (Dasgupta and Ng, 2007) and the dataset made available by the authors (http://www.hlt.utdallas.edu/∼sajib/posDatasets.html) consisting of 5000 Bengali words and their corresponding tags to evaluate our clusters. Since we d</context>
<context>rphological form. Thus, except for the verbs, the different morphological variations of a root word are not placed into different lexical categories. Ontheotherhand, thetagsetdescribedin(Dasgupta and Ng, 2007) consists of only 11 tags that partially covers the lexical categories of Bengali. Nouns are divided into 7 classes based on proper vs. common, singular vs. plural and different case-marker (genitive</context>
</contexts>
<marker>Ng, 2007</marker>
<rawString>S.DasguptaandV.Ng. 2007. Unsupervisedpart-of-speech acquisition for resource-scarce languages. In EMNLPCoNLL’07, pages 218–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I S Dhillon</author>
<author>S Mallela</author>
<author>D S Modha</author>
</authors>
<title>Information-theoretic co-clustering</title>
<date>2003</date>
<booktitle>In Proceedings of KDD’03</booktitle>
<pages>89--98</pages>
<contexts>
<context>rarchical agglomerative clustering and (Clark, 2000) applies Kullback-Leibler divergence. Slightly different variations of the above generic scheme can be found in (Clark, 2003), (Freitag, 2004) and (Dhillon et al., 2003). For small size raw corpora, Bayesian approaches are known to be capable of producing good results (Haghighi and Klein, 2006; Goldwater and Griffiths, 2007). However, these approaches rely on a pred</context>
</contexts>
<marker>Dhillon, Mallela, Modha, 2003</marker>
<rawString>I. S. Dhillon, S. Mallela, and D. S. Modha. 2003. Information-theoretic co-clustering. In Proceedings of KDD’03, pages 89–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ferrer-i-Cancho</author>
<author>R V Sole</author>
</authors>
<title>The small world of human language</title>
<date>2001</date>
<journal>Proceedings of The Royal Society of London. Series B, Biological Sciences</journal>
<volume>268</volume>
<pages>2265</pages>
<contexts>
<context>s and their possible explanations in terms of human cognition. In word collocation networks, words are the nodes and two words are linked if they are neighbors, that is they collocate, in a sentence (Ferrer-i-Cancho and Sole, 2001; Ferreri-Cancho et al., 2007b). Such networks, constructed for various languages, have been found to exhibit small world properties. The average path length between any two nodes is small (around 2 t</context>
</contexts>
<marker>Ferrer-i-Cancho, Sole, 2001</marker>
<rawString>R. Ferrer-i-Cancho and R. V. Sole. 2001. The small world of human language. Proceedings of The Royal Society of London. Series B, Biological Sciences, 268(1482):2261– 2265, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ferrer-i-Cancho</author>
<author>R V Sole</author>
</authors>
<title>Patterns in syntactic dependency networks. Physical Review E</title>
<date>2004</date>
<contexts>
<context>and semantic relationships between the words. This is because syntactic and semantic relations often extend beyond the local neighborhoodofaword. Ferrer-i-Canchoandhisco-authors(Ferreri-Cancho, 2005; Ferrer-i-Cancho and Sole, 2004) defined the syntactic dependency network (SDN) where the words are the nodes and there is a directed edge between two words if in any of the sentences of a given corpus there is a directed dependenc</context>
</contexts>
<marker>Ferrer-i-Cancho, Sole, 2004</marker>
<rawString>R. Ferrer-i-Cancho and R. V. Sole. 2004. Patterns in syntactic dependency networks. Physical Review E, 69(051915).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ferrer-i-Cancho</author>
<author>A Capocci</author>
<author>G Caldarelli</author>
</authors>
<title>Spectral methods cluster words of the same class in a syntactic dependency network</title>
<date>2007</date>
<journal>International Journal of Bifurcation and Chaos</journal>
<volume>17</volume>
<contexts>
<context>undtoexhibit strikingly similar characteristics. All the networks exhibit power-law degree distributions, small world structure, disassortative mixing and a hierarchical organization. More recently, (Ferrer-i-Cancho et al., 2007a) showed that spectral clustering of SDN puts words belonging to the same syntactic categories in the same cluster. Thus, word collocation as well as the syntactic dependency networks unfurl various </context>
</contexts>
<marker>Ferrer-i-Cancho, Capocci, Caldarelli, 2007</marker>
<rawString>R. Ferrer-i-Cancho, A. Capocci, and G. Caldarelli. 2007a. Spectral methods cluster words of the same class in a syntactic dependency network. International Journal of Bifurcation and Chaos, 17(7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>2007b</author>
</authors>
<title>Correlationsintheorganization of large-scale syntactic dependency networks</title>
<booktitle>In Proceedings of HLT/NAACL’07 workshop on TextGraphs, pages65–72. AssociationforComputational Linguistics</booktitle>
<marker>2007b, </marker>
<rawString>R. Ferrer-i-Cancho, A. Mehler, O. Pustylnikov, and A.D´ıaz-Guilera. 2007b. Correlationsintheorganization of large-scale syntactic dependency networks. In Proceedings of HLT/NAACL’07 workshop on TextGraphs, pages65–72. AssociationforComputational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ferrer-i-Cancho</author>
</authors>
<title>The structure of syntactic dependency networks: insights from recent advances in network theory</title>
<date>2005</date>
<booktitle>Problems of quantitative linguistics</booktitle>
<pages>60--75</pages>
<editor>In V. Levickij and G. Altmman, editors</editor>
<marker>Ferrer-i-Cancho, 2005</marker>
<rawString>R. Ferrer-i-Cancho. 2005. The structure of syntactic dependency networks: insights from recent advances in network theory. In V. Levickij and G. Altmman, editors, Problems of quantitative linguistics, pages 60–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Finch</author>
<author>N Chater</author>
</authors>
<title>Bootstrapping syntactic categoriesusingstatisticalmethods. InBackground and Experiments</title>
<date>1992</date>
<booktitle>in Machine Learning of Natural Language: Proceedings of the 1st SHOE Workshop</booktitle>
<pages>229--235</pages>
<institution>Katholieke Universiteit</institution>
<location>Brabant, Holland</location>
<contexts>
<context> word interaction networks. Although the scheme presented here is not essentially novel and has been motivated by several work on unsupervised induction of POS based on the distributional hypothesis (Finch and Chater, 1992; Sch¨utze, 1993; Sch¨utze, 1995; Gauch and Futrelle, 1994; Clark, 2000; Rapp, 2005; Biemann, 2006b), our main contributions reside in – (a) a comparative study of various approaches to POS tagset ind</context>
<context>xts in that sense are often restricted to the most frequent words (Miller and Charles, 1991). The words used to describe syntactic contexts are further called feature words2. The general methodology (Finch and Chater, 1992; Sch¨utze, 1993; Sch¨utze, 1995; Gauch and Futrelle, 1994; Clark, 2000; Rapp, 2005; Biemann, 2006b) for inducing word class information can be outlined as follows, (a) collect global context vectors </context>
<context>the feature words appearing in the immediate neighborhood of a word. The word’s global context is the sum of all its contexts. Clustering consists of a similarity measure and a clustering algorithm. (Finch and Chater, 1992) uses the Spearman Rank Correlation Coefficient and a hierarchical clustering, (Sch¨utze, 1993; Sch¨utze, 1995) use the cosine of the angle between the vectors and Buckshot clustering, (Gauch and Fut</context>
</contexts>
<marker>Finch, Chater, 1992</marker>
<rawString>S. Finch and N. Chater. 1992. Bootstrapping syntactic categoriesusingstatisticalmethods. InBackground and Experiments in Machine Learning of Natural Language: Proceedings of the 1st SHOE Workshop, pages 229–235. Katholieke Universiteit, Brabant, Holland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Freitag</author>
</authors>
<title>Toward unsupervised whole-corpus tagging</title>
<date>2004</date>
<booktitle>In COLING’04</booktitle>
<pages>357</pages>
<contexts>
<context>tion vectors for hierarchical agglomerative clustering and (Clark, 2000) applies Kullback-Leibler divergence. Slightly different variations of the above generic scheme can be found in (Clark, 2003), (Freitag, 2004) and (Dhillon et al., 2003). For small size raw corpora, Bayesian approaches are known to be capable of producing good results (Haghighi and Klein, 2006; Goldwater and Griffiths, 2007). However, thes</context>
</contexts>
<marker>Freitag, 2004</marker>
<rawString>D. Freitag. 2004. Toward unsupervised whole-corpus tagging. In COLING’04, page 357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gauch</author>
<author>R Futrelle</author>
</authors>
<title>Experiments in Automatic Word Class and Word Sense Identification for Information Retrieval</title>
<date>1994</date>
<booktitle>In Proceedings of the 3rd Annual Symposium on Document Analysis and Information Retrieval</booktitle>
<pages>425--434</pages>
<contexts>
<context>d here is not essentially novel and has been motivated by several work on unsupervised induction of POS based on the distributional hypothesis (Finch and Chater, 1992; Sch¨utze, 1993; Sch¨utze, 1995; Gauch and Futrelle, 1994; Clark, 2000; Rapp, 2005; Biemann, 2006b), our main contributions reside in – (a) a comparative study of various approaches to POS tagset induction on Bengali, (b) rigorous linguistic analysis of the</context>
<context>ent words (Miller and Charles, 1991). The words used to describe syntactic contexts are further called feature words2. The general methodology (Finch and Chater, 1992; Sch¨utze, 1993; Sch¨utze, 1995; Gauch and Futrelle, 1994; Clark, 2000; Rapp, 2005; Biemann, 2006b) for inducing word class information can be outlined as follows, (a) collect global context vectors of target words by counting how often feature words appear</context>
<context> Chater, 1992) uses the Spearman Rank Correlation Coefficient and a hierarchical clustering, (Sch¨utze, 1993; Sch¨utze, 1995) use the cosine of the angle between the vectors and Buckshot clustering, (Gauch and Futrelle, 1994) uses cosine on Mutual Information vectors for hierarchical agglomerative clustering and (Clark, 2000) applies Kullback-Leibler divergence. Slightly different variations of the above generic scheme c</context>
</contexts>
<marker>Gauch, Futrelle, 1994</marker>
<rawString>S. Gauch and R. Futrelle. 1994. Experiments in Automatic Word Class and Word Sense Identification for Information Retrieval. In Proceedings of the 3rd Annual Symposium on Document Analysis and Information Retrieval, pages 425–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>T Griffiths</author>
</authors>
<title>A fully bayesian approach to unsupervised part-of-speech tagging</title>
<date>2007</date>
<booktitle>In Proceedings of ACL’07</booktitle>
<pages>744--751</pages>
<contexts>
<context>can be found in (Clark, 2003), (Freitag, 2004) and (Dhillon et al., 2003). For small size raw corpora, Bayesian approaches are known to be capable of producing good results (Haghighi and Klein, 2006; Goldwater and Griffiths, 2007). However, these approaches rely on a predefined set of tags and a small annotated corpus or a partial lexicon. A further related work is (Dasgupta and Ng, 2007), which proposes an unsupervised morph</context>
</contexts>
<marker>Goldwater, Griffiths, 2007</marker>
<rawString>S. Goldwater and T. Griffiths. 2007. A fully bayesian approach to unsupervised part-of-speech tagging. In Proceedings of ACL’07, pages 744–751, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A HaghighiandD Klein</author>
</authors>
<title>Prototype-drivenlearning forsequencemodels</title>
<date>2006</date>
<journal>InProceedingsofHLT-NAACL’06</journal>
<contexts>
<context>neric scheme can be found in (Clark, 2003), (Freitag, 2004) and (Dhillon et al., 2003). For small size raw corpora, Bayesian approaches are known to be capable of producing good results (Haghighi and Klein, 2006; Goldwater and Griffiths, 2007). However, these approaches rely on a predefined set of tags and a small annotated corpus or a partial lexicon. A further related work is (Dasgupta and Ng, 2007), which</context>
</contexts>
<marker>Klein, 2006</marker>
<rawString>A.HaghighiandD.Klein. 2006. Prototype-drivenlearning forsequencemodels. InProceedingsofHLT-NAACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z S Harris</author>
</authors>
<title>Mathematical Structures of Language</title>
<date>1968</date>
<publisher>Wiley</publisher>
<contexts>
<context>n there be a principled and computational approach to this problem of identification of the lexical categories? The answer turns out to be ‘yes’, thanks to the concept of “distributional hypothesis” (Harris, 1968). In fact, this hypothesis is the underlying (implicit or explicit) assumption of all computational approaches to POS tagging which is a very important preprocessing task for several NLP applications</context>
<context>h in turn helps in deciding on a tagset for the language. There are a number of approaches to derive syntactic categories. All of them employ a syntactic version of Harris’ distributional hypothesis (Harris, 1968), which states that words of similar parts of speech can be observed in the same syntactic contexts. Since the function words form the syntactic skeleton of a language and almost exclusively contribu</context>
</contexts>
<marker>Harris, 1968</marker>
<rawString>Z. S. Harris. 1968. Mathematical Structures of Language. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>W G Charles</author>
</authors>
<date>1991</date>
<booktitle>Contextual Correlates of Semantic Similarity. Language and Cognitive Processes</booktitle>
<pages>6--1</pages>
<contexts>
<context>ction words form the syntactic skeleton of a language and almost exclusively contribute to the most frequent words in a corpus, contexts in that sense are often restricted to the most frequent words (Miller and Charles, 1991). The words used to describe syntactic contexts are further called feature words2. The general methodology (Finch and Chater, 1992; Sch¨utze, 1993; Sch¨utze, 1995; Gauch and Futrelle, 1994; Clark, 20</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>G. A. Miller and W. G. Charles. 1991. Contextual Correlates of Semantic Similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E J Newman</author>
</authors>
<title>The structure and function of complex networks</title>
<date>2003</date>
<journal>SIAM Review</journal>
<pages>45--167</pages>
<contexts>
<context>e target words. 2.2. Syntax as a Self-organizing Phenomenon Recently, there has been several studies on the structural patterns of human languages within the framework of complex network theory (see (Newman, 2003) for a review). A complex network is a collection of entities (represented as nodes) and their interactions (represented as links or edges between the nodes). Such networks have been successfully use</context>
</contexts>
<marker>Newman, 2003</marker>
<rawString>M. E. J. Newman. 2003. The structure and function of complex networks. SIAM Review, 45:167–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>A practical solution to the problem of automaticpart-of-speechinductionfromtext</title>
<date>2005</date>
<booktitle>InProceedings of ACL’05 (companion volume</booktitle>
<pages>77--80</pages>
<contexts>
<context> been motivated by several work on unsupervised induction of POS based on the distributional hypothesis (Finch and Chater, 1992; Sch¨utze, 1993; Sch¨utze, 1995; Gauch and Futrelle, 1994; Clark, 2000; Rapp, 2005; Biemann, 2006b), our main contributions reside in – (a) a comparative study of various approaches to POS tagset induction on Bengali, (b) rigorous linguistic analysis of the word classes and suggest</context>
<context>he words used to describe syntactic contexts are further called feature words2. The general methodology (Finch and Chater, 1992; Sch¨utze, 1993; Sch¨utze, 1995; Gauch and Futrelle, 1994; Clark, 2000; Rapp, 2005; Biemann, 2006b) for inducing word class information can be outlined as follows, (a) collect global context vectors of target words by counting how often feature words appear in the neighboring posit</context>
<context>re words are the most frequent 50-250 words. Some authors employ a much larger number of features and reduce the dimensions of the resulting matrix using Singular Value Decomposition (Sch¨utze, 1993; Rapp, 2005). The choice of high frequency words as features is motivated by Zipf’s law: these few stop words constitute the bulk of the tokens in a corpus. Contexts are the feature words appearing in the immedi</context>
<context>uster information received from the neighbors. The algorithm terminates when the labels do not change considerably over successive iterations. Agglomerative Hierarchical Clustering: In this approach (Rapp, 2005), initially all the words are in separate clusters. At every iteration, two clusters closest to each other (where “closeness” between the centroids of the two clusters is measured by sim(w,v)) are me</context>
</contexts>
<marker>Rapp, 2005</marker>
<rawString>R. Rapp. 2005. A practical solution to the problem of automaticpart-of-speechinductionfromtext. InProceedings of ACL’05 (companion volume), pages 77 – 80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Part-of-speech induction from scratch</title>
<date>1993</date>
<booktitle>In Proceedings of ACL’93</booktitle>
<pages>251--258</pages>
<marker>Sch¨utze, 1993</marker>
<rawString>H. Sch¨utze. 1993. Part-of-speech induction from scratch. In Proceedings of ACL’93, pages 251–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Distributional part-of-speech tagging</title>
<date>1995</date>
<booktitle>In Proceedings of EACL’95</booktitle>
<pages>141--148</pages>
<publisher>Morgan Kaufmann Publishers Inc</publisher>
<location>San Francisco, CA, USA</location>
<marker>Sch¨utze, 1995</marker>
<rawString>H. Sch¨utze. 1995. Distributional part-of-speech tagging. In Proceedings of EACL’95, pages 141–148, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Shannon</author>
<author>W Weaver</author>
</authors>
<title>The Mathematical Theory of Information</title>
<date>1949</date>
<publisher>University of Illinois Press</publisher>
<contexts>
<context>are distributed randomly across them. Our objective is to define a metric over the tag vectors of the words in c, which will be able to quantify the cohesiveness of the cluster. Since binary entropy (Shannon and Weaver, 1949) measures the disorderedness of a system, we define the (in)cohesiveness of a cluster c of size s as TE(c) = − Tsummationdisplay i=1 (pi(c)log2 pi(c) + qi(c)log2 qi(c)) (1) where pi(c) = 1s[# words i</context>
</contexts>
<marker>Shannon, Weaver, 1949</marker>
<rawString>C. E. Shannon and W. Weaver. 1949. The Mathematical Theory of Information. University of Illinois Press.</rawString>
</citation>
</citationList>
</algorithm>

