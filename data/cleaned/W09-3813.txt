Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 81–84,
Paris, October 2009. c 2009 Association for Computational Linguistics
Analysis of Discourse Structure with Syntactic Dependencies and 
Data-Driven Shift-Reduce Parsing 
 
 
Kenji Sagae 
USC Institute for Creative Technologies 
Marina del Rey, CA 90292 USA 
sagae@ict.usc.edu 
 
 
 
 
 
 
Abstract 
We present an eficient aproach for dis-
course parsing within and acros sen-
tences, where the unit of procesing is an 
entire document, and not a single sen-
tence.  We aply shift-reduce algorithms 
for dependency and constituent parsing to 
determine syntactic dependencies for the 
sentences in a document, and subse-
quently a Rhetorical Structure Theory 
(RST) tre for the entire document. Our 
results show that our linear-time shift-
reduce framework achieves high acu-
racy and a large improvement in efi-
ciency compared to a state-of-the-art ap-
proach based on chart parsing with dy-
namic programing.  
1 Introduction

Transition-based dependency parsing using shift-
reduce algorithms is now in wide use for de-
pendency parsing, where the goal is to determine 
the syntactic structure of sentences. State-of-the-
art results have ben achieved for syntactic 
analysis in a variety of languages (Bucholz and 
Marsi, 206).  In contrast to graph-based ap-
proaches, which use edge-factoring to alow for 
global optimization of parameters over entire tre 
structures using dynamic programing or maxi-
mum spaning tre algorithms (McDonald et al., 
205) transition-based models are usualy opti-
mized at the level of individual shift-reduce ac-
tions, and can be used to drive parsers that pro-
duce competitive acuracy using gredy search 
strategies in linear time. 
Recent research in data-driven shift-reduce 
parsing has shown that the basic algorithms used 
for determining dependency tres (Nivre, 204) 
can be extended to produce constituent structures 
(Sagae and Lavie, 205), and more general de-
pendency graphs, where words can be linked to 
more than one head (Henderson et al., 208; Sa-
gae and Tsuji, 208).  A remarkably similar 
parsing aproach, which predates the curent 
wave of interest in data-driven shift-reduce pars-
ing sparked by Yamada and Matsumoto (203) 
and Nivre and Scholz (204), was proposed by 
Marcu (199) for data-driven discourse parsing, 
where the goal is to determine the rhetorical 
structure of a document, including relationships 
that span multiple sentences.  The linear-time 
shift-reduce framework is particularly wel suited 
for discourse parsing, since the length of the in-
put string depends on document length, not sen-
tence length, making cubic run-time chart pars-
ing algorithms often impractical. 
Soricut and Marcu (203) presented an ap-
proach to discourse parsing that relied on syntac-
tic information produced by the Charniak (200) 
parser, and used a standard botom-up chart pars-
ing algorithm with dynamic programing to 
determine discourse structure.  Their aproach 
greatly improved on the acuracy of Marcu’s 
shift-reduce aproach, showing the value of us-
ing syntactic information in discourse analysis, 
but recovered only discourse relations within 
sentences.  
We present an eficient aproach to discourse 
parsing using syntactic information, inspired by 
Marcu’s aplication of a shift-reduce algorithm 
for discourse analysis with Rhetorical Structure 
Theory (RST), and Soricut and Marcu’s use of 
syntactic structure to help determine discourse 
structure. Our transition-based discourse parsing 
framework combines elements from Nivre 
(204)’s aproach to dependency parsing, and 
Sagae and Lavie (205)’s aproach to constituent 
parsing.  Our results improve on acuracy over 
existing aproaches for data-driven RST parsing, 
while also improving on sped over Soricut and 
Marcu’s chart parsing approach, which produces 
state-of-the-art results for RST discourse rela-
tions within sentences. 
81
2 Discourse
analysis with the RST Dis-
course Treebank 
The discourse parsing aproach presented here is 
based on the formalization of Rhetorical Struc-
ture Theory (RST) (Man and Thompson, 198) 
used in the RST Discourse Trebank (Carlson et 
al., 203). In this scheme, the discourse structure 
of a document is represented as a tre, where the 
leaves are contiguous spans of text, caled ele-
mentary discourse units, or EDUs. Each node in 
the tre coresponds to a contiguous span of text 
formed by concatenation of the spans core-
sponding to the node’s children, and represents a 
rhetorical relation (atribution, enablement, 
elaboration, consequence, etc.) betwen these 
text segments. In adition, each node is marked 
as a nucleus or as a satelite, depending on 
whether its text span represents an esential unit 
of information, or a suporting or background 
unit of information, respectively.  While the no-
tions of nucleus and satelite are in some ways 
analogous to head and dependent in syntactic 
dependencies, RST alows for multi-nuclear rela-
tions, where two nodes marked as nucleus can be 
linked into one node. 
Our parsing framework includes thre compo-
nents: (1) syntactic dependency parsing, where 
standard techniques for sentence-level parsing 
are aplied; (2) discourse segmentation, which 
uses syntactic and lexical information to segment 
text into EDUs; and (3) discourse parsing, which 
produces a discourse structure tre from a string 
of EDUs, also benefiting from syntactic informa-
tion.  In contrast to the aproach of Soricut and 
Marcu (203), which also includes syntactic 
parsing, discourse segmentation and discourse 
parsing, our aproach asumes that the unit of 
procesing for discourse parsing is an entire 
document, and that discourse relations may exist 
within sentences as wel as acros sentences, 
while Soricut and Marcu’s proceses one sen-
tence at a time, independently, finding only dis-
course relations within individual sentences. 
Parsing entire documents at a time is made pos-
sible in our aproach through the use of linear-
time transition-based parsing. An aditional mi-
nor diference is that in our aproach syntactic 
information is represented using dependencies, 
while Soricut and Marcu used constituent tres. 
2.1 Syntactic
parsing and discourse seg-
mentation 
Asuming the document has ben segmented into 
sentences, a task for which there are aproaches 
with very high acuracy (Gilick, 209), we start 
by finding the dependency structure for each sen-
tence.  This includes part-of-spech (POS) tag-
ging using a CRF tager trained on the Wal 
Stret Journal portion of the Pen Trebank, and 
transition-based dependency parsing using the 
shift-reduce arc-standard algorithm (Nivre, 204) 
trained with the averaged perceptron (Colins, 
202).  The dependency parser is also trained 
with the WSJ Pen Trebank, converted to de-
pendencies using the head percolation rules of 
Yamada and Matsumoto (203). 
Discourse segmentation is performed as a bi-
nary clasification task on each word, where the 
decision is whether or not to insert an EDU 
boundary betwen the word and the next word. 
In a sentence of length n, containing the words 
w
1, w
2
 … w
n, we perform one clasification per 
word, in order. For word w
i, the binary choice is 
whether to insert an EDU boundary betwen w
i
 
and w
i+1
. The EDUs are then the words betwen 
EDU boundaries (asuming boundaries exist in 
the begining and end of each sentence).  
The features used for clasification are: the 
curent word, its POS tag, its dependency label, 
and the direction to its head (whether the head 
apears before or after the word); the previous 
two words, their POS tags and dependency la-
bels; the next two words, their POS tags and de-
pendency labels; the direction from the previous 
word to its head; the leftmost dependent to the 
right of the curent word, and its POS tag; the 
rightmost dependent to the left of the curent 
word, and its POS tag; whether the head of the 
curent word is betwen the previous EDU 
boundary and the curent word; whether the head 
of the next word is betwen the previous EDU 
boundary and the curent word.  In adition, we 
used templates that combine these features (in 
pairs or triples).  Clasification was done with 
the averaged perceptron. 
2.2 Transition-based discourse parsing 
RST tres can be represented in a similar way as 
constituent tres in the Pen Trebank, with a 
few diferences: the tres represent entire docu-
ments, instead of single sentences; the leaves of 
the tres are EDUs consisting of one or more 
contiguous words; and the node labels contain 
nucleus/satelite status, and posibly the name of 
a discourse relation.  Once the document has 
ben segmented into a sequence of EDUs, we 
use a transition-based constituent parsing ap-
proach (Sagae and Lavie, 205) to build an RST 
tre for the document. 
82
Sagae and Lavie’s constituent parsing algo-
rithm uses a stack that holds subtres, and con-
sumes the input string (in our case, a sequence of 
EDUs) from left to right, using four types of ac-
tions: (1) shift, which removes the next token 
from the input string, and pushes a subtre con-
taining exactly that token onto the stack; (2) re-
duce-unary-LABEL, which pops the stack, and 
push onto it a new subtre where a node with 
label LABEL dominates the subtre that was 
poped (3) reduce-left-LABEL, and (4) reduce-
right-LABEL, which each pops two items from 
the stack, and pushes onto it a new subtre with 
rot LABEL, which has as right child the subtre 
previously on top of the stack, and as left child 
the subtre previously imediately below the top 
of the stack. The diference betwen reduce-left 
and reduce-right is whether the head of the new 
subtre comes from the left or right child.  The 
algorithm asumes tres are lexicalized, and in 
our use of the algorithm for discourse parsing, 
heads are entire EDUs, and not single words. 
Our proces for lexicalization of discourse 
tres, which is required for the parsing algorithm 
to function properly, is a simple percolation of 
“head EDUs,” performed in the same way as 
lexical heads can be asigned in Pen Trebank-
style tres using a head percolation table 
(Colins, 199).  To determine head EDUs, we 
use the nucleus/satelite status of nodes, as fol-
lows: for each node, the leftmost child with nu-
cleus status is the head; if no child is a nucleus, 
the leftmost satelite is the head.  Most nodes 
have exactly two children, one nucleus and one 
satelite.  The parsing algorithm deals only with 
binary tres. We use the same binarization trans-
form as Sagae and Lavie, converting the tres in 
the training set to binary tres prior to training 
the parser, and converting the binary tres pro-
duced by the parser at run-time into n-ary tres.  
As with the dependency parser and discourse 
segmenter, learning is performed using the aver-
aged perceptron. We use similar features as Sa-
gae and Lavie, with one main diference: since 
there is usualy no single head-word asociated 
with each node, but a EDU that contains a se-
quence of words, we use the dependency struc-
ture of the EDU to determine what lexical fea-
tures and POS tags should be used as features 
asociated with each RST tre node. In place of 
the head-word and POS tag of the top four items 
on the stack, and the next four items in the input, 
we use subsets of the words and POS tags in the 
EDUs for each of those items.  The subset of 
words (and POS tags) that represent an EDU 
contain the first two and last words in the EDU, 
and each word in the EDU whose head is outside 
of the EDU.  In the vast majority of EDUs, this 
subset of words with heads outside the EDU (the 
EDU head set) contains a single word.  In adi-
tion, we extract these features for the top thre 
(not four) items on the stack, and the next thre 
(not four) words in the input.  For the top two 
items on the stack, in adition to subsets of 
words and POS tags described above, we also 
take the words and POS tags for the leftmost and 
rightmost children of each word in the EDU head 
set.  Finaly, we use feature templates that com-
bine these and other individual features from Sa-
gae and Lavie, who used a polynomial kernel 
and had no ned for such templates (at the cost of 
increased time for both training and runing). 
3 Results

To test our discourse parsing aproach, we used 
the standard training and testing sections of the 
RST Discourse Trebank and the compacted 18-
label set described by Carlson et al. (203). We 
used aproximately 5% of the standard training 
set as a development set. 
Our part-of-spech tager and syntactic parser 
were not trained using the standard splits of the 
Pen Trebank for those tasks, since there are 
documents in the RST Discourse Trebank test 
section that are included in the usual training sets 
for POS tagers and parsers.  The POS tager 
and syntactic parser were then trained on sec-
tions 2 to 21 of the WSJ Pen Trebank, exclud-
ing the specific documents used in the test sec-
tion of the RST Discourse Trebank. 
Table 1 shows the precision, recal and f-score 
of our discourse segmentation aproach on the 
test set, compared to that of Soricut and Marcu 
(203) and Marcu (199).  In al cases, results 
were obtained with automaticaly produced syn-
tactic structures.  We also include the total time 
required for syntactic parsing (required in our 
 Prec. Recal F-score Time 
Marcu9 83.3 7.1 80.1 
S&M03 83.5 82.7 83.1 361s 
this work 87.4 86.0 86.7 40s 
 
Table 1: Precision, recal, f-score and time 
for discourse segmenters, tested on the RST 
Discourse Trebank. Time includes syntactic 
parsing, Charniak (200) for S&M03, and 
our implemetation of Nivre arc-standard for 
our segmenter. 
83
segmentation aproach and Soricut and Marcu’s) 
and segmentation. For comparison with previous 
results, we include only segmentation within sen-
tences (if al discourse boundaries are counted, 
including sentence boundaries, our f-score is 
92.9). 
Using our discourse segmentation and transi-
tion-based discourse parsing aproach, we obtain 
42.9 precision and 46.2 recal (4.5 f-score) for 
al discourse structures in the test set.  Table 2 
shows f-score of labeled bracketing for discourse 
relations within sentences only, for comparison 
with previously published results.  We note that 
human performance on this task has f-score 7.0. 
While our f-score is stil far below that of hu-
man performance, we have achieved a large gain 
in sped of procesing compared to a state-of-
the-art aproach. 
4 Conclusion

We have presented an aproach to discourse 
analysis based on transition-based algorithms for 
dependency and constituent tres.  Dependency 
parsing is used to determine the syntactic struc-
ture of text, which is then used in discourse seg-
mentation and parsing.  A simple discriminative 
aproach to segmentation results in an overal 
improvement in discourse parsing f-score, and 
the use of a linear-time algorithm results in an a 
large improvement in sped over a state-of-the-
art aproach. 
Acknowledgments 
The work described here has ben sponsored by 
the U.S. Army Research, Development, and En-
ginering Comand (RDECOM). Statements 
and opinions expresed do not necesarily reflect 
the position or the policy of the United States 
Government, and no oficial endorsement should 
be infered. 
References 
Bucholz, S. and Marsi, E. 206. CoNL-X shared 
task on multilingual dependency parsing. In Proc. 
of CoNL 206 Shared Task. 
Carlson, L., Marcu, D., and Okurowski, M. E. 203. 
Building a discourse-taged corpus in the frame-
work of Rhetorical Structure Theory. In J. van 
Kupevelt and R. W. Smith, editors, Curent and 
New Directions in Discourse and Dialogue. Klu-
wer Academic Publishers. 
Charniak, E. 200. A maximum-entropy-inspired 
parser. In Proc. of NACL. 
Colins, M. 199. Head-driven statistical models for 
natural language procesing.  PhD disertation, 
University of Pensylvania. 
Colins, M. 202. Discriminative Training Methods 
for Hiden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In Proc. of 
EMNLP.  Philadelphia, PA. 
Gilick, D. 209. Sentence Boundary Detection and 
the Problem with the U.S. In Proc. of the NACL 
HLT: Short Papers. Boulder, Colorado. 
Henderson, J., Merlo, P., Musilo, G., Titov, I. 208. 
A Latent Variable Model of Synchronous Parsing 
for Syntactic and Semantic Dependencies. In Proc. 
of CoNL 208 Shared Task, Manchester, UK. 
Man, W. C. and Thompson, S. A. 198. Rhetorical 
Structure Theory: toward a functional theory of 
text organization. Text, 8(3):243-281. 
Marcu, D. 199. A decision-based aproach to rhe-
torical parsing. In Proc. of the Anual Meting of 
the Asociation for Computational Linguistics. 
McDonald, R., Pereira, F., Ribarov, K., and Hajic, J. 
205. Non-projective dependency parsing using 
spaning tre algorithms. In Proc. of HLT/EMNLP. 
Nivre, J. 204. Incrementality in Deterministic De-
pendency Parsing. In Incremental Parsing: Bring-
ing Enginering and Cognition Together (work-
shop at ACL-204). Barcelona, Spain.  
Nivre, J. and Scholz, M. 204. Deterministic Depend-
ency Parsing of English Text. In Proc. of COLING. 
Sagae, K. and Lavie, A. 205.  A clasifier-based 
parser with linear run-time complexity. In Proc. of 
IWPT. 
Sagae, K. and Tsuji, J. 208. Shift-reduce depend-
ency DAG parsing. In Proc. of COLING. 
Soricut, R. and Marcu, D. 203. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In Proc. of NACL. Edmonton, Canada. 
Yamada, H. and Matsumoto, Y. 203. Statistical de-
pendency analysis with suport vector machines. In 
Proc. of IWPT. 
 
 F-score Time 
Marcu9 37.2 
S&M03 49.0 481s 
this work 52.9 69s 
human 7.0 
 
Table 2: F-score for bracketing of RST dis-
course tres on the test set of the RST Dis-
course Trebank, and total time (syntactic 
parsing, segmentation and discourse parsing) 
required to parse the test set (S&M03 and our 
aproach were run on the same hardware). 
 
84

