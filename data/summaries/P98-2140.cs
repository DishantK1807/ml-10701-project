We adopted the stop condition suggested in (Berger et al., 1996) the maximization of the likelihood on a cross-validation set of samples which is unseen at the parameter estimation.
J96-1002
(Reynar&Ratnaparkhi, 1997) don't report on the number of features utilized by their model and don't describe their approach to feature selection but judging by the time their system was trained (18 minutes 3) it did not aim to produce the best performing feature-set but estimated a given one.
A97-1004
The simplest "period-space-capital_letter" approach works well for simple texts but is rather unreliable for texts with many proper names and abbreviations at the end of sentence as, for instance, the Wall Street Journal (WSJ) corpus ( (Marcus et al., 1993) ).
J93-2004
We also do not require a newly added feature to be either atomic or a collocation of an atomic feature with a feature already included into the model as it was proposed in (Della Pietra et al., 1995) (Berger et al., 1996).
J96-1002
were also used by (Reynar&Ratnaparkhi, 1997) in the evaluation of their system.
A97-1004
For the evaluation we used the same 27,294 sentences as in (Palmer&Hearst, 1997) 4 which aPersonal communication 4We would like to thank David Palmer for making his test data available to us.
J97-2002
We attribute this to the fact that although we started with roughly the same atomic features as (Reynar&Ratnaparkhi, 1997) our system created complex features with higher prediction power.
A97-1004
Another automatically trainable system described in (Reynar&Ratnaparkhi, 1997).
A97-1004
One well-known trainable systems SATZ is described in (Palmer&Hearst, 1997).
J97-2002
To 848 make feature ranking computationally tractable in (Della Pietra et al., 1995) and (Berger et al., 1996) a simplified process proposed: at the feature ranking stage when adding a new feature to the model, all previously computed parameters are kept fixed and, thus, we have to fit only one new constraint imposed by the candidate feature.
J96-1002
