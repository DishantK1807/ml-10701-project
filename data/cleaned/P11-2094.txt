Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 534–539,
Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
Nonparametric Bayesian Machine Transliteration with Synchronous
Adaptor Grammars
Yun Huang1,2 Min Zhang1 Chew Lim Tan2
huangyun@comp.nus.edu.sg mzhang@i2r.a-star.edu.sg tancl@comp.nus.edu.sg
1Human Language Department 2Department of Computer Science
Institute for Infocomm Research National University of Singapore
1 Fusionopolis
Way, Singapore 13 Computing Drive, Singapore
Abstract
Machine transliteration is defined as auto-
matic phonetic translation of names across
languages. In this paper, we propose syn-
chronous adaptor grammar, a novel nonpara-
metric Bayesian learning approach, for ma-
chine transliteration. This model provides
a general framework without heuristic or re-
striction to automatically learn syllable equiv-
alents between languages. The proposed
model outperforms the state-of-the-art EM-
based model in the English to Chinese translit-
eration task.
1 Introduction
Proper names are one source of OOV words in many
NLP tasks, such as machine translation and cross-
lingual information retrieval. They are often trans-
lated through transliteration, i.e. translation by pre-
serving how words sound in both languages. In
general, machine transliteration is often modelled
as monotonic machine translation (Rama and Gali,
2009; Finch and Sumita, 2009; Finch and Sumita,
2010), the joint source-channel models (Li et al.,
2004; Yang et al., 2009), or the sequential label-
ing problems (Reddy and Waxmonsky, 2009; Ab-
dul Hamid and Darwish, 2010).
Syllable equivalents acquisition is a critical phase
for all these models. Traditional learning approaches
aim to maximize the likelihood of training data
by the Expectation-Maximization (EM) algorithm.
However, the EM algorithm may over-fit the training
data by memorizing the whole training instances. To
avoid this problem, some approaches restrict that a
single character in one language could be aligned
to many characters of the other, but not vice versa
(Li et al., 2004; Yang et al., 2009). Heuristics are
introduced to obtain many-to-many alignments by
combining two directional one-to-many alignments
(Rama and Gali, 2009). Compared to maximum
likelihood approaches, Bayesian models provide a
systemic way to encode knowledges and infer com-
pact structures. They have been successfully applied
to many machine learning tasks (Liu and Gildea,
2009; Zhang et al., 2008; Blunsom et al., 2009).
Among these models, Adaptor Grammars (AGs)
provide a framework for defining nonparametric
Bayesian models based on PCFGs (Johnson et al.,
2007). They introduce additional stochastic pro-
cesses (named adaptors) allowing the expansion of
an adapted symbol to depend on the expansion his-
tory. Since many existing models could be viewed
as special kinds of PCFG, adaptor grammars give
general Bayesian extension to them. AGs have been
used in various NLP tasks such as topic modeling
(Johnson, 2010), perspective modeling (Hardisty et
al., 2010), morphology analysis and word segmenta-
tion (Johnson and Goldwater, 2009; Johnson, 2008).
In this paper, we extend AGs to Synchronous
Adaptor Grammars (SAGs), and describe the in-
ference algorithm based on the Pitman-Yor process
(Pitman and Yor, 1997). We also describe how
transliteration could be modelled under this formal-
ism. It should be emphasized that the proposed
method is language independent and heuristic-free.
Experiments show the proposed approach outper-
forms the strong EM-based baseline in the English
to Chinese transliteration task.
534

