1:272	Object-Extraction and Question-Parsing using CCG Stephen Clark and Mark Steedman School of Informatics University of Edinburgh 2 Buccleuch Place, Edinburgh, UK a0 stevec,steedman a1 @inf.ed.ac.uk James R. Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Abstract Accurate dependency recovery has recently been reported for a number of wide-coverage statistical parsers using Combinatory Categorial Grammar (CCG).
2:272	However, overall figures give no indication of a parsers performance on specific constructions, nor how suitable a parser is for specific applications.
3:272	In this paper we give a detailed evaluation of a CCG parser on object extraction dependencies found in WSJ text.
4:272	We also show how the parser can be used to parse questions for Question Answering.
5:272	The accuracy of the original parser on questions is very poor, and we propose a novel technique for porting the parser to a new domain, by creating new labelled data at the lexical category level only.
6:272	Using a supertagger to assign categories to words, trained on the new data, leads to a dramatic increase in question parsing accuracy.
7:272	1 Introduction Several wide-coverage statistical parsers have recently been developed for Combinatory Categorial Grammar (CCG; Steedman, 2000) and applied to the WSJ Penn Treebank (Clark et al. , 2002; Hockenmaier and Steedman, 2002; Hockenmaier, 2003b; Clark and Curran, 2004b).
8:272	One motivation for using CCG is the recovery of the long-range dependencies inherent in phenomena such as coordination and extraction.
9:272	Recovery of these dependencies is important for NLP tasks which require semantic interpretation and for processing text which contains a high frequency of such cases, e.g. Wh-questions fed to a Question Answering (QA) system.
10:272	One shortcoming of treebank parsers such as Collins (1999) and Charniak (2000) is that they typically produce phrase-structure trees containing only local syntactic information.
11:272	Johnson (2002) uses post-processing methods to insert empty nodes into the trees, and Dienes and Dubey (2003) use preprocessing methods to determine where discontinuities are likely to appear in the sentence.
12:272	In contrast, the CCG parsers detect long-range dependencies as an integral part of the parsing process.
13:272	The CCG parser used here (Clark and Curran, 2004b) is highly accurate and efficient, recovering labelled dependencies with an overall F-score of over 84% on WSJ text, and parsing up to 50 sentences per second.
14:272	Thus the parser should be useful for large-scale NLP tasks.
15:272	However, the overall accuracy figures give no indication of the parsers performance on specific constructions, nor how suitable the parser is for specific applications.
16:272	In this paper we give a detailed evaluation for object extraction dependencies and show how the parser can be used to parse questions for QA.
17:272	We find that the parser performs well on the object extraction cases found in the Penn Treebank, given the difficulty of the task.
18:272	In contrast, the parser performs poorly on questions from TREC, due to the small number of questions in the Penn Treebank.
19:272	This motivates the remainder of the paper, in which we describe the creation of new training data consisting of labelled questions.
20:272	Crucially, the questions are labelled at the lexical category level only, and not at the derivation level, making the creation of new labelled data relatively easy.
21:272	The parser uses a supertagger to assign lexical categories to words, and the supertagger can be adapted to the new question domain by training on the newly created data.
22:272	We find that using the original parsing model with the new supertagger model dramatically increases parsing accuracy on TREC questions, producing a parser suitable for use in a QA system.
23:272	For evaluation we focus on What questions used in the TREC competitions.
24:272	As well as giving an overall evaluation on this test set, we also consider a number of object extraction cases.
25:272	The creation of new training data at the lexical category level alone is a technique which could be used to rapidly port the parser to other domains.
26:272	This technique may also be applicable to other lexicalised grammar formalisms, such as Tree Adjoining Grammar (Bangalore and Joshi, 1999).1 1Doran et al.27:272	(1997) propose using a supertagger for semiautomatically porting the XTAG grammar to a new domain.
28:272	2 The Parser The parser used in this paper is described in Clark and Curran (2004b).
29:272	It takes as input a POS tagged sentence with a set of lexical categories assigned to each word.
30:272	The CCG combinatory rules are used to combine the categories.
31:272	A packed chart efficiently represents all of the possible analyses for a sentence, and the CKY chart parsing algorithm described in Steedman (2000) is used to build the chart.
32:272	A Maximum Entropy CCG supertagger (Clark and Curran, 2004a) is used to assign the categories.
33:272	The lexical category set is obtained from CCGbank (Hockenmaier, 2003a), a treebank of normal-form CCG derivations derived from the Penn Treebank.
34:272	CCGbank is also used for learning the parameters of the supertagger and parsing models.
35:272	2.1 The Supertagger The supertagger uses a log-linear model to define a distribution for each word over the lexical category set.
36:272	Model features are defined by the words and POS tags in the 5-word window surrounding the target word.
37:272	The supertagger selects the most probable categories locally rather than maximising the sequence probability, assigning all categories whose probability is within some factor,, of the highest probability category.
38:272	For a word seen frequently in the training data, the supertagger can only assign categories from the words entry in the tag dictionary, which lists the categories each word has been seen with in the data.
39:272	In Clark et al.s (2002) parser, a supertagger is used as follows: first around 4 lexical categories are assigned to each word, on average; if the chart gets too big or parsing takes too long, the number of categories is reduced until the sentence can be parsed.
40:272	In this paper we use our more recent approach (Clark and Curran, 2004a): first a small number of categories is assigned to each word, e.g. 1.5, and the parser requests more categories if a spanning analysis cannot be found.
41:272	This method relies on the grammar being constraining enough to decide whether the categories provided by the supertagger are likely to contain the correct sequence.
42:272	Section 6 shows that this approach works well for parsing questions.
43:272	2.2 Parsing Model In Clark and Curran (2004b) we investigate several log-linear parsing models for CCG.
44:272	In this paper we use the following conditional model: p(yjx)= 1Z(x)e P i i fi(y) (1) where y is a normal-form derivation and x is a sentence.
45:272	(A normal-form derivation is one where composition and type-raising are used only when necessary).
46:272	There are various features, fi, used by the model: rule instantiation features which count the number of times a local tree occurs in a derivation; features defined by the root category of a derivation; and features defined by the lexical categories at the leaves.
47:272	Each feature type has unlexicalised and head-lexicalised versions.
48:272	The remaining features capture word-word dependencies, which significantly improve accuracy.
49:272	The best-performing model encodes word-word dependencies in terms of the local rule instantiations, as in Hockenmaier and Steedman (2002).
50:272	We have also tried predicate-argument dependencies, including long-range dependencies, but these have not improved performance.
51:272	Note we still recover longrange dependencies, even if modelling them does not improve performance.
52:272	The parser returns a derived structure corresponding to the most probable derivation.
53:272	For evaluation the parser returns dependency structures, but we have also developed a module which builds firstorder semantic representations from the derivations, which can be used for inference (Bos et al. , 2004).
54:272	3 Object Extraction Steedman (1996) presents a detailed study of various extraction phenomena.
55:272	Here we focus on object extraction, since the dependencies in such cases are unbounded, and CCG has been designed to handle these cases.
56:272	Correct dependency recovery for object extraction is also difficult for shallow methods such as Johnson (2002) and Dienes and Dubey (2003).
57:272	We consider three types of object extraction: object relative clauses, free object relatives, and toughadjectives (Hockenmaier, 2003a).
58:272	Examples of the first two from CCGbank are given in Figures 1 and 2, together with the normal-form derivation.
59:272	The caption gives the number of sentences containing such a case in Sections 2-21 of CCGbank (the training data) and Section 00 (development data).
60:272	The pattern of the two derivations is similar: the subject of the verb phrase missing an object is type-raised (T); the type-raised subject composes (B) with the verb-phrase; and the category for the relative pronoun ((NPnNP)=(S[dcl]=NP) or NP=(S[dcl]=NP)) applies to the sentence-missingits-object (S[dcl]=NP).
61:272	Clark et al.62:272	(2002) show how the dependency between the verb and object can be captured by co-indexing the heads of the NPs in the relative pronoun category.
63:272	Figure 3 gives the derivation for a toughadjective.
64:272	The dependency between take and That can be recovered by co-indexing the heads of NPs in an excellent publication that I enjoy reading NP=N N=N N (NPnNP)=(S[dcl]=NP) NP (S[dcl]nNP)=(S[ng]nNP) (S[ng]nNP)=NP > >T >BN S=(SnNP) (S[dcl]nNP)=NP) > >BNP S[dcl]=NP >NPnNP <NP Figure 1: Extraction from object relative clause; 431 sentences in Sections 2-21, 20 in Section 00 he believes in what he plays NP (S[dcl]nNP)=PP PP=NP NP=(S[dcl]=NP) NP (S[dcl]nNP)=NP) >B >T(S[dcl]nNP)=NP) S=(SnNP) >BS[dcl]=NP >NP >S[dcl]nNP <S[dcl] Figure 2: Free object relative example; 269 sentences in Sections 2-21, 16 sentences in Section 00 That got hard to take NP (S[dcl]nNP)=(S[adj]nNP) (S[adj]nNP)=((S[to]nNP)=NP) (S[to]nNP)=(S[b]nNP) (S[b]nNP)=NP) >B(S[to]nNP)=NP) >S[adj]nNP >S[dcl]nNP <S[dcl] Figure 3: tough-adjective example; 52 sentences in Sections 2-21, 2 sentences in Section 00 the categories for hard and got.
65:272	These cases are relatively rare, with around 50 occurring in the whole of the treebank, and only two in the development set; the parser correctly recovers one of the two object dependencies for the tough-adjective cases in 00.
66:272	For the free object relative cases in Section 00, the parser recovers 14 of the 17 gold-standard dependencies2 between the relative pronoun and the head of the relative clause.
67:272	The precision is 14/15.
68:272	For the three gold standard cases that are misanalysed, the category NP=S[dcl] is assigned to the relative pronoun, rather than NP=(S[dcl]=NP).
69:272	For the cases involving object relative clauses the parser provides a range of errors for which it is useful to give a detailed analysis.
70:272	3.1 Analysis of Object Extraction Cases Figure 4 gives the 20 sentences in Section 00 which contain a relative pronoun with the category (NPnNP)=(S[dcl]=NP).
71:272	There are 24 object dependencies in total, since some sentences contain more than one extraction (11), and some extractions involve more than one head (8, 18, 19).
72:272	For evaluation, we determined whether the parser correctly re2One of the 16 sentences contains two such dependencies.
73:272	covered the dependency between the head of the extracted object and the verb.
74:272	For example, to get the two dependencies in sentence 18 correct, the parser would have to assign the correct lexical category to had, and return respect and confidence as objects.
75:272	The parser correctly recovers 15 of the 24 object dependencies.3 Overall the parser hypothesises 20 extracted object dependencies, giving a precision of 15/20.
76:272	Hockenmaier (2003a) reports similar results for a CCG parser using a generative model: 14/24 recall and 14/21 precision.
77:272	The results here are a significant improvement over those in Clark et al.78:272	(2002), in which only 10 of the 24 dependencies were recovered correctly.
79:272	Below is a detailed analysis of the mistakes made by the parser.
80:272	For Sentence 1 the parser cannot provide any analysis.
81:272	This is because the correct category for estimated, ((S[pt]nNP)=PP)=NP, is not in the tag dictionarys entry for estimated.
82:272	Since estimated occurs around 200 times in the data, the supertagger only considers categories from the tag dictionary entry, and thus cannot provide the correct category as an option.
83:272	3Unless stated otherwise the parser uses automatically assigned, rather than gold standard, POS tags.
84:272	1.
85:272	Commonwealth Edison now faces an additional court-ordered refund on its summer/winter rate differential collections that the Illinois Appellate Court has estimated at $140 million.
86:272	2.
87:272	Mrs. Hills said many of the 25 countries that she placed under varying degrees of scrutiny have made genuine progress on this touchy issue.p 3.
88:272	Its the petulant complaint of an impudent American whom Sony hosted for a year while he was on a Luce Fellowship in Tokyo  to the regret of both parties.p 4.
89:272	It said the man, whom it did not name, had been found to have the disease after hospital tests.
90:272	5.
91:272	Democratic Lt. Gov. Douglas Wilder opened his gubernatorial battle with Republican Marshall Coleman with an abortion commercial produced by Frank Greer that analysts of every political persuasion agree was a tour de force.
92:272	6.
93:272	Against a shot of Monticello superimposed on an American flag, an announcer talks about the strong tradition of freedom and individual liberty that Virginians have nurtured for generations.p 7.
94:272	Interviews with analysts and business people in the U.S. suggest that Japanese capital may produce the economic cooperation that Southeast Asian politicians have pursued in fits and starts for decades.
95:272	8.
96:272	Another was Nancy Yeargin, who came to Greenville in 1985, full of the energy and ambitions that reformers wanted to reward.
97:272	9.
98:272	Mostly, she says, she wanted to prevent the damage to self-esteem that her low-ability students would suffer from doing badly on the test.p 10.
99:272	Mrs. Ward says that when the cheating was discovered, she wanted to avoid the morale-damaging public disclosure that a trial would bring.p 11.
100:272	In CAT sections where students knowledge of two-letter consonant sounds is tested, the authors noted that Scoring High concentrated on the same sounds that the test does  to the exclusion of other sounds that fifth graders should know.p 12.
101:272	Interpublic Group said its television programming operations  which it expanded earlier this year  agreed to supply more than 4,000 hours of original programming across Europe in 1990.
102:272	13.
103:272	Interpublic is providing the programming in return for advertising time, which it said will be valued at more than $75 million in 1990 and $150 million in 1991.p 14.
104:272	Mr. Sherwood speculated that the leeway that Sea Containers has means that Temple would have to substantially increase their bid if theyre going to top us.p 15.
105:272	The Japanese companies bankroll many small U.S. companies with promising products or ideas, frequently putting their money behind projects that commercial banks wont touch.p 16.
106:272	In investing on the basis of future transactions, a role often performed by merchant banks, trading companies can cut through the logjam that small-company owners often face with their local commercial banks.
107:272	17.
108:272	A high-balance customer that banks pine for, she didnt give much thought to the rates she was receiving, nor to the fees she was paying.p 18.
109:272	The events of April through June damaged the respect and confidence which most Americans previously had for the leaders of China.p 19.
110:272	He described the situation as an escrow problem, a timing issue, which he said was rapidly rectified, with no losses to customers.p 20.
111:272	But Rep. Marge Roukema (R. , N.J).
112:272	instead praised the Houses acceptance of a new youth training wage, a subminimum that GOP administrations have sought for many years.
113:272	Figure 4: Cases of object extraction from a relative clause in 00; the extracted object, relative pronoun and verb are in italics; for sentences marked with a p the parser correctly recovers all dependencies involved in the object extraction.
114:272	For Sentence 2 the correct category is assigned to the relative pronoun that, but a wrong attachment results in many as the object of placed rather than countries.
115:272	In Sentence 5 the incorrect lexical category ((SnNP)n(SnNP))=S[dcl] is assigned to the relative pronoun that.
116:272	In fact, the correct category is provided as an option by the supertagger, but the parser is unable to select it.
117:272	This is because the category for agree is incorrect, since again the correct category, ((S[dcl]nNP)=NP)=(S[dcl]nNP), is not in the verbs entry in the tag dictionary.
118:272	In Sentence 6 the correct category is assigned to the relative pronoun, but a number of mistakes elsewhere result in the wrong noun attachment.
119:272	In Sentences 8 and 9 the complementizer category S[em]=S[dcl] is incorrectly assigned to the relative pronoun that.
120:272	For Sentence 8 the correct analysis is available but the parsing model chose incorrectly.
121:272	For Sentence 9 the correct analysis is unavailable because the correct category for suffer, ((S[b]nNP)=PP)=NP, is not in the verbs entry in the tag dictionary.
122:272	In Sentence 13 the correct category is again assigned to the relative pronoun, but a wrong attachment results in return being the object of placed, rather than time.
123:272	In Sentence 17 the wrong category S[em]=S[b] is assigned to the relative pronoun that.
124:272	Again the problem is with the category for the verb, but for a different reason: the POS tagger incorrectly tags pine as a base form (VB), rather than VBP, which completely misleads the supertagger.
125:272	This small study only provides anecdotal evidence for the reasons the parser is unable to recover some long-range object dependencies.
126:272	However, the analysis suggests that the parser fails largely for the same reasons it fails on other WSJ sentences: wrong attachment decisions are being made; the lexical coverage of the supertagger is lacking for some verbs; the model is sometimes biased towards incorrect lexical categories; and the supertagger is occasionally led astray by incorrect POS tags.
127:272	Note that the recovery of these dependencies is a difficult problem, since the parser must assign the correct categories to the relative pronoun and verb, and make two attachment decisions: one attaching the relative pronoun to the verb, and one attaching it to the noun phrase.
128:272	The recall figures for the individual dependencies in the relative pronoun category are 16/21 for the verb attachment and 15/24 for the noun attachment.
129:272	In conclusion, the kinds of errors made by the parser suggest that general improvements in the coverage of the lexicon and parsing models based on CCGbank will lead to better recovery of longrange object dependencies.
130:272	4 Parsing Questions Wide-coverage parsers are now being successfully used as part of open-domain QA systems, e.g. Pasca and Harabagiu (2001).
131:272	The speed and accuracy of our CCG parser suggests that it could be used to parse answer candidates, and we are currently integrating the parser into a QA system.
132:272	We would also like to apply the parser to the questions, for two reasons: the use of CCG allows the parser to deal with extraction cases, which occur relatively frequently in questions; and the comparison of potential answers with the question, performed by the answer extraction component, is simplified if the same parser is used for both.
133:272	Initially we tried some experiments applying the parser to questions from previous TREC competitions.
134:272	The results were extremely poor, largely because the questions contain constructions which appear very infrequently, if at all, in CCGbank.4 For example, there are no What questions with the general form of What President became Chief Justice after his precidency?
135:272	in CCGbank, but this is a very common form of Wh-question.
136:272	(There is a very small number (3) of similar question types beginning How or Which in Sections 221.)
137:272	One solution is to create new annotated question data and retrain the parser, perhaps combining the data with CCGbank.
138:272	However, the creation of goldstandard derivation trees is very expensive.
139:272	A novel alternative, which we pursue here, is to annotate questions at the lexical category level only.
140:272	Annotating sentences with lexical categories is simpler than annotating with derivations, and can be done with the tools and resources we have available.
141:272	The key question is whether training only the supertagger on new question data is enough to give high parsing accuracy; in Section 6 we show that it is. The next Section describes the creation of the question corpus.
142:272	5 A What-Question Corpus We have created a corpus consisting of 1,171 questions beginning with the word What, taken from the TREC 912 competitions (20002003).
143:272	We chose to focus on What-questions because these are a com4An earlier version of our QA system used RASP (Briscoe and Carroll, 2002) to parse the questions, but this parser also performed extremely poorly on some question types.
144:272	1.
145:272	What are Cushman and Wakefield known for?
146:272	2.
147:272	What are pomegranates?
148:272	3.
149:272	What is hybridization?
150:272	4.
151:272	What is Martin Luther King Jr.s real birthday?
152:272	5.
153:272	What is one of the cities that the University of Minnesota is located in?
154:272	6.
155:272	What do penguins eat?
156:272	7.
157:272	What amount of folic acid should an expectant mother take daily?
158:272	8.
159:272	What city did the Flintstones live in?
160:272	9.
161:272	What instrument is Ray Charles best known for playing?
162:272	10.
163:272	What state does Martha Stewart live in?
164:272	11.
165:272	What kind of a sports team is the Wisconsin Badgers?
166:272	12.
167:272	What English word contains the most letters?
168:272	13.
169:272	What king signed the Magna Carta?
170:272	14.
171:272	What caused the Lynmouth floods?
172:272	Figure 5: Examples from the What-question corpus CATEGORY FOR What FREQ % S[wq]=(S[q]=NP) 728 62.2 (S[wq]=(S[q]=NP))=N 221 18.9 (S[wq]=(S[dcl]nNP))=N 207 17.7 S[wq]=(S[dcl]nNP) 15 1.3 Table 1: Distribution of What categories in questions mon form of question, and many contain cases of extraction, including some unbounded object extraction.
173:272	A sample of questions from the corpus is given in Figure 5.
174:272	The questions were tokenised according to the Penn Treebank convention and automatically POS tagged.
175:272	Some of the obvious errors made by the tagger were manually corrected.
176:272	The first author then manually labelled 500 questions with lexical categories.
177:272	The supertagger was trained on the annotated questions, and used to label the remaining questions, which were then manually corrected.
178:272	The performance of the supertagger was good enough at this stage to significantly reduce the effort required for annotation.
179:272	The second author has verified a subset of the annotated sentences.
180:272	The question corpus took less than a week to create.
181:272	Figure 6 gives the derivations for some example questions.
182:272	The lexical categories, which make up the annotation in the question corpus, are in bold.
183:272	Note the first example contains an unbounded object extraction, indicated by the question clause missing an object (S[q]=NP) which is an argument of What.
184:272	Table 1 gives the distribution of categories assigned to the first word What in each question in the corpus.
185:272	The first row gives the category of object question What.
186:272	The second row is the object question determiner.
187:272	The third row is the subject question determiner.
188:272	And What Cruise Line does Kathie Gi ord advertise for ?
189:272	(S[wq]=(S[q]=NP))=N N=N N (S[q]=(S[b]nNP))=NP N=N N (S[b]nNP)=PP PP=NP : > > >BN N (S[b]nNP)=NP) >S[wq]=(S[q]=NP) NP >S[q]=(S[b]nNP) >BS[q]=NP >S[wq] S[wq] What English word contains the most letters ?
190:272	(S[wq]=(S[dcl]nNP))=N N=N N (S[dcl]nNP)=NP NP=N N=N N : > >N N > >S[wq]=(S[dcl]nNP) NP >S[dcl]nNP >S[wq] S[wq] Figure 6: Derivations for example What-questions; lexical categories are in bold the final row is the root subject question What.
191:272	For the examples in Figure 5, S[wq]=(S[q]=NP) appears in questions 16, (S[wq]=(S[q]=NP))=N in 711, (S[wq]=(S[dcl]nNP))=N in 1213, and S[wq]=(S[dcl]nNP) in 14.
192:272	6 Evaluation A development set was created by randomly selecting 171 questions.
193:272	For development purposes the remaining 1,000 questions were used for training; these were also used as a final cross-validation training/test set.
194:272	The average length of the tokenised questions in the whole corpus is 8.6 tokens.
195:272	The lexical category set used by the parser contains all categories which occur at least 10 times in CCGbank, giving a set of 409 categories.
196:272	In creating the question corpus we used a small number of new category types, of which 3 were needed to cover common question constructions.
197:272	One of these, (S[wq]=(S[dcl]nNP))=N, applies to What, as in the second example in Figure 6.
198:272	This category does appear in CCGbank, but so infrequently that it is not part of the parsers lexical category set.
199:272	Two more apply to question words like did and is; for example, (S[q]=(S[pss]nNP))=NP applies to is in What instrument is Ray Charles best known for playing?, and (S[q]=PP)=NP applies to is in What city in Florida is Sea World in?.
200:272	6.1 Supertagger Accuracy As an initial evaluation we tested the accuracy of just the supertagger on the development data.
201:272	The supertagger was run in two modes: one in which a single category was assigned to each word, and one in which 1.5 categories were assigned to each 1 CAT 1.5 CATS ACCURACY: WORD SENT WORD SENT MODEL CCGbank 72.0 2 84.8 11 Qs 92.3 67 96.6 81 Qs+CCGbank 93.1 61 98.1 87 10Qs+CCGbank 93.6 67 97.9 83 Table 2: Accuracy of supertagger on dev question data word, on average.
202:272	Table 2 gives the per-word accuracy on the development question data for a number of supertagging models; SENT accuracy gives the percentage of sentences for which every word is assigned the correct category.
203:272	Four supertagging models were used: one trained on CCGbank only; one trained on the 1,000 questions; one trained on the 1,000 questions plus CCGbank; and one trained on 10 copies of the 1,000 questions plus CCGbank.
204:272	The supertagger performs well when trained on the question data, and benefits from a combination of the questions and CCGbank.
205:272	To increase the influence of the questions, we tried adding 10 copies of the question data to CCGbank, but this had little impact on accuracy.
206:272	However, the supertagger performs extremely poorly when trained only on CCGbank.
207:272	One reason for the very low SENT accuracy figure is that many of the questions contain lexical categories which are not in the supertaggers category set derived from CCGbank: 56 of the 171 development questions have this property.
208:272	The parsing results in Clark and Curran (2004b) rely on a supertagger per-word accuracy of at least 97%, and a sentence accuracy of at least 60% (for 1.5 categories per word).
209:272	Thus the sentence accuSUPERTAGGING / ACCURACY PARSING METHOD WORD SENT WHAT Increasing av.
210:272	cats 94.6 82 91 Decreasing av.
211:272	cats 89.7 65 80 Increasing cats (rand) 93.4 79 88 Decreasing cats (rand) 64.0 9 21 Baseline 68.5 0 61 Table 3: Parser category accuracy on dev data racy of 11% confirms that our parsing system based only on CCGbank is quite inadequate for accurate question parsing.
212:272	6.2 Parser Accuracy Since the gold-standard question data is only labelled at the lexical category level, we are only able to perform a full evaluation at that level.
213:272	However, the scores in Clark and Curran (2004b) give an indication of how supertagging accuracy corresponds to overall dependency recovery.
214:272	In addition, in Section 6.3 we present an evaluation on object extraction dependencies in the development data.
215:272	We applied the parser to the 171 questions in the development data, using the supertagger model from the third row in Table 2, together with a loglinear parsing model trained on CCGbank.
216:272	We used the supertagging approach described in Section 2.1, in which a small number of categories is initially assigned to each word, and the parser requests more categories if a spanning analysis cannot be found.
217:272	We used 4 different values for the parameter (which determines the average number of categories per word): 0.5, 0.25, 0.075 and 0.01.
218:272	The average number of categories at each level for the development data is 1.1, 1.2, 1.6 and 3.8.
219:272	The parser provided an analysis for all but one of the 171 questions.
220:272	The first row of Table 3 gives the per-word, and sentence, category accuracy for the parser output.
221:272	Figures are also given for the accuracy of the categories assigned to the first word What.
222:272	The figures show that the parser is more accurate at supertagging than the single-category supertagger.
223:272	The second row gives the results if the original supertagging approach of Clark et al.224:272	(2002) is used, i.e. starting with a high number of categories per word, and reducing the number if the sentence cannot be parsed within reasonable space and time constraints.
225:272	The third row corresponds to our new supertagging approach, but chooses a derivation at random, by randomly traversing the packed chart representation used by the parser.
226:272	The fourth row corresponds to the supertagging approach of Clark et al.227:272	(2002), together with a random selection of SUPERTAGGING / ACCURACY PARSING METHOD WORD SENT WHAT Increasing av.
228:272	cats 94.4 79 92 Decreasing av.
229:272	cats 89.5 64 81 Table 4: Cross-validation results the derivation.
230:272	The baseline method in the fifth row assigns to a word the category most frequently seen with it in the data; for unseen words N is assigned.
231:272	The results in Table 3 demonstrate that our new supertagging approach is very effective.
232:272	The reason is that the parser typically uses the first supertagger level, where the average number of categories per word is only 1.1, and the per-word/sentence category accuracies are 95.5 and 70.8%, repsectively.
233:272	136 of the 171 questions (79.5%) are parsed at this level.
234:272	Since the number of categories per word is very small, the parser has little work to do in combining the categories; the supertagger is effectively an almost-parser (Bangalore and Joshi, 1999).
235:272	Thus the parsing model, which is not tuned for questions, is hardly used by the parser.
236:272	This interpretation is supported by the high scores for the random method in row 3 of the table.
237:272	In contrast, the previous supertagging method of Clark et al.238:272	(2002) results in a large derivation space, which must be searched using the parsing model.
239:272	Thus the accuracy of the parser is greatly reduced, as shown in rows 2 and 4.
240:272	As a final test of the robustness of our results, we performed a cross-validation experiment using the 1,000 training questions.
241:272	The 1,000 questions were randomly split into 10 chunks.
242:272	Each chunk was used as a test set in a separate run, with the remaining chunks as training data plus CCGbank.
243:272	Table 4 gives the results averaged over the 10 runs for the two supertagging approaches.
244:272	6.3 Object Extraction in Questions For the object extraction evaluation we considered the 36 questions in the development data which have the category (S[wq]=(S[q]=NP))=N assigned to What.
245:272	Table 7 gives examples of the questions.
246:272	We assume these are fairly representative of the kinds of object extraction found in other question types, and thus present a useful test set.
247:272	We parsed the questions using the best performing configuration from the previous section.
248:272	All but one of the sentences was given an analysis.
249:272	The perword/sentence category accuracies were 90.2% and 71.4%, respectively.
250:272	These figures are lower than for the corpus as a whole, suggesting these object extraction questions are more difficult than average.
251:272	What amount of folic acid should an expectant mother take daily?
252:272	What movie did Madilyn Kahn star in with Gene Wilder?
253:272	What continent is Egypt on?
254:272	What year was Ebbets Field, home of Brooklyn Dodgers, built?
255:272	What body of water does the Colorado River empty into?
256:272	Figure 7: Examples of object extraction questions We inspected the output to see if the object dependencies had been recovered correctly.
257:272	To get the object dependency correct in the first question in Table 7, for example, the parser would need to assign the correct category to take and return amount as the object of take.
258:272	Of the 37 extracted object dependencies (one question had two such dependencies), 29 (78.4%) were recovered correctly.
259:272	Given that the original parser trained on CCGbank performs extremely poorly on such questions, we consider this to be a highly promising result.
260:272	7 Conclusion We have presented a detailed evaluation of a CCG parser on object extraction dependencies in WSJ text.
261:272	Given the difficulty of the task, the accuracy of the parser is encouraging.
262:272	The errors made by the parser suggest that general improvements in the coverage of the lexicon and parsing models derived from CCGbank will lead to improved recovery of long-range object dependencies.
263:272	In contrast, we have suggested that general improvements in CCGbank parsing models will not lead to satisfactory performance on question parsing.
264:272	The reason is that the Wh-question domain is syntactically distinct from WSJ text.
265:272	We have presented a novel method for porting the parser to the question domain, which has led to good performance on question parsing.
266:272	This has also demonstrated the close integration of the supertagger and the CCG parser on which our method depends.
267:272	One of the major drawbacks of current NLP technology is that in general it performs very poorly outside of the training data domain.
268:272	Our porting method only requires lexical category data, which is far easier to produce than full parse trees.
269:272	This is an efficient method for porting the parser to other domains.
270:272	The method may also be applicable to other lexicalised grammar formalisms.
271:272	We will extend the question corpus to other question types.
272:272	We are also continuing to develop the supertagger, which we have demonstrated is central to efficient portable wide-coverage CCG parsing.

