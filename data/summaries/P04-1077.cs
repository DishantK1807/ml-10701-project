Saggion et al.(2002) used normalized pairwise LCS (NP-LCS) to compare similarity between two texts in automatic summarization evaluation.
C02-1073
Melamed et al.(2003) used unigram F-measure to estimate machine translation quality and showed that unigram F-measure was as good as BLEU.
N03-2021
ROUGE-L, ROUGE-W, and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results (Lin 2004).
C04-1072 W04-1013
In Lin and Och (2004), we proposed a framework that automatically evaluated automatic MT evaluation metrics using only manual translations without further human involvement.
C04-1072 W04-1013
Although brevity penalty will penalize candidate translations with low recall by a factor of e (1|r|/|c|), it would be nice if we can use the traditional recall measure that has been a well known measure in NLP as suggested by Melamed (2003).
N03-2021
Lin, C.Y. 2004.
C04-1072 W04-1013
Melamed, I.D. 1995.
W95-0115
Paraphrases can also be automatically acquired using statistical methods as shown by Barzilay and Lee (2003).
N03-1003
Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995).
W95-0115
Melamed (1995) used the ratio (LCSR) between the length of the LCS of two words and the length of the longer word of the two words to measure the cognateness between them.
W95-0115
In Proceedings of 20 th International Conference on Computational Linguistic (COLING 2004), Geneva, Switzerland.
C04-1072 W04-1013
Su et al.(1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations.
C92-2067
