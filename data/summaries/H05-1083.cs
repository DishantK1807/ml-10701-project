These feature functions bear similarity to rules used in other coreference systems (Lappin and Leass, 1994; Mitkov, 1998; Stuckardt, 2001), except that the feature weights fλig are automatically trained over a corpus with coreference information.
J01-4002 J94-4002 P98-2143
As for parser, we train three off-shelf maximum-entropy parsers (Ratnaparkhi, 1999) using the Arabic, Chinese and English Penn treebank (Maamouri and Bies, 2004; Xia et al., 2000; Marcus et al., 1993).
J93-2004 W04-1602
Feature weights are automatically trained in our system while (Lappin and Leass, 1994; Stuckardt, 2001) assign weights manually.
J01-4002 J94-4002
The number of common mentions of the best alignment is 2 2Another possible choice is the MUC F-measure (Vilain et al., 1995).
M95-1005
Due to the one-to-one entity alignment constraint, theF-measure here is more stringent than the accuracy (Ge et al., 1998; Mitkov, 1998; Kehler et al., 2004) computed on antecedent-pronoun pairs.
N04-1037 P98-2143 W98-1119
Kehler et al.(2004) experiments making use of predicate-argument structure extracted from a large TDT-corpus.
N04-1037
Lappin and Leass (1994) extracted rules from the output of the English Slot Grammar (ESG) (McCord, 1993).
H93-1025 J94-4002
This is possible because of the availability of statistical parsers, which can be trained on human-annotated treebanks (Marcus et al., 1993; Xia et al., 2000; Maamouri and Bies, 2004) for multiple languages; (2) The binding theory is used as a guideline and syntactic structures are encoded as features in a maximum entropy coreference system; (3) The syntactic features are evaluated on three languages: Arabic, Chinese and English (one goal is to see if features motivated by the English language can help coreference resolution in other languages).
J93-2004 W04-1602
This contrasts with the approach of extracting rules and assigning weights to these rules by hand (Lappin and Leass, 1994; Stuckardt, 2001).
J01-4002 J94-4002
Morton. 2000.
P00-1023
Previous pronoun resolution work (Hobbs, 1976; Lappin and Leass, 1994; Ge et al., 1998; Stuckardt, 2001) explicitly utilized syntactic information before.
J01-4002 J94-4002 W98-1119
But the metric has a systematic bias for systems generating fewer entities (Bagga and Baldwin, 1998) see Luo (2005).
H05-1004
There are a large amount of published work (Morton, 2000; Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003; Luo et al., 2004; Kehler et al., 2004) using machine-learning techniques in coreference resolution.
J01-4004 N04-1037 P00-1023 P02-1014 P03-1023 P04-1018
Details of these features can be found in (Luo et al., 2004).
P04-1018
3Ng and Cardie (2002) used a BINDING feature, but it is not clear from their paper how the feature was computed and what its impact was on their system.
P02-1014
Ge et al.(1998) uses a non-parametrized statistical model to nd the antecedent from a list of candidates generated by applying the Hobbs algorithm to the English Penn Treebank.
W98-1119
For this reason, we compute an unweighted entity-constrained mention F-measure (Luo, 2005) and report all contrastive experiments with this metric.
H05-1004
In our implementation, the link model is computed as PL(L = 1je, m) max mprime∈e ˆPL(L = 1je, mprime, m), (1) where mprime is one mention in entity e, and the basic model building block ˆPL(L = 1je, mprime, m) is an exponential or maximum entropy model (Berger et al., 1996): ˆPL(Lje, mprime, m) = exp braceleftbig summationtext i λigi(e, m prime, m, L)bracerightbig Z(e, mprime, m), (2) where Z(e, mprime, m) is a normalizing factor to ensure that ˆPL( je, mprime, m) is a probability, fgi(e, mprime, m, L)g are features and fλig are feature weights.
J96-1002
Compared with these work, our work uses machine-generated parse trees from which trainable features are extracted in a maximum-entropy coreference system, while (Ge et al., 1998) assumes that correct parse trees are given.
W98-1119
