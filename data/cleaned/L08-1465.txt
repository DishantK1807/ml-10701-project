<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>S Argamon</author>
<author>C Whitelaw</author>
<author>P Chase</author>
<author>S R Hota</author>
<author>N Garg</author>
<author>S Levitan</author>
</authors>
<title>Stylistic text classification using functional lexical features: Research Articles</title>
<date>2007</date>
<journal>J</journal>
<marker>Argamon, Whitelaw, Chase, Hota, Garg, Levitan, 2007</marker>
<rawString>Argamon, S., Whitelaw, C., Chase, P., Hota, S. R., Garg, N., and Levitan, S. (2007) Stylistic text classification using functional lexical features: Research Articles. J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soc</author>
</authors>
<date>2007</date>
<journal>Inf. Sci. Technol</journal>
<volume>58</volume>
<pages>802--822</pages>
<marker>Soc, 2007</marker>
<rawString>Am. Soc. Inf. Sci. Technol. 58, 6 (Apr. 2007), 802-822.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Artstein</author>
<author>M Poesio</author>
</authors>
<title>Kappa3 = Alpha (or Beta</title>
<date>2005</date>
<tech>Technical Report NLE Technote 2005-01</tech>
<institution>University of Essex</institution>
<location>Essex</location>
<contexts>
<context>ge, versus biographical background on the artist (Passonneau et al. 2007). Interannotator agreement (IA) on semantic or pragmatic annotation tasks such as ours is typically difficult to achieve [see (Artstein &amp; Poesio 2005) for a brief review]. Because, variation within and across individuals is an inherent feature of language use, we decided to investigate how this variation affects learning performance. In consultati</context>
<context>ss that it is highly sensitive to the number and absolute frequency of categories assigned. If two categories are used, one of which is extremely frequent, percent agreement will necessarily be high (Artstein &amp; Poesio 2005). While we use more robust methods for quantifying inter-annotator agreement, we find a similar range of values across four labeling experiments we conducted. Giral and Taylor (1993) looked at indexi</context>
</contexts>
<marker>Artstein, Poesio, 2005</marker>
<rawString>Artstein, R. and M. Poesio. (2005) Kappa3 = Alpha (or Beta). Technical Report NLE Technote 2005-01, University of Essex, Essex, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baca</author>
</authors>
<title>Practical Issues in Applying Metadata Schemas and Controlled Vocabularies to Cultural Heritage Information</title>
<date>2003</date>
<publisher>The Haworth Press, Inc</publisher>
<contexts>
<context>ages. Our categories were derived from what we observed in the texts, but have a loose correspondence with categories of information discussed in the image indexing literature (Layne 1994; Chen 2001; Baca 2003). By marking up electronic text with these categories, catalogers can select the type of information they want to see in searching for metadata. Figure 1 in the next section illustrates three of the </context>
</contexts>
<marker>Baca, 2003</marker>
<rawString>Baca, M. (2003) Practical Issues in Applying Metadata Schemas and Controlled Vocabularies to Cultural Heritage Information. The Haworth Press, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Chen</author>
</authors>
<title>An analysis of image queries in the field of art history</title>
<date>2001</date>
<journal>Journal of the American Society for Information Science and Technology</journal>
<pages>260--273</pages>
<contexts>
<context>specific images. Our categories were derived from what we observed in the texts, but have a loose correspondence with categories of information discussed in the image indexing literature (Layne 1994; Chen 2001; Baca 2003). By marking up electronic text with these categories, catalogers can select the type of information they want to see in searching for metadata. Figure 1 in the next section illustrates th</context>
</contexts>
<marker>Chen, 2001</marker>
<rawString>Chen, H. (2001) An analysis of image queries in the field of art history. Journal of the American Society for Information Science and Technology, pages 260–273.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hachey</author>
<author>C Grover</author>
</authors>
<title>Sentence classification experiments for legal text summarisation</title>
<date>2004</date>
<booktitle>In Proceedings of the 17th Annual Conference on Legal Knowledge and Information Systems (Jurix</booktitle>
<marker>Hachey, Grover, 2004</marker>
<rawString>Hachey, B. and C. Grover. (2004)  Sentence classification experiments for legal text summarisation. In Proceedings of the 17th Annual Conference on Legal Knowledge and Information Systems (Jurix).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Forman</author>
</authors>
<title>An extensive empirical study of feature selection metrics for text classification</title>
<date>2003</date>
<contexts>
<context>ks. Note that the part-of-speech features do well, particularly for the Image Content class. This is not the case for typical text classification tasks, which generally do best with bag-of-words (cf. Forman 2003). The best performing pos feature for Image Content is present tense, which corresponds to cases we observe where the image is described as a visual “tour” in present tense. The key issue of interest</context>
</contexts>
<marker>Forman, 2003</marker>
<rawString>Forman, G. (2003) An extensive empirical study of feature selection metrics for text classification.</rawString>
</citation>
<citation valid="true">
<date>2003</date>
<journal>Journal of Machine Learning. Research</journal>
<volume>3</volume>
<pages>1289--1305</pages>
<marker>2003</marker>
<rawString>Journal of  Machine Learning. Research 3 (Mar. 2003), 1289-1305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Giral</author>
<author>A Taylor</author>
</authors>
<title>Indexing overlap and consistency between the Avery Index to</title>
<date>1993</date>
<booktitle>Architectural Periodicals and the Architectural Periodicals Index. Library Resources and Technical Services 37(1):1944</booktitle>
<contexts>
<context>tegories that we focus on in this paper. Our goals in conducting our pilot annotation studies were to understand why previous investigators have found such a wide range of agreement on similar tasks (Giral &amp; Taylor 1993; Markey 1984), and to develop annotation specifications for our large scale study. We conducted four experiments under a variety of annotation constraints to guide the design of a large scale annotat</context>
<context>present noise. 4. Human Labeling Given the wide range of measures of human agreement on a related task where librarians classify documents with respect to an existing set of categories (Markey 1984) (Giral &amp; Taylor 1993), we wanted to understand what factors might lead to variations on IA in our task. We conducted four pilot studies on the labeling where we varied the number of labels that could be assigned to a sin</context>
</contexts>
<marker>Giral, Taylor, 1993</marker>
<rawString>Giral, A. and A. Taylor. (1993) Indexing overlap and consistency between the Avery Index to Architectural Periodicals and the Architectural Periodicals Index. Library Resources and Technical Services 37(1):1944.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jaccard</author>
</authors>
<title>Nouvelles recherches sur la distribution florale</title>
<date>1908</date>
<booktitle>Bulletin de la Societe Vaudoise des Sciences Naturelles</booktitle>
<pages>44--223</pages>
<marker>Jaccard, 1908</marker>
<rawString>Jaccard, P. (1908). Nouvelles recherches sur la distribution florale. Bulletin de la Societe Vaudoise des Sciences Naturelles 44:223-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Klavans</author>
<author>T Sidhu</author>
<author>C Sheffield</author>
<author>D Soergel</author>
<author>J Lin</author>
<author>E Abels</author>
<author>R Passonneau</author>
</authors>
<title>Computational Linguistics for Metadata Building (CLiMB) Text Mining for the Automatic Extraction of Subject Terms for Image Metadata</title>
<date>2008</date>
<booktitle>International Conference on Computer Vision Theory and Applications, Workshop 3: Metadata Mining for Image Understanding</booktitle>
<location>Funchal, Portugal</location>
<contexts>
<context>rly subjective phenomena. 1. Introduction We conducted a series of pilot annotation studies in the context of identifying specifications for marking up textual input for an image cataloger’s toolkit (Klavans et al. 2008). Given an image, and a text extract that describes the work depicted in the image, we aimed to identify the semantic functions of the text. By semantic function, we mean the type of information prov</context>
</contexts>
<marker>Klavans, Sidhu, Sheffield, Soergel, Lin, Abels, Passonneau, 2008</marker>
<rawString>Klavans, J.; Sidhu, T.; Sheffield, C.; Soergel, D.; Lin, J.; Abels, E.; Passonneau, R. (2008) Computational Linguistics for Metadata Building (CLiMB) Text Mining for the Automatic Extraction of Subject Terms for Image Metadata. International Conference on Computer Vision Theory and Applications, Workshop 3: Metadata Mining for Image Understanding, Funchal, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology. Sage Publications</title>
<date>1980</date>
<location>Beverly Hills, CA</location>
<contexts>
<context> trainees). Experiments one through three were done on paper and pencil or electronic editors; for four a and four b we implemented a labeling interface. To measure IA, we use Krippendorff’s Alpha 1 (Krippendorff 1980) along with MASI, a set-based distance measure (Passonneau 2006). MASI allows partial credit when the set of labels chosen by one annotator overlaps another’s set. Used in the context of Alpha, MASI </context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Krippendorff, K. (1980). Content Analysis: An Introduction to Its Methodology. Sage Publications, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S S Layne</author>
</authors>
<title>Some issues in the indexing of images</title>
<date>1994</date>
<journal>Journal of the American Society for Information Science</journal>
<pages>583--8</pages>
<contexts>
<context>ciated with specific images. Our categories were derived from what we observed in the texts, but have a loose correspondence with categories of information discussed in the image indexing literature (Layne 1994; Chen 2001; Baca 2003). By marking up electronic text with these categories, catalogers can select the type of information they want to see in searching for metadata. Figure 1 in the next section ill</context>
</contexts>
<marker>Layne, 1994</marker>
<rawString>Layne, S. S. (1994) Some issues in the indexing of images. Journal of the American Society for Information Science, pages 583–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Markey</author>
</authors>
<title>Interindexer consistency tests: a literature review and report of a test of consistency in indexing visual materials</title>
<date>1984</date>
<booktitle>Library and Information Science Research</booktitle>
<pages>155--177</pages>
<contexts>
<context>s on in this paper. Our goals in conducting our pilot annotation studies were to understand why previous investigators have found such a wide range of agreement on similar tasks (Giral &amp; Taylor 1993; Markey 1984), and to develop annotation specifications for our large scale study. We conducted four experiments under a variety of annotation constraints to guide the design of a large scale annotation effort. O</context>
<context>ements that represent noise. 4. Human Labeling Given the wide range of measures of human agreement on a related task where librarians classify documents with respect to an existing set of categories (Markey 1984) (Giral &amp; Taylor 1993), we wanted to understand what factors might lead to variations on IA in our task. We conducted four pilot studies on the labeling where we varied the number of labels that coul</context>
</contexts>
<marker>Markey, 1984</marker>
<rawString>Markey, K. (1984) Interindexer consistency tests: a literature review and report of a test of consistency in indexing visual materials. Library and Information Science Research, pages 155–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Passonneau</author>
<author>T Yano</author>
<author>T Lippincott</author>
<author>J Klavans</author>
</authors>
<title>Functional semantic categories for art history text: human labeling and preliminary machine learning</title>
<date>2008</date>
<booktitle>International Conference on Computer Vision Theory and Applications, Workshop 3: Metadata Mining for Image Understanding</booktitle>
<location>Funchal, Portugal</location>
<marker>Passonneau, Yano, Lippincott, Klavans, 2008</marker>
<rawString>Passonneau, R; Yano, T.; Lippincott, T.; Klavans, J. (2008). Functional semantic categories for art history text: human labeling and preliminary machine learning. International Conference on Computer Vision Theory and Applications, Workshop 3: Metadata Mining for Image Understanding, Funchal, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Passonneau</author>
</authors>
<title>Measuring agreement on setvalued items (MASI) for semantic and pragmatic annotation</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC</booktitle>
<location>Portugal</location>
<contexts>
<context>encil or electronic editors; for four a and four b we implemented a labeling interface. To measure IA, we use Krippendorff’s Alpha 1 (Krippendorff 1980) along with MASI, a set-based distance measure (Passonneau 2006). MASI allows partial credit when the set of labels chosen by one annotator overlaps another’s set. Used in the context of Alpha, MASI weights the comparison of every pair of annotators’ choices for </context>
<context>Content, Historical Context) and (Image Content, Significance) would get a MASI distance of (1/2 x 1/3) for partial agreement rather than 0 for complete agreement or 1 for complete disagreement; see (Passonneau 2006) for details. Because IA coefficients do not directly capture the quantity of matches across annotations, we also report the average F measure taking each next annotation as the target of comparison.</context>
<context>t because IA coefficients do not have a known probability distribution, and because they are applied to many kinds of data and for many types of annotator judgements, there is no one ideal threshold (Passonneau 2006; Passonneau et al 2006). Instead, we suggest that interpretation of IA values is an empirical question that depends in part on how the data will be used. It can be investigated in many ways, for exam</context>
</contexts>
<marker>Passonneau, 2006</marker>
<rawString>Passonneau, R. (2006). Measuring agreement on setvalued items (MASI) for semantic and pragmatic annotation. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC). Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Passonneau</author>
<author>N Habash</author>
<author>O Rambow</author>
</authors>
<title>Interannotatator agreement on a multilingual semantic annotation task</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth International Conference on Language Resources and Evaluation</booktitle>
<note>To appear in Computational Linguistics</note>
<contexts>
<context>ficients do not have a known probability distribution, and because they are applied to many kinds of data and for many types of annotator judgements, there is no one ideal threshold (Passonneau 2006; Passonneau et al 2006). Instead, we suggest that interpretation of IA values is an empirical question that depends in part on how the data will be used. It can be investigated in many ways, for example relating measures o</context>
<context>parison of rows 1-3 with 4 and 5 in Table 1 indicate that IA is higher when annotators can select multiple labels, which is consistent with our previous results on a lexical semantic annotation task (Passonneau et al., 2006). The biggest drop in IA, at experiments 4a and 4b, is due to the constraint that labelers select a single label. We also found that IA varies widely between annotators, suggesting that the task is i</context>
</contexts>
<marker>Passonneau, Habash, Rambow, 2006</marker>
<rawString>Passonneau, R.; Habash, N.; Rambow, O. (2006) Interannotatator agreement on a multilingual semantic annotation task. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC). Portugal. Riedsma, D. and Carletta, J. (Forthcoming) Reliability measurement: there’s no safe limit To appear in Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine learning in automated text categorization</title>
<date>2002</date>
<journal>ACM Computing Surveys</journal>
<volume>34</volume>
<contexts>
<context>s, which can perform well even when the independence assumption is violated. We trained a binary NB classifier for each semantic category; it performs better with smaller corpora than multinomial NB (Sebastiani, 2002). Also, we wanted to investigate the relation of IA to learning for each semantic category independent of the others. 5.1. Data sets To look at the relation between IA and machine learning performanc</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>Sebastiani, F. (2002) Machine learning in automated text categorization. ACM Computing Surveys, 34(1):1–47, March 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>M Moens</author>
</authors>
<title>Summarising scientific articles – experiments with relevance and rhetorical status. Computational Linguistics</title>
<date>2002</date>
<pages>409--445</pages>
<marker>Teufel, Moens, 2002</marker>
<rawString>Teufel, S. and M. Moens. (2002) Summarising scientific articles – experiments with relevance and rhetorical status. Computational Linguistics, pages 409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining</title>
<date>2005</date>
<booktitle>Practical Machine Learning Tools and Techniques, 2nd Edition</booktitle>
<contexts>
<context>ity of three of our functional semantic categories: Image Content, Historical Context and Implementation. There were insufficient examples from the other categories. All learning was done using WEKA (Witten and Frank, 2005). Due to the small size of our dataset, and the similarities of the categories, we used Naïve Bayes, which can perform well even when the independence assumption is violated. We trained a binary NB c</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Witten, I. H. and E. Frank (2005) Data Mining: Practical Machine Learning Tools and Techniques, 2nd Edition.</rawString>
</citation>
<citation valid="false">
<date>2005</date>
<publisher>Morgan Kaufmann</publisher>
<location>San Francisco</location>
<marker>2005</marker>
<rawString>Morgan Kaufmann, San Francisco, 2005.</rawString>
</citation>
</citationList>
</algorithm>

