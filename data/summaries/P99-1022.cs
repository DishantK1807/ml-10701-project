Cache language models (Kuhn and de Mori (1992),Rosenfeld (1994)) try to overcome this limitation by boosting the probability of the words already seen in the history; trigger models (Lau et al.(1993)), even more general, try to capture the interrelationships between words.
H94-1013
Models based on syntactic structure (Chelba and Jelinek (1998), Wright et al.(1993)) effectively estimate intra-sentence syntactic word dependencies.
P98-1035
Iyer et al.(1994) used bottom-up clustering techniques on discourse contexts, performing sentencelevel model interpolation with weights updated dynamically through an EM-like procedure.
H94-1014
Statistical language models are core components of speech recognizers, optical character recognizers and even some machine translation systems Brown et al.(1990). The most common language modeling paradigm used today is based on n-grams, local word sequences.
J90-2002
