<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>P BoersmaandD Weenink</author>
</authors>
<title>PRAAT:Doingphonetics by computer (version 4.6.34). http://www.praat.org/. Computer program</title>
<date>2007</date>
<contexts>
<context> MoveOn recordings involves two independent steps: (i) annotation of speech data, and (ii) annotation of background noise and transient interferences. Both steps are realized using PRAAT (Boersma and Weenink, 2007). The result of the annotation procedure is saved in the TextGrid file format (Boersma and Weenink, 2007). 6.1. Speech Annotation During the annotation of the speech data two relevant categories are </context>
</contexts>
<marker>Weenink, 2007</marker>
<rawString>P.BoersmaandD.Weenink. 2007. PRAAT:Doingphonetics by computer (version 4.6.34). http://www.praat.org/. Computer program.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cowie</author>
<author>E Douglas-Cowie</author>
<author>S Savvidou</author>
<author>E McMahon</author>
<author>M Sawey</author>
<author>M Schr¨oder</author>
</authors>
<title>feeltrace’: An instrument for recording perceived emotion in real time</title>
<date>2000</date>
<booktitle>In Proceedings of the ISCA Workshop on Speech and Emotion</booktitle>
<pages>pages</pages>
<publisher>ISCA</publisher>
<marker>Cowie, Douglas-Cowie, Savvidou, McMahon, Sawey, Schr¨oder, 2000</marker>
<rawString>R. Cowie, E. Douglas-Cowie, S. Savvidou, E. McMahon, M. Sawey, and M. Schr¨oder. 2000. ’feeltrace’: An instrument for recording perceived emotion in real time. In Proceedings of the ISCA Workshop on Speech and Emotion, pages 19–24. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Matassoni L Cristoforetti</author>
<author>andP Svaizer M Omologo</author>
</authors>
<title>Use of parallel recognizers for robust in-car speech interaction</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 IEEE International Conference on Acoustic, Speech, and Signal Processing (ICASSP ’03</booktitle>
<volume>1</volume>
<marker>Cristoforetti, Omologo, 2003</marker>
<rawString>L.Cristoforetti, M.Matassoni, M.Omologo, andP.Svaizer. 2003. Use of parallel recognizers for robust in-car speech interaction. In Proceedings of the 2003 IEEE International Conference on Acoustic, Speech, and Signal Processing (ICASSP ’03), volume 1, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V D Heuvel</author>
<author>L Boves</author>
<author>A Moreno</author>
<author>M Omologo</author>
<author>G Richard</author>
<author>E Sanders</author>
</authors>
<title>Annotation in the SpeechDat projects</title>
<date>2001</date>
<journal>International Journal of Speech Technology</journal>
<pages>4--127</pages>
<contexts>
<context>lication words, as well as their most often used synonyms. For balancing the phonetic contents, a number of phonetically rich sentences, taken from the British English SpeechDat(II)-FDB4000 database (Heuvel et al., 2001), were added. Due to the specifics of the MoveOn application – involving hands-busy and eyes-busy motorcyclists – the prompt sheet provided was a sequence of recorded audio prompts. Table 1 presents </context>
<context>dinarydictionaryoftheEnglishlanguage (Heuveletal., 2001). Wordtruncations, mispronunciations, non-understandable speech and non-speech acoustic events are denoted following the SpeechDat conventions (Heuvel et al., 2001). In the annotation of the emotional contents, i.e. the affect tier, the annotators were asked to place an appropriate affect marker for each transcribed utterance, based on their human intuition. Ta</context>
</contexts>
<marker>Heuvel, Boves, Moreno, Omologo, Richard, Sanders, 2001</marker>
<rawString>V.D. Heuvel, L. Boves, A. Moreno, M. Omologo, G. Richard, and E. Sanders. 2001. Annotation in the SpeechDat projects. International Journal of Speech Technology, 4:127–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kaiser</author>
<author>H M¨ogele</author>
<author>F Schiel</author>
</authors>
<title>Bikers accessing the web: The SmartWeb Motorbike Corpus</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation, (LREC</booktitle>
<pages>1628--1631</pages>
<publisher>ELRA</publisher>
<location>Genova, Italy</location>
<marker>Kaiser, M¨ogele, Schiel, 2006</marker>
<rawString>M. Kaiser, H. M¨ogele, and F. Schiel. 2006. Bikers accessing the web: The SmartWeb Motorbike Corpus. In Proceedings of the 5th International Conference on Language Resources and Evaluation, (LREC 2006), pages 1628–1631, Genova, Italy, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Kun</author>
<author>W T Miller</author>
<author>W H Lenharth</author>
</authors>
<title>Evaluating the user interfaces of an integrated system of incar electronic devices</title>
<date>2005</date>
<booktitle>In Proceedings of the 8th International IEEE Conference on Intelligent Transportation Systems</booktitle>
<location>Vienna, Austria</location>
<contexts>
<context> and results of the database creation. 1. Introduction Robust speech recognition for hands-free speech input in the vehicle environment is a challenging field of research (Cristoforetti et al., 2003; Kun et al., 2005; Li et al., 2007). Suitable speech data for training and testing such a system is vitally important. SpeechDat-Car (Moreno et al., 2000), for example, is a corpora with speech data recorded in a car </context>
</contexts>
<marker>Kun, Miller, Lenharth, 2005</marker>
<rawString>A.L. Kun, W.T. Miller, and W.H. Lenharth. 2005. Evaluating the user interfaces of an integrated system of incar electronic devices. In Proceedings of the 8th International IEEE Conference on Intelligent Transportation Systems, Vienna, Austria, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Li</author>
<author>K Takeda</author>
<author>F Itakura</author>
</authors>
<title>Robust in-car speech recognition based on nonlinear multiple regressions</title>
<date>2007</date>
<journal>EURASIP J. Appl. Signal Process</journal>
<volume>2007</volume>
<contexts>
<context>e database creation. 1. Introduction Robust speech recognition for hands-free speech input in the vehicle environment is a challenging field of research (Cristoforetti et al., 2003; Kun et al., 2005; Li et al., 2007). Suitable speech data for training and testing such a system is vitally important. SpeechDat-Car (Moreno et al., 2000), for example, is a corpora with speech data recorded in a car environment. An e</context>
</contexts>
<marker>Li, Takeda, Itakura, 2007</marker>
<rawString>W. Li, K. Takeda, and F. Itakura. 2007. Robust in-car speech recognition based on nonlinear multiple regressions. EURASIP J. Appl. Signal Process., 2007(1):5–5.</rawString>
</citation>
</citationList>
</algorithm>

