Several of these parsers generate constituents by conditioning probabilities on non-terminal labels, part-of-speech (POS) tags, and some headword information (Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000).
A00-2018
As in (Ratnaparkhi, 1999; Charniak, 2000; Collins, 1999), we evaluate on all sentences with length • 40 words (2,245 sentences) and length • 100 words (2,416 sentences).
A00-2018
An almost-parsing LM based on CDG has been developed in (Wang and Harper, 2002).
W02-1031
Modifiee lexical features: The SuperARV structure employed in the SuperARV LM (Wang and Harper, 2002) uses only lexical categories of modifiees as modifiee constraints.
W02-1031
Statistical parsing has been an important focus of recent research (Magerman, 1995; Eisner, 1996; Charniak, 1997; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000).
A00-2018 P95-1037
to Other Parsers Charniak’s state-of-the-art PCFG parser (Charniak, 2000) has achieved the highest PARSEVAL LP/LR when compared to Collins’ Model 2 and Model 3 (Collins, 1999), Roark’s (Roark, 2001), Ratnaparkhi’s (Ratnaparkhi, 1999), and Xu & Chelba’s (Xu et al., 2002) parsers.
A00-2018 J01-2004 P02-1025
This lexicalization is able to include not only lexical category and syntactic constraints, but also a rich set of lexical features to model subcategorization and wh-movement without a combinatorial explosion of the parametric space (Wang and Harper, 2002).
W02-1031
Models • 40 words (2,245 sentences) Without TRACE All (1,903 sentences) (2,245 sentences) governor only all roles governor only all roles RLP RLR RLP RLR RLP RLR RLP RLR L 92.4 92.4 89.5 88.7 92.4 92.3 89.1 88.6 T 93.2 92.9 89.9 89.3 93.2 92.9 89.8 89.2 Charniak (Charniak, 2000) 92.6 92.5 89.4 88.9 92.5 92.3 88.9 88.7 Collins, Model 2 (Collins, 1999) 92.5 92.3 89.1 88.5 92.2 92.1 89.0 88.5 Collins, Model 3 (Collins, 1999) 92.8 92.7 89.9 89.4 92.7 92.4 89.3 89.1 Models • 100 words (2,416 sentences) Without TRACE All (1,979 sentences) (2,416 sentences) governor only all roles governor only all roles RLP RLR RLP RLR RLP RLR RLP RLR L 91.9 91.6 88.8 88.1 91.8 91.5 88.5 87.8 T 92.7 92.3 89.4 88.7 92.6 92.2 89.1 88.5 Charniak (Charniak, 2000) 92.0 91.8 88.8 88.2 91.9 91.6 88.4 87.9 Collins, Model 2 (Collins, 1999) 91.8 91.6 88.6 88.0 91.7 91.5 88.2 87.9 Collins, Model 3 (Collins, 1999) 92.2 92.1 89.4 88.8 92.1 91.9 88.8 88.5 CFG parsers may loose accuracy from the CFG-toCDG transformation, similarly to Collins’ experiment reported in (Hajic et al., 1998), we also transformed our CDG parses to Penn Treebank style CFG parse trees and scored them using PARSEVAL.
A00-2018
Charniak (Charniak, 2000) developed a state-of-the-art statistical CFG parser and then built an effective language model based on it (Charniak, 2001).
A00-2018 P01-1017
SuperARV LMs have been effective at reducing word error rate (WER) on wide variety of continuous speech recognition (CSR) tasks, including Wall Street Journal (Wang and Harper, 2002), Broadcast News (Wang et al., 2003), and Switchboard tasks (Wang et al., 2004).
W02-1031
Magerman. 1995.
P95-1037
