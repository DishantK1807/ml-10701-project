Many of these studies aimed at finding classes based on co-occurrences, often combined with the aim of establishing semantic similarity between words (McMahon and Smith, 1996, Brown et el., 1992, Dagan, Markus, and Markovitch, 1993, Dagan, Pereira, and Lee, 1994, Pereira and Tishby, 1992, Grefenstette, 1992).
J92-4003 J96-2003 P93-1022 P94-1038
These have been used in various studies in this field, see (Collins, 1996, Magerman, 1995, Jelinek et el., 1994).
H94-1052 P95-1037 P96-1025
Besides hard algorithms there have also been studies to soft clustering (Pereira, Tishby, and Lee, 1993, Dagan, Pereira, and Lee, 1994) where the distribution of every word is smoothed with the nearest k words rather than placed in a class which supposedly has a uniform behavior.
P93-1024 P94-1038
In fact, in (Dagan, Markus, and Markovitch, 1993) it was argued that reduction to a relatively small number of predetermined word classes or clusters may lead to substantial loss of information.
P93-1022
This is similar to what McMahon (1996) refers to as a structural tag.
J96-2003
Another application of hard clustering methods (in particular bottom-up variants) is that they can also produce a binary tree, which can be used for decision-tree based systems such as the SPATTER parser (Magerman, 1995) or the ATR Decision-Tree Part-OfSpeech Tagger (Black et al., 1992, Ushioda, 1996).
H92-1023 P95-1037 W96-0103
Various methods are based on Mutual Information between classes, see (Brown et al., 1992, McMahon and Smith, 1996, Kneser and Ney, 1993, Jardino and Adda, 1993, Martin, Liermann, and Ney, 1995, Ueberla, 1995).
J92-4003 J96-2003
A second application of the binary word tree can be found in decision-tree based systems such as the SPATTER parser (Magerman, 1995) or the ATR Decision-Tree Part-Of-Speech Tagger, as described by Ushioda (Ushioda, 1996).
P95-1037 W96-0103
A solution along this line was chosen for co-occurrence based clustering in (McMahon and Smith, 1996), where a first algorithm handles more frequent words, and a second algorithm adds the low-frequency words afterwards.
J96-2003
Studies in distribution based clustering often use the KuUback-Leibler (KL) distance, see for example (Pereira, Tishby, and Lee, 1993, Dagan, Pereira, and Lee, 1994).
P93-1024 P94-1038
Following the techniques suggested by (Collins, 1996), a parse tree can subsequently be described as a set of dependencies.
P96-1025
