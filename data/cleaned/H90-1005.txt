Finite-State Approximations of Grammars Fernando Pereira AT&T Bell Laboratories 600 Mountain Ave., Murray Hill, NJ 07974 Motivation Grammars for spoken language systems are subject to the conflicting requirements of language modeling for recognition and of language analysis for sentence interpretation.
Current recognition algorithms can most directly use finite-state acceptor (FSA) language models.
However, these models are inadequate for language interpretation, since they cannot express the relevant syntactic and semantic regularities.
Augmented phrase structure grammar (APSG) formalisms, such as unification grammars, can express many of those regularities, but they are computationally less suitable for language modeling, because of the inherent cost of computing state transitions in APSG parsers.
The above problems might be circumvented by using separate grammars for language modeling and language interpretation.
Ideally, the recognition grammar should not reject sentences acceptable by the interpretation grammar, but it should contain as much as possible of the constraints built into the interpretation grammar.
However, if the two grammars are built independently, those constraints are difficult to maintain.
For this reason, we have developed a method for constructing automatically a finite-state approximation for an 'APSG.
Since the purpose of the approximation is to serve as a filter in a speech-recognition front-end to the real parser, the approximation language is a superset of the language accepted by the APSG.
The term "approximation" will always be used in this sense in what follows.
If no further constraints were placed on the closeness of the approximation, the trivial algorithm that assigns to any APSG over alphabet ~ the regular language ~* would do.
Clearly, this is not what is required.
One possible criterion for "goodness" of approximation arises from the observation that many interesting phrasestructure grammars have substantial parts that accept regular languages.
That does not mean that the grammar rules are in the standard forms for defining :regular languages (left-linear or right-linear), because syntactic and semantic considerations often require that strings in a regular set be assigned structural descriptions not definable by regular productions.
A useful criterion is thus that if a grammar generates a regular language, the approximation algorithm yields an acceptor for that :regular language.
In other words, one would like the algorithm to be exact for APSGs yielding regular languages.
We have not yet proved that our method satisfies the above exactness criterion, but some experiments have shown that the method is exact for a variety of interesting grammars.
The Algorithm Our approximation method applies to any context-free grammar (CFG), or any unification grammar that can be fully expanded into a context-free grammar) The resulting FSA accepts a superset of the sentences accepted by the input grammar.
The current implementation accepts as input a form of unification grammar in which features can take only atomic values drawn from a specified finite set.
It is clear that such grammars can only generate context-free languages, since an equivalent CFG can be obtained by instantiating features in rules in all possible ways.
The heart of our approximation method is an algorithm to convert the LR(0) characteristic machine AA(G) (Aho and Ullman, 1977; Uackhouse, 1979) of a CFG G into an FSA for a superset of the language L(G) defined by G.
The characteristic machine for a CFG G is an FSA for the viable prefixes of G, which are just the possible stacks built by the standard shift-reduce recognizer for G when recognizing strings in L(G).
This is not the place to review the characteristic machine construction in detail.
However, to explain the approximation algorithm we will need to recall the main aspects of the construction.
The states of.hA(G) are sets of dotted rules A ---+ or.
fl where A ---+ aft is some rule of G..A4(G) is the determinization by the standard subset construction (Aho and Ullman, 1977) of the FSA defined as follows: • The initial state is the dotted rule S' ~ S.
where S is the start symbol of G and S' is a new auxiliary start symbol.
• The final state is S' ---, S..
1 Unification
grammars not in this class must first be weakened using techniques such as Shieber's restrictor (Shieber, 1985).
20 1 S'->.S ~,, S'->S.
S ->.
Ab 3 A->.Aa (4 S'>Ab" A ->.
A~2S -> A.
b A -> A.
a a A->Aa.
Figure 1: Characteristic Machine for G1 • The other states are all the possible dotted rules of G.
* There is a transition labeled X, where X is a terminal or nonterminal symbol, from dotted rule A ~ a.X/3 to A ~ aX ./3.
• There is an e-transition from A -+ a.
B/3 to B ~ "7, where B is a nonterminal symbol and B ~ "7 a rule in G.
.M(G) can be seen as a finite state control for a nondeterministic shift-reduce pushdown recognizer for G.
A state transition labeled by a terminal symbol z from state s to state s' licenses a shift move, pushing onto the stack of the recognizer the pair (s, Ix).
Arrival at a state containing a completed dotted rule A ~ a.
licenses a reduction move.
This pops from the stack as many pairs as the symbols in a, checking that the symbols in the pairs match the corresponding elements of a, and then takes the transition out of the last state popped s labeled by A, pushing (s, A) onto the stack.
The basic ingredient of our approximation algorithm is the flattening of a shift-reduce recognizer for a grammar G into an FSA by eliminating the stack and turning reduce moves into e-transitions.
However, as we will see below, flattening the characteristic machine recognizer directly will lead to poor approximations in many interesting cases.
Instead, the characteristic machine must be unfolded into a larger machine whose states carry information about the possible shift-reduce stacks at states of the characteristic machine.
The quality of the approximation is crucially influenced by how much stack information is encoded in the states of the unfolded machine: too little leads to coarse approximations, while too much leads to redundant automata needing very expensive optimization.
The algorithm is best understood with a simple example.
Consider the left-linear grammar G1 S---tAb A---+ Aale .M(G1) is shown on Figure 1.
Unfolding is not required for this simple example, so the approximating FSA is obtained from .A//(G1) by the flattening method outlined above.
The reducing states in .h4(G1), those containing completed dotted rules, are states 0, 3 and 4.
For instance, the reduction at state 4 would lead to a transition on nonterminal A, to state 2, from the state that Figure 2: Flattened FSA activated the rule being reduced.
Thus the corresponding e-transition goes from state 4 to state 2.
Adding all the transitions that arise in this way we obtain the FSA in Figure 2.
From this point on, the arcs labeled with nonterminals can be deleted.
Doing that and simplifying, we get finally the FSA in Figure 3.
which is the minimal FSA for the input left-linear grammar.
If flattening were applied to the LR(0) characteristic machine as in the example above, even simple grammars defining regular languages might be inexactly approximated by the algorithm.
The reason for this is that in general the reduction at a given reducing state in the characteristic machine transfers to different states depending on stack contents.
In other words, the reducing state might be reached by different routes which use the result of the reduction in different ways.
Consider for example the grammar G2 S --~ aXa \[ bXb X ---~ c which accepts the two strings aca and bcb.
Flattening .A4(G2) will produce an FSA that will also accept acb and bca, clearly an undesirable outcome.
The reason for this is that the e-transitions leaving the reducing state containing X ~ c.
do not distinguish between the different ways of reaching that state, which are encoded in the stack of the characteristic recognizer.
One way of solving the above problem is to unfold each state of the characteristic machine into a set of states corresponding to different stacks at that state, and flattening the unfolded acceptor rather than the original one.
However, the set of possible stacks at a state is in general infinite.
Therefore, it is necessary to do the unfolding not with respect to stacks, but with respect to a finite par® a b-© Figure 3: Minimal Acceptor 21 tition of the set of stacks possible at the state, induced by an appropriate equivalence relation.
The relation we use currently makes two stacks equivalent if they can be made identical by collapsing loops, that is, removing portions of stack pushed between two arrivals at the same state in the finite-state control of the shift-reduce recognizer.
The purpose of collapsing loops is to "forget" stack segments that may be arbitrarily repeated3 Clearly, each equivalence class is uniquely defined by the shortest stack in the class, and the classes can be constructed without having to consider all the (infinitely) many possible stacks.
Soundness of the Algorithm We will show here that the approximation method described informally in the previous section is sound, in the sense that the approximating FSA will always accept a superset of the language accepted by the input CFG.
In what follows, G is a fixed CFG with terminal vocabulary ~, nonterminal vocabulary N and start symbol S..M is the characteristic machine for G, with state set Q, start state so, final states F, and transition function 6 : S x (E U N) --~ S.
As usual, transition functions such as 6 are extended from input symbols to input strings by defining 6(s, e) = s and 6(s, aft) = 6(6(s, a), fl).
The shift-reduce recognizer T¢ associated to .M has the same states, start state and final states.
Its configurations are triples i s, 05 w) of a state, a stack and an input string.
The stack is a sequence of pairs (s, X) of a state and a terminal or nonterminal symbol.
The transitions of the shift-reduce recognizer are given as follows: Shift: i s, o', xw) ~(s', o'is, x), w) if 6(s, x) = s' Reduce: (s, ar, w) ~(s', o'is", A), w} if 6(s", A) = s' and either (1) A + • is a completed dotted rule in s, s" = s and r is empty, or (2) A X1...Xnis a completed dotted rule in s, r = (sx,Xi)--.(s.,X.} and s" = sl.
The initial configurations of ~ are is0, e, w) for some input string w, and the final configurations are (s, (s0,S},e) for some state s E F.
A derivation of a string w is a sequence of configurations co,..., cm such that co = (so, e, w}, cm = i s, (So, S}, e} for some final state p, and ci-1 ~ci for 1 < i < n.
Let s be a state.
The set Stacks(s) contains each sequence (so,Xo}...isk,Xk} such that si = 6(s~_i,X~_~), 1 < i < k and s = 6(sk,Xk).
In addition, Stacks(s0) contains the empty sequence e.
By construction, it is clear that if i s, o', w} is reachable from an initial configuration in 7~, then (r E Stacks(s).
A stack congruence on T¢ is a family of equivalence relations _---~ on Stacks(s) for each state s E S such that if o" =--, tr' and 6(s, X) = s' then ~(s, X) _---,, ais, X}.
A 2Since possible stacks can be easily shown to form a regular language, loop collapsing has a direct connection to the pumping lemma for regular languages.
stack congruence ~ partitions each Stacks(s) into equivalence classes \[a\], of the stacks in Stacks(s) equivalent to a under ~,.
Each stack congruence ~ on T~ induces a corresponding unfolded recognizer ~_.
The states of the unfolded recognizer are pairs of a state and stack equivalence class at that state.
The initial state is (so, \[e\]so), and the final states are all (s, \[a\]s) with s E F.
The transition function 5__of the unfolded recognizer is defined by M,), x) = (6(s, x), X)l,(,,x)) That this is well-defined follows immediately from the definition of stack congruence.
The definitions of dotted rules in states, configurations, shift and reduce transitions given above carry over immediately to unfolded recognizers.
Also, the characteristic recognizer can also be seen as an unfolded recognizer for the trivial coarsest congruence.
For any unfolded state p, let Pop(p) be the set of states reachable from p by a reduce transition.
More precisely, Pop(p) contains any state p' such that there is a completed dotted rule A --~ c~.
in p and a state p" such that 6___(p",c~) = p and 6-(p",A) = p'.
Then the flattening Uof 7~_is a nodeterministie FSA with the same state set, start state and final states as 7~and nondeterministi¢ transition function ¢defined as follows: • If 6~_(p,x) = p' for some x E E, then p' E ¢_=(p,x) • If p' E Pop(p) then p' E ¢= (p, e).
Let co,..., cm be a derivation of string w in TO, and put ci = (qi, o'i, wi}, and Pi = (qi, \[o'i\]p,}.
By construction, if ci-1 Fci is a shift move on x (wi-1 = zwi), then 6_--(pi-1, x) = Pi, and thus Pie ¢_=(Pi-1, x).
Alternatively, assume the transition is a reduce move associated to the completed dotted rule A ~ a..
We consider first the case a # e.
Put a = X1...X,.
By definition of reduce move, there is a sequence of states ra,..., rn and a stack a such that o'i-1 = a(rl,X1)...(rn,X,}, ai = o'irl,A ), 6irl,A) = qi, and 6(rj,Xj) = rj+l for 1 < j < n.
By definition of stack congruence, we will then have where n = and = {r,, X -i) for j > 1.
Furthermore, again by definition of stack congruence we have 6_=((rl, \[o'\]r,),A) = Pi.
Therefore, Pi E Pop(pi-1) and thus Pi E ¢_=(Pi-1, e).
A similar but simpler argument allows us to reach the same conclusion for the case a = e.
Finally, the definition of final state for TO__and .T~ makes Pm a final state.
We have thus shown how to construct from a derivation of a string in T¢ an accepting path for the same string in .T=-.
This proves that every string in L(G) is accepted by .T~, that is, that our construction is sound.
Finally, we should show that the stack collapsing equivalence informally described earlier is indeed a stack congruence.
A stack r is a loop if r = 22 Symbol Category Features s np vp args det n pron v sentence noun phrase verb phrase verb arguments determiner noun pronoun verb n (number), p (person) n, p, c (case) n, p, t (verb type) t n n n, p, c n, p, t Table 1: Categories of Example Grammar (sl,X1)...(sk,Xk) and 6(sk,Xk) = sx.
A stack a collapses to a stack a' if o" = pry, ~' = pv and r is a loop.
Two stacks are equivalent if they can be collapsed to the same stack.
Clearly, this equivalence relation is closed under suffixing, therefore it is a stack congruence.
A Complete Example The example grammar in the appendix shows an APSG for a small fragment of English, written in the notation accepted by the curent version of our grammar compiler.
The categories and features used in the grammar are described in Tables 1 and 2 (categories without features are omitted).
The example grammar accepts sentences such as I give a cake to Tom Tom sleeps I eat every nice cake but rejects ill-formed inputs such as I sleeps I eats a cake I give Tom eat The current grammar compiler factors out terminal productions to make the approximation algorithm independent of vocabulary size; transitions are labeled by automatically generated preterminal symbols instead of terminal symbols.
After this factoring, the full instantiation of the example grammar has 181 rules, its characteristic machine 222 states and 922 transitions, the Feature Values n (number) p (person) ¢ (case) t (verb type) s (singular), p (plural) 1 (first), 2 (second), 3 (third) s (subject), o (nonsubject) i (intransitive), t (transitive), d (ditransitive) Table 2: Features of Example Grammar unfolded and flattened FSA 3417 states and 4255 transitions, and the determinized and minimized final DFA 18 states and 67 transitions.
The compilation time is 123.19 seconds on a Sun SparcStation 1, with the determinizer and minimizer written in C and the rest of the compiler in Quintus Prolog.
Most of the time is spent in the unfolding and flattening phases (90.62 seconds unfolding and 17.33 flattening).
It is hoped that recoding these phases in C using carefully tuned data structures will speed them up by between one and two orders of magnitude.
Substantially larger grammars, with thousands of instantiated rules, have been developed for a speech-tospeech translation project.
Compilation times range from the very reasonable (around 10 minutes) to the very high (10 hours).
Very long compilations are caused by a combinatorial explosion in the unfolding of right recursions that will be discussed further in the next section.
Informal Analysis The present algorithm has not yet been analyzed sufficiently to determine the class of context-free grammars generating regular languages for which it is exact.
IIowever, it is exact for in a variety of interesting cases, including the examples of Church and Patil (Church and Patil, 1982), which illustrate how typical attachment ambiguities arise as structural ambiguities on regular string sets.
For example, the left-linear grammar S--tAb A--, Aa le and the right-linear grammar S---~aS \[ b both of which generate the regular set a'b, are mapped by the algorithm into the FSA in Figure 3.
The algorithm is also exact for some self-embedding grammars s of regular languages, such as S---+ aS \[ Sb l c defining the regular language a*cb*.
A more interesting example is the following simplified grammar for the structure of English noun phrases: NP ~ Det Nom I PN Det ~ Art \] NP's Nom ~ N I Nom PP I Adj Nom PP ~ P NP The symbols Art, N, PN and P correspond to the parts of speech article, noun, proper noun and preposition.
From this grammar, the algorithm derives the FSA in Figure 4.
3A grammar is self-embeddlng if and only if licenses the derivation X ~ c~X~ for nonempty a and/3.
A language is regular if and only if it can be described by some non-self-embedding grammar.
23 Figure 4: Acceptor for Noun Phrases As an example of inexact approximation, consider the the self-embedding CFG S --~ aSb \] e for the nonregular language anb n, n _> O.
This grammar is mapped by the algorithm into an FSA accepting e I a+b +.
The effect of the algorithm is thus to "forget" the pairing between a's and b's mediated by the stack in a pushdown acceptor for the CFG.
As noted earlier, right recursion is rather bad for the present unfolding scheme.
It is easy to see that the number of unfolded states for a grammar of the form S--~ X1S \] ...
I XnS I Y is exponential in n.
However, the problem can be circumvented by left factoring the grammar as follows: S~ZSiY z xl I...Ix.
This kind of situation often arises indirectly in the expansion of an APSG when some features in the right-hand side of a rule are unconstrained and thus lead to many different instantiated rules.
Related Work and Conclusions Our work can be seen as an algorithmic realization of suggestions of Church and Patil (Church, 1980; Church and Patil, 1982) on algebraic simplifications of CFGs of regular languages.
Other work on finite state approximations of phrase structure grammars has typically relied on arbitrary depth cutoffs in rule application.
While this is reasonable for psycholinguistic modeling of performance restrictions on center embedding (Pulman, 1986), it does not seem appropriate for speeech recognition where the approximating FSA is intended to work as a filter and not reject inputs acceptable by the given grammar.
For instance, depth cutoffs in the method described by Black (1989) lead to approximating FSAs whose language is neither a subset nor a superset of the language of the given phrase-structure grammar, In contrast, our method will produce an exact FSA for many interesting grammars generating regular languages, such as those arising from systematic attachment ambiguities (Church and Patil, 1982).
It important to note, however, that even when the result FSA accepts the same language, the original grammar is still necessary because interpretation algorithms are generally expressed in terms of phrase structures described by that grammar, not in terms of the states of the FSA.
The current algorithm can be combinatorially explosive in two places: the instantiation of unification grammar rules to derive an equivalent CFG, and the unfolding of the characteristic machine, in particular for rightrecursive rules.
The former problem can be alleviated by avoiding full instantiation of unification grammar rules with respect to "don't care" features, that is, features that are not constrained by the rule.
This can also help decrease the right-recursion unfolding explosion discussed earlier.
As for the cost of unfolding, preliminary experiments suggest that dividing the grammar into nonmutually-recursive components and applying the LR(0) construction and unfolding separately to those components could lead to much smaller unfolded automata.
Acknowledgments Thanks are due to Mark Liberman for suggesting that finite-state approximations might be worth investigating to David Roe and Pedro Moreno for using the grammar compiler prototype and patiently putting up with its bugs and inefficiencies.
References Alfred V.
Aho and Jeffrey D.
Ullman. 1977.
Principles of Compiler Design.
Addison-Wesley, Reading, Massachusetts.
Roland C.
Backhouse. 1979.
Syntax of Programming Languages--Theory and Practice.
Series in Computer Science.
Prentice-Hall, Englewood Cliffs, New Jersey.
Alan W.
Black. 1989.
Finite state machines from feature grammars.
In Masaru Tomita, editor, International Workshop on Parsing Technologies, pages 277285, Pittsburgh, Pennsylvania.
Carnegie Mellon University.
Kenneth W.
Church and Ramesh Patil.
1982. Coping with syntactic ambiguity or how to put the block in the box on the table.
Computational Linguistics, 8(34):139-149.
Kenneth W.
Church. 1980.
On memory limitations in natural language processing.
Master's thesis, M.I.T.
Published as Report MIT/LCS/TR-245.
Steven G.
Pulman. 1986.
Grammars, parsers, and memory limitations.
Language and Cognitive Processes, 1(3):197-225.
Stuart M.
Shieber. 1985.
Using restriction to extend parsing algorithms for complex-feature-based formalisms.
In 23rd Annual Meeting of the Association 24 for Computational Linguistics, pages 145-152, Morristown, New Jersey.
Association for Computational Linguistics.
Appendix Nonterminal symbols (syntactic categories) may have features that specify variants of the category (eg.
singular or plural noun phrases, intransitive or transitive verbs).
A category cat with feature constraints is written CarE\[el,,..., Crn\] . Feature constraints for feature f have the form f=v for a single value v or f = (,1,..., v,) for several alternative values.
The symbol ")" appearing as the value of a feature in the right-hand side of a rule indicates that that feature must have the same value as the feature of the same name of the category in the left-hand side of the rule.
This can be used to enforce feature agreement, for instance, number agreement between subject and verb.
It is convenient to declare the features and possible values of categories with category declarations appearing before the grammar rules.
Category declarations have the form cat cat#\[ fl (Vll,...,Vlkl),..., /m=(Vml .....,Vmk ) 1.
giving all the possible values of all the features for the category.
The declaration start cat.
declares cat as the start symbol of the grammar.
In the grammar rules, the symbol "'" prefixes terminal symbols, commas are used for sequencing and "l" for alternation.
start s.
cat s#\[n=(s,p),p=(l,2,3)\].
cat np#\[n=(s,p),p=(1,2,3),c=(s,o)\] . cat vp#\[n=(s,p),p=(1,2,3),type=(i,t,d)\].
cat args# \[type=(i,t,d)\] . cat det#\[n=(s,p)\].
cat n#\[n=(s,p)\].
cat pron#\[n=(s,p),p=(1,2,3),c=(s,o)\].
cat v#\[n=(s,p),p=(1,2,3),type=(i,t,d)\].
s => np#\[n=!,p=!,c=s\], vp#\[n=!,p=!\].
np#\[p=3\] => det#\[n=!\], adjs, n#\[n=!\].
np#\[n=s,p=3\] => pn.
np => pron#\[n=!, p=!, c=!\].
pron#\[n=s,p=l,c=s\] => 'i.
pron#\[p=2\] => 'you.
pron#\[n=s,p=3,c=s\] => 'he I 'she.
pron#\[n=s,p=3\] => 'it.
pron#\[n=p,p=l,c=s\] => ',e.
pron#\[n=p,p=3,c=s\] => 'they.
pron#\[n=s,p=l,c=o\] => 'me.
pron#\[n=s,p=3,c=o\] => 'him I 'her.
pron#\[n=p,p=l,c=o\] => 'us.
pron#\[n=p,p=3,c=o\] => 'them.
vp => v#\[n=!,p=!,type=!\], args#\[type=!\].
adjs => \[1.
adjs => adj, adjs.
args#\[type=i\] => \[\].
args#\[type=t\] => np#\[c=o\].
args#\[type=d\] => np#\[c=o\], 'to, np#\[c=o\].
pn => 'tom \] 'dick I 'harry.
det => 'somel 'the.
det#\[n=s\] => 'every I 'a.
det#\[n=p\] => 'all I 'most.
n#\[n=s\] => 'child \[ 'cake.
n#\[n=p\] => 'children \[ 'cakes.
adj => 'nice \[ 'sweet.
v#\[n=s,p=3,type=i\] => 'sleeps.
v#\[n=p,type=i\] => 'sleep.
v#\[n=s,p=(l,2),type=i\] => 'sleep.
v#\[n=s,p=3,type=t\] => 'eats.
v#\[n=p,type=t\] => 'eat.
v#\[n=s,p=(1,2),type=t\] => 'eat.
v#\[n=s,p=3,type=d\] => 'gives.
v#\[n=p,type=d\] => 'give.
v#\[n=e,p=(1,2),type=d\] => 'give. 25

