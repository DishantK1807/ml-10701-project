<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>V Alabau</author>
<author>C D Mart´ınez</author>
</authors>
<title>Bilingual speech corpus in two phonetically similar languages</title>
<date>2006</date>
<booktitle>In Proc. of LREC’06</booktitle>
<pages>1624--1627</pages>
<marker>Alabau, Mart´ınez, 2006</marker>
<rawString>Alabau, V. and C.D. Mart´ınez, 2006. Bilingual speech corpus in two phonetically similar languages. In Proc. of LREC’06, pp. 1624–1627.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Casacuberta</author>
<author>H Ney</author>
<author>F J Och</author>
<author>E Vidal</author>
<author>J M Vilar</author>
<author>S Barrachina</author>
<author>I Garcia-Varea</author>
<author>C Martinez D Llorens</author>
<author>S Molau</author>
<author>F Nevado</author>
<author>M Pastor</author>
<author>D Pico</author>
<author>A Sanchis</author>
</authors>
<title>Some approaches to statistical and finitestate speech-to-speech translation. Computer Speech and Language</title>
<date>2004</date>
<pages>18--25</pages>
<contexts>
<context> in Valencian to adapt acoustic models that were trained from a large Spanish corpus. We used the Senglar corpus as the initial training corpus. This corpus has been successfully used in other tasks (Casacuberta et al., 2004). The recording conditions for the Senglar corpus were different from those of our corpus. For this reason, we obtained adapted acoustic models from the Senglar acoustic models for both Spanish and V</context>
<context> used adapted acoustic models for Spanish and Valencian. These acoustic models were adapted with the adaptation data in Spanish and Valencian from the acoustic models trained with the Senglar corpus (Casacuberta et al., 2004). The adaptation data was small, so we adapted the acosutic models with only one regression class and, therefore, we only computed one transformation matrix. We implemented the three variants of MLLR</context>
</contexts>
<marker>Casacuberta, Ney, Och, Vidal, Vilar, Barrachina, Garcia-Varea, Llorens, Molau, Nevado, Pastor, Pico, Sanchis, 2004</marker>
<rawString>Casacuberta, F., H. Ney, F. J. Och, E. Vidal, J. M. Vilar, S. Barrachina, I. Garcia-Varea, C. Martinez D. Llorens, S. Molau, F. Nevado, M. Pastor, D. Pico, and A. Sanchis., 2004. Some approaches to statistical and finitestate speech-to-speech translation. Computer Speech and Language, 18:25–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Leggetter</author>
<author>P C Woodland</author>
</authors>
<title>Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models. Computer Speech and Language</title>
<date>1995</date>
<pages>9--171</pages>
<contexts>
<context>ly trained with a large acoustic corpus. The resulting adapted models are commonly better than those trained with a small corpus of the real language. The Maximum Likelihood Linear Regression (MLLR) (Leggetter and Woodland, 1995) technique has been commonly used in speaker adaptation. However, in a few works it has also been applied to language adaptation (Zhao and O’Shaughnessy, 2007). We used MLLR in language adaptation to</context>
<context>arameters to maximize the likelihood of the adaptation data. The means are updated using a transformation matrix, which is estimated from the adaptation data. We applied the formulation presented in (Leggetter and Woodland, 1995) to language adaptation: we take some adaptation data from a language to adapt the acoustic models using MLLR in the same way as in speaker adaptation. The theory is based on the concept of regressio</context>
<context>et of regression classes can be manually or automatically defined over the gaussians of the HMM. There is no method to analytically determine the optimal number and composition of regression classes (Leggetter and Woodland, 1995). To perform the adaptation of the means, we computed a transformation matrix ~W for each regression class. This matrix is applied to the extended mean vector of all the mixtures pertaining to the re</context>
<context>ithout the same covariances of the distributions (with a full or a diagonal matrix) or with the same covariances of the distributions. Details on the estimation of these variants can be consulted in (Leggetter and Woodland, 1995). The following formulation assumes only one adaptation sample, but it can be easily extended for n adaptation samples. 2.1. Full matrix Given a state q in a HMM, for the ith gaussian of the output d</context>
</contexts>
<marker>Leggetter, Woodland, 1995</marker>
<rawString>Leggetter, C.J. and P.C. Woodland, 1995. Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models. Computer Speech and Language, 9:171–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moreno</author>
<author>A Febrer</author>
<author>L M´arquez</author>
</authors>
<title>Generation of language resources for the development of speech tecnologies in Catalan</title>
<date>2006</date>
<booktitle>In Proc. of LREC’06</booktitle>
<pages>1632--1635</pages>
<marker>Moreno, Febrer, M´arquez, 2006</marker>
<rawString>Moreno, A., A. Febrer, and L. M´arquez, 2006. Generation of language resources for the development of speech tecnologies in Catalan. In Proc. of LREC’06, pp. 1632– 1635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Schultz</author>
<author>A Waibel</author>
</authors>
<title>Language-independent and language-adaptive acoustic modeling for speech recognition</title>
<date>2001</date>
<journal>Speech Communication</journal>
<volume>35</volume>
<pages>1--2</pages>
<contexts>
<context>to build a good ASR and to obtain reliable acoustic models. One way to obtain better acoustic models would be to use adaptation techniques to adapt acoustic models of a phonetically similar language (Schultz and Waibel, 2001). These models are usually trained with a large acoustic corpus. The resulting adapted models are commonly better than those trained with a small corpus of the real language. The Maximum Likelihood L</context>
<context>s been used before to obtain acoustic models for multilingual speech recognition. The quantity of signal that we used to adapt the models was similar to the quantity of signal used in previous works (Schultz and Waibel, 2001) and (Zhao and O’Shaughnessy, 2007). 5. Experiments and Results To analyze the results, we used the Word Error Rate (WER) as the evaluation measure. This measure computes the edit distance between a </context>
</contexts>
<marker>Schultz, Waibel, 2001</marker>
<rawString>Schultz, T. and A. Waibel, 2001. Language-independent and language-adaptive acoustic modeling for speech recognition. Speech Communication, 35(Issues 1-2):31– 51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Uebler</author>
</authors>
<title>Multilingual speech recognition in seven languages. Speech Communication</title>
<date>2001</date>
<pages>35--53</pages>
<contexts>
<context>language adaptation in order to choose the best alternative for our system. 1. Introduction Multilingual Automatic Speech Recognition (ASR) systems are of great interest in multilingual environments (Uebler, 2001). In a multilingual environment, where each potential user has a different native language, a Multilingual Automatic Speech Recognition System must deal with different languages and with the inapprop</context>
</contexts>
<marker>Uebler, 2001</marker>
<rawString>Uebler, U., 2001. Multilingual speech recognition in seven languages. Speech Communication, 35:53–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordi Vilajoana</author>
<author>Dami`a Pons</author>
</authors>
<title>Catalan, language of Europe. Generalitat de Catalunya</title>
<date>2001</date>
<contexts>
<context>dard Catalan. This is due to its dialectal variance and the great influence that Spanish has had on it. This influence has been much greater on Valencian than on other Catalan dialects. According to (Vilajoana and Pons, 2001), the total number of people who speak Catalan is 7,200,000, and the number of people who understand it is over 9,800,000. The Valencian dialect is spoken by 27% of all Catalan speakers. The great in</context>
</contexts>
<marker>Vilajoana, Pons, 2001</marker>
<rawString>Vilajoana, Jordi and Dami`a Pons, 2001. Catalan, language of Europe. Generalitat de Catalunya.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P C Woodland</author>
</authors>
<title>Speaker Adaptation for Continuous Density HMMs: A Review. ITRW on Adaptation Methods for Speech Recognitionn</title>
<date>2001</date>
<pages>11--19</pages>
<contexts>
<context>se more iterations provided worse results. In general, when the initial models provide good Gaussian frame alignments, only a single iterarion of EM is required to estimate the transformation matrix (Woodland, 2001). To obtain baseline results, we performed experiments for Spanish and Valencian with the Senglar acoustic models without adaptation. The Senglar Valencian models were built by cloning the most simil</context>
</contexts>
<marker>Woodland, 2001</marker>
<rawString>Woodland, P.C., 2001. Speaker Adaptation for Continuous Density HMMs: A Review. ITRW on Adaptation Methods for Speech Recognitionn, 11–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Luj´an</author>
<author>C D Mart´ınez</author>
<author>V Alabau</author>
</authors>
<title>A study on bilingual speech recognition involving a minority language</title>
<date>2007</date>
<booktitle>33rd Language &amp; Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics</booktitle>
<pages>138--42</pages>
<marker>Luj´an, Mart´ınez, Alabau, 2007</marker>
<rawString>Luj´an, M. and C. D. Mart´ınez and V. Alabau, 2007. A study on bilingual speech recognition involving a minority language. 33rd Language &amp; Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics, 138–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Young</author>
<author>G Evermann</author>
<author>T Hain</author>
<author>D Kershaw</author>
<author>G Moore</author>
<author>J Odell</author>
<author>D Ollason</author>
<author>D Povey</author>
<author>V Valtchev</author>
<author>P Woodland</author>
</authors>
<date>2004</date>
<booktitle>The HTK Book. CUED, UK, v3.2</booktitle>
<pages>edition.</pages>
<contexts>
<context>onophones) in order to make a comparison of the features of an acoustic sequence with the acoustic models. The acoustic models were hidden Markov models (HMM) that were trained using the HTK toolkit (Young et al., 2004). The HMMs followed a three-state, left-to-right topology without skips. We tested models with 32 Gaussians per state. Each gaussian modeled a 33-component feature vector (10 cepstrals coefficients p</context>
<context>dface) Table 2 shows the best results of experiments. In the same conditions, another standard MLLR tool (HTK) provided similar results for the full matrix case (6.0% in Spanish, 11.0% in Valencian) (Young et al., 2004). As the results show, in this case, it is best to use Mean Square because the WER improved 5 points in Spanish (from 11.0% to 5.6%) and 6 points in Valencian (from 16.5% to 10.4%). Mean Square obtai</context>
</contexts>
<marker>Young, Evermann, Hain, Kershaw, Moore, Odell, Ollason, Povey, Valtchev, Woodland, 2004</marker>
<rawString>Young, S., G. Evermann, T. Hain, D. Kershaw, G. Moore, J. Odell, D. Ollason, D. Povey, V. Valtchev, and P. Woodland, July, 2004. The HTK Book. CUED, UK, v3.2 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xufang Zhao</author>
<author>Douglas O’Shaughnessy</author>
</authors>
<title>An Evaluation of Cross-Language Adaptation and Native Speech Training for Rapid HMM Construction Based on Very Limited Training Data. Interspeech</title>
<date>2007</date>
<marker>Zhao, O’Shaughnessy, 2007</marker>
<rawString>Xufang Zhao and Douglas O’Shaughnessy, August 27-31, 2007 An Evaluation of Cross-Language Adaptation and Native Speech Training for Rapid HMM Construction Based on Very Limited Training Data. Interspeech 2007.</rawString>
</citation>
</citationList>
</algorithm>

