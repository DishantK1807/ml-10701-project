(Soon et al., 2001) advocate the link-first decision, which links a mention to its closest candidate referent, while (Ng and Cardie, 2002) consider instead the link-best decision, which links a mention to its most confident 1This definition was introduced in (NIST, 2003).
J01-4004 P02-1014
The main difference between this approach and ours is that (Ng, 2005)’s approach takes coreference resolution one step further, by comparing the results of multiple systems, while our system is a single resolver; furthermore, he emphasizes the global optimization of ranking clusters obtained locally, whereas our focus is on globally optimizing the clusterization method inside the resolver.
P05-1020
The ECM-F scorer overcomes two shortcomings of the MUC scorer: not considering single mentions and treating every error as equally important (Baldwin et al., 1998), which makes the ECM-F a more adequate measure of coreference.
M98-1022
Baseline system has the (Luo et al., 2004) features.
P04-1018
We have experimentally discovered that the use of word sense disambiguation improves the performance tremendously (a boost in score of 10%), therefore all the features use the word senses from a previously-applied word sense disambiguation program, taken from (Mihalcea and Csomai, 2005).
P05-3014
The experiments conducted on MUC and ACE data indicate state-of-the-art results when compared with the methods reported in (Ng and Cardie, 2002) and (Luo et al., 2004).
P02-1014 P04-1018 P04-1020
Moreover, the fact that the two implementations are comparable is not inconceivable once we consider that (Luo et al., 2004) never compared their system to another coreference resolver and reported their competitive results on true mentions only.
P04-1018
Work It is of interest to discuss why our implementation of the Belltree system (Luo et al., 2004) is comparable in performance to Link-Best (Ng and Cardie, 2002).
P02-1014 P04-1018
In contrast, globally optimized clustering decisions were reported in (Luo et al., 2004) and (DaumeIII and Marcu, 2005a), where all clustering possibilities are considered by searching on a Bell tree representation or by using the Learning as Search Optimization (LaSO) framework (DaumeIII and Marcu, 2005b) respectively, but the first search is partial and driven by heuristics and the second one only looks back in text.
P04-1018
We generated a negative example for each pair (S = {ei},T = {ej}) with i negationslash= j – each entity must be separated from any other en5As introduced by (Luo et al., 2004).
P04-1018
(Ng, 2005) treats coreference resolution as a problem of ranking candidate partitions generated by a set of coreference systems.
P05-1020
Most of the previous coreference resolution methods have similar classification phases, implemented either as decision trees (Soon et al., 2001) or as maximum entropy classifiers (Luo et al., 2004).
J01-4004 P04-1018
We created the training examples in the same way as (Luo et al., 2004), by pairing all mentions of the same type, obtaining their feature vectors and taking the outcome (coreferent/noncoreferent) from the key files.
P04-1018
(Luo et al., 2004)’s features were not reproduced here from lack of space; please refer to the relevant paper for details.
P04-1018
This only shows that none of them is particularly poor, but it is not a relevant way of comparing methods– the MUC metric has been found too indulgent by researchers ((Luo et al., 2004), (Baldwin et al., 1998)).
M98-1022 P04-1018
BESTCUT replaces the bottom-up search in a tree representation (as it was performed in (Luo et al., 2004)) with the top-down problem of obtaining the best partitioning of a graph.
P04-1018
Third, as opposed to (Luo et al., 2004), who represented all numerical features quantized, we translated each numerical feature into a set of binary features that express whether the value is in certain intervals.
P04-1018
The MUC scorer counts the common links between the 281 MUC score Clusterization algorithm Mentions ECM-F% MUC P% MUC R% MUC F% BESTCUT key 82.7 91.1 88.2 89.63 detected 73.0 88.3 75.1 81.17 undetected 41.2 52.0 82.4 63.76 Belltree (Luo et al., 2004) key 77.9 88.5 89.3 88.90 detected 70.8 86.0 76.6 81.03 undetected 52.6 40.3 87.1 55.10 Link-Best (Ng and Cardie, 2002) key 77.9 88.0 90.0 88.99 detected 70.7 85.1 77.3 81.01 undetected 51.6 39.6 88.5 54.72 Table 4: Comparison of results between three clusterization algorithms on ACE Phase 2.
P02-1014 P04-1018
Results The clusterization algorithms that we implemented to evaluate in comparison with our method are (Luo et al., 2004)’s Belltree and Link-Best (best-first clusterization) from (Ng and Cardie, 2002).
P02-1014 P04-1018
(Luo et al., 2004) do the clusterization through a beam-search in the Bell tree using either a mention-pair or an entity-mention model, the first one performing better in their experiments.
P04-1018
Representation We duplicated the statistical model used by (Luo et al., 2004), with three differences.
P04-1018
Based on the data seen, a maximum entropy model (Berger et al., 1996) offers an expression (1) for the probability that there exists coreference C between a mention mi and a mention mj.
J96-1002
Since we aimed to measure the performance of coreference, the metrics used for evaluation are the ECMF (Luo et al., 2004) and the MUC P, R and F scores (Vilain et al., 1995).
M95-1005 P04-1018
