Experiments of Ng and Low (2004) on CTB 3.0
W04-3236
N best candidates, following Huang (2008, Sec.
P08-1067
We adopt the perceptron algorithm (Collins, 2002)
W02-1001
rameters” of Collins (2002) to alleviate overfitting
W02-1001
Huang, Liang. 2008. Forest reranking: Discrimina-
P08-1067
(Ng and Low, 2004). In the rest of the paper, we
W04-3236
for parsing (Huang, 2008), this strategy
P08-1067
erated from the templates of Ng and Low (2004).
W04-3236
tion of Huang (2008).
P08-1067
Ng, Hwee Tou and Jin Kiat Low. 2004. Chinese part-
W04-3236
tron training algorithm (Collins, 2002), etc. Com-
W02-1001
tices (Collins, 2000; Huang, 2008). Given the can-
P08-1067
pecially, Huang (2008) reranked the packed for-
P08-1067
Jiang et al. (2008), we use s indicating a single-
P08-1102
Ratnaparkhi and Adwait. 1996. A maximum entropy
W96-0213
introduced by Jiang et al. (2008). They are gener-
P08-1102
of Ng and Low (2004) shown that, compared with
W04-3236
cordingly. As described in Ng and Low (2004) and
W04-3236
Following Jiang et al. (2008), we describe segmen-
P08-1102
Collins, Michael. 2002. Discriminative training meth-
W02-1001
immediately from Ng and Low (2004). they
W04-3236
According to Collins (2002), the function
W02-1001
Collins (2002)’s perceptron training algorithm
W02-1001
Algorithm 1 Oracle Diameter,cjkA8A4la Huang (2008,
P08-1067
(Ratnaparkhi and Adwait, 1996), Conditional Ran-
W96-0213
est reranking of Huang (2008), it performs rerank-
P08-1067
translation (Collins, 2000; Huang, 2008), etc. Es-
P08-1067
