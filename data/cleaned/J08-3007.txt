Computational Approaches to Morphology and Syntax
Brian Roark and Richard Sproat
(OregonHealthandScienceUniversityandUniversityofIllinoisat
Urbana–Champaign)
Oxford:OxfordUniversityPress(Oxfordsurveysinsyntaxandmorphology,editedby
RobertD.VanValinJr,volume4),2007,xx+316pp;hardbound,ISBN978-0-19-927477-2,
$110.00,£60.00;paperbound,ISBN978-0-19-927478-9,$45.00,£24.99
Reviewedby
NoahA.Smith
CarnegieMellonUniversity
BrianRoarkandRichardSproathavewrittenacompactandveryreadablebooksurvey-
ingcomputationalmorphologyandcomputationalsyntax.Thistextisnotintroductory;
instead, it will help bring computational linguists who do not work on morphology
or syntax up to date on these areas’ latest developments. Certain chapters (in particu-
lar, Chapters 2 and 8) provide especially good starting points for advanced graduate
coursesorseminars.ThetextisdividedintoanIntroductionandPreliminarieschapter,
fourchaptersoncomputationalapproachestomorphology,andfourchaptersoncom-
putational approaches to syntax. The morphology chapters focus primarily on formal
and theoretical issues, and are likely to be of interest to morphologists, computational
andnot.Thesyntaxchaptersaredrivenmorebyengineeringgoals,withmorealgorithm
details. Because a good understanding of probabilistic modeling is assumed, these
chapters will also be useful for machine learning researchers interested in language
processing.
Despite the authors’ former afﬁliations, this book is not an AT&T analogue of
Beesley and Karttunen’s (2003) pedagogically motivated text on the Xerox ﬁnite-state
tools.ThistextisnotabouttheAT&TFSMlibrariesorthealgorithmsunderlyingthem
(cf.RocheandSchabes1997).
1. Chapter 1: Introduction and Preliminaries
Theﬁrstchapterisatake-no-prisonersintroductiontoﬁnite-stateautomataandtrans-
ducers and their semiring-weighted generalizations. Algorithms (e.g., for FST compo-
sition) are discussed but not presented in detail. Epsilon removal, minimization, and
determinizationarementionedbutnotdeﬁned.Thismaterialisprobablytoocursoryto
serve as a lone introduction for those wishing to fully understand weighted FSTs, but
thatlackofunderstandingwillnotbeanimpedimentintheensuingchaptersbecause
weights do not re-surface until chapter 6, in the context of n-gram models, and even
thenthealgebraicviewgivenhereisnotmentioned.
Thechapterconcludeswithadefenseoftheplaceofﬁnite-statemodelsinlinguis-
tics,followedbyaclearexplanationofthetrade-offsincomputationallinguistics(e.g.,
betweencomputationalcost,expressivepower,andannotationcost).
2. Part I (Chapters 2–5): Computational Approaches to Morphology
Thesechaptersareprimarilyanargumentfortheeffectivenessofﬁnite-statetransducers
inmodelingnaturallanguagemorphology.
ComputationalLinguistics Volume34,Number3
Chapter2providesalaundrylistofmorphologicalphenomena,arguingthatﬁnite-
state composition captures each of them, even in cases where there is a more obvious
solution (e.g., ﬁnite-state concatenation for concatenative phenomena). Examples of
many kinds of phenomena are given from diverse languages: prosodic restrictions in
Yowulmne, phonological effects of German afﬁxes, and subsegmental morphology in
Welsh, to name a few. Importantly, the compile-reduce and merge operations are argued
tobesyntacticsugarforeffectsachievablebyﬁnite-statecomposition,sothatevenroot-
and-patternArabicmorphologyisexplainedinthesamealgebraicframework.
Reduplication effects, of course, challenge ﬁnite-state explanations, and so receive
theirown section. Extended (non-regular) computational models are presented along-
sidedatafromGothic,Dakota,andSye.Theauthorsspeculatethat,incontrastwiththe
commonly accepted Correspondence theory, Morphological Doubling theory (Inkelas
and Zoll 1999), if correct, would imply that a non-regular “copying” process is not at
workinreduplication.Itisatthispointthatthereadermayexperiencesomediscomfort;
should the reduplication problem be addressed in syntax rather than morphology?
Whereexactlydoestheboundarylie?Readershopingforareassessmentofthisbound-
ary,orevenanewbridgeoverit,willnotﬁndithere.
Chapter3beginswithStump’s(2001)two-dimensionaltaxonomyofmorphological
theories,whichappearsratherdivorcedfromtherichworkonﬁnite-statecomputational
morphologyinChapter2.Thesubtletiesamongthefourtypesoftheories(lexicalvs.in-
ferential and incremental vs. realizational, a more nuanced breakdown of the debate
over “item-and-arrangement” vs. “item-and-process”) may be difﬁcult to understand
for the reader not trained in morphological theory, but resolution comes quickly. We
are presented with a series of examples showing “proof-of-concept” fragmentary im-
plementations (in AT&T’s lextools) of phenomena in Sanskrit, Swahili, and Breton to
arguethatlexical-incrementalistandinferential-realizationaltheoriesarecomputation-
allyequivalent;bothcanbeimplementedusingFSTsandcanleadtothesamemodels.
Chapter 4 gives an algebraic analysis of Koskenniemi’s (1983) “KIMMO” two-
levelmorphologicalanalysissystem.Koskenniemi’shand-codedmorphologyrulesare
argued to be a historical accident; if only computers had been more powerful in the
1980s, compilation of those rules into FSTs might have been automated, and in fact
Kaplan and Kay had already developed the algorithms.
1
In the spirit of the previous
chapter, Sproat andRoark also notethatmorphological accounts thatuse one,two,or
more“cascaded”levelsareallcomputationallyequivalentrationalrelationsunderthe
ﬁnite-stateapproach,andthatOptimalityTheorycan(undercertainassumptionsabout
constraints)beimplementedwithﬁnite-stateoperationsaswell(Ellison1994).
Chapter5,“Machinelearningofmorphology,”focusesonunsupervisedmorphol-
ogy induction methods. There is about a page of discussion on statistical language
modeling approaches for disambiguation in agglutinative languages; no mention is
made of the more recent use of discriminative machine learning in morphological
disambiguation (Kudo, Yamamoto, and Matsumoto 2004; Habash and Rambow 2005;
Smith,Smith,andTromble2005).ThechapterfocusesontheapproachesofGoldsmith
(2001), Schone and Jurafsky (2001), and Yarowsky and Wicentowski (2001). Although
eachapproachisinterestingonitsown,littleeffortismadetounifyworkinthisarea,
and none to bring the reader back full circle to ﬁnite-state models or the problem of
inducingfromdataregulargrammars(StolckeandOmohundro1993)ortheirweights
1 TheauthorsrightlypointoutthatKoskenniemideservesmuchcreditforbuildinganimplementation
thataimedtohavebroadcoverage,notmerelyaproof-of-concept.
454
BookReviews
(Eisner 2002). Another missed opportunity here is the recent introduction of Bayesian
learningforwordsegmentation(Goldwater,Grifﬁths,andJohnson2006).
Part I, in summary, aims to reduce many accounts of morphological phenomena
to ﬁnite-state transducer composition, drawing on a wealth of illustrative examples.
Twenty-two languages are listed in the language index at the end of the book, and,
tellingly,allofthemarediscussedexclusivelyinPartI.Thesechaptersaregooddiplo-
macy toward theoretical linguistics, showing how computational arguments can have
theoreticalimplications.
3. Part II (Chapters 6–9): Computational Approaches to Syntax
In Part II, Roark and Sproat turn to models of syntax in computational linguistics.
Because most research in this area has been on English, English parsing is what they
present.
Chapter 6 covers ﬁnite-state approaches to syntax, including n-gram models and
smoothing, class-based language models, hidden Markov models (though without a
formal deﬁnition), part-of-speech tagging, log-linear models, and shallow parsing/
chunking. The Forward, Viterbi, Viterbi n-best, Forward–Backward algorithms, and
“Forward–Backward Decoding” (also known as posterior or minimum Bayes risk de-
coding) are covered with examples. This chapter is not as leisurely as the treatments
ofHMMsbyManningandSch¨utze(1999)orCharniak(1993),anditomitsbasicback-
ground on probabilistic modeling. For example, why must we ensure that an n-gram
model’s total probability sums exactly to one? The answer relies on an understanding
of perplexity and its use in evaluation, now in decline (cf. “stupid backoff” in Brants
etal.2007).ThechapterdoesnotreconnectwiththealgebraicviewpresentedinChap-
ter1;forexample,theconnectionbetweenHMMsandWFSAsisneverexpressed.
Chapter 7 introduces context-free grammars and their parsers, broken down into
“deterministic”and“nondeterministic”approaches.
2
ProbabilisticCFGsandtreebanks
are introduced informally alongside the latter, which may confuse some readers. Am-
biguity is only presented as a natural phenomenon, not a problem of crude, over-
generatinggrammars.TheprobabilisticCKYandEarleyalgorithmsarepresented.The
Inside–Outside algorithm is presented in the context of Goodman’s (1996) maximum
expectedrecallparsing(anotherinstanceofminimumBayesrisk).Asinthecaseofthe
dynamicprogrammingalgorithmsforHMMsinChapter6,theexpositionisprobably
toobrisktobeanintroductiontothetopic.
Chapter8containsathoughtfuldiscussionofmanybestpracticesinstatisticalpars-
ing:treebank“decoration”techniquessuchasparentannotationandlexicalization,and
theprobabilitymodelsunderlyingtheparsersofCollins(1997)andCharniak(1997).De-
pendencyparsing,unsupervisedgrammarinduction,andﬁnite-stateapproximationsto
PCFGsareallottedshortsections.
Chapter 9 covers context-sensitive models of syntax. Uniﬁcation-based parsing
is presented at a high level, without formal details of uniﬁcation or the differences
between theories such as LFG and HPSG. The “lexicalized” models (TAG and CCG)
are treated more thoroughly; pseudocode for a TAG dynamic programming parser is
provided. There is brief treatment of Data-Oriented Parsing, reranking (a section that
2 Theseterms,thoughinwideuse,aremisnomers.Alloftheseparsersaredeterministic,sincenone
involverandomnessornondeterministicbehaviorresultingfrommultipleprocessors.Here
“(non)deterministic”referstothegrammar,nottheparser.
455
ComputationalLinguistics Volume34,Number3
wouldhavebeenofmorepracticaluseinChapter8),andtransductiongrammars(i.e.,
grammarsovermorethanonestring,mostfrequentlyusedinmachinetranslation).
TheabundanceofdynamicprogrammingalgorithmsinPartIIleadstothequestion
of whether such algorithms can be more easily taught (and uniﬁed) using recursive
equations (Manning and Sch¨utze 1999), or a more declarative framework (Shieber,
Schabes,andPereira1995).Readerswhopreferproceduralpseudocodewillﬁndithere,
thoughthebookdoesnotaddressimplementationtricksforstoringandindexingparse
charts,oragenda-orderingmethodstomakeparsingefﬁcient.
Thesechaptersareneitheragentleintroductiontoprobabilisticmodelingofsyntax
forlinguistsnorahandbookforthelanguageengineerwhowantstobuildanefﬁcient,
competitive parser. (There is also no advice on the relative merits of today’s parsers
available for download.) The audience that will ﬁnd Part II most valuable will be
researchers who understand the principles of probabilistic modeling but want a more
up-to-dateviewofstatisticalparsingthanofferedbyManningandSch¨utze(1999),with
more coverage of advanced topics than Jurafsky and Martin (2008). This group might
include structured machine-learning researchers interested in the nuances of natural
languageparsingandcomputationallinguistswhodonotworkonsyntaxbutwantto
keepupwiththearea.
4. Conclusion
The two major parts of this book stand as clear, up-to-date, and concisely written
summariesofparticularsub-ﬁeldsincomputationallinguistics:ﬁnite-statemorphology
and English syntactic processing. The book does a ﬁne job of elucidating the trade-
offs that make computational linguistics a tightrope act, and therefore serves as good
diplomacyforresearchersinrelatedﬁelds.At112and146pages,respectively,eitherof
thepartsisreadableonahalf-dayplaneortraintrip.
Today,thestrongestbridgebetweenmorphologyandsyntaxistheChomskyhier-
archy, which is mentioned frequently in this book (but never depicted). The contrast
between Parts I and II implies blueprints for more bridges: data resources to support
more powerful learning algorithms for morphology (as we have seen in syntax), a
stronger inﬂuence of non-English data on computational syntactic modeling (as we
have seen in morphology), and practical ways to accomplish the amalgamation of
morphologyandsyntax.ThisreviewerbelievesComputationalApproachestoMorphology
and Syntax will re-introduce the two sub-communities to each other and help each to
leveragethesuccessesoftheother.
References
Beesley,KennethR.andLauriKarttunen.
2003.FiniteStateMorphology.CSLI
Publications,Stanford,CA.
Brants,Thorsten,AshokC.Popat,PengXu,
FranzJ.Och,andJeffreyDean.2007.Large
languagemodelsinmachinetranslation.
InProceedingsoftheJointConferenceon
EmpiricalMethodsinNaturalLanguage
ProcessingandComputationalNatural
LanguageLearning,pages858–867,Prague.
Charniak,Eugene.1993.StatisticalLanguage
Learning.MITPress,Cambridge,MA.
Charniak,Eugene.1997.Statisticalparsing
withacontext-freegrammarandword
statistics.InProceedingsofthe14thNational
ConferenceonArtiﬁcialIntelligence,
pages598–603,Providence,RI.
Collins,Michael.1997.Threegenerative,
lexicalisedmodelsforstatisticalparsing.In
Proceedingsofthe35thAnnualMeetingofthe
AssociationforComputationalLinguistics,
pages16–23,Madrid.
Eisner,Jason.2002.Parameterestimationfor
probabilisticﬁnite-statetransducers.In
Proceedingsofthe40thAnnualMeetingofthe
AssociationforComputationalLinguistics,
pages1–8,Philadelphia,PA.
Ellison,T.Mark.1994.Phonological
derivationinOptimalityTheory.In
456
BookReviews
Proceedingsofthe15thInternational
ConferenceonComputationalLinguistics,
vol.2,pages1007–1013,Kyoto.
Goldsmith,John.2001.Unsupervised
acquisitionofthemorphologyofanatural
language.ComputationalLinguistics
27(2):153–198.
Goldwater,Sharon,ThomasL.Grifﬁths,and
MarkJohnson.2006.Contextual
dependenciesinunsupervisedword
segmentation.InProceedingsofthe21st
InternationalConferenceonComputational
Linguisticsand44thAnnualMeetingofthe
AssociationforComputationalLinguistics,
pages673–680,Sydney.
Goodman,Joshua.1996.Parsingalgorithms
andmetrics.InProceedingsofthe34th
AnnualMeetingoftheAssociationfor
ComputationalLinguistics,pages177–183,
SantaCruz,CA.
Habash,NizarandOwenRambow.2005.
Arabictokenization,part-of-speech
tagging,andmorphological
disambiguationinonefellswoop.In
Proceedingsofthe43rdAnnualMeetingofthe
AssociationforComputationalLinguistics,
pages573–580,AnnArbor,MI.
Inkelas,SharonandCherylZoll.1999.
Reduplicationasmorphologicaldoubling.
Technicalreport412-0800,Rutgers
OptimalityArchive.
Jurafsky,DanielandJamesH.Martin.2008.
SpeechandLanguageProcessing(2nd
edition).PrenticeHall,UpperSaddle
River,NJ.
Koskenniemi,Kimmo.1983.Two-Level
Morphology:AGeneralComputationalModel
forWord-FormRecognitionandProduction.
Ph.D.thesis,DepartmentofGeneral
Linguistics,UniversityofHelsinki,
Helsinki,Finland.
Kudo,Taku,KaoruYamamoto,andYuji
Matsumoto.2004.Applyingconditional
randomﬁeldstoJapanesemorphological
analysis.InProceedingsoftheConferenceon
EmpiricalMethodsinNaturalLanguage
Processing,pages230–237,Barcelona.
Manning,ChristopherD.andHinrich
Sch¨utze.1999.FoundationsofStatistical
NaturalLanguageProcessing.MITPress,
Cambridge,MA.
Roche,EmmanuelandYvesSchabes
(editors).1997.Finite-StateLanguage
Processing.MITPress,Cambridge,MA.
Schone,PatrickandDanielJurafsky.2001.
Knowledge-freeinductionofmorphology
usinglatentsemanticanalysis.In
Proceedingsofthe5thConferenceon
ComputationalNaturalLanguageLearning,
pages67–72,Toulouse.
Shieber,StuartandYvesSchabesand
FernandoC.N.Pereira.1995.Principles
andimplementationofdeductive
parsing.JournalofLogicProgramming
24(1–2):3–36.
Smith,NoahA.,DavidA.Smith,andRoy
W.Tromble.2005.Context-based
morphologicaldisambiguationwith
randomﬁelds.InProceedingsoftheHuman
LanguageTechnologyConferenceand
ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pages475–482,
Vancouver.
Stolcke,AndreasandStephenOmohundro.
1993.HiddenMarkovmodelinductionby
Bayesianmodelmerging.InStephenJos´e
Hanson,JackD.Cowen,andC.LeeGiles,
editors,AdvancesinNeuralInformation
ProcessingSystems,vol.5.Morgan
Kaufmann,SanMateo,CA,pages11–18.
Stump,GregoryT.2001.Inﬂectional
Morphology:ATheoryofParadigm
Structure.CambridgeUniversityPress,
Cambridge,UK.
Yarowsky,DavidandRichardWicentowski.
2001.Minimallysupervisedmorphological
analysisbymultimodalalignment.In
Proceedingsofthe39thAnnualMeetingofthe
AssociationforComputationalLinguistics,
pages207–216,Toulouse.
Noah A. Smith is an assistant professor at Carnegie Mellon University. He conducts research in
statistical models and learning algorithms for natural language processing, including morphol-
ogy and syntax, as well as applications such as machine translation and question answering.
Smith’saddressisLanguageTechnologiesInstitute,SchoolofComputerScience,CarnegieMel-
lon University, 5000 Forbes Avenue, Pittsburgh, PA 15213; e-mail: nasmith@cs.cmu.edu; URL:
www.cs.cmu.edu/∼nasmith.
457


