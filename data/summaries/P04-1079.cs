A similar observation was made in (Papineni et al., 2002: 313).
P02-1040
On the one hand using 1 human reference with uniform results is essential for our methodology, since it means that there is no more “trouble with Recall” (Papineni et al., 2002:314) – a system’s ability to avoid under-generation of N-grams can now be reliably measured.
P02-1040
Some of them use human reference translations, e.g., the BLEU method (Papineni et al., 2002), which is based on comparison of N-gram models in MT output and in a set of human reference translations.
P02-1040
Besides saving cost, the ability to dependably work with a single human translation has an additional advantage: it is now possible to create Recall-based evaluation measures for MT, which has been problematic for evaluation with multiple reference translations, since only one of the choices from the reference set is used in translation (Papineni et al.2002:314). Notably, Recall of weighted N-grams is found to be a good estimation of human judgements about translation Adequacy.
P02-1040
Automatic evaluation methods such as BLEU (Papineni et al., 2002), RED (Akiba et al., 2001), or the weighted N-gram model proposed here may be more consistent in judging quality as compared to human evaluators, but human judgments remain the only criteria for metaevaluating the automatic methods.
P02-1040
