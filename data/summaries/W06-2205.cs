Abstracting from results for concrete test sets, Weeds et al.(2004) try to identify statistical and linguistic properties on that the performance of similarity metrics generally depends.
C04-1146
Both weighted metrics achieved results comparable with those reported by Curran and Moens in (Curran and Moens, 2002) and Turney in (Turney, 2001).
W02-0908
to eliminate the bias discussed in (Weeds et al., 2004)) an edge length from a property to a ranked term e(pk,vj) is weighted by the square root of its absolute frequency radicalBig freq(vj).
C04-1146
The common evaluation method for similarity metrics is comparing their performance on the same test set with the same context representations with some manually created semantic source as the gold standard (Curran and Moens, 2002).
W02-0908
Many approaches are guided by the assumption that similar terms occur in similar context and obtain a context representation of terms as attribute vectors or relation tuples (Curran and Moens, 2002), (Ruge, 1997), (Lin, 1998).
W02-0908
(Manning and SchÂ¨utze, 1999), (Curran and Moens, 2002)) are the essential distinguishing features of the approaches.
W02-0908
Many domainspecific words or word senses are not included; inconsistency and bias are often cited as further major deficiencies of hand-made thesauri (Curran and Moens, 2002), (Senellart and Blondel, 2003).
W02-0908
The evaluation strategy is similar to that pursued in(Curran and Moens, 2002).
W02-0908
The method can be enhanced by applying filtering steps and iterating over new found instances (Phillips and Riloff, 2002).
W02-1017
