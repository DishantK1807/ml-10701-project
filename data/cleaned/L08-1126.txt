<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Chinatsu Aone</author>
<author>Mary Ellen Okurowski</author>
<author>James Gorlinsky</author>
</authors>
<title>Trainable, scalable summarization using robust NLP and machine learning</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (COLING-ACL</booktitle>
<pages>62--66</pages>
<contexts>
<context>Sentence relevance is usually defined as a function of the scores of the terms in the sentence. The simplest technique is to compute sentence score as the average of the term scores in that sentence (Aone et al., 1998). Luhn (1958) proposes a sophisticated variant of the average approach, which promotes clusters of relevant terms, i.e., relevant terms that occur in close proximity to each other. Only up to four no</context>
</contexts>
<marker>Aone, Okurowski, Gorlinsky, 1998</marker>
<rawString>Chinatsu Aone, Mary Ellen Okurowski, and James Gorlinsky. 1998. Trainable, scalable summarization using robust NLP and machine learning. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (COLING-ACL), pages 62–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Cummins</author>
<author>Colm O’Riordan</author>
</authors>
<title>Evolving general term-weighting schemes for information retrieval: Tests on larger collections</title>
<date>2005</date>
<journal>Artificial Intelligence Review</journal>
<pages>24--3</pages>
<marker>Cummins, O’Riordan, 2005</marker>
<rawString>Ronan Cummins and Colm O’Riordan. 2005. Evolving general term-weighting schemes for information retrieval: Tests on larger collections. Artificial Intelligence Review, 24(3-4):277–299, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Edmundson</author>
</authors>
<title>New methods in automatic extracting</title>
<date>1969</date>
<journal>Journal of the ACM</journal>
<volume>16</volume>
<contexts>
<context>tions ignore such lexical relations. (iii) Highly-frequent words: Determiners, prepositions, etc. can be easily eliminated by a high-frequency cutoff (Luhn, 1958) or a list of stop words (Luhn, 1958; Edmundson, 1969). Alternatively, measures from Information Retrieval, such as TF-IDF (‘term frequency ∗ inverse document frequency’) can be used (Salton and McGill, 1983): the document frequency of a term is the num</context>
</contexts>
<marker>Edmundson, 1969</marker>
<rawString>H.P. Edmundson. 1969. New methods in automatic extracting. Journal of the ACM, 16(2):264–285.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An electronic lexical database</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor</editor>
<publisher>MIT Press</publisher>
<contexts>
<context>ms with house. (ii) Lexically-(un)related words: Synonyms etc. can be identified by resources such as WordNet (Fellbaum, 1998); WordNet has been exploited for Text Summarization by, e.g., Aone et al. (1998). Word-sense disambiguation can, e.g., be achieved by training a disambiguator on manuallyannotated data. However, such resources and methods go beyond knowledge-poor approaches. For reasons of robust</context>
<context>nces (which correspond to the set of sentences that received between 12 and 15 annotations), percent agreement is 89.5%. Overall percent agreement is 81.0%. These results are in line with Jing et al. (1998), who found that human agreement decreases as the length of summary increases. To evaluate the automatic scoring methods described in the previous sections, there are roughly two ways: (i) to measure </context>
<context> exact ranking and, instead, to measure how well the methods overlap with the manual annotations in the top-most and bottom-most set of sentences, thus taking into account the findings of Jing et al. (1998). In our evaluation, we persued both ways, by assuming the following evaluation measures: 1. Rank Correlation Rho (RCR): Spearman’s rank correlation coefficient. The sentences of a document are ranked</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
<author>David Yarowsky</author>
</authors>
<title>Estimating upper and lower bounds on the performance of word-sense disambiguation programs</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the ACL</booktitle>
<pages>249--256</pages>
<contexts>
<context> the manual annotations; (ii) to ignore the 6Percent agreement measures the ratio of observed agreements with the majority opinion to possible agreements with the majority opinion (Jing et al., 1998; Gale et al., 1992). [1] ¨Agypten [‘Egypt’] [2] (gr. Aigyptos; ¨agypt. Quemt, “schwarze Erde”) [‘Greek: Aigyptos, Egyptian: Quemt, “black earth” ’] [3] Eine fremdartige Welt ist die alt¨agyptische Hochkultur mit ihren </context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William A. Gale, Kenneth W. Church, and David Yarowsky. 1992. Estimating upper and lower bounds on the performance of word-sense disambiguation programs. In Proceedings of the 30th Annual Meeting of the ACL, pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yihong Gong</author>
<author>Xin Liu</author>
</authors>
<title>Generic text summarization using relevance measure and latent semantic analysis</title>
<date>2001</date>
<booktitle>In Proceedings of the 24th annual international ACM SIGIR conference on research and development in Information Retrieval</booktitle>
<pages>pages</pages>
<marker>Gong, Liu, 2001</marker>
<rawString>Yihong Gong and Xin Liu. 2001. Generic text summarization using relevance measure and latent semantic analysis. In Proceedings of the 24th annual international ACM SIGIR conference on research and development in Information Retrieval, pages 19–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
<author>Regina Barzilay</author>
<author>Kathleen McKeown</author>
<author>Michael Elhadad</author>
</authors>
<title>Summarization evaluation methods experiments and analysis</title>
<date>1998</date>
<booktitle>In Proceedings of the AAAI Intelligent Text Summarization Workshop</booktitle>
<pages>60--68</pages>
<contexts>
<context>king resulting from the manual annotations; (ii) to ignore the 6Percent agreement measures the ratio of observed agreements with the majority opinion to possible agreements with the majority opinion (Jing et al., 1998; Gale et al., 1992). [1] ¨Agypten [‘Egypt’] [2] (gr. Aigyptos; ¨agypt. Quemt, “schwarze Erde”) [‘Greek: Aigyptos, Egyptian: Quemt, “black earth” ’] [3] Eine fremdartige Welt ist die alt¨agyptische Ho</context>
</contexts>
<marker>Jing, Barzilay, McKeown, Elhadad, 1998</marker>
<rawString>Hongyan Jing, Regina Barzilay, Kathleen McKeown, and Michael Elhadad. 1998. Summarization evaluation methods experiments and analysis. In Proceedings of the AAAI Intelligent Text Summarization Workshop, pages 60–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
<author>Jan O Pedersen</author>
<author>Francine Chen</author>
</authors>
<title>A trainable document summarizer</title>
<date>1995</date>
<booktitle>In roceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</booktitle>
<pages>68--73</pages>
<marker>Kupiec, Pedersen, Chen, 1995</marker>
<rawString>Julian Kupiec, Jan O. Pedersen, and Francine Chen. 1995. A trainable document summarizer. In roceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 68–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Peter Luhn</author>
</authors>
<title>The automatic creation of literature abstracts</title>
<date>1958</date>
<journal>IBM Journal of Research &amp; Development</journal>
<volume>2</volume>
<contexts>
<context>easons of robustness and efficiency, many applications ignore such lexical relations. (iii) Highly-frequent words: Determiners, prepositions, etc. can be easily eliminated by a high-frequency cutoff (Luhn, 1958) or a list of stop words (Luhn, 1958; Edmundson, 1969). Alternatively, measures from Information Retrieval, such as TF-IDF (‘term frequency ∗ inverse document frequency’) can be used (Salton and McGi</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>Hans Peter Luhn. 1958. The automatic creation of literature abstracts. IBM Journal of Research &amp; Development, 2(2):159–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Larocca Neto</author>
<author>Alexandre D Santos</author>
<author>Celso A A Kaestner</author>
<author>Alex A Freitas</author>
</authors>
<title>Document clustering and text summarization</title>
<date>2000</date>
<booktitle>In Proceedings of the 4th Int. Conference on Practical Applications of Knowledge Discovery and Data Mining (PADD-2000</booktitle>
<pages>41--55</pages>
<marker>Neto, Santos, Kaestner, Freitas, 2000</marker>
<rawString>Joel Larocca Neto, Alexandre D. Santos, Celso A.A. Kaestner, and Alex A. Freitas. 2000. Document clustering and text summarization. In Proceedings of the 4th Int. Conference on Practical Applications of Knowledge Discovery and Data Mining (PADD-2000), pages 41–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Constantin Orasan</author>
<author>Viktor Pekar</author>
<author>Laura Hasler</author>
</authors>
<title>A comparison of summarisation methods based on term specificity estimation</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC-04</booktitle>
<pages>1037--1041</pages>
<marker>Orasan, Pekar, Hasler, 2004</marker>
<rawString>Constantin Orasan, Viktor Pekar, and Laura Hasler. 2004. A comparison of summarisation methods based on term specificity estimation. In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC-04), pages 1037–1041.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin F Porter</author>
</authors>
<title>An algorithm for suffix stripping</title>
<date>1980</date>
<contexts>
<context>ase. Luhn (1958) and Orasan et al. (2004) use pattern matching to identify related words by common prefixes. A more linguistically-motivated method is stemming, by applying, e.g., the Porter stemmer (Porter, 1980) to the normalized words, as has been done by, e.g., Neto et al. (2000) or Orasan et al. (2004) in Text Summarization. Finally, words may be represented by a list of ngrams (Shannon, 1948). For insta</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin F. Porter. 1980. An algorithm for suffix stripping.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Program</author>
</authors>
<title>Reprinted in: K. Sparck Jones</title>
<date>1997</date>
<booktitle>Eds.) Readings in Information Retrieval</booktitle>
<pages>313--316</pages>
<marker>Program, 1997</marker>
<rawString>Program, 14:130–137. Reprinted in: K. Sparck Jones and P. Willet (Eds.) Readings in Information Retrieval, 313–316. 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Michael J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval</title>
<date>1983</date>
<publisher>McGrawHill</publisher>
<location>New York</location>
<contexts>
<context>off (Luhn, 1958) or a list of stop words (Luhn, 1958; Edmundson, 1969). Alternatively, measures from Information Retrieval, such as TF-IDF (‘term frequency ∗ inverse document frequency’) can be used (Salton and McGill, 1983): the document frequency of a term is the number of documents in a document collection that contain that term. This measure indicates how frequently a term is used in general. Words that are frequent</context>
<context>as all individual sentences are represented as vectors in a multidimensional term space. The score of a sentence is computed as the similarity between the document’s vector and the sentence’s vector (Salton and McGill, 1983). This technique from Information Retrieval has been applied to Text Summarization by, e.g., Gong and Liu (2001). Often, however, publications remain unclear as to how exactly the relevances of the i</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Gerard Salton and Michael J. McGill. 1983. Introduction to Modern Information Retrieval. New York: McGrawHill.</rawString>
</citation>
</citationList>
</algorithm>

