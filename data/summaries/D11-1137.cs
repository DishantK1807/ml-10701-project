Koo et al. (2008) semi-sup 93.16
P08-1068
L. Huang and K. Sagae. 2010. Dynamic programming
P10-1110
parsers such as Koo et al. (2008)’s semi-sup and
P08-1068
vided in (McDonald et al., 2005), using the second-
P05-1012
McDonald et al. (2005) 90.9
P05-1012
(Collins,2000;CharniakandJohnson,2005;Huang,
P05-1022
or grandchild part used in Carreras (2007). Our
D07-1101
Koo and Collins (2010) model2 92.93
P10-1001
T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-
P08-1068
J. M. Eisner. 1996b. Three new probabilistic models for
C96-1058
(Eisner, 1996a). Parameters θ are trained using
C96-1058
mented using Huang and Chiang (2005)’s algorithm
W05-1506
R. McDonald and F. Pereira. 2006. Online learning of
E06-1011
Suzuki et al. (2009), because ours does not use addi-
D09-1058
Koo and Collins (2010) presented third-order de-
P10-1001
Charniak and Johnson (2005) and Huang (2008)
P05-1022 P08-1067
L. Huang and D. Chiang. 2005. Better k-best parsing. In
W05-1506
F. J. Och. 2003. Minimum error rate training in statisti-
P03-1021
MERT (Och, 2003) and for each sentence in the de-
P03-1021
Eisner, 1996a) for reranking and extend it to capture
C96-1058
Huang and Sagae (2010) 92.1
P10-1110
second-order Eisner (1996a)
C96-1058
Collins (2000) and Charniak and Johnson (2005)
P05-1022
reranking algorithm, opposed to Sangati et al.(2009)’s approach which reranks only k-best candi-
W09-3839
and lazy computation (Huang and Chiang, 2005).
W05-1506
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
P05-1022
Suzuki et al. (2009) 93.79
D09-1058
Z. Tu, Y. Liu, Y. Hwang, Q. Liu, and S. Lin. 2010. De-
C10-1123
based MT systems (Mi and Huang, 2008). Lines
D08-1022 P08-1067
guage model (Chiang, 2007). It is an approximate
J07-2003
X. Carreras. 2007. Experiments with a higher-order
D07-1101
(sibling) McDonald et al. (2005)
P05-1012
H. Mi and L. Huang. 2008. Forest-based translation rule
D08-1022 P08-1067
generative model C (Eisner, 1996b; Eisner, 1996a),
C96-1058
parsers. Huang (2008) extended it to a forest rerank-
P08-1067
Following Huang (2008), we also prune away nodes
P08-1067
pergraph data strucureHG(Tu et al., 2010).
C10-1123
Sangati et al. (2009) reversed the usual order of the
W09-3839
search. For a constituent parser, Huang (2008) ap-
P08-1067
T. Koo and M. Collins. 2010. Efficient third-order de-
P10-1001
(tri-sibling) Model 2 (Koo and Collins, 2010)
P10-1001
MERT algorithm (Och, 2003) on development data
P03-1021
of reductions list is identical to Eisner (1996a) and
C96-1058
McDonald and Pereira (2006) 91.5
E06-1011
third-order grandsibling model (Sangati et al., 2009)
W09-3839
first-order McDonald et al. (2005)
P05-1012
sparseness issues. To overcome this, Eisner (1996a)
C96-1058
L. Huang. 2008. Forest reranking: Discriminative pars-
P08-1067
Koo et al. (2008) standard 92.02
P08-1068
size of a forest. Huang (2008)’s pruning algo-
P08-1067
F. Sangati, W. Zuidema, and R. Bod. 2009. A generative
W09-3839
compactly than k-best lists (Huang, 2008). More-
P08-1067
J. M. Eisner. 1996a. An empirical comparison of prob-
C96-1058
Koo and Collins (2010) model1 93.04
P10-1001
J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. 2009.
D09-1058
(grandsibling) Model 1 (Koo and Collins, 2010)
P10-1001
(Goldberg and Elhadad, 2010) or higher-order struc-
N10-1115
order features (Koo and Collins, 2010). We can not
P10-1001
2005; McDonald and Pereira, 2006). Therefore,
E06-1011
ant of Eisner’s generative model C (Eisner, 1996b;
C96-1058
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
P05-1012
Y. Goldberg and M. Elhadad. 2010. An efficient algo-
N10-1115
Second, following Sangati et al. (2009), we define
W09-3839
namic programming algorithm. Sangati et al. (2009)
W09-3839
D. Chiang. 2007. Hierarchical phrase-based translation.
J07-2003
