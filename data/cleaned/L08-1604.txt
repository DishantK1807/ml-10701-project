<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>S Ait-Mokhtar</author>
<author>J-P Chanod</author>
<author>C Roux</author>
</authors>
<title>Robustness beyond shallowness: incremental deep parsing</title>
<date>2002</date>
<journal>Natural Language Engineering</journal>
<volume>8</volume>
<contexts>
<context>ampaign (Vanrullen et al., 2006) • CORDIAL, a rule based parser developed by SYNAPSE9; • SYGMART, developed at LIRMM10; • XIP, a cascade rule-based parser developed at Xerox Research Center Europe11 (Ait-Mokhtar et al., 2002). It may be noted that these parsing systems are based on very different paradigms and produce different kinds of output. While keeping their specificities, the parsers are compared using a common sy</context>
</contexts>
<marker>Ait-Mokhtar, Chanod, Roux, 2002</marker>
<rawString>S. Ait-Mokhtar, J.-P. Chanod, and C. Roux. 2002. Robustness beyond shallowness: incremental deep parsing. Natural Language Engineering,, 8(3):121–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Allison</author>
<author>C S Wallace</author>
<author>C N Yee</author>
</authors>
<title>When is a string like a string</title>
<date>1990</date>
<booktitle>In Proceedings of International Symposium on Artificial Intelligence in Mathematics (AIM</booktitle>
<location>Ft. Lauderdale, Florida</location>
<contexts>
<context>s, 1997) in a DARPA/NIST evaluation campaign about speech recognition. He found out that by aligning the output of the participating speech transcription systems with a dynamic programming algorithm (Allison et al., 1990) and by selecting the hypothesis which was proposed by the majority of the systems, he obtained better performances than with the best system. Since, the idea gained support, first in the speech proc</context>
</contexts>
<marker>Allison, Wallace, Yee, 1990</marker>
<rawString>L. Allison, C. S. Wallace, and C. N. Yee. 1990. When is a string like a string? In Proceedings of International Symposium on Artificial Intelligence in Mathematics (AIM), Ft. Lauderdale, Florida, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Besanc¸on</author>
<author>G de Chalendar</author>
</authors>
<title>L’analyseur syntaxique de lima dans la campagne d’´evaluation easy</title>
<date>2005</date>
<location>Dourdan, France</location>
<marker>Besanc¸on, de Chalendar, 2005</marker>
<rawString>R. Besanc¸on and G. de Chalendar. 2005. L’analyseur syntaxique de lima dans la campagne d’´evaluation easy. Dourdan, France, June. TALN’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blache</author>
</authors>
<title>Property grammars: A fully constraintbased theory</title>
<date>2005</date>
<booktitle>Constraint Solving and Language Processing</booktitle>
<volume>3438</volume>
<editor>In H. Christiansen, editor</editor>
<publisher>LNAI, Springer</publisher>
<marker>Blache, 2005</marker>
<rawString>P. Blache. 2005. Property grammars: A fully constraintbased theory. In H. Christiansen, editor, Constraint Solving and Language Processing, volume 3438. LNAI, Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Boullier</author>
<author>B Sagot</author>
</authors>
<title>Analyse syntaxique profonde `a grande ´echelle: Sxlfg. Traitement Automatique des Langues</title>
<date>2005</date>
<contexts>
<context>ures good chances of success for Passage. The parsing systems are provided by participants or contractors, including: • FRMG, an hybrid TIG/TAG parser derived from a metagrammar, developed at INRIA4 (Boullier and Sagot, 2005), (Thomasset and de la Clergerie, 2005), (de la Clergerie, 2005b); • SXLFG, a LFG-based parser, developed at INRIA (Boullier and Sagot, 2005), (Boullier et al., 2005), • LLP2 a TAG parser also derive</context>
</contexts>
<marker>Boullier, Sagot, 2005</marker>
<rawString>P. Boullier and B. Sagot. 2005. Analyse syntaxique profonde `a grande ´echelle: Sxlfg. Traitement Automatique des Langues, 46(2):65–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Boullier</author>
<author>L Cl´ement</author>
<author>B Sagot</author>
<author>E Villemonte de la Clergerie</author>
</authors>
<title>Simple comme easy</title>
<date>2005</date>
<pages>57--60</pages>
<location>Dourdan, France</location>
<marker>Boullier, Cl´ement, Sagot, Clergerie, 2005</marker>
<rawString>P. Boullier, L. Cl´ement, B. Sagot, and E. Villemonte de la Clergerie. 2005. Simple comme easy. pages 57–60, Dourdan, France, June. TALN’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>D Lin</author>
<author>D Prescher</author>
<author>H Uszkoreit</author>
</authors>
<title>Proceedings of the workshop beyond parseval toward improved evaluation measures for parsing systems</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC</booktitle>
<location>Las Palmas, Spain</location>
<contexts>
<context>anging from shallow to deep parsers, and that it is possible to run an evaluation campaign from end to end using a common syntactic formalism with a very positive impact for the field, inspired from (Carroll et al., 2002). PASSAGE aims at pursuing and extending the line of research initiated by the EASY campaign. Its main objective is to use 10 of the participating parsing systems to EASY to jointly parse a French co</context>
</contexts>
<marker>Carroll, Lin, Prescher, Uszkoreit, 2002</marker>
<rawString>J. Carroll, D. Lin, D. Prescher, and H. Uszkoreit. 2002. Proceedings of the workshop beyond parseval toward improved evaluation measures for parsing systems. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC), Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Villemonte de la Clergerie</author>
</authors>
<title>Dyalog: a tabular logic programming based environment for nlp</title>
<date>2005</date>
<location>Barcelona, Spain</location>
<contexts>
<context>ebank, and validating automatic parser combination for the purpose, we believe that PASSAGE will help seeing the emergence of linguistic processing chains exploiting richer lexical information (de la Clergerie, 2005a), in particular semantic ones bootstrapped from the large Treebank by acquisition. At the end of the project, the final set of syntactic annotations will also be made freely available to the communi</context>
<context>rsing systems are provided by participants or contractors, including: • FRMG, an hybrid TIG/TAG parser derived from a metagrammar, developed at INRIA4 (Boullier and Sagot, 2005), (Thomasset and de la Clergerie, 2005), (de la Clergerie, 2005b); • SXLFG, a LFG-based parser, developed at INRIA (Boullier and Sagot, 2005), (Boullier et al., 2005), • LLP2 a TAG parser also derived from a metagrammar, developed at LORI</context>
</contexts>
<marker>Clergerie, 2005</marker>
<rawString>E. Villemonte de la Clergerie. 2005a. Dyalog: a tabular logic programming based environment for nlp. Barcelona, Spain, October. CSLP’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Villemonte</author>
</authors>
<title>de la Clergerie. 2005b. From metagrammars to factorized tag/tig parsers</title>
<date></date>
<location>Vancouver, Canada</location>
<marker>Villemonte, </marker>
<rawString>E. Villemonte de la Clergerie. 2005b. From metagrammars to factorized tag/tig parsers. Vancouver, Canada, October. IWPT’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan G Fiscus</author>
</authors>
<title>A post-processing system to yield reduced word error rates: recognizer output voting error reduction (rover</title>
<date>1997</date>
<booktitle>In In proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding</booktitle>
<pages>347--357</pages>
<location>Santa Barbara, CA</location>
<contexts>
<context>obtain a combination with better performance than the best one is not new. What now is known as the ROVER (Reduced Output Voting Error Reduction) algorithm was invented to our knowledge by J. Fiscus (Fiscus, 1997) in a DARPA/NIST evaluation campaign about speech recognition. He found out that by aligning the output of the participating speech transcription systems with a dynamic programming algorithm (Allison</context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>Jonathan G Fiscus. 1997. A post-processing system to yield reduced word error rates: recognizer output voting error reduction (rover). In In proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding, pages 347–357, Santa Barbara, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Francopoulo</author>
</authors>
<title>Tagparser et technolangue-easy</title>
<date>2005</date>
<location>Dourdan, France</location>
<contexts>
<context>loped at LORIA5 (Roussanaly et al., 2005); • LIMA, dependency based parser developed at LIC2M / CEA-LIST6 (Besanc¸on and de Chalendar, 2005); • TAGParser, an extended chunker developed at TAGMATICA7 (Francopoulo, 2005); • Two parsers based on Property Grammars, developed at LPL8and using constraint satisfaction (Blache, 4http://www.inria.fr/rocquencourt 5http://www.loria.fr/ 6http://www-list.cea.fr/ 7http://www.ta</context>
</contexts>
<marker>Francopoulo, 2005</marker>
<rawString>G. Francopoulo. 2005. Tagparser et technolangue-easy. Dourdan, France, June. TALN’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Galliano</author>
<author>E Geoffrois</author>
<author>G Gravier</author>
<author>J-F Bonastre</author>
<author>D Mostefa</author>
<author>K Choukri</author>
</authors>
<title>Corpus description of the ester evaluation campaign for the rich transcription of french broadcast news</title>
<date>2006</date>
<booktitle>In ELRA, editor, In proceedings of the fifth international conference on Language Resources and Evaluation (LREC 2006</booktitle>
<publisher>ELRA</publisher>
<location>Genoa, Italy</location>
<contexts>
<context>rench part. JRC-ACQUIS 120K Part of the total body of European Union laws, existing in several languages of the European Commission. ESTER 100K A corpus of oral transcriptions from the ESTER project (Galliano et al., 2006) LE MONDE 100K A journalistic corpus, with worldwide news. EASY: 1M The corpus used for the EASY campaign already covers various genres and includes a subset of around 4K sentences (76K words) that h</context>
</contexts>
<marker>Galliano, Geoffrois, Gravier, Bonastre, Mostefa, Choukri, 2006</marker>
<rawString>S. Galliano, E. Geoffrois, G. Gravier, J.-F. Bonastre, D. Mostefa, and K. Choukri. 2006. Corpus description of the ester evaluation campaign for the rich transcription of french broadcast news. In ELRA, editor, In proceedings of the fifth international conference on Language Resources and Evaluation (LREC 2006), Genoa, Italy, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L¨o¨of</author>
<author>C Gollan</author>
<author>S Hahn</author>
<author>G Heigold</author>
<author>B Hoffmeister</author>
<author>C Plahl</author>
<author>D Rybach</author>
<author>R Schl¨uter</author>
</authors>
<title>The rwth 2007 tc-star evaluation system for european english and spanish</title>
<date>2007</date>
<booktitle>In In proceedings of the Interspeech Conference</booktitle>
<pages>2145--2148</pages>
<marker>L¨o¨of, Gollan, Hahn, Heigold, Hoffmeister, Plahl, Rybach, Schl¨uter, 2007</marker>
<rawString>J. L¨o¨of, C. Gollan, S. Hahn, G. Heigold, B. Hoffmeister, C. Plahl, D. Rybach, R. Schl¨uter, , and H. Ney. 2007. The rwth 2007 tc-star evaluation system for european english and spanish. In In proceedings of the Interspeech Conference, pages 2145–2148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>N Ueffing</author>
<author>Herman Ney</author>
</authors>
<title>Automatic sentence segmentation and punctuation prediction for spoken language translation</title>
<date>2006</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT</booktitle>
<pages>158--165</pages>
<location>Trento, Italy</location>
<contexts>
<context>fidence annotation to yield a validated language resource from data produced in an evalation campaign (Paroubek, 2000). Machine translation evaluation is another area where ROVER algorithms are used (Matusov et al., 2006). In our case, we will use the text itself to realign the annotations provided by the various parser before computing their combination, as we did for our first experiments with the EASY evaluation c</context>
</contexts>
<marker>Matusov, Ueffing, Ney, 2006</marker>
<rawString>Evgeny Matusov, N. Ueffing, and Herman Ney. 2006. Automatic sentence segmentation and punctuation prediction for spoken language translation. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), pages 158–165, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Paroubek</author>
<author>Isabelle Robba</author>
<author>Anne Vilnat</author>
<author>Christelle Ayache</author>
</authors>
<title>Data, annotations and measures in EASY the evaluation campaign for parsers of French</title>
<date>2006</date>
<contexts>
<context>ic annotations to move forward (the acronym stands for ”Produire des Annotations Syntaxiques `a Grande ´Echelle in French). It builds up on the results of the EASY French parsing evaluation campaign (Paroubek et al., 2006), funded by the French Technolangue program, which has shown that French parsing systems are now available, ranging from shallow to deep parsers, and that it is possible to run an evaluation campaign</context>
<context>, coordination, apposition and collocation. More details can be found in (Vilnat et al., 2004). The evaluation metrics used are precision, recall and fmeasure, with 15 various relaxation constraints (Paroubek et al., 2006). Resource #words Description WIKIPEDIA 200K A freely available corpus covering many domains of knowledge and collectively written by many authors, with various styles though biased toward descriptio</context>
</contexts>
<marker>Paroubek, Robba, Vilnat, Ayache, 2006</marker>
<rawString>Patrick Paroubek, Isabelle Robba, Anne Vilnat, and Christelle Ayache. 2006. Data, annotations and measures in EASY the evaluation campaign for parsers of French.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>In proceedings of the fifth international conference on Language Resources and Evaluation (LREC 2006</booktitle>
<pages>315--320</pages>
<editor>In ELRA, editor</editor>
<publisher>ELRA</publisher>
<location>Genoa, Italy</location>
<marker></marker>
<rawString>In ELRA, editor, In proceedings of the fifth international conference on Language Resources and Evaluation (LREC 2006), pages 315–320, Genoa, Italy, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Paroubek</author>
<author>Isabelle Robba</author>
<author>Anne Vilnat</author>
<author>Christelle Ayache</author>
</authors>
<title>Easy, evaluation of parsers of french: what are the results</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC</booktitle>
<location>Marrakech, Morroco</location>
<contexts>
<context>, we will use the text itself to realign the annotations provided by the various parser before computing their combination, as we did for our first experiments with the EASY evaluation campaign data (Paroubek et al., 2008). Note that if the different parser do not necessarily use the same word and sentence segmentation, we will need to first realign all the data with a common word and sentence segmentation (which can </context>
</contexts>
<marker>Paroubek, Robba, Vilnat, Ayache, 2008</marker>
<rawString>Patrick Paroubek, Isabelle Robba, Anne Vilnat, and Christelle Ayache. 2008. Easy, evaluation of parsers of french: what are the results? In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC), Marrakech, Morroco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Paroubek</author>
</authors>
<title>Language resources as by-product of evaluation: the multitag example. In</title>
<date>2000</date>
<booktitle>In proceedings of the Second International Conference on Language Resources and Evaluation (LREC2000</booktitle>
<volume>1</volume>
<pages>151--154</pages>
<contexts>
<context>ne such instance is for POS tagging, where the algorithm was applied to provide POS tags with confidence annotation to yield a validated language resource from data produced in an evalation campaign (Paroubek, 2000). Machine translation evaluation is another area where ROVER algorithms are used (Matusov et al., 2006). In our case, we will use the text itself to realign the annotations provided by the various pa</context>
</contexts>
<marker>Paroubek, 2000</marker>
<rawString>Patrick Paroubek. 2000. Language resources as by-product of evaluation: the multitag example. In In proceedings of the Second International Conference on Language Resources and Evaluation (LREC2000), volume 1, pages 151–154.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Vilnat</author>
</authors>
<title>Christelle Ayache Patrick Paroubek, Isabelle Robba</title>
<date>2006</date>
<marker>Vilnat, 2006</marker>
<rawString>Anne Vilnat Christelle Ayache Patrick Paroubek, Isabelle Robba. 2006. Data, annotations and measures in easy the evaluation campaign for parsers of french.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>In proceedings of the fifth international conference on Language Resources and Evaluation (LREC 2006</booktitle>
<pages>315--320</pages>
<editor>In ELRA, editor</editor>
<publisher>ELRA</publisher>
<location>Genoa, Italy</location>
<marker></marker>
<rawString>In ELRA, editor, In proceedings of the fifth international conference on Language Resources and Evaluation (LREC 2006), pages 315–320, Genoa, Italy, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Roussanaly</author>
<author>B Crabb´e</author>
<author>J Perrin</author>
</authors>
<title>L’analyseur syntaxique de lima dans la campagne d’´evaluation easy</title>
<date>2005</date>
<location>Dourdan, France</location>
<marker>Roussanaly, Crabb´e, Perrin, 2005</marker>
<rawString>A. Roussanaly, B. Crabb´e, and J. Perrin. 2005. L’analyseur syntaxique de lima dans la campagne d’´evaluation easy. Dourdan, France, June. TALN’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Improved rover using language model information. In</title>
<date>2000</date>
<booktitle>In proceedings of the ISCA ITRW Workshop on Automatic Speech Recognition: Challenges for the new Millenium</booktitle>
<pages>47--52</pages>
<location>Paris</location>
<contexts>
<context>t speech recognizers as confidence weights in the hypothesis lattice obtained by combining the different ouptuts and by applying language models to guide the final stage of best hypothesis selection (Schwenk and Gauvain, 2000). In general better results are obtained with retaining only the output of the two or three best performing systems, in which case the relative improvement can go up to 20% with respect to the best p</context>
</contexts>
<marker>Schwenk, Gauvain, 2000</marker>
<rawString>Holger Schwenk and Jean-Luc Gauvain. 2000. Improved rover using language model information. In In proceedings of the ISCA ITRW Workshop on Automatic Speech Recognition: Challenges for the new Millenium, pages 47–52, Paris, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Thomasset</author>
<author>E Villemonte de la Clergerie</author>
</authors>
<title>Comment obtenir plus des meta-grammaires</title>
<date>2005</date>
<location>Dourdan, France</location>
<marker>Thomasset, Clergerie, 2005</marker>
<rawString>F. Thomasset and E. Villemonte de la Clergerie. 2005. Comment obtenir plus des meta-grammaires. Dourdan, France, June. TALN’05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Vanrullen</author>
<author>P Blache</author>
<author>J-M Balfourier</author>
</authors>
<date>2006</date>
<contexts>
<context>gmatica.com/ 8http://cnrs.oxcs.fr/ 2005). The first parser is symbolic and deterministic while the second one is statistical and trained thanks to the results of the parsers during the EASY campaign (Vanrullen et al., 2006) • CORDIAL, a rule based parser developed by SYNAPSE9; • SYGMART, developed at LIRMM10; • XIP, a cascade rule-based parser developed at Xerox Research Center Europe11 (Ait-Mokhtar et al., 2002). It m</context>
</contexts>
<marker>Vanrullen, Blache, Balfourier, 2006</marker>
<rawString>T. Vanrullen, P. Blache, and J.-M. Balfourier. 2006.</rawString>
</citation>
<citation valid="true">
<title>Constraint-based parsing as an efficient solution: Results from the parsing evaluation campaign easy</title>
<date></date>
<booktitle>In proceedings of the fifth international conference on Language Resources and Evaluation (LREC 2006</booktitle>
<editor>In ELRA, editor</editor>
<publisher>ELRA</publisher>
<location>Genoa, Italy</location>
<marker></marker>
<rawString>Constraint-based parsing as an efficient solution: Results from the parsing evaluation campaign easy. In ELRA, editor, In proceedings of the fifth international conference on Language Resources and Evaluation (LREC 2006), Genoa, Italy, May. ELRA.</rawString>
</citation>
</citationList>
</algorithm>

