The experiments reported here were done using data extracted by Ratnaparkhi et al.(1994) from the Penn Treebank (Marcus et al., 1993) WSJ corpus.
J93-2004
Using the classes in addition to the original noun (Brill and R~nik, 1994; Resnik, 1992; Resnik, 1995)seems, however, a better strategy.
C94-2195 W95-0105
Earlier works on this problem (Ratnaparkhi et al., 1994; Brill and Resnik, 1994; Collins and Brooks, 1995; Zavrel et al., 1997) represented an example by the 4-tuple <v, nl, p, n2> containing the VP head, the direct object NP head, the preposition, and the indirect object NP head respectively.
C94-2195 W95-0103 W95-0105 W97-1016
The results are compared with a maximum-entropy method (Ratnaparkhi et al., 1994), transformation-based learning (TBL, Brill and Resnik (1994)), an instantiation of the backoff estimation (Collins and Brooks, 1995) and a memory-based method (Zavrel et al., 1997).
C94-2195 W95-0103 W95-0105 W97-1016
In all cases, the quoted figures axe the best results obtained by the authors; with the exception of the Brill and Resnik (1994) result, which was obtained by Zavrel et al.(1997) using the same method.
C94-2195 W97-1016
Both Collins and Brooks (1995) and Zavrel et al.(1997) have enhanced the feature generation in various ways; as described in this paper, this was also done for SNOW.
W95-0103 W97-1016
That algorithm has been applied for natural language disambiguation tasks and related problems and perform remarkably well (Golding and Roth, 1996; Dagan et al., 1997; Roth and Zelenko, 1998).
P98-2186 W97-0306
Originally, TBL was evaluated by Brill and Resnik (1994) 125 I | I I i | ! ! | t I Bas~linellemma\[ +CL+FF \[WN5+FFIWN10+FF \[ WN15+FF \[ 58.1 77.4 79.1 78.8 78.1 77.9 Table 3: Learning results for combinations of FF and other sources: The four leftmost columns indicate the classes added to our basic feature set, 1emma.
C94-2195
