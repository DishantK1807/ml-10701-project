Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 72–82,
Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
Exact Decoding of Syntactic Translation Models
through Lagrangian Relaxation
Alexander M. Rush
MIT CSAIL,
Cambridge, MA 02139, USA
srush@csail.mit.edu
Michael Collins
Department of Computer Science,
Columbia University,
New York, NY 10027, USA
mcollins@cs.columbia.edu
Abstract
We describe an exact decoding algorithm for
syntax-based statistical translation. The ap-
proach uses Lagrangian relaxation to decom-
pose the decoding problem into tractable sub-
problems, thereby avoiding exhaustive dy-
namic programming. The method recovers ex-
act solutions, with certificates of optimality,
on over 97% of test examples; it has compa-
rable speed to state-of-the-art decoders.
1 Introduction
Recent work has seen widespread use of syn-
chronous probabilistic grammars in statistical ma-
chine translation (SMT). The decoding problem for
a broad range of these systems (e.g., (Chiang, 2005;
Marcu et al., 2006; Shen et al., 2008)) corresponds
to the intersection of a (weighted) hypergraph with
an n-gram language model.1 The hypergraph rep-
resents a large set of possible translations, and is
created by applying a synchronous grammar to the
source language string. The language model is then
used to rescore the translations in the hypergraph.
Decoding with these models is challenging,
largely because of the cost of integrating an n-gram
language model into the search process. Exact dy-
namic programming algorithms for the problem are
well known (Bar-Hillel et al., 1964), but are too ex-
pensive to be used in practice.2 Previous work on
decoding for syntax-based SMT has therefore been
focused primarily on approximate search methods.
This paper describes an efficient algorithm for ex-
act decoding of synchronous grammar models for
translation. We avoid the construction of (Bar-Hillel
1This problem is also relevant to other areas of statistical
NLP, for example NL generation (Langkilde, 2000).
2E.g., with a trigram language model they run in O(nullEnullw6)
time, where nullEnull is the number of edges in the hypergraph, and
w is the number of distinct lexical items in the hypergraph.
et al., 1964) by using Lagrangian relaxation to de-
compose the decoding problem into the following
sub-problems:
1. Dynamic programming over the weighted hy-
pergraph. This step does not require language
model integration, and hence is highly efficient.
2. Application of an all-pairs shortest path al-
gorithm to a directed graph derived from the
weighted hypergraph. The size of the derived
directed graph is linear in the size of the hyper-
graph, hence this step is again efficient.
Informally, the first decoding algorithm incorporates
the weights and hard constraints on translations from
the synchronous grammar, while the second decod-
ing algorithm is used to integrate language model
scores. Lagrange multipliers are used to enforce
agreement between the structures produced by the
two decoding algorithms.
In this paper we first give background on hyper-
graphs and the decoding problem. We then describe
our decoding algorithm. The algorithm uses a sub-
gradient method to minimize a dual function. The
dual corresponds to a particular linear programming
(LP) relaxation of the original decoding problem.
The method will recover an exact solution, with a
certificate of optimality, if the underlying LP relax-
ation has an integral solution. In some cases, how-
ever, the underlying LP will have a fractional solu-
tion, in which case the method will not be exact. The
second technical contribution of this paper is to de-
scribe a method that iteratively tightens the underly-
ing LP relaxation until an exact solution is produced.
We do this by gradually introducing constraints to
step 1 (dynamic programming over the hypergraph),
while still maintaining efficiency.
72
We report experiments using the tree-to-string
model of (Huang and Mi, 2010). Our method gives
exact solutions on over 97% of test examples. The
method is comparable in speed to state-of-the-art de-
coding algorithms; for example, over 70% of the test
examples are decoded in 2 seconds or less. We com-
pare our method to cube pruning (Chiang, 2007),
and find that our method gives improved model
scores on a significant number of examples. One
consequence of our work is that we give accurate
estimates of the number of search errors for cube
pruning.
2 Related
Work
A variety of approximate decoding algorithms have
been explored for syntax-based translation systems,
including cube-pruning (Chiang, 2007; Huang and
Chiang, 2007), left-to-right decoding with beam
search (Watanabe et al., 2006; Huang and Mi, 2010),
and coarse-to-fine methods (Petrov et al., 2008).
Recent work has developed decoding algorithms
based on finite state transducers (FSTs). Iglesias et
al. (2009) show that exact FST decoding is feasible
for a phrase-based system with limited reordering
(the MJ1 model (Kumar and Byrne, 2005)), and de
Gispert et al. (2010) show that exact FST decoding
is feasible for a specific class of hierarchical gram-
mars (shallow-1 grammars). Approximate search
methods are used for more complex reordering mod-
els or grammars. The FST algorithms are shown to
produce higher scoring solutions than cube-pruning
on a large proportion of examples.
Lagrangian relaxation is a classical technique
in combinatorial optimization (Korte and Vygen,
2008). Lagrange multipliers are used to add lin-
ear constraints to an existing problem that can be
solved using a combinatorial algorithm; the result-
ing dual function is then minimized, for example
using subgradient methods. In recent work, dual
decomposition—a special case of Lagrangian relax-
ation, where the linear constraints enforce agree-
ment between two or more models—has been ap-
plied to inference in Markov random fields (Wain-
wright et al., 2005; Komodakis et al., 2007; Sontag
et al., 2008), and also to inference problems in NLP
(Rush et al., 2010; Koo et al., 2010). There are close
connections between dual decomposition and work
on belief propagation (Smith and Eisner, 2008).
3 Background: Hypergraphs
Translation with many syntax-based systems (e.g.,
(Chiang, 2005; Marcu et al., 2006; Shen et al., 2008;
Huang and Mi, 2010)) can be implemented as a
two-step process. The first step is to take an in-
put sentence in the source language, and from this
to create a hypergraph (sometimes called a transla-
tion forest) that represents the set of possible trans-
lations (strings in the target language) and deriva-
tions under the grammar. The second step is to
integrate an n-gram language model with this hy-
pergraph. For example, in the system of (Chiang,
2005), the hypergraph is created as follows: first, the
source side of the synchronous grammar is used to
create a parse forest over the source language string.
Second, transduction operations derived from syn-
chronous rules in the grammar are used to create the
target-language hypergraph. Chiang’s method uses
a synchronous context-free grammar, but the hyper-
graph formalism is applicable to a broad range of
other grammatical formalisms, for example depen-
dency grammars (e.g., (Shen et al., 2008)).
A hypergraph is a pair (V,E) where V =
null1,2,...,nullVnullnull is a set of vertices, and E is a set of
hyperedges. A single distinguished vertex is taken
as the root of the hypergraph; without loss of gener-
ality we take this vertex to be v = 1. Each hyper-
edge e null E is a tuple nullnullv1,v2,...,vknull,v0null where
v0 null V , and vi null null2...nullVnullnull for i = 1...k. The
vertex v0 is referred to as the head of the edge. The
ordered sequence nullv1,v2,...,vknull is referred to as
the tail of the edge; in addition, we sometimes refer
to v1,v2,...vk as the children in the edge. The num-
ber of children k may vary across different edges,
but k null 1 for all edges (i.e., each edge has at least
one child). We will use h(e) to refer to the head of
an edge e, and t(e) to refer to the tail.
We will assume that the hypergraph is acyclic: in-
tuitively this will mean that no derivation (as defined
below) contains the same vertex more than once (see
(Martin et al., 1990) for a formal definition).
Each vertex v null V is either a non-terminal in the
hypergraph, or a leaf. The set of non-terminals is
VN = nullv null V : nulle null E such that h(e) = vnull
Conversely, the set of leaves is defined as
VL = nullv null V :null nulle null E such that h(e) = vnull
73
Finally, we assume that each v null V has a label
l(v). The labels for leaves will be words, and will
be important in defining strings and language model
scores for those strings. The labels for non-terminal
nodes will not be important for results in this paper.3
We now turn to derivations. Define an index set
null = V null E. A derivation is represented by a vector
y = nullyr : r null nullnull where yv = 1 if vertex v is used in
the derivation, yv = 0 otherwise (similarly ye = 1 if
edge e is used in the derivation, ye = 0 otherwise).
Thus y is a vector in null0,1nullnullnullnull. A valid derivation
satisfies the following constraints:
null y1 = 1 (the root must be in the derivation).
null For all v null VN, yv =summationtexte:h(e)=v ye.
null For all v null 2...nullVnull, yv =summationtexte:vnullt(e) ye.
We use null to refer to the set of valid derivations.
The set null is a subset of null0,1nullnullnullnull (not all members of
null0,1nullnullnullnull will correspond to valid derivations).
Each derivation y in the hypergraph will imply an
ordered sequence of leaves v1 ...vn. We use s(y) to
refer to this sequence. The sentence associated with
the derivation is then l(v1)...l(vn).
In a weighted hypergraph problem, we assume a
parameter vector θ = nullθr : r null nullnull. The score for
any derivation is f(y) = θ null y = summationtextrnull θryr. Sim-
ple bottom-up dynamic programming—essentially
the CKY algorithm—can be used to find ynull =
argmaxynull f(y) under these definitions.
The focus of this paper will be to solve problems
involving the integration of a k’th order language
model with a hypergraph. In these problems, the
score for a derivation is modified to be
f(y) =
summationdisplay
rnullnull
θryr +
nsummationdisplay
i=k
θ(vinullk+1,vinullk+2,...,vi) (1)
where v1 ...vn = s(y). The θ(vinullk+1,...,vi)
parameters score n-grams of length k. These
parameters are typically defined by a language
model, for example with k = 3 we would have
θ(vinull2,vinull1,vi) = logp(l(vi)nulll(vinull2),l(vinull1)).
The problem is then to find ynull = argmaxynull f(y)
under this definition.
Throughout this paper we make the following as-
sumption when using a bigram language model:
3They might for example be non-terminal symbols from the
grammar used to generate the hypergraph.
Assumption 3.1 (Bigram start/end assump-
tion.) For any derivation y, with leaves
s(y) = v1,v2,...,vn, it is the case that: (1)
v1 = 2 and vn = 3; (2) the leaves 2 and 3 cannot
appear at any other position in the strings s(y) for
y null null; (3) l(2) = <s> where <s> is the start
symbol in the language model; (4) l(3) = </s>
where </s> is the end symbol.
This assumption allows us to incorporate lan-
guage model terms that depend on the start and end
symbols. It also allows a clean solution for boundary
conditions (the start/end of strings).4
4 A
Simple Lagrangian Relaxation
Algorithm
We now give a Lagrangian relaxation algorithm for
integration of a hypergraph with a bigram language
model, in cases where the hypergraph satisfies the
following simplifying assumption:
Assumption 4.1 (The strict ordering assumption.)
For any two leaves v and w, it is either the case
that: 1) for all derivations y such that v and w are
both in the sequence l(y), v precedes w; or 2) for all
derivations y such that v and w are both in l(y), w
precedes v.
Thus under this assumption, the relative ordering
of any two leaves is fixed. This assumption is overly
restrictive:5 the next section describes an algorithm
that does not require this assumption. However de-
riving the simple algorithm will be useful in devel-
oping intuition, and will lead directly to the algo-
rithm for the unrestricted case.
4.1 A
Sketch of the Algorithm
At a high level, the algorithm is as follows. We in-
troduce Lagrange multipliers u(v) for all v null VL,
with initial values set to zero. The algorithm then
involves the following steps: (1) For each leaf v,
find the previous leaf w that maximizes the score
θ(w,v) null u(w) (call this leaf αnull(v), and define
αv = θ(αnull(v),v) null u(αnull(v))). (2) find the high-
est scoring derivation using dynamic programming
4The assumption generalizes in the obvious way to k’th or-
der language models: e.g., for trigram models we assume that
v1 = 2, v2 = 3, vn = 4, l(2) = l(3) = <s>, l(4) = </s>.
5It is easy to come up with examples that violate this as-
sumption: for example a hypergraph with edges nullnull4,5null,1null and
nullnull5,4null,1null violates the assumption. The hypergraphs found in
translation frequently contain alternative orderings such as this.
74
over the original (non-intersected) hypergraph, with
leaf nodes having weights θv + αv + u(v). (3) If
the output derivation from step 2 has the same set of
bigrams as those from step 1, then we have an exact
solution to the problem. Otherwise, the Lagrange
multipliers u(v) are modified in a way that encour-
ages agreement of the two steps, and we return to
step 1.
Steps 1 and 2 can be performed efficiently; in par-
ticular, we avoid the classical dynamic programming
intersection, instead relying on dynamic program-
ming over the original, simple hypergraph.
4.2 A
Formal Description
We now give a formal description of the algorithm.
Define null null VLnullVL to be the set of all ordered pairs
nullv,wnull such that there is at least one derivation y with
v directly preceding w in s(y). Extend the bit-vector
y to include variables y(v,w) for nullv,wnull null null where
y(v,w) = 1 if leaf v is followed by w in s(y), 0
otherwise. We redefine the index set to be null = V null
E null null, and define null null null0,1nullnullnullnull to be the set of all
possible derivations. Under assumptions 3.1 and 4.1
above, null = nully : y satisfies constraints C0, C1, C2null
where the constraint definitions are:
null (C0) The yv and ye variables form a derivation
in the hypergraph, as defined in section 3.
null (C1) For all v null VL such that v null= 2, yv =summationtext
w:nullw,vnullnull y(w,v).
null (C2) For all v null VL such that v null= 3, yv =summationtext
w:nullv,wnullnull y(v,w).
C1 states that each leaf in a derivation has exactly
one in-coming bigram, and that each leaf not in the
derivation has 0 incoming bigrams; C2 states that
each leaf in a derivation has exactly one out-going
bigram, and that each leaf not in the derivation has 0
outgoing bigrams.6
The score of a derivation is now f(y) = θnully, i.e.,
f(y) =
summationdisplay
v
θvyv+
summationdisplay
e
θeye+
summationdisplay
nullv,wnullnull
θ(v,w)y(v,w)
where θ(v,w) are scores from the language model.
Our goal is to compute ynull = argmaxynull f(y).
6Recall that according to the bigram start/end assumption
the leaves 2/3 are reserved for the start/end of the sequence
s(y), and hence do not have an incoming/outgoing bigram.
Initialization: Set u0(v) = 0 for all v null VL
Algorithm: For t = 1...T:
null yt = argmaxynullnullnull L(utnull1,y)
null If yt satisfies constraints C2, return yt,
Else nullv null VL, ut(v) =
utnull1(v) nullδt
parenleftBig
yt(v) nullsummationtextw:nullv,wnullnull yt(v,w)
parenrightBig
.
Figure 1: A simple Lagrangian relaxation algorithm.
δt > 0 is the step size at iteration t.
Next, define nullnull as
nullnull = nully : y satisfies constraints C0 and C1null
In this definition we have dropped the C2 con-
straints. To incorporate these constraints, we use
Lagrangian relaxation, with one Lagrange multiplier
u(v) for each constraint in C2. The Lagrangian is
L(u,y) = f(y) +
summationdisplay
v
u(v)(y(v) null
summationdisplay
w:nullv,wnullnull
y(v,w))
= β nully
where βv = θv + u(v), βe = θe, and β(v,w) =
θ(v,w) nullu(v).
The dual problem is to find minu L(u) where
L(u) = max
ynull null
L(u,y)
Figure 1 shows a subgradient method for solving
this problem. At each point the algorithm finds
yt = argmaxynull null L(utnull1,y), where utnull1 are the
Lagrange multipliers from the previous iteration. If
yt satisfies the C2 constraints in addition to C0 and
C1, then it is returned as the output from the algo-
rithm. Otherwise, the multipliers u(v) are updated.
Intuitively, these updates encourage the values of yv
and summationtextw:nullv,wnullnull y(v,w) to be equal; formally, these
updates correspond to subgradient steps.
The main computational step at each iteration is to
compute argmaxynull null L(utnull1,y) This step is easily
solved, as follows (we again use βv, βe and β(v1,v2)
to refer to the parameter values that incorporate La-
grange multipliers):
null For all v null VL, define αnull(v) =
argmaxw:nullw,vnullnull β(w,v) and αv =
β(αnull(v),v). For all v null VN define αv = 0.
75
null Using dynamic programming, find values for
the yv and ye variables that form a valid deriva-
tion, and that maximize
fnull(y) =summationtextv(βv + αv)yv +summationtexte βeye.
null Set y(v,w) = 1 iff y(w) = 1 and αnull(w) = v.
The critical point here is that through our definition
of nullnull, which ignores the C2 constraints, we are able
to do efficient search as just described. In the first
step we compute the highest scoring incoming bi-
gram for each leaf v. In the second step we use
conventional dynamic programming over the hyper-
graph to find an optimal derivation that incorporates
weights from the first step. Finally, we fill in the
y(v,w) values. Each iteration of the algorithm runs
in O(nullEnull + nullnullnull) time.
There are close connections between Lagrangian
relaxation and linear programming relaxations. The
most important formal results are: 1) for any value
of u, L(u) null f(ynull) (hence the dual value provides
an upper bound on the optimal primal value); 2) un-
der an appropriate choice of the step sizes δt, the
subgradient algorithm is guaranteed to converge to
the minimum of L(u) (i.e., we will minimize the
upper bound, making it as tight as possible); 3) if
at any point the algorithm in figure 1 finds a yt that
satisfies the C2 constraints, then this is guaranteed
to be the optimal primal solution.
Unfortunately, this algorithm may fail to produce
a good solution for hypergraphs where the strict or-
dering constraint does not hold. In this case it is
possible to find derivations y that satisfy constraints
C0, C1, C2, but which are invalid. As one exam-
ple, consider a derivation with s(y) = 2,4,5,3 and
y(2,3) = y(4,5) = y(5,4) = 1. The constraints
are all satisfied in this case, but the bigram variables
are invalid (e.g., they contain a cycle).
5 The
Full Algorithm
We now describe our full algorithm, which does not
require the strict ordering constraint. In addition, the
full algorithm allows a trigram language model. We
first give a sketch, and then give a formal definition.
5.1 A
Sketch of the Algorithm
A crucial idea in the new algorithm is that of
paths between leaves in hypergraph derivations.
Previously, for each derivation y, we had de-
fined s(y) = v1,v2,...,vn to be the sequence
of leaves in y. In addition, we will define
g(y) = p0,v1,p1,v2,p2,v3,p3,...,pnnull1,vn,pn
where each pi is a path in the derivation between
leaves vi and vi+1. The path traces through the non-
terminals that are between the two leaves in the tree.
As an example, consider the following derivation
(with hyperedges nullnull2,5null,1null and nullnull3,4null,2null):
1
2
3 4
5
For this example g(y) is null1 null,2 nullnull null2 null,3 nullnull
null3 nullnull, 3, null3 nullnull null3 null,4 nullnull null4 nullnull, 4, null4 nullnull null4 null,2 nullnull
null2 null,5 nullnull null5 nullnull, 5, null5 nullnull null5 null,1 nullnull. States of the
form nulla nullnull and nulla nullnull where a is a leaf appear in
the paths respectively before/after the leaf a. States
of the form nulla,bnull correspond to the steps taken in a
top-down, left-to-right, traversal of the tree, where
down and up arrows indicate whether a node is be-
ing visited for the first or second time (the traversal
in this case would be 1,2,3,4,2,5,1).
The mapping from a derivation y to a path g(y)
can be performed using the algorithm in figure 2.
For a given derivation y, define E(y) = nully : ye =
1null, and use E(y) as the set of input edges to this
algorithm. The output from the algorithm will be a
set of states S, and a set of directed edges T, which
together fully define the path g(y).
In the simple algorithm, the first step was to
predict the previous leaf for each leaf v, under
a score that combined a language model score
with a Lagrange multiplier score (i.e., compute
argmaxw β(w,v) where β(w,v) = θ(w,v) +
u(w)). In this section we describe an algorithm that
for each leaf v again predicts the previous leaf, but in
addition predicts the full path back to that leaf. For
example, rather than making a prediction for leaf 5
that it should be preceded by leaf 4, we would also
predict the path null4 nullnullnull4 null,2 nullnull null2 null,5 nullnullnull5 nullnull be-
tween these two leaves. Lagrange multipliers will
be used to enforce consistency between these pre-
dictions (both paths and previous words) and a valid
derivation.
76
Input: A set E of hyperedges. Output: A directed graph
S,T where S is a set of vertices, and T is a set of edges.
Step 1: Creating S: Define S = nullenullES(e) where S(e)
is defined as follows. Assume e = nullnullv1,v2,...,vknull,v0null.
Include the following states in S(e): (1) nullv0 null,v1 nullnull and
nullvknull,v0nullnull. (2) nullvjnull,vj+1nullnull for j = 1...k null 1 (if k = 1
then there are no such states). (3) In addition, for any vj
for j = 1...k such that vj null VL, add the states nullvj nullnull
and nullvj nullnull.
Step 2: Creating T: T is formed by including the fol-
lowing directed arcs: (1) Add an arc from nulla,bnull null S
to nullc,dnull null S whenever b = c. (2) Add an arc from
nulla,b nullnull null S to nullc nullnull null S whenever b = c. (3) Add
an arc from nulla nullnull null S to nullb null,cnull null S whenever a = b.
Figure 2: Algorithm for constructing a directed graph
(S,T) from a set of hyperedges E.
5.2 A
Formal Description
We first use the algorithm in figure 2 with the en-
tire set of hyperedges, E, as its input. The result
is a directed graph (S,T) that contains all possible
paths for valid derivations in V,E (it also contains
additional, ill-formed paths). We then introduce the
following definition:
Definition 5.1 A trigram path p is p =
nullv1,p1,v2,p2,v3null where: a) v1,v2,v3 null VL;
b) p1 is a path (sequence of states) between nodes
nullv1 nullnull and nullv2 nullnull in the graph (S,T); c) p2 is a
path between nodes nullv2 nullnull and nullv3 nullnull in the graph
(S,T). We define null to be the set of all trigram paths
in (S,T).
The set null of trigram paths plays an analogous role
to the set null of bigrams in our previous algorithm.
We use v1(p),p1(p),v2(p),p2(p),v3(p) to refer
to the individual components of a path p. In addi-
tion, define SN to be the set of states in S of the
form nulla,bnull (as opposed to the form nullc nullnull or nullc nullnull
where c null VL).
We now define a new index set, null = V null E null
SN nullnull, adding variables ys for s null SN, and yp for
p null null. If we take null null null0,1nullnullnullnull to be the set of
valid derivations, the optimization problem is to find
ynull = argmaxynull f(y), where f(y) = θ nully, that is,
f(y) =
summationdisplay
v
θvyv +
summationdisplay
e
θeye +
summationdisplay
s
θsys +
summationdisplay
p
θpyp
In particular, we might define θs = 0 for all s,
and θp = logp(l(v3(p))nulll(v1(p)),l(v2(p))) where
null D0. The yv and ye variables form a valid derivation
in the original hypergraph.
null D1. For all s null SN, ys =summationtexte:snullS(e) ye (see figure 2
for the definition of S(e)).
null D2. For all v null VL, yv =summationtextp:v3(p)=v yp
null D3. For all v null VL, yv =summationtextp:v2(p)=v yp
null D4. For all v null VL, yv =summationtextp:v1(p)=v yp
null D5. For all s null SN, ys =summationtextp:snullp1(p) yp
null D6. For all s null SN, ys =summationtextp:snullp2(p) yp
null Lagrangian with Lagrange multipliers for D3–D6:
L(y,λ,γ,u,v) = θ nully
+summationtextv λv
parenleftBig
yv nullsummationtextp:v2(p)=v yp
parenrightBig
+summationtextv γv
parenleftBig
yv nullsummationtextp:v1(p)=v yp
parenrightBig
+summationtexts us
parenleftBig
ys nullsummationtextp:snullp1(p) yp
parenrightBig
+summationtexts vs
parenleftBig
ys nullsummationtextp:snullp2(p) yp
parenrightBig
.
Figure 3: Constraints D0–D6, and the Lagrangian.
p(w3nullw1,w2) is a trigram probability.
The set null is large (typically exponential in size):
however, we will see that we do not need to represent
the yp variables explicitly. Instead we will be able
to leverage the underlying structure of a path as a
sequence of states.
The set of valid derivations is null = nully :
y satisfies constraints D0–D6null where the constraints
are shown in figure 3. D1 simply states that ys = 1
iff there is exactly one edge e in the derivation such
that s null S(e). Constraints D2–D4 enforce consis-
tency between leaves in the trigram paths, and the yv
values. Constraints D5 and D6 enforce consistency
between states seen in the paths, and the ys values.
The Lagrangian relaxation algorithm is then de-
rived in a similar way to before. Define
nullnull = nully : y satisfies constraints D0–D2null
We have dropped the D3–D6 constraints, but these
will be introduced using Lagrange multipliers. The
resulting Lagrangian is shown in figure 3, and can
be written as L(y,λ,γ,u,v) = β null y where βv =
θv+λv+γv, βs = θs+us+vs, βp = θpnullλ(v2(p))null
γ(v1(p)) nullsummationtextsnullp1(p) u(s) nullsummationtextsnullp2(p) v(s).
The dual is L(λ,γ,u,v) =
maxynull null L(y,λ,γ,u,v); figure 4 shows a sub-
gradient method that minimizes this dual. The key
step in the algorithm at each iteration is to compute
77
Initialization: Set λ0 = 0,γ0 = 0,u0 = 0,v0 = 0
Algorithm: For t = 1...T:
null yt = argmaxynullnullnull L(y,λtnull1,γtnull1,utnull1,vtnull1)
null If yt satisfies the constraints D3–D6, return yt, else:
nullv null VL, λtv = λtnull1v nullδt(ytv nullsummationtextp:v2(p)=v ytp)
nullv null VL, γtv = γtnull1v nullδt(ytv nullsummationtextp:v1(p)=v ytp)
nulls null SN, uts = utnull1s nullδt(yts nullsummationtextp:snullp1(p) ytp)
nulls null SN, vts = vtnull1s nullδt(yts nullsummationtextp:snullp2(p) ytp)
Figure 4: The full Lagrangian relaxation algortihm. δt >
0 is the step size at iteration t.
argmaxynull null L(y,λ,γ,u,v) = argmaxynull null β null y
where β is defined above. Again, our definition
of nullnull allows this maximization to be performed
efficiently, as follows:
1. For each v null VL, define αnullv =
argmaxp:v3(p)=v β(p), and αv = β(αnullv).
(i.e., for each v, compute the highest scoring
trigram path ending in v.)
2. Find values for the yv, ye and ys variables that
form a valid derivation, and that maximize
fnull(y) =summationtextv(βv +αv)yv +summationtexte βeye+summationtexts βsys
3. Set yp = 1 iff yv3(p) = 1 and p = αnullv3(p).
The first step involves finding the highest scoring in-
coming trigram path for each leaf v. This step can be
performed efficiently using the Floyd-Warshall all-
pairs shortest path algorithm (Floyd, 1962) over the
graph (S,T); the details are given in the appendix.
The second step involves simple dynamic program-
ming over the hypergraph (V,E) (it is simple to in-
tegrate the βs terms into this algorithm). In the third
step, the path variables yp are filled in.
5.3 Properties
We now describe some important properties of the
algorithm:
Efficiency. The main steps of the algorithm are:
1) construction of the graph (S,T); 2) at each it-
eration, dynamic programming over the hypergraph
(V,E); 3) at each iteration, all-pairs shortest path al-
gorithms over the graph (S,T). Each of these steps
is vastly more efficient than computing an exact in-
tersection of the hypergraph with a language model.
Exact solutions. By usual guarantees for La-
grangian relaxation, if at any point the algorithm re-
turns a solution yt that satisfies constraints D3–D6,
then yt exactly solves the problem in Eq. 1.
Upper bounds. At each point in the algorithm,
L(λt,γt,ut,vt) is an upper bound on the score of
the optimal primal solution, f(ynull). Upper bounds
can be useful in evaluating the quality of primal so-
lutions from either our algorithm or other methods
such as cube pruning.
Simplicity of implementation. Construction of
the (S,T) graph is straightforward. The other
steps—hypergraph dynamic programming, and all-
pairs shortest path—are widely known algorithms
that are simple to implement.
6 Tightening
the Relaxation
The algorithm that we have described minimizes
the dual function L(λ,γ,u,v). By usual results for
Lagrangian relaxation (e.g., see (Korte and Vygen,
2008)), L is the dual function for a particular LP re-
laxation arising from the definition of nullnull and the ad-
ditional constaints D3–D6. In some cases the LP
relaxation has an integral solution, in which case
the algorithm will return an optimal solution yt.7
In other cases, when the LP relaxation has a frac-
tional solution, the subgradient algorithm will still
converge to the minimum of L, but the primal solu-
tions yt will move between a number of solutions.
We now describe a method that incrementally
adds hard constraints to the set nullnull, until the method
returns an exact solution. For a given y null nullnull,
for any v with yv = 1, we can recover the previ-
ous two leaves (the trigram ending in v) from ei-
ther the path variables yp, or the hypergraph vari-
ables ye. Specifically, define vnull1(v,y) to be the leaf
preceding v in the trigram path p with yp = 1 and
v3(p) = v, and vnull2(v,y) to be the leaf two posi-
tions before v in the trigram path p with yp = 1 and
v3(p) = v. Similarly, define vnullnull1(v,y) and vnullnull2(v,y)
to be the preceding two leaves under the ye vari-
ables. If the method has not converged, these two
trigram definitions may not be consistent. For a con-
7Provided that the algorithm is run for enough iterations for
convergence.
78
sistent solution, we require vnull1(v,y) = vnullnull1(v,y)
and vnull2(v,y) = vnullnull2(v,y) for all v with yv = 1.
Unfortunately, explicitly enforcing all of these con-
straints would require exhaustive dynamic program-
ming over the hypergraph using the (Bar-Hillel et
al., 1964) method, something we wish to avoid.
Instead, we enforce a weaker set of constraints,
which require far less computation. Assume some
function pi : VL null null1,2,...qnull that partitions the
set of leaves into q different partitions. Then we will
add the following constraints to nullnull:
pi(vnull1(v,y)) = pi(vnullnull1(v,y))
pi(vnull2(v,y)) = pi(vnullnull2(v,y))
for all v such that yv = 1. Finding argmaxynull null θ null
y under this new definition of nullnull can be performed
using the construction of (Bar-Hillel et al., 1964),
with q different lexical items (for brevity we omit
the details). This is efficient if q is small.8
The remaining question concerns how to choose
a partition pi that is effective in tightening the relax-
ation. To do this we implement the following steps:
1) run the subgradient algorithm until L is close to
convergence; 2) then run the subgradient algorithm
for m further iterations, keeping track of all pairs
of leaf nodes that violate the constraints (i.e., pairs
a = vnull1(v,y)/b = vnullnull1(v,y) or a = vnull2(v,y)/b =
vnullnull2(v,y) such that a null= b); 3) use a graph color-
ing algorithm to find a small partition that places all
pairs nulla,bnull into separate partitions; 4) continue run-
ning Lagrangian relaxation, with the new constraints
added. We expand pi at each iteration to take into ac-
count new pairs nulla,bnull that violate the constraints.
In related work, Sontag et al. (2008) describe
a method for inference in Markov random fields
where additional constraints are chosen to tighten
an underlying relaxation. Other relevant work in
NLP includes (Tromble and Eisner, 2006; Riedel
and Clarke, 2006). Our use of partitions pi is related
to previous work on coarse-to-fine inference for ma-
chine translation (Petrov et al., 2008).
7 Experiments
We report experiments on translation from Chinese
to English, using the tree-to-string model described
8In fact in our experiments we use the original hypergraph
to compute admissible outside scores for an exact A* search
algorithm for this problem. We have found the resulting search
algorithm to be very efficient.
Time %age %age %age %age
(LR) (DP) (ILP) (LP)
0.5s 37.5 10.2 8.8 21.0
1.0s 57.0 11.6 13.9 31.1
2.0s 72.2 15.1 21.1 45.9
4.0s 82.5 20.7 30.7 63.7
8.0s 88.9 25.2 41.8 78.3
16.0s 94.4 33.3 54.6 88.9
32.0s 97.8 42.8 68.5 95.2
Median time 0.79s 77.5s 12.1s 2.4s
Figure 5: Results showing percentage of examples that are de-
coded in less than t seconds, for t = 0.5,1.0,2.0,...,32.0. LR
= Lagrangian relaxation; DP = exhaustive dynamic program-
ming; ILP = integer linear programming; LP = linear program-
ming (LP does not recover an exact solution). The (I)LP ex-
periments were carried out using Gurobi, a high-performance
commercial-grade solver.
in (Huang and Mi, 2010). We use an identical
model, and identical development and test data, to
that used by Huang and Mi.9 The translation model
is trained on 1.5M sentence pairs of Chinese-English
data; a trigram language model is used. The de-
velopment data is the newswire portion of the 2006
NIST MT evaluation test set (616 sentences). The
test set is the newswire portion of the 2008 NIST
MT evaluation test set (691 sentences).
We ran the full algorithm with the tightening
method described in section 6. We ran the method
for a limit of 200 iterations, hence some exam-
ples may not terminate with an exact solution. Our
method gives exact solutions on 598/616 develop-
ment set sentences (97.1%), and 675/691 test set
sentences (97.7%).
In cases where the method does not converge
within 200 iterations, we can return the best primal
solution yt found by the algorithm during those it-
erations. We can also get an upper bound on the
difference f(ynull)nullf(yt) using mint L(ut) as an up-
per bound on f(ynull). Of the examples that did not
converge, the worst example had a bound that was
1.4% of f(yt) (more specifically, f(yt) was -24.74,
and the upper bound on f(ynull) nullf(yt) was 0.34).
Figure 5 gives information on decoding time for
our method and two other exact decoding methods:
integer linear programming (using constraints D0–
D6), and exhaustive dynamic programming using
the construction of (Bar-Hillel et al., 1964). Our
9We thank Liang Huang and Haitao Mi for providing us with
their model and data.
79
method is clearly the most efficient, and is compara-
ble in speed to state-of-the-art decoding algorithms.
We also compare our method to cube pruning
(Chiang, 2007; Huang and Chiang, 2007). We reim-
plemented cube pruning in C++, to give a fair com-
parison to our method. Cube pruning has a parame-
ter, b, dictating the maximum number of items stored
at each chart entry. With b = 50, our decoder
finds higher scoring solutions on 50.5% of all exam-
ples (349 examples), the cube-pruning method gets a
strictly higher score on only 1 example (this was one
of the examples that did not converge within 200 it-
erations). With b = 500, our decoder finds better so-
lutions on 18.5% of the examples (128 cases), cube-
pruning finds a better solution on 3 examples. The
median decoding time for our method is 0.79 sec-
onds; the median times for cube pruning with b = 50
and b = 500 are 0.06 and 1.2 seconds respectively.
Our results give a very good estimate of the per-
centage of search errors for cube pruning. A natural
question is how large b must be before exact solu-
tions are returned on almost all examples. Even at
b = 1000, we find that our method gives a better
solution on 95 test examples (13.7%).
Figure 5 also gives a speed comparison of our
method to a linear programming (LP) solver that
solves the LP relaxation defined by constraints D0–
D6. We still see speed-ups, in spite of the fact
that our method is solving a harder problem (it pro-
vides integral solutions). The Lagrangian relaxation
method, when run without the tightening method
of section 6, is solving a dual of the problem be-
ing solved by the LP solver. Hence we can mea-
sure how often the tightening procedure is abso-
lutely necessary, by seeing how often the LP solver
provides a fractional solution. We find that this is
the case on 54.0% of the test examples: the tighten-
ing procedure is clearly important. Inspection of the
tightening procedure shows that the number of par-
titions required (the parameter q) is generally quite
small: 59% of examples that require tightening re-
quire q null 6; 97.2% require q null 10.
8 Conclusion
We have described a Lagrangian relaxation algo-
rithm for exact decoding of syntactic translation
models, and shown that it is significantly more effi-
cient than other exact algorithms for decoding tree-
to-string models. There are a number of possible
ways to extend this work. Our experiments have
focused on tree-to-string models, but the method
should also apply to Hiero-style syntactic transla-
tion models (Chiang, 2007). Additionally, our ex-
periments used a trigram language model, however
the constraints in figure 3 generalize to higher-order
language models. Finally, our algorithm recovers
the 1-best translation for a given input sentence; it
should be possible to extend the method to find k-
best solutions.
A Computing the Optimal Trigram Paths
For each v null VL, define αv = maxp:v3(p)=v β(p), where
β(p) = h(v1(p),v2(p),v3(p))nullλ1(v1(p))nullλ2(v2(p))nullsummationtext
snullp1(p) u(s)null
summationtext
snullp2(p) v(s). Here h is a function that
computes language model scores, and the other terms in-
volve Lagrange mulipliers. Our task is to compute αnullv for
all v null VL.
It is straightforward to show that the S,T graph is
acyclic. This will allow us to apply shortest path algo-
rithms to the graph, even though the weights u(s) and
v(s) can be positive or negative.
For any pair v1,v2 null VL, define null(v1,v2) to be the
set of paths between nullv1 nullnull and nullv2 nullnull in the graph S,T.
Each path p gets a score scoreu(p) = nullsummationtextsnullp u(s).
Next, define pnullu(v1,v2) = argmaxpnullnull(v1,v2) scoreu(p),
and scorenullu(v1,v2) = scoreu(pnull). We assume similar
definitions for pnullv(v1,v2) and scorenullv(v1,v2). The pnullu and
scorenullu values can be calculated using an all-pairs short-
est path algorithm, with weights u(s) on nodes in the
graph. Similarly, pnullv and scorenullv can be computed using
all-pairs shortest path with weights v(s) on the nodes.
Having calculated these values, define null (v) for any
leaf v to be the set of trigrams (x,y,v) such that: 1)
x,y null VL; 2) there is a path from nullx nullnull to nully nullnull and from
nully nullnull to nullv nullnull in the graph S,T. Then we can calculate
αv = max
(x,y,v)nullnull (v)
(h(x,y,v) nullλ1(x) nullλ2(y)
+pnullu(x,y) + pnullv(y,v))
in O(nullnull (v)null) time, by brute force search through the set
null (v).
Acknowledgments Alexander Rush and Michael
Collins were supported under the GALE program of the
Defense Advanced Research Projects Agency, Contract
No. HR0011-06-C-0022. Michael Collins was also sup-
ported by NSF grant IIS-0915176. We also thank the
anonymous reviewers for very helpful comments; we
hope to fully address these in an extended version of the
paper.
80
References
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal
properties of simple phrase structure grammars. In
Language and Information: Selected Essays on their
Theory and Application, pages 116–150.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics, pages 263–270. Association for
Computational Linguistics.
D. Chiang. 2007. Hierarchical phrase-based translation.
computational linguistics, 33(2):201–228.
Adria de Gispert, Gonzalo Iglesias, Graeme Blackwood,
Eduardo R. Banga, and William Byrne. 2010. Hierar-
chical Phrase-Based Translation with Weighted Finite-
State Transducers and Shallow-n Grammars. In Com-
putational linguistics, volume 36, pages 505–533.
Robert W. Floyd. 1962. Algorithm 97: Shortest path.
Commun. ACM, 5:345.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 144–151,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Liang Huang and Haitao Mi. 2010. Efficient incremental
decoding for tree-to-string translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 273–283, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Gonzalo Iglesias, Adri`a de Gispert, Eduardo R. Banga,
and William Byrne. 2009. Rule filtering by pattern
for efficient hierarchical translation. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 380–388, Athens, Greece,
March. Association for Computational Linguistics.
N. Komodakis, N. Paragios, and G. Tziritas. 2007.
MRF optimization via dual decomposition: Message-
passing revisited. In International Conference on
Computer Vision.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decompo-
sition for parsing with non-projective head automata.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1288–1298, Cambridge, MA, October. Association for
Computational Linguistics.
B.H. Korte and J. Vygen. 2008. Combinatorial optimiza-
tion: theory and algorithms. Springer Verlag.
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation.
In Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in Nat-
ural Language Processing, pages 161–168, Vancou-
ver, British Columbia, Canada, October. Association
for Computational Linguistics.
I. Langkilde. 2000. Forest-based statistical sentence gen-
eration. In Proceedings of the 1st North American
chapter of the Association for Computational Linguis-
tics conference, pages 170–177. Morgan Kaufmann
Publishers Inc.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 44–
52, Sydney, Australia, July. Association for Computa-
tional Linguistics.
R.K. Martin, R.L. Rardin, and B.A. Campbell. 1990.
Polyhedral characterization of discrete dynamic pro-
gramming. Operations research, 38(1):127–138.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using lan-
guage projections. In Proceedings of the 2008 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 108–116, Honolulu, Hawaii, October.
Association for Computational Linguistics.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ’06, pages 129–137, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 1–11, Cambridge, MA, October. Association for
Computational Linguistics.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577–585, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
D.A. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proc. EMNLP, pages 145–156.
D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and
Y. Weiss. 2008. Tightening LP relaxations for MAP
using message passing. In Proc. UAI.
Roy W. Tromble and Jason Eisner. 2006. A fast
finite-state relaxation method for enforcing global con-
straints on sequence decoding. In Proceedings of
81
the main conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics, HLT-NAACL
’06, pages 423–430, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
M. Wainwright, T. Jaakkola, and A. Willsky. 2005. MAP
estimation via agreement on trees: message-passing
and linear programming. In IEEE Transactions on In-
formation Theory, volume 51, pages 3697–3717.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics, ACL-44, pages 777–784,
Morristown, NJ, USA. Association for Computational
Linguistics.
82

