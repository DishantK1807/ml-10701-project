~1 Michael Elhadad Mathemat~s and Computer Saence Dept Ben Gunon Umveraty m the Negev Beer-Sheva, 84105 Israel http //mr¢ cs.bgu ac.xl/ elhadad Abstract We investigate one techmque to produce a summary of an original text without requmng zts full semanttc interpretation, but instead relying on a model of the topic progresston m the text derived from lexlcal chains We present a new algonthm to compute lexlcal chains m a text, merging several robust knowledge sources the WordNet thesaurus, a partof-speech tagger and shallow parser for the identification of nominal groups, and a segmentatton algorithm dernved from (Hearst, 1994) Summarization proceeds m three steps the ongmal text is first segmented, lexxcal chmns are constructed, strong chains are ldsnhfied and ssgnzflcant sentences are extracted from the text We present m tins paper empirical results on the tdent~catlon of strong chains and of slgmfieant sentences Introduction Summarization ts the process of condensing a source text into a shorter Version preserving its reformation content It can serve several goals -from survey analysis of a sctenttfic field to qmck mchcatzve notes on the general toplc of a text Producing a quahty reformative summary of an arbitrary text remams a challenge winch reqmres full understanding of the text Indtcattves, lm~artes, winch can be used to qmckly decide whether a text is worth reading, are naturally easter to produce In tins paper we investigate a method for the production of such mdxcatlve summaries from arintrary text (Jones, 1993) descnbes summarization as a twostep process (1) Building from the source text a source representatton, (2) Summary generationfonmng summary representation from the source representation bmlt m the first step and synthesismg the output summary text Within this framework, the relevantquestion is what reformation has to be included m the source representation m order to create a summary There are three types of source text reformation hngmstlc, domain and commumcatlve Each of these text aspects can be chosen as a barns for source representatlon Summaries can be bmlt on a deep semantic anal= ysis of the source text For example, (McKcown and Radsv, !905)investigate ways to produce a coherent summary of several texts describing the same event, when a detaded semantic representation of the source texts m available (m their case, they use MUC-style systems to interpret the source texts) Alternatzvely, early summarisatzon systems (Luhn, 1968) used only hngumtlc source mformation The mtmtlon was that the moat frequent words represent the tmportant concepts of the text In this approach the source representation was the frequency table of text words Tins representation abstracts the text into the umon of its words w~thout conmdermg any connectlon among them In contrast to these two extreme pcsltlous (using as a source representation a full semantic representation of the text or reducing ltto a simple frequency table), we deal m tins paper wttb the issue of producmg a summary from an arbitrary text without reqmrmg zts full understanding, but using wtdely avadable knowledge sources Our mare goal is therefore to find a middle ground for source representation, rich enough to braid quality indicative summaries, but easy enough to extract from the source text to work on arbltrary text Over-slmphficatlon can harm the quahty of the source representation As a trivial illustration, consider the following two sequences • 1 "Dr Kenny has sn~ented an anesthetsc maehsne Thss devwe controls the rate at wh:ch an anaesthctsc ss pumped into the blood" 2 "Dr Kenny has :nvented an anesthet:c machsne The Doctor spent two years on thu research" ~Dr Kenny ~ appears once m both sequences and I0 I I I I i I II II I ! so does ~nach:n¢ ~ But sequence 1 ts about the roach:he, and sequence 2 m about the *doctor ~ Tlus example mchcates that zf the source representation does not supply mformatlon about semantically related terms, one cannot capture the %boutnesg' of the text, and therefore the summary will not capture the mare point of the original text The norton of cohemon, introduced m (Halhday and Hasan, 1976) captures part of the mtmtmn Cohereon is a dewce for "sticking together" different parts of the text Cohesion m achmved through the use of semantmaUy related terms, reference, elhpsm and conjunctlous Among these dtfferent means, the most easdy zdentfllable and the most frequent type m lemcal cohe" slon (as discussed m (Hoey, ~ 1991)) Lexlcal cohesion is created by usmg semantically related words Halhday and Hasan classflled lemcal cohesion into relteratlon category and collocatlon category Rezteratlon can be achmved by repetltlon, synonyms and hyponyms Collocatmn relatzons spectfy the relation between words that tend to co-occur m the same lexzeal contexts (e g, "She works as a teacher m the.School ~) Collocation relations are more problematzc for ldenttticat~on than rezterat|on, but both of t~hese categones are Identifiable on the surface oi~ the text Lextcal cohemon occurs not only between two terms, but among sequences of related words ~ called/ez~cal chains (Morns and Hlrst, 1991) Lemcal chains provide a representahon of the lemcal cohemve structare of the text Lemcal chains have also been used for mfo~nahon retrieval (Stamnand, 1996) and for correction ofmalaproptsms (Htrst and St-Onge, 1997 (to appear)) In tlus paper, we mveshgate how lemcal chmns can be used as a source representation for summarization Another nnportant dunenmon of the lmgumtzc structure of a source,text m captured under the related not,on of coherence Coherence defines the macro-level semantic structure of a connected dLscourse, while cohesion creates connectedness m a non-structural manner Coherence m represented m terms of coherence relat~ous between text segments, such as cla~orahon, cause and ezplanat|on Some researchers, e g, (Ono, Kazuo, and Seljl, 1994), use chscourse structure (encoded umng RST (MAnn and Thompson, 1987) as a source representatxon for summanzatxon) Clearly, thin representation ms expresmve enough, the question m whether ~t m computable In contrast to lemcal cohemon, coherence m chfl~cult to zdent|fy mthout complete understandmg of the text and complex reference In ad&tton, there m no prease criteria for clasmficat~on of different relatlous Consider the following example from Hobbs(1978) "John can open the safe He Imows the combmahon " (Morns and H~mt, 1991) show that the relation between these two sentences can he interpreted as daborahon or as ezplanahon, depen&ng on %ontext, knowledge and behefs" There m, however, a close connechon between dincourse structure and cohemon Related words tend to co-occur mthm a dmcourse umt of the text So cohemon m one of the surface mgns of dmcourse structure and lexlcal chaln~ can' be used to Identify it Other mgns can be used to ldentzfy dmcourse structure as well (connect,yes, paragraph markers, tense shifts) In thls paper, we investigate the use oflemcal chains as a model of the source text for the purpose of producing a summary Obviously, other pects of the source text need to be integrated m the text representation to produce quahty summaries, but we want to empmcally investigate how far one can go exploiting mainly lemcal chains In the rest of the paper we first present our algorithm for lexzeal chain construct,on We then present empmcal results on the ldentlficatzon of strong chains among the posmble can&dates produced by our algorithm Finally, we describe how lexlcal chains are used to identify mgmficant sentences mtlnn the source text and eventually produce a surQmary Algorithm for Chain Computing One of the clnef advantages of lemcal cohesmn m that zt m an easdy reco~m~able relatmn, enabhng lexlcal chains computation The first computational model for lemcal chains was presented m (Morns and Hlrst, 1991) They define lexlcal cohesmn relatzons m terms of categories, index entries and pointers m Roget's Thesaurus Morns and Hlrst evaluated that their relatedness criterion covered over 90% of the mtmttve lexzcal relatzons Cham~ are created by taking a new text word and findtng a related chain for it according to relatedness criteria Morns and HLrst introduce the notion of "actzvated chain ~ and ~cham returns", to take into account the dmtance between occurrences of related words They also analyze factors contributing to the strength of a chain -repetltxon, density and length Morns and Hn'st &d not ~nplement their algorithm, because there was no machine-readable vermon of Roget's Thesaurus at the tzme One of the drawbacks of thelr approach was that they chd not reqmre the same word to appear ruth the same sense m ~ts &ffexent occurrences for tt to belong to a chain For semantically ambiguous 11 words, this can lead to confnslous (e g, mixing two senses of taSle as aptece 0f furniture or an array) Note that choosing the appropriate chain for a word is eqmvalent to dzsamblguatmg tins word m context, which is a well-known d~fl~cult problem m text understanding More recently, two algorithms for the calculation of lexlcal chains have been presented m Hirst and StOnge (1995) and Stairmand (1996) Both of these algornthms use the WordNet lexlcal database for determining relatedness of the words (Miller et al, 1990) Senses m the WordNet database are represented relatlonally by synonym sets ('synsets') -which are the sets of all the words sharing a common sense For example two senses of "computer" are represented as {calculator, reckoner, figurer, estimator, computer) (s e, a person who computes) and {computer, data processor, electromc computer, reformation processing system) WordNet contains more than 118,000 dflferent word forms Words of the same category are hnked through semantic relations hke synonymy and hyponymy Polysemous words appear m more than one synsets (for example, comptdcr occurs m two synsets) Approxtmately 17% of the words m WordNet are polysemous But, as noted by Stairmand, this figure is very tmsleadmg "a slguxficant proportion of WordNet nouns are Latin labels for biological entitles, which by their nature are monosemons and our experience wtth the news-report texts we have processed ts that approxtmately half of the nouns encountered are polysemous" (Stairmand, 1996) Generally, a procedure for constructing lexlcal chains follows three steps (1) Select a set of can&date words, (2) For each candldate word, find an appropriate chain relying on a relatedness cute.on among members of the chains, (3) If It is found, insert the word m the chain and update It accorchngly An example of such a procedure was represented by Hlrst and St-Onge (H&S) In the preprocessor step, all words that appear as a noun entry m WordNet are chosen Relatedness of words xs dstermmed m terms of the distance between their occurrences and the shape of the path connecting them m the WordNet thesaurus Three kinds of relation are defined extra-strong (between a word and tts repetxt~on), strong (between two words connected by a Wordnet relatxon) and mechum-stroug when the hnk between the synsets of the words is longer than one (only paths satisfying certain restrictions are accepted as vahd connectxons) The maxtmum distance between related words depends on the kind of relatxon for extra-strong relattons, there is not hxmt m &stance, for strong relatlons, it is hmlted to a window of seven sentences, and for mechum-strong relations, It is wltinn three sentences back To find a chain m winch to insert a given candtdate word, extra-strong relattons are preferred to strong-relations and both of them are preferred to medmm-strong relations If a chain is found, then the candtdate word is inserted with the appropriate sense, and the senses of the other words m the receiving chain are updated, so that every word connected to the new word m the chain relates to Its selected senses only If no chaan is found, then a new chain Is created and the can&date word ts inserted with all its possible senses m WordNet The greedy &samblguatzon strategy Implemented m this algorithm has some lmntatlonsdinstrated by the following example Mr.
C94-1056 P94-1002
. • In snmmary, our algorithm differs from H&S's algorithm m that It introduces, m addition to the relatedness criterion for members~p to a chain, a nongreedy dzsainbiguatlon heuristic to select the appropriate senses of chain members The two algonthms differ m two other major aspects the criterion for the selection of candidate words and the operative defimhon of a text unit We choose as candidate words simple nouns and noun compounds As mentioned above, nouns are the main contributors to the =aboutness" of a text, and noun synsets dominate m WordNet Both (Stairmand, 1996) and H&S rely only on nouns as candidate words In our algorithm, we rely on the results of Brdl's part-of-speech tagging algorithm to idsntlfy nouns, whl\]e H&S do not go through this step and only select tokens that• happen to occur as nouns m WordNet In addition, we extend the set of candidate words to include noan compound We first empmcally evaluated the unportance of noun compounds by taking mto account the noun compounds exphcttly present m WordNet (some 50,000 entries m WordNet are noun compounds such as "sea level" or co\].locatlons 13 (Mr,lms~e¢} ~('MLczq-~__'~ {PC, rmarocomputer, } t Iperso~ Figure 3 Step 3 Interpretation 1 Figure 4 Step 3 Interpretation 2 such as "digital computeff) However, Enghsh includes a productive system of noun comp0hnds, and m each domain, new noun-compounds and collocations not present m WordNet play a major role We addreseed the issue, by usmg a shallow parser (developed by Ido Dagan's team at Bar Ilan Umverslty) to identify noun-compounds using a snnple characterization of noun sequences Tins has two major benefits (1) it ldentflles Important concepts m the domain (for example, m a text on "quantum computing", the mare token was the noun compound ``~uantum computing" winch was not present m WordNet), (2) it chromates words that occur as modn~ere as posmble can&dates for chain membersinp For example, when ``quantum computing" m selected as a smgle umt, the word ``¢uantum ~ is not selected This Is beneficial because m tins example, the text was not about-"quantum', but more about computers When a noun compound ~s selected, the relatedness criterion in WordNet ~s used by cousldermg its head noun only Thus, "quantum computer ~ ~s related to ``machine ~ as a ~computer ~ The second dflfexence m our algorithm hes m the operative defuntion we gwe to the notion of text umt We use as text umts the segments obtained from Hearst's algorithm of text segmentation (Hearst, 1994) We braid chains m every segment according to relatedness criteria, and in a second stage, we merge chains from the dflferent segments using much stronger criteria for connectedness only two chains are merged across a segment boundary only if they contain a common word with the same sense Our mira-segment relatedness criterion.is less strict members of the same synsets are related, a node and its offspnng m the hyperonym graph are related, mbhngs m the hyperonym graph are related only ffthe length of the path m less thana threshold The relation between text segmentation and lexlcal chain is dehcate, since they are both derived from partially common source of knowledge lexlcal &stnbutlon and repetitions In fact, lexlcal chains could serve as a barns for an algorithm for segmentation We have found empmcally, however, that Hearst's algorithm behaves well on the type of texts • we checked and that it prowdes effectively a sohd basLS for lexlcal chains construction Building Summaries Using Lexical Chains We now investigate how lexlcal chains can serve as a source representation of the original text to budd a summary The next question m how to build.summary representation from tins source representation The most prevalent dmcourse topic will play an important role m the summary We first present the mtmtlon why lex~cal chains are a good m&cator of the central topic of a text G!ven an approprnate measure of strength, we show that picking the concepts represented by strong lexlcal chains glves a better mchcatlon of the central toplc of a text than snnply plckmg the most frequent words m the text (which forms the zero-hypothesis) For example, we show m Appendix a sample text about Bayeman Network technology There, the concept of network was represented by the words "network" with 6 occurrences, %ct" with 2, and ``system ~ ruth 4 But the summary representation has to reflect that all these words represent the same concept Otherwise, the summary generation stage would extract information separately for each term The chain representation approach avmds completely this problem, because all tl~ese terms occur m the same chain, winch reflects that they represent the same concept An ad&tlonal argument for the chain representation as opposed to a rumple word frequency model is the case when a tangle concept is represented by a number of words, each with relatively low fTequency In the same Bayesian Network sample text, the concept of "reformat:on" was represented by the words ",nformatson" (3), "datum" (2), "Irnowledge" (3), "concept" (1) and "model" 1 In tins text, "mforma.
P94-1002
who wrote one of the key early texts on 0aye•on networks tn 1988 and has become an unoflu:~d •pokesrcan for the hdd "They ve Ipven a boost to the whole area" M~'osoft m wodtm s on technques that wdl enable the Bayeamn networks to ke~rn or update them•dyes automatu:;dly based on new knowledge 8 task that m currently cumbersome Bayesian Network Text: the Strongest Chain The Criterion t~ 3 ~8, here are the five strong chasn~ OHAIN I Score = 14 0 mzcro~oft 10 concern I company 6 enterta~tment-~ervlce 1 enterprbe 1 ma~ttchu~et t e-mats• ut e 1 ~HA/,N £ Score ffi 9 0 ba~'e~l&n-~y~tem 2 ~y~tem 2 baye~za~s-net 2 network 1 baye~z~n-network 5 weapon 1 : CHAIN 3 Score ---7 0 m 2 a~ttficzal-mtolhgunce /~ field 7 technology 1 t, czence I CHAIN ~ Score ffi 6 O tochmquo 1 b&ye~tsn-techmque I condztzon I datum 2 model I mformatton 3 area I knowledge 3 ~HAIN S Score = 3 0 computer 4 Acknowledgements Tkts work has been supported by the Israeh Mlmstry of Science We axe grateful to Graeme Ktrst, Dragonnr Radev and Claude Bneson for thezr feedback on a previous vermon References Black, Wflham J 1994 Parsing, lmgmstlc resources and semantic analysis, for abstzactmg and categorization Halhday, lqhchael and Ruqatya Hasan 1976 Cohesion in Enghsh Longman, London Hearst, Marti A 1994 Multi-paragraph segmentation of exposltoZT text In Proceedmgs of the 3~nd Annual Meetmg of the Assocmhon for Computational Lmgutshcs HLmt, Graeme and Dared St-Onge 1997 (to appear) Lemca\] chains as representation of context for the detection and correction of malapropisms In Chnstiane Fellbanm, edxtor, WordNet An electmmc leztcal database and some of,ts appheat:ons Cambridge, MA The MIT Press Hoey, M 1991 Patterns of Leats m Tezt Oxford Umvermty Press, Oxford Jones,.KarenSpaxck 1993 What mightbe m summary ? Informahon Retrleval Lulm, H P 1968 The automatxc ereatzon of hterature abstracts In Schultz, edttor, H P Luhn Pioneer of lnformahon Science Spartan Mann, W C and S Thompson 1987 RhetoncaJ.
P94-1002
