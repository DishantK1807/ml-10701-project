A Quasi-Dependency Model for Structural Analysis 
of Chinese BaseNPs* 
Zhao Jun Huang Changning 
Department of Computer Science & Technology, 
The State Key Lab of Intelligent Technology & Systems, 
Tsinghua University, Beijing, China, 100084 
Email: _zj@s1000e.cs.tsinghua.edu.cn, Hcn@tsinghua.edu.cn 
Abstract: The paper puts forward a quasi
dependency model for structural analysis of Chinese 
baseNPs and a MDL-based algorithm for quasi
dependency-strength acquisition. The experiments 
show that the proposed model is more suitable for 
Chinese baseNP analysis and the proposed MDL
based algorithm is superior to the traditional ML
based algorithm. The paper also discusses the 
problem of incorporating the linguistic knowledge 
into the above statistical model. 
1. Introduction 
The concept of baseNP is initially put forward 
by Church. In English, baseNP is defined as 
'simple non-recursive noun phrases', which means 
that there is no sub-noun-phrases contained in a 
baseNP\[1\]. But the definition can not meet the 
needs in Chinese information retrieval. The noun 
phrases such as "n~(natural) ~(language) 
~-(process)", "~-\[E~\](Asian) _~\[l(finance) :f~ 
})L(crisis)" and "i~(political) ~fJlJ(system) 
~(reformation) ~(process)" are critical for 
information retrieval, but they are not non
recursive noun phrases. 
In Chinese, the attribute of noun phrases can be 
classified into three types, that is restrictive 
attributes, distinctive attributes and descriptive 
attributes, among which the restrictive attributes 
have agglutinative relation with the heads. The 
paper defines the Chinese baseNP using the 
restrictive attributes. 
\[ Definition 1\] Chinese baseNP (hereafter 
abbreviated as baseNP): 
baseNP -~ baseNP + baseNP 
baseNP ~ baseNP + N \] VN 
baseNP --* restrictive-attribute + baseNP 
baseNP -* restrictive-attribute + N I VN 
restrictive-attribute ~ A I B I V IN I S IX I 
(M+Q) 
Where, the terminal symbols A, B, V, N, VN, S, X, 
M, Q stand for respectively adjective, distinctives, 
verbs, nouns, norminalized verbs, locatives, non
Chinese string, numerals and quantifiers. 
According to the definition, noun phrases 
falls into baseNPs and non-baseNPs (abbreviated 
as -baseNP). Table-1 gives some examples. 
Table1 • Examples of baseNP and ~baseNP 
Type Examples 
BaseNP ~ 00/air ~lN/corridor 
BaseNP i~ N/politics ~k$1J/system ~:~/reform .... 
BaseNP ~ El/export ~ m/commodity ~}:~r/price ~u~H ~/index 
baseNP _~g..~,/complicated m/de ~tt\]~/feature 
baseNP ~/research -~/and )~.~/development 
baseNP :~l)iTi/teacher q/write m/de \]2~N/comment 
Both baseNP recognition and baseNP structural 
analysis are basic tasks in Chinese information 
retrieval. The paper mainly discusses the problems 
in structural analysis of baseNPs, which is 
essential for generating the compositional 
indexing units from a baseNE The task of baseNP 
" The research is supported by the key project of the National Natural Science Foundation 
1 
structural analysis is to determine the syntactic the dependency relation between two constituents 
structure of a baseNP. In this paper, we use is composed of two steps. The first step is to 
dichotomy for baseNP analysis. For example, the determine whether they have the possibility to 
structure of "~1 ,~,~/natural "~-~/language ~. constituent dependency relation. The second step 
/process" is "(N~/natural ~/language) ~3~is to determine whether they have dependency 
/process". Obviously, a baseNP composed of three relation in the given context. The former is called 
or more than three words has syntactic ambiguities, the quasi-dependency-relation, which can be 
For example, baseNP "x y z" has two possible acquired from collocation dictionaries or corpora. 
structures, that is "(x y) z" and "x (y z)". The task The determination of the latter is difficult, because 
of baseNP structural analysis is to select the multiple information in the given context should 
correct structure from the possible structures, be taken into consideration, such as syntax Or 
The paper mainly discusses the problems semantics information, etc. 
related to Chinese baseNP structural analysis. \[Definition 2\] Quasi-Dependency-Relation: If 
Section 2 puts forward a quasi-dependency model two words x and y have the possibility to 
for structure analysis of Chinese baseNPs. Section constituent dependency relation, then we say that 
3 gives an unsupervised quasi-dependencythey have quasi-dependency-relation in the given 
strength estimation algorithm based on the baseNP, formulated as x-"y (where y is called the 
minimum description length (MDL) principle, head) or y~x (where x is called the head); 
Section 4 analyzes the performance of the Otherwise, we say that they have no quasi 
proposed model and the algorithm. Section 5 
discusses some issues in the implementation of dependency relation, formulated as x-~ y and 
baseNP structure analysis and quasi-dependencyY -~ x. 
strength estimation. Section 6 is the conclusion. \[Assumption 1 \] In a Chinese baseNP, if two 
words x and y can constituent dependency relation, 2. The quasi-dependency model 
then the head is always the post-positon word y, 
There are two kinds of structural analysis that isx-*y. 
models for Eng!ish noun phrase, that is adjacency According to the Definition 1, there is no 
model and dependency model. The research of preposition phrase, verb phrase, locality phrase or 
Lauer shows that the dependency model is (l~l)-structure in a baseNP, so assumption-I is 
superior to the adjacency model for structural reasonable. 
analysis of English noun phrase\[2\]. However, On the basis of assumption-l, we put forward 
there is no model for structural analysis of Chinese the quasi-dependency model for structural analysis 
baseNP till now. of Chinese baseNPs. 
According to the dependency grammar, two There are the following 3 kinds of quasi
constituents can be bound together they are dependency-pattern for a tri-word-composed 
determined to be dependent. The determination of baseNP xyz. 
y z y z x z 
Y s31 Y s32 Y s33 
s3~ =(x y) z s32=x (v z) 
/reform", there are quasi-dependency-relations "i!~ Where, pattern s31 means x~y, y~z andx -~ z, 
which corresponds to structure (x y)z; pattern s32 ~/politics~/e~k~fJ/system '', "i~/politics--~ 
means x-'z, y~z andx ~ y, which corresponds /reform" and "/VT~,~lJ/system~Y~-/reform". If we 
to the structure x (y z); However, the quasiknow that the quasi-dependency-relations "i~ 
dependency-strength must be used to determine /politics~/~k~lJ/system '' and "~lJ/system--,-~ 
the corresponding structure for pattern s33, which /reform" are stronger than "i~/politics~ 
means x~y, y~z and x--'z. For example, as for /reform", the structure of the baseNP can be 
baseNP " i~ ~ /politics /e~ ~rJ/system ~ ~ determined to "(~/politics /e~k~lJ/system) ~ 
/reform". 
In the following, we give the definition of 
quasi-dependency-strength and the formula for 
determining the syntactic structure of baseNPs 
based on the quasi-dependency-strengths. 
\[ Definition 3 \] quasi-dependency-strength: Given 
a baseNP set NP={npi,npz,...,npM} and lexicon 
I~={MJI,...,WM} , VWi, Wj e W , the quasi
dependency-strength of w~ -~ u~. is defined as: 
Z dep(w i --q, w j, npk ) 
np ~. c Nf' 
ds(w i -~ w./) = Z c°(wi ~ w j, npk ) 
np~ • NP 
where dep(w i ~ wj,npk ) is the count of 
dependent word pair w~u~ contained in np~, 
co(w~,w.i,npk) is the count of cooccurent word 
pair (w,, wj) contained in np~. 
The formula for determining the syntactic 
structure of baseNP based on the quasi
dependency-strengths is as follows. 
d~(u ~ v) 
(u.--~v)ED(np~ ,sj ) belief (s j I ) np~ 
d~(, -. ~) + ~ d~(,, -~ v) 
(u--~v)eD(npi ,s i ) (u--~v)~D(npi ,s j ) 
Where, belief(sj \[npi) represents the belief in 
which the structure of np~ is sj. D(np~,sj) represents 
the set of quasi-dependency-relations included in 
the quasi-dependency-pattern corresponding to 
structure sj. 
A tri-word-composed baseNP has two possible 
syntactic structures, that is s3~ and $32. Similarly, a 
four-word-composed baseNP has the following 
five possible structures. 
w 
x 
y 
x y z x y z x y z x y z x y z 
x x wE x x wFZ., x ×., wEx 
4 X
x X d x 4 X x d X x X d 
Y Y Y Y 
$41 = ((wx)y)z 
In summary, we can compute the belief in 
which the structure of np~ is S/ using the 
correspondence between the quasi-dependency
pattern and the baseNP structure. The acquisition 
of quasi-dependency-strength between words is 
the critical problem. 
3. The acquisition of quasi
dependency-strength between words 
If we have a large scale baseNP annotated 
corpus in which the baseNPs have been assigned 
the syntactic structures, the quasi-dependency
strength between words can be acquired through a 
simple statistics. However, such an annotated 
corpus is not available. We only have a baseNP 
corpus which has no structural information. How 
to acquire the quasi-dependency-strength from 
such a corpus is the main task of the section. 
Given a baseNP set NP={np~,np2,...,npM} and a 
lexicon W={Wl,W2,...,wM}, the problem can be 
described as learning a quasi-dependency-strength 
set G (abbreviated as model) from the training set. 
Where, G= {dso. I dsij :ds( w i ---> wj ), Vwi , w j e W} 
$42 = (WX)(yz) $43 = (w(xy))z $44 = W((Xy)Z) S45 = W(X(yZ)) 
Zhai Chengxiang puts forward an unsupervised 
algorithm for acquiring quasi-dependency-strength 
from noun phrase set\[3\]. The algorithm is derived 
from the EM algorithm. Because the algorithm is 
based on the maximum likelihood (ML) principle, 
it usually leads to overfitness between the data and 
the model\[4\]. For example, given a simple baseNP 
set NP={i~/politics ~k~\]tJ/system ~/reform, 
~;zff/economics ~k~\[~lJ/system ~/reform, ~(~ 
/politics ~ ~\[\]lJ/system ~ @/revolute , ~ 
/economics/g~:~IJ/system ~/revolute}, there are 
sixteen possible models for the training set, among 
them G4, G7, Gl0 and Gl3 have the best fitness to 
NP, that is Num(NPIG)=6. However, in the 
linguistic view, Gi is the correct model, though it 
has lower fitness to NP, that is Num(NPIG)=4 (see 
the appendix). 
3.1 The
estimation of the quasi-dependency
strength under Bayesian framework 
In Bayesian framework, the task of acquiring 
the quasi-dependency-strength can be described as 
the problem of selecting G which has the highest 
posterior probabilityp(G INP). 
a = arg max p(G I NP) 
G 
According to Bayesian theorem, we have the 
following inference. 
G = arg max p(UP \[ G)p(G) 
a p(NP) 
= arg max p(NP \[ G)p(G) 
G 
Besides using conditional probability p(NPIG) 
to measure the fitness between the training set and 
the model G, Bayesian modeling gives additional 
consideration to the generality of the model 
through the prior probability p(G), that is simpler 
model has higher probability. The central idea of 
Bayesian modeling is to find a compromise 
between the goodness of fit and the simplicity of 
the model. 
3.2 Defining
the evaluation function of Bayesian 
modeling using MDL principle 
The difficulty in Bayesian modeling is the 
estimation of the prior probabilityp(G). According 
to the coding theory, the lower bound of the 
coding length (bit-string) of an information with 
probability p is log 2 l/p\[5\]. The theorem 
connects Bayesian modeling with the MDL 
principle in the coding theory. 
a = arg max p(NP \[ G)p(G) 
G 
-arg rain 1-log~ \[p(gP I G)p(G)\]} 
G 
1 1 = arg min {log2 p(NP\[G) + l°g2 p~} 
G 
= arg min {L(NPIG) + L(G)} 
G 
Where, L(ct) is the optimal coding length of 
information a. Specially, L(NPIG) is called the 
data description length and L(G) is called the 
model description length. 
Therefore, the problem of estimating the prior 
probability p(G) and the conditional probability 
p(NPIG) is converted to the problem of estimating 
the model description length L(G) and the data 
description length L(NPIG). 
3.3 The
MDL-based quasi-dependency-strength 
estimation algorithm 
In MDL principle, the modeling problem can be 
viewed as a problem of finding a model G which 
has the smallest sum of the data description length 
and the model description length. Because the 
search space is huge, we can not find the optimal 
model in a transversal manner. The model must be 
improved in an iterative manner in order to arrive 
at a minimum description length. 
In the research, the model is composed of the 
quasi-dependency-strength ds(w i --> w j), where 
each ds(w i ~ wj) can be decomposed into two 
parts: C)the structure part: the quasi-dependency
relation (w i ~ wj); (~)the parameter part: the 
quasi-dependency-strength ds. Therefore, the 
learning process is divided into two steps: (~) 
Keeping the structure part fixed, optimize the 
parameter part; (~)Keeping the parameter part 
fixed, optimize the structure part. The two steps go 
on alternately until the process arrives at a 
convergent point. 
Algorithm 1: The MDL-based algorithm for quasi-dependency-strength estimation 
Olnitialize model G; 
QLet L = L( NP \[ G) + L( G), G = ( G s, Gp) ,where Gs and G: represent respectively the structure part and the 
parameter part. Execute the following two steps alternately, until L converged. 
• Keeping Gs fixed, optimize Gp, until L(NP I G) converges, that is L converges; 
• Keeping Gp fixed, optimize G.,, until L(G) converges, that is L converges. 
On condition that the structure part of the model 
is fixed, the parameter optimization means to find 
the optimal sets of quasi-dependency-strength in 
order that the data description length minimized, 
4 
that is 
c = arg rain L(N? F6) 
G 
Where L(NP\[G) is the optimal coding length of NP 
when G is known. 
The parameter optimization step can be 
implemented using EM algorithm\[3\]. In the 
process of parameter optimization, the structure 
part of the model is kept fixed. The optimum 
estimates of the parameters are obtained through 
Algorithm 2: The structure optimization algorithm 
the gradual reduction of data description length. 
In MDL principle, the model description length 
can be gradually reduced through the modification 
of the structure part of the'. model, therefore the 
overall description length of the model is reduced. 
Let the model after the parameter optimization process is G, which is composed of the quasi
dependency-strength ds( w i --+ %). 
(l~Sort the quasi-dependency-strengths of model G in ascending order, that is ds Itl, ds r'l, ds TM, . ..... ; 
~2~Repeat the following steps, until \[L(NP\]G'} + L(G')\]-\[L(NPIG} +L(G)\] <= Th L (ThL is the selected 
threshold). Let i=1, 
• Delete the quasi-dependency-strength ds tq from model G; 
• Construct the new model G'; 
• If \[L(NPIG')+L(G')\]-\[L(NPIG)+L(G)\]<=Th~. Then the cycle ends Else let G=G', i=i+1 arid 
continue the next cycle. ,1. The performance analysis 
This section takes the N2+N2+N2-type (where N2 
represents bi-syllable noun) baseNPs as the testing 
data in order to discuss the performance of the quasi
dependency-based model for structural analysis of 
baseNPs and the MDL-based algorithm for quasi
dependency-strength acquisition. The training set 
includes 7,500 N2+N2+N2-type baseNPs. The close 
testing set is the 500 baseNPs included in the 
training set. The open testing set is the 500 baseNPs 
outside the training set. The testing target is the 
precision of baseNP structural analysis, that is 
a 
precision = -× 100%; b 
Where a is the count of the baseNPs which are 
correctly analyzed, b is the count of the baseNPs in 
lhe tesing set. ,1.1 The performance of the quasi-dependency 
model 
The experiments shows: (~)In the N2+N2+N2
type baseNPs, the left-binding structure is about two 
times of the right-binding structure; (~)The analysis 
precision of the quasi-dependency model is about 
7% higher than that of the adjacency model. This 
conclusion can be explained intuitively through the 
following example. The structure of baseNP "t~-~ 
/doctor ~/dissertation ~/outline" can not be 
correctly determined through the adjacency model, 
because we can not find that the dependency strength 
of "t~-):/doctor ~3~/dissertation" is stronger than 
that of"J~3~/dissertation -~~/outline". In the other 
hand, the structure of the above baseNP can be 
determined to "(t~/doctor ~/dissertation) ~
ffq/outline" through the quasi-dependency model, 
because both "|~:\[:./doctor :L~3~/dissertation" and " 
~3~:/dissertation ~-~ff~J/outline" are dependent word 
pairs, while " t~ ~/doctor' {~ ~/outline" is an 
independent word pair. Table 2 is the testing result. 
Table-2: The analysis precision ofN2+N2+N2-type baseNP 
Testing type Right-binding 
Open test 
Close test 31.5% 
32.7% 
Left-binding Adjacency model Quasi-dependency model 
68.5% 84.6% ,91.5% 
67.3% 815% 88.7% 
4.2 The
performance of the MDL-based 
algorithm for quasi-dependency-strength 
acquisition 
The ML algorithm is equivalent to the first 
parameter optimization process of the MDL 
algorithm. The MDL process is composed of two 
iterative optimization steps. In the iterative process, 
the parameters are optimized gradually and the 
model is simplified gradually as well. Therefore, the 
overfitness problem inherent in the ML algorithm is 
solved to a great extent. In the following, the 
performance of the ML algorithm and the MDL 
algorithm are compared through comparing the 
baseNP analysis precision of the models constructed 
using the above two algorithms. The precision is 
listed in Table-3. The experiment shows that the 
MDL algorithm is superior to the ML algorithm. 
Table-3: The performance of ML algorithm and MDL algorithm 
BaseNP analysis precision 
Close test 
5. Implementation issues 
ML algorithm MDL algorithm 
89.0% 91.5% 
The most difficult problem related to the 
structural analysis of baseNPs is the acquisition of 
the quasi-dependency-strength. The proposed 
algorithm(Algorithm 2) is an unsupervised 
algorithm, that is the parameters are estimated 
over the baseNP corpus which has no structural 
information. In order to improve the estimation 
results and speed up the iteration process, some 
measures are taken during the implementation. 
5.1 The
pre-assignment of the baseNP structure 
The structures of some baseNPs can be 
determined using the linguistic knowledge. Such 
knowledge includes: 
(~) In a baseNP, a word pair which has the 
following syntactic composition is independent. 
• Noun+Adjective: for example, " :1~ ~\]~ 
/ground/Noun :~jl¢ ~complicated~Adjective ~ 
/condition", "~\[\[~till/glass/Noun ~;~/curved/Adjective 
~/pipe"; 
• Noun+Distinctive: for example, " pJ~ 
/elementary-school/Noun ~ l~ /of-the-right
age/Distinctive ) L~-~/chiid"; 
• Distinctive+Verb: for example, " ~k: ~_ 
/large/Distinctive ~l~ ~1~/fightNerb ~ ~()l/plane", 
"~,l~\[/elementary/Distinctive )~j:/creep/Verb ~3 
W/animal". 
(~) If two verbs cooccur in a baseNP, then they are 
dependent. For example," (~J~l~/prospectNerb 
/design/Verb ) -~ ~/group ", " ( ~\[~ El/Anti
Japanese/Verb ~\]3l/save-the-nation/verb) ~ 
/campaign". 
If we preprocess the baseNP corpus using the 
Open test 
ML algorithm t MDL algorithm 
82.5% I 88.7% 
above knowledge, it is beneficial for the estimation 
process. 
5.2 The
complex-feature-based modeling 
If the lexicon size is \[W\], then the parameter 
number of the above word-based acquisition 
algorithm amounts to \[W\] 2. The enormous parameter 
space will lead to the data sparseness problem during 
the estimation. Therefore, the paper puts forward the 
complex-feature-based acquisition algorithm. First, 
map each word to a complex-feature-set according to 
the multiple feature of the words; Then, acquire the 
quasi-dependency-strength between the complex
feature-sets. During analyzing the structure of a 
baseNP, the strength between the complex-feature
sets is used instead of that between the words. In the 
research, the multiple features include part-of-speech, 
number of syllables and word sense categories. 
6. Conclusions 
The paper put forward a quasi-dependency model 
for structural analysis of Chinese baseNPs, and a 
MDL-based algorithm for the quasi-dependency
strength acquisition. The experiments show that the 
proposed model is more suitable for Chinese baseNP 
analysis and the proposed MDL-based algorithm is 
superior to the traditional ML-based algorithm. The 
further research will focus on incorporating more 
linguistic knowledge into the above statistical model. 

References 

\[1\] Church K., A stochastic parts program and  noun phrase parser for unrestricted text, In:  Proceedings of the Second Conference on Applied  Natural Language Processing, 1988. 

\[2\] Lauer M. Conceptual association for compound  noun analysis, In: Proceedings of the 32 "~ Annual  Meeting of the Association for Computational  Linguistics, Student Session, Las Cruces, NM,  1994. 

\[3\] Zhai Chengxiang, Fast Statistical Parsing of  Noun Phrases for Document Indexing, In:  Proceedings of the 35 t~ Annual Meeting of the  Association for Computational Linguistics, USA.:  Association for Computational Linguistics. 1997.  311-318. 

\[4\] Stolcke A. Bayesian learning of probabilistic  language models, Dissertatiion for Ph.D. Degree,  Berkeley, California: University of California,  1994. 

\[5\] Solomonoff R. The meclhanization of linguistic  learning, In: Proceedings of the 2nd International  Conference on Cybernetics.  

