<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>R Bowden</author>
<author>D Windridge</author>
<author>T Kadir</author>
<author>A Zisserman</author>
<author>M Brady</author>
</authors>
<title>A linguistic feature vector for the visual interpretation of sign language</title>
<date>2004</date>
<booktitle>In European Conf. Computer Vision</booktitle>
<volume>1</volume>
<pages>390--401</pages>
<contexts>
<context>ure recognition using small vocabularies. Most databases used in sign language processing so far do not provide or include what is important for the evaluation of sign language processing algorithms (Bowden et al., 2004; Martinez et al., 2002). An overview of available language resources for sign language processing and especially recognition is presented in (Zahedi et al., 2006). Recently an Irish sign language dat</context>
</contexts>
<marker>Bowden, Windridge, Kadir, Zisserman, Brady, 2004</marker>
<rawString>R. Bowden, D. Windridge, T. Kadir, A. Zisserman, and M. Brady. 2004. A linguistic feature vector for the visual interpretation of sign language. In European Conf. Computer Vision, volume 1, pages 390–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Dreuw</author>
<author>T Deselaers</author>
<author>D Rybach</author>
<author>D Keysers</author>
<author>H Ney</author>
</authors>
<title>Trackingusingdynamicprogrammingfor appearance-based sign language recognition</title>
<date>2006</date>
<booktitle>In IEEE Intl. Conf. on Automatic Face and Gesture Recognition</booktitle>
<pages>293--298</pages>
<location>Southampton</location>
<contexts>
<context> (Erdem and Sclaroff, 2002), recognition of facial expressions (Vogler and Goldenstein, 2007), hand tracking and recognition of hand shapes and movements (Vogler and Metaxas, 2004; Yuan et al., 2005; Dreuw et al., 2006), as well as categorization of signs (Zahedi et al., 2005; Tsechpenakis et al., 2006). Both data sets can be used for the training and evaluation of sign language recognition algorithms, as well as d</context>
<context>atistical machine translation on the recognizer output have been presented in (Stein et al., 2007). We achieve a 2.30% tracking error rate for a 20×20 search window on the RWTH-BOSTON-Hands database (Dreuw et al., 2006). Movement epenthesis refers to movements that occur in natural sign languages when the location in the signing space changes between one sign and the next. In this work, Dreuw et al. added special l</context>
</contexts>
<marker>Dreuw, Deselaers, Rybach, Keysers, Ney, 2006</marker>
<rawString>P. Dreuw, T. Deselaers, D. Rybach, D. Keysers, and H.Ney. 2006. Trackingusingdynamicprogrammingfor appearance-based sign language recognition. In IEEE Intl. Conf. on Automatic Face and Gesture Recognition, pages 293–298, Southampton, April.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Dreuw</author>
<author>D Rybach</author>
<author>T Deselaers</author>
<author>M Zahedi</author>
<author>H Ney</author>
</authors>
<marker>Dreuw, Rybach, Deselaers, Zahedi, Ney, </marker>
<rawString>P. Dreuw, D. Rybach, T. Deselaers, M. Zahedi, and H. Ney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>2007a</author>
</authors>
<title>Speech recognition techniques for a sign language recognition system</title>
<date>2007</date>
<booktitle>In Interspeech</booktitle>
<pages>2513--2516</pages>
<location>Antwerp, Belgium</location>
<marker>2007a, 2007</marker>
<rawString>2007a. Speech recognition techniques for a sign language recognition system. In Interspeech 2007, pages 2513–2516, Antwerp, Belgium, August. ISCA best student paper award of Interspeech 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Dreuw</author>
<author>D Stein</author>
<author>H Ney</author>
</authors>
<title>Enhancing a sign language translation system with vision-based features</title>
<date>2007</date>
<booktitle>In International Workshop on Gesture in HumanComputer Interaction and Simulation</booktitle>
<pages>18--20</pages>
<location>Lisbon, Portugal</location>
<contexts>
<context>cently, this database has been extended by Dreuw and colleagues with pronunciation information (see Section 5.2.), and has been used successfully for continuous sign language recognition experiments (Dreuw et al., 2007a). The corpus statistics and language model perplexities are shown in Table 1 and Table 4. 4.3. RWTH-BOSTON-400 Many different information tiers that are of interest for linguistic research are avail</context>
<context> have been annotated manually in 15 videos. 1119 frames in total are annotated. 5. System Overview &amp; Features We give a short overview of the recognition framework and the used features presented in (Dreuw et al., 2007a). 5.1. Visual Modeling Phonological analysis going back to (Stokoe et al., 1965) has revealed that signs are made up out of basic articulatory units, initially referred to as cheremes by Stokoe, now</context>
<context> is tracked in each image sequence. Given the hand position ut = (x,y) at time t in signing space, features such as hand velocity mt = ut − ut−δ can easily be extracted. The hand trajectory features (Dreuw et al., 2007a) are similar to the features presented in (Vogler and Metaxas, 2001). 5.2. Pronunciation Handling Signed languages exhibit dialectal variation comparable to that found in spoken languages. Thus, one</context>
<context>ntly best known word-error-rate of 17.2% WER has been reported in (Zahedi et al., 2005). Figure 2 shows the effect of using different n-gram language models and scales on the RWTHBOSTON-104 database (Dreuw et al., 2007a). As in ASR, the language model adaptation by using sign language pronunciations achieves large improvements (the currently best known word-error-rate is 17.8% WER). Interestingly, the improvement f</context>
<context>he earlier RWTH-BOSTON-104 corpus annotations, Dreuw and colleagues added pronunciation information to the (preliminary version of the) Boston annotations and adapted the language models accordingly (Dreuw et al., 2007a). The new RWTH-BOSTON-400 corpus makes use of the newly released, fully verified, Boston NCSLGR annotations (Neidle, 2002b; Neidle, 2007). (The final verification of those annotations took some time</context>
</contexts>
<marker>Dreuw, Stein, Ney, 2007</marker>
<rawString>P. Dreuw, D. Stein, and H. Ney. 2007b. Enhancing a sign language translation system with vision-based features. In International Workshop on Gesture in HumanComputer Interaction and Simulation, pages 18–20, Lisbon, Portugal, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U M ErdemandS Sclaroff</author>
</authors>
<title>Automaticdetectionof relevant head gestures in American Sign Language communication</title>
<date>2002</date>
<booktitle>In International Conf. on Pattern Recognition (ICPR</booktitle>
<volume>1</volume>
<pages>460--463</pages>
<marker>Sclaroff, 2002</marker>
<rawString>U.M.ErdemandS.Sclaroff. 2002. Automaticdetectionof relevant head gestures in American Sign Language communication. In International Conf. on Pattern Recognition (ICPR), volume 1, pages 460–463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klakow</author>
<author>J Peters</author>
</authors>
<title>Testing the correlation of word error rate and perplexity. Speech Communication</title>
<date>2002</date>
<pages>38--19</pages>
<contexts>
<context>guage pronunciations achieves large improvements (the currently best known word-error-rate is 17.8% WER). Interestingly, the improvement factors achieved are similar to those from speech recognition (Klakow and Peters, 2002). Preliminary results for statistical machine translation on the recognizer output have been presented in (Stein et al., 2007). We achieve a 2.30% tracking error rate for a 20×20 search window on the</context>
</contexts>
<marker>Klakow, Peters, 2002</marker>
<rawString>D. Klakow and J. Peters. 2002. Testing the correlation of word error rate and perplexity. Speech Communication, 38:19–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backing-off for mgram language modeling</title>
<date>1995</date>
<booktitle>In IEEE ICASSP</booktitle>
<volume>1</volume>
<pages>49--52</pages>
<location>Detroit, MI</location>
<contexts>
<context>lts for different LMs and scales on the RWTH-BOSTON-104 database using the SRILM toolkit. final sign in a question. Furthermore, sentence boundaries and unknown words are also labeled and handled by (Kneser and Ney, 1995) discounting with interpolation in the LM. Preliminary results for movement epenthesis in another subset of the database have been presented in (Yang et al., 2007). With pronunciation information, mo</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>R. Kneser and H. Ney. 1995. Improved backing-off for mgram language modeling. In IEEE ICASSP, volume 1, pages 49–52, Detroit, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M Martinez</author>
<author>R B Wilbur</author>
<author>R Shay</author>
<author>A C Kak</author>
</authors>
<title>Purdue RVL-SLLL ASL database for Automatic Recognition of American Sign Language</title>
<date>2002</date>
<booktitle>In IEEE Int. Conf. on Multimodal Interfaces</booktitle>
<location>Pittsburg, PA, USA</location>
<contexts>
<context> small vocabularies. Most databases used in sign language processing so far do not provide or include what is important for the evaluation of sign language processing algorithms (Bowden et al., 2004; Martinez et al., 2002). An overview of available language resources for sign language processing and especially recognition is presented in (Zahedi et al., 2006). Recently an Irish sign language database has been released</context>
</contexts>
<marker>Martinez, Wilbur, Shay, Kak, 2002</marker>
<rawString>A. M. Martinez, R. B. Wilbur, R. Shay, and A. C. Kak. 2002. Purdue RVL-SLLL ASL database for Automatic Recognition of American Sign Language. In IEEE Int. Conf. on Multimodal Interfaces, Pittsburg, PA, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Neidle</author>
<author>S Sclaroff</author>
<author>V Athitsos</author>
</authors>
<title>SignstreamTM: A tool for linguistic and computer vision research on visual-gestural language data</title>
<date>2001</date>
<journal>Behavior Research Methods, Instruments, and Computers</journal>
<volume>3</volume>
<contexts>
<context>deo formats, along with linguistic annotations that have been carried out in conjunction with the American Sign Language Linguistic Research Project (ASLLRP) at Boston University, using SignStreamTM (Neidle et al., 2001; Neidle, 2002a)2. The annotations, available as SignStreamTM files and in XML format, include indication of the start and end points of linguistically significant behaviors, including individual sign</context>
</contexts>
<marker>Neidle, Sclaroff, Athitsos, 2001</marker>
<rawString>C. Neidle, S Sclaroff, and V. Athitsos. 2001. SignstreamTM: A tool for linguistic and computer vision research on visual-gestural language data. Behavior Research Methods, Instruments, and Computers, 3(33):311–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Neidle</author>
</authors>
<title>SignstreamTM: A database tool for research on visual-gestural language</title>
<date>2002</date>
<journal>Journal of Sign Language and Linguistics</journal>
<volume>1</volume>
<contexts>
<context>th linguistic annotations that have been carried out in conjunction with the American Sign Language Linguistic Research Project (ASLLRP) at Boston University, using SignStreamTM (Neidle et al., 2001; Neidle, 2002a)2. The annotations, available as SignStreamTM files and in XML format, include indication of the start and end points of linguistically significant behaviors, including individual signs, produced by</context>
<context>logical annotation. 3The research described here that has been carried out at Boston University has been supported in part by grants from the National Science Foundation (HCC0705749, CNS-04279883, (Neidle, 2002b; Neidle, 2007) and available for download4. These data are distributed in a couple of ways: (1) The video files and SignStreamTM annotations are available on CD-ROM; six new CD-ROMs, containing 15 s</context>
<context>f the) Boston annotations and adapted the language models accordingly (Dreuw et al., 2007a). The new RWTH-BOSTON-400 corpus makes use of the newly released, fully verified, Boston NCSLGR annotations (Neidle, 2002b; Neidle, 2007). (The final verification of those annotations took some time, largely because of the importance of enforcing such oneto-one sign-gloss correspondences.) Preliminary results on the lar</context>
</contexts>
<marker>Neidle, 2002</marker>
<rawString>C. Neidle. 2002a. SignstreamTM: A database tool for research on visual-gestural language. Journal of Sign Language and Linguistics, 1/2(4):203–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Neidle</author>
</authors>
<title>SignstreamTMannotation: Conventions used for the American Sign Language Linguistic Research Project</title>
<date>2002</date>
<tech>Technical Report 11</tech>
<institution>Boston University</institution>
<contexts>
<context>th linguistic annotations that have been carried out in conjunction with the American Sign Language Linguistic Research Project (ASLLRP) at Boston University, using SignStreamTM (Neidle et al., 2001; Neidle, 2002a)2. The annotations, available as SignStreamTM files and in XML format, include indication of the start and end points of linguistically significant behaviors, including individual signs, produced by</context>
<context>logical annotation. 3The research described here that has been carried out at Boston University has been supported in part by grants from the National Science Foundation (HCC0705749, CNS-04279883, (Neidle, 2002b; Neidle, 2007) and available for download4. These data are distributed in a couple of ways: (1) The video files and SignStreamTM annotations are available on CD-ROM; six new CD-ROMs, containing 15 s</context>
<context>f the) Boston annotations and adapted the language models accordingly (Dreuw et al., 2007a). The new RWTH-BOSTON-400 corpus makes use of the newly released, fully verified, Boston NCSLGR annotations (Neidle, 2002b; Neidle, 2007). (The final verification of those annotations took some time, largely because of the importance of enforcing such oneto-one sign-gloss correspondences.) Preliminary results on the lar</context>
</contexts>
<marker>Neidle, 2002</marker>
<rawString>C. Neidle. 2002b. SignstreamTMannotation: Conventions used for the American Sign Language Linguistic Research Project. Technical Report 11, Boston University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Neidle</author>
</authors>
<title>SignstreamTMannotation: Addendum to conventions used for the American Sign Language Linguistic Research Project</title>
<date>2007</date>
<tech>Technical Report 13</tech>
<institution>Boston University</institution>
<contexts>
<context>ion. 3The research described here that has been carried out at Boston University has been supported in part by grants from the National Science Foundation (HCC0705749, CNS-04279883, (Neidle, 2002b; Neidle, 2007) and available for download4. These data are distributed in a couple of ways: (1) The video files and SignStreamTM annotations are available on CD-ROM; six new CD-ROMs, containing 15 short narratives</context>
<context>nnotations and adapted the language models accordingly (Dreuw et al., 2007a). The new RWTH-BOSTON-400 corpus makes use of the newly released, fully verified, Boston NCSLGR annotations (Neidle, 2002b; Neidle, 2007). (The final verification of those annotations took some time, largely because of the importance of enforcing such oneto-one sign-gloss correspondences.) Preliminary results on the large RWTH-BOSTON-</context>
</contexts>
<marker>Neidle, 2007</marker>
<rawString>C. Neidle. 2007. SignstreamTMannotation: Addendum to conventions used for the American Sign Language Linguistic Research Project. Technical Report 13, Boston University, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Stein</author>
<author>P Dreuw</author>
<author>H Ney</author>
<author>S Morrissey</author>
<author>A Way</author>
</authors>
<title>Hand in hand: Automatic sign language to speech translation</title>
<date>2007</date>
<booktitle>In 11th Conference on Theoretical and MethodologicalIssuesinMachineTranslation(TMI-07</booktitle>
<pages>214--220</pages>
<location>Sk¨ovde, Sweden</location>
<contexts>
<context> An overview of available language resources for sign language processing and especially recognition is presented in (Zahedi et al., 2006). Recently an Irish sign language database has been released (Stein et al., 2007). Here we focus on benchmark databases that can be used for investigating linguistic problems, and evaluating automatic sign language recognition systems or statistical machine translation systems. T</context>
<context>ement factors achieved are similar to those from speech recognition (Klakow and Peters, 2002). Preliminary results for statistical machine translation on the recognizer output have been presented in (Stein et al., 2007). We achieve a 2.30% tracking error rate for a 20×20 search window on the RWTH-BOSTON-Hands database (Dreuw et al., 2006). Movement epenthesis refers to movements that occur in natural sign languages</context>
<context> was presented. Promising results on the publicly available benchmark database RWTH-BOSTON-104 have been achieved for automatic recognition (Dreuw et al., 2007a) and translation (Dreuw et al., 2007b; Stein et al., 2007) that can be used as baseline reference for other researchers. However, the preliminary results on the larger RWTH-BOSTON-400 database show the limitations of the proposed framework and the need for </context>
</contexts>
<marker>Stein, Dreuw, Ney, Morrissey, Way, 2007</marker>
<rawString>D. Stein, P. Dreuw, H. Ney, S. Morrissey, and A. Way. 2007. Hand in hand: Automatic sign language to speech translation. In 11th Conference on Theoretical and MethodologicalIssuesinMachineTranslation(TMI-07), pages 214–220, Sk¨ovde, Sweden, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Stokoe</author>
<author>D Casterline</author>
<author>C Croneberg</author>
</authors>
<title>A Dictionary of American Sign Language on Linguistic Principles</title>
<date>1965</date>
<publisher>Gallaudet College Press</publisher>
<location>Washington D.C., USA</location>
<contexts>
<context>5. System Overview &amp; Features We give a short overview of the recognition framework and the used features presented in (Dreuw et al., 2007a). 5.1. Visual Modeling Phonological analysis going back to (Stokoe et al., 1965) has revealed that signs are made up out of basic articulatory units, initially referred to as cheremes by Stokoe, now commonly called phonemes because of their similarity with the discriminatory uni</context>
</contexts>
<marker>Stokoe, Casterline, Croneberg, 1965</marker>
<rawString>W. Stokoe, D. Casterline, and C. Croneberg. 1965. A Dictionary of American Sign Language on Linguistic Principles. Gallaudet College Press, Washington D.C., USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM an extensible language modeling toolkit</title>
<date>2002</date>
<booktitle>In ICSLP</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver, CO</location>
<contexts>
<context>be modeled by separate models, i.e. a different number of states and GMMs. 5.3. Language Models A trigram LM was trained on the main gloss annotations of the training corpora using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney discounting with interpolation. As explained in Section 4.3. for the RWTH-BOSTON400 database, we train 483 word classes but we have only 400 evaluation tokens due to differe</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM an extensible language modeling toolkit. In ICSLP, volume 2, pages 901–904, Denver, CO, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tsechpenakis</author>
<author>D Metaxas</author>
<author>C Neidle</author>
</authors>
<title>Learning-baseddynamiccouplingofdiscreteandcontinuous trackers</title>
<date>2006</date>
<journal>Computer Vision and Image Understanding</journal>
<pages>104--2</pages>
<contexts>
<context>denstein, 2007), hand tracking and recognition of hand shapes and movements (Vogler and Metaxas, 2004; Yuan et al., 2005; Dreuw et al., 2006), as well as categorization of signs (Zahedi et al., 2005; Tsechpenakis et al., 2006). Both data sets can be used for the training and evaluation of sign language recognition algorithms, as well as development and testing of human-machine interface approaches, data mining, etc. Since</context>
</contexts>
<marker>Tsechpenakis, Metaxas, Neidle, 2006</marker>
<rawString>G. Tsechpenakis, D. Metaxas, and C. Neidle. 2006. Learning-baseddynamiccouplingofdiscreteandcontinuous trackers. Computer Vision and Image Understanding, 104(2-3):140–156, December.</rawString>
</citation>
<citation valid="true">
<title>The Gallaudet Dictionary of American Sign Language. Gallaudet U</title>
<date>2006</date>
<editor>C. Valli, editor</editor>
<publisher>Press</publisher>
<location>Washington, DC, USA</location>
<marker>2006</marker>
<rawString>C. Valli, editor. 2006. The Gallaudet Dictionary of American Sign Language. Gallaudet U. Press, Washington, DC, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Vogler</author>
<author>S Goldenstein</author>
</authors>
<title>Facial movement analysis in ASL</title>
<date>2007</date>
<journal>Springer Journal on Universal Access in the Information Society, page</journal>
<note>to appear</note>
<contexts>
<context>L corpus has been used previously in evaluation of computer vision and pattern recognition methods, including detection of head gestures (Erdem and Sclaroff, 2002), recognition of facial expressions (Vogler and Goldenstein, 2007), hand tracking and recognition of hand shapes and movements (Vogler and Metaxas, 2004; Yuan et al., 2005; Dreuw et al., 2006), as well as categorization of signs (Zahedi et al., 2005; Tsechpenakis e</context>
</contexts>
<marker>Vogler, Goldenstein, 2007</marker>
<rawString>C. Vogler and S. Goldenstein. 2007. Facial movement analysis in ASL. Springer Journal on Universal Access in the Information Society, page to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Vogler</author>
<author>D Metaxas</author>
</authors>
<title>A framework for recognizing the simultaneous aspects of American Sign Language</title>
<date>2001</date>
<journal>Computer Vision &amp; Image Understanding</journal>
<volume>81</volume>
<contexts>
<context>= (x,y) at time t in signing space, features such as hand velocity mt = ut − ut−δ can easily be extracted. The hand trajectory features (Dreuw et al., 2007a) are similar to the features presented in (Vogler and Metaxas, 2001). 5.2. Pronunciation Handling Signed languages exhibit dialectal variation comparable to that found in spoken languages. Thus, one may find signs quite different from one another (in all aspects of t</context>
</contexts>
<marker>Vogler, Metaxas, 2001</marker>
<rawString>C. Vogler and D. Metaxas. 2001. A framework for recognizing the simultaneous aspects of American Sign Language. Computer Vision &amp; Image Understanding, 81(3):358–384, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Vogler</author>
<author>D Metaxas</author>
</authors>
<title>Handshapes and movements: Multiple-channel ASL recognition</title>
<date>2004</date>
<booktitle>Springer Lecture Notes in Artificial Intelligence</booktitle>
<contexts>
<context>methods, including detection of head gestures (Erdem and Sclaroff, 2002), recognition of facial expressions (Vogler and Goldenstein, 2007), hand tracking and recognition of hand shapes and movements (Vogler and Metaxas, 2004; Yuan et al., 2005; Dreuw et al., 2006), as well as categorization of signs (Zahedi et al., 2005; Tsechpenakis et al., 2006). Both data sets can be used for the training and evaluation of sign langua</context>
</contexts>
<marker>Vogler, Metaxas, 2004</marker>
<rawString>C. Vogler and D. Metaxas. 2004. Handshapes and movements: Multiple-channel ASL recognition. Springer Lecture Notes in Artificial Intelligence, (2915):247–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yang</author>
<author>S Sarkar</author>
<author>B Loeding</author>
</authors>
<title>Enhanced level building algorithm for the movement epenthesis problem in sign language recognition</title>
<date>2007</date>
<booktitle>In Computer Vision and Pattern Recognition</booktitle>
<contexts>
<context> are also labeled and handled by (Kneser and Ney, 1995) discounting with interpolation in the LM. Preliminary results for movement epenthesis in another subset of the database have been presented in (Yang et al., 2007). With pronunciation information, movement epenthesis information, and sentence boundary information, the vocabulary to be trained consists of 482 words. In training, 475 words from the complete voca</context>
</contexts>
<marker>Yang, Sarkar, Loeding, 2007</marker>
<rawString>R. Yang, S. Sarkar, and B. Loeding. 2007. Enhanced level building algorithm for the movement epenthesis problem in sign language recognition. In Computer Vision and Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Yuan</author>
<author>S Sclaroff</author>
<author>V Athitsos</author>
</authors>
<title>Automatic 2d hand tracking in video sequences</title>
<date>2005</date>
<booktitle>In IEEE Workshop on Applications of Computer Vision</booktitle>
<contexts>
<context>on of head gestures (Erdem and Sclaroff, 2002), recognition of facial expressions (Vogler and Goldenstein, 2007), hand tracking and recognition of hand shapes and movements (Vogler and Metaxas, 2004; Yuan et al., 2005; Dreuw et al., 2006), as well as categorization of signs (Zahedi et al., 2005; Tsechpenakis et al., 2006). Both data sets can be used for the training and evaluation of sign language recognition algo</context>
</contexts>
<marker>Yuan, Sclaroff, Athitsos, 2005</marker>
<rawString>Q. Yuan, S. Sclaroff, and V. Athitsos. 2005. Automatic 2d hand tracking in video sequences. In IEEE Workshop on Applications of Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zahedi</author>
<author>D Keysers</author>
<author>H Ney</author>
</authors>
<title>Pronunciation clustering and modeling of variability for appearancebased sign language recognition</title>
<date>2005</date>
<booktitle>In International GestureWorkshop2005, volume3881</booktitle>
<location>Vannes, France</location>
<contexts>
<context>sions (Vogler and Goldenstein, 2007), hand tracking and recognition of hand shapes and movements (Vogler and Metaxas, 2004; Yuan et al., 2005; Dreuw et al., 2006), as well as categorization of signs (Zahedi et al., 2005; Tsechpenakis et al., 2006). Both data sets can be used for the training and evaluation of sign language recognition algorithms, as well as development and testing of human-machine interface approach</context>
<context>am-, unigram-, bigram-, trigramlanguage models describe different linguistic contexts 4.1. RWTH-BOSTON-50 The RWTH-BOSTON-50 database was created for the task of isolated sign language recognition (Zahedi et al., 2005; Zahedi et al., 2006). It has been used for nearest-neighbor leaving-one-out evaluationof isolated signlanguage words. 7http://www.bu.edu/asllrp/ 8http://www-i6.informatik.rwth-aachen.de/ aslr/ Table</context>
<context>2 trigram 30.1 25.1 6. Experimental Results For the recognition of isolated signs we used the RWTHBOSTON-50 database, where the currently best known word-error-rate of 17.2% WER has been reported in (Zahedi et al., 2005). Figure 2 shows the effect of using different n-gram language models and scales on the RWTHBOSTON-104 database (Dreuw et al., 2007a). As in ASR, the language model adaptation by using sign language </context>
</contexts>
<marker>Zahedi, Keysers, Ney, 2005</marker>
<rawString>M. Zahedi, D. Keysers, and H. Ney. 2005. Pronunciation clustering and modeling of variability for appearancebased sign language recognition. In International GestureWorkshop2005, volume3881, Vannes, France, May.</rawString>
</citation>
</citationList>
</algorithm>

