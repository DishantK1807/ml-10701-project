1:196	Adaptive Language and Translation Models for Interactive Machine Translation Laurent Nepveu, Guy Lapalme Philippe Langlais RALI/DIRO Universite de Montreal, C.P. 6128, succursale Centre-ville Montreal, Quebec, Canada H3C 3J7 {nepveul,lapalme,felipe} @iro.umontreal.ca George Foster Language Technologies Research Centre National Research Council Canada A-1330, 101 rue Saint-Jean Bosco, Gatineau, Quebec, Canada K1A 0R6 George.Foster@nrc-cnrc.gc.ca Abstract We describe experiments carried out with adaptive language and translation models in the context of an interactive computer-assisted translation program.
2:196	We developed cache-based language models which were then extended to the bilingual case for a cachebased translation model.
3:196	We present the improvements we obtained in two contexts: in a theoretical setting, we achieved a drop in perplexity for the new models and, in a more practical situation simulating a user working with the system, we showed that fewer keystrokes would be needed to enter a translation.
4:196	1 Introduction Cache-based language models were introduced by Kuhn and de Mori (1990) for the dynamic adaptation of speech language models.
5:196	These models, inspired by the memory caches on modern computer architectures, are motivated by the principle of locality which states that a program tends to repeatedly use memory cells that are physically close.
6:196	Similarly, when speaking or writing, humans tend to use the same words and phrase constructs from paragraph to paragraph and from sentence to sentence.
7:196	This leads us to believe that, when processing a document, the part of a document that is already processed (e.g. for speech recognition, translation or text prediction) gives us very useful information for future processing in the same document or in other related documents.
8:196	A cache-based language model is a language model to which is added a smaller model trained only on the history of the document being processed.
9:196	The history is usually the last N words or sentences seen in the document.
10:196	Kuhn and de Mori (1990) obtained a drop in perplexity of nearly 68% when adding an unigram POS (part-of-speech) cache on a 3g-gram model.
11:196	Martin and al.
12:196	(1997) obtained a drop of nearly 21% when adding a bigram cache to a trigram model.
13:196	Clarkson and Robertson (1997) also obtained similar results with an exponentially decaying unigram cache.
14:196	The major problem with these theoretical results is that they assume the correctness of the material entering the cache.
15:196	In practice, this assumption does not always hold, and so a cache can sometimes do more harm than good.
16:196	1.1 Interactive translation context Over the last few years, an interactive machine translation (IMT) system (Foster et al. , 2002) has been developed which, as the translator is typing, suggests word and phrase completions that the user can accept or ignore.
17:196	The system uses a translation engine to propose the words or phrases which it judges the most probable to be immediately typed.
18:196	This engine includes a translation model (TM) and a language model (LM) used jointly to produce proposals that are appropriate translations of source words and plausible completions of the current text in the target language.
19:196	The translator remains in control of the translation because what is typed by the user is taken as a constraint to which the model must continually adapt its completions.
20:196	Experiments have shown that the use of this system can save about 50% of the keystrokes needed for entering a translation.
21:196	As the translation and language models are built only once, before the user starts to work with the system, the translator is often forced to repeatedly correct similar suggestions from the system.
22:196	The interactive nature of this setup made us believe that it is a good prospect for dynamic adaptive modeling.
23:196	If the dynamic nature of the system can be disadvantageous for static language and translation models, it is an incomparable advantage for a cache based approach because human correction intervenes before words go in the cache.
24:196	As the translator is using the system to correctly enter his translation progressively, we can expect the theoretical results presented in the literature to be obtainable in practice in the IMT context.
25:196	The first advantage of dynamic adaptation would be to help the translation engine make better predictions, but it has a further psychological advantage: as the translator works and potentially corrects the proposals of the engine, the user would feel that the software is learning from its errors.
26:196	The next section describes the models currently embedded within our IMT prototype.
27:196	Section 3 describes the cache-based adaptation we performed on the target language model.
28:196	In section 4, we present the different types of adaptations we performed on the translation model.
29:196	Section 5 then puts the results in the context of our IMT application.
30:196	Section 6 discusses the implications of our experiments and suggests some improvements that could be made to the system.
31:196	2 Current IMT models The word-based translation model embedded within the IMT system has been designed by Foster (2000).
32:196	It is a Maximum Entropy/Minimum Divergence (MEMD) translation model (Berger et al. , 1996), which mimics the parameters of the IBM model 2 (Brown et al. , 1993) within a log-linear setting.
33:196	The resulting model (named MDI2B) is of the following form, where h is the current target text, s the source sentence being translated, s a particular word in s and w the next word to be predicted: p(w|h,s) = q(w|h)exp( summationtext ss sw + AB) Z(h,s) (1) The q distribution represents the prior knowledge that we have about the true distribution and is modeled by an interpolated trigram in this study.
34:196	The  coefficients are the familiar transfer or lexical parameters, and the  ones can be understood as their position dependent correction.
35:196	Z is a normalizing factor, the sum of the numerator for every w in the target vocabulary.
36:196	Our baseline model used an interpolated trigram of the following form as the q distribution: p(w|h) = 1(wi2wi1) ptri(wi|wi2wi1) + 2(wi2wi1) pbi(wi|wi1) + 3(wi2wi1) puni(wi) + 4(wi2wi1)  1|V |+1 where 1(wi2wi1) + 2(wi2wi1) + 3(wi2wi1) + 4(wi2wi1) = 1 and |V| + 1 is the size of the event space (including a special unknown word).
37:196	As mentioned above, the MDI2B model is closely related to the IBM2 model (Brown et al. , 1988).
38:196	It contains two classes of features: word pair features and positional features.
39:196	The word pair feature functions are defined as follows: fst(w,h,s) = braceleftbigg 1 if s  s and t = w 0 otherwise This function is on if the predicted word is t and s is in the current source sentence.
40:196	Each feature fst has a corresponding weight st (for brevity, this is defined to be 0 in equation 1 if the pair s,t is not included in the model).
41:196	The positional feature functions are defined as follows: fA,B(w,i,s) = Jsummationdisplay j=1 [(i,j,J)  A  (sj,w)  B j = sj] where [X] is 1 if X is true, otherwise 0; and sj is the position of the occurrence of sj that is closest to i according to an IBM2 model.
42:196	A is a class that groups positional (i,j,J) configurations having similar IBM2 alignment probabilities, in order to reduce data sparseness.
43:196	B is a class of word pairs having similar weights st. Its purpose is to simulate the way IBM2 alignment probabilities modulate IBM1 word-pair probabilities, by allowing the value of the positional feature weight to depend on the magnitude of the corresponding word-pair weight.
44:196	As with the word pair features, each fA,B has a corresponding weight AB.
45:196	Since feature selection is applied at training time in order to improve speed, avoid overfitting, and keep the model compact, the summation in the exponential term in (1) is only carried out over the set of active pairs maintained by the model and not over all pairs as might be inferred from the formulation.
46:196	To give an example of how the model works, if the source sentence is the fruit I am eating is a banana and we are predicting the word banane following the target words: Le fruit que je mange est une, the active pairs involving banana would be (fruit, banana) and (banane, banana) since, of all the pairs (s,t) they would be the only ones kept by the feature selection algorithm1.
47:196	The probability of banane would therefore depend on the weights of those two pairs, along with position weights which capture the relative proximity of the words involved.
48:196	3 Language model adaptation We implemented a first monolingual dynamic adaptation of this model by inserting a cache component in its reference distribution, thus only affecting the q distribution.
49:196	We obtained similar results 1See (Foster, 2000) for the description of this algorithm.
50:196	as for classical ngram models: the unigram cache model proved to be less efficient than the bigram one, and the trigram cache suffered from sparsity.
51:196	We also tested a model where we interpolated the three cache models to gain information from each of the unigram, bigram, and trigram cache models.
52:196	For completeness, this generalized model is described in equation 2 under the usual constraints thatsummationtext i i(h) = 1 for all h. p(w|h) = 1(h) ptri(wi|wi2wi1) + 2(h) pbi(wi|wi1) + 3(h) puni(wi) + 4(h)  1|V |+1 + 5(h) ptric(wi|wi2wi1) + 6(h) pbic(wi|wi1) + 7(h) punic(wi) (2) Those models were trained from splits of the Canadian Hansard corpus.
53:196	The base ngram model was estimated with a 30M word split of the corpus.
54:196	The weighting coefficients of both the base trigram and the cache models were estimated with an EM algorithm trained with 1M words.
55:196	We tested our models, translating from English to French, on two corpora of different types: the first one hansard is a document taken from the same large corpus that was used for training (the testing and training corpora were exclusive splits).
56:196	The second one sniper, which describes the job of a sniper, is from another domain characterized by lexical and phrasal constructions very different from those used to estimate the probabilities of our models.
57:196	Table 1 shows the perplexity on the hansard and the sniper corpora.
58:196	Preliminary experiments led us to two sizes of cache which seemed promising: 2000 and 5000 corresponding to the last 2000 and 5000 words seen during the processing of a document.
59:196	The BI column gives the results of the bigram cache model and the 1+2+3 gives the results of the interpolated cache model which included the unigram, bigram and trigram cache.
60:196	The results show that our models improve the base static model by 5% on documents supposedly well known by the models and by more that 52% on documents that are unknown to the model.
61:196	Section 5 puts these results in the perspective of our actual IMT system.
62:196	Note that he addition of a cache component to a language model involves negligible extra training time.
63:196	Taille BI  1+2+3  base hansard=17.6584 2000 16.937 -4.1% 16.840 -4.6% 5000 16.903 -4.3% 16.777 -5.0% base sniper=135.808 2000 73.936 -45.6% 67.780 -50.1% 5000 70.514 -48.1% 64.204 -52.7% Table 1: Perplexities of the MDI2B model with a cache component included in the reference distribution on the hansard and sniper corpora.
64:196	4 Translation model adaptation With those excellent results in mind, we extended the idea of dynamic adaptation to the bilingual case which, to our knowledge, has never been tried before.
65:196	We developed a model called MDI2BCache which is a MDI2B model to which we added a cache component based on word pairs.
66:196	Recall that, when predicting a word w at a certain point in a document, the probability depends on the weights of the pairs (s,w) for each active word s in the current source sentence.
67:196	As the prediction of the words of the document goes on, our model keeps in a cache each active pair used for the prediction of each word.
68:196	In the example above, if the translator accepts the word banane, then the two pairs (fruit, banana) and (banane, banana) will be added to the cache.
69:196	We added a new feature to the MEMD model to take into account the presence of a certain pair in the recent history of the processed document: fcache st(w,h,s) =    1 if    s  s, t = w, (s,t)  cache st > p 0 otherwise We added a threshold value p to the feature function because while analyzing the pair weights, we discovered that low weight pairs are usually pairs of utility words such as conjunctions and punctuation.
70:196	We also came to the conclusion that they are not the kind of words we want to have in the cache, since their presence in a sentence implies little about their presence in the next.
71:196	The resulting model is of the form: p(w|h,s) = q(w|h)exp( summationtext ss sw + AB + sw) Z(h,s) Thus, every fcache sw has a corresponding weight sw for the calculation of the probability of w. Size 0.3  0.5  0.7  base One feature weight, no Viterbi orig perp=17.6584 1000 17.5676 -0.51% 17.5756 -0.47% 17.5983 -0.34% 2000 17.5698 -0.50% 17.5766 -0.46% 17.5976 -0.34% 5000 17.5743 -0.48% 17.5776 -0.46% 17.5965 -0.35% 10000 17.5777 -0.46% 17.5791 -0.45% 17.5962 -0.35% base One feature weight per pair, no Viterbi orig perp=17.6584 1000 17.5817 -0.43% 17.5858 -0.41% 17.6065 -0.29% 2000 17.5933 -0.37% 17.5918 -0.38% 17.6061 -0.30% 5000 17.5849 -0.42% 17.5874 -0.40% 17.6076 -0.29% 10000 17.5890 -0.39% 17.5891 -0.39% 17.6069 -0.29% base One feature weight, Viterbi orig perp=17.6584 1000 17.5602 -0.56% 17.5697 -0.50% 17.5940 -0.36% 2000 17.5676 -0.51% 17.5695 -0.50% 17.5896 -0.39% 5000 17.5614 -0.55% 17.5687 -0.51% 17.5925 -0.37% 10000 17.5650 -0.53% 17.5687 -0.51% 17.5906 -0.38% Table 2: MDI2BCache test perplexities.
72:196	One feature weight, Viterbi alignment version.
73:196	4.1 Number of cache features We implemented two versions of the model, one in which we estimated only one cache feature weight for the whole model and another in which we estimated one cache feature weight for every word pair in the model.
74:196	The first model is simpler and is easier to estimate.
75:196	The assumption is made that every pair in the model has the same tendency to repeat itself.
76:196	The second model doubles the number of wordpair parameters compared to MDI2B, and thus leads to a linear increase in training time.
77:196	Extra training time is negligible in the first model.
78:196	4.2 Word alignment One of the main difficulties of automatic MT is determining which source word(s) translate to which target word(s).
79:196	It is very difficult to do this task automatically, in part because it is also very difficult manually.
80:196	If a pair of sentences are given to 10 translators for alignment, the results would likely not be identical in all cases.
81:196	As it is nearly impossible to determine such an alignment, most translation models consider every source word to have an effect on the translation of every target word.
82:196	This difficulty shows up in our cache-based model.
83:196	When adding word pairs to the cache, we ideally would like to add only word pairs that were really in a translation relation in the given sentence.
84:196	This is why we also implemented a version of our model in which a word alignment is first carried out in order to select good pairs to be added to the cache.
85:196	For this purpose, we computed a Viterbi alignment based on an IBM model 2.
86:196	This results in a subset of the good active pairs to be added to the cache.
87:196	The Viterbi algorithm gives us a higher confidence level that the pair of words added to the cache were really in a translation relation.
88:196	But it can also lead to word pairs not added to the cache that should have been added.
89:196	4.3 Results Table 2 shows the results of the different configurations of the MDI2BCache model.
90:196	For every configuration we trained and tested on splits of the Canadian Hansard with threshold values of 0.3, 0.5, and 0.7 and cache sizes of 1000, 2000, 5000, and 10000.
91:196	The top of the table is the version of the model with only one feature weight without Viterbi alignment.
92:196	The middle of the table is the version with one feature weight per word pair without Viterbi alignment.
93:196	Finally, the bottom is for the version with only one feature weight and a Viterbi alignment made prior to adding pairs to the cache.
94:196	Threshold values of 0.3, 0.5, and 0.7 led to 75%, 50%, and 25% of the pairs considered for addition to the cache respectively.
95:196	The results show that the threshold values of 0.5 and 0.7 are removing too many pairs.
96:196	The best results are obtained with a threshold of 0.3 in all tests.
97:196	Since the number of pairs kept in the model appears to vary in proportion to the threshold value, we did not consider it necessary to use an automatic search algorithm to find an optimal threshold value.
98:196	The gain in performance would have been negligible.
99:196	The results also show that having one feature weight per word pair leads to lower results.
100:196	This can be explained by the fact that it is much more Size 0.3  0.5  base MDI2B=135.808 1000 132.865 -2.17% 132.751 -2.25% 2000 132.771 -2.23% 132.752 -2.25% 5000 132.733 -2.26% 132.628 -2.34% 10000 132.997 -2.07% 132.674 -2.31% Table 3: MDI2BCache test perplexities.
101:196	One feature weight, Viterbi alignment version.
102:196	Sniper test difficult to estimate a weight for every pair that one weight for all pairs.
103:196	Since we use only thousands of words in the cache, the training process suffers from a poor data representation.
104:196	The Viterbi alignment seems to be helping the models.
105:196	The best results are obtained with the version of our model with Viterbi alignment.
106:196	However, this gives only a 0.56% percent drop in perplexity.
107:196	We then tested our best configuration on the sniper corpus.
108:196	Table 3 shows the results.
109:196	We dropped threshold value 0.7 and tested only the model with only one feature weight and a Viterbi alignment.
110:196	Results show that our bilingual cache model shows improvement (four times higher) in drop of perplexity when used on documents very different from the training corpus.
111:196	In general, results give lower perplexity than our base model showing that the bilingual cache is helpful to the model, but the results are not as good as that the ones obtained in the unilingual case.
112:196	Section 6 discusses these results further.
113:196	5 Evaluation of IMT As stated earlier, drops in perplexity are theoretical results that have been obtained previously in the case of unilingual dynamic adaptation but for which a corresponding level of practical success was rarely attained because of the cache correctness problem.
114:196	To show that the interactive nature of our assistedtranslation application can really benefit from dynamic adaptation, we tested our models in a more realistic translation context.
115:196	This test consists of simulating a translator using the IMT system as it proposes words and phrases and accepting, correcting or rejecting the proposals by trying to reproduce a given target translation (Foster et al. , 2002).
116:196	The metric used is the percentage of keystrokes saved by the use of the system instead of having to type directly all the target text.
117:196	For these simulations, we used only a 10K word split of the hansard and of the sniper corpus.
118:196	The reason is that the IMT application potenTaille BI  1+2+3  base hansard=27.435 2000 27.784 +1.3% 27.719 +1.0% 5000 27.837 +1.5% 27.821 +1.4% base sniper=9.686 2000 11.404 +15.1% 11.294 +14.2% 5000 11.498 +15.8% 11.623 +16.7% Table 4: Saved keystrokes raises for the MDI2B model with cache component in the reference distribution on the hansard and sniper corpora.
119:196	0.3  base hansard=27.4358 1000 27.557 +0.44% 2000 27.531 +0.35% 5000 27.488 +0.18% 10000 27.468 +0.12% base sniper=9.686 1000 9.896 +2.17% 2000 10.023 +3.48% 5000 9.983 +3.07% 10000 9.957 +2.80% Table 5: Saved keystrokes raises for the MDI2BCache model with only one feature weight and Viterbi alignment on the hansard and sniper corpora.
120:196	tially proposes new completions after every character typed by the user.
121:196	For a 10K word document, it needs to search about 1 million times for high probability words and phrases.
122:196	This leads to relatively long simulation times, even though predictions are made at real time speeds.
123:196	Table 4 shows the results obtained with the MDI2B model to which we added a cache component for the reference interpolated trigram distribution.
124:196	We can see that the saved keystroke percentages are proportional to the perplexity drops reported in section 3.
125:196	The use of our models raises the saved keystrokes by nearly 1.5% in the case of well known documents and by nearly 17% in the case of very different documents.
126:196	These are very interesting results for a potential professional use of TransType.
127:196	Table 5 shows an increase in the number of saved keystrokes: 0.44% on the hansard and 3.5% on the sniper corpora.
128:196	Once again, the results are not as impressive as the ones obtained for the monolingual dynamic adaptation case.
129:196	6 Discussion The results presented in section 3 on language model adaptation confirmed what had been reported in the literature: adding a cache component to a language model leads to a drop in perplexity.
130:196	Moreover, we were able to demonstrate that using a cache-based language model inside a translation model leads to better performance for the whole translation model.
131:196	We obtained drops in perplexity of 5% on a corpus of the same type as the training corpus and of 50% on a different one.
132:196	These theoretical results lead to very good practical results.
133:196	We were able to increase the saved keystroke percentage by 1.5% on the similar corpus as the training and by nearly 17% on the different corpus.
134:196	These results confirm our hypothesis that dynamic adaptation with cache-based language model can be useful in the context of IMT, particularly for new types of texts.
135:196	Results presented in section 4 on translation model adaptation show that our approach has led to drops in perplexity although not as high as we would have hoped.
136:196	To understand these disappointing results, we analyzed the content of the cache for different configurations of our MDI2BCache model.
137:196	base 0.3 viterbi + 0.3 (is,qu) (to,afin) (offence,crime) (.
138:196	,sa) (was,a) (was,ete) (this,,) (UNK,UNK) (very,tr`es) (all,toutes) (piece,legislative) (today,aujourdhui) (have,du) (this,ce) (jobs,emploi) (the,pour) (per,100) (concern,inquietude) (on,du) (that,soient) (skin,peau) (of,un) (,,,) (there,y) (we,nous) (?,il) (government,le) (the,du) (any,tout) (an,un) 18 68 86 Table 6: Cache sampling of different configurations of MDI2BCache model.
139:196	Table 6 shows the results of our sampling.
140:196	We tested three model configurations.
141:196	The first one, in the first column, was the base MDI2BCache model which adds all active pairs to the cache.
142:196	The second configuration, in the second column, was a threshold value of 0.3 that brings about 75% of the pairs being added to the cache.
143:196	The last configuration was a model with threshold value of 0.3 and a Viterbi alignment made prior to the addition of pairs in the cache.
144:196	The three model configuration were with only one feature weight.
145:196	For all three configurations, we took a sample of 10 pairs (shown in table 6) and a sample of 100 pairs.
146:196	With the second sample, we manually analyzed each pair and counted the number of pairs (shown in the last row of the table) we believed were useful for the model (words that are occasionally translations of one another).
147:196	The results obtained in section 4 seem to agree with the current analysis.
148:196	From left to right in the table, the pairs seem to contain more information and to be more appropriate additions to the cache.
149:196	The configuration with Viterbi alignment which contains 86 good pairs clearly seems to be the configuration with the most interesting pairs.
150:196	The problem with such a cache-based translation model seem to be similar to the balance between precision and recall in information retrieval.
151:196	On one hand, we want to add in the cache every word pair in which the two words are in translation relation in the text.
152:196	We further want to add only the pairs in which the two words are really in translation relation in the text.
153:196	It seems that with our base model, we add most of the good pairs, but also a lot of bad ones.
154:196	With the Viterbi alignment and a threshold value of 0.3, most of the pairs added are good ones, but we are probably missing a number of other appropriate ones.
155:196	This comes back to the task of word alignment, which is a very difficult task for computers (Mihalcea and Pedersen, 2003).
156:196	Moreover, we would want to add in the cache only those words for which more than one translation is possible.
157:196	For example, the pair (today, aujourdhui), though it is a very useful pair for the base model, is unlikely to help when added to the cache.
158:196	The reason is simple: they are two words that are always translations of one another, so the model will have no problem predicting them.
159:196	This ideal of precision and recall and of useful pairs in the cache is obtained by our model with threshold of 0.3, a Viterbi alignment and a cache size of 1000.
160:196	One disadvantage of our bilingual adaptive model is the way it handles unknown words.
161:196	In the cachebased language model, the unknown words were dealt with normally, i.e. they were added to the cache and given a certain probability afterwards.
162:196	So, if an unknown word was seen in a certain sentence and then later on, it would receive a probability mass of its own but not the one given to any unknown word.
163:196	By having its own probability mass due to its presence in the cache, such previously unknown word can be predicted by the model.
164:196	In the case of our MDI2BCache model, because we have not yet implemented an algorithm for guessing the translations of unknown words, they are simply represented within the model as UNK words, which means that the model never learns them.
165:196	The results obtained with the sniper corpus shows us that dynamic adaptation is also more helpful for documents that are little known to the model in the bilingual context.
166:196	The results are four times better on the sniper corpus than on the Hansard testing corpus.
167:196	Once again for the bilingual case, the practical test results in the number of saved keystrokes agree with the theoretical results of drops in perplexity.
168:196	This result shows that bilingual dynamic adaptation also can be implemented in a practical context and obtain results similar to the theoretical results.
169:196	All things considered, we believe that a cachebased translation model shows a great potential for bilingual adaptation and that greater perplexity drops and keystroke savings could be obtained by either reengineering the model or by improving the MDI2BCache model.
170:196	6.1 Key improvements to the model Following the analysis of the results obtained by our model, we have pointed out some key improvements that the model would need in order to get better results.
171:196	In this list we focus on ways of improving adaptation strategies for the current model, omitting other obvious enhancements such as adding phrase translations.
172:196	Unknown word processing Learning new words would be a very important feature to add to the model and would lead to better results.
173:196	We did not incorporate the processing of unknown words in the MDI2BCache because the structure of model did not lend itself to this addition.
174:196	Especially with documents such as the sniper corpus, we believe that this could be a key improvement for a dynamic adaptive model.
175:196	Better alignment As mentioned before, the ultimate goal for our cache is that it contains only the pairs present in the perfect alignment.
176:196	Better performance from the alignment would lead to pairs in the cache closer to this ideal.
177:196	In this study we computed Viterbi alignments from an IBM model 2, because it is very efficient to compute and also because for training MDI2B, we do use the IBM model 2.
178:196	We could consider also more advanced word alignment models (Och and Ney, 2000; Lin and Cherry, 2003; Moore, 2001).
179:196	To keep the alignment model simple, we could still use an IBM model 2, but with the compositionality constraint that has been shown to give better word alignment than the Viterbi one (Simard and Langlais, 2003).
180:196	Feature weights We implemented two versions of our model: one with only one feature weight and another with one feature weight for each word pair.
181:196	The second model suffered from poor data representation and our training algorithm wasnt able to estimate good cache feature weights.
182:196	We think that creating classes of word pairs, such as it was done for positional alignment features, would lead to better results.
183:196	It would enable the model to take into account the tendency that a pair has to repeat itself in a document.
184:196	Relative weighting Another key improvement is that changes to word-pair weights should be relative to each source word.
185:196	For example, if (house, maison) is a pair in the cache, we would like to favour maison over possible alternatives such as chambre as a translation of house.
186:196	In the existing model this is done by boosting the weight on (house,maison), which has the undesirable side-effect of making maison more important in the model than translations of other source words in the current sentence which have not appeared in the cache.
187:196	One way of eliminating this behaviour would be to learn negative weights on alternatives like (house,chambre) which do not appear in the cache.
188:196	We believe these improvements would better show the potential of bilingual dynamic adaptation.
189:196	7 Conclusion We have presented dynamic adaptive translation models using cache-based implementations.
190:196	We have shown that monolingual dynamic adaptive models exhibit good theoretical performance in a bilingual translation context.
191:196	We observed that these theoretical results carry over to practical gains in the context of an IMT application.
192:196	We have developed bilingual dynamic adaptation through a cache-based translation model.
193:196	Our results show the potential of bilingual dynamic adaptation.
194:196	We have given explanations about why the results obtained are not as high as hoped and presented some key improvements that should be made to our model or should be taken into account in the development of a new model.
195:196	We believe that this study reveals the potential for adaptive interactive machine translation system and we hope to read similar reports for other implementations of the same interactive scenario e.g.
196:196	(Och et al. , 2003).

