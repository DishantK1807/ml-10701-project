<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Dan Moldovan</author>
<author>Marius Pasca</author>
</authors>
<title>Rada Mihalcea, Mihai Surdeanu, Razvan Bunsecu, Roxana Girju, Vasile Rus</title>
<date>2000</date>
<booktitle>In Proceedings of the 9th Text REtrieval Conference</booktitle>
<location>and</location>
<contexts>
<context>a?, ATD components enable Q/A systems to retrieve the sets of candidate answers which include the INDIVIDUALs and/or ORGANIZATIONs who provided aid to the victims of the hurricane. Early work in ATD (Harabagiu et al., 2000; Harabagiu et al., 2001) leveraged sets of heuristics in order to identify the expected answer types (EATs) of questions submitted to a Q/A system. Most modern Q/A systems, however, follow work done </context>
<context>junction with a state-of-theart question-answering system, LCC’s FERRET (Hickl et al., 2006a), and Section 7 presents our conclusions. 2. Previous Work Answer type hierarchies were first employed by (Harabagiu et al., 2000) using a heuristic classifier based on the WORDNET (Miller, 1995) ontology. (Li and Roth, 2002) explored the use of machine learning techniques to answer type detection and (Krishnan et al., 2005) im</context>
<context>f information sought by a factoid question. We know of no previous work which combines the ability to scale to large ATHs and provide the benefits of a machinelearning based system. While the ATH in (Harabagiu et al., 2000) could easily be scaled to include a potentially very large number of types (e.g. see (Harabagiu et al., 2005) for an example of how this could be accomplished for a top-performing TREC Q/A system), </context>
</contexts>
<marker>Harabagiu, Moldovan, Pasca, 2000</marker>
<rawString>Sanda Harabagiu, Dan Moldovan, Marius Pasca, Rada Mihalcea, Mihai Surdeanu, Razvan Bunsecu, Roxana Girju, Vasile Rus, and Paul Morarescu. 2000. FALCON: Boosting knowledge for answer engines. In Proceedings of the 9th Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
<author>M Pasca</author>
<author>M Surdeanu</author>
<author>R Mihalcea</author>
<author>R Girju</author>
<author>V Rus</author>
<author>F Lacatusu</author>
<author>P Morarescu</author>
<author>R Bunescu</author>
</authors>
<title>Answering Complex, List and Context Questions with LCC’s Question-Answering Server</title>
<date>2001</date>
<booktitle>In Proceedings of the Tenth Text REtrieval Conference</booktitle>
<contexts>
<context>e Q/A systems to retrieve the sets of candidate answers which include the INDIVIDUALs and/or ORGANIZATIONs who provided aid to the victims of the hurricane. Early work in ATD (Harabagiu et al., 2000; Harabagiu et al., 2001) leveraged sets of heuristics in order to identify the expected answer types (EATs) of questions submitted to a Q/A system. Most modern Q/A systems, however, follow work done by (Li and Roth, 2002) i</context>
</contexts>
<marker>Harabagiu, Moldovan, Pasca, Surdeanu, Mihalcea, Girju, Rus, Lacatusu, Morarescu, Bunescu, 2001</marker>
<rawString>S. Harabagiu, D. Moldovan, M. Pasca, M. Surdeanu, R. Mihalcea, R. Girju, V. Rus, F. Lacatusu, P. Morarescu, and R. Bunescu. 2001. Answering Complex, List and Context Questions with LCC’s Question-Answering Server. In Proceedings of the Tenth Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
<author>C Clark</author>
<author>M Bowden</author>
<author>A Hickl</author>
<author>P Wang</author>
</authors>
<title>Employing Two Question Answering Systems in TREC 2005</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourteenth Text REtrieval Conference</booktitle>
<contexts>
<context> large ATHs and provide the benefits of a machinelearning based system. While the ATH in (Harabagiu et al., 2000) could easily be scaled to include a potentially very large number of types (e.g. see (Harabagiu et al., 2005) for an example of how this could be accomplished for a top-performing TREC Q/A system), it is constrained in its aapproach to WORDNET’s hand-built hypernym relations, which does not coincide with co</context>
<context>verage answer type detection system into the factoid questionanswering pipeline implemented in LCC’s FERRET question-answering system. First evaluated in the 2005 TREC Question-Answering Evaluations (Harabagiu et al., 2005), FERRET leverages a wide range of lexico-semantic annotations (including output from LCC’s (1) syntactic and (2) semantic dependency parsers, (3) LCC’s CICEROLITE named entity recognition system and</context>
</contexts>
<marker>Harabagiu, Moldovan, Clark, Bowden, Hickl, Wang, 2005</marker>
<rawString>S. Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl, and P. Wang. 2005. Employing Two Question Answering Systems in TREC 2005. In Proceedings of the Fourteenth Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>Patrick Wang</author>
<author>John Lehamnn</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Ferret: Interactive Question-Answering for Real-World Research Environments</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 COLING-ACL Interactive Presentations Session</booktitle>
<contexts>
<context> English factoid questions. We show how an answer type detection system trained on this corpus can be used to enhance the accuracy of a state-of-the-art question-answering system (Hickl et al., 2007; Hickl et al., 2006b) by more than 7% overall. 1. Introduction Work in factoid question-answering (Q/A) has long leveraged answer type detection (ATD) systems in order to identify the semantic class of the entities, wor</context>
<context>order to annotate a corpus of more than 10,000 English questions mined from web documents and show how the hierarchy we propose can boost the accuracy of a state-of-the-art question-answering system (Hickl et al., 2006a) by more than 7% overall. The rest of this paper is organized in the following way. Section 2 presents a discussion of previous heuristicand classification-based approaches to the problem of ATD f</context>
<context>questions. Section 5 provides details of our ATD system. Section 6 details results of evaluations using our proposed ATH in conjunction with a state-of-theart question-answering system, LCC’s FERRET (Hickl et al., 2006a), and Section 7 presents our conclusions. 2. Previous Work Answer type hierarchies were first employed by (Harabagiu et al., 2000) using a heuristic classifier based on the WORDNET (Miller, 1995) on</context>
<context>22 today.) and DATE (He was born on 5/5/89.). Furthermore, answers need not necessarily be drawn from entities. While some Q/A Systems detect the difference between “factoid” and “complex” questions (Hickl et al., 2006a), it is often desirable to include both in the ATH such as the DESCRIPTION coarse type in the UIUC hierarchy. 3.2. Constructing an ATH from an ETH The following steps demonstrate how one could const</context>
<context>ions compiled from (1) existing annotated question corpora (Li and Roth, 2002), (2) collections of questions mined from the web, and (3) questions submitted to LCC’s FERRET question-answering system (Hickl et al., 2006a). (A breakdown of the number of questions obtained from each of these three strategies is provided in Table 3.) Question Set # Questions UIUC Train &amp; Test 5,952 Web Crawl 3,485 FERRET log 563 Table </context>
</contexts>
<marker>Hickl, Wang, Lehamnn, Harabagiu, 2006</marker>
<rawString>Andrew Hickl, Patrick Wang, John Lehamnn, and Sanda Harabagiu. 2006a. Ferret: Interactive Question-Answering for Real-World Research Environments. In Proceedings of the 2006 COLING-ACL Interactive Presentations Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>John Williams</author>
<author>Jeremy Bensley</author>
<author>Kirk Roberts</author>
<author>Ying Shi</author>
<author>Bryan Rink</author>
</authors>
<title>Question Answering with LCC’s Chaucer at TREC 2006</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifteenth Text REtrieval Conference</booktitle>
<contexts>
<context> English factoid questions. We show how an answer type detection system trained on this corpus can be used to enhance the accuracy of a state-of-the-art question-answering system (Hickl et al., 2007; Hickl et al., 2006b) by more than 7% overall. 1. Introduction Work in factoid question-answering (Q/A) has long leveraged answer type detection (ATD) systems in order to identify the semantic class of the entities, wor</context>
<context>order to annotate a corpus of more than 10,000 English questions mined from web documents and show how the hierarchy we propose can boost the accuracy of a state-of-the-art question-answering system (Hickl et al., 2006a) by more than 7% overall. The rest of this paper is organized in the following way. Section 2 presents a discussion of previous heuristicand classification-based approaches to the problem of ATD f</context>
<context>questions. Section 5 provides details of our ATD system. Section 6 details results of evaluations using our proposed ATH in conjunction with a state-of-theart question-answering system, LCC’s FERRET (Hickl et al., 2006a), and Section 7 presents our conclusions. 2. Previous Work Answer type hierarchies were first employed by (Harabagiu et al., 2000) using a heuristic classifier based on the WORDNET (Miller, 1995) on</context>
<context>22 today.) and DATE (He was born on 5/5/89.). Furthermore, answers need not necessarily be drawn from entities. While some Q/A Systems detect the difference between “factoid” and “complex” questions (Hickl et al., 2006a), it is often desirable to include both in the ATH such as the DESCRIPTION coarse type in the UIUC hierarchy. 3.2. Constructing an ATH from an ETH The following steps demonstrate how one could const</context>
<context>ions compiled from (1) existing annotated question corpora (Li and Roth, 2002), (2) collections of questions mined from the web, and (3) questions submitted to LCC’s FERRET question-answering system (Hickl et al., 2006a). (A breakdown of the number of questions obtained from each of these three strategies is provided in Table 3.) Question Set # Questions UIUC Train &amp; Test 5,952 Web Crawl 3,485 FERRET log 563 Table </context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Shi, Rink, 2006</marker>
<rawString>Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts, Ying Shi, and Bryan Rink. 2006b. Question Answering with LCC’s Chaucer at TREC 2006. In Proceedings of the Fifteenth Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>Kirk Roberts</author>
<author>Bryan Rink</author>
<author>Jeremy Bensely</author>
<author>Tobias Jungen</author>
<author>Ying Shi</author>
<author>John Williams</author>
</authors>
<date>2007</date>
<booktitle>Question Answering with LCC’s Chaucer-2 at TREC 2007. In Proceedings of the Sixteenth Text REtrieval Conference</booktitle>
<contexts>
<context> of more than 10,000 English factoid questions. We show how an answer type detection system trained on this corpus can be used to enhance the accuracy of a state-of-the-art question-answering system (Hickl et al., 2007; Hickl et al., 2006b) by more than 7% overall. 1. Introduction Work in factoid question-answering (Q/A) has long leveraged answer type detection (ATD) systems in order to identify the semantic class </context>
</contexts>
<marker>Hickl, Roberts, Rink, Bensely, Jungen, Shi, Williams, 2007</marker>
<rawString>Andrew Hickl, Kirk Roberts, Bryan Rink, Jeremy Bensely, Tobias Jungen, Ying Shi, and John Williams. 2007. Question Answering with LCC’s Chaucer-2 at TREC 2007. In Proceedings of the Sixteenth Text REtrieval Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Krishnan</author>
<author>S Das</author>
<author>S Chakrabarti</author>
</authors>
<title>Enhanced answer type inference from questions using sequential models</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<contexts>
<context>y (Harabagiu et al., 2000) using a heuristic classifier based on the WORDNET (Miller, 1995) ontology. (Li and Roth, 2002) explored the use of machine learning techniques to answer type detection and (Krishnan et al., 2005) improved accuracy through the use of their informer span. Alternatively, (Pinchak and Lin, 2006) use a probabilistic model with no pre-defined hierarchy in order to identify the type of information </context>
<context>earning based while the remainder are heuristic classifiers. Table 8: Experiments used for classifying answer types. In a departure from previous machine-learning based approaches (Li and Roth, 2002; Krishnan et al., 2005), we used a maximum entropy classifier to learn our ATH. Our classification process currently uses three machinelearned classifiers. The first resolves all questions into one of 11 “coarse” types tha</context>
<context>tion of a new answer type detection system capable of recognizing more than 200 different expected answer types with greater 85% precision. In a departure from previous work in answer type detection (Krishnan et al., 2005; Li and Roth, 2002), we have demonstrated how a large, multi-tiered answer type hierarchy can be created which incorporates many of the entity types included in LCC’s wide coverage named entity recog</context>
</contexts>
<marker>Krishnan, Das, Chakrabarti, 2005</marker>
<rawString>V. Krishnan, S. Das, and S. Chakrabarti. 2005. Enhanced answer type inference from questions using sequential models. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Learning question classifiers</title>
<date>2002</date>
<booktitle>In Proc. the International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context>rabagiu et al., 2001) leveraged sets of heuristics in order to identify the expected answer types (EATs) of questions submitted to a Q/A system. Most modern Q/A systems, however, follow work done by (Li and Roth, 2002) in using machine learning classifiers in order to select the one (or more) EATs from a fixed hierarchy of answer types which are most appropriate for a particular question. While learning-based appr</context>
<context>nly been tasked with distinguishing amongst a limited set of EATs. For example, the most commonly-used answer type hierarchy (ATH), the University of Illinois (UIUC) answer type hierarchy created by (Li and Roth, 2002), includes only a total of 50 unique expected answer types (generally referred to as “fine” answer types), organized into 6 different categories (referred to as “coarse” answer types). (The entire UI</context>
<context>ection 7 presents our conclusions. 2. Previous Work Answer type hierarchies were first employed by (Harabagiu et al., 2000) using a heuristic classifier based on the WORDNET (Miller, 1995) ontology. (Li and Roth, 2002) explored the use of machine learning techniques to answer type detection and (Krishnan et al., 2005) improved accuracy through the use of their informer span. Alternatively, (Pinchak and Lin, 2006) </context>
<context> for a top-performing TREC Q/A system), it is constrained in its aapproach to WORDNET’s hand-built hypernym relations, which does not coincide with common answer types in questions. in contrast, the (Li and Roth, 2002) UIUC ATH is designed especially for questions, but lacks the ability to extend the depth of the hierarchy when Q/A systems are capable of handling more detailed answer types. (See Section 3 for a mo</context>
<context>rent semantic classes that all share a hypernym-like relationship with their parent. (A graphical representation of a portion of the LCC ATH is presented in Figure 1.) The UIUC answer type hierarchy (Li and Roth, 2002) (and corresponding annotated corpus of nearly 6000 questions) has provided a solid test-bed for researchers to develop and benchmark a variety of machine learning techniques for answer type detectio</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>X. Li and D. Roth. 2002. Learning question classifiers. In Proc. the International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for English</title>
<date>1995</date>
<journal>Communications of the Association for Computing Machinery</journal>
<volume>38</volume>
<contexts>
<context> such a score increase. First, using a multi-pass machine classification approach has the additional benefit of allowing for different feature sets to be used. For example, resources such as WORDNET (Miller, 1995) may be used to aid in the classification of geo-political entities (GPE) (e.g., knowing that a hamlet should map to the answer type CITY). Only a simple “is a hypernym of location” feature is necess</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: a lexical database for English. Communications of the Association for Computing Machinery, 38(11):39–41.</rawString>
</citation>
</citationList>
</algorithm>

