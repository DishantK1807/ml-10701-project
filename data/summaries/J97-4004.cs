\[\] 6.5 Principle of Maximum Tokenization The three tokenization definitions in this section are essentially descriptive restatements of the corresponding constructive tokenization procedures, which in turn are realizations of the widely followed principle of maximum tokenization (e.g., Liu 1986; Liang 1986a, 1986b; Wang 1989; Jie 1989; Wang, Su, and Mo 1990; Jie, Liu, and Liang 1991a, b; Yeh and Lee 1991; Webster and Kit 1992; Chen and Liu 1992; Guo 1993; Wu and Su 1993; Nie, Jin, and Hannah 1994; Sproat et al.1996; Wu et al.1994; Li et al.1995; Sun and T'sou 1995; Wong et al.1995; Bai 1995; Sun and Huang 1996).
C92-1019 C92-4173 J96-3004
This definition is in fact a descriptive interpretation of the widely recommended conventional constructive forward maximum tokenization procedure (Liu 1986a, 1986b; Liang 1986, 1987; Chen and Liu 1992; Webster and Kit 1992).
C92-1019 C92-4173
This definition is in fact a descriptive interpretation of the widely recommended conventional constructive backward maximum tokenization procedure (Liu 1986a, 1986b; Liang 1986, 1987; Chen and Liu 1992; Webster and Kit 1992).
C92-1019 C92-4173
For example, Chen and Liu (1992) acknowledged that the heuristic of maximum matching alone has "many variations" and tested six different implementations.
C92-1019
While it may not be totally impossible to fully incorporate such knowledge and heuristics into the general framework of path evaluation and searching, they are apparently employed neither in Sproat et al.(1996) nor in Ma (1996).
J96-3004
Sentence, or string, tokenization, the process of mapping sentences from character strings to strings of words, is the initial step in natural language processing (Webster and Kit 1992).
C92-4173
Critical ambiguity in tokenization is the precise mathematical description of conventional concepts such as disjunctive ambiguity (Webster and Kit \[1992, 1108\], for example) and overlapping ambiguity (Sun and T'sou \[1995, 121\], for example).
C92-4173
Hidden ambiguity in tokenization is the precise mathematical description of conventional concepts such as conjunctive ambiguity (Webster and Kit \[1992, 1108\], for example), combinational ambiguity (Liang \[1987\], for example) and categorical ambiguity (Sun and T'sou \[1995, 121\], for example).
C92-4173
As Webster and Kit (1992, 1108) noted, "segmentation methods were invented one after another and seemed inexhaustible".
C92-4173
The weighted finite-state transducer model developed by Sproat et al.(1996) is another excellent representative example.
J96-3004
Unfortunately, in Webster and Kit (1992, 1108), they unnecessarily made the following overly strong claim: It is believed that all elemental methods are included in this model.
C92-4173
As a representa584 Guo Critical Tokenization tive example, in Webster and Kit (1992, 1108), both conjunctive (combinational) and disjunctive (overlapping) ambiguities are defined in the manner given below.
C92-4173
As all critical tokenizations are minimal elements on the word string cover relationship, the existence of critical ambiguity in tokenization implies that the "most powerful and commonly used" (Chen and Liu 1992, 104) principle of maximum tokenization would not be effective in resolving critical ambiguity in tokenization and implies that other means such as statistical inferencing or grammatical reasoning have to be introduced.
C92-1019
A good representative work is by Kit and his colleagues (Jie 1989; Jie, Liu, and Liang 1991a, b; Webster and Kit 1992), who proposed a three-dimensional structural tokenization model.
C92-4173
As Chen and Liu (1992, 104) noted, "there are a few variations of the sense of maximal matching".
C92-1019
As Webster and Kit (1992, 1108) acknowledged, different realizations of the principle "were invented one after another and seemed inexhaustible".
C92-4173
Institute of Systems Science, National University of Singapore, Kent Ridge, Singapore 119597; e-maih guojin@iss.nns.sg (~) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 4 The tokenization problem exists in almost all natural languages, including Japanese (Yosiyuki, Takenobu, and Hozumi 1992), Korean (Yun, Lee, and Rim 1995), German (Pachunke et al.1992), and English (Garside, Leech, and Sampson 1987), in various media, such as continuous speech and cursive handwriting, and in numerous applications, such as translation, recognition, indexing, and proofreading.
C92-4195
Since in written Chinese there is no explicit word delimiter (equivalent to the blank space in written English), the problem of Chinese sentence tokenization has been the focus of considerable research efforts, and significant advancements have been made (e.g., Bai 1995; Zhang et al.1994; Chen and Liu 1992; Chiang et al.1992; Fan and Tsai 1988; Gan 1995; Gan, Palmer, and Lua 1996; Guo 1993; He, Xu, and Sun 1991; Huang 1989; Huang and Xia 1996; Jie 1989; Jie, Liu, and Liang 1991a, 1991b; Jin and Chen 1995; Lai et al.1992; Li et al.1995; Liang 1986, 1987, 1990; Liu 1986a, 1986b; Liu, Tan, and Shen 1994; Lua 1990, 1994, and 1995; Ma 1996; Nie, Jin, and Hannan 1994; Sproat and Shih 1990; Sproat et al.1996; Sun and T'sou 1995; Sun and Huang 1996; Tung and Lee 1994; Wang, Su, and Mo 1990; Wang 1989; Wang, Wang, and Bai 1991; Wong et al.1995; Wong et al.1994; Wu et al.1994; Wu and Su 1993; Yao, Zhang, and Wu 1990; Yeh and Lee 1991; Zhang, Chen, and Chen 1991).
C92-1019 J96-3004 J96-4004
For example, Seo and Simmons (1989) introduced the concept of the syntactic graph, which is, in essence, a union of all possible parse trees.
J89-1002
