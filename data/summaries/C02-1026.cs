For example, Abney et al.(2000) used 5; Ittycheriah et al.(2001), 31; Hovy et al.(2001), 140; Harabagiu et al.(2001), 8,797.
A00-1041
They are not named entities that have been the cornerstones of many. high performance QA systems (Srihari and Li 2000; Harabagiu et al.2001). By reranking we mean the following.
A00-1023
If the expected answer types are typical named entities, information extraction engines (Bikel et al.1999, Srihari and Li 2000) are used to extract candidate answers.
A00-1023
(1) Question Analysis: We used an in-house parser, CONTEX (Hermjakob 2001), to parse and analyze questions and relied on BBN’s IdentiFinder (Bikel et al., 1999) to provide basic named entity extraction capability.
W01-1203
(4) Answer ranking – assign scores to candidate answers according to their frequency in top ranked passages (Abney et al.2000; Clarke et al.2001), similarity to candidate answers extracted from external sources such as the web (Brill et al.2001; Buchholz 2001) or WordNet (Harabagiu et al.2001; Hovy et al.2001), density, distance, or order of question keywords around the candidates, similarity between the dependency structures of questions and candidate answers (Harabagiu et al.2001; Hovy et al.2001; Ittycheriah et al.2001), and match of expected answer types.
A00-1041
Please refer to (Hovy et al.2002) for more detail.
C02-1042
