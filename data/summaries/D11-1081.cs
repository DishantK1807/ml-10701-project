(Chan et al., 2007) to test scalability. A word level
P07-1005
Christoph Tillmann and Tong Zhang. 2006. A discrim-
P06-1091
et al., 2006; Tillmann and Zhang, 2006; Watanabe
P06-1091
2006; Li et al., 2009), MIRA (Watanabe et al., 2007;
D07-1080 D09-1005 P09-1067
David Chiang, Kevin Knight, and Wei Wang. 2009.
N09-1025
(Zhang et al., 2008). Some minimum rules in the
C08-1136
scribed in Chiang (2007) and also added rules (Sec-
J07-2003
weights. * means significantly (Koehn, 2004) better than MERT (p < 0.01).
W04-3250
Hao Zhang, Daniel Gildea, and David Chiang. 2008. Ex-
C08-1136
MERT. We use MERT (Och, 2003) to training 8
J03-1002 P03-1021
Philipp Koehn. 2004. Statistical significance tests for
W04-3250
as Chiang (2007) including 4 rule scores (direct and
J07-2003
Minimum error rate training (Och, 2003) is perhaps
J03-1002 P03-1021
2007; Chiang et al., 2009). Obviously, using much
N09-1025
ZhifeiLiandJasonEisner. 2009. First-andsecond-order
D09-1005 P09-1067
coding (Germann et al., 2001). Such types of local
P01-1030
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
N04-1023
final-and (Koehn et al., 2003) to generate symmet-
N03-1017
trastive estimation (Smith and Eisner, 2005; Poon et
P05-1044
Franz Josef Och. 2003. Minimum error rate training in
J03-1002 P03-1021
et al., 2007; Blunsom et al., 2008; Chiang et al.,
D08-1023 P08-1024
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
P08-1023
tight consistent initial phrases (Chiang, 2007).
J07-2003
(SCFG) (Chiang, 2007) based translation. The el-
J07-2003
Phil Blunsom and Miles Osborne. 2008. Probabilistic
D08-1023 P08-1023 P08-1024
Michael Collins. 2002. Discriminative training methods
W02-1001
totrainmanyfeatures: perceptron(Shenetal., 2004;
N04-1023
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
J07-2003 P07-1005
et al., 2008; Li and Eisner, 2009) is a compact repre-
D09-1005 P09-1067
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
D08-1023 P08-1023 P08-1024
We use GIZA++ (Och and Ney, 2003) to perform
J03-1002 P03-1021
al., 2008; Blunsom and Osborne, 2008). The com-
D08-1023 P08-1024
Chiang et al., 2009), gradient descent (Blunsom et
N09-1025
(Zhang et al., 2008) bi-spans with the help of word
C08-1136
weight vector (Collins, 2002) to avoid overfitting
W02-1001
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
P05-1044
Cohn and Blunsom, 2009).
D09-1037 P09-1088
2006; Watanabe et al., 2007; Chiang et al., 2009),
D07-1080 N09-1025
mation (Smith and Eisner, 2005; Poon et al., 2009)
N09-1024 P05-1044
Liangetal., 2006), minimumrisk(SmithandEisner,
P06-1096
ging” algorithm (Brown et al., 1993) and greedy de-
J93-2003
phrase translation (Koehn et al., 2003) or a transla-
N03-1017
allel tree from the Zhang et al. (2008) algorithm,
C08-1136
pler (Blunsom et al., 2009) which has been used for
P09-1088
Franz Josef Och and Hermann Ney. 2003. A system-
J03-1002 P03-1021
David Chiang. 2007. Hierarchical phrase-based transla-
J07-2003
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
D09-1005 P09-1067
Franz J. Och and Hermann Ney. 2002. Discriminative
P02-1038
chronous grammar induction (Blunsom et al., 2009;
P09-1088
Chiang (2007). We also add these rules to the rule
J07-2003
David A. Smith and Jason Eisner. 2006. Minimum risk
P06-2101
Discriminative model (Och and Ney, 2002) can
P02-1038
Chris Dyer. 2010. Two monolingual parses are better
N10-1033
latent variable as describe in Blunsom et al.(2008).
D08-1023 P08-1024
(Dyer, 2010) over the source sentence f and the ref-
N10-1033
TrevorCohnandPhilBlunsom. 2009. ABayesianmodel
D09-1037 P09-1088
