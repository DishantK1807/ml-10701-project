<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<title>Boosting applied to tagging and PP attachment</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99</booktitle>
<pages>38--45</pages>
<marker>1999</marker>
<rawString>1999. Boosting applied to tagging and PP attachment. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99), pages 38–45. 4.2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Allen</author>
<author>Mark Core</author>
</authors>
<title>Draft of DAMSL: Dialog act markup in several layers</title>
<date>1997</date>
<note>Unpublished manuscript. 2.5. Collin</note>
<contexts>
<context>5. Dialogue acts The annotation of dialogue acts is based on the annotation of predicate structure. We annotate each utterance using a multidimensional annotation scheme partially based in the DAMSL (Allen and Core, 1997). 3. The Italian LUNA Corpus 3.1. General description of the data The Italian corpus what is being currently transcribed and annotated consists of two different data sets: a set of human-human sponta</context>
</contexts>
<marker>Allen, Core, 1997</marker>
<rawString>James Allen and Mark Core. 1997. Draft of DAMSL: Dialog act markup in several layers. Unpublished manuscript. 2.5. Collin F. Baker, Charles J. Fillmore, and John B. Lowe.</rawString>
</citation>
<citation valid="true">
<title>The berkeley framenet project</title>
<date>1998</date>
<journal>Association for Computational Linguistics</journal>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics</booktitle>
<volume>2</volume>
<pages>86--90</pages>
<location>Morristown, NJ, USA</location>
<marker>1998</marker>
<rawString>1998. The berkeley framenet project. In Proceedings of the 17th international conference on Computational linguistics, pages 86–90, Morristown, NJ, USA. Association for Computational Linguistics. 2.3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Basili</author>
<author>Maria Teresa Pazienza</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Lexicalizing a Shallow Parser</title>
<date>1999</date>
<booktitle>In Conference sur le Traitment Automatique des Langues Naturelles (TALN99</booktitle>
<pages>3--4</pages>
<contexts>
<context>3.4. Morphosyntactic annotation The transcribed data is annotated with Part of Speech and morphosyntactic features on the word level and the words grouped in syntactic chunks using the Chaos Parser ((Basili et al., 1999)). 3.5. Semantic annotation on the attribute-value level After an analysis of a set of dialogues we defined a hierarchy of 55 concept names and constraints for the possible values. This representatio</context>
</contexts>
<marker>Basili, Pazienza, Zanzotto, 1999</marker>
<rawString>Roberto Basili, Maria Teresa Pazienza, and Fabio Massimo Zanzotto. 1999. Lexicalizing a Shallow Parser. In Conference sur le Traitment Automatique des Langues Naturelles (TALN99). 3.4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H´el`ene Bonneau-Maynard</author>
<author>Sophie Rosset</author>
</authors>
<title>A semantic representation for spoken dialogues</title>
<date>2003</date>
<booktitle>In Proceedings of Eurospeech, Geneva. Semantizer available at: http://www.limsi.fr/Individu/hbm</booktitle>
<volume>2</volume>
<pages>3--5</pages>
<contexts>
<context> recommendations made in (EAGLES, 1996). 2.2. Domain-attribute level At this level semantic segments are being annotated following a similar approach to the used for the French MEDIA dialogue corpus (Bonneau-Maynard and Rosset, 2003). Domain knowledge is organized in a concept dictionary for each application domain. The concept dictionary contains: Concepts: corresponding to the attributes of the annotation Values Constraints on</context>
<context>ral expressions We use this concept dictionary to annotate a first set of 140 dialogues on the domain attribute level as presented in the example (1)2. The tool used for the annotation is Semantizer (Bonneau-Maynard and Rosset, 2003) (fig. 1), a tool that was previously used for the annotation of the MEDIA corpus. (1) Operator: sto guardando [lex=filler] l’ [avete aperta]concept1 [stamattina]concept2 &lt;concept1 action:open&gt; &lt;conc</context>
</contexts>
<marker>Bonneau-Maynard, Rosset, 2003</marker>
<rawString>H´el`ene Bonneau-Maynard and Sophie Rosset. 2003. A semantic representation for spoken dialogues. In Proceedings of Eurospeech, Geneva. Semantizer available at: http://www.limsi.fr/Individu/hbm. 2.2., 3.5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>EAGLES</author>
</authors>
<title>Recomendations for the Morphosyntactic Annotation of Corpora. 2.1. Taku Kudo. Crf++. Available online at http://crfpp</title>
<date>1996</date>
<journal>sourceforge.net</journal>
<volume>4</volume>
<publisher>John</publisher>
<contexts>
<context>ng annotated with Part of Speech tags, morphosyntactic information and segmented based on syntactic constituency. For the POS-tags and morphosyntactic features, we follow the recommendations made in (EAGLES, 1996). 2.2. Domain-attribute level At this level semantic segments are being annotated following a similar approach to the used for the French MEDIA dialogue corpus (Bonneau-Maynard and Rosset, 2003). Dom</context>
</contexts>
<marker>EAGLES, 1996</marker>
<rawString>EAGLES. 1996. Recomendations for the Morphosyntactic Annotation of Corpora. 2.1. Taku Kudo. Crf++. Available online at http://crfpp. sourceforge.net/. 4.1. John Lafferty, Andrew McCallum, and Fernando Pereira.</rawString>
</citation>
<citation valid="true">
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
<date>2001</date>
<booktitle>In Proceedings of 18th International Conference on Machine Learning</booktitle>
<pages>282--289</pages>
<marker>2001</marker>
<rawString>2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of 18th International Conference on Machine Learning, pages 282–289. 4.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Lewis</author>
<author>Jason Catlett</author>
</authors>
<title>Heterogeneous uncertainty sampling for supervised learning</title>
<date>1994</date>
<booktitle>In Proceedings of ICML-94, 11th International Conference on Machine Learning</booktitle>
<pages>148--156</pages>
<location>New Brunswick, US</location>
<contexts>
<context>or manual annoFigure 1: Semantizer Screenshot tation and thus reduces the number of supervised training examples needed to achieve a given level of performance. We use an uncertainty-based AL method (Lewis and Catlett, 1994) which selects for labeling the examples that the learner is least confident about. To use this method, we need a learner and an associated confidence measure. The choice of one or the other is not c</context>
</contexts>
<marker>Lewis, Catlett, 1994</marker>
<rawString>David Lewis and Jason Catlett. 1994. Heterogeneous uncertainty sampling for supervised learning. In Proceedings of ICML-94, 11th International Conference on Machine Learning, pages 148–156, New Brunswick, US. 4.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Detecting errors in corpora using support vector machines</title>
<date>2002</date>
<journal>Association for Computational Linguistics</journal>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics</booktitle>
<volume>4</volume>
<pages>1--7</pages>
<location>Morristown, NJ, USA</location>
<contexts>
<context>statistical algorithm, the examples receiving low confidence are likely to be erroneous or hard examples. In (Abney et al., 1999) they use the highest weighted examples by the boosting algorithm, in (Nakagawa and Matsumoto, 2002) they use the weight assigned by their SVM classifier, in (Raymond and Riccardi, 2008) they use the conditional probability provided by the CRF. 5. Active Annotation We implement an Active Annotation</context>
</contexts>
<marker>Nakagawa, Matsumoto, 2002</marker>
<rawString>Tetsuji Nakagawa and Yuji Matsumoto. 2002. Detecting errors in corpora using support vector machines. In Proceedings of the 19th international conference on Computational linguistics, pages 1–7, Morristown, NJ, USA. Association for Computational Linguistics. 4.2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Ron Artstein</author>
</authors>
<title>Further developments in anaphoric annotation: the ARRAU corpus</title>
<date>2008</date>
<booktitle>In Proc. of LREC, Marrakesh</booktitle>
<volume>2</volume>
<contexts>
<context>in knowledge we define a set of frames for each domain. 2.4. Coreference The annotation of coreference follows a scheme close to the used in the annotation of dialogues of the TRAINS corpus in ARRAU (Poesio and Artstein, 2008). Markables are annotated with giveness and relatedness to previously mentioned objects. 2.5. Dialogue acts The annotation of dialogue acts is based on the annotation of predicate structure. We annot</context>
</contexts>
<marker>Poesio, Artstein, 2008</marker>
<rawString>Massimo Poesio and Ron Artstein. 2008. Further developments in anaphoric annotation: the ARRAU corpus. In Proc. of LREC, Marrakesh. 2.4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance Ramshaw</author>
<author>Mitch Marcus</author>
</authors>
<title>Text chunking using transformation-based learning</title>
<date>1995</date>
<journal>Computational Linguistics</journal>
<booktitle>In David Yarovsky and Kenneth Church, editors, Proceedings of the Third Workshop on Very Large Corpora</booktitle>
<volume>5</volume>
<pages>82--94</pages>
<publisher>Association for</publisher>
<location>Somerset, New Jersey</location>
<contexts>
<context> training CRF 5.2. Segmentation and classification problem The model is a CRF tagger used to do concept segmentation and classification. The sequence labeling problem is solved by BIO representation (Ramshaw and Marcus, 1995): each word in the sequence is associated with the corresponding concept together with the B (Begin) or I (Inside) markers to make the concepts boundaries explicit3: tags :vDC-B vDC-I vDC-I null null</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Lance Ramshaw and Mitch Marcus. 1995. Text chunking using transformation-based learning. In David Yarovsky and Kenneth Church, editors, Proceedings of the Third Workshop on Very Large Corpora, pages 82–94, Somerset, New Jersey. Association for Computational Linguistics. 5.2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Raymond</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Generative and discriminative algorithms for spoken language understanding</title>
<date>2007</date>
<booktitle>In Interspeech</booktitle>
<location>Antwerp, Belgium</location>
<contexts>
<context> situation where we process manual transcription: we do not have real-time constraints nor the need to be robust to the recognition errors. The discriminant algorithms in this situation are accurate (Raymond and Riccardi, 2007) and able to integrate many different knowledge sources. In addition to these abilities, Conditional Random Fields (CRF) (Lafferty et al., 2001) provide the conditional probability over the whole ann</context>
</contexts>
<marker>Raymond, Riccardi, 2007</marker>
<rawString>Christian Raymond and Giuseppe Riccardi. 2007. Generative and discriminative algorithms for spoken language understanding. In Interspeech, Antwerp, Belgium, August. 4.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Raymond</author>
<author>Giuseppe Riccardi</author>
</authors>
<title>Learning with noisy supervision for spoken language understanding</title>
<date>2008</date>
<booktitle>In ICASSP</booktitle>
<pages>183--188</pages>
<location>Las Vegas, USA</location>
<contexts>
<context>or hard examples. In (Abney et al., 1999) they use the highest weighted examples by the boosting algorithm, in (Nakagawa and Matsumoto, 2002) they use the weight assigned by their SVM classifier, in (Raymond and Riccardi, 2008) they use the conditional probability provided by the CRF. 5. Active Annotation We implement an Active Annotation approach (figure 3) in order to reduce the human effort. This approach is based on st</context>
</contexts>
<marker>Raymond, Riccardi, 2008</marker>
<rawString>Christian Raymond and Giuseppe Riccardi. 2008. Learning with noisy supervision for spoken language understanding. In ICASSP, pages 183–188, Las Vegas, USA. (Accepted). 4.2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kepa Joseba Rodriguez</author>
<author>Stefanie Dipper</author>
<author>Michael G¨otze</author>
<author>Massimo Poesio</author>
<author>Giuseppe Riccardi</author>
<author>Christian Raymond</author>
<author>Joanna Rabiega-Wi´sniewska</author>
</authors>
<title>Standoff coordination for multi-tool annotation in a dialogue corpus</title>
<date>2007</date>
<booktitle>In Linguistic Annotation Workshop</booktitle>
<pages>2</pages>
<location>Prague</location>
<marker>Rodriguez, Dipper, G¨otze, Poesio, Riccardi, Raymond, Rabiega-Wi´sniewska, 2007</marker>
<rawString>Kepa Joseba Rodriguez, Stefanie Dipper, Michael G¨otze, Massimo Poesio, Giuseppe Riccardi, Christian Raymond, and Joanna Rabiega-Wi´sniewska. 2007. Standoff coordination for multi-tool annotation in a dialogue corpus. In Linguistic Annotation Workshop, Prague. http://www.aclweb.org/anthology-new/W/W07/W071524.pdf. 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>BoosTexter: A boosting-based system for text categorization. Machine Learning, 39:135–168. Available online at http://www.cs.princeton.edu/ ˜schapire/boostexter.html. 2</title>
<date>2000</date>
<journal>IEEE Computer Society</journal>
<booktitle>In ICTAI: Proceedings of the 18th IEEE International Conference on Tools with Artificial Intelligence</booktitle>
<volume>4</volume>
<pages>323--331</pages>
<location>Washington, DC, USA</location>
<contexts>
<context>tion is done by a classifier. In this case, no supervision is necessary. The new introduced value by the human annotators will be covered in the next active turn. The classifier chosen is BoosTexter (Schapire and Singer, 2000) an implementation of the boosting algorithm. method attribute chunk value computerdel mio componentHardware computer pc Classifier azione cancelli cancellare dei problemiproblema su intranet probl</context>
</contexts>
<marker>Schapire, Singer, 2000</marker>
<rawString>Robert E. Schapire and Yoram Singer. 2000. BoosTexter: A boosting-based system for text categorization. Machine Learning, 39:135–168. Available online at http://www.cs.princeton.edu/ ˜schapire/boostexter.html. 2 Christopher T. Symons, Nagiza F. Samatova, Ramya Krishnamurthy, Byung H. Park, Tarik Umar, David Buttler, Terence Critchlow, and David Hysom. 2006. Multicriterion active learning in conditional random fields. In ICTAI: Proceedings of the 18th IEEE International Conference on Tools with Artificial Intelligence, pages 323– 331, Washington, DC, USA. IEEE Computer Society. 4.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gokhan Tur</author>
<author>Mazin Rahim</author>
<author>Dilek Hakkani-T¨ur</author>
</authors>
<title>Active labeling for spoken language understanding</title>
<date>2003</date>
<booktitle>In Eurospeech</booktitle>
<location>Geneva</location>
<marker>Tur, Rahim, Hakkani-T¨ur, 2003</marker>
<rawString>Gokhan Tur, Mazin Rahim, and Dilek Hakkani-T¨ur. 2003. Active labeling for spoken language understanding. In Eurospeech, Geneva. 1.</rawString>
</citation>
</citationList>
</algorithm>

