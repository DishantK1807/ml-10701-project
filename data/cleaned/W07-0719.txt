BusTUC A natural language bus route oracle Tore A m b l e Dept.
of computer and information science University of Trondheim Norway, N-7491 amble@idi, nt nu.
no Abstract The paper describes a natural language based expert system route advisor for the public bus transport in Trondheim, Norway.
The system is available on the Internet,and has been intstalled at the bus company's web server since the beginning of 1999.
The system is bilingual, relying on an internal language independent logic representation.
In between the question and the answer is a process of lexical analysis, syntax analysis, semantic analysis, pragmatic reasoning and database query processing.
One could argue that the information content could be solved by an interrogation, whereby the customer is asked to produce 4 items: s t a t i o n of departure, station of arrival, earliest departure timeand/or latest arrival time.
It Introduction A natural language interface to a computer database provides users with the capability of obtaining information stored in the database by querying the system in a natural language (NL).
With a natural language as a means of communication with a computer system, the users can make a question or a statement in the way they normally think about the information being discussed, freeing them from having to know how the computer stores or processes the information.
The present implementation represents a a major effort in bringing natural language into practical use.
A system is developed that can answer queries about bus routes, stated as natural language texts, and made public through the Internet World Wide Web is a myth that natural language is a better way of communication because it is "natural language".
The challenge is to prove by demonstration that an NL system can be made that will be preferred to the interrogative mode.
To do that, the system has to be correct, user friendly and almost complete within the actual domain.
P r e v i o u s Efforts, C H A T 8 0, P R A T 8 9 and HSQL Trondheim is a small city with a university and 140000 inhabitants.
Its central bus systems has 42 bus lines, serving 590 stations, with 1900 departures per day (in average).
T h a t gives approximately 60000 scheduled bus station passings per day, which is somehow represented in the route data base.
The starting point is to automate the function of a route information agent.
The following example of a system response is using an actual request over telephone to the local route information company: Hi, I live in Nidarvoll and tonight i must reach a train to Oslo at 6 oclock.
The system, called BusTUC is built upon the classical system CHAT-80 (Warren and Pereira, 1982).
CHAT-80 was a state of the art natural language system that was impressive on its own merits, but also established Prolog as a viable and competitive language for Artificial Intelligence in general.
The system was a brilliant masterpiece of software, efficient and sophisticated.
The natural language system was connected to a small query system for international geography.
The following query could be analysed and answered in a split second: Which country bordering the Mediterranean borders a country that is bordered by a country whose population exceeds the population of India?
(The answer 'Turkey' has become incorrect as time has passed.
The irony is that Geography was chosen as a domain without time.) and a typical answer would follow quickly: Bus number 54 passes by Nidarvoll skole at 1710 and arrives at Trondheim Railway Station at 1725.
The abi!ity to answer ridiculously long queries is of course not the main goal.
The main lesson is that complex sentences are analysed with a proper understanding without sacrificing efficiency.
Any superfificial pattern matching technique would prove futile sooner or later.
Making a N o r w e g i a n CHAT-80, PRAT-89 At the University of Trondheim (NTNU), two students made a Norwegian version of CHAT-80,called PRAT-89 (Teigen and Vetland, 1988),(Teigen and Vetland, 1989).
(Also, a similar Swedish project SNACK-85 was reported).
The dictionary was changed from English to Norwegian together with new rules for morphological analysis.
The change of grammar from English to Norwegian proved to be amazingly easy.
It showed that the langauges were more similar than one would believe, given that the languages are incomprehensible to each other's communities.
After changing the dictionary and graramar, the following Norwegian query about the same domain could be answered correctly in a few seconds.
Hvilke afrikanske land som hat en befolkning stoerre enn 3 millioner og mindre enn 50 millioner og er nord for Botswana og oest for Libya hat en hovedstad som hat en befolkning stoerre enn 100 tusen.
Coupling the s y s t e m to an SQL database.
After the remodelling, the system could answer queries in "Scandinavian" to an internal hospital database as well as CHAT-80 could answer Geography questions.
HSQL produced a Prolog-like code FOL (First Order Logic) for execution.
A mapping from FOL to the data base Schema was defined, and a translator from FOL to SQL was implemented.
The example Hvilke menn ligger i en kvinnes seng?
(Which men lie in a woman's bed?
) would be translated dryly into the SQL query: SELECT DISTINCT T3.name,Tl.sex,T2.reg_no,T3.sex, T4.reg_no,T4.bed_no,T5.hosp_no,T5.ward_no FROM PATIENT TI,OCCUPANCY T2,PATIENT T3, OCCUPANCY T4,WARD T5 WHERE (Tl.sex='f') AND (T2.reg_no=Tl.reg_no) AND (T3.sex='m') AND (T4.reg_no=T3.reg_no) AND (T4.bed_no=T2.bed_no) AND (T5.hosp_no=T4.hosp_no) AND (T5.ward_no=T4.ward_no) 2.3 T h e T h e U n d e r s t a n d i n g C o m p u t e r The HSQL was a valuable experience in the effort to make transportable natural language interfaces.
However, the underlying system CHAT-80 restricted the further development.
After the HSQL Project was finished, an internal reseach project TUC (the Understanding Computer) was initiated at NTNU to carry on the results from HSQL.
The project goals differed from those of HSQL in a number of ways, and would not be concerned with multimedia interfaces. On the other hand, portability and versatility were made central issues concerning the generality of the language and its applications.
The research goals could be summarised as to Give computers an operational understanding of natural language.
 Build intelligent systems with natural language capabilities.
 Study common sense reasoning in natural language.
A test criterion for the understanding capacity is that after a set of definitions in a Naturally Readable Logic, NRL, the system's answer to queries in NRL should conform to the answers of an idealised rational agent.
( A translation is beside the point o.f being a long query in Norwegian.) 2.2 HSQL H e l p S y s t e m for SQL A Nordic project HSQL (Help System for SQL) was accomplished in 1988-89 to make a joint Nordic effort interfaces to databases.
The HSQL project was led by the Swedish State Bureau (Statskontoret), with participants from Sweden, Denmark, Finland and Norway (Amble et al., 1990).
The aim of HSQL was to build a natural language interface to SQL databases for the Scandinavian languages Swedish, Danish and Norwegian.
These languages are very similar, and the Norwegian version of CHAT-80 was easily extended to the other Scandinavian languages.
Instead of Geography, a more typical application area was chosen to be a query system for hospital administration.
We decided to target an SQL database of a hospital administration which had been developed already.
The next step was then to change the domain of discourse from Geography to hospital administration, using the same knowledge representation techniques used in CHAT-80.
A semantic model of this domain was made, and then implemented in the CHAT-80 framework.
The modelling technique that proved adequate was to use an extended Entity Relationship (ER) model with a class (type) hierarchy, attributes belonging to each class, single inheritance of attributes and relationships.
Every man that lives loves Mary.
John is a man.
John lives.
Who loves Mary?
==> John 3 Anatomy of the bus route oracle The main components of the bus route information systems are:  A parser system, consisting of a dictionary, a lexical processor, a grammar and a parser.
 A knowledge base (KB), divided into a semantic KB and an application KB  A query processor, contalng a routing logic system, and a route data base.
The system is bilingual and contains a double set of dictionary, morphology and grammar.
Actually, it detects which language is most probable by counting the number of unknown words related to each language, and acts accordingly.
The grammars are surprisingly similar, but no effort is made to coalesce them.
The Norwegian grammar is slightly bigger than the English grammar, mostly because it is more elaborated but also because Norwegian allows a freer word order.
3.1 Features
of BusTUC For the Norwegian systems, the figures give an indication of the size of the domain: 420 nouns, 150 verbs, 165 adjectives, 60 prepositions, etc.
There are 1300 grammar rules ( 810 for English) although half of the rules are very low level.
The semantic net described below contains about 4000 entries.
A big name table of 3050 names in addition to the official station names, is required to capture the variety of naming.
A simple spell correction is a part of the system ( essentially 1 character errors).
The pragmatic reasoning is needed to translate the output from the parser to a route database query language . This is done by a production system called Pragma, which acts like an advanced rewriting system with 580 rules.
In addition, there is another rule base for actually generating the natural language answers (120 rules).
The system is mainly written in Prolog (Sicstus Prolog 3.7), with some Perl programs for the communication and CGI-scripts.
At the moment, there are about 35000 lines of programmed Prolog code (in addition to route tables which are also in Prolog).
Average response time is usually less than 2 seconds, but there are queries that demand up to 10 seconds.
The error rate for single, correct, complete and relevant questions is about 2 percent.
NRL is defined in a closed context.
Thus interfaces to other systems are in principle defined through simulating the environment as a dialogue partner.
TUC is a prototypical natural language processor for English written in Prolog.
It is designed to be a general purpose easily adaptable natural language processor.
It consists of a general grammar for a subset of English, a semantic knowledge base, and modules for interfaces to other interfaces like UNIX, SQL-databases and general textual information sources.
2.4 The
TABOR Project It so happened that a Universtity Project was starteded in 1996, called T A B O R ( " Speech based user interfaces and reasoning systems "), with the aim of building an automatic public transport route oracle, available over the public telephone.
At the onset of the project, the World Wide Web was fresh, and not as widespread as today, and the telephone was still regarded as the main source of information for the public.
Since then, the Internet became the dominant medium, and it is as likeley to find a computer with Internet connection, as to find a local busroute table.
( The consequtive wide spreading of cellular phones changed the picture in favour of the telephone, but that is another story).
It was decided that a text based information system should be built, regardless of the status of the speech rocgnition and speech synthesis effort, which proved to lag behind after a while.
The BusTUC system The resulting system BusTUC grew out as a natural application of TUC, and an English prototype could be built within a few months (Bratseth, 1997).
Since the summer 1996, the prototype was put onto the Internet, and been developed and tested more or less continually until today.
The most important extension was that the system was made bilingual (Norwegian and English) during the fall 1996.
In spring 1999, the BusTUC was finally adopted by the local bus company in Trondheim ( A/S Trondheim Trafikkselskap), which set up a server ( a 300 MHz PC with Linux).
Until today, over 150.000 questions have been answered, and BusTUC seems to stabilize and grow increasingly popular.
3.2 The
Parser S y s t e m The G r a m m a r S y s t e m The grammar is based on a simple grammar for statements, while questions and commands are derived by the use of movements.
The grammar 3 fiformalism which is called Consensical Grammar, (CONtext SENSitive CompositionAL Grammar) is an easy to use variant of Extraposition Grammar (Pereira and Warren, 1980), which is a generalisation of Definite Clause Grammars.
Compositional grammar means that the semantics of a a phrase is composed of the semantics of the subphrases; the basic constituents being a form of verb complements.
As for Extraposition grammars, a grammar is translated to Definite Clause Grammars, and executed as such.
A characteristic syntactic expression in Consensical G r a m m a r m a y define an incomplete construct in terms of a "difference " between complete constructs.
W h e n possible, the parser will use the subtracted part in stead of reading from the input, after a gap if necessary.
The effect is the same as for Exwhich is analysed as for which X is it true that the (X) person has a dog that barked? where the last line is analysed as a s t a t e m e n t . Movement is easily handled in Consensical Grammar without making special phrase rules for each kind of movement.
The following example shows how TUC manages a variety of analyses using movements: Max said Bill thought Joe believed Fido Barked.
Who said Bill thought Joe believed Fido barked?
Who did Max say thought Joe believed Fido barked? traposition grammars, but the this format is more intuitive.
Examples of grammar rules.
Who did Max say Bill thought believed Fido barked?
T h e parser The experiences with Consensical grammars are a bit mixed however.
The main problem is the parsing method itself, which is top down with backtracking.
Many principles that would prove elegant for small domains turned out to be too costly for larger domains, due to the wide variety of modes of expressions, incredible ambiguities and the sheer size of the covered language.
The disambiguation is a major problem for small grammars and large languages, and was solved by the following guidelines:  a semantic type checking was integrated into the parser, and would help to discard sematica/ly wrong parses from the start.
 a heuristics was followed that proved almost irreproachable: The longest possible phrase of a category that is semantically correct is in most cases the preferred interpretation.
 due to the perplexity of the language, some committed choices (cuts) had to be inserted into the grammar at strategic places.
As one could fear however, this implied that wrong choices being made at some point in the parsing could not be recovered by backtracking.
These problems also made it imperative to introduce a timeout on the parsing process of embarassing 10 seconds.
Although most sentences, would be parsed within a second, some legal sentences of moderate size actually need this time.
4 Example: Whose dog barked? is analysed as if the sentence had been Who has a dog t h a t barked? which is analysed as Which p e r s o n has a dog t h a t barked? fi3.3 The semantic knowledge base Adaptability means that the system does not need to be reprogrammed for each new application.
The design principle of TUC is that most of the changes are made in a tabular semantic knowledge base, while there is one general grammar and dictionary.
In general, the logic is generated automatically from the semantic knowledge base.
The nouns play a key role in the understanding part as they constitute the class or type hierarchy.
Nouns are defined in an a k i n d o f hierarchy.
The hierarchy is tree-structured with single inheritance.
The top level also constitute the top level ontology of TUC's world.
In fact, a type check of the compliances of verbs, nouns adjectives and prepositions is not only necessary for the semantic processing but is essential for the syntax analysis for the disambiguation as well.
In TUC, the legal combinations are carefully assembled in the semantic network, which then serves a dual purpose.
These semantic definitions are necessary to allow for instance the following sentences The dog saw a man with a telescope.
The man saw a dog with a telescope.
gives exactly the same code.
% Type of question % tuc is a program % A is a real bus % B isa saturday % Nidar is a place % D is an event Y.
C was known at D Y.
E is an event in C action(go,E), Y.
the action of E is Go actor(A,E), Y.
the actor of E is A srel(to,place,nidar,E),Y.
E is to nidar srel(on,time,B,E), y, E is on the saturday B to be treated differently because with telescope m a y modify the noun man but not the noun dog, while with telescope modifies the verb see, restricted to person.
The event parameter plays an important role in the semantics.
It is used for various purposes.
The most salient role is to identify a subset of time and space in which an action or event occured.
Both the actual time and space coordinates are connected to the actions through the event parameter.
Pragmatic reasoning The TQL is translated to a route database query language (BusLOG) which is actually a Prolog program.
This is done by a production system called Pragma, which acts like an advanced rewriting system with 580 rules.
In addition, there is another rule base for actually generating the natural language answers (120 rules).
4 Conclusions
The TUC approach has as its goal to automate the creation of new natural language interfaces for a well defined subset of the language and with a minimum of explicit programming.
The implemented system has proved its worth, and is interesting if for no other reason.
There is also an increasing interest from other bus companies and route information companies alike to get a similar system for their customers.
Further work remains to make the parser really efficient, and much work remains to make the language coverage complete within reasonable limits.
It is an open question whether the system of this kind will be a preferred way of offering information to the public.
If it is, it is a fair amount of work to make it a portable system that can be implemented elsewhere, also connecting various travelling agencies.
If not, it will remain a curiosity.
But anyway, a system like this will be a contribution to the development of intelligent systems.
3.4 The
Query Processor Event Calculus The semantics of the phrases are built up by a kind of verb complements, where the event play a central role.
The text is translated from Natural language into a form called TQL (Temporal Query Language/ TUC Query Language) which is a first order event calculus expression, a self contained expression containing the literal meaning of an utterance.
A formalism TQL that was defined, inspired by the Event Calculus by Kowalski and Sergot (Kowalski and Sergot, 1986).
The TQL expressions consist of predicates, functions, constants and variables.
The textual words of nouns and verbs are translated to generic predicates using the selected interpretation.
The following question Do you know whether the bus goes to Nidar on Saturday ? would give the TQL expression below.
BusTUC A natural language bus route oracle Tore A m b l e Dept.
of computer and information science University of Trondheim Norway, N-7491 amble@idi, nt nu.
no Abstract The paper describes a natural language based expert system route advisor for the public bus transport in Trondheim, Norway.
The system is available on the Internet,and has been intstalled at the bus company's web server since the beginning of 1999.
The system is bilingual, relying on an internal language independent logic representation.
In between the question and the answer is a process of lexical analysis, syntax analysis, semantic analysis, pragmatic reasoning and database query processing.
One could argue that the information content could be solved by an interrogation, whereby the customer is asked to produce 4 items: s t a t i o n of departure, station of arrival, earliest departure timeand/or latest arrival time.
It Introduction A natural language interface to a computer database provides users with the capability of obtaining information stored in the database by querying the system in a natural language (NL).
With a natural language as a means of communication with a computer system, the users can make a question or a statement in the way they normally think about the information being discussed, freeing them from having to know how the computer stores or processes the information.
The present implementation represents a a major effort in bringing natural language into practical use.
A system is developed that can answer queries about bus routes, stated as natural language texts, and made public through the Internet World Wide Web is a myth that natural language is a better way of communication because it is "natural language".
The challenge is to prove by demonstration that an NL system can be made that will be preferred to the interrogative mode.
To do that, the system has to be correct, user friendly and almost complete within the actual domain.
P r e v i o u s Efforts, C H A T 8 0, P R A T 8 9 and HSQL Trondheim is a small city with a university and 140000 inhabitants.
Its central bus systems has 42 bus lines, serving 590 stations, with 1900 departures per day (in average).
T h a t gives approximately 60000 scheduled bus station passings per day, which is somehow represented in the route data base.
The starting point is to automate the function of a route information agent.
The following example of a system response is using an actual request over telephone to the local route information company: Hi, I live in Nidarvoll and tonight i must reach a train to Oslo at 6 oclock.
The system, called BusTUC is built upon the classical system CHAT-80 (Warren and Pereira, 1982).
CHAT-80 was a state of the art natural language system that was impressive on its own merits, but also established Prolog as a viable and competitive language for Artificial Intelligence in general.
The system was a brilliant masterpiece of software, efficient and sophisticated.
The natural language system was connected to a small query system for international geography.
The following query could be analysed and answered in a split second: Which country bordering the Mediterranean borders a country that is bordered by a country whose population exceeds the population of India?
(The answer 'Turkey' has become incorrect as time has passed.
The irony is that Geography was chosen as a domain without time.) and a typical answer would follow quickly: Bus number 54 passes by Nidarvoll skole at 1710 and arrives at Trondheim Railway Station at 1725.
The abi!ity to answer ridiculously long queries is of course not the main goal.
The main lesson is that complex sentences are analysed with a proper understanding without sacrificing efficiency.
Any superfificial pattern matching technique would prove futile sooner or later.
Making a N o r w e g i a n CHAT-80, PRAT-89 At the University of Trondheim (NTNU), two students made a Norwegian version of CHAT-80,called PRAT-89 (Teigen and Vetland, 1988),(Teigen and Vetland, 1989).
(Also, a similar Swedish project SNACK-85 was reported).
The dictionary was changed from English to Norwegian together with new rules for morphological analysis.
The change of grammar from English to Norwegian proved to be amazingly easy.
It showed that the langauges were more similar than one would believe, given that the languages are incomprehensible to each other's communities.
After changing the dictionary and graramar, the following Norwegian query about the same domain could be answered correctly in a few seconds.
Hvilke afrikanske land som hat en befolkning stoerre enn 3 millioner og mindre enn 50 millioner og er nord for Botswana og oest for Libya hat en hovedstad som hat en befolkning stoerre enn 100 tusen.
Coupling the s y s t e m to an SQL database.
After the remodelling, the system could answer queries in "Scandinavian" to an internal hospital database as well as CHAT-80 could answer Geography questions.
HSQL produced a Prolog-like code FOL (First Order Logic) for execution.
A mapping from FOL to the data base Schema was defined, and a translator from FOL to SQL was implemented.
The example Hvilke menn ligger i en kvinnes seng?
(Which men lie in a woman's bed?
) would be translated dryly into the SQL query: SELECT DISTINCT T3.name,Tl.sex,T2.reg_no,T3.sex, T4.reg_no,T4.bed_no,T5.hosp_no,T5.ward_no FROM PATIENT TI,OCCUPANCY T2,PATIENT T3, OCCUPANCY T4,WARD T5 WHERE (Tl.sex='f') AND (T2.reg_no=Tl.reg_no) AND (T3.sex='m') AND (T4.reg_no=T3.reg_no) AND (T4.bed_no=T2.bed_no) AND (T5.hosp_no=T4.hosp_no) AND (T5.ward_no=T4.ward_no) 2.3 T h e T h e U n d e r s t a n d i n g C o m p u t e r The HSQL was a valuable experience in the effort to make transportable natural language interfaces.
However, the underlying system CHAT-80 restricted the further development.
After the HSQL Project was finished, an internal reseach project TUC (the Understanding Computer) was initiated at NTNU to carry on the results from HSQL.
The project goals differed from those of HSQL in a number of ways, and would not be concerned with multimedia interfaces. On the other hand, portability and versatility were made central issues concerning the generality of the language and its applications.
The research goals could be summarised as to Give computers an operational understanding of natural language.
 Build intelligent systems with natural language capabilities.
 Study common sense reasoning in natural language.
A test criterion for the understanding capacity is that after a set of definitions in a Naturally Readable Logic, NRL, the system's answer to queries in NRL should conform to the answers of an idealised rational agent.
( A translation is beside the point o.f being a long query in Norwegian.) 2.2 HSQL H e l p S y s t e m for SQL A Nordic project HSQL (Help System for SQL) was accomplished in 1988-89 to make a joint Nordic effort interfaces to databases.
The HSQL project was led by the Swedish State Bureau (Statskontoret), with participants from Sweden, Denmark, Finland and Norway (Amble et al., 1990).
The aim of HSQL was to build a natural language interface to SQL databases for the Scandinavian languages Swedish, Danish and Norwegian.
These languages are very similar, and the Norwegian version of CHAT-80 was easily extended to the other Scandinavian languages.
Instead of Geography, a more typical application area was chosen to be a query system for hospital administration.
We decided to target an SQL database of a hospital administration which had been developed already.
The next step was then to change the domain of discourse from Geography to hospital administration, using the same knowledge representation techniques used in CHAT-80.
A semantic model of this domain was made, and then implemented in the CHAT-80 framework.
The modelling technique that proved adequate was to use an extended Entity Relationship (ER) model with a class (type) hierarchy, attributes belonging to each class, single inheritance of attributes and relationships.
Every man that lives loves Mary.
John is a man.
John lives.
Who loves Mary?
==> John 3 Anatomy of the bus route oracle The main components of the bus route information systems are:  A parser system, consisting of a dictionary, a lexical processor, a grammar and a parser.
 A knowledge base (KB), divided into a semantic KB and an application KB  A query processor, contalng a routing logic system, and a route data base.
The system is bilingual and contains a double set of dictionary, morphology and grammar.
Actually, it detects which language is most probable by counting the number of unknown words related to each language, and acts accordingly.
The grammars are surprisingly similar, but no effort is made to coalesce them.
The Norwegian grammar is slightly bigger than the English grammar, mostly because it is more elaborated but also because Norwegian allows a freer word order.
3.1 Features
of BusTUC For the Norwegian systems, the figures give an indication of the size of the domain: 420 nouns, 150 verbs, 165 adjectives, 60 prepositions, etc.
There are 1300 grammar rules ( 810 for English) although half of the rules are very low level.
The semantic net described below contains about 4000 entries.
A big name table of 3050 names in addition to the official station names, is required to capture the variety of naming.
A simple spell correction is a part of the system ( essentially 1 character errors).
The pragmatic reasoning is needed to translate the output from the parser to a route database query language . This is done by a production system called Pragma, which acts like an advanced rewriting system with 580 rules.
In addition, there is another rule base for actually generating the natural language answers (120 rules).
The system is mainly written in Prolog (Sicstus Prolog 3.7), with some Perl programs for the communication and CGI-scripts.
At the moment, there are about 35000 lines of programmed Prolog code (in addition to route tables which are also in Prolog).
Average response time is usually less than 2 seconds, but there are queries that demand up to 10 seconds.
The error rate for single, correct, complete and relevant questions is about 2 percent.
NRL is defined in a closed context.
Thus interfaces to other systems are in principle defined through simulating the environment as a dialogue partner.
TUC is a prototypical natural language processor for English written in Prolog.
It is designed to be a general purpose easily adaptable natural language processor.
It consists of a general grammar for a subset of English, a semantic knowledge base, and modules for interfaces to other interfaces like UNIX, SQL-databases and general textual information sources.
2.4 The
TABOR Project It so happened that a Universtity Project was starteded in 1996, called T A B O R ( " Speech based user interfaces and reasoning systems "), with the aim of building an automatic public transport route oracle, available over the public telephone.
At the onset of the project, the World Wide Web was fresh, and not as widespread as today, and the telephone was still regarded as the main source of information for the public.
Since then, the Internet became the dominant medium, and it is as likeley to find a computer with Internet connection, as to find a local busroute table.
( The consequtive wide spreading of cellular phones changed the picture in favour of the telephone, but that is another story).
It was decided that a text based information system should be built, regardless of the status of the speech rocgnition and speech synthesis effort, which proved to lag behind after a while.
The BusTUC system The resulting system BusTUC grew out as a natural application of TUC, and an English prototype could be built within a few months (Bratseth, 1997).
Since the summer 1996, the prototype was put onto the Internet, and been developed and tested more or less continually until today.
The most important extension was that the system was made bilingual (Norwegian and English) during the fall 1996.
In spring 1999, the BusTUC was finally adopted by the local bus company in Trondheim ( A/S Trondheim Trafikkselskap), which set up a server ( a 300 MHz PC with Linux).
Until today, over 150.000 questions have been answered, and BusTUC seems to stabilize and grow increasingly popular.
3.2 The
Parser S y s t e m The G r a m m a r S y s t e m The grammar is based on a simple grammar for statements, while questions and commands are derived by the use of movements.
The grammar 3 fiformalism which is called Consensical Grammar, (CONtext SENSitive CompositionAL Grammar) is an easy to use variant of Extraposition Grammar (Pereira and Warren, 1980), which is a generalisation of Definite Clause Grammars.
Compositional grammar means that the semantics of a a phrase is composed of the semantics of the subphrases; the basic constituents being a form of verb complements.
As for Extraposition grammars, a grammar is translated to Definite Clause Grammars, and executed as such.
A characteristic syntactic expression in Consensical G r a m m a r m a y define an incomplete construct in terms of a "difference " between complete constructs.
W h e n possible, the parser will use the subtracted part in stead of reading from the input, after a gap if necessary.
The effect is the same as for Exwhich is analysed as for which X is it true that the (X) person has a dog that barked? where the last line is analysed as a s t a t e m e n t . Movement is easily handled in Consensical Grammar without making special phrase rules for each kind of movement.
The following example shows how TUC manages a variety of analyses using movements: Max said Bill thought Joe believed Fido Barked.
Who said Bill thought Joe believed Fido barked?
Who did Max say thought Joe believed Fido barked? traposition grammars, but the this format is more intuitive.
Examples of grammar rules.
Who did Max say Bill thought believed Fido barked?
T h e parser The experiences with Consensical grammars are a bit mixed however.
The main problem is the parsing method itself, which is top down with backtracking.
Many principles that would prove elegant for small domains turned out to be too costly for larger domains, due to the wide variety of modes of expressions, incredible ambiguities and the sheer size of the covered language.
The disambiguation is a major problem for small grammars and large languages, and was solved by the following guidelines:  a semantic type checking was integrated into the parser, and would help to discard sematica/ly wrong parses from the start.
 a heuristics was followed that proved almost irreproachable: The longest possible phrase of a category that is semantically correct is in most cases the preferred interpretation.
 due to the perplexity of the language, some committed choices (cuts) had to be inserted into the grammar at strategic places.
As one could fear however, this implied that wrong choices being made at some point in the parsing could not be recovered by backtracking.
These problems also made it imperative to introduce a timeout on the parsing process of embarassing 10 seconds.
Although most sentences, would be parsed within a second, some legal sentences of moderate size actually need this time.
4 Example: Whose dog barked? is analysed as if the sentence had been Who has a dog t h a t barked? which is analysed as Which p e r s o n has a dog t h a t barked? fi3.3 The semantic knowledge base Adaptability means that the system does not need to be reprogrammed for each new application.
The design principle of TUC is that most of the changes are made in a tabular semantic knowledge base, while there is one general grammar and dictionary.
In general, the logic is generated automatically from the semantic knowledge base.
The nouns play a key role in the understanding part as they constitute the class or type hierarchy.
Nouns are defined in an a k i n d o f hierarchy.
The hierarchy is tree-structured with single inheritance.
The top level also constitute the top level ontology of TUC's world.
In fact, a type check of the compliances of verbs, nouns adjectives and prepositions is not only necessary for the semantic processing but is essential for the syntax analysis for the disambiguation as well.
In TUC, the legal combinations are carefully assembled in the semantic network, which then serves a dual purpose.
These semantic definitions are necessary to allow for instance the following sentences The dog saw a man with a telescope.
The man saw a dog with a telescope.
gives exactly the same code.
% Type of question % tuc is a program % A is a real bus % B isa saturday % Nidar is a place % D is an event Y.
C was known at D Y.
E is an event in C action(go,E), Y.
the action of E is Go actor(A,E), Y.
the actor of E is A srel(to,place,nidar,E),Y.
E is to nidar srel(on,time,B,E), y, E is on the saturday B to be treated differently because with telescope m a y modify the noun man but not the noun dog, while with telescope modifies the verb see, restricted to person.
The event parameter plays an important role in the semantics.
It is used for various purposes.
The most salient role is to identify a subset of time and space in which an action or event occured.
Both the actual time and space coordinates are connected to the actions through the event parameter.
Pragmatic reasoning The TQL is translated to a route database query language (BusLOG) which is actually a Prolog program.
This is done by a production system called Pragma, which acts like an advanced rewriting system with 580 rules.
In addition, there is another rule base for actually generating the natural language answers (120 rules).
4 Conclusions
The TUC approach has as its goal to automate the creation of new natural language interfaces for a well defined subset of the language and with a minimum of explicit programming.
The implemented system has proved its worth, and is interesting if for no other reason.
There is also an increasing interest from other bus companies and route information companies alike to get a similar system for their customers.
Further work remains to make the parser really efficient, and much work remains to make the language coverage complete within reasonable limits.
It is an open question whether the system of this kind will be a preferred way of offering information to the public.
If it is, it is a fair amount of work to make it a portable system that can be implemented elsewhere, also connecting various travelling agencies.
If not, it will remain a curiosity.
But anyway, a system like this will be a contribution to the development of intelligent systems.
3.4 The
Query Processor Event Calculus The semantics of the phrases are built up by a kind of verb complements, where the event play a central role.
The text is translated from Natural language into a form called TQL (Temporal Query Language/ TUC Query Language) which is a first order event calculus expression, a self contained expression containing the literal meaning of an utterance.
A formalism TQL that was defined, inspired by the Event Calculus by Kowalski and Sergot (Kowalski and Sergot, 1986).
The TQL expressions consist of predicates, functions, constants and variables.
The textual words of nouns and verbs are translated to generic predicates using the selected interpretation.
The following question Do you know whether the bus goes to Nidar on Saturday ? would give the TQL expression below.
Machine Translation of Very Close Languages Jan HAJI(~ Computer Science Dept.
Johns Hopkins University 3400 N.
Charles St., Baltimore, MD 21218, USA hajic@cs.jhu.edu Jan HRIC KTI MFF UK Malostransk6 nfim.25 Praha 1, Czech Republic, 11800 hric@barbora.m ff.cuni.cz Vladislav KUBON OFAL MFF UK Malostransk6 mim.25 Praha 1, Czech Republic, 11800 vk@ufal.mff.cuni.cz Abstract Using examples of the transfer-based MT system between Czech and Russian RUSLAN and the word-for-word MT system with morphological disambiguation between Czech and Slovak (~ESILKO we argue that for really close languages it is possible to obtain better translation quality by means of simpler methods.
The problem of translation to a group of typologically similar languages using a pivot language is also discussed here.
demonstrate that this assumption holds only for really very closely related languages.
1. Czech-to-Russian MT system RUSLAN 1.1 History Introduction Although the field of machine translation has a very long history, the number of really successful systems is not very impressive.
Most of the funds invested into the development of various MT systems have been wasted and have not stimulated a development of techniques which would allow to translate at least technical texts from a certain limited domain.
There were, of course, exceptions, which demonstrated that under certain conditions it is possible to develop a system which will save money and efforts invested into human translation.
The main reason why the field of MT has not met the expectations of sci-fi literature, but also the expectations of scientific community, is the complexity of the task itself.
A successful automatic translation system requires an application of techniques from several areas of computational linguistics (morphology, syntax, semantics, discourse analysis etc).
as a necessary, but not a sufficient condition.
The general opinion is that it is easier to create an MT system for a pair of related languages.
In our contribution we would like to The first attempt to verify the hypothesis that related languages are easier to translate started in mid 80s at Charles University in Prague.
The project was called RUSLAN and aimed at the translation of documentation in the domain of operating systems for mainframe computers.
It was developed in cooperation with the Research Institute of Mathematical Machines in Prague.
At that time in former COMECON countries it was obligatory to translate any kind of documentation to such systems into Russian.
The work on the Czech-to-Russian MT system RUSLAN (cf.
Oliva (1989)) started in 1985.
It was terminated in 1990 (with COMECON gone) for the lack of funding.
System description The system was rule-based, implemented in Colmerauer's Q-systems.
It contained a fullfledged morphological and syntactic analysis of Czech, a transfer and a syntactic and morphological generation of Russian.
There was almost no transfer at the beginning of the project due to the assumption that both languages are similar to the extent that does not require any transfer phase at all.
This assumption turned to be wrong and several phenomena were covered by the transfer in the later stage of the project (for example the translation of the Czech verb "b~" [to be] into one of the three possible Russian equivalents: empty form, the form "byt6" in future fitense and the verb "javljat6sja"; or the translation of verbal negation).
At the time when the work was terminated in 1990, the system had a main translation dictionary of about 8000 words, accompanied by so called transducing dictionary covering another 2000 words.
The transducing dictionary was based on the original idea described in Kirschner (1987).
It aimed at the exploitation o f the fact that technical terms are based (in a majority o f European languages) on Greek or Latin stems, adopted according to the particular derivational rules o f the given languages.
This fact allows for the "translation" o f technical terms by means of a direct transcription of productive endings and a slight (regular) adjustment o f the spelling of the stem.
For example, the English words localization and discrimination can be transcribed into Czech as "lokalizace" and "diskriminace" with a productive ending -ation being transcribed to -ace.
It was generally assumed that for the pair Czech/Russian the transducing dictionary would be able to profit from a substantially greater number o f productive rules.
This hypothesis proved to be wrong, too (see B6mov~, Kubofi (1990)).
The set o f productive endings for both pairs (English/Czech, as developed for an earlier MT system from English to Czech, and Czech/Russian) was very similar.
The evaluation o f results o f RUSLAN showed that roughly 40% o f input sentences were translated correctly, about 40% with minor errors correctable by a human post-editor and about 20% of the input required substantial editing or re-translation.
There were two main factors that caused a deterioration of the translation.
The first factor was the incompleteness o f the main dictionary of the system.
Even though the system contained a set of so-called fail-soft rules, whose task was to handle such situations, an unknown word typically caused a failure o f the module o f syntactic analysis, because the dictionary entries contained besides the translation equivalents and morphological information very important syntactic information.
The second factor was the module of syntactic analysis o f Czech.
There were several reasons of parsing failures.
Apart from the common inability of most rule-based formal grammars to cover a particular natural language to the finest detail o f its syntax there were other problems.
One o f them was the existence of non-projective constructions, which are quite common in Czech even in relatively short sentences.
Even though they account only for 1.7/'o of syntactic dependencies, every third Czech sentence contains at least one, and in a news corpus, we discovered as much as 15 non-projective dependencies; see also Haji6 et al.(1998). An example o f a non-projective construction is "Soubor se nepodafilo otev~it".
[lit.: File Refl.
was_not._possible to_open.
It was not possible to open the file].
The formalism used for the implementation (Q-systems) was not meant to handle non-projective constructions.
Another source of trouble was the use o f so-called semantic features.
These features were based on lexical semantics o f individual words.
Their main task was to support a semantically plausible analysis and to block the implausible ones.
It turned out that the question o f implausible combinations o f semantic features is also more complex than it was supposed to be.
The practical outcome o f the use o f semantic features was a higher ratio of parsing failures semantic features often blocked a plausible analysis.
For example, human lexicographers assigned the verb 'to run' a semantic feature stating that only a noun with semantic features o f a human or other living being may be assigned the role o f subject of this verb.
The input text was however full o f sentences with 'programs' or 'systems' running etc.
It was o f course very easy to correct the semantic feature in the dictionary, but the problem was that there were far too many corrections required.
On the other hand, the fact that both languages allow a high degree o f word-order freedom accounted for a certain simplification o f the translation process.
The grammar relied on the fact that there are only minor word-order differences between Czech and Russian.
1.3 Lessons
learned from RUSLAN We have learned several lessons regarding the MT o f closely related languages:  The transfer-based approach provides a similar quality o f translation both for closely related and typologically different languages  Two main bottlenecks o f full-fledged transfer-based systems are: ficomplexity o f the syntactic dictionary relative unreliability o f the syntactic analysis of the source language Even a relatively simple component (transducing dictionary) was equally complex for English-to-Czech and Czech-to-Russian translation Limited text domains do not exist in real life, it is necessary to work with a high coverage dictionary at least for the source language.
2. Translation and localization 2.1 A pivot language Localization o f products and their documentation is a great problem for any company, which wants to strengthen its position on foreign language market, especially for companies producing various kinds o f software.
The amounts o f texts being localized are huge and the localization costs are huge as well.
It is quite clear that the localization from one source language to several target languages, which are typologically similar, but different from the source language, is a waste of money and effort.
It is o f course much easier to translate texts from Czech to Polish or from Russian to Bulgarian than from English or German to any o f these languages.
There are several reasons, why localization and translation is not being performed through some pivot language, representing a certain group o f closely related languages.
Apart from political reasons the translation through a pivot language has several drawbacks.
The most important one is the problem o f the loss o f translation quality.
Each translation may to a certain extent shift the meaning o f the translated text and thus each subsequent translation provides results more and more different from the original.
The second most important reason is the lack of translators from the pivot to the target language, while this is usually no problem for the translation from the source directly to the target language.
MAHT (Machine-aided human translation) systems.
We have chosen the TRADOS Translator's Workbench as a representative system o f a class o f these products, which can be characterized as an example-based translation tools.
IBM's Translation Manager and other products also belong to this class.
Such systems uses so-called translation memory, which contains pairs o f previously translated sentences from a source to a target language.
When a human translator starts translating a new sentence, the system tries to match the source with sentences already stored in the translation memory.
If it is successful, it suggests the translation and the human translator decides whether to use it, to modify it or to reject it.
The segmentation o f a translation memory is a key feature for our system.
The translation memory may be exported into a text file and thus allows easy manipulation with its content.
Let us suppose that we have at our disposal two translation memories one human made for the source/pivot language pair and the other created by an MT system for the pivot/target language pair.
The substitution o f segments o f a pivot language by the segments of a target language is then only a routine procedure.
The human translator translating from the source language to the target language then gets a translation memory for the required pair (source/target).
The system o f penalties applied in TRADOS Translator's Workbench (or a similar system) guarantees that if there is already a human-made translation present, then it gets higher priority than the translation obtained as a result o f the automatic MT.
This system solves both problems mentioned above the human translators from the pivot to the target language are not needed at all and the machinemade translation memory serves only as a resource supporting the direct human translation from the source to the target language.
3. M a c h i n e translation o f (very) closely related Slavic languages In the group o f Slavic languages, there are more closely related languages than Czech and Russian.
Apart from the pair o f Serbian and Croatian languages, which are almost identical and were Translation memory is the key The main goal of this paper is to suggest how to overcome these obstacles by means o f a combination of an MT system with commercial ficonsidered one language just a few years ago, the most closely related languages in this group are Czech and Slovak.
This fact has led us to an experiment with automatic translation between Czech and Slovak.
It was clear that application of a similar method to that one used in the system RUSLAN would lead to similar results.
Due to the closeness of both languages we have decided to apply a simpler method.
Our new system, (~ESILKO, aims at a maximal exploitation of the similarity of both languages.
The system uses the method of direct word-for-word translation, justified by the similarity of syntactic constructions of both languages.
Although the system is currently being tested on texts from the domain of documentation to corporate information systems, it is not limited to any specific domain.
Its primary task is, however, to provide support for translation and localization of various technical texts.
3.1 System
( ~ E S i L K O and its governing noun.
An alternative way to the solution of this problem was the application of a stochastically based morphological disambiguator (morphological tagger) for Czech whose success rate is close to 92/'0.
Our system therefore consists of the following modules: 1.
Import of the input from so-called 'empty' translation memory 2.
Morphological analysis of Czech 3.
Morphological disambiguation 4.
Domain-related bilingual glossaries (incl.
singleand multiword terminology) 5.
General bilingual dictionary 6.
Morphological synthesis of Slovak 7.
Export of the output to the original translation memory Letus now look in a more detail at the individual modules of the system: ad 1.
The input text is extracted out of a translation memory previously exported into an ASCII file.
The exported translation memory (of TRADOS) has a SGML-Iike notation with a relatively simple structure (cf.
the following example): Example 1.
A sample of the exported translation memory <RTF Preamble>...</RTF Preamble> <TrU> <CrD>23051999 <CrU>VK <Seg L=CS_01>Pomoci v~kazu ad-hoc m65ete rychle a jednoduge vytv~i~et regerge.
<Seg L=SK_01 >n/a </TrU> Our system uses only the segments marked by <Seg L=CS_01>, which contain one source language sentence each, and <Seg L=SK_01>, which is empty and which will later contain the same sentence translated into the target language The greatest problem of the word-for-word translation approach (for languages with very similar syntax and word order, but different morphological system) is the problem of morphological ambiguity of individual word forms.
The type of ambiguity is slightly different in languages with a rich inflection (majority of Slavic languages) and in languages which do not have such a wide variety of forms derived from a single lemma.
For example, in Czech there are only rare cases of part-of-speech ambiguities (st~t [to stay/the state], zena [woman/chasing] or tri [three/rub(imperative)]), much more frequent is the ambiguity of gender, number and case (for example, the form of the adjective jam[ [spring] is 27-times ambiguous).
The main problem is that even though several Slavic languages have the same property as Czech, the ambiguity is not preserved.
It is distributed in a different manner and the "form-for-form" translation is not applicable.
Without the analysis of at least nominal groups it is often very difficult to solve this problem, because for example the actual morphemic categories of adjectives are in Czech distinguishable only on the basis of gender, number and case agreement between an adjective by CESiLKO.
ad 2.
The morphological analysis of Czech is based on the morphological dictionary developed by Jan Haji6 and Hana Skoumalov~i in 1988-99 (for latest description, see Haji~ (1998)).
The dictionary contains over 700 000 dictionary entries and its typical coverage varies between fi99% (novels) to 95% (technical texts).
The morphological analysis uses the system of positional tags with 15 positions (each morphological.category, such as Part-of-speech, Number, Gender, Case, etc.
has a fixed, singlesymbol place in the tag).
Example 2 tags assigned to the word-form "pomoci" (help/by means of) pomoci: NFP2 ......
A ....
]NFS7 ......
A ....
I R--2 ........... where : N noun; R preposition F feminine gender S singular, P plural 7, 2 case (7 instrumental, 2 genitive) A affirmative (non negative) ad 3.
The module of morphological disambiguation is a key to the success o f the translation.
It gets an average number of 3.58 tags per token (word form in text) as an input.
The tagging system is purely statistical, and it uses a log-linear model of probability distribution see Haji~, Hladkfi (1998).
The learning is based on a manually tagged corpus of Czech texts (mostly from the general newspaper domain).
The system learns contextual rules (features) automatically and also automatically determines feature weights.
The average accuracy o f tagging is between 91 and 93% and remains the same even for technical texts (if we disregard the unknown names and foreign-language terms that are not ambiguous anyway).
The lemmatization immediately follows tagging; it chooses the first lemma with a possible tag corresponding to the tag selected.
Despite this simple lemmatization method, and also thanks to the fact that Czech words are rarely ambiguous in their Part-of-speech, it works with an accuracy exceeding 98%.
The multiple-word terms are sequences of lemmas (not word forms).
This structure has several advantages, among others it allows to minimize the size of the dictionary and also, due to the simplicity of the structure, it allows modifications of the glossaries by the linguistically naive user.
The necessary morphological information is introduced into the domain-related glossary in an off-line preprocessing stage, which does not require user intervention.
This makes a big difference when compared to the RUSLAN Czech-to-Russian MT system, when each multiword dictionary entry cost about 30 minutes of linguistic expert's time on average.
ad 5.
The main bilingual dictionary contains data necessary for the translation o f both lemmas and tags.
The translation of tags (from the Czech into the Slovak morphological system) is necessary, because due to the morphological differences both systems use close, but slightly different tagsets.
Currently the system handles the 1:1 translation of tags (and 2:2, 3:3, etc.).
Different ratio of translation is very rare between Czech and Siovak, but nevertheless an advanced system of dictionary items is under construction (for the translation 1:2, 2:1 etc.).
It is quite interesting that the lexically homonymous words often preserve their homonymy even after the translation, so no special treatment of homonyms is deemed necessary.
ad 6.
The morphological synthesis of Slovak is based on a monolingual dictionary of SIovak, developed by J.Hric (1991-99), covering more than ]00,000 dictionary entries.
The coverage of the dictionary is not as high as o f the Czech one, but it is still growing.
It aims at a similar coverage of Slovak as we enjoy for Czech.
ad 7.
The export o f the output of the system (~ESILKO into the translation memory (of TRADOS Translator's Workbench) amounts mainly to cleaning of all irrelevant SGML markers.
The whole resulting Slovak sentence is inserted into the appropriate location in the original translation memory file.
The following example also shows that the marker <CrU> contains an information that the target language sentence was created by an M T system.
ad 4.
The domain-related bilingual glossaries contain pairs of individual words and pairs of multiple-word terms.
The glossaries are organized into a hierarchy specified by the user; typically, the glossaries for the most specific domain are applied first.
There is one general matching rule for all levels of glossaries the longest match wins.
languages, namely for Czech-to-Polish translation.
Although these languages are not so similar as Czech and Slovak, we hope that an addition of a simple partial noun phrase parsing might provide results with the quality comparable to the fullfledged syntactic analysis based system RUSLAN (this is of course true also for the Czechoto-Slovak translation).
The first results of Czech-to Polish translation are quite encouraging in this respect, even though we could not perform as rigorous testing as we did for Slovak.
Acknowledgements 3.2 Evaluation of results The problem how to evaluate results of automatic translation is very difficult.
For the evaluation of our system we have exploited the close connection between our system and the TRADOS Translator's Workbench.
The method is simple the human translator receives the translation memory created by our system and translates the text using this memory.
The translator is free to make any changes to the text proposed by the translation memory.
The target text created by a human translator is then compared with the text created by the mechanical application of translation memory to the source text.
TRADOS then evaluates the percentage of matching in the same manner as it normally evaluates the percentage of matching of source text with sentences in translation memory.
Our system achieved about 90% match (as defined by the TRADOS match module) with the results of human translation, based on a relatively large (more than 10,000 words) test sample.
This project was supported by the grant GAt~R 405/96/K214 and partially by the grant GA(~R 201/99/0236 and project of the Ministry of Education No.
VS96151. The accuracy of the translation achieved by our system justifies the hypothesis that word-forword translation might be a solution for MT of really closely related languages.
The remaining problems to be solved are problems with the oneto many or many-to-many translation, where the lack of information in glossaries and dictionaries sometimes causes an unnecessary translation error.
A u t o m a t i c construction of parallel English-Chinese corpus for cross-language information retrieval Jiang Chen and Jian-Yun Nie D ~ p a r t e m e n t d ' I n f o r m a t i q u e et R e c h e r c h e O p ~ r a t i o n n e l l e Universit~ de M o n t r e a l C.P. 6128, succursale C E N T R E V I L L E M o n t r e a l (Quebec), C a n a d a H 3 C 3 J 7 {chen, nie} @iro.
umontreal, ca Abstract A major obstacle to the construction of a probabilistic translation model is the lack of large parallel corpora.
In this paper we first describe a parallel text mining system that finds parallel texts automatically on the Web.
The generated Chinese-English parallel corpus is used to train a probabilistic translation model which translates queries for Chinese-English cross-language information retrieval (CLIR).
We will discuss some problems in translation model training and show the preliminary C U R results.
1 Introduction
2 Parallel Text Mining Algorithm The PTMiner system is an intelligent Web agent that is designed to search for large amounts of parallel text on the Web.
The mining algorithm is largely language independent.
It can thus be adapted to other language pairs with only minor modifications.
Taking advantage of Web search engines as much as possible, PTMiner implements the following steps (illustrated in Fig.
1): 1 Search for candidate sites Using existing Web search engines, search for the candidate sites that may contain parallel pages; 2 File name fetching For each candidate site, fetch the URLs of Web pages that are indexed by the search engines; 3 Host crawling Starting from the URLs collected in the previous step, search through each candidate site separately for more URLs; 4 Pair scan From the obtained URLs of each site, scan for possible parallel pairs; 5 Download and verifying Download the parallel pages, determine file size, language, and character set of each page, and filter out non-parallel pairs.
2.1 Search
for candidate Sites We take advantage of the huge number of Web sites indexed by existing search engines in determining candidate sites.
This is done by submitting some particular requests to the search engines.
The requests are determined according to the following observations.
In the sites where parallel text exists, there are normally some pages in one language containing links to the parallel version in the other language.
These are usually indicated by those links' anchor texts 1.
For example, on some English page there may be a link to its Chinese version with the anchor text "Chinese Version" or "in Chinese".
1An a n c h o r t e x t is a piece of text on a W e b page which, w h e n clicked on, will take you to a n o t h e r linked page.
To be helpful, it u s u a l l y c o n t a i n s t h e key i n f o r m a t i o n about the linked page.
Parallel texts have been used in a number of studies in computational linguistics.
Brown et al.(1993) defined a series of probabilistic translation models for MT purposes.
While people may question the effectiveness of using these models for a full-blown MT system, the models are certainly valuable for developing translation assistance tools.
For example, we can use such a translation model to help complete target text being drafted by a human translator (Langlais et al., 2000).
Another utilization is in cross-language information retrieval (CLIR) where queries have to be translated from one language to another language in which the documents are written.
In CLIR, the quality requirement for translation is relatively low.
For example, the syntactic aspect is irrelevant.
Even if the translated word is not a true translation but is strongly related to the original query, it is still helpful.
Therefore, CLIR is a suitable application for such a translation model.
However, a major obstacle to this approach is the lack of parallel corpora for model training.
Only a few such corpora exist, including the Hansard English-French corpus and the HKUST EnglishChinese corpus (Wu, 1994).
In this paper, we will describe a method which automatically searches for parallel texts on the Web.
We will discuss the text mining algorithm we adopted, some issues in translation model training using the generated parallel corpus, and finally the translation model's performance in CLIR.
21 Figure 1: The workflow of the mining process.
T h e same phenomenon can be observed on Chinese pages.
Chances are t h a t a site with parallel texts will contain such links in some of its documents.
This fact is used as the criterion in searching for candidate sites.
Therefore, to determine possible sites for EnglishChinese parallel texts, we can request an English document containing the following anchor: Host Crawling anchor : "english version H ["in english",...].
Similar requests are sent for Chinese documents.
From the two sets of pages obtained by the above queries we extract two sets of Web sites.
T h e union of these two sets constitutes then the candidate sites.
T h a t is to say, a site is a candidate site when it is found to have either an English page linking to its Chinese version or a Chinese page linking to its English version.
A host crawler is slightly different from a Web crawler.
Web crawlers go through innumerable pages and hosts on the Web.
A host crawler is a Web crawler t h a t crawls through documents on a given host only.
A breadth-first crawling algorithm is applied in P T M i n e r as host crawler.
The principle is t h a t when a link to an unexplored document on the same site is found in a document, it is added to a list t h a t will be explored later.
In this way, most file names from the candidate sites are obtained.
Pair Scan File N a m e Fetching We now assume t h a t a pair of parallel texts exists on the same site.
To search for parallel pairs on a site, P T M i n e r first has to obtain all (or at least p a r t of) the H T M L file names on the site.
From these names pairs are scanned.
It is possible to use a Web crawler to explore the candidate sites completely.
However, we can take advantage of the search engines again to accelerate the process.
As the first step, we submit the following query to the search engines: to fetch the Web pages t h a t they indexed from this site.
If we only require a small a m o u n t of parallel texts, this result m a y be sufficient.
For our purpose, however, we need to explore the sites more thoroughly using a host crawler.
Therefore, we continue our search for files with a host crawler which uses the documents found by the search engines as the starting point.
After collecting file names for each candidate site, the next task is to determine the parallel pairs.
Again, we t r y to use some heuristic rules to guess which files m a y be parallel texts before downloading them.
The rules are based on external features of the documents.
By external feature, we mean those features which m a y be known without analyzing the contents of the file, such as its URL, size, and date.
This is in contrast with the internal features, such as language, character set, and H T M L structure, which cannot be known until we have downloaded the page and analyzed its contents.
The heuristic criterion comes from the following observation: We observe t h a t parallel text pairs usually have similar n a m e patterns.
The difference between the names of two parailel pages usually lies in a segment which indicates the language.
For example, "file-ch.html" (in Chinese) vs.
"file-en.html" (in English).
T h e difference m a y also appear in the path, such as ".../chinese/.../file.html" vs.
".../english/.../file.html'. T h e n a m e patterns described above are commonly used by webmasters to help organize their sites.
Hence, we can suppose t h a t a pair of pages with this kind of p a t t e r n are probably parallel texts.
First, we establish four lists for English prefixes, English suffixes, Chinese prefixes and Chinese suffixes.
For example: E n g l i s h P r e f i x = {e, en, e_, en_, e -, e n -, ...}.
For each file in one language, if a segment in its name corresponds to one of the language affixes, several new names are generated by changing the segment to the possible corresponding affixes of the other language.
If a generated name corresponds to an existing file, then the file is considered as a candidate parallel document of the original file.
Filtering Next, we further examine the contents of the paired files to determine if they are really parallel according to various external and internal features.
This may further improve the pairing precision.
The following methods have been implemented in our system.
usually have similar H T M L structures.
However, we also noticed that parallel texts may have quite different HTML structures.
One of the reasons is that the two files may be created using two HTML editors.
For example, one may be used for English and another for Chinese, depending on the language handling capability of the editors.
Therefore, caution is required when measuring structure difference numerically.
Parallel text alignment is still an experimental area.
Measuring the confidence values of an alignment is even more complicated.
For example, the alignment algorithm we used in the training of the statistical translation model produces acceptable alignment results but it does not provide a confidence value that we can "confidently" use as an evaluation criterion.
So, for the moment this criterion is not used in candidate pair evaluation.
Generated Corpus and Translation Model Training In this section, we describe the results of our parallel text mining and translation model training.
3 Text
Length Parallel files often have similar file lengths.
One simple way to filter out incorrect pairs is to compare the lengths of the two files.
The only problem is to set a reasonable threshold that will not discard too many good pairs, i.e. balance recall and precision.
The usual difference ratio depends on the language pairs we are dealing with.
For example, ChineseEnglish parallel texts usually have a larger difference ratio than English-French parallel texts.
The filtering threshold had to be determined empirically, from the actual observations.
For Chinese-English, a difference up to 50% is tolerated.
2.5.2 L
a n g u a g e a n d Character Set It is also obvious that the two files of a pair have to be in the two languages of interest.
By automatically identifying language and character set, we can filter out the pairs that do not satisfy this basic criterion.
Some Web pages explicitly indicate the language and the character set.
More often such information is omitted by authors.
We need some language identification tool for this task.
SILC is a language and encoding identification system developed by the RALI laboratory at the University of Montreal.
It employs a probabilistic model estimated on tri-grams.
Using these models, the system is able to determine the most probable language and encoding of a text (Isabelle et al., 1997).
2.5.3 H
T M L Structure and Alignment In the STRAND system (Resnik, 1998), the candidate pairs are evaluated by aligning them according to their H T M L structures and computing confidence values.
Pairs are assumed to be wrong if they have too many mismatching markups or low confidence values.
Comparing H T M L structures seems to be a sound way to evaluate candidate pairs since parallel pairs 23 The Corpus Using the above approach for Chinese-English, 185 candidate sites were searched from the domain hk.
We limited the mining domain to hk because Hong Kong is a bilingual English-Chinese city where high quality parallel Web sites exist.
Because of the small number of candidate sites, the host crawler was used to thoroughly explore each site.
The resulting corpus contains 14820 pairs of texts including 117.2Mb Chinese texts and 136.5Mb English texts.
The entire mining process lasted about a week.
Using length comparison and language identification, we refined the precision of the corpus to about 90%.
The precision is estimated by examining 367 randomly picked pairs.
Statistical Translation Model Many approaches in computational linguistics try to extract translation knowledge from previous translation examples.
Most work of this kind establishes probabilistic models from parallel corpora.
Based on one of the statistical models proposed by Brown et al.(1993), the basic principle of our translation model is the following: given a corpus of aligned sentences, if two words often co-occur in the source and target sentences, there is a good likelihood that they are translations of each other.
In the simplest case (model 1), the model learns the probability, p(tls), of having a word t in the translation of a sentence containing a word s.
For an input sentence, the model then calculates a sequence of words that are most probable to appear in its translation.
Using a similar statistical model, Wu (1995) extracted a largescale English-Chinese lexicon from the H K U S T corFigure 2: An alignment example using pure length-based method.
pus which is built manually.
In our case, the probabilistic translation model will be used for CLIR.
The requirement on our translation model may be less demanding: it is not absolutely necessary that a word t with high p(tls ) always be a true translation of s.
It is still useful if t is strongly related to s.
For example, although "railway" is not a true translation of "train" (in French), it is highly useful to include "railway" in the translation of a query on "train".
This is one of the reasons why we think a less controlled parallel corpus can be used to train a translation model for CLIR.
very noisy.
Aligning English-Chinese parallel texts is already very difficult because of the great differences in the syntactic structures and writing systems of the two languages.
A number of alignment techniques have been proposed, varying from statistical methods (Brown et al., 1991; Gale and Church, 1991) to lexical methods (Kay and RSscheisen, 1993; Chen, 1993).
The method we adopted is t h a t of Simard et al.(1992). Because it considers both length similarity and cognateness as alignment criteria, the method is more robust and better able to deal with noise than pure length-based methods.
Cognates are identical sequences of characters in corresponding words in two languages.
T h e y are commonly found in English and French.
In the case of English-Chinese alignment, where there are no cognates shared by the two languages, only the H T M L markup in both texts are taken as cognates.
Because the H T M L structures of parallel pages are normally similar, the markup was found to be helpful for alignment.
To illustrate how markup can help with the alignment, we align the same pair with both the pure length-based method of Gale & Church (Fig.
2), and the method of Simard et al.(Fig. 3).
First of all, we observe from the figures that the two texts are Parallel Text Alignment Before the mined documents can be aligned into parallel sentences, the raw texts have to undergo a series of some preprocessing, which, to some extent, is language dependent.
For example, the major operations on the Chinese-English corpus include encoding scheme transformation (for Chinese), sentence level segmentation, parallel text alignment, Chinese word segmentation (Nie et al., 1999) and English expression extraction.
The parallel Web pages we collected from various sites are not all of the same quality.
Some are highly parallel and easy to align while others can be Figure 3: An alignment example considering cognates.
divided into sentences.
The sentences are marked by <s i d = " x x x x " > and < / s > . Note that we determine sentences not only by periods, but also by means of H T M L markup.
We further notice that it is difficult to align sentences 0002.
The sentence in the Chinese page is much longer than its counterpart in the English page because some additional information (font) is added.
The length-based method thus tends to take sentence 0002, 0003, and 0004 in the English page as the translation of sentence 0002 in the Chinese page (Fig.
2), which is wrong.
This in turn provocated the three following incorrect alignments.
As we can see in Fig.
3, the cognate method did not make the same mistake because of the noise in sentence 0002.
Despite their large length difference, the two 0002 sentences are still aligned as a 1-1 pair, because the sentences in the following 4 alignments (0003 0003; 0004 0004, 0005; 0005 0006; 0006 0007) have rather similar H T M L markups and are taken by the program to be the most likely alignments.
Beside HTML markups, other criteria may also be incorporated.
For example, it would be helpful to consider strong correspondence between certain English and Chinese words, as in (Wu, 1994).
We hope to implement such correspondences in our future research.
3.4 Lexicon
Evaluation To evaluate the precision of the English-Chinese translation model trained on the Web corpus, we examined two sample lexicons of 200 words, one in each direction.
The 200 words for each lexicon were randomly selected from the training source.
We examined the most probable translation for each word.
The Chinese-English lexicon was found to have a precision of 77%.
The English-Chinese lexicon has a higher precision of 81.5%.
Part of the lexicons are shown in Fig.
4, where t / f indicates whether a translation is true or false.
These precisions seem to be reasonably high.
They are quite comparable to that obtained by Wu (1994) using a manual Chinese-English parallel corpus.
Effect of Stopwords We also found that stop-lists have significant effect on the translation model.
Stop-list is a set of the most frequent words that we remove from the train3.5 English word t/f access adaptation add adopt agent agree airline amendment, appliance apply attendance auditor -,average base_on t t t t t t t t t t t/f t t t t t t t t t t Translation office protection report prepare local follow standard adult inadequate part financial visit bill vehicle saving Figure 4: Part of the evaluation lexicons.
Figure 5: Effect of stop lists in C-E translation.
ing source.
Because these words exist in most alignments, the statistical model cannot derive correct translations for them.
More importantly, their existence greatly affects the accuracy of other translations.
They can be taken as translations for many words.
A priori, it would seem that both the English and Chinese stop-lists should be applied to eliminate the noise caused by them.
Interestingly, from our observation and analysis we concluded that for better precision, only the stop-list of the target language should be applied in the model training.
We first explain why the stop-list of the target language has to be applied.
On the left side of Fig.
5, if the Chinese word C exists in the same alignments with the English word E more than any other Chinese words, C will be the most probable translation for E.
Because of their frequent appearance, some Chinese stopwords may have more chances to be in the same alignments with E.
The probability of the translation E --+ C is then reduced (maybe even less than those of the incorrect ones).
This is the reason why many English words are translated to " ~ ' (of) by the translation model trained without using the Chinese stop-list.
We also found that it is not necessary to remove the stopwords of the source language.
In fact, as illustrated on the right side of Fig.
5, the existence of the English stopwords has two effects on the probability of the translation E -~ C: 1 They may often be found together with the Chinese word C.
Owing to the Expectation Maximization algorithm, the probability of E -~ C may therefore be reduced.
2 On
the other hand, there is a greater likelihood that English stopwords will be found together with the most frequent Chinese words.
Here, we use the term "Chinese frequent words" instead of "Chinese stopwords" because even if a stop-list is applied, there may still remain some common words that have the same effect as the stopwords.
The coexistence of English and Chinese frequent words reduces the probability that the Chinese frequent words are the translations of E, and thus raise the probability of E -+ C.
The second effect was found to be more significant than the first, since the model trained without the English stopwords has better precision than the model trained with the English stopwords.
For the correct translations given by both models, the model Mono-Lingual IR Translation Model Dictionary trained without considering the English stopwords gives higher probabilities.
4 English-Chinese CLIR Results Our final goal was to test the performance of the translation models trained on the Web parallel corpora in CLIR.
We conducted CLIR experiments using the Smart IR system.
4.1 Results
The English test corpus (for C-E CLIR) was the AP corpus used in TREC6 and TREC7.
The short English queries were translated manually into Chinese and then translated back to English by the translation model.
The Chinese test corpus was the one used in the TREC5 and TREC6 Chinese track.
It contains both Chinese queries and their English translations.
Our experiments on these two corpora produced the results shown in Tab.
1. The precision of monolingual IR is given as benchmark.
In both E-C and C-E CLIR, the translation model achieved around 40% of monolingual precision.
To compare with the dictionary-based approach, we employed a ChineseEnglish dictionary, CEDICT (Denisowski, 1999), and an English-Chinese online dictionary (Anonymous, 1999a) to translate queries.
For each word of the source query, all the possible translations given by the dictionary are included in the translated query.
The Chinese-English dictionary has about the same performace as the translation model, while the English-Chinese dictionary has lower precision than that of the translation model.
We also tried to combine the translations given by the translation model and the dictionary.
In both C-E and E-C CLIR, significant improvements were achieved (as shown in Tab.
1). The improvements show that the translations given by the translation model and the dictionary complement each other well for IR purposes.
The translation model may give either exact translations or incorrect but related words.
Even though these words are not correct in the sense of translation, they are very possibly related to the subject of the query and thus helpful for IR purposes.
The dictionary-based approach expands a query along another dimension.
It gives all the possible translations for each word including those that are missed by the translation model.
4.2 C
o m p a r i s o n W i t h M T S y s t e m s One advantage of a parallel text-based translation model is that it is easier to build than an MT system.
Now that we have examined the CLIR performance of the translation model, we will compare it with two existing MT systems.
Both systems were tested in E-C CLIR.
4.2.1 S
u n s h i n e W e b T r a n Server Using the Sunshine WebTran server (Anonymous, 1999b), an online Engiish-Chinese MT system, to translate the 54 English queries, we obtained an average precision of 0.2001, which is 50.3% of the mono-lingual precision.
The precision is higher than that obtained using the translation model (0.1804) or the dictionary (0.1427) alone, but lower than the precison obtained using them together (0.2232).
4.2.2 Transperfect
Kwok (1999) investigated the CLIR performance of an English-Chinese MT software called Transperfect, using the same TREC Chinese collection as we used in this study.
Using the MT software alone, Kwok achieved 56% of monolingual precision.
The precision is improved to 62% by refining the translation with a dictionary.
Kwok also adopted pretranslation query expansion, which further improved the precison to 70% of the monolingual results.
In our case, the best E-C CLIR precison using the translation model (and dictionary) is 56.1%.
It is lower than what Kwok achieved using Transperfect, however, the difference is not large.
4.3 F
u r t h e r P r o b l e m s The Chinese-English translation model has a fax lower CLIR performance than that of the EnglishFrench model established using the same method (Nie et al., 1999).
The principal reason for this is the fact that English and Chinese are much more different than English and French.
This problem surfaced in many phases of this work, from text alignment to query translation.
Below, we list some further factors affecting CLIR precision.
 The Web-collected corpus is noisy and it is difficult to align English-Chinese texts.
The alignment method we employed has performed more poorly than on English-French alignment.
This in turn leads to poorer performance of the translation model.
In general, we observe a higher fivariability in Chinese-English translations than in English-French translations.
 For E-C CLIR, although queries in both languages were provided, the English queries were not strictly translated from the original Chinese ones.
For example, A J g, ~ (human right situation) was translated into human right issue.
We cannot expect the translation model to translate issue back to ~ (situation).
 The training source and the CLIR collections were from different domains.
The Web corpus are retrieved from the parallel sites in Hong Kong while the Chinese collection is from People's Daily and Xinhua News Agency, which are published in mainland China.
As the result, some important terms such as ~ $ $ (mostfavored-nation) and --I!!
~ ~ (one-nation-twosystems) in the collection are not known by the model.
5 Summary
The goal of this work was to investigate the feasibility of using a statistical translation model trained on a Web-collected corpus to do English-Chinese CLIR.
In this paper, we have described the algorithm and implementation we used for parallel text mining, translation model training, and some results we obtained in CLIR experiments.
Although further work remains to be done, we can conclude that it is possible to automatically construct a Chinese-English parallel corpus from the Web.
The current system can be easily adapted to other language pairs.
Despite the noisy nature of the corpus and the great difference in the languages, the evaluation lexicons generated by the translation model produced acceptable precision.
While the current CLIR results are not as encouraging as those of English-French CLIR, they could be improved in various ways, such as improving the alignment method by adapting cognate definitions to HTML markup, incorporating a lexicon and/or removing some common function words in translated queries.
We hope to be able to demonstrate in the near future that a fine-tuned English-Chinese translation model can provide query translations for CLIR with the same quality produced by MT systems.
D i s t i l l i n g dialogues A m e t h o d using natural dialogue c o r p o r a for dialogue s y s t e m s d e v e l o p m e n t Arne Department JSnsson and Nils Dahlb~ick of Computer and Information Science LinkSping University S-581 83, L I N K O P I N G SWEDEN nilda@ida.liu.se, arnjo@ida.liu.se Abstract We report on a method for utilising corpora collected in natural settings.
It is based on distilling (re-writing) natural dialogues to elicit the type of dialogue that would occur if one the dialogue participants was a computer instead of a human.
The method is a complement to other means such as Wizard of Oz-studies and un-distilled natural dialogues.
We present the distilling method and guidelines for distillation.
We also illustrate how the method affects a corpus of dialogues and discuss the pros and cons of three approaches in different phases of dialogue systems development.
1 Introduction
on measures for inter-rater reliability (Carletta, 1996), on frameworks for evaluating spoken dialogue agents (Walker et al., 1998) and on the use of different corpora in the development of a particular system (The Carnegie-Mellon Communicator, Eskenazi et al.(1999)). The question we are addressing in this paper is how to collect and analyse relevant corpora.
We begin by describing what we consider to be the main advantages and disadvantages of the two currently used methods; studies of human dialogues and Wizard of Oz-dialogues, especially focusing on the ecological validity of the methods.
We then describe a m e t h o d called 'distilling dialogues', which can serve as a supplement to the other two.
It has been known for quite some time now, that the language used when interacting with a computer is different from the one used in dialogues between people, (c.f.
JSnsson and Dahlb~ick (1988)).
Given that we know that the language will be different, but not how it will be different, we need to base our development of natural language dialogue systems on a relevant set of dialogue corpora.
It is our belief that we need to clarify a number of different issues regarding the collection and use of corpora in the development of speech-only and multimodal dialogue systems.
Exchanging experiences and developing guidelines in this area are as important as, and in some sense a necessary pre-requisite to, the development of computational models of speech, language, and dialogue/discourse.
It is interesting to note the difference in the state of art in the field of natural language dialogue systems with that of corpus linguistics, where issues of the usefulness of different samples, the necessary sampling size, representativeness in corpus design and other have been discussed for quite some time (e.g.
(Garside et al., 1997; Atkins et al., 1992; Crowdy, 1993; Biber, 1993)).
Also the neighboring area of evaluation of NLP systems (for an overview, see Sparck Jones and Galliers (1996)) seems to have advanced further.
Some work have been done in the area of natural language dialogue systems, e.g. on the design of Wizard of Oz-studies (Dahlb~ck et al., 1998), 2 Natural and Wizard of Oz-Dialogues The advantage of using real dialogues between people is that they will illustrate which tasks and needs that people actually bring to a particular service provider.
Thus, on the level of the users' general goals, such dialogues have a high validity.
But there are two drawbacks here.
First; it is not self-evident that users will have the same task expectations from a computer system as they have with a person.
Second, the language used will differ from the language used when interacting with a computer.
These two disadvantages have been the major force behind the development of Wizard of Ozmethods.
The advantage here is that the setting will be human-computer interaction.
But there are important disadvantages, too.
First, on the practical side, the task of setting up a high quality simulation environment and training the operators ('wizards') to use this is a resource consuming task (Dahlb~ck et al., 1998).
Second, and probably even more important, is that we cannot then observe real users using a system for real life tasks, where they bring their own needs, motivations, resources, and constraints to bear.
To some extent this problem can be overcome using well-designed so called 'scenarios'.
As pointed out in Dahlb~ck (1991), on many levels of analysis the artificiality of the situation will not affifect the language used.
An example of this is the pattern of pronoun-antecedent relations.
But since the tasks given to the users are often pre-described by the researchers, this means t h a t this is not a good way of finding out which tasks the users actually want to perform.
Nor does it provide a clear enough picture on how the users will act to find something t h a t satisfies their requirements.
If e.g. the task is one of finding a charter holiday trip or buying a TVset within a specified set of constraints (economical and other), it is conceivable t h a t people will stay with the first item t h a t matches the specification, whereas in real life they would probably look for alternatives.
In our experience, this is primarily a concern if the focus is on the users' goals and plans, but is less a problem when the interest is on lowerlevel aspects, such as, syntax or patterns of pronounantecedent relationship (c.f.
Dahlb~ick (1991)).
To summarize; real life dialogues will provide a reasonably correct picture of the way users' approach their tasks, and what tasks they bring to the service provider, but the language used will not give a good approximation of what the system under construction will need to handle.
Wizard of Ozdialogues, on the other hand, will give a reasonable approximation of some aspects of the language used, but in an artificial context.
The usual approach has been to work in three steps.
First analyse real h u m a n dialogues, and based on these, in the second phase, design one or more Wizard of Oz-studies.
The final step is to fine-tune the system's performance on real users.
A good example of this method is presented in Eskenazi et al.(1999). But there are also possible problems with this approach (though we are not claiming that this was the case in their particular project).
Eskenazi et al.(1999) asked a h u m a n operator to act 'computerlike' in their Wizard of Oz-phase.
The advantage is of course that the h u m a n operator will be able to perform all the tasks t h a t is usually provided by this service.
The disadvantage is t h a t it puts a heavy burden on the h u m a n operator to act as a computer.
Since we know that lay-persons' ideas of what computers can and cannot do are in m a n y respects far removed from what is actually the case, we risk introducing some systematic distortion here.
And since it is difficult to perform consistently in similar situations, we also risk introducing non-systematic distortion here, even in those cases when the 'wizard' is an NLP-professional.
Our suggestion is therefore to supplement the above mentioned methods, and bridge the gap between them, by post-processing h u m a n dialogues to give them a computer-like quality.
The advantage, compared to having people do the simulation on the fly, is both that it can be done with more consistency, and also that it can be done by researchers t h a t actually know what h u m a n c o m p u t e r natural language dialogues can look like.
A possible disadvantage with using both Wizard of Oz-and real computer dialogues, is that users will quickly a d a p t to what the system can provide t h e m with, and will therefore not try to use it for tasks they know it cannot perform.
Consequently, we will not get a full picture of the different services they would like the system to provide.
A disadvantage with this method is, of course, t h a t post-processing takes some time compared to using the natural dialogues as they are.
There is also a concern on the ecological validity of the results, as discussed later.
Distilling dialogues Distilling dialogues, i.e. re-writing h u m a n interactions in order to have them reflect what a humancomputer interaction could look like involves a number of considerations.
The main issue is t h a t in corp o r a of natural dialogues one of the interlocutors is not a dialogue system.
The system's task is instead performed by a h u m a n and the problem is how to anticipate the behaviour of a system that does not exist based on the performance of an agent with different performance characteristics.
One important aspect is how to deal with h u m a n features that are not part of what the system is supposed to be a b l e to handle, for instance if the user talks about things outside of the domain, such as discussing an episode of a recent T V show.
It also involves issues on how to handle situations where one of the interlocuters discusses with someone else on a different topic, e.g. discussing the up-coming Friday party with a friend in the middle of an information providing dialogue with a customer.
It is i m p o r t a n t for the distilling process to have at least an outline of the dialogue system t h a t is under development: Will it for instance have the capacity to recognise users' goals, even if not explicitly stated?
Will it be able to reason about the discourse domain?
W h a t services will it provide, and what will be outside its capacity to handle?
In our case, we assume that the planned dialogue system has the ability to reason on various aspects of dialogue and properties of the application.
In our current work, and in the examples used for illustration in this paper, we assume a dialogue model that can handle any relevant dialogue phenomenon and also an interpreter and speech recogniser being able to understand any user input that is relevant to the task.
There is is also a powerful domain reasoning module allowing for more or less any knowledge reasoning on issues that can be accomplished within the domain (Flycht-Eriksson, 1999).
Our current system does, however, not have an explicit user task model, as opposed to a system task model (Dahlb~ick fiand JSnsson, 1999), which is included, and thus, we can not assume that the 'system' remembers utterances where the user explains its task.
Furthermore, as our aim is system development we will not consider interaction outside the systems capabilities as relevant to include in the distilled dialogues.
The context of our work is the development a multi-modal dialogue system.
However, in our current work with distilling dialogues, the abilities of a multi-modal system were not fully accounted for.
The reason for this is that the dialogues would be significantly affected, e.g. a telephone conversation where the user always likes to have the n e x t connection, please will result in a table if multi-modal output is possible and hence a fair amount of the dialogne is removed.
We have therefore in this paper analysed the corpus assuming a speech-only system, since this is closer to the original telephone conversations, and hence needs fewer assumptions on system performance when distilling the dialogues.
distilling. The system might in such cases provide less information.
The principle of providing all relevant information is based on the assumption that a computer system often has access to all relevant information when querying the background system and can also present it more conveniently, especially in a multimodal system (Ahrenberg et al., 1996).
A typical example is the dialogue fragment in figure 1.
In this fragment the system provides information on what train to take and how to change to a bus.
The result of distilling this fragment provides the revised fragment of figure 2.
As seen in the fragment of figure 2 we also remove a number of utterances typical for human interaction, as discussed below.
* S y s t e m utterances are m a d e m o r e computer-like and do n o t include irrelevant i n f o r m a t i o n. The Distillation guidelines Distilling dialogues requires guidelines for how to handle various types of utterances.
In this section we will present our guidelines for distilling a corpus of telephone conversations between a human information provider on local buses 1 to be used for developing a multimodal dialogue system (Qvarfordt and JSnsson, 1998; Flycht-Eriksson and JSnsson, 1998; Dahlb~ick et al., 1999; Qvarfordt, 1998).
Similar guidelines are used within another project on developing Swedish Dialogue Systems where the domain is travel bureau information.
We can distinguish three types of contributors: 'System' (i.e.
a future systems) utterances, User utterances, and other types, such as moves by other speakers, and noise.
latter is seen in $9 in the dialogue in figure 3 where the provided information is not relevant.
It could also be possible to remove $5 and respond with $7 at once.
This, however, depends on if the information grounded in $5-U6 is needed for the 'system' in order to know the arrival time or if that could be concluded from U4.
This in turn depends on the system's capabilities.
If we assume that the dialogue system has a model of user tasks, the information in $5-U6 could have been concluded from that.
We will, in this case, retain $5-U6 as we do not assume a user task model (Dahlb/ick and JSnsson, 1999) and in order to stay as close to the original dialogue as possible.
The next problem concerns the case when 'system' utterances are changed or removed.
 Dialogue contributions provided by s o m e t h i n g or s o m e o n e other than the u s e r or the ' s y s t e m ' are removed.
These are regarded as not being part Modifying system utterances The problem of modifying 'system' utterances can be divided into two parts: how to change and when to change.
They are in some respects intertwined, but as the how-part affects the when-part more we will take this as a starting point.
 The ' s y s t e m ' provides as m u c h relevant inform a t i o n as possible at once.
This depends on the capabilities of the systems output modalities.
If we have a screen or similar output device we present as much as possible which normally is all relevant information.
If we, on the other hand, only have spoken output the amount of information that the hearer can interpret in one utterance must be considered when 1The bus time table dialogues are collected at LinkSping University and are available (in Swedish) on http://www.ida.liu.se/~arnjo/kfb/dialoger.html of the interaction.
This means that if someone interrupts the current interaction, say that the telephone rings during a face-to-face interaction, the interrupting interaction is normally removed from the corpus.
Furthermore, 'system' interruptions are removed.
A human can very well interrupt another human interlocuter, but a computer system will not do that.
However, this guideline could lead to problems, for instance, when users follow up such interruptions.
If no information is provided or the interrupted sequence does not affect the dialogue, we have no problems removing the interruption.
The problem is what to do when information from the 'system' is used in the continuing dialogue.
For such cases we have no fixed strategy, yes I wonder if you have any m m buses or ().
like express buses leaving from LinkSping to Vadstena ().
on sunday ja ville undra om ni hade ndgra 5h bussar eUer ().
typ expressbussar sore dkte frdn LinkSping till Vadstena ().
pd sSnda $5: no the bus does not run on sundays nej bussen g~r inte pd sSndagar U6: how can you ().
can you take the train and then change some way ().
because ().
to MjSlby 'n' so hur kan man ().
kan man ta tdg d sen byta p~ ndtt sStt ().
fSr de ().
till mjSlby ~ sd $7: that you can do too yes de kan du gSra ocksd ja U8: how ().
do you have any such suggestions hut ().
har du n~ra n~gra s~na fSrslag $9: yes let's see (4s) a m o m e n t (15s) now let us see here ().
was it on the sunday you should travel ja ska se h~ir (4s) eft 5gonblick (15s) nu ska vise hSr ().
va de p~ sSndagen du skulle dka pd U10: yes right afternoon preferably ja just de eftermidda ggirna $11: afternoon preferable ().
you have train from LinkSping fourteen twenty nine eftermidda gSrna ().
du hat t~g frdn LinkSping fjorton d tjugonie U12: m m mm S13: and then you will change from MjSlby station six hundred sixty sd byter du frdn MjSlby station sexhundrasexti sexhundrasexti $15: fifteen and ten Figure 1: Dialogue fragment from a real interaction on bus time-table information U4: S5: U6: $7: I wonder if you have any buses or ().
like express buses going from LinkSping to Vadstena ().
on sunday no the bus does not run on sundays how can you ().
can you take the train and then change some way ().
because ().
to MjSlby and so you can take the train from LinkSping fourteen and twenty nine and then you will change at MjSlby station to bus six hundred sixty at fifteen and ten Figure 2: A distilled version of the dialogue in figure 1 the dialogue needs to be rearranged depending on how the information is to be used (c.f.
the discussion in the final section of this paper).
in figure 4).
A common case of this is when the ' s y s t e m ' is talking while looking for information, $5 in the dialogue fragment of figure 4 is an example of this.
Related to this is when the system provides its own comments.
If we can assume that it has such capabilities they are included, otherwise we remove them.
 'System' utterances which are no longer valid are removed.
Typical examples of this are the utterances $7, $9, $11 and $13 in the dialogue fragment of figure 1.
* Remove sequences of utterances where the 'system' behaves in a way a computer would not do.
For instance jokes, irony, humor, commenting on the other dialogue participant, or dropping the telephone (or whatever is going on in $7 The system does not repeat information that has already been provided unless explicitly asked to do so.
In human interaction it is not uncommon to repeat what has been uttered for purposes other than to provide grounding information or feedback.
This is for instance common during 'n' I must be at Resecentrum before fourteen and thirty five ().
'cause we will going to the interstate buses $5: U6: $7: aha ().
'n' then you must be there around twenty past two something then yes around t h a t ja ungefgir let's see here ( l l s ) two hundred and fourteen R y d end station leaves forty six ().
thirteen 'n' forty six then you will be down fourteen oh seven (.) jaha 'n' ().
the next one takes you there ().
fourteen thirty seven ().
but t h a t is too late Figure 3: Dialogue fragment from a real interaction on bus time-table information U2: $3: U4: $5: U6: $7: Well, hi ().
I a m going to Ugglegatan eighth ja hej ().
ja ska till Ugglegatan dtta Yes ja and ().
I wonder ().
it is somewhere in Tannefors och ().
jag undrar ().
det ligger ndnstans i Tannefors Yes ().
I will see here one one I will look exactly where it is one m o m e n t please ja ().
jag ska se hhr eft eft jag ska titta exakt vat det ligger eft 6gonblick barn Oh Yeah (operator disconnects) (25s) m m ().
okey (hs) what the hell (2s) (operator connects again) hello yes ((Telefonisten kopplar ur sig)) (25s) iihh ().
okey (hs) de va sore ]aan (2s) ((Telefonisten kopplar in sig igen)) halld ja ja hej It is bus two hundred ten which runs on old tannefors road t h a t you have to take and get off at the bus stop at t h a t bus stop named vetegatan Figure 4: Dialogue fragment from a natural bus timetable interaction search procedures as discussed above.
want to develop systems where the user needs to restrict his/her behaviour to the capabilities of the dialogue system.
However, there are certain changes m a d e to user utterances, in most cases as a consequence of changes of system utterances.
 The system does not ask for information it has already achieved.
For instance asking again if it is on Sunday as in $9 in figure 1.
This is not uncommon in h u m a n interaction and such utterances from the user are not removed.
However, we can assume t h a t the dialogue system does not forget what has been talked about before.
4.2 M
o d i f y i n g u s e r u t t e r a n c e s The general rule is to change user utterances as little as possible.
The reason for this is that we do not Utterances that are no longer valid are removed.
The most common cases are utterances whose request has already been answered, as seen in the distilled dialogue in figure 2 of the dialogue in figure 1.
sixteen fifty five sexton ]emti/em U12: sixteen fifty five ().
aha sexton f e m t i / e m ().
jaha S13: bus line four hundred thirty five linje ]yrahundra tretti/em Figure 5: Dialogue fragment from a natural bus timetable interaction  Utterances are removed where the user discusses things that are in the environment.
For instance commenting the 'systems' clothes or hair.
This also includes other types of communicative signals such as laughter based on things outside the interaction, for instance, in the environment of the interlocuters.
 User utterances can also be added in order to make the dialogue continue.
In the dialogue in figure 5 there is nothing in the dialogue explaining why the system utters S13.
In such cases we need to add a user utterance, e.g.
Which bus is that?.
However, it might turn out that there are cues, such as intonation, found when listening to the tapes.
If such detailed analyses are carried out, we will, of course, not need to add utterances.
Furthermore, it is sometimes the case t h a t the telephone operator deliberately splits the information into chunks t h a t can be comprehended by the user, which then must be considered in the distillation.
5 Applying
the method To illustrate the m e t h o d we will in this section t r y to characterise the results from our distillations.
The illustration is based on 39 distilled dialogues from the previously mentioned corpus collected with a telephone operator having information on local bus time-tables and persons calling the information service.
The distillation took a b o u t three hours for all 39 dialogues, i.e. it is reasonably fast.
The distilled dialogues are on the average 27% shorter.
However, this varies between the dialogues, at most 73% was removed but there were also seven dialogues t h a t were not changed at all.
At the most 34 utterances where removed from one single dialogue and that was from a dialogue with discussions on where to find a parking lot, i.e. discussions outside the capabilities of the application.
There was one more dialogue where more t h a n 30 utterances were removed and that dialogue is a typical example of dialogues where distillation actually is very useful and also indicates what is normally removed from the dialogues.
This particular dialogue begins with the user asking for the telephone number to 'the Lost property office' for a specific bus operator.
However, the operator starts a discussion on what bus the traveller traveled on before providing the requested telephone number.
The reason for this discussion is probably t h a t the operator knows that different bus companies are utilised and would like to make sure that the user really understands his/her request.
The interaction t h a t follows can, thus, in t h a t respect be relevant, but for our purpose of developing systems based on an overall goal of providing information, not to understand human interaction, our dialogue system will not able to handle such phenomenon (JSnsson, 1996).
The dialogues can roughly be divided into five different categories based on the users task.
The discussion in twenty five dialogues were on bus times between various places, often one departure and one arrival but five dialogues involved more places.
In five dialogues the discussion was one price and various types of discounts.
Five users wanted to know the telephone number to 'the Lost property office', two discussed only bus stops and two discussed how they could utilise their season ticket to travel outside the trafficking area of the bus company.
It is interesting to note that there is no correspondence between the task being performed during the interaction and the amount of changes made to the d i a logue.
Thus, if we can assume that the amount of distillation indicates something about a user's interaction style, other factors t h a n the task are important when characterising user behaviour.
Looking at what is altered we find t h a t the most i m p o r t a n t distilling principle is that the 'system' provides all relevant information at once, c.f. figures 1 and 2.
This in turn removes utterances provided by both 'system' and user.
Most added utterances, both from the user and the 'system', provide explicit requests for information that is later provided in the dialogue, e.g. utterance $3 in figure 6.
We have added ten utterances in all 39 dialogues, five 'system' utterances and five user utterances.
Note, however, that we utilised the transcribed dialogues, without information on intonation.
We would probably not have needed to add this m a n y utterances if we had utilised the tapes.
Our reason for not using information on intonation is that we do not assume t h a t our system's speech recogniser can recognise intonation.
Finally, as discussed above, we did not utilise the full potential of multi-modality when distilling the dialogues.
For instance, some dialogues could be further distilled if we had assumed t h a t the system had presented a time-table.
One reason for this is t h a t we wanted to capture as m a n y interesting aspects intact as possible.
The advantage is, thus, that we have a better corpus for understanding humanYees hi Anna Nilsson is my name and I would like to take the bus from Ryd center to Resecentrum in LinkSping jaa hej Anna Nilsson heter jag och jag rill ~ka buss ~r~n Ryds centrum till resecentrum i LinkSping.
$3: U4: mm When do you want to leave? mm N~ir r i l l d u  k a ? 'n' I must be at Resecentrum before fourteen and thirty five ().
'cause we will going to the interstate buses ja ska va p~ rececentrum innan fjorton d trettifem ().
f5 vi ska till l~ngfiirdsbussarna Figure 6: Distilled dialogue fragment with added utterance computer interaction and can from t h a t corpus do a second distillation where we focus more on multimodal interaction.
One example of this is whether the system is meant to acquire information on the user's underlying motivations or goals or not.
In the examples presented, we have not assumed such capabilities, but this assumption is not an absolute necessity.
We believe, however, that the distilling process should be based on one such model, not the least to ensure a consistent t r e a t m e n t of similar recurring phenomena at different places in the corpora.
The validity of the results based on analysing distilled dialogues depends p a r t l y on how the distillation has been carried out.
Even when using natural dialogues we can have situations where the interaction is somewhat mysterious, for instance, if some of the dialogue participants behaves irrational such as not providing feedback or being too elliptical.
However, if careful considerations have been made to stay as close to the original dialogues as possible, we believe that distilled dialogues will reflect what a hum a n would consider to be a natural interaction.
Acknowledgments This work results from a n u m b e r of projects on development of natural language interfaces supported by The Swedish Transport & Communications Research Board (KFB) and the joint Research P r o g r a m for Language Technology ( H S F R / N U T E K ) . We are indebted to the participants of the Swedish Dialogue Systems project, especially to Staffan Larsson, Lena S a n t a m a r t a, and Annika Flycht-Eriksson for interesting discussions on this topic.
Discussion We have been presenting a method for distilling hum a n dialogues to make t h e m resemble h u m a n computer interaction, in order to utilise such dialogues as a knowledge source when developing dialogue systems.
Our own main purpose has been to use t h e m for developing multimodal systems, however, as discussed above, we have in this p a p e r rather assumed a speech-only system.
But we believe that the basic approach can be used also for multi-modal systems and other kinds of natural language dialogue systems.
It is i m p o r t a n t to be aware of the limitations of the method, and how 'realistic' the produced result will be, compared to a dialogue with the final system.
Since we are changing the dialogue moves, by for instance providing all required information in one move, or never asking to be reminded of what the user has previously requested, it is obvious t h a t what follows after the changed sequence would probably be affected one way or another.
A consequence of this is that the resulting dialogue is less accurate as a model of the entire dialogue.
It is therefore not an ideal candidate for trying out the systems over-all performance during system development.
But for the smaller sub-segments or sub-dialogues, we believe that it creates a good approximation of what will take place once the system is up and running.
Furthermore, we believe distilled dialogues in some respects to be more realistic than Wizard of Ozdialogues collected with a wizard acting as a computer.
Another issue, t h a t has been discussed previously in the description of the method, is t h a t the distilling is made based on a particular view of what a dialogue with a computer will look like.
While not necessarily being a detailed and specific model, it is at least an instance of a class of computer dialogue models.
Plan-Based Dialogue Management in a Physics Tutor Reva Freedman Learning Research and Development Center University of Pittsburgh Pittsburgh, PA 15260 freedrk+@pitt, edu http://www.pitt, edu/~freedrk Abstract This paper describes an application of APE (the Atlas Planning Engine), an integrated planning and execution system at the heart of the Atlas dialogue management system.
APE controls a mixedinitiative dialogue between a human user and a host system, where turns in the 'conversation' may include graphical actions and/or written text.
APE has full unification and can handle arbitrarily nested discourse constructs, making it more powerful than dialogue managers based on finitestate machines.
We illustrate this work by describing Atlas-Andes, an intelligent tutoring system built using APE with the Andes physics tutor as the host.
1 Introduction
The purpose of the Atlas project is to enlarge the scope of student interaction in an intelligent tutoring system (ITS) to include coherent conversational sequences, including both written text and GUI actions.
A key component o f Atlas is APE, the Atlas Planning Engine, a "just-intime" planner specialized for easy construction and quick generation of hierarchically organized dialogues.
APE is a domainand task-independent system.
Although to date we have used APE as a dialogue manager for intelligent tutoring systems, APE could also be used to manage other types of human-computer conversation, such as an advicegiving system or an interactive help system.
Planning is an essential component of a dialogue-based ITS.
Although there are many reasons for using natural language in an ITS, as soon as the student gives an unexpected response to a tutor question, the tutor needs to be able to plan in order to achieve its goals as well as respond appropriately to the student's statement.
Yet classical planning is inappropriate for dialogue generation precisely because it assumes an unchanging world.
A more appropriate approach is the "practical reason" approach pioneered by Bratman (1987, 1990).
According to Bratman, human beings maintain plans and prefer to follow them, but they are also capable of changing the plans on the fly when needed.
Bratman's approach has been introduced into computer science under the name of reactive planning (Georgeff and Ingrand 1989, Wilkins et al.1995). In this paper we discuss the rationale for the use of reactive planning as well as the use of the hierarchical task network (HTN) style o f plan operators.
Then we describe APE (the Atlas Planning Engine), a dialogue planner we have implemented to embody the above concepts.
We demonstrate the use of APE by showing how we have used it to add a dialogue capability to an existing ITS, the Andes physics tutor.
By showing dialogues that Atlas-Andes can generate, we demonstrate the advantages of this architecture over the finite-state machine approach to dialogue management.
Integrated planning and execution for dialogue generation 2.1 'Practical reason' and the BDI model For an ITS, planning is required in order to ensure a coherent conversation as well as to accomplish tutorial goals.
But it is impossible to plan a whole conversation in advance when the student can respond freely at every turn, just as human beings cannot plan their daily lives in advance because of possible changes in conditions.
Classical planning algorithms are inappropriate because the tutor must be able to change plans based on the This research was supported by NSF grant number 9720359 to CIRCLE, the Center for Interdisciplinary Research on Constructive Learning Environments at the University of Pittsburgh and Carnegie-Mellon University.
fistudent's responses.
For this reason we have adopted the ideas of the philosopher Michael Bratman (1987, 1990).
Bratman uses the term "practical reason" to describe his analysis since he is concerned with how to reason about practical matters.
For human beings, planning is required in order to accomplish one's goals.
Bratman's key insight is that human beings tend to follow a plan once they have one, although they are capable of dropping an intention or changing a partial plan when necessary.
In other words, human beings do not decide what to do from scratch at each turn.
Bratman and others who have adopted his approach use a tripartite mental model that includes beliefs, desires and intentions (Bratman, Israel and Pollack 1988, Pollack 1992, Georgeff et al.1998), hence the name "BDI model".
Beliefs, which are uninstantiated plans in the speaker's head, are reified by the plan library.
Desires are expressed as the agent's goals.
Intentions, or plan steps that the agent has committed to but not yet acted on, are stored in an agenda.
Thus the agent's partial plan for achieving a goal is a network of intentions.
A plan can be left in a partially expanded state until it is necessary to refine it further.
2.2 Implementation
via reactive planning can be achieved via a series of subgoals instead of relying on means-end reasoning.
Hierarchical decomposition is more appropriate to dialogue generation for a number of reasons.
First, decomposition is better suited to the type of largescale dialogue planning required in a real-world tutoring system, as it is easier to establish what a human speaker will say in a given situation than to be able to understand why in sufficient detail and generality to do means-end planning.
Second, Hierarchical decomposition minimizes search time.
Third, our dialogues are task-oriented and have a hierarchical structure (Grosz and Sidner 1986).
In such a case, matching the structure of the domain simplifies operator development because they can often be derived from transcripts of human tutoring sessions.
The hierarchy information is also useful in determining appropriate referring expressions.
Fourth, interleaved planning and execution is important for dialogue generation because we cannot predict the human user's future utterances.
In an HTN-based system, it is straightforward to implement interleaved planning and execution because one only needs to expand the portion of the plan that is about to be executed.
Finally, the conversation is in a certain sense the trace of the plan.
In other words, we care much more about the actions generated by the planner than the states involved, whether implicitly or explicitly specified.
Hierarchical decomposition provides this trace naturally.
3 Background: the Andes physics tutor Andes (Gertner, Conati and VanLehn 1998) is an intelligent tutoring system in the domain of firstyear college physics.
Andes teaches via coached problem solving (VanLehn 1996).
In coached problem solving, the tutoring system tracks the student as the latter attempts to solve a problem.
If the student gets stuck or deviates too far from a correct solution path, the tutoring system provides hints and other assistance.
A sample Andes problem is shown in midsolution in Figure 1.
A physics problem is given in the upper-left corner with a picture below it.
Next to the picture the student has begun to sketch the vectors involved using the GUI buttons along the left-hand edge of the screen.
As the fistudent draws vectors, Andes and the student cooperatively fill in the variable definitions in the upper-right corner.
Later the student will use the space below to write equations connecting the variables.
In this example, the elevator is decelerating, so the acceleration vector should face the opposite direction from the velocity vector.
(If the acceleration vector went the same direction as the velocity vector, the speed of the elevator would increase and it would crash into the ground).
This is an important issue in beginning physics; it occurs in five Andes problems.
When such errors occur, Andes turns the incorrect item red and provides hints to students in the lower-left corner of the screen.
A sample of these hints, shown in the order a student would encounter them, is shown in Fig.
2. But hints are an output-only form of natural language; the student can't take the initiative or ask a question.
In addition, there is no way for the system to ask the student a question or lead the student through a multi-step directed line of reasoning.
Thus there is no way to use some of the effective rhetorical methods used by skilled human tutors, such as analogy and reductio ad absurdum.
Current psychological research suggests that active methods, where students have to answer questions, will improve the performance of tutoring systems.
Structure of the Atlas Planning Engine Figure3 shows a sample plan operator.
For legibility, the key elements have been rendered in English instead of in Lisp.
The hiercx slot provides a way for the planner to be aware of the context in which a decomposition is proposed.
Items in the hiercx slot are instantiated and added to the transient database only so long as the operator which spawned them is in the agenda.
To initiate a planning session, the user invokes the planner with an initial goal.
The system searches the operator library to find all operators whose goal field matches the next goal on the agenda and whose filter conditions and preconAn elevator slows to a stop from an initial downward velocity of 10.0 m]s in 2.00 seconds.
A passenger in the elevator is holding a 3.00 kilogram package by a vertical string.
What is the tension in the string during the process? e',ev~o, at 10 m/s elev~or at a stop mass of p~:w'.,I,,~ magnitude of the inst~~taneous Velocity of pack,age ~ {rkneTO magnitude of the avelage Acceleratiorl of package,dudngTO... pkg Figure I: Screen shot of the Andes physics tutor fiS: T: S: T: S: T: S: T: S: (draws acceleration vector in same direction as velocity) Wrong.
What's wrongwith that?
Think about the direction of the acceleration vector.
Please explain further.
Remember that the direction of acceleration is the direction of the change in velocity.
Please explain further.
The'direction o f the acceleration vector is straight up.
(draws acceleration vector correctly) Figure 2: Andes hint sequence formatted as dialogue ditions are satisfied.
Goals are represented in first-order logic without quantifiers and matched via unification.
Since APE is intended especially for generation of hierarchically organized taskoriented discourse, each operator has a multi-step recipe in the style of Wilkins (1988).
When a match is found, the matching goal is removed from the agenda and is replaced by the steps in the recipe.
APE has two kinds of primitive actions; one ends a turn and the other doesn't.
From the point of view of discourse generation, the most important APE recipe items are those allowing the planner to change the agenda when necessary.
These three types of recipe items make APE more powerful than a classical planner.
 Fact: Evaluate a condition.
If false, skip the rest of the recipe.
Fact is used to allow run-time decision making by bypassing the rest o f an operator when circumstances change during its execution.
Fact can be used with retry-at to implement a loop just as in Prolog.
 Retry-at.
The purpose of retry-at is to allow the planner to back up to a choice point and make a new decision.
It removes goals sequentially from the top of the agenda, a full operator at a time, until the supplied argument is false.
Then it restores the parent goal of the last operator removed, so that further planning can choose a new way to achieve it.
Retry-at implements a Prolog-like choice of alternatives, but it differs from backtracking in that the new operator is chosen based on conditions that apply when the retry operation is executed, rather than on a list of possible operators formed when the original operator was chosen.
For retry-at to be useful, the author must provide multiple operators for the same goal.
Each operator must have a set of preconditions enabling it to be chosen at the appropriate time.
 Prune-replace: The intent of prune-replace is (def-operator handle-same-direction :goal ()... :filter () :precond ()...
We h a v e a s k e d a q u e s t i o n a b o u t a c c e l e r a t i o n ;...
a n d t h e s t u d e n t h a s g i v e n an a n s w e r ; ...
f r o m w h i c h we c a n d e d u c e t h a t s / h e t h i n k s a c c e l, a n d v e l o c i t y go in ; the same direction ; a n d we h a v e n o t g i v e n t h e e x p l a n a t i o n below yet : r e c i p e ()...
Tell the student: "But if the a c c e l e r a t i o n went the same direction as t h e v e l o c i t y, t h e n t h e e l e v a t o r w o u l d be s p e e d i n g u p . " ; M a r k t h a t we a r e g i v i n g t h i s e x p l a n a t i o n ; T e l l t h e s t u d e n t t h a t t u t o r is r e q u e s t i n g another answer ("Try again").
Edit the agenda ( u s i n g prune-replace) so t h a t r e s p o n d i n g to a n o t h e r a n s w e r is at t h e t o p of t h e a g e n d a :hiercx ()) Figure 3: Sample plan operator 55 fito allow the planner to remove goals from the agenda based on a change in circumstances.
It removes goals sequentially from the top of the agenda, one at a time, until the supplied argument becomes false.
Then it replaces the removed goals with an optional list of new goals.
Prune-replace allows a type of decision-making frequently used in dialogue generation.
When a conversation partner does not give the expected response, one would often like to remove the next goal from the agenda and replace it with one or more replacement goals.
Prune-replace implements a generalized version of this concept.
APE is domain-independent and communicates with a host system via an API.
As a partner in a dialogue, it needs to obtain information from the world as well as produce output turns.
Preconditions on plan operators can be used to access information from external knowledge sources.
APE contains a recipe item type that can be used to execute an external program such as a call to a GUI interface.
APE also has recipe items allowing the user to assert and retract facts in a knowledge base.
Further details about the APE planner can be found in (Freedman, 2000).
I m p l e m e n t a t i o n of Atlas-Andes 5.1 Architecture of Atlas-Andes The first system we have implemented with APE is a prototype Atlas-Andes system that replaces the hints usually given for an incorrect acceleration vector by a choice of generated subdialogues.
Figure 4 shows the architecture of Atlas-Andes; any other system built with APE would look similar.
Robust natural language understanding in Atlas-Andes is provided by Ros6's CARMEL system (Ros6 2000); it uses the spelling correction algorithm devised by Elmi and Evens (1998).
5.2 Structure
of human tutorial dialogues In an earlier analysis (Kim, Freedman and Evens 1998) we showed that a significant portion of human-human tutorial dialogues can be modeled with the hierarchical structure o f task-oriented dialogues (Grosz and Sidner 1986).
Furthermore, a main building block o f the discourse hierarchy, corresponding to the transaction level in Conversation Analysis (Sinclair and Coulthard 1975), matches the tutoring episode defined by VanLehn et al.(1998). A tutoring episode consists of the turns necessary to help the student make one correct entry on the interface.
NLU (CARMEL) Plan Library User Interface Host (Andes) GUI Interpreter (Andes) Transient Knowledge Base Figure 4: Interface between Atlas and host system fiTo obtain empirical data for the Atlas-Andes plan operators, we analyzed portions of a corpus of human tutors helping students solve similar physics problems.
Two experienced tutors were used.
Tutor A was a graduate student in computer science who had majored in physics; tutor B was a professional physics tutor.
The complete corpus contained solutions to five physics problems by 41 students each.
We analyzed every tutoring episode dealing with the acceleration vector during deceleration, totaling 29 examples divided among 20 students and both tutors.
The tutors had very different styles.
Tutor A tended to provide encouragement rather than content, making those transcripts less useful for deriving an information-based approach.
Tutor B used an information-based approach, but after one wrong answer tended to complete the solution as a monologue.
Largely following tutor B's approach to sequence and content, we isolated six ways of teaching the student about direction of acceleration.
Tutoring schemata Switching between schemata API and GUI handling Answer handling Domain-dep.
lex. insertion Domain-indep.
lex. insertion TOTAL 5.3 Sample output and evaluation Figure 5 shows an example of text that can be generated by the Atlas-Andes system, showing an analogy-based approach to teaching this content.
The operator library used to generate this text could generate a combinatorially large number of versions of this dialogue as well as selected examples of other ways o f teaching about direction of acceleration.
This operator library used to generate this text contained 1 l 1 plan operators, divided as follows: We are currently working on components that will allow us to increase the number of physics concepts covered without a corresponding increase in the number of operators.
The schema switching operators prevent the tutor from repeating itself during a physics problem.
They could be reduced or eliminated by a general discourse history component that tutoring schema operators could refer to.
Domain-dependent lexical insertion refers to the choice of lexical items such as car and east in the sample dialogue, while domain-independent iexical insertion refers to items such as O K and exactly.
Both categories could be eliminated, or at least severely reduced, through the use of a text realization package.
Together that would provide a one-third reduction in the number o f operators needed.
As the set of API and GUI handling operators is fixed, that would reduce by half the number of application operators needed.
The largest remaining category of operators is the answer handlers.
These operators handle a variety of answers for each o f the five questions that the system can ask.
The answers we recognize include categories such as "don't know" as well as specific answers (e.g.
a direction perpendicular to the correct answer) which we recognize because the tutor has specific replies for them.
In order to reduce the number o f S: T: S: T: S: T: S: (draws acceleration vector in same direction as velocity) What is the definition of acceleration?
Don't know.
OK, let's try this.
If a car was driving along east, which way would you have to push on it to make it stop?
West. Exactly.
The opposite direction.
So the net force goes the opposite direction, and so does the acceleration.
Try to draw the acceleration vector again now.
(draws acceleration vector correctly) Figure 5: Example of generated dialogue 57 fioperators further, we must investigate more general methods of handling student errors.
In particular, we plan to investigate error-classifying predicates that apply to more than one question as well as the use of intention-based predicates.
Since the system only covers one rule of physics, albeit in a variety of ways, we plan to make some of these efficiency improvements before adding new rules o f physics and testing it with users.
Preconditions for the operators in the plan library utilize discourse or interaction history, the current goal hierarchy, recent information such as the tutor's current goal and the student's latest response, shared information such as a model o f objects on the screen, and domain knowledge.
As an example of the latter, if the student draws an acceleration vector which is incorrect but not opposite to the velocity vector, a different response will be generated.
Related work Wenger (1987), still the chief textbook on ITSs, states that using a global planner to control an ITS is too inefficient to try.
This is no longer true, if indeed it ever was.
Vassileva (1995) proposes a system based on AND-OR graphs with a separate set of rules for reacting to unexpected events.
Lehuen, Nicolle and Luzzati (1996) present a method of dialogue analysis that produces schemata very similar to ours.
Earlier dialoguebased ITSs that use augmented finite-state machines or equivalent include CIRCSIM-Tutor (Woo et al.1991, Z h o u e t al.
1999) and the system described by Woolf (1984).
Cook (1998) uses levels of finite-state machines.
None of these systems provides for predicates with variables or unification.
Conclusions 5.4 Discussion Many previous dialogue-based ITSs have been implemented with finite-state machines, either simple or augmented.
In the most common finite state mode[, each time the human user issues an utterance, the processor reduces it to one of a small number of categories.
These categories represent the possible transitions between states.
Thus history can be stored, and context considered, only by expanding the number o f states.
This approach puts an arbitrary restriction on the amount of context or depth of conversational nesting that can be considered.
More importantly, it misses the significant generalization that these types of dialogues are hierarchical: larger units contain repeated instances of the same smaller units in different sequences and instantiated with different values.
Furthermore, the finite-state machine approach does not allow the author to drop one line of attack and replace it by another without hardcoding every possible transition.
It is also clear that the dialogue-based approach has many benefits over the hint-sequence approach.
In addition to providing a multi-step teaching methods with new content, it can respond flexibly to a variety of student answers at each step and take context into account when generating a reply.
In this paper we described APE, an integrated planner and execution system that we have implemented as part o f the Atlas dialogue manager.
APE uses HTN-style operators and is based on reactive planning concepts.
Although APE is intended largely for use in domains with hierarchical, multi-turn plans, it can be used to implement any conversation-based system, where turns in the 'conversation' may include graphical actions and/or text.
We illustrated the use of APE with an example from the Atlas-Andes physics tutor.
We showed that previous models based on finite-state machines are insufficient to handle the nested subdialogues and abandoned partial subdialogues that occur in practical applications.
We showed how APE generated a sample dialogue that earlier systems could not handle.
Acknowledgments We thank Abigail Gertner for her generous assistance with the Andes system, and Michael Ringenberg for indispensible programming support.
Carolyn Ros6 built the CARMEL natural language understanding component.
Mohammed EImi and Michael Glass of Illinois Institute o f Technology provided the spelling correction code.
We thank Pamela Jordan and the referees for their comments.
Bratman's approach has been elaborated in a computer science context by subsequent researchers (Bratman, Israel and Pollack 1988, Pollack 1992, Georgeff et al.1998). Reactive planning (Georgeff and Ingrand 1989, Wilkins et al.1995), originally known as "integrated planning and execution," is one way of implementing Bratman's model.
Originally developed for real-time control of the space shuttle, reactive planning has since been used in a variety of other domains.
For the Atlas project we have developed a reactive planner called APE (Atlas Planning Engine) which uses these ideas to conduct a conversation.
After each student response, the planner can choose to continue with its previous intention or change something in the plan to respond better to the student's utterance.
Like most reactive planners, APE is a hierarchical task network (HTN) style planner (Yang 1990, Erol, Hendler and Nau 1994).
A Framework for MT and Multilingual NLG Systems Based on Uniform Lexico-Structural Processing Benoit Lavoie CoGenTex, Inc.
840 Hanshaw Road Ithaca, NY USA, 14850 benoit@cogentex.com Richard Kittredge CoGenTex, Inc.
840 Hanshaw Road Ithaca, NY USA, 14850 richard @cogentex.com Tanya Korelsky CoGenTex, Inc.
840 Hanshaw Road Ithaca, NY USA, 14850 tanya @cogentex.com Owen Rambow * ATT Labs-Research, B233 180 Park Ave, PO Box 971 Florham Park, NJ USA, 07932 rambow @research.att.com Abstract In this paper we describe an implemented framework for developing monolingual or multilingual natural language generation (NLG) applications and machine translation (MT) applications.
The framework demonstrates a uniform approach to generation and transfer based on declarative lexico-structural transformations of dependency structures of syntactic or conceptual levels ("uniform lexico-structural processing").
We describe how this framework has been used in practical NLG and MT applications, and report the lessons learned.
1 Introduction
The framework consists of a portable Java environment for building NLG or MT applications by defining modules using a core tree transduction engine and single declarative ASCII specification language for conceptual or syntactic dependency tree structures 1 and their transformations.
Developers can define new modules, add or remove modules, or modify their connections.
Because the processing of the transformation engine is restricted to transduction of trees, it is computationally efficient.
Having declarative rules facilitates their reuse when migrating from one programming environment to another; if the rules are based on functions specific to a programming language, the implementation of these functions might no longer be available in a different environment.
In addition, having all lexical information and all rules represented declaratively makes it relatively easy to integrate into the framework techniques for generating some of the rules automatically, for example using corpus-based methods.
The declarative form of transformations makes it easier to process them, compare them, and cluster them to achieve proper classification and ordering.
In this paper we present a linguistically motivated framework for uniform lexicostructural processing.
It has been used for transformations of conceptual and syntactic structures during generation in monolingual and multilingual natural language generation (NLG) and for transfer in machine translation (MT).
Our work extends directions taken in systems such as Ariane (Vauquois and Boitet, 1985), FoG (Kittredge and Polgu6re, 1991), JOYCE (Rainbow and Korelsky, 1992), and LFS (Iordanskaja et al., 1992).
Although it adopts the general principles found in the abovementioned systems, the approach presented in this paper is more practical, and we believe, would eventually integrate better with emerging statistics-based approaches to MT.
* The work performed on the framework by this coauthor was done while at CoGenTex, Inc.
1 In
this paper, we use the term syntactic dependency (tree) structure as defined in the Meaning-Text Theory (MTT; Mel'cuk, 1988).
However, we extrapolate from this theory when we use the term conceptual dependency (tree) structure, which has no equivalent in MTT (and is unrelated to Shank's CD structures proposed in the 1970s).
Thus, the framework represents a generalized processing environment that can be reused in different types of natural language processing (NLP) applications.
So far the framework has been used successfully to build a wide variety of NLG and MT applications in several limited domains (meteorology, battlefield messages, object modeling) and for different languages (English, French, Arabic, and Korean).
In the next sections, we present the design of the core tree transduction module (Section 2), describe the representations that it uses (Section 3) and the linguistic resources (Section 4).
We then discuss the processing performed by the tree transduction module (Section 5) and its instantiation for different applications (Section 6).
Finally, we discuss lessons learned from developing and using the framework (Section 7) and describe the history of the framework comparing it to other systems (Section 8).
2 The
Framework's Tree Transduction Module Input Lexico-Structm'al Processing Dependency SUucturc Lexico-Structural Postprocessing Figure 1: Design of the Tree Transduction Module 3 The Framework's Representations The core processing engine of the framework is a generic tree transduction module for lexicostructural processing, shown in Figure 1.
The module has dependency stuctures as input and output, expressed in the same tree formalism, although not necessarily at the same level (see Section 3).
This design facilitates the pipelining of modules for stratificational transformation.
In fact, in an application, there are usually several instantiations of this module.
The transduction module consists of three processing steps: lexico-structural preprocessing, main lexico-structural processing, and lexico-structural post-processing.
Each of these steps is driven by a separate grammar, and all three steps draw on a common feature data base and lexicon.
The grammars, the lexicon and the feature data base are referred to as the linguistic resources (even if they sometimes apply to a conceptual representation).
All linguistic resources are represented in a declarative manner.
An instantiation of the tree transduction module consists of a specification of the linguistic resources.
The representations used by all instantiations of the tree transduction module in the framework are dependency tree structures.
The main characteristics of all the dependency tree structures are:  A dependency tree is unordered (in contrast with phrase structure trees, there is no ordering between the branches of the tree).
 All the nodes in the tree correspond to lexemes (i.e., lexical heads) or concepts depending on the level of representation.
In contrast with a phrase structure representation, there are no phrase-structure nodes labeled with nonterminal symbols.
Labelled arcs indicate the dependency relationships between the lexemes.
The first of these characteristics makes a dependency tree structure a very useful representation for MT and multilingual NLG, since it gives linguists a representation that allows them to abstract over numerous crosslinguistic divergences due to language specific ordering (Polgu~re, 1991).
We have implemented 4 different types of dependency tree structures that can be used for NLG, MT or both:  Deep-syntactic structures (DSyntSs);  Surface syntactic structures (SSyntSs); Conceptual structures (ConcSs); Parsed syntactic structures (PSyntSs).
The DSyntSs and SSyntSs correspond closely to the equivalent structures of the Meaning-Text Theory (MTT; Mel'cuk, 1988): both structures are unordered syntactic representations, but a DSyntS only includes full meaning-bearing lexemes while a SSyntS also contains function words such as determiners, auxiliaries, and strongly governed prepositions.
In the implemented applications, the DSyntSs are the pivotal representations involved in most transformations, as this is also often the case in practice in linguistic-based MT (Hutchins and Somers, 1997).
Figure 2 illustrates a DSyntS from a meteorological application, MeteoCogent (Kittredge and Lavoie, 1998), represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework (Lavoie and Rambow, 1997).
As Figure 2 illustrates, there is a straightforward mapping between the graphical notation and the ASCII notation supported in the framework.
This also applies for all the transformation rules in the framework which illustrates the declarative nature of our approach, Figure 3 illustrates the mapping between an interlingua defined as a ConcS and a corresponding English DSyntS.
This example, also taken from MeteoCogent, illustrates that the conceptual interlingua in NLG can be closer to a database representation of domain data than to its linguistic representations.
As mentioned in (Polgu~re, 1991), the high level of abstraction of the ConcSs makes them a suitable interlingua for multilingual NLG since they bridge the semantic discrepancies between languages, and they can be produced easily from the domain data.
However, most off-the-shelf parsers available for MT produce only syntactic structures, thus the DSyntS level is often more suitable for transfer.
Cones TO ItlGH LOW Low 5 to Mlgh 20 Figure 3: ConcS Interlingua and English DSyntS Finally, the PSyntSs correspond to the parser outputs represented using RealPro's dependency structure formalism.
The PSyntSs may not be valid directly for realization or transfer since they may contain unsupported features or dependency relations.
However, the PSyntSs are represented in a way to allow the framework to convert them into valid DSyntS via lexicostructural processing.
This conversion is done via conversion grammars customized for each parser.
There is a practical need to convert one syntactic formalism to another and so far we have implemented converters for three off-theshelf parsers (Palmer et al., 1998).
4 The
Framework's Linguistic Resources 't Low S to high 20 Figure 2: DSyntS(Graphicaland ASCIINotation) The ConcSs correspond to the standard framelike structures used in knowledge representation, with labeled arcs corresponding to slots.
We have used them only for a very limited meteorological domain (in MeteoCogent), and we imagine that they will typically be defined in a domain-specific manner.
As mentioned previously, the framework is composed of instantiations of the tree fitransduction module shown in Figure 1.
Each module has the following resources:  Feature Data-Base: This consists of the feature system defining available features and their possible values in the module.
 Lexicon: This consists of the available lexemes or concepts, depending on whether the module works at syntactic or conceptual level.
Each lexeme and concept is defined with its features, and may contain specific lexico-structural rules: transfer rules for MT, mapping rules to the next level of representation for surface realization of DSyntS or lexicalization of ConcS.
 Main Grammar: This consists of the lexicostructural mapping rules that apply at this level and which are not lexemeor conceptspecific (e.g.
DSynt-rules for the DSyntmodule, Transfer-rules for the Transfer module, etc).
 Preprocessing grammar: This consists of the lexico-structural mapping rules for transforming the input structures in order to make them compliant with the main grammar, if this is necessary.
Such rules are used to integrate new modules together when discrepancies in the formalism need to be fixed.
This grammar can also be used for adding default features (e.g.
setting the default number of nouns to singular) or for applying default transformations (e.g.
replacing non meaning-bearing lexemes with features).
Postprocessing grammar: This consists of lexico-structural mapping rules for transforming the output structures before they can be processed by the next module.
As for the preprocessing rules, these rules can be used to fix some discrepancies between modules.
Our representation of the lexicon at the lexical level (as opposed to conceptual) is similar to the one found in RealPro.
Figure 4 shows a specification for the lexeme SELL.
This lexeme is defined as a verb of regular morphology with two lexical-structural mappings, the first one introducing the preposition TO for its 3r actant, and the preposition FOR for its 4 th actant: (a seller) X1 sells (merchandise) X2 to (a buyer) X3 f o r (a price) X4.
What is important is that 63 each mapping specifies a transformation between structures at different levels of representation but that are represented in one and the same representation formalism (DSyntS and SSyntS in this case).
As we will see below, grammar rules are also expressed in a similar way.
( c o m p l e t i v e 3 FOR ( prepositional Figure 4: Specification of Lexeme SELL At the conceptual level, the conceptual lexicon associates lexical-structural mapping with concepts in a similar way.
Figure 5 illustrates the mapping at the deep-syntactic level associated with the concept #TEMPERATURE.
Except for the slight differences in the labelling, this type of specification is similar to the one used on the lexical level.
The first mapping rule corresponds to one of the lexico-structural transformations used to convert the interlingual ConcS of Figure 3 to the corresponding DSyntS.
SY SX Note that since each lexicon entry can have more than one lexical-structural mapping rule, the list of these rules represents a small grammar specific to this lexeme or concept.
Realization grammar rules of the main grammar include generic mapping rules (which are not lexeme-specific) such as the DSyntS-rule illustrated in Figure 6, for inserting a determiner.
DSYNT-RULE: More general lexico-structural rules for transfer can also be implemented using our grammar rule formalism.
Figure 8 gives an English-French transfer rule applied to a weather domain for the transfer of a verb modified by the adverb ALMOST: It almost rained.
--o II a failli pleuvoir.
Figure 6: Deep-Syntactic Rule for Determiner Insertion The lexicon formalism has also been extended to implement lexeme-specific lexico-structural transfer rules.
Figure 7 shows the lexicostructural transfer of the English verb lexeme MOVE to French implemented for a military and weather domain (Nasr et al., 1998): Cloud will move into the western regions.
Des nuages envahiront les rdgions ouest.
They moved the assets forward.
-.9 lls ont amen~ les ressources vers l 'avant.
The 79 dcg moves forward.
---~La 79 dcg a v a n c e vers l'avant.
A disturbance will move north of Lake Superior.
--~ Une perturbation se diplacera au nord du lac supdrieur.
Figure 8: English to French Lexico-Structural Transfer Rule with Verb Modifier ALMOST More details on how the structural divergences described in (Dorr, 1994) can be accounted for using our formalism can be found in (Nasr et al., 1998).
5 The
Rule Processing Before being processed, the rules are first compiled and indexed for optimisation.
Each module applies the following processing.
The rules are assumed to be ordered from most specific to least specific.
The application of the rules to the structures is top-down in a recursive way from the f'n-st rule to the last.
For the main grammar, before applying a grammar rule to a given node, dictionary lookup is carried out in order to first apply the lexemeor conceptspecific rules associated with this node.
These are also assumed to be ordered from the most specific to the least specific.
If a lexico-structural transformation involves switching a governor node with one of its dependents in the tree, the process is reapplied with the new node governor.
When no more rules can be applied, the same process is applied to each dependent of the current governor.
When all nodes have been processed, the processing is completed, 6 Using the Framework to build Applications Figure 7: Lexico-Structural Transfer of English Lexerne MOVE to French Figure 9 shows how different instantiations of the tree transduction module can be combined to fibuild NLP applications.
The diagram does not represent a particular system, but rather shows the kind of transformations that have been implemented using the framework, and how they interact.
Each arrow represents one type of processing implemented by an instantiation of the tree transduction module.
Each triangle represents a different level of representation.
Sentence interlingua can also support the generation of French but this functionality has not yet been implemented).
MT:  Transfer on the DSyntS level and realization via SSyntS level for English--French, English--Arabic, English---Korean and Korean--English.
Translation in the meteorology and battlefield domains (Nasr et al., 1998).
 Conversion of the output structures from off-the-shelf English, French and Korean parsers to DSyntS level before their processing by the other components in the framework (Palmer et al., 1998).
7 Lessons
Learned Using the Framework PI "ng Scopeof the Framework SSyntSLI Parsing Input Sentence LI yntS ealization Generated Sentence 1.2 Generated Sentence LI Sentence SSyntS Figure 9: Scope of the Framework's Transformations For example, in Figure 9, starting with the "Input Sentence LI" and passing through Parsing, Conversion, Transfer, DSyntS Realization and SSyntS Realization to "Generated Sentence L2" we obtain an Ll-to-L2 MT system.
Starting with "Sentence Planning" and passing through DSyntS Realization, and SSyntS Realization (including linearization and inflection) to "Generated Sentence LI", we obtain a monolingual NLG system for L1.
So far the framework has been used successfully for building a wide variety of applications in different domains and for different languages: NLG:  Realization of English DSyntSs via SSyntS level for the domains of meteorology (MeteoCogent; Kittredge and Lavoie, 1998) and object modeling (ModelExplainer; Lavoie et al., 1997).
 Generation of English text from conceptual interlingua for the meteorology domain (MeteoCogent).
(The design of the Empirical results obtained from the applications listed in Section 6 have shown that the approach used in the framework is flexible enough and easily portable to new domains, new languages, and new applications.
Moreover, the time spent for development was relatively short compared to that formerly required in developing similar types of applications.
Finally, as intended, the limited computational power of the transduction module, as well as careful implementation, including the compilation of declarative linguistic knowledge to Java, have ensured efficient run-time behavior.
For example, in the MT domain we did not originally plan for a separate conversion step from the parser output to DSyntS.
However, it quickly became apparent that there was a considerable gap between the output of the parsers we were using and the DSyntS representation that was required, and furthermore, that we could use the tree transduction module to quickly bridge this gap.
Nevertheless, our tree transduction-based approach has some important limitations.
In particular, the framework requires the developer of the transformation rules to maintain them and specify the order in which the rules must be applied.
For a small or a stable grammar, this does not pose a problem.
However, for large or rapidly changing grammar (such as a transfer grammar in MT that may need to be adjusted when switching from one parser to another), the fiburden of the developer's task may be quite heavy.
In practice, a considerable amount of time can be spent in testing a grammar after its revision.
Another major problem is related to the maintenance of both the grammar and the lexicon.
On several occasions during the development of these resources, the developer in charge of adding lexical and grammatical data must make some decisions that are domain specific.
For example, in MT, writing transfer rules for terms that can have several meanings or uses, they may simplify the problem by choosing a solution based on the context found in the current corpus, which is a perfectly natural strategy.
However, later, when porting the transfer resources to other domains, the chosen strategy may need to be revised because the context has changed, and other meanings or uses are found in the new corpora.
Because the current approach is based on handcrafted rules, maintenance problems of this sort cannot be avoided when porting the resources to new domains.
An approach such as the one described in (Nasr et al., 1998; and Palmer and al., 1998) seems to be solving a part of the problem when it uses corpus analysis techniques for automatically creating a first draft of the lexical transfer dictionary using statistical methods.
However, the remaining work is still based on handcrafting because the developer must refine the rules manually.
The current framework offers no support for merging handcrafted rules with new lexical rules obtained statistically while preserving the valid handcrafted changes and deleting the invalid ones.
In general, a better integration of linguistically based and statistical methods during all the development phases is greatly needed.
8 History
of the Framework and Comparison with Other Systems realization of deep-syntactic structures in NLG (Lavoie and Rambow, 1997).
It was later extended for generation of deep-syntactic structures from conceptual interlingua (Kittredge and Lavoie, 1998).
Finally, it was applied to MT for transfer between deep-syntactic structures of different languages (Palmer et al., 1998).
The current framework encompasses the full spectrum of such transformations, i.e. from the processing of conceptual structures to the processing of deep-syntactic structures, either for NLG or MT.
Compared to its predecessors (Fog, LFS, JOYCE), our approach has obvious advantages in uniformity, declarativity and portability.
The framework has been used in a wider variety of domains, for more languages, and for more applications (NLG as well as MT).
The framework uses the same engine for all the transformations at all levels because all the syntactic and conceptual structures are represented as dependency tree structures.
In contrast, the predecessor systems were not designed to be rapidly portable.
These systems used programming languages or scripts for the implementation of the transformation rules, and used different types of processing at different levels of representation.
For instance, in LFS conceptual structures were represented as graphs, whereas syntactic structures were represented as trees which required different types of processing at these two levels.
Our approach also has some disadvantages compared with the systems mentioned above.
Our lexico-structural transformations are far less powerful than those expressible using an arbitrary programming language.
In practice, the formalism that we are using for expressing the transformations is inadequate for long-range phenomena (inter-sentential or intra-sentential), including syntactic phenomena such as longdistance wh-movement and discourse phenomena such as anaphora and ellipsis.
The formalism could be extended to handle intrasentential syntactic effects, but inter-sentential discourse phenomena probably require procedural rules in order to access lexemes in The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory: FoG (Kittredge and Polgu~re, 1991), LFS (Iordanskaja et al., 1992), and JOYCE (Rambow and Korelsky, 1992).
The framework was originally developed for the fiother sentences.
In fact, LFS and JOYCE include a specific module for elliptical structure processing.
Similarly, the limited power of the tree transformation rule formalism distinguishes the framework from other NLP frameworks based on more general processing paradigms such as unification of FUF/SURGE in the generation domain (Elhadad and Robin, 1992).
9 Status
The framework is currently being improved in order to use XML-based specifications for representing the dependency structures and the transformation rules in order to offer a more standard development environment and to facilitate the framework extension and maintenance.
Acknowledgements A first implementation of the framework (C++ processor and ASCII formalism for expressing the lexico-structural transformation rules) applied to NLG was developed under SBIR F30602-92-C-0015 awarded by USAF Rome Laboratory.
The extensions to MT were developed under SBIR DAAL01-97-C-0016 awarded by the Army Research Laboratory.
The Java implementation and general improvements of the framework were developed under SBIR DAAD17-99-C-0008 awarded by the Army Research Laboratory.
We are thankful to Ted Caldwell, Daryl McCullough, Alexis Nasr and Mike White for their comments and criticism on the work reported in this paper.
REES: A Large-Scale Relation and Event Extraction System Abstract This paper reports on a large-scale, end-toend relation and event extraction system.
At present, the system extracts a total of 100 types of relations and events, which represents a much wider coverage than is typical of extraction systems.
The system consists of three specialized pattem-based tagging modules, a high-precision coreference resolution module, and a configurable template generation module.
We report quantitative evaluation results, analyze the results in detail, and discuss future directions.
Introduction One major goal of information extraction (IE) technology is to help users quickly identify a variety of relations and events and their key players in a large volume of documents.
In contrast with this goal, state-of-the-art information extraction systems, as shown in the various Message Understanding Conferences (MUCs), extract a small number of relations and events.
For instance, the most recent MUC, MUC-7, called for the extraction of 3 relations (person-employer, maker-product, and organization-location) and 1 event (spacecraft launches).
Our goal is to develop an IE system which scales up to extract as many types of relations and events as possible with a minimum amount of porting effort combined with high accuracy.
Currently, REES handles 100 types of relations and events, and it does so in a modular, configurable, and scalable manner.
Below, Section 1 presents the ontologies of relations and events that we have developed.
Section 2 describes REES' system architecture.
Section 3 evaluates the system's performance, and offers a qualitative analysis of system errors.
Section 4 discusses future directions.
1 Relation
and Event Ontologies As the first step in building a large-scale relation and event extraction system, we developed ontologies of the relations and events to be extracted.
These ontologies represent a wide variety of domains: political, financial, business, military, and life-related events and relations.
"Relations" covers what in MUC-7 are called Template Elements (TEs) and Template Relations (TRs).
There are 39 types of relations.
While MUC TE's only dealt with singular entities, REES extracts both singular and plural entities (e.g., "five executives").
The TR relations are shown in italic in the table below.
Relations 'Artifact Relations Artifact-Name&Aliases Artifact-Type Artifact-Subtype Artifact-Descriptor Place Relations Place-Name&Aliases Place-Type Place-Subtype Place-Descriptor Place-Country Artifact-Maker Artifact-Owner Person Relations Person-Name&Aliases Person-Type Person-Subtype Person-Descriptor Person-Honorific Person-Age Person-PhoneNumber Person-Nationality Organization Relations Org-Name&Aliases Org-Descriptor Org-FoundationDate Org-Nationality Org-TickerSymbol Org-Location Org-ParentOrg Org-Owner Org-Founder Org-StockMarket Person-Affiliation Person-Sibling Person-Spouse Person-Parent Person-Grandparent Person-OtherRelative Person-BirthPlace Person-BirthDate Table 1: Relation Ontology "Events" are extracted along with their event participants, e.g., "who did what to whom when and where"?
For example, for a BUYING event, REES extracts the buyer, the artifact, the seller, and the time and location of the BUYING event.
REES currently covers 61 types of events, as shown below.
Figures 1 and 2 show sample relation and event templates.
Figure 1 shows a Person-Affiliation relation template for "Frank Ashley, a spokesman for Occidental Petroleum Corp'".
<PERSON TYPE: PERSON: ORG: AFFILIATION-AP8802230207-54> := PERSON AFFILIATION [TE for"Frank Ashley"] [TE for "Occidental Petroleum"] Figure 1: Example of Relation Template Figure 2 shows an Attack Target event template for the sentence "an Iraqi warplane attacked the frigate Stark with missiles May 17, 1987.
" <ATTACK TARGET-AP8804160078-12>: = TYPE: CONFLICT SUBTYPE: ATTACK TARGET ATTACKER: [TE for "an Iraqi warplane"] TARGET: [TE for "the frigate Stark"] WEAPON: [TE for "missiles"] TIME: "May 17, 1987" PLACE: [TE for "the gulf'] COMMENT: "attacked" Events Vehicle Vehicle departs Vehicle arrives Spacecraft launch Vehicle crash Personnel Change Hire Terminate contract Promote Succeed Start office Transaction Buy artifact Sell artifact Import artifact Export artifact Give money Business Start business Close business Make artifact Acquire company Sell company Sue organization Merge company Financial Currency moves up Currency moves down Stock moves up Stock moves down Stock market moves up Stock market moves down Stock index moves up Stock index moves down Conflict Kill Injure Hijack vehicle Hold hostages Attack target Fire weapon Weapon hit Invade land Move forces Retreat Surrender Evacuate Figure 2: Example of Event Template Crime Sexual assault Steal money Seize drug Indict Arrest Try Convict Sentence Jail Political Nominate Appoint Elect Expel person Reach agreement Hold meeting Impose embargo Topple Family Die Marry System Architecture and Components Figure 3 illustrates the REES system architecture.
REES consists of three main components: a tagging component (cf.
Section 2.1), a co-reference resolution module (cf.
Section 2.2), and a template generation module (cf.
Section 2.3).
Figure 3 also illustrates that the user may run REES from a Graphical User Interface (GUI) called TemplateTool (cf.
Section 2.4).
Tagging Modules The tagging component consists of three modules as shown in Figure 3: NameTagger, NPTagger and EventTagger.
Each module relies on the same pattern-based extraction engine, but uses different sets o f patterns.
The NameTagger recognizes names o f people, organizations, places, and artifacts (currently only vehicles).
Table 2: Event Ontology GUI interaction Figure 3: The REES System Architecture syntactically-based generic patterns.
These The NPTagger then takes the XML-tagged output of the NameTagger through two phases.
First, it recognizes non-recursive Base Noun Phrase (BNP) (our specifications for BNP resemble those in Ramshaw and Marcus 1995).
Second, it recognizes complex NPs for only the four main semantic types of NPs, i.e., Person, Organization, Location, and Artifact (vehicle, drug and weapon).
It makes postmodifier attachment decisions only for those NPs that are crucial to the extraction at hand.
During this second phase, relations which can be recognized locally (e.g., Age, Affiliation, Maker) are also recognized and stored using the XML attributes for the NPs.
For instance, the XML tag for "President of XYZ Corp".
below holds an AFFILIATION attribute with the ID for "XYZ Corp".
<PNP ID="03" AFFILIATION="O4">Presidentof <ENTITY ID="04">XYZ Corp.</ENTITY> </PNP> patterns tag events in the presence of at least one of the arguments specified in the lexical entry for a predicate.
Subsequent pattems try to find additional arguments as well as place and time adjunct information for the tagged event.
As an example of the EventTagger's generic patterns, consider the simplified pattern below.
This pattem matches on an event-denoting verb that requires a direct object of type weapon (e.g., "fire a gun") (& {AND $VP {ARG2_SYN=DO} {ARG2_SEM=WEAPON}} {AND $ARTIFACT {SUBTYPE=WEAPON}})1 The important aspect of REES is its declarative, lexicon-driven approach.
This approach requires a lexicon entry for each event-denoting word, which is generally a I &=concatenation, AND=Boolean operator, $VP and SARTIFACT are macro references for complex phrases.
71:1 Building upon the XML output of the NPTagger, the EventTagger recognizes events applying its lexicon-driven, fiverb.
The lexicon entry specifies the syntactic and semantic restrictions on the verb's arguments.
For instance, the following lexicon entry is for the verb "attack".
It indicates that the verb "attack" belongs to the CONFLICT ontology and to the ATTACK_TARGET type.
The first argument for the verb "attack" is semantically an organization, location, person, or artifact (ARGI_SEM), and syntactically a subject (ARGI_SYN).
The second argument is semantically an organization, location, person or artifact, and syntactically a direct object.
The third argument is semantically a weapon and syntactically a prepositional phrase introduced by the preposition "with".
ATTACK {{{CATEGORY VERB} {ONTOLOGY CONFLICT} {TYPE ATTACK_TARGET} {ARGI_SEM {ORGANIZATION LOCATION PERSON ARTIFACT} } {ARGI_SYN {SUBJECT}} {ARG2_SEM {ORGANIZATION LOCATION PERSON ARTIFACT} } {ARG2_SYN {DO}} {ARG3_SEMWEAPON} } {ARG3_SYN {WITH}}}} About 50 generic event extraction patterns, supported by lexical information as shown above, allow extraction of events and their arguments in cases like: An lraqi warplane attacked the frigate Stark with missiles May 17, 1987.
This generic, lexicon-driven event extraction approach makes REES easily portable because new types of events can be extracted by just adding new verb entries to the lexicon.
No new patterns are required.
Moreover, this approach allows for easy customization capability: a person with no knowledge of the pattern language would be able to configure the system to extract new events.
While the tagging component is similar to other pattern-based IE systems (e.g., Appelt et al.1995; Aone et al.1998, Yangarber and Grishman 1998), our EventTagger is more portable through a lexicon-driven approach.
Co-reference Resolution After the tagging phase, REES sends the XML output through a rule-based co-reference resolution module that resolves:   definite noun phrases of Organization, Person, and Location types, and singular person pronouns: he and she.
Only "high-precision" rules are currently applied to selected types of anaphora.
That is, we resolve only those cases of anaphora whose antecedents the module can identify with high confidence.
For example, the pronoun rules look for the antecedents only within 3 sentences, and the definite NP rules rely heavily on the head noun matches.
Our highprecision approach results from our observation that unless the module is very accurate (above 80% precision), the coreference module can hurt the overall extraction results by over-merging templates.
Template Generation Module A typical template generation module is a hard-coded post-processing module which has to be written for each type of template.
By contrast, our Template Generation module is unique as it uses declarative rules to generate and merge templates automatically so as to achieve portability.
Declarative Template Generation REES outputs the extracted information in the form of either MUC-style templates, as illustrated in Figure 1 and 2, or XML.
A crucial part of a portable, scalable system is to be able to output different types of relations and events without changing the template generation code.
REES maps XML-tagged output of the co-reference module to templates using declarative template definitions, which specifies the template label (e.g., ATTACK_TARGET), XML attribute names (e.g., ARGUMENT l), corresponding template slot names (e.g., ATTACKER), and the type restrictions on slot values (e.g., string).
Event Merging One of the challenges of event extraction is to be able to recognize and merge those event descriptions which refer to the same event.
The Template Generation module uses a set of declarative, customizable rules to merge coreferring events into a single event.
Often, the rules reflect pragmatic knowledge of the world.
For example, consider the rule below for the DYING event type.
This rule establishes that if two die events have the same subject, then they refer to the same event (i.e., a person cannot die more than once).
{merge {EVENT 1 {AND {SUBTYPE DIE} {PERSON training set (200 texts) and the blind set (208 texts) from about a dozen news sources.
Each set contains at least 3 examples of each type of relations and events.
As we mentioned earlier, "relations" includes MUC-style TEs and TRs.
Text Set Task Train Blind Rel.
Events Rel.
& Events Rel.
Events Rel.
& Events Templates in keys 9955 2525 10707 F-M {EVENT 2 {AND {SUBTYPE DIE} {PERSON Table 3: Evaluation Results 2.4 Graphical User Interface (GUI) For some applications such as database population, the user may want to validate the system output.
REES is provided with a Javabased Graphical User Interface that allows the user to run REES and display, delete, or modify the system output.
As illustrated in Figure 4, the tool displays the templates on the bottom half of the screen, and the user can choose which template to display.
The top half of the screen displays the input document with extracted phrases in different colors.
The user can select any slot value, and the tool will highlight the portion of the input text responsible for the slot value.
This feature is very useful in efficiently verifying system output.
Once the system's output has been verified, the resulting templates can be saved and used to populate a database.
3 System
Evaluation The blind set F-Measure for 31 types of relations (73.95%) exceeded our initial goal of 70%.
While the blind set F-Measure for 61 types o f events was 53.75%, it is significant to note that 26 types of events achieved an FMeasure over 70%, and 37 types over 60% (cf.
Table 4).
For reference, though not exactly comparable, the best-performing MUC-7 system achieved 87% in TE, 76% in TR, and 51% in event extraction.
F-M in blind set 90-100 80-89 Event types 2 : Buy artifact.
Marry 9 : Succeed, Merge company, Kill, Surrender, Arrest, Convict, Sentence, Nominate, Expel.
15 : Die, Sell artif~/ct,Export Artifact, Hire, Start office, Make artifact, Acquire company, Sue organization, Stock Index moves down, Steal money, Indict, Jail, Vehicle crash, Elect, Hold meeting.
Table 4: Top-performing Event Types The table below shows the system's recall, precision, and F-Measure scores for the Regarding relation extraction, the difference in the score between the training and blind sets was very small.
In fact, the total F-Measure on the blind set is less than 2 points lower than that of the training set.
It is also interesting to note that for 8 of the 12 relation types where the F-Measure dropped more than 10 points, the training set includes less than 20 instances.
In other words, there seems to be a natural correlation between low number of instances in the training set and low performance in the blind set.
There was a significant drop between the training and blind sets in event extraction: 11 points.
We believe that the main reason is that the total number of events in the training set is fairly low: 801 instances of 61 types of events (an average of 13/event), where 35 o f the event types had fewer than 10 instances.
In fact, 9 out of the 14 event types which scored lower than 40% F-Measure had fewer than I0 examples.
In comparison, there were 34,000 instances of 39 types of relations in the training set.
The contribution o f the co-reference module is illustrated in the table below.
Co-reference resolution consistently improves F-Measures both in training and blind sets.
Its impact is larger in relation than event extraction.
Text set Task Coreference rules No coreference rules Training Blind Relations Events Relations & Events Relations Events Relations & Events Table 5: Comparative results with and without co-reference rules In the next two sections, we analyze both false positives and false negatives.
False Positives (or Precision Errors) REES produced precision errors in the following cases:  Most of the errors were due to overgeneration of templates.
These are mostly cases of co-referring noun phrases that the system failed to resolve.
For example: "Panama...
the nation ...
this country.., his country" Rules for the co-reference module are still under development, and at present REES handles only limited types of plural noun phrase anaphora.
Spurious events resulted from verbs in conditional constructions (e.g., "if ...
then")... or from ambiguous predicates.
For instance, "appoint" as a POLITICAL event vs.
a PERSONNEL CHANGE event.
The subject of a verb was misidentified.
This is particularly frequent in reduced relative clauses.
Kabul radio said the latest deaths brought to 38 the number o f people killed in the three car bomb explosions, (Wrong subject: "the number of people" as the KILLER instead of the victim) False Negatives (or Recall Errors) Below, we list the most frequent recall errors in the training set.
 Some event arguments are mentioned with event nouns instead of event verbs.
The current system does not handle noun-based event extraction.
India's acquisition last month of the nuclear submarine from the Soviet Union...
(SELLER="Soviet Union" and TIME="last month'" come with the nounbased event "acquisition").
 Pronouns "it" and "they," which carry little semantic information, are currently not resolved by the co-reference module.
It also has bought three late-1970s vintage ICilo class Soviet submarines and two West German HDW 209 subs (Missed BUYER=India because of unresolved it).
Verb arguments are a conjunction of noun phrases.
The current system does not handle coordination of verb arguments.
Hezbollah killed 21 lsraelis and 43 o f Lahad's soldiers (The system gets only the first object: 21 Israelis.
) Ellipsis cases.
The current system does not handle ellipsis.
The two were sentenced to five-year prison terms with hard labor by the state security court...
(Missed PERSON_SENTENCED fill because of unresolved the two).
The subject of the event is relatively far from the event-denoting verb: Vladislav Listyev, 38, who brought television interview shows in the style of Phil Donahue or Larry King to Russian viewers and pioneered hard-hitting television journalism in the 1980s, was shot in the heart by unknown assailants and died immediately...
(The system missed subject Vladislav Listyev for attack event shot) Missed ORG LOCATION relations for locations that are part o f the organization's name.
Larnaca General Hospital (Missed ORG_LOCATION TR for this and Larnaca.
) We asked a person who is not involved in the development of REES to review the event extraction output for the blind set.
This person reported that:  In 35% of the cases where the REES system completely missed an event, it was because the lexicon was missing the predicate.
REES's event predicate lexicon is rather small at present (a total of 140 verbs for 61 event types) and is mostly based on the examples found in the training set,  In 30% of the cases, the subject or object was elliptical.
The system does not currently handle ellipsis.
In 25% of the cases, syntactic/semantic argument structures were missing from existing lexical entries.
It is quite encouraging that simply adding additional predicates and predicate argument structures to the lexicon could significantly increase the blind set performance.
Desmond Tutu and Albertina Sisulu are important...
We plan to develop a generic set of patterns for noun-based event extraction to complement the set of generic verb-based extraction patterns.
5 4 Future Directions We believe that improving co-reference resolution and adding noun-based event extraction capability are critical to achieving our ultimate goal of at least 80% F-Measure for relations and 70% for events.
4.1 Co-reference Resolution Conclusions As discussed in Section 3.1 and 3.2, accurate co-reference resolution is crucial to improving the accuracy of extraction, both in terms of recall and precision.
In particular, we identified two types of high-payoff coreference resolution:  definite noun phrase resolution, especially plural noun phrases  3 rd person neutral pronouns "it" and "they".
4.2 Noun-based Event Extraction In this paper, we reported on a fast, portable, large-scale event and relation extraction system REES.
To the best of our knowledge, this is the first attempt to develop an IE system which can extract such a wide range of relations and events with high accuracy.
It performs particularly well on relation extraction, and it achieves 70% or higher F-Measure for 26 types of events already.
In addition, the design of REES is highly portable for future addition of new relations and events.
Acknowledgements This project would have not been possible without the contributions of Arcel Castillo, Lauren Halverson, and Sandy Shinn.
Our thanks also to Brandon Kennedy, who prepared the hand-tagged data.
REES currently handles only verb-based events.
Noun-based event extraction adds more complexity because: Nouns are often used in a generic, nonreferential manner (e.g., "We see a m e r g e r as being in the consumer's interest"), and When referential, nouns often refer to verb-based events, thus requiring nounverb co-reference resolution ("An F-14 crashed shortly after takeoff...
The crash").
PROBLEMS IN NATURAL-LANGUAGE INTERFACE WITH EXAMPLES FROM EUFID Marjorie T e m p l e t o n John Burger S y s t e m Development Corporation Santa Mortice, California TO DSMS ABSTRACT For five years t h e End-User Friendly Interface to Data management (EUFID) project team at System Development Corporation worked on the design and implementation of a Natural-Language Interface (NLI) system that was to be independent of both the application and the database management system.
In this paper we describe application, n a t u r a l -l a n g u a g e and d a t a b a s e management problems involved in NLI development, with specific reference to the EUFID system as an example.
I INTRODUCTION users.
Tools that could assist in automating this process are badly needed.
The second set of issues involves language processing techniques: how to assign constituent structure and interpretation to queries using robust and general methods that allow extension to additional lexical items, sentence types and semantic relationships.
Some NLI systems d i s t i n g u i s h the assignment of syntactic structure, o r parsing, from the interpretation.
Other systems, including EUFID, combine information about constituent and semantic structure into an integrated semantic grammar.
The third class involves database issues: how to actually perform the intent of the natural-language question by formulating the correct structured query and e f f i c i e n t l y n a v i g a t i n g through the database to retrieve the right answer.
This involves a thorough understanding of the DBMS structure underlying the a p p l i c a t i o n, the operations and functions the query language supports, and the nature and volatility of the database.
Obviously issues in these three areas are related, and the knowledge needed to deal with them may be distributed throughout a natural-language interface system.
The purpose of this paper is to show how such issues might be addressed in NLI development, with illustrations from EUFID.
The next section includes a brief review of related work, and an o v e r v i e w of the EUFID system.
The third section describes the goals that EUFID achieved, and section four discusses in detail ~ome of the major application, language, and database problems that arose.
Section five suggests guidelines for determining whether an application is an appropriate target for a n a t u r a l l a n g u a g e interface.
From 1976 t o 1981 SDC was involved in the development of the End-User Friendly Interface to Data management (EUFID) system, a n a t u r a l l a n g u a g e interface (NLI) that is designed to be independent of both the application and the underlying d a t a b a s e management system (DBMS).
[TEMP79, TEMP80, BURG80, BURG82].
The EUFID system permits users to communicate with database management systems in natural English rather than formal query languages.
It is assumed that the application domain is well defined and bounded, that users share a common language to address the application, and that users may have little experience with computers or DBMSs but are competent in the application area.
At least three broad categories of issues had to be addressed during EUFID development, and it is apparent that they are common to any general naturallanguage interface to database management systems.
The first category involves the application: how to c h a r a c t e r i z e the requirements of the human-machine dialogue and interaction, capture that information efficiently, formalize the information and incorporate that knowledge into a framework that can be used by the system.
The major problems in this area are knowledge acquisition and representation.
For many NLI systems, bringing up a new application requires extensive effort by system designers with cooperation from a representative set of endfiII BACKGROUND Over the past two decades a considerable amount of work has gone into the d e v e l o p m e n t of natural-language systems.
Early developments were in the areas of text processing, syntactic parsing techniques, machine translation, and early attempts at English-language question answering systems.
Several early question-answering experiments are reviewed by R.
F. Simmons in [SIMM65].
Waltz has edited a collection of short papers on topics related to naturallanguage and artificial intelligence in a survey of NLI research [WALT77].
A survey of NLIs and evaluation of several systems with respect t o their applicability to command and control environments can be found in [OS179].
A. RELATED WORK involved with problems of semantics and has three separate layers of semantic u n d e r s t a n d i n g. The layers are called "English Formal Language", "World Model Language", and "Data Base Language" and appear to c o r r e s p o n d roughly to the "external", "conceptual", and "internal" views of data as d e s c r i b e d by C.
J. Date [DATE77].
PHLIQAI can interface to a v a r i e t y of d a t a b as e structures and DBMSs.
5. The Programmed LANguage-based Enquiry System (PLANES) [WALT78] uses an ATN based parser and a semantic case frame analysis to understand questions.
Case frames are used to handle pronominal and elliptical reference and to g e n e r a t e responses to clarify partially interpreted questions.
REL [THOM69], initially written entirely in assembler code for an IBM36@, has been in continuous development since 1967.
REL allows a user to make interactive extensions to the g r a m m a r and semantics of the system.
It uses a formal grammar expressed as a set of general re-write rules with semantic transformations attached to each rule.
Answers are obtained from a b u i l t i n database.
RENDEZVOUS [CODD74] addresses the problem of c e r t a i n t y regarding the machine's understanding of the user's question.
It engages the user in d i a l o g u e to specify and disambiguate the question and will not route the formal query to the relational DBMS until the user is satisfied with the machine's interpretation.
ROBOT [HARR78] is one of the few NLI systems currently a v a i l a b l e on the commercial market.
It is the basis for Cullinane's OnLine English [CULL80] and Artificial Intelligence C o r p o r a t i o n ' s Intellect [EDP82].
It uses an extracted version of the database for lexical data to assist the ATN parser.
TORUS [MYLO76], like RENDEZVOUS, engages the user in a d i a l o g u e to specify and d i s a m b i g u a t e the user's question.
It is a research o r i e n t e d system looking at the problems of knowledge representation, and some effort has been spent on the understanding of text as well as questions.
While few NLIs have reached the commercial marketplace, many systems have c o n t r i b u t e d to advancing the state of the art.
Several representative systems and the problems they addressed are described in this section.
i. CONVERSE [KELLT1] used formal syntactic analysis to g e n e r a t e surfaceand d e e p s t r u c t u r e parsings together with formal semantic t r a n s f o r m a t i o n rules to produce queries for a built-in relational DBMS.
It was written in SDC LISP and ran on IBM 37@ computers.
Started in 1968, it was one of the first naturallanguage processors to be built for the purpose of querying a separate data m a n a g e m e n t system.
LADDER [HEND77] was designed to access large d i s t r i b u t e d databases.
it is implemented in INTERLISP, runs on a PDP-I@, and can interface to different DBMSs with proper configuration.
It uses a semantic g r a m mar and, like EUFID and most NLIs, a different grammar must be defined for each application.
The Lunar Rocks system LSNLIS [WOOD72] was the first to use the Augmented Transition Network (ATN) grammar.
Wrl~ten in LISP, it transformed formally parsed questions into representations of the first-order predicate calculus for deductive processing against a built-in DBMS.
PHLIQAI [SCHA77] uses a syntactic parser which runs as a separate pass from the semantic understanding passes.
This system is mainly fiB.
OVERVIEW OF EUFID EUFID is a general purpose naturallanguage front-end for database management.
The original design goals for EUFID were: to b e application independent.
This means that the program must be table driven.
The tables contain the dictionary and semantic information and are loaded with a p p l i c a t i o n s p e c i f l c data.
It was desired that the tables could be constructed by someone other than the EUFID staff, so t h a t users could build new applications on their o w n . to be database independent.
This means that the organization of the data in the database must be representable in tables that drive the query generator.
~ A database reorganization that does not change the semantics of the application should be transparen~ to the user.
written in a high level language; initially a customer required code to be written in FORTRAN, later we were able to use the "C" programming language.
to support different views data for security purposes.
of the The design which met these requirements is a modular system which uses an Intermediate Language (IL) as the output of the natural-language analysis system [BURG82].
This language represents, in many ways, the union of the c a p a b i l i t i e s of many "target" DBMS q u e r y languages.
The EUFID system consists of three major modules, not counting the DBM3 (see Figure I).
The analyzer (parser) module is table driven.
It is n e c e s s a r y only to properly build and load the tables to interface EUFID to a new application.
Mapping a question from its d i c t i o n a r y (user) representation to DBMS representation is handled by mapping functions contained in a table and applied by a separate module, t h e "mapper".
Each c o n tent (application dependent) word in the d i c t i o n a r y has one or more mapping functions defined for it.
A final stage of the mapper is a q u e r y l a n g u a g e generator containing the syntax of IL.
This stage writes a query in IL using the group/field names found by the mapper t o represent the user's concepts and the structural relationships between them.
This design satisfies t h e requirement of application independence.
ENGLISH QUESTION to be DBMS independent.
This means that it must be able to generate requests to different DBMSs in the DBMS's query language and that the interface of EUF~D to a different DBMS should not require changes to t h e NLI modules.
Transferring the same database with the same semantic content to another DBMS should be transparent to the natural-language users.
to run on a mini-computer that might possibly be different from the computer with the DBMS.
to have a fast response time, even when the question cannot be interpreted.
This means it must be able quickly to recognize unanalyzable constructs.
Figure i: EUFID Block Diagram to handle nonstandard or poorlyformed (but, nevertheless, meaningful) questions.
to be portable to various machines.
This means that the system had to be * We make a technical distinction between the words "question" and "query".
A question is any string entered by the user to the EUFID analyzer, regardless of the terminating punctuation.
This is consistent with the design since EUFID treats all input as a request for information.
A query is a formal representation of a question in either the EUFID intermediate language IL, or in the formal query language of a DBMS.
For each different DBMS used by a EUFID application, a "translator" module needs to be written to convert a query in IL to the equivalent in the DBMS query language.
This design satisfies the requirement of DBMS independence.
Other modules are the system controller, a "help" module, and a " s y n o n y m editor".
An "Application Definition Module" is used off-line to assist in the creation of the run-time application description tables.
The following subsections descrloe each of the modules of the EUFID system, and give our m o t i v a t i o n for design.
i. A~plication Definitions Bringing up a new a p p l i c a t i o n is a long and complex process.
The d a t a b a s e d e f i n i t i o n must be transmitted to EUFID.
A large corpus of "typical" user questions must be collected from a representative set of users and from these the dictionary and mapping tables are designed.
A "semantic graph" is defined for the application.
This graph is implicitly realized in t h e dictionary where the nodes of the graph are the definitions of English content words and the c o n n e c t i v i t y of the graph is implied by the case-structure relationships defined for the nodes.
All d i c t i o n a r y and mapping-function are then entered into computer files which are processed by the Application Definition Module (ADM) to produce t h e run-time tables.
These final tables are complex structures of pointers, character strings, and index tables, designed to decrease access time to the information required by the analyzer and mapper modules.
data considered.
Frequently, desig~ :o,~s i d e r a t i o n s in the m a p p i n g f u n c t i o n list necessitate going back and m o d i f y i n g the content of the d i c t i o n a r y . This is an example of the o v e r l a p of the l i n g u i s t i c and database issues in assigning an interpretation to a question.
c. Database Representation The ADM, typically, needs to be run several times to "debug" the tables.
EUFID interfaces to three applications currently exist, and building tables for each new a p p l i c a t i o n took less time than the previous one, b u t it still requires several staff-months to bring up a new application.
a. User-View Representation The structure o f the data in the user's database is represented in two tables, called the CAN (for canonical) and REL (for relationships) tables.
Taking advantage of the fact that any database can be represented in relational form, EUFID lists each d a t a b a s e g r o u p as if it were a relation.
Group-to-group linkage (represented in the REL table) is d e a l t with as if a join* were necessary to implement the link.
For h i e r a r c h i c a l and network DBMSs the join will not be needed: the link is "wired in" to the d a t a b a s e structure.
EUFID nevertheless assumes a join m a i n l y in order to facilitate the writing of g r o u p t o g r o u p links in IL, which is a relational language.
The CAN table includes database-specific information for each field (attribute) of each group (relation), such as field name, containing group, name of d o m a i n from which attributed gets its values, and a pointer to a set of c o n v e r s i o n functions for numeric v a l u e s which can be be used to convert from one unit of m e a s ure to another (e.g., feet to meters).
These data are used by the run-time modules which map and translate the t r e e s t r u c t u r e d output of the analyzer to IL on the actual g r o u p / f i e l d names of the database, and then co the language of the DBMS.
These modules are d i s c u s s e d in the next sections.
2. The EUFID Analyzer All information on the user's view of the database is kept in the d i c t i o n ary.
The dictionary consists of two kinds of words and definitions.
Function words, such as p r e p o s i t i o n s and Conjunctions, are pre-stored in each a p p l i c a t i o n ' s d i c t i o n a r y and are used by the analyzer for direction on how to connect the semantic-graph nodes during analysis.
Content words are application dependent.
The d c r O o n s of content words are semantic-graph nodes.
The connectivity o the graph is indicated by semantic case slots and pointers contained in the nodes.
A form of semantic-case is used to indicate the attributes of an entity (e.g., adjectives, prepositional phrases, and other modifiers of a noun).
b. Mapping Functions The current version of the EUFID analyzer employs a variant of the CockeK a s a m i Y o u n g e r algorithm for parsing its input.
This classical nonpredictive b o t t o m u p algorithm has been used in a family of "chart parsers" developed by Kay, Earley, and others [AHO72].
The main features of these parsers are: (i) They use a r b i t r a r y c o n t e x t f r e e grammars.
There are no r e s t r i c t i o n s on rules which have l e f t r e c u r s i o n or other c h a r a c t e r i s tics which sometimes cause difficulty.
(2) They produce all possible parses of a given input string.
The g r a m m a r s they use may be ambiguous at either the nonterminalor t e r m i n a l s y m b o l levels.
In natural-language processing, this allows for a precise r e p r e s e n t a t i o n of * The t e r m "join" refers to a composite o p e r a t i o n between two relations in a relational DBMS.
The list of mapping functions is derived from the dictionary.
Every possible connection of every node has to be fiboth the syntactic and lexical ambiguities which may be present in an input sentence.
(3) They provide partial parses of the input.
Each non-terminal symbol derives some input substring.
Even if no such substring spans the entire sentence, i.e., no complete parse is achieved, analyses of various regions o f t h e s e n t e n c e a r e available.
(4) They are conceptually straightforward and easy t o implement.
The speed and storage considerations which have kept such parsers from being widely used in compilers are less relevant in the analysis o f short strings such as queries to a DBMS.
The grammar used b y the EUFID parser is essentially semantic.
The symbols of the grammar r e p r e s e n t t h e concepts underlying lexical items, and the rules specify the ways in which these concepts can be combined.
More s p e c i f i c a l l y, the concepts are o r g a n i z e d into a case system.
Each rule states that a given pair of constituents can be linked if the conc e p t u a l head of o n e constituent fills a case on the conceptual head of t h e other.
A degree of context sensitivity is achieved b y attaching predicates to the rules.
These predicates b l o c k application of t h e rules unless certain (usually syntactic) conditions hold true.
The parser uses syntactic information only "on demand", that is, only when such information is necessary to resolve semantic ambiguities.
This a d d s to its coverage and robustness, and makes it relatively insensitive to the phrasing variations which must be explicitly accounted for in many other systems.
3. Mapping to-field and g r o u p t o g r o u p tions of t h e database.
connecThe mapper makes use of a table of mapping functions.
The table contains at least one mapping function for every content word in the dictionary.
The analyzer's tree is traversed bottom up, applying mapping functions to each node on t h e way.
Mapping f u n c t i o n s are context sensitive with respect to those nodes below it in the tree: nodes that have already been mapped.
A new tree is g r a d u a l l y formed and connected this way.
Mapping functions may indicate that the map of a semantic-graph node is a database node (that is, a group or field name), o r a pre-connected sub-tree of database nodes.
The mapping function may also indicate removal of a database node or m o d i f i c a t i o n to the existing structure of the tree being constructed.
The new t r e e i s c r e a t e d in terms of the database groups and f i e l d s and i t s structure reflects the connectivity of the database.
A final stage of the mapper traverses this new tree and generates the EL statement of the query using a table of the syntax and keywords of EL and the database names from the tree.
The mapper module converts the output of the analyzer to input for the translator module.
Analyzer output is a tree structure where the nodes are semantic-graph nodes corresponding to the content words in the user's question and obtained from the dictionary.
An alternative method of mapping that is now being investigated involves breaking the process into two basic parts.
The first step would be to map the tree o u t p u t o f the analyzer t o an IL query on what C.
J. Date calls the "conceptual schema" of the database [DATE77].
A second step would take this IL input and re-arrange the schema connectivity (and names of groups and fields) from that of the conceptual schema to that of the actual target database, generating another IL query as input to the current translators.
Input to the translator module is a string in the syntax of IL which contains the names of actual groups and fields in the database.
The mapping algorithm, thus, has to make several levels of conversion simultaneously: it must convert a into a linear string it must convert into database names, and tree structure of tokens, semantic-graph nodes groupand fieldit must convert the connectivity of the tree (representing concept-toconcept linkage in English) into the (frequently very different) groupThe final run-time module in EUFID is a syntax translator that converts IL to the actual DBMS query language.
If necessary, the translator can also add access-path information related t o database search.
Currently, two translators have been written.
One converts IL to QUEL, a relatively simple conversion into the language of the relational database management system INGRES [STONY6].
The other translator converts IL into the query language of the World-Wide Data Management System (WWDMS) [HONE76] used by the Department of Defense, and also handles additional access path information.
This translator was quite difficult to design and build because of the highly procedural nature of the WWDMS query system.
The output of a translator is sent to the appropriate DBMS.
In the EUFID system running at SDC, a QUEL query is submitted directly to INGRES running on the same PDP-II/70 as EUFID.
For testing purposes, queries generated by the WWDMS translator were transmitted from a PDP11/70 to a Honeywell H6000 with a WWDMS database.
5. Application Description some coming from open-ended domains.
A I R E P has a network database structure and contains the same data s t r u c t u r e in four d i f f e r e n t files.
III LEVEL OF SUCCESS EUFID runs o n three d i f f e r e n t application databases.
The METRO a p p l i c a t i o n involves monitoring of shipping transactions between companies in a city called "Metropolis".
There are ten companies located in any one of three n e i g h b o r hoods.
Each company rents warehouse space for shipping/recelving transactions, and has local offices which receive goods.
The data is organized telationally using the INGRES database m a n a g e m e n t system.
That means that there are no n a v i g a t i o n a l links stored in the records (called "relations") and there is no predefined "root" to the database structure.
Access may be made from any relation to any other relation as long as there is a field in each of the two relations which has the same "domain" (set of values).
AIREP (ADP Incident REPorting) is a network database, implemented in WWDMS.
It c o n t a i n s reports about hardware and software failures and resolution of the problems in a large computer system.
Active problems are maintained in an active file and old, solved problems are moved to an historical file.
If a problem [s reported more than once, an abbreviated record is made for the additional report, called the "duplicate incident" record.
This means that there are four basic type of report: active incidents, duplicate incidents, historical incidents, and historical duplicate incidents.
In addition, there are records about sites, problems, and solutions.
The A P P L I C A N T database is a relational database implemented in INGRES that contains information about job applicants and their backgrounds.
The central entity is the "applicant", while other relations describe the a p p l i c a n t ' s specialties, education, previous employment, computer experience, and interviews.
Each database has d i f f e r e n t features chat may present problems for a naturallanguage interface but which are typical of 'real-world' applications.
METRO has relatively few entities but has complex relationships among them.
APPLICANT has many updates and many different values, Most of the EUFID d e s i g n g o a l s were actually met.
EUFID runs on a minicomputer, a DEC PDP 11/70.
It is application, database, and DBMS independent.
A typical q u e s t i o n is analyzed, mapped and translated in five to fifteen seconds even with g r a m m a t i c a l l y incorrect input.
The analyzer c o n t a i n s a good spelling corrector and a good morphology a l g o r i t h m that strips inflectional endings so that all inflected forms of words need not be stored explicitly.
A "synonym editor" permits the user to replace any word or string of words in the dicionary with another word or string, to accommodate personal jargon and expressability.
A "Concept Graph Editor s allows a database administrator to m o d i f y tables and define user profiles so that d i f f e r e n t users may have limited views of the data for s e c u r i t y purposes.
The analysis strategy, based on a semantic grammar, permits easy and natural paraphrase recognition, although there are linguistic c o n s t r u c t s it cannot handle.
These are d i s c u s s e d below.
An English word may have more than one definition without c o m p l i c a t i n g the analysis strategy.
For example, "ship" as a vessel and as a verb meaning "to send" can be defined in the same d i c t i o n ary.
Words used as database values, such as names, may also have m u l t i p l e definitions, e.g., "New York" used as the name of both a city and a state.
The mapper, despite its many limitations, can c o r r e c t l y map almost all trees output by the analyzer.
It is able to handle English c o n j u n c t i o n s, mapping them a p p r o p r i a t e l y to logical ANDs or ORs, and understanding that some "ands" may need to be interpreted as OR and vice-versa under certain c i r c u m s t a n c e s . It is able to g e n e r a t e calls on DBMS calculations (e.g., average) and user-defined functions (e.g., marine great-circle distance) if the user-function exists and is supported by the DBMS.
Questions involving time are interpreted in a reasonable way.
Functions are defined for "between" and "during" in the METRO application.
The AIREP application allows time comparisons such as "What system was running when incident J123 occurred" which require a test to see if a point in time is within an interval.
The mapper can translate "user values" (e.g., "Russian") to database values (e.g., "USSR"), and convert one unit of measure (e.g., feet) to another (e.g., meters).
EUFID c a n i n t e r f a c e to very complex relational and CODASYL-type databases having difficult n a v i g a t i o n and parallel structures.
In t h e AIREP application a consistent WWDMS navigational m e t h o d o l o g y is used to access non-key records.
The system c a n also map to t h e parallel, but not identical, structures for duplicate and historical incidents.
I n the INGRES applications, EUFID is able to use and correctly map to = r e l a tionship relations" which relate two or more other relations.
For example, the METRO relation =cw" contains a company name, a warehouse name, a n d a date.
This represents the initial business contact.
A user might ask, =When d i d C o l o n i a l start t o do b u s i n e s s w i t h Superior?
= or  When d i d b u s i n e s s b e g i n b e t w e e n C o l o n i a l and S u p e r i o r ? =, e i t h e r of which must ~oin both t h e c o m p a n y ( " c =) a n d t h e w a r e h o u s e ('w') relations t o the =cw" relation.
The system c o n t r o l module keeps a journal of all user-system interaction together with internal module-to-module data such as the IL for the user's question and the generated DBMS query.
The system also employs a very effective HELP module which, under certain circumstances, is context sensitive t o the problem affecting the user.
IV PROBLEMS APPLICANT database may wish to fill a specific Job opening while others may collect statistics on types of appli~ cants.
The language used for these two functions can be quite different, and it is n e c e s s a r y to have extensive interaction with cooperative users in order to characterize the kinds of dialogues they will have with the system.
Not only must representative language protocols be collected, but desired responses must be understood.
For example, to answer a question such as =What is t h e status of our forces in Europe =, the system must know whether 'our' refers to U.S. or NATO or some other unit.
The importance of this interaction between potential users and system developers should n o t b e underestimated, as it is the basis for defining much of the knowledge base needed by the system, and may also be t h e basis for eventual user acceptance o r rejection of the NLI system.
2. Value R e c o g n i t i o n This section describes problems associated with EUFID development that appear to be common to natural-language interfaces to database management systems.
They are loosely classified into the major areas Of application, language and database management issues, although there may be overlap.
Criteria for evaluating whether an application is appropriate for a natural-language front-end are also described.
A. APPLICATION DEFINITION PROBLEMS A "value = is a specific datum stored in the database, and is the smallest piece of data obtainable as the result o f a database query.
For example, in response to the question "What companies in North Hills shipped light freight to Superior?
= the METRO DBMS returns two values: "Colonial" and "Supreme'.
Values can also be used in a query to qualify or select certain records for output, e.g., in t h e above question "North Hills" and "Superior" are values that must be represented in the query to the DBMS.
As long as the alphanumeric values used in a particular database field are the same as words in t h e English questions, there are no difficult problems involved in recognizing values as selectors in a query.
There are three basic ways to recognize these value words in a question.
They can be explicitly listed in the dictionary, recognized by a pattern or context, or found in the database itself.
If the value words are stored in the dictionary, they can be subject to spelling correction because the spelling corrector uses the dictionary to locate words which are a close match to unrecognized words in a question.
This means, though, that all possible values and variant legitimate spellings of values for a concept must be put either into the dictionary or into the synonym list.
This is reasonable for concepts which have a small and controlled set of _values* such as the names of the * A set of v a l u e s is called a "domain,r.
The primary issue in this area is concerned with problems of defining, creating, and bringing up the necessary data for a new application.
The discussion points out the difficulties associated with systematic knowledge acquisition.
I. User Model A single database may be used by different groups of users for different purposes.
For example, some users of the ficompanies in METRO, but may u n w i e l d y for large sets of values.
become If a value can be recognized by a pattern, it is not n e c e s s a r y to itemize all instances in the dictionary.
For example, a date may be entered as "yy/mm/dd" so that any input matching the pattern "nn/nn/nn" is recognized as a date.
This is the approach used for dates and for names of applicants in the A P P L I C A N T database, where names of people match the pattern "I.I.Lastname".
In another approach, OnLine English [CULL80] and Intellect [HARR78, EDP82] (two v a r i a t i o n s of ROBOT) used the database to recognize values.
This is a s a t i s f a c t o r y solution if the database is small or if the small number of d i f f e r e n t values is stored in an index accessible to the NLI, and if the values in the database are suitable for use in English questions.
Each of these solutions has disadvantages.
If values are stored in the d i c t i o n a r y there may be many different ways to spell each particular value.
For example, the company name for "System Development Corporation" may also be given as "S.D.C.", "S D C", or "System Development Cotp".
While each d i f f e r e n t spelling could be entered as a synonym for the "correct" spelling in the database, this would result in an enormous proliferation of the d i c t i o n a r y entries and problems with concurrency control between the updates directed to the data m a n a g e m e n t system and the updates to the dictionary.
A creative solution might he to define rules for synonym generation and apply them to database updates.
A somewhat different example is from the A P P L I C A N T application which has many open ended domains, such as names of applicants and previous employers.
In this case, the application designer may have to treat certain fields as "retrieve-only", meaning that the data can be asked ~or but not used as a selection criterion.
A database with a large number of retrieve-only fields may be a poor candidate for an NLI.
Patterns can be used only if they can be enforced, and probably few values really fit the patterns nicely.
Proper names ate a poor choice for patterns because of variations such as middle initial or title such as "Dr".
or "Jr.".
Also, spelling correction cannot be performed unless the value is stored in the dictionary.
Finally, the solution of using the database itself to recognize v a l u e s is u n s a t i s f a c t o r y to a general NLI for anything other than trivial databases, unless an inverted index of values is easily accessible.
There are the problems of spelling c o r r e c t i o n and synonyms for database values, the inefficiency involved in accessing the DBMS for every unrecognized word, and the d i f f i culty of knowing which fields in the d a t a b a s e to search.
3. Semantic Variation By Value Databases are generally designed with a m i n i m u m number of d i f f e r e n t record types.
When there are entities which are similar, but p o s s i b l y have a small number of a t t r i b u t e s which are not shared, the entities will be stored in the same record type with null values for the attributes that do not apply.
The user, in his questions, may view these similar entities as very d i f f e r e n t e nt i t i e s and talk about them d i f f e r e n t l y . We did not encounter the problem with METRO or AIREP.
For example, in METRO, the user asks the same type of questions about the c o m p a n y named "Colonial" as about the company named "Supreme".
In APPLICANT, however, each a p p l i c a n t has a set of "specialties" such as "computer programmer", "a c c o u n t i n g clerk", or "gardener".
These are all stored as values of the s p e c i a l t y field in the database.
Unfortunately, in this case different specialties evoke completely d i f f e r e n t concepts to the end user.
The user may ask q u e s t i o n s such as, "What p r o g r a m m e r s know COBOL?", "Who can program in COBOL?", and "How m a n y a p p l i c a n t s with a s p e c i a l t y in computer programming applied in 1982?".
Notice the new nouns and verbs that are introduced by this s p e c i a l t y name.
A value domain such as specialties should be handled with an ISA hierarchy.
Each d i f f e r e n t type of s p e c i a l t y such as gardener or programmer could have a different concept that is a subset of the concept "specialty".
Some questions could be asked about all s p e c i al t i e s and others could be directed only to certain subconcepts.
However, there is no [SA hierarchy in EUFID, and it would have been inefficient to treat each specialty and subspecialty as a separate concept since there are 30 specialties and 196 subspecialties.
Therefore, we required the users to know the exact values, to know which values are for s p e c i a l t i e s and which are for subspecialties, and to ask q u e s t i o n s using the values only as nouns.
This is not "user friendly".
Even if it were possible to build a different concept for each different skill, there is an update problem.
When a new value is a d d e d to a v a l u e domain where there ace uniform semantics (as in adding a new company name in METRO), the new value is simply attached to the existing concept, when the new value has different semantics, t h e newly associated concepts, nouns, and verbs cannot be added automatically.
If t h e NLI supports an ISA hierarchy, someone w i l l need to categorize t h e new value and add a new node to the hierarchy or specify a position in the hierarchy.
4. Automation of D e f i n i t i o n subset who l i v e in Nevada.
One s o l u t i o n is to provide commands that allow u s e r s to d e f i n e s u b s e t s of the database to which to address questions.
This removes the ambiguity and speeds up retrieval time on a large d a t a b a s e . However, it moves the NLI interaction toward that of a structured query language, and forces the user to be a w a r e of the level of subset b e i n g accessed.
It is also difficult to implement because a subset may involve projections and joins to build a new relation containing the subset.
The NLI must be able dynamically and temporarily to change the mapping tables t o map t o this new relation.
2. Intelll~ent Interaction A natural-language interface system will not be practical u n t i l a new a p p l i c a t i o n can b e installed easily.
"Easily" means that the end-user organization must be able to create and modify the driving tables for the application relatively quickly without the help of the NLI developer, and must b e able to use the NLI without restructuring the d a t a b a s e . Each EUFID application required "handcrafted" tables that were built by the development staff.
Each new application was done in less time than the previous one, but still required several staff-months to bring up.
Clearly, the goal of facilitating the building of the tables by end users was not met.
Computer-assisted tools for defining new applications are a prerequisite for practical NLIs.
B. LANGUAGE PROBLEMS One of the EUFID design goals was to r e s p o n d promptly either with an answer or with a message that the question could not be interpreted.
The system handles spelling or typographical errors by interacting with the user t o select the correct word.
However, when all of the words are recognized but do n o t connect semantically, It is difficult to identify a single point in analysis which caused the failure.
It is i n this a r e a that the absence of a syntactic mechanism for determining well-formedness was most noticeable.
There are times when a question has a proper syntactic structure, but co n t a i n s semantic relationships u n r e c o g n i z a b l e to the application as in "What is the locatlon of North Hills?".
A response of "Location is not defined f o r North Hills in this appllcacion" should be derivable from the recognizable semantic failure.
Similarly, it would be useful to have a framework for interpreting partial trees, as in the question "What companies does Mohawk ship to"? where Mohawk is not a recognized word within the application.
An appropriate response might be "Companies ship to receiving offices and companies; Mohawk is neither a receiving office nor a company.
The names of offices and companies are ...".
Interpretation of partial a n a l y s e s is not possible within the EUFID system; it either succeeds or fails completely.
3. Yes/No Questions The basic approach to language analysis in EUFID involves a bottom up parser using a semantic grammar.
The symbols of the grammar are concepts underlying lexical items, and the rules of the grammar ace based o n a case framework.
Essentially syntactic information is used only when needed to resolve ambiguity.
The language features that this technique has t o handle are common to any NLI, and some of the problem areas are described in the following sections.
I. Anaphora and Ellipsis To support natural interaction it is desirable to allow the use of anaphoric reference and elliptical constructions across sentence sequences, such as "What applicants know Fortran and C?", "Which of them live in California?", "In Nevada?", "How many know Pascal?'.
One of the biggest problems is to define the scope of the reference in such cases.
In the example, it is not clear whether the user wishes to retrieve the set of all applicants who know Pascal or only the II In normal NLI interaction users may wish to ask "yes/no" questions, yet no DBMS has the ability to answer "yes" or "no" explicitly.
The EUFID mapper maps a yes/no question into a query which will retrieve some data, such as an " o u t p u t identifier" or default name for a concept, if the answer is "yes" and no data if the answer if "no".
However, the answer may be "no" for several reasons.
For example, a "no" response to the question "Has John Smith been interviewed"? may mean that the database has knowledge about John Smith and about interviews and Smith is not listed as having had an interview*, or the database knows about John Smith and no data about interviews is available.
A third p o s s i b i l i t y could be that the database has information about John Smith and his employment situation (already hired), and the response might include that information, as in "No, but he has already been hired'.
4. Conjunctions uncertain whether they should be returned in the answer.
It is also d i f f i c u l t to take a c o m p l e m e n t of a set of data using the m a n y data m a n a g e m e n t systems that do not support set o p e r a t o r s between relations.
Questions which require a "yes" or "no" response are difficult to answer because often the "no" is due to a p r e s u p p o s i t i o n which is invalid.
This is e s p e c i a l l y true with negation.
For example, if the user asks, "Does e v e r y company in North Hills except Supreme use NH2?", the answer may be "no" because Supreme is not in North Hills.
The current i m p l e m e n t a t i o n of EUFID does not allow explicit negation, a l t h o u g h some n e g a t i v e concepts are handled such as "What c o m p a n i e s ship to companies other than Colonial?".
"Other than" is interpreted as the "!-" o p e r a t o r in e x a c t l y the same way that "greater than" is interpreted as ">".
C. INTERPRETATION AND DATABASE ISSUES T h e s c o p e of c o n j u n c t i o n s is a difficult problem for any parsing or analyzing algorithm.
The n a t u r a l l a n g u a g e use of "and" and "or" does not n e c e s s a r i l y correspond to the logical meaning, as in the question "List the applicants who live in C a l i f o r n i a a n d Arizona.".
Multiple c o n j u n c t i o n s in a single q u e s t i o n can be ambiguous as in "which minority and female applicants know Fortran and Cobol?'.
This could be interpreted with logical "and" or with logical "or" as in "Which a p p l i c a n t s who are minority or female know either Fortran or Cobol?".
The EUFID mapper will change English "and" to logical "or" when the two phrases within the scope of the conjunction are values for the same field.
In the example above, an applicant has only one state of residence.
Many q u e s t i o n s make perfect sense semantically but are difficult to map into DBMS q u e r i e s because of the d a t a b a s e structure.
The problems become worse when access is through an NLI because of increased e x p e c t a t i o n s on the part of the user and because it may be d i f f i c u l t for a help system a d e q u a t e l y to d e s c r i b e the problem to the user who is unaware of the database structure.
I. IL Limitations Nepption Negative requests may contain explicit negative words such as "not" and "never" or may contain implicit negatives such as "only", "except" and "other than" [OLNE78].
The interpretation of negatives can be very difficult.
For example, "Which c o m p a n i e s did not ship any perishable freight in 1976" could mean either "Which (of all the companies) shipped no perishable freight in 1976"? or "Which (of the companies that ship perishable freight) shipped none in 1976?'.
Moreover, if some companies were only receivers and never shippers it is "-"~e is the important d i s t i n c t i o n between a "closed world" database in which the assumption is that the database covers the whole world (of the application) and an "open world" database in which it is understood that the database does not represent all there is to the real world of the application.
In the open world database, which we encounter most of the time, a response of "not that this database knows of" might be more appropriate.
~Z The design of the IL is critical.
It must be rich enough to support retrieval from all the underlying DBMSs.
However, if it c o n t a i n s c a p a b i l i t i e s that do not exist in a specific DBMS, it is difficult to d e s c r i b e this d e f i c i e n c y to the user.
In APPLICANT, the user cannot get both the major and minor fields of study by asking "List applicants and field of study", because a limitation in the EUFID IL prevents making two joins between education and subject records.
This problem was corrected in a subsequent version of IL with the addition of a "range" statement similar to that used by QUEL [STON76].
The current IL does not contain an "EXISTS" or "FAILS" operator which can test for the existence of a record.
Such an operator is frequently used to test an interrecord link in a network or hierarchical DBMS.
It is needed to express "What problems are unsolved"? to the AIREP application, which requires a test for a database link between a set and a solution Mixed Case Values set.
generate the IL q u e r y EUFID allows a value in the database to be upper or lower case and will c o n v e r t a value in the question either to all upper or all lower case in the IL, or leave it as input b y the user.
If the d a t a b a s e values are mixed case, it is not possible to convert the user's input to a single case.
If the user does not enter each letter in t h e p r o p e r c a s e, t h e v a l u e will n o t match.
3. Granularit~ Differences retrieve [cct.scname] where (cct.date  198~) and (cct.lf >{retrieve [avg (cct.lf)] where (cct.date 1980)}) Here, =cct" i s t h e name o f the companyto-company transaction relation.
" S c n a m e " is the name of a shipping company in this relation.
Note again that the qualification on " 1 9 8 ~ " n e e d s to be done both inside and o u t s i d e the nested p a r t o f t h e query.
In the query language for INGRES such a request is expressed in a manner very similtar t o t h e IL e x p r e s s i o n s . For WWDMS a very complex procedure is generated.
In all cases, t h e DBMS n e e d s to answer the inner request and s a v e t h e result for usa in qualifying the outer request.
There are many database management systems that cannot handle such questions and t h e s e I L s t a t e m e n t s cannot be translated into the system's query language.
5. Inconsistency In Retrieval The NLI user is n o t expected to understand exactly how d a t a is stored, and yet must understand something about the g r a n u l a r i t y of the data.
Time fields often cause problems because time m a y be given by year or by fractions of a second.
U s e r s may make t i m e comparisons that require more granularity than is stored in t h e database.
For example, t h e user can ask "What incidents were reported at SAC while system release 3.4 was installed?".
If incidents were reported by day but system release dates were given by month, the system would return i n c i d e n t s which occurred in the days of the month before the system release was i n s t a l l e d . 4.
Nested Queries A very simple question in English can turn into a very complicated request in t h e query language if it involves retrieval of data which must b e used f o r qualification in another part of the same query.
In IL these are called "nested queries".
Most o f t e n some qualification needs to be done b o t h "inside" and "outside" t h e clause of the query that does the internal retrieve.
For example, t h e question "What i n c i d e n t at SAC had the longest d o w n t i m e ? " f r o m o u r AIREP a p p l i cation i s e x p r e s s e d i n I L as retrieve [INCA.
ID] where (INCA.SITENAME = "SAC") and (INCA.DNTM [retrieve [ max (INCA.DNTM)] where (INCA.SITENAME = "SAC")}) The nested part of t h e query is enclosed in braces.
"INCA" is the database name of the active incident records.
Notice that removing the "INCA.SITENAME = 'SAC'" clause from either the inner or outer query would result in an incorrect formulation of the question.
A similar example from the METRO application is the question, "What company shipped more than the average amount of light freight in 198~"? which will 13 The NLI presents a uniform view of all d a t a b a s e s a n d DBMSs, but it is difficult to truly mask all differences in the behavior o f t h e DBMSS b e c a u s e t h e y d o n o t all process the equivalent query in the same way.
For example, when data are retrieved from two relations in a relational database, the two relations must be J o i n e d on a common attribute.
The answer forms a new relation which may be displayed to t h e user o r stored.
Since the join clause acts as qualification, a record (tuple) in either relation which has no corresponding t u p l e in t h e other relation does not participate in the result.
This is a different concept from the hierarchical and network models where the system retrieves all records from a master record and then retrieves corresponding records from a subfile.
This difference can cause anomalies with retrieval.
For example, in a pure relational system "List applicants and thei~ interviews" would be treated as "List applicants who have had interviews together with their interview information".
A h i e r a r c h i c a l or network DBMS would treat it as "List all applicants (whether or n o t they have been interviewed) plus any interview information that exists".
This second interpretation is more likely to be the correct one.
fiD. OVERALL NLI DESIGN There are several problems that affect the selection of a p p l i c a t i o n s for the NLI.
Some d a t a b a s e s and data m a n a g e ment systems may not be a p p r o p r i a t e targets for natural-language interfaces.
Some DBMS functions may be d i f f i c u l t to support.
It is important to have a clear understanding of these problems so that the NLI can mediate between the user view, as represented by the naturallanguage questions, and the underlying d a t a b a s e structure.
i. ~ Design C o n s i d e r a t i o n map q u e r i e s and t o explain problems to t h e u s e r when t h e m a p p i n g c a n n o t b e m a d e . However, there can be "reasonable" queries that cannot be answered d i r e c t l y because of the database structure.
Hierarchical DBMSs present the most problems with n a v i g a t i o n because access must start from the root.
For example, if the APPLICANT database were under an hierarchical DBMS, the q u e s t i o n "List t h e s p e c i a l t i e s for each applicant" could be answered directly but not "What are the specialties"? as there would be no way to get to the s p e c i a l t y records except via particular applicant records.
An array allows more than one instance of a field or set of fields in a single record.
There may be arrays of values or even arrays of sets of values in nonrelatlonal databases.
When the user retrieves a field that is an array the DBMS requires a subscript into the array.
Either the user must s p e c i f l y this s u b s c r i p t or the NLI must map to all members of the array with a test for missing data.
3. Class of DBMS to Supp%rt For any d a t a b a s e there are naturallanguage q u e s t i o n s that cannot be interpreted because the concepts involved lle outside the world of the database.
Questions can also involve structural complexity that is n o t r e p r e s e n t a b l e in the DBMS q u e r y language.
A p a r t i c u l a r l y difficult d e c i s i o n in the overall design of an NLI is the issue of where in the chain of events of processing a user's question into a DBMS q u e r y to trap these q u e s t i o n s and stop processing.
One approach is to decide that if a question is not meaningful to the world of the d a t a b a s e it should not be m e a n i n g ful to the NLI and, therefore, not analyzable on semantic grounds.
Another assumes that if the NLI can analyze a question that cannot be asked of the database, it has a much better chance of d e s c r i b i n g to the user what is wrong with the question and how it might be rephrased to get the desired information.
Codd made good use of the dialogue procedures of the RENDEZVOUS [CODD74] system to avoid questions that the DBMS could not handle, as well as avoiding g e n e r a t i o n of DBMS queries that did not represent the user's intent.
Such a system, however, requires a very large semantic base (much larger than that of the database) in order to make meaningful communication with the user during the dialogue.
2. Class of Database to Support For systems such as EUFID, the database must be organized within a data m a n a g d m e n t system so that the data is structured and individual fields are named.
If the data is just text, the EUFID approach cannot be used.
Current NLI systems are de s i g n e d to be used interactively by a user, which means that the DBMS should also have an interactive query language.
However, noc all data m a n a g e m e n t systems are interactive.
WWDMS [HONE76] has a user query language, b u t queries are entered into a batch job queue and answers may not return for many minutes.
If an Nil front end is to be added to such a DBMS, i~ must have the capability to generate query programs without any access to the database for parsing or for processing the returned answer.
The query language should support operations equivalent to the relational o p e r a t i o n s of select, project, and join.
Also, the query language should support some arithmetic capability.
Most have aggregate functions such as SUM and COUNT.
WWDMS does not have an easy-touse average operation, but it does have a procedural language with arithmetic operators so that EUFID can produce a "query" that p r o c e d u r a l l y calculates an average.
Basic c a l c u l a t i o n s should be supported such as " a g e = t o d a y b i r t h d a t e " . It is also d e s i r a b l e to be able to call special functions to do complex c a l c u l a t i o n s Some databases are simply not good candidates for an NLI because of characteristics mentioned in previous sections such as many retrieve-only fields, or domains that have a high update rate but cannot be recognized by a pattern.
There are also some structural problems chat must be recognized.
If the database contains "flat" files about one basic entity, it is reasonably easy to fisuch as required in navigational calculations a naval database.
the input standardize must be values, controlled to Support for Metadata Metadata is data about the data in the database.
It would be able to tell the user of the METRO application, for example, the kind of information the database has for warehouses and other entities in the application.
Such metadata might be extensions of active integrated data dictionaries now available i n some DBMSs.
I n an a p p l i c a t i o n l e v e l system the user should be able to query the metadata to learn about the structure of the database.
A different mode, such as the menus used by the EUFID help system, could be used to access metadata, or English language questions to both meta information and the database could be supported.
there should be few fields than have values that change rapidly, cannot be recognized by a pattern, and that must be used in qualification, the users of the NLI should have a common use for the data and a common vlew of the data, and there must be some user who understands the questions that will be asked and is available to work with the d e v e l o p e r s of the NLI.
Updates Some potential users would like a n a t u r a l l a n g u a g e interface to include the capability to update the database.
Currently, updating through any high level view of the database should be avoided, especially when the view contains joins or derlve4 data, because of the risk of inadvertently entering incorrectly-interpreted data.
SUMMARY AND CONCLUSIONS We believe that current system development is limited by the need for good semantic modelling techniques and the length of time needed to build the knowledge base required to interface with a new application.
When the knowledge base for the NLI is developed, the database as well as sample input must be considered in the design.
Parsing of questions to a database cannot be divorced from the database contents since semantic interpretation can only be determined in the context of that database.
On the other hand, a robust system cannot be developed by considering only database structure and content, because the range of the questions allowed would not accurately reflect the user view of the application and also would not account for all the information that is inferred at some level.
For many years, researchers have been attempting to build robust systems for natural-language access to databases.
It is not clear that such a system exists for general use [0SI79].
There are problems that need to be solved on both the front end, the parsing of the English question, and the back end, the translation of the question into a data management system query.
It is important to understand the types of requests, types of functions, and types of databases that can be supported by a specific NLI.
Some general guidelines that can be applied to the selection of applications for current NLI front ends are suggested below: lo the underlying DBMS should interactive query language, have an the DMS view should be relational or at least support multiple access paths, the database arrays either tures, should not contain of values or of strucACKNOWLEDGEMENTS We would like to acknowledge the many people who have contributed to EUFID development: David Brill, Marilyn Crilley, Dolores Dawson, LeRoy Gates, Iris Kameny, Philip Klahr, Antonio Leal, Charlotte Linde, Eric Lund, Fillp Machi, Kenneth Miller, Eileen Lepoff, Beatrice Oshika, Roberta Peeler, Douglas Pintar, Arie Shoshani, Martin Vago, and Jim Weiner.
REFERENCES [AHO72] Aho, A.
V. and J.
D. Ullman.
"The Theory of Parsing, Translation, and Compiling", Vol.
I: Parsing, Prentice-Hall, 1972, pp.
314-23Z. [BURGBff] Burger, J.
F . "Semantic Database Mapping in EUFID", Proceedings of the 198Z ACM/SIGMOD Conference, ~3"n~-a'-o~caY-'-Ca~., May 14-16, 198ff.
[BURG82] Burger, J.
F. and Marjorie Templeton.
"Recommendations for an Internal Input Language Eor the Knowledge-Based System', System Development Corporation internal paper N-(L)-24890/021/00, January 5, 1982.
[CODD74] Codd, E.
F., "Seven Steps to Rendezvous with the Casual User', Proc.
IFIP TC-2 Working Conference on Data-'5"~e'-~a~a~emen~ ~ystems, Car ~ gese, Corsica, April 1-5, 1974, in J.
W. Kimbie and K.
I. Koffeman (Eds.), "Data Base Management" North-Holland, 1974.
[CULL80] Cullinane Corporation, "IQS Summary Description", May 1980.
[DATE77] Date, C.
J., "An Introduction to Database Systems', second edition, Addison-Wesley Publishing, Menlo Park, CA, 1977.
[EDP82] "Query Systems for End Users", EDP Analyzer, Vol.
20, No.
9, September, 1982.
[HARR78] Harris, L.
R., "The ROBOT System: Natural Language Processing Applied to Data Base Query', Proceedings ACM 78 Annual Conference, 1978.
[HEND77] Hendrix, G.
G., E.
D. Sacerdoti, D.
Sagalowicz, and J.
Slocum, "Developing a Natural Language Interface to Complex Data" SRI Report 78-305, August 1977.
[HONE76] Honeywell, WWMCCS: World Wide Data Management System User's Guide, Honeywell DE97 Ray.3, April 1976.
[KELL71] Kellogg, C.
H., J.
F. Burger, T.
billet, and K.
Fogt, "The CONVERSE Natural Language Data management System: Current Status and Plans", Proceedings of the ACM SZmposium on :ntormation ~ ~ a n d ~etrleval-~, University o Maryland, College Park, MD, 1971, pp.
33-46. [MYLO76] Mylopoulos, J., A.
8 o r g i d a, P.
Cohen, N.
Roussopoulos, J.
Tsotsos, and H.
Wong, "TORUS: A Step Towards Bridging the Gap between Data Bases and the Casual User", in Information Volume 2 1976, Pergamon Press, pp 49-64.
[OLNE78] Olney, John, "Enabling EUFID to Handle Negative Expressions", SDC SP-3996, August 1978.
[OS179] Operating Systems, Inc., "An Assessment of Natural Language Interfaces for Command and Control Database Query", Logicon/OSI Division report for WWMCCS System 16 [SCHA77] Scha, R.
J. H., "Phillips Question-Answering System PHLIQAI", in SIGART Newsletter Number 61, February 1977, Association for Computing machinery, New York.
[SIMM65] Simmons, R.
F., "Answering English Questions by Computer -a Survey', Comm.
ACM 8,1, January 1965, 53-70.
[STON76] Stonebraker, M., et.
al., "The Design and Implementation of INGRES', Electronics Research Laboratory, College of Engineering, University of California at Berkeley, Memorandum No.
ERL-M577, 27 January 1976.
[TEMP79] Templeton, M.
P., "EUFID: A Friendly and Flexible Frontend for Data Management Systems", Proceedings of the 1979 National Conference Association of Computational Linguistics, August, 1979.
[TEMP80] Templeton, M.
P., "A Natural Language User Interface", Proceedings of "Pathwazs ~o System rn~ri%7", washington DYC.
C a h ~ o ACM, 1980.
[THOM69], Thompson, F.
B., P.
C. Lockemann, B.
H. Dostert, and R.
Deverill, "REL: A Rapidly Extensible Language System", in Proceedings of the 24th ACM National Conference, s~ociation--"~or Computing machinery, New York, 1969, pp 399-417.
[WALT77] Waltz, D.
L., " N a t u r a l Language Interfaces", in SIGART Newsletter Number 61, F e b r u a r y ' ~ 7, Association for Computing machinery, New York.
[WALT78] Waltz, D.
L., "An English language Question Answering System for a Large Relational Database", Communications of the ACM 21, 7(July 1978), pp 526-539.
[WOOD72] Woods, W.
A., R.
M. Kaplan, B.
Nash-Webber, The Lunar Sciences N a t u r a l L a n @ u a ~ e " r n f o r m a t i o n ' System ~'~Report, Report number~, Bolt, Beranek, and Newman, Inc., Cambridge, MA, 15 June 1972 .
INTRODUCING ASK, A SIMPLE KNOWLEDGEABLE SYSTEM Bozenn H.
Thompson F r e d e r i c k B.
Thompson California Inatitnce of Technology Pasadena, California 91125 ABSTRACT ASK, ~ ~ i m p l e K n o w l e d g e a b l e S y s t e m, i s a t o t a l system for the structuring, manipulation and communication of information.
It is a simple system in t h e sense thaC its development concentrated on c l e a n e n g i n e e r i n g solutions to w h a t c o u l d be d o n e now w i t h g o o d r e s p o n s e t i m e s. The user interface is a limited dialect of English.
In contrast to expert systems, in which experts build the knowledge base and users make u s e o f t h i s e x p e r t k n o w l e d g e, ASK i s a i m e d a t t h e u s e r who w i s h e s t o c r e a t e, test, modify, extend a n d m a k e u s e o f h i s own k n o w l e d g e b a s e . It is a s y s t e m for a research team, a m a n a g e m e n t or military staff, or a business office.
some h a v e t h e f o l l o w i n g n u m b e r a t t r i b u t e s : speed length beam >List the destinations a n d home p o r t o f each ship, ship destination home p o r t Ubu New York Naples Tokyo --Morn 0slo Tokyo Kittyhawk Naples Boston Boston -London This paper is designed to give you a feel for the general performance of t h e ASK S y s t e m a n d overview of its operational capabilities.
To Chin end, the movie you see will continue throughout the talk.
Indeed, the talk itself is a commentary on t h i s b a c k g r o u n d m o v i e . The m o v i e i s bona f i d e and in real time, i t i s o f t h e ASK S y s t e m i n action.
(Many o f t h e i l l u s t r a t i o n s from the movie are reproduced in the written paper.) I.
ASK AS A DATABASE SYSTEM A.
Examples o f ASK English To i n t r o d u c e a few examples you to ASK, we w i l l s t a r t o u t w i t h of queries of a simple data base The uninitiated user may wish London London N e w York --North Scar London New York gimitz London Norfolk Saratoga unknown Norfolk >What c i t i e s a r e t h e home p o r t s o f s h i p s whose d e s t i n a t i o n i s London?
Boston London New York Norfolk >Are t h e r e s h i p s t h a t do n o t h a v e a c a r g o ? yes >What i s t h e number o f New York s h i p s ? There are 2 answers: ( 1 ) New York ( d e s t i n a t i o n ) ships 2 ( 2 ) New York (home p o r t ) s h i p s 1 >How many s h i p s a r e t h e r e w i t h l n e g t h g r e a t e r t h a n 600 f e e t ? Spelling correction: " l n e g t h " to " l e n g t h " 4 >What ships t h a t carry wheat go to London or Alamo Oslo? ships that carry wheat London Maru Oslo Alamo >Does the Maru carry wheat and go co London? yes concerning ships.
simply to ask: >How many ships are there? 7 >What is known about ships? some are in the following classes: Navy freighter old S.
The ASK Data Structures A l t h o u g h in the t e r m i n o l o g y of data base theory, ASK can be considered as an "entityrelation" system, ASK retains its information in records w h i c h are interlinked in a s e m a n t i c net.
One reason we refer to ALE as simple is because ic uses only a few kinds of nodes in its s e m a n t i c tanker a l l have t h e f o l l o w i n g a t t r i b u t e s : destination home p o r t some have t h e f o l l o w i n g a t t r i b u t e s : cargo a l l have t h e f o l l o w i n g number a t t r i b u t e s : age 17 net, namely: fio Attributes Relations and the ctbvious c o r r e s p o n d i n g arcs.
We speak of this as the COAR structure.
A~tributes are single valued, e.g., "father", "home port", " t i t l e " ; relations may be m u l t i p l e valued, e.g., "child"~ "cargo", "author".
The d i f f e r e n c e between attributes and relations can be seen in the following p r o t o c o l . >What is the cargo and home port of the Maru? cargo home port wheat London >The home port of Maru is Boston.
London has been replaced by Boston as the home port of Maru.
>The cargo of Maru is coal.
coal has been added as the cargo of Maru.
>What i s the cargo and home port of the Maru? cargo home port wheat BosCon coal -->definition:long:paper whose number of pages e x c e e d s 49 Defined.
>definition:long:book whose number o f p a g e s e x c e e d s 800 Defined.
>What AI bibliography i t e m s a r e long?
There are 2 answers: (1) long:paper whose number of pages exceeds 49 Physical Symbol Systems A General Syntactic P r o c e s s o r (2) long:book whose number of pages exceeds 800 Human Problem Solving >What long books were written in 19727 long:book whose number of pages exceeds 800 Human Problem Solving Family relationships make for a g o o d illustration of definitions; we switch to a small family relationship context.
>What are attributes? individual/individual attributes : spouse >What are relations? individua I/individual relations : parent >What are classes? individual classes : male female >What are definitions? definition:mother :female parent definition: father :male parent definition:child:converse of parent definition:sibling:child of parent bur not oneself definition'cousin:child of sibling of parent >List the father and mother of each of Billy Smith's cousins.
Billy Smith's cousins father mother Baby Boyd R o b e r t Boyd J i l l Boy C.
Extendin K and Hodifyin~ I.
Definitions t h e Dat~ To make such a system more knowledgeable, one needs to be able co add d e f i n i t i o n s that e m b o d y interrelationships a m o n g the basic classes, objects, a t t r i b u t e s and relations of the data.
The simplest form of definition is synonym: >definition:tub:old Defined.
ship Although this form of definition allows one to introduce abbreviations and many forms of jargon, more extensive forms of definition are desirable.
Here are three illustrations using the same "ship" file as above.
In the third definition, note the use of quotes to create local '~ariables".
>definition:area:length * beam Defined >List the length, beam and area of each tub.
tub length beam area foot foot foot**2 Ubu 231.667 48 11120.016 Alamo 564.5 84 47418.
>definition:meter:39.37 * (foot / 12) Defined.
>beam of the Alamo squared in square meters? 655.526472343 square meters >definition:longest "ship":"ship" whose length is the maximum length of "ship"s Defined.
>What is the length in meters of the longest ship whose home port is Naples? 121.920243840 meters T h e n o t i o n of w h a t is l o n g m a y be q u i t e different in another context, say in the context of b i b l i o g r a p h y of a r t i f i c i a l intelligence literature.
18 2.
Verbs Most verbs e m b o d y k n o w l e d g e specific to the application in which they are used, the exceptions being the copula verbs.
Therefore the only verbs initially known to the ASK System are "to be" and "to have".
The user c a n add n e w v e r b s by paraphrase.
>verb:ships "go" to New York:destination of ships is New York Defined.
>verb:ships "carry" coal from London to Boston:ships have coal as cargo, have L o n d o n as home port and go to Boston Defined.
>Each old ship carries what cargo to each port? old ship port cargo Ubu New York oil Tokyo oil Alamo London wheat coal fi>What i s c a r r i e d by t h e Alamo? wheat coal >Wheat i s c a r r i e d to London from what p o r t s ? New York >What c i t i e s does t h e Alamo c a r r y wheat to?
London Pronouns and Ellinses >Create t h e a t t r i b u t e : r a t i n g The a t t r i b u t e r a t i n g h a s been a d d e d . >Create i n d i v i d u a l s : s e m i n a l, e x c e l l e n t, f a i r and i m p o s s i b l e The f o l l o w i n g i n d i v i d u a l s have been added: seminal excellent fair impossible >The r a t i n g o f W i n o g r a d ' s 1980 p a p e r i n Cognitive Science is excellent.
e x c e l l e n t h a s been added a s t h e r a t i n g o f W i n o g r a d ' 8 1980 p a p e r in C o g n i t i v e S c i e n c e >Rating o f A Framework f o r R e p r e s e n t i n g In p r a c t i c a l s y s t e m s f o r e x p e r t s, a b b r e v i a t e d f o r m s of a d d r e s s i n g t h e c o m p u t e r a r e common.
Thus the ability to h a n d l e p r o n o m i n a l and e l l i p t i c a l constructions are of considerable importance.
A l t h o u g h t h e r e has been p r o g r e s s i n t h e l a s t few years in the linguistic understanding of these c o n s t r u c t i o n s, many d i f f i c u l t i e s r e m a i n . However, b u i l d i n g on t h e work t h a t ham been a c c o m p l i s h e d, many o f t h e s e c o n s t r u c t i o n s can be h a n d l e d by t h e ASK System.
In o r d e r to a v o i d m i s l e a d i n g t h e u s e r when t h e c o m p u t a t i o n a l a l g o r i t h m does not make t h e c o r r e c t i n t e r p r e t a t i o n, echo i s u s e d t o i n f o r m t h e u s e r of t h e i n t e r p r e t a t i o n t h a t h a s been t a k e n . >Is t h e r e a s h i p whose d e s t i n a t i o n i s unknown? yes >What is it?
What is it [ship whose destination is ..knov.] ? Saratoga >Is its cargo wheat or c o a l ? I s i t s [ s h i p whose d e s t i n a t i o n i s unknown] cargo wheat or c o a l ? wheat no coal yes >Does t h e A l a m o ' s l e n g t h exceed t h a t of t h e Ubu? yes >What about t h e N o r t h e r n S t a r ? [the Northern Star?] does t h e A l a m o ' s l e n g t h exceed t h a t of t h e Northern Star? no >Is i t s cargo wheat?
Is its [Northern Star] cargo wheat? no >Is it that of the Ubu?
Is i t [Norther, Star cargo] that [cargo] of the Obu? yes >Wheat is carried by how many ships? 2 >Coal and oil? c o a l and o i l i s c a r r i e d by how many ship*? coal I oil 2 >by the Ubu? c o a l and o i l i s c a r r i e d by t h e Ubu? coal no oil yes Knowledge i s i m p o s s i b l e . i m p o s s i b l e h a s been added as t h e r a t i n g o f A remework f o r R e p r e s e n t i n g Knowledge.
>Scripts, P l a n s, Goals and U n d e r s t a n d i n g ' s rating is se~.nal.
s e m i n a l h a s been added a s t h e r a t i n g o f S c r i p t s, P l a n s, Goals and U n d e r s t a n d i n g . >The r a t i n g o f M i n s k y ' s paper i n The P s y c h o l o g y of Computer V i s i o n is seminal.
i m p o s s i b l e has been r e p l a c e d by s e m i n a l as t h e r a t i n g o f M i n s k y ' s p a p e r i n The P s y c h o l o g y o f Computer V i s i o n . > L i s t t h e a u t h o r of e a c h paper t h a t h a s a r a t i n g o f e i t h e r s e m i n a l or e x c e l l e n t . paper S c r i p t s, Plans, Goals and Understanding R.
C. Schauk A Framework f o r R e p r e s e n t i n g Knowledge M.
Minsky What Does I t Mean to U n d e r s t a n d N a t u r a l Language To Winograd > r e l a t i o n : keyword The r e l a t i o n keyword h a s been added.
>individuals:context, natural language, knowledge r e p r e s e n t a t i o n The f o l l o w i n g i n d i v i d u a l s have been added: c o n t e x t n a t u r a l l a n g u a g e knowledge representation >Context and knowledge r e p r e s e n t a t i o n a r e k e ~ o r d a of S c r i p t s, P l a n s, Goals and U n d e r s t a n d i n g and A Framework f o r R e p r e s e n t i n g Knowledge.
c o n t e x t knowledge r e p r e s e n t a t i o n have been added as keywords o f S c r i p t s, P l a n s, Goals and U n d e r s t a n d i n g . c o n t e x t knowledge r e p r e s e n t a t i o n have been added as keywords o f A Framework f o r R e p r e s e n t i n g Knowledge.
> N a t u r a l l a n g u a g e i s a keyword of S c r i p t s, P l a n s, Goals and U n d e r s t a n d i n g . n a t u r a l l a n g u a g e h a s been added a s keyword of Scripts, Plans, Goals and Understanding.
>definition:about "booka'*:converse of keyword of "books" Defined.
>What AI Sibliosraphy items are about knowledge r e p r e s e n t a t i o n ? S c r i p t s, P l a n s, Goals and U n d e r s t a n d i n g A Framework f o r R e p r e s e n t i n g Knowledge D.
Inheritanc~ 4.
E x t e n d i n 2 th_...eeVocabular 7 an...~dDa~a Bas~ New o b j e c t s, c l a s s e s, a t t r i b u t e s and r e l a t i o n s may be a d d e d a t w i l l . T h i s w i l l be i l l u s t r a t e d using the AI Bibliography.
19 Classes, as well as objects, may be t h e a r g u m e n t s or v a l u e s o f a t t r i b u t e s and r e l a t i o n s . T h i s r a i s e s t h e q u e s t i o n of i n h e r i t a n c e of properties and attributes.
The a n s w e r is to be fifound in the following protocol, using attribute of color; inheritance for relations h a n d l e d i n s i m i l a r ways.
>class: r a c i n g sloop, sports b o a t, Norwegian The following classes have been added: racing sloop sports boat Norwegian >attribute: color The a t t r i b u t e c o l o r h a s been a d d e d . >individuals: white, blue, b l a c k, g r a y The following i n d i v i d u a l s have b e e n added: white blue black gray >Saratoga is a racing sloop.
S a r a t o g a has b e e n added to r a c i n g s l o o p . >Racing s l o o p s a r e s p o r t s b o a t s . r a c i n g s l o o p s have b e e n added to s p o r t s boats.
>Alamo is industrial and Norwegian.
Alamo has b e e n added to i n d u s t r i a l Norwegian.
>The c o l o r o f s p o r t s b o a t s i s w h i t e . w h i t e h a s b e e n added a s c o l o r o f s p o r t s boats  >Maru's color is gray.
g r a y has b e e n added a s c o l o r o f Maru.
>Black i s t h e c o l o r of i n d u s t r i a l . b l a c k has b e e n added as t h e c o l o r o f industrial.
>Blue is the color of Norwegian.
b l u e h a s b e e n added a s t h e color of Norwegian.
>What i s the color of each s h i p ? There are 3 answers: (t) ship color Maru gray Saratoga white (2) Alamo as industrial ship color Alamo black (3) Alamo as Norwegian ship color Alamo blue the is There are 2 answers: ( i ) T h e r e a r e no p o r t s ( 2 ) T h e r e a r e no U.S. d e s t i n a t i o n s o f Maru.
> I s some E u r o p e a n p o r t a p o r t o f Maru?
The f o l l o w i n g word i s n o t i n t h e v o c a b u l a r y : portof Correction: Is some European port a port of Maru?
There is no port.
>London is Alamo's port.
London h a s b e e n added a s t h e p o r t o f Alamo.
> I s som E u r o p e a n p o r t a p r o t o f Maru?
Spelling corrections: "son" to "some" "prot" tO " p o r t " T h e r e i s no p o r t o f Maru.
>New York i s M a r u ' s p o r t . New York h a s b e e n added a s p o r t o f Maru.
> I s some E u r o p e a n p o r t a p o r t o f Maru.
I s some E u r o p e a n p o r t a p o r t o f Maru? no II.
INTEGRATION OF MULTIPLE OBJECT TYPES A.
Extension of COAR ~o Multiple Object Types So far we have i l l u s t r a t e d ASK using only two types of objects: capabilities individuals, e.g., "John Jones", "Maru" numbers, e.g., "34.6 feet", "length of Maru", "number of ships".
ASK has been designed, however, to facilitate many kinds of objects.
This is a capability orthogonal to the simple COAR s t r u c t u r e in that for any types of objects there m a y be c o r r e s p o n d i n g classes, attributes and relations.
B. An Examvle: E.
DiaKnostics Our objectives in the design respond to the experienced user: o o of ASK are to with the responses s/he desires in at l e a s t 90% o f t h e c a s e s ; w i t h a r e s p o n s e that includes what s/he desires, but perhaps additional information also, in 90% of t h e remaining cases; with a response, although oot the desired one, but one which clearly indicates the system's interpretation of the request in the remaining cases.
We will illustrate this m u l t i p l e object type capability with the additional object type: text.
Once this new object type was added (together with p r o c e d u r e s t o m a n i p u l a t e texts, i.e., a "word processor") then text classes, individual/text and text/individual a t t r i b u t e s and r e l a t i o n s w e r e immediately available.
It was a small task to add an e l e c t r o n i c mail s y s t e m to ASK; all that w a s required was an a d d i t i o n to the a u t h o r i z a t i o n procedure that assigned to each newly authorized person a new text class as his/her mail box.
>Create the t e x t / t e x t relation:comment The t e x t / t e x t r e l a t i o n co-~.ent has been added.
>Create a file: budget correspondence The text class budget correspondence has been added.
>File the Budget Proposal in budget correspondence.
Budget Proposal has been added to the text class budget correspondence.
>Who have I received mail from?
J o h n Smith Peter Jackson 20 This third objective calls for good diagnostics.
The diagnostic capabilities of ASK are illustrated in the following protocol.
>attribute:port The a t t r i b u t e p o r t has been added.
>What a r e t h e p o r t s and U.S. d e s t i n a t i o n s t h e Maru? of fi> D i s p l a y my m a i l f r o m J o h n S m i t h . F r e d, Your B u d g e t P r o p o s a l l o o k s good t o me.
S u g g e s t you s u b m i t i t a s i s . John > F i l e i t i n my b u d g e t c o r r e s p o n d e n c e f i l e . F i l e i t [my m a i l f r o m J o h n S m i t h ] i n my budget correspondence file.
my m a i l f r o m J o h n S m i t h h a s b e e n a d d e d co the text class budget correspondence.
>amm-va i t f r o m my m a i l b o x . Bemove i c [my m a i l f r o m J o h n S m i t h ] f r o m my m a i l box.
my m a i l f r o m J o h n S m i t h h a s b e e n r e m o v e d from your mail.
> C r e a t e a b u d g e t c o r r e s p o n d e n c e named Budget Plans Please e n t e r t e x t : S t a f f l e v e l b u d g e t m e e t i n g on Wed.
a t 3 i n Tom's o f f i c e . P l e a s e s e n d me y o u r c o m m e n t s b e f o r e t h e m e e t i n g ; f i l e t h e m a s "commenCe on B u d g e t P l a n e " . \ Budget Plane class budget >Mail Budget Budget plane manager.
h a s b e e n a d d e d Co t h e t e x t correspondence.
Plans to each section manager.
h a s b e e n s e n t to e a c h s e c t i o n Ill.
MORE GENERAL ASPECTS OF THE ASK SYSTEM A.
R e s v o n s e Times The movie, which accompanied the oral presentation of this paper, demonstrated that the response rime, i.e., the time between completion of t h e t y p i n g of t h e i n p u t by t h e u s e r Co t h e appearance of t h e r e s p o n s e on t h e t e r m i n a l, is very good.
But the data bases used in the illustrations have been small, Coy d a t a b a s e s . The f o l l o w i n g t a b l e g i v e s a v e r a g e r e s p o n s e t i m e s for a few cases using larger data bases.
The query used for this illustration is: >What arm t h e d e s t i n a t i o n s of tankers?
The r e s p o u s e t i m e i s r a t h e r i n s e n s t i t i v e to the Coral number of individuals, classes, attributes and relations in the data base, depending primarily on the size of the relation (destination) and i t s a r g u m e n t ( C a n k e r s ) . Suppose t h a t t h e r e a r e m t a n k e r s i n t h e d a t a b a s e and t h a t n individuals have destinations, i.e., the size of the destination relation i s n.
T h e t a b l e g i v e s time in seconds.
no. of tankers > D i s p l a y t h e commence on B u d g e t P l a n s by e a c h section manager.
D i s p l a y i n g commence on B u d g e t P l a n s by e a c h section manager: J o h n Dobbs: D ( i s p l a y ), S(kip), o r Q(uit): 2 2 a 9 dastinscions I I I I C.
A d d i n z New O b i e c t T y ~ e s A l t h o u g h t h e ASK S y s t e m h a s b e e n d e s i g n e d t o allow the addition o f new o b j e c t t y p e s, t h i s c a n be d o n e o n l y by a n a p p l i c a t i o n programmer.
The major obstacle is the necessity to provide a procedure to initialize instances of the new object type and procedures that carry out their intrinsic manipulation.
However, we expect the addition of new object types to be a c o m m o n occurrence in the applications of the ASK System.
In any potential applicaion areas, using groups have accumulations of data already structured in specific ways and families of procedures that they have developed to manipulate these structures.
In ASK, they can identify these data structures as a new object type, design simple syucax for them to invoke their procedures, and thus embed their familar objects and manipulations within the ASK English dialect and within the same context as other associated aspects of their tasks.
The class, attributed and relation constructions become immediately available.
B e s p o n e e Time i n S e c o n d s f o r : >What a r e t h e d e s t i n a t i o n s of tankers?
B. The C q n c e v t o f A Use ~ C o n t e x t an_...dd the Basing Ooeration In the t e r m i n o l o g y of ASK, a user "Context" is a knowledge base together with the vocabulary and definitions that S o with it.
A given user will usually have several Contexts for v a r i o u s purposes, some of which may be the small "Ships" Context, a (truncated) bibliography of Artificial Intelligence literature and an a d m i n i s t r a t i v e Context concerning budget matters.
When one initiates a session with the ASK System, one is initially in the Command Context: >Welcome to ASK Please identify yourself.
>Fred >Pass word: You have mail.
Fred is in COMMAND, proceed.
At this point, you can list the Directory of Contexts available to you, create or delete Contexts, authorize others to use Contexts which you have created, and enter any of the Contexts in 21 fi>Directory context BASE Ships AI Bibliography Family Management Matters creator MASTER Fred Fred Fred Fred enter no yes yes yes yes b~s~ yes yes yes yes yes >enter Management Matters You are in Management Matters, proceed.
>Who have I received mail from?
Peter Jackson John Dobbs A new C o n t e x t is c r e a t e d by basing it on an already existing one.
Consider a u s e r who h a s b e e n a u t h o r i z e d f o r b a s i n g on t h e AI B i b l i o g r a p h y Context illustrated a b o v e and who w a n t s t o b u i l d a w i d e r b i b l i o g r a p h y C o n t e x t ( a d d i n g new i n f o r m a t i o n --vocabulary, data and definitions), however, without disturbing the old one.
To do s o, a l l s / h e n e e d s t o do i s s e l e c t a new n a m e, s a y CS B i b l i o g r a p h y, and t y p e : >exit You a r e >Base CS The new created >individual: E x p e r i e n c e w i t h ROBOT, L.
H a r r i s The f o l l o w i n g i n d i v i d u a l s have been added: E x p e r i e n c e w i t h ROBOT L.
H a r r i s >The a u t h o r o f E x p e r i e n c e w i t h ROBOT i s L.
Harris. L.
H a r r i s h a s b e e n a d d e d a s a u t h o r o f E x p e r i e n c e w i t h ROBOT.
>Keyword o f E x p e r i e n c e w i t h ROBOT i s d a t a b a s e . database has been added as keyword of E x p e r i e n c e with ROBOT.
>Who wrote what about databases? author D.
L. W a l t z N a t u r a l L a n g u a g e A c c e s s t o a Large Data Base L.
H a r r i s E x p e r i e n c e w i t h ROBOT > e x i t t o CB B i b l i o g r a p h y, You a r e i n CS B i b l i o g r a p h y, proceed.
>Who w r o t e w h a t a b o u t d a t a b a s e s ? author D.
L. W a l t z N a t u r a l L a n g u a g e A c c e s s t o a Large Data Base C.
J. D a t e An I n t r o d u c t i o n to Database Systems L.
H a r r i s E x p e r i e n c e w i t h ROBOT Several C o n t e x t s can be based on a g i v e n one, and one C o n t e x t can be b a s e d on several, thus a hierarchical structure of Contexts can be realized.
All Contexts are directly or indirectly based upon the BASE Context, w h i c h c o n t a i n s the f u n c t i o n words and g r a m m a r of the ASK d i a l e c t of English, the mathematical and statistical capabilities, and the word processor.
i n COMMAND, p r o c e e d . Bibliography on AI Bibliography context CS Bibliography has been b a s e d on AI B i b l i o g r a p h y basing action is t h i s new C o n t e x t : a ne w The r e s u l t of this Context.
Upon e n t e r i n g > E n t e r CS B i b l i o g r a p h y You a r e in CS Bibliography, one c a n make a d d i t i o n s : proceed.
C. T~anspo~tabilitv It is easy and fast to apply ASK to a n e w domain, given that a data base for this new domain is a v a i l a b l e in m a c h i n e r e a d a b l e form.
The vehicle is t h e ASK dialogue-driven Bulk Data Input capability, w h i c h can be called upon to build an existing database into one's Context.
The result not only i n t e g r a t e s this n e w data w i t h that already in the C o n t e x t and under the ASK d i a l e c t of English, but in m a n y c i r c u m s t a n c e s w i l l make the use of this data m o r e r e s p o n s i v e to users" > i n d i v i d u a l s :An I n t r o d u c t i o n to Database S y s t e m s, C.
J . D a t e The f o l l o w i n g i n d i v i d u a l s h a v e b e e n a d d e d : An I n t r o d u c t i o n to D a t a b a s e S y s t e m s C.
J . D a t e >The a u t h o r o f An I n t r o d u c t i o n to Database S y s t e m s i s C.
J . D a t e . C.
J . D a t e h a s been added a s a u t h o r of An Introduction to Database Systems.
>Keyword o f An I n t r o d u c t i o n t o D a t a b a s e Systems is database.
d a t a b a s e h a s been a d d e d a s k e y w o r d o f An Introduction to Database Systems.
>Who w r o t e w h a t a b o u t d a t a b a s e s ? author D.
L. W a l t z N a t u r a l L a n g u a g e A c c e s s t o a L a r g e D a t a Base C.
J. D a t e An I n t r o d u c t i o n to D a t a b a s e Systems These additions to the CS B i b l i o g r a p h y would not, of c o u r s e, e f f e c t She AI B i b l i o g r a p h y Context.
However, a d d i t i o n s and m o d i f i c a t i o n s that are subsequently made in the AI Bibliography Context would automatically be reflected in the CS Bibliography.
>exit You are in COMMAND, proceed.
>Enter AI Bibliography You are in AI Bibliography, proceed.
22 needs.
The Bulk Data Input D i a l o g u e p r o m p t s the user for n e c e s s a r y i n f o r m a t i o n to (i) e s t a b l i s h t h e physical structure of the d a t a b a s e to be included, (2) add necessary classes and attributes as needed for the new data entries.
The user also indicates, using E n g l i s h c o n s t r u c t i o n s, the i n f o r m a t i o n a l r e l a t i o n s h i p s a m o n g the fields in the p h y s i c a l records of the d a t a b a s e file that s/he wishes carried over to the ASK Context.
IV. DIALOGUES IN ASK Some have raised the question whether natural language is always the most desirable medium for a user to c o m m u n i c a t e w i t h the computer.
Expert systems, for example, have tended to use computer guided dialogues.
One simple form such a dialogue fimight take is illustrated by t h e f o l l o w i n s in w h i c h a new e n t r y i s a d d e d t o t h e AI B i b l i o g r a p h y : >New b i b l i o g r a p h y i t e m >Add to what b i b l i o g r a p h y ? AI B i b l i o g r a p h y >Title: Natural Language Processing >Author: Harry Tennant >Keyword: n a t u r a l l a n g u a g e >Keyword: s y n t a x p r o c e s s i n g >Keyword: s p e e c h a c t s >Keyword: Natural Language Processing has been added t o AI B i b l i o g r a p h y . >Title: The "new b i b l i o g r a p h y item" dialogue i8 completed.
>What A I B i b l i o g r a p h y items were written by Harry Tennant?
E x p e r i e n c e with the Evaluation of Natural Language Question Answerers Natural Language Processing necessary, respond with a diagnostic, (2) f i l l in other fields with data developed from the knowledge base, (3) extend the knowledge base, adding to the vocabulary and adding or changing the data itself, (4) file the completed form in p r e s c r i b e d f i l e s o r i n t h o s e i n d i c a t e d by t h e u s e r and a l s o m a i l it t o a s p e c i f i e d d i s t r i b u t i o n list through the electronic mail subsystem.
Since the Form p r o c e s s i n g c a n c h e c k c o n s i s t e n c y and m o d i f y the knowledge base, Forms can be used to facilitate data input.
S i n c e Form p r o c e s s i n g c a n fill f i e l d s in t h e Form, the forms c a p a b i l i t y includes the functions of a report generator.
L e t t e r s and memos c a n be written a s s p e c i a l c a s e s of Form filling, automatically adding dates, addresses, etc.
and filing and dispatching the result.
It must be easy and natural to add new Forms, if they are to be a convenient tool.
That is the function of the Forms Designing Dialogue.
Much like the Bulk Data Input Dialogue, the Forms Designing Dialogue holds a dialogue with the the user through which s/he can specify the fields of the Form itself and the processing of the above k i n d s t o be a u t o m a t i c a l l y accomplished at the time the Form is filled in.
Here is a simple example of a from that was designed using the Forms Designing Dialogue.
>What i s t h e bona p o r t and c o ~ a n d e r old ship?
There are 2 answers: (i) The~e is no c o ~ . n d e r . of each Other alternative media for user/system communication are menu boards, selection arrays and q u e r y by e x a m p l e . Many o t h e r c r y p t i c w a y s to communicate user needs to a knowledgeable system c a n be t h o u g h t o f ; o f t e n t h e m o s t u s e f u l m e a n s will be highly specific to the particular application.
For e x a m p l e, i n p o s i t i o n i n g c a r g o i n t h e h o l d o f a s h i p, o n e w o u l d l i k e t o be a b l e t o display the particular cargo space, showing its current cargo, and call for and move into place o t h e r i t e m s t h a t a r e to be i n c l u d e d . In the past, enabling the system to respond more intelligently to the user's needs required the provision of elaborate programs since the u s e r ' s t a s k s m a y be q u i t e i n v o l v e d, w i t h c o m p l e x decision structures.
The introduction of terse, effective communication has incurred lout delays and thus the changing needs of a user had little c h a n c e o f b e i n g m e t . I n t h e ASK S y s t e m, t h e u s e r s themselves can provide this knowledge.
They c a n i n s t r u c t t h e system on how to e l i c i t the necessary i n f o r m a t i o n and how to c o m p l e t e t h e r e q u i r e d t a s k . This ASK capability is quite facile, opening the way f o r i t s u b i q u i t o u s use in extending the knowledgeable responsiveness of the computer to user's immediate needs.
ASK i n c l u d e s two s y s t e m guided dialogues, similar to the Bulk Data Input d i a l o g u e by w h i c h u s e r s c a n i n s t r u c t t h e S y s t e m on how to be more r e s p o n s i v e t o t h e i r n e e d s . A.
Forms Desi~nin2 Dialogue The Form is an efficient means of communication with which we are all familiar.
A number of computer systems include a Forms package.
For most of these, however, filling in a Form results only in a document; the Form does not constitute a medium for interacting with the knowledge base or controllin K the actions of the system.
The ASK Forms capability enlarges the roles and ways in which Forms can be used as a m e d i u m for user interaction.
As the user fills in the fields of a Form, the System can make use of the information being supplied to (1) check its consistency with the data already in the k n o w l e d g e base and, if old ship hone port Ubu Naples Alamo London >Who i s J o h n S m i t h ? The f o l l o w i n s w o r d s a r e n o t i n t h e v o c a b u l a r y : John Smith > I n v e n t o r y o f wheat and c o r n o i l ? w h e a t and c o r n o i l i n v e n t o r y wheat 86.7 corn oil 123~00.
Note that the home port of the Alamo is London and that it does not have a commander, further that John Smith is not known to the System.
>Fill s h i p p i n g (For the purposes of the published paper, in contrast to the film shown at the presentation of the paper, only the initial and final copies of the form are given, under~ines indicate fields filled in by the "user", the o t h e r f i e l d s automatically being filled by the System).
(before) Shipping Form ship: port: quantity item $ price $ total commander: 23 fi(after) S h i p p i n g Form ship: port: A;amQ London item whvac corn oi~ J@hn SmiC~ price $ 35.75 $ 2.50 total $ 107.25 $1250.00 quantity ! 500 colmander: natural language programming capabiltty.
We hasten to add that it is not a general purpose program environment.
It is for "ultra-high" level programming, gaining its programming efficiency t h r o u g h t h e a s s u m p t i o n o f an e x t e n s i v e v o c a b u l a r y and knowledge base on which it can draw.
The illustrative d i a l o g u e a b o v e, w h i c h a d d s ' a new i t e m to a bibliography, is an example of a simple d i a l o g u e d e s i g n e d u s i n g DDD.
V. ACKNOWLEDGEMENTS AND CURRENT STATUS Shipping List for Alamo has been filed in Shipping Invoice File.
S h i p p i n g L i s t for Alamo h a s b e e n m a i l e d to J o n e s . mail t o : Fill shipping has been completed.
> L i s t t h e home p o r t and co-w,a n d e r o f e a c h old ship.
old ship home p o r t commander Ubu Naples -Alamo London John Smith >Inventory of wheat and corn oil? w h e a t and c o r n o i l i n v e n t o r y wheat 83.7 c o r n oil 122900.
>What is in the Shipping Invoice File?
Shipping List for Alamo The three System guided dialogues, Bulk Data Input, Dialogue Designing Dialogue and Forms D e s i g n i n g D i a l o g u e, are f r o m the d o c t o r a l dissertation of Tai-Ping Ho.
The aspects of ASK c o n c e r n i n g b a s i n g o n e C o n t e x t on a n o t h e r a r e f r o m the doctoral dissertation o f K w a n 8 I Yu.
The methods for handling anaphora, fragments and correction of inputs are from the doctoral dissertation of David Trawick.
ASK is implemented on the Hewlett Packard HP9836 desktop computer.
To handle Contexts of r e a s o n a b l e s i z e, one n e e d s a b a r d d i s k . An HP9836 with an HP9725 disk was used in the illustrations in this paper.
Our work is supported by the Hewlett Packard Corporation, Desktop Computer Division.
B. DialoKue Desi~nin~ Dialogue In the day-by-day use of an interactive system, users are very often involved in repetitive tasks.
They c o u l d be r e l i e v e d o f much o f t h e d r u d g e r y o f such tasks if the system were more knowledgeable.
Such a knowledgeable system, as it goes about a t a s k f o r t h e u s e r, may need a d d i t i o n a l information from the user.
What information it needs aCa particular point may depend on earlier user inputs and the current state of the database.
The user must provide the system with knowledge of a particular cask; more precisely s/he must program this knowledge into t h e system.
The result of this programming will be a system guided dialogue which the user can subsequently initiate and which will then elicit the necessary inputs.
Using these inputs in c o n j u n c t i o n w i t h the knowledge already available, particularly the data base, the system completes the task.
It is this system-guided dialogue chat the user needs to be able to d e s i g n . In the ASK System, there is a special dialogue w h i c h can be used co d e s i g n s y s t e m g u i d e d dialogues Co accomplish particular casks.
We call this the Dialogue Designing Dialogue (DDD).
Using DDD, the user becomes a computer-aided designer.
Since DDD, in conducting its dialogue with the user, only requires simple responses or responses phrased in ASK English, the user need have little programming skill or experience.
Using DDD, the user alone can replace a tedious, repetitive cask with an efficient system guided dialogue, all in a natural language environment.
The ASK Dialogue Designing Dialogue constitutes a high level, 24
A Robust P o r t a b l e N a t u r a l L a n g u a g e D a t a B a s e I n t e r f a c e Jerrold M.
Ginsparg Bell Laboratories Murray Hill, New Jersey 07974 A BSTRA C T This paper describes a NL data base interface which consists oF two parts: a Natural Language Processor (NLP) and a data base application program (DBAP).
The NLP is a general pur!~se language processor which builds a formal representation of the meaning of the English utterances it is given.
The DBAP is an algorithm with builds a query in a augmented relational algebra from the output of the NLP.
This approach yields an interface which is both extremely robust and portable.
where "colored", "'color" and "'house" are system primitives called concepts.
Each concept is an extended case frame, [Fillmore 2].
The meaning of each concept to the system is implicit in its relation to the other system concepts and the way the system manipulates it.
Each concept has case preferences associated with =IS cases.
For example, the case preference of color is "color and the case preference of coloredis "physical-object.
The case preferences induce a network among the concepts.
For example, "color is connected to "physical-object via the path: ['physical-object colored'colored color "color].
In addition.
"color is connected to "writing,implement, a refinement ot" "physicalobject, by a path whose meaning is that the writing implement writes with that color.
This network is used by the NLP to determine the meaning of many modifications, For example, "red pencil" is either a pencil which is red or a pencil that writes red, depending on which path is chosen.
In the absence of contextual information, the NLP chooses the shortest path.
In normal usage, case preferences are often broken.
The meaning of the broken preference involves coercing the offending concept to another one via a path in the network.
Examples are: "Turn on the soup".
"Turn on the burner that has soup on it".
"My car will drink beer".
"The passengers in my car will drink beer" This paper describes an extremely robust and portable NL data base interface which consists of two parts: a Natural Language Processor (NLP) and a data base application program (DBAP).
The NLP is a general purpose language processor which builds a formal representation of the meaning of the English utterances it is given.
The DBAP is an algorithm with builds a query in an augmented relational algebra from the output of the NLP.
The system is portable, or data base independent, because all that is needed to set up a new data base interface are definitions for concepts the NLP doesn't have, plus what I will call the +data base connection", i.e,, the connection between the relations in the data base and the NLP's concepts.
Demonstrating the portability and the robustness gained from using a general purpose NLP are the main subjects of this paper Discussion of the NLP will be limited to its interaction with the DBAP and the data base connection, which by design, is minimal.
[Ginsparg 5] contains a description of the NLP parsing algorithm.
3. The Data Base Connection 2.
NLP overview The formal language the NLP uses to represent meaning is a variant of semantic nets [Quillian 8].
For example, the utterances "The color of the house is green," "The house's color is green".
"Green,~ the color that the house is".
would all be ~ransformed to: Consider the data base given by the following scheme: Parts(pno,pname,color.cosl,weight) Spj( sno,pno,jno.quantity,m, y ) Suppliers and proiects have a number, )~ame and c~tV Parts ha'.,: a number, name, color, cost and weight Supplier wl(~,,unphe,, a quanntYof parts pno to prolect /no in month,nor year The data base connection has four parts: gl Isa: "colored Tense: present Colored: g2 Color: g3 lsa: "house Definite: the Isa: "color Value: green I.
Connecting each relation to the appropriate concept: Suppliers > "supplier fi2.
Connecting each attribute to the appropriate concept: leg., Spj(sno,pno,jno,cost,quantity)) depended on the supplier in which the cost ol a part 4.
C r e a t i n g pseudo relations Pseudo Cities jcity,scity This creates a pseudo relation, Cities(cname), so that the query building algorithm can treat all attributes as if they belong to a relation.
The query produced by the system will refer to the Cities relation.
A postprocessor is used to remove references to pseudo relations from the final query.
Pseudo relations are important because they ensure uniform handling of attributes.
With the pseudo Cities relation, questions like "Who supplies every city?
= and "List the cities".
can be treated identically to "Who supplies every project'"? and "List the suppliers".
The remainder of the data base connection is a set of switches which provide information on how to print out the relations.
whether all proper nouns have been defined or are to be inferred.
whether relations are multivalued, etc.
The switch settings and the four components above constitute the entire data base connection, Nothing else iS needed.
The network of concepts in the N L P should only be augmented for a particular data base; never changed.
Yet different data base schemes will require different representations for the same word.
For example, depending on the data base scheme, it could be correct to represent "box" as either, gl g2 g3 [sa: "part Conditions: "named(gl,box) Isa: "container Conditions: "named(g2.box) [sa: "box 3.
Capturing the information implicit in each relation: Parts(pno,pname,color,cost,weight ) "indexnumberp i n d e x n u m b e r > pno n u m b e r e d > Parts "named n a m e > pname n a m e d > Parts "colored color > color c o l o r e d > Parts costobj > Parts "weighs w e i g h t > weight w e i g h t o b j > Parts Projects(jno.jnamedcity) "indexnumberp indexnumber > jno n u m b e r e d > Projects "named name > jname n a m e d > Projects "located location > jetty located > Prolects Suppliers(sno,sname,scity) "indexnumberp indexnumber > sno n u m b e r e d > Suppliers "named name > sname n a m e d > Suppliers "located location > sctty located > Suppliers %pl O~no.pno.lno.quant Hv.m.y ) "supply supplier > '.no supplied > pno suppliee > mo (cardinality-of pno) > quantity u m e > m.y "spend spender > 1no s p e n d f o r -> pno amount (" cost quantity) The a m o u m case of "spend maps to a c o m p u t a t i o n rather than a,~mgle a t t r i b u t e It' all the attributes in the c o m p u t a h o n are not present,n the relation being defined, the query building program ioms,n the necessary extra relations.
So the definition of "spend ~mrks equally well irl tile example scheme as well as in a scheme 26 The solution is to define each word to map to the lowest possible concept.
W h e n a concept is e n c o u n t e r e d that has a data base relation associated with )t.
there is no problem.
If there )s no relauon associated with a concept, the N L p searchs For a concept that d o e s correspond to a relation and is also a generalization ot" the concept in question.
I f one is found, it is used with an appropriate condilion, usually "tilled or "named.
So "box" has a definition which m a p s to "box.
In the data base c o n n e c t i o n given above.
"box" would be instantiated as a "=part" since " ' b o x " is a r e f i n e m e n t of "'part" and no relation maps to "box," Using the Connection The information in the data base connection ts primarily used m building the query (section.~).
But It IS ~llso used Io augment the knowledge base of Ihe N L P The data base connection is used to overwrite the NLP's ca~e preferences.
Since I o c a w d > S u p p h e r s ()r Projects.
the preference ot" localed ts spec)fied to "suppliers or "protects.
This enables the NLP to interpret the first noun group )n "Do,m', suppliers that supply widgets located nl london also supply,~cre',vs )" as "'suppliers in London that supply widgets" rather than "supphers that,;upph London wldgets" This )s in contrast to [Gawron 31 which u'..es,i separate "disambiguator" phase to ehmlnale parses that do 11()i make sense =n the conceptual scheme of the dala base.
Tile additional preference informamm supplied bv the data base connection is used to induce coercions (section 2).
thai would rlot be made in the absence of the connection (~r under,mother data fibase scheme.
"Who supplies London" does not break any real world preferences, but does break one of the preferences induced by this data base scheme, namely that Suppliee is a "project.
London. a "city, is coerced to "project via the path [*project located *located /ocanon cityl and the question is understood to mean "Who supplies projects which are in London".
As mentioned in Section 2., the NLP determines the meanin~ of many modifications by searching for connections in a semantic net.
The data base connection is used to augment and highlight the existing network of the NLP.
I f the user says, "What colors do parts come in?', the NLP can infer that the meaning of "come-in" intended by the user is "colored since the only path through the net between "color and "part derived from the case preferences induced by the data base connection is ['part colored "colored color "color] Similarly, when given the noun group "London suppliers" the meaning is determined by tracing the shortest path through the highlighted net, ['supplier located'located Iocanon "city] The longer path connecting "supplier and "city, ['supplier supplier "supply suppliee *project located "location location *city] which means "the suppliers that supply to london projects" is found when the NLP rejects the first meaning because of context, If the user says "What are the locations of the London suppliers" the system assumes the second meaning since the first (in the domain of this data base scheme) leads to a tautological reading.
The NLP is able to infer that "The locations of the suppliers located in London" is tautological while "The locations of the suppliers located in England" is not, because the data base connection has specified "located to be a single valued concept with its Iocarton case typed to "city.
I f the system were asked for the locations of suppliers in England, and it knew England was a country, the question would be interpreted as "the cities of the suppliers that are located in cities located in England." The NLP treats most true-l'aise questions with indefinites as requests for the data which would make the statement true.
The question's meaning is "to show the subset of london proiects that are supplied by Blake".
The query building algorithm builds up the query recursively Given an instantiated concept with cases, =t expands the contents of each case and links the results together with the relation corresponding to the concept.
Given an instantiated concept with conditions, it expands each condition.
For the example, we have.
Expand gl Expand g2, the Element of gl Expand gg, the Condition of g2.
Expand g3, the Supplier case of gg.
Expand g9, the Condition of g3.
From the data base connection, a "named whose named case is a *supplier is realized by the Suppliers relation using the sname attribute So we have, g9 select tram Suppliers where sname -blake From the data base connection, a "supply is realized by the Spj relation.
This results in, gga -project]no/i'om.joinSpj to g9 g8 -joingga toProjects g8 is the projects supplied by Blake.
Expand 84, the set gl is a subset of, by expanding its element.
g6 Expand glO.
the Condition of gb Expand g7, the location case of glO yielding g l l -select #am Cities wherecname london A "located with a "project in the Iocotedcase ~s realized by the Projects relation using the ]city attribute.
So we have.
glOa -join Projects Io gl I where]city = cname glOb proiect ]no /'romglOa glO ]oinglOb toProjects g[0 is the projects in London.
Intersect the expansions of g2 and g4 and project the prolect names.
gl3 = pro/eel]name lrom imersectton g 8 glO 5.
A trtee of the query building algorithm.
The query budding algorithm is illustrated by tracmg its operation on the question, "Does blake supply any prolects in london'?" The entire query is, The NLP's meaning representation I'or this question ts shown below.
gO Isa: "show g5 Isa: "name Value: blake g9 Isa: "named Tense: present Named: g3 Name: g5 Isa: "tocated Tense: present Located: gb Location: g7 Isa: "named Tense: present Named: g7 Name: g12 Isa: *name Value: london Tense: present Toshow: g l gO [sa: "set Element: 2 Subset-of': g4 Isa: "protect Isa: "project Element-of: g4 Conditions: glO glO lsa: city Conditions: gll Isa: "supply Tense: Present Suppler: g3 Suppliee: g2 gl [ g9 = select/romSuppliers where s n a m e = blake g8a -/~'oiecrjno #om ioin Spj to g9 g8 = loin gga to Projects gl0 = select #am Projects where icily = london g 13 -prelect iname lrom mter'~e('tlo~t g8 g I0 where the: extra loin resulting f'rom the pseudo (:h=e~ relation ha', been rernoved by the post processor (section 3 ) Entirely as a side effe,'t of the way the query rs generated, the -,,,,,tern can easily correct any l'alse assumptions made by the u~,,2r [Kaplan 71.
For example, if there were no projects in London.
gill would be empty and system would respond, generating Irom the instantiated concept glO li.e., the names used in query correspond to the names used in the knowledge representatmnL "There arc no suppliers located in London".
No additional "'.=oiated presupposition" mechanism is requ+red.
The remainder of this section discusses several aspects o the query building process that the trace does not show.
Negations are handled by introducing a set difference when necessary If the example query were "Does Blake supply any projects that aren't in London?", the expansion of g7 would have been.
I f we ask.
"Who frequents a bar that serves a beer John likes?".
we get the following query.
81 82 g3 84 85 =" select from Likes where drinker john project beer l'rom g 1 .join Serves to 82 =" project bar I~om g3 "" join Frequents to 84 Expand g7.
the location case of glO yielding g i l a select [romCities wherecname -london gl 1 difference o f Cities a n d g l la Conjunctions are handled by introducing an an intersection or union.
I f the example query were "Does Blake supply any projects in London or Paris'?', the /ocanon case of g10 would have.
been the conjunction 813.
I f we ask "Who frequents a bar that serves a beer that he likes"? the correct query, is.
isa "conjunction Type: or Conjoins: g7 g14 [sa: "city Conditions: g l 5 lsa: "named Named: g15 Name: g l 6 Isa: "name Value: paris glb In the first query "beer" was the only attribute projected from g l [n the second, the system projected both "beer" and "drinker", because in expanding "a beer he likes" it needed to expand an instantiated concept (the one representing "who") that was already being expanded.
All of these cases interact gracefully with one another.
For example.
there is no problem in handling "Who supplies every project that is not supplied by blake and bowles".
The result of expanding gl3 would be, [n general, "or" becomes a union and "and" becomes an intersection.
However, if an "and" conjunction is in a single valued case (information obtained from the data base connection), a union is used instead.
Thus "Who supplies london and paris"? is interpreted as "Who supplies both London and Paris'"? and "Who is in London and Pans"? is interpreted as "Who is in London and who ~s m Paris"?
)n the example data base scheme.
6. Advantages of this approach The system can understand anything it has a concept about.
regardless o f whether the concept is attached to a relation in the data base scheme.
In the Suppliers data base from Secuon 4., parts had costs and weights associated with them, but not sizes.
I f a user asks "How big are each of the parts"? and the interface has a "size primitive (which it does), the query building process wdl attempt to find the relation which "size maps to and on fading wdl report back to the user.
"There is no information in the data base about the size of the parts".
This gives the user some informatmn about the what the data base contains, An answer like "1 don't know what "big" means".
would leave the user wondering whether size information was in the data base and obtainable if only the "right" word was used.
The system can interpret user statements that are not queries.
If the user says "A big supplier is a supplier that supplies more than 3 projects" the NLP can use, the definition qn answering later queries.
The definition is not made on a "string" basis e.g., substttuting the words of one side of the definition for the other Instead.
whenever the query building algorithm encounters an mstantiated concept that is a supplier wnh the condition "size~x.
big) it builds a query substnuting the condiuon from the definition that it can expand as a data base query Thus the .~vstern can handle "big london suppliers" and answer "Which sunpliers are big" which it couldn't if ~t were doing strlct string substitution.
This Facility can be used to bootstrap common definitions In,~ commercial flights application, with data base scheme, Flights(fl#,carrier,from.to,departure,arrival.stops.cost ) the word "nonstop" is defined to the system in English as, "A nonstop flight is a night that does not make any stops " and then saved along wuh the rest of the system's defimt~ons.
28 Quantifiers are handled by a post processing phase.
"Does blake supply every project in London"? is handled identically to "Does Blake supply a prolect in London'"? except that the expansion of "projects m London" is marked so that the post processor will be called.
The post processor adds on a set of commands which check that the set difference of London projects and London prolects that Blake supplies is empty.
The rasulhn 8 query is.
gl = =_2 g3 = g4 = =_5 = gO = g7 = g8 = ~e/ect lrom Suppliers w/weresname = blake ~elect l m m Projects where jcity london /otnSpl togl tomg3 to g2 protect jno from g2 protect ino /tom g4 {hl]~'rem'e org5 andgO empn, g7 ] h e first tour commands are the query for "Does Blake supply a llrolect m London'?".
The last tour check that no project in London is not supplied by Blake.
-\ minor modification is needed to cover cases in which the query building algorithm is expanding an instantiated concept that refercnces an instuntiated concept that is being expanded in a higher recursmve call The following examples illustrate this.
Consider the data base scheme below, taken from [Ullman ql.
Coercions (section 2).
can be used solve problems that may require inferences in other systems.
[Grosz 6] discusses the query "Is there a doctor within 200 miles of Philadelphia" in the context of a scheme in which doctors are on ships and ships have distances from cities, and asserts that a system which handles this query must be able to inter that if a doctor is on a ship, and the ship is with 200 miles of Philadelphia, then the doctor is within 200 miles of Philadelphia.
Using coercions, the query would be understood as "is there a ship with a doctor on it that is within 200 miles of Philadelphia?', which solves the problem immediately.
Since the preference information is only used to choose among competing interpretations, broken preferences can still be understood and responded to.
The preference for the supplier case is specified to supplier but if the user says "How many parts does the sorter project supply"? the NLP will find the only interpretation and respond "projects do not supply parts, suppliers do".
Ambiguities inherent in attribute values are handled using the same methods which handles words with multiple definitions.
For example, 1980 may be an organization number, a telephone extension, a number, or a year.
The NLP has a rudimentary (so far) expert system inference mechanism which can easily be used by the DBAP.
One of the rules it uses is "If x is a precondition of y and z knows y is true then z knows x was and may still be true" One of the ['acts in the NLP knowledge base is that being married is a precondition of being divorced or widowed.
I f a user asks "Did Fred Smith used to be married"? in a data base with the relation Employees(name, marital-status) the system can answer correctly by using its inference mechanism.
The exact method is as follows.
The data base application receives the true-false question: "Fred Smith was married and Fred Smith is no longer married" The system handles all the examples in this paper as well as a wide range of others (Appendix A.).
Several different data bases schemes have been connected to the system for demonstrations, including one "real data base" abstracted from the on-line listing of the Bell Laboratories Company Directory.
9. Since the data base includes only current marital status information.
the only way to answer the first part of the question is to inl'cr it from some other information in the data base.
The data base application sends the query to the NLP inference mechanism which would ordinarily attempt to answer it by matching it against its knowledge base or by finding a theorem which would gives it something else to match ['or When called by the data base application, the inference mechanism simply uses its rules base to decide what it should match ['or, and then returns to the data base program.
In this, example, the inference mechanism receives "Fred Smith was married" and using the precondition rule mentioned above, returns to the data base program, "Is Fred Smith divorced" or "is Fred Smith widowed", which can be answered by the data base.
The DBAP can call the inference mechanism recursively if necessary.
Appendix A: Sample Questions and Answers  does every department head in 1135 have an office in holmde[?
No. all heads Ihat work For center number 11.]5 are not located in an office in ho  who isn't alles,h g is the head not located in an ofi~ce in ho that works for center number [ 135  who has been with us for longer than fred smith? 7.
Implementation Status and Details The DBAP ~s fully implemented and debugged.
The NLP is ~mplemented and sail growing.
Both are implemented in Franz Lisp.
a dialect of LISP Language processing and query generai~on are performed m virtually real time (average 1-3 cpu seconds) on a Vax I 1-780 The system ~s intended to be used with u Data Base Management system.
The interface between the DBAP and the I-)BMS is a,,tralghtforward translator from relational algebra to the query language of Ihe DflMS I have written a Ir;.mslator I'or Polaris [Gielan 41.
There are no employees that have worked ('or you ['or more than the length or" the time fred smith has worked for ~.'~'~u  list the employees who used to be married and are not married now, paul rochester, verlon biggs and willie davis are the empin~ee,~ that were married and that are not married now.
TEAM: A TRANSPORTABLE NATURAL-LANGUAGE INTERFACE SYSTEM B a r b a r a J.
G r o s z Artificial Intelligence Center SRI I n t e r n a t i o n a l Menlo P a r k, CA 94025 A.
Overview A major benefit of u s i n g n a t u r a l language to the i n f o r m a t i o n in a database is that it shifts o n t o t h e system t h e b u r d e n of m e d i a t i n g b e t w e e n two v i e w s o f t h e d a t a : t h e way i n which t h e d a t a i s s t o r e d ( t h e " d a t a b a s e v i e w " ), and t h e way i n which an e n d u s e r thinks about it (the "user*s view").
Database information is recorded in terms of files, r e c o r d s, and fields, while natural-language expressions refer t o the same information i n terms of entities and relationships in the world.
A major problem in constructing a natural-language interface is determining how to encode and use the information needed to bridge these two views.
Current natural-language interface systems require extensive efforts by specialists in natural-language processing to p r o v i d e them w i t h t h e i n f o r m a t i o n t h e y need t o do the bridging.
The systems are, in effect, handtallored to provide access to particular databases.
access how Co o b t a i n t h e information requested.
Moving s u c h s y s t e m s to a new d a t a b a s e r e q u i r e s c a r e f u l handcrafting that involves d e t a i l e d knowledge o f such things ae p a r s i n g p r o c e d u r e s, t h e p a r t i c u l a r way i n which domain i n f o r m a t i o n i s stored, and data-access procedures.
To provide for transportability, TEAM s e p a r a t e s i n f o r m a t i o n a b o u t language, about the domain, and a b o u t the database.
The d e c i s i o n t o p r o v i d e t r a n s p o r t a b i l i t y to existing conventional databases (which d i s t i n g u i s h e s TEAM from CHAT [ W a r r e n, 1981]) means that t h e d a t a b a s e c a n n o t be r e s t r u c t u r e d t o make t h e way i n w h i c h i t s t o r e s d a t a more c o m p a t i b l e w i t h t h e way i n which a u s e r may a s k a b o u t t h e data.
A l t h o u g h many p r o b l e m s can be a v o i d e d i f one i s a l l o w e d t o d e s i g n t h e d a t a b a s e a s w e l l a s the natural-language system, given the prevalence of existing conventional databases, approaches w h i c h make t h i s assumption are likely t o have limited applicability in the near-term.
This paper f o c u s e s on the p r o b l e m of constructing transportable natural-language interfaces, i. e ., s y s t e m s t h a t can be a d a p t e d t o p r o v i d e a c c e s s t o d a t a b a s e s f o r which t h e y were not specifically handtailored.
It describes an initial version of a transportable system, called TEAM (for ~ransportable E_ngllsh A_ccess Data manager).
The hypothesis underlying the research described in this paper is that the i n f o r m a t i o n required for the adaptation can be obtained through an Lnteractlve dialogue with database management personnel who are not familiar with natural-language processing techniques.
The TEAM s y s t e m h a s three major components: ( 1 ) an a c q u Z s t t i o n component, ( 2 ) t h e DIALOGIC language system [Grosz, et al., 1982], and (3) a data-access ccaponent.
Section C descrlbes how the language and data-access components were designed to accommodate the needs of transportability.
S e c t i o o D d e s c r i b e s the d e s i g n of the acquisition component to allow flexible interaction ~rlth a database expert and discusses acquisition problems caused by the differences between the database view and user view.
Section E shows how end-user queries are interpreted after an acquisition has been completed.
Section F describes the current state of development of TEAM and lists several problems currently under investigation.
B. I s s u e s of T r a n s p o r t a b i l i t y C.
System Design The insistence on transportability distinguishes TEAM from previous systems such as LADDER [Hendrlx ec al., [978] LUNAR [Woods, Kaplan, and Webber, 1972], PLANES [Waltz, 1975], REL [Thompson, [975], and has affected ~he design of the natural-language processln~ system in several ways.
Most previously built naturallanguage interface systems have used techniques that make them inherently difficult to transfer to new domains and databases.
The internal representations [n these systems typically intermix (in their data structures and procedures) information about language with information about the domain and the database.
In addition, in Interpretln~ a query, the systems conflate what a user is requesting (what hls query "means") with 39 I n TEAM, t h e t r a n s l a t i o n o f an E n g l i s h q u e r y into a database query takes place in two s t e p s . First, the DIALOGIC system constructs a representation of the literal meaning or "logical form" of the query [Moore, 1981].
Second, the data-access component translates the logical form into a formal database query.
Each of these steps requires a combination of some information that is dependent on the domain or the database wlth some information that is not.
To provide for transportability, the TEAM system carefully separates these two kinds of information.
fiI. Domainand Database-Dependent Information To adapt TEAM to a new database three kinds of information must be acquired: information about words, about concepts, and about the structure of the database.
The data structures that encode this information--and the language processing and data-access procedures that use them--are designed to allow for acquiring new information automatically.
Information about words, lexlcal information, includes the syntactic properties of the words that will be used in querying the database and semantic information about the kind of concept t o which a particular word refers.
TEAM records the lexlcal information specific to a given domain in a lexicon.
Conceptual information includes information about taxonomic relationships, about the kinds of objects that can serve as arguments to a predicate, and a b o u t t h e k i n d s o f p r o p e r t i e s an object can have.
I n TEAM, t h e internal representation of information about the entities in the domain of discourse and the relationships that can hold among them is provided by a conceptual schema.
This schema includes a sort hierarchy encoding the taxonomic relationships among objects in the domain, information about constraints on arguments to predicates, and information about relationships among certain types of predicates.
database schema encodes information about how concepts in the conceptual schena map onto the structures of a particular database.
In particular, it links conceptual-schema representations of entities and relationships in the domain to their realization in a particular database.
TEAM currently assumes a relational database with a number of f i l e s . (No languageprocesslng-related problems are entailed in moving TEAM to other database models).
Each file is about some kind of object (e.g., employees, students, ships, processor chips); the fields of the file record properties of the object (e.g., department, age, length).
A To provide access to the informa=,on in a particular database, each of the components of DIALOG~C must access domain-speciflc information about the words and concepts relevant to that database.
The information required by the syntactic rules is found in the lexicon.
Information required by the semantic and pragmatic rules is found in the lexicon or the conceptual schema.
The rules themselves however do not include such domain-dependent information and therefore do not need to be changed for different databases.
In a similar manner, the data-access component separates general rules for translating logical forms into database queries from information about a particular database.
The rules access information i n the conceptual and database schemata to interpret queries for a particular database.
D. Acquisition TEAM i s d e s i g n e d t o i n t e r a c t w i t h two k i n d s o f u s e r s : a d a t a b a s e e x p e r t (DBE) and an e n d u s e r . The DBE provides information about the files and fields in the database through a system-dlrected acquisition dialogue.
As a result of this dlaloEue, the language-processlng and data-access components are extended so that the end-user may query the new database in natural-language.
i. Acquisition Questions Because the DBE is assumed to be familiar with database structures, but not with language-processlng techniques, the acquisition dialogue is oriented around database structures.
That is, the questions are about the kinds of things in the files and fields of the database, rather than about lexlcal entries, sort hierarchies, and predicates.
The disparity between the database view of the data and the end-user's view make the acquisition process nontrlvlal.
For instance, consider a database of information about students in a university.
From the perspective of an enduser "sophomore" refers to a subset of all of the students, those who are in their second year at the university.
The fact that a particular student is a sophomore might be recorded in the database in a number of ways, including: (l) in a separate file containing information about the sophomore students; (2) by a special value in a symbolic field (e.g., a CLASS field [n which the value SOPH indicates "sophomore"); (3) by a "true" value in a Boolean field (e.g., a * in an [S-$O?H field).
For natural-language querying to be useful, the end-user must be protected from having to know which type of representation was chosen.
The questions posed to the DBE for each kind of database construct must be sufficient to allow DIALOGIC to handle approximately the same range of Domain-lndependent Information The language executive [Grosz, e t a l ., 1982; Walker, 1978|, DIALOGIC, coordinates syntactic, semantic, and basic pragmatic rules in translating an English query into logical form.
DIALOGIC's syntactic rules provide a general grammar of English [Robinson, 1982].
A semantic "translation" rule associated with each syntactic phrase rule specifies how the constituents of the phrase are to be interpreted.
Basic pragmatic functions take local context into account in providing the interpretation of such things as noun-noun combinations.
DIALOGIC also includes a quantlfler-scoping algorithm.
filinguistic expressions (e.g., for referring to "students i n t h e sophomore c l a s s ' ) r e g a r d l e s s o f the particular database implementation chosen.
In all c a s e s, TEAM w i l l c r e a t e a l e x i c a l e n t r y f o r " s o p h o m o r e " and an e n t r y i n t h e c o n c e p t u a l schema to represent the concept of sophomores.
The database attachment for thls concept will depend on t h e p a r t i c u l a r d a t a b a s e s t r u c t u r e, as w i l l the kinds of predicates f o r which i t can be an argument.
Example of Acquisition Queeclons I n d e s i g n i n g TEAM we f o u n d i t i m p o r t a n t to distinguish three differanc kinds of fields N arlthmeCic, feature (Boolean), and s y m b o l l c o n the b a s i s of t h e r a n g e of l i n g u i s t i c expressions to which each gives r i s e . AriChmetic fields contain numeric values on which comparisons and computations llke averaging are likely to be done.
(Fields containing dates a r e n o t y e t h a n d l e d by TEAM).
Feature fields contain true/false values w h i c h r e c o r d w h e t h e r o r n o t some a t t r i b u t e i s a property of the object d e s c r i b e d by t h e file.
Symbolic f i e l d s typically contain values that c o r r e s p o n d to n o u n s o r a d j e c t i v e s t h a t d e n o t e t h e s u b t y p e s o f t h e domain d e n o t e d by t h e f i e l d . D i f f e r e n t a c q u i s i t i o n q u e s t i o n s a r e asked f o r each type of field.
These are illustrated in the example i n S e c t i o n D.3.
To illustrate the acquisition of information, consider a database, called CHIP, containing information about processor chips.
In particular, the fields in this database contain the following information: the identification number o f a c h i p ( I D ), its m a n u f a c t u r e r (MAKER) its width i n b i t s (WIDTH), ice speed in m e g a h e r t z (SPEED), its cost i n d o l l a r s (PRICE), the kind of technology (FAMILY), and a flag indicating wheCher o r noc t h e r e is an e x p o r t l i c e n s e f o r t h e c h i p (EXP).
In the figures discussed below, the DBE's r e s p o n s e is indicated in uppercase.
For many quesClone the DBE is presented wlch a llst of options from which ha can choose.
For these questions, the complete llst is shown and the answer indicated in boldface.
F i g u r e i shows t h e short-form of the questions asked about the file itself.
In r e s p o n s e to q u e s t i o n ( 1 ), t h e DBE t e l l s TEAM w h a t fields are in the file.
Responses to the r e m a i n i n g quesCloms allow TEAM t o identify t h e kind of object the file contains information about (2), types of linguistic expressions used to refer to It [ (6) and (7)], how to identify individual objects in the database (4), and how to s p e c i f y i n d i v i d u a l o b j e c t s to the u s e r ( 5 ) . These responses result in the words "chip" and " p r o c e s s o r " b e i n g added t o t h e l e x i c o n, a new s o r t added to the taxonomy (providing the interpretation f o r t h e s e w o r d s ), and a l i n k made i n t h e d a t a b a s e schema b e t w e e n t h i s sort and records i n the file CHIP.
Figure 2 gives the short-form of the most central questions asked about symbolic fields, using the field MAKER (chip manufacturers) as exemplar.
These questions are used to determine the kinds of properties represented, how t h e s e r e l a t e t o p r o p e r t i e s i n o t h e r f i e l d s, and the k i n d s of linguistic expressions the field values can give rise to.
Question (4) allows TEAM to determine that individual field values refer to manufacturers rather than chips.
The long-form of Q u e s t i o n (7) i s : Will you want to ask, for example, "How many MOTOROLA processors are there"? to get a count of the number of PROCESSORS with CHIP-MAKER-MOTOROLA?
Question (8) expands to: Will you want to ask, for example, "How many HOTOROLAS are there"? to get a count of the number of PROCESSORS with CHIP-MAKER-MOTOROLA?
Acquisition Strategy The ~ a J o r features of the s tra te gy developed for acquiring information about a database from a DBE include: (1) providiu E multiple levels of detail for each question posed to the DBE; (2) allowing a DBE to review previous answers and change them; and (3) checking for legal answers.
At present, TEAM initially presents the DBE wlth the short-form of a quesclou.
A more detailed version ("long-form') of the question, including examples illustratlng different kinds of responses, can be requested by the DBE.
An obvious excenslon to this strategy would be to present different Inltial levels t o different users ( d e p e n d i n g, f o r e x a m p l e, on t h e i r p r e v i o u s experience wlth the system).
A c q u i s i t i o n I s e a s i e r i f e a c h new p i e c e of information is immediately i n t e g r a t e d into the u n d e r l y i n g knowledge s t r u c t u r e s o f t h e p r o g r a m . 8 o w e v e r, we a l s o wanted Co a l l o w t h e DSE t o change a n s w e r s to p r e v i o u s q u e s t i o n s ( t h i s has t u r n e d o u t to be an essential feature of TEAM).
Some questions (e.g., those about irregular plural forms and synonyms) affect only a single part of TEAM (the lexicon).
Other questions (e.g., those about feature fields) affect all components of the system.
Because of the complex interaction between acquisition questions and components of the system to be updated, immediate integration of new information is not possible.
As a result, updating of the lexicon, conceptual schema, and database schema Is not done until an acqulsition dialogue is completed.
In t h i s ease, t h e a n s w e r to q u e s t i o n ( 7 ) I s " y e s " and to q u e s t i o n ( 8 ) " n o " ; the field has v a l u e s that can be used as explicit, but not implicit, classifiers.
Contrast this wlth a symbolic field in a file about students that contains the class of a student; in this case the answer to both fiauesclons would be affirmative because, for example, the phrases "sophomore woman" and "sophomores" can be used to refer to refer to STUDENTS with CLASS=SOPHOMORE.
In other cases, the values may serve neither as explicit nor as implicit classifiers.
For example, one cannot say *"the shoe employees" or *"the shoes" to mean "employees in the SHOE department".
For both questions (7) and (8) a positive answer i s the default.
It i s i m p o r t a n t to allow the user to override thls default, because TEAM must be able to avoid spurious ambiguities (e.g., where two fields have identical field values, but where the values can be classifiers for only one field.).
Following acquisition of this field, lexical entries are made for "maker" and any synonyms supplied by the user.
Again a new s o n is created.
It i s marked a s h a v i n g v a l u e s t h a t can be explicit, b u t not implicit, classifiers.
Later, when the actual connection to the database is made, individual field values (e.g., "Motorola") will be made individual instances of this new sort.
Figure (3) presents the questions asked about arithmetic fields, using the PRICE field as exemplar.
Because dates, measures, and count quantities are all handled differently, TEAM must first determine which kind of arithmetic object is in the field (2).
In this case we have a unit of "worth" (6) measured in "dollars" (4).
Questions (8) and (9) supply information needed for interpreting expressions Involvlng comparatives (e.g., "What chips are more expensive than the Z8080")? and superlatives (e--~7, "What is the cheapest chip?").
Figure 4 gives the expanded version of these questions.
As a result of thls acquisition, a new subsort of the (measure) sort WORTH i s added to the taxonomy for PRICE, and is noted as measured in dollars.
In addition, lexlcal entries are created for adjectives indicating positive ("expensive") and negative ("cheap") degrees of price and are linked to a binary predicate that relates a chip to its price.
Feature fields are the most difficult fields to handle.
They represent a single (arbitrary) property of an entity, with values that indicate whether or not the entity has the property, and they give rise to a wide range of linguistic expresslons--adJectlvals, nouns, phrases.
The short-form of the questions asked about feature fields are given in Figure 5, using the field EXP; the value YES indicates there is an export license for a given processor, and NO indicates there is not.
Figures 6, 7, and 8 give the expanded form of questions (4), (6), and (B) respectively.
The expanded form illustrates the kinds of end-user queries that TEAM can handle after the DBE has answered these questions (see also Figure 9).
Providing thls kind of illustration has turned out to be essential for getting these questions answered correctly.
Each of these types of expression leads to new lexlcal, conceptual schema, and database schema entries.
I n general in the conceptual schema, feature field adJectlvals and abstract nouns result in the creation of new predicates (see Section E for an example); count nouns result in the creation of new subsorts of the file subject sort.
The database schema contains informatlon about which field to access and what field value is required.
TEAM also includes a limlted capability for acqulrln8 verbs.
At present, only transitive verbs can be acquired.
One of the arguments to the predicate cozTespondlng to a verb must be of the same sort as the file subject.
The other argument must correspond to the sort of one of the fields.
For the CHIP database, the DBE could specify that the verb "make" (and/or "manufacture") takes a CHIP as one argument and a MAKER as the second argument.
E. Sample Q u e r i e s and T h e i r [nterpretatlons After the DBE has completed an acquisition session for a file, TEAM can interpret and respond Co end-user queries.
Figure 9 lists some sample end-user queries for the file illustrated in the previous section.
The role of the different kinds of informatlon acquired above can be seen by considering the logical forms produced for several queries and the database attachments for the sorts and predicates that appear in them.
The following examples illustrate the information acquired for the three different fields described in the preceding section.
Given the query, What are the Motorola chips?
DIALOGIC produces the following logical form: (Query (WHAT tl (THING tl) (THE p2 (AND (PROCESSOR p2) (MAKER-OF p2 MOTOROLA)) (EQ p2 tl)))) where WHAT and THE are quantifiers; 1 tl and p2 are variables; AND and EQ have their usual interpretation.
The predicates PROCESSOR and MAKER-OF and the constant MOTOROLA were created as a result of acquisition.
The schema: PROCESSOR: MAKER-OF: following information in the database 1 Because the current version of DIALOGIC takes no account of the slngular/plural distinction, the uniqueness presupposition normally associated with "the" is not enforced.
42 fii s u s e d, a l o n g with s o r ~ h i e r a r c h y i n f o r m a t i o n i n the conceptual schema, t o g e n e r a t e the actual database query.
Similarly, t h e e n d u s e r query chips? new acqulslClon component allows t h e user more flexibility i n answering questions and provides a wider range of default answers.
TEAM c u r r e n t l y h a n d l e s m u l t i p l e files and provides transportability to a l i m i t e d r a n g e o f databases.
As menCloned previously, a relational database model is assumed.
Currently, TEAM also assumes all files are In third normal form.
The acquisition of verbs is limited Co allowing t h e DBE Co s p e c i f y t r a n s I C l v e v e r b s, as described in S e c t i o n D.3.
We a r e c u r r e n t l y excending TEAM t o What a r e t h e e x p o r t a b l e would l e a d to t h e l o g i c a l form: ( Q u e r y (WHAT t l (THING cl) (THE p2 (AND (PROCESSOR p2) where EXP-POS is a predlcace created by acquisIClon; it is true if its argumanC is exportable.
In thls case the relevant database scheme information I s : PROCESSOR: EXP-POS: file-CHIP keyfleld-[D file-CHIP fleld-EXP fieldvalue-T (I) Provide for interpretation of expressions involving such things as mass terms, aggregates, quantified c o a m a n d s, and commands t h a c r e q u i r e t h e s y s t e m Co p e r f o r m f u n c t i o n s o t h e r t h a n q u e r y i n g che d a t a b a s e . Provide for efficient p r o c e s s i n g of the m o s t common f o r m s o f c o n j u n c t i o n . Generalize the verb acquisition p r o c e d u r e s and e x t e n d TEAM t o h a n d l e more complex verbs, including such Chings as verbs wlth mulClple delineations, verbs chat require special prepositions, and verbs that allow senCenclel complements.
Handle d a t a b a s e s encoding time-related information and e x t e n d DIALOGIC to handle expressions involving clme and tense.
Finally, co illustrate how TEAM h a n d l e s arithmetic f i e l d s, and I n p a r t i c u l a r the use of comparatives, consider the query: What c h i p i s c h e a p e r chart 5 d o l l a r s ? The l o g i c a l form f o r Chin q u e r y I s ( Q u e r y (WHAT pl (PROCESSOR pl) ((MORE C ~ A P ) pl (DOLLAH 5)))) The conceptual schema encodes the relationship between the predicates CHEAP and PRICE-OF (again, both concepts created as a result of acquisition), wlCh t h e following information CHEAP: measure-predlcate-PRICE-OF scale-negative G.
Acknowledgments The d e v e l o p m e n t of TEAM has involved the efforts of many people.
Doug Appelc, Armar Archbold, Bob Moore, Jerry Hobbs, Paul Marcln, Pernando Pereira, Jane Robinson, Daniel Sagalowicz, and David Warren have made ~ a J o r contributions.
This research was supported by the Defense Advanced Research Projects Agency with the Naval Electronic Systems Command under Contract N0003980-'<:-0645.
The views and conclusions contained in Chin document are chose of the author and should not be interpreted as representative of the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the United States Government.
And the relevant database schema Informaclon is: PROCESSOR: PRICE-OF: file-CHIP keyfield-[D flit-CHIP field(argl)=[D fleld(arg2)-PRICE F.
Status and Future Research An initial version of TEAM was implemented in a combination of Incerlisp (acquisition and DIALOGIC components) and Prolog (data access component) on the DEC2060, but address space llmicatlons made continued development difficult.
Current research on TEAM is being done on the Symbolics LISP machine.
The acquisition component has been redesigned co cake advantage of capabilities provided by che blcmap display.
The 43 File nameC H ~ (1} Fields (ID MAKER WIDTH SPEED PRICE FAMILY EXP) (2) Subject P R O C E S S O R (31 Synonyms for P R O C E S S O R C H I P (4} Primazy key ID {5} IdentifyingfieldsM A K E R ID (8) Can one say W h o are the P R O C E S S O R S ? Y E S N O (7) Pronouns for filesubject H E S H E IT T H E Y (8) Field containing the name of each file subject ID Figure 1: Questions About File Field P R I C E ( 1) Type of field SYMBOLIC A R I T l t M E T I C FEATURE (2) Value t y p e . DATES M E A S U R E S COUNTS [3) Are the units implicit?
Y E S N O (4) Enter implicit unit DOLLAR (5) Abbreviation for this unit.~ (6) Measure type of this trait TIME WEIGHT SPEED VOLUME LINEAR AREA W O R T H OTHER {7) Minimum and maximum numeric valucs(1,100) (8} Positive adjectives (EXPENSIVE COSTLY) (9) Negative adjective (CHEAP) Figure 3: Questions for Arithmetic Field P R I C E Please specify any adjectives that can be used in their comparative or superlative form to indicate how much each P R O C E S S O R is in a positivedirectionon the scale measured by the values of CHIP-PRICE.
In a file about machine-tools with a numeric field called PRICE, one could ask: How E X P E N S I V E is each tool? to mean What is the price of each tool.~ EXPENSIVE, COSTLY, AND (HIGH PRICED) ~re positive adjectives designating the upper range of the PRICE scale.
C H E A P and (LOW PRICED), which designate the lower range of the PRICE scale, are negative adjectives.
Field M A K E R ( I ) Type of field S Y M B O L I C ARITHMETIC FEATURE (2) .Axe field values units of measure?
YES N O (3} Noun subvategory P R O P E R COUNT MASS (4} Domain of field value's reference SUBJECT F I E L D (5) Can you say W h o is the C H I P M A K E R t Y E S N O (6) Typical value M O R T O R O L A (7) Will values of this field be used as cia~sifers.~ E S N O Y {8) Will the values in this field be used alone as implicit classifiers?
YES N O Figure 2: Questions for Symbolic Field M A K E R Please enter any such adjectives you will want to ~ querying the database.
Figure 4: Expanded Version of Adjective Questions (Arithmetic Field} in Field E X P (I) Type of field SYMBOLIC ARITHMETIC F E A T U R E (2) Positive value YES (3) Negative value NO (4) Positive adjectives EXPORTABLE (5) Negative adjectives UNEXPORTABLE (6) Positive abstraA't nouns EXPORT AUTHORIZATION (7) Negative abstract no~.1 (8) Pmitive common nouns (9) Negative common nouns Figure 5: Questions for Feature Field ]gXP List any count nous~ ammciated with positive field value YES.
In general, this is any word wwww such that you might want to u k : What PROCESSORS are wwww-s! to mean What PROCESSORS have a CHIP-EXP of YES?
For example, in a file about EMPLOYEEs with  feature field CITIZEN having a positive field value Y and n e ~ t i v e field value N, you might want to aek: Which employees are citizens? instead of Which employees have a CITIZEN of Y?
Figure 8: Feature Field Count Nouns What adjectivab are aasoeiated with the field values YES in this field?
In general these are word.5 wwww such that you might want to Mk: Which PROCESSORS are www~' Which PROCESSORS have  CHIP-EXP of YES!
For example, in s medical file about PATIENTs with a feature field IMM having a positive field value Y and a negative filed value N, you might want to ask: Which patients are IMMUNE (or RESISTANT, PROTECTED)!
Figure 6: Feature Field Adjectivals ~,Vhat 8 bit chips are cheaper than the fastest exportable chip made by Zilogt Who makes the fastest exportable N M O $ chip costing less than 10 dollars!
By whom is the most expensive chip reader Who b the cheapest exportable chip made by!
Who is the most expensive chip made?
What is the fastest exportable chip that Motorola makes?
What 16 bit chips does Zilog make?
Who makes the fastest exportable N M O S chip?
Who makes the faatest exportable chip.~ Does Zilog make a chip that is faster than every chip that Intel makes?
Are there any 8 bit Ziiog chipe? is some exportable chip faster than 12 mhz?
Is every Ziiog chip that is f ~ t e r than 5 mhz exportable?
How faat is the faate~t exportable chip?
How expensive is the f~stest ~'~MOS chipt Figure 9: Sample questions for CHIP databaae List any abstrart nouns ~k~tociated with the positive feature value YES.
In general this is any word wwww such that you might want to ask a question of the form: Which PROCESSORS hove wwww? tO m e a n Which PROCESSORS have CHIP-EXP of YES!
For example, in a medical databaae about PATIENTs with a feature field IMM having a positive field value Y and a negative field value N, you might want to a~k: ~,Vhich patients have IMMUNITY? instead of Which patients have aa IMM of Y?
Figure 7: Feature Field Abstract Nouns REFERENCES Grosz, B.
et al . [1982] "DIALOGIC: A Core Natural Language Processing System," Proceedings of the Ninth International Conference on Computational Linguistics, Prague, Czechoslovakia (July 1982).
Moore, R.
C. [1981] "Problems in Logical Form," in Proceedings of the 19th Annual Meeting of the Association for Computaional Linguistics, pp.
117-L24. The Association for Computaional Linguistics, SRI International, Menlo Park, Californla (June 1981)..
Waltz, D.
[1975] "Natural Language Access to a Large Data Base: An Engineering Approach," Proc.
4th International Joint Conference on Artificial Intelligence, Tbillsl, USSR, pp.
868-872 (September 1975).
Warren, D . R . [1981] "Efficient Processing of Interactive Relational Database Queries Expressed in Logic," Proc.
Seventh International Conference on Very Large DataBases, Cannes, France, pp.
2"'2~-2--~', Robinson, J.
[1982] "DIAGRAM: A Grammar for Dialogues," Communications of the ACM, Vol.
25, No.
1, pp.
27-47 (January 1982).
Thompson, g . B . and Thompson, B . H . [1975] "Practical Natural Language Processing: The REL System as Prototype," H.
Rubinoff and M.
C. Yovlts, eds., pp.
109-168, Advances in Computers 13, Academic Press, New York, (New York 1975).
Woods, W.
A., R.
M. Kaplan, and B.
N-Nebber [I972] "The Lunar Sciences Natural Language Information System," BBN Report 2378, Bolt Beranek and Newman, Cambridge, Massachusetts (1972).
Walker, D.
E. (ed).
[1978] Understanding Spoken Language, Elsevier North-Hollam~, New York, New York, (1978) .
Customizable Descriptions of Object-Oriented Models Benoit Lavoie CoGenTex, Inc.
840 Hanshaw Road Ithaca, NY 14850, USA benoitOcogentex, com Owen Rambow CoGenTex, Inc.
840 Hanshaw Road Ithaca, NY 14850, USA owen~cogentex, com Ehud Reiter Department of Computer Science University of Aberdeen Aberdeen AB9 2UE, Scotland ereiter~csd, abdn.
ac. uk 1 Introduction: Object Models With the emergence of object-oriented technology and user-centered software engineering paradigms, the requirements analysis phase has changed in two important ways: it has become an iterative activity, and it has become more closely linked to the design phase of software engineering (Davis, 1993).
A requirements analyst builds a formal object-oriented (OO) domain model.
A user (domain expert) validates the domain model.
The domain model undergoes subsequent evolution (modification or adjustment) by a (perhaps different) analyst.
Finally, the domain model is passed to the designer (system analyst), who refines the model into a OO design model used as the basis for implementation.
Thus, we can see that the OO models form the basis of many important flows of information in OO software engineering methodologies.
How can this information best be communicated?
It is widely believed that graphical representations are easy to learn and use, both for modeling and for communication among the engineers and domain experts who tqgether develop the OO domain model.
This belief is reflected by the large number of graphical OO modeling tools currently in research labs and on the market.
However, this belief is not accurate, as some recent empirical studies show.
For example, Kim (1990) simulated a modeling task with experienced analysts and a validation task with sophisticated users not familiar with the particular graphical language.
Both user groups showed semantic error rates between 25% and 70% for the separately scored areas of entities, attributes, and relations.
Relations were particularly troublesome to both analysts and users.
Petre (1995) compares diagrams with textual representations of nested conditional structures (which can be compared to OO modeling in the complexity of the "paths" through the system).
She finds that "the intrinsic difficulty of the graphics mode was the strongest effect observed" (p.35).
We therefore conclude that graphics, in order to assure maximum communicative efficiency, needs to be complemented by an alternate view of the data.
We claim that the alternate view should be provided by an explanation tool that represents the data in the form of a fluent English text.
This paper presents such a tool, the MODELEXPLAINER, or MODEx for short, and focuses on the customizability of the system.1 Automatically generating natural-language descriptions of software models and specifications is not a new idea.
The first such system was Swartout's GIST Paraphraser (Swartout, 1982).
More recent projects include the paraphraser in ARIES (Johnson et al., 1992); the GEMA data-flow diagram describer (Scott and de Souza, 1989); and Gulla's paraphraser for the PPP system (Gulla, 1993).
MoDEx certainly belongs in the tradition of these specification paraphrasers, but the combination of features that we will describe in the next section (and in particular the customizability) is, to our knowledge, unique.
2 Features
of MoDEx MODEx was developed in conjunction with Andersen Consulting, a large systems consulting company, and the Software Engineering Laboratory at the Electronic Systems Division of Raytheon, a large Government contractor.
Our design is based on initial interviews with software engineers working on a project at Raytheon, and was modified in response to feedback during iterative prototyping when these software engineers were using our system.
 MoDEx output integrates tables, text generated automatically, and text entered freely by the user.
Automatically generated text includes paragraphs describing the relations between classes, and paral(Lavoie et al., 1996) focuses on an earlier version of MoDEx which did not yet include customization.
253 graphs describing examples.
The human-anthored text can capture information not deducible from the model (such as high-level descriptions of purpose associated with the classes).
 MoDEx lets the user customize the text plans at run-time, so that the text can reflect individual user or organizational preferences regarding the content and/or layout of the output.
 MoDEx uses an interactive hypertext interface (based on standard HTML-based WWW technology) to allow users to browse through the model.
 Input to MoDEx is based on the ODL standard developed by the Object Database Management Group (Cattell, 1994).
This allows for integration with most existing commercial off the shelf OO modeling tools.
Some previous systems have paraphrased complex modeling languages that are not widely used outside the research community (GIST, PPP).
 MODEX does not have access to knowledge about the domain of the OO model (beyond the OO model itself) and is therefore portable to new domains.
3 A
MoDEx Scenario Suppose that a university has hired a consulting company to build an information system for its administration.
Figure 1 shows a sample object model for the university domain (adapted from (Cattell, 1994, p.56), using the notation for cardinality of Martin and Odell (1992)) that could be designed by a requirements analyst.
Figure 1: The University OoO Diagram Once the object model is specified, the analyst must validate her model with a university administrator (and maybe other university personnel, such as dataentry clerks); as domain expert, the university administrator may find semantic errors undetected by the analyst.
However, he is unfamiliar with the "crow's foot" notation used in Figure 1.
Instead, he uses MoDEx to generate fluent English descriptions of the model, which uses the domain terms from the model.
Figure 2 shows an example of a description generated by MoDEx for the university model.
Suppose that in browsing through the model 254 using the hypertext interface, the university administrator notices that the model allows a section to belong to zero courses, which is in fact not the case at his university.
He points out the error to the analyst, who can change the model.
Suppose now that the administrator finds the texts useful but insufficient.
To change the content of the output texts, he can go to the Text Plan Configuration window for the text he has been looking at, shown in Figure 3.
He can add to the text plan specification one or more constituents (paragraphs) from the list of pre-built constituents (shown in the lower right corner of Figure 3).
After saving his modifications, he can return to browsing the model and obtain texts with his new specifications.
File Edit View Go Bookmarks Options Directory ~indow Help [List of Classes] [List of Models] [Reload Models] [Configuration] ~ [About ModeIF.xolame,] Description of the Class" Section' General Observations: A Section must be taught by exactly one F$ofesso, and may ~clong to zezo oz more Cqu~e s.
It must be tako by one ca more Students and may have at most one TA.
Examples: For example, Sectl is a Section and is taught by the professor Jolm Brown.
It belongs to two Courses, Math165 and Math201, and is take~ by two Students.
Frank Belfo~d and Sue Jones.
It has the TA Sally Blake.
Figure 2: Description Used for Validation............,-.,m~ .................
= ....................
I;|? i[Jlc Edll ~ew Go Bookmarks ~Jans Dlrc~r/ ~qndow Help Text Plsm Conflgm'aflon Tat Plmv V -'~'4'~"-(2~  ..,:L ~..cr=,.on o= = .=.,c~s ] 0 z~.~ ~.: ~===~==) i --=~'~ ~omponent I . -[ ~ose ~butes 3peretions :telafions-Teble :~elQ~ons-Te)d -:xemples-Long :xemples-Shod ~~ ~ ~'~ " Rle-Reference Figure 3: Text Plan Configuration Interface Once the model has been validated by the univerFile Edit View Go Bookmarks Options Directory Window _Help [List of Classes] [List of Model.~] [Reload Models] [Co:ffi~otation] [H~ [About ModelEx~01amer] [Q3_~] ~==:==~=~=~=~==::~==~==~:~:~====~===~::::::::::::::::::::::::::::~=~====~ Business Class: "Section' Purpose/Role: Course unit a student can take.
Ed11. Pu~o.~e Attributes: ii Am~u~ JiDeser~ t~n . iiTY~e .................. i i i ...................................... n~ber iSecUo n T'--"T""'7""" identifier ~#1NTF~3 ~ .................
]~'~ :::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::: Edit Attdbutee Relationships: A Section must be taught by exactly one Ptofee~ot and may belong to zero or more Cotuses.
It must b e taken by one or more Stud~nt.~ and may have at most one TAD server which receives requests via a standard Web CGI interface and returns HTML-formatted documents which can be displayed by any standard Web browser.
The documents generated by MoDEx are always generated dynamically in response to a request, and are composed of human-authored text, generated text and/or generated tables.
The main requests are the following: ModEx m Request i Figure 4: Description Used for Documentation sity administrator, the analyst needs to document it, including annotations about the purpose and rationale of classes and attributes.
To document it, she configures an output text type whose content and structure is compatible with her company's standard for OO documentation.
An example of a description obtained in modifying the text plan of Figure 3 is shown in Figure 4.
(This description follows a format close to Andersen Consulting's standard for documentation).
This description is composed of different types of information: text generated automatically (section Relationships), text entered manually by the analyst because the information required is not retrievable from the CASE tool object model (section Purpose), and tables composed both of information generated automatically and information entered manually (section Attributes).
The analyst then saves the text plan under a new name to use it subsequently for documentation purposes.
Note that while the generated documentation is in hypertext format and can be browsed interactively (as in the I-DOC system of Johnson and Erdem (1995)), it can of course also be printed for traditional paperbased documentation and/or exported to desktop publishing environments.
4 How
MODEX Works As mentioned above, MODEx has been developed as a WWW application; this gives the system a platform-independent hypertext interface.
Figure 5 shows the MoDEx architecture.
MoDEx runs as a Figure 5: MODEx Server Architecture  Text Plan Editing.
This generates an HTML document such as that shown in Figure 3 which allows a user to load/edit/save a text plan macro-structure specification.
A representation corresponding to the text plan of Figure 3 is shown in Figure 6.
Once edited, this representation can be stored permanently in the library of text plans and can be used to generate descriptions.
In this representation, User Text indicates free text entered for a title, while RelationsText and Examples-Short are schema names referring to two of the eight predefined text functions found in a C++ class library supplied with MoDEx.
alidation-Class) Ti~e, User Text Ti~e Schema ~itle Schema i I I I User Text Relations-Text User Text Examples-Short Figure 6: Macro-Stucture for Text Plan of Figure 3  Object Model Loading.
This loads an object model specification and generates a document displaying the list of classes found in the model.
 Description Generation.
This returns a description such as that shown in Figures 2 or 4.
To generate a description, the text planner creates a text structure corresponding to the text plan configuration selected by the user.
This text structure is a constituency tree where the internal nodes define the text organization, while the bottom nodes define its content.
The text content can be specified as syntactic repre255 sentations, as table specification and/or as humanauthored text for the titles and the object model annotations.
The text structure is transformed by the sentence planner which can aggregate the syntactic representations (cf.
conjunctions and in description on Figure 2) or introduce cue words between constituents (cf.
expression For example on Figure 2).
The resulting text structure is then passed to the text realizer which uses REALPRO (Lavoie and Rambow, 1997), a sentence realizer, to realize each individual syntactic representation in the text structure.
Finally, a formatter takes the final text structure to produce an HTML document.
 Object Model Annotation Editing.
This allows the user to edit human-authored annotations of the object model.
This editing can be done via links labelled Edit ...
which appear in Figure 4.
These human-authored texts are used by some of the predefined text functions to generate the descriptions.
5 Outlook
MoDEx is implemented in C++ on both UNIX and PC platforms.
It has been integrated with two object-oriented modeling environments, the ADM (Advanced Development Model) of the KBSA (Knowledge-Based Software Assistant) (Benner, 1996), and with Ptech, a commercial off-the-shelf object modeling tool.
MoDEx has been fielded at a software engineering lab at Raytheon, Inc.
The evaluation of MoDEx is based on anecdotal user feedback obtained during iterative prototyping.
This feedback showed us that the preferences regarding the content of a description can vary depending on the organization (or type of user).
The control that MoDEx gives over the text macro-structure is one step toward satisfying different types of text requirements.
We are currently extending MoDEx in order to give the user a better control over the text micro-structure, by replacing the set of predefined C++ text functions with customizable ASCII specifications.
This feature should make MODEx more easely portable among different types of users.
In addition, we intend to port MODEX to at least two new OO modeling environments in the near future.
Acknowledgments The first version of MoDEx for ADM was supported by USAF Rome Laboratory under contract F30602-92-C0015.
General enhancements to the linguistic machinery were supported by SBIR F30602-92-C-0124, awarded by USAF Rome Laboratory.
Current work on MODEx is supported by the TRP-ROAD cooperative agreement F30602-95-2-0005 with the sponsorship of DARPA and Rome Laboratory.
We are thankful to K.
Benner, M.
DeBellis, J.
Silver and S.
Sparks of Andersen Consulting, and to F.
Ahmed and B.
Bussiere of Raytheon Inc., for their comments and suggestions made during the development of MoDEx.
We also thank T.
Caldwell, R.
Kittredge, T.
Korelsky, D.
McCullough, A.
Nasr and M.
White for their comments and criticism of MoDEx.
Identifying Terms by their Family and Friends Diana Maynard Sophia Ananiadou Dept.
of Computer Science University of Sheffield Regent Court, 211 Portobello St Sheffield, $1 4DP, UK d.
maynard0dcs, shef.
ac. uk Computer Science, School of Sciences University of Saltbrd, Newton Building Saltbrd, M5 4WT, U.K. s.
ananiadou@salf ord.
ac. uk Abstract Multi-word terms are traditionally identified using statistical techniques or, more recently, using hybrid techniques combining statistics with shallow linguistic information.
Al)proaches to word sense disambiguation and machine translation have taken advantage of contextual information in a more meaningflfl way, but terminology has rarely followed suit.
We present an approach to t e r m recognition which identifies salient parts of the context and measures their strength of association to relevant candidate terms.
The resulting list of ranked terms is shown to improve on that produced by traditional methods, in terms of precision and distribution, while the information acquired in the process can also be used for a variety of other applications, such as disambiguation, lexical tuning and term clustering.
Introduction Although contextual information has been previously used, e.g. in general language (Grefenstette, 1994) mid in the NC-Value method for term recognition (Frantzi, 1998; Frantzi and Ananiadou, 1999), only shallow syntactic information is used in these cases.
The T R U C K S approach identifies different; elements of the context which are combined to form the Information Weight, a measure of how strongly related the context is to a candidate term.
The hffbrmation Weight is then combined with the statistical information about a candidate t e r m and its context, acquired using the NC-Value method, to form the SNC-Value.
Section 2 describes the NCValue method.
Section 3 discusses the importance of contextual information and explains how this is acquired.
Sections 4 and 5 describe the hffbrmation Weight and the SNC-VMue respectively.
We finish with an evaluation of the method and draw some conclusions about the work and its fllture.
Although statistical approaches to automatic term recognition, e.g.
(Bourigault, 1992; Daille et al., 1994; Enguehard and Pantera, 1994; 3usteson and Katz, 1995; Lauriston, 1996), have achieved relative success over the years, the addition of suitable linguistic information has the potential to enhance results still further, particularly in the case of small corpora or very specialised domains, where statistical information may not be so accurate.
One of the main reasons for the current lack of diversity in approaches to term recognition lies in the difficulty of extracting suitable semantic information from speeialised corpora, particularly in view of the lack of appropriate linguistic resources.
The increasing development of electronic lexieal resources, coupled with new methods for automatically creating and fine-tuning them from corpora, has begun to pave the way for a more dominant appearance of natural language processing techniques in the field of terminology.
The T R U C K S approach to t e r m recognition (Term Recognition Using Combined Knowledge Sources) focuses on identifying relevant contextual information from a variety of sources, in order to enhance traditional statistical techniques of t e r m recognition.
The NC-Value m e t h o d The NC-Value method uses a combination of linguistic and statistical information.
Terms are first extracted from a corpus using the C-Value method (Frantzi and Ananiadou, 1999), a measure based on frequency of occurrence and term length.
This is defined formally as: is not nested l~('n,) ~b~T~f(b)) a is nested where a is the candidate string, f(a) is its frequency in the corpus, eT, is the set of candidate terms that contain a, P(Ta) is the number of these candidate terms.
Two different cases apply: one for terms t h a t are found as nested, and one for terms that are not.
If a candidate string is not found as nested, its termhood is calculated from its total frequency and length.
If it is found as nested, termhood is calculated from its total frequency, length, frequency as a nested string, fiand the tmmber of longer candidate terms it; ai)l)ears in.
The NC-Value metho(1 builds oil this by incorl)orating contextual information in the form of a context factor for each candidate term.
A context word can be any noun, adjective or verb apI)earing within a fixed-size window of tim candidate term.
Each context word is assigned a weight, based on how frequently it appears with a ca lldidate term.
Ttmse weights m'e titan SUllslned for all colltext words relative to a candidate term.
The Context l"actor is combined with the C-Value to form tlm NC-Value: Category Verb Prep Noun Adj Weight 1.2 1.1 0.9 0.7 Table 1: We.ights for categories of boundary words where a is tile candidate term, Cvahte(a) is the Cvalue fin' tlm candidate term, CF(a) is the context factor tbr the candidate term.
Terminological knowledge Ternfinological knowledge concerns the terminological sta.tus of context words.
A context word whicll is also a term (whicll we call a context term) is likely to 1)e a better indicator than one wlfich is not.
The terminological status is determined by applying the NC-Value at)proach to the corlms, and considering tile top third of the list; of ranked results as valid terms.
A context term (CT) weight is then produced fin" each candidate term, based on its total frequency of occurrence with all relewmt context terms.
The CT weight is formally described as follows: Contextual Information: a Term's where a is the candidate term, 7', is the set: of context terms of a, d is a word from Ta, fa(d) is the frequency of d as a context term of a.
Semantic knowledge Semantic knowledge is obtained about context terms using the UMLS Metathesaurus and Semantic Network (NLM, 1997).
The former provides a semantic tag for each term, such as Acquired Abnormality.
The latte, r provides a hierarchy of semantic types, from wlfich we compute the similarity between a candidate term and the context I;erms it occurs with.
An example of part of tim network is shown in Figure Social Life Just as a person's social life can provide valuable clues al)out their i)ersonality, so we can gather much information about the nature of a term by investigating the coral)any it keeps.
We acquire this knowledge by cxtra:ting three different types of contextual information: 1.
syntactic; 2.
terminologic~fl; Syntactic knowledge Syntactic knowledge is based on words in the context which occur immediately t)efore or afl;er a candidatc term, wtfich we call boundary words.
Following "barrier word" al)proaches to term recoglfition (Bourigault, 1992; Nelson et al., 1995), where partitular syntactic categories are used to delimit era> didate terms, we develop this idea fllrther by weighting boundary words according to tlmir category.
The weight for each category, shown in Table 1, is all)cate(1 according to its relative likelihood of occurring with a term as opposed to a non-term.
A verb, therefore, occurring immediately before or after a candidate, term, is statistically a better indicator of a term than an adjective is.
By "a better indicator", we mean that a candidate term occurring with it is more likely to be valid.
Each candidate term is assigned a syntactic weight, calculated by summing the category weights tbr the context bomsdary words occurring with it.
Similarity is measured because we believe that a context term which is semantically similar to a candidate term is more likely to be significant than one wlfieh is less similar.
We use tim method for semantic distance described in (M~\ynard and Ananiadou, 1999a), wtfich is based on calculating the vertical position and horizontal distance between nodes in a hierarchy.
Two weights are cMculated:  positionah measured by the combined distance from root to each node measured by the number of shared common ancestors multiplied by the munber of words (usuMly two).
Similarity between the nodes is calculated by dividing tim commomflity weight by the 1)ositional weight to t)roduce a figure between 0 and 1, I being the ease The Information Weight The three individual weights described above are calculated for all relevant context words or context terms.
The total weights for the context are then combined according to the following equation: beC.
[TAIII OIIGANISM ITAIIlll ALGA Figure 1: Fragment of the Semantic Network where tile two nodes are identical, and 0 being the case where there is no common ancestor.
This is formally defined as follows: where a is the candidate term, Cais the set of context words of a, b is a word from C,, f,(b) is tlm frequency of b as a context word of a, syn~(b) is the syntactic weight of b as a context word of a, T.
is the set of context terms of a, d is a word fl'om T., fi,(d) is the frequency of d as a context term of a, sims(d) is the similarity weight of d as a context term of a.
This basically means t h a t the Infornlation Weight is composed of the total terminological weight, 511151tiplied by tile total semantic weight, and then added to the total syntactic weight of all the context words or context terms related to the candidate term.
where corn(w1...w,~) is the commonality weight of words The SNC-Value pos('wl...w,~) is the positional weight of words Let us take an example from the UMLS.
The similarity between a term t)elonging to the semantic category Plant and one belonging to the category Fungus would be calculated as follows:Tile Information Weight gives a score for each candidate term based on the ilnt)ortance of the contextual intbrmation surrounding it.
To obtain the final SNCValue ranking, the Information Weight is combined with the statistical information obtained using the NC-Vahm nmthod, as expressed formally below: where  Plant has the semantic code T A l l l and Fungus has the semantic code T A l l 2 .  The commonality weight is the number of nodes in common, multiplied by the number of terms we are considering.
T A l l l and T A l l 2 have 4 nodes in common (T, TA, TA1 and T A l l ) . So the weight will be 4 * 2 = 8.
 The positional weight is the total height of each of the terms (where tile root node has a height of 1).
T A l l l has a height of 5 (T, TA, TA1, T A l l and T A l l 1 ), and TAl12 also has a height of 5 (T, TA, TA1, T A l l and T A l l 2 ) . The weight will therefore be 5 + 5 = 10.
 The similarity weight is tile comlnonality weight divided by the positional weight, i.e. a is the candidate t e r m NCValue(a) is the NC-Value of a I W is the Inqmrtance Weight of a For details of the NC-Value, see (l:5'antzi and Ananiadou, 1999).
An example of the final result is shown in Table 2.
This corot)ares tile top 20 results from the SNCValue list with the top 20 from the NC-Value list.
The terms in italics are those which were considered as not valid.
We shall discuss the results in more detail in the next section, but we can note here three points.
Firstly, the weights for the SNC-Value are substantially greater than those for the NC-Vahm.
This, in itself, is not important, since it, is the position in the list, i.e. the relative weight, rather t h a n the absolute weight, which is important.
Secondly, we can see that there are more valid terms in the SNC-Value results than in the NC-Value results.
It Table 2: Top 20 results for the SNC-VaIue and NC-Value in hard to make flu:ther judgements based on this list alone, 1)ecause we cmmot s~3; wlmther on(; ter]u is 1)etter than another, if tiE(; two terms are both valid.
Thirdly, we can nee that more of the top 20 terms are valid tin' tim SNC-Vahm than for the NCValue: 17 (851X,) as ot)t)osed to 10 (50%).
discrei)an(:y 1)etween this lint and the lint validated by the manual experts (only 20% of the terms they judged valid were fOtlEl(1 ill the UMLS).
There are also further limitations to the UMLS, such as the fact that it is only nl)e(:ific to medicine in general, 1)ut not to eye t)athology, and the fact that it; is organised ill nllch a way that only the preferred terms, and not lexical variants, m'e actively and (:onnistently 1)r(~sent.
We first evaluate the similarity weight individually, since this is the main 1)rinciple on which the SNC-\Sflue method relies.
We then ewduate the SNC-VaIue as a whole t)y comparing it with the NCValue, so I;hat we can ewfluate the impact of tile addition of the deel)er forms of linguistic information incorl)orated in {:he hnI)ortance Weight.
Evaluation The SNC-Value method wan initially t(;sted on a eorl)US of 800,000 eye t)athoh)gy reI)ortn, which had 1)een tagged with the Brill t)art-of-nl)eeeh tagger (Brill, 1992).
The ca.ndidate terms we,'e first extracted using the NC-Value method (lhantzi, 1998), and the SNC-Value was then (:alculated.
To exvduate the results, we examined the p(.'rformanee of the similarity weight alone, and the overall 1)erformance of the system.
Similarity Weight Evaluation m e t h o d s The main evaluation i)rocedure was carried out with resl)ect to a manual assessment of tim list of terms l)y 2 domain exI)erts.
There are, however, 1)roblems associated with such an evaluation.
Firstly, there ix no gold standm:d of evaluation, and secondly, manual evaluation is both fallil)le and sul)jective.
To avoid this 1)rol)lem, we measure the 1)erformance of the system ill relative termn rather than in absolute terms, by measuring the improveln(mt over the results of tile NC-Value as eomt)ared with mmmal evahlation.
Although we could have used the list of terms 1)rovided in the UMLS, instead of a manu~ ally evahlated list, we found that there was a huge One of the 1)roblems with our method of calculating similarity is that it relies on a 1)re-existing lexi(:al resource, which Eneans it is 1)rone to errors and omissions.
Bearing in mind its innate inadequacies, we can nevertheless evaluate the expected theoretical performance of tilt measure by concerning ourselves only with what is covered by the thesaurus.
This means that we assume COml)leteness (although we know that this in not the case) and evahtate it accordingly, ignoring anything which may be inissing.
The semantic weight ix based on the premise that tile more similar a context term is to the candidate term it occurs with, the better an indicator that context term is.
So the higher the total semantic weight Section top set middle set b o t t o m set Table 3: Semantic weights of terms and non-terms for the candidate term, the higher the ranking of the term and the better the chance that the candidate term is a valid one.
To test the performmme of the semantic weight, we sorted the terms in descending order of their semantic weights and divided the list into 3, such that the top third contained the terms with the highest semantic weights, and the b o t t o m third contained those with the lowest.
We then compared how m a n y valid and non-valid terms (according to the manual evaluation) were contained in each section of the list,.
Tile results, depicted in Table 3, can be interpreted as follows.
In the top third of the list;, 76% were terms and 24% were non-terms, whilst in the middle third, 56% were terms and 44% were non-terms, and so on.
This means that most of the valid terms are contained in the top third of tile list mid the fewest valid terms are contained in the bottom third of the list.
Also, the proportion of terms to non-terms in tile top of tile list is such that there are more terms than non-terms, whereas in the b o t t o m of the list; there are more non-terms than ternis.
This therefore demonstrates two things:  more of' the terms with the highest semantic weights are valid, and fewer of those with the lowest semmitic weights are valid;  more valid terms have high semantic weights than non-terms, mid more non-terms have lower semantic weights than valid terms.
We also tested the similarity measure to see whether adding sosne statistical information would improve its results, and regulate any discrepancies in tile uniformity of the hierarchy.
The methods which intuitively seem most plausible are based on information content, e.g.(Resnik, 1995; Smeaton and Quigley, 1996).
The informatiosl content of a node is related to its probability of occurrence in the corpus.
Tile snore fi'equently it appears, the snore likely it is to be important in terms of conveying information, and therefore the higher weighting it should receive.
We performed experiments to cosnpare two such methods with our similarity measure.
The first considers the probability of the MSCA of the two terms (the lowest node which is an ancestor of both), whilst the second considers the probability of the nodes of the terms being colnpared.
However, the tindings showed a negligible difference between the three methods, so we conchlde that there is no Table 4: Precision of SNC-Vahle and NC-Value advantage to be gained by adding statistical int'ormation, fbr this particular corpus.
It; is possible that with a larger corlms or different hierarchy, this might slot be the case.
Overall E v a l u a t i o n of t h e S N C V a l u e We first; compare the precision rates for the SNCValue and the NC-Value (Table 4), by dividing tile ranked lists into 10 equal sections.
Each section contains 250 terms, marked as valid or invalid by the manual experts.
In the top section, the precision is higher for the SNC-Value, and in the b o t t o m section, it is lower.
This indicates that the precision span is greater fl~r the SNC-Value, and therefore that the ranking is improved.
The distribution of valid terms is also better for the SNC-Value, since of the valid terms, more appear at the top of the list than at the bottom.
Looking at Figure 2, we can see that the SNCValue graph is smoother than that of the NC-Vahle.
We can compare the graphs niore accurately using a method we call comparative upward trend.
Becruise there is no one ideal graph, we instead measure how much each graph deviates from a monotonic line downwards.
This is calculated by dividing the total rise in precision percentage by the length of the graph.
A graph with a lower upward trend will therefore be better than a graph with a higher upward trend.
If we compare the upward trends of the two graphs, we find that the trend for the SNCValue is 0.9, whereas the trend for the NC-Value is 2.7.
This again shows that the SNC-Value rmiking is better thmi the NC-Value ranking, since it is more consistent.
Table 5 shows a more precise investigation of the top portion of the list, (where it is to be expected that ternis are most likely to be wflid, and which is therefore the inost imi)ortant part of the list) We see that the precision is most iml)roved here, both in terms of accuracy and in terms of distribution of weights.
At the I)ottom of the top section, the PlccJshm T~ T T I Scctionollist tics for creating such a thesaurus automatically, or entrancing an existing one, using the contextual information we acquire (Ushioda, 1996; MaynaM and Anmfiadou, 1999b).
There is much scope tbr filrther extensions of this research.
Firstly, it; could be extended to other (lomains and larger corpora, in order to see the true benefit of such a.n apl)roach.
Secondly, the thesaurus could be tailored to the corpus, as we have mentioncd.
An incremental approach might be possible, whereby the similarity measure is combined with statistical intbrmation to tune an existing ontology.
Also, the UMLS is not designed as a linguistic resource, but as an information resource.
Some kind of integration of the two types of resource would be usefifl so that, for example, lexical variation could be more easily handled.
Table 5: Precision of SNC-\Sdue and NC-Vahm for top 250 terms precision is much higher for the SNC-Value.
This is important because ideally, all the terms in this part of the list should be valid, 7 Conclusions In this paper, we have described a method for multiword term extraction which improves on traditional statistical at)proaches by incorporating more specific contextual information.
It focuses particularly on measuring the strength of association (in semantic terms) l)etween a candidate term and its context.
Evahlation shows imi)rovement over the NC-Vahm approach, although the percentages are small.
This is largely l)ecmlse we have used a very small corpus for testing.
The contextuM information acquired can also be used for a mmlber of other related tasks, such as disambiguation and clustering.
C1D2D8CTD6D0CTCPDACTCS D7CTD1CPD2D8CXCR CXD2D8CTD6D4D6CTD8CPD8CXD3D2 CXD2 CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS D4CPD6D7CXD2CV A3 CFCXD0D0CXCPD1 CBCRCWD9D0CTD6 BVD3D1D4D9D8CTD6 CPD2CS C1D2CUD3D6D1CPD8CXD3D2 CBCRCXCTD2CRCT BWCTD4D8BA CDD2CXDACTD6D7CXD8DD D3CU C8CTD2D2D7DDD0DACPD2CXCP C8CWCXD0CPCSCTD0D4CWCXCPB8 C8BT BDBLBDBCBF D7CRCWD9D0CTD6BSD0CXD2CRBACRCXD7BAD9D4CTD2D2BACTCSD9 BTCQD7D8D6CPCRD8 CCCWCXD7 D4CPD4CTD6 CTDCD8CTD2CSD7 CP D4D3D0DDD2D3D1CXCPD0B9D8CXD1CT D4CPD6D7CXD2CV CPD0B9 CVD3D6CXD8CWD1 D8CWCPD8 D6CTD7D3D0DACTD7 D7D8D6D9CRD8D9D6CPD0 CPD1CQCXCVD9CXD8DD CXD2 CXD2D4D9D8 D7CTD2D8CTD2CRCTD7 CQDD CRCPD0CRD9D0CPD8CXD2CV CPD2CS CRD3D1D4CPD6CXD2CV D8CWCT CSCTD2D3B9 D8CPD8CXD3D2D7 D3CU D6CXDACPD0 CRD3D2D7D8CXD8D9CTD2D8D7B8 CVCXDACTD2 D7D3D1CT D1D3CSCTD0 D3CU D8CWCT CPD4D4D0CXCRCPD8CXD3D2 CTD2DACXD6D3D2D1CTD2D8 B4CBCRCWD9D0CTD6B8 BEBCBCBDB5BA CCCWCT CPD0CVD3D6CXD8CWD1 CXD7 CTDCD8CTD2CSCTCS D8D3 CXD2CRD3D6D4D3D6CPD8CT CP CUD9D0D0 D7CTD8 D3CU D0D3CVCXCRCPD0 D3D4CTD6CPD8D3D6D7B8 CXD2CRD0D9CSCXD2CV D5D9CPD2D8CXACCTD6D7 CPD2CS CRD3D2CYD9D2CRB9 D8CXD3D2D7B8 CXD2D8D3 D8CWCXD7 CRCPD0CRD9D0CPD8CXD3D2 DBCXD8CWD3D9D8 CXD2CRD6CTCPD7CXD2CV D8CWCT CRD3D1D4D0CTDCCXD8DD D3CU D8CWCT D3DACTD6CPD0D0 CPD0CVD3D6CXD8CWD1 CQCTDDD3D2CS D4D3D0DDD2D3B9 D1CXCPD0 D8CXD1CTB8 CQD3D8CW CXD2 D8CTD6D1D7 D3CU D8CWCT D0CTD2CVD8CW D3CU D8CWCT CXD2B9 D4D9D8 CPD2CS D8CWCT D2D9D1CQCTD6 D3CU CTD2D8CXD8CXCTD7 CXD2 D8CWCT CTD2DACXD6D3D2D1CTD2D8 D1D3CSCTD0BA BD C1D2D8D6D3CSD9CRD8CXD3D2 CCCWCT CSCTDACTD0D3D4D1CTD2D8 D3CU D7D4CTCPCZCTD6B9CXD2CSCTD4CTD2CSCTD2D8 D1CXDCCTCSB9 CXD2CXD8CXCPD8CXDACT D7D4CTCTCRCWCXD2D8CTD6CUCPCRCTD7B8 CXD2 DBCWCXCRCW D9D7CTD6D7 D2D3D8 D3D2D0DD CPD2D7DBCTD6 D5D9CTD7D8CXD3D2D7 CQD9D8 CPD0D7D3 CPD7CZ D5D9CTD7D8CXD3D2D7 CPD2CS CVCXDACT CXD2B9 D7D8D6D9CRD8CXD3D2D7B8 CXD7 CRD9D6D6CTD2D8D0DD D0CXD1CXD8CTCS CQDD D8CWCT CXD2CPCSCTD5D9CPCRDD D3CU CTDCCXD7D8CXD2CV CRD3D6D4D9D7B9CQCPD7CTCS CSCXD7CPD1CQCXCVD9CPD8CXD3D2 D8CTCRCWD2CXD5D9CTD7BA CCCWCXD7 D4CPD4CTD6 CTDCD4D0D3D6CTD7 D8CWCT D9D7CT D3CU D7CTD1CPD2D8CXCR CPD2CS D4D6CPCVB9 D1CPD8CXCR CXD2CUD3D6D1CPD8CXD3D2B8 CXD2 D8CWCT CUD3D6D1 D3CU D8CWCT CTD2D8CXD8CXCTD7 CPD2CS D6CTD0CPD8CXD3D2D7 CXD2 D8CWCT CXD2D8CTD6CUCPCRCTCS CPD4D4D0CXCRCPD8CXD3D2B3D7 D6D9D2B9D8CXD1CT CTD2B9 DACXD6D3D2D1CTD2D8B8 CPD7 CPD2 CPCSCSCXD8CXD3D2CPD0 D7D3D9D6CRCT D3CU CXD2CUD3D6D1CPD8CXD3D2 D8D3 CVD9CXCSCT CSCXD7CPD1CQCXCVD9CPD8CXD3D2BA C1D2 D4CPD6D8CXCRD9D0CPD6B8 D8CWCXD7 D4CPD4CTD6 CTDCD8CTD2CSD7 CPD2 CTDCCXD7D8CXD2CV D4CPD6D7B9 CXD2CV CPD0CVD3D6CXD8CWD1 D8CWCPD8 CRCPD0CRD9D0CPD8CTD7 CPD2CS CRD3D1D4CPD6CTD7 D8CWCT CSCTB9 D2D3D8CPD8CXD3D2D7 D3CU D6CXDACPD0 D4CPD6D7CT D8D6CTCT CRD3D2D7D8CXD8D9CTD2D8D7 CXD2 D3D6CSCTD6 D8D3 D6CTD7D3D0DACT D7D8D6D9CRD8D9D6CPD0 CPD1CQCXCVD9CXD8DD CXD2 CXD2D4D9D8 D7CTD2D8CTD2CRCTD7 B4CBCRCWD9D0CTD6B8 BEBCBCBDB5BA CCCWCT CPD0CVD3D6CXD8CWD1 CXD7 CTDCD8CTD2CSCTCS D8D3 CXD2CRD3D6B9 D4D3D6CPD8CT CP CUD9D0D0 D7CTD8 D3CU D0D3CVCXCRCPD0 D3D4CTD6CPD8D3D6D7 CXD2D8D3 D8CWCXD7 CRCPD0CRD9B9 D0CPD8CXD3D2 D7D3 CPD7 D8D3 CXD1D4D6D3DACT D8CWCT CPCRCRD9D6CPCRDD D3CU D8CWCT D6CTD7D9D0D8CXD2CV CSCTD2D3D8CPD8CXD3D2D7 DF CPD2CS D8CWCTD6CTCQDD CXD1D4D6D3DACT D8CWCT CPCRCRD9D6CPCRDD D3CU D4CPD6D7CXD2CV DF DBCXD8CWD3D9D8 CXD2CRD6CTCPD7CXD2CV D8CWCT CRD3D1D4D0CTDCCXD8DD D3CU D8CWCT D3DACTD6CPD0D0 CPD0CVD3D6CXD8CWD1 CQCTDDD3D2CS D4D3D0DDD2D3D1CXCPD0 D8CXD1CT B4CQD3D8CW CXD2 D8CTD6D1D7 D3CU D8CWCT D0CTD2CVD8CW D3CU D8CWCT CXD2D4D9D8 CPD2CS D8CWCT D2D9D1CQCTD6 D3CU CTD2D8CXD8CXCTD7 CXD2 D8CWCT CTD2DACXD6D3D2D1CTD2D8 D1D3CSCTD0B5BA CCCWCXD7 D4CPD6D7CXD1D3D2DD CXD7 CPCRCWCXCTDACTCS CQDD D0D3CRCPD0CXDECXD2CV CRCTD6D8CPCXD2 CZCXD2CSD7 D3CU D7CTD1CPD2D8CXCR D6CTD0CPD8CXD3D2D7 CSD9D6CXD2CV D4CPD6D7CXD2CVB8 D4CPD6D8CXCRD9D0CPD6D0DD D8CWD3D7CT CQCTD8DBCTCTD2 D5D9CPD2D8CXACCTD6D7 CPD2CS D8CWCTCXD6 D6CTD7D8D6CXCRD8D3D6 CPD2CS CQD3CSDD CPD6CVD9D1CTD2D8D7 A3 CCCWCT CPD9D8CWD3D6 DBD3D9D0CS D0CXCZCT D8D3 D8CWCPD2CZ BWCPDACXCS BVCWCXCPD2CVB8 C3CPD6CXD2 C3CXD4B9 D4CTD6B8 CPD2CS BTD0CTDCCPD2CSCTD6 C3D3D0D0CTD6B8 CPD7 DBCTD0D0 CPD7 D8CWCT CPD2D3D2DDD1D3D9D7 D6CTDACXCTDBCTD6D7 CUD3D6 CRD3D1D1CTD2D8D7 D3D2 D8CWCXD7 D1CPD8CTD6CXCPD0BA CCCWCXD7 DBD3D6CZ DBCPD7 D4CPD6D8CXCPD0D0DD D7D9D4B9 D4D3D6D8CTCS CQDD C6CBBY C1C1CBB9BLBLBCBCBEBLBJ CPD2CS BWBTCAC8BT C6BIBIBCBCBDB9BCBCB9BDB9BKBLBDBHBA B4D7CXD1CXD0CPD6 D8D3 D8CWCT DBCPDD CSCTD4CTD2CSCTD2CRCXCTD7 CQCTD8DBCTCTD2 D4D6CTCSCXCRCPD8CT CPD2CS CPD6CVD9D1CTD2D8 CWCTCPCS DBD3D6CSD7 CPD6CT D0D3CRCPD0CXDECTCS CXD2 D0CTDCCXCRCPD0CXDECTCS CUD3D6D1CPD0CXD7D1D7 D7D9CRCW CPD7 D8D6CTCT CPCSCYD3CXD2CXD2CV CVD6CPD1D1CPD6D7B5B8 CXD2 D3D6B9 CSCTD6 D8D3 CPDAD3CXCS CRCPD0CRD9D0CPD8CXD2CV CTDCD4D3D2CTD2D8CXCPD0 CWCXCVCWCTD6B9D3D6CSCTD6 CSCTB9 D2D3D8CPD8CXD3D2D7 CUD3D6 CTDCD4D6CTD7D7CXD3D2D7 D0CXCZCT CVCTD2CTD6CPD0CXDECTCS D5D9CPD2D8CXACCTD6D7BA BE BUCPD7CXCR CPD0CVD3D6CXD8CWD1 CCCWCXD7 D7CTCRD8CXD3D2 CSCTD7CRD6CXCQCTD7 D8CWCT CQCPD7CXCR CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS D4CPD6D7CTD6 B4CBCRCWD9D0CTD6B8 BEBCBCBDB5 DBCWCXCRCW DBCXD0D0 CQCT CTDCD8CTD2CSCTCS CXD2 CBCTCRB9 D8CXD3D2 BFBA BUCTCRCPD9D7CT CXD8 DBCXD0D0 CRD6D9CRCXCPD0D0DD D6CTD0DD D3D2 D8CWCT CSCTD2D3D8CPB9 D8CXD3D2D7 B4D3D6 CXD2D8CTD6D4D6CTD8CPD8CXD3D2D7B5 D3CU D4D6D3D4D3D7CTCS CRD3D2D7D8CXD8D9CTD2D8D7 CXD2 D3D6CSCTD6 D8D3 CVD9CXCSCT CSCXD7CPD1CQCXCVD9CPD8CXD3D2B8 D8CWCT D4CPD6D7CTD6 DBCXD0D0 CQCT CSCTACD2CTCS D3D2 CRCPD8CTCVD3D6CXCPD0 CVD6CPD1D1CPD6D7 B4BTCYCSD9CZCXCTDBCXCRDEB8 BDBLBFBHBN BUCPD6B9C0CXD0D0CTD0B8 BDBLBHBFB5B8 DBCWD3D7CT CRCPD8CTCVD3D6CXCTD7 CPD0D0 CWCPDACTDBCTD0D0 CSCTB9 ACD2CTCS D8DDD4CTD7 CPD2CS DBD3D6D7D8B9CRCPD7CT CSCTD2D3D8CPD8CXD3D2D7BA CCCWCTD7CT CRCPD8B9 CTCVD3D6CXCTD7 CPD6CT CSD6CPDBD2 CUD6D3D1 CP D1CXD2CXD1CPD0 D7CTD8 D3CU D7DDD1CQD3D0D7 BV D7D9CRCW D8CWCPD8BM C6C8 BEBVCPD2CS CB BEBVBN CXCU ADBN BEBVD8CWCTD2 ADBP BEBVCPD2CS ADD2 BEBVBM C1D2D8D9CXD8CXDACTD0DDB8 D8CWCT CRCPD8CTCVD3D6DD C6C8 CSCTD7CRD6CXCQCTD7 CP D2D3D9D2 D4CWD6CPD7CT CPD2CS D8CWCT CRCPD8CTCVD3D6DD CB CSCTD7CRD6CXCQCTD7 CP D7CTD2D8CTD2CRCTB8 CPD2CS D8CWCT CRD3D1D4D0CTDC CRCPD8CTCVD3D6CXCTD7 ADBPCPD2CS ADD2 CSCTD7CRD6CXCQCT COCP AD D0CPCRCZCXD2CV CP  D8D3 D8CWCT D6CXCVCWD8B3 CPD2CS COCP AD D0CPCRCZCXD2CV CP  D8D3 D8CWCT D0CTCUD8B3 D6CTD7D4CTCRD8CXDACTD0DDBN D7D3 CUD3D6 CTDCCPD1D4D0CT CBD2C6C8 DBD3D9D0CS CSCTD7CRD6CXCQCT CP CSCTCRD0CPD6CPD8CXDACTDACTD6CQ D4CWD6CPD7CT D0CPCRCZCXD2CV CPD2 C6C8 D7D9CQCYCTCRD8 D8D3 CXD8D7 D0CTCUD8 CXD2 D8CWCT CXD2D4D9D8BA CCCWCT D8DDD4CT CC CPD2CS DBD3D6D7D8B9CRCPD7CT B4D1D3D7D8 CVCTD2CTD6CPD0B5 CSCTD2D3D8CPB9 D8CXD3D2 CF D3CU CTCPCRCW D4D3D7D7CXCQD0CT CRCPD8CTCVD3D6DD CPD6CT CSCTACD2CTCS CQCTD0D3DBB8 CVCXDACTD2 CP D7CTD8 D3CU CTD2D8CXD8CXCTD7 BX CPD7 CPD2 CTD2DACXD6D3D2D1CTD2D8BM CCB4CBB5 BP D8 BM D8D6D9D8CW DACPD0D9CT CFB4CBB5 BP CUCCCACDBXBNBYBTC4CBBXCV CCB4C6C8B5BPCT BMCTD2D8CXD8DD CFB4C6C8B5 BP BX CCB4ADBPB5BPCWCCB4B5BNCCB4ADB5CX CFB4ADBPB5BPCFB4B5 A2CFB4ADB5 CCB4ADD2B5BPCWCCB4B5BNCCB4ADB5CX CFB4ADD2B5BPCFB4B5 A2CFB4ADB5 CCCWCT CSCTD2D3D8CPD8CXD3D2 BW D3CU CPD2DD D4D6D3D4D3D7CTCS CRD3D2D7D8CXD8D9CTD2D8 CXD7 CRD3D2D7D8D6CPCXD2CTCS D8D3 CQCT CP D7D9CQD7CTD8 D3CU D8CWCT DBD3D6D7D8B9CRCPD7CT CSCTD2D3B9 D8CPD8CXD3D2 CF D3CU D8CWCT CRD3D2D7D8CXD8D9CTD2D8B3D7 CRCPD8CTCVD3D6DDBN D7D3 CP CRD3D2B9 D7D8CXD8D9CTD2D8 D3CU CRCPD8CTCVD3D6DD C6C8 DBD3D9D0CS CSCTD2D3D8CT CP D7CTD8 D3CU CTD2B9 D8CXD8CXCTD7B8 CUCT BD BNCT BE BNBMBMBMCVB8 CPD2CS CP CRD3D2D7D8CXD8D9CTD2D8 D3CU CRCPD8CTCVD3D6DD CBD2C6C8 DBD3D9D0CS CSCTD2D3D8CT CP D7CTD8 D3CU CTD2D8CXD8DD A2 D8D6D9D8CW DACPD0D9CT D4CPCXD6D7B8 CUCWCT BD BNCCCACDBXCXBNCWCT BE BNBYBTC4CBBXCXBNBMBMBMCVBA C6D3D8CT D8CWCPD8 D2D3 CSCTD2D3D8CPD8CXD3D2 D3CU CP CRD3D2D7D8CXD8D9CTD2D8 CRCPD2 CRD3D2D8CPCXD2 D1D3D6CT D8CWCPD2 C7B4CYBXCY DA B5 CSCXABCTD6CTD2D8 CTD0CTD1CTD2D8D7B8 DBCWCTD6CT DA CXD7 CP DACPD0CTD2CRDD D1CTCPB9 D7D9D6CT D3CU D8CWCT D2D9D1CQCTD6D3CUC6C8D7DDD1CQD3D0D7 D3CRCRD9D6D6CXD2CV DBCXD8CWCXD2 D8CWCT CRD3D2D7D8CXD8D9CTD2D8B3D7 CRCPD8CTCVD3D6DDBA CCCWCXD7 D4CPD4CTD6 DBCXD0D0 D9D7CT D8CWCT CUD3D0D0D3DBCXD2CV CSCTACD2CXD8CXD3D2 D3CU CP CRCPD8CTCVD3D6CXCPD0 CVD6CPD1D1CPD6 B4BVBZB5BM BWCTACD2CXD8CXD3D2 BT CRCPD8CTCVD3D6CXCPD0 CVD6CPD1D1CPD6 BZ CXD7 CP CUD3D6D1CPD0 CVD6CPD1D1CPD6 B4C6BNA6BNC8B5 D7D9CRCW D8CWCPD8BM AF A6 CXD7 CP ACD2CXD8CT D7CTD8 D3CU DBD3D6CSD7 DBBN AF C8 CXD7 CP ACD2CXD8CT D7CTD8 D3CU D4D6D3CSD9CRD8CXD3D2D7 CRD3D2D8CPCXD2CXD2CVBM AD AX DB CUD3D6 CPD0D0 DBBEA6B8 DBCXD8CW AD BEBVB8 AD AX ADBP  CUD3D6 CTDACTD6DD D6D9D0CT ADBP AX BMBMBM CXD2 C8B8 AD AX  ADD2 CUD3D6 CTDACTD6DD D6D9D0CT ADD2 AX BMBMBM CXD2 C8B8 CPD2CS D2D3D8CWCXD2CV CTD0D7CTBN AF C6 CXD7 D8CWCT D2D3D2D8CTD6D1CXD2CPD0 D7CTD8 CUAD CY AD AX BMBMBM BE C8CVBA CPD2CS D8CWCT CUD3D0D0D3DBCXD2CV CSCTCSD9CRD8CXDACT D4CPD6D7CTD6B8 BD DBCWCXCRCW DBCXD0D0 CQCT CTDCD8CTD2CSCTCS D0CPD8CTD6 D8D3 CWCPD2CSD0CT CP D6CXCRCWCTD6 D7CTD8 D3CU D7CTD1CPD2D8CXCR D3D4B9 CTD6CPD8CXD3D2D7BA CCCWCT D4CPD6D7CTD6 CXD7 CSCTACD2CTCS DBCXD8CWBM AF CRD3D2D7D8CXD8D9CTD2D8CRCWCPD6D8 CXD8CTD1D7 CJCXBNCYBNADCL CSD6CPDBD2 CUD6D3D1 C1 D2 BC A2 C1 D2 BC A2C6B8 CXD2CSCXCRCPD8CXD2CV D8CWCPD8 D4D3D7CXD8CXD3D2D7 CX D8CWD6D3D9CVCW CY CXD2 D8CWCT CXD2D4D9D8 CRCPD2 CQCT CRCWCPD6CPCRD8CTD6CXDECTCS CQDD CRCPD8CTCVD3D6DD ADBN AF CP D0CTDCCXCRCPD0 CXD8CTD1 CJCXBNCYBNADCL CUD3D6 CTDACTD6DD D6D9D0CT AD AX DB BE C8 CXCU DB D3CRCRD9D6D7 CQCTD8DBCTCTD2 D4D3D7CXD8CXD3D2D7 CX CPD2CS CY CXD2 D8CWCT CXD2D4D9D8BN AF CP D7CTD8 D3CU D6D9D0CTD7 D3CU D8CWCT CUD3D6D1BM CJCXBNCZBNADBPCLCJCZBNCYBNCL CJCXBNCYBNADCL CUD3D6 CPD0D0 AD AX ADBP  BE C8BN CXBNCYBNCZ BE C1 D2 BC B8 CJCZBNCYBNADD2CLCJCXBNCZBNCL CJCXBNCYBNADCL CUD3D6 CPD0D0 AD AX  ADD2 BE C8BN CXBNCYBNCZ BE C1 D2 BC BA CPD2CS CRCPD2 D6CTCRD3CVD2CXDECT CPD2 D2B9D0CTD2CVD8CW CXD2D4D9D8 CPD7 CP CRD3D2D7D8CXD8D9CTD2D8 D3CU CRCPD8CTCVD3D6DD AD B4CUD3D6 CTDCCPD1D4D0CTB8 CPD7 CPD2 CBB5 CXCU CXD8 CRCPD2 CSCTCSD9CRCT D8CWCT CRCWCPD6D8 CXD8CTD1 CJBCBND2BNADCLBA CCCWCXD7 D4CPD6D7CTD6 CRCPD2 CQCT CXD1D4D0CTD1CTD2D8CTCS CXD2 CP CSDDD2CPD1CXCR D4D6D3B9 CVD6CPD1D1CXD2CV CPD0CVD3D6CXD8CWD1B8 D9D7CXD2CV D8CWCT D6CTCRD9D6D7CXDACT CUD9D2CRD8CXD3D2BM BYB4DCB5BP CN CP BD BMBMBMCP CZ D7BMD8BM CP BD BMBMBMCP CZ DC CZ CM CXBPBD BYB4CP CX B5 B4DBCWCTD6CT DCBNCP BD BMBMBMCP CZ CPD6CT D4D6D3D4D3D7CTCS CRD3D2D7D8CXD8D9CTD2D8D7 CSD6CPDBD2 CUD6D3D1 C1 D2 BC A2C1 D2 BC A2C6B8 CF BN BPBYBTC4CBBXB8 CPD2CS CE BN BPCCCACDBXB5B8 CQDD D6CTCRD3D6CSCXD2CV D8CWCT D6CTD7D9D0D8 D3CU CTDACTD6DD D6CTCRD9D6D7CXDACT D7D9CQB9CRCPD0D0 D8D3 BYB4DCB5CXD2CPCRCWCPD6D8B8 D8CWCTD2 CRD3D2D7D9D0D8CXD2CV D8CWCXD7 CRCWCPD6D8 D3D2 D7D9CQB9 D7CTD5D9CTD2D8 CRCPD0D0D7 D8D3 BYB4DCB5 CUD3D6 D8CWCT D7CPD1CT DC CRD3D2D7D8CXD8D9CTD2D8BA BE CBCXD2CRCT D8CWCT CXD2CSCXCRCTD7 CXD2 CTDACTD6DD D6D9D0CTB3D7 CPD2D8CTCRCTCSCTD2D8 CRD3D2B9 D7D8CXD8D9CTD2D8D7 CP BD BMBMBMCP CZ CTCPCRCW CRD3DACTD6 D7D1CPD0D0CTD6 D7D4CPD2D7 D8CWCPD2 D8CWD3D7CT CXD2 D8CWCT CRD3D2D7CTD5D9CTD2D8 DCB8 D8CWCT CPD0CVD3D6CXD8CWD1 DBCXD0D0 D2D3D8 CTD2D8CTD6 CXD2D8D3 CPD2 CXD2ACD2CXD8CT D6CTCRD9D6D7CXD3D2BN CPD2CS D7CXD2CRCT D8CWCTD6CT CPD6CT D3D2D0DD D2 BE CYC6CY CSCXABCTD6CTD2D8DACPD0D9CTD7 D3CU DCB8 CPD2CS D3D2D0DD BED2 CSCXABCTD6B9 CTD2D8 D6D9D0CTD7 D8CWCPD8 CRD3D9D0CS D4D6D3DACTCPD2DDCRD3D2D7CTD5D9CTD2D8 DC B4D8DBD3D6D9D0CT CUD3D6D1D7 CUD3D6 BP CPD2CS D2B8 CTCPCRCW DBCXD8CW D2 CSCXABCTD6CTD2D8DACPD0D9CTD7 D3CU CZB5B8 D8CWCT CPD0CVD3D6CXD8CWD1 D6D9D2D7 CXD2 D4D3D0DDD2D3D1CXCPD0 D8CXD1CTBM C7B4D2 BF CYC6CYB5BA CCCWCT D6CTD7D9D0D8CXD2CV CRCWCPD6D8 CRCPD2 D8CWCTD2 CQCT CPD2D2D3D8CPD8CTCS DBCXD8CW CQCPCRCZ D4D3CXD2D8CTD6D7 D8D3 D4D6D3CSD9CRCT CP D4D3D0DDD2D3D1CXCPD0B9D7CXDECTCS D7CWCPD6CTCS CUD3D6CTD7D8 BD BYD3D0D0D3DBCXD2CV CBCWCXCTCQCTD6 CTD8 CPD0BA B4BDBLBLBHB5BA BE BYD3D0D0D3DBCXD2CV BZD3D3CSD1CPD2 B4BDBLBLBLB5BA D6CTD4D6CTD7CTD2D8CPD8CXD3D2 D3CU CPD0D0 D4D3D7D7CXCQD0CT CVD6CPD1D1CPD8CXCRCPD0 D8D6CTCTD7 B4BUCXD0B9 D0D3D8 CPD2CS C4CPD2CVB8 BDBLBKBLB5BA CCD6CPCSCXD8CXD3D2CPD0 CRD3D6D4D9D7B9CQCPD7CTCS D4CPD6D7CTD6D7 D7CTD0CTCRD8 D4D6CTCUCTD6D6CTCS D8D6CTCTD7 CUD6D3D1 D7D9CRCW CUD3D6CTD7D8D7 CQDD CRCPD0CRD9D0CPD8CXD2CV CECXD8CTD6CQCX D7CRD3D6CTD7 CUD3D6 CTCPCRCW D4D6D3D4D3D7CTCS CRD3D2D7D8CXD8D9CTD2D8B8 CPCRCRD3D6CSCXD2CV D8D3 D8CWCT D6CTB9 CRD9D6D7CXDACT CUD9D2CRD8CXD3D2BM CB CE B4DCB5BP D1CPDC CP BD BMBMBMCP CZ D7BMD8BM CP BD BMBMBMCP CZ DC AW CZ CH CXBPBD CB CE B4CP CX B5 AX A1C8B4CP BD BMBMBMCP CZ CY DCB5 CCCWCTD7CT D7CRD3D6CTD7 CRCPD2 CQCT CRCPD0CRD9D0CPD8CTCS CXD2 D4D3D0DDD2D3D1CXCPD0 D8CXD1CTB8 D9D7CXD2CV D8CWCT D7CPD1CT CSDDD2CPD1CXCR D4D6D3CVD6CPD1D1CXD2CV CPD0CVD3D6CXD8CWD1 CPD7 D8CWCPD8 CSCTD7CRD6CXCQCTCS CUD3D6 D4CPD6D7CXD2CVBA BT D8D6CTCT CRCPD2 D8CWCTD2 CQCT D7CTB9 D0CTCRD8CTCSB8 CUD6D3D1 D8CWCT D8D3D4 CSD3DBD2B8 CQDD CTDCD4CPD2CSCXD2CV D8CWCT CWCXCVCWCTD7D8B9 D7CRD3D6CXD2CV D6D9D0CT CPD4D4D0CXCRCPD8CXD3D2 CUD3D6 CTCPCRCW CRD3D2D7D8CXD8D9CTD2D8BA CCCWCT CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS D4CPD6D7CTD6 CSCTD7CRD6CXCQCTCS CWCTD6CT D9D7CTD7 CP D7CXD1CXD0CPD6D1CTCRCWCPD2CXD7D1 D8D3 D7CTD0CTCRD8 D4D6CTCUCTD6D6CTCSD8D6CTCTD7B8 CQD9D8 D8CWCT D7CRD3D6CTD7 CPD6CT CQCPD7CTCS D3D2 D8CWCT D4D6CTD7CTD2CRCT D3D6 CPCQD7CTD2CRCT D3CU CTD2D8CXB9 D8CXCTD7 CXD2 D8CWCT CSCTD2D3D8CPD8CXD3D2 B4CXD2D8CTD6D4D6CTD8CPD8CXD3D2B5 D3CU CTCPCRCW D4D6D3B9 D4D3D7CTCS CRD3D2D7D8CXD8D9CTD2D8BM BF CB BW B4DCB5BP D1CPDC CP BD BMBMBMCP CZ D7BMD8BM CP BD BMBMBMCP CZ DC AW CZ CG CXBPBD CB BW B4CP CX B5 AX B7 B4 BD CXCU BWB4DCB5BIBPBN BC D3D8CWCTD6DBCXD7CT DBCWCTD6CT D8CWCT CSCTD2D3D8CPD8CXD3D2 BWB4DCB5 D3CU CP D4D6D3D4D3D7CTCS CRD3D2D7D8CXD8D9CTD2D8 DC CXD7 CRCPD0CRD9D0CPD8CTCS D9D7CXD2CV CPD2D3D8CWCTD6 D6CTCRD9D6D7CXDACT CUD9D2CRD8CXD3D2BM BWB4DCB5BP CJ CP BD BMBMBMCP CZ D7BMD8BM CP BD BMBMBMCP CZ DC AW AP CZ D3D2 CXBPBD BWB4CP CX B5 AX D3D2 B4 CAB4DCB5 CXCU CZ BPBC CUCWCXCV D3D8CWCTD6DBCXD7CT CXD2 DBCWCXCRCW CAB4DCB5 CXD7 CP D0CTDCCXCRCPD0 D6CTD0CPD8CXD3D2 CSCTACD2CTCS CUD3D6 CTCPCRCW CPDCCXD3D1 DC D3CU CRCPD8CTCVD3D6DD AD CTD5D9CPD0 D8D3 D7D3D1CT D7D9CQD7CTD8 D3CU ADB3D7 DBD3D6D7D8B9CRCPD7CT CSCTD2D3D8CPD8CXD3D2 CFB4ADB5B8 CPD7 CSCTACD2CTCS CPCQD3DACTBA BG CCCWCT D3D4CTD6CPD8D3D6 D3D2 CXD7 D2CPD8D9D6CPD0 B4D6CTD0CPD8CXD3D2CPD0B5 CYD3CXD2 D3D2 D8CWCT ACCTD0CSD7 D3CU CXD8D7 D3D4CTD6CPD2CSD7BM BTD3D2BU BP CUCWCT BD BMBMBMCT D1CPDCB4CPBNCQB5 CXCYCWCT BD BMBMBMCT CP CXBEBTBNCWCT BD BMBMBMCT CQ CXBEBUCV DBCWCTD6CT CPBNCQ AL BCBN CPD2CS AP CXD7 CP D4D6D3CYCTCRD8CXD3D2 D8CWCPD8 D6CTD1D3DACTD7 D8CWCT ACD6D7D8 CTD0CTD1CTD2D8 D3CU D8CWCT D6CTD7D9D0D8 B4CRD3D6D6CTD7D4D3D2CSCXD2CV D8CWCT D1D3D7D8 D6CTCRCTD2D8D0DD CSCXD7CRCWCPD6CVCTCS CPD6CVD9D1CTD2D8 D3CU D8CWCT CWCTCPCS D3D6 CUD9D2CRD8D3D6 CRCPD8CTCVD3D6DDB5BM APBT BP CUCWCT BE BMBMBMCT CP CXCYCWCT BD BMBMBMCT CP CXBEBTCV CCCWCXD7 CXD2D8CTD6D0CTCPDACXD2CV D3CU D7CTD1CPD2D8CXCR CTDACPD0D9CPD8CXD3D2 CPD2CS D4CPD6D7B9 CXD2CV CUD3D6 D8CWCT D4D9D6D4D3D7CT D3CU CSCXD7CPD1CQCXCVD9CPD8CXD3D2 CWCPD7 D1D9CRCW CXD2 CRD3D1D1D3D2 DBCXD8CW D8CWCPD8 D3CU BWD3DBCSCXD2CV CTD8 CPD0BA B4BDBLBLBGB5B8 CTDCCRCTD4D8 BF C0CTD6CTB8 D8CWCT D7CRD3D6CT CXD7 D7CXD1D4D0DD CTD5D9CPD0 D8D3 D8CWCT D2D9D1CQCTD6 D3CU D2D3D2B9 CTD1D4D8DD CRD3D2D7D8CXD8D9CTD2D8D7 CXD2 CPD2 CPD2CPD0DDD7CXD7B8 CQD9D8 D3D8CWCTD6 D1CTD8D6CXCRD7 CPD6CT D4D3D7B9 D7CXCQD0CTBA BG CBD3 CP D0CTDCCXCRCPD0 D6CTD0CPD8CXD3D2 CUD3D6 D8CWCT CRD3D2D7D8CXD8D9CTD2D8 COD0CTD1D3D2B3 D3CU CRCPD8CTCVD3D6DD C6C8 DBD3D9D0CS CRD3D2D8CPCXD2 CPD0D0 CPD2CS D3D2D0DD D8CWCT D0CTD1D3D2D7 CXD2 D8CWCT CTD2DACXD6D3D2D1CTD2D8B8 CPD2CS CP D0CTDCCXCRCPD0 D6CTD0CPD8CXD3D2 CUD3D6 D8CWCT CRD3D2B9 D7D8CXD8D9CTD2D8 COCUCPD0D0CXD2CVB3 D3CU CRCPD8CTCVD3D6DD CBD2C6C8 DBD3D9D0CS CRD3D2D8CPCXD2 CP D1CPD4B9 D4CXD2CV CUD6D3D1 CTDACTD6DD CTD2D8CXD8DD CXD2 D8CWCT CTD2DACXD6D3D2D1CTD2D8 D8D3 D7D3D1CT D8D6D9D8CW DACPD0D9CT B4CCCACDBX CXCU D8CWCPD8 CTD2D8CXD8DD CXD7 CUCPD0D0CXD2CVB8 BYBTC4CBBX D3D8CWCTD6DBCXD7CTB5BM CTBACVBA CUCWD0CTD1D3D2 BD BNCCCACDBXCXBNCWD0CTD1D3D2 BE BNBYBTC4CBBXCXBNBMBMBMCVBA C6C8CJD0CTD1D3D2CL CUD0 BD BND0 BE BND0 BF BND0 BG CV C8BMC6C8D2C6C8BBC6C8CJCXD2CL CUCWCQ BD BNCWD0 BD BND0 BD CXCXBNCWD1 BD BNCWD0 BE BND0 BE CXCXCV C6C8CJCQCXD2CL CUCQ BD BNCQ BE CV C8BMC6C8D2C6C8BBC6C8CJCQDDCL CUCWD1 BD BNCWCQ BD BNCQ BD CXCXBNCWD1 BE BNCWCQ BE BNCQ BE CXCXCV C6C8CJD1CPCRCWCXD2CTCL CUD1 BD BND1 BE BND1 BF CV C8C8BMC6C8D2C6C8CJCXD2CL CUCWD0 BD BND0 BD CXCV C8C8BMC6C8D2C6C8CJCQDDCL CUCWCQ BD BNCQ BD CXBNCWCQ BE BNCQ BE CXCV C6C8CJD0CTD1D3D2CL CUD0 BD CV C6C8CJCQCXD2CL CUCQ BD BNCQ BE CV C8C8BMC6C8D2C6C8CJCXD2CL CUCWD0 BD BND0 BD CXCV C6C8CJD0CTD1D3D2CL CUD0 BD CVCJBN BYCXCVD9D6CT BDBM BWCTD2D3D8CPD8CXD3D2B9CPD2D2D3D8CPD8CTCS CUD3D6CTD7D8 CUD3D6 COD0CTD1D3D2 CXD2 CQCXD2 CQDD D1CPCRCWCXD2CTBAB3 D8CWCPD8 CXD2 D8CWCXD7 CRCPD7CTB8 CRD3D2D7D8CXD8D9CTD2D8D7 CPD6CT D2D3D8 D3D2D0DD D7CTD1CPD2B9 D8CXCRCPD0D0DD D8DDD4CTB9CRCWCTCRCZCTCSB8 CQD9D8 CPD6CT CPD0D7D3 CUD9D0D0DD CXD2D8CTD6D4D6CTD8CTCS CTCPCRCW D8CXD1CT D8CWCTDD CPD6CT D4D6D3D4D3D7CTCSBA BYCXCVD9D6CT BD D7CWD3DBD7 CP D7CPD1D4D0CT CSCTD2D3D8CPD8CXD3D2B9CPD2D2D3D8CPD8CTCS CUD3D6CTD7D8 CUD3D6 D8CWCT D4CWD6CPD7CT COD8CWCT D0CTD1D3D2 CXD2 D8CWCT CQCXD2 CQDD D8CWCT D1CPCRCWCXD2CTB3B8 D9D7CXD2CV D8CWCT D0CTDCCXCRCPD0CXDECTCS CVD6CPD1D1CPD6BM D0CTD1D3D2B8 CQCXD2B8 D1CPCRCWCXD2CT BM C6C8 D8CWCTBMC6C8BPC6C8 CXD2B8 CQDDBMC6C8D2C6C8BPC6C8 CXD2 DBCWCXCRCW D8CWCT CSCTD2D3D8CPD8CXD3D2 D3CU CTCPCRCW CRD3D2D7D8CXD8D9CTD2D8 B4D8CWCT D7CTD8 CXD2 CTCPCRCW D6CTCRD8CPD2CVD0CTB5 CXD7 CRCPD0CRD9D0CPD8CTCS D9D7CXD2CV CP CYD3CXD2 D3D2 D8CWCT CSCTD2D3D8CPD8CXD3D2D7D3CU CTCPCRCWD4CPCXD6D3CU CRD3D2D7D8CXD8D9CTD2D8D7D8CWCPD8 CRD3D1CQCXD2CT D8D3 D4D6D3CSD9CRCT CXD8BA C1D2 D8CWCXD7 CTDCCPD1D4D0CTB8 D8CWCT D6CXCVCWD8B9CQD6CPD2CRCWCXD2CV D8D6CTCT DBD3D9D0CS CQCT D4D6CTCUCTD6D6CTCS CQCTCRCPD9D7CT D8CWCT CSCTD2D3D8CPD8CXD3D2 D6CTB9 D7D9D0D8CXD2CV CUD6D3D1 D8CWCT CRD3D1D4D3D7CXD8CXD3D2 CPD8 D8CWCT D6D3D3D8 D3CU D8CWCT D3D8CWCTD6 D8D6CTCT DBD3D9D0CS CQCT CTD1D4D8DDBA CBCXD2CRCT D8CWCXD7 D9D7CT D3CU D8CWCT CYD3CXD2 D3D4CTD6CPD8CXD3D2 CXD7 D0CXD2CTCPD6 D3D2 D8CWCT D7D9D1 D3CU D8CWCT CRCPD6CSCXD2CPD0CXD8CXCTD7 D3CU CXD8D7 D3D4CTD6CPD2CSD7B8 CPD2CS D7CXD2CRCT D8CWCT CSCTD2D3D8CPD8CXD3D2D7 D3CU D8CWCT CRCPD8CTCVD3D6CXCTD7 CXD2 CP CVD6CPD1D1CPD6 BZ CPD6CT CQD3D9D2CSCTCS CXD2 CRCPD6CSCXD2CPD0CXD8DDCQDD C7B4CYBXCY DA B5 DBCWCTD6CT DA CXD7 D8CWCT D1CPDCCXD1D9D1 DACPD0CTD2CRDD D3CU D8CWCT CRCPD8CTCVD3D6CXCTD7 CXD2 BZB8 D8CWCT D8D3D8CPD0 CRD3D1D4D0CTDCCXD8DD D3CU D8CWCT CPCQD3DACT CPD0CVD3D6CXD8CWD1 CRCPD2 CQCT D7CWD3DBD2 D8D3 CQCT C7B4D2 BF CYBXCY DA B5BM D4D3D0DDD2D3D1CXCPD0 D2D3D8 D3D2D0DD D3D2 D8CWCT D0CTD2CVD8CW D3CU D8CWCT CXD2D4D9D8 D2B8 CQD9D8 CPD0D7D3D3D2 D8CWCT D7CXDECT D3CU D8CWCT CTD2DACXD6D3D2D1CTD2D8BX B4CBCRCWD9D0CTD6B8 BEBCBCBDB5BA BF BXDCD8CTD2CSCTCS CPD0CVD3D6CXD8CWD1 CCCWCT CPCQD3DACT CPD0CVD3D6CXD8CWD1 DBD3D6CZD7 DBCTD0D0 CUD3D6 CPD8D8CPCRCWCXD2CV D3D6CSCXB9 D2CPD6DD CRD3D1D4D0CTD1CTD2D8D7 CPD2CS D1D3CSCXACCTD6D7B8 CQD9D8 CPD7 CP D7CTD1CPD2D8CXCR D8CWCTD3D6DD CXD8 CXD7 D2D3D8 D7D9CRCXCTD2D8D0DD CTDCD4D6CTD7D7CXDACTD8D3 D4D6D3CSD9CRCT CRD3D6B9 D6CTCRD8 CSCTD2D3D8CPD8CXD3D2D7CXD2 CPD0D0CRCPD7CTD7BA BYD3D6 CTDCCPD1D4D0CTB8 D8CWCT D0CTDCCXCRCPD0 D6CTD0CPD8CXD3D2D7 CSCTACD2CTCS CPCQD3DACT CPD6CT CXD2D7D9CRCXCTD2D8 D8D3 D6CTD4D6CTD7CTD2D8 D5D9CPD2D8CXACCTD6D7 D0CXCZCT COD2D3B3 B4D9D7CXD2CV CRCPD8CTCVD3D6DD C6C8BPC6C8B5CXD2D8CWCT D4CWD6CPD7CT COD8CWCT CQD3DD DBCXD8CW D2D3 CQCPCRCZD4CPCRCZBAB3 BH BT D7CXD1CXD0CPD6 D4D6D3CQB9 D0CTD1 D3CRCRD9D6D7 DBCXD8CW CRD3D2CYD9D2CRD8CXD3D2D7BN CUD3D6 CTDCCPD1D4D0CTB8 D8CWCT DBD3D6CS COCPD2CSB3 B4D9D7CXD2CV CRCPD8CTCVD3D6DDC6C8D2C6C8BPC6C8B5 CXD2 D8CWCT D4CWD6CPD7CT COD8CWCT CRCWCXD0CS DBCTCPD6CXD2CV CVD0CPD7D7CTD7 CPD2CS CQD0D9CT D4CPD2D8D7B3B8 CPD0D7D3 CRCPD2D2D3D8 CQCT D4D6D3D4CTD6D0DD D6CTD4D6CTD7CTD2D8CTCS CPD7 CP D0CTDCCXCRCPD0 D6CTD0CPD8CXD3D2BA BI CCCWCXD7 D6CPCXD7CTD7 D8CWCT D5D9CTD7D8CXD3D2BM CWD3DB D1D9CRCW CTDCD4D6CTD7D7CXDACXD8DD CRCPD2 CQCT CPD0D0D3DBCTCS CXD2 CP D7CWCPD6CTCS D7CTD1CPD2D8CXCR CXD2D8CTD6D4D6CTD8CPD8CXD3D2 DBCXD8CWD3D9D8 CTDCCRCTCTCSCXD2CV D8CWCT D8D6CPCRD8CPCQD0CT D4CPD6D7CXD2CV CRD3D1D4D0CTDCCXD8DD D2CTCRCTD7B9 D7CPD6DD CUD3D6 D4D6CPCRD8CXCRCPD0 CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS D4CPD6D7CXD2CVBR C1D2 D8D6CPCSCXD8CXD3D2CPD0 CRCPD8CTCVD3D6CXCPD0 D7CTD1CPD2D8CXCRD7 B4C5D3D2D8CPCVD9CTB8 BDBLBJBFBN BUCPD6DBCXD7CT CPD2CS BVD3D3D4CTD6B8 BDBLBKBDBN C3CTCTD2CPD2 CPD2CS CBD8CPDACXB8 BDBLBKBIB5 D5D9CPD2D8CXACCTD6D7 CPD2CS D2D3D9D2 D4CWD6CPD7CT CRD3D2CYD9D2CRD8CXD3D2D7 CSCTB9 D2D3D8CT CWCXCVCWCTD6B9D3D6CSCTD6 D6CTD0CPD8CXD3D2D7BM D8CWCPD8 CXD7B8 D6CTD0CPD8CXD3D2D7 CQCTB9 D8DBCTCTD2 DBCWD3D0CT D7CTD8D7 D3CU CTD2D8CXD8CXCTD7 CXD2D7D8CTCPCS D3CU CYD9D7D8 CQCTB9 D8DBCTCTD2 CXD2CSCXDACXCSD9CPD0D7BA CDD2CSCTD6 D8CWCXD7 CXD2D8CTD6D4D6CTD8CPD8CXD3D2B8 CP D5D9CPD2D8CXACCTD6 D0CXCZCT COD2D3B3 DBD3D9D0CS CSCTD2D3D8CT CP D7CTD8 D3CU D4CPCXD6D7 CUCWBT BD BNBU BD CXBNCWBT BE BNBU BE CXBNBMBMBMCV DBCWCTD6CT CTCPCRCW BT CX CPD2CS BU CX CPD6CT CSCXD7CYD3CXD2D8 D7D9CQD7CTD8D7 D3CU BXB8 CRD3D6D6CTD7D4D3D2CSCXD2CV D8D3 CPD2 CPCRCRCTD4D8B9 CPCQD0CT D4CPCXD6 D3CU D6CTD7D8D6CXCRD8D3D6 CPD2CS CQD3CSDD D7CTD8D7 D7CPD8CXD7CUDDCXD2CV D8CWCT D5D9CPD2D8CXACCTD6 COD2D3B3BA CDD2CUD3D6D8D9D2CPD8CTD0DDB8 D7CXD2CRCT D8CWCT CRCPD6CSCXD2CPD0CXD8CXCTD7 D3CU D8CWCTD7CT CWCXCVCWCTD6B9D3D6CSCTD6 CSCTD2D3D8CPD8CXD3D2D7 CRCPD2 CQCT CTDCD4D3D2CTD2D8CXCPD0 D3D2 D8CWCT D7CXDECT D3CU D8CWCT CTD2DACXD6D3D2D1CTD2D8 BX B4D8CWCTD6CT CPD6CT BE CYBXCY D4D3D7B9 D7CXCQD0CT D7D9CQD7CTD8D7 D3CU BX CPD2CS BE BECYBXCY D4D3D7D7CXCQD0CT CRD3D1CQCXD2CPD8CXD3D2D7 D3CU D8DBD3 D7D9CRCW D7D9CQD7CTD8D7B5B8 D7D9CRCW CPD2 CPD4D4D6D3CPCRCWDBD3D9D0CS CSCTD7D8D6D3DD D8CWCT D4D3D0DDD2D3D1CXCPD0 CRD3D1D4D0CTDCCXD8DD D3CU D8CWCT CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS D4CPD6D7CXD2CV CPD0CVD3D6CXD8CWD1BA BH BTD7D7CXCVD2CXD2CV D8CWCT CXCSCTD2D8CXD8DD D6CTD0CPD8CXD3D2 CUCWCT BD BNCT BD CXBNCWCT BE BNCT BE CXBNBMBMBMCV D8D3 D8CWCT D5D9CPD2D8CXACCTD6 DBD3D9D0CS CXD2CRD3D6D6CTCRD8D0DD DDCXCTD0CS D8CWCT D7CTD8 D3CU CQD3DDD7 DBCXD8CW CP CQCPCRCZD4CPCRCZ CPD7 CP CSCTD2D3D8CPD8CXD3D2 CUD3D6 D8CWCT CUD9D0D0 D2D3D9D2 D4CWD6CPD7CTBN CPD2CS CPD7D7CXCVD2B9 CXD2CV D8CWCT CRD3D2DACTD6D7CT D6CTD0CPD8CXD3D2 B4CUD6D3D1 CTCPCRCWCTD2D8CXD8DD CXD2 D8CWCT CTD2DACXD6D3D2D1CTD2D8 D8D3 CTDACTD6DD D3D8CWCTD6 CTD2D8CXD8DD CUCWCT BD BNCT BE CXBNCWCT BD BNCT BF CXBNBMBMBMCVB5DBD3D9D0CS CXD2CRD3D6D6CTCRD8D0DD DDCXCTD0CS D8CWCT D7CTD8 D3CU CQD3DDD7 DBCXD8CW CPD2DDD8CWCXD2CV D8CWCPD8 CXD7 D2D3D8 CP CQCPCRCZD4CPCRCZBA BI CCCWCT CXCSCTD2D8CXD8DD D6CTD0CPD8CXD3D2 CUCWCT BD BNCT BD BNCT BD CXBNCWCT BE BNCT BE BNCT BE CXBNBMBMBMCVB8 DBCWCXCRCW DDCXCTD0CSD7 CP CRD3D6D6CTCRD8 CXD2D8CTD6D4D6CTD8CPD8CXD3D2 CXD2 DACTD6CQ D4CWD6CPD7CT CRD3D2CYD9D2CRD8CXD3D2B8 DBD3D9D0CS DDCXCTD0CS CPD2 CXD2CRD3D6D6CTCRD8 CSCTD2D3D8CPD8CXD3D2 CUD3D6 D8CWCT D2D3D9D2 D4CWD6CPD7CT COCVD0CPD7D7CTD7 CPD2CS CQD0D9CT D4CPD2D8D7B8B3 CRD3D2D8CPCXD2CXD2CV D3D2D0DD CTD2D8CXD8CXCTD7 DBCWCXCRCW CPD6CT CPD8 D3D2CRCT CQD3D8CW CVD0CPD7D7CTD7 CPD2CS D4CPD2D8D7BA C0D3DBCTDACTD6B8 CXCU D8CWCT D2D9D1CQCTD6 D3CU D4D3D7D7CXCQD0CT CWCXCVCWCTD6B9D3D6CSCTD6 CUD9D2CRD8CXD3D2D7 CXD7 D6CTD7D8D6CXCRD8CTCS D8D3 CP ACD2CXD8CT D7CTD8 B4D7CPDDB8 D8D3 D7D3D1CT D7D9CQD7CTD8 D3CU DBD3D6CSD7 CXD2 CP D0CTDCCXCRD3D2B5B8 CXD8 CQCTCRD3D1CTD7 D8D6CPCRD8CPCQD0CT D8D3 D7D8D3D6CT D8CWCTD1 CQDD D2CPD1CT D6CPD8CWCTD6 D8CWCPD2 CQDD CSCTD2D3D8CPD8CXD3D2 B4CXBACTBA CPD7 D7CTD8D7B5BA CBD9CRCW CUD9D2CRD8CXD3D2 CRCPD2 D8CWCTD2 CSCXD7CRCWCPD6CVCT CPD0D0 D8CWCTCXD6 ACD6D7D8B9D3D6CSCTD6 CPD6CVD9D1CTD2D8D7 CXD2 CP D7CXD2CVD0CT CSCTD6CXDACPD8CXD3D2CPD0 D7D8CTD4 D8D3 D4D6D3CSD9CRCT CP ACD6D7D8B9D3D6CSCTD6 D6CTD7D9D0D8B8 CXD2 D3D6CSCTD6 D8D3 CPDAD3CXCS CVCTD2CTD6CPD8CXD2CV D3D6 CTDACPD0D9CPD8CXD2CV CPD2DD CWCXCVCWCTD6B9D3D6CSCTD6 D4CPD6D8CXCPD0 D6CTB9 D7D9D0D8D7BA CBDDD2D8CPCRD8CXCRCPD0D0DDB8 D8CWCXD7 DBD3D9D0CS CQCT CPD2CPD0D3CVD3D9D7 D8D3 CRD3D1B9 D4D3D7CXD2CV CP D5D9CPD2D8CXACCTD6 DBCXD8CW CQD3D8CW CP D2D3D9D2 D4CWD6CPD7CT D6CTD7D8D6CXCRB9 D8D3D6 CPD2CS CP CQD3CSDD D4D6CTCSCXCRCPD8CT B4CTBACVBA CP DACTD6CQ D3D6 DACTD6CQ D4CWD6CPD7CTB5 CPD8 D8CWCT D7CPD1CT D8CXD1CTB8 D8D3 D4D6D3CSD9CRCT CPD2D3D8CWCTD6 ACD6D7D8B9D3D6CSCTD6 D4D6CTCSCXCRCPD8CT B4CTBACVBA CP DACTD6CQ D4CWD6CPD7CT D3D6 D7CTD2D8CTD2CRCTB5BA CBCXD2CRCT CP CVCTD2CTD6CPD0CXDECTCS D5D9CPD2D8CXACCTD6 CUD9D2CRD8CXD3D2 D1CTD6CTD0DD CRD3D9D2D8D7 CPD2CS CRD3D1D4CPD6CTD7 D8CWCT CRCPD6CSCXD2CPD0CXD8CXCTD7 D3CU CXD8D7 CPD6CVD9D1CTD2D8D7 CXD2 CP D0CXD2B9 CTCPD6 D8CXD1CT D3D4CTD6CPD8CXD3D2B8 D8CWCXD7 CPD2CPD0DDD7CXD7 D4D6D3DACXCSCTD7 CP D8D6CPCRD8CPCQD0CT D7CWD3D6D8CRD9D8 D8D3 D8CWCT CTDCD4D3D2CTD2D8CXCPD0 CRCPD0CRD9D0CPD8CXD3D2D7 D6CTD5D9CXD6CTCS CXD2 D8CWCT CRD3D2DACTD2D8CXD3D2CPD0 CPD2CPD0DDD7CXD7BA C6D3D8CT D8CWCPD8 D8CWCXD7 CPD2CPD0DDD7CXD7 CQDD CXD8D7CTD0CU CSD3CTD7 D2D3D8 CPCSD1CXD8 D4D6D3CSD9CRD8CXDACT D1D3CSCXACCRCPD8CXD3D2 D3CU D5D9CPD2D8CXACCTD6D7 B4CQCTCRCPD9D7CT D8CWCTCXD6 CUD9D2CRD8CXD3D2D7 CPD6CT CSD6CPDBD2 CUD6D3D1 D7D3D1CT ACD2CXD8CT D7CTD8B5 D3D6 D3CU D5D9CPD2B9 D8CXACCTCS D2D3D9D2 D4CWD6CPD7CTD7 B4CQCTCRCPD9D7CT D8CWCTDD CPD6CT D2D3 D0D3D2CVCTD6 CSCTB9 D6CXDACTCS CPD7 CP D4CPD6D8CXCPD0 D6CTD7D9D0D8B5BA CCCWCXD7 CRCPD9D7CTD7 D2D3 CSCXD7D6D9D4D8CXD3D2 D8D3 D8CWCT CPD8D8CPCRCWD1CTD2D8 D3CU D2D3D2B9CRD3D2CYD9D2CRD8CXDACT D1D3CSCXACCTD6D7B8 CQCTB9 CRCPD9D7CT D3D6CSCXD2CPD6DD D7DDD2D8CPCRD8CXCR D1D3CSCXACCTD6D7 D3CU D5D9CPD2D8CXACCTD6 CRD3D2B9 D7D8CXD8D9CTD2D8D7 CPD6CT D7CTD0CSD3D1 D4D6D3CSD9CRD8CXDACT B4CXD2 D8CWCT D7CTD2D7CT D8CWCPD8 D8CWCTCXD6 CRD3D1D4D3D7CXD8CXD3D2 CSD3CTD7 D2D3D8 DDCXCTD0CS CUD9D2CRD8CXD3D2D7 D3D9D8D7CXCSCT D7D3D1CT ACD2CXD8CT D7CTD8B5B8 CPD2CS D7DDD2D8CPCRD8CXCR D1D3CSCXACCTD6D7 D3CU C6C8 CRD3D2B9 D7D8CXD8D9CTD2D8D7 D9D7D9CPD0D0DD D3D2D0DD D1D3CSCXCUDD D8CWCT D6CTD7D8D6CXCRD8D3D6 D7CTD8 D3CU D8CWCT D5D9CPD2D8CXACCTD6 D6CPD8CWCTD6 D8CWCPD2 D8CWCT CTD2D8CXD6CT D5D9CPD2D8CXACCTCS CUD9D2CRD8CXD3D2B8 CPD2CS CRCPD2 D8CWCTD6CTCUD3D6CT D7CPCUCTD0DD CQCT D8CPCZCTD2 D8D3 CPD8D8CPCRCW CQCTD0D3DB D8CWCT D5D9CPD2D8CXACCTD6B8 D8D3 D8CWCT D9D2D5D9CPD2D8CXACCTCS C6C8BA BUD9D8 D8CWCXD7 CXD7 D2D3D8 D8D6D9CT CXD2 CRCPD7CTD7 CXD2DAD3D0DACXD2CV CRD3D2CYD9D2CRB9 D8CXD3D2BA BVD3D2CYD3CXD2CTCS D5D9CPD2D8CXACCTD6D7B8 D0CXCZCT COD7D3D1CT CQD9D8 D2D3D8 CPD0D0B8B3 CRCPD2D2D3D8 CPD0DBCPDDD7CQCT CSCTACD2CTCS D9D7CXD2CV CP D7CXD2CVD0CT D7D8CPD2CSCPD6CS D0CTDCB9 CXCRCPD0 CUD9D2CRD8CXD3D2BN CPD2CS CRD3D2CYD9D2CRD8CXD3D2D7 D3CU D5D9CPD2D8CXACCTCS D2D3D9D2 D4CWD6CPD7CTD7B8 D0CXCZCT COD3D2CT D3D6CPD2CVCT CPD2CS D3D2CT D0CTD1D3D2B3B8 CRCPD2D2D3D8 CQCT CPD4D4D0CXCTCS D8D3 D9D2D5D9CPD2D8CXACCTCS D7D9CQCRD3D2D7D8CXD8D9CTD2D8D7 B4D7DDD2D8CPCRB9 D8CXCRCPD0D0DDB8 CQCTCRCPD9D7CT D8CWCXD7 DBD3D9D0CS CUCPCXD0 D8D3 D7D9CQD7D9D1CT D8CWCT D7CTCRB9 D3D2CS D5D9CPD2D8CXACCTD6B8 CPD2CS D7CTD1CPD2D8CXCRCPD0D0DDB8 CQCTCRCPD9D7CT CXD8 CXD7 D2D3D8 D8CWCT D6CTD7D8D6CXCRD8D3D6 D7CTD8D7 DBCWCXCRCW CPD6CT CRD3D2CYD3CXD2CTCSB5BA C3CTCTD2CPD2 CPD2CS CBD8CPDACX B4BDBLBKBIB5 D1D3CSCTD0 CRD3D2CYD9D2CRD8CXD3D2D7 D3CU D5D9CPD2D8CXACCTD6D7 CPD2CS D5D9CPD2D8CXACCTCS D2D3D9D2 D4CWD6CPD7CTD7 D9D7CXD2CV D0CPD8D8CXCRCT D3D4CTD6CPD8CXD3D2D7 D3D2 CWCXCVCWCTD6B9D3D6CSCTD6 D7CTD8D7B8 CQD9D8 CPD7 D4D6CTDACXD3D9D7D0DD D7D8CPD8CTCSB8 D8CWCTD7CT CWCXCVCWCTD6B9D3D6CSCTD6 D7CTD8D7 D4D6CTCRD0D9CSCT D8D6CPCRD8CPCQD0CT CXD2D8CTD6D0CTCPDACXD2CV D3CU D7CTD1CPD2D8CXCR CXD2D8CTD6D4D6CTD8CPD8CXD3D2 DBCXD8CW D4CPD6D7CXD2CVBA CCCWCT D7D3D0D9D8CXD3D2 D4D6D3D4D3D7CTCS CWCTD6CT CXD7 D8D3 D8D6CTCPD8 CTCPCRCW D5D9CPD2B9 D8CXACCTD6 D3D6 D5D9CPD2D8CXACCTCS D2D3D9D2 D4CWD6CPD7CT CRD3D2CYD9D2CRD8CXD3D2 CPD7 CPD2 CTD0B9 D0CXD4D8CXCRCPD0 CRD3D2CYD9D2CRD8CXD3D2 D3CU D8DBD3 CRD3D1D4D0CTD8CT ACD6D7D8B9D3D6CSCTD6 D4D6CTCSB9 CXCRCPD8CTD7 B4CTBACVBA DACTD6CQ D4CWD6CPD7CTD7 D3D6 D7CTD2D8CTD2CRCTD7B5B8 CTCPCRCW D7D9CQD7D9D1B9 CXD2CV CP CSCXABCTD6CTD2D8 D5D9CPD2D8CXACCTD6 CPD2CS D2D3D9D2 D4CWD6CPD7CT D6CTD7D8D6CXCRD8D3D6 B4CXD2 D8CWCT CRCPD7CT D3CU C6C8 CRD3D2CYD9D2CRD8CXD3D2B5B8 CQD9D8 D7CWCPD6CXD2CV D3D6 CSD9B9 D4D0CXCRCPD8CXD2CV CP CRD3D1D1D3D2 CQD3CSDD D4D6CTCSCXCRCPD8CTBA CCCWCXD7 CPD2CPD0DDD7CXD7 D6CTD5D9CXD6CTD7 D1D9D0D8CXD4D0CT CRD3D1D4D3D2CTD2D8D7 D8D3 CZCTCTD4 D8D6CPCRCZ D3CU D8CWCT CSD9D4D0CXCRCPD8CTCS D1CPD8CTD6CXCPD0 CPCQD3DACT D8CWCT CRD3D2CYD9D2CRD8CXD3D2B8 CQD9D8 CPD7 D0D3D2CV CPD7 D8CWCT D2D9D1CQCTD6 D3CU CRD3D1D4D3D2CTD2D8D7 CXD7 CQD3D9D2CSCTCSB8 D8CWCT D4D3D0DDD2D3D1CXCPD0 CRD3D1D4D0CTDCCXD8DD D3CU D8CWCT D4CPD6D7CXD2CV CPD0CVD3D6CXD8CWD1 CXD7 CRD3D2D8CPCXD2CXD2CV B4CSD9D4D0CXCRCPD8CTCSB5 D3D2CT D3D6CPD2CVCT B4D9D2CSD9D4D0CXCRCPD8CTCSB5 CPD2CS D3D2CT D0CTD1D3D2 B4D9D2CSD9D4D0CXCRCPD8CTCSB5 BYCXCVD9D6CT BEBM BWD9D4D0CXCRCPD8CTCS DACTD6CQ CXD2 C6C8 CRD3D2CYD9D2CRD8CXD3D2BA D6CTD8CPCXD2CTCSBA BJ BYCXCVD9D6CT BE D7CWD3DBD7 CP CSD9D4D0CXCRCPD8CTCS DACTD6CQ D4D6CTCSCXCRCPD8CT CXD2 D8CWCT CSCTD6CXDACPD8CXD3D2 D3CU CPD2 C6C8 CRD3D2CYD9D2CRD8CXD3D2BA CCCWCT CRD3D2CYD3CXD2CTCS CRD3D2D7D8CXD8D9CTD2D8D7 B4D8CWCT D7CWCPCSCTCS D6CTCVCXD3D2D7 CXD2 D8CWCT ACCVD9D6CTB5 CPD6CT CTCPCRCW CRD3D1D4D3D7CTCS D3CU D8DBD3 CRD3D1D4D3D2CTD2D8D7BM D3D2CT CUD3D6 D8CWCT C6C8 CXD8D7CTD0CUB8 CRD3D2D8CPCXD2CXD2CV D8CWCT D5D9CPD2D8CXACCTD6 CPD2CS D8CWCT D6CTD7D8D6CXCRD8D3D6 D4D6CTCSCXCRCPD8CTB8 CPD2CS D3D2CT CUD3D6 D8CWCT DACTD6CQ DBCWCXCRCW D7D9D4D4D0CXCTD7 D8CWCT CQD3CSDD D4D6CTCSCXCRCPD8CT D3CU D8CWCT D5D9CPD2D8CXACCTD6BA CBCXD2CRCT D8CWCT CRD3D2CYD3CXD2CTCS CRD3D2D7D8CXD8D9CTD2D8D7 CQD3D8CW CRD3D6D6CTD7D4D3D2CS D8D3 CRD3D1D4D0CTD8CT D5D9CPD2D8CXB9 ACCTD6 CTDCD4D6CTD7D7CXD3D2D7 DBCXD8CW D2D3 D9D2D7CPD8CXD7ACCTCS ACD6D7D8B9D3D6CSCTD6 CPD6CVD9B9 D1CTD2D8D7B8 D8CWCTCXD6 CRCPD8CTCVD3D6CXCTD7 CPD6CT D8CWCPD8 D3CU D7CXD1D4D0CT ACD6D7D8B9D3D6CSCTD6 D4D6CTCSCXCRCPD8CTD7 B4D8CWCTDD CPD6CT CTCPCRCW CRD3D1D4D0CTD8CT DACTD6CQ D4CWD6CPD7CTD7 CXD2 CTD7D7CTD2CRCTBM COCRD3D2D8CPCXD2CXD2CV D3D2CT D3D6CPD2CVCTB3 CPD2CS COCRD3D2D8CPCXD2CXD2CV D3D2CT D0CTD1D3D2B3B5BA CCCWCT CRD3D2CYD9D2CRD8CXD3D2 D8CWCTD2 CUD3D6D1D7 CP D0CPD6CVCTD6 CRD3D2B9 D7D8CXD8D9CTD2D8 D3CU D8CWCT D7CPD1CT CUD3D6D1 B4D8CWCT D9D2D7CWCPCSCTCS D3D9D8D0CXD2CT CXD2 D8CWCT ACCVD9D6CTB5B8 DBCXD8CW CP D0D3DBCTD6 CRD3D1D4D3D2CTD2D8 CRD3D2D8CPCXD2CXD2CV D8CWCT CRD3D2CYD3CXD2CTCS CRD3D2D7D8CXD8D9CTD2D8D7B3 C6C8 CRD3D1D4D3D2CTD2D8D7 CRD3D2CRCPD8CTB9 D2CPD8CTCS CXD2 D8CWCT D9D7D9CPD0 DBCPDDB8 CPD2CS CPD2 D9D4D4CTD6 CRD3D1D4D3D2CTD2D8CXD2 DBCWCXCRCW D8CWCT CRD3D2CYD3CXD2CTCS CRD3D2D7D8CXD8D9CTD2D8D7B3 D2D3D2B9C6C8 CRD3D1D4D3B9 D2CTD2D8D7 CPD6CT CXCSCTD2D8CXACCTCS D3D6 D3DACTD6D0CPD4D4CTCSBA C1CU D8CWCT CSD9D4D0CXCRCPD8CTCS CRD3D1D4D3D2CTD2D8D7 CSD3 D2D3D8 CRD3DACTD6 D8CWCT D7CPD1CT D7D8D6CXD2CV DDCXCTD0CSB8 D8CWCT CRD3D2CYD9D2CRD8CXD3D2 CSD3CTD7 D2D3D8 CPD4D4D0DDBA C6D3D8CT D8CWCPD8B8 D7CXD2CRCT D8CWCTDD CPD6CT D3D2D0DD CPD4D4D0CXCTCS D8D3 D3D6CSCXD2CPD6DD ACD6D7D8B9D3D6CSCTD6 D4D6CTCSCXCRCPD8CTD7 B4CTBACVBA D7CTD2D8CTD2CRCTD7 D3D6 DACTD6CQ D4CWD6CPD7CTD7B5 CXD2 D8CWCXD7 CPD2CPD0DDD7CXD7B8 CRD3D2CYD9D2CRD8CXD3D2D7 CRCPD2 D2D3DB D7CPCUCTD0DD CQCT CPD7B9 D7CXCVD2CTCS D8CWCT CUCPD1CXD0CXCPD6 D8D6D9D8CWB9CUD9D2CRD8CXD3D2CPD0 CSCTD2D3D8CPD8CXD3D2D7 CXD2 CTDACTD6DD CRCPD7CTBA BK BTD0D7D3B8 D7CXD2CRCT D8CWCT D6CTD7D9D0D8CXD2CV CRD3D2D7D8CXD8D9CTD2D8 CWCPD7 D8CWCT D7CPD1CT D2D9D1CQCTD6 D3CU CRD3D1D4D3D2CTD2D8D7 CPD7 D8CWCT CRD3D2CYD3CXD2CTCS CRD3D2D7D8CXD8D9CTD2D8D7B8 D8CWCTD6CT CXD7 D2D3D8CWCXD2CV D8D3 D4D6CTDACTD2D8 CXD8D7 D9D7CT CPD7 CPD2 CPD6CVD9D1CTD2D8 CXD2 D7D9CQD7CTD5D9CTD2D8 CRD3D2CYD9D2CRD8CXD3D2 D3D4CTD6CPD8CXD3D2D7BA BT D7CPD1D4D0CT D1D9D0D8CXB9CRD3D1D4D3D2CTD2D8 CPD2CPD0DDD7CXD7 CUD3D6 D5D9CPD2D8CXACCTD6D7 CXD7 D7CWD3DBD2 CQCTD0D3DBB8 CPD0D0D3DBCXD2CV D1CPD8CTD6CXCPD0 D8D3 CQCT CSD9D4D0CXCRCPD8CTCS CQD3D8CW D8D3 D8CWCT D0CTCUD8 CPD2CS D8D3 D8CWCT D6CXCVCWD8 D3CU CP CRD3D2CYD3CXD2CTCS C6C8BM D7D3D1CTB8CPD0D0B8D2D3B8CTD8CRBA BM CGD2C6C8 D5 A1 C6C8 D5 D2C6C8 D5 A1C6C8 D5 BPC6C8 AF CGBPC6C8 D5 A1 C6C8 D5 D2C6C8 D5 A1C6C8 D5 BPC6C8 AF CCCWCT D0CTDCCXCRCPD0 CTD2D8D6DD CUD3D6 CP D5D9CPD2D8CXACCTD6 CRCPD2 CQCT D7D4D0CXD8 CXD2 D8CWCXD7 BJ BWCPCWD0 CPD2CS C5CRBVD3D6CS B4BDBLBKBFB5 D4D6D3D4D3D7CT CP D7CXD1CXD0CPD6 CSD9D4D0CXCRCPD8CXD3D2 D1CTCRCWCPD2CXD7D1 D8D3 D4D6D3CSD9CRCT CPD4D4D6D3D4D6CXCPD8CT D7CTD1CPD2D8CXCR D6CTD4D6CTD7CTD2D8CPD8CXD3D2D7 CUD3D6 C6C8 CPD2CS D3D8CWCTD6 CRD3D2CYD9D2CRD8CXD3D2D7B8 CQD9D8 CUD3D6 CSCXABCTD6CTD2D8 D6CTCPD7D3D2D7BA BK CTBACVBA CUD3D6 D8CWCT DBD3D6CS COCPD2CSB3BM CUCWBMBMBMCCCACDBXBNBMBMBMCCCACDBXBNBMBMBMCCCACDBXCXBN CWBMBMCCCACDBXBNBMBMBYBTC4CBBXBNBMBMBYBTC4CBBXCXBN CWBMBMBYBTC4CBBXBNBMBMCCCACDBXBNBMBMBYBTC4CBBXCXBN CWBMBMBYBTC4CBBXBNBMBMBYBTC4CBBXBNBMBMBYBTC4CBBXCXCV DBCPDDCXD2D8D3CPD2D9D1CQCTD6 D3CU CRD3D1D4D3D2CTD2D8D7B8 D8CWCT D0CPD7D8 B4D3D6 D0D3DBB9 CTD7D8B5 D3CU DBCWCXCRCW CXD7 D2D3D8 CSD9D4D0CXCRCPD8CTCS CXD2 CRD3D2CYD9D2CRD8CXD3D2 DBCWCXD0CT D3D8CWCTD6D7 D1CPDD D3D6 D1CPDD D2D3D8 CQCTBA CCCWCTD7CT CXD2CRD0D9CSCT CP CRD3D1B9 D4D3D2CTD2D8 CUD3D6 D8CWCT D5D9CPD2D8CXACCTD6 C6C8 D5 BPC6C8 AF B4DBCWCXCRCW DBCXD0D0 D9D0D8CXB9 D1CPD8CTD0DD CPD0D7D3 CRD3D2D8CPCXD2 CP D2D3D9D2 D4CWD6CPD7CT D6CTD7D8D6CXCRD8D3D6 D3CU CRCPD8CTB9 CVD3D6DD C6C8 AF B5B8 CP CRD3D1D4D3D2CTD2D8 CUD3D6 D6CTD7D8D6CXCRD8D3D6 C8C8D7 CPD2CS D6CTD0CPB9 D8CXDACT CRD0CPD9D7CTD7 D3CU CRCPD8CTCVD3D6DD C6C8 D5 D2C6C8 D5 D8CWCPD8 CPD6CT CPD8D8CPCRCWCTCS CPCQD3DACT D8CWCT D5D9CPD2D8CXACCTD6 CPD2CS CSD9D4D0CXCRCPD8CTCS CXD2 D8CWCT CRD3D2CYD9D2CRB9 D8CXD3D2B8 CPD2CS CP CRD3D1D4D3D2CTD2D8 CUD3D6 D8CWCT CQD3CSDD B4CP DACTD6CQ D3D6 DACTD6CQ D4CWD6CPD7CT D3D6 D3D8CWCTD6 D4D6CTCSCXCRCPD8CTB5 D3CU CRCPD8CTCVD3D6DD CGD2C6C8 D5 D3D6 CGBPC6C8 D5 BA CCCWCT D7D9CQD7CRD6CXD4D8 D5 D7D4CTCRCXACCTD7 D3D2CT D3CU CP ACD2CXD8CT D7CTD8 D3CU D5D9CPD2D8CXACCTD6D7B8 CPD2CS D8CWCT D7D9CQD7CRD6CXD4D8 AF CXD2CSCXCRCPD8CTD7 CPD2 D9D2D5D9CPD2D8CXACCTCS C6C8BA CCCWCT CSCTCSD9CRD8CXDACT D4CPD6D7CTD6 D4D6CTD7CTD2D8CTCS CXD2 CBCTCRD8CXD3D2 BE CRCPD2 D2D3DB CQCT CTDCD8CTD2CSCTCS CQDD CXD2CRD3D6D4D3D6CPD8CXD2CV D7CTD5D9CTD2CRCTD7 D3CU D6CTCRB9 D3CVD2CXDECTCS CPD2CS D9D2D6CTCRD3CVD2CXDECTCS CRD3D1D4D3D2CTD2D8D7 CXD2D8D3 D8CWCT CRD3D2B9 D7D8CXD8D9CTD2D8 CRCWCPD6D8 CXD8CTD1D7BA BTD7 CRD3D2D7D8CXD8D9CTD2D8D7 CPD6CT CRD3D1B9 D4D3D7CTCSB8 CRD3D1D4D3D2CTD2D8D7 CPD6CT D7CWCXCUD8CTCS CUD6D3D1 D8CWCT D9D2D6CTCRD3CVB9 D2CXDECTCS D7CTD5D9CTD2CRCT AD BD A1A1A1AD CR D8D3 D8CWCT D6CTCRD3CVD2CXDECTCS D7CTD5D9CTD2CRCT CWCX BD BNCY BD BNAD BD CXA1A1A1CWCX CR BNCY CR BNAD CR CXB8 D9D2D8CXD0 D8CWCT D9D2D6CTCRD3CVD2CXDECTCS D7CTB9 D5D9CTD2CRCT CXD7 CTD1D4D8DDBA CCCWCT CTDCD8CTD2CSCTCS D4CPD6D7CTD6 CXD7 CSCTACD2CTCS DBCXD8CWBM AF CRCWCPD6D8 CXD8CTD1D7 D3CU D8CWCT CUD3D6D1 CJCXBNCYBNA1BNA6CLB8 DBCWCTD6CT A1 CXD7 CP D7CTD5D9CTD2CRCT D3CU D9D2D6CTCRD3CVD2CXDECTCS CRD3D1D4D3D2CTD2D8D7 ADB8A6CXD7 CP D7CTD5D9CTD2CRCT D3CU D6CTCRD3CVD2CXDECTCS CRD3D1D4D3D2CTD2D8D7 CWCPBNCQBNADCXB8 CPD2CS CXBNCYBNCZBNCPBNCQBNCR CPD6CT CXD2CSCXCRCTD7 CXD2 D8CWCT CXD2D4D9D8BA BXCPCRCW CXD8CTD1 CJCXBNCYBNA1A1ADBNCWCX BD BNCY BD BNAD BD CXA1A1A1CWCX CR BNCY CR BNAD CR CXCL CXD2CSCXCRCPD8CTD7 D8CWCPD8 D8CWCT D7D4CPD2 CUD6D3D1 CX D8D3 CY CXD2 D8CWCT CXD2D4D9D8 CRCPD2 CQCT CRCWCPD6CPCRD8CTD6CXDECTCS CQDD D8CWCT CRCPD8CTCVD3D6CXCTD7 AD BD D8CWD6D3D9CVCW AD CR CPD8 D4D3D7CXD8CXD3D2D7CX BD D8D3CY BD D8CWD6D3D9CVCWCX CR D8D3CY CR D6CTD7D4CTCRD8CXDACTD0DDB8D7D3 D8CWCPD8 CXCU D8CWCTD7CT D7D4CPD2D7 CPD6CT CRD3D2CRCPD8CTD2CPD8CTCS CXD2 DBCWCPD8CTDACTD6 D3D6CSCTD6 D8CWCTDD D3CRCRD9D6 CXD2 D8CWCT CXD2D4D9D8 D7D8D6CXD2CVB8 D8CWCTDD CUD3D6D1 CP CVD6CPD1D1CPD8CXCRCPD0 CRD3D2D7D8CXD8D9CTD2D8 D3CU CRCPD8CTCVD3D6DD AD DBCXD8CW D9D2D6CTCRD3CVD2CXDECTCS CRD3D1D4D3D2CTD2D8D7 A1BA AF CP D0CTDCCXCRCPD0 CXD8CTD1 CJCXBNCYBNADBNCWCXBNCYBNADCXCL CUD3D6 CTDACTD6DD D6D9D0CT AD AX DB BE C8 CXCU DB D3CRCRD9D6D7 CQCTD8DBCTCTD2 D4D3D7CXD8CXD3D2D7 CX CPD2CS CY CXD2 D8CWCT CXD2D4D9D8BN AF CP D7CTD8 D3CU D6D9D0CTD7 CUD3D6 CPD0D0 CXBNCYBNCZBNCPBNCQBNCR BE C1 D2 BC CPD7 CQCTD0D3DBBA CCDBD3 D6D9D0CTD7 D8D3 CXD2DAD3CZCT D0CTCUD8 CPD2CS D6CXCVCWD8 CUD9D2CRD8CXD3D2 CPD4B9 D4D0CXCRCPD8CXD3D2 D8D3 CPD2 CTDCCXD7D8CXD2CV CRD3D1D4D3D2CTD2D8BM CJCXBNCZBNADBPBNCWCXBNCZBNADBPCXCLCJCZBNCYBNA1A1BNCWCZBNCQBNBPAYCXA1A6CL CJCXBNCYBNA1A1ADBNCWCXBNCQBNADBPAYCXA1A6CL ADAXADBP BEC8B8 CJCZBNCYBNADD2BNCWCZBNCYBNADD2CXCLCJCXBNCZBNA1A1BNCWCPBNCZBND2AYCXA1A6CL CJCXBNCYBNA1A1ADBNCWCPBNCYBNADD2AYCXA1A6CL ADAXADD2BEC8B8 CCDBD3 D6D9D0CTD7 D8D3 CXD2DAD3CZCT D0CTCUD8 CPD2CS D6CXCVCWD8 CUD9D2CRD8CXD3D2 CPD4B9 D4D0CXCRCPD8CXD3D2 D8D3 CP CUD6CTD7CW CRD3D1D4D3D2CTD2D8BM CJCXBNCZBNADBPBNCWCXBNCZBNADBPCXCLCJCZBNCYBNA1A1ADBPA1BNA6CL CJCXBNCYBNA1A1ADBNCWCXBNCZBNADBPCXA1A6CL ADAXADBP BEC8B8 CJCZBNCYBNADD2BNCWCZBNCYBNADD2CXCLCJCXBNCZBNA1A1ADD2A1BNA6CL CJCXBNCYBNA1A1ADBNCWCZBNCYBNADD2CXA1A6CL ADAXADD2BEC8B8 CCDBD3 D6D9D0CTD7 D8D3 CSCXD7CRCWCPD6CVCT CTD1D4D8DD CRD3D1D4D3D2CTD2D8D7BM CJCXBNCYBNA1A1ADBPA1BNA6CL CJCXBNCYBNA1A1ADBNA6CL CJCXBNCYBNA1A1ADD2A1BNA6CL CJCXBNCYBNA1A1ADBNA6CL CCCWD6CTCT D6D9D0CTD7 D8D3 D7CZCXD4 CRD3D2CYD9D2CRD8CXD3D2D7B8 CQDD CPCSCSCXD2CV CP CVCPD4 CQCTD8DBCTCTD2 D8CWCT CRD3D1D4D3D2CTD2D8D7 CXD2 CP CRD3D2D7D8CXD8D9CTD2D8 B4D8CWCT ACD6D7D8 D6D9D0CT CRD3D2D7D9D1CTD7 D8CWCT CRD3D2CYD9D2CRD8CXD3D2 D8D3 CRD6CTB9 CPD8CT CP D4CPD6D8CXCPD0 D6CTD7D9D0D8 D3CU CRCPD8CTCVD3D6DD BVD3D2CY BC  B8 CPD2CS D8CWCT D0CPD8D8CTD6 D8DBD3 D9D7CT D8CWCXD7 D8D3 D7CZCXD4 D8CWCT D3D4D4D3D7CXD2CV C6C8B5BM CJCZBNCYBNA1A1BNA6CL CJCXBNCYBNA1A1BVD3D2CY BC  BNA6CL CJCXBNCZBNBVD3D2CYBNCWCXBNCZBNBVD3D2CYCXCL CJCZBNCYBNA1A1BVD3D2CY BC  BNA6CL CJCXBNCYBNA1A1BNA6CL CJCXBNCZBNA1A1BN CL CJCXBNCZBNA1A1BNA6CL CJCXBNCYBNA1A1BNA6CL CJCZBNCYBNA1A1BVD3D2CY BC  BN CL CCDBD3 D6D9D0CTD7 D8D3 D6CTCPD7D7CTD1CQD0CT CSCXD7CRD3D2D8CXD2D9D3D9D7 CRD3D2B9 D7D8CXD8D9CTD2D8D7 B4CPCVCPCXD2B8 D9D7CXD2CV CP D4CPD6D8CXCPD0 D6CTD7D9D0D8 BVD3D2CY BC  D8D3 D6CTCSD9CRCT D8CWCT D2D9D1CQCTD6 D3CU D6CPD2CVCXD2CV DACPD6CXCPCQD0CTD7B5BM CJCPBNCRBNBVD3D2CYBNCWCPBNCRBNBVD3D2CYCXCLCJCXBNCYBNADBNA6A1CWCRBNCQBNCXCL CJCXBNCYBNADBNA6A1CWCPBNCQBNBVD3D2CY BC  CXCL CJCXBNCYBNADBNA6A1CWCRBNCQBNBVD3D2CY BC  CXCLCJCXBNCYBNADBNA6A1CWCPBNCRBNCXCL CJCXBNCYBNADBNA6A1CWCPBNCQBNCXCL CCDBD3 D6D9D0CTD7 D8D3 CRD3D1CQCXD2CT CPCSCYCPCRCTD2D8 CRD3D1D4D3D2CTD2D8D7BM CJCXBNCYBNADBNA6A1CWCPBNCRBNBPAYCXA1CWCRBNCQBNAYCXCL CJCXBNCYBNADBNA6A1CWCPBNCQBNCXCL CJCXBNCYBNADBNA6A1CWCRBNCQBND2AYCXA1CWCPBNCRBNAYCXCL CJCXBNCYBNADBNA6A1CWCPBNCQBNCXCL BTD2CS D3D2CT D6D9D0CT D8D3 CPD4D4D0DD D5D9CPD2D8CXACCTD6 CUD9D2CRD8CXD3D2D7BM CJCXBNCYBNADBNA6A1CWCPBNCQBN D5 CXCL CJCXBNCYBNADBNA6A1CWCPBNCQBN AF CXCL CCCWCT D4CPD6D7CXD2CV CPD2CS D7CRD3D6CXD2CV CUD9D2CRD8CXD3D2D7 D6CTD1CPCXD2 CXCSCTD2D8CXB9 CRCPD0 D8D3 D8CWD3D7CT CXD2 CBCTCRD8CXD3D2 BEB8 CQD9D8 CPD2 CPCSCSCXD8CXD3D2CPD0 CZ BP BD CRCPD7CT CRD3D2D8CPCXD2CXD2CV CP D1D3CSCXACCTCS D4D6D3CYCTCRD8CXD3D2 CUD9D2CRD8CXD3D2 AP CXD7 D2D3DB CPCSCSCTCS D8D3 D8CWCT CXD2D8CTD6D4D6CTD8CPD8CXD3D2 CUD9D2CRD8CXD3D2B8 CXD2 D3D6CSCTD6 D8D3 D1CPCZCT D8CWCT CSCTD2D3D8CPD8CXD3D2D7 D3CU D5D9CPD2D8CXACCTCS CRD3D2D7D8CXD8D9CTD2D8D7 CSCTD4CTD2CS D3D2 D8CWCTCXD6 CPD7D7D3CRCXCPD8CTCS D5D9CPD2D8CXACCTD6D7BM BWB4DCB5BP CJ CP BD BMBMBMCP CZ D7BMD8BM CP BD BMBMBMCP CZ DC BK BQ BQ BQ BQ BQ BQ BQ BO BQ BQ BQ BQ BQ BQ BQ BM CAB4DCB5 CXCU CZ BPBC AP D5 BWB4CP BD B5 CXCU CZ BP BD CPD2CS CP BD DC BP CJBMBMBMCWBMBMBM D5 CXCL CJBMBMBMCWBMBMBM AF CXCL CZ D3D2 CXBPBD BWB4CP CX B5 D3D8CWCTD6DBCXD7CT CCCWCT D1D3CSCXACCTCS D4D6D3CYCTCRD8CXD3D2 CUD9D2CRD8CXD3D2 CTDACPD0D9CPD8CTD7 CP D5D9CPD2B9 D8CXACCTD6 CUD9D2CRD8CXD3D2 D5 D3D2 D7D3D1CT CPD6CVD9D1CTD2D8 CSCTD2D3D8CPD8CXD3D2 BTB8 CRD3D1D4CPD6CXD2CV D8CWCT CRCPD6CSCXD2CPD0CXD8DD D3CU D8CWCT CXD1CPCVCT D3CU D8CWCT D6CTB9 D7D8D6CXCRD8D3D6 D7CTD8 CXD2 BT DBCXD8CW D8CWCT D8CWCT CRCPD6CSCXD2CPD0CXD8DD D3CU CXD1CPCVCT D3CU D8CWCT CXD2D8CTD6D7CTCRD8CTCS D6CTD7D8D6CXCRD8D3D6 CPD2CS CQD3CSDD D7CTD8D7 CXD2 BTBM BL AP D5 BT BP CUCWCT BE BMBMBMCT CP BND8CXCYCW BNCT BE BMBMBMCT CP BN CXBEBTBN D8 BP D5B4CYCACYBNCYCBCYB5 CA BP BTD3D2CUCW BNCT BE BMBMBMCT CP BN CXCVBN CB BP BTD3D2CUCW BNCT BE BMBMBMCT CP BNCCCACDBXCXCVCV CCCWCXD7 CPD0CVD3D6CXD8CWD1 D4CPD6D7CTD7 CP CRCPD8CTCVD3D6CXCPD0 CVD6CPD1D1CPD6 CXD2 D8CWCT D9D7D9CPD0 DBCPDD DF CRD3D2D7D8CXD8D9CTD2D8D7 CPD6CT CXD2CXD8CXCPD0D0DD CPCSCSCTCS D8D3 D8CWCT CRCWCPD6D8 CPD7 D7CXD2CVD0CT CRD3D1D4D3D2CTD2D8D7 CRD3DACTD6CXD2CV CP CRCTD6D8CPCXD2 DDCXCTD0CS CXD2 D8CWCT CXD2D4D9D8 D7D8D6CXD2CV B4D8CWCT CXD2CSCXCRCTD7 D3CU D8CWCT CRD3D1D4D3D2CTD2D8 CPD6CT D8CWCT D7CPD1CT CPD7 D8CWCT CXD2CSCXCRCTD7 D3CU D8CWCT CRD3D2D7D8CXD8D9CTD2D8 CXD8D7CTD0CUB5B8 CPD2CS D8CWCTDD CPD6CT CRD3D1CQCXD2CTCS CQDD CRD3D2CRCPD8CTD2CPD8CXD2CV D8CWCT DDCXCTD0CSD7 D3CU D7D1CPD0D0CTD6 CRD3D2D7D8CXD8D9CTD2D8D7 D8D3 D1CPCZCT D0CPD6CVCTD6 D3D2CTD7 DF D9D2D8CXD0 CP CRD3D2CYD9D2CRD8CXD3D2 CXD7 CTD2CRD3D9D2D8CTD6CTCSBA CFCWCTD2 CP CRD3D2CYD9D2CRD8CXD3D2 CXD7 BL BYD3D0D0D3DBCXD2CV C3CTCTD2CPD2 CPD2CS CBD8CPDACX B4BDBLBKBIB5BA BC BD BE BF BG CRD3D2D8CPCXD2CXD2CV D3D2CT D3D6CPD2CVCT CPD2CS D3D2CT D0CTD1D3D2 CJBCBNBDBNCBD2C6C8 D5 BPC6C8 D5 BCBN CJBDBNBEBNCGD2C6C8 BL A1 C6C8 BL D2C6C8 BL A1 C6C8 BL BN CJBEBNBFBNBVD3D2CYBN CJBFBNBGBNCGD2C6C8 BL A1 C6C8 BL D2C6C8 BL A1 C6C8 BL BN CWBCBNBDBNCBD2C6C8 D5 BPC6C8 D5 BCCXCL CWBDBNBEBNC6C8 BL CXCL CUD3 BD BND3 BE BND3 BF BND3 BG CV CWBEBNBFBNBVD3D2CYCXCL CWBFBNBGBNC6C8 BL CXCL CUD0 BD BND0 BE BND0 BF CV CUCWD3 BD BNDC BD CXBNCWD0 BE BNDC BD CXBNCWD0 BF BNDC BF CXCV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BDB5 CJBDBNBGBNCGD2C6C8 BL A1 C6C8 BL D2C6C8 BL A1 C6C8 BL BNCWBDBNBEBNC6C8 BL CXCL CUD3 BD BND3 BE BND3 BF BND3 BG CV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BEB5 CJBDBNBGBNCGD2C6C8 BL A1 C6C8 BL D2C6C8 BL A1 C6C8 BL BNCWBFBNBGBNC6C8 BL CXCL CUD0 BD BND0 BE BND0 BF CV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BFB5 CJBDBNBGBNCGD2C6C8 BL A1 C6C8 BL BNCWBDBNBEBNC6C8 BL CXCL CUD3 BD BND3 BE BND3 BF BND3 BG CV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BGB5 CJBDBNBGBNCGD2C6C8 BL A1 C6C8 BL BNCWBFBNBGBNC6C8 BL CXCL CUD0 BD BND0 BE BND0 BF CV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BHB5 CJBCBNBGBNCBD2C6C8 D5 BNCWBCBNBDBNCBD2C6C8 D5 BPC6C8 BL CXA1CWBDBNBEBNC6C8 BL CXCL CUCWD3 BD BNDC BD CXCV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BIB5 CJBCBNBGBNCBD2C6C8 D5 BNCWBCBNBDBNCBD2C6C8 D5 BPC6C8 BL CXA1CWBFBNBGBNC6C8 BL CXCL CUCWD0 BE BNDC BD CXBNCWD0 BF BNDC BF CXCV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BJB5 CJBCBNBGBNCBD2C6C8 D5 BNCWBCBNBDBNCBD2C6C8 D5 BPC6C8 AF CXA1CWBDBNBEBNC6C8 AF CXCL CUDC BD CV BABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABABA B4BKB5 CJBCBNBGBNCBD2C6C8 D5 BNCWBCBNBDBNCBD2C6C8 D5 BPC6C8 AF CXA1CWBFBNBGBNC6C8 AF CXCL CUDC BD BNDC BF CV B4BLB5 CJBCBNBGBNCBD2C6C8 D5 BNCWBCBNBDBNCBD2C6C8 D5 BPC6C8 AF CXA1CWBDBNBGBNC6C8 AF CXCL CUDC BD CV B4BDBCB5 CJBCBNBGBNCBD2C6C8 D5 BNCWBCBNBGBNCBD2C6C8 D5 CXCL CUDC BD CV BYCXCVD9D6CT BFBM CBCPD1D4D0CT CSCTD6CXDACPD8CXD3D2 D3CU CRD3D2CYD3CXD2CTCS C6C8BA CTD2CRD3D9D2D8CTD6CTCS CXD1D1CTCSCXCPD8CTD0DD D8D3 D8CWCT D0CTCUD8 D3D6 D6CXCVCWD8 D3CU CP D6CTCRB9 D3CVD2CXDECTCS CRD3D2D7D8CXD8D9CTD2D8 CRD3D2D7D8CXD8D9CTD2D8 DCB8 CPD2CS CPD2D3D8CWCTD6 CRD3D2B9 D7D8CXD8D9CTD2D8 D3CU D8CWCT D7CPD1CT CRCPD8CTCVD3D6DD CXD7 CUD3D9D2CS CXD1D1CTCSCXCPD8CTD0DD CQCTDDD3D2CS D8CWCPD8 CRD3D2CYD9D2CRD8CXD3D2B8 D8CWCT D4CPD6D7CTD6 CRD6CTCPD8CTD7 CP D2CTDB CRD3D2D7D8CXD8D9CTD2D8 D8CWCPD8 CWCPD7 D8CWCT CRD3D1CQCXD2CTCS DDCXCTD0CS D3CU CQD3D8CW CRD3D2B9 D7D8CXD8D9CTD2D8D7B8 CQD9D8 CRD3D4CXCTD7 DCB3D7 CRD3D1D4D3D2CTD2D8 DDCXCTD0CS B4D8CWCT D7D8D6CXD2CV CXD2CSCXCRCTD7 D3CU DCB3D7 D3D6CXCVCXD2CPD0 CRD3D1D4D3D2CTD2D8D7B5 DBCXD8CW D2D3 CRCWCPD2CVCTBA CCCWCXD7 CWCPD7 D8CWCT CTABCTCRD8 D3CU CRD6CTCPD8CXD2CV D8DBD3 D2CTDB CRD3D2D7D8CXD8D9CTD2D8D7 CTDACTD6DD D8CXD1CT D8DBD3 CTDCCXD7D8CXD2CV CRD3D2D7D8CXD8D9CTD2D8D7 CPD6CT CRD3D2CYD3CXD2CTCSBM CTCPCRCW DBCXD8CW CP CSCXABCTD6CTD2D8 CRD3D1D4D3D2CTD2D8 DDCXCTD0CSB8 CQD9D8 CQD3D8CW DBCXD8CW D8CWCT D7CPD1CT B4CRD3D1CQCXD2CTCSB5 CRD3D2D7D8CXD8D9CTD2D8 DDCXCTD0CSBA CCCWCTD7CT D2CTDB CSCXD7CRD3D2D8CXD2D9D3D9D7 CRD3D2D7D8CXD8D9CTD2D8D7 B4DBCXD8CW CRD3D1D4D3D2CTD2D8 DDCXCTD0CSD7 D8CWCPD8 CSD3 D2D3D8 CTDCCWCPD9D7D8 D8CWCTCXD6 CRD3D2D7D8CXD8D9CTD2D8 DDCXCTD0CSD7B5 CPD6CT D7D8CXD0D0 D8D6CTCPD8CTCS CPD7 D3D6CSCXD2CPD6DDCRD3D2D7D8CXD8D9CTD2D8D7 CQDD D8CWCT D4CPD6D7CTD6B8 DBCWCXCRCW CRD3D1CQCXD2CTD7 D8CWCTD1 DBCXD8CW CPD6CVD9D1CTD2D8D7 CPD2CS D1D3CSCXACCTD6D7 D9D2D8CXD0 CPD0D0 D3CU D8CWCTCXD6 CPD6CVD9D1CTD2D8 D4D3D7CXD8CXD3D2D7 CWCPDACT CQCTCTD2 D7D9CRCRCTD7D7B9 CUD9D0D0DD CSCXD7CRCWCPD6CVCTCSB8 CPD8 DBCWCXCRCW D4D3CXD2D8 D4CPCXD6D7 D3CU CSCXD7CRD3D2D8CXD2D9B9 D3D9D7 CRD3D2D7D8CXD8D9CTD2D8D7 DBCXD8CW D8CWCT D7CPD1CT CRD3D2D7D8CXD8D9CTD2D8 DDCXCTD0CS CRCPD2 CQCT D6CTCPD7D7CTD1CQD0CTCS CXD2D8D3 DBCWD3D0CT DF D3D6 CPD8 D0CTCPD7D8 D0CTD7D7 CSCXD7CRD3D2B9 D8CXD2D9D3D9D7 DF CRD3D2D7D8CXD8D9CTD2D8D7 CPCVCPCXD2BA BT D7CPD1D4D0CT CSCTD6CXDACPD8CXD3D2 CUD3D6 D8CWCT DACTD6CQ D4CWD6CPD7CT COCRD3D2B9 D8CPCXD2CXD2CV D3D2CT D3D6CPD2CVCT CPD2CS D3D2CT D0CTD1D3D2B8B3 CXD2DAD3D0DACXD2CV CRD3D2B9 CYD9D2CRD8CXD3D2 D3CU CTDCCXD7D8CTD2D8CXCPD0D0DD D5D9CPD2D8CXACCTCS D2D3D9D2 D4CWD6CPD7CTD7B8 CXD7 D7CWD3DBD2 CXD2 BYCXCVD9D6CT BFB8 D9D7CXD2CV D8CWCT CPCQD3DACT D4CPD6D7CT D6D9D0CTD7 CPD2CS D8CWCT D0CTDCCXCRCPD0CXDECTCS CVD6CPD1D1CPD6BM CRD3D2D8CPCXD2CXD2CV BM CBD2C6C8 D5 BPC6C8 D5 BC D3D2CT BM CGD2C6C8 D5 A1 C6C8 D5 D2C6C8 D5 A1 C6C8 D5 BPC6C8 AF CGBPC6C8 D5 A1 C6C8 D5 D2C6C8 D5 A1C6C8 D5 BPC6C8 AF D3D6CPD2CVCTB8 D0CTD1D3D2 BM C6C8 AF CPD2CS BM BVD3D2CY BYCXD6D7D8 D8CWCT D4CPD6D7CTD6 CPD4D4D0CXCTD7 D8CWCT D7CZCXD4 CRD3D2CYD9D2CRD8CXD3D2 D6D9D0CTD7 D8D3 D3CQD8CPCXD2 D8CWCT CSCXD7CRD3D2D8CXD2D9D3D9D7 CRD3D2D7D8CXD8D9CTD2D8D7 D7CWD3DBD2 CPCUB9 D8CTD6 D7D8CTD4D7 B4BDB5 CPD2CS B4BEB5B8 CPD2CS CP CRD3D1D4D3D2CTD2D8 CXD7 CSCXD7CRCWCPD6CVCTCS CUD6D3D1 CTCPCRCW D3CU D8CWCT D6CTD7D9D0D8CXD2CV CRD3D2D7D8CXD8D9CTD2D8D7 D9D7CXD2CV D8CWCT CTD1D4D8DD CRD3D1D4D3D2CTD2D8 D6D9D0CT CXD2 D7D8CTD4D7 B4BFB5 CPD2CS B4BGB5BA CCCWCT CRD3D2D7D8CXD8D9CTD2D8D7 D6CTD7D9D0D8CXD2CV CUD6D3D1 B4BFB5 CPD2CS B4BGB5 CPD6CT D8CWCTD2 CRD3D1B9 D4D3D7CTCS DBCXD8CW D8CWCT DACTD6CQ CRD3D2D7D8CXD8D9CTD2D8 CUD3D6 COCRD3D2D8CPCXD2CXD2CVB3 CXD2 D7D8CTD4D7 B4BHB5 CPD2CS B4BIB5B8 D9D7CXD2CV D8CWCT D0CTCUD8 CPD8D8CPCRCWD1CTD2D8 D6D9D0CT CUD3D6 CUD6CTD7CW CRD3D1D4D3D2CTD2D8D7BA CCCWCT D5D9CPD2D8CXACCTD6D7 CPD6CT D8CWCTD2 CPD4D4D0CXCTCS CXD2 D7D8CTD4D7 B4BJB5 CPD2CS B4BKB5B8 CPD2CS D8CWCT D6CTD7D9D0D8CXD2CV CRD3D2D7D8CXD8D9CTD2D8D7 CPD6CT D6CTCPD7D7CTD1CQD0CTCS D9D7CXD2CV D8CWCT CRD3D2CYD9D2CRD8CXD3D2 D6D9D0CTD7 CXD2 D7D8CTD4 B4BLB5BA CCCWCT CPCSCYCPCRCTD2D8 CRD3D1D4D3D2CTD2D8D7 CXD2 D8CWCT CRD3D2D7D8CXD8D9CTD2D8 D6CTD7D9D0D8CXD2CV CUD6D3D1 D7D8CTD4 B4BLB5 CPD6CT D8CWCTD2 D1CTD6CVCTCS D9D7CXD2CV D8CWCT CRD3D1CQCXD2CPD8CXD3D2 D6D9D0CT CXD2 D7D8CTD4 B4BDBCB5B8 D4D6D3CSD9CRCXD2CV CP CRD3D1D4D0CTD8CT CVCPD4D0CTD7D7 CRD3D2D7D8CXD8D9CTD2D8 CUD3D6 D8CWCT CTD2D8CXD6CT CXD2D4D9D8BA CBCXD2CRCT D8CWCT D4CPD6D7CTD6 D6D9D0CTD7 CPD6CT ACDCCTCSB8 CPD2CS D8CWCT D2D9D1CQCTD6 D3CU CRD3D1D4D3D2CTD2D8D7 CXD2 CPD2DDCRCWCPD6D8 CRD3D2D7D8CXD8D9CTD2D8 CXD7 CQD3D9D2CSCTCS CQDD D8CWCT D1CPDCCXD1D9D1 D2D9D1CQCTD6 D3CU CRD3D1D4D3D2CTD2D8D7 CXD2 CP CRCPD8CTCVD3D6DD B4CXD2CPD7D1D9CRCW CPD7 D8CWCT D6D9D0CTD7 CRCPD2 D3D2D0DD CPCSCS CP CRD3D1D4D3D2CTD2D8 D8D3 D8CWCT D6CTCRD3CVD2CXDECTCS D0CXD7D8 CQDD D7D9CQD8D6CPCRD8CXD2CV D3D2CT CUD6D3D1 D8CWCT D9D2D6CTCRD3CVD2CXDECTCS D0CXD7D8B5B8 D8CWCT CPD0CVD3D6CXD8CWD1 D1D9D7D8 D6D9D2 CXD2 D4D3D0DDB9 D2D3D1CXCPD0 D7D4CPCRCT CPD2CS D8CXD1CT D3D2 D8CWCT D0CTD2CVD8CW D3CU D8CWCT CXD2D4D9D8 D7CTD2D8CTD2CRCTBA CBCXD2CRCT D8CWCT CRCPD6CSCXD2CPD0CXD8DD D3CU CTCPCRCW CRD3D2D7D8CXD8D9CTD2D8B3D7 CSCTD2D3D8CPD8CXD3D2 CXD7 CQD3D9D2CSCTCS CQDD CYBXCY DA B4DBCWCTD6CT BX CXD7 D8CWCT D7CTD8 D3CU CTD2D8CXD8CXCTD7 CXD2 D8CWCT CTD2DACXD6D3D2D1CTD2D8 CPD2CS DA CXD7 D8CWCT D1CPDCCXB9 D1D9D1 DACPD0CTD2CRDD D3CU CPD2DD CRCPD8CTCVD3D6DDB5B8 D8CWCT CPD0CVD3D6CXD8CWD1 D6D9D2D7 CXD2 DBD3D6D7D8B9CRCPD7CT D4D3D0DDD2D3D1CXCPD0 D7D4CPCRCT D3D2 CYBXCYBN CPD2CS D7CXD2CRCT D8CWCTD6CT CXD7 D2D3 D1D3D6CT D8CWCPD2 D3D2CT D7CTD8 CRD3D1D4D3D7CXD8CXD3D2 D3D4CTD6CPD8CXD3D2 D4CTD6B9 CUD3D6D1CTCS DBCWCTD2 CP D6D9D0CT CXD7 CPD4D4D0CXCTCSB8 CPD2CS CTCPCRCW CRD3D1D4D3D7CXD8CXD3D2 D3D4CTD6CPD8CXD3D2 D6D9D2D7 CXD2 DBD3D6D7D8B9CRCPD7CT D5D9CPCSD6CPD8CXCR D8CXD1CT D3D2 D8CWCT D7CXDECT D3CU CXD8D7 CRD3D1D4D3D7CTCS D7CTD8D7 B4CSD9CT D8D3 D8CWCT D5D9CPD2D8CXACCTD6 D3D4CTD6B9 CPD8CXD3D2B5B8 D8CWCT CPD0CVD3D6CXD8CWD1 D6D9D2D7 CXD2 DBD3D6D7D8B9CRCPD7CT D4D3D0DDD2D3D1CXCPD0 D8CXD1CT D3D2 CYBXCY CPD7 DBCTD0D0BA BG BXDACPD0D9CPD8CXD3D2 CCCWCT CTDCD8CTD2CSCTCS D4CPD6D7CTD6 CSCTD7CRD6CXCQCTCS CPCQD3DACT CWCPD7 CQCTCTD2 CXD1B9 D4D0CTD1CTD2D8CTCS CPD2CS CTDACPD0D9CPD8CTCS D3D2 CP CRD3D6D4D9D7 D3CU BFBGBC D7D4D3B9 CZCTD2 CXD2D7D8D6D9CRD8CXD3D2D7 D8D3 D7CXD1D9D0CPD8CTCS CWD9D1CPD2B9D0CXCZCT CPCVCTD2D8D7 CXD2 CP CRD3D2D8D6D3D0D0CTCS BFB9BW CTD2DACXD6D3D2D1CTD2D8 B4D8CWCPD8 D3CU CRCWCXD0CSD6CTD2 D6D9D2B9 D2CXD2CV CP D0CTD1D3D2CPCSCT D7D8CPD2CSB8 DBCWCXCRCWDBCPD7 CSCTCTD1CTCS D7D9CXD8CPCQD0DD CUCPD1CXD0CXCPD6 D8D3 D9D2CSCTD6CVD6CPCSD9CPD8CT D7D8D9CSCTD2D8 D7D9CQCYCTCRD8D7B5BA CCCWCT D4CPD6D7CTD6 DBCPD7 D6D9D2 D3D2 D8CWCT DBD3D6CS D0CPD8D8CXCRCT D3D9D8D4D9D8 D3CU CPD2 D3ABB9D8CWCTB9D7CWCTD0CU D7D4CTCTCRCW D6CTCRD3CVD2CXDECTD6 B4BVC5CD CBD4CWCXD2DC C1C1B5 CPD2CS D8CWCT D4CPD6D7CTD6 CRCWCPD6D8 DBCPD7 D7CTCTCSCTCS DBCXD8CW CTDACTD6DD CWDDD4D3D8CWCTD7CXDECTCS DBD3D6CSBA CCCWCT D4CPD6D7CTD6 DBCPD7 CPD0D7D3 CRD3D1D4CPD6CTCS DBCXD8CW D8CWCT D6CTCRB9 D3CVD2CXDECTD6 CQDD CXD8D7CTD0CUB8 CXD2 D3D6CSCTD6 D8D3 CSCTD8CTD6D1CXD2CT D8CWCT CSCTCVD6CTCT D8D3 DBCWCXCRCW CPD2 CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS CPD4D4D6D3CPCRCW CRD3D9D0CS CRD3D1B9 D4D0CTD1CTD2D8 CRD3D6D4D9D7B9CQCPD7CTCS CSCXD7CPD1CQCXCVD9CPD8CXD3D2BA CCCWCT D7DDD7D8CTD1D7 DBCTD6CT CTDACPD0D9CPD8CTCS CPD7 DBD3D6CS D6CTCRD3CVD2CXDECTD6D7 B4CXBACTBA CXCVD2D3D6CXD2CV D8CWCT CQD6CPCRCZCTD8D7 CXD2 D8CWCT D4CPD6D7CTD6 D3D9D8D4D9D8B5 D3D2 D8CWCT ACD6D7D8 BDBCBC D7CTD2B9 D8CTD2CRCTD7 D3CU D8CWCT CRD3D6D4D9D7 B4CRD3D6D6CTD7D4D3D2CSCXD2CV D8D3 D8CWCT ACD6D7D8 D7CTDACTD2 D3CU BFBF D7D9CQCYCTCRD8D7B5BN D8CWCT D0CPD8D8CTD6 BEBGBC D7CTD2D8CTD2CRCTD7 DBCTD6CT D6CTB9 D7CTD6DACTCS CUD3D6 D8D6CPCXD2CXD2CV D8CWCT D6CTCRD3CVD2CXDECTD6 CPD2CS CUD3D6 CSCTDACTD0D3D4CXD2CV D8CWCT CVD6CPD1D1CPD6 CPD2CS D7CTD1CPD2D8CXCR D0CTDCCXCRD3D2BA CCCWCT CPDACTD6CPCVCT D9D8D8CTD6CPD2CRCT D0CTD2CVD8CW DBCPD7 CPD4D4D6D3DCCXD1CPD8CTD0DD D8CWD6CTCT D7CTCRD3D2CSD7 B4D7D9CQD7D9D1CXD2CV CPCQD3D9D8 BFBCBC CUD6CPD1CTD7 D3D6 D4D3D7CXB9 D8CXD3D2D7 CXD2 D8CWCT D4CPD6D7CTD6 CRCWCPD6D8B5B8 CRD3D2D8CPCXD2CXD2CV CPD2 CPDACTD6CPCVCT D3CU D2CXD2CT DBD3D6CSD7BA C8CPD6D7CXD2CV D8CXD1CT CPDACTD6CPCVCTCS D9D2CSCTD6 BGBC D7CTCRD3D2CSD7 D4CTD6 D7CTD2D8CTD2CRCT D3D2 CP C8BGB9BDBHBCBCC5C0DEB8 D1D3D7D8 D3CU DBCWCXCRCWDBCPD7 D7D4CTD2D8 CXD2 CUD3D6CTD7D8 CRD3D2D7D8D6D9CRD8CXD3D2 D6CPD8CWCTD6 D8CWCPD2 CSCTD2D3D8CPD8CXD3D2 CRCPD0CRD9D0CPD8CXD3D2BA BTCRCRD9D6CPCRDD D6CTD7D9D0D8D7 D7CWD3DB D8CWCPD8 D8CWCT D4CPD6D7CTD6 DBCPD7 CPCQD0CT D8D3 CRD3D6D6CTCRD8D0DD CXCSCTD2D8CXCUDD CP D7CXCVD2CXACCRCPD2D8D2D9D1CQCTD6 D3CU DBD3D6CSD7 D8CWCPD8 D8CWCT D6CTCRD3CVD2CXDECTD6 D1CXD7D7CTCS B4CPD2CS DACXCRCT DACTD6D7CPB5B8 D7D9CRCW D8CWCPD8 CP D4CTD6CUCTCRD8 D7DDD2D8CWCTD7CXD7 D3CU D8CWCT D8DBD3 B4CRCWD3D3D7CXD2CV D8CWCT CRD3D6D6CTCRD8 DBD3D6CS CXCU CXD8 CXD7 D6CTCRD3CVD2CXDECTCS CQDD CTCXD8CWCTD6 D7DDD7D8CTD1B5 DBD3D9D0CS D4D6D3B9 CSD9CRCT CPD2 CPDACTD6CPCVCT D3CU BK D4CTD6CRCTD2D8CPCVCT D4D3CXD2D8D7 D1D3D6CT D6CTCRCPD0D0 D8CWCPD2 D8CWCT D6CTCRD3CVD2CXDECTD6 CQDD CXD8D7CTD0CU D3D2 D7D9CRCRCTD7D7CUD9D0 D4CPD6D7CTD7B8 CPD2CS CPD7 D1D9CRCW CPD7 BDBL D4CTD6CRCTD2D8CPCVCT D4D3CXD2D8D7 D1D3D6CT CUD3D6 D7D3D1CT D7D9CQCYCTCRD8D7BM BDBC D6CTCRD3CVD2CXDECTD6 D4CPD6D7CTD6 CYD3CXD2D8 D7D9CQCYCTCRD8 D4D6CTCR D6CTCRCPD0D0 CUCPCXD0 D4D6CTCR D6CTCRCPD0D0 D6CTCRCPD0D0 BC BJBI BJBL BDBK BJBE BJBG BLBE BD BJBJ BJBH BEBK BIBF BHBH BKBF BE BJBC BJBD BFBF BGBL BHBG BIBL BF BJBD BIBJ BGBF BGBL BGBH BIBL BG BIBI BHBG BFBJ BGBG BFBL BIBJ BH BHBF BHBE BHBG BFBI BFBD BJBE BI BKBG BKBG BHBC BHBI BIBF BKBF CPD0D0 BIBK BIBJ BFBJ BHBF BHBC BJBH DBCWCXCRCW CXD2CSCXCRCPD8CTD7 D8CWCPD8 D8CWCT CTD2DACXD6D3D2D1CTD2D8 D1CPDD D3ABCTD6 CP D9D7CTCUD9D0 CPCSCSCXD8CXD3D2CPD0 D7D3D9D6CRCT D3CU CXD2CUD3D6D1CPD8CXD3D2 CUD3D6 CSCXD7CPD1B9 CQCXCVD9CPD8CXD3D2BA CCCWD3D9CVCW CXD8 D1CPDD D2D3D8 CQCT D4D3D7D7CXCQD0CT D8D3 CXD1D4D0CTB9 D1CTD2D8 CP D4CTD6CUCTCRD8 D7DDD2D8CWCTD7CXD7 D3CU D8CWCT CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS BDBC CBD9CRCRCTD7D7CUD9D0 D4CPD6D7CTD7 CPD6CT D8CWD3D7CT D8CWCPD8 D6CTD7D9D0D8 CXD2 D3D2CT D3D6 D1D3D6CT CRD3D1D4D0CTD8CT CPD2CPD0DDD7CTD7 D3CU D8CWCT CXD2D4D9D8B8 CTDACTD2 CXCU D8CWCT CRD3D6D6CTCRD8 D8D6CTCT CXD7 D2D3D8 CPD1D3D2CV D8CWCTD1BA CPD2CS CRD3D6D4D9D7B9CQCPD7CTCS CPD4D4D6D3CPCRCWCTD7B8 CXCU CTDACTD2 CWCPD0CU D3CU D8CWCT CPCQD3DACT CVCPCXD2D7 CRCPD2 CQCT D6CTCPD0CXDECTCSB8 CXD8 DBD3D9D0CS D1CPD6CZ CP D7CXCVB9 D2CXACCRCPD2D8 CPCSDACPD2CRCTBA BH BVD3D2CRD0D9D7CXD3D2 CCCWCXD7 D4CPD4CTD6 CWCPD7 CSCTD7CRD6CXCQCTCS CPD2 CTDCD8CTD2D7CXD3D2 D8D3 CPD2 CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS D4CPD6D7CXD2CV CPD0CVD3D6CXD8CWD1B8 CXD2CRD6CTCPD7CXD2CV CXD8D7 D7CTD1CPD2D8CXCR CRD3DACTD6CPCVCT D8D3 CXD2CRD0D9CSCT D5D9CPD2D8CXACCTD6 CPD2CS CRD3D2CYD9D2CRB9 D8CXD3D2 D3D4CTD6CPD8CXD3D2D7 DBCXD8CWD3D9D8 CSCTD7D8D6D3DDCXD2CV CXD8D7 D4D3D0DDD2D3D1CXCPD0 DBD3D6D7D8B9CRCPD7CT CRD3D1D4D0CTDCCXD8DDBA BXDCD4CTD6CXD1CTD2D8D7 D9D7CXD2CV CPD2 CXD1D4D0CTB9 D1CTD2D8CPD8CXD3D2 D3CU D8CWCXD7 CPD0CVD3D6CXD8CWD1 D3D2 CP CRD3D6D4D9D7 D3CU D7D4D3CZCTD2 CXD2D7D8D6D9CRD8CXD3D2D7 CXD2CSCXCRCPD8CT D8CWCPD8 BDB5 D8CWCT D3CQD7CTD6DACTCS CRD3D1D4D0CTDCB9 CXD8DD D3CU D8CWCT CPD0CVD3D6CXD8CWD1 CXD7 D7D9CXD8CPCQD0CT CUD3D6 D4D6CPCRD8CXCRCPD0 D9D7CTD6 CXD2B9 D8CTD6CUCPCRCT CPD4D4D0CXCRCPD8CXD3D2D7B8 CPD2CS BEB5 D8CWCT CPCQCXD0CXD8DD D8D3 CSD6CPDB D3D2 D8CWCXD7 CZCXD2CS D3CU CTD2DACXD6D3D2D1CTD2D8 CXD2CUD3D6D1CPD8CXD3D2 CXD2 CPD2 CXD2D8CTD6B9 CUCPCRCTCS CPD4D4D0CXCRCPD8CXD3D2 CWCPD7 D8CWCT D4D3D8CTD2D8CXCPD0 D8D3 CVD6CTCPD8D0DD CXD1B9 D4D6D3DACT D6CTCRD3CVD2CXD8CXD3D2 CPCRCRD9D6CPCRDD CXD2 D7D4CTCPCZCTD6B9CXD2CSCTD4CTD2CSCTD2D8 D1CXDCCTCSB9CXD2CXD8CXCPD8CXDACTCXD2D8CTD6CUCPCRCTD7BA CACTCUCTD6CTD2CRCTD7 C3CPDECXD1CXCTD6DE BTCYCSD9CZCXCTDBCXCRDEBA BDBLBFBHBA BWCXCT D7DDD2D8CPCZD8CXD7CRCWCT CZD3D2D2CTDCB9 CXD8CPD8BA C1D2 CBBA C5CRBVCPD0D0B8 CTCSCXD8D3D6B8 C8D3D0CXD7CW C4D3CVCXCR BDBLBEBCB9BDBLBFBLB8 D4CPCVCTD7 BEBCBJDFBEBFBDBA C7DCCUD3D6CS CDD2CXDACTD6D7CXD8DD C8D6CTD7D7BA CCD6CPD2D7D0CPD8CTCS CUD6D3D1 CBD8D9CSCXCP C8CWCXD0D3D7D3D4CWCXCRCP BDBM BDDFBEBJBA CHCTCWD3D7CWD9CP BUCPD6B9C0CXD0D0CTD0BA BDBLBHBFBA BT D5D9CPD7CXB9CPD6CXD8CWD1CTD8CXCRCPD0 D2D3D8CPB9 D8CXD3D2 CUD3D6 D7DDD2D8CPCRD8CXCR CSCTD7CRD6CXD4D8CXD3D2BA C4CPD2CVD9CPCVCTB8 BEBLBMBGBJDFBHBKBA C2D3D2 BUCPD6DBCXD7CT CPD2CS CAD3CQCXD2 BVD3D3D4CTD6BA BDBLBKBDBA BZCTD2CTD6CPD0CXDECTCS D5D9CPD2D8CXACCTD6D7 CPD2CS D2CPD8D9D6CPD0 D0CPD2CVD9CPCVCTBA C4CXD2CVD9CXD7D8CXCRD7 CPD2CS C8CWCXB9 D0D3D7D3D4CWDDB8BGBA CBDDD0DACXCT BUCXD0D0D3D8 CPD2CS BUCTD6D2CPD6CS C4CPD2CVBA BDBLBKBLBA CCCWCT D7D8D6D9CRD8D9D6CT D3CU D7CWCPD6CTCS CUD3D6CTD7D8D7 CXD2 CPD1CQCXCVD9D3D9D7 D4CPD6D7CXD2CVBA C1D2 C8D6D3CRCTCTCSCXD2CVD7 D3CU D8CWCT BEBJ D8CW BTD2D2D9CPD0 C5CTCTD8CXD2CV D3CU D8CWCT BTD7D7D3CRCXCPD8CXD3D2 CUD3D6 BVD3D1B9 D4D9D8CPD8CXD3D2CPD0 C4CXD2CVD9CXD7D8CXCRD7 B4BTBVC4 B3BKBLB5B8 D4CPCVCTD7 BDBGBFDFBDBHBDBA CECTD6AJD3D2CXCRCP BWCPCWD0 CPD2CS C5CXCRCWCPCTD0 BVBA C5CRBVD3D6CSBA BDBLBKBFBA CCD6CTCPD8CXD2CV CRD3D3D6CSCXD2CPD8CXD3D2 CXD2 D0D3CVCXCR CVD6CPD1D1CPD6D7BA BTD1CTD6CXCRCPD2 C2D3D9D6D2CPD0 D3CU BVD3D1D4D9D8CPD8CXD3D2CPD0 C4CXD2CVD9CXD7D8CXCRD7B8 BLB4BEB5BMBIBLDFBLBDBA C2D3CWD2 BWD3DBCSCXD2CVB8 CAD3CQCTD6D8 C5D3D3D6CTB8 BYD6CPD2AOCRD3CXD7 BTD2CSCTD6DDB8 CPD2CS BWD3D9CVD0CPD7 C5D3D6CPD2BA BDBLBLBGBA C1D2D8CTD6D0CTCPDACXD2CV D7DDD2D8CPDC CPD2CS D7CTD1CPD2B9 D8CXCRD7 CXD2 CPD2 CTCRCXCTD2D8 CQD3D8D8D3D1B9D9D4 D4CPD6D7CTD6BA C1D2 C8D6D3CRCTCTCSCXD2CVD7 D3CU D8CWCT BFBED2CS BTD2D2D9CPD0 C5CTCTD8CXD2CV D3CU D8CWCT BTD7D7D3CRCXCPD8CXD3D2 CUD3D6 BVD3D1B9 D4D9D8CPD8CXD3D2CPD0 C4CXD2CVD9CXD7D8CXCRD7 B4BTBVC4B3BLBGB5BA C2D3D7CWD9CP BZD3D3CSD1CPD2BA BDBLBLBLBA CBCTD1CXD6CXD2CV D4CPD6D7CXD2CVBA BVD3D1D4D9D8CPB9 D8CXD3D2CPD0 C4CXD2CVD9CXD7D8CXCRD7B8 BEBHB4BGB5BMBHBJBFDFBIBCBHBA BXBA C3CTCTD2CPD2 CPD2CS C2BA CBD8CPDACXBA BDBLBKBIBA BT D7CTD1CPD2D8CXCR CRCWCPD6CPCRD8CTD6CXDECPB9 D8CXD3D2 D3CU D2CPD8D9D6CPD0 D0CPD2CVD9CPCVCT CSCTD8CTD6D1CXD2CTD6D7BA C4CXD2CVD9CXD7D8CXCRD7 CPD2CS C8CWCXD0D3D7D3D4CWDDB8 BLBMBEBHBFDFBFBEBIBA CACXCRCWCPD6CS C5D3D2D8CPCVD9CTBA BDBLBJBFBA CCCWCT D4D6D3D4CTD6 D8D6CTCPD8D1CTD2D8D3CU D5D9CPD2B9 D8CXACCRCPD8CXD3D2 CXD2 D3D6CSCXD2CPD6DD BXD2CVD0CXD7CWBA C1D2 C2BA C0CXD2D8CXCZCZCPB8 C2BAC5BABXBA C5D3D6CPDACRD7CXCZB8 CPD2CS C8BA CBD9D4D4CTD7B8 CTCSCXD8D3D6D7B8 BTD4D4D6D3CPCRCWCTD7 D8D3 C6CPD8B9 D9D6CPD0 C4CPD2CVCPD9CVCTB8 D4CPCVCTD7 BEBEBDDFBEBGBEBA BWBA CACXCTCSCTD0B8 BWD3D6CSD6CTCRCWD8BA CACTD4D6CXD2D8CTCS CXD2 CABA C0BA CCCWD3D1CPD7D3D2 CTCSBAB8 BYD3D6D1CPD0 C8CWCXD0D3D7D3D4CWDDB8 CHCPD0CT CDD2CXDACTD6D7CXD8DD C8D6CTD7D7B8 BDBLBLBGBA CFCXD0D0CXCPD1 CBCRCWD9D0CTD6BA BEBCBCBDBA BVD3D1D4D9D8CPD8CXD3D2CPD0 D4D6D3D4CTD6D8CXCTD7 D3CU CTD2DACXD6D3D2D1CTD2D8B9CQCPD7CTCS CSCXD7CPD1CQCXCVD9CPD8CXD3D2BA C1D2 C8D6D3CRCTCTCSCXD2CVD7 D3CU D8CWCT BFBLD8CW BTD2D2D9CPD0 C5CTCTD8CXD2CV D3CU D8CWCT BTD7D7D3CRCXCPD8CXD3D2 CUD3D6 BVD3D1B9 D4D9D8CPD8CXD3D2CPD0 C4CXD2CVD9CXD7D8CXCRD7 B4BTBVC4 B3BCBDB5B8CCD3D9D0D3D9D7CTB8 BYD6CPD2CRCTBA CBD8D9CPD6D8 C5BA CBCWCXCTCQCTD6B8 CHDACTD7 CBCRCWCPCQCTD7B8 CPD2CS BYCTD6D2CPD2CSD3 BVBAC6BA C8CTD6CTCXD6CPBA BDBLBLBHBA C8D6CXD2CRCXD4D0CTD7 CPD2CS CXD1D4D0CTD1CTD2D8CPD8CXD3D2 D3CU CSCTB9 CSD9CRD8CXDACT D4CPD6D7CXD2CVBA C2D3D9D6D2CPD0 D3CU C4D3CVCXCR C8D6D3CVD6CPD1D1CXD2CVB8 BEBGBMBFDF BFBIBA References Kazimierz Ajdukiewicz.
1935. Die syntaktische konnexitat.
In S.
McCall, editor, Polish Logic 1920-1939, pages 207231.
Oxford University Press.
Translated from Studia Philosophica 1: 127.
Yehoshua Bar-Hillel.
1953. A quasi-arithmetical notation for syntactic description.
Language, 29:4758.
Jon Barwise and Robin Cooper.
1981. Generalized quanti ers and natural language.
Linguistics and Philosophy, 4.
Sylvie Billot and Bernard Lang.
1989. The structure of shared forests in ambiguous parsing.
In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics (ACL '89), pages 143151.
Veronica Dahl and Michael C.
McCord. 1983.
Treating coordination in logic grammars.
American Journal of Computational Linguistics, 9(2):6991.
John Dowding, Robert Moore, Francois Andery, and Douglas Moran.
1994. Interleaving syntax and semantics in an ecient bottom-up parser.
In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics (ACL'94).
Joshua Goodman.
1999. Semiring parsing.
Computational Linguistics, 25(4):573605.
E. Keenan and J.
Stavi. 1986.
A semantic characterization of natural language determiners.
Linguistics and Philosophy, 9:253326.
Richard Montague.
1973. The proper treatment of quanti cation in ordinary English.
In J.
Hintikka, J.M.E.
Moravcsik, and P.
Suppes, editors, Approaches to Natural Langauge, pages 221242.
D. Riedel, Dordrecht.
Reprinted in R.
H. Thomason ed., Formal Philosophy, Yale University Press, 1994.
William Schuler.
2001. Computational properties of environment-based disambiguation.
In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL '01), Toulouse, France.
Stuart M.
Shieber, Yves Schabes, and Fernando C.N.
Pereira. 1995.
Principles and implementation of deductive parsing.
Journal of Logic Programming, 24:336.
 The Elimination of Grammatical Restrictions M.
Salkoff and in a String Grammar of English N.
Sager Institute for Computer Research in the Humanities New York University, New York i.
Sun~nary of String Theory In writing a grammar of a natural language, one is faced with the problem o f e x p r e s s i n g grammatica NVN number: sequence N1 and N 2 (N, noun: V, verb), the subject The boy eats the meat; Q N1 P N2 N For example, i n t h e s e n t e n c e form and the verb V must agree in Or, in the five feet in length, One of the ~ The boys eats the meat.
e.g., (Q a number; P, preposition), are of particular subclasses: theories of linguistic structure which is particularly relevant to this problem is linguistic string analysis[1].
In this theory, the major syntactic (a string is structures of English are stated as a set of elementary strings a sequence of word categories, e.g., N V____NN, N V P N, eta).
Each sentence of the language consists of one elementary sentence (its center string) plus zero or more elementary adjunct strings which are adjoined either to the right or left or in place of particular elements of other elementary strings in the sentence.
17.~ The elementary strings can be grouped into classes according to how and where they can be inserted into other strings.
an elementary string, X If Y = X 1 X 2. . . Xn is ranging over the category symbols, the following classes of strings are defined: left adjuncts of X: adjoined to a string Y to the left of X in Y, or to the left of an ~X adjoined to Y in this manner.
rX right adjuncts of X: adjoined to a string Y to the right of X in Y, or to the right of an rX adjoined to Y in this manner.
replacement strings of X: adjoined to a string Y, replacing X in Y.
sentences adjuncts of the string Y, adjoined to the left of X 1 or after X i in Y (l~ i ~ n), or to the right of an Sy adjoined to Y in this manner.
Cy, i conjunctional strings of Y, conjoined after X i in Y (i< i < n), or to _ _ the right of a Cy, i adjoined to Y in this manner.
z center strings, not adjoined to any string.
These string-class definitions, with various restrictions on the repetition ~and order of members of the classes, constitute rules of combination on the elementary strings to form sentences.
Roughly speaking, a center string is the skeleton of a sentence and the adjuncts are modifiers.
green we met in in An example of a left adjunct of N is the adjective A right adjunet of N is the clause whom the green blackboard.
the man whom we met.
in the sentence A replacement formula of N is, for example, The same sentence what he said What he said was interesting.
with a noun instead of a noun replacement string might be interesting.
since he left.
An example is are Examples of sentence adjuncts are The lecture was in general, at this time, The c strings have coordinating conjunctions at their head.
but left in He was here but left.
Examples of center strings He understood and also We wondered whether he understood.
The grammatical dependencies are expressed by restrictions on the strings as to the word subcategories which can occur together in a string or in strings related by the rules of combination.
Thus, in the center string N 1 V N2, the figrammatical dependency mentioned above is formulated by the restriction: if N1 is plural, theh V does not carry the singular morpheme -_ss.
The string grammar with restrictions gives a compact representation of the linguistic data of a language, and provides a framework within which it is relatively simple to incorporate more linguistic refinement, restrictions.
J i.e., more detailed One may ask whether it is possible to write such a string grammar without any restrictions at all, i.e., to express the grammatical dependencies (restrictions) in the syntactic structures themselves.
In the resulting restrictionless grammar, any elements which are related by a grammatical dependency w i l L b e e l e m e n t s relations, other of the same elementary string.
No grammatical than those given by the simple rule of string combination, The result of this paper is to obtain between two strings of a sentence.
demonstrate that such a restrictionless grammar can be written [4].
In order to obtain a restrictionless form of a string grammar of English, we take as a point of departure the grammar used by the computer program for string decomposition of sentences, developed at the University of Pennsylvania [2,3].
This gran~nar is somewhat more detailed than the sketch of an English A summary of the form of the computer grammar is In section 3 we show how the restrictions can be string grammar in Ill.
presented below in section 2.
eliminated from the gran~nar.
An example of a typical output obtained for a short sentence from a text of a medical abstract is shown in Figs.
1 and 2.
The decomposition of the sentence into a sequence of nested strings is indicated in the output by the numbering of the strings.
As indicated in line 1., the sentence consists of the two assertion centers in lines 2.and ~ ~ conjoined by and.
The line B  ficontains a sentence adjunct th~_~) on the assertion center as a whole . The assertion center 2 . is of the form N V A : Spikes would be effective . The noun spikes has a left adjunct (such enhanced) in line 5  as indicated by the appearance of 5 . to the left of spikes . The object effective has a left adjunct ~ 9 _ ~ ) in line 6 . and a right adjunct in line 7  In the same wsy, each of the elements of the adjunct strings may have its own left and right adjuncts.
Line IO . contains an assertion center in which the subject and the This zeroing is indicated in the modal verb (woul____dd)have been zeroed.
output by printing the zeroe~ element in parentheses.
The difference between the two analyses in Figs.
decomposition of the sequence in initiating analysis (Fig.
i an~ 2 lies in the In the first synaptlc action.
I), this sequence is taken as a P_~N right adjunct on of the effective, where initiating synaptlc is a left adjunct (onaction) form of a repeated adjective (parallel to escaping toxic in the sequence in eseap.ing toxic gases) . In the second analysis (~ig.
2), this same sequence is taken as a ~ right adjunct of effective, where initiating is the Ving, and synaptic action is the Object of initiating.
The Computer String Grammar.
In representing the string grammar in the computer, a generalized grammar where Y-.
= Y' IS where Y' is a grammar string like Y.
This system of nested gram~nar strings terminates when one of the grammar strings is equal to an atomic string (one of the word-category symbols).
The Y.
are called the options of Y, and each option Y.
consists of the elements Y... l l 13 Not every option of a grammar string Y will be well-formed each time the sentence analysis program finds an instance of Y in the sentence being analyzed.
Associated with each option Yi is a series of zero or more tests, called restrictions.
'If RiP is the set of tests associated with Yi then the grammar A restriction is a test (which will be descrfbed below) so written that if it does not give a positive result its attached option may not be chosen.
All of the restrictions in the grammar fall into two types: TypeA: The restrictions of type A enable one to avoid defining many The options of the grammar string Y similar related sets of grammar strings.
have been chosen so that Y represents a group of strings which have related filinguistic properties.
This allows the grammar to be written very compactly, and each grammar string can be formulated as best suits the linguistic data.
However, when a grammar string Y appears as a Y' ij of some other string Y', some of the options of Y may lead to . non-wellformed sequences.
In order to retain the group of options of Y and yet not allow non-wellformed sequences wherever options of Y which would have that effect are used, we attach a restriction of type A to th0s~ options of Y.
For example, let Y be where and YI = which Z V (e.g., which he chose) Y2 = what E V (e.g., what he chose) Then Y can appear in the subject Z of the linguistic center string CI: Cl = z v n What he chose was impDrtant.
This yields Which he chose was important; As it is defined here, Y can also be used to represent the wh-clauses in the right adjuncts of the noun: but in rN only the which option of Y gives wellformed sequences: 3 the book which he chose the book what he chose Hence a restriction R a is attached to the what option of Y (eq.
5) whose effect is to prevent that option from being used in rN.
Type B: With some given set of rather broadly defined major categories (noun, verb, adjective, etc).
it is always possible to express more detailed linguistic relations by defining sub-categories of the major categories.
These relations then appear as constraints on how the sub-categories may appear together in the grammar strings Y.
If some element Yij of Yi is an atomic string (hence a word-category symbol) representing some major category, say C, then Rb may exclude the subcategory Cj as value of Yij if some other element Yik of Yi has the value Ck.
Y i k m a y also be a grammar string, in which case R b m a y exclude a particular option of Yik when Yij has value C..
The restrictions Rb may be classified into three kinds: (a) Between elements of some string Y.
where the Y..
correspond to elements 1 i~ of a linguistic string.
For example, A noun in the sub-category singular cannot appear with a verb in the sub-category plural.
~ The man agree.
Only a certain sub-category of adjective can appear in the sentence adjunct P__AA : in general, (b) in particular, ~ in ha~py.
Between a Yij and a Yik where Yij corresponds to an element of a linguistic For example, string and Yik corresponds to a set of adjuncts of that element.
In rN, the string to V 2 cannot adjoin a noun of sub-categoryN 2 (proper names): the man to do the job ~ John to do the ~ob.
Only a certain adjective sub-category (e.g., re~/.e~, available) can appear in rN without any left or right adjunct of its own: the people present ; (c) ~ the people happy.
Between Yij and Yik ' where one corresponds to an element of a linguistic string and the other corresponds to an adjunct set which can repeat itself, i.e., which allows 2 or more adjuncts on the same linguistic element.
These restrictions enable one to express the ordering among adjuncts in some adjunct sets.
For example, Q (quantifier) and A (adjective) are both in the set N ' the left adjuncts of the noun.
However, _Q can precede A but A cannot precede _Q when both are adjuncts of the same N in a sentence: 3 Q A N books, but ~AQN e.g., five green e.g., green five books.
The string grammar defined by eqs.
i-3, together with the atomic strings (word-category symbols) have the form of a BNF definition.
The system with eq.
4, however, departs from a BNF definition in two important respects : (a) it contains restrictions (tests) on the options of a definition; (b) the atomic strings (word-categories) of the grammar have sub-classifications.
With the elimination of the restrictions, the computer grammar will again have the form of a BNF definition.
fi3. Elimination of the Restrictions The restrictionless string grammar is obtained from the grammar (in described above by the methods of (A) and (B) below.
Initially this paper), conjunctional restrictionless strings have not been included in the grammar.
We estimate that the addition of conjunctions/ grammar by a strings will increase the size of the restrictionless factor of about 5.
(A) The linguistic strings represented in the computer graz~,ar are reformulated in accordance with the following requirement.
any utterance of a language containing Given grammatical dependency obtains between A and B, the elementary strings of a restrictionless string grammar are defined so that A and B appear together in the same linguistic string, and any iterable sequence between A and B is an adjunct of that string.
Iterable sequences of the type seemed to begin to in It seemed to be~in to surprise him that we in It is said to be known worked seriously, or is said to be known to to surprise him that we worked seriuusly are analyzed as adjuncts.
If we place such sequences among the left adjuncts of the verb, v ' then the sentences above can be put in the form It~_v surprise him that we worked seriously fi~v = seemed to begin to ; However, when the adjunct by definition), surprise verb of ~v is said to be known to ; etc.
takes on the value zero (as can all adjuncts, sequence It then (9) above becomes the non-grammatical him that we worked seriously.
~v (seemed ~ This happens because the first and the latter is__) carries the tense morpheme, disappears when We separate the tense morpheme from the verb, and place it in the center string as one of the required elements.
(i0) C1 = Z t ~ V g; This formulation of the assertion center string C1 (lO), in which the tense morpheme is an independent element and iterable sequences are taken as adjuncts, is necessary in ord@r to preserve, for example, the dependence surprises him that we In the between the particle it and the succeeding sequence worked seriously: grammar~which ~ The book surprises him that we worked seriously.
includes restrictions, this formulation is not necessary because this dependence can be checked by a restriction.
(B) Turning to the computer form of the grammar, all the restrictions of the grammar are eliminated either by defining new grammar strings (for the elimination of the restrictions categories by the particular required by the restriction Ra) ' or by replacing the general wordsubclasses of those categories which are (to eliminate Rb).
The application of this procedure increases the number of strings in the grammar, of course.
The restrictions R a can be eliminated in the following manner.
Suppose the option Yi of Y has a restriction R a on it which prevents it from being chosen in Y' (Y is a Y'ij of Y').
Then define a new grammar string Y ' w h i c h ficontains all the options of Y but Y.
: Then the new gran~nar string Y* replaces Y in Y'.
R a on p.
5, the string Y* = which Z t fv V / ....
Thus, in the example of (in the modified treatment of tense and iterable sequences) would replace Y in r N.
The restrictions R b are eliminated in a different way, according to the types described on p.
6. (a) New strings must be written in which only the wellformed sequences In the example of subject-verb agreement, the of subcategories appear.
where N s and Np are singular and plural nouns, V s and Vp singular and plural verbs.
(b) If an element of a particular subcategory, say Ai, can take only a rAi is defined.
It subset of the adjuncts rA, then a new adjunct s~ring contains those options~_ of rA which can appear only with A i plus all the options of r A which are common to all the sub-categ0ries of A.
When this to rA, : has been done f0r  all A i having some particular behavior w i t h r e s p e c t all the remaining sub-categories A rA ~ AlrA1 of A will have a common adjunct string r a As many new sets rAi must be defined as there were special sub-categories A.
A similar argument holds for ~A and other adjunct sets which depend on A.
A new element corresponding to the/adjunct set must be defined in which the adjuncts appear correctly ordered with respect to each other, and each one must be able to take on the value zero.
This procedure for eliminating restrictions is also the algorithm for introducing further grammatical refinements into the restrictionless grammar.
Such a general procedure can be formulated because of an essential property of a string grammar: In terms of linguistic (elementary) strings, all a) between elements of a string, or b) between an restrictions are either element of the string and its adjunct, or same string.
c) between related adjuncts of the Further, there is no problem with discontinuous elements in a all elements which depend in some way on each other grammaticstring grammar: ally appear in the same string or in strings which are contiguous by adjunction.
The cost of the elimination of all restrictions in this way is about an order of magnitude increase in the number of strings of the grammar.
Instead of about 200 strings of the computer grammar, the grammar presented here has about 2000 strings.
It is interesting that the increase in the size of the This suggests that in a program Also, since grammar is not greater than roughly one order of magnitude.
there may be practical applications for such a grammar, e.g. designed to carry out all analyses of a sentence in real time.
the restrictionless grammar is equivalent to a B.N.F. grammar of English, it may prove useful in adding English-language features to programming languages which are written in B.N.F. fiSENTENCE N E U H I B  SUCE ENHANCED SPIKES WOULD BE MORE E F F E C T I V E IN I N I T I A T I N G SYNAPTIC ACTION AND THUS BE RESPONSIBLE FOR THE OBSERVED POST-TETANIC POTENTIATION  Ol I.
PARSE SENTENCE INTRODUCER CENTER AND Z, AND CI ASSERTION SUBJECT 5 . SPIKES VERB $ OBJECT gOULD BE 6.
EFFECTIVE RV T, ACVERB ADVERB THUS CONJUNCTION LN ARTICLE QUANTIFIER SUCH ADJECTIVE ENHANCED TYPE-NS" NOUN AEVERB PN PREPOSITION IN ACTION lO.
CI ASSERTION BE OBJECT RESPONSIBLE II.
LN ARTICLE QUANTIFIER ADJECTIVE INITIATING TYPE-MS SYNAPTIC NOUN PN POTENTIATION LN GUANTIFIER ADJECTIVE OBSERVED P O S T T E T A N I C TYPE-NS NOUN SENIENCE NEUH-.IB  SUCH ENHANCED SPIKES kOULD BE MORE E F F E C T I V E IN I N I | i A T I N G S Y N A P T I C A C T I O N AND THUS UE R E S P O N S I B L E FOR THE OBSERVED P O S T T E T A N I C P O T E N T I A T I O N  02 = |NTROOUCER CENTER AND Z.
AND 3 6 END MARK  PARSE SENTENCE CI VERB  kOULD BE RV T, ACVERB S ADVERB IHUS LN ARTICLE QUANTIFIER SUCH ADJECTIVE ENHANCED TYPE-NS NOUN lCVERB To P NS V I N G I O F | 0 = PREPOSITION IN SN INIIIATING ACTION CI ASSERTION VERB (WOULD) OBJECT RESPONSIBLE LN QUANTIFIER TYPE-NS NOUN PN = LP P R E P O S I T I C N FOR POTENTIATION QUANTIFIER ADJECTIVE OBSERVEO P O S T T E T A N I C TYPE-NS NOUN NG MCRE PARSES Conclusion 4.
This problem was suggested by Professor J.
Schwartz of the Courant institute of Mathematical Sciences, New York University.
5. The option Yi here corresponds to the linguistic string Y of the previous section.
The symbol / separates the options of a string definition.
Academic Press, REFERENCES 1.
Harris, Z.
S., . String Analysis of Sentence Structure.
Papers on Formal Linguistics, No.
l, Mouton and Co., The Hague, 1962.
2. Sager, N., Salkoff, M., Morris, J., and Raze, C., . Report on the String Analysis Programs.
Department of Linguistics, University of Pennsylvania, March 1966.
3. Sager, N., . Syntactic Analysis of Natural Language.
Advances in Computers (Alt, F.
and Rubinoff, M., eds.), vol.
8, pp.
153-188. New York, 1967 .
CONTEXTUAL GRAMMARS Solomon Marcus InsUtutul de Matematica Str, Mihal Eminescuo 47 BuchareSt 9, ROMANIA In the following, we shall introduce a type of generative grammars, called contextual grammars.
They are not comparable with regular grammarsBut every language generated by a contextual grammar is a context-ree language.
Generalized contextual grammars are introduced, which may generate non-cox,text-free languages.
Let V be a finite non-void set ; V lary.
Every finite sequence of elements in ia called a vocnbuV is said to be a string on V.
Given a string x = ala2...an, the number n is called the length of x.
The string of length zero is called the n tring and is denoted by r~J. Any set of strings on V is called a language on V.
The set of all strings on V (the null-string inclusively)is called the universal language on V.
By a nwe denote the string a...a, where a is iterated n times.
Any ordered pair (u,v~ of strings on V_ is said to be a contex~ on V.
The string x is admitted by the context <u,v> With respect to the language L if u~ G L.
Let .~ be a finite set of strings on the vocabulary V~ and let@be a finite se@ of contexts on V.
The triple (v,~, ~)) (1) is said to be a contextual l~rammar ; V is the vocabulary of the grammar, ~ is the ba_s_e_ of the grammar and ~is the co m~,-2textual ccmoonent of the grammar.
Let us denote by ~ the contextual grammar defined by (1).
Oonsider the smallest language L on Vj fulfilling the following two conditiom8 (~J Iz ~ and <u,v>,(~), th-~=,L.
The language L is said to be the lsmguage generated by the contextual grammar G.
This means that the language generated by G is the intersection of all languages L fulfilling the conai~ions (~) and (pj . A language ~L is said to be a eonteF~ual language if there exists a contextual grammar G which generates L.
Proposition i.
Eyer~ finite language is a cont~ual lanProo__f.
Let V be a vocabulary and let ~ be a finite lan.
guage on V.
It is obvious that the contextual grammar (V,L4jO), where 9 dauotes the void set of contexts, gauerates the language L I.
The same language may be gamerated by means of the .g  contextual grammar (V,I~, where is formed by the nu~ cont ex~ only.
Two contextual grammars are called e~uivalemt if they gemssame language.
The grammars CV,LI, O ) and (V,~,~ rate the are equivalent, since they both generate the language ~ The converse of Proposition 1 is not true.
Indeed, we have Proposition 2.
The universal language is a contextual language.
ProOf. Let V = ~alLa2,...~ ~.
De~ote by I~ the umiver.
Sal language on V.'T.et us put ~" S~.~.~ and t <~''i" " C -3~,a~,,...s(~,ian> ~ It is easy to see that thegrammar . (V,~ generates the universal language on V.
Remarks. If we put, in the proof Of Proposition 2, LI-V instead of h =~' then the grammar (V_,h,@) does not generate the universal language on V, since the language it generates dDes not contain the nu/l-strlng.
In order to illustrate the activity of the grammar (V,~, ~defined in the of let consider the proof proposition 2, US particular case when the vocabulary iS formed by two elements only : V =(a.b~.
The general form of a string x on V is x = a ~b ~a-~b~...a ~b~, where il, Jl, i2,j2,...,~,j ~ are arbitra~non-negative integers.
In order to generate the string x, we start with the null.string @@ and we apply il times the context ~,a~ . The result of this operation is the string a 11,to which we apply Jl times the context <~,b> and obtain the string al~ ~I . Now we apply i~ times the context <~,a>, than J2 ti~es the context ~,b_> and we continue so alternatively.
~hen, ~dter 2p-2 steps, we have obtained the J = ai bJlai b 2ooo LP" ib -i, it is ply .~ ti~es ~ne context ~@,~ and, to the string so obtained, jp times ~:e contex~ ~gb>, in order to generate completely Uhe string Xo Haskell Curry considered Ghe larlg~age L = {abn~ (n=l,2~..o) as a model of ~he set of natural numbers \[5~ o We call L the language of Curry.
Prooosiuion 3.
The language of Curry is a contextual langu~eo proof.
The considered language is generated by the grammar (V,LI,~), where V= ~a.b~, I~ =~a 3 -nd~ ~<~,b~.
We recall that a language is Said to be regular if it may be generated by means of a finite aatoma$on (or, equivalently, by means of a finite state grammar in the sense of Ohomsky).
Proposition 4.
There exis$~ a contextual language which is Proof.
Let us consider the language L = ~a-nb n} (n=l,2,)...
If we put V = {ab}, L 1 = ~ab~ and ~ ~<a.b>}, then it is easy to see that L is generated by the con~extuai grammar (V, LI, ~.
On the other hand, L is n~t a regular language.
This fact was assel~ed by Ghomsky in \[3~ and\[~\], but the proof he gives is wrong.
A correct proof of this assertion and a.discussion of Chomsky' s proof were given in \[~\].
and ~.
Propositions 2,3 and # show that there are many infinite languages w~ioh are oontextual.
This fact may be explained by means of P~posi~ion 5.
If the set ~ is non-void and if the set~ contains at least one non-nu/1 contex~ I ~hen the contextual gramma___r_r (V.Ll, ~ ~enerate s an infinite language.
Proof. Since L A is non-void, we may find a string x be@ longing to ~i o Since contains, at least one non-nu/\] context, @  let ~u,v~ be a non-null context belonging to . l~rom these assumptions, we infer that the strings, u2xv 2, . .,un~,,..
are mutually distinc~ and belong all to the language generated by the grammar (V,I~,).
Thus, ~his language is infinite.
The converse of Proposition 5 is true.
Indeed, we have / / -5Proposition 6.
If the contextual 6rammar (V, LI~ gau~ rates an infillite language, then ~Ll.
is non-void, whereas@ c0nrains a no~-nult context.
proof. Let L be the language generated by ~V)LI~.
If is void, L is void too, hence it cannot be infinite.
If contains no non-null context, we have L = L I.
But ~d is in any ease flni~e ; ~hus, L is finite, in contradictiom With the h vpothesis.
Since there are contextual language which are not regular (see Proposition 4 above), it would be interesting to establish whether all contextual languages are context-free ls~guages.
The amswer is affirmative : Proposition 7.
_Every contextua~ lan?.ua~e is a context-free PrP~oof.
Let b be a contextual language.
If L is finite, it is a regular language.
But i~ is well knowm that every regular language is a context-free language.
Therefore, L is a context-free language.
Nowe let us suppose that L is infinite.
Deao~e by G = (V,,L l, a contextual grammar which generates the language L, In view of Proposition 6, L I is non-void,wheream there exists an integer i, l~ i~p, such that the con~ext ~ui,vi~ is non-nu~ Joe.
at least one of the equalities ui =co, v i =~ is false.
Let us make a choice a~d suppose tha@ ~.i ~ ~ Let L~ = {xcA,xp_, ...
9~a} and (~) ={<ultVl>, ~.,U,.~,V."y} . We define a context-free grammar ~)..@| as follows.
The terminal vocabulary of ~ is V.
The nonterminal vocabulary of ~ contains one element onlydenoted by S which is, of course, the axiom of the grammar ~ . The ter-6minal ~ rul es o f ~' are, S -->_x 1, g--~x a, S.--* _Xn whereas uhe non-terminal rules are S ---> u~5% v-i ' S ---) uqS v~, It is obvious tha~ the number of terminal rules is equal to the number of strings in ~, whereas the number of non-terminal rules is precisely the number of conuexts in ~.
Among the nonterminal rules, there is one at least which is non-trivial : it is the rule S ---> UiS vv~., where '*4 {~ " It is not difficult to nrove that the grammar ~ generates the given language L.
Indeed, the general form of a string in L__ is where yG V and <~i, V~ >E~ for s = 1,2,...,p.
In order to generate the considered string we begin by applying --Jl $imes She rule In this way, we obtain the expression h The next step consists in applying J~2 times the rule v -7-a ~,~2 s vv~.~., which yields the expression J~ Ja -Ja -J~ s Ha N 1 ))t~..., -7! l Oontinuing in this way, we arrive, after pression,~z "ia ~-i s J~-\].
J2 ~a us-1 ~,a .... .'$1 "-%'i "'" ~12 ".% " Vie now apply j.p times the rule and thus we obtain the expression p-1 steps, to the ex--Jp.~ .~!2-1 . Ja ':ll ..J.l.,.i2. u jp-1 ~ S "Ull '~i2 ....
~-l . V~p ~-l "" vi2-V-il where, by applying the terminal rule S---@ Z, the considered string is completely generated, Thus, we have proved that L is contained in the language generated by ~, Conversely, let z be a string generated by ~ . The general form of this generation involves sev(ral consecutive applications of non-terminal rules (the number of these applications may be eventually equal to zero) followed by one and only one application of a terminal rule.
It is easy to see that the result of this generation  is always a string of the form (2).
Thus we have proved that the language generated by ~ is contaiued in L, In view of the precedim~ eonsiderations, L iS precisely the language generated by ~ o Proposition 7 easily permits to obtain simple examples of .....
J -8languages which are not contextual l~guages.
For instance, ~he l~~uage of Kleene~an~ (m=~,2,...), the first example of an infinite language which is not regular, is a very simple example of ~ contextual language.
It is enough to remark that the sequence ~n2} (~ = 1,2,)... contains nO subsequaucewhioh is an infinite ari~hmebio progression ~ (We have (n+l)2-n22~+l and lira (~+i)=~, therefore for every subsequence of ~n2} the difi'erance of two consecutive terms has the limiu equal to +oo wh~ n-@ ~ ).
But a result of\[4\] asserts, among others, that given am infinite contex~-f1~ee lan~ guage L, the set of integers which represent the len~hs of the strings in L contains an infinite arithmetic progression.
It follows uhab b~Je language of Kleene is not context-free and, in view of Prooosition 7, it is not a conbex~ual language.
T~ sa-~ ~a~T ~@low* ~,~ ~h~-,~ :3.A,~.
~ \[g'J, ~,,#.
A natural question now arrises : Do there exist non-contextual languages a~ong context-free languages 7 The affirmative answer follows fro~ the following remark : The converse of Proposition 7 is not true.
Indeed, we have Prooositiou 8.
There exists a.
cont e.~-free language which is not a contextual language.
Proof. Let V = ~a,b~.
In view of a theorem of Gru~Lkl ~ ~__.-----~there exists, for every positive integer _n~ a context-free language I~ on V, such that every context-free grammar of I~ contains at least n non-terminal symbols.
But, as we can see in the proof of Proposition 7, every contextual language may be generated with a context-free grammar containing only one non-~erminal symbol.
Therefore, if _n ~ 2, ~ is not a contex~usl l~guage.
Proposition 8 suggests the natural question whe~bsr ~bere exist regular languages which are not contextual lan~ages.
The I -9answer is affirmative : Pronosition 9There exists a regular language which is not a context ual language.
Proof~ Let us consider the laugaage L = {abm-~c~a.~ n,) ~,n= =1,2,...), which was used b~ H.B.Curry \[5\], in order to descrlbe the set of mathematical (true or not) propositions.
This language is regular, since it can be generated by the rules S--> Abj Ac->Ab, A.--~ Ba, B--> CCC., G_--~ ~, C--~ Db, .D_--~ a, We shall show that .L is not a contextual language, Tndeed, let us admit that the contrary holds and let G = <V,~, ~> be a contextual grammar of L_ 2 Here, the gene_.-al form of a string in L is ~ ""-us ~I x~ -~ ...
vi= (3), wh eas (t = 1,2,...,=) where "'',Pn are arbitrary positive integers.
This means that ul,~2,... .---,_Un, Vl,Y2,-..,v ~ in the expression (3) are formed only by those elements of V whnse number of occurences in the strings of L is unlimited.
Only h satisfies this requirement.
It follows that in any string of .L both occurrences of sand the occurence of ~ are terms of the string x in (3).
But this implies that the intermediate terms between the occurrences of a are terms of x, hence we can find two strings y and, m l The string y is obvioasly .the null-string ~o the form 11~., hence " z such that,whereas z is of But m may be here an arbitrary positive integer.
Therefore, since -loX6~, it follows that ~ is an infinite se~ of mtrimgs.
This fact contradicts the assus~tion concern_tug G ! v, is ~t a contextual language and Proposition 9 is proved.
The contextual grammars may be generalized in order to generate some lauguages which are not context-free.
A generalized contextual ~r~mmar is a quadruple G =~,,L2, ~, where V, L I and ~have the same meal~g as in bhe definition of a contextual grammar, whereas J'2 is a finite set of strings on the vocabulary V.
We define the language L G generabed by G in the following way : Y~ is a language on V a~d xe~ if and only if we may e~press x in the form .where z~, y~Le, <ui,Yi>~for i : 1,2,...,n and pl,P2,...,pn, p are positive integers such that pl+P2..,~n=p. Every language generated by a generalized contextual grammar is said to be a generalized contextual lsnguage.
I~, in the delini~ion of G, we take L~ =~c~}, G is equivalent ~o a contextual grammar ! the lang,.% is then precisely the language generated by the contextual grammar ~V,LI~.In_ deed, the general form of a string in the contextual language generated by ~Y~LA, ~ is l P~a Pn Pa P2 Pl p roy ed ~roposition lo.
\]~ery oontex~ual language ~s a ~eneralized context ual lan~uaKe, llWe may consider a conte~ual grammar as a parbicular case of generalized contextual grammar, .by ideatifyimg the contextual grammar ~'~1~ with the generalized contextual grammam~,V,,~, " It is interesting to point out that somet~imes a cont~ual language may be easy generated by a generalized contextual grammar which is not a contextual grammar.
For instance, let.
us consider the l~.~e L= (~=} (~X,2,)...
. ~ ~is, or the proof of ProDosition A, L is a contextual language.
We map generate L by the generalized contextual grammar (which is not a co=textual ~r~a~) <v, h~>, where v : {~,b}, .Li_\[c~}, ~ = Ibm, ~= \[a,~ . It is known that,~_ is not reS~L%ag.
We ma~ give a similar example, wi~h a language which is regular.
In this respect let us consider the language of G~x~V~.~.
In view of Proposition 5, it is a contextual language.
It is a regular language too~ since it may be generated by the regular gramm~r contain~ ~he following two rules : q--~ Sb and S--> a.
Now let us consider the generalized contextual grammar < ~i' This grammar generates the language of Curlew, but if'is not a cent ext,~ al gran~nar.
~ow let us show that generalized contextual languages are an effective generalization of contextual languages.
Propo .sit ion ii.
Th ere _ exist s a_g en=e=~ iaed_gA~nt ext ua! language which is ~IQ~ a eon~ext~sl language, ~, Let us consider the language T, = an_b.n~..
n} (n:=-l,2,.
4 It
is known that this language is not context-free (see,or instance,66\],p.~).
7n view of Proposition 7, every contextual 12language is a context-free language ; hence~ ~ is not a con.
textual language.
Now let us consider the generalized contextual gr~m~ G = <V,~,~2,~>, .here v = ~,~, ~ ~,,~{~ and~ ~(~a>~ . It is easy to see that G generate the ieaguage L.
Yrom the proof of Proposition ll it follows immediately; Proposition 12.
There exists a ~eneralized contextual lang u_~e which is not a ~.nnteYt-f~ee language.
We may now ask whether the converse of Proposition 12 is true.
The answer is given by Proposition 13.
Th ere exists a cont ext-free~a~e~ even a regular language,~ which is.,not a generalized contextual !~ua~e.-~ P#oof.
We may consider the language L = ~sbmc_abn-} (~,n= =1,2,)... used in the proof of Proposition 9.
It was showed in the proof of Proposition 9 that L is regular.
Let us admit that ~ is a generalized contextual language.
Given a string x in L, its representation is of the form Pl P2 Pn P P~ P2Pl ~m ~ : ui u~ ....~n..~.
y'v". ...
v2v i where ~ui,vi~ ~ (i = 1 .....,n),ZG~, y~L2,pl+...+pn = p end G = ~V, L1,L~, ~ is the grsmmar of L.
By a reasoning similar to "that used in the proof of Proposition 9, we find that for every positive integer m there exists a string z in \]i I such that z = abmcab s~, where s is a non.negative integer, depending mf m.
But thls means that ~ eontain~ infinitely ma~ strips.
This fact con... tradicts the definition of a generalized contextu~ grammar.
It / L 13 follows that L is not a generalized contextual language, It is to be expected ~hat every generalized contextual language is a contex~-s~itive language.
But the construction of the corresponding context-sensitive grammar seems to be very complicated, if we thin~ to the generation of the language ~u.A.~reider has introduced a new type of grammars, called gralamatlkl) and defined i~ neighborhood ~ira~.L~ars (okrestnostnye ' the following way (\[4o); see ~4\].
Our presentation is some what different).
Given a finite set V called vocabulary, two strings x and y on V, and a context <u,v> on V, We say that the pair ~u.v>,y) is a neighborhood of y with respect to x if we can find two strings z and w, such that x=zu~vw.
Every pair of the i or~ ~<u,v>, ~\], where ~u,v> is a context on ~, Whereas y is a string on V, is called a neighborhood on V.
Let us consider an element e which does not belong to V ; G will be called the bo~3dary element.
A neighborhood grammar is a triple of the form ~ V, e,~, where V is a vocabulary, is the boundary element and ~is a finite set of neighborhoods on the vocabulary VU(e} . Let L be a l~aguage on V.
2e say that L is generated by the considered neighborhood grammar if ~i every string x of the form x =~ye (with ymL).and only in such strings there ~ists in ~, far every tera a i of X=~la2...a s, a neighborhood of a i with respect to x.
Neighborhood gray, mrs are closely related to the notion of context, since this notion occurs in the definition of a neighborhood.
There is another notion, due to Ja.p.L.Vasilevski~ and 14 ~.V.Ghom~ak6v (see ~he refermnce in~2\],p,~o), which e~lains this fact.
Following these authors, a grammar of contexts (this name is imp_roper, since no context occurs among its objects) is a triple <V, e,9>, where .V and @ have the s-me meaning as in the definition of a neighborhood grammar, whereas Q is a finite set of strings on the vocabulary Vt3e~  This grammar generates the language _L on V in the following way : x6 if and only if for every string y and a~y strings z and w for which there exist strings u and v such that @ x@ = = uzyuv we have either l) y = rasp, where sE Q, whereas the strings m and p may be eventually or 2) (~x@ = urynt, where qr = z, n t = w mad ryn is a string belong~g to Q.
A string belonging to Q is said to be closed from t~ le~ (from the right) if its first (last) term is @ . A string belonging to Q is said to be ~ if it is closed bosh from the left and f~m the right.
A grammar of contexts is said ~o be k-bounded if every non-closed string of _~ is of length _k, whereas every Clesed string of ~ is of length not greater than _kj An important theorem of Bor~ev asserts the equivalance between languages generated by neighborhood grammars and languages generated by k-bounded grammars of con~s (~3,p.4o).
Since grammars of contexts and contextual grammars have some similarities in their definitions, it is Interesting to establish more ~xac~ly the relation b~een them.
v 15 Proposition 14.
There exists a contextual language ~hioh is regular, but which is not a neighborhood language.
Proof. Let us consider the language L = ~a~n~ (n=l,2,...).
This language is regular, since it is generated by the regular grammar consisting in the rules S ~ ~a, T--->Ua, U--->Ta, --->a, where ~ is the start symbol, La~ is the terminal vocabulary, whereas {S,T,U} is the non-terlainal vocabulary.
Let us consider the contextual gramnu~r G =~ {a},{CO}, {~a,a>~.
I@ is easy to see that G generates the language ~ $ therefore L is a contextual language.
We shall show that L is not a neighborhood language.
In this respect, our method will be the following.
We shall consider all systems of possible neighborhoods of the terms of ~he string 0aae and we shall show t~}at every such sysbem is either a system of aeighborhoods of the ~erms of every string Cane (n= = 2,3,4,)... or it is not a system of nei@\]borhoods of the terms 0t the string ea@e . It is easy to see that the first ~erm of the string @aa@ admits ~he following neighborhoods : 1)e, 2) Ca, 3) @aa, ~) eaa~ . The second term has the neighborhooas : l) G_a,~)-a~) aa, 4) ~e ., 5) e_a_a, 6) e_~ae.
The neighborhoods of the third term are : i) e@a, . 2) aa, ~) a, ~) _ae, 5) eaa8, 6)_aaE) . The lass term has the neighborhoods 1) 8, 2) _a~_, 3) a a@_, 4) @aa~ . The noration _u_xv.
represents hier the neighborh~d {<u,v>,x} . It is easy to see that the fourth neighborhood of the firs~ and of the lass term c~t b'e a neighborhood of e with respect @o @g48 . On She other hand, a is a neighborhood of .aa with respect to ea~@ for every n = 1,2, ....
It follows that no 16 neighborhood grammar of L = ~a2ZX 3 may contain one of She neigh.
borhoods _0a2@, Q a2~ and a.
Thus, if a neighborhood grammar of \]~ exists, it contains at leas~ one neighborhood from every group of the following four groups of neighborhoods : ~) _0, _~a, _ea 2 . ~) e_a, _aa, _aae, G~a, ~.aaO.
b') 6~-~, aa, ..aO, ea_a~, agO.
We shall consider all possible combinations betweau a neighborhood of the group ~ and a neighborhood of the group E . By mn we shall denote the combination formed by the m.th neighborhood of ~ and the n-th neighborhood of ~ . It is easy to see Chat every neighborhood grammar containing one of the combinations 12, 22, 23, 25, 42 generates a language whioh eontain~ every string a n with n $ 2.
On the other hand~ every neighborhood grammar containing one of the combinations ll, 13, 14, 15, 21, 24, 31, 32, 33, ~, 35, 41, 43, 44, 45, 51, 52, 53, 5~, 55 generates a language which either does not contain the string a 4 or contains every string a n with n~ 2  (This depends on the fact if the neighborhoods aa or aa belong or not to the considered neighborhood grammar).
Thus, there exists no neighborhood grammar which generates the language ~2n 3.
But the definition of (generalized) contextual grammars, though adequate to the investigation of the generative power of purely contextual operations, does not correspond to ~he situation existing in real (natural or artificial) l~guages, where every string is admired only by some contexts and every o~u~ / 17admits only some strings.
Let us try to obtain a type of grammar corresponding to this more complex situation.
We define a con___y textual grammar with choice as a system G_ =<V,L,~,~o>, where V, L1 and~are the objects of a contextual grammar, whereas is a mappi~ defined on the universal language on V and havi~ the values in the set of subsets of~.
We define the language generated by G as the smallest language L having the follow1  ~ L l x ~ L 2  ing properties : If x, ! If ye L, <u,y>6 ~(y) and Z&~l, then u~L, z v~L and ~L.
Thus, every strin~ chooses some contexts and every context chooses some strings.
We define a contextual language with choice a language which is generated by a contextual grammar wit~oioe.
The investigation of these grammars and languages would better show the generative power of contextual operations, in a manner which corresponds to the situation existing in real languages.
--~$ References i.
Y.BAR-HILLEL, ~I.PEEL~, E.SHA~R : On formal properties of simole phrase structure grammars.
Zei~schrift fur Phonetik, Sorachwissensc~aft und ~ommunikatio~forechung,vol.14,1961, p.145-172. 2.
V.B.BOP~EV : O krestnostn~e gram~atiki.
Nau~o-Techni~eskaja Informacija, Serija 2, 1967, ~o.ll, p.39-41. 3.
N.GHO~SKY : Three models for the description of language.IRE Transactions on Information Theory, IT-2, 3,1956, p.ll~-12@. 4.
N.~{O~KY : Syntactic Structures ~ Gravenhage,1957.
5. H.CURRY : Some logical aspects of ~ra~matical structure, proeeedings of the Symposium in Applied ~athematics, vol.12, S~ructure of language and its mathematical aspects, Amer~ath.
~oc., 1961, p.56-68. 6.
S.GI~SBURG : The mathe~atical ~heo~f context-free languages.
~cGraw-Hill Book Company, New York, 1966.
7. J~XA : On a classification of context-fre~lauguages.
Kybernetika, vol.3, me.l, 1967, p.22-29. 8.
S.~CUS : Gramatici ~i automate finite.
Editura Academiei R.P.R., Bucure~ti,196@.
9. S.~AR~JS : Su~_ les grammaires & un hombre d' ~tats fini.
Ca. hiers de lin~uistique th@orique et appliqu~%vole~,196~,p,~W~-~&@.
io, Ju,A.~REIDER : 0krestnos~naja model jazyka, Tru~ 8impoziuma pO primenenijam poro~.daju~ioh grammatik, Tar~u,septjabr~1967.
ii. Ju.A.~REIDER : Topologi~eskie mod~ll dazvka.
Vsesojuzz~yi stitu$ nau~noi i techni~eskoi informacii, ~oscou,1968 .,., , SERGE BOISVERT ANDI~ DUGAS DENISE BI'LANGER OBLING: A TESTER.
FOIL TR`ANSFORMATIONAL' GKAMMAKS ~.
INTRODUCTION Transformational grammars have developed with recent research in linguistics.
They appear to be a powerful and explicit device for characterizing the description of sentences; they also meet conditions of adequacy that can be applied to check that a sentence, or a set of rules, is well-formed.
A transformational grammar tester is part of a strategy for the selection of a well-formed grammar matching the data base.
To put it more explicitly, a tester of this sort should provide the linguist a class of the possible grammars which concerns precisely the linguistic theory.
These grammars have the form given by CUOMSKY in Aspects (see bibliograIShy).
2. GENERAL DESCRIPTION OF THE SYSTEM O~UNO is a .program for dealing with the structures of the French language: it performs the verification of phrase structure rules, the derivation of sentences according to the transformational component and the graphic illustration of the intermediate or final structures.
In the program, UNG is the routine that controls all the subroutines and the matching of the input structures with those allowed by the phrase structure rules.
If the matching is impossible, a comment is Acknowledgments.
This work was supported in part by Canada Council of Arts grants @69-0404 and @71-0819.
We are also indebted to the staff of the Computer Center of the Universit4 du Qu4bec ~ Montr4al for providing computing facilities, and giving this project high priority.
David Sankoff, of the Universit4 de Montr6al, is also responsible for the first version of the tree editing program, l~inaUy, Jossdyne G4rard helped debugging linguistic rules.
122 SERGE BOISVERTANDIL~ DUGASDENISE BI~,/ANGER ieeIIIItiIeletlIillllllelIlIlIllll~ : o :.
-*:: Iieeee~ ...........................~> ...........................9= ~z *,0,~o : .....
~Z ...................
~g o D~O o aO OUd 3N1 z o..~ ~ 3z O.
~ Z o k9 bO.
~3J ATd o:r =:~ o 0~3 qD_~  :: u.
0. tk NNN IOH U~ ~ Uj~ (~ ~ZD~-Z NUd ~dO NWZDU~ Fig.
1. Tree for an input sentence OBLING: A TESTER FOR TRANSFORMATIONAL GRAMMARS 123 made.
Otherwise, the output gives the graphic illustration of the tree for this sentence, or the input structures are immediately processed using transformational rules.
For example, General transformational rules are operated by a number of subroutines of which the main are explained hereafter.
3. GENERAL CAPACITIES OF THE SYSTEM The system OBLING is divided into four parts: a main program LING, the service subroutines, the phrase structure grammar tester and the transformational grammar testers.
LING and the service subroutines are stored in the central memory while the two grammars testers operate on disks.
The main program invokes the various linguistic rules and controls the application of these rules to the data base structure(s) or the derived structure(s).
The service subroutines are called by the routines concerning the application of the transformational rules and work in parallel with LING during the processing.
Phrase structure grammar tester " LING Service I Subroutines (processing memories) 4--1~ ' grammar testers Fig.
2. The OBLING system 4.
SPECIFIC CAPABILITIES OF THE PROGRAM LING The program LING will first initialize the working areas.
Then, it loads and operates the program V~rlCATEU~ which, after the reading and the verification of the input data, returns control to LINe.
124 SERGE BOISVERTANDI~ DUGASDENISE BELANGER ZING will then load and execute, using an overlay technique, the small control programs cYcH Q1, CYCLI Q2 ....., cYcLI Qi.
Each of these handles, in conjunction with HNG, the mapping on the input structure of a fixed number of transformation rules.
In the current version of the program, cYctI Q1 deals with the linguistic transformational rules T1 to Ts included, cYcrI Q2 the rules To to T10 included, etc.
The total number of these control programs cYcrI Q depends on the memory space allowed; processing is most efficient if the number of these control programs is as small as possible.
5. INFORMATION PATTERN BETWEEN LING AND VERIFICATEUR When VERI~CATEUR (the phrase structure grammar tester) is in memory, the structure to be analysed is read from the standard input unit (punched cards) and is processed by the subroutine Cr~RB~ to LING v V~ICATBD~ c'zcLi qO  T CKARBRE ARBRE verification printing of syntagmatic of the tree rules Fig.
3. The Vm~L~CAWSUa program (see figure 1 for updated tree and structure) OBLING: A TESTER FOR TRANSFORMATIONAL GRAMMARS 125 be validated.
This subroutine first checks if the phrase structure is consistent, then calls up eke which tests the names of the constituents describing the structure; finally, it compares this structure with those allowed by the phrase structure rules.
When errors are discovered during the processing, various sorts of comments are printed and followed if possible by a partial or full tree of the sentence.
When updating is done, the tree is printed and the program VERrFACATEUR passes control to LING.
The following illustrations concern first, the program VERIHCATEUR and second, an example of an updated tree and structure.
6. INFORMATION PATTERN BETWEEN LING AND THE TRANSFORMATIONAL GRAMMAR TESTERS Each time LING receives the control from VERIFICATEUR, that is, when no further errors have been detected, it loops in order to call successively the monitors CYCLI ql ....., CYCLI Q9 which contain up 45 different rules; we suppose that we are working now with a specific version of a grammar.
The first of these monitors has the following structure.
Transformational rule # 1 Transformational rule # 2 Transformational rule # 3 Transformational rule # 4 Transformational rule # 5 Fig.
4. The cYcLI Q1 program When CYCLI Q1 gets control, it is botmd to the application of 7'1,  .., Ts which correspond to the first five transformational rules; then control is switched to LING which calls cYCLI Q2.
The programs CYCLI qn process cyclic rules and the output structure is the input structure for the following rule.
When all the cyclic rules have been applied to the input structure, LING starts over again at CYCLI Q1.
If no modifications 126 SERGE BOISVERTANDR~ DUGASDENISE BI~LANGER to the already processed structures occur, or if new errors are discovered, control returns to LING.
After all the cyclic rules have been applied, the post-cyclic rules are processed in a similar manner: cYcu qA comprises the first five post-cyclic rules CYCLI Q~, the five following, and so on.
This chart illustrates the general interaction between the programs for the processing of cyclic or post-cyclic rules.
I_ cYcI.t Q1 cYcu Q2 CYCI, I Q9 CYCLI QA I CYCLI QB cYC~i Qi I End Fig.
5. Flow of Control between control programs under the direction of LInG 7.
SERVICE SUBROUTINES They are implemented within the main monitor ZING.
All but a few of these subroutines are called during the execution of the routines corresponding to the 88 rules, that is during the phrase structure analysis or the mapping of n structures.
A short description of the main subroutines follows: ^R~ (tree).
This subroutine is responsible for printing the tree.
At the input, we find a vector D of ND elements which represents the tree.
The horizontal distance for printing is calculated along with the total number and the relative position of these nodes; the vertical one is fixed.
OBLING: A TESTER FOR TRANSFORMATIONAL GRAMMARS For example, fACHE~ 2 # NOMRRE DE NOEUDS NOMBRE DE NOEUDS CHA/NE 1 = $ O(.
I) 2 = LE D( 21 3 = N D(3) 4 = V O( ~) 5 = $ D( 5\] 6 = $ D( 6) 7 = PRP D( 7) 8 = OIJE O( 8) g : LE O( 91 I0 = N D(IO) 11 = V O(ll 12 = $ D(12) 13 : DET D(13) 14 : DET 0(14) 15 : GN D(iS) 16 = GV D(16) 17 = C D(17) 18 = GN O(18) 19 = GV D(lg) ~0 = P O(~O) ~I = P D(21 21 TERMINAUX = 2'0 = 13 = 15 = 16 = 20 = 21 = 17 = 17 = 16 = 18 ) = Ig = 21 = 15 = 18 = 20 = 20 = 21 = 21 = -I ) = -I 12 ARBQRESCENCE NON PRODUITE SUR DEMANDE : AUCU~E REGLES IGNORFES SUR DENANDE t AIJCUNES Fig.
6a. Representation of the tree in memory 127 FIW~ f~ 7.1.
OT~ (Remove).
This subroutine is needed when nodes are erased; another subroutine, NEWTgF_~ will erase the nodes.
In the example below, oxv sets D(6), D(7), 9(13) to zero, and NEWTREE erases nodes numbered 6, 7 and 13.
If node 12 was also erased, OT~ and NEWT~E would have erased node 28 automatically.
The same holds for the node 32, where all the nodes between 6 and 13 would have been erased.
7.2. DFER, DFERX, GFER, GFERX.
Except for a few details, these four subroutines do the same work.
For example, Dr~RX \[I, J\] is applied to a structure J that has to be moved to the right under a dominating node L As illustrated below, Dr~Rx \[31, 30\] moves the structure headed by 30 to the right under the node 128 SERGE BOISVERTANDRE DUGASDENISE B~LANGER ....................~.
0":> ~ IJ-: .....
~g ............. 9~ : : ............
~ m : .....
~g ............
~ ~ ........... ..~z ~ : .....
~ ~w ....................~.
~Z~ ZU wz mz~wz ~ ~z~z m Fig.
6b. Corresponding printed tree OBLING: A TESTER FOR TRANSFORMATIONAL GRAMMARS 129 .......,.......,..,.....~........,.....~.~,     ,cu~ W~ud ~ : ".
...........o...............~ ....................~.
=> .~> g~ .............~z ~z ow .....
'.~ : ~J ......
~ : .....
~ ............. 9~  o ....~......,..................,...~> Fig.
7a. Sample tree before OTE and NEwra~ apply ~z~wz ~z~wz wz wz~o ~ w O~d 3NI II O~Do H~N ~ZD~ ION N~d dOD HN  NH" JI  + o=~Do 06d II ~NN ' I0~ O~DO 3~d dO0 ~AV ~I" ~ ~zz~ dN" c~o 130 SERGE BOISVERTANDP~ DUGASDENISE BELANGER,........,......,.......,................~, 8~ U oo.o o~ .oo*o--~ : ............
~ ...........................~ elost$ eset0 t eQ o o  " "  " o   o   "Z      oJo :     oz      .~ : ..... h~o :e ...... go ............
~ o . ~  --~ o.,' .......
., ...........  ....
.> m .....
0 ....  ........,..*0o2 ~z **.~,..,oo, o,,o-,o,~o.,,..,....o....,.oo.,~ ~ q~L0 tn>uz NgCW {gh.w DX~ 02 ~uu 08d JNr a.=~..=oc :~ o=II cz Frld 0~3 u~ .J m snN ION ado 0.
~--u o.
3Z :~d03 ~ 0.
N, u~-,x~..~h.Z NO" ~cxo..~ -~wl     oliN"  ~NI cc~ x'W-Jcc II Bo.
o_ ngd ~D~>O_:E0C GND r-o.J ~ :g uJ o_ HnN : o.
i.-.~ ~ u.
x: |ON H~d,o 3~d ~dO :g :~ ~ 0 dO3  ~ uJ .J~c EZZo.
~zzo--I + O~D W W Z D bJ .J O: _J,Xb.C~O.
Fig. 7b.
Sample tree where oT\]~ and N~WTREB have applied OBLING: A TESTER FOR TRANSFORMATIONAL GRAMMARS 131 31.
(Node 31 was created by rule T2 and DF~R was applied on the resuiting tree) \[I,/\] DFERX \[;r, j\] GF R \[1, J\] G RX \[I, j\] makes node J the next younger brother of node I makes node J the youngest son of node I makes node J the next older brother of node I makes node J the oldest son in node I The general technique for these four subroutines is the following.
Before modification, the tree is copied in another area of memory.
All the terminal nodes identified with the structure J take the place of the terminal nodes identified with the structure L Then, the terminal nodes of I are requested in the copied structure and parsed with their dominating nonterminal nodes at the right place.
Gr~R,permits the new numbering of the sequence and, if necessary, prunes the tree.
In the example illustrated below (Fig.
9a and 9b), Gr~R \[14, 13\] is applied and node CPL (13) has been attached to node 16, the father of node 14.
If GF~RX \[14, 13\] had been specified, node CPL would have been attached directly to node 14, rather than the father of node 14.
7.3. INTERV, This subroutine is used for the permutation of 2 structures.
For example, INT~RV.
\[I, J\] where I = 24 andJ = 28 gives rise to the structural change illustrated below.
7.4. INSERT.
This subroutine is used for the insertion of a new terminal node; for example, INSERT \[4, 1HE, 1HT, 1HR, 9\] introduces node with name ETR which becomes a new son of node 9.
7.5. Other subroutines.
There is a number of other subroutines concerning conditions specified within a rule, such as the presence or absence of a node or of a feature in the whole structure, the logical restrictions of loose equality or strict equality.
132 SERGE BOISVERTANDR~ DUGASDENISE B\]~LAlqGER ..................................S.
.o*~*~oi~*~*~Z ,~,..~~ ~W ~,*...**~  ~W :~.**~ ..................................~.
..................................~ .-~z : ...................
~.~ oi ............ ....~.,.,.,..~.........>   .  ......
                * .z ......
~_~ ~ : ~>~ ~Z~W 0~td O~ ~ g ~ 3NI II 177d N~N IOH H~d =l~d ~ldO  ~wz~w~ o .-INI  o0N33 flTd IOH un bJ z ~ h2 .-J ~ N~d ~o.o_ Od0 4" c3~ a~ zza-~x :~zo~co dO3 ~..~-a:Q.~ .-e~   j   ...aAV m~w~2w OFig.
8a. Sample tree before GFER applied OBLING~ A TESTER FOR TRANSFORMATIONAL GRAMMARS .........................................~.
d8 ......
~ *~ ...... g~> ............
"~> : .............
~z ......
~g  :.,.,.~..,. o,~ .~ : --z      m(  o      ~o ; .,-w .     . ,   .   ,.j ....................~,~: .~ ............
~.~ ......................0...........~.
.,..,,~.,~,,,,~,~,..,,,~...,..~ ......
~g ~  ,~ 0 , ., , .,., m m ,,,,,,,,,,,,,~ 133 =>~ ~z ~z~z ~z~z~o rOBd  4NI,o II H3.-I + HAN oz~o lOW 3~d ~ 0.
o o.
(~ < P..~ o(J =0 z dOO . ~IAV 30" eU NO" HN ~ * 3NI o~x~o II 0~ Ld z ~ W,J ~' .J  :~ h.
(l. ~ 1~3_4 fVld ~AN ION O~Z~O H~d hJ Z ~ ~J.Ja: ~"~  ~ h.
a.o. 3~d ~dO dO3,O ~AV 30"+ o~-~o NO'* .-I  ~ u.
0. 0.
HN" NH" Fig.
8b. Sample tree where Grsa has applied 134 SERGE BOISVERTANDRI~ DUGASDENISE B~LANGER .....................,,......,..........9~ :...................9~  ~ ~ m ~Z o O~ld JNI II nqd c k~RN e... ozc,~,~.~ zw~g 3aa ~dO 0=~ dO3 . ~ t,.
cL cL ~AV 30" Q.
t-.(.,.~ O.
~2 --)-Z NO" O.
C30. J ~ v.J 0 J ~ ~ HN* NH" ~O_ N  U..~ .:~oAI" e3 oz~ ha~ zh4 ha Fig.
9a. Input structure OBLING: A TESTER FOR TRANSFORMATIONAL GRAMMARS 135 ~i.
i le~eee ee$ lse eee eeeee~ e,e,eee,ee,,,~,,,le,e,~Z ~z ...........-.~z II  -g .......
~ ~ ......
~ , ~ ,1 ,, ,..
 ~ ~, , .,   ,.
1~ ozJ~z 2 08d II ~z~ W33 ~z~wz ~Id WnN 8~d 3~d HdO dO3~AV 30  NO" Fig.
9b. Output structure after cFm~ has applied 136 SERGE BOISVERTANDI~ DUGASDENISE BIILANGER ...................,.....................~, Ii J .~ : ....
~ ....... o~  ~~ ...........
~ ~ : ......
~  ~: .....
~ : .~ ~  ...
.,~.0,~... mz ~,~,,,,,,',,~,~,.,,,,,,,.~ Fig.
lOa. Input structure O~d 4N\] TT nqd HnN zoH HHd * dOC) 0~:~0 ~IAV NO" _~I.
'-)~_ N  b.~o:~ no" O~d JNI II N3J Flqd HnN IOH ~d 3~d ~dO OBLING: A TESTER POR TRANSFORMATIONAL GRAMMARS @ @ i@ Ii @@I@ @@@ @Ii@ I@ i oII iI @ iI i@I @Ii@ kU II~leo~$e~Ioo$oi@$~ ...... g ............. o,.~ ......
~g .....
~ ............
R~ ~d I M! ~ed .,,....,.,,..,,,~z o~ ...~..No .,.,~.,.....,......,,.
..................................~>,~..,,.~z . "-~ "''   "  "''''  "" "" " "" "  ''~ .....,..,,..,...........,...,.-,, .... ..~ 137 ~ ~z~o  m2~Nz m + O~DO  m oz~w zww~ 08d ..4NI II F mqd 083 k(nN IOn k4~d 38d 8dO dO:) SAY 30" NO" 8N  NH" 4IFIO" dN" 08d 3NI II H3../ f)ld 083 HNN ION HSd 38,:t tldO dO3 dAY :)0"4.
NO., NN" NH" AI" NO-~dN" Fig.
lob. Output structure after ~r~v has applied 138 SERGE BOISVERT ANDl~ DUGAS DENISE B\]~LANGER :...................~ u_ ,,*,0,,o$1,$1,,, ............~z >.
m ~Z .,-,,, : ~ "...  . . .............~, + oxx~O D.
Fig. 11a.
Insertion of a node (before) OBLING" A TESTER FOR TRANSFORMATIONAL GRAMMARS 139 .   . . ..
 ,,.........~.,,,,......,.....~.=.
***eeeoeo$ooe~ o e~ x tn mo~ o~ ~Z~WZ ~S~W m   Fig.
1lb. Insertion of a node (after) 140 SERGE BOISVERTANDRI~ DUGASDENISE BELANGER 8.
CONCLUSIONS OBJ-mC is a system which has been implemented in low-level tORTeN IV for the CDC 6400.
It occupies 55,000s 60-bit words of memory.
It has about 7000 lines of comments and instructions.
REFERENCES N.
CHOMSKY, Aspects of the Theory of syntax, Cambridge (Mass.), 1965.
A. Ducas, et al., Description syntaxique ~l/mentaire du franfais inspir~ des th/ories transformationnelles, Montr6al, 1969.
J. FIUEDM~, A Computer Model of Transformational Grammar, New York, 1971.
D. LIEBERMAN, (ed), Specification and Utilization of a Transformational Grammar, Yorktown Heights (N.Y.), 1966.
D. L.
LotrD~, W.
J. SCHO~N~, TOT: A Transformational Grammar Tester, in Proc.
Spring Joint Computer Conference, Part I, 1968, pp.
385-393. R.
PETRICK, A Recognition Procedure for Transformational Grammars, Ph.-D.
Dissertation, Cambridge (Mass.), 1965.
J. R.
Ross, A proposed rule of tree-pruning, NSF-17, Computation Laboratory, Harvard University, IV (1966), pp.
1-18. A.
M. 7.WICKY, J.
FRIEDMAN, \]3.
HALL, D.
E. W.~a.g.F.R, The MrrRE Syntactic Analysis Procedure for Transformational Grammars, in Proc.
Fall Joint Computer Conference, Vol.
27, Pt.
1, 1965, pp. 317-326 .
S$ I:Mct ur a 1 Cor r e AponfJet\]Ge S~ec \[ f I c_a t j O0_#DvLEonmer~ Yongfeng YAN Groupe d'Etudes pour la TradlJctlon Automatlque (GETA) B.P. 68 Unlverslty of Grenoble 38402 Saint Martln d'H~res FRANCE ABSTRACT This article presents tile Structural Correspondence Speclflcatlon Environment (S('SE) being Implemented at GETA The SCSE is designed to help linguists to develop, consult and verify the SCS Gr'alt~nar s (SCSG) which specify I lngulst ic models.
It Integrates the t eclln 1 clues of' data bases, structured edltors and language interpreters.
We argue that formalisms and tools of specification are as Important as the specification itself.
z NT ROD_UCT tqN For quite some time, It has been recognized that tile specification Is very important in tile development of large computer systems as well as the linguistic computer systenls.
But it ls very difficult to make good use of specification without a well defined formalism and convenient tool.
The Structural Correspondence Specification Gran~ar (SCSG) is a powerful linguist ic specification Formalism.
the SCSGs were ftrst studied in S.Chappuy's thesis (1}, under the supervision of Professor B.
VaLIqUOt s.
In their paper presented at Colgate University in 1985 {6} SCSG was called Static Greener, as opposed to dynamic grammars which are executable programs, because the 8CSG aims at specifying WI4AT the linguistic models are rather than IIOW they are calculated, A SCSG describes a llnqulstlc medel by specifying the correspondence between the valid surface strings of words and the multi=level structures or a language.
Thus, from a SCSG, one can obtain at the same tlme valid str lngs, valid structures and the relat ton between them.
A SCSG can be used for the synthesis of' dynamic gralr~}lars (analyser and generator) and as a reference for large linguistic systems.
An SOS Language (SCSL) has been designed at GETA, tn whlcll the SCSG can be \]lnearly written.
The SCS Environment (SCSE) presented here ts a compLIter aided SCSG des lgn system.
I t wl 1 1 al low lhlgulsts to create, modify, consult and verify their granlnars in a convenient way and therefore to augment their productivity.
Sect 1on I gives a outline of the system: Its architecture, pr Inciple, data structure and comdnand syntax.
Section II describes the malrl functions of the system.
We conclude by gtvtng a perspective for luther developments el' the system.
I=.AN OVERVIEW OF TI4E S YSTE_M 1.
ARC HI\]EC T URE The SCSE can be logically divided tn five parts: 1 SCSG base 2.
monitor 3.
input 4.
output 5.
procedures The SCSG base consists of a set of files Contalnlng tile grarrlnars, lhe base has a hlerarchtca\] structure.
A tree form directory describes tile relationship between the data of the base.
The monitor Is the interface between the system and the user.
It reads and analyses colTinands from the input and then calls the procedures to execute the cormlands.
1he input is the support containing the COlrrnands to be executed and the data to update the base.
rhere is a standard input (usually the keyboard) from which the data and cormlands sllould be read unless an Input ls explicitly specified by a conlnand.
The output is a supper t receiving the system's dialogue messages and execution results.
There is a standard output (usual ly the screen) to which tile message and results should be sent unless all output Is explicitly specified by a con~and.
The procedures are the most irnportant part of tl~e systenl.
It ls the execution of procedures that carries out a COn~land.
The procedures can communicate directly wtth the user and with other procedures.
2. THE_E.RJNCU}LE An SCSE session begins by loading the original SCSG base or the one saved from the last Session, Then the monitor reads lines from tile com~nand input and calls the corresponding procedures to execute the COmd~lands found.
When an SCSE session Is ended by the colm~and "QUIT", the current state of tlqe base Is saved.
The SCSG base can only be updated by the execution of c omrlland s, The original SCSG base contains two SCSGs : one describes the syntax of the SCSI_ and the other gives the correspondence between the directory's nodes and the syntactic units of the SCSL.
The first gralmlar ls read-only but the second one can be modified by a user.
This allows a user to have his prefered logical view over the base's physical data.
These two grammars serve also as all Oil-line reference of the system.
Several Interactive levels can be chosen by the user or by the system according to the number of errors in the con~aapd lines.
The system sends a prc~npt message only when a "RETURN" ls met in the CO~nand lines.
So gee carl avoid prompt messages by entering several cen~nands at a time.
;3. DATA S:\[f~UCTURE There are two data structure levels.
The lower one Is linear, supported by the host system.
Tile base Is a set of files containing a llst of strings of characters.
Tile base carl be seen as a single string of characters thai: Is the concatenation of all lines tn the ft\]es of the Llase so that tile structure is said to be llnear.
TIlls structure is the physical structure.
The higher one Is hierarchical, defined by the directory of the base.
Tile base is composed of a number of SCSGs ; each gral~ar contains a declaration sect Ion, a rule (chart) sect Ion...
etc. and the components of a gran~nar (declarat 1Ol1, rules . . . etc, ) have their own structure.
The hierarchical structure ts the logical structure of the base.
The directory has a tree form.
A node In the tree represents a logical data unit that ts its content (for instance a gran~nar).
Every node has a type and a list of attributes characterlslng the node's content, rhe lnternode's content is the composition of those of its descendents, \]he lear's content Is directly associated 81 with a physical data unit (a string o1' characters).
The following figure shows the relation between the two structures.
LOGICAL STRUCTURE (i) 7, 2Y LOGICAL S'\[RUCTURE (2) language date \[Grammar English -----i node type attributes The directory is slmllar to a UNIX directory.
But In our directory, tile leaves do not correspond to Flies but to loglcal data units and Furthermore an attribute list is attached to each node.
The correspondence between two structures is maintained by SCSE.
We shall see later that this organlsatlon allows a more efficient Information retrieval.
It ls possible For" users to have access to the data by means of both structures.
The logical one Is more convenient but the physical one may be more efficient in some cases.
4:~ _COMMANp__SyNTAX The general command format is : <operator> <operand> <options> The "operator" is a word or an abbreviation recalling the operation of tile colmland.
The "operand" is a pattern giving the range OF the operation.
The "options" is a list of optlonal parameters of the COw,land.
For example, the Con~nand : V GRAMMAR ( LANGUAGE = ENGLISH ) visualizes, at the standard output, all the English grammars In the base.
Here V is the operator, GRAMMAR(LANGUAGE=ENGLISti) ls tile operand pattern and no option Is given.
The operand being mostly a node in the directory tree, the pattern is USUally a tree pattern.
When the pattern matches a subtree of the directory, the part that matches a specially marked node Is the effective operand.
The pattern is expressed by a geometric structure and a constraint condition.
The structure ts a tree written in parenthesized form perhaps containing variables eacll representing a tree or a forest.
The coeditlon Is a first order logic predicate In terms of the attributes of the nodes occurring in the geometric structure.
More sophisticated conditions may be expressed by a predicate combined with geometric structure to efficiently select information from the base.
Pattern writing should be reduced to a minimum.
In the abeve example, the geometric structure is shnply a grammar type node and the constraint is the node's language attribote having the value= Erlgllsl\].
The use of a current node tn the directory allows not only the simplification of pattern writing but also the reduction of the pattern matching range.
The effective operand becomes the new current node after the execution of a command.
II. THE MAIN FUNCTIONS We shall Just descrlbe the functions ttiat seem essential, lhe functions may be divided Into four groups= 1.
general 2.
SCSG base updating 3.
SCSG base inquiry 4.
SCSG verification.
_1 ~ _GI~ t>\[E__R A L _F U_N__C__T._I.D_N S These functions Include: SCSE session options setting, the system's miscellaneous lnformatlon inquiry and access to host system's commands.
The following options can'be set by user co,hands: 1.
tnteractivtty 2.
dlalogue language 3.
auto--verlfilcatlon 4.
session trace 5.
standard Input/output.
One of the 4 Following Interactive modes may be chosen: 1.
non-interactive 2.
brief 3.
detalled 4.
system controled.
In non-interactive mode, no question is asked by tile system.
An error con~and Is ignored and a message will be sent but the process continues.
In brief mode, the current accesslble command names are displayed when a corm, and Is completed and a RETURN in the command lines is Found.
In detailed mode, the functton and parameters of the accessible commands are displayed and 1F an error ts Found in the user's Input data, the system will diagnose it and help him to complete the command.
A prompt message ls sent every time RETURN is Found in the COn~nand lines.
In the system controlled mode, the lnteractlvlty Is dynamically chosen by tile system according to the system=user dialogue.
For the tlme being, only French is used as the dialogue language.
But the mu.ltl-langueage dialogue is taken tnto account tn design.
It is simpler In PROLOG to add a new dialogue language.
The auto-verification option Indicates whether the static coherence (see 4.
SCSG verification) of a gra~nar will be verified each time it ls modified.
The trace option is a switch that turns on or off the trace of the session.
The standard Input/output option changes the standard input/output.
Some Inquiries about the system's general Information, such as the current options and directory content, are also ~ncluded in this group of Functions.
The access to host system's co~Ylands without leaving SCSE can augment the efficiency.
But any object modlfted out of SCSE is consided no more coherent.
2. SCSG BASE UPDATING This group of fiuectlons are: CREATE, COPY, CHANGE, LOCATE, DESTROY and MODIFY.
\]hey may be found In all the classic editors or file management systems.
The advantage of our system is that the operand of commands can be specified according to the logical structure of the base.
For example, the col~nand : DESTROY CI4ARTS(TYPE=NP) Destroys all the charts which describe a Noun Phrase.
82 The SCSE has a syntaci Ic editor that knows the logical structure of the texts being edited.
Ihls editor Is used by tile con'Jnands MODIF and CREATE.
The command CREA1 <operand> <options> calls the edltor, creating a logical data unit specified by tile operaod.
If the interactive option ts demanded, the editor will guide the user to write correct ly according to the nature of the data.
Following the same tdea of different interact lye levels, we try to improve on tile classical structural editor, Per instance that of Cornell University \]\[5}, so that one carl enter a piece of text longer than that prompted by the system.
If the interactive option Is not demanded, one Just enters into the editor wlth an empty work space.
The CO~T~nand "MODIF <logical unit>" calls the system's edltor with the logical data untt as the workspace.
The data ill the workspace may be displayed In a legible form which reflects Its logical structure.
The mul t l-w \]ndows facll ity of the editor makes it possible to see simultaneously on tile screen the source text and tile text In structured form.
The SCSE editor inherits the usual editing con~llands from the host editor.
Thus one can change all the occurrences Of a rule's name fn a grarrnlar without cilanglng the strlngs containing the same characters, using a loglcal structure change : C NAME(type=rule) old name new _nan/e, while tile physlcal structure command : C/o 1 d..
name/new .name/* * changes all the strings "old_name" In the workspace by new name.
When an obJect's deflnltloo Is modified, all Its occurrences may need to be revised and vice versa even if the modification does not cause a syntactic error.
A structure location command flndlng the definition and all the occurrences of an object can be used In this case.
Only tile logical units defined in the directory and the SCSL syntax can be manipulated by the structural COrr~land s.
SCSGBA=SI~_INQUIRY These functions allow users to express what they are interested ill and to get the Inquiry results In a legible form.
A part of the on-llne manual of usage in the form of SCSG may also be consulted by them.
The operand patterns discussed above are used to select the relevant data.
The operator and options of co~nands choose the output device and corresponding parameters.
A parametered output form for each logical data unit has been defined.
The data matching the operand pattern are shaped according to their output form.
The data may of course be obtained in their source form.
One may wish to examine an object at different levels (e.g.
Just tile abstract or some comments).
The options of the con~and can specify this.
If one Just wants to change the current node in the directory for factlltatlng the following retrieval, the same locating co~nand as before may be used.
4. SCSG VERIEICAT#ONS.
Two klnds of verifications may be distinguished : static and dynamic.
Tile static verification checks whether a grammar or a part of a gra~nar respects the syntax and semantics of the formalism.
The dynamic verification tests whether a given gran'mnar specifies what we want It to.
S tatlc_ve, r Ifica~ton All internal representation of the analyzed text ts produced and used by the system for structural manipulation, the analyser may produce a list of cross references of = nameable objects and a list of syntaxo-semantlc errors found In the text.
The exemples of nameable objects are the charts, tile macros, the attributes.
The list of cross-references reveals the objects which are used but never defined or those defined but never used.
A chart may refer to other charts.
This reference relation can be represented by an oriented graph where the nodes stand for a set of charts.
A hlerarciltcal reference graph is often given before writing the charts.
A program can calculate the effective graph of a grammar according to the result o analysis and compare It with the given one.
The cornlland options may cancel the output of tllese two llsts and the graph calculat Ion.
The graph calculation may also be executed alone.
One of optlons Indicates whether the analysis wtll be Interactive.
D.y.n ~!# J c.
v. ~gr :1 f i canon Tile dynamic verification Is tile calculatlon of a subset of the st ring-tree relation defined by a gr altrnar.
A member of the relation is a pair <string,tree>.
Ti)e command gives the granYnar and the subset to be calculated.
The subset may be one of the four following forms : I.
a pair with a given string and a given tree (to see whether It belongs to the relation) 2.
pairs with a given string and an arbitrary tree 3.
pairs with an arbitrary string and a given tree 4.
all possible pairs rhe calculation is carried out by all interpreter.
The user may give interpretation parameters Indicating interactive and trace modes, slze o the subset to be calculated and other constraints such as a list of passive (or active) charts during this interpretation, the depth and width of trees and length of the string etc..
As SCSGs are statlc gral~nars, no heuristic strategy wllt be used In the lnterprete's algorithm.
So the interpretation will not be efficient.
Since the goal ts rather to test gramnars than to apply them on a real scale, the efficiency of the interpreter Is of no import ance.
CONCLUS I0N The system presented Is being implemented at GETA.
In thls article, we Put emphasis on the system's design principles and specification rather tilan on the detalis of lmplementatlon.
We have to1 lowed three widely recommended des ign principles: a} early focus on users and tasks, b) empirical measurement and c) Interactive design \]\[2\]\[.
The specification of the functions are checked by the system's future users before implementation.
The user's advice Is taken into account.
This dtalogue continues during lhe implementation.
The top-down and modular programming approaches are followed so ttlat, even 1f the Implementation ls not completly acilieved, the implemented part can still be used.
The system Is designed for being rapidly implemented and east ly modt f led thanks to Its modular lty and especially to a htgh level logic programming language: PROLOG (3\].
We have tried our best to make the system as user-fr lendly as possible.
The system's most remarkable character is that the users manage their data according to the loglcal structure adapted to tile human be I rig.
What ts interesting In our system ls not that it shows sonle very original ideas or the most recent techniques In state-of-the-art but tt shows that tile combination of well-known techniques used orignally In different fields may flnd its application in other fields.
83 Long term perspectives of the system are numerous.
Wlth the evaluation o the SCSG, some strategic and heuristic meta.-rules may be added to a grammar.
Equipped by an expert system of SCSG, SCSE could lnterprete effclently a static grammar and synthetlse from It efficacious dynamic grammars.
It Is also interesting to integrate into SCSE an expert system which could compare two SCSGs of two languages and produce a transfer grammar or' at least glve some advice for constructing it.
Using its logical structure manipulation mechanism, SCSE can be extended to deal with other types of structured texts.
Thanks to Its efficient Interpreter or in Cooperation with a powerful machine translation system such as ARIANE, SCSE could be capable of offering multi-llngual editing facilities (4~.
-O--O--O-O--O-O-O-O84 BIBLIOGRAPHY S.Chappuy, "Formallsatlon de la Description des Niveaux d'Intepretation des Langues Nature\]les.
Etude Men~e en Vue de l'Analyse et de la G6n@ratlon au Moyeo de Transducteur.", Th~se de trotsl~me cycle & I'USMG-INPG, Juillet 1983.
2. John G.
Gould and Clayton Lewis.
"Designing for Useabllity: Key Principles and What Designers Think", Co~nunIcatlon of the ACM, March 1985 Volume 28 N .
Ph. Donz, "PROLOGCRISS, une extention du langage PROLOG", CRISS, Unlverslte II de Grenoble, Verston 4.0, Juillet 1985.
HEIDORN G.E., JENSEN K., MILLER L.A., BYRD R.J.0 CHODOROW M.S., "The EPISTLE text-crltlauing system.", IBM Syst.
Journal, 21/3, 1982.
TEITELBAUM 1.
et al, "The Cornell Program Synthesizer: a syntax directed pr'ogra~ntng environments.
", Co~nunicatlon of ACM, 24(9), Sept.
1981. VAUOOIS & S.
CHAPPUY, "Static Gran~ars : a formalism for the descrlbtion of linguistic models", Proceedings of the conference on theoretical and methodological issues in machtne translation of natural language, Colgate University, Hamilton N.-Y., USA, August 14-16, 1985 .
CondiLioned UnificaLiou for Natural l,an~uage Processinp, A\]\]STV~ACT This paper prescnLs what wc c.all a condiLiol'md unification, a r'm'w meLhod of unificatiol'~ for processing natural languages.
The key idea is to annotate Lhe patterns wiLh a cerLcdn sort of conditions, so that they carry abundant inforrnation.
'\]'his met.bed t.ransmits inforrnaLion frorn one pattern to another more efl'icienLly Lhan proecdurc aLLachmenLs, in which information cortLaincd in the procedure is embcddcd in the progranl rather Lhan dirccl./y aLLachcd Lo paLL(ms Coupled wilt techniques in forrnal linguistics> i\]\]orcovcr, conditiorled unification serves most.
types o1" opcrations for natlu'ai \]ar/guage processil'q~,.
KSiti f/asida \]'\]\]ecLroLechllica\[ 1,abe ' A.ory Ul\]lczorlo I } 4, 7~aktlra MtlFa, Niibari-Gurl, Ibaraki, \[tOb Japan (\[3) ptlt_tllS psrl nrnb(prcsonL, P, N) : notAlrd ~'-tng(P, N) l~ut_Lns, psn nl:nl)(T, l", N) : not_pres(T) noL_3rd sng(lst.,,N).
not pres(past) not_~rd~'-;ng(;~nd, N) not pres(past4~a'rtlclp\]e ) rlotA/rd, sr~g(,'Wd, plural) not pres(basc).
1. Introduction A currcr'd, major t.rcrY.t of naturul la~/guage processing is ehara.cterized by Lhc overall use o\[ unification (Sttiebc~r (198'l), Kay (1985), Proudir:~ and Pollard (1,985), Percira (198b), Shicbcr (1985), etc) reflecLing lhe recent develop merits in nonLra.nsformaLJonal linguistic \[ormalisrns, such as Lexical FuncUonal Grammar (}lrcsnan (198E)), Generalizcd Phrase St.r'tJcl.ur( (\]rarnrnar (GPS(\]) (Gazdar, Klein, Pulhlm and Sag (I 985)), i Icad Grammar (Pol}ard (19f1,1)), and tIcadl)riven Phrase Structure Grammar (lIPS(;) (Pc\]lard (1985a.,b)). These formalisnls dispense wiLt,qlobal opcraLioits sueh as t.ransfornlaLion, alld instead cxp!oit h~cal operations each C'Ollf'lrled wttJ/i\[l a local tree Such local operations ar'c forunulatcd in Lcrms of uni~caLion \]Iowevcr, Lhe ordinary unification as in Prolog is insufficient, seeu rrorn both scientific (here, alias liriguJsl,ic) and cngin(.'ering poilfl'. of vicw '\['he F, robh-'trl is that p,t tc\]~\[\]s Lo bc tl\[li(ie({ wiL\]l each other lack the cape.city rot carrying irfforrnaLion In Lhis papcr we \[)rcscnl a new mcLhod of unificaLion which we (call conditioned unification.
The essence of the method is t.o deal wit.h paLLcrns aimoLated by seine sort of condit, ioils.
These eondiLioi<ls are so cortsl, raincd /-Is Lo 'oe cfficicntly operated on, and yet to be able to carry rich enough informaLion t.o caDLure linguistic gcneralizations.
2. The Problem Ordinary patterns as it/ Pr(;h)g Is.el< cxprcssivc power, t)esatlSC var\[ablcs theFcirl arc Sil)i\[)ly il\](iClCl"tlliltdt(7 alld Ihtis car'ry almost no irffqrrnalion 'l'hercforc, stie}l palL(ms aud unification among thcm arc msuffiei0nt for' capturlng t\]le {~,i<'al/l rYlat ic al <!,>erm r'al ixat ior~ and tim process:n~> effici(ncy.
\],It us look a.t some c:<amph.~s below A ~,,l'anl matical catc>ory is assumed Lo be it llst of features A feature consisLs of a feature nalnc and a w~hic, and rcprcscnLcd as a t.cmn like tt~rn.e (vat,z().
'\['hc \]cxical entry of English vcrb p'u,t, for instance, can not.
be described as a I'roloc patLcrn, bill needs some arlllOLation (i.c.,p~zt Im.s~)s?t.~zmS(T, P, N)) as in (1) (1) k:xicorl(puL, I tensc('I'), p(msorl(\]')> number(N)I) : put_Lns_ psn nrnb(T, P, N) }let(?, fcaLLIICS olincr Lhan ten, se., perso?t, and ?lattnher arc omitLed, arm predicate p~ztmtm.spsTt~z?n.b is dcfinc.d c,s in (2) For a biL morc COIT/I)I i:aLcd instance, conmdcr the relationship between a synLacLic gap and iLs filler.
In GPSG, IIPSC, cte., tiffs relationship is; captured in terms of the SI,ASI\[ fea/.urc, which reprcscnts gaps in the category of \] U~.i~tk is craz U, for cxamplc, thc SI,ASII feature is spcciflcd as \[NI j\] ller'e SI,ASI is assumed to take as its wdue alist of catcgorncs.
SLaLcd below is a simplil3cd principle about the disti'ibution of this featqrc in I.yptca\] cascs (3) lu a local tr'ec, Lhc rllotl;cr catcgor/s S\],ASI\] tea.Lure is obl.aJncd by coneatcr~atir, g from h.fi\[.
Lo rip, ht the S I,ASII fcat,wcs of hey de.,.ightcrs In order to describe: this principle, s('nnetlting :uorc than a nlerc pat t.crn ts lcquir(x\] again: (i) IocalJr'cc.(lslc, sh(X)\], Ismsh()l, Islash(Z)\])append(Y, Z, X) l'eaturcs othcr thanSI,AS}l arc omitted herc.
The so called procedure altachmcnts is the most colnrnon way or conllflclncntJnp, the \])oor clcscriplivc capacM.y or ordinary patterns \["or instance, you may regard Lhc bodies of \]h)rn elaus(s (1) and (4) as at la,_hed procedures The dr'awbacl< of procedure atLachr~lcnl is ut the fact t.hnt the ouly way of using Lhc proccclurcs Is to execute thorn I"or t.his reason, proecdur,}s arc Irmrcly embedded lu programs, rcAhcr than at.lath(x\] to those paLterrls which th(sc itrotu'ams operate on The irtforrnaLion which \[)ro((durcs cantain car/rx~t {U.'nera\]\]y be I~.',rricd aFOlllld a( ross scvc;a/ part ial s\[rtlC\[ tlI( s ci\]ch Of which it pFoocc\]arc dircclly operates on, bccausc> oncc a procc(lurc is cxc cnt.td, the informs.lion whkh it c.ontainc:d is palqially lost For instance, when Icxical entry (1) is cxploiLecl, p~ztJ.n.s pstt.n;/m,6(?', I), /\i) is cxecut.cd and 7' and /~ arc il~stanliatcd Lo be preset~.t and Ist, icsp(cLivc\[y 'l'h'ds Ic\[L bchh~d is the informaLion abotlL the other ways Lo instanLiaLc those wwiablcs.
Actual procedure attaclu'ncrd.s musL be arr-ar<e, ed so that infornlat on shouhl not be it)st whelt procedures arc cxccutcd Freeze of Prolog (Colmcraucr (1982)), for instanc/, is a mcans of tins arr~ingerncnt.
\]\]y exc(tll.i\[\]g freeze(V, "~), atomic formula ~ is frozen; i.c, the exccuLlon of'~ is >-uspcnded until w~riable X is instanttat.cd \]f' contams.'(, thcl'cforc, }lop(fully uot.
so rnuch lrtforrnat.iol~ is test.
whcc  is cxecuLed Ncvc.rthcless, freeze is problematic in two rcspt(ts Virsk, irJorn\]ation cart still be lost when the frozen pro-ccdtll'CS LIFe cxccnted.
Seeond, too nltlCh illforrllatiol3 cat\] be accumulatcd whilc several procedures arc frozen Sup pose, for itlst.ance, thaL freeze IX, t~tet~ber(.Y, \[a, 6 })) and fr<,t~.~<~.
(r,,~.',~,.~.,'(}'. I~'.~ I)) have bccn execut, cd '\['herr, X and Y can be ulfif\]cd with each other witt~ouL awakening ciLbcr procedure.
In that (asc, Lho iifforn/at, ion that X may bc t) is redundant bctwccn Lhc I.wo proccdures, and Litc other part or irlfornmLion those Droecdtlres contain is Ill(Of\]" sistcitL What one might hope here is \[o Jrlstitntiatc )( (and Y) to be b If we had cxectitcd freeze(Y, member(Y, It, cL ) iristcad of freeze (Y, rn.ernber(Y, Ib, c I), computational 85 resources would be wasted as the price for a wrong processint.
After all, it is up to a programmer to Lake a deliberate care so that information should t)e efficiently transmitted across patterns This causes sewral problems interwoven with one another.
First, since those programs reflect the intended order of execution, they fad to straightforwardly capture the nniforrnitJes captured by rules or principles such as (3).
Accordingly, programnnng takes rnuch labor'.
Moreover, the resulting programs work efficiently only along t.he initially inLer~ded order.
3o Conditioned Unification 3.1.
Conditioned Patterns These problems will be.
settled if we earl attach information to patterns, instead of attaching procedures to programs l\[ere wc consider that such information is carried by some conditions on variables Variables are then regarded as carrying some information rather than remain:ing simply indcterminatc I-}y a conditioned pal.tern let.
us refer to a pair o\[a pat tern and a condition on the w~riables contained in that pat.tern.
l~'or simplicity, assume LhaL the condition of a conditioned pattern consists of atomic formulas of Pro/og whose argument positions are filled with variables appearing m tile pattx.'rn, and that the pre(hcates heading those atomic for mulas are defh~ed in l.erms of Horn clauses.
For instance, we would hkc to regard the whole tbing in (\[) or (4) as a condJtioncd pattern.
 3.2.
Modular Conditions The conditions in conditioned patterns must not be executed, or the contained information would be partially lost Tile conditions have to be somehow joined when conditioned patterns are unified, so t.hat the information they contain should be transmitted properly in tile sense that the resulting condit.ion is equivalent \[o the Logical conjunction of tam input renditions and contains nciCrmr rcdnndant nor ineon sistent information.
We call suet a unification a conditioned unification A simple way to reduce redundancy and inconsistency in a ('.ondiLion is to let each part of each possible value of each variable be sLlbjcct to at, most one constraint.
\],eL us formulate this below.
We say that a condition is superficially modular, when no variable appears twice in that rendition For instance, (Sa) is a superficially modular condition, whereas (Sb,c) are not.
(Conditions are some.
times wr'itterr as lists of atomic forrnulas ) (',9 a \[a(X, Y), b(Z), a(U, v)\] b.
l a(X, Y), b(Y)\] e \[a(X,Y,X)\] l,'urther we say that a condition ~I' is modular, when all the relevant renditions are superficially modular, lIere, the relevant, conditions are {I} and the bodies of Horn clauses reached by descending along the definitions of the predicat.es appearing in .
A predicate is said to be modular when its definition contains only those Iiorn clauses whose bodies are modular conditions.
A predicate is potentially modular when it is equivalent to some modular predicate A modular condition does not.
impose two constraints on any one part.
of any variable, and thcrcfore contains ne> kher redundancy nor ineonsistency, ltereaRer we consider that the condition m (.'very conditioned pattern should be modular.
a.a. l'Jxpressive Power Conditioned patterns can carry rich enough information for capturing the linguistic generality.
Obviously, at 86 'st., they can describe any finite set of finite patterns.
\];'or instance, (I) is regarded as a conditioned pattern with modular condition \[pztt_g'ms_pstt~q,r~zb (T, P, N)\].
Moreover, also some recursivc predicates are modular, as is demonstrated below.
(6) a appcnd(\[\], Y, Y): append(\[U I X\], Y, \[U I Z\]) :append(X, Y, Z).
b sublist(\[\], Y).
sub\]ist(\[U I<I, \[U I Y\]) :sublist(X, Y).
sublJst(X, \[U IY\]):-sublist(X, Y).
Thus, (4) is also a conditioned pattern.
\]lowever, some recursive predicates are not potentially modular.
They include reverse (the binary predieate which is satisfied i~r its two arguments are the reversals of each oilier, as in reverse(\[tot, b\], c, all, \[d, c,\[ct b\]\])), .perm (Lbe binary predicate satisfied iff its arguments arc permutat, ions of each other, as inperm(\[i, 2, 3\], \[2, 1, 3\])), subset (the binary predicate which obtains iff the first argument is a subset of the second, as in s~zbset(\[d! b\], to, b, c, all)), etc.
New.'rtheless, t.his causes no problem regarding natural language proeessing, because potentially infinite patterns corne up only out of features such as SLASt\[, which do not require those non ruodular predicates.
3.4. The Unifier Shown below is a'trace of the conditioned unification between conditioned patter'us (7) and (8) (here we use the same notation for eondit.ioned patterns as for IIorn clauses), where the predicates therein have been defied as in (9).
(The definitions of c0 and e3 are not exploited).
First, we unify iX, )2 Z, g/\] and \[A, 7\], C, D\] with one another and get.
XA, Y : /3, Z = C, and W = D \]n the environment under lifts unification, the two conditions are concatenated, resulting in \[c0(X), e I(Y, Z), e2(Z, W)\].
The major task of this conditioned unification is to obtain a modular condition equivalent to this rmn-rnodular conditiorl This is tire job of funcl.ion ~tod~zlayi, ze.
Mod~zla.~tze rails function ~;~ttegrctte, which r'eLtlrns an atomic formula equiwrlent Lo the gives condition.
The Lcrminatior~ of a ?rtodulct,'ize or anir~fegrate is indicated by ~ preceding the reLurn-waluc, with the same amount of indentation as the outset of this functionrail was indicated witb When an {~ztegro, te calls a ~zodula~'ize, the alphabetic identifier of the exploited Ilorn clause is indicated to the h.'ft hand side, and the temporal unification to the right-hand side.
Atomic formulas made in integrate is written following 4.
Each lIorn clause entered into the definition is shown following % and given an alphabetic identifier indicatedto the right-hand side.
(7) IX, Y, Z, W\] :-e0(X), el (Y, Z).
(8) \[A, \]~, C, D\] :e2(C, D).
(9) e*(0, \]).
(a) e ~ (q, e) (b) ca(l, P):-e:Xl').
(c) c~(e, 0).
(d) modularize(\[e0(X), cl(Y, Z), c2(Z, W)\]) integrate(\[e0(X) \]) cO(X) integPate(\[cl(Y, Z), e2(Z, W)\]) c4(Y, Z, W) (a) modularize(\[e2(1, W)\]) Y = 0, Z = 1 integrate(to2(1' W)\]) * eS(W) (c) rnodularize(\[e3(P)\]) W = P integrate(\[e3(P)\]) =~ e3(P) tea(p)\] c~(p) :ca(P).
(0 =:> eS(W) -~ \[c,~)(w)\] 1' o4(o, :, w) :.
~:',~(w).
(j) (b) n:odular:'zc(\[c2(2, W)i)  :q, '/, :~ i:,t.o~ra~,'.(Im<3(a W)\]) * cO(W) (d) nladularizc(l I) w =.: o =-~ \[\] cS(0) (k) =+ c6(W) > I cs(w)J " o,3(q, ~, w) : o6(w).
(I) => c4(Y, Z, W) > \[co(x), <:4(< z, W) l We CaN refine Lhe progra\]'n o\[ "btt~.grcs, ta so that it should avoid ally predicaLe w}iose definiLion coiuLains only one llorn clause.
For instance, Lhe definiLion of cb consisLs only o\[(i) InsLead of (j), LhereR)re, we may }rove cd(0, 1, P) : c3(P) Also (1) can bc replaced by c 4(0, 2> 0), based on (k) We are able Lo work out r'ccursivc condiLions from F, lvor; recursivc coI:dit.iolls, lVor example, considor how X and Z arc unifiod under" t, ha conclit,iol: (10), whore ~rte'n~be.r is defined as in (1 1) (10) \[n,e:nher(X, Y), o0(z)\] (11) n/cinber(A, IA I IJl).
(a) member(A, IC I i~i) :-i ....... her(A, it) (b) The Lracc of this ulfif\]cat.k~n is showl~ b('\]:'w, whc's'c prcdica.l~' c 1 is rccursivcly (\]o/~ll/Cd based on Lhc i'(,ctlrsiv(! dcfillJLioH of 77"~ ( 77}, t) I~, '? modularizc(lmcmbcr(X, Y), cO(X)I) int.cgraLo(I member(X, Y), cO(X)\]), el(X, Y) (a) modularize(\[cO(A)\]) X = A, V -.
\[A Ill\] int.e<~';ratc (I c O(A)I) = > oO(A) =~ 1 <:)(A)\] 1' c I(A, \[A I ~<J) :-c0(A) (1)) n:odularizc(lmcn,bcr(A, 11), c0(A)l) X :: A, Y :: \[c!i;I Jnl.c<qral.e(\[ nlernbcr(A, I~), cO(A)I) :~ el(A, 1~) ::> \[cl(A, 33)1 1'ci(A, Iclt<l): c~(A, 13).
.... > c:(X, Y) :.~ \[o:(x, Y)I IL Js a job of in, tegra, te, Lo handle re,cursive de,hiLton.
The lasL g?l, te.g?,ts, te.
above recognizes Lhat.
the first 4m.tegrate, which is Lrying Lo (\]cNr/c c 1, was called wit.h the same arguITlCrlI.S except, the variable narnes, llencc t.he last "inttegrctte simply reLul'ns c.
I(A, H), because t,hc conLent, or cl is now bring worked ouL tlrldoY Lhc J'\]rsl.
~?ttegro.te arid thus it is rednndanL fol' t, he lasL {~tfegrate Lo \[urLhcr examine c 1.
It is not.
a\]ways possible fro' the above unifier t.~ unify paLL(2i"\[/s tlrl(\](~r roc.
tlr'sivo Col\]d\[Liol/S \["or J//sLalloL', J\[ Cf/illIOL unify X with )" under \[appe~td.(X, Y, Z)\], becal_tsc Lhe result ing condiLion is noL potsnLia\]\]y rnodular, llowcver, such a situaLiol'l CtOCS FioL seoln t.o occur Jn actual \[al:g:lagc proccssir:g.
4. Conclusiori We have prc.'-~er, Led a new nle/hod a\[ umfiealion, wh,ch wc call o.
coildiLior~c(\] tltti(loaLioli, Whcl'e paLLorils to be uniPlc(\] "'re annoLaked by a certain sort.
of corldit.ions on lhe variables wifich octroi" ill those patt.crn.<;.
Theso condiLions are so r'est.ricted t.haL Lhey conlain as lit.Lie redundancy a<'; possible, and d'ms arc always assured to be satisfiable.
This method has Lhe fo/h)wtng welcome characteristics l"h'st, I.he \])attorns to bc unified can carry at)llllda\]'lL infos' mat.ion rcprcscnLcd by t.he conditions han:,~in!,; on t.hClll The expressive capacity of Lhe,<c condiLion,s is sl:fl'Jcen\[ for capt.uring \]JH~U, IIihLJ(: ~sCHCl'a\]i',,iOS ~ccorld, such irfformat.ion is cfreclivcly Ir'ansrnitLed, by h~t.egrat.\[n? the col~dil,ion.~ v;her, pat.'..crl:,<s o.ro unif'ied Unlike procedure aLLacl:ment.s, in thil~ COllne(:lion, Ll/c infornGaLioi~-conveying <.'fficicl:cy of our Colt dilioiued unif'icat.\[on is no afl'c'(gcd by the direct.ion of t.i~c daLa.flow Therefore, O/ll" col'l(\]{lioned unifies.Lion is oo;rnplel/ly r(ver'sJbk< and ',hns is \[n'on:ising its a Los\] for dcscril)h'T> <~{l'all/lllilrs fOF bolh SCllL(Hb':C comprel/ensiol: slid prochl(d toll Owing t.o Lhese cllar'act(!risLics, Otll conditioned unif'caLian l)r'avh|es a now prog, ra.unniiug 1.1aradigtn foi I/illt/l'/tl lar/y,/lag(".
\[)l'OCCSSil/lJ>, rcph~.cing proccd/1Fc aLt, o.ctlI3:ont.s which haw3 tradlLionally el2joyed i.\]lc Llbiq/lity Lhat.
t.hcy do noL descrvc References Bresnan, J.
(ed). (1982) The Mental Representation of Grammatical Relations, MIT Press, Cambridge, Massachusetts.
Cohnerauer, A.
(1982) Prolog II Reference Manual and Theoretical Model ERA CNRS 363, Groupe d'Intelligence Artificielle, Universit de Marseilic, Marselle.
Gazdar, G.
E. Klein, G.
K. Pullum, and J.
A. Sag (1985) Generalized Phrase Structure Grammar; Basil Blackwell.
Oxford. Kay, M.
(1985) "Parsing in Functional Unification Grammar".
Natural Language Parsing, pp.
251--278, Cambridge University Press, Cambridge, England.
Fernando C.
N. Pereira, A structure-sharing representation for unification-based grammar formalisms, Proceedings of the 23rd annual meeting on Association for Computational Linguistics, p.137-144, July 08-12, 1985, Chicago, Illinois Pollard, C.
J. (1984) Generalized Phrase Structure Grammar.
Head Grammars, and Natural Languages.
Doctoral dissertation, Stanford University, Stanford, California.
Pollard, C.
J. (1985a) Lecture Notes on Head-Driven Phrase Structure Grammar.
Center for the Study of Language and Information.
Pollard, C.
J. (1985b) "Phrase Structure Grammar without Metarules," Proceedings of the Fourth West Coast Conference on Formal Linguistics, University of Southern California, Los Angeles, California.
Derek Proudian, Carl Pollard, Parsing Head-driven Phrase Structure Grammar, Proceedings of the 23rd annual meeting on Association for Computational Linguistics, p.167-171, July 08-12, 1985, Chicago, Illinois Stuart M.
Shieber, The design of a computer language for linguistic information, Proceedings of the 22nd annual meeting on Association for Computational Linguistics, p.362-366, July 02-06, 1984, Stanford, California Stuart M.
Shieber, Using restriction to extend parsing algorithms for complex-feature-based formalisms, Proceedings of the 23rd annual meeting on Association for Computational Linguistics, p.145-152, July 08-12, 1985, Chicago, Illinois
LA RESOUTION D'ANAPHORE A PARTIR D'UN LEXIQUEGRAMMAIRE DE45 VERBES ANAPHORIQUES Blandine GEL.AIN & Gelestin SEDOGBO 26 place Ovale BULL cediag 94230 Cachan 78430 Leuveciennes (France) Abstract This paper presents a system which intends to resolve anaphora in the framework of the Discourse Representation Theory, arrd using a lexicon-grammar of anaphoric verbs, through the application of selection criteria for assignment of a referent to an anaphora.
From a semantic representation of text provided by a DRT system implemented in Prolog, the system uses several criteria of selection of referent.
One of these criteria is the anaphoric conditions of verbs described as a lexicon-grammar of anaphoric verbs.
The present paper investigates a transformational analysis of verbs related to their anaphoric behaviour, and the adequacy of extension of the lexicon~grammar of MGROSS to anaphonc conditions on verbs.
1. Introduction La Th~orie de la Representation du Discours (ou DRT 1) propose une approche unifiee de phenom~nes du langage naturel tels que le temps, I'evenernent, I'anaphore Elle se caracterise par sa filiation avec la semantique Iogique, et sa distance d'avec les niveaux de representations basees sur la Iogique des predieats et ses extensions.
Ainsi les notions de consequence Iogique et de validite (\[SEDO 87\]) peuvent s'appliquet naturellement aux structures maniputees par la DR1 Cette theorie propose une explication de la formation de I'anaphore, sans en proposer ta resolution.
Celle-ci passe en general par I'application de orit~res de selection syntaxiques, semantiques et pragmatiques, qui levent les ambiguites engendrees par t'usage de I'anaphore.
\[GUIN 85\], \[CARB 88\], \[RICH 88\] entre autres ont intloduit les notions de foyer et de contramtes successives 8 activer Cependant cos criteres sont paffois insufiisants pour etablir une relation evidente entre un nomet un pronom Dar~s sa theorie du Oouvernement-liage, 1 H.KAMP "A Theory of T\[uth and SemarY0c Interpretation" GroP.nendiik Amsterdam 1-(:JR t NCHOMSKY 2 explique I'anaphore ~ partir d'un mecanisme de liage et s'appuie sur la remarque que le liage d'une anaphore A son referent depend aussi des proprietes anaphoriques du vetbe.
Aucune etude empirique des proprietes anaphoriques n'a ete faite ~) ce jour, alors que toute approche de resolution d'anaphore devrait etre basee sur un texique des verbes et leurs proprietes anaphonques associees.
Le present article decdt une approche de resolution d'anaphore qui repose sur: la mise en oeuvre de ta DRT pour representer la semantique d'un discours; I'elaboration d'un lexique-.grammaire des verbes anaphotique@; un systeme de filtre base stir differents criteres de selection Ce systeme illustte la resolution automatique de certaines anaphores en partant de la representation semantique d'un texte obtenue d'apres la DRT los ptonoms que nous avons etudies font partie d'un type d'anaphore qui represente une relation pouvant s'etablir entre deux phrases saris mettre forcement en jeu une regle syntaxique (le pronom pout identifier un referent dans le discours precedent): Jean croit que Mane IU/ offre un //we Cet article est divise en cinq parties: introduction ~ notre travail, presentation de la DRT, puis de son implementation en Prolog, description des lexique-grammaires et leur extension aux proprietes anaphoriques, presentation generale de I'architecture de notre systeme de resolution, et enfin perspectives de creation systematique d'un lexique-grammaire.
2. La reprdsentatlon s~manticlue La DRT se fonde done sur un ensemble de regtes de construction traduisant un disoours en une representation semantique formelte: la Structure de Representation du Discours.
Pour chaque pattie de discours, une DRS est construite, boite pouvant err contenir d'autres, 2 "Government Binding", notee GB 3 Exlension des tables de verbes d#vetoppt~es au LADL, aux propri~tes anaphonques (cf \[GELA g2\]) Ac:iEs DE COLING-92.
NAtCI~S, 23-28 AO~ t992 90 1 PROC.
OI; COLING=92, NANI'E,'I.
AUG. 23-28, 1992 qui reprL~sente le contenu significatif de cette par'tie Une DRS complete est I'ensemble de plusieurs DRSs apparaissant t~ mesure que le discours continue.
La DRT etudie doric les contraintes sur cette continuation.
La forme d'une DRS consiste en une paire <U,Con> constituant deux z6nes, o0 U (univers) est un ensemble des r~fdrents #u discours representant les entites du disc, ours, et Con un ensemble de conditions qua doivent satisfaire ces referents.
Celles-ci sont des predicats et des relations de referents du discours mais peuvent etre plus complexes.
Notees comme en Iogique des pr~dicats, les conditions de verite sont definies par rapport & la possibilit~ d'incture la DRS dens un modele (pour plus de details, consulter \[GUEN 85\]).
D'autre part, I'extension d'une DRS ne peut changer les valeurs d(~j& assignees: tout Ce qui etait vrai auparavant restera vrai par la suite: Un aamion bansporte une charge u.K=\[xl,x2\], Con.K=\[carnion(Xl),charge(X2),t~'ansporte(Xl,X2)\].
Tout camion b'ansporte une charge U.K=\[ \], Con.K = \[:>,K1,K2\]; U.KI=\[Xl\], Con.K1= \[camion(X1)\]; U.K2=\[X2\], Con.
K2=\[charge(X2),transporte(Xl,X2.)\].
2. I La notion d'accessibilite On peut representer des restrictions configurationnelles sur les relations anaphoriques possibles entre les pronoms et leurs antecedents.
Ces restrictions sont obtenues en reduisant I'accessibilitd des referents.
L'accessibilit~ permet donc de determiner les liens anaphoriques entre un marqueur pronominal et un marqueur de discours.
Toute DRS est accessible d'eltem~me; son univets de marqueurs accessibles est I'unNers du discours de la DRS.
Dans une DRS implicative, la DRS antecedente est accessible de la DRS consequente Enfin, la relation d'accessJbilite est transitive.
Donc pour "tout camion qui transporte une charge la declare", I'antecedent du pronom obJet laest une charge.
Ici, U.K1 est accessible & K2.
Les conditions de continuite d'un discours sont aussi fonotion de I'accessibilite: la phrase: "il va & Berlin" ne peut continuer la precedente puisqu'aucun marqueur de diseours n'est accessible de la DRS K (voir ce sujet \[KASP 86\]) Ceci explique pourquoi une phrase comme: "Cheque chauffeur possC~de un camion, fl te conduit": doit etre formulae: "Chaque chauffeur qui poss~de un camion le conduit* (avec "qui poss~e un cam/on" comma extension de "cheque chauffeur") pour Otre representable.
Mais la DRT a des limites; elle n'explique pas la bonne formation de ce texte, par example: Cheque chauffeur transpo~te une charge.
II n)et plusieurs jours & la tivrer.
Ella ne sere livr~e qu'au bout de 3lOUtS Les cleux dernieres phrases, selon la DRT, ne peuvent suivre la premiere, & cause de la portee du quantificateur cheque.
Par contre la phrase "Cheque chauffeur fransporfe une charge qui ne sere livree qu'au bout de 3 jours" sere parfaitement representee par la DRT.
Ces r~gles obligeraient doric le Iocuteur & d~crire une situation en une seule phrase! 2.2 Imp.~.
mentation de la DRT Notre analyseur semantique demane avec des arbres syntaxiques resultant d'une grammaire de type GPSG, programmee en Prolog La grammaire semantique est bas~e sur les memes principes: unification de structures, augmentation de listes ordonnees, presentee sous forme de regles de reecriture suivies de contraintes, de type: ph -> gn gv <ph drs courante>=<gn drs courante> <gn suite drs>=<gv drs_courante> <gn argument>=<gv arg_sujet> <ph arg sujet>=<gv arg suJet> <ph arg objet>=<gv arg_objet> Ceci donne, 8 partir de regles DCG issues de la compilation des precedentes: traduire(ph(GN, G V), P) :~'aduire(G V, P2), traduire(GN, PI), imerge(P,\[courante\],Pl,\[courante), imerge(P l, \[suite\], F~2, \[courente\]), tmerge(Pl, \[arg\], P2,\[arg sujet\], imerge(P,\[arg sujet\], P2,\[erg_sulet), tmerge(P, \[arg_objet\], P2,\[arg_objet\]) soit la formule semantique: drs (arg sujet(Xl),arg objet(X2), cour(cond( \[imp(drs(cond(\[camion(X l)\]), univ(\[X1\])), drs(cond( \[charge(X2), transporte(X l, X2)\]),univ(\[X2\])) )\] )) ).
correspondant ~ la phrase "tout camion ffansporfe une charge".
C'est 8 partir d'une telle formula que commence la resolution anaphorique 3.Le lexique-clrammaire Le lexique-grammaire, represente sous forme de tables (matrices composees de colonnes ACIT~ DE COLING-92, NAN-IT~, 23-28 AOt~Zr 1992 9 0 2 PRoc.
OF COLING-92, NANTES, AUO.
23-28, 1992 et de rangees), contient les phrases strnples, les diff~rents emplois verbaux et los propri~tes qui leur sent assoei~es: nature semantique des arguments, trallsforrnations possibles et tours conditions, nombre et structure des complements, type de la preposition associ~e etc I/ consid~re la nominalisation comma la transformation d'une phrase contenant un op~rateur verbal, en une autre phrase contenarlt url op~rateur nominal.
On y introduit un verbe predicativement vide -verbe supportdent le r,~le est d'actualiser le substantif qui n'a pas de marques morphologiques susceptibles de le taire: l.uc complimente los acteurs = Luc fail des compliments aux acteuts 3 1 L'entoura~e lexical Nous nous limiterons dans cet article aux possessifs, sur lesquels \[GULL 81\], \[DANL 80\], \[GROS 89\] et \[VtVE 83\] notamment nous ont fourni des informations fort utiles.
L'examen des roots voisins du possessir est important, e.
g; Luci donne # L~.aj son i+ j argent Luc i donne a L~,a son i amour Dens le premier cas, donner est un verbe ordinaire (plein) alors qua dens le second, 41 est un verbe support (V-sup) Seut le substantif N2 change (\[NO donne ~ N1 N2jaoss\]).
Pourtant dens la premiere phrase, son peut ref~rer & trois personnes: si t'argent est & Lea, son est relie ~ L#a; si I'argent est ALuc, son est relie a Luc; si I'argent n'est ni ~ I'un ni ~ I'autre, son est relic,9 une tierce personne du discours pr~o~dent.
Dans la seconde phrase par centre il n'y a qu'un r#ferent: Luc, ~ cause du terme amour qui appartient aux roots "abstFaits" ou de sentiments, pour lesquels on ne peut pas trouver, dans ce type de structure, d'autre relation ~ sot\] qua le sujet, ici l,uc.
II s'agit de cor~f~rence obligatoire au sujet Dens "Luci cheque L~j par sos i id~'es ~ et "Luc i cheque Leajdans sos11rig, as", il y a le verbe chequer Pourtant dens ta pre.miere phrase sos est forcement relic 8 Luc.
II s'agit d'un cas de coreference obligatoire au sujet, induit par la preposition par.
Alors qua dens la seconde, la cor(~f~rence est oblJgMoire au cempl~ment d'objet sos est reli~ ~ L~4a, ~ cause de la preposition dens 3.2 Phrases construites aulour d'un V-sUlq On trouve des expressions verbales figees et d'autres mettant enjeu la paire Vsup/Npred mais sans 6tre des expressions figees.
I.es expressions verbMes fi~es sent de la torme \[NO V Nl-poss/, construites autour de variantes aspectuelles et d'op~.rateur causatif du verbe avozr, dent le N1 est teuioum "pattie du corps" ou "abstrait" ("Luc i rettent sos i tarmes'), et dent la hansformation en gnest impossible.
Elles peuvent aussi etre completees par url troisieme argument (\[NO V N1 Pr#p N2\] avec V support ou non.
L& encore, le nom (N1 ou N2) d~tenniue pal le possessif est "pantie du corps" ou "abstrait", et aucune restructuration n'est possible: L.ucHelte son i d#volu sur L~a l.uci #erda L#a sous sa i protection Si le possessif d~termine I'objet direct dens une structure INO V Nl-poss Prep N2\], \[Prep N2\] pout 0tre remplac# par une compl~tive I'infinitive, donnant \[NO V Nl-poss Pr~p Vlnf\]: Luc i passe son i temps au travail Luc~ passe son i temps ~ travailler Daris toutes ces phrases, la cor~ference est touIours obligatoire au sujet \[.es expressions non #g#es sent construites autaur de verbes support ou de variantes aspectuelles 1NO V N%poss\] ou \[NO V Nl-poss Prep N2\]: Luci a perdu ses i illusions (~ur L~a).
On ne peut pas transformer \[NO V Npred (Prep N1)\] en \[NO V Npred de N2 (Prep N1)\] (* Luc a pe~du les illusions de Paul sur L~a).
En d'autres termes, I'argument NO de Nprep est le sujet du verbe, mais ce verbene pout prendre une expression \[Npred Pr#p NO\] comme complement (ici: los illusions de Paul).
La presence ou non du complement d'objet indirect n'a pas d'incidence sur la relation qui lie le possessif au sujet.
Ces phrases nese construisent pas avec une infinitive.
Certaines expressions non fig~es se construisent avec le possessif comma d(~terminant de N3; dens cecas, los trois arguments sent obligatoires etla cor~f~rence n'est pas obtigatoire au suiet, II s'agit de cas de norPcor~f~rence obligatoire ~ I'objet ("Lucj met Lea ~ sai4 j disposition") On pout restructurer en reliant les complements par ~tre: Lea est ~ ta disposition de (L uc, Paul) Fin resume, parmi les phrases construites autour dt t Vsup, I'adjectif possessif qu'elles contieonent n'est jamals coref~rent ~ I'objet, AcrEs DE COLING-92, NANrHS, 23-28 AO~r 1992 9 0 3 l'rtOC.
Ot; C()I.ING-92, NAN'I'ES, AUG.
23-28, 1992 mais toulours au moins cor(~f~rent a.
sujet En reconnaissant d'emblee ces phrases et teur verbe comma Vsup, on pourra r/esoudre automatiquement I'anaphore Pour cela nous proposons de reperer les autres phrases, pu~s de considerer les phrases non reconnues comme ~tant de carte categorie 33 Phrases construites autour de verbes ordinalres Elles s'articulent autour d'un verbe ~ un ou deux arguments, rNement predicatif de la phrase puisqu'il definit la structure des arguments II est determin#, par: le nombre d'arguments I'articulation syntaxique de ces arguments les traits semantiques de ces aguments Dans la st\[ucture \[NO V Nl-poss\] que ron ne peut poursuivre avec \[&/de N2\], la coreference est obligatoire aun autre nero que le sujet NO (du discours anterieur) Luc approuve son choix Luc apptouve le choix de L#a Par centre, si une phrase a deux arguments et qu'elle peut etre completee par un troJsiCme, la presence ou non de ce dernier fait varier la coref~rence, ou tout au moins la preference entre lee antecedents possibles: Luc i avoue son i depit.
Lucj avoue soni+ j d~p/t a Lea k La relation de coreference existe toujours entre le possessif et le sujet dane ~es phrases simples \[NO V NI\]; ou complet~es par \[Pr~p N2\] oQ la relation peut aussi exister entre le poss et un referent du discours ant~rieur.
n a donc la un cas de non-cor#f~rence obligatoire ~ I'objet Tous les verbes qui donnent ces r~sultats dans une telle structure appartiennent a la table 9.
Quand le syntagme prepositionnel est obligatoire, on distingue les phrases ou I'adjeetif possessif determine le N1 et celles oO il d~terrnine le N2.
Parmi tes premieres, on trouve une cer~fdrence obligatoire au sujet Iorsqu'il y a possibilit~ de pronominalisation: Luc i consacre sa i vie a ta pemture.
Luc se consacre e la peinture OU de verbalisation simple: Luc i accorde son i pardon ~ L#a Luc pardonne a L#.~ Mais pour celles dent la transformation donne une completive ~nfinitive \[nO V N1 VinfJ, la coref~rence est obligatolre ~ I'ebjet: Luc i motive L~ajdans son 1 travail Luc motive Lea g~ travailter Tous ces verbes appartiennent a la table 11 Parmi les phrases de structure \[NO V NI Pr~p N2-poss\], la relation est obligatoire au sujet, ou obIigatoire ~ t'objet Dans les exemples suivants (verbe de la table 4), oQ la 1estructuration est possible en IN2 de NO V N1\], la relation est etablie entre le possessif et le sujet, Iorsque ta preposition est par ou avec: Luc i cheque L&aj paffavec see i idles.
Lea Ides de Luc choquent Lea.
Pa~ centre, la mC~me phrase avec clans ou pout; par exemple, donne \[a transformation \[NO V N2 de N1\], et on etablit alors la relation entre le possessif et le ~:ompldment d'ebjet: LUC I cheque Lea\] darts.~esj Ideas.
Luc cheque tee id#es de L&a.
4.Architecture .qdndralle du s.ystdme Notre systeme se compose donc d'un analyseur sfntaxirlue qui donne des arbres & paftir desquels un analyseur s~mantique produit des ORS.
C'est sur etles qu'oNrera le piogramme de ~euolution anaphorique Ce systerne se complete d'url fichier de verbes par tables, et d'un fichier "fonetionnel", constitue ~ mesure de I'arlalyse semantique, ou sent stockes tousles norns et pronoms, et leur forlction grammaticale.
La procedure de r~solution: apres reperage des pronoms, commence par une recherche des verbee (A chacun est associ~ un trait pour sa table d'appartenance) et de leur structure, dans le fichier lexique-grammaire.
Si cela est trouve, on cherche sila coreference est obligatoire.
Si oui, le traiternent est termine.
Sinon, il faut activer d'autres filtres syntaxicos~mantiques: en partant des listes ordonn~es de pronoms, univers et conditions, on verifie la compatibilite de fonction 4, de genre et de hombre, semantique 5.
II faut parfois chercher te verbe darts plusieurs tables.
Si I'identite de structure entre le texte etles fables n'est pas ~tabtie, on examine I'entuurage substantival du verbe si robjet est concret, il taut activer tes autres filtres.
S'il est abstrait ou "partie du corps", on a affaire ~ une phrase a verbe suppolt (peut-t~tle figee) dont le statut induit la (non) coreference obtigatoire.
&ConclUsion et perspectives \[~emarques sur Des travaux: 4 Un candidat sulet est pr~f~e aun autre pour tre reli~ & un pronom suJet, dans deux phrases dtes paralleles 5 Des traits semanhques sent associes aux roots lexicaux ACRES I)E C()LIN(;-92, NAm'~s, 23-28 aot~rr 1992 9 0 4 Pr~oc.
oJ; COLfNG-92, NANtES, AUrJ.
23-28, 1992 -lous les ver'~;t; donl~uHt lieH ~'~ Hn type de construction partmulier, appadiormuHt ~'~ k~ ~me table.
Sur chaque table on p~;\[IL contraindre la relation de (no~ 0 coiet~rence entre le possessif et Hn argurnent du w~'ibe.
..Tous les uornpl6ments d'obje/ d6termine par le possessif sent "abstraits" uu "pattie du colps".
Avec d'aubes 1~ulns, nun pr~dicatifs, les verbes sotR ordinailt;s et on ne peut resoudre I'ambiguRe anapholique Done en ajoutant ces caract~ristiques dabs la table en question on peat resoudrc, automatiquement I'anaptiofe Ccci ~)11i/iHi~: I'hypath~se que routes les tables peuvent, a priori, tre uinsi complete;as par les sp~.cillcites liees a remploi d'adjectif..
possessifs et permethe ainsi aH syst~me d'viter d'autres filtres plus co0teHx en calClll et pas toujours fiables.
I1 taut done ~.tablir HI~ lexique-grammaim des ve~bes anaphorique'., (pris darts une structure mettant en jeLl till pronom ou, ici, tin adjectif ~x~ssessif) Darts la table 4 du lexiquegran.nu.e, pal exemple, nous repaltissons leS verbes erl: roul~.~__J_: verL~+par (ou avec et pa~ft~isd~;), et cor~f~mnce obligatoite au a~jet (NO): \[ uc i d,~prime L g, aj par son i attilu&e.
r u.q.E~_~e~: verbe+darts, e.t co~ef~len(;u obligatoire ~ I'objet: \[.uc i d~lonne l.g~j dg~tla ~esj propo~.
NOUS avons ajot~te A la table 1me colol,le concernant la pr6sence ou non de #)t~p N2\], divisee en deux colorines: les deux cas,3u cor~ference obligatoile.
Les verbes de la table 11 suHi ~;partis an: ~: V N1 darts \[NO V NI & VOinfl puut etre remplace par pronomi~-~#sation (hi verbe Le possessif est tore.~.ment coref~lent au sujer Luc consacre se vie (e d~.ssme:) au d~s~it~ Luc se consacre (,~ dessJne 0 au dessin Si N1 n'est pas abstrait la pior~t~mirlalisation est imF~ssible:(*Luc con~a:m son hao#~ au dessm): Grou~: M6rne structt.e sans prouontina.lisation.
II peut y avoir simple ve~r-b~l/saiJ~a~ L.e possessit est toujol,s cor~f6rc'r~t au soj~ N1 est toujours abst~ait et pemiet M verbalisation: Luc appolle SOn,sogtie~l ~ c(flh ~.
',dfa#O Luc ~outJent celia ~nb6"prl~e; ~: Verbes qHi, clans la tlarlsknmatiu1~ de \[NO V N1 &/pour VOilffJ eH \[NO V Nf ~/dans N2\] oQ N2 est d,~terniin,} par un poss~ssif, induisent obligatoirement une I t ~~t.,!~fin Ittk; I.(~:? pou; #av~#liu'~ Nuuu avo~ls &t~;Hdll t;~tke tabk; (a~ y ajoutaHt ulle CLtla(..tk')if, ti(\]iit;i CXi'G\[t\](I(;(!
~)11 111011 d'Llll(; Stlucttll~; lllOllOlllilal(~, .~:;tl\[}~iJvik::k'~e {;11 110i14, pol_~r k;,,,,';|ltli;klli~tu i\]hrastiq111:,-,; (,~hldi(~e~ (;t I'oblig~ioil (:lt~ colOi:Or<~i .L, qtli k'H~ c(~lll.~,'.po|ld \[(:Ab',l ~ \[\]~q JCAFd ~,()1\11<l !, I-~ I~ROWN ~_~lq~//h,:v~\] Re.~o!~jti_u_t;: q mul(V :gha2?g~ A#pl~?:~uIE ( .:131 IN(;, l }uduHa.-~t.
AI tj 1988 \[CI IAN,L~TJ \].1 "4 I/\Nll !R, !C,! il All\J, (; SEDf)(;I t ) Ih~, A_~rc,.hu Uni~eyL't~ .SVq/~xe~ ~21 rift M,Mrn_a-f~J~Ll(~J!.{:c;,9 6s,'IFC(k I, Sofia Antipoti~a Nov 1987 \[I)ANL 89\] L,DANL(:),% Rep!~se.~jt~#90.~: d','ni.fm~#i~p~; huguL.4igU~L.~.
COf*St~O*~Ql*S /V t}~rt; t/*e~'p X.
I |10.~,t) de 3~;I m~ cycle, LADI Pail! 7, !i\]\[;U JGt I/~, 91} HGFAAIrq these de ductorat (~'~ p:aHii,t~) I ~aH.'; /, 199>' \[~z,t ~()~ 89\] (:; (~Ro,~.~; lug~t~J~.~ammair~_:.
!AUI.,::L Lh,v.
Paris; ? S\[fMANTICA f>afi:-: Juin igB.q \[GROS /sJ M GhtOSS M~t!ode.u.~jL~A/~{~x P.
L::d.{ k;in|arln, i'ari-u 1!)\[,*; \[k-Jk ILiN 85\] F.GI IENTI tNLR, P,'4ABAIli \]-~ ser~{qtjgp.
F:NS, I )nivel-sit0 t tlbingeu I )(.'-u 1985 \[GIlIN 85\] R GIJIN\[)ON Focusing.
MCC Austin AC.I.i.'185 \[Gt I1\[ 81\] A.Gt IILI.EI ~, CLECI,ERIZ f 3\[n~s.
~ e tpj~_ff;\]&; :;S3ff_(tntiq ~!.'..
\[.angages n"63.
Ed I alottsse Paris "H;WI \[I(AX;t > 8.5\] W KASPER Dtscourse R~#/es~/!t~liun 7hec~E.
Rapport ACORD, Ur~iv.Stuttgart, Mai 1986 \[IdCtl 88\] ERICH, S.LUPERFC>Y tir~ A.
MCC A u sti n, A.
';. l..
f#,v 1988 \[t;I \[)O 871 C SEI )( )GB( ) SEatet~le r~ueJ_.
~OJL-U~t2g)!!s~{. Thbse de DooLorat d'Etat \[J ive~sit#, dr; Ma~seille, 1:)87 \[VlVl/83J I VIVES ::~}rne Cyck; IAI)I Pad:; 7 1983 \[WAI)A 811 tt WAI\]A, N.AsI let4 /~e~o/(~hlo q t hfivn.~.ity Texas, Austin, .l~m 1.987 References 1 Jaime G.
Carbonell, Ralf D.
Brown, Anaphora resolution: a multi-strategy approach, Proceedings of the 12th conference on Computational linguistics, p.96-101, August 22-27, 1988, Budapest, Hungry 2 {CHAN 87} T.
CHAWIER, B.
GELAIN, C.
SEDOGBO Une Approcho Unifice de le Syntaxe et de la Smantigue.
Congios AFCET, Sofia Antipolis Nov 1987 3 {DANL 80} L.
DANLOS Reprsentations d'informations linguistigues, constructions N ire prep X.
These de 3eme cycle, LADL, Paris 7, 1980 4 {GFLA 91} B.
GELAIN Thse de doctorat ( paratre) Paris 7, 1992 5 {GROS 89} G.
GROSS Dsanbiguisation smantique  l'aido d'un lexique grammaire.
LADL et Univ.
Paris 7.
SEMANTICA. Paris Juin 1989 6 {GROS 75} M.
GROSS Mthodes en syntaxe.
Ed. Hermann, Paris 1975 7 {GUEN 85} F.
GUENTHNER, P.
SABATIER Formal Semantics and Knowledge Representation.
FNS, Univ.
Tubingen. Dc 1985 8 {GUIN 85} R.
GUINDON Anaphora Resolution: Short-Term Memory and Focusing.
MCC Austin.
A.C.I., 1985 9 {GUIL 81} A.
GUILLET, C.
LECLERE Formes syntaxiques et prdicats smantiques.
Languages n"63.
Ed Laroussa.
Paris 1981 10 {KASP 85} W.
KASPER Montague Grammar, Situation semantics and Discourse Representation Theory.
Rapport ACORD, Univ.
Stuttgart, Mai 1986 11 {RICH 88} E.
RICH, S.
LUPERFOY An Architeture Program for Anaphora Resolution.
MCC Austin.
A.C.I., fv 1988 12 {SEDO 87} C.
SEDOGBO De la Grammaire en Chaine du Franais  on Systme Question-Rponse.
Thse de Doctorat d'Etat.
Univ. de Marseille, 1987 13 {VIVE 83} R.
VIVES Avoir, prendre, perdre: constructions  verbe support et extensions aspactuelles.
Thse de 3me cycle.
LADI. Paris 7 1983 14 {WADA 87} H.
WADA, N.
ASHER A Computational Account of Syntactic, Semantic and Discourse Principles for Anaphora Resolution.
University Texas, Austin, Jan 1987
Parole et traduction automatique : le module de reconnaissance R A P H A E L
Mohammad AKBAR GEOD, CLIPS/IMAG Universitd Joseph Fourier, BP. 53 38041 Grenoble cedex 9, France Mohammad.Akbar@imag. fr Jean CAELEN GEOD, CLIPS/IMAG Universitd Joseph Fourier, BP. 53 38041 Grenoble cedex 9, France Jean.Caelen@imag.fr

R6sum6

Pour la traduction de parole, il est ndcessaire de disposer d'un syst~me de reconnaissance de la parole spontande grand vocabulaire, tournant en temps rdel. Le module RAPHAEL a dtd congu sur la plateforme logicielle de JANUS-III ddveloppde au laboratoire ISL (Interactive Systems Laboratory) des universitds Karlsruhe et Carnegie Mellon. Le corpus BREF-80 (textes lus extraits du Journal Le Monde) a 6td utilis6 pour le ddveloppement, l'apprentissage et l'dvaluation du module. Les rdsultats obtenus sont de l'ordre de 91% de bonne reconnaissance de mots. L'article ddcrit l'architecture du module de reconnaissance et son int6gration /~ un module de traduction automatique.
Introduction

La traduction des documents dcrits a fait de rdels progrbs pendant ces dcrni6res anndes. Nous pouvons constater l'6mergence de nouveaux syst6mes de traduction de textes qui proposent une traduction soignde en diffdrentes I Synthbse ~ de la parole ] ~

langues[1]. I1 semble envisageable de les adapter pour la traduction de l'oral, /t condition d'en amdliorer le temps de rdponse et la robustesse : c'est le <<challenge >7 pos6 fi ces systbmes mais aussi au module de reconnaissance de la parole. Un syst6me de traduction de l'oral repose sur l'intdgration des modules de reconnaissance et de synth6se de la parole et des modules de traduction, pour obtenir une boucle complbte d'analyse et de synth6se entre les deux interlocuteurs [Fig. 1]. Le projet CSTAR-II [3] est un projet international dans lequel toutes les dquipes travaillent sur tousles aspects de ce mod61e. Pour permettre /t deux personnes de communiquer, il faut deux sdries de processus symdtriques dans les deux langues : un module dc reconnaissance pour acqudrir et transcrire les dnoncds dits par un locuteur dans sa langue puis un module de traduction qui traduit la transcription dans la langue du destinateur ou dans un format d'dchange standard (IF = Interchange Format) et enfin un module de synth6se de la parole (et de gdndration si on utilise le format IF) dans la langue cible du

rReconnaissance Traduction la instantan_._____~ ) ~ ,___~de parole ._J

!/
(Reconnaissance'~_~ T r a d u c t i o n ' ~ ~ ~. instantan6 __) ~. de la p a r o l e )
Fig. 1. L'architecture d'un syst~me de traduction instantan~e.

36

destinateur. Dans le cadre du projet C-STAR II nous avons en charge la conception et la rdalisation du module de reconnaissance de la parole continue h grand vocabulaire pour le fiangais. Nous collaborons avec l'6quipe GETA du laboratoire CLIPS-IMAG et le laboratoire LATL pour la traduction automatique et le laboratoire LAIP pour la synth~se de la parole. Ce consortium s'est fix6 l'objeetif de r6aliser un syst6me de traduction de I'oral pour le frangais. Dans cet article nous allons tout d'abord presenter l'architecture du syst6me de traduction et la plate-forme de ddveloppement JANUS-III [2], puis les diff6rentes 6tapes du d6veloppement du module RAPHAEL et enfin, les premiers rdsultats obtenus. 1 R A P H A E L p o u r la T r a d u e t i o n L'architecture du syst6me de traduction de parole est compos6e de trois modules essentiels (la reconnaissance, la traduction et la synth~se de la parole) [Fig. 2]. Dans ee projet nous utilisons ARIANE et GB [3] pour la traduction ct LAIP-TTS [4] pour la synth6se. Le

point de vue de la robustesse) nous envisageons l'intdgration d'une seconde couche de contr61e pour permettre le <<rescoring >> des hypoth6ses en tenant compte des taux de confiance associds aux diff6rents mots de l'dnoncd reconnu.

1.1

Plate-forme de J A N U S I l l

I

Recolmaissance la Parole ] de RAPIIAEL(CLIPS/IMAG-ISL) Texte
V

Contr61e

Cette plate-forme de traduction a dtd ddvelopp6e dans le laboratoire d'ISL des universitds Carnegie Mellon et Karlsruhe et contient tous les composants ndcessaires au ddveloppement d'un syst6me de reconnaissance phondmique/t grand vocabulaire h base de Cha~nes de Markov Cach6es (CMC) et de rdseaux de neurones. La facilitd d'dcrire un module de reconnaissance en langage Tcl/Tk avec JANUS-III nous permet d'adapter ses capacitds selon les besoins d'application et les caractdristiques du frangais. De cette plate-forme, seul le moteur de reconnaissance est directement exploit& Mais le travail de pr6paration des bases de donndes, l'apprentissage des mod6les de phon6mes, l'dvaluation sont dgalement effectuds dans cet environnement de programmation. Le langage PERL est en grand partie utilisd parall61ement pour traitement du texte du corpus. Les d6tails techniques de JANUS-III sont donnds dans [2], [5], [6]. Cependant nous en prdsentons bri6vement quelques points ci-apr6s. 2 Le Module RAPHAEL L'architecture du module de reconnaissance RAPHAEL est pr6sent6e sur la [Fig. 3]. L'analyse de la parole produit une suite de vecteurs de param6tres acoustiques. Ces vecteurs sont utilis6s par un moteur de recherche base de CMC pour estimer la suite des phon6mes 6nonc6s. Un mod61e de langage stochastique h bigramme et trigramme, et un dictionnaire des variantes phon6tiques sont en parall61e exploit6s pour restreindre le champ de recherche I. ALl cours de la recherche le dictionnaire phon6tique fournit le(s) phon6me(s) suivant(s). Le modble probabiliste de langage base de bigramme et de trigramme est utilis6 Iors de la transition entre deux mots pour fournir un ensemble de roots [Fig. 4]. 1 Avec 45 phon6mes en moyenne une suite de cinq phon6mes se transforme th6oriquement en un arbre de d6cision de 455 = 184,528,125 feuilles ! 37

~_~Traduction Automatique IANE(GETA),GB(I,ATL) J


I Synth~se la Parole de LA1P-~VI'S (LAII') 1 Fig.2. Lescomposantsdu syst~me
d6veloppement du module de reconnaissance RAPHAEL a dt6 effectu6 sur la plate-forme iogicielle de JANUS-Ill. RAPHAEL donne en sortie un treillis de roots sous le protocole TCP/IP. Le traducteur utilise ce rdsultat pour en donner une version traduite. Cette version est ensuite envoyde au synthdtiseur de la parole. Dans cet article nous nous int6resserons seulement au module de reconnaissance RAPHAEL. Pour l'instant la stratdgie d'dchange entre les modules est enti6rement sdquentielle. Afin d'amdliorer le rdsultat final (surtout du

~_~__~ Acquisitionde 1 la parole

I,

Base de donndesdes paramdtres ] des ChaTnesde Markov Cachdes

J
~Z%%? ' o

I Traitementnumdrique,Estimationdes param~tresacoustiques ~ ModUlestochastiquede langage (bigrammeet trigramme)

Chainesde MarkovCachdespour la reconnaissancephondmique Dictionnairephondtique Dict (vocabulaire de reconnaissance)

Fig. 3. Sehdmadu modulede reconnaissance phondmiqueRAPHAEL.
2.1 C
h a i n e d e M a r k o v Cachdes cet alignement l'algorithme de Baum-Welch [5] procdde ~ l'estimation des paramdtres de chaque CMC prdsente dans la cha~ne. Ce procddd est rdpdt6 pour tous les 6noncds du corpus d'apprentissage et cela plusieurs fois. La prdsence des diffdrents contextes phondmiques permet /i ce procdd6 de minimiser le taux d'erreur de reconnaissance. L'dvaluation du taux d'erreur /l la fin de chaque itdration permet d'dtudier l'avancement de l'apprentissage.

Pour utiliser les CMC il faut conduire une phase d'apprentissage prdalable dans laquelle on adapte les probabilitds des transitions et des symboles sortis pour un phondme donnd de manidre ii ce que la probabilit6 du processus associd soit maximale. Les paramdtres des moddles et la transcription phondtique des 6noncds du corpus sont deux 616ments essentiels d'apprentissage. RAPHAEL comporte 45 CMC reprdsentant 42 phondmes de base du fran~ais et 3 moddles pour le silence et le bruit. A quelques exceptions prds les CMC se composent de trois 6tats. Le vecteur de paramdtres d'entrde est de dimension 122. Les CMC ont 16 distributions Gaussiennes pour chaque dtat. Lots de l'apprentissage nous produisons la transcription phondtique correspondante chaque dnoncd (cela se fait ~ l'aide du dictiomaaire phondtique). Pour chaque 6nonc~ les CMC correspondant aux phondmes sont concatdndes pour crder une longue chaine. Ensuite i'algorithme de Viterbi [5] propose un alignement de I'dnoncd avec cette chaine. Avec

2.2

Mod/~lede iangage stochastique

2 Les
coefficients MFCC [5] d'ordre 16 sont calculds sur une trame de 16 ms de parole, avec un pas d'avancement de 10ms. La parole est 6chantillonnde /l 16 kHz et sur 16 bits. Les MFCC, l'dnergie du signal, et leurs premiere et seconde ddrivdes (51 valeurs) subissent ensuite une analyse en composantes principales (ACP) pour rdduire la dimension du vecteur /l 12. La matrice d'ACP est calculde avant la phase d'apprentissage, sur un grand corpus enregistrd. 38

Afin de rdduire le champ de recherche, un moddle de langage doit ~tre utilisd. Bien que dans les systdmes /i commande vocale qui utilisent une syntaxe rdduite les grammaires finies ou rdcurrentes peuvent ~tre utilisdes, ceiles-ci ne sont pas capables de ddcrire tousles phdnomdnes de la langue par[de (ellipses, hdsitations, rdpdtitions, etc.). Pour cette raison il est souhaitable d'utiliser un moddle stochastique qui estime dans un contexte donnd, la probabilit6 de succession des mots. Dans le moddle actuel les contextes gauches d'ordres un et deux (bigramme et trigramme) Sont en marne temps exploitds. Le bigramme est utilisd dans la premidre phase de recherche pour crder un treillis de mots, puis le trigramme est utilisd pour raffiner le rdsultat et ddterminer les N meilleurs phrases plausibles. Le moddle de langage se charge en m~me temps de ia rdsolution de l'aceord en frangais. Le calcui des paramdtres de ce moddle a dt6 effectual /t partir des corpus enregistrds et transcrits. Dans l'6tat actuel un vocabulaire de 7000 mots a dt6 sdlectionnd.

-..

-

L'hypoth6se e mot #1

--:~,.

L'hypoth6se de mot #2

Repr6sentation d'un phoneme

Dans un mot le dictionnaire phon6tique est utilis6 pour trouver et enchMner les phonemes suivants selon les variantes phon6tiques disponibles.

Pour d6terminer les roots et les phon6mes suivants le module stochastique du langage et le vocabulairc transcrit en phon6tique sont en m6me temps utilis6s.

Fig. 4. Representation de I'algorithme de recherche

2.3 Dictionnaire
Phon~tique
La conversion d'une chMne d'hypoth~ses phon6tiques en une chMne orthographique se fair ~t partir d'un dictionnaire phon6tique. Pour couvrir un grand nombre de prononciations diff&entes dues aux diff&ents dialectes de la langue et aux habitudes des locuteurs, ce dictionnaire contient pour chaque mot un ensemble de variantes phon6tiques. A chaque hypoth6se de mot propos6 par le mod61e de langage on associe cet ensemble de variantes. Ind6pendamment donc de la variante utilis6e dans 1'6nonc6, nous obtenons la m6me transcription orthographique. Nous utilisons sp6cifiquement cette technique pour couvrir les variantes produites par ia liaison, par exemple : Je suis parti de la maison. Je suis all~ h la maison. 3 (Z& sHi paRti ...) (Z& sHiz ale ...)

ensemble de BREF-80 comprenant les 6nonc6s de 4 femmes et 4 hommes a 6t6 utilis6 pour l'6valuation 4. Le vocabulaire a 6t6 transcrit soit manuellement, soit ~ partir du dictionnaire phon6tique BDLEX-23000. Le module de langage a 6t6 estim6 fi partir de BREF-80 et un corpus de texte d'~ peu pr6s 10 millions de mots extrait du journal Le Monde. Pour l'initialisation des CMC, au lieu d'utiliser les valeurs al6atoires (technique habituelle), nous avons choisi d'utiliser les mod61es issus du projet GlobalPhone [7]. Pour chaque phon6me de notre module nous avons manuellement choisi un phon6me dans une des langues support6es par GlobalPhone (principalement allemande) et nous avons utilis6 ses param6tres comme valeurs initiales de nos CMC. Ensuite ces mod61es ont 6t6 adapt6s au frangais au moyen de l'algorithme d'apprentissage d6crit en 2.1. A la fin de chaque it6ration et ce pour 3

L'apprentissage
4 Les
sous-corpus de l'apprentissage et de l'6valuation n'ont aucun 6nonc6 et locuteur en commun. En r6alit6, nous avons enlev6 tousles 6nonc6s en communs entre ces deux sous corpus. Ainsi le sous-corpus d'apprentissage comprend 4854 6nonc6s et le sous-corpus d'6valuation 371 6nonc6s. Nous avons retir6 105 6noncds pour assurer la disjonction des deux sous-corpus. 39

Le corpus BREF-80 [8] comportant 5330 6nonc6s par 80 locuteurs (44 femmes et 36 hommes) 3 a 6t6 utilis6 pour les phases d'apprentissage et d'6valuation. Un sous3 BREF-80 contient 3747 textes diff6rents et environ 150,000 mots.

it6rations, le syst~me a 6t~ 6valu6 avec le sous corpus de l'6valuation.

4 R6sultats
Les r6sultats d'dvaluation en terme de taux de reconnaissance sont donn6s dans le [Tableau 1]. Systdmes M0dbles issus de GlobalPhone Premiere it6ration Troisi6me it6ration % mots reconnus 29 88,8 91,1

Tableau 1. Les r6sultats de 1'6valuation

4.1

Commentaires

obtenus. Notre but est d'am61iorer le taux de reconnaissance par l'utilisation des mod61es phon6tiques contextuels et d'61argir le vocabulaire utilis6/t plus de 10000 mots. Pour atteindre ce but nous allons sp6cialiser le vocabulaire dans le domaine du tourisme et utiliser d'autres corpus de la parole spontan6e dans ce domaine avec un nombre plus important de locuteurs. En mfime temps nous d6finirons un protocole d'6change plus 61abor6 avec le module de traduction afin de permettre la communication d'informations linguistiques et statistiques au module de traduction, toujour dans le but d'amdliorer les performances de notre syst6me.

Une tr6s bonne initialisation de certaines consonnes identiques dans des diff6rentes langues (p, t, k, b, d, g, etc.) a rapidement permis d'obtenir un syst6me fonctionnel. On constate une saturation tr6s rapide du taux de reconnaissance d6s la troisi+me it6ration. Nous pouvons distinguer trois types de probl6me qui nous empachent d'atteindre un meilleur taux de reconnaissance :  Fautes de frappe dans le texte du corpus,  Transcription erron6e ou insuffisamment d6taill6e des ~noncds, ,, La couverture partielle de toutes les variantes phon6tiques d'un mot. Ces trois probl~mes sont les causes d'un grand nombre d'erreurs d'alignement qui vont directement influencer le r6sultat final. Nous devons donc effectuer une v6rification compl6te du corpus et du dictionnaire phon6tique. Les mots hors du vocabulaire sont fi l'origine d'un pourcentage important d'erreurs. En effet, dans 371 6noncds du sous-corpus de l'6valuation nous rencontrons environ 300 mots hors vocabulaire. Ces mots repr6sentent environ 3,5 % de la taille du vocabulaire. I I n e sont pas reprdsentds dans le corpus d'apprentissage et leur transcription n'existe pas dans le dictionnaire phon6tique.

Remerciement
Nous remercions Alex Waibel pour la mise /t disposition de JANUS-III et Tanja Schultz pour son support scientifique et technique dans l'utilisation des r6sultats du projet GlobalPhone.

References

1 Hutchins
W. J. (1986) Machine Translation : Past, Present, Future. Ellis Horwood, John Wiley & Sons, Chichester, England, 382 p. 

2 Finke
M., Geutner P., Hild H., Kemp T., Ries K., Westphal M. (1997) : The KarlsruheVerbmobil Speech Recognition Engine, Proc. of ICASSP, Munich, Germany. 
																		
3 Boitet
Ch., (1986) GETA's MTmethodology and a blueprint for its adaptation to speech translation within C-STARI1, ATR International Workshop on Speech Translation, Kyoto, Japan. 

4 Keller, E. (1997). Simplification of TTS architecture versus Operational quality, Proceedings of EuroSpeech'97, Rhodes, Greece. 

5 Rabiner
L., Juang B.H. (1993), Fundamentals of Speech Recognition, Prentice Hall, 507 p. 

6 Haton
J.P., Pierrel J.M., Perennou G., Caelen J., Gauvain J.L. (1991), Reconnaissance automatique de laparole, BORDAS, Paris, 239 p. 

Schultz T. Waibel A., Fast Bootstrapping of LVCSR systems with multilingual phonem sets, Proceedings of EuroSpeech'97, Rhodes, Greece. 

Lamel L.F., Gauvain J.L., Eskenazi M. (1991), BREF, a Large Vocabulary Spoken Colpus for French, Proceedings of. EuroSpeech'91, Genoa, Italy.
References 1 Appelt D.
and Israel D.
(1999). Introduction to Information Extraction Technology.
(IJCAI-99) Tutorial, Stockholm, Sweden (available at: http://www.ai.sri.
com/~appelt/ie-tutorial/) 2 Masayuki Asahara, Yuji Matsumoto, Extended models and tools for high-performance part-of-speech tagger, Proceedings of the 18th conference on Computational linguistics, p.21-27, July 31-August 04, 2000, Saarbrcken, Germany 3 Frdric Bchet, Alexis Nasr, Franck Genet, Tagging unknown proper names using decision trees, Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, p.77-84, October 03-06, 2000, Hong Kong 4 Daniel M.
Bikel, Scott Miller, Richard Schwartz, Ralph Weischedel, Nymble: a high-performance learning name-finder, Proceedings of the fifth conference on Applied natural language processing, p.194-201, March 31-April 03, 1997, Washington, DC 5 Andrew Eliot Borthwick, Ralph Grishman, A maximum entropy approach to named entity recognition, 1999 6 Collins M.
and Singer Y.
(1999) Unsupervised models for named entity classification.
In Proceedings of EMNLP/WVLC, 1999, MA, pp.
189--196. 7 Cucchiarelli A.
and Velardi P.
(1999) Adaptability of linguistic resources to new domains: an experiment with proper noun dictionaries.
In Proceedings of the Vextal Conference, Venice, Italy, pp.
25--30. 8 Andrei Mikheev, Marc Moens, Claire Grover, Named Entity recognition without gazetteers, Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics, June 08-12, 1999, Bergen, Norway 9 Raymond J.
Mooney, Induction Over the Unexplained: Using Overly-General Domain Theories to Aid Concept Learning, Machine Learning, v.10 n.1, p.79-110, Jan.
1993 10 MUC-6 (1995) Proceedings of the Sixth Message Understanding Conference (DARPA), Morgan Kaufmann Publishers, San Francisco.
11 Poibeau
T and Kosseim L.
(2001) Proper-name Extraction from Non-Journalistic Texts.
Proceeding of the 11th Conference Computational Linguistics in the Netherlands, Tilburg.
Netherlands, Rodopi.
12 Satoshi
Sekine, Yoshio Eriguchi, Japanese named entity extraction evaluation: analysis of results, Proceedings of the 18th conference on Computational linguistics, July 31-August 04, 2000, Saarbrcken, Germany 13 Silberztein M.
(1993) Dictionnaires lectroniques.
Masson, Paris.
14 David
Yarowsky, Unsupervised word sense disambiguation rivaling supervised methods, Proceedings of the 33rd annual meeting on Association for Computational Linguistics, p.189-196, June 26-30, 1995, Cambridge, Massachusetts
References 1 Benson, M.
(1990). Collocations and general-purpose dictionaries.
International Journal of Lexicography, 3(1), 23--35.
2 Peter
F.
Brown, Jennifer C.
Lai, Robert L.
Mercer, Aligning sentences in parallel corpora, Proceedings of the 29th annual meeting on Association for Computational Linguistics, p.169-176, June 18-21, 1991, Berkeley, California 3 Catizone R., Russell G., and Warwick S.
(1989). Deriving Translation Data from Bilingual Texts.
In Proceedings of the First International Lexical Acquisition Workshop, Detroit.
4 Church, K., Gale, W., Hanks, P., and Hindle, D.
(1991). Using Statistics in Lexical Analysis.
In Zernick, U.
(ed.), Lexical Acquisition: Exploiting On-Line Resources to Build a Lexicon, Lawrence Erlbaum Associates, pp.
115--164. 5 Ted Dunning, Accurate methods for the statistics of surprise and coincidence, Computational Linguistics, v.19 n.1, March 1993 6 William A.
Gale, Kenneth W.
Church, A program for aligning sentences in bilingual corpora, Computational Linguistics, v.19 n.1, March 1993 7 Gross, G.
(1996). Les expressions figes en franais.
OPHRYS, Paris.
8 Pierre
Isabelle, Marc Dymetman, George Foster, Jean-Marc Jutras, Elliott Macklovitch, Francois Perrault, Xiaobo Ren, Michel Simard, Translation analysis and translation automation, Proceedings of the 1993 conference of the Centre for Advanced Studies on Collaborative research: distributed computing, October 24-28, 1993, Toronto, Ontario, Canada 9 Laenzlinger, C.
and Wehrli, E.
(1991). Fips, un analyseur interactif pour le franais.
TA informations, 32(2): 35--49.
10 Christopher
D.
Manning, Hinrich Schtze, Foundations of statistical natural language processing, MIT Press, Cambridge, MA, 1999 11 Melby A.
(1982). A Bilingual Concordance System and its Use in Linguistic Studies.
In Proceedings of the Eighth LACUS Forum, Columbia, SC, pp.
541--549. 12 Romary L.
and Bonhomme P.
(2000). Parallel alignment of structured documents.
Vronis J.
(Ed.). Parallel Text Processing.
Dordrecht: Kluwer.
13 Michel
Simard, George F.
Foster, Pierre Isabelle, Using cognates to align sentences in bilingual corpora, Proceedings of the 1993 conference of the Centre for Advanced Studies on Collaborative research: distributed computing, October 24-28, 1993, Toronto, Ontario, Canada 14 Frank Smadja, Retrieving collocations from text: Xtract, Computational Linguistics, v.19 n.1, March 1993 15 Eric Wehrli, Parsing and Collocations, Proceedings of the Second International Conference on Natural Language Processing, p.272-282, June 02-04, 2000
Classifying Biological Full-Text Articles for Multi-Database Curation Wen-Juan Hou, Chih Lee and Hsin-Hsi Chen Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan {wjhou, clee}@nlg.csie.ntu.edu.tw; hhchen@csie.ntu.edu.tw Abstract In this paper, we propose an approach for identifying curatable articles from a large document set.
This system considers three parts of an article (title and abstract, MeSH terms, and captions) as its three individual representations and utilizes two domain-specific resources (UMLS and a tumor name list) to reveal the deep knowledge contained in the article.
An SVM classifier is trained and cross-validation is employed to find the best combination of representations.
The experimental results show overall high performance.
Track (http://ir.ohsu.edu/genomics) of TREC 2004 and 2005 organized categorization tasks.
The former focused on simplified GO terms while the latter included the triage for "tumor biology", "embryologic gene expression", "alleles of mutant phenotypes" and "GO" articles.
The increase of the numbers of participants at Genomics Track shows that biological classification problems attracted much attention.
This paper employs the domain-specific knowledge and knowledge learned from full-text articles to classify biological text.
Given a collection of articles, various methods are explored to extract features to represent a document.
We use the experimental data provided by the TREC 2005 Genomics Track to evaluate different methods.
The rest of this paper is organized as follows.
Section 2 sketches the overview of the system architecture.
Section 3 specifies the test bed used to evaluate the proposed methods.
The details of the proposed system are explained in Section 4.
The experimental results are shown and discussed in Section 5.
Finally, we make conclusions and present some further work.
1 Introduction
Organism databases play a crucial role in genomic and proteomic research.
It stores the up-to-date profile of each gene of the species interested.
For example, the Mouse Genome Database (MGD) provides essential integration of experimental knowledge for the mouse system with information annotated from both literature and online sources (Bult et al., 2004).
To provide biomedical scientists with easy access to complete and accurate information, curators have to constantly update databases with new information.
With the rapidly growing rate of publication, it is impossible for curators to read every published article.
Since fully automated curation systems have not met the strict requirement of high accuracy and recall, database curators still have to read some (if not all) of the articles sent to them.
Therefore, it will be very helpful if a classification system can correctly identify the curatable or relevant articles in a large number of biological articles.
Recently, several attempts have been made to classify documents from biomedical domain (Hirschman et al., 2002).
Couto et al.(2004) used the information extracted from related web resources to classify biomedical literature.
Hou et al.(2005) used the reference corpus to help classifying gene annotation.
The Genomics System Overview Figure 1 shows the overall architecture of the proposed system.
At first, we preprocess each training article, and divide it into three parts, including (1) title and abstract, (2) MeSH terms assigned to this article, and (3) captions of figures and tables.
They are denoted as "Abstract", "MeSH", and "Caption" in this paper, respectively.
Each part is considered as a representation of an article.
With the help of domain-specific knowledge, we obtain more detail representations of an article.
In the model selection phase, we perform feature ranking on each representation of an article and employ cross-validation to determine the number of features to be kept.
Moreover, we use cross-validation to obtain the best combination of all the representations.
Finally, a support vector machine (SVM) (Vapnik, 1995; Hsu et al., 2003) classifier is obtained.
Abstract AbsSEM/TM Full-Text Training Articles Preprocessing MeSH MeSHSEM Model Selection Caption CapSEM/TM Domain-Specific Knowledge A New Full-Text Article Preprocessing Multiple Parts PartsSEM/TM SVM Classifier Yes/No Figure 1.
System Architecture Experimental Data We train classifiers for classifying biomedical articles on the Categorization Task of the TREC 2005 Genomics Track.
The task uses data from the Mouse Genome Informatics (MGI) system (http://www.informatics.jax.org/) for four categorization tasks, including tumor biology, embryologic gene expression, alleles of mutant phenotypes and GO annotation.
Given a document and a category, we have to identify whether it is relevant to the given category.
The document set consists of some full-text data obtained from three journals, i.e., Journal of Biological Chemistry, Journal of Cell Biology and Proceedings of the National Academy of Science in 2002 and 2003.
There are 5,837 training documents and 6,043 testing documents.
Methods Document Preprocessing In the preprocessing phase, we perform acronym expansion on the articles, remove the remaining tags from the articles and extract three parts of interest from each article.
Abbreviations are often used to replace long terms in writing articles, but it is possible that several long terms share the same short form, especially for gene/protein names.
To avoid ambiguity and enhance clarity, the acronym expansion operation replaces every tagged abbreviation with its long form followed by itself in a pair of parentheses.
4.2 Employing
Domain-Specific Knowledge can identify the gene names contained in an article.
Moreover, by further consulting organism databases, we can get the properties of the genes.
Two domain-specific resources are exploited in this study.
One is the Unified Medical Language System (UMLS) (Humphreys et al., 1998) and the other is a list of tumor names obtained from Mouse Tumor Biology Database (MTB)1.
UMLS contains a huge dictionary of biomedical terms  the UMLS Metathesaurus and defines a hierarchy of semantic types  the UMLS Semantic Network.
Each concept in the Metathesaurus contains a set of strings, which are variants of each other and belong to one or more semantic types in the Semantic Network.
Therefore, given a string, we can obtain a set of semantic types to which it belongs.
Then we obtain another representation of the article by gathering the semantic types found in the part of the article.
Consequently, we get another three much deeper representations of an article after this step.
They are denoted as "AbstractSEM", "MeSHSEM" and "CaptionSEM".
We use the list of tumor names on the Tumor task.
We first tokenize all the tumor names and stem each unique token.
With the resulting list of unique stemmed tokens, we use it as a filter to remove the tokens not in the list from the "Abstract" and "Caption", which produce "AbstractTM" and "CaptionTM".
4.3 Model
Selection As mentioned above, we generate several representations for an article.
In this section, we explain how feature selection is done and how the best combination of the representations With the help of domain-specific knowledge, we can extract the deeper knowledge in an article.
For example, with a gene name dictionary, we fiof an article is obtained.
For each representation, we first rank all the tokens in the training documents via the chi-square test of independence.
Postulating the ranking perfectly reflects the effectiveness of the tokens in classification, we then decide the number of tokens to be used in SVM classification by 4-fold cross-validation.
In cross-validation, we use the TF*IDF weighting scheme.
Each feature vector is then normalized to a unit vector.
We set C+ to ur* Cbecause of the relatively small number of positive examples, where C+ and Care the penalty constants on positive and negative examples in SVMs.
After that, we obtain the optimal number of tokens and the corresponding SVM parameters Cand gamma, a parameter in the radial basis kernel.
In the rest of this paper, "Abstract30" denotes the "Abstract" representation with top-30 tokens, "CaptionSEM10" denotes "CaptionSEM" with top-10 tokens, and so forth.
After feature selection is done for each representation, we try to find the best combination by the following algorithm.
Given the candidate representations with selected features, we start with an initial set containing some or zero representation.
For each iteration, we add one representation to the set by picking the one that enhances the cross-validation performance the most.
The iteration stops when we have exhausted all the representations or adding more representation to the set doesn't improve the cross-validation performance.
For classifying the documents with better features, we run the algorithm twice.
We first start with an empty set and obtain the best combination of the basic three representations, e.g., "Abstract10", "MeSH30" and "Caption10".
Then, starting with this combination, we attempt to incorporate the three semantic representations, e.g., "Abstract30SEM", "MeSH30SEM" and "Caption10SEM", and obtain the final combination.
Instead of using this algorithm to incorporate the "AbstractTM" and "CaptionTM" representations, we use them to replace their unfiltered counterparts "Abstract" and "Caption" when the cross-validation performance is better.
Results and Discussions Utility (NU)2 measure).
For category Allele, "Caption" and "AbstractSEM" perform the best among the basic and semantic representations, respectively.
For category Expression, "Caption" plays an important role in identifying relevant documents, which agrees with the finding by the winner of KDD CUP 2002 task 1 (Regev et al., 2002).
Similarly, MeSH terms are crucial to the GO category, which are used by top-performing teams (Dayanik et al., 2004; Fujita, 2004) in TREC Genomics 2004.
For category Tumor, MeSH terms are important, but after semantic type extraction, "AbstractSEM" exhibits relatively high cross-validation performance.
Since only 10 features are selected for the "AbstractSEM", using this representation alone may be susceptible to over-fitting.
Finally, by comparing the performance of the "AbstractTM" and "Abstract", we find the list of tumor names helpful for filtering abstracts.
We list the results for the test data in Table 2.
Column "Experiment" identifies our proposed methods.
We show six experiments in Table 2: one for Allele (AL), one for Expression (EX), one for GO (GO) and three for Tumor (TU, TN and TS).
Column "cv NU" shows the cross-validation NU measure, "NU" shows the performance on the test data and column "Combination" lists the combination of the representations used for each experiment.
In this table, "M30" is the abbreviation for "MeSH30", "CS10" is for "CaptionSEM10", and so on.
The combinations for the first 4 experiments, i.e., AL, EX, GO and TU, are obtained by the algorithm described in Section 4.3, while the combination for TN is obtained by substituting "AbstractTM30" for "Abstract30" in the combination for TU.
The experiment TS only uses the "AbstractSEM10" because its cross-validation performance beats all other combinations for the Tumor category.
The combinations of the first 5 experiments illustrate that adding other inferior representations to the best one enhances the performance, which implies that the inferior ones may contain important exclusive information.
The cross-validation performance fairly predicts the performance on the test data, except for the last experiment TS, which relies on only 10 features and is therefore susceptible to over-fitting.
Table 1 lists the cross-validation results of each representation for each category (in Normalized Please refer to the TREC 2005 Genomics Track Protocol (http://ir.ohsu.edu/genomics/2005protocol.html).
Abstract MeSH Caption AbstractSEM MeSHSEM CaptionSEM AbstractTM CaptionTM Table 1.
Partial Cross-validation Results.
Experiment AL (for Allele) EX (for Expression) GO (for GO) TU (for Tumor) TN (for Tumor) TS (for Tumor) cv NU 0.8717 0.7691 0.5402 0.8742 0.8764 0.8814 NU 0.8423 0.7515 0.5332 0.8299 0.8747 0.5699 Recall 0.9488 0.8190 0.8803 0.9000 0.9500 0.6500 Precision 0.3439 0.1593 0.1873 0.0526 0.0518 0.0339 F-score 0.5048 0.2667 0.3089 0.0994 0.0982 0.0645 Combination M30+C10+A10+CS10+AS10+MS10 M10+C10+CS10+MS10 M10+C10+MS10 M30+C30+A30+AS10+CS30 M30+C30+AT30+AS10+CS30 AS10 Table 2.
Evaluation Results.
Subtask Allele Expression GO Annotation Tumor NU (Best/Median) 0.8710/0.7773 0.8711/0.6413 0.5870/0.4575 0.9433/0.7610 Recall (Best/Median) 0.9337/0.8720 0.9333/0.7286 0.8861/0.5656 1.0000/0.9500 Precision (Best/Median) 0.4669/0.3153 0.1899/0.1164 0.2122/0.3223 0.0709/0.0213 F-score (Best/Median) 0.6225/0.5010 0.3156/0.2005 0.3424/0.4107 0.1325/0.0417 Table 3.
Best and Median Results for Each Subtask on TREC 2005 (Hersh et al., 2005).
To compare with our performance, we list the best and median results for each subtask on the genomics classification task of TREC 2005 in Table 3.
Comparing to Tables 2 and 3, it shows our experimental results have overall high performance.
Conclusions and Further Work In this paper, we demonstrate how our system is constructed.
Three parts of an article are extracted to represent its content.
We incorporate two domain-specific resources, i.e., UMLS and a list of tumor names.
For each categorization work, we propose an algorithm to get the best combination of the representations and train an SVM classifier out of this combination.
Evaluation results show overall high performance in this study.
Except for MeSH terms, we can try other sections in the article, e.g., Results, Discussions and Conclusions as targets of feature extraction besides the abstract and captions in the future.
Finally, we will try to make use of other available domain-specific resources in hope of enhancing the performance of this system.
Acknowledgements Research of this paper was partially supported by National Science Council, Taiwan, under the contracts NSC94-2213-E-002-033 and NSC94-2752-E-001-001-PAE.
References Bult, C.
J., Blake, J.
A., Richardson, J.
E., Kadin, J.
A., Eppig, J.
T. and the Mouse Genome Database Group.
The Mouse Genome Database (MGD): Integrating Biology with the Genome.
Nucleic Acids Research, 32, D476D481, 2004.
Couto, F.
M., Martins, B.
and Silva, M.
J . Classifying Biological Articles Using Web Resources.
Proceedings of the 2004 ACM Symposium on Applied Computing, 111-115, 2004.
Dayanik, A., Fradkin, D., Genkin, A., Kantor, P., Lewis, D.
D., Madigan, D.
and Menkov, V . DIMACS at the TREC 2004 Genomics Track.
Proceedings of the Thirteenth Text Retrieval Conference, 2004.
Fujita, S., . Revisiting Again Document Length Hypotheses TREC-2004 Genomics Track Experiments at Patolis.
Proceedings of the Thirteenth Text Retrieval Conference, 2004.
Hersh, W., Cohen, A., Yang, J., Bhuptiraju, R.T., Toberts, P.
and Hearst, M . TREC 2005 Genomics Track Overview.
Proceedings of the Fourteenth Text Retrieval Conference, 2005.
Hirschman, L., Park, J., Tsujii, J., Wong, L.
and Wu, C.
H . Accomplishments and Challenges in Literature Data Mining for Biology.
Bioinformatics, 18(12): 1553-1561, 2002.
Hou, W.
J., Lee, C., Lin, K.
H. Y.
and Chen, H.
H . A Relevance Detection Approach to Gene Annotation.
Proceedings of the First International Symposium on Semantic Mining in Biomedicine, http://ceur-ws.org, 148: 15-23, 2005.
Hsu, C.
W., Chang, C.
C. and Lin, C.
J . A Practical Guide to Support Vector Classification.
http://www.csie.ntu.edu.tw /~cjlin/libsvm/index.html, 2003.
Humphreys, B.
L., Lindberg, D.
A., Schoolman, H.
M. and Barnett, G.
O . The Unified Medical Language System: an Informatics Research Collaboration.
Journal of American Medical Information Association, 5(1):1-11, 1998.
Regev, Y., Finkelstein-Landau, M.
and Feldman, R . Rule-based Extraction of Experimental Evidence in the Biomedical Domain the KDD Cup (Task 1).
SIGKDD Explorations, 4(2):90-92, 2002.
Vapnik, V . The Nature of Statistical Learning Theory, Springer-Verlag, 1995 .
Multidocument Summarization via Information Extraction Michael White and Tanya Korelsky CoGenTex, Inc.
Ithaca, NY Claire Cardie, Vincent Ng, David Pierce, and Kiri Wagstaff Department of Computer Science Cornell University, Ithaca, NY mike,tanya@cogentex.com We present and evaluate the initial version of RIPTIDES, a system that combines information extraction, extraction-based summarization, and natural language generation to support userdirected multidocument summarization.
IE system and the Summarizer in turn.
2.1 IE
System The domain for the initial IE-supported summarization system and its evaluation is natural disasters.
Very briefly, a top-level natural disasters scenario template contains: document-level information (e.g.
docno, date-time); zero or more agent elements denoting each person, group, and organization in the text; and zero or more disaster elements.
Agent elements encode standard information for named entities (e.g.
name, position, geo-political unit).
For the most part, disaster elements also contain standard event-related fields (e.g.
type, number, date, time, location, damage sub-elements).
The final product of the RIPTIDES system, however, is not a set of scenario templates, but a user-directed multidocument summary.
This difference in goals influences a number of template design issues.
First, disaster elements must distinguish different reports or views of the same event from multiple sources.
As a result, the system creates a separate disaster event for each such account.
Disaster elements should also include the reporting agent, date, time, and location whenever possible.
In addition, damage elements (i.e.
human and physical effects) are best grouped according to the reporting event.
Finally, a slight broadening of the IE task was necessary in that extracted text was not constrained to noun phrases.
In particular, adjectival and adverbial phrases that encode reporter confidence, and sentences and clauses denoting relief effort progress appear beneficial for creating informed summaries.
Figure 2 shows the scenario template for one of 25 texts tracking the 1998 earthquake in Afghanistan (TDT2 Topic 89).
The texts were also manually annotated for noun phrase coreference; any phrase involved in a coreference relation appears underlined in the running text.
The RIPTIDES system for the most part employs a traditional IE architecture [4].
In addition, we use an in-house implementation of the TIPSTER architecture [8] to manage all linguistic annotations.
A preprocessor first finds sentences and tokens.
For syntactic analysis, we currently use the Charniak [5] parser, which creates Penn Treebank-style parses [9] rather than the partial parses used in most IE systems.
Output from the parser is converted automatically into TIPSTER parse and part-of-speech annotations, which are added to the set of linguistic annotations for the document.
The extraction phase of the system identifies domain-specific relations among relevant entities in the text.
It relies on Autoslog-XML, an XSLT implementation of the Autoslog-TS system [12], to acquire extraction patterns.
Autoslog-XML is a weakly supervised learning system that requires two sets of texts for training -one set comprises texts relevant to the domain of interest and the other, texts not relevant Although recent years has seen increased and successful research efforts in the areas of single-document summarization, multidocument summarization, and information extraction, very few investigations have explored the potential of merging summarization and information extraction techniques.
This paper presents and evaluates the initial version of RIPTIDES, a system that combines information extraction (IE), extraction-based summarization, and natural language generation to support userdirected multidocument summarization.
(RIPTIDES stands for RapIdly Portable Translingual Information extraction and interactive multiDocumEnt Summarization).
Following [10], we hypothesize that IE-supported summarization will enable the generation of more accurate and targeted summaries in specific domains than is possible with current domain-independent techniques.
In the sections below, we describe the initial implementation and evaluation of the RIPTIDES IE-supported summarization system.
We conclude with a brief discussion of related and ongoing work.
Figure 1 depicts the IE-supported summarization system.
The system first requires that the user select (1) a set of documents in which to search for information, and (2) one or more scenario templates (extraction domains) to activate.
The user optionally provides filters and preferences on the scenario template slots, specifying what information s/he wants to be reported in the summary.
RIPTIDES next applies its Information Extraction subsystem to generate a database of extracted events for the selected domain and then invokes the Summarizer to generate a natural language summary of the extracted information subject to the user's constraints.
In the subsections below, we describe the fiuser information need Summarizer multi-document template merging event-oriented structure text collection IE System slot filler slot slot filler filler slot filler slotslot filler filler slot filler slot filler...
... slot filler slot filler filler slot slot filler slot filler slot filler ...
... slot filler slot filler scenario templates content selection event-oriented structure with slot importance scores A powerful earthquake struck Afghanistan on May 30 at 11:25...
Damage VOA (06/02/1998) estimated that 5,000 were killed by the earthquake, whereas AP (APW, 06/02/1998) instead reported ...
Relief Status NLG of summary CNN (06/02/1998): Food, water, medicine and other supplies have started to arrive.
[...] summary Figure 1.
RIPTIDES System Design to the domain.
Based on these and a small set of extraction pattern templates, the system finds a ranked list of possible extraction patterns, which a user then annotates with the appropriate extraction label (e.g.
victim). Once acquired, the patterns are applied to new documents to extract slot fillers for the domain.
Selectional restrictions on allowable slot fillers are implemented using WordNet [6] and BBN's Identifinder [3] named entity component.
In the current version of the system, no coreference resolution is attempted; instead, we rely on a very simple set of heuristics to guide the creation of output templates.
The disaster scenario templates extracted for each text are provided as input to the summarization component along with all linguistic annotations accrued in the IE phase.
No relief slots are included in the output at present, since there was insufficient annotated data to train a reliable sentence categorizer.
Selected News Excerpts, as shown in the two sample summaries appearing in Figures 3 and 4, and discussed further in Section 2.2.5 below.
2.2.1 Summarization
Stages The Summarizer produces each summary in three main stages.
In the first stage, the output templates are merged into an eventoriented structure, while keeping track of source information.
The merge operation currently relies on simple heuristics to group extracted facts that are comparable; for example, during this phase damage reports are grouped according to whether they pertain to the event as a whole, or instead to damage in the same particular location.
Heuristics are also used in this stage to determine the most relevant damage reports, taking into account specificity, recency and news source.
Towards the same objective but using a more surface-oriented means, simple word-overlap clustering is used to group sentences from different documents into clusters that are likely to report similar content.
In the second stage, a base importance score is first assigned to each slot/sentence based on a combination of document position, document recency and group/cluster membership.
The base importance scores are then adjusted according to user-specified preferences and matching 2.2 The Summarizer In order to include relief and other potentially relevant information not currently found in the scenario templates, the Summarizer extracts selected sentences from the input articles and adds them to the summaries generated from the scenario templates.
The extracted sentences are listed under the heading Document no.: ABC19980530.1830.0342 Date/time: 05/30/1998 18:35:42.49 Disaster Type: earthquake location: Afghanistan date: today magnitude: 6.9 magnitude-confidence: high epicenter: a remote part of the country PAKISTAN MAY BE PREPARING FOR ANOTHER TEST Thousands of people are feared dead following...
(voiceover) ...a powerful earthquake that hit Afghanistan today.
The quake registered 6.9 on the Richter scale, centered in a remote part of the country.
(on camera) Details now hard to come by, but reports say entire villages were buried by the quake.
human-effect: victim: Thousands of people number: Thousands outcome: dead confidence: medium confidence-marker: feared physical-effect: object: entire villages outcome: damaged confidence: medium confidence-marker: Details now hard to come by / reports say Figure 2.
Example scenario template for the natural disasters domain criteria.
The adjusted scores are used to select the most important slots/sentences to include in the summary, subject to the userspecified word limit.
In the third and final stage, the summary is generated from the resulting content pool using a combination of top-down, schema-like text building rules and surface-oriented revisions.
The extracted sentences are simply listed in document order, grouped into blocks of adjacent sentences.
(2) any intermediate estimates that are lower than the maximum estimate.1 In the content determination stage, scores are assigned to the derived information units based on the maximum score of the underlying units.
In the summary generation stage, a handful of text planning rules are used to organize the text for these derived units, highlighting agreement and disagreement across sources.
2.2.2 Specificity
of Numeric Estimates In order to intelligently merge and summarize scenario templates, we found it necessary to explicitly handle numeric estimates of varying specificity.
While we did find specific numbers (such as 3,000) in some damage estimates, we also found cases with no number phrase at all (e.g.
entire villages).
In between these extremes, we found vague estimates (thousands) and ranges of numbers (anywhere from 2,000 to 5,000).
We also found phrases that cannot be easily compared (more than half the region's residents).
To merge related damage information, we first calculate the numeric specificity of the estimate as one of the values NONE, VAGUE, RANGE, SPECIFIC, or INCOMPARABLE, based on the presence of a small set of trigger words and phrases (e.g.
several, as many as, from ...
to). Next, we identify the most specific current estimates by news source, where a later estimate is considered to update an earlier estimate if it is at least as specific.
Finally, we determine two types of derived information units, namely (1) the minimum and maximum estimates across the news sources, and 2.2.3 Improving the Coherence of Extracted Sentences In our initial attempt to include extracted sentences, we simply chose the top ranking sentences that would fit within the word limit, subject to the constraint that no more than one sentence per cluster could be chosen, in order to help avoid redundancy.
We found that this approach often yielded summaries with very poor coherence, as many of the included sentences were difficult to make sense of in isolation.
To improve the coherence of the extracted sentences, we have experimented with trying to boost coherence by favoring sentences in the context of the highest-ranking sentences over those with lower ranking scores, following the hypothesis that it is better to cover fewer topics in more depth than to change topics excessively.
In particular, we assign a score to a set of sentences by summing the base scores plus increasing coherence boosts for adjacent sentences, sentences that precede ones with an initial Less specific estimates such as "hundreds" are considered lower than more specific numbers such as "5000" when they are lower by more than a factor of 10.
Earthquake strikes Afghanistan A powerful earthquake struck Afghanistan last Saturday at 11:25.
The earthquake was centered in a remote part of the country and had a magnitude of 6.9 on the Richter scale.
Earthquake strikes quake-devastated villages in northern Afghanistan A earthquake struck quake-devastated villages in northern Afghanistan Saturday.
The earthquake had a magnitude of 6.9 on the Richter scale on the Richter scale.
Damage Estimates of the death toll varied.
VOA (06/02/1998) provided the highest estimate of 5,000 dead.
CNN (05/31/1998) and CNN (06/02/1998) supplied lower estimates of 3,000 and up to 4,000 dead, whereas APW (06/02/1998) gave the lowest estimate of anywhere from 2,000 to 5,000 dead.
People were injured, while thousands more were missing.
Thousands were homeless.
Quake-devastated villages were damaged.
Estimates of the number of villages destroyed varied.
CNN (05/31/1998) provided the highest estimate of 50 destroyed, whereas VOA (06/04/1998) gave the lowest estimate of at least 25 destroyed.
In Afghanistan, thousands of people were killed.
Damage Estimates of the death toll varied.
CNN (06/02/1998) provided the highest estimate of 4,000 dead, whereas ABC (06/01/1998) gave the lowest estimate of 140 dead.
In capital: Estimates of the number injured varied.
Selected News Excerpts CNN (06/01/98): Thousands are dead and thousands more are still missing.
Red cross officials say the first priority is the injured.
Getting medicine to them is difficult due to the remoteness of the villages affected by the quake.
PRI (06/01/98): We spoke to the head of the international red cross there, Bob McCaro on a satellite phone link.
He says it's difficult to know the full extent of the damage because the region is so remote.
There's very little infrastructure.
PRI (06/01/98): Bob McCaro is the head of the international red cross in the neighboring country of Pakistan.
He's been speaking to us from there on the line.
APW (06/02/98): The United Nations, the Red Cross and other agencies have three borrowed helicopters to deliver medical aid.
Figure 4.
200 word summary of actual IE output, with emphasis on Red Cross Further Details Heavy after shocks shook northern afghanistan.
More homes were destroyed.
More villages were damaged.
Landslides or mud slides hit the area.
Another massive quake struck the same region three months earlier.
Some 2,300 victims were injured.
Selected News Excerpts ABC (05/30/98): PAKISTAN MAY BE PREPARING FOR ANOTHER TEST Thousands of people are feared dead following...
ABC (06/01/98): RESCUE WORKERS CHALLENGED IN AFGHANISTAN There has been serious death and devastation overseas.
In Afghanistan...
CNN (06/02/98): Food, water, medicine and other supplies have started to arrive.
But a U.N. relief coordinator says it's a "scenario from hell".
Figure 3.
200 word summary of simulated IE output, with emphasis on damage cases.
We then perform a randomized local search for a good set of sentences according to these scoring criteria.
2.2.4 Implementation
The Summarizer is implemented using the Apache implementation of XSLT [1] and CoGenTex's Exemplars Framework [13].
The Apache XSLT implementation has provided a convenient way to rapidly develop a prototype implementation of the first two processing stages using a series of XML transformations.
In the first step of the third summary generation stage, the text building component of the Exemplars Framework constructs a "rough draft" of the summary text.
In this rough draft version, XML markup is used to partially encode the rhetorical, referential, semantic and morpho-syntactic structure of the text.
In the second generation step, the Exemplars text polishing component makes use of this markup to trigger surfacepronoun, and sentences that preceded ones with strongly connecting discourse markers such as however, nevertheless, etc.
We have also softened the constraint on multiple sampling from the same cluster, making use of a redundancy penalty in such fioriented revision rules that smooth the text into a more polished form.
A distinguishing feature of our text polishing approach is the use of a bootstrapping tool to partially automate the acquisition of application-specific revision rules from examples.
2.2.5 Sample
Summaries Figures 3 and 4 show two sample summaries that were included in our evaluation (see Section 3 for details).
The summary in Figure 3 was generated from simulated output of the IE system, with preference given to damage information; the summary in Figure 4 was generated from the actual output of the current IE system, with preference given to information including the words Red Cross.
While the summary in Figure 3 does a reasonable job of reporting the various current estimates of the death toll, the estimates of the death toll shown in Figure 4 are less accurate, because the IE system failed to extract some reports, and the Summarizer failed to correctly merge others.
In particular, note that the lowest estimate of 140 dead attributed to ABC is actually a report about the number of school children killed in a particular town.
Since no location was given for this estimate by the IE system, the Summarizer's simple heuristic for localized damaged reports -namely, to consider a damage report to be localized if a location is given that is not in the same sentence as the initial disaster description -did not work here.
The summary in Figure 3 also suffered from some problems with merging: the inclusion of a paragraph about thousands killed in Afghanistan is due to an incorrect classification of this report as a localized one (owing to an error in sentence boundary detection), and the discussion of the number of villages damaged should have included a report of at least 80 towns or villages damaged.
Besides the problems related to slot extraction and merging mentioned above, the summaries shown in Figures 3 and 4 suffer from relatively poor fluency.
In particular, the summaries could benefit from better use of descriptive terms from the original articles, as well as better methods of sentence combination and rhetorical structuring.
Nevertheless, as will be discussed further in Section 4, we suggest that the summaries show the potential for our techniques to intelligently combine information from many articles on the same natural disaster.
earlier version of the Summarizer uses the simulated output of the IE system as its input, including the relief annotations; in the second variant (RIPTIDES-SIM2), the current version of the Summarizer uses the simulated output of the IE system, without the relief annotations; and in the third variant (RIPTIDES-IE), the Summarizer uses the actual output of the IE system as its input.2 Summaries generated by the RIPTIDES variants were compared to a Baseline system consisting of a simple, sentence-extraction multidocument summarizer relying only on document position, recency, and word overlap clustering.
(As explained in the previous section, we have found that word overlap clustering provides a bare bones way to help determine what information is repeated in multiple articles, thereby indicating importance to the document set as a whole, as well as to help reduce redundancy in the resulting summaries).
In addition, the RIPTIDES and Baseline system summaries were compared against the summaries of two human authors.
All of the summaries were graded with respect to content, organization, and readability on an A-F scale by three graduate students, all of whom were unfamiliar with this project.
Note that the grades for RIPTIDES-SIM1, the Baseline system, and the two human authors were assigned during a first evaluation in October, 2000, whereas the grades for RIPTIDESSIM2 and RIPTIDES-IE were assigned by the same graders in an update to this evaluation in April, 2001.
Each system and author was asked to generate four summaries of different lengths and emphases: (1) a 100-word summary of the May 30 and May 31 articles; (2) a 400-word summary of all test articles, emphasizing specific, factual information; (3) a 200-word summary of all test articles, focusing on the damage caused by the quake, and excluding information about relief efforts, and (4) a 200-word summary of all test articles, focusing on the relief efforts, and highlighting the Red Cross's role in these efforts.
The results are shown in Tables 1 and 2.
Table 1 provides the overall grade for each system or author averaged across all graders and summaries, where each assigned grade has first been converted to a number (with A=4.0 and F=0.0) and the average converted back to a letter grade.
Table 2 shows the mean and standard deviations of the overall, content, organization, and readability scores for the RIPTIDES and the Baseline systems averaged across all graders and summaries.
Where the differences vs.
the Baseline system are significant according to the t-test, the p-values are shown.
Given the amount of development effort that has gone into the system to date, we were not surprised that the RIPTIDES variants fared poorly when compared against the manually written summaries, with RIPTIDES-SIM2 receiving an average grade of C, vs.
Aand B+ for the human authors.
Nevertheless, we were pleased to find that RIPTIDES-SIM2 scored a full grade ahead of the Baseline summarizer, which received a D, and that 3.
EVALUATION AND INITIAL RESULTS To evaluate the initial version of the IE-supported summarization system, we used Topic 89 from the TDT2 collection -25 texts on the 1998 Afghanistan earthquake.
Each document was annotated manually with the natural disaster scenario templates that comprise the desired output of the IE system.
In addition, treebank-style syntactic structure annotations were added automatically using the Charniak parser.
Finally, MUC-style noun phrase coreference annotations were supplied manually.
All annotations are in XML.
The manual and automatic annotations were automatically merged, leading to inaccurate annotation extents in some cases.
Next, the Topic 89 texts were split into a development corpus and a test corpus.
The development corpus was used to build the summarization system; the evaluation summaries were generated from the test corpus.
We report on three different variants of the RIPTIDES system here: in the first variant (RIPTIDES-SIM1), an Note that since the summarizers for the second and third variants did not have access to the relief sentence categorizations, we decided to exclude from their input the two articles (one training, one test) classified by TDT2 Topic 89 as only containing brief mentions of the event of interest, as otherwise they would have no means of excluding the largely irrelevant material in these documents.
Table 1 Baseline D RIPTIDES-SIM1 C/CRIPTIDES-SIM2 C RIPTIDES-IE D+ Person 1 APerson 2 B+ RIPTIDES-IE managed a slightly higher grade of D+, despite the immature state of the IE system.
As Table 2 shows, the differences in the overall scores were significant for all three RIPTIDES variants, as were the scores for organization and readability, though not for content in the cases of RIPTIDESSIM1 and RIPTIDES-IE.
our efforts in this area on attempting to balance coherence and informativeness in selecting sets of sentences to include in the summary.
In ongoing work, we are investigating techniques for improving merging accuracy and summary fluency in the context of summarizing the more than 150 news articles we have collected from the web about each of the recent earthquakes in Central America and India (January, 2001).
We also plan to investigate using tables and hypertext drill-down as a means to help the user verify the accuracy of the summarized information.
By perusing the web collections mentioned above, we can see that trying to manually extricate the latest damage estimates from 150+ news articles from multiple sources on the same natural disaster would be very tedious.
Although estimates do usually converge, they often change rapidly at first, and then are gradually dropped from later articles, and thus simply looking at the latest article is not satisfactory.
While significant challenges remain, we suggest that our initial system development and evaluation shows that our approach has the potential to accurately summarize damage estimates, as well as identify other key story items using shallower techniques, and thereby help alleviate information overload in specific domains.
4. RELATED AND ONGOING WORK The RIPTIDES system is most similar to the SUMMONS system of Radev and McKeown [10], which summarized the results of MUC-4 IE systems in the terrorism domain.
As a pioneering effort, the SUMMONS system was the first to suggest the potential of combining IE with NLG in a summarization system, though no evaluation was performed.
In comparison to SUMMONS, RIPTIDES appears to be designed to more completely summarize larger input document sets, since it focuses more on finding the most relevant current information, and since it includes extracted sentences to round out the summaries.
Another important difference is that SUMMONS sidestepped the problem of comparing reported numbers of varying specificity (e.g.
several thousand vs.
anywhere from 2000 to 5000 vs.
up to 4000 vs.
5000), whereas we have implemented rules for doing so.
Finally, we have begun to address some of the difficult issues that arise in merging information from multiple documents into a coherent event-oriented view, though considerable challenges remain to be addressed in this area.
The sentence extraction part of the RIPTIDES system is similar to the domain-independent multidocument summarizers of Goldstein et al.[7] and Radev et al.[11] in the way it clusters sentences across documents to help determine which sentences are central to the collection, as well as to reduce redundancy amongst sentences included in the summary.
It is simpler than these systems insofar as it does not make use of comparisons to the centroid of the document set.
As pointed out in [2], it is difficult in general for multidocument summarizers to produce coherent summaries, since it is less straightforward to rely on the order of sentences in the underlying documents than in the case of single-document summarization.
Having also noted this problem, we have focused We thank Daryl McCullough for implementing the coherence boosting randomized local search, and we thank Ted Caldwell, Daryl McCullough, Corien Bakermans, Elizabeth Conrey, Purnima Menon and Betsy Vick for their participation as authors and graders.
This work has been partially supported by DARPA TIDES contract no.
N66001-00-C-8009. References [1] The Apache XML Project.
2001. "Xalan Java." [2] Barzilay, R., Elhadad, N.
and McKeown, K . 2001.
"Sentence Ordering in Multidocument Summarization".
In Proceedings of HLT 2001.
[3] Bikel, D., Schwartz, R.
and Weischedel, R . 1999.
"An Algorithm that Learns What's in a Name".
Machine Learning 34:1-3, 211-231.
[4] Cardie, C . 1997.
"Empirical Methods in Information [5] Charniak, E . 1999.
"A maximum-entropy-inspired parser".
Brown University Technical Report CS99-12.
[6] Fellbaum, C . 1998.
WordNet: An Electronic Lexical Database.
MIT Press, Cambridge, MA.
[7] Goldstein, J., Mittal, V., Carbonell, J.
and Kantrowitz, M . 2000.
"Multi-document summarization by sentence extraction".
In Proceedings of the ANLP/NAACL Workshop on Automatic Summarization, Seattle, WA.
[8] Grishman, R . 1996.
"TIPSTER Architecture Design Document Version 2.2".
DARPA, available at http://www.tipster.org/.
[9] Marcus, M., Marcinkiewicz, M.
and Santorini, B . 1993.
"Building a Large, Annotated Corpus of English: The Penn Treebank".
Computational Linguistics 19:2, 313-330.
[10] Radev, D.
R. and McKeown, K.
R . 1998.
"Generating natural language summaries from multiple on-line sources".
Computational Linguistics 24(3):469-500.
[11] Radev, D.
R., Jing, H.
and Budzikowska, M . 2000.
"Summarization of multiple documents: clustering, sentence extraction, and evaluation".
In Proceedings of the ANLP/NAACL Workshop on Summarization, Seattle, WA.
[12] Riloff, E . 1996.
"Automatically Generating Extraction Patterns from Untagged Text".
In Proceedings of the Thirteenth National Conference on Artificial Intelligence, Portland, OR, 1044-1049.
AAAI Press / MIT Press.
[13] White, M.
and Caldwell, T . 1998.
"EXEMPLARS: A Practical, Extensible Framework for Dynamic Text Generation".
In Proceedings of the Ninth International Workshop on Natural Language Generation, Niagara-onthe-Lake, Canada, 266-275 .
Structural variation in generated health reports Catalina Hallett and Donia Scott Centre for Research in Computing The Open University Walton Hall Milton Keynes MK7 6AA {c.hallett,d.scott}@open.ac.uk Abstract We present a natural language generator that produces a range of medical reports on the clinical histories of cancer patients, and discuss the problem of conceptual restatement in generating various textual views of the same conceptual content.
We focus on two features of our system: the demand for "loose paraphrases" between the various reports on a given patient, with a high degree of semantic overlap but some necessary amount of distinctive content; and the requirement for paraphrasing at primarily the discourse level.
which aims at providing tools to facilitate easy access to a patient's medical history.
In particular, we describe a natural language generation system that produces a range of summarised reports of patient records from data-encoded views of patient histories which we call chronicles.
Although we are concentrating on cancer patients, we aim to produce good quality reports without the need to construct extensive domain models.
Our typical user is a GP or clinician who uses electronic patient records at the point of care to familiarise themselves with a patient's medical history and current situation.
A number of specific requirements arise from this particular setting:  Reports that provide a quick potted overview of the patient's history are essential; this type of report should not be too long (ideally they should fit entirely on a computer screen) and should take less than a minute to read;  At the same time, a complete view of the medical history must always be available on demand;  Clinicians often need to examine a patient's history from a particular perspective (e.g., tests administered, treatments undertaken, drugs prescribed), and having focussed reports is also a requirement;  Reports should be formatted to enhance readability;  The selection of events for inclusion in a report should follow some basic rules: 1 Introduction Patient records are typically large collections of documents that reflect the medical history of a patient over a period of time.
On average, the electronic patient record of a cancer patient contains information from over 150 documents, representing consult notes, referral letters, letters to and from the patient's GP, hospital admission and discharge notes, laboratory test results, surgery and other treatment descriptions, and drug dispensing notes.
Although each document in this collection will have a specified purpose, there tends to be a high degree of redundancy between documents, but the sheer volume of information makes access extremely difficult.
The work presented in this paper is part of the Clinical E-Science Framework project (CLEF), fi Events that deviate from what is considered to be normal are more important than normal events (for example, an examination of the lymphnodes that reveals lymphadenopathy is more important than an examination that doesn't).
 Some events are more important than others and should not only be included in the report but also highlighted (e.g., through colour coding, graphical timelines or similar display features).
 Less important events should be available on a need-to-know basis These requirements impose important restrictions on the content of the reports and implicitly on the variety of lexical and syntactical devices we can employ: (a) the veracity of the report is essential, therefore we are not at liberty to employ synonymy or lexical paraphrasing that may alter (however slightly) the meaning of the original input, (b) we are required to maintain a certain syntactical ordering throughout a report in order to allow the user to quickly scan through the report with ease, and (c) we have to produce several types of reports from the same input data.
In this paper, we focus on this last requirement, describing the methods we employ for reformulating content according to the type and focus of the generated report.
example displays a fragment of a generated longitudinal report1 : Example 1 The patient is diagnosed with grade 9 invasive medullary carcinoma of the breast.
She was 39 years old when the first cell became malignant.
The history covers 1517 weeks, from week 180 to week 1697.
During this time, the patient attended 38 consults.
YEAR 3: Week 183  Radical mastectomy on the breast was performed to treat primary cancer of the left breast.
 Histopathology revealed primary cancer of the left breast.
Week 191  Examination of the abdomen revealed no enlargement of the liver or of the spleen.
 Examination of the axillary lymphnodes revealed no lymphadenopathy of the left axillary lymphnodes.
 Examination of the breast revealed no recurrent cancer of the left breast.
 Testing of the blood revealed no abnormality of the haemoglobin concentration or of the leucocyte count.
 Radiotherapy was initiated to treat primary cancer of the left breast.
Week 192  First radiotherapy cycle was performed.
...
2 Types
of report In the current implementation, the generator produces two main types of report.
The first is a longitudinal report, which is intended to provide a quick historical overview of the patient's illness, whilst preserving the main events (such as diagnoses, investigations and interventions).
It presents the events in the patient's history ordered chronologically and grouped according to type.
In this type of report, events are fully described (i.e., an event description includes all the attributes of the event) and aggregation is minimal (events with common attributes are aggregated, but there is no aggregation through generalization, for example).
The following The second type of report focusses on a given type of event in a patient's history, such as the history of diagnoses, interventions, investigations or drug prescription.
Under this category fall user-defined reports as well, where the user selects classes of interesting events (for example, Investigations of type CT scan and Interventions of type surgery).
A report of the diagnoses, for example, will focus on the Problem events that are recorded in the chronicle (e.g., cancer, anaemia, lymphadenopathy); other event types will only All the examples presented in this paper are extracted from summaries produced by our Report generator.
fiappear if they are directly related to a Problem.
As it can be seen in Example 2, this type of report is necessarily more condensed, since the events do not have to appear chronologically and can be grouped in larger clusters.
Secondary events are also more highly aggregated.
Example 2  In week 483, primary cancer of the right breast was revealed by the histopathology report.
The cancer was treated with radical mastectomy on the breast.
 In week 491, no abnormality of the leucocyte count or of the haemoglobin concentration, no lymphadenopathy of the right axillary lymphnodes, no enlargement of the spleen or of the liver and no recurrent cancer of the right breast were found.
Radiotherapy was initiated to treat primary cancer of the right breast.
 In the weeks 492 to 496, 5 radiotherapy cycles were performed.
of the right breast.
It is important to note that although the reports are generated from the same input content, they are not exact reformulations of each other, but rather different views of the same content with a large degree of overlap.
This feature is a direct result of the report requirements.
3 Input
As mentioned earlier, the input to our Report Generator is a data-encoded chronicle of the patient's medical history.
Technically, the chronicle is the partial result of information extraction applied on clinical narratives, combined with structured data (such as radiology results or demographic data), and supplemented with inferences.
However, in developing our report generator, we are currently using a Chronicle Simulator, which constructs invented chronicles, allowing us to ignore for the time being some problems that can appear when using an information extraction system (being developed in parallel).
Firstly, the resulting data is complete and correct, thus allowing us to concentrate on the design and testing of the generation and summarisation system without having to take into account at this point errors in the Information Extraction.
Secondly, our data on cancer patients is highly confidential, which makes presentation of the output of the report generator (e.g., for evaluation with real subjects, or dissemination purposes) very difficult.
Using a simulator also means that we can have instant access to a large number of randomly generated chronicles, which at this stage of the project are not yet available.
The Chronicle Simulator simulates the history of a patient's illness, and links the events in the history in a manner that closely resembles the expected output of the real Automatic Chronicler.
The current output format of the simulator is a relational database that stores six types of event2 (interventions, investigations, consults, drugs, problems and loci) and 14 types of relation between events (e.g., Problem 2 The term event is loosely used to denote dynamic (such as interventions) as well as static concepts (such as problems).
If the focus is on Interventions, the same information in the previous example will be presented as: Example 3  In week 483, histopathology revealed primary cancer of the right breast.
Radical mastectomy on the breast was performed to treat the cancer.
Radiotherapy was initiated to treat primary cancer of the right breast.
 In the weeks 492 to 496, 5 radiotherapy cycles were performed.
In an Investigation-focussed report, the intervention will be omitted, since they are not directly relevant: Example 4  In week 483, histopathology revealed primary cancer of the right breast  In week 491, examination revealed no abnormality of the leucocyte count or of the haemoglobin concentration, no lymphadenopathy of the right axillary lymphnodes, no enlargement of the spleen or of the liver and no recurrent cancer Locus, Intervention CAUSED-BY Problem, Intervention SUBPART-OF Intervention, Investigation HAS-INDICATION Problem).
Each event has a variable number of attributes, and each dynamic event is time-stamped with a start date and an end date3 . A typical chronicle contains around 350 events and about 600 relations.
HAS-LOCUS Locus Investigation Problem Intervention Locus Problem Investigation Intervention Locus Problem Drug Problem 4 Architecture The design of the Report Generator follows a classical pipeline architecture, with a content selector, content planner and syntactic realiser.
The Content Planner is tightly coupled to the Content Selector, since part of the discourse structure is already determined in the event selection phase.
Aggregation is mostly conceptual rather than syntactic, and thus it is performed in the content planning stage as well as during realisation (Reape and Mellish, 1999).
4.1 Content
selection Figure 1: Example of a generated spine structure The Content selection process represents the most important component of the Report Generator.
Although in some contexts it may be useful to generate reports containing all the events in a chronicle, the most useful types of report are the focused, summarised ones, for which good selection of important events is essential.
The process of content selection is currently driven by two parameters of a report: type and length.
We define the concept of report spine to represent a list of concepts that are essential to the construction of a given type of report.
For example, in a report of the diagnoses, all events of type Problem will be part of the spine.
Events linked to the spine through some kind of relation may or may not be included in the summary, depending on the type and length of the summary (see Figure 1).
The design of the system does not restrict the spine to containing only events of the same type.
In future extensions to the system where the user will be able to select facts they want in the summary, a spine could contain, for example, problems of type cancer, investigations of type x-ray and interventions of type surgery.
3 In
the current implementation of the chronicle, time stamps are week numbers starting with the date of the first diagnosis.
Spines are not predefined templates, but structures that are constructed dynamically with each request and they depend on the type of request and on the length of the summary.
Important events are selected according to semantic relations.
The first step in the selection process is to cluster related events based on the relations stored in the chronicle.
A cluster of events may tell us, for example, that a patient was diagnosed with cancer following a clinical examination, for which she had a mastectomy to remove the tumour, was given a histopathological test of the removed tumour, which confirmed the cancer, and had a complete radiotherapy course to treat the cancer; the radiotherapy caused an ulcer, which in turn was treated with some drug.
A typical chronicle contains a small number of clusters, typically one or two large clusters and several small ones.
Smaller clusters are generally not related to the main thread of events.
The summarisation process starts with the removal of small clusters, which in the current implementation are defined as clusters containing at most three events4 . This excludes some specified types of information that will be included in the report even when they only appear in short clusters; for example, all reports will contain essential information such as the initial diagnosis and the cause of death (if available).
The next step is the selection of important events, as defined by the type of report.
Each cluster of events is a graph, with some nodes representing spine events.
For each cluster, the spine events are selected, as well as all nodes that are at a distance of less than n from spine events, This threshold was set following a series of experiments.
fiwhere the depth n is a user-defined parameter used to adjust the size of the report.
For example, in the cluster presented in Fig.
2, assuming a depth value of 1, the content selector will choose cancer, left breast and radiotherapy but not radiotherapy cycle or ulcer.
Document planning cancer Ind i d_ cate By Has cus radiotherapy left breast Cau sed radiotherapy cycle ulcer Figure 2: Example of a cluster The first stage in structuring the body of the report is to combine messages linked through attributive relations (e.g., combining messages of type Problem with messages of type Locus if the Problem has a HAS-LOCUS relation pointing to a Locus).
In the second stage, messages are grouped according to specific rules, depending on the type of report.
For longitudinal reports, the rules stipulate that events occurring in the same week should be grouped together, and further grouped into years.
In event-specific reports, patterns of similar events are first identified and then grouped according to the week(s) they occur in.
For example, if in week 1 the patient was examined for enlargement of the liver and of the spleen with negative results and in week 2 the patient was again examined with the same results and had a mastectomy, two groups of events will be constructed: Example 5  In weeks 1 and 2, examination of the abdomen revealed no enlargement of the liver or of the spleen.
 In week 2, the patient underwent a mastectomy.
A document plan is typically a hierarchical structure that contains and combines the messages to be conveyed by the report generator.
Technically, a document plan is an ordered collection of message clusters, where messages within a cluster are combined using rhetorical relations, while individual clusters are ordered and linked according to the type of report.
The construction of document plans is partly performed in the content selection phase, since the content is selected according to the relations between events, which in turn provide information about the structure of the target text.
The actual document planner component is concerned with the construction of complete document plans, according to the type of report and cohesive relations identified in the previous stage.
A report typically consists of three parts: (a) a schematic description of the patient's demographic information (name, age, gender), (b) a two sentence summary of the patient's record (presenting the time span of the illness, the number of consults the patient attended, and the number of investigations and interventions performed) and (c) the actual report of the record produced from the events selected to be part of the content.
We focus here on this last part.
Within groups, messages are structured according to discourse relations that are either deduced from the input database or automatically inferred by applying domain specific rules.
At the moment, the input provides three types of rhetorical relation: Cause, Result and Sequence.
The domain specific rules specify the ordering of messages, and always introduce a Sequence relation.
An example of such a rule is that a histopathology event has to follow a biopsy event, if both of them are present and they start and end at the same time.
These rules facilitate the construction of a partial rhetorical structure tree.
Messages that are not connected in the tree are by default assumed to be in a List relation to other messages in the group, and their position is set arbitrarily.
The document planner also applies aggregation rules between similar messages and employs ellipsis and conjunction in order to create a more fluent text.
Simple aggregation rules state, for example, that two investigations with Figure 3: Aggregation of Investigation messages on the HAS-TARGET field the same name and two different target loci can be collapsed into one investigation with two target loci (Fig.3).
Aggregation rules of this type are designed to make the resulting text more fluent, however they do not always provide the degree of condensation required by the summary.
For example, each clinical examination consists of examinations of the abdomen for enlargement of internal organs (liver and spleen) and examination of the lymphnodes.
Thus, each clinical examination will typically consist of three independent Investigation events.
When fully aggregated according to conceptual and syntactical rules, the three Investigation messages are collapsed into one structure such as: Example 6 Examination revealed no enlargement of the spleen or of the liver and no lymphadenopathy of the axillary nodes.
this can be described as Clinical examination was normal, apart from an enlargement of the spleen.
4.3 Maintaining
the thread of discourse In producing multiple reports on the same patient from different perspectives, or of different types, we operate under the strong assumption that event-focussed reports should be organised in a way that emphasises the importance of the event in focus.
From a document structure viewpoint, this equates to constructing rhetorical structures where the focus event (i.e., the spine event) is expressed in a nuclear unit, and skeleton events are preferably in sattelite units.
Within sentences, spine events are assigned salient syntactical roles that allows them to be kept in focus.
For example, a relation such as Problem CAUSED-BY Intervention is more likely to be expressed as : The patient developed a Problem as a result of an Intervention.
However, this level of aggregation that only takes into account the semantics of individual messages may be not enough, since clinical examinations are performed repeatedly and consist of the same types of investigation.
Two approaches have been implemented in the Report Generator, both of which make use of domain specific rules.
The first is to report only events that deviate from the norm.
In the case of investigations, for example, this equates to reporting only those that have abnormal results.
The second, which produces larger reports, is to produce synthesised descriptions of events.
In the case of clinical examination for example, we could describe a sequence of investigations such as the one in example (5) as Clinical examination was normal.
If the examination deviates from the norm on a restricted numbers of parameters only, when the focus is on Problem events, and, when the focus is on Interventions as: An Intervention caused a Problem.
This kind of variation reflects the different emphasis that is placed on spine events, although the wording in the actual report may be different.
Rhetorical relations holding between simple event descriptions are most often realised as a single sentence (as in the examples above).
Complex individual events are realised in individual clauses or sentences which are connected to other accompanying events through the appropriate rhetorical relation.
For example, a Problem event has a large number of attributes, consisting of name, status, existence, number of nodes counted, number of nodes involved, clinical course, tumour size, genotype, grade, tumour marker and histology, as well as the usual time stamp.
The selection of attributes that are going to be included in a Problem description depends on a number of factors, including whether the Problem is a spine or a skeleton event, and whether the event is mentioned for the first time or is a subsequent mention.
Aditionally, the number of attributes included in the description of a Problem is a decisive factor in realising the Problem as a phrase, a sentence or a group of sentences.
In the following two examples, there are two Problem events (cancer and lymphnode count) linked through an Investigation event (excision biopsy, which is indicated by the first problem and has as a finding the second problem.
In Example 7, the problems are first mentioned spine events, while in Example 8, the problems are skeleton events (the cancer is a subsequent mention and the lymphnode count is a first mention), with the Investigation being the spine event.
Example 7 A 10mm, EGFR +ve, HER-2/neu +ve, oestrogen receptor positive cancer was found in the left breast (histology: invasive tubular adenocarcinoma).
Consequently, an excision biopsy was performed which revealed no metastatic involvement in the 5 nodes sampled.
Example 8 An excision biopsy on the left breast was performed because of cancer.
It revealed no metastatic involvement in the 5 nodes sampled.
5 Evaluation
Automatic evaluation of the generated reports is not possible, as there is no gold standard for such documents.
Additionally, a full-blown quantitative evaluation is not yet feasible, since our users are cancer specialists who cannot easily dedicate time to evaluating large numbers of reports.
However, we have conducted an informal survey with two cancer clinicians to gain feedback on the quality of the current output of the Report Generator.
To do this, we showed them three patient records encoded as chronicles, and, for each patient, two types of report produced from that record: a longitudinal report, and a summarised report of diagnoses.
The three patient records were selected to display a variety of events and sizes (a 6-year history containing 621 events, a 12-year history with 1418 events, and a 9-year history with 717 events).
Although they were (unusually) familiar with the coding scheme of the chronicles, the clinicians found it very difficult to extract a useful overview of the patients' histories from the three chronicles we showed them.
In contrast, they found the generated reports to be much more useful and the quality of the text to be very good.
The clinicians commended the reports for their ability to provide a quick and clear view of data that would be otherwise difficult to access and process.
Most importantly, the various report types were judged to be highly appropriate for use in clinical care.
Whilst this preliminary evaluation was conducted with the aim of finding early shortcomings of the Report Generator and receiving feedback from potential users, we are now embarking on a more extensive formal evaluation with cancer clinicians and medical researchers with specialist knowledge in the area of cancer.
We believe, however, that the true test of utility will be the actual use of the Report Generator in practice.
As can be seen from the examples above, the same basic rhetorical structure consisting of three nodes and two relations (causality and consequence) is realised differently in a Problemfocussed report compared to an Investigationbased report.
The conceptual reformulation is guided by the type of report, which in turn has consequences at syntactical level.
6 Conclusions
We have described a system that generates a range of health reports on individual cancer patients.
At present, our intended readership is composed of clinicians and medical researchers, and the fitype of report will depend on his or her stated needs.
Reports that are required at the point of care (e.g., for a doctor interviewing a newly referred patient, or a team of medics on ward rounds) are likely to be short "30-second" potted histories.
At other times longer, more detailed reports will be required, as will reports that focus on particular aspects of the patient's "journey" through their disease (e.g., from the perspective of the diagnoses that have been made, the drugs they have been prescribed, or surgery they have undergone).
The system is fully implemented in Java and currently generates this full range of reports on-the-fly.
A summarised report based on about 1000 input events is constructed in less than 2 seconds, a speed which is highly appropriate to the demands of clinical practice.
While the various types of generated report all share the same input (i.e., the patient's chronicle), and thus will have a large degree of conceptual overlap, clearly there will be occassions when information that is included in some reports will not be in others.
The range of reports for any given patient at any given point in their illness thus present a special class of paraphrase, with a looser adherance to semantic equivalence between versions than is typically found in other paraphrase generators, for example Kozlowski et al (2003), McKeown et al (1994), Power, Scott and Bouyaad-Agha (2003), Rosner and Stede (1994),(1996), and Scott and Souza (1990).
In this sense, our Report Generator is rather closer in spirit to Hovy's PAULINE system, which generates descriptions of given news events from different perspectives and with different stylistic goals (Hovy, 1988).
However, we achieve our goal with less reliance on terminological variation and more on structural variation at the discourse level.
Syntactic variation, where it does occur, is almost always simply a side-effect of an earlier discourse choice.
Terminological variation is deliberately avoided to prevent false implicatures; however, we are about to introduce a further class of readership, namely patients, at which stage we will make fuller use of our lexical resources.
7 Acknowledgments
CLEF is supported in part by grant G0100852 under the E-Science Initiative.
Thanks are due its clinical collaborators at the Royal Marsden and Royal Free hospitals, to colleagues at the National Cancer Research Institute (NCRI) and NTRAC and to its industrial collaborators.
Special thanks to Dr.
Jeremy Rogers who provided us with the automated Chronicle Simulator that we have used in all our experiments.
References Eduard H.
Hovy. 1988.
Generating natural language under pragmatic constraints.
Lawrence Erlbaum, Hillsdale, New Jersey.
Raymond Kozlowski, Kathleen F.
McCoy, and K.
Vijay-Shanker. 2003.
Generation of singlesentence paraphrases from predicate/argument structure using lexico-grammatical resources.
In Kentaro Inui and Ulf Hermjakob, editors, Proceedings of the Second International Workshop on Paraphrasing, pages 18.
Kathleen McKeown, Karen Kukich, and James Shaw.
1994. Practical issues in automatic document generation.
In Proceedings of the Fourth Conference on Applied Natural-Language Processing (ANLP-1994), pages 714, Stuttgart, Germany.
Richard Power, Donia Scott, and Nadjet BouayadAgha.
2003. Document structure.
Computational Linguistics, 29(2):211260.
Mike Reape and Chris Mellish.
1999. Just what is aggregation anyway?
In Proceedings of the 7th European Workshop on Natural Language Generation (EWNLG'99), pages 2029, Toulouse, France.
Dietmar R sner and Manfred Stede.
1994. Generating multilingual documents from a knowledge base: the TECHDOC project.
In Proceedings of the 15th conference on Computational Linguistics (Coling'94), pages 339343, Kyoto, Japan.
Donia Scott and Clarisse de Souza.
1990. Getting the message across in RST-based text generation.
In R.
Dale C.
Mellish and M.
Zock, editors, Current Research in Natural Language Generation, pages 31  56.
Academic Press.
Manfred Stede.
1996. Lexical paraphrases in multilingual sentence generation.
Machine Translation, 11:75107 .
Automatic Summarization of Open-Domain Multiparty Dialogues in Diverse Genres Klaus Zechner Educational Testing Service Automatic summarization of open-domain spoken dialogues is a relatively new research area.
This article introduces the task and the challenges involved and motivates and presents an approach for obtaining automatic-extract summaries for human transcripts of multiparty dialogues of four different genres, without any restriction on domain.
We address the following issues, which are intrinsic to spoken-dialogue summarization and typically can be ignored when summarizing written text such as news wire data: (1) detection and removal of speech disfluencies; (2) detection and insertion of sentence boundaries; and (3) detection and linking of cross-speaker information units (question-answer pairs).
A system evaluation is performed using a corpus of 23 dialogue excerpts with an average duration of about 10 minutes, comprising 80 topical segments and about 47,000 words total.
The corpus was manually annotated for relevant text spans by six human annotators.
The global evaluation shows that for the two more informal genres, our summarization system using dialoguespecific components significantly outperforms two baselines: (1) a maximum-marginal-relevance ranking algorithm using TF*IDF term weighting, and (2) a LEAD baseline that extracts the first n words from a text.
1. Introduction Although the field of summarizing written texts has been explored for many decades, gaining significantly increased attention in the last five to ten years, summarization of spoken language is a comparatively recent research area.
As the number of spoken audio databases is growing rapidly, however, we predict that the need for high-quality summarization of information contained in this medium will increase substantially.
Summarization of spoken dialogues, in particular, may aid in the archiving, indexing, and retrieval of various records of oral communication, such as corporate meetings, sales interactions, or customer support.
The purpose of this article is to explore the issues of spoken-dialogue summarization and to describe and evaluate an implementation addressing some of the core challenges intrinsic to the task.
We will use an implementation of a state-of-the-art text summarization method (maximum marginal relevance, or MMR) as the main baseline for comparative evaluations, and then add a set of components addressing issues specific to spoken dialogues to this MMR module to create our spoken dialogue summarization system, which we call DIASUMM.
We consider the following dimensions to be relevant for our research; the combination of these dimensions distinguishes our work from most other work in the field Educational Testing Service, Rosedale Road MS 11-R, Princeton, NJ 08541.
E-mail: kzechner@ets.org c 2002 Association for Computational Linguistics Computational Linguistics Volume 28, Number 4 of summarization:     spoken versus written language multiparty dialogues versus texts written by one author unrestricted versus restricted domains diverse genres versus a single genre The main challenges this work has to address, in addition to the challenges of writtentext summarization, are as follows:     coping with speech disfluencies identifying the units for extraction maintaining cross-speaker coherence coping with speech recognition errors We will discuss these challenges in more detail in the following section.
Although we have addressed the issue of speech recognition errors in previous related work (Zechner and Waibel 2000b), for the purpose of this article, we exclusively use human transcripts of spoken dialogues.
Intrinsic evaluations of text summaries usually use sentences as their basic units.
For our data, however, sentence boundaries are typically not available in the first place.
Thus we devise a word-based evaluation metric derived from an average relevance score from human relevance annotations (section 6.2).
The organization of this article is as follows: Section 2 provides the motivation for our research, introducing and discussing the main challenges of spoken-dialogue summarization, followed by a section on related work (section 3).
Section 4 describes the corpus we use to develop and evaluate our system, along with the procedures employed for corpus annotation.
The system architecture and its components are described in detail in section 5, along with evaluations thereof.
Section 6 presents the global evaluation of our approach, before we conclude the article with a discussion of our results, contributions, and directions for future research in this field (sections 7 and 8).
2. Motivation Consider the following example from a phone conversation drawn from the English CALLHOME database (LDC 1996).
It is a transcript of a conversation between two native speakers of American English; one person is in the New York area (speaker a), the other one (speaker b) in Israel.
It was recorded about a month after Yitzhak Rabin's assassination (1995).
This dialogue segment is about one minute of real time.
The audio is segmented into speaker turns using silence heuristics,1 and each turn is marked with a turn number and with the speaker label.
Noises are removed to increase readability.2 1 Therefore, in some cases, we can find several turns of one speaker following each other.
2 Hence
there can be "missing" turns (e.g., turn 37), in case they contain only noises and no actual words.
Zechner Automatic Summarization of Dialogues 28 a: oh 29 b: they didn't know he was going to get shot but it was at a peace rally so i mean it just worked out 30 b: i mean it was a good place for the poor guy to die i mean because it was you know right after the rally and everything was on film and everything 31 a: yeah 32 b: oh the whole country we just finished the thirty days mourning for him now you know it's uh oh everybody's still in shock it's 33 a: oh 34 a: i know 35 b: terrible what's going on over here 36 b: and this guy that killed him they show him on t v smiling he's all happy he did it and everything he isn't even sorry or anything 38 a: there are i 39 b: him him he and his brother you know the two of them were in it together and there's a whole group now it's like a a conspiracy oh it's eh 40 a: mm 41 a: with the kahane chai 42 b: unbelievable 43 b: yeah yeah it's all those people yeah you probably see them running around new york don't you they're all 44 a: yeah 45 a: oh yeah they're here 46 b: new york based yeah 47 a: oh there's 48 a: all those fanatics 49 a: like the extreme 50 b: oh but 51 b: but whwhat's the reaction in america really i mean i mean do people care you know i mean you know do they 52 a: yeah momost pei mean uh 53 a: i don't know what commui mean like the jewish community 54 a: a lot eall of us were 55 a: very upset and there were lots all the 56 b: yeah 57 a: like two days after did it happen like on a sunday 58 b: yeah it hapit happened on it happened on a saturday night By looking at this transcript we can readily identify some of the phenomena that would cause difficulties for conventional summarizers of written texts:  Some turns (e.g., turn 51) contain many disfluencies that (1) make them hard to read and (2) reduce the relevance of the information contained therein.
Some (important) pieces of information are distributed over a sequence of turns (e.g., turns 535455, 45474849); this is due to a silence-based 449 Computational Linguistics Volume 28, Number 4 segmentation algorithm that causes breaks in logically connected clauses.
A traditional summarizer might render these sequences incompletely.
 Some turns are quite long (e.g., 36, 39) and contain several sentences; a within-turn segmentation seems necessary to avoid the extraction of too much extraneous information when only parts of a turn contain relevant information.
Some of the information is constructed interactively by both speakers; the prototypical cases are question-answer pairs (e.g., turns 5152ff., turns 5758).
A traditional text summarizer might miss either question or answer and hence produce a less meaningful summary.
We shall discuss these arising issues along with an indication of our computational remedies in the following subsections.
We want to stress beforehand, though, that the originality of our system should not be seen in the particular implementation of its individual components, but rather in their selection and specific composition to address the issues at hand in an effective and also efficient way.
2.1 Disfluency
Detection The two main negative effects speech disfluencies have on summarization are that they (1) decrease the readability of the summary and (2) increase its noncontent noise.
In particular for informal conversations, the percentage of disfluent words is quite high, typically around 20% of the total words spoken.3 This means that this issue should, in our opinion, be addressed to improve the quality (readability and conciseness) of the generated summaries.
In section 5.3 we shall present three components for identifying most of the major classes of speech disfluencies in the input of the summarization system, such as filled pauses, repetitions, and false starts.
All detected disfluencies are marked in this process and can be selectively excluded during summary generation.
2.2 Sentence
Boundary Detection Unlike written texts, in which punctuation markers clearly indicate clause and sentence boundaries, spoken language is generated as a sequence of streams of words, in which pauses (silences between words) do not always match linguistically meaningful segments: A speaker can pause in the middle of a sentence or even a phrase, or, on the other hand, might not pause at all after the end of a sentence or clause.
This mismatch between acoustic and linguistic segmentation is reflected in the output of a speech recognizer, which typically generates a sequence of speaker turns whose boundaries are marked by periods of silence (or nonspeech).
As a result, one speaker's turn may contain multiple sentences, or, on the other hand, a speaker's sentence might span more than one turn.
In a test corpus of five English CALLHOME dialogues with an average length of 320 turns, we found on average of about 30 such continuations of logical clauses over automatically determined acoustic segments per dialogue.
The main problems for a summarizer would thus be (1) the lack of coherence and readability of the output because of incomplete sentences and (2) extraneous information due to extracted units consisting of more than one sentence.
In section 5.4 we 3 Although other studies have found percentages lower than this figure, we included content-less categories such as discourse markers or rhetorical connectives, which are often not regarded as disfluencies per se.
Zechner Automatic Summarization of Dialogues describe a component for sentence segmentation that addresses both of these problems.
2.3 Distributed
Information Since we have multiparty conversations as opposed to monologues, sometimes the crucial information is found in a sequence of turns from several speakers, the prototypical case of this being a question-answer pair.
If the summarizer were to extract only the question or only the answer, the lack of the corresponding answer or question would often cause a severe reduction of coherence in the summary.
In some cases, either the question or the answer is very short and does not contain any words with high relevance that would yield a substantial weight in the summarizer.
In order not to lose these short sentences at a later stage, when only the most relevant sentences are extracted, we need to identify matching question-answer pairs ahead of time, so that the summarizer can output the matching sentences during summary generation as one unit.
We describe our approach to cross-speaker information linking in section 5.5. 2.4 Other Issues We see the work reported in this article as the first in-depth analysis and evaluation in the area of open-domain spoken-dialogue summarization.
Given the large scope of this undertaking, we had to restrict ourselves to those issues that are, in our opinion, the most salient for the task at hand.
A number of other important issues for summarization in general and for speech summarization in particular are either simplified or not addressed in this article and left for future work in this field.
In the following, we briefly mention some of these issues, indicating their potential relevance and promise.
2.4.1 Topic
Segmentation.
In many cases, spoken dialogues are multitopical.
For the English CALLHOME corpus, we determined an average topic length of about one to two minutes' speaking time (or about 200400 words).
Summarization can be accomplished faster and more concisely if it operates on smaller topical segments rather than on long pieces of input consisting of diverse topics.
Although we have implemented a topic segmentation component as part of our system for these reasons, all of the evaluations are based on the topical segments determined by human annotators.
Therefore, this component will not be discussed in this article.
Furthermore, topical segmentation is not an issue intrinsic to spoken dialogues, which in our opinion justifies this simplification.
2.4.2 Anaphora
Resolution.
An analogous reasoning holds for the issue of anaphora resolution: Although it would certainly be desirable, for the sake of increased coherence and readability, to employ a well-working anaphora resolution component, this issue is not specific to the task at hand, either.
One could argue that particularly for summarization of more informal conversations, in which personal pronouns are rather frequent, anaphora resolution might be more helpful than for, say, summarization of written texts.
But we conjecture that this task might also prove more challenging than written-text anaphora resolution.
In our system, we did not implement a module for anaphora resolution.
2.4.3 Discourse
Structure.
Previous work indicates that information about discourse structure from written texts can help in identifying the more salient and relevant sentences or clauses for summary generation (Marcu 1999; Miike et al.1994). Much 451 Computational Linguistics Volume 28, Number 4 less exploration has been done, however, in the area of automatic analysis of discourse structure for non-task-oriented spoken dialogues in unrestricted domains, such as CALLHOME (LDC 1996).
Research for those kinds of corpora reported in Jurafsky et al.(1998), Stolcke et al.(2000), Levin et al.(1999), and Ries et al.(2000) focuses more on detecting localized phenomena such as speech acts, dialogue games, or functional activities.
We conjecture that there are two reasons for this: (1) free-flowing spontaneous conversations have much less structure than task-oriented dialogues, and (2) the automatic detection of hierarchical structure would be much harder than it is for written texts or dialogues based on a premeditated plan.
Although we believe that in the long run attempts to automatically identify the discourse structure of spoken dialogues may benefit summarization, in this article, we greatly simplify this matter and exclusively look at local contexts in which speakers interactively construct shared information (the question-answer pairs).
2.4.4 Speech
Recognition Errors.
Throughout this article, our simplifying assumption is that our input comes from a perfect speech recognizer; that is, we use human textual transcripts of the dialogues in our corpus.
Although there are cases in which this assumption is justifiable, such as transcripts provided by news services in parallel to the recorded audio data, we believe that in general a spoken dialogue summarizer has to be able to accept corrupted input from an automatic speech recognizer (ASR), as well.
Our system is indeed able to work with ASR output; it is integrated in a larger system (Meeting Browser) that creates, summarizes, and archives meeting records and is connected to a speech recognition engine (Bett et al.2000). Further, we have shown in previous work how we can use ASR confidence scores (1) to reduce the word error rate within the summary and (2) to increase the summary accuracy (Zechner and Waibel 2000b).
2.4.5 Prosodic
Information.
A further simplifying assumption of this work is that prosodic information is not available, with the exception of start and end times of speaker turns.
Considering the results reported by Shriberg et al.(1998) and Shriberg et al.(2000), we conjecture that future work in this field will demonstrate the additional benefit of incorporating prosodic information, such as stress, pitch, and intraturn pauses, into the summarization system.
In particular, we would expect improved system performance when speech recognition hypotheses are used as input: In that case, the prosodic information could compensate to some extent for incorrect word information.
3. Related Work The vast majority of summarization research in the past clearly has focused exclusively on written text.
A good selection of both early seminal papers and more recent work can be found in Mani and Maybury (1999).
In general, most summarization approaches can be classified as either corpus-based, statistical summarization (such as Kupiec, Pedersen, and Chen [1995]), or knowledge-based summarization (such as Reimer and Hahn [1988]) in which the text domain is restricted.
(The MMR method [Carbonell, Geng, and Goldstein 1997], which we are using as the summarization engine for our DIASUMM system, belongs to the first category).
More recently, Marcu (1999) presented work on using automatically detected discourse structure for summarization.
Knight and Marcu (2000) and Berger and Mittal (2000) presented approaches in which summarization can be reformulated as a problem of machine translation: 452 Zechner Automatic Summarization of Dialogues translating a long sentence into a shorter sentence, or translating a Web page into a brief gist, respectively.
Two main areas are exceptions to the focus on text summarization in past work: (1) summarization of task-oriented dialogues in restricted domains and (2) summarization of spoken news in unrestricted domains.
We shall discuss both of these areas in the following subsections, followed by a discussion of prosody-based emphasis detection in spoken language, and finally by a summary of research most closely related to the topic of this work.
3.1 Summarization
of Dialogues in Restricted Domains During the past decade, there has been significant progress in the area of closeddomain spoken-dialogue translation and understanding, even with automatic speech recognition input.
Two examples of systems developed in that time frame are JANUS (Lavie et al.1997) and VERBMOBIL (Wahlster 1993).
In that context, several spoken-dialogue summarization systems have been developed whose goal it is to capture the essence of the task-based dialogues at hand.
The MIMI system (Kameyama and Arima 1994; Kameyama, Kawai, and Arima 1996) deals with the travel reservation domain and uses a cascade of finite-state pattern recognizers to find the desired information.
Within VERBMOBIL, a more knowledge-rich approach is used (Alexandersson and Poller 1998; Reithinger et al.2000). The domain here is travel planning and negotiation of a trip.
In addition to finite-state transducers for content extraction and statistical dialogue act recognition, VERBMOBIL also uses a dialogue processor and a summary generator that have access to a world knowledge database, a domain model, and a semantic database.
The abstract representations built by this summarizer allow for summary generation in multiple languages.
3.2 Summarization
of Spoken News Within the context of the Text Retrieval Conference (TREC) spoken document retrieval (SDR) conferences (Garofolo et al.1997; Garofolo et al.1999) as well as the recent Defense Advanced Research Project Agency (DARPA) broadcast news workshops, a number of research groups have been developing multimedia browsing tools for text, audio, and video data, which should facilitate the access to news data, combining different modalities.
Hirschberg et al.(1999) and Whittaker et al.(1999) present a system that supports local navigation for browsing and information extraction from acoustic databases, using speech recognizer transcripts in tandem with the original audio recording.
Although their interface helps users in the tasks of relevance ranking and fact finding, it is less helpful in the creating of summaries, partly because of imperfect speech recognition.
Valenza et al.(1999) present an audio summarization system that combines acoustic confidence scores with relevance scores to obtain more accurate and reliable summaries.
An evaluation showed that human judges preferred summaries with a compression rate of about 15% (30 words per minute at a speaking rate of about 200 words per minute) and that the summary word error rate was significantly smaller than the word error rate for the full transcript.
Hori and Furui (2000) use salience features in combination with a language model to reduce Japanese broadcast news captions by about 3040% while keeping the meaning of about 72% of all sentences in the test set.
Another speech-related reduction approach was presented recently by Koumpis and Renals (2000), who summarize voice mail in the Small Message format.
453 Computational Linguistics Volume 28, Number 4 3.3 Prosody-Based Emphasis Detection in Spoken Audio Whereas most approaches to summarizing acoustic data rely on the word information (provided by a human or ASR transcript), there have been attempts to generate summaries based on emphasized regions in a discourse, using only prosodic features.
Chen and Withgott (1992) train a hidden Markov model on transcripts of spontaneous speech, labeled for different degrees of emphasis by a panel of listeners.
Their "audio summaries" on an unseen (but rather small) test set achieve a remarkably good agreement with human annotators ( > 0.5).
Stifelman (1995) uses a pitch-based emphasis detection algorithm developed by Arons (1994) to find emphasized passages in a 13-minute discourse.
In her analysis, she finds good agreement between these emphasized regions and the beginnings of manually marked discourse segments (in the framework of Grosz and Sidner [1986]).
Although these are promising results, being suggestive of the role of prosody for determining emphasis, relevance, or salience in spoken discourse, in this work we restrict the use of prosody to the turn length and interturn pause features.
We conjecture, however, that the integration of prosodic and word level information would be a fruitful research area that would have to be explored in future work.
3.4 Spoken
Dialogue Summarization in Unrestricted Domains Waibel, Bett, and Finke (1998) report results of their summarizer on automatically transcribed SWITCHBOARD (SWBD) data (Godfrey, Holliman, and McDaniel 1992), the word error rate being about 30%.
Their implementation used an algorithm inspired by MMR, but they did not address any dialogueor speech-related issues in their summarizer.
In a question-answer test with summaries of five dialogues, participants could identify most of the key concepts using a summary size of only five turns.
These results varied widely (between 20% and 90% accuracy) across the five different dialogues tested in this experiment.
Our own previous work (Zechner and Waibel 2000a) addressed for the first time the combination of challenges of dialogue summarization with summarization of spoken language in unrestricted domains.
We presented a first prototype of DIASUMM that addressed the issues of disfluency detection and removal and sentence boundary detection, as well as cross-speaker information linking.
This work extends and expands these initial attempts substantially, in that we are now focusing on (1) a systematic training of the major components of the DIASUMM system, enabled by the recent availability of a large corpus of disfluency-annotated conversations (LDC 1999b), and (2) the exploration of three more genres of spoken dialogues in addition to the English CALLHOME corpus (NEWSHOUR, CROSSFIRE, GROUP MEETINGS).
Further, the relevance annotations are now performed by a set of six human annotators, which makes the global system evaluation more meaningful, considering the typical divergence among different annotators' relevance judgments.
4. Data Annotation 4.1 Corpus Characteristics Table 1 provides the statistics on the corpus used for the development and evaluation of our system.
We use data from four different genres, two being more informal, two more formal:  English CALLHOME and CALLFRIEND: from the Linguistic Data Consortium (LDC) collections, eight dialogues for the devtest set Zechner Automatic Summarization of Dialogues Table 1 Data characteristics for the corpus (average over dialogues).
8E-CH, 4E-CH: English CallHome; NHOUR: NewsHour; XFIRE: CrossFire; G-MTG: Group Meetings.
Data Set 8E-CH 4E-CH NHOUR formal yes 3 8 2 25 101 4.1 6.3 2.0 1224 12.1 5.1 48 0.48 64.8 17.2 3.4 0.7 13.8 XFIRE G-MTG Formal/informal informal informal Topics predetermined no no Dialogue excerpts (total) 8 4 Topical segments (total) 28 23 Different speakers 2.1 2 Turns 242 276 Sentences 280 366 Sentences per turn 1.2 1.3 Questions (in %) 3.7 6.4 False starts (in %) 12.1 11.0 Words 1685 1905 Words per sentence 6.0 5.2 Disfluent (in %) 16.0 16.3 Disfluencies 222 259 Disfluencies per sentence 0.79 0.71 Empty coordinating conjunctions (in %) 30.3 30.4 Lexicalized filled pauses (in %) 18.8 21.0 Editing terms (in %) 3.6 1.6 Nonlexicalized filled pauses (in %) 20.8 29.9 Repairs (in %) 26.6 17.1 (8E-CH) and four dialogues for the eval set (4E-CH).4 These are recordings of phone conversations between two family members or friends, typically about 30 minutes in length; the excerpts we used were matched with the transcripts, which typically represent 510 minutes of speaking time.
   NEWSHOUR (NHOUR): Excerpts from PBS's NewsHour television show with Jim Lehrer (recorded in 1998).
CROSSFIRE (XFIRE): Excerpts from CNN's CrossFire television show with Bill Press and Robert Novak (recorded in 1998).
GROUP MEETINGS (G-MTG): Excerpts from recordings of project group meetings in the Interactive Systems Labs at Carnegie Mellon University.
Furthermore, we used the Penn Treebank distribution of the SWITCHBOARD corpus, annotated with disfluencies, to train the major components of the system (LDC 1999b).
From Table 1 we can see that the two more formal corpora, NEWSHOUR and CROSSFIRE, have longer sentences, more sentences per turn, and fewer disfluencies (particularly nonlexicalized filled pauses and false starts) than English CALLHOME and the GROUP MEETINGS.
This means that their flavor is more like that of written text and not so close to the conversational speech typically found in the SWITCHBOARD or CALLHOME corpora.
4 We
used the devtest set corpus for system development and tuning and set aside the eval set for the final global system evaluation.
For the other three genres, two dialogue excerpts each were used for the devtest set, the remainder for the eval set.
Computational Linguistics Volume 28, Number 4 4.2 Corpus Annotation 4.2.1 First Annotation Phase.
All the annotations were performed on human-generated transcripts of the dialogues.
The CALLHOME and GROUP MEETINGS dialogues were automatically partitioned into speaker turns (by means of a silence heuristic); the other corpora were segmented manually (based on the contents and flow of the conversation).5 There were six naive human annotators performing the task;6 only four, however, completed the entire set of dialogues.
Thus, the number of annotations available for each dialogue varies from four to six.
Prior to the relevance annotations, the annotators had to mark topical boundaries, because we want to be able to define and then create summaries for each topical segment separately (as opposed to a whole conversation consisting of multiple topics).
The notion of a topic was informally defined as a region in the text that ends, according to the annotation manual, "when the speakers shift their topic of discussion".
Once the topical segments were marked, for each such segment, each annotator had to identify the most relevant information units (IUs), called nucleus IUs, and somewhat relevant IUs, called satellite IUs.
IUs are often equivalent to sentences but can span longer or shorter contiguous segments of text, dependent on the annotator's choice.
The overall goal of this relevance markup was to create a concise and readable summary containing the main information present in the topical segment.
Annotators were also asked to mark the most salient words within their annotated IUs with a +, which would render a summary with a somewhat more telegraphic style (+-marked words).
We also asked that the human annotators stay within a preset target length for their summaries: The +-marked words in all IUs within a topical segment should be 1020% of all the words in the segment.
The guideline was enforced by a checker program that was run during and after annotation of a transcript and that also ensured that no markup errors and no accidental word deletions occurred.
We provide a brief example here (n[, n] mark the beginning and end of a nucleus IU, the phrase they fly to Boston was +-marked as the core content within this IU): B: heck it might turn out that you know n[ if +they +fly in +to +boston i can n] 4.2.2 Creation of Gold-Standard Summaries.
After the first annotation phase, in which each coder worked independently according to the guidelines described above, we devised a second phase, in which two coders from the initial group were asked to create a common-ground annotation, based on the majority opinion of the whole group.
To construct such a majority opinion guideline automatically, we assigned weights to all words in nucleus IUs and satellite IUs and added all weights for all marked words of all coders for every turn.7 The total turn weights were then sorted by decreasing value to provide a guide for the two coders in the second phase as to which turns they should focus their annotations on for the common-ground or gold-standard summaries.
5 This
fact may partially account for NEWSHOUR and CROSSFIRE turns being longer than CALLHOME and GROUP MEETING turns.
6 Naive
in this context means that they were nonexperts in linguistics or discourse analysis.
7 The
weights were set as follows: nucleus IUs: 3.0 if +-marked, 2.0 otherwise; satellite IUs: 1.0 if +-marked, 0.5 otherwise.
Zechner Automatic Summarization of Dialogues Table 2 Nuclei and satellites: Length in tokens and relative frequency (in % of all tokens).
Annotator/ Data Set LB BR SC RW RC JK Gold CALLHOME NEWSHOUR CROSSFIRE MEETINGS All Dialogues Avg.
Nuc. Avg.
Sat. Nuc-Tokens Nuc-+-Marked Sat-Tokens Sat-+-Marked Length Length (in %) (in %) (in %) (in %) 12.993 16.507 20.720 22.899 23.741 39.203 21.763 17.108 25.828 33.923 37.674 23.152 13.732 14.551 14.093 19.576 18.553 9.794 6.462 13.099 16.733 22.132 23.413 16.173 11.646 11.978 29.412 19.352 43.573 26.355 13.934 21.962 29.536 21.705 23.034 22.796 8.558 8.339 18.045 11.332 15.434 11.204 6.573 11.003 13.530 10.615 9.222 10.807 5.363 10.558 6.517 2.757 12.749 0.711 0.179 5.126 4.300 1.853 7.456 4.665 3.818 7.645 4.796 1.718 0.333 0.465 0.000 1.932 2.947 0.976 1.123 1.636 Other than this guideline, the requirements were almost exactly identical to those in phase 1, except that (1) the pair of annotators was required to work together on this task to be able to reach a consensus opinion, and (2) the preset relative word length of the gold summary (1020%) applied only to the nucleus IUs.
As for the topical boundaries, which obviously vary among coders, a list of boundary positions chosen by the majority (at least half) of the coders in the first phase was provided.
In this gold-standard phase, the two coders mostly stayed with these suggestions and changed less than 15% of the suggested topic boundaries, the majority of which were minor (less than two turns' difference in boundary position).
4.2.3 General
Annotation Analysis.
Table 2 provides the statistics on the frequencies of the annotated nucleus and satellite IUs.
We make the following observations:  On average, about 23% of all tokens were assigned to a nucleus IU and 5% to a satellite IU; counting only the +-marked tokens, this reduces to about 11% and 2% of all tokens, respectively.
The average total lengths of nuclei and satellites vary widely across corpora: between 17.1 (13.1) tokens for CALLHOME and 37.7 (23.4) tokens for GROUP MEETINGS data.
This is most likely a reflection on the typical length of turns in the different subcorpora.
A similar variation is also observed across annotators: between 12 and 40 tokens for nucleus-IUs and between 9 and 20 tokens for satellites.
The granularity of IUs is quite different across annotators.
Since some annotators mark a larger number of IUs than others, there is an even larger discrepancy in the relative number of words assigned to nucleus IUs and satellite IUs among the different annotators: 1144% (nucleus IUs) and 013% (satellite IUs).
The ratio of nucleus versus satellite tokens also varies greatly among the annotators: from about 1:1 to 40:1.
457 Computational Linguistics Volume 28, Number 4 The ratio of nucleus and satellite tokens that are +-marked varies greatly: between 36 and 77% for nucleus IUs and between 2 and 80% for satellite IUs.
From these observations, we conclude that merging the nucleus and satellite IUs into one class would yield a more consistent picture than keeping them separate.
A similar argument can be made for the +-marked passages, in which we also find a quite high intercoder variation in relative +-marking.
This led us to the decision of giving equal weight to any word in an IU, irrespective of IU type or marking, for the purpose of global system evaluation.
Finally, we conjecture that the average length of our extraction units should be in the 1040 words range, which roughly corresponds to about 312 seconds of real time, assuming an average word length of 300 milliseconds.
As a comparison, we note that Valenza et al.(1999) found summaries with 30-grams8 working well in their experiments, a finding that is in line with our observations here on typical human IU lengths.
4.2.4 Intercoder
Agreement.
Agreement between coders (and between automatic methods and coders) has been measured in the summarization literature with quite a wide range of methods: Rath, Resnick, and Savage (1961) use Kendall's ; Kupiec, Pedersen, and Chen (1995) (among many others) use percentage agreement; and Aone, Okurowski, and Gorlinsky (1997) (among others) use the notions of precision, recall, and F1 -score, which are commonly employed in the information retrieval community.
Similarly, in the literature on discourse segmentation and labeling, a variety of different agreement measures have been used, including precision and recall (Hearst 1997; Passonneau and Litman 1997), Krippendorff's (1980) (Passonneau and Litman 1997) and Cohen's (1960) (Carletta et al.1997). In this work, we use the two following metrics: (1) the -statistic in its extension for more than two coders (Davies and Fleiss 1982); and (2) precision, recall, and F1 -score.9 We will discuss the -statistic first.
For intercoder agreement with respect to topical boundaries, agreement is found if boundaries fall within the same 50-word bin of a dialogue.
Relevance agreements are computed at the word level.
For relevance markings, we compute both for the three-way case (nucleus IUs, satellite IUs, unmarked) and the two-way case (any IUs, unmarked).10 Topical-boundary agreement was not evaluated for two of the GROUP MEETINGS dialogues, in which only one of four annotators marked any text-internal topic boundary.
We compute agreements for each dialogue separately and report the arithmetic means for the five subcorpora in Table 3.
We observe that agreement for topical boundaries is much higher than for relevance markings.
Furthermore, agreement is generally higher for CALLHOME and comparatively low for the GROUP MEETINGS corpus.
As a second evaluation metric, we compute precision, recall, and F1 -scores for the same four annotators and the same sets of subcorpora as before.
For topical boundaries, a match means that the boundaries fall within 3 turns of each other, and for relevant 8 A 30-gram is a passage of text containing 30 adjacent words.
9 Precision
is the ratio of correctly matched items over all items (boundaries, marked words); recall is the ratio of correctly matched items over all items that need to be matched; and the F1 -score combines 2PR precision (P) and recall (R) in the following way: F1 = P+R. 10 These computations were performed for those four (out of six) annotators who completed the entire corpus markup.
Zechner Automatic Summarization of Dialogues Table 3 Intercoder annotation agreement for topical boundaries and relevance markings.
8E-CH Topical boundaries Relevance markings (3 way) Relevance markings (2 way) 0.503 0.147 0.157 4E-CH 0.402 0.161 0.169 NHOUR 0.256 0.123 0.124 XFIRE 0.331 0.089 0.100 G-MTG 0.174 0.040 0.046 Overall 0.384 0.117 0.126 Table 4 Intercoder annotation F1 -agreement for topical boundaries and relevance markings.
8E-CH Topical boundaries Relevance markings (2 way) .54 .38 4E-CH .44 .39 NHOUR .53 .38 XFIRE .38 .32 G-MTG .18 .32 Overall .45 .36 words a match means that the two words to be compared are both in a nucleus or satellite IU.
The results can be seen in Table 4.
4.2.5 Disfluency
and Sentence Boundary Annotation.
In addition to the annotation for topic boundaries and relevant text spans, the corpus was also annotated for speech disfluencies in the same style as the Penn Treebank SWITCHBOARD corpus (LDC 1999b).
One coder (different from the six annotators mentioned before) manually tagged the corpus for disfluencies and sentence boundaries following the SWITCHBOARD disfluency annotation style book (Meteer et al.1995). 4.2.6 Question-Answer Annotation.
A final type of annotation was performed on the entire corpus to mark all questions and their answers, for the purpose of training and evaluation of the question-answer linking system component.
Questions and answers were annotated in the following way: Every sentence that is a question was marked as either a Yes-No-question or a Wh-question.
Exceptions were back-channel questions, such as "Is that right?"; rhetorical questions, such as "Who would lie in public?"; and other questions that do not refer to a propositional content.
These were not supposed to be marked (even if they have an apparent answer), since we see the latter class of questions as irrelevant for the purpose of increasing the local coherence within summaries.
For each Yes-No-question and Wh-question that has an answer, the answer was marked with its relative offset to the question to which it belongs.
Some answers are continued over several sentences, but only the core answer (which usually consists of a single sentence) was marked.
This decision was made to bias the answer detection module toward brief answers and to avoid the question-answer regions' getting too lengthy, at the expense of summary conciseness.
5. Dialogue Summarization System 5.1 System Architecture The global system architecture of the spoken-dialogue summarization system presented in this article (DIASUMM) is depicted in Figure 1.
The input data are a timeordered sequence of speaker turns with the following quadruple of information: start time, end time, speaker label, and word sequence.
The seven major components are executed sequentially, yielding a pipeline architecture.
459 Computational Linguistics dialogue transcript Volume 28, Number 4 POS Tagger Disfluency Detection Sentence Boundary Detection Extraction Unit Identification False Start Detection (+ Chunk Parser) Question & Answer Detection Repetition Filter Topic Segmentation Sentence Ranking & Selection dialogue summary Figure 1 Global system architecture.
The following subsections describe the components of the system in more detail.
As argued earlier, the topic detection component is not relevant for the way we conduct the global system evaluation and hence is not discussed here.
(We implemented a variant of Hearst's [1997] TextTiling algorithm).
The three components involved in disfluency detection are the part-of-speech (POS) tagger, the false-start detection module, and the repetition filter.
They are discussed in subsection 5.3, followed by a subsection on sentence boundary detection (5.4).
The question-answer pair detection is described in subsection 5.5, and the sentence selection module, performing relevance ranking, in subsection 5.6. 5.2 Input Tokenization We eliminate all human and nonhuman noises and incomplete words from the input transcript.
Further, we eliminate all information on case and punctuation, since 460 Zechner Automatic Summarization of Dialogues we emulate the ASR output in that regard, which does not provide this information.
Contractions such as don't or I'll are divided and treated as separate words--in these examples we would obtain do n't and I 'll.
5.3 Disfluency
Detection 5.3.1 Motivation.
Conversational, informal spoken language is quite different from written language in that a speaker's utterances are typically much less well-formed than a writer's sentences.
We can observe a set of disfluencies such as false starts, hesitations, repetitions, filled pauses, and interruptions.
Additionally, in speech there is no good match between linguistically motivated sentence boundaries and turn boundaries or recognition hypotheses from automatic speech recognition.
5.3.2 Types
of Disfluencies.
The classification of disfluencies in this work follows Shriberg (1994), Meteer et al.(1995), and Rose (1998).
It is worth noting, however, that any disfluency classification will be only an approximation of the assumed real phenomena and that often boundaries between different classes are fuzzy and hard to decide for human annotators (cf.
Meteer et al.[1995] on annotators' problems with the classification of the word so).
 Filled pauses: We follow Rose's (1998) classification of nonlexicalized filled pauses (typically uh, um) and lexicalized filled pauses (e.g., like, you know).
Whereas the former are usually nonambiguous and hence easy to detect, the latter are ambiguous and much harder to detect accurately.
Restarts or repairs: These are fragments that are resumed, but without completely abandoning the first attempt.
We follow the notation in Meteer et al.(1995) and Shriberg (1994), which has these parts: (1) reparandum, (2) interruption point (+), (3) interregnum (editing phase, {.
. . }), and (4) repair.
    Repetition: A restart with a verbatim repetition of a word or a sequence of words: [ she is + she is ] happy.
Insertion: A repetition of the reparandum, with some word(s) inserted: [ she liked + {um} she really liked ] it.
Substitution: The reparandum is not repeated: [ she + {uh} my wife ] liked it.
False starts: These are abandoned, incomplete clauses.
In some cases, they may occur at the end of an utterance, and they can be due to interruption by another speaker.
Example: so we didn't--they have not accepted our proposal.
5.3.3 Related
Work.
The past decade has produced a substantial amount of research in the area of detecting intonational and linguistic boundaries in conversational speech, as well as in the area of detecting and correcting speech disfluencies.
Whereas earlier work tended to look at these phenomena in isolation (Nakatani and Hirschberg 1994; Stolcke and Shriberg 1996), more recent work has attempted to solve several tasks within one framework (Heeman and Allen 1999; Stolcke et al.1998). Most approaches use some kind of prosodic information, such as duration of pauses, stress, and pitch contours, and most of them combine this prosodic information with information about word identity and sequence (n-grams, hidden Markov 461 Computational Linguistics Volume 28, Number 4 models).
In the study of Stolcke et al.(1998), the goal was to detect sentence boundaries and a variety of speech disfluencies on a large portion of the SWITCHBOARD corpus.
An explicit comparison was made between prosodic and word-based models, and the results showed that an n-gram model, enhanced with segmental information about turn boundaries, significantly outperformed the prosodic model.
Model combination improved the overall results, but only to a small extent.
In more recent research, Shriberg et al.(2000) reported that for sentence boundary detection in two different corpora (BROADCAST NEWS and SWITCHBOARD), prosodic models outperform word-based language models and a model combination yields additional performance gains.
5.3.4 Overview.
In the following, we will discuss the three components of the DIASUMM system that perform disfluency detection:  a POS tagger that tags, in addition to the standard SWITCHBOARD Treebank-3 tag set (LDC 1999b), the following disfluent regions or words: 1.
coordinating conjunctions that don't serve their usual connective role, but act more as links between subsequent speech acts of a speaker (e.g., and then; we call these empty coordinating conjunctions in this work) lexicalized filled pauses (labeled as discourse markers in the Treebank-3 corpus; e.g., you know, like) editing terms within speech repairs (e.g., I mean) nonlexicalized filled pauses (e.g., um, uh) a decision tree (supported by a shallow chunk parser) that decides whether to label a particular sentence as a false start a repetition detection script (for repeated sequences of up to four words) 5.3.5 Training Corpus.
For training, we used a part of the SWITCHBOARD transcripts that was manually annotated for sentence boundaries, POS, and the following types of disfluent regions (LDC 1999b):       {A.
. . }: asides (very rare; we ignore them in our experiments) {C.
. . }: empty coordinating conjunctions (e.g., and then) {D.
. . }: discourse markers (i.e., lexicalized filled pauses in our terminology, e.g., you know) {E.
. . }: editing terms (within repairs; e.g., I mean) {F.
. . }: filled pauses (nonlexicalized; e.g., uh) [.
. . + . . .]: repairs: the part before the + is called reparandum (to be removed), the part after the + repair (proper) Sentence boundaries can be at the end of completed sentences (E S) or of noncompleted sentences, such as false starts or abandoned clauses (N S).
462 Zechner Automatic Summarization of Dialogues Table 5 Precision, recall and F1 -scores of the four disfluency tag categories for the SWITCHBOARD test set.
Description Empty coordinating conjunctions Lexicalized filled pauses Editing terms Nonlexicalized filled pauses Count 5,990 5,787 1,004 12,926 Tag CO DM ET UH Precision 0.84 0.95 0.98 0.98 Recall 0.93 0.90 0.94 0.98 F1 0.88 0.93 0.96 0.98 Table 6 POS tagging accuracy on five subcorpora (evaluated on 500-word samples).
8E-CH Known words Unknown words (total) Overall 92.8 48.0 (25) 90.6 4E-CH 90.6 44.4 (9) 89.8 NHOUR 92.7 69.6 (23) 91.6 XFIRE 90.6 86.4 (22) 90.4 G-MTG 93.2 92.6 (27) 93.2 5.3.6 POS Tagger.
We are using Brill's rule-based POS tagger (Brill 1994).
Its basic algorithm at run time (after training) can be described as follows: 1.
2. Tag every word with its most likely tag, predicting tags of unknown words based on rules.
Change every tag according to its right and left context (both words and tags are considered), following a list of rules.
For preprocessing, we replaced the tags in the regions of {C.
. . }, {D.
. . }, and {E.
. . } with the tags CO (coordinating), DM (discourse marker), and ET (editing term), respectively.
(The filler regions {F.
. . } are already tagged with UH in the corpus).
Lines that contain typographical errors were excluded from the training corpus.
We further eliminated all incomplete words (XX tag) and combined multiwords, marked by a GW tag, into a single word (hence eliminating the GW tag).11 The entire resulting new tag set had 42 tags.12 Training of the POS tagger proceeded in three stages, using about 250,000 tagged words for each stage.
The trained POS tagger's performance on an unseen test set of about 185,000 words is 94.1% tag accuracy (untrained baseline: 84.8% accuracy).
Table 5 shows precision, recall, and F1 -scores for the four categories of disfluency tags, measured on the test set after the last training phase.
We see that the nonlexicalized filler words are almost perfectly tagged (F1 = 0.98), whereas the hardest task for the tagger is the empty coordinating conjunctions (F1 = 0.88): There are a few highly ambiguous words in that set, such as and, so, and or.
Table 6 shows the POS tagging accuracy on the five subcorpora of our dialogue corpus, evaluated on a sample of 500 words per subcorpus.
We see that the POStagging accuracy is slightly lower than for the SWITCHBOARD set that was used for 11 The sole function of the GW tag is to label words that are considered to be parts of other words but were transcribed separately, such as: drug/GW testing/NN.
12 For
a description of the POS tags used in that database see Santorini (1990) and LDC (1999a).
Computational Linguistics Volume 28, Number 4 Table 7 Disfluency tag detection (F1 ) for five subcorpora (results in parentheses: less than 10 tags to be detected).
8E-CH CO DM ET UH .89 .93 .95 .56 4E-CH .89 .73 .95 .62 NHOUR .38 .90 (.94) (.14) XFIRE .77 .82 .85 (.28) G-MTG .54 .30 .88 .45 training (approximately 9093%; global average: 91.1%).
Further we observe that with the exception of the CALLHOME corpora, the majority of unknown words were actually tagged correctly.
The most frequent errors were (1) conjunctions tagged as empty coordinated conjunctions, (2) proper names tagged as regular nouns, and (3) adverbs tagged as adjectives.
Finally, we look at the POS tagger's performance for the four disfluency tags CO, DM, ET, and UH in our five subcorpora; the results of this evaluation are presented in Table 7.
We can see that the detection accuracy is generally lower than for the corpus on which we trained the tagger (SWITCHBOARD), but still quite good in general.
The major exceptions are the UH tags, on which the F1 -scores are comparatively low for all subcorpora.
The reason for this can be found mostly in words like yes, no, uh-huh, right, okay, and yeah, which are often tagged with UH in SWITCHBOARD but frequently are not considered to be irrelevant words in our corpus and hence not marked as disfluent (e.g., if they are considered to be the answer to a question or a summary-relevant acknowledgment).
We circumvent potential exclusion from the summary output of these and other words that might be erroneously tagged as nonlexicalized filled pauses (UH) by marking a small set of words as exempt from removal (see section 5.5.6). 5.3.7 False Start Detection.
False starts are quite frequent in spontaneous speech, occurring at a rate of about 1015% of all sentences (SWITCHBOARD, CALLHOME).
They involve less than 10% of the total words of a dialogue; about 34% of the words in these incomplete sentences are part of some other disfluencies, such as filled pauses or repairs.
(In complete sentences, only about 15% of the words are part of these disfluencies).
For CALLHOME, the average length of complete sentences is about 6 words, of incomplete sentences about 4.1 words (including disfluencies).
We trained a C4.5 decision tree (Quinlan 1992) on 8,000 sentences of SWITCHBOARD.
As features we use the first and last four trigger words (words that have a high incidence around sentence boundaries) and POS of every sentence, as well as the first and last four chunks from a POS-based chunk parser.
This chunk parser is based on a simple context-free POS grammar for English.
It outputs a phrasal bracketing of the input string (e.g., noun phrases or prepositional phrases).
Further, we encode the length of the sentence in words and the number of the words not parsed by the chunk parser.
We observed that whereas the chunk information itself does not improve performance over the baseline of using trigger words and POS information only, the derived feature of "number of not parsed words" actually does improve the results.
We ran the decision tree on data with perfect POS tags (for SWITCHBOARD only), disfluency tags (except for repairs), and sentence boundaries.
The evaluations were performed on independent test sets of about 3,000 sentences for SWITCHBOARD and of our complete dialogue corpus.
Table 8 shows the results of these experiments.
Typical errors, where complete sentences were classified as incomplete, are inverted forms or 464 Zechner Automatic Summarization of Dialogues Table 8 False start classification results for different corpora (F1 ).
SWBD False start frequency (in %) False start detection (F1 ) 12.3 .611 8E-CH 12.1 .545 4E-CH 11.0 .640 NHOUR 2.0 .286 XFIRE 7.2 .352 G-MTG 13.9 .557 Table 9 Detection accuracy for repairs on the basis of individual word tokens using the repetition filter.
8E-CH Repair tokens (%) Precision Recall F1 -score 4.7 .88 .41 .56 4E-CH 3.8 .78 .32 .45 NHOUR 2.2 .25 .01 .02 XFIRE 1.3 .35 .04 .08 G-MTG 7.9 .91 .27 .41 ellipsis at the end of a sentence (e.g., neither do I, it seems to).
The performance for the informal corpora (CALLHOME, GROUP MEETINGS) is better than that for the formal corpora (NEWSHOUR, CROSSFIRE); this is related to the fact that the relative frequency of false starts is markedly lower in these latter data sets and that these corpora are more dissimilar to the training corpus (SWITCHBOARD).
5.3.8 Repetition
Detection.
The repetition detection component is concerned with (verbatim) repetitions within a speaker's turn, the most frequently occurring case of all speech repairs for informal dialogues (insertions and substitutions are comparatively less frequent).
Repeated phrases can potentially be interrupted by other disfluencies, such as filled pauses or editing terms.
Repetition detection is performed with a script that can identify repetitions of word/POS sequences of length one to four (longer repetitions are extremely rare: on average, less than 1% of all repetitions).
Words that have been marked as disfluent by the POS tagger are ignored when the repeated sequences are considered, so we can correctly detect repetitions such as [ he said uh to + he said to ] him.
. . . We are evaluating the precision, recall, and F1 -scores for this component at the level of individual words when the POS tagger and the sentence boundary detection component are used.
Table 9 shows the results.
We see that for the informal subcorpora, we get very good precision (only a few repetitions detected are incorrect), and recall is in the 2545% range (since we cannot detect substitution or insertion type of repairs).
The results for the formal subcorpora are considerably worse, so this filter should probably not be used for corpora with as few repetitions as NEWSHOUR or CROSSFIRE.
We checked all of the 95 false positives of this evaluation and observed that in the majority of cases (41%), the repetition was correctly detected but was not marked by the human annotator, since it might be considered a case of emphasis.
We believe that although some nuances of the sentence(s) might be lost, for the purpose of summarization it makes perfect sense to reduce this information.
Sometimes, individual words are repeated for emphasis, sometimes whole sentences (e.g., "Good./ Good./").
In the following example from English CALLHOME, the emphasis is rather extreme: 203 B: [...] How is the new person doing? q/ 204 A: Very very very very very well.
/ [...] Computational Linguistics Volume 28, Number 4 Further, about 19% of false positives were correct but not annotated because they span multiple turns, and about 14% were erroneously missed by the human annotator.
Only the remaining cases (26%) were actual false positives, caused by incorrect POS tags (5%, typically an incorrectly tagged "that/WDT that/DT" sequence at the beginning of a relative clause) or incorrect sentence boundaries (21%).
There have been attempts to get a more complete coverage of detection and correction of all types of speech repairs (Heeman and Allen 1999).
We decided, however, to use a simple method here that works well for a large subset of cases and is very efficient at the same time.
5.3.9 Disfluency
Correction in DIASUMM.
After detection, the correction of disfluencies is straightforward.
When DIASUMM generates its output from the ranked list of sentences, it skips the false starts, the repetitions, and the words that were tagged with CO, DM, ET, or UH by the POS tagger.
5.4 Sentence
Boundary Detection 5.4.1 Introduction.
The purpose of the sentence boundary detection component is to insert linguistically meaningful sentence boundaries in the text, given a POS-tagged input.
We consider all intraturn and interturn boundary positions for every speaker in a conversation.
We use the abbreviations EOS for end of complete sentence (E S in the SWITCHBOARD corpus) and NEOS for end of noncomplete sentence (N S in the SWITCHBOARD corpus).
The frequency of sentence boundaries (with respect to the total number of words) is about 13.3%, most of the boundaries (almost 90%) being end markers of completed sentences (SWITCHBOARD).
5.4.2 Training
and Testing.
We trained a C4.5 decision tree and computed its input features from a context of four words before and after a potential sentence boundary, motivated by the results of Gavald`, Zechner, and Aist (1997).
Also following Gavald`, a a Zechner, and Aist (1997), we used 60 trigger words with high predictive potential, employing the score computation method described in this article.
The decision tree input features for every word position are as follows:     POS tag (42 different tags) trigger word (60 different trigger words) turn boundary before this word? if turn boundary: length of pause after last turn of same speaker Since NEOS boundaries occur very infrequently (only about 10% of all boundaries, which is only about 1% of all potential boundaries), we decided to merge this class with the EOS class and report results for this combined class only (CEOS).
(We relied on the false-start detection module described above to identify the NEOS sentences within this merged class of sentences after the sentence boundary classification).
For training, we used 25,000 words from the Treebank-3 corpus; the test set size was 1,000 words.
Table 10 shows the results in detail for the various parameter combinations.
We see that for good performance we need to know about one of these two features: "is there a turn boundary before this word"? or "pause duration after last turn from same speaker".
466 Zechner Automatic Summarization of Dialogues Table 10 Sentence boundary detection accuracy (F1 -score).
With Interturn Pause Duration?
With Turn Boundary Info?
Training set Test set Yes Yes .904 .887 No .903 .884 Yes .900 .884 No No .884 .825 Table 11 Interand intraturn boundary detection (BD) results on 1,000-word test set.
Occurrence (%) Interturn Interturn Intraturn Intraturn non-BD BD non-BD BD 12 112 809 61 (1.2) (11.3) (81.4) (6.1) Detection Accuracy (F1 ) .56 .95 .99 .77 5.4.3 Effect of Imperfect POS Tagging.
To see how much influence an imperfect POS tagging might have on these results, we POS-tagged the test set data using the POS tagger described above.
For this and the following experiments, we increased the training corpus for the decision tree to 40,000 words.
The POS tagger accuracy for this test set was about 95.3%, and the F1 -score for CEOS was .882, which is 98.9% of .892 on perfect POS-tagged input.
This is encouraging, since it shows that the decision tree is not very sensitive to the majority of POS errors.
5.4.4 Interturn
and Intraturn Boundaries.
In this analysis, we are interested in comparing the detection of sentence boundaries between turns (interturn) to the detection of boundaries within a turn (intraturn).
Table 11 shows the results of this analysis (same test set as above).
As might be expected, the performance is very good for the two frequent classes: sentence boundaries at the end of turns and nonboundaries within turns (F1 > .95), but considerably worse for the two more infrequent cases.
The very rare cases (around 1% only) of nonsentence boundaries at the end of turns (i.e.
turn continuations) show the lowest performance (F1 = .56).
5.4.5 Sentence
Boundary Detection on Dialogue Corpus.
To get a picture of the realistic performance of the sentence boundary detection component, using the (imperfect) POS tagger and a faster, but slightly less accurate, decision tree,13 we evaluate the sentence boundary detection accuracy for all five subcorpora of our dialogue corpus.
Table 12 provides the results of these experiments.
The results reflect a trend very similar to that for the SWITCHBOARD corpus, in that the two more frequent classes (interturn boundaries and intraturn nonboundaries) have high detection scores, whereas the two more infrequent classes are less well detected.
Furthermore, we observe that in cases in which the relative frequency of rare classes is further reduced, the classification accuracy declines overproportionally (particularly for the rarest class of the interturn nonboundaries).
Also, overall boundary detection is better for the two more informal corpora, CALLHOME and GROUP MEETINGS (F1 > .72).
13 This
decision tree uses a different type of encoding, but the same input features.
Computational Linguistics Volume 28, Number 4 5.5 Cross-Speaker Information Linking 5.5.1 Introduction.
One of the properties of multiparty dialogues is that shared information is created between dialogue participants.
The most obvious interactions of this kind are question-answer (Q-A) pairs.
The purpose of this component is to create automatically such coherent pieces of relevant information, which can then be extracted together while generating the summary.
The effects of such linkings on actual summaries can be seen in two dimensions: (1) increased local coherence in the summary and (2) a potentially higher informativeness of the summary.
Since Q-A linking has a side effect in that other information will be lost with respect to a summary of the same length without Q-A linking, the second claim is much less certain to hold than the first.
We investigated these questions in related work (Zechner and Lavie 2001) and found that although Q-A linking does not significantly change the informativeness of summaries on average, it does increase summary coherence (fluency) significantly.
In this section, we will be concerned with the following two intuitive subtasks of Q-A linking: (1) identifying questions (Qs) and (2) finding their corresponding answers.
5.5.2 Related
Work.
Detecting a question and its corresponding answer can be seen as a subtask of the speech act detection and classification task.
Recently, Stolcke et al.(2000) presented a comprehensive approach to dialogue act modeling with statistical techniques.
A good overview and comparison of recent related work can also be found in Stolcke et al.'s article.
Results from their evaluations on SWITCHBOARD data show that word-based speech act classifiers usually perform better than prosody-based classifiers, but that a model combination of the two approaches can yield an improvement in classification accuracy.
5.5.3 Corpus
Statistics.
For training of the question detection module, we used the manually annotated set of about 200,000 SWITCHBOARD speech acts14 (SAs);15 for training of the answer detection component, we used the eight English CALLHOME dialogues (8E-CH), which were manually annotated for Q-A pairs.
Although we were aiming to detect all questions in the question detection module, the answer detection module focuses on Q-A pairs only: We exclude all questions from consideration that are not Yes-No(YN) or Wh-questions (such as rhetorical or back-channel questions), 14 In this work, the notions of speech acts and sentences can be considered equivalent.
15 From
the Johns Hopkins University Large Vocabulary Continuous Speech Recognition (LVCSR) Summer Workshop 1997.
Thanks to Klaus Ries for providing the data, which are also available from http://www.colorado.edu/ling/jurafsky/ws97/.
Zechner Automatic Summarization of Dialogues Table 13 Frequency of different types of questions in the 8E-CH data set.
Sentences Wh-questions total . . . With immediate answers YN-questions total . . . With immediate answers Qs excluded for Q-A detection Questions total 2,211 20 15 (75%) 48 38 (79%) 15 83 (3.75%) as well as those that do not have an answer in the dialogue.
Thus we employ only 68 pf the 83 questions marked in the 8E-CH data set for these evaluations.
Table 13 provides the statistics concerning questions and answers for the 8E-CH subcorpus and shows that for a small but significant number of questions, the answer does not immediately follow the question speech act (delayed answers).
5.5.4 Automatic
Question Detection.
We used two different methods, both trained on SWITCHBOARD data: (1) a speech act tagger16 and (2) a decision tree based on trigger word and part-of-speech information.
Speech act tagger.
The speech act tagger tags one speech act at a time and hence can make use only of speech act unigram information.
Within a speech act, it uses a language model based on POS and the 500 most frequent word/POS pairs.
It was trained on the aforementioned SWITCHBOARD speech act training set.
It was not optimized for the task of question detection.
Its typical run time for speech act classification is about 10 speech acts per second.
Decision tree question classifier.
The decision tree classifier (C4.5) uses the following set of features: (1) POS and trigger word information for the first and last five tokens of each speech act;17 (2) speech act length, and (3) occurrence of POS bigrams.
The set of trigger words is the same as for the sentence boundary detection module.
The POS bigrams were designed to be most discriminative between question speech acts (q-SAs) and nonquestion speech acts (non-q-SAs).
The bigrams were obtained as follows: 1.
For a balanced set of q-SAs and non-q-SAs (about 9,000 SAs each): Count all the POS bigrams in positions 1 . . . 5 and (n 4) . . . n (using START and END for the first and last bigrams, respectively) and memorize position (beginning or end of SA) and type (q-SA vs.
non-q-SA). For all bigrams: (a) (b) (c) (d) 3.
Add one to the count (to prevent division by zero).
Divide the q-SA count by the non-q-SA count.
If the ratio is smaller than one, invert it (ratio := 1/ratio).
Multiply the result of (c) by the sum of q-SA count and non-q-SA count.18 Extract the 100 bigrams with the highest value.
16 Thanks
to Klaus Ries for providing us with the software.
17 Shorter
speech acts are padded with dummies.
18 Leaving
out this step favors low-frequency, high-discriminative bigrams too much and causes a slight reduction in overall Q-detection performance.
Computational Linguistics Volume 28, Number 4 Table 14 Question detection on the 8E-CH corpus using two different classifiers.
SA Tagger Overall error Precision Recall F1 Typical classification time (SAs/sec) 3.2% .57 .61 .59 10 Decision Tree 4.7% .63 .51 .56 1,000 Experiments and results.
The question detection decision tree was trained on a set of about 20,000 speech acts from the SWITCHBOARD corpus.
We first evaluated the speech act tagger and the decision tree classifier on the 8E-CH data set.
Whereas in the later stage of answer detection, questions without answers and nonpropositional questions are ignored, at this point we are interested in the detection of all annotated questions in the corpus.
This also reflects the fact that the training set contains all possible types of questions.
Table 14 reports the results of the question detection experiments with the two classifiers used on the 8E-CH subcorpus.
We note that whereas the decision tree is performing only slightly worse than the speech act tagger, its typical classification time is two orders of magnitude faster.
Based on these observations, we decided to use the question detection decision tree in the Q-A linking component of the DIASUMM system.
5.5.5 Detecting
the Answers.
After identifying which sentences are questions, the next step is to identify the answers to them.
From the 8E-CH statistics of Table 13 we observe that for more than 75% of the YNand Wh-questions, the answer is to be found in the first sentence of the speaker talking after the speaker uttering the question.
In the remainder of cases, the majority of answers are in the second (instead of the first) sentence of the responding speaker.
Further, the speaker who has posed a question usually utters no (or only very few) sentences after the question is asked and before the next speaker starts talking.
In addition to detecting sequential Q-A pairs, we also want to be able to detect simple embedded questions, as shown in this example of a brief clarification dialogue: Q Q 1 2 3 4 A: B: A: B: When are we meeting then?
You mean tomorrow?
Yes. At 4pm.
We devise the following heuristics to detect answers to question speech acts which have been previously identified:  If the first speaker change after the question occurs more than maxChg sentences after the question, the search is stopped and no Q-A pair is returned.
Answer hypotheses are sought for maximally maxSeek sentences after the first speaker change following the question, but not over interruptions by any other speaker; that is, we check within a single speaker region Zechner Automatic Summarization of Dialogues (this is the stopping criterion for the following two heuristics).
An exception occurs if there is an embedded question in the first single speaker region: In that case, we look at the next region where a speaker different from the initial Q-speaker is active.19   Answers have to be minimally minAns words long; if they are shorter, we add the next sentence to the current answer hypothesis.
Even if the minimum answer length is reached, the answer can be optionally extended if at least one word in the answer matches a word from the question (one of two different stop lists (StopShort, StopLong) or no stop list is used to remove function words from consideration).20 We have these further restrictions for the case of embedded questions: 1.
If we detect a potential embedded Q-A pair, the answer to the surrounding question must immediately follow the answer to the embedded question (i.e., the region following the potential answer region of the embedded question--sentence 4 in our above example--must (1) not contain a question itself and (2) be from a different speaker than the surrounding question).
A crossover is prohibited; that is, we eliminate all pairs Qj, Al when a pair Qi, Ak was already detected, with i < j < k < l (k, l being start indices of answer spans).
The output of the algorithm is a list of triples Q, Astart, Aend, where Q is the sentence ID of the question and Astart the first and Aend the last sentence of the answer.
As mentioned above, we use only 68 of the 83 questions marked in the 8ECH data set for these evaluations, since only these are YNor Wh-questions that actually have answers in the dialogue.
There are four possible outcomes for each triple: (1) irrelevant: a Q-A pair with an incorrectly hypothesized question (this is the fault of the question detection module, not of this heuristic); (2) missed: the answer was missed entirely; (3) completely correct: Aend coincides with the correct answer sentence ID; and (4) correct range: the answer is contained in the interval [Astart, Aend ] but does not coincide with Aend . For the calculation of precision, recall, and F1 -score, we count classes (3) and (4) as correct and use the sum of all classes for the denominator of precision and the total number of Q-A pairs (68 in this development set) as the denominator of recall.
To determine the best parameters, we varied them across a reasonable set of values and ran the answer detection script for all combinations of parameters.
The best results (with respect to F1 -score) using questions detected by the speech act tagger and the decision tree are reported in Table 15.
In the DIASUMM system, we use the following optimal parameter settings for the answer detection heuristics: maxChg = 2, maxSeek = 4, minAns = 10, sim = on, stop = no.
Finally, we evaluated the performance of both the Q-detection module and the combined Q-A detection on all five subcorpora, using the decision tree for question detection; the results are reported in Table 16.
Except for the rather small NEWSHOUR 19 This would be sentence 4 in the example above.
20 StopLong contains 571 words, StopShort only 89 words, most of which are auxiliary verbs and filler words.
Computational Linguistics Volume 28, Number 4 Table 15 Q-A detection results using two different classifiers for question detection (68 Q-A pairs to be detected).
SA Tagger All hypothesized Q-A pairs Correct [(3) and (4)] maxChg (15) maxSeek (24) minAns (110) Similarity extension (on/off) Stop list (no/short/long) Precision Recall F1 -score 80 42 4 34 510 on no/short .53 .62 .57 Decision Tree 54 31 2 24 210 on no/short .57 .46 .51 Table 16 Performance comparison for Qand Q-A detection (Q-detection with the decision tree question classifier).
8E-CH Q to detect Q-hypotheses Q-detection (F1 ) Q-A pairs to detect Q-A pair hypotheses Q-A detection (F1 ) 83 67 .56 68 54 .51 4E-CH 94 60 .58 69 54 .60 NHOUR 19 16 .80 18 14 .81 XFIRE 110 71 .60 79 54 .51 G-MTG 49 52 .59 32 33 .51 corpus (with fewer than 20 questions or Q-A pairs to identify), the typical Q-detection F1 -score is around .6 and the Q-A detection F1 -score around .5.
In two cases, the Q-A detection performance is slightly better than the Q-detection performance.
This can be explained by the fact that the answer detection algorithm prunes away a number of Q-hypotheses, reducing the space for potential Q-A hypotheses.
5.5.6 Q-A Detection within DIASUMM.
When we use the Q-A detection module as part of the DIASUMM system, we want to ensure that (1) there are no Q-A pairs containing Q-sentences that are false starts and that (2) the initial part of an answer is not lost in case the disfluency detection component marks some indicative words as disfluencies.
To satisfy the first constraint, we block Q-detection of sentences that have been previously classified as false starts; as for the second constraint, we create a list of indicative words (relevant for YN-questions) that are not to be removed by the summary generator if they appear in the beginning (leading five words) of answers.21 5.6 Sentence Ranking and Selection 5.6.1 Introduction.
The sentence ranking and selection component is an implementation of the MMR algorithm (Carbonell, Geng, and Goldstein 1997), applied to extracting the most relevant sentences from a topical segment of a dialogue.
The component's output in isolation serves as the MMR baseline for the global system evaluation.
Its 21 The current list comprises the following words: no, yes, yeah, yep, sure, uh-huh, mhm, nope.
Zechner Automatic Summarization of Dialogues purpose is to determine weights for terms and sentences, to rank the sentences according to their relevance within each topical segment of the dialogue, and finally to select the sentences for the summary output according to their rank, as well as to other criteria, such as question-answer linkages, established by previous components.
The selected sentences are presented to the user in text order.
5.6.2 Tokenization.
In addition to the tokenization rules for the global system (section 5.2), we apply a simple six-character truncation for stemming and use a stop word list to eliminate frequent noncontent words.
In the experiments, we used the following five different stop word lists:      the original SMART list (Salton 1971) (SMART-O) a manually edited stop list based on SMART (SMART-M) a stop list with all closed-class words from the POS tagger's lexicon (POS-O) a manually edited stop list based on the POS tagger's lexicon and frequent closed-class words in the CALLHOME training corpus (POS-M) an empty stop list (EMPTY) 5.6.3 Term and Sentence Weighting.
The basic idea for determining the most relevant sentences within a topical segment is as follows: First, we compute a vector of word weights for the segment tfq (including all stemmed nonstop words) and do the same for each sentence (tft ), then we compute the similarity between sentence and segment vectors for each sentence.
That way, sentences that have many words in common with the segment vector are rewarded and receive a higher relevance weight.
Whereas we compose the sentence vectors tft using direct term frequency counts, the weights for segment terms are determined according to one of the three formulae in equation (1) (freq, smax, and log), inspired by Cornell University's SMART system (Salton 1971): fi,s or 1 + log fi,s, (1) tfi,s = fi,s or 0.5 + 0.5 fsmax where fi,s are the in-segment frequencies of a stem and fsmax are maximal segment frequencies of any stem in the segment.
Finally, we multiply an inverse document frequency (IDF) weight to tfs to obtain the segment vectors tfq, as shown in equations (2) and (3): tfi,q IDFi,s = tfi,s IDFi,s = 1 + log Nseg iseg or Nseg . iseg (2) (3) IDF values are computed with respect to a collection of topical segments, either the current dialogue (DIALOGUE) or a set of dialogues (CORPUS).
Nseg is the total number of topical segments in the IDF corpus, and iseg is the number of segments in which the token i appears at least once.
The effect of using IDF values is to boost those words that are (relatively) unique to any given segment over those that are more evenly distributed across the corpus.
As stated above, the main algorithm is a version of the MMR algorithm (Carbonell, Geng, and Goldstein 1997; Carbonell and Goldstein 1998), which emphasizes sentences 473 Computational Linguistics Volume 28, Number 4 that contain many highly weighted terms for the current segment (salience) and are sufficiently dissimilar to previously ranked sentences (diversity or antiredundancy).
The MMR formula is given in equation (4): nextsentence = arg max(sim 1 (query, tnr,j ) (1 ) max sim 2 (tnr,j, tr,k )).
The MMR formula describes an iterative algorithm and states that the next sentence to be put in the ranked list will be taken from the sentences that have not yet been ranked (tnr ).
This sentence is (1) maximally similar to a query and (2) maximally dissimilar to the sentences that have already been ranked (tr ).
We use the topical segment word vector tfq as query vector.
The parameter (0.0 1.0) is used to trade off the influence of salience against that of redundancy.
Both similarity metrics (sim 1, sim 2 ) are inner vector products of stemmed-term frequencies (equations (5) and (6)).
sim 1 can be normalized in different ways: (1) to yield a cosine vector product (division by product of vector lengths), (2) division by number of content words,22 and (3) no normalization: sim 1 = tfq tft |tfq ||tft | tft1 tft2 |tft1 ||tft2 | or tfq tft 1 + i tfi,t or tfq tft (5) Emphasis factors.
Every sentence's similarity weight (sim 1 ) can be (de)emphasized, based on a number of its properties.
We implemented optional emphasis factors for:     Lead emphasis: for the leading n% of a segment's sentences: sim 1 = sim 1 l, with l being the lead factor.
Q-A emphasis: for all sentences that belong to a question-answer pair: sim 1 = sim 1 q, with q being the Q-A emphasis factor.
False-start deemphasis: for all sentences that are false starts: sim 1 = sim 1 f, with f being the false-start factor.
Speaker emphasis: for each individual speaker s, an emphasis factor se can be defined: sim 1 = sim 1 se for all sentences of speaker s.23 These parameters can serve to fine-tune the system for particular applications or user preferences.
For example, if the false starts are deemphasized, they are less likely to trigger a question's being linked to them in the linking process.
If questions and answers are emphasized, more of them will show up in the summary, increasing its coherence and readability.
In a situation in which a particular speaker's statements are of higher interest than those of other speakers, his sentences can be emphasized, as well.
Since sim 2 is a cosine vector product and hence in [0,1], we have to normalize sim 1 to [0,1] as well to enable a proper application of the MMR formula.
For this normalization of sim 1, we divide each sim 1 score by the maximum of all sim 1 scores in a segment after initial computation and application of the various emphasis factors described here.
22 To avoid division by zero, we add one to every sentence length.
23 Speaker emphasis is not used in our evaluations.
Zechner Automatic Summarization of Dialogues 5.6.4 Q-A Linking.
While generating the output summary from the MMR-ranked list of sentences, whenever a question or an answer is encountered (detected previously by the Q-A detection module), the corresponding answer/question is linked to it and moved up the relevance ranking list to immediately follow the current question/answer.
If the question-answer pair consists of more than two sentences, the linkages are repeated until no further questions or answers can be added to the current linkage cluster.
5.6.5 Summary
Types.
DIASUMM can generate several different types of summaries, the two main versions being (1) the CLEAN summary, which is based on the output of all DIASUMM components (disfluency detection, sentence boundary detection, Q-A linking), and (2) the TRANS summary, in which all dialogue specific components are ignored (essentially, this is an MMR summary of the original dialogue transcript).
For the purpose of the global system evaluation, we use only these two versions of summaries, as well as LEAD baseline summaries, where the summary is formed by extracting the first n words from a topical segment.24 Furthermore, the system can generate phrasal summaries, which render the sentences in the same ranking order as the CLEAN summary but reduce the output to noun phrases and potentially other phrases, depending on the setting of parameters.25 In Figure 2 we show an example of a set of LEAD, TRANS, CLEAN, and PHRASAL summaries.
The set was generated from the CALLHOME transcript presented in section 2.
5.6.6 System
Tuning.
This section describes how we arrive at an optimal parameter setting for each subcorpus (CALLHOME, NEWSHOUR, CROSSFIRE, GROUP MEETINGS).
We want to establish an MMR baseline for the global system evaluations with which we can then compare the results of the entire DIASUMM system.
Note that for all the tuning experiments described in this subsection, we did not make use of any other DIASUMM components, namely, disfluency detection, sentence boundary detection, and questionanswer linking.
All experiments were based on the human gold standard with respect to topical segments.
We used only the devtest set for the four subcorpora here (8ECH = CALLHOME, DT-NH = NEWSHOUR, DT-XF = CROSSFIRE, and DT-MTG = GROUP MEETINGS).
Since the length of turns varies widely, one could argue that an easy way to increase performance for the MMR baseline (which does not use automatic sentence boundary detection) might be to split overly long turns evenly into shorter chunks.
This was done by Valenza et al.(1999), who experimented with lengths of 1030 words per extract fragment.
We add this option as an additional parameter to the MMR baseline.
If the parameter is set to n words, turns with a length l 1.5n get cut into pieces of lengths n iteratively until the last remaining piece is l < 1.5n.
Evaluation metric.
To evaluate the performance of this component, we use the word-based evaluation metric described in section 6.2, which gives the highest scores to summaries containing words with the highest average relevance scores, as marked by human annotators.
We then average these scores over all topical segment summaries of a particular subcorpus.
24 Note that LEAD summaries are to be distinguished from summaries in which lead emphasis is used, as described above.
In the latter case, the segment-initial sentence weights are increased, whereas in the former case, we strictly extract the leading n words from a given segment.
25 To determine these constituents, we use the output of the chunk parser employed by the false start detection component.
Computational Linguistics Volume 28, Number 4 LEAD: 1 a: Oh 2 b: They didn't know he was going to get shot but it was at a peace rally so I mean it just worked out 3 b: I mean it was a good place for the poor guy to die I mean because it was you know right after the rally and everything was on film and everything [...] TRANS: 2 b: They didn't know he was going to get shot but it was at a peace rally so I mean it just worked out 3 b: I mean it was a good place for the poor guy to die I mean because it was you know right after the rally and everything was on film and everything 11 b: Him [...] CLEAN: 7 b: We just finished the thirty days mourning for him now it's everybody's still in shock it's terrible what's going on over here 31 b: What's the reaction in america really do people care [...] 34 a: Most I don't know what I mean like the jewish community a lot all of us were very upset PHRASAL: 4 b: it just worked ...
it was a good place for the poor guy to die ...
it was [...] 7 b: we just finished the thirty days mourning for him ...
it's ...
everybody's ...
in shock it's ...
going ...
31 b: 's the reaction in america ...
do people care ...
34 a: i don't know ...
mean like the jewish community a lot ...
of us were Note: The turn IDs are just indicating the relative position of the turns within the original text and do not always correspond to the turn numbers of the original or to the turn numbers of the other summaries.
The . . . marks the position in those sentences where the length threshold for a summary was reached.
Figure 2 Example summaries of 20% length: LEAD, TRANS, CLEAN and PHRASAL.
Parameter tuning.
The system tuning proceeded in three phases, in which we held the summary size constant to 15% and optimized the following set of parameters: 1.
2. 3.
4. 5.
476 Term weight type: freq, smax, log Normalization: cos, length, none IDF type: corpus, dialogue, none IDF method: log, mult Extract span: 1030 or original turn (orig) Zechner Automatic Summarization of Dialogues Table 17 Optimally tuned parameters for MMR baseline system (tuning on devtest set subcorpora).
8E-CH Term weight type Normalization IDF type IDF method Extract span MMRStop list Lead factor smax cos corpus log 20 0.85 SMART-M 1.0 DT-NH DT-XF DT-Mtg smax none corpus log orig 0.8 POS-M 1.0 smax cos corpus mult 25 1.0 POS-M 1.0 smax none corpus log orig 0.8 POS-M 2.0 MMR-: 0.81.0 Stop lists: SMART-O, SMART-M, POS-O, POS-M, EMPTY Lead factor: 1.05.0 (applied to first 20% of sentences) Table 17 shows the parameter settings that were determined to be optimal for the MMR baseline system (TRANS summaries).
5.7 System
Performance The majority of the system components are implemented in Perl5, except for the C4.5 decision tree (Quinlan 1992), the chunk parser (Ward 1991), and the POS tagger (Brill 1994), which were implemented in C by the respective authors.
We measured the system runtime on a 300 MHz Sun Ultra60 dual-processor workstation with 1 GB main memory, summarizing all 23 dialogue excerpts from our corpus.
The average runtime for the whole system, including all of its components except for the topic segmentation module, was 17.8 seconds, and for the sentence selection component alone 7.0 seconds (per-dialogue average).
The average ratio of system runtime to dialogue duration was 0.029 (2.9% of real speaking time).
6. Evaluation 6.1 Introduction Traditionally, summarization systems have been evaluated in two major ways: (1) intrinsically, measuring the amount of the core information preserved from the original text (Kupiec, Pedersen, and Chen 1995; Teufel and Moens 1997), and (2) extrinsically, measuring how much the summary can benefit in accomplishing another task (e.g., finding a document relevant to a query or classifying a document into a topical category) (Mani et al.1998). In this work, we focus on intrinsic evaluation exclusively.
That is, we want to assess how well the summaries preserve the essential information contained in the original texts.
As other studies have shown (Rath, Resnick, and Savage 1961; Marcu 1999), the level of agreement between human annotators about which passages to choose to form a good summary is usually quite low.
Our own findings, reported in section 4.2.4, support this in that the intercoder agreement, here measured on a word level, is rather low.
We decided to minimize the bias that would result from selecting either a particular human annotator, or even the manually created gold standard, as a reference 477 Computational Linguistics Volume 28, Number 4 for automatic evaluation; instead, we weigh all annotations from all human coders equally.
Intuitively, we want to reward summaries that contain a high number of words considered to be relevant by most annotators.
We formalize this notion in the following subsection.
6.2 Evaluation
Metric All evaluations are based on topically coherent segments from the dialogue excerpts of our corpus.
As mentioned before, the segment boundaries were chosen from the human gold standard for the purpose of the global system evaluation.
For each segment s, for each annotator a, and for each word position wi, we define a boolean word vector of annotations ws,a, each component ws,a,i being 1 if the word wi is part of a nucleus IU or a satellite IU for that annotator and segment, and 0 otherwise.
We then sum over all annotators' annotation vectors and normalize them by the number of annotators per segment (A) to obtain the average relevance vector for segment s, rs : A ws,a,i rs,i = a=1 . (7) A To obtain the summary accuracy score sas,n for any segment summary with length n, we multiply the boolean summary vector summs 26 by the average relevance vector rs, and then divide this product by the sum of the n highest scores within rs (maximum achievable score), rsorts being the vector rs sorted by relevance weight in descending order: summs rs sas,n = n (8) i=1 rsorts,i It is easy to see that the summary accuracy score always is in the interval [0.0, 1.0]. 6.3 Global System Evaluation Whereas section 5 was concerned with the design and evaluation of the individual system components, the goal here is to describe and analyze the quality of the global system, with all its components combined.
In this section, we compare our DIASUMM system with the MMR baseline system, which operates without any dialogue-specific components, and with the LEAD baseline.
We described the optimization and finetuning of the MMR system in subsection 5.6.6.
The second column of Table 18 presents the average relevance scores for this MMR baseline, averaged over the five summary sizes of 5%, 10%, 15%, 20%, and 25% length, for the four devtest set and the four eval set subcorpora; the first column of this table shows the results for the LEAD baseline.
We used the optimized baseline MMR parameters and varied only the emphasis parameters for (1) false starts, (2) lead factor, and (3) Q-A sentences, to optimize the CLEAN summaries further.
(Again, for this step, we used only the devtest subcorpora).
For each corpus in the devtest set, we determined the optimal parameter settings and report the corresponding results also for the eval set subcorpora.
Column 3 in Table 18 provides the results for this optimized DIASUMM system.
Further, in column 4, we provide the summary accuracy averages for the human gold standard (nucleus IUs only, fixed-length summaries).
Table 19 shows the best emphasis parameter combinations for the DIASUMM summaries used in these evaluations.
We determined the statistical differences between the DIASUMM system and the two baselines for the eval set, using the Wilcoxon rank sum test for each of the four 26 Definition: 1 if summs,i is contained in the summary, 0 otherwise.
Zechner Automatic Summarization of Dialogues Table 18 Average summary accuracy scores: devtest set and eval set subcorpora on optimized parameters, comparing LEAD, MMR baseline, DIASUMM, and the human gold standard.
Subcorpus 8E-CH DT-NH DT-XF DT-MTG EVAL-NH EVAL-XF EVAL-MTG Table 19 Best emphasis parameters for the DIASUMM system, trained on the devtest set.
Corpus CALLHOME NEWSHOUR CROSSFIRE GROUP MEETINGS False Start 0.5 0.5 0.5 0.5 Q-A 1.0 2.0 1.0 1.0 Lead Factor 2.0 1.0 1.0 3.0 Table 20 Average summary accuracy scores for different system configurations for the four different subcorpora.
Corpus 4E-CH EVAL-NH EVAL-XF EVAL-GMTG LEAD .438 .692 .378 .324 MMR .526 .526 .564 .449 DFF-ONLY .599 .551 .528 .488 SB-ONLY .547 .608 .525 .513 NO-QA DIASUMM .614 .506 .566 .583 subcorpora.
Comparisons were made for each of the five summary sizes within each topical segment.
For the CALLHOME and GROUP MEETINGS subcorpora, our DIASUMM system is significantly better than the MMR baseline (p < 0.01); for the two more formal subcorpora, NEWSHOUR and CROSSFIRE, the differences between the performance of the two systems are not significant.
Except for on the NEWSHOUR subcorpus, both the MMR baseline and the DIASUMM system perform significantly better than the LEAD baseline.
6.4 Discussion
Table 20 shows the average performance of the following six system configurations, averaged over all topical segments and all summary sizes (525% length summaries; in configurations 35 below, components used are in addition to the core MMR summarizer): 1.
2. LEAD: using the first n% of the words in a segment MMR: the MMR baseline (tuned; see above) 479 Computational Linguistics Volume 28, Number 4 DFF-ONLY: using the disfluency detection components (POS tagger, false-start detection, repetition detection), but no sentence boundary detection or question-answer linking SB-ONLY: using the sentence boundary detection module, but no other dialogue-specific modules NO-QA: a combination of DFF-ONLY and SB-ONLY (all preprocessing components used except for question-answer linking) DIASUMM: complete system with all components (all disfluency detection components, sentence boundary detection, and Q-A linking) We observe that in all subcorpora, except for CROSSFIRE, the addition of either the disfluency components or the sentence boundary component improves the summary accuracy over that of the MMR baseline.
As we would expect, given the much higher frequency of disfluencies in the two informal subcorpora (CALLHOME, GROUP MEETINGS), the relative performance increase of DFF-ONLY over the MMR baseline is much higher here (about 1015%) than for the two more formal subcorpora (5% and below).
Looking at the performance increase of SB-ONLY, we find marked improvements over the MMR baseline for those two subcorpora that use the true original turn boundaries in the MMR baseline: GROUP MEETINGS and NEWSHOUR (>10%); for the two other subcorpora, the improvement is below 5%.
Furthermore, the combination of the disfluency detection and sentence boundary detection components (NO-QA) improves the results over the configurations DFF-ONLY and SB-ONLY.
The situation is much less uniform when we add the question-answer detection component (this then corresponds to the full DIASUMM system): In the CROSSFIRE corpus, we have the largest performance increase (we also have the highest relative frequency of question speech acts here).
For the two informal corpora, the change is only minor; for NEWSHOUR, the performance decreases substantially.
We showed in Zechner and Lavie (2001), however, that in general, for dialogues with relatively frequent Q-A exchanges, the accuracy of a summary (informativeness) does not change significantly when the Q-A detection component is applied.
On the other hand, the (local) coherence of the summary does increase significantly, but we cannot measure this increase with the evaluation criterion of summary accuracy used here.
To conclude, we have shown that using dialogue-specific components, with the possible exception of the Q-A detection module, can help in creating more accurate summaries for more informal, casual, spontaneous dialogues.
When more formal conversations (which may even be partially scripted), containing relatively few disfluencies, are involved, either a simple LEAD method or a standard MMR summarizer will be much harder to improve upon.
7. Discussion and Directions for Future Work The problem of how to generate readable and concise summaries automatically for spoken dialogues of unrestricted domains involves many challenges that need to be addressed.
Some of the research issues are similar or identical to those faced in summarizing written texts (such as topic segmentation, determining the most salient/relevant information, anaphora resolution, summary evaluation), but other additional dimensions are added on top of this list, including speech disfluency detection, sentence boundary detection, cross-speaker information linking, and coping with imperfect speech recognition.
The line of argument of this article has been that whereas using a 480 Zechner Automatic Summarization of Dialogues traditional approach for written text summarization (such as the MMR-based sentence selection component within DIASUMM) may be a good starting point, addressing the dialogue-specific issues is key for obtaining better summaries for informal genres.
We decided to focus on the three problems of (1) speech disfluency detection, (2) sentence boundary detection, and (3) cross-speaker information linking and implemented trainable system components to address each of these issues.
Both the evaluations of the individual components of our spoken-dialogue summarization system and the global evaluations as well have shown that we can successfully make use of the SWITCHBOARD corpus (LDC 1999b) to train a system that works well on two other genres of informal dialogues, CALLHOME and GROUP MEETINGS.
We conjecture that the reasons why the DIASUMM system was not able to improve over the MMR baseline for the two other corpora, which are more formal, lies in their very nature of being of a quite different genre: the NEWSHOUR and CROSSFIRE corpora have longer turns and sentences, as well as fewer disfluencies.
We would also conjecture that their sentence structures are more complex than what we typically find in the other corpora of more colloquial, spontaneous conversations.
Future work will have to address the issue of whether the availability of training data for more formal dialogues (in size and annotation style comparable to the SWITCHBOARD corpus, though) could lead to an improvement in performance on those data sets, as well, or if even then a standard written-text-based summarizer would be hard to improve upon.
Given the complexity of the task, we had to make a number of simplifying assumptions, most notably about the input data for our system: We use perfect transcripts by humans instead of ASR transcripts, which, for these genres, typically show word error rates (WERs) ranging from 15% to 35%.
Previous related work (Valenza et al.1999; Zechner and Waibel 2000b) demonstrated that the actual WERs in summaries generated from ASR output are usually substantially lower than the full-ASR-transcript WER and can further be reduced by taking acoustically derived confidence scores into account.
We further did not explore the potential improvements of components as well as of the system overall when prosodic information such as stress and pitch is added as an input feature.
Past work in related fields (Shriberg et al.1998; Shriberg et al.2000) suggests that particularly for ASR input, noticeable improvements might be achievable when such input is provided.
Although presegmentation of the input into topically coherent segments certainly is a useful step in summarization for any kind of texts (written or spoken), we have not addressed and discussed this issue in this article.
Finally, we think that there is more work needed in the area of automatically deriving discourse structures for spoken dialogues in unrestricted domains, even if the text spans covered might be only local (because of a lack of global discourse plans).
We believe that a summarizer, in addition to knowing about the interactively constructed and coherent pieces of information (such as in question-answer pairs), could make good use of such structured information and be better guided in making its selections for summary generation.
In addition, this discourse structure might aid modules that perform automatic anaphora detection and resolution.
8. Conclusions We have motivated, implemented, and evaluated an approach for automatically creating extract summaries for open-domain spoken dialogues in informal and formal genres of multiparty conversations.
Our dialogue summarization system DIASUMM 481 Computational Linguistics Volume 28, Number 4 uses trainable components to detect and remove speech disfluencies (making the output more readable and less noisy), to determine sentence boundaries (creating suitable text spans for summary generation), and to link cross-speaker information units (allowing for increased summary coherence).
We used a corpus of 23 dialogue excerpts from four different genres (80 topical segments, about 47,000 words) for system development and evaluation and the disfluencyannotated SWITCHBOARD corpus (LDC 1999b) for training of the dialogue-specific components.
Our corpus was annotated by six human coders for topical boundaries and relevant text spans for summaries.
Additionally, we had annotations made for disfluencies, sentence boundaries, question speech acts, and the corresponding answers to those question speech acts.
In a global system evaluation we compared the MMR-based sentence selection component with the DIASUMM system using all of its components discussed in this article.
The results showed that (1) both a baseline MMR system as well as DIASUMM create better summaries than a LEAD baseline (except for NEWSHOUR) and that (2) DIASUMM performs significantly better than the baseline MMR system for the informal dialogue corpora (CALLHOME and GROUP MEETINGS).
Acknowledgments We are grateful to Alex Waibel, Alon Lavie, Jaime Carbonell, Vibhu Mittal, Jade Goldstein, Klaus Ries, Lori Levin, and Marsal Gavald` for many discussions, a suggestions, and comments regarding this work.
We also want to commend the corpus annotators for their efforts.
Finally, we want to thank the four anonymous reviewers for their detailed feedback on a preliminary draft, which greatly helped improve this article.
This work was performed while the author was affiliated with the Language Technologies Institute at Carnegie Mellon University and was supported in part by grants from the U.S.
Department of Defense.
References Alexandersson, Jan and Peter Poller.
1998. Towards multilingual protocol generation for spontaneous speech dialogues.
In Proceedings of INLG-98, Niagara-on-the-Lake, Canada, August.
Aone, Chinatsu, Mary Ellen Okurowski, and James Gorlinsky.
1997. Trainable, scalable summarization using robust NLP and machine learning.
In ACL/EACL-97 Workshop on Intelligent and Scalable Text Summarization, Madrid.
Arons, Barry.
1994. Pitch-based emphasis detection for segmenting speech.
In Proceedings of ICSLP-94, pages 19311934.
Berger, Adam L.
and Vibhu O.
Mittal. 2000.
OCELOT: A system for summarizing Web pages.
In Proceedings of the 23rd ACM-SIGIR Conference.
Bett, Michael, Ralph Gross, Hua Yu, Xiaojin Zhu, Yue Pan, Jie Yang, and Alex Waibel.
2000. Multimodal meeting tracker.
In Proceedings of the Conference on Content-Based Multimedia Information Access (RIAO-2000), Paris, April.
Brill, Eric.
1994. Some advances in transformation-based part of speech tagging.
In Proceedings of AAAI-94.
Carbonell, Jaime, Yibing Geng, and Jade Goldstein.
1997. Automated query-relevant summarization and diversity-based reranking.
In Proceedings of the IJCAI-97 Workshop on AI and Digital Libraries, Nagoya, Japan.
Carbonell, Jaime and Jade Goldstein.
1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries.
In Proceedings of the 21st ACM-SIGIR International Conference on Research and Development in Information Retrieval, Melbourne, Australia.
Carletta, Jean, Amy Isard, Stephen Isard, Jacqueline C.
Kowtko, Gwyneth Doherty-Sneddon, and Anne H.
Anderson. 1997.
The reliability of a dialogue structure coding scheme.
Computational Linguistics, 23(1):1331.
Chen, Francine R.
and Margaret Withgott.
1992. The use of emphasis to automatically summarize a spoken discourse.
In Proceedings of ICASSP-92, pages 229332.
Cohen, Jacob.
1960. A coefficient of agreement for nominal scales.
Educational and Psychological Measurement, 20(1):3746.
Davies, Mark and Joseph L.
Fleiss. 1982.
Measuring agreement for multinomial data.
Biometrics, 38:10471051, December.
Garofolo, John S., Ellen M.
Voorhees, Cedric G.
P. Auzanne, and Vincent M.
Stanford. Zechner Automatic Summarization of Dialogues 1999.
Spoken document retrieval: 1998 evaluation and investigation of new metrics.
In Proceedings of the ESCA Workshop: Accessing Information in Spoken Audio, pages 17, Cambridge, UK, April.
Garofolo, John S., Ellen M.
Voorhees, Vincent M.
Stanford, and Karen Sparck Jones.
1997. TREC-6 1997 spoken document retrieval track overview and results.
In Proceedings of the 1997 TREC-6 Conference, pages 8391, Gaithersburg, MD, November.
Gavalda, Marsal, Klaus Zechner, and Gregory Aist.
1997. High performance segmentation of spontaneous speech using part of speech and trigger word information.
In Proceedings of the fifth ANLP Conference, Washington, DC, pages 1215.
Godfrey, J.
J., E.
C. Holliman, and J.
McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for research and development.
In Proceedings of ICASSP-92, volume 1, pages 517520.
Grosz, Barbara J.
and Candace L.
Sidner. 1986.
Attention, intentions, and the structure of discourse.
Computational Linguistics, 12(3):175204.
Hearst, Marti A.
1997. TextTiling: Segmenting text into multi-paragraph subtopic passages.
Computational Linguistics, 23(1):3364.
Heeman, Peter A.
and James F.
Allen. 1999.
Speech repairs, intonational phrases, and discourse markers: Modeling speakers' utterances in spoken dialogue.
Computational Linguistics, 25(4):527571.
Hirschberg, Julia, Steve Whittaker, Don Hindle, Fernando Pereira, and Amit Singhal.
1999. Finding information in audio: A new paradigm for audio browsing/retrieval.
In Proceedings of the ESCA Workshop: Accessing Information in Spoken Audio, pages 117122, Cambridge, UK, April.
Hori, Chiori and Sadaoki Furui.
2000. Automatic speech summarization based on word significance and linguistic likelihood.
In Proceedings of ICASSP-00, pages 15791582, Istanbul, Turkey, June.
Jurafsky, Daniel, Rebecca Bates, Noah Coccaro, Rachel Martin, Marie Meteer, Klaus Ries, Elizabeth Shriberg, Andreas Stolcke, Paul Taylor, and Carol Van Ess-Dykema.
1998. SwitchBoard discourse language modeling project: Final report.
Research Note 30, Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD.
Kameyama, Megumi, and I.
Arima. 1994.
Coping with aboutness complexity in information extraction from spoken dialogues.
In Proceedings of ICSLP 94, pages 8790, Yokohama, Japan.
Kameyama, Megumi, Goh Kawai, and Isao Arima.
1996. A real-time system for summarizing human-human spontaneous spoken dialogues.
In Proceedings of ICSLP-96, pages 681684.
Knight, Kevin and Daniel Marcu.
2000. Statistics-based summarization--Step one: Sentence compression.
In Proceedings of the 17th National Conference of the AAAI.
Koumpis, Konstantinos and Steve Renals.
2000. Transcription and summarization of voicemail speech.
In Proceedings of ICSLP-00, pages 688691, Beijing, China, October.
Krippendorff, Klaus.
1980. Content Analysis.
Sage, Beverly Hills, CA.
Kupiec, J., J.
Pedersen, and F.
Chen. 1995.
A trainable document summarizer.
In Proceedings of the 18th ACM-SIGIR Conference, pages 6873.
Lavie, Alon, Alex Waibel, Lori Levin, Michael Finke, Donna Gates, Marsal Gavalda, Torsten Zeppenfeld, and Puming Zhan.
1997. Janus III: Speech-to-speech translation in multiple languages.
In IEEE International Conference on Acoustics, Speech and Signal Processing, Munich.
Levin, Lori, Klaus Ries, Ann Thyme-Gobbel, and Alon Lavie.
1999. Tagging of speech acts and dialogue games in Spanish call home.
In Proceedings of the ACL-99 Workshop on Discourse Tagging, College Park, MD.
Linguistic Data Consortium (LDC).
1996. CallHome and CallFriend LVCSR databases.
Linguistic Data Consortium (LDC).
1999a. Addendum to the part-of-speech tagging guidelines for the Penn Treebank project (Modifications for the SwitchBoard corpus).
LDC CD-ROM LDC99T42.
Linguistic Data Consortium (LDC).
1999b. Treebank-3: Databases of disfluency annotated Switchboard transcripts.
LDC CD-ROM LDC99T42.
Mani, Inderjeet, David House, Gary Klein, Lynette Hirschman, Leo Obrst, Therese Firmin, Michael Chrzanowski, and Beth Sundheim.
1998. The TIPSTER SUMMAC text summarization evaluation.
Technical Report MTR 98W0000138, Mitre Corporation, October 1998.
Mani, Inderjeet and Mark T.
Maybury, editors.
1999. Advances in Automatic Text Summarization.
MIT Press, Cambridge.
Marcu, Daniel.
1999. Discourse trees are good indicators of importance in text.
In I.
Mani and M.
T. Maybury, editors, Computational Linguistics Volume 28, Number 4 Advances in Automatic Text Summarization.
MIT Press, Cambridge, pages 123136.
Meteer, Marie, Ann Taylor, Robert MacIntyre, and Rukmini Iyer.
1995. Dysfluency annotation stylebook for the Switchboard corpus.
Linguistic Data Consortium (LDC) CD-ROM LDC99T42.
Miike, Seiji, Etuso Itoh, Kenji Onon, and Kazuo Sumita.
1994. A full-text retrieval system with a dynamic abstract generation function.
In Proceedings of the 17th ACM-SIGIR Conference, pages 318 327.
Nakatani, Christine H.
and Julia Hirschberg.
1994. A corpus-based study of repair cues in spontaneous speech.
Journal of the Acoustic Society of America, 95(3):16031616.
Passonneau, Rebecca J.
and Diane J.
Litman. 1997.
Discourse segmentation by human and automated means.
Computational Linguistics, 23(1):103139.
Quinlan, J.
Ross. 1992.
C4.5: Programs for Machine Learning.
Morgan Kaufmann, San Mateo, CA.
Rath, G.
J., A.
Resnick, and T.
R. Savage.
1961. The formation of abstracts by the selection of sentences.
American Documentation, 12(2):139143.
Reimer, U.
and U.
Hahn. 1988.
Text condensation as knowledge base abstraction.
In Proceedings of the fourth Conference on Artificial Intelligence Applications, pages 338344, San Diego.
Reithinger, Norbert, Michael Kipp, Ralf Engel, and Jan Alexandersson.
2000. Summarizing multilingual spoken negotiation dialogues.
In Proceedings of the 38th Conference of the Association for Computational Linguistics, pages 310317, Hong Kong, China, October.
Ries, Klaus, Lori Levin, Liza Valle, Alon Lavie, and Alex Waibel.
2000. Shallow discourse genre annotation in CALLHOME Spanish.
In Proceedings of the Second Conference on Language Resources and Evaluation (LREC-2000), Athens, May/June.
Rose, Ralph Leon.
1998. The Communicative Value of Filled Pauses in Spontaneous Speech.
Ph.D. thesis, University of Birmingham, Birmingham, UK.
Salton, Gerard, editor.
1971. The SMART Retrieval System--Experiments in Automatic Text Processing.
Prentice Hall, Englewood Cliffs, NJ.
Santorini, Beatrice.
1990. Part-of-Speech Tagging guidelines for the Penn Treebank project.
Linguistic Data Consortium (LDC) CD-ROM LDC99T42.
Shriberg, Elizabeth E . 1994.
Preliminaries to a Theory of Speech Disfluencies.
Ph.D. thesis, University of Berkeley, Berkeley.
Shriberg, Elizabeth, Rebecca Bates, Andreas Stolcke, Paul Taylor, Daniel Jurafsky, Klaus Ries, Noah Coccaro, Rachel Martin, Marie Meteer, and Carol Van Ess-Dykema.
1998. Can prosody aid the automatic classification of dialog acts in conversational speech?
Language and Speech, 41(34):439487.
Shriberg, Elizabeth, Andreas Stolcke, Dilek Hakkani-Tur, and Gokhan Tur.
2000. Prosody-based automatic segmentation of speech into sentences and topics.
Speech Communication, 32(12):127154.
Stifelman, Lisa J . 1995.
A discourse analysis approach to structured speech.
In AAAI-95 Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, Stanford, March.
Stolcke, Andreas, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema, and Marie Meteer.
2000. Dialogue act modeling for automatic tagging and recognition of conversational speech.
Computational Linguistics, 26(3):339373.
Stolcke, Andreas and Elizabeth Shriberg.
1996. Automatic linguistic segmentation of conversational speech.
In Proceedings of ICSLP-96, pages 10051008.
Stolcke, Andreas, Elizabeth Shriberg, Rebecca Bates, Mari Ostendorf, Dilek Hakkani, Madeleine Plauche, Gokhan Tur, and Yu Lu.
1998. Automatic  detection of sentence boundaries and disfluencies based on recognized words.
In Proceedings of ICSLP-98, volume 5, pages 22472250, Sydney, December.
Teufel, Simone and Marc Moens.
1997. Sentence extraction as a classification task.
In ACL/EACL-97 Workshop on Intelligent and Scalable Text Summarization, Madrid.
Valenza, Robin, Tony Robinson, Marianne Hickey, and Roger Tucker.
1999. Summarisation of spoken audio through information extraction.
In Proceedings of the ESCA Workshop: Accessing Information in Spoken Audio, pages 111116, Cambridge, UK, April.
Wahlster, Wolfgang.
1993. Verbmobil--Translation of face-to-face dialogs.
In Proceedings of MT Summit IV, Kobe, Japan.
Waibel, Alex, Michael Bett, and Michael Finke.
1998. Meeting browser: Tracking and summarizing meetings.
In Proceedings of the DARPA Broadcast News Workshop.
Ward, Wayne.
1991. Understanding spontaneous speech: The PHOENIX system.
In Proceedings of ICASSP-91, Zechner Automatic Summarization of Dialogues pages 365367.
Whittaker, Steve, Julia Hirschberg, John Choi, Don Hindle, Fernando Pereira, and Amit Singhal.
1999. SCAN: Designing and evaluating user interfaces to support retrieval from speech archives.
In Proceedings of the 22nd ACM-SIGIR International Conference on Research and Development in Information Retrieval, pages 2633, Berkeley, August.
Zechner, Klaus and Alon Lavie.
2001. Increasing the coherence of spoken dialogue summaries by cross-speaker information linking.
In Proceedings of the NAACL-01 Workshop on Automatic Summarization, pages 2231, Pittsburgh, June.
Zechner, Klaus and Alex Waibel.
2000a. DIASUMM: Flexible summarization of spontaneous dialogues in unrestricted domains.
In Proceedings of COLING-2000, pages 968974, Saarbrucken, Germany,  July/August.
Zechner, Klaus and Alex Waibel.
2000b. Minimizing word error rate in textual summaries of spoken language.
In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-2000), pages 186193, Seattle, April/May .
From Conceptual Time to Linguistic Time Michel Gagnon* Machina Sapiens Guy Lapalme t Universit4 de Montr4al In this paper, we present a method for generating French texts conveying temporal information that integrates Discourse Representation Theory (DRT) and Systemic Grammar Theory.
DRT is used to represent temporal information and an intermediate semantic level for the temporal localization expressed by temporal adverbial phrases and verb phrases.
This representation is then translated into a syntactic form using Systemic Grammar Theory.
We have implemented this method in a working prototype called Prdtexte.
1. Introduction In speaker-generated texts, reference is made to facts taking place in time.
To use the same kind of references in automatically generated text, the mechanisms that govern the expression of temporal concepts must be identified.
There is no simple or direct mapping between conceptual time, as it is perceived in the real world, and linguistic time, which refers to the way time is formulated in language.
There may be different ways to present the same temporal concept in a text, and a single linguistic marker can be used to convey different temporal meanings.
For example, the discourse below (Discourse 1) is a text generated by Pr4texte, a system we developed for implementing the expression of temporal localization in French texts 1.
It is a slightly modified version of an example used by Bras (1990) for the extraction of temporal information in text analysis.
The sentences report occurrences that are facts taking place in time.
We have inserted labels in parentheses to distinguish the twelve occurrences reported in the text.
Hier l'avion a effectu6 un vol (ol).
A 8h00 il a quitt6 Paris (02).
Quand il a survol6 Barcelone (o3), le r6acteur fonctionnait (o4).
~, 10h15, un voyant a clignot6 (o5).
Auparavant, il s'6tait allure6 (o6).
Puis il s'6tait 6teint (o7).
Pendant 35 minutes, l'avion a survol6 lamer (o8).
Puis il a atteint la c6te (o9).
Jusqu'a 10h50, il a survol6 l'Alg6rie (o10).
A llh30 il 6tait sur la piste (o11).
A ce moment-la le r6acteur a explos6 (o12).
Yesterday the plane made a flight (ol).
At 8:00 A.M. it left Paris (o2).
When it flew over Barcelona (o3), the engine was working (o4).
At 10:15, a warning light flashed (o5).
Previously it had come on (o6).
Then it had gone out (o7).
For 35 minutes the plane flew over the sea (o8).
Then it reached the coast (o9).
Until 10:50 it flew over Algeria (olo).
At 11:30 it was on the landing runway (o11).
At this moment the engine exploded (o12).
Discourse 1 * 3535 Queen-Mary, Bur.
420, Montr6al (Quebec), Canada H3V 1H8, Tel: (514) 733-3959.
E-maih gagnon@iro.umontreal.ca.
This article was written while the author was at "Laboratoire Langue, Raisonnement et Calcul" of IRIT, Toulouse, France.
JD6partement cl'informatique et de recherche op6rationnelle, C.
P. 6128, Succ.
Centre-Ville, Montr6al (Qu6bec), Canada H3C 3J7.
E-mail: lapalme@iro.umontreal.ca.
1 The
French text shown on the left in Discourse 1 was generated by Pr6texte; we give an English translation on the right.
@ 1996 Association for Computational Linguistics Computational Linguistics Volume 22, Number 1 In Discourse 1, we find two types of temporal markers: verb tense and what we call adverbials of temporal location (ATL).
An ATL is an adjunct, such as yesterday, until 10:50, or when it flew over Barcelona, that provides information about the temporal localization of an occurrence or its duration, or both at the same time.
For verb tense, we distinguish different ways of indicating localization in the past.
Three French verb tenses can be used: pass~ composG imparfait, and plus-que-parfait; their closest equivalents in English are the simple past, past progressive, and past perfect.
The passf compos~ il a survold 'it flew over' presents the occurrence as an event and localizes it in relation to the time of speech.
With the plus-que-parfait il s'dtait allum~ 'it had come on', the occurrence is also presented as an event, but localized in relation to a perspective point other than speech time.
The imparfait le r~acteur fonctionnait 'the engine was working' presents the occurrence as being in progress.
For present and future tenses there are fewer options than for past tense, but more than one form is available for theses tenses as well.
For ATLs, temporal localization can be achieved in many ways; for example, in relation to the time of speech (hier 'yesterday'), by designating an absolute temporal location (~ 8hO0 'at 8:00 A.M.'), or in relation to another fact (puis 'then', ~ ce moment-ld 'at this moment', quand il a survold Barcelone 'when it flew over Barcelona').
To this variety in the semantics of localization we must add the variety of syntactic forms.
Localization can be expressed by an adverb (puis 'then'), a prepositional phrase (jusqu'd 10h50 'until 10:50'), a nominal phrase (le lendemain 'the day after') or a subordinate clause (quand il a survold Barcelone 'when it flew over Barcelona').
No text generator has yet been developed to solve the problem of the expression of time.
The ones that have tackled this problem have focused on the production of verb tenses, without solving the choice of temporal adverbs.
The work presented in this paper addresses the problem of generating the elements that convey temporal localization in French, including both verbs and temporal adverbs.
In a previous paper (Gagnon and Lapalme 1992), we proposed a method of integrating the expression of temporal concepts into the text-generation process.
In particular, we showed how to produce different types of text in French from a single representation of events.
Unfortunately, the method governing the planning process was too determined by temporal concepts, so it was difficult to link this planning process with other frameworks, such as the schema proposed by McKeown (1985) or Rhetorical Structure Theory (RST) (Mann 1991; Hovy 1991).
As we were not really successful in integrating the expression of time in French into the text-generation process, we decided to pursue our research with a different perspective.
We designed a system covering many of the possibilities of expressing time in French, our hypothesis being that the achievement of this task would facilitate the design of a text-planning process.
We believed it would be easier to organize the structure of the discourse with a better understanding of the way temporal information can be expressed by adverbs and verb tenses.
We started from the work of Bras (1990), who proposes a method of extracting the temporal structure of a text, according to Discourse Representation Theory (Kamp 1981), that relies on an analysis of adverbials of temporal location made by Molin6s (1990).
To implement the production of ATLs and verb tenses, we have chosen Systemic Grammar (Halliday 1985; Berry 1975, 1976), which formulates the syntactic structure of a sentence as the result of a sequence of semantic choices.
We developed a grammar interpreter inspired by Nigel (Matthiessen and Bateman 1991), but departing from it in many respects; in particular, our representation of the production of verb tenses and adverbs is quite different.
In this paper, we discuss the elements required to produce a text such as Dis92 Gagnon and Lapalme From Conceptual Time to Linguistic Time / DEEP / GENERATIONN f SURFACE GENERATION", Conceptual representation Discourse representation t Semantic representation 1 Syntactic representation Occurrences as primitive concepts Temporal relations The tempolral Jocaligafion is represented as an overlapmg relation Sellmentafion of the conceptual ret~resentaaon Structured information Rhetorical relations Linearization Choice of aspect Identification of temporal markers Temporal adverbial Verb tense Figure 1 The global process.
course 1.
The process starts from a conceptual representation that encodes the facts to be reported in the text, associated with their position in time.
The information at this objective conceptual level must be translated into a semantic representation where the facts are presented according to a subjective perspective.
The semantic representation is then used to produce the text.
We have concentrated our attention on this last stage, but we cannot avoid the problem of determining how the representation used at this level is obtained from previous levels.
In the following sections, we describe the two stages of the text-generation process.
2. The Global Process It is generally accepted that the generation process requires at least two parts.
The first part, deep generation, is a planning process in which the content and the overall structure of the text are established.
In the second part, surface generation, the words and the syntactic structure of the text are chosen.
Figure 1 summarizes our view of the global process, starting from a conceptual representation that contains occurrences and relations between them.
The fact that an occurrence takes place at a certain time is expressed by an overlapping relation between this occurrence and the object representing this time.
The deep generation process is decomposed into two steps.
In the first step, the conceptual representation is segmented and structured to build a discourse representation.
In our discourse representation, which uses Segmented Discourse Representa93 Computational Linguistics Volume 22, Number 1 tion Theory (SDRT) (Asher 1993), the information is cut into smaller segments each of which contains the information to be expressed by a single sentence.
The structure linking these segments relies on a set of rhetorical relations.
In the second step of the deep generation process, the discourse representation is traversed and, for each segment, rules are applied to identify the feature values needed to translate it into a sentence.
We thus obtain a linear structure in which each element is a set of features that determine the syntactic form of the sentence.
In the surface generation process, the information in the semantic representation is used to select the appropriate syntactic structure for the expression of time: an adverbial of temporal location (ATL) or a verb phrase (VP), or both.
3. The Deep Generation Process Although our work focuses on surface generation, we cannot ignore the issue of deep generation, because the nature of the semantic representation is determined not only by the syntax of the language, but also by the temporal concepts available.
Therefore, in this section, we first present the conceptual representation that induces the semantic representation used by our generator.
We then explain the intermediate discourse level.
We do not know yet in detail how to produce the semantic representation from the conceptual representation, but we do have an idea of what information each level of representation must contain and what choices must be made at each stage of the process.
3.1 Conceptual
Representation To represent temporal concepts in Pr6texte, we chose the principles of Discourse Representation Theory (DRT), which offers one of the most interesting explanations of how temporal notions are conveyed by a text.
DRT was developed to deal with specific problems of discourse understanding: in particular, problems with anaphora and the differences between some verb tenses, with respect to temporal localization.
Our goal is not to show how this theory can be used for generation but rather to use its principles as a convenient formalism for the representation of time.
In DRT, a text is associated with a Discourse Representation Structure (DRS) that is updated incrementally by the processing of each sentence.
A DRS is a structure containing a set of entities and a set of conditions on these entities.
There are different types of entities in DRT: a temporal fact can either be presented as an event (having a punctual aspect), or as a situation (having a certain extent in time, but considered from an internal perspective at a given moment in time); a temporal constant that designates a segment of the temporal axis; entities that participate in the events or situations.
In Pr6texte, conceptual knowledge is represented as a DRS, which we adapted slightly for text generation.
We do not distinguish between events and situations in the conceptual representation, because we want this level to remain independent of the language.
Furthermore, we feel that this distinction should not be encoded at the conceptual level, rather the generation system should choose among these possibilities.
Therefore, at the conceptual level we use the concept of occurrence for either an event or a situation.
94 Gagnon and Lapalme From Conceptual Time to Linguistic Time n tl t2 t3 t4 01 02 03 04 a l p b r plane(a) flight(l) engine(r) city(p) Paris(p) city(b) o20 t2 Barcelona(b) ol < n o1: make(a, 1) o3 < n o 2" leave(a,p) o~ C tl o3: flightover(a, b) o4 < n 04: work(r) o3 C 04 n c t3 03 C) t4 tl = Sept.
10 1992 t2 = Sept.
10 1992 at 8:10 am t3 = Sept.
11 1992 t4 = Sept.
10 1992 at 9:00 am 02 <n Figure 2 Conceptual representation for the first three sentences of Discourse 1.
There are essentially two ways of considering time or, to be more precise, the notion of temporal location: either temporal location is determined using a preexisting time scale, or it is deduced from the occurrence.
Following Kamp (1981), we think that the second possibility, in which temporal location is a relative concept, is more suitable for natural text processing.
Treating occurrences as entities, rather than making them subordinate to temporal intervals or points, has been proposed by Davidson (1967).
An occurrence may be represented in relation to another temporal object, without any reference to its own location in time.
This approach eases the representation of underspecified temporal localizers--an important point for our semantics.
For further discussion of the advantages of taking occurrences as primitives, see (Bras 1990; Kamp 1979, 1981).
In the conceptual representation, we find four types of information:  the description of occurrences;  the description of participants in the occurrences;  the description of temporal localizers, which are called temporal constants (they usually refer to time periods of the calendar);  temporal relations between occurrences and temporal localizers: The relation < represents temporal precedence.
The overlap relation C) indicates that two temporal objects are somehow simultaneous.
Thus, in our representation, "Y happens at time X" is represented by "Y temporally overlaps X".
The relation C expresses the fact that the temporal extent of a temporal object is a subset of the temporal extent of another object.
Figure 2 shows the part of the DRS used to generate the first three sentences of Discourse 1.
It contains five temporal constants: n, tl, t2, t3 and t4.
It is not clear how 95 Computational Linguistics Volume 22, Number 1 these temporal constants are to be described in DRT, so we have proposed elsewhere a formalization of the type of objects designated by these constants (Gagnon and Bras 1994).
In this article, we give only an English description of them: n represents speech time, which, in Figure 2, is included in the time represented by t3 (September 11 1992).
Four occurrences are represented: ol, o2, o3, and o4, all of which take place before n.
Not all temporal relations in the DRS need to be given as input because many relations can be inferred using three kinds of knowledge:  The representation of conventional time to identify a specific period in time; this representation relies on a structure of conventional time, together with reasoning mechanisms to deduce temporal relations (see Gagnon and Bras \[1994\] for an implementation of such a structure).
For example, from this knowledge we can deduce that September 10 must be before September 11, which would be represented as tl < t3.
Similarly, we can deduce t2 C tl, t4 C tl, and t2 < t4.
 World knowledge about the occurrences: knowing that o2, 03, and o4 are part of Ol implies that they are all temporally included in it.
 A reasoning mechanism on the temporal relations, using a set of axioms, such as: Vx, y(xOyvx < yvy < x) Vu, v,x,y(uOxAvOyAx < y~ uOvVu < v) The first axiom states that for any two times, either they overlap or one precedes the other.
The second axiom states that if two other times u, v overlap two times x, y that are in a precedence relation, either u overlaps v, or it precedes it.
So in Figure 2, from the relations 02 O t2, 03 O t4, and t2 < t4, we can infer o2 < 03 V o20 o3.
From world knowledge, we can infer that o2 and 03 cannot overlap (leaving of Paris cannot overlap flying over Barcelona); Therefore, we conclude that 02 ~ 03.
3.2 The
Discourse Representation To generate a text from an input such as Figure 2, we must choose a discourse structure that segments the message into sentences.
Figure 3 illustrates one discourse representation, inspired by the Segmented Discourse Representation Theory (SDRT) proposed by Asher (1993), which extends DRT by adding rhetorical relations such as those found in RST (Mann and Thompson 1987).
A discourse structure that contains the same information as in Figure 2, except that it has been segmented, we call a Segmented Discourse Representation Structure (or SDRS).
The top-level DRS contains three small DRSs that are linked by rhetorical relations: each DRS corresponds to a sentence.
In addition to these three small DRSs, the top-level DRS contains the global text information: the description of participants and the description of speech time.
We do not yet produce this discourse structure, but we are working on this problem, using the results of researchers who have applied SDRT to the analysis process (Lascarides and Asher 1993; Bras and Asher 1994).
In the discourse structure of Figure 3, one sentence is elaborated by two other sentences that constitute a narration.
96 Gagnon and Lapalme From Conceptual Time to Linguistic Time plane(a) fight(l) city(p) Paris(p) n t3 a l p b r Ol tl o1: make(a, 1) tl = Sept.
10 1992 Ol Kn Ol C tl Elaboration Barcelona(b) engine(r) t3 = Sept.
11 1992 n C t3 city(b) 02 t2 o2: leave(a, p) t2 = Sept.
10 1992 at 8:10 am 02 Kn 02 0 t2 Narration 03 04 t4 03: flightover(a,b) 04: work(r) O3 (n 04 <n t4 = Sept.
10 1992 at 9:00 am 03 C 04 03 0 t4 Figure 3 Discourse representation for the first three sentences of Discourse 1.
3.3 Semantic
Representation The discourse structure is then translated into a semantic representation of the form $1, $2,..., Sn where Si designates the i th element of a semantic representation S.
Translation of the SDRS is obtained by a depth-first traversal of the DRSs it contains.
For each DRS, we establish its corresponding feature structure in the semantic representation.
Figure 4 is a semantic representation produced from the SDRS of Figure 3.
Each structure contains five features.
The feature message refers to the occurrence that must be reported by the sentence, and specifies its aspect.
We distinguish, as Kamp does, two aspects that can be used to present an occurrence: event or situation.
Situations can be open or resulting.
A situation is open when the speaker/hearer is located at a time within an occurrence.
A resulting situation is the state following the termination of an occurrence.
In French, the event aspect for a past occurrence results in the use of the pass~ composd (simple past in English).
The imparfait and the plus-que-parfait correspond to open and resulting situations (the closest tenses in English are the past progressive and the past perfect).
In the first two elements of Figure 4, the occurrence is presented as an event, whereas in the last one it is presented as an open situation.
Among the four occurrences contained in the DRS, only three of them constitute the main "message" of the text: ol, o2, and o4.
The four other features in a structure Si give the value of four temporal markers that express the localization of the occurrence.
These markers correspond to the four markers proposed by Kamp and Rohrer (Bras 1990) for the analysis of texts, which we have adapted for text generation.
They can be seen as an extension of Reichenbach's markers (1947).
Essentially, the values of these four features depend on two data:  the DRSs to which the visited DRS is attached in the discourse structure, and  the rhetorical relations.
97 Computational Linguistics Volume 22, Number 1 Figure 4 message = event(ol) N=n REF = nil PERSP = n LOC = tl S1 message = event(o2) N=n R =Ol PERSP = n LOG = t2 $2 message : open_situation ( o 4 ) N=n R=o2 PERSP = n LOC = o3 S3 Semantic representation for the first three sentences of Discourse 1.
O1 :....
~. Elaboration 102 ....
LNa a onL04 ....
INaatinIO5 ....
I a atin ~.
Flashback h 08: ....
I 06: ....
\[ Narration ~, 07: ....
\] Figure 5 Discourse representation for the first seven sentences of Discourse 1.
The marker N represents the time of speech.
This marker is constant in our example, but it could be locally altered in the discourse, in indirect speech for example.
We did not study such cases, but we think that the marker N would be required to deal with them.
Perspective point PERSP refers to an instant from which the occurrence must be considered.
Usually it is the same as the time of speech, but in some cases, such as a flashback, it has a different value.
In Discourse 1, there is one such case.
The fifth and sixth sentences (where occurrences o6 and o7 are reported) constitute a flashback: the perspective point is the occurrence of the fourth sentence (os).
In discourse structure, the flashback is represented with a rhetorical structure.
Consider for example the discourse structure for the first seven sentences of Discourse 1 as sketched in Figure 5.
For the translation of the two DRSs containing 06 and o7, the value of PERSP will be the occurrence o5, since the DRS containing this occurrence dominates the two others with the relation flashback.
For the next DRS, the one containing os, the perspective point will be reset to the value it had before entering the flashback, that is, the value when the DRS of o6 was considered.
The value of PERSP is used for the choice of verb tense.
In Discourse 1, the flashback results in the choice of the plus-que-parfait.
LOC represents the temporal location of the occurrence reported.
If this occurrence overlaps another temporal object, this object may be used as a value for LOC.
In 98 Gagnon and Lapalme From Conceptual Time to Linguistic Time Figure 4, the values of LOC show that tl and t2 are used to localize the first two occurrences.
In the third sentence, the situation corresponding to o4 is presented at the instant where o3 takes place.
If no other temporal object overlaps the one that constitutes the message, the temporal location represented by LOC can be defined in relation to another temporal object.
We will see examples of this in the next section.
LOC represents the information to be expressed by an ATL in the sentence and does not necessarily have a value, because a sentence may not contain an ATL.
In a text, when we want to express a succession of occurrences, we need a way to memorize the occurrence that is used as a reference for the localization of the next one.
This is exactly the role of the marker REF.
The values of REF are used to represent the progression of time in the discourse.
Each time a sentence expresses a new temporal location (which can be an occurrence or a temporal constant), the value of REF is updated to this new value, and the temporal localization in the following sentence may be achieved in relation to this reference.
The following rules are used for identifying the value of REF: 1.
Identify the S-antecedent, the DRS to which the current DRS is attached in the discourse structure, and Sa, the feature structure associated with this DRS in the semantic representation.
2. If the occurrence reported as message in Sa is presented as a situation, it cannot be used as a reference point, since a situation cannot state a progression in time.
3. If LOC has a value that is temporally more precise than the occurrence in the message, REF will take on its value, otherwise REF is bound to the occurrence in the message, if this occurrence is not presented as a situation.
4. If LOC has no value and the occurrence in the message is a situation, the antecedent sentence does not state a progress in time.
Therefore, REF takes the same value as in Sa.
In Figure 4, the context for the first sentence is empty, so no value is given to REE For the second and third sentences, the value of REF is the event presented in the previous sentence.
The occurrence in the third sentence is expressed as a situation, so it cannot be the reference for the fourth sentence (not shown in the figure).
Consequently, REF for the fourth sentence takes the value of LOC in the structure of the third sentence: t4.
We will see in Section 5 how the value of REF is used to produce the temporal adverb.
The choice of aspect in building the semantic structure is achieved by taking into account pragmatic information and the interaction with other choices, such as the type of temporal localizer.
Currently, we first identify the localizer that constrains the selection of aspect, but more study is needed to clarify their interaction.
If an occurrence is presented as a situation, the temporal localizer must be a time included in it; but an event aspect cannot be combined with a localizer.
In Figure 4, the occurrence of $1 must be presented as an event, since the localizer tl includes the occurrence.
In $2, the occurrence is also an event, even if the localizer overlaps the occurrence: the overlapping relation does not prevent the existence of an inclusion relation.
If an inclusion relation between t2 and o2 could be deduced, then the situation aspect could be chosen.
In $3, the localizer is included in the occurrence of the message, so the situation aspect is selected.
99 Computational Linguistics Volume 22, Number 1 message = event (oi) N=n R = nil P=n L = ch message = event(o2) N=n ' R =01 P=n L = ct2 Figure 6 An alternative semantic representation.
message = event(03) N=n ' R =02 P=n L = ct4 message = open_situation ( o 4 ) N=n ' R =03 P=n L = ct4 ) The semantic representation given in Figure 4 is not unique.
Figure 6 shows another semantic representation built from the DRS of Figure 2.
It contains a fourth sentence.
The main difference from the previous representation is that 03, instead of acting as a localizer for o4, is the message of a sentence; t3, referring to a moment located two hours after t2, localizes o3.
Therefore, instead of the third sentence of Discourse 1, we would obtain these two sentences: Deux heures plus tard, il a survol6 Two hours later, it flew over Barcelona (o'3).
At Barcelone (o~).
Ace moment-la, le r6acteur this moment, the engine was working (o'4).
foncfionnait (o~).
Once the semantic representation is produced, the adverbial or temporal location (ATL) and the verb phrase (VP) can be generated independently.
The syntactic form of the verb phrase is determined by the combination of the following information:  temporal relation between localizer LOC and speech time N;  temporal relation between localizer LOC and perspective point PERSP;  aspect of the occurrence.
The choice of the syntactic structure of the ATL depends on the value of LOC, which may refer to N or REF.
The interaction of temporal information conveyed by verb tense and adverbs is taken into account in the process of translation from the conceptual representation to the semantic level, where the choices of aspect, perspective point PERSP and localizer LOC are made.
We still have not entirely solved the problem of choosing among all semantic representations that can be built from a DRS.
In the current implementation of Pr6texte, we have identified a set of rules to produce the semantic representation.
In particular, these rules insure that the values of the four temporal markers are coherent with the aspect chosen to present the occurrences.
What remains to be done, essentially, is to identify the knowledge that governs these rules causing them to select a particular semantic representation.
3.4 Representation
of Localization We have argued in the previous section that four temporal markers are needed to express the temporal location of an occurrence.
In this section, we discuss the marker LOC, the localizer that provides information about the location in time of the occurrence using another entity.
The localizer is usually a temporal constant, but it can also be another occurrence, whose approximate location in time is already known.
An ATL can convey localizers of two types: in the first type, a localizer directly identities the temporal zone of an occurrence using another temporal object that overlaps it; in the second type the temporal zone of an occurrence is conveyed in relation 100 Gagnon and Lapalme From Conceptual Time to Linguistic Time to another localizer.
In Figure 4, all occurrences are localized directly.
Occurrence 04 is localized directly by another occurrence, whereas 01 and o2 are localized directly by a temporal constant.
For $1 and $2, the values for LOC are simply constants tl and t2.
The same temporal localization can usually be expressed in many ways, and we must also specify how these constants are worded.
For example, in Discourse 1, T1 has been translated into hier 'yesterday' but it could also have been translated into le 10 septembre 1992 'September 10th 1992', or mercredi dernier 'last Wednesday'.
Thus, the value of LOC in the semantic representation determines the expression of the localization, giving rise to three main problems:  how to represent the temporal constants in the conceptual representation;  how to determine the link between these conceptual temporal constants and their semantic representation, which specifies how they are to be expressed in the text; and  how to implement the selection mechanism, which relies on pragmatic and stylistic information to choose between the many different ways of expressing the same temporal localization.
In Gagnon and Bras (1994), we gave a solution to the first two problems but the last one still remains to be solved.
Here, we discuss only the second problem: the semantic expression of localization.
First, a few words about temporal context: usually, temporal localizers may be understood only in reference to some time in the context.
For example, in on April 15th, it is assumed that it is clear which year this time is part of.
Thus, we take for granted that all expressions of temporal localization are made in relation to such a contextual time.
Let ti be a temporal constant, taken from the conceptual representation, which is to be used as a localizer in the semantic representation, and tcont the contextual time.
The expression in the semantic representation is based on a term of the following form: \[ti, Type, Naming\] The first argument is the identifier of the constant in the conceptual representation.
The second argument is the type of the temporal localizer (day, month, year, etc.).
Thus, tcont may be decomposed into times of type Type, of which t i is one.
The last argument names the time referred to by the localizer ti.
There is exactly one time in the "real world" that corresponds to the temporal constant ti.
We call it objective time.
We use the notation t* to represent the objective time that corresponds to a localizer ti.
For example, the expression for the temporal localizer en avril 'in April', would be something like this (here t67 is the corresponding temporal constant in the conceptual representation): \[t67, mort th, april\] The naming april is not the syntactic word "April" but an internal keyword that helps distinguish between the time referred to and the other months of the contextual year.
An important distinction is made between a temporal constant ti and a objective time t*.
The constant ti pertains to the way a temporal location is expressed in the discourse, whereas t~ can be considered the corresponding portion of time in the real world.
More than one temporal constant may correspond to a single objective time.
101 Computational Linguistics Volume 22, Number 1 Table 1 Relative localizers.
Localizer Description inclin(\[ti, Ti, Ni\],\[tj, Tj, Nj\]) incl(\[ti, Ti, Ni\],\[tj, Tj, Nj\]) begin(\[ti, Ti, Ni\],\[tj, Tj, Nj\]) end(\[ti, Ti, Ni\],\[tj, Tj, Nj\]) after(\[ti, Ti, Ni\],\[tj, Tj, Nj\],D ) before(\[ti, T,, Ni\],\[tj, Tj, Nj\],D) relpos(X,\[ti, T, Ni\],\[tj, T, Nj\]) extent(\[ti, Ti, Ni\],\[G Tj, Nj\],\[tk, Tk, Nk\]) t~ is a time included in the time t 7 t* is a time that includes the time tj* t~ is a time whose beginning overlaps the time t~ t~ is a time whose ending overlaps the time t; t~ is a time after the time t~ with a temporal distance D (expressed as a duration) t~ is a time before time t 7 with a temporal distance D (expressed as a duration) t~ is the X th (-X th, if X<0) time of type T after (before, if X<0) time tj* t~ is a time period starting at time t7 and ending at time t; Suppose, for example, that the discourse contains two temporal constants, corresponding to yesterday and two days after Robert's departure.
We can imagine a situation where both designate exactly the same day.
But it is not possible for a single temporal constant to correspond to more than one objective time.
If it were, it would mean that an ATL could be ambiguous.
In our computational perspective, we accept some underspecified ATLs (not precisely specifying the temporal location), but not ambiguous ones.
A triplet is the simplest expression of a temporal localization.
Usually, expressing a temporal localization is more complex, because the temporal constant used as localizer cannot be rendered directly in relation to the contextual time.
This is the case, for example, if the localizer is a day, and the contextual time a year, because there is no natural way of decomposing a year into days 2.
In these cases, we must express relations with some intermediate localizers, until we reach one that can be related to the contextual time.
Let \[ti, Ti, Nj\] be the localizer and \[tj, Tj, Nj\], \[tk, Tk, Nk\] be intermediates localizers.
Many kinds of relations can be distinguished.
They are listed in Table 1.
Note that these relations can be combined recursively.
This means that the triplets used as arguments may also be represented using a relation.
We will show examples of this in the following discussion.
Among the arguments of these relations, one pertains to the temporal localizer, and one (two, in the case of the relation extent) pertains to an intermediate localizer to which the temporal localizer is related.
We call this last argument an anchor, since it represents a time to which the relation must be "anchored" in order to deduce the time of localization.
We will now give a short discussion of the relations of Table 1.
In the following examples, hoe designates the temporal constant corresponding to the localizer of the occurrence, and n and trey designate the time of speech and the reference time, respectively.
2 It
is possible to name the day using the religious calendar, using something like the day of St-Andrew, but it is far from usual to do so, except maybe for holidays such as Christmas, Easter, or Thanksgiving.
102 Gagnon and Lapalme From Conceptual Time to Linguistic Time The first relation is the most frequent for expressing temporal localization.
It is used to express localizations like le 3 avril 'on April 3rd'.
In this case, the localizer could be formulated as: inclin ( \[ tto, day, 3\], \[t~, month, april\]) As expected, the intermediate localizer tl is to be interpreted in relation to the contextual year.
The semantics of a more complex localizer like le matin du 3 avril 1994 'on the morning of April 3rd 1994' is an example of using recursivity for the expression: inclin ( \[hoc, momen t-of-day, morning\], inclin ( \[ h, day, 3\], inclin (\[t2, month, april\], It3, year, 1994\] ))) The second relation, incl, is required to deal with adverbials such as le jour oft Paul est parti 'the day when Paul left', aujourd'hui 'today' and ce jour-l~ 'that day'.
All of these refer to a day, but this day is not expressed in relation to a time that includes it.
On the contrary, the other time is included in it: the time when Paul left in the first example, the time of speech in the second example, and the referent time associated with REF in the third one.
Suppose that in the conceptual representation, 023 is the object representing the departure of Paul.
These three examples could be represented, respectively, as: incl(\[hoc, day, _\], \[O23, -, -\]) incl(\[ttoc, day, _\], \[n, _, _\]) incl ( \[ tloc, day, _\], \[ tref, -, -\] ) where "_" is used for arguments whose values are not relevant or unknown.
The relations begin and end represent the case where only one boundary of the localizer is known.
This results in an ATL such as depuis le 3 avril 'since April 3rd' or jusqu'au 3 avril 'until April 3rd' where meaning is ambiguous.
What do we mean exactly, when we write that ti begins at time tj?
That the initial boundary of ti is included in tj or that the ending of tj "meets" the beginning of ti?
If the answer is that tj includes the initial boundary of ti, what is the constraint on the duration of ti?
It is clear that it must be shorter than tj.
We do not have any answers to these crucial questions, and other similar ones.
These are problems that pertain to the deep generation process, which is not the focus of this paper.
We think that at the level of the semantic representation, this relation need not be further clarified, since it corresponds to the way time is expressed in the language.
In fact, all our relations have this underspecified nature.
For the relations before and after, the value of the temporal distance is given as a duration, using an expression of this form: duration (N, Type) The value of the duration is obtained by calculating the period corresponding to N periods of type Type.
For example, the adverbial deuxjours apr~s le ddpart de Paul 'two days after Paul's departure' would be represented as (023 represents the occurrence of Paul's departure): afler(\[hoc, -, -\], \[O23, -, -\], duration(2, day)) If the temporal distance is not known (or irrelevent), it is indicated by indefinite.
Thus, aprbs le ddpart de Paul 'after Paul's departure' would be represented as: after(\[ttoc, -,-\], \[023,-, -\], indefinite) 103 Computational Linguistics Volume 22, Number 1 Now, let's suppose that the localizer ti is of type T.
tn some cases, a good way to express it is by giving its position relative to another time tj of the same type.
For example, the ATL cinq jours plus tard 'five days later' is not used to mean "at a time in the future, five days from the reference time," but rather "the 5th day after the one which included the reference time".
If the reference time is itself a day, the semantics of this ATL could be: relpos( 5, \[hoc, day, _\], \[trey,-,-\]) If trey is not a day, we must express the relation by taking as anchor the day which includes it: relpos (5, \[ Gc, day, _\], incl ( \[ t l, day, _\], \[trey, day, -\])) (This takes for granted that the time trey is not bigger than a day.
If trey were bigger than a day, it would not make any sense to express relative position by specifying the temporal distance in days).
Similarly, hier 'yesterday' would be expressed semantically as "the day that is the first one before the day that includes the time of speech": relpos(-1, \[tloc, day, -\], incl ( \[\[h, day, _\], \[n, _, _\])) We have seen a way of expressing duration, by giving the length as a number of time units.
But there is another way of expressing duration: by indicating the two boundaries of the period.
By using this method, not only the duration of an occurrence is expressed, but so is its temporal location, at least partially.
The relation extent is used to express this kind of duration.
For example, the semantics of du 3 avril au 5 mai 'from April 3rd to May 5th' is formulated as: extent(\[hoe, _, _\], inclin ( \[ h, day, 3\], \[t2, month, april\]), inclin ( \[t3, day, 5\], \[t4, month, may\])) The semantics of du 3 au 10 avril 'from April 3 to 10' should represent the fact that the whole duration is included in the same month: inclin (extent (\[boo -, -\], \[h, daY, 3\], \[t2, day, 10\]), \[t3, month, april\]) The relation extent is also used to represent adverbials like depuis trois jours 'for three days' and pendant trois jours ?z partir du 3 avril 'during three days starting from April 3rd'.
These adverbials explicitly express one of the two boundaries.
In the first example, it is either the time of speech or the reference time (the ATL means "for three days until now" or "for three days until then").
In the second example, it is the time corresponding to April 3rd.
The other boundary is indicated implicitly by giving a temporal distance from the anchor.
The first example, supposing that the explicit boundary is speech time, would be represented as: extent(\[tloc, -, -\], before(It1, -, -\], \[n, _, _\], duration (3, day)), In, _, _\]) This expresses a period whose ending is the time of speech and whose beginning must be calculated by finding the time that is three days before speech time.
The second example would be represented as: extent( \[tloc, _, _\], inclin (\[tl, day, 3\], \[t2, month, april\]), after(It3, -, -\], \[t l, -, -\], duration(3, day))) Note that in both examples, the same temporal constant represents both the explicit boundary and the anchor of the relation after or before used to express the implicit boundary (n and tl, respectively).
104 Gagnon and Lapalrne From Conceptual Time to Linguistic Time Table 2 Relative localizers.
Adverbial Semantics entre le 3 avril et le 10 mai inclin(\[hoo_,d,extent(\[ti,_,d, (between April 3rd and May lOth) inclin(\[t2,day,3\],\[t3,month,april\]), inclin(lt4,day,10\],lts,month,aprill))) jusqu'tt il y a trois jours end(\[h ..... -\], (until three days ago) before(lt2,_,_\],\[n,_,d,duration(3,day))) jusqu'/i mercredi de cette semaine end(\[tloo_,_\],inclin(\[t2,moment-of-day,wednesday\] (until Wednesday of this week) incl(\[tg,week,_\],\[n,_,_\]))) Considering the examples we have just given, one may think that recursivity applies only to the anchor.
This is not the case.
The triplet that gives the location time in the expression can be replaced by a complex expression.
We have such a situation with le 3 avril dernier 'the last April 3rd' represented as: relpos (1, inclin ( \[tlo, day, 3\], \[tl, month, april\]), incl( \[t2, day, _\], \[n, _, _\])) More precisely, this expression means "the April 3rd that is the first one in the past, taking speech time as starting point".
Finally, to illustrate the richness of our semantics for expressing ATLs, in Table 2 we give a list of more adverbials with their semantics.
Note the extensive use of the combination property.
Thus, to specify the localization of an occurrence, we can use another simultaneous object, or use a localizer that is expressed in relation to another localizer.
The list of relations given in Table 1, together with the possibility of combining them, offers a very powerful way of expressing the great diversity in the semantics of temporal localizers.
Of course, not all the combinations may be expressed naturally in the language, but we are convinced that most of the ATLs can be expressed with this semantics.
3 The
problem of representing temporal location has received a lot of attention in the past (Dowty 1979, 1982, 1986; Bach 1986; Verkuyl 1989; and Vlach 1993).
But these works have focused on the aspectual structure of adverbials and their relation to tense and aspect.
We have not found any previous proposals of a recursive semantics like ours for representing the various types of localizations.
More related to our work is Allen (1983) who proposes a set of primitive relations to represent all possible relations between two temporal intervals.
The relations defined in Table I differ in many respects from the relations proposed by Allen.
As mentioned before, ours are less precise.
For example, the moment represented using the relation end in our model corresponds to three relations in Allen's model.
Suppose that t* and tj* are the objective times corresponding to the localizer and the anchor, respectively.
Then, in Allen's model, the end of t~ could coincide with the beginning of tj*, could coincide with the end of tj*, or could be included in t 7.
The main reason for using a different set of primitives is to represent as closely as possible the way temporal localization is dealt with in language.
The result of our 3 In fact, the set of relations described here is not sufficient.
In Gagnon and Bras (1994), we define a more complete set of relations.
105 Gagnon and Lapalme From Conceptual Time to Linguistic Time I t ENVIRONMENT ...... ------o-M"-'--"~-~'........ n-Knowte.l.e.,; I base I structure concepts t I I I I I I ! ! I ! _J P R E T E R I SEMANTIC INTERFACE I -.~ ENGINE <  GRAMMAR t (Systemic I t network) I I Ii I t I t < --.... -*BLACK E I BOARD I R I Figure 8 Implementation of the surface generation process in Prdtexte.
To produce a sentence, the network is processed from left to right.
When a system is entered, a choice is made that may lead to another system or to a conjunction of systems processed concurrently.
The syntactic structure of the phrase results from a set of constraints determined by features selected during the traversal of the entire network.
The choices made in the first traversal of the network determine the overall structure of the sentence, represented as an ordered sequence of functions that must be fulfilled.
The term "function," in this context, refers to the role played by a constituent of a phrase in achieving a communicative goal (Halliday 1985).
For example, a sentence can often be decomposed into three constituents fulfilling the following functions: Subject, Predicate, and Object.
4 Once
the functional structure of the sentence is established, its network is traversed again to determine the syntactic structure, which is then further refined until each function is realized by a single word.
Figure 8 illustrates the organization of the modules in Pr6texte, inspired by Nigel (Mann 1983; Mann 1985; Matthiessen 1985; Matthiessen and Bateman 1991).
To produce a sentence, Pr6texte uses three information components: the environment, containing the information about the message and a knowledge base describing how to achieve lexicalization; the grammar, represented as a systemic network; and the blackboard, used to determine the syntactic structure.
The engine controls the surface generation process and accesses the three information components through three interface modules: semantic interface, interpreter and realizer.
The solver determines the final structure of the constituent, using constraints posted in the blackboard during the traversal of the network.
4 In
this text, names of functions will always be capitalized.
107 Computational Linguistics Volume 22, Number 1 Before starting the surface generation process, the environment is augmented with information that determines the message:  a semantic structure such as the one illustrated in Figure 4;  a set of relevant concepts, which are the elements of the conceptual representation pertaining to the entities in the semantic structure;  some pragmatic information, which specifies how to transmit the message.
The engine starts by posting on the blackboard the description of the realized constituent representing the sentence.
It then activates the traversal of the network by the interpreter.
To select a feature in a system, the interpreter transmits inquiries to the engine.
If the information necessary to respond to the inquiry is in the environment, the engine transmits the inquiry to the semantic interface.
If an inquiry is about a decision previously made in the surface generation process (for example, a question about what features have been selected in a system visited earlier), then the engine transmits the inquiry to the realizer.
Answers to inquiries enable the selection of a feature in the visited system.
The interpreter then extracts a set of realization statements associated with the selected feature.
After the execution of these statements by the realizer, the information about the structure of the realized constituent, contained in the blackboard, is updated.
Three kinds of action may be executed by the realizer:  addition of a new constituent with the appropriate semantic information fulfilling a specific function;  updating of the information pertaining to one constituent;  addition of partial ordering constraints that identify the sequence of functions composing the final structure.
This process goes on until no more system can be visited.
The solver then solves the ordering constraints on the blackboard.
We thus obtain a sequence of functions that constitutes the final structure of the realized constituent.
Each of these functions is associated with semantic information extracted from the environment.
For example, the sentence may contain the function Temp_loc (temporal localizer), whose semantic information will be the expression associated with the temporal marker LOC in the input.
If a function is to be lexicalized as a word, the lexicon is consulted to identify the word, taking into account the features selected during the traversal.
Otherwise the grammar is re-entered using the function as the new realized constituent with some features preselected.
For example, to generate the sentence in 2: (2) Jusqu'a 10h50, il a survol6 l'Alg6rie 'until 10:50, it flew over Algeria' the following semantic structure and relations are posted in the environment: message = event(olo) N=n R =09 PERSP = n LOC = end(\[hl, _, _\], inclin(\[t6, minute, 50\], \[t4, hour, lO\])) (a) o10 < n (b) o10 C) tu 108" Gagnon and Lapalme From Conceptual Time to Linguistic Time Positioner \[ Jusque I A A /\ Reference Zon 4 il \ I 1 h lOh50 a survol4 l'Alg6rie Figure 9 The structure of jusqu'd 10h50, il a survold l'Alg&ie.
where o10 must be expressed as an event and tll is a period terminating at 10:50.
The event o10 is before the time of speech and it coincides more or less with the time used as localizer.
The first traversal of the grammar determines the overall structure of the sentence made of the four functions at the top of Figure 9.
As none of them may be lexicalized directly as a word, they must be realized by re-entering the grammar.
We describe only the realization of the function Tempdoc.
The semantics associated with Temp_loc is the value of LOC in the semantic representation.
Traversal of the network for the realization of Temploc results in a structure with two functions: the Positioner and the Reference Zone.
The first function is lexicalized with the preposition jusque; for the second function, the grammar is re-entered, taking as semantics the anchor of the expression associated with Tempdoc: inclin(\[t6,minute,50\], \[t4,hour, lO\]).
Again, this results in a structure with two functions; in this case, both functions can be lexicalized, ending the realization of the function Tempdoc.
The same kind of processing is done for the other functions in the sentence.
5. The Production of ATLs In Section 3.3, we showed how the semantics of ATLs is represented; in this section, we present how ATLs can be lexicalized.
Table 3 gives a list of semantic representations and their translations into ATLs produced by our generator.
As in Section 3.4, we use ttoc for the temporal constant corresponding to the localizer of the occurrence, n for speech time and tref for the reference time.
5.1 Syntactic
Compositions Some ATLs (1-7, 16) are simple, while others (8-15, 17-18) contain an embedded temporal adverbial.
For example, in (11) jusqu'?l mercredi de cette semaine 'until Wednesday of this week' contains another ATL, which itself contains another embedded ATL cette semaine 'this week'.
109 Computational Linguistics Volume 22, Number 1 Table 3 List of adverbial temporal locations.
Semantics ATL (1) relpos(-1, \[tloc, day, _\], incl(\[t2, day, _\], \[n, _, _\])) (2) relpos(-1, \[hoc, daY,-\], incl(\[t2, day, _\], \[tref, _, _\])) (3) incl(\[tloc, day, _\], \[n, _, -\]) (4) incl(\[tloc, month, _\], \[n, _, _\]) (5) incl(\[tloc, month, _\], \[tref, -, -\]) (6) \[tloc,month,April\] (7) \[01,-,-\] (8) inclin(\[tloc,moment-of-day, morning\], inclin(\[t2,season,summer\],\[t3,year,1995\])) (9) inclin(\[ tloc,half-hour,1\],occurrence(ol ) ) (10) duration(3,day) (11) end(\[tloc,_,_\],inclin(\[t2,day, wednesday\], incl(\[t3,week,_\],\[n,_,_\]))) (12) begin(\[tloc,_,_\],relpos(1,\[t2,month,_\], incl (\[ t 3,month,-\],\[ tref,-,-\]) ) ) (13) begin(\[tloc,_,_\],inclin(\[t2,day, lO\],\[t3,month,may\])) (14) relpos(3,\[tloc,day,_\],occurrence(ol )) (15) after(\[tloc,day,_\],\[tref,_,_\],duration(3,day)) (16) after(\[tloc,_,_\],\[tref,_,_\],indefinite) (17) extent(\[tloc,_,_\], inclin(\[t2,day,3\],\[t3,month,april\]), inclin(\[t4,day, lO\],\[ts,month,may\])) (18) extent(\[tloc,_,_\], before(\[t2,-,-\],\[tre f,-,-\],duration(3,day)), \[tre/,-,-l) hier (yesterday) la veille (the day before) aujourd'hui (today) ce mois-ci (this month) ce mois-la (that month) en avril (in April) quand Robert est parti (when Robert left) le matin du 3 avril 1995 (the morning of April 3rd 1995) la premi6re demi-heure de l'6mission (the first half hour of the program) durant trois jours (during three days) jusqu'a mercredi de cette semaine (until Wednesday of this week) a partir du mois suivant (from the following month) depuis le 10 mai (sinceMay lOth) trois jours apr6s le d6part de Robert (three days after Robert's departure) trois jours plus tard (three days later) puis (then) du 3 avril au 10 mai (from April 3rd to May lOth) depuis trois jours (since three days) Unfortunately, the combination of localizers in the semantics does not always correspond to the combination of adverbials.
For example, if there were such a correspondence, the adverbial in (1) would be something like lejour avant lejour qui contient l'instant d'~nonciation 'the day before the day that contains the time of speech'.
Instead, we get the simple adverbial hier 'yesterday'.
For complex semantic expressions, in all examples except (15), (16) and (18), there is one embedded adverbial corresponding to each anchor.
For example, the anchor relpos(1,\[t2,month,_\], incl(\[tB,month,_\], \[tref,-,-\])) in (12) corresponds to le mois suivant 'the following month' in the adverbial.
5 Examples
(1-5) are special, since the relation and the anchor are combined in the same syntactic structure.
In (15), direct translation of the anchor into an adverbial would produce trois jours apr~s ce mornent-l?l 'three days after that moment', and in (18), pendant trois jours jusqu'?~ ce moment-l?l 'during three days until that moment'.
Since there is not always a direct correspondence between semantic and syntactic 5 Note that in the adverbial, du is a contraction of de le.
110 Gagnon and Lapalme From Conceptual Time to Linguistic Time QTY" ANCHORS I unique double ( Figure 10 Section of Pr4texte's grammar for adverbials.
ANCHOR ANCHORI ANCIIOR2 deictic ~ anaphoric autonomous ~ deictic anaphoric autonomous i deictic anaphoric autonomous forms, which one should be used in the grammar to distinguish among ATLs?
We have chosen the semantic form because adverbials are distinguished not only by the number of anchors but also by their nature.
Examples (4) and (5) are both syntactically simple---the anchor in the semantic form is not expressed--but it is the anchor that explains their difference: the first uses speech time, whereas the second uses reference time.
Figure 10 illustrates the part of the network taking into account the combination property.
We first identify the number of anchors.
If there is only one, the feature unique is selected in the system QTY_ANCHORS.
Otherwise, double is selected.
Then, for each anchor, we must establish if it is deictic, anaphoric, or autonomous.
If the localization represented by the anchor is made in relation to the time of speech, deictic is selected; if the localization is made in relation to the reference time, anaphoric is selected; if the anchor achieves a localization without using either of the two temporal markers, autonomous is selected.
In Table 4, we indicate the features selected in the systems of Figure 10, for the production of the adverbials given in Table 3.
Some adverbials may be distinguished using the systems of Figure 10, but Table 4 shows that these systems are not enough.
The features have a strong influence on the most embedded adverbials.
For example, in (11), the selection of deictic results in cette semaine 'this week' for the most embedded adverbial but if the feature anaphoric had been selected, it would have produced cette semaine-lit 'that week'.
These features alone do not explain the recursive form of adverbials.
Figure 11 shows the structure of two adverbials from Table 3.
The structure of adverbial (11) is given in (a).
It has three levels, each one corresponding to one adverbial.
The simple structure of adverbial (4) is shown in (b).
Their difference is not only due to the number of levels in the structure.
In (a), the structure contains a function, the Positioner, that expresses the relation to the anchor; there is no function in (b).
Sometimes an anchor is not realized syntactically at all.
In Figure 12, we consider (13) and (15): the anchor is expressed in (13), shown in (a), but not in (15), shown in (b).
The structure of (15) contains the Positioner and a function conveying the temporal distance to an implicit anchor.
These examples show that features are not sufficient; to determine the syntactic structure of the adverbial, we need more systems in our grammar, such as the network of Figure 7.
The two networks of Figure 7 and Figure 10 must be traversed in parallel.
The system ZONE_DESIGNATION first distinguishes between adverbials that ex111 Computational Linguistics Volume 22, Number 1 Table 4 Distinction of adverbials using the anchor.
(The numbers correspond to the adverbial's position in Table 3).
Adverbial QTY_ANCHORS ANCHOR ANCHOR1 ANCHOR2 (1) hier (3) aujourd'hui (4) ce mois-ci (11) jusqu'h mercredi de cette semaine (2) la veille (5) ce mois-lh (12) a partir du mois suivant (15) trois jours plus tard (16) puis (6) en avril (7) quand Robert est parti (8) le matin du 3 avril 1995 (9) la premi6re demi-heure de l'6mission (10) durant trois jours (13) depuis le 10 rnai (14) trois jours apr6s le d6part de Robert (17) du 3 avril au 10 mai (18) depuis trois jours unique umque unique umque umque unique unique unique umque umque unique unique umque unique umque umque double double deictic deictic deictic deictic anaphoric anaphonc anaphoric anaphoric anaphoric autonomous autonomous autonomous autonomous autonomous autonomous autonomous autonomous anaphonc autonomous anaphoric Positioner \[ Jusque Until Reference Zon~ \ \[Zone Designator\[ Attributor Reference Zon~ h mercredi de \[,-Zone Designator\] Wednesday of / NNN, N cette semaine this week (a) (b) Figure 11 Difference of structure for (11) and (4).
Zone Designator\] /\ ce mois-ci this month press localization directly, as in (1-9), and adverbials that relate it to other localizers.
Selection of direct includes a function Zone Designator in the structure for the most embedded adverbial of (a) and the adverbial of (b) in Figure 11.
This function is realized by a phrase expressing the temporal location, which may be a temporal constant (feature chronological) or an occurrence (feature occurrential).
If relational is selected in ZONE_DESIGNATION, a function Positioner is inserted in the structure.
This function is realized by a phrase that expresses the relation of the localizer to its anchor.
There are two types of relational localizers: those that express a 112 Gagnon and Lapalme From Conceptual Time to Linguistic Time Positioner \[ RefercnceZonc( /\ depuis le 10 mai Since May lOth (a) Figure 12 Difference of structure for (13) and (15).
I T m ora, sta eol pvs,tionor I trois jours plus tard three days later (b) Table 5 Distinction of adverbials with type of designation.
(The numbers correspond to Table 3).
Adverbial ZONE DESIGNATION TYPE LOC ZONE LOCT ASPECT (1) hier direct chronological -(2) la veille direct chronological -(3) aujourd'hui direct chronological -(4) ce mois-ci direct chronological -(5) ce mois-la direct chronological -(6) en avril direct chronological -(8) le matin du 3 avril 1995 direct chronological -(9) la premi6re demi-heure de l'6mission direct chronological -(7) quand Robert est parti direct occurrential -(10) durant trois jours relational -durative (11) jusqu'~ mercredi de cette semaine relational -durative (12) a partir du mois suivant relational -durative (13) depuis le 10 mai relational -durative (17) du 3 avril au 10 mai relational -durative (18) depuis trois jours relational -durative (14) trois jours apr6s le d6part de Robert relational -punctual (15) trois jours plus tard relational -punctual (16) puis relational -punctual duration (see (a) in Figure 12) and those that designate a punctual temporal location (see (a) in Figure 11 and (b) in Figure 12).
The classification of adverbials using these distinctions is shown in Table 5.
But even by combining this classification with that of Table 4, we cannot distinguish between all adverbials.
For example, (1, 3, and 4) select the same features in both networks, as do (2 and 5), (6, 8, and 9), and (15 and 16).
For each of the four cases of Table 5, we will show how the adverbials can be distinguished.
5.2 Relational
Localizers 5.2.1 Punctual Localizers.
The function Positioner is always present in adverbials for which the feature punctual has been selected; this is a consequence of the selection of the feature relational.
In addition to Positioner, there can be two more functions.
One is the Temporal Reference Zone, which conveys the localizer to which the relation expressed by the Positioner pertains.
In our list, only adverbial (14) contains this function: le ddpart de Robert 'Robert's departure'.
The other function is Temporal Distance, which expresses the length of time from the localizer used as anchor.
This function occurs in 113 Computational Linguistics Volume 22, Number 1 Positioner l Puis Then Auparavant Before (a) \[ Positioner \] Temporal Distance \[ Dans trois jours In three days from now I1 y a trois jours Three days ago (b) \[ T~Zone\[ Positioner \] Trois jours pllas tard Three days tater Trois jours av ant Three -days earlier (c) \[ Positioner \] Temp Ref Zone \[ a~re~s le d61~art de Robert R o berY s departure avant 8h00 before 8:00 \] Temporal Distance \] Positioner \[ Temp Ref Zone\[ /\ /\ T,~trOiS jours al~r~s !e d6partde Robert ee aays after Kooert's aeparture Trois jours avant le d6part de Robert Three ~ays before Robert's departure (d) (e) Figure 13 Structure for punctual adverbials.
RELATION TYPE V before punctual L_ after f explicit REFERENCE ZONE E implicit 3-TEMtK)RALDISTANCE F definite ~-indefinite Figure 14 Grammar section for relational punctual adverbials.
adverbials (14) and (15): trois jours 'three days'.
Thus, (14) contains both functions and (15) contains only the Temporal Distance.
Adverbial (16) is distinguished from (14) and (15) because it contains neither of these functions.
In dans trois jours 'in three days from now' and apr~s le 9 octobre 'after October 9th', we find two different structures.
The elements of the first structure are exactly the same as the structure of adverbial (15), but they occur in a different order: Temporal Distance comes before the Positioner.
In the second structure, there is a Temporal Reference Zone, le 9 octobre, but no Temporal Distance.
Thus, for punctual adverbials, there are five possible structures, illustrated in Figure 12.
To distinguish between these adverbials, we use a network, part of which is shown in Figure 13.
Two features are expressed by the system RELATION TYPE: before and after.
To realize the Positioner, there is no need to re-enter the grammar, since it may be found directly in the lexicon.
The lexical choice depends not only on the selection achieved 114 Gagnon and Lapalme From Conceptual Time to Linguistic Time Table 6 List of punctual adverbials.
Adverbial RELATION REFERENCE TEMPORAL TYPE ZONE DISTANCE (14) trois jours apr6s le d6part de Robert after explicit definite (three days after Robert's departure) apr6s le 9 octobre (after October 9th) after explicit indefinite (15) trois jours plus tard (three days later) after implicit definite dans trois jours (three days from now) after implicit definite (16) puis (then) after implicit indefinite trois jours avant le d6part de Robert before explicit definite (three days before Robert's departure) avant 8h00 (before 8:00) before explicit indefinite trois jours avant (three days earlier) before implicit definite il y a trois jours(three days ago) before implicit definite auparavant (before) before implicit indefinite in RELATION TYPE, but also in the choice made in the system ANCHOR of figure 10.
For example, in the cases where after is chosen, the Positioner could be lexicalized as puis 'then' or plus tard 'later'; if anaphoric is chosen in ANCHOR it can be lexicalized as dans 'in' if deictic is chosen or apr~s 'after' if autonomous is chosen.
The fact that Temporal Distance and Temporal Reference Zone are optional in the structure is represented in the grammar by two parallel systems: REFERENCE ZONE and TEMPORAL DISTANCE.
If, in REFERENCE ZONE, explicit is chosen, the function Temporal Reference Zone is included in the structure.
Since this function represents another localizer, the anchor, it is realized by re-entering the grammar, taking as input the semantic representation of this anchor.
In TEMPORAL DISTANCE, the selection of definite results in the inclusion of the function Temporal Distance.
To realize it, the grammar must be re-entered, and some features must be preselected so that it is realized as a noun phrase.
Table 6 lists all possible adverbials represented by the network of Figure 13 together with their selected features.
The three adverbials taken from Table 3 are preceded by their reference number to ease the comparison of their semantics with the selected features; their relations will be discussed later.
The distinction between structures (b) and (c) in Figure 12 is not explained by the grammar section shown in Figure 13, since the same features are selected for trois jours plus tard and dans trois jours.
However, structure (b) in Figure 12 is found only for deictic localizers.
Therefore, features for deictic localizers selected in the system ANCHOR, will distinguish structure (b) from structure (c).
Let us now see how the features are selected for the production of relational punctual localizers.
In RELATION TYPE, the feature reflects the relation used in the semantics, if this relation is before or after.
Adverbial (14) deserves some explanation.
Since its semantic expression uses the relation relpos, we would expect its syntactic realization to be: le troisi~me jour apr~s le jour du d6part de Robert 'the third day after the day of Robert's departure', but this usage is rare.
Instead we find trois jours apr~s le d6part de Robert 'three days after Robert's departure', which is what we would expect if the semantic expression used the relation after.
This seems to be because if the temporal distance is one unit, a direct localizer is preferred.
So, instead of generating un mois plus tard 'one month later', we produce le mois suivant 'the next month'.
Our intuition 115 Computational Linguistics Volume 22, Number 1 durative DURATION TYPE bound quantified DURATION ANCHOR DURATION PERSPECTWE -anterior posterior double QUANTDURATION E anterior ANCHOR posterior nil Figure 15 Grammar section for relational durative adverbials.
-internal external \[P.~i--, \] D~oo Q~,ty i l I ''ia'= \] Bo.d~, \] ..~e~ ~ays . Jusque Is mercredi de three days Sm.ce for ~rom that moment) A r~oir de le 10 mai may lOth (a) (b) Figure 16 Structure for durative localizers.
Du 3 avril au lOmai From April 3rd to May 10th (c) is that when X is '%ig" we have this equivalence: relpos(X, \[ti, Ti, Ni\], Z) ::~ afler(\[ti, Ti, Ni\], Z, duration(X, Zi) ) and similarly for X negative and the relation before.
More study is needed to determine the threshold at which the two relations become equivalent in the linguistic realization.
We are sure that for X = I or X = -1, they are not equivalent, so in our implementation, we use 2 and -2 as thresholds.
In the system REFERENCE ZONE, the feature implicit is chosen if the anchor is a simple localizer using the reference time or the time of speech, otherwise explicit is chosen.
Feature selection in TEMPORAL DISTANCE depends on whether the third argument in the semantic expression is indefinite or a specified duration.
5.2.2 Durative
Adverbials.
We now show how the durative adverbials of Table 5 can be differentiated.
The part of the network that generates these adverbials is shown in Figure 14.
We give the three kinds of structure identified for these adverbials in Figure 15, and, finally, in Table 7 we list the durative adverbials of Table 5 with their corresponding features according to the systems of Figure 14.
To give a complete illustration of all adverbials generated with the network of Figure 14, we added one adverbial to the list: pendant trois jours 'for three days from now', which is symmetrical to depuis trois jours 'for three days until now'.
In Figure 14, we distinguish two types of durative adverbial phrases: bound, if the duration is expressed by specifying one or two of its boundaries; and quantified, 116 Gagnon and Lapalme From Conceptual Time to Linguistic Time Table 7 List of durative adverbials.
Adverbial DUR.
DUR. DUR.
QUANZ TYPE ANCH.
PERSE DUR.
ANCH. (13) depuis le 10 mai (since May lOth) bound ant.
int. -(12) a partir du rnois suivant bound ant.
ext. -(from next month) (11) jusqu'~ mercredi de cette semaine bound post.
--(until Wednesday of this week) (17) du 3 avril au 10 rnai bound double --(from April 3rd to May lOth) -pendant trois jours quant.
--ant.
(for three days until that moment) (18) depuis trois jours quant.
--post.
(for three days from that moment) (10) durant trois jours (during three days) quant.
--nil if the duration is expressed as a quantity of time units.
In the first case, we get a structure such as (b) or (c) in Figure 15.
In (b) there is only one boundary, and the Positioner indicates which one is used: its lexicalization depends on the feature selected in DURATION ANCHOR.
If anterior is selected, one further distinction is required to lexicalize the Positioner, as represented by the system DURATION PERSPECTIVE.
The Positioner is realized by the phrase ~ partir de if the feature chosen is external, otherwise it is realized as depuis.
The choice depends on the aspect of the occurrence reported: the feature external is chosen if the occurrence is presented as an event, and internal is chosen if the occurrence is presented as a situation.
These two cases are exemplified in the following two sentences: (3) a.
A partir de 1972, il enseigna a l'Universit6 de Montr6al.
'From 1972 on, he taught at Universit6 de Montr6al'.
b. Depuis 1972, il enseignait a l'Universit6 de Montr6al.
'Since 1972 he was teaching at Universit6 de Montr6al'.
In (3a), since the occurrence is presented as an event, the feature external is selected during the determination of the ATL, thus resulting in the form ~ partir de 1972 'from 1972'.
In (3b), the same occurrence is presented as a situation considered from the reference time.
Thus, internal is selected, resulting in the form depuis 1972 'since 1972'.
This is a good example of the interaction of ATLs with the aspect of the occurrence.
In the interpretation of the ATL in (3b), the duration is anchored not only on the year 1972 but also on the reference time included in the occurrence.
But in the semantics of the ATL, which uses the relation begin, as well as ATL (12) in Table 3, there is only one anchor.
Even if the reference time is involved in the understanding of the whole sentence, it is not directly expressed in the semantics of the ATL.
An alternative would be to express the same localization using the relation extent, as in (18) of Table 3.
If the reference time is included in 1982 (the beginning of the duration expressed in (3b) thus being 10 years before), the semantics would be: extent(\[hoc,_,_\],before(\[tl,year, d,\[t2,_,-\],duration(lO,year)), incl(\[ta,year, d,\[tref,_,_\])) 117 Computational Linguistics Volume 22, Number 1 In depuis dix ans 'For 10 years', the meaning of the ATL, which is "since 10 years in the past starting from this moment" requires the use of the reference time.
If, in the system DURATION ANCHOR, the feature double is chosen, we get a structure containing two boundaries, as in (c) in Figure 15.
A boundary, in the structure of a bound localizer, is always realized as a temporal adverbial, by re-entering the grammar.
When the feature quantified is selected, the structure in (a) of Figure 15 is obtained.
To realize the Positioner in this case, another system is required, because the quantity of time that constitutes the duration can be worded in many ways.
We can express the duration of the occurrence without giving any hint about its location in time, as in durant trois jours 'during three days', or we can indicate a duration that begins or ends at some time.
To see how the features are selected in the grammar section of Figure 14, compare the adverbials of Table 7 with their semantics as given in Table 3 (the semantics for pendant trois jours is the same as depuis trois jours, but the relation after is substituted before and the two anchors are reversed).
First, the feature bound in DURATION TYPE is selected if the relation used in the semantic representation is either begin or end, or if it is extent and the two anchors are autonomous.
In DURATION ANCHOR, features corresponding to these three cases are selected.
In DURATION PERSPECTIVE, the selection depends on the aspect of the occurrence reported in the sentence.
In DURATION TYPE, quantified is selected if the semantics uses the relation extent and one anchor is deictic or anaphoric, as in our examples, or if it uses the relation duration.
In the first case, the selection depends on the position of the anaphoric or deictic anchor in the expression.
If the relation duration is used, since there is no anchor, the feature nil is selected in the next system.
5.3 Direct
Localizers We complete our discussion of adverbials by explaining how the direct localizers can be differentiated.
Figure 16 shows the structure of the direct adverbials that constitute the first half of Table 5; there are three possible structures for a direct localizer.
The simplest ones, in (a) and (b), contain only one function, Zone Designator, that expresses the temporal location zone designated by the adverbial.
This function is realized directly by an adverb, in (a), using the lexicon depending on the system ANCHOR of Figure 10.
In (b), the grammar must be re-entered to generate a nominal phrase whose form also depends on the choice in ANCHOR.
In some cases, it is not sufficient to specify a temporal location zone: we must also add what we call a Pointer to relate the occurrence with this zone.
In our examples, the Pointer indicates that the occurrence takes place during the month of April, or when Robert left.
The existence of such a localizer in the structure seems to depend on the level of the adverbial in the embedding structure.
For example, we find a Pointer in the adverbial ~ ShOO 'at 8:00' if it is used alone, but not if it is embedded in another adverbial, like depuis 8hO0 'since 8:00'.
Our approach to this problem may be contrasted with Forster's (1989), who determines the realization of the Pointer by the temporal aspect of the Zone Designator (durative or punctual) using a constraint propagation technique.
Other possible structures for direct localizers are illustrated in (d) and (e) in Figure 16.
One function is the Zone Designator, which designates the direct expression of temporal zone.
If this zone is included in another localizer or in a position relative to another localizer, we must include another function in the structure: the Reference Zone, which corresponds to this second localizer.
The Attributor links these two functions.
In (e), le matin du 3 avril 1995 'the morning of April 3rd 1995' directly expresses 118 Gagnon and Lapalme From Conceptual Time to Linguistic Time Izo=~i~o~l Iz/;n~sign~,rl \[Po~i~ tznem~'a~"q /X /X Aujourd'hui Ce mois-ci En avril Today This month In April l-Iier Ce mois-l~t Quand Robert est parti Yesterday That month When Robert lift La veille The day before (a) Co) (c) I ZoneDesignat~rAttributor \] ReferenceZonb \[Zone Designatot de I Zne DesignatOr l /\ La premi&e demi-heure 1'6mission The first half-hour the program (d) I ZneDesignat1 Attributor Referen.eZorle / Zone Designatoq Auributor I Kelerence z.o~e /\ I d~ | \[ ZoneDestgnator I Attn0utor li~c~crencez-on~ \[ Zone Designator I of I the3rd I avril ~ t ator I ! Le matin I April ~ I j//N'N I The morning ! I 1995 I I=_-I I ............
=_----:-'---(e) Figure 17 Structure for direct localizers.
a morning.
This morning is itself part of another localizer, and so on.
The Attributor is sometimes lexicalized as an empty item.
To these direct localizers, we must add the embedded direct adverbials found in the relational adverbials of table 5.
In Figure 17, we use by dashed-line boxes to indicate those that differ from Figure 16.
The system shown in Figure 18 differentiates among these different forms of direct adverbials.
For occurrential adverbials, for example, the second adverbial of (b) in Figure 16 and the embedded adverbials in (d) in Figure 16 and (b) in Figure 17, the occurrence may be nominalized or not.
We do not have any satisfying answers to the question of how to choose between these two possibilities.
We will state only that when the adverbial is embedded, a nominalized form may be preferred to another embedded adverbial.
For chronological adverbials, the system AUTONOMOUS ZONE distinguishes between those that have an anaphoric or deictic temporal location zone, and those for which the temporal location zone is autonomous.
Adverbials of the first type always have a simple structure: aujourd'hui, hier, demain, ce mois-ci, ce mois-lh, cette semaine, le mois suivant.
The temporal location zone is different from the anchor.
In mercredi de cette semaine "Wednesday of this week', the temporal location zone, expressed by mercredi, is autonomous whereas the anchor expressed by cette semaine is deictic.
When the network is traversed the first time, yes is selected in the system AUTONOMOUS ZONE.
Its only in the second traversal, when cette semaine is generated, that no is selected in this system.
We must further distinguish adverbials with an autonomous temporal location zone, by deciding if their structure contains a Reference Zone or not.
The feature 119 Computational Linguistics Volume 22, Number 1 I Bouna y I Jusque IZoneDesignatorl Attributor I Referencezon4 Until / \[ Pointer \[Zone Designator I /\ a mercredi Wednesday de of ! r ..... .T ......
V \[Zone Designator \] /\ cette semaine this week ........... i ..................................
! (a) \[ Temporal Distance I Positioner \] Temp Ref Zone I /\ Trois jours Three days apr~s after \[Zone Designator \[ /\ le d6part de Robert Robert's departure I Positionerl Boundary \] A partir de \[Zone Designator \[ Fro,.
/~ Depuis From le mois prochain the next month le 10 mai May lOth ...........
_1 (b) (c) Figure 18 Structure for direct localizers.
TYPE_LOC_ZONE AUTONOMOUS no I chronological ZONE E yes hNCLUD1NG TIME implicit E explicit nominalised occurrential NOMINAL not nominalized Figure 19 Grammar section for direct adverbials.
implicit, implying the non-existence of Reference Zone, is selected in the system INCLUDING TIME if the semantic form is a single triplet \[ti Type, Naming\].
There is yet another system that decides if there is a Pointer or not, but as the problem of the existence of the Pointer is not completely solved and not really important to our discussion, we do not consider this system here.
120 Gagnon and Lapalme From Conceptual Time to Linguistic Time Table 8 List of durative adverbials.
Adverbial AUTONOMOUS INCLUDING ZONE TIME aujourd'hui (today) no hier (yesterday) no demain (tomorrow) no ce mois-ci (this month) no ce mois-l& (that month) no le mois suivant (the following month) no en avril (in April) yes mercredi de cette semaine (Wednesday of this week) yes cette semaine (this week) no la premi6re demi-heure de l'6mission yes (the first half-hour of the program) le matin du 3 avril 1995 (the morning of April 3rd 1995) yes le 3 avril 1995 yes avril 1995 yes 1995 yes implicit explicit explicit explicit explicit explicit implicit In Table 8, we present the features selected for direct chronological adverbials, embedded or not.
The systems in Figure 19 do not suffice to distinguish all direct adverbials.
The selections in these systems must be combined with the selections made in the systems of Figure 10.
5.4 Related
Work on the Generation of ATLs The problem of temporal localization has already been studied by many researchers, but most of them have focused on the aspectual interaction of the adverbials with verb tense; the problem of the semantic and syntactic structures of ATLs has been neglected.
Molin6s' (1990) study from a linguistic perspective characterizes the adverbials based on noun phrases.
Our work extends hers because our computational perspective has made us go farther in the formalization.
Bras and Molin6s (1993) made a similar attempt, but from the perspective of discourse understanding.
Since the problems of understanding are very different from the problems of generation, we could not simply use their method in a "reversed mode".
Their method relies on a compositional analysis of the language, where all information units extracted from the semantic structure are combined to select one meaning for the adverbial.
This compositional approach is not easily reversible, and it does not provide any insight into the selection problem inherent to the generation task.
Ehrich (1987) classifies adverbials in the context of generation, but she does not cover all the cases presented in this section.
Concerning the problem of the generation of ATLs, Maybury (1991) shows how the notion of focus as used by McKeown (1985) can be extended to include a temporal focus that corresponds essentially to the reference point in the Reichenbach model (1947).
An operation on the temporal focus, in combination with the value of speech and event times, selects the temporal adverbial and the verb tense.
Since the emphasis in this work was on the planning aspect of the task, the variety of adverbials that can be generated is limited.
Forster (1989) explains how the syntactic structure of a temporal adverbial may be controlled by semantic information such as the durative or punctual nature of the localizer.
Essential134 the final structure is obtained by propagating constraints associated with each syntactic subpart of the structure.
In particular, he focuses on the interaction between prepositional phrases and noun phrases.
For example, the preposition on is 121 Computational Linguistics Volume 22, Number 1 selected in on Sunday because Sunday is identified as a punctual localizer; this rules out in, which implies a durative localizer.
We have already presented one problem with this approach: it is not clear how the choice of these prepositions can be achieved by propagating semantic constraints.
The choice of preposition in French is very different from English and it often appears to be arbitrary or conventional.
Furthermore, many aspects of the problem are neglected, such as the type of reference expressed by the adverbial: it is not clear how Forster's system can represent the distinctions between anaphoric, deictic and autonomous localizers because the link between the semantic and syntactic levels is not fully explained.
Nigel (Matthiessen and Bateman 1991) offers the widest coverage of English but the variety of forms for ATLs is quite limited.
The temporal localization that may be expressed by different types of syntactic structures is represented in Nigel by systems dispersed throughout the whole grammar network.
For the expression of temporal localizers, their grammar is more dependent on the syntactic structure than ours, which is mainly determined by the semantics.
To summarize, our approach departs from previous approaches by covering more types of adverbials, by proposing a semantics for localization, and by explaining in detail how the different syntactic structures may be obtained from this semantics.
6. The Production of Verb Phrases In our work, we have focused on the generation of adverbials because we felt this problem had not received enough attention and because the temporal localization achieved by ATLs is more complex and more diversified than that expressed by verb tenses.
To generate a discourse like Discourse 1, however, we cannot avoid the problem of determining the structure of the verb phrase, because part of the localization is achieved by the verb, and because of the relations between verbs and adverbials.
In our implemention of the expression of temporal localization, the relation between the verb and the adverbial is taken into account mainly in the deep generation process.
In the semantic representation, we find traces of this interaction.
By keeping these decisions in the deep generation process, the verb phrase and the ATL can be generated independently.
Our method for generating the verb phrase takes advantage of the kind of information directly represented in DRT: the relation of the occurrence to speech time, which we call the primary localization, the aspect of the occurrence, and the presence or absence of a perspective point.
It is implemented by the grammar section illustrated in Figure 19.
In Prdtexte, the production of verb phrases requires many traversals of the network.
First, when the structure of the sentence is determined, choices are made regarding localization, aspect, and perspective.
After a first traversal of the network, the sentence's structure contains a function called Predicate, realized as a verb phrase.
The grammar must be re-entered to realize the Predicate.
The systems visited during this second traversal (not shown here) classify verb tenses in French.
Most of the selections during this second traversal were preselected during the first traversal.
For each verb tense, there is one associated structure, which contains a main verb and one or two auxiliaries.
To generate each verb or auxiliary, another traversal is needed.
In the first system of Figure 19, PRIMARY LOC, the selection depends on the temporal relation between the localized occurrence and the speech time.
The features of the systems ASPECT and SIT TYPE reflect the value of aspect in the semantic representation.
If the aspect is event, the system PERSPECTIVE determines if this event is presented using a perspective.
If there is one, another choice must be made regarding its type.
122 Gagnon and Lapalme From Conceptual Time to Linguistic Time,---past /I'~YL~ ~ present situation ASI~CT event ~RS~CTIVE E resultirlg open I.
Figure 20 Generation of verb phrase--sentence level.
perspective no perspective PERSPECTIVfi a n teri ority TYPE posteriority Table 9 Production of VP--examples.
Selections during first traversal Tense selected during Example second traversal past situation resulting past situation open past event perspective anteriority past event perspective posteriority past event no perspective plus-que-parfait pass4 ant6rieur imparfait imparfait conditionnel plus-que-parfait pass6 compos6 A 8h00, il avait terrain4.
(At 8:00, he had finished).
Une fois qu'il eut termin4 (Once he had finished) A 8h00, Robert regardait la t616vision.
(At 8:00, Robert was watching television).
J'ai rencontr4 Robert jeudi dernier.
I1 partait le lendemain.
(I met Robert last Thursday.
He was going to leave the day after).
J'ai rencontr6 Robert jeudi dernier.
I1 m'a dit qu'il partirait le lendemain.
(I met Robert last Thursday.
He told me that he would leave the next day).
J'ai rencontr6 Robert jeudi dernier.
I1 4tait arriv4 la veille.
(I met Robert last Thursday.
He had arrived the day before).
Robert a parl4 ~ Marie.
(Robert talked to Marie).
Table 9 shows examples of verb phrases, including the list of features selected for each example during the two traversals.
The same tense can be used for different feature patterns.
This is the case with the imparfait and the plus-que-parfait: the imparfait expresses an open situation or an anterior perspective, while the plus-que-parfait presents a resulting situation or a posterior perspective.
This may be a problem in an understanding process, since it is a source of ambiguity, but not in a generation process since it does not matter if two different inputs map into the same syntactic structure.
More than one verb tense may be used for the same features.
This means that our grammar is not complete: more systems would be needed to distinguish among these different cases.
For example, to distinguish the two tenses used with the first feature pattern of Table 9, we would have to augment the grammar section of Figure 19 123 Computational Linguistics Volume 22, Number 1 to determine if the verb phrase is part of a temporal adverbial or not.
For the two cases in the third feature pattern, the difference relates to the use of indirect discourse.
Here, not only would the grammar have to be modified, but so would the semantic representation, to take into account indirect speech.
In Discourse 1, this problem is not apparent: all verb tenses used are distinguished in our grammar because we limited ourselves to a subset of the data.
Thus, the production of VP is more complex than what we have implemented and we have not completely identified all the rules for the selection of verb tense: indirect discourse is not implemented and we have not identified how modal information can be used to select forms such as the subjunctive and the conditional.
But our approach is a good start and it could be extended by adding more systems and their selection rules, without changing the overall structure of the network.
We can see from the approximate translations given in Table 9 that the systems for generating French and English verb tenses differ greatly.
For English verb tenses, the method implemented in Nigel resorts to a recursive semantics involving temporal markers, as proposed by Halliday (Matthiessen and Bateman 1991).
The purpose in this approach is to deal correctly with complex structures, such as will have been eating.
Put simply, the idea is that each auxiliary reflects a relation between two temporal markers.
This suggests a network that displays a recursive process.
Thus, the phrase will have been going to eat would be represented semantically as to eat at a time that is in the future relative to another time that is in the past relative to a time that is in the future relative to speech time.
This method may be adequate for English, since it seems to capture the recursive structure of verb tenses; but in French, this recursive structure is not found.
Furthermore, nothing is said about how a deep generation process could produce the corresponding semantic structure with the intermediate temporal markers.
In fact, we are not convinced that this could be easily done.
Rather, we think that it is the overall structure that is selected for a particular usage.
This completes our brief description of the generation of verb tenses.
We have not completely solved the problem.
In particular, we have chosen to put most of the problems pertaining to verb tense in the deep generation process, in order to facilitate their generation at the surface level.
This approach greatly simplifies the process and our grammar could be easily completed to encompass all cases.
Once the semantic demands are better understood, it should be easier to solve the problem of deep generation.
7. Conclusion and Future Work In this paper, we have presented a method that has been successfully used to produce text conveying temporal information.
Our method combines the principles of two theories: Kamp's Discourse Representation Theory, which guides the expression of temporal information, and Halliday's Systemic Functional Grammar, which provides a generation process controlled by a set of semantic choices, with the syntactic form resulting from these choices.
We argued for the use of a conceptual structure, a Discourse Representation Structure, combined with rhetorical principles and pragmatic information, and for its translation into a semantic structure that is easily realized syntactically.
The deep generation process is hard to implement, mainly because of the difficulty in formalizing this information.
Since we assume that the deep generation 124 References 1 James F.
Allen, Maintaining knowledge about temporal intervals, Communications of the ACM, v.26 n.11, p.832-843, Nov.
1983 2 Asher, Nicholas.
(1993). Reference to Abstract Objects in Discourse.
Kluwer Academics, Dordrecht.
3 Bach, Emmon.
(1986). The Algebra of Events.
Linguistics and Philosophy, 9(1): 5--16.
4 Berry, M.
(1975). An Introduction to Systemic Linguistics, vol.
1 Structures
and Systems.
St. Martin Press, New York.
5 Orna
Berry, Mathias Hein, Ellon Littwitz, Introduction to Ethernet Switching, International Thomson Computer Press, Boston, MA, 1995 6 Bras, M.
(1990). Calcul des structures temporelles du discours.
Thse de doctorat, Universit Paul-Sabatier.
7 Bras, Myriam and Nicholas Asher.
(1994). Le raisonnement non monotone dans la construction de la structure temporelle de textes en franais.
In 9me congrs AFCET-RFIA.
Paris. 8 Bras, Myriam and Molins, Frdrique.
(1993). "Adverbials of Temporal Location: Linguistic Description and Automatic Processing".
In Sprache Kommunikation Informatik.
Linguistiche Arbeiten 293, edited by J.
Darski and Z.
Vetulani. Max Niemeyer Verlag, Tubingen.
9 Davidson, D.
(1967). "The Logical Form of Action Sentences".
In Essays on Action and Events, edited by D.
Davidson. Clarendon Press.
10 Dowty, D.
(1979). Word Meaning and Montague Grammar.
Reidel, Dordrecht.
11 Dowty, D.
(1982). Tenses, Time Adverbs, and Compositional Semantic Theory.
Linguistics and Philosophy, 5(1): 23--55.
12 Dowty, D.
(1986). The Effects of Aspectual Class on the Temporal Structure of Discourse: Semantics or Pragramatics?
Linguistics and Philosophy, 9: 37--61.
13 Ehrich, Veronika.
(1987). "The generation of tense".
In Natural Language Generation: New Results in Artificial Intelligence, Psychology and Linguistics, edited by Gerard Kempen.
Martinus Nijhoff Publishers, Boston, Dordrecht, 423--440.
14 Fawcett, R.
(1988). "Language Generation as Choice in Social Interaction".
In Advances in Natural Language Generation---An Interdisciplinary Perspective, edited by Michael Zock and Gerard Sabah.
Pinter Publishers, London, 27--48.
15 Forster, D.
(1989). Generating Temporal Expressions in Natural Language.
In Proceedings, 11th Annual Conference of the Cognitive Science Society.
16 Gagnon, M., and Lapalme, G.
(1992). Un gnrateur de texte exprimant des concepts temporels.
Technique et science informatiques, 11(2): 25--44.
17 Gagnon, Michel and Bras, Myriam.
(1994). "Discourse Interpretation and Time Representation".
Technical report 94/54-r, IRIT.
18 Halliday, M.
(1985). An Introduction to Functional Grammar.
Edward Arnold, London.
19 Hovy, Eduard H.
(1991). "Approaches to Planning of Coherent Text".
In Natural Language Generation in Artificial Intelligence and Computational Linguistics, edited by W.
Swartout and W.
Mann. Kluwer Academics Publishers, Boston, 83--102.
20 Kamp, H.
(1979). "Events, Instants and Temporal Reference".
In Semantics from different points of view, edited by R.
Bauerle, U.
Egli, and A.
von Stechow.
Springer Verlag, Berlin, 376--417.
21 Kamp, H.
(1981). Evnements, reprsentations discursives et rfrence temporelle.
Langages, 64: 34--64.
22 Lascarides, Alex, and Asher, Nicholas.
(1993). Temporal Interpretation, Discourse Relations and Commonsense Entailment.
Linguistics and Philosophy, 16: 437--493.
23 Mann, W.
(1983). "An Overview of the Nigel Generation Grammar".
Technical report ISI-RR-83-113, USC/ISI.
24 Mann, W.
(1985). An Introduction to the Nigel Text Generation System.
In Systemic Perspective on Discourse, vol.
1, edited by James D.
Benson and William S.
Greaves. Ablex, Norwood, 84--95.
25 Mann, W., and Thompson, S.
(1987). "Rhetorical Structure Theory: Description and Construction of Text Structure".
In Natural Language Generation, edited by Gerard Kempen.
Martinus Nijhoff, Dordrecht, 85--95.
26 William C.
Mann, Discourse structures for text generation, Proceedings of the 22nd annual meeting on Association for Computational Linguistics, p.367-375, July 02-06, 1984, Stanford, California 27 Matthiessen, C.
(1985). The Systemic Framework in Text Generation: Nigel.
In Systemic Perspective on Discourse, vol.
1, edited by James D.
Benson and William S.
Greaves. Ablex, Norwood, 96--118.
28 Matthiessen, C., and Bateman, J.
(1991). Text Generation and Systemic-Functional Linguistics.
Pinter, London.
29 Maybury, Mark T.
(1991). Topical, Temporal, and Spatial Constraints on Linguistic Realization.
Computational intelligence, 7(4): 266--275.
30 Kathleen R.
McKeown, Discourse strategies for generating natural-language text, Artificial Intelligence, v.27 n.1, p.1-41, Sept.
1985 31 Molins, F.
(1990). Acceptabilit et interprtation des adverbiaux de localisation temporelle.
Mmorie de D.E.A., Universit de Toulouse---Le Mirail.
32 Reichenbach, H.
(1947). Elements of Symbolic Logic.
McMillan, New York.
33 Verkuyl, H.
J. (1989).
Aspectual Classes and Aspectual Composition.
Linguistics and Philosophy, 12: 39--94.
34 Vlach, Frank.
(1993). Temporal Adverbials, Tenses and the Perfect.
Linguistics and Philosophy, 16: 231--283.
35 Winograd, T.
(1983). Language as a Cognitive Process.
Addison Wesley .
Finite-State Transducers in Language and Speech Processing Mehryar Mohri* AT&T Labs-Research Finite-state machines have been used in various domains of natural language processing.
We consider here the use of a type of transducer that supports very efficient programs: sequential transducers.
We recall classical theorems and give new ones characterizing sequential string-tostring transducers.
Transducers that output weights also play an important role in language and speech processing.
We give a specific study of string-to-weight transducers, including algorithms for determinizing and minimizing these transducers very efficiently, and characterizations of the transducers admitting determinization and the corresponding algorithms.
Some applications of these algorithms in speech recognition are described and illustrated.
1. Introduction Finite-state machines have been used in many areas of computational linguistics.
Their use can be justified by both linguistic and computational arguments.
Linguistically, finite automata are convenient since they allow one to describe easily most of the relevant local phenomena encountered in the empirical study of language.
They often lead to a compact representation of lexical rules, or idioms and cliches, that appears natural to linguists (Gross 1989).
Graphic tools also allow one to visualize and modify automata, which helps in correcting and completing a grammar.
Other more general phenomena, such as parsing context-free grammars, can also be dealt with using finitestate machines such as RTN's (Woods 1970).
Moreover, the underlying mechanisms in most of the methods used in parsing are related to automata.
From the computational point of view, the use of finite-state machines is mainly motivated by considerations of time and space efficiency.
Time efficiency is usually achieved using deterministic automata.
The output of deterministic machines depends, in general linearly, only on the input size and can therefore be considered optimal from this point of view.
Space efficiency is achieved with classical minimization algorithms (Aho, Hopcroft, and Ullman 1974) for deterministic automata.
Applications such as compiler construction have shown deterministic finite automata to be very efficient in practice (Aho, Sethi, and Ullman 1986).
Finite automata now also constitute a rich chapter of theoretical computer science (Perrin 1990).
Their recent applications in natural language processing, which range from the construction of lexical analyzers (Silverztein 1993) and the compilation of morphological and phonological rules (Kaplan and Kay 1994; Karttunen, Kaplan and Zaenen 1992) to speech processing (Mohri, Pereira, and Riley 1996) show the usefulness of finite-state machines in many areas.
In this paper, we provide theoretical and algorithmic bases for the use and application of the devices that support very efficient programs: sequential transducers.
* 600 Mountain Avenue, Murray Hill, NJ 07974, USA.
(~) 1997 Association for Computational Linguistics Computational Linguistics Volume 23, Number 2 We extend the idea of deterministic automata to transducers with deterministic input, that is, machines that produce output strings or weights in addition to (deterministically) accepting input.
Thus, we describe methods consistent with the initial reasons for using finite-state machines, in particular the time efficiency of deterministic machines, and the space efficiency achievable with new minimization algorithms for sequential transducers.
Both time and space concerns are important when dealing with language.
Indeed, one of the recent trends in language studies is a large increase in the size of data sets.
Lexical approaches have been shown to be the most appropriate in many areas of computational linguistics ranging from large-scale dictionaries in morphology to large lexical grammars in syntax.
The effect of the size increase on time and space efficiency is probably the main computational problem of language processing.
The use of finite-state machines in natural language processing is certainly not new.
The limitations of the corresponding techniques, however, are pointed out more often than their advantages, probably because recent work in this field is not yet described in computer science textbooks.
Sequential finite-state transducers are now used in all areas of computational linguistics.
In the following sections, we give an extended description of these devices.
We first consider string-to-string transducers, which have been successfully used in the representation of large-scale dictionaries, computational morphology, and local grammars and syntax, and describe the theoretical bases for their use.
In particular, we recall classical theorems and provide some new ones characterizing these transducers.
We then consider the case of sequential string-to-weight transducers.
Language models, phone lattices, and word lattices are among the objects that can be represented by these transducers, making them very interesting from the point of view of speech processing.
We give new theorems extending the known characterizations of stringto-string transducers to these transducers.
We define an algorithm for determinizing string-to-weight transducers, characterize the unambiguous transducers admitting determinization, and describe an algorithm to test determinizability.
We also give an algorithm to minimize sequential transducers that has a complexity equivalent to that of classical automata minimization and that is very efficient in practice.
Under certain restrictions, the minimization of sequential string-to-weight transducers can also be performed using the determinization algorithm.
We describe the corresponding algorithm and give the proof of its correctness in the appendix.
We have used most of these algorithms in speech processing.
In the last section, we describe some applications of determinization and minimization of string-to-weight transducers in speech recognition, illustrating them with several results that show them to be very efficient.
Our implementation of the determinization is such that it can be used on the fly: only the necessary part of the transducer needs to be expanded.
This plays an important role in the space and time efficiency of speech recognition.
The reduction in the size of word lattices that these algorithms provide sheds new light on the complexity of the networks involved in speech processing.
2. Sequential String-to-String Transducers Sequential string-to-string transducers are used in various areas of natural language processing.
Both determinization (Mohri 1994c) and minimization algorithms (Mohri 1994b) have been defined for the class of p-subsequential transducers, which includes sequential string-to-string transducers.
In this section, the theoretical basis of the use of sequential transducers is described.
Classical and new theorems help to indicate the usefulness of these devices as well as their characterization.
270 Mohri Transducers in Language and Speech b:E Figure 1 Example of a sequential transducer.
2.1 Sequential
Transducers We consider here sequential transducers, namel3~ transducers with a deterministic input.
At any state of such transducers, at most one outgoing arc is labeled with a given element of the alphabet.
Figure 1 gives an example of a sequential transducer.
Notice that output labels might be strings, including the empty string ~.
The empty string is not allowed on input, however.
The output of a sequential transducer is not necessarily deterministic.
The one in Figure 1 is not since, for instance, two distinct arcs with output labels b leave the state 0.
Sequential transducers are computationally interesting because their use with a given input does not depend on the size of the transducer but only on the size of the input.
Since using a sequential transducer with a given input consists of following the only path corresponding to the input string and in writing consecutive output labels along this path, the total computational time is linear in the size of the input, if we consider that the cost of copying out each output label does not depend on its length.
Definition More formally, a sequential string-to-string transducer T is a 7-tuple (Q, i, F, G, A, 6, or), with:  Q the set of states,  i E Q the initial state,  F c Q the set of final states,  ~ and A finite sets corresponding respectively to the input and output alphabets of the transducer,  6 the state transition function, which maps Q x G to Q,   the output function, which maps Q x G to A'.
The functions 6 and rr are generally partial functions: a state q c Q does not necessarily admit outgoing transitions labeled on the input side with all elements of the alphabet.
These functions can be extended to mappings from Q x G* by the following classical recurrence relations: Vs E Q, Vw E ~*,Va c G, 6(s,) = s, 6(s, wa) = 6(6(s,w),a); = wa) = 271 Computational Linguistics Volume 23, Number 2 Figure 2 Example of a 2-subsequential transducer T1.
Thus, a string w E ~* is accepted by Tiff 6(i, w) C F, and in that case the output of the transducer is or(i, w).
2.2 Subsequential
and p-Subsequential Transducers Sequential transducers can be generalized by introducing the possibility of generating an additional output string at final states (Sch~itzenberger 1977).
The application of the transducer to a string can then possibly finish with the concatenation of such an output string to the usual output.
Such transducers are called subsequential transducers.
Language processing often requires a more general extension.
Indeed, the ambiguities encountered in language--ambiguity of grammars, of morphological analyzers, or that of pronunciation dictionaries, for instance---cannot be taken into account when using sequential or subsequential transducers.
These devices associate at most a single output to a given input.
In order to deal with ambiguities, one can introduce p-subsequential transducers (Mohri 1994a), namely transducers provided with at most p final output strings at each final state.
Figure 2 gives an example of a 2-subsequential transducer.
H~re, the input string w = aa gives two distinct outputs aaa and aab.
Since one cannot find any reasonable case in language in which the number of ambiguities would be infinite, p-subsequential transducers seem to be sufficient for describing linguistic ambiguities.
However, the number of ambiguities could be very large in some cases.
Notice that 1-subsequential transducers are exactly the subsequential transducers.
Transducers can be considered to represent mappings from strings to strings.
As such, they admit the composition operation defined for mappings, a useful operation that allows the construction of more complex transducers from simpler ones.
The result of the application of T 20 T 1 to a string s can be computed by first considering all output strings associated with the input s in the transducer T1, then applying T2 to all of these strings.
The output strings obtained after this application represent the result (T2 o T1)(S).
In fact, instead of waiting for the result of the application of T1 to be completely given, one can gradually apply T2 to the output strings of ~-1 yet to be completed.
This is the basic idea of the composition algorithm, which allows the transducer T 2 o T 1 to be directly constructed given T1 and 72.
We define sequential (resp.
p-subsequential) functions to be those functions that can be represented by sequential (resp.
p-subsequential) transducers.
We noted previously that the result of the composition of two transducers is a transducer that can be directly constructed.
There exists an efficient algorithm for the general case of the composition of transducers (transducers subsequential or not, having e-transitions or not, and with outputs in ~*, or in ~* t2 {oo} x T4+ U {oo}) (Mohri, Pereira, and Riley 1996).
The following theorem gives a more specific result for the case of subsequential and p-subsequential functions, which expresses their closure under composition.
We use the expression p-subsequential in two ways here.
One means that a finite number of 272 Mohri Transducers in Language and Speech Figure 3 Example of a subsequential transducer v2.
a ambiguities is admitted (the closure under composition matches this case), the second indicates that this number equals exactly p.
Theorem 1 Let f: E* --* A* be a sequential (resp.
p-subsequential) and g: A* -, f~* be a sequential (resp.
q-subsequential) function, then g of is sequential (resp.
pq-subsequential). Proof We prove the theorem in the general case of p-subsequential transducers.
The case of sequential transducers, first proved by Choffrut (1978), can be derived from the general case in a trivial way.
Let 7-1 be a p-subsequential transducer representing f, T1 = (Ql, i1,F1, E,A, 61,a1,pl), and 7"2 = (Q2, i2, F2, A,~,62, cr2, P2) a q-subsequential transducer representing g.
pl and p2 denote the final output functions of T1 and r2, which map F1 to (A*)P and F2 to (f~*)q, respectively, pl(r) represents, for instance, the set of final output strings at a final state r.
Define the pq-subsequential transducer T = (Q,i,F,E,f~,6,a,p) by Q = Q1 x Q2, i = (il, i2), F -{(91,92) E Q: 91 c F1,62(q2,p1(q1))f3 F2 ~ 0}, with the following transition and output functions: Va E E, V(ql, q2) E Q,6((ql, q2),a) = (61(ql,a),62(q2,al(ql,a))) o'((ql,q2),a) = cra(q2, crl(ql,a)) and with the final output function defined by: V(ql,q2) E F, p((ql, q2)) =a2(q2, Pl(ql))p2(6(q2, Pl(ql))) Clearly, according to the definition of composition, the transducer ~realizes g of.
The definition of p shows that it admits at most pq distinct output strings for a given input one.
This ends the proof of the theorem.
\[\] Figure 3 gives an example of a 1-subsequential or subsequential transducer T2.
The result of the composition of the transducers rl and T2 is shown in Figure 4.
States in the transducer 73 correspond to pairs of states of ;1 and "r2.
The composition consists essentially of making the intersection of the outputs of T1 with the inputs of ~'2.
Transducers admit another useful operation: union.
Given an input string w, a transducer union of "0 and T2 gives the set union of the strings obtained by application of 71 to w and r2 to w.
We denote by ~-1 + r2 the union of 7"1 and T2.
The following theorem specifies the type of the transducer 71 + ~'2, implying in particular the closure under union of p-subsequential transducers.
It can be proved in a way similar to the composition theorem.
Theorem 2 Let f: E* ~ A* be a sequential (resp.
p-subsequential) and g: E* --* A* be a sequential (resp.
q-subsequential) function, then g + f is 2-subsequential (resp.
(p + q)subsequential).
273 Computational Linguistics Volume 23, Number 2 Figure 4 2-subsequential transducer r3, obtained by composition of T1 and r2.
The union transducer T1 + ~-2 Can be constructed from rl and r2 in a way close to the union of automata.
One can indeed introduce a new initial state connected to the old initial states of rl and "r2 by transitions labeled with the empty string both on input and output.
But the transducer obtained using this construction is not sequential, since it contains c-transitions on the input side.
There exists, however, an algorithm to construct the union of p-subsequential and q-subsequential transducers directly as a p + q-subsequential transducer.
The direct construction consists of considering pairs of states (ql, q2), ql being a state of ~-1 or an additional state that we denote by an underscore, q2 a state of r2 or an additional state that we denote by an underscore.
The transitions leaving (ql, q2) are obtained by taking the union of the transitions leaving ql and q2, or by keeping only those of ql if q2 is the underscore state, similarly by keeping only those of q2 if ql is the underscore state.
The union of the transitions is performed in such a way that if ql and q2 both have transitions labeled with the same input label a, then only one transition labeled with a is associated to (O, q2)The output label of that transition is the longest common prefix of the output transitions labeled with a leaving ql and q2.
See Mohri (1996b) for a full description of this algorithm.
Figure 5 shows the 2-subsequential transducer obtained by constructing the union of the transducers rl and ;2 this way.
Notice that according to the theorem the result could be a priori 3-subsequential, but these two transducers share no common accepted string.
In such cases, the resulting transducer is max(p, q)-subsequential.
2.3 Characterization
and Extensions The linear complexity of their use makes sequential or p-subsequential transducers both mathematically and computationally of particular interest.
However, not all transducers, even when they realize functions (rational functions), admit an equivalent sequential or subsequential transducer.
Consider, for instance, the function f associated with the classical transducer represented in Figure 6; f can be defined by: 1 Vw c {x} +, f(w) = alwl if Iwl is even, (1) = blwl otherwise This function is not sequential, that is, it cannot be realized by any sequential transducer.
Indeed, in order to start writing the output associated to an input string w = x n, a or b according to whether n is even or odd, one needs to finish reading the whole input string w, which can be arbitrarily long.
Sequential functions, namely functions that 1 We denote by \]w\[ the length of a string w.
274 Mohri Transducers in Language and Speech a:E b:a Figure 5 2-subsequential transducer v4, union of 7-1 and r2.
x:a x:a x:b Figure 6 Transducer T with no equivalent sequential representation.
can be represented by sequential transducers do not allow such unbounded delays.
More generally, sequential functions can be characterized among rational functions by the following theorem: Theorem 3 (Ginsburg and Rose 1966) Letf be a rational function mapping G* to A*.
f is sequential iff there exists a positive integer K such that: Vu 6 G*,Va E G, 3w 6 A*, Iwl < K: f(ua) =f(u)w (2) In other words, for any string u and any element of the alphabet a, f(ua) is equal to f(u) concatenated with some bounded string.
Notice that this implies that flu) is always a prefix of f(ua), and more generally that if f is sequential then it preserves prefixes.
275 Computational Linguistics Volume 23, Number 2 Q x:xl Figure 7 Left-to-right sequential transducer L.
x.x2 xl:a Figure 8 Right-to-left sequential transducer R.
The fact that not all rational functions are sequential could reduce the interest of sequential transducers.
The following theorem, due to Elgot and Mezei (1965), shows, however, that transducers are exactly compositions of left and right sequential transducers.
Theorem 4 (Elgot and Mezei 1965) Letf be a partial function mapping G" to A*.f is rational iff there exists a left sequential function h ~* --* f~* and a right sequential function r: f~* --* A ~ such thatf = r o I.
Left sequential functions or transducers are those we previously defined.
Their application to a string proceeds from left to right.
Right sequential functions apply to strings from right to left.
According to the theorem, considering a new sufficiently large alphabet f~ allows one to define two sequential functions I and r that decompose a rational function f.
This result considerably increases the importance of sequential functions in the theory of finite-state machines as well as in the practical use of transducers.
Berstel (1979) gives a constructive proof of this theorem.
Given a finite-state transducer T, one can easily construct a left sequential transducer L and a right sequential transducer R such that R o L = T.
Intuitively, the extended alphabet f~ keeps track of the local ambiguities encountered when applying the transducer from left to right.
A distinct element of the alphabet is assigned to each of these ambiguities.
The right sequential transducer can be constructed in such a way that these ambiguities can then be resolved from right to left.
Figures 7 and 8 give a decomposition of the nonsequential transducer T of Figure 6.
The symbols of the alphabet f~ = {xl, x2} store information about the size of the input string w.
The output of L ends with xl iff Iwl is odd.
The right sequential function R is then easy to construct.
276 Mohri Transducers in Language and Speech Sequential transducers offer other theoretical advantages.
In particular, while several important tests, such as equivalence, are undecidable with general transducers, sequential transducers have the following decidability property.
Theorem 5 Let T be a transducer mapping G* to A ".
It is decidable whether T is sequential.
A constructive proof of this theorem was given by Choffrut (1978).
An efficient polynomial algorithm for testing the sequentiability of transducers based on this proof was given by Weber and Klemm (1995).
Choffrut also gave a characterization of subsequential functions based on the definition of a metric on G*.
Denote by u/~ v the longest common prefix of two strings u and v in G*.
It is easy to verify that the following defines a metric on G*: d(u,v) = tul + Ivl 21u A v I (3) The following theorem describes this characterization of subsequential functions.
Theorem 6 Letf be a partial function mapping G* to A*.
f is subsequential iff: 1.
f has bounded variation (according to the metric defined above).
2. for any rational subset Y of A*,f-I(Y) is rational.
The notion of bounded variation can be roughly understood here as follows: if d(x,y) is small enough, namely if the prefix that x and y share is sufficiently long compared to their lengths, then the same is true of their images by f, f(x) and f(y).
This theorem can be extended to describe the case of p-subsequential functions by defining a metric d~ on (A*)p.
For any u = (u I.....,Up) and v = (vl .....,Vp) C (A*)P, we define: doo(u, v) = max d(ui, vi) (4) 1Ki~p Theorem 7 Let f = (fl ..... fp) be a partial function mapping Dom(f) c G* to (A*)P.
f is psubsequential iff: 1.
f has bounded variation (using the metric d on G* and d~ on (A*)p).
2. for all i (1 < i < p) and any rational subset Y of A*,f/-I(Y) is rational.
Proof Assume f p-subsequential, and let T be a p-subsequential transducer realizing f.
A transducer Ti, 1 ~ i < p, realizing a component fi of f can be obtained from T simply by keeping only one of the p outputs at each final state of T.
Ti is subsequential by construction, hence the component )~ is subsequential.
Then the previous theorem implies that each component fi has bounded variation, and by definition of d~, f has also bounded variation.
Conversely, if the first condition holds, afortiori each)~ has bounded variation.
This combined with the second condition implies that each)~ is subsequential.
A transducer T realizing f can be obtained by taking the union of p subsequential transducers realizing each componentS.
Thus, in view of the theorem 2,f is p-subsequential.
\[\] 277 Computational Linguistics Volume 23, Number 2 One can also give a characterization of p-subsequential transducers irrespective of the choice of their components.
Let d~ be the semimetric defined by: V(u,v) E \[(A*)P\] 2, d'p(U,V) = max d(ui, vj) l<i,j<p (5) The following theorem that follows then gives that characterization.
Theorem 8 Letf be a rational function mapping E* to (A*)P.
f is p-subsequential iff it has bounded variation (using the semimetric d~ on (A*)P).
Proof According to the previous theorem the condition is sufficient since: V(u,v) c < a'p(u,v) Conversely if f is p-subsequential, let T = (Q, i,F, ~, A, 6,or, p) be a p-subsequential transducer representing f, where p = (pl ..... pp) is the output function mapping Q to (A*)P.
Let N and M be defined by: N= max Ipi(q)l and M= max Icffq, a)l (6) qEF, l <id< p aE~,qEQ We denote by Dom(T) the set of strings accepted by T.
Let k > 0 and (ul, U2) E \[Dora(T)\] 2 such that d(ul, u2) _< k.
Then, there exists u E E* such that: U 1 = UVl, U 2 = UV2, and \[vii + Iv2l G k (7) Hence, f(Ul) = {cr(i,u)rr(~(i,u),vl)pj(6(i, ul)): 1 G j < p} f(u2) = {rr(i,u)cr(6(i,u),v2)Pj(~(i, u2)): 1 < j < p} (8) Let K = kM + 2N.
We have: dp(f(ul),f(u2)) M(ivl\[ + Iv2I) + dlp(p(6(i, ul)),p(~(i, u2))) < kM+2N = K Thus, f has bounded variation using d~.
This ends the proof of the theorem.
\[\] 2.4 Application to Language Processing We briefly mentioned several theoretical and computational properties of sequential and p-subsequential transducers.
These devices are used in many areas of computational linguistics.
In all those areas, the determinization algorithm can be used to obtain a p-subsequential transducer (Mohri 1996b), and the minimization algorithm to reduce the size of the p-subsequential transducer used (Mohri 1994b).
The composition, union, and equivalence algorithms for subsequential transducers are also useful in many applications.
278 Mohri Transducers in Language and Speech 2.4.1 Representation of Dictionaries.
Very large-scale dictionaries can be represented by p-subsequential dictionaries because the number of entries and that of the ambiguities they contain are finite.
The corresponding representation offers fast look-up since the recognition does not depend on the size of the dictionary but only on that of the input string considered.
The minimization algorithm for sequential and p-subsequential transducers allows the size of these devices to be reduced to the minimum.
Experiments have shown that these compact and fast look-up representations for large natural language dictionaries can be efficiently obtained.
As an example, a French morphological dictionary of about 21.2 Mb can be compiled into a p-subsequential transducer of 1.3 Mb, in a few minutes (Mohri 1996b).
2.4.2 Compilation
of Morphological and Phonological Rules.
Similarly, context-dependent phonological and morphological rules can be represented by finite-state transducers (Kaplan and Kay 1994).
Most phonological and morphological rules correspond to p-subsequential functions.
The result of the computation described by Kaplan and Kay (1994) is not necessarily a p-subsequential transducer.
But, it can often be determinized using the determinization algorithm for p-subsequentiable transducers.
This considerably increases the time efficiency of the transducer.
It can be further minimized to reduce its size.
These observations can be extended to the case of weighted rewrite rules (Mohri and Sproat 1996).
2.4.3 Syntax.
Finite-state machines are also currently used to represent local syntactic constraints (Silberztein 1993; Roche 1993; Karlsson et al.1995; Mohri 1994d).
Linguists can conveniently introduce local grammar transducers that can be used to disambiguate sentences.
The number of local grammars for a given language and even for a specific domain can be large.
The local grammar transducers are mostly p-subsequential.
Determinization and minimization can then be used to make the use of local grammar transducers more time efficient and to reduce their size.
Since p-subsequential transducers are closed under composition, the result of the composition of all local grammar transducers is a p-subsequential transducer.
The equivalence of local grammars can also be tested using the equivalence algorithm for sequential transducers.
For a more detailed overview of the applications of sequential string to string transducers to language processing, see Mohri (1996a).
Because they are so time and space efficient, sequential transducers will likely be used increasingly often in natural language processing as well as in other connected fields.
In the following, we consider the case of string-to-weight transducers, which are also used in many areas of computational linguistics.
3. Power Series and Subsequential String-to-Weight Transducers We consider string-to-weight transducers, namely transducers with input strings and output weights.
These transducers are used in various domains, such as language modeling, representation of word or phonetic lattices, etc., in the following way: one reads and follows a path corresponding to a given input string and outputs a number obtained by combining the weights along this path.
In most applications to natural language processing, the weights are simply added along the path, since they are interpreted as (negative) logarithms of probabilities.
In case the transducer is not sequential, that is, when it does not have a deterministic input, one proceeds in the same way for all the paths corresponding to the input string.
In natural language processing, specifically in speech processing, one keeps the minimum of the weights associated to 279 Computational Linguistics "Volume 23, Number 2 Figure 9 Example of a string-to-weight transducer.
these paths.
This corresponds to the Viterbi approximation in speech recognition or in other related areas for which hidden Markov models (HMM's) are used.
In all such applications, one looks for the best path, i.e., the path with the minimum weight.
3.1 Definitions
In this section, we give the definition of string-to-weight transducers and other deftnitions useful for the presentation of the theorems of the following sections.
In addition to the output weights of the transitions, string-to-weight transducers are provided with initial and output weights.
For instance, when used with the input string ab, the transducer in Figure 9 outputs: 5 + 1 + 2 + 3 = 11, 5 being the initial and 3 the final weight.
Definition More formally, a string-to-weight transducer T is defined by T = (Q, ~, I, F, E, A, p) with:  Q a finite set of states,  G the input alphabet,  I C Q the set of initial states,  F C Q the set of final states,  E C Q x G x T4+ x Q a finite set of transitions,  A the initial weight function mapping I to 7Z+,  p the final weight function mapping F to 7"4+.
One can define for T a transition (partial) function 6 mapping Q x E to 2 Q by: V(q,a) E Q x G,~(q,a) = {q' I 3x E 7"4+: (q,a,x,q') E E} and an output function cr mapping E to T4+ by: Vt = (p,a,x,q) E E, cr(t) = x A path ~r in T from q E Q to q' E Q is a set of successive transitions from q to q': 7r = ((qo, ao, xo, ql) ....., (qm-l,am-l, Xm-l, qm)), with Vi E \[0, m1\], qi+l E 6(qi, ai).
We can extend the definition of a to paths by: cr(Tr) -XoXl""Xm-1.
We denote by ~E q w q, the set of paths from q to q' labeled with the input string w.
The definition of 6 can be extended to Q x G* by: V(q,w) E Q x ~*,6(q,w) = {q': 3 path 7r in T, Tr E qW q,} 280 Mohri Transducers in Language and Speech and to 2 Q x E*, by: VR G Q, Vw E ~*, 6(R, w) = U t~(q, w) qER For (q, w, q') E Q x ~ x Q such that there exists a path from q to ql labeled with w, we define O(q, w, q') as the minimum of the outputs of all paths from q to q' with input w: O(q, w,q') = min cr(Tr) w IrEq...~q' A successful path in T is a path from an initial state to a final state.
A string w E E* is accepted by Tiff there exists a successful path labeled with w: w E 6(L w) N F.
The output corresponding to an accepted string w is then obtained by taking the minimum of the outputs of all successful paths with input label w: min (A(i) + O(i, w,f) + p(f)) (i,f)EIxF: fE6(i,w) A transducer T is said to be trim if all states of T belong to a successful path.
String-toweight transducers clearly realize functions mapping ~* to 74+.
Since the operations we need to consider are addition and min, and since (74+ U {oo}, min, +, cxD, 0) is a semiring, we call these functions formal power series.
2 We
adopt the terminology and notation used in formal language theory (Berstel and Reutenauer 1988; Kuich and Salomaa 1986; Salomaa and Soittola 1978): the image by a formal power series S of a string w is denoted by (S, w) and called the coefficient of w in S, the notation S = ~,w~.
(S, w)w is then used to define a power series by its coefficients, the support of S is the language defined by: suep(S) = {w (S,w) # The fundamental theorem of Schtitzenberger (1961), analogous to Kleene's theorem for formal languages, states that a formal power series S is rational iff it is recognizable, that is, realizable by a string-to-weight transducer.
The semiring (74+ U {cx~}, rain, +, c~, 0) used in many optimization problems is called the tropical semiring.
3 So, the functions we consider here are more precisely rational power series over the tropical semiring.
A string-to-weight transducer T is said to be unambiguous if for any given string w there exists at most one successful path labeled with w.
In the following, we examine, more specifically, efficient string-to-weight transducers: subsequential transducers.
A transducer is said to be subsequential if its input is 2 Recall that a semiring is essentially a ring that may lack negation, namely in which the first operation does not necessarily admit inversion.
(TZ, +,., 0,1), where 0 and 1 are, respectively, the identity elements for + and., or, for any non-empty set E, (2 E, U, n, 0, E), where 0 and E are, respectively, the identity elements for U and O, are other examples of semirings.
3 This
terminology is often used more specifically when the set is restricted to natural integers (Nu {oo},min, +,~,0).
281 Computational Linguistics Volume 23, Number 2 deterministic, that is if at any state there exists at most one outgoing transition labeled with a given element of the input alphabet G.
Subsequential string-to-weight transducers are sometimes called weighted automata, or weighted acceptors, or probabilistic automata, or distance automata.
Our terminology is meant to favor the functional view of these devices, which is the view that we consider here.
Not all string-to-weight transducers are subsequential but we define an algorithm to determinize nonsubsequential transducers when possible.
Definition More formally a string-to-weight subsequential transducer "r = (Q, i, F, ~, 6, or, )~, p) is an 8-tuple, with:  Q the set of its states,  i E Q its initial state,  F c_ Q the set of final states,  G the input alphabet,  6 the transition function mapping Q x E to Q, 6 can be extended as in the string case to map Q x G* to Q,  cr the output function, which maps Q x G to 7%+, cr can also be extended to Q x ~,*,  ;~ E T4+ the initial weight,  p the final weight function mapping F to T4+.
A string w E ~,* is accepted by a subsequential transducer T if there exists f E F such that 6(i, w) =f.
The output associated to w is then: )~ + or(i, w) + p(f).
We will use the following definition for characterizing the transducers that admit determinization.
Definition Two states q and q' of a string-to-weight transducer T = (Q, I, F, G, 6, rr, )~, p), not necessarily subsequential, are said to be twins if: V(u,v) E (~,)2, ({q,q,} C 6(I,u),q E 6(q,v),q' E 6(q',v)) ~ ~(q,v,q) = ~(q',v,q') (9) In other words, q and q' are twins if, when they can be reached from the initial state by the same string u, the minimum outputs of loops at q and q' labeled with any string v are identical.
We say that T has the twins property when any two states q and q' of T are twins.
Notice that according to the definition, two states that do not have cycles with the same string v are twins.
In particular, two states that do not belong to any cycle are necessarily twins.
Thus, an acyclic transducer has the twins property.
In the following section, we consider subsequential power series in the tropical semiring, that is, functions that can be realized by subsequential string-to-weight transducers.
Many rational power series defined on the tropical semiring considered in practice are subsequential, in particular, acyclic transducers represent subsequential power series.
282 Mohri Transducers in Language and Speech We introduce a theorem giving an intrinsic characterization of subsequential power series irrespective of the transducer realizing them.
We then present an algorithm that allows one to determinize some string-to-weight transducers.
We give a general presentation of the algorithm since it can be used with many other semirings, in particular, with string-to-string transducers and with transducers whose output labels are pairs of strings and weights.
We then use the twins property to define a set of transducers to which the determinization algorithm applies.
We give a characterization of unambiguous transducers admitting determinization, and then use this characterization to define an algorithm to test if a given transducer can be determinized.
We also present a very efficient minimization algorithm that applies to subsequential string-to-weight transducers.
In many cases, the determinization algorithm can also be used to minimize a subsequential transducer; we describe this use of the algorithm and give the related proofs in the appendix.
3.2 Characterization
of Subsequential Power Series Recall that one can define a metric on E* by: d(u,v) = lu\[ + Iv\[ 21u A v\[ (10) where we denote by u A v the longest common prefix of two strings u and v in E*.
The definition we gave for subsequential power series depends on the transducers representing them.
The theorem that follows gives an intrinsic characterization of subsequential power series.
4 Theorem
9 Let S be a rational power series defined on the tropical semiring.
S is subsequential iff it has bounded variation.
Proof Assume that S is subsequential.
Let ~= (Q, i, F, E, 6, or, A, p) be a subsequential transducer.
5 denotes the transition function associated with T, cr its output function, and )~ and p the initial and final weight functions.
Let L be the maximum of the lengths of all output labels of T: L= max \[cr(q,a)\[ (11) (q,a)CQxE and R the upper bound of all output differences at final states: R= max \[p(q)-p(q')\[ (12) (q,q')EF 2 and define M as M = L + R.
Let (Ul, u2) be in (E*) 2.
By definition of d, there exists u E E* such that: Ul = uvl, u2 = uv2, and Iv1\[ + Iv2\[ = d(ul,u2) (13) Hence, cr(i,u,) = cr(i,u) + cr(5(i,u),vl) (i, u2) = cr(i,u) + cr(6(i,u),v2) 4 This is an extension of the characterization theorem of Choffrut (1978) for string-to-string functions.
The extension is not straightforward because the length of an output string is a natural integer.
Here we deal with real numbers.
283 Computational Linguistics Volume 23, Number 2 Since \[(6(i,u),vl) ~(6(i,u),v2)l <_ L.
(Iv1\] + Iv21) = L.d(ul, u2) and \[p(6(i, ul))-p(~(i, u2))l <_ R we have IA+(i, ul) + p(6(i, ul)) A+~(i, u2) + p(6(i, u2))\[ <_ L.d(ul, u2) + a Notice that if ul # u2, R <_ R.
d(ul, u2).
Thus \[A + a(i, ul) + p(6(i, Ul)) )~ + or(i, u2) + p(6(i, u2))\[ <_ (L + n) . d(ul, u2) Therefore: V(Ul, U2) E (E*) 2, \[S(Ul)S(u2)\[ < M'd(Ul, U2) (14) This proves that S is M-Lipschitzian s and afortiori that it has bounded variation.
Conversely, suppose that S has bounded variation.
Since S is rational, according to the theorem of Schtitzenberger (1961) it is recognizable and therefore there exists a string-to-weight transducer ~= (Q,I,F, E, ~, or,,k, p) realizing S.
As in the case of string-to-string transducers, one can show that any transducer admits an equivalent trim unambiguous transducer.
So, without loss of generality we can assume T trim  and unambiguous.
Furthermore, we describe in the next sections a determinization algorithm.
We show that this algorithm applies to any transducer that has the twins property.
Thus, in order to show that S is subsequentiable, it is sufficient to show that ~has the twins property.
Consider two states q and q' of ~and let (u,v) E (E*) 2 be such that: {q, q'} c ~(1, u), q E ~(q, v), q' ~ ~(ql, v) Since ~is trim there exists (w,w') E (~.)2 such that 6(q,w) NF 0 and ~(q, w') NF # 0.
Notice that Vk >_ O, cl(uvkw, uvkw ') = a(w, w') Thus, since S has bounded variation 3K >_ 0,Vk _> 0, IS(uvkw) S(uv~w')l <_ K Since r is unambiguous, there is only one path from I to F corresponding to uvkw (resp.
uvkw'). We have: S(uvkw) = O(I, uw, F) +kO(q,v,q) S(uvkw ') = O(I, uw',F) + kO(q',v,q') 5 This implies in particular that the subsequential power series over the tropical semiring define continuous functions for the topology induced by the metric d on E*.
Also this shows that in the theorem one can replace has bounded variation by is Lipschitzian.
284 Mohri Transducers in Language and Speech 9 10 11 12 13 14 15 Figure 10 Power Series Determinization(T~, T2) 1 F2 *-0 2 )~2 ~ (~E~AI(i) iEI1 3 i2 *-U(i,)~21  ~1(i))} iEI1 4 Q ~ {/2} 5 while Q # 0 6 do q2 ~-head\[Q\] 7 if (there exists (q, x) E q2 such that q E F1) 8 then F2 *--F2 U {q2} /:}2 (q2) *-~ X @ PA (q) qEFl,(q,x) Eq2 for each a such that F(q2,a) # 0 do a2(q2,a) *(~ \[x (~ cq (t)\] (q,x)EI'(q2,a) t=(q,a,crl(t),nl(t))EE1 62(q2,a)*-U {(q" ED \[cr2(q2"a)\]-lx(gal(t)} q'Ev(q2,a) (q,x,t) ET(q2,a),nl(t)=q' if (62(q2,a) is a new state) then ENQUEUE(Q, 62(q2, a)) DEQUEUE(Q) Algorithm for the determinization of a transducer ~-~ representing a power series defined on the semiring (S, E3, , 0, i).
Hence 3K > 0,Vk _> 0, i(O(I, uw, F) O(I, uw',F)) + k(O(q,v,q) O(q',v,q')) I <_ K ==~ #(q,v,q) ~)(q',v,q') = 0 Thus T has the twins property.
This ends the proof of the theorem.
\[\] 3.3 General Determinization Algorithm for Power Series We describe in this section an algorithm for constructing a subsequential transducer "1" 2 = (Q2, i2, F2,~,~2, cr2,,,~2, P2 ) equivalent to a given nonsubsequential one ~-I = (Q1, G, I1, F1, El, A1, pl).
The algorithm extends our determinization algorithm for stringto-string transducers representing p-subsequential functions to the case of transducers outputting weights (Mohri 1994c).
Figure 10 gives the pseudocode of the algorithm.
We present the algorithm in the general case of a semiring (S, ~, , 0,1) on which the transducer T1 is defined.
Indeed, the algorithm we are describing here applies as well to transducers representing power series defined on many other semirings.
6 We
describe the algorithm in the case of the tropical semiring.
For the tropical semiring, one can replace @ by min and  by + in the pseudocode of Figure 10.
7 6 In particular, the algorithm also applies to string subsequentiable transducers and to transducers that output pairs of strings and weights.
We will come back to this point later.
7 Similarly, ~21 should be interpreted as -A, and \[~2(q2,a)\] -1 as -cr2(q2,a ).
285 Computational Linguistics Volume 23, Number 2 The algorithm is similar to the powerset construction used for the determinization of automata.
However, since the outputs of two transitions bearing the same input label might differ, one can only output the minimum of these outputs in the resulting transducer, therefore one needs to keep track of the residual weights.
Hence, the subsets q2 that we consider here are made of pairs (q, x) of states and weights.
The initial weight &2 of T2 is the minimum of all the initial weights of ~-1 (line 2).
The initial state i2 is a subset made of pairs (i, x), where i is an initial state of T1, and x = &l (i) )~2 (line 3).
We use a queue Q to maintain the set of subsets q2 yet to be examined, as in the classical powerset construction, s Initially, Q contains only the subset i2.
The subsets q2 are the states of the resulting transducer, q2 is a final state of T2 iff it contains at least one pair (q, x), with q a final state of ~1 (lines 7-8).
The final output associated to q2 is then the minimum of the final outputs of all the final states in q2 combined with their respective residual weight (line 9).
For each input label a such that there exists at least one state q of the subset q2 admitting an outgoing transition labeled with a, one outgoing transition leaving q2 with the input label a is constructed (lines 10-14).
The output o'2(q2, a) of this transition is the minimum of the outputs of all the transitions with input label a that leave a state in the subset q2, when combined with the residual weight associated to that state (line 11).
The destination state 62(q2, a) of the transition leaving q2 is a subset made of pairs (q', x'), where q' is a state of T1 that can be reached by a transition labeled with a, and x' the corresponding residual weight (line 12).
x' is computed by taking the minimum of all the transitions with input label a that leave a state q of q2 and reach q', when combined with the residual weight of q minus the output weight cr2(q2,a).
Finally, 62(q2,a) is enqueued in Q iff it is a new subset.
We denote by nl (t) the destination state of a transition t E El.
Hence nl (t) = q', if t -(q,a,x,q') E El.
The sets F(q2,a), 7(q2,a), and v(q2,a) used in the algorithm are defined by:  F(q2,a) = {(q,x) E q2: 3t = (q,a, rrl(t),nl(t)) E El}  7(q2,a) = {(q,x,t) E q2 x El: t= (q,a, cq(t),nl(t)) E El}  ~(q2,a) = {q': 3(q,x) E q2,3t = (q,a, rrfft),q') E El} F(q2, a) denotes the set of pairs (q, x), elements of the subset q2, having transitions labeled with the input a.
7(q2, a) denotes the set of triples (q, x, t) where (q, x) is a pair in q2 such that q admits a transition with input label a.
v(q2,a) is the set of states q' that can be reached by transitions labeled with a from the states of the subset q2.
The algorithm is illustrated in Figures 11 and 12.
Notice that the input ab admits several outputs in #1:{1 + 1 = 2,1 + 3 = 4,3 + 3 = 6,3 + 5 = 8}.
Only one of these outputs (2, the smallest) is kept in the determinized transducer 1'2, since in the tropical semiring one is only interested in the minimum outputs for any given string.
Notice that several transitions might reach the same state with a priori different residual weights.
Since one is only interested in the best path, namely the path corresponding to the minimum weight, one can keep the minimum of these weights for a given state element of a subset (line 11 of the algorithm of Figure 10).
In the next section, we give a set of transducers "rl for which the determinization algorithm terminates.
The following theorem shows the correctness of the algorithm when it terminates.
8 The
algorithm works with any queue discipline chosen for Q.
286 Mohri Transducers in Language and Speech Figure 11 Transducer #1 representing a power series defined on (7"4+ U {o0}, min, +).
Figure 12 Transducer #2 obtained by power series determinization of #1.
Theorem 10 Assume that the determinization algorithm terminates, then the resulting transducer ~'2 is equivalent to ~1.
Proof We denote by Offq, w,q') the minimum of the outputs of all paths from q to q'.
By construction we have: )~2 "~-min/~1 (il) il E I1 We define the residual output associated to q in the subset (~2(/2, W) as the weight c(q, w) associated to the pair containing q in ~2(i2, w).
It is not hard to show by induction on Iwl that the subsets constructed by the algorithm are the sets 62(i2, w), w E ~*, such that: Vw E E*, ~2(i2,w) = U {(q,c(q,w)} qE61 (ll,w) c(q,w) = m~1~1(/~1(/1 ) q-Ol(il, w,q)) -cr2(/2,w) --/~2 ff2(i2,w) = min w (/~1(/1) qOl(il, w,q)) -~2 q~1(I1, ) (15) 287 Computational Linguistics Volume 23, Number 2 Notice that the size of a subset never exceeds \[QI\[: card(62(i2,w)) ~ IQI\[.
A state q belongs at most to one pair of a subset, since for all paths reaching q, only the minimum of the residual outputs is kept.
Notice also that, by definition of min, in any subset there exists at least one state q with a residual output c(q, w) equal to 0.
A string w is accepted by ~-1 iff there exists q E F1 such that q c 61 (I1, w).
Using equations 15, it is accepted iff 62(i2,w) contains a pair (q,c(q,w)) with q E F1.
This is exactly the definition of the final states F2 (line 7).
So ~1 and T2 accept the same set of strings.
Let w C E* be a string accepted by ~1 and ~-2.
The definition of p2 in the algorithm of figure 10, line 9, gives: p2(62(i2,w)) = rain pl(q) + m'.m(Al(h) + 81(il, w,q)) a2(i2,w) )~2 (16) qE 61 ( II,w )NF1 ll El1 Thus, if we denote by S the power series realized by "rl, we have: p2(62(i2,w)) = (S,w) cr2(/2,w) )~2 (17) Hence: &2 + cr2(i2, w) + p2(62(/2, w)) = (S, w).
\[\] The power series determinization algorithm is equivalent to the usual determinization of automata when the initial weight, the final weights, and all output labels are equal to 0.
The subsets considered in the algorithm are then exactly those obtained in the powerset determinization of automata, all residual outputs c(q, w) being equal to 0.
Both space and time complexity of the determinization algorithm for automata are exponential.
There are minimal deterministic automata with exponential size with respect to an equivalent nondeterministic one.
A fortiori the complexity of the determinization algorithm in the weighted case we just described is also exponential.
However, in some cases in which the degree of nondeterminism of the initial transducer is high, the determinization algorithm turns out to be fast and the resulting transducer has fewer states than the initial one.
We present examples of such cases, which appear in speech recognition, in the last section.
We also present a minimization algorithm that allows the size of subsequential transducers representing power series to be reduced.
The complexity of the application of subsequential transducers is linear in the size of the string to which it applies.
This property makes it worthwhile to use the power series determinization to speed up the application of transducers.
Not all transducers can be determinized using the power series determinization.
In the following section, we define a set of transducers that admit determinization, and characterize unambiguous transducers that admit the application of the algorithm.
Since determinization does not apply to all transducers, it is important to be able to test the determinizability of a transducer.
We present, in the next section, an algorithm to test this property in the case of unambiguous trim transducers.
The proofs of some of the theorems in the next two sections are complex; they can be skipped on first reading.
3.4 Determinizable
Transducers There are transducers with which determinization does not halt, but rather generates an infinite number of subsets.
We define determinizable transducers as those transducers with which the algorithm terminates.
We first show that a large set of transducers 288 Mohri Transducers in Language and Speech admit determinization, then give a characterization of unambiguous transducers admitting determinization.
In what follows, the states of the transducers considered will be assumed to be accessible from the initial one.
The following lemma will be useful in the proof of the theorems.
Lemma 1 W Let T = (Q, E, I, F, E, A, p) be a string-to-weight transducer, ~r E p "-* q a path in T from the state p ~ Q to q ~ Q, and ~" E p',G q' a path from p' E Q to q' ~ Q both labeled with the input string w ~ E*.
Assume that the lengths of 7r and ~-' are greater than \[Q\[2 _ 1, then there exist strings Ul, u2, u3 in E*, and states pit p2, p~r and p~ such that \[u2\[ > 0, UlU2U3 = w and such that 7r and 7r' be factored in the following way: 7r' E p,,,.,u' P~',,~ p~,,~ q, Proof The proof is based on the use of a cross product of two transducers.
Given two transducers T1 = (Q1, E, I1, F1, El, A1, pl) and T2 = (Q2, G,/2, F2, E2, A2, #2), we define the cross product of T1 and T2 as the transducer: T1 X T2 = (Q1 x Q2,G, I1 x I2,F1 x F2,E,A,#) with outputs in 7"4+ x T4+ such that t = ((ql, q2), a, (Xl, x2), (q~, q~)) E Q1 x E x 14+ x 74+ x Q2 is a transition of T1 x T2, namely t E E, iff (ql,a, xl, q~) E E1 and (q2,a, x2,2) E E2.
We also define A and p by: V(i1,i2) E/1 x I2, A(il, i2) = (Al(i1),A2(i2)), V(fl,f2) E F1 x F2, #(fl,f2) ---(Pl 0Cl ), P20c2)).
Consider the cross product of T with itself, T x T.
Let 7r and 7r' be two paths in T with lengths greater than IQI 2 1, (m > IQI2 _ 1): ~r = ( (p = qo, ao, xo, ql) .....
(qm-l,am-l, Xm-l, qm = q) ) V V z ! a x ! ! 7r' = ((p' = q'o, ao, xo, qO .....
~qm-1, m-I, m-l, qm = q')) then: II = (((q0, q~), a0, (x0, x~), (ql, q~)),  ., ((qm-1, q~-x), am-I, (Xm-l., Xm_ 1),, (qm, Cm))) is a path in T x T with length greater than \[Q\]2 _ 1.
Since T x T has exactly \[Q\[2 states, II admits at least one cycle at some state (pl, p~) labeled with a non-empty input string u2.
This shows the existence of the factorization above and proves the lemma.
\[\] Theorem 11 Let 7-1 -(Q1, E, I1, F1, El, A1, pl) be a string-to-weight transducer defined on the tropical semiring.
If T1 has the twins property then it is determinizable.
Proof Assume that ~has the twins property.
If the determinization algorithm does not halt, there exists at least one subset of 2 Q, {q0 ..... qm}, such that the algorithm generates an infinite number of distinct weighted subsets {(q0, Co) .....
(qm, Cm)}.
289 Computational Linguistics Volume 23, Number 2 Then we have necessarily m > 1.
Indeed, we mentioned previously that in any subset there exists at least one state qi with a residual output ci = 0.
If the subset contains only one state qo, then Co = 0.
So there cannot be an infinite number of distinct subsets ((qo, Co)}.
Let A c G* be the set of strings w such that the states of 62(i2, w) be {qo ..... qm}.
We have: Vw E A, ~2(/2, W) = {(q0, c(qo, w)),..., (qm, C(qm, w))}.
Since A is infinite, and since in each weighted subset there exists a null residual output, there exist i0, 0 ~ i0 ~ m, such that c(q/0, w) -0 for an infinite number of strings w E A.
Without loss of generality we can assume that i0 -0.
Let B C_ A be the infinite set of strings w for which c(q0, w) = 0.
Since the number of subsets ((qo, c(qo, w)) .....
(qm, C(qm, W))}, w E B, is infinite, there exists j, 0 < j _< m, such that c(qj, w) be distinct for an infinite number of strings w E B.
Without loss of generality we can assume j = 1.
Let C c B be an infinite set of strings w with c(ql, w) all distinct.
Define R(qo, ql) to be the finite set of differences of the weights of paths leading to q0 and ql labeled with the same string w, \[w\[ _< \[Qll 2 1: R(qo, q,) = {(A(i,) +~(~-,)) (A(io) + (~'o)): 7to E io w qo, Tr, E i, w q', io E/,il E/,Iw I _( IQ, I2 1} We will show that {c(ql, w): w E C} C_ R(qo, ql).
This will yield a contradiction with the infinity of C, and will therefore prove that the algorithm terminates.
Let w E C, and consider a shortest path 7r0 from a state i0 E I to q0 labeled with the input string w and with total cost c~(zr0).
Similarly consider a shortest path ~rl from il E I to ql labeled with the input string w and with total cost cr(zrl).
By definition of the subset construction we have: (A(h) + ~r(~rl)) (A(/o) + er(~r0)) = c(ql, w).
Assume that Iw\[ > \[Q1\] 2 1.
Using the lemma 1, there exists a factorization of ~r0 and zrl of the type: ~ro E io "~ po "~ po d~ qo 7rl E il d2, pl ~ pl ~ ql with \]u2\] > 0.
Since Po and pl are twins, 01(po, u2,po) = 01(pl, u2,Pl).
Define zr~ and ~r~ by: ~r~ E i0 G p0 "~ q0 7r~ E il G pl "~ ql Since ~r and ~r' are shortest paths, we have: a(Tro) = cr(zr~) + 01(po, u2,po) and a0rl ) = cr(zr~) + 01 (Pl, u2, pl).
Hence: (A(il) + rr0r~)) (A(io) + a(Tr~)) = c(ql, w).
By induction on \]w I, we can therefore find shortest paths Ho and H1 from io to qo (resp.
il to ql) with length less or equal to \]Q112 1 and such that (A(h) +a(H1)) (A(io) +a(Ho)) = c(ql, w).
Since a(H1) a(IIo) E R(qo, ql), c(ql, w) E R(qo, ql) and C is finite.
This ends the proof of the theorem.
\[\] There are transducers that do not have the twins property and that are still determinizable.
To characterize such transducers, more complex conditions that we will not describe here are required.
However, in the case of trim unambiguous transducers, the twins property provides a characterization of determinizable transducers.
290 Mohri Transducers in Language and Speech Theorem 12 Let "rl = (Q1,E, I1,F1, El, ~1,Pl) be a trim unambiguous string-to-weight transducer defined on the tropical semiring.
Then ~rl is determinizable iff it has the twins property.
Proof According to the previous theorem, if ~1 has the twins property, then it is determinizable.
Assume now that T does not have the twins property, then there exist at least two states q and q~ in Q that are not twins.
There exists (u, v) E E* such that: ({q,q'} C 61(I,u),q E 51(q,v),q' E 5ffq',v)) and ~l(q,v,q) ~ ~l(q',v,q').
Consider the weighted subsets 62(/2, uvk), with k E N, constructed by the determinization algorithm.
A subset 62(/2, uv k) contains the pairs (q, c(q, uvk)) and (q', c(q', uvk)).
We will show that these subsets are all distinct.
This will prove that the determinization algorithm does not terminate if ~-1 does not have the twins property.
Since T1 is a trim unambiguous transducer, there exits only one path in ~-1 from I to q or to qt with input string u.
Similarly, the cycles at q and q~ labeled with v are unique.
Thus, there exist i E I and i ~ E I such that: VkEN, c(q, uv k) = )~l(i)+01(i,u,q)+kOl(q,v,q)-~2(i2,uvk)-)~2 (18) Vk E ./V', c(q', uv k) = )~1(/') q81 (i', u, q') + k01(q', v, q') or2(/2, uv k) ~2 Let )~ and 0 be defined by: )~ = (,~1(i') )~1(i)) q( l(i',u,q') 01(i,u,q)) 0 = Ol(q',v,q') Offq, v,q) (19) We have: 'Ok E N, c(q', uv k) c(q, uv k) = ~ + k~) (20) Since (1 ~ 0, equation 20 shows that the subsets 62(i2, uv k) are all distinct.
\[\] 3.5 Test of Determinizability The characterization of determinizable transducers provided by theorem 12 leads to the definition of an algorithm for testing the determinizability of trim unambiguous transducers.
Before describing the algorithm, we introduce a lemma that shows that it suffices to examine a finite number of paths to test the twins property.
Lemma 2 Let "rl = (Q1, E, I1,F1, El, )~1,Pl) be a trim unambiguous string-to-weight transducer defined on the tropical semiring.
7-1 has the twins property iff V(u,v) E (E*) 2, luvl <_ 2IQ112 1, ({q,q'} C 61(I,u),q E 51(q,v),q' E 61(q',v)) ~ 01(q,v,q) = 01(q',v,q') (21) Proof Clearly if "O has the twins property, then (21) holds.
Conversely, we prove that if (21) holds, then it also holds for any (u,v) E (E*) 2, by induction on luvI . Our proof is similar to that of Berstel (1979) for string-to-string transducers.
Consider (u, v) E (E*) 2 and (q,q') E IQll 2 such that: {q,q'} c 61(I,u),q E 61(q,v),q' E 61(q',v).
Assume that luvI > 21Qll 2 1 with Iv I > o.
Then either luI > IQll 2 1 or IvI > IQll 2 1.
Assume that lul > IQ112-1.
Since T1 is a trim unambiguous transducer there exists a unique path ~r in rl from i E I to q labeled with the input string u, and a unique path 291 Computational Linguistics Volume 23, Number 2 7r' from i' E I to q'.
In view of lemma 2, there exist strings ul, u2, u3 in ~*, and states pl, p2, p~, and p~ such that \]u2\] > 0, UlU2U3 -~ U and such that lr and 7r' be factored in the following way: ~r E i u"G~ pl "~ pl,G q ~r' E i',-,,*ul Pl',`% P~ "~ q' Since \]ulu3vl < luv\], by induction 01(q,v,q) = Offq',v,q').
Next, assume that Iv I > \]Qll 2 1.
Then according to lemma 1, there exist strings Vl, v2, v3 in E*, and states ql, q2, q~, and q~ such that Iv21 > 0, vlv2v3 = v and such that lr and lr' be factored in the following way: ~r E q~ ql~ ql ~ q ~r' E q',G q~,~ q~,~ q' Since lUVl V3\] < \]uv\], by induction, 81 ( q, vl v3, q) = 81 ( q', vl v3, q').
Similarly, since luvl v2\] < l uv\], 01 (ql, v2, ql) = 01 (q~, v2, q~).
"rl is a trim unambiguous transducer, so: 01(q,v,q) =01(q, vlv3,q)+Ol(ql, v2,ql) 01(q', v, q') = 01(q', vlv3, q') + 01 (q~, v2, q~) Thus, 01 (q, v, q) = 01(q', v, q').
This completes the proof of the lemma.
\[\] Theorem 13 Let T1 = (Q1, E, 11, F1, El, A1, Pl) be a trim unambiguous string-to-weight transducer defined on the tropical semiring.
There exists an algorithm to test the determinizability of ~1.
Proof According to theorem 12, testing the determinizability of T1 is equivalent to testing for the twins property.
We define an algorithm to test this property.
Our algorithm is close to that of Weber and Klemm (1995) for testing the sequentiability of string-to-string transducers.
It is based on the construction of an automaton A = (Q,/, F, E) similar to the cross product of ~-1 with itself.
Let K C T~ be the finite set of real numbers defined by: K= ((t~)-cr(ti)): l <k <2iQ1\]2-1,Vi<_k(ti, tl) EE We define A by the following:  The set of states of A is defined by Q = Q1 x Q1 x K,  The set of initial states by I = h x/1 x {0},  The set of final states by F = F1 x F1 x K,  The set of transitions by: E = {((ql, q2,c),a,(q~,q~2,c')) E.
Q x E x Q: 3 (ql,a, x, q2) E El, ' ' ' x' x}.
(ql,a,x,q2) EEl, C'-C= 292 Mohri Transducers in Language and Speech By construction, two states ql and q2 of Q can be reached by the same string u, lut < 21Qll 2 1, iff there exists c E K such that (ql, q2,c) can be reached from I in A.
The set of such (ql, q2, c) is exactly the transitive closure of I in A.
The transitive closure of I can be determined in time linear in the size of A, O(IQI + IEI).
Two such states ql and q2 are not twins iff there exists a path in A from (ql, q2, 0) to (ql, q2, c), with c # 0.
Indeed, this is exactly equivalent to the existence of cycles at ql and q2 with the same input label and distinct output weights.
According to lemma 2, it suffices to test the twins property for strings of length less than 21Qll 2 1.
So the following gives an algorithm to test the twins property of a transducer ~-1:, 2.
3. Compute the transitive closure of h T(I).
Determine the set of pairs (ql, q2) of T(I) with distinct states ql # q2For each such {ql, q2}, compute the transitive closure of (ql, q2, 0) in A.
If it contains (ql, q2, c) with c # 0, then ~-1 does not have the twins property.
The operations used in the algorithm (computation of the transitive closure, determination of the set of states) can all be done in polynomial time with respect to the size of A, using classical algorithms (Aho, Hopcroft, and Ullman 1974).
\[\] This provides an algorithm for testing the twins property of an unambiguous trim transducer T.
It is very useful when T is known to be unambiguous.
In many practical cases, the transducer one wishes to determinize is ambiguous.
It is always possible to construct an unambiguous transducer T' from T (Eilenberg 1974-1976).
The complexity of such a construction is exponential in the worst case.
Thus the overall complexity of the test of determinizability is also exponentia ! in the worst case.
Notice that if one wishes to construct the result of the determinization of T for a given input string w, one does not need to expand the whole result of the determinization, but only the necessary part of the determinized transducer.
When restricted to a finite set the function realized by any transducer is subsequentiable, since it has bounded variation?
Acyclic transducers have the twins property, so they are determinizable.
Therefore, it is always possible to expand the result of the determinization algorithm for a finite set of input strings, even if T is not determinizable.
3.6 Determinization
in Other Semirings The determinization algorithm that we previously presented applies as well to transducers mapping strings to other semirings.
We gave the pseudocode of the algorithm in the general case.
The algorithm applies for instance to the real semiring (7"4, +,., 0,1).
One can also verify that (~* U {oc}, A,., cx~, e), where A denotes the longest common prefix operation and  concatenation, o~ a new element such that for any string w E (~* U {~}), w A oo = oo A w = w and woo = eo.
w = oo, defines a left semiring}  We call this semiring the string semiring.
The algorithm of Figure 10 used with the string semiring is exactly the determinization algorithm for subsequentiable string-to-string transducers, as defined by Mohri (1994c).
The cross product of two semirings defines a semiring.
The algorithm also applies when the semiring is the cross product of 9 Using the proof of the theorem of the previous section, it is easy to convince oneself that this assertion can be generalized to any rational subset Y of E* such that the restriction of S, the function T realizes, to Y has bounded variation.
10 A
left semiring is a semiring that may lack right distributivity.
293 Computational Linguistics Volume 23, Number 2 a:b/3 Figure 13 Transducer 7-1 with outputs in ~* x 74.
a:b/3 ~ c:c/5 = b.a/~ d:a/~ Figure 14 Sequential transducer r2 with outputs in ~,,* x 74 obtained from fll by determinization.
(E* Ucx~}, A,., cx~, c) and (T4+ Uoo}, min, +, oo, 0), which allows transducers outputting pairs of strings and weights to be determined.
The determinization algorithm for such transducers is illustrated in Figures 13 and 14.
Subsets in this algorithm are made of triples (q, w, x) E Q x E* u {oo} x 7-4 u {cx~}, where q is a state of the initial transducer, w a residual string, and x a residual output weight.
3.7 Minimization
We here define a minimization algorithm for subsequential power series defined on the tropical semiring, which extends the algorithm defined by Mohri (1994b) in the case of string-to-string transducers.
For any subset L of G* and any string u we define u-lL by: u-IL = {w: uw E L} (22) Recall that L is a regular language iff there exists a finite number of distinct u-lL Nerode (1958).
In a similar way, given a power series S we define a new power series u-is by: n u-iS = y~ (S, uw)w (23) wE~* 11 One can prove that S, a power series defined on a field, is rational if it admits a finite number of independent u-iS (Carlyle and Paz 1971).
This is the equivalent, for power series, of Nerode's theorem for regular languages.
294 Mohri Transducers in Language and Speech For any subsequential power series S we can now define the following relation on ~,*: V(u, v) E ~*, u Rs v 4=~ 3k E T4, (u-lsupp(S) = v-lsupp(S)) and (\[u-iS -1 -V S\]/u-lsupp(S ) = k) (24) It is easy to show that Rs is an equivalence relation.
(u-lsupp(S) = v-lsupp(S)) defines the equivalence relation for regular languages.
Rs is a finer relation.
The additional condition in the definition of Rs is that the restriction of the power series \[u-iS -v-iS\] to u-lsupp(S) = v-lsupp(S) is constant.
The following lemma shows that if there exists a subsequential transducer T computing S with a number of states equal to the number of equivalence classes of Rs, then T is a minimal transducer computing f.
Lemma 3 If S is a subsequential power series defined on the tropical semiring, Rs has a finite number of equivalence classes.
This number is bounded by the number of states of any subsequential transducer realizing S.
Proof Let T -(Q, i, F, ~, 6, or, ~, p) be a subsequential transducer realizing S.
Clearly, ~(U,V) E (~*)2, 6(i,u) = 6(i,v) ~ Vw E u-lsupp(S),6(i, uw) E F ~ 6(i, vw) E F u-lsupp(S) = v-lsupp(S) Also, if u-lsupp(S) = v-lsupp(S), V(u, v) E (~,)2, 6(i, u) = 6(i,v) ~ VW E u-lsupp(S), (S, uw) (S, vw) = or(i, u) (i,v) 4=~ \[u-iS v-lS\]/u-~supp(S) = cr(i,u) cr(i,v) So V(u,v) E (G,)2, 6(i,u) = 6(i,v) ~ (uRsv).
This proves the lemma.
\[\] The following theorem proves the existence of a minimal subsequential transducer representing S.
Theorem 14 For any subsequential function S, there exists a minimal subsequential transducer computing it.
Its number of states is equal to the index of Rs.
Proof Given a subsequential power series S, we define a power series f by: Vu E ~*: u-lsupp(S) = 0, (f,u) = 0 Vu E G*: u-lsupp(S) # O, u) = min (S, uw) wEu-lsupp(S) We then define a subsequential transducer T = (Q, i, F, ~, 6, or, )~, p) by: 12  Q={~: uEG*}; 12 We denote by ~ the equivalence class of u E G*.
295 Computational Linguistics Volume 23, Number 2  i=~;  F = {a: u E ~,* Msupp(S)};  Vu e ~*,Va e ~,6(a,a) = Ca;  vu ~ y,*,va ~ z,~(~,a) = (f,u~) (f,u); ,~ = ff,~);  VqEQ, p(q)=0.
Since the index of Rs is finite, Q and F are well-defined.
The definition of 6 does not depend on the choice of the element u in ~, since for any a E ~, u Rs v implies (ua) Rs (va).
The definition of rr is also independent of this choice, since by definition of Rs, if uRsv, then (ua) Rs (va) and there exists k E T4 such that Vw E ~*, (S, uaw) (S, vaw)= (S, uw) (S, vw) = k.
Notice that the definition of G implies that: Vw ~ s*,(i,w) = (f,w) ff,~) (25) So: Vw E supp(S), A + r(i, w) + p(q) -= (f, w) = rnin (S, ww') w' ew-lsupp(S) S is subsequential, hence: Vw' E w-lsupp(S), (S, ww') < (S, w).
Since Vw E supp(S),  E w-lsupp(S), we have: m:m (S, ww') = (S,w) w' ew-lsupp(S) T realizes S.
This ends the proof of the theorem.
\[\] Given a subsequential transducer T = (Q, i, F, G, 6, cr, A, p), we can define for each state q E Q, d(q) by: d(q) -rnin (er(q,w) + p(6(q,w))) (26) 6(q,w) EF Definition We define a new operation of pushing, which applies to any transducer T.
In particular, if T is subsequential the result of the application of pushing to T is a new subsequential transducer T' --(Q, i, F, ~.., 6, er', A', p') that only differs from T by its output weights in the following way:  ' = ;~ +,~(i);  V(q,a) E Q x Z, G'(q,a) = rr(q,a) +d(6(q,a))-d(q);  Vq E Q,#(q) = O.
According to the definition of d, we have: Vw E G*: 6(q, aw) E F,d(q) < cr(q,a) + er(6(q,a),w) + p(6(6(q,a),w)) This implies that: a(q) <_ o(q,a) + a(6(q,a)) So, r ~ is well-defined: v(q,a) ~ Q x z,~'(q,a) > o 296 Mohri Transducers in Language and Speech Lemma 4 Let T' be the transducer obtained from T by pushing.
T ~ is a subsequential transducer which realizes the same function as T.
Proof That T' is subsequential follows immediately its definition.
Also, Vw E ~',q C Q, cr'(q,w) = rr(q,w) + d(6(q,w) ) -d(q) Since 6(i,w) E F =~ d(6(i,w)) = p(6(i,w)), we have: + o'(i,w) + p'(6(i,w)) = ;, + a(i) + o(q,w) + w)) d(i) + 0 This proves the lemma.
\[\] The following theorem defines the minimization algorithm.
Theorem 15 Let T be a subsequential transducer realizing a power series on the tropical semiring.
Then applying the following two operations: 1.
pushing 2.
automata minimization leads to a minimal transducer.
This minimal transducer is exactly the one defined in the proof of theorem 14.
The automata minimization step in the theorem consists of considering pairs of input labels and associated weights as a single label and of applying classical minimization algorithms for automata (Aho, Hopcroft, and Ullman 1974).
We do not give the proof of the theorem; it can be proved in a way similar to what is indicated in Mohri (1994b).
In general, there are several distinct minimal subsequential transducers realizing the same function.
Pushing introduces an equivalence relation on minimal transducers: T Rp T' iff p(T) = p(T'), where p(T) (resp.
p(T')) denotes the transducer obtained from T (resp.
T t) by pushing.
Indeed, if T and T t are minimal transducers realizing the same function, then p(T) and p(T') are both equal to the unique minimal transducer equivalent to T and T t as defined in theorem 14.
So, two equivalent minimal transducers only differ by their output labels, they have the same topology.
They only differ by the way the output weights are spread along the paths.
Notice that if we introduce a new super final state @ to which each final state q is connected by a transition of weight p(q), then d(q) in the definition of T' is exactly the length of a shortest path from  to q.
Thus, T' can be obtained from T using the classical single-source shortest paths algorithms such as that of Dijkstra (Cormen, Leiserson, and Rivest 1992).
13 In
case the transducer is acyclic, a classical linear time algorithm based on a topological sort of the graph allows one to obtain d.
13 This
algorithm can be extended to the case where weights are negative.
If there is no negative cycle the Bellman-Ford algorithm can be used.
297 Computational Linguistics Volume 23, Number 2 d/O / ~ ~ c/1 e,,Q Figure 15 Transducer ill.
d/O / N ~ c/1 e/O =Q Figure 16 Transducer ~'1 obtained from fll by pushing.
Once the function d is defined, the transformation of T into T' can be done in linear time, namely O(IQ\]+IEI), if we denote by E the set of transitions of T.
The complexity of pushing is therefore linear (O(IQI + IEI)) if the transducer is acyclic.
In the general case, the complexity of pushing is O(IE \] log IQI) if we use classical heaps, O(\]E I + IQI log \]Q\]) if we use Fibonacci heaps, and O(IE I log log IQI) if we use the efficient implementation of priority queues by Thorup (1996).
In case the maximum output weight W is small, we can use the algorithm of Ahuja et al.(1988); the complexity of pushing is then O(IEI + IQIx/fwI).
In case the transducer is acyclic, we can use a specific automata minimization algorithm (Revuz 1992) with linear time complexity, O(\]Q\] + IE\]).
In the general case, an efficient implementation of Hopcroft's algorithm (Aho, Hopcroft, and Ullman 1974) leads to O(\]E\] log \]Q\]).
Thus, the overall complexity of the minimization of subsequential transducers is always as good as that of classical automata minimization: O(IQI + IE\]) in the acyclic case, and O(\]E I log \]Q\[) in the general case.
Figures 15 to 17 illustrate the minimization algorithm.
131 (Figure 15) represents a subsequential string-to-weight transducer.
Notice that the size of fll cannot be reduced using the automata minimization.
71 represents the transducer obtained by pushing, and 51 a minimal transducer realizing the same function as fll in the tropical semiring.
298 Mohri Transducers in Language and Speech d/O c/1 Figure 17 Minimal transducer 61 obtained from "n by automata minimization.
The transducer obtained by this algorithm is the one defined in the proof of theorem 14 and has the minimal number of states.
This raises the question of whether there exists a subsequential transducer with the minimal number of transitions and computing the same function as a given subsequential transducer T.
The following corollary offers an answer.
Corollary 1 A minimal subsequential transducer has also the minimal number of transitions among all subsequential transducers realizing the same function.
Proof This generalizes the analogous theorem that holds in the case of automata.
The proof is similar.
Let T be a subsequential transducer with a minimal number of transitions.
Clearly, pushing does not change the number of transitions of T and automatan minimization, which consists of merging equivalent states, reduces or does not change this number.
Thus, the number of transitions of the minimal transducer equivalent to T as previously defined is less or equal to that of T.
This proves the corollary since, as previously pointed out, equivalent minimal transducers all have the same topology: in particular, they have the same number of states and transitions.
\[\] Given two subsequential transducers, one might wish to test their equivalence.
The importance of this problem was pointed out by Hopcroft and Ullman (1979, 284).
The following corollary addresses this question.
Corollary 2 There exists an algorithm to determine if two subsequential transducers are equivalent.
Proof The algorithm of theorem 15 associates a unique minimal transducer to each subsequential transducer T.
More precisely, this minimal transducer is unique up to a renumbering of the states.
The identity of two subsequential transducers with different numbering of states can be tested in the same way as that of two deterministic automata; for instance, by testing the equivalence of the automata and the equality of their number of states.
An efficient algorithm for testing the equivalence of two deterministic automata is given in Aho, Hopcroft, and Ullman (1974).
14 Since
the min14 The automata minimization step can in fact be omitted if this equivalence algorithm is used, since it does not affect the equivalence of the two subsequential transducers, considered as automata.
299 Mohri Transducers in Language and Speech weight transducer, each path of which corresponds to a sentence.
The weight of the path can be interpreted as a negative log of the probability of that sentence given the sequence of acoustic observations (utterance).
Such acyclic string-to-weight transducers are called word lattices.
4.2 Word
Lattices For a given utterance, the word lattice obtained in such a way contains many paths labeled with the possible sentences and their associated weights.
A word lattice often contains a lot of redundancy: many paths correspond to the same sentence but with different weights.
Word lattices can be directly searched to find the most probable sentences, those which correspond to the best paths, the paths with the smallest weights.
Figure 18 shows a word lattice obtained in speech recognition for the 2,000-word ARPA ATIS Task.
It corresponds to the following utterance: Which flights leave Detroit and arrive at Saint Petersburg around nine am?
Clearly the lattice is complex; it contains about 83 million paths.
Usually, it is not enough to consider the best path of a word lattice.
It is also necessary to correct the best path approximation by considering the n best paths, where the value of n depends on the task considered.
Notice that in case n is very large, one would need to consider, for the lattice in Figure 18, all 83 million paths.
The transducer contains 106 states and 359 transitions.
Determinization applies to this lattice.
The resulting transducer W2 (Figure 19) is sparser.
Recall that it is equivalent to W1, realizing exactly the same function mapping strings to weights.
For a given sentence s recognized by W1, there are many different paths with different total weights.
W2 contains a path labeled with s and with a total weight equal to the minimum of the weights of the paths of W1.
Let us insist on the fact that no pruning, heuristic, or approximation has been used here.
The lattice W2 only contains 18 paths.
Obviously, the search stage in speech recognition is greatly simplified when applied to W2 rather than W1.
W2 admits 38 states and 51 transitions.
The transducer W2 can still be minimized.
The minimization algorithm described in the previous section leads to the transducer W3 shown in Figure 20.
It contains 25 states and 33 transitions and of course the same number of paths as W2, 18.
The effect of minimization appears to be less important.
This is because, in this case, determinizafion includes a large part of the minimization by reducing the size of the first lattice.
This can be explained by the degree of nondeterminism of word lattices such as 14/1.15 Many states can be reached by the same set of strings.
These states are grouped into a single subset during determinization.
Also, the complexity of determinization is exponential in general, but in the case of the lattices considered in speech recognition, it is not.
16 Since
they contain a lot of redundancy, the resulting lattice is smaller than the initial one.
In fact, the time complexity of determinization can be expressed in terms of the initial and resulting lattices, W1 and W2, by O(l~ I log IGl(IWllIW21)2), where IWll and IW21 denote the sizes of W1 and W2.
Clearly if we restrict determinization to the cases where I w21 < I W1 I, its complexity is polynomial in terms of the size of the initial transducer \[Wll.
This also 15 The notion of ambiguity of a finite automaton can be formalized conveniently using the tropical semiring.
Many important studies of the degree of ambiguity of automata have been done in connection with the study of the properties of this semiring (Simon 1987).
16 A
more specific determinization can be used in the cases often encountered in natural language processing where the graph admits a loop at the initial state over the elements of the alphabet (Mohri 1995).
301 Mohri Transducers in Language and Speech Figure 19 Equivalent word lattice W2 obtained by determinization of W1.
@ ....
 .... .@ ~,, @ ~,o ...
@  Figure 20 Equivalent word lattice Wa obtained by minimization from W2.
rescoring l approximate_~J detailed \]._..l~ cde4Pls ~ lattice ~ models Figure 21 Rescoring.
applies to the space complexity of the algorithm.
In practice, the algorithm appears to be very efficient.
As an example, it took about 0.02s on a Silicon Graphics (Indy 100 MHZ Processor, 64 Mb RAM) to determinize the transducer of Figure 18.17 Determinization makes the use of lattices much faster.
Since at any state there exists at most one transition labeled with the word considered, finding the weight associated with a sentence does not depend on the size of the lattice.
The time and space complexity of such an operation is simply linear in the size of the sentence.
When dealing with large tasks, most speech recognition systems use a rescoring method (Figure 21).
This consists of first using a simple acoustic and grammar model to produce a word lattice, and then to reevaluate this word lattice with a more sophisticated model.
The size of the word lattice is then a critical parameter in the time and space efficiency of the system.
The determinization and minimization algorithms we presented allow the size of such word lattices to be considerably reduced, as seen in the examples.
We experimented with both determinization and minimization algorithms in the ATIS task.
Table I illustrates these results.
It shows these algorithms to be very effective in reducing the redundancy of speech networks in this task.
The reduction is also illustrated by an example in the ATIS task.
17 Part
of this time corresponds to I/O's and is therefore independent of the algorithm.
303 Computational Linguistics Volume 23, Number 2 Table 1 Word lattices in the ATIS task.
Determinization Determinization + Minimization Objects reduction factor reduction factor States ~ 3 ~ 5 Transitions,-~ 9 ~ 17 Paths > 232 > 232 Table 2 Subsequential word lattices in the NAB task.
Minimization results Objects reduction factor States ~ 4 Transitions,~ 3 Example 1 Example of a word lattice in the ATIS task.
States: 187 --* 37 Transitions: 993 ~ 59 Paths: > 232 ~ 6,993 The number of paths of the word lattice before determinization was larger than that of the largest integer representable with 32 bit machines.
We also experimented with the minimization algorithm by applying it to several word lattices obtained in the 60,000-word ARPA North American Business News task (NAB).
These lattices were already determinized.
Table 2 shows the average reduction factors we obtained when using the minimization algorithms with several subsequential lattices obtained for utterances of the NAB task.
The reduction factors help to measure the gain of minimization alone, since the lattices are already subsequential.
The numbers in example 2, an example of reduction we obtained, correspond to a typical case.
Example 2 Example of a word lattice in NAB task.
Transitions: 10,8211 ~ 37,563 States: 10,407 --* 2,553 4.3 On-the-fly Implementation of Determinization An important characteristic of the determinization algorithm is that it can be used on-the-fly.
Indeed, the determinization algorithm is such that given a subset representing a state of the resulting transducer, the definition of the transitions leaving that state depends only on that state or, equivalently, on the states of that subset, and on the transducer to determinize.
In particular, the definition and construction of these transitions do not depend directly on the previous subsets constructed.
We have produced an implementation of the determinization that allows one both to completely expand the result or to expand it on demand.
Arcs leaving a state of 304 Mohri Transducers in Language and Speech the determinized transducer are expanded only if necessary.
This characteristic of the implementation is important.
It can then be used, for instance, at any step in an onthe-fly cascade of composition of transducers in speech recognition to expand only the necessary part of a lattice or transducer (Pereira and Riley 1996; Mohri, Pereira, and Riley 1996).
One of the essential implications of the implementation is that it contributes to saving space during the search stage.
It is also very useful in speeding up the n-best decoder in speech recognition.
TM The determinization and minimization algorithms for string-to-weight transducers seem to have other applications in speech processing.
Many new experiments can be done using these algorithms at different stages of speech recognition, which might lead to the reshaping of some of the methods used in this field and create a renewed interest in the theory of automata and transducers.
5. Conclusion We have briefly presented the theoretical bases, algorithmic tools, and practical use of a set of devices that seem to fit the complexity of language and provide efficiency in space and time.
From the theoretical point of view, the understanding of these objects is crucial.
It helps to describe the possibilities they offer and to guide algorithmic choices.
Many new theoretical issues arise when more precision is sought.
The notion of determinization can be generalized to that of E-determinization for instance (Salomaa and Soittola 1978, chapter 3, exercise) requiring more general algorithms.
It can also be extended to local determinization: determinization at only those states of a transducer that admit a predefined property, such as that of having a large number of outgoing transitions.
An important advantage of local determinization is that it can be applied to any transducer without restriction.
Furthermore, local determinization also admits an on-the-fly implementation.
New characterizations of rational functions shed new light on some aspects of the theory of finite-state transducers (Reutenauer and Schiitzenberger 1995).
We have also offered a generalization of the operations we use based on the notions of semiring and power series, which help to simplify problems and algorithms used in various cases.
In particular, the string semiring that we introduced makes it conceptually easier to describe many algorithms and properties.
Subsequential transducers admit very efficient algorithms.
The determinization and minimization algorithms in the case of string-to-weight transducers presented here complete a large series of algorithms that have been shown to give remarkable results in natural language processing.
Sequential machines lead to useful algorithms in many other areas of computational linguistics.
In particular, subsequential power series allow for efficient results in indexation of natural language texts (Crochemore 1986; Mohri 1996b).
We briefly illustrated the application of these algorithms to speech recognition.
More precision in acoustic modeling, finer language models, large lexicon grammars, and a larger vocabulary will lead, in the near future, to networks of much larger sizes in speech recognition.
The determinization and minimization algorithms might help to limit the size of these networks while maintaining their time efficiency.
These algorithms can also be used in text-to-speech synthesis.
In fact, the same operations of composition of transducers (Sproat 1995) and perhaps more important size issues can be found in this field.
18 We
describe this application of determinization elsewhere.
305 Computational Linguistics Volume 23, Number 2 Figure 22 Subsequential power series S nonbisubsequential.
Appendix The determinization algorithm for power series can also be used to minimize transducers in many cases.
Let us first consider the case of automata.
Brzozowski (1962) showed that determinization can be used to minimize automata.
This nice result has also been proved more recently in elegant papers by Bauer (1988) and Urbanek (1989).
These authors refine the method to obtain better complexities.
19 Theorem
16 (Brzozowski 1962) Let A be a nondeterministic automaton.
Then the automaton A' = (Q', i', F', E, 6') obtained by reversing A, applying determinization, rereversing the obtained automaton and determinizing it is the minimal deterministic automaton equivalent to A.
We generalize this theorem to the case of string-to-weight transducers.
We say that a rational power series S is bisubsequential when S is subsequential and the power series S R = Y~w~, (S, wR)w is also subsequential.
2 Not
all subsequential transducers are bisubsequential.
Figure 22 shows a transducer representing a power series S that is not bisubsequential.
S is such that: Vn E.M, (S, ba") = n+l (27) Vn E Af, (S, ca n) = 0 The transducer of Figure 22 is subsequential so S is subsequential.
But the reverse S a is not, because it does not have bounded variation.
Indeed, since: We have: Vn E Af, (S a,anb) = n + l VnE.Af, (Sa, anc) = 0 Vn ~ A/', I(Sa, a"b) (Sa, a"c)l = n + 1 (28) 19 See Watson (1993) for a taxonomy of minimization algorithms for automata; see also Courcelle, Niwinski, and Podelski 1991.
20 For any string w E ~*, we denote by w a its reverse.
306 Mohri Transducers in Language and Speech A characterization similar to that of string-to-string transducers (Choffrut 1978) is possible for bisubsequential power series defined on the tropical semiring.
In particular, the theorem of the previous sections shows that S is bisubsequential iff S and S n have bounded variation.
We similarly define bideterminizable transducers as the transducers T defined on the tropical semiring admitting two applications of determinization, as follows: . . The reverse of T, T a can be determinized.
We denote by det(T a) the resulting transducer.
The reverse of det(TR), \[det(Ta)\] R can also be determinized.
We denote by det(\[det(Ta)\] ~) the resulting transducer.
In this definition, we assume that the reverse operation is performed simply by reversing the direction of the transitions and exchanging initial and final states.
Given this definition, we can present the extension of the theorem of Brzozowski (1962) to bideterminizable transducers.
21 Theorem 17 Let T be a bideterminizable transducer defined on the tropical semiring.
Then the transducer det(\[det(TR)\] R) obtained by reversing T, applying determinization, rereversing the obtained transducer and determinizing it is a minimal subsequential transducer equivalent to T.
Proof We denote by:  T1 = (Q1,il,F1,G,61,Crl,)~l, p1) det(Ta),  T'= (Q',i',F',E,6',',&',p') det(\[det(Ta)\] a)  T" = (Q', i', F', G, ~', rr', &', p') the transducer obtained from T by pushing.
The double reverse and determinization algorithms clearly do not change the function that T realizes.
So T' is a subsequential transducer equivalent to T.
We only need to prove that T ~ is minimal.
This is equivalent to showing that T" is minimal, since T' and T" have the same number of states.
T1 is the result of a determinization, hence it is a trim subsequential transducer.
We show that T' = det(T~) is minimal if T1 is a trim subsequential transducer.
Notice that the theorem does not require that T be subsequential.
Let $1 and $2 be two states of T" equivalent in the sense of automata.
We prove that $1 = $2, namely that no two distinct states of T" can be merged.
This will prove that T" is minimal.
Since pushing only affects the output labels, T' and T" have the same set of states: Q' = Q".
Hence $1 and $2 are also states of T'.
The states of T' can be viewed as weighted subsets whose state elements belong to T1, because T' is obtained by determinization of T~.
Let (q, c) E Q1 x T4 be a pair of the subset $1.
Since T1 is trim there exists w c G* such that 61(il, w) = q, so 6~($1,w) E F'.
Since $1 and S 2 are equivalent, we also have: 21 The theorem also holds in the case of string-to-string bideterminizable transducers.
We give the proof in the more complex case of string-to-weight transducers.
307 Computational Linguistics Volume 23, Number 2 b/1 Figure 23 Transducer f12 obtained by reversing ill.
a/O c/l a/4 ~b/1 ~ Figure 24 Transducer/33 obtained by determinizafion of/32.
d/0 a/4 Figure 25 Minimal transducer f14 obtained by reversing fig and applying determinization.
6~($2, w) E F ~.
Since T1 is subsequential, there exists only one state of T~ admitting a path labeled with w to il; that state is q.
Thus, q E $2.
Therefore any state q member of a pair of $1 is also member of a pair of $2.
By symmetry the reverse is also true.
Thus exactly the same states are members of the pairs of $1 and $2.
There exists k > 0 such that: Sl = {(qo, clo), (ql, c11) .....
(qk, clk)} S 2 ~{(qo, c20), (ql, c21) .....
(qk, C2k)} (29) We prove that weights are also the same in $1 and $2.
Let IIj, (0 > j > k), be the set of strings labeling the paths from il to qi in T1.
Crl(il, w) is the weight output corresponding to a string w E IIj.
Consider the accumulated weights cq, 1 < i < 2, 0 < j < k, in determinization of T~.
Each Clj for instance corresponds to the weight not yet output in the paths reaching $1.
It needs to be added to the weights of any path from qj c $1 to a final state in rev(T1).
In other terms, the determinization algorithm will assign the weight Clj qff1(/1, W) q.X1 to a path labeled with w a reaching a final state of T ~ from $1.
T" is obtained by pushing from T ~.
Therefore the weight of such 308 References 1 Alfred V.
Aho, John E.
Hopcroft, The Design and Analysis of Computer Algorithms, Addison-Wesley Longman Publishing Co., Inc., Boston, MA, 1974 2 Alfred V.
Aho, Ravi Sethi, Jeffrey D.
Ullman, Compilers: principles, techniques, and tools, Addison-Wesley Longman Publishing Co., Inc., Boston, MA, 1986 3 Ahuja, Ravindra K., Kurt Mehlhorn, James B.
Orlin, and Robert Tarjan.
1988. Faster algorithms for the shortest path problem.
Technical Report 193, MIT Operations Research Center.
4 Bauer, W.
1988. On minimizing finite automata.
EATCS bULLETIN, 35.
5 Berstel, Jean.
1979. Transductions and Context-Free Languages.
Teubner Studienbucher, Stuttgart.
6 Jean
Berstel, JJr., Christophe Reutenauer, Rational series and their languages, Springer-Verlag New York, Inc., New York, NY, 1988 7 Brzozowski, J.
A. 1962.
Canonical regular expressions and minimal state graphs for definite events.
Methematical Theory of Automata, 12:529--561.
8 Carlyle, J.
W. and A.
Paz. 1971.
Realizations by stochastic finite automaton.
Journal of Computer and System Sciences, 5:26--40.
9 Choffrut, Christian.
1978. Contributions  l'tude de quelques familles remarquables de functions rationnelles.
Ph.D. thesis, (thse de doctorat d'Etat), Universit Paris 7, LITP, Paris.
10 Thomas
T.
Cormen, Charles E.
Leiserson, Ronald L.
Rivest, Introduction to algorithms, MIT Press, Cambridge, MA, 1990 11 Courcelle, Bruno, Damian Niwinski, and Andreas Podelski.
1991. A geometrical view of the determinization and minimization of finite-state utomata.
Mathematical Systems Theory, 24:117--146.
12 Maxine
Crochemore, Transducers and repetitions, Theoretical Computer Science, v.45 n.1, p.63-86, Sept.
1986 13 Samuel Eilenberg, Automata, Languages, and Machines, Academic Press, Inc., Orlando, FL, 1976 14 Elgot, C.
C. and J.
E. Mezei.
1965. On relations defined by generalized finite automata.
IBM Journal of Research and Development, 9.
15 Ginsburg, S.
and G.
F. Rose.
1966. A characterization of machine mappings.
Canadian Journal of Mathematics, 18.
16 Gross, Maurice.
1989. The use of finite automata in the lexical representation of natural language.
Lecture Notes in Computer Science, 377.
17 John
E.
Hopcroft, Jeffrey D.
Ullman, Introduction To Automata Theory, Languages, And Computation, Addison-Wesley Longman Publishing Co., Inc., Boston, MA, 1990 18 Ronald M.
Kaplan, Martin Kay, Regular models of phonological rule systems, Computational Linguistics, v.20 n.3, September 1994 19 Fred Karlsson, Atro Voutilainen, Juha Heikkila, Arto Anttila, Constraint Grammar: A Language-Independent System for Parsing Unrestricted Text, Mouton de Gruyter, 1995 20 Lauri Karttunen, Ronald M.
Kaplan, Annie Zaenen, Two-level morphology with composition, Proceedings of the 14th conference on Computational linguistics, August 23-28, 1992, Nantes, France 21 Krob, daniel.
1994. The equality problem for rational series with multiplicities in the tropical semiring is undecidable.
Journal of Algebra and Computation, 4.
22 Werner Kuich, Arto Salomaa, Semirings, automata, languages, Springer-Verlag, London, 1985 23 Mehryar Mohri, Compact representations by finite-state transducers, Proceedings of the 32nd annual meeting on Association for Computational Linguistics, p.204-209, June 27-30, 1994, Las Cruces, New Mexico 24 Mehryar Mohri, Minimization of Sequential Transducers, Proceedings of the 5th Annual Symposium on Combinatorial Pattern Matching, p.151-163, June 05-08, 1994 25 Mohri, Mehryar.
1994c. On some applications of finite-state automata theory to natural language processing: Representation of morphological dictionaries, compaction, and indexation.
Technical Report IGM 94--22, Institut Gaspard Monge, Noisy-le-Grand.
26 Mohri, Mehryar.
1994b. Syntactic analysis by local grammars automata: An efficient algorithm.
In Proceedings of the International Conference on Computational Lexicography (COMPLEX94).
Linguistic Institute, Hungarian Academy of Science, Budapest, Hungary.
27 Mohri, Mehryar.
1995. Matching patterns of an automaton.
Lecture Notes in Computer Science, 937.
28 Mohri, Mehryar, 1996a.
On The Use of Sequential Transducers in Natural Language Processing.
In Yves Shabes, editor, Finite State Devices in Natural Language Processing.
MIT Press, Cambridge, MA.
To appear.
29 Mehryar Mohri, On some applications of finite-state automata theory to natural language processing, Natural Language Engineering, v.2 n.1, p.61-80, March 1996 30 Morhi, Mehryar, Fernando C.
N. Pereira, and Michael Riley.
1996. Weighted automata in text and speech processing.
In ECAI-96 Workshop, Budapest, Hungary.
ECAI. 31 Mehryar Mohri, Richard Sproat, An efficient compiler for weighted rewrite rules, Proceedings of the 34th annual meeting on Association for Computational Linguistics, p.231-238, June 24-27, 1996, Santa Cruz, California 32 Nerode, Anil.
1958. Linear automaton transformations.
In Proceedings of AMS, volume 9.
33 Pereira, Fernando C.
N. and Michael Riley, 1996.
Weighted Rational Transductions and their Application to Human Language Processing.
In Yves Shabes, editor, Finite State Devices in Natural Language Processing.
MIT Press, Cambridge, MA.
To appear.
34 Dominique Perrin, Finite automata, Handbook of theoretical computer science (vol.
B): formal models and semantics, MIT Press, Cambridge, MA, 1991 35 Christophe Reutenauer, Marcel Paul Schtzenberger, Varieties and rational functions, Theoretical Computer Science, v.145 n.1-2, p.229-240, July 10, 1995 36 Dominique Revuz, Minimisation of acyclic deterministic automata in linear time, Theoretical Computer Science, v.92 n.1, p.181-189, Jan.
6, 1992 37 Roche, Emmanuel.
1993. Analyse syntaxique transformationnelle du franais par transducteurs et lexique-grammaire.
Ph.D. thesis, Universit Paris 7, Paris.
38 Arto Salomaa, M.
Soittola, Automata: Theoretic Aspects of Formal Power Series, Springer-Verlag New York, Inc., Secaucus, NJ, 1978 39 Schtzenberger, Marcel Paul.
1961. On the definition of a family of automata.
Information and Control, 4.
40 Schtzenberger, Marcel Paul.
1977. Sur une variante des fonctions squentielles.
Theoretical Computer Science.
41 Schtzenberger, Marcel Paul.
1987. Polynomial decomposition of rational functions.
In Lecture Notes in Computer Science, volume 386.
Springer-Verlag, Berlin, Heidelberg, and New York.
42 Silberztein Max.
1993. Dictionnaires lectroniques et analyse automatique de textes: le systme INTEX.
Masson, Paris.
43 Simon, Imre.
1987. The nondeterministic complexity of finite automata.
technical Report RT-MAP-8073, Instituto de Matemtica e Estatistica da Universidade de So Paulo.
44 Sproat, Richard.
1995. A finite-state architecture for tokenization and grapheme-to-phoneme conversion in multilingual text analysis.
In Proceedings of the ACL SIGDAT Workshop, Dublin, Ireland.
ACL. 45 Mikkel Thorup, On RAM priority queues, Proceedings of the seventh annual ACM-SIAM symposium on Discrete algorithms, p.59-67, January 28-30, 1996, Atlanta, Georgia, United States 46 Urbanek, F.
1989. On minimizing finite automata.
EATCS Bulletin, 39.
47 Watson, Bruce W.
1993. A taxonomy of finite automata minimization algorithms.
Technical Report 93/44, Eindhoven University of Technology, The Netherlands.
48 Weber, Andreas and Reinhard Klemm.
1995. Economyof description for single-valued transducers.
Information and Computation, 119.
49 W.
A. Woods, Transition network grammars for natural language analysis, Communications of the ACM, v.13 n.10, p.591-606, Oct. 1970
Automatic Word Sense Discrimination Hinrich Schitze* Xerox Palo Alto Research Center This paper presents context-group discrimination, a disambiguation algorithm based on clustering.
Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word.
Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity.
Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus.
The algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources.
The paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words.
1. Introduction Word sense disambiguation is the task of assigning sense labels to occurrences of an ambiguous word.
This problem can be divided into two subproblems: sense discrimination and sense labeling.
Sense discrimination divides the occurrences of a word into a number of classes by determining for any two occurrences whether they belong to the same sense or not.
Sense labeling assigns a sense to each class, and, in combination with sense discrimination, to each occurrence of the ambiguous word.
This view of disambiguation as a two-stage process may not be completely general (for example, it may not be appropriate for the iterative process by which a lexicographer arrives at the sense divisions of a dictionary entry), but it seems applicable to most work on disambiguation in computational linguistics.
In this paper, we will address the problem of sense discrimination as defined above.
That is, we will not be concerned with the sense-labeling component of word sense disambiguation.
Word sense discrimination is easier than full disambiguation since we need only determine which occurrences have the same meaning and not what the meaning actually is.
Focusing solely on word sense discrimination also liberates us of a serious constraint common to other work on word sense disambiguation.
If sense labeling is part of the task, an outside source of knowledge is necessary to define the senses.
Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al.1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996), thesauri (Yarowsky 1992; Walker and Amsler 1986), bilingual corpora (Brown et al.1991; Church and Gale 1991), or hand-labeled training sets (Hearst 1991; Leacock, Towell, and Voorhees 1993; Niwa and Nitta 1994; Bruce and Wiebe 1994), providing information for sense definitions can be a considerable burden.
What makes our approach unique is that, since we narrow the problem to sense discrimination, we can dispense of an outside source of knowledge for defining senses.
* Xerox Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304 Q 1998 Association for Computational Linguistics Computational Linguistics Volume 24, Number 1 We therefore call our approach automatic word sense discrimination, since we do not require manually constructed sources of knowledge.
In many applications, word sense disambiguation must both discriminate and label occurrences; for example, in order to find the correct translation of an ambiguous word in machine translation or the right pronunciation in a text-to-speech system.
The application of interest to us is information access, i.e., making sense of and finding information in large text databases.
For many problems in information access, it is sufficient to solve the discrimination problem only.
In one study, we measured document-query similarity based on word senses rather than words and achieved a considerable improvement in ranking relevant documents ahead of nonrelevant documents (Schi.itze and Pedersen 1995).
Since the measurement of similarity is a systeminternal process, no reference to externally defined senses need be made.
Another potentially beneficial application of word sense discrimination in information access is the design of interfaces that take account of ambiguity.
If a user enters a query that contains an ambiguous word, a system capable of discrimination can give examples of the different senses of the word in the text database.
The user can then decide which sense was intended and only documents with the intended sense would be retrieved.
Again, a reference to external sense definitions is not required for this task.
The algorithm we propose in this paper is context-group discrimination.
1 Contextgroup
discrimination groups the occurrences of an ambiguous word into clusters, where clusters consist of contextually similar occurrences.
Words, contexts, and clusters are represented in a high-dimensional, real-valued vector space.
Context vectors capture the information present in second-order co-occurrence.
Instead of forming a context representation from the words that the ambiguous word directly occurs with in a particular context (first-order co-occurrence), we form the context representation from the words that these words in turn co-occur with in the training corpus.
Second-order co-occurrence information is less sparse and more robust than first-order information.
In context-group discrimination, the context of each occurrence of the ambiguous word in the training corpus is represented as a context vector formed from secondorder co-occurrence information.
The context vectors are then clustered into coherent groups such that occurrences judged similar according to second-order co-occurrence are assigned to the same cluster.
Clusters are represented by their centroids, the average of their elements.
An occurrence in a test text is disambiguated by computing the second-order representation of the relevant context, and assigning it to the cluster whose centroid is closest to that representation.
Since the choice of representation influences the formation of clusters, we will experiment with several representations in this paper, some involving a dimensionality reduction using singular value decomposition (SVD).
Context-group discrimination can be generalized to do a discrimination task that goes beyond the notion of sense that underlies many other contributions to the disambiguation literature.
If the ambiguous word's occurrences are clustered into a large number n of clusters (e.g., n = 10), then the clusters can capture fine contextual distinctions.
Consider the example of space.
For a small number of clusters, only the senses "outer space" and "limited extent in one, two, or three dimensions" are separated.
If the word's occurrences are clustered into more clusters, then finer distinctions such as the one between "office space" and "exhibition space" are also discovered.
Note that differences between sense entries in dictionaries are often similarly fine-grained.
1 The
basic idea of the algorithm was first described in Schi~tze (1992b).
98 Schiitze Automatic Word Sense Discrimination I WORD I TRAINING TEXT \[VECTORS I WORD SPACE ----T:xxOx!
~x \[\] xi/ L.
Xx~". '......x.
.......... ", TEST CONTEXT / Figure 1 The basic design of context-group discrimination.
Contexts of the ambiguous word in the training set are mapped to context vectors in Word Space (upper dashed arrow) by summing the vectors of the words in the context.
The context vectors are grouped into clusters (dotted lines) and represented by sense vectors, their centroids (squares).
A context of the ambiguous word ("test context") is disambiguated by mapping it to a context vector in Word Space (lower dashed arrow ending in circle).
The context is assigned to the sense with the closest sense vector (solid arrow).
Even if the contextual distinctions captured by generalized context-group discrimination do not line up perfectly with finer distinctions made in dictionaries, they still help characterize the contextual meaning in which the ambiguous word is used in a particular instance.
Such a characterization is useful for the information-access applications described above, among others.
The basic idea of context-group discrimination is to induce senses from contextual similarity.
There is some evidence that contextual similarity also plays a crucial role in human semantic categorization.
Miller and Charles (1991) found evidence in several experiments that humans determine the semantic similarity of words from the similarity of the contexts they are used in.
We hypothesize that, by extension, senses are also based on contextual similarity: a sense is a group of contextually similar occurrences of a word.
The following sections describe the disambiguation algorithm, our evaluation, and the results of the algorithm for a test set drawn from the New York Times News Wire, and discuss the relevance of our approach in the context of other work on word sense disambiguation.
2. Context-Group Discrimination Context-group discrimination groups a set of contextually similar occurrences of an ambiguous word into a cluster, which is then interpreted as a sense.
The particular implementation of this idea described here makes use of a high-dimensional, real-valued vector space.
Context-group discrimination is a corpus-based method: all representations are derived from a large text corpus.
The basic design of context-group discrimination is shown in Figure 1.
Each occurrence of the ambiguous word in the training set is mapped to a point in Word Space (shown for one example occurrence: see dashed line from training text to Word Space).
The mapping is based on word vectors that are looked up in Word Space (to be described below).
Once all training-text contexts have been mapped to Word Space, the resulting point cloud is clustered into groups of points such that points are close to each other in each group and that groups are as distant from each other as 99 Computational Linguistics Volume 24, Number 1 possible.
The resulting clusters are delimited by dotted lines in the figure.
Each cluster is assumed to correspond to a sense of the ambiguous word (an assumption to be evaluated later).
The representative of each group is its centroid, depicted as a square.
After training, a new occurrence of the ambiguous word (labeled "test context" in the figure) is disambiguated by mapping its context to Word Space (see lower dashed line; the context's point is depicted as a circle).
The context is then assigned to the context group whose centroid is closest (solid arrow).
Finally, the context is categorized as being a use of the sense corresponding to this context group.
There are three types of entities that we need to represent: words, contexts, and senses.
They are represented as word vectors, context vectors, and sense vectors, respectively.
Word vectors are derived from neighbors in the corpus, context vectors are derived from word vectors, and sense vectors are derived by way of clustering from the distribution of context vectors.
The representational medium of a vector space was chosen because of its wide acceptance in information retrieval (IR) (see, e.g., Salton and McGill \[1983\]).
The vectorspace model is arguably the most common framework in IR.
Systems based on it have ranked among the best in many evaluations of IR performance (Harman 1993).
The success of the vector-space model motivates us to use it for the representation of words.
We represent words in a space in which each dimension corresponds to a word, just as documents and queries are commonly represented in this space in IR.
Another approach to computing word similarity is the representation of words in a document space in which each dimension corresponds to a document (Lesk 1969; Salton 1971; Qiu and Frei 1993).
There are fewer occurrence-in-document than wordco-occurrence events, so these word representations tend to be more sparse and, arguably, less informative than word-based representations.
Word vectors have also been based on hand-encoded features (Gallant 1991) and dictionaries (Sparck-Jones 1986; Wilks et al.1990). Corpus-based methods like the one proposed here have the advantage that no manual labor is required and that a possible mismatch between a general dictionary and a specialized text (e.g., on chemistry) is avoided.
Finally, word similarity can be computed from structural features like head-modifier relationships (Grefenstette 1994b; Ruge 1992; Dagan, Marcus, and Markovitch 1993; Pereira, Tishby, and Lee 1993; Dagan, Pereira, and Lee 1994).
Like document-based representations, structure-based representations are sparser than those based on co-occurrence.
It is debatable whether structural features are more informative than associational features (Grefenstette 1992, 1996) or not (Schtitze and Pedersen 1997).
Approaches to word representation closely related to ours were proposed by Niwa and Nitta (1994) and Burgess and Lund (1997).
Instead of co-occurrence counts, vector entries are mutual information scores between the word that is to be represented and the dimension words, in Niwa and Nitta's approach.
The algorithms for vector derivation and sense discrimination are described in what follows.
2.1 Word
Vectors A vector for word w is derived from the close neighbors of w in the corpus.
Close neighbors are all words that co-occur with w in a sentence or a larger context.
In the simplest case, the vector has an entry for each word that occurs in the corpus.
The entry for word v in the vector for w records the number of times that word v occurs close to w in the corpus.
It is this representational vector space that we refer to as WOrd Space.
Figure 2 gives a schematic example of two words being represented in a twodimensional space.
The representation is based on the co-occurrence counts of a hypo100 Schiitze Automatic Word Sense Discrimination Table 1 Co-occurrence counts for four words in a hypothetical corpus.
The words legal and clothes are interpreted as dimensions in Figure 2, judge and robe as vectors.
Vector Dimension judge robe legal 300 133 clothes 75 200 LEGAL 300 JUDGE / 133 f f/ ROBE I I 75 200 CLOTHES Figure 2 The derivation of word vectors, judge and robe are represented as word vectors in a two-dimensional space with the dimensions 'legal' and 'clothes'.
Co-occurrence data are from Table 1.
thetical corpus in Table 1.
The word judge has a value of 300 on the dimension "legal" because judge and legal co-occur 300 times with each other (see below for which words are selected as dimensions; a word can be a dimension of Word Space and represented as a word vector in Word Space at the same time).
This vector representation captures the typical topic or subject matter of a word.
For example, words like judge and law are closer to the "legal" dimension; words like robe and tailor are closer to the "clothes" dimension.
By looking at the amount of overlap between two vectors, one can roughly determine how closely they are related semantically.
This is because related meanings are often expressed by similar sets of words.
Semantically related words will therefore co-occur with similar neighbors and their vectors will have considerable overlap.
This similarity can be measured by the cosine between two vectors.
The cosine is equivalent to the normalized correlation coefficient: corr( fi, ~ ) = ~iN=l ViWi where ff and ~ are vectors and N is the dimension of the vector space.
The value of the cosine is higher, the more overlap there is between the neighbors of the two words whose vectors are compared.
If two words occur with exactly the same neighbors 101 Computational Linguistics Volume 24, Number 1 (perfect overlap), then the value of the cosine is 1.0.
If there is no overlap at all, then the value of the cosine is 0.0.
The cosine can therefore be used as a rough measure of semantic relatedness between words.
What words should serve as the dimensions of Word Space?
We will experiment with two strategies: a global and a local one.
The local strategy focuses on the contexts of the ambiguous words and ignores the rest of the corpus.
The global strategy is to select the n most frequent words of the corpus as features and use them regardless of the word that is to be disambiguated.
(See Karov and Edelman \[1996\] for a different approach that selects features according to a combination of global frequency and local salience).
For local selection, we can also use a frequency cutoff.
As an alternative, we will test selection according to a X 2 test.
For the frequency-based selection criterion, the neighbors of the ambiguous word in the corpus are counted.
A neighbor is any word that occurs at a distance of at most 25 words from the ambiguous word (that is, in a 50word window centered on the ambiguous word).
The 1,000 most frequent neighbors are chosen as the dimensions of the space.
For the x2-based criterion, a x2-measure of dependence is applied to a contingency table containing the number of contexts of the ambiguous word in which the candidate word occurs (N++) and does not occur (N+_), and the number of contexts without an occurrence of the ambiguous word in which the candidate word occurs (N_+) and does not occur (N__).
X2 = N(N++N__ N+_N_+) 2 (N++ + N+_)(N_+ + N__)(N++ + N_+)(N+_ + N__) The underlying assumption in using the x2-test is that candidate words whose occurrence depends on whether the ambiguous word occurs will be indicative of one of the senses of the ambiguous word and hence useful for disambiguation.
2 After
1,000 words have been selected in local selection, word vectors are formed by collecting a 1,000-by-I, 000 matrix C, such that element cq records the number of times that words i and j co-occur in a window of size k.
Column n (or, equivalently, row n) of matrix C represents word n.
Note that C is symmetric since the words that are represented as word vectors are also those that form the dimensions of the 1,000dimensional space.
We chose a window size of k = 50 because no improvement of discrimination performance was found in Schfitze (1997) for k > 50.
For global selection, we choose the 20,000 most frequent words as features and the 2,000 most frequent words as dimensions of Word Space.
A global 20, 000-by-2, 000 co-occurrence matrix is derived from the corpus.
Association data were extracted from the training set consisting of 17 months of the New York Times News Service, June 1989 through October 1990.
The size of this set is about 435 megabytes and 60.5 million words.
Two months (November 1990 and May 1989; 46 megabytes, 5.4 million words) were set aside as a test set.
2.2 Context
Vectors The representation for words derived above conflates senses.
For example, both senses of the word suit ('lawsuit' and 'garment') are summed in its word vector, which will therefore be positioned somewhere between the 'legal' and 'clothes' dimensions in Figure 2.
We need to go back to individual contexts in the corpus to acquire information about sense distinctions.
Contexts are represented as context vectors in Word Space.
2 Candidate
words are selected after a list of 930 stopwords has been removed.
This stop list was based on the one used in the Text Data Base system (Cutting, Pedersen, and Halvorsen 1991).
102 Sch~tze Automatic Word Sense Discrimination LEGAL CENTROID LAW / ~JUDGE /iJ~ISTATUTE / I I /// /#SUIT /// I/~// I/1/ CLOTHES Figure 3 The derivation of context vectors.
A context vector is computed as the centroid of the words occurring in the context.
The words in this example context are law, judge, statute, and suit.
A context vector is the centroid (or sum) of the vectors of the words occurring in the context.
Figure 3 shows the context vector of an example context of suit containing the words law, judge, statute, and suit.
Note that the context vector is closer to the "legal' than to the 'clothes' dimension, thus capturing that the context is a 'legal' use of suit.
(The true sum of the four vectors is longer than shown.
Since all correlation coefficients are normalized, the length of a vector does not play a role in the computations).
The centroid "averages" the direction of a set of vectors.
If many of the words in a context have a strong component for one of the topics (like 'legal' in Figure 3), then the average of the vectors, the context vector, will also have a strong component for the topic.
Conversely, if only one or two words represent a particular topic, then the context vector will be weak on this component.
The context vector hence represents the strength of different topical or semantic components in a context.
In the computation of the context vector, we will weight a word vector according to its discriminating potential.
A rough measure of how well word wi discriminates between different topics is the log inverse document frequency used in information retrieval (Salton and Buckley 1990): ai = lg/~ / where ni is the number of documents that wi occurs in and N is the total number of documents.
Poor discriminators of topics are words such as idea or help that are relatively uniformly distributed and therefore have a high document frequency.
Good content discriminators like automobile or China have a bursty distribution (they have several occurrences in a short interval if they occur at all \[Church and Gale 1995\]), and therefore a low document frequency relative to their absolute frequency.
Other algorithms for computing context vectors have been proposed by Wilks et al.(1990) (based on dictionary entries), Gallant (1991) (based on hand-encoded semantic features), Grefenstette (1994b) (based on light parsing), and Niwa and Nitta (1994) (a comparison of dictionary-based and corpus-based context vectors).
103 Computational Linguistics Volume 24, Number 1 LEGAL SENSE 1 Cl..,,,7,s ~SENSE CLOTHES Figure 4 The derivation of sense vectors.
Sense vectors are derived by clustering the context vectors of an ambiguous word (here, cl, c2, c3, c4, c5, c6, c7, and cs), and computing sense vectors as the centroids of the resulting clusters.
The vectors SENSE 1 and SENSE 2 are the sense vectors of clusters {cl, c2, c3, c4} and {cs, c6, c7, cs}, respectively.
2.3 Sense
Vectors Sense representations are computed as groups of similar contexts.
All contexts of the ambiguous word are collected from the corpus.
For each context, a context vector is computed.
This set of context vectors is then clustered into a predetermined number of coherent clusters or context groups using Buckshot (Cutting et al.1992), a combination of the EM algorithm and agglomerative clustering.
The representation of a sense is simply the centroid of its cluster.
It marks the portion of the multidimensional space that is occupied by the cluster.
We chose the EM algorithm for clustering since it is guaranteed to converge on a locally optimal solution of the clustering problem.
In our case, the solution is optimal in that the sum of the squared distances between context vectors and their centroids will be minimal.
In other words, the centroids are optimal representatives for the context vectors in their cluster.
One problem with the EM algorithm is that it finds a solution that is only locally optimal.
It is therefore important to find a good starting point since a bad starting point will lead to a local minimum that is not globally optimal.
Some experimental evidence given below shows that cluster quality varies considerably depending on the initial parameters.
In order to find a good starting point, we use group-average agglomerative clustering (GAAC) on a sample of context vectors.
For each of the 2,000 clustering experiments described below, we first choose a random sample of 50.
This size is roughly equal to v'~, the number of context vectors to be clustered.
Since GAAC is of time complexity O(n2), this guarantees overall linear time complexity of the clustering procedure.
If the training set has more than 2,000 instances of the ambiguous word, 2,000 context vectors are selected randomly.
The centroids of the resulting clusters are then the parameters for the first iteration of EM.
We compute five iterations of the EM algorithm for all experiments since in most cases only a few, if any, context vectors were reassigned in the fifth iteration.
Both the EM algorithm and group-average agglomerative clustering are described in more detail in the appendix.
104 Schfitze Automatic Word Sense Discrimination An example is shown in Figure 4.
The clustering step has grouped context vectors cl, c2, c3, and c4 in the first group and c5, c6, c7, and c8 in the second group.
The sense vector of the first group is the centroid labeled SENSE 1, the sense vector of the second group the centroid labeled SENSE 2.
The result of clustering depends on the representation of context vectors.
For this reason, we also investigate a transformation of the multidimensional space via a singular value decomposition (SVD) (Golub and van Loan 1989).
SVD is a form of dimensionality reduction that finds the major axes of variation in Word Space.
Context vectors can then be represented by their values on these principal dimensions.
The motivation for applying SVD here is much the same as the use of Latent Semantic Indexing (LSI) in information retrieval (Deerwester et al.1990). LSI abstracts away from the surface word-based representation and detects underlying features.
When similarity is computed on these features (via cosine between SVD-reduced context vectors), contextual similarity can be, potentially, better measured than via cosine between unreduced context vectors.
The appendix defines SVD and gives an example matrix decomposition.
In this paper, the word vectors will be reduced to 100 dimensions.
The experiments reported in Schfitze (1992b, 1997) give evidence that reduction to this dimensionality does not decrease accuracy of sense discrimination.
Space requirements for context vectors are reduced to about 1/10 and 1/20 for a 1,000-dimensional and a 2,000dimensional Word Space, respectively.
Although most word vectors are sparse, context vectors are dense, since they are the sum of many word vectors.
Time efficiency is increased on the same order of magnitude when the correlation of context vectors and sense vectors is computed.
The computation of the SVD's in this paper took from a few minutes per word for the local feature set to about three hours for the global feature set.
2.4 Application
of Context-Group Discrimination Context-group discrimination uses word vectors and sense vectors as follows to discriminate occurrences of the ambiguous word.
For an occurrence t of the ambiguous word v:  Map t into its corresponding context vector ~ in Word Space using the vectors of the words in t's context (the lower dashed line in Figure 1).
 Retrieve all sense vectors ~j of v (the two points marked as squares in the figure).
 Assign t to the sense j whose sense vector ~j is closest to ~ (assignment shown as a solid arrow).
This algorithm selects the context group whose sense vector is closest to the context vector of the occurrence of the word that is to be disambiguated.
Context vectors and sense vectors capture semantic characteristics of the corresponding context and sense, respectively.
Consequently, the sense vector that is closest to the context vector has the best semantic match with the context.
Therefore, context-group discrimination categorizes the occurrence as belonging to that sense.
3. Evaluation We test context-group discrimination on the 10 natural ambiguous words that formed the test set in Schfitze (1992b) and on 10 artificial ambiguous words.
Table 2 glosses the major senses of the 20 words.
105 Computational Linguistics Volume 24, Number 1 Table 2 Number of occurrences of test words in training and test set, percent rare senses in test set, baseline performance (all occurrences assigned to most frequent sense), and the two main senses of each of the 20 artificial and natural ambiguous words used in the experiment.
Word Training Test.
Rare Senses Baseline Frequent Senses wide range/ consulting firm 1,422 149 0% 62% heart disease/ reserve board 1,197 115 0% 54% urban development/ cease fire 1,582 101 0% 50% drug administration / fernando valley 1,465 122 0% wide range consulting firm heart disease reserve board urban development cease fire 52% drug administration fernando valley economic development / right field 1,030 88 0% 68% national park/ judiciary committee 1,279 122 0% 70% japanese companies / city hall 1,569 208 0% 58% drug dealers / paine webber 1,183 104 0% 55% league baseball/ square feet 1,097 143 0% 66% pete rose/ nuclear power 1,245 103 0% 52% capital/s 13,015 200 2% 64% interest/s 21,374 200 4% 58% motion/s 2,705 200 0% 55% plant/s 12,833 200 0% 54% economic development right field national park judiciary committee japanese companies city hall drug dealers paine webber league baseball square feet pete rose nuclear power stock of goods seat of government a feeling of special attention a charge for borrowed money movement proposal for action a factory living being 106 Schiitze Automatic Word Sense Discrimination Table 2 Continued.
Word Training Test Rare Senses Baseline Frequent Senses ruling 5,482 200 3.5% 60% an authoritative decision to exert control, or influence space 9,136 200 0% 56% area, volume outer space suit/s 7,467 200 12.5% 57% an action or process in a court a set of garments tank/s 3,909 200 4.5% 90% a combat vehicle a receptacle for liquids train/s 4,271 200 1.5% 74% a line of railroad cars to teach vessel/s 1,618 144 13.9% 69% a ship or plane a tube or canal (as an artery) Artificial ambiguous words or pseudowords are a convenient means of testing disambiguation algorithms (Schtitze 1992a; Gale, Church, and Yarowsky 1992).
It is time-consuming to hand-label a large number of instances of an ambiguous word for evaluating the performance of a disambiguation algorithm.
Pseudowords circumvent this need: Two or more words, e.g., banana and door, are conflated into a new type: banana~door.
All occurrences of either word in the corpus are then replaced by the new type.
It is easy to evaluate disambiguation performance for pseudowords since one can go back to the original text to decide whether a correct decision was made.
To create the pseudowords shown in Table 2, all word pairs were extracted from the corpus, i.e., all pairs of words that occurred adjacent to each other in the corpus in a particular order.
All numbers were discarded, since numbers do not seem to involve sense ambiguity.
Pseudowords were then created by randomly drawing two pairs from those that had a frequency between 500 and 1,000 in the corpus.
Pseudowords were generated from pairs rather than simple words because pairs are less likely than words to be ambiguous themselves.
Pair-based pseudowords are therefore good examples of ambiguous words with two clearly distinct senses.
Table 2 indicates how often the ambiguous word occurred in the training and test sets, how many instances were instances of rare senses, and the baseline performance that is achieved by assigning all occurrences to the most frequent sense.
In the evaluation given here, only senses that account for at least 15% of the occurrences of the ambiguous word are taken into account.
Rare senses are those that account for fewer than 15% of the occurrences.
The words in Table 2 each had two frequent senses.
The frequency of rare senses ranges from 0% to 13.9%, with an average of 2.1%.
Rare senses are not eliminated from the training set.
The training and test sets were taken from the New York Times News Service as described above (training set: June 1989-October 1990; test set: November 1990, May 1989).
If a word had more than 200 occurrences in the test set, then only the first 200 occurrences were included in the evaluation.
The labeling of words in the test corpus was performed by the author.
The distinc107 Computational Linguistics Volume 24, Number 1 tions between the senses in Table 2 are intuitively clear.
For example, the probability of a context in which suit could at the same time refer to a set of garments and an action in court is very low.
Consequently, there were fewer than five instances where the appropriate sense was not obvious from the immediate context.
In these cases, the sense that seemed more plausible to the author was assigned.
It is important to evaluate on a test set that is separate from the training set.
Context-group discrimination is based on the distribution of context vectors in the training set.
The distribution in the training set is often a bad model for the distribution in the test set.
In practice, the intended text of application will be from a time period not covered in the training set (for example, newswire text from after the date of training).
Word distributions can change considerably over time.
The test set was therefore constructed to be from a time period different from the time period of the training set.
This is also the reason that we do not do cross-validation.
Cross-validation respecting the constraint that test and training sets be from different time periods would have required a test set several times larger than the one that was available.
Clustering and evaluating on the same set is also problematic because of sampling variation.
Consider the following example.
We have a set of three context vectors C : {C 1 : (1), c2 = (2), C 3 : (3)} in a one-dimensional space.
Contexts 1 and 2 are uses of sense 1, context 3 is a use of sense 2.
If C is used as both training and evaluation set, then average performance is 83% (with probability 0.5, we get centroids 1.5 and 3 and 100% accuracy, with probability 0.5, we get centroids 1 and 2.5 and 67% accuracy).
If we split C into a training set T of size 2 and a test set E of size 1, we get an average performance of 67% (100% for E = {cl}, 50% for E = {c2}, 50% for E = {3}), which is lower than 83%.
This example shows that conflating training and test set can result in artificially high performance.
An advantage of context-group discrimination is that the granularity of sense distinctions is an adjustable parameter of the algorithm.
Experiments run directly for the senses in Table 2 will test the algorithm's ability to discriminate coarse sense distinctions.
To test performance for fine-grained sense distinctions (e.g., 'office space' vs.
'exhibition space'), we will run two experiments, one that evaluates performance for clustering the context vectors of a word into ten clusters and an information retrieval experiment in which the number of clusters is also large for sufficiently frequent words.
The goal of the 10-cluster experiments is to induce more fine-grained sense distinctions than in the 2-cluster experiments.
However, it is harder to determine the ground truth for fine sense distinctions.
When it comes to fine distinctions, a large number of occurrences are indeterminate or compatible with several of the more finely individuated senses (cf.
Kilgarriff \[1993\]).
For this reason, experiments with a large number of clusters were evaluated using two indirect measures.
The first measure is accuracy for two-way discriminations, i.e., the degree to which each of the ten clusters contained only one of the two "coarse" senses.
This evaluation is indirect because a cluster that contains, say, only 'limited extent in one, two, or three dimensions' uses of space would be deemed 100% correct, yet it could be randomly mixed as far as fine sense distinctions are concerned (e.g., 'office space' vs.
'exhibition space').
The author inspected the data and found good separation of fine-grained senses in the 10-cluster experiments to the extent that the evaluation measure indicated good performance on the two-way discrimination task.
However, because of the above-mentioned subjectivity of judgements for fine sense distinctions, this is hard to quantify.
Results from a second evaluation on an information retrieval task will be presented in Section 4.2 below.
We will show that sense-based information retrieval (in which the relevance of documents to a query is determined using context-group discrimination) 108 Schiitze Automatic Word Sense Discrimination improves the performance of an IR system considerably.
Since the success of sensebased retrieval depends on the accuracy of context-group discrimination, we can infer that the algorithm reliably assigns ambiguous instances to induced senses even in the fine-grained case.
4. Experiments 4.1 Word Sense Discrimination Table 3 shows experimental results for context-group discrimination.
There were four conditions that were varied in the experiments (as described in Section 2):  local vs.
global feature selection  feature selection according to frequency vs.
X 2  term representations vs.
SVD-reduced representations  number of clusters (2 vs.
10) For local feature selection, the other three parameters are varied systematically (the first eight columns of Table 3).
For global feature selection, selection according to X 2 is not possible, since the X 2 test presupposes an event (like the occurrence of an ambiguous word) that the occurrence of candidate words depends on.
There is no such event for global feature selection.
A larger number of dimensions (2,000) is used for the global variant of the algorithm in order to get coverage of a large range of topics that might be relevant for disambiguation.
We therefore apply SVD in the global feature selection case.
Even if word vectors are sparse, context vectors are usually not.
Clustering 2,000dimensional vectors is computationally expensive, so that we only ran experiments with SVD-reduced vectors for the global variant.
Ten experiments with different randomly chosen initial parameters were run for each of the 200 combinations of the different levels of Word, Representation, and Clustering.
The mean percentage correctness and the standard deviation for each such set of 10 experiments is shown in the cells of Table 3.
We give mean and deviation of the percentage of correctly labeled occurrences of all instances in the training set ("total" = "t.'), of the instances of sense 1 ("$1") and of the instances of sense 2 ("$2").
The bottom row of the table gives averages of the total percentage correct numbers over the 20 words covered.
The rightmost column gives averages of the means over the 10 experiments.
We analyzed the results in Table 3 via analysis of variance (ANOVA, see, for example, Ott \[1992\]).
An ANOVA was performed for a 20 x 5 x 2 design with 10 replicates.
The factors were Word, Representation (local, frequency-based, terms; local, frequency-based, SVD; local, Xa-based, terms; local, x2-based, SVD; global, frequencybased, SVD), and Clustering (coarse = 2 clusters, fine = 10 clusters).
Percentages were transformed using the functionf(X) = 2 x sin -1 (v/X) as recommended by Winer (1971).
The transformed percentages have a distribution that is close to a normal distribution as required for the application of ANOVA.
We found that the effects of all three factors and all interactions was significant at the 0.001 level.
These effects are discussed in what follows.
Factor Word.
In general, performance for pseudowords is better than for natural words.
This can be explained by the fact that pseudowords have two focussed senses--the two word pairs they are composed of.
In contrast, some of the senses of natural ambiguous 109 Computational Linguistics Volume 24, Number 1 Table 3 Results of disambiguation experiments.
Rows give total accuracy for each word ("t').
as well as accuracy for the two senses separately ("$1", "$2").
The average in the bottom row is an average over total ("t').
accuracy numbers only.
Columns describe experimental conditions and the mean ("\]~") and standard deviation ("a") of 10 replications of each experiment.
The rightmost column contains an average over the mean values of the 10 experiments.
Pseudowords are abbreviated to the first words of pairs.
Local Global wide~consul.
$1 $2 55 16 100 0 69 31 92 9 74 25 92 13 69 24 82 10 89 6 94 4 t.
51 4 62 0 60 4 66 3 56 6 64 3 65 8 66 2 87 3 87 3 heart~reserve $1 66 0 78 11 100 0 99 4.
72 0 75 12 100 0 98 2 100 0 100 0 $2 100 0 90 7 100 0 100 0 100 0 94 5 98 0 100 1 100 0 100 0 t.
84 0:85 2 100 0 99 287 0 85 3 99 0 99 1 100 0 100 0 urban~cease $1 86 1 87 2 96 0 97 191 4 90 8 98 0 98 1 100 0 98 2 ~rugffern.
.ocon./right nat./jud.
iap./city $1 71rug/paine $1 !eague/square $1 pete~nuclear $1 :apital $1 interest $1,wtion $1 ~lant $1 ruling $1 ;pace $1 ~uit S1 ~ank $1 ~rain $1 vessel $1 Average X 2 Frequency Frequency Terms I SVD Terms I SVD SVD 2 10 2 10 2 10 2 10 2 10 45 16 0 0 45 47 22 22 25 26 19 30 59 37 39 17 84 2 76 8 41.4 81.6 66.4 88.8 98.2 93.8 94.1 $2 78 l J 70 7 100 0 100 1 73 24 80 11 100 0 96 5 100 0 100 0 89.7 t.
82 0i 79 3 98 0 99 1 I 82 10 85 3 99 0 97 2 100 0 99 1 92.0 S1 89 l i 87 7 98 0 100 1 I 94 4 88 5 98 0 95 1 100 0 100 0 94.9 $2 78 1 77 12 95 0 100 1 60 35 90 7 59 8 96 2 100 0 100 1 85.5 t.
84 I 82 3 97 0 100 0 78 15 89 2 80 4 96 1 100 0 100 0 90.6 $1 72 2 89 6 92 1 95 1 92 0 87 5 98 0 98 3 100 0 100 0 92.3 $2 89 0 67 13 96 0 96 2 87 2 91 5 96 0 97 2 100 0 100 0 91.9 t.
78 1 82 1 93 1 95 1 90 1 88 2 98 0 97 1 100 0 100 0 92.1 S1 91 1 96 3 98 0 97 0 99 0 99 0 98 01 97 1 100 0 100 0 97.5 $2 73 0 53 14 100 0 100 1 70 0 61 9 92 0i 96 4 100 0 98 2 84.3 t.
85 1 83 3 98 0 98 0 90 0 87 3 96 0i 97 1 100 0 99 1 93.3 84 18 90 7 96 1 95 1 94 2 91 4 97 2 93 2 99 0 99 1 93.8 i $2 56 10 63 15 71 23 87 4 66 17 71 10 88 5 90 5 99 0 99 1 79.0 t.
73 12 79 3 86 9 92 1 82 6 83 2 93 1 92 1 99 0 99 0 87.8 68 6 76 9 86 1 81 9 70 18 81 14 95 0 85 4 100 0 97 3 83.9 $2 86 13 86 8 100 0 99 1 68 23 87 14 100 0 98 3 100 0 100 0 92.4 t.
76 9 80 2 93 0 89 5 69 19 83 3 97 0 91 2 100 0 98 2 87.6 54 8 77 8 66 41 96 3 32 31 77 10 56 32 90 4 100 0 100 1 74.-----8 $2 60 20 94 3 100 0 99 1 91 18 94 5 100 0 96 4 100 0 99 2 93.3 t.
58 16 88 1 88 14 98 2 71 13 88 1 85 11 94 3 100 0 99 li 86.9 91 0 78 10 94 1 98 2 72 21 90 10 86 19 95 6 100 0 99 1 90.3 $2 78 0 80 8 94 0 91 2 96 1 81 13 88 20 91 7 100 0 99 1 I 89.8 t.
84 0 79 2 94 0 95 2 83 11 86 3 87 19 93 4 100 0 99 1 90.0 88 16 97 3 91 4 96 2 91 3 97 3 93 1 93 2 92 1 93 1 93.1 $2 27 23 36 11 23 34 87 7 36 34 57 9 80 27 88 6 96 1 89 5 61.9 t.
66 7 75 3 66 13 93 2 71 13 82 2 88 10 91 1 94 0 91 1; 81.7 82 18 77 8 95 1 86 5 96 0 93 3 94 1 91 4 96 0 89 3 89.9 $2 43 37 87 4 90 6 96 2 83 1 85 3 71 35 91 4 88 1 93 31 82.7 t.
66 14 81 4 93 2 90 2 90 0 90 1 84 15 91 2 93 0 91 li 86.9 57 14 72 6 58 1 84 1 61 17 88 6 90 15 93 4 85 1 91 5 I 77.-----------9 $2 60 15 70 10 97 0 91 8 58 20 63 16 51 24 77 7 88 13 71 151 72.6 t.
58 10 71 3 76 1 87 3 59 12 77 4 73 12 86 2 86 5 82 5 i 75.5 i 73 20 0 0 92 4 " 0 0 91 16 0 0 54 46 2 5 70 37 0 0' 38.-----2 $2 47 12 100 0 37 5 100 0 41 30 100 0 59 36 100 0 70 26 100 0 75.4 t.
59 8 54 0 63 4 54 0 64 11 54 0 56 7 55 2 70 13 54 0 58.3 75 1 61 13 84 2 71 14 81 1 65 15 79 7 79 13 85 0 82 3 76.2 $2 86 1 90 4 93 1 96 3 87 1 93 4 93 5 95 2 95 0 95 1 92.3 t.
82 0 78 3 90 1 86 4 84 0 82 4 88 1 89 4 91 0 90 1 86.0 10 25 48 30 0 0 48 22 15 25 38 24 16 25 51 15 8 25 54 16 28.8 $2 87 7 91 7 96 0 95 3 97 1 96 2 96 2 96 2 94 10 93 3 94.1 t.
53 7 72 9 54 0 74 8 61 11 71 10 60 12 76 6 56 5 75 6 65.2 83 1 77 5 80 2 85 6 81 2 84 7 94 2 88 8 95 0 83 6 85.0 $2 80 0 i 84 4 93 0 94 2 92 2 88 6 86 29 97 2 96 0 97 2 90.7 t.
82 1 I 80 2 85 1 89 3 86 1 86 2 91 12\] 92 4 95 0 89 3 87.5 29 91 7 6 80 8 32 13 88 5 12 14 86 29i~ 31 22 92 3 28 19 48.5 $2 94 15 100 0 92 95 1 100 0 87 5\] 99 2 84 1 99 2 94.9 4 99 0 t.
87 13 I 90 1 90 3 92 1 95 1 91 1 87 2i 92 1 85 1 92 2 90.1 60 21 100 0 74 16 100 0 89 20 100 0 95 81100 1 79 19 100 0 89.7 $2 40 21 0 0 12 20 0 0 18 29 0 0 8 21! 1 3 55 31 0 0 13.4 t.
55 10' 74 0 58 7 74 0 69 11 74 0 72 1' 74 0 73 8 74 0 69.7 84 18 86 14 100 0 99 1 85 30 90 7 20 42 94 2 30 48 79 5 76.7 $2 76 14 84 9 100 0 100 0 89 3 92 5 79 17 100 0 81 9 100 0 90.1 t.
79 15\] 85 2 100 0 100 0 88 11 91 2 61 14\] 98 1 65 13 93 1 ..
86.O i\[ 72.1 I 77.9 \[ 84.1 i 88.5 i 77.8 i 81.8 i 82.9 \[ 88.3 i 89.7 i 90.6 I\] 110 Schfitze Automatic Word Sense Discrimination Table 4 The Tukey W test shows significantly different performance for the five representations.
Proportions are transformed using fiX) = 2 x sin -1 (v/X).
The rightmost column contains the accuracy A in percent that would correspond to the average value Y in the second column (i.e., f(A) = Y).
Significant difference for a = 0.01:0.034 Average of Difference Corresponding Level 2 x sin-l(V'X) from Closest Accuracy local, )/2, terms 2.11 0.13 76% local, frequency, terms 2.24 0.13 81% local, frequency, SVD 2.44 0.06 88% local, X 2, SVD 2.50 0.06 90% global, frequency, SVD 2.66 0.16 94% words (for example, space and interest) are composed of many different subsenses that are hard to identify for both people and computers.
The only pseudoword with poor performance is wide range/consultingfirm.
This is an illustrative example of a weakness of the particular implementation of contextgroup discrimination chosen here.
Since we only rely on topical information, a word composed of a nontopical sense, like wide range, that can occur in almost any subject area is disambiguated poorly.
The 'area, volume' sense of space and the 'teaching' sense of train are similarly topically amorphous and therefore hard if only topical information is considered.
The poor performance for 'plant' in the 10-cluster experiments is probably due to the way training-set clusters were assigned to senses.
The training set was clustered into 20 clusters and each cluster was given a sense label.
This procedure introduces many misclassifications of individual instances in the training set.
In contrast, a performance of 92% was achieved in Schiitze (1992b) by hand-categorizing the training set, instance by instance.
Note that for some experimental conditions and for some words, performance of two-group clustering is below baseline.
In a completely unsupervised setting, we have to make the assumption that the two induced clusters correspond to two different senses.
In the worst case, we will get, two clusters with identical proportions of the two senses and an accuracy of 50%, below the baseline of assigning all occurrences to a sense that occurs in more than 50% of all cases.
For example, for vessel the worst case would be two clusters each with 69% 'ship' instances and 31% 'tube' instances.
Overall accuracy would be 0.5 x .69 + 0.5 x .31 = 0.5.
It could be argued that the true baseline for unsupervised two-group clustering is 50%, not the proportion of the most frequent sense.
Factor Representation.
A Tukey W test (Ott 1992) was performed to evaluate the factor Representation.
The Tukey W test determines the least significant difference between sample means.
That is, it yields a threshold such that if two levels of a factor differ by more than the threshold, then they are significantly different.
For the factor Representation, in our case, this least significant difference is 0.034 for a = 0.01.
Table 4 shows that all differences are significant.
This is evidence that SVD representations perform better than term representations and that global representations perform better than local representations.
The advantage of SVD representations is partly due to the use of a normality assumption in clustering.
This is a poor approximation for term 111 Computational Linguistics Volume 24, Number 1 Table 5 Occurrence of selected term features in the test set.
The table shows number of words occurring in the test set (averaged over the 20 ambiguous words); number of words occurring per context (averaged over contexts); proportion of words from one representation occurring in another (averaged first over contexts, then over ambiguous words; e.g., on average 91% of X2-selected terms were also in the set selected by local frequency); average number of contexts that a selected term occurred in (e.g., on average a xa-selected term occurred in 8.7 contexts of the artificial ambiguous words, averaged over the words in a context).
~2 Local Frequency Global Frequency Words Occurring in Test Set 283.0 571.2 489.6 Words per Context 6.1 11.1 9.2 Term Overlap X 2 100% 91% 53% local frequency 51% 100% 68% global frequency 34% 78% 100% Average Frequency of terms artificial words 8.7 6.7 16.7 natural words 39.5 22.4 17.5 representations, but is more accurate for SVD-reduced representations.
Why do globally selected features perform better?
Table 5 presents data on the occurrence of selected terms in the test set that are relevant to this question.
Note first that locally selected features seem to do better than globally selected ones on several measures.
More locally selected features occur in the test set ("words occurring in test set": 571.2 vs.
489.6), more local features occur in the individual contexts ("words per context": 11.1 vs.
9.2), and more global features are also local features than vice versa (on a per-context basis, 78% of global features are also local features, but only 68% of local features are also global features), suggesting that local features capture more information than global features.
The first two measures also show that X2-selected features suffer from sparseness.
Both the total number of features that occur in the training set and the number of words per context are small.
This evidence explains why SVD representations that address sparseness do better than term representations for X 2.
To explain the difference in performance between local and global frequency features, we have to break down average accuracy according to artificial and natural ambiguous words.
Average accuracy for artificial ambiguous words is 89.9% (2 clusters) and 92.2% (10 clusters) for local features and 98.6% (2 clusters) and 98.0% (10 clusters) for global features.
Average accuracy for natural ambiguous words is 76.0% (2 clusters) and 84.4% (10 clusters) for local features and 80.8% (2 clusters) and 83.1% (10 clusters) for global features.
These data show a clear split.
Performance of local and global features is comparable for natural ambiguous words.
Global features perform clearly better for artificial ambiguous words.
The last two rows of Table 5 explain this difference in behavior.
The numbers correspond to the average number of contexts that the selected features occur in (averaged first over the words in a context, then over contexts; e.g., a context with three selected terms occurring in 10, 3, and 15 contexts of the ambiguous word in the training set would have an average number of contexts of (10+3+15)/3 = 9.3).
These averages are 11.2 Schi~tze Automatic Word Sense Discrimination small for X 2 and local frequency in the case of artificial ambiguous words.
Clustering can only work well if contexts have enough elements in common so that similarity can be determined robustly.
Apparently, there were too few elements in common for X 2 and local frequency in the case of artificial ambiguous word (and the patterns were so sparse that even SVD was not an effective remedy).
The problem is that artificial ambiguous words are much less frequent in the training set than natural ambiguous words (average frequencies of 1,306.9 vs.
8,231.0), so that reliable feature selection is harder for artificial ambiguous words.
With ample information on natural ambiguous words available in the training set, features can be selected that will occur densely in the test set.
The quality of feature selection for artificial ambiguous words was less successful due to smaller training set sizes.
This analysis reiterates the importance of a clear separation of training and test sets.
Performance numbers will be artificially high if feature selection is done on both training and test sets, avoiding the problems with feature coverage demonstrated in Table 5.
Since global feature selection is simpler and as effective as local approaches, global feature selection is the preferred implementation of context-group discrimination in the general case.
Note, however, that different words may have different optimal representations.
For example, local features work best for vessel.
There are similar individual differences for frequency vs.
x2-based selection.
Frequency-based selection is best for suit, but x2-based selection is better for vessel, at least for SVD-reduced representations.
Factor Clustering.
Fine clustering is generally better than coarse clustering.
The one case for which coarse clustering comes close to the performance of fine clustering is global feature selection.
But this small difference is almost entirely due to the bad performance of fine clustering for plant, which is likely to be due to insufficient handcategorization of the training set, as explained above.
That fine clustering performs better than coarse clustering is not surprising, since more information is used in the evaluation of fine clustering: the labeling of clusters in the training set.
Only coarse clustering is evaluated as strictly unsupervised disambiguation, since we do not have an evaluation set for fine sense distinctions.
Variance. In general, the variance of discrimination accuracy is higher for coarse clustering than for fine clustering.
This is not surprising, given the fact that we evaluate both types of clustering on how well they do on a two-way distinction.
There may be several quite different ways of dividing a set of context vectors into two groups.
But if we first cluster into ten groups and assign these groups to two senses, then the resulting two-way partitions are more likely to resemble each other (even if the initial 10-group clusterings are not very similar).
The experiments indicate that context-group discrimination based on globally selected features is the best implementation in the general case.
The algorithm achieves above-baseline performance (with a small number of exceptions for certain parameter settings).
The average performance of the SVD-based representations of 83% to 91% is satisfactory, although inferior by about 5% to 10%, to disambiguation with minimal manual intervention (e.g., Yarowsky \[1995\]).
3 3 Manually supplied priming information about senses is not the only difference between context-group discrimination and other disambiguation algorithms.
Could one of the other differences be responsible for the difference in performance?
The fact that the error rate more than doubles when the seeds in Yarowsky's (1995) experiments are reduced from a sense's best collocations to just one word per sense suggests that the error rate would increase further if no seeds were provided.
113 Computational Linguistics Volume 24, Number 1 4.2 Application to Information Retrieval Our principal motivation for concentrating on the discrimination subtask is to apply disambiguation to information retrieval.
While there is evidence that ambiguity resolution improves the performance of IR systems (Krovetz and Croft 1992), several researchers have failed to achieve consistent experimental improvements for practically realistic rates of disambiguation accuracy.
Voorhees (1993) compared two term-expansion methods for information retrieval queries, one in which each term was expanded with all related terms and one in which it was only expanded with terms related to the sense used in the query.
She found that disambiguation did not improve the performance of term expansion.
In our study, we will use disambiguation to eliminate document-query matches that are due to sense mismatches (that is, the word in question is used in different types of context in the query and the document).
This approach decreases the number of documents that a query matches with whereas term expansion increases it.
Another important difference in this study is that longer queries are used.
Long queries (as they may arise in an IR system after relevance feedback) provide more context than the short queries Voorhees worked with in her experiments.
Sanderson (1994) modified a test collection by creating pseudowords similar to the ones used in this study.
He found that even unrealistically high rates of disambiguation accuracy had little or no effect on retrieval performance.
An analysis presented in Schfitze and Pedersen (1995) suggests that the main reason for the minor effect of disambiguation is that most of the pseudowords created in the study had a major sense that accounted for almost all occurrences of the pseudoword.
Creating this type of pseudoword amounts to adding a small amount of noise to an unambiguous word, which is not expbcted to have a large effect on retrieval performance.
To some extent, actual dictionary senses have the same property: one sense often accounts for a large proportion of occurrences.
However, this is not necessarily true when rare senses are not taken into account and when high-frequency senses are broken up into smaller groups (the example of 'office space' vs.
'exhibition space').
Large dictionaries tend to break up high-frequency senses into such more narrowly defined subsenses.
The successful use of disambiguation in our study may be due to the fact that rare senses, which are less likely to be useful in IR, are not taken into account and that frequent senses are further subdivided.
Good evidence for the potential utility of disambiguation in information retrieval was provided by Krovetz and Croft (1992).
They showed that there is a considerable amount of ambiguity even in technical text (which is often assumed to be less ambiguous than nonspecialized writing).
Many technical terms have nontechnical meanings that are used in addition to more specialized senses even in technical text (e.g., window and application in computer magazines, convertible in automobile magazines \[Krovetz 1997\]).
Krovetz and Croft also showed that sense mismatches (i.e., spurious matching words that were used in different senses in query and document) occurred significantly more often in nonrelevant than in relevant documents.
This suggests that eliminating spurious matches could improve the separation between nonrelevant and relevant documents and hence the overall quality of retrieval results.
In order to show that context-group discrimination is an approach to disambiguation that is beneficial in information retrieval, we will now summarize the experiment presented in Schfitze and Pedersen (1995).
That experiment evaluates sense-based retrieval, a modification of the standard vector-space model in information retrieval.
(We refer to the standard vector-space model as word-based retrieval).
In word-based retrieval, documents and queries are represented as vectors in a multidimensional space in which each dimension corresponds to a word (similar to the way that we repre114 Schi~tze Automatic Word Sense Discrimination sent word vectors in Word Space).
In sense-based retrieval, documents and queries are also represented in a multidimensional space, but its dimensions are senses, not words.
Words are disambiguated using context-group discrimination.
Documents and queries that contain a word assigned to a particular sense have a nonzero value on the corresponding dimension.
The test corpus in Sch~tze and Pedersen (1995) is the Category B TREC-1 collection (about 170,000 documents from the Wall Street Journal) in conjunction with its queries 51-75 (Harman 1993).
Sense-based retrieval improved average precision by 7.4% when compared to word-based retrieval.
A combination of word-based and sense-based retrieval increased performance by 14.4%.
The greater improvement of the combination is probably due to discrimination errors (i.e., the fact that discrimination is less than 100% correct), which are partially undone by combining sense and word evidence.
Improvement was particularly high when small sets of documents were requested, for example, 16.5% (sense-based) and 19.4% (wordand sense-based combined) for a recall level of 10% of relevant documents.
This experiment suggests a high utility of sense discrimination for information retrieval.
At first sight, sense-based retrieval may seem related to term expansion.
Both sense-based retrieval and term expansion take individual terms as the starting point for modifying the similarity measure that determines which documents are deemed most closely related to the query.
However, the two approaches are actually opposites of each other in the following sense.
Term expansion increases the number of matching documents for a query.
For example, if the query contains cosmonaut and expansion adds astronaut, then documents containing astronaut become additional nonzero matches.
Sense-based retrieval decreases the number of matches.
For example, if the word suit occurs in the query and is disambiguated as being used in the 'legal' sense, then documents that contain suit in a different sense will no longer match with the query.
5. Discussion What distinguishes context-group discrimination from other work on disambiguation is that no outside source of information need be supplied as input to the algorithm.
Other disambiguation algorithms employ various sources of information.
Kelly and Stone (1975) consider hand-constructed disambiguation rules; Lesk (1986), Krovetz and Croft (1989), Guthrie et al.(1991), and Karov and Edelman (1996) use on-line dictionaries; Hirst (1987) constructs knowledge bases; Cottrell (1989) uses syntactic and semantic structure encoded in a connectionist net; Brown et al.(1991) and Church and Gale (1991) exploit bilingual corpora; Dagan, Itai, and Schwall (1991) use a bilingual dictionary; Hearst (1991), Leacock, Towell, and Voorhees (1993), Niwa and Nitta (1994), and Bruce and Wiebe (1994) exploit a hand-labeled training set; and Yarowsky (1992) and Walker and Amsler (1986) perform computations based on a hand-constructed semantic categorization of words (Roget's Thesaurus and Longman's subject codes, respectively).
For some of these algorithms, the expense of supplying information to the disambiguation algorithm is relatively small.
For example, in many of the methods using hand-labeled training sets (e.g., Hearst \[1991\]), a relatively small number of training examples is sufficient.
Yarowsky has proposed an algorithm that requires as little user input as one seed word per sense to start the training process (Yarowsky 1995).
Such minimal user input will be a negligible burden for users in some situations.
However, consider the interactive information-access application described above.
When asked to improve their initial ambiguous information request many users will be reluctant to 115 Computational Linguistics Volume 24, Number 1 give a seed word or a set of good features for each sense of the word.
They are more likely to satisfy a request by the system to choose the correct sense (e.g., by mouse click), if example contexts corresponding to different senses are presented without the requirement of additional user interaction.
In an application like this, it is of great advantage that context-group discrimination does not require any manual intervention to induce senses.
Another body of related work is the literature on word clustering in computational linguistics (Brown et al.1992; Finch 1993; Pereira, Tishby, and Lee 1993; Grefenstette 1994a) and document clustering in information retrieval (van Rijsbergen 1979; Willett 1988; Sparck-Jones 1991; Cutting et al.1992). In contrast to this earlier work, we cluster contexts or, equivalently, word tokens here, not words (or, more precisely, word types) or documents.
The straightforward extension of word-type clustering and document clustering to word-token clustering would be to represent a token by all words it cooccurs with in its context and cluster these representations.
Such an approach based on first-order co-occurrence is used, for example, by Hearst and Plaunt (1993) for the representation of tiles or document subunits that are similar to our notion of context.
Instead, we use second-order co-occurrence to represent the tokens of ambiguous words: the words that occur with the token are in turn looked up in the training corpus and the words they co-occur with are used to represent the token.
Secondorder representations are less sparse and more robust than first-order representations.
In a cluster-based approach, the subdivision of the universe of elements into clusters depends on the representation.
If the representation does not capture the information crucial for distinguishing senses, then context-group discrimination performs poorly.
The clearest such example in the above experiments is the pseudoword wide range~consulting firm.
The algorithm does not do better than the baseline of always choosing the most frequent sense.
The reason is that the representation captures only topic information.
So a cluster will contain a group of contexts that are about the same topic.
Unfortunately, the pair wide range can come up in text about almost any topic.
Since there is no clear topical characterization of one sense of the pseudoword, context-group discrimination performs poorly.
The reliance on topical similarity may also be the reason that performance for pseudowords is generally better than performance for natural ambiguous words.
All pseudowords except for wide range/consultingJirm are composed of two pairs from different topics.
For example, heart disease and reserve board pertain to biology and finance, respectively, two clearly distinct topics.
On the other hand, the senses of some of the ambiguous words have less clear associations with particular topics.
For example, one can be trained to perform a wide variety of activities, so the 'teaching' sense of train can be invoked in many different topics.
Part of the superior performance for pseudowords is due to this different topic sensitivity of natural and artificial ambiguous words.
The limitation to topical distinctions is not so much a flaw of context-group discrimination as a flaw of the particular implementation we have presented here.
It is possible to integrate information in the context vectors that reflect syntactic or subcategorization behavior of different senses, such as the output of a shallow parser as used in Pereira, Tishby, and Lee (1993).
For example, one good indicator of the two senses of the word interest is a preposition occurring to its right.
The phrase interest in invokes the 'feeling of attention' sense, the phrase interest on, the sense 'charge on borrowed money'.
It seems plausible that performance could be improved for words whose senses are less sensitive to topical distinctions if such "proximity" information is integrated.
In some recent experiments, Pedersen and Bruce (1997) have used proximity features (tags of close words and the presence or absence of close functions words 116 Schfttze Automatic Word Sense Discrimination and content words) with some promising results.
This suggests that a combination of the topical features used here and proximity features may give optimal performance of context-group discrimination.
4 We
have used only one source of information (topical features) in the interest of simplicity, not because we see any inherent advantage of topical features compared to a combination of multiple sources of evidence.
Our justification for the basic idea of context-group discrimination, inducing senses from contextual similarity, has been that its results seem to align well with the ground truth of senses defined in dictionaries.
However, there is also some evidence that contextual similarity plays a crucial role in human semantic categorization.
In one study, Miller and Charles (1991) found evidence that human subjects determine the semantic similarity of words from the similarity of the contexts they are used in.
They summarized this result in the following hypothesis: Strong Contextual Hypothesis: Two words are semantically similar to the extent that their contextual representations are similar.
(p. 8) A contextual representation of a word is knowledge of how that word is used.
The hypothesis states that semantic similarity is determined by the degree of similarity of the sets of contexts that the two words can be used in.
The hypothesis that underlies context-group discrimination is an extension of the Strong Contextual Hypothesis to senses: Contextual Hypothesis for Senses: Two occurrences of an ambiguous word belong to the same sense to the extent that their contextual representations are similar.
So a sense is simply a group of occurrence tokens with similar contexts.
The analogy between the contextual hypotheses for words and senses is that both word types and word tokens are semantically similar to the extent that their contexts are semantically similar.
A group of contextually similar word tokens is a sense.
Miller and Charles's work thus provides a justification for our framework, the induction of senses from contextual similarity.
There are several issues that need to be addressed in future work on context-group discrimination.
First, our experiments only considered words with two major senses.
The algorithm also needs to be tested for words with more than two frequent senses and for infrequent senses.
Second, our test set consisted of a relatively small number of natural ambiguous words.
This is a flaw of almost all contemporary work on word sense disambiguation, but in the future more extensive test sets will be required to establish the general applicability of disambiguation algorithms.
Finally, the implementation of context-group discrimination proposed here is based on topical similarity only.
It will be necessary to incorporate other, more structural constraints (such as the interest in vs.
interest on case discussed above) to achieve adequate performance for a wide variety of ambiguous words.
Appendix A: Singular Value Decomposition A singular value decomposition factors an m-by-n matrix A into a product of three matrices: (,)A = U diag (o1,...,o.p)V T 4 See Leacock (1993) for a discussion of proximity and topical features in supervised disambiguation.
117 Computational Linguistics Volume 24, Number 1 Table 6 Co-occurrence counts for eight words in a five-dimensional word space.
judge suit robe gangster criminal police gun bail legal 300 210 133 30 200 160 120 150 clothes 75 182 200 10 5 10 20 15 cop 100 75 25 250 10 140 200 160 fashion 5 100 200 5 5 5 5 5 pants 5 110 190 5 5 5 5 5 Table 7 SVD reduction to two dimensions of the matrix in Table 6.
judge suit robe gangster criminal police gun bail dim1 -0.47 -0.46 -0.41 -0.22 -0.31 -0.30 -0.30 -0.30 dim2 0.13 -0.31 -0.69 0.41 0.05 0.25 0.33 0.28 Table 8 Correlation coefficients of three words before and after SVD dimensionality reduction.
criminal robe Word Space SVD Space Word Space SVD Space gangster 0.17 0.61 0.15 -0.52 criminal 0.41 0.37 where p = minm, n}, U (the left matrix) is an orthonormal m-by-p matrix, V (the right matrix) is an orthonormal n-by-p matrix and diag(o.1 ....., o.p) is a matrix with the diagonal elements o.1 _> o'2 > "" _> o.p ~_ 0 (and the value zero for nondiagonal elements) (Golub and van Loan 1989).
Dimensionality reduction can be based on SVD by keeping only the first k singular values o.1    c~k and setting the remaining ones to zero.
It can be shown that the product A' = U diag(o'l ..... o.k)V T is the closest approximation to A in a k-dimensional space (that is, there is no matrix of rank k with a smaller least-square distance to A than A').
See Golub and van Loan (1989) and Berry (1992) for a detailed description of SVD and efficient algorithms to compute it.
The benefits of dimensionality reduction for our purposes can best be explained using an example.
Table 6 shows co-occurrence counts from a hypothetical corpus (e.g., legal and robe co-occur 133 times with each other).
Note that two semantically similar words, gangster and criminal, have a low correlation in the words they co-occur with because they belong to different registers (this is one of reasons that topically similar words can have few neighbors in common).
Table 7 shows the two columns of the right matrix V of the SVD of the matrix in Table 6.
Table 7 is therefore a dimensionality reduction of Table 6 to two dimensions.
The advantage of the reduced space is that it directly represents the similar topicality of gangster and criminal: their vectors are close to each other in the space, as shown in Figure 5.
On the other hand, both words' vectors 118 Schiitze Automatic Word Sense Discrimination DIMENSION 2 GANGSTER CRIMINA L ROBE r DIMENSION 1 Figure 5 The vectors for robe, gangster, and criminal in the reduced SVD space.
The words gangster and criminal are represented as semantically similar.
Both are represented as semantically dissimilar from robe.
are less correlated with a topically dissimilar word like robe in the reduced space.
The correlation coefficients of the three words are shown in Table 8 for the unreduced and the reduced space.
The correlation of the topically related words (gangster and criminal) increases from 0.17 to 0.61, whereas the correlation of both words with robe decreases.
This example demonstrates the effect of SVD dimensionality reduction: topically similar words are projected closer to each other in the reduced space; topically dissimilar words are projected to distant locations.
Part of the motivation for using SVD for word vectors is the success of latent semantic indexing (LSI) in information retrieval (Deerwester et al.1990). LSI projects topically similar documents to close locations in the reduced space, just as we project topically similar words to close locations.
Appendix B: The EM Algorithm The clustering algorithm used in this paper is the EM algorithm.
The observed data (context vectors in our case) are interpreted as being generated by hidden causes, the clusters.
The EM algorithm is an iterative procedure that, starting from an initial hypothesis of the cluster parameters, improves the estimates of the parameters in each iteration.
We follow here the discussion and notation in Dempster, Laird, and Rubin (1977) and Ghahramani (1994).
We make the assumption that each cluster j is a Gaussian source with density ~j: ~j(~) exp\[ where \]/j is the mean and Gj the covariance matrix of a;j.
We write Oj = (fij, Gj) for the parameters of cluster j.
119 Computational Linguistics Volume 24, Number 1 Assume that we have N d-dimensional context vectors,g = {Xl ...
XN} C T4 d generated by M Gaussians COl...
CVM. The EM algorithm iteratively applies the Expectation step (E step) and the Maximization step (M step).
The E step is the estimation of parameters hq where hq is the probability of event zij, the event that cluster j generated Xi (context vector i).
hij = E(zij I ~i; O k) = O k is 0 at iteration k.
P(~ I ~J ;0~) .p(~l~j;o k) -~j(~;)P(~j) G~ P(~ I ~,; o~) The M step computes the most likely parameters of the distribution given the cluster membership probabilities: ~\]i=1 /j k+l E/N1 hq(2:i lif)(Y:i 11~) T ~j = ~N=lhq These are the well-known maximum-likelihood estimates for mean and variance of a Gaussian.
Recomputed means and variances are the parameters for the next iteration k+l.
For reasons of computational efficiency, we chose the implementation of the EM clustering known as k-means or hard clustering (Duda and Hart 1973).
In each iteration, context vectors are first assigned to the cluster with the closest mean; then cluster means are recomputed as the centroid of all members of the cluster.
This amounts to assuming a very small fixed variance for all clusters and only re-estimating the means in each step.
The initial cluster parameters are computed by applying group-average agglomerative clustering to a sample of size v'N.
Appendix C: Agglomerative Clustering Agglomerative clustering is a clustering technique that starts by assigning each element to a different cluster and then iteratively merges clusters according to a goodness criterion until the desired number of clusters has been reached.
Two such goodness measures give rise to single-link clustering and complete-link clustering.
Single-link clustering in each step merges the two clusters that have two elements with the smallest distance of any two clusters.
Complete-link clustering in each step executes the merger whose resulting cluster has the smallest diameter of all possible mergers.
Single-link clustering has been found in practice to produce elongated clusters (e.g., two parallel lines) that do not correspond well to the intuitive notion of a cluster as a mass of points with a center.
Complete-link clustering is strongly affected by outliers and has a time complexity cubic in the number of points to be merged and, hence, is less efficient than single-link clustering (which can be computed in quadratic time).
In this paper, we chose group-average agglomerative clustering (GAAC) as our clustering algorithm, a hybrid of single-link and complete-link clustering.
GAAC in each iteration executes the merger that gives rise to the cluster F with the largest average correlation C(P): 1 1 C(P) 2 IPl(\[rl1) ~ ~ corr(~,7~) ~cF/~cP 120 References 1 Berry, Michael W.
1992. Large-scale sparse singular value computations.
The International Journal of Supercomputer Applications, 6(1):13--49.
2 Peter
F.
Brown, Stephen A.
Della Pietra, Vincent J.
Della Pietra, Robert L.
Mercer, Word-sense disambiguation using statistical methods, Proceedings of the 29th annual meeting on Association for Computational Linguistics, p.264-270, June 18-21, 1991, Berkeley, California 3 Peter F.
Brown, Peter V.
deSouza, Robert L.
Mercer, Vincent J.
Della Pietra, Jenifer C.
Lai, Class-based n-gram models of natural language, Computational Linguistics, v.18 n.4, p.467-479, December 1992 4 Rebecca Bruce, Janyce Wiebe, Word-sense disambiguation using decomposable models, Proceedings of the 32nd annual meeting on Association for Computational Linguistics, p.139-146, June 27-30, 1994, Las Cruces, New Mexico 5 Burgess, Curt and Kevin Lund.
1997. Modelling parsing constraints with high-dimensional context space.
Language and Cognitive Processes, 12.
To appear.
6 Church, Kenneth W.
and William A.
Gale. 1991.
Concordances for parallel text.
In Proceedings of the Seventh Annual Conference of the UW Centre for the New OED and Text Research, pages 40--62, Oxford, England.
7 Church, Kenneth and William Gale.
1995. Poisson mixtures.
Journal of Natural Language Engineering, 1(2):163--190.
8 Garrison
W.
Cottrell, A connectionist approach to word sense disambiguation, Morgan Kaufmann Publishers Inc., San Francisco, CA, 1989 9 Douglass R.
Cutting, David R.
Karger, Jan O.
Pedersen, Constant interaction-time scatter/gather browsing of very large document collections, Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, p.126-134, June 27-July 01, 1993, Pittsburgh, Pennsylvania, United States 10 Cutting, Douglass R., Jan O.
Pedersen, and Per-Kristian Halvorsen.
1991. An object-oriented architecture for text retrieval.
In Proceedings of RIAO'91, pages 285--298, Barcelona, Spain.
11 Douglass
R.
Cutting, David R.
Karger, Jan O.
Pedersen, John W.
Tukey, Scatter/Gather: a cluster-based approach to browsing large document collections, Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval, p.318-329, June 21-24, 1992, Copenhagen, Denmark 12 Ido Dagan, Alon Itai, Ulrike Schwall, Two languages are more informative than one, Proceedings of the 29th annual meeting on Association for Computational Linguistics, p.130-137, June 18-21, 1991, Berkeley, California 13 Ido Dagan, Shaul Marcus, Shaul Markovitch, Contextual word similarity and estimation from sparse data, Proceedings of the 31st annual meeting on Association for Computational Linguistics, p.164-171, June 22-26, 1993, Columbus, Ohio 14 Ido Dagan, Fernando Pereira, Lillian Lee, Similarity-based estimation of word cooccurrence probabilities, Proceedings of the 32nd annual meeting on Association for Computational Linguistics, p.272-278, June 27-30, 1994, Las Cruces, New Mexico 15 Deerwester, Scott, Susan T.
Dumais, George W.
Furnas, Thomas K.
Landauer, and Richard Harshman.
1990. Indexing by latent semantic analysis.
Journal of the American Society for Information Science, 41(6):391--407.
16 Dempster, A.
P., N.
M. Laird, and D.
B. Rubin.
1977. Maximum likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society, Series B, 39:1--38.
17 Richard
O.
Duda, Peter E.
Hart, David G.
Stork, Pattern Classification (2nd Edition), Wiley-Interscience, 2000 18 Finch, Steven Paul.
1993. Finding Structure in Language.
Ph.D. thesis, University of Edinburgh.
19 Gale, William A., Kenneth W.
Church, and David Yarowsky.
1992. Work on statistical methods for word sense disambiguation.
In Robert Goldman, Peter Norvig, Eugene Charniak, and Bill Gale, editors, Working Notes of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language, pages 54--60, AAAI Press, Menlo Park, CA.
20 Stephen I.
Gallant, A practical approach for representing context and for performing word sense disambiguation using neural networks, Neural Computation, v.3 n.3, p.293-309, Fall 1991 21 Ghahramani, Zoubin.
1994. Solving inverse problems using an EM approach to density estimation.
In Michael C.
Mozer, Paul Smolensky, David S.
Touretzky, and Andreas S.
Weigend, editors, Proceedings of the 1993 Connectionist Models Summer School, Erlbaum Associates, Hillsdale, NJ.
22 Golub, Gene H.
and Charles F.
van Loan.
1989. Matrix Computations.
The Johns Hopkins University Press, Baltimore and London.
23 Gregory Grefenstette, Use of syntactic context to produce term association lists for text retrieval, Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval, p.89-97, June 21-24, 1992, Copenhagen, Denmark 24 Grefenstette, Gregory.
1994a. Corpus-derived first, second and third-order word affinities.
In Proceedings of the Sixth Euralex International Congress, Amsterdam.
25 Gregory Grefenstette, Explorations in Automatic Thesaurus Discovery, Kluwer Academic Publishers, Norwell, MA, 1994 26 Gregory Grefenstetti, Evaluation techniques for automatic semantic extraction: comparing syntactic and window based approaches, Corpus processing for lexical acquisition, MIT Press, Cambridge, MA, 1996 27 Joe A.
Guthrie, Louise Guthrie, Yorick Wilks, Homa Aidinejad, Subject-dependent co-occurrence and word sense disambiguation, Proceedings of the 29th annual meeting on Association for Computational Linguistics, p.146-152, June 18-21, 1991, Berkeley, California 28 Harman, D.
K., editor.
1993. The First Text REtrieval Conference (TREC-1).
U.S. Department of Commerce, Washington, DC.
NIST Special Publication 500--207.
29 Hearst, Marti A.
1991. Noun homograph disambiguation using local context in large text corpora.
In Proceedings of the Seventh Annual Conference of the UW Centre for the New OED and Text Research: Using Corpora, pages 1--22, Oxford.
30 Marti A.
Hearst, Christian Plaunt, Subtopic structuring for full-length document access, Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, p.59-68, June 27-July 01, 1993, Pittsburgh, Pennsylvania, United States 31 Graeme Hirst, Semantic interpretation and the resolution of ambiguity, Cambridge University Press, New York, NY, 1987 32 Anil K.
Jain, Richard C.
Dubes, Algorithms for clustering data, Prentice-Hall, Inc., Upper Saddle River, NJ, 1988 33 Karov, Yael and Shimon Edelman.
1996. Learning similarity-based word sense disambiguation from sparse data.
In Proceedings of the Fourth Workshop on Very Large Corpora.
34 Kelly, Edward and Phillip Stone.
1975. Computer Recognition of English Word Senses.
North-Holland, Amsterdam.
35 Kilgarriff, Adam.
1993. Dictionary word sense distinctions: An enquiry into their nature.
Computers and the Humanities, 26:365--387.
36 Robert Krovetz, Homonymy and polysemy in information retrieval, Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics, p.72-79, July 07-12, 1997, Madrid, Spain 37 R.
Krovetz, W.
B. Croft, Word sense disambiguation using machine-readable dictionaries, Proceedings of the 12th annual international ACM SIGIR conference on Research and development in information retrieval, p.127-136, June 25-28, 1989, Cambridge, Massachusetts, United States 38 Robert Krovetz, W.
Bruce Croft, Lexical ambiguity and information retrieval, ACM Transactions on Information Systems (TOIS), v.10 n.2, p.115-141, April 1992 39 Leacock, Claudia, Geoffrey Towell, and Ellen Voorhees.
1993. Towards building contextual representations of word senses using statistical models.
In Branimir Boguraev and James Pustejovsky, editors, Acquisition of Lexical Knowledge From Text: Workshop Proceedings, pages 10--21, Ohio.
40 Claudia Leacock, Geoffrey Towell, Ellen Voorhees, Corpus-based statistical sense resolution, Proceedings of the workshop on Human Language Technology, March 21-24, 1993, Princeton, New Jersey 41 Lesk, M.
E. 1969.
Word-word association in document retrieval systems.
American Documentation, 20(1):27--38.
42 Michael Lesk, Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone, Proceedings of the 5th annual international conference on Systems documentation, p.24-26, June 1986, Toronto, Ontario, Canada 43 Miller, George A.
and Walter G.
Charles. 1991.
Contextual correlates of semantic similarity.
Language and Cognitive Processes, 6(1):1--28.
44 Yoshiki Niwa, Yoshihiko Nitta, Co-occurrence vectors from corpora vs.
distance vectors from dictionaries, Proceedings of the 15th conference on Computational linguistics, August 05-09, 1994, Kyoto, Japan 45 Ott, Lyman.
1992. An Introduction to Statistical Methods and Data Analysis.
Wadsworth, Belmont, CA.
46 Pedersen, Ted and Rebecca Bruce.
1997. Distinguishing word senses in untagged text.
In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 197--207, Providence, RI.
47 Fernando Pereira, Naftali Tishby, Lillian Lee, Distributional clustering of English words, Proceedings of the 31st annual meeting on Association for Computational Linguistics, p.183-190, June 22-26, 1993, Columbus, Ohio 48 Yonggang Qiu, Hans-Peter Frei, Concept based query expansion, Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, p.160-169, June 27-July 01, 1993, Pittsburgh, Pennsylvania, United States 49 Gerda Ruge, Experiment on linguistically-based term associations, Information Processing and Management: an International Journal, v.28 n.3, p.317-332, 1992 50 Salton, Gerard.
1971. Experiments in automatic thesaurus construction for information retrieval.
In Proceedings IFIP Congress, pages 43--49.
51 Salton, Gerard and Chris Buckley.
1990. Improving retrieval performance by relevance feedback.
Journal of the American Society for Information Science, 41(4):288--297.
52 Gerard Salton, Michael J.
McGill, Introduction to Modern Information Retrieval, McGraw-Hill, Inc., New York, NY, 1986 53 Mark Sanderson, Word sense disambiguation and information retrieval, Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, p.142-151, July 03-06, 1994, Dublin, Ireland 54 Schtze, Hinrich.
1992a. Context space.
In Robert Goldman, Peter Norvig, Eugene Charniak, and Bill Gale, editors, Working Notes of the AAAI Fall Symposium on Probabilistic Approaches to Natural Language, pages 113--120, AAAI Press, Menlo Park, CA.
55 H.
Schtze, Dimensions of meaning, Proceedings of the 1992 ACM/IEEE conference on Supercomputing, p.787-796, November 16-20, 1992, Minneapolis, Minnesota, United States 56 Schtze, Hinrich.
1997. Ambiguity Resolution in Language Learning.
CSLI Publications, Stanford, CA.
57 Schtze, Hinrich and Jan O.
Pedersen. 1995.
Information retrieal based on word senses.
In Proceedings for the Fourth Annual Symposium on Document Analysis and Information Retrieval, pages 161--175, Las Vegas, NV.
58 Hinrich Schtze, Jan O.
Pedersen, A cooccurrence-based thesaurus and two applications to information retrieval, Information Processing and Management: an International Journal, v.33 n.3, p.307-318, May 1997 59 Sparck-Jones, Karen.
1986. Synonymy and Semantic Classification.
Edinburgh University Press, Edinburgh.
(Publication of Ph.D. thesis, University of Cambridge, 1964).
60 Karen Sparck Jones, Notes and references on early automatic classification work, ACM SIGIR Forum, v.25 n.1, p.10-17, Spring 1991 61 C.
J. Van Rijsbergen, Information Retrieval, Butterworth-Heinemann, Newton, MA, 1979 62 Ellen M.
Voorhees, Using WordNet to disambiguate word senses for text retrieval, Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, p.171-180, June 27-July 01, 1993, Pittsburgh, Pennsylvania, United States 63 Walker, Donald E.
and Robert A.
Amsler. 1986.
The use of machine-readable dictionaries in sublanguage analysis.
In Ralph Grishman and Richard Kittredge, editors, Analyzing Language in Restricted Domains: Sublanguage Description and Processing.
L. Erlbaum Associates, Hillsdale, NJ, pages 69--84.
64 Wilks, Yorick A., Dan C.
Fass, Cheng Ming Guo, James E.
McDonald, Tony Plate, and Brian M.
Slator. 1990.
Providing machine tractable dictionary tools.
Journal of Computers and Translation, 2.
65 Peter Willett, Recent trends in hierarchic document clustering: a critical review, Information Processing and Management: an International Journal, v.24 n.5, p.577-597, 1988 66 Winer, B.
J. 1971.
Statistical Principles in Experimental Design.
Second edition.
McGraw-Hill, New York, NY.
67 David Yarowsky, Word-sense disambiguation using statistical models of Roget's categories trained on large corpora, Proceedings of the 14th conference on Computational linguistics, August 23-28, 1992, Nantes, France 68 David Yarowsky, Unsupervised word sense disambiguation rivaling supervised methods, Proceedings of the 33rd annual meeting on Association for Computational Linguistics, p.189-196, June 26-30, 1995, Cambridge, Massachusetts
NEW YORK UNIVERSITY DESCRIPTION OF THE PROTEUS SYSTEM AS USED FOR MUC4 Ralph Grishman, Catherine Macleod, and John Sterling, The PROTEUS Project Computer Science Departmen t New York University 715 Broadway, 7th Floor New York, NY 1000 3 { grishman,macleod,sterling)@cs.nyu.edu HISTORY The PROTEUS system which we have used for MUC-4 is largely unchanged from that used for MUC-3 . It has three main components : a syntactic analyzer, a semantic analyzer, and a template generator . The PROTEUS Syntactic Analyzer was developed starting in the fall of 1984 as a common base for all th e applications of the PROTEUS Project.
Many aspects of its design reflect its heritage in the Linguistic Strin g Parser, previously developed and still in use at New York University . The current system, including the Restriction Language compiler, the lexical analyzer, and the parser proper, comprise approximately 4500 lines of Commo n Lisp.
The Semantic Analyzer was initially developed in 1987 for the MUCK-I (RAINFORMs) application, extended for the MUCK-II (OPREPs) application, and has been incrementally revised since . It currently consist s of about 3000 lines of Common Lisp (excluding the domain-specific information) . The Template Generator was written from scratch for the MUC-3 task and then revised for the MUC-4 templates; it is about 1200 lines of Common Lisp..
STAGES OF PROCESSING The text goes through the five major stages of processing : lexical analysis, syntactic analysis, semantic analysis, reference resolution, and template generation (see Figure 1) . In addition, some restructuring of the logical form is performed both after semantic analysis and after reference resolution (only the restructuring after referenc e resolution is shown in Figure 1) . Processing is basically sequential: each sentence goes through lexical, syntactic, and semantic analysis and reference resolution ; the logical form for the entire message is then fed to template generation . However, semantic (selectional) checking is performed during syntactic analysis, employing essentiall y the same code later used for semantic analysis . Each of these stages is described in a section which follows . LEXICAL ANALYSI S Dictionary Format Our dictionaries contain only syntactic information : the parts of speech for each word, information about the complement structure of verbs, distributional information (e .g ., for adjectives and adverbs), etc . We follow closely the set of syntactic features established for the NYU Linguistic String Parser . This information is entered in LISP form using noun, verb, adjective, and adverb macros for the open-class words, and a word macro for other parts of speech : (ADVERB "ABRUPTLY" :ATTRIBUTES (DSA) ) (ADJECTIVE "ABRUPT" ) (NOUN :ROOT "ABSCESS" :ATTRIBUTES (NCOUNT) ) Knowledge Sources Dictionary Text Lexical Analysi s Grammar Syntactic Analysi s Semantic Models Semantic Analysis Concept Hier.
Reference Resolutio n Mapping Rules LF Transformatio n Template Generatio n Template s Figure 1 . Structure of the Proteus System as used for MUC-4 (VERB :ROOT "ABSCOND" :OBJLIST (NULLOBJ PN (PVAL (FROM WITH))) ) The noun and verb macros automatically generate the regular inflectional forms . Dictionary Files The primary source of our dictionary information about open-class words (nouns, verbs, adjectives, an d adverbs) is the machine-readable version of the Oxford Advanced Learner's Dictionary ("OALD") . We have writ ten programs which take the SGML (Standard Generalized Markup Language) version of the dictionary, extrac t information on inflections, parts of speech, and verb subcategorization (including information on adverbial particles and prepositions gleaned from the examples), and generate the LISP-ified form shown above . This is supplemented by a manually-coded dictionary (about 1500 lines, 900 entries) for closed-class words, words no t fiadequately defined in the OALD, and a few very common words . For MUC-4 we used several additional dictionaries . There was a dictionary (about 900 lines) for domain specific English words not defined in the OALD, or too richly defined there . In addition, we extracted from the tex t and templates lists of organizations, locations, and proper names, and prepared small dictionaries for each (abou t 2500 lines total) . Looku p The text reader splits the input text into tokens and then attempts to assign to each token (or sequence o f tokens, in the case of an idiom) a definition (part of speech and syntactic attributes) . The matching process proceeds in four steps: dictionary lookup, lexical pattern matching, spelling correction, and prefix stripping . Dictionary lookup immediately retrieves definitions assigned by any of the dictionaries (including inflected forms), while lexical pattern matching is used to identify a variety of specialized patterns, such as numbers, dates, times, and possessive forms . If neither dictionary lookup nor lexical pattern matching is successful, spelling correction and prefix strippin g are attempted.
For words of any length, we identify an input token as a misspelled form of a dictionary entry if on e of the two has a single instance of a letter while the other has a doubled instance of the letter (e .g ., "mispelled" and "misspelled") . For words of 8 or more letters, we use a more general spelling corrector which allows for any singl e insertion, deletion, or substitution.
[ The prefix stripper attempts to identify the token as a combination of a prefix and a word defined in the dictionary.
We currently use a list of 17 prefixes, including standard English ones like "un" and MUC-3/MUC-4 specials like "narco-" . If all of these procedures fail, the word is tagged as a proper noun (name), since we found that most of ou r remaining undefined words were names . For MUC-4, we have incorporated the stochastic part-of-speech tagger from BBN in order to assign probabilities to each part-of-speech assigned by the lexical analyzer.
The log probabilities are used as scores, and combined with other scores to determine the overall score of each parsing hypothesis . Filtering In order to avoid full processing of sentences which would make no contribution to the templates, we per form a keyword-based filtering at the sentence level : if a sentence contains no key terms, it is skipped . This filtering is done after lexical analysis because the lexical analysis has identified the root form of all inflected words ; these root forms provide links into the semantic hierarchy . The filtering can therefore be specified in terms of a small number of word classes, one of which must be present for the sentence to be worth processing . SYNTACTIC ANALYSI S Syntactic analysis involves two stages of processing : parsing and syntactic regularization . At the core of th e system is an active chart parser.
The grammar is an augmented context-free grammar, consisting of BNF rules plu s procedural restrictions which check grammatical constraints not easily captured in the BNF rules . Most restrictions are stated in PROTEUS Restriction Language (a variant of the language developed for the Linguistic String Parser ) and translated into LISP ; a few are coded directly in LISP [1] . For example, the count noun restriction (that singular countable nouns have a determiner) is stated as WCOUNT = IN LNR AFTER NVAR : IF BOTH CORE Xcore IS NCOUNT AND Xcore IS SINGULA R THEN IN LN, TPOS IS NOT EMPTY . Associated with each BNF rule is a regularization rule, which computes the regularized form of each node i n the parse tree from the regularized forms of its immediate constituents . These regularization rules are based on lambda-reduction, as in GPSG . The primary function of syntactic regularization is to reduce all clauses to a standard form consisting of aspect and tense markers, the operator (verb or adjective), and syntactically marked cases . ' The minimum word length requirement is needed to avoid false hits where proper names are incorrectly identified as misspellings of words defined in the dictionary.
For example, the definition of assertion, the basic S structure in our grammar, i s <assertion> . ._ <sa> <subject> <sa> <verb> <sa> <object> <sa > :(s !(<object> <subject> <verb> <sa*>)) . Here the portion after the single colon defines the regularized structure.
Coordinate conjunction is introduced by a metarule (as in GPSG), which is applied to the context-free components of the grammar prior to parsing . The regularization procedure expands any conjunction into a conjuntio n of clauses or of noun phrases . The output of the parser for the first sentence of TST2-0048, "SALVADORAN PRESIDENT-ELEC T ALFREDO CRISTIANI CONDEMNED THE TERRORIST KILLING OF ATTORNEY GENERAL ROBERTO GARCIA ALVARADO AND ACCUSED THE FARABUNDO MARTI NATIONAL LIBERATION FRON T (FMLN) OF THE CRIME.", i s (SENTENC E (CENTERS (CENTE R (ASSERTIO N (ASSERTIO N (SUBJEC T (NSTG (LNR (LN (NPOS (NPOSVAR (LCDN (ADJ "SALVADORAN")) (N "PRESIDENT" "-" "ELECT"))) ) (NVAR (NAMESTG (LNAMER (N "ALFREDO") (MORENAME (N "CRISTIANI"))))))) ) (VERB (LTVR (TV "CONDEMNED")) ) (OBJECT (NSTGO (NSTG (LNR (LN (TPOS (LTR (T "THE"))) (NPOS (NPOSVAR (N "TERRORIST"))) ) (NVAR (N "KILLING") ) (RN (RN-VA L (PN (P "OF" ) (NSTGO (NST G (LNR (LN (NPOS (NPOSVAR (N "ATTORNEY" "GENERAL"))) ) (NVA R (NAMEST G (LNAMER (N "ROBERTO" ) (MORENAME (N "GARCIA") (MORENAME (N "ALVARADO")))))))))))))))) ) (CONJ-WORD ("AND" "AND") ) (ASSERTION (SUBJEC T (NSTG (LN R (LN (NPOS (NPOSVAR (LCDN (ADJ "SALVADORAN")) (N "PRESIDENT" "-" "ELECT"))) ) (NVAR (NAMESTG (LNAMER (N "ALFRED O " ) (MORENAME (N "CRISTIANI"))))))) ) (VERB (LTVR (TV "ACCUSED")) ) (OBJEC T (NPN (NSTG O (NST G (LNR (LN (TPOS (LTR (T "THE"))) ) (NVAR (NAMESTG (LNAMER (N "FARABUNDO" "MARTI" "NATIONAL" "LIBERATION" "FRONT") ) fi(NAME-APPOS ("(" "(" ) (NSTG (LNR (NVAR (NAMESTG (LNAMER (N "FMLN")))))) (' ')".
")")))))) ) (PN (P "OF" ) (NSTGO (NSTG (LNR (LN (TPOS (LTR (T "THE")))) (NVAR (N "CRIME"))))))))))) ) (ENDMARK (" ".
" ")).
) and the corresponding regularized structure is (AND (S CONDEMN (VTENSE PAST ) (SUBJEC T (NP A-NAME SINGULAR (NAMES (ALFREDO CRISTIANI)) (SN NP1499 ) (N-POS (NP PRESIDENT-ELECT SINGULAR (SN NP1489) (A-POS SALVADORAN)))) ) (OBJEC T (NP KILLING SINGULAR (SN NP1532) (T-POS THE ) (N-POS (NP TERRORIST SINGULAR (SN NP1504)) ) (OF (NP A-NAME SINGULAR (NAMES (ROBERTO GARCIA ALVARADO)) (SN NP1531 ) (N-POS (NP 'ATTORNEY GENERAL' SINGULAR (SN NP1506))))))) ) (S ACCUSE (VTENSE PAST ) (SUBJECT (NP A-NAME SINGULAR (NAMES (ALFREDO CRISTIANI)) (SN NP1499 ) (N-POS (NP PRESIDENT-ELECT SINGULAR (SN NP1489) (A-POS SALVADORAN)))) ) (OBJEC T (NP FMLN SINGULAR (RN-APPOS (NP FMLN SINGULAR (SN NP1539))) (SN NP1544 ) (T-POS THE)) ) (OF (NP CRIME SINGULAR (SN NP1543) (T-POS THE)))) ) The system uses a chart parser operating top-down, left-to-right . As edges are completed (i.e., as nodes of the parse tree are built), restrictions associated with those productions are invoked to assign and test features of th e parse tree nodes . If a restriction fails, that edge is not added to the chart . When certain levels of the tree are complete (those producing noun phrase and clause structures), the regularization rules are invoked to compute a regularized structure for the partial parse, and selection is invoked to verify the semantic well-formedness of the structure (as noted earlier, selection uses the same "semantic analysis" code subsequently employed to translate the tre e into logical form).
One unusual feature of the parser is its weighting capability . Restrictions may assign scores to nodes ; th e parser will perform a best-first search for the parse tree with the highest score . This scoring is used to implement various preference mechanisms:  closest attachment of modifiers (we penalize each modifier by the number of words separating it from it s head)  preferred narrow conjoining for clauses (we penalize a conjoined clause structure by the number of words i t subsumes)  preference semantics (selection does not reject a structure, but imposes a heavy penalty if the structure doe s not match any lexico-semantic model, and a lesser penalty if the structure matches a model but with som e operands or modifiers left over) [2,3 ]  relaxation of certain syntactic constraints, such as the count noun constraint, adverb position constraints, and comma constraint s  disfavoring (penalizing) headless noun phrases and headless relatives (this is important for parsing efficiency) The grammar is based on Harris's Linguistic String Theory and adapted from the larger Linguistic Strin g Project (LSP) grammar developed by Naomi Sager at NYU [4] . The grammar is gradually being enlarged to cove r more of the LSP grammar.
The current grammar is 1600 lines of BNF and Restriction Language plus 300 lines o f Lisp; it includes 186 non-terminals, 464 productions, and 132 restrictions . Over the course of the MUCs we have added several mechanisms for recovering from sentences the gram mar cannot fully parse : allowing the grammar to skip a single word, or a series of words enclosed in parentheses or dashes, with a large score penalty if no parse is obtained for the entire sentence, taking the analysis which, starting at the first word, subsumes the most word s optionally, taking the remainder of the sentence and "covering" it with noun phrases and clauses, preferrin g the longest noun phrases or clauses which can be identifie d SEMANTIC ANALYSIS AND REFERENCE RESOLUTIO N The output of syntactic analysis goes through semantic analysis and reference resolution and is then added t o the accumulating logical form for the message . Following both semantic analysis and reference resolution certai n transformations are performed to simplify the logical form . All of this processing makes use of a concept hierarch y which captures the class/subclass/instance relations in the domain.
Semantic analysis uses a set of lexico-semantic models to map the regularized syntactic analysis into a semantic representation.
Each model specifies a class of verbs, adjectives, or nouns and a set of operands ; for eac h operand it indicates the possible syntactic case markers, the semantic class of the operand, whether or not th e operand is required, and the semantic case to be assigned to the operand in the output representation . For example, the model for "<explosive-object> damages <target>" i s (add-clause-model :id 'clause-damage3 :parent 'clause-an y :constraint 'damag e :operands (list (make-specifie r :marker 'subjec t :class 'explosive-objec t :case :instrument ) (make-specifie r :marker 'objec t :class 'target-entit y :case :patien t :essential-required 'required)) ) The models are arranged in a shallow hierarchy with inheritance, so that arguments and modifiers which are shared by a class of verbs need only be stated once.
The model above inherits only from the most general clause model, clause--any, which includes general clausal modifiers such as negation, time, tense, modality, etc . The evaluated MUC-4 system had 124 clause models, 21 nominalization models, and 39 other noun phrase models, a total of about 2500 lines . The class explosive--object in the clause model refers to the concept in the concept hierarchy, whose entries have the form : (defconcept explosive-object :typeof instrument-type ) (defconcept explosive :typeof explosive-objec t :muctype explosive ) (defconcept grenade :typeof explosive ) (defconcept explosive-charge :typeof explosiv e :alias (dynamite-charge) ) (defconcept bomb :typeof explosive-objec t :muctype bomb ) (defconcept (VEHICLE BOMB( :typeof explosive-objec t :muctype (VEHICLE BOMB( ) (defconcept car-bomb :typeof VEHICLE BOMB( ) (defconcept bus-bomb :typeof IVEHICLE BOMB( ) (defconcept dynamite :typeof explosive-objec t :alias tnt :muctype DYNAMITE ) There are currently a total of 2474 concepts in the hierarchy, of which 1734 are place names . The output of semantic analysis is a nested set of entity and event structures, with arguments labeled by key words primarily designating semantic roles . For the first sentence of TST2-0048, the output i s Reference Resolution Reference resolution is applied to the output of semantic analysis in order to replace anaphoric noun phrase s (representing either events or entities) by appropriate antecedents . Each potential anaphor is compared to prior entities or events, looking for a suitable antecedent such that the class of the anaphor (in the concept hierarchy) i s equal to or more general than that of the antecedent, the anaphor and antecedent match in number, the restrictiv e modifiers in the anaphor have corresponding arguments in the antecedent, and the non-restrictive modifiers (e .g ., apposition) of the anaphor are not inconsistent with those of the antecedent . Special tests are provided for names (people may be referred to a subset of the ir names) and for referring to groups by typical members ("terrorist force" Logical Form Transformation s The transformations which are applied after semantic analysis and after reference resolution simplify an d regularize the logical form in various ways . For example, if a verb governs an argument of a nominalization, th e argument is inserted into the event created from the nominalization : "x conducts the attack", "x claims responsibility for the attack", "x was accused of the attack" etc . are all mapped to "x attacks" (with appropriate settings of th e confidence slot) . For example, the rule to take "X was accused of Y" and make X the agent of Y i s (((event :predicate accusation-even t :agent ?agent1 :event (event :identifier ?id-1 . ?R2 ) . ?R1 ) (event :identifier ?id-1 . ?R4) ) -> ((modify 2 '( :agent ?agent-1 :confidence 'SUSPECTED OR ACCUSEDI) ) (delete 1)) ) Transformations are also used to expand conjoined structures . For example, there is a rule to expand "the towns o f x and y " into "the town of x and the town of y", and there is a rule to expand "event at location-1 and location-2 " into "event at location-1 and event at location-2" . There are currently 32 such rules.
These transformations are written as productions and applied using a simple data-driven production system interpreter which is part of the PROTEUS system . TEMPLATE GENERATO R Once all the sentences in an article have been processed through syntactic and semantic analysis, the resulting logical forms are sent to the template generator . The template generator operates in four stages . First, a frame structure resembling a simplified template (with incident-type, perpetrator, physical-target, human-target, date, location, instrument, physical-effect, and human-effect slots) is generated for each event . Date and locatio n expressions are reduced to a normalized form at this point . In particular, date expressions such as "tonight", "last month", " last April", "a year ago", etc.
are replaced by explicit dates or date ranges, based on the dateline of th e article.
Second, a series of heuristics attempt to merge these frames, mergin g  frames referring to a common target  frames arising from the same sentenc e  an effect frame following an attack frame (e.g ., "The FMLN attacked the town . Seven civilians died ".
) This merging is blocked if the dates or locations are different, the incident types are incompatible, or the perpetrators are incompatible . Third, a series of filters removes frames involving only military targets and those involvin g events more than two months old . Finally, MUC templates are generated from these frames . fiSPONSORSHIP The development of the entire PROTEUS system has been sponsored primarily by the Defense Advanced Research Projects Agency as part of the Strategic Computing Program, under Contract N00014-85-K-0163 an d Grant N00014-90-J-1851 from the Office of Naval Research . Additional support has been received from th e National Science Foundation under grant DCR-85-01843 for work on enhancing system robustness . REFERENCES [1] Grishman, R . PROTEUS Parser Reference Manual . PROTEUS Project Memorandum #4-C, Computer Science Department, New York University, May 1990.
[2] Grishman, R., and Sterling, J . Preference Semantics for Message Understanding.
Proc . DARPA Speech and Natural Language Workshop, Morgan Kaufman, 1990 (proceedings of the conference at Harwich Port, MA, Oct . 15-18, 1989) . [3] Grishman, R., and Sterling, J . Information Extraction and Semantic Constraints . Proc . 13th Int' I Conf Computational Linguistics (COLING 90), Helsinki, August 20-25, 1990 . [4] Sager, N . Natural Language Information Processing, Addison-Wesley, 1981 .
Shallow Semantic Parsing of Chinese Honglin Sun1 Center for Spoken Language Research University of Colorado at Boulder Daniel Jurafsky2 Center for Spoken Language Research University of Colorado at Boulder Abstract In this paper we address the question of assigning semantic roles to sentences in Chinese.
We show that good semantic parsing results for Chinese can be achieved with a small 1100-sentence training set.
In order to extract features from Chinese, we describe porting the Collins parser to Chinese, resulting in the best performance currently reported on Chinese syntactic parsing; we include our headrules in the appendix.
Finally, we compare English and Chinese semantic-parsing performance.
While slight differences in argument labeling make a perfect comparison impossible, our results nonetheless suggest significantly better performance for Chinese.
We show that much of this difference is due to grammatical differences between English and Chinese, such as the prevalence of passive in English, and the strict word order constraints on adjuncts in Chinese.
based on the SVM-based algorithm proposed for English by Pradhan et al (2003).
We first describe our creation of a small 1100-sentence Chinese corpus labeled according to principles from the English and (in-progress) Chinese PropBanks.
We then introduce the features used by our SVM classifier, and show their performance on semantic parsing for both seen and unseen verbs, given hand-corrected (Chinese TreeBank) syntactic parses.
We then describe our port of the Collins (1999) parser to Chinese.
Finally, we apply our SVM semantic parser to a matching English corpus, and discuss the differences between English and Chinese that lead to significantly better performance on Chinese.
2 Semantic
Annotation and the Corpus Work on semantic parsing in English has generally related on the PropBank, a portion of the Penn TreeBank in which the arguments of each verb are annotated with semantic roles.
Although a project to produce a Chinese PropBank is underway (Xue and Palmer 2003), this data is not expected to be available for another year.
For these experiments, we therefore hand-labeled a small corpus following the Penn Chinese Propbank labeling guidelines (Xue, 2002).
In this section, we first describe the semantic roles we used in the annotation and then introduce the data for our experiments.
2.1 Semantic
roles Semantic roles in the English (Kingsbury et al 2002) and Chinese (Xue 2002) PropBanks are grouped into two major types: (1) arguments, which represent central participants in an event.
A verb may require one, two or more arguments and they are represented with a contiguous sequence of numbers prefixed by arg, as arg0, arg1.
(2) adjuncts, which are optional for an event but supply more information about an event, such as time, location, 1 Introduction Thematic roles (AGENT, THEME, LOCATION, etc) provide a natural level of shallow semantic representation for a sentence.
A number of algorithms have been proposed for automatically assigning such shallow semantic structure to English sentences.
But little is understood about how these algorithms may perform in other languages, and in general the role of language-specific idiosyncracies in the extraction of semantic content and how to train these algorithms when large hand-labeled training sets are not available.
In this paper we address the question of assigning semantic roles to sentences in Chinese.
Our work is Currently at Department of Computer Science, Queens College, City University of New York.
Email: sunh@qc.edu.
2 Currently
at Department of Linguistics, Stanford University.
Email: jurafsky@stanford.edu.
fireason, condition, etc.
An adjunct role is represented with argM plus a tag.
For example, argM-TMP stands for temporal, argM-LOC for location.
In our corpus three argument roles and 15 adjunct roles appear.
The whole set of roles is given at Table 1.
Role arg0 arg1 arg2 argM-ADV argM-BFY argM-CMP argM-CND argM-CPN argM-DGR argM-FRQ argM-LOC argM-MNR argM-PRP argM-RNG argM-RST argM-SRC argM-TMP argM-TPC Table1 The list of semantic roles Freq Freq Note 556 872 23 train of senses, argument numbers and frequencies are given in Table 2.
List of verbs for experiments # of Arg Freq senses number /set up 1 2 106 /emerge 1 1 80 /publish 1 2 113 /give 2 3/2 41 /build into 2 2/3 113 /enter 1 2 123 /take place 1 2 230 /pass 3 2 75 /hope 1 2 90 /increase 1 2 167 Table 2 Verb Test adverbial beneficiary(e.g.
give support [to the plan]) object to be compared condition companion (e.g.
talk [with you]) degree frequency location manner purpose or reason range(e.g.
help you [in this aspect]) result(e.g.
increase [to $100]) source(e.g.
increase [from $50] to $100) temporal topic 3 Semantic Parsing 3.1 Architecture and Classifier Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem.
For each (non-aux/non-copula) verb in each sentence, our classifier examines each node in the syntactic parse tree for the sentence and assigns it a semantic role label.
Most constituents are not arguments of the verb, and so the most common label is NULL.
Our architecture is based on a Support Vector Machine classifier, following Pradhan et al.(2003). Since SVMs are binary classifiers, we represent this 1-of-19 classification problem (18 roles plus NULL) by training 19 binary one-versus-all classifiers.
Following Pradhan et al.(2003), we used tinySVM along with YamCha (Kudo and Matsumoto 2000, 2001) as the SVM training and test software.
The system uses a polynominal kernel with degree 2; the cost per unit violation of the margin, C=1; tolerance of the termination criterion e=0.001.
3.2 Features
The literature on semantic parsing in English relies on a number of features extracted from the input sentence and its parse.
These include the constituent's syntactic phrase type, head word, and governing category, the syntactic path in the parse tree connecting it to the verb, whether the constitutent is before or after the verb, the subcategorization bias of the verb, and the voice (active/passive) of the verb.
We investigated each of these features in Chinese; some acted quite similarly to English, while others showed interesting differences.
Features that acted similarly to English include the target verb, the phrase type, the syntactic category of the constituent.
(NP, PP, etc), and the subcategorization of the target verb.
The sub-categorization feature represents the phrase structure rule for the verb phrase 2.2 The training and test sets We created our training and test corpora by choosing 10 Chinese verbs, and then selecting all sentences containing these 10 verbs from the 250K-word Penn Chinese Treebank 2.0.
We chose the 10 verbs by considering frequency, syntactic diversity, and word sense.
We chose words that were frequent enough to provide sufficient training data.
The frequencies of the 10 verbs range from 41 to 230, with an average of 114.
We chose verbs that were representative of the variety of verbal syntactic behavior in Chinese, including verbs with one, two, and three arguments, and verbs with various patterns of argument linking.
Finally, we chose verbs that varied in their number of word senses.
In total, we selected 1138 sentences.
The first author then labeled each verbal argument/adjunct in each sentence with a role label.
We created our training and test sets by splitting the data for each verb into two parts: 90% for training and 10% for test.
Thus there are 1025 sentences in the training set and 113 sentences in the test set, and each test set verb has been seen in the training set.
The list of verbs chosen and their number ficontaining the target verb (e.g., VP -> VB NP, etc).
Five features (path, position, governing category, headword, and voice) showed interesting patterns that are discussed below.
3.2.1 Path
in the syntactic parse tree.
The path feature represents the path from a constituent to the target verb in the syntactic parse tree, using "^" for ascending a parse tree, and ""! for descending.
This feature manifests the syntactic relationship between the constituent and the target verb.
For example the path "NP^IP!VP!VP!VV" indicates that the constituent is an "NP" which is the subject of the predicate verb.
In general, we found the path feature to be sparse.
In our test set, 60% of path types and 39% of path tokens are unseen in the training.
The distributions of paths are very uneven.
In the whole corpus, paths for roles have an average frequency of 14.5 while paths for non-roles have an average of 2.7.
Within the role paths, a small number of paths account for majority of the total occurrences; among the 188 role path types, the top 20 paths account for 86% of the tokens.
Thus, although the path feature is sparse, its sparsity may not be a major problem in role recognition.
Of the 291 role tokens in our test set, only 9 have unseen paths, i.e., most of the unseen paths are due to non-roles.
Table 3 The positional distribution of roles Role arg0 arg1 arg2 argM-ADV argM-BFY argM-CMP argM-CND argM-CPN argM-DGR argM-FRQ argM-LOC argM-MNR argM-PRP argM-RNG argM-RST argM-SRC argM-TMP argM-TPC Total Before verb 547 319 223 28 38 15 10 233 11 11 9 12 408 14 1878 After verb 72 644 28 Total 619 963 28 223 28 38 15 10 57 3 238 11 11 9 16 12 421 14 2716 example, 88% of arg0s are before the verb, 67% of arg1s are after the verb and all the arg2s are after the verb.
Adjuncts have even a stronger bias.
Ten of the adjunct types can only occur before the verb, while three are always after the verb.
The two most common adjunct roles, argM-LOC and argM-TMP are almost always before the verb, a sharp difference from English.
The details are shown seen in Table 3.
3.2.3 Governing
Category.
The governing category feature is only applicable for NPs.
In the original formulation for English in Gildea and Jurafsky (2002), it answers the question: Is the NP governed by IP or VP?
An NP governed by an IP is likely to be a subject, while an NP governed by a VP is more likely to be an object.
For Chinese, we added a third option in which the governing category of an NP is neither IP nor VP, but an NP.
This is caused by the "DE" construction, in which a clause is used as a modifier of an NP.
For instance, in the example indicated in Figure 1, for the last NP, " "("international Olympic conference") the parent node is NP, from where it goes down to the target verb " "("taking place").
NP CP VP      in Paris take place  DEC DE             NP intl Olympic conf.
"the international Olympic Conference held in Paris" Figure 1 Example of DE construction Since the governing category information is encoded in the path feature, it may be redundant; indeed this redundancy might explain why the governing category feature was used in Gildea & Jurafsky(2002) but not in Gildea and Palmer(2002).
Since the "DE" construction caused us to modify the feature for Chinese, we conducted several experiments to test whether the governing category feature is useful or whether it is redundant with the path and position features.
Using the paradigm to be described in section 3.4, we found a small improvement using governing category, and so we include it in our model.
3.2.4 Head
word and its part of speech.
The head word is a useful but sparse feature.
In our corpus, of the 2716 roles, 1016 head words (type) are used, in which 646 are used only once.
The top 20 words are given in Table 4.
3.2.2 Position
before or after the verb.
The position feature indicates that a constituent is before or after the target verb.
In our corpus, 69% of the roles are before the verb while 31% are after the verb.
As in English, the position is a useful cue for role identity.
For 3.3 Experimental Results for Seen Verbs We now test the performance of our classifier, trained on the 1025-sentence training set and tested on the 113sentence test set introduced in Section 2.2.
Recall that in this `stratified' test set, each verb has been seen in the training data.
The last row in Table 5 shows the current best performance of our system on this test set.
The preceding rows show various subsets of the feature set, beginning with the path feature.
Table 5 Semantic parsing results on seen verbs feature set P R F (%) (%) (%) path 71.8 59.4 65.0 path + pt 72.9 62.9 67.5 path + position 72.5 60.8 66.2 path + head POS 77.6 63.3 69.7 path + sub-cat 80.8 63.6 71.2 path + head word 85.0 66.0 74.3 path + target verb 85.8 68.4 76.1 path + pt + gov + position + subcat + target + head word + head POS 91.7 76.0 83.1 As Table 5 shows, the most important feature is path, followed by target verb and head word.
In general, the lexicalized features are more important than the other features.
The combined feature set outperforms any other feature sets with less features and it has an Fscore of 83.1.
The performance is better for the arguments (i.e., only ARG0-2), 86.7 for arg0 and 89.4 for arg1.
3.4 Experimental
Results for Unseen Verbs To test the performance of the semantic parser on unseen verbs, we used cross-validation, selecting one verb as test and the other 9 as training, and iterating with each verb as test.
All the results are given in Table 6.
The results for some verbs are almost equal to the performance on seen verbs.
For example for "    " and "    ", the F-scores are over 80.
However, for some verbs, the results are much worse.
The worst case is the verb " ", which has an F-score of 11.
This is due to the special syntactic characteristics of this verb.
This verb can only have one argument and this argument most often follows the verb, in object position.
In the surface structure, there is often an NP before the verb working as its subject, but semantically this subject cannot be analyzed as arg0.
For example: (1) /China  /not  /will  /emerge  /food  /crisis.
(A food crisis won't emerge in China).
(2) /Finland /economy  /emerge  /AUX  /post-war  /most     /serious  /AUX     /depression.
(The most severe post-war depression emerged in the Finland economy.) In the top 20 words, 4 are prepositions (" /in    /at /than /for") and 3 are temporal nouns("  /today      /present      /recently") and 2 are adverbs("  /already,  /will").
These closed class words are highly correlated with specific semantic roles.
For example," /for" occurs 195 times as the head of a constituent, of which 172 are non-roles, 19 are argM-BFYs, 3 are arg1s and 1 is an argM-TPC." /in" occurs 644 times as a head, of which 430 are nonroles, 174 are argM-LOCs, 24 are argM-TMPs, 9 are argM-RNGs, and 7 are argM-CND.
"  /already" occurs 135 times as a head, of which 97 are non-roles and 38 are argM-ADVs.
" /today" occurs 69 times as a head, of which 41 are argM-TMPs and 28 are nonroles.
Within the open class words, some are closely correlated to the target verb.
For example, "     /meeting; conference" occurs 43 times as a head for roles, of which 24 are for the target " /take place" and 19 for "    /pass".
"    /ceremony" occurs 28 times and all are arguments of " "(take place)."  /statement" occurs 19 times, 18 for "    /release; publish" and one for " /hope".
These statistics emphasize the key role of the lexicalized head word feature in capturing the collocation between verbs and their arguments.
Due to the sparsity of the head word feature, we also use the part-of-speech of the head word, following Surdeanu et al (2003).
For example, "7  26  /July 26" may not be seen in the training, but its POS, NT(temporal noun), is a good indicator that it is a temporal.
3.2.5 Voice.
The passive construction in English gives information about surface location of arguments.
In Chinese the marked passive voice is indicated by the use of the preposition " /by" (POS tag LB in Penn Chinese Treebank).
This passive, however, is seldom used in Chinese text.
In our entire 1138-sentence corpus, only 13 occurrences of "LB" occur, and only one (in the training set) is related to the target verb.
Thus we do not use the voice feature in our system.
The subjects, " /China" in (1) and " /Finland /economy", are locatives, i.e. argM-LOC, and the objects, " /food  /crisis" in (1) and " /postwar  /most  /serious  /AUX  /depression" in (2), are analyzed as arg0.
But the parser classified the subjects as arg0 and the objects as arg1.
These are correct for most common verbs but wrong for this particular verb.
It is difficult to know how common this problem would be in a larger, test set.
The fact that we considered diversity of syntactic behavior when selecting verbs certainly helps make this test set reflect the difficult cases.
If most verbs prove not to be as idiosyncratic as " /emerge", the real performance of the parser on unseen verbs may be better than the average given here.
Table 6 Experimental Results for Unseen Verbs target P(%) R(%) F(%)  /publish 90.7 72.9 80.8  /increase 49.6 34.3 40.5   /take place 90.1 63.3 74.4  /build into 65.2 55.5 60.0  /give 65.7 37.9 48.1  /pass 85.9 77.0 81.2  /emerge 12.6 10.2 11.3  /enter 81.9 58.8 68.4  /set up 79.0 61.1 68.9  /hope 77.7 35.9 49.1 Average 69.8 50.7 58.3 Another important difficulty in processing unseen verbs is the fact that roles in PropBank are defined in a verb-dependent way.
This may be easiest to see with an English example.
The roles arg2, arg3, arg4 have different meaning for different verbs; underlined in the following are some examples of arg2: (a) The state gave CenTrust 30 days to sell the Rubens.
(b) Revenue increased 11 to 2.73 billion from 2.46 billion.
(c) One of Ronald Reagan 's attributes as President was that he rarely gave his blessing to the claptrap that passes for consensus in various international institutions.
In (a), arg2 represents the goal of "give", in (b), it represents the amount of increase, and in (c) it represents yet another role.
These complete different semantic relations are given the same semantic label.
For unseen verbs, this makes it difficult for the semantic parser to know what would count as an arg2.
parser, the Collins (1999) parser, ported to Chinese.
We first describe how we ported the Collins parser to Chinese and then present the results of the semantic parser with features drawn from the automatic parses.
4.1 The
Collins parser for Chinese The Collins parser is a state-of-the-art statistical parser that has high performance on English (Collins, 1999) and Czech(Collins et al.1999). There have been attempts in applying other algorithms in Chinese parsing (Bikel and Chiang, 2000; Chiang and Bikel 2002; Levy and Manning 2003), but there has been no report on applying the Collins parser on Chinese.
The Collins parser is a lexicalized statistical parser based on a head-driven extended PCFG model; thus the choice of head node is crucial to the success of the parser.
We analyzed the Penn Chinese Treebank data and worked out head rules for the Chinese Treebank grammar (we were unable to find any published head rules for Chinese in the literature).
There are two major differences in the head rules between English and Chinese.
First, NP heads in Chinese are rigidly rightmost, that is to say, no modifiers of an NP can follow the head.
In contrast, in English a modifier may follow the head.
Second, just as with NPs in Chinese, the head of ADJP is rigidly rightmost.
In English, by contrast, the head of an ADJP is mainly the leftmost constituent.
Our head rules for the Chinese Treebank grammar are given in the Appendix.
In addition to the head rules, we modified the POS tags for all punctuation.
This is because all cases of punctuation in the Penn Chinese Treebank are assigned the same POS tag "PU".
The Collins parser, on the other hand, expects the punctuation tags in the English TreeBank format, where the tag for a punctuation mark is the punctuation mark itself.
We therefore replaced the POS tags for all punctuation marks in the Chinese data to conform to the conventions in English.
Finally, we made one further augmentation also related to punctuation.
Chinese has one punctuation mark that does not exist in English.
This commonly used mark, `semi-stop', is used in Chinese to link coordinates within a sentence (for example between elements of a list).
This function is represented in English by a comma.
But the comma in English is ambiguous; in addition to its use in coordination and lists, it can also represent the end of a clause.
In Chinese, by contrast the semi-stop has only the conjunction/list function.
Chinese thus uses the regular comma only for representing clause boundaries.
We investigated two ways to model the use of the Chinese semi-stop: (1) just converting the semi-stop to the comma, thus conflating the two functions as in English; and (2) by giving the semi-stop the POS tag "CC", a conjunction.
We compared parsing results with these two methods; the latter (conjunction) method gained 0.5% net 4 Using Automatic Parses The results in the last section are based on the use of perfect (hand-corrected) parses drawn from the Penn Chinese Treebank.
In practical use, of course, automatic parses will not be as accurate.
In this section we describe experiments on semantic parsing when given automatic parses produced by an automatic fiimprovement in F-score over the former one.
We therefore include it in our Collins parser port.
We trained the Collins parser on the Penn Chinese Treebank(CTB) Release 2 with 250K words, first removing from the training set any sentences that occur in the test set for the semantic parsing experiments.
We then tested on the test set used in the semantic parsing which includes 113 sentences(TEST1).
The results of the syntactic parsing on the test set are shown in Table 7.
Table 7 Results for syntactic parsing, trained on CTB Release 2, tested on test set in semantic parsing LP(%) LR(%) F1(%) overall 81.6 82.1 81.0 len<=40 86.1 85.5 86.7 To compare the performance of the Collins parser on Chinese with those of other parsers, we conducted an experiment in which we used the same training and test data (Penn Chinese Treebank Release 1, with 100K words) as used in those reports.
In this experiment, we used articles 1-270 for training and 271-300 as test(TEST2).
Table 8 shows the results and the comparison with other parsers.
Table 8 only shows the performance on sentences 40 words.
Our performance on all the sentences TEST2 is P/R/F=82.2/83.3/82.7.
It may seem surprising that the overall F-score on TEST2 (82.7) is higher than the overall F-score on TEST1 (81.0) despite the fact that our TEST1 system had more than twice as much training as our TEST2 system.
The reason lies in the makeup of the two test sets; TEST1 consists of randomly selected long sentences; TEST2 consists of sequential text, including many short sentences.
The average sentence length in TEST1 is 35.2 words, vs.
22.1 in TEST2.
TEST1 has 32% long sentences (>40 words) while TEST2 has only 13%.
Comparison with other parsers: TEST2 40 words LP(%) LR(%) F1(%) Bikel & Chiang 2000 77.2 76.2 76.7 Chiang & Bikel 2002 81.1 78.8 79.9 Levy & Manning 2003 78.4 79.2 78.8 Collins parser 86.4 85.5 85.9 4.2 Semantic parsing using Collins parses In the test set of 113 sentences, there are 3 sentences in which target verbs are given the wrong POS tags, so they can not be used for semantic parsing.
For the remaining 100 sentences, we used the feature set containing eight features (path, pt, gov, position, subcat, target, head word and head POS), the same as Table 8 that used in the experiment on perfect parses.
The results are shown in Table 9.
Table 9 Result for semantic parsing using automatic syntactic parses P(%) R(%) F(%) 110 sentences 86.0 70.8 77.6 113 sentences 86.0 69.2 76.7 Compared to the F-score using hand-corrected syntactic parses from the TreeBank, using automatic parses decreases the F-score by 6.4. 5 Comparison with English English build emerge enter found give Freq 46 30 108 248 124 Chinese      English hold hope increase pass publish Freq 120 63 231 143 77 Chinese      Table 12 Role argM-ADV argM-LOC argM-MNR argM-TMP Before verb 22 25 22 119 After verb 43 82 75 164 The comparison between adjuncts in English and Chinese English Chinese Freq in PRF Before After Freq in test (%) verb verb test 5 0 00 223 0 37 11 80 36.4 50 233 5 31 14 0 00 11 0 1 37 66.7 27 38.5 408 13 44 F 70 88.5 0 78.4 After the verbs were chosen, we extracted every sentence containing these verbs from section 02 to section 21 of the Wall Street Journal data from the Penn English Propbank.
The number of sentences for each verb is given in Table 10.
5.2 Experimental
Results As in our Chinese experiments, we used our SVMbased classifier, using N one-versus-all classifiers.
Table 11 shows the performance on our English test set (with Chinese for comparison), beginning with the path feature, and incrementally adding features until in the last row we combine all 8 features together.
Experimental results of English Chinese English feature set R/F/P P/R/F path 71.8/59.4/65.0 78.2/48.3/59.7 path + pt 72.9/62.9/67.5 77.4/51.2/61.6 path + position 72.5/60.8/66.2 75.7/50.9/60.8 path + hd POS 77.6/63.3/69.7 79.1/49.7/61.0 path + sub-cat 80.8/63.6/71.2 79.9/45.3/57.8 path + hd word 85.0/66.0/74.3 84.0/47.7/60.8 path + target 85.8/68.4/76.1 85.7/49.1/62.5 COMBINED 91.7/76.0/83.1 84.1/62.2/71.5 It is immediately clear from Table 11 that using similar verbs, the same amount of data, the same classifier, the same number of roles, and the same features, the results from English are much worse than those for Chinese.
While some part of the difference is probably due to idiosyncracies of particular sentences in the English and Chinese data, other aspects of the difference might be accounted for systematically, as we discuss in the next section.
5.3 Discussion: English/Chinese differences We first investigated whether the differences between English and Chinese could be attributed to particular semantic roles.
We found that this was indeed the case.
The great bulk of the error rate difference between English and Chinese was caused by the 4 adjunct classes argM-ADV, argM-LOC, argM-MNR, and argM-TMP, which together account for 19.6% of the role tokens in our English corpus.
The average F-score in English for the four roles is 36.7, while in Chinese Table 11 the F-score for the four roles is 78.6.
Why should these roles be so much more difficult to identify in English than Chinese?
We believe the answer lies in the analysis of the position feature in section 3.2.2.
This is repeated, with error rate information in Table 12.
We see there that adjuncts in English have no strong preference for occurring before or after the verb.
Chinese adjuncts, by contrast, are well-known to have an extremely strong preference to be preverbal, as Table 12 shows.
The relatively fixed word order of adjuncts makes it much easier in Chinese to map these roles from surface syntactic constituents than in English.
If the average F-score of the four adjuncts in English is raised to the level of that in Chinese, the overall Fscore on English would be raised from 71.5 to 79.7, accounting for 8.2 of the 11.6 difference in F-scores between the two languages.
We next investigated the one feature from our original English-specific feature set that we had dropped in our Chinese system: passive.
Recall that we dropped this feature because marked passives are extremely rare in Chinese.
When we added this feature back into our English system, the performance rose from P/R/F=84.1/62.2/71.5 to 86.4/65.1/74.3.
As might be expected, this effect of voice is mainly reflected in an improvement on arg0 and arg1, as Table 13 shows below: Table 13.
Improvement in English semantic parsing with the addition of the voice feature -voice +voice P RF P R F arg0 88.9 75.3 81.5 94.4 80 86.6 arg1 86.5 82.8 84.6 88.5 86.2 87.3 A third source of English-Chinese differences is the distribution of roles; the Chinese data has proportionally more adjuncts (ARGMs), while the English data has proportionally more oblique arguments (ARG2, ARG3, ARG4).
Oblique arguments are more difficult to process than other arguments, as was discussed in section 3.4.
This difference is most likely to be caused by labeling factors rather than by true structural differences between English in Chinese.
In summary, the higher performance in our Chinese system is due to 3 factors: the importance of passive in English; the strict word-order constraints of Chinese adverbials, and minor labeling differences.
Conclusions We can draw a number of conclusions from our investigation of semantic parsing in Chinese.
First, reasonably good performance can be achieved with a very small (1100 sentences) training set.
Second, the features that we extracted for English semantic parsing worked well when applied to Chinese.
Many of these features required creating an automatic parse; in doing so we showed that the Collins (1999) parser when ported to Chinese achieved the best reported performance on Chinese syntactic parsing.
Finally, we showed that semantic parsing is significantly easier in Chinese than in English.
We show that this counterintuitive result seems to be due to the strict constraints on adjunct ordering in Chinese, making adjuncts easier to find and label.
Acknowledgements This work was partially supported by the National Science Foundation via a KDD Supplement to NSF CISE/IRI/Interactive Systems Award IIS-9978025.
Many thanks to Ying Chen for her help on the Collins parser port, and to Nianwen Xue and Sameer Pradhan for providing the data.
Thanks to Kadri Hacioglu, Wayne Ward, James Martin, Martha Palmer, and three anonymous reviewers for helpful advice.
Appendix: Head rules for Chinese Parent ADJP ADVP CLP CP DNP DP DVP IP LCP LST NP PP PRN QP UCP VCD VP VPT VRD VSB Direction Right Right Right Right Right Left Right Right Right Right Right Left Left Right Left Left Left Left Left Right Priority List ADJP JJ AD ADVP AD CS JJ NP PP P VA VV CLP M NN NP CP IP VP DEG DNP DEC QP M(r) DP DT OD DEV AD VP VP IP NP LCP LC CD NP QP NP NN IP NR NT P PP PU QP CLP CD IP NP VP VV VA VE VE VC VV VNV VPT VRD VSB VCD VP VA VV VVl VA VV VE References Baker, Collin F., Charles J.
Fillmore, and John B.
Lowe. 1998.
The Berkekey FrameNet Project.
In Proceeding of COLING/ACL.
Bikel, Daniel and David Chiang.
2000. Two Statistical Parsing models Applied to the Chinese Treebank.
In Proceedings of the Second Chinese Language Processing Workshop, pp.
1-6. Chiang, David and Daniel Bikel.
2002. Recovering Latent Information in Treebanks.
In Proceedings of COLING-2002, pp.183-189.
Collins, Michael.
1999. Head-driven Statistical Models for Natural Language Parsing.
Ph.D. dissertation, University of Pennsylvannia.
Collins, Michael, Jan Hajic, Lance Ramshaw and Christoph Tillmann.
1999. A th Statistical Parser for Czech.
In Proceedings of the 37 Meeting of the ACL, pp.
505-512. Gildea, Daniel and Daniel Jurafsky.
2002. Automatic Labeling of Semantic Roles.
Computational Linguistics, 28(3):245-288.
Gildea, Daniel and Martha Palmer.
2002. The Necessity of Parsing for Predicate Argument Recognition, In Proceedings of the 40th Meeting of the ACL, pp.
239-246. Kingsbury, Paul, Martha Palmer, and Mitch Marcus.
2002. Adding semantic annotation to the Penn Treebank.
In Proceedings of HLT-02.
Kudo, Taku and Yuji Matsumoto.
2000. Use of support vector learning for chunk Identification.
In Proceedings of the 4th Conference on CoNLL, pp.
142-144. Kudo, Taku and Yuji Matsumoto.
2001 Chunking with Support Vector Machines.
In Proceeding of the 2nd Meeting of the NAACL.
pp.192-199. Levy, Roger and Christopher Manning.
2003. Is it harder to parse Chinese, or the Chinese Treebank?
ACL 2003, pp.
439-446. Pradhan, Sameer, Kadri Hacioglu,.
Wayne Ward, James Martin, and Daniel Jurafsky.
2003. "Semantic Role Parsing: Adding Semantic Structure to Unstructured Text".
In the Proceedings of the International Conference on Data Mining (ICDM2003), Melbourne, FL, 2003 Surdeanu, Mihai, Sanda Harabagiu, John Williams and Paul Aarseth.
2003. Using Predicate-Argument Structures for Information Extraction, In Proceedings of ACL.
Xue, Nianwen.
2002.stGuidelines for the Penn Chinese Proposition Bank (1 Draft), UPenn.
Xue, Nianwen, Fu-Dong Chiou and Martha Palmer.
2002. Building a large-scale annotated Chinese corpus.
In Proceedings of COLING-2002.
Xue, Nianwen, Martha Palmer.
2003. Annotating the propositions in the Penn Chinese Treebank.
In Proceedings of the 2nd SIGHAN Workshop on Chinese Language Processing.
Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Philadelphia, July 2002, pp.
64-71. Generalized Encoding of Description Spaces and its Application to Typed Feature Structures Gerald Penn Department of Computer Science University of Toronto 10 King's College Rd.
Toronto M5S 3G4, Canada Abstract This paper presents a new formalization of a unificationor join-preserving encoding of partially ordered sets that more essentially captures what it means for an encoding to preserve joins, generalizing the standard definition in AI research.
It then shows that every statically typable ontology in the logic of typed feature structures can be encoded in a data structure of fixed size without the need for resizing or additional union-find operations.
This is important for any grammar implementation or development system based on typed feature structures, as it significantly reduces the overhead of memory management and reference-pointer-chasing during unification.
adj noun nom acc plus minus subst case bool head Figure 1: A sample type system with appropriateness conditions.
the types of values (value restrictions) those feature values must have.
In Figure 1,1 for example, all head-typed TFSs must have bool-typed values for the features MOD and PRD, and no values for any other feature.
Relative to data structures like arrays or logical terms, typed feature structures (TFSs) can be regarded as an expressive refinement in two different ways.
First, they are typed, and the type system allows for subtyping chains of unbounded depth.
Figure 1 has a chain of length from to noun.
Pointers to arrays and logical terms can only monotonically "refine" their (syntactic) type from unbound (for logical terms, variables) to bound.
Second, although all the TFSs of a given type have the same features because of appropriateness, a TFS may acquire more features when it promotes to a subtype.
If a head-typed TFS promotes to noun in the type system above, for example, it acquires one extra casevalued feature, CASE.
When a subtype has two or 1 In this paper, Carpenter's (1992) convention of using as the most general type, and depicting subtypes above their supertypes is used.
1 Motivation
The logic of typed feature structures (Carpenter, 1992) has been widely used as a means of formalizing and developing natural language grammars that support computationally efficient parsing, generation and SLD resolution, notably grammars within the Head-driven Phrase Structure Grammar (HPSG) framework, as evidenced by the recent successful development of the LinGO reference grammar for English (LinGO, 1999).
These grammars are formulated over a finite vocabulary of features and partially ordered types, in respect of constraints called appropriateness conditions.
Appropriateness specifies, for each type, all and only the features that take values in feature structures of that type, along with fiAn order-embedding preserves the behavior of the order relation (for TFS type systems, subtyping; more incomparable supertypes, a TFS can also multiply inherit features from other supertypes when it promotes.
The overwhelmingly most prevalent operation when working with TFS-based grammars is unification, which corresponds mathematically to finding a least upper bound or join.
The most common instance of unification is the special case in which a TFS is unified with the most general TFS that satisfies a description stated in the grammar.
This special case can be decomposed at compile-time into more atomic operations that (1) promote a type to a subtype, (2) bind a variable, or (3) traverse a feature path, according to the structure of the description.
TFSs actually possess most of the properties of fixed-arity terms when it comes to unification, due to appropriateness.
Nevertheless, unbounded subtyping chains and acquiring new features conspire to force most internal representations of TFSs to perform extra work when promoting a type to a subtype to earn the expressive power they confer.
Upon being repeatedly promoted to new subtypes, they must be repeatedly resized or repeatedly referenced with a pointer to newly allocated representations, both of which compromise locality of reference in memory and/or involve pointer-chasing.
These costs are significant.
Because appropriateness involves value restrictions, simply padding a representation with some extra space for future features at the outset must guarantee a proper means of filling that extra space with the right value when it is used.
Internal representations that lazily fill in structure must also be wary of the common practice in description languages of binding a variable to a feature value with a scope larger than a single TFS -for example, in sharing structure between a daughter category and a mother category in a phrase structure rule.
In this case, the representation of a feature's value must also be interpretable independent of its context, because two separate TFSs may refer to that variable.
These problems are artifacts of not using a representation which possesses what in knowledge representation is known as a join-preserving encoding of a grammar's TFSs -in other words, a representation with an operation that naturally behaves like TFS-unification.
The next section presents the standard definition of join-preserving encodings and provides a generalization that more essentially captures what it means for an encoding to preserve joins.
Section 3 formalizes some of the defining characteristics of TFSs as they are used in computational linguistics.
Section 4 shows that these characteristics quite fortuitously agree with what is required to guarantee the existence of a joinpreserving encoding of TFSs that needs no resizing or extra referencing during type promotion.
Section 5 then shows that a generalized encoding exists in which variable-binding scope can be larger than a single TFS -a property no classical encoding has.
Earlier work on graph unification has focussed on labelled graphs with no appropriateness, so the central concern was simply to minimize structure copying.
While this is clearly germane to TFSs, appropriateness creates a tradeoff among copying, the potential for more compact representations, and other memory management issues such as locality of reference that can only be optimized empirically and relative to a given grammar and corpus (a recent example of which can be found in Callmeier (2001)).
While the present work is a more theoretical consideration of how unification in one domain can simulate unification in another, the data structure described here is very much motivated by the encoding of TFSs as Prolog terms allocated on a contiguous WAM-style heap.
In that context, the emphasis on fixed arity is really an attempt to avoid copying, and lazily filling in structure is an attempt to make encodings compact, but only to the extent that join preservation is not disturbed.
While this compromise solution must eventually be tested on larger and more diverse grammars, it has been shown to reduce the total parsing time of a large corpus on the ALE HPSG benchmark grammar of English (Penn, 1993) by a factor of about 4 (Penn, 1999).
2 Join-Preserving Encodings We may begin with a familiar definition from discrete mathematics: and Definition 1 Given two partial orders, a function is an orderembedding iff, for every, iff. fif Figure 3: A non-classical join-preserving encoding between BCPOs for which no classical joinpreserving encoding exists.
Bounded completeness ensures that unification or joins are well-defined among consistent types.
Definition 3 Given two BCPOs, and, is a classical join-preserving encoding of into iff: injectivity is an injection, Join-preserving encodings are automatically orderembeddings because iff . There is actually a more general definition: Definition 4 Given two BCPOs, and, is a (generalized) join-preserving encoding of into iff: disjointness to mean We use the notation to mean is defined.
totality for all iff is undefined, and join homomorphism, where they exist.
zero preservation, and iff Definition 2 A partial order is bounded complete (BCPO) iff every set of elements with a common upper bound has a least upper bound.
When maps elements of to singleton sets in, then reduces to a classical join-preserving encoding.
It is not necessary, however, to require that only one element of represent an element of, provided that it does not matter which representative we choose at any given time.
Figure 3 shows a generalized join-preserving encoding between two partial orders for which no classical encoding exists.
There is no classical encoding of into because no three elements can be found in that pairwise unify to a common join.
A generalized encoding exists because we can choose three potential representatives for : one ( ) for unifying the representatives of and, one ( ) for unifying the representatives of and, and one ( ) for unifying the representatives of and . Notice that the set of representatives for must be closed under unification.
Although space does not permit here, this generalization has been used to prove that well-typing, an alternative interpretation of appropriateness, is equivalent in its expressive power to the interpretation used here (called total well-typing; Carpenter, 1992); that multi-dimensional inheritance (Erbach, 1994) adds no expressive power to any TFS type system; that TFS type systems can encode systemic networks in polynomial space using extensional types (Carpenter, 1992); and that certain uses of parametjoin homomorphism for all and,, where they exist.
zero preservation for all, iff, and for TFSs themselves, subsumption) in the encoding codomain.
As shown in Figure 2, however, order embeddings do not always preserve operations such as least upper bounds.
The reason is that the image of may not be closed under those operations in the codomain.
In fact, the codomain could provide joins where none were supposed to exist, or, as in Figure 2, no joins where one was supposed to exist.
Mellish (1991; 1992) was the first to formulate join-preserving encodings correctly, by explicitly refor the quiring this preservation.
Let us write join of and in partial order . Figure 2: An example order-embedding that cannot translate least upper bounds.
and fihead (Upward Closure / Right Monotonicity) if F and, then F and F F . The function Approp maps a feature and type to the value restriction on that feature when it is appropriate to that type.
If it is not appropriate, then Approp is undefined at that pair.
Feature introduction ensures that every feature has a least type to which it is appropriate.
This makes description compilation more efficient.
Upward closure ensures that subtypes inherit their supertypes' features, and with consistent value restrictions.
The combination of these two properties allows us to annotate a BCPO of types with features and value restrictions only where the feature is introduced or the value restriction is refined, as in Figure 1.
A very useful property for type systems to have is static typability.
This means that if two TFSs that are well-formed according to appropriateness are unifiable, then their unification is automatically well-formed as well -no additional work is necessary.
Theorem 1 (Carpenter, 1992) An appropriateness specification is statically typable iff, for all types such that, and all F : unrestricted if only if only otherwise if and if (Feature type Introduction) MOD PRD plus plus Figure 5: A TFS of type head from the type system in Figure 1.
Not all type systems are statically typable, but a type system can be transformed into an equivalent statically typable type system plus a set of universal constraints, the proof of which is omitted here.
In linguistic applications, we normally have a set of universal constraints anyway for encoding principles of grammar, so it is easy and computationally inexpensive to conduct this transformation.
4 Static
Encodability As mentioned in Section 1, what we want is an encoding of TFSs with a notion of unification that naturally corresponds to TFS-unification.
As discussed in Section 3, static typability is something we can reasonably guarantee in our type systems, and is therefore something we expect to be reflected in our encodings -no extra work should be done apart from combining the types and recursing on feature values.
If we can ensure this, then we have avoided the extra work that comes with resizing or unnecessary referencing and pointer-chasing.
As mentioned above, what would be best from the standpoint of memory management is simply a fixed array of memory cells, padded with extra space to accommodate features that might later be added.
We will call these frames.
Figure 4 depicts a frame for the head-typed TFS in Figure 5.
In a frame, the representation of the type can either be (1) a bit vector encoding the type,3 or (2) a reference pointer Instead of a bit vector, we could also use an index into a table if least upper bounds are computed by table look-up.
PQ Definition 5 A TFS type system consists of a finite BCPO of types,, a finite set of features Feat, and a partial function, such that, for every F : Figure 4: A fixed array representation of the TFS in Figure 5.
There are only a few common-sense restrictions we need to place on our type systems: 3 TFS Type Systems ric typing with TFSs also add no expressive power to the type system (Penn, 2000).
(head representation) (MOD representation) (PRD representation) fiF: G: H: Figure 6: A type system with three features and a three-colorable feature graph.
module of its type.
Even this number can normally be reduced: Definition 7 The feature graph,, of module is an undirected graph, whose vertices correspond to the features introduced in, and in which there is an edge,, iff and are appropriate to a common maximally specific type in . Proposition 1 The least number of feature slots required for a frame of any type in is the least for which is -colorable.
There are type systems, of course, for which modularization and graph-coloring will not help.
Figure 6, for example, has one module, three features, and a three-clique for a feature graph.
There are statistical refinements that one could additionally make, such as determining the empirical probability that a particular feature will be acquired and electing to pay the cost of resizing or referencing for improbable features in exchange for smaller frames.
4.2 Correctness
of Frames Restricting the Size of Frames At first blush, the prospect of adding as many extra slots to a frame as there could be extra features in a TFS sounds hopelessly unscalable to large grammars.
While recent experience with LinGO (1999) suggests a trend towards modest increases in numbers of features compared to massive increases in numbers of types as grammars grow large, this is nevertheless an important issue to address.
There are two discrete methods that can be used in combination to reduce the required number of extra slots:, the set of Definition 6 Given a finite BCPO, modules of is the finest partition of,, such that (1) each is upward-closed (with respect to subtyping), and (2) if two types have a least upper bound, then they belong to the same module.
Trivially, if a feature is introduced at a type in one module, then it is not appropriate to any type in any other module.
As a result, a frame for a TFS only needs to allow for the features appropriate to the Prolog terms require one additional unbound variable per TFS (sub)term in order to preserve the intensionality of the logic -unlike Prolog terms, structurally identical TFS substructures are not identical unless explicitly structure-shared.
With the exception of extra slots for unused feature values, frames are clearly isomorphic in their structure to the TFSs they represent.
The implementation of unification that we prefer to avoid resizing and referencing is to (1) find the least upper bound of the types of the frames being unified, (2) update one frame's type to the least upper bound, and point the other's type representation to it, and (3) recurse on respective pairs of feature values.
The frame does not need to be resized, only the types need to be referenced, and in the special case of promoting the type of a single TFS to a subtype, the type only needs to be trailed.
If cyclic TFSs are not supported, then acyclicity must also be enforced with an occurscheck.
The correctness of frames as a join-preserving encoding of TFSs thus depends on being able to make sense of the values in these unused positions.
The to another frame.
If backtracking is supported in search, changes to the type representation must be trailed.
For each appropriate feature, there is also a pointer to a frame for that feature's value.
There are also additional pointers for future features (for head, CASE) that are grounded to some distinguished value indicating that they are unused -usually a circular reference to the referring array position.
Cyclic TFSs, if they are supported, would be represented with cyclic (but not 1-cyclic) chains of pointers.
Frames can be implemented either directly as arrays, or as Prolog terms.
In Prolog, the type representation could either be a term-encoding of the type, which is guaranteed to exist for any finite BCPO (Mellish, 1991; Mellish, 1992), or in extended Prologs, another trailable representation such as a mutable term (Aggoun and Beldiceanu, 1990) or an attributed value (Holzbaur, 1992).
Padding the representation with extra space means using a Prolog term with extra arity.
A distinguished value for unused arguments must then be a unique unbound variable.4 Figure 7: A type system that introduces a feature at a join-reducible type.
head Figure 9: The frame for Figure 8.
MOD PRD PRD (head representation) Figure 11: The frame for Figure 10.
5 The
sole exception is a TFS of type, which by definition belongs to no module and has no features.
Its representation is a distinguished circular reference, unless two or more feature values share a single -typed TFS value, in which case one is a circular reference and the rest point to it.
The circular one can be chosen canonically to ensure that the encoding is still classical.
(MOD/PRD representation) problem is that features may be introduced at joinreducible types, as in Figure 7.
There is only one module, so the frames for a and b must have a slot available for the feature F.
When an a-typed TFS unifies with a b-typed TFS, the result will be of type c, so leaving the slot marked unused after recursion would be incorrect -we would need to look in a table to see what value to assign it.
An alternative would be to place that value in the frames for a and b from the beginning.
But since the value itself must be of type a in the case of Figure 7, this strategy would not yield a finite representation.
The answer to this conundrum is to use a distinguished circular reference in a slot iff the slot is either unused or the value it contains is (1) the most general satisfier of the value restriction of the feature it represents and (2) not structure-shared with any other feature in the TFS.5 During unification, if one TFS is a circular reference, and the other is not, the circular reference is referenced to the other.
If both values are circular references, then one is referenced to the other, which remains circular.
The feature structure in Figure 8, for example, has the frame representation shown in Figure 9.
The PRD value is a TFS of type bool, and this value is not shared with any other structure in the TFS.
If the values of MOD and PRD are both bool-typed, then if Figure 10: A TFS of type head in which both feature values are most general satisfiers of the value restrictions, but they are shared.
lar references (Figure 11), and if they are not shared (Figure 12), both of them use a different circular reference (Figure 13).
With this convention for circular references, frames are a classical join-preserving encoding of the TFSs of any statically typable type system.
Although space does not permit a complete proof here, the intuition is that (1) most general satisfiers of value restrictions necessarily subsume every other value that a totally well-typed TFS could take at that feature, and (2) when features are introduced, their initial values are not structure-shared with any other substructure.
Static typability ensures that value restrictions unify to yield value restrictions, except in the final case of Theorem 1.
The following lemma deals with this case: Lemma 1 If Approp is statically typable, and for some F, F F, then either F, and or MOD Figure 8: A TFS of type head in which one feature value is a most general satisfier of its feature's value restriction.
head bool PQ plus bool they are shared (Figure 10), they do not use circu(head representation) (MOD representation) fihead MOD PRD (head representation) Figure 15: A statically typable "type system" that multiply introduces F at join-reducible elements with different value restrictions.
duction, but in fact, the result holds if we allow for multiple introducing types, provided that all of them agree on what the value restriction for the feature should be.
Would-be type systems that multiply introduce a feature at join-reducible elements (thus requiring some kind of distinguished-value encoding), disagree on the value restriction, and still remain statically typable are rather difficult to come by, but they do exist, and for them, a frame encoding will not work.
Figure 15 shows one such example.
In this signature, the unification: s Figure 13: The frame for Figure 12.
Proof: does not exist, but the unification of their frame encodings must succeed because the -typed TFS's F value must be encoded as a circular reference.
To the best of the author's knowledge, there is no fixedsize encoding for Figure 15.
5 Generalized
Term Encoding In practice, this classical encoding is not good for much.
Description languages typically need to bind variables to various substructures of a TFS,, and then pass those variables outside the substructures of where they can be used to instantiate the value of another feature structure's feature, or as arguments to some function call or procedural goal.
If a value in a single frame is a circular reference, we can properly understand what that reference encodes with the above convention by looking at its context, i.e., the type.
Outside the scope of that frame, we have no way of knowing which feature's value restriction it is supposed to encode.
Figure 14: The second case in the proof of Lemma 1.
. F, so F and F So there are three cases to consider: Intro2 F : then the result trivially holds.
Intro F but Intro F4 (or by symmetry, the opposite): then we have the situation in Figure 14.
It must be that F4, so by static typability, the lemma holds.
Intro F and Intro F : and F, so and F4 are consistent.
By bounded completeness, F and F . By upward closure, F F4 and by static typability, F F4 F F.
Furthermore, F ; thus by static typability the lemma holds.
This lemma is very significant in its own right -it says that we know more than Carpenter's Theorem 1.
An introduced feature's value restriction can always be predicted in a statically typable type system.
The lemma implicitly relies on feature introF F Suppose Figure 12: A TFS of type head in which both feature values are most general satisfiers of the value restrictions, and they are not shared.
F: F: bool bool H' F: PQ FG!
FG! FG!
E Introduced feature has variable encoding Figure 16: A pictorial overview of the generalized encoding.
A generalized term encoding provides an elegant solution to this problem.
When a variable is bound to a substructure that is a circular reference, it can be filled in with a frame for the most general satisfier that it represents and then passed out of context.
Having more than one representative for the original TFS is consistent, because the set of representatives is closed under this filling operation.
A schematic overview of the generalized encoding is in Figure 16.
Every set of frames that encode a particular TFS has a least element, in which circular references are always opted for as introduced feature values.
This is the same element as the classical encoding.
It also has a greatest element, in which every unused slot still has a circular reference, but all unshared most general satisfiers are filled in with frames.
Whenever we bind a variable to a substructure of a TFS, filling pushes the TFS's encoding up within the same set to some other encoding.
As a result, at any given point in time during a computation, we do not exactly know which encoding we are using to represent a given TFS.
Furthermore, when two TFSs are unified successfully, we do not know exactly what the result will be, but we do know that it falls inside the correct set of representatives because there is at least one frame with circular references for the values of every newly introduced feature.
variable binding 6 Conclusion Simple frames with extra slots and a convention for filling in feature values provide a join-preserving encoding of any statically typable type system, with no resizing and no referencing beyond that of type representations.
A frame thus remains stationary in memory once it is allocated.
A generalized encoding, moreover, is robust to side-effects such as extra-logical variable-sharing.
Frames have many potential implementations, including Prolog terms, WAM-style heap frames, or fixed-sized records.
References A.
Aggoun and N.
Beldiceanu. 1990.
Time stamp techniques for the trailed data in constraint logic programming systems.
In S.
Bourgault and M.
Dincbas, editors, Programmation en Logique, Actes du 8eme Seminaire, pages 487509.
U. Callmeier.
2001. Efficient parsing with large-scale unification grammars.
Master's thesis, Universitaet des Saarlandes.
B. Carpenter.
1992. The Logic of Typed Feature Structures.
Cambridge. G.
Erbach. 1994.
Multi-dimensional inheritance.
In Proceedings of KONVENS 94.
Springer. C.
Holzbaur. 1992.
Metastructures vs.
attributed variables in the context of extensible unification.
In M.
Bruynooghe and M.
Wirsing, editors, Programming Language Implementation and Logic Programming, pages 260268.
Springer Verlag.
LinGO. 1999.
The LinGO grammar and lexicon.
Available on-line at http://lingo.stanford.edu.
C. Mellish.
1991. Graph-encodable description spaces.
Technical report, University of Edinburgh Department of Artificial Intelligence.
DYANA Deliverable R3.2B.
C. Mellish.
1992. Term-encodable description spaces.
In D.R.
Brough, editor, Logic Programming: New Frontiers, pages 189207.
Kluwer. G.
Penn. 1993.
The ALE HPSG benchmark grammar.
Available on-line at http://www.cs.toronto.edu/ gpenn/ale.html.
G. Penn.
1999. An optimized Prolog encoding of typed feature structures.
In Proceedings of the 16th International Conference on Logic Programming (ICLP-99), pages 124138.
G. Penn.
2000. The Algebraic Structure of Attributed Type Signatures.
Ph.D. thesis, Carnegie Mellon University .
A High-Performance Semi-Supervised Learning Method for Text Chunking Rie Kubota Ando Tong Zhang IBM T.J.
Watson Research Center Yorktown Heights, NY 10598, U.S.A.
rie1@us.ibm.com tongz@us.ibm.com Abstract In machine learning, whether one can build a more accurate classifier by using unlabeled data (semi-supervised learning) is an important issue.
Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear.
This paper presents a novel semi-supervised method that employs a learning paradigm which we call structural learning.
The idea is to find "what good classifiers are like" by learning from thousands of automatically generated auxiliary classification problems on unlabeled data.
By doing so, the common predictive structure shared by the multiple classification problems can be discovered, which can then be used to improve performance on the target problem.
The method produces performance higher than the previous best results on CoNLL'00 syntactic chunking and CoNLL'03 named entity chunking (English and German).
(Blum and Mitchell, 1998) automatically bootstraps labels, and such labels are not necessarily reliable (Pierce and Cardie, 2001).
A related idea is to use Expectation Maximization (EM) to impute labels.
Although useful under some circumstances, when a relatively large amount of labeled data is available, the procedure often degrades performance (e.g.
Merialdo (1994)).
A number of bootstrapping methods have been proposed for NLP tasks (e.g.
Yarowsky (1995), Collins and Singer (1999), Riloff and Jones (1999)).
But these typically assume a very small amount of labeled data and have not been shown to improve state-of-the-art performance when a large amount of labeled data is available.
Our goal has been to develop a general learning framework for reliably using unlabeled data to improve performance irrespective of the amount of labeled data available.
It is exactly this important and difficult problem that we tackle here.
This paper presents a novel semi-supervised method that employs a learning framework called structural learning (Ando and Zhang, 2004), which seeks to discover shared predictive structures (i.e.
what good classifiers for the task are like) through jointly learning multiple classification problems on unlabeled data.
That is, we systematically create thousands of problems (called auxiliary problems) relevant to the target task using unlabeled data, and train classifiers from the automatically generated `training data'.
We learn the commonality (or structure) of such many classifiers relevant to the task, and use it to improve performance on the target task.
One example of such auxiliary problems for chunking tasks is to `mask' a word and predict whether it is "people" or not from the context, like language modeling.
Another example is to predict the pre1 Introduction In supervised learning applications, one can often find a large amount of unlabeled data without difficulty, while labeled data are costly to obtain.
Therefore, a natural question is whether we can use unlabeled data to build a more accurate classifier, given the same amount of labeled data.
This problem is often referred to as semi-supervised learning.
Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear.
For example, co-training Proceedings of the 43rd Annual Meeting of the ACL, pages 19, Ann Arbor, June 2005.
c 2005 Association for Computational Linguistics fidiction of some classifier trained for the target task.
These auxiliary classifiers can be adequately learned since we have very large amounts of `training data' for them, which we automatically generate from a very large amount of unlabeled data.
The contributions of this paper are two-fold.
First, we present a novel robust semi-supervised method based on a new learning model and its application to chunking tasks.
Second, we report higher performance than the previous best results on syntactic chunking (the CoNLL'00 corpus) and named entity chunking (the CoNLL'03 English and German corpora).
In particular, our results are obtained by using unlabeled data as the only additional resource while many of the top systems rely on hand-crafted resources such as large name gazetteers or even rulebased post-processing.
model complexity.
ERM-based methods for discriminative learning are known to be effective for NLP tasks such as chunking (e.g.
Kudoh and Matsumoto (2001), Zhang and Johnson (2003)).
2.2 Linear
model for structural learning We present a linear prediction model for structural learning, which extends the traditional model to multiple problems.
Specifically, we assume that there exists a low-dimensional predictive structure shared by multiple prediction problems.
We seek to discover this structure through joint empirical risk minimization over the multiple problems.
Consider  problems indexed by   , each with  samples   indexed by   . In our joint linear model, a predictor for problem takes the following form 2 A Model for Learning Structures This work uses a linear formulation of structural learning.
We first briefly review a standard linear prediction model and then extend it for structural learning.
We sketch an optimization algorithm using SVD and compare it to related methods.
2.1 Standard
linear prediction model In the standard formulation of supervised learning, we seek a predictor that maps an input vector  to the corresponding output   . Linear prediction models are based on real-valued predictors of  the form  where is called a weight vector.
For binary problems, the sign of the linear prediction gives the class label.
For -way classification (with ), a typical method is winner takes all, where we train one predictor per class and choose the class with the highest output value.
A frequently used method for finding an accurate predictor is regularized empirical risk minimization (ERM), which minimizes an empirical loss of the predictor (with regularization) on the  training examples  : where we use to denote the identity matrix.
Matrix  (whose rows are orthonormal) is the common structure parameter shared by all the problems; and are weight vectors specific to each prediction problem . The idea of this model is to discover a common low-dimensional predictive structure (shared by the  problems) parameterized by the projection matrix .
In this setting, the goal of structural learning may also be regarded as learning a good feature map  -a low-dimensional feature vector parameterized by .
In joint ERM, we seek  (and weight vectors) that minimizes the empirical risk summed over all the problems: It can be shown that using joint ERM, we can reliably estimate the optimal joint parameter  as long as  is large (even when each  is small).
This is the key reason why structural learning is effective.
A formal PAC-style analysis can be found in (Ando and Zhang, 2004).
2.3 Alternating
structure optimization (ASO)  is a loss function to quantify the difference between the prediction   and the true output, and  is a regularization term to control the The optimization problem (2) has a simple solution using SVD when we choose square regularization: ter is given.
For clarity, let be a weight vector   Then, for problem such that: (2) becomes the minimization of the joint empirical risk written as: Input: training data ( ) Parameters: dimension and regularization param Output: matrix with rows , and arbitrary Initialize: iterate to  do for With fixed and, solve for : This minimization can be approximately solved by the following alternating optimization procedure: minimizes the joint empirical risk ((3)., and find  minimizes the joint empirical risk (3).
that  Let endfor Compute the SVD of   Let the rows of be the left singular vectors of corresponding to the largest singular values.
until converge Figure 1: SVD-based Alternating Structure Optimization (SVD-ASO) Algorithm  Iterate until a convergence criterion is met.
In the first step, we train  predictors independently.
It is the second step that couples all the problems.
Its solution is given by the SVD (singular value decomposition) of the predictor matrix   : the rows of the optimum  are given by the most sigIntuitively, the nificant left singular vectors1 of optimum  captures the maximal commonality of the  predictors (each derived from ).
These  predictors are updated using the new structure matrix  in the next iteration, and the process repeats.
Figure 1 summarizes the algorithm sketched above, which we call the alternating structure optimization (ASO) algorithm.
The formal derivation can be found in (Ando and Zhang, 2004).
Comparison with existing techniques It is important to note that this SVD-based ASO (SVD-ASO) procedure is fundamentally different from the usual principle component analysis (PCA), which can be regarded as dimension reduction in the data space . By contrast, the dimension reduction performed in the SVD-ASO algorithm is on the predictor space (a set of predictors).
This is possible because we observe multiple predictors from multiple learning tasks.
If we regard the observed predictors as sample points of the predictor distribution in 1 is computed so that the best low-rank In other words, in the least square sense is obtained by approximation of see e.g.
Golub and Loan projecting onto the row space of (1996) for SVD.
the predictor space (corrupted with estimation error, or noise), then SVD-ASO can be interpreted as finding the "principle components" (or commonality) of these predictors (i.e., "what good predictors are like").
Consequently the method directly looks for low-dimensional structures with the highest predictive power.
By contrast, the principle components of input data in the data space (which PCA seeks) may not necessarily have the highest predictive power.
The above argument also applies to the feature generation from unlabeled data using LSI (e.g.
Ando (2004)).
Similarly, Miller et al.(2004) used word-cluster memberships induced from an unannotated corpus as features for named entity chunking.
Our work is related but more general, because we can explore additional information from unlabeled data using many different auxiliary problems.
Since Miller et al.(2004)'s experiments used a proprietary corpus, direct performance comparison is not possible.
However, our preliminary implementation of the word clustering approach did not provide any improvement on our tasks.
As we will see, our starting performance is already high.
Therefore the additional information discovered by SVD-ASO appears crucial to achieve appreciable improvements.
3 Semi-supervised Learning Method For semi-supervised learning, the idea is to create many auxiliary prediction problems (relevant to the task) from unlabeled data so that we can learn the fishared structure  (useful for the task) using the ASO algorithm.
In particular, we want to create auxiliary problems with the following properties: we need to automatically generate various "labeled" data for the auxiliary problems from unlabeled data.
auxiliary problems should be related to the target problem.
That is, they should share a certain predictive structure.
The final classifier for the target task is in the form of (1), a linear predictor for structural learning.
We fix  (learned from unlabeled data through auxiliary problems) and optimize weight vectors and on the given labeled data.
We summarize this semisupervised learning procedure below.
Ex 3.1 Predict words.
Create auxiliary problems by regarding the word at each position as an auxiliary label, which we want to predict from the context.
For instance, predict whether a word is "Smith" or not from its context.
This problem is relevant to, for instance, named entity chunking since knowing a word is "Smith" helps to predict whether it is part of a name.
One binary classification problem can be created for each possible word value (e.g., "IBM", "he", "get",    ).
Hence, many auxiliary problems can be obtained using this idea.
More generally, given a feature representation of the input data, we may mask some features as unobserved, and learn classifiers to predict these `masked' features based on other features that are not masked.
The automatic-labeling requirement is satisfied since the auxiliary labels are observable to us.
To create relevant problems, we should choose to (mask and) predict features that have good correlation to the target classes, such as words on text tagging/chunking tasks.
3.1.2 Partially-supervised strategy Auxiliary problem creation The idea is to discover useful features (which do not necessarily appear in the labeled data) from the unlabeled data through learning auxiliary problems.
Clearly, auxiliary problems more closely related to the target problem will be more beneficial.
However, even if some problems are less relevant, they will not degrade performance severely since they merely result in some irrelevant features (originated from irrelevant -components), which ERM learners can cope with.
On the other hand, potential gains from relevant auxiliary problems can be significant.
In this sense, our method is robust.
We present two general strategies for generating useful auxiliary problems: one in a completely unsupervised fashion, and the other in a partiallysupervised fashion.
3.1.1 Unsupervised
strategy In the first strategy, we regard some observable as auxiliary class substructures of the input data labels, and try to predict these labels using other parts of the input data.
The second strategy is motivated by co-training.
We use two (or more) distinct feature maps:  and  . First, we train a classifier  for the target task, using the feature map  and the labeled data.
The auxiliary tasks are to predict the behavior of this classifier  (such as predicted labels) on the unlabeled data, by using the other feature map  . Note that unlike co-training, we only use the classifier as a means of creating auxiliary problems that meet the relevancy requirement, instead of using it to bootstrap labels.
Ex 3.2 Predict the topchoices of the classifier.
Predict the combination of (a few) classes to which  assigns the highest output (confidence) values.
For instance, predict whether  assigns the highest confidence values to CLASS1 and CLASS2 in this order.
By setting , the auxiliary task is simply to predict the label prediction of classifier  . By set, fine-grained distinctions (related to inting trinsic sub-classes of target classes) can be learned.
From a -way classification problem,   binary prediction problems can be created.
fi4 Algorithms Used in Experiments Using auxiliary problems introduced above, we study the performance of our semi-supervised learning method on named entity chunking and syntactic chunking.
This section describes the algorithmic aspects of the experimental framework.
The taskspecific setup is described in Sections 5 and 6.
4.1 Extension
of the basic SVD-ASO algorithm In our experiments, we use an extension of SVDASO.
In NLP applications, features have natural grouping according to their types/origins such as `current words', `parts-of-speech on the right', and so forth.
It is desirable to perform a localized optimization for each of such natural feature groups.
Hence, we associate each feature group with a submatrix of structure matrix .
The optimization algorithm for this extension is essentially the same as SVD-ASO in Figure 1, but with the SVD step performed separately for each group.
See (Ando and Zhang, 2004) for the precise formulation.
In addition, we regularize only those components of which correspond to the non-negative part of . The motivation is that positive weights are usually directly related to the target concept, while negative ones often yield much less specific information representing `the others'.
The resulting extension, in effect, only uses the positive components of in the SVD computation.
(Zhang, 2004).
As we will show in Section 7.3, our formulation (rowis relatively insensitive to the change in dimension of the structure matrix).
We fix (for each feature group) to 50, and use it in all settings.
The most time-consuming process is the training of  auxiliary predictors on the unlabeled data in Figure 1).
Fixing the number of (computing iterations to a constant, it runs in linear to  and the number of unlabeled instances and takes hours in our settings that use more than 20 million unlabeled instances.
Baseline algorithms Supervised classifier For comparison, we train a classifier using the same features and algorithm, but in effect).
without unlabeled data ( Chunking algorithm, loss function, training algorithm, and parameter settings Co-training We test co-training since our idea of partially-supervised auxiliary problems is motivated by co-training.
Our implementation follows the original work (Blum and Mitchell, 1998).
The two (or more) classifiers (with distinct feature maps) are trained with labeled data.
We maintain a pool of  unlabeled instances by random selection.
The classifier proposes labels for the instances in this pool.
We choose  instances for each classifier with high confidence while preserving the class distribution observed in the initial labeled data, and add them to the labeled data.
The process is then repeated.
We explore =50K, 100K, =50,100,500,1K, and commonly-used feature splits: `current vs.
context' and `current+left-context vs.
current+right-context'. Self-training Single-view bootstrapping is sometimes called self-training.
We test the basic selftraining2, which replaces multiple classifiers in the co-training procedure with a single classifier that employs all the features.
co/self-training oracle performance To avoid the issue of parameter selection for the coand selftraining, we report their best possible oracle performance, which is the best F-measure number among all the coand self-training parameter settings including the choice of the number of iterations.
2 We
also tested "self-training with bagging", which Ng and Cardie (2003) used for co-reference resolution.
We omit results since it did not produce better performance than the supervised baseline.
As is commonly done, we encode chunk information into word tags to cast the chunking problem to that of sequential word tagging.
We perform Viterbistyle decoding to choose the word tag sequence that maximizes the sum of tagging confidence values.
In all settings (including baseline methods), the loss function is a modification of the Huber's ro   bust loss for regression:    if  ; and  otherwise; with square regularization (  ).
One may select other loss functions such as SVM or logistic regression.
The specific choice is not important for the purpose of this paper.
The training algorithm is stochastic gradient descent, which is argued to perform well for regularized convex ERM learning formulations words, parts-of-speech (POS), character types, 4 characters at the beginning/ending in a 5-word window.
words in a 3-syntactic chunk window.
labels assigned to two words on the left.
bi-grams of the current word and the label on the left.
labels assigned to previous occurrences of the current word.
Figure 2: Feature types for named entity chunking.
POS and syntactic chunk information is provided by the organizer.
# of aux.
problems 1000 1000 1000 72 72 72 72 Auxiliary labels previous words current words next words  's top-2 choices  's top-2 choices  's top-2 choices 's top-2 choices Features used for learning aux problems all but previous words all but current words all but next words  (all but left context)  (left context) (all but right context)  (right context) Figure 3: Auxiliary problems used for named entity chunking.
3000 problems `mask' words and predict them from the other features on unlabeled data.
288 problems predict classifier 's predictions on unlabeled data, where is trained with labeled data using feature map . There are 72 possible top-2 choices from 9 classes (beginning/inside of four types of name chunks and `outside').
5 Named
Entity Chunking Experiments We report named entity chunking performance on the CoNLL'03 shared-task3 corpora (English and German).
We choose this task because the original intention of this shared task was to test the effectiveness of semi-supervised learning methods.
However, it turned out that none of the top performing systems used unlabeled data.
The likely reason is that the number of labeled data is relatively large ( 200K), making it hard to benefit from unlabeled data.
We show that our ASO-based semi-supervised learning method (hereafter, ASO-semi) can produce results appreciably better than all of the top systems, by using unlabeled data as the only additional resource.
In particular, we do not use any gazetteer information, which was used in all other systems.
The CoNLL corpora are annotated with four types of named entities: persons, organizations, locations, and miscellaneous names (e.g., "World Cup").
We use the official training/development/test splits.
Our unlabeled data sets consist of 27 million words (English) and 35 million words (German), respectively.
They were chosen from the same sources  Reuters and ECI Multilingual Text Corpus  as the provided corpora but disjoint from them.
5.1 Features
of the classifier" using feature splits `left context vs.
the others' and `right context vs.
the others'.
For word-prediction problems, we only consider the instances whose current words are either nouns or adjectives since named entities mostly consist of these types.
Also, we leave out all but at most 1000 binary prediction problems of each type that have the largest numbers of positive examples to ensure that auxiliary predictors can be adequately learned with a sufficiently large number of examples.
The results we report are obtained by using all the problems in Figure 3 unless otherwise specified.
5.3 Named
entity chunking results methods Our feature representation is a slight modification of a simpler configuration (without any gazetteer) in (Zhang and Johnson, 2003), as shown in Figure 2.
We use POS and syntactic chunk information provided by the organizer.
5.2 Auxiliary
problems test diff.
from supervised data F prec.
recall F English, small (10K examples) training set ASO-semi dev.
81.25 +10.02 +7.00 +8.51 co/self oracle 73.10 +0.32 +0.39 +0.36 ASO-semi test 78.42 +9.39 +10.73 +10.10 co/self oracle 69.63 +0.60 +1.95 +1.31 English, all (204K) training examples ASO-semi dev.
93.15 +2.25 +3.00 +2.62 co/self oracle 90.64 +0.04 +0.20 +0.11 ASO-semi test 89.31 +3.20 +4.51 +3.86 co/self oracle 85.40 German, all (207K) training examples ASO-semi dev.
74.06 +7.04 +10.19 +9.22 co/self oracle 66.47 +4.39 +1.63 ASO-semi test 75.27 +4.64 +6.59 +5.88 co/self oracle 70.45 +2.59 +1.06 Figure 4: Named entity chunking results.
No gazetteer.
Fmeasure and performance improvements over the supervised baseline in precision, recall, and F.
For coand self-training (baseline), the oracle performance is shown.
As shown in Figure 3, we experiment with auxiliary problems from Ex 3.1 and 3.2: "Predict current (or previous or next) words"; and "Predict top-2 choices Figure 4 shows results in comparison with the supervised baseline in six configurations, each trained fiwith one of three sets of labeled training examples: a small English set (10K examples randomly chosen), the entire English training set (204K), and the entire German set (207K), tested on either the development set or test set.
ASO-semi significantly improves both precision and recall in all the six configurations, resulting in improved F-measures over the supervised baseline by +2.62% to +10.10%.
Coand self-training, at their oracle performance, improve recall but often degrade precision; consequently, their F-measure improvements are relatively low: 0.05% to +1.63%.
Comparison with top systems As shown in Figure 5, ASO-semi achieves higher performance than the top systems on both English and German data.
Most of the top systems boost performance by external hand-crafted resources such as: large gazetteers4 ; a large amount (2 million words) of labeled data manually annotated with finer-grained named entities (FIJZ03); and rule-based post processing (KSNM03).
Hence, we feel that our results, obtained by using unlabeled data as the only additional resource, are encouraging.
System ASO-semi FIJZ03 CN03 KSNM03 Eng.
89.31 88.76 88.31 86.31 Ger.
75.27 72.41 65.67 71.90 Additional resources unlabeled data gazetteers; 2M-word labeled data (English) gazetteers (English); (also very elaborated features) rule-based post processing uniand bi-grams of words and POS in a 5-token window.
word-POS bi-grams in a 3-token window.
POS tri-grams on the left and right.
labels of the two words on the left and their bi-grams.
bi-grams of the current word and two labels on the left.
Figure 6: Feature types for syntactic chunking.
POS information is provided by the organizer.
prec. 93.83 94.57 93.76 recall 93.37 94.20 93.56 supervised ASO-semi co/self oracle Figure 7: Syntactic chunking results.
use the WSJ articles in 1991 (15 million words) from the TREC corpus as the unlabeled data.
6.1 Features
and auxiliary problems Our feature representation is a slight modification of a simpler configuration (without linguistic features) in (Zhang et al., 2002), as shown in Figure 6.
We use the POS information provided by the organizer.
The types of auxiliary problems are the same as in the named entity experiments.
For word predictions, we exclude instances of punctuation symbols.
6.2 Syntactic
chunking results Figure 5: Named entity chunking.
F-measure on the test sets.
Previous best results: FIJZ03 (Florian et al., 2003), CN03 (Chieu and Ng, 2003), KSNM03 (Klein et al., 2003).
As shown in Figure 7, ASO-semi improves both precision and recall over the supervised baseline.
It   in F-measure, which outperforms achieves the supervised baseline by  .
Coand selftraining again slightly improve recall but slightly degrade precision at their oracle performance, which demonstrates that it is not easy to benefit from unlabeled data on this task.
Comparison with the previous best systems As shown in Figure 8, ASO-semi achieves performance higher than the previous best systems.
Though the space constraint precludes providing the detail, we note that ASO-semi outperforms all of the previous top systems in both precision and recall.
Unlike named entity chunking, the use of external resources on this task is rare.
An exception is the use of output from a grammar-based full parser as features in ZDJ02+, which our system does not use.
KM01 and CM03 boost performance by classifier combinations.
SP03 trains conditional random fields for NP 6 Syntactic Chunking Experiments Next, we report syntactic chunking performance on the CoNLL'00 shared-task5 corpus.
The training and test data sets consist of the Wall Street Journal corpus (WSJ) sections 1518 (212K words) and section 20, respectively.
They are annotated with eleven types of syntactic chunks such as noun phrases.
We 4 Whether or not gazetteers are useful depends on their coverage.
A number of top-performing systems used their own gazetteers in addition to the organizer's gazetteers and reported significant performance improvements (e.g., FIJZ03, CN03, and ZJ03).
5 http://cnts.uia.ac.be/conll2000/chunking firow# ASO-semi KM01 CM03 SP03 ZDJ02 ZDJ02+ all 94.39 93.91 93.74  93.57 94.17 NP 94.70 94.39 94.41 94.38 93.89 94.38 description +unlabeled data SVM combination perceptron in two layers conditional random fields generalized Winnow +full parser output 4 7 9 11 15 26 Figure 8: Syntactic chunking F-measure.
Comparison with previous best results: KM01 (Kudoh and Matsumoto, 2001), CM03 (Carreras and Marquez, 2003), SP03 (Sha and Pereira, 2003), ZDJ02 (Zhang et al., 2002).
Features corresponding to significant entries Ltd, Inc, Plc, International, Ltd., Association, Group, Inc.
Co, Corp, Co., Company, Authority, Corp., Services PCT, N/A, Nil, Dec, BLN, Avg, Year-on-year, UNCH New, France, European, San, North, Japan, Asian, India Peter, Sir, Charles, Jose, Paul, Lee, Alan, Dan, John, James June, May, July, Jan, March, August, September, April Interpretation organizations organizations no names locations persons months (noun phrases) only.
ASO-semi produces higher NP chunking performance than the others.
Figure 10: Interpretation of computed from wordprediction (unsupervised) problems for named entity chunking.
words beginning with upper-case letters (i.e., likely to be names in English).
Our method captures the spirit of predictive word-clustering but is more general and effective on our tasks.
It is possible to develop a general theory to show that the auxiliary problems we use are helpful under reasonable conditions.
The intuition is as follows.
Suppose we split the features into two parts  and  and predict  based on .
Suppose features in  are correlated to the class labels (but not necessarily correlated among themselves).
Then, the auxiliary prediction problems are related to the target task, and thus can reveal useful structures of  . Under some conditions, it can be shown that features in  with similar predictive performance tend to map to similar low-dimensional vectors through .
This effect can be empirically observed in Figure 10 and will be formally shown elsewhere.
7 Empirical
Analysis 7.1 Effectiveness of auxiliary problems English named entity German named entity 90 76 89 74 88 72 87 70 86 68 85 1 supervised set dev w/ "Predict (previous, current, or next) words" w/ "Predict top-2 choices" w/ "Predict words" + "Predict top-2 choices" Figure 9: Named entity F-measure produced by using individual types of auxiliary problems.
Trained with the entire training sets and tested on the test sets.
Figure 9 shows F-measure obtained by computing  from individual types of auxiliary problems on named entity chunking.
Both types  "Predict words" and "Predict top-2 choices of the classifier"  are useful, producing significant performance improvements over the supervised baseline.
The best performance is achieved when  is produced from all of the auxiliary problems.
7.2 Interpretation
of  Effect of the  dimension 20 40 ASO-semi supervised 60 80 100 dimension To gain insights into the information obtained from unlabeled data, we examine the  entries associated with the feature `current words', computed for the English named entity task.
Figure 10 shows the features associated with the entries of  with the largest values, computed from the 2000 unsupervised auxiliary problems: "Predict previous words" and "Predict next words".
For clarity, the figure only shows Figure 11: F-measure in relation to the row-dimension of English named entity chunking, test set.
Recall that throughout the experiments, we fix the row-dimension of  (for each feature group) to 50.
Figure 11 plots F-measure in relation to the rowdimension of , which shows that the method is relatively insensitive to the change of this parameter, at least in the range which we consider.
We presented a novel semi-supervised learning method that learns the most predictive lowdimensional feature projection from unlabeled data using the structural learning algorithm SVD-ASO.
On CoNLL'00 syntactic chunking and CoNLL'03 named entity chunking (English and German), the method exceeds the previous best systems (including those which rely on hand-crafted resources) by using unlabeled data as the only additional resource.
The key idea is to create auxiliary problems automatically from unlabeled data so that predictive structures can be learned from that data.
In practice, it is desirable to create as many auxiliary problems as possible, as long as there is some reason to believe in their relevancy to the task.
This is because the risk is relatively minor while the potential gain from relevant problems is large.
Moreover, the auxiliary problems used in our experiments are merely possible examples.
One advantage of our approach is that one may design a variety of auxiliary problems to learn various aspects of the target problem from unlabeled data.
Structural learning provides a framework for carrying out possible new ideas.
Acknowledgments Part of the work was supported by ARDA under the NIMD program PNWD-SW-6059.
References Rie Kubota Ando and Tong Zhang.
2004. A framework for learning predictive structures from multiple tasks and unlabeled data.
Technical report, IBM.
RC23462. Rie Kubota Ando.
2004. Semantic lexicon construction: Learning from unlabeled data via spectral analysis.
In Proceedings of CoNLL-2004.
Avrim Blum and Tom Mitchell.
1998. Combining labeled and unlabeled data with co-training.
In proceedings of COLT-98.
Xavier Carreras and Lluis Marquez.
2003. Phrase recognition by filtering and ranking with perceptrons.
In Proceedings of RANLP-2003.
Hai Leong Chieu and Hwee Tou Ng.
2003. Named entity recognition with a maximum entropy approach.
In Proceedings CoNLL-2003, pages 160163.
Michael Collins and Yoram Singer.
1999. Unsupervised models for named entity classification.
In Proceedings of EMNLP/VLC'99.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong Zhang.
2003. Named entity recognition through classifier combination.
In Proceedings CoNLL-2003, pages 168171.
Gene H.
Golub and Charles F.
Van Loan.
1996. Matrix computations third edition.
Dan Klein, Joseph Smarr, Huy Nguyen, and Christopher D.
Manning. 2003.
Named entity recognition with character-level models.
In Proceedings CoNLL2003, pages 188191.
Taku Kudoh and Yuji Matsumoto.
2001. Chunking with support vector machines.
In Proceedings of NAACL 2001.
Bernard Merialdo.
1994. Tagging English text with a probabilistic model.
Computational Linguistics, 20(2):155171.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discriminative training.
In Proceedings of HLT-NAACL-2004.
Vincent Ng and Claire Cardie.
2003. Weakly supervised natural language learning without redundant views.
In Proceedings of HLT-NAACL-2003.
David Pierce and Claire Cardie.
2001. Limitations of co-training for natural language learning from large datasets.
In Proceedings of EMNLP-2001.
Ellen Riloff and Rosie Jones.
1999. Learning dictionaries for information extraction by multi-level bootstrapping.
In Proceedings of AAAI-99.
Fei Sha and Fernando Pereira.
2003. Shallow parsing with conditional random fields.
In Proceedings of HLT-NAACL'03.
David Yarowsky.
1995. Unsupervised word sense disambiguation rivaling supervised methods.
In Proceedings of ACL-95.
Tong Zhang and David E.
Johnson. 2003.
A robust risk minimization based named entity recognition system.
In Proceedings CoNLL-2003, pages 204207.
Tong Zhang, Fred Damerau, and David E.
Johnson. 2002.
Text chunking based on a generalization of Winnow.
Journal of Machine Learning Research, 2:615 637.
Tong Zhang.
2004. Solving large scale linear prediction problems using stochastic gradient descent algorithms.
In ICML 04, pages 919926 .
Scaling Conditional Random Fields Using Error-Correcting Codes Trevor Cohn Andrew Smith Department of Computer Science Division of Informatics and Software Engineering University of Edinburgh University of Melbourne, Australia United Kingdom tacohn@csse.unimelb.edu.au a.p.smith-2@sms.ed.ac.uk Miles Osborne Division of Informatics University of Edinburgh United Kingdom miles@inf.ed.ac.uk Abstract Conditional Random Fields (CRFs) have been applied with considerable success to a number of natural language processing tasks.
However, these tasks have mostly involved very small label sets.
When deployed on tasks with larger label sets, the requirements for computational resources mean that training becomes intractable.
This paper describes a method for training CRFs on such tasks, using error correcting output codes (ECOC).
A number of CRFs are independently trained on the separate binary labelling tasks of distinguishing between a subset of the labels and its complement.
During decoding, these models are combined to produce a predicted label sequence which is resilient to errors by individual models.
Error-correcting CRF training is much less resource intensive and has a much faster training time than a standardly formulated CRF, while decoding performance remains quite comparable.
This allows us to scale CRFs to previously impossible tasks, as demonstrated by our experiments with large label sets.
models that define a conditional distribution over label sequences given an observation sequence.
They allow the use of arbitrary, overlapping, non-independent features as a result of their global conditioning.
This allows us to avoid making unwarranted independence assumptions over the observation sequence, such as those required by typical generative models.
Efficient inference and training methods exist when the graphical structure of the model forms a chain, where each position in a sequence is connected to its adjacent positions.
CRFs have been applied with impressive empirical results to the tasks of named entity recognition (McCallum and Li, 2003), simplified part-of-speech (POS) tagging (Lafferty et al., 2001), noun phrase chunking (Sha and Pereira, 2003) and extraction of tabular data (Pinto et al., 2003), among other tasks.
CRFs are usually estimated using gradient-based methods such as limited memory variable metric (LMVM).
However, even with these efficient methods, training can be slow.
Consequently, most of the tasks to which CRFs have been applied are relatively small scale, having only a small number of training examples and small label sets.
For much larger tasks, with hundreds of labels and millions of examples, current training methods prove intractable.
Although training can potentially be parallelised and thus run more quickly on large clusters of computers, this in itself is not a solution to the problem: tasks can reasonably be expected to increase in size and complexity much faster than any increase in computing power.
In order to provide scalability, the factors which most affect the resource usage and runtime of the training method Introduction Conditional random fields (CRFs) (Lafferty et al., 2001) are probabilistic models for labelling sequential data.
CRFs are undirected graphical Proceedings of the 43rd Annual Meeting of the ACL, pages 1017, Ann Arbor, June 2005.
c 2005 Association for Computational Linguistics fimust be addressed directly  ideally the dependence on the number of labels should be reduced.
This paper presents an approach which enables CRFs to be used on larger tasks, with a significant reduction in the time and resources needed for training.
This reduction does not come at the cost of performance  the results obtained on benchmark natural language problems compare favourably, and sometimes exceed, the results produced from regular CRF training.
Error correcting output codes (ECOC) (Dietterich and Bakiri, 1995) are used to train a community of CRFs on binary tasks, with each discriminating between a subset of the labels and its complement.
Inference is performed by applying these `weak' models to an unknown example, with each component model removing some ambiguity when predicting the label sequence.
Given a sufficient number of binary models predicting suitably diverse label subsets, the label sequence can be inferred while being robust to a number of individual errors from the weak models.
As each of these weak models are binary, individually they can be efficiently trained, even on large problems.
The number of weak learners required to achieve good performance is shown to be relatively small on practical tasks, such that the overall complexity of error-correcting CRF training is found to be much less than that of regular CRF training methods.
We have evaluated the error-correcting CRF on the CoNLL 2003 named entity recognition (NER) task (Sang and Meulder, 2003), where we show that the method yields similar generalisation performance to standardly formulated CRFs, while requiring only a fraction of the resources, and no increase in training time.
We have also shown how the errorcorrecting CRF scales when applied to the larger task of POS tagging the Penn Treebank and also the even larger task of simultaneously noun phrase chunking (NPC) and POS tagging using the CoNLL 2000 data-set (Sang and Buchholz, 2000).
model are connected by edges to form a linear chain.
The joint distribution of the label sequence, y, given the input observation sequence, x, is given by p(y|x) = k fk (t, yt-1, yt, x) where T is the length of both sequences and k are the parameters of the model.
The functions fk are feature functions which map properties of the observation and the labelling into a scalar value.
Z(x) is the partition function which ensures that p is a probability distribution.
A number of algorithms can be used to find the optimal parameter values by maximising the loglikelihood of the training data.
Assuming that the training sequences are drawn IID from the population, the conditional log likelihood L is given by L= T (i) +1 (i) (i) Conditional random fields CRFs are undirected graphical models used to specify the conditional probability of an assignment of output labels given a set of input observations.
We consider only the case where the output labels of the where x(i) and y(i) are the ith observation and label sequence.
Note that a prior is often included in the L formulation; it has been excluded here for clarity of exposition.
CRF estimation methods include generalised iterative scaling (GIS), improved iterative scaling (IIS) and a variety of gradient based methods.
In recent empirical studies on maximum entropy models and CRFs, limited memory variable metric (LMVM) has proven to be the most efficient method (Malouf, 2002; Wallach, 2002); accordingly, we have used LMVM for CRF estimation.
Every iteration of LMVM training requires the computation of the log-likelihood and its derivative with respect to each parameter.
The partition function Z(x) can be calculated efficiently using dynamic programming with the forward algorithm.
Z(x) is given by y T (y) where are the forward values, defined recursively as t+1 (y) = t (y ) exp The derivative of the log-likelihood is given by L k = Error Correcting Output Codes T (i) +1 (i) (i) The first term is the empirical count of feature k, and the second is the expected count of the feature under the model.
When the derivative equals zero  at convergence  these two terms are equal.
Evaluating the first term of the derivative is quite simple.
However, the sum over all possible labellings in the second term poses more difficulties.
This term can be factorised, yielding p(Yt-1 = y, Yt = y|x(i) )fk (t, y, y, x(i) ) This term uses the marginal distribution over pairs of labels, which can be efficiently computed from the forward and backward values as t-1 (y ) exp The backward probabilities are defined by the recursive relation t (y) = t+1 (y ) exp Typically CRF training using LMVM requires many hundreds or thousands of iterations, each of which involves calculating of the log-likelihood and its derivative.
The time complexity of a single iteration is O(L2 N T F ) where L is the number of labels, N is the number of sequences, T is the average length of the sequences, and F is the average number of activated features of each labelled clique.
It is not currently possible to state precise bounds on the number of iterations required for certain problems; however, problems with a large number of sequences often require many more iterations to converge than problems with fewer sequences.
Note that efficient CRF implementations cache the feature values for every possible clique labelling of the training data, which leads to a memory requirement with the same complexity of O(L2 N T F )  quite demanding even for current computer hardware.
Since the time and space complexity of CRF estimation is dominated by the square of the number of labels, it follows that reducing the number of labels will significantly reduce the complexity.
Error-correcting coding is an approach which recasts multiple label problems into a set of binary label problems, each of which is of lesser complexity than the full multiclass problem.
Interestingly, training a set of binary CRF classifiers is overall much more efficient than training a full multi-label model.
This is because error-correcting CRF training reduces the L2 complexity term to a constant.
Decoding proceeds by predicting these binary labels and then recovering the encoded actual label.
Error-correcting output codes have been used for text classification, as in Berger (1999), on which the following is based.
Begin by assigning to each of the m labels a unique n-bit string Ci, which we will call the code for this label.
Now train n binary classifiers, one for each column of the coding matrix (constructed by taking the labels' codes as rows).
The j th classifier, j, takes as positive instances those with label i where Cij = 1.
In this way, each classifier learns a different concept, discriminating between different subsets of the labels.
We denote the set of binary classifiers as = { 1, 2,. . ., n }, which can be used for prediction as follows.
Classify a novel instance x with each of the binary classifiers, yielding a n-bit vector (x) = { 1 (x), 2 (x), . . ., n (x)}.
Now compare this vector to the codes for each label.
The vector may not exactly match any of the labels due to errors in the individual classifiers, and thus we chose the actual label which minimises the distance argmini ((x), Ci ).
Typically the Hamming distance is used, which simply measures the number of differing bit positions.
In this manner, prediction is resilient to a number of prediction errors by the binary classifiers, provided the codes for the labels are sufficiently diverse.
3.1 Error-correcting CRF training Error-correcting codes can also be applied to sequence labellers, such as CRFs, which are capable of multiclass labelling.
ECOCs can be used with CRFs in a similar manner to that given above for ficlassifiers.
A series of CRFs are trained, each on a relabelled variant of the training data.
The relabelling for each binary CRF maps the labels into binary space using the relevant column of the coding matrix, such that label i is taken as a positive for the j th model example if Cij = 1.
Training with a binary label set reduces the time and space complexity for each training iteration to O(N T F ); the L2 term is now a constant.
Provided the code is relatively short (i.e.
there are few binary models, or weak learners), this translates into considerable time and space savings.
Coding theory doesn't offer any insights into the optimal code length (i.e.
the number of weak learners).
When using a very short code, the error-correcting CRF will not adequately model the decision boundaries between all classes.
However, using a long code will lead to a higher degree of dependency between pairs of classifiers, where both model similar concepts.
The generalisation performance should improve quickly as the number of weak learners (code length) increases, but these gains will diminish as the inter-classifier dependence increases.
3.2 Error-correcting CRF decoding While training of error-correcting CRFs is simply a logical extension of the ECOC classifier method to sequence labellers, decoding is a different matter.
We have applied three decoding different strategies.
The Standalone method requires each binary CRF to find the Viterbi path for a given sequence, yielding a string of 0s and 1s for each model.
For each position t in the sequence, the tth bit from each model is taken, and the resultant bit string compared to each of the label codes.
The label with the minimum Hamming distance is then chosen as the predicted label for that site.
This method allows for error correction to occur at each site, however it discards information about the uncertainty of each weak learner, instead only considering the most probable paths.
The Marginals method of decoding uses the marginal probability distribution at each position in the sequence instead of the Viterbi paths.
This distribution is easily computed using the forward backward algorithm.
The decoding proceeds as before, however instead of a bit string we have a vector of probabilities.
This vector is compared to each of the label codes using the L1 distance, and the closest label is chosen.
While this method incorporates the uncertainty of the binary models, it does so at the expense of the path information in the sequence.
Neither of these decoding methods allow the models to interact, although each individual weak learner may benefit from the predictions of the other weak learners.
The Product decoding method addresses this problem.
It treats each weak model as an independent predictor of the label sequence, such that the probability of the label sequence given the observations can be re-expressed as the product of the probabilities assigned by each weak model.
A given labelling y is projected into a bit string for each weak learner, such that the ith entry in the string is Ckj for the j th weak learner, where k is the index of label yi . The weak learners can then estimate the probability of the bit string; these are then combined into a global product to give the probability of the label sequence p(y|x) = 1 Z (x) pj (bj (y)|x) where pj (q|x) is the predicted probability of q given x by the j th weak learner, bj (y) is the bit string representing y for the j th weak learner and Z (x) is the partition function.
The log probability is {Fj (bj (y), x)  j log Zj (x)} log Z (x) where Fj (y, x) = T +1 fj (t, yt-1, yt, x).
This log t=1 probability can then be maximised using the Viterbi algorithm as before, noting that the two log terms are constant with respect to y and thus need not be evaluated.
Note that this decoding is an equivalent formulation to a uniformly weighted logarithmic opinion pool, as described in Smith et al.(2005). Of the three decoding methods, Standalone has the lowest complexity, requiring only a binary Viterbi decoding for each weak learner.
Marginals is slightly more complex, requiring the forward and backward values.
Product, however, requires Viterbi decoding with the full label set, and many features  the union of the features of each weak learner  which can be quite computationally demanding.
fi3.3 Choice of code The accuracy of ECOC methods are highly dependent on the quality of the code.
The ideal code has diverse rows, yielding a high error-correcting capability, and diverse columns such that the weak learners model highly independent concepts.
When the number of labels, k, is small, an exhaustive code with every unique column is reasonable, given there are 2k-1 1 unique columns.
With larger label sets, columns must be selected with care to maximise the inter-row and inter-column separation.
This can be done by randomly sampling the column space, in which case the probability of poor separation diminishes quickly as the number of columns increases (Berger, 1999).
Algebraic codes, such as BCH codes, are an alternative coding scheme which can provide near-optimal error-correcting capability (MacWilliams and Sloane, 1977), however these codes provide no guarantee of good column separation.
Model Decoding Multiclass Coded standalone marginals product MLE 88.04 88.23 88.23 88.69 Regularised 89.78 88.67 89.19 89.69 Table 1: F1 scores on NER task.
trained without regularisation and with a Gaussian prior.
An exhaustive code was created with all 127 unique columns.
All of the weak learners were trained with the same feature set, each having around 315,000 features.
The performance of the standard and error-correcting models are shown in Table 1.
We tested for statistical significance using the matched pairs test (Gillick and Cox, 1989) at p < 0.001.
Those results which are significantly better than the corresponding multiclass MLE or regularised model are flagged with a, and those which are significantly worse with a . These results show that error-correcting CRF training achieves quite similar performance to the multiclass CRF on the task (which incidentally exceeds McCallum (2003)'s result of 89.0 using feature induction).
Product decoding was the better of the three methods, giving the best performance both with and without regularisation, although this difference was only statistically significant between the regularised standalone and the regularised product decoding.
The unregularised error-correcting CRF significantly outperformed the multiclass CRF with all decoding strategies, suggesting that the method already provides some regularisation, or corrects some inherent bias in the model.
Using such a large number of weak learners is costly, in this case taking roughly ten times longer to train than the multiclass CRF.
However, much shorter codes can also achieve similar results.
The simplest code, where each weak learner predicts only a single label (a.k.a.
one-vs-all), achieved an F score of 89.56, while only requiring 8 weak learners and less than half the training time as the multiclass CRF.
This code has no error correcting capability, suggesting that the code's column separation (and thus interdependence between weak learners) is more important than its row separation.
Experiments Our experiments show that error-correcting CRFs are highly accurate on benchmark problems with small label sets, as well as on larger problems with many more labels, which would be otherwise prove intractable for traditional CRFs.
Moreover, with a good code, the time and resources required for training and decoding can be much less than that of the standardly formulated CRF.
4.1 Named
entity recognition CRFs have been used with strong results on the CoNLL 2003 NER task (McCallum, 2003) and thus this task is included here as a benchmark.
This data set consists of a 14,987 training sentences (204,567 tokens) drawn from news articles, tagged for person, location, organisation and miscellaneous entities.
There are 8 IOB-2 style labels.
A multiclass (standardly formulated) CRF was trained on these data using features covering word identity, word prefix and suffix, orthographic tests for digits, case and internal punctuation, word length, POS tag and POS tag bigrams before and after the current word.
Only features seen at least once in the training data were included in the model, resulting in 450,345 binary features.
The model was fiAn exhaustive code was used in this experiment simply for illustrative purposes: many columns in this code were unnecessary, yielding only a slight gain in performance over much simpler codes while incurring a very large increase in training time.
Therefore, by selecting a good subset of the exhaustive code, it should be possible to reduce the training time while preserving the strong generalisation performance.
One approach is to incorporate skew in the label distribution in our choice of code  the code should minimise the confusability of commonly occurring labels more so than that of rare labels.
Assuming that errors made by the weak learners are independent, the probability of a single error, q, as a function of the code length n can be bounded by 90 89 88 F1 score 87 86 85 84 83 random random with replacement minimum loss bound oracle MLE multiclass CRF Regularised multiclass CRF 10 15 20 25 30 code length 35 40 45 50 Figure 1: NER F1 scores for standalone decoding with random codes, a minimum loss code and a greedy oracle.
Coding Multiclass Coded 200 One-vs-all Decoding standalone marginals product MLE 95.69 95.63 95.68 94.90 Regularised 95.78 96.03 96.03 96.57 where p(l) is the marginal probability of the label l, hl is the minimum Hamming distance between l and any other label, and p is the maximum probability ^ of an error by a weak learner.
The performance achieved by selecting the code with the minimum loss bound from a large random sample of codes is shown in Figure 1, using standalone decoding, where p was estimated on the development set.
For ^ comparison, randomly sampled codes and a greedy oracle are shown.
The two random sampled codes show those samples where no column is repeated, and where duplicate columns are permitted (random with replacement).
The oracle repeatedly adds to the code the column which most improves its F1 score.
The minimum loss bound method allows the performance plateau to be reached more quickly than random sampling; i.e. shorter codes can be used, thus allowing more efficient training and decoding.
Note also that multiclass CRF training required 830Mb of memory, while error-correcting training required only 380Mb.
Decoding of the test set (51,362 tokens) with the error-correcting model (exhaustive, MLE) took between 150 seconds for standalone decoding and 173 seconds for integrated decoding.
The multiclass CRF was much faster, taking only 31 seconds, however this time difference could be reduced with suitable optimisations.
Table 2: POS tagging accuracy.
4.2 Part-of-speech Tagging CRFs have been applied to POS tagging, however only with a very simple feature set and small training sample (Lafferty et al., 2001).
We used the Penn Treebank Wall Street Journal articles, training on sections 221 and testing on section 24.
In this task there are 45,110 training sentences, a total of 1,023,863 tokens and 45 labels.
The features used included word identity, prefix and suffix, whether the word contains a number, uppercase letter or a hyphen, and the words one and two positions before and after the current word.
A random code of 200 columns was used for this task.
These results are shown in Table 2, along with those of a multiclass CRF and an alternative one-vsall coding.
As for the NER experiment, the decoding performance levelled off after 100 bits, beyond which the improvements from longer codes were only very slight.
This is a very encouraging characteristic, as only a small number of weak learners are required for good performance.
The random code of 200 bits required 1,300Mb of RAM, taking a total of 293 hours to train and 3 hours to decode (54,397 tokens) on similar machines to those used before.
We do not have figures regarding the resources used by Lafferty et al.'s CRF for the POS tagging task and our attempts to train a multiclass CRF for full-scale POS tagging were thwarted due to lack of sufficient available computing resources.
Instead we trained on a 10,000 sentence subset of the training data, which required approximately 17Gb of RAM and 208 hours to train.
Our best result on the task was achieved using a one-vs-all code, which reduced the training time to 25 hours, as it only required training 45 binary models.
This result exceeds Lafferty et al.'s accuracy of 95.73% using a CRF but falls short of Toutanova et al.(2003)'s state-of-the-art 97.24%.
This is most probably due to our only using a first-order Markov model and a fairly simple feature set, where Tuotanova et al.include a richer set of features in a third order model.
4.3 Part-of-speech Tagging and Noun Phrase Segmentation The joint task of simultaneously POS tagging and noun phrase chunking (NPC) was included in order to demonstrate the scalability of error-correcting CRFs.
The data was taken from the CoNLL 2000 NPC shared task, with the model predicting both the chunk tags and the POS tags.
The training corpus consisted of 8,936 sentences, with 47,377 tokens and 118 labels.
A 200-bit random code was used, with the following features: word identity within a window, prefix and suffix of the current word and the presence of a digit, hyphen or upper case letter in the current word.
This resulted in about 420,000 features for each weak learner.
A joint tagging accuracy of 90.78% was achieved using MLE training and standalone decoding.
Despite the large increase in the number of labels in comparison to the earlier tasks, the performance also began to plateau at around 100 bits.
This task required 220Mb of RAM and took a total of 30 minutes to train each of the 200 binary CRFs, this time on Pentium 4 machines with 1Gb RAM.
Decoding of the 47,377 test tokens took 9,748 seconds and 9,870 seconds for the standalone and marginals methods respectively.
Sutton et al.(2004) applied a variant of the CRF, the dynamic CRF (DCRF), to the same task, modelling the data with two interconnected chains where one chain predicted NPC tags and the other POS tags.
They achieved better performance and training times than our model; however, this is not a fair comparison, as the two approaches are orthogonal.
Indeed, applying the error-correcting CRF algorithms to DCRF models could feasibly decrease the complexity of the DCRF, allowing the method to be applied to larger tasks with richer graphical structures and larger label sets.
In all three experiments, error-correcting CRFs have achieved consistently good generalisation performance.
The number of weak learners required to achieve these results was shown to be relatively small, even for tasks with large label sets.
The time and space requirements were lower than those of a traditional CRF for the larger tasks and, most importantly, did not increase substantially when the number of labels was increased.
Related work Most recent work on improving CRF performance has focused on feature selection.
McCallum (2003) describes a technique for greedily adding those feature conjuncts to a CRF which significantly improve the model's log-likelihood.
His experimental results show that feature induction yields a large increase in performance, however our results show that standardly formulated CRFs can perform well above their reported 73.3%, casting doubt on the magnitude of the possible improvement.
Roark et al.(2004) have also employed feature selection to the huge task of language modelling with a CRF, by partially training a voted perceptron then removing all features that the are ignored by the perceptron.
The act of automatic feature selection can be quite time consuming in itself, while the performance and runtime gains are often modest.
Even with a reduced number of features, tasks with a very large label space are likely to remain intractable.
Conclusion Standard training methods for CRFs suffer greatly from their dependency on the number of labels, making tasks with large label sets either difficult or impossible.
As CRFs are deployed more widely to tasks with larger label sets this problem will become more evident.
The current `solutions' to these scaling problems  namely feature selection, and the use of large clusters  don't address the heart of the problem: the dependence on the square of number of labels.
Error-correcting CRF training allows CRFs to be applied to larger problems and those with larger label sets than were previously possible, without requiring computationally demanding methods such as feature selection.
On standard tasks we have shown that error-correcting CRFs provide comparable or better performance than the standardly formulated CRF, while requiring less time and space to train.
Only a small number of weak learners were required to obtain good performance on the tasks with large label sets, demonstrating that the method provides efficient scalability to the CRF framework.
Error-correction codes could be applied to other sequence labelling methods, such as the voted perceptron (Roark et al., 2004).
This may yield an increase in performance and efficiency of the method, as its runtime is also heavily dependent on the number of labels.
We plan to apply error-correcting coding to dynamic CRFs, which should result in better modelling of naturally layered tasks, while increasing the efficiency and scalability of the method.
We also plan to develop higher order CRFs, using error-correcting codes to curb the increase in complexity.
Acknowledgements This work was supported in part by a PORES travelling scholarship from the University of Melbourne, allowing Trevor Cohn to travel to Edinburgh.
References Adam Berger.
1999. Error-correcting output coding for text classification.
In Proceedings of IJCAI: Workshop on machine learning for information filtering.
Thomas G.
Dietterich and Ghulum Bakiri.
1995. Solving multiclass learning problems via error-correcting output codes.
Journal of Artificial Intelligence Reseach, 2:263286.
L. Gillick and Stephen Cox.
1989. Some statistical issues in the comparison of speech recognition algorithms.
In Proceedings of the IEEE Conference on Acoustics, Speech and Signal Processing, pages 532535, Glasgow, Scotland.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models for segmenting and labelling sequence data.
In Proceedings of ICML 2001, pages 282289.
Florence MacWilliams and Neil Sloane.
1977. The theory of error-correcting codes.
North Holland, Amsterdam.
Robert Malouf.
2002. A comparison of algorithms for maximum entropy parameter estimation.
In Proceedings of CoNLL 2002, pages 4955.
Andrew McCallum and Wei Li.
2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.
In Proceedings of CoNLL 2003, pages 188191.
Andrew McCallum.
2003. Efficiently inducing features of conditional random fields.
In Proceedings of UAI 2003, pages 403410.
David Pinto, Andrew McCallum, Xing Wei, and Bruce Croft.
2003. Table extraction using conditional random fields.
In Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 235242.
Brian Roark, Murat Saraclar, Michael Collins, and Mark Johnson.
2004. Discriminative language modeling with conditional random fields and the perceptron algorithm.
In Proceedings of ACL 2004, pages 4855.
Erik F.
Tjong Kim Sang and Sabine Buchholz.
2000. Introduction to the CoNLL-2000 shared task: Chunking.
In Proceedings of CoNLL 2000 and LLL 2000, pages 127132.
Erik F.
Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In Proceedings of CoNLL 2003, pages 142147, Edmonton, Canada.
Fei Sha and Fernando Pereira.
2003. Shallow parsing with conditional random fields.
In Proceedings of HLT-NAACL 2003, pages 213220.
Andrew Smith, Trevor Cohn, and Miles Osborne.
2005. Logarithmic opinion pools for conditional random fields.
In Proceedings of ACL 2005.
Charles Sutton, Khashayar Rohanimanesh, and Andrew McCallum.
2004. Dynamic conditional random fields: Factorized probabilistic models for labelling and segmenting sequence data.
In Proceedings of the ICML 2004.
Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer.
2003. Feature rich part-of-speech tagging with a cyclic dependency network.
In Proceedings of HLTNAACL 2003, pages 252259.
Hanna Wallach.
2002. Efficient training of conditional random fields.
Master's thesis, University of Edinburgh .
Logarithmic Opinion Pools for Conditional Random Fields Andrew Smith Trevor Cohn Miles Osborne Division of Informatics Department of Computer Science Division of Informatics University of Edinburgh and Software Engineering University of Edinburgh United Kingdom University of Melbourne, Australia United Kingdom a.p.smith-2@sms.ed.ac.uk tacohn@csse.unimelb.edu.au miles@inf.ed.ac.uk Abstract Recent work on Conditional Random Fields (CRFs) has demonstrated the need for regularisation to counter the tendency of these models to overfit.
The standard approach to regularising CRFs involves a prior distribution over the model parameters, typically requiring search over a hyperparameter space.
In this paper we address the overfitting problem from a different perspective, by factoring the CRF distribution into a weighted product of individual "expert" CRF distributions.
We call this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs).
We apply the LOP-CRF to two sequencing tasks.
Our results show that unregularised expert CRFs with an unregularised CRF under a LOP can outperform the unregularised CRF, and attain a performance level close to the regularised CRF.
LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyperparameter search.
1 Introduction
In recent years, conditional random fields (CRFs) (Lafferty et al., 2001) have shown success on a number of natural language processing (NLP) tasks, including shallow parsing (Sha and Pereira, 2003), named entity recognition (McCallum and Li, 2003) and information extraction from research papers (Peng and McCallum, 2004).
In general, this work has demonstrated the susceptibility of CRFs to overfit the training data during parameter estimation.
As a consequence, it is now standard to use some form of overfitting reduction in CRF training.
Recently, there have been a number of sophisticated approaches to reducing overfitting in CRFs, including automatic feature induction (McCallum, 2003) and a full Bayesian approach to training and inference (Qi et al., 2005).
These advanced methods tend to be difficult to implement and are often computationally expensive.
Consequently, due to its ease of implementation, the current standard approach to reducing overfitting in CRFs is the use of a prior distribution over the model parameters, typically a Gaussian.
The disadvantage with this method, however, is that it requires adjusting the value of one or more of the distribution's hyperparameters.
This usually involves manual or automatic tuning on a development set, and can be an expensive process as the CRF must be retrained many times for different hyperparameter values.
In this paper we address the overfitting problem in CRFs from a different perspective.
We factor the CRF distribution into a weighted product of individual expert CRF distributions, each focusing on a particular subset of the distribution.
We call this model a logarithmic opinion pool (LOP) of CRFs (LOP-CRFs), and provide a procedure for learning the weight of each expert in the product.
The LOPCRF framework is "parameter-free" in the sense that it does not involve the requirement to adjust hyperparameter values.
LOP-CRFs are theoretically advantageous in that their Kullback-Leibler divergence with a given distribution can be explicitly represented as a function of the KL-divergence with each of their expert distributions.
This provides a well-founded framework for designing new overfitting reduction schemes: Proceedings of the 43rd Annual Meeting of the ACL, pages 1825, Ann Arbor, June 2005.
c 2005 Association for Computational Linguistics filook to factorise a CRF distribution as a set of diverse experts.
We apply LOP-CRFs to two sequencing tasks in NLP: named entity recognition and part-of-speech tagging.
Our results show that combination of unregularised expert CRFs with an unregularised standard CRF under a LOP can outperform the unregularised standard CRF, and attain a performance level that rivals that of the regularised standard CRF.
LOP-CRFs therefore provide a viable alternative to CRF regularisation without the need for hyperparameter search.
E p(o,s) [ fk ] E p(s|o) [ fk ] = 0, k ~ In general this cannot be solved for the k in closed form so numerical routines must be used.
Malouf (2002) and Sha and Pereira (2003) show that gradient-based algorithms, particularly limited memory variable metric (LMVM), require much less time to reach convergence, for some NLP tasks, than the iterative scaling methods (Della Pietra et al., 1997) previously used for log-linear optimisation problems.
In all our experiments we use the LMVM method to train the CRFs.
For CRFs with general graphical structure, calculation of E p(s|o) [ fk ] is intractable, but for the linear chain case Lafferty et al.(2001) describe an efficient dynamic programming procedure for inference, similar in nature to the forward-backward algorithm in hidden Markov models.
Conditional Random Fields A linear chain CRF defines the conditional probability of a state or label sequence s given an observed sequence o via1 : 1 exp Z(o) Logarithmic Opinion Pools where T is the length of both sequences, k are parameters of the model and Z(o) is the partition function that ensures (1) represents a probability distribution.
The functions f k are feature functions representing the occurrence of different events in the sequences s and o.
The parameters k can be estimated by maximising the conditional log-likelihood of a set of labelled training sequences.
The log-likelihood is given by: L ( ) = = ~ p(o, s) log p(s | o; ) In this paper an expert model refers a probabilistic model that focuses on modelling a specific subset of some probability distribution.
The concept of combining the distributions of a set of expert models via a weighted product has previously been used in a range of different application areas, including economics and management science (Bordley, 1982), and NLP (Osborne and Baldridge, 2004).
In this paper we restrict ourselves to sequence models.
Given a set of sequence model experts, indexed by, with conditional distributions p (s | o) and a set of non-negative normalised weights w, a logarithmic opinion pool 2 is defined as the distribution: pLOP (s | o) = 1 [p (s | o)]w ZLOP (o) (2) where p(o, s) and p(o) are empirical distributions ~ ~ defined by the training set.
At the maximum likelihood solution the model satisfies a set of feature constraints, whereby the expected count of each feature under the model is equal to its empirical count on the training data: this paper we assume there is a one-to-one mapping between states and labels, though this need not be the case.
with w 0 and w = 1, and where ZLOP (o) is the normalisation constant: ZLOP (o) = [p (s | o)]w (1999) introduced a variant of the LOP idea called Product of Experts, in which expert distributions are multiplied under a uniform weight distribution.
2 Hinton
The weight w encodes our confidence in the opinion of expert. Suppose that there is a "true" conditional distribution q(s | o) which each p (s | o) is attempting to model.
Heskes (1998) shows that the KL divergence between q(s | o) and the LOP, can be decomposed into two terms: K(q, pLOP ) = E A = log Z(o) = log ZLOP (o) + w log Z (o) LOP This relationship will be useful below, when we describe how to train the weights w of a LOP-CRF.
In this paper we will use the term LOP-CRF weights to refer to the weights w in the weighted product of the LOP-CRF distribution and the term parameters to refer to the parameters k of each expert CRF . 3.2 Training LOP-CRFs In our LOP-CRF training procedure we first train the expert CRFs unregularised on the training data.
Then, treating the experts as static pre-trained models, we train the LOP-CRF weights w to maximise the log-likelihood of the training data.
This training process is "parameter-free" in that neither stage involves the use of a prior distribution over expert CRF parameters or LOP-CRF weights, and so avoids the requirement to adjust hyperparameter values.
The likelihood of a data set under a LOP-CRF, as a function of the LOP-CRF weights, is given by: L(w) = = This tells us that the closeness of the LOP model to q(s | o) is governed by a trade-off between two terms: an E term, which represents the closeness of the individual experts to q(s | o), and an A term, which represents the closeness of the individual experts to the LOP, and therefore indirectly to each other.
Hence for the LOP to model q well, we desire models p which are individually good models of q (having low E) and are also diverse (having large A).
3.1 LOPs
for CRFs Because CRFs are log-linear models, we can see from equation (2) that CRF experts are particularly well suited to combination under a LOP.
Indeed, the resulting LOP is itself a CRF, the LOP-CRF, with potential functions given by a log-linear combination of the potential functions of the experts, with weights w . As a consequence of this, the normalisation constant for the LOP-CRF can be calculated efficiently via the usual forward-backward algorithm for CRFs.
Note that there is a distinction between normalisation constant for the LOP-CRF, ZLOP as given in equation (3), and the partition function of the LOP-CRF, Z.
The two are related as follows: 1 [p (s | o)]w ZLOP (o) 1 U (s | o) Z (o) ZLOP (o) [U (s | o)] ZLOP (o) [Z (o)]w LOP 1 p (s | o)w ZLOP (o; w) After taking logs and rearranging, the loglikelihood can be expressed as: ~ p(o, s) w log p (s | o) ~ p(o) log Z LOP (o; w) T +1 where U = exp t=1 k k fk (st-1, st, o,t) and so For the first two terms, the quantities that are multiplied by w inside the (outer) sums are independent of the weights, and can be evaluated once at the fibeginning of training.
The third term involves the partition function for the LOP-CRF and so is a function of the weights.
It can be evaluated efficiently as usual for a standard CRF.
Taking derivatives with respect to w and rearranging, we obtain: use a prior distribution over the LOP-CRF weights.
As the weights are non-negative and normalised we use a Dirichlet distribution, whose density function is given by: p(w) = ( ) w -1 ( ) L (w) w where the are hyperparameters.
Under this distribution, ignoring terms that are independent of the weights, the regularised loglikelihood involves an additional term: logUt (o, s) where Ut (o, s) is the value of the potential function for expert on clique t under the labelling s for observation o.
In a way similar to the representation of the expected feature count in a standard CRF, the third term may be re-written as: pLOP (st-1 = s, st = s, o) logUt (s, s, o) o t s,s We assume a single value across all weights.
The derivative of the regularised log-likelihood with respect to weight w then involves an additional 1 term w ( 1).
In our experiments we use the development set to optimise the value of . We will refer to a LOP-CRF with weights trained using this procedure as a regularised LOP-CRF.
Hence the derivative is tractable because we can use dynamic programming to efficiently calculate the pairwise marginal distribution for the LOP-CRF.
Using these expressions we can efficiently train the LOP-CRF weights to maximise the loglikelihood of the data set.3 We make use of the LMVM method mentioned earlier to do this.
We will refer to a LOP-CRF with weights trained using this procedure as an unregularised LOP-CRF.
3.2.1 Regularisation
The "parameter-free" aspect of the training procedure we introduced in the previous section relies on the fact that we do not use regularisation when training the LOP-CRF weights w . However, there is a possibility that this may lead to overfitting of the training data.
In order to investigate this, we develop a regularised version of the training procedure and compare the results obtained with each.
We 3 We must ensure that the weights are non-negative and normalised.
We achieve this by parameterising the weights as functions of a set of unconstrained variables via a softmax transformation.
The values of the log-likelihood and its derivatives with respect to the unconstrained variables can be derived from the corresponding values for the weights w . The Tasks In this paper we apply LOP-CRFs to two sequence labelling tasks in NLP: named entity recognition (NER) and part-of-speech tagging (POS tagging).
4.1 Named
Entity Recognition NER involves the identification of the location and type of pre-defined entities within a sentence and is often used as a sub-process in information extraction systems.
With NER the CRF is presented with a set of sentences and must label each word so as to indicate whether the word appears outside an entity (O), at the beginning of an entity of type X (B-X) or within the continuation of an entity of type X (I-X).
All our results for NER are reported on the CoNLL-2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003).
For this dataset the entity types are: persons (PER), locations (LOC), organisations (ORG) and miscellaneous (MISC).
The training set consists of 14, 987 sentences and 204, 567 tokens, the development set consists of 3, 466 sentences and 51, 578 tokens and the test set consists of 3, 684 sentences and 46, 666 tokens.
fi4.2 Part-of-Speech Tagging POS tagging involves labelling each word in a sentence with its part-of-speech, for example noun, verb, adjective, etc.
For our experiments we use the CoNLL-2000 shared task dataset (Tjong Kim Sang and Buchholz, 2000).
This has 48 different POS tags.
In order to make training time manageable4, we collapse the number of POS tags from 48 to 5 following the procedure used in (McCallum et al., 2003).
In summary:  All types of noun collapse to category N.
 All types of verb collapse to category V.
 All types of adjective collapse to category J.
 All types of adverb collapse to category R.
 All other POS tags collapse to category O.
The training set consists of 7, 300 sentences and 173, 542 tokens, the development set consists of 1, 636 sentences and 38, 185 tokens and the test set consists of 2, 012 sentences and 47, 377 tokens.
4.3 Expert
sets For each task we compare the performance of the LOP-CRF to that of the standard CRF by defining a single, complex CRF, which we call a monolithic CRF, and a range of expert sets.
The monolithic CRF for NER comprises a number of word and POS tag features in a window of five words around the current word, along with a set of orthographic features defined on the current word.
These are based on those found in (Curran and Clark, 2003).
Examples include whether the current word is capitalised, is an initial, contains a digit, contains punctuation, etc.
The monolithic CRF for NER has 450, 345 features.
The monolithic CRF for POS tagging comprises word and POS features similar to those in the NER monolithic model, but over a smaller number of orthographic features.
The monolithic model for POS tagging has 188, 448 features.
Each of our expert sets consists of a number of CRF experts.
Usually these experts are designed to 4 See (Cohn et al., 2005) for a scaling method allowing the full POS tagging task with CRFs.
focus on modelling a particular aspect or subset of the distribution.
As we saw earlier, the aim here is to define experts that model parts of the distribution well while retaining mutual diversity.
The experts from a particular expert set are combined under a LOP-CRF and the weights are trained as described previously.
We define our range of expert sets as follows:  Simple consists of the monolithic CRF and a single expert comprising a reduced subset of the features in the monolithic CRF.
This reduced CRF models the entire distribution rather than focusing on a particular aspect or subset, but is much less expressive than the monolithic model.
The reduced model comprises 24, 818 features for NER and 47, 420 features for POS tagging.
 Positional consists of the monolithic CRF and a partition of the features in the monolithic CRF into three experts, each consisting only of features that involve events either behind, at or ahead of the current sequence position.
 Label consists of the monolithic CRF and a partition of the features in the monolithic CRF into five experts, one for each label.
For NER an expert corresponding to label X consists only of features that involve labels B-X or IX at the current or previous positions, while for POS tagging an expert corresponding to label X consists only of features that involve label X at the current or previous positions.
These experts therefore focus on trying to model the distribution of a particular label.
 Random consists of the monolithic CRF and a random partition of the features in the monolithic CRF into four experts.
This acts as a baseline to ascertain the performance that can be expected from an expert set that is not defined via any linguistic intuition.
Experiments To compare the performance of LOP-CRFs trained using the procedure we described previously to that of a standard CRF regularised with a Gaussian prior, we do the following for both NER and POS tagging: fi Train a monolithic CRF with regularisation using a Gaussian prior.
We use the development set to optimise the value of the variance hyperparameter.
 Train every expert CRF in each expert set without regularisation (each expert set includes the monolithic CRF, which clearly need only be trained once).
 For each expert set, create a LOP-CRF from the expert CRFs and train the weights of the LOP-CRF without regularisation.
We compare its performance to that of the unregularised and regularised monolithic CRFs.
 To investigate whether training the LOP-CRF weights contributes significantly to the LOPCRF's performance, for each expert set we create a LOP-CRF with uniform weights and compare its performance to that of the LOP-CRF with trained weights.
 To investigate whether unregularised training of the LOP-CRF weights leads to overfitting, for each expert set we train the weights of the LOP-CRF with regularisation using a Dirichlet prior.
We optimise the hyperparameter in the Dirichlet distribution on the development set.
We then compare the performance of the LOP-CRF with regularised weights to that of the LOP-CRF with unregularised weights.
Expert Monolithic unreg.
Monolithic reg.
Reduced Positional 1 Positional 2 Positional 3 Label LOC Label MISC Label ORG Label PER Label O Random 1 Random 2 Random 3 Random 4 Table 1: Development set F scores for NER experts 6.2 LOP-CRFs with unregularised weights In this section we present results for LOP-CRFs with unregularised weights.
Table 2 gives F scores for NER LOP-CRFs while Table 3 gives accuracies for the POS tagging LOP-CRFs.
The monolithic CRF scores are included for comparison.
Both tables illustrate the following points:  In every case the LOP-CRFs outperform the unregularised monolithic CRF  In most cases the performance of LOP-CRFs rivals that of the regularised monolithic CRF, and in some cases exceeds it.
We use McNemar's matched-pairs test (Gillick and Cox, 1989) on point-wise labelling errors to examine the statistical significance of these results.
We test significance at the 5% level.
At this threshold, all the LOP-CRFs significantly outperform the corresponding unregularised monolithic CRF.
In addition, those marked with show a significant improvement over the regularised monolithic CRF.
Only the value marked with in Table 3 significantly under performs the regularised monolithic.
All other values a do not differ significantly from those of the regularised monolithic CRF at the 5% level.
These results show that LOP-CRFs with unregularised weights can lead to performance improvements that equal or exceed those achieved from a conventional regularisation approach using a Gaussian prior.
The important difference, however, is that the LOP-CRF approach is "parameter-free" in the Results 6.1 Experts Before presenting results for the LOP-CRFs, we briefly give performance figures for the monolithic CRFs and expert CRFs in isolation.
For illustration, we do this for NER models only.
Table 1 shows F scores on the development set for the NER CRFs.
We see that, as expected, the expert CRFs in isolation model the data relatively poorly compared to the monolithic CRFs.
Some of the label experts, for example, attain relatively low F scores as they focus only on modelling one particular label.
Similar behaviour was observed for the POS tagging models.
Expert set Monolithic unreg.
Monolithic reg.
Simple Positional Label Random Development set 88.33 89.84 90.26 90.35 89.30 88.84 Test set 81.87 83.98 84.22 84.71 83.27 83.06 Expert set Simple Positional Label Random Development set 98.30 97.97 97.85 97.82 Test set 98.12 97.79 97.73 97.74 Table 2: F scores for NER unregularised LOP-CRFs Expert set Monolithic unreg.
Monolithic reg.
Simple Positional Label Random Development set 97.92 98.02 98.31 98.03 97.99 97.99 Test set 97.65 97.84 98.12 97.81 97.77 97.76 Table 4: Accuracies for POS tagging uniform LOPCRFs general LOP-CRFs with uniform weights, although still performing significantly better than the unregularised monolithic CRF, generally under perform LOP-CRFs with trained weights.
This suggests that the choice of weights can be important, and justifies the weight training stage.
6.4 LOP-CRFs with regularised weights To investigate whether unregularised training of the LOP-CRF weights leads to overfitting, we train the LOP-CRF with regularisation using a Dirichlet prior.
The results we obtain show that in most cases a LOP-CRF with regularised weights achieves an almost identical performance to that with unregularised weights, and suggests there is little to be gained by weight regularisation.
This is probably due to the fact that in our LOP-CRFs the number of experts, and therefore weights, is generally small and so there is little capacity for overfitting.
We conjecture that although other choices of expert set may comprise many more experts than in our examples, the numbers are likely to be relatively small in comparison to, for example, the number of parameters in the individual experts.
We therefore suggest that any overfitting effect is likely to be limited.
6.5 Choice
of Expert Sets We can see from Tables 2 and 3 that the performance of a LOP-CRF varies with the choice of expert set.
For example, in our tasks the simple and positional expert sets perform better than those for the label and random sets.
For an explanation here, we refer back to our discussion of equation (5).
We conjecture that the simple and positional expert sets achieve good performance in the LOP-CRF because they consist of experts that are diverse while simultaneously being reasonable models of the data.
The label expert set exhibits greater diversity between the experts, because each expert focuses on modelling a particular label only, but each expert is a relatively Table 3: Accuracies for POS tagging unregularised LOP-CRFs sense that each expert CRF in the LOP-CRF is unregularised and the LOP weight training is also unregularised.
We are therefore not required to search a hyperparameter space.
As an illustration, to obtain our best results for the POS tagging regularised monolithic model, we re-trained using 15 different values of the Gaussian prior variance.
With the LOP-CRF we trained each expert CRF and the LOP weights only once.
As an illustration of a typical weight distribution resulting from the training procedure, the positional LOP-CRF for POS tagging attaches weight 0.45 to the monolithic model and roughly equal weights to the other three experts.
6.3 LOP-CRFs with uniform weights By training LOP-CRF weights using the procedure we introduce in this paper, we allow the weights to take on non-uniform values.
This corresponds to letting the opinion of some experts take precedence over others in the LOP-CRF's decision making.
An alternative, simpler, approach would be to combine the experts under a LOP with uniform weights, thereby avoiding the weight training stage.
We would like to ascertain whether this approach will significantly reduce the LOP-CRF's performance.
As an illustration, Table 4 gives accuracies for LOPCRFs with uniform weights for POS tagging.
A similar pattern is observed for NER.
Comparing these values to those in Tables 2 and 3, we can see that in fipoor model of the entire distribution and the corresponding LOP-CRF performs worse.
Similarly, the random experts are in general better models of the entire distribution but tend to be less diverse because they do not focus on any one aspect or subset of it.
Intuitively, then, we want to devise experts that provide diverse but accurate views on the data.
The expert sets we present in this paper were motivated by linguistic intuition, but clearly many choices exist.
It remains an important open question as to how to automatically construct expert sets for good performance on a given task, and we intend to pursue this avenue in future research.
Conclusion and future work In this paper we have introduced the logarithmic opinion pool of CRFs as a way to address overfitting in CRF models.
Our results show that a LOPCRF can provide a competitive alternative to conventional regularisation with a prior while avoiding the requirement to search a hyperparameter space.
We have seen that, for a variety of types of expert, combination of expert CRFs with an unregularised standard CRF under a LOP with optimised weights can outperform the unregularised standard CRF and rival the performance of a regularised standard CRF.
We have shown how these advantages a LOPCRF provides have a firm theoretical foundation in terms of the decomposition of the KL-divergence between a LOP-CRF and a target distribution, and how this provides a framework for designing new overfitting reduction schemes in terms of constructing diverse experts.
In this work we have considered training the weights of a LOP-CRF using pre-trained, static experts.
In future we intend to investigate cooperative training of LOP-CRF weights and the parameters of each expert in an expert set.
Acknowledgements We wish to thank Stephen Clark, our colleagues in Edinburgh and the anonymous reviewers for many useful comments.
References R.
F. Bordley.
1982. A multiplicative formula for aggregating probability assessments.
Management Science, (28):1137 1148.
T. Cohn, A.
Smith, and M.
Osborne. 2005.
Scaling conditional random fields using error-correcting codes.
In Proc.
ACL 2005.
J. Curran and S.
Clark. 2003.
Language independent NER using a maximum entropy tagger.
In Proc.
CoNLL-2003. S.
Della Pietra, Della Pietra V., and J.
Lafferty. 1997.
Inducing features of random fields.
In IEEE PAMI, volume 19(4), pages 380393.
L. Gillick and S.
Cox. 1989.
Some statistical issues in the comparison of speech recognition algorithms.
In International Conference on Acoustics, Speech and Signal Processing, volume 1, pages 532535.
T. Heskes.
1998. Selecting weighting factors in logarithmic opinion pools.
In Advances in Neural Information Processing Systems 10.
G. E.
Hinton. 1999.
Product of experts.
In ICANN, volume 1, pages 16.
J. Lafferty, A.
McCallum, and F.
Pereira. 2001.
Conditional random fields: Probabilistic models for segmenting and labeling sequence data.
In Proc.
ICML 2001.
R. Malouf.
2002. A comparison of algorithms for maximum entropy parameter estimation.
In Proc.
CoNLL-2002. A.
McCallum and W.
Li. 2003.
Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.
In Proc.
CoNLL-2003. A.
McCallum, K.
Rohanimanesh, and C.
Sutton. 2003.
Dynamic conditional random fields for jointly labeling multiple sequences.
In NIPS-2003 Workshop on Syntax, Semantics and Statistics.
A. McCallum.
2003. Efficiently inducing features of conditional random fields.
In Proc.
UAI 2003.
M. Osborne and J.
Baldridge. 2004.
Ensemble-based active learning for parse selection.
In Proc.
NAACL 2004.
F. Peng and A.
McCallum. 2004.
Accurate information extraction from research papers using conditional random fields.
In Proc.
HLT-NAACL 2004.
Y. Qi, M.
Szummer, and T.
P. Minka.
2005. Bayesian conditional random fields.
In Proc.
AISTATS 2005.
F. Sha and F.
Pereira. 2003.
Shallow parsing with conditional random fields.
In Proc.
HLT-NAACL 2003.
E. F.
Tjong Kim Sang and S.
Buchholz. 2000.
Introduction to the CoNLL-2000 shared task: Chunking.
In Proc.
CoNLL2000. E.
F. Tjong Kim Sang and F.
De Meulder.
2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In Proc. CoNLL-2003 .
Supersense Tagging of Unknown Nouns using Semantic Similarity James R.
Curran School of Information Technologies University of Sydney NSW 2006, Australia james@it.usyd.edu.au Abstract The limited coverage of lexical-semantic resources is a significant problem for NLP systems which can be alleviated by automatically classifying the unknown words.
Supersense tagging assigns unknown nouns one of 26 broad semantic categories used by lexicographers to organise their manual insertion into WORDNET.
Ciaramita and Johnson (2003) present a tagger which uses synonym set glosses as annotated training examples.
We describe an unsupervised approach, based on vector-space similarity, which does not require annotated examples but significantly outperforms their tagger.
We also demonstrate the use of an extremely large shallow-parsed corpus for calculating vector-space semantic similarity.
WORDNET with the UMLS medical resource and found only a very small degree of overlap.
Also, lexicalsemantic resources suffer from: bias towards concepts and senses from particular topics.
Some specialist topics are better covered in WORDNET than others, e.g. dog has finer-grained distinctions than cat and worm although this does not reflect finer distinctions in reality; limited coverage of infrequent words and senses.
Ciaramita and Johnson (2003) found that common nouns missing from WORDNET 1.6 occurred every 8 sentences in the BLLIP corpus.
By WORDNET 2.0, coverage has improved but the problem of keeping up with language evolution remains difficult.
consistency when classifying similar words into categories.
For instance, the WORDNET lexicographer file for ionosphere (location) is different to exosphere and stratosphere (object), two other layers of the earth's atmosphere.
These problems demonstrate the need for automatic or semi-automatic methods for the creation and maintenance of lexical-semantic resources.
Broad semantic classification is currently used by lexicographers to organise the manual insertion of words into WORDNET, and is an experimental precursor to automatically inserting words directly into the WORDNET hierarchy.
Ciaramita and Johnson (2003) call this supersense tagging and describe a multi-class perceptron tagger, which uses WORDNET's hierarchical structure to create many annotated training instances from the synset glosses.
This paper describes an unsupervised approach to supersense tagging that does not require annotated sentences.
Instead, we use vector-space similarity to retrieve a number of synonyms for each unknown common noun.
The supersenses of these synonyms are then combined to determine the supersense.
This approach significantly outperforms the multi-class perceptron on the same dataset based on WORDNET 1.6 and 1.7.1. 1 Introduction Lexical-semantic resources have been applied successful to a wide range of Natural Language Processing (NLP) problems ranging from collocation extraction (Pearce, 2001) and class-based smoothing (Clark and Weir, 2002), to text classification (Baker and McCallum, 1998) and question answering (Pasca and Harabagiu, 2001).
In particular, WORDNET (Fellbaum, 1998) has significantly influenced research in NLP.
Unfortunately, these resource are extremely timeconsuming and labour-intensive to manually develop and maintain, requiring considerable linguistic and domain expertise.
Lexicographers cannot possibly keep pace with language evolution: sense distinctions are continually made and merged, words are coined or become obsolete, and technical terms migrate into the vernacular.
Technical domains, such as medicine, require separate treatment since common words often take on special meanings, and a significant proportion of their vocabulary does not overlap with everyday vocabulary.
Burgun and Bodenreider (2001) compared an alignment of Proceedings of the 43rd Annual Meeting of the ACL, pages 2633, Ann Arbor, June 2005.
c 2005 Association for Computational Linguistics fiLEX-FILE DESCRIPTION act animal artifact attribute body cognition communication event feeling food group location motive object person phenomenon plant possession process quantity relation shape state substance time acts or actions animals man-made objects attributes of people and objects body parts cognitive processes and contents communicative processes and contents natural events feelings and emotions foods and drinks groupings of people or objects spatial position goals natural objects (not man-made) people natural phenomena plants possession and transfer of possession natural processes quantities and units of measure relations between people/things/ideas two and three dimensional shapes stable states of affairs substances time and temporal relations ing directly underneath.
Other alternative sets of supersenses can be created by an arbitrary cut through the WORDNET hierarchy near the top, or by using topics from a thesaurus such as Roget's (Yarowsky, 1992).
These topic distinctions are coarser-grained than WORDNET senses, which have been criticised for being too difficult to distinguish even for experts.
Ciaramita and Johnson (2003) believe that the key sense distinctions are still maintained by supersenses.
They suggest that supersense tagging is similar to named entity recognition, which also has a very small set of categories with similar granularity (e.g.
location and person) for labelling predominantly unseen terms.
Supersense tagging can provide automated or semiautomated assistance to lexicographers adding words to the WORDNET hierarchy.
Once this task is solved successfully, it may be possible to insert words directly into the fine-grained distinctions of the hierarchy itself.
Clearly, this is the ultimate goal, to be able to insert new terms into lexical resources, extending the structure where necessary.
Supersense tagging is also interesting for many applications that use shallow semantics, e.g. information extraction and question answering.
Table 1: 25 noun lexicographer files in WORDNET Previous Work Supersenses There are 26 broad semantic classes employed by lexicographers in the initial phase of inserting words into the WORDNET hierarchy, called lexicographer files (lexfiles).
For the noun hierarchy, there are 25 lex-files and a file containing the top level nodes in the hierarchy called Tops.
Other syntactic classes are also organised using lex-files: 15 for verbs, 3 for adjectives and 1 for adverbs.
Lex-files form a set of coarse-grained sense distinctions within WORDNET.
For example, company appears in the following lex-files in WORDNET 2.0: group, which covers company in the social, commercial and troupe fine-grained senses; and state, which covers companionship.
The names and descriptions of the noun lex-files are shown in Table 1.
Some lex-files map directly to the top level nodes in the hierarchy, called unique beginners, while others are grouped together as hyponyms of a unique beginner (Fellbaum, 1998, page 30).
For example, abstraction subsumes the lex-files attribute, quantity, relation, communication and time.
Ciaramita and Johnson (2003) call the noun lex-file classes supersenses.
There are 11 unique beginners in the WORDNET noun hierarchy which could also be used as supersenses.
Ciaramita (2002) has produced a miniWORDNET by manually reducing the WORDNET hierarchy to 106 broad categories.
Ciaramita et al.(2003) describe how the lex-files can be used as root nodes in a two level hierarchy with the WORDNET synsets appearA considerable amount of research addresses structurally and statistically manipulating the hierarchy of WORDNET and the construction of new wordnets using the concept structure from English.
For lexical FreeNet, Beeferman (1998) adds over 350 000 collocation pairs (trigger pairs) extracted from a 160 million word corpus of broadcast news using mutual information.
The co-occurrence window was 500 words which was designed to approximate average document length.
Caraballo and Charniak (1999) have explored determining noun specificity from raw text.
They find that simple frequency counts are the most effective way of determining the parent-child ordering, achieving 83% accuracy over types of vehicle, food and occupation.
The other measure they found to be successful was the entropy of the conditional distribution of surrounding words given the noun.
Specificity ordering is a necessary step for building a noun hierarchy.
However, this approach clearly cannot build a hierarchy alone.
For instance, entity is less frequent than many concepts it subsumes.
This suggests it will only be possible to add words to an existing abstract structure rather than create categories right up to the unique beginners.
Hearst and Sch tze (1993) flatten WORDNET into 726 u categories using an algorithm which attempts to minimise the variance in category size.
These categories are used to label paragraphs with topics, effectively repeating Yarowsky's (1992) experiments using the their categories rather than Roget's thesaurus.
Sch tze's (1992) u WordSpace system was used to add topical links, such as between ball, racquet and game (the tennis problem).
Further, they also use the same vector-space techniques to label previously unseen words using the most common class assigned to the top 20 synonyms for that word.
Widdows (2003) uses a similar technique to insert words into the WORDNET hierarchy.
He first extracts synonyms for the unknown word using vector-space similarity measures based on Latent Semantic Analysis and then searches for a location in the hierarchy nearest to these synonyms.
This same technique as is used in our approach to supersense tagging.
Ciaramita and Johnson (2003) implement a supersense tagger based on the multi-class perceptron classifier (Crammer and Singer, 2001), which uses the standard collocation, spelling and syntactic features common in WSD and named entity recognition systems.
Their insight was to use the WORDNET glosses as annotated training data and massively increase the number of training instances using the noun hierarchy.
They developed an efficient algorithm for estimating the model over hierarchical training data.
WORDNET 1.6 NOUN SUPERSENSE WORDNET 1.7.1 NOUN SUPERSENSE stock index fast food bottler subcompact advancer cash flow downside discounter trade-off billionaire communication food group artifact person possession cognition artifact act person week buyout insurer partner health income contender cartel lender planner time act group person state possession person group person artifact Table 2: Example nouns and their supersenses largest NLP processed corpus described in published research.
The corpus consists of the British National Corpus (BNC), the Reuters Corpus Volume 1 (RCV1), and most of the Linguistic Data Consortium's news text collected since 1987: Continuous Speech Recognition III (CSR-III); North American News Text Corpus (NANTC); the NANTC Supplement (NANTS); and the ACQUAINT Corpus.
The components and their sizes including punctuation are given in Table 3.
The LDC has recently released the English Gigaword corpus which includes most of the corpora listed above.
CORPUS BNC RCV1 CSR-III NANTC NANTS ACQUAINT DOCS.
SENTS. WORDS Evaluation Ciaramita and Johnson (2003) propose a very natural evaluation for supersense tagging: inserting the extra common nouns that have been added to a new version of WORDNET.
They use the common nouns that have been added to WORDNET 1.7.1 since WORDNET 1.6 and compare this evaluation with a standard cross-validation approach that uses a small percentage of the words from their WORDNET 1.6 training set for evaluation.
Their results suggest that the WORDNET 1.7.1 test set is significantly harder because of the large number of abstract category nouns, e.g. communication and cognition, that appear in the 1.7.1 data, which are difficult to classify.
Our evaluation will use exactly the same test sets as Ciaramita and Johnson (2003).
The WORDNET 1.7.1 test set consists of 744 previously unseen nouns, the majority of which (over 90%) have only one sense.
The WORDNET 1.6 test set consists of several cross-validation sets of 755 nouns randomly selected from the BLLIP training set used by Ciaramita and Johnson (2003).
They have kindly supplied us with the WORDNET 1.7.1 test set and one cross-validation run of the WORDNET 1.6 test set.
Our development experiments are performed on the WORDNET 1.6 test set with one final run on the WORDNET 1.7.1 test set.
Some examples from the test sets are given in Table 2 with their supersenses.
Table 3: 2 billion word corpus statistics We have tokenized the text using the Grok-OpenNLP tokenizer (Morton, 2002) and split the sentences using MXTerminator (Reynar and Ratnaparkhi, 1997).
Any sentences less than 3 words or more than 100 words long were rejected, along with sentences containing more than 5 numbers or more than 4 brackets, to reduce noise.
The rest of the pipeline is described in the next section.
Semantic Similarity Corpus We have developed a 2 billion word corpus, shallowparsed with a statistical NLP pipeline, which is by far the Vector-space models of similarity are based on the distributional hypothesis that similar words appear in similar contexts.
This hypothesis suggests that semantic similarity can be measured by comparing the contexts each word appears in.
In vector-space models each headword is represented by a vector of frequency counts recording the contexts that it appears in.
The key parameters are the context extraction method and the similarity measure used to compare context vectors.
Our approach to fivector-space similarity is based on the SEXTANT system described in Grefenstette (1994).
Curran and Moens (2002b) compared several context extraction methods and found that the shallow pipeline and grammatical relation extraction used in SEXTANT was both extremely fast and produced high-quality results.
SEXTANT extracts relation tuples (w, r, w ) for each noun, where w is the headword, r is the relation type and w is the other word.
The efficiency of the SEXTANT approach makes the extraction of contextual information from over 2 billion words of raw text feasible.
We describe the shallow pipeline in detail below.
Curran and Moens (2002a) compared several different similarity measures and found that Grefenstette's weighted JACCARD measure performed the best: min(wgt(w1, r, w ), wgt(w2, r, w )) max(wgt(w1, r, w ), wgt(w2, r, w )) (1) RELATION DESCRIPTION adj dobj iobj nn nnprep subj Table 4: Grammatical relations from SEXTANT against the CELEX lexical database (Minnen et al., 2001)  and is very efficient, analysing over 80 000 words per second.
morpha often maintains sense distinctions between singular and plural nouns; for instance: spectacles is not reduced to spectacle, but fails to do so in other cases: glasses is converted to glass.
This inconsistency is problematic when using morphological analysis to smooth vector-space models.
However, morphological smoothing still produces better results in practice.
6.3 Grammatical
Relation Extraction where wgt(w, r, w ) is the weight function for relation (w, r, w ).
Curran and Moens (2002a) introduced the TTEST weight function, which is used in collocation extraction.
Here, the t-test compares the joint and product probability distributions of the headword and context: p(w, r, w ) p(, r, w )p(w,, ) p(, r, w )p(w,, ) (2) where indicates a global sum over that element of the relation tuple.
JACCARD and TTEST produced better quality synonyms than existing measures in the literature, so we use Curran and Moen's configuration for our supersense tagging experiments.
6.1 Part
of Speech Tagging and Chunking Our implementation of SEXTANT uses a maximum entropy POS tagger designed to be very efficient, tagging at around 100 000 words per second (Curran and Clark, 2003), trained on the entire Penn Treebank (Marcus et al., 1994).
The only similar performing tool is the Trigrams `n' Tags tagger (Brants, 2000) which uses a much simpler statistical model.
Our implementation uses a maximum entropy chunker which has similar feature types to Koeling (2000) and is also trained on chunks extracted from the entire Penn Treebank using the CoNLL 2000 script.
Since the Penn Treebank separates PPs and conjunctions from NPs, they are concatenated to match Grefenstette's table-based results, i.e. the SEXTANT always prefers noun attachment.
6.2 Morphological
Analysis Our implementation uses morpha, the Sussex morphological analyser (Minnen et al., 2001), which is implemented using lex grammars for both affix splitting and generation.
morpha has wide coverage  nearly 100% After the raw text has been POS tagged and chunked, the grammatical relation extraction algorithm is run over the chunks.
This consists of five passes over each sentence that first identify noun and verb phrase heads and then collect grammatical relations between each common noun and its modifiers and verbs.
A global list of grammatical relations generated by each pass is maintained across the passes.
The global list is used to determine if a word is already attached.
Once all five passes have been completed this association list contains all of the nounmodifier/verb pairs which have been extracted from the sentence.
The types of grammatical relation extracted by SEXTANT are shown in Table 4.
For relations between nouns (nn and nnprep), we also create inverse relations (w, r, w) representing the fact that w can modify w.
The 5 passes are described below.
Pass 1: Noun Pre-modifiers This pass scans NPs, left to right, creating adjectival (adj) and nominal (nn) pre-modifier grammatical relations (GRs) with every noun to the pre-modifier's right, up to a preposition or the phrase end.
This corresponds to assuming right-branching noun compounds.
Within each NP only the NP and PP heads remain unattached.
Pass 2: Noun Post-modifiers This pass scans NPs, right to left, creating post-modifier GRs between the unattached heads of NPs and PPs.
If a preposition is encountered between the noun heads, a prepositional noun (nnprep) GR is created, otherwise an appositional noun (nn) GR is created.
This corresponds to assuming right-branching PP attachment.
After this phrase only the NP head remains unattached.
Tense Determination The rightmost verb in each VP is considered the head.
A fiis initially categorised as active.
If the head verb is a form of be then the VP becomes attributive.
Otherwise, the algorithm scans the VP from right to left: if an auxiliary verb form of be is encountered the VP becomes passive; if a progressive verb (except being) is encountered the VP becomes active.
Only the noun heads on either side of VPs remain unattached.
The remaining three passes attach these to the verb heads as either subjects or objects depending on the voice of the VP.
Pass 3: Verb Pre-Attachment This pass scans sentences, right to left, associating the first NP head to the left of the VP with its head.
If the VP is active, a subject (subj) relation is created; otherwise, a direct object (dobj) relation is created.
For example, antigen is the subject of represent.
Pass 4: Verb Post-Attachment This pass scans sentences, left to right, associating the first NP or PP head to the right of the VP with its head.
If the VP was classed as active and the phrase is an NP then a direct object (dobj) relation is created.
If the VP was classed as passive and the phrase is an NP then a subject (subj) relation is created.
If the following phrase is a PP then an indirect object (iobj) relation is created.
The interaction between the head verb and the preposition determine whether the noun is an indirect object of a ditransitive verb or alternatively the head of a PP that is modifying the verb.
However, SEXTANT always attaches the PP to the previous phrase.
Pass 5: Verb Progressive Participles The final step of the process is to attach progressive verbs to subjects and objects (without concern for whether they are already attached).
Progressive verbs can function as nouns, verbs and adjectives and once again a nave api proximation to the correct attachment is made.
Any progressive verb which appears after a determiner or quantifier is considered a noun.
Otherwise, it is a verb and passes 3 and 4 are repeated to attach subjects and objects.
Finally, SEXTANT collapses the nn, nnprep and adj relations together into a single broad noun-modifier grammatical relation.
Grefenstette (1994) claims this extractor has a grammatical relation accuracy of 75% after manually checking 60 sentences.
VP SUFFIX EXAMPLE SUPERSENSE remoteness annulment statesman bowling viscosity electronics arsine mariner entomology attribute act person act attribute cognition substance person cognition Table 5: Hand-coded rules for supersense guessing fall-back method is a simple hand-coded classifier which examines the unknown noun and makes a guess based on simple morphological analysis of the suffix.
These rules were created by inspecting the suffixes of rare nouns in WORDNET 1.6.
The supersense guessing rules are given in Table 5.
If none of the rules match, then the default supersense artifact is assigned.
The problem now becomes how to convert the ranked list of extracted synonyms for each unknown noun into a single supersense selection.
Each extracted synonym votes for its one or more supersenses that appear in WORDNET 1.6.
There are many parameters to consider:     how many extracted synonyms to use; how to weight each synonym's vote; whether unreliable synonyms should be filtered out; how to deal with polysemous synonyms.
Approach Our approach uses voting across the known supersenses of automatically extracted synonyms, to select a supersense for the unknown nouns.
This technique is similar to Hearst and Sch tze (1993) and Widdows (2003).
u However, sometimes the unknown noun does not appear in our 2 billion word corpus, or at least does not appear frequently enough to provide sufficient contextual information to extract reliable synonyms.
In these cases, our The experiments described below consider a range of options for these parameters.
In fact, these experiments are so quick to run we have been able to exhaustively test many combinations of these parameters.
We have experimented with up to 200 voting extracted synonyms.
There are several ways to weight each synonym's contribution.
The simplest approach would be to give each synonym the same weight.
Another approach is to use the scores returned by the similarity system.
Alternatively, the weights can use the ranking of the extracted synonyms.
Again these options have been considered below.
A related question is whether to use all of the extracted synonyms, or perhaps filter out synonyms for which a small amount of contextual information has been extracted, and so might be unreliable.
The final issue is how to deal with polysemy.
Does every supersense of each extracted synonym get the whole weight of that synonym or is it distributed evenly between the supersenses like Resnik (1995)?
Another alternative is to only consider unambiguous synonyms with a single supersense in WORDNET.
A disadvantage of this similarity approach is that it requires full synonym extraction, which compares the unknown word against a large number of words when, in fiSYSTEM WN Ciaramita and Johnson baseline Ciaramita and Johnson perceptron Similarity based results WN WORDNET 1.6 SUPERSENSE N P R F WORDNET 1.7.1 N P R F Table 6: Summary of supersense tagging accuracies fact, we want to calculate the similarity to a small number of supersenses.
This inefficiency could be reduced significantly if we consider only very high frequency words, but even this is still expensive.
Results We have used the WORDNET 1.6 test set to experiment with different parameter settings and have kept the WORDNET 1.7.1 test set as a final comparison of best results with Ciaramita and Johnson (2003).
The experiments were performed by considering all possible configurations of the parameters described above.
The following voting options were considered for each supersense of each extracted synonym: the initial voting weight for a supersense could either be a constant (IDENTITY) or the similarity score (SCORE) of the synonym.
The initial weight could then be divided by the number of supersenses to share out the weight (SHARED).
The weight could also be divided by the rank (RANK) to penalise supersenses further down the list.
The best performance on the 1.6 test set was achieved with the SCORE voting, without sharing or ranking penalties.
The extracted synonyms are filtered before contributing to the vote with their supersense(s).
This filtering involves checking that the synonym's frequency and number of contexts are large enough to ensure it is reliable.
We have experimented with a wide range of cutoffs and the best performance on the 1.6 test set was achieved using a minimum cutoff of 5 for the synonym's frequency and the number of contexts it appears in.
The next question is how many synonyms are considered.
We considered using just the nearest unambiguous synonym, and the top 5, 10, 20, 50, 100 and 200 synonyms.
All of the top performing configurations used 50 synonyms.
We have also experimented with filtering out highly polysemous nouns by eliminating words with two, three or more synonyms.
However, such a filter turned out to make little difference.
Finally, we need to decide when to use the similarity measure and when to fall-back to the guessing rules.
This is determined by looking at the frequency and number of attributes for the unknown word.
Not surprisingly, the similarity system works better than the guessing rules if it has any information at all.
The results are summarised in Table 6.
The accuracy of the best-performing configurations was 68% on the Table 7: Breakdown of results by supersense WORDNET 1.6 test set with several other parameter combinations described above performing nearly as well.
On the previously unused WORDNET 1.7.1 test set, our accuracy is 63% using the best system on the WORDNET 1.6 test set.
By optimising the parameters on the 1.7.1 test set we can increase that to 64%, indicating that we have not excessively over-tuned on the 1.6 test set.
Our results significantly outperform Ciaramita and Johnson (2003) on both test sets even though our system is unsupervised.
The large difference between our 1.6 and 1.7.1 test set accuracy demonstrates that the 1.7.1 set is much harder.
Table 7 shows the breakdown in performance for each supersense.
The columns show the number of instances of each supersense with the precision, recall and f-score measures as percentages.
The most frequent supersenses in both test sets were person, attribute and act.
Of the frequent categories, person is the easiest supersense to get correct in both the 1.6 and 1.7.1 test sets, followed by food, artifact and substance.
This is not surprising since these concrete words tend to have very fewer other senses, well constrained contexts and a relatively high frequency.
These factors are conducive for extracting reliable synonyms.
These results also support Ciaramita and Johnson's view that abstract concepts like communication, cognition and state are much harder.
We would expect the location fisupersense to perform well since it is quite concrete, but unfortunately our synonym extraction system does not incorporate proper nouns, so many of these words were classified using the hand-built classifier.
Also, in the data from Ciaramita and Johnson all of the words are in lower case, so no sensible guessing rules could help.
Other Alternatives and Future Work An alternative approach worth exploring is to create context vectors for the supersense categories themselves and compare these against the words.
This has the advantage of producing a much smaller number of vectors to compare against.
In the current system, we must compare a word against the entire vocabulary (over 500 000 headwords), which is much less efficient than a comparison against only 26 supersense context vectors.
The question now becomes how to construct vectors of supersenses.
The most obvious solution is to sum the context vectors across the words which have each supersense.
However, our early experiments suggest that this produces extremely large vectors which do not match well against the much smaller vectors of each unseen word.
Also, the same questions arise in the construction of these vectors.
How are words with multiple supersenses handled?
Our preliminary experiments suggest that only combining the vectors for unambiguous words produces the best results.
One solution would be to take the intersection between vectors across words for each supersense (i.e.
to find the common contexts that these words appear in).
However, given the sparseness of the data this may not leave very large context vectors.
A final solution would be to consider a large set of the canonical attributes (Curran and Moens, 2002a) to represent each supersense.
Canonical attributes summarise the key contexts for each headword and are used to improve the efficiency of the similarity comparisons.
There are a number of problems our system does not currently handle.
Firstly, we do not include proper names in our similarity system which means that location entities can be very difficult to identify correctly (as the results demonstrate).
Further, our similarity system does not currently incorporate multi-word terms.
We overcome this by using the synonyms of the last word in the multi-word term.
However, there are 174 multi-word terms (23%) in the WORDNET 1.7.1 test set which we could probably tag more accurately with synonyms for the whole multi-word term.
Finally, we plan to implement a supervised machine learner to replace the fallback method, which currently has an accuracy of 37% on the WORDNET 1.7.1 test set.
We intend to extend our experiments beyond the Ciaramita and Johnson (2003) set to include previous and more recent versions of WORDNET to compare their difficulty, and also perform experiments over a range of corpus sizes to determine the impact of corpus size on the quality of results.
We would like to move onto the more difficult task of insertion into the hierarchy itself and compare against the initial work by Widdows (2003) using latent semantic analysis.
Here the issue of how to combine vectors is even more interesting since there is the additional structure of the WORDNET inheritance hierarchy and the small synonym sets that can be used for more fine-grained combination of vectors.
Conclusion Our application of semantic similarity to supersense tagging follows earlier work by Hearst and Sch tze (1993) u and Widdows (2003).
To classify a previously unseen common noun our approach extracts synonyms which vote using their supersenses in WORDNET 1.6.
We have experimented with several parameters finding that the best configuration uses 50 extracted synonyms, filtered by frequency and number of contexts to increase their reliability.
Each synonym votes for each of its supersenses from WORDNET 1.6 using the similarity score from our synonym extractor.
Using this approach we have significantly outperformed the supervised multi-class perceptron Ciaramita and Johnson (2003).
This paper also demonstrates the use of a very efficient shallow NLP pipeline to process a massive corpus.
Such a corpus is needed to acquire reliable contextual information for the often very rare nouns we are attempting to supersense tag.
This application of semantic similarity demonstrates that an unsupervised methods can outperform supervised methods for some NLP tasks if enough data is available.
Acknowledgements We would like to thank Massi Ciaramita for supplying his original data for these experiments and answering our queries, and to Stephen Clark and the anonymous reviewers for their helpful feedback and corrections.
This work has been supported by a Commonwealth scholarship, Sydney University Travelling Scholarship and Australian Research Council Discovery Project DP0453131.
References L.
Douglas Baker and Andrew McCallum.
1998. Distributional clustering of words for text classification.
In Proceedings of the 21st annual international ACM SIGIR conference on Research and Development in Information Retrieval, pages 96103, Melbourne, Australia.
Doug Beeferman.
1998. Lexical discovery with an enriched semantic network.
In Proceedings of the Workshop on Usage of WordNet in Natural Language Processing Systems, pages 358364, Montreal, Quebec, Canada.
Thorsten Brants.
2000. TnT a statistical part-of-speech tagger.
In Proceedings of the 6th Applied Natural Language Processing Conference, pages 224231, Seattle, WA USA.
Anita Burgun and Olivier Bodenreider.
2001. Comparing terms, concepts and semantic classes in WordNet and the Unified Medical Language System.
In Proceedings of the Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, pages 7782, Pittsburgh, PA USA.
Sharon A.
Caraballo and Eugene Charniak.
1999. Determining the specificity of nouns from text.
In Proceedings of the Joint ACL SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 6370, College Park, MD USA.
Massimiliano Ciaramita and Mark Johnson.
2003. Supersense tagging of unknown nouns in WordNet.
In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, pages 168175, Sapporo, Japan.
Massimiliano Ciaramita, Thomas Hofmann, and Mark Johnson.
2003. Hierarchical semantic classification: Word sense disambiguation with world knowledge.
In Proceedings of the 18th International Joint Conference on Artificial Intelligence, Acapulco, Mexico.
Massimiliano Ciaramita.
2002. Boosting automatic lexical acquisition with morphological information.
In Proceedings of the Workshop on Unsupervised Lexical Acquisition, pages 1725, Philadelphia, PA, USA.
Stephen Clark and David Weir.
2002. Class-based probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187206, June.
Koby Crammer and Yoram Singer.
2001. Ultraconservative online algorithms for multiclass problems.
In Proceedings of the 14th annual Conference on Computational Learning Theory and 5th European Conference on Computational Learning Theory, pages 99115, Amsterdam, The Netherlands.
James R.
Curran and Stephen Clark.
2003. Investigating GIS and smoothing for maximum entropy taggers.
In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics, pages 9198, Budapest, Hungary.
James R.
Curran and Marc Moens.
2002a. Improvements in automatic thesaurus extraction.
In Proceedings of the Workshop on Unsupervised Lexical Acquisition, pages 5966, Philadelphia, PA, USA.
James R.
Curran and Marc Moens.
2002b. Scaling context space.
In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 231238, Philadelphia, PA, USA.
Christiane Fellbaum, editor.
1998. WordNet: An Electronic Lexical Database.
MIT Press, Cambridge, MA USA.
Gregory Grefenstette.
1994. Explorations in Automatic Thesaurus Discovery.
Kluwer Academic Publishers, Boston, MA USA.
Marti A.
Hearst and Hinrich Schutze.
1993. Customizing a lexicon to better suit a computational task.
In Proceedings of the Workshop on Acquisition of Lexical Knowledge from Text, pages 5569, Columbus, OH USA.
Rob Koeling.
2000. Chunking with maximum entropy models.
In Proceedings of the 4th Conference on Computational Natural Language Learning and of the 2nd Learning Language in Logic Workshop, pages 139141, Lisbon, Portugal.
Mitchell P.
Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
1993. Building a large annotated corpus of English: the Penn Treebank.
Computational Linguistics, 19(2):313330.
Guido Minnen, John Carroll, and Darren Pearce.
2001. Applied morphological processing of English.
Natural Language Engineering, 7(3):207223.
Tom Morton.
2002. Grok tokenizer.
Grok OpenNLP toolkit.
Marius Pasca and Sanda M.
Harabagiu. 2001.
The informative role of WordNet in open-domain question answering.
In Proceedings of the Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, pages 138143, Pittsburgh, PA USA.
Darren Pearce.
2001. Synonymy in collocation extraction.
In Proceedings of the Workshop on WordNet and Other Lexical Resources: Applications, Extensions and Customizations, pages 4146, Pittsburgh, PA USA.
Philip Resnik.
1995. Using information content to evaluate semantic similarity.
In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 448453, Montreal, Canada.
Jeffrey C.
Reynar and Adwait Ratnaparkhi.
1997. A maximum entropy approach to identifying sentence boundaries.
In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 1619, Washington, D.C.
USA. Hinrich Schutze.
1992. Context space.
In Intelligent Probau bilistic Approaches to Natural Language, number FS-92-04 in Fall Symposium Series, pages 113120, Stanford University, CA USA.
Dominic Widdows.
2003. Unsupervised methods for developing taxonomies by combining syntactic and statistical information.
In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 276283, Edmonton, Alberta Canada.
David Yarowsky.
1992. Word-sense disambiguation using statistical models of Roget's categories trained on large corpora.
In Proceedings of the 14th international conference on Computational Linguistics, pages 454460, Nantes, France.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 121??24, Prague, June 2007.
c2007 Association for Computational Linguistics Predicting Evidence of Understanding by Monitoring User?s Task Manipulation in Multimodal Conversations Yukiko I.
Nakano ??
Yoshiko Arimoto ??
?? Tokyo University of Agriculture and Technology 2-24-16 Nakacho, Koganeishi, Tokyo 184-8588, Japan {nakano, kmurata, menomoto}@cc.tuat.ac.jp Kazuyoshi Murata ??
Yasuhiro Asa ??
?? Tokyo University of Technology 1404-1 Katakura, Hachioji, Tokyo 192-0981, Japan ar@mf.teu.ac.jp Mika Enomoto ??
Hirohiko Sagawa ??
?? Central Research Laboratory, Hitachi, Ltd.
1-280, Higashi-koigakubo Kokubunji-shi, Tokyo 185-8601, Japan {yasuhiro.asa.mk, hirohiko.sagawa.cu}@hitachi.com Abstract The aim of this paper is to develop animated agents that can control multimodal instruction dialogues by monitoring user?s behaviors.
First, this paper reports on our Wizard-of-Oz experiments, and then, using the collected corpus, proposes a probabilistic model of fine-grained timing dependencies among multimodal communication behaviors: speech, gestures, and mouse manipulations.
A preliminary evaluation revealed that our model can predict a instructor?s grounding judgment and a listener?s successful mouse manipulation quite accurately, suggesting that the model is useful in estimating the user?s understanding, and can be applied to determining the agent?s next action.
1 Introduction
In face-to-face conversation, speakers adjust their utterances in progress according to the listener?s feedback expressed in multimodal manners, such as speech, facial expression, and eye-gaze.
In taskmanipulation situations where the listener manipulates objects by following the speaker?s instructions, correct task manipulation by the listener serves as more direct evidence of understanding (Brennan 2000, Clark and Krych 2004), and affects the speaker?s dialogue control strategies.
Figure 1 shows an example of a software instruction dialogue in a video-mediated situation (originally in Japanese).
While the learner says nothing, the instructor gives the instruction in small pieces, simultaneously modifying her gestures and utterances according to the learner?s mouse movements.
To accomplish such interaction between human users and animated help agents, and to assist the user through natural conversational interaction, this paper proposes a probabilistic model that computes timing dependencies among different types of behaviors in different modalities: speech, gestures, and mouse events.
The model predicts (a) whether the instructor?s current utterance will be successfully understood by the learner and grounded (Clark and Schaefer 1989), and (b) whether the learner will successfully manipulate the object in the near future.
These predictions can be used as constraints in determining agent actions.
For example, if the current utterance will not be grounded, then the help agent must add more information.
In the following sections, first, we collect human-agent conversations by employing a Wizardof-Oz method, and annotate verbal and nonverbal behaviors.
The annotated corpus is used to build a Bayesian network model for the multimodal instruction dialogues.
Finally, we will evaluate how ?That??(204ms pause) Pointing gesture <preparation> <stroke> Mouse move Instructor: Learner: ?at the most??(395ms pause) ?left-hand side??
Instructor: Learner: Instructor: Mouse move Figure 1: Example of task manipulation dialogue 121 accurately the model can predict the events in (a) and (b) mentioned above.
2 Related
work In their psychological study, Clark and Krych (2004) showed that speakers alter their utterances midcourse while monitoring not only the listener?s vocal signals, but also the listener?s gestural signals as well as through other mutually visible events.
Such a bilateral process functions as a joint activity to ground the presented information, and task manipulation as a mutually visible event contributes to the grounding process (Brennan 2000, Whittaker 2003).
Dillenbourg, Traum, et al.(1996) also discussed cross-modality in grounding: verbally presented information is grounded by an action in the task environment.
Studies on interface agents have presented computational models of multimodal interaction (Cassell, Bickmore, et al.2000). Paek and Horvitz (1999) focused on uncertainty in speech-based interaction, and employed a Bayesian network to understand the user?s speech input.
For user monitoring, Nakano, Reinstein, et al.(2003) used a head tracker to build a conversational agent which can monitor the user?s eye-gaze and head nods as nonverbal signals in grounding.
These previous studies provide psychological evidence about the speaker?s monitoring behaviors as well as conversation modeling techniques in computational linguistics.
However, little has been studied about how systems (agents) should monitor the user?s task manipulation, which gives direct evidence of understanding to estimate the user?s understanding, and exploits the predicted evidence as constraints in selecting the agent?s next action.
Based on these previous attempts, this study proposes a multimodal interaction model by focusing on task manipulation, and predicts conversation states using probabilistic reasoning.
3 Data
collection A data collection experiment was conducted using a Wizard-of-Oz agent assisting a user in learning a PCTV application, a system for watching and recording TV programs on a PC.
The output of the PC operated by the user was displayed on a 23-inch monitor in front of the user, and also projected on a 120-inch big screen, in front of which a human instructor was standing (Figure 2 (a)).
Therefore, the participants shared visual events output from the PC (Figure 2 (b)) while sitting in different rooms.
In addition, a rabbit-like animated agent was controlled through the instructor?s motion data captured by motion sensors.
The instructor?s voice was changed through a voice transformation system to make it sound like a rabbit agent.
4 Corpus
We collected 20 conversations from 10 pairs, and annotated 11 conversations of 6 pairs using the Anvil video annotating tool (Kipp 2004).
Agent?s verbal behaviors: The agent?s (actually, instructor?s) speech data was split by pauses longer than 200ms.
For each inter pausal unit (IPU), utterance content type defined as follows was assigned.
?? Identification (id): identification of a target object for the next operation ??
Operation (op): request to execute a mouse click or a similar primitive action on the target ??
Identification + operation (idop): referring to identification and operation in one IPU In addition to these main categories, we also used: State (referring to a state before/after an operation), Function (explaining a function of the system), Goal (referring to a task goal to be accomplished), and Acknowledgment.
The intercoder agreement for this coding scheme is very high K=0.89 (Cohen?s Kappa), suggesting that the assigned tags are reliable.
Agent?s nonverbal behaviors: As the most salient instructor?s nonverbal behaviors in the collected data, we annotated agent pointing gestures: ??
Agent movement: agent?s position movement ??
Agent touching target (att): agent?s touching the target object as a stroke of a pointing gesture (a) Instructor (b) PC output Figure 2: Wizard-of-Oz agent controlled by instructor 122 User?s nonverbal behaviors: We annotated three types of mouse manipulation for the user?s task manipulation as follows: ??
Mouse movement: movement of the mouse cursor ??
Mouse-on-target: the mouse cursor is on the target object ??
Click target: click on the target object 4.1 Example of collected data An example of an annotated corpus is shown in Figure 3.
The upper two tracks illustrate the agent?s verbal and nonverbal behaviors, and the other two illustrate the user?s behaviors.
The agent was pointing at the target (att) and giving a sequence of identification descriptions [a1-3].
Since the user?s mouse did not move at all, the agent added another identification IPU [a4] accompanied by another pointing gesture.
Immediately after that, the user?s mouse cursor started moving towards the target object.
After finishing the next IPU, the agent finally requested the user to click the object in [a6].
Note that the collected Wizard-of-Oz conversations are very similar to the human-human instruction dialogues shown in Figure 1.
While carefully monitoring the user?s mouse actions, the Wizard-of-Oz agent provided information in small pieces.
If it was uncertain that the user was following the instruction, the agent added more explanation without continuing.
5 Probabilistic
model of user-agent multimodal interaction 5.1 Building a Bayesian network model To consider multiple factors for verbal and nonverbal behaviors in probabilistic reasoning, we employed a Bayesian network technique, which can infer the likelihood of the occurrence of a target event based on the dependencies among multiple kinds of evidence.
We extracted the conversational data from the beginning of an instructor's identification utterance for a new target object to the point that the user clicks on the object.
Each IPU was split at 500ms intervals, and 1395 intervals were obtained.
As shown in Figure 4, the network consists of 9 properties concerning verbal and nonverbal behaviors for past, current, and future interval(s).
5.2 Predicting
evidence of understanding As a preliminary evaluation, we tested how accurately our Bayesian network model can predict an instructor?s grounding judgment, and the user?s mouse click.
The following five kinds of evidence were given to the network to predict future states.
As evidence for the previous three intervals (1.5 sec), we used (1) the percentage of time the agent touched the target (att), (2) the number of the user?s mouse movements.
Evidence for the current interval is (3) current IPU?s content type, (4) whether the end of the current interval will be the end of the IPU (i.e.
whether a pause will follow after the current interval), and (5) whether the mouse is on the target object.
Well, Yes View the TV right of Yes Beside the DVD There is a button starts with ?V??
Ah, yes Er, yes Press it This User Agent Speech Gesture Mouser actions id id id id id+op Mouse move click att att att Mouse on target [a2] [a3] [a4] [a5] [a6][a1] ack ack ack ack Speech Off On Figure 3: Example dialogue between Wizard-of-Oz agent and user Figure 4: Bayesian network model 123 (a) Predicting grounding judgment: We tested how accurately the model can predict whether the instructor will go on to the next leg of the instruction or will give additional explanations using the same utterance content type (the current message will not be grounded).
The results of 5-fold cross-validation are shown in Table 1.
Since 83% of the data are ?same content??cases, prediction for ?same content??is very accurate (F-measure is 0.90).
However, it is not very easy to find ?content change??case because of its less frequency (F-measure is 0.68).
It would be better to test the model using more balanced data.
(b) Predicting user?s mouse click: As a measure of the smoothness of task manipulation, the network predicted whether the user?s mouse click would be successfully performed within the next 5 intervals (2.5sec).
If a mouse click is predicted, the agent should just wait without annoying the user by unnecessary explanation.
Since randomized data is not appropriate to test mouse click prediction, we used 299 sequences of utterances that were not used for training.
Our model predicted 84% of the user?s mouse clicks: 80% of them were predicted 3-5 intervals before the actual occurrence of the mouse click, and 20% were predicted 1 interval before.
However, the model frequently generates wrong predictions.
Improving precision rate is necessary.
6 Discussion
and Future Work We employed a Bayesian network technique to our goal of developing conversational agents that can generate fine-grained multimodal instruction dialogues, and we proposed a probabilistic model for predicting grounding judgment and user?s successful mouse click.
The results of preliminary evaluation suggest that separate models of each modality for each conversational participant cannot properly describe the complex process of on-going multimodal interaction, but modeling the interaction as dyadic activities with multiple tracks of modalities is a promising approach.
The advantage of employing the Bayesian network technique is that, by considering the cost of misclassification and the benefit of correct classification, the model can be easily adjusted according to the purpose of the system or the user?s skill level.
For example, we can make the model more cautious or incautious.
Thus, our next step is to implement the proposed model into a conversational agent, and evaluate our model not only in its accuracy, but also in its effectiveness by testing the model with various utility values.
References Brennan, S.
2000. Processes that shape conversation and their implications for computational linguistics.
In Proceedings of 38th Annual Meeting of the ACL.
Cassell, J., Bickmore, T., Campbell, L., Vilhjalmsson, H.
and Yan, H.
(2000). Human Conversation as a System Framework: Designing Embodied Conversational Agents.
Embodied Conversational Agents.
J. Cassell, J.
Sullivan, S.
Prevost and E.
Churchill. Cambridge, MA, MIT Press: 29-63.
Clark, H.
H. and Schaefer, E.
F. 1989.
Contributing to discourse.
Cognitive Science 13: 259-294.
Clark, H.
H. and Krych, M.
A. 2004.
Speaking while monitoring addressees for understanding.
Journal of Memory and Language 50(1): 62-81.
Dillenbourg, P., Traum, D.
R. and Schneider, D.
1996. Grounding in Multi-modal Task Oriented Collaboration.
In Proceedings of EuroAI&Education Conference: 415-425.
Kipp, M.
2004. Gesture Generation by Imitation From Human Behavior to Computer Character Animation, Boca Raton, Florida: Dissertation.com.
Nakano, Y.
I., Reinstein, G., Stocky, T.
and Cassell, J.
2003. Towards a Model of Face-to-Face Grounding.
In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics: 553-561.
Paek, T.
and Horvitz, E.
(1999). Uncertainty, Utility, and Misunderstanding: A Decision-Theoretic Perspective on Grounding in Conversational Systems.
Working Papers of the AAAI Fall Symposium on Psychological Models of Communication in Collaborative Systems.
S. E.
Brennan, A.
Giboin and D.
Traum: 85-92.
Whittaker, S.
(2003). Theories and Methods in Mediated Communication.
The Handbook of Discourse Processes.
A. Graesser, MIT Press.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 125??28, Prague, June 2007.
c2007 Association for Computational Linguistics Automatically Assessing the Post Quality in Online Discussions on Software Markus Weimer and Iryna Gurevych and Max Muhlhauser Ubiquitous Knowledge Processing Group, Division of Telecooperation Darmstadt University of Technology, Germany http://www.ukp.informatik.tu-darmstadt.de [mweimer,gurevych,max]@tk.informatik.tu-darmstadt.de Abstract Assessing the quality of user generated content is an important problem for many web forums.
While quality is currently assessed manually, we propose an algorithm to assess the quality of forum posts automatically and test it on data provided by Nabble.com.
We use state-of-the-art classification techniques and experiment with five feature classes: Surface, Lexical, Syntactic, Forum specific and Similarity features.
We achieve an accuracy of 89% on the task of automatically assessing post quality in the software domain using forum specific features.
Without forum specific features, we achieve an accuracy of 82%.
1 Introduction
Web 2.0 leads to the proliferation of user generated content, such as blogs, wikis and forums.
Key properties of user generated content are: low publication threshold and a lack of editorial control.
Therefore, the quality of this content may vary.
The end user has problems to navigate through large repositories of information and find information of high quality quickly.
In order to address this problem, many forum hosting companies like Google Groups1 and Nabble2 introduce rating mechanisms, where users can rate the information manually on a scale from 1 (low quality) to 5 (high quality).
The ratings have been shown to be consistent with the user community by Lampe and Resnick (2004).
However, the 1http://groups.google.com 2http://www.nabble.com percentage of manually rated posts is very low (0.1% in Nabble).
Departing from this, the main idea explored in the present paper is to investigate the feasibility of automatically assessing the perceived quality of user generated content.
We test this idea for online forum discussions in the domain of software.
The perceived quality is not an objective measure.
Rather, it models how the community at large perceives post quality.
We choose a machine learning approach to automatically assess it.
Our main contributions are: (1) An algorithm for automatic quality assessment of forum posts that learns from human ratings.
We evaluate the system on online discussions in the software domain.
(2) An analysis of the usefulness of different classes of features for the prediction of post quality.
2 Related
work To the best of our knowledge, this is the first work which attempts to assess the quality of forum posts automatically.
However, on the one hand work has been done on automatic assessment of other types of user generated content, such as essays and product reviews.
On the other hand, student online discussions have been analyzed.
Automatic text quality assessment has been studied in the area of automatic essay scoring (Valenti et al., 2003; Chodorow and Burstein, 2004; Attali and Burstein, 2006).
While there exist guidelines for writing and assessing essays, this is not the case for forum posts, as different users cast their rating with possibly different quality criteria in mind.
The same argument applies to the automatic assessment of product review usefulness (Kim et al., 2006c): 125 Stars Label on the website Number star Poor Post 1251 starstar Below Average Post 44 starstarstar Average Post 69 starstarstarstar Above Average Post 183 starstarstarstarstar Excellent Post 421 Table 1: Categories and their usage frequency.
Readers of a review are asked ?Was this review helpful to you???with the answer choices Yes/No.
This is very well defined compared to forum posts, which are typically rated on a five star scale that does not advertise a specific semantics.
Forums have been in the focus of another track of research.
Kim et al.(2006b) found that the relation between a student?s posting behavior and the grade obtained by that student can be assessed automatically.
The main features used are the number of posts, the average post length and the average number of replies to posts of the student.
Feng et al.(2006) and Kim et al.(2006a) describe a system to find the most authoritative answer in a forum thread.
The latter add speech act analysis as a feature for this classification.
Another feature is the author?s trustworthiness, which could be computed based on the automatic quality classification scheme proposed in the present paper.
Finding the most authoritative post could also be defined as a special case of the quality assessment.
However, it is definitely different from the task studied in the present paper.
We assess the perceived quality of a given post, based solely on its intrinsic features.
Any discussion thread may contain an indefinite number of good posts, rather than a single authoritative one.
3 Experiments
We seek to develop a system that adapts to the quality standards existing in a certain user community by learning the relation between a set of features and the perceived quality of posts.
We experimented with features from five classes described in table 2: Surface, Lexical, Syntactic, Forum specific and Similarity features.
We use forum discussions from the Software category of Nabble.com.5 The data consists of 1968 rated posts in 1788 threads from 497 forums.
Posts can be rated by multiple users, but that happens 5http://www.nabble.com/Software-f94.html rarely.
1927 posts were rated by one, 40 by two and 1 post by three users.
Table 1 shows the distribution of average ratings on a five star scale.
From this statistics, it becomes evident that users at Nabble prefer extreme ratings.
Therefore, we decided to treat the posts as being binary rated.: Posts with less than three stars are rated ?bad??
Posts with more than three stars are ?good??
We removed 61 posts where all ratings are exactly three stars.
We removed additional 14 posts because they had contradictory ratings on the binary scale.
Those posts were mostly spam, which was voted high for commercial interests and voted down for being spam.
Additionally, we removed 30 posts that did not contain any text but only attachments like pictures.
Finally, we removed 331 non English posts using a simple heuristics: Posts that contained a certain percentage of words above a pre-defined threshold, which are non-English according to a dictionary, were considered to be non-English.
This way, we obtained 1532 binary classified posts: 947 good posts and 585 bad posts.
For each post, we compiled a feature vector, and feature values were normalized to the range [0.0,...,1.0].
We use support vector machines as a state-of-theart-algorithm for binary classification.
For all experiments, we used a C-SVM with a gaussian RBF kernel as implemented by LibSVM in the YALE toolkit (Chang and Lin, 2001; Mierswa et al., 2006).
Parameters were set to C = 10 and  = 0.1.
We performed stratified ten-fold cross validation6 to estimate the performance of our algorithm.
We repeated several experiments according to the leave-one-out evaluation scheme and found comparable results to the ones reported in this paper.
4 Results
and Analysis We compared our algorithm to a majority class classifier as a baseline, which achieves an accuracy of 62%.
As it is evident from table 3, most system configurations outperform the baseline system.
The best performing single feature category are the Forum specific features.
As we seek to build an adaptable system, analyzing the performance without these features is worthwhile: Using all other features, we 6See (Witten and Frank, 2005), chapter 5.3 for an in-depth description.
126 Feature category Feature name Description Surface Features Length The number of tokens in a post.
Question Frequency The percentage of sentences ending with ????
Exclamation Frequency The percentage of sentences ending with ????
Capital Word Frequency The percentage of words in CAPITAL, which is often associated with shouting.
Lexical Features Information about the wording of the posts Spelling Error Frequency The percentage of words that are not spelled correctly.3 Swear Word Frequency The percentage of words that are on a list of swear words we compiled from resources like WordNet and Wikipedia4, which contains more than eighty words like ?asshole?? but also common transcriptions like ?f*ckin??
Syntactic Features The percentage of part-of-speech tags as defined in the PENN Treebank tag set (Marcus et al., 1994).
We used TreeTagger (Schmid, 1995) based on the english parameter files supplied with it.
Forum specific features Properties of a post that are only present in forum postings IsHTML Whether or not a post contains HTML.
In our data, this is encoded explicitly, but it can also be determined by regular expressions matching HTML tags.
IsMail Whether or not a post has been copied from a mailing list.
This is encoded explicitly in our data.
Quote Fraction The fraction of characters that are inside quotes of other posts.
These quotes are marked explicitly in our data.
URL and Path Count The number of URLs and filesystem paths.
Post quality in the software domain may be influenced by the amount of tangible information, which is partly captured by these features.
Similarity features Forums are focussed on a topic.
The relatedness of a post to the topic of the forum may influence post quality.
We capture this relatedness by the cosine between the posts unigram vector and the unigram vector of the forum.
Table 2: Features used for the automatic quality assessment of posts.
achieve an only slightly worse classification accuracy.
Thus, the combination of all other features captures the quality of a post fairly well.
SUF LEX SYN FOR SIM Avg.
accuracy Baseline 61.82%?????????? 89.10%??
????????61.82% ??????????71.82% ??????????82.64% ??????????85.05% ??????????62.01% ??????????89.10%??
????????89.36%???? ??????85.03%??????
????82.90%???????? ??88.97% ??????????88.56%??
????????85.12% ??????????88.74% Table 3: Accuracy with different feature sets.
SUF: Surface, LEX: Lexical, SYN: Syntax, FOR: Forum specific, SIM: similarity.
The baseline results from a majority class classifier.
We performed additional experiments to identify the most important features from the Forum specific ones.
Table 4 shows that IsMail and Quote Fraction are the dominant features.
This is noteworthy, as those features are not based on the domain of discussion.
Thus, we believe that these features will perform well in future experiments on other data.
ISM ISH QFR URL PAC Avg.
accuracy?????????? 85.05%??
????????73.30% ??????????61.82% ??????????73.76% ??????????61.29% ??????????61.82% ??????????74.41%??
????????85.05%???? ??????73.30%??????
????85.05%???????? ??85.05%??
????????84.99%?????? ????85.05% Table 4: Accuracy with different forum specific features.
ISM: IsMail, ISH: IsHTML, QFR: QuoteFraction, URL: URLCount, PAC: PathCount.
Error Analysis Table 5 shows the confusion matrix of the system using all features.
Many posts that were misclassified as good ones show no apparent reason to be classified as bad posts to us.
The understanding of their rating seems to require deep knowledge about the specific subject of discussion.
The few remaining posts are either spam or rated negatively to signalize dissent with the opinion expressed in the post.
Posts that were misclassified as bad ones often contain program code, digital signatures or other non-textual parts in the body.
We plan to address these issues with better preprocessing in 127 true good true bad sum pred.
good 490 72 562 pred.
bad 95 875 970 sum 585 947 1532 Table 5: Confusion matrix for the system using all features.
the future.
However, the relatively high accuracy already achieved shows that these issues are rare.
5 Conclusion
and Future Work Assessing post quality is an important problem for many forums on the web.
Currently, most forums need their users to rate the posts manually, which is error prone, labour intensive and last but not least may lead to the problem of premature negative consent (Lampe and Resnick, 2004).
We proposed an algorithm that has shown to be able to assess the quality of forum posts.
The algorithm applies state-of-the-art classification techniques using features such as Surface, Lexical, Syntactic, Forum specific and Similarity features to do so.
Our best performing system configuration achieves an accuracy of 89.1%, which is significantly higher than the baseline of 61.82%.
Our experiments show that forum specific features perform best.
However, slightly worse but still satisfactory performance can be obtained even without those.
So far, we have not made use of the structural information in forum threads yet.
We plan to perform experiments investigating speech act recognition in forums to improve the automatic quality assessment.
We also plan to apply our system to further domains of forum discussion, such as the discussions among active Wikipedia users.
We believe that the proposed algorithm will support important applications beyond content filtering like automatic summarization systems and forum specific search.
Acknowledgments This work was supported by the German Research Foundation as part of the Research Training Group ?Feedback-Based Quality Management in eLearning??under the grant 1223.
We are thankful to Nabble for providing their data.
References Yigal Attali and Jill Burstein.
2006. Automated essay scoring with e-rater v.2.
The Journal of Technology, Learning, and Assessment, 4(3), February.
Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: a library for support vector machines.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.
Martin Chodorow and Jill Burstein.
2004. Beyond essay length: Evaluating e-raters performance on toefl essays.
Technical report, ETS.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard Hovy.
2006. Learning to detect conversation focus of threaded discussions.
In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics (HLT-NNACL).
Jihie Kim, Grace Chern, Donghui Feng, Erin Shaw, and Eduard Hovya.
2006a. Mining and assessing discussions on the web through speech act analysis.
In Proceedings of the Workshop on Web Content Mining with Human Language Technologies at the 5th International Semantic Web Conference.
Jihie Kim, Erin Shaw, Donghui Feng, Carole Beal, and Eduard Hovy.
2006b. Modeling and assessing student activities in on-line discussions.
In Proceedings of the Workshop on Educational Data Mining at the conference of the American Association of Artificial Intelligence (AAAI-06), Boston, MA.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco Penneacchiotti.
2006c. Automatically assessing review helpfulness.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 423430, Sydney, Australia, July.
Cliff Lampe and Paul Resnick.
2004. Slash(dot) and burn: Distributed moderation in a large online conversation space.
In Proceedings of ACM CHI 2004 Conference on Human Factors in Computing Systems, Vienna Austria, pages 543 550.
Mitchell P.
Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
1994. Building a Large Annotated Corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313??30.
Ingo Mierswa, Michael Wurst, Ralf Klinkenberg, Martin Scholz, and Timm Euler.
2006. YALE: Rapid prototyping for complex data mining tasks.
In KDD ??6: Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 935??40, New York, NY, USA.
ACM Press.
Helmut Schmid.
1995. Probabilistic Part-of-Speech Tagging Using Decision Trees.
In International Conference on New Methods in Language Processing, Manchester, UK.
Salvatore Valenti, Francesca Neri, and Alessandro Cucchiarelli.
2003. An overview of current research on automated essay grading.
Journal of Information Technology Education, 2:319??29.
Ian H.
Witten and Eibe Frank.
2005. Data Mining: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2 edition.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 129??32, Prague, June 2007.
c2007 Association for Computational Linguistics WordNet-based Semantic Relatedness Measures in Automatic Speech Recognition for Meetings Michael Pucher Telecommunications Research Center Vienna Vienna, Austria Speech and Signal Processing Lab, TU Graz Graz, Austria pucher@ftw.at Abstract This paper presents the application of WordNet-based semantic relatedness measures to Automatic Speech Recognition (ASR) in multi-party meetings.
Different word-utterance context relatedness measures and utterance-coherence measures are defined and applied to the rescoring of Nbest lists.
No significant improvements in terms of Word-Error-Rate (WER) are achieved compared to a large word-based ngram baseline model.
We discuss our results and the relation to other work that achieved an improvement with such models for simpler tasks.
1 Introduction
As (Pucher, 2005) has shown different WordNetbased measures and contexts are best for word prediction in conversational speech.
The JCN (Section 2.1) measure performs best for nouns using the noun-context.
The LESK (Section 2.1) measure performs best for verbs and adjectives using a mixed word-context.
Text-based semantic relatedness measures can improve word prediction on simulated speech recognition hypotheses as (Demetriou et al., 2000) have shown.
(Demetriou et al., 2000) generated N-best lists from phoneme confusion data acquired from a speech recognizer, and a pronunciation lexicon.
Then sentence hypotheses of varying Word-ErrorRate (WER) were generated based on sentences from different genres from the British National Corpus (BNC).
It was shown by them that the semantic model can improve recognition, where the amount of improvement varies with context length and sentence length.
Thereby it was shown that these models can make use of long-term information.
In this paper the best performing measures from (Pucher, 2005), which outperform baseline models on word prediction for conversational telephone speech are used for Automatic Speech Recognition (ASR) in multi-party meetings.
Thereby we want to investigate if WordNet-based models can be used for rescoring of ?real??N-best lists in a difficult task.
1.1 Word
prediction by semantic similarity The standard n-gram approach in language modeling for speech recognition cannot cope with long-term dependencies.
Therefore (Bellegarda, 2000) proposed combining n-gram language models, which are effective for predicting local dependencies, with Latent Semantic Analysis (LSA) based models for covering long-term dependencies.
WordNet-based semantic relatedness measures can be used for word prediction using long-term dependencies, as in this example from the CallHome English telephone speech corpus: (1) B: I I well, you should see what the floorleftstudentsfloorright B: after they torture them for six floorleftyearsfloorright in middle floorleftschoolfloorright and high floorleftschoolfloorright they don?t want to do anything in floorleftcollegefloorright particular.
In Example 1 college can be predicted from the noun context using semantic relatedness measures, 129 here between students and college.
A 3-gram model gives a ranking of college in the context of anything in.
An 8-gram predicts college from they don?t want to do anything in, but the strongest predictor is students.
1.2 Test
data The JCN and LESK measure that are defined in the next section are used for N-best list rescoring.
For the WER experiments N-best lists generated from the decoding of conference room meeting test data of the NIST Rich Transcription 2005 Spring (RT05S) meeting evaluation (Fiscus et al., 2005) are used.
The 4-gram that has to be improved by the WordNet-based models is trained on various corpora from conversational telephone speech to web data that together contain approximately 1 billion words.
2 WordNet-based semantic relatedness measures 2.1 Basic measures Two similarity/distance measures from the Perl package WordNet-Similarity written by (Pedersen et al., 2004) are used.
The measures are named after their respective authors.
All measures are implemented as similarity measures.
JCN (Jiang and Conrath, 1997) is based on the information content, and LESK (Banerjee and Pedersen, 2003) allows for comparison across Part-of-Speech (POS) boundaries.
2.2 Word
context relatedness First the relatedness between words is defined based on the relatedness between senses.
S(w) are the senses of word w.
Definition 2 also performs wordsense disambiguation.
rel(w,wprime) = max ci?S(w) cj?S(wprime) rel(ci,cj) (2) The relatedness of a word and a context (relW) is defined as the average of the relatedness of the word and all words in the context.
relW(w,C) = 1| C | summationdisplay wi?C rel(w,wi) (3) 2.3 Word utterance (context) relatedness The performance of the word-context relatedness (Definition 3) shows how well the measures work for algorithms that proceed in a left-to-right manner, since the context is restricted to words that have already been seen.
For the rescoring of N-best lists it is not necessary to proceed in a left-to-right manner.
The word-utterance-context relatedness can be used for the rescoring of N-best lists.
This relatedness does not only use the context of the preceding words, but the whole utterance.
Suppose U = ?w1,...,wn??is an utterance.
Let pre(wi,U) be the set uniontextj<i wj and post(wi,U) be the set uniontextj>i wj.
Then the word-utterance-context relatedness is defined as relU1(wi,U,C) = relW(wi,pre(wi,U) ??post(wi,U) ??C). (4) In this case there are two types of context.
The first context comes from the respective meeting, and the second context comes from the actual utterance.
Another definition is obtained if the context C is eliminated (C = ?? and just the utterance context U is taken into account.
relU2(wi,U) = relW(wi,pre(wi,U) ??post(wi,U)) (5) Both definitions can be modified for usage with rescoring in a left-to-right manner by restricting the contexts only to the preceding words.
relU3(wi,U,C) = relW(wi,pre(wi,U) ??C) (6) relU4(wi,U) = relW(wi,pre(wi,U)) (7) 2.4 Defining utterance coherence Using Definitions 4-7 different concepts of utterance coherence can be defined.
For rescoring the utterance coherence is used, when a score for each element of an N-best list is needed.
U is again an utterance U = ?w1,...,wn?? 130 cohU1(U,C) = 1| U | summationdisplay w?U relU1(w,U,C) (8) The first semantic utterance coherence measure (Definition 8) is based on all words in the utterance as well as in the context.
It takes the mean of the relatedness of all words.
It is based on the wordutterance-context relatedness (Definition 4).
cohU2(U) = 1| U | summationdisplay w?U relU2(w,U) (9) The second coherence measure (Definition 9) is a pure inner-utterance-coherence, which means that no history apart from the utterance is needed.
Such a measure is very useful for rescoring, since the history is often not known or because there are speech recognition errors in the history.
It is based on Definition 5.
cohU3(U,C) = 1| U | summationdisplay w?U relU3(w,U,C) (10) The third (Definition 10) and fourth (Definition 11) definition are based on Definition 6 and 7, that do not take future words into account.
cohU4(U) = 1| U | summationdisplay w?U relU4(w,U) (11) 3 Word-error-rate (WER) experiments For the rescoring experiments the first-best element of the previous N-best list is added to the context.
Before applying the WordNet-based measures, the N-best lists are POS tagged with a decision tree tagger (Schmid, 1994).
The WordNet measures are then applied to verbs, nouns and adjectives.
Then the similarity values are used as scores, which have to be combined with the language model scores of the N-best list elements.
The JCN measure is used for computing a noun score based on the noun context, and the LESK measure is used for computing a verb/adjective score based on the noun/verb/adjective context.
In the end there is a lesk score and a jcn score for each N-best list.
The final WordNet score is the sum of the two scores.
The log-linear interpolation method used for the rescoring is defined as p(S) ??pwordnet(S) pn-gram(S)1??(12) where ??denotes normalization.
Based on all WordNet scores of an N-best list a probability is estimated, which is then interpolated with the n-gram model probability.
If only the elements in an Nbest list are considered, log-linear interpolation can be used since it is not necessary to normalize over all sentences.
Then there is only one parameter  to optimize, which is done with a brute force approach.
For this optimization a small part of the test data is taken and the WER is computed for different values of .
As a baseline the n-gram mixture model trained on all available training data (??1 billion words) is used.
It is log-linearly interpolated with the WordNet probabilities.
Additionally to this sophisticated interpolation, solely the WordNet scores are used without the n-gram scores.
3.1 WER
experiments for inner-utterance coherence In this first group of experiments Definitions 8 and 9 are applied to the rescoring task.
Similarity scores for each element in an N-best list are derived according to the definitions.
The first-best element of the last list is always added to the context.
The context size is constrained to the last 20 words.
Definition 8 includes context apart from the utterance context, Definition 9 only uses the utterance context.
No improvement over the n-gram baseline is achieved for these two measures.
Neither with the log-linearly interpolated models nor with the WordNet scores alone.
The differences between the methods in terms of WER are not significant.
3.2 WER
experiments for utterance coherence In the second group of experiments Definitions 10 and 11 are applied to the rescoring task.
There is again one measure that uses dialog context (10) and one that only uses utterance context (11).
Also for these experiments no improvement over the n-gram baseline is achieved.
Neither with the 131 log-linearly interpolated models nor with the WordNet scores alone.
The differences between the methods in terms of WER are also not significant.
There are also no significant differences in performance between the second group and the first group of experiments.
4 Summary
and discussion We showed how to define more and more complex relatedness measures on top of the basic relatedness measures between word senses.
The LESK and JCN measures were used for the rescoring of N-best lists.
It was shown that speech recognition of multi-party meetings cannot be improved compared to a 4-gram baseline model, when using WordNet models.
One reason for the poor performance of the models could be that the task of rescoring simulated Nbest lists, as presented in (Demetriou et al., 2000), is significantly easier than the rescoring of ?real??Nbest lists.
(Pucher, 2005) has shown that WordNet models can outperform simple random models on the task of word prediction, in spite of the noise that is introduced through word-sense disambiguation and POS tagging.
To improve the wordsense disambiguation one could use the approach proposed by (Basili et al., 2004).
In the above WER experiments a 4-gram baseline model was used, which was trained on nearly 1 billion words.
In (Demetriou et al., 2000) a simpler baseline has been used.
650 sentences were used there to generate sentence hypotheses with different WER using phoneme confusion data and a pronunciation lexicon.
Experiments with simpler baseline models ignore that these simpler models are not used in today?s recognition systems.
We think that these prediction models can still be useful for other tasks where only small amounts of training data are available.
Another possibility of improvement is to use other interpolation techniques like the maximum entropy framework.
WordNetbased models could also be improved by using a trigger-based approach.
This could be done by not using the whole WordNet and its similarities, but defining word-trigger pairs that are used for rescoring.
5 Acknowledgements
This work was supported by the European Union 6th FP IST Integrated Project AMI (Augmented Multiparty Interaction, and by Kapsch Carrier-Com AG and Mobilkom Austria AG together with the Austrian competence centre programme Kplus.
References Satanjeev Banerjee and Ted Pedersen.
2003. Extended gloss overlaps as a measure of semantic relatedness.
In Proceedings of the 18th Int.
Joint Conf.
on Artificial Intelligence, pages 805??10, Acapulco.
Roberto Basili, Marco Cammisa, and Fabio Massimo Zanzotto.
2004. A semantic similarity measure for unsupervised semantic tagging.
In Proc.
of the Fourth International Conference on Language Resources and Evaluation (LREC2004), Lisbon, Portugal.
Jerome Bellegarda.
2000. Large vocabulary speech recognition with multispan statistical language models.
IEEE Transactions on Speech and Audio Processing, 8(1), January.
G. Demetriou, E.
Atwell, and C.
Souter. 2000.
Using lexical semantic knowledge from machine readable dictionaries for domain independent language modelling.
In Proc.
of LREC 2000, 2nd International Conference on Language Resources and Evaluation.
Jonathan G.
Fiscus, Nicolas Radde, John S.
Garofolo, Audrey Le, Jerome Ajot, and Christophe Laprun.
2005. The rich transcription 2005 spring meeting recognition evaluation.
In Rich Transcription 2005 Spring Meeting Recognition Evaluation Workshop, Edinburgh, UK.
Jay J.
Jiang and David W.
Conrath. 1997.
Semantic similarity based on corpus statistics and lexical taxonomy.
In Proceedings of the International Conference on Research in Computational Linguistics, Taiwan.
Ted Pedersen, S.
Patwardhan, and J.
Michelizzi. 2004.
WordNet::Similarity Measuring the relatedness of concepts.
In Proc.
of Fifth Annual Meeting of the North American Chapter of the ACL (NAACL-04), Boston, MA.
Michael Pucher.
2005. Performance evaluation of WordNet-based semantic relatedness measures for word prediction in conversational speech.
In IWCS 6, Sixth International Workshop on Computational Semantics, Tilburg, Netherlands.
H Schmid.
1994. Probabilistic part-of-speech tagging using decision trees.
In Proceedings of International Conference on New Methods in Language Processing, Manchester, UK, September .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 133??36, Prague, June 2007.
c2007 Association for Computational Linguistics Building Emotion Lexicon from Weblog Corpora Changhua Yang Kevin Hsin-Yih Lin Hsin-Hsi Chen Department of Computer Science and Information Engineering National Taiwan University #1 Roosevelt Rd.
Sec. 4, Taipei, Taiwan 106 {d91013, f93141, hhchen}@csie.ntu.edu.tw Abstract An emotion lexicon is an indispensable resource for emotion analysis.
This paper aims to mine the relationships between words and emotions using weblog corpora.
A collocation model is proposed to learn emotion lexicons from weblog articles.
Emotion classification at sentence level is experimented by using the mined lexicons to demonstrate their usefulness.
1 Introduction
Weblog (blog) is one of the most widely used cybermedia in our internet lives that captures and shares moments of our day-to-day experiences, anytime and anywhere.
Blogs are web sites that timestamp posts from an individual or a group of people, called bloggers.
Bloggers may not follow formal writing styles to express emotional states.
In some cases, they must post in pure text, so they add printable characters, such as ??-)??(happy) and ??-(??(sad), to express their feelings.
In other cases, they type sentences with an internet messengerstyle interface, where they can attach a special set of graphic icons, or emoticons.
Different kinds of emoticons are introduced into text expressions to convey bloggers??emotions.
Since thousands of blog articles are created everyday, emotional expressions can be collected to form a large-scale corpus which guides us to build vocabularies that are more emotionally expressive.
Our approach can create an emotion lexicon free of laborious efforts of the experts who must be familiar with both linguistic and psychological knowledge.
2 Related
Works Some previous works considered emoticons from weblogs as categories for text classification.
Mishne (2005), and Yang and Chen (2006) used emoticons as tags to train SVM (Cortes and Vapnik, 1995) classifiers at document or sentence level.
In their studies, emoticons were taken as moods or emotion tags, and textual keywords were taken as features.
Wu et al.(2006) proposed a sentencelevel emotion recognition method using dialogs as their corpus.
?Happy, ?Unhappy?? or ?Neutral?? was assigned to each sentence as its emotion category.
Yang et al.(2006) adopted Thayer?s model (1989) to classify music emotions.
Each music segment can be classified into four classes of moods.
In sentiment analysis research, Read (2005) used emoticons in newsgroup articles to extract instances relevant for training polarity classifiers.
3 Training
and Testing Blog Corpora We select Yahoo!
Kimo Blog1 posts as our source of emotional expressions.
Yahoo! Kimo Blog service has 40 emoticons which are shown in Table 1.
When an editing article, a blogger can insert an emoticon by either choosing it or typing in the corresponding codes.
However, not all articles contain emoticons.
That is, users can decide whether to insert emoticons into articles/sentences or not.
In this paper, we treat these icons as emotion categories and taggings on the corresponding text expressions.
The dataset we adopt consists of 5,422,420 blog articles published at Yahoo!
Kimo Blog from January to July, 2006, spanning a period of 212 days.
In total, 336,161 bloggers??articles were collected.
Each blogger posts 16 articles on average.
We used the articles from January to June as the training set and the articles in July as the testing set.
Table 2 shows the statistics of each set.
On average, 14.10% of the articles contain emotion-tagged expressions.
The average length of articles with tagged emotions, i.e., 272.58 characters, is shorter 1 http://tw.blog.yahoo.com/ 133 than that of articles without tagging, i.e., 465.37 characters.
It seems that people tend to use emoticons to replace certain amount of text expressions to make their articles more succinct.
Figure 1 shows the three phases for the construction and evaluation of emotion lexicons.
In phase 1, 1,185,131 sentences containing only one emoticon are extracted to form a training set to build emotion lexicons.
In phase 2, sentence-level emotion classifiers are constructed using the mined lexicons.
In phase 3, a testing set consisting of 307,751 sentences is used to evaluate the classifiers.
4 Emotion
Lexicon Construction The blog corpus contains a collection of bloggers?? emotional expressions which can be analyzed to construct an emotion lexicon consisting of words that collocate with emoticons.
We adopt a variation of pointwise mutual information (Manning and Schtze, 1999) to measure the collocation strength co(e,w) between an emotion e and a word w: )()( ),(log),(),o( wPeP wePwecwec = (1) where P(e,w)=c(e,w)/N, P(e)=c(e)/N, P(w)=c(w)/N, c(e) and c(w) are the total occurrences of emoticon e and word w in a tagged corpus, respectively, c(e,w) is total co-occurrences of e and w, and N denotes the total word occurrences.
A word entry of a lexicon may contain several emotion senses.
They are ordered by the collocation strength co.
Figure 2 shows two Chinese example words, ???(ha1ha1) and ???
(ke3wu4). The former collocates with ?laughing?? and ?big grin??emoticons with collocation strength 25154.50 and 2667.11, respectively.
Similarly, the latter collocates with ?angry??and ?phbbbbt??
When all collocations (i.e., word-emotion pairs) are listed in a descending order of co, we can choose top n collocations to build an emotion lexicon.
In this paper, two lexicons (Lexicons A and B) are extracted by setting n to 25k and 50k.
Lexicon A contains 4,776 entries with 25,000 sense pairs and Lexicon B contains 11,243 entries and 50,000 sense pairs.
5 Emotion
Classification Suppose a sentence S to be classified consists of n emotion words.
The emotion of S is derived by a mapping from a set of n emotion words to m emotion categories as follows: },...,{?},...,{ 11 m tionclassifican eeeewewS ???
Table 1.
Yahoo! Kimo Blog Emoticon Set.
ID Emoticon Code Description ID Emoticon Code Description ID Emoticon Code Description ID Emoticon Code Description 1 :) happy 11 :O surprise 21 0:) angel 31 (:| yawn 2 :( sad 12 X-( angry 22 :-B nerd 32 =P~ drooling 3 ;) winking 13 :> smug 23 =; talk to the hand 33 :-? thinking 4 :D big grin 14 B-) cool 24 I-) asleep 34 ;)) hee hee 5 ;;) batting eyelashes 15 :-S worried 25 8-) rolling eyes 35 =D> applause 6 :-/ confused 16 >:) devil 26 :-& sick 36 [-o< praying 7 :x love struck 17 :(( crying 27 :-$ don't tell anyone 37 :-< sigh 8 :?? blushing 18 :)) laughing 28 [-( not talking 38 >:P phbbbbt 9 :p tongue 19 :| straight face 29 :o) clown 39 @};rose 10 :* kiss 20 /:) raised eyebrow 30 @-) hypnotized 40 :@) pig Table 2.
Statistics of the Weblog Dataset.
Dataset Article # Tagged # Percentage Tagged Len.
Untagged L.
Training 4,187,737 575,009 13.86% 269.77 chrs.
468.14 chrs.
Testing 1,234,683 182,999 14.92% 281.42 chrs.
455.82 chrs.
Total 5,422,420 764,788 14.10% 272.58 chrs.
465.37 chrs.
Testing Set Figure 1.
Emotion Lexicon Construction and Evaluation.
Extraction Blog Articles Features Classifiers Evaluation Lexicon Construction Training Set Phase 2 Phase 3 Emotion Lexicon Phase 1 134 For each emotion word ewi, we may find several emotion senses with the corresponding collocation strength co by looking up the lexicon.
Three alternatives are proposed as follows to label a sentence S with an emotion: (a) Method 1 (1) Consider all senses of ewi as votes.
Label S with the emotion that receives the most votes.
(2) If more than two emotions get the same number of votes, then label S with the emotion that has the maximum co.
(b) Method 2 Collect emotion senses from all ewi.
Label S with the emotion that has the maximum co.
(c) Method 3 The same as Method 1 except that each ewi votes only one sense that has the maximum co.
In past research, the approach used by Yang et al.(2006) was based on the Thayer?s model (1989), which divided emotions into 4 categories.
In sentiment analysis research, such as Read?s study (2006), a polarity classifier separated instances into positive and negative classes.
In our experiments, we not only adopt fine-grain classification, but also coarse-grain classification.
We first select 40 emoticons as a category set, and also adopt the Thayer?s model to divide the emoticons into 4 quadrants of the emotion space.
As shown in Figure 3, the top-right side collects the emotions that are more positive and energetic and the bottom-left side is more negative and silent.
A polarity classifier uses the right side as positive and the left side as negative.
6 Evaluation
Table 3 shows the performance under various combinations of lexicons, emotion categories and classification methods.
?Hit #??stands for the number of correctly-answered instances.
The baseline represents the precision of predicting the majority category, such as ?happy??or ?positive?? as the answer.
The baseline method?s precision increases as the number of emotion classes decreases.
The upper bound recall indicates the upper limit on the fraction of the 307,751 instances solvable by the corresponding method and thus reflects the limitation of the method.
The closer a method?s actual recall is to the upper bound recall, the better the method.
For example, at most 40,855 instances (14.90%) can be answered using Method 1 in combination with Lexicon A.
But the actual recall is 4.55% only, meaning that Method 1?s recall is more than 10% behind its upper bound.
Methods which have a larger set of candidate answers have higher upper bound recalls, because the probability that the correct answer is in their set of candidate answers is greater.
Experiment results show that all methods utilizing Lexicon A have performance figures lower than the baseline, so Lexicon A is not useful.
In contrast, Lexicon B, which provides a larger collection of vocabularies and emotion senses, outperforms Lexicon A and the baseline.
Although Method 3 has the smallest candidate answer set and thus has the smallest upper bound recall, it outperforms the other two methods in most cases.
Method 2 achieves better precisions when using ???? (ha1ha1) ?hah hah??
Sense 1.
(laughing) ??co: 25154.50 e.g., ?...
??? ?hah hah??
I am getting lucky~??
Sense 2.
(big grin) ??co: 2667.11 e.g., ??~? ?I only memorized vowels today~ haha ??
???????? (ke3wu4) ?darn??
Sense 1.
(angry) ??co: 2797.82 e.g., ????..?? ?What's the hacker doing... darn it ??
Sense 2.
(phbbbbt) ??co: 619.24 e.g., ??????
?Damn those aliens ??
Figure 2.
Some Example Words in a Lexicon.
Arousal (energetic) Valence (negative) (positive) (silent) unassigned: Figure 3.
Emoticons on Thayer?s model.
135 Thayer?s emotion categories.
Method 1 treats the vote to every sense equally.
Hence, it loses some differentiation abilities.
Method 1 performs the best in the first case (Lexicon A, 40 classes).
We can also apply machine learning to the dataset to train a high-precision classification model.
To experiment with this idea, we adopt LIBSVM (Fan et al., 2005) as the SVM kernel to deal with the binary polarity classification problem.
The SVM classifier chooses top k (k = 25, 50, 75, and 100) emotion words as features.
Since the SVM classifier uses a small feature set, there are testing instances which do not contain any features seen previously by the SVM classifier.
To deal with this problem, we use the class prediction from Method 3 for any testing instances without any features that the SVM classifier can recognize.
In Table 4, the SVM classifier employing 25 features has the highest precision.
On the other hand, the SVM classifier employing 50 features has the highest F measure when used in conjunction with Method 3.
7 Conclusion
and Future Work Our methods for building an emotional lexicon utilize emoticons from blog articles collaboratively contributed by bloggers.
Since thousands of blog articles are created everyday, we expect the set of emotional expressions to keep expanding.
In the experiments, the method of employing each emotion word to vote only one emotion category achieves the best performance in both fine-grain and coarse-grain classification.
Acknowledgment Research of this paper was partially supported by Excellent Research Projects of National Taiwan University, under the contract of 95R0062-AE0002.
We thank Yahoo!
Taiwan Inc.
for providing the dataset for researches.
References Corinna Cortes and V.
Vapnik. 1995.
Support-Vector Network.
Machine Learning, 20:273??97.
Rong-En Fan, Pai-Hsuen Chen and Chih-Jen Lin.
2005. Working Set Selection Using Second Order Information for Training Support Vector Machines.
Journal of Machine Learning Research, 6:1889??918.
Gilad Mishne.
2005. Experiments with Mood Classification in Blog Posts.
Proceedings of 1st Workshop on Stylistic Analysis of Text for Information Access.
Jonathon Read.
2005. Using Emotions to Reduce Dependency in Machine Learning Techniques for Sentiment Classification.
Proceedings of the ACL Student Research Workshop, 43-48.
Robert E.
Thayer. 1989.
The Biopsychology of Mood and Arousal, Oxford University Press.
Changhua Yang and Hsin-Hsi Chen.
2006. A Study of Emotion Classification Using Blog Articles.
Proceedings of Conference on Computational Linguistics and Speech Processing, 253-269.
Yi-Hsuan Yang, Chia-Chu Liu, and Homer H.
Chen. 2006.
Music Emotion Classification: A Fuzzy Approach.
Proceedings of ACM Multimedia, 81-84.
Chung-Hsien Wu, Ze-Jing Chuang, and Yu-Chung Lin.
2006. Emotion Recognition from Text Using Semantic Labels and Separable Mixture Models.
ACM Transactions on Asian Language Information Processing, 5(2):165-182.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 137??40, Prague, June 2007.
c2007 Association for Computational Linguistics Construction of Domain Dictionary for Fundamental Vocabulary Chikara Hashimoto Faculty of Engineering, Yamagata University 4-3-16 Jonan, Yonezawa-shi, Yamagata, 992-8510 Japan Sadao Kurohashi Graduate School of Informatics, Kyoto University 36-1 Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501 Japan Abstract For natural language understanding, it is essential to reveal semantic relations between words.
To date, only the IS-A relation has been publicly available.
Toward deeper natural language understanding, we semiautomatically constructed the domain dictionary that represents the domain relation between Japanese fundamental words.
This is the first Japanese domain resource that is fully available.
Besides, our method does not require a document collection, which is indispensable for keyword extraction techniques but is hard to obtain.
As a task-based evaluation, we performed blog categorization.
Also, we developed a technique for estimating the domain of unknown words.
1 Introduction
We constructed a lexical resource that represents the domain relation among Japanese fundamental words (JFWs), and we call it the domain dictionary.1 It associates JFWs with domains in which they are typically used.
For example, a0a2a1a4a3a6a5a6a7 home run is associated with the domain SPORTS2.
That is, we aim to make explicit the horizontal relation between words, the domain relation, while thesauri indicate the vertical relation called IS-A.3 1In fact, there have been a few domain resources in Japanese like Yoshimoto et al.(1997). But they are not publicly available.
2Domains are CAPITALIZED in this paper.
3The lack of the horizontal relationship is also known as the ?tennis problem??(Fellbaum, 1998, p.10). 2 Two Issues You have to address two issues.
One is what domains to assume, and the other is how to associate words with domains without document collections.
The former is paraphrased as how people categorize the real world, which is really a hard problem.
In this study, we avoid being too involved in the problem and adopt a simple domain system that most people can agree on, which is as follows: CULTURE RECREATION SPORTS HEALTH LIVING DIET TRANSPORTATION EDUCATION SCIENCE BUSINESS MEDIA GOVERNMENT It has been created based on web directories such as Open Directory Project with some adjustments.
In addition, NODOMAIN was prepared for those words that do not belong to any particular domain.
As for the latter issue, you might use keyword extraction techniques; identifying words that represent a domain from the document collection using statistical measures like TF*IDF and matching between extracted words and JFWs.
However, you will find that document collections of common domains such as those assumed here are hard to obtain.4 Hence, we had to develop a method that does not require document collections.
The next section details it.
4Initially, we tried collecting web pages in Yahoo!
JAPAN. However, we found that most of them were index pages with a few text contents, from which you cannot extract reliable keywords.
Though we further tried following links in those index pages to acquire enough texts, extracted words turned out to be site-specific rather than domain-specific since many pages were collected from a particular web site.
137 Table 1: Examples of Keywords for each Domain Domain Examples of Keywords CULTURE a0a2a1 movie, a3a5a4 music RECREATION a6a5a7 tourism, a8a10a9 firework SPORTS a11a13a12 player, a14a5a15 baseball HEALTH a12a13a16 surgery, a17a19a18 diagnosis LIVING a20a13a21 childcare, a22a10a23 furniture DIET a24 chopsticks, a25a27a26 lunch TRANSPORTATION a28 station, a29a13a30 road EDUCATION a31a10a32 teacher, a33a5a34 arithmetic SCIENCE a35a10a36 research, a37a13a38 theory BUSINESS a39a13a40 import, a41a10a42 market MEDIA a43a13a44 broadcast, a45a27a46 reporter GOVERNMENT a47a13a48 judicatory, a49 tax 3 Domain Dictionary Construction To identify which domain a JFW is associated with, we use manually-prepared keywords for each domain rather than document collections.
The construction process is as follows: 1 Preparing keywords for each domain (3.1).
2 Associating
JFWs with domains (3.2).
3 Reassociating
JFWs with NODOMAIN (3.3).
4 Manual
correction (3.5).
3.1 Preparing
Keywords for each Domain About 20 keywords for each domain were collected manually from words that appear most frequently in the Web.
Table 1 shows examples of the keywords.
3.2 Associating
JFWs with Domains A JFW is associated with a domain of the highest Ad score.
An Ad score of domain is calculated by summing up the top five Ak scores of the domain.
Then, an Ak score, which is defined between a JFW and a keyword of a domain, is a measure that shows how strongly the JFW and the keyword are related (Figure 1).
Assuming that two words are related if they cooccur more often than chance in a corpus, we adopt the ?2 statistics to calculate an Ak score and use web pages as a corpus.
The number of co-occurrences is approximated by the number of search engine hits when the two words are used as queries.
Among various alternatives, the combination of the ?2 statistics and web pages is adopted following Sasaki et al.(2006). Based on Sasaki et al.(2006), Ak score between JFWs JFW1 JFW2 JFW3  DOMAIN1 kw1a kw1b  DOMAIN2 kw2a kw2b   Adscore JFWm kwna kwnb  DOMAINn Ak scores Figure 1: Associating JFWs with Domains a JFW (jw) and a keyword (kw) is given as below.
Ak(jw,kw) = n(ad ??bc) 2 (a + b)(c + d)(a + c)(b + d) where n is the total number of Japanese web pages, a = hits(jw & kw), b = hits(jw) ??a, c = hits(kw) ??a, d = n ??(a + b + c).
Note that hits(q) represents the number of search engine hits when q is used as a query.
3.3 Reassociating
JFWs with NODOMAIN JFWs that do not belong to any particular domain, i.e. whose highest Ad score is low should be reassociated with NODOMAIN.
Thus, a threshold for determining if a JFW?s highest Ad score is low is required.
The threshold for a JFW (jw) needs to be changed according to hits(jw); the greater hits(jw) is, the higher the threshold should be.
To establish a function that takes jw and returns the appropriate threshold for it, the following semiautomatic process is required after all JFWs are associated with domains: (i) Sort all tuples of the form < jw, hits(jw), the highest Ad of the jw > by hits(jw).5 (ii) Segment the tuples.
(iii) For each segment, extract manually tuples whose jw should be associated with one of the 12 domains and those whose jw should be deemed as NODOMAIN.
Note that the former tuples usually have higher Ad scores than the latter tuples.
(iv) For each segment, identify a threshold that distinguishes between the former tuples and the latter tuples by their Ad scores.
At this point, pairs of the number of hits (represented by each segment) and the appropriate threshold for it are obtained.
(v) Approximate the relation between 5Note that we acquire the number of search engine hits and the Ad score for each jw in the process 2. 138 the number of hits and its threshold by a linear function using least-square method.
Finally, this function indicates the appropriate threshold for each jw.
3.4 Performance
of the Proposed Method We applied the method to JFWs installed on JUMAN (Kurohashi et al., 1994), which are 26,658 words consisting of commonly used nouns and verbs.
As an evaluation, we sampled 380 pairs of a JFW and its domain, and measured accuracy.6 As a result, the proposed method attained the accuracy of 81.3% (309/380).
3.5 Manual
Correction Our policy is that simpler is better.
Thus, as one of our guidelines for manual correction, we avoid associating a JFW with multiple domains as far as possible.
JFWs to associate with multiple domains are restricted to those that are EQUALLY relevant to more than one domain.
4 Blog
Categorization As a task-based evaluation, we categorized blog articles into the domains assumed here.
4.1 Categorization
Method (i) Extract JFWs from the article.
(ii) Classify the extracted JFWs into the domains using the domain dictionary.
(iii) Sort the domains by the number of JFWs classified in descending order.
(iv) Categorize the article as the top domain.
If the top domain is NODOMAIN, the article is categorized as the second domain under the condition below.
|W(2ND DOMAIN)|  |W(NODOMAIN)| > 0.03 where |W(D)| is the number of JFWs classified into the domain D.
4.2 Data
We prepared two blog collections; Bcontrolled and Brandom.
As Bcontrolled, 39 blog articles were collected (3 articles for each domain including NODOMAIN) by the following procedure: (i) Query the Web using a keyword of the domain.7 (ii) From 6In the evaluation, one of the authors judged the correctness of each pair.
7To collect articles that are categorized as NODOMAIN, we used a0a2a1 diary as a query.
Table 2: Breakdown of Brandom Domain # CULTURE 4 RECREATION 1 SPORTS 3 HEALTH 1 Domain # DIET 4 BUSINESS 12 NODOMAIN 5 the top of the search result, collect 3 articles that meet the following conditions; there are enough text contents in it, and people can confidently make a judgment about which domain it is categorized as.
As Brandom, 30 articles were randomly sampled from the Web.
Table 2 shows its breakdown.
Note that we manually removed peripheral contents like author profiles or banner advertisements from the articles in both Bcontrolled and Brandom.
4.3 Result
We measured the accuracy of blog categorization.
As a result, the accuracy of 89.7% (35/39) was attained in categorizing Bcontrolled, while Brandom was categorized with 76.6% (23/30) accuracy.
5 Domain
Estimation for Unknown Words We developed an automatic way of estimating the domain of unknown word (uw) using the dictionary.
5.1 Estimation
Method (i) Search the Web by using uw as a query.
(ii) Retrieve the top 30 documents of the search result.
(iii) Categorize the documents as one of the domains by the method described in 4.1.
(iv) Sort the domains by the number of documents in descending order.
(v) Associate uw with the top domain.
5.2 Experimental
Condition (i) Select 10 words from the domain dictionary for each domain.
(ii) For each word, estimate its domain by the method in 5.1 after removing the word from the dictionary so that the word is unknown.
5.3 Result
Table 3 shows the number of correctly domainestimated words (out of 10) for each domain.
Accordingly, the total accuracy is 67.5% (81/120).
139 Table 3: # of Correctly Domain-estimated Words Domain # CULTURE 7 RECREATION 4 SPORTS 9 HEALTH 9 LIVING 3 DIET 7 Domain # TRANSPORTATION 7 EDUCATION 9 SCIENCE 6 BUSINESS 9 MEDIA 2 GOVERNMENT 9 As for the poor accuracy for RECREATION, LIVING, and MEDIA, we found that it was due to either the ambiguous nature of the words of domain or a characteristic of the estimation method.
The former brought about the poor accuracy for MEDIA.
That is, some words of MEDIA are often used in other contexts.
For example, a0a2a1 live coverage is often used in the SPORTS context.
On the other hand, the method worked poorly for RECREATION and LIVING for the latter reason; the method exploits the Web.
Namely, some words of the domains, such as a3a5a4 tourism and a6a5a7 a7a9a8 a1 shampoo, are often used in the web sites of companies (BUSINESS) that provide services or goods related to RECREATION or LIVING.
As a result, the method tends to wrongly associate those words with BUSINESS.
6 Related
Work HowNet (Dong and Dong, 2006) and WordNet provide domain information for Chinese and English, but there has been no domain resource for Japanese that are publicly available.8 Domain dictionary construction methods that have been developed so far are all based on highly structured lexical resources like LDOCE or WordNet (Guthrie et al., 1991; Agirre et al., 2001) and hence not applicable to languages for which such highly structured lexical resources are not available.
Accordingly, contributions of this study are twofold: (i) We constructed the first Japanese domain dictionary that is fully available.
(ii) We developed the domain dictionary construction method that requires neither document collections nor highly structured lexical resources.
8Some human-oriented dictionaries provide domain information.
However, domains they cover are all technical ones rather than common domains such as those assumed here.
7 Conclusion
Toward deeper natural language understanding, we constructed the first Japanese domain dictionary that contains 26,658 JFWs.
Our method requires neither document collections nor structured lexical resources.
The domain dictionary can satisfactorily classify blog articles into the 12 domains assumed in this study.
Also, the dictionary can reliably estimate the domain of unknown words except for words that are ambiguous in terms of domains and those that appear frequently in web sites of companies.
Among our future work is to deal with domain information of multiword expressions.
For example, a10a5a11 fount and a12a14a13 collection constitute a10a14a11 a12a14a13 tax deduction at source.
Note that while a10a5a11 itself belongs to NODOMAIN, a10a15a11 a12a14a13 should be associated with GOVERNMENT.
Also, we will install the domain dictionary on JUMAN (Kurohashi et al., 1994) to make the domain information fully and easily available.
References Eneko Agirre, Olatz Ansa, David Martinez, and Ed Hovy.
2001. Enriching wordnet concepts with topic signatures.
In Proceedings of the SIGLEX Workshop on ?WordNet and Other Lexical Resources: Applications, Extensions, and Customizations??in conjunction with NAACL.
Zhendong Dong and Qiang Dong.
2006. HowNet And the Computation of Meaning.
World Scientific Pub Co Inc.
Christiane Fellbaum.
1998. WordNet: An Electronic Lexical Database.
MIT Press.
Joe A.
Guthrie, Louise Guthrie, Yorick Wilks, and Homa Aidinejad.
1991. Subject-Dependent Co-Occurence and Word Sense Disambiguation.
In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 146??52.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto, and Makoto Nagao.
1994. Improvements of Japanese Mophological Analyzer JUMAN.
In Proceedings of the International Workshop on Sharable Natural Language Resources, pages 22??8.
Yasuhiro Sasaki, Satoshi Sato, and Takehito Utsuro.
2006. Related Term Collection.
Journal of Natural Language Processing, 13(3):151??76.
(in Japanese).
Yumiko Yoshimoto, Satoshi Kinoshita, and Miwako Shimazu.
1997. Processing of proper nouns and use of estimated subject area for web page translation.
In tmi97, pages 10??8, Santa Fe .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 141??44, Prague, June 2007.
c2007 Association for Computational Linguistics Extracting Word Sets with Non-Taxonomical Relation Eiko Yamamoto Hitoshi Isahara Computational Linguistics Group National Institute of Information and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan {eiko, isahara}@nict.go.jp Abstract At least two kinds of relations exist among related words: taxonomical relations and thematic relations.
Both relations identify related words useful to language understanding and generation, information retrieval, and so on.
However, although words with taxonomical relations are easy to identify from linguistic resources such as dictionaries and thesauri, words with thematic relations are difficult to identify because they are rarely maintained in linguistic resources.
In this paper, we sought to extract thematically (non-taxonomically) related word sets among words in documents by employing case-marking particles derived from syntactic analysis.
We then verified the usefulness of word sets with non-taxonomical relation that seems to be a thematic relation for information retrieval.
1. Introduction Related word sets are useful linguistic resources for language understanding and generation, information retrieval, and so on.
In previous research on natural language processing, many methodologies for extracting various relations from corpora have been developed, such as the ?is-a??relation (Hearst 1992), ?part-of??relation (Berland and Charniak 1999), causal relation (Girju 2003), and entailment relation (Geffet and Dagan 2005).
Related words can be used to support retrieval in order to lead users to high-quality information.
One simple method is to provide additional words related to the key words users have input, such as an input support function within the Google search engine.
What kind of relation between the key words that have been input and the additional word is effective for information retrieval?
As for the relations among words, at least two kinds of relations exist: the taxonomical relation and the thematic relation.
The former is a relation representing the physical resemblance among objects, which is typically a semantic relation such as a hierarchal, synonymic, or antonymic relation; the latter is a relation between objects through a thematic scene, such as ?milk??and ?cow??as recollected in the scene ?milking a cow,??and ?milk?? and ?baby,??as recollected in the scene ?giving baby milk,??which include causal relation and entailment relation.
Wisniewski and Bassok (1999) showed that both relations are important in recognizing those objects.
However, while taxonomical relations are comparatively easy to identify from linguistic resources such as dictionaries and thesauri, thematic relations are difficult to identify because they are rarely maintained in linguistic resources.
In this paper, we sought to extract word sets with a thematic relation from documents by employing case-marking particles derived from syntactic analysis.
We then verified the usefulness of word sets with non-taxonomical relation that seems to be a thematic relation for information retrieval.
2. Method In order to derive word sets that direct users to obtain information, we applied a method based on the Complementary Similarity Measure (CSM), which can determine a relation between two words in a corpus by estimating inclusive relations between two vectors representing each appearance pattern for each words (Yamamoto et al.2005). 141 We first extracted word pairs having an inclusive relation between the words by calculating the CSM values.
Extracted word pairs are expressed by a tuple <w i, w j >, where CSM(V i, V j ) is greater than CSM(V j, V i ) when words w i and w j have each appearance pattern represented by each binary vector V i and V j. Then, we connected word pairs with CSM values greater than a certain threshold and constructed word sets.
A feature of the CSM-based method is that it can extract not only pairs of related words but also sets of related words because it connects tuples consistently.
Suppose we have <A, B>, <B, C>, <Z, B>, <C, D>, <C, E>, and <C, F> in the order of their CSM values, which are greater than the threshold.
For example, let <B, C> be an initial word set {B, C}.
First, we find the tuple with the greatest CSM value among the tuples in which the word C at the tail of the current word set is the left word, and connect the right word behind C.
In this example, word ?D??is connected to {B, C} because <C, D> has the greatest CSM value among the three tuples <C, D>, <C, E>, and <C, F>, making the current word set {B, C, D}.
This process is repeated until no tuples exist.
Next, we find the tuple with the greatest CSM value among the tuples in which the word B at the head of the current word set is the right word, and connect the left word before B.
This process is repeated until no tuples exist.
In this example, we obtain the word set {A, B, C, D}.
Finally, we removed ones with a taxonomical relation by using thesaurus.
The rest of the word sets have a non-taxonomical relation ??including a thematic relation ??among the words.
We then extracted those word sets that do not agree with the thesaurus as word sets with a thematic relation.
3. Experiment In our experiment, we used domain-specific Japanese documents within the medical domain (225,402 sentences, 10,144 pages, 37MB) gathered from the Web pages of a medical school and the 2005 Medical Subject Headings (MeSH) thesaurus 1 . Recently, there has been a study on query expansion with this thesaurus as domain information (Friberg 2007).
1 The
U.S.
National Library of Medicine created, maintains, and provides the MeSH  thesaurus.
We extracted word sets by utilizing inclusive relations of the appearance pattern between words based on a modified/modifier relationship in documents.
The Japanese language has casemarking particles that indicate the semantic relation between two elements in a dependency relation.
Then, we collected from documents dependency relations matching the following five patterns; ?A <no (of)> B,???P <wo (object)> V,???Q <ga (subject)> V,???R <ni (dative)> V,??and ?S <ha (topic)> V,??where A, B, P, Q, R, and S are nouns, V is a verb, and <X> is a case-marking particle.
From such collected dependency relations, we compiled the following types of experimental data; NN-data based on co-occurrence between nouns for each sentence, NV-data based on a dependency relation between noun and verb for each case-marking particle <wo>, <ga>, <ni>, and <ha>, and SO-data based on a collocation between subject and object that depends on the same verb V as the subject.
These data are represented with a binary vector which corresponds to the appearance pattern of a noun and these vectors are used as arguments of CSM.
We translated descriptors in the MeSH thesaurus into Japanese and used them as Japanese medical terms.
The number of terms appearing in this experiment is 2,557 among them.
We constructed word sets consisting of these medical terms.
Then, we chose 977 word sets consisting of three or more terms from them, and removed word sets with a taxonomical relation from them with the MeSH thesaurus in order to obtain the rest 847 word sets as word sets with a thematic relation.
4. Verification In verifying the capability of our word sets to retrieve Web pages, we examined whether they could help limit the search results to more informative Web pages with Google as a search engine.
We assume that addition of suitable key words to the query reduces the number of pages retrieved and the remaining pages are informative pages.
Based on this assumption, we examined the decrease of the retrieved pages by additional key words and the contents of the retrieved pages in order to verify the availability of our word sets.
Among 847 word sets, we used 294 word sets in which one of the terms is classified into one category and the rest are classified into another.
142 ovary spleen palpation (NN) variation cross reactions outbreaks secretion (Wo) bleeding pyrexia hematuria consciousness disorder vertigo high blood pressure (Ga) space flight insemination immunity (Ni) cough fetus bronchiolitis obliterans organizing pneumonia (Ha) latency period erythrocyte hepatic cell (SO) Figure 1.
Examples of word sets used to verify.
Figure 1 shows examples of the word sets, where terms in a different category are underlined.
In retrieving Web pages for verification, we input the terms composed of these word sets into the search engine.
We created three types of search terms from the word set we extracted.
Suppose the extracted word set is {X 1, ..., X n, Y}, where X i is classified into one category and Y is classified into another.
The first type uses all terms except the one classified into a category different from the others: {X 1, ..., X n } removing Y.
The second type uses all terms except the one in the same category as the rest: {X 1, ..., X k-1, X k+1, ..., X n } removing X k from Type 1.
In our experiment, we removed the term X k with the highest or lowest frequency among X i . The third type uses terms in Type 2 and Y: {X 1, ..., X k-1, X k+1, ..., X n, Y}.
In other words, when we consider the terms in Type 2 as base key words, the terms in Type 1 are key words with the addition of one term having the highest or lowest frequency among the terms in the same category; i.e., the additional term has a feature related to frequency in the documents and is taxonomically related to other terms.
The terms in Type 3 are key words with the addition of one term in a category different from those of the other component terms; i.e., the additional term seems to be thematically related ??at least nontaxonomically related ??to other terms.
First, we quantitatively compared the retrieval results.
We used the estimated number of pages retrieved by Google?s search engine.
Suppose that we first input Type 2 as key words into Google, did not satisfy the result extracted, and added one word to the previous key words.
We then sought to determine whether to use Type 1 or Type 3 to obtain more suitable results.
The results are shown in Figures 2 and 3, which include the results for the highest frequency and the lowest frequency, respectively.
In these figures, the horizontal axis is the number of pages retrieved with Type 2 and the vertical axis is the number of pages retrieved when 1 10 100 1000 10000 100000 1000000 10000000 100000000 1 10 100 1000 10000 100000 1000000 10000000 100000000 1000000000 Number of Web pages retrieved with Type2 (base key words) Number of Web pages retrieved when a term is added to Type2 Type3: With additional term in a different category Type1: With additional term in same category Figure 2.
Fluctuation of number of pages retrieved (with the high frequency term).
NV Type of Data NN Wo Ga Ni Ha Word sets for verification 175 43 23 13 26 Cases in which Type 3 defeated Type 1 in retrieval 108 37 15 12 18 Table 1.
Number of cases in which Type 3 defeated Type 1 with the high frequency term.
a certain term is added to Type 2.
The circles (?? show the retrieval results with additional key word related taxonomically (Type 1).
The crosses () show the results with additional key word related non-taxonomically (Type 3).
The diagonal line shows that adding one term to the base key words does not affect the number of Web pages retrieved.
In Figure 2, most crosses fall further below the line.
This graph indicates that when searching by Google, adding a search term related nontaxonomically tends to make a bigger difference than adding a term related taxonomically and with high frequency.
This means that adding a term related non-taxonomically to the other terms is crucial to retrieving informative pages; that is, such terms are informative terms themselves.
Table 1 shows the number of cases in which term in different category decreases the number of hit pages more than high frequency term.
By this table, we found that most of the additional terms with high frequency contributed less than additional terms related non-taxonomically to decreasing the number of Web pages retrieved.
This means that, in comparison to the high frequency terms, which might not be so informative in themselves, the terms in the other category ??related nontaxonomically ??are effective for retrieving useful Web pages.
In Figure 3, most circles fall further below the line, in contrast to Figure 2.
This indicates that 143 Figure 3.
Fluctuation of number of pages retrieved (with the low frequency term).
NV Type of Data NN Wo Ga Ni Ha Word sets for verification 175 43 23 13 26 Cases in which Type 3 defeated Type 1 in retrieval 61 18 7 6 13 Table 2.
Number of cases in which Type 3 defeated Type 1 with the low frequency term.
adding a term related taxonomically and with low frequency tends to make a bigger difference than adding a term with high frequency.
Certainly, additional terms with low frequency would be informative terms, even though they are related taxonomically, because they may be rare terms on the Web and therefore the number of pages containing the term would be small.
Table 2 shows the number of cases in which term in different category decreases the number of hit pages more than low frequency term.
In comparing these numbers, we found that the additional term with low frequency helped to reduce the number of Web pages retrieved, making no effort to determine the kind of relation the term had with the other terms.
Thus, the terms with low frequencies are quantitatively effective when used for retrieval.
However, if we compare the results retrieved with Type 1 search terms and Type 3 search terms, it is clear that big differences exist between them.
For example, consider ?latency period erythrocyte hepatic cell??obtained from SO-data in Figure 1.
?Latency period??is classified into a category different from the other terms and ?hepatic cell?? has the lowest frequency in this word set.
When we used all the three terms, we obtained pages related to ?malaria??at the top of the results and the title of the top page was ?What is malaria???in Japanese.
With ?latency period??and ?erythrocyte,??we again obtained the same page at the top, although it was not at the top when we used ?erythrocyte??and ?hepatic cell??which have a taxonomical relation.
Type3: With additional term in a different category Type1: With additional term in same category 1 10 100 1000 10000 100000 1000000 10000000 As we showed above, the terms with thematic relations with other search terms are effective at directing users to informative pages.
Quantitatively, terms with a high frequency are not effective at reducing the number of pages retrieved; qualitatively, low frequency terms may not effective to direct users to informative pages.
We will continue our research in order to extract terms in thematic relation more accurately and verify the usefulness of them more quantitatively and qualitatively.
5. Conclusion We sought to extract word sets with a thematic relation from documents by employing casemarking particles derived from syntactic analysis.
We compared the results retrieved with terms related only taxonomically and the results retrieved with terms that included a term related nontaxonomically to the other terms.
As a result, we found adding term which is thematically related to terms that have already been input as key words is effective at retrieving informative pages.
References Berland, M.
and Charniak, E.
1999. Finding parts in very large corpora, In Proceedings of ACL 99, 57??4.
Friberg, K.
2007. Query expansion using domain information in compounds, In Proceedings of NAACL-HLT 2007 Doctoral Consortium, 1??.
Geffet, M.
and Dagan, I.
2005. The distribution inclusion hypotheses and lexical entailment.
In Proceedings of ACL 2005, 107??14.
Girju, R.
2003. Automatic detection of causal relations for question answering.
In Proceedings of ACL Workshop on Multilingual summarization and question answering, 76??14.
Hearst, M.
A. 1992, Automatic acquisition of hyponyms from large text corpora, In Proceedings of Coling 92, 539??45.
Wisniewski, E.
J. and Bassok.
M. 1999.
What makes a man similar to a tie?
Cognitive Psychology, 39: 208??238.
Yamamoto, E., Kanzaki, K., and Isahara, H.
2005. Extraction of hierarchies based on inclusion of co-occurring words with frequency information.
In Proceedings of IJCAI 2005, 1166??172 .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 145??48, Prague, June 2007.
c2007 Association for Computational Linguistics A Linguistic Service Ontology for Language Infrastructures Yoshihiko Hayashi Graduate School of Language and Culture, Osaka University 1-8 Machikaneyama-cho, Toyonaka, 560-0043 Japan hayashi@lang.osaka-u.ac.jp Abstract This paper introduces conceptual framework of an ontology for describing linguistic services on network-based language infrastructures.
The ontology defines a taxonomy of processing resources and the associated static language resources.
It also develops a sub-ontology for abstract linguistic objects such as expression, meaning, and description; these help define functionalities of a linguistic service.
The proposed ontology is expected to serve as a solid basis for the interoperability of technical elements in language infrastructures.
1 Introduction
Several types of linguistic services are currently available on the Web, including text translation and dictionary access.
A variety of NLP tools is also available and public.
In addition to these, a number of community-based language resources targeting particular domains of application have been developed, and some of them are ready for dissemination.
A composite linguistic service tailored to a particular user's requirements would be composable, if there were a language infrastructure on which elemental linguistic services, such as NLP tools, and associated language resources could be efficiently combined.
Such an infrastructure should provide an efficient mechanism for creating workflows of composite services by means of authoring tools for the moment, and through an automated planning in the future.
To this end, technical components in an infrastructure must be properly described, and the semantics of the descriptions should be defined based on a shared ontology.
2 Architecture
of a Language Infrastructure The linguistic service ontology described in this paper has not been intended for a particular language infrastructure.
However we expect that the ontology should be first introduced in an infrastructure like the Language Grid 1, because it, unlike other research-oriented infrastructures, tries to incorporate a wide range of NLP tools and community-based language resources (Ishida, 2006) in order to be useful for a range of intercultural collaboration activities.
The fundamental technical components in the Language Grid could be: (a) external web-based services, (b) on-site NLP core functions, (c) static language resources, and (d) wrapper programs.
Figure 1 depicts the general architecture of the infrastructure.
The technical components listed above are deployed as shown in the figure.
Computational nodes in the language grid are classified into the following two types as described in (Murakami et al., 2006).
ces. Tthe most important desideratum for the ontology, therefore, is that it be able to specify the input/output constraints of a linguistic service properly.
Such input/output specifications enable us to derive a taxonomy of linguistic service and the associated language resources.
3 The
Upper Ontology 3.1 The top level We have developed the upper part of the service ontology so far, and have been working on detailing some of its core parts.
Figure 2 shows the top level of the proposed linguistic service ontology.
Figure 2.
The Top Level of the Ontology.
The topmost class is NL_Resource, which is partitioned into ProcessingResource, and LanguageResource.
Here, as in GATE (Cunningham, 2002), processing resource refers to programmatic or algorithmic resources, while language resource refers to data-only static resources such as lexicons or corpora.
The innate relation between these two classes is: a processing resource can use language resources.
This relationship is specifically introduced to properly define linguistic services that are intended to provide access functions to language resources.
As shown in the figure, LinguisticService is provided by a processing resource, stressing that any linguistic service is realized by a processing resource, even if its prominent functionality is accessing language resources in response to a user?s query.
It also has the meta-information for advertising its non-functional descriptions.
The fundamental classes for abstract linguistic objects, Expression, Meaning, and Description and the innate relations among them are illustrated in Figure 3.
These play roles in defining functionalities of some types of processing resources and associated language resources.
As shown in Fig.
3, an expression may denote a meaning, and the meaning can be further described by a description, especially for human uses.
Figure 3.
Classes for Abstract Linguistic Objects.
In addition to these, NLProcessedStatus and LinguisticAnnotation are important in the sense that NLP status represents the so-called IOPE (Input-Output-Precondition-Effect) parameters of a linguistic processor, which is a subclass of the processing resource, and the data schema for the results of a linguistic analysis is defined by using the linguistic annotation class.
3.2 Taxonomy
of language resources The language resource class currently is partitioned into subclasses for Corpus and Dictionary.
The immediate subclasses of the dictionary class are: (1) MonolingualDictionary, (2) BihasNLProcessedStatus* NLP Tool Linguistic Service External Linguistic Service Language Resource Access Mechanism Language Resource maintains -profiles registry -workflows Core Node Service Node Application Program wrapper 146 lingualDictionary, (3) MultilingualTerminology, and (4) ConceptLexicon.
The major instances of (1) and (2) are so-called machine-readable dictionaries (MRDs).
Many of the community-based special language resources should fall into (3), including multilingual terminology lists specialized for some application domains.
For subclass (4), we consider the computational concept lexicons, which can be modeled by a WordNet-like encoding framework (Hayashi and Ishida, 2006).
3.3 Taxonomy
of processing resources The top level of the processing resource class consists of the following four subclasses, which take into account the input/output constraints of processing resources, as well as the language resources they utilize.
urce it accesses.
The input to a language resource accessor is a query (LR_AccessQuery, sub-class of Expression), and the output is a kind of ?dictionary meaning??(DictionaryMeaning), which is a sub-class of meaning class.
The dictionary meaning class is further divided into sub-classes by referring to the taxonomy of dictionary.
notations by itself or by incorporating some external standard, such as LAF (Ide and Romary, 2004).
3.5 NLP
status and the associated issues Figure 5 illustrates our working taxonomy of NLP processed status.
Note that, in this figure, only the portion related to linguistic analyzer is detailed.
Benefits from the NLP status class will be twofold: (1) as a part of the description of a linguistic analyzer, we assign corresponding instances of this class as its precondition/effect parameters, (2) any instance of the expression class can be concisely 147 ?tagged??by instances of the NLP status class, according to how ?deeply??the expression has been linguistically analyzed so far.
Essentially, such information can be retrieved from the attached linguistic annotations.
In this sense, the NLP status class might be redundant.
Tagging an instance of expression in that way, however, can be reasonable: we can define the input/output constraints of a linguistic analyzer concisely with this device.
Figure 5.
Taxonomy of NLP Status.
Each subclass in the taxonomy represents the type or level of a linguistic analysis, and the hierarchy depicts the processing constraints among them.
For example, if an expression has been parsed, it would already have been morphologically analyzed, because parsing usually requires the input to be morphologically analyzed beforehand.
The subsumption relations encoded in the taxonomy allow simple reasoning in possible composite service composition processes.
However note that the taxonomy is only preliminary.
The arrangement of the subclasses within the hierarchy may end up being far different, depending on the languages considered, and the actual NLP tools, these are essentially idiosyncratic, that are at hand.
For example, the notion of ?chunk??may be different from language to language.
Despite of these, if we go too far in this direction, constructing a taxonomy would be meaningless, and we would forfeit reasonable generalities.
4 Related
Works Klein and Potter (2004) have once proposed an ontology for NLP services with OWL-S definitions.
Their proposal however has not included detailed taxonomies either for language resources, or for abstract linguistic objects, as shown in this paper.
Graa, et al.(2006) introduced a framework for integrating NLP tools with a client-server architecture having a multi-layered repository.
They also proposed a data model for encoding various types of linguistic information.
However the model itself is not ontologized as proposed in this paper.
5 Concluding
Remarks Although the proposed ontology successfully defined a number of first class objects and the innate relations among them, it must be further refined by looking at specific NLP tools/systems and the associated language resources.
Furthermore, its effectiveness in composition of composite linguistic services or wrapper generation should be demonstrated on a specific language infrastructure such as the Language Grid.
Acknowledgments The presented work has been partly supported by NICT international joint research grant.
The author would like to thank to Thierry Declerck and Paul Buitelaar (DFKI GmbH, Germany) for their helpful discussions.
References H.
Cunningham, et al.2002. GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications.
Proc. of ACL 2002, pp.168-175.
J. Graa, et al.2006. NLP Tools Integration Using a Multi-Layered Repository.
Proc. of LREC 2006 Workshop on Merging and Layering Linguistic Information.
Y. Hayashi and T.
Ishida. 2006.
A Dictionary Model for Unifying Machine Readable Dictionaries and Computational Concept Lexicons.
Proc. of LREC 2006, pp.1-6.
N. Ide and L.
Romary. 2004.
International Standard for a Linguistic Annotation Framework.
Journal of Natural Language Engineering, Vol.10:3-4, pp.211-225.
T. Ishida.
2006. Language Grid: An Infrastructure for Intercultural Collaboration.
Proc. of SAINT-06, pp.
96-100, keynote address.
E. Klein and S.
Potter. 2004.
An Ontology for NLP Services.
Proc. of LREC 2004 Workshop on Registry of Linguistic Data Categories.
Y. Murakami, et al.2006. Infrastructure for Language Service Composition.
Proc. of
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 149??52, Prague, June 2007.
c2007 Association for Computational Linguistics Empirical Measurements of Lexical Similarity in Noun Phrase Conjuncts Deirdre Hogan??
Department of Computer Science Trinity College Dublin Dublin 2, Ireland dhogan@computing.dcu.ie Abstract The ability to detect similarity in conjunct heads is potentially a useful tool in helping to disambiguate coordination structures a difficult task for parsers.
We propose a distributional measure of similarity designed for such a task.
We then compare several different measures of word similarity by testing whether they can empirically detect similarity in the head nouns of noun phrase conjuncts in the Wall Street Journal (WSJ) treebank.
We demonstrate that several measures of word similarity can successfully detect conjunct head similarity and suggest that the measure proposed in this paper is the most appropriate for this task.
1 Introduction
Some noun pairs are more likely to be conjoined than others.
Take the follow two alternate bracketings: 1.
busloads of ((executives) and (their spouses)) and 2.
((busloads of executives) and (their spouses)).
The two head nouns coordinated in 1 are executives and spouses, and (incorrectly) in 2: busloads and spouses.
Clearly, the former pair of head nouns is more likely and, for the purpose of discrimination, a parsing model would benefit if it could learn that executives and spouses is a more likely combination than busloads and spouses.
If nouns co-occurring in coordination patterns are often semantically similar, and if a simi??Now at the National Centre for Language Technology, Dublin City University, Ireland.
larity measure could be defined so that, for example: sim(executives, spouses) > sim(busloads, spouses) then it is potentially useful for coordination disambiguation.
The idea that nouns co-occurring in conjunctions tend to be semantically related has been noted in (Riloff and Shepherd, 1997) and used effectively to automatically cluster semantically similar words (Roark and Charniak, 1998; Caraballo, 1999; Widdows and Dorow, 2002).
The tendency for conjoined nouns to be semantically similar has also been exploited for coordinate noun phrase disambiguation by Resnik (1999) who employed a measure of similarity based on WordNet to measure which were the head nouns being conjoined in certain types of coordinate noun phrase.
In this paper we look at different measures of word similarity in order to discover whether they can detect empirically a tendency for conjoined nouns to be more similar than nouns which co-occur but are not conjoined.
In Section 2 we introduce a measure of word similarity based on word vectors and in Section 3 we briefly describe some WordNet similarity measures which, in addition to our word vector measure, will be tested in the experiments of Section 4.
2 Similarity
based on Coordination Co-occurrences The potential usefulness of a similarity measure depends on the particular application.
An obvious place to start, when looking at similarity functions for measuring the type of semantic similarity common for coordinate nouns, is a similarity function based on distributional similarity with context de149 fined in terms of coordination patterns.
Our measure of similarity is based on noun co-occurrence information, extracted from conjunctions and lists.
We collected co-occurrence data on 82, 579 distinct word types from the BNC and the WSJ treebank.
We extracted all noun pairs from the BNC which occurred in a pattern of the form: noun cc noun1, as well as lists of any number of nouns separated by commas and ending in cc noun.
Each noun in the list is linked with every other noun in the list.
Thus for a list: n1, n2, and n3, there will be co-occurrences between words n1 and n2, between n1 and n3 and between n2 and n3.
To the BNC data we added all head noun pairs from the WSJ (sections 02 to 21) that occurred together in a coordinate noun phrase.2 From the co-occurrence data we constructed word vectors.
Every dimension of a word vector represents another word type and the values of the components of the vector, the term weights, are derived from the coordinate word co-occurrence counts.
We used dampened co-occurrence counts, of the form: 1 + log(count), as the term weights for the word vectors.
To measure the similarity of two words, w1 and w2, we calculate the cosine of the angle between the two word vectors, vectorw1 and vectorw2.
3 WordNet-Based Similarity Measures We also examine the following measures of semantic similarity which are WordNet-based.3 Wu and Palmer (1994) propose a measure of similarity of two concepts c1 and c2 based on the depth of concepts in the WordNet hierarchy.
Similarity is measured from the depth of the most specific node dominating both c1 and c2, (their lowest common subsumer), and normalised by the depths of c1 and c2.
In (Resnik, 1995) concepts in WordNet are augmented by corpus statistics and an informationtheoretic measure of semantic similarity is calculated.
Similarity of two concepts is measured 1It would be preferable to ensure that the pairs extracted are unambiguously conjoined heads.
We leave this to future work.
2We did not include coordinate head nouns from base noun phrases (NPB) (i.e.
noun phrases that do not dominate other noun phrases) because the underspecified annotation of NPBs in the WSJ means that the conjoined head nouns can not always be easily identified.
3All of the WordNet-based similarity measure experiments, as well as a random similarity measure, were carried out with the WordNet::Similarity package, http://search.cpan.org/dist/WordNet-Similarity.
by the information content of their lowest common subsumer in the is-a hierarchy of WordNet.
Both Jiang and Conrath (1997) and Lin (1998) propose extentions of Resnik?s measure.
Leacock and Chodorow (1998)?s measure takes into account the path length between two concepts, which is scaled by the depth of the hierarchy in which they reside.
In (Hirst and St-Onge, 1998) similarity is based on path length as well as the number of changes in the direction in the path.
In (Banerjee and Pedersen, 2003) semantic relatedness between two concepts is based on the number of shared words in their WordNet definitions (glosses).
The gloss of a particular concept is extended to include the glosses of other concepts to which it is related in the WordNet hierarchy.
Finally, Patwardhan and Pederson (2006) build on previous work on second-order co-occurrence vectors (Schutze, 1998) by constructing second-order co-occurrence vectors from WordNet glosses, where, as in (Banerjee and Pedersen, 2003), the gloss of a concept is extended so that it includes the gloss of concepts to which it is directly related in WordNet.
4 Experiments
We selected two sets of data from sections 00, 01, 22 and 24 of the WSJ treebank.
The first consists of all nouns pairs which make up the head words of two conjuncts in coordinate noun phrases (again not including coordinate NPBs).
We found 601 such coordinate noun pairs.
The second data set consists of 601 word pairs which were selected at random from all head-modifier pairs where both head and modifier words are nouns and are not coordinated.
We tested the 9 different measures of word similarity just described on each data set in order to see if a significant difference could be detected between the similarity scores for the coordinate words sample and non-coordinate words sample.
Initially both the coordinate and non-coordinate pair samples each contained 601 word pairs.
However, before running the experiments we removed all pairs where the words in the pair were identical.
This is because identical words occur more often in coordinate head words than in other lexical dependencies (there were 43 pairs with identical words in the coordination set, compared to 3 such pairs in the 150 SimTest ncoord xcoord SDcoord nnonCoord xnonCoord SDnonCoord 95% CI p-value coordDistrib 503 0.11 0.13 485 0.06 0.09 [0.04 0.07] 0.000 (Resnik, 1995) 444 3.19 2.33 396 2.43 2.10 [0.46 1.06] 0.000 (Lin, 1998) 444 0.27 0.26 396 0.19 0.22 [0.04 0.11] 0.000 (Jiang and Conrath, 1997) 444 0.13 0.65 395 0.07 0.08 [-0.01 0.11] 0.083 (Wu and Palmer, 1994) 444 0.63 0.19 396 0.55 0.19 [0.06 0.11] 0.000 (Leacock and Chodorow, 1998) 444 1.72 0.51 396 1.52 0.47 [0.13 0.27] 0.000 (Hirst and St-Onge, 1998) 459 1.599 2.03 447 1.09 1.87 [0.25 0.76] 0.000 (Banerjee and Pedersen, 2003) 451 114.12 317.18 436 82.20 168.21 [-1.08 64.92] 0.058 (Patwardhan and Pedersen, 2006) 459 0.67 0.18 447 0.66 0.2 [-0.02 0.03] 0.545 random 483 0.89 0.17 447 0.88 0.18 [-0.02 0.02] 0.859 Table 1: Summary statistics for 9 different word similarity measures (plus one random measure):ncoord and nnonCoord are the sample sizes for the coordinate and non-coordinate noun pairs samples, respectively; xcoord, SDcoord and xnonCoord, SDnonCoord are the sample means and standard deviations for the two sets.
The 95% CI column shows the 95% confidence interval for the difference between the two sample means.
The p-value is for a Welch two sample two-sided t-test.
coordDistrib is the measure introduced in Section 2.
non-coordination set).
If we had not removed them, a statistically significant difference between the similarity scores of the pairs in the two sets could be found simply by using a measure which, say, gave one score for identical words and another (lower) score for all non-identical word pairs.
Results for all similarity measure tests on the data sets described above are displayed in Table 1.
In one final experiment we used a random measure of similarity.
For each experiment we produced two samples, one consisting of the similarity scores given by the similarity measure for the coordinate noun pairs, and another set of similarity scores generated for the non-coordinate pairs.
The sample sizes, means, and standard deviations for each experiment are shown in the table.
Note that the variation in the sample size is due to coverage: the different measures did not produce a score for all word pairs.
Also displayed in Table 1 are the results of statistical significance tests based on the Welsh two sample t-test.
A 95% confidence interval for the difference of the sample means is shown along with the p-value.
5 Discussion
For all but three of the experiments (excluding the random measure), the difference between the mean similarity measures is statistically significant.
Interestingly, the three tests where no significant difference was measured between the scores on the coordination set and the non-coordination set (Jiang and Conrath, 1997; Banerjee and Pedersen, 2003; Patwardhan and Pedersen, 2006) were the three top scoring measures in (Patwardhan and Pedersen, 2006), where a subset of six of the above WordNetbased experiments were compared and the measures evaluated against human relatedness judgements and in a word sense disambiguation task.
In another comparative study (Budanitsky and Hirst, 2002) of five of the above WordNet-based measures, evaluated as part of a real-word spelling correction system, Jiang and Conrath (1997)?s similarity score performed best.
Although performing relatively well under other evaluation criteria, these three measures seem less suited to measuring the kind of similarity occurring in coordinate noun pairs.
One possible explanation for the unsuitability of the measures of (Patwardhan and Pedersen, 2006) for the coordinate similarity task could be based on how context is defined when building context vectors.
Context for an instance of the the word w is taken to be the words that surround w in the corpus within a given number of positions, where the corpus is taken as all the glosses in WordNet.
Words that form part of collocations such as disk drives or task force would then tend to have very similar contexts, and thus such word pairs, from non-coordinate modifier-head relations, could be given too high a similarity score.
Although the difference between the mean similarity scores seems rather slight in all experiments, it is worth noting that not all coordinate head words are semantically related.
To take a couple of examples from the coordinate word pair set: work/harmony extracted from hard work and harmony, and power/clause extracted from executive power and the appropriations clause.
We would not expect these word pairs to get a high similarity score.
On the other hand, it is also possible that 151 some of the examples of non-coordinate dependencies involve semantically similar words.
For example, nouns in lists are often semantically similar, and we did not exclude nouns extracted from lists from the non-coordinate test set.
Although not all coordinate noun pairs are semantically similar, it seems clear, on inspection of the two sets of data, that they are more likely to be semantically similar than modifier-head word pairs, and the tests carried out for most of the measures of semantic similarity detect a significant difference between the similarity scores assigned to coordinate pairs and those assigned to non-coordinate pairs.
It is not possible to judge, based on the significance tests alone, which might be the most useful measure for the purpose of disambiguation.
However, in terms of coverage, the distributional measure introduced in Section 2 clearly performs best4.
This measure of distributional similarity is perhaps more suited to the task of coordination disambiguation because it directly measures the type of similarity that occurs between coordinate nouns.
That is, the distributional similarity measure presented in Section 2 defines two words as similar if they occur in coordination patterns with a similar set of words and with similar distributions.
Whether the words are semantically similar becomes irrelevant.
A measure of semantic similarity, on the other hand, might find words similar which are quite unlikely to appear in coordination patterns.
For example, Cederberg and Widdows (2003) note that words appearing in coordination patterns tend to be on the same ontological level: ?fruit and vegetables??is quite likely to occur, whereas ?fruit and apples??is an unlikely cooccurrence.
A WordNet-based measure of semantic similarity, however, might give a high score to both of the noun pairs.
In the future we intend to use the similarity measure outlined in Section 2 in a lexicalised parser to help resolve coordinate noun phrase ambiguities.
Acknowledgements Thanks to the TCD Broad Curriculum Fellowship and to the SFI Research Grant 04/BR/CS370 for funding this research.
Thanks also to Padraig Cunningham, Saturnino Luz and Jennifer Foster for helpful discussions.
4Somewhat unsurprisingly given it is part trained on data from the same domain.
References Satanjeev Banerjee and Ted Pedersen.
2003 Extended Gloss Overlaps as a Measure of Semantic Relatedness.
In Proceeding of the 18th IJCAI.
Alexander Budanitsky and Graeme Hirst.
2002 Semantic Distance in WordNet: An experimental, application-oriented Evaluation of Five Measures In Proceedings of the 3rd CICLING.
Sharon Caraballo.
1999 Automatic construction of a hypernym-labeled noun hierarchy from text In Proceedings of the 37th ACL.
Scott Cederberg and Dominic Widdows.
2003. Using LSA and Noun Coordination Information to Improve the Precision and Recall of Automatic Hyponymy Extraction.
In Proceedings of the 7th CoNLL.
G. Hirst and D.
St-Onge 1998.
Lexical Chains as representations of context for the detection and correction of malapropisms.
WordNet: An electronic lexical database.
MIT Press.
J. Jiang and D.
Conrath. 1997.
Semantic similarity based on corpus statistics and lexical taxonomy.
In Proceedings of the ROCLING.
C. Leacock and M.
Chodorow. 1998.
Combining local context and WordNet similarity for word sense identification.
Word-Net: An electronic lexical database.
MIT Press.
D. Lin.
1998. An information-theoretic definition of similarity.
In Proceedings of the 15th ICML.
Siddharth Patwardhan and Ted Pedersen.
2006. Using WordNet-based Context Vectors to Estimate the Semantic Relatedness of Concepts.
In Proceedings of Making Sense of Sense Bringing Computational Linguistics and Psycholinguistics Together, EACL.
Philip Resnik.
1995. Using Information Content to Evaluate Semantic Similarity.
In Proceedings of IJCAI.
Philip Resnik.
1999. Semantic Similarity in a Taxonomy: An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language.
In Journal of Artificial Intelligence Research, 11:95-130.
Ellen Riloff and Jessica Shepherd 1997.
A Corpus-based Approach for Building Semantic Lexicon.
In Proceedings of the 2nd EMNLP.
Brian Roark and Eugene Charniak 1998.
Noun-phrase Co-occurrence Statistics for Semi-automatic semantic lexicon construction.
In Proceedings of the COLING-ACL.
Hinrich Schutze.
1998. Automatic Word Sense Discrimination.
Computational Linguistics, 24(1):97-123.
Dominic Widdows and Beate Dorow.
2002. A Graph Model for Unsupervised Lexical Acquisition.
In Proceedings of the 19th COLING.
Zhibiao Wu and Martha Palmer.
1994. Verb Semantics and Lexical Selection.
In Proceedings of the ACL.
kProceedings of the ACL 2007 Demo and Poster Sessions, pages 153??56, Prague, June 2007.
c2007 Association for Computational Linguistics Automatic Discovery of Named Entity Variants ??Grammar-driven Approaches to Non-alphabetical Transliterations Chu-Ren Huang Institute of Linguistics Academia Sinica, Taiwan churenhuang@gmail.com Petr ?Simon Institute of Linguistics Academia Sinica, Taiwan sim@klubko.net Shu-Kai Hsieh DoFLAL NIU, Taiwan shukai@gmail.com Abstract Identification of transliterated names is a particularly difficult task of Named Entity Recognition (NER), especially in the Chinese context.
Of all possible variations of transliterated named entities, the difference between PRC and Taiwan is the most prevalent and most challenging.
In this paper, we introduce a novel approach to the automatic extraction of diverging transliterations of foreign named entities by bootstrapping cooccurrence statistics from tagged and segmented Chinese corpus.
Preliminary experiment yields promising results and shows its potential in NLP applications.
1 Introduction
Named Entity Recognition (NER) is one of the most difficult problems in NLP and Document Understanding.
In the field of Chinese NER, several approaches have been proposed to recognize personal names, date/time expressions, monetary and percentage expressions.
However, the discovery of transliteration variations has not been well-studied in Chinese NER.
This is perhaps due to the fact that the transliteration forms in a non-alphabetic language such as Chinese are opaque and not easy to compare.
On the hand, there is often more than one way to transliterate a foreign name.
On the other hand, dialectal difference as well as different transliteration strategies often lead to the same named entity to be transliterated differently in different Chinese speaking communities.
Corpus Example (Clinton) Frequency XIN ????24382 CNA ????150 XIN ??0 CNA ??120842 Table 1: Distribution of two transliteration variants for ?Clinton??in two sub-corpora Of all possible variations, the cross-strait difference between PRC and Taiwan is the most prevalent and most challenging.1The main reason may lie in the lack of suitable corpus.
Even given some subcorpora of PRC and Taiwan variants of Chinese, a simple contrastive approach is still not possible.
It is because: (1) some variants might overlap and (2) there are more variants used in each corpus due to citations or borrowing crossstrait.
Table 1 illustrates this phenomenon, where CNA stands for Central News Agency in Taiwan, XIN stands for Xinhua News Agency in PRC, respectively.
With the availability of Chinese Gigaword Corpus (CGC) and Word Sketch Engine (WSE) Tools (Kilgarriff, 2004).
We propose a novel approach towards discovery of transliteration variants by utilizing a full range of grammatical information augmented with phonological analysis.
Existing literatures on processing of transliteration concentrate on the identification of either the transliterated term or the original term, given knowledge of the other (e.g.
(Virga and Khudanpur, 1For instance, we found at least 14 transliteration variants for Lewinsky,such as ?fl ?fl fl?fl?
???? fl?fl fl flfl?fl  fl and so on.
153 2003)).
These studies are typically either rule-based or statistics-based, and specific to a language pair with a fixed direction (e.g.
(Wan and Verspoor, 1998; Jiang et al., 2007)).
To the best of our knowledge, ours is the first attempt to discover transliterated NE?s without assuming prior knowledge of the entities.
In particular, we propose that transliteration variants can be discovered by extracting and comparing terms from similar linguistic context based on CGC and WSE tools.
This proposal has great potential of increasing robustness of future NER work by enabling discovery of new and unknown transliterated NE?s.
Our study shows that resolution of transliterated NE variations can be fully automated.
This will have strong and positive implications for cross-lingual and multi-lingual informational retrieval.
2 Bootstrapping
transliteration pairs The current study is based on Chinese Gigaword Corpus (CGC) (Graff el al., 2005), a large corpus contains with 1.1 billion Chinese characters containing data from Central News Agency of Taiwan (ca.
700 million characters), Xinhua News Agency of PRC (ca.
400 million characters).
These two subcorpora represent news dispatches from roughly the same period of time, i.e. 1990-2002.
Hence the two sub-corpora can be expected to have reasonably parallel contents for comparative studies.2 The premises of our proposal are that transliterated NE?s are likely to collocate with other transliterated NE?s, and that collocates of a pair of transliteration variants may form contrasting pairs and are potential variants.
In particular, since the transliteration variations that we are interested in are those between PRC and Taiwan Mandarin, we will start with known contrasting pairs of these two language variants and mine potential variant pairs from their collocates.
These potential variant pairs are then checked for their phonological similarity to determine whether they are true variants or not.
In order to effectively select collocates from specific grammatical constructions, the Chinese Word Sketch3 is adopted.
In particular, we use the Word Sketch dif2To facilitate processing, the complete CGC was segmented and POS tagged using the Academia Sinica segmentation and tagging system (Ma and Huang, 2006).
3http://wordsketch.ling.sinica.edu.tw ference (WSDiff) function to pick the grammatical contexts as well as contrasting pairs.
It is important to bear in mind that Chinese texts are composed of Chinese characters, hence it is impossible to compare a transliterated NE with the alphabetical form in its original language.
The following characteristics of a transliterated NE?s in CGC are exploited to allow discovery of transliteration variations without referring to original NE.
??frequent co-occurrence of named entities within certain syntagmatic relations ??named entities frequently co-occur in relations such as AND or OR and this fact can be used to collect and score mutual predictability.
??foreign named entities are typically transliterated phonetically ??transliterations of the same name entity using different characters can be matched by using simple heuristics to map their phonological value.
??presence and co-occurrence of named entities in a text is dependent on a text type ??journalistic style cumulates many foreign named entities in close relations.
??many entities will occur in different domains ??famous person can be mentioned together with someone from politician, musician, artist or athlete.
Thus allows us to make leaps from one domain to another.
There are, however, several problems with the phonological representation of foreign named entities in Chinese.
Due to the nature of Chinese script, NE transliterations can be realized very differently.
The following is a summary of several problems that have to be taken into account: ??word ending: ?? vs.??
???Arafat??or ? vs.?
???Mubarak?? The final consonant is not always transliterated.
XIN transliterations tend to try to represent all phonemes and often add vowels to a final consonant to form a new syllable, whereas CNA transliteration tends to be shorter and may simply leave out a final consonant.
??gender dependent choice of characters: ? ?Leslie??vs.?fl???Chris??or ?? vs.
??fl 154 ??
Some occidental names are gender neutral.
However, the choice of characters in a personal name in Chinese is often gender sensitive.
So these names are likely to be transliterated differently depending on the gender of its referent.
??divergent representations caused by scope of transliteration, e.g. both given and surname vs.
only surname:  ?/ ? ??Venus Williams??
??difference in phonological interpretation: ?
?vs. ?Rafter??or??flvs.
?fl?Connors?? ??native vs.
non-native pronunciation: ?fl?? vs.
fl???Escudero??or ? vs.
?? ?Federer?? 2.1 Data collection All data were collected from Chinese Gigaword Corpus using Chinese Sketch Engine with WSDiff function, which provides side-by-side syntagmatic comparison of Word Sketches for two different words.
WSDiff query for wi and wj returns patterns that are common for both words and also patterns that are particular for each of them.
Three data sets are thus provided.
We neglect the common patterns set and concentrate only on the wordlists specific for each word.
2.2 Pairs
extraction Transliteration pairs are extracted from the two sets, A and B, collected with WSDiff using default set of seed pairs : for each seed pair in seeds retrieve WSDiff for and/or relation, thus have pairs of word lists, < Ai,Bi > for each word wii ??Ai find best matching counterpart(s) wij ??Bi.
Comparison is done using simple phonological rules, viz.
2.3 use newly extracted pairs as new seeds (original seeds are stored as good pairs and not queried any more) loop until there are no new pairs Notice that even though substantial proportion of borrowing among different communities, there is no mixing in the local context of collocation, which means, local collocation could be the most reliable way to detect language variants with known variants.
2.3 Phonological
comparison All word forms are converted from Chinese script into a phonological representation4 during the pairs extraction phase and then these representations are compared and similarity scores are given to all pair candidates.
A lot of Chinese characters have multiple pronunciations and thus multiple representations are derived.
In case of multiple pronunciations for certain syllable, this syllable is commpared to its counterpart from the other set.
E.g. (??has three pronunciations: y`e, xie, sh`e.
When comparing syllables such as [pei,fei] and [fei], will be represented as [fei].
In case of pairs such as ??[ye er qin] and ? [ye er qin], which have syllables with multiple pronunciations and this multiple representations.
However, since these two potential variants share the first two characters (out of three), they are considered as variants without superfluous phonological checking.
Phonological representations of whole words are then compared by Levenstein algorithm, which is widely used to measure the similarity between two strings.
First, each syllable is split into initial and final components: gao:g+ao.
In case of syllables without initials like er, an ??is inserted before the syllable, thus er:??er.
Before we ran the Levenstein measure, we also apply phonological corrections on each pair of candidate representations.
Rules used for these corrections are derived from phonological features of Mandarin Chinese and extended with few rules from observation of the data: (1) For Initials, (a): voiced/voiceless stop contrasts are considered as similar for initials: g:k, e.g.
[gao] ( ?? vs.
[ke] ( ??,d:t, b:p, (b): r:l ??[rui] ( ??
??[lie] ( ?) is added to distinctive feature set based on observation.
(2). For Finals, (a): pair ei:ui is evaluated as equivalent.5 (b): oppositions of nasalised final is evaluated as dissimilar.
4http://unicode.org/charts/unihan.html 5Pinyin representation of phonology of Mandarin Chinese does not follow the phonological reality exactly: [ui] = [uei] etc.
155 2.4 Extraction algorithm Our algorithm will potentially exhaust the whole corpus, i.e. find most of the named entities that occur with at least few other names entities, but only if seeds are chosen wisely and cover different domains6.
However, some domains might not overlap at all, that is, members of those domains never appear in the corpus in relation and/or.
And concurrence of members within some domains might be sparser than in other, e.g. politicians tend to be mentioned together more often than novelists.
Nature of the corpus also plays important role.
It is likely to retrieve more and/or related names from journalistic style.
This is one of the reasons why we chose Chinese Gigaword Corpus for this task.
3 Experiment
and evaluation We have tested our method on the Chinese Gigaword Second Edition corpus with 11 manually selected seeds Apart from the selection of the starter seeds, the whole process is fully automatic.
For this task we have collected data from syntagmatic relation and/or, which contains words co-occurring frequently with our seed words.
When we make a query for peoples names, it is expected that most of the retrieved items will also be names, perhaps also names of locations, organizations etc.
The whole experiment took 505 iterations in which 494 pairs were extracted.
Our complete experiment with 11 pre-selected transliteration pairs as seed took 505 iterations to end.
The iterations identified 494 effective transliteration variant pairs (i.e.
those which were not among the seeds or pairs identified by earlier iteration).
All the 494 candidate pairs were manually evaluated 445 of them are found to be actual contrast pairs, a precision of 90.01%.
In addition, the number of new transliteration pairs yielded is 4,045%, a very productive yield for NE discovery.
Preliminary results show that this approach is competitive against other approaches reported in previous studies.
Performances of our algorithms is calculated in terms of precision rate with 90.01%. 6The term domain refers to politics,music,sport, film etc.
4 Conclusion
and Future work In this paper, we have shown that it is possible to identify NE?s without having prior knowledge of them.
We also showed that, applying WSE to restrict grammatical context and saliency of collocation, we are able to effectively extract transliteration variants in a language where transliteration is not explicitly represented.
We also show that a small set of seeds is all it needs for the proposed method to identify hundreds of transliteration variants.
This proposed method has important applications in information retrieval and data mining in Chinese data.
In the future, we will be experimenting with a different set of seeds in a different domain to test the robustness of this approach, as well as to discover transliteration variants in our fields.
We will also be focusing on more refined phonological analysis.
In addition, we would like to explore the possibility of extending this proposal to other language pairs.
References Jiang, L.
and M.Zhou and L.f.
Chien. 2007.
Named Entity Discovery based on Transliteration and WWW [In Chinese].
Journal of the Chinese Information Processing Society.
2007 no.1. pp.23-29.
Graff, David et al.2005. Chinese Gigaword Second Edition.
Linguistic Data Consortium, Philadelphia.
Ma, Wei-Yun and Huang, Chu-Ren.
2006. Uniform and Effective Tagging of a Heterogeneous Giga-word Corpus.
Presented at the 5th International Conference on Language Resources and Evaluation (LREC2006), 24-28 May.
Genoa, Italy.
Kilgarriff, Adam et al.2004. The Sketch Engine.
Proceedings of EURALEX 2004.
Lorient, France.
Paola Virga and Sanjeev Khudanpur.
2003. Transliteration of proper names in cross-lingual information retrieval.
In Proc.
of the ACL Workshop on Multi-lingual Named Entity Recognition, pp.57-64.
Wan, Stephen and Cornelia Verspoor.
1998. Automatic English-Chinese Name Transliteration for Development of Multiple Resources.
In Proc.
of COLING/ACL, pp.1352-1356.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 157??60, Prague, June 2007.
c2007 Association for Computational Linguistics Detecting Semantic Relations between Named Entities in Text Using Contextual Features Toru Hirano, Yoshihiro Matsuo, Genichiro Kikui NTT Cyber Space Laboratories, NTT Corporation 1-1 Hikarinooka, Yokosuka-Shi, Kanagawa, 239-0847, Japan {hirano.tohru, matsuo.yoshihiro, kikui.genichiro}@lab.ntt.co.jp Abstract This paper proposes a supervised learning method for detecting a semantic relation between a given pair of named entities, which may be located in different sentences.
The method employs newly introduced contextual features based on centering theory as well as conventional syntactic and word-based features.
These features are organized as a tree structure and are fed into a boosting-based classification algorithm.
Experimental results show the proposed method outperformed prior methods, and increased precision and recall by 4.4% and 6.7%. 1 Introduction Statistical and machine learning NLP techniques are now so advanced that named entity (NE) taggers are in practical use.
Researchers are now focusing on extracting semantic relations between NEs, such as ?George Bush (person)??is ?president (relation)??of ?the United States (location)?? because they provide important information used in information retrieval, question answering, and summarization.
We represent a semantic relation between two NEs with a tuple [NE1, NE2, Relation Label].
Our final goal is to extract tuples from a text.
For example, the tuple [George Bush (person), the U.S.
(location), president (Relation Label)] would be extracted from the sentence ?George Bush is the president of the U.S.??
There are two tasks in extracting tuples from text.
One is detecting whether or not a given pair of NEs are semantically related (relation detection), and the other is determining the relation label (relation characterization).
In this paper, we address the task of relation detection.
So far, various supervised learning approaches have been explored in this field (Culotta and Sorensen, 2004; Zelenko et al., 2003).
They use two kinds of features: syntactic ones and wordbased ones, for example, the path of the given pair of NEs in the parse tree and the word n-gram between NEs (Kambhatla, 2004).
These methods have two problems which we consider in this paper.
One is that they target only intrasentential relation detection in which NE pairs are located in the same sentence, in spite of the fact that about 35% of NE pairs with semantic relations are inter-sentential (See Section 3.1).
The other is that the methods can not detect semantic relations correctly when NE pairs located in a parallel sentence arise from a predication ellipsis.
In the following Japanese example1, the syntactic feature, which is the path of two NEs in the dependency structure, of the pair with a semantic relation (?Ken11??and ?Tokyo12?? is the same as the feature of the pair with no semantic relation (?Ken11??and ?New York14??.
(S-1) Ken11-wa Tokyo12-de, Tom13-wa New York14-de umareta15.
(Ken11 was born15 in Tokyo12, Tom13 in New York14.) To solve the above problems, we propose a supervised learning method using contextual features.
The rest of this paper is organized as follows.
Section 2 describes the proposed method.
We report the results of our experiments in Section 3 and conclude the paper in Section 4.
2 Relation
Detection The proposed method employs contextual features based on centering theory (Grosz et al., 1983) as well as conventional syntactic and word-based features.
These features are organized as a tree structure and are fed into a boosting-based classification algorithm.
The method consists of three parts: preprocessing (POS tagging, NE tagging, and parsing), 1The numbers show correspondences of words between Japanese and English.
157 feature extraction (contextual, syntactic, and wordbased features), and classification.
In this section, we describe the underlying idea of contextual features and how contextual features are used for detecting semantic relations.
2.1 Contextual
Features When a pair of NEs with a semantic relation appears in different sentences, the antecedent NE must be contextually easily referred to in the sentence with the following NE.
In the following Japanese example, the pair ?Ken22??and ?amerika32 (the U.S.)?? have a semantic relation ?wataru33 (go)?? because ?Ken22??is contextually referred to in the sentence with ?amerika32??(In fact, the zero pronoun ?i refers to ?Ken22??.
Meanwhile, the pair ?Naomi25?? and ?amerika32??has no semantic relation, because the sentence with ?amerika32??does not refer to ?Naomi25??
(S-2) asu21, Ken22-wa Osaka23-o otozure24 Naomi25-to au26.
(Ken22 is going to visit24 Osaka23 to see26 Naomi25, tomorrow21.) (S-3) sonogo31, (?i-ga) amerika32-ni watari33 Tom34-to ryoko35 suru.
(Then31, (hei) will go33 to the U.S.32 to travel35 with Tom34.) Furthermore, when a pair of NEs with a semantic relation appears in a parallel sentence arise from predication ellipsis, the antecedent NE is contextually easily referred to in the phrase with the following NE.
In the example of ??S-1)?? the pair ?Ken11?? and ?Tokyo12??have a semantic relation ?umareta15 (was born)??
Meanwhile, the pair ?Ken11??and ?New York14??has no semantic relation.
Therefore, using whether the antecedent NE is referred to in the context with the following NE as features of a given pair of NEs would improve relation detection performance.
In this paper, we use centering theory (Kameyama, 1986) to determine how easily a noun phrase can be referred to in the following context.
2.2 Centering
Theory Centering theory is an empirical sorting rule used to identify the antecedents of (zero) pronouns.
When there is a (zero) pronoun in the text, noun phrases that are in the previous context of the pronoun are sorted in order of likelihood of being the antecedent.
The sorting algorithm has two steps.
First, from the beginning of the text until the pronoun appears, noun Osaka23o asu 21, Naomi25others niga Ken22wa Priority Figure 1: Information Stacked According to Centering Theory phrases are stacked depending on case markers such as particles.
In the above example, noun phrases, ?asu21??
?Ken22?? ?Osaka23??and ?Naomi25?? which are in the previous context of the zero pronoun ?i, are stacked and then the information shown in Figure 1 is acquired.
Second, the stacked information is sorted by the following rules.
1. The priority of case markers is as follows: ?wa > ga > ni > o > others?? 2.
The priority of stack structure is as follows: last-in first-out, in the same case marker For example, Figure 1 is sorted by the above rules and then the order, 1: ?Ken22?? 2: ?Osaka23?? 3: ?Naomi25?? 4: ?asu21?? is assigned.
In this way, using centering theory would show that the antecedent of the zero pronoun ?i is ?Ken22?? 2.3 Applying Centering Theory When detecting a semantic relation between a given pair of NEs, we use centering theory to determine how easily the antecedent NE can be referred to in the context with the following NE.
Note that we do not explicitly execute anaphora resolutions here.
Applied centering theory to relation detection is as follows.
First, from the beginning of the text until the following NE appears, noun phrases are stacked depending on case markers, and the stacked information is sorted by the above rules (Section 2.2).
Then, if the top noun phrase in the sorted order is identical to the antecedent NE, the antecedent NE is ?positive??when being referred to in the context with the following NE.
When the pair of NEs, ?Ken22??and ?amerika32?? is given in the above example, the noun phrases, ?asu21??
?Ken22?? ?Osaka23??and ?Naomi25?? which are in the previous context of the following NE ?amerika32?? are stacked (Figure 1).
Then they are sorted by the above sorting rules and the order, 1: ?Ken22?? 2: ?Osaka23?? 3: ?Naomi25?? 4: ?asu21?? is acquired.
Here, because the top noun phrase in the sorted order is identical to the antecedent NE, the antecedent NE ?Ken22??is ?positive??when be158 amerika32 wa: Ken22 o: Osaka23 others: Naomi25 others: asu21 Figure 2: Centering Structure ing referred to in the context with the following NE ?amerika32??
Whether or not the antecedent NE is referred to in the context with the following NE is used as a feature.
We call this feature Centering Top (CT).
2.4 Using
Stack Structure The sorting algorithm using centering theory tends to rank highly thoes words that easily become subjects.
However, for relation detection, it is necessary to consider both NEs that easily become subjects, such as person and organization, and NEs that do not easily become subjects, such as location and time.
We use the stack described in Section 2.3 as a structural feature for relation detection.
We call this feature Centering Structure (CS).
For example, the stacked information shown in Figure 1 is assumed to be structure information, as shown in Figure 2.
The method of converting from a stack (Figure 1) into a structure (Figure 2) is described as follows.
First, the following NE, ?amerika32?? becomes the root node because Figure 1 is stacked information until the following NE appears.
Then, the stacked information is converted to Figure 2 depending on the case markers.
We use the path of the given pair of NEs in the structure as a feature.
For example, ?amerika32 ??wa:Ken22?? is used as the feature of the given pair ?Ken22??and ?amerika32?? 2.5 Classification Algorithm There are several structure-based learning algorithms proposed so far (Collins and Duffy, 2001; Suzuki et al., 2003; Kudo and Matsumoto, 2004).
The experiments tested Kudo and Matsumoto?s boosting-based algorithm using sub trees as features, which is implemented as the BACT system.
In relation detection, given a set of training examples each of which represents contextual, syntactic, and word-based features of a pair of NEs as a tree labeled as either having semantic relations or not, the BACT system learns that a set of rules are effective in classifying.
Then, given a test instance, which represents contextual, syntactic, and word2?A?B??means A has a dependency relation to B.
Type % of pairs with semantic relations (A) Intra-sentential 31.4% (3333 / 10626) (B) Inter-sentential 0.8% (1777 / 225516) (A)+(B) Total 2.2% (5110 / 236142) Table 1: Percent of pairs with semantic relations in annotated text based features of a pair of NEs as a tree, the BACT system classifies using a set of learned rules.
3 Experiments
We experimented with texts from Japanese newspapers and weblogs to test the proposed method.
The following four models were compared: 1.
WD : Pairs of NEs within n words are detected as pairs with semantic relation.
2. STR : Supervised learning method using syntactic3 and word-based features, the path of the pairs of NEs in the parse tree and the word ngram between pairs of NEs (Kambhatla, 2004) 3.
STR-CT : STR with the centering top feature explained in Section 2.3. 4.
STR-CS : STR with the centering structure feature explained in Section 2.4. 3.1 Setting We used 1451 texts from Japanese newspapers and weblogs, whose semantic relations between person and location had been annotated by humans for the experiments4.
There were 5110 pairs with semantic relations out of 236,142 pairs in the annotated text.
We conducted ten-fold cross-validation over 236,142 pairs of NEs so that sets of pairs from a single text were not divided into the training and test sets.
We also divided pairs of NEs into two types: (A) intra-sentential and (B) inter-sentential.
The reason for dividing them is so that syntactic structure features would be effective in type (A) and contextual features would be effective in type (B).
Another reason is that the percentage of pairs with semantic relations out of the total pairs in the annotated text differ significantly between types, as shown in Table 1.
In the experiments, all features were automatically acquired using a Japanese morphological and dependency structure analyzer.
3There is no syntactic feature in inter-sentential.
4We are planning to evaluate the other pairs of NEs.
159 (A)+(B) Total (A) Intra-sentential (B) Inter-sentential Precision Recall Precision Recall Precsion Recall WD10 43.0(2501/5819) 48.9(2501/5110) 48.1(2441/5075) 73.2(2441/3333) 8.0(60/744) 3.4(60/1777) STR 69.3(2562/3696) 50.1(2562/5110) 75.6(2374/3141) 71.2(2374/3333) 33.9(188/555) 10.6(188/1777) STR-CT 71.4(2764/3870) 54.1(2764/5110) 78.4(2519/3212) 75.6(2519/3333) 37.2(245/658) 13.8(245/1777) STR-CS 73.7(2902/3935) 56.8(2902/5110) 80.1(2554/3187) 76.6(2554/3333) 46.5(348/748) 27.6(348/1777) WD10: NE pairs that appear within 10 words are detected.
Table 2: Results for Relation Detection 0 0.2 0.4 0.6 0.8 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Recall Prec ision WDSTR STR-CTSTR-CS STR-CS STR WD STR-CT Figure 3: Recall-precision Curves: (A)+(B) total 3.2 Results To improve relation detection performance, we investigated the effect of the proposed method using contextual features.
Table 2 shows results for Type (A), Type (B), and (A)+(B).
We also plotted recallprecision curves5, altering threshold parameters, as shown in Figure 3.
The comparison between STR and STR-CT and between STR and STR-CS in Figure 3 indicates that the proposed method effectively contributed to relation detection.
In addition, the results for Type (A): intra-sentential, and (B): inter-sentential, in Table 2 indicate that the proposed method contributed to both Type (A), improving precision by about 4.5% and recall by about 5.4% and Type (B), improving precision by about 12.6% and recall by about 17.0%. 3.3 Error Analysis Over 70% of the errors are covered by two major problems left in relation detection.
Parallel sentence: The proposed method solves problems, which result from when a parallel sentence arises from predication ellipsis.
However, there are several types of parallel sentence that differ from the one we explained.
(For example, Ken and Tom was born in Osaka and New York, respectively.) 5Precision = # of correctly detected pairs / # of detected pairs Recall = # of correctly detected pairs / # of pairs with semantic relations Definite anaphora: Definite noun phrase, such as ?Shusho (the Prime Minister)??and ?Shacho (the President)?? can be anaphors.
We should consider them in centering theory, but it is difficult to find them in Japanese. 4 Conclusion In this paper, we propose a supervised learning method using words, syntactic structures, and contextual features based on centering theory, to improve both inter-sentential and inter-sentential relation detection.
The experiments demonstrated that the proposed method increased precision by 4.4%, up to 73.7%, and increased recall by 6.7%, up to 56.8%, and thus contributed to relation detection.
In future work, we plan to solve the problems relating to parallel sentence and definite anaphora, and address the task of relation characterization.
References M.
Collins and N.
Duffy. 2001.
Convolution Kernels for Natural Language.
Proceedings of the Neural Information Processing Systems, pages 625??32.
A. Culotta and J.
Sorensen. 2004.
Dependency Tree Kernels for Relation Extraction.
Annual Meeting of Association of Computational Linguistics, pages 423??29.
B. J.
Grosz, A.
K. Joshi, and S.
Weistein. 1983.
Providing a unified account of definite nounphrases in discourse.
Annual Meeting of Association of Computational Linguistics, pages 44??0.
N. Kambhatla.
2004. Combining Lexical, Syntactic, and Semantic Features with Maximum Entropy Models for Information Extraction.
Annual Meeting of Association of Computational Linguistics, pages 178??81.
M. Kameyama.
1986. A property-sharing constraint in centering.
Annual Meeting of Association of Computational Linguistics, pages 200??06.
T. Kudo and Y.
Matsumoto. 2004.
A boosting algorithm for classification of semi-structured text.
In Proceedings of the 2004 EMNLP, pages 301??08.
J. Suzuki, T.
Hirao, Y.
Sasaki, and E.
Maeda. 2003.
Hierarchical directed acyclic graph kernel : Methods for structured natural language data.
Annual Meeting of Association of Computational Linguistics, pages 32??9.
D. Zelenko, C.
Aone, and A.
Richardella. 2003.
Kernel Methods for Relation Extraction.
Journal of Machine Learning Research, pages 3:1083??106 .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 161??64, Prague, June 2007.
c2007 Association for Computational Linguistics Mapping Concrete Entities from PAROLE-SIMPLE-CLIPS to ItalWordNet: Methodology and Results Adriana Roventini, Nilda Ruimy, Rita Marinelli, Marisa Ulivieri, Michele Mammini Istituto di Linguistica Computazionale ??CNR Via Moruzzi,1 ??56124 ??Pisa, Italy {adriana.roventini,nilda.ruimy,rita.marinelli, marisa.ulivieri,michele.mammini}@ilc.cnr.it Abstract This paper describes a work in progress aiming at linking the two largest Italian lexical-semantic databases ItalWordNet and PAROLE-SIMPLE-CLIPS.
The adopted linking methodology, the software tool devised and implemented for this purpose and the results of the first mapping phase regarding 1 st OrderEntities are illustrated here.
1 Introduction
The mapping and the integration of lexical resources is today a main concern in the world of computational linguistics.
In fact, during the past years, many linguistic resources were built whose bulk of linguistic information is often neither easily accessible nor entirely available, whereas their visibility and interoperability would be crucial for HLT applications.
The resources here considered constitute the largest and extensively encoded Italian lexical semantic databases.
Both were built at the CNR Institute of Computational Linguistics, in Pisa.
The ItalWordNet lexical database (henceforth IWN) was first developed in the framework of EuroWordNet project and then enlarged and improved in the national project SI-TAL 1. The theoretical model underlying this lexicon is based on the EuroWordNet lexical model (Vossen, 1998) which is, in its turn, inspired to the Princeton WordNet (Fellbaum, 1998).
PAROLE-SIMPLE-CLIPS (PSC) is a four-level lexicon developed over three different projects: the 1 Integrated System for the Automatic Language Treatment.
LE-PAROLE project for the morphological and syntactic layers, the LE-SIMPLE project for the semantic model and lexicon and the Italian project CLIPS 2 for the phonological level and the extension of the lexical coverage.
The theoretical model underlying this lexicon is based on the EAGLES recommendations, on the results of the EWN and ACQUILEX projects and on a revised version of Pustejovsky?s Generative Lexicon theory (Pustejovsky 1995).
In spite of the different underlying principles and peculiarities characterizing the two lexical models, IWN and PSC lexicons also present many compatible aspects and the reciprocal enhancements that the linking of the resources would entail were illustrated in Roventini et al., (2002); Ruimy & Roventini (2005).
This has prompted us to envisage the semi-automatic link of the two lexical databases, eventually merging the whole information into a common representation framework.
The first step has been the mapping of the 1 st OrderEntities which is described in the following.
This paper is organized as follows: in section 2 the respective ontologies and their mapping are briefly illustrated, in section 3 the methodology followed to link these resources is described; in section 4 the software tool and its workings are explained; section 5 reports on the results of the complete mapping of the 1 st OrderEntities.
Future work is outlined in the conclusion.
2 Mapping
Ontology-based Lexical Resources In both lexicons, the backbone for lexical representation is provided by an ontology of semantic types.
2 Corpora
e Lessici dell'Italiano Parlato e Scritto.
161 The IWN Top Ontology (TO) (Roventini et al., 2003), which slightly differs from the EWN TO 3, consists in a hierarchical structure of 65 languageindependent Top Concepts (henceforth TCs) clustered in three categories distinguishing 1 st OrderEntities, 2 nd OrderEntities and 3 rd Order Entities.
Their subclasses, hierarchically ordered by means of a subsumption relation, are also structured in terms of (disjunctive and nondisjunctive) opposition relations.
The IWN database is organized around the notion of synset, i.e. a set of synonyms.
Each synset is ontologically classified on the basis of its hyperonym and connected to other synsets by means of a rich set of lexical-semantic relations.
Synsets are in most cases cross-classified in terms of multiple, non disjoint TCs, e.g.: informatica (computer science): [Agentive, Purpose, Social, Unboundedevent].
The semantics of a word sense or synset variant is fully defined by its membership in a synset.
The SIMPLE Ontology (SO) 4, which consists of 157 language-independent semantic types, is a multidimensional type system based on hierarchical and non-hierarchical conceptual relations.
In the type system, multidimensionality is captured by qualia roles that define the distinctive properties of semantic types and differentiate their internal semantic constituency.
The SO distinguishes therefore between simple (onedimensional) and unified (multi-dimensional) semantic types, the latter implementing the principle of orthogonal inheritance.
In the PSC lexicon, the basic unit is the word sense, represented by a ?semantic unit??(henceforth, SemU).
Each SemU is assigned one single semantic type (e.g.: informatica: [Domain]), which endows it with a structured set of semantic information.
A primary phase in the process of mapping two ontology-based lexical resources clearly consisted in establishing correspondences between the conceptual classes of both ontologies, with a view to further matching their respective instances.
The mapping will only be briefly outlined here for the 1 st OrderEntity.
More information can be found in (Ruimy & Roventini 2005; Ruimy, 2006).
The IWN 1 st OrderEntity class structures concrete entities (referred to by concrete nouns).
Its main cross-classifying subclasses: Form, Origin, 3 A few changes were in fact necessary to allow the encoding of new syntactic categories.
4 http://www.ilc.cnr.it/clips/Ontology.htm Composition and Function correspond to the four Qualia roles the SIMPLE model avails of to express orthogonal aspects of word meaning.
Their respective subdivisions consist of (mainly) disjoint classes, e.g.
Natural vs.
Artifact. To each class corresponds, in most of the cases, a SIMPLE semantic type or a type hierarchy subsumed by the Concrete_entity top type.
Some other IWN TCs, such as Comestible, Liquid, are instead mappable to SIMPLE distinctive features: e.g.
Plus_Edible, Plus_Liquid, etc.
3 Linking
Methodology Mapping is performed on a semantic type-driven basis.
A semantic type of the SIMPLE ontology is taken as starting point.
Considering the type?s SemUs along with their PoS and ?isa??relation, the IWN resource is explored in search of linking candidates with same PoS and whose ontological classification matches the correspondences established between the classes of both ontologies.
A characteristic of this linking is that it involves lexical elements having a different status, i.e. semantic units and synsets.
During the linking process, two different types of data are returned from each mapping run: 1) A set of matched pairs of word senses, i.e.
SemUs and synset variants with identical string, PoS and whose respective ontological classification perfectly matches.
After human validation, these matched word senses are linked.
2) A set of unmatched word senses, in spite of their identical string and PoS value.
Matching failure is due to a mismatch of the ontological classification of word senses existing in both resources.
Such mismatch may be originated by: a) an incomplete ontological information.
As already explained, IWN synsets are cross-classified in terms of a combination of TCs; however, cases of synsets lacking some meaning component are not rare.
The problem of incomplete ontological classification may often be overcome by relaxing the mapping constraints; yet, this solution can only be applied if the existing ontological label is informative enough.
Far more problematic to deal with are those cases of incomplete or little informative ontological labels, e.g. 1 st OrderEntities as different as medicinale, anello, vetrata (medicine, ring, picture window) and only classified as ?Function?? 162 b) a different ontological information.
Besides mere encoding errors, ontological classification discrepancy may be imputable to: i) a different but equally defensible meaning interpretation (e.g.: ala (aircraft wing) : [Part] vs.
[Artifact Instrument Object]).
Word senses falling into this category are clustered into numerically significant sets according to their semantic typing and then studied with a view to establishing further equivalences between ontological classes or to identify, in their classification schemes, descriptive elements lending themselves to be mapped.
ii) a different level of specificity in the ontological classification, due either to the lexicographer?s subjectivity or to an objective difference of granularity of the ontologies.
The problems in ii) may be bypassed by climbing up the ontological hierarchy, identifying the parent nodes and allowing them to be taken into account in the mapping process.
Hyperonyms of matching candidates are taken into account during the linking process and play a particularly determinant role in the resolution of cases whereby matching fails due to a conflict of ontological classification.
It is the case for sets of word senses displaying a different ontological classification but sharing the same hyperonym, e.g. collana, braccialetto (necklace, bracelet) typed as [Clothing] in PSC and as [Artifact Function] in IWN but sharing the hyperonym gioiello (jewel).
Hyperonyms are also crucial for polysemous senses belonging to different semantic types in PSC but sharing the same ontological classification in IWN, e.g.: SemU1595viola (violet) [Plant] and SemU1596viola (violet) [Flower] vs.
IWN: viola1 (has_hyperonym pianta1 (plant)) and viola3 (has_hyperonym fiore1 (flower)), both typed as [Group Plant].
4 The
Linking Tool The LINKPSC_IWN software tool implemented to map the lexical units of both lexicons works in a semiautomatic way using the ontological classifications, the ?isa??relations and some semantic features of the two resources.
Since the 157 semantic types of the SO provide a more finegrained structure of the lexicon than the 65 top concepts of the IWN ontology, which reflect only fundamental distinctions, mapping is PSC mapping the ?Animal??class).
Some of these word senses proceed from an extension of meaning, e.g.
People-Human: pigmeo, troglodita (pygmy, troglodyte) or Animal-Human verme, leone (worm, lion) and are used with different levels of intentionality: either as a semantic surplus or as dead metaphors (Marinelli, 2006).
More interestingly, the list of unmatched words also contains the IWN word senses whose synset?s ontological classification is incomplete or different w.r.t. the constraints imposed to the mapping run.
Analyzing these data is therefore crucial to identify further mapping constraints.
A list of PSC lexical units missing in IWN is also generated, which is important to appropriately assess the lexical intersection between the two resources.
5 Results
From a quantitative point of view three main issues are worth noting (cf.
Table 1): first, the considerable percentage of linked senses with respect to the linkable ones (i.e.
words with identical string and PoS value); second, the many 163 cases of multiple mappings; third, the extent of overlapping coverage.
SemUs selected 27768 Linkable senses 15193 54,71% Linked senses 10988 72,32% Multiple mappings 1125 10,23% Unmatched senses 4205 27,67% Table 1 summarizing data Multiple mappings depend on the more fine grained sense distinctions performed in IWN.
The eventual merging of the two resources would make up for such discrepancy.
During the linking process, many other possibilities of reciprocal improvement and enrichment were noticed by analyzing the lists of unmatched word-senses.
All the inconsistencies are in fact recorded together with their differences in ontological classification, or in the polysemy treatment that the mapping evidenced.
Some mapping failures have been observed due to a different approach to the treatment of polysemy in the two resources: for example, a single entry in PSC corresponding to two different IWN entries encoding very fined-grained nuances of sense, e.g. galeotto1 (galley rower) and galeotto2 (galley slave).
Other mapping failures are due to cases of encoding inconsistency.
For example, when a word sense from a multi-variant synset is linked to a SemU, all the other variants from the same synset should map to PSC entries sharing the same semantic type, yet in some cases it has been observed that SemUs corresponding to variants of the same synset do not share a common semantic type.
All these encoding differences or inconsistencies were usefully put in the foreground by the linking process and are worthy of further in-depth analysis with a view to the merging, harmonization and interoperability of the two lexical resources.
6 Conclusion
and Future Work In this paper the PSC-IWN linking of concrete entities, the methodology adopted, the tool implemented to this aim and the results obtained are described.
On the basis of the encouraging results illustrated here, the linking process will be carried on by dealing with 3 rd Order Entities.
Our attention will then be devoted to 2 nd OrderEntities which, so far, have only been object of preliminary investigations on Speech act (Roventini 2006) and Feeling verbs.
Because of their intrinsic complexity, the linking of 2 nd OrderEntities is expected to be a far more challenging task.
References James Pustejovsky 1995.
The generative lexicon.
MIT Press.
Christiane Fellbaum (ed).
1998. Wordnet: An Electronic Lexical Database.
MIT Press.
Piek Vossen (ed).
1998. EuroWordNet: A multilingual database with lexical semantic networks.
Kluwer Academic Publishers.
Adriana Roventini et al.2003. ItalWordNet: Building a Large Semantic Database for the Automatic Treatment of Italian.
Computational Linguistics in Pisa, Special Issue, XVIII-XIX, Pisa-Roma, IEPI.
Tomo II, 745--791.
Nilda Ruimy et al.2003. A computational semantic lexicon of Italian: SIMPLE.
In A.
Zampolli, N.
Calzolari, L.
Cignoni, (eds.), Computational Linguistics in Pisa, Special Issue, XVIII-XIX, (2003).
Pisa-Roma, IEPI.
Tomo II, 821-864.
Adriana Roventini, Marisa Ulivieri and Nicoletta Calzolari.
2002 Integrating two semantic lexicons, SIMPLE and ItalWordNet: what can we gain?
LREC Proceedings, Vol.
V, pp.
1473-1477. Nilda Ruimy and Adriana Roventini.
2005 Towards the linking of two electronic lexical databases of Italian, In Zygmunt Veutulani (ed.), L&T'05 Nilda Ruimy.
2006. Merging two Ontology-based Lexical Resources.
LREC Proceedings, CD-ROM, 1716-1721.
Adriana Roventini.
2006. Linking Verbal Entries of Different Lexical Resources.
LREC Proceedings, CD-ROM, 1710-1715.
Rita Marinelli.
2006. Computational Resources and Electronic Corpora in Metaphors Evaluation.
Second International Conference of the German Cognitive Linguistics Association, Munich, 5-7 October. 164
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 169??72, Prague, June 2007.
c2007 Association for Computational Linguistics An OWL Ontology for HPSG Graham Wilcock University of Helsinki PO Box 9 00014 Helsinki, Finland graham.wilcock@helsinki.fi Abstract The paper presents an OWL ontology for HPSG.
The HPSG ontology is integrated with an existing OWL ontology, GOLD, as a community of practice extension.
The basic ideas are illustrated by visualizations of type hierarchies for parts of speech.
1 Introduction
The paper presents an OWL ontology for HPSG (Head-driven Phrase Structure Grammar) (Sag et al., 2003).
OWL is the W3C Web Ontology Language (http://www.w3.org/2004/OWL).
An existing ontology is used as a starting point: GOLD (Section 2) is a general ontology for linguistic description.
As HPSG is a more specific linguistic theory, the HPSG ontology (Section 3) is integrated inside GOLD as a sub-ontology known as a community of practice extension (Section 4).
2 GOLD: A General Ontology for Linguistic Description GOLD, a General Ontology for Linguistic Description (http://www.linguistics-ontology.org/) (Farrar and Langendoen, 2003) is an OWL ontology that aims to capture ?the general knowledge of the field that is usually possessed by a well trained linguist.
This includes knowledge that potentially forms the basis of any theoretical framework.
In particular, GOLD captures the fundamentals of descriptive linguistics.
Examples of such knowledge are ?a verb is a part of speech??
?gender can be semantically grounded?? or ?linguistic expressions realize morphemes????(Farrar and Lewis, 2005).
As far as possible GOLD uses language-neutral and theory-neutral terminology.
For instance, parts of speech are subclasses of gold:GrammaticalUnit asshowninFigure1.
AsGOLDislanguage-neutral, a wide range of parts of speech are included.
For example, both Preposition and Postposition are included as subclasses of Adposition.
The classes in the OWLViz graphical visualization (on the right in Figure 1) have been selected from the complete list in the Asserted Hierarchy (on the left).
Originally GOLD was intended to be neutral where linguistic theories had divergent views, but a recent development is the idea of supporting different sub-communities as communities of practice (Farrar and Lewis, 2005) within the GOLD framework.
A community of practice may focus on developing a consensus in a specific area, for example in phonology or in Bantu languages.
On the other hand, communities of practice may focus on competing theories, where each sub-community has its own distinctive terminology and divergent conceptualization.
In this case, the aim is to capture explicitly the relationship between the sub-community view and the overall framework, in the form of a Community Of Practice Extension (COPE) (Farrar and Lewis, 2005).
A COPE is a sub-ontology that inherits from, and extends, the overall GOLD ontology.
Sub-ontology classes are distinguished from each other by different namespace prefixes, for example gold:Noun and hpsg:noun.
3 An
OWL Ontology for HPSG HPSG OWL is an OWL ontology for HPSG that is currentlyunderdevelopment.
Astheaimsofthefirst version of the ontology are clarity and acceptability, 169 Figure 1: Parts of speech in GOLD it carefully follows the standard textbook version of HPSG by Sag et al.(2003). This also means that the first version is English-specific, as the core grammars presented in the textbook are English-specific.
In HPSG OWL, parts of speech are subclasses of hpsg:pos, as shown in Figure 2.
As this version is English-specific, it has prepositions (hpsg:prep) but not postpositions.
Parts of speech that have agreement features (in English) form a distinct subclass hpsg:agr-pos including hpsg:det (determiner) and hpsg:verb.
Within hpsg:agr-pos, hpsg:comp (complementizer) and hpsg:noun form a further subclass hpsg:nominal.
This particular conceptualization of the type hierarchy is specific to (Sag et al., 2003).
The Protege-OWL (http://protege.stanford.edu) ontology editor supports both visual construction and visual editing of the hierarchy.
For example, if hpsg:adj had agreement features, it could be moved under hpsg:agr-pos by a simple drag-and-drop (in the Asserted Hierarchy pane on the left).
Both the visualization (in the OWLViz pane on the right) and the underlying OWL statements (not shown) are automatically generated.
The grammar writer does not edit OWL statements directly.
This is a significant advantage of the new technology over current grammar development tools.
For example, LKB (Copestake, 2002) can produce a visualizationofthetypehierarchyfromtheunderlying Type Definition Language (TDL) statements, but the hierarchy can only be modified by textually editing the TDL statements.
4 A
Community of Practice Extension HPSG COPE is a community of practice extension that integrates the HPSG ontology within GOLD.
The COPE is an OWL ontology that imports both the GOLD and the HPSG ontologies.
Apart from the import statements, the COPE consists entirely of 170 Figure 2: Parts of speech in HPSG rdfs:subClassOf and rdfs:subPropertyOf statements.
HPSG COPE defines HPSG classes as subclasses of GOLD classes and HPSG properties as subproperties of GOLD properties.
In the COPE, parts of speech in HPSG are subsumed by appropriate parts of speech in GOLD, as shown in Figure 3.
In some cases this is straightforward, for example hpsg:adj is mapped to gold:Adjective.
In other cases, the HPSG theoryspecific terminology differs significantly from the theory-neutral terminology in GOLD.
Some of the mappings are based on definitions of the HPSG terms given in a glossary in (Sag et al., 2003), for example the mapping of hpsg:conj (conjunction) to gold:CoordinatingConnective and the mapping of hpsg:comp (complementizer) to gold:SubordinatingConnective.
Properties in HPSG OWL are defined by HPSG COPE as subproperties of GOLD properties.
For example, the HPSG OWL class hpsg:sign (Sag et al., 2003) (p.
475) properties: PHON type: list (a sequence of word forms) SYN type: gram-cat (a grammatical category) SEM type: sem-struc (a semantic structure) are mapped to the GOLD class gold:LinguisticSign properties: hasForm Range: PhonologicalUnit hasGrammar Range: GrammaticalUnit hasMeaning Range: SemanticUnit by the HPSG COPE rdfs:subPropertyOf definitions: hpsg:PHON subproperty of gold:hasForm hpsg:SYN subproperty of gold:hasGrammar hpsg:SEM subproperty of gold:hasMeaning 5 Conclusion The paper has described an initial version of an OWLontologyforHPSG,togetherwithanapproach to integrating it with GOLD as a community of prac171 Figure 3: Parts of speech in the Community of Practice Extension tice extension.
Perhaps a rigorous foundation of typed feature structures and a clear type hierarchy makes HPSG more amenable to expression as an ontology than other linguistic theories.
Protege-OWL supports visual development and visual editing of the ontology.
This is a significant practical advantage over existing grammar development tools.
OWLViz provides graphical visualizations of any part of the ontology.
OWL DL (Description Logic) reasoners can be run inside Protege to check consistency and to do cross-classification.
One current research topic is how to exploit reasoners to perform automatically the kind of cross-classification that is widely used in HPSG linguistic analyses.
Another current topic is how to implement HPSG lexical rules and grammar rules in the ontology.
An interesting possibility is to use the W3C Semantic Web Rule Language, SWRL (Wilcock, 2006).
References Ann Copestake.
2002. Implementing Typed Feature Structure Grammars.
CSLI Publications, Stanford, CA.
Scott Farrar and D.
Terence Langendoen.
2003. A linguistic ontology for the semantic web.
GLOT International, 7.3:97??00.
Scott Farrar and William D.
Lewis. 2005.
The GOLD Community of Practice: An infrastructure for linguistic data on the web.
http://www.u.arizona.edu/?farrar/. Ivan A.
Sag, Thomas Wasow, and Emily Bender.
2003. Syntactic Theory: A Formal Introduction.
CSLI Publications, Stanford, CA.
Graham Wilcock.
2006. Natural language parsing with GOLD and SWRL.
In RuleML-2006, Rules and Rule Markup Languages for the Semantic Web (Online Proceedings), Athens, GA.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 173??76, Prague, June 2007.
c2007 Association for Computational Linguistics Classifying Temporal Relations Between Events Nathanael Chambers and Shan Wang and Dan Jurafsky Department of Computer Science Stanford University Stanford, CA 94305 {natec,shanwang,jurafsky}@stanford.edu Abstract This paper describes a fully automatic twostage machine learning architecture that learns temporal relations between pairs of events.
The first stage learns the temporal attributes of single event descriptions, such as tense, grammatical aspect, and aspectual class.
These imperfect guesses, combined with other linguistic features, are then used in a second stage to classify the temporal relationship between two events.
We present both an analysis of our new features and results on the TimeBank Corpus that is 3% higher than previous work that used perfect human tagged features.
1 Introduction
Temporal information encoded in textual descriptions of events has been of interest since the early days of natural language processing.
Lately, it has seen renewed interest as Question Answering, Information Extraction and Summarization domains find it critical in order to proceed beyond surface understanding.
With the recent creation of the Timebank Corpus (Pustejovsky et al., 2003), the utility of machine learning techniques can now be tested.
Recent work with the Timebank Corpus has revealed that the six-class classification of temporal relationsisverydifficult,evenforhumanannotators.
The highest score reported on Timebank achieved 62.5% accuracy when using gold-standard features as marked by humans (Mani et al., 2006).
This paper describes an approach using features extracted automatically from raw text that not only duplicates this performance, but surpasses its accuracy by 3%.
We do so through advanced linguistic features and a surprising finding that using automatic ratherthanhand-labeledtenseandaspectknowledge causes only a slight performance degradation.
We briefly describe current work on temporal orderinginsection2.
Section4describesthefirststage of basic temporal extraction, followed by a full description of the second stage in 5.
The evaluation and results on Timebank then follow in section 6.
2 Previous
Work Mani et.
al (2006) built a MaxEnt classifier that assigns each pair of events one of 6 relations from an augmented Timebank corpus.
Their classifier relies on perfect features that were hand-tagged in the corpus, including tense, aspect, modality, polarity and event class.
Pairwise agreement on tense and aspect are also included.
In a second study, they applied rules of temporal transitivity to greatly expand the corpus, providing different results on this enlarged dataset.
We could not duplicate their reported performance on this enlarged data, and instead focus on performing well on the Timebank data itself.
Lapata and Lascarides (2006) trained an event classifierforinter-sententialevents.
Theybuiltacorpus by saving sentences that contained two events, one of which is triggered by a key time word (e.g.
after and before).
Their learner was based on syntax and clausal ordering features.
Boguraev and Ando (2005) evaluated machine learning on related tasks, but not relevant to event-event classification.
Our work is most similar to Mani?s in that we are 173 learning relations given event pairs, but our work extends their results both with new features and by using fully automatic linguistic features from raw text that are not hand selected from a corpus.
3 Data
We used the Timebank Corpus (v1.1) for evaluation, 186 newswire documents with 3345 event pairs.
Solely for comparison with Mani, we add the 73 documentOpinionCorpus(Manietal., 2006)tocreate a larger dataset called the OTC.
We present both Timebank and OTC results so future work can compare against either.
All results below are from 10fold cross validation.
4 Stage
One: Learning Event Attributes The task in Stage One is to learn the five temporal attributes associated with events as tagged in the Timebank Corpus.
(1) Tense and (2) grammatical aspect are necessary in any approach to temporal ordering as they define both temporal location and structure of the event.
(3) Modality and (4) polarity indicate hypothetical or non-occuring situations, and finally, (5) event class is the type of event (e.g.
process, state, etc.).
The event class has 7 values in Timebank, but we believe this paper?s approach is compatible with other class divisions as well.
The range of values for each event attribute is as follows, also found in (Pustejovsky et al., 2003): tense none, present, past, future aspect none, prog, perfect, prog perfect class report, aspectual, state, I state I action, perception, occurrence modality none, to, should, would, could can, might polarity positive, negative 4.1 Machine Learning Classification We used a machine learning approach to learn each of the five event attributes.
We implemented both Naive Bayes and Maximum Entropy classifiers, but found Naive Bayes to perform as well or better than Maximum Entropy.
The results in this paper are from Naive Bayes with Laplace smoothing.
The features we used on this stage include part of speech tags (two before the event), lemmas of the event words, WordNet synsets, and the appearance tense POS-2-event, POS-1-event POS-of-event, have word, be word aspect POS-of-event, modal word, be word class synset modality none polarity none Figure 1: Features selected for learning each temporal attribute.
POS-2 is two tokens before the event.
Timebank Corpus tense aspect class Baseline 52.21 84.34 54.21 Accuracy 88.28 94.24 75.2 Baseline (OTC) 48.52 86.68 59.39 Accuracy (OTC) 87.46 88.15 76.1 Figure 2: Stage One results on classification.
of auxiliaries and modals before the event.
This latter set included all derivations of be and have auxiliaries, modal words (e.g.
may, might, etc.), and the presence/absence of not.
We performed feature selection on this list of features, learning a different set of features for each of the five attributes.
The list of selected features for each is shown in figure 1.
Modality and polarity did not select any features because their majority class baselines were so high (98%) that learning these attributes does not provide much utility.
A deeper analysis of event interaction would require a modal analysis, but it seems that a newswire domain does not provide great variation in modalities.
Consequently, modality and polarity are not used in Stage Two.
Tense, aspect and class are shown in figure 2 with majority class baselines.
Tense classification achieves36%absolute improvement, aspect 10% and class 21%.
Performance on the OTC set is similar, although aspect is not as good.
These guesses are then passed to Stage Two.
5 Stage
Two: Event-Event Features The task in this stage is to choose the temporal relation between two events, given the pair of events.
We assume that the events have been extracted and that there exists some relation between them; the task is to choose the relation.
The Timebank Corpus uses relations that are based on Allen?s set of thir174 teen (Allen, 1984).
Six of the relations are inverses of the other six, and so we condense the set to before, ibefore, includes, begins, ends and simultaneous.
We map the thirteenth identity into simultaneous.
One oddity is that Timebank includes both during and included by relations, but during does not appear in Timebank documentation.
While we don?t know how previous work handles this, we condense during into included by (invert to includes).
5.1 Features
Event Specific: The five temporal attributes from Stage One are used for each event in the pair, as well as the event strings, lemmas and WordNet synsets.
Mani added two other features from these, indicators if the events agree on tense and aspect.
We add a third, event class agreement.
Further, to capture the dependency between events in a discourse, we create new bigram features of tense, aspect and class (e.g.
?present past??if the first event is in the present, and the second past).
PartofSpeech: Foreachevent, weincludethePenn Treebank POS tag of the event, the tags for the two tokens preceding, and one token following.
We use the Stanford Parser1 to extract them.
We also extend previousworkandcreatebigramPOSfeaturesofthe event and the token before it, as well as the bigram POS of the first event and the second event.
Event-Event Syntactic Properties: A phrase P is said to dominate another phrase Q if Q is a daughter node of P in the syntactic parse tree.
We leverage the syntactic output of the parser to create the dominance feature for intra-sentential events.
It is either on or off, depending on the two events??syntactic dominance.
Lapata used a similar feature for subordinate phrases and an indicator before for textualeventordering.
Weadoptthesefeaturesandalso add a same-sentence indicator if the events appear in the same sentence.
Prepositional Phrase: Since preposition heads are often indicators of temporal class, we created a new feature indicating when an event is part of a prepositional phrase.
The feature?s values range over 34 English prepositions.
Combined with event dominance (above), these two features capture direct 1http://nlp.stanford.edu/software/lex-parser.shtml intra-sentential relationships.
To our knowledge, we are the first to use this feature in temporal ordering.
Temporal Discourse: Seeing tense as a type of anaphora, it is a natural conclusion that the relationship between two events becomes stronger as the textual distance draws closer.
Because of this, we adopted the view that intra-sentential events are generated from a different distribution than intersentential events.
We therefore train two models during learning, one for events in the same sentence, and the other for events crossing sentence boundaries.
It essentially splits the data on the same sentence feature.
As we will see, this turned out to be a very useful feature.
It is called the split approach in the next section.
Example (require, compromise): ?Their solution required a compromise...??
Features (lemma1: require) (lemma2: compromise) (dominates: yes) (tense-bigram: past-none) (aspect-bigram: none-none) (tensematch: no) (aspect-match: yes) (before: yes) (same-sent: yes) 6 Evaluation and Results All results are from a 10-fold cross validation using SVM (Chang and Lin, 2001).
We also evaluated Naive Bayes and Maximum Entropy.
Naive Bayes (NB) returned similar results to SVM and we presentfeatureselectionresultsfromNBtocompare the added value of our new features.
The input to Stage Two is a list of pairs of events; the task is to classify each according to one of six temporal relations.
Four sets of results are shown in figure 3.
Mani, Mani+Lapata and All+New correspond to performance on features as listed in the figure.
The three table columns indicate how a goldstandard Stage One (Gold) compares against imperfect guesses (Auto) and the guesses with split distributions (Auto-Split).
A clear improvement is seen in each row, indicating that our new features provide significant improvement over previous work.
A decrease in performance is seen between columns gold and auto, as expected, because imperfect data is introduced, however, the drop is manageable.
The auto-split distributions make significant gains for the Mani and Lapata features, but less when all new features are 175 Timebank Corpus Gold Auto Auto-Split Baseline 37.22 37.22 46.58 Mani 50.97 50.19 53.42 Mani+Lapata 52.29 51.57 55.10 All+New 60.45 59.13 59.43 Mani stage one attributes, tense/aspect-match, event strings Lapata dominance, before, lemma, synset New prep-phrases, same-sent, class-match, POS uni/bigrams, tense/aspect/class-bigrams Figure 3: Incremental accuracy by adding features.
Same Sentence Diff Sentence POS-1 Ev1 2.5% Tense Pair 1.6% POS Bigram Ev1 3.5% Aspect Ev1 0.5% Preposition Ev1 2.0% POS Bigram 0.2% Tense Ev2 0.7% POS-1 Ev2 0.3% Preposition Ev2 0.6% Word EV2 0.2% Figure4: Top5featuresasaddedinfeatureselection w/ Naive Bayes, with their percentage improvement.
involved. The highest fully-automatic accuracy on Timebank is 59.43%, a 4.3% gain from our new features.
Wealsoreport67.57%gold and65.48%autosplit on the OTC dataset to compare against Mani?s reported hand-tagged features of 62.5%, a gain of 3% with our automatic features.
7 Discussion
Previous work on OTC achieved classification accuracy of 62.5%, but this result was based on ?perfect data??from human annotators.
A low number from gooddataisatfirstdisappointing, however, weshow that performance can be improved through more linguistic features and by isolating the distinct tasks of ordering inter-sentential and intra-sentential events.
Our new features show a clear improvement over previous work.
The features that capture dependencies between the events, rather than isolated features provide the greatest utility.
Also, the impact of imperfect temporal data is surprisingly minimal.
Using Stage One?s results instead of gold values hurts performance by less than 1.4%.
This suggests that much of the value of the hand-coded information can be achieved via automatic approaches.
Stage One?s event class shows room for improvement, yet the negative impact on Event-Event relationships is manageable.
It is conceivable that more advanced featureswouldbetterclassifytheeventclass, butimprovement on the event-event task would be slight.
Finally, it is important to note the difference in classifying events in the same sentence vs.
crossboundary.
Splitting the 3345 pairs of corpus events into two separate training sets makes our data more sparse, but we still see a performance improvement when using Mani/Lapata features.
Figure 4 gives a hint to the difference in distributions as the best features of each task are very different.
Intra-sentence events rely on syntax cues (e.g.
preposition phrases and POS), while inter-sentence events use tense and aspect.
However, the differences are minimized as more advanced features are added.
The final row in figure 3 shows minimal split improvement.
8 Conclusion
We have described a two-stage machine learning approach to event-event temporal relation classification.
We have shown that imperfect event attributescanbeusedeffectively, thatarangeofeventevent dependency features provide added utility to a classifier, and that events within the same sentence have distinct characteristics from those across sentence boundaries.
This fully automatic raw text approach achieves a 3% improvement over previous work based on perfect human tagged features.
Acknowledgement: This work was supported in part by the DARPA GALE Program and the DTO AQUAINT Program.
References James Allen.
1984. Towards a general theory of action and time.
Artificial Intelligence, 23:123??54.
Branimir Boguraev and Rie Kubota Ando.
2005. Timeml-compliant text analysis for temporal reasoning.
In IJCA-05.
Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM: a library for support vector machines.
Software available at http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Mirella Lapata and Alex Lascarides.
2006. Learning sentence-internal temporal relations.
In Journal of AI Research, volume 27, pages 85??17.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min Lee, and James Pustejovsky.
2006. Machine learning of temporal relations.
In ACL-06, July.
James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew See, David Day, Lisa Ferro, Robert Gaizauskas, Marcia Lazo, Andrea Setzer, and Beth Sundheim.
2003. The timebank corpus.
Corpus Linguistics, pages 647??56.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177??80, Prague, June 2007.
c2007 Association for Computational Linguistics Moses: Open Source Toolkit for Statistical Machine Translation Philipp Koehn Hieu Hoang Alexandra Birch Chris Callison-Burch University of Edinburgh 1 Marcello Federico Nicola Bertoldi ITC-irst 2 Brooke Cowan Wade Shen Christine Moran MIT 3 Richard Zens RWTH Aachen 4 Chris Dyer University of Maryland 5 Ond ej Bojar Charles University 6 Alexandra Constantin Williams College 7 Evan Herbst Cornell 8 1 pkoehn@inf.ed.ac.uk, {h.hoang, A.C.Birch-Mayne}@sms.ed.ac.uk, callison-burch@ed.ac.uk.
2 {federico, bertoldi}@itc.it.
3 brooke@csail.mit.edu, swade@ll.mit.edu, weezer@mit.edu.
4 zens@i6.informatik.rwth-aachen.de.
5 redpony@umd.edu.
6 bojar@ufal.ms.mff.cuni.cz.
7 07aec_2@williams.edu.
8 evh4@cornell.edu Abstract We describe an open-source toolkit for statistical machine translation whose novel contributions are (a) support for linguistically motivated factors, (b) confusion network decoding, and (c) efficient data formats for translation models and language models.
In addition to the SMT decoder, the toolkit also includes a wide variety of tools for training, tuning and applying the system to many translation tasks.
1 Motivation
Phrase-based statistical machine translation (Koehn et al.2003) has emerged as the dominant paradigm in machine translation research.
However, until now, most work in this field has been carried out on proprietary and in-house research systems.
This lack of openness has created a high barrier to entry for researchers as many of the components required have had to be duplicated.
This has also hindered effective comparisons of the different elements of the systems.
By providing a free and complete toolkit, we hope that this will stimulate the development of the field.
For this system to be adopted by the community, it must demonstrate performance that is comparable to the best available systems.
Moses has shown that it achieves results comparable to the most competitive and widely used statistical machine translation systems in translation quality and run-time (Shen et al.2006). It features all the capabilities of the closed sourced Pharaoh decoder (Koehn 2004).
Apart from providing an open-source toolkit for SMT, a further motivation for Moses is to extend phrase-based translation with factors and confusion network decoding.
The current phrase-based approach to statistical machine translation is limited to the mapping of small text chunks without any explicit use of linguistic information, be it morphological, syntactic, or semantic.
These additional sources of information have been shown to be valuable when integrated into pre-processing or post-processing steps.
Moses also integrates confusion network decoding, which allows the translation of ambiguous input.
This enables, for instance, the tighter integration of speech recognition and machine translation.
Instead of passing along the one-best output of the recognizer, a network of different word choices may be examined by the machine translation system.
Efficient data structures in Moses for the memory-intensive translation model and language model allow the exploitation of much larger data resources with limited hardware.
177 2 Toolkit The toolkit is a complete out-of-the-box translation system for academic research.
It consists of all the components needed to preprocess data, train the language models and the translation models.
It also contains tools for tuning these models using minimum error rate training (Och 2003) and evaluating the resulting translations using the BLEU score (Papineni et al.2002). Moses uses standard external tools for some of the tasks to avoid duplication, such as GIZA++ (Och and Ney 2003) for word alignments and SRILM for language modeling.
Also, since these tasks are often CPU intensive, the toolkit has been designed to work with Sun Grid Engine parallel environment to increase throughput.
In order to unify the experimental stages, a utility has been developed to run repeatable experiments.
This uses the tools contained in Moses and requires minimal changes to set up and customize.
The toolkit has been hosted and developed under sourceforge.net since inception.
Moses has an active research community and has reached over 1000 downloads as of 1 st March 2007.
The main online presence is at http://www.statmt.org/moses/ where many sources of information about the project can be found.
Moses was the subject of this year?s Johns Hopkins University Workshop on Machine Translation (Koehn et al.2006). The decoder is the core component of Moses.
To minimize the learning curve for many researchers, the decoder was developed as a drop-in replacement for Pharaoh, the popular phrase-based decoder.
In order for the toolkit to be adopted by the community, and to make it easy for others to contribute to the project, we kept to the following principles when developing the decoder: ??Accessibility ??Easy to Maintain ??Flexibility ??Easy for distributed team development ??Portability It was developed in C++ for efficiency and followed modular, object-oriented design.
3 Factored
Translation Model Non-factored SMT typically deals only with the surface form of words and has one phrase table, as shown in Figure 1.
i am buying you a green cat using phrase dictionary: i am buying you a green cat je achte vous un vert chat a une je vous achte un chat vert Translate: In factored translation models, the surface forms may be augmented with different factors, such as POS tags or lemma.
This creates a factored representation of each word, Figure 2.
111/ sing/ je vous achet un chat PRO PRO VB ART NN je vous acheter un chat st st st present masc masc ?????? ??
?????? ??
?????? ??
?????? ??
?????? ??
?????? ?? 1 1 / 1 sing sing i buy you a cat PRO VB PRO ART NN i tobuy you a cat st st present st ????????
???????? ????????
???????? ????????
???????? Mapping of source phrases to target phrases may be decomposed into several steps.
Decomposition of the decoding process into various steps means that different factors can be modeled separately.
Modeling factors in isolation allows for flexibility in their application.
It can also increase accuracy and reduce sparsity by minimizing the number dependencies for each step.
For example, we can decompose translating from surface forms to surface forms and lemma, as shown in Figure 3.
Figure 2.
Factored translation Figure 1.
Non-factored translation 178 Figure 3.
Example of graph of decoding steps By allowing the graph to be user definable, we can experiment to find the optimum configuration for a given language pair and available data.
The factors on the source sentence are considered fixed, therefore, there is no decoding step which create source factors from other source factors.
However, Moses can have ambiguous input in the form of confusion networks.
This input type has been used successfully for speech to text translation (Shen et al.2006). Every factor on the target language can have its own language model.
Since many factors, like lemmas and POS tags, are less sparse than surface forms, it is possible to create a higher order language models for these factors.
This may encourage more syntactically correct output.
In Figure 3 we apply two language models, indicated by the shaded arrows, one over the words and another over the lemmas.
Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006).
4 Confusion
Network Decoding Machine translation input currently takes the form of simple sequences of words.
However, there are increasing demands to integrate machine translation technology into larger information processing systems with upstream NLP/speech processing tools (such as named entity recognizers, speech recognizers, morphological analyzers, etc.).
These upstream processes tend to generate multiple, erroneous hypotheses with varying confidence.
Current MT systems are designed to process only one input hypothesis, making them vulnerable to errors in the input.
In experiments with confusion networks, we have focused so far on the speech translation case, where the input is generated by a speech recognizer.
Namely, our goal is to improve performance of spoken language translation by better integrating speech recognition and machine translation models.
Translation from speech input is considered more difficult than translation from text for several reasons.
Spoken language has many styles and genres, such as, formal read speech, unplanned speeches, interviews, spontaneous conversations; it produces less controlled language, presenting more relaxed syntax and spontaneous speech phenomena.
Finally, translation of spoken language is prone to speech recognition errors, which can possibly corrupt the syntax and the meaning of the input.
There is also empirical evidence that better translations can be obtained from transcriptions of the speech recognizer which resulted in lower scores.
This suggests that improvements can be achieved by applying machine translation on a large set of transcription hypotheses generated by the speech recognizers and by combining scores of acoustic models, language models, and translation models.
Recently, approaches have been proposed for improving translation quality through the processing of multiple input hypotheses.
We have implemented in Moses confusion network decoding as discussed in (Bertoldi and Federico 2005), and developed a simpler translation model and a more efficient implementation of the search algorithm.
Remarkably, the confusion network decoder resulted in an extension of the standard text decoder.
5 Efficient
Data Structures for Translation Model and Language Models With the availability of ever-increasing amounts of training data, it has become a challenge for machine translation systems to cope with the resulting strain on computational resources.
Instead of simply buying larger machines with, say, 12 GB of main memory, the implementation of more efficient data structures in Moses makes it possible to exploit larger data resources with limited hardware infrastructure.
A phrase translation table easily takes up gigabytes of disk space, but for the translation of a single sentence only a tiny fraction of this table is needed.
Moses implements an efficient representation of the phrase translation table.
Its key properties are a prefix tree structure for source words and on demand loading, i.e. only the fraction of the phrase table that is needed to translate a sentence is loaded into the working memory of the decoder.
179 For the Chinese-English NIST task, the memory requirement of the phrase table is reduced from 1.7 gigabytes to less than 20 mega bytes, with no loss in translation quality and speed (Zens and Ney 2007).
The other large data resource for statistical machine translation is the language model.
Almost unlimited text resources can be collected from the Internet and used as training data for language modeling.
This results in language models that are too large to easily fit into memory.
The Moses system implements a data structure for language models that is more efficient than the canonical SRILM (Stolcke 2002) implementation used in most systems.
The language model on disk is also converted into this binary format, resulting in a minimal loading time during start-up of the decoder.
An even more compact representation of the language model is the result of the quantization of the word prediction and back-off probabilities of the language model.
Instead of representing these probabilities with 4 byte or 8 byte floats, they are sorted into bins, resulting in (typically) 256 bins which can be referenced with a single 1 byte index.
This quantized language model, albeit being less accurate, has only minimal impact on translation performance (Federico and Bertoldi 2006).
6 Conclusion
and Future Work This paper has presented a suite of open-source tools which we believe will be of value to the MT research community.
We have also described a new SMT decoder which can incorporate some linguistic features in a consistent and flexible framework.
This new direction in research opens up many possibilities and issues that require further research and experimentation.
Initial results show the potential benefit of factors for statistical machine translation, (Koehn et al.2006) and (Koehn and Hoang 2007).
References Axelrod, Amittai.
"Factored Language Model for Statistical Machine Translation".
MRes Thesis.
Edinburgh University, 2006.
Bertoldi, Nicola, and Marcello Federico.
"A New Decoder for Spoken Language Translation Based on Confusion Networks".
Automatic Speech Recognition and Understanding Workshop (ASRU), 2005.
Bilmes, Jeff A, and Katrin Kirchhoff.
"Factored Language Models and Generalized Parallel Back-off".
HLT/NACCL, 2003.
Koehn, Philipp.
"Pharaoh: A Beam Search Decoder for Phrase-Based Statistical Machine Translation Models".
AMTA, 2004.
Koehn, Philipp, Marcello Federico, Wade Shen, Nicola Bertoldi, Ondrej Bojar, Chris Callison-Burch, Brooke Cowan, Chris Dyer, Hieu Hoang, Richard Zens, Alexandra Constantin, Christine Corbett Moran, and Evan Herbst.
"Open Source Toolkit for Statistical Machine Translation".
Report of the 2006 Summer Workshop at Johns Hopkins University, 2006.
Koehn, Philipp, and Hieu Hoang.
"Factored Translation Models".
EMNLP, 2007.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
"Statistical Phrase-Based Translation".
HLT/NAACL, 2003.
Och, Franz Josef.
"Minimum Error Rate Training for Statistical Machine Translation".
ACL, 2003.
Och, Franz Josef, and Hermann Ney.
"A Systematic Comparison of Various Statistical Alignment Models".
Computational Linguistics 29.1 (2003): 19-51.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
"BLEU: A Method for Automatic Evaluation of Machine Translation".
ACL, 2002.
Shen, Wade, Richard Zens, Nicola Bertoldi, and Marcello Federico.
"The JHU Workshop 2006 Iwslt System".
International Workshop on Spoken Language Translation, 2006.
Stolcke, Andreas.
"SRILM an Extensible Language Modeling Toolkit".
Intl. Conf.
on Spoken Language Processing, 2002.
Zens, Richard, and Hermann Ney.
"Efficient Phrase-Table Representation for Machine Translation with Applications to Online MT and Speech Recognition".
HLT/NAACL, 2007.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 181??84, Prague, June 2007.
c2007 Association for Computational Linguistics Boosting Statistical Machine Translation by Lemmatization and Linear Interpolation Ruiqiang Zhang1,2 and Eiichiro Sumita1,2 1National Institute of Information and Communications Technology 2ATR Spoken Language Communication Research Laboratories 2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan {ruiqiang.zhang,eiichiro.sumita}@atr.jp Abstract Data sparseness is one of the factors that degrade statistical machine translation (SMT).
Existing work has shown that using morphosyntactic information is an effective solution to data sparseness.
However, fewer efforts have been made for Chinese-to-English SMT with using English morpho-syntactic analysis.
We found that while English is a language with less inflection, using English lemmas in training can significantly improve the quality of word alignment that leads to yield better translation performance.
We carried out comprehensive experiments on multiple training data of varied sizes to prove this.
We also proposed a new effective linear interpolation method to integrate multiple homologous features of translation models.
1 Introduction
Raw parallel data need to be preprocessed in the modern phrase-based SMT before they are aligned by alignment algorithms, one of which is the wellknown tool, GIZA++ (Och and Ney, 2003), for training IBM models (1-4).
Morphological analysis (MA) is used in data preprocessing, by which the surface words of the raw data are converted into a new format.
This new format can be lemmas, stems, parts-of-speech and morphemes or mixes of these.
One benefit of using MA is to ease data sparseness that can reduce the translation quality significantly, especially for tasks with small amounts of training data.
Some published work has shown that applying morphological analysis improved the quality of SMT (Lee, 2004; Goldwater and McClosky, 2005).
We found that all this earlier work involved experiments conducted on translations from highly inflected languages, such as Czech, Arabic, and Spanish, to English.
These researchers also provided detailed descriptions of the effects of foreign language morpho-syntactic analysis but presented no specific results to show the effect of English morphological analysis.
To the best of our knowledge, there have been no papers related to English morphological analysis for Chinese-to-English (CE) translations even though the CE translation has been the main track for many evaluation campaigns including NIST MT, IWSLT and TC-STAR, where only simple tokenization or lower-case capitalization has been applied to English preprocessing.
One possible reason why English morphological analysis has been neglected may be that English is less inflected to the extent that MA may not be effective.
However, we found this assumption should not be takenfor-granted.
We studied what effect English lemmatization had on CE translation.
Lemmatization is shallow morphological analysis, which uses a lexical entry to replace inflected words.
For example, the three words, doing, did and done, are replaced by one word, do.
They are all mapped to the same Chinese translations.
As a result, it eases the problem with sparse data, and retains word meanings unchanged.
It is not impossible to improve word alignment by using English lemmatization.
We determined what effect lemmatization had in experiments using data from the BTEC (Paul, 2006) CSTAR track.
We collected a relatively large corpus of more than 678,000 sentences.
We conducted comprehensive evaluations and used multiple trans181 lation metrics to evaluate the results.
We found that our approach of using lemmatization improved both the word alignment and the quality of SMT with a small amounts of training data, and, while much work indicates that MA is useless in training large amounts of data (Lee, 2004), our intensive experiments proved that the chance to get a better MT quality using lemmatization is higher than that without it for large amounts of training data.
On the basis of successful use of lemmatization translation, we propose a new linear interpolation method by which we integrate the homologous features of translation models of the lemmatization and non-lemmatization system.
We found the integrated model improved all the components??performance in the translation.
2 Moses
training for system with lemmatization and without We used Moses to carry out the expriments.
Moses is the state of the art decoder for SMT.
It is an extension of Pharaoh (Koehn et al., 2003), and supports factor training and decoding.
Our idea can be easily implemented by Moses.
We feed Moses English words with two factors: surface word and lemma.
The only difference in training with lemmatization from that without is the alignment factor.
The former uses Chinese surface words and English lemmas as the alignment factor, but the latter uses Chinese surface words and English surface words.
Therefore, the lemmatized English is only used in word alignment.
All the other options of Moses are same for both the lemmatization translation and nonlemmatization translation.
We use the tool created by (Minnen et al., 2001) to complete the morphological analysis of English.
We had to make an English part-of-speech (POS) tagger that is compatible with the CLAWS-5 tagset to use this tool.
We use our in-house tagset and English tagged corpus to train a statistical POS tagger by using the maximum entropy principle.
Our tagset contains over 200 POS tags, most of which are consistent to the CLAWS-5.
The tagger achieved 93.7% accuracy for our test set.
We use the default features defined by Pharaoh in the phrase-based log-linear models i.e., a target language model, five translation models, and one distance-based distortion model.
The weighting parameters of these features were optimized in terms of BLEU by the approach of minimum error rate training (Och, 2003).
The data for training and test are from the IWSLT06 CSTAR track that uses the Basic Travel Expression Corpus (BTEC).
The BTEC corpus are relatively larger corpus for travel domain.
We use 678,748 Chinese/English parallel sentences as the training data in the experiments.
The number of words are about 3.9M and 4.4M for Chinese and English respectively.
The number of unique words for English is 28,709 before lemmatization and 24,635 after lemmatization.
A 15%-20% reduction in vocabulary is obtained by the lemmatization.
The test data are the one used in IWSLT06 evaluation.
It contains 500 Chinese sentences.
The test data of IWSLT05 are the development data for tuning the weighting parameters.
Multiple references are used for computing the automatic metrics.
3 Experiments
3.1 Regular test The purpose of the regular tests is to find what effect lemmatization has as the amount of training data increases.
We used the data from the IWSLT06 CSTAR track.
We started with 50,000 (50 K) of data, and gradually added more training data from a 678 K corpus to this.
We applied the methods in Section 2 to train the non-lemmatized translation and lemmatized translation systems.
The results are listed in Table 1.
We use the alignment error rate (AER) to measure the alignment performance, and the two popular automatic metric, BLEU1 and METEOR2 to evaluate the translations.
To measure the word alignment, we manually aligned 100 parallel sentences from the BTEC as the reference file.
We use the ?sure??links and the ?possible??links to denote the alignments.
As shown in Table 1, we found our approach improved word alignment uniformly from small amounts to large amounts of training data.
The maximal AER reduction is up to 27.4% for the 600K.
However, we found some mixed translation results in terms of BLEU.
The lemmatized 1http://domino.watson.ibm.com/library/CyberDig.nsf (keyword=RC22176) 2http://www.cs.cmu.edu/?alavie/METEOR 182 Table 1: Translation results as increasing amount of training data in IWSLT06 CSTAR track System AER BLEU METEOR 50K nonlem 0.217 0.158 0.427 lemma 0.199 0.167 0.431 100K nonlem 0.178 0.182 0.457 lemma 0.177 0.188 0.463 300K nonlem 0.150 0.223 0.501 lemma 0.132 0.217 0.505 400K nonlem 0.136 0.231 0.509 lemma 0.102 0.224 0.507 500K nonlem 0.119 0.235 0.519 lemma 0.104 0.241 0.522 600K nonlem 0.095 0.238 0.535 lemma 0.069 0.248 0.536 Table 2: Statistical significance test in terms of BLEU: sys1=non-lemma, sys2=lemma Data size Diff(sys1-sys2) 50K -0.092 [-0.0176,-0.0012] 100K -0.006 [-0.0155,0.0039] 300K 0.0057 [-0.0046,0.0161] 400K 0.0074 [-0.0023,0.0174] 500K -0.0054 [-0.0139,0.0035] 600K -0.0103 [-0.0201,-0.0006] translations did not outperform the non-lemmatized ones uniformly.
They did for small amounts of data, i.e., 50 K and 100 K, and for large amounts, 500 K and 600 K.
However, they failed for 300 K and 400 K.
The translations were under the statistical significance test by using the bootStrap scripts3.
The results giving the medians and confidence intervals are shown in Table 2, where the numbers indicate the median, the lower and higher boundary at 95% confidence interval.
we found the lemma systems were confidently better than the nonlem systems for the 50K and 600K, but didn?t for other data sizes.
This experiments proved that our proposed approach improved the qualities of word alignments that lead to the translation improvement for the 50K, 100K, 500K and 600K.
In particular, our results revealed large amounts of data of 500 K and 600 3http://projectile.is.cs.cmu.edu/research/public/tools/bootStrap /tutorial.htm Table 3: Competitive scores (BLEU) for non-lemmatization and lemmatization using randomly extracted corpora System 100K 300K 400K 600K total lemma 10/11 5.5/11 6.5/11 5/7 27/40 nonlem 1/11 5.5/11 4.5/11 2/7 13/40 K was improved by the lemmatization while it has been found impossible in most published results.
However, data of 300 K and 400 K worsen translations achieved by the lemmatization4.
In what follows, we discuss a method of random sampling of creating multiple corpora of varied sizes to see robustness of our approach and re-investigate the results of the 300K and 400K.
3.2 Random
sampling test In this section, we use a method of random extraction to generate new multiple training data for each corpus of one definite size.
The new data are extracted from the whole corpus of 678 K randomly.
We generate ten new corpora for 100 K, 300 K, and 400 K data and six new corpora for the 678 K data.
Thus, we create eleven and seven corpora of varied sizes if the corpora in the last experiments are counted.
We use the same method as in Section 2 for each generated corpus to construct systems to compare non-lemmatization and lemmatization.
The systems are evaluated again using the same test data.
The results are listed in Table 3 and Figure 1.
Table 3 shows the ?scoreboard??of non-lemmatized and lemmatized results in terms of BLEU.
If its score for the lemma system is higher than that for the nonlem system, the former earns one point; if equal, each earns 0.5; otherwise, the nonlem earns one point.
As we can see from the table, the results for the lemma system are better than those for the nonlem system for the 100K in 10 of the total 11 corpora.
Of the total 40 random corpora, the lemma systems outperform the nonlem systems in 27 times.
By analyzing the results from Tables 1 and 3, we can arrive at some conclusions.
The lemma systems outperform the nonlem for training corpora less than 4while the results was not confident by statistical significance test, the medians of 300K and 400K were lowered by the lemmatization 183 0.16 0.25 NL-600K L-600K NL-400K L-400K NL-300K L-300K NL-100K L-100K 1110987654321 0.169 0.178 0.187 0.196 0.205 0.214 0.223 0.232 0.241 BLEU Number of randomly extracted corpora Figure 1: Bleu scores for randomly extracted corpora 100 K.
The BLEU score favors the lemma system overwhelmingly for this size.
When the amount of training data is increased up to 600 K, the lemma still beat the nonlem system in most tests while the number of success by the nonlem system increases.
This random test, as a complement to the last experiment, reveals that the lemma either performs the same or better than the nonlem system for training data of any size.
Therefore, the lemma system is slightly better than the nonlem in general.
Figure 1 illustrates the BLEU scores for the ?lemma(L)??and ?nonlem(NL)??systems for randomly extracted corpora.
A higher number of points is obtained by the lemma system than the nonlem for each corpus.
4 Effect
of linear interpolation of features We generated translation models for lemmatization translation and non-lemmatization translation.
We found some features of the translation models could be added linearly.
For example, phrase translation model p(e| f ) can be calculated as, p(e| f ) = 1 pl(e| f ) +2 pnl(e| f ) where pl(e| f ) and pnl(e| f ) is the phrase translation models corresponding to the lemmatization system and non-lemma system.
1 + 2 = 1.
s can be obtained by maximizing likelihood or BLEU scores of a development data.
But we used the same values for all the .
p(e| f ) is the phrase translation model after linear interpolation.
Besides the phrase translation model, we used this approach to integrate Table 4: Effect of linear interpolation lemma nonlemma interpolation open track 0.1938 0.1993 0.2054 the three other features: phrase inverse probability, lexical probability, and lexical inverse probability.
We tested this integration using the open track of IWSLT 2006, a small task track.
The BLEU scores are shown in Table 4.
An improvement over both of the systems were observed.
5 Conclusions
We proposed a new approach of using lemmatization and linear interpolation of homologous features in SMT.
The principal idea is to use lemmatized English for the word alignment.
Our approach was proved effective for the BTEC Chinese to English translation.
It is significant in particular that we have target language, English, as the lemmatized object because it is less usual in SMT.
Nevertheless, we found our approach significantly improved word alignment and qualities of translations.
References Sharon Goldwater and David McClosky.
2005. Improving statistical MT through morphological analysis.
In Proceedings of HLT/EMNLP, pages 676??83, Vancouver, British Columbia, Canada, October.
Philipp Koehn, Franz J.
Och, and Daniel Marcu.
2003. Statistical phrase-based translation.
In HLT-NAACL 2003: Main Proceedings, pages 127??33.
Young-Suk Lee.
2004. Morphological analysis for statistical machine translation.
In HLT-NAACL 2004: Short Papers, pages 57??0, Boston, Massachusetts, USA.
Guido Minnen, John Carroll, and Darren Pearce.
2001. Applied morphological processing of english.
Natural Language Engineering, 7(3):207??23.
Franz Josef Och and Hermann Ney.
2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19??1.
Franz Josef Och.
2003. Minimum error rate training in statistical machine translation.
In ACL 2003, pages 160??67.
Michael Paul.
2006. Overview of the IWSLT 2006 Evaluation Campaign.
In Proc.
of the IWSLT, pages 1??5, Kyoto, Japan.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 185??88, Prague, June 2007.
c2007 Association for Computational Linguistics Extractive Summarization Based on Event Term Clustering Maofu Liu 1,2, Wenjie Li 1, Mingli Wu 1 and Qin Lu 1 1 Department of Computing The Hong Kong Polytechnic University {csmfliu, cswjli, csmlwu, csluqin}@comp.polyu.edu.hk 2 College of Computer Science and Technology Wuhan University of Science and Technology mfliu_china@hotmail.com Abstract Event-based summarization extracts and organizes summary sentences in terms of the events that the sentences describe.
In this work, we focus on semantic relations among event terms.
By connecting terms with relations, we build up event term graph, upon which relevant terms are grouped into clusters.
We assume that each cluster represents a topic of documents.
Then two summarization strategies are investigated, i.e. selecting one term as the representative of each topic so as to cover all the topics, or selecting all terms in one most significant topic so as to highlight the relevant information related to this topic.
The selected terms are then responsible to pick out the most appropriate sentences describing them.
The evaluation of clustering-based summarization on DUC 2001 document sets shows encouraging improvement over the well-known PageRank-based summarization.
1 Introduction
Event-based extractive summarization has emerged recently (Filatova and Hatzivassiloglou, 2004).
It extracts and organizes summary sentences in terms of the events that sentences describe.
We follow the common agreement that event can be formulated as ??Who] did [What] to [Whom] [When] and [Where]??and ?did [What]??denotes the key element of an event, i.e. the action within the formulation.
We approximately define the verbs and action nouns as the event terms which can characterize or partially characterize the event occurrences.
Most existing event-based summarization approaches rely on the statistical features derived from documents and generally associated with single events, but they neglect the relations among events.
However, events are commonly related with one another especially when the documents to be summarized are about the same or very similar topics.
Li et al (2006) report that the improved performance can be achieved by taking into account of event distributional similarities, but it does not benefit much from semantic similarities.
This motivated us to further investigate whether event-based summarization can take advantage of the semantic relations of event terms, and most importantly, how to make use of those relations.
Our idea is grouping the terms connected by the relations into the clusters, which are assumed to represent some topics described in documents.
In the past, various clustering approaches have been investigated in document summarization.
Hatzivassiloglou et al (2001) apply clustering method to organize the highly similar paragraphs into tight clusters based on primitive or composite features.
Then one paragraph per cluster is selected to form the summary by extraction or by reformulation.
Zha (2002) uses spectral graph clustering algorithm to partition sentences into topical groups.
Within each cluster, the saliency scores of terms and sentences are calculated using mutual reinforcement principal, which assigns high salience scores to the sentences that contain many terms with high salience scores.
The sentences and key phrases are selected by their saliency scores to generate the summary.
The similar work based on topic or event is also reported in (Guo and Stylios, 2005).
The granularity of clustering units mentioned above is rather coarse, either sentence or paragraph.
In this paper, we define event term as clustering 185 unit and implement a clustering algorithm based on semantic relations.
We extract event terms from documents and construct the event term graph by linking terms with the relations.
We then regard a group of closely related terms as a topic and make the following two alterative assumptions: (1) If we could find the most significant topic as the main topic of documents and select all terms in it, we could summarize the documents with this main topic.
(2) If we could find all topics and pick out one term as the representative of each topic, we could obtain the condensed version of topics described in the documents.
Based on these two assumptions, a set of cluster ranking, term selection and ranking and sentence extraction strategies are developed.
The remainder of this paper is organized as follows.
Section 2 introduces the proposed extractive summarization approach based on event term clustering.
Section 3 presents experiments and evaluations.
Finally, Section 4 concludes the paper.
2 Summarization
Based on Event Term Clustering 2.1 Event Term Graph We introduce VerbOcean (Chklovski and Pantel, 2004), a broad-coverage repository of semantic verb relations, into event-based summarization.
Different from other thesaurus like WordNet, VerbOcean provides five types of semantic verb relations at finer level.
This just fits in with our idea to introduce event term relations into summarization.
Currently, only the stronger-than relation is explored.
When two verbs are similar, one may denote a more intense, thorough, comprehensive or absolute action.
In the case of change-of-state verbs, one may denote a more complete change.
This is identified as the strongerthan relation in (Timothy and Patrick, 2004).
In this paper, only stronger-than is taken into account but we consider extending our future work with other applicable relations types.
The event term graph connected by term semantic relations is defined formally as, where V is a set of event terms and E is a set of relation links connecting the event terms in V.
The graph is directed if the semantic relation has the characteristic of the asymmetric.
Otherwise, it is undirected.
Figure 1 shows a sample of event term graph built from one DUC 2001 document set.
It is a directed graph as the stronger-than relation in VerbOcean exhibits the conspicuous asymmetric characteristic.
For example, ?fight??means to attempt to harm by blows or with weapons, while ?resist??means to keep from giving in.
Therefore, a directed link from ?fight??to ?resist??is shown in the following Figure 1.
),( EVG = Relations link terms together and form the event term graph.
Based upon it, term significance is evaluated and in turn sentence is judged whether to be extracted in the summary.
Figure 1.
Terms connected by semantic relations 2.2 Event Term Clustering Note that in Figure 1, some linked event terms, such as ?kill??
?rob?? ?threaten??and ?infect?? are semantically closely related.
They may describe the same or similar topic somehow.
In contrast, ?toler??
?resist??and ?fight??are clearly involved in another topic; although they are also reachable from ?kill??
Based on this observation, a clustering algorithm is required to group the similar and related event terms into the cluster of the topic.
In this work, event terms are clustered by the DBSCAN, a density-based clustering algorithm proposed in (Easter et al, 1996).
The key idea behind it is that for each term of a cluster the neighborhood of a given radius has to contain at least a minimum number of terms, i.e. the density in the neighborhood has to exceed some threshold.
By using this algorithm, we need to figure out appropriate values for two basic parameters, namely, Eps (denoting the searching radius from each term) and MinPts (denoting the minimum number of terms in the neighborhood of the term).
We assign one semantic relation step to Eps since there is no clear distance concept in the event term 186 graph.
The value of Eps is experimentally set in our experiments.
We also make some modification on Easter?s DBSCAN in order to accommodate to our task.
Figure 2 shows the seven term clusters generated by the modified DBSCAN clustering algorithm from the graph in Figure 1.
We represent each cluster by the starting event term in bold font.
fight resist consider expect announce offer list public accept honor publish study found place prepare toler pass fear threaten kill feel suffer live survive undergo ambush rob infect endure run moverush report investigate file satisfy please manage accept Figure 2.
Term clusters generated from Figure 1 2.3 Cluster Ranking The significance of the cluster is calculated by ???
??? = CCCt t Ct ti iii ddCsc /)( where is the degree of the term t in the term graph.
C is the set of term clusters obtained by the modified DBSCAN clustering algorithm and is the ith one.
Obviously, the significance of the cluster is calculated from global point of view, i.e. the sum of the degree of all terms in the same cluster is divided by the total degree of the terms in all clusters.
t d i C 2.4 Term Selection and Ranking Representative terms are selected according to the significance of the event terms calculated within each cluster (i.e.
from local point of view) or in all clusters (i.e.
from global point of view) by LOCAL: or ??
?? = i ct tt ddtst /)( GLOBAL: ? ? = Ccct tt ii ddtst /)( Then two strategies are developed to select the representative terms from the clusters.
(1) One Cluster All Terms (OCAT) selects all terms within the first rank cluster.
The selected terms are then ranked according to their significance.
(2) One Term All Cluster (OTAC) selects one most significant term from each cluster.
Notice that because terms compete with each other within clusters, it is not surprising to see )()( 21 tsttst < even when,. To address this problem, the representative terms are ranked according to the significance of the clusters they belong to.
)()( 21 csccsc > ),( 2211 ctct ? 2.5 Sentence Evaluation and Extraction A representative event term may associate to more than one sentence.
We extract only one of them as the description of the event.
To this end, sentences are compared according to the significance of the terms in them.
MAX compares the maximum significance scores, while SUM compares the sum of the significance scores.
The sentence with either higher MAX or SUM wins the competition and is picked up as a candidate summary sentence.
If the sentence in the first place has been selected by another term, the one in the second place is chosen.
The ranks of these candidates are the same as the ranks of the terms they are selected for.
Finally, candidate sentences are selected in the summary until the length limitation is reached.
3 Experiments
We evaluate the proposed approaches on DUC 2001 corpus which contains 30 English document sets.
There are 431 event terms on average in each document set.
The automatic evaluation tool, ROUGE (Lin and Hovy, 2003), is run to evaluate the quality of the generated summaries (200 words in length).
The tool presents three values including unigram-based ROUGE-1, bigram-based ROUGE2 and ROUGE-W which is based on longest common subsequence weighted by the length.
Google?s PageRank (Page and Brin, 1998) is one of the most popular ranking algorithms.
It is also graph-based and has been successfully applied in summarization.
Table 1 lists the result of our implementation of PageRank based on event terms.
We then compare it with the results of the event term clustering-based approaches illustrated in Table 2.
PageRank ROUGE-1 0.32749 187 ROUGE-2 0.05670 ROUGE-W 0.11500 Table 1.
Evaluations of PageRank-based Summarization LOCAL+OTAC MAX SUM ROUGE-1 0.32771 0.33243 ROUGE-2 0.05334 0.05569 ROUGE-W 0.11633 0.11718 GLOBAL+OTAC MAX SUM ROUGE-1 0.32549 0.32966 ROUGE-2 0.05254 0.05257 ROUGE-W 0.11670 0.11641 LOCAL+OCAT MAX SUM ROUGE-1 0.33519 0.33397 ROUGE-2 0.05662 0.05869 ROUGE-W 0.11917 0.11849 GLOBAL+OCAT MAX SUM ROUGE-1 0.33568 0.33872 ROUGE-2 0.05506 0.05933 ROUGE-W 0.11795 0.12011 Table 2.
Evaluations of Clustering-based Summarization The experiments show that both assumptions are reasonable.
It is encouraging to find that our event term clustering-based approaches could outperform the PageRank-based approach.
The results based on the second assumption are even better.
This suggests indeed there is a main topic in a DUC 2001 document set.
4 Conclusion
In this paper, we put forward to apply clustering algorithm on the event term graph connected by semantic relations derived from external linguistic resource.
The experiment results based on our two assumptions are encouraging.
Event term clustering-based approaches perform better than PageRank-based approach.
Current approaches simply utilize the degrees of event terms in the graph.
In the future, we would like to further explore and integrate more information derived from documents in order to achieve more significant results using the event term clusteringbased approaches.
Acknowledgments The work described in this paper was fully supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No.
PolyU5181/03E). References Chin-Yew Lin and Eduard Hovy.
2003. Automatic Evaluation of Summaries using N-gram Cooccurrence Statistics.
In Proceedings of HLT/NAACL 2003, pp71-78.
Elena Filatova and Vasileios Hatzivassiloglou.
2004. Event-based Extractive Summarization.
In Proceedings of ACL 2004 Workshop on Summarization, pp104-111.
Hongyuan Zha.
2002. Generic Summarization and keyphrase Extraction using Mutual Reinforcement Principle and Sentence Clustering.
In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, 2002.
pp113-120. Lawrence Page and Sergey Brin, Motwani Rajeev and Winograd Terry.
1998. The PageRank CitationRanking: Bring Order to the Web.
Technical Report,Stanford University.
Martin Easter, Hans-Peter Kriegel, Jrg Sander, et al.1996. A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise.
In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Menlo Park, CA, 1996.
226-231. Lawrence Page, Sergey Brin, Rajeev Motwani and Terry Winograd.
1998. The PageRank CitationRanking: Bring Order to the Web.
Technical Report,Stanford University.
Timothy Chklovski and Patrick Pantel.
2004. VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations.
In Proceedings of Conference on Empirical Methods in Natural Language Processing, 2004.
Vasileios Hatzivassiloglou, Judith L.
Klavans, Melissa L.
Holcombe, et al.2001. Simfinder: A Flexible Clustering Tool for Summarization.
In Workshop on Automatic Summarization, NAACL, 2001.
Wenjie Li, Wei Xu, Mingli Wu, et al.2006. Extractive Summarization using Interand Intra-Event Relevance.
In Proceedings of ACL 2006, pp369-376.
Yi Guo and George Stylios.
2005. An intelligent summarization system based on cognitive psychology.
Journal of Information Sciences, Volume 174, Issue 1-2, Jun.
2005, pp1-36 .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 189??92, Prague, June 2007.
c2007 Association for Computational Linguistics Machine Translation between Turkic Languages A.
Cuneyd TANTU?G Istanbul Technical University Istanbul, Turkey tantug@itu.edu.tr Esref ADALI Istanbul Technical University Istanbul, Turkey adali@itu.edu.tr Kemal OFLAZER Sabanci University Istanbul, Turkey oflazer@sabanciuniv.edu Abstract We present an approach to MT between Turkic languages and present results from an implementation of a MT system from Turkmen to Turkish.
Our approach relies on ambiguous lexical and morphological transfer augmented with target side rule-based repairs and rescoring with statistical language models.
1 Introduction
Machine translation is certainly one of the toughest problems in natural language processing.
It is generally accepted however that machine translation between close or related languages is simpler than full-fledged translation between languages that differ substantially in morphological and syntactic structure.
In this paper, we present a machine translation system from Turkmen to Turkish, both of which belong to the Turkic language family.
Turkic languages essentially exhibit the same characteristics at the morphological and syntactic levels.
However, except for a few pairs, the languages are not mutually intelligible owing to substantial divergences in their lexicons possibly due to different regional and historical influences.
Such divergences at the lexical level along with many but minor divergences at morphological and syntactic levels make the translation problem rather non-trivial.
Our approach is based on essentially morphological processing, and direct lexical and morphological transfer, augmented with substantial multi-word processing on the source language side and statistical processing on the target side where data for statistical language modelling is more readily available.
2 Related
Work Studies on machine translation between close languages are generally concentrated around certain Slavic languages (e.g., Czech?Slovak, Czech?Polish, Czech?Lithuanian (Hajic et al., 2003)) and languages spoken in the Iberian Peninsula (e.g., Spanish?Catalan (Canals et al., 2000), Spanish?Galician (Corbi-Bellot et al., 2003) and Spanish?Portugese (Garrido-Alenda et al., 2003).
Most of these implementations use similar modules: a morphological analyzer, a part-of-speech tagger, a bilingual transfer dictionary and a morphological generator.
Except for the Czech?Lithuanian system which uses a shallow parser, syntactic parsing is not necessary in most cases because of the similarities in word orders.
Also, the lexical semantic ambiguity is usually preserved so, none of these systems has any module for handling the lexical ambiguity.
For Turkic languages, Hamzao?glu (1993) has developed a system from Turkish to Azerbaijani, and Altntas (2000) has developed a system from Turkish to Crimean Tatar.
3 Turkic
Languages Turkic languages, spoken by more than 180 million people, constitutes subfamily of Ural-Altaic languages and includes languages like Turkish, Azerbaijani, Turkmen, Uzbek, Kyrghyz, Kazakh, Tatar, Uyghur and many more.
All Turkic languages have very productive inflectional and derivational agglutinative morphology.
For example the Turkish word evlerimizden has three inflectional morphemes attached to a noun root ev (house), for the plural form with second person plural possessive agreement and ablative case: 189 evlerimizden (from our houses) ev+ler+imiz+den ev+Noun+A3pl+P1sg+Abl All Turkic languages exhibit SOV constituent order but depending on discourse requirements, constituents can be in any order without any substantial formal constraints.
Syntactic structures between Turkic languages are more or less parallel though there are interesting divergences due to mismatches in multi-word or idiomatic constructions.
4 Approach
Our approach is based on a direct morphological transfer with some local multi-word processing on the source language side, and statistical disambiguation on the target language side.
The main steps of our model are: 1.
Source Language (SL) Morphological Analysis 2.
SL Morphological Disambiguation 3.
Multi-Word Unit (MWU) Recognizer 4.
Morphological Transfer 5.
Root Word Transfer 6.
Statistical Disambiguation and Rescoring (SLM) 7.
Sentence Level Rules (SLR) 8.
Target Language (TL) Morphological Generator Steps other than 3, 6 and 7 are the minimum requirements for a direct morphological translation model (henceforth, the baseline system).
The MWU Recognizer, SLM and SLR modules are additional modules for the baseline system to improve the translation quality.
Sourcelanguagemorphologicalanalysismayproduce multiple interpretation of a source word, and usually, depending on the ambiguities brought about by multiple possible segmentations into root and suffixes, there may be different root words of possibly different parts-of-speech for the same word form.
Furthermore, each root word thus produced may map to multiple target root words due to word sense ambiguity.
Hence, among all possible sentences that can be generated with these ambiguities, the most probable one is selected by using various types of SLMs that are trained on target language corpora annotated with disambiguated roots and morphological features.
MWU processing in Turkic languages involves more than the usual lexicalized collocations and involves detection of mostly unlexicalized intraword morphological patterns (Oflazer et al., 2004).
Source MWUs are recognized and marked during source analysis and the root word transfer module maps these either to target MWU patterns, or directly translates when there is a divergence.
Morphological transfer is implemented by a set of rules hand-crafted using the contrastive knowledge between the selected language pair.
Although the syntactic structures are very similar between Turkic languages, there are quite many minor situations where target morphological features marking features such as subject-verb agreement have to be recovered when such features are not present in the source.
Furthermore, occasionally certain phrases have to be rearranged.
Finally, a morphological generator produces the surface forms of the lexical forms in the sentence.
5 Turkmen
to Turkish MT System The first implementation of our approach is from Turkmen to Turkish.
A general diagram of our MT system is presented in Figure 1.
The morphological analysis on the Turkmen side is performed by a two-level morphological analyzer developed using Xerox finite state tools (Tantu?g et al., 2006).
It takes a Turkmen word and produces all possible morphological interpretations of that word.
A simple experiment on our test set indicates that the average Turkmen word gets about 1.55 analyses.
The multiword recognition module operates on the output of the morphological analyzer and wherever applicable, combines analyses of multiple tokens into a new analysis with appropriate morphological features.
One side effect of multi-word processing is a small reduction in morphological ambiguity, as when such unitsarecombined, theremainingmorphologicalinterpretations for these tokens are deleted.
The actual transfer is carried out by transferring the morphological structures and word roots from the source language to the target language maintaining any ambiguity in the process.
These are implemented with finite state transducers that are compiled from replace rules written in the Xerox regular expressionlanguage.1 Averysimpleexampleofthis transfer is shown in Figure 2.2 1The current implementation employs 28 replace rules for morphological feature transfer and 19 rules for sentence level processing.
2+Pos:Positive polarity, +A3sg: 3rd person singular agreement, +Inf1,+Inf2: infinitive markers, +P3sg, +Pnon: possessive agreement markers, +Nom,+Acc: Nominative and ac190 Figure 1: Main blocks of the translation system osmegi ??
Source Morphological Analysis ??
os+Verb+Pos?DB+Noun+Inf1+A3sg+P3sg+Nom os+Verb+Pos?DB+Noun+Inf1+A3sg+Pnon+Acc ??
Source-to-Target Morphological Feature Transfer ??
os+Verb+Pos?DB+Noun+Inf2+A3sg+P3sg+Nom os+Verb+Pos?DB+Noun+Inf2+A3sg+Pnon+Acc ??
Source-to-Target Root word Transfer ?? ilerle+Verb+Pos?DB+Noun+Inf2+A3sg+P3sg+Nom ilerle+Verb+Pos?DB+Noun+Inf2+A3sg+Pnon+Acc buyu+Verb+Pos?DB+Noun+Inf2+A3sg+P3sg+Nom buyu+Verb+Pos?DB+Noun+Inf2+A3sg+Pnon+Acc ??
Target Morphological Generation ?? ilerlemesi (the progress of (something)) ilerlemeyi (the progress (as direct object)) buyumesi (the growth of (something)) buyumeyi (the growth (as direct object)) Figure 2: Word transfer In this example, once the morphological analysis is produced, first we do a morphological feature transfer mapping.
In this case, the only interesting mapping is the change of the infinitive marker.
The source root verb is then ambiguously mapped to two verbs on the Turkish side.
Finally, the Turkish surface form is generated by the morphological generator.
Note that all the morphological processing details such as vowel harmony resolution (a morphographemic process common to all Turkic languages though not in identical ways) are localized to morphological generation.
Root word transfer is also based on a large transcusative case markers.
ducer compiled from bilingual dictionaries which contain many-to-many mappings.
During mapping this transducer takes into account the source root word POS.3 In some rare cases, mapping the word root is not sufficient to generate a legal Turkish lexical structure, as sometimes a required feature on the target side may not be explicitly available on the source word to generate a proper word.
In order to produce the correct mapping in such cases, some additional lexicalized rules look at a wider context and infer any needed features.
While the output of morphological feature transfermoduleisusuallyunambiguous, ambiguityarises during the root word transfer phase.
We attempt to resolve this ambiguity on the target language side using statistical language models.
This however presents additional difficulties as any statistical language model for Turkish (and possibly other Turkic languages) which is built by using the surface forms suffers from data sparsity problems.
This is due to agglutinative morphology whereby a root word may give rise to too many inflected forms (about a hundred inflected forms for nouns and much more for verbs; when productive derivations are considered these numbers grow substantially!).
Therefore, instead of building statistical language models on full word forms, we work with morphologically analyzed and disambiguated target language corpora.
For example, we use a language model that is only based on the (disambiguated) root words to disambiguate ambiguous root words that arise from root 3Statistics on the test set indicate that on the average each source language root word maps to about 2 target language root words.
191 word transfer.
We also employ a language model which is trained on the last set of inflectional features of morphological parses (hence does not involve any root words.) Although word-by-word translation can produce reasonably high quality translations, but in many cases, it is also the source of many translation errors.
To alleviate the shortcomings of the word-by-word translation approach, we resort to a series of rules that operate across the whole sentence.
Such rules operate on the lexical and surface representation of the output sentence.
For example, when the source language is missing a subject agreement marker on a verb, this feature can not be transferred to the target language and the target language generator will fail to generate the appropriate word.
We use some simple heuristics that try to recover the agreement information from any overt pronominal subject in nominative case, and that failing, set the agreement to 3rd person singular.
Some sentence level rules require surface forms because this set of rules usuallymakeorthographicchangesaffectedbyprevious word forms.
In the following example, suitable variants of the clitics de and mi must be selected so that vowel harmony with the previous token is preserved.
o de gordu mi?
?o da gordu mu?
(did he see too?) A wide-coverage Turkish morphological analyzer (Oflazer, 1994) made available to be used in reverse direction to generate the surface forms of the translations.
6 Results
and Evaluation We have tracked the progress of our changes to our system using the BLEU metric (Papineni et al., 2004), though it has serious drawbacks for agglutinative and free constituent order languages.
The performance of the baseline system (all steps above, except 3, 6, and 7) and systems with additional modules are given in Table 1 for a set of 254 Turkmen sentences with 2 reference translations each.
As seen in the table, each module contributes to the performance of the baseline system.
Furthermore, a manual investigation of the outputs indicates that the actual quality of the translations is higher than the one indicated by the BLEU score.4 The errors mostly stem from the statical language models 4Therearemanytranslationswhichpreservethesamemeaning with the references but get low BLEU scores.
not doing a good job at selecting the right root words and/or the right morphological features.
System BLEU Score Baseline 26.57 Baseline + MWU 28.45 Baseline + MWU + SLM 31.37 Baseline + MWU + SLM + SLR 33.34 Table 1: BLEU Scores 7 Conclusions We have presented an MT system architecture between Turkic languages using morphological transfer coupled with target side language modelling and results from a Turkmen to Turkish system.
The results are quite positive but there is quite some room for improvement.
Our current work involves improving the quality of our current system as well as expanding this approach to Azerbaijani and Uyghur.
Acknowledgments This work was partially supported by Project 106E048 funded by The Scientific and Technical Research Council of Turkey.
Kemal Oflazer acknowledges the kind support of LTI at Carnegie Mellon University, where he was a sabbatical visitor during the academic year 2006 ??2007.
References A.
Cuneyd Tantu?g, Esref Adal, Kemal Oflazer.
2006. Computer Analysis of the Turkmen Language Morphology.
Fin-TAL, Lecture Notes in Computer Science, 4139:186-193.
A. Garrido-Alenda et al.2003. Shallow Parsing for Portuguese-Spanish Machine Translation.
in TASHA 2003: Workshop on Tagging and Shallow Processing of Portuguese, Lisbon, Portugal.
A. M.
Corbi-Bellot et al.2005. An open-source shallow-transfer machine translation engine for the Romance languages of Spain.
in 10th EAMT conference ?Practical applications of machine translation??
Budapest, Hungary.
Jan Hajic, Petr Homola, Vladislav Kubon.
2003. A simple multilingual machine translation system.
MT Summit IX.
?Ilker Hamzao?glu.
1993. Machine translation from Turkish to other Turkic languages and an implementation for the Azeri language.
MSc Thesis, Bogazici University, Istanbul.
Kemal Altntas.
2000. Turkish to Crimean Tatar Machine Translation System.
MSc Thesis, Bilkent University, Ankara.
Kemal Oflazer.
1994. Two-level description of Turkish morphology.
Literary and Linguistic Computing, 9(2).
Kemal Oflazer, Ozlem Cetino?glu, Bilge Say.
2004. Integrating Morphology with Multi-word Expression Processing in Turkish.
The ACL 2004 Workshop on Multiword Expressions: Integrating Processing.
Kishore Papineni et al.2002. BLEU : A Method for Automatic Evaluation of Machine Translation.
Association of Computational Linguistics, ACL??2.
Raul Canals-Marote et al.2000. interNOSTRUM: a Spanish-Catalan Machine Translation System.
Machine Translation Review, 11:21-25.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 193??96, Prague, June 2007.
c2007 Association for Computational Linguistics Measuring Importance and Query Relevance in Topic-focused Multi-document Summarization Surabhi Gupta and Ani Nenkova and Dan Jurafsky Stanford University Stanford, CA 94305 surabhi@cs.stanford.edu, {anenkova,jurafsky}@stanford.edu Abstract The increasing complexity of summarization systems makes it difficult to analyze exactly which modules make a difference in performance.
We carried out a principled comparison between the two most commonly used schemes for assigning importance to words in the context of query focused multi-document summarization: raw frequency (word probability) and log-likelihood ratio.
We demonstrate that the advantages of log-likelihood ratio come from its known distributional properties which allow for the identification of a set of words that in its entirety defines the aboutness of the input.
We also find that LLR is more suitable for query-focused summarization since, unlike raw frequency, it is more sensitive to the integration of the information need defined by the user.
1 Introduction
Recently the task of multi-document summarization in response to a complex user query has received considerable attention.
In generic summarization, the summary is meant to give an overview of the information in the documents.
By contrast, when the summary is produced in response to a user query or topic (query-focused, topic-focused, or generally focused summary), the topic/query determines what information is appropriate for inclusion in the summary, making the task potentially more challenging.
In this paper we present an analytical study of two questions regarding aspects of the topic-focused scenario.
First, two estimates of importance on words have been used very successfully both in generic and query-focused summarization: frequency (Luhn, 1958; Nenkova et al., 2006; Vanderwende et al., 2006) and loglikelihood ratio (Lin and Hovy, 2000; Conroy et al., 2006; Lacatusu et al., 2006).
While both schemes have proved to be suitable for summarization, with generally better results from loglikelihood ratio, no study has investigated in what respects and by how much they differ.
Second, there are many little-understood aspects of the differences between generic and query-focused summarization.
For example, we?d like to know if a particular word weighting scheme is more suitable for focused summarization than others.
More significantly, previous studies show that generic and focused systems perform very similarly to each other in query-focused summarization (Nenkova, 2005) and it is of interest to find out why.
To address these questions we examine the two weighting schemes: raw frequency (or word probability estimated from the input), and log-likelihood ratio (LLR) and two of its variants.
These metrics are used to assign importance to individual content words in the input, as we discuss below.
Word probability R(w) = nN, where n is the number of times the word w appeared in the input and N is the total number of words in the input.
Log-likelihood ratio (LLR) The likelihood ratio  (Manning and Schutze, 1999) uses a background corpus to estimate the importance of a word and it is proportional to the mutual information between a word w and the input to be summarized; (w) is defined as the ratio between the probability (under a binomial distribution) of observing w in the input and the background corpus assuming equal probability of occurrence of w in both and the probability of the data assuming different probabilities for w in the input and the background corpus.
LLR with cut-off (LLR(C)) A useful property of the log-likelihood ratio is that the quantity 193 2 log() is asymptotically well approximated by ?2 distribution.
A word appears in the input significantly more often than in the background corpus when 2 log() > 10.
Such words are called signature terms in Lin and Hovy (2000) who were the first to introduce the log-likelihood weighting scheme for summarization.
Each descriptive word is assigned an equal weight and the rest of the words have a weight of zero: R(w) = 1 if ( 2 log((w)) > 10), 0 otherwise.
This weighting scheme has been adopted in several recent generic and topic-focused summarizers (Conroy et al., 2006; Lacatusu et al., 2006).
LLR(CQ) The above three weighting schemes assign a weight to words regardless of the user query and are most appropriate for generic summarization.
When a user query is available, it should inform the summarizer to make the summary more focused.
In Conroy et al.(2006) such query sensititivity is achieved by augmenting LLR(C) with all content words from the user query, each assigned a weight of 1 equal to the weight of words defined by LLR(C) as topic words from the input to the summarizer.
2 Data
We used the data from the 2005 Document Understanding Conference (DUC) for our experiments.
The task is to produce a 250-word summary in response to a topic defined by a user for a total of 50 topics with approximately 25 documents for each marked as relevant by the topic creator.
In computing LLR, the remaining 49 topics were used as a background corpus as is often done by DUC participants.
A sample topic (d301) shows the complexity of the queries: Identify and describe types of organized crime that crosses borders or involves more than one country.
Name the countries involved.
Also identify the perpetrators involved with each type of crime, including both individuals and organizations if possible.
3 The
Experiment In the summarizers we compare here, the various weighting methods we describe above are used to assign importance to individual content words in the input.
The weight or importance of a sentence S in GENERIC FOCUSED Frequency 0.11972 0.11795 (0.11168??.12735) (0.11010??.12521) LLR 0.11223 0.11600 (0.10627??.11873) (0.10915??.12281) LLR(C) 0.11949 0.12201 (0.11249??.12724) (0.11507??.12950) LLR(CQ) not app 0.12546 (.11884??13247) Table 1: SU4 ROUGE recall (and 95% confidence intervals) for runs on the entire input (GENERIC) and on relevant sentences (FOCUSED).
the input is defined as WeightR(S) = summationdisplay w?S R(w) (1) where R(w) assigns a weight for each word w.
For GENERIC summarization, the top scoring sentences in the input are taken to form a generic extractive summary.
In the computation of sentence importance, only nouns, verbs, adjectives and adverbs are considered and a short list of light verbs are excluded: ?has, was, have, are, will, were, do, been, say, said, says??
For FOCUSED summarization, we modify this algorithm merely by running the sentence selection algorithm on only those sentences in the input that are relevent to the user query.
In some previous DUC evaluations, relevant sentences are explicitly marked by annotators and given to systems.
In our version here, a sentence in the input is considered relevant if it contains at least one word from the user query.
For evaluation we use ROUGE (Lin, 2004) SU4 recall metric1, which was among the official automatic evaluation metrics for DUC.
4 Results
The results are shown in Table 1.
The focused summarizer using LLR(CQ) is the best, and it significantly outperforms the focused summarizer based on frequency.
Also, LLR (using log-likelihood ratio to assign weights to all words) perfroms significantly worse than LLR(C).
We can observe some trends even from the results for which there is no significance.
Both LLR and LLR(C) are sensitive to the introduction of topic relevance, producing somewhat better summaries in the FOCUSED scenario 1-n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -d 194 compared to the GENERIC scenario.
This is not the case for the frequency summarizer, where using only the relevant sentences has a negative impact.
4.1 Focused
summarization: do we need query expansion?
In the FOCUSED condition there was little (for LLR weighting) or no (for frequency) improvement over GENERIC.
One possible explanation for the lack of clear improvement in the FOCUSED setting is that there are not enough relevant sentences, making it impossible to get stable estimates of word importance.
Alternatively, it could be the case that many of the sentences are relevant, so estimates from the relevant portion of the input are about the same as those from the entire input.
To distinguish between these two hypotheses, we conducted an oracle experiment.
We modified the FOCUSED condition by expanding the topic words from the user query with all content words from any of the human-written summaries for the topic.
This increases the number of relevant sentences for each topic.
No automatic method for query expansion can be expected to give more accurate results, since the content of the human summaries is a direct indication of what information in the input was important and relevant and, moreover, the ROUGE evaluation metric is based on direct n-gram comparison with these human summaries.
Even under these conditions there was no significant improvement for the summarizers, each getting better by 0.002: the frequency summarizer gets R-SU4 of 0.12048 and the LLR(CQ) summarizer achieves R-SU4 of 0.12717.
These results seem to suggest that considering the content words in the user topic results in enough relevant sentences.
Indeed, Table 2 shows the minimum, maximum and average percentage of relevant sentences in the input (containing at least one content words from the user the query), both as defined by the original query and by the oracle query expansion.
It is clear from the table that, on average, over half of the input comprises sentences that are relevant to the user topic.
Oracle query expansion makes the number of relevant sentences almost equivalent to the input size and it is thus not surprising that the corresponding results for content selection are nearly identical to the query independent Original query Oracle query expansion Min 13% 52% Average 57% 86% Max 82% 98% Table 2: Percentage of relevant sentences (containing words from the user query) in the input.
The oracle query expansion considers all content words form human summaries of the input as query words.
runs of generic summaries for the entire input.
These numbers indictate that rather than finding ways for query expansion, it might instead be more important to find techniques for constraining the query, determining which parts of the input are directly related to the user questions.
Such techniques have been described in the recent multi-strategy approach of Lacatusu et al.(2006) for example, where one of the strategies breaks down the user topic into smaller questions that are answered using robust question-answering techniques.
4.2 Why
is log-likelihood ratio better than frequency?
Frequency and log-likelihood ratio weighting for content words produce similar results when applied to rank all words in the input, while the cut-off for topicality in LLR(C) does have a positive impact on content selection.
A closer look at the two weighting schemes confirms that when cut-off is not used, similar weighting of content words is produced.
The Spearman correlation coefficient between the weights for words assigned by the two schemes is on average 0.64.
At the same time, it is likely that the weights of sentences are dominated by only the top most highly weighted words.
In order to see to what extent the two schemes identify the same or different words as the most important ones, we computed the overlap between the 250 most highly weighted words according to LLR and frequency.
The average overlap across the 50 sets was quite large, 70%.
To illustrate the degree of overlap, we list below are the most highly weighted words according to each weighting scheme for our sample topic concerning crimes across borders.
LLR drug, cocaine, traffickers, cartel, police, crime, enforcement, u.s., smuggling, trafficking, arrested, government, seized, year, drugs, organised, heroin, criminal, cartels, last, 195 official, country, law, border, kilos, arrest, more, mexican, laundering, officials, money, accounts, charges, authorities, corruption, anti-drug, international, banks, operations, seizures, federal, italian, smugglers, dealers, narcotics, criminals, tons, most, planes, customs Frequency drug, cocaine, officials, police, more, last, government, year, cartel, traffickers, u.s., other, drugs, enforcement, crime, money, country, arrested, federal, most, now, trafficking, seized, law, years, new, charges, smuggling, being, official, organised, international, former, authorities, only, criminal, border, people, countries, state, world, trade, first, mexican, many, accounts, according, bank, heroin, cartels It becomes clear that the advantage of likelihood ratio as a weighting scheme does not come from major differences in overall weights it assigns to words compared to frequency.
It is the significance cut-off for the likelihood ratio that leads to noticeable improvement (see Table 1).
When this weighting scheme is augmented by adding a score of 1 for content words that appear in the user topic, the summaries improve even further (LLR(CQ)).
Half of the improvement can be attributed to the cut-off (LLR(C)), and the other half to focusing the summary using the information from the user query (LLR(CQ)).
The advantage of likelihood ratio comes from its providing a principled criterion for deciding which words are truly descriptive of the input and which are not.
Raw frequency provides no such cut-off.
5 Conclusions
In this paper we examined two weighting schemes for estimating word importance that have been successfully used in current systems but have not todate been directly compared.
Our analysis confirmed that log-likelihood ratio leads to better results, but not because it defines a more accurate assignment of importance than raw frequency.
Rather, its power comes from the use of a known distribution that makes it possible to determine which words are truly descriptive of the input.
Only when such words are viewed as equally important in defining the topic does this weighting scheme show improved performance.
Using the significance cut-off and considering all words above it equally important is key.
Log-likelihood ratio summarizer is more sensitive to topicality or relevance and produces summaries that are better when it take the user request into account than when it does not.
This is not the case for a summarizer based on frequency.
At the same time it is noteworthy that the generic summarizers perform about as well as their focused counterparts.
This may be related to our discovery that on average 57% of the sentences in the document are relevant and that ideal query expansion leads to a situation in which almost all sentences in the input become relevant.
These facts could be an unplanned side-effect from the way the test topics were produced: annotators might have been influenced by information in the input to be summarizied when defining their topic.
Such observations also suggest that a competitive generic summarizer would be an appropriate baseline for the topicfocused task in future DUCs.
In addition, including some irrelavant documents in the input might make the task more challenging and allow more room for advances in query expansion and other summary focusing techniques.
References J.
Conroy, J.
Schlesinger, and D.
O?Leary. 2006.
Topic-focused multi-document summarization using an approximate oracle score.
In Proceedings of the COLING/ACL??6 (Poster Session).
F. Lacatusu, A.
Hickl, K.
Roberts, Y.
Shi, J.
Bensley, B.
Rink, P.
Wang, and L.
Taylor. 2006.
Lcc?s gistexter at duc 2006: Multi-strategy multi-document summarization.
In Proceedings of DUC??6.
C. Lin and E.
Hovy. 2000.
The automated acquisition of topic signatures for text summarization.
In Proceedings of COLING??0.
C. Lin.
2004. Rouge: a package for automatic evaluation of summaries.
In Proceedings of the Workshop on Text Summarization Branches Out (WAS 2004).
H. P.
Luhn. 1958.
The automatic creation of literature abstracts.
IBM Journal of Research and Development, 2(2):159??65.
C. Manning and H.
Schutze. 1999.
Foundations of Statistical Natural Language Processing.
MIT Press.
A. Nenkova, L.
Vanderwende, and K.
McKeown. 2006.
A compositional context sensitive multi-document summarizer: Exploring the factors that influence summarization.
In Proceedings of ACM SIGIR??6.
A. Nenkova.
2005. Automatic text summarization of newswire: lessons learned from the document understanding conference.
In Proceedings of AAAI??5.
L. Vanderwende, H.
Suzuki, and C.
Brockett. 2006.
Microsoft research at duc 2006: Task-focused summarization with sentence simplification and lexical expansion.
In Proceedings of DUC??6.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 197??00, Prague, June 2007.
c2007 Association for Computational Linguistics Expanding Indonesian-Japanese Small Translation Dictionary Using a Pivot Language Masatoshi Tsuchiya??Ayu Purwarianti??Toshiyuki Wakita??Seiichi Nakagawa??
?Information and Media Center / ?Department of Information and Computer Sciences, Toyohashi University of Technology tsuchiya@imc.tut.ac.jp, {wakita,ayu,nakagawa}@slp.ics.tut.ac.jp Abstract We propose a novel method to expand a small existing translation dictionary to a largetranslationdictionaryusingapivotlanguage.
Our method depends on the assumption that it is possible to nd a pivot language for a given language pair on condition that there are both a large translation dictionary from the source language to the pivot language, and a large translation dictionary from the pivot language to the destination language.
Experiments that expands the Indonesian-Japanese dictionary using the English language as a pivot language shows that the proposed method can improveperformanceofarealCLIRsystem.
1 Introduction
Rich cross lingual resources including large translation dictionaries are necessary in order to realize working cross-lingual NLP applications.
However, it is infeasible to build such resources for all languagepairs, becausetherearemanylanguagesinthe world.
Actually, while rich resources are available for several popular language pairs like the English language and the Japanese language, poor resources are only available for rest unfamiliar language pairs.
In order to resolve this situation, automatic construction of translation dictionary is effective, but it is quite difcult as widely known.
We, therefore, concentrateonthetaskofexpandingasmallexisting translation dictionary instead of it.
Let us consider three dictionaries: a small seed dictionary which consists of headwords in the source language and their translations in the destination language, a large source-pivotdictionarywhichconsistsofheadwords in the source language and their translations in the pivot language, and a large pivot-destination dictionary which consists of headwords in the pivot language and their translations in the destination language.
When these three dictionaries are given, expanding the seed dictionary is to translate words in the source language that meets two conditions: (1) they are not contained in the seed dictionary, and (2) they can be translated to the destination language transitively referring both the source-pivot dictionary and the pivot-destination dictionary.
Obviously, this task depends on two assumptions: (a) the existence of the small seed dictionary, and (b) the existence of the pivot language which meets the condition that there are both a large sourcepivot dictionary and a large pivot-destination dictionary.
Because of the rst assumption, it is true that this task cannot be applied to a brand-new language pair.
However, the number of such brandnew language pairs are decreasing while machinereadable language resources are increasing.
Moreover, The second assumption is valid for many language pairs, when supposing the English language as a pivot.
From these point of view, we think that theexpansiontaskismorepromising,althoughitdepends more assumptions than the construction task.
There are two different points among the expansion task and the construction task.
Previous researches of the construction task can be classied into two groups.
The rst group consists of researchestoconstructanewtranslationdictionaryfor a fresh language pair from existing translation dictionaries or other language resources (Tanaka and Umemura, 1994).
In the rst group, information of the seed dictionary are not counted in them unlike the expansion task, because it is assumed that there is no seed dictionary for such fresh language pairs.
The second group consists of researches to translate 197 xs v(xs) vt(xs) ys zs u(zs) Corpus in the source Source-Pivot Dictionary PivotDestination Dictionary Corpus inthe destination Seed Dictionary Select output words Figure 1: Translation Procedure novel words using both a large existing translation dictionary and other linguistic resources like huge parallel corpora (Tonoike et al., 2005).
Because almost of novel words are nouns, these researches focus into the task of translating nouns.
In the expansion task, however, it is necessary to translate verbs and adjectives as well as nouns, because a seed dictionary will be so small that only basic words will be contained in it if the target language pair is unfamiliar.
We will discuss about this topic in Section 3.2.
The remainder of this paper is organised as follows: Section 2 describes the method to expand a small seed dictionary.
The experiments presented in Section 3 shows that the proposed method can improve performance of a real CLIR system.
This paper ends with concluding remarks in Section 4.
2 Method
of Expanding Seed Dictionary The proposed method roughly consists of two steps shown in Figure 1.
The rst step is to generate a cooccurrence vector on the destination language corresponding to an input word, using both the seed dictionary and a monolingual corpus in the source language.
The second step is to list translation candidatesup,referringboththesource-pivotdictionary and the pivot-destination dictionary, and to calculate their co-occurrence vectors based on a monolingual corpus in the destination.
The seed dictionary is used to convert a cooccurrence vector in the source language into a vector in the destination language.
In this paper, f(wi,wj) represents a co-occurrence frequency of a word wi and a word wj for all languages.
A cooccurrence vector v(xs) of a word xs in the source is: v(xs) = (f(xs,x1),...,f(xs,xn)), (1) where xi(i = 1,2,...,n) is a headword of the seed dictionary D.
A co-occurrence vector v(xs), whose each element is corresponding to a word in the source, is converted into a vector vt(xs), whose each element is corresponding to a word in the destination, referring the dictionary D: vt(xs) = (ft(xs,z1),...,ft(xs,zm)), (2) where zj(j = 1,2,...,m) is a translation word which appears in the dictionary D.
The function ft(xs,zk), whichassignsaco-occurrencedegreebetween a word xs and a word zj in the destination based on a co-occurrence vector of a word xs in the source, is dened as follows: ft(xs,zj) = n?? i=1 f(xs,xi)  (xi,zj).
(3) where (xi,zj) is equal to one when a word zj is included in a translation word set D(xi), which consists of translation words of a word xi, and zero otherwise.
A set of description sentences Ys in the pivot are obtained referring the source-pivot dictionary for a word xs.
After that, a description sentence ys ??Ys in the pivot is converted to a set of description sentences Zs in the destination referring the pivot-destination dictionary.
A co-occurrence vector against a candidate description sentencezs = z1sz2s zls, which is an instance of Zs, is calculated by this equation: u(zs) = parenleftBigg l?? k=1 f(zks,z1),..., l?? k=1 f(zks,zm) ) (4) Finally, the candidate zs which meets a certain condition is selected as an output.
Two conditions are examined in this paper: (1) selecting top-n candidatesfromsortedonesaccordingtoeachsimilarity score, and (2) selecting candidates whose similarity scoresaregreaterthanacertainthreshold.
Inthispaper, cosine distance s(vt(xs),u(zs)) between a vector based on an input word xs and a vector based on 198 acandidatezs isusedasthesimilarityscorebetween them.
3 Experiments
In this section, we present the experiments of the proposed method that the Indonesian language, the English language and the Japanese language are adopted as the source language, the pivot language and the destination language respectively.
3.1 Experimental
Data The proposed method depends on three translation dictionaries and two monolingual corpora as described in Section 2.
Mainichi Newspaper Corpus (1993??995), which contains 3.5M sentences consist of 140M words, is used as the Japanese corpus.
When measuring similarity between words using co-occurrence vectors, it is common that a corpus in the source language for the similar domain to one of the corpus in the source languageismoresuitablethanoneforadifferentdomain.
Unfortunately, becausewecouldnotndsuch corpus, the articles which were downloaded from the Indonesian Newspaper WEB sites1 are used as the Indonesian corpus.
It contains 1.3M sentences, which are tokenized into 10M words.
An online Indonesian-Japanese dictionary2 contains 10,172 headwords, however, only 6,577 headwords of them appear in the Indonesian corpus.
We divide them into two sets: the rst set which consists of 6,077 entries is used as the seed dictionary, and the second set which consists of 500 entries is used to evaluate translation performance.
Moreover, an online Indonesian-English dictionary3, and an English-Japanese dictionary(Michibata, 2002) are also used as the source-pivot dictionary and the pivot-destination dictionary.
3.2 Evaluation
of Translation Performance As described in Section 2, two conditions of selecting output words among candidates are examined.
Table 1 shows their performances and the baseline, 1http://www.kompas.com/, http://www.tempointeraktif.com/ 2http://m1.ryu.titech.ac.jp/?indonesia/ todai/dokumen/kamusjpina.pdf 3http://nlp.aia.bppt.go.id/kebi that is the translation performance when all candidates are selected as output words.
It is revealed that the condition of selecting top-n candidates outperforms the another condition and the baseline.
The maximum F=1 value of 52.5% is achieved when selecting top-3 candidates as output words.
Table2showsthatthelexicaldistributionofheadwordscontainedintheseeddictionaryarequitesimilar to the lexical distribution of headwords contained in the source-pivot dictionary.
This observation means that it is necessary to translate verbs andadjectivesaswellasnouns,whenexpandingthis seed dictionary.
Table 3 shows translation performances against nouns, verbs and adjectives, when selecting top-3 candidates as output words.
The proposed method can be regarded likely because it is effective to verbs and adjectives as well as to nouns, whereas the baseline precision of verbs is considerably lower than the others.
3.3 CLIR
Performance Improved by Expanded Dictionary In this section, performance impact is presented when the dictionary expanded by the proposed method is adopted to the real CLIR system proposed in (Purwarianti et al., 2007).
NTCIR3 Web Retrieval Task(Eguchi et al., 2003) provides the evaluation dataset and denes the evaluation metric.
The evaluation metric consists of four MAP values: PC, PL, RC and RL.
They are corresponding to assessment types respectively.
The dataset consists 100GB Japanese WEB documents and 47 queries of Japanese topics.
The Indonesian queries, which are manually translated from them, are used as inputs of the experiment systems.
The number of unique words which occur in the queries is 301, and the number of unique words which are not contained in the Indonesian-Japanese dictionary is 106 (35%).
It is reduced to 78 (26%), while the existingdictionarythatcontains10,172entriesisexpanded to the dictionary containing 20,457 entries with the proposed method.
Table 4 shows the MAP values achieved by both the baseline systems using the existing dictionary and ones using the expanded dictionary.
The former three systems use existing dictionaries, and the latter three systems use the expanded one.
The 3rd system translates keywords transitively using both 199 Table 1: Comparison between Conditions of Selecting Output Words Selecting top-n candidates Selecting plausible candidates Baseline n = 1 n = 2 n = 3 n = 5 n = 10 x = 0.1 x = 0.16 x = 0.2 x = 0.3 Prec.
55.4% 49.9% 46.2% 40.0% 32.2% 20.8% 23.6% 25.8% 33.0% 18.9% Rec.
40.9% 52.6% 60.7% 67.4% 74.8% 65.3% 50.1% 40.0% 16.9% 82.5% F=1 47.1% 51.2% 52.5% 50.2% 45.0% 31.6% 32.1% 31.4% 22.4% 30.8% Table 2: Lexical Classication of Headwords IndonesianIndonesianJapanese English # of nouns 4085 (57.4%) 15718 (53.5%) # of verbs 1910 (26.8%) 9600 (32.7%) # of adjectives 795 (11.2%) 3390 (11.5%) # of other words 330 (4.6%) 682 (2.3%) Total 7120 (100%) 29390 (100%) Table 3: Performance for Nouns, Verbs and Adjectives Noun Verb Adjective n = 3 Baseline n = 3 Baseline n = 3 Baseline Prec.
49.1% 21.8% 41.0% 14.7% 46.9% 26.7% Rec.
65.6% 80.6% 52.3% 84.1% 59.4% 88.4% F=1 56.2% 34.3% 46.0% 25.0% 52.4% 41.0% Table 4: CLIR Performance PC PL RC RL (1) Existing Indonesian-Japanese dictionary 0.044 0.044 0.037 0.037 (2) Existing Indonesian-Japanese dictionary and Japanese proper name dictionary 0.054 0.052 0.047 0.045 (3) Indonesian-English-Japanese transitive translation with statistic ltering 0.078 0.072 0.055 0.053 (4) Expanded Indonesian-Japanese dictionary 0.061 0.059 0.046 0.046 (5) Expanded Indonesian-Japanese dictionary with Japanese proper name dictionary 0.066 0.063 0.049 0.049 (6) Expanded Indonesian-Japanese dictionary with Japanese proper name dictionary and statistic ltering 0.074 0.072 0.059 0.058 the source-pivot dictionary and the pivot-destination dictionary, and the others translate keywords using either the existing source-destination dictionary or the expanded one.
The 3rd system and the 6th system try to eliminate unnecessary translations based statistic measures calculated from retrieved documents.
These measures are effective as shown in (Purwarianti et al., 2007), but, consume a high runtime computational cost to reduce enormous translation candidates statistically.
It is revealed that CLIR systems using the expanded dictionary outperform ones using the existing dictionary without statistic ltering.
And more, it shows that ones using the expanded dictionary without statistic ltering achieve near performance to the 3rd system without paying a high run-time computational cost.
Once it is paid, the6thsystemachievesalmostsamescoreofthe3rd system.
These observation leads that we can concludethatourproposedmethodtoexpanddictionary is valuable to a real CLIR system.
4 Concluding
Remarks In this paper, a novel method of expanding a small existing translation dictionary to a large translation dictionary using a pivot language is proposed.
Our method uses information obtained from a small existing translation dictionary from the source languagetothedestinationlanguageeffectively.
Experiments that expands the Indonesian-Japanese dictionary using the English language as a pivot language shows that the proposed method can improve performance of a real CLIR system.
References Koji Eguchi, Keizo Oyama, Emi Ishida, Noriko Kando,, and KazukoKuriyama.
2003. Overview of the web retrieval task at the third NTCIR workshop.
In Proceedings of the Third NTCIR Workshop on research in Information Retrieval, Automatic Text Summarization and Question Answering.
Hideki Michibata, editor.
2002. Eijiro.
ALC, 3.
(in Japanese).
Ayu Purwarianti, Masatoshi Tsuchiya, and Seiichi Nakagawa.
2007. Indonesian-Japanese transitive translation using English for CLIR.
Journal of Natural Language Processing, 14(2), Apr.
Kumiko Tanaka and Kyoji Umemura.
1994. Construction of a bilingual dictionary intermediated by a third language.
In Proceedings of the 15th International Conference on Computational Linguistics.
Masatugu Tonoike, Mitsuhiro Kida, Toshihiro Takagi, Yasuhiro Sasaki, Takehito Utsuro, and Satoshi Sato.
2005. Translation estimation for technical terms using corpus collected from the web.
In Proceedings of the Pacic Association for Computational Linguistics, pages 325??31, August.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 201??04, Prague, June 2007.
c2007 Association for Computational Linguistics Shallow Dependency Labeling Manfred Klenner Institute of Computational Linguistics University of Zurich klenner@cl.unizh.ch Abstract We present a formalization of dependency labeling with Integer Linear Programming.
We focus on the integration of subcategorization into the decision making process, where the various subcategorization frames of a verb compete with each other.
A maximum entropy model provides the weights for ILP optimization.
1 Introduction
Machine learning classifiers are widely used, although they lack one crucial model property: they can?t adhere to prescriptive knowledge.
Take grammatical role (GR) labeling, which is a kind of (shallow) dependency labeling, as an example: chunkverb-pairs are classified according to a GR (cf.
(Buchholz, 1999)).
The trials are independent of each other, thus, local decisions are taken such that e.g. a unique GR of a verb might (erroneously) get multiply instantiated etc.
Moreover, if there are alternative subcategorization frames of a verb, they must not be confused by mixing up GR from different frames to a non-existent one.
Often, a subsequent filter is used to repair such inconsistent solutions.
But usually there are alternative solutions, so the demand for an optimal repair arises.
We apply the optimization method Integer Linear Programming (ILP) to (shallow) dependency labeling in order to generate a globally optimized consistent dependency labeling for a given sentence.
A maximum entropy classifier, trained on vectors with morphological, syntactic and positional information automatically derived from the TIGER treebank (German), supplies probability vectors that are used as weights in the optimization process.
Thus, the probabilities of the classifier do not any longer provide (as usually) the solution (i.e.
by picking out the most probable candidate), but count as probabilistic suggestions to a globally consistent solution.
More formally, the dependency labeling problem is: given a sentence with (i) verbs, a0a2a1, (ii) NP and PP chunks1, a3a5a4, label all pairs (a0a6a1a8a7 a3a9a4a11a10a13a12 a14 a0a6a1a15a7 a3a9a4 ) with a dependency relation (including a class for the null assignment) such that all chunks get attached and for each verb exactly one subcategorization frame is instantiated.
2 Integer
Linear Programming Integer Linear Programming is the name of a class of constraint satisfaction algorithms which are restricted to a numerical representation of the problem to be solved.
The objective is to optimize (e.g.
maximize) a linear equation called the objective function (a) in Fig.
1) given a set of constraints (b) in Fig.
1): a16 a10a18a17a20a19a22a21a11a23a25a24 a14a27a26a29a28a31a30a31a32a31a32a31a32a33a30a34a26a36a35 a10a2a23a38a37a40a39 a28a34a26a29a28a42a41a40a32a31a32a31a32a43a41 a39 a35a44a26a36a35 a45 a10 a16a25a46 a28a47a26a29a28a42a41 a16a44a46a49a48 a26 a48 a41a40a32a31a32a31a32a22a41 a16a25a46 a35a44a26a36a35 a50a51 a52a54a53a37a55 a56a58a57 a59 a45 a46 a30 Figure 1: ILP Specification where, a60a61a37a63a62 a30a31a32a31a32a31a32a22a30a34a64 and a26 a28 a32a31a32a31a32a58a26 a35 are variables, a39 a28a65a32a31a32a31a32 a39 a35, a45 a46 and a16a25a46 a28a66a32a31a32a31a32 a16a44a46 a35 are constants.
For dependency labeling we have: a26a67a35 are binary class variables that indicate the (non-) assignment of a chunk a68 to a dependency relation a69 of a subcat frame a24 of a verb a70. Thus, three indices are needed: a69a72a71a74a73a76a75 . If such an indicator variable a69a77a71a74a73a76a75 is set to 1 in the course of the maximization task, then the dependency label a69 between these chunks is said to hold, otherwise (a69a78a71a74a73a58a75a6a37a80a79 ) it doesn?t hold.
a39 a28a5a32a31a32a31a32 a39 a35 from Fig.1 are interpreted as weights that represent the impact of an assignment.
3 Dependency
Labeling with ILP Given the chunks a3a9a4a11a81 (NP, PP and verbs) of a sentence, each pair a3a9a4a11a81a82a12a83a3a9a4a11a81 is formed.
It can 1Note that we use base chunks instead of heads.
201 a0 a37 a1a2a4a3a5a1 a6 a46 a1a2a4a3a5a1 a6 a7a9a8 a46a11a10 a12 a7a14a13a16a15a18a17a20a19a21a23a22a25a24 a46 a7 (1) a26 a37 a1a27a29a28a30a1 a6 a46 a1a31a4a31a30a1 a6 a7 a15a33a32a34a19a21a25a22a23a35 a46 a7 (2) a0 a37 a1a2a4a3a9a36a37a1 a6 a75 a1a27a38a28a39a1 a6 a73 a6 a40a42a41a33a43 a71a45a44a47a46a49a48a51a50a53a52 a41a4a54 a50a56a55 a22 a69 a71a74a73a58a75 (3) a57 a37 a1a27a29a28a39a1 a6 a46 a1a27a38a28a39a1 a6 a7a16a8 a46a11a10 a12 a7a14a13 a15a30a58 a19a21 a22a60a59 a46 a7 (4) a17a20a19a22a21a11a23 a0 a41 a26 a41 a0 a41 a57 (5) Figure 2: Objective Function stand in one of eight dependency relations, including a pseudo relation representing the null class.
We consider the most important dependency labels: subject (a61 ), direct object (a62 ), indirect object (a63 ), clausal complement (a3 ), prepositional complement (a64 ), attributive (NP or PP) attachment ( a24 ) and adjunct ( a35 ).
Although coarse-grained, this set allows us to capture all functional dependencies and to construct a dependency tree for every sentence in the corpus2.
Technically, indicator variables are used to represent attachment decisions.
Together with a weight, they form the addend of the objective function.
In the case of attributive modifiers or adjuncts (the non-governable labels), the indicator variables correspond to triples.
There are two labels of this type: a24 a46 a7 represents that chunk a65 modifies chunk a60 and a35 a46 a7 represents that chunk a65 is in an adjunct relation to chunk a60 . a0 and a26 are defined as the weighted sum of such pairs (cf.
Eq. 1 and Eq 2.
from Fig.
2), the weights (e.g.
a15a30a17a20a19a21 ) stem from the statistical model.
For subcategorized labels, we have quadruples, consisting of a label name a69, a frame index a24, a verb a70 and a chunk a68 (also verb chunks are allowed as a a68 ): a69 a71a74a73a58a75 . We define a0 to be the weighted sum of all label instantiations of all verbs (and their subcat frames), see Eq.
3 in Fig.
2. The subscript a66 a73 is a list of pairs, where each 2Note that we are not interested in dependencies beyond the (base) chunk level pair consists of a label and a subcat frame index.
This way, a66 a73 represents all subcat frames of a verb a70 . For example, a66 of ?to believe??could be: a67a69a68 a61 a30 a62a71a70 a30a71a68 a62 a30 a62a71a70 a30a71a68 a61 a30a73a72 a70 a30a71a68 a3 a30a74a72 a70 a30a75a68 a61 a30a74a76 a70 a30a75a68 a63 a30a73a76 a70a74a77 . There are three frames, the first one requires a a61 and a a62 . Consider the sentence ?He believes these stories??
We have a78a80a79 =a67 believesa77 and a81a83a82 a81 = a67 He, believes, storiesa77 . Assume a66 a28 to be the a66 of ?to believe??as defined above.
Then, e.g. a84 a48 a28a86a85 a37 a62 represents the assignment of ?stories??as the filler of the subject relation a84 of the second subcat frame of ?believes??
To get a dependency tree, every chunk must find a head (chunk), except the root verb.
We define a root verb a65 as a verb that stands in the relation a57 a46a7 to all other verbs a60 . a57 (cf.
Eq.4 from Fig.2) is the weighted sum of all null assignment decisions.
It is part of the maximization task and thus has an impact (a weight).
The objective function is defined as the sum of equations 1 to 4 (Eq.5 from Fig.2).
So far, our formalization was devoted to the maximization task, i.e. which chunks are in a dependency relation, what is the label and what is the impact.
Without any further (co-occurrence) restrictions, every pair of chunks would get related with every label.
In order to assure a valid linguistic model, constraints have to be formulated.
4 Basic
Global Constraints Every chunk a65 from a81a83a82 (a87a37a88a81a89a82 a81 ) must find a head, that is, be bound either as an attribute, adjunct or a verb complement.
This requires all indicator variables with a65 as the dependent (second index) to sum up to exactly 1.
a1a2a4a3a90a1 a6 a75 a24 a75 a7 a41 a1a27a29a28a39a1 a6 a46 a35 a46 a7 a41 a1a27a38a28a39a1 a6 a73 a6 a40a42a41a33a43 a71a45a44a47a46a49a48 a50 a69 a71a74a73 a7 a37 a62 a30 (6) a91 a65a36a23a13a79a93a92a94a65 a53 a95 a81a83a82 a95 A verb is attached to any other verb either as a clausal object a3 (of some verb frame a24 ) or as a57 (null class) indicating that there is no dependency relation between them.
a57 a46 a7 a41 a6 a40a97a96a98a43 a71a45a44a47a46a49a48 a19 a3 a71 a46 a7 a37 a62 a30 a91 a60 a30 a65 a14 a60a99a87a37a88a65a25a10 a23 a79a93a92a8a60 a30 a65 a53 a95 a78a83a79 a95(7) 202 This does not exclude that a verb gets attached to several verbs as a a3 . We capture this by constraint 8: a1a27a38a28a30a1 a6 a46 a6 a40a97a96a98a43 a71a45a44a47a46a49a48 a19 a3 a71 a46 a7 a53 a62 a30 a91 a65a20a23a13a79 a92a94a65 a53 a95 a78 a79 a95 (8) Another (complementary) constraint is that a dependency label a69 of a verb must have at most one filler.
We first introduce a indicator variable a69 a71a74a73 : a69a72a71a31a73a13a37 a1a2a4a3a25a36a37a1 a6 a75 a69a72a71a74a73a58a75 (9) In order to serve as an indicator of whether a label a69 (of a frame a24 of a verb a70 ) is active or inactive, we restrict a69 a71a74a73 to be at most 1: a69a72a71a74a73 a53 a62 a30 a91 a70 a30 a24 a30 a69 a23a44a79 a92 a70 a53 a95 a78a80a79 a95 a1 a68 a69 a30 a24a38a70a3a2 a66 a73 (10) To illustrate this by the example previously given: the subject of the second verb frame of ?to believe?? is defined as a84 a48 a28 a37 a61 a48 a28a34a28 a41 a61 a48 a28a86a85 (with a84 a48 a28 a53 a62 ).
Either a61 a48 a28a34a28 a37 a62 or a61 a48 a28a86a85 a37 a62 or both are zero, but if one of them is set to one, then a84 a48 a28 = 1.
Moreover, as we show in the next section, the selection of the label indicator variable of a frame enforces the frame to be selected as well3.
5 Subcategorization
as a Global Constraint The problem with the selection among multiple subcat frames is to guarantee a valid distribution of chunks to verb frames.
We don?t want to have chunk a68 a28 be labeled according to verb frame a24 a28 and chunk a68 a48 according to verb frame a24 a48 . Any valid attachment must be coherent (address one verb frame) and complete (select all of its labels).
We introduce an indicator variable a4 a71a74a73 with frame and verb indices.
Since exactly one frame of a verb has to be active at the end, we restrict: a5a7a6 a50 a6 a71 a12 a28 a8 a71a74a73a72a37 a62 a30 a91 a70a67a23 a79a93a92 a70 a53 a95 a78 a79 a95 (11) (a9a10a4 a73 is the number of subcat frames of verb a70 ) However, we would like to couple a verb?s (a70 ) frame (a24 ) to the frame?s label set and restrict it to be active (i.e.
set to one) only if all of its labels are active.
To achieve this, we require equivalence, 3There are more constraints, e.g. that no two chunks can be attached to each other symmetrically (being chunk and modifier of each other at the same time).
We won?t introduce them here.
namely that selecting any label of a frame is equivalent to selecting the frame.
As defined in equation 10, a label is active, if the label indicator variable (a69a72a71a74a73 ) is set to one.
Equivalence is represented by identity, we thus get (cf.
constraint 12): a8 a71a74a73a72a37 a69a72a71a74a73 a30 a91 a70 a30 a24 a30 a69 a23 a79 a92 a70 a53 a95 a78a83a79 a95 a1 a68 a69 a30 a24a38a70a11a2 a66 a73 (12) If any a69 a71a74a73 is set to one (zero), then a4 a71a31a73 is set to one (zero) and all other a69a78a71a31a73 of the same subcat frame are forced to be one (completeness).
Constraint 11 ensures that exactly one subcat frame a4 a71a74a73 can be active (coherence).
6 Maximum
Entropy and ILP Weights A maximum entropy approach was used to induce a probability model that serves as the basis for the ILP weights.
The model was trained on the TIGER treebank (Brants et al., 2002) with feature vectors stemming from the following set of features: the part of speech tags of the two candidate chunks, the distance between them in chunks, the number of intervening verbs, the number of intervening punctuation marks, person, case and number features, the chunks, the direction of the dependency relation (left or right) and a passive/active voice flag.
The output of the maxent model is for each pair of chunks a probability vector, where each entry represents the probability that the two chunks are related by a particular label (a61 a30 a62 a32a31a32a31a32 including a57 ).
7 Empirical
Results A 80% training set (32,000 sentences) resulted in about 700,000 vectors, each vector representing either a proper dependency labeling of two chunks, or a null class pairing.
The accuracy of the maximum entropy classifier was 87.46%.
Since candidate pairs are generated with only a few restrictions, most pairings are null class labelings.
They form the majority class and thus get a strong bias.
If we evaluate the dependency labels, therefore, the results drop appreciably.
The maxent precision then is 62.73% (recall is 85.76%, f-measure is 72.46 %).
Our first experiment was devoted to find out how good our ILP approach was given that the correct subcat frame was pre-selected by an oracle.
Only the decision which pairs are labeled with which dependency label was left to ILP (also the selection and assignment of the non subcategorized labels).
203 There are 8000 sentence with 36,509 labels in the test set; ILP retrieved 37,173; 31,680 were correct.
Overall precision is 85.23%, recall is 86.77%, the f-measure is 85.99% (Fa0a2a1a4a3a6a5 in Fig.
3). Fa0a2a1a4a3a6a5 Fa75a8a7a6a9 a0 Prec Rec F-Mea Prec Rec F-Mea a61 91.4 86.1 88.7 90.3 80.9 85.4 a62 90.4 83.3 86.7 81.4 73.3 77.2 a63 88.5 76.9 82.3 75.8 55.5 64.1 a64 79.3 73.7 76.4 77.8 40.9 55.6 a3 98.6 94.1 96.3 91.4 86.7 89.1 a35 76.7 75.6 76.1 74.5 72.3 73.4 a24 75.7 76.9 76.3 74.1 74.2 74.2 Figure 3: Pre-selected versus Competing Frames The results of the governable labels (a61 down to a3 ) are good, except PP complements (a64 ) with a fmeasure of 76.4%.
The errors made with a4a10a0a2a1a4a3a6a5 : the wrong chunks are deemed to stand in a dependency relation or the wrong label (e.g.
a61 instead of a62 ) was chosen for an otherwise valid pair.
This is not a problem of ILP, but one of the statistical model the weights do not discriminate well.
Improvements of the statistical model will push ILP?s precision.
Clearly, performance drops if we remove the subcat frame oracle letting all subcat frames of a verb compete with each other (Fa75a11a7a12a9 a0, Fig.3).
How close can Fa75a8a7a6a9 a0 come to the oracle setting Fa0a2a1a4a3a13a5 . The overall precision of the Fa75a8a7a6a9 a0 setting is 81.8%, recall is 85.8% and the f-measure is 83.7% (f-measure of Fa0a2a1a4a3a6a5 was 85.9%).
This is not too far away.
We have also evaluated how good our model is at finding the correct subcat frame (as a whole).
First some statistics: In the test set are 23 different subcat frames (types) with 16,137 occurrences (token).
15,239 out of these are cases where the underlying verb has more than one subcat frame (only here do we have a selection problem).
The precision was 71.5%, i.e. the correct subcat frame was selected in 10,896 out of 15,239 cases.
8 Related
Work ILP has been applied to various NLP problems including semantic role labeling (Punyakanok et al., 2004), which is similar to dependency labeling: both can benefit from verb specific information.
Actually, (Punyakanok et al., 2004) take into account to some extent verb specific information.
They disallow argument types a verb does not ?subcategorize for??by setting an occurrence constraint.
However, they do not impose co-occurrence restrictions as we do (allowing for competing subcat frames).
None of the approaches to grammatical role labeling tries to scale up to dependency labeling.
Moreover, they suffer from the problem of inconsistent classifier output (e.g.
(Buchholz, 1999)).
A comparison of the empirical results is difficult, since e.g. the number and type of grammatical/dependency relations differ (the same is true wrt.
German dependency parsers, e.g (Foth et al., 2005)).
However, our model seeks to integrate the (probabilistic) output of such systems and in the best case boosts the results, or at least turn it into a consistent solution.
9 Conclusion
and Future Work We have introduced a model for shallow dependency labeling where data-driven and theory-driven aspects are combined in a principled way.
A classifier provides empirically justified weights, linguistic theory contributes well-motivated global restrictions, both are combined under the regiment of optimization.
The empirical results of our approach are promising.
However, we have made idealized assumptions (small inventory of dependency relations and treebank derived chunks) that clearly must be replaced by a realistic setting in our future work.
Acknowledgment. I would like to thank Markus Dreyer for fruitful (?long distance?? discussions and the (steadily improved) maximum entropy models.
References Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius and George Smith.
2002. The TIGER Tree-bank.
Proc. of the Wshp.
on Treebanks and Linguistic Theories Sozopol.
Sabine Buchholz, Jorn Veenstra and Walter Daelemans.
1999. Cascaded Grammatical Relation Assignment.
EMNLP-VLC??9, the Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora.
Kilian Foth, Wolfgang Menzel, and Ingo Schroder.
Robust parsing with weighted constraints.
Natural Language Engineering, 11(1):1-25 2005.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dave Zimak.
2004. Semantic Role Labeling via Integer Linear Programming Inference.
COLING ??4 .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 205??08, Prague, June 2007.
c2007 Association for Computational Linguistics Minimally Lexicalized Dependency Parsing Daisuke Kawahara and Kiyotaka Uchimoto National Institute of Information and Communications Technology, 3-5 Hikaridai Seika-cho Soraku-gun, Kyoto, 619-0289, Japan {dk, uchimoto}@nict.go.jp Abstract Dependencystructuresdonothavetheinformation of phrase categories in phrase structure grammar.
Thus, dependency parsing relies heavily on the lexical information of words.
This paper discusses our investigation into the effectiveness of lexicalization in dependency parsing.
Specically, by restricting the degree of lexicalization in the training phase of a parser, we examine the change in the accuracy of dependency relations.
Experimental results indicate that minimal or low lexicalization is sufcient for parsing accuracy.
1 Introduction
In recent years, many accurate phrase-structure parsers have been developed (e.g., (Collins, 1999; Charniak, 2000)).
Since one of the characteristics of these parsers is the use of lexical information in the tagged corpus, they are called ?lexicalized parsers??
Unlexicalized parsers, on the other hand, achieved accuracies almost equivalent to those of lexicalized parsers (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006).
Accordingly, we can say that the state-of-the-art lexicalized parsers are mainly based on unlexical (grammatical) information due to the sparse data problem.
Bikel also indicated that Collins??parser can use bilexical dependencies only 1.49% of the time; the rest of the time, itbacksofftoconditiononewordonjustphrasaland part-of-speech categories (Bikel, 2004).
This paper describes our investigation into the effectiveness of lexicalization in dependency parsing instead of phrase-structure parsing.
Usual dependency parsing cannot utilize phrase categories, and thus relies on word information like parts of speech and lexicalized words.
Therefore, we want to know the performance of dependency parsers that have minimal or low lexicalization.
Dependency trees have been used in a variety of NLP applications, such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005).
For such applications, a fast, efcient and accurate dependency parser is required to obtain dependency trees from a large corpus.
From this point of view, minimally lexicalized parsers have advantages over fully lexicalized ones in parsing speed and memory consumption.
We examined the change in performance of dependency parsing by varying the degree of lexicalization.
The degree of lexicalization is specied by giving a list of words to be lexicalized, which appear in a training corpus.
For minimal lexicalization, we used a short list that consists of only high-frequency words, and for maximal lexicalization, the whole list was used.
Consequently, minimally or low lexicalization is sufcient for dependency accuracy.
2 Related
Work Klein and Manning presented an unlexicalized PCFG parser that eliminated all the lexicalized parameters (Klein and Manning, 2003).
They manually split category tags from a linguistic view.
This corresponds to determining the degree of lexicalization by hand.
Their parser achieved an F1 of 85.7% forsection23ofthePennTreebank.
Matsuzakietal. and Petrov et al.proposed an automatic approach to 205 Dependency accuracy (DA) Proportions of words, except punctuation marks, that are assigned the correct heads.
Root accuracy (RA) Proportions of root words that are correctly detected.
Complete rate (CR) Proportions of sentences whose dependency structures are completely correct.
Table 1: Evaluation criteria.
splitting tags (Matsuzaki et al., 2005; Petrov et al., 2006).
In particular, Petrov et al.reported an F1 of 90.2%, which is equivalent to that of state-of-the-art lexicalized parsers.
Dependency parsing has been actively studied in recent years (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Isozaki et al., 2004; McDonald et al., 2005; McDonald and Pereira, 2006; Corston-Oliver et al., 2006).
For instance, Nivre and Scholz presented a deterministic dependency parser trained by memory-based learning (Nivre and Scholz, 2004).
McDonald et al.proposed an online large-margin method for training dependency parsers (McDonald et al., 2005).
All of them performed experiments using section 23 of the Penn Treebank.
Table 2 summarizes their dependency accuracies based on three evaluation criteria shown in Table 1.
These parsers believed in the generalization ability of machine learners and did not pay attention to the issue of lexicalization.
3 Minimally
Lexicalized Dependency Parsing We present a simple method for changing the degree of lexicalization in dependency parsing.
This method restricts the use of lexicalized words, so it is the opposite to tag splitting in phrase-structure parsing.
In the remainder of this section, we rst describe a base dependency parser and then report experimental results.
3.1 Base
Dependency Parser We built a parser based on the deterministic algorithm of Nivre and Scholz (Nivre and Scholz, 2004) as a base dependency parser.
We adopted this algorithm because of its linear-time complexity.
In the algorithm, parsing states are represented by triples ?S,I,A?? where S is the stack that keeps the words being under consideration, I is the list of reDA RA CR (Yamada and Matsumoto, 2003) 90.3 91.6 38.4 (Nivre and Scholz, 2004) 87.3 84.3 30.4 (Isozaki et al., 2004) 91.2 95.7 40.7 (McDonald et al., 2005) 90.9 94.2 37.5 (McDonald and Pereira, 2006) 91.5 N/A 42.1 (Corston-Oliver et al., 2006) 90.8 93.7 37.6 Our Base Parser 90.9 92.6 39.2 Table 2: Comparison of parser performance.
maining input words, and A is the list of determined dependencies.
Given an input word sequence, W, the parser is rst initialized to the triple ?nil,W,???.
The parser estimates a dependency relation between two words (the top elements of stacks S and I).
The algorithm iterates until the list I is empty.
There are four possible operations for a parsing state (where t is the word on top of S, n is the next input word in I, and w is any word): Left In a state ?t|S,n|I,A?? if there is no dependency relation (t ??w) in A, add the new dependency relation (t ??n) into A and pop S (remove t), giving the state ?S,n|I,A ??(t ?? n)??
Right In a state ?t|S,n|I,A?? if there is no dependency relation (n ??w) in A, add the new dependency relation (n ??t) into A and push n onto S, giving the state ?n|t|S,I,A??n ??t)??
Reduce In a state ?t|S,I,A?? if there is a dependency relation (t ??w) in A, pop S, giving the state ?S,I,A??
Shift In a state ?S,n|I,A?? push n onto S, giving the state ?n|S,I,A??
In this work, we used Support Vector Machines (SVMs) to predict the operation given a parsing state.
SinceSVMsarebinaryclassiers, weusedthe pair-wise method to extend them in order to classify our four-class task.
The features of a node are the word?s lemma, the POS/chunk tag and the information of its child node(s).
The lemma is obtained from the word form using a lemmatizer, except for numbers, which are replaced by ?num??
The context features are the two preceding nodes of node t (and t itself), the two succeeding nodes of node n (and n itself), and their 1We use ?nil??to denote an empty list and a|A to denote a list with head a and tail A.
206 87 87.2 87.4 87.6 87.8 88 88.2 88.4 0 1000 2000 3000 4000 5000 Accuracy (%) Number of Lexicalized Words Figure 1: Dependency accuracies on the WSJ while changing the degree of lexicalization.
child nodes (lemmas and POS tags).
The distance between nodes n and t is also used as a feature.
We trained our models on sections 2-21 of the WSJ portion of the Penn Treebank.
We used section 23 as the test set.
Since the original treebank is basedonphrasestructure, weconvertedthetreebank to dependencies using the head rules provided by Yamada 2.
During the training phase, we used intact POS and chunk tags3.
During the testing phase, we used automatically assigned POS and chunk tags by Tsuruoka?s tagger4(Tsuruoka and Tsujii, 2005) and YamCha chunker5(Kudo and Matsumoto, 2001).
We used an SVMs package, TinySVM6,and trained the SVMs classiers using a third-order polynomial kernel.
The other parameters are set to default.
The last row in Table 2 shows the accuracies of our base dependency parser.
3.2 Degree
of Lexicalization vs.
Performance The degree of lexicalization is specied by giving a list of words to be lexicalized, which appear in a training corpus.
For minimal lexicalization, we used a short list that consists of only high-frequency words, and for maximal lexicalization, the whole list was used.
To conduct the experiments efciently, we trained 2http://www.jaist.ac.jp/?h-yamada/ 3In a preliminary experiment, we tried to use automatically assigned POS and chunk tags, but we did not detect signicant difference in performance.
4http://www-tsujii.is.s.u-tokyo.ac.jp/?tsuruoka/postagger/ 5http://chasen.org/?taku-ku/software/yamcha/ 6http://chasen.org/?taku-ku/software/TinySVM/ 83.6 83.8 84 84.2 84.4 84.6 84.8 85 0 1000 2000 3000 4000 5000 Accuracy (%) Number of Lexicalized Words Figure2: DependencyaccuraciesontheBrownCorpus while changing the degree of lexicalization.
our models using the rst 10,000 sentences in sections 2-21 of the WSJ portion of the Penn Treebank.
We used section 24, which is usually used as the development set, to measure the change in performance based on the degree of lexicalization.
We counted word (lemma) frequencies in the training corpus and made a word list in descending order of their frequencies.
The resultant list consists of 13,729 words, and the most frequent word is ?the?? which occurs 13,252 times, as shown in Table 3.
We dene the degree of lexicalization as a threshold of the word list.
If, for example, this threshold is set to 1,000, the top 1,000 most frequently occurring words are lexicalized.
We evaluated dependency accuracies while changing the threshold of lexicalization.
Figure 1 shows the result.
The dotted line (88.23%) represents the dependency accuracy of the maximal lexicalization, that is, using the whole word list.
We can see that the decrease in accuracy is less than 1% at the minimal lexicalization (degree=100) and the accuracy of more than 3,000 degree slightly exceeds that of the maximal lexicalization.
The best accuracy(88.34%)wasachievedat4,500degreeand signicantly outperformed the accuracy (88.23%) of the maximal lexicalization (McNemar?s test; p = 0.017 < 0.05).
These results indicate that maximal lexicalization is not so effective for obtaining accurate dependency relations.
We also applied the same trained models to the Brown Corpus as an experiment of parser adaptation.
We rst split the Brown Corpus portion of 207 rank word freq.
rank word freq.
1 the 13,252 1,000 watch 29 2, 12,858...
... ...
... ...
... 2,000 healthvest 12 100 week 261 ...
... ...
... ...
... 3,000 whoop 7 500 estate 64 ...
... ...
... ...
... Table 3: Word list.
the Penn Treebank into training and testing parts in the same way as (Roark and Bacchiani, 2003).
We further extracted 2,425 sentences at regular intervals from the training part and used them to measure the change in performance while varying the degree of lexicalization.
Figure 2 shows the result.
The dotted line (84.75%) represents the accuracy of maximal lexicalization.
The resultant curve is similar to that of the WSJ experiment7.
We can say that our claim is true even if the testing corpus is outside the domain.
3.3 Discussion
We have presented a minimally or lowly lexicalized dependency parser.
Its dependency accuracy is close or almost equivalent to that of fully lexicalized parsers, despite the lexicalization restriction.
Furthermore, the restriction reduces the time and space complexity.
The minimally lexicalized parser (degree=100) took 12m46s to parse the WSJ development set and required 111 MB memory.
These are 36% of time and 45% of memory reduction, compared to the fully lexicalized one.
The experimental results imply that training corpora are too small to demonstrate the full potential of lexicalization.
We should consider unsupervised or semi-supervised ways to make lexicalized parsers more effective and accurate.
Acknowledgment This research is partially supported by special coordination funds for promoting science and technology.
7In the experiment on the Brown Corpus, the difference between the best accuracy and the baseline was not signicant.
References Daniel M.
Bikel. 2004.
Intricacies of Collins??parsing model.
Computational Linguistics, 30(4):479??11.
Eugene Charniak.
2000. A maximum-entropy-inspired parser.
In Proceedings of NAACL2000, pages 132??39.
Michael Collins.
1999. Head-Driven Statistical Models for Natural Language Parsing.
Ph.D. thesis, University of Pennsylvania.
Simon Corston-Oliver, Anthony Aue, Kevin Duh, and Eric Ringger.
2006. Multilingual dependency parsing using bayes point machines.
In Proceedings of HLT-NAACL2006, pages 160??67.
Aron Culotta and Jeffrey Sorensen.
2004. Dependency tree kernels for relation extraction.
In Proceedings of ACL2004, pages 423??29.
Yuan Ding and Martha Palmer.
2005. Machine translation using probabilistic synchronous dependency insertion grammars.
In Proceedings of ACL2005, pages 541??48.
Hideki Isozaki, Hideto Kazawa, and Tsutomu Hirao.
2004. A deterministic word dependency analyzer enhanced with preference learning.
In Proceedings of COLING2004, pages 275??81.
Dan Klein and Christopher D.
Manning. 2003.
Accurate un-lexicalized parsing.
In Proceedings of ACL2003, pages 423?? 430.
Taku Kudo and Yuji Matsumoto.
2001. Chunking with support vector machines.
In Proceedings of NAACL2001, pages 192??99.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations.
In Proceedings of ACL2005, pages 75??2.
Ryan McDonald and Fernando Pereira.
2006. Online learning of approximate dependency parsing algorithms.
In Proceedings of EACL2006, pages 81??8.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency parsers.
In Proceedings of ACL2005, pages 91??8.
Joakim Nivre and Mario Scholz.
2004. Deterministic dependency parsing of English text.
In Proceedings of COLING2004, pages 64??0.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.
2006. Learning accurate, compact, and interpretable tree annotation.
In Proceedings of COLING-ACL2006, pages 433??440.
Brian Roark and Michiel Bacchiani.
2003. Supervised and unsupervised PCFG adaptation to novel domains.
In Proceedings of HLT-NAACL2003, pages 205??12.
Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2005. Bidirectional inference with the easiest-rst strategy for tagging sequence data.
In Proceedings of HLT-EMNLP2005, pages 467??74.
Hiroyasu Yamada and Yuji Matsumoto.
2003. Statistical dependency analysis with support vector machines.
In Proceedings of IWPT2003, pages 195??06 .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 209??12, Prague, June 2007.
c2007 Association for Computational Linguistics HunPos ??an open source trigram tagger Peter Halacsy Budapest U.
of Technology MOKK Media Research H-1111 Budapest, Stoczek u 2 peter@halacsy.com Andras Kornai MetaCarta Inc.
350 Massachusetts Ave.
Cambridge MA 02139 andras@kornai.com Csaba Oravecz Hungarian Academy of Sciences Institute of Linguistics H-1068 Budapest, Benczur u.
33. oravecz@nytud.hu Abstract In the world of non-proprietary NLP software the standard, and perhaps the best, HMM-based POS tagger is TnT (Brants, 2000).
We argue here that some of the criticism aimed at HMM performance on languages with rich morphology should more properly be directed at TnT?s peculiar license, free but not open source, since it is those details of the implementation which are hidden from the user that hold the key for improved POS tagging across a wider variety of languages.
We present HunPos1, a free and open source (LGPL-licensed) alternative, which can be tuned by the user to fully utilize the potential of HMM architectures, offering performance comparable to more complex models, but preserving the ease and speed of the training and tagging process.
0 Introduction
Even without a formal survey it is clear that TnT (Brants, 2000) is used widely in research labs throughout the world: Google Scholar shows over 400 citations.
For research purposes TnT is freely available, but only in executable form (closed source).
Its greatest advantage is its speed, important both for a fast tuning cycle and when dealing with large corpora, especially when the POS tagger is but one component in a larger information retrieval, information extraction, or question answer1http://mokk.bme.hu/resources/hunpos/ ing system.
Though taggers based on dependency networks (Toutanova et al., 2003), SVM (Gimenez and M`arquez, 2003), MaxEnt (Ratnaparkhi, 1996), CRF (Smith et al., 2005), and other methods may reach slightly better results, their train/test cycle is orders of magnitude longer.
A ubiquitous problem in HMM tagging originates from the standard way of calculating lexical probabilities by means of a lexicon generated during training.
In highly inflecting languages considerably more unseen words will be present in the test data than in more isolating languages, which largely accounts for the drop in the performance of n-gram taggers when moving away from English.
To mitigate the effect one needs a morphological dictionary (Haji?c et al., 2001) or a morphological analyzer (Hakkani-Tur et al., 2000), but if the implementation source is closed there is no handy way to incorporate morphological knowledge in the tagger.
The paper is structured as follows.
In Section 1 we present our own system, HunPos, while in Section 2 we describe some of the implementation details of TnT that we believe influence the performance of a HMM based tagging system.
We evaluate the system and compare it to TnT on a variety of tasks in Section 3.
We don?t necessarily consider HunPos to be significantly better than TnT, but we argue that we could reach better results, and so could others coming after us, because the system is open to explore all kinds of fine-tuning strategies.
Some concluding remarks close the paper in Section 4.
209 1 Main features of HunPos HunPos has been implemented in OCaml, a highlevel language which supports a succinct, wellmaintainable coding style.
OCaml has a highperformance native-code compiler (Doligez et al., 2004) that can produce a C library with the speed of a C/C++ implementation.
On the whole HunPos is a straightforward trigram system estimating the probabilities argmax t1...tT P(tT+1|tT) Tproductdisplay i=1 P(ti|ti??,ti??)P(wi|ti??,ti) for a given sequence of words w1...wT (the additional tags t??,t0, and tT+1 are for sentence boundary markers).
Notice that unlike traditional HMM models, we estimate emission/lexicon probabilities based on the current tag and the previous tag as well.
As we shall see in the next Section, using tag bigrams to condition the emissions can lead to as much as 10% reduction in the error rate.
(In fact, HunPos can handle a context window of any size, but on the limited training sets available to us increasing this parameter beyond 2 gives no further improvement.) As for contextualized lexical probabilities, our extension is very similar to Banko and Moore (2004) who use P(wi|ti??,ti,ti+1) lexical probabilities and found, on the Penn Treebank, that ?incorporating more context into an HMM when estimating lexical probabilities improved accuracy from 95.87% to 96.59%??
One difficulty with their approach, noted by Banko and Moore (2004), is the treatment of unseen words: their method requires a full dictionary that lists what tags are possible for each word.
To be sure, for isolating languages such information is generally available from machine readable dictionaries which are often large enough to make the out of vocabulary problem negligible.
But in our situation this amounts to idealized morphological analyzers (MA) that have their stem list extended so as to have no OOV on the test set.
The strong side of TnT is its suffix guessing algorithm that is triggered by unseen words.
From the training set TnT builds a trie from the endings of words appearing less than n times in the corpus, and memorizes the tag distribution for each suffix.2 A 2The parameter n cannot be externally set ??it is documented as 10 but we believe it to be higher.
clear advantage of this approach is the probabilistic weighting of each label, however, under default settings the algorithm proposes a lot more possible tags than a morphological analyzer would.
To facilitate the use of MA, HunPos has hooks to work with a morphological analyzer (lexicon), which might still leave some OOV items.
As we shall see in Section 3, the key issue is that for unseen words the HMM search space may be narrowed down to the alternatives proposed by this module, which not only speeds up search but also very significantly improves precision.
That is, for unseen words the MA will generate the possible labels, to which the weights are assigned by the suffix guessing algorithm.
2 Inside
TnT Here we describe, following the lead of (Jurish, 2003), some non-trivial features of TnT sometimes only hinted at in the user guide, but clearly evident from its behavior on real and experimentally adjusted corpora.
For the most part, these features are clever hacks, and it is unfortunate that neither Brants (2000) nor the standard HMM textbooks mention them, especially as they often yield more significant error reduction than the move from HMM to other architectures.
Naturally, these features are also available in HunPos.
2.1 Cardinals
For the following regular expressions TnT learns the tag distribution of the training corpus separately to give more reliable estimates for open class items like numbers unseen during training: ?[0-9]+$ ?[0-9]+\.$ ?[0-9.,:-]+[0-9]+$ ?[0-9]+[a-zA-Z]{1,3}$ (The regexps are only inferred ??we haven?t attempted to trace the execution).
After this, at test time, if the word is not found in the lexicon (numerals are added to the lexicon like all other items) TnT checks whether the unseen word matches some of the regexps, and uses the distribution learned for this regexp to guess the tag.
210 2.2 Upperand lowercase The case of individual words may carry relevant information for tagging, so it is well worth preserving the uppercase feature for items seen as such in training.
For unseen words TnT builds two suffix tries: if the word begins with uppercase one trie is used, for lowercase words the other trie is applied.
The undocumented trick is to try to lookup the word in sentence initial position from the training lexicon in its lowercase variant, which contributes noticeably to the better performance of the system.
3 Evaluation
English For the English evaluation we used the WSJ data from Penn Treebank II.
We extracted sentences from the parse trees.
We split data into training and test set in the standard way (Table 1).
Set Sect?ns Sent.
Tokens Unseen Train 0-18 38,219 912,344 0 Test 22-24 5,462 129,654 2.81% Table 1: Data set splits used for English As Table 2 shows HunPos achieves performance comparable to TnT for English.
The increase in the emission order clearly improves this performance.
seen unseen overall TnT 96.77% 85.91% 96.46% HunPos 1 96.76% 86.90% 96.49% HunPos 2 96.88% 86.13% 96.58% Table 2: WSJ tagging accuracy, HunPos with first and second order emission/lexicon probabilities If we follow Banko and Moore (2004) and construct a full (no OOV) morphological lexicon from the tagged version of the test corpus, we obtain 96.95% precision where theirs was 96.59%.
For words seen, precision improves by an entirely negligible 0.01%, but for unseen words it improves by 10%, from 86.13% to 98.82%.
This surprising result arises from the fact that there are a plenty of unambiguous tokens (especially the proper names that are usually unseen) in the test corpus.
What this shows is not just that morphology matters (this is actually not that visible for English), but that the difference between systems can only be appreciated once the small (and scantily documented) tricks are factored out.
The reason why Banko and Moore (2004) get less than HunPos is not because their system is inherently worse, but rather because it lacks the engineering hacks built into TnT and HunPos.
Hungarian We evaluated the different models by tenfold cross-validation on the Szeged Corpus (Csendes et al., 2004), with the relevant data in presented Table 3.
Set Sent.
Tokens Unseens OOV Train 63,075 1,044,914 0 N.A Test 7,008 116,101 9.59% 5.64% Table 3: Data set splits used for Hungarian.
Note that the proportion of unseen words, nearly 10%, is more than three times higher than in English.
Most of these words were covered by the morphological analyzer (Tron et al., 2006) but still 28% of unseen words were only guessed.
However, this is just 2.86% of the whole corpus, in the magnitude similar to English.
morph lex order seen unseen overall no 1 98.34% 88.96% 97.27%2 98.58% 87.97% 97.40% yes 1 98.32% 96.01% 98.03%2 98.56% 95.96% 98.24% Table 4: Tagging accuracy for Hungarian of HunPos with and without morphological lexicon and with first and second order emission/lexicon probabilities.
On the same corpus TnT had 97.42% and Halacsy et al.(2006) reached 98.17% with a MaxEnt tagger that used the TnT output as a feature.
HunPos gets as good performance in one minute as this MaxEnt model which took three hours to go through the train/test cycle.
4 Concluding
remarks Though there can be little doubt that the ruling system of bakeoffs actively encourages a degree of oneupmanship, our paper and our software are not offered in a competitive spirit.
As we said at the out211 set, we don?t necessarily believe HunPos to be in any way better than TnT, and certainly the main ideas have been pioneered by DeRose (1988), Church (1988), and others long before this generation of HMM work.
But to improve the results beyond what a basic HMM can achieve one needs to tune the system, and progress can only be made if the experiments are end to end replicable.
There is no doubt many other systems could be tweaked further and improve on our results ??what matters is that anybody could now also tweak HunPos without any restriction to improve the state of the art.
Such tweaking can bring surprising results, e.g. the conclusion, strongly supported by the results presented here, that HMM tagging is actually quite competitive with, and orders of magnitude faster than, the current generation of learning algorithms including SVM and MaxEnt.
No matter how good TnT was to begin with, the closed source has hindered its progress to the point that inherently clumsier, but better tweakable algorithms could overtake HMMs, a situation that HunPos has now hopefully changed at least for languages with more complex morphologies.
Acknowledgement We thank Thorsten Brants for TnT, and Gyorgy Gyepesi for constant help and encouragement.
References Michele Banko and Robert C.
Moore. 2004.
Part of speech tagging in context.
In COLING ??4: Proceedings of the 20th international conference on Computational Linguistics, page 556, Morristown, NJ, USA.Association for Computational Linguistics.
Thorsten Brants.
2000. TnT ??a statistical part-of-speech tagger.
In Proceedings of the Sixth Applied Natural Language Processing Conference (ANLP-2000), Seattle, WA.
Kenneth Ward Church.
1988. A stochastic parts program and noun phrase parser for unrestricted text.
In Proceedings of the second conference on Applied natural language processing, pages 136??43, Morristown, NJ, USA.
Association for Computational Linguistics.
Dora Csendes, Janos Csirik, and Tibor Gyimothy.
2004. The Szeged Corpus: A POS tagged and syntactically annotated Hungarian natural language corpus.
In Karel Pala Petr Sojka, Ivan Kopecek, editor, Text, Speech and Dialogue: 7th International Conference, TSD, pages 41??7.
Steven J.
DeRose. 1988.
Grammatical category disambiguation by statistical optimization.
Computational Linguistics, 14:31??9.
Damien Doligez, Jacques Garrigue, Didier Remy, and Jer?ome Vouillon, 2004.
The Objective Caml system.
Institut National de Recherche en Informatique et en Automatique.
Jesus Gimenez and Llus M`arquez.
2003. Fast and accurate part-of-speech tagging: The svm approach revisited.
In Proceedings of RANLP, pages 153??63.
Jan Haji?c, Pavel Krbec, Karel Oliva, Pavel Kv?eto?n, and Vladimr Petkevi?c.
2001. Serial combination of rules and statistics: A case study in Czech tagging.
In Proceedings of the 39th Association of Computational Linguistics Conference, pages 260??67, Toulouse, France.
Dilek Z.
Hakkani-Tur, Kemal Oflazer, and Gokhan Tur.
2000. Statistical morphological disambiguation for agglutinative languages.
In Proceedings of the 18th conference on Computational linguistics, pages 285?? 291, Saarbrucken, Germany.
Peter Halacsy, Andras Kornai, Csaba Oravecz, Viktor Tron, and Daniel Varga.
2006. Using a morphological analyzer in high precision POS tagging of Hungarian.
In Proceedings of LREC 2006, pages 2245??248.
Bryan Jurish.
2003. A hybrid approach to part-of-speech tagging.
Technical report, Berlin-Brandenburgische Akademie der Wissenschaften.
Adwait Ratnaparkhi.
1996. A maximum entropy model for part-of-speech tagging.
In Karel Pala Petr Sojka, Ivan Kopecek, editor, Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133??42, University of Pennsylvania.
Noah A.
Smith, David A.
Smith, and Roy W.
Tromble. 2005.
Context-based morphological disambiguation with random fields.
In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, Vancouver.
Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer.
2003. Feature-rich part-of-speech tagging with a cyclic dependency network.
In Proceedings of HLT-NAACL, pages 252??59.
Viktor Tron, Peter Halacsy, Peter Rebrus, Andras Rung, Peter Vajda, and Eszter Simon.
2006. Morphdb.hu: Hungarian lexical database and morphological grammar.
In Proceedings of LREC 2006, pages 1670??673 .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 213??16, Prague, June 2007.
c2007 Association for Computational Linguistics Extending MARIE: an N-gram-based SMT decoder Josep M.
Crego TALP Research Center Universitat Polit`ecnica de Catalunya Barcelona, 08034 jmcrego@gps.tsc.upc.edu Jose B.
Mari?no TALP Research Center Universitat Polit`ecnica de Catalunya Barcelona,08034 canton@gps.tsc.upc.edu Abstract In this paper we present several extensions of MARIE1, a freely available N-gram-based statistical machine translation (SMT) decoder.
The extensions mainly consist of the ability to accept and generate word graphs and the introduction of two new N-gram models in the loglinear combination of feature functions the decoder implements.
Additionally, the decoder is enhanced with a caching strategy that reduces the number of N-gram calls improving the overall search efficiency.
Experiments are carried out over the Eurpoean Parliament Spanish-English translation task.
1 Introduction
Research on SMT has been strongly boosted in the last few years, partially thanks to the relatively easy development of systems with enough competence as to achieve rather competitive results.
In parallel, tools and techniques have grown in complexity, which makes it difficult to carry out state-of-the-art research without sharing some of this toolkits.
Without aiming at being exhaustive, GIZA++2, SRILM3 and PHARAOH4 are probably the best known examples.
We introduce the recent extensions made to an Ngram-based SMT decoder (Crego et al., 2005), which allowed us to tackle several translation issues (such as reordering, rescoring, modeling, etc).
successfully improving accuracy, as well as efficiency results.
As far as SMT can be seen as a double-sided problem (modeling and search), the decoder emerges as a key component, core module of any SMT system.
Mainly, 1http://gps-tsc.upc.es/soft/soft/marie 2http://www.fjoch.com/GIZA++.html 3http://www.speech.sri.com/projects/srilm/ 4http://www.isi.edu/publications/licensed-sw/pharaoh/ any technique aiming at dealing with a translation problem needs for a decoder extension to be implemented.
Particularly, the reordering problem can be more efficiently (and accurate) addressed when tightly coupled with decoding.
In general, the competence of a decoder to make use of the maximum of information in the global search is directly connected with the likeliness of successfully improving translations.
The paper is organized as follows.
In Section 2 we and briefly review the previous work on decoding with special attention to N-gram-based decoding.
Section 3 describes the extended log-linear combination of feature functions after introduced the two new models.
Section 4 details the particularities of the input and output word graph extensions.
Experiments are reported on section 5.
Finally, conclusions are drawn in section 6.
2 Related
Work The decoding problem in SMT is expressed by the next maximization: argmaxtI 1??
P(tI1|sJ1), where sJ1 is the source sentence to translate and tI1 is a possible translation of the set ?, which contains all the sentences of the language of tI1.
Given that the full search over the whole set of target language sentences is impracticable (? is an infinite set), the translation sentence is usually built incrementally, composing partial translations of the source sentence, which are selected out of a limited number of translation candidates (translation units).
The first SMT decoders were word-based.
Hence, working with translation candidates of single source words.
Later appeared the phrase-based decoders, which use translation candidates composed of sequences of source and target words (outperforming the wordbased decoders by introducing the word context).
In the last few years syntax-based decoders have emerged aiming at dealing with pair of languages with different syntactical structures for which the word context introduced 213 Figure 1: Generative process.
Phrase-based (left) and N-gram-based (right) approaches.
in phrase-based decoders is not sufficient to cope with long reorderings.
Like standard phrase-based decoders, MARIE employs translation units composed of sequences of source and target words.
In contrast, the translation context is differently taken into account.
Whereas phrasebased decoders employ translation units uncontextualized, MARIE takes the translation unit context into account by estimating the translation model as a standard N-gram language model (N-gram-based decoder).
Figure 1 shows that both approaches follow the same generative process, but they differ on the structure of translation units.
In the example, the units ?s1#t1??and ?s2 s3#t2 t3??of the N-gram-based approach are used considering that both appear sequentially.
This fact can be understood as using a longer unit that includes both (longer units are drawn in grey).
MARIE follows the maximum entropy framework, where we can define a translation hypothesis t given a source sentence s, as the target sentence maximizing a log-linear combination of feature functions: ?tI1 = argmax tI1 braceleftBigg Msummationdisplay m=1 mhm(sJ1,tI1) bracerightBigg (1) where m corresponds to the weighting coefficients of the log-linear combination, and the feature functions hm(s,t) to a logarithmic scaling of the probabilities of each model.
See (Mari?no et al., 2006) for further details on the N-gram-based approach to SMT.
3 N-gram Feature Functions Two language models (LM) are introduced in equation 1, aiming at helping the decoder to find the right translations.
Both are estimated as standard N-gram LM.
3.1 Target-side N-gram LM The first additional N-gram LM is destinated to be applied over the target sentence (tagged) words.
Hence, as the original target LM (computed over raw words), it is also used to score the fluency of target sentences, but aiming at achieving generalization power through using a more generalized language (such as a language of Part-of-Speech tags) instead of the one composed of raw words.
Part-Of-Speech tags have successfully been used in several previous experiments.
however, any other tag can be applied.
Several sequences of target tags may apply to any given translation unit (which are passed to the decoder before it starts the search).
For instance, regarding a translation unit with the english word ?general??in its target side, if POS tags were used as target tagged tags, there would exist at least two different tag options: noun and adjective.
In the search, multiple hypotheses are generated concerning different target tagged sides (sequences of tags) of a single translation unit.
Therefore, on the one side, the overall search is extended towards seeking the sequence of target tags that better fits the sequence of target raw words.
On the other side, this extension is hurting the overall efficiency of the decoder as additional hypotheses appear in the search stacks while not additional translation hypotheses are being tested (only differently tagged).
This extended feature may be used toghether with a limitation of the number of target tagged hypotheses per translation unit.
The use of a limited number of these hypotheses implies a balance between accuracy and efficiency.
3.2 Source-side N-gram LM The second N-gram LM is applied over the input sentence tagged words.
Obviously, this model only makes sense when reordering is applied over the source words in order to monotonize the source and target word order.
In such a case, the tagged LM is learnt over the training set with reordered source words.
Hence, the new model is employed as a reordering model.
It scores a given source-side reordering hypothesis according to the reorderings made in the training sentences (from which the tagged LM is estimated).
As for the previous extension, source tagged words are used instead of raw words in order to achieve generalization power.
Additional hypotheses regarding the same translation unit are not generated in the search as all input sentences are uniquely tagged.
Figure 2 illustrates the use of a source POS-tagged N214 gram LM.
The probability of the sequence ?PRN VRB NAME ADJ??is greater than the probability of the sequence ?PRN VRB ADJ NAME??for a model estimated over the training set with reordered source words (with english words following the spanish word order).
Figure 2: Source POS-tagged N-gram LM.
3.3 Caching
N-grams The use of several N-gram LM?s implies a reduction in efficiency in contrast to other models that can be implemented by means of a single lookup table (one access per probability call).
The special characteristics of Ngram LM?s introduce additional memory access to account for backoff probabilities and lower Ngrams fallings.
Many N-gram calls are requested repeatedly, producing multiple calls of an entry.
A simple strategy to reduce additional access consists of keeping a record (cache) for those Ngram entries already requested.
A drawback for the use of a cache consists of the additional memory access derived of the cache maintenance (adding new and checking for existing entries).
Figure 3: Memory access derived of an N-gram call.
Figure 3 illustrates this situation.
The call for a 3-gram probability (requesting for the probability of the sequence of tokens ?a b c?? may need for up to 6 memory access, while under a phrase-based translation model the final probability would always be reached after the first memory access.
The additional access in the N-gram-based approach are used to provide lower N-gram and backoff probabilities in those cases that upper N-gram probabilities do not exist.
4 Word
Graphs Word graphs are successfully used in SMT for several applications.
Basically, with the objective of reducing the redundancy of N-best lists, which very often convey serious combinatorial explosion problems.
A word graph is here described as a directed acyclic graph G = (V,E) with one root node n0 ??V. Edges are labeled with tokens (words or translation units) and optionally with accumulated scores.
We will use (ns(ne ?t??s)), to denote an edge starting at node ns and ending at node ne, with token t and score s.
The file format of word graphs coincides with the graph file format recognized by the CARMEL5 finite state automata toolkit.
4.1 Input
Graph We can mainly find two applications for which word graphs are used as input of an SMT system: the recognition output of an automatic speech recognition (ASR) system; and a reordering graph, consisting of a subset of the whole word permutations of a given input sentence.
In our case we are using the input graph as a reordering graph.
The decoder introduces reordering (distortion of source words order) by allowing only for the distortion encoded in the input graph.
Though, the graph is only allowed to encode permutations of the input words.
In other words, any path in the graph must start at node n0, finish at node nN (where nN is a unique ending node) and cover all the input words (tokens t) in whatever order, without repetitions.
An additional feature function (distortion model) is introduced in the log-linear combination of equation 1: pdistortion(uk) ?? kIproductdisplay i=k1 p(ni|ni)??
(2) where uk refers to the kth partial translation unit covering the source positions [k1,...,kI].
p(ni|ni)?? corresponds to the edge score s encoded in the edge (ns(ne ?t??s)), where ni = ne and ni??
= ns.
One of the decoding first steps consists of building (for each input sentence) the set of translation units to be used in the search.
When the search is extended with reordering abilities the set must be also extended with those translation units that cover any sequence of input words following any of the word orders encoded in the input graph.
The extension of the units set is specially relevant when translation units are built from the tranining set with reordered source words.
Given the example of figure 2, if the translation unit ?translations perfect # traducciones perfectas??is available, the decoder should not discard it, as it provides a right translation.
Notwithstanding that its source side does not follow the original word order of the input sentence.
4.2 Output
Graph The goal of using an output graph is to allow for further rescoring work.
That is, to work with alternative transla5http://www.isi.edu/licensed-sw/carmel/ 215 tions to the single 1-best.
Therefore, our proposed output graph has some peculiarities that make it different to the previously sketched intput graph.
The structure of edges remains the same, but obviously, paths are not forced to consist of permutations of the same tokens (as far as we are interested into multiple translation hypotheses), and there may also exist paths which do not reach the ending node nN.
These latter paths are not useful in rescoring tasks, but allowed in order to facilitate the study of the search graph.
However, a very easy and efficient algorithm (O(n), being n the search size) can be used in order to discard them, before rescoring work.
Additionally, given that partial model costs are needed in rescoring work, our decoder allows to output the individual model costs computed for each translation unit (token t).
Costs are encoded within the token s, as in the next example: (0 (1 "o#or1.5,0.9,0.6,0.2}" 6)) where the token t is now composed of the translation unit ?o#or?? followed by (four) model costs.
Multiple translation hypotheses can only be extracted if hypotheses recombinations are carefully saved.
As in (Koehn, 2004), the decoder takes a record of any recombined hypothesis, allowing for a rigorous N-best generation.
Model costs are referred to the current unit while the global score s is accumulated.
Notice also that translation units (not words) are now used as tokens.
5 Experiments
Experiments are carried out for a Spanish-to-English translation task using the EPPS data set, corresponding to session transcriptions of the European Parliament.
Eff. base +tpos +reor +spos Beam size = 50 w/o cache 1,820 2,170 2,970 3,260 w/ cache ??0 ??10 ??90 ??10 Beam size = 100 w/o cache 2,900 4,350 5,960 6,520 w/ cache ??75 ??10 ??25 ??40 Table 1: Translation efficiency results.
Table 1 shows translation efficiency results (measured in seconds) given two different beam search sizes.
w/cache and w/o cache indicate whether the decoder employs (or not) the cache technique (section 3.3).
Several system configuration have been tested: a baseline monotonous system using a 4-gram translation LM and a 5-gram target LM (base), extended with a target POStagged 5-gram LM (+tpos), further extended by allowing for reordering (+reor), and finally using a source-side POS-tagged 5-gram LM (+spos).
As it can be seen, the cache technique improves the efficiency of the search in terms of decoding time.
Time results are further decreased (reduced time is shown for the w/ cache setting) by using more N-gram LM and allowing for a larger search graph (increasing the beam size and introducing distortion).
Further details on the previous experiment can be seen in (Crego and Mari?no, 2006b; Crego and Mari?no, 2006a), where additionally, the input word graph and extended N-gram tagged LM?s are successfully used to improve accuracy at a very low computational cost.
Several publications can also be found in bibliography which show the use of output graphs in rescoring tasks allowing for clear accuracy improvements.
6 Conclusions
We have presented several extensions to MARIE, a freely available N-gram-based decoder.
The extensions consist of accepting and generating word graphs, and introducing two N-gram LM?s over source and target tagged words.
Additionally, a caching technique is applied over the Ngram LM?s.
Acknowledgments This work has been funded by the European Union under the integrated project TC-STAR (IST-2002-FP65067-38), the Spanish Government under the project AVIVAVOZ (TEC2006-13694-C03) and the Universitat Polit`ecnica de Catalunya under UPC-RECERCA grant.
References J.M.
Crego and J.B.
Mari?no. 2006a.
Integration of postag-based source reordering into smt decoding by an extended search graph.
Proc. of the 7th Conf.
of the Association for Machine Translation in the Americas, pages 29??6, August.
J.M. Crego and J.B.
Mari?no. 2006b.
Reordering experiments for n-gram-based smt.
1st IEEE/ACL Workshop on Spoken Language Technology, December.
J.M. Crego, J.B.
Mari?no, and A.
de Gispert.
2005. Anngram-based statistical machine translation decoder.
Proc. of the 9th European Conference on Speech Communication and Technology, Interspeech??5, pages 3193??196, September.
Ph. Koehn.
2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models.
Proc. of the 6th Conf.
of the Association for Machine Translation in the Americas, pages 115??24, October.
J.B. Mari?no, R.E.
Banchs, J.M.
Crego, A.
de Gispert, P.
Lambert, J.A.R.
Fonollosa, and M.R.
Costa-juss`a. 2006.
N-gram based machine translation.
Computational Linguistics, 32(4):527??49 .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 217??20, Prague, June 2007.
c2007 Association for Computational Linguistics A Hybrid Approach to Word Segmentation and POS Tagging Tetsuji Nakagawa Oki Electric Industry Co., Ltd.
2???? Honmachi, Chuo-ku Osaka 541??053, Japan nakagawa378@oki.com Kiyotaka Uchimoto National Institute of Information and Communications Technology 3??
Hikaridai, Seika-cho, Soraku-gun Kyoto 619??289, Japan uchimoto@nict.go.jp Abstract In this paper, we present a hybrid method for word segmentation and POS tagging.
The target languages are those in which word boundaries are ambiguous, such as Chinese and Japanese.
In the method, word-based and character-based processing is combined, and word segmentation and POS tagging are conducted simultaneously.
Experimental results on multiple corpora show that the integrated method has high accuracy.
1 Introduction
Part-of-speech (POS) tagging is an important task in natural language processing, and is often necessary for other processing such as syntactic parsing.
English POS tagging can be handled as a sequential labeling problem, and has been extensively studied.
However, in Chinese and Japanese, words are not separated by spaces, and word boundaries must be identified before or during POS tagging.
Therefore, POS tagging cannot be conducted without word segmentation, and how to combine these two processing is an important issue.
A large problem in word segmentation and POS tagging is the existence of unknown words.
Unknown words are defined as words that are not in the system?s word dictionary.
It is difficult to determine the word boundaries and the POS tags of unknown words, and unknown words often cause errors in these processing.
In this paper, we study a hybrid method for Chinese and Japanese word segmentation and POS tagging, in which word-based and character-based processing is combined, and word segmentation and POS tagging are conducted simultaneously.
In the method, word-based processing is used to handle known words, and character-based processing is used to handle unknown words.
Furthermore, information of word boundaries and POS tags are used at the same time with this method.
The following sections describe the hybrid method and results of experiments on Chinese and Japanese corpora.
2 Hybrid
Method for Word Segmentation and POS Tagging Many methods have been studied for Chinese and Japanese word segmentation, which include wordbased methods and character-based methods.
Nakagawa (2004) studied a method which combines a word-based method and a character-based method.
Given an input sentence in the method, a lattice is constructed first using a word dictionary, which consists of word-level nodes for all the known words in the sentence.
These nodes have POS tags.
Then, character-level nodes for all the characters in the sentence are added into the lattice (Figure 1).
These nodes have position-of-character (POC) tags which indicate word-internal positions of the characters (Xue, 2003).
There are four POC tags, B, I, E and S, each of which respectively indicates the beginning of a word, the middle of a word, the end of a word, and a single character word.
In the method, the word-level nodes are used to identify known words, and the character-level nodes are used to identify unknown words, because generally wordlevel information is precise and appropriate for processing known words, and character-level information is robust and appropriate for processing unknown words.
Extended hidden Markov models are used to choose the best path among all the possible candidates in the lattice, and the correct path is indicated by the thick lines in Figure 1.
The POS tags and the POC tags are treated equally in the method.
Thus, the word-level nodes and the character-level nodes are processed uniformly, and known words and unknown words are identified simultaneously.
In the method, POS tags of known words as well as word boundaries are identified, but POS tags of unknown words are not identified.
Therefore, we extend the method in order to conduct unknown word POS tagging too: Hybrid Method The method uses subdivided POC-tags in order to identify not only the positions of characters but also the parts-of-speech of the composing words (Figure 2, A).
In the method, POS tagging of unknown words is conducted at the same time as word segmentation and POS tag217 Figure 1: Word Segmentation and Known Word POS Tagging using Word and Character-based Processing ging of known words, and information of partsof-speech of unknown words can be used for word segmentation.
There are also two other methods capable of conducting unknown word POS tagging (Ng and Low, 2004): Word-based Post-Processing Method This method receives results of word segmentation and known word POS tagging, and predicts POS tags of unknown words using words as units (Figure 2, B).
This approach is the same as the approach widely used in English POS tagging.
In the method, the process of unknown word POS tagging is separated from word segmentation and known word POS tagging, and information of parts-of-speech of unknown words cannot be used for word segmentation.
In later experiments, maximum entropy models were used deterministically to predict POS tags of unknown words.
As features for predicting the POS tag of an unknown word w, we used the preceding and the succeeding two words of w and their POS tags, the prefixes and the suffixes of up to two characters of w, the character types contained in w, and the length of w.
Character-based Post-Processing Method This method is similar to the word-based postprocessing method, but in this method, POS tags of unknown words are predicted using characters as units (Figure 2, C).
In the method, POS tags of unknown words are predicted using exactly the same probabilistic models as the hybrid method, but word boundaries and POS tags of known words are fixed in the postprocessing step.
Ng and Low (2004) studied Chinese word segmentation and POS tagging.
They compared several approaches, and showed that character-based approaches had higher accuracy than word-based approaches, and that conducting word segmentation and POS tagging all at once performed better than conducting these processing separately.
Our hybrid method is similar to their character-based all-atonce approach.
However, in their experiments, only word-based and character-based methods were examined.
In our experiments, the combined method of word-based and character-based processing was examined.
Furthermore, although their experiments were conducted with only Chinese data, we conducted experiments with Chinese and Japanese data, and confirmed that the hybrid method performed well on the Japanese data as well as the Chinese data.
3 Experiments
We used five word-segmented and POS-tagged corpora; the Penn Chinese Treebank corpus 2.0 (CTB), a part of the PFR corpus (PFR), the EDR corpus (EDR), the Kyoto University corpus version 2 (KUC) and the RWCP corpus (RWC).
The first two were Chinese (C) corpora, and the rest were Japanese (J) corpora, and they were split into training and test data.
The dictionary distributed with JUMAN version 3.61 (Kurohashi and Nagao, 1998) was used as a word dictionary in the experiments with the KUC corpus, and word dictionaries were constructed from all the words in the training data in the experiments with other corpora.
Table 1 summarizes statistical information of the corpora: the language, the number of POS tags, the sizes of training and test data, and the splitting methods of them1.
We used the following scoring measures to evaluate performance of word segmentation and POS tagging: R : Recall (The ratio of the number of correctly segmented/POS-tagged words in system?s output to the number of words in test data), P : Precision (The ratio of the number of correctly segmented/POS-tagged words in system?s output to the number of words in system?s output), 1The unknown word rate for word segmentation is not equal to the unknown word rate for POS tagging in general, since the word forms of some words in the test data may exist in the word dictionary but the POS tags of them may not exist.
Such words are regarded as known words in word segmentation, but as unknown words in POS tagging.
218 Figure 2: Three Methods for Word Segmentation and POS Tagging F : F-measure (F = 2RP/(R+P)), Runknown : Recall for unknown words, Rknown : Recall for known words.
Table 2 shows the results2.
In the table, Wordbased Post-Proc., Char.-based Post-Proc.
and Hybrid Method respectively indicate results obtained with the word-based post-processing method, the character-based post-processing method, and the hybrid method.
Two types of performance were measured: performance of word segmentation alone, and performance of both word segmentation and POS tagging.
We first compare performance of both word segmentation and POS tagging.
The F-measures of the hybrid method were highest on all the corpora.
This result agrees with the observation by Ng and Low (2004) that higher accuracy was obtained by conducting word segmentation and POS tagging at the same time than by conducting these processing separately.
Comparing the word-based and the character-based post-processing methods, the F-measures of the latter were higher on the Chinese corpora as reported by Ng and Low (2004), but the F-measures of the former were slightly higher on the Japanese corpora.
The same tendency existed in the recalls for known words; the recalls of the character-based post-processing method were highest on the Chinese corpora, but 2The recalls for known words of the word-based and the character-based post-processing methods differ, though the POS tags of known words are identified in the first common step.
This is because known words are sometimes identified as unknown words in the first step and their POS tags are predicted in the post-processing step.
those of the word-based method were highest on the Japanese corpora, except on the EDR corpus.
Thus, the character-based method was not always better than the word-based method as reported by Ng and Low (2004) when the methods were used with the word and character-based combined approach on Japanese corpora.
We next compare performance of word segmentation alone.
The F-measures of the hybrid method were again highest in all the corpora, and the performance of word segmentation was improved by the integrated processing of word segmentation and POS tagging.
The precisions of the hybrid method were highest with statistical significance on four of the five corpora.
In all the corpora, the recalls for unknown words of the hybrid method were highest, but the recalls for known words were lowest.
Comparing our results with previous work is not easy since experimental settings are not the same.
It was reported that the original combined method of word-based and character-based processing had high overall accuracy (F-measures) in Chinese word segmentation, compared with the state-of-the-art methods (Nakagawa, 2004).
Kudo et al.(2004) studied Japanese word segmentation and POS tagging using conditional random fields (CRFs) and rulebased unknown word processing.
They conducted experiments with the KUC corpus, and achieved Fmeasure of 0.9896 in word segmentation, which is better than ours (0.9847).
Some features we did not used, such as base forms and conjugated forms of words, and hierarchical POS tags, were used in 219 Corpus Number Number of Words (Unknown Word Rate for Segmentation/Tagging) (Lang).
of POS [partition in the corpus] Tags Training Test CTB 34 84,937 7,980 (0.0764 / 0.0939) (C) [sec.
1??70] [sec.
271??00] PFR 41 304,125 370,627 (0.0667 / 0.0749) (C) [Jan.
1?Jan. 9] [Jan.
10?Jan. 19] EDR 15 2,550,532 1,280,057 (0.0176 / 0.0189) (J) [id = 4n+0,id = 4n+1] [id = 4n+2] KUC 40 198,514 31,302 (0.0440 / 0.0517) (J) [Jan.
1?Jan. 8] [Jan.
9] RWC 66 487,333 190,571 (0.0513 / 0.0587) (J) [1??0,000th sentences] [10,001??4,000th sentences] Table 1: Statistical Information of Corpora Corpus Scoring Word Segmentation Word Segmentation & POS Tagging (Lang).
Measure Word-based Char.-based Hybrid Word-based Char.-based Hybrid Post-Proc.
Post-Proc. Method Post-Proc.
Post-Proc. Method R 0.9625 0.9625 0.9639 0.8922 0.8935 0.8944 CTB P 0.9408 0.9408 0.9519* 0.8721 0.8733 0.8832 (C) F 0.9516 0.9516 0.9578 0.8821 0.8833 0.8887 Runknown 0.6492 0.6492 0.7148 0.4219 0.4312 0.4713 Rknown 0.9885 0.9885 0.9845 0.9409 0.9414 0.9382 R 0.9503 0.9503 0.9516 0.8967 0.8997 0.9024* PFR P 0.9419 0.9419 0.9485* 0.8888 0.8917 0.8996* (C) F 0.9461 0.9461 0.9500 0.8928 0.8957 0.9010 Runknown 0.6063 0.6063 0.6674 0.3845 0.3980 0.4487 Rknown 0.9749 0.9749 0.9719 0.9382 0.9403 0.9392 R 0.9525 0.9525 0.9525 0.9358 0.9356 0.9357 EDR P 0.9505 0.9505 0.9513* 0.9337 0.9335 0.9346 (J) F 0.9515 0.9515 0.9519 0.9347 0.9345 0.9351 Runknown 0.4454 0.4454 0.4630 0.4186 0.4103 0.4296 Rknown 0.9616 0.9616 0.9612 0.9457 0.9457 0.9454 R 0.9857 0.9857 0.9850 0.9572 0.9567 0.9574 KUC P 0.9835 0.9835 0.9843 0.9551 0.9546 0.9566 (J) F 0.9846 0.9846 0.9847 0.9562 0.9557 0.9570 Runknown 0.9237 0.9237 0.9302 0.6724 0.6774 0.6879 Rknown 0.9885 0.9885 0.9876 0.9727 0.9719 0.9721 R 0.9574 0.9574 0.9592 0.9225 0.9220 0.9255* RWC P 0.9533 0.9533 0.9577* 0.9186 0.9181 0.9241* (J) F 0.9553 0.9553 0.9585 0.9205 0.9201 0.9248 Runknown 0.6650 0.6650 0.7214 0.4941 0.4875 0.5467 Rknown 0.9732 0.9732 0.9720 0.9492 0.9491 0.9491 (Statistical significance tests were performed for R and P, and * indicates significance at p < 0.05) Table 2: Performance of Word Segmentation and POS Tagging their study, and it may be a reason of the difference.
Although, in our experiments, extended hidden Markov models were used to find the best solution, the performance will be further improved by using CRFs instead, which can easily incorporate a wide variety of features.
4 Conclusion
In this paper, we studied a hybrid method in which word-based and character-based processing is combined, and word segmentation and POS tagging are conducted simultaneously.
We compared its performance of word segmentation and POS tagging with other methods in which POS tagging is conducted as a separated post-processing.
Experimental results on multiple corpora showed that the hybrid method had high accuracy in Chinese and Japanese.
References Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying Conditional Random Fields to Japanese Morphological Analysis.
In Proceedings of EMNLP 2004, pages 230??37.
Sadao Kurohashi and Makoto Nagao.
1998. Japanese Morphological Analysis System JUMAN version 3.61.
Department of Informatics, Kyoto University.
(in Japanese).
Tetsuji Nakagawa.
2004. Chinese and Japanese Word Segmentation Using Word-Level and Character-Level Information.
In Proceedings of COLING 2004, pages 466??72.
Hwee Tou Ng and Jin Kiat Low.
2004. Chinese Part-of-Speech Tagging: One-at-a-Time or All-at-Once?
Word-Based or Character-Based?
In Proceedings of EMNLP 2004, pages 277??84.
Nianwen Xue.
2003. Chinese Word Segmentation as Character Tagging.
International Journal of Computational Linguistics and Chinese, 8(1):29??8.
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 221??24, Prague, June 2007.
c2007 Association for Computational Linguistics Automatic Part-of-Speech Tagging for Bengali: An Approach for Morphologically Rich Languages in a Poor Resource Scenario Sandipan Dandapat, Sudeshna Sarkar, Anupam Basu Department of Computer Science and Engineering Indian Institute of Technology Kharagpur India 721302 {sandipan,sudeshna,anupam.basu}@cse.iitkgp.ernet.in Abstract This paper describes our work on building Part-of-Speech (POS) tagger for Bengali.
We have use Hidden Markov Model (HMM) and Maximum Entropy (ME) based stochastic taggers.
Bengali is a morphologically rich language and our taggers make use of morphological and contextual information of the words.
Since only a small labeled training set is available (45,000 words), simple stochastic approach does not yield very good results.
In this work, we have studied the effect of using a morphological analyzer to improve the performance of the tagger.
We find that the use of morphology helps improve the accuracy of the tagger especially when less amount of tagged corpora are available.
1 Introduction
Part-of-Speech (POS) taggers for natural language texts have been developed using linguistic rules, stochastic models as well as a combination of both (hybrid taggers).
Stochastic models (Cutting et al., 1992; Dermatas et al., 1995; Brants, 2000) have been widely used in POS tagging for simplicity and language independence of the models.
Among stochastic models, bi-gram and tri-gram Hidden Markov Model (HMM) are quite popular.
Development of a high accuracy stochastic tagger requires a large amount of annotated text.
Stochastic taggers with more than 95% word-level accuracy have been developed for English, German and other European Languages, for which large labeled data is available.
Our aim here is to develop a stochastic POS tagger for Bengali but we are limited by lack of a large annotated corpus for Bengali.
Simple HMM models do not achieve high accuracy when the training set is small.
In such cases, additional information may be coded into the HMM model to achieve higher accuracy (Cutting et al., 1992).
The semi-supervised model described in Cutting et al.(1992), makes use of both labeled training text and some amount of unlabeled text.
Incorporating a diverse set of overlapping features in a HMM-based tagger is difficult and complicates the smoothing typically used for such taggers.
In contrast, methods based on Maximum Entropy (Ratnaparkhi, 1996), Conditional Random Field (Shrivastav, 2006) etc.
can deal with diverse, overlapping features.
1.1 Previous
Work on Indian Language POS Tagging Although some work has been done on POS tagging of different Indian languages, the systems are still in their infancy due to resource poverty.
Very little work has been done previously on POS tagging of Bengali.
Bengali is the main language spoken in Bangladesh, the second most commonly spoken language in India, and the fourth most commonly spoken language in the world.
Ray et al.(2003) describes a morphologybased disambiguation for Hindi POS tagging.
System using a decision tree based learning algorithm (CN2) has been developed for statistical Hindi POS tagging (Singh et al., 2006).
A reasonably good accuracy POS tagger for Hindi has been developed using Maximum Entropy Markov Model (Dalal et al., 2007).
The system uses linguistic suffix and POS categories of a word along with other contextual features.
2 Our
Approach The problem of POS tagging can be formally stated as follows.
Given a sequence of words w 1 ??w n, we want to find the corresponding sequence of tags t 1 ??t n, drawn from a set of tags T.
We use a tagset of 40 tags 1. In this work, we explore supervised and semi-supervised bi-gram 1 http://www.mla.iitkgp.ernet.in/Tag.html 221 HMM and a ME based model.
The bi-gram assumption states that the POS-tag of a word depends on the current word and the POS tag of the previous word.
An ME model estimates the probabilities based on the imposed constraints.
Such constraints are derived from the training data, maintaining some relationship between features and outcomes.
The most probable tag sequence for a given word sequence satisfies equation (1) and (2) respectively for HMM and ME model: 1 1 ...
1, (|)(| )arg max ii ii ttn in SPwtPt??
= = ??
(1) 11 1, ( ...
| ...
) ( | )nn ii in p ttww pth = = ??
(2) Here, h i is the context for word w i . Since the basic bigram model of HMM as well as the equivalent ME models do not yield satisfactory accuracy, we wish to explore whether other available resources like a morphological analyzer can be used appropriately for better accuracy.
2.1 HMM
and ME based Taggers Three taggers have been implemented based on bigram HMM and ME model.
The first tagger (we shall call it HMM-S) makes use of the supervised HMM model parameters, whereas the second tagger (we shall call it HMM-SS) uses the semi supervised model parameters.
The third tagger uses ME based model to find the most probable tag sequence for a given sequence of words.
In order to further improve the tagging accuracy, we use a Morphological Analyzer (MA) and integrate morphological information with the models.
We assume that the POS-tag of a word w can take values from the set T MA (w), where T MA (w) is computed by the Morphological Analyzer.
Note that the size of T MA (w) is much smaller than T.
Thus, we have a restricted choice of tags as well as tag sequences for a given sentence.
Since the correct tag t for w is always in T MA (w) (assuming that the morphological analyzer is complete), it is always possible to find out the correct tag sequence for a sentence even after applying the morphological restriction.
Due to a much reduced set of possibilities, this model is expected to perform better for both the HMM (HMM-S and HMM-SS) and ME models even when only a small amount of labeled training text is available.
We shall call these new models HMM-S+MA, HMM-SS+ MA and ME+MA.
Our MA has high accuracy and coverage but it still has some missing words and a few errors.
For the purpose of these experiments we have made sure that all words of the test set are present in the root dictionary that an MA uses.
While MA helps us to restrict the possible choice of tags for a given word, one can also use suffix information (i.e., the sequence of last few characters of a word) to further improve the models.
For HMM models, suffix information has been used during smoothing of emission probabilities, whereas for ME models, suffix information is used as another type of feature.
We shall denote the models with suffix information with a ??suf?? marker.
Thus, we have ??HMM-S+suf, HMMS+suf+MA, HMM-SS+suf etc.
2.1.1 Unknown
Word Hypothesis in HMM The transition probabilities are estimated by linear interpolation of unigrams and bigrams.
For the estimation of emission probabilities add-one smoothing or suffix information is used for the unknown words.
If the word is unknown to the morphological analyzer, we assume that the POS-tag of that word belongs to any of the open class grammatical categories (all classes of Noun, Verb, Adjective, Adverb and Interjection).
2.1.2 Features
of the ME Model Experiments were carried out to find out the most suitable binary valued features for the POS tagging in the ME model.
The main features for the POS tagging task have been identified based on the different possible combination of the available word and tag context.
The features also include prefix and suffix up to length four.
We considered different combinations from the following set for obtaining the best feature set for the POS tagging task with the data we have.
{ }112 212,,,,,,, 4, 4iii i i i iFwwwwwtt pre suf+? +?= ? Forty different experiments were conducted taking several combinations from set ?F??to identify the best suited feature set for the POS tagging task.
From our empirical analysis we found that the combination of contextual features (current word and previous tag), prefixes and suffixes of length ??4 gives the best performance for the ME model.
It is interesting to note that the inclusion of prefix and suffix for all words gives better result instead of using only for rare words as is described in Ratnaparkhi (1996).
This can be explained by the fact that due to small amount of annotated data, a significant number of instances 222 are not found for most of the word of the language vocabulary.
3 Experiments
We have a total of 12 models as described in subsection 2.1 under different stochastic tagging schemes.
The same training text has been used to estimate the parameters for all the models.
The model parameters for supervised HMM and ME models are estimated from the annotated text corpus.
For semi-supervised learning, the HMM learned through supervised training is considered as the initial model.
Further, a larger unlabelled training data has been used to re-estimate the model parameters of the semi-supervised HMM.
The experiments were conducted with three different sizes (10K, 20K and 40K words) of the training data to understand the relative performance of the models as we keep on increasing the size of the annotated data.
3.1 Training
Data The training data includes manually annotated 3625 sentences (approximately 40,000 words) for both supervised HMM and ME model.
A fixed set of 11,000 unlabeled sentences (approximately 100,000 words) taken from CIIL corpus 2 are used to re-estimate the model parameter during semi-supervised learning.
It has been observed that the corpus ambiguity (mean number of possible tags for each word) in the training text is 1.77 which is much larger compared to the European languages (Dermatas et al., 1995).
3.2 Test
Data All the models have been tested on a set of randomly drawn 400 sentences (5000 words) disjoint from the training corpus.
It has been noted that 14% words in the open testing text are unknown with respect to the training set, which is also a little higher compared to the European languages (Dermatas et al., 1995) 3.3 Results We define the tagging accuracy as the ratio of the correctly tagged words to the total number of words.
Table 1 summarizes the final accuracies achieved by different learning methods with the varying size of the training data.
Note that the baseline model (i.e., the tag probabilities depends 2 A part of the EMILE/CIIL corpus developed at Central Institute of Indian Languages (CIIL), Mysore.
only on the current word) has an accuracy of 76.8%.
Accuracy Method 10K 20K 40K HMM-S 57.53 70.61 77.29 HMM-S+suf 75.12 79.76 83.85 HMM-S+MA 82.39 84.06 86.64 HMM-S+suf+MA 84.73 87.35 88.75 HMM-SS 63.40 70.67 77.16 HMM-SS+suf 75.08 79.31 83.76 HMM-SS+MA 83.04 84.47 86.41 HMM-SS+suf+MA 84.41 87.16 87.95 ME 74.37 79.50 84.56 ME+suf 77.38 82.63 86.78 ME+MA 82.34 84.97 87.38 ME+suf+MA 84.13 87.07 88.41 Table 1: Tagging accuracies (in %) of different models with 10K, 20K and 40K training data.
3.4 Observations
We find that in both the HMM based models (HMM-S and HMM-SS), the use of suffix information as well as the use of a morphological analyzer improves the accuracy of POS tagging with respect to the base models.
The use of MA gives better results than the use of suffix information.
When we use both suffix information as well as MA, the results is even better.
HMM-SS does better than HMM-S when very little tagged data is available, for example, when we use 10K training corpus.
However, the accuracy of the semi-supervised HMM models are slightly poorer than that of the supervised HMM models for moderate size training data and use of suffix information.
This discrepancy arises due to the over-fitting of the supervised models in the case of small training data; the problem is alleviated with the increase in the annotated data.
As we have noted already the use of MA and/or suffix information improves the accuracy of the POS tagger.
But what is significant to note is that the percentage of improvement is higher when the amount of training data is less.
The HMMS+suf model gives an improvement of around 18%, 9% and 6% over the HMM-S model for 10K, 20K and 40K training data respectively.
Similar trends are observed in the case of the semi-supervised HMM and the ME models.
The use of morphological restriction (HMM-S+MA) gives an improvement of 25%, 14% and 9% respectively over the HMM-S in case of 10K, 20K 223 and 40K training data.
As the improvement due to MA decreases with increasing data, it might be concluded that the use of morphological restriction may not improve the accuracy when a large amount of training data is available.
From our empirical observations we found that both suffix and morphological restriction (HMMS+suf+MA) gives an improvement of 27%, 17% and 12% over the HMM-S model respectively for the three different sizes of training data.
The Maximum Entropy model does better than the HMM models for smaller training data.
But with higher amount of training data the performance of the HMM and ME model are comparable.
Here also we observe that suffix information and MA have positive effect, and the effect is higher with poor resources.
Furthermore, in order to estimate the relative performance of the models, experiments were carried out with two existing taggers: TnT (Brants, 2000) and ACOPOST 3 . The accuracy achieved using TnT are 87.44% and 87.36% respectively with bigram and trigram model for 40K training data.
The accuracy with ACOPOST is 86.3%.
This reflects that the higher order Markov models do not work well under the current experimental setup.
3.5 Assessment
of Error Types Table 2 shows the top five confusion classes for HMM-S+MA model.
The most common types of errors are the confusion between proper noun and common noun and the confusion between adjective and common noun.
This results from the fact that most of the proper nouns can be used as common nouns and most of the adjectives can be used as common nouns in Bengali.
Actual Class (frequency) Predicted Class % of total errors % of class errors NP(251) NN 21.03 43.82 JJ(311) NN 5.16 8.68 NN(1483) JJ 4.78 1.68 DTA(100) PP 2.87 15.0 NN(1483) VN 2.29 0.81 Table 2: Five most common types of errors Almost all the confusions are wrong assignment due to less number of instances in the training corpora, including errors due to long distance phenomena.
3 http://maxent.sourceforge.net 4 Conclusion In this paper we have described an approach for automatic stochastic tagging of natural language text for Bengali.
The models described here are very simple and efficient for automatic tagging even when the amount of available annotated text is small.
The models have a much higher accuracy than the nave baseline model.
However, the performance of the current system is not as good as that of the contemporary POStaggers available for English and other European languages.
The best performance is achieved for the supervised learning model along with suffix information and morphological restriction on the possible grammatical categories of a word.
In fact, the use of MA in any of the models discussed above enhances the performance of the POS tagger significantly.
We conclude that the use of morphological features is especially helpful to develop a reasonable POS tagger when tagged resources are limited.
References A.
Dalal, K.
Nagaraj, U.
Swant, S.
Shelke and P.
Bhattacharyya. 2007.
Building Feature Rich POS Tagger for Morphologically Rich Languages: Experience in Hindi.
ICON, 2007.
A. Ratnaparkhi, 1996.
A maximum entropy part-of-speech tagger.
EMNLP 1996.
pp. 133-142.
D. Cutting, J.
Kupiec, J.
Pederson and P.
Sibun. 1992.
A practical part-of-speech tagger.
In Proc.
of the 3rd Conference on Applied NLP, pp.
133-140. E.
Dermatas and K.
George. 1995.
Automatic stochastic tagging of natural language texts.
Computational Linguistics, 21(2): 137-163.
M. Shrivastav, R.
Melz, S.
Singh, K.
Gupta and P.
Bhattacharyya, 2006.
Conditional Random Field Based POS Tagger for Hindi.
In Proceedings of the MSPIL, pp.
63-68. P.
R. Ray, V.
Harish, A.
Basu and S.
Sarkar, 2003.
Part of Speech Tagging and Local Word Grouping Techniques for Natural Language Processing.
ICON 2003.
S. Singh, K.
Gupta, M.
Shrivastav and P.
Bhattacharyya, 2006.
Morphological Richness Offset Resource Demand ??Experience in constructing a POS Tagger for Hindi.
COLING/ACL 2006, pp.
779-786. T.
Brants. 2000.
TnT ??A statistical part-of-sppech tagger.
In Proc.
of the 6th Applied NLP Conference, pp. 224-231 .
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 225??28, Prague, June 2007.
c2007 Association for Computational Linguistics Japanese Dependency Parsing Using Sequential Labeling for Semi-spoken Language Kenji Imamura and Genichiro Kikui NTT Cyber Space Laboratories, NTT Corporation 1-1 Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan {imamura.kenji, kikui.genichiro}@lab.ntt.co.jp Norihito Yasuda NTT Communication Science Laboratories, NTT Corporation 2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan n-yasuda@cslab.kecl.ntt.co.jp Abstract The amount of documents directly published by end users is increasing along with the growth of Web 2.0.
Such documents often contain spoken-style expressions, which are difficult to analyze using conventional parsers.
This paper presents dependency parsing whose goal is to analyze Japanese semi-spoken expressions.
One characteristic of our method is that it can parse selfdependent (independent) segments using sequential labeling.
1 Introduction
Dependency parsing is a way of structurally analyzing a sentence from the viewpoint of modification.
In Japanese, relationships of modification between phrasal units called bunsetsu segments are analyzed.
A number of studies have focused on parsing of Japanese as well as of other languages.
Popular parsers are CaboCha (Kudo and Matsumoto, 2002) and KNP (Kurohashi and Nagao, 1994), which were developed to analyze formal written language expressions such as that in newspaper articles.
Generally, the syntactic structure of a sentence is represented as a tree, and parsing is carried out by maximizing the likelihood of the tree (Charniak, 2000; Uchimoto et al., 1999).
Units that do not modify any other units, such as fillers, are difficult to place in the tree structure.
Conventional parsers have forced such independent units to modify other units.
Documents published by end users (e.g., blogs) are increasing on the Internet along with the growth of Web 2.0.
Such documents do not use controlled written language and contain fillers and emoticons.
This implies that analyzing such documents is difficult for conventional parsers.
This paper presents a new method of Japanese dependency parsing that utilizes sequential labeling based on conditional random fields (CRFs) in order to analyze semi-spoken language.
Concretely, sequential labeling assigns each segment a dependency label that indicates its relative position of dependency.
If the label set includes self-dependency, the fillers and emoticons would be analyzed as segments depending on themselves.
Therefore, since it is not necessary for the parsing result to be a tree, our method is suitable for semi-spoken language.
2 Methods
Japanese dependency parsing for written language is based on the following principles.
Our method relaxes the first principle to allow self-dependent segments (c.f.
Section 2.3). 1.
Dependency moves from left to right.
2. Dependencies do not cross each other.
3. Each segment, except for the top of the parsed tree, modifies at most one other segment.
2.1 Dependency
Parsing Using Cascaded Chunking (CaboCha) Our method is based on the cascaded chunking method (Kudo and Matsumoto, 2002) proposed as the CaboCha parser 1. CaboCha is a sort of shiftreduce parser and determines whether or not a segment depends on the next segment by using an 1 http://www.chasen.org/?taku/software/cabocha/ 225 SVM-based classifier.
To analyze long-distance dependencies, CaboCha shortens the sentence by removing segments for which dependencies are already determined and which no other segments depend on.
CaboCha constructs a tree structure by repeating the above process.
2.2 Sequential
Labeling Sequential labeling is a process that assigns each unit of an input sequence an appropriate label (or tag).
In natural language processing, it is applied to, for example, English part-of-speech tagging and named entity recognition.
Hidden Markov models or conditional random fields (Lafferty et al., 2001) are used for labeling.
In this paper, we use linearchain CRFs.
In sequential labeling, training data developers can design labels with no restrictions.
2.3 Cascaded
Chunking Using Sequential Labeling The method proposed in this paper is a generalization of CaboCha.
Our method considers not only the next segment, but also the followingN segments to determine dependencies.
This area, including the considered segment, is called the window, and N is called the window size.
The parser assigns each segment a dependency label that indicates where the segment depends on the segments in the window.
The flow is summarized as follows: 1.
Extract features from segments such as the part-of-speech of the headword in a segment (c.f.
Section 3.1). 2.
Carry out sequential labeling using the above features.
3. Determine the actual dependency by interpreting the labels.
4. Shorten the sentence by deleting segments for which the dependency is already determined and that other segments have never depended on.
5. If only one segment remains, then finish the process.
If not, return to Step 1.
An example of dependency parsing for written language is shown in Figure 1 (a).
In Steps 1 and 2, dependency labels are supplied to each segment in a way similar to that used by Label Description ??Segment depends on a segment outside of window.
0Q Self-dependency 1D Segment depends on next segment.
2D Segment depends on segment after next.
-1O Segment is top of parsed tree.
Table 1: Label List Used by Sequential Labeling (Window Size: 2) other sequential labeling methods.
However, our sequential labeling has the following characteristics since this task is dependency parsing.
??The labels indicate relative positions of the dependent segment from the current segment (Table 1).
Therefore, the number of labels changes according to the window size.
Long-distance dependencies can be parsed by one labeling process if we set a large window size.
However, growth of label variety causes data sparseness problems.
??One possible label is that of self-dependency (noted as ??Q??in this paper).
This is assigned to independent segments in a tree.
??Also possible are two special labels.
Label ??1O?? denotes a segment that is the top of the parsed tree.
Label ??denotes a segment that depends on a segment outside of the window.
When the window size is two, the segment depends on a segment that is over two segments ahead.
??The label for the current segment is determined based on all features in the window and on the label of the previous segment.
In Step 4, segments, which no other segments depend on, are removed in a way similar to that used by CaboCha.
The principle that dependencies do not cross each other is applied in this step.
For example, if a segment depends on a segment after the next, the next segment cannot be modified by other segments.
Therefore, it can be removed.
Similarly, since the ??label indicates that the segment depends on a segment after N segments, all intermediate segments can be removed if they do not have ??labels.
The sentence is shortened by iteration of the above steps.
The parsing finishes when only one segment remains in the sentence (this is the segment 226 (a) Written Language --2D 1D 1D -1O 2D 1D -1O Output Input Label Label kare wa (he) kanojo no (her) atatakai (warm) magokoro ni (heart) kando-shita.
(be moved) (He was moved by her warm heart.) Seg.
No. 1 2 3 4 5 kare wa (he) kanojo no (her) atatakai (warm) magokoro ni (heart) kando-shita.
(be moved) (b) Semi-spoken Language Input Uuuum, kyo wa (today) ...... choshi (condition) yokatta desu.
(be good) 0Q --0Q 1D -1O 1D -1O (Uuuum, my condition .... was good today.) Seg.
No. 1 2 3 4 5 Label Label Uuuum, kyo wa (today) ...... choshi (condition) yokatta desu.
(be good) Output 1st Labeling 2nd Labeling Figure 1: Examples of Dependency Parsing (Window Size: 2) Corpus Type # of Sentences # of Segments Kyoto Training 24,283 234,685 Test 9,284 89,874 Blog Training 18,163 106,177 Test 8,950 53,228 Table 2: Corpus Size at the top of the parsed tree).
In the example in Figure 1 (a), the process finishes in two iterations.
In a sentence containing fillers, the selfdependency labels are assigned by sequential labeling, as shown in Figure 1 (b), and are parsed as independent segments.
Therefore, our method is suitable for parsing semi-spoken language that contains independent segments.
3 Experiments
3.1 Experimental Settings Corpora In our experiments, we used two corpora.
One is the Kyoto Text Corpus 4.0 2, which is a collection of newspaper articles with segment and dependency annotations.
The other is a blog corpus, which is a collection of blog articles taken as semi-spoken language.
The blog corpus is manually annotated in a way similar to that used for the Kyoto text corpus.
The sizes of the corpora are shown in Table 2.
Training We used CRF++ 3, a linear-chain CRF training tool, with eleven features per segment.
All 2 http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus.html 3 http://www.chasen.org/?taku/software/CRF++/ of these are static features (proper to each segment) such as surface forms, parts-of-speech, inflections of a content headword and a functional headword in a segment.
These are parts of a feature set that many papers have referenced (Uchimoto et al., 1999; Kudo and Matsumoto, 2002).
Evaluation Metrics Dependency accuracy and sentence accuracy were used as evaluation metrics.
Sentence accuracy is the proportion of total sentences in which all dependencies in the sentence are accurately labeled.
In Japanese, the last segment of most sentences is the top of the parsed trees, and many papers exclude this last segment from the accuracy calculation.
We, in contrast, include the last one because some of the last segments are selfdependent.
3.2 Accuracy
of Dependency Parsing Dependency parsing was carried out by combining training and test corpora.
We used a window size of three.
We also used CaboCha as a reference for the set of sentences trained only with the Kyoto corpus because it is designed for written language.
The results are shown in Table 3.
CaboCha had better accuracies for the Kyoto test corpus.
One reason might be that our method manually combined features and used parts of combinations, while CaboCha automatically finds the best combinations by using second-order polynomial kernels.
For the blog test corpus, the proposed method using the Kyoto+Blog model had the best depen227 Test Corpus Method Training Corpus Dependency Accuracy Sentence Accuracy (Model) Kyoto Proposed Method Kyoto 89.87% (80766 / 89874) 48.12% (4467 / 9284) (Written Language) (Window Size: 3) Kyoto + Blog 89.76% (80670 / 89874) 47.63% (4422 / 9284) CaboCha Kyoto 92.03% (82714 / 89874) 55.36% (5140 / 9284) Blog Proposed Method Kyoto 77.19% (41083 / 53226) 41.41% (3706 / 8950) (Semi-spoken Language) (Window Size: 3) Kyoto + Blog 84.59% (45022 / 53226) 52.72% (4718 / 8950) CaboCha Kyoto 77.44% (41220 / 53226) 43.45% (3889 / 8950) Table 3: Dependency and Sentence Accuracies among Methods/Corpora 88 88.5 89 89.5 90 90.5 91 1 2 3 4 5 0 2e+06 4e+06 6e+06 8e+06 1e+07 Dependency Accuracy (%) # of Features Window Size Dependency Accuracy # of Features Figure 2: Dependency Accuracy and Number of Features According to Window Size (The Kyoto Text Corpus was used for training and testing.) dency accuracy result at 84.59%.
This result was influenced not only by the training corpus that contains the blog corpus but also by the effect of selfdependent segments.
The blog test corpus contains 3,089 self-dependent segments, and 2,326 of them (75.30%) were accurately parsed.
This represents a dependency accuracy improvement of over 60% compared with the Kyoto model.
Our method is effective in parsing blogs because fillers and emoticons can be parsed as selfdependent segments.
3.3 Accuracy
According to Window Size Another characteristic of our method is that all dependencies, including long-distance ones, can be parsed by one labeling process if the window covers the entire sentence.
To analyze this characteristic, we evaluated dependency accuracies in various window sizes.
The results are shown in Figure 2.
The number of features used for labeling increases exponentially as window size increases.
However, dependency accuracy was saturated after a window size of two, and the best accuracy was when the window size was four.
This phenomenon implies a data sparseness problem.
4 Conclusion
We presented a new dependency parsing method using sequential labeling for the semi-spoken language that frequently appears in Web documents.
Sequential labeling can supply segments with flexible labels, so our method can parse independent words as self-dependent segments.
This characteristic affects robust parsing when sentences contain fillers and emoticons.
The other characteristics of our method are using CRFs and that long dependencies are parsed in one labeling process.
SVM-based parsers that have the same characteristics can be constructed if we introduce multi-class classifiers.
Further comparisons with SVM-based parsers are future work.
References Eugene Charniak.
2000. A maximum-entropy-inspired parser.
In Proc.
of NAACL-2000, pages 132??39.
Taku Kudo and Yuji Matsumoto.
2002. Japanese dependency analyisis using cascaded chunking.
In Proc.
of CoNLL-2002, Taipei.
Sadao Kurohashi and Makoto Nagao.
1994. A syntactic analysis method of long Japanese sentences based on the detection of conjunctive structures.
Computational Linguistics, 20(4):507??34.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data.
In Proc.
of ICML-2001, pages 282??89.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
1999. Japanese dependency structure analysis based on maximum entropy models.
In Proc.
of EACL??9, pages 196??03, Bergen, Norway .
Proceedings of ACL-08: HLT, pages 8996,
Columbus, Ohio, USA, June 2008. c2008 Association for Computational Linguistics
Measure Word Generation for English-Chinese SMT Systems 
 
 
Dongdong Zhang
1, Mu Li
1, Nan Duan
2, Chi-Ho Li
1, Ming Zhou
1
 
1
Microsoft Research Asia 
2
Tianjin University 
Beijing, China Tianjin, China 
{dozhang,muli,v-naduan,chl,mingzhou}@microsoft.com 
 
 
 
 
 
 
Abstract 
Measure words in Chinese are used to indi-
cate the count of nouns. Conventional sta-
tistical machine translation (SMT) systems do 
not perform well on measure word generation 
due to data sparseness and the potential long 
distance dependency between measure words 
and their corresponding head words. In this 
paper, we propose a statistical model to gen-
erate appropriate measure words of nouns for 
an English-to-Chinese SMT system. We mod-
el the probability of measure word generation 
by utilizing lexical and syntactic knowledge 
from both source and target sentences. Our 
model works as a post-processing procedure 
over output of statistical machine translation 
systems, and can work with any SMT system. 
Experimental results show our method can 
achieve high precision and recall in measure 
word generation. 
1 Introduction

In linguistics, measure words (MW) are words or 
morphemes used in combination with numerals or 
demonstrative pronouns to indicate the count of 
nouns
1, which are often referred to as head words 
(HW). 
Chinese measure words are grammatical units 
and occur quite often in real text. According to our 
survey on the measure word distribution in the 
Chinese Penn Treebank and the test datasets distri-
buted by Linguistic Data Consortium (LDC) for 
Chinese-to-English machine translation evaluation, 
the average occurrence is 0.505 and 0.319 measure 
                                                 
1
 The uncommon cases of verbs are not considered. 
words per sentence respectively. Unlike in Chinese, 
there is no special set of measure words in English. 
Measure words are usually used for mass nouns 
and any semantically appropriate nouns can func-
tion as the measure words. For example, in the 
phrase three bottles of water, the word bottles acts 
as a measure word. Countable nouns are almost 
never modified by measure words
2
. Numerals and 
indefinite articles are directly followed by counta-
ble nouns to denote the quantity of objects.  
Therefore, in the English-to-Chinese machine 
translation task we need to take additional efforts 
to generate the missing measure words in Chinese. 
For example, when translating the English phrase 
three books into the Chinese phrases  , 
where three corresponds to the numeral   and 
books corresponds to the noun  , the Chinese 
measure word   should be generated between 
the numeral and the noun.  
In most statistical machine translation (SMT) 
models (Och et al., 2004; Koehn et al., 2003; 
Chiang, 2005), some of measure words can be 
generated without modification or additional 
processing. For example, in above translation, the 
phrase translation table may suggest the word three 
be translated into  ,  ,  , etc, and 
the word books into  ,  ,   (scroll), 
etc. Then the SMT model selects the most likely 
combination   as the final translation re-
sult. In this example, a measure word candidate set 
consisting of   and   can be generated by 
bilingual phrases (or synchronous translation rules), 
and the best measure word   from the measure  
                                                 
2
 There are some exceptional cases, such as 100 head of cat-
tle. But they are very uncommon. 
89
 
 
 
 
 
 
 
 
 
 
 
 
 
 
word candidate set can be selected by the SMT 
decoder. However, as we will show below, existing 
SMT systems do not deal well with the measure 
word generation in general due to data sparseness 
and long distance dependencies between measure 
words and their corresponding head words.  
Due to the limited size of bilingual corpora, 
many measure words, as well as the collocations 
between a measure and its head word, cannot be 
well covered by the phrase translation table in an 
SMT system. Moreover, Chinese measure words 
often have a long distance dependency to their 
head words which makes language model ineffec-
tive in selecting the correct measure words from 
the measure word candidate set. For example, in 
Figure 1 the distance between the measure word 
  and its head word   (undertaking) is 15. 
In this case, an n-gram language model with n<15 
cannot capture the MW-HW collocation. Table 1 
shows the relative positions distribution of head 
words around measure words in the Chinese Penn 
Treebank, where a negative position indicates that 
the head word is to the left of the measure word 
and a positive position indicates that the head word 
is to the right of the measure word. Although lots 
of measure words are close to the head words they 
modify, more than sixteen percent of measure 
words are far away from their corresponding head 
words (the absolute distance is more than 5). 
To overcome the disadvantage of measure word 
generation in a general SMT system, this paper 
proposes a dedicated statistical model to generate 
measure words for English-to-Chinese translation. 
We model the probability of measure word gen-
eration by utilizing rich lexical and syntactic 
knowledge from both source and target sentences. 
Three steps are involved in our method to generate 
measure words: Identifying the positions to gener-
ate measure words, collecting the measure word 
candidate set and selecting the best measure word. 
Our method is performed as a post-processing pro-
cedure of the output of SMT systems. The advan-
tage is that it can be easily integrated into any SMT 
system. Experimental results show our method can 
significantly improve the quality of measure word 
generation. We also compared the performance of 
our model based on different contextual informa-
tion, and show that both large-scale monolingual 
data and parallel bilingual data can be helpful to 
generate correct measure words. 
Position Occurrence Position Occurrence
1 39.5% -1 0 
2 15.7% -2 0 
3 4.7% -3 8.7% 
4 1.4% -4 6.8% 
5 2.1% -5 4.3% 
>5 8.8% <-5 8.0% 
Table 1. Position distribution of head words 
2 Our
Method 
2.1 Measure
word  generation in Chinese 
In Chinese, measure words are obligatory in cer-
tain contexts, and the choice of measure word 
usually depends on the head words semantics (e.g., 
shape or material). The set of Chinese measure 
words is a relatively close set and can be classified 
into two categories based on whether they have a 
corresponding English translation. Those not hav-
ing an English counterpart need to be generated 
during translation. For those having English trans-
lations, such as   (meter),   (ton), we just 
use the translation produced by the SMT system 
itself. According to our survey, about 70.4% of 
measure words in the Chinese Penn Treebank need 
Figure 1.  Example of long distance dependency between MW and its modified HW 
 / / 
 /  / 
    
Pudong 's de-
velopment and 
opening up is
a 
century-spanning 
/ /
 / 
for vigorously promoting shanghai 
and constructing a modern econom-
ic , trade , and financial center  undertaking
 / /  /   /  /
/  /  /   / /  /  / 
  
. 

90
 
to be explicitly generated during the translation 
process. 
In Chinese, there are generally stable linguistic 
collocations between measure words and their head 
words. Once the head word is determined, the col-
located measure word can usually be selected ac-
cordingly. However, there is no easy way to identi-
fy head words in target Chinese sentences since for 
most of the time an SMT output is not a well 
formed sentence due to translation errors. Mistake 
of head word identification may cause low quality 
of measure word generation. In addition, some-
times the head word itself is not enough to deter-
mine the measure word. For example, in Chinese 
sentences  5   (there are five people 
in his family) and  5   (a 
total of five people attended the meeting), where 
  (people) is the head word collocated with two 
different measure words   and  , we cannot 
determine the measure word just based on the head 
word  .   
2.2 Framework

In our framework, a statistical model is used to 
generate measure words. The model is applied to 
SMT system outputs as a post-processing proce-
dure. Given an English source sentence, an SMT 
decoder produces a target Chinese translation, in 
which positions for measure word generation are 
identified. Based on contextual information con-
tained in both input source sentence and SMT sys-
tems output translation, a measure word candidate 
set M is constructed. Then a measure word selec-
tion model is used to select the best one from M. 
Finally, the selected measure word is inserted into 
previously determined measure word slot in the 
SMT systems output, yielding the final translation 
result. 
2.3 Measure
word position identification 
To identify where to generate measure words in the 
SMT outputs, all positions after numerals are 
marked at first since measure words often follow 
numerals. For other cases in which measure words 
do not follow numerals (e.g.,  / /  
(many computers), where   is a measure word 
and   (computers) is its head word), we just 
mine the set of words which can be followed by 
measure words from training corpus.  Most of 
words in the set are pronouns such as   (this), 
  (that) and   (several). In the SMT out-
put, the positions after these words are also identi-
fied as candidate positions to generate measure 
words.  
2.4 Candidate
measure word generation 
To avoid high computation cost, the measure word 
candidate set only consists of those measure words 
which can form valid MW-HW collocations with 
their head words. We assume that all the surround-
ing words within a certain window size centered on 
the given position to generate a measure word are 
potential head words, and require that a measure 
word candidate must collocate with at least one of 
the surrounding words. Valid MW-HW colloca-
tions are mined from the training corpus and a sep-
arate lexicon resource.  
There is a possibility that the real head word is 
outside the window of given size. To address this 
problem, we also use a source window centered on 
the position p
s, which is aligned to the target meas-
ure word position p
t
. The link between p
s
 and p
t
 
can be inferred from SMT decoding result. Thus, 
the chance of capturing the best measure word in-
creases with the aid of words located in the source 
window. For example, given the window size of 10, 
although the target head word   (undertaking) 
in Figure 1 is located outside the target window, its 
corresponding source head word undertaking can 
be found in the source window. Based on this 
source head word, the best measure word   will 
be included into the candidate measure word set. 
This example shows how bilingual information can 
enrich the measure word candidate set. 
Another special word {NULL} is always in-
cluded in the measure word candidate set. {NULL} 
represents those measure words having a corres-
ponding English translation as mentioned in Sec-
tion 2.1. If {NULL} is selected, it means that we 
need not generate any measure word at the current 
position. Thus, no matter what kinds of measure 
words they are, we can handle the issue of measure 
word generation in a unified framework.  
2.5 Measure
word selection model 
After obtaining the measure word candidate set M, 
a measure word selection model is employed to 
select the best one from M. Given the contextual 
information C in both source window and target 
91
 
window, we model the measure word selection as 
finding the measure word m* with highest post-
erior probability given C: 


=argmax
nullnull
(|)                   (1) 
To leverage the collocation knowledge between 
measure words and head words, we extend (1) by 
introducing a hidden variable h where H represents 
all candidate head words located within the target 
window: 
     

=argmax
nullnull
  ( ,| )
nullnull
 
           = argmax
nullnull
  ( | ) (|,)
nullnull
  (2) 
In (2),  ( | )  is the head word selection proba-
bility and is empirically estimated according to the 
position distribution of head words in Table 1. 
(|,)  is the conditional probability of m given 
both h and C. We use maximum entropy model to 
compute (|,) : 
            (|,) =
exp(  

 

(,)

)
 exp(  

 

(
,)

)



     (3) 
Based on the different features used in the com-
putation of (|,) , we can train two sub-
models  a monolingual model (Mo-ME) which 
only uses monolingual (Chinese) features and a 
bilingual model (Bi-ME) which integrates bilingual 
features. The advantage of the Mo-ME model is 
that it can employ an unlimited monolingual target 
training corpora, while the Bi-ME model leverages 
rich features including both the source and target 
information and may improve the precision. Com-
pared to the Mo-ME model, the Bi-ME model suf-
fers from small scale of parallel training data. To 
leverage advantages of both models, we use a 
combined model Co-ME, by linearly combing the 
monolingual and bilingual sub-models: 


=argmax
nullnull

nullnullnullnullnull
 + ( 1 ) 
nullnullnullnullnull
  
where [0,1]  is a free parameter that can be op-
timized on held-out data and it was set to 0.39 in 
our experiments. 
2.6 Features

The computation of Formula (3) involves the fea-
tures listed in Table 2 where the Mo-ME model 
only employs target features and the Bi-ME model 
leverages both target features and source features.  
For target features, n-gram language model 
score is defined as the sum of log n-gram probabil-
ities within the target window after the measure 
word is filled into the measure word slot. The 
MW-HW collocation feature is defined to be a 
function f
1
 to capture the collocation between a 
measure word and a head word. For features of 
surrounding words, the feature function f
2
 is de-
fined as 1 if a certain word exists at a certain posi-
tion, otherwise 0. For example, f
2
( ,-2)=1 means 
the second word on the left is  . f
2
( ,3)=1 
means the third word on the right is  . For 
punctuation position feature function f
3, the feature 
value is 1 when there is a punctuation following 
the measure word, which indicates the target head 
word may appear to the left of measure word. Oth-
erwise, it is 0. In practice, we can also ignore the 
position part, i.e., a word appears anywhere within 
the window is viewed as the same feature. 
 Target features Source features 
n-gram language model 
score 
MW-HW collocation
MW-HW collocation surrounding words 
surrounding words source head word 
punctuation position POS tags 
Table 2. Features used in our model 
For source language side features, MW-HW col-
location and surrounding words are used in a simi-
lar way as does with target features. The source 
head word feature is defined to be a function f
4
 to 
indicate whether a word e
i
 is the source head word 
in English according to a parse tree of the source 
sentence. Similar to the definition of lexical fea-
tures, we also use a set of features based on POS 
tags of source language. 
3 Model
Training and Application 
3.1 Training

We parsed English and Chinese sentences to get 
training samples for measure word generation 
model. Based on the source syntax parse tree, for 
each measure word, we identified its head word by 
using a toolkit from (Chiang and Bikel, 2002) 
which can heuristically identify head words for 
sub-trees. For the bilingual corpus, we also per-
form word alignment to get correspondences be-
tween source and target words. Then, the colloca-
tion between measure words and head words and 
their surrounding contextual information are ex-
tracted to train the measure word selection models. 
According to word alignment results, we classify 
92
 
measure words into two classes based on whether 
they have non-null translations. We map Chinese 
measure words having non-null translations to a 
unified symbol {NULL} as mentioned in Section 
2.4, indicating that we need not generate these kind 
of measure words since they can be translated from 
English.  
In our work, the Berkeley parser (Petrov and 
Klein, 2007) was employed to extract syntactic 
knowledge from the training corpus. We ran GI-
ZA++ (Och and Ney, 2000) on the training corpus 
in both directions with IBM model 4, and then ap-
plied the refinement rule described in (Koehn et al., 
2003) to obtain a many-to-many word alignment 
for each sentence pair. We used the SRI Language 
Modeling Toolkit (Stolcke, 2002) to train a five-
gram model with modified Kneser-Ney smoothing 
(Chen and Goodman, 1998). The Maximum Entro-
py training toolkit from (Zhang, 2006) was em-
ployed to train the measure word selection model. 
3.2 Measure
word generation 
As mentioned in previous sections, we apply our 
measure word generation module into SMT output 
as a post-processing step. Given a translation from 
an SMT system, we first determine the position p
t
 
at which to generate a Chinese measure word. Cen-
tered on p
t, a surrounding word window with spe-
cified size is determined. From translation align-
ments, the corresponding source position p
s
 aligned 
to p
t
 can be referred.  In the same way, a source 
window centered on p
s
 is determined as well. Then, 
contextual information within the windows in the 
source and the target sentence is extracted and fed 
to the measure word selection model. Meanwhile, 
the candidate set is obtained based on words in 
both windows. Finally, each measure word in the 
candidate set is inserted to the position p
t, and its 
score is calculated based on the models presented 
in Section 2.5. The measure word with the highest 
probability will be chosen.  
There are two reasons why we perform measure 
word generation for SMT systems as a post-
processing step. One is that in this way our method 
can be easily applied to any SMT system. The oth-
er is that we can leverage both source and target 
information during the measure word generation 
process. We do not integrate our measure word 
generation module into the SMT decoder since 
there is only little target contextual information 
available during SMT decoding. Moreover, as we 
will show in experiment section, a pre-processing 
method does not work well when only source in-
formation is available. 
4 Experiments

4.1 Data

In the experiments, the language model is a Chi-
nese 5-gram language model trained with the Chi-
nese part of the LDC parallel corpus and the Xin-
hua part of the Chinese Gigaword corpus with 
about 27 million words. We used an SMT system 
similar to Chiang (2005), in which FBIS corpus is 
used as the bilingual training data. The training 
corpus for Mo-ME model consists of the Chinese 
Peen Treebank and the Chinese part of the LDC 
parallel corpus with about 2 million sentences. The 
Bi-ME model is trained with FBIS corpus, whose 
size is smaller than that used in Mo-ME model 
training. 
We extracted both development and test data set 
from years of NIST Chinese-to-English evaluation 
data by filtering out sentence pairs not containing 
measure words. The development set is extracted 
from NIST evaluation data from 2002 to 2004, and 
the test set consists of sentence pairs from NIST 
evaluation data from 2005 to 2006. There are 759 
testing cases for measure word generation in our 
test data consisting of 2746 sentence pairs. We use 
the English sentences in the data sets as input to 
the SMT decoder, and apply our proposed method 
to generate measure words for the output from the 
decoder. Measure words in Chinese sentences of 
the development and test sets are used as refer-
ences. When there are more than one measure 
words acceptable at some places, we manually 
augment the references with multiple acceptable 
measure words. 
4.2 Baseline

Our baseline is the SMT output where measure 
words are generated by a Hiero-like SMT decoder 
as discussed in Section 1. Due to noises in the Chi-
nese translations introduced by the SMT system, 
we cannot correctly identify all the positions to 
generate measure words. Therefore, besides preci-
sion we examine recall in our experiments. 
4.3 Evaluation
over SMT output 
Table 3 and Table 4 show the precision and recall 
of our measure word generation method. From the 
93
 
experimental results, the Mo-ME, Bi-ME and Co-
ME models all outperform the baseline. Compared 
with the baseline, the Mo-ME method takes advan-
tage of a large size monolingual training corpus 
and reduces the data sparseness problem. The ad-
vantage of the Bi-ME model is being able to make 
full use of rich knowledge from both source and 
target sentences. Also as shown in Table 3 and Ta-
ble 4, the Co-ME model always achieve the best 
results when using the same window size since it 
leverages the advantage of both the Mo-ME and 
the Bi-ME models. 
Wsize Baseline Mo-ME Bi-ME Co-ME
6  
 
54.82% 
64.29% 67.15%  67.66% 
8 64.93% 68.50% 69.00%
10 64.72% 69.40% 69.58%
12 65.46% 69.40% 69.76%
14 65.61% 69.69% 70.03% 
Table 3. Precision over SMT output 
Wsize Baseline Mo-ME Bi-ME Co-ME
6  
 
45.61% 
51.48% 53.69%  54.09% 
8 51.98% 54.75% 55.14%
10 51.81% 55.44% 55.58%
12 52.38% 55.44% 55.72%
14 52.50% 55.67% 55.93% 
Table 4. Recall over SMT output 
We can see that the Bi-ME model can achieve 
better results than the Mo-ME model in both recall 
and precision metrics although only a small sized 
bilingual corpus is used for Bi-ME model training. 
The reason is that the Mo-ME model cannot cor-
rectly handle the cases where head words are lo-
cated outside the target window. However, due to 
word order differences between English and Chi-
nese, when target head words are outside the target 
window, their corresponding source head words 
might be within the source window. The capacity 
of capturing head words is improved when both 
source and target windows are used, which demon-
strates that bilingual knowledge is useful for meas-
ure word generation. 
We compare the results for each model with dif-
ferent window sizes. Larger window size can lead 
to better results as shown in Table 3 and Table 4 
since more contextual knowledge is used to model 
measure word generation. However, enlarging the 
window size does not bring significant improve-
ments, The major reason is that even a small win-
dow size is already able to cover most of measure 
word collocations, as indicated by the position dis-
tribution of head words in Table 1.  
The quality of the SMT output also affects the 
quality of measure word generation since our me-
thod is performed in a post-processing step over 
the SMT output. Although translation errors de-
grade the measure word generation accuracy, we 
achieve about 15% improvement in precision and a 
10% increase in recall over baseline. We notice 
that the recall is relatively lower. Part of the reason 
is some positions to generate measure words are 
not successfully identified due to translation errors. 
In addition to precision and recall, we also evaluate 
the Bleu score (Papineni et al., 2002) changes be-
fore and after applying our measure word genera-
tion method to the SMT output. For our test data, 
we only consider sentences containing measure 
words for Bleu score evaluation. Our measure 
word generation step leads to a Bleu score im-
provement of 0.32 where the window size is set to 
10, which shows that it can improve the translation 
quality of an English-to-Chinese SMT system. 
4.4 Evaluation
over reference data 
To isolate the impact of the translation errors in 
SMT output on the performance of our measure 
word generation model, we conducted another ex-
periment with reference bilingual sentences in 
which measure words in Chinese sentences are 
manually removed. This experiment can show the 
performance upper bound of our method without 
interference from an SMT system. Table 5 shows 
the results. Compared to the results in Table 3, the 
precision improvement in the Mo-ME model is 
larger than that in the Bi-ME model, which shows 
that noisy translation of the SMT system has more 
serious influence on the Mo-ME model than the 
Bi-ME model. This also indicates that source in-
formation without noises is helpful for measure 
word generation. 
Wsize Mo-ME Bi-ME Co-ME 
6 71.63% 74.92% 75.72% 
8 73.80% 75.48% 76.20% 
10 73.80% 74.76% 75.48% 
12 73.80% 75.24% 75.96% 
14 73.56% 75.48% 76.44% 
Table 5. Results over reference data 
94
 
4.5 Impacts
of features 
In this section, we examine the contribution of 
both target language based features and source 
language based features in our model. Table 6 and 
Table 7 show the precision and recall when using 
different features. The window size is set to 10. In 
the tables, Lm denotes the n-gram language model 
feature, Tmh denotes the feature of collocation be-
tween target head words and the candidate measure 
word, Smh denotes the feature of collocation be-
tween source head words and the candidate meas-
ure word, Hs denotes the feature of source head 
word selection, Punc denotes the feature of target 
punctuation position, Tlex denotes surrounding 
word features in translation, Slex denotes surround-
ing word features in source sentence, and Pos de-
notes Part-Of-Speech feature. 
Feature setting Precision Recall 
Baseline 54.82% 45.61% 
Lm 51.11% 41.24% 
+Tmh 61.43% 49.22% 
+Punc 62.54% 50.08% 
+Tlex 64.80% 51.87% 
Table 6. Feature contribution in Mo-ME model 
Feature setting Precision Recall 
Baseline 54.82% 45.61% 
Lm 51.11% 41.24% 
+Tmh+Smh 64.50% 51.64% 
+Hs 65.32% 52.26% 
+Punc 66.29% 53.10% 
+Pos 66.53% 53.25% 
+Tlex 67.50% 54.02% 
+Slex 69.52% 55.54% 
Table 7. Feature contribution in Bi-ME model 
The experimental results show that all the fea-
tures can bring incremental improvements. The 
method with only Lm feature performs worse than 
the baseline. However, with more features inte-
grated, our method outperforms the baseline, 
which indicates each kind of features we selected 
is useful for measure word generation. According 
to the results, the feature of MW-HW collocation 
has much contribution to reducing the selection 
error of measure words given head words. The 
contribution of Slex feature explains that other sur-
rounding words in source sentence are also helpful 
since head word determination in source language 
might be incorrect due to errors in English parse 
trees. Meanwhile, the contribution from Smh, Hs 
and Slex features demonstrates that bilingual 
knowledge can play an important role for measure 
word generation. Compared with lexicalized fea-
tures, we do not get much benefit from the Pos 
features. 
4.6 Error
analysis 
We conducted an error analysis on 100 randomly 
selected sentences from the test data. There are 
four major kinds of errors as listed in Table 8. 
Most errors are caused by failures in finding posi-
tions to generate measure words. The main reason 
for this is some hint information used to identify 
measure word positions is missing in the noisy 
output of SMT systems. Two kinds of errors are 
introduced by incomplete head word and MW-HW 
collocation coverage, which can be solved by en-
larging the size of training corpus. There are also 
head word selection errors due to incorrect syntax 
parsing. 
Error type Ratio 
unseen head word  32.14% 
unseen MW-HW collocation 10.71% 
missing MW position 39.29% 
incorrect HW selection 10.71% 
others 7.14%
Table 8. Error distribution 
4.7 Comparison
with other methods 
In this section we compare our statistical methods 
with the pre-processing method and the rule-based 
methods for measure word generation in a transla-
tion task.  
In pre-processing method, only source language 
information is available. Given a source sentence, 
the corresponding syntax parse tree T
s
 is first con-
structed with an English parser. Then the pre-
processing method chooses the source head word 
h
s
 based on T
s
. The candidate measure word with 
the highest probability collocated with h
s
 is se-
lected as the best result, where the measure word 
candidate set corresponding to each head word is 
mined over a bilingual training corpus in advance. 
We achieved precision 58.62% and recall 49.25%, 
which are worse than the results of our post-
processing based methods. The weakness of the 
pre-processing method is twofold. One problem is 
data sparseness with respect to collocations be-
95
 
tween English head words and Chinese measure 
words. The other problem comes from the English 
head word selection error introduced by using 
source parse trees.  
We also compared our method with a well-
known rule-based machine translation system  
SYSTRAN
3
. We translated our test data with SY-
STRANs English-to-Chinese translation engine. 
The precision and recall are 63.82% and 51.09% 
respectively, which are also lower than our method.  
5 Related
Work  
Most existing rule-based English-to-Chinese MT 
systems have a dedicated module handling meas-
ure word generation. In general a rule-based me-
thod uses manually constructed rule patterns to 
predict measure words. Like most rule based ap-
proaches, this kind of system requires lots of hu-
man efforts of experienced linguists and usually 
cannot easily be adapted to a new domain. The 
most relevant work based on statistical methods to 
our research might be statistical technologies em-
ployed to model issues such as morphology gener-
ation (Minkov et al., 2007). 
6 Conclusion
and Future Work 
In this paper we propose a statistical model for 
measure word generation for English-to-Chinese 
SMT systems, in which contextual knowledge 
from both source and target sentences is involved. 
Experimental results show that our method not on-
ly achieves high precision and recall for generating 
measure words, but also improves the quality of 
English-to-Chinese SMT systems. 
In the future, we plan to investigate more fea-
tures and enlarge coverage to improve the quality 
of measure word generation, especially reduce the 
errors found in our experiments. 
Acknowledgements 
Special thanks to David Chiang, Stephan Stiller 
and the anonymous reviewers for their feedback 
and insightful comments. 

References 

Stanley F. Chen and Joshua Goodman. 1998. An Empir-
ical study of smoothing techniques for language 
                                                 
3
 http://www.systransoft.com/ 
modeling. Technical Report TR-10-98, Harvard Uni-
versity Center for Research in Computing Technolo-
gy, 1998. 

David Chiang and Daniel M. Bikel. 2002. Recovering 
latent information in treebanks. Proceedings of COL-
ING '02, 2002.  

David Chiang. 2005. A hierarchical phrase-based mod-
el for statistical machine translation. In Proceedings 
of ACL 2005, pages 263-270. 

Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical phrase-based translation. In Proceedings of 
HLT-NAACL 2003, pages 127-133.  

Einat Minkov, Kristina Toutanova, and Hisami Suzuki. 
2007. Generating complex morphology for machine 
translation. In Proceedings of 45th Annual Meeting 
of the ACL, pages 128-135. 

Franz J. Och and Hermann Ney. 2000. Improved statis-
tical alignment models. In Proceedings of 38th An-
nual Meeting of the ACL, pages 440-447.  

Franz J. Och and Hermann Ney. 2004. The alignment 
template approach to statistical machine translation. 
Computational Linguistics, 30:417-449. 

Kishore Papineni, Salim Roukos, ToddWard, and WeiJ-
ing Zhu. 2002. BLEU: a method for automatic evalu-
ation of machine translation. In Proceedings of 40th 
Annual Meeting of the ACL, pages 311-318. 

Slav Petrov and Dan Klein. 2007. Improved inference 
for unlexicalized parsing. In Proceedings of HLT-
NAACL, 2007. 

Andreas Stolcke. 2002. SRILM an extensible language 
modeling toolkit. In Proceedings of International 
Conference on Spoken Language Processing, volume 
2, pages 901-904.  

Le Zhang. MaxEnt toolkit. 2006. http://homepages.inf. 
ed.ac.uk/s0450736/maxent_toolkit.html  
96
Proceedings of ACL-08: HLT, pages 595603,
Columbus, Ohio, USA, June 2008. c2008 Association for Computational Linguistics
Simple Semi-supervised Dependency Parsing
Terry Koo, Xavier Carreras, and Michael Collins
MIT CSAIL, Cambridge, MA 02139, USA
{maestro,carreras,mcollins}@csail.mit.edu
Abstract
We present a simple and effective semi-
supervised method for training dependency
parsers. We focus on the problem of lex-
ical representation, introducing features that
incorporate word clusters derived from a large
unannotated corpus. We demonstrate the ef-
fectiveness of the approach in a series of de-
pendency parsing experiments on the Penn
Treebank and Prague Dependency Treebank,
and we show that the cluster-based features
yield substantial gains in performance across
a wide range of conditions. For example, in
the case of English unlabeled second-order
parsing, we improve from a baseline accu-
racy of 92.02% to 93.16%, and in the case
of Czech unlabeled second-order parsing, we
improve from a baseline accuracy of 86.13%
to 87.13%. In addition, we demonstrate that
our method also improves performance when
small amounts of training data are available,
and can roughly halve the amount of super-
vised data required to reach a desired level of
performance.
1 Introduction
In natural language parsing, lexical information is
seen as crucial to resolving ambiguous relationships,
yet lexicalized statistics are sparse and difficult to es-
timate directly. It is therefore attractive to consider
intermediate entities which exist at a coarser level
than the words themselves, yet capture the informa-
tion necessary to resolve the relevant ambiguities.
In this paper, we introduce lexical intermediaries
via a simple two-stage semi-supervised approach.
First, we use a large unannotated corpus to define
word clusters, and then we use that clustering to
construct a new cluster-based feature mapping for
a discriminative learner. We are thus relying on the
ability of discriminative learning methods to identify
and exploit informative features while remaining ag-
nostic as to the origin of such features. To demon-
strate the effectiveness of our approach, we conduct
experiments in dependency parsing, which has been
the focus of much recent researche.g., see work
in the CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al., 2007).
The idea of combining word clusters with dis-
criminative learning has been previously explored
by Miller et al. (2004), in the context of named-
entity recognition, and their work directly inspired
our research. However, our target task of depen-
dency parsing involves more complex structured re-
lationships than named-entity tagging; moreover, it
is not at all clear that word clusters should have any
relevance to syntactic structure. Nevertheless, our
experiments demonstrate that word clusters can be
quite effective in dependency parsing applications.
In general, semi-supervised learning can be mo-
tivated by two concerns: first, given a fixed amount
of supervised data, we might wish to leverage ad-
ditional unlabeled data to facilitate the utilization of
the supervised corpus, increasing the performance of
the model in absolute terms. Second, given a fixed
target performance level, we might wish to use un-
labeled data to reduce the amount of annotated data
necessary to reach this target.
We show that our semi-supervised approach
yields improvements for fixed datasets by perform-
ing parsing experiments on the Penn Treebank (Mar-
cus et al., 1993) and Prague Dependency Treebank
(Hajic, 1998; Hajic et al., 2001) (see Sections 4.1
and 4.3). By conducting experiments on datasets of
varying sizes, we demonstrate that for fixed levels of
performance, the cluster-based approach can reduce
the need for supervised data by roughly half, which
is a substantial savings in data-annotation costs (see
Sections 4.2 and 4.4).
The remainder of this paper is divided as follows:
595
Ms. Haag plays Elianti .*
obj
prootnmod sbj
Figure 1: An example of a labeled dependency tree. The
tree contains a special token * which is always the root
of the tree. Each arc is directed from head to modifier and
has a label describing the function of the attachment.
Section 2 gives background on dependency parsing
and clustering, Section 3 describes the cluster-based
features, Section 4 presents our experimental results,
Section 5 discusses related work, and Section 6 con-
cludes with ideas for future research.
2 Background
2.1 Dependency
parsing
Recent work (Buchholz and Marsi, 2006; Nivre
et al., 2007) has focused on dependency parsing.
Dependency syntax represents syntactic informa-
tion as a network of head-modifier dependency arcs,
typically restricted to be a directed tree (see Fig-
ure 1 for an example). Dependency parsing depends
critically on predicting head-modifier relationships,
which can be difficult due to the statistical sparsity
of these word-to-word interactions. Bilexical depen-
dencies are thus ideal candidates for the application
of coarse word proxies such as word clusters.
In this paper, we take a part-factored structured
classification approach to dependency parsing. For a
given sentence x, letY(x) denote the set of possible
dependency structures spanning x, where each y 
Y(x) decomposes into a set of parts ry. In the
simplest case, these parts are the dependency arcs
themselves, yielding a first-order or edge-factored
dependency parsing model. In higher-order parsing
models, the parts can consist of interactions between
more than two words. For example, the parser of
McDonald and Pereira (2006) defines parts for sib-
ling interactions, such as the trio plays, Elianti,
and . in Figure 1. The Carreras (2007) parser
has parts for both sibling interactions and grandpar-
ent interactions, such as the trio *, plays, and
Haag in Figure 1. These kinds of higher-order
factorizations allow dependency parsers to obtain a
limited form of context-sensitivity.
Given a factorization of dependency structures
into parts, we restate dependency parsing as the fol-
apple pear Apple IBM bought run of in
01
100 101 110 111000 001 010 011
00
0
10
1
11
Figure 2: An example of a Brown word-cluster hierarchy.
Each node in the tree is labeled with a bit-string indicat-
ing the path from the root node to that node, where 0
indicates a left branch and 1 indicates a right branch.
lowing maximization:
PARSE(x;w) = argmax
yY(x)
summationdisplay
ry
wf(x,r)
Above, we have assumed that each part is scored
by a linear model with parameters w and feature-
mapping f(). For many different part factoriza-
tions and structure domains Y(), it is possible to
solve the above maximization efficiently, and several
recent efforts have concentrated on designing new
maximization algorithms with increased context-
sensitivity (Eisner, 2000; McDonald et al., 2005b;
McDonald and Pereira, 2006; Carreras, 2007).
2.2 Brown
clustering algorithm
In order to provide word clusters for our exper-
iments, we used the Brown clustering algorithm
(Brown et al., 1992). We chose to work with the
Brown algorithm due to its simplicity and prior suc-
cess in other NLP applications (Miller et al., 2004;
Liang, 2005). However, we expect that our approach
can function with other clustering algorithms (as in,
e.g., Li and McCallum (2005)). We briefly describe
the Brown algorithm below.
The input to the algorithm is a vocabulary of
words to be clustered and a corpus of text containing
these words. Initially, each word in the vocabulary
is considered to be in its own distinct cluster. The al-
gorithm then repeatedly merges the pair of clusters
which causes the smallest decrease in the likelihood
of the text corpus, according to a class-based bigram
language model defined on the word clusters. By
tracing the pairwise merge operations, one obtains
a hierarchical clustering of the words, which can be
represented as a binary tree as in Figure 2.
Within this tree, each word is uniquely identified
by its path from the root, and this path can be com-
pactly represented with a bit string, as in Figure 2.
In order to obtain a clustering of the words, we se-
lect all nodes at a certain depth from the root of the
596
hierarchy. For example, in Figure 2 we might select
the four nodes at depth 2 from the root, yielding the
clusters {apple,pear}, {Apple,IBM}, {bought,run},
and{of,in}. Note that the same clustering can be ob-
tained by truncating each words bit-string to a 2-bit
prefix. By using prefixes of various lengths, we can
produce clusterings of different granularities (Miller
et al., 2004).
For all of the experiments in this paper, we used
the Liang (2005) implementation of the Brown algo-
rithm to obtain the necessary word clusters.
3 Feature
design
Key to the success of our approach is the use of fea-
tures which allow word-cluster-based information to
assist the parser. The feature sets we used are simi-
lar to other feature sets in the literature (McDonald
et al., 2005a; Carreras, 2007), so we will not attempt
to give a exhaustive description of the features in
this section. Rather, we describe our features at a
high level and concentrate on our methodology and
motivations. In our experiments, we employed two
different feature sets: a baseline feature set which
draws upon normal information sources such as
word forms and parts of speech, and a cluster-based
feature set that also uses information derived from
the Brown cluster hierarchy.
3.1 Baseline
features
Our first-order baseline feature set is similar to the
feature set of McDonald et al. (2005a), and consists
of indicator functions for combinations of words and
parts of speech for the head and modifier of each
dependency, as well as certain contextual tokens.1
Our second-order baseline features are the same as
those of Carreras (2007) and include indicators for
triples of part of speech tags for sibling interactions
and grandparent interactions, as well as additional
bigram features based on pairs of words involved
these higher-order interactions. Examples of base-
line features are provided in Table 1.
1We augment the McDonald et al. (2005a) feature set with
backed-off versions of the Surrounding Word POS Features
that include only one neighboring POS tag. We also add binned
distance features which indicate whether the number of tokens
between the head and modifier of a dependency is greater than
2, 5, 10, 20, 30, or 40 tokens.
Baseline Cluster-based
ht,mt hc4,mc4
hw,mw hc6,mc6
hw,ht,mt hc*,mc*
hw,ht,mw hc4,mt
ht,mw,mt ht,mc4
hw,mw,mt hc6,mt
hw,ht,mw,mtht,mc6
 hc4,mw
hw,mc4

ht,mt,st hc4,mc4,sc4
ht,mt,gt hc6,mc6,sc6
 ht,mc4,sc4
hc4,mc4,gc4

Table 1: Examples of baseline and cluster-based feature
templates. Each entry represents a class of indicators for
tuples of information. For example, ht,mt represents
a class of indicator features with one feature for each pos-
sible combination of head POS-tag and modifier POS-
tag. Abbreviations: ht = head POS, hw = head word,
hc4 = 4-bit prefix of head, hc6 = 6-bit prefix of head,
hc* = full bit string of head; mt,mw,mc4,mc6,mc* =
likewise for modifier; st,gt,sc4,gc4,... = likewise
for sibling and grandchild.
3.2 Cluster-based features
The firstand second-order cluster-based feature sets
are supersets of the baseline feature sets: they in-
clude all of the baseline feature templates, and add
an additional layer of features that incorporate word
clusters. Following Miller et al. (2004), we use pre-
fixes of the Brown cluster hierarchy to produce clus-
terings of varying granularity. We found that it was
nontrivial to select the proper prefix lengths for the
dependency parsing task; in particular, the prefix
lengths used in the Miller et al. (2004) work (be-
tween 12 and 20 bits) performed poorly in depen-
dency parsing.2 After experimenting with many dif-
ferent feature configurations, we eventually settled
on a simple but effective methodology.
First, we found that it was helpful to employ two
different types of word clusters:
1. Short bit-string prefixes (e.g., 46 bits), which
we used as replacements for parts of speech.
2One possible explanation is that the kinds of distinctions
required in a named-entity recognition task (e.g., Alice versus
Intel) are much finer-grained than the kinds of distinctions
relevant to syntax (e.g., apple versus eat).
597
2. Full bit strings,3 which we used as substitutes
for word forms.
Using these two types of clusters, we generated new
features by mimicking the template structure of the
original baseline features. For example, the baseline
feature set includes indicators for word-to-word and
tag-to-tag interactions between the head and mod-
ifier of a dependency. In the cluster-based feature
set, we correspondingly introduce new indicators for
interactions between pairs of short bit-string pre-
fixes and pairs of full bit strings. Some examples
of cluster-based features are given in Table 1.
Second, we found it useful to concentrate on
hybrid features involving, e.g., one bit-string and
one part of speech. In our initial attempts, we fo-
cused on features that used cluster information ex-
clusively. While these cluster-only features provided
some benefit, we found that adding hybrid features
resulted in even greater improvements. One possible
explanation is that the clusterings generated by the
Brown algorithm can be noisy or only weakly rele-
vant to syntax; thus, the clusters are best exploited
when anchored to words or parts of speech.
Finally, we found it useful to impose a form of
vocabulary restriction on the cluster-based features.
Specifically, for any feature that is predicated on a
word form, we eliminate this feature if the word
in question is not one of the top-N most frequent
words in the corpus. When N is between roughly
100 and 1,000, there is little effect on the perfor-
mance of the cluster-based feature sets.4 In addition,
the vocabulary restriction reduces the size of the fea-
ture sets to managable proportions.
4 Experiments
In order to evaluate the effectiveness of the cluster-
based feature sets, we conducted dependency pars-
ing experiments in English and Czech. We test the
features in a wide range of parsing configurations,
including first-order and second-order parsers, and
labeled and unlabeled parsers.5
3As in Brown et al. (1992), we limit the clustering algorithm
so that it recovers at most 1,000 distinct bit-strings; thus full bit
strings are not equivalent to word forms.
4We used N = 800 for all experiments in this paper.
5In an unlabeled parser, we simply ignore dependency la-
bel information, which is a common simplification.
The English experiments were performed on the
Penn Treebank (Marcus et al., 1993), using a stan-
dard set of head-selection rules (Yamada and Mat-
sumoto, 2003) to convert the phrase structure syn-
tax of the Treebank to a dependency tree represen-
tation.6 We split the Treebank into a training set
(Sections 221), a development set (Section 22), and
several test sets (Sections 0,7 1, 23, and 24). The
data partition and head rules were chosen to match
previous work (Yamada and Matsumoto, 2003; Mc-
Donald et al., 2005a; McDonald and Pereira, 2006).
The part of speech tags for the development and test
data were automatically assigned by MXPOST (Rat-
naparkhi, 1996), where the tagger was trained on
the entire training corpus; to generate part of speech
tags for the training data, we used 10-way jackknif-
ing.8 English word clusters were derived from the
BLLIP corpus (Charniak et al., 2000), which con-
tains roughly 43 million words of Wall Street Jour-
nal text.9
The Czech experiments were performed on the
Prague Dependency Treebank 1.0 (Hajic, 1998;
Hajic et al., 2001), which is directly annotated
with dependency structures. To facilitate compar-
isons with previous work (McDonald et al., 2005b;
McDonald and Pereira, 2006), we used the train-
ing/development/test partition defined in the corpus
and we also used the automatically-assigned part of
speech tags provided in the corpus.10 Czech word
clusters were derived from the raw text section of
the PDT 1.0, which contains about 39 million words
of newswire text.11
We trained the parsers using the averaged percep-
tron (Freund and Schapire, 1999; Collins, 2002),
which represents a balance between strong perfor-
mance and fast training times. To select the number
6We used Joakim Nivres Penn2Malt conversion tool
(http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html). Depen-
dency labels were obtained via the Malt hard-coded setting.
7For computational reasons, we removed a single 249-word
sentence from Section 0.
8That is, we tagged each fold with the tagger trained on the
other 9 folds.
9We ensured that the sentences of the Penn Treebank were
excluded from the text used for the clustering.
10Following Collins et al. (1999), we used a coarsened ver-
sion of the Czech part of speech tags; this choice also matches
the conditions of previous work (McDonald et al., 2005b; Mc-
Donald and Pereira, 2006).
11This text was disjoint from the training and test corpora.
598
Sec dep1 dep1c MD1 dep2 dep2c MD2 dep1-L dep1c-L dep2-L dep2c-L
00 90.48 91.57(+1.09)  91.76 92.77(+1.01)  90.29 91.03(+0.74) 91.33 92.09(+0.76)
01 91.31 92.43(+1.12)  92.46 93.34(+0.88)  90.84 91.73(+0.89) 91.94 92.65(+0.71)
23 90.84 92.23(+1.39) 90.9 92.02 93.16(+1.14) 91.5 90.32 91.24(+0.92) 91.38 92.14(+0.76)
24 89.67 91.30(+1.63)  90.92 91.85(+0.93)  89.55 90.06(+0.51) 90.42 91.18(+0.76)
Table 2: Parent-prediction accuracies on Sections 0, 1, 23, and 24. Abbreviations: dep1/dep1c = first-order parser with
baseline/cluster-based features; dep2/dep2c = second-order parser with baseline/cluster-based features; MD1 = Mc-
Donald et al. (2005a); MD2 = McDonald and Pereira (2006); suffix -L = labeled parser. Unlabeled parsers are scored
using unlabeled parent predictions, and labeled parsers are scored using labeled parent predictions. Improvements of
cluster-based features over baseline features are shown in parentheses.
of iterations of perceptron training, we performed up
to 30 iterations and chose the iteration which opti-
mized accuracy on the development set. Our feature
mappings are quite high-dimensional, so we elimi-
nated all features which occur only once in the train-
ing data. The resulting models still had very high
dimensionality, ranging from tens of millions to as
many as a billion features.12
All results presented in this section are given
in terms of parent-prediction accuracy, which mea-
sures the percentage of tokens that are attached to
the correct head token. For labeled dependency
structures, both the head token and dependency label
must be correctly predicted. In addition, in English
parsing we ignore the parent-predictions of punc-
tuation tokens,13 and in Czech parsing we retain
the punctuation tokens; this matches previous work
(Yamada and Matsumoto, 2003; McDonald et al.,
2005a; McDonald and Pereira, 2006).
4.1 English
main results
In our English experiments, we tested eight differ-
ent parsing configurations, representing all possi-
ble choices between baseline or cluster-based fea-
ture sets, first-order (Eisner, 2000) or second-order
(Carreras, 2007) factorizations, and labeled or unla-
beled parsing.
Table 2 compiles our final test results and also
includes two results from previous work by Mc-
Donald et al. (2005a) and McDonald and Pereira
(2006), for the purposes of comparison. We note
a few small differences between our parsers and the
12Due to the sparsity of the perceptron updates, however,
only a small fraction of the possible features were active in our
trained models.
13A punctuation token is any token whose gold-standard part
of speech tag is one of {  : , .}.
parsers evaluated in this previous work. First, the
MD1 and MD2 parsers were trained via the MIRA
algorithm (Crammer and Singer, 2003; Crammer et
al., 2004), while we use the averaged perceptron. In
addition, the MD2 model uses only sibling interac-
tions, whereas the dep2/dep2c parsers include both
sibling and grandparent interactions.
There are some clear trends in the results of Ta-
ble 2. First, performance increases with the order of
the parser: edge-factored models (dep1 and MD1)
have the lowest performance, adding sibling rela-
tionships (MD2) increases performance, and adding
grandparent relationships (dep2) yields even better
accuracies. Similar observations regarding the ef-
fect of model order have also been made by Carreras
(2007).
Second, note that the parsers using cluster-based
feature sets consistently outperform the models us-
ing the baseline features, regardless of model order
or label usage. Some of these improvements can be
quite large; for example, a first-order model using
cluster-based features generally performs as well as
a second-order model using baseline features. More-
over, the benefits of cluster-based feature sets com-
bine additively with the gains of increasing model
order. For example, consider the unlabeled parsers
in Table 2: on Section 23, increasing the model or-
der from dep1 to dep2 results in a relative reduction
in error of roughly 13%, while introducing cluster-
based features from dep2 to dep2c yields an addi-
tional relative error reduction of roughly 14%. As a
final note, all 16 comparisons between cluster-based
features and baseline features shown in Table 2 are
statistically significant.14
14We used the sign test at the sentence level. The comparison
between dep1-L and dep1c-L is significant at p < 0.05, and all
other comparisons are significant at p < 0.0005.
599
Tagger always trained on full Treebank Tagger trained on reduced dataset
Size dep1 dep1c  dep2 dep2c 
1k 84.54 85.90 1.36 86.29 87.47 1.18
2k 86.20 87.65 1.45 87.67 88.88 1.21
4k 87.79 89.15 1.36 89.22 90.46 1.24
8k 88.92 90.22 1.30 90.62 91.55 0.93
16k 90.00 91.27 1.27 91.27 92.39 1.12
32k 90.74 92.18 1.44 92.05 93.36 1.31
All 90.89 92.33 1.44 92.42 93.30 0.88
Size dep1 dep1c  dep2 dep2c 
1k 80.49 84.06 3.57 81.95 85.33 3.38
2k 83.47 86.04 2.57 85.02 87.54 2.52
4k 86.53 88.39 1.86 87.88 89.67 1.79
8k 88.25 89.94 1.69 89.71 91.37 1.66
16k 89.66 91.03 1.37 91.14 92.22 1.08
32k 90.78 92.12 1.34 92.09 93.21 1.12
All 90.89 92.33 1.44 92.42 93.30 0.88
Table 3: Parent-prediction accuracies of unlabeled English parsers on Section 22. Abbreviations: Size = #sentences in
training corpus;  = difference between cluster-based and baseline features; other abbreviations are as in Table 2.
4.2 English
learning curves
We performed additional experiments to evaluate the
effect of the cluster-based features as the amount
of training data is varied. Note that the depen-
dency parsers we use require the input to be tagged
with parts of speech; thus the quality of the part-of-
speech tagger can have a strong effect on the per-
formance of the parser. In these experiments, we
consider two possible scenarios:
1. The tagger has a large training corpus, while
the parser has a smaller training corpus. This
scenario can arise when tagged data is cheaper
to obtain than syntactically-annotated data.
2. The same amount of labeled data is available
for training both tagger and parser.
Table 3 displays the accuracy of firstand second-
order models when trained on smaller portions of
the Treebank, in both scenarios described above.
Note that the cluster-based features obtain consistent
gains regardless of the size of the training set. When
the tagger is trained on the reduced-size datasets,
the gains of cluster-based features are more pro-
nounced, but substantial improvements are obtained
even when the tagger is accurate.
It is interesting to consider the amount by which
cluster-based features reduce the need for supervised
data, given a desired level of accuracy. Based on
Table 3, we can extrapolate that cluster-based fea-
tures reduce the need for supervised data by roughly
a factor of 2. For example, the performance of the
dep1c and dep2c models trained on 1k sentences is
roughly the same as the performance of the dep1
and dep2 models, respectively, trained on 2k sen-
tences. This approximate data-halving effect can be
observed throughout the results in Table 3.
When combining the effects of model order and
cluster-based features, the reductions in the amount
of supervised data required are even larger. For ex-
ample, in scenario 1 the dep2c model trained on 1k
sentences is close in performance to the dep1 model
trained on 4k sentences, and the dep2c model trained
on 4k sentences is close to the dep1 model trained on
the entire training set (roughly 40k sentences).
4.3 Czech
main results
In our Czech experiments, we considered only unla-
beled parsing,15 leaving four different parsing con-
figurations: baseline or cluster-based features and
first-order or second-order parsing. Note that our
feature sets were originally tuned for English pars-
ing, and except for the use of Czech clusters, we
made no attempt to retune our features for Czech.
Czech dependency structures may contain non-
projective edges, so we employ a maximum directed
spanning tree algorithm (Chu and Liu, 1965; Ed-
monds, 1967; McDonald et al., 2005b) as our first-
order parser for Czech. For the second-order pars-
ing experiments, we used the Carreras (2007) parser.
Since this parser only considers projective depen-
dency structures, we projectivized the PDT 1.0
training set by finding, for each sentence, the pro-
jective tree which retains the most correct dependen-
cies; our second-order parsers were then trained with
respect to these projective trees. The development
and test sets were not projectivized, so our second-
order parser is guaranteed to make errors in test sen-
tences containing non-projective dependencies. To
overcome this, McDonald and Pereira (2006) use a
15We leave labeled parsing experiments to future work.
600
dep1 dep1c dep2 dep2c
84.49 86.07(+1.58) 86.13 87.13(+1.00)
Table 4: Parent-prediction accuracies of unlabeled Czech
parsers on the PDT 1.0 test set, for baseline features and
cluster-based features. Abbreviations are as in Table 2.
Parser Accuracy
NivreandNilsson(2005) 80.1
McDonaldetal.(2005b) 84.4
HallandNovak(2005) 85.1
McDonaldandPereira(2006) 85.2
dep1c 86.07
dep2c 87.13
Table 5: Unlabeled parent-prediction accuracies of Czech
parsers on the PDT 1.0 test set, for our models and for
previous work.
Size dep1 dep1c  dep2 dep2c 
1k 72.79 73.66 0.87 74.35 74.63 0.28
2k 74.92 76.23 1.31 76.63 77.60 0.97
4k 76.87 78.14 1.27 78.34 79.34 1.00
8k 78.17 79.83 1.66 79.82 80.98 1.16
16k 80.60 82.44 1.84 82.53 83.69 1.16
32k 82.85 84.65 1.80 84.66 85.81 1.15
64k 84.20 85.98 1.78 86.01 87.11 1.10
All 84.36 86.09 1.73 86.09 87.26 1.17
Table 6: Parent-prediction accuracies of unlabeled Czech
parsers on the PDT 1.0 development set. Abbreviations
are as in Table 3.
two-stage approximate decoding process in which
the output of their second-order parser is deprojec-
tivized via greedy search. For simplicity, we did
not implement a deprojectivization stage on top of
our second-order parser, but we conjecture that such
techniques may yield some additional performance
gains; we leave this to future work.
Table 4 gives accuracy results on the PDT 1.0
test set for our unlabeled parsers. As in the En-
glish experiments, there are clear trends in the re-
sults: parsers using cluster-based features outper-
form parsers using baseline features, and second-
order parsers outperform first-order parsers. Both of
the comparisons between cluster-based and baseline
features in Table 4 are statistically significant.16 Ta-
ble 5 compares accuracy results on the PDT 1.0 test
set for our parsers and several other recent papers.
16We used the sign test at the sentence level; both compar-
isons are significant at p < 0.0005.
N dep1 dep1c dep2 dep2c
100 89.19 92.25 90.61 93.14
200 90.03 92.26 91.35 93.18
400 90.31 92.32 91.72 93.20
800 90.62 92.33 91.89 93.30
1600 90.87  92.20 
All 90.89  92.42 
Table 7: Parent-prediction accuracies of unlabeled En-
glish parsers on Section 22. Abbreviations: N = thresh-
old value; other abbreviations are as in Table 2. We
did not train cluster-based parsers using threshold values
larger than 800 due to computational limitations.
dep1-P dep1c-P dep1 dep2-P dep2c-P dep2
77.19 90.69 90.89 86.73 91.84 92.42
Table 8: Parent-prediction accuracies of unlabeled En-
glish parsers on Section 22. Abbreviations: suffix -P =
model without POS; other abbreviations are as in Table 2.
4.4 Czech
learning curves
As in our English experiments, we performed addi-
tional experiments on reduced sections of the PDT;
the results are shown in Table 6. For simplicity, we
did not retrain a tagger for each reduced dataset,
so we always use the (automatically-assigned) part
of speech tags provided in the corpus. Note that
the cluster-based features obtain improvements at all
training set sizes, with data-reduction factors simi-
lar to those observed in English. For example, the
dep1c model trained on 4k sentences is roughly as
good as the dep1 model trained on 8k sentences.
4.5 Additional
results
Here, we present two additional results which fur-
ther explore the behavior of the cluster-based fea-
ture sets. In Table 7, we show the development-set
performance of second-order parsers as the thresh-
old for lexical feature elimination (see Section 3.2)
is varied. Note that the performance of cluster-based
features is fairly insensitive to the threshold value,
whereas the performance of baseline features clearly
degrades as the vocabulary size is reduced.
In Table 8, we show the development-set perfor-
mance of the firstand second-order parsers when
features containing part-of-speech-based informa-
tion are eliminated. Note that the performance ob-
tained by using clusters without parts of speech is
close to the performance of the baseline features.
601
5 Related
Work
As mentioned earlier, our approach was inspired by
the success of Miller et al. (2004), who demon-
strated the effectiveness of using word clusters as
features in a discriminative learning approach. Our
research, however, applies this technique to depen-
dency parsing rather than named-entity recognition.
In this paper, we have focused on developing new
representations for lexical information. Previous re-
search in this area includes several models which in-
corporate hidden variables (Matsuzaki et al., 2005;
Koo and Collins, 2005; Petrov et al., 2006; Titov
and Henderson, 2007). These approaches have the
advantage that the model is able to learn different
usages for the hidden variables, depending on the
target problem at hand. Crucially, however, these
methods do not exploit unlabeled data when learn-
ing their representations.
Wang et al. (2005) used distributional similarity
scores to smooth a generative probability model for
dependency parsing and obtained improvements in
a Chinese parsing task. Our approach is similar to
theirs in that the Brown algorithm produces clusters
based on distributional similarity, and the cluster-
based features can be viewed as being a kind of
backed-off version of the baseline features. How-
ever, our work is focused on discriminative learning
as opposed to generative models.
Semi-supervised phrase structure parsing has
been previously explored by McClosky et al. (2006),
who applied a reranked parser to a large unsuper-
vised corpus in order to obtain additional train-
ing data for the parser; this self-training appraoch
was shown to be quite effective in practice. How-
ever, their approach depends on the usage of a
high-quality parse reranker, whereas the method de-
scribed here simply augments the features of an ex-
isting parser. Note that our two approaches are com-
patible in that we could also design a reranker and
apply self-training techniques on top of the cluster-
based features.
6 Conclusions
In this paper, we have presented a simple but effec-
tive semi-supervised learning approach and demon-
strated that it achieves substantial improvement over
a competitive baseline in two broad-coverage depen-
dency parsing tasks. Despite this success, there are
several ways in which our approach might be im-
proved.
To begin, recall that the Brown clustering algo-
rithm is based on a bigram language model. Intu-
itively, there is a mismatch between the kind of
lexical information that is captured by the Brown
clusters and the kind of lexical information that is
modeled in dependency parsing. A natural avenue
for further research would be the development of
clustering algorithms that reflect the syntactic be-
havior of words; e.g., an algorithm that attempts to
maximize the likelihood of a treebank, according to
a probabilistic dependency model. Alternately, one
could design clustering algorithms that cluster entire
head-modifier arcs rather than individual words.
Another idea would be to integrate the cluster-
ing algorithm into the training algorithm in a limited
fashion. For example, after training an initial parser,
one could parse a large amount of unlabeled text and
use those parses to improve the quality of the clus-
ters. These improved clusters can then be used to
retrain an improved parser, resulting in an overall
algorithm similar to that of McClosky et al. (2006).
Setting aside the development of new clustering
algorithms, a final area for future work is the exten-
sion of our method to new domains, such as con-
versational text or other languages, and new NLP
problems, such as machine translation.
Acknowledgments
The authors thank the anonymous reviewers for
their insightful comments. Many thanks also to
Percy Liang for providing his implementation of
the Brown algorithm, and Ryan McDonald for his
assistance with the experimental setup. The au-
thors gratefully acknowledge the following sources
of support. Terry Koo was funded by NSF grant
DMS-0434222 and a grant from NTT, Agmt. Dtd.
6/21/1998. Xavier Carreras was supported by the
Catalan Ministry of Innovation, Universities and
Enterprise, and a grant from NTT, Agmt. Dtd.
6/21/1998. Michael Collins was funded by NSF
grants 0347631 and DMS-0434222.
602

References

P.F. Brown, V.J. Della Pietra, P.V. deSouza, J.C. Lai, and R.L. Mercer. 1992. Class-Based n-gram Models of Natural Language. Computational Linguistics,
18(4):467479.

S. Buchholz and E. Marsi. 2006. CoNLL-X Shared Task on Multilingual Dependency Parsing. In Proceedings of CoNLL, pages 149164.
X. Carreras. 2007. Experiments with a Higher-Order Projective Dependency Parser. In Proceedings of EMNLP-CoNLL, pages 957961.

E. Charniak, D. Blaheta, N. Ge, K. Hall, and M. Johnson. 2000. BLLIP 198789 WSJ Corpus Release 1, LD No. LDC2000T43. Linguistic Data Consortium.

Y.J. Chu and T.H. Liu. 1965. On the shortest arborescence of a directed graph. Science Sinica, 14:1396 1400.

M. Collins, J. Hajic, L. Ramshaw, and C. Tillmann. 1999 A Statistical Parser for Czech. In Proceedings of ACL pages 505512.

M. Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of EMNLP, pages 18.

K. Crammer and Y. Singer. 2003. Ultraconservative Online Algorithms for Multiclass Problems. Journal of Machine Learning Research, 3:951991.

K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer. 2004. Online Passive-Aggressive Algorithms. In S. Thrun, L. Saul, and B. Scholkopf, editors, NIPS 16, pages 12291236.

J. Edmonds. 1967. Optimum branchings. Journal of Research of the National Bureau of Standards, 71B:233 240.

J. Eisner. 2000. Bilexical Grammars and Their Cubic-Time Parsing Algorithms. In H. Bunt and A. Nijholt, editors, Advances in Probabilistic and Other Parsing Technologies, pages 2962. Kluwer Academic Pulishers.

Y. Freund and R. Schapire. 1999. Large Margin Classification Using the Perceptron Algorithm. Machine Learning, 37(3):277296.

J. Hajic, E. Hajicova, P. Pajas, J. Panevova, and P. Sgall. 2001. The Prague Dependency Treebank 1.0, LDC No. LDC2001T10. Linguistics Data Consortium.

J. Hajic. 1998. Building a Syntactically Annotated Corpus: The Prague Dependency Treebank. In E. Hajicova, editor, Issues of Valency and Meaning. Studies in Honor of Jarmila Panevova, pages 1219.

K. Hall and V. Novak. 2005. Corrective Modeling for Non-Projective Dependency Parsing. In Proceedings of IWPT, pages 4252.

T. Koo and M. Collins. 2005. Hidden-Variable Models for Discriminative Reranking. In Proceedings of HLTEMNLP, pages 507514.

W. Li and A. McCallum. 2005. Semi-Supervised Sequence Modeling with Syntactic Topic Models. In Proceedings of AAAI, pages 813818.

P. Liang. 2005. Semi-Supervised Learning for Natural Language. Masters thesis, Massachusetts Institute of Technology.

M.P. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. ComputationalLinguistics, 19(2):313330.

T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilistic CFG with Latent Annotations. In Proceedings of ACL, pages 7582.

D. McClosky, E. Charniak, and M. Johnson. 2006. Effective Self-Training for Parsing. In Proceedings of HLT-NAACL, pages 152159.

R. McDonald and F. Pereira. 2006. Online Learning of Approximate Dependency Parsing Algorithms. In Proceedings of EACL, pages 8188.

R. McDonald, K. Crammer, and F. Pereira. 2005a. Online Large-Margin Training of Dependency Parsers. In Proceedings of ACL, pages 9198.

R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. 2005b. Non-Projective Dependency Parsing using Spanning Tree Algorithms. In Proceedings of HLT-EMNLP, pages 523530.

S. Miller, J. Guinness, and A. Zamanian. 2004. Name Tagging with Word Clusters and Discriminative Training. In Proceedings of HLT-NAACL, pages 337342.

J. Nivre and J. Nilsson. 2005. Pseudo-Projective Dependency Parsing. In Proceedings of ACL, pages 99106.

J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson, S. Riedel, and D. Yuret. 2007. The CoNLL 2007 Shared Task on Dependency Parsing. In Proceedings of EMNLP-CoNLL 2007, pages 915932.

S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proceedings of COLING-ACL, pages 433440.

A. Ratnaparkhi. 1996. A Maximum Entropy Model for Part-Of-Speech Tagging. In Proceedings of EMNLP, pages 133142.

I. Titov and J. Henderson. 2007. Constituent Parsing with Incremental Sigmoid Belief Networks. In Proceedings of ACL, pages 632639.

Q.I. Wang, D. Schuurmans, and D. Lin. 2005. Strictly Lexical Dependency Parsing. In Proceedings of IWPT, pages 152159.

H. Yamada and Y. Matsumoto. 2003. Statistical Dependency Analysis With Support Vector Machines. In Proceedings of IWPT, pages 195206. 603
Proceedings of ACL-08: HLT, pages 950958,
Columbus, Ohio, USA, June 2008. c2008 Association for Computational Linguistics
Integrating Graph-Based and Transition-Based Dependency Parsers
Joakim Nivre
Vaxjo University Uppsala University
Computer Science Linguistics and Philology
SE-35195 Vaxjo SE-75126 Uppsala
nivre@msi.vxu.se
Ryan McDonald
Google Inc.
76 Ninth Avenue
New York, NY 10011
ryanmcd@google.com
Abstract
Previous studies of data-driven dependency
parsing have shown that the distribution of
parsing errors are correlated with theoretical
properties of the models used for learning and
inference. In this paper, we show how these
results can be exploited to improve parsing
accuracy by integrating a graph-based and a
transition-based model. By letting one model
generate features for the other, we consistently
improve accuracy for both models, resulting
in a significant improvement of the state of
the art when evaluated on data sets from the
CoNLL-X shared task.
1 Introduction
Syntactic dependency graphs have recently gained
a wide interest in the natural language processing
community and have been used for many problems
ranging from machine translation (Ding and Palmer,
2004) to ontology construction (Snow et al., 2005).
A dependency graph for a sentence represents each
word and its syntactic dependents through labeled
directed arcs, as shown in figure 1. One advantage
of this representation is that it extends naturally to
discontinuous constructions, which arise due to long
distancedependenciesorinlanguageswheresyntac-
tic structure is encoded in morphology rather than in
word order. This is undoubtedly one of the reasons
for the emergence of dependency parsers for a wide
range of languages. Many of these parsers are based
on data-driven parsing models, which learn to pro-
duce dependency graphs for sentences solely from
an annotated corpus and can be easily ported to any
Figure 1: Dependency graph for an English sentence.
language or domain in which annotated resources
exist.
Practically all data-driven models that have been
proposed for dependency parsing in recent years can
be described as either graph-based or transition-
based (McDonald and Nivre, 2007). In graph-based
parsing, we learn a model for scoring possible de-
pendency graphs for a given sentence, typically by
factoring the graphs into their component arcs, and
perform parsing by searching for the highest-scoring
graph. This type of model has been used by, among
others, Eisner (1996), McDonald et al. (2005a), and
Nakagawa (2007). In transition-based parsing, we
instead learn a model for scoring transitions from
one parser state to the next, conditioned on the parse
history, and perform parsing by greedily taking the
highest-scoring transition out of every parser state
until we have derived a complete dependency graph.
This approach is represented, for example, by the
models of Yamada and Matsumoto (2003), Nivre et
al. (2004), and Attardi (2006).
Theoretically, these approaches are very different.
Thegraph-basedmodelsaregloballytrainedanduse
exactinferencealgorithms, butdefinefeaturesovera
limited history of parsing decisions. The transition-
based models are essentially the opposite. They use
local training and greedy inference algorithms, but
950
define features over a rich history of parsing deci-
sions. This is a fundamental trade-off that is hard
to overcome by tractable means. Both models have
been used to achieve state-of-the-art accuracy for a
wide range of languages, as shown in the CoNLL
shared tasks on dependency parsing (Buchholz and
Marsi, 2006; Nivre et al., 2007), but McDonald and
Nivre (2007) showed that a detailed error analysis
reveals important differences in the distribution of
errors associated with the two models.
In this paper, we consider a simple way of inte-
grating graph-based and transition-based models in
order to exploit their complementary strengths and
thereby improve parsing accuracy beyond what is
possible by either model in isolation. The method
integrates the two models by allowing the output
of one model to define features for the other. This
method is simple  requiring only the definition of
new features  and robust by allowing a model to
learn relative to the predictions of the other.
2 Two
Models for Dependency Parsing
2.1 Preliminaries
Given a set L = {l1,...,l|L|} of arc labels (depen-
dency relations), a dependency graph for an input
sentence x = w0,w1,...,wn (where w0 = ROOT) is
a labeled directed graph G = (V,A) consisting of a
set of nodes V = {0,1,...,n}1 and a set of labeled
directed arcs A  V V L, i.e., if (i,j,l)  A for
i,j  V and l  L, then there is an arc from node
i to node j with label l in the graph. A dependency
graphGforasentencexmustbeadirectedtreeorig-
inating out of the root node 0 and spanning all nodes
in V , as exemplified by the graph in figure 1. This
is a common constraint in many dependency parsing
theories and their implementations.
2.2 Graph-Based Models
Graph-based dependency parsers parameterize a
model over smaller substructures in order to search
the space of valid dependency graphs and produce
the most likely one. The simplest parameterization
is the arc-factored model that defines a real-valued
score function for arcs s(i,j,l) and further defines
the score of a dependency graph as the sum of the
1We use the common convention of representing words by
their index in the sentence.
score of all the arcs it contains. As a result, the de-
pendency parsing problem is written:
G = argmax
G=(V,A)
summationdisplay
(i,j,l)A
s(i,j,l)
This problem is equivalent to finding the highest
scoring directed spanning tree in the complete graph
over the input sentence, which can be solved in
O(n2) time (McDonald et al., 2005b). Additional
parameterizations are possible that take more than
one arc into account, but have varying effects on
complexity (McDonald and Satta, 2007). An advan-
tage of graph-based methods is that tractable infer-
ence enables the use of standard structured learning
techniques that globally set parameters to maximize
parsing performance on the training set (McDonald
et al., 2005a). The primary disadvantage of these
models is that scores  and as a result any feature
representations  are restricted to a single arc or a
small number of arcs in the graph.
The specific graph-based model studied in this
work is that presented by McDonald et al. (2006),
which factors scores over pairs of arcs (instead of
just single arcs) and uses near exhaustive search for
unlabeled parsing coupled with a separate classifier
to label each arc. We call this system MSTParser, or
simply MST for short, which is also the name of the
freely available implementation.2
2.3 Transition-Based Models
Transition-based dependency parsing systems use a
model parameterized over transitions of an abstract
machine for deriving dependency graphs, such that
every transition sequence from the designated initial
configuration to some terminal configuration derives
a valid dependency graph. Given a real-valued score
function s(c,t) (for transition t out of configuration
c), parsingcanbeperformedbystartingfromtheini-
tial configuration and taking the optimal transition
t = argmaxtT s(c,t) out of every configuration
c until a terminal configuration is reached. This can
be seen as a greedy search for the optimal depen-
dency graph, based on a sequence of locally optimal
decisions in terms of the transition system.
Many transition systems for data-driven depen-
dency parsing are inspired by shift-reduce parsing,
2http://mstparser.sourceforge.net
951
where each configuration c contains a stack c for
storing partially processed nodes and a buffer c
containingtheremaininginput. Transitionsinsucha
system add arcs to the dependency graph and mani-
pulate the stack and buffer. One example is the tran-
sition system defined by Nivre (2003), which parses
a sentence x = w0,w1,...,wn in O(n) time.
To learn a scoring function on transitions, these
systems rely on discriminative learning methods,
such as memory-based learning or support vector
machines, using a strictly local learning procedure
where only single transitions are scored (not com-
plete transition sequences). The main advantage of
these models is that features are not restricted to a
limited number of graph arcs but can take into ac-
count the entire dependency graph built so far. The
major disadvantage is that the greedy parsing strat-
egy may lead to error propagation.
The specific transition-based model studied in
this work is that presented by Nivre et al. (2006),
which uses support vector machines to learn transi-
tion scores. We call this system MaltParser, or Malt
for short, which is also the name of the freely avail-
able implementation.3
2.4 Comparison
and Analysis
These models differ primarily with respect to three
properties: inference, learning, and feature repre-
sentation. MaltParser uses an inference algorithm
that greedily chooses the best parsing decision based
on the current parser history whereas MSTParser
uses exhaustive search algorithms over the space of
all valid dependency graphs to find the graph that
maximizes the score. MaltParser trains a model
to make a single classification decision (choose the
next transition) whereas MSTParser trains a model
to maximize the global score of correct graphs.
MaltParsercanintroducea richfeaturehistorybased
on previous parser decisions, whereas MSTParser is
forced to restrict features to a single decision or a
pair of nearby decisions in order to retain efficiency.
These differences highlight an inherent trade-off
between global inference/learning and expressive-
ness of feature representations. MSTParser favors
theformerattheexpenseofthelatterandMaltParser
the opposite. This difference was highlighted in the
3http://w3.msi.vxu.se/jha/maltparser/
studyofMcDonaldandNivre(2007), whichshowed
that the difference is reflected directly in the error
distributions of the parsers. Thus, MaltParser is less
accurate than MSTParser for long dependencies and
those closer to the root of the graph, but more accu-
rate for short dependencies and those farthest away
from the root. Furthermore, MaltParser is more ac-
curate for dependents that are nouns and pronouns,
whereas MSTParser is more accurate for verbs, ad-
jectives, adverbs, adpositions, and conjunctions.
Given that there is a strong negative correlation
between dependency length and tree depth, and
given that nouns and pronouns tend to be more
deeply embedded than (at least) verbs and conjunc-
tions, these patterns can all be explained by the same
underlying factors. Simply put, MaltParser has an
advantage in its richer feature representations, but
this advantage is gradually diminished by the nega-
tive effect of error propagation due to the greedy in-
ference strategy as sentences and dependencies get
longer. MSTParser has a more even distribution of
errors, which is expected given that the inference al-
gorithm and feature representation should not prefer
one type of arc over another. This naturally leads
one to ask: Is it possible to integrate the two models
in order to exploit their complementary strengths?
This is the topic of the remainder of this paper.
3 Integrated
Models
There are many conceivable ways of combining the
two parsers, including more or less complex en-
semble systems and voting schemes, which only
perform the integration at parsing time. However,
given that we are dealing with data-driven models,
it should be possible to integrate at learning time, so
that the two complementary models can learn from
one another. In this paper, we propose to do this by
letting one model generate features for the other.
3.1 Feature-Based Integration
As explained in section 2, both models essentially
learn a scoring function s : X  R, where the
domain X is different for the two models. For the
graph-based model, X is the set of possible depen-
dency arcs (i,j,l); for the transition-based model,
X isthesetofpossibleconfiguration-transitionpairs
(c,t). But in both cases, the input is represented
952
MSTMalt  defined over (i,j,l) ( = any label/node)
Is (i,j,) in GMaltx ?
Is (i,j,l) in GMaltx ?
Is (i,j,) not in GMaltx ?
Is (i,j,l) not in GMaltx ?
Identity of lprime such that (,j,lprime) is in GMaltx ?
Identity of lprime such that (i,j,lprime) is in GMaltx ?
MaltMST  defined over (c,t) ( = any label/node)
Is (0c,0c,) in GMSTx ?
Is (0c,0c,) in GMSTx ?
Head direction for 0c in GMSTx (left/right/ROOT)
Head direction for 0c in GMSTx (left/right/ROOT)
Identity of l such that (,0c,l) is in GMSTx ?
Identity of l such that (,0c,l) is in GMSTx ?
Table 1: Guide features for MSTMalt and MaltMST.
by a k-dimensional feature vector f : X  Rk.
In the feature-based integration we simply extend
the feature vector for one model, called the base
model, with a certain number of features generated
by the other model, which we call the guide model
in this context. The additional features will be re-
ferred to as guide features, and the version of the
base model trained with the extended feature vector
will be called the guided model. The idea is that the
guided model should be able to learn in which situ-
ations to trust the guide features, in order to exploit
the complementary strength of the guide model, so
that performance can be improved with respect to
the base parser. This method of combining classi-
fiers is sometimes referred to as classifier stacking.
The exact form of the guide features depend on
properties of the base model and will be discussed
in sections 3.23.3 below, but the overall scheme for
thefeature-based integrationcanbedescribed asfol-
lows. To train a guided version BC of base model B
with guide model C and training set T, the guided
model is trained, not on the original training set T,
but on a version of T that has been parsed with the
guide model C under a cross-validation scheme (to
avoid overlap with training data for C). This means
that, for every sentence x  T, BC has access at
training time to both the gold standard dependency
graph Gx and the graph GCx predicted by C, and it is
the latter that forms the basis for the additional guide
features. When parsing a new sentence xprime with BC,
xprime is first parsed with model C (this time trained on
the entire training set T) to derive GCxprime, so that the
guide features can be extracted also at parsing time.
3.2 The
Guided Graph-Based Model
The graph-based model, MSTParser, learns a scor-
ing function s(i,j,l)  R over labeled dependen-
cies. More precisely, dependency arcs (or pairs of
arcs) are first represented by a high dimensional fea-
ture vector f(i,j,l)  Rk, where f is typically a bi-
nary feature vector over properties of the arc as well
as the surrounding input (McDonald et al., 2005a;
McDonald et al., 2006). The score of an arc is de-
fined as a linear classifier s(i,j,l) = w  f(i,j,l),
where w is a vector of feature weights to be learned
by the model.
For the guided graph-based model, which we call
MSTMalt, this feature representation is modified to
include an additional argument GMaltx , which is the
dependency graph predicted by MaltParser on the
input sentence x. Thus, the new feature represen-
tation will map an arc and the entire predicted Malt-
Parser graph to a high dimensional feature repre-
sentation, f(i,j,l,GMaltx )  Rk+m. These m ad-
ditional features account for the guide features over
the MaltParser output. The specific features used by
MSTMalt are given in table 1. All features are con-
joined with the part-of-speech tags of the words in-
volved in the dependency to allow the guided parser
to learn weights relative to different surface syntac-
tic environments. Though MSTParser is capable of
defining features over pairs of arcs, we restrict the
guide features over single arcs as this resulted in
higher accuracies during preliminary experiments.
3.3 The
Guided Transition-Based Model
The transition-based model, MaltParser, learns a
scoring function s(c,t)  Rover configurations and
transitions. The set of training instances for this
learning problem is the set of pairs (c,t) such that
t is the correct transition out of c in the transition
sequence that derives the correct dependency graph
Gx for some sentence x in the training set T. Each
training instance (c,t) is represented by a feature
vector f(c,t)  Rk, where features are defined in
terms of arbitrary properties of the configuration c,
including the state of the stack c, the input buffer
c, and the partially built dependency graph Gc. In
particular, many features involve properties of the
two target tokens, the token on top of the stack c
(0c) and the first token in the input buffer c (0c),
953
which are the two tokens that may become con-
nected by a dependency arc through the transition
out of c. The full set of features used by the base
model MaltParser is described in Nivre et al. (2006).
For the guided transition-based model, which we
call MaltMST, training instances are extended to
triples (c,t,GMSTx ), where GMSTx is the dependency
graph predicted by the graph-based MSTParser for
the sentence x to which the configuration c belongs.
We define m additional guide features, based on
properties of GMSTx , and extend the feature vector
accordingly to f(c,t,GMSTx )  Rk+m. The specific
features used by MaltMST are given in table 1. Un-
like MSTParser, features are not explicitly defined
to conjoin guide features with part-of-speech fea-
tures. These features are implicitly added through
the polynomial kernel used to train the SVM.
4 Experiments
In this section, we present an experimental evalua-
tion of the two guided models based on data from
the CoNLL-X shared task, followed by a compar-
ative error analysis including both the base models
and the guided models. The data for the experiments
are training and test sets for all thirteen languages
from the CoNLL-X shared task on multilingual de-
pendency parsing with training sets ranging in size
from from 29,000 tokens (Slovene) to 1,249,000 to-
kens (Czech). The test sets are all standardized to
about 5,000 tokens each. For more information on
the data sets, see Buchholz and Marsi (2006).
The guided models were trained according to the
scheme explained in section 3, with two-fold cross-
validation when parsing the training data with the
guide parsers. Preliminary experiments suggested
that cross-validation with more folds had a negli-
gible impact on the results. Models are evaluated
by their labeled attachment score (LAS) on the test
set, i.e., the percentage of tokens that are assigned
both the correct head and the correct label, using
the evaluation software from the CoNLL-X shared
task with default settings.4 Statistical significance
was assessed using Dan Bikels randomized pars-
ing evaluation comparator with the default setting of
10,000 iterations.5
4http://nextens.uvt.nl/conll/software.html
5http://www.cis.upenn.edu/dbikel/software.html
Language MST MSTMalt Malt MaltMST
Arabic 66.91 68.64 (+1.73) 66.71 67.80 (+1.09)
Bulgarian 87.57 89.05 (+1.48) 87.41 88.59 (+1.18)
Chinese 85.90 88.43 (+2.53) 86.92 87.44 (+0.52)
Czech 80.18 82.26 (+2.08) 78.42 81.18 (+2.76)
Danish 84.79 86.67 (+1.88) 84.77 85.43 (+0.66)
Dutch 79.19 81.63 (+2.44) 78.59 79.91 (+1.32)
German 87.34 88.46 (+1.12) 85.82 87.66 (+1.84)
Japanese 90.71 91.43 (+0.72) 91.65 92.20 (+0.55)
Portuguese 86.82 87.50 (+0.68) 87.60 88.64 (+1.04)
Slovene 73.44 75.94 (+2.50) 70.30 74.24 (+3.94)
Spanish 82.25 83.99 (+1.74) 81.29 82.41 (+1.12)
Swedish 82.55 84.66 (+2.11) 84.58 84.31 (0.27)
Turkish 63.19 64.29 (+1.10) 65.58 66.28 (+0.70)
Average 80.83 82.53 (+1.70) 80.74 82.01 (+1.27)
Table 2: Labeled attachment scores for base parsers and
guided parsers (improvement in percentage points).
10 20 30 40 50 60
Sentence Length
0.7
0.75
0.8
0.85
0.9
Accuracy
Malt
MST
Malt+MST
MST+Malt
Figure 2: Accuracy relative to sentence length.
4.1 Results
Table 2 shows the results, for each language and on
average, for the two base models (MST, Malt) and
for the two guided models (MSTMalt, MaltMST).
First of all, we see that both guided models show
a very consistent increase in accuracy compared to
their base model, even though the extent of the im-
provement varies across languages from about half
a percentage point (MaltMST on Chinese) up to al-
most four percentage points (MaltMST on Slovene).6
It is thus quite clear that both models have the capa-
city to learn from features generated by the other
model. However, it is also clear that the graph-based
MST model shows a somewhat larger improvement,
both on average and for all languages except Czech,
6The only exception to this pattern is the result for MaltMST
on Swedish, where we see an unexpected drop in accuracy com-
pared to the base model.
954
2 4 6 8 10 12      14   15+
Dependency Length
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
Recall
Malt
MST
Malt+MST
MST+Malt
2 4 6 8 10 12      14   15+
Dependency Length
0.5
0.6
0.65
0.7
0.75
0.8
0.85
Precision
Malt
MST
Malt+MST
MST+Malt
1 2 3 4 5 6 7+
Distance to Root
0.8
0.82
0.84
0.86
0.8
0.9
Recall
Malt
MST
Malt+MST
MST+Malt
1 2 3 4 5 6 7+
Distance to Root
0.78
0.8
0.82
0.84
0.86
0.8
0.9
0.92
Precision
Malt
MST
Malt+MST
MST+Malt
(a) (b)
Figure 3: Dependency arc precision/recall relative to predicted/gold for (a) dependency length and (b) distance to root.
German, Portuguese and Slovene. Finally, given
that the two base models had the previously best
performance for these data sets, the guided models
achieve a substantial improvement of the state of the
art. While there is no statistically significant differ-
ence between the two base models, they are both
outperformed by MaltMST (p < 0.0001), which in
turn has significantly lower accuracy than MSTMalt
(p < 0.0005).
Anextensiontothemodelsdescribedsofarwould
be to iteratively integrate the two parsers in the
spirit of pipeline iteration (Hollingshead and Roark,
2007). For example, one could start with a Malt
model, use it to train a guided MSTMalt model, then
use that as the guide to train a MaltMSTMalt model,
etc. We ran such experiments, but found that accu-
racy did not increase significantly and in some cases
decreasedslightly. Thiswastrueregardlessofwhich
parser began the iterative process. In retrospect, this
result is not surprising. Since the initial integration
effectively incorporates knowledge from both pars-
ing systems, there is little to be gained by adding
additional parsers in the chain.
4.2 Analysis
The experimental results presented so far show that
feature-based integration is a viable approach for
improving the accuracy of both graph-based and
transition-based models for dependency parsing, but
theysayverylittleabouthowtheintegrationbenefits
the two models and what aspects of the parsing pro-
cess are improved as a result. In order to get a better
understanding of these matters, we replicate parts of
the error analysis presented by McDonald and Nivre
(2007), where parsing errors are related to different
structural properties of sentences and their depen-
dency graphs. For each of the four models evalu-
ated, we compute error statistics for labeled attach-
ment over all twelve languages together.
Figure 2 shows accuracy in relation to sentence
length, binned into ten-word intervals (110, 11-20,
etc.). As expected, Malt and MST have very simi-
lar accuracy for short sentences but Malt degrades
more rapidly with increasing sentence length be-
cause of error propagation (McDonald and Nivre,
2007). The guided models, MaltMST and MSTMalt,
behave in a very similar fashion with respect to each
other but both outperform their base parser over the
entire range of sentence lengths. However, except
for the two extreme data points (010 and 5160)
there is also a slight tendency for MaltMST to im-
prove more for longer sentences and for MSTMalt to
improve more for short sentences, which indicates
that the feature-based integration allows one parser
to exploit the strength of the other.
Figure 3(a) plots precision (top) and recall (bot-
tom) for dependency arcs of different lengths (pre-
dicted arcs for precision, gold standard arcs for re-
call). With respect to recall, the guided models ap-
pear to have a slight advantage over the base mod-
955
Part of Speech MST MSTMalt Malt MaltMST
Verb 82.6 85.1 (2.5) 81.9 84.3 (2.4)
Noun 80.0 81.7 (1.7) 80.7 81.9 (1.2)
Pronoun 88.4 89.4 (1.0) 89.2 89.3 (0.1)
Adjective 89.1 89.6 (0.5) 87.9 89.0 (1.1)
Adverb 78.3 79.6 (1.3) 77.4 78.1 (0.7)
Adposition 69.9 71.5 (1.6) 68.8 70.7 (1.9)
Conjunction 73.1 74.9 (1.8) 69.8 72.5 (2.7)
Table 3: Accuracy relative to dependent part of speech
(improvement in percentage points).
els for short and medium distance arcs. With re-
spect to precision, however, there are two clear pat-
terns. First, the graph-based models have better pre-
cision than the transition-based models when pre-
dicting long arcs, which is compatible with the re-
sults of McDonald and Nivre (2007). Secondly, both
the guided models have better precision than their
base model and, for the most part, also their guide
model. InparticularMSTMalt outperformsMSTand
is comparable to Malt for short arcs. More inter-
estingly, MaltMST outperforms both Malt and MST
for arcs up to length 9, which provides evidence that
MaltMST has learned specifically to trust the guide
features from MST for longer dependencies. The
reasonthataccuracydoesnotimprovefordependen-
cies of length greater than 9 is probably that these
dependencies are too rare for MaltMST to learn from
the guide parser in these situations.
Figure 3(b) shows precision (top) and recall (bot-
tom) for dependency arcs at different distances from
the root (predicted arcs for precision, gold standard
arcs for recall). Again, we find the clearest pat-
terns in the graphs for precision, where Malt has
very low precision near the root but improves with
increasing depth, while MST shows the opposite
trend (McDonald and Nivre, 2007). Considering
the guided models, it is clear that MaltMST im-
proves in the direction of its guide model, with a
5-point increase in precision for dependents of the
root and smaller improvements for longer distances.
Similarly, MSTMalt improves precision in the range
where its base parser is inferior to Malt and for dis-
tances up to 4 has an accuracy comparable to or
higher than its guide parser Malt. This again pro-
vides evidence that the guided parsers are learning
from their guide models.
Table 3 gives the accuracy for arcs relative to de-
pendent part-of-speech. As expected, we see that
MST does better than Malt for all categories except
nouns and pronouns (McDonald and Nivre, 2007).
But we also see that the guided models in all cases
improve over their base parser and, in most cases,
also over their guide parser. The general trend is that
MSTimprovesmorethanMalt, exceptforadjectives
and conjunctions, where Malt has a greater disad-
vantage from the start and therefore benefits more
from the guide features.
Consideringtheresultsforpartsofspeech, aswell
as those for dependency length and root distance, it
is interesting to note that the guided models often
improve even in situations where their base parsers
are more accurate than their guide models. This sug-
gests that the improvement is not a simple function
of the raw accuracy of the guide model but depends
on the fact that labeled dependency decisions inter-
act in inference algorithms for both graph-based and
transition-based parsing systems. Thus, if a parser
can improve its accuracy on one class of dependen-
cies, e.g., longer ones, then we can expect to see im-
provements on all types of dependencies  as we do.
The interaction between different decisions may
also be part of the explanation why MST benefits
more from the feature-based integration than Malt,
with significantly higher accuracy for MSTMalt than
for MaltMST as a result. Since inference is global
(or practically global) in the graph-based model,
an improvement in one type of dependency has a
good chance of influencing the accuracy of other de-
pendencies, whereas in the transition-based model,
where inference is greedy, some of these additional
benefits will be lost because of error propagation.
This is reflected in the error analysis in the following
recurrent pattern: Where Malt does well, MaltMST
does only slightly better. But where MST is good,
MSTMalt is often significantly better.
Another part of the explanation may have to do
with the learning algorithms used by the systems.
Although both Malt and MST use discriminative
algorithms, Malt uses a batch learning algorithm
(SVM) and MST uses an online learning algorithm
(MIRA). If the original rich feature representation
of Malt is sufficient to separate the training data,
regularization may force the weights of the guided
features to be small (since they are not needed at
training time). On the other hand, an online learn-
956
ing algorithm will recognize the guided features as
strong indicators early in training and give them a
high weight as a result. Features with high weight
early in training tend to have the most impact on the
final classifier due to both weight regularization and
averaging. This is in fact observed when inspecting
the weights of MSTMalt.
5 Related
Work
Combinations of graph-based and transition-based
models for data-driven dependency parsing have
previously been explored by Sagae and Lavie
(2006), who report improvements of up to 1.7 per-
centage points over the best single parser when
combining three transition-based models and one
graph-based model for unlabeled dependency pars-
ing, evaluated on data from the Penn Treebank. The
combined parsing model is essentially an instance of
the graph-based model, where arc scores are derived
from the output of the different component parsers.
Unlike the models presented here, integration takes
place only at parsing time, not at learning time, and
requires at least three different base parsers. The
same technique was used by Hall et al. (2007) to
combine six transition-based parsers in the best per-
forming system in the CoNLL 2007 shared task.
Feature-based integration in the sense of letting a
subset of the features for one model be derived from
the output of a different model has been exploited
for dependency parsing by McDonald (2006), who
trained an instance of MSTParser using features
generated by the parsers of Collins (1999) and Char-
niak (2000), which improved unlabeled accuracy by
1.7 percentage points, again on data from the Penn
Treebank. In addition, feature-based integration has
been used by Taskar et al. (2005), who trained a
discriminative word alignment model using features
derived from the IBM models, and by Florian et al.(2004), who trained classifiers on auxiliary data to
guide named entity classifiers.
Feature-based integration also has points in com-
mon with co-training, which have been applied to
syntactic parsing by Sarkar (2001) and Steedman et
al. (2003), among others. The difference, of course,
is that standard co-training is a weakly supervised
method, where guide features replace, rather than
complement, the gold standard annotation during
training. Feature-based integration is also similar to
parse re-ranking (Collins, 2000), where one parser
produces a set of candidate parses and a second-
stage classifier chooses the most likely one. How-
ever, feature-based integration is not explicitly con-
strained to any parse decisions that the guide model
might make and only the single most likely parse is
used from the guide model, making it significantly
more efficient than re-ranking.
Finally, there are several recent developments in
data-driven dependency parsing, which can be seen
as targeting the specific weaknesses of graph-based
and transition-based models, respectively, though
without integrating the two models. Thus, Naka-
gawa (2007) and Hall (2007) both try to overcome
the limited feature scope of graph-based models by
adding global features, in the former case using
Gibbs sampling to deal with the intractable infer-
ence problem, in the latter case using a re-ranking
scheme. For transition-based models, the trend is
to alleviate error propagation by abandoning greedy,
deterministic inference in favor of beam search with
globally normalized models for scoring transition
sequences, either generative (Titov and Henderson,
2007a; Titov and Henderson, 2007b) or conditional
(Duan et al., 2007; Johansson and Nugues, 2007).
6 Conclusion
In this paper, we have demonstrated how the two
dominant approaches to data-driven dependency
parsing, graph-based models and transition-based
models, can be integrated by letting one model learn
from features generated by the other. Our experi-
mental results show that both models consistently
improve their accuracy when given access to fea-
tures generated by the other model, which leads to
a significant advancement of the state of the art in
data-driven dependency parsing. Moreover, a com-
parative error analysis reveals that the improvements
are largely predictable from theoretical properties of
the two models, in particular the tradeoff between
global learning and inference, on the one hand, and
rich feature representations, on the other. Directions
for future research include a more detailed analysis
of the effect of feature-based integration, as well as
the exploration of other strategies for integrating dif-
ferent parsing models.
957

References

Giuseppe Attardi. 2006. Experiments with a multilanguage non-projective dependency parser. In Proceedings of CoNLL, pages 166170.

Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of CoNLL, pages 149164.

Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of NAACL, pages 132139.

Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.

Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proceedings of ICML, pages 175182.

Yuan Ding and Martha Palmer. 2004. Synchronous dependency insertion grammars: A grammar formalism for syntax based statistical MT. In Proceedings of the Workshop on Recent Advances in Dependency Grammar, pages 9097.

Xiangyu Duan, Jun Zhao, and Bo Xu. 2007. Probabilistic parsing action models for multi-lingual dependency parsing. In Proceedings of EMNLP-CoNLL, pages 940946.

Jason M. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of COLING, pages 340345.

Radu Florian, Hany Hassan, Abraham Ittycheriah, Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo, Nicolas Nicolov, and Salim Roukos. 2004. A statistical model for multilingual entity detection and tracking. In Proceedings of NAACL/HLT.

Johan Hall, Jens Nilsson, Joakim Nivre, Gulsen EryigitBeata Megyesi, Mattias Nilsson, and Markus Saers. 2007. Single malt or blended? A study in multilingual parser optimization. In Proceedings of EMNLP-CoNLL.

Keith Hall. 2007. K-best spanning tree parsing. In Proceedings of ACL, pages 392399.

Kristy Hollingshead and Brian Roark. 2007. Pipeline iteration. In Proceedings of ACL, pages 952959.

Richard Johansson and Pierre Nugues. 2007. Incremental dependency parsing using online learning. In Proceedings of EMNLP-CoNLL, pages 11341138.

Ryan McDonald and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsing mod-els. In Proceedings of EMNLP-CoNLL, pages 122131.

Ryan McDonald and Giorgio Satta. 2007. On the complexityofnon-projectivedata-drivendependencyparsing. In Proceedings of IWPT, pages 122131.

Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005a. Online large-margin training of dependency parsers. In Proceedings of ACL, pages 9198.

Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of HLT/EMNLP, pages 523530.

Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006. Multilingual dependency analysis with a two-stage discriminative parser. In Proceedings of CoNLL, pages 216220.

Ryan McDonald. 2006. Discriminative Learning and Spanning Tree Algorithms for Dependency Parsing. Ph.D. thesis, University of Pennsylvania.

Tetsuji Nakagawa. 2007. Multilingual dependency parsing using global features. In Proceedings of EMNLP-CoNLL, pages 952956.

Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In Proceedings of CoNLL, pages 4956.

Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigitand Svetoslav Marinov. 2006. Labeled pseudo-projective dependency parsing with support vector machines. In Proceedings of CoNLL, pages 221225.

Joakim Nivre, Johan Hall, Sandra Kubler, Ryan McDoald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of EMNLP-CoNLL, pages 915932.

Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of IWPT, pages 149160.

Kenji Sagae and Alon Lavie. 2006. Parser combination byreparsing. InProceedingsofNAACL:ShortPapers, pages 129132.

Anoop Sarkar. 2001. Applying co-training methods to statistical parsing. In Proceedings of NAACL, pages 175182.

Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. In Proceedings of NIPS.

Mark Steedman, Rebecca Hwa, Miles Osborne, and Anoop Sarkar. 2003. Corrected co-training for statistical parsers. In Proceedings of ICML, pages 95102.

Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A discriminative matching approach to word alignment. In Proceedings of HLT/EMNLP, pages 7380.

Ivan Titov and James Henderson. 2007a. Fast and robust multilingual dependency parsing with a generative latent variable model. In Proceedings of EMNLP-CoNLL, pages 947951.

Ivan Titov and James Henderson. 2007b. A latent variable model for generative dependency parsing. In Proceedings of IWPT, pages 144155.

Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of IWPT, pages 195206
Proceedings of ACL-08: HLT, pages 959967,
Columbus, Ohio, USA, June 2008. c2008 Association for Computational Linguistics
Efficient, Feature-based, Conditional Random Field Parsing
Jenny Rose Finkel, Alex Kleeman, Christopher D. Manning
Department of Computer Science
Stanford University
Stanford, CA 94305
jrfinkel@cs.stanford.edu, akleeman@stanford.edu, manning@cs.stanford.edu
Abstract
Discriminative feature-based methods are
widely used in natural language processing,
but sentence parsing is still dominated by gen-
erative methods. While prior feature-based
dynamic programming parsers have restricted
training and evaluation to artificially short sen-
tences, we present the first general, feature-
rich discriminative parser, based on a condi-
tional random field model, which has been
successfully scaled to the full WSJ parsing
data. Our efficiency is primarily due to the
use of stochastic optimization techniques, as
well as parallelization and chart prefiltering.
On WSJ15, we attain a state-of-the-art F-score
of 90.9%, a 14% relative reduction in error
over previous models, while being two orders
of magnitude faster. On sentences of length
40, our system achieves an F-score of 89.0%,
a 36% relative reduction in error over a gener-
ative baseline.
1 Introduction
Over the past decade, feature-based discriminative
models have become the tool of choice for many
natural language processing tasks. Although they
take much longer to train than generative models,
they typically produce higher performing systems,
in large part due to the ability to incorporate ar-
bitrary, potentially overlapping features. However,
constituency parsing remains an area dominated by
generative methods, due to the computational com-
plexity of the problem. Previous work on discrim-
inative parsing falls under one of three approaches.
One approach does discriminative reranking of the
n-best list of a generative parser, still usually de-
pending highly on the generative parser score as
a feature (Collins, 2000; Charniak and Johnson,
2005). A second group of papers does parsing by a
sequence of independent, discriminative decisions,
either greedily or with use of a small beam (Ratna-
parkhi, 1997; Henderson, 2004). This paper extends
the third thread of work, where joint inference via
dynamic programming algorithms is used to train
models and to attempt to find the globally best parse.
Work in this context has mainly been limited to use
of artificially short sentences due to exorbitant train-
ing and inference times. One exception is the re-
cent work of Petrov et al. (2007), who discrimina-
tively train a grammar with latent variables and do
not restrict themselves to short sentences. However
their model, like the discriminative parser of John-
son (2001), makes no use of features, and effectively
ignores the largest advantage of discriminative train-
ing. It has been shown on other NLP tasks that mod-
eling improvements, such as the switch from gen-
erative training to discriminative training, usually
provide much smaller performance gains than the
gains possible from good feature engineering. For
example, in (Lafferty et al., 2001), when switching
from a generatively trained hidden Markov model
(HMM) to a discriminatively trained, linear chain,
conditional random field (CRF) for part-of-speech
tagging, their error drops from 5.7% to 5.6%. When
they add in only a small set of orthographic fea-
tures, their CRF error rate drops considerably more
to 4.3%, and their out-of-vocabulary error rate drops
by more than half. This is further supported by John-
son (2001), who saw no parsing gains when switch-
959
ing from generative to discriminative training, and
by Petrov et al. (2007) who saw only small gains of
around 0.7% for their final model when switching
training methods.
In this work, we provide just such a framework for
training a feature-rich discriminative parser. Unlike
previous work, we do not restrict ourselves to short
sentences, but we do provide results both for training
and testing on sentences of length  15 (WSJ15) and
for training and testing on sentences of length  40,
allowing previous WSJ15 results to be put in context
with respect to most modern parsing literature. Our
model is a conditional random field based model.
For a rule application, we allow arbitrary features
to be defined over the rule categories, span and split
point indices, and the words of the sentence. It is
well known that constituent length influences parse
probability, but PCFGs cannot easily take this infor-
mation into account. Another benefit of our feature
based model is that it effortlessly allows smooth-
ing over previously unseen rules. While the rule
may be novel, it will likely contain features which
are not. Practicality comes from three sources. We
made use of stochastic optimization methods which
allow us to find optimal model parameters with very
few passes through the data. We found no differ-
ence in parser performance between using stochastic
gradient descent (SGD), and the more common, but
significantly slower, L-BFGS. We also used limited
parallelization, and prefiltering of the chart to avoid
scoring rules which cannot tile into complete parses
of the sentence. This speed-up does not come with a
performance cost; we attain an F-score of 90.9%, a
14% relative reduction in errors over previous work
on WSJ15.
2 The
Model
2.1 A
Conditional Random Field Context Free
Grammar (CRF-CFG)
Our parsing model is based on a conditional ran-
dom field model, however, unlike previous TreeCRF
work, e.g., (Cohn and Blunsom, 2005; Jousse et al.,
2006), we do not assume a particular tree structure,
and instead find the most likely structure and la-
beling. This is similar to conventional probabilis-
tic context-free grammar (PCFG) parsing, with two
exceptions: (a) we maximize conditional likelihood
of the parse tree, given the sentence, not joint like-
lihood of the tree and sentence; and (b) probabil-
ities are normalized globally instead of locally 
the graphical models depiction of our trees is undi-
rected.
Formally, we have a CFG G, which consists of
(Manning and Schutze, 1999): (i) a set of termi-
nals {wk},k = 1,...,V ; (ii) a set of nonterminals
{Nk},k = 1,...,n; (iii) a designated start symbol
ROOT ; and (iv) a set of rules, { = Ni   j}, where
 j is a sequence of terminals and nonterminals. A
PCFG additionally assigns probabilities to each rule
 such that ij P(Ni   j) = 1. Our conditional
random field CFG (CRF-CFG) instead defines local
clique potentials (r|s;), where s is the sentence,
and r contains a one-level subtree of a tree t, corre-
sponding to a rule , along with relevant information
about the span of words which it encompasses, and,
if applicable, the split position (see Figure 1). These
potentials are relative to the sentence, unlike a PCFG
where rule scores do not have access to words at the
leaves of the tree, or even how many words they
dominate. We then define a conditional probabil-
ity distribution over entire trees, using the standard
CRF distribution, shown in (1). There is, however,
an important subtlety lurking in how we define the
partition function. The partition function Zs, which
makes the probability of all possible parses sum to
unity, is defined over all structures as well as all la-
belings of those structures. We define (s) to be the
set of all possible parse trees for the given sentence
licensed by the grammar G.
P(t|s;) = 1Z
s rt
(r|s;) (1)
where
Zs = t(s)rt (r|s;)
The above model is not well-defined over all
CFGs. Unary rules of the form Ni  N j can form
cycles, leading to infinite unary chains with infinite
mass. However, it is standard in the parsing liter-
ature to transform grammars into a restricted class
of CFGs so as to permit efficient parsing. Binariza-
tion of rules (Earley, 1970) is necessary to obtain
cubic parsing time, and closure of unary chains is re-
quired for finding total probability mass (rather than
just best parses) (Stolcke, 1995). To address this is-
sue, we define our model over a restricted class of
960
S
NP
NN
Factory
NNS
payrolls
VP
VBD
fell
PP
IN
in
NN
September
Phrasal rules
r1 = S0,5  NP0,2 VP2,5 | Factory payrolls fell in September
r3 = VP2,5  VBD2,3 PP3,5 | Factory payrolls fell in September
. . .
Lexicon rules
r5 = NN0,1  Factory | Factory payrolls fell in September
r6 = NNS1,2  payrolls | Factory payrolls fell in September
. . .
(a) PCFG Structure (b) Rules r
Figure 1: A parse tree and the corresponding rules over which potentials and features are defined.
CFGs which limits unary chains to not have any re-
peated states. This was done by collapsing all al-
lowed unary chains to single unary rules, and dis-
allowing multiple unary rule applications over the
same span.1 We give the details of our binarization
scheme in Section 5. Note that there exists a gram-
mar in this class which is weakly equivalent with any
arbitrary CFG.
2.2 Computing
the Objective Function
Our clique potentials take an exponential form. We
have a feature function, represented by f(r,s), which
returns a vector with the value for each feature. We
denote the value of feature fi by fi(r,s) and our
model has a corresponding parameter i for each
feature. The clique potential function is then:
(r|s;) = expi i fi(r,s) (2)
The log conditional likelihood of the training data
D, with an additional L2 regularization term, is then:
L(D;) =parenleftBigg

(t,s)D
parenleftbigg

rt

i
i fi(r,s)
parenrightbigg
Zs
parenrightBigg
+
i
2i
2 2 (3)
And the partial derivatives of the log likelihood, with
respect to the model weights are, as usual, the dif-
ference between the empirical counts and the model
expectations:
L
i =
parenleftBigg

(t,s)D
parenleftbigg

rt
fi(r,s)
parenrightbigg
E[fi|s]
parenrightBigg
+ i 2 (4)
1In our implementation of the inside-outside algorithm, we
then need to keep two inside and outside scores for each span:
one from before and one from after the application of unary
rules.
The partition function Zs and the partial derivatives
can be efficiently computed with the help of the
inside-outside algorithm.2 Zs is equal to the in-
side score of ROOT over the span of the entire sen-
tence. To compute the partial derivatives, we walk
through each rule, and span/split, and add the out-
side log-score of the parent, the inside log-score(s)
of the child(ren), and the log-score for that rule and
span/split. Zs is subtracted from this value to get the
normalized log probability of that rule in that posi-
tion. Using the probabilities of each rule applica-
tion, over each span/split, we can compute the ex-
pected feature values (the second term in Equation
4), by multiplying this probability by the value of
the feature corresponding to the weight for which we
are computing the partial derivative. The process is
analogous to the computation of partial derivatives
in linear chain CRFs. The complexity of the algo-
rithm for a particular sentence is O(n3), where n is
the length of the sentence.
2.3 Parallelization
Unlike (Taskar et al., 2004), our algorithm has the
advantage of being easily parallelized (see footnote
7 in their paper). Because the computation of both
the log likelihood and the partial derivatives involves
summing over each tree individually, the compu-
tation can be parallelized by having many clients
which each do the computation for one tree, and one
central server which aggregates the information to
compute the relevant information for a set of trees.
Because we use a stochastic optimization method,
as discussed in Section 3, we compute the objec-
tive for only a small portion of the training data at
a time, typically between 15 and 30 sentences. In
2In our case the values in the chart are the clique potentials
which are non-negative numbers, but not probabilities.
961
this case the gains from adding additional clients
decrease rapidly, because the computation time is
dominated by the longest sentences in the batch.
2.4 Chart
Prefiltering
Training is also sped up by prefiltering the chart. On
the inside pass of the algorithm one will see many
rules which cannot actually be tiled into complete
parses. In standard PCFG parsing it is not worth fig-
uring out which rules are viable at a particular chart
position and which are not. In our case however this
can make a big difference.We are not just looking
up a score for the rule, but must compute all the fea-
tures, and dot product them with the feature weights,
which is far more time consuming. We also have to
do an outside pass as well as an inside one, which
is sped up by not considering impossible rule appli-
cations. Lastly, we iterate through the data multi-
ple times, so if we can compute this information just
once, we will save time on all subsequent iterations
on that sentence. We do this by doing an inside-
outside pass that is just boolean valued to determine
which rules are possible at which positions in the
chart. We simultaneously compute the features for
the possible rules and then save the entire data struc-
ture to disk. For all but the shortest of sentences,
the disk I/O is easily worth the time compared to re-
computation. The first time we see a sentence this
method is still about one third faster than if we did
not do the prefiltering, and on subsequent iterations
the improvement is closer to tenfold.
3 Stochastic
Optimization Methods
Stochastic optimization methods have proven to be
extremely efficient for the training of models involv-
ing computationally expensive objective functions
like those encountered with our task (Vishwanathan
et al., 2006) and, in fact, the on-line backpropagation
learning used in the neural network parser of Hen-
derson (2004) is a form of stochastic gradient de-
scent. Standard deterministic optimization routines
such as L-BFGS (Liu and Nocedal, 1989) make little
progress in the initial iterations, often requiring sev-
eral passes through the data in order to satisfy suffi-
cient descent conditions placed on line searches. In
our experiments SGD converged to a lower objective
function value than L-BFGS, however it required far
0 5 10 15 20 25 30 35 40 45 503.5
3
2.5
2
1.5
1
0.5
0 x 10
5
Passes
Log Likelihood
SGD
LBFGS
Figure 2: WSJ15 objective value for L-BFGS and SGD
versus passes through the data. SGD ultimately con-
verges to a lower objective value, but does equally well
on test data.
fewer iterations (see Figure 2) and achieved compa-
rable test set performance to L-BFGS in a fraction of
the time. One early experiment on WSJ15 showed a
seven time speed up.
3.1 Stochastic
Function Evaluation
Utilization of stochastic optimization routines re-
quires the implementation of a stochastic objective
function. This function, L is designed to approx-
imate the true function L based off a small subset
of the training data represented by Db. Here b, the
batch size, means that Db is created by drawing b
training examples, with replacement, from the train-
ing set D. With this notation we can express the
stochastic evaluation of the function as L(Db;).
This stochastic function must be designed to ensure
that:
E
bracketleftBig
ni L(D(i)b ;)
bracketrightBig
=L(D;)
Note that this property is satisfied, without scaling,
for objective functions that sum over the training
data, as it is in our case, but any priors must be
scaled down by a factor of b/|D|. The stochastic
gradient, L(D(i)b ;), is then simply the derivative
of the stochastic function value.
3.2 Stochastic
Gradient Descent
SGD was implemented using the standard update:
k+1 = k kL(D(k)b ;k)
962
And employed a gain schedule in the form
k = 0  +k
where parameter  was adjusted such that the gain is
halved after five passes through the data. We found
that an initial gain of 0 = 0.1 and batch size be-
tween 15 and 30 was optimal for this application.
4 Features
As discussed in Section 5 we performed experi-
ments on both sentences of length  15 and length
 40. All feature development was done on the
length 15 corpus, due to the substantially faster
train and test times. This has the unfortunate effect
that our features are optimized for shorter sentences
and less training data, but we found development
on the longer sentences to be infeasible. Our fea-
tures are divided into two types: lexicon features,
which are over words and tags, and grammar fea-
tures which are over the local subtrees and corre-
sponding span/split (both have access to the entire
sentence). We ran two kinds of experiments: a dis-
criminatively trained model, which used only the
rules and no other grammar features, and a feature-
based model which did make use of grammar fea-
tures. Both models had access to the lexicon fea-
tures. We viewed this as equivalent to the more
elaborate, smoothed unknown word models that are
common in many PCFG parsers, such as (Klein and
Manning, 2003; Petrov et al., 2006).
We preprocessed the words in the sentences to ob-
tain two extra pieces of information. Firstly, each
word is annotated with a distributional similarity tag,
from a distributional similarity model (Clark, 2000)
trained on 100 million words from the British Na-
tional Corpus and English Gigaword corpus. Sec-
ondly, we compute a class for each word based on
the unknown word model of Klein and Manning
(2003); this model takes into account capitaliza-
tion, digits, dashes, and other character-level fea-
tures. The full set of features, along with an expla-
nation of our notation, is listed in Table 1.
5 Experiments
For all experiments, we trained and tested on the
Penn treebank (PTB) (Marcus et al., 1993). We used
Binary Unary
Model States Rules Rules
WSJ15 1,428 5,818 423
WSJ15 relaxed 1,428 22,376 613
WSJ40 7,613 28,240 823
Table 2: Grammar size for each of our models.
the standard splits, training on sections 2 to 21, test-
ing on section 23 and doing development on section
22. Previous work on (non-reranking) discrimina-
tive parsing has given results on sentences of length
 15, but most parsing literature gives results on ei-
ther sentences of length  40, or all sentences. To
properly situate this work with respect to both sets
of literature we trained models on both length 
15 (WSJ15) and length  40 (WSJ40), and we also
tested on all sentences using the WSJ40 models. Our
results also provide a context for interpreting previ-
ous work which used WSJ15 and not WSJ40.
We used a relatively simple grammar with few ad-
ditional annotations. Starting with the grammar read
off of the training set, we added parent annotations
onto each state, including the POS tags, resulting in
rules such as S-ROOT  NP-S VP-S. We also added
head tag annotations to VPs, in the same manner as
(Klein and Manning, 2003). Lastly, for the WSJ40
runs we used a simple, right branching binarization
where each active state is annotated with its previous
sibling and first child. This is equivalent to children
of a state being produced by a second order Markov
process. For the WSJ15 runs, each active state was
annotated with only its first child, which is equiva-
lent to a first order Markov process. See Table 5 for
the number of states and rules produced.
5.1 Experiments
For both WSJ15 and WSJ40, we trained a genera-
tive model; a discriminative model, which used lexi-
con features, but no grammar features other than the
rules themselves; and a feature-based model which
had access to all features. For the length 15 data we
also did experiments in which we relaxed the gram-
mar. By this we mean that we added (previously un-
seen) rules to the grammar, as a means of smoothing.
We chose which rules to add by taking existing rules
and modifying the parent annotation on the parent
of the rule. We used stochastic gradient descent for
963
Table 1: Lexicon and grammar features. w is the word and t the tag. r represents a particular rule along with span/split
information;  is the rule itself, rp is the parent of the rule; wb, ws, and we are the first, first after the split (for binary
rules) and last word that a rule spans in a particular context. All states, including the POS tags, are annotated with
parent information; b(s) represents the base label for a state s and p(s) represents the parent annotation on state s.
ds(w) represents the distributional similarity cluster, and lc(w) the lower cased version of the word, and unk(w) the
unknown word class.
Lexicon Features Grammar Features
t Binary-specific features
b(t) 
t,w b(p(rp)),ds(ws) b(p(rp)),ds(ws1,dsws)
t,lc(w) b(p(rp)),ds(we) PP feature:
b(t),w unary? if right child is a PP then r,ws
b(t),lc(w) simplified rule: VP features:
t,ds(w) base labels of states if some child is a verb tag, then rule,
t,ds(w1) dist sim bigrams: with that child replaced by the word
t,ds(w+1) all dist. sim. bigrams below
b(t),ds(w) rule, and base parent state Unaries which span one word:
b(t),ds(w1) dist sim bigrams:
b(t),ds(w+1) same as above, but trigrams r,w
p(t),w heavy feature: r,ds(w)
t,unk(w) whether the constituent is big b(p(r)),w
b(t),unk(w) as described in (Johnson, 2001) b(p(r)),ds(w)
these experiments; the length 15 models had a batch
size of 15 and we allowed twenty passes through
the data.3 The length 40 models had a batch size
of 30 and we allowed ten passes through the data.
We used development data to decide when the mod-
els had converged. Additionally, we provide gener-
ative numbers for training on the entire PTB to give
a sense of how much performance suffered from the
reduced training data (generative-all in Table 4).
The full results for WSJ15 are shown in Table 3
and for WSJ40 are shown in Table 4. The WSJ15
models were each trained on a single Dual-Core
AMD OpteronTM using three gigabytes of RAM and
no parallelization. The discriminatively trained gen-
erative model (discriminative in Table 3) took ap-
proximately 12 minutes per pass through the data,
while the feature-based model (feature-based in Ta-
ble 3) took 35 minutes per pass through the data.
The feature-based model with the relaxed grammar
(relaxed in Table 3) took about four times as long
as the regular feature-based model. The discrimina-
3Technically we did not make passes through the data, be-
cause we sampled with replacement to get our batches. By this
we mean having seen as many sentences as are in the data, de-
spite having seen some sentences multiple times and some not
at all.
tively trained generative WSJ40 model (discrimina-
tive in Table 4) was trained using two of the same
machines, with 16 gigabytes of RAM each for the
clients.4 It took about one day per pass through
the data. The feature-based WSJ40 model (feature-
based in Table 4) was trained using four of these
machines, also with 16 gigabytes of RAM each for
the clients. It took about three days per pass through
the data.
5.2 Discussion
The results clearly show that gains came from both
the switch from generative to discriminative train-
ing, and from the extensive use of features. In Fig-
ure 3 we show for an example from section 22 the
parse trees produced by our generative model and
our feature-based discriminative model, and the cor-
rect parse. The parse from the feature-based model
better exhibits the right branching tendencies of En-
glish. This is likely due to the heavy feature, which
encourages long constituents at the end of the sen-
tence. It is difficult for a standard PCFG to learn this
aspect of the English language, because the score it
assigns to a rule does not take its span into account.
4The server does almost no computation.
964
Model P R F1 Exact Avg CB 0 CB P R F1 Exact Avg CB 0 CB
development set  length  15 test set  length  15
Taskar 2004 89.7 90.2 90.0    89.1 89.1 89.1   
Turian 2007       89.6 89.3 89.4   
generative 86.9 85.8 86.4 46.2 0.34 81.2 87.6 85.8 86.7 49.2 0.33 81.9
discriminative 89.1 88.6 88.9 55.5 0.26 85.5 88.9 88.0 88.5 56.6 0.32 85.0
feature-based 90.4 89.3 89.9 59.5 0.24 88.3 91.1 90.2 90.6 61.3 0.24 86.8
relaxed 91.2 90.3 90.7 62.1 0.24 88.1 91.4 90.4 90.9 62.0 0.22 87.9
Table 3: Development and test set results, training and testing on sentences of length  15 from the Penn treebank.
Model P R F1 Exact Avg CB 0 CB P R F1 Exact Avg CB 0 CB
test set  length  40 test set  all sentences
Petrov 2007   88.8      88.3   
generative 83.5 82.0 82.8 25.5 1.57 53.4 82.8 81.2 82.0 23.8 1.83 50.4
generative-all 83.6 82.1 82.8 25.2 1.56 53.3      
discriminative 85.1 84.5 84.8 29.7 1.41 55.8 84.2 83.7 83.9 27.8 1.67 52.8
feature-based 89.2 88.8 89.0 37.3 0.92 65.1 88.2 87.8 88.0 35.1 1.15 62.3
Table 4: Test set results, training on sentences of length  40 from the Penn treebank. The generative-all results were
trained on all sentences regardless of length
6 Comparison
With Related Work
The most similar related work is (Johnson, 2001),
which did discriminative training of a generative
PCFG. The model was quite similar to ours, except
that it did not incorporate any features and it re-
quired the parameters (which were just scores for
rules) to be locally normalized, as with a genera-
tively trained model. Due to training time, they used
the ATIS treebank corpus , which is much smaller
than even WSJ15, with only 1,088 training sen-
tences, 294 testing sentences, and an average sen-
tence length of around 11. They found no signif-
icant difference in performance between their gen-
eratively and discriminatively trained parsers. There
are two probable reasons for this result. The training
set is very small, and it is a known fact that gener-
ative models tend to work better for small datasets
and discriminative models tend to work better for
larger datasets (Ng and Jordan, 2002). Additionally,
they made no use of features, one of the primary
benefits of discriminative learning.
Taskar et al. (2004) took a large margin approach
to discriminative learning, but achieved only small
gains. We suspect that this is in part due to the gram-
mar that they chose  the grammar of (Klein and
Manning, 2003), which was hand annotated with the
intent of optimizing performance of a PCFG. This
grammar is fairly sparse  for any particular state
there are, on average, only a few rules with that state
as a parent  so the learning algorithm may have suf-
fered because there were few options to discriminate
between. Starting with this grammar we found it dif-
ficult to achieve gains as well. Additionally, their
long training time (several months for WSJ15, ac-
cording to (Turian and Melamed, 2006)) made fea-
ture engineering difficult; they were unable to really
explore the space of possible features.
More recent is the work of (Turian and Melamed,
2006; Turian et al., 2007), which improved both the
training time and accuracy of (Taskar et al., 2004).
They define a simple linear model, use boosted de-
cision trees to select feature conjunctions, and a line
search to optimize the parameters. They use an
agenda parser, and define their atomic features, from
which the decision trees are constructed, over the en-
tire state being considered. While they make exten-
sive use of features, their setup is much more com-
plex than ours and takes substantially longer to train
 up to 5 days on WSJ15  while achieving only
small gains over (Taskar et al., 2004).
The most recent similar research is (Petrov et al.,
2007). They also do discriminative parsing of length
40 sentences, but with a substantially different setup.
Following up on their previous work (Petrov et al.,
2006) on grammar splitting, they do discriminative
965
S
S
NP
PRP
He
VP
VBZ
adds
NP
DT
This
VP
VBZ
is
RB
nt
NP
NP
CD
1987
VP
VBN
revisited
S
NP
PRP
He
VP
VBZ
adds
S
NP
DT
This
VP
VBZ
is
RB
nt
NP
CD
1987
VP
VBN
revisited
S
NP
PRP
He
VP
VBZ
adds
S
NP
DT
This
VP
VBZ
is
RB
nt
NP
NP
CD
1987
VP
VBN
revisited
(a) generative output (b) feature-based discriminative output (c) gold parse
Figure 3: Example output from our generative and feature-based discriminative models, along with the correct parse.
parsing with latent variables, which requires them
to optimize a non-convex function. Instead of us-
ing a stochastic optimization technique, they use L-
BFGS, but do coarse-to-fine pruning to approximate
their gradients and log likelihood. Because they
were focusing on grammar splitting they, like (John-
son, 2001), did not employ any features, and, like
(Taskar et al., 2004), they saw only small gains from
switching from generative to discriminative training.
7 Conclusions
We have presented a new, feature-rich, dynamic pro-
gramming based discriminative parser which is sim-
pler, more effective, and faster to train and test than
previous work, giving us new state-of-the-art per-
formance when training and testing on sentences of
length  15 and the first results for such a parser
trained and tested on sentences of length  40. We
also show that the use of SGD for training CRFs per-
forms as well as L-BFGS in a fraction of the time.
Other recent work on discriminative parsing has ne-
glected the use of features, despite their being one of
the main advantages of discriminative training meth-
ods. Looking at how other tasks, such as named
entity recognition and part-of-speech tagging, have
evolved over time, it is clear that greater gains are to
be gotten from developing better features than from
better models. We have provided just such a frame-
work for improving parsing performance.
Acknowledgments
Many thanks to Teg Grenager and Paul Heymann
for their advice (and their general awesomeness),
and to our anonymous reviewers for helpful com-
ments.
This paper is based on work funded in part by
the Defense Advanced Research Projects Agency
through IBM, by the Disruptive Technology Office
(DTO) Phase III Program for Advanced Question
Answering for Intelligence (AQUAINT) through
Broad Agency Announcement (BAA) N61339-06-
R-0034, and by a Scottish Enterprise Edinburgh-
Stanford Link grant (R37588), as part of the EASIE
project.

References

Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-best parsing and maxent discriminative reranking. In ACL 43, pages 173180.

Alexander Clark. 2000. Inducing syntactic categories by context distribution clustering. In Proc. of Conference on Computational Natural Language Learning, pages 9194, Lisbon, Portugal.

Trevor Cohn and Philip Blunsom. 2005. Semantic role labelling with tree conditional random fields. In CoNLL 2005.

Michael Collins. 2000. Discriminative reranking for natural language parsing. In ICML 17, pages 175182.

Jay Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 6(8):451455.

James Henderson. 2004. Discriminative training of a neural network statistical parser. In ACL 42, pages 96 103.

Mark Johnson. 2001. Joint and conditional estimation of tagging and parsing models. In Meeting of the Association for Computational Linguistics, pages 314321.

Florent Jousse, Remi Gilleron, Isabelle Tellier, and Marc Tommasi. 2006. Conditional Random Fields for XML trees. In ECML Workshop on Mining and Learning in Graphs.

Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the Association of Computational Linguistics (ACL).

John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic models for segmenting and labeling sequence data. In ICML 2001, pages 282289. Morgan Kaufmann, Sa Francisco, CA.

Dong C. Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Math. Programming, 45(3, (Ser. B)):503528.

Christopher D. Manning and Hinrich Schutze. 1999Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts.

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313330.

Andrew Ng and Michael Jordan. 2002. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In Advances in Neural Information Processing Systems (NIPS). 

Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In ACL 44/COLING 21, pages 433440.

Slav Petrov, Adam Pauls, and Dan Klein. 2007. Discriminative log-linear grammars with latent variables. In NIPS.

Adwait Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In EMNLP 2, pages 110.

Andreas Stolcke. 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities. Computational Linguistics, 21:165202.

Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, and Christopher D. Manning. 2004. Max-margin parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).

Joseph Turian and I. Dan Melamed. 2006. Advances in discriminative parsing. In ACL 44, pages 873880. 

Joseph Turian, Ben Wellington, and I. Dan Melamed. 2007. Scalable discriminative learning for natural language parsing and translation. In Advances in Neural Information Processing Systems 19, pages 14091416. MIT Press.

S. V. N. Vishwanathan, Nichol N. Schraudolph, Mark W. Schmidt, and Kevin P. Murphy. 2006. Accelerated training of conditional random fields with stochastic gradient methods. In ICML 23, pages 969976. 967
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 261264,
Columbus, Ohio, USA, June 2008. c2008 Association for Computational Linguistics
Evaluating Word Prediction: Framing Keystroke Savings
Keith Trnka and Kathleen F. McCoy
University of Delaware
Newark, DE 19716
trnka@cis.udel.edu
Abstract
Researchers typically evaluate word predic-
tion using keystroke savings, however, this
measure is not straightforward. We present
several complications in computing keystroke
savings which may affect interpretation and
comparison of results. We address this prob-
lem by developing two gold standards as a
frame for interpretation. These gold standards
measure the maximum keystroke savings un-
der two different approximations of an ideal
language model. The gold standards addition-
ally narrow the scope of deficiencies in a word
prediction system.
1 Introduction
Word prediction is an application of language mod-
eling to speeding up text entry, especially to entering
utterances to be spoken by an Augmentative and Al-
ternative Communication (AAC) device. AAC de-
vices seek to address the dual problem of speech and
motor impairment by attempting to optimize text in-
put. Even still, communication rates with AAC de-
vices are often below 10 words per minute (Newell
et al., 1998), compared to the common 130-200
words per minute speech rate of speaking people.
Word prediction addresses these issues by reducing
the number of keystrokes required to produce a mes-
sage, which has been shown to improve communi-
cation rate (Trnka et al., 2007). The reduction in
keystrokes also translates into a lower degree of fa-
tigue from typing all day (Carlberger et al., 1997).
Word prediction systems present multiple com-
pletions of the current word to the user. Systems
generate a list of W predictions on the basis of the
word being typed and a language model. The vo-
cabulary is filtered to match the prefix of the current
word and the language model ranks the words ac-
cording to their likelihood. In the case that no letters
of the current word have been entered, the language
model is the sole factor in generating predictions.
Systems often use a touchscreen or function/number
keys to select any of the predicted words.
Because the goal of word prediction systems is
to reduce the number of keystrokes, the primary
evaluation for word prediction is keystroke savings
(Garay-Vitoria and Abascal, 2006; Newell et al.,
1998; Li and Hirst, 2005; Trnka and McCoy, 2007;
Carlberger et al., 1997). Keystroke savings (KS)
measures the percentage reduction in keys pressed
compared to letter-by-letter text entry.
KS = keysnormal keyswith predictionkeys
normal
100%
A word prediction system that offers higher savings
will benefit a user more in practice.
However, the equation for keystroke savings has
two major deficiencies. Firstly, the equation alone
is not enough to compute keystroke savings  actu-
ally computing keystroke savings requires a precise
definition of a keystroke and also requires a method
fordetermininghowmanykeystrokesareusedwhen
predictionsareavailable, discussedinSection2. Be-
yond simply computing keystroke savings, the equa-
tionalonedoesnotprovidemuchinthewayofinter-
pretationis60%keystrokesavingsgood? Canwe
do better? Section 3 will present two gold standards
to allow better interpretation of keystroke savings.
261
2 Computing
Keystroke Savings
We must have a way to determine how many
keystrokes a user would take under both letter-
by-letter entry and word prediction to compute
keystroke savings. The common trend in research
is to simulate a perfect user that will never make
typing mistakes and will select a word from the pre-
dictions as soon as it appears.
Implementation of perfect utilization of the pre-
dictions is not always straightforward. For exam-
ple, consider the predictive interface in Microsoft
WordTM: a single prediction is offered as an inline
completion. If the prediction is selected, the user
may backspace and edit the word. However, this
freedom makes finding the minimum sequence of
keys more difficult  now the user may select a
prediction with the incorrect suffix and correct the
suffix as the optimal action. We feel that a more in-
tuitive interface would allow a user to undo the pre-
diction selection by pressing backspace, an interface
which does not support backspace-editing. In addi-
tion to backspacing, future research in multi-word
prediction will face a similar problem, analogous to
the garden-path problem in parsing, where a greedy
approach does not always give the optimal result.
The keystrokes used for training and testing word
prediction systems can affect the results. We at-
tempt to evaluate word prediction as realistically as
possible. Firstly, many corpora have punctuation
marks, but an AAC user in a conversational setting
is unlikely to use punctuation due to the high cost
of each key press. Therefore, we remove punctua-
tion on the outside of words, such as commas and
periods, but leave word-internal punctuation intact.
Also, we treat capital letters as a single key press,
reflecting the trend of many AAC users to avoid cap-
italization. Another problem occurs for a newline or
speak key, which the user would press after com-
pleting an utterance. In pilot studies, including the
simulation of a speak key lowered keystroke savings
by 0.81.0% for window sizes 110, because new-
linesarenotabletobepredictedinthesystem. How-
ever, we feel that the simulation of a speak key will
produce an evaluation metric that is closer to the ac-
tual users experience, therefore we include a speak
key in our evaluations.
An evaluation of word prediction must address
these issues, if only implicitly. The effect of these
potentially implicit decisions on keystroke savings
can make comparison of results difficult. However,
if results are presented in reference to a gold stan-
dard under the sameassumptions, wecan draw more
reliable conclusions from results.
3 Towards
a Gold Standard
In trying to improve the state of word prediction,
several researchers have noted that it seems ex-
tremely difficult to improve keystroke savings be-
yond a certain point. Copestake (1997) discussed
the entropy of English to conclude that 5060%
keystroke savings may be the most we can expect
in practice. Lesher et al. (2002) replaced the lan-
guage model in a word prediction system with a
human to try and estimate the limit of keystroke
savings. They found that humans could achieve
59% keystroke savings with access to their ad-
vanced language model and that their advanced lan-
guage model alone achieved 54% keystroke savings.
They noted that one subject achieved nearly 70%
keystroke savings on one particular text, and con-
cluded that further improvements on current meth-
ods are possible. Garay-Vitoria and Abascal (2006)
survey many prediction systems, showing a wide
spectrum of savings, but no system offers more than
70% keystroke savings.
We investigated the problem of the limitations
of keystroke savings first from a theoretical per-
spective, seeking a clearly defined upper boundary.
Keystrokesavingscanneverreach100%itwould
mean that the system divined the entire text they in-
tended without a single key.
3.1 Theoretical
keystroke savings limit
The minimum amount of input required corresponds
to a perfect system  one that predicts every word
as soon as possible. In a word completion sys-
tem, the predictions are delayed until after the first
character of the word is entered. In such a sys-
tem, the minimum amount of input using a perfect
language model is two keystrokes per word  one
for the first letter and one to select the prediction.
The system would also require one keystroke per
sentence. In a word prediction system, the predic-
tions are available immediately, so the minimal in-
262
put for a perfect system is one keystroke per word
(to select the prediction) and one keystroke per sen-
tence. Weaddedtheabilitytomeasuretheminimum
number of keystrokes and maximum savings to our
simulation software, which we call the theoretical
keystroke savings limit.
We evaluated a baseline trigram model under two
conditions with different keystroke requirements on
the Switchboard corpus. The simulation software
was modified to output the theoretical limit in ad-
dition to actual keystroke savings at various window
sizes. To demonstrate the effect of the theoretical
keystroke savings limit on actual savings, we eval-
uated the trigram model under conditions with two
different limits  word prediction and word com-
pletion. The evaluation of the trigram model using
word completion is shown in Figure 1. The actual
keystroke savings is graphed by window size in ref-
erence to the theoretical limit. As noted by other re-
searchers, keystroke savings increases with window
size, but with diminishing returns (this is the effect
of placing the most probable words first). One of
0%
10%
20%
30%
40%
50%
60%
1 2 3 4 5 6 7 8 9 10
Keystroke savings
Window size
Word completion
Theoretical limit
Figure 1: Keystroke savings and the limit vs. window
size for word completion.
the problems with word completion is that the the-
oretical limit is so close to actual performance 
around 58.5% keystroke savings compared to 50.8%
keystroke savings with five predictions. At only five
predictions, the system has already realized 87% of
the possible keystroke savings. Under these circum-
stances, it would take a drastic change in the lan-
guage model to impact keystroke savings.
We repeated this analysis for word prediction,
showninFigure2alongsidewordcompletion. Word
prediction is much higher than completion, both the-
oretically (the limit) and in actual keystroke savings.
0%
10%
20%
30%
40%
50%
60%
70%
80%
1 2 3 4 5 6 7 8 9 10
Keystroke savings
Window size
Word prediction
Word prediction limit
Word completion
Word completion limit
Figure 2: Keystroke savings and the limit vs. window
size for word prediction compared to word completion.
Word prediction offers much more headroom in
terms of improvements in keystroke savings. There-
fore our ongoing research will focus on word pre-
diction over word completion.
This analysis demonstrates a limit to keystroke
savings, but this limit is slightly different than
Copestake (1997) and Lesher et al. (2002) seek to
describe  beyond the limitations of the user in-
terface, there seems to be a limitation on the pre-
dictability of English. Ideally, we would like to have
a gold standard that is a closer estimate of an ideal
language model.
3.2 Vocabulary
limit
We can derive a more practical limit by simulating
word prediction using a perfect model of all words
that occur in the training data. This gold standard
will predict the correct word immediately so long as
it occurs in the training corpus. Words that never oc-
curred in training require letter-by-letter entry. We
call this measure the vocabulary limit and apply it to
evaluatewhetherthedifferencebetweentrainingand
testing vocabulary is significant. Previous research
has focused on the percentage of out-of-vocabulary
(OOV) terms to explain changes in keystroke sav-
ings (Trnka and McCoy, 2007; Wandmacher and
Antoine, 2006). In contrast, the vocabulary limit
gives more guidance for research by translating the
problem of OOVs into keystroke savings.
Expanding the results from the theoretical limit,
the vocabulary limit is 77.6% savings, compared to
78.4% savings for the theoretical limit and 58.7%
actual keystroke savings with 5 predictions. The
practical limit is very close to the theoretical limit
263
in the case of Switchboard. Therefore, the remain-
ing gap between the practical limit and actual per-
formance must be due to other differences between
testing and training data, limitations of the model,
and limitations of language modeling.
3.3 Application
to corpus studies
Weappliedthegoldstandardstoourcorpusstudy, in
which a trigram model was individually trained and
tested on several different corpora (Trnka and Mc-
Coy, 2007). In contrast to the actual trigram model
Corpus Trigram Vocab.
limit
Theor.
limit
AAC Email 48.92% 61.94% 84.83%
Callhome 43.76% 54.62% 81.38%
Charlotte 48.30% 65.69% 83.74%
SBCSAE 42.30% 60.81% 79.86%
Micase 49.00% 69.18% 84.08%
Switchboard 60.35% 80.33% 82.57%
Slate 53.13% 81.61% 85.88%
Table 1: A trigram model compared to the limits.
performance, the theoretical limits all fall within a
relatively narrow range, suggesting that the achiev-
able keystroke savings may be similar even across
different domains. The more technical and formal
corpora (Micase, Slate, AAC) show higher limits, as
the theoretical limit is based on the length of words
and sentences in each corpus. The practical limit
exhibits much greater variation. Unlike the Switch-
board analysis, many other corpora have a substan-
tial gap between the theoretical and practical limits.
Although the practical measure seems to match the
actual savings similarly to OOVs testing with cross-
validation (Trnka and McCoy, 2007), this measure
more concretely illustrates the effect of OOVs on
actual keystroke savings  60% keystroke savings
when training and testing on AAC Email would be
extraordinary.
4 Conclusions
Although keystroke savings is the predominant eval-
uation for word prediction, this evaluation is not
straightforward, exacerbating the problem of inter-
preting and comparing results. We have presented
a novel solution  interpreting results alongside
gold standards which capture the difficulty of the
evaluation. These gold standards are also applica-
ble to drive future research  if actual performance
is very close to the theoretical limit, then relaxing
the minimum keystroke requirements should be the
most beneficial (e.g., multi-word prediction). Sim-
ilarly, if actual performance is very close to the
vocabulary limit, then the vocabulary of the lan-
guage model must be improved (e.g., cache mod-
eling, adding general-purpose training data). In the
case that keystroke savings is far from either limit,
then research into improving the language model is
likely to be the most beneficial.
Acknowledgments
This work was supported by US Department of Ed-
ucation grant H113G040051.

References

Alice Carlberger, John Carlberger, Tina Magnuson,
M. Sharon Hunnicutt, Sira Palazuelos-Cagigas, and
Santiago Aguilera Navarro. 1997. Profet, a new gen-
eration of word prediction: An evaluation study. In
ACL-97 workshop on Natural Language Processing
for Communication Aids.

Ann Copestake. 1997. Augmented and alternative NLP
techniques for augmentative and alternative commu-
nication. In ACL-97 workshop on Natural Language
Processing for Communication Aids, pages 3742.

Nestor Garay-Vitoria and Julio Abascal. 2006. Text pre-
diction systems: a survey. Univ Access Inf Soc, 4:183
203.

Gregory W. Lesher, Bryan J. Moulton, D Jeffery Higgin-
botham, and Brenna Alsofrom. 2002. Limits of hu-
man word prediction performance. In CSUN.

Jianhua Li and Graeme Hirst. 2005. Semantic knowl-
edge in word completion. In ASSETS, pages 121128.

Alan Newell,Stefan Langer,and Marianne Hickey. 1998.
The role of natural language processing in alternative
and augmentative communication. Natural Language
Engineering, 4(1):116.

Keith Trnka and Kathleen F. McCoy. 2007. Corpus Stud-
ies in Word Prediction. In ASSETS, pages 195202.
Keith Trnka, Debra Yarrington, John McCaw, Kathleen F.
McCoy, and Christopher Pennington. 2007. The Ef-
fects of Word Prediction on Communication Rate for
AAC. In NAACL-HLT; Companion Volume: Short Papers, pages 173176.

Tonio Wandmacher and Jean-Yves Antoine. 2006.
Training Language Models without Appropriate Lan-
guage Resources: Experiments with an AAC System
for Disabled People. In Eurospeech.
Proceedings of the ACL-08: HLT Student Research Workshop (Companion Volume), pages 6166,
Columbus, June 2008. c2008 Association for Computational Linguistics
Adaptive Language Modeling for Word Prediction
Keith Trnka
University of Delaware
Newark, DE 19716
trnka@cis.udel.edu
Abstract
We present the development and tuning of a
topic-adapted language model for word pre-
diction, which improves keystroke savings
over a comparable baseline. We outline our
plans to develop and integrate style adap-
tations, building on our experience in topic
modeling to dynamically tune the model to
both topically and stylistically relevant texts.
1 Introduction
PeoplewhouseAugmentativeandAlternativeCom-
munication (AAC) devices communicate slowly, of-
ten below 10 words per minute (wpm) compared to
150 wpm or higher for speech (Newell et al., 1998).
AAC devices are highly specialized keyboards with
speech synthesis, typically providing single-button
input for common words or phrases, but requiring a
user to type letter-by-letter for other words, called
fringe vocabulary. Many commercial systems (e.g.,
PRCs ECO) and researchers (Li and Hirst, 2005;
Trnka et al., 2006; Wandmacher and Antoine, 2007;
Matiasek and Baroni, 2003) have leveraged word
prediction to help speed AAC communication rate.
While the user is typing an utterance letter-by-letter,
the system continuously provides potential comple-
tions of the current word to the user, which the user
may select. The list of predicted words is generated
using a language model.
At best, modern devices utilize a trigram model
and very basic recency promotion. However, one of
the lamented weaknesses of ngram models is their
sensitivity to the training data. They require sub-
stantialtrainingdatato beaccurate,andincreasingly
more data as more of the context is utilized. For ex-
ample, Lesher et al. (1999) demonstrate that bigram
and trigram models for word predictionare not satu-
rated even when trained on 3 million words, in con-
trast to a unigram model. In addition to the prob-
lem of needing substantial amounts of training text
to build a reasonable model, ngrams are sensitive
to the difference between training and testing/user
texts. An ngram model trained on text of a differ-
ent topic and/orstyle may performvery poorlycom-
pared to a model trained and tested on similar text.
Trnka and McCoy (2007) and Wandmacher and An-
toine(2006)have demonstratedthe domainsensitiv-
ity of ngram models for word prediction.
The problem of utilizing ngram models for con-
versational AAC usage is that no substantial cor-
pora of AAC text are available (much less conver-
sational AAC text). The most similar available cor-
pora are spoken language, but are typically much
smaller than written corpora. The problem of cor-
pora for AAC is that similarity and availability are
inversely related, illustrated in Figure 1. At one ex-
treme,a verylargeamountofformalwrittenEnglish
is available, however, it is very dissimilar from con-
versational AAC text, making it less useful for word
prediction. At the other extreme, logged text from
the current conversation of the AAC user is the most
highly related text, but it is extremely sparse. While
this trend is demonstratedwith a variety of language
modeling applications, the problem is more severe
for AAC due to the extremely limited availability of
AAC text. Even if we train our models on both a
large number of general texts in addition to highly
related in-domain texts to address the problem, we
61
Figure 1: The most relevant text available is often the smallest, while the largest corpora are often the least relevant
for AAC word prediction. This problem is exaggerated for AAC.
must focus the models on the most relevant texts.
We addressthe problemof balancingtrainingsize
and similarityby dynamicallyadaptingthe language
model to the most topically relevant portions of the
training data. We present the results of experiment-
ing with different topic segmentationsand relevance
scores in order to tune existing methods to topic
modeling. Our approach is designed to seamlessly
degrade to the baseline model when no relevant top-
ics are found, by interpolatingfrequenciesas well as
ensuringthatalltrainingdocumentscontributesome
non-zero probabilities to the model. We also out-
line our plans to adapt ngram models to the style of
discourse and then combine the topical and stylistic
adaptations.
1.1 Evaluating
Word Prediction
Word prediction is evaluated in terms of keystroke
savings  the percentage of keystrokes saved by
taking full advantage of the predictions compared to
letter-by-letter entry.
KS = keysletter-by-letter keyswith predictionkeys
letter-by-letter
100%
Keystroke savings is typically measured automati-
callyby simulatinga usertypingthe testingdataof a
corpus, where any prediction is selected with a sin-
gle keystroke and a space is automatically entered
after selecting a prediction. The results are depen-
dent on the quality of the language model as well as
the number of words in the prediction window. We
focus on 5-word prediction windows. Many com-
mercialdevicesprovideoptimizedinputforthemost
common words (called core vocabulary) and offer
word prediction for all other words (fringe vocabu-
lary). Therefore, we limit our evaluation to fringe
words only, based on a core vocabulary list from
conversations of young adults.
WefocusourtrainingandtestingonSwitchboard,
which we feel is similar to conversational AAC text.
Our overall evaluation varies the training data from
Switchboard training to training on out-of-domain
datato estimatethe effectsof topicmodelingin real-
world usage.
2 Topic
Modeling
Topic models are language models that dynamically
adapt to testing data, focusing on the most related
topics in the training data. It can be viewed as a
two stage process: 1) identifying the relevant topics
by scoring and 2) tuning the language model based
on relevant topics. Various other implementations
of topic adaptation have been successful in word
prediction (Li and Hirst, 2005; Wandmacher and
Antoine, 2007) and speech recognition (Bellegarda,
2000; Mahajan et al., 1999; Seymore and Rosen-
feld, 1997). The main difference of the topic mod-
eling approach compared to Latent Semantic Anal-
ysis (LSA) models (Bellegarda, 2000) and trigger
pair models (Lau et al., 1993; Matiasek and Baroni,
2003) is that topic models perform the majority of
generalizationabouttopicrelatednessat testingtime
rather than training time, which potentially allows
user text to be added to the training data seamlessly.
Topic modeling follows the framework below
Ptopic(w | h) =
summationdisplay
ttopics
P(t | h)P(w | h,t)
where w is the word being predicted/estimated, h
represents all of the document seen so far, and t rep-
resents a single topic. The linear combination for
topic modeling shows the three main areas of vari-
ation in topic modeling. The posterior probability,
62
P(w | h,t) represents the sort of model we have;
how topic will affect the adapted language model in
theend. Theprior,P(t | h),representsthewaytopic
is identified. Finally, the meaning of t  topics, re-
quires explanation  what is a topic?
2.1 Posterior
Probability  Topic Application
The topic modeling approach complicates the esti-
mation of probabilities from a corpus because the
additional conditioning information in the posterior
probability P(w | h,t) worsens the data sparseness
problem. This section will present our experience in
lessening the data sparseness problem in the poste-
rior, using examples on trigram models.
The posterior probability requires more data
thanatypicalngrammodel,potentiallycausingdata
sparseness problems. We have explored the pos-
sibility of estimating it by geometrically combin-
ing a topic-adapted unigram model (i.e., P(w | t))
with a context-adapted trigram model (i.e., P(w |
w1,w2)), compared to straightforward measure-
ment (P(w | w1,w2,t)). Although the first
approach avoids the additional data sparseness, it
makes an assumption that the topic of discourse
onlyaffectsthevocabularyusage. Bellegarda(2000)
usedthis approachfor LSA-adaptedmodeling,how-
ever, we found this approach to be inferior to di-
rect estimation of the posterior probability for word
prediction (Trnka et al., 2006). Part of the reason
for the lesser benefit is that the overall model is
only affected slightly by topic adaptations due to
the tuned exponential weight of 0.05 on the topic-
adapted unigram model. We extended previous re-
search by forcing trigram predictions to occur over
bigrams and so on (rather than backoff) and using
the topic-adapted model for re-ranking within each
set of predictions, but found that the forced ordering
of the ngram components was overly detrimental to
keystroke savings.
Backoff models for topic modeling can be con-
structed either before or after the linear interpola-
tion. If the backoff is performed after interpolation,
we must also choose whether smoothing (a prereq-
uisite for backoff) is performed before or after the
interpolation. If we smooth before the interpolation,
then the frequencies will be overly discounted, be-
cause the smoothing method is operating on a small
fraction of the training data, which will reduce the
benefit of higher-order ngrams in the overall model.
Also, if we combine probability distributions from
each topic, the combination approach may have dif-
ficulties with topics of varying size. We address
these issues by instead combining frequencies and
performing smoothing and backoff after the combi-
nation, similar to Adda et al. (1999), although they
used corpus-sized topics. The advantage of this ap-
proach is that the held-out probability for each dis-
tribution is appropriatefor the training data, because
the smoothing takes place knowing the number of
words that occurred in the whole corpus, rather than
for each small segment. This is especially important
when dealing with small and different sized topics.
The linear interpolation affects smoothing
methodsnegatively  because the weights are less
than one, the combination decreases the total sum
of each conditional distribution. This will cause
smoothing methods to underestimate the reliability
of the models, because smoothing methods estimate
the reliability of a distribution based on the absolute
number of occurrences. To correct this, after inter-
polating the frequencies we found it useful to scale
the distribution back to its original sum. The scal-
ing approach improved keystroke savings by 0.2%
0.4% for window size 210 and decreased savings
by 0.1% for window size 1. Becausemost AAC sys-
tems provide 57 predictions, we use this approach.
Also, because some smoothing methods operate on
frequencies, but the combination model produces
real-valued weights for each word, we found it nec-
essarytobucketthecombinedfrequenciestoconvert
them to integers.
Finally, we required an efficient smoothing
method that could discount each conditional distri-
bution individually to facilitate on-demand smooth-
ing for each conditional distribution, in contrast to
a method like Katz backoff (Katz, 1987) which
smoothes an entire ngram model at once. Also,
Good-Turing smoothingproved too cumbersome,as
wewereunabletorelyontheratiobetweenwordsin
given bins and also unable to reliably apply regres-
sion. Instead, we used an approximation of Good-
Turing smoothing that performed similarly, but al-
lowed for substantial optimization.
63
2.2 Prior
Probability  Topic Identification
Thetopicmodelingapproachusesthecurrenttesting
document to tune the language model to the most
relevant training data. The benefit of adaptation is
dependentonthequalityofthesimilarityscores. We
will first present our representation of the current
document, which is compared to unigram models of
each topic using a similarityfunction. We determine
the weight of each word in the current document us-
ing frequency, recency, and topical salience.
The recency of use of a word contributes to the
relevance of the word. If a word was used somewhat
recently, we would expect to see the word again. We
follow Bellegarda (2000) in using an exponentially
decayed cache with weight of 0.95 to model this ef-
fect of recency on importance at the current position
in the document. The weight of 0.95 represents a
preservation in topic, but with a decay for very stale
words, whereas a weight of 1 turns the exponen-
tial model into a pure frequency model and lower
weights represent quick shifts in topic.
The importance of each word occurrence in the
current document is a factor of not just its frequency
and recency, but also its topical salience  how
well the word discriminatesbetween topics. For this
reason, we decided to use a technique like Inverse
Document Frequency (IDF) to boost the weight of
words that occur in only a few documents and de-
press the weights of words that occur in most docu-
ments. However, instead of using IDF to measure
topical salience, we use Inverse Topic Frequency
(ITF), which is more specifically tailored to topic
modeling and the particular kinds of topics used.
We evaluated several similarity functions for
topic modeling, initially using the cosine measure
for similarity scoring and scaling the scores to be
a probability distribution, following Florian and
Yarowsky (1999). The intuition behind the co-
sine measure is that the similarity between two dis-
tributions of words should be independent of the
length of either document. However, researchers
have demonstrated that cosine is not the best rele-
vance metric for other applications, so we evaluated
two other topical similarity scores: Jacquards coef-
ficient, which performed better than most other sim-
ilarity measures in a different task for Lee (1999)
and Nave Bayes, which gave better results than co-
sine in topic-adapted language models for Seymore
and Rosenfeld (1997). We evaluated all three simi-
larity metrics using Switchboard topics as the train-
ing data and each of our corpora for testing us-
ing cross-validation. We found that cosine is con-
sistently better than both Jacquards coefficient and
Nave Bayes, across all corpora tested. The differ-
ences between cosine and the other methods are sta-
tisticallysignificantatp < 0.001. Itmaybepossible
that the ITF or recency weighting in the cache had a
negative interaction with Nave Bayes; traditionally
raw frequencies are used.
We found it useful to polarize the similarity
scores, following Florian and Yarowsky (1999),
who found that transformations on cosine similarity
reduced perplexity. We scaled the scores such that
the maximumscore was one and the minimumscore
was zero, which improved keystroke savings some-
what. This helps fine-tune topic modelingby further
boosting the weights of the most relevant topics and
depressing the weights of the less relevant topics.
Smoothing the scores helps prevent some scores
from being zero due to lack of word overlap. One of
themotivationsbehindusingalinearinterpolationof
all topics is that the resulting ngram model will have
the same coverage of ngrams as a model that isnt
adapted by topic. However, the similarity score will
be zero when no words overlap between the topic
and history. Therefore we decided to experiment
with similarity score smoothing, which records the
minimum nonzero score and then adds a fraction of
that score to all scores, then only apply upscaling,
where the maximumis scaled to 1, but the minimum
is not scaled to zero. In pilot experiments, we found
that smoothing the scores did not affect topic mod-
eling with traditional topic clusters, but gave minor
improvements when documentswere used as topics.
Stemming is another alternative to improving the
similarity scoring. This helps to reduce problems
with data sparseness by treating different forms of
the same word as topically equivalent. We found
that stemming the cache representations was very
useful when documentswere treated as topics (0.2%
increaseacrosswindow sizes), but detrimentalwhen
larger topics were used (0.10.2% decrease across
window sizes). Therefore, we only use stemming
when documents are treated as topics.
64
2.3 Whats
in a Topic  Topic Granularity
We adapt a language model to the most relevant top-
ics in training text. But what is a topic? Tradition-
ally, document clusters are used for topics, where
some researchers use hand-crafted clusters (Trnka
et al., 2006; Lesher and Rinkus, 2001) and oth-
ers use automatic clustering (Florian and Yarowsky,
1999). However, other researchers such as Mahajan
et al. (1999) have used each individual document as
a topic. On the other end of the spectrum, we can
use whole corpora as topics when training on mul-
tiple corpora. We call this spectrum of topic defini-
tionstopicgranularity, wheremanualandautomatic
document clusters are called medium-grained topic
modeling. When topics are individual documents,
wecalltheapproachfine-grained topicmodeling. In
fine-grained modeling, topics are very specific, such
as seasonal clothing in the workplace, compared to
a medium topic for clothing. When topics are whole
corpora, we call the approach coarse-grained topic
modeling. Coarse-grained topics model much more
high-level topics, such as research or news.
The results of testing on Switchboard across dif-
ferent topic granularities are showin in Table 1. The
in-domain test is trained on Switchboard only. Out-
of-domain training is performed using all other cor-
pora in our collection (a mix of spoken and writ-
ten language). Mixed-domain training combines the
two data sets. Medium-grained topics are only pre-
sented for in-domain training, as human-annotated
topics were only available for Switchboard. Stem-
ming was used for fine-grained topics, but similarity
score smoothing was not used due to lack of time.
The topic granularity experiment confirms our
earlier findings that topic modeling can significantly
improve keystroke savings. However, the variation
of granularity shows that the size of the topics has
a strong effect on keystroke savings. Human anno-
tatedtopicsgive thebestresults,thoughfine-grained
topicmodelinggivessimilarresultswithouttheneed
for annotation, making it applicable to training on
not just Switchboard but other corpora as well. The
coarse grained topic approach seems to be limited
to finding acceptable interpolation weights between
very similar and very dissimilar data, but is poor at
selectingthe mostrelevant corporafroma collection
of very different corpora in the out-of-domain test.
Another problem may be that many of the corpora
are only homogeneous in style but not topic. We
would like to extend our work in topic granularity to
testing on other corpora in the future.
3 Future
Work  Style and Combination
Topic modeling balances the similarity of the train-
ing data against the size by tuning a large training
set to the most topically relevant portions. However,
keystroke savings is not only affected by the topical
similarity of the training data, but also the stylistic
similarity. Therefore, we plan to also adapt models
to the style of text. Our success in adapting to the
topic of conversation leads us to believe that a sim-
ilar process may be applicable to style modeling 
splitting the model into style identification and style
application. Because we are primarily interested in
syntactic style, we will focus on part of speech as
the mechanism for realizing grammatical style. As
a pilot experiment, we compared a collection of our
technical writings on word prediction with a collec-
tion of our research emails on word prediction, find-
ing that we could observe traditional trends in the
POS ngram distributions (e.g., more pronouns and
phrasal verbs in emails). Therefore, we expect that
distributional similarity of POS tags will be useful
for style identification. We envision a single style s
affecting the likelihood of each part of speech p in a
POS ngram model like the one below:
P(w | w1,w2,s) =summationdisplay
pPOS(w)
P(p | p1,p2,s)P(w | p)
In this reformulation of a POS ngram model, the
prior is conditioned on the style and the previous
couple tags. We will use the overall framework to
combine style identification and modeling:
Pstyle(w | h) =
summationdisplay
sstyles
P(s | h)P(w | w1,w2,s)
The topical and stylistic adaptations can be com-
bined by adding topic modeling into the style model
shown above. The POS posterior probability P(w |
p) can be additionally conditioned on the topic of
discourse. Topic identification and the topic sum-
mation would be implemented consistently with the
standalone topic model. Also, the POS framework
65
Model type In-domain Out-of-domain Mixed-domain
Trigram baseline 60.35% 53.88% 59.80%
Switchboard topics (medium grained) 61.48% (+1.12%)  
Document as topic (fine grained) 61.42% (+1.07%) 54.90% (+1.02%) 61.17% (+1.37%)
Corpus as topic (coarse grained)  52.63% (-1.25%) 60.62% (+0.82%)
Table 1: Keystroke savings across different granularity topics and training domains, tested on Switchboard. Improve-
ment over baseline is shown in parentheses. All differences from baseline are significant at p < 0.001
facilitates cache modeling in the posterior, allowing
direct adaptation to the current text, but with less
sparseness than other context-aware models.
4 Conclusions
Wehavecreatedatopicadaptedlanguagemodelthat
utilizesthefulltrainingdata,butwithfocusedtuning
on the most relevant portions. The inclusion of all
the training data as well as the usage of frequencies
addresses the problem of sparse data in an adaptive
model. We have demonstrated that topic modeling
can significantly increase keystroke savings for tra-
ditional testing as well as testing on text from other
domains. We have also addressed the problem of
annotated topics through fine-grained modeling and
found that it is also a significantimprovement over a
baseline ngram model. We plan to extend this work
to build models that adapt to both topic and style.
Acknowledgments
This work was supported by US Department of Ed-
ucation grant H113G040051. I would like to thank
my advisor, Kathy McCoy, for her help as well as
the many excellent and thorough reviewers.

References

Gilles Adda, Mich`ele Jardino, and Jean-Luc Gauvain.
1999. Language modeling for broadcast news tran-
scription. In Eurospeech, pages 17591762.

Jerome R. Bellegarda. 2000. Large vocabulary
speech recognition with multispan language models.
IEEE Transactions on Speech and Audio Processing,
8(1):7684.

Radu Florian and David Yarowsky. 1999. Dynamic
Nonlocal Language Modeling via Hierarchical Topic-
Based Adaptation. In ACL, pages 167174.

Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of a
speech recognizer. IEEE Transactions on Acoustics
Speech and Signal Processing, 35(3):400401.

R. Lau, R. Rosenfeld, and S. Roukos. 1993. Trigger-
based language models: a maximum entropy ap-
proach. In ICASSP, volume 2, pages 4548.
Lillian Lee. 1999. Measures of distributional similarity.
In ACL, pages 2532.

Gregory Lesher and Gerard Rinkus. 2001. Domain-
specific word prediction for augmentative communication. In RESNA, pages 6163.

Gregory W. Lesher, Bryan J. Moulton, and D. Jeffery
Higgonbotham. 1999. Effects of ngram order and
trainingtextsizeonwordprediction. InRESNA, pages
5254.

Jianhua Li and Graeme Hirst. 2005. Semantic knowl-
edge in word completion. In ASSETS, pages 121128.

Milind Mahajan, Doug Beeferman, and X. D. Huang.
1999. Improved topic-dependent language modeling
using information retrieval techniques. In ICASSP,
volume 1, pages 541544.

Johannes Matiasek and Marco Baroni. 2003. Exploiting
long distance collocational relations in predictive typ-
ing. InEACL-03WorkshoponLanguage Modeling for
Text Entry, pages 18.

Alan Newell,Stefan Langer,and Marianne Hickey. 1998.
The role of natural language processing in alternative
and augmentative communication. Natural Language
Engineering, 4(1):116.

Kristie Seymore and Ronald Rosenfeld. 1997. Using
Story Topics for Language Model Adaptation. In Eu-
rospeech, pages 19871990.
Keith Trnka and KathleenF.McCoy. 2007. Corpus Studies in Word Prediction. In ASSETS, pages 195202.

Keith Trnka, Debra Yarrington, Kathleen McCoy, and
Christopher Pennington. 2006. Topic Modeling in
Fringe Word Prediction for AAC. In IUI, pages 276278.

Tonio Wandmacher and Jean-Yves Antoine. 2006.
Training Language Models without Appropriate Language Resources: Experiments with an AAC System
for Disabled People. In LREC.

T. Wandmacher and J.Y. Antoine. 2007. Methods to integrate a language model with semantic information
for a word prediction component. In EMNLP, pages 506513
! META-COMPILING TEXT GRAMMARS AS A MODEL FOR HUMAN BEHAVIOR Sheldon Klein Computer Sciences Department University of Wisconsin I.
BACKGROUND Folktale (Propp 1968) which generated 50 Russian fairytales, according to the r u l e s of his text grammar, at an average speed of 128 words a second, again including plot computations and specification of deep structure as well as surface syntax (Klein et al 1974, Klein et all 1975).
Our earliest automatic text generation work used syntactic dependency network/graphs with 2-valued labelling of edges as an approximation to semantic network/graphs with multi-valued labelling of edges (Klein & Simmons 1963, Klein 1965a, 1965b).
Our work on automatic inference of grammars includes the world's first program for learning context free, phrase structure grammars, for both natural and artificial languages, and the first program for learning transformational grammars (Klein 1967, Klein et al 1968, Klein & Kuppin 1970).
More recent inference work includes the formulation of techniques for automatic inference of generative semantic grammars (Klein 1973) and for the ontogeny of Pidgin and Creole languages (Klein & Rozencvejg 1974).
In formulating components for automatic inference of rules in the meta-symbolic simulation system, we find that the common notation for the semantics of the non-verbal behavioral simulation r u l e s and natural language means that the same learning heuristics may be used to infer behavioral rules as well as linguistic rules.
The implication is that the totality of human verbal and non-verbal behavior, in complex social groups, both synchronically and diachronically, may now be modelled within the same notational framework.
What for us started as a generalized device for testing varying theoretical models as part of an effort to model language change and variation (Klein 1974a, Klein & Rozencvejg 1974) now appears as the basis for a higher level theory of the linguistic basis of human behavior (Klein 1974b).
 i i I  In our efforts to model the totality of synchronic and diachronic language behavior in complex social groups, we developed a meta-symbolic simulation system that includes a powerful behavioral simulation programming language that models, generates and manipulates events in the notation of a semantic network that changes through time, and a generalized, semantics-to-surface structure generation mechanism that can describe changes in the semantic universe in the syntax of any natural language for which a grammar is supplied.
Because the system is a meta-theoretical device, it can handle generative semantic grammars formulated within a variety of theoretical frameworks.
A key feature of the system is that the semantic deep structure of the non-verbal, behavioral rules may be represented in the same network notation as the semantics for natural language grammars, and, as a consequence, provide non-verbal context for linguistic rules.
We are also experimenting with a natural language meta-compiling capability, that is, the use of the semantic network to generate productions in the simulation language itself -productions in the form of "texts" that may themselves be compiled as new behavioral rules during the flow of the simulation -rules that may themselves control the process of deriving new rules.
This feature permits non-verbal behavioral rules to be derived from natural language conversational inputs, and through inference techniques identical with those for inferring natural language generative semantic grammars.
The total system has the power of at least the 2nd order predicate calculus, and will facilitate the formulation of highly abstract meta-models of discourse, including the logical quantification of such models.
Achievements with the generative portion of the system include a text grammar model that generates 2100 word murder mystery stories in less than 19 seconds each, complete with calculation of the plot and specification of the deep structure as well as the surface syntax (Klein et al 1973).
The speed of this generation is 100 to 1000 times faster than other existing programs using transformational grammars.
(The algorithm for the semantics-to-surface structure generative component is such that processing time increases only linearly as a f u n c t i o n of sentence length and syntactic complexity).
More recent achievements include models of portions of Levi-Strauss" mythology work in The Raw & the Cooked (Levi-Strauss 1969) and a model for Propp's Morphology of the I I I I II.
WHAT IS A TEXT GRAMMAR?
The text grammarian movement, centered in Germany and Holland, includes work such as that of van Dijk, Ihwe, Pet~fi and Rieser (1972), Pet6fi and Rieser (1973), Pet~fi (1973), van Dijk (1973), and van Dijk and Pet6fi (1974).
The underlying motivation of this group is the belief that Chomskian derived linguistic theories are inadequate to handle the complexities of complex narrative and discourse -that more powerful logical devices are needed.
An attempted refutation of the text grammarian position appeared in Dascal & Margalit (1974).
Our own work on Propp and Levi-Strauss models refutes the refutation by demonstration (Klein et al 1974).
To provide the reader with an intuitive view of the nature of a text grammar, we offer the following two Russian fairytales generated by our automated model of Propp (Klein et al 1974).
The same text grammar figenerated both stories from a structural model at a level of abstraction that provided a semantic unification of the a ppa r e n t surface diversity.
Tale THE BORISIEVICHES LIVE IN A DISTANCE PROVINCE.
THE FATHER IS EMELYA.
THE ONLY SON IS BORIS.
MARTHA IS THE ONLY DAUGHTER.
EMELYA HAS THE SHEEP.
BORIS, MARTHA AND THE SHEEP ARE IN THE WOODS.
BORIS SAYS MARTHA, DO NOT LEAVE THE WOODS.
BORIS LEAVES TO GO BERRY GATHERING.
MARTHA LEAVES THE WOODS.
A WOLF APPEARS IN THE D I S T A N T PROVINCE.
EMELYA ASKS THE WOLF WHERE IS YOUR WISDOM.
THE WOLF SAYS THAT MY WISDOM IS IN A MAGIC EGG.
THE WOLF P L U N D E R S THE SHEEP.
EMELYA SENDS MARTHA TO SEARCH FOR THE WOLF.
MARTHA DECIDES TO SEARCH FOR THE WOLF.
MARTHA LEAVES ON A SEARCH.
MARTHA MEETS A WITCH ALONG THE WAY.
THE WITCH P R O P O S E S THAT MARTHA LISTEN TO THE GUSLA W I T H O U T FALLING ASLEEP.
MARTHA RESPONDS BY STAYING AWAKE WHILE L I S T E N I N G TO THE GUSLA.
A MAGIC WAFER IS C O N S U M E D BY MARTHA.
M A R T H A O B T A I N S S U P E R H U M A N STRENGTH.
MARTHA T R A V E L S TO THE L O C A T I O N OF THE WOLF IN ANOTHER KINGDOM.
MARTHA IS D I R E C T E D BY A HEDGEHOG.
MARTHA FINDS THE WOLF THEY FIGHT IN AN OPEN FIELD.
MARTHA IS WOUNDED.
MARTHA D E F E A T S THE WOLF WITH THE AID OF S U P E R H U M A N STRENGTH.
THE WOLF IS CAUGHT BY MARTHA.
MARTHA STARTS BACK HOME.
MARTHA RETURNS HOME.
THE MAGIC BOW UNPROTECTED.
THE MAGIC BOW, A MAGIC CARPET AND A MAGIC BOX ARE SEIZED BY NICHOLAS.
NICHOLAS TRAVELS TO THE LOCATION OF THE MAGIC STEED IN ANOTHER KINGDOM.
N I C H O L A S BY THE MAGIC CARPET.
N I C H O L A S FINDS THE BEAR.
N I C H O L A S S U R P R I S E S THE BEAR.
N I C H O L A S KILLS THE BEAR WITH THE AID OF THE MAGIC BOW.
THE MAGIC STEED A P P E A R S FROM THE MAGIC BOX.
N I C H O L A S STARTS BACK HOME.
THE B E A R ' S FATHER CHASES AFTER NICHOLAS.
N I C H O L A S E S C A P E S BY FLYING ON A FALCON.
N I C H O L A S RETURNS HOME.
III. THE KEY Q U E S T I O N We perceive the locus of theoretical interest to be the process of verbal and non-verbal behavior transmission across generations.
Our work on m o d e l l i n g speech c o m m u n i t i e s i n c l u d e s designs for s i m u l a t i o n s in which many modelled individuals, each with his own semantic network, his own grammar(s), his own b e h a v i o r rules, interact with each other a c c o r d i n g to the modelled rules of the social s t r u c t u r e of the s o c i e t y (Klein 1974a).
It is our hope to be able to model the t r a n s m i s s i o n process of all the rules in the system.
This means that newly born m o d e l l e d individuals will infer rules for natural language and also for n o n v e r b a l behavioral s i m u l a t i o n rules, as a function of inputs of texts supplied by other modelled individuals.
The texts may be verbal discourse, or non-verbal sequences of behavior.
The learning individual will a c t u a l l y c o m p i l e and r e c o m p i l e new versions of his own behavioral rules as the s i m u l a t i o n process proceeds.
His own test p r o d u c t i o n s of b e h a v i o r s c e n a r i o s as well as natural language d i s c o u r s e will be subject to evaluation and possible c o r r e c t i o n by o t h e r m e m b e r s of the m o d e l l e d community, and their r e a c t i o n s as well as the c o n s e q u e n c e s of the p r o d u c t i o n s, will serve as a control on the entire learning process.
And, as i n d i c a t e d earlier, the rules to be inferred, compiled and r e c o m p i l e d will include rules that govern the process of inference and c o m p i l a t i o n itself.
Tale THE M O R E V N A S LIVE IN A D I S T A N T PROVINCE.
THE FATHER IS EREMA.
THE MOTHER IS VASILISA.
THE O L D E S T SON IS BALDAK.
THE Y O U N G E R SON IS MARCO.
THE Y O U N G E S T SON IS BORIS.
THE O L D E S T D A U G H T E R IS MARIA.
THE Y O U N G E R D A U G H T E R IS KATRINA.
THE Y O U N G E S T D A U G H T E R IS MARTHA.
N I C H O L A S ALSO L I V E S IN THE SAME LAND.
N I C H O L A S IS OF M I R A C U L O U S BIRTH.
B A L D A K HAS A MAGIC STEED.
A BEAR A P P E A R S IN THE D I S T A N T PROVINCE.
THE BEAR S E I Z E S THE MAGIC STEED.
B A L D A K CALLS FOR HELP FROM NICHOLAS.
N I C H O L A S D E C I D E S TO SEARCH FOR THE MAGIC STEED.
N I C H O L A S LEAVES ON A SEARCH.
N I C H O L A S MEETS A JUG ALONG THE WAY.
THE JUG IS F I G H T I N G WITH ELENA OVER A MAGIC BOW.
THE JUG ASKS N I C H O L A S TO DIVIDE THE MAGIC BOW.
N I C H O L A S TRICKS THE D I S P U T A N T S INTO LEAVING 85 IV.
LOGICAL QUANTIFICATION, PARSING, P R E S U P P O S I T I O N A L A N A L Y S I S SEMANTIC We have mentioned the 2nd order or higher predicate calculus.
For our purposes, the e s s e n t i a l feature is that the logical quantification of the rules may be q u a n t i f i e d by the contents of the rules themselves.
Meta-compiling of rules g o v e r n i n g m e t a c o m p i l i n g is an example of this process.
There are other The b e h a v i o r a l rules classes that make it rules that can treat complex actions as techniques available.
operate with high-level possible to formulate objects, c h a r a c t e r s and manifestations of the fisame abstract semantic Unit.
A major type of b e h a v i o r rule m o d i f i c a t i o n and extension is the a b i l i t y to requantify the rules as a heuristic function of experience.
The process does not involve r e c o m p i l a t i o n -rather m o d i f i c a t i o n of the domain of a p p l i c a b i l i t y of an existing rule.
One of the types of semantic parsing possible in the system is the d e t e r m i n a t i o n of the presuppositions of the semantic content of input text.
The scenario rules that could have generated the text have preconditions, and these p r e c o n d i t i o n s also have their own p r e c o n d i t i o n s as s p e c i f i e d by other rules.
In cases where the semantic content of an input text is not potentially derivable from existing b e h a v i o r a l rules, the system can posit requantification (assignments and r e a s s i g n m e n t s to semantic classes) to make the input text derivable.
Or, if necessary, the same end can be a c h i e v e d by c o m p i l i n g new rules that would make the text plausible.
G e n e r a l i z a t i o n of the method makes it possible to build complex learning models for highly abstract, semantically driven text grammars.
Perhaps the u l t i m a t e test is the m o d e l l i n g of the h e u r i s t i c processes of Levi-Strauss.
We hope to be able to build a model that learns text grammars with arbitrarily a b s t r a c t semantics such as that m a n i f e s t e d in L e v i S t r a u s s (1969).
At the moment, we are w o r k i n g on m o d e l l i n g the text grammar he himself has derived (Klein et al 1975).
The potential of our work is to handle a degree and kind of abstraction in semantics heretofore untouched by linguistics, i n c l u d i n g the m o d e l l i n g of the automatic creation of text grammars for dreams and myths as a function of cultural rules.
GENERALITY OF THE META-SYMBOLIC S I M U L A T I O N SYSTEM AS A THEORY TESTING DEVICE V.
2. The theoretical foundations of Computer Science are identical with those of Linguistics.
3. T h e o r e t i c a l linguistic models that are not strongly linked to objective tasks are meaningless.
No semantics is m e a n i n g f u l except in terms of the o b j e c t i v e tasks it facilitates.
4. The future of Linguistics, Computational Linguistics, Artificial Intelligence, Psychological models of human behavior, are in the future of the Foundations of Programming Languages and the Theory of Operating Systems.
The human mind is at least as complicated as an operating system for a 4th g e n e r a t i o n computer.
5. An a d e q u a t e linguistic theory must account for the function of language in social groups and its transmission through time and space.
At the same time, such a theory must account for the highest semantic a t t a i n m e n t s of the human mind, i n c l u d i n g l i t e r a t u r e and art, and, in fact, the totality of symbolic processes.
6. I n p u t / o u t p u t e q u i v a l e n c e of model and modelled does not imply isomorphism between model and modelled.
(Chomskian beliefs to the c o n t r a r y have their roots in Leibniz" Theory of Monads and its required ontological argument).
There are no models of performance, only models of c o m p e t e n c e which can be compared, one against the other, for accuracy in predicting relations between input and output in real w o r l d systems.
Our m e t h o d o l o g y and programming style have y i e l d e d a system w h e r e i n all the rules, and even the form of the theories in which they are cast, are input as data.
As far as we can determine, this permits us to encode in our system v i r t u a l l y all the t h e o r e t i c a l models c u r r e n t l y p r e v a l e n t in linguistics, plus heretofore unformulated models of vastly greater power.
(Preliminary work in the classroom, for example, indicates that models of the work of Schank and his students may easily be i m p l e m e n t e d in our system, with an i n c r e a s e d speed of e x e c u t i o n of about 50 to I in favor of our versions.) VII.
THEORETICAL IMPLICATIONS of Human Mental A.
The Structures Non-inateness Our work c o n s t i t u t e s a refutation by counter example of the necessity for a c o r r e l a t i o n between models of human mental structures and the s t r u c t u r e of the human brain.
(A s o f t w a r e system can o p e r a t e with no inherent i s o m o r p h i s m s with a p a r t i c u l a r computer).
N o t h i n g need be innate except the meta-compiling capacity and the p e r c e p t i o n of time.
Our work suggests the logical p o s s i b i l i t y that the human mind can learn to learn, and learn how to learn to learn, and that each human may do it differently.
The basic principles of language inference, which can be derived from a b e h a v i o r i s t i c p s y c h o l o g i c a l framework, can alone account for the s t r u c t u r i n g of mental p r o c e s s e s as a software phenomenon, independent of physiological reality.
It follows that humans can have d i f f e r e n t rules, different data structures, different hierarchical 86 VI.
THE M E T H O D O L O G I C A L WORK S I G N I F I C A N C E OF OUR Our work over the years has suggested and r e i n f o r c e d the f o l l o w i n g m e t h o d o l o g i c a l principles: I.
No significant theories formulated in L i n g u i s t i c s not c o m p u t e d based.
can be that are fiorganizations, where the only controlling factor is the requirement that the i n t e r n a l i z e d models permit the individuals to function and interact with the inputs and outputs of other individuals in a social group.
B. History History as the Meta-language of des Linguistics.
Bucharest Implicit in our approach is an alternative to the concept of an infinite h i e r a r c h y of m e t a l a n g u a g e s, as formulated by B e r t r a n d Russell in his Theory of Types in Principia Mathematica (Whitehead & Russell 1911-1913).
The concept of successive states of time, each linked with the p o s s i b i l i t y of defining (meta-compiling) new rules of the universe for the next state, (including the rules for defining new rules), suggests that there need be only a single meta-language and a single language in any state at any point in time, and that each serves, in turn, as the m e t a l a n g u a g e for the other in successive time frames.
This is not a s t o c h a s t i c process.
It is the concepts of time and meta-compiling that appear to be the f u n d a m e n t a l aspects of human cognition.
The principle may be universal for all human b e h a v i o r a l / s y m b o l i c processes, and students of the p h i l o s o p h y of history will now recognize our meta-symbolic simulation system as equivalent to an automated Hegelian dialectic philosophy which specifies that each successive state of h i s t o r i c a l d e v e l o p m e n t is c o n t r o l l e d by the meta-language of its previous state, and becomes the m e t a l a n g u a g e of its successor state.
REFERENCES Klein, S.,. 1973.
Automatic inference of semantic deep structure rules ingenerative semantic Grammars.
Univ. if W i s c o n s i n Comp.
Sci. Tech Report 180.
Also in 1974.
Computational and Mathematical Linguistics, Proceedings of the Int.
Conf. on Computational Linguistics, Pisa, 1973.
A. Zampolli, ed., Florence: Olschki Klein, S., . 1974a.
C o m p u t e r simulation of language contact models.
In Towards Tomorrow's Linguistics, Shuy & Bailey, editors, Washington, D.C.: Georgetown University Press.
Klein, S., . 1974b.
A computer model for the linguistic basis of the t r a n s m i s s i o n of culture.
Presented at 1974 Meeting of American Anthropological Association, Mexico City, Nov.
1974. (Final draft in preparation) Klein, S., J.
F. Aeschlimann, D.
F. Balsiger, S.
L. Converse, C.
Court, M.
Foster, R.
Lao, J.
D. Oakley, and J.
Smith . 1973.
A U T O M A T I C NOVEL WRITING: a status report.
Univ. of Wisc.
Comp. Sci.
Dept. Tech Report 186.
Presented at 1973 Int.
Conf. on Computers in the Humanities.
Klein, S., J.
F. Aeschlimann, M.
A. Appelbaum, D.
F. Balsiger, E.
J. Curtis, M.
Foster, S.
D. Kalish, S.
J. Kamin, Y-D.
Lee, L.
A. Price, D.
F. Salsieder.
1974. M o d e l l i n g Propp and Levi-Strauss in a Meta-symbolic Simulation System.
Univ. of Wisc.
Comp. Sci.
Tech Report 226.
In press in Patterns in Oral Literature, edited by Heda Jason and Dimitri Segal as a retroactive contribution to this volume of the 1973 Wolrd Conference of Anthropological and Ethnological Sciences.
Chicago. Klein, S., W.
Fabens, R.
Herriot, W.
Katke, M.A.
Kuppin, A.
Towster . 1968.
The AUTOLING system.
Univ. of Wisc.
Comp. Sci.
Dept. Tech Report 43.
Klein, S.
and M.
A. Kuppin.
1970. An interactive, heuristic program for learning transformational grammars.
Computer Studies in the Humanities and Verbal Behavior, 3:144-162.
Klein, S., L.
A. Price, J.
F. Aeschlimann, D.
A. Balsiger and E.
J. Curtis.
1975. A Meta-symbolic Simulation Model for Five Myths from Levi-Strauss " The Raw and the Cooked.
Univ. of Wisc.
Comp. Sci.
Dept. Tech Report P r e s e n t e d at 2nd Int.
Conf. on Computers in the Humanities, Chicago, April 1975.
Klein, S.
and V.
Rozencvejg . 1974.
A Computer Model for the Ontogeny of Pidgin and Creole Languages, Univ.
of Wisc.
Comp. Sci.
Dept. Tech Report 238.
Presented at the 1975 Int.
Conf. on Pidgins and Creoles, Hawaii, January 1975.
87 Levi-Strauss, C . 1969, The Raw and the Cooked.
(English translation) New York: Harper & Row.
PetSfi, J.
S . 1973.
Toward an empirically motivated grammatical theory of verbal texts.
In Studies in Text Grammar, edited by J.S.
Petofi & H.
Rieser. Dordrecht: Reidel.
Pet~fi, J.
S. & H.
Rieser. 1973.
Probleme der modelltheoretischen Interpretation yon Texten.
Hamburg: Buske Verlag.
Propp, V . 1968.
Morphology of the Folkt~le (English translation) 2nd Edition, Austin: University of Texas Press.
Whitehead, A.
N. and B.
Russell. 1911-1913.
Princip~a Mathematica.
(3 volumes London: Cambridge University Press .
Limitations of Co-Training for Natural Language Learning from Large Datasets by David Pierce and Claire Cardie References S.
Argamon, I.
Dagan, and Y.
Krymolowski. 1999.
A memory-based approach to learning shallow natural language patterns.
Journal of Experimental and Theoretical Artificial Intelligence, 11(3).
Available as cmp-lg/9806011.
A. Blum and T.
Mitchell. 1998.
Combining labeled and unlabeled data with co-training.
In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT-98).
C. Cardie and D.
Pierce. 1998.
Error-driven pruning of treebank grammars for base noun phrase identification.
In Proceedings of the 36th Annual Meeting of the ACL and COLING-98, pages 218224.
Available as cmp-lg/9808015.
C. Cardie, V.
Ng, D.
Pierce, and C.
Buckley. 2000.
Examining the role of statistical and linguistic knowledge sources in a general-knowledge question answering system.
In Proceedings of the Sixth Applied Natural Language Processing Conference (ANLP-NAACL 2000), pages 180187.
K. Church.
1988. A stochastic parts programs and noun phrase parser for unrestricted text.
In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136143.
D. Cohn, L.
Atlas, and R.
Ladner. 1994.
Improving generalization with active learning.
Machine Learning, 15(2):201221.
M. Collins and Y.
Singer. 1999.
Unsupervised models for named entity classification.
In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99).
A. Dempster, N.
Laird, and D.
Rubin. 1977.
Maximum likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society, Series B, 39(1):138.
Y. Freund and R.
Shapire. 1997.
A decisiontheoretic generalization of on-line learning and an application to boosting.
Journal of Computer and System Sciences, 55(1):119 139.
D. Lewis and J.
Catlett. 1994.
Heterogeneous uncertainty sampling for supervised learning.
In Proceedings of the Eleventh International Conference on Machine Learning, pages 148 156.
M.Marcus, M.
Marcinkiewicz, andB.
Santorini. 1993.
Building a large annotated corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313330.
M. Mitra, C.
Buckley, A.
Singhal, and C.
Cardie. 1997.
An analysis of statistical and syntactic phrases.
In 5TH RIAO Conference, Computer-Assisted Information Searching On the Internet, pages 200214.
I. Muslea, S.
Minton, and C.
Knoblock. 2000.
Selective sampling with redundant views.
In Proceedings of the Seventeenth National Conference on Artificial Intelligence, pages 621 626.
K. Nigam and R.
Ghani. 2000.
Analyzing the effectiveness and applicability of co-training.
In Ninth International Conference on Information and Knowledge Management (CIKM2000).
K. Nigam, A.
McCallum, S.
Thrun, and T.
Mitchell. 2000.
Text classification from labeled and unlabeled documents using EM.
Machine Learning, 39(2/3):103134.
L. Ramshaw and M.
Marcus. 1998.
Text chunking using transformation-based learning.
In Natural Language Processing Using Very Large Corpora.
Kluwer. Originally appeared in WVLC95.
E. Riloff and R.
Jones. 1999.
Learning dictionaries for information extraction by multilevel bootstrapping.
In Proceedings of the Sixteenth National Conference on Artificial Intelligence, pages 474479.
E. Tjong Kim Sang and J.
Veenstra. 1999.
Representing text chunks.
In Proceedings of EACL99.
Available as cs.CL/9907006.
D. Yarowsky.
1995. Unsupervised word sense disambiguation rivaling supervised methods.
In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, pages 189196.
References 1 Thorsten Brants, TnT: a statistical part-of-speech tagger, Proceedings of the sixth conference on Applied natural language processing, p.224-231, April 29-May 04, 2000, Seattle, Washington 2 Eric Brill, Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging, Computational Linguistics, v.21 n.4, p.543-565, December 1995 3 Masaru Tomita, Harry C.
Bunt, Recent Advances in Parsing Technology, Kluwer Academic Publishers, Norwell, MA, 1996 4 Rich Caruana, Multitask Learning, Machine Learning, v.28 n.1, p.41-75, July 1997 5 C.
Chang and C.
Chen. 1993.
A study on integrating chinese word segmentation and part-of-speech tagging.
Communications of COLIPS, 3(1).
6 Radu
Florian, John C.
Henderson, Grace Ngai, Coaxing confidences from an old friend: probabilistic classifications from transformation rule lists, Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics, p.26-34, October 07-08, 2000, Hong Kong 7 Jan Hajic, Barbora Hladk, Tagging inflective languages: prediction of morphological categories for a rich, structured tagset, Proceedings of the 17th international conference on Computational linguistics, August 10-14, 1998, Montreal, Quebec, Canada 8 J.
Hockenmeier and C.
Brew. 1998.
Error-driven segmentation of chinese.
Communications of COLIPS, 8(1):69--84.
9 Mitchell
P.
Marcus, Mary Ann Marcinkiewicz, Beatrice Santorini, Building a large annotated corpus of English: the penn treebank, Computational Linguistics, v.19 n.2, June 1993 10 M.
Munoz, V.
Punyakanok, D.
Roth, and D.
Zimak. 1999.
A learning approach to shallow parsing.
In Proceedings of EMNLP-WVLC'99.
Association for Computational Linguistics.
11 Grace
Ngai, Radu Florian, Transformation-based learning in the fast lane, Second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies 2001, p.1-8, June 01-07, 2001, Pittsburgh, Pennsylvania 12 David D.
Palmer, A trainable rule-based algorithm for word segmentation, Proceedings of the 35th annual meeting on Association for Computational Linguistics, p.321-328, July 07-12, 1997, Madrid, Spain 13 L.
Ramshaw and M.
Marcus. 1994.
Exploring the statistical derivation of transformational rule sequences for part-of-speech tagging.
In The Balancing Act: Proceedings of the ACL Workshop on Combining Symbolic and Statistical Approaches to Language, New Mexico State University, July.
14 L.
Ramshaw and M.
Marcus, 1999.
Natural Language Processing Using Very Large Corpora, chapter Text Chunking Using Transformation-based Learning.
Kluwer. 15 A.
Ratnaparkhi. 1996.
A maximum entropy model for part of speech tagging.
In Proceedings of the First Conference on Empirical Methods in Natural Language Processing, Philadelphia.
16 Richard
Sproat, William Gale, Chilin Shih, Nancy Chang, A stochastic finite-state word-segmentation algorithm for Chinese, Computational Linguistics, v.22 n.3, p.377-404, September 1996 17 Erik F.
Tjong Kim Sang, Sabine Buchholz, Introduction to the CoNLL-2000 shared task: chunking, Proceedings of the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning, September 13-14, 2000, Lisbon, Portugal 18 Erik F.
Tjong Kim Sang, Noun phrase recognition by system combination, Proceedings of the first conference on North American chapter of the Association for Computational Linguistics, p.50-55, April 29-May 04, 2000, Seattle, Washington 19 Dekai Wu, Pascale Fung, Improving Chinese tokenization with linguistic filters on statistical lexical acquisition, Proceedings of the fourth conference on Applied natural language processing, October 13-15, 1994, Stuttgart, Germany 20 Fei Xia, Martha Palmer, Nianwen Xue, Mary Ellen Okurowski, John Kovarik, Fu-Dong Chiou, Shizhe Huang, Tony Kroch, and Mitch Marcus.
2000. Developing guidelines and ensuring consistency for chinese text annotation.
In Proceedings of the second International Conference on Language Resources and Evaluation (LREC-2000), Athens, Greece.
21 Endong Xun, Changning Huang, Ming Zhou, A unified statistical model for the identification of English baseNP, Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, p.109-116, October 03-06, 2000, Hong Kong
References Rod Ellis.
1997. Second Language Acquisition.
Oxford University Press, Oxford.
Pascale Fung.
1998. A statistical view on bilingual lexicon extraction: from parallel corpora to non-parallel corpora.
In Lecture Notes in Artificial Intelligence, pages 117.
Springer Publisher.
Rena Helms-Park.
1997. Building an L2 Lexicon: The Acquisition of Verb Classes Relevant to Causativization in English by Speakers of Hindi-Urdu and Vietnamese.
Ph.D. thesis, University of Toronto, Toronto, Canada.
Nancy Ide.
2000. Cross-lingual sense determination: Can it work?
Computers and the Humanities, 34:223234.
Shunji Inagaki.
1997. Japanese and Chinese learners acquisition of the narrow-range rules for the dative alternation in English.
Language Learning, 47(4):637669.
Alan Juffs.
2000. An overview of the second language acquisition of links between verb semantics and morpho-syntax.
In John Archibald, editor, Second Language Acquisition and Linguistic Theory, pages 170179.
Blackwell Publishers.
Maria Lapata and Chris Brew.
1999. Using subcategorization to resolve verb class ambiguity.
In Proceedings of Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 266274, College Park, MD.
Beth Levin.
1993. English Verb Classes and Alternations: A Preliminary Investigation.
University of Chicago, Chicago.
Diana McCarthy and Anna-Leena Korhonen.
1998. Detecting verbal participation in diathesis alternations.
In Proceedings of the 36th Annual Meeting of the ACL and the 17th International Conference on Computational Linguistics (COLING-ACL 1998), pages 14931495, Montreal, Canada.
I. Dan Melamed and Mitchell P.
Marcus. 1998.
Automatic construction of Chinese-English translation lexicons.
Technical Report 98-28, University of Pennsylvania, Philadelphia, PA.
I. Dan Melamed.
1997. A portable algorithm for mapping bitext correspondence.
In Proceedings of the 35th Conference of the Association for Computational Linguistics, Madrid, Spain.
Paola Merlo and Suzanne Stevenson.
2001. Automatic verb classification based on statistical distributions of argument structure.
Computational Linguistics.
To appear.
Adwait Ratnaparkhi.
1996. A maximum entropy partofspeech tagger.
In Proceedings of The Empirical Methods in Natural Language Processing Conference, Philadelphia, PA.
Philip Resnik and David Yarowsky.
2000. Distinguishing systems and distinguishing senses: New evaluation methods for word sense disambiguation.
Natural Language Engineering, 5(2):113133.
Sabine Schulte im Walde.
2000. Clustering verbs semantically according to their alternation behaviour.
In Proceedings of COLING 2000, pages 747753, Saarbrucken, Germany.
Eric V.
Siegel and Kathleen R.
McKeown. 2000.
Learning methods to combine linguistic indicators: Improving aspectual classification and revealing linguistic insights.
Journal of Computational Linguistics, 26(4):595628, December.
Vivian Tsang.
2001. Second language information transfer in automatic verb classification: A preliminary investigation.
Masters thesis, University of Toronto, Toronto, Canada.
References 1 Sabine Buchholz.
1999. Distinguishing complements from adjuncts using memory-based learning.
ILK, Computational Linguistics, Tilburg University.
2 Michael
Collins and James Brooks.
1995. Prepositional phrase attachment through a backed-off model.
In Proceedings of the Third Workshop on Very Large Corpora, pages 27--38.
3 Bonnie
J.
Dorr, Large-Scale Dictionary Construction for ForeignLanguage Tutoring and Interlingual Machine Translation, Machine Translation, v.12 n.4, p.271-322, 1997 4 Jane Grimshaw.
1990. Argument Structure.
MIT Press.
5 Donald
Hindle, Mats Rooth, Structural ambiguity and lexical relations, Computational Linguistics, v.19 n.1, March 1993 6 Ray Jackendoff.
1977. X' Syntax: A Study of Phrase Structure.
MIT Press, Cambridge, MA.
7 Beth
Levin.
1993. English Verb Classes and Alternations.
University of Chicago Press, Chicago, IL.
8 Matthias
Leybold.
2001. Automatic distinction of pp-arguments and pp-modifiers based on statistic implementations of linguistic criteria: a contribution to the problem of pp-attachment disambiguation.
Master's thesis, University of Geneva.
9 A.
Marantz. 1984.
On the Nature of Grammatical Relations.
MIT Press, Cambridge, MA.
10 Mitchell
P.
Marcus, Mary Ann Marcinkiewicz, Beatrice Santorini, Building a large annotated corpus of English: the penn treebank, Computational Linguistics, v.19 n.2, June 1993 11 Paola Merlo, Matt Crocker, and Cathy Berthouzoz.
1997. Attaching multiple prepositional phrases: Generalized backed-off estimation.
In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 145--154, Providence, RI.
12 George
Miller, R.
Beckwith, C.
Fellbaum, D.
Gross, and K.
Miller. 1990.
Five papers on Wordnet.
Technical report, Cognitive Science Lab, Princeton University.
13 Carl
Pollard, Ivan A.
Sag, Information-based syntax and semantics: Vol.
1: fundamentals, Center for the Study of Language and Information, Stanford, CA, 1988 14 J.
Ross Quinlan, C4.5: programs for machine learning, Morgan Kaufmann Publishers Inc., San Francisco, CA, 1993 15 Adwait Ratnaparkhi, Jeff Reynar, Salim Roukos, A maximum entropy model for prepositional phrase attachment, Proceedings of the workshop on Human Language Technology, March 08-11, 1994, Plainsboro, NJ 16 Ellen Riloff and Mark Schmelzenbach.
1998. An empirical approach to conceptual case frame acquisition.
In Proceedings of the Sixth Workshop on Very Large Corpora, pages 49--56.
17 Carson
T.
Schtze. 1995.
PP Attachment and Argumenthood.
MIT Working Papers in Linguistics, 26:95--151.
18 Srinivas
Bangalore, Aravind K.
Joshi, Supertagging: an approach to almost parsing, Computational Linguistics, v.25 n.2, p.237-265, June 1999 19 Manfred Stede, A generative perspective on verb alternations, Computational Linguistics, v.24 n.3, September 1998
References 1 Chinatsu Aone, Scott Bennett, Applying machine learning to anaphora resolution, Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing, p.302-314, January 1996 2 Breck Baldwin.
1997. CogNIAC: High precision coreference with limited knowledge and linguistic resources.
In Proceedings of the ACL-EACL'97 workshop on operational factors in practical, robust anaphora resolution for unrestricted texts, pages 38--45, Madrid, Spain.
3 Eric
T.
Bell. 1934.
Exponential numbers.
American Mathematical Monthly, 41:411--419.
4 Claire
Cardie and Kiri Wagstaff.
1999. Noun phrase coreference as clustering.
In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99), pages 82--89, College Park, Maryland.
5 Michael
Collins and James Brooks.
1995. Prepositional phrase attachment through a backed-off model.
In Proceedings of the 3rd Workshop on Very Large Corpora (WVLC-3), pages 27--38, Cambridge, Massachusetts.
http://xxx.lanl.gov/abs/cmp-lg/9506021. 6 Dennis Connolly, John D.
Burger, and David S.
Day. 1994.
A machine learning approach to anaphoric reference.
In Proceedings of the International Conference on New Methods in Language Processing, pages 255--261, Manchester, England.
7 Ido
Dagan, Alon Itai, Automatic processing of large corpora for the resolution of anaphora references, Proceedings of the 13th conference on Computational linguistics, p.330-332, August 20-25, 1990, Helsinki, Finland 8 Barbara J.
Grosz, Scott Weinstein, Aravind K.
Joshi, Centering: a framework for modeling the local coherence of discourse, Computational Linguistics, v.21 n.2, p.203-225, June 1995 9 Sanda M.
Harabagiu, Steven J.
Maiorano, Multilingual coreference resolution, Proceedings of the sixth conference on Applied natural language processing, p.142-149, April 29-May 04, 2000, Seattle, Washington 10 Sven Hartrumpf.
1999. Hybrid disambiguation of prepositional phrase attachment and interpretation.
In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99), pages 111--120, College Park, Maryland.
11 Hermann
Helbig and Sven Hartrumpf.
1997. Word class functions for syntactic-semantic analysis.
In Proceedings of the 2nd International Conference on Recent Advances in Natural Language Processing (RANLP'97), pages 312--317, Tzigov Chark, Bulgaria, September.
12 Hermann
Helbig.
2001. Die semantische Struktur natrlicher Sprache: Wissensreprsentation mit MultiNet.
Springer, Berlin.
13 Lynette
Hirschman and Nancy Chinchor.
1997. MUC-7 coreference task definition (version 3.0).
In Proceedings of the 7th Message Understanding Conference (MUC-7).
http://www.itl.nist.gov/iaui/894.02/related_projects/muc/. 14 Nancy Ide, Greg Priest-Dorman, and Jean Vronis, 1996.
Corpus Encoding Standard.
http://www.cs.vassar.edu/CES/. 15 Slava M.
Katz. 1987.
Estimation of probabilities from sparse data for the language model component of a speech recognizer.
IEEE Transactions on Acoustics, Speech and Signal Processing, 35(3):400--401, March.
16 Christopher
Kennedy, Branimir Boguraev, Anaphora for everyone: pronominal anaphora resoluation without a parser, Proceedings of the 16th conference on Computational linguistics, August 05-09, 1996, Copenhagen, Denmark 17 Alois Knoll, Christian Altenschmidt, Joachim Biskup, H.-M.
Blthgen, Ingo Glckner, Sven Hartrumpf, Hermann Helbig, C.
Henning, Reinhard Lling, Burkhard Monien, Thomas Noll, Norbert Sensen, An Integrated Approach to Semantic Evaluation and Content-Based Retrieval of Multimedia Documents, Proceedings of the Second European Conference on Research and Advanced Technology for Digital Libraries, p.409-428, September 21-23, 1998 18 Shalom Lappin, Herbert J.
Leass, An algorithm for pronominal anaphora resolution, Computational Linguistics, v.20 n.4, p.535-561, December 1994 19 Ruslan Mitkov.
1995. An uncertainty reasoning approach for anaphora resolution.
In Proceedings of the Natural Language Processing Pacific Rim Symposium (NLPRS'95), pages 149--154, Seoul, Korea.
20 Ruslan Mitkov.
1997. Two engines are better than one: Generating more power and confidence in the search for the antecedent.
In Ruslan Mitkov and Nicolas Nicolov, editors, Recent Advances in Natural Language Processing: Selected Papers from RANLP'95, pages 225--234.
John Benjamins, Amsterdam.
21 Ruslan Mitkov.
1998a. Evaluating anaphora resolution approaches.
In Proceedings of the Second Colloquium on Discourse Anaphora and Anaphor Resolution (DAARC 2), pages 164--177, Lancaster, England.
22 Ruslan Mitkov, Robust pronoun resolution with limited knowledge, Proceedings of the 36th annual meeting on Association for Computational Linguistics, p.869-875, August 10-14, 1998, Montreal, Quebec, Canada 23 Ruslan Mitkov, Multilingual Anaphora Resolution, Machine Translation, v.14 n.3-4, p.281-299, December 1999 24 Wee Meng Soon, Hwee Tou Ng., and Chung Yong Lim.
1999. Corpus-based learning for noun-phrase coreference resolution.
In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP/VLC-99), pages 285--291, College Park, Maryland.
25 Kees van Deemter, Rodger Kibble, On coreferring: coreference in MUC and related annotation schemes, Computational Linguistics, v.26 n.4, December 2000 26 Renata Vieira, Massimo Poesio, An empirically based system for processing definite descriptions, Computational Linguistics, v.26 n.4, p.539-593, December 2000 27 Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, Lynette Hirschman, A model-theoretic coreference scoring scheme, Proceedings of the 6th conference on Message understanding, November 06-08, 1995, Columbia, Maryland
References 1 Hans Ulrich Block.
2000. Example-Based Incremental Synchronous Interpretation.
In (Wahlster, 2000), pages 411--417.
2 Henrik
Bostrm.
1999. Induction of Recursive Transfer Rules.
In Learning Language in Logic (LLL) Workshop, Bled, Slovenia.
3 Ralf
D.
Brown. 1997.
Automated Dictionary Extraction for "Knowledge-Free" Example-Based Translation.
In TMI-97, pages 111--118.
4 Michael
Carl and Antje Schmidt-Wigger.
1998. Shallow Postmorphological Processing with KURD.
In Proceedings of NeMLaP3/CoNLL98, pages 257--265, Sydney.
5 Halil
Altay Gvenir, Ilyas Cicekli, Learning translation templates from examples, Information Systems, v.23 n.6, p.353-363, Sept.
1998 6 Kevin McTait and Arturo Trujillo.
1999. A Language-Neutral Sparse-Data Algorithm for Extracting Translation Pattern.
In TMI'99.
7 Adam
Meyers, Roman Yangarber, Ralph Grishman, Alignment of shared forests for bilingual corpora, Proceedings of the 16th conference on Computational linguistics, August 05-09, 1996, Copenhagen, Denmark 8 Adam Meyers, Roman Yangarber, Ralph Grishman, Catherine Macleod, and Antonio Moreno-Sandoval.
1998. Deriving transfer rules from dominance-preserving alignments.
In Computerm, First Workshop on Computational Terminology, Montreal, Canada.
9 Harold
Somers, Review Article: Example-based Machine Translation, Machine Translation, v.14 n.2, p.113-157, June 1999 10 Koichi Takeda, Pattern-based machine translation, Proceedings of the 16th conference on Computational linguistics, August 05-09, 1996, Copenhagen, Denmark 11 Wolfgang (ed).
Wahlster. 2000.
Verbmobil: Foundations of Speech-to-Speech Translation.
Springer, Heidelberg.
12 Hideo
Watanabe, Koichi Takeda, A pattern-based machine translation system extended by example-based processing, Proceedings of the 17th international conference on Computational linguistics, p.1369-1373, August 10-14, 1998, Montreal, Quebec, Canada 13 Hideo Watanabe, Sadao Kurohashi, Eiji Aramaki, Finding structural correspondences from bilingual parsed corpus for corpus-based translation, Proceedings of the 18th conference on Computational linguistics, July 31-August 04, 2000, Saarbrcken, Germany 14 Dekai Wu.
1995. Grammarless extraction of phrasal translation examples from parallel texts.
In TMI-95.
15 Menno
van Zaanen, ABL: alignment-based learning, Proceedings of the 18th conference on Computational linguistics, July 31-August 04, 2000, Saarbrcken, Germany
References 1 Andreas Abecker and Klaus Schmid.
1996. From theory refinement to kb maintenance: a position statement.
In ECAI'96, Budapest, Hungary.
2 Clifford
Alan Brunk.
1996. An investigation of Knowledge Intensive Approaches to Concept Learning and Theory Refinement.
Ph.D. thesis, University of California, Irvine.
3 Herv
Djean.
2000a. Theory refinement and natural language learning.
In COLING'2000, Saarbrcken.
4 Herv
Djean.
2000b. A use of xml for machine learning.
In Proceeding of the workshop on Computational Natural Language Learning, CONLL'2000.
5 David
Mckelvie, 2000.
XML QUERY 2.0.
Edinburgh. http://www.ltg.ed.ac.uk/software/ttt/.
6 Raymond
J.
Mooney, Induction Over the Unexplained: Using Overly-General Domain Theories to Aid Concept Learning, Machine Learning, v.10 n.1, p.79-110, Jan.
1993 7 Raymond J.
Mooney, Inductive Logic Programming for Natural Language Processing, Selected Papers from the 6th International Workshop on Inductive Logic Programming, p.3-22, August 26-28, 1996 8 Erik F.
Tjong Kim Sang and Herv Djean.
2001. Introduction to the conll-2001 shared task: Clause identification.
In Proceedings of CoNLL, shared task.
References 1 X.
Carreras and L.
Mrquez. 2001.
Boosting Algorithms for Anti-Spam E-mail Filtering.
Submitted to Recent Advances in Natural Language Processing RANLP'01.
2 Yoav
Freund, Robert E.
Schapire, A decision-theoretic generalization of on-line learning and an application to boosting, Journal of Computer and System Sciences, v.55 n.1, p.119-139, Aug.
1997 3 Robert E.
Schapire, Yoram Singer, Improved Boosting Algorithms Using Confidence-rated Predictions, Machine Learning, v.37 n.3, p.297-336, Dec. 1999
G36G71G68G83G87G76G81G74G3G68G81G71G3G72G91G87G72G81G71G76G81G74G3G79G72G91G76G70G68G79G3G85G72G86G82G88G85G70G72G86G3G76G81G3G68G3G71G76G68G79G82G74G88G72G3G86G92G86G87G72G80 G36G81G68G3G42G68G85G70G116G68G16G54G72G85G85G68G81G82 ISYS Group AI Department Technical University of Madrid Boadilla del Monte, 28660 Madrid, Spain agarcia@ dia.fi.upm.es G51G68G79G82G80G68G3G48G68G85G87G116G81G72G93 Advanced DB Group CS Department Universidad Carlos III de Madrid Avda.
Universidad 30 Legans 28911, Madrid, Spain pmf@inf.uc3m.es G47G88G76G86G3G53G82G71G85G76G74G82 ISYS Group AI Department Technical University of Madrid Boadilla del Monte, 28660 Madrid, Spain lrodrigo@isys.dia.fi.upm.es G36G69G86G87G85G68G70G87 This paper presents the adaptation and customization of two lexical resources: Brill tagger, Brill (1992), and EuroWordNet, Vossen et al.(1998), to be used in the ADVICE project devoted to build an intelligent virtual reality sales and service system that uses human language technology.
G20G3 G44G81G87G85G82G71G88G70G87G76G82G81 The work described in this paper is comprised in the ADVICE 1 project, which consists of the development of a new interface for a craftsmanship tools e-commerce system.
With the aim of providing full support to customers of online shops and to the users of electronic services on the Internet during the complete customer service lifecycle; the ADVICE system will provide: An advanced multimedia user interface supporting natural language text input and output, as well as an animated assistant, that ensures a high level of user-system interaction.
A software environment for building intelligent sales assistants for two types of selling and marketing services: sales-service and after sales service.
The virtual assistant supported by this application will be capable of interacting with the user in natural language, adapt its recommendations to the requirements and 1 Virtual Sales Assistant for the Complete Customer Service Process in Digital Markets.
IST Project 1999-11305 characteristics of the customers and provide explanations of the advised products as well as of the different interesting alternatives.
The dialogue is focussed on accomplishing a specific task, ordering products in an ecommerce system as well as getting advice about those products.
The complexity of this domain lies in the broad product range with specialization of the crafts for particular tasks and applications.
One of the main aspects of the buyingselling interaction in the web is the capability of the web site of generating some kind of trust feeling in the buyer, just like a human shop assistant would do in a person-to-person interaction.
Things like understanding the buyer needs, being able to give him technical advice, assisting him in the final decision are not easy things to achieve in a web selling site.
Natural language (NL) techniques can play a crucial role in providing this kind of enhancements.
Another good motivation to integrate natural language technology in this kind of sites is to make the interaction easy to those people less confident with the Internet or even computer technologies.
Inexperienced users feel much more comfortable expressing themselves and receiving information in natural language rather than through the human standard ways.
An ADVICE project objective is to build a generic language processing component following a language engineering approach, that is, to achieve maintainability, time optimization, robustness, flexibility, domain adaptability and generality.
These features require profiting from the existing resources taking into account that the goal is to achieve systems that work in real domains.
Therefore, resources adaptation is a crucial step when developing this kind of systems, because these resources are not usually compatible with domain requirements.
At current state two linguistic resources have been analyzed and adapted in order to be incorporated into the ADVICE project: Brill tagger, Brill (1992), and EuroWordNet, Vossen et al.(1998). The paper is structured as follows: next section focuses on a brief description of the NL module of ADVICE project; section 3 is devoted to explain how the corpus study has influenced the customization of the lexical resources; sections 4 and 5 outline the Brill tagger and EuroWordnet extension to cover specific domain peculiarities and, finally, in section 6 some conclusions and future work are presented.
G21G3 G50G89G72G85G89G76G72G90G3 G82G73G3 G49G68G87G88G85G68G79G3 G47G68G81G74G88G68G74G72 G70G82G80G83G82G81G72G81G87 Figure 1 shows the architecture of the ADVICE system.
The Interface Agent includes the G49G47G3 G44G81G87G72G85G83G85G72G87G72G85G3 G68G81G71G3 G42G72G81G72G85G68G87G82G85 component that is in charge of analyzing the user utterances as well as of generating the appropriate answers according to the dialogue.
The input sentence is interpreted in order to obtain a semantic representation.
At this moment, two interpretation strategies are defined.
Firstly, message extraction techniques useful in specific domains are used in ADVICE, implemented by means of semantic grammars reflecting e-commerce generic sentences and idioms, sublanguage specific patterns and keywords.
Secondly, if the pattern matching analysis does not work successfully, a robust processor that makes use of several linguistic resources (Brill tagger, EuroWordNet and a Phrase Segmenter) integrates syntactic and semantic analysis in different ways.
The aim is to attach the identified (syntactic or semantic) segments of the sentence in a partial structure to go on with the dialogue.
Concerning the generation of answers, a domain-specific template-based approach is proposed.
The templates used to generate natural language answers can be propositional (if they require arguments to fill the slots in) or not propositional (for instance, agreements, rejections and topic movements).
The information gathering for the G49G47 G44G81G87G72G85G83G85G72G87G72G85G3 G68G81G71G3 G42G72G81G72G85G68G87G82G85 involves different sources: dialogues collected from users, the database of products and users and common sense about human-computer interaction.
G22G3 G39G82G80G68G76G81G3G70G82G85G83G88G86G3G86G87G88G71G92 When dealing with such a restricted domain, a great amount of the possibilities of success relies on the domain adaptation capability that the system proves.
As long as an intelligent, open domain and human-computer dialogue is far from being realistic, the designers usually focus on the customization of the available resources so that they are as trustworthy as possible when applied to selected input, even though this implies a loss of accuracy when it has to do with out-of-thedomain texts.
The decision of directly adapting existing resources comes from the fact that the initial G41G76G74G88G85G72G3G20G29G3G36G39G57G44G38G40G3G68G85G70G75G76G87G72G70G87G88G85G72 G41G76G74G88G85G72G3G21G29G3G51G68G85G87G76G68G79G3G89G76G72G90G3G82G73G3G70G85G68G73G87G3G71G82G80G68G76G81 G82G81G87G82G79G82G74G92 GUI c44c49c55c40c53c41c36c38 c36c42c40c49 SHOP In ter net W A P c44c49c55c40c53c36c38c55c44c50 c36c42c40c49 c44c49c55c40c47c47c44c42c40c49 c36c42c40c49 Session Model NL Interpreter and Generator Dialogue Manager User Model Problem Solver Domain Model updated discourse memory 3D Character Pattern Matching Analysi Robust Processin Semantic Grammars Brill Tagger Phrase Segmenter Euro Wordnet User Utterance NL Interpreter Semantic Structures Interaction Agent Answe Generation Domain Templates NL Generator Semantic Structures Output Utterances MATERIAL FEATURES MODEL TYPE TOOL TYPE ACCESSORIESACTION OBJECT FEATURES useful for has acts on made of is a contains characterized by belongs to characterized by amount of data available was almost nonexistent.
In other situation we could have considered the possibility of developing the resources from scratch or, at least, training them (specially the tagger) so that its knowledge was completely acquired for this application, but that was too time consuming.
A Wizard of Oz experiment was conducted to gain some domain specific data to work with.
A corpus of 527 sentences resulted from this experiment, which were lexically and syntactically annotated.
As a side effect of this experiment, the members of the development team considerably increased their knowledge and familiarity with the domain characteristics.
The corpus analysis has been performed keeping in mind the need to separate domainindependent aspects of the system from the domain-specific components that serve to define specific application domains.
Regarding the adaptation of the resources, both of them (Brill tagger and EuroWordnet) were affected by the results of the abovementioned process.
As long as more and more terminology was studied, it became evident the highly structured way in which it could be organized.
Although both lexical resources are explained in detail in sections 4 and 5, some special issues relating to domain-based customization are outlined below.
After a careful study, the ontology shown in figure 2 was developed, having each of the boxes a complete set of specific terminology (see figure 3).
At this stage EuroWordnet had been understood to bring robustness to the NL analysis contributing with its huge amount of words in the form of a semantic network, but the similarity of the proposed ontology with EuroWordnet built-in structure lead us to a proposal of extending the EuroWordnet semantic network with the specific vocabulary regarding our context and, even more important, the relationships among them.
To carry out this proposal, it was firstly considered the option of making use of the existing relations of EuroWordnet and its domain labels.
We will add the terminology and definitions that were not previously integrated, and tag all the specific terminology with a domain label, so that it could be directly identified as a word of special interest for our needs and related to other words in the domain.
Unfortunately the first study of this possibility revealed that it turned to be a process much less intuitive than it appears at first sight, and a deeper study about how this process could be implemented is being carried out at the moment of writing this paper.
Concerning Brill tagger, its revision was also affected by the aforementioned corpus analysis.
First, the sentences extracted from the corpus were analyzed in order to obtain a set of generic patterns (about 90 patterns).
This had direct repercussions in the set of contextual rules (the rules that the tagger uses to select the correct tag for a word considering the context in which it appears) which were adapted to fit the domain peculiarities detected in the patterns, as shown in figure 4.
Some new rules were developed and some of the existent were modified.
The set of tags was also changed to help in the domain customization process.
A new tag XX was defined to tag the words not present in the lexicon.
The words tagged with this tag are good candidates to be domain terminology and are stored in a special file to be analyzed.
As a result of this process, after the tagging of a sufficient amount of text, we have a collection of words that are good candidates to build up a specialized lexicon.
This lexicon can be used as a support for the original one, filling the gaps G41G76G74G88G85G72G3G22G29G3G39G72G87G68G76G79G3G82G73G3G87G75G72G3G70G82G81G87G72G81G87G3G82G73G3G68 G83G68G85G87G76G70G88G79G68G85G3G70G68G87G72G74G82G85G92G3G82G73G3G87G75G72G3G82G81G87G82G79G82G74G92 MATERIAL aluminum plastic alloy steel...
PVC chipboard plaster wood ...
G41G76G74G88G85G72G3G23G29G3G44G81G73G79G88G72G81G70G72G3G82G73G3G86G72G81G87G72G81G70G72G3G83G68G87G87G72G85G81G86 G82G81G3G70G82G81G87G72G91G87G88G68G79G3G85G88G79G72G86 is the best choice for ? What Key verb Object Whi ch c54c72c81c87c72c81c70c72c3c83c68c87c87c72c85c81 contextual_rule([for/Tag1,Word2/NN|InputRest],[for/Tag1,Word2/VBG|OutputRest]):ps(Word2,TagList), belongs(VBG,TagList), !, apply_cr(InputRest, OutputRest).
c38c82c81c87c72c91c87c88c68c79c3c85c88c79c72 that it may have when dealing with specific language, and can be further refined as more and more terms are tagged as XX.
Next sections focus on the main topics of Brill tagger and EuroWordnet extension and adaptation.
G23G3 G36G71G68G83G87G76G81G74G3G37G85G76G79G79G3G55G68G74G74G72G85 The Brill tagger, Brill (1992), is a rule-based part of speech tagger programmed in Perl for English language.
A morphological analysis of a word form produces a set of possible base forms with associated inflectional information.
For each occurrence of a word form in context, a POS (Part-of-Speech) tagger discriminates which of these base forms is more likely in the context.
Our objective with this tagger was twofold.
Firstly, the tagger was translated (and softly modified) into Prolog, and then, an exhaustive study was carried out in order to adapt it to ADVICE domain.
The development environment used in ADVICE project, Ciao Prolog, Bueno et al.(1999), motivates the translation of resources into Prolog.
Furthermore, Prolog allows rapid development and facilitates efficient and realtime processing.
The tagger has three kinds of knowledge: a base lexicon, contextual rules and lexical rules.
The G79G72G91G76G70G82G81 is a long list of words with one or more possible tags associated.
It is used to the first assignment of tags to the input text.
The original knowledge is stored in a text file where each line contains the word and the possible tags that can be associated to the word.
There are 93696 different entries.
This lexicon was directly translated into Prolog facts, having the following structure: ps(Word, ListOfTags).
The words are stored as Prolog terms and have been alphabetically sorted to take advantage of the indexing capabilities of Prolog.
Here is a glance at the file contents: ps(charts,[NNS,VBZ]).
ps(chary,[JJ]).
ps(chase,[NN,JJ,VB,VBP).
ps(chased,[VBN,VBD]).
ps(chasers,[NNS]).
ps(chasing,[VBG,NN]).
The G70G82G81G87G72G91G87G88G68G79G3G85G88G79G72G86 manage the knowledge relative to the suitable tags for a word regarding the context in which it appears.
It considers both the words and the tags that surround the word under consideration and decides about the correctness of the preassigned tag.
The original file contains 284 rules, stored in a text file in the following format: NN VB PREVTAG TO, which means change a NN (noun, singular or mass) tag to VB (verb, base form) if the previous tag is TO.
These contextual rules have been translated into a set of Prolog rules plus a predicate that goes through the input and applies the appropriate rule if necessary until it reaches the end of the text to tag.
Every time it changes a tag, it checks that the new tag is contained in the set of allowed tags for that word.
In other case, the rule is not applied.
Below, the main predicate and an example rule are shown: apply_cr([],[]).
apply_cr(List1,List2):contextual_rule(List1,List2).
contextual_rule( [Word1/TO,Word2/NN|InputRest], [Word1/TO,Word2/VB|OutpRest]):ps(Word2,TagList), belongs(VB,TagList), !, apply_cr(InputRest,OutpRest).
As in the case of the original Brill, the order of the rules is relevant to the final result, as one of the early rules may affect the context of a word so that a later rule may (or may not) be applicable.
The G79G72G91G76G70G68G79G3G85G88G79G72G86 are used to infer the most possible tag for a word considering its lexical shape, specially its prefixes and suffixes.
From the 148 original rules, only the 63 regarding to suffixes have been translated at this moment.
The structure of the original rules is: ly hassuf 2 RB representing if the word has the suffix, of length 2, "-ly", assign the tag RB (adverb).
The structure of the Prolog predicates containing these lexical rules is equal to the one from the contextual rules; we have an "apply_lr" predicate and several "lexical_rule", combined in the same way.
Finally, once all the resources have been translated, the last part was the simulation of the tagging process.
This has been carried out considering the information in Brill (1992), Brill (1994) and Brill (1995), taken as a reference starting point.
The designed process works as follows: first of all, the input is pre-processed to decompose the possible contractions.
Next, each word in the input is attached its most probable tag, taking as the most probable the first one in the list of the lexicon.
If a word is not present in the lexicon, it is assigned a special tag XX representing G88G81G78G81G82G90G81G3G90G82G85G71 instead of the noun tag that it was assigned in the original tagger.
Then, lexical rules are applied (if possible) trying to disambiguate the words that were not found in the lexicon.
Once every word has a tag, we apply the contextual rules, resulting in the definitive tagged text.
Apart from the already mentioned modification of the general process of tagging, some modifications in the knowledge bases have been made, basically in order to adapt the resource to our domain (craft tools).
Regarding the G79G72G91G76G70G82G81, we have adapted the punctuation marks tags to our needs, changing the tags that the original Brill tagger used.
The objective was to assign a different tag to every punctuation mark, giving them a treatment equal to the words.
These symbols are relevant when we are looking for a sentence syntax analysis or a sentence pattern.
Another change to the lexicon is related with the order of the tags in certain words, and is further explained in the next paragraph.
The original set of contextual rules was firstly translated as is.
With this set of rules, a corpus of 20 dialogues restricted to the domain language obtained by the wizard-of-Oz experiments was tagged.
We compared the results to the manually tagged text and we extracted some common mistakes that the tagger was making (Table 1).
These mistakes could lead us in two directions.
If it was a mistagging of a particular word frequently repeated, the best way to fix it was to change the order of the tags associated to that word in the lexicon.
G38G82G80G80G82G81G3G80G76G86G87G68G78G72G86 G54G82G79G88G87G76G82G81G86 -Mistagging of a particular word.
-Change the tag preference in the lexicon.
-Problems tagging word categories.
-Modifications in the set of rules.
-Lexical rules applied to already known words.
-Assign tag XX to unknown words.
G55G68G69G79G72G20G29G3G55G68G74G74G72G85G3G72G85G85G82G85G86 For example, in our domain the word "saw" appears most of the times as a noun while the preferred tag in the lexicon for that word was verb.
In this case, placing the noun tag before the verb one in the tag list turns to be a good and simple solution.
If the problem we were facing was not related to particular words, but to word categories, the way to fix it was through the adding/deleting/modification of the contextual rules.
After the study of the errors, we developed several new rules.
For instance, a rule for changing a tag 'IN' (meaning preposition or subordinating conjunction) to RB (adverb) in the collocation As ...
as. Finally, concerning the lexical rules, the main change comes from the fact that the words that were not in the lexicon, in the original Brill tagger were annotated as 'NN', while now they have an special tag 'XX'.
As these lexical rules try to disambiguate these kind of words, whenever an original rule was applied to a word tagged as 'NN', it has been adapted to act on the words tagged as 'XX'.
Finally, only if none of the rules has been able to disambiguate it, it is automatically changed to 'NN'.
G24G3 G36G71G68G83G87G76G81G74G3G40G88G85G82G58G82G85G71G81G72G87 The first step in adapting EuroWordNet database was to translate it into Prolog.
EuroWordNet stores its knowledge in file texts that are organized in a very structured way.
As a first approach, the information extracted from the databases has been the synonymy, hyponymy and hyperonymy basic relations.
This is the minimum information needed to preserve the semantic structure that lies beyond EuroWordNet.
The resulting lexical database stores the information in the predicate ewn/5 as Prolog facts.
The first field references the word of interest, the second stores the grammatical category of the word, and the following, the synonyms, hyperonyms and hyponyms, respectively, as is shown below: ewn(phonograph, n, sin([record player]), hyper([machine]), hypo([acoustic gramophone, jukebox])).
From the analysis of domain application, an ontology of concepts incorporating the terminology has been obtained (a partial view was shown in Figure 2).
Intensive work is being made in order to merge EuroWordNet and domain ontology structures, resulting in an enlarged resource fully adapted to our needs and useful both as a lexicon and as a semantic network.
Figure 4 shows an EuroWordNet partial structure from Vossen et al.(1998) on which the vocabulary corresponding to two kinds of concepts from the domain ontology (OBJECT and ACTION) have been integrated (colored rounded boxes with a dotted line).
Moreover, a new semantic relation named G36G70G87G86G3 G82G81 is also shown to represent the association between OBJECT and ACTION.
However, some of the relationship provided by EuroWordNet (apart from synonymy, hiponymy, and meronymy) could probably cover some of the relations present in the domain ontology.
In the same way, TOOL terminology is inserted as G44G81G86G87G85G88G80G72G81G87G178G33G41G88G81G70G87G76G82G81G178 G33G20G86G87G50G85G71G72G85G40G81G87G76G87G92; terminology MATERIAL G68G86 G54G88G69G86G87G68G81G70G72G178G33G3G41G82G85G80G178G33G20G86G87G50G85G71G72G85G40G81G87G76G87G92, etc.
G25G3 G38G82G81G70G79G88G86G76G82G81 The most important aspects of Brill tagger and EuroWordNet adaptation to a specific application domain and a development platform have been explained.
These resources are going to be used in a dialogue system in order to give robustness to the NL interpretation process.
We are currently involved in the process of implementation of the enhancements proposed in the paper.
Concerning Brill tagger, only preliminary tests have been carried out.
The first translation of the tagger was used to tag the available corpus, showing an accuracy of 0.9163.
Although this result is far from the state of the art taggers, it is hopeful result, as the improvements concerning the full terminology and complete set of contextual rules had still been incorporated.
Results up to state of the art levels are expected as the enhancements are implemented.
Related EuroWordNet, we are studying if the semantic relations (apart from synonymy, hiponymy, meronymy) supported by EuroWordNet are enough to cover the relationships shown in the domain ontology of References 1 Antonietta Alonge, Definition of the links and subsets for verbs, Centre National de la Recherche Scientifique, Paris, France, 1996 2 Bloksma (1996) Bloksma, L., Dez-Orzas, P., Vossen, P., User requirements and functional specification of the eurowordnet project.
Technical report, Deliverable D001.
3 Eric
Brill, A simple rule-based part of speech tagger, Proceedings of the third conference on Applied natural language processing, March 31-April 03, 1992, Trento, Italy 4 Eric Brill, Some advances in transformation-based part of speech tagging, Proceedings of the twelfth national conference on Artificial intelligence (vol.
1), p.722-727, October 1994, Seattle, Washington, United States 5 Eric Brill, Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging, Computational Linguistics, v.21 n.4, p.543-565, December 1995 6 Bueno et al.(1999), F.
Bueno, D.
Cabeza, M.
Carro, M.
Hermenegildo, P.
Lpez, and G.
Puebla. The Ciao Prolog System: A Next Generation Logic Programming Environment, REFERENCE MANUAL.
The Ciao System Documentation Series Technical Report CLIP 3/97.1, The CLIP Group School of Computer Science Technical University of Madrid.
7 Antonietta
Alonge, Definition of the links and subsets for verbs, Centre National de la Recherche Scientifique, Paris, France, 1996 8 Gonzalo et al.(1998) Gonzalo, J., Verdejo, M.
F., Chugur, I., Lpez, F., Peas, A., Extraccin de relaciones semnticas entre nombres y verbos en EuroWordnet.
Revista SEPLN no.
23, 1998.
9 Mark
Hepple, Independence and commitment: assumptions for rapid training and execution of rule-based POS taggers, Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, p.278-277, October 03-06, 2000, Hong Kong 10 Kilgarriff (1997) Kilgarriff, A., Foreground and background lexicons and word sense disambiguation for information extraction.
Proc. Workshop on Lexicon Driven Information Extraction, Frascati, Italy, July 1997.
11 Martnez
et al.(2000), Martnez, P.
Garca-Serrano, A., The role of knowledge-based technology in language applications development.
Expert Systems with Applications 19, 31--44, 2000.
12 Miller
et al.(1990), Miller, G.
A., Beckwith, R., Fellbaum, C., Gross, D., Miller, K., Introduction to WordNet: An On-line Lexical Database.
(Revised August 1993).
Princeton University, New Jersey.
13 Vossen
et al.(1998), Vossen, P., Bloksma, L., Rodrguez, H., Climent, H., Calzolari, N., Roventini, A., Bertagna, F., Alonge, A., Peters, W., The EuroWordNet Base Concepts and Top Ontology.
Version2. EuroWordNet (LE 4003) Deliverable .
References 1 D.
Battistelli. 2000.
Passer du texte  une squence d'images, analyse spatio-temporelle de textes, modelisation et ralisation informatique (systme SPAT).
Ph.D. thesis, Universit Paris Sorbonne.
2 S.
Ben Hazez and J.-L.
Minel. 2000.
Designing tasks of identification of complex patterns used for text-filtering.
In RIAO, pages 1558--1567.
3 J.-P Descls, E.
Cartier, A.
Jackiewicz, and J.-L.
Minel. 1997.
Textual processing and contextual exploration method.
In CONTEXT'97, pages 189--197, Brasil.
Universidade Federal do Rio de Janeiro.
4 J.-P.
Descls, 1997.
Systmes d'exploration contextuelle.
Co-texte et calcul du sens, pages 215 232.
Presses Universitaires de Caen.
5 L.
Ferro, I.
Mani, B.
Sundheim, and G.
Wilson. 2001.
Tides temporal annotation guidelines.
Technical Report MTR 00W0000094, The MITRE Corporation.
6 Inderjeet
Mani, George Wilson, Lisa Ferro, Beth Sundheim, Guidelines for annotating temporal information, Proceedings of the first international conference on Human language technology research, p.1-3, March 18-21, 2001, San Diego 7 J.-L.
Minel and J-P.
Descls. 2000.
Rsum Automatique et Filtrage des textes.
Ingnierie des langues.
Editions Herms, Paris.
8 Nikolai
Vazov, Context-Scanning Strategy in Temporal Reasoning, Proceedings of the Second International and Interdisciplinary Conference on Modeling and Using Context, p.389-402, September 01, 1999 9 H.
Verkuyl. 1993.
A theory of aspectuality.
The interaction between temporal and atemporal structure.
Cambridge Studies in Lingustics.
Cambridge University Press.
10 Bonnie
Lynn Webber, Tense as discourse anaphor, Computational Linguistics, v.14 n.2, June 1988 11 Dina Wonsever, Jean-Luc Minel, Contextual Rules for Text Analysis, Proceedings of the Second International Conference on Computational Linguistics and Intelligent Text Processing, p.509-523, February 18-24, 2001
References 1 Nello Cristianini, John Shawe-Taylor, An introduction to support Vector Machines: and other kernel-based learning methods, Cambridge University Press, New York, NY, 1999 2 Taku Kudoh, Yuji Matsumoto, Use of support vector learning for chunk identification, Proceedings of the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning, September 13-14, 2000, Lisbon, Portugal 3 Taku Kudoh.
2000. Tinysvm: Support vector machines.
http://cl.aist-nara.ac.jp/ takuku//software/Tiny SVM/index.html.
4 Sadao
Kurohashi and Makoto Nagao, 1998.
Japanese Morphological Analysis System JUMAN version 3.5.
Department of Informatics, Kyoto University.
(in Japanese).
5 Masaki
Murata, Qing Ma, Kiyotaka Uchimoto, and Hitoshi Isahara.
1999. An example-based approach to Japanese-to-English translation of tense, aspect, and modality.
In TMI '99, pages 66--76.
6 Masaki
Murata, Kiyotaka Uchimoto, Qing Ma, Hitoshi Isahara, Bunsetsu identification using category-exclusive rules, Proceedings of the 18th conference on Computational linguistics, p.565-571, July 31-August 04, 2000, Saarbrcken, Germany 7 Masaki Murata, Masao Utiyama, Kiyotaka Uchimoto, Qing Ma, and Hitoshi Isahara.
2001. Correction of the modality corpus for machine translation based on machine-learning method.
7th Annual Meeting of the Association for Natural Language Processing.
(in Japanese; the English translation of this paper is available at http://arXiv.org/abs/cs/0105001).
8 Eric
Sven Ristad.
1997. Maximum Entropy Modeling for Natural Language.
ACL/EACL Tutorial Program, Madrid.
9 Eric
Sven Ristad.
1998. Maximum Entropy Modeling Toolkit, Release 1.6 beta.
http://www.mnemonic.com/software/memt. 10 Satoshi Sekine, The domain dependence of parsing, Proceedings of the fifth conference on Applied natural language processing, p.96-102, March 31-April 03, 1997, Washington, DC 11 Hirotoshi Taira and Masahiko Haruno.
2000. Feature selection in svm text categorization.
Transactions of Information Processing Society of Japan, 41(4):1113--1123.
(in Japanese).
References 1 Tilman Becker and Stephan Busemann, editors.
1999. May I Speak Freely?
Between Templates and Free Choice in Natural Language Generation.
Workshop at the 23rd German Annual Conference for Artificial Intelligence (KI '99), Saarbrcken.
DFKI. 2 R.
Cooper, S.
Larsson, M.
Poesio, D.
Traum, and C.
Matheson. 1999.
Coding instructional dialogue for information states.
In Task Oriented Instructional Dialogue (TRINDI): Deliverable 1.1.
University of Gothenburg, Gothenburg.
3 Gregor
Erbach, ProFIT: prolog with features, inheritance and templates, Proceedings of the seventh conference on European chapter of the Association for Computational Linguistics, March 27-31, 1995, Dublin, Ireland 4 Jonathan Ginzburg and Ivan Sag.
2000. English Interrogative Constructions.
Studies in Constraint-based Lexicalism.
CSLI Publications, Stanford, California.
5 Jonathan
Ginzburg, Howard Gregory, and Shalom Lappin.
2001. SHARDS: Fragment resolution in dialogue.
In Harry Bunt, Ielka van der Sluis, and Elias Thijse, editors, Proceedings of the 4th International Workshop on Computational Semantics (IWCS-4), pages 156--172, Tilburg.
6 Jonathan
Ginzburg.
2001. Clarification ellipsis and nominal anaphora.
In H.
Bunt, editor, Computing meaning, volume 2.
Kluwer, Dordrecht.
7 Howard
Gregory and Shalom Lappin.
1999. Antecedent contained ellipsis in HPSG.
In G.
Webelhuth, J.
P. Koenig, and A.
Kathol, editors, Lexical and Constructional Aspects of Linguistic Explanation, pages 331--356.
CSLI Publications, Stanford.
8 Martin
Kay, Chart generation, Proceedings of the 34th annual meeting on Association for Computational Linguistics, p.200-204, June 24-27, 1996, Santa Cruz, California 9 Kathleen R.
McKeown, Text generation: using discourse strategies and focus constraints to generate natural language text, Cambridge University Press, New York, NY, 1985 10 Nicolas Nicolov and Chris Mellish.
2000. PROTECTOR: Efficient Generation with Lexicalized Grammars.
In Recent Advances in Natural Language Processing, Current Issues in Linguistic Theory (CILT 189), pages 221--243.
John Benjamin, Amsterdam & Philadelphia.
11 Carl
Pollard and Ivan Sag.
1994. Head Driven Phrase Structure Grammar.
University of Chicago Press and CSLI Publications, Chicago.
12 Ehud
Reiter.
1995. NLG vs.
templates. In Proceedings of the Fifth European Workshop on Natural-Language Generation (ENLGW-1995), Leiden, The Netherlands.
13 Ivan
Sag.
1997. English relative clause constructions.
Journal of Linguistics, 33:431--484.
14 Stuart
M.
Shieber, Gertjan van Noord, Fernando C.
N. Pereira, Robert C.
Moore, Semantic-head-driven generation, Computational Linguistics, v.16 n.1, p.30-42, March 1990
References 1 Jens Allwood.
1976. Linguistic Communication as Action and Cooperation.
Department of Linguistics, University of Gteborg.
Gothenburg Monographs in Linguistics 2.
2 Apache
XML Project.
2001. Xalan-Java version 2.1.0. http://xml.apache.org/xalanj/index.html.
3 Alison
Cawsey, Presenting tailored resource descriptions: will XSLT do the job?, Proceedings of the 9th international World Wide Web conference on Computer networks : the international journal of computer and telecommunications netowrking, p.713-722, June 2000, Amsterdam, The Netherlands 4 Morena Danieli and Elisabetta Gerbino.
1995. Metrics for evaluating dialogue strategies in a spoken language system.
In Proceedings of the AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, pages 34--39.
5 Kristiina
Jokinen, Hideki Tanaka, and Akio Yokoo.
1998. Planning dialogue contributions with new information.
In Proceedings of the Ninth International Workshop on Natural Language Generation, pages 158--167, Niagara-on-the-Lake, Ontario.
6 Kristiina
Jokinen.
2000. Learning dialogue systems.
In L.
Dybkjaer, editor, LREC 2000 Workshop: From Spoken Dialogue to Full Natural Interactive Dialogue Theory, Empirical Analysis and Evaluation, pages 13--17, Athens.
7 Masako
Kume, Gayle K.
Sato, Kei Yoshimoto, A descriptive framework for translating speaker's meaning: towards a dialogue translation system between Japanese and English, Proceedings of the fourth conference on European chapter of the Association for Computational Linguistics, p.264-271, April 10-12, 1989, Manchester, England 8 Diane J.
Litman, Shimei Pan, Predicting and Adapting to Poor Speech Recognition in a Spoken Dialogue System, Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence, p.722-728, July 30-August 03, 2000 9 Ehud Reiter, Robert Dale, Building natural language generation systems, Cambridge University Press, New York, NY, 2000 10 Marit Theune.
2000. From Data to Speech: Language Generation in Context.
Ph.D. thesis, Eindhoven University of Technology.
11 David
R.
Traum, James F.
Allen, Discourse obligations in dialogue processing, Proceedings of the 32nd annual meeting on Association for Computational Linguistics, p.1-8, June 27-30, 1994, Las Cruces, New Mexico 12 Markku Turunen and Jaakko Hakulinen.
2000. Jaspis a framework for multilingual adaptive speech applications.
In Proceedings of 6th International Conference on Spoken Language Processing, Beijing.
13 Kees
van Deemter, Emiel Krahmer, and Marit Theune.
1999. Plan-based vs.
template-based NLG: A false opposition?
In Proceedings of the KI'99 Workshop: May I Speak Freely?, pages 1--5, Saarbrcken.
14 Marilyn
Walker, Diane Litman, Candace Kamm, and Alicia Abella.
1998. Evaluating spoken dialogue agents with PARADISE: Two case studies.
Computer Speech and Language, 12--3.
References 1 James Allen and Mark Core.
1997. Draft of DAMSL: Dialog act markup in several layers.
2 Peter
Bohlin (Ljunglf), Robin Cooper, Elisabet Engdahl, and Staffan Larsson.
1999. Information states and dialogue move engines.
In Jan Alexandersson, editor, IJCAI-99 Workshop on Knowledge and Reasoning in Practical Dialogue Systems.
3 Lou
Burnard.
2000. Reference Guide for the British National Corpus (World Edition).
Oxford University Computing Services.
4 Jean
Carletta, Assessing agreement on classification tasks: the kappa statistic, Computational Linguistics, v.22 n.2, June 1996 5 Herbert H.
Clark. 1996.
Using Language.
Cambridge University Press.
6 Charles
Fletcher.
1994. Levels of representation in memory for discourse.
In Morton Ann Gernsbacher, editor, Handbook of Psycholinguistics.
Academic Press.
7 Jonathan
Ginzburg, Robin Cooper, Resolving ellipsis in clarification, Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, p.236-243, July 06-11, 2001, Toulouse, France 8 Jonathan Ginzburg and Robin Cooper.
forthcoming. Clarification, ellipsis and utterance representation.
9 Jonathan
Ginzburg and Ivan Sag.
2000. Interrogative Investigations: the Form, Meaning and Use of English Interrogatives.
Number 123 in CSLI Lecture Notes.
CSLI Publications.
10 Jonathan
Ginzburg, Howard Gregory, and Shalom Lappin.
2001a. SHARDS: Fragment resolution in dialogue.
In Harry Bunt, Ielka van der Sluis, and Elias Thijsse, editors, Proceedings of the Fourth International Workshop on Computational Semantics (IWCS-4), pages 156--172.
ITK, Tilburg University, Tilburg.
11 Jonathan
Ginzburg, Ivan A.
Sag, and Matthew Purver.
2001b. Integrating conversational move types in the grammar of conversation.
In P.
Khnlein, H.
Rieser, and H.
Zeevat, editors, Proceedings of the Fifth Workshop on Formal Semantics and Pragmatics of Dialogue.
BIDIALOG. 12 Jonathan Ginzburg.
1996. Interrogatives: Questions, facts and dialogue.
In Shalom Lappin, editor, The Handbook of Contemporary Semantic Theory, pages 385--422.
Blackwell. 13 Staffan Larsson, Peter Ljunglf, Robin Cooper, Elisabet Engdahl, Stina Ericsson, GoDiS: an accommodating dialogue system, ANLP/NAACL 2000 Workshop on Conversational systems, p.7-10, May 04-04, 2000, Seattle, Washington 14 Matthew Purver.
2001. SCoRE: A tool for searching the BNC.
Technical report, Department of Computer Science, King's College London.
15 John
R.
Ross. 1969.
Guess who?
In R.
I. Binnick, A.
Davison, G.
Green, and J.
Morgan, editors, Papers from the Fifth Regional Meeting of the Chicago Linguistic Society, pages 252--286.
CLS, University of Chicago.
16 Jacqueline
D.
Sachs. 1967.
Recognition memory for syntactic and semantic aspects of connected discourse.
Perception and Psychophysics, 2:437--442.
17 David
Rood Traum, A computational theory of grounding in natural language conversation, University of Rochester, Rochester, NY, 1995 18 Teun A.
van Dijk and Walter Kintsch.
1983. Strategies of Discourse Comprehension.
Academic Press.
References 1 Paolo Baggia, Giuseppe Castagneri, Morena Danieli, Field trials of the Italian ARISE train timetable system, Speech Communication, v.31 n.4, p.355-367, Aug.
2000 2 L.
Lamel, S.
Rosset, J.
L. Gauvain, S.
Bennacef, M.
Garnier-Rizet, B.
Prouts, The LIMSI ARISE system, Speech Communication, v.31 n.4, p.339-353, Aug.
2000 3 Lavelle, A.
C., Calmes, H., Prennov, G., 1999.
"Confirmation strategies to improve correction rates in a telephonic inquiry dialogue system".
EUROSPEECH, Budapest, Hungary.
4 Macas-Guarasa, J.
Ferreiros, J., San-Segundo, R., Montero, J.
M., and Pardo, JM., 2000.
"Acoustical and Lexical Based Confidence Measures for a Very Large Vocabulary Telephone Speech Hypothesis-Verification System".
ICSLP. Beijing, China.
5 Pellom, B., Ward, W., Pradhan, S.
2000. "The CU Communicator: An Architecture for Dialogue Systems".
ICSLP, Beijing, China.
6 Rudnicky, A., Bennet, C., Black, A., Chotomongcol, A., Lenzo, K., Oh, A., Singh, R.
2000. "Task and domain specific modelling in the Carnegie Mellon Communicator system".
ICSLP, Beijing, China.
7 San-Segundo, R., Pellom, B., Hacioglu, K., Ward, W., and Pardo, JM., 2001.
"Confidence measures for Spoken Dialogue Systems".
ICASSP, Salt-Lake-City, Utah, USA.
8 San-Segundo, R., Cols, J., Ferreiros, J., Macas-Guarasa, J., Pardo, J.
M., 2000.
"Spanish Recognizer of continuously spelled names over the telephone".
ICSLP, Beijing, China.
9 Sturm, J., den Os, E., and Boves, L., 1999 "Issues in Spoken Dialogue Systems: Experiences with the Dutch ARISE System".
Proceedings of ESCA Workshop on Interactive Dialogue in MultiModal Systems.
Kloter Irsee, Germany, 1--4.
10 Veldhuijzen
van Zanten, G., 1999.
"User modelling in adaptative dialogue management".
EUROSPEECH, Budapest, Hungary.
References 1 Paolo Baggia, Giuseppe Castagneri, Morena Danieli, Field trials of the Italian ARISE train timetable system, Speech Communication, v.31 n.4, p.355-367, Aug.
2000 2 Nielsole Ole Bernsen, L.
Dybkjaer, Laila Dybkjaer, Designing Interactive Speech Systems: From First Ideas to User Testing, Springer-Verlag New York, Inc., Secaucus, NJ, 1997 3 L.
F. Lamel, S.
K. Bennacef, S.
Rosset, L.
Devillers, S.
Foukia, J.
J. Gangolf, J.
L. Gauvain, The LIMSI RailTel system: field trial of a telephone service for rail travel information, Speech Communication, v.23 n.1-2, p.67-82, Oct.
1997 4 L.
Lamel, S.
Rosset, J.
L. Gauvain, S.
Bennacef, M.
Garnier-Rizet, B.
Prouts, The LIMSI ARISE system, Speech Communication, v.31 n.4, p.339-353, Aug.
2000 5 Lavelle, A.
C., Calmes, H., Prennov, G., 1999.
"Confirmation strategies to improve correction rates in a telephonic inquiry dialogue system".
Proc. of EUROSPEECH, Budapest, Hungary.
Vol. 3, pp.
1399--1402. 6 Macas-Guarasa, J., Ferreiros, J.
Cols, J., Gallardo, A., and Pardo.
J. M., 2000a.
"Improved Variable List Preselection List Length Estimation Using NNs in a Large Vocabulary Telephone Speech Recognition System".
Proc. of ICSLP, Beijing, China.
Vol. II, pp.
823--826. 7 Macas-Guarasa, J., Ferreiros, J., San-Segundo, R., Montero, J.
M., and Pardo, J.
M., 2000b.
"Acoustical and Lexical Based Confidence Measures for a Very Large Vocabulary Telephone Speech Hypothesis-Verification System".
Proc. of ICSLP.
Beijing, China.
Vol. IV, pp.
446--449. 8 Montero, J.
M., Crdoba, R., Vallejo, J.
A., Gutirrez-Arriola, J., Enrquez, E., Pardo, J.
M., 2000.
"Restricted-domain female-voice synthesis in Spanish: from database design to a prosodic modeling".
Proc. of ICSLP.
Beijing, China.
9 Pellom, B., Ward, W., Pradhan, S., 2000.
"The CU Communicator: An Architecture for Dialogue Systems".
Proc. of ICSLP, Beijing, China.
10 Riccardi, G., Gorin, A., 2000.
"Stochatic Language Adaptation over time and state in natural spoken dialog systems".
IEEE Trans.
on Speech and Audio Processing, Vol 8, pp.
3--10. 11 Rudnicky, A., Bennet, C., Black, A., Chotomongcol, A., Lenzo, K., Oh, A., Singh, R.
2000. "Task and domain specific modelling in the Carnegie Mellon Communicator system".
Proc. of ICSLP, Beijing, China.
12 San-Segundo, R., Cols, J., Ferreiros, J., Macas-Guarasa, J., Pardo, J.
M., 2000.
"Spanish Recognizer of continuously spelled names over the telephone".
Proc. of ICSLP, Beijing, China.
13 H.
Schramm, B.
Rueber, A.
Kellner, Strategies for name recognition in automatic directory assistance systems, Speech Communication, v.31 n.4, p.329-338, Aug.
2000 14 Sturm, J., den Os, E., and Boves, L., 1999 "Issues in Spoken Dialogue Systems: Experiences with the Dutch ARISE System".
Proceedings of ESCA Workshop on Interactive Dialogue in MultiModal Systems.
Kloter Irsee, Germany, 1--4.
15 Veldhuijzen
van Zanten, G., "User modelling in adaptive dialogue management".
1999. Proc.
of EUROSPEECH, Budapest, Hungary.
Vol. 3, pp.
1183--1186. 16 Zue, V., Seneff, S., Glass, J., Hetherington, L., Hurley, E., Meng, H., Pao, C., Polifroni, J., Schloming, R., Schmid, P., 1997.
"From interface to content: transclingual access and delivery of on-line information".
Proc. of EUROSPEECH, Athenas, Greece.
References 1 Baggia P., Kelner A., Prennou E., Popovici C., Strum J., Wessel F.
1999. Language Modeling and Spoken Dialogue Systems the ARISE experience.
Eurospeech99, pp.
1767--1770. 2 Bonafonte A., Castell N., LLeida E., Mario J.
B., Sanchis E., Torres M.
I., Aibar P.
2000. Desarrollo de un sistema de dilogo oral en dominios restringidos.
I Jornadas en Tecnologa del Habla, Sevilla.
3 L.
Lamel, S.
Rosset, J.
L. Gauvain, S.
Bennacef, M.
Garnier-Rizet, B.
Prouts, The LIMSI ARISE system, Speech Communication, v.31 n.4, p.339-353, Aug.
2000 4 Martinez C., and Casacuberta F.
2000. A pattern recognition approach to dialog labelling using finite-state transducers.
V Iberoamerican Symposium on Pattern Recognition, pp.
669--677. 5 Pieraccini R, Levin E., and Eckert W.
1997. AMICA: the AT&T Mixed Initiative Conversational Architecture.
Eurospeech97, pp.
1875--1878. 6 Segarra E., Sanchis E., Galiano M., Garca F., Hurtado L.
F. 2001.
Extracting semantic information through automatic learning techniques.
IX Spanish Symposium on Pattern Recognition and Image Analysis (AERFAI), Castelln.
References 1 J.
Allen and R.
Perrault. 1980.
Analyzing intention in utterances.
Artificial Intelligence, 15:143--178.
2 L.
Burnard. 2000.
Reference Guide for the British National Corpus (World Edition).
Oxford Universtity Computing Services.
Accessible from: ftp://sable.ox.ac.uk/pub/ota/BNC/.
3 Sandra
Carberry, Plan Recognition in Natural Language Dialogue, MIT Press, Cambridge, MA, 1990 4 Jean Carletta, Assessing agreement on classification tasks: the kappa statistic, Computational Linguistics, v.22 n.2, June 1996 5 R.
Cooper, S.
Larsson, J.
Hieronymus, S.
Ericsson, E.
Engdahl, and P.
Ljunglof. 2001.
GODIS and Questions Under Discussion.
In The TRINDI Book.
University of Gothenburg, Gothenburg.
Available from http://www.ling.gu.se/research/projects/trindi.
6 M.
Dalrymple, F.
Pereira, and S.
Shieber. 1991.
Ellipsis and higher order unification.
Linguistics and Philosophy, (14):399--452.
7 E.
Engdahl, S.
Larsson, and S.
Ericsson. 2000.
Focus-ground articulation and parallelism in a dynamic model of dialogue.
Technical report, Trindi:Task Oriented Instructional Dialogue.
Accessible from: http://www.ling.gu.se/research/projects/trindi.
8 R.
Fernndez. 2002.
An Implemented HSPG Grammar for SHARDS.
Technical Report TR-02--04, Department of Computer Science.
King's College London.
9 J.
Ginzburg and R.
Cooper. 2001a.
Clarification, ellipsis, and the nature of contextual updates.
Under review for Linguistics and Philosophy.
10 Jonathan
Ginzburg, Robin Cooper, Resolving ellipsis in clarification, Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, p.236-243, July 06-11, 2001, Toulouse, France 11 J.
Ginzburg and I.
Sag. 2001.
Interrogative Investigations.
CSLI Publications.
12 J.
Ginzburg, H.
Gregory, and S.
Lappin. 2001.
SHARDS: Fragment resolution in dialogue.
In Proceedings of the Fourth International Workshop on Computational Semantics.
13 J.
Ginzburg. 1996.
Interrogatives: Questions, facts, and dialogue.
In Shalom Lappin, editor, Handbook of Contemporary Semantic Theory.
Blackwell, Oxford.
14 J.
Ginzburg. 1997.
Structural mismatch in dialogue.
In Proceedings of MunDial 97 (Technical Report 97--106), pages 59--80.
Universitaet Muenchen Centrum fuer Informationsund Sprachverarbeitung, Muenchen.
15 J.
Ginzburg. 1999.
Ellipsis resolution with syntactic presuppositions.
In Harry Bunt and Reinhard Muskens, editors, Computing Meaning: Current Issues in Computational Semantics.
Kluwer. 16 J.
Ginzburg. 2002.
A semantics for interaction in dialogue.
Forthcoming for CSLI Publications.
Draft chapters available from: http://www.dcs.kcl.ac.uk/staff/ginzburg.
17 Staffan
Larsson, Peter Ljunglf, Robin Cooper, Elisabet Engdahl, Stina Ericsson, GoDiS: an accommodating dialogue system, ANLP/NAACL 2000 Workshop on Conversational systems, p.7-10, May 04-04, 2000, Seattle, Washington 18 S.
Larsson. 1998.
Questions under discussion and dialogue moves.
In J.
Hulstijn and A.
Nijholt, editors, Proceedings of TwenDial 98, 13th Twente workshop on Language Technology.
Twente University, Twente.
19 Matthew
Purver, Jonathan Ginzburg, Patrick Healey, On the means for clarification in dialogue, Proceedings of the Second SIGdial Workshop on Discourse and Dialogue, p.1-10, September 01-02, 2001, Aalborg, Denmark 20 M.
Purver. 2001.
SCoRE: A tool for searching the BNC.
Technical Report TR-01-07, Department of Computer Science, King's College London.
21 Matthew Purver, Processing unknown words in a dialogue system, Proceedings of the 3rd SIGdial workshop on Discourse and dialogue, p.174-183, July 11-12, 2002, Philadelphia, Pennsylvania
References 1 James F.
Allen and Mark G.
Core. 1997.
Damsl: Dialog annotation markup in several layers.
Unpublished Manuscript.
2 Claude
Barras, Edouard Geoffrois, Zhibiao Wu, Mark Liberman, Transcriber: Development and use of a tool for assisting speech corpora production, Speech Communication, v.33 n.1-2, p.5-22, Jan.
2001 3 Steve Cassidy, Jonathan Harrington, Multi-level annotation in the Emu speech database management system, Speech Communication, v.33 n.1-2, p.61-77, Jan.
2001 4 Don Colton, Ron Cole, David G.
Novick, and Stephen Sutton.
1996. A laboratory course for designing and testing spoken dialogue systems.
In Proceedings of the International Conference on Audio, Speech and Signal Processing (ICASSP), pages 1129--1132.
5 Mark
G.
Core and James F.
Allen. 1997.
Coding dialogs with the DAMSL annotation scheme.
In Working notes of the AAAI Fall Symposium on Communicative Action in Humans and Machines.
6 Entropic
Research Laboratory, Inc., 1993.
WAVES+ Reference Manual.
Version 5.0. 7 George Ferguson.
1998. DAT: Dialogue annotation tool.
Available from www.cs.rochester.edu in the subdirectory research/speech/damsl.
8 Giovanni
Flammia.
1995. N.b.: A graphical user interface for annotating spoken dialogue.
In AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation, pages pages 40--46, Stanford, CA.
9 Giovanni
Flammia, Victor W.
Zue, Discourse segmentation of spoken dialogue: an empirical approach, 1998 10 Barbara J.
Grosz, Candace L.
Sidner, Attention, intentions, and the structure of discourse, Computational Linguistics, v.12 n.3, p.175-204, July-September 1986 11 Peter A.
Heeman, James F.
Allen, Dialogue Transcription Tools, University of Rochester, Rochester, NY, 1995 12 Peter A.
Heeman and James F.
Allen. 1995b.
The Trains spoken dialog corpus.
CD-ROM, Linguistics Data Consortium, April.
13 Peter
A.
Heeman, James F.
Allen, Speech repairs, intonational phrases, and discourse markers: modeling speakers' utterances in spoken dialogue, Computational Linguistics, v.25 n.4, p.527-571, December 1999 14 Arne Jnsson, Nils Dahlbck, Distilling dialogues: a method using natural dialogue corpora for dialogue systems development, Proceedings of the sixth conference on Applied natural language processing, p.44-51, April 29-May 04, 2000, Seattle, Washington 15 Michael Kipp.
2001. Anvil: A generic annotation tool for multimodal dialogue.
In Proceedings of the 7th European Conference on Speech Communication and Technology (Eurospeech).
16 Per
Linell.
1998. Approaching Dialogue: Talk, Interaction and Contexts in Dialogical Perspectives.
John Benjamins Publishing.
17 David
McKelvie, Amy Isard, Andreas Mengel, Morten Baun Mller, Michael Grosse, and Marion Klein.
2001. The MATE workbench --an annotation tool for XML coded speech corpora.
Speech Communications, 33:97--112.
18 John
F.
Pitrelli, Mary E.
Beckman, and Julia Hirschberg.
1994. Evaluation of prosodic transcription labeling reliability in the ToBI framework.
In Proceedings of the 3rd International Conference on Spoken Language Processing (ICSLP-94), Yokohama, September.
19 Deborah
Schiffrin.
1987. Discourse Markers.
Cambridge University Press, New York.
20 Susan E.
Strayer and Peter A.
Heeman. 2001.
Dialogue structure and mixed initiative.
In Second workshop of the Special Interest Group on Dialogue, pages 153--161, Aalborg Denmark, September.
21 Stephen Sutton, Ed Kaiser, Andrew Cronk, and Ronald Cole.
1997. Bringing spoken language systems to the classroom.
In Proceedings of the 5th European Conference on Speech Communication and Technology (Eurospeech), Rhodes, Greece.
22 S.
Sutton, R.
Cole, J.
de Villiers, J.
Schalkwyk, P.
Vermeulen, M.
Macon, Y.
Yan, E.
Kaiser, R.
Rundle, K.
Shobaki, P.
Hosom, A.
Kain, J.
Wouters, M.
Massaro, and M.
Cohen. 1998.
Universal speech tools: the cslu toolkit.
In Proceedings of the 5th International Conference on Spoken Language Processing (ICSLP-98), pages 3221--3224, Sydney Australia, November.
23 David R.
Traum and Christine H.
Nakatani. 1999.
A two-level approach to coding dialogue for discourse structure: Activities of the 1998 working group on higher-level structures.
In Proceedings of the ACL'99 Workshop Towards Standards and Tools for Discourse Tagging, pages 101--108, June.
24 Fan Yang, Susan E.
Strayer, and Peter A.
Heeman. 2002.
ACT: a graphical dialogue annotation comparison tool.
Submitted for publication.
References 1 Petra Barg, Markus Walther, Processing unknown words in HPSG, Proceedings of the 36th annual meeting on Association for Computational Linguistics, August 10-14, 1998, Montreal, Quebec, Canada 2 Lou Burnard.
2000. Reference Guide for the British National Corpus (World Edition).
Oxford University Computing Services.
3 David
Carter.
1992. Lexical acquisition.
In H.
Alshawi, editor, The Core Language Engine, pages 217--234.
MIT Press, Cambridge, MA.
4 Ann
Copestake, Dan Flickinger, Ivan Sag, and Carl Pollard.
1999. Minimal recursion semantics: An introduction.
Draft. 5 Gregor Erbach.
1990. Syntactic processing of unknown words.
In P.
Jorrand and V.
Sgurev, editors, Artificial Intelligence IV methodology, systems, applications.
North-Holland, Amsterdam.
6 Simon
Garrod and Martin Pickering.
2001. Toward a mechanistic psychology of dialogue: The interactive alignment model.
In P.
Khnlein, H.
Rieser, and H.
Zeevat, editors, Proceedings of the Fifth Workshop on Formal Semantics and Pragmatics of Dialogue.
BI-DIALOG. 7 Jonathan Ginzburg, Robin Cooper, Resolving ellipsis in clarification, Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, p.236-243, July 06-11, 2001, Toulouse, France 8 Jonathan Ginzburg and Robin Cooper.
forthcoming. Clarification, ellipsis and utterance representation.
9 Jonathan
Ginzburg, Howard Gregory, and Shalom Lappin.
2001a. SHARDS: Fragment resolution in dialogue.
In H.
Bunt, I.
van der Sluis, and E.
Thijsse, editors, Proceedings of the Fourth International Workshop on Computational Semantics (IWCS-4), pages 156--172.
ITK, Tilburg University, Tilburg.
10 Jonathan
Ginzburg, Ivan A.
Sag, and Matthew Purver.
2001b. Integrating conversational move types in the grammar of conversation.
In P.
Khnlein, H.
Rieser, and H.
Zeevat, editors, Proceedings of the Fifth Workshop on Formal Semantics and Pragmatics of Dialogue (BI-DIALOG 2001), pages 45--56.
11 Richard
H.
Granger. 1977.
FOUL-UP: A program that figures out meanings of words from context.
In Proceedings of the 5th International Joint Conference on Artificial Intelligence (IJCAI-77), volume 1, pages 172--178, August.
12 Howard
Gregory.
2001. A ProFIT grammar and resolution procedure for fragments in dialogue.
Technical Report TR-01-03, Department of Computer Science, King's College London, May.
13 Peter
Mark Hastings, Automatic acquisition of word meaning from context, University of Michigan, Ann Arbor, MI, 1994 14 Kevin Knight.
1996. Learning word meanings by instruction.
In Proceedings of the Thirteenth National Conference on Artifical Intelligence, pages 447--454.
AAAI/IAAI. 15 Ted Pedersen.
1995. Automatic acquisition of noun and verb meanings.
Technical Report 95-CSE-10, Southern Methodist University, June.
16 Matthew
Purver.
2001. SCoRE: A tool for searching the BNC.
Technical Report TR-01-07, Department of Computer Science, King's College London, October.
17 James
Pustejovsky.
1998. The semantics of lexical underspecification.
Folia Linguistica, 32(3--4):323--347.
18 Adwait
Ratnaparkhi.
1996. A maximum entropy part-of-speech tagger.
In Proceedings of the Empirical Methods in Natural Language Processing Conference.
University of Pennsylvania, May.
19 Dale
W.
Russell, Language acquisition in a unification-based grammar processing system using a real-world knowledge base, University of Illinois at Urbana-Champaign, Champaign, IL, 1993 20 Cynthia Ann Thompson, Raymond Joseph Mooney, Semantic lexicon acquisition for learning natural language interfaces, 1998 21 David Traum, Johan Bos, Robin Cooper, Staffan Larsson, Ian Lewin, Colin Matheson, and Massimo Poesio.
1999. A model of dialogue moves and information state revision.
In Task Oriented Instructional Dialogue (TRINDI): Deliverable 2.1.
University of Gothenburg.
22 Kees van Deemter.
1996. Towards a logic of ambiguous expressions.
In K.
van Deemter and S.
Peters, editors, Semantic Ambiguity and Underspecification, number 55 in CSLI Lecture Notes.
CSLI Publications.
23 Uri Zernik.
1987. Language acquisition: Learning a hierarchy of phrases.
In Proceedings of the 10th International Joint Conference on Artificial Intelligence (IJCAI-87), volume 1, pages 125--132, August.
References 1 C.
Blasche, M.
Andrade, C.
Ouzounis, and A.
Valencia. 1999.
Automatic extraction of biological information from scientific text: protein-preotein interactions.
In AAAI.
2 J.
Castao, J.
Zhang, and Pustejovsky.
2002. Anaphora resolution in biomedical literature.
In International Symposium on Reference Resolution in NLP, Alicante, Spain.
3 Mark
Craven, Johan Kumlien, Constructing Biological Knowledge Bases by Extracting Information from Text Sources, Proceedings of the Seventh International Conference on Intelligent Systems for Molecular Biology, p.77-86, August 06-10, 1999 4 Donald Hindle, Deterministic parsing of syntactic non-fluencies, Proceedings of the 21st annual meeting on Association for Computational Linguistics, June 15-17, 1983, Cambridge, Massachusetts 5 T.
Hishiki, N.
Collier, C.
Nobata, T.
Okazaki-Ohta, N.
Ogata, T.
Sekimizu, R.
Steiner, H.
S. Park, and J.
Tsujii. 1998.
Developing nlp tools for genome informatics: An information extraction perspective.
In Proc.
of Genome Informatics, Tokyo, Japan, pages 81--90.
6 B.
L. Humphreys, D.
A. B.
Lindberg, Schoolman H.
M., and Barnett G.
O. 1998.
The unified medical language system: An informatics research collaboration.
Journal of the American Medical Informatics Association, (5).
7 Christopher
Kennedy, Branimir Boguraev, Anaphora for everyone: pronominal anaphora resoluation without a parser, Proceedings of the 16th conference on Computational linguistics, August 05-09, 1996, Copenhagen, Denmark 8 A.
J. Link, J.
Eng, D.
M. Schieltz, E.
Carmack, G.
J. Mize, D.
R. Morris, B.
M. Garvik, and J.
R. 3rd.
Yates. 1999.
Direct analysis of protein complexes using mass spectrometry.
Nature Biotechnology, (17):676--82.
9 David
D.
McDonald, Robust partial-parsing through incremental, multi-algorithm processing, Text-based intelligent systems: current research and practice in information extraction and retrieval, Lawrence Erlbaum Associates, Inc., Mahwah, NJ, 1992 10 T.
Ono, H.
Hishigaki, A.
Tanigami, and T.
Takagi. 2001.
Automatic extraction of information on protein-protein interactions from the biological literature.
Bioinformatics, pages 155--161.
11 J.
Pustejovsky and P.
Hanks. July, 2001.
Very large lexical databasees: A tutorial primer.
In Association for Computational Linguistics, Toulouse.
12 J.
Pustejovsky, B.
Boguraev, M.
Verhagen, P.
Buitelaar, and M.
Johnston. 1997.
Semantic indexing and typed hyperlinking.
In AAAI Symposium on Language and the Web, Stanford, CA.
13 J.
Pustejovsky, J.
Castao, J.
Zhang, and B.
Cochran. 2002a.
Robust relational parsing over biomedical literature: Extracting inhibit relations.
In Pacific Symposium on Biocomputing.
14 J.
Pustejovsky, B.
Cochran, J.
Castao,, M.
Morrell, J.
Zhang, and R.
Saur. 2002b.
Medstract: Natural language tools for mining the biobibliome.
In Demo presented at HLT-2002, San Diego, CA.
15 J.
Pustejovsky, A.
Rumshinsky, and J.
Castao. 2002c.
Automatic extensions to UMLS through corpus analytics.
In Ontolex 2002.
16 J.
Pustejovsky, J.
Castao, and B.
Cochran. in preparation.
Exploiting given versus new information for information extraction tasks.
17 Thomas
C.
Rindflesch, Jayant V.
Rajan, Lawrence Hunter, Extracting molecular binding relationships from biomedical text, Proceedings of the sixth conference on Applied natural language processing, p.188-195, April 29-May 04, 2000, Seattle, Washington 18 T.
Sekimizu, H.
S. Park, and J.
Tsujii. 1998.
Identifying the interaction between genes and gene products based on frequently seen verbs in medline abstracts.
In Proc.
of Genome Informatics, Tokyo, Japan,, pages 62--71.
19 B.
J. Stapley and G.
Benoit. 2000.
Biobibliometrics: information retrieval and visualization from co-occurrences of gene names in medline abstracts.
In Pacific Symposium on Biocomputing, pages 529--40.
References 1 Adam Berger, Stephen Della Pietra, and Vincent Della Pietra.
1996. A maximum entropy approach to natural language processing.
Computational Linguistics, 21--22.
2 Avrim
Blum, Tom Mitchell, Combining labeled and unlabeled data with co-training, Proceedings of the eleventh annual conference on Computational learning theory, p.92-100, July 24-26, 1998, Madison, Wisconsin, United States 3 Branimir K.
Boguraev, Mary S.
Neff, The effects of analysing cohesion on document summarisation, Proceedings of the 18th conference on Computational linguistics, p.76-82, July 31-August 04, 2000, Saarbrcken, Germany 4 Branimir K.
Boguraev, Mary S.
Neff, Discourse Segmentation in Aid of Document Summarization, Proceedings of the 33rd Hawaii International Conference on System Sciences-Volume 3, p.3004, January 04-07, 2000 5 Eugene Charniak, A Maximum-Entropy-Inspired Parser, Brown University, Providence, RI, 1999 6 Stanley F.
Chen and Ronald Rosenfeld.
1999. A Gaussian prior for smoothing maximum entropy models.
Technical Report CMU-CS-99-108, Carnegie Mellon University.
7 Pedro
Domingos, Michael Pazzani, On the Optimality of the Simple Bayesian Classifier under Zero-One Loss, Machine Learning, v.29 n.2-3, p.103-130, Nov./Dec.
1997 8 Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, Jaime Carbonell, Summarizing text documents: sentence selection and evaluation metrics, Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, p.121-128, August 15-19, 1999, Berkeley, California, United States 9 Julian Kupiec, Jan Pedersen, Francine Chen, A trainable document summarizer, Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval, p.68-73, July 09-13, 1995, Seattle, Washington, United States 10 Stephen Della Pietra, Vincent Della Pietra, John Lafferty, Inducing Features of Random Fields, IEEE Transactions on Pattern Analysis and Machine Intelligence, v.19 n.4, p.380-393, April 1997 11 Daniel Marcu, The automatic construction of large-scale corpora for summarization research, Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, p.137-144, August 15-19, 1999, Berkeley, California, United States 12 A.
McCallum and K.
Nigam. 1998.
A comparison of event models for naive bayes text classificatio.
In AAAI-98 Workshop on Learning for Text Categorization.
13 Kamal
Nigam, John Lafferty, and Andrew McCallum.
1999. Using maximum entropy for text classification.
In IJCAI-99 Workshop on Machine Learning for Information Filtering,.
14 William
H.
Press, Saul A.
Teukolsky, William T.
Vetterling, Brian P.
Flannery, Numerical recipes in C (2nd ed.): the art of scientific computing, Cambridge University Press, New York, NY, 1992 15 Adwait Ratnaparkhi.
1996. A Maximum Entropy Part-Of-Speech Tagger.
In Proceedings of Empirical Methods in Natural Language, University of Pennsylvania, May.
Tagger: ftp://ftp.cis.upenn.edu/pub/adwait/jmx.
16 S.
Teufel and M.
Moens. 1997.
Sentence extraction as a classification task.
In ACL/EACL-97 Workshop on Intelligent and Scalable Text Summarization, Madrid, Spain.
17 Simone
Teufel.
2001. Task-Based Evaluation of Summary Quality: Describing Relationships Between Scientific Papers.
In NAACL Workshop on Automatic Summarization, Pittsburgh, Pennsylvania, USA, June.
Carnegie Mellon University.
References 1 M.
Arbabi, S.
M. Fischthal, V.
C. Cheng, E.
Bart, Algorithms for Arabic name transliteration, IBM Journal of Research and Development, v.38 n.2, p.183-194, March 1994 2 Asanee Kawtrakul, Amarin Deemagarn, Chalathip Thumkanon, Navapat Khantonthong, and Paul McFetridge.
1998. Backward Transliteration for Thai Document Retrieval.
In Proceedings of the 1998 IEEE Asia-Pacific Conference on Circuits and Systems (APCCAS), pages 563--566.
3 Kevin
Knight, Jonathan Graehl, Machine transliteration, Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics, p.128-135, July 07-12, 1997, Madrid, Spain 4 Klaus Lagally.
1999. ArabTEX: A System for Typesetting Arabic, User Manual Version 3.09.
Technical Report 1998/09, Universitat Stuttgart, Fakultt Informatik, Breitwiesenstrae 20--22, 70565 Stuttgart, Germany.
5 Bonnie
G.
Stalls and Kevin Knight.
1998. Translating Names and Technical Terms in Arabic Text.
In Proceedings of the COLING/ACL Workshop on Computational Approaches to Semitic Languages.
References 1 Steven P.
Shwartz, Applied Natural Language Processing, Petrocelli Books, Inc., Princeton, NJ, 1986 2 Elaine Rich, Kevin Knight, Artificial Intelligence, McGraw-Hill Higher Education, 1990 3 James Allen, Natural language understanding (2nd ed.), Benjamin-Cummings Publishing Co., Inc., Redwood City, CA, 1995 4 Bora, Satyanath, 1968.
bahal byaakaran.
Jnananath Bora, Guwahati 5 Goswami, Golokchandra, 1990.
asamiyaa byaakaranar moulik bisaar.
Bina Library, Guwahati 6 Choudhury, Bhupendranath, 18e, 1973.
asamiyaa bhaashaar byaakaran, pratham bhaag.
Lawyer's Book Stall, Guwahati 7 Sarma, Durgashankar Dev, 1977.
sahaj byaakaran.
Assam State Textbook Production and Publication Corporation Ltd., Guwahati-1 8 Baruah, Hemchandra, 1985 Hem Kosha, 6e.
Hemkosh Prakashan, Guwahati 9 Verma, Shyamji Gokul, 1981.
Maanak Hindi Byaakaran Tatha Rachnaa.
Arya Book Depot, New Delhi-5 10 Whitney, William Dwight, 1977.
Sanskrit Grammar.
Motilal Banarasidass, Delhi.
11 Whitney, William Dwight, 1979.
Roots, Verb Forms and Primary Derivatives of the Sanskrit Language.
Motilal Banarasidass, Delhi.
12 Gbor
Prszky, Balzs Kis, A unification-based approach to morpho-syntactic parsing of agglutinative and other (highly) inflectional languages, Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, p.261-268, June 20-26, 1999, College Park, Maryland 13 Bharati, Akshar, Chaitanya, Vineet and Sangal, Rajeev, 1995 Natural Language Processing A Paninian Perspective.
Prentice-Hall of India Pvt Ltd., New Delhi 14 John Goldsmith, Unsupervised learning of the morphology of a natural language, Computational Linguistics, v.27 n.2, p.153-198, June 2001 15 Kazakov, Dimitar, "Unsupervised Learning of Naive Morphology with Genetic Algorithms" Workshop Notes of the ECML/MLnet Workshop on Empirical Learning of Natural Language Processing Tasks, pp 105--112, April 26, 1997, Prague, Czech Republic
References 1 Peter F.
Brown, John Cocke, Stephen A.
Della Pietra, Vincent J.
Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L.
Mercer, Paul S.
Roossin, A statistical approach to machine translation, Computational Linguistics, v.16 n.2, p.79-85, June 1990 2 Ido Dagan, Kenneth Church, and William Gale.
1993. Robust word alignment for machine aided translation.
In Proceedings of the Workshop on Very Large Corpora, pages 1--8.
3 Pascale
Fung.
1995. Compiling bilingual lexicon entries from a non-parallel English-Chinese corpus.
In David Yarovsky and Kenneth Church, editors, Proceedings of the Third Workshop on Very Large Corpora, pages 173--183, Somerset, New Jersey.
Association for Computational Linguistics.
4 Daniel
S.
Jurafsky and James H.
Martin. 2000.
SPEECH and LANGUAGE PROCESSING.
Prentice Hall.
5 Christopher
D.
Manning, Hinrich Schtze, Foundations of statistical natural language processing, MIT Press, Cambridge, MA, 1999 6 Y.
Matsumoto, A.
Kitauchi, T.
Yamashita, Y.
Hirano, O.
Imaichi, and T.
Imamura. 1997.
Japanese morphological analysis system ChaSen manual.
Technical Report NAIST-ISTR97007, Nara Institute of Technology.
7 Andrew
McCallum, Dayne Freitag, Fernando C.
N. Pereira, Maximum Entropy Markov Models for Information Extraction and Segmentation, Proceedings of the Seventeenth International Conference on Machine Learning, p.591-598, June 29-July 02, 2000 8 Masaaki Nagata, A stochastic Japanese morphological analyzer using a forward-DP backward-A* N-best search algorithm, Proceedings of the 15th conference on Computational linguistics, August 05-09, 1994, Kyoto, Japan 9 Lawrence Rabiner, Biing-Hwang Juang, Fundamentals of speech recognition, Prentice-Hall, Inc., Upper Saddle River, NJ, 1993 10 Kouichi Takeuchi and Yuji Matsumoto.
1995. HMM parameter learning for Japanese morphological analyzer.
In Proceedings of PACLING95, pages 163--172.
11 Dekai
Wu and Xuanyin Xia.
1994. Learning an English-Chinese lexicon from a parallel corpus.
In Proceedings of the First Conference of the Association for Machine Translation in the Americas (AMTA-94).
References 1 Hiyan Alshawi, Shona Douglas, Srinivas Bangalore, Learning dependency translation models as collections of finite-state head transducers, Computational Linguistics, v.26 n.1, p.45-60, March 2000 2 D.
Carter, M.
Rayner, et al.2000. Evaluation.
In Rayner et al.(Rayner et al., 2000).
3 FlexiPC, 2002.
http://www.flexipc.com/product/, then "translator".
As of 15 Mar 2002.
4 R.
Frederking, A.
Rudnicky, and C.
Hogan. 1997.
Interactive speech translation in the diplomat project.
In Proceedings of the Spoken Language Translation workshop at the 35th Meeting of the Association for Computational Linguistics, Madrid, Spain.
5 IntegratedWaveTechnologies, 2002.
http://www.i-w-t.com/investor.html. As of 15 Mar 2002.
6 Nuance, 2002.
http://www.nuance.com. As of 1 Feb 2002.
7 M.
Rayner, D.
Carter, P.
Bouillon, V.
Digalakis, and M.
Wirn, editors.
2000. The Spoken Language Translator.
Cambridge University Press.
8 M.
Rayner, J.
Dowding, and B.
A. Hockey.
2001. A baseline method for compiling typed unification grammars into context free language models.
In Proceedings of Eurospeech 2001, pages 729--732, Aalborg, Denmark.
9 M.
Thriault, 2002.
Questionnaire de dpistage pour adultes (in French).
As of 15 Mar 2002.
10 W.
Wahlster, editor.
2000. Verbmobil: Foundations of Speech-to-Speech Translation. Springer.
References 1 Jeffrey Allen and Christopher Hogan.
1998. Expanding lexical coverage of parallel corpora for the ebmt approach.
In First Conference on Language Resources and Evaluation (LREC '98), pages 747--754, Granada, Spain, May.
2 A.
Black and K.
Lenzo. 2001.
Optimal data selection for unit selection synthesis.
In 4rd ESCA Workshop on Speech Synthesis, Scotland.
3 A.
Black, P.
Taylor, and R.
Caley. 1998.
The Festival Speech Synthesis System.
http://festvox.org/festival. 4 A.
Black, R.
Brown, R.
Frederking, K.
Lenzo, J.
Moody, A.
Rudnicky, R.
Singh, and E.
Steinbrecher. 2002a.
Rapid devlopement of speech-to-speech translation systems.
Submitted to ICSLP2002.
5 A.
Black, R.
Brown, R.
Frederking, R.
Singh, J.
Moody, and E.
Steinbrecher. 2002b.
TONGUES: Rapid Development of a Speech-to-Speech Translation System.
In Proceedings of HLT-2002, San Diego, CA, USA.
6 R.
Brown and R.
Frederking. 1995.
Applying Statistical English Language Modeling to Symbolic Machine Translation.
In Proceedings of the Sixth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-95), pages 221--239.
7 Ralf
D.
Brown, Example-Based Machine Translation in the Pangloss system, Proceedings of the 16th conference on Computational linguistics, August 05-09, 1996, Copenhagen, Denmark 8 Robert Frederking, Sergei Nirenburg, Three heads are better than one, Proceedings of the fourth conference on Applied natural language processing, October 13-15, 1994, Stuttgart, Germany 9 Robert Frederking, Alexander Rudnicky, Christopher Hogan, Kevin Lenzo, Interactive Speech Translation in the Diplomat Project, Machine Translation, v.15 n.1-2, p.27-42, June 2000 10 R.
Frederking, A.
Black, R.
Brown, J.
Moody, and E.
Steinbrecher. 2002.
Field Testing the Tongues Speech-to-Speech Machine Translation System.
LREC 2002.
11 Keiko
Horiguchi and Alexander Franz.
1997. A formal basis for spoken language translation by analogy.
In Steven Krauwer et al., editors, Proceedings of the Spoken Language Translation Workshop, pages 32--39, Madrid, Spain, July.
ELSNET. 12 X.
Huang, F.
Alleva, H.-W.
Hon, K.-F.
Hwang, M.-Y.
Lee, and R.
Rosenfeld. 1992.
The SPHINX-II Speech Recognition System: an overview.
Computer Speech and Language, 7(2):137--148.
13 Lori
Levin, Alon Lavie, Monika Woszczyna, Donna Gates, Marsal Gavald, Detlef Koll, Alex Waibel, The Janus-III Translation System: Speech-to-Speech Translation in Multiple Domains, Machine Translation, v.15 n.1-2, p.3-25, June 2000 14 M.
Rayner, D.
Carter, Hybrid Language Processing in the Spoken Language Translator, Proceedings of the 1997 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP '97) -Volume 1, p.107, April 21-24, 1997 15 Alex Waibel, Interactive Translation of Conversational Speech, Computer, v.29 n.7, p.41-48, July 1996
References 1 Eric Brill, Jun Wu, Classifier combination for improved lexical disambiguation, Proceedings of the 17th international conference on Computational linguistics, August 10-14, 1998, Montreal, Quebec, Canada 2 J.
Daud, L.
Padr, and G.
Rigau. 1999.
Experiments on applying relaxation labeling to map multilingual hierarchies.
Technical Report LSI-99-5-R, Software Department.
UPC. 3 W.
Gale, K.
Church, and D.
Yarowsky. 1992.
A method for disambiguating word senses in a large corpus.
Computers and the Humanities, 26:415--439.
4 B.
Magnini, C.
Strapparava, G.
Pezzulo, and A.
Gliozzo. 2001.
Using domain information for word sense disambiguation.
In Proceedings of SENSEVAL2.
5 D.
McCarthy, J.
Carroll, and J.
Preiss. 2001.
Disambiguating noun and verb senses using automatically acquired selectional preferences.
In Proceedings of SENSEVAL2.
6 Rada
Mihalcea, Dan I.
Moldovan, An automatic method for generating sense tagged corpora, Proceedings of the sixteenth national conference on Artificial intelligence and the eleventh Innovative applications of artificial intelligence conference innovative applications of artificial intelligence, p.461-466, July 18-22, 1999, Orlando, Florida, United States 7 George A.
Miller, WordNet: a lexical database for English, Communications of the ACM, v.38 n.11, p.39-41, Nov.
1995 8 Ted Pedersen, Rebecca Bruce, Knowledge lean word-sense disambiguation, Proceedings of the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence, p.800-805, July 1998, Madison, Wisconsin, United States 9 P.
Resnik. 1997.
Selectional preference and sense disambiguation.
In Proceedings of the ACL SIGLEX Workshop on Tagging Text with Lexical Semantics, pages 52--57.
10 German
Rigau, Jordi Atserias, Eneko Agirre, Combining unsupervised lexical knowledge methods for word sense disambiguation, Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics, p.48-55, July 07-12, 1997, Madrid, Spain 11 A.
Roventini, A.
Alonge, F.
Bertagna, B.
Magnini, and N.
Calzolari. 2000.
ItalWordNet: a large semantic database for Italian.
In Proceedings of LREC-2000, pages 783--790.
12 H.
Schmid. 1994.
Probabilistic part-of-speech tagging using decision trees.
In International Conference on New Methods in Language Processing.
13 Hinrich
Schtze, Automatic word sense discrimination, Computational Linguistics, v.24 n.1, March 1998 14 Mark Stevenson, Yorick Wilks, The interaction of knowledge sources in word sense disambiguation, Computational Linguistics, v.27 n.3, p.321-349, September 2001 15 Michael Sussna, Word sense disambiguation for free-text indexing using a massive semantic network, Proceedings of the second international conference on Information and knowledge management, p.67-74, November 01-05, 1993, Washington, D.C., United States 16 Hans van Halteren, Jakub Zavrel, Walter Daelemans, Improving data driven wordclass tagging by system combination, Proceedings of the 36th annual meeting on Association for Computational Linguistics, August 10-14, 1998, Montreal, Quebec, Canada 17 Ellen M.
Voorhees, Using WordNet to disambiguate word senses for text retrieval, Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, p.171-180, June 27-July 01, 1993, Pittsburgh, Pennsylvania, United States 18 David Yarowsky, Richard Wicentowski, Minimally supervised morphological analysis by multimodal alignment, Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, p.207-216, October 03-06, 2000, Hong Kong 19 D.
Yarowsky, S.
Cucerzan, R.
Florian, C.
Schafer, and R.
Wicentowski. 2001.
The Johns Hopkins SENSEVAL2 system descriptions.
In Proceedings of SENSEVAL2.
20 David Yarowsky, Unsupervised word sense disambiguation rivaling supervised methods, Proceedings of the 33rd annual meeting on Association for Computational Linguistics, p.189-196, June 26-30, 1995, Cambridge, Massachusetts
References 1 S.
Atkins. 1992.
Tools for corpus-aided lexicography: the HECTOR project.
Acta Linguistica Hungarica, 41:5--72.
2 Bran
Boguraev, Ted Briscoe, Large lexicons for natural language processing: utilising the grammar coding system of LDOCE, Computational Linguistics, v.13 n.3-4, July-December 1987 3 Ted Briscoe, John Carroll, Automatic extraction of subcategorization from corpora, Proceedings of the fifth conference on Applied natural language processing, p.356-363, March 31-April 03, 1997, Washington, DC 4 G.
Carroll and M.
Rooth. 1998.
Valence induction with a head-lexicalized PCFG.
In 3rd Conference on Empirical Methods in Natural Language Processing.
5 Bonnie
J.
Dorr, Large-Scale Dictionary Construction for ForeignLanguage Tutoring and Interlingual Machine Translation, Machine Translation, v.12 n.4, p.271-322, 1997 6 Ralph Grishman, Catherine Macleod, Adam Meyers, Comlex Syntax: building a computational lexicon, Proceedings of the 15th conference on Computational linguistics, August 05-09, 1994, Kyoto, Japan 7 A.
Kilgarriff. 1998.
SENSEVAL: An exercise in evaluating word sense disambiguation programs.
In Proceedings of LREC, pages 581--588.
8 A.
Korhonen. 2002.
Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge.
9 S.
Kurohashi. 2002.
SENSEVAL-2 Japanese translation task.
In Proceedings of SENSEVAL-2: Second International Workshop on Evaluating Word Sense Disambiguating Systems.
10 G.
Leech. 1992.
100 million words of English: the British National Corpus.
Language Research, 28(1):1--13.
11 B.
Levin. 1993.
English Verb Classes and Alternations.
Chicago University Press.
12 Christopher
D.
Manning, Hinrich Schtze, Foundations of statistical natural language processing, MIT Press, Cambridge, MA, 1999 13 G.
Miller, R.
Beckwith, C.
Felbaum, D.
Gross, and K.
Miller. 1990.
Introduction to WordNet: An on-line lexical database.
Journal of Lexicography, 3(4):235--244.
14 J.
Preiss, A.
Korhonen, and T.
Briscoe. 2002.
Sub-categorization acquisition as an evaluation method for WSD.
In Proceedings of LREC.
15 D.
Roland and D.
Jurafsky. 2001.
Verb sense and verb subcategorization probabilities.
In S.
Stevenson and P.
Merlo, editors, The Lexical Basis of Sentence Processing: Formal, Computational, and Experimental Issue.
Cambridge University Press, Jon Benjamins, Amsterdam.
To appear.
16 Douglas
Roland, Daniel Jurafsky, Lise Menn, Susanne Gahl, Elizabeth Elder, Chris Riddoch, Verb subcategorization frequency differences between business-news and balanced corpora: the role of verb sense, Proceedings of the workshop on Comparing corpora, p.28-34, October 07-07, 2000, Hong Kong 17 Anoop Sarkar, Daniel Zeman, Automatic extraction of subcategorization frames for Czech, Proceedings of the 18th conference on Computational linguistics, July 31-August 04, 2000, Saarbrcken, Germany
References 1 Eneko Agirre, German Rigau, and J.
Atserias. 2000.
Combining supervised and unsupervised lexical knowledge methods for word sense disambiguation.
Computers and the Humanities.
Senseval Special Issue, 34(1--2):103--108.
2 Sue
Atkins.
1993. Tools for computer-aided lexicography: the Hector project.
In Papers in Computational Lexicography: COMPLEX 93, Budapest.
3 Michele
Banko, Vibhu O.
Mittal, Michael J.
Witbrock, Headline generation based on statistical translation, Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, p.318-325, October 03-06, 2000, Hong Kong 4 Peter F.
Brown, Stephen A.
Della Pietra, Vincent J.
Della Pietra, Robert L.
Mercer, Word-sense disambiguation using statistical methods, Proceedings of the 29th annual meeting on Association for Computational Linguistics, p.264-270, June 18-21, 1991, Berkeley, California 5 John Carroll and Diana McCarthy.
2000. Word sense disambiguation using automatically acquired verbal preferences.
Computers and the Humanities.
Senseval Special Issue, 34(1--2):109--114.
6 Siobhan
Devlin and John Tait.
1998. The use of a psycholinguistic database in the simplification of text for aphasic readers.
In John Nerbonne, editor, Linguistic Databases, volume CSLI Lecture Notes Number 77, pages 161--173.
CSLI Publications, Stanford CA.
7 Adam
Kilgarriff and Joseph Rosenzweig.
2000. Framework and results for english SENSEVAL.
Computers and the Humanities.
Senseval Special Issue, 34(1--2):15--48.
8 Adam
Kilgarriff.
1997. What is word sense disambiguation good for?
In Proceedings of Natural Language Processing in the Pacific Rim, pages 209--214.
9 Adam
Kilgarriff.
1998. Gold standard datasets for evaluating word sense disambiguation programs.
Computer Speech and Language, 12(3):453--472.
10 Philip
Resnik, David Yarowsky, Distinguishing systems and distinguishing senses: new evaluation methods for Word Sense Disambiguation, Natural Language Engineering, v.5 n.2, p.113-133, June 1999 11 Hinrich Schtze, Automatic word sense discrimination, Computational Linguistics, v.24 n.1, March 1998
References 1 Bran Boguraev, Ted Briscoe, John Carroll, David Carter, Claire Grover, The derivation of a grammatically indexed lexicon from the Longman Dictionary of Contemporary English, Proceedings of the 25th annual meeting on Association for Computational Linguistics, p.193-200, July 06-09, 1987, Stanford, California 2 Michael R.
Brent, From grammar to lexicon: unsupervised learning of lexical syntax, Computational Linguistics, v.19 n.2, June 1993 3 Ted Briscoe, John Carroll, Automatic extraction of subcategorization from corpora, Proceedings of the fifth conference on Applied natural language processing, p.356-363, March 31-April 03, 1997, Washington, DC 4 J.
Carroll and E.
J. Briscoe.
1996. Apportioning development effort in a probabilistic lr parsing system through evaluation.
In 1st ACL/SIGDAT Conference on Empirical Methods in Natural Language Processing, pages 92--100.
5 G.
Carroll and M.
Rooth. 1998.
Valence induction with a head-lexicalized pcfg.
In 3rd Conference on Empirical Methods in Natural Language Processing.
6 Stanley
F.
Chen, Joshua Goodman, An empirical study of smoothing techniques for language modeling, Proceedings of the 34th annual meeting on Association for Computational Linguistics, p.310-318, June 24-27, 1996, Santa Cruz, California 7 Mahesh V.
Chitrao, Ralph Grishman, Statistical parsing of messages, Proceedings of the workshop on Speech and Natural Language, p.263-266, June 24-27, 1990, Hidden Valley, Pennsylvania 8 Hoa Trang Dang, Karin Kipper, Martha Palmer, Joseph Rosenzweig, Investigating regular sense extensions based on intersective Levin classes, Proceedings of the 17th international conference on Computational linguistics, August 10-14, 1998, Montreal, Quebec, Canada 9 Bonnie J.
Dorr, Large-Scale Dictionary Construction for ForeignLanguage Tutoring and Interlingual Machine Translation, Machine Translation, v.12 n.4, p.271-322, 1997 10 Ralph Grishman, Catherine Macleod, Adam Meyers, Comlex Syntax: building a computational lexicon, Proceedings of the 15th conference on Computational linguistics, August 05-09, 1994, Kyoto, Japan 11 Anna Korhonen, Genevieve Gorrell, Diana McCarthy, Statistical filtering and subcategorization frame acquisition, Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics, p.199-206, October 07-08, 2000, Hong Kong 12 Anna Korhonen, Using semantically motivated estimates to help subcategorization acquisition, Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics, p.216-223, October 07-08, 2000, Hong Kong 13 A.
Korhonen. 2002.
Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge, UK.
14 G.
Leech. 1992.
100 million words of english: the british national corpus.
Language Research, 28(1):1--13.
15 B.
Levin. 1993.
English Verb Classes and Alternations.
Chicago University Press, Chicago.
16 Christopher
D.
Manning, Hinrich Schtze, Foundations of statistical natural language processing, MIT Press, Cambridge, MA, 1999 17 Christopher D.
Manning, Automatic acquisition of a large subcategorization dictionary from corpora, Proceedings of the 31st annual meeting on Association for Computational Linguistics, p.235-242, June 22-26, 1993, Columbus, Ohio 18 G.
A. Miller.
1990. Wordnet: An on-line lexical database.
International Journal of Lexicography, 3(4):235--312.
19 J.
Preiss, A.
Korhonen, and E.
J. Briscoe.
2002. Subcategorization acquisition as an evaluation method for wsd.
In Language Resources and Evaluation Conference.
To appear.
20 P.
Procter. 1978.
Longman Dictionary of Contemporary English.
Longman, England.
21 Anoop Sarkar, Daniel Zeman, Automatic extraction of subcategorization frames for Czech, Proceedings of the 18th conference on Computational linguistics, July 31-August 04, 2000, Saarbrcken, Germany 22 A.
Ushioda, D.
Evans, T.
Gibson, and A.
Waibel. 1993.
The automatic acquisition of frequencies of verb subcategorization frames from tagged corpora.
In B.
Boguraev and J.
Pustejovsky, editors, SIGLEX ACL Workshop on the Acquisition of Lexical Knowledge from Text, pages 95--106.
Columbus, Ohio.
References 1 Chinatsu Aone, Mila Ramos-Santacruz, REES: a large-scale relation and event extraction system, Proceedings of the sixth conference on Applied natural language processing, p.76-83, April 29-May 04, 2000, Seattle, Washington 2 C.
Aone, L.
Halverson, T.
Hampton, and M.
Ramos-Santacruz. 1998.
SRA: Description of the IE2 system used for MUC-7.
In Proceedings of MUC-7.
3 Daniel
M.
Bikel, Richard Schwartz, Ralph M.
Weischedel, An Algorithm that Learns Whats in a Name, Machine Learning, v.34 n.1-3, p.211-231, Feb.
1999 4 M.
Collins and N.
Duffy. 2001.
Convolution kernels for natural language.
In Proceedings of NIPS-2001.
5 Corinna
Cortes, Vladimir Vapnik, Support-Vector Networks, Machine Learning, v.20 n.3, p.273-297, Sept.
1995 6 Nello Cristianini, John Shawe-Taylor, An introduction to support Vector Machines: and other kernel-based learning methods, Cambridge University Press, New York, NY, 1999 7 R.
O. Duda and P.
E. Hart.
1973. Pattern Classification and Scene Analysis.
John Wiley, New York.
8 Yoav
Freund, Robert E.
Schapire, Large Margin Classification Using the Perceptron Algorithm, Machine Learning, v.37 n.3, p.277-296, Dec.
1999 9 T.
Furey, N.
Cristianini, N.
Duffy, D.
Bednarski, M.
Schummer, and D.
Haussler. 2000.
Support vector machine classification and validation of cancer tissue samples using microarray expression.
Bioinformatics, 16.
10 D.
Haussler. 1999.
Convolution kernels on discrete structures.
UC Santa Cruz Technical Report UCS-99-10.
11 Thorsten
Joachims, Text Categorization with Suport Vector Machines: Learning with Many Relevant Features, Proceedings of the 10th European Conference on Machine Learning, p.137-142, April 21-23, 1998 12 Thorsten Joachims, Learning to Classify Text Using Support Vector Machines: Methods, Theory and Algorithms, Kluwer Academic Publishers, Norwell, MA, 2002 13 John D.
Lafferty, Andrew McCallum, Fernando C.
N. Pereira, Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data, Proceedings of the Eighteenth International Conference on Machine Learning, p.282-289, June 28-July 01, 2001 14 Nick Littlestone, Learning Quickly When Irrelevant Attributes Abound: A New Linear-Threshold Algorithm, Machine Learning, v.2 n.4, p.285-318, April 1988 15 Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, Chris Watkins, Text classification using string kernels, The Journal of Machine Learning Research, 2, p.419-444, 3/1/2002 16 Andrew McCallum, Dayne Freitag, Fernando C.
N. Pereira, Maximum Entropy Markov Models for Information Extraction and Segmentation, Proceedings of the Seventeenth International Conference on Machine Learning, p.591-598, June 29-July 02, 2000 17 S.
Miller, M.
Crystal, H.
Fox, L.
Ramshaw, R.
Schwartz, R.
Stone, and R.
Weischedel. 1998.
Algorithms that learn to extract information BBN: Description of the SIFT system.
In Proceedings of MUC-7.
18 Marcia
Munoz, Vasin Punyakanok, Dan Roth, Dav Zimak, A Learning Approach to Shallow Parsing, University of Illinois at Urbana-Champaign, Champaign, IL, 1999 19 D.
Roth and W.
Yih. 2001.
Relational learning via propositional algorithms: An information extraction case study.
In Proceedings of IJCAI-01.
20 Dan Roth, Learning in Natural Language, Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, p.898-904, July 31-August 06, 1999 21 V.
Vapnik. 1998.
Statistical Learning Theory.
John Wiley.
References 1 Peter F.
Brown, Vincent J.
Della Pietra, Stephen A.
Della Pietra, Robert L.
Mercer, The mathematics of statistical machine translation: parameter estimation, Computational Linguistics, v.19 n.2, June 1993 2 Kevin Knight, Vasileios Hatzivassiloglou, Two-level, many-paths generation, Proceedings of the 33rd annual meeting on Association for Computational Linguistics, p.252-260, June 26-30, 1995, Cambridge, Massachusetts 3 Irene Langkilde and Kevin Knight.
1998. The practical value of n-grams in generation.
In Proceedings of the International Natural Language Generation Workshop, pages 248--255, Ontario, Canada, August.
4 Franz
Josef Och, Nicola Ueffing, Hermann Ney, An efficient A* search algorithm for statistical machine translation, Proceedings of the workshop on Data-driven methods in machine translation, p.1-8, July 07-07, 2001, Toulouse, France 5 Martin Oerder and Hermann Ney.
1993. Word graphs: An efficient interface between continous speech recognition and language understanding.
In IEEE International Conference on Acoustics, Speech and Signal Processing, volume 2, pages 119--122, Minneapolis, MN, April.
6 Stefan
Ortmanns, Hermann Ney, and Xavier Aubert.
1997. A word graph algorithm for large vocabulary continuous speech recognition.
Computer, Speech and Language, 11(1):43--72, January.
7 Christoph
Tillmann, Hermann Ney, Word re-ordering and DP-based search in statistical machine translation, Proceedings of the 18th conference on Computational linguistics, July 31-August 04, 2000, Saarbrcken, Germany 8 Christoph Tillmann, Hermann Ney, Word re-ordering and DP-based search in statistical machine translation, Proceedings of the 18th conference on Computational linguistics, July 31-August 04, 2000, Saarbrcken, Germany
References 1 Ted Briscoe, Ann Copestake, Lexical rules in constraint-based grammars, Computational Linguistics, v.25 n.4, p.487-526, December 1999 2 Jean Carletta, Assessing agreement on classification tasks: the kappa statistic, Computational Linguistics, v.22 n.2, June 1996 3 M.
Collins and Y.
Singer. 1999.
Unsupervised models for named entity classification.
In Proc.
of the 1999 Joint SIGDAT Conference, College Park, MD, pages 100--110.
4 Ann
Copestake and Ted Briscoe.
1995. Semi-productive polysemy and sense extension.
Journal of Semantics, 12:15--67.
5 Steffan
Corley, Martin Corley, Frank Keller, Matthew Crocker, and Shari Trewin.
2001. Finding syntactic structure in unparsed corpora: The Gsearch corpus query system.
Computers and the Humanities, 35(2):81--94.
6 Ted
Dunning, Accurate methods for the statistics of surprise and coincidence, Computational Linguistics, v.19 n.1, March 1993 7 Dan Fass.
1997. Processing Metaphor and Metonymy.
Ablex, Stanford, CA.
8 Christiane
Fellbaum, editor.
1998. WordNet: An Electronic Lexical Database.
MIT Press, Cambridge, Mass.
9 William
Gale, Kenneth Church, and David Yarowsky.
1993. A method for disambiguating word senses in a large corpus.
Computers and the Humanities, 26:415--439.
10 Sanda
Harabagiu.
1998. Deriving metonymic coercions from WordNet.
In Workshop of the Usage of WordNet in Natural Language Processing-Systems, COLING-ACL'98, pages 142--148, Montreal, Canada.
11 Jerry
R.
Hobbs, Mark E.
Stickel, Douglas E.
Appelt, Paul Martin, Interpretation as abduction, Artificial Intelligence, v.63 n.1-2, p.69-142, Oct.
1993 12 Shin-ichiro Kamei, Takahiro Wakao, Metonymy: reassessment, survey of acceptability, and its treatment in a machine translation system, Proceedings of the 30th annual meeting on Association for Computational Linguistics, June 28-July 02, 1992, Newark, Delaware 13 Klaus Krippendorff.
1980. Content Analysis: An Introduction to Its Methodology.
Sage Publications.
14 George
Lakoff and Mark Johnson.
1980. Metaphors We Live By.
Chicago University Press, Chicago, Ill.
15 Maria
Lapata, A corpus-based account of regular polysemy: the case of context-sensitive adjectives, Second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies 2001, p.1-8, June 01-07, 2001, Pittsburgh, Pennsylvania 16 Understanding metonymies in discourse, Artificial Intelligence, v.135 n.1-2, p.145-198, 02/01/2002 17 Katja Markert and Malvina Nissim.
2002. Towards a corpus annotated for metonymies: the case of location names.
In Proc.
of the 3rd International Conference on Language Resources and Evaluation; Las Palmas, Canary Islands, 2002.
18 David
Martinez, Eneko Agirre, One sense per collocation and genre/topic variations, Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics, p.207-215, October 07-08, 2000, Hong Kong 19 Hwee Tou Ng, Hian Beng Lee, Integrating multiple knowledge sources to disambiguate word sense: an exemplar-based approach, Proceedings of the 34th annual meeting on Association for Computational Linguistics, p.40-47, June 24-27, 1996, Santa Cruz, California 20 Geoffrey Nunberg.
1978. The Pragmatics of Reference.
Ph.D. thesis, City University of New York, New York.
21 Ted Pedersen, A simple approach to building ensembles of Naive Bayesian classifiers for word sense disambiguation, Proceedings of the first conference on North American chapter of the Association for Computational Linguistics, p.63-69, April 29-May 04, 2000, Seattle, Washington 22 Ted Pedersen, A decision tree of bigrams is an accurate predictor of word sense, Second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies 2001, p.1-8, June 01-07, 2001, Pittsburgh, Pennsylvania 23 James Pustejovsky.
1995. The Generative Lexicon.
MIT Press, Cambridge, Mass.
24 David Stallard, Two kinds of metonymy, Proceedings of the 31st annual meeting on Association for Computational Linguistics, p.87-94, June 22-26, 1993, Columbus, Ohio 25 Gustav Stern.
1931. Meaning and Change of Meaning.
Gteborg: Wettergren & Kerbers Frlag.
26 Cornelia Verspoor.
1996. Lexical limits on the influence of context.
In Proc.
of the 18th Annual Conference of the Cognitive Science Society; La Jolla, Cal., 12--15 July 1996, pages 116--120.
27 Cornelia Verspoor.
1997. Conventionality-governed logical metonymy.
In H.
Bunt, L.
Kievit, R.
Muskens, and N.
Verlinden, editors, Proc.
of the 2nd International Workshop on Computational Semantics, pages 300--312, Tilburg, The Netherlands.
28 David Yarowsky, Unsupervised word sense disambiguation rivaling supervised methods, Proceedings of the 33rd annual meeting on Association for Computational Linguistics, p.189-196, June 26-30, 1995, Cambridge, Massachusetts 29 David Yarowsky.
1997. Homograph disambiguation in speech synthesis.
In R.
Sproat, J.
Olive, and J.
Hirschberg, editors, Progress in Speech Synthesis, pages 159--175. Springer-Verlag.
References 1 R.
Byrd and Y.
Ravin. 1999.
Identifying and Extracting Relations in Text.
In Proceedings of NLDB 99.
Klagenfurt, Austria.
2 Charles
L.
A. Clarke, Gordon V.
Cormack, Thomas R.
Lynam, Exploiting redundancy in question answering, Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, p.358-365, September 2001, New Orleans, Louisiana, United States 3 S.
Harabagiu, D.
Moldovan, M.
Pasca, R.
Mihalcea, M.
Surdeanu, R.
Bunescu, R.
Grju, V.
Rus, and P.
Morarescu. 2000.
FALCON: Boosting Knowledge for Answer Engines.
In Proceedings of the 9th Text Retrieval Conference (TREC-9) 4 L.
Hiyakumoto. 2001.
Planning and Execution for Open-Domain Question Answering PhD Thesis proposal.
Carnegie Mellon University.
5 H.
Hovy, L.
Gerber, U.
Hermjakob, M.
Junk, and C-Y.
Lin. 2001 Question Answering in Webclopedia.
In Proceedings of the 9th Text REtrieval Conference (TREC9).
6 Marius
A.
Pasca, Sandra M.
Harabagiu, High performance question/answering, Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, p.366-374, September 2001, New Orleans, Louisiana, United States 7 John Prager, Eric Brown, Anni Coden, Dragomir Radev, Question-answering by predictive annotation, Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, p.184-191, July 24-28, 2000, Athens, Greece 8 John Prager, Dragomir Radev, Krzysztof Czuba, Answering what-is questions by Virtual Annotation, Proceedings of the first international conference on Human language technology research, p.1-5, March 18-21, 2001, San Diego 9 Dragomir R.
Radev, John Prager, Valerie Samn, Ranking suspected answers to natural language questions using predictive annotation, Proceedings of the sixth conference on Applied natural language processing, p.150-157, April 29-May 04, 2000, Seattle, Washington 10 E.
Voorhees. 2001.
QA-Track Overview.
In Proceedings of the 10th Text REtrieval Conference (TREC2001).
11 Weka.
On-line information and implementation available at http://www.cs.waikato.ac.nz/ml/weka/.
12 Ian
H.
Witten, Eibe Frank, Data mining: practical machine learning tools and techniques with Java implementations, Morgan Kaufmann Publishers Inc., San Francisco, CA, 2000
References 1 Didier Bourigault, Christian Jacquemin, and Mairie-Claude L'Homme.
2001. Recent Advances in Computational Terminology.
John Benjamins Publishing Company, Amsterdam/Philadelphia.
2 Peter
F.
Brown, Stephen A.
Della Pietra, Vincent J.
Della Pietra, Meredith J.
Goldsmith, Jan Hajic, Robert L.
Mercer, Surya Mohanty, But dictionaries are data too, Proceedings of the workshop on Human Language Technology, March 21-24, 1993, Princeton, New Jersey 3 Michael Carl, Johann Haller, Christoph Horschmann, Dieter Maas, and Jrg Schtz.
2002. The TETRIS Terminology Tool.
TAL, Structuration de terminologie(1).
4 Thierry
Hamon and Adeline Nazarenko.
2001. Detection of synonymy links between terms: Experiment and results.
In in (Bourigault et al., 2001), pages 185--208.
5 Christian
Jacquemin, A symbolic and surgical acquisition of terms through variation, Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing, p.425-438, January 1996 6 Philipe Langlais.
2002. Ressources terminologiques et traduction probabiliste: premiers pas positifs vers un systme adaptatif.
In TALN.
7 Elliott
Macklovitch.
1995. Can terminological consistency be validated automatically?
Technical report, CITI/RALI, Montral, Canada.
8 Ingrid
Meyer.
2001. Extracting knowledge-rich contexts for terminography.
In in (Bourigault et al., 2001), pages 279--302.
9 Raymond
J.
Mooney. 2000.
Integrating abduction and induction in machine learning.
In P.
Flach and A.
Kakas, editors, Abduction and Induction, pages 181--191, Kluwer Academic Publishers.
10 Franz
Josef Och, Hermann Ney, A comparison of alignment models for statistical machine translation, Proceedings of the 18th conference on Computational linguistics, July 31-August 04, 2000, Saarbrcken, Germany 11 Oliver Streiter.
2001. Treebank Development with Deductive and Abductive Explanation-based Learning: Exploratory Experiments.
unpublished draft.
References 1 Lars Borin, You'll take the high road and I'll take the low road: using a third language to improve bilingual word alignment, Proceedings of the 18th conference on Computational linguistics, p.97-103, July 31-August 04, 2000, Saarbrcken, Germany 2 Pascale Fung, A Statistical View on Bilingual Lexicon Extraction: From Parallel Corpora to Non-parallel Corpora, Proceedings of the Third Conference of the Association for Machine Translation in the Americas on Machine Translation and the Information Soup, p.1-17, October 28-31, 1998 3 William A.
Gale, Kenneth W.
Church, A program for aligning sentences in bilingual corpora, Proceedings of the 29th annual meeting on Association for Computational Linguistics, p.177-184, June 18-21, 1991, Berkeley, California 4 Jin-Xia Huang, Key-Sun Choi, Chinese-Korean word alignment based on linguistic comparison, Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, p.392-399, October 03-06, 2000, Hong Kong 5 Martin Kay, Martin Rscheisen, Text-translation alignment, Computational Linguistics, v.19 n.1, March 1993 6 S.
J. Ker and J.
S. Chang.
1997. Aligning more words with high precision for small bilingual corpora.
Computational Linguistics and Chinese Language Processing, 2(2):63--96.
7 O.
Y. Kwong and B.
K. Tsou.
2001. Automatic corpus-based extraction of Chinese legal terms.
In Proceedings of the 6th Natural Language Processing Pacific Rim Symposium (NLPRS 2001), pages 669--676, Tokyo, Japan.
8 O.
Y. Kwong.
2002. Toward a bilingual legal term glossary from context profiles.
In Proceedings of the 16th Pacific Asia Conference on Language, Information and Computation (PACLIC 16), pages 249--258, Jeju, Korea.
9 Gideon
S.
Mann, David Yarowsky, Multipath translation lexicon induction via bridge languages, Second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies 2001, p.1-8, June 01-07, 2001, Pittsburgh, Pennsylvania 10 I.
Dan Melamed, A word-to-word model of translational equivalence, Proceedings of the eighth conference on European chapter of the Association for Computational Linguistics, p.490-497, July 07-12, 1997, Madrid, Spain 11 S.
Piperidis, S.
Boutsis, and I.
Demiros. 1997.
Automatic translation lexicon generation from multilingual texts.
In Proceedings of the 2nd Workshop on Multilinguality in Software Industry: The AI Contribution (MULSAIC'97), Nagoya, Japan.
12 M.
F. Porter.
1980. An algorithm for suffix stripping.
Program, 14(3):130--137, July.
13 Philip
Resnik, I.
Dan Melamed, Semi-automatic acquisition of domain-specific translation lexicons, Proceedings of the fifth conference on Applied natural language processing, p.340-347, March 31-April 03, 1997, Washington, DC 14 M.
Simard, G.
F. Foster, and P.
Isabelle. 1992.
Using cognates to align sentences in bilingual corpora.
In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI-92), pages 67--81.
15 Frank
Smadja, Kathleen R.
McKeown, Vasileios Hatzivassiloglou, Translating collocations for bilingual lexicons: a statistical approach, Computational Linguistics, v.22 n.1, p.1-38, March 1996 16 D.
Wu and X.
Xia. 1995.
Large-scale automatic extraction of an English-Chinese translation lexicon.
Machine Translation, 9(3--4):285--313.
17 Dekai
Wu, Aligning a parallel English-Chinese corpus statistically with lexical criteria, Proceedings of the 32nd annual meeting on Association for Computational Linguistics, p.80-87, June 27-30, 1994, Las Cruces, New Mexico
Probabilistic named entity verification Yi-Chung Lin and Peng-Hsiang Hung Advanced Technology Center, Computer and Communications Laboratories, Industrial Technology Research Institute, Taiwan {lyc,phhung}@itri.org.tw Abstract Named entity (NE) recognition is an important task for many natural language applications, such as Internet search engines, document indexing, information extraction and machine translation.
Moreover, in oriental languages (such as Chinese, Japanese and Korean), NE recognition is even more important because it significantly affects the performance of word segmentation, the most fundamental task for understanding the texts in oriental languages.
In this paper, a probabilistic verification model is designed for verifying the correctness of a named entity candidate.
This model assesses the confidence level of a candidate not only according to the candidates structure but also according to its context.
In our design, the clues for confidence measurement are collected from both positive and negative examples in the training data in a statistical manner.
Experimental results show that the proposed method significantly improves the F-measure of Chinese personal name recognition from 86.5% to 94.4%.
Introduction Named entity (NE) recognition (or proper name recognition) is a task to find the entities of person, location, organization, date, time, percentage and monetary value in text documents.
It is an important task for many natural language applications, such as Internet search engines, document indexing, information extraction and machine translation.
Moreover, in oriental languages (such as Chinese, Japanese and Korean), NE recognition is even more important because it significantly affects the performance of word segmentation, the most fundamental task for understanding the texts in oriental languages.
Therefore, a high-accuracy NE recognition method is highly demanded for most natural language applications in various languages.
There are two major approaches to NE recognition: the handcrafted approach (Grishman, 1995) and the statistical approach (Bikel, 1997; Chen, 1998; Yu, 1998).
In the first approach, a system usually relies on a large number of handcrafted rules.
This kind of systems can be rapid prototyped but are hard to scale up.
In fact, there will be numerous exceptions for most handcrafted rules.
It is generally expensive and impossible to code for every exception we can imagine, not to mention those exceptions we are not able to think of.
Another serious problem with the handcrafted approach is that systems are hard to be ported across different domains and different languages.
Porting a handcrafted system usually means rewriting all its rules.
Therefore, the statistical approach is becoming more and more popular because of its costeffectiveness in scaling up and porting systems.
In general, the statistical approach to NE recognition can be viewed as a two-stage process.
First, according to dictionaries and/or pattern matching rules, the input text is tokenized into tokens.
Each token may be a word or an NE candidate which can consist of more than one word.
Then, the most likely token sequence is selected according to a statistical model, such as Markov model (Bikel, 1997; Yu, 1998) or maximum entropy model (Borthwick, 1999).
Although, the statistical NE recognition is much more scalable and portable, its performance is still not satisfactory.
The insufficient coverage/precision of pattern matching rules and unknown words are the major sources of errors.
Furthermore, the role of the statistical model is to assess the relative possibilities of all possible token sequences and select the most probable one.
The scores obtained from the statistical model can be used for a comparison of competing token sequences, but not for an assessment of the probability that a spotted named entity is correct.
To reduce the recognition errors, we propose a probabilistic verification model to verify the correctness of a named entity.
This model assesses the confidence level of a named entity candidate not only according to the candidates structure but also according to its contexts.
In our design, the clues for confidence measurement are collected from both positive and negative examples in the training data.
Therefore, the confidence measure has strong discriminant power for judging the correctness of a named entity.
In the experiments of Chinese personal name recognition, the proposed verification model increases the F-measure from 86.5% to 94.4%, which corresponds to 58.5% error reduction rate, where error rate is defined as 100% F-measure .
1. Named Entity Verification As mentioned before, there are several kinds of named entities, including person, location, organization, date, time, percentage and monetary value.
In the following description, we use the task of verifying Chinese personal name as an example.
However, our proposed method is also applicable on verifying other kinds of named entities in different languages.
Before introducing our approach, we first describe the notations that will be used.
In this proposal, a random variable is written with a boldface italic letter.
An outcome of a random variable is written with the same italic letter but in normal face.
For example, an outcome of the random variable o is denoted as o.Ifthereis no confusion, we usually use ()P o to denote the probability ()P o=o . A symbol sequence  1,, n xx "  is denoted as  1 n x .
Likewise, ,,1 Yn Y x  denotes the sequence ,1,,, Y Y n x x " .
The task of verifying a named entity candidate is viewed as making an acceptance or rejection decision according to the text segment consisting of the candidate and its context.
Without loss of generality, a text segment is considered as an outcome of the random vector,,,,1,1,1 [,,] L xCyRz LC R =Oooo.
The outcome of each random variable in O is one basic element of text.
In Chinese, the basic elements of text are Chinese characters.
However, in English, the basic elements are English words.
Figure 1 shows an example of a text segment in which the size of the candidate to be verified is 3 (i.e., consists of three Chinese characters) and the sizes of its left context and right context are set to 2 (i.e., two Chinese characters).
Figure 2 depicts the flowchart of our verification approach.
First, the candidate in the input text segment is parsed by a predefined grammar.
If the candidate is ill-formed (i.e., fail Figure 1: Example of a text segment.
Candidate Parsing Confidence Measurement Ill-formed?
Yes Reject No cm < Yes Reject No Accept Text Segment Figure 2: Flowchart of the verification method.
             G471 shi,1L o G7cf zhang Gb2d ma G92e ying G390 jiu G7c4 biao G4a2 shi,2L o,1C o,2C o,3C o,1R o,2R o               Left Context Right Context Candidate to be parsed), it will be rejected immediately.
Otherwise, the text segment is passed to the confidence measurement module to assess the confidence level that the candidate in the text segment is to be a named entity.
If the confidence measure is less than a predefined threshold, the candidate will be rejected.
Otherwise, it will be accepted.
2. Confidence Measurement The basic idea of our approach is to formulate the confidence measurement problem as the problem of hypothesis testing.
The null hypothesis 0 H in which the candidate is a name is tested against the alternative hypothesis 1 H in which the candidate is not a name.
According to Neyman-Pearson Lemma, an optimal hypothesis test involves the evaluation of the following log likelihood ratio:,,,,1,1,1,,,,1,1,1 0,,,,1,1,1 1,,,,1,1,1 0,,,,1,1,11 (,,) (,, |) log (,, |) log (,, | ) log (,, | ) Lx Cy Rz LC R Lx Cy Rz LC R Lx Cy Rz LC R Lx Cy Rz LC R Lx Cy Rz LC R LLR o o o Po o o H Po o o H Po o o H P ooo H = =  (1) where,,,,1,1,1 0 (,, |) Lx Cy Rz LC R P ooo His the likelihood of the candidate and its left and right contexts given the hypothesis that the candidate is a name and,,,,1,1,11 (,, |) Lx Cy Rz LC R P ooo H is the likelihood of the candidate and its left and right contexts given the hypothesis that the candidate is not a name.
The hypothesis test is performed by comparing the log likelihood ratio,,,,1,1,1 (,,) L xCyRz LC R LLR o o o to a predefined critical threshold  .If,,,,1,1,1 (,,) Lx Cy Rz LC R LLR o o o ,the null hypothesis will be accepted.
Otherwise, it will be rejected.
In our design, as shown in Figure 3, the confidence measurement module consists of two models, named NE model and anti-NE model.
The NE model is used to assess the value of,,,,1,1,1 0 log (,, | ) Lx Cy Rz LC R P ooo Hand the anti-NE model is used to assess the value of,,,,1,1,11 log (,, | ) Lx Cy Rz LC R P ooo H.
2.1. NE Model The purpose of the NE model is to evaluate the value of,,,,1,1,1 0 log (,, | ) Lx Cy Rz LC R P ooo H, the log likelihood of the candidate and its left and right contexts given the hypothesis that the candidate is a name.
Since it is infeasible to directly estimate the probability,,,,1,1,1 0 (,, |) Lx Cy Rz LC R P ooo H,itis approximated as follows:,,,,,,,1,1,1 0 0,1,1,1,,, 0,10,10,1 (,, |) (,) ()()() L xCyRz LxCyRz LC R LC R Lx Cy Rz LC Po o o H P o o o Po Po Po   (2) where the subscript of 0 ()P  indicates the probability is evaluated given that the null hypothesis is true.
The probability, 0,1 () L x L P o is further approximated according to the bigram model as follows:, 0,1 0,, 1 1 () (| ) x Lx LLiLi i Po Po o  =   (3) where 0,1,0 0,1 (|) () LL L P oo Po . One should notice that we do not assume that the random sequence,,1 Lx L o is time invariant.
For example, the probability,,1 (| ) Li Li P x y  ==oo is not assumed to be equal to,2,1 (|) LL P xy==oo for 3i  . Likewise, the probability, 0,1 () R z R P o is also further approximated as follows:, 0,1 0,, 1 1 () (| ) z Rz RRiRi i Po Po o  =   (4) where 0,1,0 0,1 (|) () RR R P oo Po . The probability corresponding to the candidate is evaluated by applying the SCFG (StoNE ()S  G2be cm NE Model anti-NE ()S  Anti-NE Model G2c0G2c4 Text Segment Figure 3: Block diagram of the confidence measurement module.
chastic Context-free Grammar) model (Fujisaki, 1989) as follows:, 0,1 0 00 () () max ( ) max ( | ) Cy C T TT AT Po PT P TPA    = =   (5) where T stands for one possible parse tree that derive the candidate, A  indicates a rule in the parse tree T, A stands for the left-handside symbol of the rule and  stands for the sequence of right-hand-side symbols of the rule.
Figure 4 shows an example of a parse tree of the Chinese personal name candidate Gb2dG92eG390(ma ying jiu), where Gb2d(ma) is the surname and G92eG390(ying jiu) is the given name.
In this figure, the symbol S denotes the start symbol, the symbol SNG denotes the nonterminal deriving surname characters and the symbol GNC denotes the nonterminal deriving given name characters.
As a result, according to equations (2) -(5), the scoring function in the NE model is defined as equation (6) to assess the log likelihood of the text segment ,,,,1,1,1,, L xCyRz L CR o oogiven the null hypothesis that ,,1 Cy C o isaname.,,, NE,1,1,1 0,,1 0,,1 11 0 (,,) log ( | ) log ( | ) max log ( | ) Lx Cy Rz LC R xz Li Li Ri Ri ii T AT Sooo Po o Po o PA    ==  =+ +   (6) where T is one possible parse tree that derive the candidate ,,1 Cy C o .
2.2. Anti-NE Model The purpose of the anti-NE model is to evaluate the value of,,,,1,1,11 log (,, | ) Lx Cy Rz LC R P ooo H,thelog likelihood of the candidate and its left and right contexts given the hypothesis that the candidate is not a name.
Since it is infeasible to directly estimate the probability,,,,1,1,11 (,, |) Lx Cy Rz LC R P ooo H,it is approximated as follows:,,,,,,,1,1,11 1,1,1,1 1,,1 1,,1 11 1,,1 1 (,, |) (,) (| ) (| ) (| ) L xCyRz LxCyRz LC R LC R yx Li Li Ci Ci ii z Ri Ri i Po o o H P o o o Po o Po o Po o  ==  = =     (7) where,0,R C y o o,,0,C L x o o,and 1,1,0 (|) LL P oo 1,1 () L P o . Therefore, the following scoring function is used in the anti-NE model to assess the log likelihood of the text segment ,,,,1,1,1,, L xCyRz L CR o oo given the alternative hypothesis that ,,1 Cy C o  is not a nname.,,, anti-NE,1,1,1 1,,1 1,,1 11 1,,1 1 (,,) log ( | ) log ( | ) log ( | ) Lx Cy Rz LC R yx Li Li Ci Ci ii z Ri Ri i Sooo Po o Po o Po o  ==  = =+ +   (8) 3.
Experiment Setup The proposed named entity verification method is used to recognize Chinese personal names from news.
In Chinese, most of the personal names consist of three Chinese characters.
The first character is a surname.
The last two characters are a given name.
Therefore, our preliminary experiments are focused on recognizing the personal names of three Chinese characters.
In our experiments, the training corpus consists of about 14,339,000 Chinese characters collected from economy and industry news.
This corpus should be annotated to estimate the probabilistic parameters of the scoring functions NE ()S  and anti-NE ()S  . However, labeling such large amount of data is too costly or prohibited even if it is possible.
Therefore, labeling Gb2d (ma) G92e (ying) G390 (jiu) SNC GNC GNC S Figure 4: A parse tree of the Chinese personal name Gb2dG92eG390(ma ying jiu).
methods that can be bootstrapped from a little seed data or a few seed rules (Collins, 1999; Cucerzan, 1999) are highly demanded to automatically annotate the training data.
In the following section, we propose an EM-style bootstrapping procedure (Cucerzan, 1999) for annotating the training data automatically.
3.1. EM-Style Bootstrapping The Expectation-Maximization (EM) algorithm (Moon, 1996) has been widely used to estimate model parameters from incomplete data in many different applications.
In this section, an EMstyle bootstrapping procedure is proposed to automatically annotate the named entities in the training corpus.
It iteratively uses the proposed verification model to label the training corpus (expectation step), and then uses the labeled training corpus to re-estimate the parameters of the verification model (maximization step).
Figure 5 shows the flowchart of the bootstrapping procedure.
First, we collect the names of 541 famous people, including government officers and CEOs of big companies.
These names are used as seed names of the name corpus.
Then, the news is automatically annotated according to the name corpus.
The annotated corpus is used to estimate the probabilistic parameters of the scoring functions.
Afterward, the proposed verification procedure is used to verify every possible name candidate in the news.
The candidates whose confidence measures are larger than a predefined threshold are determined to be names.
Currently, if the confidence measures of two overlapped candidates (such as ma ying jiu and ying jiu biao in Figure 1) pass the threshold, both of them are determined as names.
Although this strategy is inadequate, it does not make too much trouble because the chance to get overlapped names is very small in our experiments.
Finally, these guessed names are added to the name corpus which will be used to annotate the news in next iteration.
In our case, after four iterations, the size of name corpus is enlarged from 541 to 6,296, as shown in Table 1.
The total occurrence frequency of these 6,296 names in the training corpus is 40,345.
3.2. Baseline Model In the past, many researchers have studied the problem of Chinese personal name recognition.
Chang (1994) used the 0-order Markov model to segment a text into words, including Chinese personal names.
In his approach, a name probability model is proposed to estimate the probabilities of Chinese personal names.
Sproat (1994) proposed to recognize Chinese personal names with the stochastic finite-state word segmentation algorithm.
His approach is similar to Changs, except that the name probability model is slightly different.
In addition to name probability, Chen (1998) also add extra scores to a name candidate according to context clues (such as position, title, speech-act verbs).
In the researches mentioned above, the reported Fmeasure performances on recognizing Chinese personal names are somewhere between 70% and 86%.
Since these performances are measNews Name Corpus Name Verification Guessed Names Seed Names News Annotation Parameter Estimation Figure 5: EM-style bootstrapping.
Iteration Nunber of distinct names Total frequency of names 0 541 18310 1 3389 31157 2 5327 37423 3 6055 39977 4 6296 40345 Table 1: Number of distinct names in the name corpus and total frequency of names in the annotated news during the bootstrapping iteration.
ured based on different data, higher reported performance does not imply better.
In fact, the name probability models used in these researches are very similar.
Their performances should be comparable to each other.
Therefore, in this paper, Changs approach, whose reported F-measure is 86%, is chosen as the baseline model.
The baseline model is additionally equipped with a dictionary of 72,333 Chinese words.
The prior probabilities of words are estimated from Academia Sinica Balanced Corpus, which contains about 2 million Chinese words.
4. Experimental Results and Discussions Both the baseline model and the proposed name entity verification model (named NEV model) are tested on the same testing corpus.
The testing corpus, also collected from economy and industry news, consists of about 737,000 characters.
This corpus is annotated manually and contains totally 2,545 Chinese personal names.
The F-measure of the baseline model is 86.5% (as indicated by the dashed line in Figure 6).
The precision and recall rates of the baseline model are 79.1% and 95.5% respectively.
Although the recall rate of the baseline model is high, the precision rate is pretty low.
Over 20% of the name candidates proposed by the baseline model are incorrect.
In our experiments, the sizes of the leftand right-context windows of the NEV model are set to 2.
In Figure 6, the solid line with triangle markers depicts the F-measure of the NEV model versus the iteration number of bootstrapping.
The F-measure saturates after 3 iterations.
After 4 iterations, the F-measure of the NEV model reaches 94.4%.
The corresponding precision and recall rates are 96.4% and 92.5% respectively.
Compared with the baseline model, the precision rate is greatly improved from 79.1% to 96.4% with a little sacrifice in recall rate.
The F-measure is improved from 86.5% to 94.4%, which corresponds to 58.5% error reduction rate, where error rate is defined as 100% F-measure .
Table 2 lists three examples of the misrecognized names made by the baseline model.
These examples clearly show that the baseline model tends to incorrectly group consecutive single characters, either from unknown words or single-character words, into names.
In the first two examples, the single characters come from the unknown location name G44cG16e5G670Gaaf(ga luo lai na; Carolina) and the unknown company name G16e5G5de(luo ji; Logitech).
The single characters in the last example are single-character words G6d1(gi; quarter), G4c4(quan; all) and G90d (mei; USA).
Without the inadequate strong tendency of grouping single characters, the NEV model is able to avoid the misrecognition errors made by the baseline model.
The NEV model assesses the confidence measure of each name candidate according to the context around the candidate.
In Table 2, the name candidates in the shaded boxes are rejected by the NEV model because 0 1 2 3 4 0.85 0.90 0.95 1.00 Iteration Fmeasure NEV Baseline Figure 6: The performances of baseline and name entity verification (NEV).
G3b5 da G1277 chang G16e5 luo G5de ji G4e7 zai G450 qu G503 nian ...
(In last year, the big company Logitech ...) Gb4f dong Gc83 di G38b yi G6d1 ji G4c4 quan G90d mei Gd2e lao G7a3 zhe ...
(In the first quarter, the workers in all USA ...) G447 bei G44c ga G16e5 luo G670 lai Gaaf na G500 zhou G8c1 wei G66f li G430 yi (take North Carolina State as an example) Table 2: Examples of the incorrect Chinese personal names (in the shaded boxes) produced by the baseline model.
their confidence measures are too low.
To sum up, the experimental results demonstrate that the contextual information, either from positive examples or from negative examples, is very helpful for named entity verification.
Besides, the superiority of the NEV model also shows that the proposed probabilistic score functions NE ()S  and anti-NE ()S  are effective in providing the scores to produce a reliable confidence measure.
Especially, the proposed named entity verification approach does not require any dictionary in advance.
Conclusion Named entity (NE) recognition is an important task for many natural language applications, such as Internet search engines, document indexing, information extraction and machine translation.
Moreover, in oriental languages (such as Chinese, Japanese and Korean), NE recognition is even more important because it significantly affects the performance of word segmentation, the most fundamental task for understanding the texts in oriental languages.
In this paper, a probabilistic verification model is proposed to verify the correctness of a named entity.
This model assesses the confidence level of a name candidate not only according to the candidates structure but also according to its contexts.
The clues for confidence measurement are collected from both positive and negative examples in the training data.
Therefore, the confidence measure has strong discriminant power for judging the correctness of a named entity.
In the experiments of Chinese personal name recognition, the proposed verification model greatly increases the precision rate from 79.1% to 96.4% with a little sacrifice in recall rate.
The F-measure is improved from 86.5% to 94.4%, which corresponds to 58.5% error reduction rate, where error rate is defined as 100% F-measure .
Acknowledgements This paper is a partial result of Project A311XS1211 conducted by ITRI under sponsorship of the Ministry of Economic Affairs, R.O.C.
Especially thanks to the CKIP group of Academia Sinica for providing the Academia Sinica Balanced Corpus.
References Bikel, D., Miller S., Schwartz R., and Weischedel R.
(1997) Nymble: A High-performance Learning Name Finder.
In Proceedings of the Fifth Conference on Applied Natural Language Processing, pp.
194201. Borthwick, A.
(1999) A Maximum Entropy Approach to Named Entity Recognition.Ph.D.Thesis,New York University.
ChangJ.,ChenS.,KerS.,ChenY.andLiuJ.(1994) A Multiple-Corpus Approach to Recognition of Proper Names in Chinese Texts.
Computer Processing of Chinese and Oriental Languages, Vol.
8, No.
1, pp.
75-85. Chen, H., Ding Y., Tsai S.
and Bian G.
(1998) Description of the NTU System used for MET2.in Proceedings of the 7th Message Understanding Conference (MUC-7) Collins, M.
and Singer, Y.
(1999) Unsupervised Models for Named Entity Classification.InProceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pp.
100-110. Cucerzan S.
and Yarowsky D.
(1999) Language independent named entity recognition combining morphological and contextual evidence.
In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pp.
90-99. Fujisaki, T., Jelinek F., Cocke J., Black E.
and Nishino T.
(1989), A Probabilistic Parsing Method for Sentence Disambiguation.
In Proceedings of the International Workshop on Parsing Technologies, pp.
85-94. Grishman, R.
(1995) The NYU system for MUC-6 or where's the syntax?
In Proceedings of the 6th Message Understanding Conference (MUC-6), pp.
167175.
Moon, T.
K. (1996) The Expectation-Maximization Algorithm, IEEE Signal Processing Magazine, November, 1996, pp.
47-60. Sproat R.
and Chang N.
(1994) A Stochastic FiniteState Word-Segmentation Algorithm for Chinese.
In Proceeding of 32nd Annual Meeting of the Association for Computational Linguistics, pp.
66-73. Yu,S.,BaiS.andWuP.(1998)Description of the Kent Ridge Digital Labs System Used for MUC-7.
In Proceedings of the 7th Message Understanding Conference (MUC-7)
References 1 S.
At-Mokhtar. L'analyse prsyntaxique en une seul tape.
PhD thesis, Universit Blaise Pascal, 1998.
2 S.
At-Mokhtar, J-P Chanod, and C.
Roux. A multi-input dual-entry dependency parser.
In Proceedings of IWPT 2001, Beijing, 2001.
3 G.
Bs. La phrase verbale noyau en franais.
Recherches sur le franais parl, 15, 1999.
4 G
Bs and P.
Blache. Proprits et analyse d'un langage.
In Actes de TALN 99, July Cargse, 1999.
5 G
Bs and C.
Hagge. Properties in 5p.
Technical report, Groupe de Recherche dans les Industries de la Langue (GRIL), URL:lgril.univ-bpclermont.fr, 2001.
6 G
Bs, C.
Hagge, and L.
Coheur. Des proprits linguistiques  l'analyse d'une langue.
In Proceedings of the VEXTAL Conference, November Venice, 1999.
7 Philippe
Blache, Constraints, Linguistic Theories and Natural Language Processing, Proceedings of the Second International Conference on Natural Language Processing, p.221-232, June 02-04, 2000 8 Pollard C.
and I.
Sag. Head-Driven Phrase Structure Grammar.
CSLI Lecture Notes.
Center for the Study of Language and Information, 1994.
9 C.
Hagge. Analyse syntaxique automatique du portugais.
PhD thesis, Universit Blaise Pascal, 2000.
10 C.
Hagge and G.
Bs. Da observao de propriedades lingusticas  sua formalizao numa gramtica do processamento da lngua.
In Actas do III Encontro para o Processamento Computacional da Lngua Portuguesa (PROPOR'98), Porto Alegre, 1998.
11 Carl
Pollard, Ivan A.
Sag, Information-based syntax and semantics: Vol.
1: fundamentals, Center for the Study of Language and Information, Stanford, CA, 1988 12 I.
Sag and T.
Wasow. Syntactic Theory: A formal Introduction.
Center for the Study of Language and Information, Stanford University, 1999.
13 L.
Tesnire. Elments de syntaxe structurale.
Klincksiek, 1969.
14 Pasi
Tapanainen, Timo Jrvinen, A non-projective dependency parser, Proceedings of the fifth conference on Applied natural language processing, p.64-71, March 31-April 03, 1997, Washington, DC 15 T.
Torris and P.
Miller. Formalismes syntaxiques pour le traitement automatique du langage naturel.
Herms, 1990.
References 1 N.
Bohan, E.
Breidt, and M.
Volk. 2000.
Evaluating translation quality as input to product development.
In Proceedings of 2nd International Conference on Language Resources and Evaluation, Athens, Greece.
2 J.B.
Carroll. 1966.
An experiment in evaluating the quality of translations.
Mechanical Translation, 9(3--4):55--66.
3 B.
Dorr, P.
W. Jordan, and J.
W. Benoit.
1999. A Survey of Current Research in Machine Translation.
Advances in Computers, M.
Zelkowitz (ed), 49:1--68.
4 EAGLES, 1994.
Interim Report.
Obtainable from Center for Language Technology, Njalsgade 80, DK 2300 Copenhagen.
5 T.
Hirao, Y.
Sasaki, and H.
Isozaki. 2001.
An Extrinsic Evaluation for Question-Biased Text Summarization on QA Tasks.
In NAACL Workshop on Automatic Summarization, pages 61--68.
6 E.
Hovy and D.
Marcu, 1998.
Automated Text Summarization: Tutorial Notes.
COLING-ACL'98, Montral, Canada.
7 E.
Hovy. 1999.
Toward Finely Differentiated Evaluation Metrics for Machine Translation.
In EAGLES Workshop on Standards and Evaluation, Pisa, Italy.
8 Bowen
Hui, Eric S.
K. Yu, Extracting Conceptual Relationships from Specialized Documents, Proceedings of the 21st International Conference on Conceptual Modeling, p.232-246, October 07-11, 2002 9 H.
Jing, R.
Barzilay, K.
McKeown, and M.
Elhadad. 1998.
Summarization Evaluation Methods: Experiments and Analysis.
In AAAI Intelligent Text Summarization Workshop, pages 60--68.
10 Margaret
King, Kirsten Falkedal, Using test suites in evaluation of machine translation systems, Proceedings of the 13th conference on Computational linguistics, p.211-216, August 20-25, 1990, Helsinki, Finland 11 M.
King. 1997.
Evaluating translation.
In C.
Hauenschild & S.
Heizmann (eds.), Machine Translation and Translation Theory.
Walter de Gruyter & Co.: Berlin.
12 J.C.
Loehlin. 1992.
Latent Variable Models.
Erlbaum Associates, Hillsdale NJ.
13 Keith
J.
Miller, Catherine Ball, The lexical choice of prepositions in machine translation, 2000 14 Jakob Nielsen, Usability Engineering, Morgan Kaufmann Publishers Inc., San Francisco, CA, 1993 15 Eric H.
Nyberg, Teruko Mitamura, Jaime G.
Carbonell, Evaluation metrics for knowledge-based machine translation, Proceedings of the 15th conference on Computational linguistics, August 05-09, 1994, Kyoto, Japan 16 F.
Reeder and E.
Hovy, 2000.
Workshop on Machine Translation Evaluation.
AMTA-00, October.
17 Karen
Sparck Jones, Julia R.
Galliers, J.
R. Galliers, Evaluating Natural Language Processing Systems: An Analysis and Review, Springer-Verlag New York, Inc., Secaucus, NJ, 1996 18 K.
Sparck-Jones. 1996.
Towards Better NLP System Evaluation.
In Proceedings of the Human Language Technology Workshop, pages 102--107.
ARPA. 19 S.
Teufel. 2001.
Task-Based Evaluation of Summary Quality: Describing Relationships Between Scientific Papers.
In NAACL Workshop on Automatic Summarization.
20 J.
S. White, T.
O'Connell, and F.
E. O'Mara.
1994. The ARPA MT evaluation methodologies: Evolution, lessons and further approaches.
In Technology partnerships for corssing the language barrier: Proceedings of the first conference of the Association for Machine Translation in the Americas, pages 193--205, Columbia, USA.
References 1 Ralf D.
Brown, Automated generalization of translation examples, Proceedings of the 18th conference on Computational linguistics, p.125-131, July 31-August 04, 2000, Saarbrcken, Germany 2 M.
Carl. 1999.
Inducing translation templates for example-based machine translation.
In Proc.
of the Machine Translation Summit VII, pages 250--258.
3 Thomas
H.
Cormen, Clifford Stein, Ronald L.
Rivest, Charles E.
Leiserson, Introduction to Algorithms, McGraw-Hill Higher Education, 2001 4 Makoto Nagao, A framework of a mechanical translation between Japanese and English by analogy principle, Proc.
of the international NATO symposium on Artificial and human intelligence, p.173-180, October 1984, Lyon, France 5 S.
Nirenburg, S.
Beale, and C.
Domashnev. 1994.
A full-text experiment in example-based machine translation.
In Proc.
of the International Conference on New Methods in Language Processing, pages 78--87.
6 Eiichiro
Sumita, Example-based machine translation using DP-matching between word sequences, Proceedings of the workshop on Data-driven methods in machine translation, p.1-8, July 07-07, 2001, Toulouse, France 7 T.
Takezawa, E.
Sumita, F.
Sugaya, H.
Yamamoto, and S.
Yamamoto. 2002.
Toward a broad-coverage bilingual corpus for speech translation of travel conversations in the real world.
In LREC-2002.
8 T.
Veale and A.
Way. 1997.
Gaijin: A bootstrapping, template-driven approach to example-based MT.
In Proc.
of the NeMNLP97.
References 1 Ampornaramveth V., Aizawa A.
& Oyama K.
(2000) An Internet-based Collaborative Dictionary Development Project: SAIKAM.
Proc. of 7th Intl.
Workshop on Academic Information Networks and Systems (WAINS'7), Bangkok, 7--8 December 2000, Kasetsart University.
2 Blanc
., Srasset G.
& Tchou F.
(1994) Designing an Acception-Based Multilingual Lexical Data Base under HyperCard: PARAX Research Report, GETA, IMAG (UJF & CNRS), Aug.
1994, 10 p.
3 Connolly, Dan (1997) XML Principles, Tools and Techniques World Wide Web Journal, Volume 2, Issue 4, Fall 1997, O'REILLY & Associates, 250 p.
4 Nancy
M.
Ide, Jean Veronis, Text Encoding Initiative: Background and Contexts, Kluwer Academic Publishers, Norwell, MA, 1995 5 Mangeot-Lerebours M.
(2000) Papillon Lexical Database Project: Monolingual Dictionaries & Interlingual Links.
Proc. of 7th Workshop on Advanced Information Network and System Pacific Association for Computational Linguistics 1997 Conference (WAINS'7), Bangkok, Thailande, 7--8 dcembre 2000, Kasetsart University, 6 p.
6 Mangeot-Lerebours M.
(2001) Environnements centraliss et distribus pour lexicographes et lexicologues en contexte multilingue.
Nouvelle thse, Universit Joseph Fourier (Grenoble I), 27 September 2001, 280 p.
7 Mel'tchuk I., Clas A.
& Polgure A.
(1995) Introduction  la lexicologie explicative et combinatoire.
AUPELF-UREF/Duculot, Louvain-la-Neuve, 256 p.
8 Polgure, A.
(2000) Towards a theoretically-motivated general public dictionary of semantic derivations and collocations for French.
Proc. EURALEX'2000, Stuttgart, pp 517--527.
9 Gilles
Srasset, Interlingual lexical organisation for multilingual lexical databases in NADIA, Proceedings of the 15th conference on Computational linguistics, August 05-09, 1994, Kyoto, Japan 10 Srasset G.
(1994b) SUBLIM, un systme universel de bases lexicales multilingues; et NADIA, sa spcialisation aux bases lexicales interlingues par acceptions.
Nouvelle thse, UJF (Grenoble 1), dc.
1994. 11 Srasset G.
(1997) Le projet NADIA-DEC: vers un dictionnaire explicatif et combinatoire informatis?
Proc. of La mmoire des mots, 5me journes scientifiques du rseau LTT, Tunis, 25--27 septembre 1997, AUPELFUREF, 7 p.
12 Sasset
G.
& Mangeot-Lerebours M.
(2001) Papillon Lexical Database Project: Monolingual Dictionaries & Interlingual Links.
Proc. NLPRS'2001, Hitotsubashi Memorial Hall, National Center of Sciences, Tokyo, Japan, 27--30 November 2001, vol 1/1, pp.
119--125. 13 Tomokiyo M., Mangeot-Lerebours M.
& Planas E.
(2000) Papillon: a Project of Lexical Database for English, French and Japanese, using Interlingual Links.
Proc. of Journes des Sciences et Techniques de l'Ambassade de France au Japon, Tokyo, Japon, 13--14 novembre 2000, Ambassade de France au Japon, 3 p.
References 1 S.
Bird and M.
Liberman. 1999.
A Formal Framework for Linguistic Annotation.
Technical Report MS-CIS-99-01, Department of Computer and Information Science, University of Pennsylvania.
2 Steven
Bird, Kazuaki Maeda, and Xiaoyi Ma.
2001. Agtk: the annotation graph toolkit.
In Peter Buneman Steven Bird and Mark Liberman, editors, IRCS Workshop on Linguistic Databases, University of Pennsylvania, Philadelphia, USA.
3 P.
Boersma. 2001.
Praat, a system for doing phonetics by computer.
Glot International, 5(9/10):341--345.
4 Hennie
Brugman and Peter Wittenburg.
2001. Mpi tools for linguistic annotation.
In Peter Buneman Steven Bird and Mark Liberman, editors, IRCS Workshop on Linguistic Databases, University of Pennsylvania, Philadelphia, USA.
5 J.
Carletta, D.
McKelvie, and Isard A.
2002. Supporting linguistic annotation using xml and stylesheets.
In G.
Sampson and D.
McCarthy, editors, Readings in Corpus Linguistic, Continuum International.
6 L.
Dybkaer, M.
B. Moeller, N.
O. Bernsen, J.
Carletta, A.
Isard, M.
Klein, D.
McKelvie, and A.
Mengel. June 1999.
The mate work-bench.
In David Traum, editor, Proceedings of ACL'99, Demonstration Abstracts.
University of Maryland, pages 12 -13.
7 J.E.
Garcia, U.
B. Gut, and A.
Galves. 2002.
Vocale a semi-automatic annotation tool for prosodic research.
In B.
Bel and I.
Marlien, editors, Proceedings of the Speech Prosody 2002 conference, 11--13 April 2002.
Aix-en-Provence: Laboratoire Parole et Langage, pages 327 -330.
8 D.
Gibbon and T.
Trippel. 2001.
Pax an annotation based concordancing toolkit.
In Peter Buneman Steven Bird and Mark Liberman, editors, IRCS Workshop on Linguistic Databases, University of Pennsylvania, Philadelphia, USA.
9 R.
Ihaka and R.
Gentleman. 1996.
R: A language for data analysis and graphics.
Journal of Computational and Graphical Statistics, 5(3):299--314.
10 Michael
Kipp.
2001. Anvil a generic annotation tool for multimodal dialogue.
In Proceedings of the Eurospeech 2001, Aalborg, pages 1367 -1370.
11 J.-T.
Milde and U.
B. Gut.
2001. The TASX-engine: an XML-based corpus database for time aligned language data.
In Peter Buneman Steven Bird and Mark Liberman, editors, IRCS Workshop on Linguistic Databases, University of Pennsylvania, Philadelphia, USA.
12 J.-T.
Milde and U.
B. Gut.
2002a. A prosodic corpus of non-native speech.
In B.
Bel and I.
Marlien, editors, Proceedings of the Speech Prosody 2002 conference, 11--13 April 2002.
Aix-en-Provence: Laboratoire Parole et Langage, pages 503 -506.
13 J.-T.
Milde and U.
B. Gut.
2002b. The tasx-environment: an xml-based toolset for time aligned speech corpora.
In Proceedings of the third international conference on language resources and evaluation (LREC 2002, Gran Canaria.
14 T.
Schmidt. 2001.
Gesprchstranskription auf dem Computer das System EXMARaLDA.
Gesprchsforschung, http://www.gespraechsforschung-ozs.de, 2.
15 W.
N. Venables and B.
D. Ripley.
1999. Modern Applied Statistics with S-Plus.
Third Edition.
Springer. ISBN 0-387-98825-4.
References 1 Michael R.
Brent, An Efficient, Probabilistically Sound Algorithm for Segmentation andWord Discovery, Machine Learning, v.34 n.1-3, p.71-105, Feb.
1999 2 Jing-Shin Chang and Keh-Yih Su.
1997. An unsupervised iterative method for Chinese new lexicon extraction.
International Journal of Computational Linguistics & Chinese Language Processing, 1(1):101--157.
3 Jyun-Sheng Chang, C.-D.
Chen, and S.-D.
Chen. 1991.
Chinese word segmentation through constraint satisfaction and statistical optimization.
In ROCLING-IV, pages 147--165, National Chiao-Thung University, Hsinchu, Taiwan.
4 Jing-Shin Chang, Yi-Chung Lin, and Keh-Yih Su.
1995. Automatic construction of a Chinese electronic dictionary.
In David Yarovsky and Kenneth Church, editors, WVLC-3, pages 107--120, Somerset, New Jersey, June.
5 Keh-Jiann Chen, Shing-Huan Liu, Word identification for Mandarin Chinese sentences, Proceedings of the 14th conference on Computational linguistics, August 23-28, 1992, Nantes, France 6 Tung-Hui Chiang, Ming-Yu Lin, and Keh-Yih Su.
1992. Statistical models for word segmentation and unknown word resolution.
In ROCLING-V, pages 121--146, Taiwan.
7 W.
Daelemans, S.
Buchholz, and J.
Veenstra. 1999.
Memory-based shallow parsing.
In CoNLL-99, pages 53--60, Bergen, Norway.
8 Walter
Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch.
2001. Timbl: Tilburg memory based learner, version 4.0, reference guide.
Technical Report ILK Technical Report 01--04, Induction of Linguistic Knowledge, Tilburg University, The Netherlands.
9 Carl
G.
De Marcken, Robert C.
Berwick, Unsupervised language acquisition, 1996 10 A.
P. Dempster, N.
M. Laird, and D.
B. Rubin.
1977. Maximum likelihood from incomplete data via the em algorithm.
Journal of the Royal Statistical Society, Series B, 34:1--38.
11 Charng-Kang Fan and Wen-Hsiang Tsai.
1988. Automatic word identification in Chinese sentences by the relaxation technique.Computer Processing of Chinese and Oriental Languages, 4(1):33--56.
12 Kok-Wee Gan, Kim-Teng Lua, Martha Palmer, A statistically emergent approach for language processing: application to modeling context effects in ambiguous Chinese word boundary perception, Computational Linguistics, v.22 n.4, p.531-553, December 1996 13 Xianping Ge, Wanda Pratt, Padhraic Smyth, Discovering Chinese words from unsegmented text (poster abstract), Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, p.271-272, August 15-19, 1999, Berkeley, California, United States 14 Gregory Grefenstette and P.
Tapanainen. 1994.
What is a word, what is a sentence?
Problems of tokenization.
In 3rd Conference on Computational Lexicography and Text Research, COMPLEX'94, Budapest, July 7--10.
15 Gregory
Grefenstette, Anne Schiller, and Salah At-Mokhtar.
2000. Recognizing lexical patterns in text.
In F.
van Eynde, D.
Gibbon, and I.
Schuurman, editors, Lexicon Development for Speech and Language Processing, pages 141--168.
Kluwer, Dordrecht.
16 Gregory
Grefenstette.
1999. Tokenization.
In Hans van Halteren, editor, Syntactic Wordclass Tagging, pages 117--133.
Kluwer, Dordrecht.
17 Yingchun
Guan and Bei Qin.
1986. The design and implementation of a Chinese word statistical system.
Journal of Chinese Information Processing, 1(1):26--32.
(In Chinese).
18 Jin
Guo, Critical tokenization and its properties, Computational Linguistics, v.23 n.4, p.569-596, December 1997 19 J.
Hockenmaier and C.
Brew. 1998.
Error-driven learning of Chinese word segmentation.
In PACLIC-12, pages 218--229, Singapore.
Chinese and Oriental Languages Processing Society.
20 Frederick Jelinek, Statistical methods for speech recognition, MIT Press, Cambridge, MA, 1998 21 Wanying Jin.
1992. A case study: Chinese segmentation and its disambiguation.
Technical Report MCCS-92-227, Computing Research Laboratory, New Mexico State University, Las Cruces.
22 Wanying Jin, Chinese segmentation disambiguation, Proceedings of the 15th conference on Computational linguistics, August 05-09, 1994, Kyoto, Japan 23 Chunyu Kit and Yorick Wilks.
1999. Unsupervised learning of word boundary with description length gain.
In M.
Osborne and E.
T. K.
Sang, editors, CoNLL-99, pages 1--6, Bergen, June.
24 Chunyu Kit, Yuan Liu, and Nanyuan Liang.
1989. On methods of Chinese automatic word segmentation.
Journal of Chinese Information Processing, 3(l):1--32.
(In Chinese).
25 Chunyu Kit.
2000. Unsupervised Lexical Learning as Inductive Inference.
Ph.D. thesis, University of Sheffield, UK.
26 Tom B.
Y. Lai, Sun C.
Lin, Chaofen Sun, and Maosong Sun.
1991. A maximal matching automatic Chinese word segmentation algorithm using corpus tagging for ambiguity resolution.
In ROCLING-IV, pages 17--23.
27 Nanyuan Liang and Yuan Liu.
1985. The OM method of automatic word segmentation.
Chinese Information, 1(2).
(In Chinese).
28 Nanyuan Liang.
1984. Automatic word segmentation for written Chinese and the segmentation system CDWS.
Journal of Beijing University of Aeronautics and Astronautics, ?(4).
(In Chinese).
29 Nanyuan Liang.
1986. CDWS an automatic word segmentation system for written Chinese.
Journal of Chinese Information Processing, 1(2):44--52.
(In Chinese).
30 Nanyuan Liang.
1989. Knowledge for Chinese word segmentation.
Journal of Chinese Information Processing, 4(2):29--33.
(In Chinese).
31 Yuan Liu and Nanyuan Liang.
1986. Basic engineering for Chinese processing Modern Chinese word frequency counting.
Journal of Chinese Information Processing, 1(1):17--23.
(In Chinese).
32 Yuan Liu, Qiang Tan, and Xukun Shen.
1994. Contemporary Chinese Word Segmentation Standard Used for Information Processing, and Automatic Word Segmentation Methods.
Tsinghua University Press, Bejing.
(In Chinese).
33 Hong I Ng and Kim Teng Lua.
(forthcoming). A word finding automation for Chinese sentence tokenization.
Submitted to ACM Transaction of Asian Languages Processing.
34 David Palmer and J.
Burger. 1997.
Chinese word segmentation and information retrieval.
In AAAI Spring Symposium on Cross-Language Text and Speech Retrieval.
35 David D.
Palmer, A trainable rule-based algorithm for word segmentation, Proceedings of the 35th annual meeting on Association for Computational Linguistics, p.321-328, July 07-12, 1997, Madrid, Spain 36 David D.
Palmer. 2000.
Tokenization and sentence segmentation.
In R.
Dale, H.
Moisl, and H.
Somers, editors, Handbook of Natural Language Processing, pages 11--35.
Marcel Dekker, New York.
37 Fuchun Peng, Dale Schuurmans, Self-Supervised Chinese Word Segmentation, Proceedings of the 4th International Conference on Advances in Intelligent Data Analysis, p.238-247, September 13-15, 2001 38 J.
Ponte, USe: A Retargetable Word Segmentation Procedure for Information Retrieval, University of Massachusetts, Amherst, MA, 1996 39 Richard Sproat, William Gale, Chilin Shih, Nancy Chang, A stochastic finite-state word-segmentation algorithm for Chinese, Computational Linguistics, v.22 n.3, p.377-404, September 1996 40 Mark Stevenson, Yorick Wilks, The interaction of knowledge sources in word sense disambiguation, Computational Linguistics, v.27 n.3, p.321-349, September 2001 41 Maosong Sun and Benjamin K.
T'sou. 1995.
Ambiguity resolution in Chinese word segmentation.
In Benjamin K.
T'sou and Tom B.
Y. Lai, editors, PACLIC-10, Hong Kong, December 27--28.
42 Maosong Sun and Zhengping Zhou.
1998. Word segmentation ambiguity in Chinese texts.
In Benjiamin K.
T'sou, Tom B.
Y. Lai, Samuel W.
K. Chan, and Williams S-Y.
Wang, editors, Quantitative and Computational Studies on the Chinese Language, pages 323--338.
Language Information Sciences Research Centre, City University of Hong Kong.
43 W.
J. Teahan, Rodger McNab, Yingying Wen, Ian H.
Witten, A compression-based algorithm for Chinese word segmentation, Computational Linguistics, v.26 n.3, p.375-393, September 2000 44 J.
Veenstra, A.
Van den Bosch, S.
Buchholz, W.
Daelemans, and J.
Zavrel. 2000.
Memory-based word sense disambiguation.
Computing and the Humanities, special issue on SENSEVAL, 34(1--2y).
45 Anand Venkataraman, A statistical model for word discovery in transcribed speech, Computational Linguistics, v.27 n.3, p.352-372, September 2001 46 Jonathan J.
Webster, Chunyu Kit, Tokenization as the initial phase in NLP, Proceedings of the 14th conference on Computational linguistics, August 23-28, 1992, Nantes, France 47 Jonathan J.
Webster and Chunyu Kit.
1992b. Tokenization for machine translation: What can be learned from Chinese word identification.
In Proc.
of 3rd International Conference on Chinese Information Processing, Beijing.
48 Zimin Wu, Gwyneth Tseng, Chinese text segmentation for text retrieval: achievements and problems, Journal of the American Society for Information Science, v.44 n.9, p.532-542, Oct.
1993 49 Shiwen Yu.
1998. Knowledge Base of Grammatical Information for Contemporary Chinese.
Tsinghua University Press, Bejing.
(In Chinese).
50 Jakub Zavrel, Walter Daelemans, and Jorn Veenstra.
1998. Resolving PP attachment ambiguities with memory-based learning.
In T.
Mark Ellison, editor, CoNLL97: Computational Natural Language Learning, pages 136--144, Somerset, New Jersey.
51 Guodong Zhou and Kim Teng Lua.
(forthcoming). A hybrid approach toward ambiguity resolution in segmenting Chinese sentences.
Submitted to Computer Processing of Oriental Languages.
Design of Chinese Morphological Analyzer Huihsin Tseng Institute of Information Science Academia Sinica, Taipei kaori@hp.iis.sinica.edu.tw Keh-Jiann Chen Institute of Information Science Academia Sinica, Taipei kchen@iis.sinica.edu.tw Abstract This is a pilot study which aims at the design of a Chinese morphological analyzer which is in state to predict the syntactic and semantic properties of nominal, verbal and adjectival compounds.
Morphological structures of compound words contain the essential information of knowing their syntactic and semantic characteristics.
In particular, morphological analysis is a primary step for predicting the syntactic and semantic categories of out-of-vocabulary (unknown) words.
The designed Chinese morphological analyzer contains three major functions, 1) to segment a word into a sequence of morphemes, 2) to tag the part-of-speech of those morphemes, and 3) to identify the morpho-syntactic relation between morphemes.
We propose a method of using associative strength among morphemes, morpho-syntactic patterns, and syntactic categories to solve the ambiguities of segmentation and part-of-speech.
In our evaluation report, it is found that the accuracy of our analyzer is 81%.
5% errors are caused by the segmentation and 14% errors are due to part-of-speech.
Once the internal information of a compound is known, it would be beneficial for the further researches of the prediction of a word meaning and its function.
1. Introduction This is the first attempt to design a morphological analyzer to automatically analyze the morphological structures of Chinese compound words 1. Morphological structures of compound words contain the essential information of knowing their syntactic and semantic characteristics.
In particular, morphological analysis is a primary step for predicting the syntactic and semantic categories of out-of-vocabulary (unknown) words.
The existence of unknown words is a major obstacle in Chinese natural language processing.
Due to the 1 Compound words here include compounds in traditional Chinese linguistics and morphological complex words.
fact that new words are easily coined by morphemes in Chinese text, the number of unknown words is increasingly large.
As a result, we cannot collect all the unknown words and manually mark their syntactic categories and meanings.
Our hypothesis to predict the category and the meaning of a word is basically based on Freges principle: The meaning of the whole is a function of the meanings of the parts.
The meanings of morphemes are supposed to make up the meanings of the words.
However, some words like idioms and proper nouns cannot be included in the principle.
In general, unknown words could be divided into two different types: the type that has the property of semantic transparency, i.e. the words whose meanings can be derived from their morphemes and the type without meaning transparency, such as proper nouns.
In this paper we are dealing with the compound words with semantic transparency only.
For the type of compounds without semantic transparency, such as proper nouns, their morphemes and morphological structures do not provide useful information for predicting their syntactic and semantic categories.
Therefore they are processed differently and independently.
In addition, some regular types of compounds, such as numbers, dates, and determinant-measure compounds, are easily analyzed by matching their morphological structures with their regular expression grammars and the result can be used to predict their syntactic and semantic properties, so they will be handled by matching regular expressions at the stage of word segmentation.
According to our observation, most Chinese compounds have semantic transparency except proper nouns, which means the meaning of an unknown word can be interpreted by their own morpheme components.
The design of our morphological analyzer will focus on processing these compounds, but words without semantic transparency are excluded.
It takes a compound word as input and produces the morphological structure of the word.
The major functions are 1) to segment a word into a sequence of morphemes, 2) to tag the part-of-speech of those morphemes, and 3) to identify the References Bosch, Antal van den, Walter Daelemans and Ton Weijters.
(1996) Morphological Analysis Classification: an Inductive-Learning Approach.
NeMLaP. Chao, Yuen Ren.
(1968) A grammar of spoken Chinese.
Berkeley:University of California Press.
Chen, Chao-jan, Ming-hung Bai and Keh-jiann Chen.
(1997) Category Guessing for Chinese Unknown Words.
Proceedings of the Natural Language Processing Pacific Rim Symposium 1997, 35-40.
Chen Yun-chai.
(2001) Corpus Analysis of Reduplication in Mandarin Chinese.
National Kaohsiung Normal University: English Department.
CKIP. (1993) Technical Report no.
93-05: The analysis of Chinese category.
[??????] CKIP:Nankang Creutz, Mathias and Krista Lagus.
(2002) Unsupervised Discovery of Morphemes.
Proceedings of Morphological and Phonological Learning Workshop of ACL'02.
Beaney, Michael.(editor) (1997) The Frege Reader.
Oxfort: Blackwell.
Li, Charles and Sandra A.
Thompson. (1981) Mandarin Chinese.
Berkeley: University of California Press.
Ma, Weiyun, Youming Hsieh, Changhua Yang, and Keh-jiann Chen.
(2001) Chinese Corpus Development and Management System  [????
??????????]. Proceedings of Research on Computational Linguistics Conference XIV, 175-191 .
References 1 Erin L.
Allwein, Robert E.
Schapire, Yoram Singer, Reducing multiclass to binary: a unifying approach for margin classifiers, The Journal of Machine Learning Research, 1, p.113-141, 9/1/2001 2 Xavier Carreras, Llus Mrquez, Boosting trees for clause splitting, Proceedings of the 2001 workshop on Computational Natural Language Learning, p.1-3, July 06-07, 2001, Toulouse, France 3 Xavier Carreras, Llus Mrquez, Vasin Punyakanok, Dan Roth, Learning and Inference for Clause Identification, Proceedings of the 13th European Conference on Machine Learning, p.35-47, August 19-23, 2002 4 Robert E.
Schapire, Yoram Singer, Improved Boosting Algorithms Using Confidence-rated Predictions, Machine Learning, v.37 n.3, p.297-336, Dec.
1999 5 R.
E. Schapire.
2002. The boosting approach to machine learning.
an overview.
In Proceedings of the MSRI Workshop on Nonlinear Estimation and Classification, Berkeley, CA.
References 1 H.
At-Kaci and R.
Di Cosmo.
Compiling order-sorted feature term unification.
Technical report, Digital Paris Research Laboratory, 1993.
PRL Technical Note 7, downloadable from http://www.isg.sfu.ca/life/.
2 H.
At-Kaci and A.
Podelski. Towards a meaning of LIFE.
Journal of Logic Programming, 16:195--234, 1993.
3 H.
At-Kaci, A.
Podelski, and S.
C. Goldstein.
Order-sorted feature theory unification.
Journal of Logic, Language and Information, 30:99--124, 1997.
4 Ulrich
Callmeier, PET  a platform for experimentation with efficient HPSG processing techniques, Natural Language Engineering, v.6 n.1, p.99-107, March 2000 5 Bob Carpenter, The logic of typed feature structures, Cambridge University Press, New York, NY, 1992 6 L.
Ciortuz. Towards ILP-based learning of attribute path values in typed-unification grammars.
2002. (Submitted).
7 L.
Ciortuz. On compilation of head-corner bottom-up chart-based parsing with unification grammars.
In Proceedings of the IWPT 2001 International Workshop on Parsing Technologies, pages 209--212, Beijing, China, October 17--19, 2001.
8 Liviu-Virgil Ciortuz, LIGHT A Constraint Language and Compiler System for Typed-Unification Grammars, Proceedings of the 25th Annual German Conference on AI: Advances in Artificial Intelligence, p.3-17, September 16-20, 2002 9 A.
Copestake, D.
Flickinger, and I.
Sag. A Grammar of English in HPSG: Design and Implementations.
Stanford: CSLI Publications, 1999.
10 D.
Gerdemann. Term encoding of typed feature structures.
In Proceedings of the 4th International Workshop on Parsing Technologies, pages 89--97, Prague, Czech Republik, 1995.
11 R.
M. Kaplan and J.
Bresnan. Lexical-functional grammar: A formal system for grammatical representation.
In J.
Bresnan, editor, The Mental Representation of Grammatical Relations, pages 173--381.
The MIT Press, 1982.
12 Robert
Malouf, John Carroll, Ann Copestake, Efficient feature structure operations without compilation, Natural Language Engineering, v.6 n.1, p.29-46, March 2000 13 Y.
Mitsuishi, K.
Torisawa, and J.
Tsujii. HPSG-Style Underspecified Japanese Grammar with Wide Coverage.
In Proceedings of the 17th International Conference on Computational Linguistics: COLING-98, pages 867--880, 1998.
14 S.
Muggleton and L.
De Raedt.
Inductive logic programming: Theory and methods.
Journal of Logic Programming, 19,20:629--679, 1994.
15 Stefan
Mller.
Deutsche Syntax deklarativ.
Head-Driven Phrase Structure Grammar fr das Deutsche.
Number 394 in Linguistische Arbeiten.
Max Niemeyer Verlag, Tbingen, 1999.
16 Stephan
Oepen, John Carroll, Parser engineering and performance profiling, Natural Language Engineering, v.6 n.1, p.81-97, March 2000 17 Stephan Oepen, John Carroll, Ambiguity packing in constraint-based parsing: practical results, Proceedings of the first conference on North American chapter of the Association for Computational Linguistics, p.162-169, April 29-May 04, 2000, Seattle, Washington 18 Fernando C.
N. Pereira, A structure-sharing representation for unification-based grammar formalisms, Proceedings of the 23rd annual meeting on Association for Computational Linguistics, p.137-144, July 08-12, 1985, Chicago, Illinois 19 C.
Pollard and I.
Sag. Head-driven Phrase Structure Grammar.
Center for the Study of Language and Information, Stanford, 1994.
20 S.
M. Shieber, H.
Uszkoreit, F.
C. Pereira, J.
Robinson, and M.
Tyson. The formalism and implementation of PATR-II.
In J.
Bresnan, editor, Research on Interactive Acquisition and Use of Knowledge.
SRI International, Menlo Park, Calif., 1983.
21 M.
Siegel. HPSG analysis of Japanese.
In Verb-mobil: Foundations of Speech-to-Speech Translation.
Springer Verlag, 2000.
22 Hideto Tomabechi, Quasi-destructive graph unification with structure-sharing, Proceedings of the 14th conference on Computational linguistics, August 23-28, 1992, Nantes, France 23 Hans Uszkoreit, Categorial unification grammars, Proceedings of the 11th coference on Computational linguistics, August 25-29, 1986, Bonn, Germany 24 S.
Wintner and N.
Francez. Efficient implementation of unification-based grammars.
Journal of Language and Computation, 1(1):53--92, 1999.
References 1 Ted Briscoe, John Carroll, Automatic extraction of subcategorization from corpora, Proceedings of the fifth conference on Applied natural language processing, p.356-363, March 31-April 03, 1997, Washington, DC 2 Glenn Carroll and Mats Rooth.
1998. Valence induction with a head-lexicalized PCFG.
In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing, Granada, Spain.
3 Stanley
F.
Chen, Joshua Goodman, An empirical study of smoothing techniques for language modeling, Proceedings of the 34th annual meeting on Association for Computational Linguistics, p.310-318, June 24-27, 1996, Santa Cruz, California 4 Thomas M.
Cover, Joy A.
Thomas, Elements of information theory, Wiley-Interscience, New York, NY, 1991 5 Ido Dagan, Lillian Lee, Fernando C.
N. Pereira, Similarity-Based Models of Word Cooccurrence Probabilities, Machine Learning, v.34 n.1-3, p.43-69, Feb.
1999 6 William B.
Frakes, Ricardo Baeza-Yates, Information retrieval: data structures and algorithms, Prentice-Hall, Inc., Upper Saddle River, NJ, 1992 7 I.
J. Good.
1953. The population frequencies of species and the estimation of population parameters.
Biometrika, 40:16--264.
8 Slava
M.
Katz. 1987.
Estimation of probabilities from sparse data for the language model component of a speech recogniser.
IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3):400--401.
9 Anna
Korhonen, Semantically motivated subcategorization acquisition, Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition, p.51-58, July 12-12, 2002, Philadelphia, Pennsylvania 10 Anna Korhonen.
2002b. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge, UK.
11 Maria
Lapata, The disambiguation of nominalizations, Computational Linguistics, v.28 n.3, p.357-388, September 2002 12 P.
S. Laplace.
1814. Essai philosophique sur les probabilites.
Mme. Ve.
Courcier. 13 Lillian Lee, Measures of distributional similarity, Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, p.25-32, June 20-26, 1999, College Park, Maryland 14 Lillian Lee.
2001. On the effectiveness of the skew divergence for statistical language analysis.
In Artificial Intelligence and Statistics 2001, pages 65--72.
15 Geoff
Leech.
1992. 100 million words of English: the British National Corpus.
Language Research, 28(1):1--13.
16 Beth
Levin.
1993. English Verb Classes and Alternations.
Chicago University Press, Chicago.
17 Jianhua
Lin.
1991. Divergence measures based on the Shannon entropy.
IEEE Transactions on Information Theory, 37(1):145--151.
18 Dekang
Lin, Automatic retrieval and clustering of similar words, Proceedings of the 17th international conference on Computational linguistics, p.768-774, August 10-14, 1998, Montreal, Quebec, Canada 19 Christopher D.
Manning, Hinrich Schtze, Foundations of statistical natural language processing, MIT Press, Cambridge, MA, 1999 20 Diana McCarthy, Using semantic preferences to identify verbal participation in role switching alternations, Proceedings of the first conference on North American chapter of the Association for Computational Linguistics, p.256-263, April 29-May 04, 2000, Seattle, Washington 21 George A.
Miller. 1990.
WordNet: An online lexical database.
International Journal of Lexicography, 3(4):235--312.
22 C.
Spearman. 1904.
The proof and measurement of association between two things.
American Journal of Psychology, 15:72--101.
23 I.
H. Witten and T.
C. Bell.
1991. The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression.
IEEE Transactions on Information Theory, 37(4):1085--1094.
References 1 Naoki Abe, Hiroshi Mamitsuka, Query Learning Strategies Using Boosting and Bagging, Proceedings of the Fifteenth International Conference on Machine Learning, p.1-9, July 24-27, 1998 2 Leo Breiman, Bagging predictors, Machine Learning, v.24 n.2, p.123-140, Aug.
1996 3 Walter Daelemans, Antal Van Den Bosch, Jakub Zavrel, Forgetting Exceptions is Harmful in Language Learning, Machine Learning, v.34 n.1-3, p.11-41, Feb.
1999 4 I.
Dagan and Y.
Krymolowski. 2001.
Compositional memory-based partial parsing.
In R.
Bod, R.
Scha, and K.
Sima'an, editors, Data-Oriented Parsing, chapter II.6.
CSLI Publications, Stanford.
to appear, http://turing.wins.uva.nl/~rens/dopbook.html.
5 Yoav
Freund, Robert E.
Schapire, A decision-theoretic generalization of on-line learning and an application to boosting, Proceedings of the Second European Conference on Computational Learning Theory, p.23-37, March 13-15, 1995 6 Vronique Hoste and Walter Daelemans.
2000. Comparing bagging and boosting for natural language processing tasks: a typicality approach.
In Ad Feelders, editor, Proceedings of Benelearn 2000, pages 101--108.
7 Yuval
Krymolowski and Zvika Marx.
2002. Clustering the space of phrases identified by an ensemble of supervised shallow parsers.
In Proceedings of ICML'02 Workshop on Text Learning (TextML-2002), Sydney, Australia, July.
8 Mitchell
P.
Marcus, Mary Ann Marcinkiewicz, Beatrice Santorini, Building a large annotated corpus of English: the penn treebank, Computational Linguistics, v.19 n.2, June 1993 9 Ted Pedersen, Assessing system agreement and instance difficulty in the lexical sample tasks of SENSEVAL-2, Proceedings of the ACL-02 workshop on Word sense disambiguation: recent successes and future directions, p.40-46, July 11, 2002 10 L.
A. Ramshaw and M.
P. Marcus.
1995. Text chunking using transformation-based learning.
In Proceedings of the Third Workshop on Very Large Corpora.
11 Dan
Roth, Learning to resolve natural language ambiguities: a unified approach, Proceedings of the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence, p.806-813, July 1998, Madison, Wisconsin, United States 12 Remko Scha, Rens Bod, and Khalil Sima'an.
1999. A memory-based model of syntactic analysis: Data-oriented parsing.
Journal of Experimental and Theoretical AI, 11:409--440.
13 H.
S. Seung, M.
Opper, H.
Sompolinsky, Query by committee, Proceedings of the fifth annual workshop on Computational learning theory, p.287-294, July 27-29, 1992, Pittsburgh, Pennsylvania, United States 14 David Bingham Skalak, Prototype selection for composite nearest neighbor classifiers, University of Massachusetts, Amherst, MA, 1997 15 Erik F.
Tjong Kim Sang, Sabine Buchholz, Introduction to the CoNLL-2000 shared task: chunking, Proceedings of the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning, September 13-14, 2000, Lisbon, Portugal 16 Jianping Zhang, Selecting typical instances in instance-based learning, Proceedings of the ninth international workshop on Machine learning, p.470-479, July 1992, Aberdeen, Scotland, United Kingdom
References 1 R.
C. Angell, G.
E. Freund, and P.
Willett. 1983.
Automatic spelling correction using a trigram similarity measure.
Information Processing and Management, 19(4):255--261.
2 Michele
Banko, Eric Brill, Scaling to very very large corpora for natural language disambiguation, Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, p.26-33, July 06-11, 2001, Toulouse, France 3 Walter Daelemans, Antal Van Den Bosch, Jakub Zavrel, Forgetting Exceptions is Harmful in Language Learning, Machine Learning, v.34 n.1-3, p.11-41, Feb.
1999 4 W.
Daelemans, J.
Zavrel, K.
van der Sloot, and A.
van den Bosch.
2001. Timbl: Tilburg memory based learner, version 4.0, reference guide.
Technical report, University of Antwerp.
5 M.
El-Bze, B.
Mrialdo, B.
Rozeron, and A.
Derouault. 1994.
Accentuation automatique des textes par des mthodes probabilistes.
Techniques et sciences informatique, 16(6):797--815.
6 Sofia
N.
Galicia-Haro, Igor A.
Bolshakov, Alexander F.
Gelbukh, A Simple Spanish Part of Speech Tagger for Detection and Correction of Accentuation Error, Proceedings of the Second International Workshop on Text, Speech and Dialogue, p.219-222, September 01, 1999 7 A.
Kilgarriff, editor.
2001. Proceedings of SENSEVAL-2, Association for Computational Linguistics Workshop, Toulouse, France.
8 G.
Nagy, Nagy N., and M.
Sabourin. 1998.
Signes diacritiques: perdus et retrouvs.
In Actes du 1er Collque International Francophone sur l'crit et le Document CIFED'98, pages 404--412, Quebec, Canada.
9 J.
Ross Quinlan, C4.5: programs for machine learning, Morgan Kaufmann Publishers Inc., San Francisco, CA, 1993 10 M.
Simard. 1998.
Automatic insertion of accents in French text.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing EMNLP-3, Granada.
11 D.
Tufis; and A.
Chitccedil;u. 1999.
Automatic diacritics insertion in Romanian texts.
In Proceedings of the International Conference on Computational Lexicography COMPLEX'99, Pecs, Hungary, June.
12 C.
J. Van Rijsbergen, Information Retrieval, Butterworth-Heinemann, Newton, MA, 1979 13 David Yarowsky, Decision lists for lexical ambiguity resolution: application to accent restoration in Spanish and French, Proceedings of the 32nd annual meeting on Association for Computational Linguistics, p.88-95, June 27-30, 1994, Las Cruces, New Mexico 14 D.
Yarowsky, 1999.
Corpus-based techniques for Restoring accents in Spanish and French Text, pages 99--120.
Kluwer Academics Publisher.
References 1 Daniel M.
Bikel, Scott Miller, Richard Schwartz, Ralph Weischedel, Nymble: a high-performance learning name-finder, Proceedings of the fifth conference on Applied natural language processing, p.194-201, March 31-April 03, 1997, Washington, DC 2 A.
Borthwick, J.
Sterling, E.
Agichtein, and R.
Grishman. 1998.
Exploiting diverse knowledge sources via maximum entropy in named entity recognition.
3 S.
Buchholz and A.
van den Bosch.
2000. Integrating seed names and n-grams for a named entity list and classifier.
4 S.
Cucerzan and D.
Yarowsky. 1999.
Language independent named entity recognition combining morphological and contextual evidence.
5 A.
Mikheev, M.
Moens, and C.
Grover. 1999.
Named entity recognition without gazetteers.
6 Jon
D.
Patrick and Ishaan Goyal.
2001. Boosted decision graphs for nlp learning tasks.
In Walter Daelemans and Rmi Zajac, editors, Proceedings of CoNLL-2001, pages 58--60.
Toulouse, France.
References 1 Daniel M.
Bikel, Scott Miller, Richard Schwartz, Ralph Weischedel, Nymble: a high-performance learning name-finder, Proceedings of the fifth conference on Applied natural language processing, p.194-201, March 31-April 03, 1997, Washington, DC 2 Eric Brill, A simple rule-based part of speech tagger, Proceedings of the third conference on Applied natural language processing, March 31-April 03, 1992, Trento, Italy 3 Eric Brill, Transformation-based error-driven learning and natural language processing: a case study in part-of-speech tagging, Computational Linguistics, v.21 n.4, p.543-565, December 1995 4 Nigel Collier, Chikashi Nobata, Jun-ichi Tsujii, Extracting the names of genes and gene products with a hidden Markov model, Proceedings of the 18th conference on Computational linguistics, p.201-207, July 31-August 04, 2000, Saarbrcken, Germany 5 N.
Collier, K.
Takeuchi, C.
Nobata, J.
Fukumoto, and N.
Ogata. 2002.
Progress on multi-lingual named entity annotation guidelines using RDF(S).
In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC'2002), Las Palmas, Spain, pages 2074--2081, May 27th -June 2nd.
6 M.
Collins and Y.
Singer. 1999.
Unsupervised models for named entity classification.
In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.
7 Corinna
Cortes, Vladimir Vapnik, Support-Vector Networks, Machine Learning, v.20 n.3, p.273-297, Sept.
1995 8 K.
Fukuda, T.
Tsunoda, A.
Tamura, and T.
Takagi. 1998.
Toward information extraction: identifying protein names from biological papers.
In Proceedings of the Pacific Symposium on Biocomputing'98 (PSB'98), pages 707--718, January.
9 Thorsten
Joachims, Making large-scale support vector machine learning practical, Advances in kernel methods: support vector learning, MIT Press, Cambridge, MA, 1999 10 Taku Kudoh, Yuji Matsumoto, Use of support vector learning for chunk identification, Proceedings of the 2nd workshop on Learning language in logic and the 4th conference on Computational natural language learning, September 13-14, 2000, Lisbon, Portugal 11 DARPA.
1995. Proceedings of the Sixth Message Understanding Conference(MUC-6), Columbia, MD, USA, November.
Morgan Kaufmann.
12 Chikashi
Nobata, Nigel Collier, Jun'ichi Tsujii, Comparison between tagged corpora for the named entity task, Proceedings of the workshop on Comparing corpora, p.20-27, October 07-07, 2000, Hong Kong 13 David D.
Palmer, David S.
Day, A statistical profile of the Named Entity task, Proceedings of the fifth conference on Applied natural language processing, p.190-193, March 31-April 03, 1997, Washington, DC 14 L.
Rabiner and B.
Juang. 1986.
An introduction to hidden Markov models.
IEEE ASSP Magazine, pages 4--16, January.
15 Pasi
Tapanainen, Timo Jrvinen, A non-projective dependency parser, Proceedings of the fifth conference on Applied natural language processing, p.64-71, March 31-April 03, 1997, Washington, DC 16 Y.
Tateishi, T.
Ohta, N.
Collier, C.
Nobata, K.
Ibushi, and J.
Tsujii. 2000.
Building an annotated corpus in the molecular-biology domain.
In COLING'2000 Workshop on Semantic Annotation and Intelligent Content, Luxemburg, 5th--6th August.
17 C.
J. Van Rijsbergen, Information Retrieval, Butterworth-Heinemann, Newton, MA, 1979 18 Vladimir N.
Vapnik, The nature of statistical learning theory, Springer-Verlag New York, Inc., New York, NY, 1995 19 A.
J. Viterbi.
1967. Error bounds for convolutions codes and an asymptotically optimum decoding algorithm.
IEEE Transactions on Information Theory, IT-13(2):260--269.
References 1 Steven Abney.
1996. Partial Parsing via Finite-State Cascades.
In Proceedings of the ESSLLI-96 Workshop on "Robust Parsing", Prague.
2 Thorsten
Brants, TnT: a statistical part-of-speech tagger, Proceedings of the sixth conference on Applied natural language processing, p.224-231, April 29-May 04, 2000, Seattle, Washington 3 Christian Braun.
1999. Flaches und robustes Parsen deutscher Satzgefge.
Diplomarbeit, Universitt des Saarlandes, Saarbrcken.
4 Walter
Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch.
2001. Tilburg Memory Based Learner, version 4.0, Reference Guide.
Technical Report 01--04, ILK, Tilburg.
5 Peter
Eisenberg.
1999 Grundri der deutschen Grammatik, volume 2: Der Satz.
Metzler, Stuttgart.
6 C.
Grover, C.
Matheson, A.
Mikheev, and M.
Moens. 2000.
LT TTT a Flexible To-kenisation Tool.
In Proceedings of the Second Language Resources and Evaluation Conference (LREC2000), 31 May--2 June.
7 Erhard
W.
Hinrichs, Sandra Kbler, Frank H.
Mller, and Tylman Ule.
2002. A Hybrid Archictecture for Robust Parsing of German.
In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC 2002), Las Palmas, Gran Canaria.
8 Tilman
Hhle.
1986. Der Begriff 'Mittelfeld', Anmerkungen ber die Theorie der topologischen Felder.
In Akten des Siebten Internationalen Germanistenkongresses, pages 329--340, Gttingen.
9 Walt
Detmar Meurers.
2002. On the use of electronic corpora for theoretical linguistics.
Case studies from the syntax of German.
Lingua. Forthcoming.
10 Gnter
Neumann, Christian Braun, Jakub Piskorski, A divide-and-conquer strategy for shallow parsing of German free texts, Proceedings of the sixth conference on Applied natural language processing, p.239-246, April 29-May 04, 2000, Seattle, Washington 11 Li-Shiuan Peh and Christopher H.
Ting. 1996.
A Divide-and-Conquer Strategy for Parsing.
In Proceedings of the ACL/SIGPARSE Fifth International Workshop on Parsing Technologies, pages 57--66.
12 L.
A. Ramshaw and M.
P. Marcus.
1995. Text Chunking using Transformation-Based Learning.
In Proc.
of third workshop on very large corpora, pages 82--94, June.
13 Helmut
Schmid.
2000. Lopar: Design and Implementation.
Arbeitspapiere des Sonderforschungsbereichs 340, Linguistic Theory and the Foundations of Computational Linguistics, 149, Institut fr Maschinelle Sprachverarbeitung, Universitt Stuttgart.
14 Rosmary
Stegmann, Heike Telljohann, and Erhard W.
Hinrichs. 2000.
Stylebook for the German Treebank in VERBMOBIL.
Technical Report Verbmobil-Report 239, Universitt Tbingen.
References 1 A.
Banfield. 1982.
Unspeakable Sentences.
Routledge and Kegan Paul, Boston.
2 Lynn
Carlson, Daniel Marcu, Mary Ellen Okurowski, Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory, Proceedings of the Second SIGdial Workshop on Discourse and Dialogue, p.1-10, September 01-02, 2001, Aalborg, Denmark 3 Eduard Hendrik Hovy, Generating natural language under pragmatic constraints, 1987 4 Brett Kessler, Geoffrey Numberg, Hinrich Schtze, Automatic detection of text genre, Proceedings of the 35th annual meeting on Association for Computational Linguistics, p.32-38, July 07-12, 1997, Madrid, Spain 5 Dekang Lin, Automatic retrieval and clustering of similar words, Proceedings of the 17th international conference on Computational linguistics, p.768-774, August 10-14, 1998, Montreal, Quebec, Canada 6 Ellen Riloff, Rosie Jones, Learning dictionaries for information extraction by multi-level bootstrapping, Proceedings of the sixteenth national conference on Artificial intelligence and the eleventh Innovative applications of artificial intelligence conference innovative applications of artificial intelligence, p.474-479, July 18-22, 1999, Orlando, Florida, United States 7 E.
Spertus. 1997.
Smokey: Automatic recognition of hostile messages.
In Proc.
IAAI. 8 Simone Teufel, Marc Moens, What's yours and what's mine: determining intellectual attribution in scientific text, Proceedings of the 2000 Joint SIGDAT conference on Empirical methods in natural language processing and very large corpora: held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics, p.9-17, October 07-08, 2000, Hong Kong 9 Janyce M.
Wiebe, Rebecca F.
Bruce, Thomas P.
O'Hara, Development and use of a gold-standard data set for subjectivity classifications, Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, p.246-253, June 20-26, 1999, College Park, Maryland 10 Janyce Wiebe, Rebecca Bruce, Matthew Bell, Melanie Martin, Theresa Wilson, A corpus study of evaluative and speculative language, Proceedings of the Second SIGdial Workshop on Discourse and Dialogue, p.1-10, September 01-02, 2001, Aalborg, Denmark 11 J.
Wiebe, T.
Wilson, and M.
Bell. 2001b.
Identifying collocations for recognizing opinions.
In ACL-01 Workshop on Collocation.
12 Janyce
M.
Wiebe, Tracking point of view in narrative, Computational Linguistics, v.20 n.2, p.233-287, June 1994 13 Janyce Wiebe, Learning Subjective Adjectives from Corpora, Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence, p.735-740, July 30-August 03, 2000
References 1 Eric Bauer, Ron Kohavi, An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants, Machine Learning, v.36 n.1-2, p.105-139, July-August 1999 2 W.
Daelemans, J.
Zavrel, and S.
Berck. 1996.
MBT: A memory-based part of speech tagger-generator.
3 Gerard
Escudero, Llus Mrquez, German Rigau, Boosting Applied toe Word Sense Disambiguation, Proceedings of the 11th European Conference on Machine Learning, p.129-141, May 31-June 02, 2000 4 Yoav Freund, Robert E.
Schapire, A decision-theoretic generalization of on-line learning and an application to boosting, Journal of Computer and System Sciences, v.55 n.1, p.119-139, Aug.
1997 5 D.
Opitz and R.
Maclin. 1999.
Popular ensemble methods: An empirical study.
In Journal of Artificial Intelligence Researce, 11, pages 169--198.
6 A.
Ratnaparkhi. 1996.
A maximum entropy part-of-speech tagger.
In Proceedings of the Empirical Methods in Natural Language Processing Conference, Philadelphia, PA, May 17--18.
ACL. 7 F.
Sanchez. 1995.
Development of a Spanish version of the Xerox tagger.
8 Robert
E.
Schapire, Yoram Singer, Improved Boosting Algorithms Using Confidence-rated Predictions, Machine Learning, v.37 n.3, p.297-336, Dec.
1999 9 Robert E.
Schapire, Yoram Singer, BoosTexter: A Boosting-based Systemfor Text Categorization, Machine Learning, v.39 n.2-3, p.135-168, May-June 2000 10 R.
E. Schapire.
2002. The boosting approach to machine learning.
an overview.
In MSRI Workshop on Nonlinear Estimation and Classification.
11 David
Yarowsky, Richard Wicentowski, Minimally supervised morphological analysis by multimodal alignment, Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, p.207-216, October 03-06, 2000, Hong Kong
 ThePotentialandLimitationsofAutomatic SentenceExtractionforSummarization Chin-YewLinandEduardHovy UniversityofSouthernCalifornia/InformationSciencesInstitute 4676AdmiraltyWay MarinadelRey,CA90292,USA {cyl,hovy}@isi.edu   Abstract Inthispaperwepresentanempiricalstudyof thepotentialandlimitationofsentenceextraction in text summarization. Our results show that the single document generic summarizationtaskasdefinedinDUC2001needstobe carefullyrefocusedasreflectedinthelowinter-human agreement at 100-word 1 (0.40 score) and high upper bound at full text 2  (0.88) summaries. For 100-word summaries, theperformanceupperbound,0.65,achieved oracleextracts 3.Suchoracleextractsshowthe promise of sentence extraction algorithms; however, we first need to raise inter-human agreementtobeabletoachievethisperformance level. We show that compression is a promisingdirectionandthatthecompression ratioofsummariesaffectsaveragehumanand systemperformance. 1 Introduction Most automatic text summarization systems existing todayareextractionsystemsthatextractpartsoforiginal documents and output the results as summaries. Among them, sentence extraction is by far the most  1 Wecomputeunigramco-occurrencescoreofapairofmanual summaries, one as candidate summary and the other as reference. 2 Wecomputeunigramco-occurrencescoresofafulltextand itsmanualsummariesof100words.Thesescoresarethebest achievable using the unigram co-occurrence scoring metric sinceallpossiblewordsarecontainedinthefulltext.Three manualsummariesareused. 3 Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 1005words. popular (Edmundson 1969, Luhn 1969, Kupiec et al. 1995,Goldsteinetal.1999,HovyandLin1999).The majorityofsystemsparticipatinginthepastDocument Understanding Conference (DUC 2002), a large scale summarization evaluation effort sponsored by the US government, are extraction based. Although systems basedoninformationextraction(RadevandMcKeown 1998,Whiteetal.2001,McKeownetal.2002)anddiscourseanalysis(Marcu1999b,Strzalkowskietal.1999) alsoexist,wefocusourstudyonthepotentialandlimitationsofsentenceextractionsystemswiththehopethat ourresultswillfurtherprogressinmostoftheautomatic textsummarizationsystemsandevaluationsetup. TheevaluationresultsofthesingledocumentsummarizationtaskinDUC2001and2002(DUC2002,Paul& Liggett2002)indicatethatmostsystemsareasgoodas thebaselinelead-basedsystemandthathumansaresignificantlybetter,thoughnotbymuch.Thisleadstothe beliefthatlead-basedsummariesareasgoodaswecan get for single document summarization in the news genre, implying that the research community should investfutureeffortsinotherareas.Infact,averyshort summary of about 10 words (headline-like) task has replaced the single document 100-word summary task inDUC2003.Thegoalofthisstudyistorenewinterest in sentence extraction-based summarization and its evaluationby estimatingtheperformanceupperbound usingoracleextracts,andtohighlighttheimportanceof taking into account the compression ratio when we evaluateextractsorsummaries. Section 2 gives an overview of DUC relevant to this study.Section3introducesa recall-basedunigram cooccurrenceautomaticevaluationmetric.Section4presentstheexperimentaldesign.Section5showstheempirical results. Section 6 concludes this paper and discussesfuturedirections.  2 DocumentUnderstandingConference Fully automatic single-document summarization was one of two main tasks in the 2001 Document UnderstandingConference.Participantswererequiredtocreate a generic 100-word summary.There were 30 test setsinDUC2001andeachtestsetcontainedabout10 documents.Foreachdocument,onesummarywascreated manually as the ideal model summary at approximately 100 words.We will refer to this manual summary as H1. Two other manual summaries were alsocreatedataboutthatlength.Wewillrefertothese twoadditionalhumansummariesasH2  andH3.Inaddition,baselinesummarieswerecreatedautomaticallyby taking the first n sentences up to 100 words. We will referthisbaselineextractasB1. 3 UnigramCo-OccurrenceMetric Inarecentstudy(LinandHovy2003),weshowedthat therecall-basedunigramco-occurrenceautomaticscoringmetriccorrelatedhighlywithhumanevaluationand has high recall and precision in predicting statistical significanceofresultscomparingwithitshumancounterpart. The idea is to measure the content similarity betweenasystemextractandamanualsummaryusing simple n-gram overlap. A similar idea called IBM BLEU score has proved successful in automatic machinetranslationevaluation(Papinenietal.2001,NIST 2002).Forsummarization,wecanexpressthedegreeof contentoverlapintermsofn-grammatchesasthefollowingequation: )1( )( )( }{ }{       = UnitsModelCCgramn UnitsModelCCgramn match n gramnCount gramnCount C  Modelunitsaresegmentsof manual summaries.They are typically either sentences or elementary discourse unitsasdefinedbyMarcu(1999b).Count match (n-gram) is the maximum number of n-grams co-occurring in a systemextractandamodelunit.Count(n-gram)isthe number of n-grams in the model unit. Notice that the averagen-gramcoveragescore,C n,asshowninequation1,isarecall-basedmetric,sincethedenominatorof equation 1 is the sum total of the number of n-grams occurringinthemodelsummaryinsteadofthesystem summaryandonlyonemodelsummaryisusedforeach evaluation. In summary, the unigram co-occurrence statisticsweuseinthefollowingsectionsarebasedon thefollowingformula: )2(logexp),(         =  = j in nn CwjiNgram  Wherej i,iandjrangefrom1to4,andw n is1/(ji+1).Ngram(1,4)isaweightedvariablelengthn-gram match score similar to the IBM BLEU score; while Ngram(k,k),i.e.i=j=k,issimplytheaveragek-gram co-occurrencescoreC k .Inthisstudy,weseti=j=1, i.e.unigramco-occurrencescore. Withatestcollectionavailableandanautomaticscoring metric defined, we describe the experimental setup in thenextsection. 4 ExperimentalDesigns As stated in the introduction, we aim to find the performanceupperboundofasentenceextractionsystem andtheeffectofcompressionratioonitsperformance. We present our experimental designs to address these questionsinthefollowingsections. 4.1 Performance Upper Bound Estimation UsingOracleExtract Inordertoestimatethepotentialofsentenceextraction systems,itisimportanttoknowtheupperboundthatan ideal sentence extraction method might achieve and howfarthestate-of-the-artsystemsareawayfromthe bound. If the upper bound is close to state-of-the-art systems performance then we need to look for other summarizationmethodstoimproveperformance.Ifthe upper bound is much higher than any current systems canachieve,thenitisreasonabletoinvestmoreeffortin sentence extraction methods. The question is how to estimatetheperformanceupperbound.Oursolutionis tocastthisestimationproblemasanoptimizationproblem. We exhaustively generate all possible sentence combinationsthatsatisfygivenlengthconstraintsfora summary, for example, all the sentence combinations totaling 1005 words. We then compute the unigram co-occurrence score for each sentence combination, against the ideal. The best combinations are the ones withthehighestunigramco-occurrencescore.Wecall this sentence combination the oracle extract. Figure 1 showsanoracleextractfordocumentAP900424-0035. OneofitshumansummariesisshowninFigure2.The oracle extract covers almost all aspects of the human summaryexceptsentences5and6andpartofsentence 4.However,ifweallowtheautomaticextracttocontain morewords,forexample,150wordsshowninFigure3, the longeroracleextractthen covers everythinginthe humansummary.Thisindicatesthatlowercompression can boost system performance. The ultimate effect of compressioncanbecomputedusingthefulltextasthe oracleextract,sincethefulltextshouldcontaineverything included in the human summary. That situation provides the best achievable unigram co-occurrence score.Anearoptimalscorealsoconfirmsthevalidityof usingtheunigramco-occurrencescoringmethodasan automaticevaluationmethod.  4.2 Compression Ratio and Its Effect on System Performance Oneimportantfactorthat affectsthe averageperformance of sentence extraction system is the number of sentences contained in the original documents. This factorisoftenoverlookedandhasneverbeenaddressed systematically. For example, if a document contains onlyonesentencethenthisdocumentwillnotbeuseful indifferentiatingsummarizationsystemperformance there is only one choice. However, for a document of 100sentencesandassumingeachsentenceis20words long, there are C(100,5) = 75,287,520 different 100wordextracts.Thishugesearchspacelowersthechance of agreement between humans on what constitutes a good summary. It also makes system and human performance approach average since it is more likely to includesomegoodsentencesbutnotallofthem.EmpiricalresultsshowninSection5confirmthisandthat leadsustothequestionofhowtoconstructacorpusto evaluatesummarizationsystems.Wediscussthisissue intheconclusionsection. 4.3 Inter-HumanAgreementandItsEffecton SystemPerformance In this section we study how inter-human agreement affects system performance. Lin and Hovy (2002) reportedthat, comparedtoa manually createdideal,humansscoredabout0.40inaveragecoveragescoreand the best system scored about 0.35. According to these numbers,wemightassumethathumanscannotagreeto eachotheronwhatisimportantandthebestsystemis almostasgoodashumans.Ifthisistruethenestimating anupperboundusingoracleextractsismeaningless.No matterhowhightheestimatedupperboundsmaybe,we probablywouldneverbeabletoachievethatperformance due to lack of agreement between humans: the oracle approximating one human would fail miserably withanother.Thereforewesetupexperimentstoinvestigatethefollowing: 1. Whatisthedistributionofinter-humanagreement? Figure3.A150-wordoracleextractfordocumentAP900424-0035. Figure 2. A manual summary for document AP900424-0035. Figure1.A100-wordoracleextractfordocumentAP900424-0035. <DOC> <DOCNO>AP900424-0035</DOCNO> <DATE>04/24/90</DATE> <HEADLINE> <SHSNTNO="1">ElizabethTaylorinIntensiveCareUnit</S> <SHSNTNO="2">ByJEFFWILSON</S> <SHSNTNO="3">AssociatedPressWriter</S> <SHSNTNO="4">SANTAMONICA,Calif.(AP)</S> </HEADLINE> <TEXT> <SSNTNO="1">AseriouslyillElizabethTaylorbattledpneumoniaather hospital,herbreathingassistedbyaventilator,doctorssay.</S> <SSNTNO="2">HospitalofficialsdescribedherconditionlateMonday asstabilizingafteralungbiopsytodeterminethecauseofthepneumonia.</S> <SSNTNO="3">Analysisofthetissuesamplewasexpectedtotakeuntil Thursday,saidherspokeswoman,ChenSam.</S> <SSNTNO="9">Anotherspokewomanfortheactress,LisaDelFavaro, saidMissTaylor'sfamilywasatherbedside.</S> <SSNTNO="13">``Itisserious,buttheyarereallypleasedwithher progress.</S> <SSNTNO="22">Duringanearlyfatalboutwithpneumoniain1961, MissTaylorunderwentatracheotomy,anincisionintoherwindpipeto helpherbreathe.</S> </TEXT> </DOC> <DOC> <TEXT> <SSNTNO="1">ElizabethTaylorbattledpneumoniaatherhospital, assistedbyaventilator,doctorssay.</S> <SSNTNO="2">HospitalofficialsdescribedherconditionlateMonday asstabilizingafteralungbiopsytodeterminethecauseofthepneumonia.</S> <SSNTNO="3">AnalysisofthetissuesamplewasexpectedtobecompletebyThursday.</S> <SSNTNO="4">Ms.Sam,spokeswomansaid"itisserious,buttheyare reallypleasedwithherprogress.</S> <SSNTNO="5">She'snotwell.</S> <SSNTNO="6">She'snotonherdeathbedoranything.</S> <SSNTNO="7">Anotherspokeswoman,LisaDelFavaro,saidMiss Taylor'sfamilywasatherbedside.</S> <SSNTNO="8">Duringanearlyfatalboutwithpneumoniain1961,Miss Taylorunderwentatracheotomytohelpherbreathe.</S> </TEXT> </DOC> <DOC> <DOCNO>AP900424-0035</DOCNO> <DATE>04/24/90</DATE> <HEADLINE> <SHSNTNO="1">ElizabethTaylorinIntensiveCareUnit</S> <SHSNTNO="2">ByJEFFWILSON</S> <SHSNTNO="3">AssociatedPressWriter</S> <SHSNTNO="4">SANTAMONICA,Calif.(AP)</S> </HEADLINE> <TEXT> <SSNTNO="1">AseriouslyillElizabethTaylorbattledpneumoniaather hospital,herbreathingassistedbyaventilator,doctorssay.</S> <SSNTNO="2">HospitalofficialsdescribedherconditionlateMonday asstabilizingafteralungbiopsytodeterminethecauseofthepneumonia.</S> <SSNTNO="3">Analysisofthetissuesamplewasexpectedtotakeuntil Thursday,saidherspokeswoman,ChenSam.</S> <SSNTNO="4">The58-year-oldactress,whowonbest-actressOscars for``Butterfield8''and``Who'sAfraidofVirginiaWoolf,''hasbeen hospitalizedmorethantwoweeks.</S> <SSNTNO="8">Herconditionispresentlystabilizingandherphysicians arepleasedwithherprogress.''</S> <SSNTNO="9">Anotherspokewomanfortheactress,LisaDelFavaro, saidMissTaylor'sfamilywasatherbedside.</S> <SSNTNO="13">``Itisserious,buttheyarereallypleasedwithher progress.</S> <SSNTNO="14">She'snotwell.</S> <SSNTNO="15">She'snotonherdeathbedoranything,''Ms.Samsaid lateMonday.</S> <SSNTNO="22">Duringanearlyfatalboutwithpneumoniain1961, MissTaylorunderwentatracheotomy,anincisionintoherwindpipeto helpherbreathe.</S> </TEXT> </DOC>  2. Howdoesastate-of-the-artsystemdifferfrom averagehumanperformanceatdifferentinterhumanagreementlevels? We present our results in the next section using 303 newspaperarticlesfromtheDUC2001singledocument summarizationtask.Besidestheoriginaldocuments,we also have three human summaries, one lead summary (B1), and one automatic summary from one top performingsystem(T)foreachdocument. 5 Results In order to determine the empirical upper and lower bounds of inter-human agreement, we first ran crosshumanevaluationusingunigramco-occurrencescoring through six human summary pairs, i.e. (H1,H2), (H1,H3),(H2,H1),(H2,H3),(H3,H1),and(H3,H2).For a summary pair (X,Y), we used X as the model summaryandYasthesystemsummary.Figure4showsthe distributionsoffourdifferentscenarios.TheMaxHdistribution picks the best inter-human agreement scores foreachdocument,theMinHdistributiontheminimum one,theMedHdistributionthemedian,andthe AvgH distributiontheaverage.Theaverageofthebestinterhuman agreement and the average of average interhumanagreementdifferbyabout10percentinunigram co-occurrencescoreand18percentbetweenMaxHand MinH. These big differences might come from two sources. The first one is the limitation of the unigram 0 10 20 30 40 50 60 70 80 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 UnigramCo-occurrenceScores #  of  I n st an ce s AvgH MaxH MedH MinH AverageMAX=0.50 AverageAVG=0.40 AverageMED=0.39 AverageMIN=0.32 Figure 4. DUC 2001 single document interhuman unigram co-occurrence score distributions for maximum, minimum, average, and median. 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00 D 4 1.
A P 88 12 1 1 -0 0 2 7 D 53 . FB I S 3 2 2 9 4 2 D3 1 .L A 0 2 1 6 8 9 -0 2 27 D3 4 .A P8 8 0 9 14 -0 07 9 D 5 3 .A P8 8 0 8 16 -0 23 4 D 2 8 .L A 1 1 0 5 9 0 0 0 38 D 1 9.
A P 8 80 33 0 01 1 9 D1 4 . A P 9 01 01 0 00 3 6 D2 2 . L A 0 70 18 900 8 0 D 2 7 . LA 10 07 89 0 00 7 D 1 2.
FT9 34 1 1 0 1 4 D2 2 .A P8 8 1 2 16 -0 01 7 D3 7 .F B I S 311 91 9 D 1 9 .L A 1 0 2 1 8 9 0 1 51 D 0 5.
F T 9 41 1 54 7 D 5 7.
L A 1 10 58 90 0 82 D3 4 . A P 8 80 91 3 02 0 4 D 4 5.
A P 90 06 2 5 0 1 6 0 D 5 0 .A P8 8 1 2 22 -0 11 9 D 1 4 .A P9 0 1 0 12 0 03 2 D 4 1.
S JM N 9 1 0 6 07 1 0 2 2 D 4 3.
F T 9 23 5 85 9 D0 8 . A P 8 90 31 6 00 1 8 D 1 9.
A P 88 06 2 3 -0 1 3 5 D 4 3 . FT9 3 3 8 9 4 1 D 4 4 . FT9 3 4 9 1 1 6 D 1 2 . W S J 8 70 22 7-01 49 D0 4 . FT 92 35 08 9 D 1 5 .A P8 9 0 3 02 0 06 3 D 0 4.
F T 9 23 6 03 8 D3 7 . A P 8 90 70 4 00 4 3 D 12 . W S J 8 7 01 23 0 1 0 1 D 1 5.
A P 89 05 1 1 -0 1 2 6 D 1 5.
A P 90 0 5 21 -0 0 6 3 D 0 6.
FT9 22 -1 0 2 0 0 D3 4 .A P9 0 0 6 01 -0 04 0 DocumentIDs Un i g r a m  C o -o ccu r r en ce  Sc o r e s MaxH B1 T E100 E150 FT AvgMaxH AvgB1 AvgT AvgE100 AvgE150 AvgFT Figure5.DUC2001singledocumentinter-human,baseline,system,100-word,150-word,andfulltext oracleextractsunigramco-occurrencescoredistributions(#ofsentences<=30).DocumentIDsaresorted bydecreasingMaxH.  co-occurrencescoringappliedtomanualsummariesthat itcannotrecognizesynonymsorparaphrases.Thesecondoneisthetruelackofagreementbetweenhumans. Wewouldliketoconductanin-depthstudytoaddress this question, and would just assume the unigram cooccurrencescoringisreliable. In other experiments, we used the best inter-human agreementresultsasthereferencepointforhumanperformanceupperbound.Thisalsoimpliedthatweused the human summary achieving the best inter-human agreementscoreasourreferencesummary. Figure 5 shows the unigram co-occurrence scores of human,baseline,systemT,andthreeoracleextraction systemsatdifferentextractionlengths.Wegeneratedall possible sentence combinations that satisfied 1005 wordsconstraints.Duetocomputation-intensivenature ofthistask,weonlyuseddocumentswithfewerthan30 sentences. We then computed the unigram cooccurrencescoreforeachcombination,selectedthebest oneastheoracleextraction,andplottedthescoreinthe figure.Thecurvefor1005wordsoracleextractionsis theupperboundthat asentence extractionsystem can achieve within the given word limit. If an automatic systemisallowedtoextractmorewords,wecanexpect that longer extracts would boost system performance. Thequestionishow muchbetterandwhatistheultimate limit? To address these questions, we also computed unigram co-occurrence scores for oracle extractionsof1505wordsandfulltext 4 .Theperformanceoffulltextistheultimateperformanceanextractionsystemcanreachusingtheunigramco-occurrence scoring method. We also computed the scores of the leadbaselinesystem(B1)andanautomaticsystem(T). The average unigram co-occurrence score for full text (FT)was0.833,1505words(E150)was0.796,1005 words (E100) was 0.650, the best inter-human agreement(MaxH)was0.546,systemTwas0.465,andbaselinewas0.456.Itisinterestingtonotethatthestate-ofthe-artsystemperformedatthesamelevelasthebaselinesystembutwasstillabout10%awayfromhuman. The10%differencebetweenE100andMaxH(0.650vs. 0.546)implies we might needto constrainthumansto focustheirsummariesincertainaspectstoboostinterhumanagreementtothelevelofE100;whilethe15% and24%improvementsfromE100toE150andFTindicatecompressionwouldhelppushoverallsystemperformancetoamuchhigherlevel,ifasystemisableto compress longersummariesintoashorterwithout losingimportantcontent. To investigate relative performance of humans, systems, and oracle extracts at different inter-human agreement levels, we created three separate document sets based on their maximum inter-human agreement (MaxH)scores.SetSetAhadMaxHscoregreaterthan orequalto0.70,setBwasbetween0.70and0.60,and  4 We used full text asextract and computed its unigram cooccurrencescoreagainstareferencesummary. Figure 7. DUC 2001 single document interhuman,baseline,system,andfulltextunigram co-occurrencescoredistributions(SetB). Figure 6. DUC 2001 single document interhuman,baseline,system,andfulltextunigram co-occurrencescoredistributions(SetA). 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00 D41 .AP 8 8 12 11 -0 02 7 D11 .A P 8 9 040 3 -01 2 3 D1 4 .A P 88 090 20 06 2 D41 .A P 89 080 100 2 5 D 0 6 .S J MN 9 1 -06 19 1 08 1 D41 .LA 0 51 590 -00 65 D06.
W SJ 9 107 100 148 D 53 .FB IS 3229 4 2 D28 .LA 1 1 0490 -0 1 8 4 D31 .LA 0 308 89 -01 63 D13 .S J M N 91 -0 6 2 5543 4 D2 4 .L A0 5119 0 -0 1 85 D31 .LA 0 2 1689 -02 27 D05 .FT9 31 -3 88 3 D06 .SJ M N 9 1 -06 28 3 08 3 D31 .A P 8 9 100 600 2 9 D50 .AP 8 8 07 14 -0 14 2 D34 .AP 8 8 0 91 400 7 9 D1 4 .A P 88 062 90 15 9 D13 .A P 90 030 601 0 5 D31 .LA 0 3078 9 -0 0 4 7 D14 .LA 1 03 089 -00 70 DocumentIDs Un i g ra m  C o o ccu r r e n c e  Sc o r e s MaxH B1 T AvgMaxH AvgB1 AvgT AvgFT FT AvgE100 AvgE150 AvgE150=0.863 AvgFT=0.924 AvgE100=0.705 AvgMaxH=0.741 AvgT=0.525 AvgB1=0.516 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00 D53 .A P 88 081 602 3 4 D05 .FT9 21 -931 0 D41 .LA 0 81490 -0 0 3 0 D24 .AP 9 0 04 24 -0 03 5 D 3 7.
F B IS 4 -276 02 D06 .LA 0 71 590 -00 68 D28 .LA 1 1 0590 -0 0 3 8 D31 .LA 0 615 89 -01 43 D32 .A P 90 032 300 3 6 D37 .AP 9 0 10 13 -0 04 6 D56 .A P 88 112 600 0 7 D06 .AP 8 9 03 22 -0 01 0 D19 .A P 88 033 001 1 9 D14 .AP 8 8 09 13 -0 12 9 D04.
F T 923 -5 7 9 7 D44 .FT9 32 -585 5 D14 .A P 90 101 000 3 6 D37 .AP 8 8 05 10 -0 17 8 D41 .A P 89 080 501 2 6 D22 .AP 8 8 07 05 -0 10 9 D50 .A P 90 091 000 2 0 D 3 1 .S J MN 9 1 -06 08 4 22 8 D34 .A P 90 052 900 0 5 D22 .LA 0 701 89 -00 80 D04.
F T 923 -583 5 D 1 4 .A P 88 122 20 12 6 D24 .A P 90 051 200 3 8 D 2 7 .LA 1 007 89 -00 07 D31 .A P 88 100 900 7 2 D 4 5 .A P 88 052 00 26 4 D08 .A P 88 031 800 5 1 D 1 5.
F B IS4 -6 77 21 D 1 2 .FT9 3 41 1014 D 3 7 .A P 90 042 80 10 8 D4 5.
F T 921305 D 5 4 .LA 0 927 90 -00 10 D56 .SJ M N 9 1 -0 6 1 3630 5 DcoumentIDs Un i g ra m  C o o ccu r r e n c e  Sc o r e s MaxH B1 T FT AvgMaxH AvgB1 AvgT AvgFT AvgE100 AvgE150 AvgE150=0.840 AvgE100=0.698 AvgFT=0.917 AvgMaxH=0.645 AvgB1=0.509 AvgT=0.490 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00 D22 .AP 8 8 12 16 -0 01 7 D 4 4 .FT9 3 31 0881 D4 3 .A P 8 9 013 1 -02 8 0 D06 .LA 0 11 889 -00 67 D27 .AP 8 9 0 72 200 8 1 D41 .SJ M N 9 1 -06 14 2 12 6 D57 .AP 9 0 1 20 301 6 6 D31 .S J M N9 1 -0 6 0 1222 4 D 2 8 .S J MN 9 1 -06 31 2 12 0 D30 .AP 9 0 0 41 601 8 8 D27.
W S J9 111 210 136 D 3 4 .A P 88 091 30 20 4 D2 7.
W SJ 9 112 12 -0 0 80 D06 .W SJ 91 0 4 050 154 D15 .LA 1 01 690 -00 40 D50 .A P 88 122 201 1 9 D0 8 .A P 89 030 70 15 0 D4 4.
F T 933 -570 9 D37 .A P 90 042 800 0 5 D5 0 .A P 8 9 121 3 -00 0 4 D54 .W SJ 91 10 310 01 2 D1 9 .L A0 7158 9 -0 0 76 D14 .AP 9 0 08 29 -0 12 0 D43.
F T 911 -346 3 D31 .A P 8 8 092 7 -01 1 7 D28 .LA 1 211 89 -00 17 D08 .AP 8 9 0 31 600 1 8 D44.
F T 9 3 4 -8 6 2 8 D 0 8 .A P 90 072 10 11 0 D19 .A P 8 8 062 301 3 5 D 2 2 .A P 88 070 50 01 8 D32 .AP 8 9 0 32 600 8 1 D43.
F T 933 -8 9 4 1 D5 3 .A P 88 061 30 16 1 DocumentIDs Un i g ra m  C o o ccu r r e n c e  Sc o r e s MaxH B1 T AvgMaxH AvgB1 AvgT AvgFT FT AvgE100 AvgE150 AvgE150=0.790 AvgE100=0.645 AvgFT=0.897 AvgMaxH=0.536 AvgT=0.435 AvgB1=0.423 Figure 8. DUC 2001 single document interhuman,baseline,system,andfulltextunigram co-occurrencescoredistributions(SetC).  setCbetween0.60and0.50.Ahad22documents,setB 37,andsetC100.Totalwasabout52%(=159/303)of the test collection. The 1005 and 1505 words averages were computed over documents which contain at most30sentences.TheresultsareshowninFigures6, 7,and8.Inthehighestinter-humanagreementset(A), we found that average MaxH, 0.741, was higher than average 1005 words oracle extract, 0.705; while the average automatic system performance was around 0.525. This is good news since the high inter-human agreementandthebigdifference(0.18)between1005 words oracle and automatic system performance presents a research opportunity for improving sentence extraction algorithms. The scores of MaxH (0.645 for setBand0.536forsetC)intheothertwosetsareboth lowerthan1005wordsoracles(0.698forsetB,5.3% lower, and 0.645 for set C, 9.9% lower). This result suggeststhatoptimizingsentenceextractionalgorithms at the Set C level might not be worthwhile since the algorithms are likely to overfit the training data. The reason is that the average run time performance of a sentenceextractionalgorithmdependsonthemaximum inter-human agreement. For example, given a training referencesummaryT SUM1 anditsfulldocumentT DOC1, weoptimizeoursentenceextractionalgorithmtogenerateanoracleextractbasedonT SUM1 fromT DOC1 .Inthe runtime,wetestonareferencesummaryR SUM1 andits fulldocumentR DOC1 .IntheunlikelycasethatR DOC1 is thesameasT DOC1 andR SUM1 isthesameasT SUM1,i.e. T SUM1 andR SUM1 haveunigramco-occurrencescoreof1 (perfect inter-human agreement for two summaries of onedocument),theoptimizedalgorithmwillgeneratea perfectextractforR DOC1 andachievethebestperformancesinceitisoptimizedonT SUM1 . However,usually T SUM1 andR SUM1 aredifferent.Thentheperformanceof thealgorithmwillnotexceedthemaximumunigramcooccurrencescorebetweenT SUM1 andR SUM1 .Thereforeit is important to ensure high inter-human agreement to allowresearchersroomtooptimizesentenceextraction algorithmsusingoracleextracts. Finally, we present the effect of compression ratio on inter-human agreement (MaxH) and performance of baseline(B1),automaticsystemT(T),andfulltextoracle(FT)inFigure9.Compressionratioiscomputedin termsofwordsinsteadofsentences.Forexample,a100 wordssummaryofa 500 wordsdocumenthas a compressionratioof0.80(=1100/500). Thefigureshows thatthreehumansummaries(H1,H2,andH3)haddifferent compression ratios (CMPR H1, CMPR H2, and CMPR H3) for different documents but did not differ 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 1.00 D 24.
LA 0511 90018 5 D 19 . A P 88021 7 -0 1 7 5 D 24.
A P 90051 101 59 D 56.
A P 881 12 600 07 D 53.
F B IS 322 942 D 44.
F T 934133 50 D 57.
A P 89101 702 04 D 0 4.
F T 9236038 D 37.
A P 88 051 001 78 D 22.
LA 0701 89008 0 D 44.
F T 9223171 D 31.
A P 88100 9 0 0 7 2 D 31.
S J M N 9106 01 2 224 D 06.
A P 9 0102 900 35 D 08.
A P 90123 10 01 2 D 15.
LA 1016900 04 0 D 44.
F T 9332760 D 08.
S J M N 9 106 1 93081 D 08.
A P 88031 800 51 D 15 . A P 9 0052 1 -0 0 6 3 D 06 . A P 89032 2 -0 0 1 0 D3 1 .S JM N 9 1 0 6 0 84228 D 32.
A P 89050 202 0 5 D 45.
S J M N 9 106 1 82091 D 32.
A P 9 0031 30 19 1 D 39 . A P 88101 7 -0 2 3 5 D 32.
LA 040789005 1 D 53.
A P 8812 2 701 85 D 50.
A P 8 9121 000 79 D 06.
L A 080 790011 1 D 54.
LA 1021900 04 5 D 37.
S J MN 9 1 06 1 4 307 0 D 13.
W S J 9 10702 -0 0 78 D 22.
W S J 8 80923 0163 DocumentIDs Un i g r a m  C o o cc u rre n c e  Sc o r e s B1 MaxH T CMPRH1 CMPRH2 CMPRH3 FT Linear(B1) Linear(MaxH) Linear(T) Linear(FT) Figure9.DUC2001singledocinter-human,baseline,andsystemunigramco-occurrencescoreversus compressionratio.DocumentIDsaresortedbyincreasingcompressionratioCMPRH1.  much.Theunigramco-occurrencescoresforB1,T,and MaxHwerenoisybuthad ageneraltrend(Linear B1, Linear T, and Linear MaxH) of drifting into lower performance when compression ratio increased (i.e. when summaries became shorter); while the performance of FT did not exhibit a similar trend. This confirms our earlier hypothesis that humans are less likely to agree at high compression ratio and system performancewillalsosufferathighcompressionratio. TheconstancyofFTacrossdifferentcompressionratios is reasonable since FT scores should only depend on how well the unigram co-occurrence scoring method capturescontentoverlapbetweenafulltextanditsreference summaries and how likely humans use vocabularyoutsidetheoriginaldocument. 6 Conclusions In this paper we presented an empirical study of the potential and limitations of sentence extraction as a method of automatic text summarization. We showed thefollowing: (1) Howtouseoracleextractstoestimatetheperformance upper bound of sentence extraction methodsatdifferentextractlengths.Weunderstandthatsummariesoptimizedusingunigram co-occurrence score do not guarantee good quality in terms of coherence, cohesion, and overallorganization.However,wewouldargue thatagoodsummarydoesrequiregoodcontent andwewillleavehowtomakethecontentcohesive, coherent, and organized to future research. (2) Inter-humanagreementvariedalotandthedifferencebetweenmaximumagreement(MaxH) and minimum agreement (MinH) was about 18%ontheDUC2001data.Tominimizethe gap,weneedtodefinethesummarizationtask better. This has been addressed by providing guided summarization tasks in DUC 2003 (DUC 2002). We guesstimate the gap should besmallerinDUC2003data. (3) State-of-the-artsystemsperformedatthesame levelasthebaselinesystembutwerestillabout 10% away from the average human performance. (4) The potential performance gains (15% from E100 to E150 and 24% to FT) estimated by oracleextractsofdifferentsizesindicatedthat sentence compression or sub-sentence extractionarepromisingfuturedirections. (5) Therelativeperformanceofhumansandoracle extracts at three inter-human agreement intervalsshowedthatitwasonlymeaningfultooptimize sentence extraction algorithms if interhuman agreement was high. Although overall highinter-humanagreementwaslowbutsubsets of high inter-human agreement did exist. For example, about human achieved at least 60% agreement in 59 out of 303 (~19%) documentsof30sentencesorless. (6) We also studied how compression ratio affectedinter-humanagreementandsystemperformance, and the results supported our hypothesis that humans tend to agree less at high compression ratio, and similar between humansandsystems.Howtotakeintoaccount thisfactorinfuturesummarizationevaluations isaninterestingtopictopursuefurther. Usingexhaustivesearchtoidentifyoracleextractionhas beenstudiedbyotherresearchersbutindifferentcontexts.Marcu(1999a)suggestedusingexhaustivesearch tocreatetrainingextractsfromabstracts.Donawayetal. (2000)usedexhaustivesearchtogenerateallthreesentencesextractstoevaluatedifferentevaluationmetrics. Themaindifferencebetweentheirworkandoursisthat we searched for extracts of a fixed number of words whiletheylookedforextractsofafixednumberofsentences. Inthefuture,wewouldliketoapplyasimilarmethodologytodifferenttextunits,forexample,sub-sentence unitssuchaselementarydiscourseunit(Marcu1999b). We wanttostudyhowtoconstrainthesummarization task to achieve higher inter-human agreement, train sentence extraction algorithms using oracle extracts at different compression sizes, and explore compression techniquestogobeyondsimplesentenceextraction. References Donaway, R.L., Drummey, K.W., and Mather, L.A. 2000. A Comparison of Rankings Produced by Summarization Evaluation Measures. In Proceeding oftheWorkshoponAutomaticSummarization,postconferenceworkshopofANLP-NAACL-2000,Seattle,WA,USA,6978. DUC.2002.TheDocumentUnderstandingConference. http://duc.nist.gov. Edmundson, H.P. 1969. New Methods in Automatic Abstracting.JournaloftheAssociationforComputingMachinery.16(2). Goldstein, J., M. Kantrowitz, V. Mittal, and J. Carbonell. 1999. Summarizing Text Documents: Sentence Selection and Evaluation Metrics. In Proceedingsofthe22ndInternationalACMConference on Research and Development in Information Retrieval(SIGIR-99),Berkeley,CA,USA,121128. Hovy,E.andC.-Y.Lin.1999.AutomaticTextSummarization in SUMMARIST. In I. Mani and M. Maybury (eds), Advances in Automatic Text Summarization,8194.MITPress. Kupiec,J.,J.Pederson,andF.Chen.1995.ATrainable Document Summarizer. In Proceedings of the 18th InternationalACMConferenceonResearchandDevelopmentinInformationRetrieval(SIGIR-95),Seattle,WA,USA,6873. Lin,C.-Y.and E.Hovy.2002. Manualand Automatic Evaluations of Summaries. In Proceedings of the Workshop on Automatic Summarization, postconference workshop of ACL-2002, pp. 45-51, Philadelphia,PA,2002. Lin,C.-Y.andE.H.Hovy.2003.AutomaticEvaluation of Summaries Using N-gram Co-occurrence Statistics. In Proceedings of the 2003 Human Language Technology Conference (HLT-NAACL 2003), Edmonton,Canada,May27June1,2003. Luhn,H.P.1969.TheAutomaticCreationofLiterature Abstracts. IBM Journal of Research and Development.2(2),1969. Marcu,D.1999a.Theautomaticconstructionoflargescale corpora for summarization research. Proceedings of the 22nd International ACM Conference on ResearchandDevelopmentinInformationRetrieval (SIGIR-99),Berkeley,CA,USA,137144. Marcu,D.1999b.Discoursetreesaregoodindicatorsof importanceintext.InI.ManiandM.Maybury(eds), Advances in Automatic Text Summarization, 123 136.MITPress. McKeown, K., R. Barzilay, D. Evans, V. Hatzivassiloglou, J. L. Klavans, A. Nenkova, C. Sable, B. Schiffman,S.Sigelman.2002.TrackingandSummarizingNewsonaDailyBasiswithColumbiasNewsblaster. In Proceedings of Human Language Technology Conference 2002 (HLT 2002). San Diego,CA,USA. NIST.2002.AutomaticEvaluationofMachineTranslationQualityusingN-gramCo-OccurrenceStatistics. Over, P. and W. Liggett. 2002. Introduction to DUC2002:anIntrinsicEvaluationofGenericNewsText Summarization Systems. In Proceedings of Workshop on Automatic Summarization (DUC 2002), Philadelphia,PA,USA. http://www-nlpir.nist.gov/projects/duc/pubs/ 2002slides/overview.02.pdf Papineni, K., S. Roukos, T. Ward, W.-J. Zhu. 2001. Bleu:aMethodforAutomaticEvaluationofMachine Translation. IBM Research Report RC22176 (W0109-022). Radev, D.R. and K.R. McKeown. 1998. Generating NaturalLanguageSummariesfromMultipleOn-line Sources.ComputationalLinguistics,24(3):469500. Strzalkowski,T,G.Stein,J.Wang,andB,Wise.ARobustPracticalTextSummarizer.1999.InI.Maniand M. Maybury (eds), Advances in Automatic Text Summarization,137154.MITPress. White, M., T. Korelsky, C. Cardie, V. Ng, D. Pierce, andK.Wagstaff.2001.MultidocumentSummarization via Information Extraction. In Proceedings of Human Language Technology Conference 2001 (HLT2001),SanDiego,CA,USA.
References 1 Collin F.
Baker, Charles J.
Fillmore, John B.
Lowe, The Berkeley FrameNet Project, Proceedings of the 17th international conference on Computational linguistics, August 10-14, 1998, Montreal, Quebec, Canada 2 Hoa Trang Dang, Karin Kipper, Martha Palmer, and Joseph Rosenzweig.
1998. Investigating regular sense extensions based on intersective levin classes.
In Proceedings of ACL98, Montreal, Canada, August.
3 Hoa
Trang Dang, Karin Kipper, Martha Palmer, Integrating compositional semantics into a verb lexicon, Proceedings of the 18th conference on Computational linguistics, July 31-August 04, 2000, Saarbrcken, Germany 4 Christiane Fellbaum, editor.
1998. WordNet: An Eletronic Lexical Database.
Language, Speech and Communications.
MIT Press, Cambridge, Massachusetts.
5 Daniel
Gildea, Probabilistic models of verb-argument structure, Proceedings of the 19th international conference on Computational linguistics, p.1-7, August 24-September 01, 2002, Taipei, Taiwan 6 Paul Kingsbury and Martha Palmer.
2002. From treebank to propbank.
In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002), Las Palmas, Canary Islands, Spain.
7 Karin
Kipper, Hoa Trang Dang, Martha Palmer, Class-Based Construction of a Verb Lexicon, Proceedings of the Seventeenth National Conference on Artificial Intelligence and Twelfth Conference on Innovative Applications of Artificial Intelligence, p.691-696, July 30-August 03, 2000 8 Beth Levin.
1993. English Verb Classes and Alternation, A Preliminary Investigation.
The University of Chicago Press.
9 Mitchell
Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, Britta Schasberger, The Penn Treebank: annotating predicate argument structure, Proceedings of the workshop on Human Language Technology, March 08-11, 1994, Plainsboro, NJ 10 Diana McCarthy, Using semantic preferences to identify verbal participation in role switching alternations, Proceedings of the first conference on North American chapter of the Association for Computational Linguistics, p.256-263, April 29-May 04, 2000, Seattle, Washington 11 Paola Merlo, Suzanne Stevenson, Automatic verb classification based on statistical distributions of argument structure, Computational Linguistics, v.27 n.3, p.373-408, September 2001 12 George Miller.
1985. Wordnet: A dictionary browser.
In Proceedings of the First International Conference on Information in Data, Waterloo, Ontario.
13 Marc
Moens, Mark Steedman, Temporal ontology and temporal reference, Computational Linguistics, v.14 n.2, June 1988 14 Sabine Schulte im Walde, Clustering verbs semantically according to their alternation behaviour, Proceedings of the 18th conference on Computational linguistics, July 31-August 04, 2000, Saarbrcken, Germany
References 1 Zhen Dong and Q.
Dong. 1999-2003.
Hownet, http://www.keenage.com/ 2 Jerry R.
Hobbs, Mark Stickel, Paul Martin, Douglas Edwards, Interpretation as abduction, Proceedings of the 26th annual meeting on Association for Computational Linguistics, p.95-103, June 07-10, 1988, Buffalo, New York 3 Doug Lenat, George Miller, Toshio Yokoi, CYC, WordNet, and EDR: critiques and responses, Communications of the ACM, v.38 n.11, p.45-48, Nov.
1995 4 Bernardo Magnini and Manuela Speranza.
2002. Merging Global and Specialized Linguistic Ontologies, Proceedings of Ontolex 2002 (Workshop held in conjunction with LREC-2002), Las Palmas.
5 George
A.
Miller, WordNet: a lexical database for English, Communications of the ACM, v.38 n.11, p.39-41, Nov.
1995 6 Dan Moldovan, Adrian Novischi, Lexical chains for question answering, Proceedings of the 19th international conference on Computational linguistics, p.1-7, August 24-September 01, 2002, Taipei, Taiwan 7 Takanoa Ogino and Masahiro Kobayashi.
2000. Verb Patterns extracted from EDR Concept Description, IPSJ SIGNotes Natural Language Abstract, No.
138-006:39--46. 8 Alexandra Marius Pasa.
2001. High-Performance, Open-Domain Question Answering from Large Text Collections.
Ph.D Dissertation, Southern Methodist University.
9 H.
Sofia Pinto, Asuncin Gmez-Prez and Joo P.
Martins. 1999.
Some Issues on Ontology Integration, Proceedings of the IJCAI-99 workshop on Ontologies and Problem-Solving Methods (KRR5), Stockholm.
10 Stuart
J.
Russell, Peter Norvig, Artificial intelligence: a modern approach, Prentice-Hall, Inc., Upper Saddle River, NJ, 1995 11 Toshio Yokoi, The EDR electronic dictionary, Communications of the ACM, v.38 n.11, p.42-44, Nov. 1995
References 1 Daniel M.
Bikel, Richard Schwartz, Ralph M.
Weischedel, An Algorithm that Learns Whats in a Name, Machine Learning, v.34 n.1-3, p.211-231, Feb.
1999 2 Eric Brill, A simple rule-based part of speech tagger, Proceedings of the third conference on Applied natural language processing, March 31-April 03, 1992, Trento, Italy 3 H.
Cunningham, D.
Maynard, K.
Bontcheva, V.
Tablan, and C.
Ursu. 2002.
The GATE User Guide.
http://gate.ac.uk/. 4 O.
Hamza, D.
Maynard V.
Tablan, C.
Ursu, H.
Cunningham, and Y.
Wilks. 2002.
Named Entity Recognition in Romanian.
Technical report, Department of Computer Science, University of Sheffield.
5 Diana
Maynard, Hamish Cunningham, Multilingual adaptations of ANNIE, a reusable information extraction tool, Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics, April 12-17, 2003, Budapest, Hungary 6 Diana Maynard, Hamish Cunningham, Kalina Bontcheva, Marin Dimitrov, Adapting a Robust Multi-genre NE System for Automatic Content Extraction, Proceedings of the 10th International Conference on Artificial Intelligence: Methodology, Systems, and Applications, p.264-273, September 04-06, 2002 7 David D.
Palmer, David S.
Day, A statistical profile of the Named Entity task, Proceedings of the fifth conference on Applied natural language processing, p.190-193, March 31-April 03, 1997, Washington, DC 8 K.
Pastra, D.
Maynard, H.
Cunningham, O.
Hamza, and Y.
Wilks. 2002.
How feasible is the reuse of grammars for Named Entity Recognition?
In Proceedings of 3rd Language Resources and Evaluation Conference.
References Jian Sun, et al.2002. Chinese Named Entity Identification Using Class-based Language Model.
Proceedings of the 19th International Conference on Computational Linguistics Hsin-His Chen, et al.1997. Description of the NTU System Used for MET2.
Proceedings of the Seventh Message Understanding Conference Tat-Seng Chua, et al.2002. Learning Pattern Rules for Chinese Named Entity Extraction.
Proceedings of AAAI02 W.J.Teahan, et al.1999. A Compression-based Algorithm for Chinese Word Segmentation.
Computational Linguistic 26(2000) 375-393 Maosong Sun, et al.1994. Identifying Chinese Names in Unrestricted Texts.
Journal of Chinese Information Processing.
1994,8(2) Collins, Singer.
1999. Unsupervised Models for Named Entity Classification.
Proceedings of 1999 Joint SIGDAT Conference on Empirical Methods in NLP and Very Large Corpora Daniel M.
Bikel, et al.1997. Nymble: a HighPerformance Learning Name-finder.
Proceedings of ANLP-97, page 194-201, 1997 Yu et al.1998. Description of the Kent Ridge Digital Labs System Used for MUC-7.
Proceedings of the Seventh Message Understanding Conference Silviu Cucerzan, David Yarowsky.
1999. Language Independent Named Entity Recognition Combining Morphological and Contextual Evidence.
Proceedings 1999 Joint SIGDAT Conference on EMNLP and VLC Peter F.Brown, et al.1992. Class-Based n-gram Model of Natural Language.
1992 Association for Computational Linguistics A.
Mikheev, M.
Moens, and C.
Grover. 1999.
Named entity recognition without gazetteers.
Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics.
Bergen, Norway Borthwich.
A. 1999.
A Maximum Entropy Approach to Named Entity Recognition.
PhD Dissertation Dong & Dong.
2000. Hownet.
At: http://www.keenage.com Yu, S.W. 1999.
The Specification and Manual of Chinese Word Segmentation and Part of Speech Tagging.
At: http://www.icl.pku.edu.cn/Introduction/ corpustagging.
htm Mei, J.J, et al.1983. ????/TONG YI CI CI LIN.
Shanghai CISHU Press
References 1 Eric Brill, A simple rule-based part of speech tagger, Proceedings of the third conference on Applied natural language processing, March 31-April 03, 1992, Trento, Italy 2 Julio Gonzalo, Felisa Verdejo, Irina Chugur, and Juan Cigarran.
1998. Indexing with WordNet synsets can improve text retrieval.
In Proceedings of the COLING-ACL'98 Workshop on Usage of WordNet in Natural Language Processing Systems, pages 38--44, Montreal, Canada.
3 Sanda
Harabagiu, Dan Moldovan, Marius Pasca, Rada Mihalcea, Mihai Surdeanu, Razvan Bunescu, Roxana Grju, Vasile Rus, Paul Morarescu, The role of lexico-semantic feedback in open-domain textual question-answering, Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, p.282-289, July 06-11, 2001, Toulouse, France 4 Irene Langkilde, Kevin Knight, Generation that exploits corpus-based statistical knowledge, Proceedings of the 36th annual meeting on Association for Computational Linguistics, August 10-14, 1998, Montreal, Quebec, Canada 5 Dekang Lin, Automatic retrieval and clustering of similar words, Proceedings of the 17th international conference on Computational linguistics, p.768-774, August 10-14, 1998, Montreal, Quebec, Canada 6 Steven Lytinen, Noriko Tomuro, and Tom Repede.
2000. The use of WordNet sense tagging in FAQfinder.
In Proceedings of the AAAI00 Workshop on AI and Web Search, Austin, Texas.
7 Rada
Mihalcea, Dan I.
Moldovan, A method for word sense disambiguation of unrestricted text, Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, p.152-158, June 20-26, 1999, College Park, Maryland 8 George Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller.
1990. Introduction to WordNet: An on-line lexical database.
Journal of Lexicography, 3(4):235--244.
9 Dan
Moldovan, Marius Pasca, Sanda Harabagiu, Mihai Surdeanu, Performance issues and error analysis in an open-domain Question Answering system, Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, July 07-12, 2002, Philadelphia, Pennsylvania 10 Judea Pearl, Probabilistic reasoning in intelligent systems: networks of plausible inference, Morgan Kaufmann Publishers Inc., San Francisco, CA, 1988 11 Gerard Salton, Michael J.
McGill, Introduction to Modern Information Retrieval, McGraw-Hill, Inc., New York, NY, 1986 12 Hinrich Schtze and Jan O.
Pedersen. 1995.
Information retrieval based on word senses.
In Proceedings of the Fourth Annual Symposium on Document Analysis and Information Retrieval, pages 161--175, Las Vegas, Nevada.
13 Ingrid
Zukerman, Sarah George, Towards a noise-tolerant, representation-independent mechanism for argument interpretation, Proceedings of the 19th international conference on Computational linguistics, p.1-7, August 24-September 01, 2002, Taipei, Taiwan 14 Ingrid Zukerman, Bhavani Raskutti, Lexical query paraphrasing for document retrieval, Proceedings of the 19th international conference on Computational linguistics, p.1-7, August 24-September 01, 2002, Taipei, Taiwan
References 1 P.
Brown, J.
Cocke, S.
Della Pietra, V.
Della Pietra, F.
Jelinek, R.
Mercer, P.
Roossin, A statistical approach to language translation, Proceedings of the 12th conference on Computational linguistics, p.71-76, August 22-27, 1988, Budapest, Hungry 2 Peter F.
Brown, Vincent J.
Della Pietra, Stephen A.
Della Pietra, Robert L.
Mercer, The mathematics of statistical machine translation: parameter estimation, Computational Linguistics, v.19 n.2, June 1993 3 Brown, R.
D.: Automated dictionary example-based translation, Proceedings of the Seventh International Conference on Theoretical and Methodological Issues in Machine Translation, pp.
111--118, 1997.
4 Dagan, I., Church, K.
W., Gale, W.
A.: Robust bilingual word alignment for machine aided translation, Proceedings of the Workshop on Very Large Corpora, pp.
1--8, 1993.
5 Kurohashi, S., Nagao, M: Kyoto University text corpus project, Proc.
3rd Annual Meeting of the Association for Natural Language Processing, pp.
115--118, 1997 (in Japanese).
6 Hiroyuki
Kaji, Yuuko Kida, Yasutsugu Morimoto, Learning translation templates from bilingual text, Proceedings of the 14th conference on Computational linguistics, August 23-28, 1992, Nantes, France 7 Qing Ma, Min Zhang, Masaki Murata, Ming Zhou, Hitoshi Isahara, Self-organizing Chinese and Japanese semantic maps, Proceedings of the 19th international conference on Computational linguistics, p.1-7, August 24-September 01, 2002, Taipei, Taiwan 8 Yuji Matsumoto, Hiroyuki Ishimoto, Takehito Utsuro, Structural matching of parallel texts, Proceedings of the 31st annual meeting on Association for Computational Linguistics, p.23-30, June 22-26, 1993, Columbus, Ohio 9 Macklovitch, E., Hannan, M.
L.: Line 'em up: advances in alignment technology and their impact on translation support tools, in Expanding MT Horizons, Second Conference of the Association for Machine Translation in the Americas, Montreal, pp.
145--156, 1996.
10 Ismael
Garca Varea, Franz J.
Och, Hermann Ney, Francisco Casacuberta, Improving alignment quality in statistical machine translation using context-dependent maximum entropy models, Proceedings of the 19th international conference on Computational linguistics, p.1-7, August 24-September 01, 2002, Taipei, Taiwan 11 Dekai Wu, An algorithm for simultaneously bracketing parallel texts by aligning words, Proceedings of the 33rd annual meeting on Association for Computational Linguistics, p.244-251, June 26-30, 1995, Cambridge, Massachusetts 12 Imamura, K.: Hierarchical phrase alignment harmonized with parsing, NLPRS2001, pp.
377--384, 2001.
13 Teuvo
Kohonen, Self-organizing maps, Springer-Verlag New York, Inc., Secaucus, NJ, 1997 14 Zhou, Q.
Duan, H.: Segmentation and tagging in modern Chinese corpus, Chinese Journal of Computers, Vol.
85, 1994.
Combining Segmenter and Chunker for Chinese Word Segmentation Masayuki Asahara, Chooi Ling Goh, Xiaojie Wang, Yuji Matsumoto Graduate School of Information Science Nara Institute of Science and Technology, Japan fmasayu-a,ling-g,xiaoji-w,matsug@is.aist-nara.ac.jp Abstract Our proposed method is to use a Hidden Markov Model-based word segmenter and a Support Vector Machine-based chunker for Chinese word segmentation.
Firstly, input sentences are analyzed by the Hidden Markov Model-based word segmenter.
The word segmenter produces n-best word candidates together with some class information and confidence measures.
Secondly, the extracted words are broken into character units and each character is annotated with the possible word class and the position in the word, which are then used as the features for the chunker.
Finally, the Support Vector Machine-based chunker brings character units together into words so as to determine the word boundaries.
1 Methods
We participate in the closed test for all four sets of data in Chinese Word Segmentation Bakeoff.
Our method is based on the following two steps: 1.
The input sentence is segmented into a word sequence by Hidden Markov Model-based word segmenter.
The segmenter assigns a word class with a confidence measure for each word at the hidden states.
The model is trained by Baum-Welch algorithm.
2. Each character in the sentence is annotated with the word class tag and the position in the word.
The n-best word candidates derived from the word segmenter are also extracted as the features.
A support vector machine-based chunker corrects the errors made by the segmenter using the extracted features.
We will describe each of these steps in more details.
1.1 Hidden
Markov Model-based Word Segmenter Our word segmenter is based on Hidden Markov Model (HMM).
We first decide the number of hidden states (classes) and assume that the each word can belong to all the classes with some probability.
The problem is defined as a search for the sequence of word classes C = c1;:::;cn given a word sequence W = w1;:::;wn.
The target is to find W and C for a given input S that maximizes the following probability: argmax W;C P(WjC)P(C) We assume that the word probability P(WjC) is constrained only by its word class, and that the class probability P(C) is constrained only by the class of the preceding word.
These probabilities are estimated by the Baum-Welch algorithm using the training material (See (Manning and Schutze., 1999)).
The learning process is based on the Baum-Welch algorithm and is the same as the well-known use of HMM for part-of-speech tagging problem, except that the number of states are arbitrarily determined and the initial probabilities are randomly assigned in our model.
1.2 Correction
by Support Vector Machine-based Chunker While the HMM-based word segmenter achieves good accuracy for known words, it cannot identify compound words and out-of-vocabulary words.
Therefore, we introduce a Support Vector Machine(below SVM)-based chunker (Kudo and Matsumoto, 2001) to cover the errors made by the segmenter.
The SVM-based chunker re-assigns new word boundaries to the output of the segmenter.
An SVM (Vapnik, 1998) is a binary classifier.
Suppose we have a set of training data for a binary class problem: (x1;y1);:::;(xN;yN), where xi 2 Rn is a feature vector of the i th sample in the training data and References T.
Kudo and Y.
Matsumoto. 2001.
Chunking with Support Vector Machines.
In Proc.
of NAACL 2001, pages 192199.
C. D.
Manning and H.
Schutze. 1999.
Foundation of Statistical Natural Language Processing.
Chapter 9.
Markov Models, pages 317340.
Y. Matsumoto, A.
Kitauchi, T.
Yamashita, Y.
Hirano, K.
Takaoka and M.
Asahara 2003.
Morphological Analyzer ChaSen-2.3.0 Users Manual Tech.
Report. Nara Institute of Science and Technology, Japan.
L. A.
Ramshaw and M.
P. Marcus.
1995 Text chunking using transformation-bases learning In Proc.
of the 3rd Workshop on Very Large Corpora, pages 8394.
V. N.
Vapnik. 1998.
Statistical Learning Theory.
A Wiley-Interscience Publication.
References 1 N.
Aussenac-Gilles, B.
Biebow, and S.
Szulman. 2003.
D'une mthode a un guide pratique de modlisation de connaissances a partir de textes.
In Proc.
of the 5th Conference on Terminologie et Intelligence Artificielle, Strasbourg, March 31 -April 1.
2 Ken
Barker, Stan Szpakowicz, Semi-automatic recognition of noun modifier relationships, Proceedings of the 17th international conference on Computational linguistics, August 10-14, 1998, Montreal, Quebec, Canada 3 D.
Bourigault, C.
Jacquemin, and M-C.
L'Homme, editors.
2001. Recent Advances in Computational Terminology, volume 2.
John Benjamins.
4 J.
Dowdall, M.
Hess, N.
Kahusk, K.
Kaljurand, M.
Koit, F.
Rinaldi, and K.
Vider. 2002.
Technical Terminology as a Critical Resource.
In Proc.
of LREC-02, Las Palmas, 29--31 May.
5 P.
Downing. 1977.
On the creation and use of english compound nouns.
Language, (53):810--842.
6 D.
A Evans, R.
G. Lefferts, G.
Grefenstette, S.
K. Handerson, W.
R. Hersh, and A.
A. Archbold.
1992. CLARIT TREC design, experiments and results.
Technical report, Carnegie Mellon University.
7 T.
Finin. 1980.
The semantic interpretation of nominal compounds.
In Proceedings "Artificial Intelligence, pages 310 312.
Stanford. 8 L.
Gay, W.
Croft, Interpreting nominal compounds for information retrieval, Information Processing and Management: an International Journal, v.26 n.1, p.21-38, 1990 9 F.
Ibekwe-SanJuan and E.
SanJuan. 2003.
From term variants to research topics.
Journal of Knowledge Organization (ISKO), special issue on Human Language Technology, 29(3/4).
10 Fidelia
Ibekwe-Sanjuan, Terminological variation, a means of identifying research topics from texts, Proceedings of the 17th international conference on Computational linguistics, August 10-14, 1998, Montreal, Quebec, Canada 11 F.
Ibekwe-SanJuan. 2001.
Extraction terminologique avec intex.
In Proc.
of the 4th Annual INTEX Workshop, Bordeaux, 10--11 June.
12 Pierre
Isabelle, Another look at nominal compounds, Proceedings of the 22nd annual meeting on Association for Computational Linguistics, p.509-516, July 02-06, 1984, Stanford, California 13 C.
Jacquemin. 1995.
A symbolic and surgical acquisition of terms through variation.
In Proc.
of IJCAI95, Montral.
14 C.
Jacquemin. 2001.
Spotting and discovering terms through Natural Language Processing.
MIT Press.
15 K.
Kageura. 2002.
The dynamics of Terminology: A descriptive theory of term formation and terminological growth.
John Benjamins, Amsterdam.
16 J.
N. Levi.
1979. The syntax and semantics of complex nominals.
Academic press, New York.
17 J.
Pearson. 1998.
Terms in Context.
John Benjamins, Amsterdam.
18 Fred
Popowich, Paul Mcfetridge, Dan Fass, Gary Hall, Processing complex noun phrases in a natural language interface to a statistical database, Proceedings of the 14th conference on Computational linguistics, August 23-28, 1992, Nantes, France 19 Fabio Rinaldi, Michael Hess, Diego Moll, Rolf Schwitter, James Dowdall, Gerold Schneider, Rachel Fournier, Answer Extraction in Technical Domains, Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing, p.360-369, February 17-23, 2002 20 Ivan A.
Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, Dan Flickinger, Multiword Expressions: A Pain in the Neck for NLP, Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing, p.1-15, February 17-23, 2002 21 M.
Silberztein. 1993.
Dictionnaires Electroniques et Analyse Lexicale du Francais Le Systeme INTEX.
Masson, Paris.
22 Frank Smadja, Retrieving collocations from text: Xtract, Computational Linguistics, v.19 n.1, March 1993 23 Paraic Sheridan, Alan F.
Smeaton, The application of morpho-syntactic language processing to effective phrase matching, Information Processing and Management: an International Journal, v.28 n.3, p.349-369, 1992
Pseudo Relevance Feedback Method Based on Taylor Expansion of Retrieval Function in NTCIR-3 Patent Retrieval Task Kazuaki KISHIDA Faculty of Cultural Information Resources Surugadai University 698 Azu, Hanno, Saitama 357-8555 JAPAN kishida@surugadai.ac.jp Abstract Pseudo relevance feedback is empirically known as a useful method for enhancing retrieval performance.
For example, we can apply the Rocchio method, which is well-known relevance feedback method, to the results of an initial search by assuming that the top-ranked documents are relevant.
In this paper, for searching the NTCIR-3 patent test collection through pseudo feedback, we employ two relevance feedback mechanism; (1) the Rocchio method, and (2) a new method that is based on Taylor formula of linear search functions.
The test collection consists of near 700,000 records including full text of Japanese patent materials.
Unfortunately, effectiveness of our pseudo feedback methods was not empirically observed at all in the experiment.
1 Introduction
Relevance feedback is widely recognized as an effective method for improving retrieval effectiveness in the context of interactive IR.
As often pointed out, it is difficult for users to represent their own information needs into a well-defined set of search terms or statements.
The resulting short or poor queries would bring them unsatisfactory results.
However, if a few relevant documents happen to be found by the search, we could automatically or manually extract some useful terms from the documents, and add them to the initial search expression.
It is obviously expected that search effectiveness of the second search using the extended query will be improved significantly.
This is a basic idea of relevance feedback.
Inevitably, for executing automatic relevance feedback, the system has to obtain relevance information, i.e., relevant or irrelevant documents, from the users interactively.
However, some researchers have tried to employ relevance feedback techniques with no relevance information.
The objective is to enhance search performance of retrieval models such as vector space model, probabilistic model and so on, without interaction on relevance information between system and users.
The technique is usually called pseudo relevance feedback, in which a standard feedback method (e.g., the Rocchio method) is applied by assuming that top-ranked documents searched by the initial search are relevant.
The purpose of this paper is to report results of retrieval experiments for examining effectiveness of pseudo relevance feedback in the case of searching a patent collection.
In particular, we attempt to compare search performance of the traditional Rocchio method with that of an alternative method, which is based on Taylor approximation of retrieval function proposed by Kishida[1].
This report is based on two experiments using the NTCIR-1 test collection and the NTCIR-3 patent test collection, respectively.
As to the latter, the results were obtained at the time of NTCIR-3 Workshop held in October 2002 [2].
The rest of this paper is organized as follows.
In Section 2, the Rocchio method and an alternative method proposed by Kishida[1] will be introduced.
In Section 3 a preliminary experiment for confirming how well the alternative method works in a normal relevance feedback situation will be described.
The NTCIR-1 test collection with relevance judgment information is used for the preliminary experiment.
In Section 4, results of an experiment on pseudo relevance feedback method References [1] K.
Kishida. 2001.
Feedback method for docu-ment retrieval using numerical values on rele-vance given by users.
IPSJ SIG Notes Fundamental Infology, 61: 189-196.
(in Japa-nese) [2] K.
Kishida. 2003.
Experiment on Pseudo Rele-vance Feedback Method Using Taylor Formula at NTCIR-3 Patent Retrieval Task.
Proceed-ings of the Third NTCIR Workshop on Re-search in Information Retrieval, Automatic Text Summarization and Question Answering, NII, Tokyo.
http://research.nii.ac.jp/ntcir/ [3] J.
J. Rocchio, Jr.
1971. Relevance feedback in information retrieval.
in G.
Salton ed., The SMART Retrieval System: Experiments in Automatic Document Processing, Prentice-Hall, Englewood Cliffs, NJ, 313-323.
[4] G.
Salton and C.
Buckley. 1990.
Improving retrieval performance by relevance feedback.
Journal of the American Society for Informa-tion Science, 41: 288-297.
[5] P.
Sarinivasan. 1996.
Query expansion and MEDLINE.
Information Processing and Management, 32: 431-443.
[6] J.
H. Lee.
1998. Combining the evidence of different relevance feedback methods for in-formation retrieval.
Information Processing and Management, 34: 681-691 [7] R.
Mandala, T.
Tokunaga and H.
Tanaka. 2000.
Query expansion using heterogeneous thesauri.
Information Processing and Management, 36: 361-378.
[8] M.
Iwayama. 2000.
Relevance feedback with a small number of relevance judgments: incre-mental relevance feedback vs.
Document clus-tering.
in Proceedings of the 23rd Annual International ACM SIGIR Conference on Re-search and Development in Information Re-trieval, ACM Press, 10-16 [9] G.
Ciocca. and R.
Schettini.1999. A relevance feedback mechanism for content-based image retrieval.
Information Processing and Man-agement, 35: 605-632.
[10] M.
F. Moens and J.
Dumortier. 2000.
Text categorization: the assignment of subject de-scriptors to magazine articles.
Information Processing and Management, 36: 841-861.
[11] C.
Buckley, J.
Allan, and G.
Salton. 1994.
Automatic routing and ad-hoc retrieval using SMART: TREC2.
in D.K.
Harman ed., The Second Text Retrieval Conference (TREC2).
National Institute of Standards and Technology, Gaithersburg MD, 45-55.
[12] S.
E. Robertson, et al.1995. Okapi at TERC-3.
in D.K.
Harman ed.
Overview of the Third Text Retrieval Conference (TREC-3).
National Institute of Standards and Technology, Gaithersburg MD, 109-126.
[13] D.
A. Harville.
1997. Matrix Algebra from a Statisticians Perspective.
Springer, New York.
[14] Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshitaka Hirano, Hiroshi Matsuda, Kazuma Takaoka and Masayuki Asahara.
2000. Morphological Analysis System ChaSen version 2.2.1 Manual. http://chasen.aist-nara.ac.jp/
LiveTree: An Integrated Workbench for Discourse Processing Gian Lorenzo Thione, Martin van den Berg, Chris Culy, Livia Polanyi FX Palo Alto Laboratory 3400 Hillview Ave, Bldg.
4 Palo
Alto, CA 94304 {thione|vdberg|culy|polanyi}@fxpal.com Abstract In this paper, we introduce LiveTree, a core component of LIDAS, the Linguistic Discourse Analysis System for automatic discourse parsing with the Unified Linguistic Discourse Model (U-LDM) (X et al, 2004).
LiveTree is an integrated workbench for supervised and unsupervised creation, storage and manipulation of the discourse structure of text documents under the U-LDM.
The LiveTree environment provides tools for manual and automatic U-LDM segmentation and discourse parsing.
Document management, grammar testing, manipulation of discourse structures and creation and editing of discourse relations are also supported.
1 Introduction
ing structural relations among segments, and creating and editing discourse relations (O'Donnell 2003).
Similar to the DLTAG system described in Forbes et al (2003) LiveTree is an experimental discourse parser implementing a theory of sentential and discourse relations.
However, LiveTree is also a complete document handling and manual and automatic discourse parsing system.
Various applications are supported as web services.
Accordingly, LiveTree serves as both the user interface and theory development environment for PALSUMM, a text summarization system built on top of LIDAS (See Section 5 below) In this paper, we describe the resources LiveTree workbench provides for discourse level theoretical development as well as document handling, manual and automatic text annotation and parsing.
1.2 LiveTree
Functionalities LiveTree's Java architecture shown in Figures 1 is modular and highly extensible.
LiveTree is made up by: (1) a Model Manager which provides interfaces for manipulation, storage and retrieval of actual documents and discourse representations; (2) a Module Manager, which handles and provides access to the main GUI and to all installed modules; and (3) a Service Manager providing a polling interface for all active LiveTree Services.
Manager components rely on stubs which can be implemented and extended from outside the framework's core.
The LiveTree Module Manager and all installed LiveTree Modules lie on top of a general GUI Layer handling the main LiveTree window, which includes a menu bar, a tool bar, a status bar and four docking areas.
The status bar is used for messages to the user and notification of status for asynchronous services.
The menu bar In this paper, we introduce LiveTree, a core component of LIDAS (the Linguistic Discourse Analysis System) for automatic discourse parsing with the Unified Linguistic Discourse Model (U-LDM) (Polanyi et al, 2004).
The U-LDM is a theory of discourse structure and semantics that has as its goal assigning the correct interpretation to natural language utterances.
1.1 Overview
of LiveTree LiveTree is an integrated workbench for supervised and unsupervised creation, storage and manipulation of the discourse structure of text documents under the ULDM.
LiveTree does not support speech, dialog or interaction annotation (Bernsen et al.2002, 2003 and over view of systems in Bernsen et al.2002). The LiveTree environment provides tools for manual and automatic U-LDM segmentation and discourse parsing.
Like RSTTool, LiveTree provides support for segmentation, markModule Manager GUI Interface Layer Model Manager Model Stub Document Stub Discourse Theory Stub Service Manager Service Synchronization Layer Module Stub Service Stub Parsing Segmentation Parsing Ontology Document Module Context Module D-Grammar Module Tree Module BDU-Tree Module HTML U-LDM LDM Parser OntoPAL RST Framework/Architectural o Module Manager Architecture with independent dockable floating windows for each independent module o Service Manager with multi-threaded support for asynchronous services, notification management and SOAP/webservice support o Generalized Model Manager for support of multiple discourse theories/models o Scripting Engine for automatic batch execution of actions and commands Default Modules Default Services o Document Module; BDU-Tree Module; DPT Module; o Discourse Parsing Service; U-LDM Search Module; Content Module; Grammar Module; Model Persistence Service; Xle2Xml Segments Module.
Syntax Parsing Service; Discourse Segmentation Service; OntoPAL Figure 1: LiveTree Architecture.
Core system and several implemented modules and services and the toolbar allow rapid access to general functions and to module-specific actions, including hiding and showing module windows.
Every module is assigned a window which can be resized, docked or hidden/shown.
When multiple modules are docked in the same docking area they are arranged in a tabbed interface which allows easy access and maximizes display real-estate.
Finally, the GUI Layer administers contextual pop-up menus in a general, module-independent fashion: any module can register a number of actions bound to a specific context (e.g.
a sentence, a node, a sub-tree, etc).
and at the user's request, the GUI Layer polls the Module Manager for appropriate actions from every installed module.
LiveTree's clean and intuitive interface is independent of the specific modules installed and allows for seamless integration of custom modules not part of the current implementation.
Table 1 gives a comprehensive overview of LiveTree features as well as identifying the modules or services that provide them.
2 Document
Handling vices provide functionalities needed for persistent storage and retrieval of annotated documents.
As long as documents are not modified externally, their discourse representations can also be retrieved from a persistent XML format encoding ULDM tree structure, visualization parameters, surface and deep node content along with other user-defined annotations.
The Document Module (DM) enables full document creation, modification and annotation at the document, region/selection, and sentence level.
The DM provides the visual representation of an HTML document 1 and preserves the text organization, formatting, and nontextual information (figures, tables, etc).
of HTML source documents.
The DM also provides visual feedback capabilities including highlighting and hiding/showing sections of documents.
The Document Stub Interface provides the mapping between a document's content and notions of paragraphs, sentences, units and spans.
In the current implementation, a document is divided in paragraphs according to standard notions; paragraphs are then tokenized in sentences using simple heuristics and sentences are segmented into Basic 1 Currently, only HTML document formats are supported.
Other data formats can also be supported by implementing the Document Stub Interface (DSI) appropriately, The Model Manager (MM) is the main access point to models, defined as the synchronized unions of a document and its (annotated) discourse structure.
The MM requires that appropriate LiveTree SerFeature Document Handling Discourse Segmentation (Automatic & Manual) Discourse Structure Creation Document and Sentence Level (Automatic & Manual) Semantic Content Inspection Search Description Support for HTML Documents (Import, Export, Create, Edit, Print, Tokenize in sentences) Support for LDM Discourse Segments (Automatic Sentence Segmentation; Manual Editing of Segments; Manual Sentence Segmentation; Inspect Segments' Syntax Content) Support for LDM DPT and BDU Trees (Automatic Discourse Parsing; Sub-tree Attachment via Drag `n Drop; Editing including Node Type Editing and Content Editing; Node/Sub-tree Removal; Node-Specific Notes Editing; Expand/Collapse Sub-Trees; Export to JPG; Printing; Extensible Semantic Composition) Support for Feature Structure-like Semantic Content of LDM Nodes (Node Specific via mouse selection; F-Structure graphical view; In Place Editing; Grammar Condition Querying) Full Text and RE search on: Document content, Node Surface Content, Nodes Semantic Content, Node-Specific Notes ; Online retrieval of matching sources Grammar Editor: reusable conditions; easy-to-use GUI Support for Manual Grammar Testing (Check for rule enablement between attachment point and MBDU selected from actual subtrees; Support Scripted Testing with XML Based Language) Implements and supports serialization and deserialization in LiveTree XML format of LDM Annotation for documents.
Tree Structure Zooming and Panning, Print Preview Functionalities, Copy/Cut/Paste for text and trees) Document Module BDU-Tree Module, Content Module, Xle2Xml Syntax parsing Service, Discourse Segmenter BDU-Tree Module, DPT Module, Content Module, Notes Module, Discourse Parsing Service BDU-Tree Module, DPT Module, Content Module Search Module, Document Module, DPT Module Grammar Module Grammar Module, DPT Module, Discourse Parsing Service LDM Persistence Service Tree Module, Document Module, BDU-Tree Module U-LDM Rule Editing Discourse Grammar Testing (Manual & Scripted) Persistence Support Other Functionalities Table 1: Overview of LiveTree features and the modules or services that provide them.
Discourse Units following the U-LDM discourse segmentation conventions discussed below.
3 LiveTree
Support for Discourse Annotation Units (BDUs) and then for combining the BDUs into an Open-Right Discourse Parse Tree (DPT) that captures structural relations among constituent structures.
The ULDM discourse parsing process can be summarized as follows:  Identify potential Basic Discourse Units (BDUs) within sentence from output of analysis of sentence documents from the Xerox Linguistic Environment (XLE) Lexical Functional Grammar (LFG) parser using sentence-segmentation rules.
 Construct a set of Open-Right BDUtrees representations which map onto top-level coordinated structures within the sentence, using syntactic information from the XLE parse and sentential discourse rules to identify the relationships among BDUs.
The LiveTree Workbench supports manual and automatic, supervised and unsupervised annotation practices for each step in the analysis process.
In addition, our default implementation includes a completely integrated interface for writing, testing and debugging U-LDM Discourse grammar rules which are used for automatically constructing the discourse representation for individual sentences and entire texts.
3.1 U-LDM Parsing Steps The U-LDM specifies rules both for segmenting sentences into Basic Discourse Attach the BDU-trees, each one as a single unit, to the DPT by computing the relationship between the node corresponding to the root of a BDU-Tree to accessible DCUs aligned along the right edge of the DPT using rules of discourse relations.
There are 3 possible macro-types of relation: Coordination: new unit continues development of previous unit Subordination: new unit provides additional information about previous unit N-ary: new unit bears a special logical, rhetorical or genre based relationship to previous unit Once a BDU-tree is attached, its leaves become terminal nodes of the DPT and nodes on its right edge become therefore accessible for attachment in the next iteration of the process.
Live Tree Modules for U-LDM discourse annotation keep sub-sentential units available for attachment at independent nodes along the right edge of the DPT.
For discourse segmentation, the U-LDM depends on the syntactic analysis of constituent sentences.
Initially, sentences are divided up into discourse segments reflecting syntactic encodings of minimal units of meaning or function.
Subsequently, some segments are identified as Basic Discourse Units (BDUs).
Only those discourse segments that are of a type that can be independently continued are BDUs.
Operator segments are one example of non-BDU segments.
Gerunds, nominalizations, auxiliary and modal verbs or clefts are verb based constituents but not segments because they do not independently establish an interpretation context for update by subsequent units (Polanyi, 2004).
Live Tree Modules (LTM) provide extensive manual and automatic capabilities for annotating documents with U-LDM discourse tags.
They are local to the framework and provide user-directed functionalities, relying on mutual interaction through the LiveTree GUI.
Two modules in LiveTree's current implementation contribute primarily to discourse annotation (besides the DM): the BDU-Tree Module and the DPT Module.
3.2.1 Discourse
Segmentation A critical task for U-LDM analysis is to account for the availability for update of appropriate discourse contexts or subcontexts introduced in earlier text.
Thus, discourse segmentation under the U-LDM requires the identification of discourse units within the sentence that can function as possible attachment points as well as segmenting sentential units and nonsentential structures such as titles from other units.
The U-LDM may match incoming discourse utterances with target contexts which are in syntactically subordinated positions within a previous sentence.
In order to construct the appropriate representation of the rhetorical or semantic structure of discourse we must therefore Figure 2: A segmented sentence and the BDU-Trees corresponding to its two coordination-chunks.
In LiveTree, the BDU-Tree Module shown in Figure 2 provides the visual interface and annotation tools for sentence segmentation.
The top section of the BDU-Tree window is composed of two areas: a small toolbar, and the sentence/segment viewer.
A simple togglebutton interface allows the user to select between automatic or manual segmentation.
In automatic mode, an external Segmentation Service (part of LIDAS) is polled and a set of segments retrieved.
Segments are automatically colored, and segments embedding other segments are represented by non-contiguous spans of text associated by the same highlighting color.
In manual mode segmentation is performed by dragging the divider (the firightmost button in the toolbar) to the desired span boundaries and, if necessary, assigning non-contiguous spans to the same segment using drag-n-drop.
3.2.2 BDU
Tree Construction In LIDAS operating in automatic mode, BDU-Trees are constructed from segmented sentences by mapping the LFG fstructure representations of sentential syntax produced by the XLE onto appropriated sentence-level discourse attachments.
The resulting structure is a BDU-Tree, a DPT of an individual sentence.
Although automatic BDU-Tree parsing can only be performed on automatically generated segments, LiveTree supports manual construction of BDU-Trees regardless of how segmentation occurred.
In manual mode, segments can be dragged from the Sentence Viewer area to the bottom section of the window.
When dropped, these become BDU nodes and the content of the node can be manually annotated.
To create the relationship between two nodes the user drags one node over the other as attachment point and selects a preferred relation from a pop-up menu.
If syntactic/semantic annotations are present they are correctly percolated and composed throughout the BDU-Tree.
BDU Trees can be easily edited and manipulated for correcting or changing annotations, and for improving results generated by automatic BDU-Tree parsing.
Nodes can be removed, their associated annotations inspected and modified, and the type of relation node changed.
When the type of a relation node is changed, the annotations of all nodes dominating the changed relation are updated and the correct syntactic/semantic information percolated through the tree in accord with the new relation type.
Nodes and whole sub-trees can be detached and reattached at a different point using simple mouse gestures.
3.2.3 Discourse
Parse Tree Construction U-LDM discourse parsing is a three step process: (1) segmentation, (2) BDU-Tree Parsing, and (3) DPT parsing.
LiveTree supports automatic and manual modes at all three stages enabling multiple annotation scenarios.
For example, users can segment and annotate a document entirely by hand, or, alternatively, rely on automatic segmentation and BDU-Tree parsing while manually completing the more error-prone stage of DPT parsing.
Another option is to bootstrap the annotation at every stage using LiveTree automatic resources and then manually correct mistakes and undesired choices (supervised mode).
A Discourse Segmentation Service and a Discourse Parsing Service using two separate discourse grammars provide automation.
The user interfaces of the BDU-Tree Module and of the DPT module allow for manual and supervised annotation.
3.3 Discourse
Relations under the ULDM Automatic DPT parsing is rule based.
Lexical information (synonym, antonym, hypernym, discourse connectives), semantics (involving genericity, modality, cardinality, temporal interpretation etc.), and syntactic information (including topicalization, grammatical function promotion/demotion, etc).
are used by weighted ordered discourse grammar rules to determine both the site of attachment and the relationship obtaining among the nodes.
Rules may combine different sources of evidential information.
LiveTree provides a complete rule development and testing environment used for both theoretical investigation and automatic parsing.
When a BDU-Tree is available for attachment, linguistic information available at DCUs along the right edge of the DPT is compared with evidence retrieved from the incoming BDU-Tree to identify semantic information that acts as an "anaphoric anchor" for information in the incoming BDU-Tree by examining the content of the root node (M-BDU).
Each attachment rule is checked against information available at the M-BDU and at the available DCUs and an ordered set of attachment sites and associated relations, as specified by the winning rules, is generated.
Local semantic, lexical and syntactic information is percolated up through the tree according to the constraints of the discourse relations at each dominating node.
Figure 3: Two views of a document's discourse structure.
Trees and subtrees can be modified, rearranged and moved through simple drag `n drop operations.
If multiple attachments are possible, ambiguous parses ordered by likelihood are generated.
In LiveTree operating in automatic mode, the system proposes a preferred structure.
Dispreferred structures can be obtained by operating in supervised mode.
3.4 The
DPT Module The DPT Module shown in Figure 3 provides the visual representation and manipulation interface for U-LDM Discourse Trees.
Advanced viewing capabilities help the user analyze large complex discourse structures: sub-trees can be collapsed and expanded; zooming and panning capabilities and fit-to-page printing are fully supported.
Trees and sub-trees can be moved, rearranged, and removed with the same editing functions available as in the BDUTree Module.
In addition, automatic layout capabilities enable even the most graphically complex discourse structures to be displayed clearly.
4 Discourse
parsing with LiveTree changes 2. In Incremental Automatic (Supervised) mode, the user is prompted for corrections at selected stages of the process.
For example, after a sentence is selected by the system for processing, automatically segmented 3, and parsed into BDU-Trees, the user can rearrange nodes, change relationships between nodes, and if necessary, even merge multiple BDUTrees into one.
The BDU-Tree(s) might then be automatically attached to the DPT and the user prompted again to correct any mistakes.
When the parsing process is supervised in this way, the number of overall mistakes is often reduced because attachments occur on incrementally checked structures thus maintaining the correct open right edge at all times.
Finally, in Manual DPT Parsing mode, BDU-Trees can be dragged from the BDU-Tree module to the DPT module and manually attached to the DPT however the BDU-Trees were computed.
The decision of how to combine manual and automatic processing is made by the user.
For large documents problems often arise as parsing mistakes build on themselves as the right edge changes and large structures are harder to examine and manipulate.
3 Optionally
the user can correct any segmentation mistake at this stage, though this interrupts the automatic mode and the attachment of the sentence must be completed manually, since the necessary syntactic information is no longer attached to the segments.
Of course, this information which was likely to have been incorrect anyway, thereby necessitating correcting the segmentation.
In order to create an DPT, a user can work in different modes.
In Fully Automatic (Unsupervised) mode, the user simply selects a document for full processing.
The document is tokenized, each sentence is automatically segmented, and passed to the parser.
The discourse parsing service automatically creates BDU-Trees from each sentence and as trees are created they are attached to the emerging DPT.
The user can then revise the structure and make Discourse Grammar Writing and Testing LiveTree incorporates facilities for writing, accessing and testing discourse grammars both at the sentence and at the document level.
Rules are edited via a dialog window which allows the user to create new rules by reusing macros and conditions previously used in other rules.
Access to all defined types of discourse relations is permitted and it is easy to set priorities and preempting relations among existing rules.
Rules are tested in two ways.
In scripted mode, testcase files are written specifying exemplary sentences and the discourse rule(s) to be tested, along with the expected outcome.
This way, several rules and several testcases can be tested automatically at once.
A report is created at the end of the process with information about the outcome of the tests.
In manual mode, a rule can be selected for testing from the Grammar Module and a node or subtree from the DPT can be dragged on a candidate attachment site.
The parser attempts to make an attachment using the selected rule and reports the result to the user.
This mechanism has proven very useful during grammar creation providing important information to understand why expected structures are not created by the parser.
5 The
PALSUMM Text Summarizer to thirty or more pages.
All 300 reports have been automatically summarized.
Initial results, though hardly perfect, are encouraging.
6 Conclusion
LiveTree is a powerful and extremely flexible workbench for discourse level NLP annotation and parsing tasks.
Throughout the design and implementation of LiveTree, our goal has been to support a full range of work-practices and to make sure that annotation steps were integrated in an intuitive and seamless fashion.
Services and modules make use of available resources efficiently and interoperate unobtrusively.
New functionalities can be easily added on top of existing ones and the service-oriented LiveTree architecture enables concurrent and asynchronous services to be executed locally or remotely as automatically generated web services.
Working in LiveTree has proven very efficient without waste of user's time.
For example, a document can be parsed automatically in the background while other tasks such as manual annotation, grammar writing or testing are performed.
While LiveTree has been designed an implemented as a workbench for U-LDM analysis, many of the features and aspects of the architecture could be adopted for use with other analytic frameworks.
The global discourse trees resulting from U-LDM parsing in the LiveTree Environment are used for text summarization in the PALSUMM System.
PALSUMM is a hybrid sentence extraction system that uses conventional statistical methods to identify important information in a text and then marks for extraction those discourse segments in the DPT that are necessary in order to provide context for reference and proper resolution of anaphors.
The goal of PALSUMM Summarization is to produce high quality readable summaries.
We have tested our summarization methods using both manually annotated and automatically created U-LDM structures of Technical Reports taken from the FX Palo Alto archive of over 300 reports in more than 10 domains of computer science.
These reports vary in size from a few References Bernsen, N.
O., Dybkjr, L., and Kolodnytsky, M . An Interface for Annotating Natural Interactivity.
In J.
v. Kuppevelt and R.
W. Smith (Eds.): Current and New Directions in Discourse and Dialogue, Dordrecht: Kluwer 2003.
Ch. 3.
pp. 3562.
Bernsen, N.
O., Dybkjr, L.
and Kolodnytsky, M . The NITE Workbench A Tool for Annotation of Natural Interactivity and Multimodal Data.
Proceedings of the Third International Conference on Language Resources and Evaluation (LREC-2002), Las Palmas, 2002, 43-49.
Katherine Forbes, Eleni Miltsakaki, Rashmi Prasad, Anoop Sarkar, Aravind Joshi and Bonnie Webber.
2003. D-LTAG System Discourse Parsing with a Lexicalized TreeAdjoining Grammar, Journal of Language, Logic and Information, 12(3).
Barbara Grosz and Candace Sidner.
1986. Attention, Intention and the Structure of Discourse.
Computational Linguistics 12:175204.
Bonnie Webber and Aravind Joshi.
1998. Anchoring a Lexicalized Tree-Adjoining Grammar for Discourse.
ACL/COLING Workshop on Discourse Relations and Discourse Markers, Montreal, Canada.
William C.
Mann and Sandra A.
Thompson. 1988.
Rhetorical Structure Theory: Towards a Functional Theory of Text Organization.
Text 8(3)243-281.
Marcu, Daniel.
2000. The Theory and Practice of Discourse Parsing and Summarization.
The MIT Press.
Cambridge, MA.
ODonnell, Michael.
2003. RSTTool.
(http:www.waysoft.com/RSTTool.) Livia Polanyi and Remko Scha.
1984. A syntactic approach to discourse semantics.
In Proceedings of COLING 6.
Stanford, CA.
413-419. Livia Polanyi, Martin van den Berg, Chris Culy, Gian Lorenzo Thione, David Ahn.
2004. A Rule Based Approach to Discourse Parsing.
Proceedings of SIGDIAL '04.
Boston MA .
Structural Semantic Interconnection: a knowledge-based approach to Word Sense Disambiguation Roberto NAVIGLI Dipartimento di Informatica, Universit di Roma La Sapienza Via Salaria, 113 00198 Roma, Italy navigli@di.uniroma1.it Paola VELARDI Dipartimento di Informatica, Universit di Roma La Sapienza Via Salaria, 113 00198 Roma, Italy velardi@di.uniroma1.it Abstract In this paper we describe the SSI algorithm, a structural pattern matching algorithm for WSD.
The algorithm has been applied to the gloss disambiguation task of Senseval-3.
1 Introduction
Our approach to WSD lies in the structural pattern recognition framework.
Structural or syntactic pattern recognition (Bunke and Sanfeliu, 1990) has proven to be effective when the objects to be classified contain an inherent, identifiable organization, such as image data and time-series data.
For these objects, a representation based on a flat vector of features causes a loss of information that negatively impacts on classification performances.
Word senses clearly fall under the category of objects that are better described through a set of structured features.
The classification task in a structural pattern recognition system is implemented through the use of grammars that embody precise criteria to discriminate among different classes.
Learning a structure for the objects to be classified is often a major problem in many application areas of structural pattern recognition.
In the field of computational linguistics, however, several efforts have been made in the past years to produce large lexical knowledge bases and annotated resources, offering an ideal starting point for constructing structured representations of word senses.
2 Building
structural representations of word senses We build a structural representation of word senses using a variety of knowledge sources, i.e.
WordNet, Domain Labels (Magnini and Cavaglia, 2000), annotated corpora like SemCor and LDCDSO 1. We use this information to automatically 1 LDC http://www.ldc.upenn.edu/ generate labeled directed graphs (digraphs) representations of word senses.
We call these semantic graphs, since they represent alternative conceptualizations for a lexical item.
Figure 1 shows an example of the semantic graph generated for senses #1 of market, where nodes represent concepts (WordNet synsets), and edges are semantic relations.
In each graph, we include only nodes with a maximum distance of 3 from the central node, as suggested by the dashed oval in Figure 1.
This distance has been experimentally established.
m a rk et # 1 go o ds #1 t r a di n g# 1 g l o s s g l o s s m e r c hand i s e #1 k i n d o f m on o p ol y #1 k i n d o f e xp or t # 1 h a s k i n d a c ti v i ty # 1 h a s k i n d c o ns um e r go od s #1 g r oc e r y #2 k i n d o f k i n d o f l oa d #3 k i n d o f co m m e r c i a l en t er p r i s e # 2 h a s p a r t c o m m er c e#1 k i n d o f t r a n s po r t at i on # 5 h a s p a r t bu s i ne s s a c ti v i ty # 1 g l o s s se r v i c e # 1 g l o s s t o p i c i nd us t r y # 2 k i n d o f h a s p a r t g l o s s k i n d o f f oo d #1 c lo t h in g # 1 g l o s s g l o s s e n t e rp ri se # 1 k i n d o f pr o du c t i on # 1 a r t i f ac t #1 k i n d o f ex p r e s s # 1 k i n d o f c o ns u m pt i o n#1 g l o s s F i gu r e 1.
G r a ph r e pr e s e n t a t i ons f or s e ns e #1 of m a r ket . A ll t h e u s e d s e m a n ti c r e l a ti o n s a r e e x p l ic i tl y e nc ode d i n W o r dN e t, e xc e pt f o r t h r e e r e l a t i on s na m e d t opi c, gl o s s an d dom ai n, ex t r ac t e d r e s p e c t i v e l y f r om a nnot a t e d c o r po r a, s e ns e de f i ni t i o ns a n d dom a i n l a be l s . 3 S u mma r y d e s c r i p t i o n o f t h e S S I a lg o r i th m T he S S I a l g or i t hm c ons i s t s of a n i ni t i a l i z a t i on s t e p a nd a n i t e r a t i v e s t e p.
I n a g e ne r i c i t e r a t i on of t he a l g or i t hm t he i npu t is a l is t o f c o o c c u r r in g te r m s T = [ t 1, , t n ] an d a l i s t o f as so c i a t e d s en se s I = ], ..
.,[ 1 n tt SS, i .e . t h e s e m a nt i c i nt e r p r e t a t i on of T, w h er e i t S 2 is e it h e r t h e ch o sen s en se f o r t i ( i .e ., t he r e s u l t of a pr e v i ou s 2 N o te t h a t wi t h i t S w e r ef er i n t er c h a n g ea b l y t o t h e s em a n t i c g r ap h as s o c i at e d w i t h a s e n s e o r t o t h e s e n s e na me . Association for Computational Linguistics for the Semantic Analysis of Text, Barcelona, Spain, July 2004 SENSEVAL-3: Third International Workshop on the Evaluation of Systems disambiguation step) or the empty set (i.e., the term is not yet disambiguated).
A set of pending terms is also maintained, P = }|{ = i t i St . I is named the semantic context of T and is used, at each step, to disambiguate new terms in P.
The algorithm works in an iterative way, so that at each stage either at least one term is removed from P (i.e., at least a pending term is disambiguated) or the procedure stops because no more terms can be disambiguated.
The output is the updated list I of senses associated with the input terms T.
Initially, the list I includes the senses of monosemous terms in T.
If no monosemous terms are found, the algorithm makes an initial guess based on the most probable sense of the less ambiguous term.
The initialisation policy is adjusted depending upon the specific WSD task considered.
Section 5 describes the policy adopted for the task of gloss disambiguation in WordNet.
During a generic iteration, the algorithm selects those terms t in P showing an interconnection between at least one sense S of t and one or more senses in I.
The likelihood for a sense S of being the correct interpretation of t, given the semantic context I, is estimated by the function CxTf I :, where C is the set of all the concepts in the ontology O, defined as follows:      = otherwise SynsetstSensesSifISSS tSf I 0 )(})'|)',(({ ),(  where Senses(t) is the subset of concepts C in O associated with the term t, and })'...|)...(({')',( 1121 121 SSSSeeewSS nn e n eee n =   , i.e. a function ( ) of the weights (w) of each path connecting S with S, where S and S are represented by semantic graphs.
A semantic path between two senses S and S, '... 11 121 SSSS nn e n eee   , is represented by a sequence of edge labels n eee ...
21 . A proper choice for both and  may be the sum function (or the average sum function).
A context-free grammar G = (E, N, S G, P G ) encodes all the meaningful semantic patterns.
The terminal symbols (E) are edge labels, while the non-terminal symbols (N) encode (sub)paths between concepts; S G is the start symbol of G and P G the set of its productions.
We associate a weight with each production A in P G, where NA and *)( EN  , i.e.
 is a sequence of terminal and non-terminal symbols.
If the sequence of edge labels n eee ...
21 belongs to L(G), the language generated by the grammar, and provided that G is not ambiguous, then )...( 21 n eeew is given by the sum of the weights of the productions applied in the derivation nG eeeS  + ...
21 . The grammar G is described in the next section.
Finally, the algorithm selects ),(maxarg tSf I CS as the most likely interpretation of t and updates the list I with the chosen concept.
A threshold can be applied to ),( tSf to improve the robustness of systems choices.
At the end of a generic iteration, a number of terms is disambiguated and each of them is removed from the set of pending terms P.
The algorithm stops with output I when no sense S can be found for the remaining terms in P such that 0),( >tSf I, that is, P cannot be further reduced.
In each iteration, interconnections can only be found between the sense of a pending term t and the senses disambiguated during the previous iteration.
A special case of input for the SSI algorithm is given by ]...,,,[ =I, that is when no initial semantic context is available (there are no monosemous words in T).
In this case, an initialization policy selects a term t T and the execution is forked into as many processes as the number of senses of t.
4 The
grammar The grammar G has the purpose of describing meaningful interconnecting patterns among semantic graphs representing conceptualisations in O.
We define a pattern as a sequence of consecutive semantic relations n eee ...
21 where Ee i, the set of terminal symbols, i.e. the vocabulary of conceptual relations in O.
Two relations 1+ii ee are consecutive if the edges labelled with i e and 1+i e are incoming and/or outgoing from the same concept node, that is 1 )( +  ii ee S, 1 )( +  ii ee S, 1 )( +  ii ee S, 1 )( +  ii ee S . A meaningful pattern between two senses S and S is a sequence n eee ...
21 that belongs to L(G).
In its current version, the grammar G has been defined manually, inspecting the intersecting patterns automatically extracted from pairs of manually disambiguated word senses co-occurring in different domains.
Some of the rules in G are inspired by previous work on the eXtended WordNet project described in (Milhalcea and Moldovan, 2001).
The terminal symbols e i are the conceptual relations extracted from WordNet and other on-line lexical-semantic resources, as described in Section 2.
G is defined as a quadruple (E, N, S G, P G ), where E = { e kind-of, e has-kind, e part-of, e has-part, e gloss, e isin-gloss, e topic,  }, N = { S G, S s, S g, S 1, S 2, S 3, S 4, S 5, S 6, E 1, E 2,  }, and P G includes about 50 productions.
As stated in previous section, the weight )...( 21 n eeew of a semantic path n eee ...
21 is given by the sum of the weights of the productions applied in the derivation nG eeeS  + ...
21 . These weights have been learned using a perceptron model, trained with standard word sense disambiguation data, such as the SemCor corpus.
Examples of the rules in G are provided in the subsequent Section 5.
5 Application
of the SSI algorithm to the disambiguation of WordNet glosses For the gloss disambiguation task, the SSI algorithm is initialized as follows: In step 1, the list I includes the synset S whose gloss we wish to disambiguate, and the list P includes all the terms in the gloss and in the gloss of the hyperonym of S.
Words in the hyperonyms gloss are useful to augment the context available for disambiguation.
In the following, we present a sample execution of the SSI algorithm for the gloss disambiguation task applied to sense #1 of retrospective: an exhibition of a representative selection of an artists life work.
For this task the algorithm uses a context enriched with the definition of the synset hyperonym, i.e. art exhibition#1: an exhibition of art objects (paintings or statues).
Initially we have: I = { retrospective#1 } 3 P = { work, object, exhibition, life, statue, artist, selection, representative, painting, art } At first, I is enriched with the senses of monosemous words in the definition of retrospective#1 and its hyperonym: I = { retrospective#1, statue#1, artist#1 } P = { work, object, exhibition, life, selection, representative, painting, art } since statue and artist are monosemous terms in WordNet.
During the first iteration, the algorithm finds three matching paths 4 : retrospective#1 2  ofkind exhibition#2, statue#1 3  ofkind art#1 and statue#1 3 For convenience here we denote I as a set rather than a list.
4 With
S R   i S we denote a path of i consecutive edges labeled with the relation R interconnecting S with S.
6  ofkind object#1 This leads to: I = { retrospective#1, statue#1, artist#1, exhibition#2, object#1, art#1 } P = { work, life, selection, representative, painting } During the second iteration, a hyponymy/holonymy path (rule S 2 ) is found: art#1 2  kindhas painting#1 (painting is a kind of art)which leads to: I = { retrospective#1, statue#1, artist#1, exhibition#2, object#1, art#1, painting#1 } P = { work, life, selection, representative } The third iteration finds a co-occurrence (topic rule) path between artist#1 and sense 12 of life (biography, life history): artist#1  topic life#12 then, we get: I = { retrospective#1, statue#1, artist#1, exhibition#2, object#1, art#1, painting#1, life#12 } P = { work, selection, representative } The algorithm stops because no additional matches are found.
The chosen senses concerning terms contained in the hyperonyms gloss were of help during disambiguation, but are now discarded.
Thus we have: GlossSynsets(retrospective#1) = { artist#1, exhibition#2, life#12, work#2 } 6 Evaluation The SSI algorithm is currently tailored for noun disambiguation.
Additional semantic knowledge and ad-hoc rules would be needed to detect semantic patterns centered on concepts associated to verbs.
Current research is directed towards integrating in semantic graphs information from FrameNet and VerbNet, but the main problem is harmonizing these knowledge bases with WordNets senses and relations inventory.
A second problem of SSI, when applied to unrestricted WSD tasks, is that it is designed to disambiguate with high precision, possibly low recall.
In many interesting applications of WSD, especially in information retrieval, improved document access may be obtained even when only few words in a query are disambiguated, but the disambiguation precision needs to be well over the 70% threshold.
Supporting experiments are described in (Navigli and Velardi, 2003).
The results obtained by our system in Senseval3 reflect these limitations (see Figure 2).
The main run, named OntoLearn, uses a threshold to select only those senses with a weight over a given threshold.
OntoLearnEx uses a nongreedy version of the SSI algorithm.
Again, a threshold is used to accepts or reject sense choices.
Finally, OntoLearnB uses the first sense heuristics to select a sense, every since a sense choice is below the threshold (or no patterns are found for a given word).
82.60% 75.30% 37.50% 68.50% 68.40% 32.30% 39.10% 49.70% 99.90% 0% 20% 40% 60% 80% 100% OntoLearn OntoLearnB OntoLearnEx P r e c is io n R ec al l At te m p t e d F i gu r e 2.
R e s u l t s of t h r e e r un s s u b m i t t e d t o S e n s e v a l 3.
T a b l e 1 s how s t he pr e c i s i on a nd r e c a l l o f O nt oL e a r n m a i n r un by s y nt a c t i c c a t e g or y . I t s how s t ha t, a s e xp e c t e d, t he S S I a l g or i t hm i s c ur r e nt l y t une d f o r noun di s a m bi g ua t i on.
N ouns V e r bs A dj . P r e c i s i on 86.0% 69.4% 78.6% R e c a l l 44.7% 13.5% 26.2% A t t e m pt e d 52.0% 19.5% 33.3% T a b le 1 . P r e c is io n a n d R e c a ll b y s y n ta c t ic c a te g o r y . T he o f f i c i a l S e ns e v a l 3 e v a l ua t i on ha s be e n p er f o r m e d a g ai n st a se t o f so cal l ed g o l d e n g l os s e s  p r odu c e d by D a n M ol dov a n a nd i t s g r oup 5 . T h i s t es t s e t h o w ev er h ad s e v er a l pr obl e m s, t ha t w e pa r t l y de t e c t e d a nd s ubm i t t e d t o t he o r g a ni s e r s . B esi d e s so m e t ech n i c al e r r o r s i n t h e d at a s e t ( pr e s e n c e o f W or dN e t 1.7 a nd 2.0 s e n s e s, m i s s i ng g l o sses, e t c . ) t h e r e a r e sen set ag g i n g i nc on s i s t e nc i e s t ha t a r e v e r y e v i de nt . F or e xa m pl e, one of our hi g he s t pe r f or m i ng sen se t ag g i n g r u l es i n S S I i s t h e di r e c t hy pe r o ny m y p a t h . T h i s r u l e r e ad s as f o l l o w s : i f t he w or d w j a ppe a r s i n t he g l os s of a s y ns e t S i, a nd i f one of t h e s y ns e t s o f w j, S j, i s t h e d i r e ct hy pe r onym of S i, th e n, s e le c t S j as t h e c o r r ec t sen se f o r w j .
A n e xa m pl e i s c us t om #4 de f i ne d a s  ha b i t ua l pa t r ona g e  . W e ha v e t ha t : { c us t om n#4} kin d _ of      { t r a de,pa t r on a g e n#5} 5 h ttp :// x w n . h l t.
u td a lla s . e d u / w s d . h t m l t he r e f o r e w e s e l e c t s e ns e # 5 of pa t r ona g e, w hi l e M ol d ov a n s  g ol d e n s e ns e i s #1.
W e do no t i nt e nd t o d i s pu t e w he t he r t h e q u e s t i o n ab l e s e n se a ss i g n m en t i s t h e o n e pr ov i de d i n t he g ol de n g l os s or r a t he r t he hy pe r onym s e l e c t e d by t he W or dN e t l e x i c og r a ph e r s . I n a ny c a s e, t he de t e c t e d pa t t e r n s s how a c l e a r i nc on s i s t e n c y i n t h e da t a . T he s e pa t t e r ns ( 3 13) ha v e be e n s ubm i t t e d t o t he or g a ni s e r s, w ho t he n de c i de d t o r e m ov e t he m fro m th e d a ta s e t . 7 C on c l u s i on T h e in t e r e s ti n g f e a tu r e o f th e S S I a lg o r it h m, u n l i k e m an y co o cc u r r en c e b ased an d s t at i s t i cal a ppr o a c he s t o W S D, i s a ju s ti fi c a t io n ( i . e.
a s e t o f s e m a nt i c p a t t e r ns ) t o s u ppor t a s e ns e c hoi c e . F u r t h er m o r e, ea c h sen se ch o i c e h a s a w e i g h t r e pr e s e nt i ng t he c on f i d e nc e of t h e s y s t e m i n i t s out pu t . T he r e f o r e S S I c a n be t une d f or h i g h pr e c i s i on ( pos s i b l y l ow r e c a l l ), a n a s s e t t h a t w e c o n s i d e r m o re re a l i s t i c fo r p ra c t i c a l W S D a ppl i c a t i ons . C ur r e n t l y, t he s y s t e m i s t une d f or nou n di s a m bi g ua t i on, s i n c e w e bui l d s t r uc t u r a l r ep r e s e n t a t i o n s o f w o r d sen s es u si n g l e x i ca l k now l e dg e ba s e s t h a t a r e c ons i de r a bl y r i c h e r f or nouns . E xt e ndi ng s e m a nt i c g r a phs a s s oc i a t e d t o v e r bs a nd a d di ng a pp r o pr i a t e i n t e r c onne c t i on r ul e s i m pl i e s h a r m oni z i ng W or dN e t a nd a v a i l a b l e l e x i ca l r eso u r c es f o r v er b s, e.
g . F r a m eN et a n d V e r bN e t . T hi s e x t e ns i on i s i n pr og r e s s . References H.
Bunke and A.
Sanfeliu (editors) (1990) Syntactic and Structural pattern Recognition: Theory and Applications World Scientific, Series in Computer Science vol.
7, 1990.
A. Gangemi, R.
Navigli and P.
Velardi (2003) The OntoWordNet Project: extension and axiomatization of conceptual relations in WordNet, 2nd Int.
Conf. ODBASE, ed.
Springer Verlag, 3-7 November 2003, Catania, Italy.
B. Magnini and G.
Cavaglia (2000) Integrating Subject Field Codes into WordNet, Proceedings of LREC2000, Atenas 2000.
Milhalcea R., Moldovan D.
I. (2001) eXtended WordNet: progress report.
NAACL 2001 Workshop on WordNet and other lexical resources, Pittsburg, June 2001.
Navigli R.
and Velardi P.
(2003) An Analysis of Ontology-based Query Expansion Strategies, Workshop on Adaptive Text Extraction and Mining September 22nd, 2003 Cavtat-Dubrovnik (Croatia), held in conjunction with ECML 2003 .
References Chang, Jing-Shin and Keh-Yih Su, 1997.
An Unsupervised Iterative Method for Chinese New Lexicon Extraction, International Journal of Computational Linguistics and Chinese Language Processing (CLCLP), 2(2): 97-148.
Chiang, Tung-Hui, Jing-Shin Chang, Ming-Yu Lin and Keh-Yih Su, 1992.
Statistical Models for Word Segmentation and Unknown Word Resolution, Proceedings of ROCLING-V, pages 123-146, Taipei, Taiwan, ROC.
Dempster, A.
P., N.
M. Laird, and D.
B. Rubin, 1977.
Maximum Likelihood from Incomplete Data via the EM Algorithm, Journal of the Royal Statistical Society, 39 (b): 1-38.
Gao, Jianfeng, Mu Li, Chang-Ning Huang, 2003.
Improved Source-Channel Models for Chinese Word Segmentation, Proc.
ACL 2003, pages 272-279.
Huang, Chu-Ren, Kathleen Ahrens, and Keh-Jiann Chen, 1994a.
A data-driven approach to psychological reality of the mental lexicon: Two studies on Chinese corpus linguistics. InLanguage and its Psychobiological Bases, Taipei.
Huang, Chu-Ren, Wei-Mei Hong, and Keh-Jiann Chen, 1994b.
Suoxie: An information based lexical rule of abbreviation. In Proceedings of the Second Pacific Asia Conference on Formal and Computational Linguistics II, pages 4952,Japan.
Katz, Slava M., 1987.
Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer, IEEE Trans.
ASSP-35 (3).
Lai, Yu-Tso, 2003.
A Probabilistic Model for Chinese Abbreviations, Master Thesis, National Chi-Nan University, ROC.
Lin, Ming-Yu, Tung-Hui Chiang and Keh-Yih Su, 1993.
A Preliminary Study on Unknown Word Problem in Chinese Word Segmentation, Proceedings of ROCLING VI, pages 119-142.
Rabiner, L., and B.-H., Juang, 1993.
Fundamentals of Speech Recognition, Prentice-Hall.
Sun, Jian, Jianfeng Gao, Lei Zhang, Ming Zhou and Chang-Ning Huang, 2002.
Chinese named entity identification using class-based language model, Proc.
of COLING 2002, Taipei, ROC.
Sproat, Richard, 2002.
Corpus-Based Methods inChinese Morphology, Pre-conference Tutorials, COLING-2002, Taipei, Taiwan, ROC.
References Pierrette Bouillon, Vincent Claveau, Ccile Fabre, and Pascale Sbillot.
2001. Using Part-of-Speech and Semantic Tagging for the Corpus-Based Learning of Qualia Structure Elements.
In First International Workshop on Generative Approaches to the Lexicon, GL'2001, Geneva, Switzerland.
Pierrette Bouillon, Vincent Claveau, Ccile Fabre, and Pascale Sbillot.
2002. Acquisition of Qualia Elements from Corpora  Evaluation of a Symbolic Learning Method.
In 3rd International Conference on Language Resources and Evaluation, LREC 02, Las Palmas de Gran Canaria, Spain.
Kenneth W.
Church and Patrick Hanks.
1990. Word Association Norms, Mutual Information, and Lexicography.
Computational Linguistics, 16(1):2229.
Vincent Claveau, Pascale Sbillot, Ccile Fabre, and Pierrette Bouillon.
2003. Learning Semantic Lexicons from a Part-of-Speech and Semantically Tagged Corpus using Inductive Logic Programming.
Journal of Machine Learning Research, special issue on ILP, 4:493525.
Batrice Daille.
2003. Conceptual structuring through term variation.
In Workshop on Multiword Expressions.
Analysis, Acquisition and Treatment.
Proceedings of the ACL'03, Sapporo, Japan.
Patrick Drouin.
2003. Term-extraction using non-technical corpora as a point of leverage.
Terminology, 9(1):99115.
Ted E.
Dunning. 1993.
Accurate methods for the statistics of surprise and coincidence.
Computational Linguistics, 19(1):6174.
Natalia Grabar and Pierre Zweigenbaum.
2002. Lexically-based terminology structuring.
some inherent limits.
In Second Workshop on Computational Terminology, Computerm 2002.
Coling 2002, Taipei, Taiwan.
Benot Habert, Ellie Naulleau, and Adeline Nazarenko.
1996. Symbolic word clustering for medium-sized corpora.
In Proceedings of the 16th Conference on Computational Linguistics, Coling'96, Copenhagen, Denmark.
Zellig Harris.
1971. Structures mathmatiques du langage.
Paris: Dunod.
Adam Kilgarriff and David Tugwell.
2001. Word-Sketch: Extraction and Display of Signicant Collocations for Lexicography.
In Workshop on Collocation: Computational Extraction, Analysis and Exploitation, 39th ACL and 10th EACL Conference, Toulouse, France.
Chantal Lemay, Marie-Claude L'Homme, and Patrick Drouin.
2004. Two methods for extracting "specic" single-word terms from specialized corpora.
Forthcoming. Marie-Claude L'Homme.
2004. Slection de termes dans un dictionnaire d'informatique : Comparaison de corpus et critres lexicos mantiques.
In Euralex 2004.
Proceedings, Lorient, France.
Forthcoming. Christopher D.
Manning and Hinrich Schtze.
1999. Foundations of Statistical Natural Language Processing.
The MIT Press, Cambridge, MA, USA.
Igor Mel'uk, Nadia Arbatchewsky-Jumarie, Lo Elnitsky, Lidija Iordanskaja, Adle Lessard, Louise Dagenais, Marie-Nolle Lefebvre, Suzanne Mantha, and Alain Polgure.
19841999. Dictionnaire explicatif et combinatoire du franais contemporain, Recherches lexico-smantiques, volumes I-IV.
Les Presses de l'Universit de Montral, Montral, QC, Canada.
Stephen Muggleton and Luc De-Raedt.
1994. Inductive Logic Programming: Theory and Methods.
Journal of Logic Programming, 1920:629679.
Rochdi Oueslati.
1999. Aide  l'acquisition de connaissances  partir de corpus.
Ph.D. thesis, Univesit Louis Pasteur, Strasbourg, France.
Darren Pearce.
2002. A Comparative Evaluation of Collocation Extraction Techniques.
In 3rd International Conference on Language Resources and Evaluation, LREC 02, Las Palmas de Gran Canaria, Spain.
James Pustejovsky.
1995. The Generative Lexicon.
The MIT Press, Cambridge, MA, USA.
Frank Smadja.
1993. Retrieving Collocations from Text: Xtract.
Computational Linguistics, 19(1):143178.
Ellen M.
Voorhees. 1994.
Query Expansion Using Lexical-Semantic Relations.
In Proceedings of ACM SIGIR'94, Dublin, Ireland.
AMulti lingualDecisionSupportPrototypefortheMedicalDomain DavidDinh DennisChan JackChen  osTechnologyPty.Ltd. PSTResearchGroup PSTResearchGroup HealthTechnologySolutions VoiceSolutionsDeveloper HealthSolutionsD eveloper david.dinh@ostechnology.com.au dennis.chan@pstresearch.info jack.chen@pstresearch.info  Abstract  Inthispaper,weareproposingamulti  lingualprototypethatcanefectivelycol lect,recordanddocumentmedicaldatain adomainspecificenvironment.Theaim ofthisprojectistodevelopanelectronic supportsystemthatcanbeusedtoassist asth mamanagementinanemergencyde partment.
 1 Introduction  Speechtechnologyhastheabilitytogenerater e sourceandtimesavingswithinahospitalenviron ment.Recordingandmanagingpatientdatafrom nonEnglishbackgroundscanbeachievedsuccess fullythro ughtheimplementationofamultilingual voicesystemandastandardisedelectronicmedical decisionsupportsystemsuchasACAFE(ACAFE 2006)describedinSection5.3.Byimplementing theACAFEsta ndardizedprotocolstogetherwitha voicesystem,wearea bletoassistinthefirststage oftheclinicalpathwayinthetreatmentandma n agementofAsthma(seillustrationofStage1in fig ure3). Inthisdemonstrationdescription,wearepr o posingamulti lingualvoicesystembasedona standardizedpatient managementsystemcalled ACAFEthatcanefectivelycollectpatientdatain electronicformat.Thecombinationofthetwosy s temswouldmakeiteasiertoassistintherecording anddocumentationofvastamountsofinfo rmation whilstovercomingcommunicationandeficiency bariers.Thisdatacanthenbeaggregatedandan a lyzedaftertheeventtoassistwithclinicalandpe r for mancemeasures.Thismakesefectiveuseof emer gencydepartmentresourceswhileproviding theemergencystafwithimmediateacesstoim portantpatientinform ation.
 2 Objectives  Toshowhowqualityhealthcarecanbedelivered inacomplexmultilingualhospitalenvironment withtheaidofanelectronicdecisionsupportsy s temsuchasACAFE.
 3 DemoDescription  Ourdemoprototypeintegr atesavoicerecogni tion systemtogetherwiththeACAFEsystemdescribed inmoredetailinsection5.3.Ourvoicerecognition prototypereliesondataextractedfromthesta n dardizedtreatmentprot ocolsthathavebenbased onresearchby ACAFE(ACAFEeta l.,2006). Thesestandardizedprotocolsformthebasisofour system patientinter actiontothemedicalsub domain(Sta rlanderetal.,2005). SinceoursystemisheavilydrivenbyACAFE, wehavebenabletominimizetherequirementfor anopenrangeof questionsthatr equiretranslation. Asaresult,weonlyrequiretheuseofthegra m mar basedlanguagemodel(GLM)thathasben implementedusingNuancesspechrecognizer (Nuance2005),andnotastatisticalla nguage model(SLM).
Thestandardizedproto colsrequirenomanipul a tionorchangesintenseastheACAFEsystemis essentiallyadecisionsupporttool. Theflex ibility ofthedecisionsupporttoolallowstheclini cianto makethefinaldecisionandvaryanyresponsesor inputs. Hencetherangeofq uestionsourmultilin gualsystemposestothepatientisalsostandar d izedandlimited.Withthe smallersetofquestions itisfeasiblefortran slationtooccurviadirect ACAFEto'target language'mappings (subjectla n guagetomanyvariationsofatarge tlanguage). TheuseofGLMsoverSLMsformedical speechtranslationhasbenproventoprovide highertranslationacuracy(Rayneretal.,2004, Rayneretal.,2005).Weexpectthatbycombining thehighera curacylevelsofrecognitionthrough theuse ofGLMswithalimitedsetofpossible questionsforapar ticularmedicalsubdomain,we canachieveanimprovedtran slationsuccessrate. Currently,oursystemrequirestheOverser (suchasanurse)tospecifythepa tientsnative language(inourexamp leChineseMandarin)and problemsub domain(inourexampleasthma). Fromthere,theOversercaneitherspeakaque s tionasde finedintheprotocolscontainedwithin theACAFEsystem(usingEnglish),orselectone usingtheterminal.Thequestionisthen rendered usingr ecordedaudio(TSisusedasafallback strategy)andplayedtothepatient.Oncethepa tientrespondsve rballyorphysically(e.g.nodof thehead),theOverserisrequiredtoenterthat responseintothesystem. TheOverseriscapabl eofviewingreportsthat detailaparticularpatientsresponsespriortofur theranalysis/treatment,ortheycanviewsta tistical reports.Asaprofofconcept,theOversercan generateastatisti calreportthatdetailspatient backgroundprecipitating factors(numbersofre s piratorytractinfections,coldweather,exe rciseand dust/pollens)  4 SuggestedScenario  Thetriagenursewillidentifythepatientsnative languagetoenablethecorectvoicesystemtran s lator.Thevoicesystemwilltranslatethe standar d izedasthmamanagementplanquestionsintothe patientsnativela nguage. Patientwillanswereachquestionintheirnative tongue.Thevoicesystemwillconvertthisinfo r mationintotheACAFEsystemformat.Wheneach questionhasbeenanswered, theACAFEsystem willstorethea nswersandthevoicesystemwill thenfollowthroughtothenextACAFEquestion. UponcompletionofthesetofACAFEbased questionsthevoicesystemwillthenprovidear e viewofthequestionswithanswersintheACAFE systemineitherEng lishorthenativelanguage.A voicerecordingwillalsobestoredtoplaybackfor futurereference. Triagereferstotheanswersthathavebenco l latedintheACAFEsystemviatheassistanceof thevoicesystem.Thisinformationcan beunder  stoodbyalleme rgencyteamstafasthevoicesys temhastran slatedtheanswersofthepatientinto Englishacordingtothestandardizedmanagement answers. TheEmergencyDepartmentnowhasapre  compiledlistofpatientinformationcompliant with Stage1oftheclinicalpathwaycontainedinthe ACAFEsystemtohelpassistinthetreatmentof asthma,withouthavingtoworyaboutcommuni cationdificultiesb etweenpatientandmedical staf.
 4.1 Demoscript TriageNurse:  Hello,whatpainsordificulties areyouexperiencing?  Patient: UnderstandEnglishnogood,asthma  TriageNurse:  Canyouconfirmyourlanguage, MandarinorCantonese?  Patient: Chinese,mandarin.  TriageNurse:  OK,whatIwilldonowisusea specialmachinetoaskafewsimplequestions,you canjustansweryesorno,itwillasktheque stions inmandarinsoyoucanunderstandbetter.OK, herewego Triagenursethenactivatesthevoicesystemwhich goesthroughthesetofACAFEbasedquestionsin mandarin.
Patient ACAFE NursingTriage  Figure1:High levelviewofuserACAFEinte r action  5 SystemArchitecture 5.1 Overview  Figure2illustratesacomponentviewofthede sign forourprototypesystem.TheOverseractsasan overidingauthorityfortheACAFEDecisionSup port component,providinginterpretationsofthe Pa tientsnativelanguage,medicalproblemsub domain,andasafailover,thePatientsresponses (bothverbalandphysical)totheque stionsasked. Records Overseer Patient Multilingual Recogntion AudioOutput ACAFE questionsresponses Reports Language/ Problem/ Responses MultilanguageMappings question Language/ Problem/ Responses  Figure2: ComponentoverviewoftheSystem Architecture  5.2 SystemComponents  Thefollowingsectionoutlineseachcomponent shownintheOverviewdiagram(Figure2).
 AudioOutput   Rendersquestions(asrequired bytheDecisionSupport)inthePatientsnative languageusingrecordedspech,orTexttoSpeech (TTS)ifther ecordedspechisnotavailable.
 MultilingualRecognition   Themajorityof questionsposedtothePa tientareintheformof yes/noquestions.Assuch,therecognitionofthe Patientsutteranceneedsonlytorecogniz ebasic responsesinthePatientsselectednativela nguage.
 ACAFE   Providedwiththemedicalsub domain(e.g.asthma/breathingdificulties),spec i fiesquestionsacordingtoastandardsetofdia g nosisque stions.
 Records   RecordsPatientresponsestoQue s tions(bothtextualandaudiorepr esentations),final outcome,andstatisticsthatareusedforbothindi vidualPatientreportingandstatisticalrepor ting.
 Reports   ProvidesindividualPatientreporting (i.e.nativelanguage,medicalsub domain,r e sponsestoquestions,andfinaloutcome)andstati s ticalreportingfortheuseofmeasuringthe relationshipbetweenasthmaandthepr ecipitating factors.
 5.3 AsthmaDecisionSuport  ACAFEisanelectronicinterfacefortheEmer gencyDepartmentthatprovidesclini cianswitha decisionsupporttooltoassistinthemanagement andtreatmentofasthma.Thesystemincorporates clinicalde cisionsupportbasedoncurrentevidence andguidelinesthatissimpletoacess,adaptableto theneedsoftheclinicianswor kingintheERandis capableofbeingintegratedwithexistingmedical databases. Thesystemscorefocusliesinclinicalpathways forthetreatmentofasthma.ThisisshowninFig ure3below.Aclinicalpathwayinthemedical senseisadecisiontrebased onclinicalassess mentthatguidesthemanagementandfurtherin vestigationofapatientwithaparticularclinical problem.Thisdecisiontrehasbenbasedoncon sensusguidelinesandinstitutionalprotocolsbased onthebestavailableevidenceforthe management ofasthma.
STAGE 1  PatientHistory  Presentingproblem Historyofpresentingproblem Specificasthmariskhistory Medication, Alergy  STAGE 2  Examination  GeneralApearance VitalSigns RespiratoryExamination  STAGE 5  FinalAsesment STAGE 3  Diagnosis  WorkingDiagnosis DiferentialDiagnosis ConfoundingFactors  STAGE 4  ElectronicDecisionSuport  Figure3: TheACAFEclinicalpathway IntheACAFEsystemtheclinicalpathwayis representedbytheinformationrequiredtoasce r tainthes everityofasthmatodecideonalistof furtherinvestigations,consultationsandmedica tiono rders.Theclinicalpathwayoutlinesthe meansthroughwhichthesystemcanadvisethe doctorontheoptimalasthmamanagementcare plan.
Atthisstage,ourvoicesystemwillbeintegra ted withstage1ofACAFEsclinical pathway,inpa r ticularthehistory/informationcollectionsideof things.
 6 Conclusion  WehaveshownthattheACAFEsystemwiththe assistanceofourvoicesystemcancapturethein formationrequiredtoassistcliniciansbetterma n agethetreatmentofasthmainanemergency department.Incapturingthisdata,theACAFEand voicesystemincorporatestheclinicalpathways anddecisionsupportintheworkflowofthedo ctor. Inthisdemonstratorpaper,weproposedasy stem that: ReliesonACAFEbyprovidingan electronic standardizedprotocolforthetreatmentof asthma. Allowsmulti lingualsupporttherebyincreasing communicationbetweenmedicalstafand patientsduringinformationcollectionand follow upreviewafterthepatienthasbeen discharged. Increa seseficiencybyautomatinghowinfo r mationiscollectedbyassistinginther e cordinganddocumentationofvastamounts ofinformationwhilealsostreamliningthe updateofdataelectronicallyintothepa tient medicalsystem.
 Acknowledgements  WewouldliketothankosTechnologyfortheir inputrelatingtotheACAFEsystemdevelopment. Wewouldalsoliketothanktheemergencyde partmentteamatCanterburyHospitalfortheira s sistanceandexperta dviceinthefieldofasthma patientcareandinformationc ollection.
 References ACAFEResearchProjectandDevelopmentteam, http:/ww.ostechnology.com.au/acafe/our_team.ht ml.AsofJanuary2006 Nuance, htp:/ww.nuance.com.Asof8December 2005. M.Rayner,P.Bouilon,N .Chatzichrisafis,B.A. Hockey,M.Santaholma,M.Starlander,H.Isa hara, K.Kanzaki,Y.Nakao(2005).
 AMethodologyfor ComparingGrammarBasedandR obustApproaches toSpeechUnderstanding,In  ProcedingsofEu rospeechInterspeech,4 8,September,2005, Lisboa, Portugal.
 M.Rayner,P.Builon,B.A.Hockey,N.Chat zichrisafis,M.Starlander(2004).
 ComparingRu le BasedandStatisticalApproachestoSpechUnder standinginaLimitedDomainSpechTranslation System .In  ProcedingsofTMI2004,Baltimore, MD UA,2004.
 M.Starlander,P.Bouilon,N.Chatzichrisafis,M. Rayner,B.A.Hockey,H.Isahara,K.Kanzaki,Y. Nakao,M.Santaholma(2005). BreakingtheLa n guageBarrier:MachineAsistedDiagnosisusingthe MedicalSpeechTranslator .In  Procedingsofthe XIXInternationCongresoftheEuropeanFedera tionforMedicalInformaticsMIE,28August 1Sep tember,2005,Geneva,Switzerland .
E C  B     ' @     6   ! 4 ) 1 )     '  $     ! !                       FD##76A987&52320Q(&%#"R C)QI

u  ~ u | ~ u w  ~ y  u w u t | ~} u |  | w u t |  ~} | u v  v u | ~} v y u  ~ u v u u v   w  y t w |  u v u   xAgexc8xxcgpgcjgGgAcAgepuxAxCGchAAcAp} u t |  y u w { u t | w   u } v v  ~ t y  t } t    | v y  w ~  v |  y w ~  u  u t |  ~} u  w   u z y c6gAjpGecgIxpAIAw  u v u  | y ~ ~   u w v { y  w}   ~ y  u w u t | ~} wz u    u t | |  t | w} u v u t | ~} y  u t Cc5jjxcpAx1%ppA1g0gcxne16s  d    r f  r  b q  bh  b  f r  r  d r    r    dh d   r f f   r h    b   gpgpd qgg5c5gcpqx  C  d    r f  r  b q  bh  b  f r   d r    r    dh d   r f f   r h    b   epgjd qg5c5xpqx          00'    ~} v u | z  ~    ~}  u w w {  w}  ~ u u  w  t  ~  w |  u  w RphAgp8xcA8Ax02gCx z  v y   u | w u z y  ~} y wz  v}   z  } ~}   ~}  ~} | w} w ~ y  uz     u  ~}  yzz y u t gAguAep!u0pGpcAjC5pp6s Axx1pgcjxgjAxAAAjpjxp~  pcrxg| v u t | y t   u ~}  v | w ~ y  ~ y} | {z y w u v u  ~ u v u u v  ~}  } v   ~  u   uz  y z v y   w ~ y}  z u v u w v { y  w}   y ~ y} |  | {   y  u t |  y t  y t w z v  uz  w uz     u u  y   u t ixccxcpggjxGjgjcpAxjCG16s  | y ~ z { y        ~          uz} t   ~ y} | z u x2puC%jBQscCppiAv u w v { y  w}  }z    u  z { y          w { t | u w   w} t | ~   y | w  ~ yz u  v u}z u  ~  t cxcp!p6prQsAujhxgGwgpAxpxj0 u t |  y y v u t |  y  ~} v u | ~ u u t | w u  u  u v  | ~ u  u  ~} u u w u t | }  y y v u t | y |  ~}  ~ y 5gpcxAngxAxxcACxAngng1g!pjpz  u  w   u z y w u v u  z ~ y ~              ~ y} | } v  w u  u |} ~  u  u t |   y y v u   CuxcxBxCQCj0pgpAA0Ce8gw u t | ~} | y ~ u v  t } t   y  ~}   t  { y v t | w |  u   y u u w z ~ y ~   u  |  t |  ~}  { pcp%gAAcA2%Grgjpw  w   ~   y y v u t |  y | v    ~} u  w   u z y w u v w}  y  ~}  u t | |  t |  u }  y v ch!cx!xA5pjg50gxc     bh  r f   q   f  r h r  b    r f   d r       r f  r  r f b r b           '6pwp6gpgrnwhp5qqgsx 00'  uz     pjCu  ~ }  y zz y  u t | v u  } w ~ y  w | u z  y   | y ~ z { y            u z} t   ~ y } | { z y w u v u t jppghApcx6Cpj2rQRAp&pjpcxcg| t |}  | ~ u | w} w ~ y  u  z { y            v y          ~} v v u ~}  v z { } | v   ~ gexcpcA!r6i5qAsrCnQRcnBcxj0xpxgc0w AxcxsjgnxxcxjpiAcApjcxcAg8xp} v u  v y u   v { w u t | w u  v u w u v  ~ y} | z u v u w v { y  w}   ~}  ~ y  w u v v y  u t |    u w y   v u  v y z  v y   u | u t | } z ~ y  u }  | ~ u  u  ~} v u | ~ u u t | w  y zz y  | ~ u  u  ~ } u u w u t |  xjgxGg8jnQexAxexp!eAAjpxAI} u  ~ u | ~ u w | w v  u t | ~}  y y v u t | y |  ~}z u  z ~ y ~   v u}z u  ~  t  u t |  |} ~} | y xAgexcncgcgBxu  p2xpxj0Gx~ u v  u  }  y y v  ~} w |  u   y u u w | y ~ ~   u  |  t |  ~}  { w w   |   ~   | w v  u t ruAxcA!ungpcCswBxcc0g| y |  u t   | |  w} u  ~ u | ~ u w  ~ y  u w u t | t } t    ~ y} | z u v u w v { y  w}  u t | ~ y w  ~ u  u gBAgrpAxexjxAjg1gpxcxcpghjxx CQCj0xjxAAcpcWgjAjghjCgWw           u  ~ u v u u v  ~}  } v  u t |  y ~ y} | {z y w u v u t |  uz     u w} t | ~  f  h    bh     d  r h r  b    r f   d r       r f  r  r f b r b            icpxgpgrnwpGwsg2x s00'    yv Q  ~  | w} uz     u  ~}  yzz y u t |  |   w} t | u |  v y  y v v y  z v  uz  t } t  w uz     xu  pjCjppg)CspGgcx8cxpApjCu |exA1rpw!jxcprpCcxgeGggepjxp~  ~ u w u v  u  w  yzz y |  t  ~  u w v { y  w}    y ~ y} |  | u v  v u | ~} u t | y | ~} u   uz  y z v y   y ~ y} |  v  u | ~} u t | t |}  w  zz u  w  w ~ y} | z u v u w v { y  w}   y ~ y} |  | {   y pcrpgxge81pAu11gpxccApjggjx u t | t |}   u | z u v v u | ~} z t } t z ~}  | v u  w} w u  ~ u v u u v  ~}  } v   y ~ y} | {z y w u v u t 8g2AgpxcxeppjxxxxjxAAcjp2pjpcxc6s f kp g o m l k i g f rq0&n0jhe r  q  d       f h d  r  h b  y x v a t r f q f h f d b cuwiiccgp&&wu6seipigeca ` Y XH VH T S SHPH F I&QWQUIRRQIGE d

D      9 8CBA)%@58166532)10)'&%$    7 44 "    (

" #!           

           aIDD{i&W&XD3fr`{f&dsg3DqaD&Ur`{f&ddp&                             

r u ryr u uz xSx{ya{X'D3}{`aXxvsq  r    z  w q ~ | u zw yw u t r

F X

9 p

o )l nm

 w  yzz y w   v y u t | v { y ~ xp6cxjp} wgnx gpG piAAcexxcGcxcppC } t | u } |   y}   u  ~ y} | z u v z  } v y | u t v u  y w   w | ~ u   u w u w v { y  w}  w { y}  u v y  u |  u ~ ~ y  u  y  u u ~ w | ~ u   u w u w v { y  w}  |  t |  s g| xgCxjxg| Ax1AxcA10 ~} | ~} y  ~}   u t p1eg| u  y  u  u v y  v u t | v {  ~     z  | u v u t w xA2Ag jUQAxc epixAcxu xc Ax|    zz }  u  w  u {  v ~ u u  w  t
k l

             pC&Cr gp
j $ i

@A `B@

R g R v u es b Re R d h x  e v Rs pe R  b R  v Rs p  x  v Rs Rp xp V h1yqItSiDfWawIwu V X'Wdiwu V t'Squ V Xrar v ue qUT V X'qDDq`T V t'qiSwu V tWy)dtWddqdt'Wi Rs p h Rp x  vuY Rs p h R  x v Rs R R  v u bs b v u Rs p e R  b R v u Rs p  x  qWX'qWX'qiiyqct'qiDDt'qiaywdtSirqigfdca`XWUSQ v u Rs p h R  R v u bT Rs p h Rp x  v u YT Rs p h R  x v u Rs b Rp h R eT bT YTV RT R P

H I

 G

 w| ~ u  u  y v u v y u  v {w u t| w u| y ~ u AgexAxc5xsnAgjx u v u t   w  yzz y w  A0xpw  p2pjpcxcxjpj0 pjCxjpjx y yz ~ y} | } v  w u  | {  ~} u t | t } t  v y    uz     u v u } w ~ y uz     u v y  w | u  w | ~ u  u  u z y  ~} u t |  y v u  v y u    v { w u t | w u |  | w v  z {  } | v  n5C' xgeAAxjgu!Axscc5AggciApg0 ~} u w v { y  w}  u t |  y ~ y} | } v  w u  | {  ~} u t s  u z  ~  y | | ~   u  u w v { y  w}  t   p!cxcp!gpgpAAp!66A 0Bgeh5u5cxcpu v y w u} v   |} w   v y u t | u t |  y | v   uz  } v   u t | w u | { |} | w ~ y  u w v { y  w}  | {  ~} u t rxpcrhBcAgjg'uc0pjicguAgjcAcApj6j6s
F B

           gpr
E 9 

CA DB@

 w ~ y} |  u w  ~}  yzz y u t | ~} w | ~ u ~ y   y  u w u t |  y t   u u } v  w u  u  u   uz  y AgAAGjppgBexx5AgnAxj5 jxp~  z v y     ~   w  y}   z  v y   u |  ~  u w ~ u |  }  w ~ y} | z u v u w v { y  w}  ~ y w  y}   }} pcrGjAjgAgcx)jxpgixc6cxcpp'p}  w uz }  ~} v  u w v { y  w}  }}  u w v { y  w}  | {  ~} u t |  y ~ y} | } v  w u   }  w | v    ~}  yzz y AxpApcjxcpipcxjjpgpjpcxcxixc0pp u t |  y w | w} w ~ y  z u  y  z  } ~}    ~  y | | ~   u  t } t  v y  v y u t | z  }  yz u t cpcAxp1hr2hBxppn6s
8 3

ik r)

% $3 2

g 1

4 7 6

op k qu 

# 1 5$3 4 % 2

g k gf pl i f  W u n) r e
0 1 ( ) % ' % & # $ "

!

 

  

 

 

 

 

 

 

 

 

 

  y |  u v v u u v w} v u   u v  u | w u v u | ~} u t | v y   u | z u v  y  u}  v u  y  uz}  | u R 2AcxAc6pxjcxcAxepng0  uxiAUCpcAxpgA u v y   v y w  zz u  w   ~} | w u v u | ~} u v  wz u  y  z  } ~}   t  | { y   ~ y} |  } | y c2!rpAunjpgxcxge!xpe!pg v u t | v { v y  uz   }  u  u  zz  { |   z { y t w ~ y} |  | u v  v u | ~} u w v { y  w}   t  ~ y w  u xjgcju0 0xAp0AcBggAjxepcApjeBxv w { y}   y y ~ |   ~} w} u v u t |   u}  v { y ~  t   y v    v { y ~} As2cxnCpwuec2p ~ y z u v u    xc!rQ y | | w  v | ~ y  ~  ~ y} |  | u v  v u | ~} u w v { y  w}   y uz }  ~} v   ~} } v  u t | w} |}z  } ~} gcgeAwhggAcAgepGcx5pApcpcjgp1wp |  t | ~}   z  | u w   y w  zz u  w      v ~ y 02) QCj pAu  jeAn xjej!g|  ~ | ~u v y wu t yv   u t t |}  u ~}z ~} w} t   y v    v {  ~ y} | z u v u w v { y  w}    u v v u ~} ~}  | v u    y w ~ y} |  gpBhpej6 &piAcApjrxcAjppgcxAujp &xrAp~  rueAgcx5puugpQ p} }z  } u t | w  zz u  w  u   uz  y z v y  t |}  | ~ u | w} w ~ y  y wz  w} w { t | t } t    ~ w  u w v { y  w}    y z u  y  z  } ~}    ~} z} {   y |  {  y v     w} ~ y} | {z y w u v u  ~ u v jxcpn)0Ajp55p&hCe!pgjAAxcxu iCjp%8eAxxcwc!giAx5AAGrAg|  u v  ~}  } v  t } t  ~} | ~ { y    v u  v y  | w v  u } |  v z  u   | ~ u w u v  u  v u    w} t ~}  w ~ y} | z u v u w v { y  w}   y ~ y} |  | {   y  u t |  ~  ~ y} |   v y ~} u w ~ u |  u   uz  y p'AgpxcApju%pgjA%%gjpAg)jxp~  z v y   ~ y} | {z y w u v u  ~ u v u u v  ~}  } v  ~ u u | u  y ~ y} |   v y ~} u t | z u  y  y | v u pcrgjA6AxcxAcjp1AxrwA 0 gjphjgexgx qw&pcjrcx5e)pAu')jpiAjxcp5pgcAxne!Axjcgep} v y ~  ~ y} |   v y ~} u w ~ u |   w  zz u  w  w ~ y} | z u v u w v { y  w}  ~}  | v u     u  {  y v | ~ wAxA cx1xrnxj~  pcr5exgpcx1g0t u  ~ u { u w ~ y  u t | t |}  w  zz u  w  u   uz  y z v y  t |}  | ~ u | w} w ~ y  u  y | w  u  ~ u v u u v  ~}  } v    y ~ y} | {z y w u v u t | |  t | u  {z  ~ y  y | u   t u  z  v u ~ u  ~ xAAA11I2gjA5g05jpAxg505u'xAw  ~ y} | {z y w u v u  ~ u v u u v  ~}  } v   y | ~ { y   gjA5xAAAjpeAx  ~     u |} } t y v  u  w { t | z { y t w  u } v v  ~ t y  t } t    | v y  w ~  v |  y w ~  u e%xgpjgcxcpjecjgx u2xnxrgjA6AxgcpA55pejg t |  ~} u  w  z u    u t |  y ~ y} | {z y w u v u t s  w u |  | w  ~}   z v u  y |  w} w  y u t | | u  w  zz u  w  w} w  y u t | |  u  | y ~ z { y  wz u    u t | w    ~ u | w} w ~ y  ~} ~  z u} pAunjg1npjuxjgCxcpcApxp  z { y  t } t       ~    u v    y   z  } v v  u t | t |}   ~}    z v u  y zz  v y   u | w pu%& GAsg!pxGgxr
  

 3    

 {

 )

 W

 d 

 f d

 D

 



   

 Bd

  dDD  d3   

 Bd

 d   



 ds      s   

     f dIIS3

  {

 )

 

 

 s  

 





    I  IS3     IS3  p3   IS3

y wx g  #6 ef   I " d u d u d  iu  u    I ( d u& i # 
 w  

 v  

 d

 w

   qfrcB D3     

 

    cfr&r)   

 

  



 ( iu  ( iu  (  (   i u  iu    I i u x I yw" d u  i u ! u i v ts


~| 3}{`aXxvsq u zw yw u t r



F `

F `



 q

  !

 )



 w  yzz y w  d a  y w  v u | ~}  u ~  u  ~ u t | w} i a u |  }  u v   ~}  ~ y  w u v v y AphGrq6Agp8x0C2x!phgxpA1Acx u t s  ~ y} |  u w v u | ~} |   u u t |  ~} | y ~ u 6gCxcxwjxgjpgx gpcpxAcB10ppGrAxc0 f ~ } w z }  u  w  u   t zz}  u   v u t | v { Axp0geAAuwWAxrwA gAAcxgegjxecgC8xjcg| w u} |}z  { | ~ u  u y | ~ u u | u ~ y} |  u w v u | ~} u t |  ~} | y ~ u  d a ~ y} |  ~ {  u  {  y v  ~} zz}  u   |   ~  w u} |}z  { | ~ u  u ~ u u | u prAswGAxg0ACxAuwC biAr%pGxjcgep} a  z v u  y  y ~ y} | y ~ u t | u  {  y v | ~ y |  u u ~ u   v u  u  y t  u v y u Y  |  u v u  } u t | ~ y uz }  ~} v  | w v  u t | u } |   y gWAxuxAArhCu`AAcAp8gpxc8Bx p}    z ~ y u   w { t s   v u  v y u   v { w u t | y |  ~ y  w u v v y  w   z  | y ~ w u y  v u  v ux6CgxcBAsccgAxh1xGxjy z  v y   u | w   u } |   y}   u  y |  ~ y v | w y | z    y v  w} uz }  ~} v  ~ y  u w u t gAgs xA g2gcg0cppx xA6s  w | ~ u  u  u | v y  u v u t |  y v u  v y z  v y   u | v z} } xgeAA2xcxcngxc&gAgiw   u  ~ y  |  u v u  u t | ~} w u  ~ u | ~ u w  y ~ y} w w u   { w  |  t | w } z     2AxAAcx5gBxAxApcxAxcguppxn h t } t  v y xAxxgrchuph6xcAxuAAcABpAgcAsexAu |  u v u  u t | v y  u w   u t | | y ~ w} w} t s   u v v {   y  |  u v u  u t | ~}  u | v y  u v | ~ u  z0Aj6gGIxjIxAjxx6pjpAAxAjxepIgr  { |  ~ {  u t | t } t  v u  ~ { w u  ~  | w  {  v}  u t |  ~} } v  w u  w   u | u v  v u | ~} w { t | u v w u ~ y v u  v y u t s   u v u | ~ { y  ~ u w} |  u v u  u t | ~}  u | v y  u v | ~ u  u | w v  u t | z} | ~ xxG6GAxexjxnAxxg%xcxcexA0Gjgge{ |CxcAprexCnpAxApgpxIAAcxp6GpxgxexAu  u v u  } ~} w | ~ u  u  ~}  u  u v  zz  t |}  w  z v u  y |  u v u  } u t | ~}  u | v y  u v | ~ u  ~    ~}  yzz y u t | w u |  | w zz  } w   u v {  u  y v  u t s  u w v { y  w}   ~}  u  u v  u t | y pGjgx8pxpc01jxxjj6GcApAxABg| u | z u v |  u v u  } u t | ~} v y |  u v u  u t | ~} u  ~ u | ~ u w iAAAcApnAxxngp!xAgexc jpxcxu C0R   y t  ~} } v  w u   X      u v {  u  y v   w | ~ u w u v      v z { } | v   ~  u  z   ~  | w} ~ y} |   u t | t } t  ~ jxxjgexcxjn AiApc0wxiGjp}  pBpgCBpp}  ~} | | u w u t | | ~ u w u v  y | w u  v u w |  u v u  } u t | uz} t    v y | w   y ~ y} |    y u w v { y pC1g!exA2g5xcxcnAAcxpgGp)g2upAujx ~}   u t |  y w | ~ u  uz u u } w w u   { w u t | | v y  u v y |  u w { zz  }   | w} V t  ~ u v v y | nexxpAcxAxc5jg6cAgxjpxpwhpBWxc h} w u w w {  w}  u t w   uz  } u w w   V       Q  u t | |  t |   ~}  u {  v  w  t    xcxcppj ECjpAC0 Ac0n
m h   ! 

 Q  Q

P

  Q c0

U

  njC
 

U

@T

A B@

 }

   ~}                  R             u t | w  zz u RQscC&C0xSjsRjwG'pAu wic0xQRAc C0QRc!xxjx cAgAg|    u v    y           ~                ~ y w u  ~ u { u w ~ y  z  v y   u  u u ~ zz}  u   v u    w} t |  y w u w y  v {  u t | v y  ~ y} | z u v u w v { y  w} ~ u }    xxjup2rxx01gxcgG piAcApj Ay w u  ~ u { u w ~ y  z  v y   u | y} |   w u t |  ~} }  u  w w  y}   u ~  u  y |  u u ~ u   v u t | v { AxA cxgxgQp0cuxAcjh0CxAurAxc0
  

             jQRCp
i

   Q  GQ

P IH
m

A B@

 w ~ y} | z u v  y   u | w ~} w | ~  | w ~ y  v u  v  | w v ApiA2ccjx6Ay c0 w  w ~ y} | z u v u w v { y  w}   ~} |  u v | u v  u    ~ v y  u t | y | | w  v | ~ y  ~}  y   | npgixc1jxcpgxgcrI p}  u2!gxp) Au g1E9QRcCCr'9QsAAC@Cph 9Qs      G F   D 8        B   8           8     8     8          w  t  { w   55 3 u v u t ~}  u } v  w u  w ~ y} | z u v u w v { y  w}  u t |  ~} | ~ u w u v  u v w | ~  | w ~ y  u v pAAxjjpiAhcAupexAAIcjx 2 3 5 76% 4cx
j  q

2# $

    DdISDS

# ' ) ' %#  10)0)( &$
   i

    `ISD

#! "


    `ISD{ i&aW3IS3    

  u zw y {`aw

  
r 

r    z  w a{X' q

|~ | t3}{`aXxvsq u zw yw u t r

w~u Ax          |  t  u ~ u  y| u   t z ~ y u  y    ~     y wu t  yv    u t 0j0A2g10r Q j !xejg| t |}  u ~}z ~} w} w} t s   w | ~ u  u  y v u  v y u   v { w u t | y |  ~}  v y    | ~ u  u  u ~ y} | ~ u grpGIp6IgeAAxc6xscchgAxsACGxpgeA z w { y}  u v  u  y w y |  u |  u ~ ~ y  u w v { y  w}  u  y | w  t | ~ u  u t   u  w ~  u  |  t pAc1g2xgCxjxncApj51ghexA2Ax06s
 

k g

u

s

u

s

u V fdUT V tdXqu V fa`T V Xdtywu V Xs eTe Rs e e v eTY Rs e e v e

h p xe  WD'dXe

V SQ e

u

s

u V fXiq T Rs R  s

xe 'pv

V

R BdUa`T V XdXpxR v ubT Y s e e v u

 $wca`aXdte v u bT YT s e  ubTe ccts

V faQ T

u

u bTeT Rs e e ccUtdXR

s

h `t)t3SQ v ues   xe e

u

s

upe cf WdUT V UXcWd Rb RT Rse R b d u

u h p x b b x h BWDDdWawUT V faXcWdd RT Rse R b s u V UtWrWwqW'ad RT Rs d Rp e R h h  e

H

w  zz u  w  r m| {  n n  y r m s  w r  v u  s r m o   n m    k   | u  u  ~ u t |     pAuD}z$zH2eDx&t$)3&xtDqpwuleCrAg0QRc j 6BeAgAAA0gj &pjpcxcAxcxAcppcg|     u |  u ~ ~ y  u v   u  ~  u |  t | u  { w w  ~ y} | {z y w u v u  ~ u v u u v  ~}  } v  u t eGAgAA&UpgixcucA5pjjAugcAAgep   u |  u   w} w ~ y} | z u v u w v { y  w}   y ~ y} |  | {   y  u t |  y t w}  ~} | w u v u | ~}  y i G   h'    '    E g h     '     c  h7Q 8  ! '    c@   Ef  &   d e d r     8R   u E        "      ! 7   v     '  ' E     (     78'sCD# y"               E       x  v u &88Q  8 y  D w" Et  w u  ~ u v u ~}  ~}  yzz y u t | | u   ~  xxAxcxjGp6A25 ~ y} | } v  w u  | {  ~} u t | u  { w w  zz}  u   uz     u  y ~ y} w w {  w}  u t | | v  | w  | u gjxcxjhujppr   jxujgwA'
9  H X H   ( %4 'sryph d q6i g

k g f kp g op juq 
6 7

4 fec d d

b

9

 

  y| vu   uv u|wuvu| ~ Qg6xjc8AgcAAgep} u t | v u u v u  v u    w} t |  y u  y  w u t |  y | { y w} u  u t  w  ~} ~ y w  u v  ~  ~ y} |  | ~ u rxC Ax0p&xgrjpxjjpccBggAw  u v  u v u   uz  y CcxcGAp~  pw)AxCu gC0&pQsC0xR  } ~ y | y ~ y  ~ y ~   v u  u  y  Q                     `    T    T          Q       T           V  Q    V  CaB 08 UIYG gG Cn5uXW rj!106&0      u }  ~ y } t w    } ~ y | y ~ y  ~ y ~  ~ }  u |  z {  v y  u  z z  { |    z { y t w w  y }  pQ&pcspg 2pxi%pgApnp u  y   u t |  y | w y  |  t | u | y ~ y | | ~  | v y  } w}   u v { } ~} ~  y t w u v  v u   GIcg!Bcpp|  p p%!x0 w} t |  y w u w y  v {  u t | v y  u  u u ~ w  y}   u t s  w | ~ u  u  y w |  u  u  ~  w ~ y} |}  ~ y gAjGg%xAx6!xgeAA5AA&6%pjx  u v   ~} } v  w u  w  y}   u   t zz}  u  z | ~  | v y  } | w y   w  y}   w w u ~ | ~} y w} Ccpcxcx0pugegcpB6xpcxepcp  ~}  ~ y  w u v v y  u t | t |}   t  v  v u} t |  u  ~ y    ~}  y  ~ u w  y}    ~}   t w u } w u pjxcAg%egx5jxAA2xAp0nApcxuY  u   uz  y xj~  pcupjAxphc!cAgp!pApAjpcpu6s z v y   ~}  y  ~ u w  y}   u v   v y u t | z  }  yz v { y ~} | ~ u}  u v  ~} | w z u t
H 

 G   T   S   p6U&52C

R

PA QB@

 3 S   

g# e  1
    cfS

     HdDW{hC   I F GDd ) C

u i  (

  

       DHdDW{&

@ A sg   


u i

 

  I  IS3  

  E  '&d)6aD

 (

 saDBdDW{&         

@

 

  E     D6aDW    7aDv  

@

 d

 IS3  IS3 

#g# p
 { 

 saDBdDW{&          

@ B 

g #g
H f

@ A 

 r `

 

 h 9

7~ | 83}{`aXxvsq u zw yw u t r

 a

 u w  t  | ~ u { u w ~ y    ~   v y |  v   u v    y  ~} | w} w ~ y   X  ~} w  u v { |  { v | 0jex Acg0xc11gpjx' !cgCcw          5  ~   y u  ~ u | w}  u u t | u  { w w  u  z  v u ~ u  ~} w | ~ u  u v y  v u t | v { CpC00I0C6nxAgpCc5ugAxexCuxxjgcj
  

du

 D

) 34aD  
 D

) 32    ) 0    # 

iu iu

 

 IS3   IS3 

 

du

) 1

 aD

 ' yw r u r % z v(t{&$z 

!~ | "3}{`aXxvsq u zw yw u t r





H

  |}z  { | ~ u  u ~ u } wpgexAx  |  z y t ~   t } t  w ~ y} |}  ~ y  | ~ u v u }  zz  v y  u | } | ~  | w ~} u  y | w  u u ~ t } t 6pjjpx6excx hxgigegcgujxxjp   u t   y}   ~  zz  { |   w  y}    ~}  yzz y u t s Aw pp0gC1p} p%jpp2j6  j pAxcAgep} ~  y ~ y} |  u w v u | ~ u t | |  v z { } | v   ~} z y t  y w ~ y} |}  ~ y  u t   |}z  { | ~ u  u  ~}   z v u  y  ~  v y urijxpcpp rpgA| WwpgexAGpAer |0pg%ep5cxjjrpu6nc%et  t |  ~}   w   w} w} t | w w u v   u u  t } t  ~}    u t s    y | v    ~  v y w z y y wz  | ~ u t |   |}z  { | ~ u  u u t | |  w z y t  ~ y} |}  ~ y   } |  t |  ~} |  | w   ~ } xr wp0exC8jgpjx0g%ggcx #p}  u w y  y v  u ~ y u t | y | v z} } w  y}   |} u ~ u  y  y t  u   t zz}  u   v u t | v { xcgnippcpwAxj80rpBrIxxjgcj
  

iu

 



 

 &  

 &d   &d

 

     f dDd&

# & g
 )

    dDd{       

g

             X dD&Ur`{f&ddDWf D&        DW{

  

  &  Dd{    

g

    D3fr`{f&dD  3t         

     aD)S

#g

g f

           D3fr`)U&p3 

 y  r  SScr

 3}{`aXxvsq   ~ | u zw yw u t r

 w u  ~ u v u ~}  ~}  yzz y AxxjxAjp!pp u t | z u}  u   w} w  y u t | y |  ~}  ~ yz u  w   u z y w u v u v  w z   u t | |  t |  ~}  { w w BpxuxpceGgjppxh8xcxcu0h0Gpc
k u s  $tBWXid1Bu V t'S1wu V trrI$qtdfT V tSiw e x  v u Rs   b  v Rs p  x  v Rs Rp xp  v u T e Rs b R d h v u es e R qtXW$aIqXDSdyqdt'Wiqdt'Sqdt'qiiR xe v u s   x  v u Rs p e R  b R  v u Rs p  x  v u Rs p h R  v u T Rs p h Rp x  qUt'qiDD1q`t'qiSyqdt'x v u YT Rs p h R  x v u Rs p R  b b x iWtdfUa`T V USQ T eT T Y RT R

H s

 

H

H 

H 



 3 











 | {  ~}  ~}  yzz y u t | u  { w w  u     uz     u v y  u v y   ~  z  } ~}  u jjpGppnjgjr  pjC60 &pjp | y ~ z { y  z u  y  u t | |  t | |z { w u v u t | t |}   u |   y   y    w} v u}z u  ~  t  u t |  %rA0jxcg8g8xg0xApxpxj0GI}  uz     u ~}  u | }  u v  u  z ~ y ~            v y      w ~ y} | z u v u w v { y  w}  pjjpxAxcG%x2Qscn!Cngpxjxcp u t|  |   ~  s r m| {  m r  n  |  s |     r  AswIyDGzBw G{ &y&s}R| |m }|    DUn {  }&Al vm  r|   s t |} ~ y} | }   v | ~ y   z u}  z { y  w z y t       ~ y} |}  ~ y  u t | t } t  ~}  u |  | w gAgexpApr3 p9 Igjxg81ggc w u w   t | y  ~} u w {   u   u v v u ~} u  ~           v y ~      v u t |} u ~ |  t | w u}z   xcGp6cxxGxcxpuGxQRcCxxj0xpp} u   uz  y Ap~  pu wpiCncxIp  pjCeAp0pCu z v y    uz     u ~   ~} z   u y |  w  u w}  uz     u  w ~ y} |  ~ z   u w u t | ~ u }  u  ~ u | ~ u w | w v  u t | ~}  u ~ y} | ~ u Ag%x AxAn0GpApex jgBgxxpxj0  y y v u t | y | w  ~ yz u  v u}z u  ~  t u t | |  t | ~ y} |   { w w  u t | t |}  | ~ u | w} w ~ y  u v           ~       uz} t  ~ y} 08gjcgexgpcx!2QscCC1p8g|  z u v u w v { y  w}  w   u v v u ~} u  | y ~ ~             y y v u t | y |  ~}  ~ yz u ixccx@xcAjp@QsA2cggpjpA w   u | u v  v u | ~} w} v u}z u  ~  t  u t | |  t |  ~}  { w w   w { t s z  ~ y} |  ~ { w}   |  t AgAcAgep!p!xpxj0Bg8jpcx62Q0pgC5p10g| |   u t | y | u {  ~ y} | }   v | ~ y   ~} w |z { w u v z v  uz  t } t     {|  r AsggG8pAgexB%gAxpx3G&)&  m e m  ~   r m| { w r  s    u  y     ~   DGzytv a|  RGB! j c3 p9u  ea IupAu3G&)$   y v    d    w  zz u  w    {|  r      uy m mj~n  3 p9  ea E0uhCsBwApjx  yv    d     |  t | w z y t |}  |   ~   w ~ y} |}  ~ y v y | }  v | y  u  gCpgge~AB0t  gAgex2%g!AgciBjg{ v   ~   w  ~ y} | }   v | ~ y   y |  ~}   uz v u | | z u t |    { x    pa ltgn m  &)6  $ hDGzB)A'Dm &t$)3&xs i w { | ~             r m| {  r     r s  w r  v u  r PDmqo g}AGr%xut0QsAexAxjx%gc t |  | u u ~ |             u |  u ~ ~ y  u v   u  ~  u |  t | u  { w w   ~} u u w u t | t |}  w  z v u  y  y y v u t | ~}  ~} u  w ~ t y    pAxc5jggjicxcngp1xjuQ}
k     k   

 v      i { y{ x fpa g    k6   $eGv &BDl&t$)3u  w {t|  ~      { s   r m s  w r  v   s m t |}  | u  u ~ u t |         u |  u ~ ~ y  u v    ~  u |  t | u  { w w pxtr o A%u xRCe#AgAAA%hu 0gc   R p9m  Bs p EFj c DGzBzH8y Dm &t&3&xtDt0zB           ~    r m| {  n n  r s  w r  v u  s r m o  | {                       p`!3t`!BtP

 W#  Wamg#s V v f b f X V v p #g V U y  X g#s  fsX f Xs sX  XWa#s  #s  WaX#gs a  9EWXufX s  22 Ca#BuCXf s V  ##gTT fs # V v T  hfg q fs X X bs S    0 @         2    gcCq  f b fs X V v f p fs X V v fsfs X T 4 0    @  tQ  v f           f ~ ha  mus  X6Wamg#s  Xf  #tuus  CS69Bk&kXsgCq  a fs Y Vv f p f Vv f d p a U i a m5s WXa #s s WXCfV 2iCYV IX  9U XY Y igd f b CaCfV  rXq Gv X3a V  I9i U  Ba  V   Was i s 2 gX x f d E9Ui  y #s s T i fp X p pf V S  D  7 &  0 D  Q   A  i % 2 0 & 4 7  4   1 0 D             9I6IB62 I3iBB`ud2 Ci 'W% `C2  WC426gEi 2Cq  i  aV U p f V i Wa#s s gX f 9U p  d b X i v f p X  p  2    p f  cWaCfV 2 tq  GX3a i V 2 IU  i Ba i   V i I Waus i s i  VI i   rEq #s i s I T UUi f S 410D & 4 7    DD  q       CCW4g3gEd2C'W%HC19u3Cq                        IGGxm#g## GuEG#u9G m93$GuCuEy#R93uguu                              gx$kRGxP2xGm##9#EmyGu2kGg$2G B uuiR a }~ f p f s X V v f p f V v f s f #a f cBa #g#s  WXWa#s s XrfV mt2d ba 9CXV v |a V IU 9us V  q i Ea s V   Ra )I{ X j3E9yw i x  r#s  s T on X p X p a X p i p V q zwXU VX S   0 @ 4  i  D   A i 0 & 4 7  & v  A   v 2 u(  t D (  y     f r   x        g 9B32E2gcw 'd2C '2 W% P $i 2BBi C` E BH2 Wg#sgCqI i  0  aa fs X V v f p f Vv V f #Waug#s 9EWXWas s Xrfp2d ba h9CXV v  3a V  IU  9msi V   q i on X p X p a X pka i s  V  lka  V E rs i s  j9g9iH6x)2& C'W%hi Wg2ui e 2    i   2  i p  V X T S    0 @ 4  % 0 & 4 7      f        a a p  iv p aU Vv f paU  Vv f    V X a s V Gs s Xp s s I Xi hp V !   V  V s  d b aIWU` Y  E U p X   p  Ed9X 6a  Ii  V 2  I9U  i 6a i s  V i 2 x  q  i i a i s  i V 2  a  2U I IsU  i s I T   'EHHW92I3R#p  2    S %   D 0   D            aa Y f b a V y i w V v xrvV ie6C2U  qf  B 3xCv)T aaUs q p #9utrfV $BCai CfV ihgecWU`2 I9WU6T Y f fd b a V YXX V U S & 7 % 4 Q   D  &  %  &  D 0 & D 4  A  0 ( @ 8 & 7 % 4  1 0 ( & %i   "         I'60R3PI6GFHGFE3)2C2BB699'60532 2)'$#! 















 PPb ySEFHt`YHFT   a  R X  D   Q  @   X @ X D V   6 r  b r  r r        rf  b    f b       r  6  @     X w     @   6     zcjgqgiqwrs#j qB rgjFW3y9)~3c 4 d  3  Pe tPHdfd`pPbdigipsjiij'iiigch6p Gi8 q h e e  V V 6  a a  6 d hf dh q  bh    b hf f q    w    b hf h   d d t r f   bhf r r f  6 r   r   b  r h d  r r   b  d b hf  r  r d  q   dh  6   s    u   @   6     @ R  @ Q 4 FIAxqi5eigqcgbyHssR3BYvda 3  a p a  6      V w 6  rh   df bh  `Ebbyd pp f b r  r h     d hf b   r y f  6 r  b r  r r        rf  b  df b f d bh y df b r   6 p 6 V  @  4 gqgu  gingjFBzcjgqgiqwrgsppI`s7| 3  q z|ta e fd6 PPb s gH3t39yESydYH c h  VV gaa 7  R     X    @ R  X  6 b hf  q    d  b hf f r   rf b a 6  X  @      @ X  V V    6  m R X w   6 8 6  j j  p 4 fiqxiwsgsc'FYBkU3f$n3YSxbfPHdtp 3  ehpc VV dd ca d bEdf6  P`q  y t`Wg#BfEv`dPfEfig5c   X @   I     @   @ D   @ Q 6 R   Q I   @   D  8 6 b hf  q h    dh  r d  q   dh bh  bh b  d  r  rf   f q   r d q r f d      6   j j       @  T 6 X     @ B #pxqq sne pxw8byf''~y3B|4 re3  d hga  VV ddd ca `q tPbdf6 PP`q   X @ X D V   T   @   @ D   @ Q I   @   D  t`YHFUBfEsPfE8 6 b hf  r b r   r     bh d q d rfh b r   bhf r   rf b a 6  @      fi jggC8si)WpiqscFhhj |'Uy3B|e 3  @  T 6X @ B 4  `q idfPEbd3~}ySH|U{dzgUiiq'xpqv d c h   c  V V p p a  6      X           j  @ T 6 r  b rh  y r hfh b   w bh d  bh   r y  bh  bh   6    X   6    @      @   @ Qk     s  8   f  6  bh  h  x 6 p 6 m  @ T 4 xe!93HF9u5ktgtsrE)GF`PpCiuqYBS`c 3
   

I        j g R V g w   g    s D    @ m    jI     g f V X X s e d d  s D    @ xrVoyHGCH9B`nlkiDdSi@!55Phg7y6  EP`q  H9B I  X       6      X D X X    6 X   V    @ R  s R   6 r   r   b       b  d b hf  r Py'dE9YSYdSb)IAGeigq r d  q   dh  y d r  h  b r r f r  b hf  r  r f  f b q      r  f d   t 6  6   @  T 4 cgg icWigqBp 5c  g))9Sg 3   PPb   ySyW3cxPfgEdEpc3BYvfHftyY'|4 rq3 aaa !  R X  @   w I   @   D  8 6  bh  h  x 6     @ R  @ Q     @  u 6   s    a p e h  g e  V V c a a     X @ X D V   T R   c`EipPfdHd6  Pb  0t`YHWiUSQ I   @   D  8 6 h         rf bh df b r  r  b  d b hf  t 6    D     B   @  8 6    4 P5HEG hwrgsp1pgeiiCF9Eyi CA973t'|5 3
( % 7 % & % 0 %



 



 

9

of i  ) 2  v y u t | z  }  yz  t  { w  y w | ~ u}  u v  ~} u t | | { xppj6gexxcpn6jy  ~}zz u w ~} w u} ~ y} | {  v | ~ y  {  pxp6Appz2gj}xv R 2cAg&Ap ~} w  v y u t | z  }  yz  v y z u  y  z  } ~}   ~} z {   y |   y v    w  u  v u   ~  } |  z  u  w}  y ~ y} |   v y ~} w} t p}C{ewxxxu8nuggvpxxjU0 gjp6g| t   y v   v { y ~    uz  ecuwuxy~  pcvrgp{jppzrpzAupiAjxcp z y   ~ } | w}  ~} w  z u  w  w ~ y} | z u v u w v { y  w}  ~ y} |  | {  y  u t | ~ y} | {z w u v ~  | v u  ~ u u  u yggjxpjpyxc1p}gcxA5xAuw|C  ypjgeAx y ~ } |   v y ~} u t | | ~ { y   y | ~  ~  | y} | {z y w v u  ~ u u u v ~}  } v y | t  y v  ge}}  ~pgpcxuxAvxC2ppcgej~AgexcxjB0 u| ~uwuv  u  t u f kp m o f rj c rk 1 0
( $4

  r m| { w r  s| q0)D}zttv alw
 H 

m r

  ~    n  & )%
 

# ({

v

  { de{ 
{


v y


  { I3G&)$       {|  r


m

m 



n 

{

v

{

{ G|

r $

m

m l

 f







   uy   y| u {    a  d   )G  njr     uea  h p9m  a 'ra       ~      d    t |}  ~ y} | }   v | ~ y   z u}  w z u}  t } t       & g%pApjgxpAppApp '% $ e  PI3 #       p)    |5 s    "ea $ea  R p9m "ea ea  uQ   uy      d  d         d   d    } d a   d   }v {  |v y w ~ v|  y u  y  u t| t }   u| z uvv y  z v y   u  }|  w eCPea ~cg6|xiAxnzgx| yg0cIpw} ~ t y  |  t | |   u t | y | u {  w}   ~ u | w} w ~ y  ~} w} t s  " | v y  w ~  v |  y u  y  u t | t |} u0As65jp1Ajxgpcxjpp6%g61  zu  u t| wu }| ~u } t } t  zu  y  vuu }  ~u| }w ~ y } ~ |u  u  w { t pwxxguA0ex2xBcxA~BAjxgpwcxjp~Crj6s
 u V

i G

  76'    1 s 
s

e

h 
 Q



E! E E
f g

u s

 7

v 

 & s
u

d  R v uT Y eywUa`T V XdXyh&$wUa`dte s e e vR d   v u T YT s e 

u

   #"!  Q
u

$  j  1

ueT `UcT V V XdtyqXfa`T V V Xdtywf'WWUBdb  Rs e e v ueT Y Rs e e vu s p b   h x

 Rd ad$
R eRxb WWDp



   cD  
s

VV R

 V

R



     $  R    98 #D! ' 
u R ywUa`T V V tSrp v uT Y Rs 

 V R E
V V SQ R

u

d 

V V yqWX{fd R v u Rs R b

  V V

E E
 t 



!DQ    D v Q& 28   ! '    # s R    v      78'sCD# y"s              

upe   R b cfWdUT V UXcWd(BWDDdWawUT V faXcWdd RT Rse R b d    u h p x b b x h RT Rse R b

u

u &88Q  8 y  D w"s u V UtWrWwqW'ad E RT Rs d Rp e R h h  e       x  v u uTe Rs e       7  s UdfT V Xdte  Q   7 U{tT V Xdte E    u T  Rs e 


Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 1??, Prague, Czech Republic, June 2007 c2007 Association for Computational Linguistics A Linguistic Investigation into Unsupervised DOP Rens Bod School of Computer Science University of St Andrews ILLC, University of Amsterdam rb@cs.st-and.ac.uk Abstract Unsupervised Data-Oriented Parsing models (U-DOP) represent a class of structure bootstrapping models that have achieved some of the best unsupervised parsing results in the literature.
While U-DOP was originally proposed as an engineering approach to language learning (Bod 2005, 2006a), it turns out that the model has a number of properties that may also be of linguistic and cognitive interest.
In this paper we will focus on the original U-DOP model proposed in Bod (2005) which computes the most probable tree from among the shortest derivations of sentences.
We will show that this U-DOP model can learn both rule-based and exemplar-based aspects of language, ranging from agreement and movement phenomena to discontiguous contructions, provided that productive units of arbitrary size are allowed.
We argue that our results suggest a rapprochement between nativism and empiricism.
1 Introduction
This paper investigates a number of linguistic and cognitive aspects of the unsupervised data-oriented parsing framework, known as U-DOP (Bod 2005, 2006a, 2007).
U-DOP is a generalization of the DOP model which was originally proposed for supervised language processing (Bod 1998).
DOP produces and analyzes new sentences out of largest and most probable subtrees from previously analyzed sentences.
DOP maximizes what has been called the ?structural analogy??between a sentence and a corpus of previous sentence-structures (Bod 2006b).
While DOP has been successful in some areas, e.g. in ambiguity resolution, there is also a serious shortcoming to the approach: it does not account for the acquisition of initial structures.
That is, DOP assumes that the structures of previous linguistic experiences are already given and stored in a corpus.
As such, DOP can at best account for adult language use and has nothing to say about language acquisition.
In Bod (2005, 2006a), DOP was extended to unsupervised parsing in a rather straightforward way.
This new model, termed U-DOP, again starts with the notion of tree.
But since in language learning we do not yet know which trees should be assigned to initial sentences, it is assumed that a language learner will initially allow (implicitly) for all possible trees and let linguistic experience decide which trees are actually learned.
That is, U-DOP generates a new sentence by reconstructing it out of the largest possible and most frequent subtrees from all possible (binary) trees of previous sentences.
This has resulted in state-of-theart performance for English, German and Chinese corpora (Bod 2007).
Although we do not claim that U-DOP provides any near-to-complete theory of language acquisition, we intend to show in this paper that it can learn a variety of linguistic phenomena, some of which are exemplar-based, such as idiosyncratic constructions, others of which are typically viewed as rule-based, such as auxiliary fronting and subject-verb agreement.
We argue that U-DOP can be seen as a rapprochement between nativism and empiricism.
In particular, we argue that there is a fallacy in the argument that for syntactic facets to be learned they must be either innate or in the input data: they can just as well emerge from an analogical process without ever hearing the particular facet and without assuming that it is hard-wired in the mind.
In the following section, we will start by reviewing the original DOP framework in Bod (1998).
In section 3 we will show how DOP can be 1 generalized to language learning, resulting in U-DOP.
Next, in section 4, we show that the approach can learn idiosyncratic constructions.
In section 5 we discuss how U-DOP can learn agreement phenomena, and in section 6 we extend our argument to auxiliary movement.
We end with a conclusion.
2 Review
of ?supervised??DOP In its original version, DOP derives new sentences by combining subtrees from previously derived sentences.
One of the main motivations behind the DOP framework was to integrate rule-based and exemplarbased aspects of language processing (Bod 1998).
A simple example may illustrate the approach.
Consider an extremely small corpus of only two phrase-structure trees that are labeled by traditional categories, shown in figure 1.
the NPP on rack PP the NP dress NP V wanted VP NP she S the NPP with telescope PP the NP saw dog VP V VP NP she S Figure 1.
An extremely small corpus of two trees A new sentence can be derived by combining subtrees from the trees in the corpus.
The combination operation between subtrees is called label substitution, indicated as .
Starting out with the corpus of figure 1, for instance, the sentence She saw the dress with the telescope may be derived as shown in figure 2.
the NPP with telescope PP NP saw VP V VP NP she S the NP dress = the NPP with telescope PP the NP saw VP V VP NP she S dress Figure 2.
Analyzing a new sentence by combining subtrees from figure 1 We can also derive an alternative tree structure for this sentence, namely by combining three (rather than two) subtrees from figure 1, as shown in figure 3.
We will write (t  u)  v as t  u  v with the convention that  is left-associative.
PP the NP dress NP V VP NP she S saw V the NPP with telescope PP =  PP the NP dress NP V VP NP she S saw the NPP with telescope Figure 3.
A different derivation for the same sentence DOP?s subtrees can be of arbitrary size: they can range from context-free rewrite rules to entire sentence-analyses.
This makes the model sensitive to multi-word units, idioms and other idiosyncratic constructions, while still maintaining full productivity.
DOP is consonant with the view, as expressed by certain usage-based and constructionist accounts in linguistics, that any string of words can function as a construction (Croft 2001; Tomasello 2003; Goldberg 2006; Bybee 2006).
In DOP such constructions are formalized as lexicalized subtrees, which form the productive units of a Stochastic TreeSubstitution Grammar or STSG.
Note that an unlimited number of sentences can be derived by combining subtrees from the corpus in figure 1.
However, virtually every sentence generated in this way is highly ambiguous, yielding several syntactic analyses.
Yet, most of these analyses do not correspond to the structure humans perceive.
Initial DOP models proposed an exclusively probabilistic metric to rank different candidates, where the ?best??tree was computed from the frequencies of subtrees in the corpus (see Bod 1998).
While it is well known that the frequency of a structure is a very important factor in language comprehension and production (Jurafsky 2003), it is not the only factor.
Discourse context, semantics and recency also play an important role.
DOP can straightforwardly take into account discourse and semantic information if we have corpora with such information from which we take our subtrees, and the notion of recency can be incorporated by a frequencyadjustment function (Bod 1998).
There is, however, an important other factor which does not correspond to the notion of frequency: this is the simplicity of a structure (cf.
Frazier 1978; Chater 1999).
In Bod (2002), the simplest structure was formalized by the shortest derivation of a sentence consisting of the fewest subtrees from the corpus.
Note that the shortest derivation will include the largest possible subtrees from the corpus, thereby maximizing the structural overlap between a sentence and previous sentence2 structures.
Only in case the shortest derivation is not unique, the frequencies of the subtrees are used to break ties among the shortest derivations.
This DOP model assumes that language users maximize what has been called the structural analogy between a sentence and previous sentence-structures by computing the most probable tree with largest structural overlaps between a sentence and a corpus.
We will use this DOP model from Bod (2002) as the basis for our investigation of unsupervised DOP.
We can illustrate DOP?s notion of structural analogy with the examples given in the figures above.
DOP predicts that the tree structure in figure 2 is preferred because it can be generated by just two subtrees from the corpus.
Any other tree structure, such as in figure 3, would need at least three subtrees from the training set in figure 1.
Note that the tree generated by the shortest derivation indeed tends to be structurally more similar to the corpus (i.e.
having a larger overlap with one of the corpus trees) than the tree generated by the longer derivation.
Had we restricted the subtrees to smaller sizes -for example to depth-1 subtrees, which makes DOP equivalent to a (probabilistic) context-free grammar -the shortest derivation would not be able to distinguish between the two trees in figures 2 and 3 as they would both be generated by 9 rewrite rules.
When the shortest derivation is not unique, we use the subtree frequencies to break ties.
The ?best tree??of a sentence is defined as the most probable tree generated by a shortest derivation of the sentence, also referred to as ?MPSD??
The probability of a tree can be computed from the relative frequencies of its subtrees, and the definitions are standard for Stochastic Tree-Substitution Grammars (STSGs), see e.g.
Manning and Schtze (1999) or Bod (2002).
Interestingly, we will see that the exact computation of probabilities is not necessary for our arguments in this paper.
3 U-DOP: from sentences to structures DOP can be generalized to language learning by using the same principle as before: language users maximize the structural analogy between a new sentence and previous sentence-structures by computing the most probable shortest derivation.
However, in language learning we cannot assume that the correct phrase-structures of previously heard sentences are already known.
Bod (2005) therefore proposed the following generalization of DOP, which we will simply refer to as U-DOP: if a language learner does not know which syntactic tree should be assigned to a sentence, s/he initially allows (implicitly) for all possible trees and let linguistic experience decide which is the ?best??tree by maximizing structural analogy (i.e.
the MPSD).
Although several alternative versions of UDOP have been proposed (e.g.
Bod 2006a, 2007), we will stick to the computation of the MPSD for the current paper.
Due to its use of the shortest derivation, the model?s working can often be predicted without any probabilistic computations, which makes it especially apt to investigate linguistic facets such as auxiliary fronting (see section 6).
From a conceptual perspective we can distinguish three learning phases under U-DOP, which we shall discuss in more detail below.
(i) Assign all unlabeled binary trees to a set of sentences Suppose that a language learner hears the following two (?Childes-like?? sentences: watch the dog and the dog barks.
How could a rational learner figure out the appropriate tree structures for these sentences?
UDOP conjectures that a learner does so by allowing any fragment of the heard sentences to form a productive unit and to try to reconstruct these sentences out of most probable shortest combinations.
Consider the set of all unlabeled binary trees for the sentences watch the dog and the dog barks given in figure 4.
Each node in each tree is assigned the same category label X, since we do not (yet) know what label each phrase will receive.
watch the dog X X X watch the dog X the dog X X barks X X the dog barks Figure 4.
The unlabeled binary tree set for watch the dog and the dog barks Although the number of possible binary trees for a sentence grows exponentially with sentence length, these binary trees can be efficiently represented by means of a chart or tabular diagram.
By adding pointers between the nodes we obtain a structure 3 known as a shared parse forest (Billot and Lang 1989).
(ii) Divide the binary trees into all subtrees Figure 5 exhaustively lists the subtrees that can be extracted from the trees in figure 4.
The first subtree in each row represents the whole sentence as a chunk, while the second and the third are ?proper??subtrees.
watch the dog X X dog X X watch the X X watch the dog X X watch X the dog X the dog X X barks X X barks the dog X X X the dog barks X X the X dog barks Figure 5.
The subtree set for the binary trees in figure 4.
Note that while most subtrees occur once, the subtree [the dog]X occurs twice.
There exist efficient algorithms to convert all subtrees into a compact representation (Goodman 2003) such that standard best-first parsing algorithms can be applied to the model (see Bod 2007).
(iii) Compute the ?best??tree for each sentence Given the subtrees in figure 5, the language learner can now induce analyses for a sentence such as the dog barks in various ways.
The phrase structure [the [dog barks]X]X can be produced by two different derivations, either by selecting the large subtree that spans the whole sentence or by combining two smaller subtrees: X X the dog barks or X X the X dog barks o Figure 6.
Deriving the dog barks from figure 5 Analogously, the competing phrase structure [[the dog]X barks]X can also produced by two derivations: the dog X X barks or X X barks the dog Xo Figure 7.
Other derivations for the dog barks Note that the shortest derivation is not unique: the sentence the dog barks can be trivially parsed by any of its fully spanning trees.
Such a situation does not usually occur when structures for new sentences are learned, i.e. when we induce structures for a held-out test set using all subtrees from all possible trees assigned to a training set.
For example, the shortest derivation for the new ?sentence??watch dog barks is unique, given the set of subtrees in figure 5.
But in the example above we need subtree frequencies to break ties, i.e.
U-DOP computes the most probable tree from among the shortest derivations, the MPSD.
The probability of a tree is compositionally computed from the frequencies of its subtrees, in the same way as in the supervised version of DOP (see Bod 1998, 2002).
Since the subtree [the dog]X is the only subtree that occurs more than once, we can predict that the most probable tree corresponds to the structure [[the dog]X barks]X in figure 7 where the dog is a constituent.
This can also be shown formally, but a precise computation is unnecessary for this example.
4 Learning
constructions by U-DOP For the sake of simplicity, we have only considered subtrees without lexical labels in the previous section.
Now, if we also add an (abstract) label to each word in figure 4, then a possible subtree would the subtree in figure 9, which has a discontiguous yield watch X dog, and which we will therefore refer to as a ?discontiguous subtree??
X watch dog X X X X Figure 9.
A discontiguous subtree Thus lexical labels enlarge the space of dependencies covered by our subtree set.
In order for U-DOP to 4 take into account both contiguous and non-contiguous patterns, we will define the total tree-set of a sentence as the set of all unlabeled trees that are unary at the word level and binary at all higher levels.
Discontiguous subtrees, like in figure 9, are important for acquiring a variety of constructions as in (1)-(4): (1) Show me the nearest airport to Leipzig.
(2) BA carried more people than cargo in 2005.
(3) What is this scratch doing on the table?
(4) Don?t take him by surprise.
These constructions have been discussed at various places in the literature, and all of them are discontiguous in that the constructions do not appear as contiguous word strings.
Instead the words are separated by ?holes??which are sometimes represented by dots as in more ??than ?? or by variables as in What is X doing Y (cf.
Kay and Fillmore 1999).
In order to capture the syntactic structure of discontiguous constructions we need a model that allows for productive units that can be partially lexicalized, such as subtrees.
For example, the construction more...
than ??in (2) can be represented by a subtree as in figure 10.
more than XX X X X X X Figure 10.
Discontiguous subtree for more...than...
U-DOP can learn the structure in figure 10 from a few sentences only, using the mechanism described in section 3.
While we will go into the details of learning discontiguous subtrees in section 5, it is easy to see that U-DOP will prefer the structure in figure 10 over a structure where e.g.
[X than] forms a constituent.
First note that the substring more X can occur at the end of a sentence (in e.g.
Can I have more milk?), whereas the substring X than cannot occur at the end of a sentence.
This means that [more X] will be preferred as a constituent in [more X than X].
The same is the case for than X in e.g.
A is cheaper than B.
Thus both [more X] and [than X] can appear separately from the construction and will win out in frequency, which means that U-DOP will learn the structure in figure 10 for the construction more ?? than ??
Once it is learned, (supervised) DOP enforces the application of the subtree in figure 10 whenever a new form using the construction more ...
than ...
is perceived or produced because the particular subtree will directly cover it and lead to the shortest derivation.
5 Learning
agreement by U-DOP Discontiguous context is important not only for learning constructions but also for learning various syntactic regularities.
Consider the following sentence (5): (5) Swimming in rivers is dangerous How can U-DOP deal with the fact that human language learners will perceive an agreement relation between swimming and is, and not between rivers and is?
We will rephrase this question as follows: which sentences must be perceived such that U-DOP can assign as the best structure for swimming in rivers is dangerous the tree 16(a) which attaches the constituent is dangerous to swimming in rivers, and not an incorrect tree like 16(b) which attaches is dangerous to rivers?
Note that tree (a) correctly represents the dependency between swimming and is dangerous, while tree (b) misrepresents a dependency between rivers and is dangerous.
swimming is X X X X X X X in rivers X X dangerous swimming is XX X X X X X in rivers X X dangerous (a) (b) Figure 16.
Two possible trees for Swimming in rivers is dangerous It turns out that we need to observe only one additional sentence to overrule tree (b), i.e. sentence (6): (6) Swimming together is fun The word together can be attached either to swimming or to is fun, as illustrated respectively by 17(a) and 17(b) (of course, together can also be attached to is alone, and the resulting phrase together is to fun, but our argument still remains valid): 5 swimming is X X X X X X X together fun swimming is XX X X X XX together fun (a) (b) Figure 17.
Two possible trees for Swimming together is fun First note that there is a large common subtree between 16(a) and 17(a), as shown in figure 18.
swimming is X X X X X X X Figure 18.
Common subtree in the trees 16(a) and 17(a) Next note that there is not such a large common subtree between 16(b) and 17(b).
Since the shortest derivation is not unique (as both trees can be produced by directly using the largest tree from the binary tree set), we must rely on the frequencies of the subtrees.
It is easy to see that the trees 16(a) and 17(a) will overrule respectively 16(b) and 17(b), because 16(a) and 17(a) share the largest subtree.
Although 16(b) and 17(b) also share subtrees, they cover a smaller part of the sentence than does the subtree in figure 18.
Next note that for every common subtree between 16(a) and 17(a) there exists a corresponding common subtree between 16(b) and 17(b) except for the common subtree in figure 18 (and one of its sub-subtrees by abstracting from swimming).
Since the frequencies of all subtrees of a tree contribute to its probability, if follows that figure 18 will be part of the most probable tree, and thus 16(a) and 17(a) will overrule respectively 16(b) and 17(b).
However, our argument is not yet complete: we have not yet ruled out another possible analysis for swimming in rivers is dangerous where in rivers forms a constituent together with is dangerous.
Interestingly, it suffices to perceive a sentence like (7): He likes swimming in river.
The occurrence of swimming in rivers at the end of this sentence will lead to a preference for 16(a) because it will get a higher frequency as a group.
An implementation of U-DOP confirmed our informal argument.
We conclude that U-DOP only needs three sentences to learn the correct tree structure displaying the dependency between the subject swimming and the verb is, known otherwise as ?agreement??
Once we have learned the correct structure for subject-verb agreement by the subtree in figure 18, (U-)DOP enforces agreement by the shortest derivation.
It can also be shown that U-DOP still learns the correct agreement if sentences with incorrect agreement, like *Swimming in rivers are dangerous, are heard as long as the correct agreement has a higher frequency than the incorrect agreement during the learning process.
6 Learning
?movement??by U-DOP We now come to what is often assumed to be the greatest challenge for models of language learning, and what Crain (1991) calls the ?parade case of an innate constraint?? the problem of auxiliary movement, also known as auxiliary fronting or inversion.
Let?s start with the typical examples, which are similar to those used in Crain (1991), MacWhinney (2005), Clark and Eyraud (2006) and many others: (8) The man is hungry If we turn sentence (8) into a (polar) interrogative, the auxiliary is is fronted, resulting in sentence (9).
(9) Is the man hungry?
A language learner might derive from these two sentences that the first occurring auxiliary is fronted.
However, when the sentence also contains a relative clause with an auxiliary is, it should not be the first occurrence of is that is fronted but the one in the main clause, as shown in sentences (11) and (12).
(11) The man who is eating is hungry (12) Is the man who is eating hungry?
There is no reason that children should favor the correct auxiliary fronting.
Yet children do produce the correct sentences of the form (12) and rarely if ever of the form (13) even if they have not heard the correct form before (see Crain and Nakayama 1987).
(13) *Is the man who eating is hungry?
How can we account for this phenomenon?
According to the nativist view, sentences of the type 6 in (12) are so rare that children must have innately specified knowledge that allows them to learn this facet of language without ever having seen it (Crain and Nakayama 1987).
On the other hand, it has been claimed that this type of sentence is not rare at all and can thus be learned from experience (Pullum and Scholz 2002).
We will not enter the controversy on this issue, but believe that both viewpoints overlook a very important alternative possibility, namely that auxiliary fronting needs neither be innate nor in the input data to be learned, but may simply be an emergent property of the underlying model.
How does (U-)DOP account for this phenomenon?
We will show that the learning of auxiliary fronting can proceed with only two sentences: (14) The man who is eating is hungry (15) Is the boy hungry?
Note that these sentences do not contain an example of the fact that an auxiliary should be fronted from the main clause rather than from the relative clause.
For reasons of space, we will have to skip the induction of the tree structures for (14) and (15), which can be derived from a total of six sentences using similar reasoning as in section 5, and which are given in figure 20a,b (see Bod forthcoming, for more details and a demonstration that the induction of these two tree structures is robust).
is X X X X X X is eating X X hungry X the man X X X who X X X X X X the boy X X is hungry (a) (b) Figure 20.
Tree structures for the man who is eating is hungry and is the boy hungry? learned by U-DOP Given the trees in figure 20, we can now easily show that U-DOP?s shortest derivation produces the correct auxiliary fronting, without relying on any probability calculations.
That is, in order to produce the correct interrogative, Is the man who is eating hungry, we only need to combine the following two subtrees from the acquired structures in figure 20, as shown in figure 21 (note that the first subtree is discontiguous): X X X X X is hungry X X is eating X X X the man X X X who X o Figure 21.
Producing the correct auxiliary fronting by combining two subtrees from figure 20 On the other hand, to produce the sentence with incorrect auxiliary fronting *Is the man who eating is hungry? we need to combine many more subtrees from figure 20.
Clearly the derivation in figure 21 is the shortest one and produces the correct sentence, thereby blocking the incorrect form.1 Thus the phenomenon of auxiliary fronting needs neither be innate nor in the input data to be learned.
By using the notion of shortest derivation, auxiliary fronting can be learned from just a couple sentences only.
Arguments about frequency and ?poverty of the stimulus??(cf.
Crain 1991; MacWhinney 2005) are therefore irrelevant ?? provided that we allow our productive units to be of arbitrary size.
(Moreover, learning may be further eased once the syntactic categories have been induced.
Although we do not go into category induction in the current paper, once unlabeled structures have been found, category learning turns out to be a relatively easy problem).
Auxiliary fronting has been previously dealt with in other probabilistic models of structure learning.
Perfors et al.(2006) show that Bayesian model selection can choose the right grammar for auxiliary fronting.
Yet, their problem is different in that Perfors et al.start from a set of given grammars from which their selection model has to choose the correct one.
Our approach is more congenial to Clark and Eyraud (2006) who show that by distributional analysis in the vein of Harris (1954) auxiliary fronting can be correctly predicted.
However, different from Clark and Eyraud, we have shown that U-DOP can also learn complex, discontiguous constructions.
In order to learn both rule-based phenomena like auxiliary inversion and exemplar-based phenomena like idiosyncratic constructions, we believe we need 1 We are implicitly assuming here an extension of DOP which computes the most probable shortest derivation given a certain meaning to be conveyed.
This semantic DOP model was worked out in Bonnema et al.(1997) where the meaning of a sentence was represented by its logical form.
7 the richness of a probabilistic tree grammar rather than a probabilistic context-free grammar.
7 Conclusion
We have shown that various syntactic phenomena can be learned by a model that only assumes (1) the notion of recursive tree structure, and (2) an analogical matching algorithm which reconstructs a new sentence out of largest and most frequent fragments from previous sentences.
The major difference between our model and other computational learning models (such as Klein and Manning 2005 or Clark and Eyraud 2006) is that we start with trees.
But since we do not know which trees are correct, we initially allow for all of them and let analogy decide.
Thus we assume that the language faculty (or ?Universal Grammar?? has knowledge about the notion of tree structure but no more than that.
Although we do not claim that we have developed any near-to-complete theory of all language acquisition, our argument to use only recursive structure as the core of language knowledge has a surprising precursor.
Hauser, Chomksy and Fitch (2002) claim that the core language faculty comprises just ?recursion??and nothing else.
If one takes this idea seriously, then U-DOP is probably the first fully computational model that instantiates it: UDOP?s trees encode the ultimate notion of recursion where every label can be recursively substituted for any other label.
All else is analogy.
References Billot, S.
and B.
Lang, 1989.
The Structure of Shared Forests in Ambiguous Parsing.
Proceedings ACL 1989.
Bod, R.
1998. Beyond Grammar.
Stanford: CSLI Publications.
Bod, R.
2002. A Unified Model of Structural Organization in Language and Music, Journal of Artificial Intelligence Research, 17, 289-308.
Bod, R.
2005. Combining Supervised and Unsupervised Natural Language Processing.
The 16th Meeting of Computational Linguistics in the Netherlands (CLIN 2005).
Bod, R.
2006a. An All-Subtrees Approach to Unsupervised Parsing.
Proceedings ACL-COLING 2006, 865-872.
Bod, R.
2006b. Exemplar-Based Syntax: How to Get Productivity from Examples.
The Linguistic Review 23, 291-320.
Bod, 2007.
Is the End of Supervised Parsing in Sight?.
Proceedings ACL 2007, Prague.
Bod, forthcoming.
From Exemplar to Grammar: How Analogy Guides Language Acquisition.
In J.
Blevins and J.
Blevins (eds).
Analogy in Grammar, Oxford University Press.
Bonnema, R., R.
Bod and R.
Scha, 1997.
A DOP Model for Semantic Interpretation.
Proceedings ACL/EACL 1997, Madrid, Spain, 159-167.
Bybee, J.
2006. From Usage to Grammar: The Mind?s Response to Repetition.
Language 82(4), 711-733.
Chater, N.
1999. The Search for Simplicity: A Fundamental Cognitive Principle?
The Quarterly Journal of Experimental Psychology, 52A(2), 273-302.
Clark, A.
and R.
Eyraud, 2006.
Learning Auxiliary Fronting with Grammatical Inference.
Proceedings CONLL 2006, New York.
Crain, S.
1991. Language Acquisition in the Absence of Experience.
Behavorial and Brain Sciences 14, 597-612.
Crain, S.
and M.
Nakayama, 1987.
Structure Dependence in Grammar Formation.
Language 63, 522-543.
Croft, B.
2001. Radical Construction Grammar.
Oxford University Press.
Frazier, L.
1978. On Comprehending Sentences: Syntactic Parsing Strategies.
PhD. Thesis, U.
of Connecticut.
Goldberg, A.
2006. Constructions at Work: the nature of generalization in language.
Oxford University Press.
Goodman, J.
2003. Efficient algorithms for the DOP model.
In R.
Bod, R.
Scha and K.
Sima'an (eds.).
Data-Oriented Parsing, CSLI Publications, 125-146.
Harris, Z.
1954. Distributional Structure.
Word 10, 146-162.
Hauser, M., N.
Chomsky and T.
Fitch, 2002.
The Faculty of Language: What Is It, Who Has It, and How Did It Evolve?, Science 298, 1569-1579.
Jurafsky, D.
2003. Probabilistic Modeling in Psycholinguistics.
In Bod, R., J.
Hay and S.
Jannedy (eds.), Probabilistic Linguistics, The MIT Press, 39-96.
Kay, P.
and C.
Fillmore 1999.
Grammatical constructions and linguistic generalizations: the What's X doing Y? construction.
Language, 75, 1-33.
Klein, D.
and C.
Manning 2005.
Natural language grammar induction with a generative constituent-context model.
Pattern Recognition 38, 1407-1419.
MacWhinney, B.
2005. Item-based Constructions and the Logical Problem.
Proceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, Ann Arbor.
Manning, C.
and H.
Schtze 1999.
Foundations of Statistical Natural Language Processing.
The MIT Press.
Perfors, A., Tenenbaum, J., Regier, T.
2006. Poverty of the Stimulus?
A rational approach.
Proceedings 28th Annual Conference of the Cognitive Science Society.
Vancouver Pullum, G.
and B.
Scholz 2002.
Empirical assessment of stimulus poverty arguments.
The Linguistic Review 19, 9-50.
Tomasello, M.
2003. Constructing a Language.
Harvard University Press .
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 9??6, Prague, Czech Republic, June 2007 c2007 Association for Computational Linguistics Using Classifier Features for Studying the Effect of Native Language on the Choice of Written Second Language Words Oren Tsur Institute of Computer Science The Hebrew University Jerusalem, Israel oren@cs.huji.ac.il Ari Rappoport Institute of Computer Science The Hebrew University Jerusalem, Israel www.cs.huji.ac.il/?arir Abstract We apply machine learning techniques to study language transfer, a major topic in the theory of Second Language Acquisition (SLA).
Using an SVM for the problem of native language classification, we show that a careful analysis of the effects of various features can lead to scientific insights.
In particular, we demonstrate that character bigrams alone allow classification levels of about 66% for a 5-class task, even when content and function word differences are accounted for.
This may show that native language has a strong effect on the word choice of people writing in a second language.
1 Introduction
While advances in NLP achieve improved results for NLP applications such as machine translation, question answering and document summarization, there are other fields of research that can benefit from the methods used by the NLP community.
Second Language Acquisition (SLA), a major area in Applied Linguistics and Cognitive Science, is one such field.
In this paper we demonstrate how modern machine learning tools can contribute to SLA theory.
In particular, we address the major SLA topic of language transfer, the effect of native language on second language learners.
Using an SVM for the computational problem of native language classification, we study in detail the effects of various SVM features.
Surprisingly, character bi-grams alone lead to a classification accuracy of about 66% in a 5-class task, even when accounting for differences in content and function words.
This result leads us to form a novel hypothesis on the role of language transfer in SLA: that the choice of words people make when writing in a second language is strongly influenced by the phonology of their native language.
As far as we know, this is the first time that such a hypothesis has beed formulated.
Moreover, this is the first statistical learning-supported hypothesis in language transfer.
Our results should be further substantiated by additional psycholinguistic and computational experiments; nonetheless, we provide a strong starting point.
The next section provides some essential background.
In Section 3 we describe our experimental setup and feature selection, and in Section 4 we detail an array of variations of experiments for ruling out some possible types of bias that might have affected the results.
In Section 5 we discuss our hypothesis in the context of psycho-linguistic theory.
We conclude with directions for future research.
2 Background
Our hypothesis is tested within an algorithm addressing the practical problem of determining the native language of an anonymous writer writing in a foreign language.
The problem is applicable to different fields, such as language instructing, tailored error correction, security applications and psycholinguistic research.
As background, we start from the somewhat related problem of authorship attribution.
The authorship attribution problem was addressed by lin9 guists and other literary experts trying to pinpoint an anonymous author, such as that of The Federalist Papers (Holmes and Forsyth, 1995).
Traditionally, authorship experts analyzed topics, stylistic idiosyncrasies and personal information about the possible candidates in order to determine an author.
While authorship is usually addressed with deep human inspection of the texts in question, it has already been shown that automatic text analysis based on various stylistic features can identify the gender of an anonymous author with accuracy above 80% (Argamon et al, 2003).
Various papers (Diedrich et al, 2003; Koppel and Schler, 2003; Koppel et al, 2005a; Stamatatos et al, 2004) report relative success in machine based authorship attribution tasks for small sets of known candidates.
Native language detection is a harder problem than the authorship attribution problem, since we wish to characterize the writing style of a set of writers rather than the unique style of a single person.
There are several works presenting nonnative speech recognition and dialect analysis systems (Bouselmi et al, 2005; Bouselmi et al, 2006; Hansen et al, 2004).
However, all those works are based on acoustic signals, not on written texts.
Koppel et al (2005a) report an accuracy of 80% in the task of determining a writer?s native language.
To the best of our knowledge, this is the only published work on automated classification of an author?s native language (along with another version of the paper by the same authors (Koppel et al, 2005b)).
Koppel et al used an SVM (Scholkopf and Smola, 2002) and a combination of features in their system (such as errors analysis and POS-error cooccurrences, as described in section 2.2), but surprisingly, it appears that a very naive set of features achieves a relatively high accuracy.
The character bi-gram frequencies feature performs rather well, and definitely outperforms the intuitive contribution of frequent bigrams in this type of task.
3 Experimental
Setting 3.1 The Corpus The corpus that served for all of the experiments described in this paper is the International Corpus of Learner English (ICLE) (Granger et al, 2002), which was also the one used by Koppel et al (2005a; 2005b).
The corpus was compiled for the purpose of studying the English writing of non-native speakers.
All contributors to the corpus are advanced English students and are roughly the same age.
The corpus is combined from a number of sub-corpora, each containing one native language.
The corpus was assembled in ten years of international collaboration between a number of universities and it contains more than 2 million words of writing by students from 19 different native language backgrounds.
We followed Koppel et al (2005a) and worked on 5 sub-corpora, each containing 238 randomly selected essays by native speakers of the following languages: Bulgarian, Czech, French, Russian and Spanish.
Each of the texts in the corpus was written by a different author and is of length between 500 to 1,000 words.
Each of the sub corpora contains about 180,000 (unique) types, for a total of 886,677 tokens.
Essays in the corpus are of two types: argumentative essays and literature examination papers.
Descriptive, narrative or technical subjects were not included in the corpus.
The literature examination essays were restricted to no more than 25% of each sub-corpus.
Each contributor was requested to fill a learner profile that was used to fine-proof the corpus as needed.
In order to verify our results we used another control corpus containing the Dutch and Italian subcorpora contained in the ICLE instead of the Bulgarian and French ones.
3.2 Document
Representation In the original experiment by Koppel et al (2005a) each document was represented by a numerical vector of 1,035 dimensions.
Each vector entry represented the frequency (relative to the document?s length) of a given feature.
The features were of 4 types: ??400 function words ??200 most frequent letter n-grams ??250 rare POS bi-gram ??185 error types While the first three types of attributes are relatively straightforward, the fourth is more complex.
It represents clusters of families of spelling errors as well as co-occurrences of errors and POS tags.
Document 10 representation is described in detail in (Koppel et al, 2005a; Koppel et al, 2005b).
A multi-class SVM (Witten and Frank, 2005) was employed for learning and evaluating the classification model.
The experiment was run in a 10-fold cross validation manner in order to test the effectiveness of the model.
3.3 Previous
Results Koppel et al (2005a) report that when all features types were used in tandem, an accuracy of 80.2% was achieved.
In the discussion section they analyze the frequency of a few function words, error types, the co-occurrences of POS tags and errors, and the co-occurrences of POS tags and certain function words that seem to have significance in the support vectors learnt by the SVM.
The goal of their research was to obtain the best classification, therefore the results obtained by using only bi-grams of characters were not particularly noted, although, surprisingly, representing each document by only using the relative frequency of the top 200 characters bi-grams achieves an accuracy of about 66%.
We believe that this surprising fact exposes some fundamental phenomenon of human language behavior.
In the next section we describe a set of experiments designed to isolate the causes of this phenomenon.
4 Experimental
Variations and Results Intuitively, we do not expect the most frequent character n-grams to serve as good native language predictors, expecting that these will only reflect the most frequent English words (and characters sequences).
Accordingly, without language transfer effects, a naive baseline classifier based on an ngram model is expected to achieve about 20% accuracy in a 5 native languages classification task.
However, using classification based on the relative frequency of top 200 bi-grams achieves about 66%1 in all experiments, substantially higher than the random baseline.
These results are so surprising that they suggest that the characters bi-grams classification masks some other bias or noise in the corpus, or, conversely, that it mirrors other simple-to1Koppel et al did not report these results explicitly.
However, they can be roughly estimated from their graph.
Figure 1: Classification accuracy of the different variations of document representation.
b-g: bigrams, f-w: function words, c-w: content words.
explain phenomena such as shallow language transfer through the use of function words, or content bias.
The following sub-sections describe different variations of the experiment, ruling out the effect of these different types of bias.
4.1 Unigram
Baseline We first implemented a naive baseline classifier.
We represented each document by the normalized frequencies of the (de-capitalized) letters it contains2.
These frequencies are simply a unigram model of the sub-corpora.
Using the multi-class SVM (Witten and Frank, 2005) we obtained 46.78% accuracy.
This accuracy is more than twice the random baseline accuracy.
This result is in accordance with our bi-grams results.
Our discussion focuses on bi-grams rather than unigrams because the former?s results are much higher and because bi-grams are much closer to the phonology of the language (for alphabetic scripts, of course).
4.2 Bi-grams Based Classification Choosing the 200 most frequent character bi-grams in the corpus, we used a vector of the same dimension.
Each vector entry contained the normalized frequency of one of the bi-grams.
Using a multiclass SVM in a 10-fold cross validation manner we 2White spaces were considered a letter.
However, sequences of white spaces and tabs were collapsed to a single white space.
All the experiments that make use of character frequencies were performed twice, including and excluding punctuation marks.
Results for both experiments are similar, therefore all the numbers reported in this paper are based on letters and punctuation marks.
11 Bulg.
Czech French Russian Spanish dr 170 183 n/a 195 n/a am 117 135 142 140 152 m 121 120 133 119 139 iv 104 138 144 148 148 y 161 181 196 183 166 la 122 123 122 142 105 Table 1: Some of the separating bi-grams found in the feature selection process.
????indicates a white space.
The numbers are the frequency ranking of the bi-grams in each sub-corpus (e.g., there are 103 bi-grams more frequent than ?iv??in the Bulgarian corpus).
n/a indicates that this bi-gram is not one of the 200 most frequent bi-grams of the sub-corpus.
achieved 65.60% accuracy with standard deviation of 3.99.
The bi-grams features in the 200 dimensional vector are the 200 most frequent bi-grams in the whole corpus, regardless of their frequency in each subcorpus.
We note that the effect of misspelled words on the 200 most frequent bi-grams is negligible.
A more sophisticated feature selection could reduce the dimension of the representation vector without detracting from the results.
Careful feature selection can also give a better intuition regarding the support vectors.
We performed feature selection in the following manner: we chose the top 200 bi-grams of each sub-corpus, getting 245 unique bi-grams in total.
We then chose all the bi-grams that were ranked significantly higher or significantly lower in one language than in at least one other language, assuming that those bi-grams have strong separating power.
With the threshold of significance set to 20 we obtained 84 separating bi-grams.
Table 1 shows some of the separating bi-grams thus found.
For example, ?la??is a good separator between Russian and Spanish (its rank in the Spanish corpus is much higher than that in the Russian corpus), but not between other pairs.
Using only those 84 bigrams we obtained classification accuracy of 61.38%, a drop of only 4% compared to the results achieved with the 200 dimensional vectors.
These results show that increasing the dimension of the representation vector using additional bi-grams contribute a marginal improvement while it does not introduce substantial noise.
4.3 Using
Tri-gram Frequencies as Features Repeating the same experiment with the top 200 trigrams, we obtained an accuracy of 59.67%, which is 40% higher than the expected baseline and 15% higher than the uni-grams baseline.
These results show that the texts in our corpus can be classified by only using naive n-gram models, while the optimal n of the n-gram is a different question that might be addressed in a different work (and might be language-dependent).
4.4 Function
Words Based Classification Function words are words that have a little lexical meaning but instead serve to express grammatical relations within a sentence or specify the attitude of the speaker (function words should not be confused with stopwords, although the lists of most frequent function words and the stopword list share a large subset).
We used the same list of 460 function words used by Koppel et al (2005a).
A partial list includes: {a, afterward, although, because, cannot, do, enter, eventually, fifteenth, hither, hath, hence, lastly, occasionally, presumable, said, seldom, undoubtedly, was}.
In this variation of the experiment, we represented each document only by the relative frequencies of the function words it contained.
Using the same experimental setup as before, we achieved an accuracy of 66.7%.
These results are less surprising than the results obtained by the character n-grams vectors, since we do expect native speakers of a certain language to use, misuse or ignore certain function words as a result from language transfer mechanisms (Odlin, 1989).
For example, it is well known that native speakers of Russian tend to omit English articles.
4.5 Function
Words Bias The previous results suggest that the n-gram based classification is simply the result of the different uses of function words by speakers of different native languages.
In order to rule out the effect of the function words on the bi-gram-based classification, we removed all function words from the corpus, recalculated the bi-gram frequencies and ran the experiment once again, this time achieving an accuracy of 62.92% in the 10-fold cross validation test.
12 These
results, obtained on the function words-free corpus, clearly show that n-gram based classification is not a mere artifact masking the use of function words.
4.6 Content
Bias Bi-gram frequencies could also reflect content bias rather than language use.
By content bias we mean that the subject matter of the documents in the different sub-corpora could exhibit internal sub-corpus uniformity and external sub-corpus disparity.
In order to rule this out, we employed a variation on the Term Frequency ??Inverted Document Frequency (tf-idf ) content analysis metric.
The tf-idf measure is a statistical measure that is used in information retrieval tasks to evaluate how important a word/term is to a document in a collection or corpus (Salton and Buckley, 1988).
Given a collection of documents D, the tf-idf weight of term t in a document d ??D is computed as follows: tfidft = ft,d log |D|f t,D where ft,d is the frequency of term t in document d, and ft,D is the number of documents in which t appears.
Therefore, the weight of term t ??d is maximal if it is a common term in d while the number of documents it appears in is relatively low.
We used the tf-idf weights in the information retrieval sense in order to discover the dominant content words of each sub-corpus.
We treated each subcorpus (set of documents by writers who share a native language) as a single document and calculated the tf-idf of each word.
In order to determine whether there is a content bias or not, we set a dominance threshold, and removed all words such that the difference between their tf-idf score in two different sub-corpora is higher than the dominance threshold.
Given a threshold t, the dominance Dw,t, of a token w is given by: Dw,t = maxi,j|tfidfw,i ?tfidfw,j| where tfidfw,k is the tf-idf score of token w in sub-corpus k.
Changing the threshold in 0.0005 intervals, we removed from 1 to 340 unique content words (between 1,545 and 84,725 word tokens in total).
However, the classification accuracy was essentially the same (see Figure 2), with a slight drop of Word Bulg.
Czech Fr.
Rus. Spa.
europe 0 0.3 2.7 0.2 0.2 european 0 0.3 3 0.1 0.5 imagination 4.3 2 0.8 1 0.8 television 0 3.6 1.9 3.1 0.3 women 0.4 1.7 1.2 5.5 2.6 Table 2: The tf-idf score of some of the most dominant words, multiplied by 1,000 for easier reading.
Subcorpus content function unique words words stems Bulgarian 1543 94685 11325 Czech 2784 110782 12834 French 2059 67016 9474 Russian 2730 112410 12338 Spanish 2985 108052 12627 Total 12101 492945 36474 Table 3: Numbers of dominant content words (with a threshold of 0.0025) and function words that were removed from each sub-corpus.
The unique stems column indicates the number of unique stems (types) that remained after removal of c-w and f-w.
only 2% after removing 51 content words (by using a threshold of 0.0015).
We calculated the tf-idf weights after stop-words removal and stemming (using a Porter stemmer (Porter, 1980)), trying to pinpoint dominant stems.
The results were similar to the word?s tf-idf and no significantly dominant stem was found in either of the sub-corpora.
A drop of only 3% in accuracy was noticed after removing both dominant content words and function words.
These results show that if a content bias exists in the corpus it has only a minor effect on the SVM classification, and that the n-grams based clasFigure 2: Classification accuracy as a function of the threshold (removed content words).
13 Thresh.
0.004 0.003 0.0025 0.0015 0.0012 c-w 9 c-w 15 c-w 51 c-w 113 c-w Bulg.
77 908 1543 3955 7426 Czech 306 1829 2784 5139 8588 French 665 1829 2059 3603 6205 Russian 781 1886 2730 6302 9918 Spanish 389 1418 2985 6548 10521 Total 2218 7970 12101 25547 42658 Table 4: Number of occurrences of content words that were removed from each sub-corpus for some of the thresholds.
The numbers in the top row indicate the threshold and the number of unique content words that were found with this threshold.
sification is not an artifact of a content bias.
We ran the same experiment five more times, each time on 4 sub-corpora instead of 5, removing one (different) language each time.
The results in all 5 4-class experiments were essentially the same, and similar to those of the 5 language task (beyond the fact that the random baseline for the former is 25% rather than 20%).
4.7 Suffix
Bias Bias might also be attributed to the use of suffixes.
There are numerous types of English suffixes, which, roughly speaking, may be categorized as derivational or inflectional.
It is reasonable to expect that just like a use of function words, use or misuse of certain suffixes might occur due to language transfer.
Frequent use of a certain suffix or avoidance of the use of a certain suffix may influence the bi-grams statistics and thus the bi-grams classification may be only an artifact of the suffixes usage.
Checking the use of the 50 most productive suffixes taken from a standard list (e.g.
ing, ed, less, able, most, en) we have found that only a small number of suffixes are not equally used by speakers of all 5 languages.
Most notable are the differences in the use of ing between native French speakers and native Czech speakers and the differences of use of less between Bulgarian and Spanish speakers (Table 5).
However, no real bias can be attributed to the use of any of the suffixes because their relative aggregate effect on the values in the support vector entries is very small.
Suffix Bulg.
Czech French Russian Spanish ing 872 719 932 903 759 less 47 36 39 45 32 Table 5: Counts of two of the suffixes whose frequency of use differs the most between sub-corpora.
4.8 Control
Corpus Finally, we have also ran the experiment on a different corpus replacing the French and the Spanish subcorpora by the Dutch and Italian ones, introducing a new Roman language and a new Germanic language to the corpus.
We obtained 64.66% accuracy, essentially the same as in the original 5-language setting.
The corpus was compiled from works of advanced English students of the same level who write essays of approximately the same length, on a set of randomly and roughly equally distributed topics.
We expected that these students will use roughly the same n-grams distribution.
However, the results described above suggest that there exists some mechanism that influences the authors??choice of words.
In the next section we present a computational psycholinguistic framework that might explain our results.
5 Statistical
Learning and Language Transfer in SLA 5.1 Statistical Learning by Infants Psychologists, linguists, and cognitive science researchers try to understand the process of language learning by infants.
Many models for language learning and cognitive language modeling were suggested (Clark, 2003).
Infants learn their first language by a combination of speech streams, vocal cues and body gestures.
Infants as young as 8 months old have a limited grasp of their native tongue as they react to familiar words.
In that age they already understand the meaning of single words, they learn to spot these words in a speech stream, and very soon they learn to combine different words into new sentential units.
Parental speech stream analysis shows that it is impossible to separate between words by identifying sequences of silence between words (Saffran, 2001).
Recent studies of infant language learning are in favor of the statistical framework (Saffran, 2001; Saffran et al, 1996).
Saffran (2002) exam14 ined 8 month-old to one year-old infants who were stimulated by speech sequences.
The infants showed a significant discrimination between word and nonword stimuli.
In a different experimental setup infants showed a significant discrimination between frequent syllable n-grams and non frequent syllable n-grams, heard as part of a gibberish speech sequence generated by a computer according to various statistical language models.
In a third experimental setup infants showed a significant discrimination in favor of English-like gibberish speech sequences upon non-English-like gibberish speech sequences.
These findings along with the established finding (Jusczyk, 1997) that infants prefer the sound of their native tongue suggest that humans learn basic language units in a statistical manner and that they store some statistical parameters pertaining to these units.
We should note that some researchers doubt these conclusions (Yang, 2004).
5.2 Language
Transfer in SLA The role of the first language in second language acquisition is under a continuous debate (Ellis, 1999).
Language Transfer between L1 and L2 is the process in which a language learner of L2 whose native language is L1, is influenced by L1 when using L2 (actually, when building his/her inter-language).
This influence might appear helpful when L2 is relatively close to L1, but it interferes with the learning process due to overand under-generalization or other problems.
Although there is clear evidence that language learners use constructs of their first language when learning a foreign language (James, 1980; Odlin, 1989), it is not clear that the majority of learner errors can be attributed to the L1 transfer (Ellis, 1999).
5.3 Sound
Transfer Hypothesis For alphabetic scripts, character bi-grams reflect basic sounds and sound sequences of the language3.
We have shown that native language strongly correlates with character bi-grams when people write in English as a second language.
After ruling out usage of function words, content bias, and morphologyrelated influences, the most plausible explanation is 3Note that for English, they do not directly correspond to phonemes or syllables.
Nonetheless, they do reflect English phonology to some extent.
that these are language transfer effects related to L1 sounds.
We hypothesize that there are language transfer effects related to L1 sounds and manifested by the words that people choose to use when writing in a second language.
(We say ?writing??because we have only experimented with written texts; a more general hypothesis covering speaking and writing can be formulated as well.) Furthermore, since the acquisition and representation of phonology is strongly influenced by statistical considerations (Section 5.1), we speculate that the general language transfer phenomenon might be related to frequency.
This does not directly follow from our findings, of course, but is an exciting direction to investigate, and it is in accordance with the growing body of work on the effects of frequency on language learning and the emergence of syntax (Ellis, 2002; Bybee, 2006).
We note that there is one obvious and well-known lexical transfer effect: the usage of cognates (words that have similar form (sound) and meaning in two different languages).
However, the languages we used in our experiments contain radically differing amounts of cognates of English words (just consider French vs.
Bulgarian, for example), while the classification results were about the same for all 5 languages.
Hence, cognates might play a role, but they do not constitute a single major explaining factor for our findings.
We note that the hypothesis put forward in the present paper is the first that attributes a language transfer phenomenon to a cognitive representation (phonology) whose statistical nature has been seriously substantiated.
6 Conclusion
In this paper we have demonstrated how modern machine learning can aid other fields, here the important field of Second Language Acquisition (SLA).
Our analysis of the features useful for a multi-class SVM in the task of native language classification has resulted in the formulation of a hypothesis of potential significance in the theory of language transfer in SLA.
We hypothesize language transfer effects at the level of basic sounds and short sound sequences, manifested by the words that people choose when 15 writing in a second language.
In other words, we hypothesize that use of L2 words is strongly influenced by L1 sounds and sound patterns.
As noted above, further experiments (psychological and computational) must be conducted for validating our hypothesis.
In particular, construction of a wide-scale learners??corpus with tight control over content bias is essential for reaching stronger conclusions.
Additional future work should address sound sequences vs.
the orthographic sequences that were used in this work.
If our hypothesis is correct, then using spoken language corpora should produce even stronger results, since (1) writing systems rarely show a 1-1 correspondence with how words are at the phonological level; and (2) writing allows more conscious thinking that speaking, thus potentially reduces transfer effects.
Our eventual goal is creating a unified model of statistical transfer mechanisms.
References Argamon S., Koppel M.
and Shimoni A.
2003. Gender, Genre, and Writing Style in Formal Written Texts.
Text 23(3).
Bouselmi G., Fohr D., Illina, I., and Haton J.P. 2005.
Fully Automated Non-Native Speech Recognition Using Confusion-Based Acoustic Model.
Eurospeech/Interspeech ??5.
Bouselmi G., Fohr D., Illina I., and Haton J.P. 2006.
Fully Automated Non-Native Speech Recognition Using Confusion-Based Acoustic Model Integration and Graphemic Constraints.
IEEE International Conference on Acoustics, Speech and Signal Processing, 2006.
Bybee J.
2006. Frequency of Use and the Organization of Language.
Oxford University Press.
Clark, E.
2003. First Language Acquisition.
Cambridge University Press.
Diederich J., Kindermann J., Leopold E.
and Paass G.
2004. Authorship Attribution with Support Vector Machines.
Applied Intelligence, 109??23.
Ellis N.
2002. Frequency Effects in Language Processing.
Studies in Second Language Acquisition, 24(2):143??88.
Ellis R.
1999. Understanding Second Language Acquisition.
Oxford University Press.
Granger S., Dagneaux E.
and Meunier F.
2002. International Corpus of Learner English.
Presses universitaires de Louvain.
Hansen J.
H., Yapanel U., Huang, R.
and Ikeno A.
2004. Dialect Analysis and Modeling for Automatic Classification.
Interspeech-2004/ICSLP-2004: International Conference Spoken Language Processing.
Jeju Island, South Korea.
Holmes D.
and Forsyth R.
1995. The Federalist Revisited: New Directions in Authorship Attribution.
Literary and Linguistic Computing, pp.
111??27. James C.
E. 1980.
Contrastive Analysis.
New York: Longman.
Jusczyk P.
W. 1997.
The Discovery of Spoken Language.
MIT Press.
Koppel M.
and Schler J.
2003. Exploiting Stylistic Idiosyncrasies for Authorship Attribution.
In Proceedings of IJCAI ??3 Workshop on Computational Approaches to Style Analysis and Synthesis.
Acapulco, Mexico.
Koppel M., Schler J.
and Zigdon K.
2005(a). Determining an Author?s Native Language by Mining a Text for Errors.
Proceedings of KDD ??5.
Chicago IL.
Koppel M., Schler J.
and Zigdon K.
2005(b). Automatically Determining an Anonymous Author?s Native Language.
In Intelligence and Security Informatics (pp.
209??17). Berlin / Heidelberg: Springer.
Odlin T.
1989. Language Transfer: Cross-Linguistic Influence in Language Learning.
Cambridge University Press.
Porter F.
M. 1980.
An Algorithm for Suffix Stripping.
Program, 14(3):130??37.
Saffran J.
R. 2001.
Words in a Sea of Sounds: The Output of Statistical Learning.
Cognition, 81, 149??69.
Saffran J.
R. 2002.
Constraints on Statistical Language Learning.
Journal of Memory and Language, 47, 172??196.
Saffran J.
R., Aslin R.
N. and Newport E.
N. 1996.
Statistical Learning by 8-month Old Infants.
Science, issue 5294, 1926??928.
Salton G.
and Buckley C.
1988. Term Weighing Approaches in Automatic Text Retrieval.
Information Processing and Management, 24(5):513??23.
Scholkopf B,.
Smola A 2002.
Learning with Kernels.
MIT Press.
Stamatatos E,.
Fakotakis N.
and Kokkinakis G.
2004. Computer-Based Authorship Attribution Without Lexical Measures.
Computers and the Humanities, 193??214.
Witten I.
H. and Frank E.
2005. Data Mining: Practical Machine Learning Tools and Techniques.
San Francisco: Morgan Kaufmann.
Yang C.
2004. Universal Grammar, Statistics, or Both?.
Trends in Cognitive Science 8(10):451??56, 2004.
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 17??4, Prague, Czech Republic, June 2007 c2007 Association for Computational Linguistics Phon 1.2: A Computational Basis for Phonological Database Elaboration and Model Testing Yvan Rose 1, Gregory J.
Hedlund 1, Rod Byrne 2, Todd Wareham 2, Brian MacWhiney 3 1 Department of Linguistics Memorial University of Newfoundland 2 Department of Computer Science Memorial University of Newfoundland 3 Department of Psychology Carnegie Melon University yrose@mun.ca, ghedlund@cs.mun.ca, rod@cs.mun.ca, harold@cs.mun.ca, macw@cmu.edu Abstract This paper discuses a new, open-source software program, called Phon, that is designed for the transcription, coding, and analysis of phonological corpora.
Phon provides suport for multimedia data linkage, segmentation, multiple-blind transcription, transcription validation, sylabification, alignment of target and actual forms, and data analysis.
Al of these functions are available through a user-friendly graphical interface.
Phon, available on most computer platforms, suports data exchange among researchers with the TalkBank XML document format and the Unicode character set.
This program provides the basis for the elaboration of PhonBank, a database project that seeks to broaden the scope of CHILDES into phonological development and disorders.
1 Introduction
Empirical studies of natural language and language acquisition wil always be required in most types of linguistic research.
These studies provide the basis for describing languages and linguistic patterns.
In addition to providing us with baseline data, empirical data allow us to test theoretical, neurological, psychological and computational models.
However, the construction of natural language corpora is an extremely tedious and resourceconsuming process, despite tremendous advances in data recording, storage, and coding methods in recent decades.
Thanks to corpora and tols such as those developed in the context of the CHILDES project (htp:/childes.psy.cmu.edu/), researchers in areas such as morphology and syntax have enjoyed a convenient and powerful method to analyze the morphosyntactic properties of adult languages and their acquisition by first and second language learners.
In the area of phonetics, the Praat system (htp:/ww.fon.hum.uva.nl/praat/) has expanded our abilities to conduct phonological modeling, computational simulations based on a variety of theoretical approaches, and articulatory synthesis.
In this rapidly-expanding software universe, phonologists interested in the organization of sound systems (e.g.
phones, sylables, stress and intonational patterns) and their acquisition have not yet enjoyed the same level of computational support.
There is no developed platform for phonological analysis and no system for datasharing parallel to that found in CHILDES.
Unfortunately, this situation negatively affects the study of natural language phonology and phonological development.
It also undermines potential studies pertaining to interfaces between various components of the grammar or the elaboration of computational models of language or language development.
It is largely accepted that the grammar is hierarchically organized such that larger domains (e.g.
a sentence or a phrase) provide the conditioning environments for patterns occurring in the domains 17 located lower in the hierarchy (e.g.
the word or the sylable), as indicated in Figure 1.
Figure 1: General grammatical hierarchy This hierarchical view of grammatical organization allows us to make reference to factors that link phonology to syntax.
For example, in English, the phonological phrase, a domain that constrains phonological phenomena such as intonation, is best described using syntactic criteria (e.g.
Selkirk 1986).
Data on the acquisition of these grammatical structures and their phonological consequences can help us understand how they are learned and assimilated by the learner.
In this paper we discus Phon 1.2, the current version of an open-source software program that offers significant methodological advances in research in phonology and phonological development.
On the one hand, Phon provides a powerful and flexible solution for phonological corpus elaboration and analysis.
On the other hand, its ability to integrate with other open-source software wil facilitate the construction of complete analyses acros all levels of grammatical organization represented in Figure 1.
The paper is organized as folows.
In section 2, we discus the general motivation behind the Phon project.
In section 3, we discus the current functionality suported in Phon 1.2.
In section 4, we offer a glance at future plans for this project.
Section 5 provides a final summary.
2 The
PhonBank Project PhonBank, the latest initiative within the CHILDES project, focuses on the construction of corpora suitable for phonological and phonetic analysis.
In this section we first describe the goals and orientations of PhonBank.
We then describe Phon, the software project designed to facilitate this endeavor.
2.1 PhonBank
The PhonBank project seeks to broaden the scope of the current CHILDES system to include the analysis of phonological development in first and second languages for language learners with and without language disorders.
To achieve this goal, we wil create a new phonological database called PhonBank and a program called Phon to facilitate analysis of PhonBank data.
Using these tols, researchers wil be in position to conduct a series of developmental, croslinguistic, and methodological analyses based on large-scale corpora.
2.2 Phon
Phon consists of inter-conected modules that offer functionality to assist the researcher in important tasks related to corpus transcription, coding and analysis.
(The main functions suported are discused in the next section).
The application is developed in Java and is packaged to run on Macintosh (Mac OS X 10.4+) and Windows (Vista not tested yet) platforms.
1 Phon
is Unicode-compliant, a required feature for the sharing of data transcribed with phonetic symbols acros computer platforms.
Phon can share data with programs which utilize the TalkBank XML schema for their documents such as those provided by the TalkBank and CHILDES projects.
Phon is available as free download directly from CHILDES (htp:/childes.psy.cmu.edu/phon/).
At the time of writing these lines, Phon is available in its version 1.1, an iteration of the program that offered a proof of concept for the application envisioned (see Rose et al., 206).
Over the past year, however, we have thoroughly revised significant portions of the code to refine the functionality, ensure further compatibility with other TalkBankcompliant applications, and streamline the interface for better user experience and improved workflow.
Despite what the minor version increment (1.1 to 1.2) may imply, the new version, which is currently being tested internally and due for public release in June 207, offers significant improvements as well as novel and inovative functionality.
1 Suport
for the Unix/Linux platform is curently compromised, primarily because of licensing isues related to the multimedia functions of the aplication.
18 3 Phon 1.2 As ilustrated in Figure 2, the general interface of Phon 1.2 consists of a media centre (top left of the interface), a section for metadata (e.g.
recorded participants and their linguistic profiles; botom left) and a Transcript Editor, the interface that provides access to most of the functionality (right).
Figure 2: Phon 1.2 General Interface One of the most significant improvements brought to version 1.2 comes from the integration of common tasks within the same user interface.
In the previous version, completely separate interfaces had to be accessed to achieve the folowing tasks, all of which are required in the elaboration of any corpus: ??Media linkage and segmentation.
??Data transcription and validation (including suport for multiple-blind transcriptions).
??Segmentation of transcribed uterances (into e.g. phrases, words).
??Labeling of transcribed forms for sylabification.
??Phone and sylable alignment between target (expected) and actual (produced) forms.
As a result the user often had to navigate between various modules in order to accomplish relatively simple operations.
For example, a simple modification to a transcription required, in addition to the modification itself, revalidation of the data, and then a verification of the sylabification and alignment data generated from this revised transcription, each of these steps requiring access to and subsequent exit from a separate module.
In Phon 1.2, most of this hurdle has been alleviated through an integration of most of the functions into the Transcript Editor, while the others (e.g.
media linkage and segmentation; transcript validation) are accessed directly from the general interface, without a need to exit the Transcript Editor.
In the next subsections, we describe the main functions suported by the application.
2 3.1 Media linkage and segmentation As mentioned above, linkage of multimedia data and subsequent identification of the portions of the recorded media that are relevant for analysis are now available directly from the application?s main interface.
These tasks folow the same logic as similar systems in programs like CLAN (htp:/childes.psy.cmu.edu/clan/).
In addition to its integrated interface, Phon 1.2 offers suport for linking different portions to a single transcript to different media files.
3.2 Data
transcription The Transcript Editor now incorporates in a single interface access to data transcription and annotation, transcription segmentation, sylabification and alignment.
This module is ilustrated in more detail with the screen shot of a data record (corresponding to an uterance) in Figure 3.
Figure 3: Data record in Transcript Editor 2 Aditional functions, such as user management, are also suported by Phon; we wil however restrict ourselves to the most central functions of the program.
19 As
can be seen, the interface incorporates tiers for orthographic and phonetic transcriptions as well as other textual annotations.
Phon also provides support for an unlimited number of user-defined fields that can be used for all kinds of textual annotations that may be relevant to the coding of a particular dataset.
Al fields can be ordered to accommodate specific data visualization needs.
Phonetic transcriptions are based on the phonetic symbols and conventions of the International Phonetic Asociation (IPA).
A useful IPA character map is easily accessible from within the application, in the shape of a floating window within which IPA symbols and diacritics are organized into intuitive categories.
This map facilitates access to the IPA symbols for which there is no keyboard equivalent.
Target and actual IPA transcriptions are stored internally as strings of phonetic symbols.
Each symbol is automatically associated with a set of descriptive features generally accepted in the fields of phonetics and phonology (e.g.
bilabial, alveolar, voiced, voiceless, aspirated) (Ladefoged and Maddieson, 196).
These features are extremely useful in the sense that they provide series of descriptive labels to each transcribed symbol.
The availability of these labels is essential for research involving the grouping of various sounds into natural classes (e.g.
voiced consonants; non-high front vowels).
The built-in set of features can also be reconfigured as needed to fit special research needs.
Phon 1.2 is also equiped with functionality to automatically insert IPA Target transcriptions based on the orthographic transcriptions.
Citation form IPA transcriptions of these words are currently available for English and French.
The English forms were obtained from the CMU Pronouncing Dictionary (ww.speech.cs.cmu.edu/cgibin/cmudict); the French forms were obtained from the Lexique Project database (ww.lexique.org).
In cases when more than one pronunciation are available from the built-in dictionaries for a given writen form (e.g.
the present and past tense versions of the English word ?read??, the application provides a quick way to select the wanted form.
Of course, idealized citation forms do not provide accurate fine-grained characterizations of variations in the target language (e.g.
dialectspecific pronunciation variants; phonetic details such as degree of aspiration in obstruent stops).
They however typically provide a useful general baseline against which paterns can be identified.
1.1 Media
playback and exporting Actual forms (e.g.
the forms produced by a language learner) must be transcribed manually.
Transcript validation, the task described in the next section, also requires access to the recorded data.
To facilitate these tasks, Phon provides direct access to the segmented portions of the media for playback in each record (see the ?Segment??tier in Figure 3).
The begining and end times of these segments can be edited directly from the record, which facilitates an accurate circumscription of the relevant portions of the recorded media.
Finally, Phon can export the segmented portions of the media into a sound file, which enables quick acoustic verifications using sound visualizing software such as Praat (htp:/ww.fon.hum.uva.nl/praat/), SFS (htp:/ww.phon.ucl.ac.uk/resource/sfs/), Signalyze (htp:/ww.signalyze.com/) or CSL (htp:/ww.kayelemetrics.com/).
1.2 Transcript
validation In projects where only a single transcription of the recorded data is utilized, this transcription can be entered directly in the Transcript Editor.
In projects that rely on a multiple-blind transcription method, each transcription for a given form is stored separately.
To appear in the Transcript Editor, a blind transcription must be selected through the Transcript Validation mode.
This interface allows the transcription supervisor (or, in a beter setting, a team of supervisors working together) to compare competing transcriptions and resolve divergences.
Alternative, non-validated transcriptions are preserved for data recoverability and verification purposes.
They are however unavailable for further processing, coding or analysis.
1.3 Transcription
segmentation Researchers often wish to divide transcribed uterances into specific domains such as the phrase or the word.
Phon fulfils this need by incorporating a text segmentation module that enables the identification of strings of symbols corresponding to such morphosyntactic and phonological domains.
For example, using the sylabification module described immediately below, the researcher can test hypotheses about what domains are relevant for resylabification processes acros words.
Wordlevel segmentation is exemplified in Figure 3, as can be seen from the gray bracketing circumscrib20 ing each word.
Not readily visible from this interface however is the important fact that the bracketing enforces a logical organization between Orthographic, IPA Target and IPA Actual forms, the latter two being treated as daughter nodes directly related to their corresponding parent bracketed form in the Orthography tier.
This system of tier dependency offers several analytical advantages, for example for the identification of patterns that can relate to a particular grammatical category or position within the uterance.
In addition to the textual entry fields just described, the Transcript Editor contains color-coded graphical representations of sylabification information for both IPA Target and IPA Actual forms as well as for the segmental and sylabic alignment of these forms.
1.4 Sylabification
algorithm Once the researcher has identified the domains that are relevant for analysis, segmentation at the level of the sylable is performed automatically: segments are asigned descriptive sylable labels (visually represented with colors) such as ?onset?? or ?coda??for consonants and ?nucleus??for vowels.
The program also identifies segmental sequences within sylable constituents (e.g.
complex onsets or nuclei).
Since controversy exists in both phonetic and phonological theory regarding guidelines for sylabification, the algorithm is parameterized to alow for analytical flexibility.
The availability of different parameter settings also enables the researcher to test hypotheses on which analysis makes the best prediction for a given dataset.
Phon 1.2 contains built-in sylabification algorithms for both English and French.
The algorithm for English incorporates fine distinctions such as those proposed by Davis and Hammond (195) for the sylabification of on-glides.
Both algorithms are based on earlier work by, e.g.
Selkirk (1982) and Kaye and Lowenstamm (1984), the latter also documenting the most central properties of French sylabification.
While these algorithms use specific sylable positions such as the left appendix (utilized to identify strident fricatives at the left-edge of triconsonantal onset clusters; e.g.
?strap??, a simple sylabification algorithm is also suplied, which restricts sylable position to onset, nucleus and coda only.
Aditional algorithms (for other languages or assuming different sylable constructs) can easily be added to the program.
Our currently-implemented sylabification algorithms use a scheme based on a compositioncascade of seven deterministic FSTs (Finite State Tols).
This cascade takes as input a sequence of phones and produces a sequence of phones and associated sylable-constituent symbols, which is subsequently parsed to create the ful multi-level metrical structure.
The initial FST in the cascade places sylable nuclei and the subsequent FSTs establish and adjust the boundaries of associated onsetand coda-domains.
Changes in the definition of sylable nuclei in the initial FST and/or the ordering and makeup of the subsequent FSTs give language-specific sylabification algorithms.
To ease the development of this cascade, initial FST prototypes were writen and tested using the Xerox Finite-State Tol (xFST) (Beesley and Karttunen 203).
However, folowing the requirements of easy algorithm execution within and integration into Phon, these FSTs were subsequently coded in Java.
To date, the implemented algorithm has been tested on corpora from English and French, and has obtained accuracies of almost 10%.
Occasionally, the algorithm may produce spurious results or flag symbols as unsylabified.
This is particularly true in the case of IPA Actual forms produced by young language learners, which sometimes contain strings of sounds that are not attested in natural languages.
Sylabification is generated on the fly upon transcription of IPA forms; the researcher can thus quickly verify all results and modify them through a contextual menu (represented in Figure 3) whenever needed.
Segments that are left unsylabified are available for all queries on segmental features and strings of segments, but are not available for queries referring to aspects of sylabification (see also Figure 4 for a closer lok at the display of sylabification).
The sylabification labels can then be used in database query (for example, to access specific information about sylable onsets or codas).
In addition, because the algorithm is sensitive to main and secondary stress marks and domain edges (i.e.
first and final sylables), each sylable identified is given a prosodic status and position index.
Using the search functions, the researcher can thus use search criteria as precisely defined as, for example, complex onsets realized in word-medial, secondary-stressed sylables.
This level of functionality is central to the study of several phenomena in phonological acquisition that are determined by the 21 status of the sylable as stressed or unstressed, or by the position of the sylable within the word (e.g.
Inkelas and Rose 203).
1.5 Alignment
algorithm After sylabification, a second algorithm performs automatic, segment-by-segment and sylable-by-sylable alignment of IPA-transcribed target and actual forms.
Building on featural similarities and differences between the segments in each syllable and on sylable properties such as stress, this algorithm automatically aligns corresponding segments and sylables in target and actual forms.
It provides alignments for both corresponding sounds and sylables.
For example, in the target-actual word pair ?apricot??> ?a_cot?? the algorithm aligns the first and final sylables of each form, and identifies the midle sylable (?pri?? as truncated.
This is ilustrated in Figure 4.
Similarly, in cases of renditions such as ?blow??> ?bolow??the alignment algorithm relates both sylables of the actual form to the only sylable of the target form and diagnoses a case of vowel epenthesis.
Figure 4: Sylabification and Alignment In this alignment algorithm, forms are viewed as sequences of phones and sylable-boundary markers and the alignment is done on the phones in a way that preserves sylable integrity.
This algorithm is a variant of the standard dynamic programming algorithm for pairwise global sequence alignment (see Sankoff and Kruskal 1983 and references therein); as such, it is similar to but extends the phone-alignment algorithm described in Kondrak (203).
At the core of the Phon alignment algorithm is a function sim(x, y) that assesses the degree of similarity of a symbol x from the first given sequence and a symbol y from the second given sequence.
In our sim() function, the similarity value of phones x and y is a function of a basic score (which is the number of phonetic features shared by x and y) and the associated values of various applicable reward and penalty conditions, each of which encodes a linguistically-motivated constraint on the form of the alignment.
There are nine such reward and penalty conditions, and the interaction of these rewards and penalties on phone matchings effectively simulates sylable integrity and matching constraints.
Subsequent to this enhanced phone alignment, a series of rules is invoked to reintroduce the actual and target form sylable boundaries.
A ful description of the alignment algorithm is given in Maddocks (205) and Hedlund et al.(205). Preliminary tests on attested data from the published literature on Dutchand Englishlearning children (Fikert, 194; Pater, 197) indicate an accuracy rate above 95% (96% for a Dutch corpus and 98% for an English corpus).
As it is the case with the other algorithms included in the program, the user is able to perform manual adjustments of the computer-generated sylable alignments whenever necessary.
This process was made as easy as posible: it consists of clicking on the segment that needs to be realigned and moving it leftward or rightward using keyboard arrows.
The alignment algorithm, as well as the data processing steps that precede it (especially, sylabification), are essential to any acquisition study that requires pair-wise comparisons between target and actual forms, from both segmental and sylabic perspectives.
Implicit to the description of the implementation of the sylabification and alignment functions is a careful approach whereby the algorithms implemented at this stage are used to assist data compilation; because every result generated by the algorithms can be modified by the user, no data analysis directly depends on them.
The user thus has complete control on the processing of the data being readied for analysis.
After extensive testing on additional types of data sets, we wil be able to optimize their degree of reliability and then determined how they can be used in truly automated analyses.
1.6 Database
query Phon sports a simple search function built directly in the main interface (see Figure 2 above).
More complex queries are now suported through a series of built-in analysis and reporting functions.
22 Using these functions, the research can identify records that contain: ??Phones and phone sequences (defined with IPA symbols or descriptive feature sets).
??Sylable types (e.g.
CV, CVC, CGV, ??.
3 ??Word types (e.g.
number of sylables and the stress patterns that they compose).
??Segmental processes (obtained through featural comparisons between Target-Actual aligned phones; e.g. devoicing, gliding).
??Sylabic processes (obtained through comparisons between target-actual aligned sylables e.g. complex onset reduction).
Using these functions, the researcher can quickly identify the records that match the search criteria within the transcript.
The reported data are visualized in tables which can be saved as commaseparated value text files (.csv) that can subsequently be open in statistical or spreadsheet applications.
Using an expression builder, i.e. a system to combine simple searches using functions such as intersection and union, the researcher can also take advantage of more elaborate search criteria.
The expression builder thus enables the study of interaction between factors such as feature combinations, stress, position within the sylable, word or any other larger domain circumscribed through the uterance segmentation function described above.
2 Future
projects Phon 1.2 now provides all the functionality required for corpus elaboration, as well as a versatile system for data extraction.
In future versions, we wil incorporate an interface for the management of acoustic data and fuler suport for data querying and searching.
At a later stage, we wil construct a system for model testing.
We discus these plans briefly in the next subsections.
2.1 Interface
for acoustic data In order to facilitate research that requires acoustic measurements, Phon wil also incorporate ful interfacing with Praat and Speech Filing System, two software programs designed for acoustic analysis of speech sounds.
As a result, researchers that util3 C=consonant; V=vowel; G=glide.
ize these programs wil be able to take advantage of some of Phon?s unique functions and, similarly, researchers using Phon wil be able to take advantage of the functionality of these two applications.
2.2 Extension
of database query functionality The search and report functions described in section 3.8 provide simple and flexible tols to generate general assessments of the corpus or detect and extract particular phonological patterns.
However, to take ful advantage of all of the research potential that Phon offers, a more powerful query system wil be designed.
This system wil take the form of a query language suplemented with statistical functions.
Such a system wil enable precise assessments of developmental data within and acros corpora of language learners or learning situations.
The query language wil also offer the relevant functionality to take ful advantage of the module for management of acoustic data described in the preceding subsection.
2.3 Platform
for model testing As presently implemented, Phon wil allow us to continue with the construction of PhonBank and wil provide tols for analyzing the new database.
Once this system is in place, we wil begin to develop additional tols for model testing.
These new systems wil formalize learning algorithms in ways that wil allow users to run these algorithms on stored data, much as in the ?Learn??feature in Praat.
This new model-testing application wil include functions such as: ??Run an arbitrary language learning algorithm.
??Compare the results of the grammar produced by such a language learning algorithm against actual language data.
??In the event that the learning algorithm provides a sequence of grammars corresponding to the stages of human language learning, compare the results of this sequence of grammars against actual longitudinal language data.
By virtue of its software architecture, formcomparison routines, and stored data, Phon provides an excellent platform for implementing such an application.
Runing arbitrary language learn23 ing algorithms could be facilitated using a Java API/interface-class combination specifying subroutines provided by Phon.
The outputs of a given computational model could be compared against adult productions stored in Phon using the alignment algorithm described in Section 3.7 (which internally produces but does not output a score giving the similarity of the two forms being aligned).
Finally, the outputs of a sequence of algorithmproduced grammars relative to a given target word could be compared against the sequence of productions of that word made over the course of acquisition by a particular learner by aligning these production sequences.
Such an alignment could be done using the alignment algorithm described in Section 3.7 as a sim() function for matching up production-pairs in these sequences.
In this case, more exotic forms of alignment such as local alignment or time-warping may be more appropriate than the global alignment used in Section 3.7.
For a ful description of such alignment options, see Gusfield (197) and Sankoff and Kruskal (1983).
3 Discusion
In its current form, Phon 1.2 provides a powerful system for corpus transcription, coding and analysis.
It also offers a sound computational foundation for the elaboration of the PhonBank database and its incorporation to the CHILDES system.
Finally, it sets the basis for further improvements of its functionality, some of which was discused briefly in the preceding section.
The model-testing tol design sketched above is ambitious and perhaps premature in some aspects ?for example, should we expect the current (or even next) generation of language learning algorithms to mimic the longitudinal behavior of actual language learners?
This question is especially relevant given that some language behaviors observed in learners can be driven by articulatory or perceptual factors, the consideration of which implies relatively more complex models.
That being said, the above sugests how Phon, by virtue of its longitudinal data, output-form comparison routines, and software architecture, may provide an excellent platform for implementing the next generation of computational language analysis tols.
References Besley, K.R. and L.
Kartunen (203) Finite-State Morphology.
Stanford CA: CSLI Publications.
Davis, S.
and M.
Hamond (195).
On the Status of Onglides in American English.
Phonology 12:159-182.
Fikert, P.
(194). On the Acquisition of Prosodic Structure.
Dordrecht: ICG Printing.
Gusfield, D.
(197) Algorithms on Strings, Tres, and Sequences: Computer Science and Computational Biology.
Cambridge: Cambridge University Pres.
Hedlund, G.J., K.
Madocks, Y.
Rose, and T.
Wareham (205) Natural Language Sylable Alignment: From Conception to Implementation.
Procedings of the Fiftenth Anual Newfoundland Electrical and Computer Enginering Conference (NECEC 205).
Inkelas, S.
and Y.
Rose (203).
Velar Fronting Revisited.
Procedings of the 27th Boston University Conference on Language Development.
Somervile, MA: Cascadila Pres.
34-345. Kaye, J.
and J.
Lowenstam (1984).
De la sylabicit.
Forme sonore du langage.
Paris: Herman, 123-161.
Kondrak, G.
(203) Phonetic alignment and similarity.
Computers and the Humanities 37: 273-291.
Ladefoged, P.
and I.
Madieson (196).
The Sounds of the World?s Languages.
Cambridge, MA: Blackwel.
Madocks, K.
(205) An Efective Algorithm for the Alignment of Target and Actual Sylables for the Study of Language Acquisition.
B.Sc.h. Thesis.
Department of Computer Science, Memorial University of Newfoundland.
Pater, J.
(197). Minimal Violation and Phonological Development.
Language Acquisition 6, 201-253.
Rose, Y., B.
MacWhiney, R.
Byrne, G.
Hedlund, K.
Madocks, P.
O?Brien and T.
Wareham (206).
Introducing Phon: A Software Solution for the Study of Phonological Acquisition.
Procedings of the 30th Boston University Conference on Language Development.
Somervile, MA: Cascadila Pres.
489-50. Sankof, D.
and J.B.
Kruskal (eds., 1983) Time Warps, String Edits, and Macromolecules: The Theory and Practice of String Comparison.
Reading, MA: Adison-Wesley.
Selkirk, E.
(1982) The Sylable.
The Structure of Phonological Representation.
Dordrecht: Foris, 37-385.(1986) On Derived domains in Sentence Phonology.
Phonology 3: 371-405.
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 25??2, Prague, Czech Republic, June 2007 c2007 Association for Computational Linguistics High-accuracy Annotation and Parsing of CHILDES Transcripts Kenji Sagae Department of Computer Science University of Tokyo Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan sagae@is.s.u-tokyo.ac.jp Eric Davis Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 dhdavis@cs.cmu.edu Alon Lavie Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 alavie@cs.cmu.edu Brian MacWhinney Department of Psychology Carnegie Mellon University Pittsburgh, PA 15213 macw@cmu.edu Shuly Wintner Department of Computer Science University of Haifa 31905 Haifa, Israel shuly@cs.haifa.ac.il Abstract Corpora of child language are essential for psycholinguistic research.
Linguistic annotation of the corpora provides researchers with better means for exploring the development of grammatical constructions and their usage.
We describe an ongoing project that aims to annotate the English section of the CHILDES database with grammatical relations in the form of labeled dependency structures.
To date, we have produced a corpus of over 65,000 words with manually curated gold-standardgrammatical relation annotations.
Using this corpus, we have developed a highly accurate data-driven parser for English CHILDES data.
The parser and the manually annotated data are freely available for research purposes.
1 Introduction
In order to investigate the development of child language, corpora which document linguistic interactions involving children are needed.
The CHILDES database (MacWhinney, 2000), containing transcripts of spoken interactions between children at various stages of language development with their parents, provides vast amounts of useful data for linguistic, psychological, and sociological studies of childlanguagedevelopment.
Therawinformationin CHILDES corpora was gradually enriched by providing a layer of morphological information.
In particular, the English section of the database is augmented by part of speech (POS) tags for each word.
However, this information is usually insufficient for investigations dealing with the syntactic, semantic or pragmatic aspects of the data.
In this paper we describe an ongoing effort aiming to annotate the English portion of the CHILDES database with syntactic information based on grammatical relations represented as labeled dependency structures.
Although an annotation scheme for syntactic information in CHILDES data has been proposed (Sagae et al., 2004), until now no significant amount of annotated data had been made publicly available.
Intheprocessofmanuallyannotatingseveral thousands of words, we updated the annotation scheme, mostly by extending it to cover syntactic phenomena that occur in real data but were unaccounted for in the original annotation scheme.
The contributions of this work fall into three main categories: revision and extension of the annotation scheme for representing syntactic information in CHILDES data; creation of a manually annotated 65,000 word corpus with gold-standard syntactic analyses; and implementation of a complete parser that can automatically annotate additional data with high accuracy.
Both the gold-standard annotated data and the parser are freely available.
In addition to introducing the parser and the data, we report on many of the specific annotation issues that we encountered during the manual annotation pro25 cess, which should be helpful for those who may use the annotated data or the parser.
The annotatedcorporaandtheparserarefreelyavailablefrom http://childes.psy.cmu.edu/.
We describe the annotation scheme in the next section, along with issues we faced during the process of manual annotation.
Section 3 describes the parser, andanevaluationoftheparserispresentedin section 4.
We analyze the remaining parsing errors in section 5 and conclude with some applications of the parser and directions for future research in section 6.
2 Syntactic
annotation The English section of the CHILDES database is augmented with automatically produced ambiguous part-of-speech and morphological tags (MacWhinney, 2000).
Some of these data have been manually disambiguated, but we found that some annotation decisions had to be revised to facilitate syntactic annotation.
Wediscussbelowsomeoftherevisionswe introduced, as well as some details of the syntactic constructions that we account for.
2.1 The
morphological annotation scheme The English morphological analyzer incorporated in CHILDES produces various part-of-speech tags (there are 31 distinct POS tags in the CHILDES tagset), including ADJective, ADVerb, COmmunicator, CONJunction, DETerminer, FILler, Noun, NUMeral, ONomatopoeia, PREPosition, PROnoun, ParTicLe, QuaNtifier, RELativizer and Verb1.
In most cases, the correct annotation of a word is obvious from the context in which the word occurs, but sometimes a more subtle distinction must be made.
Wediscusssomecommonproblematicissuesbelow. Adverb vs.
preposition vs.
particle The words about, across, after, away, back, down, in, off, on, out, over, up belong to three categories: ADVerb, PREPosition and ParTicLe.
To correctly annotate them in context, we apply the following criteria.
First, a preposition must have a prepositional object, which is typically realized as a noun phrase (which may be topicalized, or even elided).
Second, a preposition forms a constituent with its noun 1We use capital letters to denote the actual tag names in the CHILDES tagset.
phrase object.
Third, a prepositional object can be fronted (for example, he sat on the chair becomes the chair on which he sat), whereas a particle-NP sequence cannot (*the phone number up which he looked cannot be obtained from he looked up the phone number).
Finally, a manner adverb can be placed between the verb and a preposition, but not between a verb and a particle.
To distinguish between an adverb and a particle, the meaning of the head verb is considered.
If the meaning of the verb and the target word, taken together, cannot be predicted from the meanings of the verb and the target word separately, then the target word is a particle.
In all other cases it is an adverb.
Verbs vs.
auxiliaries Distinguishing between Verb and AUXiliary is often straightforward, but special attention is given when tagging the verbs be, do andhave.
Ifthetargetwordisaccompaniedbyan non-finite verb in the same clause, as in I have had enough or I do not like eggs, it is an auxiliary.
Additionally, in interrogative sentences, the auxiliary is moved to the beginning of the clause, as in have I had enough? and do I like eggs?, whereas the main verb is not.
However, this test does not always work for the verb be, which may head a non-verbal predicate, as in John is a teacher, vs.
John is smiling.
In verb-participle constructions headed by the verb be, if the participle is in the progressive tense, then the head verb is labeled as auxiliary.
Communicators vs.
locative adverbs COmmunicators can be hard to distinguish from locative adverbs, especially at the beginning of a sentence.
Our convention is that CO must modify an entire sentence, so if a word appears by itself, it cannot be a CO.
For example, utterances like here or there are labeled as ADVerb.
However, if these words appear at the beginning of a sentence, are followed by a breakorpause, anddonotclearlyexpressalocation, then they are labeled CO.
Additionally, in here/there you are/go, here and there are labeled CO.
2.2 The
syntactic annotation scheme Our annotation scheme for representing grammatical relations, or GRs (such as subjects, objects and adjuncts), in CHILDES transcripts is a slightly extended version of the scheme proposed by Sagae et al.(2004), which was inspired by a general annota26 tion scheme for grammatical relations (Carroll et al., 1998), but adapted specifically for CHILDES data.
Our scheme contains 37 distinct GR types.
Sagae et al.reported 96.5% interannotator agreement, and we do not believe our minor updates to the annotation scheme should affect interannotator agreement significantly.
The scheme distinguishes among SUBJects, (finite) Clausal SUBJects2 (e.g., that he cried moved her) and XSUBJects (eating vegetables is important).
Similarly, we distinguish among OBJects, OBJect2, which is the second object of a ditransitive verb, and IOBjects, which are required verb complementsintroducedbyprepositions.
Verbcomplements that are realized as clauses are labeled COMP if they are finite (I think that was Fraser) and XCOMP otherwise (you stop throwing the blocks).
Additionally, we mark required locative adjectival orprepositionalphraseargumentsofverbsasLOCatives, as in put the toys in the box/back.
PREDicates are nominal, adjectival or prepositional complements of verbs such as get, be and become, as in I?m not sure.
Again, we specifically mark Clausal PREDicates (This is how I drink my coffee) and XPREDicates (My goal is to win the competition).
Adjuncts (denoted by JCT) are optional modifiers of verbs, adjectives or adverbs, and we distinguish among non-clausal ones (That?s much better; sit on the stool), finite clausal ones (CJCT, Mary left after she saw John) and non-finite clausal ones (XJCT, Mary left after seeing John).
MODifiers, which modify or complement nouns, again come in three flavors: MOD (That?s a nice box); CMOD (the movie that I saw was good); and XMOD (the student reading a book is tall).
We then identify AUXiliary verbs, as in did you do it?
NEGation (Fraser is not drinking his coffee); DETerminers(afly); QUANTifiers(somejuice); the objects of prepositions (POBJ, on the stool); verb ParTicLes (can you get the blocks out?); ComPlementiZeRs (wait until the noodles are cool); COMmunicators (oh, I took it); the INfinitival to; VOCatives (Thank you, Eve); and TAG questions (you know how to count, don?t you?).
2As with the POS tags, we use capital letters to represent the actual GR tags used in the annotation scheme.
Finally, we added some specific relations for handling problematic issues.
For example, we use ENUMeration for constructions such as one, two, three, go or a, b, c.
In COORDination constructions, each conjunct is marked as a dependent of the conjunction (e.g., go and get your telephone).
We use TOPicalization to indicate an argument that is topicalized, as in tapioca, there is no tapioca.
We use SeRiaL to indicate serial verbs as in come see if we can find it or go play with your toys.
Finally, we mark sequences of proper names which form the same entity (e.g., New York) as NAME.
The format of the grammatical relation (GR) annotation, which we use in the examples that follow, associateswitheachwordinasentenceatriple i|j|g, where i istheindexofthewordinthesentence, j the indexof theword?ssyntactic head, and g isthe name of the grammatical relation represented by the syntactic dependency between the i-th and j-th words.
If the topmost head of the utterance is the i-th word, it is labeled i|0|ROOT.
For example, in: a cookie. 1|2|DET 2|0|ROOT 3|2|PUNCT the first word a is a DETerminer of word 2 (cookie), which is itself the ROOT of the utterance.
2.3 Manual
annotation of the corpus We focused our manual annotation on a set of CHILDES transcripts for a particular child, Eve (Brown, 1973), and we refer to these transcripts, distributed in a set of 20 files, as the Eve corpus.
We hand-annotated (including correcting POS tags) the first 15 files of the Eve corpus following the GR scheme outlined above.
The annotation process started with purely manual annotation of 5,000 words.
This initial annotated corpus was used to train a data-driven parser, as described later.
This parser was then used to label an additional 20,000 wordsautomatically,followedbyathoroughmanual checking stage, where each syntactic annotation was manually verified and corrected if necessary.
We retrainedtheparserwiththenewlyannotateddata, and proceeded in this fashion until 15 files had been annotated and thoroughly manually checked.
Annotating child language proved to be challenging, and as we progressed through the data, we noticed grammatical constructions that the GRs could 27 notadequatelyhandle.
Forexample, theoriginalGR scheme did not differentiate between locative argumentsandlocativeadjuncts, sowecreatedanewGR label, LOC, to handle required verbal locative arguments such as on in put it on the table.
Put licenses a prepositional argument, and the existing JCT relation could not capture this requirement.
In addition to adding new GRs, we also faced challenges with telegraphic child utterances lacking verbs or other content words.
For instance, Mommy telephone could have one of several meanings: Mommy this is a telephone, Mommy I want the telephone, that is Mommy?s telephone, etc.
We tried to be as consistent as possible in annotating such utterances and determined their GRs from context.
It was often possible to determine the VOC reading vs.the MOD (Mommy?s telephone) reading by looking at context.
If it was not possible to determine the correct annotation from context, we annotated such utterances as VOC relations.
After annotating the 15 Eve files, we had 18,863 fully hand-annotated utterances, 10,280 adult and 8,563 child.
The utterances consist of 84,226 GRs (including punctuation) and 65,363 words.
The average utterance length is 5.3 words (including punctuation) for adult utterances, 3.6 for child, 4.5 overall.
The annotated Eve corpus is available at http://childes.psy.cmu.
edu/data/Eng-USA/brown.zip. It was used for the Domain adaptation task at the CoNLL-2007 dependency parsing shared task (Nivre, 2007).
3 Parsing
Although the CHILDES annotation scheme proposed by Sagae et al.(2004) has been used in practice for automatic parsing of child language transcripts (Sagae et al., 2004; Sagae et al., 2005), such work relied mainly on a statistical parser (Charniak, 2000) trained on the Wall Street Journal portion of the Penn Treebank, since a large enough corpus of annotated CHILDES data was not available to train a domain-specific parser.
Having a corpus of 65,000 words of CHILDES data annotated with grammatical relations represented as labeled dependencies allows us to develop a parser tailored for the CHILDES domain.
Our overall parsing approach uses a best-first probabilisticshift-reducealgorithm,workingleft-toright to find labeled dependencies one at a time.
The algorithm is essentially a dependency version of the data-driven constituent parsing algorithm for probabilistic GLR-like parsing described by Sagae and Lavie (2006).
Because CHILDES syntactic annotations are represented as labeled dependencies, using a dependency parsing approach allows us to work with that representation directly.
This dependency parser has been shown to have state-of-the-art accuracy in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre, 2007)3.
Sagae and Tsujii (2007) present a detailed description of the parsing approach used in our work, including the parsing algorithm.
In summary, the parser uses an algorithm similar to the LR parsing algorithm (Knuth, 1965), keeping a stack of partially built syntactic structures, and a queue of remaining input tokens.
At each step in the parsing process, the parser can apply a shift action (remove a token from the front of the queue and place it on top of the stack), or a reduce action (pop the two topmost stack items, and push a new item composed of the two popped items combined in a single structure).
This parsing approach is very similar to the one used successfully by Nivre et al.(2006), but we use a maximum entropy classifier (Berger et al., 1996) to determine parser actions, which makes parsing extremely fast.
In addition, our parsing approach performs a search over the space of possible parser actions, while Nivre et al.?s approach is deterministic.
See Sagae and Tsujii (2007) for more information on the parser.
Features used in classification to determine whether the parser takes a shift or a reduce action at any point during parsing are derived from the parser?s current configuration (contents of the stack and queue) at that point.
The specific features used are:4 ??Word and its POS tag: s(1), q(2), and q(1).
??POS: s(3) and q(2).
3The parser used in this work is the same as the probabilistic shift-reduce parser referred to as ?Sagae??in the cited shared task descriptions.
In the 2007 shared task, an ensemble of shiftreduce parsers was used, but only a single parser is used here.
4s(n) denotes the n-th item from the top of the stack (where s(1) is the item on the top of the stack), and q(n) denotes the n-th item from the front of the queue.
28 ??The dependency label of the most recently attached dependent of: s(1) and s(2).
??The previous parser action.
4 Evaluation
4.1 Methodology We first evaluate the parser by 15-fold crossvalidation on the 15 manually curated gold-standard Eve files (to evaluate the parser on each file, the remaining 14 files are used to train the parser).
Singleword utterances (excluding punctuation) were ignored, since their analysis is trivial and their inclusion would artificially inflate parser accuracy measurements.
The size of the Eve evaluation corpus (with single-word utterances removed) was 64,558 words (or 59,873 words excluding punctuation).
Of these, 41,369 words come from utterances spoken by adults, and 18,504 come from utterances spoken by the child.
To evaluate the parser?s portability to other CHILDES corpora, we also tested the parser (trained only on the entire Eve set) on two additional sets, one taken from the MacWhinney corpus (MacWhinney, 2000) (5,658 total words, 3,896 words in adult utterances and 1,762 words in child utterances), and one taken from the Seth corpus (Peters, 1987; Wilson and Peters, 1988) (1,749 words, 1,059 adult and 690 child).
The parser is highly efficient: training on the entire Eve corpus takes less that 20 minutes on standard hardware, and once trained, parsing the Eve corpus takes 18 seconds, or over 3,500 words per second.
Following recent work on dependency parsing (Nivre, 2007), we report two evaluation measures: labeled accuracy score (LAS) and unlabeled accuracy score (UAS).
LAS is the percentage of tokens for which the parser predicts the correct head-word and dependency label.
UAS ignores the dependency labels, and therefore corresponds to the percentage of words for which the correct head was found.
In addition to LAS and UAS, we also report precision and recall of certain grammatical relations.
For example, compare the parser output of go buy an apple to the gold standard (Figure 1).
This sequence of GRs has two labeled dependency errors and one unlabeled dependency error.
1|2|COORD for the parser versus1|2|SRLis a labeled error because the dependency label produced by the parser (COORD) does not match the gold-standard annotation (SRL), although the unlabeled dependency is correct, since the headword assignment, 1|2, is the same for both.
On the other hand, 5|1|PUNCTversus 5|2|PUNCT is both a labeled dependency error and an unlabeled dependency error, since the headword assignment produced by the parser does not match the gold-standard.
4.2 Results
Trained on domain-specific data, the parser performed well on held-out data, even though the training corpus is relatively small (about 60,000 words).
The results are listed in Table 1.
LAS UAS Eve cross-validation 92.0 93.8 Table 1: Average cross-validation results, Eve The labeled dependency error rate is about 8% and the unlabeled error rate is slightly over 6%.
Performance in individual files ranged between the best labeled error rate of 6.2% and labeled error rate of 4.4% for the fifth file, and the worst error rates of 8.9% and 7.8% for labeled and unlabeled respectively in the fifteenth file.
For comparison, Sagae et al.(2005) report 86.9% LAS on about 2,000 words of Eve data, using the Charniak (2000) parser with a separate dependency-labeling step.
Part of the reason we obtain levels of accuracy higher than usually reported for dependency parsers is that the averagesentencelengthinCHILDEStranscriptsismuch lower than in, for example, newspaper text.
The average sentence length for adult utterances in the Eve corpus is 6.1 tokens, and 4.3 tokens for child utterances5.
Certain GRs are easily identifiable, such as DET, AUX, and INF.
The parser has precision and recall of nearly 1.00 for those.
For all GRs that occur more than 1,000 times in the Eve corpus (which contrains more than 60,000 tokens), precision and recall are above 0.90, with the exception of COORD, which 5This differs from the figures in section 2.3 because for the purpose of parser evaluation we ignore sentences composed only of a single word plus punctuation.
29 go buy an apple . parser: 1|2|COORD 2|0|ROOT 3|4|DET 4|2|OBJ 5|1|PUNCT gold: 1|2|SRL 2|0|ROOT 3|4|DET 4|2|OBJ 5|2|PUNCT Figure 1: Example output: parser vs.
gold annotation occurs 1,163 times in the gold-standard data.
The parser?s precision for COORD is 0.73, and recall is 0.84.
Other interesting GRs include SUBJ, OBJ, JCT (adjunct), COM, LOC, COMP, XCOMP, CJCT (subordinate clause acting as an adjunct), and PTL (verb particle, easily confusable with prepositions and adverbs).
Their precision and recall is shown in table 2.
GR Precision Recall F-score SUBJ 0.96 0.96 0.96 OBJ 0.93 0.94 0.93 JCT 0.91 0.90 0.90 COM 0.96 0.95 0.95 LOC 0.95 0.90 0.92 COMP 0.83 0.86 0.84 XCOMP 0.86 0.87 0.87 CJCT 0.61 0.59 0.60 PTL 0.97 0.96 0.96 COORD 0.73 0.84 0.78 Table 2: Precision, recall and f-score of selected GRs in the Eve corpus We also tested the accuracy of the parser on child utterances and adult utterances separately.
To do this, we split the gold standard files into child and adult utterances, producing gold standard files for both child and adult utterances.
We then trained the parser on 14 of the 15 Eve files with both child and adult utterances, and parsed the individual child and adult files.
Not surprisingly, the parser performed slightly better on the adult utterances due to theirgrammaticalityandthefactthattherewasmore adult training data than child training data.
The results are listed in Table 3.
LAS UAS Eve Child 90.0 91.7 Eve Adult 93.1 94.8 Table 3: Average child vs.
adult results, Eve Our final evaluation of the parser involved testing the parser on data taken from a different parts of the CHILDES database.
First, the parser was trained on all gold-standard Eve files, and tested on manually annotated data taken from the MacWhinney transcripts.
Although accuracy was lower for adult utterances (85.8% LAS) than on Eve data, the accuracy for child utterances was slightly higher (92.3% LAS), even though child utterances were longer on average (4.7 tokens) than in the Eve corpus.
Finally, because a few aspects of the many transcript sets in the CHILDES database may vary in ways not accounted for in the design of the parser or the annotation of the training data, we also report results on evaluation of the Eve-trained parser on a particularly challenging test set, the Seth corpus.
Because the Seth corpus contains transcriptions of language phenomena not seen in the Eve corpus (see section 5), parser performance is expected to suffer.
Althoughaccuracyonadultutterancesishigh (92.2% LAS), accuracy on child utterances is very low (72.7% LAS).
This is due to heavy use of a GR label that does not appear at all in the Eve corpus that was used to train the parser.
This GR is used to represent relations involving filler syllables, which appear in nearly 45% of the child utterances in the Seth corpus.
Accuracy on the sentences that do not contain filler syllables is at the same level as in the other corpora (91.1% LAS).
Although we do not expect to encounter many sets of transcripts that are as problematic as this one in the CHILDES database, it is interesting to see what can be expected from the parser under unfavorable conditions.
The results of the parser on the MacWhinney and Seth test sets are summarized in table 4, where Seth (clean) refers to the Seth corpus without utterances that contain filler sylables.
5 Error
Analysis A major source for parser errors on the Eve corpus (112 out of 5181 errors) was telegraphic speech, 30 LAS UAS MacWhinney Child 92.3 94.8 MacWhinney Adult 85.8 89.4 MacWhinney Total 88.0 91.2 Seth Child 72.7 82.0 Seth Adult 92.2 94.4 Seth Total 84.6 89.5 Seth (clean) Child 91.1 92.7 Seth (clean) Total 92.0 93.9 Table 4: Training on Eve, testing on MacWhinney and Seth as in Mommy telephone or Fraser tape+recorder floor.
Telegraphic speech may be the most challenging, since even for a human annotator, determining a GR is difficult.
The parser usually labeled such utterances with the noun as the ROOT and the proper noun as the MOD, while the gold annotation is context-dependent as described above.
Another category of errors, with about 150 instances, is XCOMP errors.
The majority of the errors in this category revolve around dropped words in the main clause, for example want eat cookie.
Often, the parser labels such utterances with COMP GRs, because of the lack of to.
Exclusive training on utterances of this type may resolve the issue.
Many of the errors of this type occur with want: the parser could be conditioned to assign an XCOMP GR with want as the ROOT of an utterance.
COORD and PRED errors would both benefit from more data as well.
The parser performs admirably on simple coordination and predicate constructions, but has troubles with less common constructions such as PRED GRs with get, e.g., don?t let your hands get dirty (69 errors), and coordination of prepositional objects, as in a birthday cake with Cathy and Becky (154 errors).
The performance drop on the Seth corpus can be explained by a number of factors.
First and foremost, Seth is widely considered in the literature to be the child who is most likely to invalidate any theory (Wilson and Peters, 1988).
He exhibits false starts and filler syllables extensively, and his syntax violates many ?universal??principles.
This is reflected in the annotation scheme: the Seth corpus, following the annotation of Peters (1983), is abundant with filler syllables.
Because there was no appropriate GR label for representing the syntactic relationships involving the filler syllables, we annotated those with a special GR (not used during parser training), which the parser is understandably not able to produce.
Filler syllables usually occur near the start of the sentence, and once the parser failed to label them, it could not accurately label the remaining GRs.
Other difficulties in the Seth corpus include the usage of dates, of which there were no instances in the Eve corpus.
The parser had not been trained on the new DATE GR and subsequently failed to parse it.
6 Conclusion
We described an annotation scheme for representing syntactic information as grammatical relations inCHILDESdata, amanuallycuratedgold-standard corpus of 65,000 words annotated according to this GR scheme, and a parser that was trained on the annotated corpus and produces highly accurate grammatical relations for both child and adult utterances.
These resources are now freely available to the research community, and we expect them to be instrumental in psycholinguistic investigations of language acquisition and child language.
Syntactic analysis of child language transcripts using a GR scheme of this kind has already been shown to be effective in a practical setting, namely in automatic measurement of syntactic development in children (Sagae et al., 2005).
That work relied on a phrase-structure statistical parser (Charniak, 2000) trained on the Penn Treebank, and the output of that parser had to be converted into CHILDES grammatical relations.
Despite the obvious disadvantage of using a parser trained on a completely different language genre, Sagae et al.(2005) demonstrated how current natural language processing techniques can be used effectively in child language work, achieving results that are close to those obtained by manual computation of syntactic development scores for child transcripts.
Still, the use of tools not tailored forchildlanguageandextraeffortnecessarytomake them work with community standards for child language transcription present a disincentive for child language researchers to incorporate automatic syntactic analysis into their work.
We hope that the GR 31 representation scheme and the parser presented here will make it possible and convenient for the child language community to take advantage of some of therecentdevelopmentsinnaturallanguageparsing, as was the case with part-of-speech tagging when CHILDES specific tools were first made available.
Our immediate plans include continued improvement of the parser, which can be achieved at least in part by the creation of additional training data from other English CHILDES corpora.
We also plan to release automatic syntactic analyses for the entire English portion of CHILDES.
Although we have so far focused exclusively on English CHILDES data, dependency schemes based on functional relationships exist for a number of languages (Buchholz and Marsi, 2006), and the general parsing techniques used in the present work have been shown to be effective in several of them (Nivre et al., 2006).
As future work, we plan to adapt existing dependency-based annotation schemes and apply our current syntactic annotation and parsing framework to other languages in the CHILDES database.
Acknowledgments We thank Marina Fedner for her help with annotation of the Eve corpus.
This work was supported in part by the National Science Foundation under grant IIS-0414630.
References A.Berger, S.A.DellaPietra, andV.J.DellaPietra. 1996.
A maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39??1.
Roger Brown.
1973. A first language: the early stages.
George Allen & Unwin Ltd., London.
Sabine Buchholz and Erwin Marsi.
2006. Conll-x shared task on multilingual dependency parsing.
In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-X), pages 149??64, New York City, June.
Association for Computational Linguistics.
John Carroll, Edward Briscoe, and Antonio Sanfilippo.
1998. Parser evaluation: a survey and a new proposal.
In Proceedings of the 1st International Conference on Language Resources and Evaluation, pages 447??54, Granada, Spain.
Eugene Charniak.
2000. A maximum-entropy-inspired parser.
In Proceedings of the first conference on North American chapter of the Association for Computational Linguistics, pages 132??39, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
D. Knuth.
1965. On the translation of languages from left to right.
Information and Control, 8(6):607??39.
Brian MacWhinney.
2000. The CHILDES Project: Tools for Analyzing Talk.
Lawrence Erlbaum Associates, Mahwah, NJ, third edition.
Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit, and Svetoslav Marinov.
2006. Labeled pseudo-projective dependency parsing with support vector machines.
In Proceedings of the Tenth Conference on Computational Natural Language Learning.
Joakim Nivre, editor.
2007. CoNLL-XI Shared Task on Multilingual Dependency Parsing, Prague, June.
Association for Computational Linguistics.
Ann M.
Peters. 1983.
The Units of Language Acquisition.
Monographs in Applied Psycholinguistics.
Cambridge University Press, New York.
Ann M.
Peters. 1987.
The role of immitation in the developing syntax of a blind child.
Text, 7:289??11.
Kenji Sagae and Alon Lavie.
2006. A best-first probabilistic shift-reduce parser.
In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 691??98, Sydney, Australia, July.
Association for Computational Linguistics.
Kenji Sagae and Jun?ichi Tsujii.
2007. Dependency parsing and domain adaptation with lr models and parser ensembles.
In Proceedings of the Eleventh Conference on Computational Natural Language Learning.
Kenji Sagae, Alon Lavie, and Brian MacWhinney.
2004. Adding syntactic annotations to transcripts of parent-child dialogs.
In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC 2004), Lisbon, Portugal.
Kenji Sagae, Alon Lavie, and Brian MacWhinney.
2005. Automatic measurement of syntactic development in child language.
In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL??5), pages 197??04, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
B. Wilson and Ann M.
Peters. 1988.
What are you cookin??on a hot?: A three-year-old blind child?s ?violation??of universal constraints on constituent movement.
Language, 64:249??73 .
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 33??0, Prague, Czech Republic, June 2007 c2007 Association for Computational Linguistics I will shoot your shopping down and you can shoot all my tins Automatic Lexical Acquisition from the CHILDES Database Paula Buttery and Anna Korhonen RCEAL, University of Cambridge 9 West Road, Cambridge, CB3 9DB, UK pjb48, alk23@cam.ac.uk Abstract Empirical data regarding the syntactic complexity of children?s speech is important for theories of language acquisition.
Currently much of this data is absent in the annotated versions of the CHILDES database.
In this perliminary study, we show that a state-ofthe-art subcategorization acquisition system of Preiss et al.(2007) can be used to extract largescale subcategorization (frequency) information from the (i) child and (ii) child-directed speech within the CHILDES database without any domain-specific tuning.
We demonstrate that the acquired information is sufficiently accurate to confirm and extend previously reported research findings.
We also report qualitative results which can be used to further improve parsing and lexical acquisition technology for child language data in the future.
1 Introduction
Large empirical data containing children?s speech are the key to developing and evaluating different theories of child language acquisition (CLA).
Particularly important are data related to syntactic complexity of child language since considerable evidence suggests that syntactic information plays a central role during language acquisition, e.g.
(Lenneberg, 1967; Naigles, 1990; Fisher et al., 1994).
The standard corpus in the study of CLA is the CHILDES database (MacWhinney, 2000) 1 which provides 300MB of transcript data of interactions be1 See http://childes.psy.cmu.edu for details.
tween children and parents over 25 human languages.
CHILDES is currently available in raw, part-of-speechtagged and lemmatized formats.
However, adequate investigation of syntactic complexity requires deeper annotations related to e.g. syntactic parses, subcategorization frames (SCFs), lexical classes and predicateargument structures.
Although manual syntactic annotation is possible, it is extremely costly.
The alternative is to use natural language processing (NLP) techniques for annotation.
Automatic techniques are now viable, cost effective and, although not completely error-free, are sufficiently accurate to yield annotations useful for linguistic purposes.
They also gather important qualitative and quantitative information, which is difficult for humans to obtain, as a side-effect of the acquisition process.
For instance, state-of-the-art statistical parsers, e.g.
(Charniak, 2000; Briscoe et al., 2006), have wide coverage and yield grammatical representations capable of supporting various applications (e.g.
summarization, information extraction).
In addition, lexical information (e.g.
subcategorization, lexical classes) can now be acquired automatically from parsed data (McCarthy and Carroll, 2003; Schulte im Walde, 2006; Preiss et al., 2007).
This information complements the basic grammatical analysis and provides access to the underlying predicate-argument structure.
Containing considerable ellipsis and error, spoken child language can be challenging for current NLP techniques which are typically optimized for written adult language.
Yet Sagae et al.(2005) have recently demonstrated that existing statistical parsing techniques can be usefully modified to analyse CHILDES 33 with promising accuracy.
Although further improvements are still required for optimal accuracy, this research has opened up the exciting possibility of automatic grammatical annotation of the entire CHILDES database in the future.
However, no work has yet been conducted on automatic acquisition of lexical information from child speech.
The only automatic lexical acquisition study involving CHILDES that we are aware of is that of Buttery and Korhonen (2005).
The study involved extracting subcategorization information from (some of) the adult (child-directed) speech in the database, and showing that this information differs from that extracted from the spoken part of the British National Corpus (BNC) (Burnard, 1995).
In this paper, we investigate whether state-of-theart subcategorization acquisition technology can be used?without any domain-specific tuning?to obtain large-scale verb subcategorization frequency information from CHILDES which is accurate enough to show differences and similarities between child and adult speech, and thus be able to provide support for syntactic complexity studies in CLA.
We use the new system of Preiss et al.(2007) to extract SCF frequency data from the (i) child and (ii) child-directed speech within CHILDES.
We show that the acquired information is sufficiently accurate to confirm and extend previously reported SCF (dis)similarities between the two types of data.
In particular, we demonstrate that children and adults have different preferences for certain types of verbs, and that these preferences seem to influence the way children acquire subcategorization.
In addition, we report qualitative results which can be used to further improve parsing and lexical acquisition technology for spoken child language data in the future.
2 Subcategorization
Acquisition System We used for subcategorization acquisition the new system of Preiss, Briscoe and Korhonen (2007) which is essentially a much improved and extended version of Briscoe and Carroll?s (1997) system.
It incorporates 168 SCF distinctions, a superset of those found in the COMLEX Syntax (Grishman et al., 1994) and ANLT (Boguraev et al., 1987) dictionaries.
Currently, SCFs abstract over specific lexically governed particles and prepositions and specific predicate selectional preferences but include some derived semi-predictable bounded dependency constructions, such as particle and dative movement?this will be revised in future versions of the SCF system.
The system tokenizes, tags, lemmatizes and parses input sentences using the recent (second) release of the RASP (Robust Accurate Statistical Parsing) system (Briscoe et al., 2006) which parses arbitrary English text with state-of-the-art levels of accuracy.
SCFs are extracted from the grammatical relations (GRs) output of the parser using a rule-based classifier.
This classifier operates by exploiting the close correspondence between the dependency relationships which the GRs embody and the head-complement structure which subcategorization acquisition attempts to recover.
Lexical entries of extracted SCFs are constructed for each word in the corpus data.
Finally, the entries may be optionally filtered to obtain a more accurate lexicon.
This is done by setting empirically determined thresholds on the relative frequencies of SCFs.
When evaluated on cross-domain corpora containing mainly adult language, this system achieves 68.9 F-measure 2 in detecting SCF types?a result which compares favourably to those reported with other comparable SCF acquisition systems.
3 Data
The English (British and American) sections of the CHILDES database (MacWhinney, 2000) were used to create two corpora: 1) CHILD and 2) CDS.
Both corpora contained c.
1 million utterances which were selected from the data after some utterances containing un-transcribable sections were removed.
Speakers were identified using speaker-id codes within the CHAT transcriptions of the data: 3 CHILD contained the utterances of speakers identified as target children; CDS contained input from speakers identified as parents/caretakers.
The mean utterance length (measured in words) in CHILD and CDS were 3.48 and 4.61, respectively.
The mean age of the child speaker in CHILD is around 3 years 6 months.
4 2 See Section 4 for details of F-measure.
3 CHAT
is the transcription and coding format used by all the transcriptions within CHILDES.
4 The
complete age range is from 1 year and 1 month up to 7 years.
34 3.1 Test Verbs and SCF Lexicons We selected a set of 161 verbs for experimentation.
The words were selected at random, subject to the constraint that a sufficient number of SCFs would be extracted (> 100) from both corpora to facilitate maximally useful comparisons.
All sentences containing an occurrence of one of the test verbs were extracted from the two corpora and fed into the SCF acquisition system described earlier in section 2.
In some of our experiments the two lexicons were compared against the VALEX lexicon (Korhonen et al., 2006)?a large subcategorization lexicon for English which was acquired automatically from several crossdomain corpora (containing both written and spoken language).
VALEX includes SCF and frequency information for 6,397 English verbs.
We employed the most accurate version of the lexicon here (87.3 Fmeasure)?this lexicon was obtained by selecting high frequency SCFs and supplementing them with lower frequency SCFs from manually built lexicons.
4 Analysis
4.1 Methods for Analysis The similarity between verb and SCF distributions in the lexicons was examined.
To maintain a robust analysis in the presence of noise, multiple similarity measures were used to compare the verb and SCF distributions (Korhonen and Krymolowski, 2002).
In the following p =(p i ) and q =(q i ) where p i and q i are the probabilities associated with SCF i in distributions (lexicons) P and Q: ??Intersection (IS) the intersection of non-zero probability SCFsinp and q; ??Spearman rank correlation (RC) lies in the range [1;1], with values near 0 denoting a low degree of association and values near -1 and 1 denoting strong association; ??Kullback-Leibler (KL) distance a measure of the additional information needed to describe p using q, KL is always ??0 and =0only when p ??q; The SCFs distributions acquired from the corpora for the chosen words were evaluated against: (i) a gold standard SCF lexicon created by merging the SCFsin the COMLEX and ANLT syntax dictionaries?this enabled us to determine the accuracy of the acquired SCFs; (ii) another acquired SCF lexicon (as if it were a gold standard)?this enabled us to determine similarity of SCF types between two lexicons.
In each case Verb CHILD CDS go 1 1 want 2 2 get 3 3 know 4 4 put 5 6 see 6 5 come 7 10 like 8 7 make 9 11 say 10 8 take 11 13 eat 12 14 play 13 15 need 14 16 look 15 12 fall 16 22 sit 17 21 think 18 9 break 19 27 give 20 17 Table 1: Ranks of the 20 most frequent verbs in CHILD and in CDS we recorded the number of true positives (TPs), correct SCFs, false positives (FPs), incorrect SCFs, and false negatives (FNs), correct SCFs not in the gold standard.
Using these counts, we calculated type precision (the percentage of SCF types in the acquired lexicon which are correct), type recall (the percentage of SCF types in the gold standard that are in the lexicon) and F-measure: F = 2  precision  recall precision + recall (1) 4.2 Verb Analysis Before conducting the SCF comparisons we first compared (i) our 161 test verbs and (ii) all the 1212 common verbs and their frequencies in CHILD and CDS using the Spearman rank correlation (RC) and the Kullback-Leibler distance (KL).
The result was a strong correlation between the 161 test verbs (RC = 0.920  0.0791, KL = 0.05) as well as between all the 1212 verbs (RC = 0.851  0.0287, KL = 0.07) in the two corpora.
These figures suggest that the child-directed speech (which is less diverse in general than speech between adults, see e.g. the experiments of Buttery and Korhonen (2005)) contains a very similar distribution of verbs to child speech.
This is to be expected since the 35 corpora essentially contain separate halves of the same interactions.
However, our large-scale frequency data makes it possible to investigate the cause for the apparently small differences in the distributions.
We did this by examining the strength of correlation throughout the ranking.
We compared the ranks of the individual verbs and discovered that the most frequent verbs in the two corpora have indeed very similar ranks.
Table 1 lists the 20 most frequent verbs in CHILD (starting from the highest ranked verb) and shows their ranks in CDS.
As illustrated in the table, the top 4 verbs are identical in the two corpora (go, want, get, know) while the top 15 are very similar (including many action verbs e.g. put, look, sit, eat, and play).
Yet some of the lower ranked verbs turned out to have large rank differences between the two corpora.
Two such relatively highly ranked verbs are included in the table?think which has a notably higher rank in CDS than in CHILD, and break which has a higher rank in CHILD than in CDS.
Many other similar cases were found in particular among the medium and low frequency verbs in the two corpora.
To obtain a better picture of this, we calculated for each verb its rank difference between CHILD vs.
CDS. Table 2 lists 40 verbs with substantial rank differences between the two corpora.
The first column shows verbs which have higher ranks in CHILD than in CDS, and the second column shows verbs with higher ranks in CDS than in CHILD.
We can see e.g. that children tend to prefer verbs such as shoot, die and kill while adults prefer verbs such as remember, send and learn.
To investigate whether these differences in preferences are random or motivated in some manner, we classified the verbs with the largest differences in ranks (>10) into appropriate Levin-style lexicalsemantic classes (Levin, 1993) according to their predominant senses in the two corpora.
5 We
discovered that the most frequent classes among the verbs that children prefer are HIT (e.g.
bump, hit, kick), BREAK (e.g.
crash, break, rip), HURT (e.g.
hurt, burn, bite) and MOTION (e.g.
fly, jump, run) verbs.
Overall, many of the preferred verbs (regardless of the class) express negative actions or feelings (e.g.
shoot, die, scare, hate).
5 This
classification was done manually to obtain a reliable result.
CHILD CDS shoot tie remember hope hate wish send suppose die cut learn bet write crash wipe kiss use kick pay smell bump scare feed guess win step ask change lock burn feel set fight stand listen stand jump care wait wonder Table 2: 20 verbs ranked higher in (i) child speech and (ii) child-directed speech.
In contrast, adults have a preference for verbs from classes expressing cognitive processes (e.g.
remember, suppose, think, wonder, guess, believe, hope, learn)or those that can be related to the education of children, e.g. the WIPE verbs wash, wipe and brush and the PERFORMANCE verbs draw, dance and sing.
In contrast to children, adults prefer verbs which express positive actions and feelings (e.g.
share, help, love, kiss).
It is commonly reported that child CLA is motivated by a wish to communicate desires and emotions, e.g.
(Pinker, 1994), but a relative preference in child speech over child-directed speech for certain verb types or verbs expressing negative actions and feelings has not been explicitly shown on such a scale before.
While this issue requires further investigation, our findings already demonstrate the value of using large scale corpora in producing novel data and hypotheses for research in CLA.
4.3 SCF
Analysis 4.3.1 Quantitative SCF Comparison The average number of SCFs taken by studied verbs in the two corpora proved quite similar.
In unfiltered SCF distributions, verbs in CDS took on average a larger number of SCFs (29) than those in CHILD (24), but in the lexicons filtered for accuracy the numbers were identical (8??0, depending on the filtering threshold applied).
The intersection between the CHILD / CDS SCFs and those in the VALEX lexicon was around 0.5, indicating that the two lexicons included only 50% of the SCFs in the lexicon extracted from general (cross-domain) adult language corpora.
Recall against VALEX was consequently low (between 48% and 68% depending on the filtering threshold) but precision was around 50-60% for both CHILDES and CDS lexicons 36 Measures Unfilt.
Filt. Precision (%) 82.9 88.7 Recall (%) 69.3 44.5 F-measure 75.5 59.2 IS 0.73 0.62 RC 0.69 0.72 KL 0.33 0.46 Table 3: Average results when SCF distributions in CHILD and CDS are compared against each other.
(also depending on the filtering threshold), which is a relatively good result for the challenging CHILDES data.
However, it should be remembered that with this type of data it would not be expected for the SCF system to achieve as high precision and recall as it would on, for instance, adult written text and that the missing SCFs and/or misclassified SCFs are likely to provide us with the most interesting information.
As expected, there were differences between the SCF distributions in the two lexicons.
Table 3 shows the results when the CHILD and CDS lexicons are compared against each other (i.e.
using the CDS as a gold standard).
The comparison was done using both the unfiltered and filtered (using relative frequency threshold of 0.004) versions of the lexicons.
The similarity in SCF types is 75.5 according to F-measure in the unfiltered lexicons and 59.2 in filtered ones.
6 4.3.2 Qualitative SCF Comparison Our qualitative analysis of SCFs in the two corpora revealed reasons for the differences.
Table 4 lists the 10 most frequent SCFsinCHILD (starting from the highest ranked SCF), along with their ranks in CDS and VALEX.
The top 3 SCFs(NP, INTRANSITIVE and PP frames) are ranked quite similarly in all the corpora.
Looking at the top 10 SCFs, CHILD appears, as expected, more similar to CDS than with VALEX, but large differences can be detected in lower ranked frames.
To identify those frames, we calculated for each SCF its difference in rank between CHILD vs.
CDS. Table 5 exemplifies some of the SCFs with the largest rank differences.
Many of these concern frames involving sentential complementation.
Children use more fre6 The fact that the unfiltered lexicons appear so much more similar suggests that some of the similarity is due to similarity in incorrect SCFs (many of which are low in frequency, i.e. fall under the threshold).
quently than adults SCFs involving THAT and HOW complementation, while adults have a preference for SCFs involving WHETHER, ING and IF complementation.
Although we have not yet looked at SCF differences across ages, these discoveries are in line with previous findings, e.g.
(Brown, 1973), which indicate that children master the sentential complementation SCFs preferred by adults (in our experiment) fairly late in the acquisition process.
With a mean utterance length for CHILD at 3.48, we would expect to see relatively few of these frames in the CHILD corpus?and consequently a preference for the simpler THAT constructions.
4.4 The
Impact of Verb Type Preferences on SCF Differences Given the new research findings reported in Section 4.2 (i.e.
the discovery that children and adults have different preferences for many medium-low frequency verbs) we investigated whether verb type preferences play a role in SCF differences between the two corpora.
We chose for experimentation 10 verbs from 3 groups: 1.
Group 1 ??verbs with similar ranks in CHILD and CDS: bring, find, give, know, need, put, see, show, tell, want 2.
Group 2 ??verbs with higher ranks in CDS: ask, feel, guess, help, learn, like, pull, remember, start, think 3.
Group 3 ??verbs with higher ranks in CHILD: break, die, forget, hate, hit, jump, scare, shoot, burn, wish The test verbs were selected randomly, subject to the constraint that their absolute frequencies in the two corpora were similar.
7 We
first correlated the unfiltered SCF distributions of each test verb in the two corpora against each other and calculated the similarity in the SCF types using the F-measure.
We then evaluated for each group, the accuracy of SCFsinunfiltered distributions against our gold standard (see Section 4.1).
Because the gold standard was too ambitious in terms of recall, we only calculated the precision figures: the average number of TP and FP SCFs taken by test verbs.
The results are included in Table 6.
Verbs in Group 1 show the best SCF type correlation (84.7 F-measure) between the two corpora although they are the richest in terms of subcategorization (they take the highest number of SCFs out of the three groups).
The SCF correlation is clearly lower in Groups 2 and 3, although 7 This requirement was necessary because frequency may influence subcategorization acquisition performance.
37 SCF Example sentence CHILD CDS VALEX NP I love rabbits 1 1 1 INTRANS I sleep with a pillow and blanket 2 2 2 PP He can jump over the fence 3 4 3 PART I can?t give up 4 7 9 TO-INF-SC I want to play with something else 5 3 6 PART-NP/NP-PART He looked it up 6 6 7 NP-NP Ask her all these questions 7 5 18 NP-INF-OC Why don?t you help her put the blocks in the can ? 8 9 60 INTR-RECIP So the kitten and the dog won?t fight 9 8 48 NP-PP He put his breakfast in the bin 10 10 4 Table 4: 10 most frequent SCFsinCHILD, along with their ranks in CDS and VALEX.
SCF Example sentence CHILD MP I win twelve hundred dollars INF-AC You can help me wash the dishes PP-HOW-S He explained to her how she did it HOW-TO-INF Daddy can you tell me how to spell Christmas carols?
NP-S He did not tell me that it was gonna cost me five dollars CDS ING-PP Stop throwing a tantrum NP-AS-NP I sent him as a messenger NP-WH-S I?ll tell you whether you can take it off IT WHS, SUBTYPE IF How would you like it if she pulled your hair?
NP-PP-PP He turned it from a disaster into a victory Table 5: Typical SCFs with higher ranks in (i) CHILD and (ii) CDS.
Measures Group1 Group2 Group3 SCF similarity F-measure 84.7 72.17 75.60 SCF accuracy TPs CDS 12 11 7 TPs CHILD 10 9 8 FPs CDS 36 29 13 FPs CHILD 32 18 15 Table 6: Average results for 3 groups when (i) unfiltered SCF distributions in CHILD and CDS are compared against each other (SCF similarity) and when (ii) the SCFs in the distributions are evaluated against a gold standard (SCF accuracy).
the verbs in these groups take fewer SCFs.
Interestingly, Group 3 is the only group where children produce more TPs and FPs on average than adults do, i.e. both correct and incorrect SCFs which are not exemplified in the adult speech.
The frequency effects controlled, the reason for these differences is likely to lie in the differing relative preferences children and adults have for verbs in groups 2 and 3, which we think may impact the richness of their language.
4.5 Further
Analysis of TP and FP Differences We looked further at the interesting TP and FP differences in Group 3 to investigate whether they tell us something about (i) how children learn SCFs (via both TPs and FPs), and (ii) how the parsing / SCF extraction system could be improved for CHILDES data in the future (via the FPs).
We first made a quantitative analysis of the relative difference in TPs and FPs for all the SCFs in both corpora.
The major finding of this high level analysis was a significantly high FP rate for some ING frames (e.g.
PART-ING-SC, ING-NP-OMIT, NP-INGOC) within CHILD (e.g.
?car going hit??
?I hurt hand moving??.
This agrees with many previous studies, e.g.
(Brown, 1973), which have shown that children overextend and incorrectly use the ?ing??morpheme during early acquisition.
A qualitative analysis of the verbs from Group 3 was then carried out, looking for the following scenarios: ??SCF is a FP in both CHILD and CDS either i) the gold standard is incomplete, or ii) there is error in the parser/subcategorization system with respect to the CHILDES domain.
??SCF is a TP in CDS and not present in CHILD children have not acquired the frame despite exposure to it (perhaps it is complicated to acquire).
??SCF is a TP in CHILD but not present in CDS adults are not using the frame but the children have acquired it.
This indicates that either i) children are acquiring the frame from elsewhere in their environment (perhaps from a television), 38 NP-INF NP-NP INTRANS ADJP PART NP PP PART-NP PART-NP-PP PART-PP PP-PP PP-BASE NP-S NP-PP NP-ADJP NP-NP-up Figure 1: SCFs obtained for the verb shoot or ii) there is a misuse of the verb?s semantic class in child speech.
??SCF is a FP in CHILD but not present in CDS children should not have been exposed to this frame but they have acquired it.
This indicates either i) a misuse of the verb?s semantic class, or ii) error in the parsing/subcategorization technology with respect to the child-speech domain.
These scenarios are illustrated in Figure 1 which graphically depicts the differences in TPs and FPs for the verb shoot.
The SCFs have been arranged in a complexity hierarchy where complexity is defined in terms of increasing argument structure.
8 SCFs
found within our ANLT-COMLEX gold standard lexicon for shoot are indicated in bold-face.
A right-angled rectangle drawn around a SCF indicates that the frame is present in CHILD?a solid line indicating a strong presence (relative frequency > 0.010) and a dotted line indicating a weak presence (relative frequency > 0.005).
Rounded-edge rectangles represent the presence of SCFs within CDS similarly.
For example, the frame NP represents a TP in both CHILD and CDS and the frame NP-NP represents a FP within CHILD.
With reference to Figure 1, we notice that all of the SCFs present in CHILD are directly connected within the hierarchy and there is a tendency for weakly present SCFs to inherit from those strongly present.
A possible explanation for this is that children are exploring SCFs?trying out frames that are slightly more complex than those already acquired (for a learning 8 For instance, the intransitive frame INTRANS is less complex than the transitive frame NP, which in turn is less complex than the di-transitive frame NP-NP.
For a detailed description of all SCFs see (Korhonen, 2002).
algorithm that exploits such a hypothesis in general see (Buttery, 2006)).
The SCF NP-NP is strongly present in CHILD despite being a FP.
Inspection of the associated utterances reveals that some instances NP-NP are legitimate but so uncommon in adult language that they are omitted from the gold-standard (e.g.
?can i shoot us all to pieces??
However, other instances demonstrate a misunderstanding of the semantic class of the verb; there is possible confusion with the semantic class of send or throw (e.g.
?i shoot him home??.
The frame NP-INF is a FP in both corpora and a frequent FP in CHILD.
Inspection of the associated utterances flags up a parsing problem.
Frame NP-INF can be illustrated by the sentences ?he helped her bake the cake??or ?he made her sing?? however, within CHILD the NP-INF has been acquired from utterances such as ?i want ta shoot him??
The RASP parser has mistagged the word ?ta??leading to a misclassification by the SCF extraction system.
This problem could be solved by augmenting RASP?s current grammar with a lexical entry specifying ?ta??as an alternative to infinitival ?to??
In summary, our analysis of TP and FP differences has confirmed previous studies regarding the nature of child speech (the over-extension of the ?ing??morpheme).
It has also demonstrated that TP/FP analysis can be a useful diagnostic for parsing/subcategorization extraction problems within a new data domain.
Further, we suggest that analysis of FPs can provide empirical data regarding the manner in which children learn the semantic classes of 39 verbs (a matter that has been much debated e.g.
(Levin, 1993), (Brooks and Tomasello, 1999)).
5 Conclusion
We have reported the first experiment for automatically acquiring verbal subcategorization from both child and child-directed parts of the CHILDES database.
Our results show that a state-of-the-art subcategorization acquisition system yields useful results on challenging child language data even without any domain-specific tuning.
It produces data which is accurate enough to confirm and extend several previous research findings in CLA.
We explore the discovery that children and adults have different relative preferences for certain verb types, and that these preferences influence the way children acquire subcategorization.
Our work demonstrates the value of using NLP technology to annotate child language data, particularly where manual annotations are not readily available for research use.
Our pilot study yielded useful information which will help us further improve both parsing and lexical acquisition performance on spoken/child language data.
In the future, we plan to optimize the technology so that it can produce higher quality data for investigation of syntactic complexity in this domain.
Using the improved technology we plan to then conduct a more thorough investigation of the interesting CLA topics discovered in this study?first concentrating on SCF differences in child speech across age ranges.
References B.
Boguraev, J.
Carroll, E.
J. Briscoe, D.
Carter, and C.
Grover. 1987.
The derivation of a grammatically-indexed lexicon from the Longman Dictionary of Contemporary English.
In Proc.
of the 25th Annual Meeting of ACL, pages 193??00, Stanford, CA.
E Briscoe and J Carroll.
1997. Automatic extraction of subcategorization from corpora.
In 5th ACL Conference on Applied Natural Language Processing, pages 356??63, Washington, DC.
ACL. E.
J. Briscoe, J.
Carroll, and R.
Watson. 2006.
The second release of the rasp system.
In Proc.
of the COLING/ACL 2006 Interactive Presentation Sessions, Sydney, Australia.
P Brooks and M Tomasello.
1999. Young children learn to produce passives with nonce verbs.
Developmental Psychology, 35:29??4.
R Brown.
1973. A first Language: the early stages.
Harvard University Press, Cambridge, MA.
L. Burnard, 1995.
The BNC Users Reference Guide.
British National Corpus Consortium, Oxford, May.
P. Buttery and A.
Korhonen. 2005.
Large-scale analysis of verb subcategorization differences between child directed speech and adult speech.
In Proceedings of the Interdisciplinary Workshop on the Identification and Representation of Verb Features and Verb Classes, Saarbrucken, Germany.
P Buttery.
2006. Computational Models for First Language Acquisition.
Ph.D. thesis, University of Cambridge.
E. Charniak.
2000. A maximum-entropy-inspired parser.
In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics, Seattle, WA.
C. Fisher, G.
Hall, S.
Rakowitz, and L.
Gleitman. 1994.
When it is better to receive than to give: syntactic and conceptual constraints on vocabulary growth.
Lingua, 92(1??):333??75, April.
R. Grishman, C.
Macleod, and A.
Meyers. 1994.
COMLEX Syntax: Building a Computational Lexicon.
In Proc.
of COLING, Kyoto.
A. Korhonen and Y.
Krymolowski. 2002.
On the Robustness of Entropy-Based Similarity Measures in Evaluation of Subcategorization Acquisition Systems.
In Proc.
of the 6th CoNLL,pages 91??7, Taipei, Taiwan.
A. Korhonen, Y.
Krymolowski, and E.
J. Briscoe.
2006. A large subcategorization lexicon for natural language processing applications.
In Proc.
of the 5th LREC, Genova, Italy.
A Korhonen.
2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge.
Thesis published as Technical Report UCAM-CL-TR-530.
E Lenneberg.
1967. Biological Foundations of Language.
Wiley Press, New York, NY.
B Levin.
1993. English Verb Classes and Alternations.
Chicago University Press, Chicago, IL.
B. MacWhinney.
2000. The CHILDES Project: Tools for Analyzing Talk.
Lawrence Erlbaum, Mahwah, NJ, 3rd edition.
D. McCarthy and J.
Carroll. 2003.
Disambiguating nouns, verbs, and adjectives using automatically acquired selectional preferences.
Computational Linguistics, 29(4).
L Naigles.
1990. Children use syntax to learn verb meanings.
Journal of Child Language, 17:357??74.
S Pinker.
1994. The Language Instinct: How the Mind Creates Language.
Harper Collins, New York, NY.
J. Preiss, E.
J. Briscoe, and A.
Korhonen. 2007.
A system for large-scale acquisition of verbal, nominal and adjectival subcategorization frames from corpora.
In Proceedings of the 45th Annual Meeting of ACL, Prague, Czech Republic.
To appear.
K. Sagae, A.
Lavie, and B.
MacWhinney. 2005.
Automatic measurement of syntactic development in child langugage.
In Proceedings of the 42nd Meeting of the Association for Computational Linguistics, Ann Arbor, Michigan.
S. Schulte im Walde.
2006. Experiments on the automatic induction of german semantic verb classes.
Computational Linguistics, 32(2):159??94.
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 41??8, Prague, Czech Republic, June 2007 c2007 Association for Computational Linguistics A Cognitive Model for the Representation and Acquisition of Verb Selectional Preferences Afra Alishahi Department of Computer Science University of Toronto afra@cs.toronto.edu Suzanne Stevenson Department of Computer Science University of Toronto suzanne@cs.toronto.edu Abstract We present a cognitive model of inducing verb selectional preferences from individual verb usages.
The selectional preferences for each verb argument are represented as a probability distribution over the set of semantic properties that the argument can possess?a semantic profile.
The semantic profiles yield verb-specific conceptualizations of the arguments associated with a syntactic position.
The proposed model can learn appropriate verb profiles from a small set of noisy training data, and can use them in simulating human plausibility judgments and analyzing implicit object alternation.
1 Introduction
Verbs have preferences for the semantic properties of the arguments filling a particular role.
For example, the verb eat expects that the object receiving its theme role will have the property of being edible, among others.
Learning verb selectional preferences is an important aspect of human language acquisition, and the acquired preferences have been shown to guide children?s expectations about missing or upcoming arguments in language comprehension (Nation et al., 2003).
Resnik (1996) introduced a statistical approach to learning and use of verb selectional preferences.
In this framework, a semantic class hierarchy for words is used, together with statistical tools, to induce a verb?s selectional preferences for a particular argument position in the form of a distribution over all the classes that can occur in that position.
Resnik?s model was proposed as a model of human learning of selectional preferences that made minimal representational assumptions; it showed how such preferences could be acquired from usage data and an existing conceptual hierarchy.
However, his and later computational models (see Section 2) have properties that do not match with certain cognitive plausibility criteria for a child language acquisition model.
All these models use the training data in ?batch mode?? and most of them use information theoretic measures that rely on total counts from a corpus.
Therefore, it is not clear how the representation of selectional preferences could be updated incrementally in these models as the person receives more data.
Moreover, the assumption that children have access to a full hierarchical representation of semantic classes may be too strict.
We propose an alternative view in this paper which is more plausible in the context of child language acquisition.
In previous work (Alishahi and Stevenson, 2005), we have proposed a usage-based computational model of early verb learning that uses Bayesian clustering and prediction to model language acquisition and use.
Individual verb usages are incrementally grouped to form emergent classes of linguistic constructions that share semantic and syntactic properties.
We have shown that our Bayesian model can incrementally acquire a general conception of the semantic roles of predicates based only on exposure to individual verb usages (Alishahi and Stevenson, 2007).
The model forms probabilistic associations between the semantic properties of arguments, their syntactic positions, and the semantic primitives 41 of verbs.
Our previous experiments demonstrated that, initially, this probability distribution for an argument position yields verb-specific conceptualizations of the role associated with that position.
As the model is exposed to more input, the verb-based roles gradually transform into more abstract representations that reflect the general properties of arguments across the observed verbs.
A shortcoming of the model was that, because the prediction of the semantic roles was based only on the groupings of verbs, it could not make use of verb-specific knowledge in generating expectations about a particular verb?s arguments.
That is, once it was exposed to a range of verbs, it no longer had access to the verb-specific information, only to generalizations over clusters of verbs.
In this paper, we propose a new version of our model that, in addition to learning general semantic roles for constructions, can use its verb-specific knowledge to predict intuitive selectional preferences for each verb argument position.
We introduce a new notion, a verb semantic profile, as a probability distribution over the semantic properties of an argument for each verb.
A verb semantic profile is predicted from both the verb-based and the construction-based knowledge that the model has learned through clustering, and reflects the properties of the arguments that are observed for that verb.
Our proposed prediction model makes appropriate generalizations over the observed properties, and captures expectations about previously unseen arguments.
As in other work on selectional preferences, the semantic properties that we use in our representation of arguments are drawn from a standard lexical ontology (WordNet; Miller, 1990), but we do not require knowledge of the hierarchical structure of the WordNet concepts.
From the computational point of view, this makes use of an available resource, while from the cognitive view, this avoids ad hoc assumptions about the representation of a conceptual hierarchy.
However, we do require some properties to be more general (i.e., shared by more words) than others, which eventually enables the model to make appropriate generalizations.
Otherwise, the selected semantic properties are not fundamental to the model, and could in the future be replaced with an approach that is deemed more appropriate to child language acquisition.
Each argument contributes to the semantic profile of the verb through its (potentially large) set of semantic properties instead of its membership in a single class.
As input to our model, we use an automatically parsed corpus, which is very noisy.
However, as a result of our novel representation, the model can induce and use selectional preferences using a relatively small set of noisy training data.
2 Related
Computational Models A variety of computational models for verb selectional preferences have been proposed, which use different statistical models to induce the preferences of each verb from corpus data.
Most of these models, however, use the same representation for verb selectional preferences: the preference can be thought of as a mapping, with respect to an argument position for a verb, of each class to a real number (Light and Greiff, 2002).
The induction of a verb?s preferences is, therefore, modeled as using a set of training data to estimate that number.
Resnik (1996) defines the selectional preference strength of a verb as the divergence between two probability distributions: the prior probabilities of the classes, and the posterior probabilities of the classes given that verb.
The selectional association of a verb with a class is also defined as the contribution of that class to the total selectional preference strength.
Resnik estimates the prior and posterior probabilities based on the frequencies of each verb and its relevant argument in a corpus.
Li and Abe (1998) model selectional preferences of a verb (for an argument position) as a set of nodes in the semantic class hierarchy with a probability distribution over them.
They use the Minimum Description Length (MDL) principle to find the best set for each verb and argument based on the usages of that verb in the training data.
Clark and Weir (2002) also find an appropriate set of concept nodes to represent the selectional preferences for a verb, but do so using a ?2 test over corpus frequencies mapped to concepts to determine when to generalize from a node to its parent.
Ciaramita and Johnson (2000) use a Bayesian network with the same topology as WordNet to estimate the probability distribution of the relevant set of nodes in the hierarchy.
Abney 42 and Light (1999) use a different representational approach: they train a separate hidden Markov model for each verb, and the selectional preference is represented as a probability distribution over words instead of semantic classes.
3 The
Bayesian Verb-Learning Model 3.1 Overview of the Model Our model learns the set of argument structure frames for each verb, and their grouping across verbs into constructions.
An argument structure frame is a set of features of a verb usage that are both syntactic (the number of arguments, the syntactic pattern of the usage) and semantic (the semantic properties of the verb, the semantic properties of each argument).
The syntactic pattern indicates the word order of the verb and arguments.
A construction is a grouping of individual frames which probabilistically share syntactic and semantic features, and form probabilistic associations across verb semantic properties, argument semantic properties, and syntactic pattern.
These groupings typically correspond to general constructions in the language such as transitive, intransitive, and ditransitive.
For each verb, the model associates an argument position with a probability distribution over a set of semantic properties?a semantic profile.
In doing so, the model uses the knowledge that it has learned for that verb, as well as the grouping of frames for that verb into constructions.
The semantic properties of words are taken from WordNet (version 2.0) as follows.
We extract all the hypernyms (ancestors) for all the senses of the word, and add all the words in the hypernym synsets to the list of the semantic properties.
Figure 1 shows an example of the hypernyms for dinner, and its resulting set of semantic properties.1 The following sections review basic properties of the model from Alishahi and Stevenson (2005, 2007), and introduce extensions that give the model its ability to make verb-based predictions.
3.2 Learning
as Bayesian Clustering Each argument structure frame for an observed verb usage is input to an incremental Bayesian clustering 1We do not remove alternate spellings of a term in WordNet; this will be seen in the profiles in the results section.
Sense 1 dinner => meal, repast => nutriment, nourishment, nutrition, sustenance, aliment, alimentation, victuals => food, nutrient => substance, matter => entity Sense 2 dinner, dinner party => party => social gathering, social affair => gathering, assemblage => social group => group, grouping dinner: {meal, repast, nutriment, nourishment, nutrition, substance, aliment, alimentation, victuals, food, nutrient, substance, matter, entity, party, social gathering, social affair, gathering, assemblage, social group, group, grouping } Figure 1: Semantic properties for dinner from WordNet process.
This process groups the new frame together with an existing group of frames?a construction?? that probabilistically has the most similar semantic and syntactic properties to it.
If no construction has sufficiently high probability for the new frame, then a new construction is created for it.
We use the probabilistic model of Alishahi and Stevenson (2007) for learning constructions, which is itself an adaptation of a Bayesian model of human categorization proposed by Anderson (1991).
It is important to note that the categories (i.e., constructions) are not predefined, but rather are created according to the patterns of similarity over observed frames.
Grouping a frame F with other frames participating in construction k is formulated as finding the k with the maximum probability given F: BestConstruction(F) = argmax k P(k|F) (1) where k ranges over the indices of all constructions, with index 0 representing recognition of a new construction.
Using Bayes rule, and dropping P(F) which is constant for all k: P(k|F) = P(k)P(F|k)P(F) ??P(k)P(F|k) (2) The prior probability, P(k), indicates the degree of entrenchment of construction k, and is given by the relative frequency of its frames over all observed frames.
The posterior probability of a frame F is expressed in terms of the individual probabilities of its features, which we assume are independent, thus yielding a simple product of feature probabilities: 43 P(F|k) = productdisplay i?FrameFeatures Pi(j|k) (3) where j is the value of the ith feature of F, and Pi(j|k) is the probability of displaying value j on feature i within construction k.
Given the focus here on semantic profiles, we next focus on the calculation of the probabilities of semantic properties.
3.3 Probabilities
of Semantic Properties The probability in equation (3) of value j for feature i in construction k is estimated using a smoothed version of this maximum likelihood formula: Pi(j|k) = count ki (j) nk (4) where nk is the number of frames participating in construction k, and countki (j) is the number of those with value j for feature i.
For most features, countki (j) is calculated by simply counting those members of construction k whose value for feature i exactly matches j.
However, for the semantic properties of words, counting only the number of exact matches between the sets is too strict, since even highly similar words very rarely have the exact same set of properties.
We instead use the following Jaccard similarity score to measure the overlap between the set of semantic properties, SF, of a particular argument in the frame to be clustered, and the set of semantic properties, Sk, of the same argument in a member frame of a construction: sem score(SF,Sk) = |SF ?Sk||S F ?Sk| (5) For example, assume that the new frame F represents a usage of John ate cake.
In the construction that we are considering for inclusion of F, one of the member frames represents a usage of Mom got water.
We must compare the semantic properties of the corresponding arguments cake and water: cake: {baked goods,food,solid,substance,matter,entity} water: {liquid,fluid,food,nutrient,substance,matter,entity} The intersection of the two sets is {food, substance, matter, entity}, yielding a sem score of 49.
In general, to calculate the conditional probability for the set of semantic properties, we set countki (j) in equation (4) to the sum of the sem score?s for the new frame and every member of construction k, and normalize the resulting probability over all possible sets of semantic properties in our lexicon.
3.4 Predicting
Semantic Profiles for Verbs We represent the selectional preferences of a verb for an argument position as a semantic profile, which is a probability distribution over all the semantic properties.
To predict the profile of a verb v for an argument position arg, we need to estimate the probability of each semantic property j separately: Parg(j|v) = summationdisplay k Parg(j,k|v) (6) ?? summationdisplay k P(k,v)Parg(j|k,v) Here, j ranges over all the possible semantic properties that an argument can have, and k ranges over all constructions.
The prior probability of having verb v in construction k, or P(k,v), takes into account two important factors: the relative entrenchment of the construction k, and the (smoothed) frequency with which v participates in k.
The posterior probability Parg(j|k,v) is calculated analogously to Pi(j|k) in equation (4), but limiting the count of matching features to those frames in k that contain v: Parg(j|k,v) = verb count k arg(j,v) nkv (7) where nkv is the number of frames for v participating in construction k, and verb countkarg(j,v) is the number of those with semantic property j for argument arg.
We use a smoothed version of the above formula, where the relative frequency of each property j among all nouns is used as the smoothing factor.
3.5 Verb-Argument Compatibility In one of our experiments, we need to measure the compatibility of a particular noun n for an argument position arg of some verb v.
That is, we need to estimate how much the semantic properties of n conform to the acquired semantic profile of v for arg.
We formulate the compatibility as the conditional probability of observing n as an argument arg of v: compatibility(v,n) = log(Parg(jn|v)) (8) 44 where jn is the set of the semantic properties for word n, and Parg(jn|v) is estimated as in equation (7).
However, since jn here is a set of properties (as opposed to j in equation (7) being a single property), verb countkarg in equation (7) should be modified as described in Section 3.3: we set verb countkarg(jn,v) to the sum of the sem score?s (equation (5)) for jn and every frame of v that participates in construction k.
4 Experimental
Results In the following sections, we first describe the training data for our model.
In accordance with other computational models, we focus here on the verb preferences for the direct object position.2 Next, we provide a qualitative analysis of our model through examination of the semantic profiles for a number of verbs.
We then evaluate our model through two tasks of simulating verb-argument plausibility judgment, and analyzing the implicit object alternation, following Resnik (1996).3 4.1 The Training Data In earlier work (Alishahi and Stevenson, 2005, 2007), we used a method to automatically generate training data with the same distributional properties as the input children receive.
However, this relies on manually-compiled data about verbs and their argument structure frames from the CHILDES database (MacWhinney, 1995).
To evaluate the new version of our model for the task of learning selectional preferences, we need a wide selection of verbs and their arguments that is impractical to compile by hand.
The training data for our experiments here are generated as follows.
We use 20,000 sentences randomly selected from the British National Corpus (BNC),4 automatically parsed using the Collins parser (Collins, 1999), and further processed with TGrep2,5 and an NP-head extraction software.6 For 2To our knowledge, the only work that considers selectional preferences of subjects and prepositional phrases as well as direct objects is Brockmann and Lapata (2003).
3Computational models of verb selectional preference have been evaluated through disambiguation tasks (Li and Abe, 1998; Abney and Light, 1999; Ciaramita and Johnson, 2000; Clark and Weir, 2002), but for to evaluate our cognitive model, the experiments from Resnik (1996) are the most interesting.
4http://www.natcorp.ox.ac.uk 5http://tedlab.mit.edu/?dr/Tgrep2 6The software was provided to us by Eric Joanis, and Afeach verb usage in a sentence, we construct a frame by recording the verb in root form, the number of the arguments for that verb, and the syntactic pattern of the verb usage (i.e., the word order of the verb and the arguments).
We also record in the frame the semantic properties of the verb and each of the argument heads (each noun is also converted to root form); these properties are extracted from WordNet (as discussed in Section 3.1 and illustrated in Figure 1).
This process results in 16,300 frames which serve as input data to our learning model.
4.2 Formation
of Semantic Profiles for Verbs After training our model on the above data, we use equation (7) to predict the semantic profile of the direct object position for a range of verbs.
Some of these verbs, such as write and sing, have strong selectional preferences, whereas others, such as want and put, can take a wide range of nouns as direct object (as confirmed by Resnik?s (1996) estimated strength of selectional preference for these verbs).
The semantic profiles for write and sing are displayed in Figure 2, and the profiles for want and put are displayed in Figure 3.
(Due to limited space, we only include the 25 properties that have the highest probability in each profile.) Because we extract the semantic properties of words from WordNet, which has a hierarchical structure, the properties that come from nodes in the higher levels of the hierarchy (such as entity and abstraction) appear as the semantic property for a very large set of words, whereas the properties that come from the leaves in the hierarchy are specific to a small set of words.
Therefore, the general properties are more likely to be associated with a higher probability in the semantic profiles for most verbs.
In fact, a closer look at the semantic profiles for want and put reveals that the top portion of the semantic profile for these verbs consists solely of such general properties that are shared among a large group of words.
However, this is not the case for the more restrictive verbs.
The semantic profiles for write and sing show that the specific properties that these verbs demand from their direct object appear amongst the highest-ranked properties, even though only a small set of words share these properties (e.g., content, saneh Fazly helped us in using the above-mentioned tools for generating our input corpora.
45 write (0.024) abstraction (0.022) entity (0.021) location (0.020) substance (0.019) destination (0.018) relation (0.015) communication (0.015) social relation (0.013) content (0.011) message (0.011) subject matter (0.011) written communication (0.011) written language (0.010) object (0.010) physical object (0.010) writing (0.010) goal (0.010) unit (0.009) whole (0.009) whole thing (0.009) artifact (0.009) artefact (0.009) state (0.009) amount (0.009) measure sing (0.020) abstraction (0.015) relation (0.015) communication (0.015) social relation (0.013) act (0.013) human action (0.013) human activity (0.013) auditory communication (0.012) music (0.010) entity (0.010) piece (0.009) composition (0.009) musical composition (0.009) opus (0.009) piece of music (0.009) psychological feature (0.008) cognition (0.008) knowledge (0.008) noesis (0.008) activity (0.008) content (0.008) grouping (0.008) group (0.008) amount (0.008) measure Figure 2: Semantic profiles of write and sing for the direct object position.
message, written communication, written language,...
for write, and auditory communication, music, musical composition, opus, ...
for sing).
The examination of the semantic profiles for fairly frequent verbs in the training data shows that our model can use the verb usages to predict an appropriate semantic profile for each verb.
When presented with a novel verb (for which no verb-based information is available), equation (7) predicts a semantic profile which reflects the relative frequencies of the semantic properties among all words (due to the smoothing factor added to equation (7)), modulated by the prior probability of each construction.
The predicted profile is displayed in Figure 4.
It shows similarities with the profiles for want and put in Figure 3, but the general properties in this profile have an even higher probability.
Since the profile for the novel verb is predicted in the absence of any evidence (i.e., verb usage) in the training data, we later use it as the base for estimating other verbs??strength of selectional preference.
want (0.016) entity (0.015) object (0.015) physical object (0.014) abstraction (0.013) act (0.012) human action (0.012) human activity (0.012) relation (0.011) unit (0.011) whole (0.011) whole thing (0.011) artifact (0.011) artefact (0.008) communication (0.008) social relation (0.008) activity (0.007) cause (0.007) state (0.007) instrumentality (0.007) instrumentation (0.007) event (0.006) being (0.006) living thing (0.006) animate thing (0.006) organism put (0.015) entity (0.015) object (0.013) physical object (0.013) abstraction (0.011) unit (0.011) whole (0.011) whole thing (0.011) artifact (0.011) artefact (0.010) act (0.009) relation (0.008) human action (0.008) human activity (0.008) communication (0.008) social relation (0.007) substance (0.007) content (0.007) instrumentality (0.007) instrumentation (0.007) measure (0.006) amount (0.006) quantity (0.006) cause (0.006) causal agent (0.006) causal agency Figure 3: Semantic profiles of want and put for the direct object position.
4.3 Verb-Argument Plausibility Judgments Holmes et al.(1989) evaluate verb argument plausibility by asking human subjects to rate sentences like The mechanic warned the driver and The mechanic warned the engine.
Resnik (1996) used this data to assess the performance of his model by comparing its judgments of selectional fit against the plausibility ratings elicited from human subjects.
He showed that his selectional association measure for a verb and its direct object can be used to select the more plausible verb-noun pair among the two (e.g., <warn,driver> vs.
<warn,engine> in the previous example).
That is, a higher selectional association between the verb and one of the nouns compared to the other noun indicates that the former is the more plausible pair.
Resnik (1996) used the Brown corpus as training data, and showed that his model arrives at the correct ordering of more and less plausible arguments in 11 of the 16 cases.
We repeated this experiment, using the same 16 pairs of verb-noun combinations.
For each pair of <v,n1> and <v,n2>, we calculate the compatibility measure using equation (8); these values are shown in Figure 5.
(Note that because these are 46 A novel verb (0.021) entity (0.017) object (0.017) physical object (0.015) abstraction (0.010) act (0.010) human action (0.010) human activity (0.010) unit (0.009) whole (0.009) whole thing (0.009) artifact (0.009) artefact (0.009) being (0.009) living thing (0.009) animate thing (0.009) organism (0.008) cause (0.008) causal agent (0.008) causal agency (0.008) relation (0.008) person (0.008) individual (0.008) someone (0.008) somebody (0.008) mortal Figure 4: Semantic profile of a novel verb for the direct object position.
log-probabilities and therefore negative numbers, a lower absolute value of compatibility(v,n) shows a better compatibility between the verb v and the argument n).
For example, <see,friend> has a higher compatibility score (-30.50) than <see,method> (-32.14).
Similar to Resnik, our model detects 11 plausible pairs out of 16.
However, these results are reached with a much smaller training corpus (around 500,000 words), compared to the Brown corpus used by Resnik (1996) which contains one million words.
Moreover, whereas the Brown corpus is tagged and parsed manually, the portion of the BNC that we use is parsed automatically, and as a result our training data is very noisy.
Nonetheless, the model achieves the same level of accuracy in distinguishing plausible verb-argument pairs from implausible ones.
4.4 Implicit
Object Alternations In English, some inherently transitive verbs can appear with or without their direct objects (e.g., John ate his dinner as well as John ate), but others cannot (e.g., Mary made a cake but not *Mary made).
It is argued that implicit object alternations involve a Verb Plausible Implausible see friend -30.50 method -32.14 read article -32.76 fashion -33.33 find label -32.05 fever -33.30 hear story -32.11 issue -32.40 write letter -31.37 market -32.46 urge daughter -36.73 contrast -35.64 warn driver -33.68 engine -34.42 judge contest -39.05 climate -38.23 teach language -45.64 distance -45.11 show sample -31.75 travel -31.42 expect visit -33.88 mouth -32.87 answer request -31.89 tragedy -33.95 recognize author -32.53 pocket -32.62 repeat comment -33.80 journal -33.97 understand concept -32.25 session -32.93 remember reply -33.79 smoke -34.29 Figure 5: Compatibility scores for plausible vs.
implausible verb-noun pairs.
particular relationship between the verb and its argument.
In particular, for verbs that participate in the implicit object alternation, the omitted object must be in some sense inferable or typical for that verb (Levin, 1993, among others).
Resnik (1996) used his model of selectional preferences to analyze implicit object alternations, and showed a relationship between his measure of selectional preference strength and the notion of typicality of an object.
He calculated this measure for two groups of Alternating and Non-alternating verbs, and showed that, on average, the Alternating verbs have a higher strength of selectional preference for the direct object than the Non-alternating verbs.
However, there was no threshold separating the two groups of verbs.
To repeat Resnik?s experiment, we need a measure of how ?strongly constraining??a semantic profile is.
We can do this by measuring the similarity between the semantic profile we generate for the object of a particular verb and some ?default??notion of the argument for that position across all verbs.
We use the semantic profile predicted for the object position of a novel verb, shown earlier in Figure 4, as the default profile for that argument position.
Because this profile is predicted in the absence of any evidence in the training data, it makes the minimum assumptions about the properties of the argument and thus serves as a suitable default.
We then assume that verbs with weaker selectional preferences have semantic profiles more similar to the default profile 47 Alternating verbs Non-alternating verbs write 0.61 hang 0.56 sing 0.67 wear 0.71 drink 0.67 say 0.75 eat 0.74 catch 0.76 play 0.74 show 0.77 pour 0.76 make 0.78 watch 0.77 hit 0.78 pack 0.78 open 0.81 steal 0.80 take 0.83 push 0.80 see 0.87 call 0.80 like 0.87 pull 0.80 get 0.87 explain 0.81 find 0.87 read 0.82 give 0.88 hear 0.87 bring 0.89 want 0.89 put 0.90 Mean: 0.76 Mean: 0.81 Figure 6: Similarity with the base profile for Alternating and Non-alternating verbs.
than verbs with stronger preferences.
We use the cosine measure to estimate the similarity between two profiles p and q: cosine(p,q) = pq||p||||q|| (9) The similarity values for the Alternating and Nonalternating verbs are shown in Figure 6.
The larger values represent more similarity with the base profile, which means a weaker selectional preference.
The means for the Alternating and Non-alternating verbs were respectively 0.76 and 0.81, which confirm the hypothesis that verbs participating in implicit object alternations select more strongly for the direct objects than verbs that do not.
However, like Resnik (1996), we find that it is not possible to set a threshold that will distinguish the two sets of verbs.
5 Conclusions
We have proposed a cognitively plausible model for learning selectional preferences from instances of verb usage.
The model represents verb selectional preferences as a semantic profile, which is a probability distribution over the semantic properties that an argument can take.
One of the strengths of our model is the incremental nature of its learning mechanism, in contrast to other approaches which learn selectional preferences in batch mode.
Here we have only reported the results for the final stage of learning, but the model allows us to monitor the semantic profiles during the course of learning, and compare it with child data for different age groups, as we do with semantic roles (Alishahi and Stevenson, 2007).
We have shown that the model can predict appropriate semantic profiles for a variety of verbs, and use these profiles to simulate human judgments of verbargument plausibility, using a small and highly noisy set of training data.
The model can also use the profiles to measure verb-argument compatibility, which was used in analyzing the implicit object alternation.
References Abney, S.
and Light, M.
(1999). Hiding a semantic hierarchy in a Markov model.
In Proc.
of the ACL Workshop on Unsupervised Learning in Natural Language Processing.
Alishahi, A.
and Stevenson, S.
(2005). A probabilistic model of early argument structure acquisition.
In Proc.
of the CogSci 2005.
Alishahi, A.
and Stevenson, S.
(2007). A computational usage-based model for learning general properties of semantic roles.
In Proc.
of the EuroCogSci 2007.
Anderson, J.
R. (1991).
The adaptive nature of human categorization.
Psychological Review, 98(3):409??29.
Brockmann, C.
and Lapata, M.
(2003). Evaluating and combining approaches to selectional preference acquisition.
In Proc.
of the EACL 2003.
Ciaramita, M.
and Johnson, M.
(2000). Explaining away ambiguity: Learning verb selectional preference with Bayesian networks.
In Proc.
of the COLING 2000.
Clark, S.
and Weir, D.
(2002). Class-based probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187??06.
Collins, M.
(1999). Head-Driven Statistical Models for Natural Language Parsing.
PhD thesis, University of Pennsylvania.
Holmes, V.
M., Stowe, L., and Cupples, L.
(1989). Lexical expectations in parsing complement-verb sentences.
Journal of Memory and Language, 28:668??89.
Levin, B.
(1993). English verb classes and alternations: A preliminary investigation.
The University of Chicago Press.
Li, H.
and Abe, N.
(1998). Generalizing case frames using a thesaurus and the MDL principle.
Computational Linguistics, 24(2):217??44.
Light, M.
and Greiff, W.
(2002). Statistical models for the induction and use of selectional preferences.
Cognitive Science, 26(3):269??81.
MacWhinney, B.
(1995). The CHILDES project: Tools for analyzing talk.
Lawrence Erlbaum.
Miller, G.
(1990). WordNet: An on-line lexical database.
International Journal of Lexicography, 17(3).
Nation, K., Marshall, C.
M., and Altmann, G.
T. M.
(2003). Investigating individual differences in children?s real-time sentence comprehension using language-mediated eye movements.
J. of Experimental Child Psych., 86:314??29.
Resnik, P.
(1996). Selectional constraints: An information-theoretic model and its computational realization.
Cognition, 61:127??99 .
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 49??6, Prague, Czech Republic, June 2007 c2007 Association for Computational Linguistics ISA meets Lara: An incremental word space model for cognitively plausible simulations of semantic learning Marco Baroni CIMeC (University of Trento) C.so Bettini 31 38068 Rovereto, Italy marco.baroni@unitn.it Alessandro Lenci Department of Linguistics University of Pisa Via Santa Maria 36 56126 Pisa, Italy alessandro.lenci@ilc.cnr.it Luca Onnis Department of Psychology Cornell University Ithaca, NY 14853 lo35@cornell.edu Abstract We introduce Incremental Semantic Analysis, a fully incremental word space model, and we test it on longitudinal child-directed speech data.
On this task, ISA outperforms the related Random Indexing algorithm, as well as a SVD-based technique.
In addition, the model has interesting properties that might also be characteristic of the semantic space of children.
1 Introduction
Word space models induce a semantic space from raw textual input by keeping track of patterns of co-occurrence of words with other words through a vectorial representation.
Proponents of word space models such as HAL (Burgess and Lund, 1997) and LSA (Landauer and Dumais, 1997) have argued that such models can capture a variety of facts about human semantic learning, processing, and representation.
As such, word space methods are not only increasingly useful as engineering applications, but they are also potentially promising for modeling cognitive processes of lexical semantics.
However, to the extent that current word space models are largely non-incremental, they can hardly accommodate how young children develop a semantic space by moving from virtually no knowledge of the language to reach an adult-like state.
The family of models based on singular value decomposition (SVD) and similar dimensionality reduction techniques (e.g., LSA) first construct a full cooccurrence matrix based on statistics extracted from the whole input corpus, and then build a model at once via matrix algebra operations.
Admittedly, this is hardly a plausible simulation of how children learn word meanings incrementally by being exposed to short sentences containing a relatively small number of different words.
The lack of incrementality of several models appears conspicuous especially given their explicit claim to solve old theoretical issues about the acquisition of language (e.g., (Landauer and Dumais, 1997)).
Other extant models display some degree if incrementality.
For instance, HALandRandomIndexing(KarlgrenandSahlgren, 2001) can generate well-formed vector representations at intermediate stages of learning.
However, they lack incrementality when they make use of stop word lists or weigthing techniques that are based on whole corpus statistics.
For instance, consistently with the HAL approach, Li et al.(2000) first build a word co-occurrence matrix, and then compute the variance of each column to reduce the vector dimensions by discarding those with the least contextual diversity.
Farkas and Li (2000) and Li et al.(2004) propose an incremental version of HAL by using a a recurrent neural network trained with Hebbian learning.
The networks incrementally build distributional vectors that are then used to induce word semantic clusters with a Self-Organizing Map.Farkas and Li (2000) does not contain any evaluation of the structure of the semantic categories emerged in the SOM.
A more precise evaluation is instead performed by Li et al.(2004), revealing the model?s ability to simulate interesting aspects of early vocabulary dynamics.
However, this is achieved by using hybrid word 49 representations, in which the distributional vectors are enriched with semantic features derived from WordNet.
Borovsky and Elman (2006) also model word learning in a fairly incremental fashion, by using the hidden layer vectors of a Simple Recurrent Network as word representations.
The network is probed at different training epochs and its internal representations are evaluated against a gold standard ontologyofsemanticcategoriestomonitortheprogressin word learning.
Borovsky and Elman (2006)?s claim that their model simulates relevant aspects of child word learning should probably be moderated by the fact that they used a simplified set of artificial sentences as training corpus.
From their simulations it is thus difficult to evaluate whether the model would scale up to large naturalistic samples of language.
In this paper, we introduce Incremental Semantic Indexing (ISA), a model that strives to be more developmentally plausible by achieving full incrementality.
We test the model and some of its less incremental rivals on Lara, a longitudinal corpus of childdirected speech based on samples of child-adult linguistic interactions collected regularly from 1 to 3 years of age of a single English child.
ISA achieves the best performance on these data, and it learns a semantic space that has interesting properties for our understanding of how children learn and structure word meaning.
Thus, the desirability of incrementality increases as the model promises to capture specific developmental trajectories in semantic learning.
The plan of the paper is as follows.
First, we introduce ISA together with its main predecessor, Random Indexing.
Then, we present the learning experiments in which several versions of ISA and other models are trained to induce and organize lexical semantic information from child-directed speech transcripts.
Lastly, we discuss further work in developmental computational modeling using word space models.
2 Models
2.1 Random Indexing Since the model we are proposing can be seen as a fully incremental variation on Random Indexing (RI), we start by introducing the basic features of RI (Karlgren and Sahlgren, 2001).
Initially, each context word is assigned an arbitrary vector representation of fixed dimensionality d made of a small number of randomly distributed +1 and -1, with all other dimensions assigned a 0 value (d is typically much smaller than the dimensionality of the full cooccurrence matrix).
This vector representation is called signature.
The context-dependent representation for a given target word is then obtained by adding the signatures of the words it co-occurs with to its history vector.
Multiplying the history by a small constant called impact typically improves RI performance.
Thus, at each encounter of target word t with a context word c, the history of t is updated as follows: ht += isc (1) where i is the impact constant, ht is the history vector of t and sc is the signature vector of c.
In this way, the history of a word keeps track of the contexts in which it occurred.
Similarity among words is then measured by comparing their history vectors, e.g., measuring their cosine.
RI is an extremely efficient technique, since it directly builds and updates a matrix of reduced dimensionality (typically, a few thousands elements), instead of constructing a full high-dimensional cooccurrence matrix and then reducing it through SVD or similar procedures.
The model is incremental to the extent that at each stage of corpus processing the vector representations are well-formed and could be used to compute similarity among words.
However, RI misses the ?second order??effects that are claimed to account, at least in part, for the effectiveness of SVD-based techniques (Manning and Schutze, 1999, 15.4).
Thus, for example, since different random signatures are assigned to the words cat, dog and train, the model does not capture the fact that the first two words, but not the third, should count as similar contexts.
Moreover, RI is not fully incremental in several respects.
First, on each encounter of two words, the same fixed random signature of one of them is added to the history of the other, i.e., the way in which a word affects another does not evolve with the changes in the model?s knowledge about the words.
Second, RI makes use of filtering and weighting procedures that rely on 50 global statistics, i.e., statistics based on whole corpus counts.
These procedures include: a) treating the most frequent words as stop words; b) cutting off the lowest frequency words as potential contexts; and c) using mutual information or entropy measures to weight the effect of a word on the other).
In addition, although procedures b) and c) may have some psychological grounding, procedure a) would implausibly entail that to build semantic representations the child actively filters out high frequency words as noise from her linguistic experience.
Thus, as it stands RI has some noticeable limitations as a developmental model.
2.2 Incremental
Semantic Analysis Incremental Semantic Analysis (ISA) differs from RI in two main respects.
First and most importantly, when a word encounters another word, the history vector of the former is updated with a weighted sum of the signature and the history of the latter.
This corresponds to the idea that a target word is affected not only by its context words, but also by the semantic information encoded by that their distributional histories.
In this way, ISA can capture SVDlike second order effects: cat and dog might work like similar contexts because they are likely to have similar histories.
More generally, this idea relies on two intuitively plausible assumptions about contextual effects in word learning, i.e., that the information carried by a context word will change as our knowledge about the word increases, and that knowing about the history of co-occurrence of a context word is an important part of the information being contributed by the word to the targets it affects.
Second, ISA does not rely on global statistics for filtering and weighting purposes.
Instead, it uses a weighting scheme that changes as a function of the frequency of the context word at each update.
This makes the model fully incremental and (together with the previous innovation) sensitive not only to the overall frequency of words in the corpus, but to the order in which they appear.
Moreexplicitly, ateachencounterofatargetword t with a context word c, the history vector of t is updated as follows: ht += i(mchc + (1?mc)sc) (2) The constant i is the impact rate, as in the RI formula (1) above.
The valuemc determines how much the history of a word will influence the history of another word.
The intuition here is that frequent words tend to co-occur with a lot of other words by chance.
Thus, the more frequently a word is seen, the less informative its history will be, since it will reflect uninteresting co-occurrences with all sorts of words.
ISA implements this by reducing the influence that thehistoryofacontextwordchasonthetargetword t as a function of the token frequency of c (notice that the model still keeps track of the encounter with c, by adding its signature to the history of t; it is just the history of c that is weighted down).
More precisely, the m weight associated with a context word c decreases as follows: mc = 1 exp parenleftBigCount(c) km parenrightBig where km is a parameter determining how fast the decay will be.
3 Experimental
setting 3.1 The Lara corpus The input for our experiments is provided by the Child-Directed-Speech (CDS) section of the Lara corpus (Rowland et al., 2005), a longitudinal corpus of natural conversation transcripts of a single child, Lara, between the ages of 1;9 and 3;3.
Lara was the firstborn monolingual English daughter of two White university graduates and was born and brought up in Nottinghamshire, England.
The corpus consists of transcripts from 122 separate recording sessions in which the child interacted with adult caretakers in spontaneous conversations.
The total recording time of the corpus is of about 120 hours, representing one of the densest longitudinal corpora available.
The adult CDS section we used contains about 400K tokens and about 6K types.
We are aware that the use of a single-child corpus may have a negative impact on the generalizations on semantic development that we can draw from the experiments.
On the other hand, this choice has the important advantage of providing a fairly homogeneous data environment for our computational simulations.
In fact, we can abstract from the intrinsic variability characterizing any multi-child corpus, 51 and stemming from differences in the conversation settings, in the adults??grammar and lexicon, etc.
Moreover, whereas we can take our experiments to constitute a (very rough) simulation of how a particular child acquires semantic representations from herspecificlinguisticinput,itisnotclearwhatsimulations based on an ?averages??of different linguistic experiences would represent.
The corpus was part-of-speech-tagged and lemmatized using the CLAN toolkit (MacWhinney, 2000).
The automated output was subsequently checked and disambiguated manually, resulting in very accurate annotation.
In our experiments, we use lemma-POS pairs as input to the word space models (e.g., go-v rather than going, goes, etc.) Thus, we make the unrealistic assumptions that the learner already solved the problem of syntactic categorization and figured out the inflectional morphology of her language.
While a multi-level bootstrapping process in which the morphosyntactic and lexical properties of words are learned in parallel is probably cognitively more likely, it seems reasonable at the current stage of experimentation to fix morphosyntax and focus on semantic learning.
3.2 Model
training We experimented with three word space models: ISA, RI (our implementations in both cases) and the SVD-based technique implemented by the Infomap package.1 Parameter settings may considerably impact the performance of word space models (Sahlgren, 2006).
In a stage of preliminary investigations (not reported here, and involving also other corpora) we identified a relatively small range of values for each parameter of each model that produced promising results, and we focused on it in the subsequent, more systematic exploration of the parameter space.
For all models, we used a context window of five words to the left and five words to the right of the target.
For both RI and ISA, we set signature initialization parameters (determining the random assignment of 0s, +1s and -1s to signature vectors) similar to those described by Karlgren and Sahlgren (2001).
For RI and SVD, we used two stop word filtering lists (removing all function words, and removing the 1http://infomap-nlp.sourceforge.net/ top 30 most frequent words), as well as simulations with no stop word filtering.
For RI and ISA, we used signature and history vectors of 1,800 and 2,400 dimensions (the first value, again, inspired by Karlgren and Sahlgren?s work).
Preliminary experiments with 300 and 900 dimensions produced poor results, especially with RI.
For SVD, we used 300 dimensions only.
This was in part due to technical limitations of the implementation, but 300 dimensions is also a fairly typical choice for SVD-based models such as LSA, and a value reported to produce excellent results in the literature.
More importantly, in unrelated experiments SVD with 300 dimensions and function word filtering achieved state-of-the-art performance (accuracy above 90%) in the by now standard TOEFL synonym detection task (Landauer and Dumais, 1997).
After preliminary experiments showed that both models (especially ISA) benefited from a very low impact rate, the impact parameter i of RI and ISA was set to 0.003 and 0.009.
Finally, km (the ISA parameter determining the steepness of decay of the influence of history as the token frequency of the context word increases) was set to 20 and 100 (recall that a higher km correspond to a less steep decay).
The parameter settings we explored were systematically crossed in a series of experiments.
Moreover, for RI and ISA, given that different random initializations will lead to (slightly) different results, each experiment was repeated 10 times.
Below, we will report results for the best performing models of each type: ISA with 1,800 dimensions, i set to 0.003 and km set to 100; RI with 2,400 dimensions, i set to 0.003 and no stop words; SVD with 300-dimensional vectors and function words removed.
However, it must be stressed that 6 out of the 8 ISA models we experimented with outperformedthebestRImodel(andtheyalloutperformed the best SVD model) in the Noun AP task discussed in section 4.1.
This suggests that the results we report are not overly dependent on specific parameter choices.
3.3 Evaluation
method The test set was composed of 100 nouns and 70 verbs (henceforth, Ns and Vs), selected from the most frequent words in Lara?s CDS section (word frequency ranges from 684 to 33 for Ns, and from 52 3501 to 89 for Vs).
This asymmetry in the test set mirrors the different number of V and N types that occur in the input (2828 Ns vs.
944 Vs).
As a further constraint, we verified that all the words in the test set also appeared among the child?s productions in the corpus.
The test words were unambiguously assigned to semantic categories previously used to model early lexical development and represent plausible early semantic groupings.
Semantic categories for nouns and verbs were derived by combining two methods.
For nouns, we used the ontologies from the Macarthur-Bates Communicative Development Inventories (CDI).2 All the Ns in the test set also appear in the Toddler?s List in CDI.
The noun semantic categories are the following (in parenthesis, we report the number of words per class and an example): ANIMALS REAL OR TOY (19; dog), BODY PARTS (16; nose), CLOTHING (5; hat), FOOD AND DRINK (13; pizza), FURNITURE AND ROOMS (8; table), OUTSIDE THINGS AND PLACES TO GO (10; house), PEOPLE (10; baby), SMALL HOUSEHOLD ITEMS (13; bottle), TOYS (6; pen).
Since categories for verbs were underspecified in the CDI, we used 12 broad verb semantic categories for event types, partly extending those in Borovsky and Elman (2006): ACTION (11; play), ACTION BODY (6; eat), ACTION FORCE (5; pull), ASPECTUAL (6; start), CHANGE (12; open), COMMUNICATION (4; talk), MOTION (5; run), PERCEPTION (6; hear), PSYCH (7; remember), SPACE (3; stand), TRANSFER (6; buy).
It is worth emphasizing that this experimental setting is much more challenging than those that are usually adopted by state-of-the-art computational simulations of word learning, as the ones reported above.
For instance, the number of words in our test set is larger than the one in Borovsky and Elman (2006), and so is the number of semantic categories, both for Ns and for Vs.
Conversely, the Lara corpus is much smaller than the data-sets normally used to train word space models.
For instance, the best results reported by Li et al.(2000) are obtained with an input corpus which is 10 times bigger than ours.
As an evaluation measure of the model performance in the word learning task, we adopted Aver2http://www.sci.sdsu.edu/cdi/ age Precision (AP), recently used by Borovsky and Elman (2006).
AP evaluates how close all members of a certain category are to each other in the semantic space built by the model.
To calculate AP, for each wi in the test set we first extracted the corresponding distributional vector vi produced by the model.
Vectors were used to calculate the pair-wise cosine between each test word, as a measure of their distance in the semantic space.
Then, for each target word wi, we built the list ri of the other test words ranked by their decreasing cosine values with respect to wi.
The ranking ri was used to calculate AP(wi), the Word Average Precision for wi, with the following formula: AP(wi) = 1|C wi| summationdisplay wj?Cwi nwj(Cwi) nwj where Cwi is the semantic category assigned to wi, nwj is the set of words appearing in ri up to the rank occupiedbywj,andnwj(Cwi)isthesubsetofwords in nwj that belong to category Cwi.
AP(wi) calculates the proportion of words that belong to the same category of wi at each rank in ri, and then divides this proportion by the number of words that appear in the category.
AP ranges from 0 to 1: AP(wi) = 1 would correspond to the ideal case in which all the closest words to wi in ri belonged to the same category as wi; conversely, if all the words belonging to categories other than Cwi were closer to wi than the words in Cwi, AP(wi) would approach 0.
We also defined the Class AP for a certain semantic category by simply averaging over the Word AP(wi) for each word in that category: AP(Ci) = summationtextj=|Ci| j=1 AP(wj) |Ci| We adopted AP as a measure of the purity and cohesiveness of the semantic representations produced by the model.
Words and categories for which the model is able to converge on well-formed representations should therefore have higher AP values.
If we define Recall as the number of words in nwj belonging to Cwi divided by the total number of words in Cwi, then all the AP scores reported in our experiments correspond to 100% Recall, since the neighbourhood we used to compute AP(wi) always included all the words in Cwi.
This represents a very 53 Nouns Tokens ISA RI SVD 100k 0.321 0.317 0.243 200k 0.343 0.337 0.284 300k 0.374 0.367 0.292 400k 0.400 0.393 0.306 Verbs 100k 0.242 0.247 0.183 200k 0.260 0.266 0.205 300k 0.261 0.266 0.218 400k 0.270 0.272 0.224 Table 1: Word AP scores for Nouns (top) and Verbs (bottom).
For ISA and RI, scores are averaged across 10 iterations stringentevaluationconditionforourmodels,farbeyond what is commonly used in the evaluation of classification and clustering algorithms.
4 Experiments
and results 4.1 Word learning Since we intended to monitor the incremental path of word learning given increasing amounts of linguistic input, AP scores were computed at four ?training checkpoints??established at 100K, 200K, 300K and 400K word tokens (the final point corresponding to the whole corpus).3 Scores were calculated independently for Ns and Vs.
In Table 1, we report the AP scores obtained by the best performing models of each type, as described in section 3.2.
The reported AP values refer to Word AP averaged respectively over the number of Ns and Vs in the test set.
Moreover, for ISA and RI we report mean AP values across 10 repetitions of the experiment.
For Ns, both ISA and RI outperformed SVD at all learning stages.
Moreover, ISA also performed significantly better than RI in the full-size input condition (400k checkpoint), as well as at the 300k checkpoint (Welch t-test; df = 17, p <.05).
One of the most striking results of these experimentswasthestrongN-VasymmetryintheWordAP scores, with the Vs performing significantly worse than the Ns.
For Vs, RI appeared to have a small advantage over ISA, although it was never significant at any stage.
The asymmetry is suggestive of the widely attested N-V asymmetry in child word 3The checkpoint results for SVD were obtained by training different models on increasing samples from the corpus, given the non-incremental nature of this method.
learning. A consensus has gathered in the early word learning literature that children from several languages acquire Ns earlier and more rapidly than Vs (Gentner, 1982).
An influential account explains this noun-bias as a product of language-external factors such as the different complexity of the world referents for Ns and Vs.
Recently, Christiansen and Monaghan (2006) found that distributional information in English CDS was more reliable for identifying Ns than Vs.
This suggests that the categorybias may also be partly driven by how good certain language-internal cues for Ns and Vs are in a given language.
Likewise, distributional cues to semantics may be stronger for English Ns than for Vs.
The noun-bias shown by ISA (and by the other models) could be taken to complement the results of Christiansen and Monaghan in showing that English Ns are more easily discriminable than Vs on distributionally-grounded semantic terms.
4.2 Category
learning In Table 2, we have reported the Class AP scores achieved by ISA, RI and SVD (best models) under the full-corpus training regime for the nine nominal semantic categories.
Although even in this case ISA and RI generally perform better than SVD (with the only exceptions of FURNITURE AND ROOMS and SMALL HOUSEHOLD ITEMS), results show a more complex and articulated situation.
With BODY PARTS, PEOPLE, and SMALL HOUSEHOLD ITEMS, ISA significantly outperforms its best rival RI (Welch t-test; p < .05).
For the other classes, the differences among the two models are not significant, except for CLOTHING in which RI performs significantly better than ISA.
For verb semantic classes (whose analytical data are not reported here for lack of space), no significant differences exist among the three models.
Some of the lower scores in Table 2 can be explained either by the small number of class members (e.g.
TOYS has only 6 items), or by the class highly heterogeneous composition (e.g.
in OUTSIDE THINGS AND PLACES TO GO we find nouns like garden, flower and zoo).
The case of PEOPLE, for which the performance of all the three models is far below their average Class AP score (ISA = 0.35; RI = 0.35; SVD = 0.27), is instead much more surprising.
In fact, PEOPLE is one of the classes 54 Semantic class ISA RI SVD ANIMALS REAL OR TOY 0.616 0.619 0.438 BODY PARTS 0.671 0.640 0.406 CLOTHING 0.301 0.349 0.328 FOOD AND DRINK 0.382 0.387 0.336 FURNITURE AND ROOMS 0.213 0.207 0.242 OUTSIDE THINGS PLACES 0.199 0.208 0.198 PEOPLE 0.221 0.213 0.201 SMALL HOUSEHOLD ITEMS 0.208 0.199 0.244 TOYS 0.362 0.368 0.111 Table 2: Class AP scores for Nouns.
For ISA and RI, scores are averaged across 10 iterations with the highest degree of internal coherence, being composed only of nouns unambiguously denoting human beings, such as girl, man, grandma, etc.
The token frequency of the members in this class is also fairly high, ranging between 684 and 55 occurrences.
Last but not least, in unrelated experiments we found that a SVD model trained on the British National Corpus with the same parameters as those used with Lara was able to achieve very good performances with human denoting nouns, similar to the members of our PEOPLE class.
These facts have prompted us to better investigate the reasons why with Lara none of the three models was able to converge on a satisfactory representation for the nouns belonging to the PEOPLE class.
We zoomed in on this semantic class by carrying out another experiment with ISA.
This model underwent 8 cycles of evaluation, in each of which the 10 words originally assigned to PEOPLE have been reclassified into one of the other nominal classes.
For each cycle, AP scores were recomputed for the 10 test words.
The results are reported in Figure 1 (where AP refers to the average Word AP achieved by the 10 words originally belonging to the class PEOPLE).
The highest score is reached when the PEOPLE nouns are re-labeled as ANIMALS REAL OR TOY (we obtained similar results in a parallel experiment with SVD).
This suggests that the low score for the class PEOPLE in the original experiment was due to ISA mistaking people names for animals.
What prima facie appeared asanerrorcouldactuallyturnouttobeaninteresting feature of the semantic space acquired by the model.
The experiments show that ISA (as well as the other models) groups together animals and people Ns, as Figure 1: AP scores for Ns in PEOPLE reclassified in the other classes it has formed a general and more underspecified semantic category that we might refer to as ANIMATE.
This hypothesis is also supported by qualitative evidence.
A detailed inspection of the CDS in the Lara corpus reveals that the animal nouns in the test set are mostly used by adults to refer either to toy-animals with which Lara plays or to characters in stories.
In the transcripts, both types of entities display a very human-like behavior (i.e., they talk, play, etc.), as it happens to animal characters in most children?s stories.
Therefore, the difference between model performance and the gold standard ontology can well be taken as an interesting clue to a genuine peculiarity in children?s semantic space with respect to adult-like categorization.
Starting from an input in which animal and human nouns are used in similar contexts, ISA builds a semantic space in which these nouns belong to a common underspecified category, much like the world of a child in which cats and mice behave and feel like human beings.
5 Conclusion
Our main experiments show that ISA significantly outperforms state-of-the-art word space models in a learning task carried out under fairly challenging training and testing conditions.
Both the incremental nature and the particular shape of the semantic representations built by ISA make it a (relatively) realistic computational model to simulate the emer55 gence of a semantic space in early childhood.
Of course, many issues remain open.
First of all, although the Lara corpus presents many attractive characteristics, it still contains data pertaining to a single child, whose linguistic experience may be unusual.
The evaluation of the model should be extended to more CDS corpora.
It will be especially interesting to run experiments in languages such as as Korean (Choi and Gopnik, 1995), where no nounbias is attested.
There, we would predict that the distributional information to semantics be less skewed in favor of nouns.
All CDS corpora we are aware of are rather small, compared to the amount of linguistic input a child hears.
Thus, we also plan to test the model on ?artificially enlarged??corpora, composed of CDS from more than one child, plus other texts that might be plausible sources of early linguistic input, such as children?s stories.
In addition, the target of the model?s evaluation should not be to produce as high a performance as possible, but rather to produce performance matching that of human learners.4 In this respect, the output of the model should be compared to what is known about human semantic knowledge at various stages, either by looking at experimental results in the acquisition literature or, more directly, by comparing the output of the model to what we can infer about the semantic generalizations made by the child from her/his linguistic production recorded in the corpus.
Finally, further studies should explore how the space constructed by ISA depends on the order in which sentences are presented to it.
This could shed some light on the issue of how different experiential paths might lead to different semantic generalizations.
While these and many other experiments must be run to help clarifying the properties and effectiveness of ISA, we believe that the data presented here constitute a very promising beginning for this new line of research.
References Borovsky, A.
and J.
Elman. 2006.
Language input and semantic categories: a relation between cognition and 4We thank an anonymous reviewer for this note early word learning.
Journal of Child Language, 33: 759-790.
Burgess, C.
and K.
Lund. 1997.
Modelling parsing constraints with high-dimensional context space.
Language and Cognitive Processes, 12: 1-34.
Choi, S.
and A.
Gopnik, A.
1995. Early acquisition of verbs in Korean: a cross-linguistic study.
Journal of Child Language 22: 497-529.
Christiansen, M.H. and P.
Monaghan. 2006.
Discovering verbs through multiple-cue integration.
In K.
Hirsh-Pasek and R.M.
Golinkoff (eds.), Action meets word: How children learn verbs.
OUP, Oxford.
Farkas, I.
and P.
Li. 2001.
A self-organizing neural network model of the acquisition of word meaning.
Proceedings of the 4th International Conference on Cognitive Modeling.
Gentner, D.
1982. Why nouns are learned before verbs: Linguistic relativity versus natural partitioning.
In S.A.Kuczaj(ed.), Language development, vol.
2: Language, thought and culture.
Erlbaum, Hillsdale, NJ.
Karlgren, J.
and M.
Sahlgren. 2001.
From words to understanding.
In Uesaka, Y., P.
Kanerva and H.
Asoh (eds.), Foundations of real-world intelligence, CSLI, Stanford: 294-308, Landauer, T.K. and S.T.
Dumais. 1997.
A solution to Plato?s problem: The Latent Semantic Analysis theory of acquisition, induction and representation of knowledge.
Psychological Review, 104(2): 211-240.
Li, P., C.
Burgess and K.
Lund. 2000.
The acquisition of word meaning through global lexical co-occurrences.
Proceedings of the 31st Child Language Research Forum: 167-178.
Li, P., I.
Farkas and B.
MacWhinney. 2004.
Early lexical acquisition in a self-organizing neural network.
Neural Networks, 17(8-9): 1345-1362.
Manning Ch.
and H.
Schutze. 1999.
Foundations of statistical natural language processing The MIT Press, Cambridge, MASS.
MacWhinney, B.
2000. The CHILDES project: Tools for analyzing talk (3d edition).
Erlbaum, Mahwah, NJ.
Rowland, C., J.
Pine, E.
Lieven and A.
Theakston. 2005.
The incidence of error in young children?s whquestions.
Journal of Speech, Language and Hearing Research, 48(2): 384-404.
Sahlgren, M.
2006. The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces.
Ph.D.dissertation, Department of Linguistics, Stockholm University .
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 57??4, Prague, Czech Republic, June 2007 c2007 Association for Computational Linguistics Simulating the acquisition of object names Alessio Plebe and Vivian De la Cruz Dept.
Cognitive Science University of Messina Italy {alessio.plebe,vdelacruz}@unime.it Marco Mazzone Lab.
Cognitive Science University of Catania Italy mazzonem@unict.it Abstract Naming requires recognition.
Recognition requires the ability to categorize objects and events.
Infants under six months of age are capable of making ne-grained discriminations of object boundaries and three-dimensional space.
At 8 to 10 months, a child?s object categories are suf ciently stable and exible to be used as the foundation for labeling and referencing actions.
What mechanisms in the brain underlie the unfolding of these capacities?
In this article, we describe a neural network model which attempts to simulate, in a biologically plausible way, the process by which infants learn how to recognize objects and words through exposure to visual stimuli and vocal sounds.
1 Introduction
Humans, come to recognize an in nite variety of natural and man-made objects and make use of sounds to identify and categorize them.
How do human beings arrive at this capacity?
Different explanations have been offered to explain the processes, and those behind the learning of rst words in particular.
Evidence has made clear that object recognition and categorization in early infancy is much more sophisticated than was previously thought.
By the time children are 8 to 10 months old their object categories are suf ciently stable and exible to be used as the foundation for labeling and referencing actions.
Increasing amounts of evidence point to the growing capacity of infants at this stage to reliably map arbitrary sounds onto meanings and this mapping process is crucial to the acquisition of language.
The word-learning mechanisms used at this early phase of language learning could very well involve a mapping of words onto the most perceptually interesting objects in an infant?s environment (Pruden et al., 2006).
There are those that claim that early word learning is not purely associative and that it is based on a sensitivity to social intent (Tomasello, 1999), through joint attention phenomena (Bloom, 2000).
Pruden et al.have demonstrated that 10-month-old infants are sensitive to social cues but cannot recruit them for word learning and therefore, at this age infants presumably have to learn words on a simple associative basis.
It is not by chance, it seems, that early vocabulary is made up of the objects infants most frequently see (Gershkoff-Stowe and Smith, 2004).
Early word-learning and object recognition can thus be explained, according to a growing group of researchers, by associational learning strategies alone.
There are those such as Carey and Spelke that postulate that there must necessarily be innate constraints that have the effect of making salient certain features as opposed to others, so as to narrow the hypothesis space with respect to the kinds of objects to be categorized rst (Carey and Spelke, 1996).
They reject the idea that object categorization in infants could emerge spontaneously from the ability to grasp patterns of statistical regularities.
Jean Mandler presents evidence that the rst similarity dimensions employed in categorization processes are indeed extremely general (Mandler, 2004); in other words, these dimensions single out wide domains of objects, with further re nements coming only later.
Mandler claims, however, that the early salience of 57 these extremely general features could have a different explanation other than nativism: for example, that salience could emerge from physiological constraints.
Using a connectionist model with backpropagation, Rogers and McClelland have shown that quite general dimensions of similarity can emerge without appealing to either physiological or cognitive constraints, simply as the result of a coherent covariation of features, that is, as an effect of mere statistical regularities (Rogers and McClelland, 2006).
What Rogers and McClelland say about the most general features obviously apply also to more speci c features which become salient later on.
However, interesting as it is from a computational point of view, this model is rather unrealistic as a simulation of biological categorization processes.
Linda Smith, suggests that words can contribute to category formation, in that they behave as features which co-vary with other language-independent features of objects (Smith, 1999).
In general, her idea is that the relevant features simply emerge from regularities in the input.
Terry Regier, building upon the proposal offered by Smith, has shown that word learning might behave in analogy with what we have said about categorization (Regier, 2005): certain features of both objects and words (i.e., phonological forms) can be made more salient than others, simply as a consequence of regularities in objects, words, and their co-variation.
Regier?s training sets however, are constituted by wholly arti cial phonological or semantic features, rather than by natural features such as voicing or shape. The positions mentioned above con ict with others, such as that of Lila Gleitman and her colleagues, according to which some innate constraints are needed in order to learn words.
It should be noted, however, that even in Gleitman?s proposal the need for innate constraints on syntax-semantic mapping mainly concerns verbs; moreover, the possibility to apprehend a core set of concrete terms without the contribution of any syntactic constraint is considered as a precondition for verb acquisition itself (Gillette et al., 1999).
This paper describes a neural network model which attempts to simulate the process by which infants learn how to recognize objects and words in the rst year of life through exposure to visual stimuli and vocal sounds.
The approach here pursued is in line with the view that a coherent covariation of features is the major engine leading to object name acquisition, the attempt made however, is to rely on biological ways of capturing coherent covariation.
The pre-established design of the mature functions of the organism is avoided, and the emergence of the nal function of each component of the system is left to the plastic development of the neural circuits.
In the cortex, there is very little differentiation in the computational capability that neural circuits will potentially perform in the mature stage.
The interaction between environmental stimuli and some of the basic mechanisms of development is what drives differentiation in computational functions.
This position has large empirical support (Katz and Callaway, 1992; Lcurrency1owel and Singer, 2002), and is compatible with current knowledge on neural genetics (Quartz, 2003).
The model here described, can be considered an implementation of the processes that emerge around the 10 month of age period.
It can also be used to consider what happens in a hypothesized subsequent period, in which the phenomenon of joint attention provides the social cueing that leads to the increased ability to focus on certain objects as opposed to others.
2 The
proposed model First the mathematics common to the modules will be described, then the model will be outlined.
Details of the visual and the auditory paths will be provided along with a description of the learning procedures.
2.1 The
mathematical abstraction of the cortical maps All the modules composing this model are implemented as arti cial cortical maps, adopting the LISSOM (Laterally Interconnected Synergetically SelfOrganizing Map) architecture (Sirosh and Miikkulainen, 1997; Bednar, 2002).
This architecture has been chosen because of its reproduction of neural plasticity, through the combination of Hebb?s principle and neural homeostasis, and because it is a good compromise between a number of realistic features and the simplicity necessary for building complex 58 LGNMGN A1 LPC HPC STS LOC V2 V1 VO AAM Figure 1: Overall scheme of the model.
models. The LISSOM is a two dimensional arrangement of neurons, where each cell is not only connected with the afferent input vector, but receives excitatory and inhibitory inputs from several neighbor neurons on the same map: x(k)i = f parenleftBigg A 1 + NvectorI vectorvrA,ivectorarA,i vectorvrA,i + EvectorerE,i vectorx (k??)rE,i HvectorhrH,i vectorx (k??)rH,i parenrightBig, (1) where x(k)i is the activation of the neuron i at time step k.
All vectors are composed by a circular neighborhood of given radius around the neuron i: vectors vectorx (k)?? are activations of neurons on the same layer at the previous time step.
Vector vectorvrA,i comprises all neurons in the underlying layer, in a circular area centered on the projection of i on this layer, with radius rA.
Vectors vectorarA,i, vectorerE,i, and vectorhrH,i are composed by all connection strengths of, afferent, excitatory or inhibitory neurons respectively, projecting to i, inside circular areas of radius rA, rE, rH.
Vector vectorI is just a vector of 1?s of the same dimension of vectorvrA,i.
The scalars A, E, and H, are constants modulating the contribution of afferent, excitatory and inhibitory connections.
The scalar N controls the setting of a push-pull effect in the afferent weights, allowing inhibitory effect without negative weight values.
Mathematically, it represents dividing the response from the excitatory weights by the response from a uniform disc of inhibitory weights over the receptive eld of neuron i.
The map is characterized by the matrices A,E,H, which columns are all vectors vectora, vectore, vectorh for every neuron in the map.
The function f is a monotonic non-linear function limited between 0 and 1.
The nal activation value of the neurons is assessed after settling time K.
All connection strengths to neuron i adapt by following the rules: ?vectorarA,i = vectorarA,i + AxivectorvrA,ikvectora rA,i + AxivectorvrA,ik vectorarA,i, (2) ?vectorerE,i = vectorerE,i + ExivectorxrE,ikvectora rE,i + ExivectorxrE,ik vectorerE,i, (3) ?vectorhrH,i = vectorhr H,i + AxivectorxrH,ivextenddoublevextenddouble vextenddoublevectorhrH,i + AxivectorxrH,i vextenddoublevextenddouble vextenddouble vectorhrH,i, (4) where {A,E,H} are the learning rates for afferent, excitatory and inhibitory synaptic modi cations.
All rules are based on the Hebbian law, with an additional competitive factor, here implemented as a normalization, that maintains constant the integration of all connection strengths to the same neu59 layer size rA rE rH A E H N LGN Lateral Geniculated Nucleus 120120 MGN Medial Geniculated Nucleus 3232 V1 Primary Visual Cortex 9696 8.5 1.5 7.0 1.5 1.0 1.0 0.0 V2 Secondary Visual Cortex 3030 7.5 8.5 3.5 50.0 3.2 2.5 0.7 VO Ventral Occipital 3030 24.5 4.0 8.0 1.8 1.0 1.0 0.0 A1 Auditory Primary Cortex 2424 3.5 2.5 5.5 5.0 5.0 6.7 0.8 LOC Lateral Occipital Complex 1616 6.5 1.5 3.5 1.2 1.0 1.5 0.0 STS Superior Temporal Sulcus 1616 3.5 2.5 2.5 2.0 1.6 2.6 0.0 Table 1: Legend of all modules, and main parameters of the cortical layers composing the model.
ron, and to the same type (afferent, excitatory or inhibitory).
This is a computational account of the biological phenomena of homeostatic plasticity, that induce neurons in the cortex to maintain an average ring rate by correcting their incoming synaptic strengths.
2.2 The
overall model An outline of the modules that make up the model is shown in Fig.
1. The component names and their dimensions are in Tab.
1. All cortical layers are implemented by LISSOM maps, where the afferent connections vectorv in (1) are either neurons of lower LISSOM maps, or neurons in the thalamic nuclei MGN and LGN.
There are two main paths, one for the visual process and another for the auditory channel.
Both paths include thalamic modules, which are not the object of this study and are therefore hardwired according to what is known about their functions.
The two higher cortical maps, LOC and STS, will carry the best representation coded by models on object visual features and word features.
These two representations are associated in an abstract type map, called AAM (Abstract Associative Map).
This component is implemented using the SOM (Self Organized Map) (Kohonen, 1995) architecture, known to provide non linear bidimensional ordering of input vectors by unsupervised mechanisms.
It is the only component of the model that cannot be conceptually referred to as a precise cortical area.
It is an abstraction of processes that actually involve several brain areas in a complex way, and as such departs computationally from realistic cortical architecture.
2.3 The
visual pathway As shown in Fig.
1, the architecture here used includes hardwired extracortical maps with simple oncenter and off-center receptive elds.
There are three pairs of sheets in the LGN maps: one connected to the intensity image plane, and the other two connected to the medium and long wavelength planes.
In the color channels the internal excitatory portion of the receptive eld is connected to the channel of one color, and the surrounding inhibitory part to the opposite color.
The cortical process proceeds along two different streams: the achromatic component is connected to the primary visual map V1 followed by V2, the two spectral components are processed by map VO, the color center, also called hV4 or V8 (Brewer et al., 2005).
The two streams rejoin in the cortical map LOC, the area recently suggested as being the rst involved in object recognition in humans (Malach et al., 1995; Kanwisher, 2003).
Details of the visual path are in (Plebe and Domenella, 2006).
2.4 The
auditory pathway The hardwired extracortical MGN component is just a placeholder for the spectrogram representation of the sound pressure waves, which is extracted with tools of the Festival software (Black and Taylor, 1997).
It is justi ed by evidence of the spectro-temporal process performed by the cochlear-thalamic circuits (Escabi and Read, 2003).
The auditory primary cortex is simulated by a double sheet of neurons, taking into account a double population of cells found in this area (Atzori et al., 2001), where the so-called LPC (Low-Probability Connections) is sensitive to the stationary component of the sound signal and the HPC (High-Probability Connections) population responds to transient inputs mainly.
The next map in the auditory path of the model is STS, because the superior temporal sulcus is believed to be the main brain area responsive to 60 vocal sounds (Belin et al., 2002).
2.5 The
Abstract Associative Map The upper AAM map in the model re ects how the system associates certain sound forms with the visual appearance of objects, and has the main purpose of showing what has been achieved in the cortical part of the model.
It is trained using the outputs of the STS and the LOC maps of the model.
After training, each neuron x in AAM is labeled, according to different test conditions X.
The labeling function l( ) associates the neuron x with an entity e, which can be an object o of the COIL set O, when X 2 fA, Bg or a category c of the set C for the test condition X 2 fC, Dg.
The general form of the labeling function is: l(X)(x) = arg max e?E braceleftBigvextendsinglevextendsingle vextendsingleW(e)x vextendsinglevextendsingle vextendsingle bracerightBig (5) where W(e)x is a set of sensorial stimuli related to the element e 2 E, such that their processing in the model activate x as winner in the AMM map.
The set E can be O or C depending on X.
The neuron x elicited in the AAM map as the consequence of presenting a visual stimulus vo of an object o and a sound stimulus sc of a tagory c is given by the function x = w(vo, sc) with the convention that w(v, epsilon1) computes the winning neuron in AAM comparing only the LOC portion of the coding vector, and w(epsilon1, s) only the STS portion.
The function b(o) : O ! C associates an object o to its category.
Here four testing conditions are used: A object recognition by vision and audio B object recognition by vision only C category recognition by vision and audio D category recognition by audio only corresponding to the following W sets in (5): A : braceleftbigvo : x = w(vo, sc(o))bracerightbig (6) B : fvo : x = w(vo, epsilon1)g (7) C : fvo : c = b(o) ^ x = w(epsilon1, sc)g (8) D : fsc : x = w(epsilon1, sc)g (9) From the labeling functions the possibility of estimating the accuracy of recognition immediately follows, simply by weighing the number of cases where the category or the object has been classi ed as the prevailing one in each neuron of the AAM SOM.
2.6 Exposure
to stimuli The visual path in the model develops in two stages.
Initially the inputs to the network are synthetic random blobs, simulating pre-natal waves of spontaneous activity, known to be essential in the early development of the visual system (Sengpiel and Kind, 2002).
In the second stage, corresponding to the period after eye opening, natural images are used.
In order to address one of the main problems in recognition, the identifying of an object under different views, the COIL-100 collection has been used (Nayar and Murase, 1995) where 72 different views are available for each of the 100 objects.
Using natural images where there is only one main object is cleary a simpli cation in the vision process of this model, but it does not compromise the realism of the conditions.
It always could be assumed that the single object analysis corresponds to a foval focusing as consequence of a saccadic move, cued by any attentive mechanism.
In the auditory path there are different stages as well.
Initially, the maps are exposed to random patches in frequency-time domain, with shorter duration for HPC and longer for LPC.
Subsequently, all the auditory maps are exposed to the 7200 most common English words (from http://www.bckelk.uklinux.net/menu.html) with lengths between 3 and 10 characters.
All words are converted from text to waves using Festival (Black and Taylor, 1997), with cepstral order 64 and a uni ed time window of 2.3 seconds.
Eventually, the last stage of training simulates events when an object is viewed and a word corresponding to its basic category is heard simultaneously.
The 100 objects have been grouped manually into 38 categories.
Some categories, such as cup or medicine count 5 exemplars in the object collection, while others, such as telephone, have only one exemplar.
3 Results
3.1 Developed functions in the cortical maps At the end of development each map in the model has evolved its own function.
Different functions 61 have emerged from identical computational architectures.
The differences are due to the different positions of a maps in the modules hierarchy, to different exposure to environmental stimuli, and different structural parameters.
The functions obtained in the experiment are the following.
In the visual path orientation selectivity emerged in the model?s V1 map as demonstrated in (Sirosh and Miikkulainen, 1997) and (Plebe and Domenella, 2006).
Orientation selectivity is the main organization in primary visual cortex, where the responsiveness of neurons to oriented segments is arranged over repeated patterns of gradually changing orientations, broken by few discontinuities (Vanduffel et al., 2002).
Angle selectivity emerged in the model?s V2 map.
In the secondary visual cortex the main recently discovered phenomena is the selectivity to angles (Ito and Komatsu, 2004), especially in the range between 60 and 150 degrees.
The essential features of color constancy are reproduced in the model?s VO map, which is the ability of neurons to respond to speci c hues, regardless of intensity.
Color constancy is the tendency of the color of a surface to appear more constant that it is in reality.
This property is helpful in object recognition, and develops sometime between two and four months of age.
(Dannemiller, 1989).
One of the main functions shown by the LOC layer in the model is visual invariance, the property of neurons to respond to peculiar object features despite changes in the object?s appearance due to different points of view.
Invariance indeed is one of the main requirements for an object-recognition area, and is found in human LOC (Grill-Spector et al., 2001; Kanwisher, 2003).
Tonotopic mapping is a known feature of the primary auditory cortex that represents the dimensions of frequency and time sequences in a sound pattern (Verkindt et al., 1995).
In the model it is split into a sheet where neurons have receptive elds that are more elongated along the time dimension (LPC) and another where the resulting receptive elds are more elongated along the frequency dimension (HPC).
The spectrotemporal mapping obtained in STS is a population coding of features, in frequency and time domains, representative of the sound patterns heard during the development phase.
It therefore re ects the statistical phonemic regularities in common spoken English, extracted from the 7200 training samples.
category test A test B test C test D medicine 0.906 0.803 1.0 1.0 fruit 1.0 0.759 1.0 1.0 boat 0.604 0.401 1.0 1.0 tomato 1.0 0.889 1.0 1.0 sauce 1.0 1.0 1.0 1.0 car 0.607 0.512 0.992 1.0 drink 0.826 0.812 1.0 1.0 soap 0.696 0.667 1.0 1.0 cup 1.0 0.919 1.0 0.0 piece 0.633 0.561 1.0 1.0 kitten 1.0 0.806 1.0 1.0 bird 1.0 1.0 1.0 1.0 truck 0.879 0.556 1.0 1.0 dummy 1.0 0.833 1.0 1.0 tool 0.722 0.375 1.0 1.0 pottery 1.0 1.0 1.0 1.0 jam 1.0 1.0 1.0 1.0 frog 1.0 0.806 1.0 1.0 cheese 0.958 0.949 1.0 1.0 bottle 0.856 0.839 1.0 1.0 hanger 1.0 0.694 1.0 1.0 sweets 1.0 0.701 1.0 1.0 tape 1.0 0.861 1.0 1.0 mug 0.944 0.889 1.0 1.0 spoon 1.0 0.680 1.0 1.0 cigarettes 0.972 0.729 0.972 1.0 ring 1.0 1.0 1.0 1.0 pig 1.0 0.778 1.0 1.0 dog 1.0 0.917 1.0 1.0 toast 1.0 0.868 1.0 1.0 plug 1.0 0.771 1.0 1.0 pot 1.0 0.681 1.0 1.0 telephone 1.0 0.306 1.0 1.0 pepper 1.0 0.951 1.0 1.0 chewinggum 0.954 0.509 1.0 1.0 chicken 1.0 0.944 1.0 1.0 jug 1.0 0.917 1.0 1.0 can 1.0 0.903 1.0 1.0 Table 2: Accuracy in recognition measured by labeling in the AAM, for objects grouped by category.
3.2 Recognition
and categorization in AAM The accuracy of object and category recognition under several conditions is shown in Table 2.
All tests clearly prove that the system has learned an ef cient capacity of object recognition and naming, with respect to the small world of object and names used in the experiment.
Tests C and D demonstrate that the recognition of categories by names is almost complete, both when hearing a name or when seeing an object and hearing its name.
In tests A and B, the recognition of individual objects is also very high.
In several cases, it can be seen that names also help in the recognition of individual objects.
One of the clearest cases is the category tool (shown in Fig.
2), 62 shape test A test B ?? h-parallelepiped 0.921 0.712 0.209 round 1.0 0.904 0.096 composed 0.702 0.565 0.137 q-cylindrical 0.884 0.861 0.023 q-h-parallelepiped 0.734 0.513 0.221 cylindrical 0.926 0.907 0.019 cup-shaped 0.975 0.897 0.078 q-v-parallelepiped 0.869 0.754 0.115 body 1.0 0.869 0.131 conic 1.0 1.0 0.0 parallelepiped 0.722 0.510 0.212 q-parallelepiped 1.0 0.634 0.366 Table 3: Accuracy in recognition measured by labeling in the AAM, for objects grouped by their visual shape, ??is the improvement gained with naming.
where the accuracy for each individual object doubles when using names.
It seems to be analogous to the situation described in (Smith, 1999), where the word contributes to the emergence of patterns of regularity.
The 100% accuracy for the category, in this case, is better accounted for as an emergent example of synonymy, where coupling with the same word is accepted, despite the difference in the output of the visual process.
In table 3 accuracy results for individual objects are listed, grouped by object shape.
In this case category accuracy cannot be computed, because shapes cross category boundaries.
It can be seen that the improvement ??is proportional to the salience in shape: it is meaningless for common, obvious shapes, and higher when object shape is uncommon.
This result is in agreement with ndings in (Gershkoff-Stowe and Smith, 2004).
4 Conclusions
The model here described attempts to simulate lexical acquisition from auditory and visual stimuli from a brain processes point of view.
It models these processes in a biologically plausible way in that it does not begin with a predetermined design of mature functions, but instead allows nal functions of the components to emerge as a result of the plastic development of neural circuits.
It grounds this choice and its design principles in what is known of the cerebral cortex.
In this model, the overall important result achieved so far, is the emergence of naming and recognition abilities exclusively through exposure of the system to environmental stimuli, in terms of activities similar to pre-natal spontaneous activities, and later to natural images and vocal sounds.
This result has interesting theoretical implications for developmental psychologists and may provide a useful computational tool for future investigations on phenomena such as the effects of shape on object recognition and naming.
In conclusion this model represents a rst step in simulating the interaction of the visual and the auditory cortex in learning object recognition and naming, and being a model of high level complex cognitive functions, it necessarily lacks several details of the biological cortical circuits.
It lacks biological plausibility in the auditory path because of the state of current knowledge of the processes going on there.
Future developments of the model will foresee the inclusion of backprojections between maps in the hierarchy, trials on preliminary categorization at the level of phonemes and syllables in the auditory path, as well as exposure to images with multiple objects in the scene.
References Marco Atzori, Saobo Lei, D.
Ieuan P.
Evans, Patrick O.
Kanold, Emily Phillips-Tansey, Orinthal McIntyre, and Chris J.
McBain. 2001.
Differential synaptic processing separates stationary from transient inputs to the auditory cortex.
Neural Networks, 4:1230 1237.
James A.
Bednar. 2002.
Learning to See: Genetic and Environmental In uences on Visual Development.
Ph.D. thesis, University of Texas at Austin.
Tech Report AI-TR-02-294.
Pascal Belin, Robert J.
Zatorre, and Pierre Ahad.
2002. Human temporal-lobe response to vocal sounds.
Cognitive Brain Research, 13:17 26.
Alan W.
Black and Paul A.
Taylor. 1997.
The festival speech synthesis system: System documentation.
Technical Report HCRC/TR-83, Human Communciation Research Centre, University of Edinburgh, Edinburgh, UK.
Paul Bloom.
2000. How children learn the meanings of words.
MIT Press, Cambridge (MA).
Alyssa A.
Brewer, Junjie Liu, Alex R.
Wade, and Brian A.
Wandell. 2005.
Visual eld maps and stimulus selectivity in human ventral occipital cortex.
Nature Neuroscience, 8:1102 1109.
Susan Carey and Elizabeth Spelke.
1996. Science and core knowledge.
Journal of Philosophy of Science, 63:515 533.
James L.
Dannemiller. 1989.
A test of color constancy in 9and 20-weeks-old human infants following simulated illuminant changes.
Developmental Psychology, 25:171 184.
Monty A.
Escabi and Heather L.
Read. 2003.
Representation of spectrotemporal sound information in the ascending auditory pathway.
Biological Cybernetics, 89:350 362.
Lisa Gershkoff-Stowe and Linda B.
Smith. 2004.
Shape and the rst hundred nouns.
Child Development, 75:1098 1114.
Jane Gillette, Henry Gleitman, Lila Gleitman, and Anne Lederer.
1999. Human simulations of vocabulary learning.
Cognition, 73:135 176.
Kalanit Grill-Spector, Zoe Kourtzi, and Nancy Kanwisher.
2001. The lateral occipital complex and its role in object recognition.
Vision Research, 41:1409 1422.
Minami Ito and Hidehiko Komatsu.
2004. Representation of angles embedded within contour stimuli in area V2 of macaque monkeys.
Journal of Neuroscience, 24:3313 3324.
Nancy Kanwisher.
2003. The ventral visual object pathway in humans: Evidence from fMRI.
In Leo Chalupa and John Werner, editors, The Visual Neurosciences.
MIT Press, Cambridge (MA).
Lawrence C.
Katz and Edward M.
Callaway. 1992.
Development of local circuits in mammalian visual cortex.
Annual Review Neuroscience, 15:31 56.
Teuvo Kohonen.
1995. Self-Organizing Maps.
Springer-Verlag, Berlin.
Siegrid Lcurrency1owel and Wolf Singer.
2002. Experience-dependent plasticity of intracortical connections.
In Manfred Fahle and Tomaso Poggio, editors, Perceptual Learning.
MIT Press, Cambridge (MA).
R. Malach, J.
B. Reppas, R.
R. Benson, K.
K. Kwong, H.
Jiang, W.
A. Kennedy, P.
J. Ledden, T.
J. Brady, B.
R. Rosen, and R.
B.H. Tootell.
1995. Object-related activity revealed by functional magnetic resonance imaging in human occipital cortex.
Proceedings of the Natural Academy of Science USA, 92:8135 8139.
Jean Matter Mandler.
2004. The Foundations of Mind.
Oxford University Press, Oxford (UK).
Shree Nayar and Hiroshi Murase.
1995. Visual learning and recognition of 3-d object by appearence.
International Journal of Computer Vision, 14:5 24.
Alessio Plebe and Rosaria Grazia Domenella.
2006. Early development of visual recognition.
BioSystems, 86:63 74.
Shannon M.
Pruden, Kathy Hirsh-Pasek, Roberta Michnick Golinkoff, and Elizabeth A.
Hennon. 2006.
The birth of words: Ten-month-olds learn words through perceptual salience.
Child Development, 77:266 280.
Steven R.
Quartz. 2003.
Innateness and the brain.
Biology and Philosophy, 18:13 40.
Terry Regier.
2005. The emergence of words: Attentional learning in form and meaning.
Cognitive Science, 29:819 865.
Timothy T.
Rogers and James L.
McClelland. 2006.
Semantic Cognition A Parallel Distributed Processing Approach.
MIT Press, Cambridge (MA).
Frank Sengpiel and Peter C.
Kind. 2002.
The role of activity in development of the visual system.
Current Biology, 12:818 826.
Joseph Sirosh and Risto Miikkulainen.
1997. Topographic receptive elds and patterned lateral interaction in a selforganizing model of the primary visual cortex.
Neural Computation, 9:577 594.
Linda B.
Smith. 1999.
Children?s noun learning: How general learning processes make specialized learning mechanisms.
In Brian MacWhinney, editor, The Emergence of Language.
Lawrence Erlbaum Associates, Mahwah (NJ).
Second Edition.
Michael Tomasello.
1999. The cultural origins of human cognition.
Harvard University Press, Cambridge (MA).
Wim Vanduffel, Roger B.H.
Tootell, Anick A.
Schoups, and Guy A.
Orban. 2002.
The organization of orientation selectivity throughout the macaque visual cortex.
Cerebral Cortex, 12:647 662.
Chantal Verkindt, Olivier Bertrand, Frano?is Echallier, and Jacques Pernier.
1995. Tonotopic organization of the human auditory cortex: N100 topography and multiple dipole model analysis.
Electroencephalography and Clinical Neurophisiology, 96:143 156 .
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 65??2, Prague, Czech Republic, June 2007 c2007 Association for Computational Linguistics Rethinking the syntactic burst in young children Christophe Parisse INSERM-Modyco Paris X Nanterr CNR @vjf es.
The procedure is tested using the large Manchester corpus in t database.
The results constraints on the grammar of the human languages and the human mind (Pinker, 1984; Wexler, 1982).
This report uses an iterative procedure to demonstrate that what appears to be near magical could result mostly from mechanisms that do not require the existence of innate principles of grammar, as they are based on children?s inherent capacities for perception, 7; Saffran, Johnson, Aslin, & Newport, 1999).
The mmar pacitie tion (words produced in part 1), along with other words nev ever produced at part 1), a that children use to spe from children?s input; this ated by the children?s knowledge of isolated words.
These multi-word utteran od by chile University S.cnrs.fr showed that young children grammatical capabilities (before age three) could be the results of simple mechanisms and that complex linguistic mastery does not need to be available so early in the course of language development.
1 Introduction
Between the ages of two and three, most children go through a syntactic burst.
In other words, they progress from uttering one word at a time to constructing utterances with a mean length of more than three words, and frequently longer, and they do this without any negative evidence and with limited input data (Ritchie & Bhatia, 1999).
This represents quite a mystery, which is often explained by postulating the existence of innate memory and association (Jusczyk & Hohne, 199 parisse Abstract A testing procedure is proposed to reevaluate the syntactic burst in children over age two.
The experimentation is based on the children?s capacities in perception, memory, association and cognition, and does not presuppose any specific innate grammatical capaciti he CHILDES acquisition of complex ?across the board??gra does not appear to be necessary to explain children?s behavior before age three or more.
At that age, much more complex and structured input data will be available to children, thereby increasing their learning capacities and reducing the limitations on knowledge they may acquire.
2 A
testing procedure in three parts The testing procedure for grammatical development that will be implemented in this report is made of three parts.
The goal of the first and the second part is to determine the basic elements that children use to construct language.
Two assumptions are made about young children?s perceptive and mnemonic ca s: anything they have once produced, they can produce again; and, when their language exactly reproduces an adult?s, this can be explained as a simple copy of their input.
Part 1: All single-word utterances produced by children are meaningful to them; they are directly derived from adults??output.
They are the basic elements that children use to build language.
Part 2: Children?s multi-word utterances containing only one word already produced in isolaer produced in isolation (n re also basic elements ak.
They are also directly derived is facilit ces are manipulated and understo 65 dren as single blocks, just as isolated words are.
Th account for the children?s multiword utteran events them from producing aberrant ut f unfinished ut e been carried out in order to answer this question.
3 token (SD = 9,653) and 1,913 in type (SD = nt co, Led from the adult?s output (list L-adult ab do not include utte ey may also be called frozen forms.
The goal of the third part is to check whether the basic elements identified in part 1 and 2 are sufficient to ces.
Part 3: Children link utterances produced at parts 1 and 2 to produce multi-word utterances with more than one word already produced in isolation (words produced in part 1).
They do this using a simple concatenation mechanism and the fact that the utterances they create have a pertinent meaning pr terances.
Since the productions of children and their adult partners are easy to record, it is possible to test whether the testing procedure has sufficient generative power to account for all children?s productions.
However, some points could make such a demonstration more difficult than it appears.
First of all, the assumption made in part 1 is not always true, as it is quite possible for a child to reproduce any sequence of sounds while playing with language.
This uncertainty about part 1 is only important in conjunction with part 2, as isolated words are the key used to parse the elements of part 2.
To decide that a word has meaning in isolation for a child, it has been assumed that it must first have meaning in isolation for an adult.
Words in the categories of determiner and auxiliary produced in isolation have been considered as not having meaning in isolation and have therefore been removed from the elements gathered at part 1.
Analysis of language data demonstrated that this assumption is quite reasonable, as the use of these words in isolation is often the result o terances, with incomplete prosody.
Measuring the generative power of the testing procedure implies evaluating the accuracy of the assumptions made in parts 1, 2 and 3.
These assumptions are quite easy to accept for very young children, at the time of the first multi-word utterances, i.e. before age two.
The question is: to what extent is this true and until what age?
Two experiments hav Experiment 1 The experiment 1 used a corpus extracted from the CHILDES database (MacWhinney, 2000).
It is referred to as the Manchester corpus (Theakston, Lieven, Pine, & Rowland, 1999) and consists of recordings of 12 children from the age of 1;10 to 2;9.
Their mean length of utterance varies from 1.5 to 2.9 words.
Each child was seen 34 times and each recording lasted one hour.
This results in a total production of 537,811 words in token and 7,840 in type.
For each child, the average is 44,817 words in 372).
The testing procedure was run in three steps in an iterative way.
Each step from the experime rresponds to one of the parts described above.
Step 1: For each transcript, the child?s singleword utterances are extracted and added to a cumulative list of words uttered in isolation, referred to as L1.
It is possible to measure at this point whether the words on L1 can be derived from the adult?s output.
In order to do this, a cumulative list adult, of all adult utterances is also maintained.
Step 2: For each multi-word utterance in the transcript, the number of words previously uttered in isolation is computed using list L1.
Multi-word utterances with only one word uttered in isolation are added to a list called L2.
It is possible to measure at this point whether the utterances on L2 can be deriv ove).
Step 3: the remaining utterances (list L3), which contain more than one word previously uttered in isolation, are used to test the final step of the algorithm.
The test consists in trying to reconstruct these utterances using a catenation of the utterances from lists L1 and L2 only.
Two measurements can be obtained: the percentage of utterances on list L3 that can be fully reconstructed (referred to below as the ?percentage of exact reconstruction?? and the percentage of words in the utterances on list L3 that contribute to a reconstruction (referred to below as the ?percentage of reconstruction covering??.
For example, for the utterance ?The boy has gone to school?? if L1 and L2 contain ?the boy??and ?has gone??but not ?to school?? only ?the boy has gone??can be reconstructed, thus leading to a percentage of reconstruction covering of 66%.
Thus, the percentage of exact reconstruction is the percentage of utterances with a 100% reconstruction covering.
The percentages of list L3 that are reconstructed or recovered rances from L1 and L2 lists.
66 The testing procedure is iterative because it is performed in turn for each of the transcripts of the corpus.
List L1, L2 and L-adult are cumulative, which means that the list obtained with transcript 1 are used as a starting point for the analysis of transcript 2, and so on.
This presupposes that children ca ercentage of elements of L2 present in adult t does not evolve much, varying between 6 and 8.
n reuse data they heard only once a long time after they heard it.
In Step 1 it was found that the percentage of words on L1 present in adult speech has a mean value of 91% (SD = 0.03).
Step 2 revealed that the speech has a mean value of 67% (SD = 0.05).
These two results are stable across ages?even though lists L1, L2 and L-adult are growing continuously.
After two transcripts, for all 12 children, lists L1 + L2 represent 11,979 words in token and L-adult contains 82,255 words in token.
After 17 transcripts, these totals are 89,479 and 688,802, respectively.
After 34 transcripts, they total 167,149 and 1,370,565.
The ratio comparing the size of L1 + L2 and L-adul p 40% 50% 60% 70% 80% 90% 100% Children's age All corpora Mean % per child '+1' SD '-1' SD 1;10 2;9 igure 1: Percentage of utterances exactly reconstructed F 70% 75% 80% 85% 90% 95% 100% Children's age All corpora Mean % per child '+1' SD '-1' SD 1;10 2;9 igure 2: Percentage of reconstruction covering in all utterances F 67 The results for Step 3 are presented in Figures 1 and 2.
Each point in the series corresponds to the nth iteration performed with the nth transcript.
The mean value is the mean of the percentage for all children considered as individuals (reconstruction between a child?s corpus and his/her parents?? corpus only).
The algorithm is also applied to all corpora: for each point in the series of recordings, the 12 files corresponding to 12 children are gathered into a single file used to run the nth iteration of the algorithm.
Percentages for all corpora are shown with a bold line.
The percentages are clearly higher for the aggregated corpora, although the number of unknown utterances (list L3) increased more than the number of known utterances (lists L1 and L2).
After two transcripts, there are half as many elements in list L3 as in L1 + L2.
But after 17 transcripts, L3 is 42% larger than L1 + L2, and after 34 transcripts, it is 127% larger.
As children grow older, there is a decrease in the scores for exact reconstruction and reconstruction covering.
This decrease is greater in individuals than for the children as a group, which suggests a size effect.
4 Experiment
2 The second experiment uses the same corpus and reproduces the same tests but assumes that children have knowledge of the syntactic categories Noun and Verb.
The conditions of step 2 and step 3 are more easily fulfilled if the children have a certain amount of syntactic class knowledge.
As described by Maratsos and Chalkley (1980), it is possible for children to learn syntactic classes from the contexts in which words occur.
However, knowledge of part of speech is unlikely in very young children on the basis of syntactic distribution.
Semantic knowledge can also help to construct syntactic knowledge (Bloom, 1999) for classes such as common nouns, proper nouns and verbs, and perhaps also adjectives and adverbs.
To simulate the fact that children are able to construct the classes of common nouns, proper nouns and non-auxiliary verbs, it suffices to substitute every occurrence of common or proper nouns in the Manchester corpus by the symbol ?noun??and every occurrence of nonauxiliary verbs by the symbol ?verb??
This is easy to realize because the Manchester corpus has been fully tagged for part of speech, as described in the MOR section of the CHILDES manual (MacWhinney, 2000).
The result is that list L1 now includes all nouns, all verbs plus all words occurring in isolation, as in the first experiment.
In list L2, in utterances that include a word from the categories Noun or Verb, this word is substituted by the symbol ?noun??or ?verb??
These utterances now form rule-like productive patterns known as formulaic frames (Peters, 1995) or slot-and-frame structures (Lieven, Pine, & Baldwin, 1997) ??for example, ?my + NOUN??
When we reproduce the first experiment under these conditions, the new results obtained at steps 2 and 3 should be better, in the sense that they should correspond more closely to the adult input, and should hold up longer on the age scale.
The results for Step 1 and Step 2 are indeed better than before.
The percentage of utterances on L2 present in adult speech has a mean value of 91% (SD = 0.02).
The results for Step 3 are presented in Figure 3 (for exact reconstruction) and Figure 4 (for reconstruction covering).
In each of these figures, two results are presented for the whole Manchester corpus: one assuming no category knowledge, and one assuming the knowledge of the three categories proper noun, common noun and verb.
The percentages of reconstruction become markedly higher, as any combination that contains some of three categories proper noun, common noun and verb is known for all occurrences of words from these categories.
The mean for exact reconstruction with ?no category??knowledge is 67% (SD = 5.7) and 87% (SD = 2.0) for reconstruction covering.
These values increase to 83% (SD = 5.2) and 95% (SD = 2.6) for ?noun and verb??knowledge.
68 70% 75% 80% 85% 90% 95% 100% Children's age No category Noun & Verb 2;91;10 Figure 3: Percentage of utterances exactly reconstructed, depending on the degree of knowledge of noun and verb categories 85% 90% 95% 100% Children's age No category Noun & Verb 1;10 2;9 Figure 4: Percentage of reconstruction covering in all utterances, depending on the degree of knowledge of noun and verb categories 5 Experiment 3 A limit of experiments 1 and 2 is that nothing indicates how long the three-step mechanisms would remain efficient and appropriate.
We supposed that these mechanisms would remain operational at an older age.
This can be checked using other material from the CHILDES database with recordings spanning a longer period.
The corpus chosen for the test is Brown?s (1973) Sarah corpus, which ranges from age 2;3 to age 5;1; with its 139 different transcripts, it follows the development of the child?s language quite well and is well suited for the purposes of this study, which requires lengthy corpora.
The mean length of utterance varies from 1.47 to 4.85 words.
This results in a total production of 99,918 words in token and 3,990 in type.
Step 1 found the percentage of words on L1 present in adult speech to have a mean value of 77% (SD = 14.5).
Step 2 revealed that the percentage of elements of L2 present in adult speech had a mean value of 38% (SD = 11.5).
These two results are 69 stable across ages.
With the assumption of a knowledge of the Noun and Verb categories, results for Step 1 and 2 are, respectively, 83% (SD = 13.8) and 55% (SD = 16.6).
The results for Step 3 are presented in Figure 5 (for exact reconstruction) and Figure 6 (for reconstruction covering).
In each of these figures, two results are presented: one assuming no category knowledge and one assuming knowledge of the three categories Proper Noun, Common Noun and Verb.
The mean for exact reconstruction with ?no category??knowledge is 54% (SD = 17.6) and 84% (SD = 6.6) for reconstruction covering.
These values increase 72% (SD = 11.9) and 93% (SD = 4.0) for ?Noun and Verb??knowledge.
30% 40% 50% 60% 70% 80% 90% 100% Children's age No category Noun & Verb 2;3 5;1 Figure 5: Percentage of utterances in the Sarah corpus exactly reconstructed, depending on the degree of knowledge of vocabulary and syntactic categories 70% 75% 80% 85% 90% 95% 100% Children's age No category Noun & Verb 2;3 5;1 Figure 6: Percentage of reconstruction covering in all utterances in the Sarah corpus, depending on the degree of knowledge of vocabulary and syntactic categories 70 The average percentages of reconstruction are lower for the Sarah corpus than for the Manchester corpus.
Comparing Figures 3 and 6 and Figures 4 and 7, one can see that there is a drop in the reconstruction performances in the third year.
The percentages for Sarah in her second year were as high as those for the Manchester corpus children.
Part of this drop in performance may be attributed to the smaller corpus.
Indeed, comparing Figures 1 and 3 and Figures 2 and 4, it appears that the drop in performance that became visible when single child corpora were used was not in evidence when all the corpora were amalgamated into one big corpus.
It is also possible that the drop in performance found in the Sarah corpus reflects a progressive decrease in the systematic use of a simple concatenation procedure by the child.
6 Discussion
The testing procedure does not achieve a full 100% reconstruction in the test conditions described above, where the database consists of only 34 onehour recordings for each of the 12 children in the corpus.
This corresponds globally to a pseudocorpus of 408 hours, which amounts to 8 to 10 weeks of speech.
With a larger corpus, the results would probably be better, as indicated by the increase in percentage of recovery when one moves from children in isolation to children as a group (see Figures 1 and 2).
In addition, there are bound to be words that children utter for the first time in multi-word utterances even though they could have been produced as isolated utterances.
The percentage of reconstruction, however, is still quite high, as was the case for results obtained using a similar methodology with Hungarian children (MacWhinney, 1975).
With the assumption of a benefit from the use of the Noun and Verb categories, which somewhat circumvents the limited size of the corpus, the results are very high.
A problem with the second experiment is that it is not sure that children can have a knowledge of part of speech (even very general part of speech such as noun and verb) with semantic knowledge only.
However, the experiment 2 is interesting as it can be viewed as a way to extend artificially a limited corpus.
Instead of saying that children have the knowledge of part of speech, we propose that noun and verb as so common in adult speech that an extended corpus will contain all basic utterances with a single content word and the appropriate grammatical context.
In other words, list L2 will contain all the most basic syntactic constructions.
Although this will not be the case in reality, it is indeed possible that a full corpus covering all utterances produced by adults will contains a very large number of L2 structures.
In this way, experirment 2 provides a measure of the upper limit that can be reached by the crude mechanism presented in this article (L3 constructions).
The testing procedure does not cover all language acquisition processes before the age of three.
Its rather crude mechanisms would, on their own, produce many aberrant utterances if they were not regulated by other mechanisms.
The first of these regulatory mechanisms is semantics, as children produce language that, for them, makes sense.
They will articulate thoughts with two or three elements that complement each other logically and thus create utterances interpretable by adults.
Strange utterances may be produced on occasion but none will sound alien.
Secondly, even though children sometimes join words or groups of words randomly when very young, they soon start to follow a systematic order probably copied from adults??utterances (Sinclair & Bronckart, 1972).
To do this, they merely have to concentrate on the words or groups of words that they already master, having previously uttered them as single words.
Indeed, form-function mapping is easier with single-word utterances than with multi-word utterances and this helps to manipulate single-word forms consciously.
Thus, single-word utterances are better candidates than most to become the first elements in a combinatorial system and to undergo representational redescription (Karmiloff-Smith, 1992).
Their semantic values allow one to perform semantic combinations.
By the age of two, associations words or frozen forms may be sufficient to allow children to produce and control language.
The fact that children can learn to produce complex speech patterns quickly without complex grammatical knowledge casts a whole new light on the problem of the acquisition of syntax.
The testing procedure relies heavily on semantics because it is assumed that what children understand, they will remember and manipulate.
This does not necessarily contradict all the theories that claim that there are some innate principles specific to grammar acquisition (Pinker, 1984; Wexler, 1982).
If 71 children acquire high-level grammatical rules at a later period of their development than is usually admitted in these theories, then the structure of their input?the couple ?base phrase marker??plus ?surface sentence??(Wexler, 1982) ??will be more complex.
The more complex these structures, the lower the innate conditions on grammars.
It would then be possible to progress from a simple system such as the association of frozen elements to a more complex one.
Late grammatical acquisition is a very important notion as it goes a long way towards explaining why there do not seem to be any neuronal structures specific to language or grammar (Elman et al., 1996; Muller, 1996).
Late grammatical acquisition is also highly compatible with constructivist proposals such as Tomasello?s (2003) and Goldberg?s (2006).
It has often been said that children already master syntax by the age of three, which is quite remarkable considering the complexity of what they are acquiring.
This report suggests that some simple generative mechanisms can explain the explosive acquisition of an apparent mastery of language observed in young children.
It demonstrates once again that, as already shown for other linguistic developmental features (Elman et al., 1996), an apparently complex output may be the product of a simple system.
The need for large-scale corpora to better tackle the problem of language acquisition with improved tools is also highlighted here.
References Bloom, P.
(1999). Theories of word learning: Rationalist alternatives to associationism.
In W.
C. Ritchie & T.
K. Bhatia (Eds.), Handbook of language acquisition.
San Diego: Academic Press.
Elman, J.
L., Bates, E., Johnson, M., Karmiloff-Smith, A., Parisi, D., & Plunkett, K.
(1996). Rethinking innateness: A connectionist perspective on development.
Cambridge, MA: MIT Press/Bradford Books.
Goldberg, A.
(2006). Constructions at Work: the nature of generalization in language.
Oxford University Press.
Jusczyk, P.
W., & Hohne, E.
A. (1997).
Infants' memory for spoken words [see comments].
Science, 277(5334), 1984-6.
Karmiloff-Smith, A.
(1992). Beyond modularity: a developmental perspective on cognitive science.
Cambridge, Mass.: MIT Press/Bradford Books.
Lieven, E.
V., Pine, J.
M., & Baldwin, G.
(1997). Lexically-based learning and early grammatical development.
Journal of Child Language, 24(1), 187-219.
MacWhinney, B.
(1975). Rules, rote, and analogy in morphological formations by Hungarian children.
Journal of Child Language, 2, 65-77.
MacWhinney, B.
(2000). The CHILDES project : Tools for analyzing talk (3rd).
Hillsdale, N.J, Lawrence Erlbaum.
Maratsos, M.
P., & Chalkley, M.
A. (1980).
The internal language of children's syntax: The ontogenesis and representation of syntactic categories.
In K.
E. Nelson (Ed.), Children's language.
Vol: 2 . New York, NY: Gardner Press.
Muller, R.-A.
(1996). Innateness, autonomy, universality?
Neurobiological approaches to language.
Behavioral and Brain Sciences, 19(4), 611-675.
Peters, A.
M. (1995).
Strategies in the acquisition of syntax.
In P.
Fletcher & B.
MacWhinney (Eds.), The handbook of child language . Oxford, UK: Blackwell.
Pinker, S.
(1984). Language learnability and language development.
Cambridge, MA: Harvard University Press.
Ritchie, W.
C., & Bhatia, T.
K. (1999).
Child language acquisition: Introduction, foundations, and overview.
In W.
C. Ritchie & T.
K. Bhatia (Eds.), Handbook of language acquisition . San Diego: Academic Press.
Saffran, J.
R., Johnson, E.
K., Aslin, R.
N., & Newport, E.
L. (1999).
Statistical learning of tone sequences by human infants and adults.
Cognition, 70(1), 27-52.
Sinclair, H., & Bronckart, J.
P. (1972).
S.V.O. A linguistic universal?
A study in developmental psycho-linguistics.
Journal of Experimental Psychology, 14(3), 329-348.
Theakston, A.
L., Lieven, E.
V. M., Pine, J.
M., & Rowland, C.
F. (1999).
The role of performance limitations in the acquisition of 'mixed' verb-argument structure at stage 1.
In M.
Perkins & S.
Howard (Eds.), New directions in language development and disorders : Plenum Press.
Tomasello, M.
(2003). Constructing a language: A usage-based theory of language acquisition.
Cambridge: MA, Harvard.
Wexler, K.
(1982). A principle theory for language acquisition.
In E.
Wanner & L.
R. Gleitman (Eds.), Language acquisition the state of the art.
New York: Cambridge University Press .
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 73??0, Prague, Czech Republic, June 2007 c2007 Association for Computational Linguistics The Topology of Synonymy and Homonymy Networks James Gorman and James R.
Curran School of Information Technologies University of Sydney NSW 2006, Australia {jgorman2,james}@it.usyd.edu.au Abstract Semantic networks have been used successfully to explain access to the mental lexicon.
Topological analyses of these networks have focused on acquisition and generation.
We extend this work to look at models that distinguish semantic relations.
We find the scale-free properties of association networks are not found in synonymy-homonymy networks, and that this is consistent with studies of childhood acquisition of these relationships.
We further find that distributional models of language acquisition display similar topological properties to these networks.
1 Introduction
Semantic networks have played an important role in the modelling of the organisation of lexical knowledge.
In these networks, words are connected by graph edges based on their semantic relations.
In recent years, researchers have found that many semantic networks are small-world, scale-free networks, having a high degree of structure and a short distance between nodes (Steyvers and Tenenbaum, 2005).
Early models were taxonomic and explained some aspects of human reasoning (Collins and Quillian, 1969) (and are still used in artificial reasoning systems), but were replaced by models that focused on general graph structures (e.g.
Collins and Loftus, 1975).
These better modelled many observed phenomena but explained only the searching of semantic space, not its generation or properties that exist at a whole-network level.
Topological analyses, looking at the statistical regularities of whole semantic networks, can be used to model phenomena not easily explained from the smaller scale data found in human experiments.
These networks are typically formed from corpora, expert compiled lexical resources, or human wordassociation data.
Existing work has focused language acquisition (Steyvers and Tenenbaum, 2005) and generation (Cancho and Sole, 2001).
These models use the general notion of semantic association which subsumes all specific semantic relations, e.g. synonymy.
There is evidence that there are distinct cognitive processes for different semantic relations (e.g.
Casenhiser, 2005).
We perform a graph analysis of synonymy, nearness of meaning, and homonymy, shared lexicalisation.
We find that synonymy and homonymy produce graphs that are topologically distinct from those produced using association.
They still produce smallworld networks with short path lengths but lack scale-free properties.
Adding edges of different semantic relations, in particular hyponymy, produces graphs more similar to the association networks.
We argue our analyses consistent with other semantic network models where nodes of a common type share edges of different types (e.g.
Collins and Loftus, 1975).
We further analyse the distributional model of language acquisition.
We find that it does not well explain whole-language acquisition, but provides a model for synonym and homonym acquisition.
73 2 Graph Theory Our overview of graph theory follows Watts (1999).
A graph consists of a set of n vertices (nodes) and a set of edges, or arcs, which join pairs of vertices.
Edges are undirected and arcs are directed.
Edges and arcs can be weighted or unweighted, with weights indicating the relative strength or importance of the edges.
We will only consider unweighted, undirected networks.
Although there is evidence that semantic relations are both directed (Tversky, 1977) and weighted (Collins and Loftus, 1975), we do not have access to this information in a consistent and meaningful format for all our resources.
Two vertices connected by an edge are called neighbours.
The degree k of a vertex is the count of it neighbours.
From this we measure the average degree ?k??for the graph and the degree distribution P(k) for all values of k.
The degree distribution is the probability of a vertex having a degree k.
The neighbourhood ?v of a vertex v is the set of all neighbours of v not including v.
The neighbourhood ?S of a subgraph S is the set of all neighbours of S, not including the members of S. The distance between any two vertices is the shortest path length, or the minimum number of edges that must be traversed, to reach the first from the second.
The characteristic path length L is the average distance between vertices.1 The diameter D of a graph is the maximum shortest path length between any two vertices.
At most D steps are required to reach any vertex from any other vertex but, on average, only L are required.
For very large graphs, calculating the values for L and D is computationally difficult.
We instead sample nprime lessmuch n nodes and find the mean values of L and D across the samples.
The diameter produced will always be less than or equal to the true diameter.
We found nprime = 100 to be most efficient.
It is not a requirement that every vertex be reachable from every other vertex and in these cases both L and D will be infinite.
In these cases we analyse the largest connected subgraph.
1Here we follow Steyvers and Tenenbaum (2005) as it is more commonly used in the cognitive science literature.
Watts (1999) defines the characteristic path length as the median of the means of shortest path lengths for each vertex.
2.1 Small-world Networks Traditional network models assume that networks are either completely random or completely regular.
Many natural networks are somewhere between these two extremes.
These small-world networks a have the high degree of clustering of a regular lattice and the short average path length of a random network (Watts and Strogatz, 1998).
The clustering is indicative of organisation, and the short paths make for easier navigation.
The clustering coefficient Cv is used to measure the degree of clustering around a vertex v: Cv = |E(?v)|parenleftBigk v 2 parenrightBig where |E(?v)| is the number of edges in the neighbourhood ?v and parenleftBigk v 2 parenrightBig is the total number of possible edges in ?v.
The clustering coefficient C of a graph is the average over the coefficients of all the vertices.
2.2 The
Scale of Networks Amaral et al.(2000) describe three classes of small world networks based on their degree distributions: Scale-free networks are characterised by their degree distribution decaying as a power law, having a small number of vertices with many links (hubs) and many vertices with few links.
Networks in this class include the internet (Faloutsos et al., 1999) and semantic networks (Steyvers and Tenenbaum, 2005).
Broad-scale networks are characterised by their degree distribution decaying as a power law followed by a sharp cut-off.
This class includes collaborative networks (Watts and Strogatz, 1998).
Single-scale networks are characterised by fast decaying degree distribution, such exponential or Gaussian, in which hubs are scarce or nonexistent.
This class includes power grids (Watts and Strogatz, 1998) and airport traffic (Amaral et al., 2000).
Amaral et al.(2000) model these differences using a constrained preferential attachment model, where new nodes prefer to attach to highly connected nodes.
Scale-free networks result when there are no constraints.
Broad-scale networks are produced when ageing and cost-to-add-link constraints are added, making it more difficult to produce very high degree hubs.
Single-scale networks occur when 74 these constraints are strengthened.
This is one of several models for scale-free network generation, and different models will result in different internal structures and properties (Keller, 2005).
3 Semantics
Networks Semantic networks represent the structure of human knowledge through the connections of words.
Collins and Quillian (1969) proposed a taxonomic representation of knowledge, where words are connected by hyponym relations, like in the WordNet noun hierarchy (Fellbaum, 1998).
While this structure predicted human reaction times for verifying facts it allows only a limited portion of knowledge to be expressed.
Later models represented knowledge as semi-structured networks, and focused on explaining performance in memory retrieval tasks.
One such model is spreading-activation, in which the degree to which a concept is able to be recalled is related to its similarity both to other concepts in general and to some particular prime or primes (Collins and Loftus, 1975).
In this way, if one is asked to name a red vehicle, fire truck is more likely response than car: while both are strongly associated with vehicle, fire truck is more strongly associated with red than is car.
More recently, graph theoretic approaches have examined the topologies of various semantic networks.
Cancho and Sole (2001) examine graphs of English modelled from the British National Corpus.
Since co-occurrence is non-trivial ??words in a sentence must share some semantic content for the sentence to be coherent ??edges were formed between adjacent words, with punctuation skipped.
Two graphs were formed: one from all co-occurrences and the other from only those co-occurrences with a frequency greater than chance.
Both models produced scale-free networks.
They find this model compelling for word choice during speech, noting function words are the most highly connected.
These give structure without conveying significant meaning, so can be omitted without rendering a sentence incoherent, but when unavailable render speech non-fluent.
This is consistent with work by Albert et al.(2000) showing that scale-free networks are tolerant to random deletion but sensitive to targeted removal of highly connected vertices.
Sigman and Cecchi (2002) investigate the structure of WordNet to study the effects of nounal polysemy on graph navigation.
Beginning with synsets and the hyponym tree, they find adding polysemy both reduces the characteristic path length and increases the clustering coefficient, producing a smallworld network.
They propose, citing word priming experiments as evidence, that these changes in structure give polysemy a role in metaphoric thinking and generalisation by increasing the navigability of semantic networks.
Steyvers and Tenenbaum (2005) examine the growth of semantic networks using graphs formed from several resources: the free association index collected by Nelson et al.(1998), Wordnet and the 1911 Roget?s thesaurus.
All these produced scale-free networks, and, using an age of acquisition and frequency weighted preferential attachement model, show that this corresponds to age-ofacquisition norms for a small set of words.
This is compared to networks produced by Latent Semantic Analysis (LSA, Landauer and Dumais, 1997), and conclude that LSA is an inadequate model for language growth as it does not produce the same scalefree networks as their association models.
3.1 Synonymy
and Homonymy While there have been many studies using human subjects on the acquisition of particular semantic relations, there have been no topological studies differentiating these from the general notion of semantic association.
This is interesting as psycholinguistic studies have shown that semantic relationships are distinguishable (e.g.
Casenhiser, 2005).
Here we consider synonymy and homonymy.
There are very few cases of true synonymy, where two words are substitutable in all contexts.
Nearsynonymy, where two words share some close common meaning, is more common.
Sets of synonyms can be grouped together into synsets, representing a common idea.
Homonymy occurs when a word has multiple meanings.
Formally, homonymy is occurs when words do not share an etymological root (in linguistics) or when the distinction between meanings is coarse (in cognitive science).
When the words share a root or meanings are close, the relationship is called polysemy.
This distinction is significant 75 in language acquisition, but as yet little research has been performed on the learning of polysemes (Casenhiser, 2005).
It is also significant for Natural Language Processing.
The effect of disambiguating homonyms is markedly different from polysemes in Information Retrieval (Stokoe, 2005).
We do not have access to these distinctions, as they are not available in most resources, nor are there techniques to automatically acquire these distinctions (Kilgarriff and Yallop, 2000).
For simplicity, will conflate the categories under homonymy.
There have been several studies into synonymy and homonymy acquisition in children, and these have shown that it lags behind vocabulary growth (Doherty and Perner, 1998; Garnham et al., 2000).
A child will associate both rabbit and bunny with the same concept, but before the age of four, most children have difficulty in choosing the word bunny if they have already been presented with the word rabbit.
Similarly, a young child asked to point to two pictures that have the same name but mean different things will have difficulty, despite knowing each of the things independently.
Despite this improvement with age, there are tendencies for language to avoid synonyms and homonyms as a more general principle of economy (Casenhiser, 2005).
This is balanced by the utility of ambiguous relations for mental navigation (Sigman and Cecchi, 2002) which goes some way to explaining why they still play such a large role in language.
4 The
Topology of Synonymy and Homonymy Relations For each of our resources we form a graph based on the relations between lexical items.
This differs to the earlier work of Sigman and Cecchi (2002), who use synsets as vertices, and Steyvers and Tenenbaum (2005) who use both lexical items and synsets..
This is motivated largely by our automatic acquisition techniques, and also by human studies, in which we can only directly access relationships between words.
This also allows us to directly compare resources where we have information about synsets to those without.
We distinguish parts of speech as disambiguation across them is relatively easy psychologically (Casenhiser, 2005) and computationally (e.g.
Ratnaparkhi, 1996).
4.1 Lexical
Semantic Resources A typical resource for providing this information are manually constructed lexical semantic resources.
We will consider three: Roget?s, WordNet and Moby Roget?s thesaurus is a common language thesaurus providing a hierarchy of synsets.
Synsets with the same general or overlapping meaning and part of speech are collected into paragraphs.
The parts of speech covered are nouns, verbs, adjectives, adverbs, prepositions, phrases, pronouns, interjections, conjunctions, and interrogatives.
Paragraphs with similar meaning are collated by part of speech into labeled categories.
Categories are then collected into classes using a three-tiered hierarchy, with the most general concept at the top.
Where a word has several senses, it will appear in several synsets.
Several editions of Roget?s have been released representing the change in language since the first edition in 1852.
The last freely available edition is the 1911, which uses outdated vocabulary, but the global topology has not changed with more recent editions (Old, 2003).
As our analysis is not concerned with the specifics of the vocabulary, this is the edition we will use.
It consists of a vocabulary of 29,460 nouns, 15,173 verbs, 13,052 adjectives and 3,005 adverbs.
WordNet (Fellbaum, 1998) is an electronic lexical database.
Like Roget?s, it main unit of organisation is the synset, and a word with several senses will appear in several synsets.
These are divided into four parts of speech: nouns, verbs, adjectives and adverbs.
Synsets are connected by semantic relationships, e.g antonymy, hyponymy and meronym.
WordNet 2.1 provides a vocabulary of 117,097 nouns, 11,488 verbs, 22,141 adjectives and 4,601 adverbs.
The Moby thesaurus provides synonymy lists for over 30,000 words, with a total vocabulary of 322,263 words.
These lists are not distinguished by part of speech.
A separate file is supplied containing part of speech mappings for words in the vocabulary.
We extracted separate synonym lists for nouns, verbs, adjectives and adverbs using this list combined with WordNet part of speech information.2 This produces a vocabulary of 42,821 nouns, 11,957 verbs, 16,825 adjectives and 3,572 adverbs.
Table 1 presents the statistics for the largest con2http://aspell.sourceforge.net/wl/ 76 Roget?s WordNet Moby Noun Verb Adj Adv Noun Verb Adj Adv Noun Verb Adj Adv n 15,517 8,060 6,327 626 11,746 6,506 4,786 62 42,819 11,934 16,784 3501 ?k??8.97 8.46 7.40 7.17 4.58 6.34 5.16 4.97 34.65 51.98 39.26 16.07 L 6.5 6.0 6.4 10.5 9.8 6.0 9.5 5.6 3.7 3.1 3.4 3.7 D 21.4 17 17 31 27 15.3 26.4 14 9.6 9.8 9.3 9.8 C 0.74 0.68 0.69 0.77 0.63 0.62 0.66 0.57 0.60 0.49 0.57 0.55 Lr 4.7 4.5 4.6 3.5 6.3 5.0 5.9 3.3 3.4 2.8 2.9 3.2 Dr 8.5 8.4 9.0 7 13.3 10.1 11.8 8 5.5 5 5 6 Cr 0.00051 0.0011 0.0012 0.0090 0.00036 0.00099 0.00094 0.028 0.00081 0.0043 0.0023 0.0047 Table 1: Topological statistics for nouns, verbs, adjectives and adverbs for our three gold standard resources 1e-04 0.001 0.01 0.1 1 1 10 100 1000 10000 P( k ) k Roget?s WordNet Moby Random Figure 1: Degree distributions for nouns nected subgraph for the four parts of speech considered, along with statistics for random graphs of equivalent size and average degree (subscript r).
In all cases the clustering coefficient is significantly higher than that for the random graph.
While the characteristic path length and diameter are larger than for the random graphs, they are still short in comparison to an equivalent latice.
This, combined with the high clustering coefficient, indicates that they are producing small-world networks.
The diameter is larger still than for the random graphs.
Together these indicate a more lattice like structure, which is consistent with the intuition that dissimilar words are unlikely to share similar words.
This is independent of part of speech.
Figure 1 shows the degree distributions for nouns, and for a random graph plotted on log-log axes.
Other parts of speech produce equivalent graphs.
These clearly show that we have not produced scalefree networks as we are not seeing straight line power law distributions.
Instead we are seeing what is closer to singleor broad-scale distributions.
The differences in the graphs is explained by the WordNet Roget?s Hyp Synset Para Cat n 11,746 118,264 15,517 27,989 29,431 ?k??4.58 6.61 8.97 26.84 140.36 L 9.8 6.3 6.5 4.3 2.9 D 27 16.4 21.4 12.6 7 C 0.63 0.74 0.74 0.85 0.86 Table 2: Effect of adding hyponym relations granularity of the synonymy relations presented, as indicated by the characteristic path length.
WordNet has fine grained synsets and the smallest characteristic path length, while Moby has coarse grained synonyms and the largest characteristic path length.
4.2 Synonymy-Like Relations Having seen that synonymy and homonymy alone do not produce scale-free networks, we investigate the synonymy-like relations of hyponymy and topic relatedness.
Hyponymy is the IS-A class subsumption relationship and occurs between noun synsets in WordNet.
Topic relatedness occurs in the grouping of synsets in Roget?s in paragraphs and categories.
Table 2 compares adding hyponym edges to the graph of WordNet nouns and increasing the granularity of Roget?s synsets using edges between all words in a paragraph or category.
Adding hyponymy relations increases the connectivity of the graph significantly and there are no longer any disconnected subgraphs.
At the same time the diameter is nearly halved and characteristic path length reduce one third, but average degree only increases by one third.
To achieving the same reduction in path length and diameter by the granularity of Roget?s requires the average degree to increase by nearly three times.
Figure 2 shows the degree distributions when hyponyms are added to WordNet nouns and the granularity of Roget?s is increased.
Roget?s category level graph is omitted for clarity.
We can see that the orig77 1e-05 1e-04 0.001 0.01 0.1 1 1 10 100 1000 10000 P( k ) k Roget?s Paragraph WordNet Hyponym Figure 2: Degree distributions adding hyponym relations to nouns inally broad-scale structure of the Roget?s distribution is tending to have a more gaussian distribution.
The addition of hyponyms produces a power law distribution for k > 10 of P(k) ??k??.7.
Additional constraints on attachment reduce the ability of networks to be scale-free (Amaral et al., 2000).
The difference between synonymyhomonymy networks and association networks can be explained by this.
Steyvers and Tenenbaum (2005) propose a plausible attachment model for their association networks which has no additional constraint function.
If we use the tendency for languages to avoid lexical ambiguity from synonymy and homonymy as a constraint to the production of edges we will produce broad-scale networks rather than scale-free networks.
As hyponymy is primarily semantic and does not produce lexical ambiguity, adding hyponym edges weakens the constraint on ambiguity, producing a scale-free network.
Generalising synonymy to include topicality weakens the constraints, but at the same time reduces preference in attachment.
The results of this is the gaussian-like distribution with very few low degree nodes.
The difference between this thesaurus based topicality and that found in human association data is that human association data only includes the most similar words.
5 Distributional
Similarity Networks Lexical semantic resources can be automatically extracted using distributional similarity.
Here words are projected into a vector space using the contexts in which they appear as axes.
Contexts can be as 1e-05 1e-04 0.001 0.01 0.1 1 1 10 100 1000 10000 P( k ) k k=5 *k=5 k=50 *k=50 Figure 3: Degree distributions of Jaccard wide as document (Landauer and Dumais, 1997) or close as grammatical dependencies (Grefenstette, 1994).
The distance between words in this space approximates the similarity measured by synonymy.
We use the noun similarities produced by Gorman and Curran (2006) using the weighted Jaccard measure and the t-test weight and grammatical relations extracted from their LARGE corpus, the method found to perform best against their goldstandard evaluation.
Only words with a corpus frequency higher than 100 are included.
This method is comparable to that used in LSA, although using grammatical relations as context produces similarity much more like synonymy than those taken at a document level (Kilgarriff and Yallop, 2000).
Distributional similarity produces a list of vocabulary words, their similar neighbours and the similarity to the neighbours.
These lists approximate synonymy by measuring substitutability in context, and do not only find synonyms as near neighbours as both antonyms and hyponyms are frequently substitutable in a grammatical context (Weeds, 2003).
From this we generate graphs by taking either the k nearest neighbours to each word (k-NN), or by using a threshold.
To produce a threshold we take the mean similarity of the kth neighbour of all words (*kNN).
We compare both these methods.
Figure 3 compares the degree distributions of these.
Using k-NN produces a degree distribution that is close to a Gaussian, where as *k-NN produces a distribution much more like that of our expert compiled resources.
This is unsurprising when the distribution of distributional distances is considered.
Some words will have many near neighbours, 78 Roget?s WordNet Hyp k-NN *k-NN n 15,517 11,746 118,264 35,592 19,642 ?k??8.97 4.58 6.61 8.26 13.86 L 6.5 9.8 6.3 6.2 6.4 D 21.4 27 16.4 12 25.6 C 0.74 0.63 0.74 0.18 0.37 Table 3: Comparing nouns in expert and distributional resources and other few.
In the first case, k-NN will fail to include some near neighbours, and in the second will include some distant neighbours that are note semantically related.
This result is consistent between k = 5 and 50.
Introduction of random edges from the noise of distant neighbours reduces the diameter and missing near neighbours reduces the clustering coefficient (Table 3).
In Table 3 we also compare these to noun synonymy in Roget?s, and to synonymy and hyponymy in WordNet.
Distributional similarity (*k-NN) produces a network with similar degree, characteristic path length and diameter.
The clustering coefficient is much less than that from expert resources, is still several orders of magnitude larger than an equivalent random graph (Table 1).
Figure 4 compares a distributional network to networks WordNet and Moby.
We can see the same broad-scale in the distributional and synonym networks, and a distinct difference with the scale-free WordNet hyponym distribution.
The distributional similarity distribution is similar to that found in networks formed from LSA by Steyvers and Tenenbaum (2005).
Steyvers and Tenenbaum hypothesise that the distributions produced by LSA might be due more to frequency distribution effects that correct language modelling.
In light of our analysis of synonymy relations, we propose a new explanation.
Given that: distributional similarity has been shown to approximate the semantic similarity in synonymy relations found in thesaurus type resources (Curran, 2004); distributional similarity produces networks with similar statistical properties to those formed by synonym and homonymy relations; and, the synonym and homonymy relations found in thesauri produce networks with different statistical properties to those found in the association networks analysed by Steyvers and Tenenbaum; it can be plausibly 1e-04 0.001 0.01 0.1 1 1 10 100 1000 10000 P( k ) k WordNet Hyponym Moby *k=5 Figure 4: Degree distributions for nouns hypothesised that distributional techniques are modeling the acquisition of synonyms and homonyms, rather than all semantic relationships.
This is given further credence by experimental findings that acquisition of homonyms occurs at a different rate to the acquisition of vocabulary.
This indicates that there are different mechanisms for learning the meaning of lexical items and learning to relate the meanings of lexical items.
Any wholelanguage model would then be composed of a common set of lexical items related by disparate relations, such as synonymy, homonymy and hyponymy.
This type of model is predicted by spreading activation (Collins and Loftus, 1975).
It is unfortunate that there is a lack of data with which to validate this model, or our constraint model, empirically.
This should not prevent further analysis of network models that distiguish semantic relations, so long as this limitation is understood.
6 Conclusion
Semantic networks have been used successfully to explain access to the mental lexicon.
We use both expert-compiled and automatically extracted semantic resources, we compare the networks formed from semantic association and synonymy and homonymy.
These relations produce small-world networks, but do not share the same scale-free properties as for semantic association.
We find that this difference can be explained using a constrained attachment model informed by childhood language acquisition experiments.
It is also predicted by spreading-activation theories of seman79 tic access where a common set of lexical items is connected by a disparate set of relations.
We further find that distributional models of language acquisition produce relations that approximate synonymy and networks topologically similar to synonymyhomonymy networks.
7 Acknowledgements
We would like to thank the anonymous reviewers for their helpful feedback and corrections.
This work has been supported by the Australian Research Council under Discovery Projects DP0453131 and DP0665973.
References Reka Albert, Hawoong Jeong, and Albert-Laszlo Barabasi.
2000. Error and attack tolerance of complex networks.
Nature, 406:378??81.
Lus A.
Nunes Amaral, Antonio Scala, Marc Barthelemy, and H.
Eugene Stanley.
2000. Classes of small-world networks.
Proceedings of the National Academy of Sciences, 97(21):11149??1152, October 10.
Ramon F.
i Cancho and Ricard V.
Sole. 2001.
The small world of human language.
Proceedings of The Royal Society of London.
Series B, Biological Sciences, 268(1482):2261?? 2265, November.
Devin M.
Casenhiser. 2005.
Children?s resistance to homonymy: an experimental study of pseudohomonyms.
Journal of Child Language, 32:319??43.
Allan M.
Collins and Elizabeth F.
Loftus. 1975.
A spreading-activation theory of semantic processing.
Psychological review, 82(6):407??28.
Allan M.
Collins and M.
Ross Quillian.
1969. Retrieval time from semantic memory.
Journal of Verbal Learning and Verbal Behavior, 8:240??47.
James R.
Curran. 2004.
From Distributional to Semantic Similarity.
Ph.D. thesis, University of Edinburgh.
Martin Doherty and Josef Perner.
1998. Metalinguistic awareness and theory of mind: just two words for the same thing?
Congitive Development, 13:279??05.
Michalis Faloutsos, Petros Faloutsos, and Christos Faloutsos.
1999. On power-law relationships of the internet topology.
In Proceedings of the conference on Applications, technologies, architectures, and protocols for computer communication, pages 251??62.
Christiane Fellbaum, editor.
1998. WordNet: an electronic lexical database.
The MIT Press, Cambridge, MA, USA.
Wendy A.
Garnham, Julie Brooks, Alan Garnham, and Anne-Marie Ostenfeld.
2000. From synonyms to homonyms: exploring the role of metarepresentation in language understanding.
Developmental Science, 3(4):428??41.
James Gorman and James R.
Curran. 2006.
Scaling distributional similarity to large corpora.
In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics, Sydney, Australia, 17??1 July.
Gregory Grefenstette.
1994. Explorations in Automatic Thesaurus Discovery.
Kluwer Academic Publishers, Boston.
Evelyn F.
Keller. 2005.
Revisiting ?scale-free??networks.
Bioessays, 27(10):1060??068, October.
Adam Kilgarriff and Colin Yallop.
2000. What?s in a thesaurus?
In Proceedings of the Second International Conference on Language Resources and Evaluation, pages 1371??1379.
Thomas K.
Landauer and Susan T.
Dumais. 1997.
A solution to plato?s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.
Psychological Review, 104(2):211??40, April.
Douglas L.
Nelson, Cathy L.
McEvoy, and Thomas A.
Schreiber. 1998.
The university of south florida word association, rhyme, and word fragment norms.
http://www.usf.edu/FreeAssociation/. L.
John Old.
2003. The Semantic Structure of Roget?s, a Whole-Language Thesaurus.
Ph.D. thesis, Indiana University.
Adwait Ratnaparkhi.
1996. A maximum entropy part-of-speech tagger.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133??142, 17??8 May.
Mariano Sigman and Guillermo A.
Cecchi. 2002.
Global organization of the WordNet lexicon.
Proceedings of the National Academy of Sciences, 99(3):1742??747.
Mark Steyvers and Joshua B.
Tenenbaum. 2005.
The large-scale structure of semantic networks: statistical analyses and a model of semantic growth.
Cognitive Science, 29(1):41??78.
Christopher Stokoe.
2005. Differentiating homonymy and polysemy in information retrieval.
In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 403??10.
Amos Tversky.
1977. Features of similarity.
Psychological Review, 84(4):327??52, July.
Duncan J.
Watts and Steven H.
Strogatz. 1998.
Collective dynamics of small-world networks, 393:440??42, 4 June.
Duncan J.
Watts. 1999.
Small Worlds: The Dynamics of Networks between Order and Randomness.
Princeton University Press, Princeton, NJ, USA.
Julie E.
Weeds. 2003.
Measures and Applications of Lexical Distributional Similarity.
Ph.D. thesis, University of Sussex .
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 81??8, Prague, Czech Republic, June 2007 c2007 Association for Computational Linguistics The Benefits of Errors: Learning an OT Grammar with a Structured Candidate Set Tamas Biro ACLC, Universiteit van Amsterdam Spuistraat 210 Amsterdam, The Netherlands t.s.biro@uva.nl Abstract We compare three recent proposals adding a topology to OT: McCarthy?s Persistent OT, Smolensky?s ICS and Bro?s SA-OT.
To test their learnability, constraint rankings are learnt from SA-OT?s output.
The errors in the output, being more than mere noise, follow from the topology.
Thus, the learner has to reconstructs her competence having access only to the teacher?s performance.
1 Introduction: topology and OT The year 2006 witnessed the publication of several novel approaches within Optimality Theory (OT) (Prince and Smolensky, 1993 aka 2004) introducing some sort of neighbourhood structure (topology, geometry) on the candidate set.
This idea has been already present since the beginnings of OT but its potentialities had never been really developed until recently.
The present paper examines the learnability of such an enriched OT architecture.
Traditional Optimality Theory?s GEN function generates a huge candidate set from the underlying form (UF) and then EVAL finds the candidate w that optimises the Harmony function H(w) on this unrestricted candidate set.
H(w) is derived from the violation marks assigned by a ranked set of constraints to w.
The surface form SF corresponding to UF is the (globally) optimal element of GEN(UF): SF(UF) = argoptw?GEN(UF)H(w) (1) Yet, already Prince and Smolensky (1993/2004:94-95) mention the possibility of restricting GEN, creating an alternative closer to standard derivations.
Based the iterative syllabification in Imdlawn Tashlhiyt Berber, they suggest: ?some general procedure (Do-) is allowed to make a certain single modification to the input, producing the candidate set of all possible outcomes of such modification.??The outputs of Doare ?neighbours??of its input, so Dodefines a topology.
Subsequently, EVAL finds the most harmonic element of this restricted candidate set, which then serves again as the input of Do-.
Repeating this procedure again and again produces a sequence of neighbouring candidates with increasing Harmony, which converges toward the surface form.
Calling Doa restricted GEN, as opposed to the freedom of analysis offered by the traditional GEN, McCarthy (2006) develops this idea into the Persistent OT architecture (aka.
harmonic serialism, cf.
references in McCarthy 2006).
He demonstrates on concrete examples how repeating the GEN ??
EVAL ??GEN ??EVAL ??..
cycle until reaching some local optimum will produce a more restrictive language typology that conforms rather well to observation.
Importantly for our topic, learnability, he claims that Persistent OT ?can impose stricter ranking requirements than classic OT because of the need to ensure harmonic improvement in the intermediate forms as well as the ultimate output??
In two very different approaches, both based on the traditional concept of GEN, Smolensky?s Integrated Connectionist/Symbolic (ICS) Cognitive Architecture (Smolensky and Legendre, 2006) and the strictly symbolic Simulated Annealing for Optimality Theory Algorithm (SA-OT) proposed by 81 Bro (2005a; 2005b; 2006a), use simulated annealing to find the best candidate w in equation (1).
Simulated annealing performs a random walk on the search space, moving to a similar (neighbouring) element in each step.
Hence, it requires a topology on the search space.
In SA-OT this topology is directly introduced on the candidate set, based on a linguistically motivated symbolic representation.
At the same time, connectionist OT makes small changes in the state of the network; so, to the extent that states correspond to candidates, we obtain again a neighbourhood relation on the candidate set.
Whoever introduces a neighbourhood structure (or a restricted GEN) also introduces local optima: candidates more harmonic than all their neighbours, independently of whether they are globally optimal.
Importantly, each proposal is prone to be stuck in local optima.
McCarthy?s model repeats the generation-evaluation cycle as long as the first local optimum is not reached; whereas simulated annealing is a heuristic optimisation algorithm that sometimes fails to find the global optimum and returns another local optimum.
How do these proposals influence the OT ?philosophy??
For McCarthy, the first local optimum reached from UF is the grammatical form (the surface form predicted by the linguistic competence model), so he rejects equation (1).
Yet, Smolensky and Bro keep the basic idea of OT as in (1), and Bro (2005b; 2006a) shows the errors made by simulated annealing can mimic performance errors (such as stress shift in fast speech).
So mainstream Optimality Theory remains the model of linguistic competence, whereas its cognitively motivated, though imperfect implementation with simulated annealing becomes a model of linguistic performance.
Or, as Bro puts it, a model of the dynamic language production process in the brain.
(See also Smolensky and Legendre (2006), vol.
1, pp.
227-229.) In the present paper we test the learnability of an OT grammar enriched with a neighbourhood structure.
To be more precise, we focus on the latter approaches: how can a learner acquire a grammar, that is, the constraint hierarchy defining the Harmony function H(w), if the learning data are produced by a performance model prone to make errors?
What is the consequence of seeing errors not simply as mere noise, but as the result of a specific mechanism? 2 Walking in the candidate set First, we introduce the production algorithms (section 2) and a toy grammar (section 3), before we can run the learning algorithms (section 4).
Equation (1) defines Optimality Theory as an optimisation problem, but finding the optimal candidate can be NP-hard (Eisner, 1997).
Past solutions?? chart parsing (Tesar and Smolensky, 2000; Kuhn, 2000) and finite state OT (see Biro (2006b) for an overview)?require conditions met by several, but not by all linguistic models.
They are also ?too perfect?? not leaving room for performance errors and computationally too demanding, hence cognitively not plausible.
Alternative approaches are heuristic optimization techniques: genetic algorithms and simulated annealing.
These heuristic algorithms do not always find the (globally) optimal candidate, but are simple and still efficient because they exploit the structure of the candidate set.
This structure is realized by a neighbourhood relation: for each candidate w there exists a set Neighbours(w), the set of the neighbours of w.
It is often supposed that neighbours differ only minimally, whatever this means.
The neighbourhood relation is usually symmetric, irreflexive and results in a connected structure (any two candidates are connected by a finite chain of neighbours).
The topology (neighbourhood structure) opens the possibility to a (random) walk on the candidate set: a series w0,w1,w2,...,wL such that for all 0 ??i < L, candidate wi+1 is wi or a neighbour of wi.
(Candidate w0 will be called winit, and wL will be wfinal, henceforth).
Genetic algorithms start with a random population of winit?s, and employ OT?s EVAL function to reach a population of wfinal?s dominated by the (globally) optimal candidate(s) (Turkel, 1994).
In what follows, however, we focus on algorithms using a single walk only.
The simplest algorithm, gradient descent, comes in two flavours.
The version on Fig.
1 defines wi+1 as the best element of setwi}?Neighbours(wi).
It runs as long as wi+1 differs from wi, and is deterministic for each winit.
Prince and Smolensky?s and McCarthy?s serial evaluation does exactly this: winit is the underlying form, Do(the restricted GEN) creates the set {w}?Neighbours(w), and EVAL finds its best element.
82 ALGORITHM Gradient Descent: OT with restricted GEN w := w_init; repeat w_prev := w; w := most_harmonic_element( {w_prev} U Neighbours(w_prev) ); until w = w_prev return w # w is an approximation to the optimal solution Figure 1: Gradient Descent: iterated Optimality Theory with a restricted GEN (Do-).
ALGORITHM Randomized Gradient Descent w := w_init ; repeat Randomly select w??from the set Neighbours(w); if (w??not less harmonic than w) then w := w?? until stopping condition = true return w # w is an approximation to the optimal solution Figure 2: Randomized Gradient Descent The second version of gradient descent is stochastic (Figure 2).
In step i, a random w????Neighbours(wi) is chosen using some pre-defined probability distribution on Neighbours(wi) (often a constant function).
If neighbour w??is not worse than wi, then the next element wi+1 of the random walk will be w?? otherwise, wi+1 is wi.
The stopping condition requires the number of iterations reach some value, or the average improvement of the target function in the last few steps drop below a threshold.
The output is wfinal, a local optimum if the walk is long enough.
Simulated annealing (Fig.
3) plays with this second theme to increase the chance of finding the global optimum and avoid unwanted local optima.
The idea is the same, but if w??is worse than wi, then there is still a chance to move to w??
The transition probability of moving to w??depends on the target function E at wi and w?? and on ?temperature??T: P(wi ??w??T) = exp parenleftBig ?E(w???E(wi)T parenrightBig. Using a random r, we move to w??iff r < P(wi ??w??T).
Temperature T is gradually decreased following the cooling schedule.
Initially the system easily climbs larger hills, but later it can only descend valleys.
Importantly, the probability wfinal is globally optimal converges to 1 as the number of iterations grows.
But the target function is not real-valued in Optimality Theory, so how can we calculate the transition probability?
ICS (Smolensky and Legendre, 2006) approximates OT?s harmony function with a real-valued target function, while Bro (2006a) introduces a novel algorithm (SA-OT, Figure 4) to guarantee the principle of strict domination in the constraint ranking.
The latter stays on the purely symbolic level familiar to the linguist, but does not always display the convergence property of traditional simulated annealing.
Temperature in the SA-OT Algorithm is a pair (K,t) with t > 0, and is diminished in two, embedded loops.
Similarly, the difference in the target function (Harmony) is not a single real number but a pair (C,d).
Here C is the fatal constraint, the highest ranked constraint by which wi and w??behave differently, while d is the difference of the violations of this constraint.
(For H(wi) = H(w?? let the difference be (0,0)).
Each constraint is assigned a realvalued rank (most often an integer; we shall call it a K-value) such that a higher ranked constraint has a higher K-value than a lower ranked constraint (hierarchies are fully ranked).
The K-value of the fatal constraint corresponds to the first component of the temperature, and the second component of the difference in the target function corresponds to the second component of the temperature.
The transition probability from wi to its neighbour w??is 1 if w??is not less harmonic than wi; otherwise, the originally exponential transition probability becomes P parenleftbigwi ??w??(K,t)parenrightbig = ?   1 if K-value of C< K e?dt if K-value of C= K 0 if K-value of C> K 83 ALGORITHM Simulated Annealing w := w_init ; T := T_max ; repeat CHOOSE random w??in Neighbours(w); Delta := E(w?? E(w); if ( Delta < 0 ) then w := w?? else # move to w??with transition probability P(Delta;T) = exp(-Delta/T): generate random r uniformly in range (0,1); if ( r < exp(-Delta / T) ) then w := w??
T := alpha(T); # decrease T according to some cooling schedule until stopping condition = true return w # w is an approximation to the minimal solution Figure 3: Minimizing a real-valued energy function E(w) with simulated annealing.
Again, wi+1 is w??if the random number r generated between 0 and 1 is less than this transition probability; otherwise wi+1 = wi.
Bro (2006a, Chapt.
2-3) argues that this definition fits best the underlying idea behind both OT and simulated annealing.
In the next part of the paper we focus on SA-OT, and return to the other algorithms afterwards only.
3 A
string grammar To experiment with, we now introduce an abstract grammar that mimics real phonological ones.
Let the set of candidates generated by GEN for any input be {0,1,...,P ??}L, the set of strings of length L over an alphabet of P phonemes.
We shall use L = P = 4.
Candidate w??is a neighbour of candidate w if and only if a single minimal operation (a basic step) transforms w into w??
A minimal operation naturally fitting the structure of the candidates is to change one phoneme only.
In order to obtain a more interesting search space and in order to meet some general principles?the neighbourhood relation should be symmetric, yielding a connected graph but be minimal?a basic step can only change the value of a phoneme by 1 modulo P.
For instance, in the L = P = 4 case, neighbours of 0123 are among others 1123, 3123, 0133 and 0120, but not 1223, 2123 or 0323.
If the four phonemes are represented as a pair of binary features (0 = [?], 1 = [+??, 2 = [++] and 3 = [??]), then this basic step alters exactly one feature.
We also need constraints.
Constraint No-n counts the occurrences of phoneme n (0 ??n < P) in the candidate (i.e., assigns one violation mark per phoneme n).
Constraint No-initial-n punishes phoneme n word initially only, whereas No-final-n does the same word finally.
Two more constraints sum up the number of dissimilar and similar pairs of adjacent phonemes.
Let w(i) be the ith phoneme in string w, and let [b] = 1 if b is true and [b] = 0 if b is false; then we have 3P + 2 markedness constraints: No-n: non(w) = summationtextL??i=0 [w(i) = n] No-initial-n: nin(w) = [w(0) = n] No-final-n: nfn(w) = [w(L)??
= n] Assimilate: ass(w) = summationtextL??i=0 [w(i) negationslash= w(i+1)] Dissimilate: dis(w) = summationtextL??i=0 [w(i) = w(i+1)] Grammars also include faithfulness constraints punishing divergences from a reference string ?, usually the input.
Ours sums up the distance of the phonemes in w from the corresponding ones in ?: FAITH?(w) = summationtextL??i=0 d(?(i),w(i)) where d(a,b) = min((a ??b) mod P,(b ??a) mod P)) is the minimal number of basic steps transforming phoneme a into b.
In our case, faithfulness is also the number of differing binary features.
To illustrate SA-OT, we shall use grammar H: H: no0 ??ass ??Faith?=0000 ??ni1 ?? ni0 ??ni2 ??ni3 ??nf0 ??nf1 ??nf2 ?? nf3 ??no3 ??no2 ??no1 ??dis A quick check proves that the global optimum is candidate 3333, but there are many other local optima: 1111, 2222, 3311, 1333, etc.
Table 1 shows the frequencies of the outputs as a function of t step, all other parameters kept unchanged.
Several characteristics of SA-OT can be observed.
For high t step, the thirteen local optima ({1,3}4 and 2222) are all produced, but as the number of 84 ALGORITHM Simulated Annealing for Optimality Theory w := w_init ; for K = K_max to K_min step K_step for t = t_max to t_min step t_step CHOOSE random w??in Neighbours(w); COMPARE w??to w: C := fatal constraint d := C(w?? C(w); if d <= 0 then w := w?? else w := w??with transition probability P(C,d;K,t) = 1, if K-value(C) < K = exp(-d/t), if K-value(C) = K = 0, if K-value(C) > K end-for end-for return w # w is an approximation to the optimal solution Figure 4: The Simulated Annealing for Optimality Theory Algorithm (SA-OT).
iterations increases (parameter t step drops), the probability of finding the globally optimal candidate grows.
In many grammars (e.g., ni1 and ni3 moved to between no0 and ass in H), the global optimum is the only output for small t step values.
Yet, H also yields irregular forms: 1111 and 2222 are not globally optimal but their frequencies grow together with the frequency of 3333.
4 Learning
grammar from performance To summarise, given a grammar, that is, a constraint hierarchy, the SA-OT Algorithm produces performance forms, including the grammatical one (the global optimum), but possibly also irregular forms and performance errors.
The exact distribution depends on the parameters of the algorithm, which are not part of the grammar, but related to external (physical, biological, pragmatic or sociolinguistic) factors, for instance, to speech rate.
Our task of learning a grammar can be formulated thus: given the output distribution of SA-OT based on the target OT hierarchy (the target grammar), the learner seeks a hierarchy that produces a similar performance distribution using the same SA-OT Algorithm.
(See Yang (2002) on grammar learning as parameter setting in general).
Without any information on grammaticality, her goal is not to mimic competence, not to find a hierarchy with the same global optima.
The grammar learnt can diverge from the target hierarchy, as long as their performance is comparable (see also Apoussidou (2007), p.
203). For instance, if ni1 and ni3 change place in grammar H, the grammaticality of 1111 and 3333 are reversed, but the performance stays the same.
This resembles two native speakers whose divergent grammars are revealed only when they judge differently forms otherwise produced by both.
We suppose that the learner employs the same SA-OT parameter setting.
The acquisition of the parameters is deferred to future work, because this task is not part of language acquisition but of social acculturation: given a grammar, how can one learn which situation requires what speed rate or what level of care in production?
Consequently, finetuning the output frequencies, which can be done by fine-tuning the parameters (such as t step) and not the grammar, is not our goal here.
But language learners do not seem to do it, either.
Learning algorithms in Optimality Theory belong to two families: off-line and on-line algorithms.
Offline algorithms, the prototype of which is Recursive Constraint Demotion (RCD) (Tesar, 1995; Tesar and Smolensky, 2000), first collect the data and then attempt to build a hierarchy consistent with them.
On-line algorithms, such as Error Driven Constraint Demotion (ECDC) (Tesar, 1995; Tesar and Smolensky, 2000) and Gradual Learning Algorithm (GLA) (Boersma, 1997; Boersma and Hayes, 2001), start with an initial hierarchy and gradually alter it based on discrepancies between the learning data and the data produced by the learner?s current hierarchy.
Since infants gather statistical data on their mother tongue-to-be already in pre-linguistic stages (Saffran et al., 1996; Gervain et al., submitted), an off-line algorithm created our initial grammar.
Then, on-line learning refined it, modelling child language 85 output t step = 1 t step = 0.1 t step = 0.01 t step = 0.001 3333 0.1174  0.0016 0.2074  0.0108 0.2715  0.0077 0.3107  0.0032 1111 0.1163  0.0021 0.2184  0.0067 0.2821  0.0058 0.3068  0.0058 2222 0.1153  0.0024 0.2993  0.0092 0.3787  0.0045 0.3602  0.0091 1133 0.0453  0.0018 0.0485  0.0038 0.0328  0.0006 0.0105  0.0014 3311 0.0436  0.0035 0.0474  0.0054 0.0344  0.0021 0.0114  0.0016 others 0.5608 0.1776 < 0.0002 ??
Table 1: Outputs of SA-OT for hierarchy H.
?Others??are twelve forms, each with a frequency between 2% and 8% for t step = 1, and lower than 4.5% for t step = 0.1.
(Forms produced in 8% of the cases at t step = 1 are not produced if t step = 0.01)!
An experiment consisted of running 4096 simulations and counting relative frequencies; each cell contains the mean and standard deviation of three experiments.
development. (Although on-line algorithms require virtual production only, not necessarily uttered in communication, we suppose the two go together.) We defer for future work issues as parsing hidden structures, learning underlying forms and biases for ranking markedness above faithfulness.
4.1 Learning
SA-OT We first implemented Recursive Constraint Demotion with SA-OT.
To begin with, RCD creates a winner/loser table, in which rows correspond to pairs (w,l) such that winner w is a learning datum, and loser l is less harmonic than w.
Column winner marks contains the constraints that are more severely violated by the winner than by the loser, and viceversa for column loser marks.
Subsequently, RCD builds the hierarchy from top.
It repeatedly collects the constraints not yet ranked that do not occur as winner marks.
If no such constraint exists, then the learning data are inconsistent.
These constraints are then added to the next stratum of the hierarchy in a random order, while the rows in the table containing them as loser marks are deleted (because these rows have been accounted for by the hierarchy).
Given the complexity of the learning data produced by SA-OT, it is an advantage of RCD that it recognises inconsistent data.
But how to collect the winner-loser pairs for the table?
The learner has no information concerning the grammaticality of the learning data, and only knows that the forms produced are local optima for the target (unknown) hierarchy and the universal (hence, known) topology.
Thus, we constructed the winner-loser table from all pairs (w,l) such that w was an observed form, and l was a neighbour of w.
To avoid the noise present in real-life data, we considered only w?s with a frequency higher than ?N, where N was the number of learning data.
Applying then RCD resulted in a hierarchy that produced the observed local optima?? and most often also many others, depending on the random constraint ranking in a stratum.
These unwanted local optima suggest a new explanation of some ?child speech forms??
Therefore, more information is necessary to find the target hierarchy.
As learners do not use negative evidence (Pinker, 1984), we did not try to remove extra local optima directly.
Yet, the learners do collect statistical information.
Accordingly, we enriched the winner/loser table with pairs (w,l) such that w was a form observed significantly more frequently than l; l?s were observed forms and the extra local optima.
(A difference in frequency was significant if it was higher than ?N).
The assumption that frequency reflects harmony is based on the heuristics of SA-OT, but is far not always true.
So RCD recognised this new table often to be inconsistent.
Enriching the table could also be done gradually, adding a new pair only if enough errors have supported it (Error-Selective Learning, Tessier (2007).
The pair is then removed if it proves inconsistent with stronger pairs (pairs supported by more errors, or pairs of observed forms and their neighbours).
Yet, we instead turned to real on-line algorithms, namely to Boersma?s Gradual Learning Algorithm (GLA) (Boersma, 1997).
(Error Driven Constraint Demotion is not robust, and gets stuck for inconsistent data).
Similarly to Error-Selective Learning, GLA accumulates gradually the arguments for 86 reranking two constraints.
The GLA Algorithm assigns a real-valued rank r to each constraint, so that a higher ranked constraint has a higher r.
Then, in each learning step the learning datum (the winner) is compared to the output produced by the learner?s actual hierarchy (the loser).
Every constraint?s rank is decreased by a small value (the plasticity) if the winner violates it more than the loser, and it is increased by the same value if the loser has more violations than the winner.
Often?still, not always (Pater, 2005)?these small steps accumulate to converge towards the correct constraint ranking.
When producing an output (the winner) for the target hierarchy and another one (the loser) for the learner?s hierarchy, Boersma uses Stochastic OT (Boersma, 1997).
But one can also employ traditional OT evaluation, whereas we used SA-OT with t step = 0.1.
The learner?s actual hierarchy in GLA is stored by the real-valued ranks r.
So the fatal constraint in the core of SA-OT (Fig.
4) is the constraint that has the highest r among the constraints assigning different violations to w and w??
(A random one of them, if more constraints have the same r-values, but this is very rare.).
The K-values were the floor of the r-values.
(Note the possibility of more constraints having the same K-value.) The r-values could also be directly the K-values; but since parameters K max,K minand K stepare integers, this would cause the temperature not enter the domains of the constraints, which would skip an important part of simulated annealing.
Similarly to Stochastic OT, our model also displayed different convergence properties of GLA.
Quite often, GLA reranked its initial hierarchy (the output of RCD) into a hierarchy yielding the same or a similar output distribution to that produced by the target hierarchy.
The simulated child?s performance converged towards the parent?s performance, and ?child speech forms??were dropped gradually.
In other cases, however, the GLA algorithm turned the performance worse.
The reason for that might be more than the fact that GLA does not always converge.
Increasing or decreasing the constraints??rank by a plasticity in GLA is done in order to make the winners gradually better and the losers worse.
But in SA-OT the learner?s hierarchy can produce a form that is indeed more harmonic (but not a local optimum) for the target ranking than the learning datum; then the constraint promotions and demotions miss the point.
Moreover, unlike in Stochastic OT, these misguided moves might be more frequent than the opposite moves.
Still, the system performed well with our grammar H.
Although the initial grammars returned by RCD included local optima (?child speech forms?? e.g., 0000), learning with GLA brought the learner?s performance most often closer to the teacher?s.
Still, final hierarchies could be very diverse, with different global optima and frequency distributions.
In another experiment the initial ranking was the target hierarchy.
Then, 13 runs returned the target distribution with some small changes in the hierarchy; in five cases the frequencies changed slightly, but twice the distribution became qualitatively different (e.g., 2222 not appearing).
4.2 Learning
in other architectures Learning in the ICS architecture involves similar problems to those encountered with SA-OT.
The learner is faced again with performance forms that are local optima and not always better than unattested forms.
The learning differs exclusively as a consequence of the connectionist implementation.
In McCarthy?s Persistent OT, the learner only knows that the observed form is a local optimum, i.
e., it is better than all its neighbours.
Then, she has to find a path backwards, from the surface form to the underlying form, such that in each step the candidate closer to the SF is better than all other neighbours of the candidate closer to the UF.
Hence, the problem is more complex, but it results in a similar winner/loser table of locally close candidates.
5 Conclusion
and future work We have tested the learnability of an OT grammar enriched with a neighbourhood structure.
The learning data were produced by a performance model (viz., SA-OT), so the learner only had access to the teacher?s performance.
But by knowing the mechanism distorting production, she still could learn the target competence more or less.
(Minor differences in competence are possible, as long as the performance is very similar).
She made use of the structure (the topology) of the candidate set, but also of the observed error patterns.
Future work may exploit 87 the fact that different parameter settings of SA-OT yield different distributions.
Not correctly reconstructed grammars often lead to different grammaticality judgements, but also to quantitative differences in the performance distribution, despite the qualitative similarity.
This fact can explain diachronic changes and why some grammars are evolutionarily more stable than others.
Inaccurate reconstruction, as opposed to exact learning, is similar to what Dan Sperber and others said about symbolic-cultural systems: ?The tacit knowledge of a participant in a symbolic-cultural system is neither taught nor learned by rote.
Rather each new participant [...] reconstructs the rules which govern the symbolic-cultural system in question.
These reconstructions may differ considerably, depending upon such factors as the personal history of the individual in question.
Consequently, the products of each individual?s symbolic mechanism are idiosyncratic to some extent.??(Lawson and McCauley, 1990, p.
68, italics are original).
This observation has been used to argue that cultural learning is different from language learning; now we turn the table and claim that acquiring a language is indeed similar in this respect to learning a culture.
References Diana Apoussidou.
2007. The Learnability of Metrical Phonology.
Ph.D. thesis, University of Amsterdam.
Tamas Bro.
2005a. How to define Simulated Annealing for Optimality Theory?
In Proc.
10th FG and 9th MoL, Edinburgh.
Also ROA-8971.
Tamas Bro.
2005b. When the hothead speaks: Simulated Annealing Optimality Theory for Dutch fast speech.
In C.
Cremers et al., editor, Proc.
of the 15th CLIN, pages 13??8, Leiden.
Also ROA-898.
Tamas Bro.
2006a. Finding the Right Words: Implementing Optimality Theory with Simulated Annealing.
Ph.D. thesis, University of Groningen.
ROA-896. Tamas Bro.
2006b. Squeezing the infinite into the finite.
In A.
Yli-Jyr et al., editor, Finite-State Methods and Natural Language Processing, FSMNLP 2005, Helsinki, LNAI-4002, pages 21??1.
Springer. Paul Boersma and Bruce Hayes.
2001. Empirical tests of the Gradual Learning Algorithm.
Linguistic Inquiry, 32:45??6.
Also: ROA-348.
1ROA: Rutgers Optimality Archive at http://roa.rutgers.edu Paul Boersma.
1997. How we learn variation, optionality, and probability.
Proceedings of the Institute of Phonetic Sciences, Amsterdam (IFA), 21:43??8.
Jason Eisner.
1997. Efficient generation in primitive optimality theory.
In Proc.
of the 35th Annual Meeting of the Association for Computational Linguistics and 8th EACL, pages 313??20, Madrid.
Judit Gervain, Marina Nespor, Reiko Mazuka, Ryota Horie, and Jacques Mehler.
submitted. Boot strapping word order in prelexical infants: a japanese-italian cross-linguistic study.
Cognitive Psychology.
Jonas Kuhn.
2000. Processing optimality-theoretic syntax by interleaved chart parsing and generation.
In Proc.ACL-38, Hongkong, pages 360??67.
E. Thomas Lawson and Robert N.
McCauley. 1990.
Rethinking Religion: Connecting Cognition and Culture.
Cambridge University Press, Cambridge, UK.
John J.
McCarthy. 2006.
Restraint of analysis.
In E.
Bakovic et al., editor, Wondering at the Natural Fecundity of Things: Essays in Honor of A.
Prince, pages 195??19.
U. of California, Santa Cruz.
ROA-844. Joe Pater.
2005. Non-convergence in the GLA and variation in the CDA.
ms., ROA-780.
Steven Pinker.
1984. Language Learnability & Language Development.
Harvard UP, Cambridge, Mass.
Alan Prince and Paul Smolensky.
1993 aka 2004.
Optimality Theory: Constraint Interaction in Generative Grammar.
Blackwell, Malden, MA, etc.
Also: RuCCS-TR-2, 1993; ROA Version: 537-0802, http://roa.rutgers.edu, 2002.
Jenny R.
Saffran, Richard N.
Aslin, and Elissa L.
Newport. 1996.
Statistical learning by 8-month-old infants.
Science, 274(5294):1926??928.
Paul Smolensky and Geraldine Legendre.
2006. The Harmonic Mind: From Neural Computation to Optimality-Theoretic Grammar.
MIT P., Cambridge.
Bruce Tesar and Paul Smolensky.
2000. Learnability in Optimality Theory.
MIT Press, Cambridge, MA.
Bruce Tesar.
1995. Computational Optimality Theory.
Ph.D. thesis, University of Colorado.
Also: ROA-90.
Anne-Michelle Tessier.
2007. Biases and Stages in Phonological Acquisition.
Ph.D. thesis, University of Massachusetts Amherst.
Also: ROA-883.
Bill Turkel.
1994. The acquisition of Optimality Theoretic systems.
m.s., ROA-11.
Charles D.
Yang. 2002.
Knowledge and Learning in Natural Language.
Oxford U.
P., Oxford?New York .
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 89??6, Prague, Czech Republic, June 2007 c2007 Association for Computational Linguistics Learning to interpret novel noun-noun compounds: evidence from a category learning experiment Barry Devereux & Fintan Costello School of Computer Science and Informatics, University College Dublin, Belfield, Dublin 4, IRELAND {barry.devereux, fintan.costello}@ucd.ie Abstract The ability to correctly interpret and produce noun-noun compounds such as WIND FARM or CARBON TAX is an important part of the acquisition of language in various domains of discourse.
One approach to the interpretation of noun-noun compounds assumes that people make use of distributional informationabouthowtheconstituentwords of compounds tend to combine; another assumes that people make use of information about the two constituent concepts??features toproduceinterpretations.
Wepresentanexperiment that examines how people acquire both the distributional information and conceptual information relevant to compound interpretation.
A plausible model of the interpretation process is also presented.
1 Introduction
People frequently encounter noun-noun compounds such as MEMORY STICK and AUCTION POLITICS in everyday discourse.
Compounds are particularly interesting from a language-acquisition perspective: children as young as two can comprehend and produce noun-noun compounds (Clark & Barron, 1988), and these compounds play an important role in adult acquisition of the new language and terminology associated with particular domains of discourse.
Indeed, most new terms entering the English language are combinations of existing words (Cannon, 1987; consider FLASH MOB, DESIGNER BABY, SPEED DATING and CARBON FOOTPRINT).
These noun-noun compounds are also interesting from a computational perspective, in that they pose a significant challenge for current computational accounts of language.
This challenge arises from the fact that the semantics of noun-noun compounds are extremely diverse, with compounds utilizing many different relations between their constituent words (consider the examples at the end of the previous paragraph).
Despite this diversity, people typically interpret even completely novel compoundsextremelyquickly,intheorderofhundredths of seconds in reaction time studies.
One approach that has been taken in both cognitive psychology and computational linguistics can be termed the relation-based approach (e.g.
Gagne & Shoben, 1997; Kim & Baldwin, 2005).
In this approach, the interpretation of a compound is represented as the instantiation of a relational link between the modifier and head noun of the compound.
Such relations are usually represented as a set of taxonomic categories; for example the meaning of STUDENT LOAN might be specified with a POSSESSOR relation (Kim & Baldwin, 2005) or MILK COW might be specified by a MAKES relation (Gagne & Shoben, 1997).
However, researchers are not close to any agreement on a taxonomy of relation categories classifying noun-noun compounds; indeed a wide range of typologies have been proposed (e.g.
Levi, 1977; Kim & Baldwin, 2005).
In these relation-based approaches, there is often little focus on how the meaning of the relation interacts with the intrinsic properties of the constituent concepts.
Instead, extrinsic information about concepts, such as distributional information about how often different relations are associated with a concept, is used.
For example, Gagne & Shoben?s CARIN model utilizes the fact that the modifier MOUNTAIN is frequently associated with the LOCATED relation (in compounds such as MOUNTAIN CABIN or MOUNTAIN GOAT); the model does not utilize the fact that the concept MOUNTAIN has in89 trinsic properties such as is large and is a geological feature: features which may in general precipitate the LOCATION relation.
An approach that is more typical of psychological theories of compound comprehension can be termed the concept-based approach (Wisniewski, 1997; Costello and Keane, 2000).
With such theories, the focus is on the intrinsic properties of the constituent concepts, and the interpretation of a compound is usually represented as a modification of the head noun concept.
So, for example, the compound ZEBRA FISH may involve a modification of the FISH concept, by asserting a feature of the ZEBRA concept (e.g.
has stripes) for it; in this way, a ZEBRA FISH can be understood as a fish with stripes.
Concept-based theories do not typically use distributional information about how various relations are likely to be used with concepts.
The information assumed relevant to compound interpretation is therefore quite different in relationbased and concept-based theories.
However, neither approach typically deals with the issue of how people acquire the information that allows them to interpret compounds.
In the case of the relation-based approaches, for example, how do people acquire the knowledge that the modifier MOUNTAIN tends to be used frequently with the LOCATED relation and that this information is important in comprehending compounds with that modifier?
In the case of concept-based approaches, how do people acquire the knowledge that features of ZEBRA are likely to influence the interpretation of ZEBRA FISH?
This paper presents an experiment which examines how both distributional information about relations and intrinsic information about concept features influence compound interpretation.
We also address the question of how such information is acquired.
Rather than use existing, real world concepts, our experiment used laboratory generated conceptsthatparticipantswererequiredtolearnduring the experiment.
As well as learning the meaning of these concepts, participants also built up knowledge during the experiment about how these concepts tend to combine with other concepts via relational links.
Using laboratory-controlled concepts allows us to measure and control various factors that might be expected to influence compound comprehension; for example, concepts can be designed to vary in their degree of similarity to one another, to be associated with potential relations with a certain degree of frequency, or to have a feature which is associated with a particular relation.
It would be extremely difficult to control for such factors, or investigate the aquisition process, using natural, real world concepts.
2 Experiment
Our experiment follows a category learning paradigm popular in the classification literature (Medin & Shaffer, 1978; Nosofsky, 1984).
The experiment consists of two phases, a training phase followed by a transfer phase.
In the training phase, participants learned to identify several laboratory generated categories by examining instances of these categories that were presented to them.
These categories were of two types, conceptual and relational.
The conceptual categories consisted of four ?plant?? categories and four ?beetle??categories, which participants learned to distinguish by attending to differences between category instances.
The relational categories were three different ways in which a beetle could eat a plant.
Each stimulus consisted of a picture of a beetle instance and a picture of a plant instance, with a relation occurring between them.
The category learning phase of our experiment therefore has three stages: one for learning to distinguish between the four beetle categories, one for learning to distinguish between the four plant categories, and one for learning to distinguish between the three relation categories.
The training phase was followed by a transfer phase consisting of two parts.
In the first part participants were presented with some of the beetleplant pairs that they had encountered in the training phase together with some similar, though previously unseen, pairs.
Participants were asked to rate how likely each of the three relations were for the depicted beetle-plant pair.
This part of the transfer phase therefore served as a test of how well participants had learned to identify the appropriate relation (or relations) for pairs of conceptual category exemplars and also tested their ability to generalize their knowledge about the learned categories to previously unseen exemplar pairs.
In the second part of the transfer phase, participants were presented with 90 pairs of category names (rather than pairs of category items), presented as noun-noun compounds, and were asked to rate the appropriateness of each relation for each compound.
In the experiment, we aim to investigate three issues that may be important in determining the most appropriate interpretation for a compound.
Firstly, the experiment aims to investigate the influence of concept salience (i.e.
how important to participants information about the two constituent concepts are, or how relevant to finding a relation that information is)ontheinterpretationofcompounds.
Forexample, if the two concepts referenced in a compound are identical with respect to the complexity of their representation, how well they are associated with various alternative relations (and so on), but are of differing levels of animacy, we might expect the relation associated with the more animate concept to be selected by participants more often than a different relation associated equally strongly with the less animate concept.
In our experiment, all three relations involveabeetleeatingaplant.
Sinceineachcasethe beetleis theagent inthe EATS(BEETLE,PLANT) scenario, it is possible that the semantics of the beetle conceptsmightbemorerelevanttorelationselection than the semantics of the plant concepts.
Secondly, the experiment is designed to investigate the effect of the ordering of the two nouns within the compound: given two categories named A and B, our experiment investigates whether the compound ?A B??is interpreted in the same way as the compound ?B A??
In particular, we were interested in whether the relation selected for a compound would tend to be dependent on the concept in the head position or the concept in the modifier position.
Also of interest was whether the location of the more animate concept in the compound would have an effect on interpretation.
For example, since the combinedconceptisaninstanceoftheheadconcept, we might hypothesize that compounds for which the head concept is more animate than the modifier concept may be easier to interpret correctly.
Finally, were interested in the effect of concept similarity: would compounds consisting of similar constituent categories tend to be interpreted in similar ways? learntrans.
Nr Rel BcatPcat B1 B2 B3 P1 P2 P3 l 1 1 1 3 4 1 1 3 2 3l 2 1 1 3 4 4 1 2 3 3 l t 3 1 1 3 1 1 1 3 3 2l t 4 1 1 3 4 1 2 3 3 3 l t 5 2 2 2 2 2 2 2 2 3l 6 2 2 2 2 2 1 2 3 2 l 7 2 2 2 2 3 2 2 2 1l t 8 2 2 2 2 2 3 2 2 2 l t 9 3 3 1 3 3 3 4 1 2l t 10 3 3 1 3 3 2 1 1 1 l 11 3 3 1 2 3 3 4 4 1l 12 3 3 1 3 2 3 4 1 1 l t 13 1 4 4 1 1 4 4 4 4l t 14 2 4 4 4 1 4 4 1 4 l t 15 3 4 4 4 4 4 1 1 4 t 16 1 1 4 1 1 4 1 1t 17 3 3 3 3 3 3 3 3 t 18 2 4 2 2 2 4 1 4t 19 4 2 4 1 4 2 2 2 Table 1: The experiment?s abstract category structure 2.1 Method 2.1.1 Participants The participants were 42 university students.
2.1.2 Materials
The abstract category structure used in the experiment is presented in Table 1.
There are 19 items in total; the first and second columns in the table indicate if the item in question was one of the 15 items used in the learning phase of the experiment (l) or as one of the 13 items used in the transfer stage of the experiment (t).
There were four beetle categories (Bcat), four plant categories (Pcat) and three relation categories used in the experiment.
Both the beetle and plant categories were represented by features instantiated on three dimensions (B1, B2 & B3 and P1, P2 & P3, respectively).
The beetle and plant categories were identical with respect to their abstract structure (so, for example, the four exemplars of Pcat1 have the same abstract features as the four exemplars of Bcat1).
Beetles and plants were associated with particular relations; Bcat1, Bcat2 and Bcat3 were associated with Relations 1, 2 and 3, respectively, whereas Pcat1, Pcat2 and Pcat3 were associated with Relations 3, 2 and 1, respectively.
Bcat4 and Pcat4 were not associated with any relations; the three exemplar 91 instances of these categories in the learning phase appeared once with each of the three relations.
The features of beetles and plants were sometimes diagnostic of a category (much as the feature has three wheels is diagnostic for TRICYCLE); for example, a particular feature associated with Bcat1 is a 1 on the B3dimension: 3ofthe4Bcat1trainingphaseexemplarshavea1ondimensionB3whileonlyoneofthe remaining 11 training phase exemplars do.
Also, the intrinsicfeaturesofbeetlesandplantsaresometimes diagnostic of a relation category (much as the intrinsic feature has a flat surface raised off the ground is diagnostic for the relational scenario sit on); values on dimensions B1, P1, B2 and P2 are quite diagnostic of relations.
Participants learned to identify the plant, beetle and relation categories used in the experiment by attending to the associations between beetle, plant and relation categories and feature diagnosticity for those categories.
The beetle and plant categories were also designed to differ in terms of their similarity.
For example, categories Bcat1 and Bcat4 are more similar to each other than Bcat3 and Bcat4 are: the features for Bcat1 and Bcat4 overlap to a greater extent than the features for Bcat3 and Bcat4 do.
The aim of varying categories with respect to their similarity was to investigate whether similar categories would yield similar patterns of relation likelihood ratings.
In particular, Bcat4 (and Pcat4) occurs equally often with the three relations; therefore if category similarity has no effect we would expect people to select each of the relations equally often for this category.
However, if similarity influences participants??relation selection, then we would expect that Relation 1 would be selected more often than Relations 2 or 3.
The abstract category structure was mapped to concrete features in a way that was unique for each participant.
Each beetle dimension was mapped randomly to the concrete dimensions of beetle shell color, shell pattern and facial expression.
Each plant dimension was randomly mapped to the concrete dimensions of leaf color, leaf shape, and stem color.
The three relations were randomly mapped to eats from leaf, eats from top, and eats from trunk.
2.1.3 Procedure
The experiment consisted of a training phase and a transfer phase.
The training phase itself consisted Figure 1: Example of a relation learning stimulus of three sub-stages in which participants learned to distinguish between the plant, beetle and relation categories.
During each training sub-stage, the 15 trainingitemswerepresentedtoparticipantssequentially on a web-page in a random order.
Underneath each item, participants were presented with a question of the form ?What kind of plant is seen in this picture???
?What type of beetle is seen in this picture???and ?How does this ?Bcat??eat this ?Pcat???? in the plant learning, beetle learning, and relation learning training sub-stages, respectively (e.g.
Figure 1).
Underneath the question were radio buttons on which participants could select what they believed to be the correct category; after participants had made their selection, they were given feedback about whether their guess had been correct (with the correct eating relation shown taking place).
Each of the three substages was repeated until participants had correctly classified 75% or more of the items.
Once they had successfully completed the training phase they moved on to the transfer phase.
The transfer phase consisted of two stages, an exemplar transfer stage and a compound transfer stage.
In the exemplar transfer stage, participants were presented with 13 beetle-plant items, some of which had appeared in training and some of which were new items (see Table 1).
Underneath each picture was a question of the form ?How does this ?Bcat??eat this ?Pcat????and three 5-point scales for the three relations, ranging from 0 (unlikely) to 4 (likely).
Thematerialsusedinthecompoundtransferstage of the experiment were the 16 possible noun-noun 92 compounds consisting of a beetle and plant category label.
Participants were presented with a sentence of the form ?There are a lot of ?Pcat???Bcat?s around at the moment.??and were asked ?What kind of eating activity would you expect a ?Pcat???Bcat??to have???
Underneath, participants rated the likelihood of each of the three relations on 5-point scales.
One half of participants were presented with the compounds in the form ?Bcat???Pcat??whereas the other half of participants saw the compounds in the form ?Pcat???Bcat?? 2.2 Results 2.2.1 Performance during training Two of the participants failed to complete the training phase.
For the remaining 40 participants, successful learning took on average 5.8 iterations of the training items for the plant categories, 3.9 iterations for the beetle categories, and 2.1 iterations for the relation categories.
The participants therefore learned to distinguish between the categories quite quickly,whichisconsistentwiththefactthatthecategories were designed to be quite easy to learn.
2.2.2 Performance
during the exemplar transfer stage Participants??mean ratings of relation likelihood for the nine previously seen exemplar items is presented in Figure 2 (items 3 to 15).
For each of these items there was a correct relation, namely the one that the item was associated with during training.
The difference between the mean response for the correct relation (M = 2.76) and the mean response for the two incorrect relations (M = 1.42) was significant (ts(39) = 7.50, p <.01; ti(8) = 4.07, p < .01).
These results suggest that participants wereabletolearnwhichrelationstendedtoco-occur with the items in the training phase.
Participants??mean ratings of relation likelihood for the four exemplar items not previously seen in training are also presented in Figure 2 (items 16 to 19).
Each of these four items consisted of a prototypical example of each of the four beetle categories and each of the four plant categories (with each beetle and plant category appearing once; see Table 1 for details).
For these four items there was no correct answer; indeed, the relation consistent with the beetle exemplar was always different to the relation Figure 2: Participants??mean responses for the exemplar transfer items.
suggestedbytheplantexemplar. Foreachtrial,then, one relation is consistent with the beetle exemplar (rb), one is consistent with the plant exemplar (rp) and one is neutral (rn).
One-way repeated measures ANOVAs with response type (rb, rp or rn) as a fixed factor and either subject or item as a random factor wereusedtoinvestigatethedata.
Therewasasignificant effect of response type in both the by-subjects and by-items analysis (Fs(2,39) = 19.10, p < .01; Fi(2,3) = 24.14,p < .01).
Pairwise differences between the three response types were investigated using planned comparisons in both the by-subject and by-items analyses (with paired t-tests used in both cases).
The difference between participants??mean response for the relation associated with the beetle exemplar, rb (M = 2.68), and their mean response for the neutral relation, rn (M = 1.44) was significant (ts(39) = 5.63, p < .001; ti(3) = 5.34, p = .01).
These results suggest that participants were strongly influenced by the beetle exemplar when making their category judgments.
However, the difference between participants??mean response for the relation associated with the plant exemplar, rp (M = 1.62), and their mean response for the neutral relation was not significant (ts(39) = 1.11, p = .27; ti(3) = 0.97, p = .40).
These results suggest that participants were not influenced by the plant exemplar when judging relation likelihood.
Since the beetle and plant categories have identical abstract structure, these results suggest that otherfactors(suchastheanimacyofaconceptorthe roleitplaysintherelation)areimportanttointerpretation.
The data from all 13 items were also analysed taken together.
To investigate possible effects of cat93 egory similarity, a repeated measures ANOVA with beetlecategoryandresponserelationtakenaswithin subject factors and subject taken as a random factor was undertaken.
There was a significant effect of the category that the beetle exemplar belonged to on participants??responses for the three relations (the interaction between beetle category and response relation was significant; F(6,39) = 26.83, p < .01.
Planned pairwise comparisons (paired t-tests) were conducted to investigate how ratings for the correct relation (i.e.
the relation consistent with training) differed for the ratings for the other two relations.
For Bcat1, Bcat2 and Bcat3, the ratings for the relation consistent with learning was higher than the two alternative relations (p < .01 in all cases).
However, for the Bcat4 items, there was no evidence that participants we more likely to rate Relation 1 (M = 2.09) higher than either Relation 2 (M = 1.97; t(39) = 0.54, p = .59) or Relation 3 (M = 1.91; t(39) = 0.69, p > .50).
Though thedifferenceisinthedirectionpredictedbyBcat4?s similarity to Bcat1, there is no evidence that participants made use of Bcat4?s similarity to Bcat1 when rating relation likelihood for Bcat4.
In summary, the results suggest that participants were capable of learning the training items.
Participants appeared to be influenced by the beetle exemplar but not the plant exemplar.
There was some evidence that conceptual similarity played a role in participants??judgments of relation likelihood for Bcat4 exemplars (e.g.
the responses for item 19) but over all Bcat4 exemplars this effect was not significant.
2.2.3 Performance
on the noun-noun compound transfer stage In the noun-noun compound transfer stage, each participant rated relation likelihood for each of the 16 possible noun-noun compounds that could be formed from combinations of the beetle and plant category names.
Category name order was a between subject factor: half of the participants saw the compounds with beetle in the modifier position and plant in the head position whilst the other half of participants saw the reverse.
First of all, we were interested in whether or not the training on exemplar items would transfer to noun-noun compounds.
Another question of interest is whether or not participants??responses would be affected by the order in which the categories were presented.
For example, perhaps it is the concept in the modifier position that is most influential in determining the likelihood of different relations for a compound.
Alternatively perhaps it is the concept in the head position that is most influential.
To answer such questions a 4432 repeated measures ANOVA with beetle category, plant category and response relation as within subject factors andcategorylabelorderingasabetweensubjectfactor was used to analyze the data.
The interaction between beetle category and response relation was significant (F(6,38) = 59.79, p < .001).
Therefore, the beetle category present in the compound tended to influence participants??relation selections.
Theinteractionbetween plantcategoryandresponse relation was weaker, but still significant (F(6,38) = 5.35, p < 0.01).
Therefore, the plant category present in the compound tended to influence participants??relation selections.
These results answer the first question above; training on exemplar items was transferred to the noun-noun compounds.
However, therewerenoothersignificantinteractionsfound.
In particular, the interaction between category ordering, beetle category and response relation was not significant (F(6,38) = 1.82, p = .09).
In other words, there is no evidence that the influence of beetlecategoryonparticipants?relationselectionswhen the beetle was in the modifier position differed from theinfluenceofbeetlecategoryonparticipants?relation selections when the beetle was in the head-noun position.
Similarly, the interaction between noun ordering, plant category and response relation was not significant (F(6,38) = 0.68, p = .67); there is no evidence that the influence of the plant category on relation selection differed depending on the location of the plant category in the compound.
Planned pairwise comparisons (paired t-tests) were used to investigate the significant interactions further: for Bcat1, Bcat2 and Bcat3, the ratings for the relation consistent with learning was significantly higher than the two alternative relations (p < .001 in all cases).
However, for Bcat4, there were no significant differences between the ratings for the three relations (p > .31 for each of the three comparisons).
For the plants, however, the only significant differences were between the response for Relation 1 and Relation 2 for Pcat2 (t(39) = 2.12, 94 p = .041) and between Relation 2 and Relation 3 for Pcat2 (t(39) = 3.08, p = .004), although the differences for Pcat1 and Pcat3 are also in the expected direction.
In summary, the results of the noun-noun compound stage of the experiment show that participants??learning of the relations and their associations with beetle and plant categories during training transferred to a task involving noun-noun compound interpretation.
This is important as it demonstrates how the interpretation of compounds can be derived from information about how concept exemplars tend to co-occur together.
2.3 Modelling
relation selection One possible hypothesis about how people decide on likely relations for a compound is that the mention of the two lexemes in the compound activates stored memory traces (i.e.
exemplars) of the concepts denoted by those lexemes.
Exemplars differ in how typical they are for particular conceptual categories and we would expect the likelihood of an exemplar?s activation to be in proportion to its typicality for the categories named in the compound.
As concept instances usually do not happen in isolation but rather in the context of other concepts, this naturally results in extensional relational information about activated exemplars also becoming activated.
This activated relational information is then available to form a basis for determining the likely relation or relations for the compound.
A strength of this hypothesis is that it incorporates both intensional information about concepts??features (in the form of concept typicality) and also extrinsic, distributional information about how concepts tend to combine (in the form of relational information associated with activated exemplars).
In this section, we present a model instantiating this hybrid approach.
The hypothesis proposed above assumes that extensional information about relations is associated with exemplars in memory.
In the context of our experiment, the extensional, relational information about beetle and plant exemplars participants held in memoryisrevealedinhowtheyratedrelationallikelihood during the exemplar transfer stage of the ex1This is not significant if Bonferroni correction is used to control the familywise Type I error rate amongst the multiple comparisons periment.
For each of the 13 beetle and plant exemplars, we therefore assume that the average ratings for each of the relations describes our participants?? knowledgeabouthowexemplarscombinewithother exemplars.
Also, we can regard the three relation likelihood ratings as being a 3-dimensional vector.
Given that category ordering did not appear to have an effect on participants??responses in the compound transfer phase of the experiment, we can calculate therelationvectorvectorrB,P forthenovelcompounds?B P??or ?P B??as vectorrB,P = summationdisplay e?U (typ(eb,B) + typ(ep,P)) vectorre summationdisplay e?U (typ(eb,B) + typ(ep,P)) where e denotes one of the 13 beetle-plant exemplar items rated in the exemplar transfer stage, typ(eb,B) denotes the typicality of the beetle exemplar present in item e in beetle category B and typ(ep,P) denotes the typicality of the plant exemplar present in item e in plant category P.
U is the set of 13 beetle-plant exemplar pairs and  is a magnification parameter to be estimated empirically which describes the relative importance of exemplar typicality.
Inthismodel,werequireameasureofhowtypical of a conceptual category an exemplar is (i.e.
a measure of how good a member of a category a particular category instance is).
In our model, we use the Generalized Context Model (GCM) to derive measures of exemplar typicality.
The GCM is a successfulmodelofcategorylearningthatimplementsanan exemplar-based account of how people make judgments of category membership in a category learning task.
The GCM computes the probability Pr of an exemplar e belonging in a category C as a function of pairwise exemplar similarity according to: Pr(e,C) = summationdisplay i?C sim(e,i) summationdisplay i?U sim(e,i) where U denotes the set of all exemplars in memory and sim(e,i) is a measure of similarity between exemplars e and i.
Similarity between exemplars is inturndefinedasanegative-exponentialtransforma95 tion of distance: sim(i,j) = e?cdist(i,j) (1) where c is a free parameter, corresponding to how quicklysimilaritybetweentheexemplarsdiminishes as a function of their distance.
The distance between two exemplars is usually computed as the city-block metric summed over the dimensions of the exemplars, with each term weighted by empirically estimated weighting parameters constrained to sum to one.
According to the GCM, the probability that a given exemplar belongs to a given category increases as the average similarity between the exemplar and the exemplars of the category increases; in other words, as it becomes a more typical member of the category.
In our model, we use the probability scores produced by the GCM as a means for computing concept typicality (although other methods for measuring typicality could have been used).
We compared the relation vector outputted by the model for the 16 possible compounds to the relation vectors derived from participants??ratings in the compound transfer phase of the experiment.
The agreement between the model and the data was high across the three relations (for Relation 1, r = 0.84, p < 0.01; for Relation 2, r = 0.90, p < 0.01; for Relation 3, r = 0.87, p < 0.01), using only one free parameter, , to fit the data2.
3 Conclusions
The empirical findings we have described in this paper have several important implications.
Firstly, the findings have implications for relation-based theories.
In particular, the finding that only beetle exemplars tended to influence relation selection suggest that factors other than relation frequency are relevant to the interpretation process (since the beetle and plants in our experiment were identical in their degreeofassociationwithrelations).
Complexinteractions between concepts and relations (e.g.
agency in the EATS(AGENT,OBJECT) relation) is information that is not possible to capture using a taxonomic approach to relation meaning.
Secondly, the fact that participants could learn to identify the relations between exemplars and also 2In the GCM, c was set equal to 1 and the three dimensional weights in the distance calculation were set equal to 1/3 transfer that knowledge to a task involving compounds has implications for concept-based theories of compound comprehension.
No concept-based theory of conceptual combination has ever adopted an exemplar approach to concept meaning; models based on concept-focused theories tend to represent concepts as frames or lists of predicates.
Our approach suggests an exemplar representation is a viable alternative.
Also, distributional knowledge about relations forms a natural component of an exemplar representation of concepts, as different conceptinstanceswilloccurwithinstancesofotherconcepts with varying degrees of frequency.
Given the success of our model, assuming an exemplar representation of concept semantics would seen to offer a natural way of incorporating both information about concept features and information about relation distribution into a single theory.
References G.
Cannon. 1987.
Historical change and English word formation.
New York: Lang.
E. V.
Clark and B.J.
Barron. 1988.
A thrower-button or a button-thrower?
Children?s judgments of grammatical and ungrammatical compound nouns.
Linguistics, 26:3??9.
F. J.
Costello & M.T.
Keane. 2000.
Efficient creativity: Constraint guided conceptual combination..
Cognitive Science, 24(2):299??49.
C. L.
Gagne and E.J.
Shoben. 1997.
Influence of thematic relations on the comprehension of modifier noun combinations.
Journal of Experimental Psychology: Learning, Memory, and Cognition, 23:71??8.
S. N.
Kim and T.
Baldwin. 2005.
Automatic Interpretation of Noun Compounds Using WordNet Similarity.
Lecture Notes in Computer Science, 3651:945??56.
J. N.
Levi. 1978.
The Syntax and Semantics of Complex Nominals.
New York: Academic Press.
D. L.
Medin & M.M.
Schaffer. 1978.
Context theory of classification learning.
Psychological Review, 85:207??38.
R. N.
Nosofsky. 1984.
Choice, similarity, and the context theory of classification..
Journal of Experimental Psychology: Learning, Memory, and Cognition, 10(1):104??14.
E. J.
Wisniewski 1997.
When concepts combine.
Psychonomic Bulletin & Review, 4(2):167??83 .
Proceedings of the Second Workshop on Statistical Machine Translation, pages 1??, Prague, June 2007.
c2007 Association for Computational Linguistics Using Dependency Order Templates to Improve Generality in Translation Arul Menezes and Chris Quirk Microsoft Research One Microsoft Way, Redmond, WA 98052, USA {arulm, chrisq}@microsoft.com Abstract Today's statistical machine translation systems generalize poorly to new domains.
Even small shifts can cause precipitous drops in translation quality.
Phrasal systems rely heavily, for both reordering and contextual translation, on long phrases that simply fail to match outof-domain text.
Hierarchical systems attempt to generalize these phrases but their learned rules are subject to severe constraints.
Syntactic systems can learn lexicalized and unlexicalized rules, but the joint modeling of lexical choice and reordering can narrow the applicability of learned rules.
The treelet approach models reordering separately from lexical choice, using a discriminatively trained order model, which allows treelets to apply broadly, and has shown better generalization to new domains, but suffers a factorially large search space.
We introduce a new reordering model based on dependency order templates, and show that it outperforms both phrasal and treelet systems on in-domain and out-of-domain text, while limiting the search space.
1 Introduction
Modern phrasal SMT systems such as (Koehn et al., 2003) derive much of their power from being able to memorize and use long phrases.
Phrases allow for non-compositional translation, local reordering and contextual lexical choice.
However the phrases are fully lexicalized, which means they generalize poorly to even slightly outof-domain text.
In an open competition (Koehn & Monz, 2006) systems trained on parliamentary proceedings were tested on text from 'news commentary' web sites, a very slightly different domain.
The 9 phrasal systems in the English to Spanish track suffered an absolute drop in BLEU score of between 4.4% and 6.34% (14% to 27% relative).
The treelet system of Menezes et al.(2006) fared somewhat better but still suffered an absolute drop of 3.61%.
Clearly there is a need for approaches with greater powers of generalization.
There are multiple facets to this issue, including handling of unknown words, new senses of known words etc.
In this work, we will focus on the issue of reordering, i.e. can we learn how to transform the sentence structure of one language into the sentence structure of another, in a way that is not tied to a specific domain or sub-domains, or indeed, sequences of individual words.
An early attempt at greater generality in a purely phrasal setting was the alignment template approach (Och & Ney 2004); newer approaches include formally syntactic (Chiang 2005), and linguistically syntactic approaches (Quirk et al.2005), (Huang et al.2006). In the next section, we examine these representative approaches to the reordering problem.
2 Related
Work Our discussion of related work will be grounded in the following tiny English to Spanish example, where the training set includes: a very old book un libro ms antiguo a book very old 1 the old man el hombre viejo the man old it is very important es muy importante is very important 1 English gloss of Spanish sentences in italics.
1 and the test sentence and reference translation are a very old man un hombre muy viejo a man very old Note that while the first training pair has the correct structure for the test sentence, most of the contextually correct lexical choices come from the other two pairs.
2.1 Phrasal
translation, Alignment templates The relevant phrases (i.e.
those that match the test sentence) extracted from these training pairs are shown in Table 2.1.
Only phrases up to size 3 are shown.
The ones in italics are 'correct' in that they can lead to the reference translation.
Note that none of the multi-word phrases lead to the reference, so the local reordering often captured in the phrasal model is no help at all in ordering this sentence.
The system is unable to learn the correct structure from the first sentence because the words are wrong, and from the second sentence even though the phrase old man has the right words in the right order, it does not lead to the reference translation because the translation of very cannot be inserted in the right place.
a un very ms old antiguo very old ms antiguo old viejo man hombre old man hombre viejo very muy Table 2.1: Relevant extracted phrases Looking at this as a sparse data issue we might suspect that generalization could solve the problem.
The alignment template approach (Och & Ney, 2004) uses word classes rather than lexical items to model phrase translation.
Yet this approach loses the advantage of context-sensitive lexical selection: the word translation model depends only on the word classes to subcategorize for translations, which leads to less accurate lexical choice in practice (Zens & Ney, 2004).
2.2 Hierarchical
translation Hierarchical systems (Chiang, 2005) induce a context-free grammar with one non-terminal directly from the parallel corpus, with the advantage of not requiring any additional knowledge source or tools, such as a treebank or a parser.
However this can lead to an explosion of rules.
In order to make the problem tractable and avoid spurious ambiguity, Chiang restricts the learned rules in several ways.
The most problematic of these is that every rule must have at least one pair of aligned words, and that adjacent non-terminals are not permitted on the source side.
In Table 2.2 we show the additional hierarchical phrases that would be learned from our training pairs under these restrictions.
Again only those applicable to the test sentence are shown and the 'correct' rules, i.e. those that lead to the reference, are italicized.
X1 old X1 antiguo very X1 ms X1 very old X1 X1 ms antiguo X1 old X2 X2 X1 antiguo very X1 X2 X2 ms X1 X1 man hombre X1 old X1 X1 viejo X1 old man X1 hombre viejo X1 very X1 muy very X2 muy X2 X1 very X2 X1 muy X2 Table 2.2: Additional hierarchical phrases Note that even though from the first pair, we learn several rules with the perfect reordering for the test sentence, they do not lead to the reference because they drag along the contextually incorrect lexical choices.
From the second pair, we learn a rule (X1 old man) that has the right contextual word choice, but does not lead to the reference, because the paucity of the grammar's single nonterminal causes this rule to incorrectly imply that the translation of very be placed before hombre.
2.3 Constituency
tree transduction An alternate approach is to use linguistic information from a parser.
Transduction rules between Spanish strings and English trees can be learned from a word-aligned parallel corpus with parse trees on one side (Graehl & Knight, 2004).
Such rules can be used to translate from Spanish to English by searching for the best English language tree for a given Spanish language string (Marcu et al., 2006).
Alternately English trees produced by a parser can be transduced to 2 Spanish strings using the same rules (Huang et al., 2006).
Translation rules may reach beyond one level in the syntax tree; this extended domain of locality allows many phenomena including both lexicalized and unlexicalized rules.
However reordering and translation are modeled jointly, which may exacerbate data sparsity.
Furthermore it forces the system to pick between unlexicalized rules that capture reordering and lexicalized rules that model context-sensitive translation.
For instance, the following rules can be extracted from the first sentence of the corpus: r 1 : un x1 x2 null NP(DT(a) ADJP:x2 NN:x1) r 2 : x1 x2 null ADJP(RB:x1 JJ:x2) Although together they capture the necessary reordering for our test sentence pair, they do not allow for context sensitive translations of the ambiguous terms very and old; each must be selected independently.
Disappointingly, no single constituency tree transduction rule derived from this corpus translates old man as hombre viejo in a single step on the test sentence: the syntactic structures are slightly different, but the difference is sufficient to prevent matching.
2 Again
we note that phrases provide utility by capturing both reordering and context.
While xRS 2 Marcu et al.(2006) and Zollmann et al.(2006) recognize this problem and attempt to alleviate it by grafting surface phrases into constituency trees by various methods.
rules provide an elegant and powerful model of reordering, they come with a potential cost in context-sensitive translation.
2.4 Dependency
treelet translation We previously described (Quirk et al, 2005) a linguistically syntax-based system that parses the source language, uses word-based alignments to project a target dependency tree, and extracts paired dependency tree fragments (treelets) instead of surface phrases.
In contrast to the xRS approach, ordering is very loosely coupled with translation via a separate discriminatively trained dependency tree-based order model.
The switch to a dependency parse also changes the conditioning information available for translation: related lexical items are generally adjacent, rather than separated by a path of unlexicalized nonterminals.
In effect, by using a looser matching requirement, treelets retain the context-sensitive lexical choice of phrases: treelets must only be a connected subgraph of the input sentence to be applicable; some children may remain uncovered.
Figure 2.2 shows source dependency parses and projected target dependencies for our training data; Figure 2.3 shows the treelet pairs that this system would extract that match the input a very old book DT RB JJ NN ADJP NP un libro ms antiguo the old man DT JJ NN NP el hombre viejo it is very important PN VB RB JJ ADJP VP S es muy importante Figure 2.1: Constituency parses Figure 2.2: Dependency trees for training pairs Figure 2.3: Relevant extracted treelets 3 sentence (treelets of size 1 are not shown).
The second treelet supplies the order of viejo with respect to its head, and unlike the case with xRS rules, we can use this to make the correct contextual word choice.
The difference is that because xRS rules provide both reordering and word choice, each rule must match all of the children at any given tree node.
On the other hand, treelets are allowed to match more loosely.
The translations of the unmatched children (un and muy in this case) are placed by exploring all possible orderings and scoring them with both order model and language model.
Although this effectively decouples lexical selection from ordering, it comes at a huge cost in search space and translation quality may suffer due to search error.
However, as mentioned in Section 1, this approach is able to generalize better to out-ofdomain data than phrasal approaches.
Koehn and Monz (2006) also include a human evaluation, in which this system ranked noticeably higher than one might have predicted from its BLEU score.
3 Dependency
Order Templates The Dependency Order Templates approach leverages the power of the xR rule formalism, while avoiding the problems mentioned in Section 2.3, by constructing the rules on the fly from two separately matched components: (a) Dependency treelet translation pairs described in Section 2.4 that capture contextual lexical translations but are underspecified with respect to ordering, and (b) Order templates, which are unlexicalized rules (over dependency, rather than constituency trees) that capture reordering phenomena.
Formally, an order template is an unlexicalized transduction rule mapping dependency trees containing only parts of speech to unlexicalized target language trees (see Figure 4.1b).
Given an input sentence, we combine relevant treelet translation pairs and order templates to construct lexicalized transduction rules for that sentence, and then decode using standard transduction approaches.
By keeping lexical and ordering information orthogonal until runtime, we can produce novel transduction rules not seen in the training corpus.
This allows greater generalization capabilities than the constituency tree transduction approaches of Section 2.3.
As compared to the treelet approach described in Section 2.4, the generalization capability is somewhat reduced.
In the treelet system all reorderings are exhaustively evaluated, but the size of the search space necessitates tight pruning, leading to significant search error.
By contrast, in the order template approach we consider only reorderings that are captured in some order template.
The drastic reduction in search space leads to an overall improvement, not only in decoding speed, but also in translation quality due to reduced search error.
3.1 Extracting
order templates For each pair of parallel training sentences, we parse the source sentence, obtain a source dependency tree, and use GIZA++ word alignments to project a target dependency tree as described in Quirk et al.(2005). Given this pair of aligned source and target dependency trees, we recursively extract one order template for each pair of aligned non-leaf source and target nodes.
In the case of multi-word alignments, all contiguous 3 aligned nodes are added to the template.
Next we recursively add child nodes as follows: For each node in the template, add all its children.
For each such child, if it is aligned, stop recursing, if it is unaligned, recursively add its children.
On each template node we remove the lexical items; we retain the part of speech on the source nodes (we do not use target linguistic features).
We also keep node alignment information 4. The resulting aligned source and target sub-graphs comprise the order template.
Figure 4.1b lists the order templates extracted from the training pairs in Figure 2.1 that capture all the patterns necessary to correctly reorder the test sentence.
4 Decoding
Decoding is treated as a problem of syntaxdirected transduction.
Input sentences are segmented into a token stream, annotated with part-of-speech information, and parsed into 3 If a multi-word alignment is not contiguous in either source or target dependency tree no order template is extracted.
4 If
a source or target node aligns to a tree node outside the template, the template breaks phrasal cohesion and is currently discarded.
We intend to address these 'structural divergence' patterns in future work.
4 unlabeled dependency trees.
At each node in the input dependency tree we first find the set of matching treelet pairs: A pair matches if its source side corresponds to a connected subgraph of the input tree.
Next we find matching order templates: order templates must also match a connected subgraph of the input tree, but in addition, for each input node, the template must match either all or none of its children 5 . Compatible combinations of treelets and order templates are merged to form xR rules.
Finally, we search for the best transduction according to the constructed xR rules as scored by a log-linear combination of models (see Section 5).
4.1 Compatibility
A treelet and an order template are considered compatible if the following conditions are met: The treelet and the matching portions of the template must be structurally isomorphic.
Every treelet node must match an order template node.
Matching nodes must have the same part of speech.
Unaligned treelet nodes must match an unaligned template node.
Aligned treelet nodes must match aligned template nodes.
Nodes that are aligned to each other in the treelet pair must match template nodes that are aligned to each other.
4.2 Creating
transduction rules Given a treelet, we can form a set of tree transduction rules as follows.
We iterate over each source node n in the treelet pair; let s be the corresponding node in the input tree (identified during the matching).
If, for all children of s there is a corresponding child of n, then this treelet specifies the placement of all children and no changes are necessary.
Otherwise we pick a template that matched at s and is compatible with the treelet.
The treelet and template are unified to produce an updated rule with variables on the source and target sides for each uncovered child of s.
When all treelet nodes have been visited, we are left with a transduction rule that specifies the translation of all nodes in the treelet and contains variables that specify the placement of all 5 This is so the resulting rules fit within the xR formalism.
At each node, a rule either fully specifies its ordering, or delegates the translation of the subtree to other rules.
uncovered nodes.
Due to the independence of ordering and lexical information, we may produce novel transduction rules not seen in the training corpus.
Figure 4.1 shows this process as it applies to the test sentence in Section 2.
If, at any node s, we cannot find a matching template compatible with the current treelet, we create an artificial source order template, which simply preserves the source language order in the target translation.
We add a feature function that counts the number of such templates and train its weight during minimum error rate training.
4.3 Transduction
using xR rules In the absence of a language model or other contextually dependent features, finding the highest scoring derivation would be a simple dynamic program (Huang et al.2006) 6 .However exact search using an null -gram language model leads to split states for each null -gram context.
Instead we use an approximate beam search moving bottom-up in the tree, much like a CKY parser.
Candidates in this search are derivations with respect to the transducer.
Each transduction rule null has a vector of variables null nullnull,?null nullnull . Each variable is associated with an input node nullnullnullnull . For each input node null, we keep a beam of derivations nullnullnullnull . Derivations are represented as a pair nullnull,nullnull where null is a transduction rule and nullnullnull null is a vector with one integer for each of the null variables in null . The interpretation is that the complete candidate can be constructed by recursively substituting for each 6 Like Chiang (2005) we only search for the yield of the most likely derivation, rather than the most likely yield.
Figure 4.1: Merging templates and treelets 5 null nullnull null null nullnull ?null nullnull the candidate constructed from the null null th entry in the beam null null null null null nullnull null null . Figure 4.2 describes the transduction process.
Since we approach decoding as xR transduction, the process is identical to that of constituencybased algorithms (e.g.
Huang and Chiang, 2007).
There are several free parameters to tune: ??Beam size ??Maximum number of candidates per input node (in this paper we use 100) ??Beam threshold ??maximum range of scores between top and bottom scoring candidate (we use a logprob difference of 30) ??Maximum combinations considered ??To bound search time, we can stop after a specified number of elements are popped off the priority queue (we use 5000) 5 Models We use all of the Treelet models we described in Quirk et al.(2005) namely: ??Treelet table with translation probabilities estimated using maximum likelihood, with absolute discounting.
??Discriminative tree-based order model.
??Forward and backward lexical weighting, using Model-1 translation probabilities.
??Trigram language model using modified Kneser-Ney smoothing.
??Word and phrase count feature functions.
In addition, we introduce the following: ??Order template table, with template probabilities estimated using maximum likelihood, with absolute discounting.
??A feature function that counts the number of artificial source order templates (see below) used in a candidate.
The models are combined in a log-linear framework, with weights trained using minimum error rate training to optimize the BLEU score.
6 Experiments
We evaluated the translation quality of the system using the BLEU metric (Papineni et al., 2002).
We compared our system to Pharaoh, a leading phrasal SMT decoder (Koehn et al., 2003), and our treelet system.
We report numbers for English to Spanish.
6.1 Data
We used the Europarl corpus provided by the NAACL 2006 Statistical Machine Translation workshop.
The target language model was trained using only the target side of the parallel corpus.
The larger monolingual corpus was not utilized.
The corpus consists of European Parliament proceedings, 730,740 parallel sentence pairs of English-Spanish, amounting to about 15M words in each language.
The test data consists of 2000 sentences each of development (dev), development-test (devtest) and test data (test) from the same domain.
There is also a separate set of 1064 test sentences (NC-test) gathered from "news commentary" web sites.
6.2 Training
We parsed the source (English) side of the corpus using NLPWIN, a broad-coverage rule-based parser able to produce syntactic analyses at varying levels of depth (Heidorn, 2002).
For the purposes of these experiments we used a dependency tree output with part-of-speech tags and unstemmed, case-normalized surface words.
For word alignment we used GIZA++, under a training regimen of five iterations of Model 1, five iterations of HMM, and five iterations of Model 4, in both directions.
The forward and backward alignments were symmetrized using a tree-based heuristic combination.
The word GetTranslationBeam(null ) // memoized prioq null null beam nullnull for nullnullnull null null null Enqueue(prioq, null null,null null, EarlyScore(null null,null null )) while Size(prioq) null0 null null,null null null PopBest(prioq) AddToBeam(beam, null null,null null, TrueScore(null null,null null )) for null in 1..
| null | Enqueue(prioq, null null,nullnullnull null null, EarlyScore(null null,nullnullnull null null )) return beam EarlyScore(nullnull,nullnull ) nullnull RuleScore(null ) for null in 1..|null| nullnull InputNode(GetVariable (null, null )) beam null GetTranslationBeam(null ) nullnullnullnull TrueScore(GetNthEntry(beam, null null )) return null Figure 4.2: Beam tree transduction 6 alignments and English dependency tree were used to project a target tree.
From the aligned tree pairs we extracted a treelet table and an order template table.
The comparison treelet system was identical except that no order template model was used.
The comparison phrasal system was constructed using the same GIZA++ alignments and the heuristic combination described in (Och & Ney, 2003).
Except for the order models (Pharaoh uses a penalty on the deviance from monotone), the same models were used.
All systems used a treelet or phrase size of 7 and a trigram language model.
Model weights were trained separately for all 3 systems using minimum error rate training to maximize BLEU (Och, 2003) on the development set (dev).
Some decoder pruning parameters were tuned on the development test (devtest).
The test and NC-test data sets were not used until final tests.
7 Results
We present the results of our system comparisons in Table 7.1 and Figure 7.1 using three different test sets: The in-domain development test data (devtest), the in-domain blind test data (test) and the out-of-domain news commentary test data (NC-test).
All differences (except phrasal vs.
template on devtest), are statistically significant at the p>=0.99 level under the bootstrap resampling test.
Note that while the systems are quite comparable on the in-domain data, on the out-ofdomain data the phrasal system's performance drops precipitously, whereas the performance of the treelet and order template systems drops much less, outperforming the phrasal system by 2.7% and 3.46% absolute BLEU.
devtest test NC-test Phrasal 0.2910 0.2935 0.2354 Treelet 0.2819 0.2981 0.2624 Template 0.2896 0.3045 0.2700 Table 7.1: System Comparisons across domains Further insight may be had by comparing the recall 7 for different n-gram orders (Table 7.2).
The phrasal system suffers a greater decline in the higher order n-grams than the treelet and template 7 n-gram precision cannot be directly compared across output from different systems due to different levels of 'brevity' systems, indicating that latter show improved generality in reordering.
1gm 2gm 3gm 4gm Test Phrasal 0.61 0.35 0.23 0.15 treelet 0.62 0.36 0.23 0.15 template 0.62 0.36 0.24 0.16 NC-test phrasal 0.58 0.30 0.17 0.10 treelet 0.60 0.33 0.20 0.12 template 0.61 0.34 0.20 0.13 Table 7.2: n-gram recall across domains 7.1 Treelet vs.
Template systems As described in Section 3.1, the order templates restrict the broad reordering space of the treelet system.
Although in theory this might exclude reorderings necessary for some translations, Table 7.3 shows that in practice, the drastic search space reduction allows the decoder to explore a wider beam and more rules, leading to reduced search error and increased translation speed.
(The topK parameter is the number of phrases explored for each span, or rules/treelets for each input node).
Devtest BLEU Sents.
per sec Pharaoh, beam=100, topK=20 0.2910 0.94 Treelet, beam=12, topK=5 0.2819 0.21 Template, beam=100, topK=20 0.2896 0.56 Table 7.3: Performance comparisons Besides the search space restriction, the other significant change in the template system is to include MLE template probabilities as an Figure 7.1: In-domain vs.
Out-of-domain BLEU 23 24 25 26 27 28 29 30 31 development in-domain out-of-domain Phrasal Treelet Order Template 7 additional feature function.
Given that the template system operates over rules where the ordering is fully specified, and that most tree transduction systems use MLE rule probabilities to model both lexical selection and reordering, one might ask if the treelet system's discriminatively trained order model is now redundant.
In Table 7.4 we see that this is not the case.
8 (Differences are significant at p>=0.99).
devtest test NC-test MLE model only 0.2769 0.2922 0.2512 Discriminative and MLE models 0.2896 0.3045 0.2700 Table 7.4: Templates and discriminative order model Finally we examine the role of frequency thresholds in gathering templates.
In Table 7.5 it may be seen that discarding singletons reduces the table size by a factor of 5 and improves translation speed with negligible degradation in quality.
devtest BLEU Number of templates Sentences per sec.
No threshold 0.2898 752,165 0.40 Threshold=1 0.2896 137,584 0.56 Table 7.5: Effect of template count cutoffs 8 Conclusions and Future Work We introduced a new model of Dependency Order Templates that provides for separation of lexical choice and reordering knowledge, thus allowing for greater generality than the phrasal and xRS approaches, while drastically limiting the search space as compared to the treelet approach.
We showed BLEU improvements over phrasal of over 1% in-domain and nearly 3.5% out-of-domain.
As compared to the treelet approach we showed an improvement of about 0.5%, but a speedup of nearly 3x, despite loosening pruning parameters.
Extraposition and long distance movement still pose a serious challenge to syntax-based machine translation systems.
Most of the today's search algorithms assume phrasal cohesion.
Even if our search algorithms could accommodate such movement, we don't have appropriate models to 8 We speculate that other systems using transducers with MLE probabilities may also benefit from additional reordering models.
account for such phenomena.
Our system already extracts extraposition templates, which are a step in the right direction, but may prove too sparse and brittle to account for the range of phenomena.
References Chiang, David.
A hierarchical phrase-based model for statistical machine translation.
ACL 2005.
Galley, Michel, Mark Hopkins, Kevin Knight, and Daniel Marcu.
What?s in a translation rule?
HLT-NAACL 2004.
Graehl, Jonathan and Kevin Knight.
Training Tree Transducers.
NAACL 2004.
Heidorn, George.
?Intelligent writing assistance??
In Dale et al.Handbook of Natural Language Processing, Marcel Dekker.
(2000) Huang, Liang, Kevin Knight, and Aravind Joshi.
Statistical Syntax-Directed Translation with Extended Domain of Locality.
AMTA 2006 Huang, Liang and David Chiang.
Faster Algorithms for Decoding with Integrated Language Models.
ACL 2007 (to appear) Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
Statistical phrase based translation.
NAACL 2003.
Koehn, Philipp and Christof Monz.
Manual and automatic evaluation of machine translation between european languages.
Workshop on Machine Translation, NAACL 2006.
Marcu, Daniel, Wei Wang, Abdessamad Echihabi, and Kevin Knight.
SPMT: Statistical Machine Translation with Syntactified Target Language Phrases.
EMNLP-2006. Menezes, Arul, Kristina Toutanova and Chris Quirk.
Microsoft Research Treelet translation system: NAACL 2006 Europarl evaluation.
Workshop on Machine Translation, NAACL 2006 Och, Franz Josef and Hermann Ney.
A systematic comparison of various statistical alignment models, Computational Linguistics, 29(1):19-51 (2003).
Och, Franz Josef.
Minimum error rate training in statistical machine translation.
ACL 2003.
Och, Franz Josef and Hermann Ney: The Alignment Template Approach to Statistical Machine Translation.
Computational Linguistics 30 (4): 417-449 (2004) Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
BLEU: a method for automatic evaluation of machine translation.
ACL 2002.
Quirk, Chris, Arul Menezes, and Colin Cherry.
Dependency Tree Translation: Syntactically informed phrasal SMT.
ACL 2005 Zens, Richard and Hermann Ney.
Improvements in phrase-based statistical machine translation.
HLT-NAACL 2004
Proceedings of the Second Workshop on Statistical Machine Translation, pages 9??6, Prague, June 2007.
c2007 Association for Computational Linguistics CCG Supertags in Factored Statistical Machine Translation Alexandra Birch Miles Osborne Philipp Koehn a.c.birch-mayne@sms.ed.ac.uk miles@inf.ed.ac.uk pkoehn@inf.ed.ac.uk School of Informatics University of Edinburgh 2 Buccleuch Place Edinburgh, EH8 9LW, UK Abstract Combinatorial Categorial Grammar (CCG) supertags present phrase-based machine translation with an opportunity to access rich syntactic information at a word level.
The challenge is incorporating this information into the translation process.
Factored translation models allow the inclusion of supertags as a factor in the source or target language.
We show that this results in an improvement in the quality of translation and that the value of syntactic supertags in flat structured phrase-based models is largely due to better local reorderings.
1 Introduction
In large-scale machine translation evaluations, phrase-based models generally outperform syntaxbased models1.
Phrase-based models are effective because they capture the lexical dependencies between languages.
However, these models, which are equivalent to finite-state machines (Kumar and Byrne, 2003), are unable to model long range word orderdifferences.
Phrase-based modelsalso lackthe ability to incorporate the generalisations implicit in syntactic knowledge and they do not respect linguistic phrase boundaries.
This makes it difficult to improve reordering in phrase-based models.
Syntax-based models can overcome some of the problems associated with phrase-based models because they are able to capture the long range structural mappings that occur in translation.
Recently 1www.nist.gov/speech/tests/mt/mt06eval official results.html there have been a few syntax-based models that show performance comparable to the phrase-based models (Chiang, 2005; Marcu et al., 2006).
However, reliably learning powerful rules from parallel data is very difficult and prone to problems with sparsity and noise in the data.
These models also suffer from a large search space when decoding with an integrated language model, which can lead to search errors (Chiang, 2005).
In this paper we investigate the idea of incorporating syntax into phrase-based models, thereby leveraging the strengths of both the phrase-based models and syntactic structures.
This is done using CCG supertags, which provide a rich source of syntactic information.
CCG contains most of the structure of the grammar in the lexicon, which makes it possible to introduce CCG supertags as a factor in a factored translation model (Koehn et al., 2006).
Factored models allow words to be vectors of features: one factor could be the surface form and other factors could contain linguistic information.
Factored models allow for the easy inclusion of supertags in different ways.
The first approach is to generate CCG supertags as a factor in the target and then apply an n-gram model over them, increasing the probability of more frequently seen sequences of supertags.
This is a simple way of including syntactic information in a phrase-based model, and has also been suggested by Hassan et al.(2007). For both Arabic-English (Hassan et al., 2007) and our experiments in Dutch-English, n-gram models over CCG supertags improve the quality of translation.
By preferring more likely sequences of supertags, it is conceivable that the output of the decoder is 9 more grammatical.
However, its not clear exactly how syntactic information can benefit a flat structured model: the constraints contained within supertags are not enforced and relationships between supertags are not linear.
We perform experiments to explore the nature and limits of the contribution of supertags, using different orders of n-gram models, reordering models and focussed manual evaluation.
It seems that the benefit of using n-gram supertag sequence models is largely from improving reordering, as much of the gain is eroded by using a lexicalised reordering model.
This is supported by the manualevaluationwhichshowsa44%improvement in reordering Dutch-English verb final sentences.
The second and novel way we use supertags is to direct the translation process.
Supertags on the source sentence allows the decoder to make decisions based on the structure of the input.
The subcategorisation of a verb, for instance, might help select the correct translation.
Using multiple dependencies on factors in the source, we need a strategy for dealing with sparse data.
We propose using a logarithmic opinion pool (Smith et al., 2005) to combinethemorespecificmodels(whichdependon both words and supertags) with more general models(whichonlydependsonwords).
Thispaperisthe first to suggest this approach for combining multiple information sources in machine translation.
Although the addition of supertags to phrasebased translation does show some improvement, their overall impact is limited.
Sequence models over supertags clearly result in some improvements in local reordering but syntactic information contains long distance dependencies which are simply not utilised in phrase-based models.
2 Factored
Models Inspired by work on factored language models, Koehn et al.(2006) extend phrase-based models to incorporate multiple levels of linguistic knowledge as factors.
Phrase-based models are limited to sequences of words as their units with no access to additional linguistic knowledge.
Factors allow for richer translation models, for example, the gender or tense of a word can be expressed.
Factors also allow the model to generalise, for example, the lemma of a word could be used to generalise to unseen inflected forms.
The factored translation model combines features in a log-linear fashion (Och, 2003).
The most likely target sentence ?t is calculated using the decision rule in Equation 1: ?t = argmax t braceleftBigg Msummationdisplay m=1 mhm(sFs1,tFt1 ) bracerightBigg (1) ?t ??
Msummationdisplay m=1 mhm(sFs1,tFt1 ) (2) where M is the number of features, hm(sFs1,tFt1 ) are the feature functions over the factors, and  are the weights which combine the features which are optimised using minimum error rate training (Venugopal and Vogel, 2005).
Each function depends on a vector sFs1 of source factors and a vector tFt1 of target factors.
An example of a factored model used in upcoming experiments is: ?t ??
Msummationdisplay m=1 mhm(sw,twc) (3) where sw means the model depends on (s)ource (w)ords, and twc means the model generates (t)arget (w)ords and (c)cg supertags.
The model is shown graphically in Figure 1.
WordWord CCG SOURCE TARGET Figure 1.
Factored translation with source words determining target words and CCG supertags For our experiments we used the following features: the translation probabilities Pr(sFs1 |tFt1 ) and Pr(tFt1 |sFs1 ),thelexicalweights(Koehnetal.,2003) lex(sFs1 |tFt1 ) and lex(tFt1 |sFs1 ), and a phrase penalty e, which allows the model to learn a preference for longer or shorter phrases.
Added to these features 10 is the word penalty e?? which allows the model to learn a preference for longer or shorter sentences, the distortion model d that prefers monotone word order, and the language model probability Pr(t).
All these features are logged when combined in the log-linear model in order to retain the impact of very unlikely translations or sequences.
One of the strengths of the factored model is it allows for n-gram distributions over factors on the target.
We call these distributions sequence models.
By analogy with language models, for example, we can construct a bigram sequence model as follows: p(f1,f2,...fn) = p(f1) nproductdisplay i=2 p(fi|f(i??)) where f is a factor (eg.
CCG supertags) and n is the length of the string.
Sequence models over POS tags or supertags are smaller than language models because they have restricted lexicons.
Higher order, more powerful sequence models can therefore be used.
Applyingmultiplefactorsinthesourcecanleadto sparse data problems.
One solution is to break down the translation into smaller steps and translate each factor separately like in the following model where source words are translated separately to the source supertags: ?t ??
Msummationdisplay m=1 mhm(sw,tw) + Nsummationdisplay n=1 nhn(sc,tw) However, in many cases multiple dependencies are desirable.
For instance translating CCG supertags independently of words could introduce errors.
Multiple dependencies require some form of backing off to simpler models in order to cover the cases where, for instance, the word has been seen in training, but not with that particular supertag.
Different backoff paths are possible, and it would be interesting but prohibitively slow to apply a strategy similar to generalised parallel backoff (Bilmes and Kirchhoff, 2003) which is used in factored language models.
Backoff in factored language models is made more difficult because there is no obvious backoff path.
This is compounded for factoredphrase-basedtranslationmodelswhereonehas to consider backoff in terms of factors and n-gram lengths in both source and target languages.
Furthermore, the surface form of a word is probably the most valuable factor and so its contribution must alwaysbetakenintoaccount.
Wethereforedidnotuse backoff and chose to use a log-linear combination of features and models instead.
Our solution is to extract two translation models: ?t ??
Msummationdisplay m=1 mhm(swc,tw) + Nsummationdisplay n=1 nhn(sw,tw) (4) One model consists of more specific features m and would return log probabilities, for example log2Pr(tw|swc), if the particular word and supertag had been seen before in training.
Otherwise it returns ?C, a negative constant emulating log2(0).
The other model consist of more general features n and always returns log probabilities, for example log2Pr(tw|sw).
3 CCG
and Supertags CCGs have syntactically rich lexicons and a small set of combinatory operators which assemble the parse-trees.
Each word in the sentence is assigned a category from the lexicon.
A category may either be atomic (S, NP etc).
or complex (S\S, (S\NP)/NP etc.).
Complex categories have the general form / or \ where  and  are themselves categories.
An example of a CCG parse is given: Peter eats apples NP (S\NP)/NP NP >S\NP <S where the derivation proceeds as follows: ?eats?? is combined with ?apples??under the operation of forward application.
?eats??can be thought of as a function that takes a NP to the right and returns a S\NP.
Similarly the phrase ?eats apples??can be thought of as a function which takes a noun phrase NP to the left and returns a sentence S.
This operation is called backward application.
A sentence together with its CCG categories already contains most of the information present in a full parse.
Because these categories are lexicalised, 11 they can easily be included into factored phrasebased translation.
CCG supertags are categories that have been provided by a supertagger.
Supertags were introduced by Bangalore (1999) as a way of increasing parsing efficiency by reducing the number of structures assigned to each word.
Clark (2002) developed a suppertagger for CCG which uses a conditional maximum entropy model to estimate the probability of words being assigned particular categories.
Here is an example of a sentence that has been supertagged in the training corpus: We all agree on that. NP NP\NP (S[dcl]\NP)/PP PP/NP NP . Theverb?agree?hasbeenassignedacomplexsupertag (S[dcl]\NP)/PP which determines the type and direction of its arguments.
This information can be used to improve the quality of translation.
4 Experiments
The first set of experiments explores the effect of CCG supertags on the target, translating from Dutch into English.
The last experiment shows the effect of CCG supertags on the source, translating from German into English.
These language pairs present a considerable reordering challenge.
For example, DutchandGermanhaveSOVwordorderinsubordinate clauses.
This means that the verb often appears at the end of the clause, far from the position of the English verb.
4.1 Experimental
Setup The experiments were run using Moses2, an open source factored statistical machine translation system.
The SRILM language modelling toolkit (Stolcke, 2002) was used with modified Kneser-Ney discounting and interpolation.
The CCG supertagger (Clark, 2002; Clark and Curran, 2004) was provided with the C&C Language Processing Tools3.
The supertagger was trained on the CCGBank in English (Hockenmaier and Steedman, 2005) and in German (Hockenmaier, 2006).
The Dutch-English parallel training data comes from the Europarl corpus (Koehn, 2005) and excludes the proceedings from the last quarter of 2000.
2see http://www.statmt.org/moses/ 3see http://svn.ask.it.usyd.edu.au/trac/candc/wiki This consists of 855,677 sentences with a maximum of 50 words per sentence.
500 sentences of tuning data and the 2000 sentences of test data are taken fromtheACLWorkshoponBuildingandUsingParallel Texts4.
The German-English experiments use data from the NAACL 2006 Workshop on Statistical Machine Translation5.
Thedataconsistsof751,088sentences of training data, 500 sentences of tuning data and 3064 sentences of test data.
The English and German training sets were POS tagged and supertagged before lowercasing.
The language models and the sequence models were trained on the Europarl training data.
Where not otherwise specified, the POS tag and supertag sequence models are 5-gram models and the language model is a 3-gram model.
4.2 Sequence
Models Over Supertags Our first Dutch-English experiment seeks to establish what effect sequence models have on machine translation.
We show that supertags improve translation quality.
Together with Shen et al.(2006) it is one of the first results to confirm the potential of the factored model.
Model BLEU sw,tw 23.97 sw,twp 24.11 sw,twc 24.42 sw,twpc 24.43 Table 1.
The effect of sequence models on Dutch-English BLEU score.
Factors are (w)ords, (p)os tags, (c)cg supertags on the source s or the target t Table1showsthatsequencemodelsoverCCGsupertags in the target (model sw,twc) improves over the baseline (model sw,tw) which has no supertags.
Supertag sequence models also outperform models which apply POS tag sequence models (sw,twp) and, interestingly do just as well as models which apply both POS tag and supertag sequence models (sw,twps).
Supertags are more informative than POS tags as they contain the syntactic context of a word.
These experiments were run with the distortion limit set to 6.
This means that at most 6 words in 4see http://www.statmt.org/wpt05/ 5see http://www.statmt.org/wpt06/ 12 the source sentence can be skipped.
We tried setting the distortion limit to 15 to see if allowing longer distance reorderings with CCG supertag sequence modelscouldfurtherimproveperformance,however it resulted in a decrease in performance to a BLEU score of 23.84. 4.3 Manual Analysis The BLEU score improvement in Table 1 does not explain how the supertag sequence models affect the translation process.
As suggested by Callison-Burch et al.(2006) we perform a focussed manual analysis of the output to see what changes have occurred.
From the test set, we randomly selected 100 sentences which required reordering of verbs: the Dutch sentences ended with a verb which had to be movedforwardintheEnglishtranslation.
Werecord whether or not the verb was correctly translated and whether it was reordered to the correct position in the target sentence.
Model Translated Reordered sw,tw 81 36 sw,twc 87 52 Table 2.
Analysis of % correct translation and reordering of verbs for Dutch-English translation In Table 2we can seethat the addition ofthe CCG supertag sequence model improved both the translation of the verbs and their reordering.
However, the improvement is much more pronounced for reordering.
Thedifferenceinthereorderingresultsissignificant at p < 0.05 using the ?2 significance test.
This shows that the syntactic information in the CCG supertags is used by the model to prefer better word order for the target sentence.
In Figure 2 we can see two examples of DutchEnglish translations that have improved with the application of CCG supertag sequence models.
In the firstexampletheverb?heeft?occursattheendofthe source sentence.
The baseline model (sw,tw) does not manage to translate ?heeft??
The model with the CCG supertag sequence model (sw,twc) translates it correctly as ?has??and reorders it correctly 4 places to the left.
The second example also shows the sequence model correctly translating the Dutch verb at the end of the sentence ?nodig??
One can see that it is still not entirely grammatical.
The improvements in reordering shown here are reorderings over a relatively short distance, two or three positions.
This is well within the 5-gram order of the CCG supertag sequence model and we therefore consider this to be local reordering.
4.4 Order
of the Sequence Model The CCG supertags describe the syntactic context of the word they are attached to.
Therefore they have an influence that is greater in scope than surface words or POS tags.
Increasing the order of the CCG supertag sequence model should also increase the ability to perform longer distance reordering.
However, at some point the reliability of the predictions of the sequence models is impaired due to sparse counts.
Model None 1gram 3gram 5gram 7gram sw, twc 24.18 23.96 24.19 24.42 24.32 sw, twpc 24.34 23.86 24.09 24.43 24.14 Table 3.
BLUE scores for Dutch-English models which apply CCG supertag sequence models of varying orders In Table 3 we can see that the optimal order for the CCG supertag sequence models is 5.
4.5 Language
Model vs.
Supertags The language model makes a great contribution to the correct order of the words in the target sentence.
In this experiment we investigate whether by using a stronger language model the contribution of the sequence model will no longer be relevant.
The relative contribution of the language mode and differentsequencemodelsisinvestigatedfordifferentlanguage model n-gram lengths.
Model None 1gram 3gram 5gram 7gram sw, tw 21.22 23.97 24.05 24.13 sw, twp 21.87 21.83 24.11 24.25 24.06 sw, twc 21.75 21.70 24.42 24.67 24.60 sw, twpc 21.99 22.07 24.43 24.48 24.42 Table 4.
BLEU scores for Dutch-English models which use language models of increasing n-gram length.
Column None does not apply any language model.
Model sw, tw does not apply any sequence models, and model sw, twpc applies both POS tag and supertag sequence models.
In Table 4 we can see that if no language model is present(None), the system benefits slightly from 13 source:hij kan toch niet beweren dat hij daar geen exacte informatie over heeft ! reference: how can he say he does not have any precise information ? sw, tw:he cannot say that he is not an exact information about . sw, twc: he cannot say that he has no precise information on this ! source: wij moeten hun verwachtingen niet beschamen . meer dan ooit hebben al die landen thans onze bijstand nodig reference: we must not disappoint them in their expectations, and now more than ever these countries need our help sw, tw:we must not fail to their expectations, more than ever to have all these countries now our assistance necessary sw, twc: we must not fail to their expectations, more than ever, those countries now need our assistance Figure 2.
Examples where the CCG supertag sequence model improves Dutch-English translation having access to all the other sequence models.
However, the language model contribution is very strong and in isolation contributes more to translation performance than any other sequence model.
Even with a high order language model, applying the CCG supertag sequence model still seems to improve performance.
This means that even if we use a more powerful language model, the structural information contained in the supertags continues to be beneficial.
4.6 Lexicalised
Reordering vs.
Supertags In this experiment we investigate using a stronger reordering model to see how it compares to the contribution that CCG supertag sequence models make.
Moses implements the lexicalised reordering model described by Tillman (2004), which learns whether phrases prefer monotone, inverse or disjoint orientations with regard to adjacent phrases.
We apply this reordering models to the following experiments.
Model None Lex.
Reord. sw, tw 23.97 24.72 sw, twc 24.42 24.78 Table 5.
Dutch-English models with and without a lexicalised reordering model.
In Table 5 we can see that lexicalised reordering improves translation performance for both models.
However, the improvement that was seen using CCG supertags without lexicalised reordering, almost disappears when using a stronger reordering model.
This suggests that CCG supertags??contribution is similar to that of a reordering model.
The lexicalised reordering model only learns the orientation of a phrase with relation to its adjacent phrase, so its influence is very limited in range.
If it can replace CCG supertags, it suggests that supertags??influence is also within a local range.
4.7 CCG
Supertags on Source Sequence modelsover supertags improvethe performance of phrase-based machine translation.
However, this is a limited way of leveraging the rich syntactic information available in the CCG categories.
We explore the potential of letting supertags direct translation by including them as a factor on the source.
This is similar to syntax-directed translation originally proposedfor compiling (Ahoand Ullman, 1969),andalsousedinmachinetranslation(Quirket al., 2005; Huang et al., 2006).
Information about the source words??syntactic function and subcategorisation can directly influence the hypotheses being searched in decoding.
These experiments were performed on the German to English translation task, in contrast to the Dutch to English results given in previous experiments.
We use a model which combines more specific dependencies on source words and source CCG supertags, with a more general model which only has dependancies on the source word, see Equation 4.
We explore two different ways of balancing the statistical evidence from these multiple sources.
The first way to combine the general and specific sources of information is by considering features from both models as part of one large log-linear model.
However, by including more and less informative features in one model, we may transfer too much explanatory power to the more specific features.
To overcome this problem, Smith et al.(2006) demonstrated that using ensembles of separately trained models and combining them in a logarithmic opinion pool (LOP) leads to better parameter values.
This approach was used as the second way in which 14 wecombinedourmodels.
Anensembleoflog-linear models was combined using a multiplicative constant  which we train manually using held out data.
?t ??
Msummationdisplay m=1 mhm(swc,tw) +  parenleftBigg Nsummationdisplay n=1 nhn(sw,tw) parenrightBigg Typically, the two models would need to be normalised before being combined, but here the multiplicative constant fulfils this r?ole by balancing their separate contributions.
This is the first work suggesting the application of LOPs to decoding in machine translation.
In the future more sophisticated translation models and ensembles of models will need methods such as LOPs in order to balance statistical evidence from multiple sources.
Model BLEU sw,tw 23.30 swc,tw 19.73 single 23.29 LOP 23.46 Table 6.
German-English: CCG supertags are used as a factor on the source.
The simple models are combined in two ways: either as a single log-linear model or as a LOP of log-linear models Table 6 shows that the simple, general model (model sw,tw) performs considerably better than the simple specific model, where there are multiple dependencies on both words and CCG supertags (model swc,tw).
This is because there are words in the test sentence that have been seen before but not with the CCG supertag.
Statistical evidence from multiple sources must be combined.
The first way to combine them is to join them in one single loglinear model, which is trained over many features.
This makes finding good weights difficult as the influence of the general model is greater, and its difficult for the more specific model to discover good weights.
The second method for combining the information is to use the weights from the separately trained simple models and then combine them in a LOP.
Held out data is used to set the multiplicative constant needed to balance the contribution of the two models.
We can see that this second approach is more successful and this suggests that it is important tocarefullyconsiderthebestwaysofcombiningdifferent sources of information when using ensembles of models.
However, the results of this experiment are not very conclusive.
There is no uncertainty in the source sentence and the value of modelling it using CCG supertags is still to be demonstrated.
5 Conclusion
The factored translation model allows for the inclusion of valuable sources of information in many different ways.
We have shown that the syntactically rich CCG supertags do improve the translation process and we investigate the best way of including them in the factored model.
Using CCG supertags over the target shows the most improvement, especially when using targeted manual evaluation.
However, this effect seems to be largely due to improved local reordering.
Reordering improvements can perhaps be more reliably made using better reordering models or larger, more powerful language models.
A further consideration is that supertags will always be limited to the few languages for which there are treebanks.
Syntactic information represents embedded structures which are naturally incorporated into grammar-based models.
The ability of a flat structured model to leverage this information seems to be limited.
CCG supertags??ability to guide translation would be enhanced if the constraints encoded in the tags were to be enforced using combinatory operators.
6 Acknowledgements
WethankHieuHoangforassistancewithMoses, Julia Hockenmaier for access to CCGbank lexicons in German and English, and Stephen Clark and James Curranforprovidingthesupertagger.
Thisworkwas supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No.
HR0011-06-C-0022 and in part under the EuroMatrix project funded by the European Commission (6th Framework Programme).
15 References
Alfred V.
Aho and Jeffrey D.
Ullman. 1969.
Properties of syntax directed translations.
Journal of Computer and System Sciences, 3(3):319??34.
Srinivas Bangalore and Aravind Joshi.
1999. Supertagging: An approach to almost parsing.
Computational Linguistics, 25(2):237??65.
Jeff Bilmes and Katrin Kirchhoff.
2003. Factored language models and generalized parallel backoff.
In Proceedings of the North American Association for Computational Linguistics Conference, Edmonton, Canada.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluating the role of Bleu in machine translation research.
In Proceedings of the European Chapter of the Association for Computational Linguistics, Trento, Italy.
David Chiang.
2005. A hierarchical phrase-based model for statistical machine translation.
In Proceedings of the Association for Computational Linguistics, pages 263??70, Ann Arbor, Michigan.
Stephen Clark and James R.
Curran. 2004.
Parsing the wsj using ccg and log-linear models.
In Proceedings of the Association for Computational Linguistics, pages 103??10, Barcelona, Spain.
Stephen Clark.
2002. Supertagging for combinatory categorial grammar.
In Proceedings of the International Workshop on Tree Adjoining Grammars, pages 19??4, Venice, Italy.
Hany Hassan, Khalil Sima?an, and Andy Way.
2007. Supertagged phrase-based statistical machine translation.
In Proceedings of the Association for Computational Linguistics, Prague, Czech Republic.
(to appear).
Julia Hockenmaier and Mark Steedman.
2005. Ccgbank manual.
Technical Report MS-CIS-05-09, Department of Computer and Information Science, University of Pennsylvania.
Julia Hockenmaier.
2006. Creating a ccgbank and a wide-coverage ccg lexicon for german.
In Proceedings of the International Conference on Computational Linguistics and of the Association for Computational Linguistics, Sydney, Australia.
Liang Huang, Kevin Knight, and Aravind Joshi.
2006. A syntax-directed translator with extended domain of locality.
In Proceedings of the Workshop on Computationally Hard Problems and Joint Inference in Speech and Language Processing, pages 1??, New York City, New York.
Association for Computational Linguistics.
Philipp Koehn, Franz Och, and Daniel Marcu.
2003. Statistical phrase-based translation.
In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference, pages 127??33, Edmonton,Canada.AssociationforComputationalLinguistics.
Philipp Koehn, Hieu Hoang, Chris Callison-Burch, Marcello Federico, NicolaBertoldi, RichardZens, ChrisDyer, Brooke Cowan, Wade Shen, Christine Moran, Ondrej Bojar, Alexandra Constantin, and Evan Herbst.
2006. Open source toolkit for statistical machine translation.
In Summer Workshop on Language Engineering, John Hopkins University Center for Language and Speech Processing.
Philipp Koehn.
2005. Europarl: A parallel corpus for statistical machine translation.
In MT Summit.
Shankar Kumar and William Byrne.
2003. A weighted finite state transducer implementation of the alignment template model for statistical machine translation.
In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference, pages 63??0, Edmonton, Canada.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight.
2006. SPMT: Statistical machine translation with syntactified target language phrases.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 44??2, Sydney, Australia.
Franz Josef Och.
2003. Minimum error rate training in statistical machine translation.
In Proceedings of the Association for Computational Linguistics, pages 160??67, Sapporo, Japan.
Chris Quirk, Arul Menezes, and Colin Cherry.
2005. Dependency treelet translation: Syntactically informed phrasal SMT.
In Proceedings of the Association for Computational Linguistics, pages 271??79, Ann Arbor, Michigan.
Wade Shen, Richard Zens, Nicola Bertoldi, and Marcello Federico.
2006. The JHU workshop 2006 IWSLT system.
In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), pages 59??3, Kyoto, Japan.
Andrew Smith and Miles Osborne.
2006. Using gazetteers in discriminative information extraction.
In The Conference on Natural Language Learning, New York City, USA.
Andrew Smith, Trevor Cohn, and Miles Osborne.
2005. Logarithmic opinion pools for conditional random fields.
In Proceedings of the Association for Computational Linguistics, pages 18??5, Ann Arbor, Michigan.
Andreas Stolcke.
2002. SRILM an extensible language modeling toolkit.
In Proceedings of Spoken Language Processing, pages 901??04.
Christoph Tillman.
2004. A unigram orientation model for statistical machine translation.
In Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference, pages 101??04, Boston, USA.
Association for Computational Linguistics.
Ashish Venugopal and Stephan Vogel.
2005. Considerations in MCE and MMI training for statistical machine translation.
In Proceedings of the European Association for Mchine Translation, Budapest, Hungary .
Proceedings of the Second Workshop on Statistical Machine Translation, pages 17??4, Prague, June 2007.
c2007 Association for Computational Linguistics Integration of an Arabic Transliteration Module into a Statistical Machine Translation System Mehdi M.
Kashani +, Eric Joanis ++, Roland Kuhn ++, George Foster ++, Fred Popowich + + School of Computing Science Simon Fraser University 8888 University Drive Burnaby, BC V5A 1S6, Canada mmostafa@sfu.ca popowich@sfu.ca ++ NRC Institute for Information Technology 101 St-Jean-Bosco Street Gatineau, QC K1A 0R6, Canada firstname.lastname@cnrc-nrc.gc.ca Abstract We provide an in-depth analysis of the integration of an Arabic-to-English transliteration system into a general-purpose phrase-based statistical machine translation system.
We study the integration from different aspects and evaluate the improvement that can be attributed to the integration using the BLEU metric.
Our experiments show that a transliteration module can help significantly in the situation where the test data is rich with previously unseen named entities.
We obtain 70% and 53% of the theoretical maximum improvement we could achieve, as measured by an oracle on development and test sets respectively for OOV words (out of vocabulary source words not appearing in the phrase table).
1 Introduction
Transliteration is the practice of transcribing a word or text written in one writing system into another writing system.
The most frequent candidates for transliteration are person names, locations, organizations and imported words.
The lack of a fully comprehensive bilingual dictionary including the entries for all named entities (NEs) renders the task of transliteration necessary for certain natural language processing applications dealing with named entities.
Two applications where transliteration can be particularly useful are machine translation (MT) and cross lingual information retrieval.
While transliteration itself is a relatively wellstudied problem, its effect on the aforementioned applications is still under investigation.
Transliteration as a self-contained task has its own challenges, but applying it to a real application introduces new challenges.
In this paper we analyze the efficacy of integrating a transliteration module into a real MT system and evaluate the performance.
When working on a limited domain, given a sufficiently large amount of training data, almost all of the words in the unseen data (in the same domain) will have appeared in the training corpus.
But this argument does not hold for NEs, because no matter how big the training corpus is, there will always be unseen names of people and locations.
Current MT systems either leave such unknown names as they are in the final target text or remove them in order to obtain a better evaluation score.
None of these methods can give the reader who is not familiar with the source language any information about those out-of-vocabulary (OOV) words, especially when the source and target languages use different scripts.
If these words are not names, one can usually guess what they are, by using the partial information of other parts of speech.
But, in the case of names, there is no way to determine the individual or location the sentence is talking about.
So, to improve the usability of a translation, it is particularly important to handle NEs well.
The importance of NEs is not yet reflected in the evaluation methods used in the MT community, the most common of which is the BLEU metric.
BLEU (Papineni et al, 2002) was devised to provide automatic evaluation of MT output.
In this metric n-gram similarity of the MT output is computed with one or more references made by human 17 translators.
BLEU does not distinguish between different words and gives equal weight to all.
In this paper, we base our evaluation on the BLEU metric and show that using transliteration has impact on it (and in some cases significant impact).
However, we believe that such integration is more important for practical uses of MT than BLEU indicates.
Other than improving readability and raising the BLEU score, another advantage of using a transliteration system is that having the right translation for a name helps the language model select a better ordering for other words.
For example, our phrase table 1 does not have any entry for ????(Dulles) and when running MT system on the plain Arabic text we get and this trip was cancelled [?? by the american authorities responsible for security at the airport . We ran our MT system twice, once by suggesting ?dallas??and another time ?dulles??as English equivalents for ????and the decoder generated the following sentences, respectively: and this trip was cancelled [?? by the american authorities responsible for security at the airport at dallas . and this trip was cancelled [?? by the american authorities responsible for security at dulles airport . 2 Every statistical MT (SMT) system assigns a probability distribution to the words that are seen in its parallel training data, including proper names.
The richer the training data, the higher the chance for a given name in the test data to be found in the translation tables.
In other words, an MT system with a relatively rich phrase table is able to translate many of the common names in the test data, with all the remaining words being rare and foreign.
So unlike a self-contained transliteration module, which typically deals with a mix of ?easy??and 1 A table where the conditional probabilities of target phrases given source phrases (and vice versa) is kept.
2 Note
that the language model can be trained on more text, and hence can know more NEs than the translation model does.
?hard??names, the primary use for a transliteration module embedded in an SMT system will be to deal with the ?hard??names left over after the phrase tables have provided translations for the ?easy??ones.
That means that when measuring the performance improvements caused by embedding a transliteration module in an MT system, one must keep in mind that such improvements are difficult to attain: they are won mainly by correctly transliterating ?hard??names.
Another issue with OOV words is that some of them remained untranslated due to misspellings in the source text.
For example, we encountered ? ??(?Hthearow?? instead of ?? ??
(?Heathrow?? or ????(?Brezer?? instead of ??? ??(?Bremer?? in our development test set.
Also, evaluation by BLEU (or a similar automatic metric) is problematic.
Almost all of the MT evaluations use one or more reference translations as the gold standard and, using some metrics, they give a score to the MT output.
The problem with NEs is that they usually have more than a single equivalent in the target language (especially if they don't originally come from the target language) which may or may not have been captured in the gold standard.
So even if the transliteration module comes up with a correct interpretation of a name it might not receive credit as far as the limited number of correct names in the references are concerned.
Our first impression was that having more interpretations for a name in the references would raise the transliteration module?s chance to generate at least one of them, hence improving the performance.
But, in practice, when references do not agree on a name?s transliteration that is the sign of an ambiguity.
In these cases, the transliteration module often suggests a correct transliteration that the decoder outputs correctly, but which fails to receive credit from the BLEU metric because this transliteration is not found in the references.
As an example, for the name ???? four references came up with four different interpretations: swerios, swiriyus, severius, sweires.
A quick query in Google showed us another four acceptable interpretations (severios, sewerios, sweirios, sawerios).
Machine transliteration has been an active research field for quite a while (Al-Onaizan and Knight, 2002; AbdulJaleel and Larkey, 2003; Klementiev and Roth, 2006; Sproat et al, 2006) but to 18 our knowledge there is little published work on evaluating transliteration within a real MT system.
The closest work to ours is described in (Hassan and Sorensen, 2005) where they have a list of names in Arabic and feed this list as the input text to their MT system.
They evaluate their system in three different cases: as a word-based NE translation, phrase-based NE translation and in presence of a transliteration module.
Then, they report the BLEU score on the final output.
Since their text is comprised of only NEs, the BLEU increase is quite high.
Combining all three models, they get a 24.9 BLEU point increase over the nave baseline.
The difference they report between their best method without transliteration and the one including transliteration is 8.12 BLEU points for person names (their best increase).
In section 2, we introduce different methods for incorporating a transliteration module into an MT system and justify our choice.
In section 3, the transliteration module is briefly introduced and we explain how we prepared its output for use by the MT system.
In section 4, an evaluation of the integration is provided.
Finally, section 5 concludes the paper.
2 Our
Approach Before going into details of our approach, an overview of Portage (Sadat et al, 2005), the machine translation system that we used for our experiments and some of its properties should be provided.
Portage is a statistical phrase-based SMT system similar to Pharaoh (Koehn et al, 2003).
Given a source sentence, it tries to find the target sentence that maximizes the joint probability of a target sentence and a phrase alignment according to a loglinear model.
Features in the loglinear model consist of a phrase-based translation model with relativefrequency and lexical probability estimates; a 4gram language model using Kneser-Ney smoothing, trained with the SRILM toolkit; a singleparameter distortion penalty on phrase reordering; and a word-length penalty.
Weights on the loglinear features are set using Och's algorithm (Och, 2003) to maximize the system's BLEU score on a development corpus.
To generate phrase pairs from a parallel corpus, we use the "diag-and" phrase induction algorithm described in (Koehn et al, 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al, 1993).
Portage allows the use of SGML-like markup for arbitrary entities within the input text.
The markup can be used to specify translations provided by external sources for the entities, such as rule-based translations of numbers and dates, or a transliteration module for OOVs in our work.
Many SMT systems have this capability, so although the details given here pertain to Portage, the techniques described can be used in many different SMT systems.
As an example, suppose we already have two different transliterations with their probabilities for the Arabic name ????
We can replace every occurrence of the ????in the Arabic input text with the following: <NAME target="mohammed|mohamed" prob=".7|.3">  </NAME> By running Portage on this marked up text, the decoder chooses between entries in its own phrase table and the marked-up text.
One thing that is important for our task is that if the entry cannot be found in Portage?s phrase tables, it is guaranteed that one of the candidates inside the markup will be chosen.
Even if none of the candidates exist in the language model, the decoder still picks one of them, because the system assigns a small arbitrary probability (we typically use e -18 ) as unigram probability of each unseen word.
We considered four different methods for incorporating the transliteration module into the MT system.
The first and second methods need an NE tagger and the other two do not require any external tools.
Method 1: use an NE tagger to extract the names in the Arabic input text.
Then, run the transliteration module on them and assign probabilities to top candidates.
Use the markup capability of Portage and replace each name in the Arabic text with the SGML-like tag including different probabilities for different candidates.
Feed the marked-up text to Portage to translate.
Method 2: similar to method 1 but instead of using the marked-up text, a new phrase table, only containing entries for the names in the Arabic input text is built and added to Portage?s existing phrase tables.
A weight is given to this phrase table and 19 then the decoder uses this phrase table as well as its own phrase tables to decide which translation to choose when encountering the names in the text.
The main difference between methods 1 and 2 is that in our system, method 2 allows for a bleuoptimal weight to be learned for the NE phrase table, whereas the weight on the rules for method 1 has to be set by hand.
Method 3: run Portage on the plain Arabic text.
Extract all untranslated Arabic OOVs and run the transliteration module on them.
Replace them with the top candidate.
Method 4: run Portage on the plain Arabic text.
Extract all untranslated Arabic OOVs and run the transliteration module on them.
Replace them with SGML-like tags including different probabilities for different candidates, as described previously.
Feed the marked-up text to Portage to translate.
The first two methods need a powerful NE tagger with a high recall value.
We computed the recall value on the development set OOVs using two different NE taggers, Tagger A and Tagger B (each from a different research group).
Taggers A and B showed a recall of 33% and 53% respectively, both being low for our purposes.
Another issue with these two methods is that for many of the names the transliteration module will compete with the internal phrase table.
Our observations show that if a name exists in the phrase table, it is likely to be translated correctly.
In general, observed parallel data (i.e.
training data) should be a more reliable source of information than transliteration, encouraging us to use transliteration most appropriately as a ?back-off??method.
In a few cases, the Arabic name is ambiguous with a common word and is mistakenly translated as such.
For example, ? ? ???is an Arabic name that should be transliterated as ?Hani Abu Nahl??but since ? ??also means ?solve?? the MT system outputs ?Hani Abu Solve??
The advantage of the first two methods is that they can deal with such cases.
But considering the noise in the NE detectors, handling them increases the risk of losing already correct translations of other names.
The third method is simple and easy to use but not optimal: it does not take advantage of the decoder?s internal features (notably the language models) and only picks up the highest scoring candidate from the transliteration module.
The fourth method only deals with those words that the MT system was unable to deal with and had to leave untranslated in the final text.
Therefore whatever suggestions the transliteration module makes do not need to compete with the internal phrase tables, which is good because we expect the phrase tables to be a more reliable source of information.
It is guaranteed that the translation quality will be improved (in the worst case, a bad transliteration is still more informative than the original word in Arabic script).
Moreover, unlike the third method, we take advantage of all internal decoder features on the second pass.
We adopt the fourth method for our experiment.
The following example better illustrates how this approach works: Example: Suppose we have the following sentence in the Arabic input text: ?????????? ??.
Portage is run on the Arabic plain text and yields the following output: blair accepts ????report in full . The Arabic word ? ??(Hutton) is extracted and fed to the transliteration module.
The transliteration module comes up with some English candidates, each with different probabilities as estimated by the HMM.
They are rescaled (as will be explained in section 3) and the following markup text will be generated to replace the untranslated ? ??in the first plain Arabic sentence: <NAME target="hoton|hutton|authon" prob="0.1|0.00028|4.64e-05">????</NAME> Portage is then run on this newly marked up text (second pass).
From now on, with the additional guidance of the language models, it is the decoder?s task to decide between different markup suggestions.
For the above example, the following output will be generated: blair accepts hutton report in full . 20 3 Transliteration System In this section we provide a brief overview of the embedded transliteration system we used for our experiment.
For the full description refer to (Kashani et al, 2007).
3.1 Three
Phase Transliteration The transliteration module follows the noisy channel framework.
The adapted spelling-based generative model is similar to (Al-Onaizan and Knight, 2002).
It consists of three consecutive phases, the first two using HMMs and the Viterbi algorithm, and the third using a number of monolingual dictionaries to match the close entries or to filter out some invalid candidates from the first two phases.
Since in Arabic, the diacritics are usually omitted in writing, a name like ????(Mohamed) would have an equivalent like ?mhmd??if we only take into account the written letters.
To address this issue, we run Viterbi in two different passes (each called a phase), using HMMs trained on data prepared in different ways.
In phase 1, the system tries to find the best transliterations of the written word, without caring about what the hidden diacritics would be (in our example, mhmd).
In phase 2, given the Arabic input and the output candidates from phase 1, the system fills in the possible blanks in between using the characterbased language model (yielding ?mohamed??as a possible output, among others).
To prepare the character-level translation model for both phases we adopted an approach similar to (AbdulJaleel and Larkey, 2003).
In phase 3, the Google unigram model (LDC2006T13 from the LDC catalog) is first used to filter out the noise (i.e.
those candidates that do not exist in the Google unigram are removed from the candidate list).
Then a combination of some monolingual dictionaries of person names is used to find close matches between their entries and the HMM output candidates based on the Levenshtein distance metric.
3.2 Task-specific Changes to the Module Due to the nature of the task at hand and by observing the development test set and its references, the following major changes became necessary: Removing Part of Phase Three: By observing the OOV words in the development test set, we realized that having the monolingual dictionary in the pipeline and using the Levensthtein distance as a metric for adding the closest dictionary entries to the final output, does not help much, mainly because OOVs are rarely in the dictionary.
So, the dictionary part not only slows down the execution but would also add noise to the final output (by adding some entries that probably are not the desired outputs).
However, we kept the Google unigram filtering in the pipeline.
Rescaling HMM Probabilities: Although the transliteration module outputs HMM probability score for each candidate, and the MT system also uses probability scores, in practice the transliteration scores have to be adjusted.
For example, if three consecutive candidates have log probabilities -40, -42 and -50, the decoder should be given values with similar differences in scale, comparable with the typical differences in its internal features (eg.
Language Models).
Knowing that the entries in the internal features usually have exponential differences, we adopted the following conversion formula: p' i = 0.1*(p i /p max ) ??
Equation 1 where p i = 10 (output of HMM for candidate i) and max is the best candidate.
We rescale the HMM probability so that the top candidate is (arbitrarily) given a probability of p' max = 0.1.
It immediately follows that the rescaled score would be 0.1 * p i / p max . Since the decoder combines its models in a log-linear fashion, we apply an exponent ??to the HMM probabilities before scaling them, as way to control the weight of those probabilities in decoding.
This yields equation 1.
Ideally, we would like the weight ??to be optimized the same way other decoder weights are optimized, but our decoder does not support this yet, so for this work we arbitrarily set the weight to ??= 0.2, which seems to work well.
For the above example, the distribution would be 0.1, 0.039 and 0.001. 21 Prefix Detachment: Arabic is a morphologically rich language.
Even after performing tokenization, some words still remain untokenized.
If the composite word is frequent, there is a chance that it exists in the phrase table but many times it does not, especially if the main part of that word is a named entity.
We did not want to delve into the details of morphology: we only considered two frequent prefixes: ????(?va??meaning ?and?? and ????(?al??determiner in Arabic).
If a word starts with either of these two prefixes, we detach them and run the transliteration module once on the detached name and a second time on the whole word.
The output candidates are merged automatically based on their scores, and the decoder decides which one to choose.
Keeping the Top 5 HMM Candidates: The transliteration module uses the Google unigram model to filter out the candidate words that do not appear above a certain threshold (200 times) on the Internet.
This helps eliminate hundreds of unwanted sequences of letters.
But, we decided to keep top-5 candidates on the output list, even if they are rejected by the Google unigram model because sometimes the transliteration module is unable to suggest the correct equivalent or in other cases the OOV should actually be translated rather than transliterated 3 . In these cases, the closest literal transliteration will still provide the end user more information about the entity than the word in Arabic script would.
4 Evaluation
Although there are metrics that directly address NE translation performance 4, we chose to use BLEU because our purpose is to assess NE translation within MT, and BLEU is currently the standard metric for MT.
3 This
would happen especially for ancient names or some names that underwent sophisticated morphological transformations (For example, Abraham in English and ??? (Ibrahim) in Arabic).
4 NIST?s NE translation task (http://www.nist.gov/speech/tests/ace/index.htm) is an example.
4.1 Training
Data We used the data made available for the 2006 NIST Machine Translation Evaluation.
Our bilingual training corpus consisted of 4M sentence pairs drawn mostly from newswire and UN domains.
We trained one language model on the English half of this corpus (137M running words), and another on the English Gigaword corpus (2.3G running words).
For tuning feature weights, we used LDC's "multiple translation part 1" corpus, which contains 1,043 sentence pairs.
4.2 Test
Data We used the NIST MT04 evaluation set and the NIST MT05 evaluation set as our development and blind test sets.
The development test set consists of 1353 sentences, 233 of which contain OOVs.
Among them 100 sentences have OOVs that are actually named entities.
The blind test set consists of 1056 sentences, 189 of them having OOVs and 131 of them having OOV named entities.
The number of sentences for each experiment is summarized in table 1.
Whole Text OOV Sentences OOV-NE Sentences Dev test set 1353 233 100 Blind test set 1056 189 131 Table 1: Distribution of sentences in test sets.
4.3 Results
As the baseline, we ran the Portage without the transliteration module on development and blind test sets.
The second column of table 2 shows baseline BLEU scores.
We applied method 4 as outlined in section 2 and computed the BLEU score, also in order to compare the results we implemented method 3 on the same test sets.
The BLEU scores obtained from methods 3 and 4 are shown in columns 3 and 4 of table 2.
baseline Method 3 Method 4 Oracle Dev 44.67 44.71 44.83 44.90 Blind 48.56 48.62 48.80 49.01 Table 2: BLEU score on different test sets.
Considering the fact that only a small portion of the test set has out-of-vocabulary named entities, 22 we computed the BLEU score on two different sub-portions of the test set: first, on the sentences with OOVs; second, only on the sentences containing OOV named entities.
The BLEU increase on different portions of the test set is shown in table 3.
baseline Method 4 Dev OOV sentences 39.17 40.02 OOV-NE Sentences 44.56 46.31 blind OOV sentences 43.93 45.07 OOV-NE Sentences 42.32 44.87 Table 3: BLEU score on different portions of the test sets.
To set an upper bound on how much applying any transliteration module can contribute to the overall results, we developed an oracle-like dictionary for the OOVs in the test sets, which was then used to create a markup Arabic text.
By feeding this markup input to the MT system we obtained the result shown in column 5 of table 2.
This is the performance our system would achieve if it had perfect accuracy in transliteration, including correctly guessing what errors the human translators made in the references.
Method 4 achieves 70% of this maximum gain on dev, and 53% on blind.
5 Conclusion
This paper has described the integration of a transliteration module into a state-of-the-art statistical machine translation (SMT) system for the Arabic to English task.
The final version of the transliteration module operates in three phases.
First, it generates English letter sequences corresponding to the Arabic letter sequence; for the typical case where the Arabic omits diacritics, this often means that the English letter sequence is incomplete (e.g., vowels are often missing).
In the next phase, the module tries to guess the missing English letters.
In the third phase, the module uses a huge collection of English unigrams to filter out improbable or impossible English words and names.
We described four possible methods for integrating this module in an SMT system.
Two of these methods require NE taggers of higher quality than those available to us, and were not explored experimentally.
Method 3 inserts the top-scoring candidate from the transliteration module in the translation wherever there was an Arabic OOV in the source.
Method 4 outputs multiple candidates from the transliteration module, each with a score; the SMT system combines these scores with language model scores to decide which candidate will be chosen.
In our experiments, Method 4 consistently outperformed Model 3.
Note that although we used BLEU as the metric for all experiments in this paper, BLEU greatly understates the importance of accurate transliteration for many practical SMT applications.
References Nasreen AbdulJaleel and Leah S.
Larkey, 2003.
Statistical Transliteration for English-Arabic Cross Language Information Retrieval, Proceedings of the Twelfth International Conference on Information and Knowledge Management, New Orleans, LA Yaser Al-Onaizan and Kevin Knight, 2002.
Machine Transliteration of Names in Arabic Text, Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages Peter F.
Brown, Vincent J.
Della Pietra, Stephen A.
Della Pietra, and Robert L.
Mercer, 1993.
The Mathematics of Statistical Machine Translation: Parameter Estimation, Computational Linguistics Hany Hassan and Jeffrey Sorensen, 2005.
An Integrated Approach for Arabic-English Named Entity Translation, Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages (ACL), University of Michigan, Ann Arbor Mehdi M.
Kashani, Fred Popowich, and Anoop Sarkar, 2007.
Automatic Transliteration of Proper Nouns from Arabic to English, Proceedings of the Second Workshop on Computational Approaches to Arabic Script-based Languages Alexandre Klementiev and Dan Roth, 2006.
Named Entity Transliteration and Discovery from Multilingual Comparable Corpora, COLING-ACL, Sidney, Australia Philipp Koehn, Franz Josef Och, and Daniel Marcu, 2003.
Statistical Phrase-based Translation, In Proceedings of HLT-NAACL, Edmonton, Canada Franz Josef Och, 2003.
Minimum Error Rate Training for Statistical Machine Translation, In Proceedings of the 41th Annual Meeting of the Association for Computation Linguistics, Sapporo Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu, 2002.
BLEU: a Method for Automatic Evaluation of Machine Translation.
In Proceedings 23 of the 40th Annual Conference of the Association for Computational Linguistics (ACL), Philadelphia, PA Fatiha Sadat, Howard Johnson, Akakpo Agbago, George Foster, Roland Kuhn, Aaron Tikuisis, 2005.
Portage: A Phrase-base Machine Translation System.In Proceedings of the ACL Workshop on Building and Using Parallel Texts, Ann Arbor, Michigan Richard Sproat, Tao Tao, and ChengXiang Zhai, 2006, Named Entity Transliteration with Comparable Corpora, COLING-ACL, Sidney, Australia
Proceedings of the Second Workshop on Statistical Machine Translation, pages 25??2, Prague, June 2007.
c2007 Association for Computational Linguistics Exploring Different Representational Units in English-to-Turkish Statistical Machine Translation Kemal Oflazer????
?Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA oflazer@sabanciuniv.edu ?Ilknur Durgar El-Kahlout??
??Faculty of Engineering and Natural Sciences Sabanc University Istanbul, Tuzla, 34956, Turkey ilknurdurgar@su.sabanciuniv.edu Abstract We investigate different representational granularities for sub-lexical representation in statistical machine translation work from English to Turkish.
We find that (i) representing both Turkish and English at the morpheme-level but with some selective morpheme-grouping on the Turkish side of the training data, (ii) augmenting the training data with ?sentences??comprising only the content words of the original training data to bias root word alignment, (iii) reranking the n-best morpheme-sequence outputs of the decoder with a word-based language model, and (iv) using model iteration all provide a non-trivial improvement over a fully word-based baseline.
Despite our very limited training data, we improve from 20.22 BLEU points for our simplest model to 25.08 BLEU points for an improvement of 4.86 points or 24% relative.
1 Introduction
Statistical machine translation (SMT) from Englishto-Turkish poses a number of difficulties.
Typologically English and Turkish are rather distant languages: while English has very limited morphology and rather fixed SVO constituent order, Turkish is an agglutinative language with a very rich and productive derivational and inflectional morphology, and a very flexible (but SOV dominant) constituent order.
Another issue of practical significance is the lack of large scale parallel text resources, with no substantial improvement expected in the near future.
In this paper, we investigate different representational granularities for sub-lexical representation of parallel data for English-to-Turkish phrase-based SMT and compare them with a word-based baseline.
We also employ two-levels of language models: the decoder uses a morpheme based LM while it is generating an n-best list.
The n-best lists are then rescored using a word-based LM.
The paper is structured as follows: We first briefly discuss issues in SMT and Turkish, and review related work.
We then outline how we exploit morphology, and present results from our baseline and morphologically segmented models, followed by some sample outputs.
We then describe discuss model iteration.
Finally, we present a comprehensive discussion of our approach and results, and briefly discuss word-repair ??fixing morphologicaly malformed words ??and offer a few ideas about the adaptation of BLEU to morphologically complex languages like Turkish.
2 Turkish
and SMT Our previous experience with SMT into Turkish (Durgar El-Kahlout and Oflazer, 2006) hinted that exploiting sub-lexical structure would be a fruitful avenue to pursue.
This was based on the observation that a Turkish word would have to align with a complete phrase on the English side, and that sometimes these phrases on the English side could be discontinuous.
Figure 1 shows a pair of English and Turkish sentences that are aligned at the word (top) and morpheme (bottom) levels.
At the morpheme level, we have split the Turkish words into their lexical morphemes while English words with overt morphemes have been stemmed, and such morphemes have been marked with a tag.
The productive morphology of Turkish implies potentially a very large vocabulary size.
Thus, sparseness which is more acute when very modest 25 Figure 1: Word and morpheme alignments for a pair of English-Turkish sentences parallel resources are available becomes an important issue.
However, Turkish employs about 30,000 root words and about 150 distinct suffixes, so when morphemes are used as the units in the parallel texts, the sparseness problem can be alleviated to some extent.
Our approach in this paper is to represent Turkish words with their morphological segmentation.
We use lexical morphemes instead of surface morphemes, as most surface distinctions are manifestations of word-internal phenomena such as vowel harmony, and morphotactics.
With lexical morpheme representation, we can abstract away such word-internal details and conflate statistics for seemingly different suffixes, as at this level of representation words that look very different on the surface, look very similar.1 For instance, although the words evinde ?in his house??and masasnda ?on his table??look quite different, the lexical morphemes except for the root are the same: ev+sH+ndA vs.
masa+sH+ndA. We should however note that although employing a morpheme based representations dramatically reduces the vocabulary size on the Turkish side, it also runs the risk of overloading distortion mechanisms to account for both word-internal morpheme sequencing and sentence level word ordering.
The segmentation of a word in general is not unique.
We first generate a representation that contains both the lexical segments and the morphological features encoded for all possible segmenta1This is in a sense very similar to the more general problem of lexical redundancy addressed by Talbot and Osborne (2006) but our approach does not require the more sophisticated solution there.
tions and interpretations of the word.
For the word emeli for instance, our morphological analyzer generates the following with lexical morphemes bracketed with (..): (em)em+Verb+Pos(+yAlH)?DB+Adverb+Since since (someone) sucked (something) (emel)emel+Noun+A3sg(+sH)+P3sg+Nom his/her ambition (emel)emel+Noun+A3sg+Pnon(+yH)+Acc ambition (as object of a transitive verb) These analyses are then disambiguated with a statistical disambiguator (Yuret and Ture, 2006) which operates on the morphological features.2 Finally, the morphological features are removed from each parse leaving the lexical morphemes.
Using morphology in SMT has been recently addressed by researchers translation from or into morphologically rich(er) languages.
Niessen and Ney (2004) have used morphological decomposition to improve alignment quality.
Yang and Kirchhoff (2006) use phrase-based backoff models to translate words that are unknown to the decoder, by morphologically decomposing the unknown source word.
They particularly apply their method to translating from Finnish ??another language with very similar structural characteristics to Turkish.
Corston-Oliver and Gamon (2004) normalize inflectional morphology by stemming the word for German-English word alignment.
Lee (2004) uses a morphologically analyzed and tagged parallel corpus for ArabicEnglish SMT.
Zolmann et al.(2006) also exploit morphology in Arabic-English SMT.
Popovic and Ney (2004) investigate improving translation qual2This disambiguator has about 94% accuracy.
26 ity from inflected languages by using stems, suffixes and part-of-speech tags.
Goldwater and McClosky (2005) use morphological analysis on Czech text to get improvements in Czech to English SMT.
Recently, Minkov et al.(2007) have used morphologicalpostprocessingontheoutputsideusingstructural information and information from the source side, to improve SMT quality.
3 Exploiting
Morphology Our parallel data consists mainly of documents in international relations and legal documents from sources such as the Turkish Ministry of Foreign Affairs, EU, etc.
We process these as follows: (i) We segment the words in our Turkish corpus into lexical morphemes whereby differences in the surface representations of morphemes due to word-internal phenomena are abstracted out to improve statistics during alignment.3 (ii) We tag the English side using TreeTagger (Schmid, 1994), which provides a lemma and a part-of-speech for each word.
We then remove any tags which do not imply an explicit morpheme or an exceptional form.
So for instance, if the word book gets tagged as +NN, we keep book in the text, but remove +NN.
For books tagged as +NNS or booking tagged as +VVG, we keep book and +NNS, and book and +VVG.
A word like went is replaced by go +VVD.4 (iii) From these morphologically segmented corpora, we also extract for each sentence, the sequence of roots for open class content words (nouns, adjectives, adverbs, and verbs).
For Turkish, this corresponds to removing all morphemes and any roots for closed classes.
For English, this corresponds to removing all words tagged as closed class words along with the tags such as +VVG above that signal a morpheme on an open class content word.
We use this to augment the training corpus and bias content word alignments, with the hope that such roots may get a chance to align without any additional ?noise??from morphemes and other function words.
From such processed data, we compile the data sets whose statistics are listed in Table 1.
One can notethatTurkishhasmanymoredistinctwordforms (about twice as many as English), but has much less 3So for example, the surface plural morphemes +ler and +lar get conflated to +lAr and their statistics are hence combined.
4Ideally, it would have been very desirable to actually do derivational morphological analysis on the English side, so that one could for example analyze accession into access plus a marker indicating nominalization.
Turkish Sent.
Words (UNK) Uniq.
Words Train 45,709 557,530 52,897 Train-Content 56,609 436,762 13,767 Tune 200 3,258 1,442 Test 649 10,334 (545) 4,355 English Train 45,709 723,399 26,747 Train-Content 56,609 403,162 19,791 Test 649 13,484 (231) 3,220 MorphUniq.
Morp./ Uniq.
Uniq. Turkish emes Morp.
Word Roots Suff.
Train 1,005,045 15,081 1.80 14,976 105 Tune 6,240 859 1.92 810 49 Test 18,713 2,297 1.81 2,220 77 Table 1: Statistics on Turkish and English training and test data, and Turkish morphological structure number of distinct content words than English.5 For language models in decoding and n-best list rescoring, we use, in addition to the training data, a monolingual Turkish text of about 100,000 sentences (in a segmented and disambiguated form).
A typical sentence pair in our data looks like the following, where we have highlighted the content root words with bold font, coindexed them toshow their alignments and bracketed the ?words?? that evaluation on test would consider.
??T: [kat1 +hl +ma] [ortaklk2 +sh +nhn] [uygula3 +hn +ma +sh] [,] [ortaklk4] [anlasma5 +sh] [cerceve6 +sh +nda] [izle7 +hn +yacak +dhr] [.] ??E: the implementation3 of the accession1 partnership2 will be monitor7 +vvn in the framework6 of the association4 agreement5. Note that when the morphemes/tags (starting with a +) are concatenated, we get the ?word-based?? version of the corpus, since surface words are directly recoverable from the concatenated representation.
We use this word-based representation also for word-based language models used for rescoring.
We employ the phrase-based SMT framework (Koehn et al., 2003), and use the Moses toolkit (Koehn et al., 2007), and the SRILM language modelling toolkit (Stolcke, 2002), and evaluate our decoded translations using the BLEU measure (Papineni et al., 2002), using a single reference translation.
5The training set in the first row of 1 was limited to sentences on the Turkish side which had at most 90 tokens (roots and bound morphemes) in total in order to comply with requirements of the GIZA++ alignment tool.
However when only the content words are included, we have more sentences to include since much less number of sentences violate the length restriction when morphemes/function word are removed.
27 Moses Dec.
Parms. BLEU BLEU-c Default 16.29 16.13 dl = -1, -weight-d = 0.1 20.16 19.77 Table 2: BLEU results for baseline experiments.
BLEU is for the model trained on the training set BLEU-Cisforthemodeltrainedontrainingsetaugmentedwith the content words.
3.1 The
Baseline System As a baseline system, we trained a model using default Moses parameters (e.g., maximum phrase length = 7), using the word-based training corpus.
The English test set was decoded with both default decoder parameters and with the distortion limit (-dl in Moses) set to unlimited (-1 in Moses) and distortion weight (-weight-d in Moses) set to a very low value of 0.1 to allow for long distance distortions.6 We also augmented the training set with the content word data and trained a second baseline model.
Minimum error rate training with the tune set did not provide any tangible improvements.7 Table 2 shows the BLEU results for baseline performance.
It can be seen that adding the content word training data actually hampers the baseline performance.
3.2 Fully
Morphologically Segmented Model We now trained a model using the fully morphologically segmented training corpus with and without content word parallel corpus augmentation.
For decoding, we used a 5-gram morpheme-based language model with the hope of capturing local morphotactic ordering constraints, and perhaps some sentence level ordering of words.8 We then decoded and obtained 1000-best lists.
The 1000-best sentences were then converted to ?words??(by concatenating the morphemes) and then rescored with a 4gram word-based language model with the hope of enforcing more distant word sequencing constraints.
For this, we followed the following procedure: We 6We arrived at this combination by experimenting with the decoder to avoid the almost monotonic translation we were getting with the default parameters.
7We ran MERT on the baseline model and the morphologically segmented models forcing -weight-d to range a very small around 0.1, but letting the other parameters range in their suggested ranges.
Even though the procedure came back claiming that it achieved a better BLEU score on the tune set, running the new model on the test set did not show any improvement at all.
This may have been due to the fact that the initial choice of -weight-d along with -dl set to 1 provides such a drastic improvement that perturbations in the other parameters do not have much impact.
8Given that on the average we have almost two bound morphemes per ?word??(for inflecting word classes), a morpheme 5-gram would cover about 2 ?words?? tried various linear combinations of the word-based language model and the translation model scores on the tune corpus, and used the combination that performed best to evaluate the test corpus.
We also experimented with both the default decoding parameters, and the modified parameters used in the baseline model decoding above.
The results in Table 3 indicate that the default decoding parameters used by the Moses decoder provide a very dismal results ??much below the baseline scores.
We can speculate that as the constituent orders of Turkish and English are very different, (root) words may have to be scrambled to rather long distances along with the translations of functions words and tags on the English side, to morphemes on the Turkish side.
Thus limiting maximum distortion and penalizing distortions with the default higher weight, result in these low BLEU results.
Allowing the decoder to consider longer range distortions and penalizing such distortions much less with the modified decoding parameters, seem to make an enormous difference in this case, providing close to almost 7 BLEU points improvement.9 We can also see that, contrary to the case with the baseline word-based experiments, using the additional content word corpus for training actually provides a tangible improvement (about 6.2% relative (w/o rescoring)), most likely due to slightly better alignments when content words are used.10 Rescoring the 1000-best sentence output with a 4gram word-based language model provides an additional 0.79 BLEU points (about 4% relative) ??from 20.22 to 21.01 ??for the model with the basic training set, and an additional 0.71 BLEU points (about 3% relative) ??from 21.47 to 22.18??for the model with the augmented training set.
The cumulative improvement is 1.96 BLEU points or about 9.4% relative.
3.3 Selectively
Segmented Model A systematic analysis of the alignment files produced by GIZA++ for a small subset of the training sentences showed that certain morphemes on the 9The ?morpheme??BLEU scores are much higher (34.43 on the test set) where we measure BLEU using decoded morphemes as tokens.
This is just indicative and but correlates with word-level BLEU which we report in Table 3, and can be used to gauge relative improvements to the models.
10We also constructed phrase tables only from the actual training set (w/o the content word section) after the alignment phase.
The resulting models fared slightly worse though we do not yet understand why.
28 Moses Dec.
Parms. BLEU BLEU-c Default 13.55 NA dl = -1, -weight-d = 0.1 20.22 21.47 dl = -1, -weight-d = 0.1 + word-level LM rescoring 21.01 22.18 Table 3: BLEU results for experiments with fully morphologically segmented training set Turkish side were almost consistently never aligned with anything on the English side: e.g., the compoundnounmarkermorphemeinTurkish(+sh)does not have a corresponding unit on the English side since English noun-noun compounds do not carry any overt markers.
Such markers were never aligned to anything or were aligned almost randomly to tokens on the English side.
Since we perform derivational morphological analysis on the Turkish side but not on the English side, we noted that most verbal nominalizations on the English side were just aligned to the verb roots on the Turkish side and the additional markers on the Turkish side indicating the nominalization and agreement markers etc., were mostly unaligned.
For just these cases, we selectively attached such morphemes (and in the case of verbs, the intervening morphemes) to the root, but otherwise kept other morphemes, especially any case morphemes, still by themselves, as they almost often align with prepositions on the English side quite accurately.11 This time, we trained a model on just the contentword augmented training corpus, with the better performing parameters for the decoder and again did 1000-best rescoring.12 The results for this experiment are shown in Table 4.
The resulting BLEU represents 2.43 points (11% relative) improvement overthebestfullysegmentedmodel(and4.39points 21.7% compared to the very initial morphologically segmented model).
This is a very encouraging result that indicates we should perhaps consider a much more detailed analysis of morpheme alignments to uncover additional morphemes with similar status.
Table 5 provides additional details on the BLEU 11It should be noted that what to selectively attach to the root should be considered on a per-language basis; if Turkish were to be aligned with a language with similar morphological markers, this perhaps would not have been needed.
Again one perhaps can use methods similar to those suggested by Talbot and Osborne (2006).
12Decoders for the fully-segmented model and selectively segmented model use different 5-gram language models, since the language model corpus should have the same selectively segmented units as those in the training set.
However, the wordlevel language models used in rescoring are the same.
Moses Dec.
Parms. BLEU-c dl = -1, -weight-d = 0.1 + word-level LM rescoring 22.18 (Full Segmentation (from Table 3)) dl = -1, -weight-d = 0.1 23.47 dl = -1, -weight-d = 0.1 + word-level LM rescoring 24.61 Table 4: BLEU results for experiments with selectivelysegmentedandcontent-wordaugmentedtraining set Range Sent.
BLEU-c 1 10 172 44.36 1 15 276 34.63 5 15 217 33.00 1 20 369 28.84 1 30 517 27.88 1 40 589 24.90 All 649 24.61 Table 5: BLEU Scores for different ranges of (source) sentence length for the result in Table 4 scores for this model, for different ranges of (English source) sentence length.
4 Sample
Rules and Translations We have extracted some additional statistics from the translations produced from English test set.
Of the 10,563 words in the decoded test set, a total of 957 words (9.0 %) were not seen in the training corpus.
However, interestingly, of these 957 words, 432 (45%) were actually morphologically well-formed (some as complex as having 4-5 morphemes)!
This indicates that the phrase-based translation model is able to synthesize novel complex words.13 In fact, some phrase table entries seem to capture morphologically marked subcategorization patterns.
An example is the phrase translation pair after examine +vvg ??
+acc incele+dhk +abl sonra which very much resembles a typical structural transfer rule one would find in a symbolic machine translation system PP(after examine +vvg NPeng) ??
PP(NPturk+acc incele+dhk +abl sonra) in that the accusative marker is tacked to the translation of the English NP.
Figure 2 shows how segments are translated to Turkish for a sample sentence.
Figure 3 shows the translations of three sentences from the test data 13Though whether such words are actually correct in their context is not necessarily clear.
29 cocuk [[ child ]] hak+lar+sh +nhn [[ +nns +pos right ]] koru+hn+ma+sh [[ protection ]] +nhn [[ of ]] tesvik et+hl+ma+sh [[ promote ]] +loc [[ +nns in ]] ab [[ eu ]] ve ulus+lararasi standart +lar [[ and international standard +nns ]] +dat uygun [[ line with ]] +dhr . [[ .]] Figure 2: Phrasal translations selected for a sample sentence Inp.: 1 . everyone?s right to life shall be protected by law . Trans.: 1 . herkesin yasama hakk kanunla korunur.
Lit.: everyone?s living right is protected with law . Ref.: 1 . herkesin yasam hakk yasann korumas altndadr . Lit.: everyone?s life right is under the protection of the law.
Inp.: promote protection of children?s rights in line with eu and international standards . Trans.: cocuk haklarnn korunmasnn ab ve uluslararas standartlara uygun sekilde gelistirilmesi.
Lit.: develop protection of children?s rights in accordance with eu and international standards . Ref.: ab ve uluslararas standartlar do?grultusunda cocuk haklarnn korunmasnn tesvik edilmesi.
Lit.: in line with eu and international standards promote/motivate protection of children?s rights . Inp.: as a key feature of such a strategy, an accession partnership will be drawn up on the basis of previous european council conclusions.
Trans.: bu stratejinin kilit unsuru bir katlm ortakl?g belgesi hazrlanacak kadarn temelinde, bir onceki avrupa konseyi sonuclardr . Lit.: as a key feature of this strategy, accession partnership document will be prepared ??? based are previous european council resolutions . Ref.: bu stratejinin kilit unsuru olarak, daha onceki ab zirve sonuclarna dayanlarak bir katlm ortakl?g olusturulacaktr.
Lit.: as a key feature of this strategy an accession partnership based on earlier eu summit resolutions will be formed . Figure 3: Some sample translations along with the literal paraphrases of the translation and the reference versions.
The first two are quite accurate and acceptable translations while the third clearly has missing and incorrect parts.
5 Model
Iteration We have also experimented with an iterative approach to use multiple models to see if further improvements are possible.
This is akin to post-editing (though definitely not akin to the much more sophisticated approach in described in Simard et al.(2007)). We proceeded as follows: We used the selective segmentation based model above and decoded our English training data ETrain and English test data ETest to obtain T1Train and T1Test reStep BLEU From Table 4 24.61 Iter.
1 24.77 Iter.
2 25.08 Table 6: BLEU results for two model iterations spectively.
We then trained the next model using T1Train and TTrain, to build a model that hopefully will improve upon the output of the previous model, T1Test, to bring it closer to TTest.
This model when applied toT1Train andT1Test produceT2Train and T2Test respectively.
We have not included the content word corpus in these experiments, as (i) our few very preliminary experiments indicated that using a morphemebased models in subsequent iterations would perform worse than word-based models, and (ii) that for word-basedmodels addingthecontent wordtraining data was not helpful as our baseline experiments indicated.
The models were tested by decoding the output of the previous model for original test data.
For word-based decoding in the additional iterations we used a 3-gram word-based language model but reranked the 1000-best outputs using a 4-gram language model.
Table 6 provides the BLEU results for these experiments corresponding to two additional model iterations.
The BLEU result for the second iteration, 25.08, represents a cumulative 4.86 points (24% relative) improvement over the initial fully morphologically segmented model using only the basic training set and no rescoring.
6 Discussion
Translation into Turkish seems to involve processes that are somewhat more complex than standard statistical translation models: sometimes words on the Turkish side are synthesized from the translations of two or more (SMT) phrases, and errors in any translated morpheme or its morphotactic position render the synthesized word incorrect, even though the rest of the word can be quite fine.
If we just extract the root words (not just for content words but all words) in the decoded test set and the reference set, and compute root word BLEU, we obtain 30.62, [64.6/35.7/23.4/16.3].
The unigram precision score shows that we are getting almost 65% of the root words correct.
However, the unigram precision score with full words is about 52% for our best model.
Thus we are missing about 13% of the words although we seem to be getting their roots 30 correct.
With a tool that we have developed, BLEU+ (Tantu?g et al., 2007), we have investigated such mismatches and have found that most of these are actually morphologically bogus, in that, although they have the root word right, the morphemes are either not the applicable ones or are in a morphotactically wrong position.
These can easily be identified with the morphological generator that we have.
In many cases, such morphologically bogus words are one morpheme edit distance away from the correct form in the reference file.
Another avenue that could be pursued is the use of skip language models (supported by the SRILM toolkit) so that the content word order could directly be used by the decoder.14 Atthispointitisveryhardtocomparehowourresults fare in the grand scheme of things, since there isnotmuchpriorresultsforEnglishtoTurkishSMT.
Koehn (2005) reports on translation from English to Finnish, another language that is morphologically as complex as Turkish, with the added complexity of compounding and stricter agreement between modifiers and head nouns.
A standard phrase-based system trained with 941,890 pairs of sentences (about 20 times the data that we have)! gives a BLEU score of 13.00.
However, in this study, nothing specific for Finnish was employed, and one can certainly employ techniques similar to presented here to improve upon this.
6.1 Word
Repair The fact that there are quite many erroneous words which are actually easy to fix suggests some ideas to improve unigram precision.
One can utilize a morpheme level ?spelling corrector??that operates on segmented representations, and corrects such forms to possible morphologically correct words in order to form a lattice which can again be rescored to select the contextually correct one.15 With the BLEU+ tool, we have done one experiment that shows that if we could recover all morphologically bogus words that are 1 and 2 morpheme edit distance from the correct form, the word BLEU score could rise to 29.86, [60.0/34.9/23.3/16.] and 30.48 [63.3/35.6/23.4/16.4] respectively.
Obviously, these are upper-bound oracle scores, as subsequent candidate generation and lattice rescoring could make er14This was suggested by one of the reviewers.
15It would however perhaps be much better if the decoder could be augmented with a filter that could be invoked at much earlier stages of sentence generation to check if certain generated segments violate hard-constraints (such as morphotactic constraints) regardless of what the statistics say.
rors, but nevertheless they are very close to the root word BLEU scores above.
Another path to pursue in repairing words is to identify morphologically correct words which are either OOVs in the language model or for which the language model has low confidence.
One can perhaps identify these using posterior probabilities (e.g., using techniques in Zens and Ney (2006)) and generate additional morphologically valid words that are ?close??and construct a lattice that can be rescored.
6.2 Some
Thoughts on BLEU BLEU is particularly harsh for Turkish and the morpheme based-approach, because of the all-or-none nature of token comparison, as discussed above.
There are also cases where words with different morphemes have very close morphosemantics, convey the relevant meaning and are almost interchangeable: ??gel+hyor (geliyor he is coming) vs.
gel+makta (gelmekte he is (in a state of) coming) are essentially the same.
On a scale of 0 to 1, one could rate these at about 0.95 in similarity.
??gel+yacak (gelecek he will come) vs.
gel+yacak+dhr (gelecektir he will come) in a sentence final position.
Such pairs could be rated perhaps at 0.90 in similarity.
??gel+dh (geldi he came (past tense)) vs.
gel+mhs (gelmis he came (hearsay past tense)).
These essentially mark past tense but differ in how the speaker relates to the event and could be rated at perhaps 0.70 similarity.
Note that using stems and their synonyms as used in METEOR (Banerjee and Lavie, 2005) could also be considered for word similarity.
Again using the BLEU+ tool and a slightly different formulation of token similarity in BLEU computation, we find that using morphological similarity our best score above, 25.08 BLEU increases to 25.14 BLEU, while using only root word synonymy and very close hypernymy from Wordnet, gives us 25.45 BLEU.
The combination of rules and Wordnet match gives 25.46 BLEU.
Note that these increases are much less than what can (potentially) be gained from solving the word-repair problem above.
7 Conclusions
We have presented results from our investigation into using different granularity of sub-lexical representations for English to Turkish SMT.
We have found that employing a language-pair specific representation somewhere in between using full wordforms and fully morphologically segmented representations and using content words as additional 31 data provide a significant boost in BLEU scores, in addition to contributions of word-level rescoring of 1000-best outputs and model iteration, to give a BLEU score of 25.08 points with very modest parallel text resources.
Detailed analysis of the errors point at a few directions such as word-repair, to improve word accuracy.
This also suggests perhaps hooking into the decoder, a mechanism for imposing hard constraints (such as morphotactic constraints) during decoding to avoid generating morphologically bogus words.
Another direction is to introduce exploitation of limited structures such as bracketed noun phrases before considering full-fledged syntactic structure.
Acknowledgements This work was supported by TUB?ITAK ??The Turkish National Science and Technology Foundation under project grant 105E020.
We thank the anonymous reviewer for some very useful comments and suggestions.
References Satanjeev Banerjee and Alon Lavie.
2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.
In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65??2, AnnArbor, Michigan, June.
Simon Corston-Oliver and Michael Gamon.
2004. Normalizing German and English inflectional morphology to improve statistical word alignment.
In Proceedings of AMTA, pages 48??7.
?Ilknur Durgar El-Kahlout and Kemal Oflazer.
2006. Initial explorations in English to Turkish statistical machine translation.
In Proceedings on the Workshop on Statistical Machine Translation, pages 7??4, New York City, June.
Sharon Goldwater and David McClosky.
2005. Improving statistical MT through morphological analysis.
In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 676??83, Vancouver, British Columbia, Canada, October.
Philipp Koehn, Franz J.
Och, and Daniel Marcu.
2003. Statistical phrase-based translation.
In Proceedings of HLT/NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst.
2007. Moses: Open source toolkit for statistical machine translation.
In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL??7) ??Companion Volume, June.
Philip Koehn.
2005. Europarl: A parallel corpus for statistical machine translation.
In MT Summit X, Phuket, Thailand.
Young-Suk Lee.
2004. Morphological analysis for statistical machine translation.
In Proceedings of HLT-NAACL 2004 Companion Volume, pages 57??0.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine translation.
In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL??7), Prague, Czech Republic, June.
Sonja Niessen and Hermann Ney.
2004. Statistical machine translation with scarce resources using morpho-syntatic information.
Computational Linguistics, 30(2):181??04.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002. Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, University of Pennsylvania.
Maja Popovic and Hermann Ney.
2004. Towards the use of word stems and suffixes for statistical machine translation.
n Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC), pages 1585??588 ay.
helmut Schmid.
1994. Probabilistic part-of-speech tagging using decision trees.
In Proceedings of International Conference on New Methods in Language Processing.
Michel Simard, Cyril Goutte, and Pierre Isabelle.
2007. Statistical phrase-based post-editing.
In Proceedings of NAACL, April.
Andreas Stolcke.
2002. SRILM ??an extensible language modeling toolkit.
In Proceedings of the Intl.
Conf. on Spoken Language Processing.
David Talbot and Miles Osborne.
2006. Modelling lexical redundancyformachinetranslation.
In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 969??76, Sydney, Australia, July.
Cuneyd Tantu?g, Kemal Oflazer, and ?Ilknur Durgar El-Kahlout.
2007. BLEU+: a tool for fine-grained BLEU computation.
in preparation.
Mei Yang and Katrin Kirchhoff.
2006. Phrase-based backoff modelsformachinetranslationofhighlyinflectedlanguages.
In Proceedings of EACL, pages 41??8.
Deniz Yuret and Ferhan Ture.
2006. Learning morphological disambiguation rules for Turkish.
In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 328??34, New York City, USA, June.
Richard Zens and Hermann Ney.
2006. N-gram posterior probabilities for statistical machine translation.
In Proceedings on the Workshop on Statistical Machine Translation, pages 72??7, New York City, June.
Association for Computational Linguistics.
Andreas Zollmann, Ashish Venugopal, and Stephan Vogel.
2006. Bridging the inflection morphology gap for Arabic statistical machine translation.
In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 201??04, New York City, USA, June .
Proceedings of the Second Workshop on Statistical Machine Translation, pages 33??9, Prague, June 2007.
c2007 Association for Computational Linguistics Can We Translate Letters?
David Vilar, Jan-T.
Peter and Hermann Ney Lehrstuhl fur Informatik 6 RWTH Aachen University D-52056 Aachen, Germany {vilar,peter,ney}@cs.rwth-aachen.de Abstract Current statistical machine translation systems handle the translation process as the transformation of a string of symbols into another string of symbols.
Normally the symbols dealt with are the words in different languages, sometimes with some additional information included, like morphological data.
In this work we try to push the approach to the limit, working not on the level of words, but treating both the source and target sentences as a string of letters.
We try to find out if a nearly unmodified state-of-the-art translation system is able to cope with the problem and whether it is capable to further generalize translation rules, for example at the level of word suffixes and translation of unseen words.
Experiments are carried out for the translation of Catalan to Spanish.
1 Introduction
Most current statistical machine translation systems handle the translation process as a ?blind??transformation of a sequence of symbols, which represent the words in a source language, to another sequence of symbols, which represent words in a target language.
This approach allows for a relative simplicity of the models, but also has drawbacks, as related word forms, like different verb tenses or pluralsingular word pairs, are treated as completely different entities.
Some efforts have been made e.g. to integrate more information about the words in the form of Part Of Speech tags (Popovic and Ney, 2005), using additional information about stems and suffixes (Popovic and Ney, 2004) or to reduce the morphological variability of the words (de Gispert, 2006).
State of the art decoders provide the ability of handling different word forms directly in what has been called factored translation models (Shen et al., 2006).
In this work, we try to go a step further and treat the words (and thus whole sentences) as sequences of letters, which have to be translated into a new sequence of letters.
We try to find out if the translation models can generalize and generate correct words out of the stream of letters.
For this approach to work we need to translate between two related languages, in which a correspondence between the structure of the words can be found.
For this experiment we chose a Catalan-Spanish corpus.
Catalan is a romance language spoken in the north-east of Spain and Andorra and is considered by some authors as a transitional language between the Iberian Romance languages (e.g.
Spanish) and Gallo-Romance languages (e.g.
French). A common origin and geographic proximity result in a similarity between Spanish and Catalan, albeit with enough differences to be considered different languages.
In particular, the sentence structure is quite similar in both languages and many times a nearly monotonical word to word correspondence between sentences can be found.
An example of Catalan and Spanish sentences is given in Figure 1.
The structure of the paper is as follows: In Section 2 we review the statistical approach to machine translation and consider how the usual techniques can be adapted to the letter translation task.
In Sec33 Catalan Perqu`e a mi m?agradaria estar-hi dues, una o dues setmanes, mes o menys, depenent del preu i cada hotel.
Spanish Porque a m me gustara quedarme dos, una o dos semanas, mas o menos, dependiendo del precio y cada hotel.
English Because I would like to be there two, one or two weeks, more or less, depending on the price of each hotel.
Catalan Si baixa aqu tenim una guia de la ciutat que li podem facilitar en la que surt informacio sobre els llocs mes interessants de la ciutat.
Spanish Si baja aqu tenemos una gua de la ciudad que le podemos facilitar en la que sale informacion sobre los sitios mas interesantes de la ciudad.
English If you come down here we have a guide book of the city that you can use, in there is information about the most interesting places in the city.
Figure 1: Example Spanish and Catalan sentences (the English translation is provided for clarity).
tion 3 we present the results of the letter-based translation and show how to use it for improving translation quality.
Although the interest of this work is more academical, in Section 4 we discuss possible practical applications for this approach.
The paper concludes in Section 5.
2 From
Words To Letters In the standard approach to statistical machine translation we are given a sentence (sequence of words) fJ1 = f1...fJ in a source language which is to be translated into a sentence ?eI1 = ?e1 ...
?eI in a target language.
Bayes decision rule states that we should choose the sentence which maximizes the posterior probability ?eI1 = argmax eI1 p(eI1|fJ1 ), (1) where the argmax operator denotes the search process.
In the original work (Brown et al., 1993) the posterior probability p(eI1|fJ1 ) is decomposed following a noisy-channel approach, but current stateof-the-art systems model the translation probability directly using a log-linear model(Och and Ney, 2002): p(eI1|fJ1 ) = exp parenleftBigsummationtextM m=1 mhm(e I1,fJ1 ) parenrightBig summationdisplay ?eI1 exp parenleftBigsummationtextM m=1 mhm(?eI1,fJ1 ) parenrightBig, (2) with hm different models, m scaling factors and the denominator a normalization factor that can be ignored in the maximization process.
The m are usually chosen by optimizing a performance measure over a development corpus using a numerical optimization algorithm like the downhill simplex algorithm (Press et al., 2002).
The most widely used models in the log linear combination are phrase-based models in sourceto-target and target-to-source directions, ibm1-like scores computed at phrase level, also in source-totarget and target-to-source directions, a target language model and different penalties, like phrase penalty and word penalty.
This same approach can be directly adapted to the letter-based translation framework.
In this case we are given a sequence of letters FJ1 corresponding to a source (word) string fJ1, which is to be translated into a sequence of letters EI1 corresponding to a string eI1 in a target language.
Note that in this case whitespaces are also part of the vocabulary and have to be generated as any other letter.
It is also important to remark that, without any further restrictions, the word sequences eI1 corresponding to a generated letter sequence EI1 are not even composed of actual words.
2.1 Details
of the Letter-Based System The vocabulary of the letter-based translation system is some orders of magnitude smaller than the vocabulary of a full word-based translation system, at least for European languages.
A typical vocabulary size for a letter-based system would be around 70, considering upperand lowercase letter, digits, 34 whitespace and punctuation marks, while the vocabulary size of a word-based system like the ones used in current evaluation campaigns is in the range of tens or hundreds of thousands words.
In a normal situation there are no unknowns when carrying out the actual translation of a given test corpus.
The situation can be very different if we consider languages like Chinese or Japanese.
This small vocabulary size allows us to deal with a larger context in the models used.
For the phrasebased models we extract all phrases that can be used when translating a given test corpus, without any restriction on the length of the source or the target part1.
For the language model we were able to use a high-order n-gram model.
In fact in our experiments a 16-gram letter-based language model is used, while state-of-the-art translation systems normally use 3 or 4-grams (word-based).
In order to better try to generate ?actual words?? in the letter-based system, a new model was added in the log-linear combination, namely the count of words generated that have been seen in the training corpus, normalized with the length of the input sentence.
Note however that this models enters as an additional feature function in the model and it does not constitute a restriction of the generalization capabilities the model can have in creating ?new words??
Somehow surprisingly, an additional word language model did not help.
While the vocabulary size is reduced, the average sentence length increases, as we consider each letter to be a unit by itself.
This has a negative impact in the running time of the actual implementation of the algorithms, specially for the alignment process.
In order to alleviate this, the alignment process was split into two passes.
In the first part, a word alignment was computed (using the GIZA++ toolkit (Och and Ney, 2003)).
Then the training sentences were split according to this alignment (in a similar way to the standard phrase extraction algorithm), so that the length of the source and target part is around thirty letters.
Then, a letter-based alignment is computed.
2.2 Efficiency
Issues Somewhat counter-intuitively, the reduced vocabulary size does not necessarily imply a reduced mem1For the word-based system this is also the case.
ory footprint, at least not without a dedicated program optimization.
As in a sensible implementations of nearly all natural language processing tools, the words are mapped to integers and handled as such.
A typical implementation of a phrase table is then a prefix-tree, which is accessed through these word indices.
In the case of the letter-based translation, the phrases extracted are much larger than the word-based ones, in terms of elements.
Thus the total size of the phrase table increases.
The size of the search graph is also larger for the letter-based system.
In most current systems the generation algorithm is a beam search algorithm with a ?source synchronous??search organization.
As the length of the source sentence is dramatically increased when considering letters instead of words, the total size of the search graph is also increased, as is the running time of the translation process.
The memory usage for the letter system can actually be optimized, in the sense that the letters can act as ?indices??themselves for addressing the phrase table and the auxiliary mapping structure is not necessary any more.
Furthermore the characters can be stored in only one byte, which provides a significant memory gain over the word based system where normally four bytes are used for storing the indices.
These gains however are not expected to counteract the other issues presented in this section.
3 Experimental
Results The corpus used for our experiment was built in the framework of the LC-STAR project (Conejero et al., 2003).
It consists of spontaneous dialogues in Spanish, Catalan and English2 in the tourism and travelling domain.
The test corpus (and an additional development corpus for parameter optimization) was randomly extracted, the rest of the sentences were used as training data.
Statistics for the corpus can be seen in Table 1.
Details of the translation system used can be found in (Mauser et al., 2006).
The results of the word-based and letter-based approaches can be seen in Table 2 (rows with label ?Full Corpus??.
The high BLEU scores (up to nearly 80%) denote that the quality of the translation is quite good for both systems.
The word2The English part of the corpus was not used in our experiments.
35 Spanish Catalan Training Sentences 40 574 Running Words 482 290 485 514 Vocabulary 14 327 12 772 Singletons 6 743 5 930 Test Sentences 972 Running Words 12 771 12 973 OOVs [%] 1.4 1.3 Table 1: Corpus Statistics based system outperforms the letter-based one, as expected, but the letter-based system also achieves quite a good translation quality.
Example translations for both systems can be found in Figure 2.
It can be observed that most of the words generated by the letter based system are correct words, and in many cases the ?false??words that the system generates are very close to actual words (e.g.
?elos??instead of ?los??in the second example of Figure 2).
We also investigated the generalization capabilities of both systems under scarce training data conditions.
It was expected that the greater flexibility of the letter-based system would provide an advantage of the approach when compared to the wordbased approach.
We randomly selected subsets of the training corpus of different sizes ranging from 1 000 sentences to 40 000 (i.e.
the full corpus) and computed the translation quality on the same test corpus as before.
Contrary to our hopes, however, the difference in BLEU score between the wordbased and the letter-based system remained fairly constant, as can be seen in Figure 3, and Table 2 for representative training corpus sizes.
Nevertheless, the second example in Figure 2 provides an interesting insight into one of the possible practical applications of this approach.
In the example translation of the word-based system, the word ?centreamericans??was not known to the system (and has been explicitly marked as unknown in Figure 2).
The letter-based system, however, was able to correctly learn the translation from ?centre-?? to ?centro-??and that the ending ??ans??in Catalan is often translated as ??anos??in Spanish, and thus a correct translation has been found.
We thus chose to combine both systems, the word-based system doing most of the translation work, but using the letterbased system for the translation of unknown words.
The results of this combined approach can be found in Table 2 under the label ?Combined System??
The combination of both approaches leads to a 0.5% increase in BLEU using the full corpus as training material.
This increase is not very big, but is it over a quite strong baseline and the percentage of out-ofvocabulary words in this corpus is around 1% of the total words (see Table 1).
When the corpus size is reduced, the gain in BLEU score becomes more important, and for the small corpus size of 1 000 sentences the gain is 2.5% BLEU.
Table 2 and Figure 3 show more details.
4 Practical
Applications The approach described in this paper is mainly of academical interest.
We have shown that letterbased translation is in principle possible between similar languages, in our case between Catalan and Spanish, but can be applied to other closely related language pairs like Spanish and Portuguese or German and Dutch.
The approach can be interesting for languages where very few parallel training data is available.
The idea of translating unknown words in a letterbased fashion can also have applications to state-ofthe-art translation systems.
Nowadays most automatic translation projects and evaluations deal with translation from Chinese or Arabic to English.
For these language pairs the translation of named entities poses an additional problem, as many times they were not previously seen in the training data and they are actually one of the most informative words in the texts.
The ?translation??of these entities is in most cases actually a (more or less phonetic) transliteration, see for example (Al-Onaizan and Knight, 2002).
Using the proposed approach for the translation of these words can provide a tighter integration in the translation process and hopefully increase the translation performance, in the same way as it helps for the case of the Catalan-Spanish translation for unseen words.
Somewhat related to this problem, we can find an additional application in the field of speech recognition.
The task of grapheme-to-phoneme conversion aims at increasing the vocabulary an ASR system can recognize, without the need for additional 36 BLEU WER PER Word-Based System Full Corpus 78.9 11.4 10.6 10k 74.0 13.9 13.2 1k 60.0 21.3 20.1 Letter-Based System Full Corpus 72.9 14.7 13.5 10k 69.8 16.5 15.1 1k 55.8 24.3 22.8 Combined System Full Corpus 79.4 11.2 10.4 10k 75.2 13.4 12.6 1k 62.5 20.2 19.0 Table 2: Translation results for selected corpus sizes.
All measures are percentages.
Source (Cat) Be, en principi seria per a les vacances de Setmana Santa que son les seguents que tenim ara, entrant a juliol.
Word-Based Bueno, en principio sera para las vacaciones de Semana Santa que son las siguientes que tenemos ahora, entrando en julio.
Letter-Based Bueno, en principio sera para las vacaciones de Semana Santa que son las siguientes que tenemos ahora, entrando bamos en julio . Reference Bueno, en principio sera para las vacaciones de Semana Santa que son las siguientes que tenemos ahora, entrando julio.
Source (Cat) Jo li recomanaria per exemple que intentes apropar-se a algun pas ve tambe com poden ser els pasos centreamericans, una mica mes al nord Panama.
Word-Based Yo le recomendara por ejemplo que intentase acercarse a algun pas vecino tambien como pueden ser los pases UNKNOWN centreamericans, un poco mas al norte Panama.
Letter-Based Yo le recomendara por ejemplo que intentaseo acercarse a algun pas ve tambien como pueden ser elos pases centroamericanos, un poco mas al norte Panama.
Combined Yo le recomendara por ejemplo que intentase acercarse a algun pas vecino tambien como pueden ser los pases centroamericanos, un poco mas al norte Panama.
Reference Yo le recomendara por ejemplo que intentase acercarse a algun pas vecino tambien como pueden ser los pases centroamericanos, un poco mas al norte Panama.
Figure 2: Example translations of the different approaches.
For the word-based system an unknown word has been explicitly marked.
37 50 55 60 65 70 75 80 0 5000 10000 15000 20000 25000 30000 35000 40000 Word-Based Letter-Based Combined Figure 3: Translation quality depending of the corpus size.
acoustic data.
The problem can be formulated as a translation from graphemes (?letters?? to a sequence of graphones (?pronunciations??, see for example (Bisani and Ney, 2002).
The proposed letter-based approach can also be adapted to this task.
Lastly, a combination of both, word-based and letter-based models, working in parallel and perhaps taking into account additional information like base forms, can be helpful when translating from or into rich inflexional languages, like for example Spanish.
5 Conclusions
We have investigated the possibility of building a letter-based system for translation between related languages.
The performance of the approach is quite acceptable, although, as expected, the quality of the word-based approach is superior.
The combination of both techniques, however, allows the system to translate words not seen in the training corpus and thus increase the translation quality.
The gain is specially important when the training material is scarce.
While the experiments carried out in this work are more interesting from an academical point of view, several practical applications has been discussed and will be the object of future work.
Acknowledgements This work was partly funded by the Deutsche Forschungsgemeinschaft (DFG) under the project ?Statistische Textubersetzung??(NE 572/5-3).
References Yaser Al-Onaizan and Kevin Knight.
2002. Machine transliteration of names in arabic text.
In Proceedings of the ACL-02 workshop on Computational approaches to semitic languages, pages 1??3, Morristown, NJ, USA.
Association for Computational Linguistics.
Max Bisani and Hermann Ney.
2002. Investigations on joint-multigram models for grapheme-to-phoneme conversion.
In Proceedings of the 7th International Conference on Spoken Language Processing, volume 1, pages 105??08, Denver, CO, September.
Peter F.
Brown, Stephen A.
Della Pietra, Vincent J.
Della Pietra, and Robert L.
Mercer. 1993.
The mathematics of statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263??11, June.
D. Conejero, J.
Gimnez, V.
Arranz, A.
Bonafonte, N.
Pascual, N.
Castell, and A.
Moreno. 2003.
Lexica and corpora for speech-to-speech translation: A trilingual approach.
In European Conf.
on Speech Communication and Technology, pages 1593??596, Geneva, Switzerland, September.
Adri`a de Gispert.
2006. Introducing Linguistic Knowledge into Statistical Machine Translation.
Ph.D. thesis, Universitat Polit`ecnica de Catalunya, Barcelona, October.
Arne Mauser, Richard Zens, Evgeny Matusov, Sa?sa Hasan, and Hermann Ney.
2006. The RWTH Statistical Machine Translation System for the IWSLT 2006 Evaluation.
In Proc.
of the International Workshop on Spoken Language Translation, pages 103??10, Kyoto, Japan.
Franz Josef Och and Hermann Ney.
2002. Discriminative training and maximum entropy models for statistical machine translation.
In Proc.
of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 295??02, Philadelphia, PA, July.
Franz Josef Och and Hermann Ney.
2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19??1, March.
Maja Popovic and Hermann Ney.
2004. Towards the Use of Word Stems and Suffixes for Statistical Machine Translation.
In 4th International Conference on Language Resources and Evaluation (LREC), pages 1585??588, Lisbon, Portugal, May.
Maja Popovic and Hermann Ney.
2005. Exploiting Phrasal Lexica and Additional Morpho-syntactic Language Resources for Statistical Machine Translation with Scarce Training Data.
In 10th Annual Conference of the European Association for Machine Translation (EAMT), pages 212??18, Budapest, Hungary, May.
William H.
Press, Saul A.
Teukolsky, William T.
Vetterling, and Brian P.
Flannery. 2002.
Numerical Recipes in C++.
Cambridge University Press, Cambridge, UK.
Wade Shen, Richard Zens, Nicola Bertoldi, and Marcello Federico.
2006. The JHU Workshop 2006 IWSLT System.
In Proc.
of the International Workshop on Spoken Language Translation, pages 59??3, Kyoto, Japan .
Proceedings of the Second Workshop on Statistical Machine Translation, pages 40??7, Prague, June 2007.
c2007 Association for Computational Linguistics A Dependency Treelet String Correspondence Model for Statistical Machine Translation Deyi Xiong, Qun Liu and Shouxun Lin Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences Beijing, China, 100080 {dyxiong, liuqun, sxlin}@ict.ac.cn Abstract This paper describes a novel model using dependency structures on the source side for syntax-based statistical machine translation: Dependency Treelet String Correspondence Model (DTSC).
The DTSC model maps source dependency structures to target strings.
In this model translation pairs of source treelets and target strings with their word alignments are learned automatically from the parsed and aligned corpus.
The DTSC model allows source treelets and target strings with variables so that the model can generalize to handle dependency structures with the same head word but with different modifiers and arguments.
Additionally, target strings can be also discontinuous by using gaps which are corresponding to the uncovered nodes which are not included in the source treelets.
A chart-style decoding algorithm with two basic operations?? substituting and attaching?is designed for the DTSC model.
We argue that the DTSC model proposed here is capable of lexicalization, generalization, and handling discontinuous phrases which are very desirable for machine translation.
We finally evaluate our current implementation of a simplified version of DTSC for statistical machine translation.
1 Introduction
Over the last several years, various statistical syntaxbased models were proposed to extend traditional word/phrase based models in statistical machine translation (SMT) (Lin, 2004; Chiang, 2005; Ding et al., 2005; Quirk et al., 2005; Marcu et al., 2006; Liu et al., 2006).
It is believed that these models can improve the quality of SMT significantly.
Compared with phrase-based models, syntax-based models lead to better reordering and higher flexibility by introducing hierarchical structures and variables which make syntax-based models capable of hierarchical reordering and generalization.
Due to these advantages, syntax-based approaches are becoming an active area of research in machine translation.
In this paper, we propose a novel model based on dependency structures: Dependency Treelet String Correspondence Model (DTSC).
The DTSC model maps source dependency structures to target strings.
It just needs a source language parser.
In contrast to the work by Lin (2004) and by Quirk et al.(2005), the DTSC model does not need to generate target language dependency structures using source structures and word alignments.
On the source side, we extract treelets which are any connected subgraphs and consistent with word alignments.
While on the target side, we allow the aligned target sequences to be generalized and discontinuous by introducing variables and gaps.
The variables on the target side are aligned to the corresponding variables of treelets, while gaps between words or variables are corresponding to the uncovered nodes which are not included by treelets.
To complete the translation process, we design two basic operations for the decoding: substituting and attaching.
Substituting is used to replace variable nodes which have been already translated, while attaching is used to attach uncov40 ered nodes to treelets.
In the remainder of the paper, we first define dependency treelet string correspondence in section 2 and describe an algorithm for extracting DTSCs from the parsed and word-aligned corpus in section 3.
Then we build our model based on DTSC in section 4.
The decoding algorithm and related pruning strategies are introduced in section 5.
We also specify the strategy to integrate phrases into our model in section 6.
In section 7 we evaluate our current implementation of a simplified version of DTSC for statistical machine translation.
And finally, we discuss related work and conclude.
2 Dependency
Treelet String Correspondence A dependency treelet string correspondence pi is a triple < D,S,A > which describes a translation pair < D,S > and their alignment A, where D is the dependency treelet on the source side and S is the translation string on the target side.
< D,S > must be consistent with the word alignment M of the corresponding sentence pair ?(i,j) ??M,i ??D ??j ??S A treelet is defined to be any connected subgraph, which is similar to the definition in (Quirk et al., 2005).
Treelet is more representatively flexible than subtree which is widely used in models based on phrase structures (Marcu et al., 2006; Liu et al., 2006).
The most important distinction between the treelet in (Quirk et al., 2005) and ours is that we allow variables at positions of subnodes.
In our definition, the root node must be lexicalized but the subnodes can be replaced with a wild card.
The target counterpart of a wildcard node in S is also replaced with a wild card.
The wildcards introduced in this way generalize DTSC to match dependency structures with the same head word but with different modifiers or arguments.
Another unique feature of our DTSC is that we allow target strings with gaps between words or wildcards.
Since source treelets may not cover all subnodes, the uncovered subnodes will generate a gap as its counterpart on the target side.
A sequence of continuous gaps will be merged to be one gap and gaps at the beginning and the end of S will be removed automatically.
?pd101d101d101d101d101d101d101 d31 d31 d31 d31?Td101d101d101d101d101d101d101 d115 d115 d115 d115 d115 d115 d115 d115NQ d31d31 d31d101d101d101d101d101d101d101 d31d31 d31 ??d93d93d93d93d93d93d93d93d93d93 the conference cooperation of the ??
 d101d101d101d101d101d101d101 d98d98d98d98d98d98d98d98d98d98d98d98d98 d115 d115 d115 d115 d115 d115 d115 d115?? 1 d119 d119 d119 d119 d89d89d89d89d89d89d89 d83d83d83 d83d83d83 d83  ?? d93d93d93d93d93d93d93d93d93d93 ?? keep a G with the ??
Figure 1: DTSC examples.
Note that ??represents variable and G represents gap.
Gap can be considered as a special kind of variable whose counterpart on the source side is not present.
This makes the model more flexible to match more partial dependency structures on the source side.
If only variables can be used, the model has to match subtrees rather than treelets on the source side.
Furthermore, the positions of variables on the target side are fixed so that some reorderings related with them can be recorded in DTSC.
The positions of gaps on the target side, however, are not fixed until decoding.
The presence of one gap and its position can not be finalized until attaching operation is performed.
The introduction of gaps and the related attaching operation in decoding is the most important distinction between our model and the previous syntax-based models.
Figure 1 shows several different DTSCs automatically extracted from our training corpus.
The top left DTSC is totally lexicalized, while the top right DTSC has one variable and the bottom has two variables and one gap.
In the bottom DTSC, note that the node  which is aligned to the gap G of the target string is an uncovered node and therefore not included in the treelet actually.
Here we just want to show there is an uncovered node aligned with the gap G.
Each node at the source treelet has three attributes 1.
The head word 2.
The category, i.e. the part of speech of the head word 3.
The node order which specifies the local order of the current node relative to its parent node.
41 4/VV d101d101d101d101d101d101d101d98d98d98d98d98d98d98d98d98d98d98d98d98d98 d93d93d93d93d93d93d93d93d93d93d93d93d93d93d93d93d93d93d93d93d93 d31 d31 d31 d31 d31 ??VV d31 d31 d31 d68d68 d68d68 d68_/P d89d89d89d89d89d89d89 d88d88d88d88d88 d88d88d88d88d88 d88d88d88d88 ??NN d101d101d101d101d101d101d101 d122 d122 d122 d122 d122 ? /NR d92d92d92d92d92d92d92d92d92d92d92 d92d92d92??NN d106 d106 d106 d106d31d31 go on providingfinancial aid to Palestine 1 2 3 4 5 6 7 Figure 2: An example dependency tree and its alignments Note that the node order is defined at the context of the extracted treelets but not the context of the original tree.
For example, the attributes for the node? in the bottom DTSC of Figure 1 are {?
P, -1}.
For two treelets, if and only if their structures are identical and each corresponding nodes share the same attributes, we say they are matched.
3 Extracting
DTSCs To extract DTSCs from the training corpus, firstly the corpus must be parsed on the source side and aligned at the word level.
The source structures produced by the parser are unlabelled, ordered dependency trees with each word annotated with a part-ofspeech.
Figure 2 shows an example of dependency tree really used in our extractor.
When the source language dependency trees and word alignments between source and target languages are obtained, the DTSC extraction algorithm runs in two phases along the dependency trees and alignments.
In the first step, the extractor annotates each node with specific attributes defined in section 3.1.
These attributes are used in the second step which extracts all possible DTSCs rooted at each node recursively.
3.1 Node
annotation For each source dependency node n, we define three attributes: word span, node span and crossed.
Word span is defined to be the target word sequence aligned with the head word of n, while node span is defined to be the closure of the union of node spans of all subnodes of n and its word span.
These two attributes are similar to those introduced by Lin (Lin, 2004).
The third attribute crossed is an indicator that has binary values.
If the node span of n overlaps the word span of its parent node or the node span of its siblings, the crossed indicator of n is 1 and n is therefore a crossed node, otherwise the crossed indicator is 0 and n is a non-crossed node.
Only non-crossed nodes can generate DTSCs because the target word sequence aligned with the whole subtree rooted at it does not overlap any other sequences and therefore can be extracted independently.
For the dependency tree and its alignments shown in Figure 2, only the node?is a crossed node since its node span ([4,5]) overlaps the word span ([5,5]) of its parent node?? 3.2 DTSCs extraction The DTSC extraction algorithm (shown in Figure 3) runs recursively.
For each non-crossed node, the algorithm generates all possible DTSCs rooted at it by combining DTSCs from some subsets of its direct subnodes.
If one subnode n selected in the combination is a crossed node, all other nodes whose word/node spans overlap the node span of n must be also selected in this combination.
This kind of combination is defined to be consistent with the word alignment because the DTSC generated by this combination is consistent with the word alignment.
All DTSCs generated in this way will be returned to the last call and outputted.
For each crossed node, the algorithm generates pseudo DTSCs1 using DTSCs from all of its subnodes.
These pseudo DTSCs will be returned to the last call but not outputted.
During the combination of DTSCs from subnodes into larger DTSCs, there are two major tasks.
One task is to generate the treelet using treelets from subnodes and the current node.
This is a basic tree generation operation.
It is worth mentioning that some non-crossed nodes are to be replaced with a wild card so the algorithm can learn generalized DTSCs described in section 2.
Currently, we replace any non-crossed node alone or together with their sibling non-crossed nodes.
The second task is to combine target strings.
The word sequences aligned with uncovered nodes will be replaced with a gap.
The word sequences aligned with wildcard nodes will be replaced with a wild card.
If a non-crossed node n has m direct subnodes, all 2m combinations will be considered.
This will generate a very large number of DTSCs, which is 1Some words in the target string are aligned with nodes which are not included in the source treelet.
42 DTSCExtractor(Dnode n) Rfractur := ??(DTSC container of n) for each subnode k of n do R := DTSCExtractor(k) L := LuniontextR end for if n.crossed!
= 1 and there are no subnodes whose span overlaps the word span of n then Create a DTSC pi =< D,S,A > where the dependency treelet D only contains the node n (not including any children of it) output pi for each combination c of n?s subnodes do if c is consistent with the word alignment then Generate all DTSCs R by combining DTSCs (L) from the selected subnodes with the current node n Rfractur := RfracturuniontextR end if end for output Rfractur return Rfractur else if n.crossed == 1 then Create pseudo DTSCs P by combining all DTSCs from n?s all subnodes.
Rfractur := RfracturuniontextP return Rfractur end if Figure 3: DTSC Extraction Algorithm.
undesirable for training and decoding.
Therefore we filter DTSCs according to the following restrictions 1.
If the number of direct subnodes of node n is larger than 6, we only consider combining one single subnode with n each time because in this case reorderings of subnodes are always monotone.
2. On the source side, the number of direct subnodes of each node is limited to be no greater than ary-limit; the height of treelet D is limited to be no greater than depth-limit.
3. On the target side, the length of S (including gaps and variables) is limited to be no greater than len-limit; the number of gaps in S is limited to be no greater than gap-limit.
4. During DTSC combination, the DTSCs from each subnode are sorted by size (in descending order).
Only the top comb-limit DTSCs will be selected to generate larger DTSCs.
As an example, for the dependency tree and its alignments in Figure 2, all DTSCs extracted by the Treelet String (??VV/0) go on (? /NR/0) Palestine (_/P/0) to (_/P/0 (? /NR/1)) to Palestine (_/P/0 (??1)) to ??
(??NN/0 (??NN/-1)) financial aid (4/VV/0) providing (4/VV/0 (??1)) providing ??
(4/VV/0 (??-1)) providing G??
(4/VV/0 (??VV/-1)) go on providing (4/VV/0 (??-1)) ??providing (4/VV/0 (??/-1) (??/1)) providing ??
?? (4/VV/0 (??/-1 ) (??/1)) ?? providing ??
Table 1: Examples of DTSCs extracted from Figure 2.
Alignments are not shown here because they are self-evident.
algorithm with parameters { ary-limit = 2, depthlimit = 2, len-limit = 3, gap-limit = 1, comb-limit = 20 } are shown in the table 1.
4 The
Model Given an input dependency tree, the decoder generates translations for each dependency node in bottom-up order.
For each node, our algorithm will search all matched DTSCs automatically learned from the training corpus by the way mentioned in section 3.
When the root node is traversed, the translating is finished.
This complicated procedure involves a large number of sequences of applications of DTSC rules.
Each sequence of applications of DTSC rules can derive a translation.
We define a derivation  as a sequence of applications of DTSC rules, and let c() and e() be the source dependency tree and the target yield of , respectively.
The score of  is defined to be the product of the score of the DTSC rules used in the translation, and timed by other feature functions: () = productdisplay i (i)plm(e)lm exp(?apA()) (1) where (i) is the score of the ith application of DTSC rules, plm(e) is the language model score, and exp(?apA()) is the attachment penalty, where A() calculates the total number of attachments occurring in the derivation .
The attachment penalty gives some control over the selection of DTSC rules which makes the model prefer rules 43 with more nodes covered and therefore less attaching operations involved.
For the score of DTSC rule pi, we define it as follows: (pi) = productdisplay j fj(pi)j (2) where the fj are feature functions defined on DTSC rules.
Currently, we used features proved to be effective in phrase-based SMT, which are: 1.
The translation probability p(D|S).
2. The inverse translation probability p(S|D).
3. The lexical translation probability plex(D|S) which is computed over the words that occur on the source and target sides of a DTSC rule by the IBM model 1.
4. The inverse lexical translation probability plex(S|D) which is computed over the words that occur on the source and target sides of a DTSC rule by the IBM model 1.
5. The word penalty wp.
6. The DTSC penalty dp which allows the model to favor longer or shorter derivations.
It is worth mentioning how to integrate the Ngram language mode into our DTSC model.
During decoding, we have to encounter many partial translations with gaps and variables.
For these translations, firstly we only calculate the language model scores for word sequences in the translations.
Later we update the scores when gaps are removed or specified by attachments or variables are substituted.
Each updating involves merging two neighbor substrings sl (left) and sr (right) into one bigger string s.
Let the sequence of n ??1 (n is the order of N-gram language model used) rightmost words of sl be srl and the sequence of n?? leftmost words of sr be slr.
we have: LM(s) = LM(sl)+LM(sr)+LM(srlslr) ?LM(srl)?LM(slr) (3) where LM is the logarithm of the language model probability.
We only need to compute the increment of the language model score: triangleLM = LM(srlslr)?LM(srl)?LM(slr) (4) for each node n of the input tree T, in bottom-up order do Get all matched DTSCs rooted at n for each matched DTSC pi do for each wildcard node n??in pi do Substitute the corresponding wildcard on the target side with translations from the stack of n?? end for for each uncovered node n@ by pi do Attach the translations from the stack of n@ to the target side at the attaching point end for end for end for Figure 4: Chart-style Decoding Algorithm for the DTSC Model.
Melamed (2004) also used a similar way to integrate the language model.
5 Decoding
Our decoding algorithm is similar to the bottom-up chart parsing.
The distinction is that the input is a tree rather than a string and therefore the chart is indexed by nodes of the tree rather than spans of the string.
Also, several other tree-based decoding algorithms introduced by Eisner (2003), Quirk et al.(2005) and Liu et al.(2006) can be classified as the chart-style parsing algorithm too.
Our decoding algorithm is shown in Figure 4.
Given an input dependency tree, firstly we generate the bottom-up order by postorder transversal.
This order guarantees that any subnodes of node n have been translated before node n is done.
For each node n in the bottom-up order, all matched DTSCs rooted at n are found, and a stack is also built for it to store the candidate translations.
A DTSC pi is said to match the input dependency subtree T rooted at n if and only if there is a treelet rooted at n that matches 2 the treelet of pi on the source side.
For each matched DTSC pi, two operations will be performed on it.
The first one is substituting which replaces a wildcard node with the corresponding translated node.
The second one is attaching which attaches an uncovered node to pi.
The two operations are shown in Figure 5.
For each wildcard node n?? translations from the stack of it will be selected to replace the corresponding wildcard on the 2The words, categories and orders of each corresponding nodes are matched.
Please refer to the definition of matched in section 2.
44 (a) A d101d101d101d101d101d101d101 d89d89d89d89d89d89d89B d101d101d101d101d101d101d101 ??+ D C  De ?e Ae Be Ce Substitute ??
(b) A d101d101d101d101d101d101d101 d89d89d89d89d89d89d89B d101d101d101d101d101d101d101 D + E C  Ee De Ae Be Ce Attach ??
(c) A d101d101d101d101d101d101d101 d89d89d89d89d89d89d89B d101d101d101d101d101d101d101 d89d89d89d89d89d89d89 D C E De Ae Be Ee Ce Figure 5: Substituting and attaching operations for decoding.
Xe is the translation of X.
Node that ??is a wildcard node to be substituted and node  is an uncovered node to be attached.
target side and the scores of new translations will be calculated according to our model.
For each uncovered node n@, firstly we determine where translations from the stack of n@ should be attached on the target side.
There are several different mechanisms for choosing attaching points.
Currently, we implement a heuristic way: on the source side, we find the node n@p which is the nearest neighbor of n@ from its parent and sibling nodes, then the attaching point is the left/right of the counterpart of n@p on the target side according to their relative order.
As an example, see the uncovered node  in Figure 5.
The nearest node to it is node B.
Since node  is at the right of node B, the attaching point is the right of Be.
One can search all possible points using an ordering model.
And this ordering model can also use information from gaps on the target side.
We believe this ordering model can improve the performance and let it be one of directions for our future research.
Note that the gaps on the target side are not necessarily attaching points in our current attaching mechanism.
If they are not attaching point, they will be removed automatically.
The search space of the decoding algorithm is very large, therefore some pruning techniques have to be used.
To speed up the decoder, the following pruning strategies are adopted.
1. Stack pruning.
We use three pruning ways.
The first one is recombination which converts the search to dynamic programming.
When two translations in the same stack have the same w leftmost/rightmost words, where w depends on the order of the language model, they will be recombined by discarding the translation with lower score.
The second one is the threshold pruning which discards translations that have a score worse than stack-threshold times the best score in the same stack.
The last one is the histogram pruning which only keeps the top stack-limit best translations for each stack.
2. Node pruning.
For each node, we only keep the top node-limit matched DTSCs rooted at that node, as ranked by the size of source treelets.
3. Operation pruning.
For each operation, substituting and attaching, the decoding will generate a large number of partial translations3 for the current node.
We only keep the top operation-limit partial translations each time according to their scores.
6 Integrating
Phrases Although syntax-based models are good at dealing with hierarchical reordering, but at the local level, translating idioms and similar complicated expressions can be a problem.
However, phrase-based models are good at dealing with these translations.
Therefore, integrating phrases into the syntax-based models can improve the performance (Marcu et al., 2006; Liu et al., 2006).
Since our DTSC model is based on dependency structures and lexicalized naturally, DTSCs are more similar to phrases than other translation units based on phrase structures.
This means that phrases will be easier to be integrated into our model.
The way to integrate phrases is quite straightforward: if there is a treelet rooted at the current node, 3There are wildcard nodes or uncovered nodes to be handled.
45 of which the word sequence is continuous and identical to the source of some phrase, then a phrasestyle DTSC will be generated which uses the target string of the phrase as its own target.
The procedure is finished during decoding.
In our experiments, integrating phrases improves the performance greatly.
7 Current
Implementation To test our idea, we implemented the dependency treelet string correspondence model in a ChineseEnglish machine translation system.
The current implementation in this system is actually a simplified version of the DTSC model introduced above.
In this version, we used a simple heuristic way for the operation of attaching rather than a sophisticated statistical model which can learn ordering information from the training corpus.
Since dependency structures are morences.
We selected 580 short sentences of length at most 50 characters from the 2002 NIST MT Evaluation test set as our development corpus and used it to tune s by maximizing the BLEU score (Och, 2003), and used the 2005 NIST MT Evaluation test set as our test corpus.
From the training corpus, we learned 2, 729, 964 distinct DTSCs with the configuration { arylimit = 4, depth-limit = 4, len-limit = 15, gap-limit = 2, comb-limit = 20 }.
Among them, 160,694 DTSCs are used for the test set.
To run our decoder on the development and test set, we set stackthrshold = 0.0001, stack-limit = 100, node-limit = 100, operation-limit = 20.
We also ran a phrase-based system (PB) with a distortion reordering model (Xiong et al., 2006) on the same corpus.
The results are shown in table 2.
For all BLEU scores, we also show the 95% confidence intervals computed using Zhang?s significant tester (Zhang et al., 2004) which was modified to conform to NIST?s definition of the BLEU brevity penalty.
The BLEU score of our current system with the DTSC model is lower than that of the phrasebased system.
However, with phrases integrated, the performance is improved greatly, and the new BLEU score is higher than that of the phrase-based SMT.
This difference is significant according to Zhang?s tester.
This result can be improved further using a better parser (Quirk et al., 2006) or using a statistical attaching model.
8 Related
Work The DTSC model is different from previous work based on dependency grammars by Eisner (2003), Lin (2004), Quirk et al.(2005), Ding et al.(2005) since they all deduce dependency structures on the target side.
Among them, the most similar work is (Quirk et al., 2005).
But there are still several major differences beyond the one mentioned above.
Our 46 treelets allow variables at any non-crossed nodes and target strings allow gaps, which are not available in (Quirk et al., 2005).
Our language model is calculated during decoding while Quirk?s language model is computed after decoding because of the complexity of their decoding.
The DTSC model is also quite distinct from previous tree-string models by Marcu et al.(2006) and Liu et al.(2006). Firstly, their models are based on phrase structure grammars.
Secondly, subtrees instead of treelets are extracted in their models.
Thirdly, it seems to be more difficult to integrate phrases into their models.
And finally, our model allow gaps on the target side, which is an advantage shared by (Melamed, 2004) and (Simard, 2005).
9 Conclusions
and Future Work We presented a novel syntax-based model using dependency trees on the source side?dependency treelet string correspondence model?for statistical machine translation.
We described an algorithm to learn DTSCs automatically from the training corpus and a chart-style algorithm for decoding.
Currently, we implemented a simple version of the DTSC model.
We believe that our performance can be improved greatly using a more sophisticated mechanism for determining attaching points.
Therefore the most important future work should be to design a better attaching model.
Furthermore, we plan to use larger corpora for training and n-best dependency trees for decoding, which both are helpful for the improvement of translation quality.
Acknowledgements This work was supported by National Natural Science Foundation of China, Contract No.
60603095 and 60573188.
References David Chiang.
2005. A hierarchical phrase-based model for statistical machine translation.
In Proceedings of ACL.
Yuan Ding and Martha Palmer.
2005. Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars.
In Proceedings of ACL.
Jason Eisner.
2003. Learning non-isomorphic tree mappings for machine translation.
In Proceedings of ACL.
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris Callison-Burch, Miles Osborne and David Talbot.
2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation.
In International Workshop on Spoken Language Translation.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin Knight.
2006. SPMT: Statistical Machine Translation with Syntactified Target Language Phraases.
In Proceedings of EMNLP.
I. Dan Melamed.
2004. Algorithms for Syntax-Aware Statistical Machine Translation.
In Proceedings of the Conference on Theoretical and Methodological Issues in Machine Translation (TMI), Baltimore, MD.
Dekang Lin.
2004. A path-based transfer model for machine translation.
In Proceedings of COLING.
Yang Liu, Qun Liu, and Shouxun Lin.
2006. Tree-to-String Alignment Template for Statistical Machine Translation.
In Proceedings of ACL.
Franz Josef Och.
2003. Minimum error rate training in statistical machine translation.
In Proceedings of ACL.
Franz Josef Och and Hermann Ney.
2000. Improved statistical alignment models.
In Proceedings of ACL.
Chris Quirk, Arul Menezes and Colin Cherry.
2005. Dependency Treelet Translation: Syntactically Informed Phrasal SMT.
In Proceedings of ACL.
Chris Quirk and Simon Corston-Oliver.
2006. The impact of parse quality on syntactically-informed statistical machine translation.
In Proceedings of EMNLP, Sydney, Australia.
Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc Dymetman, Eric Gaussier, Cyril Goutte, Kenji Yamada.
2005. Translating with non-contiguous phrases.
In Proceedings of HLT-EMNLP.
Andreas Stolcke.
2002. SRILM an extensible language modeling toolkit.
In Proceedings of International Conference on Spoken Language Processing, volume 2, pages 901-904.
Fei Xia.
1999. Automatic Grammar Generation from Two Different Perspectives.
PhD thesis, University of Pennsylvania.
Deyi Xiong, Qun Liu, and Shouxun Lin.
2006. Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation.
In Proceedings of COLING-ACL, Sydney, Australia.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, Yueliang Qian.
2005. Parsing the Penn Chinese Treebank with Semantic Knowledge.
In Proceedings of IJCNLP, Jeju Island, Korea.
Ying Zhang, Stephan Vogel, and Alex Waibel.
2004. Interpreting BLEU/NIST scores: How much improvement do we need to have a better system?
In Proceedings of LREC, pages 2051&?054.
Proceedings of the Second Workshop on Statistical Machine Translation, pages 48??5, Prague, June 2007.
c2007 Association for Computational Linguistics Word Error Rates: Decomposition over POS Classes and Applications for Error Analysis Maja Popovic Lehrstuhl fur Informatik 6 RWTH Aachen University Aachen, Germany popovic@cs.rwth-aachen.de Hermann Ney Lehrstuhl fur Informatik 6 RWTH Aachen University Aachen, Germany ney@cs.rwth-aachen.de Abstract Evaluation and error analysis of machine translation output are important but difficult tasks.
In this work, we propose a novel method for obtaining more details about actual translation errors in the generated output by introducing the decomposition of Word Error Rate (WER) and Position independent word Error Rate (PER) over different Partof-Speech (POS) classes.
Furthermore, we investigate two possible aspects of the use of these decompositions for automatic error analysis: estimation of inflectional errors and distribution of missing words over POS classes.
The obtained results are shown to correspond to the results of a human error analysis.
The results obtained on the European Parliament Plenary Session corpus in Spanish and English give a better overview of the nature of translation errors as well as ideas of where to put efforts for possible improvements of the translation system.
1 Introduction
Evaluation of machine translation output is a very important but difficult task.
Human evaluation is expensive and time consuming.
Therefore a variety of automatic evaluation measures have been studied over the last years.
The most widely used are Word Error Rate (WER), Position independent word Error Rate (PER), the BLEU score (Papineni et al., 2002) and the NIST score (Doddington, 2002).
These measures have shown to be valuable tools for comparing different systems as well as for evaluating improvements within one system.
However, these measures do not give any details about the nature of translation errors.
Therefore some more detailed analysis of the generated output is needed in order to identify the main problems and to focus the research efforts.
A framework for human error analysis has been proposed in (Vilar et al., 2006), but as every human evaluation, this is also a time consuming task.
This article presents a framework for calculating the decomposition of WER and PER over different POS classes, i.e. for estimating the contribution of each POS class to the overall word error rate.
Although this work focuses on POS classes, the method can be easily extended to other types of linguistic information.
In addition, two methods for error analysis using the WER and PER decompositons together with base forms are proposed: estimation of inflectional errors and distribution of missing words over POS classes.
The translation corpus used for our error analysis is built in the framework of the TC-STAR project (tcs, 2005) and contains the transcriptions of the European Parliament Plenary Sessions (EPPS) in Spanish and English.
The translation system used is the phrase-based statistical machine translation system described in (Vilar et al., 2005; Matusov et al., 2006).
2 Related
Work Automatic evaluation measures for machine translation output are receiving more and more attention in the last years.
The BLEU metric (Papineni et al., 2002) and the closely related NIST metric (Doddington, 2002) along with WER and PER 48 have been widely used by many machine translation researchers.
An extended version of BLEU which uses n-grams weighted according to their frequency estimated from a monolingual corpus is proposed in (Babych and Hartley, 2004).
(Leusch et al., 2005) investigate preprocessing and normalisation methods for improving the evaluation using the standard measures WER, PER, BLEU and NIST.
The same set of measures is examined in (Matusov et al., 2005) in combination with automatic sentence segmentation in order to enable evaluation of translation output without sentence boundaries (e.g.
translation of speech recognition output).
A new automatic metric METEOR (Banerjee and Lavie, 2005) uses stems and synonyms of the words.
This measure counts the number of exact word matches between the output and the reference.
In a second step, unmatched words are converted into stems or synonyms and then matched.
The TER metric (Snover et al., 2006) measures the amount of editing that a human would have to perform to change the system output so that it exactly matches the reference.
The CDER measure (Leusch et al., 2006) is based on edit distance, such as the well-known WER, but allows reordering of blocks.
Nevertheless, none of these measures or extensions takes into account linguistic knowledge about actual translation errors, for example what is the contribution of verbs in the overall error rate, how many full forms are wrong whereas their base forms are correct, etc.
A framework for human error analysis has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out.
However, human error analysis, like any human evaluation, is a time consuming task.
Whereas the use of linguistic knowledge for improving the performance of a statistical machine translation system is investigated in many publications for various language pairs (like for example (Nieen and Ney, 2000), (Goldwater and McClosky, 2005)), its use for the analysis of translation errors is still a rather unexplored area.
Some automatic methods for error analysis using base forms and POS tags are proposed in (Popovic et al., 2006; Popovic and Ney, 2006).
These measures are based on differences between WER and PER which are calculated separately for each POS class using subsets extracted from the original texts.
Standard overall WER and PER of the original texts are not at all taken into account.
In this work, the standard WER and PER are decomposed and analysed.
3 Decomposition
of WER and PER over POS classes The standard procedure for evaluating machine translation output is done by comparing the hypothesis document hyp with given reference translations ref, each one consisting of K sentences (or segments).
The reference document ref consists of R reference translations for each sentence.
Let the length of the hypothesis sentence hypk be denoted as Nhypk, and the reference lengths of each sentence Nref k,r. Then, the total hypothesis length of the document is Nhyp =summationtextk Nhypk, and the total reference length is Nref = summationtextk N?ref k where N?ref k is defined as the length of the reference sentence with the lowest sentence-level error rate as shown to be optimal in (Leusch et al., 2005).
3.1 Standard
word error rates (overview) The word error rate (WER) is based on the Levenshtein distance (Levenshtein, 1966) the minimum number of substitutions, deletions and insertions that have to be performed to convert the generated text hyp into the reference text ref . A shortcoming of the WER is the fact that it does not allow reorderings of words, whereas the word order of the hypothesis can be different from word order of the reference even though it is correct translation.
In order to overcome this problem, the position independent word error rate (PER) compares the words in the two sentences without taking the word order into account.
The PER is always lower than or equal to the WER.
On the other hand, shortcoming of the PER is the fact that the word order can be important in some cases.
Therefore the best solution is to calculate both word error rates.
Calculation of WER: The WER of the hypothesis hyp with respect to the reference ref is calculated as: WER = 1N?? ref Ksummationdisplay k=1 minr dL(ref k,r,hypk) where dL(ref k,r,hypk) is the Levenshtein distance between the reference sentence ref k,r and the hypothesis sentence hypk.
The calculation of WER 49 is performed using a dynamic programming algorithm.
Calculation of PER: The PER can be calculated using the counts n(e,hypk) and n(e,ref k,r) of a word e in the hypothesis sentence hypk and the reference sentence ref k,r respectively: PER = 1N?? ref Ksummationdisplay k=1 minr dPER(ref k,r,hypk) where dPER(ref k,r,hypk) = 12 parenleftbigg |Nref k,r ??Nhypk|+ summationdisplay e |n(e,ref k,r) ??n(e,hypk)| parenrightbigg 3.2 WER decomposition over POS classes The dynamic programming algorithm for WER enables a simple and straightforward identification of each erroneous word which actually contributes to WER.
Let errk denote the set of erroneous words in sentence k with respect to the best reference and p be a POS class.
Then n(p,errk) is the number of errors in errk produced by words with POS class p.
It should be noted that for the substitution errors, the POS class of the involved reference word is taken into account.
POS tags of the reference words are also used for the deletion errors, and for the insertion errors the POS class of the hypothesis word is taken.
The WER for the word class p can be calculated as: WER(p) = 1N?? ref Ksummationdisplay k=1 n(p,errk) The sum over all classes is equal to the standard overall WER.
An example of a reference sentence and hypothesis sentence along with the corresponding POS tags is shown in Table 1.
The WER errors, i.e. actual words participating in WER together with their POS classes can be seen in Table 2.
The reference words involved in WER are denoted as reference errors, and hypothesis errors refer to the hypothesis words participating in WER.
Standard WER of the whole sentence is equal to 4/12 = 33.3%.
The contribution of nouns is reference: Mister#N Commissioner#N,#PUN twenty-four#NUM hours#N sometimes#ADV can#V be#V too#ADV much#PRON time#N .#PUN hypothesis: Mrs#N Commissioner#N,#PUN twenty-four#NUM hours#N is#V sometimes#ADV too#ADV much#PRON time#N .#PUN Table 1: Example for illustration of actual errors: a POS tagged reference sentence and a corresponding hypothesis sentence reference errors hypothesis errors error type Mister#N Mrs#N substitution sometimes#ADV is#V substitution can#V deletion be#V sometimes#ADV substitution Table 2: WER errors: actual words which are participating in the word error rate and their corresponding POS classes WER(N) = 1/12 = 8.3%, of verbs WER(V) = 2/12 = 16.7% and of adverbs WER(ADV) = 1/12 = 8.3% 3.3 PER decomposition over POS classes In contrast to WER, standard efficient algorithms for the calculation of PER do not give precise information about contributing words.
However, it is possible to identify all words in the hypothesis which do not have a counterpart in the reference, and vice versa.
These words will be referred to as PER errors.
reference errors hypothesis errors Mister#N Mrs#N be#V is#V can#V Table 3: PER errors: actual words which are participating in the position independent word error rate and their corresponding POS classes An illustration of PER errors is given in Table 3.
50 The number of errors contributing to the standard PER according to the algorithm described in 3.1 is 3 there are two substitutions and one deletion.
The problem with standard PER is that it is not possible to detect which words are the deletion errors, which are the insertion errors, and which words are the substitution errors.
Therefore we introduce an alternative PER based measure which corresponds to the F-measure.
Let herrk refer to the set of words in the hypothesis sentence k which do not appear in the reference sentence k (referred to as hypothesis errors).
Analogously, let rerrk denote the set of words in the reference sentence k which do not appear in the hypothesis sentence k (referred to as reference errors).
Then the following measures can be calculated: ??reference PER (RPER) (similar to recall): RPER(p) = 1N?? ref Ksummationdisplay k=1 n(p,rerrk) ??hypothesis PER (HPER) (similar to precision): HPER(p) = 1N hyp Ksummationdisplay k=1 n(p,herrk) ??F-based PER (FPER): FPER(p) = 1N?? ref + Nhyp   Ksummationdisplay k=1 (n(p,rerrk) + n(p,herrk)) Since we are basically interested in all words without a counterpart, both in the reference and in the hypothesis, this work will be focused on FPER.
The sum of FPER over all POS classes is equal to the overall FPER, and the latter is always less or equal to the standard PER.
For the example sentence presented in Table 1, the number of hypothesis errors n(e,herrk) is 2 and the number of reference errors n(e,rerrk) is 3 where e denotes the word.
The number of errors contributing to the standard PER is 3, since |Nref ??Nhyp| = 1 and summationtexte |n(e,ref k) ??n(e,hypk)| = 5.
The standard PER is normalised over the reference length Nref = 12 thus being equal to 25%.
The FPER is the sum of hypothesis and reference errors divided by the sum of hypothesis and reference length: FPER = (2 + 3)/(11 + 12) = 5/23 = 21.7%.
The contribution of nouns is FPER(N) = 2/23 = 8.7% and the contribution of verbs is FPER(V) = 3/23 = 13%.
4 Applications
for error analysis The decomposed error rates described in Section 3.2 and Section 3.3 contain more details than the standard error rates.
However, for more precise information about certain phenomena some kind of further analysis is required.
In this work, we investigate two possible aspects for error analysis: ??estimation of inflectional errors by the use of FPER errors and base forms ??extracting the distribution of missing words over POS classes using WER errors, FPER errors and base forms.
4.1 Inflectional
errors Inflectional errors can be estimated using FPER errors and base forms.
From each referencehypothesis sentence pair, only erroneous words which have the common base forms are taken into account.
The inflectional error rate of each POS class is then calculated in the same way as FPER.
For example, from the PER errors presented in Table 3, the words ?is??and ?be??are candidates for an inflectional error because they are sharing the same base form ?be??
Inflectional error rate in this example is present only for the verbs, and is calculated in the same way as FPER, i.e.
IFPER(V) = 2/23 = 8.7%. 4.2 Missing words Distribution of missing words over POS classes can be extracted from the WER and FPER errors in the following way: the words considered as missing are those which occur as deletions in WER errors and at the same time occur only as reference PER errors without sharing the base form with any hypothesis error.
The use of both WER and PER errors is much more reliable than using only the WER deletion erros because not all deletion errors are produced by missing words: a number of WER deletions appears 51 due to reordering errors.
The information about the base form is used in order to eliminate inflectional errors.
The number of missing words is extracted for each word class and then normalised over the sum of all classes.
For the example sentence pair presented in Table 1, from the WER errors in Table 2 and the PER errors in Table 3 the word ?can??will be identified as missing.
5 Experimental
settings 5.1 Translation System The machine translation system used in this work is based on the statistical aproach.
It is built as a log-linear combination of seven different statistical models: phrase based models in both directions, IBM1 models at the phrase level in both directions, as well as target language model, phrase penalty and length penalty are used.
A detailed description of the system can be found in (Vilar et al., 2005; Matusov et al., 2006).
5.2 Task
and corpus The corpus analysed in this work is built in the framework of the TC-STAR project.
The training corpus contains more than one million sentences and about 35 million running words of the European Parliament Plenary Sessions (EPPS) in Spanish and English.
The test corpus contains about 1 000 sentences and 28 000 running words.
The OOV rates are low, about 0.5% of the running words for Spanish and 0.2% for English.
The corpus statistics can be seen in Table 4.
More details about the EPPS data can be found in (Vilar et al., 2005).
TRAIN Spanish English Sentences 1 167 627 Running words 35 320 646 33 945 468 Vocabulary 159 080 110 636 TEST Sentences 894 1 117 Running words 28 591 28 492 OOVs 0.52% 0.25% Table 4: Statistics of the training and test corpora of the TC-STAR EPPS Spanish-English task.
Test corpus is provided with two references.
6 Error
analysis The translation is performed in both directions (Spanish to English and English to Spanish) and the error analysis is done on both the English and the Spanish output.
Morpho-syntactic annotation of the English references and hypotheses is performed using the constraint grammar parser ENGCG (Voutilainen, 1995), and the Spanish texts are annotated using the FreeLing analyser (Carreras et al., 2004).
In this way, all references and hypotheses are provided with POS tags and base forms.
The decomposition of WER and FPER is done over the ten main POS classes: nouns (N), verbs (V), adjectives (A), adverbs (ADV), pronouns (PRON), determiners (DET), prepositions (PREP), conjunctions (CON), numerals (NUM) and punctuation marks (PUN).
Inflectional error rates are also estimated for each POS class using FPER counts and base forms.
Additionally, details about the verb tense and person inflections for both languages as well as about the adjective gender and person inflections for the Spanish output are extracted.
Apart from that, the distribution of missing words over the ten POS classes is estimated using the WER and FPER errors.
6.1 WER
and PER (FPER) decompositions Figure 1 presents the decompositions of WER and FPER over the ten basic POS classes for both languages.
The largest part of both word error rates comes from the two most important word classes, namely nouns and verbs, and that the least critical classes are punctuations, conjunctions and numbers.
Adjectives, determiners and prepositions are significantly worse in the Spanish output.
This is partly due to the richer morphology of the Spanish language.
Furthermore, the histograms indicate that the number of erroneus nouns and pronouns is higher in the English output.
As for verbs, WER is higher for English and FPER for Spanish.
This indicates that there are more problems with word order in the English output, and more problems with the correct verb or verb form in the Spanish output.
In addition, the decomposed error rates give an idea of where to put efforts for possible improvements of the system.
For example, working on improvements of verb translations could reduce up to about 10% WER and 7% FPER, working on nouns 52 0 1 2 3 4 5 6 7 8 9 10 11 PUNNUMPREP CONDETPRONADVAVN WER over POS classes [%] English Spanish 0 1 2 3 4 5 6 7 8 9 PUNNUMPREP CONDETPRONADVAVN FPER over POS classes [%] English Spanish Figure 1: Decomposition of WER and FPER [%] over the ten basic POS classes for English and Spanish output up to 8% WER and 5% FPER, whereas there is no reason to put too much efforts on e.g. adverbs since this could lead only to about 2% of WER and FPER reduction.
1 6.2 Inflectional errors Inflectional error rates for the ten POS classes are presented in Figure 2.
For the English language, these errors are significant only for two POS classes: nouns and verbs.
The verbs are the most problematic category in both languages, for Spanish having almost two times higher error rate than for English.
This is due to the very rich morphology of Spanish verbs one base form might have up to about fourty different inflections.
1Reduction of FPER leads to a similar reduction of PER.
0 0.5 1 1.5 2 2.5 PUNNUMPREP CONDETPRONADVAVN inflectional errors [%] English Spanish Figure 2: Inflectional error rates [%] for English and Spanish output Nouns have a higher error rate for English than for Spanish.
The reason for this difference is not clear, since the noun morphology of neither of the languages is particularly rich there is only distinction between singular and plural.
One possible explanation might be the numerous occurences of different variants of the same word, like for example ?Mr??and ?Mister??
In the Spanish output, two additional POS classes are showing significant error rate: determiners and adjectives.
This is due to the gender and number inflections of those classes which do not exist in the English language for each determiner or adjective, there are four variants in Spanish and only one in English.
Working on inflections of Spanish verbs might reduce approximately 2% of FPER, on English verbs about 1%.
Improvements of Spanish determiners could lead up to about 2% of improvements.
6.2.1 Comparison
with human error analysis The results obtained for inflectional errors are comparable with the results of a human error analysis carried out in (Vilar et al., 2006).
Although it is difficult to compare all the numbers directly, the overall tendencies are the same: the largest number of translation errors are caused by Spanish verbs, and much less but still a large number of errors by English verbs.
A much smaller but still significant number of errors is due to Spanish adjectives, and only a few errors of English adjectives are present.
Human analysis was done also for the tense and 53 person of verbs, as well as for the number and gender of adjectives.
We use more detailed POS tags in order to extract this additional information and calculate inflectional error rates for such tags.
It should be noted that in contrast to all previous error rates, these error rates are not disjunct but overlapping: many words are contributing to both.
The results are shown in Figure 3, and the tendencies are again the same as those reported in (Vilar et al., 2006).
As for verbs, tense errors are much more frequent than person errors for both languages.
Adjective inflections cause certain amount of errors only in the Spanish output.
Contributions of gender and of number are aproximately equal.
0 0.5 1 1.5 2 A numberA genderV personV tense inflectional errors of verbs and adjectives [%] English Spanish Figure 3: More details about inflections: verb tense and person error rates and adjective gender and number error rates [%] 6.3 Missing words Figure 4 presents the distribution of missing words over POS classes.
This distribution has a same behaviour as the one obtained by human error analysis.
Most missing words for both languages are verbs.
For English, the percentage of missing verbs is significantly higher than for Spanish.
The same thing happens for pronouns.
The probable reason for this is the nature of Spanish verbs.
Since person and tense are contained in the suffix, Spanish pronouns are often omitted, and auxiliary verbs do not exist for all tenses.
This could be problematic for a translation system, because it processes only one Spanish word which actually contains two (or more) English words.
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 PUNNUMPREP CONDETPRONADVAVN missing words [%] eng esp Figure 4: Distribution of missing words over POS classes [%] for English and Spanish output Prepositions are more often missing in Spanish than in English, as well as determiners.
A probable reason is the disproportion of the number of occurrences for those classes between two languages.
7 Conclusions
This work presents a framework for extraction of linguistic details from standard word error rates WER and PER and their use for an automatic error analysis.
We presented a method for the decomposition of standard word error rates WER and PER over ten basic POS classes.
We also carried out a detailed analysis of inflectional errors which has shown that the results obtained by our method correspond to those obtained by a human error analysis.
In addition, we proposed a method for analysing missing word errors.
We plan to extend the proposed methods in order to carry out a more detailed error analysis, for example examining different types of verb inflections.
We also plan to examine other types of translation errors like for example errors caused by word order.
Acknowledgements This work was partly funded by the European Union under the integrated project TC-STAR??Technology and Corpora for Speech to Speech Translation (IST2002-FP6-506738).
54 References Bogdan Babych and Anthony Hartley.
2004. Extending BLEU MT Evaluation Method with Frequency Weighting.
In Proc.
of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL), Barcelona, Spain, July.
Satanjeev Banerjee and Alon Lavie.
2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgements.
In 43rd Annual Meeting of the Assoc.
for Computational Linguistics: Proc.
Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 65??2, Ann Arbor, MI, June.
Xavier Carreras, Isaac Chao, Llus Padro, and Muntsa Padro.
2004. FreeLing: An Open-Source Suite of Language Analyzers.
In Proc.
4th Int.
Conf. on Language Resources and Evaluation (LREC), pages 239?? 242, Lisbon, Portugal, May.
George Doddington.
2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.
In Proc.
ARPA Workshop on Human Language Technology, pages 128??32, San Diego.
Sharon Goldwater and David McClosky.
2005. Improving stastistical machine translation through morphological analysis.
In Proc.
of the Conf.
on Empirical Methods for Natural Language Processing (EMNLP), Vancouver, Canada, October.
Gregor Leusch, Nicola Ueffing, David Vilar, and Hermann Ney.
2005. Preprocessing and Normalization for Automatic Evaluation of Machine Translation.
In 43rd Annual Meeting of the Assoc.
for Computational Linguistics: Proc.
Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 17??4, Ann Arbor, MI, June.
Association for Computational Linguistics.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2006. CDER: Efficient MT Evaluation Using Block Movements.
In EACL06, pages 241??48, Trento, Italy, April.
Vladimir Iosifovich Levenshtein.
1966. Binary Codes Capable of Correcting Deletions, Insertions and Reversals.
Soviet Physics Doklady, 10(8):707??10, February.
Evgeny Matusov, Gregor Leusch, Oliver Bender, and Hermann Ney.
2005. Evaluating Machine Translation Output with Automatic Sentence Segmentation.
In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), pages 148??54, Pittsburgh, PA, October.
Evgeny Matusov, Richard Zens, David Vilar, Arne Mauser, Maja Popovic, and Hermann Ney.
2006. The RWTH Machine Translation System.
In TC-Star Workshop on Speech-to-Speech Translation, pages 31??36, Barcelona, Spain, June.
Sonja Nieen and Hermann Ney.
2000. Improving SMT quality with morpho-syntactic analysis.
In COLING ??0: The 18th Int.
Conf. on Computational Linguistics, pages 1081??085, Saarbrucken, Germany, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-Jing Zhu.
2002. BLEU: a method for automatic evaluation of machine translation.
In Proc.
of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311??18, Philadelphia, PA, July.
Maja Popovic and Hermann Ney.
2006. Error Analysis of Verb Inflections in Spanish Translation Output.
In TC-Star Workshop on Speech-to-Speech Translation, pages 99??03, Barcelona, Spain, June.
Maja Popovic, Adri`a de Gispert, Deepa Gupta, Patrik Lambert, Hermann Ney, Jose B.
Mari?no, Marcello Federico, and Rafael Banchs.
2006. Morpho-syntactic Information for Automatic Error Analysis of Statistical Machine Translation Output.
In Proc.
of the HLT-NAACL Workshop on Statistical Machine Translation, pages 1??, New York, NY, June.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul.
2006. A Study of Translation Error Rate with Targeted Human Annotation.
In Proc.
of the 7th Conf.
of the Association for Machine Translation in the Americas (AMTA 06), pages 223??31, Boston, MA.
2005. TC-STAR technology and corpora for speech to speech translation.
Integrated project TCSTAR (IST2002-FP6-506738) funded by the European Commission.
http://www.tc-star.org/. David Vilar, Evgeny Matusov, Sa?sa Hasan, Richard Zens, and Hermann Ney.
2005. Statistical Machine Translation of European Parliamentary Speeches.
In Proc.
MT Summit X, pages 259??66, Phuket, Thailand, September.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Hermann Ney.
2006. Error Analysis of Statistical Machine Translation Output.
In Proc.
of the Fifth Int.
Conf. on Language Resources and Evaluation (LREC), pages 697??02, Genoa, Italy, May.
Atro Voutilainen.
1995. ENGCG -Constraint Grammar Parser of English. http://www2.lingsoft.fi/doc/engcg/intro/ .
Proceedings of the Second Workshop on Statistical Machine Translation, pages 56??3, Prague, June 2007.
c2007 Association for Computational Linguistics Speech-input multi-target machine translation Alicia Perez, M.
Ines Torres Dep.
of Electricity and Electronics University of the Basque Country manes@we.lc.ehu.es M.
Teresa Gonzalez, Francisco Casacuberta Dep.
of Information Systems and Computation Technical University of Valencia fcn@dsic.upv.es Abstract In order to simultaneously translate speech into multiple languages an extension of stochastic finite-state transducers is proposed.
In this approach the speech translation model consists of a single network where acoustic models (in the input) and the multilingual model (in the output) are embedded.
The multi-target model has been evaluated in a practical situation, and the results have been compared with those obtained using several mono-target models.
Experimental results show that the multi-target one requires less amount of memory.
In addition, a single decoding is enough to get the speech translated into multiple languages.
1 Introduction
In this work we deal with finite-state models which constitute an important framework in syntactic pattern recognition for language and speech processing applications (Mohri et al., 2002; Pereira and Riley, 1997).
One of their outstanding characteristics is the availability of efficient algorithms for both optimization and decoding purposes.
Specifically, stochastic finite-state transducers (SFSTs) have proved to be useful for machine translation tasks within restricted domains.
There are several approaches implemented over SFSTs which range from word-based systems (Knight and AlOnaizan, 1998) to phrase-based systems (Perez et al., 2007).
SFSTs usually offer high speed during the decoding step and they provide competitive results in terms of error rates.
In addition, SFSTs have proved to be versatile models, which can be easily integrated with other finite-state models, such as a speech recognition system for speech-input translation purposes (Vidal, 1997).
In fact, the integrated architecture has proved to work better than the decoupled one.
Our main goal is, hence, to extend and assess these methodologies to accomplish spoken language multi-target translation.
As far as multilingual translation is concerned, there are two main trends in machine translation devoted to translate an input string simultaneously into m languages (Hutchins and Somers, 1992): interlingua and parallel transfer.
The former has historically been a knowledge-based technique that requires a deep-analysis effort, and the latter consists on m decoupled translators in a parallel architecture.
These translators can be either knowledge or example-based.
On the other hand, in (Gonzalez and Casacuberta, 2006) an example based technique consisting of a single SFST that cope with multiple target languages was presented.
In that approach, when translating an input sentence, only one search through the multi-target SFST is required, instead of the m independent decoding processes required by the mono-target translators.
The classical layout for speech-input multi-target translation includes a speech recognition system in a serial architecture with m decoupled text-to-text translators.
Thus, this architecture entails a decoding stage of the speech signal into the source language text, and m further decoding stages to translate the source text into each of the m target lan56 guages.
If we supplant the m translators with the multi-target SFST, the problem would be reduced to 2 searching stages.
Nevertheless, in this paper we propose a natural way for acoustic models to be integrated in the multilingual network itself, in such a way that the input speech signal can be simultaneously decoded and translated into m target languages.
As a result, due to the fact that there is just a single searching stage, this novel approach entails less computational cost.
The remainder of the present paper is structured as follows: section 2 describes both multi-target SFSTs and the inference algorithm from training examples; in section 3 a novel integrated architecture for speech-input multi-target translation is proposed; section 4 presents a practical application of these methods, including the experimental setup and the results they produced; finally, section 5 summarizes the main conclusions of this work.
2 Multi-target stochastic finite-state transducers A multi-target SFST is a generalization of standard SFSTs, in such a way that every input string in the source language results in a tuple of output strings each being associated to a different target language.
2.1 Definition
A multi-target stochastic finite-state transducer is a tupleT =????...?m,Q,q0,R,F,P?? where:  is a finite set of input symbols (source vocabulary); ??...?m are m finite sets of output symbols (target vocabularies); Q is a finite set of states; q0 ?Q is the initial state; R?Q?1...?mQ is a set of transitions such as (q,w, ?p1,..., ?pm,qprime), which is a transition from the state q to the state qprime, with the source symbol w and producing the substrings (?p1,..., ?pm); P : R??[0,1] is the transition probability distribution; F : Q??[0,1] is the final state probability distribution; The probability distributions satisfy the stochastic constraint: ?q?Q (1) F(q)+ summationtext w,?p1,...,?pm,qprime P(q,w, ?p1,..., ?pm,qprime) = 1 2.2 Training the multilingual translation model Both topology and parameters of an SFST can be learned fully automatically from bilingual examples making use of underlying alignment models (Casacuberta and Vidal, 2004).
Furthermore, a multi-target SFST can be inferred from a multilingual set of samples (Gonzalez and Casacuberta, 2006).
Even though in realistic situations multilingual corpora are too scarce, recent works (Popovic et al., 2005) show that bilingual corpora covering the same domain are sufficient to obtain generalized corpora based on which one can subsequently create the required collections of aligned tuples.
The inference algorithm, GIAMTI (grammatical inference and alignments for multi-target transducer inference), requires a multilingual corpus, that is, a finite set of multilingual samples (s,t1,...,tm) ??
????m, where ti denotes the translation of the source sentence s into thei-th target language;  denotes the source language vocabulary, and ?i the i-th target language vocabulary; the algorithm can be outlined as follows: 1.
Each multilingual sample is transformed into a single string from an extended vocabulary (?
???1 ?m) using a labeling function (Lm).
This transformation searches an adequate monotonic segmentation for each of the m source-target language pairs on the basis of bilingual alignments such as those given by GIZA++ (Och, 2000).
A monotonic segmentation copes with monotonic alignments, that is, j < k ??aj < ak following the notation of (Brown et al., 1993).
Each source token, which can be either a word or a phrase (Perez et al., 2007), is then joined with a target phrase of each language as the corresponding segmentation suggests.
Each extended symbol consists of a token from the source language plus zero 57 Alignment #0 0:tenperatura 1:minimoa 2:jeitsiko 3:da 0:temperaturas 1:minimas 2:en 3:descenso (a) Spanish-Basque Alignment #0 0:low 1:temperatures 2:falling 0:temperaturas 1:minimas 2:en 3:descenso (b) Spanish-English 0 1 temperaturas | temperatura | NIL 2 maximas | maximoak | high temperatures minimas | minimoak | low temperatures 3 en | NIL | NIL 5 descenso | jaitsiko da | falling ascenso | igoko da | rising (c) Multi-target SFST from Spanish into English and Basque.
Figure 1: Example of a trilingual alignment over a trilingual sentence extracted from the task under consideration;the related multi-target SFST (with Spanish as input, and English and Basque as output).
or more words from each target language in their turn.
2. Once the set of multilingual samples has been converted into a set of single extended strings (z?????, a stochastic regular grammar can be inferred.
Specifically, in this work we deal with k-testable in the string-sense grammars (Garca and Vidal, 1990), which are considered to be a syntactic approach of the n-gram models.
In addition, they allow the integration of several order models in a single smoothed automaton (Torres and Varona, 2001).
3. The extended symbols associated with the transitions of the automaton are transformed into one input token and m output phrases (w/?p1|...|?pm) by the inverse labeling function (L?m), leading to the required transducer.
Example An illustration of the inference of the multi-target SFST can be shown over a couple of simple trilingual sentences from the corpus (where ?B??stands for Basque, ?S??for Spanish and ?E??for English): 1-B tenperatura maximoa jaitsiko da 1-S temperaturas maximas en descenso 1-E high temperatures falling 2-B tenperatura minimoa igoko da 2-S temperaturas mnimas en ascenso 2-E low temperatures rising From the alignments, depicted in Figures 1(a) and 1(b), an input-language-synchronized monotonous segmentation can be built (bear in mind that we are considering Spanish as the input language).
The corresponding extended strings with the following constituents for the first and second samples respectively are the following ones: 1 temperaturas|tenperatura| mnimas|minimoa|low temperatures en|| descenso|jaitsiko da|falling 58 2 temperaturas|tenperatura| maximas|maximoa|high temperatures en|| ascenso|igoko da|rising Finally, from this representation of the data, the multi-target SFST can be built as shown in Figure 1(c).
2.3 Decoding
Given an input string s (a sentence in the source language), the decoding module has to search the optimal m output strings tm ???1?m (a sentence in each of the target language) according to the underlying translation model (T): hatwidertm = arg max tm????m PT(s,tm) (2) Solving equation (2) is a hard computational problem, however, it can be efficiently computed under the so called maximum approach as follows: PT(s,tm)??max ?(s,tm) PT(?(s,tm)) (3) where ?(s,tm) is a translation form, that is, a sequence of transitions in the multi-target SFST compatible with both the input and the m output strings.
?(s,tm) : (q0,w1, ?pm1,q1)(qJ??,wJ, ?pmJ,qJ) The input string (s) is a sequence of J input symbols, s = wJ1, and each of the m output strings consists of J phrases in its corresponding language tm = (t1,,tm) = ( ?p1)J1,,( ?pm)J1. Thus, the probability supplied by the multi-target SFST to the translation form is given by: PT(?(s,tm)) = F(qJ) Jproductdisplay j=1 P(qj??,wj, ?pmj,qj) (4) In this context, the Viterbi algorithm can be used to obtain the optimal sequence of states through the multi-target SFST for a given input string.
As a result, the established m translations are built concatenating the (J) output phrases for each language through the optimal path.
3 An
embedded architecture for speech-input multi-target translation 3.1 Statistical framework Given the acoustic representation (x) of a speech signal, the goal of multi-target speech translation is to find the most likely m target strings (tm); that is, one string (ti) per target language involved (i ??{1,...,m}).
This approach is summarized in eq.
(5), where the hidden variable s can be interpreted as the transcription of the speech signal: hatwidertm = arg max tm P(t m|x) = arg max tm summationdisplay s P(tm,s|x) (5) Making use of Bayes??rule, the former expression turns into: hatwidertm = arg max tm summationdisplay s P(tm,s)P(x|tm,s) (6) Empirically, there is no loss of generality if we assume that the acoustic signal representation depends only on the source string, i.e.
P(x|tm,s) is independent of tm.
In this sense, eq.
(6) can be rewritten as: hatwidertm = arg max tm summationdisplay s P(tm,s)P(x|s) (7) Equation (7) combines a standard acoustic model, P(x|s), and a multi-target translation model, P(tm,s), both of whom can be integrated on the fly during the searching routine as shown in Figure 2.
That is, each acoustic sub-network is only expanded at decoding time when it is required.
The outer sum is computationally very expensive to search for the optimal tuple of target strings tm in an effective way.
Thus we make use of the so called Viterbi approximation, which finds the best path over the whole transducer.
3.2 Practical
issues The underlying recognizer used in this work is our own continuous-speech recognition system, which implements stochastic finite-state models at all levels: acoustic-phonetic, lexical and syntactic, and which allows to infer them based on samples.
The signal analysis was carried out in a standard way, based on the classical Mel-cepstrum parametrization.
Each phone-like unit was modeled 59 1 /e/ | NIL | NIL 2 /n/ | NIL | NIL Figure 2: Integration on the fly of acoustic models in one edge of the SFST shown in Figure 1(c) by a typical left to right hidden Markov model.
A phonetically-balanced Spanish database, called Albayzin (Moreno et al., 1993), was used to train these models.
The lexical model consisted of the extended tokens of the multi-target SFST instead of running words.
The acoustic transcription for each extended token was automatically obtained on the basis of the input projection of each unit, that is, the Spanish vocabulary in this case.
Instead of the usual language model, we make use of the multi-target SFST itself, which had the syntactic structure provided by a k-testable in the strict sense model, with k=3, and Witten-Bell smoothing.
Note that the SFST implicitly involves both input and output language models.
4 Experimental
results 4.1 Task and corpus The described general methodology has been put into practice in a highly practical application that aims to translate on-line TV weather forecasts into several languages, taking the speech of the presenter as the input and producing as output text-strings, or sub-titles, in several languages.
For this purpose, we used the corpus METEUS which consists of a set of trilingual sentences, in English, Spanish and Basque, as extracted from weather forecast reports that had been published on the Internet.
Let us notice that it is a real trilingual corpus, which they are usually quite scarce.
Basque is a pre-Indoeuropean language of still unknown origin.
It is a minority language, spoken in a small area of Europe and also within some small American communities (such as that in Reno, Nevada).
In the Basque Country (located in the north of Spain) it has an official status along with Spanish.
However, despite having coexisted for centuries in the same area, they differ greatly both in syntax and in semantics.
Hence, efforts are being devoted nowadays to machine translation tools involving these two languages (Alegria et al., 2004), although they are still scarce.
With regard to the order of the phrases within a sentence, the most common one in Basque is Subject plus Objects plus Verb (even though some alternative structures are also accepted), whereas in Spanish and English other constructions such as Subject plus Verb plus Objects are more frequent (see Figures 1(a) and 1(b)).
Another difference between Basque and Spanish or English is that Basque is an extremely inflected language.
In this experiment we intend to translate Spanish speech simultaneously into both Basque and English.
Just by having a look at the main features of the corpus in Table 1, we can realize that there are substantial differences among these three languages, in terms both of the size of the vocabulary and of the amount of running words.
These figures reveal the agglutinant nature of the Basque language in comparison with English or Spanish.
Spanish Basque English Training Total sentences 14,615 Different sentences 7,225 7,523 6,634 Words 191,156 187,462 195,627 Vocabulary 702 1,147 498 Average Length 13.0 12.8 13.3 Test Sentences 500 Words 8,706 8,274 9,150 Average Length 17.4 16.5 18.3 Perplexity (3grams) 4.8 6.7 5.8 Table 1: Main features of the METEUS corpus.
With regard to the speech test, the input consisted of the speech signal recorded by 36 speakers, each one reading out 50 sentences from the test-set in Table 1.
That is, each sentence was read out by at least three speakers.
The input speech resulted in approximately 3.50 hours of audio signal.
Needless to say, the application that we envisage has to be speaker60 independent if it is to be realistic.
4.2 System
evaluation The performance obtained by the acoustic integration has been experimentally tested for both multitarget and mono-target devices.
As a matter of comparison, text-input translation results are also reported.
The multi-target SFST was learned from the training set described in Table 1 using the previously described GIAMTI algorithm.
The 500 test sentences were then translated by the multi-target SFST.
The translation provided by the system in each language was compared to the corresponding reference sentence.
Additionally, two mono-target SFSTs were inferred with their outputs for the aforementioned test to be taken as baseline.
The evaluation includes both computational cost and performance of the system.
4.2.1 Computational
cost The expected searching time and the amount of memory that needs to be allocated for a given model are two key parameters to bear in mind in speechinput machine translation applications.
These values can be objectively measured in terms of the size and on the average branching factor of the model displayed in Table 2.
multi-target mono-targetS2B S2E Nodes 52,074 35,034 20,148 Edges 163,146 115,526 69,690 Branching factor 3.30 3.13 3.46 Table 2: Features of multi-target model and the two decoupled mono-target models (one for Spanish to Basque translation, referred to as S2B, and the second for Spanish to English, S2E).
Adding the edges up for the two mono-target SFSTs that take part in the decoupled architecture (see Table 2), we conclude that the decoupled model needs a total of 185,216 edges to be allocated in memory, which represents an increment of 13% in memory-space with respect to the multi-target model.
On the other hand, the multi-target approach offers a slightly smaller branching factor than each mono-target approach.
As a result, fewer paths have to be explored with the multi-target approach than with the decoupled one, which suggests that searching for a translation might be faster.
As a matter of fact, experimental results in Table 3 show that the mono-target architecture works 11% more slowly than the multi-target one for speech-input machine translation and decoding, and 30% for text to text translation.
Time (s) multi-target mono-targetS2B+S2E Text-input 0.36 0.47 Speech-input 16.9 18.9 Table 3: Average time needed to translate each input sentence into two languages.
Summarizing, in terms of computational cost (space and time), a multi-target SFST performs better than the mono-target decoupled system.
4.2.2 Performance
So far, the capability of the systems has been assessed in terms of time and spatial costs.
However, the quality of the translations they provide is, doubtless, the most relevant evaluation criterion.
In order to determine the performance of the system in a quantitative manner, the following evaluation parameters were computed for each scenario: bilingual evaluation under study (BLEU), position independent error rate (PER) and word error rate (WER).
Both text and speech-input translation results provided by the multi-target and the mono-target models respectively are shown in Table 4.
As can be derived from the translation results, for text-input translation the classical approach performs slightly better than the multi-target one, but for speech-input translation from Spanish into English is the other way around.
In any case, the differences in performance are marginal.
Comparing the text-input with the speech-input results we realize that, as could be expected, the process of speech signal decoding is itself introducing some errors.
In an attempt to measure these errors, the text transcription of the recognized input signal was extracted and compared to the input reference in terms of WER as shown in the last row of the Table 4.
Note that even though the input sentences are the same the three results differ due to the fact that 61 we are making use of different SFST models that decode and translate at the same time.
multi-target mono-target S2B S2E S2B S2E Text BLEU 42.7 66.7 43.4 67.8 PER 39.9 19.9 38.2 19.0 WER 48.0 27.5 46.2 26.6 Speech BLEU 39.5 59.0 39.2 61.1 PER 42.2 25.3 41.5 23.6 WER 51.5 33.9 50.5 31.9 recognition WER 10.7 9.3 9.1 Table 4: Text-input and speech-input translation results for Spanish into Basque (S2B) and Spanish into English (S2E) using a multi-target SFST (columns on the left) or two mono-target SFSTs (columns on the right).
The last row shows Spanish speech decoding results using each of the three devices.
In these series of experiments the same task has been compared with two extremely different language pairs under the same conditions.
There is a noticeable difference in terms of quality between the English and the Basque translations.
The underlying reason might be due to the fact that SFST models do not capture properly the rich morphology of the Basque as they have to face long-distance reordering issues.
These differences in the performance of the system when translating into English or into Basque have been previously detected in other works (Ortiz et al., 2003).
In our case, a manual review of the models and the obtained translations encourage us to make use of reordering models in future work, since they have proved to report good results in a similar framework (Kanthak et al., 2005).
5 Concluding
remarks and further work The main contribution of this paper is the proposal of a fully embedded architecture for multiple speech translation.
Thus, acoustic models are integrated on the fly into a multi-target translation model.
The most significant feature of this approach is its ability to carry out both the recognition and the translation into multiple languages integrated in a unique model.
Due to the finite-state nature of this model, the speech translation engine is based on a Viterbilike algorithm.
In contrast to the mono-target systems, multitarget SFSTs enable the translation from one source language simultaneously into several target languages with lower computational costs (in terms of space and time) and comparable qualitative results.
Moreover, the integration of several languages and acoustic models is straightforward on means of finite-state devices.
Nevertheless, the integrated architecture needs more parameters to be estimated.
In fact, as the amount of targets increase the data sparseness might become a difficult problem to cope with.
In future work we intend to make a deeper study on the performance of the multi-target system with regard to the amount of parameters to be estimated.
In addition, as the first step of the learning algorithm is decisive, we are planning to make use of reordering models in an attempt to face up to with long distance reordering and in order to homogenize all the languages involved.
Acknowledgments This work has been partially supported by the University of the Basque Country and by Spanish CICYT under grants 9/UPV 00224.310-15900/2004, TIC2003-08681-C02-02, and CICYT es TIN200508660-C04-03 respectively.
References I?naki Alegria, Olatz Ansa, Xabier Artola, Nerea Ezeiza, Koldo Gojenola, and Ruben Urizar.
2004. Representation and treatment of multiword expressions in basque.
In Takaaki Tanaka, Aline Villavicencio, Francis Bond, and Anna Korhonen, editors, Second ACL Workshop on Multiword Expressions: Integrating Processing, pages 48??5, Barcelona, Spain, July.
Association for Computational Linguistics.
Peter F.
Brown, Stephen A.
Della Pietra, Vincent J.
Della Pietra, and R.
L. Mercer.
1993. The mathematics of statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263??11.
Francisco Casacuberta and Enrique Vidal.
2004. Machine translation with inferred stochastic finite-state transducers.
Computational Linguistics, 30(2):205?? 225.
P. Garca and E.
Vidal. 1990.
Inference of k-testable languages in the strict sense and application to syntactic pattern recognition.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(9):920??25.62 M.T.
Gonzalez and F.
Casacuberta. 2006.
Multi-Target Machine Translation using Finite-State Transducers.
In Proceedings of TC-Star Speech to Speech Translation Workshop, pages 105??10.
John Hutchins and Harold L.
Somers. 1992.
An Introduction to Machine Translation.
Academic Press, Cambridge, MA.
Stephan Kanthak, David Vilar, Evgeny Matusov, Richard Zens, and Hermann Ney.
2005. Novel reordering approaches in phrase-based statistical machine translation.
In Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 167??74, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
K. Knight and Y.
Al-Onaizan. 1998.
Translation with finite-state devices.
In 4th AMTA (Association for Machine Translation in the Americas).
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2002. Weighted finite-state transducers in speech recognition.
Computer, Speech and Language, 16(1):69??8, January.
A. Moreno, D.
Poch, A.
Bonafonte, E.
Lleida, J.
Llisterri, J.
B. Mario, and C.
Nadeu. 1993.
Albayzin speech database: Design of the phonetic corpus.
In Proc.
of the European Conference on Speech Communications and Technology (EUROSPEECH), Berln, Germany.
Franz J.
Och. 2000.
GIZA++: Training of statistical translation models.
http://www.fjoch.com/GIZA++.html. Daniel Ortiz, Ismael Garca-Varea, Francisco Casacuberta, Antonio Lagarda, and Jorge Gonzalez.
2003. On the use of statistical machine translation techniques within a memory-based translation system (AMETRA).
In Proc.
of Machine Translation Summit IX, pages 115??20, New Orleans, USA, September.
Fernando C.N.
Pereira and Michael D.
Riley. 1997.
Speech Recognition by Composition of Weighted Finite Automata.
In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing, Language, Speech and Communication series, pages 431??453.
The MIT Press, Cambridge, Massachusetts.
Alicia Perez, M.
Ines Torres, and Francisco Casacuberta.
2007. Speech translation with phrase based stochastic finite-state transducers.
In Proceedings of the 32nd International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2007), Honolulu, Hawaii USA, April 15-20.
IEEE. Maja Popovic, David Vilar, Hermann Ney, Slobodan Jovi?cic, and Zoran ?Saric.
2005. Augmenting a small parallel text with morpho-syntactic language.
In Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 41??8, Ann Arbor, Michigan, June.
Association for Computational Linguistics.
M. Ines Torres and Amparo Varona.
2001. k-tss language models in speech recognition systems.
Computer Speech and Language, 15(2):127??49.
Enrique Vidal.
1997. Finite-state speech-to-speech translation.
In Proc.
IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 1, pages 111??14, Munich, Germany, April .
Proceedings of the Second Workshop on Statistical Machine Translation, pages 64??1, Prague, June 2007.
c2007 Association for Computational Linguistics 1 Meta-Structure Transformation Model for Statistical Machine Translation Jiadong Sun, Tiejun, Zhao and Huashen Liang MOE-MS Key Lab of National Language Processing and speech Harbin Institute of Technology No.
92, West Da-zhi Street,Harbin Heilongjiang,150001,China jiadongsun@hit.edu.cn {tjzhao, hsliang }@mtlab.hit.edu.cn Abstract We propose a novel syntax-based model for statistical machine translation in which meta-structure (MS) and meta-structure sequence (SMS) of a parse tree are defined.
In this framework, a parse tree is decomposed into SMS to deal with the structure divergence and the alignment can be reconstructed at different levels of recombination of MS (RM).
RM pairs extracted can perform the mapping between the substructures across languages.
As a result, we have got not only the translation for the target language, but an SMS of its parse tree at the same time.
Experiments with BLEU metric show that the model significantly outperforms Pharaoh, a state-art-theart phrase-based system.
1 Introduction
The statistical approach has been widely used in machine translation, which use the noisy-channelbased model.
A joint probability model, proposed by Marcu and Wong (2002), is a kind of phrasebased one.
Och and Ney (2004) gave a framework of alignment templates for this kind of models.
All of the phrase-based models outperformed the word-based models, by automatically learning word and phrase equivalents from bilingual corpus and reordering at the phrase level.
But it has been found that phrases longer than three words have little improvement in the performance (Koehn, 2003).
Above the phrase level, these models have a simple distortion model that reorders phrases independently, without consideration of their contents and syntactic information.
In recent years, applying different statistical learning methods to structured data has attracted various researchers.
Syntax-based MT approaches began with Wu (1997), who introduced the Inversion Transduction Grammars.
Utilizing syntactic structure as the channel input was introduced into MT by Yamada (2001).
Syntax-based models have been presented in different grammar formalisms.
The model based on Head-transducer was presented by Alshawi (2000).
Daniel Gildea (2003) dealt with the problem of the parse tree isomorphism with a cloning operation to either tree-tostring or tree-to-tree alignment models.
Ding and Palmer (2005) introduced a version of probabilistic extension of Synchronous Dependency Insertion Grammars (SDIG) to deal with the pervasive structure divergence.
All these approaches don?t model the translation process, but formalize a model that generates two languages at the same time, which can be considered as some kind of tree transducers.
Graehl and Knight (2004) described the use of tree transducers for natural language processing and addressed the training problems for this kind of transducers.
In this paper, we define a model based on the MS decomposition of the parse trees for statistical machine translation, which can capture structural variations and has a proven generation capacity.
During the translation process of our model, the parse tree of the source language is decomposed into different levels of MS and then transformed into the ones of the target language in the form of RM.
The source language can be reordered according to the structure transformation.
At last, the target translation string is generated in the scopes of RM.
In the framework of this model, 64 2 Figure 1: MS and the SMS and RM for a given parser tree the RM transformation can be regarded as production rules and be extracted automatically from the bilingual corpus.
The overall translation probability is thus decomposed.
In the rest of this paper, we first give the definitions for MS, SMS, RM and the decomposition of the parse tree in section 2.1, we give a detailed description of our model in section 2.2, section 3 describes the training details and section 4 describes the decoding algorithms, and then the experiment (section 5) proves that our model can outperform the baseline model, pharaoh, under the same condition.
2 The
model 2.1 MS for a parse tree A source language sentence (s1 s2 s3 s4 s5 s6), and its parse tree S-P, are given in Figure 1.We also give the translation of the sentence, which is illustrated as (t1 t2 t3).Its parse tree is T-P.
Definition 1 MS of a parse tree We call a sub-tree a MS of a parse tree, if it satisfies the following constraints: 1.
An MS should be a sub-tree of a parse tree 2.
Its direct sons of the leaf nodes in the subtree are the words or punctuations of the sentence For example, each of the sub-trees in the righthand of Figure 1 is an MS for the parse tree of S-P.
The sub-tree of [I [G, D, H]] of S-P is not an MS, because the direct sons of the leaf nodes, G, D, H, are not words in the sentence of (s1 s2 s3 s4 s5 s6).
Definition 2 SMS and RM A sequence of MS is called a meta-structure sequence (SMS) of a parse tree if and only if, 1.
Its elements are MS of the parse tree 2.
The parse tree can be reconstructed with the elements in the same order as in the sequence.
It is denoted as SMS [T(S)].
1 Two
examples for the concept of SMS can be found in Figure1.
RM(recombination of MS) is a sub-sequence of SMS.
We can express an SMS as different )]([ 1 STRM k.The parse tree of S-P in Figure1 is decomposed into SMS and expressed in the framework of RM.
The two RM, ][ 2 1 PSRM ??, are used to express its parse tree in Figure1.It is noted that there is structure divergence between the two parse trees in Figure1.
The corresponding node of Node I in the tree S-P cannot be found in the tree T-P.
But under the conception of RM, the structure alignments can be achieved at the level of RM, which is illustrated in Figure2.
Figure2.The RM alignments for S-P and T-P 1 T[S] denotes the parse tree of a given sentence f and e denote the foreign and target sentences 65 3 In Figure2, both of the parse trees are decomposed and reconstructed in the forms of RM.
The alignments based on RM are illustrated at the same time.
2.2 Description
of the model In the framework of Statistical machine translation, the task is to find the sentence e for the given foreign language f, which can be described in the following formulation.
)}|(maxarg ~ fePe e = (1) To make the model have the ability to model the structure transformation, some hidden variables are introduced into the probability equation.
To make the equations simple to read, we take some denotations different from the above definitions.
SMS[T(S)] is denoted as SM[T(S)].
The first variable is the SM[T(S)], we induce the equation as follows?
?? = ))(( )|)]([,()|( fTSM ffTSMePfeP ))],([|()|)]([( )]([ ffTSMePffTSMP fTSM ??
= ?2?
?? = )](SM[ ))],([|)](SM[( ))],([|( eT ffTSMeTeP ffTSMeP ?
?? = )](SM[ ))],([|)](SM[( eT ffTSMeTP ))],([)],(SM[|( ffTSMeTeP (3) In order to simplify this model we have two assumptions: An assumption is that the generation of SMS [T (e)] is only related with SMS[T(f)]: ))],([|)]([( ffTSMeTSMP )])([|)]([( fTSMeTSMP??
(4) Here we do all segmentations for any SMS of [T (f)] to get different )]([ 1 fTRM k . ? = = k i ii fRMT fTRMeTRMP fTSMeTSMP 1 )]( )])([|)]([( )])([|)]([( (5) The use of RM is to decompose bi-lingual parse trees and get the alignments in different hierarchical levels of the structure.
Now we have another assumption that all )|)]([( ffTSMP should have the same probability . A simplified form for this model is derived: =)|( feP  ????
 ))(())((fTSM eTSM ))],([)],([|( )])([|)]([( )]([ 1 ffTRMeTRMeP fTRMeTRMP ii fTRM k i ii  ????
= (6), Where ))],([)],([|( ffTRMeTRMeP ii can be regarded as a lexical transformation process, which will be further decomposed.
In order to model the direct translation process better by extending the feature functions, the direct translation probability is obtained in the framework of maximum entropy model: ( ) () () ? ??
= = = )]([)],([,1 1 )],([)],([,exp[ )],([)],([,exp[ | fTSMeTSMe M m mm M m mm ffTSMeTSMe ffTSMeTSMe feP h h   (7) We can achieve the translation according to the function below: ( ){ } ??
= = M m mm ffTSMeTSMee h 1 )],([)],([,exp[maxarg ~  (8) A detailed list of the feature functions for the model and some explanations are given as below: ere the symbol L (RM i [T(S)]) denotes the 66 4 words belonging to this sub-structure in the sentence.
In Figure1, L (RM 1 ) denotes the words, s1 s2 s3, in the source language.
This part of transformation happens in the scope of each RM, which means that all the words in any RM can be transformed into the target language words just in the way of phrase-based model, serving as another reordering factor at a different level: () ??
= = k i ii fTRMLeTRMLPfe h 1 3 )])([(|)]))([((log, (11) () ??
= = k i ii eTRMLfTRMLPfe h 1 4 )])([(|)]))([((log, (12) nsformed to target languages, the source language is reordered at the RM level first.
In this process, only the knowledge of the structure is taken into consideration.
It is obvious that a lot of sentences in the source language can have the same RM.
So this model has better generative ability.
At the same time, RM is a subsequence of SMS, which consists of different hierarchical MS.
So RM is a structure, which can model the structure mapping across the sub-tree structure.
By decomposing the source parse tree, the isomorphic between the parse trees can be obtained, at the level of RM.
When reordering at the RM level, this model just takes an RM as a symbol, and it can perform a long distance reordering job according to the knowledge of RM alignments.
3 Training
For training the model, a parallel tree corpus is needed.
The methods and details are described as follows: 3.1 Decomposition of the parse tree To reduce the amount of MS used in decoding and training, we take some constrains for the MS.
?1?.The height of the sub-tree shouldn?t be greater than a fixed value ; ? 2?.
?? ??
)( )( heightN nodesLeafN Given a parse tree, we get the initial SMS in such a top -down and leftto ?right way.
Any node is deleted if the sub-tree can?t satisfy the constrains (1), (2).
Figure3. Decomposition of a parse tree 67 RMS for Ch-Parse Tree RMS for EN-Parse Tree Pro for transformation AP[AP[AP[a-a]-usde]-m] NPB [DT-JJ-NN-PUNC.] 0.000155497 AP[AP[AP[r-a]-usde]-m] NPB[PDT-DT-JJ-NN] 0.0151515 AP[AP[BMP[m-q]-a]-usde] wj ADVP [RB-RB-PUNC.] 0.00344828 AP[AP[BMP[m-q]-a]-usde] wj DT CD JJ NNS PUNC 0.0833333 AP[AP[BMP[m-q]-a]-usde] wj DT JJ NN NNS PUNC.
0.015625 Table 1 some examples of the RM transformation RM1 RM2 RM3 P(RM3|RM1,RM2) IN NP-A[NPB[PRP-NN] IN 0.2479237 NPB NP-A[NPB[PRP-NN] VBZ 0.2479235 IN NP-A[NPB[PRP-NN] MD 0.6458637 <s> NP-A[NPB[PRP-NN] VBD 0.904308 Table 2 Examples for the 3-gram structure model of RM Generate all of the SMS by deleting a node in any Ms to generate new SMS, applying the same operation to any SMS 3.2 Parallel SMS and Estimation of the parameters for RM transformations We can get bi-lingual SMS by recombining all the possible SMS obtained from the parallel parse trees.
nm?? Parallel SMS can be obtained if m is the number of SMS for a parse tree in the source language, n for the target one.
The alignments of the parallel MS and extraction can be performed in such a simple way.
Given the parallel tree corpus, we first get the alignments based on the level of words, for which we used GIZA++ in both of the directions.
According to the knowledge of the word alignments, we derived the alignments of leave nodes of the given parse trees, which are the direct root nodes of the words.
Then all the knowledge of the words is discarded for the RM extraction.
The next step for the extraction of the RM is based on the popular phrase-extraction algorithm of the phrasebased statistical machine translation model.
The present alignment and phrase extraction methods can be applied to the extraction of the MS and RM [T(S)].
),( ),( )|( EiFi RM EIFi FiEI RMRMCount RMRMCount RMRMP Ei ??
= ),( BAountC is the expected number of times A is aligned with B in the training corpus.Table1 shows some parameters for this part in the model.
Training n-gram model for the monolingual structure model is based on the English RM of each parse tree, selected from the parallel tree corpus.
The 3-gram structure model is defined as follows: = ? )])([)],([|)]([( 12 eTRMeTRMeTRMP iiI ),,( ),,( 12 12 jII j III RMRMRMCount RMRMRMCount ? ? ??
),,( CBAountC is the times of the situation, in which the RM is consecutive sub-trees of the parse trees in the training set.
Some 3-gram parameters in the training task are given in Table2.
We didn?t meet with the serious data sparseness problem in this part of work, because most of the MS structures have occurred enough times for parameters estimation.
But we still set some fixed value for the unseen parameters in the training set.
4 Decoding
A beam search algorithm is applied to this model for decoding, which is based on the frame of the beam search for phrase-based statistical machine translation (Koehn et al, 03).
Here the process of the hypothesis generation is presented.
Given a sentence and its parse tree, all the possible candidate RM are collected, which can cover a part of the parse tree at the bottom.
With the candidates, the hypotheses can be formed and extended.
For example, all the parse tree?s leaf nodes of a Chinese sentence in Figure4, are covered by [r], [ pron ] and VP[vg-BNP[pron-n]] in the order of choosing candidate RM (1), (2), (3)}.
68 6 Figure4.
Process of translation based on RM ),( VBDWRBr (1)  ?how did ),( PRPpron (2) ??you ]])[[ ]],[[( NNDTNPBVBVP npronBNPvgVP ? ? (3)    ?find the information Before the next expansion of a hypothesis, the words in the scope of the present RM are translated into the target language and the corresponding )]([ eTRM i is generated.
For example, when ),( VBDWRBr, is used to expand the hypothesis, the words in the sub-tree are translated into the target language,  ?how did.
We also need to calculate the cost for the hypotheses according to the parameters in the model to perform the beam search.
The task for the beam search is to find the hypothesis with the least cost.
When the expansion of a hypothesis comes to the final state, the target language is generated.
All of the leave nodes of the parse tree for the source language are covered.
The parser for the target language isn?t used for decoding.
But a target SMS is generated during the process of decoding to achieve better reordering performance.
5 Experiments
The experiment was conducted for the task of Chinese-to-English translation.
A corpus, which consists of 602,701 sentence pairs, was used as the training set.
We took CLDC 863 test set as our test set (http://www.chineseldc.org/resourse.asp), which consists of 467 sentences with an average length of 14.287 Chinese words and 4 references.
To evaluate the result of the translation, the BLEU metric (Papineni et al.2002) was used.
5.1 The
baseline System used for comparison was Pharaoh (Koehn et al., 2003; Koehn, 2004), which uses a beam search algorithm for decoding.
In its model, it takes the following features: language model, phrase translation probability in the two directions, distortion model, word penalty and phrase penalty, all of which can be achieved with the training toolkits distributed by Koehn.
The training set and development set mentioned above were used to perform the training task and to tune the feature weights by the minimum error training algorithm.
All the other settings were the same as the default ones.
SRI Language Modeling Toolkit was used to train a 3-gram language model.
After training, 164 MB language model were obtained.
5.2 Our
model All the common features shared with Pharaoh were trained with the same toolkits and the same corpus.
Besides those features, we need to train the structure transformation model and the monolingual structure model for our model.
First, 10,000 sentence pairs were selected to achieve the 69 7 BLEU-n n-gram precisions System 4 1 2 3 4 5 6 7 8 Pharaoh 0.2053 0.6449 0.4270 0.2919 0.2053 0.1480 0.1061 0.0752 0.0534 Ms system 0.2232 0.6917 0.4605 0.3160 0.2232 0.1615 0.1163 0.0826 0.0587 Table3.
Comparison of Pharaoh and our system Features System P lm (e) P(RT) P( IRT ) P w ( f|e ) P w ( e|f ) Word Phr Ph(RM) Pharaoh 0.151 --------0.08 0.14 -0.29 0.26 ----MS system 0.157 0.16 0.23 0.06 0.11 -0.20 0.22 0.36 Table4.Feature weights obtained by minimum error rate training on development set training set for this part of task.
The Collins parser and a Chinese parser of our own lab were used.
After processing this corpus, we get a parallel tree corpus.
SRI Language Modeling Toolkits were used again to train this part of parameters.
In this experiment, we set 3=,and 5.1= . 149MB )]([ sTRMS pairs and a 25 MB 3-gram monolingual structure model were obtained.
6. Conclusion and Future work A framework for statistical machine translation is created in this paper.
The results of the experiments show that this model gives better performance, compared with the baseline system.
This model can incorporate the syntactic information into the process of translation and model the sub-structure projections across the parallel parse trees.
The advantage of this frame work lies in that the reordering operations can be performed at the different levels according to the hierarchical RM of the parse tree.
But we should notice that some independent assumptions were made in the decomposition of the parse tree.
In the future, a proper method should be introduced into this model to achieve the most possible decomposition of the parse tree.
In fact, we can incorporate some other feature functions into the model to model the structure transformation more effectively.
Acknowledgement Thanks to the reviewers for their reviews and comments on improving our presentation of this paper.
References A.P.Dempster,N.M.Laird, and D.B.Rubin 1977.Maximum likelihood from imcomplete data via the EM algorithm.
Journal of the Royal Statistical Society, 39(Ser B):1-38.
Christoph Tillman.
A projection extension algorithm for statistical machine translation.
Proceedings of the Conference on Empirical Methods in Natural Language Processing, Sapporo, Japan, June 30-July 4, 2003, 1-8.
Daniel Gildea.2003.Loosely tree based alignment for machine translation.
In Proceedings of ACL-03 Daniel Marcu, William Wong.
A phrase-based, joint probability model for statistical machine translation.
Proceedings of the Conference on Empirical Methods in Natural Language Processing, Philadelphia, PA, USA, July 11-13, 2002, 133-139.
Dekai Wu.
1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):3-403.
F.Casacuberta, E.
Vidal: Machine Translation with Inferred Stochastic Finite-state Transducers.
Computational Linguistics, Vol.
30, No.
2, pp.
205-225, June 2004 Franz J.
Och, C.
Tillmann, Hermann Ney.
Improved alignment models for statistical machine translation.
Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP), College Park, MD, USA, June 21-22, 1999, 20-28.
Franz J.
Och, Hermann Ney.2002 Discriminative training and maximum entropy models.
In Proceedings of ACL-00, pages 440-447, Hong Kong, Octorber.
Hiyan Alshawi, Srinvas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as collections of finite state head transducers Computational Linguistics, 26(1):45-60.
Ilya D.
Melamed. Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons.
Proceedings of the Third Workshop on Very Large Corpora, Boston, USA, July 30, 1995, 197-211.
Jonathan Graehl Kevin Knight Training Tree Transducers In Proceedings of NAACL-HLT 2004, pages 105-112.
Kenji Yamada and Kevin Knight 2001.
A Syntax-based statistical translation model.
In Proceedings of the 39th Annual Meeting of the association for computational Linguists(ACL 01), Toulouse, France, July 6-11 Michael John Collins.
1999. Head-driven statistical Models for Natural Language Parsing.
Ph.D. thesis,University of Pennsyvania,Philadelphia.
P. Koehn, Franz Josef Och, Daniel Marcu.
Statistical phrase-based translation.
Proceedings of the Conference on Human Language Technology, Edmonton, Canada, May 27-June 1, 2003, 127-133.
P. Koehn: Pharaoh: a Beam Search Decoder for Phrase-based Statistical Machine Translation Models . Meeting of the American Association for machine translation(AMTA), Washington DC, pp.
115-124 Sep./Oct.
2004 Peter F.
Brown,Stephen A.
Della Pietra,Vincent J.Della Pietra, and Robert Merrcer.1993.
The mathematics of statistical machine translation:Parameter estimation.Computational Linguistics,19(2).:263-311.
Quirk, Chris, Arul Menezes, and Colin Cherry.
Dependency Tree Translation.
Microsoft Research Technical Report: MSR-TR-2004-113.
Regina Barzilay and Lillian Lee.
2003. Learning to paraphrase: An supervised approach using multiple-sequence alignment.
In Proceedings of HLT/NAACL S.
Nie en, H.
Ney: Statistical Machine Translation with Scarce Resources using Morpho-syntactic Information.
Computational Linguistics, Vol.
30 No.
2, pp.
181-204, June 20 Yuan Ding and Martha Palmer.
2005. Machine translation using probabilistic synchronous dependency insert grammars.
In Proceedings of 43rd Annual Meeting of the NAACL-HLT2004, pages 273-280 .
Proceedings of the Second Workshop on Statistical Machine Translation, pages 72??9, Prague, June 2007.
c2007 Association for Computational Linguistics Training Non-Parametric Features for Statistical Machine Translation Patrick Nguyen, Milind Mahajan and Xiaodong He Microsoft Corporation 1 Microsoft Way, Redmond, WA 98052 {panguyen,milindm,xiaohe}@microsoft.com Abstract Modern statistical machine translation systems may be seen as using two components: feature extraction, that summarizes information about the translation, and a log-linear framework to combine features.
In this paper, we propose to relax the linearity constraints on the combination, and hence relaxing constraints of monotonicity and independence of feature functions.
We expand features into a non-parametric, non-linear, and high-dimensional space.
We extend empirical Bayes reward training of model parameters to meta parameters of feature generation.
In effect, this allows us to trade away some human expert feature design for data.
Preliminary results on a standard task show an encouraging improvement.
1 Introduction
In recent years, statistical machine translation have experienced a quantum leap in quality thanks to automatic evaluation (Papineni et al., 2002) and errorbased optimization (Och, 2003).
The conditional log-linear feature combination framework (Berger, Della Pietra and Della Pietra, 1996) is remarkably simple and effective in practice.
Therefore, recent efforts (Och et al., 2004) have concentrated on feature design ??wherein more intelligent features may be added.
Because of their simplicity, however, log-linear models impose some constraints on how new information may be inserted into the system to achieve the best results.
In other words, new information needs to be parameterized carefully into one or more real valued feature functions.
Therefore, that requires some human knowledge and understanding.
When not readily available, this is typically replaced with painstaking experimentation.
We propose to replace that step with automatic training of non-parametric agnostic features instead, hopefully relieving the burden of finding the optimal parameterization.
First, we define the model and the objective function training framework, then we describe our new non-parametric features.
2 Model
In this section, we describe the general log-linear model used for statistical machine translation, as well as a training objective function and algorithm.
The goal is to translate a French (source) sentence indexed by t, with surface string ft.
Among a set of Kt outcomes, we denote an English (target) a hypothesis with surface string e(t)k indexed by k.
2.1 Log-linear Model The prevalent translation model in modern systems is a conditional log-linear model (Och and Ney, 2002).
From a hypothesis e(t)k, we extract features h(t)k, abbreviated hk, as a function ofe(t)k andft.
The conditional probability of a hypothesis e(t)k given a source sentence ft is: pk definesp(e(t)k |ft) defines exp[hk]Z ft;, 72 where the partition function Zft; is given by: Zft; = summationdisplay j exp[hj].
The vector of parameters of the model , gives a relative importance to each feature function component.
2.2 Training
Criteria In this section, we quickly review how to adjust  to get better translation results.
First, let us define the figure of merit used for evaluation of translation quality.
2.2.1 BLEU
Evaluation The BLEU score (Papineni et al., 2002) was defined to measure overlap between a hypothesized translation and a set of human references.
n-gram overlap counts {cn}4n=1 are computed over the test set sentences, and compared to the total counts of n-grams in the hypothesis: cn,(t)k defines max.
# of matching n-grams for hyp.
e(t)k, an,(t)k defines # of n-grams in hypothesis e(t)k. Those quantities are abbreviated ck and ak to simplify the notation.
The precision ratio Pn for an ngram order n is: Pn defines summationtext tc n,(t) ksummationtext ta n,(t) k . A brevity penalty BP is also taken into account, to avoid favoring overly short sentences: BP defines min1;exp(1 ??ra)}, where r is the average length of the shortest sentence1, and a is the average length of hypotheses.
The BLEU score the set of hypotheses {e(t)k } is: B({e(t)k }) defines BP exp parenleftbigg 4summationdisplay n=1 1 4 logPn parenrightbigg . 1As implemented by NIST mteval-v11b.pl.
Oracle BLEU hypothesis: There is no easy way to pick the set hypotheses from an n-best list that will maximize the overall BLEU score.
Instead, to compute oracle BLEU hypotheses, we chose, for each sentence independently, the hypothesis with the highest BLEU score computed for a sentence itself.
We believe that it is a relatively tight lower bound and equal for practical purposes to the true oracle BLEU.
2.2.2 Maximum
Likelihood Used in earlier models (Och and Ney, 2002), the likelihood criterion is defined as the likelihood of an oracle hypothesis e(t)k??, typically a single reference translation, or alternatively the closest match which was decoded.
When the model is correct and infinite amounts of data are available, this method will converge to the Bayes error (minimum achievable error), where we define a classification task of selecting k??against all others.
2.2.3 Regularization
Schemes One can convert a maximum likelihood problem into maximum a posteriori using Bayes??rule: argmax  productdisplay t p(|{e(t)k,ft}) = argmax  productdisplay t pkp0(), where p0() is the prior distribution of .
The most frequently used prior in practice is the normal prior (Chen and Rosenfeld, 2000): logp0() defines??||| 2 2?2 ??log|?|, where ?2 > 0 is the variance.
It can be thought of as the inverse of a Lagrange multiplier when working with constrained optimization on the Euclidean norm of .
When not interpolated with the likelihood, the prior can be thought of as a penalty term.
The entropy penalty may also be used: H defines??1T Tsummationdisplay t=1 Ktsummationdisplay k=1 pk logpk.
Unlike the Gaussian prior, the entropy is independent of parameterization (i.e., it does not depend on how features are expressed).
73 2.2.4 Minimum Error Rate Training A good way of training is to minimize empirical top-1 error on training data (Och, 2003).
Compared to maximum-likelihood, we now give partial credit for sentences which are only partially correct.
The criterion is: argmax  summationdisplay t B({e(t)?k }) : e(t)?k = argmax e(t)j pj.
We optimize the  so that the BLEU score of the most likely hypotheses is improved.
For that reason, we call this criterion BLEU max.
This function is not convex and there is no known exact efficient optimization for it.
However, there exists a linear-time algorithm for exact line search against that objective.
The method is often used in conjunction with coordinate projection to great success.
2.2.5 Maximum
Empirical Bayes Reward The algorithm may be improved by giving partial credit for confidence pk of the model to partially correct hypotheses outside of the most likely hypothesis (Smith and Eisner, 2006): 1 T Tsummationdisplay t=1 Ktsummationdisplay k=1 pk logB({ek(t)}).
Instead of the BLEU score, we use its logrithm, because we think it is exponentially hard to improve BLEU.
This model is equivalent to the previous model when pk give all the probability mass to the top-1.
That can be reached, for instance, when  has a very large norm.
There is no known method to train against this objective directly, however, efficient approximations have been developed.
Again, it is not convex.
It is hoped that this criterion is better suited for high-dimensional feature spaces.
That is our main motivation for using this objective function throughout this paper.
With baseline features and on our data set, this criterion also seemed to lead to results similar to Minimum Error Rate Training.
We can normalize B to a probability measure b({e(t)k }).
The empirical Bayes reward also coincides with a divergence D(p||b).
2.3 Training
Algorithm We train our model using a gradient ascent method over an approximation of the empirical Bayes reward function.
2.3.1 Approximation
Because the empirical Bayes reward is defined over a set of sentences, it may not be decomposed sentence by sentence.
This is computationally burdensome.
Its sufficient statistics are r, summationtexttck andsummationtext tak.
The function may be reconstructed in a firstorder approximation with respect to each of these statistics.
In practice this has the effect of commuting the expectation inside of the functional, and for that reason we call this criterion BLEU soft.
This approximation is called linearization (Smith and Eisner, 2006).
We used a first-order approximation for speed, and ease of interpretation of the derivations.
The new objective function is: J defines log BP + 4summationdisplay n=1 1 4 log summationtext t Ec n,(t) ksummationtext t Ea n,(t) k, where the average bleu penalty is: log BP defines min0;1 ??r Ek,ta1,(t)k }.
The expectation is understood to be under the current estimate of our log-linear model.
Because BP is not differentiable, we replace the hard min function with a sigmoid, yielding: log BP ?u(r??Ek,ta1,(t)k ) parenleftBigg 1??r Ek,ta1,(t)k parenrightBigg, with the sigmoid function u(x) defines a soft step function: u(x) defines 11 +e?x, with a parameter ? ??1.
2.3.2 Gradients
and Sufficient Statistics We can obtain the gradients of the objective function using the chain rule by first differentiating with respect to the probability.
First, let us decompose the log-precision of the expected counts: log ?Pn = log Ecn,(t)k ??log Ean,(t)k . 74 Each n-gram precision may be treated separately.
For each n-gram order, let us define sufficient statistics ? for the precision: ?c defines summationdisplay t,k (?pk)ck; ?a defines summationdisplay t,k (?pk)ak, where the gradient of the probabilities is given by: ?pk = pk(hk ??h), with: hdefines Ktsummationdisplay j=1 pjhj.
The derivative of the precision ?Pn is: ?log ?Pn = 1T bracketleftbigg?c  Eck ??
?a Eak bracketrightbigg For the length, the derivative of log BP is: u(r?Ea) bracketleftbigg (ra ??1)[1 ?u(r??Ea)]?
+ r(Ea)2 bracketrightbigg ?a1, where ?a1 is the 1-gram component of ?a.
Finally, the derivative of the entropy is: ?H = summationdisplay k,t (1 + logpk)?pk.
2.3.3 RProp
For all our experiments, we chose RProp (Riedmiller and Braun, 1992) as the gradient ascent algorithm.
Unlike other gradient algorithms, it is only based on the sign of the gradient components at each iteration.
It is relatively robust to the objective function, requires little memory, does not require meta parameters to be tuned, and is simple to implement.
On the other hand, it typically requires more iterations than stochastic gradient (Kushner and Yin, 1997) or L-BFGS (Nocedal and Wright, 1999).
Using fairly conservative stopping criteria, we observed that RProp was about 6 times faster than Minimum Error Rate Training.
3 Adding
Features The log-linear model is relatively simple, and is usually found to yield good performance in practice.
With these considerations in mind, feature engineering is an active area of research (Och et al., 2004).
Because the model is fairly simple, some of the intelligence must be shifted to feature design.
After having decided what new information should go in the overall score, there is an extra effort involved in expressing or parameterizing features in a way which will be easiest for the model learn.
Experimentation is usually required to find the best configuration.
By adding non-parametric features, we propose to mitigate the parameterization problem.
We will not add new information, but rather, propose a way to insulate research from the parameterization.
The system should perform equivalently invariant of any continuous invertible transformation of the original input.
3.1 Existing
Features The baseline system is a syntax based machine translation system as described in (Quirk, Menezes and Cherry, 2005).
Our existing feature set includes 11 features, among which the following: ??Target hypothesis word count.
??Treelet count used to construct the candidate.
??Target language models, based on the Gigaword corpus (5-gram) and target side of parallel training data (3-gram).
??Order models, which assign a probability to the position of each target node relative to its head.
??Treelet translation model.
??Dependency-based bigram language models.
3.2 Re-ranking Framework Our algorithm works in a re-ranking framework.
In particular, we are adding features which are not causal or additive.
Features for a hypothesis may not be accumulating by looking at the English (target) surface string words from the left to the right and adding a contribution per word.
Word count, for instance, is causal and additive.
This property is typically required for efficient first-pass decoding.
Instead, we look at a hypothesis sentence as a whole.
Furthermore, we assume that the Kt-best list provided to us contains the entire probability space.
75 In particular, the computation of the partition function is performed over all Kt-best hypotheses.
This is clearly not correct, and is the subject of further study.
We use the n-best generation scheme interleaved with  optimization as described in (Och, 2003).
3.3 Issues
with Parameterization As alluded to earlier, when designing a new feature in the log-linear model, one has to be careful to find the best embodiment.
In general, a set of features must satisfy the following properties, ranked from strict to lax: ??Linearity (warping) ??Monotonicity ??Independence (conjunction) Firstly, a feature should be linearly correlated with performance.
There should be no region were it matters less than other regions.
For instance, instead of a word count, one might consider adding its logarithm instead.
Secondly, the ?goodness??of a hypothesis associated with a feature must be monotonic.
For instance, using the signed difference between word count in the French (source) and English (target) does not satisfy this.
(In that case, one would use the absolute value instead).
Lastly, there should be no inter-dependence between features.
As an example, we can consider adding multiple language model scores.
Whether we should consider ratios those of, globally linearly or log-linearly interpolating them, is open to debate.
When features interact across dimensions, it becomes unclear what the best embodiment should be.
3.4 Non-parametric Features A generic solution may be sought in non-parametric processing.
Our method can be derived from a quantized Parzen estimate of the feature density function.
3.4.1 Parzen
Window The Parzen window is an early empirical kernel method (Duda and Hart, 1973).
For an observation hm, we extrapolate probability mass around it with a smoothing window ().
The density function is: p(h) = 1M Ksummationdisplay m=1 (h?hm), assuming () is a density function.
Parzen windows converge to the true density estimate, albeit slowly, under weak assumptions.
3.4.2 Bin
Features One popular way of using continuous features in log-linear models is to convert a single continuous feature into multiple ?bin??features.
Each bin feature is defined as the indicator function of whether the original continuous feature was in a certain range.
The bins were selected so that each bin collects an equal share of the probability mass.
This is equivalent to the maximum likelihood estimate of the density function subject to a fixed number of rectangular density kernels.
Since that mapping is not differentiable with respect to the original features, one may use sigmoids to soften the boundaries.
Bin features are useful to relax the requirements of linearity and monotonicity.
However, because they work on each feature individually, they do not address the problem of inter-dependence between features.
3.4.3 Gaussian
Mixture Model Features Bin features may be generalized to multidimensional kernels by using a Gaussian smoothing window instead of a rectangular window.
The direct analogy is vector quantization.
The idea is to weight specific regions of the feature space differently.
Assuming that we have M Gaussians each with mean vector m and diagonal covariance matrix Cm, and prior weight wm.
We will add m new features, each defined as the posterior in the mixture model: hm defines wmN(h;m,Cm)summationtext rwrN(h;r,Cr) . It is believed that any reasonable choice of kernels will yield roughly equivalent results (Povey et al., 2004), if the amount of training data and the number of kernels are both sufficiently large.
We show two methods for obtaining clusters.
In contrast with bins, lossless representation becomes rapidly impossible.
ML kernels: The canonical way of obtaining cluster is to use the standard Gaussian mixture training.
First, a single Gaussian is trained on the whole data set.
Then, the Gaussian is split into two Gaussians, with each mean vector perturbed, and the Gaussians are retrained using maximum-likelihood in an 76 expectation-maximization framework (Rabiner and Huang, 1993).
The number of Gaussians is typically increased exponentially.
Perceptron kernels: We also experimented with another quicker way of obtaining kernels.
We chose an equal prior and a global covariance matrix.
Means were obtained as follows: for each sentence in the training set, if the top-1 candidate was different from the approximate maximum oracle BLEU hypothesis, both were inserted.
It is a quick way to bootstrap and may reach the oracle BLEU score quickly.
In the limit, GMMs will converge to the oracle BLEU.
In the next section, we show how to reestimate these kernels if needed.
3.5 Re-estimation Formul Features may also be trained using the same empirical maximum Bayes reward.
Let  be the hyperparameter vector used to generate features.
In the case of language models, for instance, this could be backoff weights.
Let us further assume that the feature values are differentiable with respect to .
Gradient ascent may be applied again but this time with respect to .
Using the chain rule: ?J = (?h)(?hpk)(?pkJ), with ?hpk = pk(1 ?pk).
Let us take the example of re-estimating the mean of a Gaussian kernel m: ?mhm = ?wmhm(1 ?hm)C??m (m ?h), for its own feature, and for other posteriors r negationslash= m: ?mhr = ?wrhrhmC??m (m ?h), which is typically close to zero if no two Gaussians fire simultaneously.
4 Experimental
Results For our experiments, we used the standard NIST MT-02 data set to evaluate our system.
4.1 NIST
System A relatively simple baseline was used for our experiments.
The system is syntactically-driven (Quirk, Menezes and Cherry, 2005).
The system was trained on 175k sentences which were selected from the NIST training data (NIST, 2006) to cover words in source language sentences of the MT02 development and evaluation sets.
The 5-gram target language model was trained on the Gigaword monolingual data using absolute discounting smoothing.
In a single decoding, the system generated 1000 hypotheses per sentence whenever possible.
4.2 Leave-one-out Training In order to have enough data for training, we generated our n-best lists using 10-fold leave-one-out training: base feature extraction models were trained on 9/10th of the data, then used for decoding the held-out set.
The process was repeated for all 10 parts.
A single  was then optimized on the combined lists of all systems.
That  was used for another round of 10 decodings.
The process was repeated until it reached convergence after 7 iterations.
Each decoding generated about 100 hypotheses, and there was relatively little overlap across decodings.
Therefore, there were about 1M hypotheses in total.
The combined list of all iterations was used for all subsequent experiments of feature expansion.
4.3 BLEU
Training Results We tried training systems under the empirical Bayes reward criterion, and appending either bin or GMM features.
We will find that bin features are essentially ineffective while GMM features show a modest improvement.
We did not retrain hyperparameters.
4.3.1 Convexity
of the Empirical Bayes Reward The first question to ask is how many local optima does the cost surface have using the standard features.
A complex cost surface indicates that some gain may be had with non-linear features, but it also shows that special care should be taken during optimization.
Non-convexity is revealed by sensitivity to initialization points.
Thus, we decided to initialize from all vertices of the unit hypercube, and since we had 11 features, we ran 211 experiments.
The histogram of BLEU scores on dev data after convergence is shown on Figure 1.
We also plotted the histogram of an example dimension in Figure 2.
The range of BLEU scores and lambdas is reasonably narrow.
Even though  seems to be bimodal, we see 77 that this does not seriously affect the BLEU score.
This is not definitive evidence but we provisionally pretend that the cost surface is almost convex for practical purposes.
24.8 24.9 25 25.1 25.2 25.3 25.4 0 200 400 600 800 BLEU score number of trained models Figure 1: Histogram of BLEU scores after training from 211 initializations.
??0 ??0 ??0 0 0 100 200 300 400 500 600 700  value number of trained models Figure 2: Histogram of one  parameter after training from 211 initializations.
4.3.2 Bin
Features A log-linear model can be converted into a bin feature model nearly exactly by setting  values in such a way that scores will be equal.
Equivalent weights (marked as ?original??in Figure 3) have the shape of an error function (erf): this is because the input feature is a cummulative random variable, which quickly converges to a Gaussian (by the central limit theorem).
After training the  weights for the log-linear model, weights may be converted into bins and re-trained.
On Figure 3, we show that relaxing the monotonicity constraint leads to rough values for .
Surprisingly, the BLEU score and objective on the training set only increases marginally.
Starting from  = 0, we obtained nearly exactly the same training objective value.
By varying the number of bins (20-50), we observed similar behavior as well.
0 10 20 30 40 50 ??.5 ??
??.5 0 0.5 1 bin id value original weights trained weights Figure 3: Values before and after training bin features.
Monotonicity constraint has been relaxed.
BLEU score is virtually unchanged.
4.3.3 GMM
Features Experiments were carried out with GMM features.
The summary is shown on Table 1.
The baseline was the log-linear model trained with the baseline features.
The baseline features are included in all systems.
We trained GMM models using the iterative mixture splitting interleaved with EM reestimation, split up to 1024 and 16384 Gaussians, which we call GMM-ML-1k and GMM-ML-16k respectively.
We also used the ?perceptron??selection features on the training set to bootstrap quickly to 300k Gaussians (GMM-PCP-300k), and ran the same algorithm on the development set (GMMPCP-2k).
Therefore, GMM-PCP-300k had 300k features, and was trained on 175k sentences (each with about 700 hypotheses).
For all experiments but ?unreg??(unregularized), we chose a prior Gaussian prior with variance empirically by looking at the development set.
For all but GMM-PCP-300k, regularization did not seem to have a noticeably positive effect on development BLEU scores.
All systems were seeded with the baseline log-linear model, and 78 all additional weights set to zero, and then trained with about 50 iterations, but convergence in BLEU score, empirical reward, and development BLEU score occurred after about 30 iterations.
In that setting, we found that regularized empirical Bayes reward, BLEU score on training data, and BLEU score on development and evaluation to be well correlated.
Cursory experiments revealed that using multiple initializations did not significantly alter the final BLEU score.
System Train Dev Eval Oracle 14.10 N/A N/A Baseline 10.95 35.15 25.95 GMM-ML-1k 10.95 35.15 25.95 GMM-ML-16k 11.09 35.25 25.89 GMM-PCP-2k 10.95 35.15 25.95 GMM-PCP-300k-unreg 13.00 N/A N/A GMM-PCP-300k 12.11 35.74 26.42 Table 1: BLEU scores for GMM features vs the linear baseline, using different selection methods and number of kernels.
Perceptron kernels based on the training set improved the baseline by 0.5 BLEU points.
We measured significance with the Wilcoxon signed rank test, by batching 10 sentences at a time to produce an observation.
The difference was found to be significant at a 0.9-confidence level.
The improvement may be limited due to local optima or the fact that original feature are well-suited for log-linear models.
5 Conclusion
In this paper, we have introduced a non-parametric feature expansion, which guarantees invariance to the specific embodiment of the original features.
Feature generation models, including feature expansion, may be trained using maximum regularized empirical Bayes reward.
This may be used as an end-to-end framework to train all parameters of the machine translation system.
Experimentally, we found that Gaussian mixture model (GMM) features yielded a 0.5 BLEU improvement.
Although this is an encouraging result, further study is required on hyper-parameter re-estimation, presence of local optima, use of complex original features to test the effectiveness of the parameterization invariance, and evaluation on a more competitive baseline.
References K.
Papineni, S.
Roukos, T.
Ward, W.-J.
Zhu. 2002.
BLEU: a method for automatic evaluation of machine translation.
ACL??2. A.
Berger, S.
Della Pietra, and V.
Della Pietra.
1996. A Maximum Entropy Approach to Natural Language Processing.
Computational Linguistics, vol 22:1, pp.
39??1. S.
Chen and R.
Rosenfeld. 2000.
A survey of smoothing techniques for ME models.
IEEE Trans.
on Speech and Audio Processing, vol 8:2, pp.
37??0. R.
O. Duda and P.
E. Hart.
1973. Pattern Classification and Scene Analysis.
Wiley & Sons, 1973.
H. J.
Kushner and G.
G. Yin.
1997. Stochastic Approximation Algorithms and Applications.
Springer-Verlag,1997.
National Institute of Standards and Technology.
2006. The 2006 Machine Translation Evaluation Plan.
J. Nocedal and S.
J. Wright.
1999. Numerical Optimization.
Springer-Verlag, 1999.
F. J.
Och. 2003.
Minimum Error Rate Training in Statistical Machine Translation.
ACL??3. F.
J. Och, D.
Gildea, S.
Khudanpur, A.
Sarkar, K.
Yamada, A.
Fraser, S.
Kumar, L.
Shen, D.
Smith, K.
Eng, V.
Jain, Z.
Jin, and D.
Radev. 2004.
A Smorgasbord of Features for Statistical Machine Translation.
HLT/NAACL??4. F.
J. Och and H.
Ney. 2002.
Discriminative Training and Maximum Entropy Models for Statistical Machine Translation.
ACL??2. D.
Povey, B.
Kingsbury, L.
Mangu, G.
Saon, H.
Soltau and G.
Zweig. 2004.
fMPE: Discriminatively trained features for speech recognition.
RT??4 Meeting.
C. Quirk, A.
Menezes and C.
Cherry. 2005.
Dependency Tree Translation: Syntactically Informed Phrasal SMT.
ACL??5. L.
R. Rabiner and B.-H.
Huang. 1993.
Fundamentals of Speech Recognition.
Prentice Hall.
M. Riedmiller and H.
Braun. 1992.
RPROP: A Fast Adaptive Learning Algorithm.
Proc. of ISCIS VII.
D. A.
Smith and J.
Eisner. 2006.
Minimum-Risk Annealing for Training Log-Linear Models. ACL-COLING??6 .
Proceedings of the Second Workshop on Statistical Machine Translation, pages 80??7, Prague, June 2007.
c2007 Association for Computational Linguistics Using Word Dependent Transition Models in HMM based Word Alignment for Statistical Machine Translation Xiaodong He Microsoft Research One Microsoft Way Redmond, WA 98052 USA xiaohe@microsoft.com Abstract In this paper, we present a Bayesian Learning based method to train word dependent transition models for HMM based word alignment.
We present word alignment results on the Canadian Hansards corpus as compared to the conventional HMM and IBM model 4.
We show that this method gives consistent and significant alignment error rate (AER) reduction.
We also conducted machine translation (MT) experiments on the Europarl corpus.
MT results show that word alignment based on this method can be used in a phrase-based machine translation system to yield up to 1% absolute improvement in BLEU score, compared to a conventional HMM, and 0.8% compared to a IBM model 4 based word alignment.
1 Introduction
Word alignment is an important step of most modern approaches to statistical machine translation (Koehn et al., 2003).
The classical approaches to word alignment are based on IBM models 1-5 (Brown et al., 1994) and the HMM based alignment model (Vogel et al., 1996) (Och and Ney, 2000a, 2000b), while recently discriminative approaches (Moore, 2006) and syntax based approaches (Zhang and Gildea, 2005) for word alignment are also studied.
In this paper, we present improvements to the HMM based alignment model originally proposed by (Vogel et al., 1996, Och and Ney, 2000a).
Although HMM based word alignment approaches give good performance, one weakness of it is the coarse transition models.
In the HMM based alignment model (Vogel et al., 1996), it is assumed that the HMM transition probabilities depend only on the jump width from the last state to the next state.
Therefore, the knowledge of transition probabilities given a particular source word e is not sufficiently modeled.
In order to improve transition models in the HMM based alignment, Och and Ney (2000a) extended the transition models to be word-class dependent.
In that approach, words of the source language are first clustered into a number of word classes, and then a set of transition parameters is estimated for each word class.
In (2002), Toutanova et al.modeled self-transition (i.e., jump width is zero) probability separately from other transition probabilities.
A word dependent self-transition model P(stay|e) is introduced to decide whether to stay at the current source word e at the next step, or jump to a different word.
It was also shown that with the assumption that a source word with fertility greater than one generates consecutive words in the target language, this probability approximates fertility modeling.
Deng and Byrne in (2005) improved this idea.
They proposed a word-to-phrase HMM in which a source word dependent phrase length model is used to model the approximate fertility, i.e., the length of consecutive target words generated by the source word.
It provides more powerful modeling of approximate fertility than the single P(stay|e) parameter.
However, these methods only model the probability of state occupancy rather than a full set of transition probabilities.
Important knowledge of jumping from e to another position, e.g., jumping 80 forward (monotonic alignment) or jumping backward (non-monotonic alignment), is not modeled.
In this paper, we present a method to further improve the transition models for HMM alignment model.
For each source word e, we not only model its self-transition probability, but also the probability of jumping from word e to a different word.
For this purpose, we estimate a full transition model for each source word.
A key problem for detailed word-dependent transition modeling is data sparsity.
In (Toutanova et al., 2002), the word dependent self-transition probability P(stay|e) is interpolated with the global HMM self-transition probability to alleviate the data sparsity problem, where an interpolation weight is used for all words and that weight is tuned on a hold-out set.
In the proposed word dependent transition model, because there are a large number of parameters to estimate, the data sparsity problem is even more severe.
Moreover, since the sparsity of different words are very different, it is difficult to find a one-size-fits-all interpolation weight, and therefore simple linear interpolation is not optimal.
In order to address this problem, we use Bayesian learning so that the transition model parameters are estimated by maximum a posteriori (MAP) training.
With the help of the prior distribution of the model, the training is regularized and results in robust models.
In the next section we briefly review modeling of transition probabilities in a conventional HMM alignment model (Vogel et al., 1996, Och and Ney, 2000a).
Then we describe the equations of MAP training for word dependent transition models.
In section 5, we present word alignment results that show significant alignment error rate reductions compared to the baseline HMM and IBM model 4.
We also conducted phrase-based machine translation experiments on the Europarl corpus, English ??
French track, and shown that the proposed method can lead to significant BLEU score improvement compared to the HMM and IBM model 4.
2 Baseline
HMM alignment model We briefly review the HMM based word alignment models (Vogel, 1996, Och and Ney, 2000a).
Let?s denote by 11 (,..., ) J J fff= as the French sentence, 11 (,..., ) I I e ee= as the English sentence, and 11 (,..., ) J J a aa= as the alignment that specifies the position of the English word aligned to each French word.
In the HMM based word alignment, a HMM is built at English side, i.e., each (position, word) pair, (, ) j ja a e, is a HMM state, which emits the French word f j. In order to mitigate the sparse data problem, it is assumed that the emission probability only depends on the English word, i.e., (|,) (|) j j jja ja p fae pfe=, and the transition probability only depends on the position of the last state and the length of the English sentence, i.e., 1 11 (|,,) (|,) j jj a jj p aa e I paa I ??
? = . Then, Vogel et al.(1996) give 1 11 1 1 (|) (|,)(|) j J J JI jj ja ja pf e pa a Ipf e ??
= ????
= ????
? (1) In the HMM of (Vogel et al., 1996), it is further assumed these transition probabilities 1 (|,) ??
??= jj p aia iI depend only on the jump width (i i'), i.e., 1 () (|,) () I l c ii pi i I c li = ? ??= ? ??
(2) Therefore, the transition probability 1 (|,) jj p aa I ?? depends on a j-1 but only through the distortion set {c(i i')}.
In (Och and Ney, 2000a), the word null is introduced to generate the French words that don't align to any English words.
If we denote by j_ the position of the last French word before j that aligns to a non-null English word, the transition probabilities 1 (|,) jj p aia iI ??
??= in (1) is computed as _ ( |,) (|,) jj p aia iI piiI??==%, where 0 0 if 0 (|,) (1 ) ( |, ) otherwise pi pi i I ppiiI =??
??= ??
??? ??
% state i=0 denotes the state of a null word at the English side, and p 0 is the probability of jumping to state 0, which is estimated from hold-out data.
For convenience, we denote by { } (|,), ( | ) ji p iiI pf e?? the HMM parameter set.
81 In the training stage, ? are usually estimated through maximum likelihood (ML) training, i.e., 11 arg max ( |, ) JI ML pf e ? ?= ? (3) and the efficient Expectation-Maximization algorithm can be used to optimize ? iteratively until convergence (Rabiner 1989).
For the interest of this paper, we elaborate transition parameter estimation with more details.
These transition probabilities { }(|,)p iiI?? is a multinomial distribution estimated according to (2), where at each iteration the distortion set {c(i i')} is the fractional count of transitions with jump width d = i i', i.e., 1 111 11 () Pr(, |,, ) JI JI jj ji cd a ia i d f e ??
+ == ??+?
? (4) where ? ' is the model obtained from the immediate previous iteration and these terms in (4) can be efficiently computed by using the ForwardBackward algorithm (Rabiner 1989).
In practice, we can bucket the distortion parameters {c(d)} into a few buckets as implemented in (Liang et al., 2006).
In our implementation, 15 buckets are used for c(??7), c(-6), ...
c(0), ..., c(??).
The probability mass for transitions with jump width larger than 6 is uniformly divided.
As suggested in (Liang et al., 2006), we also use two separate sets of distortion parameters for transitioning into the first state, and for transitioning out of the last state, respectively.
Finally, we further smooth transition probabilities with a uniform distribution as described in (Och and Ney, 2000a), __ 1 (|,) (1 )(|,) jj jj p aa I paa I I ??=??????.
After training, Viterbi decoding is used to find the best alignment sequence 1 ? J a . i.e., 1 1_ 1 ? arg max ( |, ) ( | ) j J J J jj ja a j apIpf = ? = ? ??
. 3 Word-dependent transition models in HMM based alignment model As discussed in the previous sections, conventional transition models that only depend on source word positions are not accurate enough.
There are only limited distortion parameters to model the transition between HMM states for all English words, and the knowledge of transition probabilities given a particular source word is not represented.
In order to improve the transition model in HMM, we extend the transition probabilities to be word dependent so that the probability of jumping from state a j_ to a j not only depends on a j_, but also depends on the English word at position a j_ . This gives _ 1 11 _ 1 (|) (|,,)(|) jj J J JI jja ja ja pf e pa a e Ipf e = ????
= ????
? . Compared to (1), we need to estimate the transition parameter _ _ (|,,) j jja pa a e I which is _j a e dependent.
Correspondingly, the HMM parameters we need to estimate are { } (|,,), ( | ) i ji p iie I pf e ??
??, which provides a much richer set of free parameters to model transition probabilities.
4 Bayesian
Learning for word-dependent transition models 4.1 Maximum a posteriori training Using ML training, we can obtain the estimation formula for word dependent transition probabilities { }(|,,)p iieI ?? similar as (2), i.e., 1 (;) (|,,) (;) ML I l c iie piieI c lie = ? ??= ? ??
(5) where at each training iteration the word dependent distortion set {c(i i';e)} is computed by 1 111 11 (;) ()Pr(, |,,) j JI JI ajj ji cde eeaia idfe ??
+ == = ??==+ ? ? (6) where d = i i' is the jump width, and () j a e e = is the Kronecker delta function that equals one if j a ee=, and zero otherwise.
However, for many non-frequent words, the data samples for c(d;e) is very limited and therefore may lead to a biased model that severely overfits to the sparse data.
In order to address this issue, maximum a posteriori (MAP) framework is applied (Gauvain and Lee, 1994).
In MAP training, an appropriate prior distribution is used to incorpo82 rate prior knowledge into the model parameter estimation, 1 11 arg max ( |, ) ( | ) J II MAP p fe g e ? ?= ??
(7) where the prior distribution 1 (|) I g e? characterizes the distribution of the model parameter set ? given the English sentence.
The relation between ML and MAP estimation is through the Bayes' theorem where the posterior distribution 1 111 1 (|,) ( |,)(|) J IJI I p fe pf e g ee?????, and 11 (|,) JI pf e ? is the likelihood function.
In transition model estimation, the transition model { }(|,,) i p iie I ??
?? is a multinomial distribution.
Its conjugate prior distribution is a Dirichlet distribution taking the following form (Bishop 2006), (), 1 1 1 (|,,)| (|,,) ii I vI ii i gpiieI e piieI ??
?? ??
= ??
?? (8) where { },i i v ?? is the set of hyper-parameters of the prior distribution.
Note that for mathematic tractability,,ii v ?? needs to be greater than 1, which is usually the case in practice.
Substitute (8) into (7) and using EM algorithm, we can obtain the iterative MAP training formula for transition models (Gauvain and Lee, 1994),, 11 (;) 1 (|,,) (;) ii MAP II il ll ci i e v piieI c lie v I ??
?? == ?+??
??= ?+ ??
? (9) 4.2 Setting hyper-parameters for the prior distribution In Bayesian learning, the hyper-parameter set { },i i v ?? of the prior distribution is assumed known based on a subjective knowledge about the model.
In our method, we set the prior with wordindependent transition pprobabilities., (|,) 1 ii vpiI?
?? ????+ (10) where ? is a positive parameter that needs to tune on a hold-out data set.
We will investigate the effect of ? with experimental results in later sections.
Substituting (10) into (9), the MAP based transition model training formula becomes 1 (;) (|,) (|,,) (;) MAP I l c iie piiI piieI cl i e ? ? = ?+??
??= ?+ ??
(11) Note that for frequent words that have a large amount of data samples for c(d;e), the sum of 1,..., (;) = ? ?? lI c lie is large, so that (|,,) MAP piieI??is dominated by the data distribution.
For rare words that have low counts of c(d;e), (|,,) MAP piieI?? will approach to the word independent model.
On the other hand, for the same word, when a small ? is used, a weak prior is applied, and the transition probability is more dependent on the training data of that word.
When ? becomes larger and larger, a stronger prior knowledge is applied, and the word dependent transition model will approach to the word-independent transition model.
Therefore, we can vary the parameter ? to control the contribution of prior distribution in model training and tune the word alignment performance.
5 Experimental
Results 5.1 Word alignment on the Canadian Hansards English-French corpus We evaluated our word dependent transition models for HMM based word alignment on the English-French Hansards corpus.
Only a subset of 500K sentence pairs was used in our experiments including 447 test sentence-pairs.
Tests sentencepairs were manually aligned and were marked with both sure and possible alignments (Och and Ney 2000a).
Using this annotation, we report the word alignment performance in terms of alignment error rate (AER) as defined by Och and Ney (2000a): |||| 1 |||| A SAP AER AS ????
=?? + (12) where S denotes the set of sure gold alignments, P denotes the set of possible gold alignments, A denotes the set of alignments generated by the word alignment method under test.
We first trained the IBM model 1 and then a baseline HMM model as described in section 2 on the Hansards corpus.
As the common practice, we initialized the translation probabilities of model 1 with uniform distribution over word pairs occur together in a same sentence pair.
HMM was initia83 lized with uniform transition probabilities and model 1 translation probabilities.
Both model 1 and HMM were trained with 5 iterations.
For the proposed word dependent transition model based HMM (WDHMM), we used the same settings as the HMM baseline except that the transition probability is computed according to (11).
We also trained IBM model 4 using GIZA++ provided by Och and Ney (2000c), where 5 iterations of model 4 training was performed after 5 iterations of model 1 plus 5 iterations of HMM.
The effect of hyper-parameters in the prior distribution for WDHMM is shown in Figure 1.
The horizontal dot line represents the AER given by the baseline HMM.
The dash-line curve represents the AERs of WDHMM given different ? ?s.
We vary the value of ? in the range from 0 to 1E5 and present that range in a log-scale in the figure.
Since ? = 0 is not a valid value in the log domain, we actually use the left-most point in the figure to represent the case of ? = 0.
From Fig.
1 it is shown that when ? is zero, we actually use the ML trained word-dependent transition model.
Due to the sparse data problem, the model is poorly estimated and lead to a high AER.
When increase ? to a larger value, a stronger prior is applied to give a more robust model.
Then in a large range of [100,2000]?
??, WDHMM outperforms baseline HMM significantly.
When ? gets even larger, MAP model training becomes being over-dominated by the prior distribution, and that eventually results in a performance approaching to that of the baseline HMM.
Fig. 1 only presents AER results that are calculated after combination of word alignments of both E??F and F??E directions based on a set of heuristics proposed by Och and Ney (2000b).
We have observed the similar trend of AER change for the E??F and F??E alignment directions, respectively.
However, due to the limit of the space, we didn?t include them in this paper.
In table 1-3, we give a detailed comparison between baseline HMM, WDHMM (with ? = 1000), and IBM model 4.
Compared to the baseline HMM, the proposed WDHMM can reduce AER by more than 13%.
It even outperforms IBM model 4 after two direction word alignment combination.
Meanwhile we noticed that although IBM model 4 gives superior performance over the baseline HMM on both of the two alignment directions, its AER after combination is almost the same as that of the baseline HMM.
We hypothesize that it may due to the modeling mechanism difference between HMM and model 4.
0 1 2 3 4 5 8 8.5 9 9.5 10 10.5 11 log10(tau) AE R % WDHMM HMM baseline Figure 1: The AER of HMM baseline and the AER of WDHMM as the prior parameter ? is varied from 0 to 1E5.
Note that the x axis is in log scale and we use the left-most point in the figure to represent the case of ? = 0.
These results are calculated after combination of word alignments of both E??F and F??E directions.
model E ??
F F ??
E combined baseline HMM 12.7 13.7 9.8 WDHMM (?
= 1000) 11.6 12.7 8.5 IBM model 4 (GIZA++) 11.3 12.1 9.7 Table 1: Comparison of test set AER between various models trained on 500K sentence pairs.
All numbers are in percentage.
model E ??
F F ??
E combined baseline HMM 85.2 83.1 91.7 WDHMM (?
= 1000) 86.1 83.8 93.3 IBM model 4 (GIZA++) 87.2 86.2 91.6 Table 2: Comparison of test set Precision between various models trained on 500K sentence pairs.
All numbers are in percentage.
model E ??
F F ??
E combined baseline HMM 90.6 91.4 88.3 WDHMM (?
= 1000) 91.9 92.6 89.1 IBM model 4 (GIZA++) 91.1 90.8 88.4 Table 3: Comparison of test set Recall between various models trained on 500K sentence pairs.
All numbers are in percentage.
84 5.2 Machine translation on Europarl corpus We further tested our WDHMM on a phrase-based machine translation system to see whether our improvement on word alignment can also improve MT accuracy measured by BLEU score (Papineni et al., 2002).
The machine translation experiment was conducted on the English-to-French track of NAACL 2006 Europarl evaluation workshop.
The supplied training corpus contains 688K sentence pairs.
Text data are already tokenized.
In our experiment, we first lower-cased all text, then word clustering was performed to cluster words of English and French into 32 word classes respectively using the tool provided by (J.
Goodman). Then word alignment was performed.
Both baseline HMM and IBM model 4 use word-class based transition models, and in WDHMM the word-class based transition model was used for prior distribution.
The IBM model 4 is trained by GIZA++ with a regimen of 5 iterations of Model 1, 5 iterations of HMM, and 5 iterations of Model 4.
Alignments of both directions are generated and then are combined by heuristic rules described in (Och and Ney 2000b).
Then phrase table was extracted from the word aligned bilingual texts.
The maximum phrase length was set to 7.
In the phrase-based MT system, there are four channel models.
They are direct maximum likelihood estimate of the probability of target phrase given source phrase, and the same estimate of source given target; we also compute the lexicon weighting features for source given target and target given source, respectively.
Other models include word count and phrase count, and a 3-gram language model provided by the workshop.
These models are combined in a log-linear framework with different weights (Och and Ney, 2002).
The model weight vector is trained on a dev set with 2000 English sentences, each of which has one French translation reference.
In the experiment, only the first 500 sentences were used to train the log-linear model weight vector, where minimum error rate (MER) training was used (Och, 2003).
After MER training, the weight vector that gives the best accuracy on the development set was selected.
We then applied it to tests.
There are 2000 sentences in the development-test set devtest, 2000 sentences in a test set test, and 1064 out-of-domain sentences called nc-test.
The Pharaoh phrase-based decoder (Koehn 2004b) was used for decoding.
The maximum re-ordering limit for decoding was set to 7.
We used default settings for all other parameters.
We present BLEU scores of MT systems using different word alignments on all three test sets, where Fig 2 shows BLEU scores of the two indomain tests, and Fig 3 shows MT results on the out-of-domain test set.
In testing, the prior parameter ? of WDHMM was varied in the range of [20, 5000].
In Fig.
2, the horizontal dash line and the horizontal dot line represent BLEU scores of the baseline HMM on devtest set and test set, respectively.
The dash-line curve and dot-line curve represent the BLEU scores of WDHMM on these two tests.
It is shown in the figure that WDHMM can achieve the best BLEU scores on both devtest and test when the prior parameter ? is set to 100.
Furthermore, WDHMM also gives considerable improvement on BLEU score over the baseline HMM in a broad range of ? from 50 to 1000, which indicates that WDHMM works pretty stable within a reasonable range of prior distributions.
In Fig.
3, the horizontal dash line represents the BLEU score of baseline HMM on nc-test set and the dash-line curve represents BLEU scores of WDHMM on the out-of-domain test.
The best BLEU is obtained at ? = 500.
It is interesting to see that the best ? for the out-of-domain test is larger than that of an in-domain test.
One possible explanation is that for out-of-domain data, we need more robust modeling for outliers other than more accurate (in-domain) modeling.
However, since the difference between ? = 500 and ? = 100 are very small, further experiments are needed before we can draw a conclusion.
We gives a detailed BLEU-wise comparison between baseline HMM and WDHMM in Table 4, where for WDHMM, ? =100 is used since it gives the best performance on the development-test set devtest.
In the same table, we also provide BLEU results of using IBM model 4.
Compared to baseline HMM alignment model, WDHMM can improve the BLEU score nearly 1% on in-domain test sets, and the improvement reduces to about 0.5% on the out-of-domain test.
When compared to IBM model 4, WDHMM still gives higher BLEU scores, and outperform model 4 by about 0.8% on the test set.
However the gain is reduced to 0.3% on devtest and 0.5% on the out-of-domain nc-test.
85 1 1.5 2 2.5 3 3.5 4 29 29.5 30 30.5 31 log10(tau) BL E U % devtest test Figure 2: Machine translation results on Europarl, English to French track, devtest and test sets.
The BLEU score of HMM baseline and the BLEU score of WDHMM as the prior parameter ? is varied from 20 to 5000.
Note that the x axis is in log scale.
1 1.5 2 2.5 3 3.5 4 20 20.5 21 21.5 22 log10(tau) BL E U % nc-test Figure 3: Machine translation results on Europarl, English to French track, out-of-domain test sets.
The BLEU score of HMM baseline and the BLEU score of WDHMM as the prior parameter ? is varied from 20 to 5000.
Note that the x axis is in log scale.
model devtest test nc-test baseline HMM 29.69 29.65 20.51 WDHMM (?
= 100) 30.59 30.65 20.96 IBM model 4 30.29 29.86 20.51 Table 4: Comparison of BLEU scores on devtest, test, and nc-test set between various word alignment models.
All numbers are in percentage.
In order to verify whether these gains from WDHMM are statistically significant, we implemented paired bootstrap resampling method proposed by Koehn (2004b) to compute statistical significance of the above test results.
In table 5, it is shown that BLEU gains of WDHMM over HMM and IBM-4 on different test sets, except the gain over IBM model 4 on the devtest set, are statistically significant with a significance level > 95%.
significance level devtest test nc-test WDHMM (?
=100) vs.
HMM 99.9% 99.9% 99.5% WDHMM (?
=100) vs.
IBM model 4 93.7% 99.9% 99.3% Table 5: Statistical significance test of the BLEU improvement of WDHMM (?
= 100) vs.
HMM baseline, and WDHMM (?
= 100) vs.
IBM model 4 on devtest, test, and nc-test sets.
5.3 Runtime
performance of WDHMM WDHMM runs as fast as a normal HMM, and the extra memory needed for the word dependent transition model is proportional to the vocabulary size of the source language given that the distortion sets of {c(d;e)} are bucketed.
Runtime speed of WDHMM and IBM-model 4 using GIZA++ is tabulated in table 6.
The results are based on Europarl English to French alignment and these tests were conducted on a fast PC with 3.0GHz CPU and 16GB memory.
In Table 6, WDHMM includes 5 iterations of model 1 training followed by 5 iterations of WDHMM, while "IBM model 4" includes 5 iterations for model 1, 5 iterations for HMM, and 5 iterations for model 4.
It is shown in Table 6 that WDHMM is more than four times faster to produce the end-to-end word alignment.
model runtime (min) WDHMM 121 IBM model 4 537 Table 6: comparison of runtime performance bewteen WDHMM training and IBM model 4 training using GIZA++.
6 Discussion
Other works have been done to improve transition models in HMM based word alignment.
Och and Ney (2000a) have suggested estimating word-class based transition models so as to provide more detailed transition probabilities.
However, due to the sparse data problem, only a small number of word classes are usually used and the many words in the same class still have to share the same transition model.
Toutanova et al.(2002) has proposed to 86 estimate a word-dependent self-transition model P(stay|e) so that each word can have its own probability to decide whether to stay or jump to a different word.
Later Deng and Byrne (2005) proposed a word dependent phrase length model to better model state occupancy.
However, these model can only model the probability of selfjumping.
Important knowledge of jumping from e to a different position should also be word dependent but is not modeled.
Another interesting comparison is between WDHMM and the fertility-based models, e.g., IBM model 3-5.
Compared to these models, a major disadvantage of HMM is the absence of a model of source word fertility.
However, as discussed in (Toutanova et al.2002),the word dependent selftransition model can be viewed as an approximation of fertility model.
i.e., it models the number of consecutive target words generated by the source word with a geometric distribution.
Therefore, with a well estimated word dependent transition model, this weakness of HMM is alleviated.
In this work, we proposed estimating a full word-dependent transition models in HMM based word alignment, and with Bayesian learning we can achieve robust model estimation under the sparse data condition.
We have conducted a series of experiments to evaluate this method on word alignment and machine translation tests, and show significant improvement over baseline HMM in terms of AER and BLEU.
It also performs better than the much more complicated IBM model 4 based word alignment model on various word alignment and machine translation tasks.
Acknowledgments The author is grateful to Chris Quirk and Arul Menezes for assistance with the MT system and for the valuable suggestions and discussions.
References C.
M. Bishop, 2006.
Pattern Recognition and Machine Learning.
Springer. P.
Brown, S.
D. Pietra, V.
J. D.
Pietra, and R.
L. Mercer.
1994. The Mathematics of Statistical Machine Translation: Parameter Estimation.
Computational Linguistics, 19:263??11.
Y. Deng and W.
Byrne, 2005, HMM Word and Phrase Alignment For Statistical Machine Translation, in Proceedings of HLT/EMNLP.
J. Gauvain and C.-H.
Lee, 1994, Maximum a Posteriori Estimation For Multivariate Gaussian Mixture Observations Of Markov Chains, IEEE Trans on Speech and Audio Processing.
J. Goodman, http://research.microsoft.com/~joshuago/ P.
Koehn, F.
J. Och, and D.
Marcu. 2003.
Statistical Phrase-Based Translation.
In Proceedings of HLT-NAACL.
P. Koehn, 2004a, Statistical Significance Tests for Machine Translation Evaluation, in Proceedings of EMNLP.
P. Koehn.
2004b. Pharaoh: A Beam Search Decoder For Phrase Based Statistical Machine Translation Models.
In Proceedings of AMTA.
P. Liang, B.
Taskar, and D.
Klein, 2006, Alignment by Agreement, in Proceedings of NAACL.
R. Moore, W.
Yih and A.
Bode, 2006, Improved Discriminative Bilingual Word Alignment, In Proceedings of COLING/ACL.
F. J.
Och and H.
Ney. 2000a.
A comparison of Alignment Models for Statistical Machine Translation.
In Proceedings of COLING.
F. J.
Och and H.
Ney. 2000b.
Improved Statistical Alignment Models.
In Proceedings of ACL.
F. J.
Och and H.
Ney. 2000c.
Giza++: Training of statistical translation models.
http://www-i6.informatik. rwthaachen.de/och/software/GIZA++.html.
F. J.
Och and H.
Ney. 2002.
Discriminative training and Maximum Entropy Models for Statistical Machine Translation, In Proceedings of ACL.
F. J.
Och, 2003, Minimum Error Rate Training in Statistical Machine Translation.
In Proceedings of ACL.
K. A.
Papineni, S.
Roukos, T.
Ward, and W.-J.
Zhu. 2002.
Bleu: A Method For Automatic Evaluation Of Machine Translation.
in Proceedings of ACL.
L. R.
Rabiner, 1989 A tutorial on hidden Markov models and selected applications in speech recognition.
Proceedings of the IEEE.
K. Toutanova, H.
T. Ilhan, and C.
D. Manning.
2002. Extensions to HMM-based Statistical Word Alignment Models.
In Proceedings of EMNLP.
S. Vogel, H.
Ney, and C.
Tillmann. 1996.
HMM-based Word Alignment In Statistical Translation.
In Proceedings of COLING.
H. Zhang and D.
Gildea, 2005, Stochastic Lexicalized Inversion Transduction Grammar for Alignment, In Proceedings of ACL .
Proceedings of the Second Workshop on Statistical Machine Translation, pages 88??5, Prague, June 2007.
c2007 Association for Computational Linguistics Efficient Handling of N-gram Language Models for Statistical Machine Translation Marcello Federico Fondazione Bruno Kessler IRST I-38050 Trento, Italy federico@itc.it Mauro Cettolo Fondazione Bruno Kessler IRST I-38050 Trento, Italy cettolo@itc.it Abstract Statistical machine translation, as well as other areas of human language processing, have recently pushed toward the use of large scale n-gram language models.
This paper presents efficient algorithmic and architectural solutions which have been tested within the Moses decoder, an open source toolkit for statistical machine translation.
Experiments are reported with a high performing baseline, trained on the Chinese-English NIST 2006 Evaluation task and running on a standard Linux 64-bit PC architecture.
Comparative tests show that our representation halves the memory required by SRI LM Toolkit, at the cost of 44% slower translation speed.
However, as it can take advantage of memory mapping on disk, the proposed implementation seems to scale-up much better to very large language models: decoding with a 289-million 5-gram language model runs in 2.1Gb of RAM.
1 Introduction
In recent years, we have seen an increasing interest toward the application of n-gram Language Models (LMs) in several areas of computational linguistics (Lapata and Keller, 2006), such as machine translation, word sense disambiguation, text tagging, named entity recognition, etc.
The original framework of n-gram LMs was principally automatic speech recognition, under which most of the standard LM estimation techniques (Chen and Goodman, 1999) were developed.
Nowadays, the availability of larger and larger text corpora is stressing the need for efficient data structures and algorithms to estimate, store and access LMs.
Unfortunately, the rate of progress in computer technology seems for the moment below the space requirements of such huge LMs, at least by considering standard lab equipment.
Statistical machine translation (SMT) is today one of the research areas that, together with speech recognition, is pushing mostly toward the use of huge n-gram LMs.
In the 2006 NIST Machine Translation Workshop (NIST, 2006), best performing systems employed 5-grams LMs estimated on at least 1.3 billion-word texts.
In particular, Google Inc.
presented SMT results with LMs trained on 8 trillion-word texts, and announced the availability of n-gram statistics extracted from one trillion of words.
The n-gram Google collection is now publicly available through LDC, but their effective use requires either to significantly expand computer memory, in order to use existing tools (Stolcke, 2002), or to develop new ones.
This work presents novel algorithms and data structures suitable to estimate, store, and access very large LMs.
The software has been integrated into a popular open source SMT decoder called Moses.1 Experimental results are reported on the Chinese-English NIST task, starting from a quite well-performing baseline, that exploits a large 5gram LM.
This paper is organized as follows.
Section 2 presents techniques for the estimation and represen1http://www.statmt.org/moses/ 88 tation in memory of n-gram LMs that try to optimize space requirements.
Section 3 describes methods implemented in order to efficiently access the LM at run time, namely by the Moses SMT decoder.
Section 4 presents a list of experiments addressing specific questions on the presented implementation.
2 Language
Model Estimation LM estimation starts with the collection of n-grams and their frequency counters.
Then, smoothing parameters are estimated (Chen and Goodman, 1999) for each n-gram level; infrequent n-grams are possibly pruned and, finally, a LM file is created containing n-grams with probabilities and back-off weights.
2.1 N-gram Collection Clearly, a first bottleneck of the process might occur if all n-grams have to be loaded in memory.
This problem is overcome by splitting the collection of ngrams statistics into independent steps and by making use of an efficient data-structure to collect and store n-grams.
Hence, first the dictionary of the corpus is extracted and split into K word lists, balanced with respect to the frequency of the words.
Then, for each list, only n-grams whose first word belongs to the list are extracted from the corpus.
The value of K is determined empirically and should be sufficiently large to permit to fit the partial n-grams into memory.
The collection of each subset of n-grams exploits a dynamic prefix-tree data structure shown in Figure 1.
It features a table with all collected 1grams, each of which points to its 2-gram successors, namely the 2-grams sharing the same 1-gram prefix.
All 2-gram entries point to all their 3-gram successors, and so on.
Successor lists are stored in memory blocks allocated on demand through a memory pool.
Blocks might contain different number of entries and use 1 to 6 bytes to encode frequencies.
In this way, a minimal encoding is used in order to represent the highest frequency entry of each block.
This strategy permits to cope well with the high sparseness of n-grams and with the presence of relatively few highly-frequent n-grams, that require counters encoded with 6 bytes.
The proposed data structure differs from other implementations mainly in the use of dynamic allocation of memory required to store frequencies of n3 w | fr | succ | ptr | flags 6 3 8 1 3 w | fr 1 1-gr 2-gr 3-gr Figure 1: Dynamic data structure for storing ngrams.
Blocks of successors are allocated on demand and might vary in the number of entries (depth) and bytes used to store counters (width).
Size in bytes is shown to encode words (w), frequencies (fr), and number of (succ), pointer to (ptr) and table type of (flags) successors.
grams. In the structure proposed by (Wessel et al., 1997) counters of n-grams occurring more than once are stored into 4-byte integers, while singleton ngrams are stored in a special table with no counters.
This solution permits to save memory at the cost of computational overhead during the collection of ngrams.
Moreover, for historical reasons, this work ignores the issue with huge counts.
In the SRILM toolkit (Stolcke, 2002), n-gram counts are accessed through a special class type.
Counts are all represented as 4-byte integers by applying the following trick: counts below a given threshold are represented as unsigned integers, while those above the threshold, which are typically very sparse, correspond indeed to indexes of a table storing their actual value.
To our opinion, this solution is ingenious but less general than ours, which does not make any assumption about the number of different high order counts.
2.2 LM
Smoothing For the estimation of the LM, a standard interpolation scheme (Chen and Goodman, 1999) is applied in combination with a well-established and simple smoothing technique, namely the WittenBell linear discounting method (Witten and Bell, 1991).
Smoothing of probabilities up from 2-grams is performed separately on each subset of n-grams.
89 For example, smoothing statistics for a 5-gram (v,w,x,y,z) are computed by means of statistics that are local to the subset of n-grams starting with v.
Namely, they are the counters N(v,w,x,y,z), N(v,w,x,y), and the number D(v,w,x,y) of different words observed in context (v,w,x,y).
Finally, K LM files are created, by just reading through the n-gram files, which are indeed not loaded in memory.
During this phase pruning of infrequent n-grams is also permitted.
Finally, all LM files are joined, global 1-gram probabilities are computed and added, and a single large LM file, in the standard ARPA format (Stolcke, 2002), is generated.
We are well aware that the implemented smoothing method is below the state-of-the-art.
However, from one side, experience tells that the gap in performance between simple and sophisticated smoothing techniques shrinks when very large corpora are used; from the other, the chosen smoothing method is very suited to the kind of decomposition we are applying to the n-gram statistics.
In the future, we will nevertheless address the impact of more sophisticated LM smoothing on translation performance.
2.3 LM
Compilation The final textual LM can be compiled into a binary format to be efficiently loaded and accessed at runtime.
Our implementation follows the one adopted by the CMU-Cambridge LM Toolkit (Clarkson and Rosenfeld, 1997) and well analyzed in (Whittaker and Raj, 2001).
Briefly, n-grams are stored in a data structure which privileges memory saving rather than access time.
In particular, single components of each n-gram are searched, via binary search, into blocks of successors stored contiguously (Figure 2).
Further improvements in memory savings are obtained by quantizing both back-off weights and probabilities.
2.4 LM
Quantization Quantization provides an effective way of reducing the number of bits needed to store floating point variables.
(Federico and Bertoldi, 2006) showed that best results were achieved with the so-called binning method.
This method partitions data points into uniformly populated intervals or bins.
Bins are filled in in a greedy manner, starting from the lowest value.
The center of each bin corresponds to the mean value 1-gr 2-gr 3-gr 3 w | bo | pr | idx 1 1 4 w | pr 3 1 Figure 2: Static data structure for LMs.
Number of bytes are shown used to encode single words (w), quantized back-off weights (bo) and probabilities (pr), and start index of successors (idx).
of all its points.
Quantization is applied separately at each n-gram level and distinctly to probabilities or back-off weights.
The chosen level of quantization is 8 bits (1 byte), that experimentally showed to introduce negligible loss in translation performance.
The quantization algorithm can be applied to any LM represented with the ARPA format.
Quantized LMs can also be converted into a binary format that can be efficiently uploaded at decoding time.
3 Language
Model Access One motivation of this work is the assumption that efficiency, both in time and space, can be gained by exploiting peculiarities of the way the LM is used by the hosting program, i.e. the SMT decoder.
An analysis of the interaction between the decoder and the LM was carried out, that revealed some important properties.
The main result is shown in Figure 3, which plots all calls to a 3-gram LM by Moses during the translation from German to English of the following text, taken from the Europarl task: ich bin kein christdemokrat und glaube daher nicht an wunder. doch ich mochte dem europaischen parlament, so wie es gegenwurtig beschaffen ist, fur seinen grossen beitrag zu diesen arbeiten danken.
Translation of the above text requires about 1.7 million calls of LM probabilities, that however involve only 120,000 different 3-grams.
The plot shows typical locality phenomena, that is the decoder tends to 90 Figure 3: LM calls during translation of a German text: each point corresponds to a specific 3-gram.
access the LM n-grams in nonuniform, highly localized patterns.
Locality is mainly temporal, namely the first call of an n-gram is easily followed by other calls of the same n-gram.
This property suggests that gains in access speed can be achieved by exploiting a cache memory in which to store already called n-grams.
Moreover, the relatively small amount of involved n-grams makes viable the access of the LM from disk on demand.
Both techniques are briefly described.
3.1 Caching
of probabilities In order to speed-up access time of LM probabilities different cache memories have been implemented through the use of hash tables.
Cache memories are used to store all final n-gram probabilities requested by the decoder, LM states used to recombine theories, as well as all partial n-gram statistics computed by accessing the LM structure.
In this way, the need of performing binary searches, at every level of the LM tables, is reduced at a minimum.
All cache memories are reset before decoding each single input set.
3.2 Memory
Mapping Since a limited collection of all n-grams is needed to decode an input sentence, the LM is loaded on demand from disk.
The data structure shown in Figure 2 permits indeed to efficiently exploit the socalled memory mapped file access.2 Memory mapping basically permits to include a file in the address 2POSIX-compliant operating systems and Windows support some form of memory-mapped file access.
Memory 1-gr 2-gr 3-gr Disk file Figure 4: Memory mapping of the LM on disk.
Only the memory pages (grey blocks) of the LM that are accessed while decoding the input sentence are loaded in memory.
space of a process, whose access is managed as virtual memory (see Figure 4).
During decoding of a sentence, only those ngrams, or better memory pages, of the LM that are actually accessed are loaded into memory, which results in a significant reduction of the resident memory space required by the process.
Once the decoding of the input sentence is completed, all loaded pages are released, so that resident memory is available for the n-gram probabilities of the following sentence.
A remarkable feature is that memorymapping also permits to share the same address space among multiple processes, so that the same LM can be accessed by several decoding processes (running on the same machine).
4 Experiments
In order to assess the quality of our implementation, henceforth named IRSTLM, we have designed a suite of experiments with a twofold goal: from one side the comparison of IRSTLM against a popular LM library, namely the SRILM toolkit (Stolcke, 2002); from the other, to measure the actual impact of the implementation solution discussed in previous sections.
Experiments were performed on a common statistical MT platform, namely Moses, in which both the IRSTLM and SRILM toolkits have been integrated.
The following subsection lists the questions 91 set type |W| source target large parallel 83.1M 87.6M giga monolingual 1.76G NIST 02 dev 23.7K 26.4K NIST 03 test 25.6K 28.5K NIST 04 test 51.0K 58.9K NIST 05 test 31.2K 34.6K NIST 06 nw test 18.5K 22.8K NIST 06 ng test 9.4K 11.1K NIST 06 bn test 12.0K 13.3K Table 1: Statistics of training, dev.
and test sets.
Evaluation sets of NIST campaigns include 4 references: in table, average lenghts are provided.
which our experiments aim to answer.
Assessing Questions 1.
Is LM estimation feasible for large amounts of data? 2.
How does IRSTLM compare with SRILM w.r.t.: (a) decoding speed?
(b) memory requirements?
(c) translation performance? 3.
How does LM quantization impact in terms of (a) memory consumption?
(b) decoding speed?
(c) translation performance?
(d) tuning of decoding parameters? 4.
What is the impact of caching on decoding speed? 5.
What are the advantages of memory mapping?
Task and Experimental Setup The task chosen for our experiments is the translation of news from Chinese to English, as proposed by the NIST MT Evaluation Workshop of 2006.3 A translation system was trained according to the large-data condition.
In particular, all the allowed bilingual corpora have been used for estimating the phrase-table.
The target side of these texts was also employed for the estimation of three 5-gram LMs, henceforth named large.
In particular, two LMs 3www.nist.gov/speech/tests/mt/ were estimated with the SRILM toolkit by pruning singletons events and by employing the WittenBell and the absolute discounting (Kneser and Ney, 1995) smoothing methods; the shorthand for these two LMs will be ?lrg-sri-wb??and ?lrg-sri-kn?? respectively.
Another large LM was estimated with the IRSTLM toolkit, by employing the only smoothing method available in the package (Witten-Bell) and by pruning singletons n-grams; its shorthand will be ?lrg??
An additional, much larger, 5-gram LM was instead trained with the IRSTLM toolkit on the socalled English Gigaword corpus, one of the allowed monolingual resources for this task.
Automatic translation was performed by means of Moses which, among other things, permits the contemporary use of more LMs, feature we exploited in our experiments as specified later.
Optimal interpolation weights for the log-linear model were estimated by running a minimum error training algorithm, available in the Moses toolkit, on the evaluation set of the NIST 2002 campaign.
Tests were performed on the evaluation sets of the successive campaigns (2003 to 2006).
Concerning the NIST 2006 evaluation set, results are given separately for three different types of texts, namely newswire (nw) and newsgroup (ng) texts, and broadcast news transcripts (bn).
Table 1 gives figures about training, development and test corpora, while Table 2 provides main statistics of the estimated LMs.
LM millions of 1-gr 2-gr 3-gr 4-gr 5-gr lrg-sri-kn 0.3 5.2 5.9 7.1 6.8 lrg-sri-wb 0.3 5.2 6.4 7.8 6.8 lrg 0.3 5.3 6.6 8.4 8.0 giga 4.5 64.4 127.5 228.8 288.6 Table 2: Statistics of LMs.
MT performance are provided in terms of caseinsensitive BLEU and NIST scores, as computed with the NIST scoring tool.
For time reasons, the decoder run with monotone search; preliminary experiments showed that this choice does not affect comparison of LMs.
Reported decoding speed is the elapsed real time measured with the Linux/UNIX time command divided by the number of source words to be translated.
dual Intel/Xeon 92 CPU 3.20GHz with 8Gb RAM.
Experiments run on dual Intel/Xeon CPUs 3.20GHz/8Gb RAM.
4.1 LM
estimation First of all, let us answer the question (number 1) on the feasibility of the procedure for the estimation of huge LMs.
Given the amount of training data employed, it is worth to provide some details about the estimation process of the ?giga??LM.
According to the steps listed in Section 2.1, the whole dictionary was split into K = 14 frequency balanced lists.
Then, 5-grams beginning with words from each list were extracted and stored.
Table 3 shows some figures about these dictionaries and 5-gram collections.
Note that the dictionary size increases with the list index: this means only that more frequent words were used first.
This stage run in few hours with 1-2Gb parallel processes.
list dictionary number of 5-grams: index size observed distinct non-singletons 0 4 217M 44.9M 16.2M 1 11 164M 65.4M 20.7M 2 8 208M 85.1M 27.0M 3 44 191M 83.0M 26.0M 4 64 143M 56.6M 17.8M 5 137 142M 62.3M 19.1M 6 190 142M 64.0M 19.5M 7 548 142M 66.0M 20.1M 8 783 142M 63.3M 19.2M 9 1.3K 141M 67.4M 20.2M 10 2.5K 141M 69.7M 20.5M 11 6.1K 141M 71.8M 20.8M 12 25.4K 141M 74.5M 20.9M 13 4.51M 141M 77.4M 20.6M total 4.55M 2.2G 951M 289M Table 3: Estimation of the ?giga??LM: dictionary and 5-gram statistics (K = 14).
The actual estimation of the LM was performed with the scheme presented in Section 2.2.
For each collection of non-singletons 5-grams, a sub-LM was built by computing smoothed n-gram (n = 1    5) probabilities and interpolation parameters.
Again, by exploiting parallel processing, this phase took only few hours on standard HW resources.
Finally, sub-LMs were joined in a single LM, which can be stored in two formats: (i) the standard textual ARPA LM format quantization file size lrg-sri-kn textual n 893Mb lrg-sri-wb textual n 952Mb lrg textual n 1088Mb y 789Mb binary n 368Mb y 220Mb giga textual n 28.0Gb y 21.0Gb binary n 8.5Gb y 5.1Gb Table 4: Figures of LM files.
format, and (ii) the binary format of Section 2.3.
In addition, LM probabilities can be quantized according to the procedure of Section 2.4.
The estimation of the ?lrg-sri??LMs, performed by means of the SRILM toolkit, took about 15 minutes requiring 5Gb of memory.
The ?lrg??LM was estimated as the ?giga??LM in about half an hour demanding only few hundreds of Mb of memory.
Table 4 lists the size of files storing various versions of the ?large??and ?giga??LMs which differ in format and/or type.
4.2 LM
run-time usage Tables 5 and 6 shows BLEU and NIST scores, respectively, measured on test sets for each specific LM configuration.
The first two rows of the two tables regards runs of Moses with the SRILM, that uses ?lrg-sri??LMs.
The other rows refer to runs of Moses with IRSTLM, either using LM ?lrg??only, or both LMs, ?lrg??and ?giga??
LM quantization is marked by a ?q??
Finally, in Table 7 figures about the decoding processes are recorded.
For each LM configuration, the process size, both virtual and resident, is provided together with the average time required for translating a source word with/without the activation of the caching mechanism described in Section 3.1.
It is to worth noticing that the ?giga??LM (both original and quantized) is loaded through the memory mapping service presented in Section 3.2.
Table 7 includes most of the answers to question number 2: 2.a Under the same conditions, Moses running with SRILM permits almost double faster 93 LM NIST test set 03 04 05 06 06 06 nw ng bn lrg-sri-kn 28.74 30.52 26.99 29.28 23.47 27.27 lrg-sri-wb 28.05 29.86 26.52 28.37 23.13 26.37 lrg 28.49 29.84 26.97 28.69 23.28 26.70 q-lrg 28.05 29.66 26.48 28.58 22.64 26.05 lrg+giga 30.77 31.93 29.09 29.74 24.39 28.50 q-lrg+q-giga 30.42 31.47 28.62 29.76 24.28 28.23 Table 5: BLEU scores on NIST evaluation sets for different LM configurations.
LM NIST test set 03 04 05 06 06 06 nw ng bn lrg-sri-kn 8.73 9.29 8.47 8.98 7.81 8.52 lrg-sri-wb 8.52 9.14 8.27 8.96 7.90 8.34 lrg 8.73 9.21 8.45 8.95 7.82 8.47 q-lrg 8.60 9.11 8.32 8.88 7.73 8.31 lrg+giga 9.08 9.49 8.80 8.92 7.86 8.66 q-lrg+q-giga 8.93 9.38 8.65 9.05 7.99 8.60 Table 6: NIST scores on NIST evaluation sets for different LM configurations.
translation than IRSTLM (13.33 vs.
6.80 words/s).
Anyway, IRSTLM can be sped-up to 7.52 words/s by applying caching.
2.b IRSTLM requires about half memory than SRILM for storing an equivalent LM during decoding.
If the LM is quantized, the gain is even larger.
Concerning file sizes (Table 4), the size of IRSTLM binary files is about 30% of the corresponding textual versions.
Quantization further reduces the size to 20% of the original textual format.
2.c Performance of IRSTLM and SRILM on the large LMs smoothed with the same method are comparable, as expected (see entries ?lrg-sriwb??and ?lrg??of Tables 5 and 6).
The small differences are due to different probability values assigned by the two libraries to out-ofvocabulary words.
Concerning quantization, gains in terms of memory space (question 3.a) have already been highlighted (see answer 2.b).
For the remaining points: 3.b comparing ?lrg??vs.
?q-lrg??rows and LM process size caching dec.
speed virtual resident (src w/s) lrg-sri-kn/wb 1.2Gb 1.2Gb 13.33 lrg 750Mb 690Mb n 6.80 y 7.42 q-lrg 600Mb 540Mb n 6.99 y 7.52 lrg+giga 9.9Gb 2.1Gb n 3.52 y 4.28 q-lrg+q-giga 6.8Gb 2.1Gb n 3.64 y 4.35 Table 7: Process size and decoding speed with/wo caching for different LM configurations.
?lrg+giga??vs. ?q-lrg+q-giga??rows of Table 7, it results that quantization allows only a marginal decoding time reduction (1-3%) 3.c comparing the same rows of Tables 5 and 6, it can be claimed that quantization doesn?t affect translation performance in a significant way 3.d no specific training of decoder weights is required since the original LM and its quantized version are equivalent.
For example, by translating the NIST 05 test set with the weights estimated on the ?lrg+giga??configuration, the following BLEU/NIST scores are got: 28.99/8.79 with the ?q-lrg+q-giga??LMs, 29.09/8.80 with the ?lrg+giga??LMs (the latter scores are also given in Tables 5 and 6).
Employing weights estimated on ?q-lrg+q-giga?? scores are: 28.58/8.66 with ?lrg+giga??LMs, 28.62/8.65 with ?q-lrg+q-giga??LMs (again also in Tables 5 and 6).
Also on other test sets differences are negligible.
Table 7 answers the question number 4 on caching, by reporting the decoding speed-up due to this mechanism: a gain of 8-9% is observed on ?lrg?? and ?q-lrg??configurations, of 20-21% in case also ?giga/q-giga??LMs are employed.
The answer to the last question is that thanks to the memory mapping mechanism it is possible run Moses with huge LMs, which is expected to improve performance.
Tables 5 and 6 provide quantitative support to the statement.
In fact, a gain of 1-2 absolute BLEU was measured on different test sets when ?giga??LM was employed in addition to 94 NIST test set 03 04 05 06 06 06 nw ng bn BLEU ci 33.62 35.04 31.92 32.74 26.18 32.43 cs 31.44 32.99 29.95 30.49 24.35 31.10 NIST ci 9.27 9.75 9.00 9.24 8.00 8.97 cs 8.88 9.40 8.64 8.82 7.69 8.77 Table 8: Case insensitive (ci) and sensitive (cs) scores of the best performing system.
?lrg??LM. The SRILM-based decoder would require a process of about 30Gb to load the ?giga??LM; on the contrary, the virtual size of the IRSTLM-based decoder is 6.8Gb, while the actual resident memory is only 2.1Gb. 4.3 Best Performing System Experimental results discussed so far are not the best we are able to get.
In fact, the adopted setup fixed the monotone search and the use of no reordering model.
Then, in order to allow a fair comparison of the IRSTLM-based Moses system with the ones participating to the NIST MT evaluation campaigns, we have (i) set the maximum reordering distance to 6 and (ii) estimated a lexicalized reordering model on the large parallel data by means of the training option ?orientation-bidirectional-fe??
Table 8 shows BLEU/NIST scores measured on test sets by employing the IRSTLM-based Moses with this setting and employing ?q-lrg+q-giga??
LMs. It ranks at the top 5 systems (out of 24) with respect to the results of the NIST 06 evaluation campaign.
5 Conclusions
We have presented a method for efficiently estimating and handling large scale n-gram LMs for the sake of statistical machine translation.
LM estimation is performed by splitting the task with respect to the initial word of the n-grams, and by merging the resulting sub-LMs.
Estimated LMs can be quantized and compiled in a compact data structure.
During the search, LM probabilities are cached and only the portion of effectively used LM n-grams is loaded in memory from disk.
This method permits indeed to exploit locality phenomena shown by the search algorithm when accessing LM probabilities.
Results show an halving of memory requirements, at the cost of 44% slower decoding speed.
In addition, loading the LM on demand permits to keep the size of memory allocated to the decoder nicely under control.
Future work will investigate the way for including more sophisticated LM smoothing methods in our scheme and will compare IRSTLM and SRILM toolkits on increasing size training corpora.
6 Acknowledgments
This work has been funded by the European Union under the integrated project TC-STAR Technology and Corpora for Speech-to-Speech Translation (IST-2002-FP6-506738, http://www.tc-star.org).
References S.F.
Chen and J.
Goodman. 1999.
An empirical study of smoothing techniques for language modeling.
Computer Speech and Language, 4(13):359??93.
P. Clarkson and R.
Rosenfeld. 1997.
Statistical language modeling using the CMU?cambridge toolkit.
In Proc.
of Eurospeech, pages 2707??710, Rhodes, Greece.
M. Federico and N.
Bertoldi. 2006.
How many bits are needed to store probabilities for phrase-based translation?
In Proc.
of the Workshop on Statistical Machine Translation, pages 94??01, New York City, June.
Association for Computational Linguistics.
R. Kneser and H.
Ney. 1995.
Improved backing-off for m-gram language modeling.
In Proc.
of ICASSP, volume 1, pages 181??84, Detroit, MI.
M. Lapata and F.
Keller. 2006.
Web-based models for natural language processing.
ACM Transactions on Speech and Language Processing, 1(2):1??1.
NIST. 2006.
Proc. of the NIST MT Workshop.
Washington, DC.
NIST. A.
Stolcke. 2002.
SRILM an extensible language modeling toolkit.
In Proc.
of ICSLP, Denver, Colorado.
F. Wessel, S.
Ortmanns, and H.
Ney. 1997.
Implementation of word based statistical language models.
In Proc.
SQEL Workshop on Multi-Lingual Information Retrieval Dialogs, pages 55??9, Pilsen, Czech Republic.
E. W.
D. Whittaker and B.
Raj. 2001.
Quantization-based Language Model Compression.
In Proc.
of Eurospeech, pages 33??6, Aalborg.
I. H.
Witten and T.
C. Bell.
1991. The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression.
IEEE Trans.
Inform. Theory, IT-37(4):1085??1094 .
Proceedings of the Second Workshop on Statistical Machine Translation, pages 96??03, Prague, June 2007.
c2007 Association for Computational Linguistics Human Evaluation of Machine Translation Through Binary System Comparisons David Vilar, Gregor Leusch and Hermann Ney Lehrstuhl fur Informatik 6 RWTH Aachen University D-52056 Aachen, Germany {vilar,leusch,ney}@cs.rwth-aachen.de Rafael E.
Banchs D.
of Signal Theory and Communications Universitat Polit`ecnica de Catalunya 08034 Barcelona, Spain rbanchs@gps.tsc.upc.edu Abstract We introduce a novel evaluation scheme for the human evaluation of different machine translation systems.
Our method is based on direct comparison of two sentences at a time by human judges.
These binary judgments are then used to decide between all possible rankings of the systems.
The advantages of this new method are the lower dependency on extensive evaluation guidelines, and a tighter focus on a typical evaluation task, namely the ranking of systems.
Furthermore we argue that machine translation evaluations should be regarded as statistical processes, both for human and automatic evaluation.
We show how confidence ranges for state-of-the-art evaluation measures such as WER and TER can be computed accurately and efficiently without having to resort to Monte Carlo estimates.
We give an example of our new evaluation scheme, as well as a comparison with classical automatic and human evaluation on data from a recent international evaluation campaign.
1 Introduction
Evaluation of machine translation (MT) output is a difficult and still open problem.
As in other natural language processing tasks, automatic measures which try to asses the quality of the translation can be computed.
The most widely known are the Word Error Rate (WER), the Position independent word Error Rate (PER), the NIST score (Doddington, 2002) and, especially in recent years, the BLEU score (Papineni et al., 2002) and the Translation Error Rate (TER) (Snover et al., 2005).
All of theses measures compare the system output with one or more gold standard references and produce a numerical value (score or error rate) which measures the similarity between the machine translation and a human produced one.
Once such reference translations are available, the evaluation can be carried out in a quick, efficient and reproducible manner.
However, automatic measures also have big disadvantages; (Callison-Burch et al., 2006) describes some of them.
A major problem is that a given sentence in one language can have several correct translations in another language and thus, the measure of similarity with one or even a small amount of reference translations will never be flexible enough to truly reflect the wide range of correct possibilities of a translation.
1 This
holds in particular for long sentences and wideor open-domain tasks like the ones dealt with in current MT projects and evaluations.
If the actual quality of a translation in terms of usefulnessforhumanusersistobeevaluated,human evaluation needs to be carried out.
This is however a costly and very time-consuming process.
In this work we present a novel approach to human evaluation that simplifies the task for human judges.
Instead of having to assign numerical scores to each sentence to be evaluated, as is done in current evaluation procedures, human judges choose the best one out of two candidate translations.
We show how this method can be used to rank an arbitrary number of systems and present a detailed analysis of the statistical significance of the method.
1Compare this with speech recognition, where apart from orthographic variance there is only one correct reference.
96 2 State-of-the-art The standard procedure for carrying out a human evaluation of machine translation output is based on the manual scoring of each sentence with two numerical values between 1 and 5.
The first one measures the fluency of the sentence, that is its readability and understandability.
This is a monolingual feature which does not take the source sentence into account.
The second one reflects the adequacy, that is whether the translated sentence is a correct translation of the original sentence in the sense that the meaning is transferred.
Since humans will be the end users of the generated output,2 it can be expected that these human-produced measures will reflect the usability and appropriateness of MT output better than any automatic measure.
This kind of human evaluation has however additional problems.
It is much more time consuming than the automatic evaluation, and because it is subjective, results are not reproducible, even from the same group of evaluators.
Furthermore, there can be biases among the human judges.
Large amounts of sentences must therefore be evaluated and procedures like evaluation normalization must be carried out before significant conclusions from the evaluation can be drawn.
Another important drawback, which is also one of the causes of the aforementioned problems, is that it is very difficult to define the meaning of the numerical scores precisely.
Even if human judges have explicit evaluation guidelines at hand, they still find it difficult to assign a numerical value which represents the quality of the translation for many sentences (Koehn and Monz, 2006).
In this paper we present an alternative to this evaluation scheme.
Our method starts from the observation that normally the final objective of a human evaluationistofinda?ranking?ofdifferentsystems, andtheabsolutescoreforeachsystemisnotrelevant (and it can even not be comparable between different evaluations).
We focus on a method that aims to simplify the task of the judges and allows to rank the systems according to their translation quality.
3 Binary
System Comparisons The main idea of our method relies in the fact that a human evaluator, when presented two different translations of the same sentence, can normally choose the best one out of them in a more or less 2With the exception of cross-language information retrieval and similar tasks.
definite way.
In social sciences, a similar method has been proposed by (Thurstone, 1927).
3.1 Comparison
of Two Systems For the comparison of two MT systems, a set of translated sentence pairs is selected.
Each of these pairs consists of the translations of a particular source sentence from the two systems.
The human judge is then asked to select the ?best??translation of these two, or to mark the translations to be equally good.
We are aware that the definition of ?best??here is fuzzy.
In our experiments, we made a point of not giving the evaluators explicit guidelines on how to decidebetweenbothtranslations.
Asaconsequence, the judges were not to make a distinction between fluency and adequacy of the translation.
This has a two-fold purpose: on the one hand it simplifies the decision procedure for the judges, as in most of the cases the decision is quite natural and they do not need to think explicitly in terms of fluency and adequacy.
On the other hand, one should keep in mind that the final goal of an MT system is its usefulness for a human user, which is why we do not want to impose artificial constraints on the evaluation procedure.
If only certain quality aspects of the systems are relevant for the ranking, for example if we want to focus on the fluency of the translations, explicit guidelines can be given to the judges.
If the evaluators are bilingual they can use the original sentences to judge whether the information was preserved in the translation.
After our experiment, the human judges provided feedback on the evaluation process.
We learned that the evaluators normally selected the translation which preserved most of the information from the original sentence.
Thus, we expect to have a slight preference for adequacy over fluency in this evaluation process.
Note however that adequacy and fluency have shown a high correlation3 in previous experiments.
This can be explained by noting that a low fluency renders the text incomprehensible and thus the adequacy score will also be low.
The difference in the amount of selected sentences of each system is an indicator of the difference in quality between the systems.
Statistics can be carried out in order to decide whether this difference is statistically significant; we will describe this in more detail in Section 3.4. 3At least for ?sensible??translation systems.
Academic counter-examples could easily be constructed.
97 3.2 Evaluation of Multiple Systems We can generalize our method to find a ranking of several systems as follows: In this setting, we have a set of n systems.
Furthermore, we have defined an order relationship ?is better than??between pairs of these systems.
Our goal now is to find an ordering of the systems, such that each system is better than its predecessor.
In other words, this is just a sorting problem ??as widely known in computer science.
Several efficient sorting algorithms can be found in the literature.
Generally, the efficiency of sorting algorithms is measured in terms of the number of comparisons carried out.
State-of-the-art sorting algorithms have a worst-case running time of O(nlogn), where n is the number of elements to sort.
In our case, because such binary comparisons are very time consuming, we want to minimize the absolute number of comparisons needed.
This minimization should be carried out in the strict sense, not just in an asymptotic manner.
(Knuth, 1973) discusses this issue in detail.
It is relatively straightforward to show that, in the worst case, the minimum number of comparisons to be carried out to sort n elements is at least ceilingleftlogn!ceilingright (for which nlogn is an approximation).
It is not always possible to reach this minimum, however, as was proven e.g. for the case n = 12 in (Wells, 1971) and for n = 13 in (Peczarski, 2002).
(Ford Jr and Johnson, 1959) propose an algorithm called merge insertion which comes very close to the theoretical limit.
This algorithm is sketched in Figure 1.
There are also algorithms with a better asymptotic runtime (Bui and Thanh, 1985), but they only take effect for values of n too large for our purposes (e.g., more than 100).
Thus, using the algorithm from Figure 1 we can obtain the ordering of the systems with a (nearly) optimal number of comparisons.
3.3 Further
Considerations In Section 3.1 we described how to carry out the comparison between two systems when there is only one human judge carrying out this comparison.
The comparison of systems is a very time consuming task.
Therefore it is hardly possible for one judge to carry out the evaluation on a whole test corpus.
Usually, subsets of these test corpora are selected for human evaluations instead.
In order to obtain a better coverage of the test corpus, but also to try to alleviate the possible bias of a single evaluator, it is advantageous to have several evaluators carrying out the comparison between two systems.
However, there are two points that must be considered.
The first one is the selection of sentences each human judge should evaluate.
Assume that we have already decided the amount of sentences m each evaluator has to work with (in our case m = 100).
One possibilityisthatallhumanjudgesevaluatethesame set of sentences, which presumably will cancel possible biases of the evaluators.
A second possibility is to give each judge a disjunct set of sentences.
In this waywebenefitfromahighercoverageofthecorpus, but do not have an explicit bias compensation.
In our experiments, we decided for a middle course: Each evaluator receives a randomly selected set of sentences.
There are no restrictions on the selection process.
This implicitly produces some overlap while at the same time allowing for a larger set of sentences to be evaluated.
To maintain the same conditions for each comparison, we also decided that each human judge should evaluate the same set of sentences for each system pair.
The other point to consider is how the evaluation results of each of the human judges should be combined into a decision for the whole system.
One possibility would be to take only a ?majority vote?? among the evaluators to decide which system is the best.
By doing this, however, possible quantitative information on the quality difference of the systems isnottakenintoaccount.
Consequently,theoutputis strongly influenced by statistical fluctuations of the data and/or of the selected set of sentences to evaluate.
Thus, in order to combine the evaluations we just summed over all decisions to get a total count of sentences for each system.
3.4 Statistical
Significance The evaluation of MT systems by evaluating translations of test sentences ??be it automatic evaluation or human evaluation ??must always be regarded as a statistical process: Whereas the outcome, or score R, of an evaluation is considered to hold for ?all?? possible sentences from a given domain, a test corpusnaturallyconsistsofonlyasamplefromallthese sentences.
Consequently, R depends on that sample of test sentences.
Furthermore, both a human evaluation score and an automatic evaluation score for a hypothesis sentence are by itself noisy: Human evaluation is subjective, and as such is subject to ?human noise?? as described in Section 2.
Each automatic score, on the other hand, depends heavily ontheambiguousselectionofreferencetranslations.
Accordingly, evaluation scores underly a probability 98 1.
Make pairwise comparisons of floorleftn/2floorright disjoint pairs of elements.
(If n is odd, leave one element out).
2. Sort the floorleftn/2floorright larger elements found in step 1, recursively by merge insertion.
3. Name the floorleftn/2floorright elements found in step 2 a1,a2,...,afloorleftn/2floorright and the rest b1,b2,...,bceilingleftn/2ceilingright, such that a1 ??a2 ?? ??afloorleftn/2floorright and bi ??ai for 1 ??i ??floorleftn/2floorright.
Call b1 and the a?s the ?main chain?? 4.
Insert the remaining b?s into the main chain, using binary insertion, in the following order (ignore the bj such that j > ceilingleftn/2ceilingright): b3,b2;b5,b4;b11,...,b6;...btk,...,btk??+1;... with tk = 2k+1+(??)k3. Figure 1: The merge insertion algorithm as presented in (Knuth, 1973).
distribution, and each evaluation result we achieve must be considered as a sample from that distribution.
Consequently, both human and automatic evaluation results must undergo statistical analysis before conclusions can be drawn from them.
A typical application of MT evaluation ??for example in the method described in this paper ??is to decide whether a given MT system X, represented byasetoftranslatedsentences,issignificantlybetter than another system Y with respect to a given evaluation measure.
This outcome is traditionally called the alternative hypothesis.
The opposite outcome, namely that the two systems are equal, is known as the null hypothesis.
We say that certain values of RX,RY confirm the alternative hypothesis if the null hypothesis can be rejected with a given level of certainty, e.g. 95%.
In the case of comparing two MT systems, the null hypothesis would be ?both systems are equal with regard to the evaluation measure; thatis, bothevaluationscoresR, Rprime comefrom the same distribution R0??
As R is randomly distributed, it has an expectation E[R] and a standard error se[R].
Assuming a normal distribution for R, we can reject the null hypothesis with a confidence of 95% if the sampled score R is more than 1.96 times the standard error away from the null hypothesis expectation: R significant ??|E[R0] ??R| > 1.96se[R0] (1) The question we have to solve is: How can we estimate E[R0] and se[R0]?
The first step is that we consider R and R0 to share the same standard error se[R0] = se[R].
This value can then be estimated from the test data.
In a second step, we give an estimate for E[R0], either inherent in the evaluation measure (see below), or from the estimate for the comparison system Rprime.
A universal estimation method is the bootstrap estimate: The core idea is to create replications of R by random sampling from the data set (Bisani and Ney, 2004).
Bootstrapping is generally possible for all evaluation measures.
With a high number of replicates, se[R] and E[R0] can be estimated with satisfactory precision.
For a certain class of evaluation measures, these parameters can be estimated more accurately and efficiently from the evaluation data without resorting to Monte Carlo estimates.
This is the class of errors based on the arithmetic mean over a sentencewise score: In our binary comparison experiments, each judge was given hypothesis translations ei,X, ei,Y . She could then judge ei,X to be better than, equal to, or worse than ei,Y . All these judgments werecountedoverthesystems.
Wedefineasentence score ri,X,Y for this evaluation method as follows: ri,X,Y := ?   +1 ei,X is better than ei,Y 0 ei,X is equal to ei,Y ?? ei,X is worse than ei,Y . (2) Then, the total evaluation score for a binary comparison of systems X and Y is RX,Y := 1m msummationdisplay i=1 ri,X,Y, (3) with m the number of evaluated sentences.
Forthiscase, namelyR beinganarithmeticmean, (Efron and Tibshirani, 1993) gives an explicit formula for the estimated standard error of the score RX,Y . To simplify the notation, we will use R instead of RX,Y from now on, and ri instead of ri,X,Y . se[R] = 1m ??1 radicaltpradicalvertex radicalvertexradicalbt msummationdisplay i=1 (ri ??R)2 . (4) With x denoting the number of sentences where ri = 1, and y denoting the number of sentences 99 where ri = ??, R = x ??ym (5) and with basic algebra se[R] = 1m ??1 radicalbigg x + y ??(x ??y) 2 m . (6) Moreover, we can explicitly give an estimate for E[R0]: The null hypothesis is that both systems are ?equally good??
Then, we should expect as many sentences where X is better than Y as vice versa, i.e. x = y.
Thus, E[R0] = 0.
Using Equation 4, we calculate se[R] and thus a significance range for adequacy and fluency judgments.
When comparing two systems X and Y, we assume for the null hypothesis that se[R0] = se[RX] and E[R0] = E[RY ] (or vice versa).
A very useful (and to our knowledge new) result for MT evaluation is that se[R] can also be explicitly estimated for weighted means ??such as WER, PER, and TER.
These measures are defined as follows: Letdi,i = 1,...,mdenotethenumberof?errors??(edit operations) of the translation candidate ei with regard to a reference translation with length li.
Then, the total error rate will be computed as R := 1L msummationdisplay i=1 di (7) where L := msummationdisplay i=1 li (8) As a result, each sentence ei affects the overall score with weight li ??the effect of leaving out a sentence with length 40 is four times higher than that of leaving out one with length 10.
Consequently, these weights must be considered when estimating the standard error of R: se[R] = radicaltpradicalvertex radicalvertexradicalbt 1 (m ??1)(L ??1) msummationdisplay i=1 parenleftbiggd i li ??R parenrightbigg2  li (9) With this Equation, Monte-Carlo-estimates are no longer necessary for examining the significance of WER, PER, TER, etc.
Unfortunately, we do not expect such a short explicit formula to exist for the standard BLEU score.
Still, a confidence range for BLEU can be estimated by bootstrapping (Och, 2003; Zhang and Vogel, 2004).
Spanish English Train Sentences 1.2M Words 32M 31M Vocabulary 159K 111K Singletons 63K 46K Test Sentences 1117 Words 26K OOV Words 72 Table 1: Statistics of the EPPS Corpus.
4 Evaluation
Setup Theevaluationprocedurewascarriedoutonthedata generated in the second evaluation campaign of the TC-STAR project4.
The goal of this project is to build a speech-to-speech translation system that can deal with real life data.
Three translation directions are dealt with in the project: Spanish to English, EnglishtoSpanishandChinesetoEnglish.
Forthesystem comparison we concentrated only in the English to Spanish direction.
The corpus for the Spanish?English language pair consistsoftheofficialversionofthespeechesheldin the European Parliament Plenary Sessions (EPPS), as available on the web page of the European Parliament.
A more detailed description of the EPPS data can be found in (Vilar et al., 2005).
Table 1 shows the statistics of the corpus.
A total of 9 different MT systems participated in this condition in the evaluation campaign that took place in February 2006.
We selected five representative systems for our study.
Henceforth we shall refer to these systems as System A through System E.
We restricted the number of systems in order to keep the evaluation effort manageable for a first experimental setup to test the feasibility of our method.
The ranking of 5 systems can be carried out with as few as 7 comparisons, but the ranking of 9 systems requires 19 comparisons.
5 Evaluation
Results Seven human bilingual evaluators (6 native speakers and one near-native speaker of Spanish) carried out the evaluation.
100 sentences were randomly chosen and assigned to each of the evaluators for every system comparison, as discussed in Section 3.3.
The results can be seen in Table 2 and Figure 2.
Counts 4http://www.tc-star.org/ 100 0 10 20 30 40 50 60 70 0 10 20 30 40 50 60 70 a71 a71 a71 a71 a71 a71 a71 # "First system better" # "Second system better" a71 B?A D?C A?C E?A E?B B?D D?A (a) Each judge.
0 100 200 300 400 0 100 200 300 400 # "First system better" # "Second system better" a71 B?A D?C A?CE?A E?B B?D D?A (b) All judges.
Figure2: Resultsofthebinarycomparisons.
Numberoftimesthewinningsystemwasreallyjudged?better?? vs.
number of times it was judged ?worse??
Results in hatched area can not reject null hypothesis, i.e. would be considered insignificant.
missing to 100 and 700 respectively denote ?same quality??decisions.
As can be seen from the results, in most of the cases the judges clearly favor one of the systems.
The most notable exception is found when comparing systems A and C, where a difference of only 3 sentences is clearly not enough to decide between the two.
Thus, the two bottom positions in the final ranking could be swapped.
Figure 2(a) shows the outcome for the binary comparisonsseparatelyforeachjudge,togetherwith an analysis of the statistical significance of the results.
As can be seen, the number of samples (100) would have been too low to show significant resultsinmanyexperiments(datapointsinthehatched area).
In some cases, the evaluator even judged better the system which was scored to be worse by the majority of the other evaluators (data points above the bisector).
As Figure 2(b) shows, ?the only thing better than data is more data??
When we summarize R over all judges, we see a significant difference (with a confidence of 95%) at all comparisons but two (A vs.
C, and E vs.
B). It is interesting to note that exactly these two pairs do not show a significant difference when using a majority vote strategy.
Table 3 shows also the standard evaluation metrics.
Three BLEU scores are given in this table, the one computed on the whole corpus, the one computed on the set used for standard adequacy and fluency computations and the ones on the set we selected for this task5.
It can be seen that the BLEU scores are consistent across all data subsets.
In this casetherankingaccordingtothisautomaticmeasure matches exactly the ranking found by our method.
When comparing with the adequacy and fluency scores, however, the ranking of the systems changes considerably: B D E C A.
However, the difference between the three top systems is quite small.
This can be seen in Figure 3, which shows some automatic and human scores for the five systems in our experiments, along with the estimated 95% confidence range.
The bigger difference is found when comparing the bottom systems, namely System A and System C.
While our method produces nearly no difference the adequacy and fluency scores indicate System C as clearly superior to System A.
It is worth noting that the both groups use quite different translation approaches (statistical vs.
rule-based). 5Regretfully these two last sets were not the same.
This is due to the fact that the ?AF Test Set??was further used for evaluating Text-to-Speech systems, and thus a targeted subset of sentences was selected.
101 Sys E1 E2 E3 E4 E5 E6 E7 summationtext A 29 19 38 17 32 29 41 205 B 40 59 48 53 63 64 45 372 C 32 22 29 23 32 34 42 214 D 39 61 59 50 64 58 46 377 A 32 31 31 31 47 38 40 250 C 37 29 32 22 39 45 43 247 A 36 28 17 28 34 37 31 211 E 41 47 44 43 53 45 58 331 B 26 29 18 24 43 36 33 209 E 34 33 28 27 32 29 43 226 B 34 28 30 31 40 41 48 252 D 23 17 23 17 24 28 38 170 A 36 14 27 9 31 30 34 181 D 34 50 40 50 57 61 57 349 Final ranking (best?worst): E B D A C Table 2: Result of the binary system comparison.
Numbers of sentences for which each system was judged better by each evaluator (E1-E7).
Subset: Whole A+F Binary Sys BLEU BLEU A F BLEU A 36.3 36.2 2.93 2.46 36.3 B 49.4 49.3 3.74 3.58 49.2 C 36.3 36.2 3.53 3.31 36.1 D 48.2 46.8 3.68 3.48 47.7 E 49.8 49.6 3.67 3.46 49.4 Table 3: BLEU scores and Adequacy and Fluency scores for the different systems and subsets of the whole test set.
BLEU values in %, Adequacy (A) and Fluency (F) from 1 (worst) to 5 (best).
6 Discussion
In this section we will review the main drawbacks of the human evaluation listed in Section 2 and analyze how our approach deals with them.
The first one was the use of explicit numerical scores, which are difficult to define exactly.
Our system was mainly designed for the elimination of this issue.
Our evaluation continues to be time consuming.
Even more, the number of individual comparisons needed is in the order of log(n!), in contrast with the standard adequacy-fluency evaluation which needs 2n individual evaluations (two evaluations per system, one for fluency, another one for adequacy).
For n in the range of 1 up to 20 (a realistic number of systems for current evaluation campaigns) these two quantities are comparable.
And actually each of our C A D B E C A D B E C A D B E C A D B E a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 a71 Fluency Adequacy 1?WER BLEU 0.3 0.4 0.5 0.6 0.7 worse <?? normalized score ?? better Measure & System Figure 3: Normalized evaluation scores.
Higher scores are better.
Solid lines show the 95% confidence range.
Automatic scores calculated on the whole test set, human scores on the A+F subset.
evaluations should be simpler than the standard adequacy and fluency ones.
Therefore the time needed for both evaluation procedures is probably similar.
Reproducibilityoftheevaluationisalsoanimportant concern.
We computed the number of ?errors?? in the evaluation process, i.e. the number of sentences evaluated by two or more evaluators where the evaluators??judgement was different.
Only in 10% of the cases the evaluation was contradictory, in the sense that one evaluator chose one sentence as better than the other, while the other evaluator chose the other one.
In 30% of the cases, however, one evaluator estimated both sentences to be of the same quality while the other judged one sentence as superiortotheotherone.
Ascomparison, forthefluencyadequacy judgement nearly one third of the common evaluations have a difference in score greater or equal than two (where the maximum would be four), and another third a score difference of one point6.
With respect to biases, we feel that it is almost impossibletoeliminatethemifhumansareinvolved.
If one of the judges prefers one kind of structure, there will a bias for a system producing such output, independently of the evaluation procedure.
However, the suppression of explicit numerical scores eliminates an additional bias of evaluators.
It has been observed that human judges often give scores within 6Note however that possible evaluator biases can have a great influence in these statistics.
102 a certain range (e.g.
in the mid-range or only extreme values), which constitute an additional difficulty when carrying out the evaluation (Leusch et al., 2005).
Our method suppresses this kind of bias.
Another advantage of our method is the possibility of assessing improvements within one system.
With one evaluation we can decide if some modifications actually improve performance.
This evaluation even gives us a confidence interval to weight the significance of an improvement.
Carrying out a full adequacy-fluency analysis would require a lot more effort, without giving more useful results.
7 Conclusion
We presented a novel human evaluation technique thatsimplifiesthetaskoftheevaluators.
Ourmethod relies on two basic observations.
The first one is that in most evaluations the final goal is to find a ranking of different systems, the absolute scores are usually not so relevant.
Especially when considering human evaluation, the scores are not even comparable between two evaluation campaigns.
The second one is the fact that a human judge can normally choose the best one out of two translations, and this is a much easier process than the assessment of numerical scores whose definition is not at all clear.
Taking this into consideration we suggested a method that aims at finding a ranking of different MT systems based on the comparison of pairs of translation candidates for a set of sentences to be evaluated.
A detailed analysis of the statistical significance of the method is presented and also applied to some wide-spread automatic measures.
The evaluation methodology was applied for the ranking of 5 systems that participated in the second evaluation campaign of the TC-STAR project and comparison with standard evaluation measures was performed.
8 Acknowledgements
We would like to thank the human judges who participated in the evaluation.
This work has been funded by the integrated project TC-STAR??Technology and Corpora for Speech-to-Speech Translation ??(IST-2002-FP6-506738).
References M.
Bisani and H.
Ney. 2004.
Bootstrap estimates for confidence intervals in ASR performance evaluationx.
IEEE ICASSP, pages 409??12, Montreal, Canada, May.
T. Bui and M.
Thanh. 1985.
Significant improvements to the Ford-Johnson algorithm for sorting.
BIT Numerical Mathematics, 25(1):70??5.
C. Callison-Burch, M.
Osborne, and P.
Koehn. 2006.
Re-evaluating the role of BLEU in machine translation research.
Proceeding of the 11th Conference of the European Chapter of the ACL: EACL 2006, pages 249?? 256, Trento, Italy, Apr.
G. Doddington.
2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.
Proc. ARPA Workshop on Human Language Technology.
B. Efron and R.
J. Tibshirani.
1993. An Introduction to the Bootstrap.
Chapman & Hall, New York and London.
L.Ford Jr and S.Johnson. 1959.
ATournamentProblem. The American Mathematical Monthly, 66(5):387??89.
D. E.
Knuth. 1973.
The Art of Computer Programming, volume 3.
Addison-Wesley, 1st edition.
Sorting and Searching.
P. Koehn and C.
Monz. 2006.
Manual and automatic evaluation of machine translation between european languages.
Proceedings of the Workshop on Statistical Machine Translation, pages 102??21, New York City, Jun.
G. Leusch, N.
Ueffing, D.
Vilar, and H.
Ney. 2005.
Preprocessing and normalization for automatic evaluation of machine translation.
43rd ACL: Proc.
Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 17??4, Ann Arbor, Michigan, Jun.
F. J.
Och. 2003.
Minimum error rate training in statistical machine translation.
Proc. of the 41st ACL, pages 160??67, Sapporo, Japan, Jul.
K. Papineni, S.
Roukos, T.
Ward, and W.-J.
Zhu. 2002.
Bleu: a method for automatic evaluation of machine translation.
Proc. of the 40th ACL, pages 311??18, Philadelphia, PA, Jul.
M. Peczarski.
2002. Sorting 13 elements requires 34 comparisons.
LNCS, 2461/2002:785??94, Sep.
M.Snover, B.J.Dorr, R.Schwartz, J.Makhoul, L.Micciulla, and R.
Weischedel. 2005.
A study of translation error rate with targeted human annotation.
Technical Report LAMP-TR-126, CS-TR-4755, UMIACS-TR2005-58, University of Maryland, College Park, MD.
L. Thurstone.
1927. The method of paired comparisons forsocialvalues.
Journalof AbnormalandSocial Psychology, 21:384??00.
D. Vilar, E.
Matusov, S.
Hasan, R.
Zens, and H.
Ney. 2005.
Statistical Machine Translation of European Parliamentary Speeches.
Proceedings of MT Summit X, pages 259??66, Phuket, Thailand, Sep.
M. Wells.
1971. Elements of combinatorial computing.
Pergamon Press.
Y. Zhang and S.
Vogel. 2004.
Measuring confidence intervals for the machine translation evaluation metrics.
Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation, pages 4??, Baltimore, MD .
Proceedings of the Second Workshop on Statistical Machine Translation, pages 104??11, Prague, June 2007.
c2007 Association for Computational Linguistics Labelled Dependencies in Machine Translation Evaluation Karolina Owczarzak Josef van Genabith Andy Way National Centre for Language Technology School of Computing, Dublin City University Dublin 9, Ireland {owczarzak,josef,away}@computing.dcu.ie Abstract We present a method for evaluating the quality of Machine Translation (MT) output, using labelled dependencies produced by a Lexical-Functional Grammar (LFG) parser.
Our dependencybased method, in contrast to most popular string-based evaluation metrics, does not unfairly penalize perfectly valid syntactic variations in the translation, and the addition of WordNet provides a way to accommodate lexical variation.
In comparison with other metrics on 16,800 sentences of Chinese-English newswire text, our method reaches high correlation with human scores.
1 Introduction
Since the creation of BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), the subject of automatic evaluation metrics for MT has been given quite a lot of attention.
Although widely popular thanks to their speed and efficiency, both BLEU and NIST have been criticized for inadequate accuracy of evaluation at the segment level (Callison-Burch et al., 2006).
As string based-metrics, they are limited to superficial comparison of word sequences between a translated sentence and one or more reference sentences, and are unable to accommodate any legitimate grammatical variation when it comes to lexical choices or syntactic structure of the translation, beyond what can be found in the multiple references.
A natural next step in the field of evaluation was to introduce metrics that would better reflect our human judgement by accepting synonyms in the translated sentence or evaluating the translation on the basis of what syntactic features it shares with the reference.
Our method follows and substantially extends the earlier work of Liu and Gildea (2005), who use syntactic features and unlabelled dependencies to evaluate MT quality, outperforming BLEU on segment-level correlation with human judgement.
Dependencies abstract away from the particulars of the surface string (and syntactic tree) realization and provide a ?normalized??representation of (some) syntactic variants of a given sentence.
While Liu and Gildea (2005) calculate n-gram matches on non-labelled head-modifier sequences derived by head-extraction rules from syntactic trees, we automatically evaluate the quality of translation by calculating an f-score on labelled dependency structures produced by a LexicalFunctional Grammar (LFG) parser.
These dependencies differ from those used by Liu and Gildea (2005), in that they are extracted according to the rules of the LFG grammar and they are labelled with a type of grammatical relation that connects the head and the modifier, such as subject, determiner, etc.
The presence of grammatical relation labels adds another layer of important linguistic information into the comparison and allows us to account for partial matches, for example when a lexical item finds itself in a correct relation but with an incorrect partner.
Moreover, we use a number of best parses for the translation and the reference, which serves to decrease the amount of noise that can be introduced by the process of parsing and extracting dependency information.
The translation and reference files are analyzed by a treebank-based, probabilistic LFG parser (Cahill et al., 2004), which produces a set of dependency triples for each input.
The translation set is compared to the reference set, and the number of matches is calculated, giving the 104 precision, recall, and f-score for each particular translation.
In addition, to allow for the possibility of valid lexical differences between the translation and the references, we follow Kauchak and Barzilay (2006) in adding a number of synonyms in the process of evaluation to raise the number of matches between the translation and the reference, leading to a higher score.
In an experiment on 16,800 sentences of Chinese-English newswire text with segment-level human evaluation from the Linguistic Data Consortium?s (LDC) Multiple Translation project, we compare the LFG-based evaluation method with other popular metrics like BLEU, NIST, General Text Matcher (GTM) (Turian et al., 2003), Translation Error Rate (TER) (Snover et al., 2006)1, and METEOR (Banerjee and Lavie, 2005), and we show that combining dependency representations with synonyms leads to a more accurate evaluation that correlates better with human judgment.
Although evaluated on a different test set, our method also outperforms the correlation with human scores reported in Liu and Gildea (2005).
The remainder of this paper is organized as follows: Section 2 gives a basic introduction to LFG; Section 3 describes related work; Section 4 describes our method and gives results of the experiment on the Multiple Translation data; Section 5 discusses ongoing work; Section 6 concludes.
2 Lexical-Functional Grammar In Lexical-Functional Grammar (Kaplan and Bresnan, 1982; Bresnan, 2001) sentence structure is represented in terms of c(onstituent)-structure and f(unctional)-structure.
C-structure represents the word order of the surface string and the hierarchical organisation of phrases in terms of CFG trees.
F-structures are recursive feature (or attribute-value) structures, representing abstract grammatical relations, such as subj(ect), obj(ect), obl(ique), adj(unct), etc., approximating to predicate-argument structure or simple logical forms.
C-structure and f-structure are related in 1 We omit HTER (Human-Targeted Translation Error Rate), as it is not fully automatic and requires human input.
terms of functional annotations (attribute-value structure equations) in c-structure trees, describing f-structures.
While c-structure is sensitive to surface rearrangement of constituents, f-structure abstracts away from the particulars of the surface realization.
The sentences John resigned yesterday and Yesterday, John resigned will receive different tree representations, but identical f-structures, shown in (1).
(1) C-structure: F-structure: S NP VP | John V NP-TMP | | resigned yesterday SUBJ PRED john NUM sg PERS 3 PRED resign TENSE past ADJ {[PRED yesterday]} S NP NP VP | | | Yesterday John V | resigned SUBJ PRED john NUM sg PERS 3 PRED resign TENSE past ADJ {[PRED yesterday]} Note that if these sentences were a translationreference pair, they would receive a less-thanperfect score from string-based metrics.
For example, BLEU with add-one smoothing2 gives this pair a score of barely 0.3781.
This is because, although all three unigrams from the ?translation??
(John; resigned; yesterday) are present in the reference, which contains four items including the comma (Yesterday;,; John; resigned), the ?translation??contains only one bigram (John resigned) that matches the ?reference??(Yesterday,;, John; John resigned), and no matching trigrams.
The f-structure can also be described in terms of a flat set of triples.
In triples format, the fstructure in (1) is represented as follows: {subj(resign, john), pers(john, 3), num(john, sg), tense(resign, past), adj(resign, yesterday), pers(yesterday, 3), num(yesterday, sg)}.
2 We
use smoothing because the original BLEU metric gives zero points to sentences with fewer than one fourgram.
105 Cahill et al.(2004) presents a set of Penn-II Treebank-based LFG parsing resources.
Their approach distinguishes 32 types of dependencies, including grammatical functions and morphological information.
This set can be divided into two major groups: a group of predicate-only dependencies and non-predicate dependencies.
Predicate-only dependencies are those whose path ends in a predicate-value pair, describing grammatical relations.
For example, for the fstructure in (1), predicate-only dependencies would include: {subj(resign, john), adj(resign, yesterday)}.
Other predicate-only dependencies include: apposition, complement, open complement, coordination, determiner, object, second object, oblique, second oblique, oblique agent, possessive, quantifier, relative clause, topic, and relative clause pronoun.
The remaining non-predicate dependencies are: adjectival degree, coordination surface form, focus, complementizer forms: if, whether, and that, modal, number, verbal particle, participle, passive, person, pronoun surface form, tense, and infinitival clause.
In parser evaluation, the quality of the fstructures produced automatically can be checked against a set of gold standard sentences annotated with f-structures by a linguist.
The evaluation is conducted by calculating the precision and recall between the set of dependencies produced by the parser, and the set of dependencies derived from the human-created f-structure.
Usually, two versions of f-score are calculated: one for all the dependencies for a given input, and a separate one for the subset of predicate-only dependencies.
In this paper, we use the parser developed by Cahill et al.(2004), which automatically annotates input text with c-structure trees and f-structure dependencies, obtaining high precision and recall rates.
3 3 Related work 3.1 String-based metrics The insensitivity of BLEU and NIST to perfectly legitimate syntactic and lexical variation has been raised, among others, in Callison-Burch et al.(2006), but the criticism is widespread.
Even the 3 A demo of the parser can be found at http://lfgdemo.computing.dcu.ie/lfgparser.html creators of BLEU point out that it may not correlate particularly well with human judgment at the sentence level (Papineni et al., 2002).
Recently a number of attempts to remedy these shortcomings have led to the development of other automatic MT evaluation metrics.
Some of them concentrate mainly on word order, like General Text Matcher (Turian et al., 2003), which calculates precision and recall for translationreference pairs, weighting contiguous matches more than non-sequential matches, or Translation Error Rate (Snover et al., 2006), which computes the number of substitutions, insertions, deletions, and shifts necessary to transform the translation text to match the reference.
Others try to accommodate both syntactic and lexical differences between the candidate translation and the reference, like CDER (Leusch et al., 2006), which employs a version of edit distance for word substitution and reordering; or METEOR (Banerjee and Lavie, 2005), which uses stemming and WordNet synonymy.
Kauchak and Barzilay (2006) and Owczarzak et al.(2006) use paraphrases during BLEU and NIST evaluation to increase the number of matches between the translation and the reference; the paraphrases are either taken from WordNet4 in Kauchak and Barzilay (2006) or derived from the test set itself through automatic word and phrase alignment in Owczarzak et al.(2006). Another metric making use of synonyms is the linear regression model developed by Russo-Lassner et al.(2005), which makes use of stemming, WordNet synonymy, verb class synonymy, matching noun phrase heads, and proper name matching.
Kulesza and Shieber (2004), on the other hand, train a Support Vector Machine using features such as proportion of ngram matches and word error rate to judge a given translation?s distance from human-level quality.
3.2 Dependency-based metric The metrics described above use only string-based comparisons, even while taking into consideration reordering.
By contrast, Liu and Gildea (2005) present three metrics that use syntactic and unlabelled dependency information.
Two of these metrics are based on matching syntactic subtrees between the translation and the reference, and one 4 http://wordnet.princeton.edu/ 106 is based on matching headword chains, i.e. sequences of words that correspond to a path in the unlabelled dependency tree of the sentence.
Dependency trees are created by extracting a headword for each node of the syntactic tree, according to the rules used by the parser of Collins (1999), where every subtree represents the modifier information for its root headword.
The dependency trees for the translation and the reference are converted into flat headword chains, and the number of overlapping n-grams between the translation and the reference chains is calculated.
Our method, extending this line of research with the use of labelled LFG dependencies, partial matching, and n-best parses, allows us to considerably outperform Liu and Gildea?s (2005) highest correlations with human judgement (they report 0.144 for the correlation with human fluency judgement, 0.202 for the correlation with human overall judgement), although it has to be kept in mind that such comparison is only tentative, as their correlation is calculated on a different test set.
4 LFG
f-structure in MT evaluation LFG-based automatic MT evaluation reflects the same process that underlies the evaluation of parser-produced f-structure quality against a gold standard: we parse the translation and the reference, and then, for each sentence, we check the set of labelled translation dependencies against the set of labelled reference dependencies, counting the number of matches.
As a result, we obtain the precision and recall scores for the translation, and we calculate the f-score for the given pair.
4.1 Determining
parser noise Because we are comparing two outputs that were produced automatically, there is a possibility that the result will not be noise-free, even if the parser fails to provide a parse only in 0.1% of cases.
To assess the amount of noise that the parser introduces, Owczarzak et al.(2006) conducted an experiment where 100 English sentences were hand-modified so that the position of adjuncts was changed, but the sentence remained grammatical and the meaning was not influenced.
This way, an ideal parser should give both the source and the modified sentence the same f-structure, similarly to the example presented in (1).
The modified sentences were treated like a translation file, and the original sentences played the part of the reference.
Each set was run through the parser, and the dependency triples obtained from the ?translation??were compared against the dependency triples for the ?reference?? calculating the f-score.
Additionally, the same ?translationreference??set was scored with other metrics (TER, METEOR, BLEU, NIST, and GTM).
The results, including the distinction between f-scores for all dependencies and predicate-only dependencies, appear in Table 1.
baseline modified TER 0.0 6.417 METEOR 1.0 0.9970 BLEU 1.0000 0.8725 NIST 11.5232 11.1704 (96.94%) GTM 100 99.18 dep f-score 100 96.56 dep_preds f-score 100 94.13 Table 1.
Scores for sentences with reordered adjuncts The baseline column shows the upper bound for a given metric: the score which a perfect translation, word-for-word identical to the reference, would obtain.5 The other column lists the scores that the metrics gave to the ?translation??containing reordered adjunct.
As can be seen, the dependency and predicate-only dependency scores are lower than the perfect 100, reflecting the noise introduced by the parser.
We propose that the problem of parser noise can be alleviated by introducing a number of best parses into the comparison between the translation and the reference.
Table 2 shows how increasing the number of parses available for comparison brings our method closer to an ideal noise-free parser.
5 Two
things have to be noted here: (1) in the case of NIST the perfect score differs from text to text, which is why the percentage points are provided along the numerical score, and (2) in the case of TER the lower the score, the better the translation, so the perfect translation will receive 0, and there is no upper bound on the score, which makes this particular metric extremely difficult to directly compare with others.
107 dependency f-score 1 best 96.56 2 best 97.31 5 best 97.90 10 best 98.31 20 best 98.59 30 best 98.74 50 best 98.79 baseline 100 Table 2.
Dependency f-scores for sentences with reordered adjuncts with n-best parses available It has to be noted, however, that increasing the number of parses beyond a certain threshold does little to further improve results, and at the same time it considerably decreases the efficiency of the method, so it is important to find the right balance between these two factors.
In our opinion, the optimal value would be 10-best parses.
4.2 Correlation
with human judgement ??
MultiTrans 4.2.1 Experimental design To evaluate the correlation with human assessment, we used the data from the Linguistic Data Consortium Multiple Translation Chinese (MTC) Parts 2 and 4, which consists of multiple translations of Chinese newswire text, four humanproduced references, and segment-level human scores for a subset of the translation-reference pairs.
Although a single translated segment was always evaluated by more than one judge, the judges used a different reference every time, which is why we treated each translation-referencehuman score triple as a separate segment.
In effect, the test set created from this data contained 16,800 segments.
As in the previous experiment, the translation was scored using BLEU, NIST, GTM, TER, METEOR, and our labelled dependencybased method.
4.2.2 Labelled
dependency-based method We examined a number of modifications of the dependency-based method in order to find out which one gives the highest correlation with human scores.
The correlation differences between immediate neighbours in the ranking were often too small to be statistically significant; however, there is a clear overall trend towards improvement.
Besides the plain version of the dependency fscore, we also looked at the f-score calculated on predicate dependencies only (ignoring ?atomic?? features such as person, number, tense, etc.), which turned out not to correlate well with human judgements.
Another addition was the use of 2-, 10-, or 50best parses of the translation and reference sentences, which partially neutralized parser noise and resulted in increased correlations.
We also created a version where predicate dependencies of the type subj(resign,John) are split into two parts, each time replacing one of the elements participating in the relation with a variable, giving in effect subj(resign,x) and subj(y,John).
This lets us score partial matches, where one correct lexical object happens to find itself in the correct relation, but with an incorrect ?partner??
Lastly, we added WordNet synonyms into the matching process to accommodate lexical variation, and to compare our WordNet-enhanced method with the WordNet-enhanced version of METEOR.
4.2.3 Results
We calculated Pearson?s correlation coefficient for segment-level scores that were given by each metric and by human judges.
The results of the correlation are shown in Table 3.
Note that the correlation for TER is negative, because in TER zero is the perfect score, in contrast to other metrics where zero is the worst possible score; however, this time the absolute values can be easily compared to each other.
Rows are ordered by the highest value of the (absolute) correlation with the human score.
First, it seems like none of the metrics is very good at reflecting human fluency judgments; the correlation values in the first column are significantly lower than the correlation with accuracy.
This finding has been previously reported, among others, in Liu and Gildea (2005).
However, the dependency-based method in almost all its versions has decidedly the highest correlation in this area.
This can be explained by the method?s sensitivity to the grammatical structure of the sentence: a more grammatical translation is also a translation that is more fluent.
As to the correlation with human evaluation of translation accuracy, our method currently falls 108 short of METEOR.
This is caused by the fact that METEOR assign relatively little importance to the position of a specific word in a sentence, therefore rewarding the translation for content rather than linguistic form.
Interestingly, while METEOR, with or without WordNet, considerably outperforms all other metrics when it comes to the correlation with human judgements of translation accuracy, it falls well behind most versions of our dependency-based method in correlation with human scores of translation fluency.
Surprisingly, adding partial matching to the dependency-based method resulted in the greatest increase in correlation levels, to the extent that the partial-match versions consistently outperformed versions with a larger number of parses available but without the partial match.
The most interesting effect was that the partial-match versions (even those with just a single parse) offered results comparable to or higher than the addition of WordNet to the matching process when it comes to accuracy and overall judgement.
5 Current
and future work Fluency and accuracy are two very different aspects of translation quality, each with its own set of conditions along which the input is evaluated.
Therefore, it seems unfair to expect a single automatic metric to correlate highly with human judgements of both at the same time.
This pattern is very noticeable in Table 3: if a metric is (relatively) good at correlating with fluency, its accuracy correlation suffers (GTM might serve as an example here), and the opposite holds as well (see METEOR?s scores).
It does not mean that any improvement that increases the method?s correlation with one aspect will result in a decrease in the correlation with the other aspect; but it does suggest that a possible way of development would be to target these correlations separately, if we want our automated metrics to reflect human scores better.
At the same time, string-based metrics might have already exhausted their potential when it comes to increasing their correlation with human evaluation; as has been pointed out before, these metrics can only tell us that two strings differ, but they cannot distinguish legitimate grammatical variance from ungrammatical variance.
As the quality of MT Table 3.
Pearson?s correlation between human scores and evaluation metrics.
Legend: d = dependency f-score, _pr = predicate-only f-score, 2, 10, 50 = n-best parses; var = partial-match version; M = METEOR, WN = WordNet6 improves, the community will need metrics that are more sensitive in this respect.
After all, the true quality of MT depends on producing grammatical output which describes the same concept as the source utterance, and the string identity with a reference is only a very selective approximation of this goal.
6 In
general terms, an increase of 0.022 or more between any two scores in the same column is significant with a 95% confidence interval.
The statistical significance of correlation differences was calculated using Fisher?s z?? transformation and the general formula for confidence interval.
fluency accuracy average d_50+WN 0.177 M+WN 0.294 M+WN 0.255 d+WN 0.175 M 0.278 d_50_var 0.252 d_50_var 0.174 d_50_var 0.273 d_50+WN 0.250 GTM 0.172 NIST 0.273 d_10_var 0.250 d_10_var 0.172 d_10_var 0.273 d_2_var 0.247 d_50 0.171 d_2_var 0.270 d+WN 0.244 d_2_var 0.168 d_50+WN 0.269 d_50 0.243 d_10 0.168 d_var 0.266 d_var 0.243 d_var 0.165 d_50 0.262 M 0.242 d_2 0.164 d_10 0.262 d_10 0.242 d 0.161 d+WN 0.260 NIST 0.238 BLEU 0.155 d_2 0.257 d_2 0.237 M+WN 0.153 d 0.256 d 0.235 M 0.149 d_pr 0.240 d_pr 0.216 NIST 0.146 GTM 0.203 GTM 0.208 d_pr 0.143 BLEU 0.199 BLEU 0.197 TER -0.133 TER -0.192 TER -0.182 109 In order to maximize the correlation with human scores of fluency, we plan to look more closely at the parser output, and implement some basic transformations which would allow an even deeper logical analysis of input (e.g.
passive to active voice transformation).
Additionally, we want to take advantage of the fact that the score produced by the dependencybased method is the proportional average of matches for a group of up to 32 (but usually far fewer) different dependency types.
We plan to implement a set of weights, one for each dependency type, trained in such a way as to maximize the correlation of the final dependency fscore with human evaluation.
In a preliminary experiment, for example, assigning a low weight to the topic dependency increases our correlations slightly (this particular case can also be seen as a transformation into a more basic logical form by removing non-elementary dependency types).
In a similar direction, we want to experiment more with the f-score calculations.
Initial check shows that assigning a higher weight to recall than to precision improves results.
To improve the correlation with accuracy judgements, we would like to experiment using a paraphrase set derived from a large parallel corpus, as described in Owczarzak et al.(2006). While retaining the advantage of having a similar size to a corresponding set of WordNet synonyms, this set will also capture low-level syntactic variations, which can increase the number of matches.
6 Conclusions
In this paper we present a linguisticallymotivated method for automatically evaluating the output of Machine Translation.
Most currently used popular metrics rely on comparing translation and reference on a string level.
Even given reordering, stemming, and synonyms for individual words, current methods are still far from reaching human ability to assess the quality of translation, and there exists a need in the community to develop more dependable metrics.
Our method explores one such direction of development, comparing the sentences on the level of their grammatical structure, as exemplified by their fstructure labelled dependency triples produced by an LFG parser.
In our experiments we showed that the dependency-based method correlates higher than any other metric with human evaluation of translation fluency, and shows high correlation with the average human score.
The use of dependencies in MT evaluation has not been extensively researched before (one exception here would be Liu and Gildea (2005)), and requires more research to improve it, but the method shows potential to become an accurate evaluation metric.
Acknowledgements This work was partly funded by Microsoft Ireland PhD studentship 2006-8 for the first author of the paper.
We would also like to thank our reviewers and Dan Melamed for their insightful comments.
All remaining errors are our own.
References Satanjeev Banerjee and Alon Lavie.
2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.
Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization at the Association for Computational Linguistics Conference 2005: 65-73.
Ann Arbor, Michigan.
Joan Bresnan.
2001. Lexical-Functional Syntax, Blackwell, Oxford.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef van Genabith, and Andy Way.
2004. Long-Distance Dependency Resolution in Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations, In Proceedings of Association for Computational Linguistics 2004: 320-327.
Barcelona, Spain.
Chris Callison-Burch, Miles Osborne and Philipp Koehn.
2006. Re-evaluating the role of BLEU in Machine Translation Research.
Proceedings of the European Chapter of the Association for Computational Linguistics 2006: 249-256.
Oslo, Norway.
Michael J.
Collins. 1999.
Head-driven Statistical Models for Natural Language Parsing.
Ph.D. thesis, University of Pennsylvania, Philadelphia.
George Doddington.
2002. Automatic Evaluation of MT Quality using N-gram Co-occurrence Statistics.
Proceedings of Human Language Technology Conference 2002: 138-145.
San Diego, California.
Kaplan, R.
M., and J.
Bresnan. 1982.
Lexical-functional Grammar: A Formal System for Grammatical Representation.
In J.
Bresnan (ed.), The Mental Representation of Grammatical Relations.
MIT Press, Cambridge.
David Kauchak and Regina Barzilay.
2006. Paraphrasing for Automatic Evaluation.
Proceedings of Human Language Technology ??North American Chapter of the Association for Computational Linguistics Conference 2006: 45-462.
New York, New York.
Philipp Koehn.
2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models.
Proceedings of the Workshop on Machine Translation: From real users to research at the Association for Machine Translation in the Americas Conference 2004: 115-124.
Washington, DC.
Philipp Koehn.
2005. Europarl: A Parallel Corpus for Statistical Machine Translation.
Proceedings of MT Summit 2005: 79-86.
Phuket, Thailand.
Alex Kulesza and Stuart M.
Shieber. 2004.
A learning approach to improving sentence-level MT evaluation.
In Proceedings of the Conference on Theoretical and Methodological Issues in Machine Translation 2004: 75-84.
Baltimore, Maryland.
Gregor Leusch, Nicola Ueffing and Hermann Ney.
2006. CDER: Efficient MT Evaluation Using Block Movements.
Proceedings of European Chapter of the Association for Computational Linguistics Conference 2006: 241-248.
Trento, Italy.
Ding Liu and Daniel Gildea.
2005. Syntactic Features for Evaluation of Machine Translation.
In Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization at the Association for Computational Linguistics Conference 2005.
Ann Arbor, Michigan.
Franz Josef Och and Hermann Ney.
2003. A Systematic Comparison of Various Statistical Alignment Modes.
Computational Linguistics, 29:19-51.
Karolina Owczarzak, Declan Groves, Josef van Genabith, and Andy Way.
2006. Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation.
Proceedings of the Workshop on Statistical Machine Translation at the Human Language Technology ??North American Chapter of the Association for Computational Linguistics Conference 2006: 86-93.
New York, New York.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu.
2002. BLEU: a method for automatic evaluation of machine translation.
In Proceedings of Association for Computational Linguistics Conference 2002: 311-318.
Philadelphia, Pennsylvania.
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.
2005. A Paraphrase-based Approach to Machine Translation Evaluation.
Technical Report LAMP-TR-125/CS-TR-4754/UMIACS-TR-2005-57, University of Maryland, College Park, Maryland.
Mathew Snover, Bonnie Dorr, Richard Schwartz, John Makhoul, Linnea Micciula.
2006. A Study of Translation Error Rate with Targeted Human Annotation.
Proceedings of the Association for Machine Translation in the Americas Conference 2006: 223-231.
Boston, Massachusetts.
Joseph P.
Turian, Luke Shen, and I.
Dan Melamed.
2003. Evaluation of Machine Translation and Its Evaluation.
Proceedings of MT Summit 2003: 386-393.
New Orleans, Luisiana.
Ying Zhang and Stephan Vogel.
2004. Measuring confidence intervals for the machine translation evaluation metrics.
Proceedings of Conference on Theoretical and Methodological Issues in Machine Translation 2004: 85-94.
Baltimore, Maryland.
Proceedings of the Second Workshop on Statistical Machine Translation, pages 112??19, Prague, June 2007.
c2007 Association for Computational Linguistics An Iteratively-Trained Segmentation-Free Phrase Translation Model for Statistical Machine Translation Robert C.
Moore Chris Quirk Microsoft Research Redmond, WA 98052, USA {bobmoore,chrisq}@microsoft.com Abstract Attempts to estimate phrase translation probablities for statistical machine translation using iteratively-trained models have repeatedly failed to produce translations as good as those obtained by estimating phrase translation probablities from surface statistics of bilingual word alignments as described by Koehn, et al.(2003). We propose a new iteratively-trained phrase translation model that produces translations of quality equal to or better than those produced by Koehn, et al.?s model.
Moreover, with the new model, translation quality degrades much more slowly as pruning is tightend to reduce translation time.
1 Introduction
Estimates of conditional phrase translation probabilities provide a major source of translation knowledge in phrase-based statistical machine translation (SMT) systems.
The most widely used method for estimating these probabilities is that of Koehn, et al.(2003), in which phrase pairs are extracted from word-aligned bilingual sentence pairs, and their translation probabilities estimated heuristically from surface statistics of the extracted phrase pairs.
We will refer to this approach as ?the standard model??
There have been several attempts to estimate phrase translation probabilities directly, using generative models trained iteratively on a parallel corpus using the Expectation Maximization (EM) algorithm.
The first of these models, that of Marcu and Wong (2002), was found by Koehn, et al.(2003), to produce translations not quite as good as their method.
Recently, Birch et al.(2006) tried the Marcu and Wong model constrained by a word alignment and also found that Koehn, et al.?s model worked better, with the advantage of the standard model increasing as more features were added to the overall translation model.
DeNero et al.(2006) tried a different generative phrase translation model analogous to IBM word-translation Model 3 (Brown et al., 1993), and again found that the standard model outperformed their generative model.
DeNero et al.(2006) attribute the inferiority of their model and the Marcu and Wong model to a hidden segmentation variable, which enables the EM algorithm to maximize the probability of the training data without really improving the quality of the model.
We propose an iteratively-trained phrase translation model that does not require different segmentations to compete against one another, and we show that this produces translations of quality equal to or better than those produced by the standard model.
We find, moreover, that with the new model, translation quality degrades much more slowly as pruning is tightend to reduce translation time.
Decoding efficiency is usually considered only in the design and implementation of decoding algorithms, or the choice of model structures to support faster decoding algorithms.
We are not aware of any attention previously having been paid to the effect of different methods of parameter estimation on translation efficiency for a given model structure.
The time required for decoding is of great importance in the practical application of SMT tech112 nology.
One of the criticisms of SMT often made by adherents of rule-based machine translation is that SMT is too slow for practical application.
The rapidly falling price of computer hardware has ameliorated this problem to a great extent, but the fact remains that every factor of 2 improvement in translation efficiency means a factor of 2 decrease in hardware cost for intensive applications of SMT, such as a web-based translation service (?Translate this page??.
SMT surely needs all the help in can get in this regard.
2 Previous
Approaches Koehn, et al.?s (2003) method of estimating phrasetranslation probabilities is very simple.
They start with an automatically word-aligned corpus of bilingual sentence pairs, in which certain words are linked, indicating that they are translations of each other, or that they are parts of phrases that are translations of each other.
They extract every possible phrase pair (up to a given length limit) that (a) contains at least one pair of linked words, and (b) does not contain any words that have links to other words not included in the phrase pair.1 In other words, word alignment links cannot cross phrase pair boundaries.
Phrase translation probabilities are estimated simply by marginalizing the counts of phrase instances: p(x|y) = C(x,y)summationtext xprime C(xprime,y) This method is used to estimate the conditional probabilities of both target phrases give source phrases and source phrases given target phrases.
In contrast to the standard model, DeNero, et al.(2006) estimate phrase translation probabilities according to the following generative model: 1.
Begin with a source sentence a.
2. Stochastically segment a into some number of phrases.
3. For each selected phrase in a, stochastically choose a phrase position in the target sentence b that is being generated.
1This method of phrase pair extraction was originally described by Och et al.(1999). 4.
For each selected phrase in a and the corresponding phrase position in b, stochastically choose a target phrase.
5. Read off the target sentence b from the sequence of target phrases.
DeNero et al.?s analysis of why their model performs relatively poorly hinges on the fact that the segmentation probabilities used in step 2 are, in fact, not trained, but simply assumed to be uniform.
Given complete freedom to select whatever segmentation maximizes the likelihood of any given sentence pair, EM tends to favor segmentations that yield source phrases with as few occurrences as possible, since more of the associated conditional probability mass can be concentrated on the target phrase alignments that are possible in the sentence at hand.
Thus EM tends to maximize the probability of the training data by concentrating probability mass on the rarest source phrases it can construct to cover the training data.
The resulting probability estimates thus have less generalizability to unseen data than if probability mass were concentrated on more frequently occurring source phrases.
3 A
Segmentation-Free Model To avoid the problem identified by DeNero et al., we propose an iteratively-trained model that does not assume a segmentation of the training data into non-overlapping phrase pairs.
We refer to our model as ?iteratively-trained??rather than ?generative??because we have not proved any of the mathematical properties usually associated with generative models; e.g., that the training procedure maximizes the likelihood of the training data.
We will motivate the model, however, with a generative story as to how phrase alignments are produced, given a pair of source and target sentences.
Our model extends to phrase alignment the concept of a sentence pair generating a word alignment developed by Cherry and Lin (2003).
Our model is defined in terms of two stochastic processes, selection and alignment, as follows: 1.
For each word-aligned sentence pair, we identify all the possible phrase pair instances according to the criteria used by Koehn et al.113 2.
Each source phrase instance that is included in any of the possible phrase pair instances independently selects one of the target phrase instances that it forms a possible phrase pair instance with.
3. Each target phrase instance that is included in any of the possible phrase pair instances independently selects one of the source phrase instances that it forms a possible phrase pair instance with.
4. A source phrase instance is aligned to a target phrase instance, if and only if each selects the other.
Given a set of selection probability distributions and a word-aligned parallel corpus, we can easily compute the expected number of alignment instances for a given phrase pair type.
The probability of a pair of phrase instances x and y being aligned is simply ps(x|y)  ps(y|x), where ps is the applicable selection probability distribution.
The expected number of instances of alignment, E(x,y), for the pair of phrases x and y, is just the sum of the alignment probabilities of all the possible instances of that phrase pair type.
From the expected number of alignments and the total number of occurrences of each source and target phrase type in the corpus (whether or not they particpate in possible phrase pairs), we estimate the conditional phrase translation probabilities as pt(y|x) = E(x,y)C(x), pt(x|y) = E(x,y)C(y), where E denotes expected counts, and C denotes observed counts.
The use of the total observed counts of particular source and target phrases (instead of marginalized expected joint counts) in estimating the conditional phrase translation probabilities, together with the multiplication of selection probabilities in computing the alignment probability of particular phrase pair instances, causes the conditional phrase translation probability distributions generally to sum to less than 1.0.
We interpret the missing probability mass as the probability that a given word sequence does not translate as any contiguous word sequence in the other language.
We have seen how to derive phrase translation probabilities from the selection probabilities, but where do the latter come from?
We answer this question by adding the following constraint to the model: The probabilty of a phrase y selecting a phrase x is proportional to the probability of x translating as y, normalized over the possible non-null choices for x presented by the word-aligned sentence pair.
Symbolically, we can express this as ps(x|y) = pt(y|x)summationtext xprime pt(y|xprime) where ps denotes selection probability, pt denotes translation probability, and xprime ranges over the phrase instances that could possibly align to y.
We are, in effect, inverting and renormalizing translation probabilities to get selection probabilities.
The reason for the inversion may not be immediately apparent, but it in fact simply generalizes the e-step formula in the EM training for IBM Model 1 from words to phrases.
This model immediately suggests (and, in fact, was designed to suggest) the following EM-like training procedure: 1.
Initialize the translation probability distributions to be uniform.
(It doesn?t matter at this point whether the possibility of no translation is included or not.) 2.
E step: Compute the expected phrase alignment counts according to the model, deriving the selection probabilities from the current estimates of the translation probabilities as described.
3. M step: Re-estimate the phrase translation probabilities according to the expected phrase alignment counts as described.
4. Repeat the E and M steps, until the desired degree of convergence is obtained.
We view this training procedure as iteratively trying to find a set of phrase translation probabilities that satisfies all the constraints of the model, although we have not proved that this training procedure always converges.
We also have not proved that 114 the procedure maximizes the likelihood of anything, although we find empirically that each iteration decreases the conditional entropy of the phrase translation model.
In any case, the training procedure seems to work well in practice.
It is also very similar to the joint training procedure for HMM wordalignment models in both directions described by Liang et al.(2006), which was the original inspiration for our training procedure.
4 Experimental
Set-Up and Data We evaluated our phrase translation model compared to the standard model of Koehn et al.in the context of a fairly typical end-to-end phrase-based SMT system.
The overall translation model score consists of a weighted sum of the following eight aggregated feature values for each translation hypothesis: ??the sum of the log probabilities of each source phrase in the hypothesis given the corresponding target phrase, computed either by our model or the standard model, ??the sum of the log probabilities of each target phrase in the hypothesis given the corresponding source phrase, computed either by our model or the standard model, ??the sum of lexical scores for each source phrase given the corresponding target phrase, ??the sum of lexical scores for each target phrase given the corresponding source phrase, ??the log of the target language model probability for the sequence of target phrases in the hypothesis, ??the total number of words in the target phrases in the hypothesis, ??the total number of source/target phrase pairs composing the hypothesis, ??the distortion penalty as implemented in the Pharaoh decoder (Koehn, 2003).
The lexical scores are computed as the (unnormalized) log probability of the Viterbi alignment for a phrase pair under IBM word-translation Model 1 (Brown et al., 1993).
The feature weights for the overall translation models were trained using Och?s (2003) minimum-error-rate training procedure.
The weights were optimized separately for our model and for the standard phrase translation model.
Our decoder is a reimplementation in Perl of the algorithm used by the Pharaoh decoder as described by Koehn (2003).2 The data we used comes from an English-French bilingual corpus of Canadian Hansards parliamentary proceedings supplied for the bilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003).
Automatic sentence alignment of this data was provided by Ulrich Germann.
We used 500,000 sentences pairs from this corpus for training both the phrase translation models and IBM Model 1 lexical scores.
These 500,000 sentence pairs were word-aligned using a state-ofthe-art word-alignment method (Moore et al., 2006).
A separate set of 500 sentence pairs was used to train the translation model weights, and two additional held-out sets of 2000 sentence pairs each were used as test data.
The two phrase translation models were trained using the same set of possible phrase pairs extracted from the word-aligned 500,000 sentence pair corpus, finding all possible phrase pairs permitted by the criteria followed by Koehn et al., up to a phrase length of seven words.
This produced approximately 69 million distinct phrase pair types.
No pruning of the set of possible phrase pairs was done during or before training the phrase translation models.
Our phrase translation model and IBM Model 1 were both trained for five iterations.
The training procedure for our phrase translation model trains models in both directions simultaneously, but for IBM Model 1, models were trained separately in each direction.
The models were then pruned to include only phrase pairs that matched the source sides of the small training and test sets.
5 Entropy
Measurements To verify that our iterative training procedure was behaving as expected, after each training iteration 2Since Perl is a byte-code interpreted language, absolute decoding times will be slower than with the standard machinelanguage-compiled implementation of Pharaoh, but relative times between models should be comparable.
115 we measured the conditional entropy of the model in predicting English phrases given French phrases, according to the formula H(E|F) =summationdisplay f p(f)summationdisplay e pt(e|f)log2 pt(e|f), where e and f range over the English and French phrases that occur in the extracted phrase pairs, and p(f) was estimated according to the relative frequency of these French phrases in a 2000 sentence sample of the French sentences from the 500,000 word-aligned sentence pairs.
Over the five training iterations, we obtained a monotonically decreasing sequence of entropy measurements in bits per phrase: 1.329, 1.177, 1.146, 1.140, 1.136.
We also compared the conditional entropy of the standard model to the final iteration of our model, estimating p(f) using the first of our 2000 sentence pair test sets.
For this data, our model measured 1.38 bits per phrase, and the standard model measured 4.30 bits per phrase.
DeNero et al.obtained corresponding measurements of 1.55 bits per phrase and 3.76 bits per phrase, for their model and the standard model, using a different data set and a slightly different estimation method.
6 Translation
Experiments We wanted to look at the trade-off between decoding time and translation quality for our new phrase translation model compared to the standard model.
Since this trade-off is also affected by the settings of various pruning parameters, we compared decoding time and translation quality, as measured by BLEU score (Papineni et al, 2002), for the two models on our first test set over a broad range of settings for the decoder pruning parameters.
The Pharaoh decoding algorithm, has five pruning parameters that affect decoding time: ??Distortion limit ??Translation table limit ??Translation table threshold ??Beam limit ??Beam threshold The distortion limit is the maximum distance allowed between two source phrases that produce adjacent target phrases in the decoder output.
The distortion limit can be viewed as a model parameter, as well as a pruning paramter, because setting it to an optimum value usually improves translation quality over leaving it unrestricted.
We carried out experiments with the distortion limit set to 1, which seemed to produce the highest BLEU scores on our data set with the standard model, and also set to 5, which is perhaps a more typical value for phrasebased SMT systems.
Translation model weights were trained separately for these two settings, because the greater the distortion limit, the higher the distortion penalty weight needed for optimal translation quality.
The translation table limit and translation table threshold are applied statically to the phrase translation table, which combines all components of the overall translation model score that can be computed for each phrase pair in isolation.
This includes all information except the distortion penalty score and the part of the language model score that looks at n-grams that cross target phrase boundaries.
The translation table limit is the maximum number of translations allowed in the table for any given source phrase.
The translation table threshold is the maximum difference in combined translation table score allowed between the highest scoring translation and lowest scoring translation for any given source phrase.
The beam limit and beam threshold are defined similarly, but they apply dynamically to the sets of competing partial hypotheses that cover the same number of source words in the beam search for the highest scoring translation.
For each of the two distortion limits we tried, we carried out a systematic search for combinations of settings of the other four pruning parameters that gave the best trade-offs between decoding time and BLEU score.
Starting at a setting of 0.5 for the threshold parameters3 and 5 for the limit parameters we performed a hill-climbing search over step-wise relaxations of all combinations of the four parame3We use difference in weighted linear scores directly for our pruning thresholds, whereas the standard implementation of Pharaoh expresses these as probability ratios.
Hence the specific values for these parameters are not comparable to published descriptions of experiments using Pharaoh, although the effects of pruning are exactly the same.
116 30.2 30.3 30.4 30.5 0.1 1 10 100 BLE U[% ] milliseconds per word Figure 1: BLEU vs Decoding Time (DL = 1) re-estimated phrase table standard phrase table re-estimated phrase table convex hull standard phrase table convex hull 29.4 29.6 29.8 30 30.2 30.4 30.6 1 10 100 1000 BLEU [%] milliseconds per word Figure 2: BLEU vs Decoding Time (DL = 5) re-estimated phrase table standard phrase table re-estimated phrase table convex hull standard phrase table convex hull ters, incrementing the threshold parameters by 0.5 and the limit parameters by 5 at each step.
For each resulting point that provided the best BLEU score yet seen for the amount of decoding time used, we iterated the search.
The resulting possible combinations of BLEU score and decoding time for the two phrase translation models are displayed in Figure 1, for a distortion limit of 1, and Figure 2, for a distortion limit of 5.
BLEU score is reported on a scale of 1??00 (BLEU[%]), and decoding time is measured in milliseconds per word.
Note that the decoding time axis is presented on a log scale.
The points that represent pruning parameter settings one might consider using in a practical system are those on or near the upper convex hull of the set of points for each model.
These upper-convexhull points are highlighted in the figures.
Points far from these boundaries represent settings of one or more of the parameters that are too restrictive to obtain good translation quality, together with settings of other parameters that are too permissive to obtain good translation time.
Examining the results for a distortion limit of 1, we found that the BLEU score obtained with the loosest pruning parameter settings (2.5 for both threshold paramters, and 25 for both limit parameters) were essentially identical for the two models: 30.42 BLEU[%].
As the pruning parameters are tightened to reduce decoding time, however, the new model performs much better.
At a decoding time almost 6 times faster than for the settings that produced the highest BLEU score, the change in score was only ??.07 BLEU[%] with the new model.
To obtain a slightly worse4 BLEU score (??.08 BLEU[%]) using the standard model took 90% more decoding time.
It does appear, however, that the best BLEU score for the standard model is slightly better than the best BLEU score for the new model: 30.43 vs.
30.42. It is in fact currious that there seem to be numerous points where the standard model gets a slightly 4Points on the convex hulls with exactly comparable BLEU scores do not often occur.
117 better BLEU score than it does with with the loosest pruning settings, which should have the lowest search error.
We conjectured that this might be an artifact of our test procedure.
If a model is at all reasonable, most search errors will reduce the ultimate objective function, in our case the BLEU score, but occasionally a search error will increase the objective function just by chance.
The smaller the number of search errors in a particular test, the greater the likelihood that, by chance, more search errors will increase the objective function than decrease it.
Since we are sampling a fairly large number of combinations of pruning parameter settings (179 for the standard model with a distortion limit of 1), it is possible that a small number of these have more ?good??search errors than ?bad??search errors simply by chance, and that this accounts for the small number of points (13) at which the BLEU score exceeds that of the point which should have the fewest search errors.
This effect may be more pronounced with the standard model than with the new model, simply because there is more noise in the standard model.
To test the hypothesis that the BLEU scores greater than the score for the loosest pruning settings simply represent noise in the data, we collected all the pruning settings that produced BLEU scores greater than or equal to the the one for the loosest pruning settings, and evaluated the standard model at those settings on our second held-out test set.
We then looked at the correlation between the BLEU scores for these settings on the two test sets, and found that it was very small and negative, with r = ??.099.
The standard F-test for the significance of a correlation yielded p = 0.74; in other words, completely insignificant.
This strongly suggests that the apparent improvement in BLEU score for certain tighter pruning settings is illusory.
As a sanity check, we tested the BLEU score correlation between the two test sets for the points on the upper convex hull of the plot for the standard model, between the point with the fastest decoding time and the point with the highest BLEU score.
That correlation was very high, with r = 0.94, which was significant at the level p = 0.0004 according to the F-test.
Thus the BLEU score differences along most of the upper convex hull seem to reflect reality, but not in the region where they equal or exceed the score for the loosest pruning settings.
At a distortion limit of 5, there seems no question that the new model performs better than the standard model.
The difference BLEU scores for the upperconvex-hull points ranges from about 0.8 to 0.2 BLEU[%] for comparable decoding times.
Again, the advantage of the new model is greater at shorter decoding times.
Compared to the results with a distortion limit of 1, the standard model loses translation quality, with a change of about ??.2 BLEU[%] for the loosest pruning settings, while the new model gains very slightly (+0.04 BLEU[%]).
7 Conclusions
This study seems to confirm DeNero et al.?s diagnosis that the main reason for poor performance of previous iteratively-trained phrase translation models, compared to Koehn et al.?s model, is the effect of the hidden segmentation variable in these models.
We have developed an iteratively-trained phrase translation model that is segmentation free, and shown that, at a minimum, it eliminates the shortfall in BLEU score compared to the standard model.
With a larger distortion limit, the new model produced translations with a noticably better BLEU score.
From a practical point of view, the main result is probably that BLEU score degrades much more slowly with our model than with the standard model, when the decoding search is tuned for speed.
For some settings that appear reasonable, this difference is close to a factor of 2, even if there is no difference in the translation quality obtainable when pruning is loosened.
For high-demand applications like web page translation, roughly half of the investment in translation servers could be saved while providing this level of translation quality with the same response time.
Acknowledgement The authors would like to thank Mark Johnson for many valuable discussions of how to analyze and present the results obtained in this study.
References Alexandra Birch, Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Constraining the Phrase-Based, Joint Probability Statistical Translation Model.
In Proceedings of the HLTNAACL 06 Workshop, Statistical Machine Translation, pp.
154??57, New York City, New York, USA.
Peter F.
Brown, Stephen A.
Della Pietra, Vincent J.
Della Pietra, and Robert L.
Mercer. 1993.
The Mathematics of Statistical Machine Translation: Parameter Estimation.
Computational Linguistics, 19(2):263??11.
Colin Cherry and Dekang Lin.
2003. A Probability Model to Improve Word Alignment.
In Proceedings of the 41st Annual Meeting of the ACL, pp.
88??5, Sapporo, Japan.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why Generative Phrase Models Underperform Surface Heuristics.
In Proceedings of the HLT-NAACL 06 Workshop, Statistical Machine Translation, pp.
31??8, New York City, New York, USA.
Philipp Koehn.
2003. Noun Phrase Translation.
PhD Dissertation, Computer Science, University of Southern California, Los Angeles, California, USA.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation.
In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pp.
127??33, Edmonton, Alberta, Canada.
Percy Liang, Ben Taskar, and Dan Klein.
2006. Alignment by Agreement.
In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pp.
104??11, New York City, New York, USA.
Daniel Marcu and William Wong.
2002. A Phrase-Based, Joint Probability Model for Statistical Machine Translation.
In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pp.
133??39, Philadelphia, Pennsylvania, USA.
Rada Mihalcea and Ted Pedersen.
2003. An Evaluation Exercise for Word Alignment.
In Proceedings of the HLT-NAACL 2003 Workshop, Building and Using Parallel Texts: Data Driven Machine Translation and Beyond, pp.
1??, Edmonton, Alberta, Canada.
Robert C.
Moore, Wen-tau Yih, and Andreas Bode.
2006. Improved Discriminative Bilingual Word Alignment.
In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pp.
513-520, Sydney, Australia.
Franz Joseph Och, Christoff Tillmann, and Hermann Ney.
1999. Improved Alignment Models for Statistical Machine Translation.
In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pp.
20??8, College Park, Maryland, USA.
Franz Joseph Och.
2003. Minimum Error Rate Training in Statistical Machine Translation.
In Proceedings of the 41st Annual Meeting of the ACL, pp.
160??67, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002. BLEU: a Method for Automatic Evaluation of Machine Translation.
In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pp.
311?? 318, Philadelphia, Pennsylvania, USA.
Proceedings of the Second Workshop on Statistical Machine Translation, pages 120??27, Prague, June 2007.
c2007 Association for Computational Linguistics Using Paraphrases for Parameter Tuning in Statistical Machine Translation Nitin Madnani, Necip Fazil Ayan, Philip Resnik & Bonnie J.
Dorr Institute for Advanced Computer Studies University of Maryland College Park, MD, 20742 {nmadnani,nfa,resnik,bonnie}@umiacs.umd.edu Abstract Most state-of-the-art statistical machine translation systems use log-linear models, whicharedefinedintermsofhypothesisfeatures and weights for those features.
It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations.
However, obtaining reference translations is expensive.
In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resultingparaphrasescanbeusedtodrasticallyreducethenumberofhumanreferencetranslations needed for parameter tuning, without a significant decrease in translation quality.
1 Introduction
Viewed at a very high level, statistical machine translationinvolvesfourphases: languageandtranslation model training, parameter tuning, decoding, and evaluation (Lopez, 2007; Koehn et al., 2003).
SincetheirintroductioninstatisticalMTbyOchand Ney (2002), log-linear models have been a standard way to combine sub-models in MT systems.
Typically such a model takes the form summationdisplay i i?i( f,e) (1) where ?i are features of the hypothesis e and i are weights associated with those features.
Selecting appropriate weights i is essential in order to obtain good translation performance.
Och (2003) introduced minimum error rate training (MERT), a technique for optimizing log-linear modelparametersrelativetoameasureoftranslation quality.
This has become much more standard than optimizing the conditional probability of the trainingdatagiventhemodel(i.e.,amaximumlikelihood criterion), as was common previously.
Och showed thatsystemperformanceisbestwhenparametersare optimizedusingthesameobjectivefunctionthatwill be used for evaluation; BLEU (Papineni et al., 2002) remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used, e.g., (Banerjee and Lavie, 2005; Snover et al., 2006).
Minimum error rate training?and more generally, optimization of parameters relative to a translation quality measure?relies on data sets in which source language sentences are paired with (sets of) reference translations.
It is widely agreed that, at least for the widely used BLEU criterion, which is based on n-gram overlap between hypotheses and reference translations, the criterion is most accurate when computed with as many distinct reference translationsaspossible.
Intuitivelythismakessense: if there are alternative ways to phrase the meaning of the source sentence in the target language, then the translation quality criterion should take as many of those variations into account as possible.
To do otherwise is to risk the possibility that the criterion might judge good translations to be poor when they fail to match the exact wording within the reference translations that have been provided.
This reliance on multiple reference translations createsaproblem, becausereferencetranslationsare labor intensive and expensive to obtain.
A common source of translated data for MT research is the Linguistic Data Consortium (LDC), where an elaborate process is undertaken that involves translation agencies, detailed translation guidelines, and quality control processes (Strassel et al., 2006).
Some 120 efforts have been made to develop alternative processes for eliciting translations, e.g., from users on the Web (Oard, 2003) or from informants in lowdensity languages (Probst et al., 2002).
However, reference translations for parameter tuning and evaluation remain a severe data bottleneck for such approaches.
Note, however, one crucial property of reference translations: they are paraphrases, i.e., multiple expressions of the same meaning.
Automatic techniques exist for generating paraphrases.
Although one would clearly like to retain human translations as the benchmark for evaluation of translation, might it be possible to usefully increase the number of reference translations for tuning by using automatic paraphrase techniques?
In this paper, we demonstrate that it is, in fact, possible to do so.
Section 2 briefly describes our translation framework.
Section 3 lays out a novel technique for paraphrasing, designed with the application to parameter tuning in mind.
Section 4 presentsevaluationresultsusingastateoftheartstatistical MT system, demonstrating that half the human reference translations in a standard 4-reference tuning set can be replaced with automatically generated paraphrases, with nosignificant decrease inMT system performance.
In Section 5 we discuss related work, and in Section 6 we summarize the results and discuss plans for future research.
2 Translation
Framework The work described in this paper makes use of the Hiero statistical MT framework (Chiang, 2007).
Hiero is formally based on a weighted synchronous context-free grammar (CFG), containing synchronous rules of the form X ???e, f,?k1( f,e,X)??(2) where X is a symbol from the nonterminal alphabet, and e and f can contain both words (terminals) andvariables(nonterminals)thatserveasplaceholders for other phrases.
In the context of statistical MT,wherephrase-basedmodelsarefrequentlyused, these synchronous rules can be interpreted as pairs of hierarchical phrases.
The underlying strength of a hierarchical phrase is that it allows for effective learning of not only the lexical re-orderings, but phrasal re-orderings, as well.
Each ?(e, f,X) denotes a feature function defined on the pair of hierarchical phrases.1 Feature functions represent conditional and joint co-occurrence probabilities over the hierarchical paraphrase pair.
The Hiero framework includes methods to learn grammars and feature values from unannotated parallel corpora, without requiring syntactic annotation of the data.
Briefly, training a Hiero model proceeds as follows: ??GIZA++ (Och and Ney, 2000) is run on the parallel corpus in both directions, followed by an alignment refinement heuristic that yields a many-to-many alignment for each parallel sentence.
??Initial phrase pairs are identified following the procedure typically employed in phrase based systems (Koehn et al., 2003; Och and Ney, 2004).
??Grammar rules in the form of equation (2) are induced by ?subtracting??out hierarchical phrase pairs from these initial phrase pairs.
??Fractional counts are assigned to each produced rule: c(X ???e, f??
= msummationdisplay j=1 1 njr (3) where m is the number of initial phrase pairs that give rise to this grammar rule and njr is the number of grammar rules produced by the jth initial phrase pair.
??Feature functions ?k1( f,e,X) are calculated for each rule using the accumulated counts.
Oncetraininghastakenplace,minimumerrorrate training (Och, 2003) is used to tune the parameters i.
Finally, decoding in Hiero takes place using a CKY synchronous parser with beam search, augmented to permit efficient incorporation of language model scores (Chiang, 2007).
Given a source language sentence f, the decoder parses the source language sentence using the grammar it has learned 1Currently only one nonterminal symbol is used in Hiero productions.
121 during training, with parser search guided by the model; a target-language hypothesis is generated simultaneously via the synchronous rules, and the yieldofthathypothesizedanalysisrepresentsthehypothesized string e in the target language.
3 Generating
Paraphrases As discussed in Section 1, our goal is to make it possible to accomplish the parameter-tuning phase using fewer human reference translations.
We accomplish this by beginning with a small set of human reference translations for each sentence in the development set, and expanding that set by automatically paraphrasing each member of the set rather than by acquiring more human translations.
Most previous work on paraphrase has focused on high quality rather than coverage (Barzilay and Lee, 2003; Quirk et al., 2004), but generating artificial references for MT parameter tuning in our setting has two unique properties compared to other paraphrase applications.
First, we would like to obtain 100% coverage, in order to avoid modifications to our minimum error rate training infrastructure.2 Second, we prefer that paraphrases be as distinct as possible from the original sentences, while retaining as much of the original meaning as possible.
In order to satisfy these two properties, we approach sentence-level paraphrase for English as a problem of English-to-English translation, constructing the model using English-F translation, for a second language F, as a pivot.
Following Bannard and Callison-Burch (2005), we first identify English-to-F correspondences, then map from English to English by following translation units from English to F and back.
Then, generalizing their approach, we use those mappings to create a well defined English-to-English translation model.
The parameters of this model are tuned using MERT, and then the model is used in an the (unmodified) statistical MT system, yielding sentence-level English paraphrases by means of decoding input English sentences.
The remainder of this section presents this process in detail.
2Strictly speaking, this was not a requirement of the approach, but rather a concession to practical considerations.
3.1 Mapping
and Backmapping We employ the following strategy for the induction oftherequiredmonolingualgrammar.
First,wetrain the Hiero system in standard fashion on a bilingual English-F training corpus.
Then, for each existing production in the resulting Hiero grammar, we create multiple new English-to-English productions by pivoting on the foreign hierarchical phrase in the rule.
For example, assume that we have the following toy grammar for English-F, as produced by Hiero: X ????e1, f1??
X ????e3, f1??
X ????e1, f2??
X ????e2, f2??
X ????e4, f2??
If we use the foreign phrase f1 as a pivot and backmap, we can extract the two English-to-English rules: X ????e1, e3??and X ????e3, e1??
Backmapping using both f1 and f2 produces the following new rules (ignoring duplicates and rules that map any English phrase to itself): X ????e1, e2??
X ????e1, e3??
X ????e1, e4??
X ????e2, e1??
X ????e2, e4?? 3.2 Feature values Each rule production in a Hiero grammar is weighted by several feature values defined on the rule themselves.
In order to perform accurate backmapping, we must recompute these feature functions for the newly created English-to-English grammar.
Rather than computing approximations based on feature values already existing in the bilingual Hiero grammar, we calculate these features in a more principled manner, by computing maximum likelihood estimates directly from the fractional counts that Hiero accumulates in the penultimate training step.
We use the following features in our induced English-to-English grammar:3 3Hiero also uses lexical weights (Koehn et al., 2003) in both 122 ??The joint probability of the two English hierarchical paraphrases, conditioned on the nonterminal symbol, as defined by this formula: p(e1, e2|x) = c(X ???e1, e2??summationtext e1prime, e2prime c(X ???e1prime, e2prime??
= c(X ???e1, e2??c(X) (4) where the numerator is the fractional count of the rule under consideration and the denominator represents the marginal count over all the English hierarchical phrase pairs.
??The conditionals p(e1,x|e2) and p(e2,x|e1) defined as follows: p(e1,x|e2) = c(X ???e1, e2??summationtext e1prime c(X ???e1prime, e2??
(5) p(e2,x|e1) = c(X ???e1, e2??summationtext e2prime c(X ???e1, e2prime??
(6) Finally, for all induced rules, we calculate a word penalty exp(?T(e2)), where T(e2) just counts the number of terminal symbols in e2.
This feature allows the model to learn whether it should produce shorter or longer paraphrases.
Inadditiontothefeaturesabovethatareestimated from the training data, we also use a trigram language model.
Since we are decoding to produce English sentences, we can use the same language model employed in a standard statistical MT setting.
Calculating the proposed features is complicated by the fact that we don?t actually have the counts for English-to-English rules because there is no English-to-English parallel corpus.
This is where the counts provided by Hiero come into the picture.
We estimate the counts that we need as follows: c(X ???e1, e2??
= summationdisplay f c(X ???e1, f??c(X ???e2, f??
(7) An intuitive way to think about the formula above is by using an example at the corpus level.
Assume that, in the given bilingual parallel corpus, there are m sentences in which the English phrase directions as features but we don?t use them for our grammar.
e1 co-occurs with the foreign phrase f and n sentences in which the same foreign phrase f co-occurs with the English phrase e2.
The problem can then be thought of as defining a function g(m,n) which computes the number of sentences in a hypothetical English-to-English parallel corpus wherein the phrases e1 and e1 co-occur.
For this paper, we define g(m,n) to be the upper bound mn.
Tables 1 and 2 show some examples of paraphrases generated by our system across a range of paraphrase quality for two different pivot languages.
3.3 Tuning
Model Parameters Although the goal of the paraphrasing approach is to make it less data-intensive to tune log-linear model parameters for translation, our paraphrasing approach, since it is based on an English-to-English log-linear model, also requires its own parameter tuning.
This, however, is straightforward: regardless of how the paraphrasing model will be used in statistical MT, e.g., irrespective of source language,itispossibletouseanyexistingsetofEnglish paraphrases as the tuning set for English-to-English translation.
We used the 2002 NIST MT evaluation test set reference translations.
For every item in the set, we randomly chose one sentence as the source sentence, and the remainder as the ?reference translations?forpurposesofminimumerrorratetraining.
4 Evaluation
Havingdevelopedaparaphrasingapproachbasedon English-to-English translation, we evaluated its use in improving minimum error rate training for translation from a second language into English.
Generating paraphrases via English-to-English translation makes use of a parallel corpus, from which a weighted synchronous grammar is automatically acquired.
Although nothing about our approachrequiresthattheparaphrasesystem?straining bitext be the same one used in the translation experiments (see Section 6), doing so is not precluded, either, and it is a particularly convenient choice when the paraphrasing is being done in support of MT.4 The training bitext comprised of Chinese-English 4The choice of the foreign language used as the pivot should not really matter but it is worth exploring this using other language pairs as our bitext.
123 O: we must bear in mind the community as a whole. P: we must remember the wider community . O: thirdly, the implications of enlargement for the union ?s regional policy cannot be overlooked . P: finally, the impact of enlargement for eu regional policy cannot be ignored . O: how this works in practice will become clear when the authority has to act . P: how this operate in practice will emerge when the government has to play . O: this is an ill-advised policy . P: this is an unwelcome in europe . Table 1: Example paraphrases with French as the pivot language.
O = Original Sentence, P = Paraphrase.
O: alcatel added that the company?s whole year earnings would be announced on february 4 . P: alcatel said that the company?s total annual revenues would be released on february 4 . O: he was now preparing a speech concerning the us policy for the upcoming world economic forum . P: he was now ready to talk with regard to the us policies for the forthcoming international economic forum . O: tibet has entered an excellent phase of political stability, ethnic unity and people living in peace . P: tibetans have come to cordial political stability, national unity and lived in harmony . O: its ocean and blue-sky scenery and the mediterranean climate make it world?s famous scenic spot . P: its harbour and blue-sky appearance and the border situation decided it world?s renowned tourist attraction . Table 2: Example paraphrases with Chinese as the pivot language.
O = Original Sentence, P = Paraphrase.
Corpus # Sentences # Words HK News 542540 11171933 FBIS 240996 9121210 Xinhua 54022 1497562 News1 9916 314121 Treebank 3963 125848 Total 851437 22230674 Table 3: Chinese-English corpora used as training bitext both for paraphrasing and for evaluation.
parallelcorporacontaining850,000sentencepairs?? approx.
22 million words (details shown in Table 3).
As the source of development data for minimum error rate training, we used the 919 source sentences and human reference translations from the 2003 NIST Chinese-English MT evaluation exercise.
As raw material for experimentation, we generated a paraphrase for each reference sentence via 1-best decoding using the English-to-English translation approach of Section 3.
As our test data, we used the 1082 source sentences and human reference translations from the 2005 NIST Chinese-English MT evaluation.
Our core experiment involved three conditions where the only difference was the set of references for the development set used for tuning feature weights.
For each condition, once the weights were tuned, they were used to decode the test set.
Note that for all the conditions, the decoded test set was alwaysscoredagainstthesamefourhigh-qualityhuman reference translations included with the set.
The three experimental conditions were designed around the constraint that our development set contains a total of four human reference translations per sentence, and therefore a maximum of four human references with which to compute an upper bound: ??Baseline (2H): For each item in the development set, we randomly chose two of the four human-constructed reference translations as references for minimum error rate training.
??Expanded (2H + 2P): For each of the two human references in the baseline tuning set, we automatically generated a corresponding paraphrase using (1-best) English-to-English translation, decoding using the model developed in Section 3.
This condition represents the critical case in which you have a limited number of hu124 man references (two, in this case) and augment themwithartificiallygeneratedreferencetranslations.
This yields a set of four references for minimum error rate training (two human, two paraphrased), which permits a direct comparison against the upper bound of four humangenerated reference translations.
??Upper bound: 4H: We performed minimum error rate training using the four human references from the development set.
In addition to these core experimental conditions, we added a fourth condition to assess the effect on performance when all four human reference translations are used in expanding the reference set via paraphrase: ??Expanded(4H+4P): ThisisthesameasCondition 2, but using all four human references.
Note that since we have only four human references per item, this fourth condition does not permit comparison with an upper bound of eight human references.
Table 4 shows BLEU and TER scores on the test set for all four conditions.5 If only two human references were available (simulated by using only two of the available four), expanding to four using paraphrases would yield a clear improvement.
Using bootstrap resampling to compute confidence intervals (Koehn, 2004), we find that the improvement in BLEU score is statistically significant at p < .01.
Equally interesting, expanding the number of reference translations from two to four using paraphrases yields performance that approaches the upper bound obtained by doing MERT using all four human reference translations.
The difference in BLEU between conditions 2 and 3 is not significant.
Finally, our fourth condition asks whether it is possible to improve MT performance given the typical four human reference translations used for MERT in most statistical MT systems, by adding a paraphrase to each one for a total eight references per translation.
There is indeed further improvement, although the difference in BLEU score does not reach significance.
5We plan to include METEOR scores in future experiments.
Condition References used BLEU TER 1 2 H 30.43 59.82 2 2 H + 2 P 31.10 58.79 3 4 H 31.26 58.66 4 4 H + 4 P 31.68 58.24 Table 4: BLEU and TER scores showing utility of paraphrased reference translations.
H = human references, P = paraphrased references.
We also evaluated our test set using TER (Snover etal.,2006)andobservedthattheTERscoresfollow the same trend as the BLEU scores.
Specifically, the TER scores demonstrate that using paraphrases to artificially expand the reference set is better than using only 2 human reference translations and as good as using 4 human reference translations.6 5 Related Work The approach we have taken here arises from a typical situation in NLP systems: the lack of sufficient data to accurately estimate a model based on supervised training data.
In a structured prediction problem such as MT, we have an example input and a single labeled, correct output.
However, this output is chosen from a space in which the number of possible outputs is exponential in the input size, and in which there are many good outputs in this space (although they are vastly outnumbered by the bad outputs).
Various discriminative learning methods have attempted to deal with the first of these issues, often by restricting the space of examples.
For instance, some max-margin methods restrict their computations to a set of examples from a ?feasible set,?? where they are expected to be maximally discriminative (Tillmann and Zhang, 2006).
The present approach deals with the second issue: in a learning problem where the use of a single positive example is likely to be highly biased, how can we produce a set of positive examples that is more representative of the space of correct outcomes?
Our method exploits alternative sources of information to produce new positive examples that are, we hope, reasonably likely to represent a consensus of good examples.
Quite a bit of work has been done on paraphrase, 6We anticipate doing significance tests for differences in TER in future work.
125 some clearly related to our technique, although in general previous work has been focused on human readability rather than high coverage, noisy paraphrases for use downstream in an automatic process.
At the sentence level, (Barzilay and Lee, 2003) employed an unsupervised learning approach to cluster sentences and extract lattice pairs from comparable monolingual corpora.
Their technique produces a paraphrase only if the input sentence matches any of the extracted lattice pairs, leading to a bias strongly favoring quality over coverage.
They were able to generate paraphrases for 59 sentences (12%) out of a 484-sentence test set, generating no paraphrases at all for the remainder.
Quirk et al.(2004) also generate sentential paraphrases using a monolingual corpus.
They use IBM Model-1 scores as the only feature, and employ a monotone decoder (i.e., one that cannot produce phrase-level reordering).
This approach emphasizes very simple ?substitutions of words and short phrases,??and, in fact, almost a third of their best sentential ?paraphrases??are identical to the input sentence.
A number of other approaches rely on parallel monolingual data and, additionally, require parsing of the training sentences (Ibrahim et al., 2003; Pang et al., 2003).
Lin and Pantel (2001) use a non-parallelcorpusandemployadependencyparser and computation of distributional similarity to learn paraphrases.
There has also been recent work on using paraphrases to improve statistical machine translation.
Callison-Burch et al.(2006) extract phrase-level paraphrases by mapping input phrases into a phrase table and then mapping back to the source language.
However, they do not generate paraphrases of entire sentences,butinsteademployparaphrasestoaddentries to an existing phrase table solely for the purpose of increasing source-language coverage.
Other work has incorporated paraphrases into MT evaluation: Russo-Lassner et al.(2005) use a combination of paraphrase-based features to evaluate translation output; Zhou et al.(2006) propose a new metric that extends n-gram matching to include synonyms and paraphrases; and Lavie?s METEOR metric (Banerjee and Lavie, 2005) can be used with additionalknowledgesuchasWordNetinordertosupport inexact lexical matches.
6 Conclusions
and Future Work We introduced an automatic paraphrasing technique based on English-to-English translation of full sentences using a statistical MT system, and demonstrated that, using this technique, it is possible to cut in half the usual number of reference translations used for minimum error rate training with no significant loss in translation quality.
Our method enables the generation of paraphrases for thousands of sentences in a very short amount of time (much shorter than creating other low-cost human references).
This might prove beneficial for various discriminative training methods (Tillmann and Zhang, 2006).
This has important implications for data acquisition strategies For example, it suggests that rather than obtaining four reference translations per sentence for development sets, it may be more worthwhile to obtain fewer translations for a wider range of sentences, e.g., expanding into new topics and genres.
In addition, this approach can significantly increase the utility of datasets which include only a single reference translation.
A number of future research directions are possible.
First, since we have already demonstrated that noisy paraphrases can nonetheless add value, it would be straightforward to explore the quantity/quality tradeoff by expanding the MERT reference translations with n-best paraphrases for n > 1.
We also plan to conduct an intrinsic evaluation of the quality of paraphrases that our technique generates.
It is important to note that a different tradeoff ratio may lead to even better results, e.g, using only the paraphrased references when they pass some goodness threshold, as used in Ueffing?s (2006) selftraining MT approach.
We have also observed that named entities are usually paraphrased incorrectly if there is a genre mismatchbetweenthetrainingandthetestdata.
The Hiero decoder allows spans of source text to be annotated with inline translations using XML.
We plan to identify and annotate named entities in the English source so that they are left unchanged.
Also,sincethelanguageF forEnglish-F pivoting is arbitrary, we plan to investigate using English-toEnglish grammars created using multiple English-F grammars based on different languages, both indi126 vidually and in combination, in order to improve paraphrase quality.
We also plan to explore a wider range of paraphrase-creation techniques, ranging from simple word substitutions (e.g., based on WordNet) to usingthepivottechniquewithothertranslationssystems.
7 Acknowledgments
We are indebted to David Chiang, Adam Lopez and Smaranda Muresan for insights and comments.
This work has been supported under the GALE program of the Defense Advaned Research Projects Agency, ContractNo.HR0011-06-2-001.
Anyopinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the view of DARPA.
References S.
Banerjee and A.
Lavie. 2005.
Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.
In Proceedings of Workshop on Intrinsic and Extrinsic Evaluation Measuresfor MT and/or Summarization at ACL.
Colin Bannard and Chris Callison-Burch.
2005. Paraphrasing with bilingual parallel corpora.
In Proceedings of ACL.
Regina Barzilay and Lillian Lee.
2003. Learning to paraphrase: An unsupervised approach using multiple-sequence alignment.
In Proceedings of HLT-NAACL.
Chris Callison-Burch, Philipp Koehn, and Miles Osborne.
2006. Improved statistical machine translation using paraphrases.
In Proceedings of HLT-NAACL.
David Chiang.
2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
A. Ibrahim, B.
Katz, and J.
Lin. 2003.
Extracting structural paraphrases from aligned monolingual corpora.
In Proceedings the Second International Workshop on Paraphrasing (ACL 2003).
Philipp Koehn, Franz Och, and Daniel Marcu.
2003. Statistical phrase-based translation.
In Proceedings of HLT-NAACL.
Philipp Koehn.
2004. Statistical significance tests for machine translation evaluation.
In Proceedings of EMNLP.
Dekang Lin and Patrick Pantel.
2001. DIRT discovery of inference rules from text.
In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
A. Lopez.
2007. A survey of statistical machine translation.
Technical Report 2006-47, University of Maryland, College Park.
D. W.
Oard. 2003.
The surprise langauge exercises.
ACM Transactions on Asian Language Information Processing, 2(3).
Franz J.
Och and Hermann Ney.
2000. Improved statistical alignment models.
In Proceedings of ACL.
Franz J.
Och and Hermann Ney.
2002. Discriminative training and maximum entropy models for statistical machine translation.
In Proceedings of ACL.
Franz Och and Hermann Ney.
2004. The alignment template approach to statistical machine translation.
Computational Linguistics, 30(4).
Franz Josef Och.
2003. Minimum error rate training in statistical machine translation.
In Proceedings of ACL.
Bo Pang, Kevin Knight, and Daniel Marcu.
2003. Syntax-based alignment of multiple translations: Extracting paraphrases and generating new sentences.
In Proceedings of HLT/NAACL.
K. Papineni, S.
Roukos, T.
Ward, and W.-J.
Zhu. 2002.
Bleu: a method for automatic evaluation of machine translation.
In Proceedings of ACL.
K. Probst, L.
Levin, E.
Peterson, A.
Lavie, and J.
Carbonell. 2002.
Mt for minority languages using elicitation-based learning of syntactic transfer rules.
Machine Translation, 17(4).
Chris Quirk, Chris Brockett, and William Dolan.
2004. Monolingual machine translation for paraphrase generation.
In Proceedings of EMNLP 2004.
Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.2005.
A paraphrase-based approach to machine translation evaluation.
Technical Report UMIACS-TR-2005-57, University of Maryland, College Park.
M. Snover, B.
Dorr, R.
Schwartz, L.
Micciulla, and J.Makhoul. 2006.
A study of translation edit rate with targeted human annotation.
In Proceedings of AMTA.
S. Strassel, C.
Cieri, A.
Cole, D.
DiPersio, M.
Liberman, X.
Ma, M.
Maamouri, and K.
Maeda. 2006.
Integrated linguistic resources for language exploitation technologies.
In Proceedings of LREC.
Christoph Tillmann and Tong Zhang.
2006. A discriminative global training algorithm for statistical MT.
In Proceedings of ACL.
Nicola Ueffing.
2006. Using monolingual source-language data to improve MT performance.
In Proceedings of IWSLT.
L. Zhou, C.-Y.
Lin, D.
Muntenau, and E.
Hovy. 2006.
ParaEval: Using paraphrases to evaluate summaries automatically.
In Proceedings of HLT-NAACL .
Proceedings of the Second Workshop on Statistical Machine Translation, pages 128??35, Prague, June 2007.
c2007 Association for Computational Linguistics Mixture-Model Adaptation for SMT George Foster and Roland Kuhn National Research Council Canada first.last@nrc.gc.ca Abstract We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components.
We investigate a number of variants on this approach, including cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to.
The best methods achieve gains of approximately one BLEU percentage point over a state-of-the art non-adapted baseline system.
1 Introduction
Language varies significantly across different genres, topics, styles, etc.
This affects empirical models: a model trained on a corpus of car-repair manuals, for instance, will not be well suited to an application in the field of tourism.
Ideally, models should be trained on text that is representative of the area in which they will be used, but such text is not always available.
This is especially the case for bilingual applications, because parallel training corpora are relatively rare and tend to be drawn from specific domains such as parliamentary proceedings.
In this paper we address the problem of adapting a statistical machine translation system by adjusting its parameters based on some information about a test domain.
We assume two basic settings.
In cross-domain adaptation, a small sample of parallel in-domain text is available, and it is used to optimize for translating future texts drawn from the same domain.
In dynamic adaptation, no domain information is available ahead of time, and adaptation is based on the current source text under translation.
Approaches developed for the two settings can be complementary: an in-domain development corpus can be used to make broad adjustments, which can then be fine tuned for individual source texts.
Our method is based on the classical technique of mixture modeling (Hastie et al., 2001).
This involves dividing the training corpus into different components, training a model on each part, then weighting each model appropriately for the current context.
Mixture modeling is a simple framework that encompasses many different variants, as described below.
It is naturally fairly low dimensional, because as the number of sub-models increases, the amount of text available to train each, and therefore its reliability, decreases.
This makes it suitable for discriminative SMT training, which is still a challenge for large parameter sets (Tillmann and Zhang, 2006; Liang et al., 2006).
Techniques for assigning mixture weights depend on the setting.
In cross-domain adaptation, knowledge of both source and target texts in the in-domain sample can be used to optimize weights directly.
In dynamic adaptation, training poses a problem because no reference text is available.
Our solution is to construct a multi-domain development sample for learning parameter settings that are intended to generalize to new domains (ones not represented in the sample).
We do not learn mixture weights directly with this method, because there is little hope 128 that these would be well suited to new domains.
Instead we attempt to learn how weights should be set as a function of distance.
To our knowledge, this approach to dynamic adaptation for SMT is novel, and it is one of the main contributions of the paper.
A second contribution is a fairly broad investigation of the large space of alternatives defined by the mixture-modeling framework, using a simple genrebased corpus decomposition.
We experimented with the following choices: cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; various text distance metrics; different ways of converting distance metrics into weights; and granularity of the source unit being adapted to.
The remainder of the paper is structured follows: section 2 briefly describes our phrase-based SMT system; section 3 describes mixture-model adaptation; section 4 gives experimental results; section 5 summarizes previous work; and section 6 concludes.
2 Phrase-based Statistical MT Our baseline is a standard phrase-based SMT system (Koehn et al., 2003).
Given a source sentence s, this tries to find the target sentence ?t that is the most likely translation of s, using the Viterbi approximation: ?t = argmax t p(t|s) ??argmax t,a p(t,a|s), where alignment a = (?s1,?t1,j1),...,(?sK,?tK,jK); ?tk are target phrases such that t = ?t1...?tK; ?sk are source phrases such that s = ?sj1 ...?sjK; and ?sk is the translation of the kth target phrase ?tk.
To model p(t,a|s), we use a standard loglinear approach: p(t,a|s) ??exp bracketleftBiggsummationdisplay i ifi(s,t,a) bracketrightBigg (1) where each fi(s,t,a) is a feature function, and weights i are set using Och?s algorithm (Och, 2003) to maximize the system?s BLEU score (Papineni et al., 2001) on a development corpus.
The features used in this study are: the length of t; a single-parameter distortion penalty on phrase reordering in a, as described in (Koehn et al., 2003); phrase translation model probabilities; and 4-gram language model probabilities logp(t), using Kneser-Ney smoothing as implemented in the SRILM toolkit.
Phrase translation model probabilities are features of the form: logp(s|t,a) ??summationtextKk=1 logp(?sk|?tk).
We use two different estimates for the conditional probabilities p(?t|?s) and p(?s|?t): relative frequencies and ?lexical??probabilities as described in (Zens and Ney, 2004).
In both cases, the ?forward??phrase probabilities p(?t|?s) are not used as features, but only as a filter on the set of possible translations: for each source phrase ?s that matches some ngram in s, only the 30 top-ranked translations ?t according to p(?t|?s) are retained.
To derive the joint counts c(?s,?t) from which p(?s|?t) and p(?t|?s) are estimated, we use the phrase induction algorithm described in (Koehn et al., 2003), with symmetrized word alignments generated using IBM model 2 (Brown et al., 1993).
3 Mixture-Model Adaptation Our approach to mixture-model adaptation can be summarized by the following general algorithm: 1.
Split the corpus into different components, according to some criterion.
2. Train a model on each corpus component.
3. Weight each model according to its fit with the test domain: ??For cross-domain adaptation, set parameters using a development corpus drawn from the test domain, and use for all future documents.
??For dynamic adaptation, set global parameters using a development corpus drawn from several different domains.
Set mixture weights as a function of the distances from corpus components to the current source text.
4. Combine weighted component models into a single global model, and use it to translate as described in the previous section.
We now describe each aspect of this algorithm in more detail.
129 3.1 Corpus Decomposition We partition the corpus into different genres, defined as being roughly identical to corpus source.
This is the simplest way to exploit heterogeneous training material for adaptation.
An alternative, which we have not explored, would be to cluster the corpus automatically according to topic.
3.2 Component
Models We adapt both language and translation model features within the overall loglinear combination (1).
To train translation models on each corpus component, we used a global IBM2 model for word alignment (in order to avoid degradation in alignment quality due to smaller training corpora), then extracted component-specific relative frequencies for phrase pairs.
Lexical probabilities were also derived from the global IBM2 model, and were not adapted.
The procedure for training component-specific language models on the target halves of each corpus component is identical to the procedure for the global model described in section 2.
In addition to the component models, we also used a large static global model.
3.3 Combining
Framework The most commonly-used framework for mixture models is a linear one: p(x|h) = summationdisplay c cpc(x|h) (2) where p(x|h) is either a language or translation model; pc(x|h) is a model trained on component c, and c is the corresponding weight.
An alternative, suggested by the form of the global model, is a loglinear combination: p(x|h) = productdisplay c pc(x|h)c where we write c to emphasize that in this case the mixing parameters are global weights, like the weights on the other features within the loglinear model.
This is in contrast to linear mixing, where the combined model p(x|h) receives a loglinear weight, but the weights on the components do not participate in the global loglinear combination.
One consequence is that it is more difficult to set linear weights using standard minimum-error training techniques, which assume only a ?flat??loglinear model.
3.4 Distance
Metrics We used four standard distance metrics to capture the relation between the current source or target text q and each corpus component.1 All are monolingual?they are applied only to source text or only to target text.
The tf/idf metric commonly used in information retrieval is defined as cos(vc,vq), where vc and vq are vectors derived from component c and document q, each consisting of elements of the form: ?p(w)log ?pdoc(w), where ?p(w) is the relative frequency of word w within the component or document, and pdoc(w) is the proportion of components it appears in.
Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is a technique for implicitly capturing the semantic properties of texts, based on the use of Singular Value Decomposition to produce a rankreduced approximation of an original matrix of word and document frequencies.
We applied this technique to all documents in the training corpus (as opposed to components), reduced the rank to 100, then calculated the projections of the component and document vectors described in the previous paragraph into the reduced space.
Perplexity (Jelinek, 1997) is a standard way of evaluating the quality of a language model on a test text.
We define a perplexity-based distance metric pc(q)1/|q|, where pc(q) is the probability assigned to q by an ngram language model trained on component c.
The final distance metric, which we call EM, is based on expressing the probability of q as a wordlevel mixture model: p(q) = producttext|q|i=1summationtextc dcpc(wi|hi), where q = w1 ...w|q|, and pc(w|h) is the ngram probability of w following word sequence h in component c.
It is straighforward to use the EM algorithm to find the set of weights ?dc,?c that maximizes the likelihood of q.
The weight ?dc is defined as the distance to component c.
For all experiments described below, we used a probability difference threshold of 0.001 as the EM convergence criterion.
1Although we refer to these metrics as distances, most are in fact proximities, and we use the convention throughout that higher values mean closer.
130 3.5 Learning Adaptive Parameters Our focus in this paper is on adaptation via mixture weights.
However, we note that the usual loglinear parameter tuning described in section 2 can also be considered adaptation in the cross-domain setting, because learned preferences for word penalty, relative LM/TM weighting, etc, will reflect the target domain.
This is not the case for dynamic adaptation, where, in the absence of an in-domain development corpus, the only information we can hope to glean are the weights on adapted models compared to other features of the system.
The method used for adapting mixture weights depends on both the combining framework (loglinear versus linear), and the adaptive setting (crossdomain versus dynamic), as described below.
3.5.1 Setting
Loglinear Mixture Weights When using a loglinear combining framework as described in section 3.3, mixture weights are set in the same way as the other loglinear parameters when performing cross-domain adaptation.
Loglinear mixture models were not used for dynamic adaptation.
3.5.2 Setting
Linear Mixture Weights For both adaptive settings, linear mixture weights were set as a function of the distance metrics described in section 3.4.
Given a set of metrics {D1,...,Dm}, let di,c be the distance from the current text to component c according to metric Di.
A simple approach to weighting is to choose a single metric Di, and set the weights in (2) to be proportional to the corresponding distances: c = di,c/ summationdisplay cprime di,cprime.
(3) Because different distance metrics may capture complementary information, and because optimal weights might be a non-linear function of distance, we also experimented with a linear combination of metrics transformed using a sigmoid function: c = msummationdisplay i=1 i 1+exp(ai(bi ?di,c)) (4) where i reflects the relative predictive power of Di, and the sigmoid parametes ai and bi can be set to selectively suppress contributions from components that are far away.
Here we assume that i absorbs a normalization constant, so that the c?s sum to 1.
In this approach, there are three parameters per distance metric to learn: i, ai, and bi.
In general, these parameters are also specific to the particular model being adapted, ie the LM or the TM.
To optimize these parameters, we fixed global loglinear weights at values obtained with Och?s algorithm using representative adapted models based on a single distance metric in (3), then used the Downhill Simplex algorithm (Press et al., 2002) to maximize BLEU score on the development corpus.
For tractability, we followed standard practice with this technique and considered only monotonic alignments when decoding (Zens and Ney, 2004).
The two approaches just described avoid conditioning c explicitly on c.
This is necessary for dynamic adaptation, since any genre preferences learned from the development corpus cannot be expected to generalize.
However, it is not necessary for cross-domain adaptation, where the genre of the development corpus is assumed to represent the test domain.
Therefore, we also experimented with using Downhill Simplex optimization to directly learn the set of linear weights c that yield maximum BLEU score on the development corpus.
A final variant on setting linear mixture weights is a hybrid between cross-domain and dynamic adaptation.
In this approach, both the global loglinear weights and, if they are being used, the mixture parameters i,ai,bi are set to characterize the test domain as in cross-domain adaptation.
When translating, however, distances to the current source text are used in (3) or (4) instead of distances to the indomain development corpus.
This obviously limits the metrics used to ones that depend only on source text.
4 Experiments
All experiments were run on the NIST MT evaluation 2006 Chinese data set.
Table 1 summarizes the corpora used.
The training corpus was divided into seven components according to genre; in all cases these were identical to LDC corpora, with the exception of the Newswire component, which was amalgamated from several smaller corpora.
The target 131 genre for cross-domain adaptation was newswire, for which high-quality training material is available.
The cross-domain development set NIST04nw is the newswire subset of the NIST 2004 evaluation set, and the dynamic adaptation development set NIST04-mix is a balanced mixed-genre subset of NIST 2004.
The NIST 2005 evaluation set was used for testing cross-domain adaptation, and the NIST 2006 evaluation set (both the ?GALE??and ?NIST?? parts) was used to test dynamic adaptation.
Because different development corpora are used for cross-domain and dynamic adaptation, we trained one static baseline model for each of these adaptation settings, on the corresponding development set.
All results given in this section are BLEU scores.
role corpus genres sent train FBIS04 nw 182k HK Hans proceedings 1,375k HK Laws legal 475k HK News press release 740k Newswire nw 26k Sinorama news mag 366k UN proceedings 4,979k dev NIST04-nw nw 901 NIST04-mix nw, sp, ed 889 test NIST05 nw 1,082 NIST06-GALE nw, ng, bn, bc 2,276 NIST06-NIST nw, ng, bn 1,664 Table 1: Corpora.
In the genres column: nw = newswire, sp = speeches, ed = editorial, ng = newsgroup, bn = broadcast news, and bc = broadcast conversation.
4.1 Linear
versus Loglinear Combination Table 2 shows a comparison between linear and loglinear mixing frameworks, with uniform weights used in the linear mixture.
Both types of mixture model are better than the baseline, but the linear mixture is slightly better than the loglinear mixture.
This is quite surprising, because these results are on the development set: the loglinear model tunes its component weights on this set, whereas the linear model only adjusts global LM and TM weights.
We speculated that this may have been due to non-smooth component models, and tried various smoothing schemes, including Kneser-Ney phrase table smoothing similar to that described in (Foster et al., 2006), and binary features to indicate phrasepair presence within different components.
None helped, however, and we conclude that the problem is most likely that Och?s algorithm is unable to find a good maximimum in this setting.
Due to this result, all experiments we describe below involve linear mixtures only.
combination adapted model LM TM LM+TM baseline 30.2 30.2 30.2 loglinear mixture 30.9 31.2 31.4 uniform linear mixture 31.2 31.1 31.8 Table 2: Linear versus loglinear combinations on NIST04-nw.
4.2 Distance
Metrics for Weighting Table 3 compares the performance of all distance metrics described in section 3.4 when used on their own as defined in (3).
The difference between them is fairly small, but appears to be consistent across LM and TM adaptation and (for the LM metrics) across source and target side matching.
In general, LM metrics seem to have a slight advantage over the vector space metrics, with EM being the best overall.
We focus on this metric for most of the experiments that follow.
metric source text target text LM TM LM TM tf/idf 31.3 31.3 31.1 31.1 LSA 31.5 31.6 perplexity 31.6 31.3 31.7 31.5 EM 31.7 31.6 32.1 31.3 Table 3: Distance metrics for linear combination on the NIST04-nw development set.
(Entries in the top right corner are missing due to lack of time.) Table 4 shows the performance of the parameterized weighting function described by (4), with source-side EM and LSA metrics as inputs.
This is compared to direct weight optimization, as both these techniques use Downhill Simplex for parameter tuning.
Unfortunately, neither is able to beat 132 the performance of the normalized source-side EM metric on its own (reproduced on the first line from table 3).
In additional tests we verified that this also holds for the test corpus.
We speculate that this disappointing result is due to compromises made in order to run Downhill Simplex efficiently, including holding global weights fixed, using only a single starting point, and running with monotone decoding.
weighting LM TM EM-src, direct 31.7 31.6 EM-src + LSA-src, parameterized 31.0 30.0 direct optimization 31.7 30.2 Table 4: Weighting techniques for linear combination on the NIST04-nw development set.
4.3 Cross-Domain versus Dynamic Adaptation Table 5 shows results for cross-domain adaptation, using the source-side EM metric for linear weighting.
Both LM and TM adaptation are effective, with test-set improvements of approximately 1 BLEU point over the baseline for LM adaptation and somewhat less for TM adaptation.
Performance also improves on the NIST06 out-of-domain test set (although this set includes a newswire portion as well).
However, combined LM and TM adaptation is not better than LM adaptation on its own, indicating that the individual adapted models may be capturing the same information.
model dev test nist04nist05 nist06nw nist baseline 30.2 30.3 26.5 EM-src LM 31.7 31.2 27.8 EM-src TM 31.6 30.9 27.3 EM-src LM+TM 32.5 31.2 27.7 Table 5: Cross-Domain adaptation results.
Table 6 contains results for dynamic adaptation, using the source-side EM metric for linear weighting.
In this setting, TM adaptation is much less effective, not significantly better than the baseline; performance of combined LM and TM adaptation is also lower.
However, LM adaptation improves over the baseline by up to a BLEU point.
The performance of cross domain adaptation (reproduced from table 5 on the second line) is slightly better for the in-domain test set (NIST05), but worse than dynamic adaptation on the two mixed-domain sets.
model dev test nist04nist05 nist06nist06mix nist gale baseline 31.9 30.4 27.6 12.9 cross LM n/a 31.2 27.8 12.5 LM 32.8 30.8 28.6 13.4 TM 32.4 30.7 27.6 12.8 LM+TM 33.4 30.8 28.5 13.0 Table 6: Dynamic adaptation results, using src-side EM distances.
model NIST05 baseline 30.3 cross EM-src LM 31.2 cross EM-src TM 30.9 hybrid EM-src LM 30.9 hybrid EM-src TM 30.7 Table 7: Hybrid adaptation results.
Table 7 shows results for the hybrid approach described at the end of section 3.5.2: global weights are learned on NIST04-nw, but linear weights are derived dynamically from the current test file.
Performance drops slightly compared to pure crossdomain adaptation, indicating that it may be important to have a good fit between global and mixture weights.
4.4 Source
Granularity The results of the final experiment, to determine the effects of source granularity on dynamic adaptation, are shown in table 8.
Source-side EM distances are applied to the whole test set, to genres within the set, and to each document individually.
Global weights were tuned specifically for each of these conditions.
There appears to be little difference among these approaches, although genre-based adaptation perhaps has a slight advantage.
133 granularity dev test nist04nist05 nist06nist06mix nist gale baseline 31.9 30.4 27.6 12.9 file 32.4 30.8 28.6 13.4 genre 32.5 31.1 28.9 13.2 document 32.9 30.9 28.6 13.4 Table 8: The effects of source granularity on dynamic adaptation.
5 Related
Work Mixture modeling is a standard technique in machine learning (Hastie et al., 2001).
It has been widely used to adapt language models for speech recognition and other applications, for instance using cross-domain topic mixtures, (Iyer and Ostendorf, 1999), dynamic topic mixtures (Kneser and Steinbiss, 1993), hierachical mixtures (Florian and Yarowsky, 1999), and cache mixtures (Kuhn and De Mori, 1990).
Most previous work on adaptive SMT focuses on the use of IR techniques to identify a relevant subset of the training corpus from which an adapted model can be learned.
Byrne et al (2003) use cosine distance from the current source document to find relevant parallel texts for training an adapted translation model, with background information for smoothing alignments.
Hildebrand et al (1995) describe a similar approach, but apply it at the sentence level, and use it for language model as well as translation model adaptation.
They rely on a perplexity heuristic to determine an optimal size for the relevant subset.
Zhao et al (2004) apply a slightly different sentence-level strategy to language model adaptation, first generating an nbest list with a baseline system, then finding similar sentences in a monolingual target-language corpus.
This approach has the advantage of not limiting LM adaptation to a parallel corpus, but the disadvantage of requiring two translation passes (one to generate the nbest lists, and another to translate with the adapted model).
Ueffing (2006) describes a self-training approach that also uses a two-pass algorithm.
A baseline system generates translations that, after confidence filtering, are used to construct a parallel corpus based on the test set.
Standard phrase-extraction techniques are then applied to extract an adapted phrase table from the system?s own output.
Finally, Zhang et al (2006) cluster the parallel training corpus using an algorithm that heuristically minimizes the average entropy of source-side and target-side language models over a fixed number of clusters.
Each source sentence is then decoded using the language model trained on the cluster that assigns highest likelihood to that sentence.
The work we present here is complementary to both the IR approaches and Ueffing?s method because it provides a way of exploiting a preestablished corpus division.
This has the potential to allow sentences having little surface similarity to the current source text to contribute statistics that may be relevant to its translation, for instance by raising the probability of rare but pertinent words.
Our work can also be seen as extending all previous approaches in that it assigns weights to components depending on their degree of relevance, rather than assuming a binary distinction between relevant and non-relevant components.
6 Conclusion
and Future Work We have investigated a number of approaches to mixture-based adaptation using genres for Chinese to English translation.
The most successful is to weight component models in proportion to maximum-likelihood (EM) weights for the current text given an ngram language model mixture trained on corpus components.
This resulted in gains of around one BLEU point.
A more sophisticated approach that attempts to transform and combine multiple distance metrics did not yield positive results, probably due to an unsucessful optmization procedure.
Other conclusions are: linear mixtures are more tractable than loglinear ones; LM-based metrics are better than VS-based ones; LM adaptation works well, and adding an adapted TM yields no improvement; cross-domain adaptation is optimal, but dynamic adaptation is a good fallback strategy; and source granularity at the genre level is better than the document or test-set level.
In future work, we plan to improve the optimization procedure for parameterized weight functions.
We will also look at bilingual metrics for cross134 domain adaptation, and investigate better combinations of cross-domain and dynamic adaptation.
References Peter F.
Brown, Stephen A.
Della Pietra, Vincent Della J.
Pietra, and Robert L.
Mercer. 1993.
The mathematics of Machine Translation: Parameter estimation.
Computational Linguistics, 19(2):263??12, June.
W. Byrne, S.
Khudanpur, W.
Kim, S.
Kumar, P.
Pecina, P.
Virga, P.
Xu, and D.
Yarowsky. 2003.
The JHU 2003 Chinese-English Machine Translation System.
In MT Summit IX, New Orleans, September.
S. Deerwester, S.
T. Dumais, G.
W. Furnas, T.
K. Landauer, and R.
Harshman. 1990.
Indexing by latent semantic analysis.
JASIS, 41(6):391??07.
Radu Florian and David Yarowsky.
1999. Dynamic non-local language modeling via hierarchical topic-based adaptation.
In ACL 1999, pages 167??74, College Park, Maryland, June.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine translation.
In EMNLP 2006, Sydney, Australia.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
2001. The Elements of Statistical Learning.
Springer. Almut Silja Hildebrand, Matthias Eck, Stephan Vogel, and Alex Waibel.
1995. Adaptation of the translation model for statistical machine translation based on information retrieval.
In EAMT 1995, Budapest, May.
R. Iyer and M.
Ostendorf. 1999.
Modeling long distance dependence in language: Topic mixtures vs.
dynamic cache models.
In IEEE Trans on Speech and Language Processing, 1999.
Frederick Jelinek.
1997. Statistical Methods for Speech Recognition.
MIT Press.
Reinhard Kneser and Volker Steinbiss.
1993. On the dynamic adaptation of stochastic language models.
In ICASSP 1993, pages 586??89, Minneapolis, Minnesota.
IEEE. Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation.
In NAACL 2003, pages 127??33.
Roland Kuhn and Renato De Mori.
1990. A cache-based natural language model for speech recognition.
IEEE Trans on PAMI, 12(6):570??83, June.
Percy Liang, Alexandre Bouchard-C?ote, Dan Klein, and Ben Taskar.
2006. An end-to-end discriminative approach to machine translation.
In ACL 2006 Franz Josef Och.
2003. Minimum error rate training for statistical machine translation.
In ACL 2003, Sapporo, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001. BLEU: A method for automatic evaluation of Machine Translation.
Technical Report RC22176, IBM, September.
William H.
Press, Saul A.
Teukolsky, William T.
Vetterling, and Brian P.
Flannery. 2002.
Numerical Recipes in C++.
Cambridge University Press, Cambridge, UK.
Christoph Tillmann and Tong Zhang.
2006. A discriminative global training algorithm for statistical MT.
In ACL 2006.
Nicola Ueffing.
2006. Self-training for machine translation.
In NIPS 2006 Workshop on MLIA, Whistler, B.C., December.
Richard Zens and Hermann Ney.
2004. Improvements in phrase-based statistical machine translation.
In HLT/NAACL 2004, Boston, May.
R. Zhang, H.
Yamamoto, M.
Paul, H.
Okuma, K.
Yasuda, Y.
Lepage, E.
Denoual, D.
Mochihashi, A.
Finch, and E.
Sumita. 2006.
The NiCT-ATR statistical machine translation system for the IWSLT 2006 evaluation.
In IWSLT 2006.
Bing Zhao, Matthias Eck, and Stephan Vogel.
2004. Language model adaptation for statistical machine translation with structured query models.
In COLING 2004, Geneva, August .
Proceedings of the Second Workshop on Statistical Machine Translation, pages 136??58, Prague, June 2007.
c2007 Association for Computational Linguistics (Meta-) Evaluation of Machine Translation Chris Callison-Burch Johns Hopkins University ccb clsp jhu edu Cameron Fordyce CELCT fordyce celct it Philipp Koehn University of Edinburgh pkoehn inf ed ac uk Christof Monz Queen Mary, University of London christof dcs qmul ac uk Josh Schroeder University of Edinburgh j schroeder ed ac uk Abstract This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back.
We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process.
We measured timing and intraand inter-annotator agreement for three types of subjective evaluation.
We measured the correlationofautomaticevaluationmetricswith human judgments.
This meta-evaluation reveals surprising facts about the most commonly used methodologies.
1 Introduction
This paper presents the results for the shared translation task of the 2007 ACL Workshop on Statistical Machine Translation.
The goals of this paper are twofold: First, we evaluate the shared task entries in order to determine which systems produce translations with the highest quality.
Second, we analyze the evaluation measures themselves in order to try to determine ?best practices??when evaluating machine translation research.
Previous ACL Workshops on Machine Translation were more limited in scope (Koehn and Monz, 2005; Koehn and Monz, 2006).
The 2005 workshop evaluated translation quality only in terms of Bleu score.
The 2006 workshop additionally included a limited manual evaluation in the style of NIST machine translation evaluation workshop.
Here we apply eleven different automatic evaluation metrics, and conduct three different types of manual evaluation.
Beyond examining the quality of translations produced by various systems, we were interested in examining the following questions about evaluation methodologies: How consistent are people when they judge translation quality?
To what extent do they agree with other annotators?
Can we improve human evaluation?
Which automatic evaluation metrics correlate most strongly with human judgments of translation quality?
This paper is organized as follows: ??Section 2 gives an overview of the shared task.
It describes the training and test data, reviews the baseline system, and lists the groups that participated in the task.
??Section 3 describes the manual evaluation.
We performed three types of evaluation: scoring with five point scales, relative ranking of translations of sentences, and ranking of translations of phrases.
??Section 4 lists the eleven different automatic evaluation metrics which were also used to score the shared task submissions.
??Section5presentstheresultsofthesharedtask, giving scores for each of the systems in each of the different conditions.
??Section 6 provides an evaluation of the different types of evaluation, giving intraand 136 inter-annotator agreement figures for the manual evaluation, and correlation numbers for the automatic metrics.
2 Shared
task overview This year?s shared task changed in some aspects from last year?s: ??We gave preference to the manual evaluation of system output in the ranking of systems.
Manual evaluation was done by the volunteers from participating groups and others.
Additionally, there were three modalities of manual evaluation.
??Automatic metrics were also used to rank the systems.
In total eleven metrics were applied, and their correlation with the manual scores was measured.
??As in 2006, translation was from English, and into English.
English was again paired with German, French, and Spanish.
We additionally included Czech (which was fitting given the location of the WS).
Similar to the IWSLT International Workshop on Spoken Language Translation (Eck and Hori, 2005; Paul, 2006), and the NIST Machine Translation Evaluation Workshop (Lee, 2006) we provide the shared task participants with a common set of training and test data for all language pairs.
The major part of data comes from current and upcoming full releases of the Europarl data set (Koehn, 2005).
2.1 Description
of the Data The data used in this year?s shared task was similar tothedatausedinlastyear?ssharedtask.
Thisyear?s data included training and development sets for the NewsCommentarydata, whichwasthesurpriseoutof-domain test set last year.
The majority of the training data for the Spanish, French, and German tasks was drawn from a new version of the Europarl multilingual corpus.
Additional training data was taken from the News Commentary corpus.
Czech language resources were drawn from the News Commentary data.
Additional resources for Czech came from the CzEng Parallel Corpus (Bojar and ?Zabokrtsky, 2006).
Overall, there are over 30 million words of training data per language from the Europarl corpus and 1 million words from the News Commentary corpus.
Figure 1 provides some statistics about the corpora used this year.
2.2 Baseline
system To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.
To summarize, we provided: ??sentence-aligned training corpora ??development and dev-test sets ??language models trained for each language ??an open source decoder for phrase-based SMT called Moses (Koehn et al., 2006), which replaces the Pharaoh decoder (Koehn, 2004) ??a training script to build models for Moses Theperformanceofthisbaselinesystemissimilar to the best submissions in last year?s shared task.
2.3 Test
Data The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.
Participants were also provided with three sets of parallel text to be used for system development and tuning.
In addition to the Europarl test set, we also collected editorials from the Project Syndicate website1, which are published in all the five languages of the shared task.
We aligned the texts at a sentence level across all five languages, resulting in 2,007 sentences per language.
For statistics on this test set, refer to Figure 1.
The News Commentary test set differs from the Europarl data in various ways.
The text type are editorials instead of speech transcripts.
The domain is general politics, economics and science.
However, it is also mostly political content (even if not focused ontheinternalworkingsoftheEuropeanUnion)and opinion.
2.4 Participants
We received submissions from 15 groups from 14 institutions, as listed in Table 1.
This is a slight 1http://www.project-syndicate.com/ 137 Europarl Training corpus Spanish?English French?English German?English Sentences 1,259,914 1,288,901 1,264,825 Foreign words 33,159,337 33,176,243 29,582,157 English words 31,813,692 32,615,285 31,929,435 Distinct foreign words 345,944 344,287 510,544 Distinct English words 266,976 268,718 250,295 News Commentary Training corpus Spanish?English French?English German?English Czech?English Sentences 51,613 43,194 59,975 57797 Foreign words 1,263,067 1,028,672 1,297,673 1,083,122 English words 1,076,273 906,593 1,238,274 1,188,006 Distinct foreign words 84,303 68,214 115,589 142,146 Distinct English words 70,755 63,568 76,419 74,042 Language model data English Spanish French German Sentence 1,407,285 1,431,614 1,435,027 1,478,428 Words 34,539,822 36,426,542 35,595,199 32,356,475 Distinct words 280,546 385,796 361,205 558,377 Europarl test set English Spanish French German Sentences 2,000 Words 53,531 55,380 53,981 49,259 Distinct words 8,558 10,451 10,186 11,106 News Commentary test set English Spanish French German Czech Sentences 2,007 Words 43,767 50,771 49,820 45,075 39,002 Distinct words 10,002 10,948 11,244 12,322 15,245 Figure 1: Properties of the training and test sets used in the shared task.
The training data is drawn from the Europarl corpus and from the Project Syndicate, a web site which collects political commentary in multiple languages.
138 ID Participant cmu-uka Carnegie Mellon University, USA (Paulik et al., 2007) cmu-syntax Carnegie Mellon University, USA (Zollmann et al., 2007) cu Charles University, Czech Republic (Bojar, 2007) limsi LIMSI-CNRS, France (Schwenk, 2007) liu University of Linkoping, Sweden(Holmqvist et al., 2007) nrc National Research Council, Canada (Ueffing et al., 2007) pct a commercial MT provider from the Czech Republic saar Saarland University & DFKI, Germany (Chen et al., 2007) systran SYSTRAN, France & U.
Edinburgh, UK (Dugast et al., 2007) systran-nrc National Research Council, Canada (Simard et al., 2007) ucb University of California at Berkeley, USA (Nakov and Hearst, 2007) uedin University of Edinburgh, UK (Koehn and Schroeder, 2007) umd University of Maryland, USA (Dyer, 2007) upc University of Catalonia, Spain (Costa-Juss`a and Fonollosa, 2007) upv University of Valencia, Spain (Civera and Juan, 2007) Table 1: Participants in the shared task.
Not all groups participated in all translation directions.
increase over last year?s shared task where submissions were received from 14 groups from 11 institutions.
Of the 11 groups that participated in last year?s shared task, 6 groups returned this year.
This year, most of these groups follow a phrasebased statistical approach to machine translation.
However, several groups submitted results from systems that followed a hybrid approach.
While building a machine translation system is a serious undertaking we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.
The creation of parallel corpora suchastheEuroparl,theCzEng,andtheNewsCommentarycorporashouldhelpinthisdirectionbyproviding freely available language resources for buildingsystems.
Thecreationofanopensourcebaseline system should also go a long way towards achieving this goal.
For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.
3 Human
evaluation Weevaluatedthesharedtasksubmissionsusingboth manual evaluation and automatic metrics.
While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are an imperfect substitute for human assessment of translation quality.
Manual evaluation is time consuming and expensive to perform, so comprehensive comparisons of multiple systems are rare.
For our manual evaluation we distributed the workload across a number of people, including participants in the shared task, interested volunteers, and a small number of paid annotators.
More than 100 people participated in the manual evaluation, with 75 of those people putting in at least an hour?s worth of effort.
A total of 330 hours of labor was invested, nearly doubling last year?s all-volunteer effort which yielded 180 hours of effort.
Beyond simply ranking the shared task submissions, we had a number of scientific goals for the manual evaluation.
Firstly, we wanted to collect data which could be used to assess how well automatic metrics correlate with human judgments.
Secondly, we wanted to examine different types of manual evaluation and assess which was the best.
A number of criteria could be adopted for choosing among different types of manual evaluation: the ease with which people are able to perform the task, their agreement with other annotators, their reliability when asked to repeat judgments, or the number of judgments which can be collected in a fixed time period.
There are a range of possibilities for how human 139 evaluation of machine translation can be done.
For instance, it can be evaluated with reading comprehension tests (Jones et al., 2005), or by assigning subjective scores to the translations of individual sentences (LDC, 2005).
We examined three differentwaysofmanuallyevaluatingmachinetranslation quality: ??Assigning scores based on five point adequacy and fluency scales ??Ranking translated sentences relative to each other ??Ranking the translations of syntactic constituents drawn from the source sentence 3.1 Fluency and adequacy The most widely used methodology when manually evaluatingMTistoassignvaluesfromtwofivepoint scales representing fluency and adequacy.
These scales were developed for the annual NIST Machine Translation Evaluation Workshop by the Linguistics Data Consortium (LDC, 2005).
The five point scale for adequacy indicates how much of the meaning expressed in the reference translation is also expressed in a hypothesis translation: 5 = All 4 = Most 3 = Much 2 = Little 1 = None The second five point scale indicates how fluent the translation is.
When translating into English the values correspond to: 5 = Flawless English 4 = Good English 3 = Non-native English 2 = Disfluent English 1 = Incomprehensible Separate scales for fluency and adequacy were developed under the assumption that a translation might be disfluent but contain all the information from the source.
However, in principle it seems that people have a hard time separating these two aspects of translation.
The high correlation between people?s fluency and adequacy scores (given in Tables 17 and 18) indicate that the distinction might be false.
? people 's Iraq to services basic other and, care health, food provide cannot it if occupation its sustain US the Can ?knnenanbietenDienstleistungengrundlegendeandereundGesundheitsfrsorge,NahrungnichtV olk irakischendemsiewenn,USAdieKnnen aufrechterhaltenBesetzung ihre Reference translation NP NP NP VP NP VP S S CNP NP Constituents selected for evaluation Target phrases highlighted via word alignments Parsed source sentence Figure 2: In constituent-based evaluation, the source sentence was parsed, and automatically aligned with the reference translation and systems??translations Another problem with the scores is that there are no clear guidelines on how to assign values to translations.
No instructions are given to evaluators in terms of how to quantify meaning, or how many grammatical errors (or what sort) separates the different levels of fluency.
Because of this many judges either develop their own rules of thumb, or use the scales as relative rather than absolute.
These are borne out in our analysis of inter-annotator agreement in Section 6.
3.2 Ranking
translations of sentences Because fluency and adequacy were seemingly difficult things for judges to agree on, and because many peoplefromlastyear?sworkshopseemedtobeusing them as a way of ranking translations, we decided to try a separate evaluation where people were simply 140 asked to rank translations.
The instructions for this task were: Rank each whole sentence translation from Best to Worst relative to the other choices (ties are allowed).
These instructions were just as minimal as for fluency and adequacy, but the task was considerably simplified.
Rather than having to assign each translation a value along an arbitrary scale, people simply had to compare different translations of a single sentence and rank them.
3.3 Ranking
translations of syntactic constituents In addition to having judges rank the translations of whole sentences, we also conducted a pilot study of a new type of evaluation methodology, which we call constituent-based evaluation.
In our constituent-based evaluation we parsed the source language sentence, selected constituents from the tree, and had people judge the translations of those syntactic phrases.
In order to draw judges??attention to these regions, we highlighted the selected source phrasesandthecorrespondingphrasesinthetranslations.
The corresponding phrases in the translations were located via automatic word alignments.
Figure 2 illustrates the constituent based evaluation when applied to a German source sentence.
The German source sentence is parsed, and various phrases are selected for evaluation.
Word alignments are created between the source sentence and the reference translation (shown), and the source sentence and each of the system translations (not shown).
We parsed the test sentences for each of the languages aside from Czech.
We used Cowan and Collins (2005)?s parser for Spanish, Arun and Keller (2005)?s for French, Dubey (2005)?s for German, and Bikel (2002)?s for English.
The word alignments were created with Giza++ (Och and Ney, 2003) applied to a parallel corpus containing 200,000 sentence pairs of the training data, plus sets of 4,007 sentence pairs created by pairing the test sentences with the reference translations, and the test sentences paired with each of the system translations.
The phrases in the translations were located using techniques from phrase-based statistical machine translation which extract phrase pairsfromwordalignments(Koehnetal., 2003; Och and Ney, 2004).
Because the word-alignments were created automatically, and because the phrase extraction is heuristic, the phrases that were selected may not exactly correspond to the translations of the selected source phrase.
We noted this in the instructions to judges: Rank each constituent translation from Best to Worst relative to the other choices (ties are allowed).
Grade only the highlighted part of each translation.
Please note that segments are selected automatically, and they should be taken as an approximate guide.
They might includeextrawordsthatarenotintheactual alignment, or miss words on either end.
The criteria that we used to select which constituents were to be evaluated were: ??The constituent could not be the whole source sentence ??The constituent had to be longer three words, and be no longer than 15 words ??The constituent had to have a corresponding phrase with a consistent word alignment in each of the translations The final criterion helped reduce the number of alignment errors.
3.4 Collecting
judgments We collected judgments using a web-based tool.
Shared task participants were each asked to judge 200 sets of sentences.
The sets consisted of 5 system outputs, as shown in Figure 3.
The judges were presented with batches of each type of evaluation.
We presented them with five screens of adequacy/fluency scores, five screens of sentence rankings, and ten screens of constituent rankings.
The order of the types of evaluation were randomized.
In order to measure intra-annotator agreement 10% of the items were repeated and evaluated twice by each judge.
In order to measure inter-annotator agreement 40% of the items were randomly drawn from a common pool that was shared across all 141 http://www.statmt.org/wmt07/shared-task/judge/do_task.php WMT07 Manual Evaluation Rank Segments You have judged 25 sentences for WMT07 German-English News Corpus, 190 sentences total taking 64.9 seconds per sentence.
Source: Knnen die USA ihre Besetzung aufrechterhalten, wenn sie dem irakischen Volk nicht Nahrung, Gesundheitsfrsorge und andere grundlegende Dienstleistungen anbieten knnen?
Reference: Can the US sustain its occupation if it cannot provide food, health care, and other basic services to Iraq's people?
Translation Rank The United States can maintain its employment when it the Iraqi people not food, health care and other basic services on offer?.
1 Worst
2 3 4 5 Best The US can maintain its occupation, if they cannot offer the Iraqi people food, health care and other basic services? 1 Worst 2 3 4 5 Best Can the US their occupation sustained if it to the Iraqi people not food, health care and other basic services can offer? 1 Worst 2 3 4 5 Best Can the United States maintain their occupation, if the Iraqi people do not food, health care and other basic services can offer? 1 Worst 2 3 4 5 Best The United States is maintained, if the Iraqi people, not food, health care and other basic services can offer? 1 Worst 2 3 4 5 Best Annotator: ccb Task: WMT07 German-English News Corpus Instructions: Rank each constituent translation from Best to Worst relative to the other choices (ties are allowed).
Grade only the highlighted part of each translation.
Please note that segments are selected automatically, and they should be taken as an approximate guide.
They might include extra words on either end that are not in the actual alignment, or miss words.
Figure 3: For each of the types of evaluation, judges were shown screens containing up to five different system translations, along with the source sentence and reference translation.
annotators so that we would have items that were judged by multiple annotators.
Judges were allowed to select whichever data set they wanted, and to evaluate translations into whatever languages they were proficient in.
Shared task participants were excluded from judging their own systems.
Table 2 gives a summary of the number of judgments that we collected for translations of individual sentences.
Since we had 14 translation tasks and four different types of scores, there were 55 different conditions.2 In total we collected over 81,000 judgments.
Despite the large number of conditions we managed to collect more than 1,000 judgments for most of them.
This provides a rich source of data for analyzing the quality of translations produced by different systems, the different types of human evaluation, and the correlation of automatic metrics with human judgments.3 2We did not perform a constituent-based evaluation for Czech to English because we did not have a syntactic parser for Czech.
We considered adapting our method to use Bojar (2004)?sdependencyparserforCzech,butdidnothavethetime.
3The judgment data along with all system translations are available at http://www.statmt.org/wmt07/ 4 Automatic evaluation The past two ACL workshops on machine translation used Bleu as the sole automatic measure of translation quality.
Bleu was used exclusively since it is the most widely used metric in the field and has been shown to correlate with human judgments of translation quality in many instances (Doddington, 2002; Coughlin, 2003; Przybocki, 2004).
However, recent work suggests that Bleu?s correlation with human judgments may not be as strong as previously thought (Callison-Burch et al., 2006).
The results of last year?s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006).
We used the manual evaluation data as a means of testing the correlation of a range of automatic metrics in addition to Bleu.
In total we used eleven different automatic evaluation measures to rank the shared task submissions.
They are: ??Meteor (Banerjee and Lavie, 2005)?Meteor measures precision and recall of unigrams when comparing a hypothesis translation 142 Language Pair Test Set Adequacy Fluency Rank Constituent English-German Europarl 1,416 1,418 1,419 2,626 News Commentary 1,412 1,413 1,412 2,755 German-English Europarl 1,525 1,521 1,514 2,999 News Commentary 1,626 1,620 1,601 3,084 English-Spanish Europarl 1,000 1,003 1,064 1,001 News Commentary 1,272 1,272 1,238 1,595 Spanish-English Europarl 1,174 1,175 1,224 1,898 News Commentary 947 949 922 1,339 English-French Europarl 773 772 769 1,456 News Commentary 729 735 728 1,313 French-English Europarl 834 833 830 1,641 News Commentary 1,041 1,045 1,035 2,036 English-Czech News Commentary 2,303 2,304 2,331 3,968 Czech-English News Commentary 1,711 1,711 1,733 0 Totals 17,763 17,771 17,820 27,711 Table 2: The number of items that were judged for each task during the manual evaluation against a reference.
It flexibly matches words using stemming and WordNet synonyms.
Its flexible matching was extended to French, Spanish, German and Czech for this workshop (Lavie and Agarwal, 2007).
??Bleu (Papineni et al., 2002)?Bleu is currently the de facto standard in machine translation evaluation.
It calculates n-gram precision and a brevity penalty, and can make use of multiple reference translations as a way of capturing some of the allowable variation in translation.
We use a single reference translation in our experiments.
??GTM (Melamed et al., 2003)?GTM generalizes precision, recall, and F-measure to measure overlap between strings, rather than overlap between bags of items.
An ?exponent??parameter which controls the relative importance of word order.
A value of 1.0 reduces GTM to ordinary unigram overlap, with higher values emphasizing order.4 ??Translation Error Rate (Snover et al., 2006)?? 4The GTM scores presented here are an F-measure with a weight of 0.1, which counts recall at 10x the level of precision.
The exponent is set at 1.2, which puts a mild preference towards items with words in the correct order.
These parameters could be optimized empirically for better results.
TER calculates the number of edits required to change a hypothesis translation into a reference translation.
The possible edits in TER include insertion, deletion, and substitution of single words, and an edit which moves sequences of contiguous words.
??ParaEval precision and ParaEval recall (Zhou et al., 2006)?ParaEval matches hypothesis and reference translations using paraphrases that are extracted from parallel corpora in an unsupervised fashion (Bannard and Callison-Burch, 2005).
It calculates precision and recall using a unigram counting strategy.
??Dependency overlap (Amigo et al., 2006)??
This metric uses dependency trees for the hypothesis and reference translations, by computing the average overlap between words in the two trees which are dominated by grammatical relationships of the same type.
??Semantic role overlap (Gimenez and M`arquez, 2007)?This metric calculates the lexical overlap between semantic roles (i.e., semantic arguments or adjuncts) of the same type in the the hypothesis and reference translations.
It uniformly averages lexical overlap over all semantic role types.
143 ??Word Error Rate over verbs (Popovic and Ney, 2007)?WER??creates a new reference and a new hypothesis for each POS class by extracting all words belonging to this class, and then tocalculatethestandardWER.Weshowresults for this metric over verbs.
??Maximumcorrelationtrainingonadequacyand on fluency (Liu and Gildea, 2007)?a linear combination of different evaluation metrics (Bleu, Meteor, Rouge, WER, and stochastic iterative alignment) with weights set to maximize Pearson?s correlation with adequacy and fluency judgments.
Weights were trained on WMT-06 data.
The scores produced by these are given in the tables at the end of the paper, and described in Section 5.
We measured the correlation of the automatic evaluation metrics with the different types of human judgments on 12 data conditions, and report these in Section 6.
5 Shared
task results The results of the human evaluation are given in Tables 9, 10, 11 and 12.
Each of those tables present four scores: ??FLUENCY and ADEQUACY are normalized versions of the five point scores described in Section 3.1.
The tables report an average of the normalized scores.5 ??RANK is the average number of times that a system was judged to be better than any other system in the sentence ranking evaluation described in Section 3.2.
??CONSTITUENT is the average number of times that a system was judged to be better than any other system in the constituent-based evaluation described in Section 3.3.
There was reasonably strong agreement between these four measures at which of the entries was the best in each data condition.
There was complete 5Since different annotators can vary widely in how they assign fluency and adequacy scores, we normalized these scores on a per-judge basis using the method suggested by Blatz et al.(2003) in Chapter 5, page 97.
SYSTRAN (systran) 32% University of Edinburgh (uedin) 20% University of Catalonia (upc) 15% LIMSI-CNRS (limsi) 13% University of Maryland (umd) 5% National Research Council of Canada?s joint entry with SYSTRAN (systran-nrc) 5% Commercial Czech-English system (pct) 5% University of Valencia (upv) 2% Charles University (cu) 2% Table 3: The proportion of time that participants?? entries were top-ranked in the human evaluation University of Edinburgh (uedin) 41% University of Catalonia (upc) 12% LIMSI-CNRS (limsi) 12% University of Maryland (umd) 9% Charles University (cu) 4% Carnegie Mellon University (cmu-syntax) 4% Carnegie Mellon University (cmu-uka) 4% University of California at Berkeley (ucb) 3% National Research Council?s joint entry with SYSTRAN (systran-nrc) 2% SYSTRAN (systran) 2% Saarland University (saar) 0.8% Table 4: The proportion of time that participants?? entries were top-ranked by the automatic evaluation metrics agreement between them in 5 of the 14 conditions, and agreement between at least three of them in 10 of the 14 cases.
Table 3 gives a summary of how often different participants??entries were ranked #1 by any of the four human evaluation measures.
SYSTRAN?s entries were ranked the best most often, followed by University of Edinburgh, University of Catalonia and LIMSI-CNRS.
The following systems were the best performing for the different language pairs: SYSTRAN was ranked the highest in German-English, University of Catalonia was ranked the highest in Spanish-English, LIMSI-CNRS was ranked highest in French-English, and the University of Maryland and a commercial system were the highest for 144 Evaluation type P(A) P(E) K Fluency (absolute).400 .2 .250 Adequacy (absolute) .380 .2 .226 Fluency (relative) .520 .333 .281 Adequacy (relative) .538 .333 .307 Sentence ranking .582 .333 .373 Constituent ranking .693 .333 .540 Constituent ranking .712 .333 .566 (w/identical constituents) Table 5: Kappa coefficient values representing the inter-annotator agreement for the different types of manual evaluation Czech-English.
While we consider the human evaluation to be primary, it is also interesting to see how the entries were ranked by the various automatic evaluationmetrics.
Thecompletesetofresultsfortheautomatic evaluation are presented in Tables 13, 14, 15, and 16.
An aggregate summary is provided in Table 4.
The automatic evaluation metrics strongly favor the University of Edinburgh, which garners 41% of the top-ranked entries (which is partially due to the fact it was entered in every language pair).
Significantly, the automatic metrics disprefer SYSTRAN, whichwasstronglyfavoredinthehumanevaluation.
6 Meta-evaluation In addition to evaluating the translation quality of the shared task entries, we also performed a ?metaevaluation??of our evaluation methodologies.
6.1 Interand
Intra-annotator agreement We measured pairwise agreement among annotators usingthekappacoefficient(K)whichiswidelyused in computational linguistics for measuring agreement in category judgments (Carletta, 1996).
It is defined as K = P(A)?P(E)1?P(E) where P(A) is the proportion of times that the annotators agree, and P(E) is the proportion of time that they would agree by chance.
We define chance agreement for fluency and adequacy as 15, since they are based on five point scales, and for ranking as 13 Evaluation type P(A) P(E) K Fluency (absolute) .630 .2 .537 Adequacy (absolute) .574 .2 .468 Fluency (relative) .690 .333 .535 Adequacy (relative) .696 .333 .544 Sentence ranking .749 .333 .623 Constituent ranking .825 .333 .738 Constituent ranking .842 .333 .762 (w/identical constituents) Table 6: Kappa coefficient values for intra-annotator agreement for the different types of manual evaluation since there are three possible out comes when ranking the output of a pair of systems: A > B, A = B, A < B.
For inter-annotator agreement we calculated P(A) for fluency and adequacy by examining all items that were annotated by two or more annotators, and calculating the proportion of time they assigned identical scores to the same items.
For the ranking tasks we calculated P(A) by examining all pairs of systems which had been judged by two or more judges, and calculated the proportion of time that they agreed that A > B, A = B, or A < B.
For intra-annotator agreement we did similarly, but gathered items that were annotated on multiple occasions by a single annotator.
Table 5 gives K values for inter-annotator agreement, and Table 6 gives K values for intra-annoator agreement.
These give an indication of how often different judges agree, and how often single judges are consistent for repeated judgments, respectively.
The interpretation of Kappa varies, but according to Landis and Koch (1977) 0?.2 is slight, .21?.4 is fair,.41?.6 is moderate,.61?.8 is substantial and the rest almost perfect.
The K values for fluency and adequacy should give us pause about using these metrics in the future.
Whenweanalyzedthemastheyareintendedto be?scores classifying the translations of sentences into different types?the inter-annotator agreement was barely considered fair, and the intra-annotator agreement was only moderate.
Even when we reassessed fluency and adequacy as relative ranks the agreements increased only minimally.
145 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0 10 20 30 40 50 60 num sentences taking this long (%) time to judge one sentence (seconds) constituent rank sentence rank fluency+adequacy scoring Figure 4: Distributions of the amount of time it took to judge single sentences for the three types of manual evaluation The agreement on the other two types of manual evaluation that we introduced were considerably better.
Theboththesentenceandconstituentranking had moderate inter-annotator agreement and substantialintra-annotatoragreement.
Becausetheconstituent ranking examined the translations of short phrases, often times all systems produced the same translations.
Since these trivially increased agreement (since they would always be equally ranked) we also evaluated the interand intra-annotator agreement when those items were excluded.
The agreement remained very high for constituent-based evaluation.
6.2 Timing
We used the web interface to collect timing information.
The server recorded the time when a set of sentences was given to a judge and the time when the judge returned the sentences.
We divided the time that it took to do a set by the number of sentences in the set.
The average amount of time that it took to assign fluency and adequacy to a single sentence was 26 seconds.6 The average amount of time it took to rank a sentence in a set was 20 seconds.
The average amount of time it took to rank a highlighted constituent was 11 seconds.
Figure 4 shows the distribution of times for these tasks.
6Sets which took longer than 5 minutes were excluded from these calculations, because there was a strong chance that annotators were interrupted while completing the task.
These timing figures are promising because they indicate that the tasks which the annotators were the most reliable on (constituent ranking and sentence ranking) were also much quicker to complete than the ones that they were unreliable on (assigning fluency and adequacy scores).
This suggests that fluency and adequacy should be replaced with ranking tasks in future evaluation exercises.
6.3 Correlation
between automatic metrics and human judgments To measure the correlation of the automatic metrics with the human judgments of translation quality we used Spearman?s rank correlation coefficient ?.
We opted for Spearman rather than Pearson because it makes fewer assumptions about the data.
Importantly, it can be applied to ordinal data (such as the fluency and adequacy scales).
Spearman?s rank correlation coefficient is equivalent to Pearson correlation on ranks.
Aftertherawscoresthatwereassignedtosystems by an automatic metric and by one of our manual evaluation techniques have been converted to ranks, we can calculate ? using the simplified equation: ? = 1??6 summationtextd2 i n(n2 ??) where di is the difference between the rank for systemi and n is the number of systems.
The possible values of?range between 1 (where all systems arerankedinthesameorder)and??(wherethesystems are ranked in the reverse order).
Thus an automatic evaluation metric with a higher value for ? is making predictions that are more similar to the human judgments than an automatic evaluation metric with a lower ?.
Table 17 reports ? for the metrics which were used to evaluate translations into English.7.
Table 7 summarizes the results by averaging the correlation numbers by equally weighting each of the data conditions.
The table ranks the automatic evaluation metrics based on how well they correlated with human judgments.
While these are based on a relatively few number of items, and while we have not performed any tests to determine whether the differences in ? are statistically significant, the results 7The Czech-English conditions were excluded since there were so few systems 146 are nevertheless interesting, since three metrics have higher correlation than Bleu: ??Semantic role overlap (Gimenez and M`arquez, 2007), which makes its debut in the proceedings of this workshop ??ParaEval measuring recall (Zhou et al., 2006), which has a model of allowable variation in translation that uses automatically generated paraphrases (Callison-Burch, 2007) ??Meteor (Banerjee and Lavie, 2005) which also allows variation by introducing synonyms and by flexibly matches words using stemming.
Tables 18 and 8 report ? for the six metrics which were used to evaluate translations into the other languages.
Here we find that Bleu and TER are the closest to human judgments, but that overall the correlations are much lower than for translations into English.
7 Conclusions
Similar to last year?s workshop we carried out an extensive manual and automatic evaluation of machine translation performance for translating from four European languages into English, and vice versa.
This year we substantially increased the number of automatic evaluation metrics and were also able to nearly double the efforts of producing the human judgments.
There were substantial differences in the results results of the human and automatic evaluations.
We take the human judgments to be authoritative, and used them to evaluate the automatic metrics.
We measured correlation using Spearman?s coefficient and found that three less frequently used metrics were stronger predictors of human judgments than Bleu.
They were: semantic role overlap (newly introduced in this workshop) ParaEval-recall and Meteor.
Although we do not claim that our observations are indisputably conclusive, they again indicate that the choice of automatic metric can have a significant impact on comparing systems.
Understanding the exact causes of those differences still remains an important issue for future research.
metric AD EQ UA CY FL UE NC Y RA NK CO NS TI TU EN TO VE RA LL Semantic role overlap .774 .839 .803 .741 .789 ParaEvalRecall .712 .742 .768 .798 .755 Meteor .701 .719 .745 .669 .709 Bleu .690 .722 .672 .602 .671 1-TER .607 .538 .520 .514 .644 Max adequcorrelation .651 .657 .659 .534 .626 Max fluency correlation .644 .653 .656 .512 .616 GTM .655 .674 .616 .495 .610 Dependency overlap .639 .644 .601 .512 .599 ParaEvalPrecision .639 .654 .610 .491 .598 1-WER of verbs .378 .422 .431 .297 .382 Table 7: Average corrections for the different automatic metrics when they are used to evaluate translations into English metric AD EQ UA CY FL UE NC Y RA NK CO NS TI TU EN TO VE RA LL Bleu .657 .445 .352 .409 .466 1-TER .589 .419 .361 .380 .437 Max fluency correlation .534 .419 .368 .400 .430 Max adequcorrelation .498 .414 .385 .409 .426 Meteor .490 .356 .279 .304 .357 1-WER of verbs .371 .304 .359 .359 .348 Table 8: Average corrections for the different automatic metrics when they are used to evaluate translations into the other languages 147 This year?s evaluation also measured the agreement between human assessors by computing the Kappa coefficient.
One striking observation is that inter-annotator agreement for fluency and adequacy can be called ?fair??at best.
On the other hand, comparing systems by ranking them manually (constituents or entire sentences), resulted in much higher inter-annotator agreement.
Acknowledgments This work was supported in part by the EuroMatrix project funded by the European Commission (6th Framework Programme), and in part by the GALE program of the US Defense Advanced Research Projects Agency, Contract No.
HR0011-06C-0022.
We are grateful to Jesus Gimenez, Dan Melamed, Maja Popvic, Ding Liu, Liang Zhou, and Abhaya Agarwal for scoring the entries with their automatic evaluation metrics.
Thanks to Brooke Cowan for parsing the Spanish test sentences, to Josh Albrecht for his script for normalizing fluency and adequacy on a per judge basis, and to Dan Melamed, Rebecca Hwa, AlonLavie, ColinBannardandMirellaLapata for their advice about statistical tests.
References Enrique Amigo, Jesus Gimenez, Julio Gonzalo, and Llus M`arquez.
2006. MT Evaluation: Human-Like vs.
Human Acceptable.
In Proceedings of COLING-ACL06.
Abhishek Arun and Frank Keller.
2005. Lexicalization in crosslinguistic probabilistic parsing: The case of French.
In Proceedings of ACL.
Satanjeev Banerjee and Alon Lavie.
2005. Meteor: An automatic metric for MT evaluation with improved correlation with human judgments.
In Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, Ann Arbor, Michigan.
Colin Bannard and Chris Callison-Burch.
2005. Paraphrasing with bilingual parallel corpora.
InACL-2005. Dan Bikel.
2002. Design of a multi-lingual, parallel-processing statistical parsing engine.
In Proceedings of HLT.
John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing.
2003. Confidence estimation for machine translation.
CLSP Summer Workshop Final Report WS2003, Johns Hopkins University.
Ond?rej Bojar and Zden?ek ?Zabokrtsky.
2006. CzEng: Czech-English Parallel Corpus, Release version 0.5.
Prague Bulletin of Mathematical Linguistics, 86.
Ond?rej Bojar.
2004. Problems of inducing large coverage constraint-based dependency grammar for Czech.
In Constraint Solving and Language Processing, CSLP 2004, volume LNAI 3438.
Springer. Ond?rej Bojar.
2007. English-to-Czech factored machine translation.
In Proceedings of the ACL-2007 Workshop on Statistical Machine Translation (WMT-07), Prague.
Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
2006. Re-evaluating the role of Bleu in machine translation research.
In Proceedings of EACL.
Chris Callison-Burch.
2007. Paraphrasing and Translation.
Ph.D. thesis, University of Edinburgh, Scotland.
Jean Carletta.
1996. Assessing agreement on classification tasks: The kappa statistic.
Computational Linguistics, 22(2):249??54.
Yu Chen, Andreas Eisele, Christian Federmann, Eva Hasler, Michael Jellinghaus, and Silke Theison.
2007. Multi-engine machine translation with an open-source decoder for statistical machine translation.
In Proceedings of the ACL-2007 Workshop on Statistical Machine Translation (WMT-07), Prague.
Jorge Civera and Alfons Juan.
2007. Domain adaptation in statistical machine translation with mixture modelling.
In Proceedings of the ACL-2007 Workshop on Statistical Machine Translation (WMT-07), Prague.
Marta R.
Costa-Juss`a and Jose A.R.
Fonollosa. 2007.
Analysis of statistical and morphological classes to generate weighted reordering hypotheses on a statistical machine translation system.
In Proceedings of the ACL-2007 Workshop on Statistical Machine Translation (WMT-07), Prague.
Deborah Coughlin.
2003. Correlating automated and human assessments of machine translation quality.
In Proceedings of MT Summit IX.
Brooke Cowan and Michael Collins.
2005. Morphology and reranking for the statistical parsing of Spanish.
In Proceedings of EMNLP 2005.
George Doddington.
2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.
In Human Language Technology: Notebook Proceedings, pages 128??32, San Diego.
Amit Dubey.
2005. What to do when lexicalization fails: parsing German with suffix analysis and smoothing.
In Proceedings of ACL.
148 Loc Dugast, Jean Senellart, and Philipp Koehn.
2007. Statistical post-editing on SYSTRAN?s rule-based translation system.
In Proceedings of the ACL-2007 Workshop on Statistical Machine Translation (WMT07), Prague.
Christopher J.
Dyer. 2007.
The ?noisier channel?? translation from morphologically complex languages.
In Proceedings of the ACL-2007 Workshop on Statistical Machine Translation (WMT-07), Prague.
Matthias Eck and Chiori Hori.
2005. Overview of the IWSLT 2005 evaluation campaign.
In Proceedings of International Workshop on Spoken Language Translation.
Jesus Gimenez and Llus M`arquez.
2007. Linguistic features for automatic evaluation of heterogenous mt systems.
In Proceedings of ACL Workshop on Statistical Machine Translation.
Maria Holmqvist, Sara Stymne, and Lars Ahrenberg.
2007. Getting to know Moses: Initial experiments on German-English factored translation.
In Proceedings of the ACL-2007 Workshop on Statistical Machine Translation (WMT-07), Prague.
Douglas Jones, Wade Shen, Neil Granoien, Martha Herzog, and Clifford Weinstein.
2005. Measuring translation quality by testing english speakers with a new defense language proficiency test for arabic.
In Proceedings of the 2005 International Conference on Intelligence Analysis.
Philipp Koehn and Christof Monz.
2005. Shared task: Statistical machine translation between European languages.
In Proceedings of ACL 2005 Workshop on Parallel Text Translation.
Philipp Koehn and Christof Monz.
2006. Manual and automatic evaluation of machine translation between European languages.
In Proceedings of NAACL 2006 Workshop on Statistical Machine Translation.
Philipp Koehn and Josh Schroeder.
2007. Experiments in domain adaptation for statistical machine translation.
In Proceedings of the ACL-2007 Workshop on Statistical Machine Translation (WMT-07), Prague.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statisticalphrase-basedtranslation.
InProceedings of HLT/NAACL.
Philipp Koehn, Nicola Bertoldi, Ondrej Bojar, Chris Callison-Burch, Alexandra Constantin, Brooke Cowan, Chris Dyer, Marcello Federico, Evan Herbst, Hieu Hoang, Christine Moran, Wade Shen, and Richard Zens.
2006. Factored translation models.
CLSP Summer Workshop Final Report WS-2006, Johns Hopkins University.
Philipp Koehn.
2004. Pharaoh: A beam search decoder for phrase-based statistical machine translation models.
In Proceedings of AMTA.
Philipp Koehn.
2005. A parallel corpus for statistical machine translation.
In Proceedings of MT-Summit.
J. R.
Landis and G.
G. Koch.
1977. The measurement of observer agreement for categorical data.
Biometrics, 33:159??74.
Alon Lavie and Abhaya Agarwal.
2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments.
In Proceedings of the Workshop on Statistical Machine Translation, Prague, June.
Association for Computational Linguistics.
LDC. 2005.
Linguistic data annotation specification: Assessment of fluency and adequacy in translations.
Revision 1.5.
Audrey Lee.
2006. NIST 2006 machine translation evaluation official results.
Official release of automatic evaluation scores for all submissions, November.
Ding Liu and Daniel Gildea.
2007. Source-language features and maximum correlation training for machine translation evaluation.
In Proceedings of NAACL.
Dan Melamed, Ryan Green, and Jospeh P.
Turian. 2003.
Precision and recall of machine translation.
In Proceedings of HLT/NAACL.
Preslav Nakov and Marti Hearst.
2007. UCB system description for the WMT 2007 shared task.
In Proceedings of the ACL-2007 Workshop on Statistical Machine Translation (WMT-07), Prague.
Franz Josef Och and Hermann Ney.
2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19??1, March.
Franz Josef Och and Hermann Ney.
2004. The alignment template approach to statistical machine translation.
Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002. Bleu: A method for automatic evaluation of machine translation.
In Proceedings of ACL.
Michael Paul.
2006. Overview of the IWSLT 2006 evaluation campaign.
In Proceedings of International Workshop on Spoken Language Translation.
Matthias Paulik, Kay Rottmann, Jan Niehues, Silja Hildebrand, and Stephan Vogel.
2007. The ISL phrase-based MT system for the 2007 ACL Workshop on Statistical Machine Translation.
In Proceedings of the ACL-2007 Workshop on Statistical Machine Translation (WMT-07), Prague.
49 Maja Popovic and Hermann Ney.
2007. Worderrorrates: Decomposition over POS classes and applications for error analysis.
In Proceedings of ACL Workshop on Statistical Machine Translation.
Mark Przybocki.
2004. NIST 2004 machine translation evaluation results.
Confidential e-mail to workshop participants, May.
Holger Schwenk.
2007. Building a statistical machine translation system for French using the Europarl corpus.
In Proceedings of the ACL-2007 Workshop on Statistical Machine Translation (WMT-07), Prague.
Michel Simard, Nicola Ueffing, Pierre Isabelle, and Roland Kuhn.
2007. Rule-based translation with statistical phrase-based post-editing.
In Proceedings of the ACL-2007 Workshop on Statistical Machine Translation (WMT-07), Prague.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul.
2006. A study of translation edit rate with targeted human annotation.
In Proceedings of Association for Statistical Machine Translation in the Americas.
Nicola Ueffing, Michel Simard, Samuel Larkin, and Howard Johnson.
2007. NRC?s PORTAGE system for WMT 2007.
In Proceedings of the ACL-2007 Workshop on Statistical Machine Translation (WMT-07), Prague.
LiangZhou, Chin-YewLin, andEduardHovy.
2006. Re-evaluating machine translation results with paraphrase support.
In Proceedings of EMNLP.
Andreas Zollmann, Ashish Venugopal, Matthias Paulik, and Stephan Vogel.
2007. The syntax augmented MT (SAMT) system for the shared task in the 2007 ACL Workshop on Statistical Machine Translation.
In Proceedings of the ACL-2007 Workshop on Statistical Machine Translation (WMT-07), Prague .
Proceedings of the Second Workshop on Statistical Machine Translation, pages 159??66, Prague, June 2007.
c2007 Association for Computational Linguistics Context-aware Discriminative Phrase Selection for Statistical Machine Translation Jesus Gimenez and Llus M`arquez TALP Research Center, LSI Department Universitat Polit`ecnica de Catalunya Jordi Girona Salgado 1??, E-08034, Barcelona {jgimenez,lluism}@lsi.upc.edu Abstract In this work we revise the application of discriminative learning to the problem of phrase selection in Statistical Machine Translation.
Inspired by common techniques used in Word Sense Disambiguation, we train classifiers based on local context to predict possible phrase translations.
Our work extends that of Vickrey et al.(2005) in two main aspects.
First, we move from word translation to phrase translation.
Second, we move from the ?blank-filling??task to the ?full translation??task.
We report results on a set of highly frequent source phrases, obtaining a significant improvement, specially with respect to adequacy, according to a rigorous process of manual evaluation.
1 Introduction
Translations tables in Phrase-based Statistical Machine Translation (SMT) are often built on the basis of Maximum-likelihood Estimation (MLE), being one of the major limitations of this approach that the source sentence context in which phrases occur is completely ignored (Koehn et al., 2003).
In this work, inspired by state-of-the-art Word Sense Disambiguation (WSD) techniques, we suggest using Discriminative Phrase Translation (DPT) models which take into account a wider feature context.
Following the approach by Vickrey et al.(2005), we deal with the ?phrase translation??problem as a classification problem.
We use Support Vector Machines (SVMs) to predict phrase translations in the context of the whole source sentence.
We extend the work by Vickrey et al.(2005) in two main aspects.
First, we move from ?word translation??to ?phrase translation??
Second, we move from the ?blank-filling??task to the ?full translation??task.
Our approach is fully described in Section 2.
We apply it to the Spanish-to-English translation of European Parliament Proceedings.
In Section 3, prior to considering the ?full translation??task, we analyze the impact of using DPT models for the isolated ?phrase translation??task.
In spite of working on a very specific domain, a large room for improvement, coherent with WSD performance, and results by Vickrey et al.(2005), is predicted.
Then, in Section 4, we tackle the full translation task.
DPT models are integrated in a ?soft??manner, by making them available to the decoder so they can fully interact with other models.
Results using a reduced set of highly frequent source phrases show a significant improvement, according to several automatic evaluation metrics.
Interestingly, the BLEU metric (Papineni et al., 2001) is not able to reflect this improvement.
Through a rigorous process of manual evaluation we have verified the gain.
We have also observed that it is mainly related to adequacy.
These results confirm that better phrase translation probabilities may be helpful for the full translation task.
However, the fact that no gain in fluency is reported indicates that the integration of these probabilities into the statistical framework requires further study.
2 Discriminative
Phrase Translation In this section we describe the phrase-based SMT baseline system and how DPT models are built and integrated into this system in a ?soft??manner.
159 2.1 Baseline System The baseline system is a phrase-based SMT system (Koehn et al., 2003), built almost entirely using freely available components.
We use the SRI Language Modeling Toolkit (Stolcke, 2002) for language modeling.
We build trigram language models applying linear interpolation and Kneser-Ney discounting for smoothing.
Translation models are built on top of word-aligned parallel corpora linguistically annotated at the level of shallow syntax (i.e., lemma, part-of-speech, and base phrase chunks) as described by Gimenez and M`arquez (2005).
Text is automatically annotated, using the SVMTool (Gimenez and M`arquez, 2004), Freeling (Carreras et al., 2004), and Phreco (Carreras et al., 2005) packages.
We used the GIZA++ SMT Toolkit1 (Och and Ney, 2003) to generate word alignments.
We apply the phrase-extract algorithm, as described by Och (2002), on the Viterbi alignments output by GIZA++ following the ?global phrase extraction?? strategy described by Gimenez and M`arquez (2005) (i.e., a single phrase translation table is built on top of the union of alignments corresponding to different linguistic data views).
We work with the union of source-to-target and target-to-source alignments, with no heuristic refinement.
Phrases up to length five are considered.
Also, phrase pairs appearing only once are discarded, and phrase pairs in which the source/target phrase is more than three times longer than the target/source phrase are ignored.
Phrase pairs are scored on the basis of unsmoothed relative frequency (i.e., MLE).
Regarding the argmax search, we used the Pharaoh beam search decoder (Koehn, 2004), which naturally fits with the previous tools.
2.2 DPT
for SMT Instead of relying on MLE estimation to score the phrase pairs (fi,ej) in the translation table, we suggest considering the translation of every source phrase fi as a multi-class classification problem, where every possible translation of fi is a class.
We use local linear SVMs 2.
Since SVMs are binary classifiers, the problem must be binarized.
We 1http://www.fjoch.com/GIZA++.html 2We use the SVMlight package, which is freely available at http://svmlight.joachims.org(Joachims, 1999).
have applied a simple one-vs-all binarization, i.e., a SVM is trained for every possible translation candidate ej.
Training examples are extracted from the same training data as in the case of MLE models, i.e., an aligned parallel corpus, obtained as described in Section 2.1.
We use each sentence pair in which the source phrase fi occurs to generate a positive example for the classifier corresponding to the actual translation of fi in that sentence, according to the automatic alignment.
This will be as well a negative example for the classifiers corresponding to the rest of possible translations of fi.
2.2.1 Feature
Set We consider different kinds of information, always from the source sentence, based on standard WSD methods (Yarowsky et al., 2001).
As to the local context, inside the source phrase to disambiguate, and 5 tokens to the left and to the right, we use n-grams (n ??{1,2,3}) of: words, partsof-speech, lemmas and base phrase chunking IOB labels.
As to the global context, we collect topical information by considering the source sentence as a bag of lemmas.
2.2.2 Decoding.
A Trick.
At translation time, we consider every instance of fi as a separate case.
In each case, for all possible translations of fi, we collect the SVM score, according to the SVM classification rule.
We are in fact modeling P(ej|fi).
However, these scores are not probabilities.
We transform them into probabilities by applying the softmax function described by Bishop (1995).
We do not constrain the decoder to use the translation ej with highest probability.
Instead, we make all predictions available and let the decoder choose.
We have avoided implementing a new decoder by pre-computing all the SVM predictions for all possible translations for all source phrases appearing in the test set.
We input this information onto the decoder by replicating the entries in the translation table.
In other words, each distinct occurrence of every single source phrase has a distinct list of phrase translation candidates with their corresponding scores.
Accordingly, the source sentence is transformed into a sequence of identifiers, 160 in our case a sequence of (w,i) pairs3, which allow us to uniquely identify every distinct instance of every word in the test set during decoding, and to retrieve DPT predictions in the translation table.
For that purpose, source phrases in the translation table must comply with the same format.
This imaginative trick4 saved us in the short run a gigantic amount of work.
However, it imposes a severe limitation on the kind of features which the DPT system may use.
In particular, features from the target sentence under construction and from the correspondence between source and target (i.e., alignments) can not be used.
3 Phrase
Translation Analogously to the ?word translation??definition by Vickrey et al.(2005), rather than predicting the sense of a word according to a given sense inventory, in ?phrase translation?? the goal is to predict the correct translation of a phrase, for a given target language, in the context of a sentence.
This task is simpler than the ?full translation??task, but provides an insight to the gain prospectives.
We used the data from the Openlab 2006 Initiative5 promoted by the TC-STAR Consortium6.
This test suite is entirely based on European Parliament Proceedings.
We have focused on the Spanish-toEnglish task.
The training set consists of 1,281,427 parallel sentences.
Performing phrase extraction over the training data, as described in Section 2.1, we obtained translation candidates for 1,729,191 source phrases.
We built classifiers for all the source phrases with more than one possible translation and more than 10 occurrences.
241,234 source phrases fulfilled this requirement.
For each source phrase, we used 80% of the instances for training, 10% for development, and 10% for test.
Table 1 shows ?phrase translation??results over the test set.
We compare the performance, in terms of accuracy, of DPT models and the ?most frequent translation??baseline (?MFT??.
The MFT base3w is a word and i corresponds to the number of instances of word w seen in the test set before the current instance.
4We have checked that results following this type of decoding when translation tables are estimated on the basis of MLE are identical to regular decoding results.
5http://tc-star.itc.it/openlab2006/ 6http://www.tc-star.org/ phrase set model macro micro all MFT 0.66 0.70 DPT 0.68 0.76 frequent MFT 0.76 0.75 DPT 0.86 0.86 Table 1: ?Phrase Translation??Accuracy (test set).
line is equivalent to selecting the translation candidate with highest probability according to MLE.
The ?macro??column shows macro-averaged results over all phrases, i.e., the accuracy for each phrase counts equally towards the average.
The ?micro??column shows micro-averaged accuracy, where each test example counts equally.
The ?all??set includes results for the 241,234 phrases, whereas the ?frequent??set includes results for a selection of 41 very frequent phrases ocurring more than 50,000 times.
A priori, DPT models seem to offer a significant room for potential improvement.
Although phrase translation differs from WSD in a number of aspects, the increase with respect to the MFT baseline is comparable.
Results are also coherent with those attained by Vickrey et al.(2005). -1 -0.5 0 0.5 1 0 50000 100000 150000 200000 250000 300000 accuracy(DPT) accuracy(MLE) #examples Figure 1: Analysis of ?Phrase Translation??Results on the development set (Spanish-to-English).
Figure 1 shows the relationship between the accuracy7 gain and the number of training examples.
In general, with a sufficient number of examples (over 10,000), DPT outperforms the MFT baseline.
7We focus on micro-averaged accuracy.
161 4 Full Translation In the ?phrase translation??task the predicted phrase does not interact with the rest of the target sentence.
In this section we analyze the impact of DPT models when the goal is to translate the whole sentence.
For evaluation purposes we count on a set of 1,008 sentences.
Three human references per sentence are available.
We randomly split this set in two halves, and use them for development and test, respectively.
4.1 Evaluation
Evaluating the effects of using DPT predictions, directed towards a better word selection, in the full translation task presents two serious difficulties.
In first place, the actual room for improvement caused by a better translation modeling is smaller than estimated in Section 3.
This is mainly due to the SMT architecture itself which relies on a search over a probability space in which several models cooperate.
For instance, in many cases errors caused by a poor translation modeling may be corrected by the language model.
In a recent study, Vilar et al.(2006) found that only around 25% of the errors are related to word selection.
In half of these cases errors are caused by a wrong word sense disambiguation, and in the other half the word sense is correct but the lexical choice is wrong.
In second place, most conventional automatic evaluation metrics have not been designed for this purpose.
For instance, metrics such as BLEU (Papineni et al., 2001) tend to favour longer n-gram matchings, and are, thus, biased towards word ordering.
We might find better suited metrics, such as METEOR (Banerjee and Lavie, 2005), which is oriented towards word selection8.
However, a new problem arises.
Because different metrics are biased towards different aspects of quality, scores conferred by different metrics are often controversial.
In order to cope with evaluation difficulties we have applied several complementary actions: 1.
Based on the results from Section 3, we focus on a reduced set of 41 very promising phrases trained on more than 50,000 examples.
This set covers 25.8% of the words in the test set, 8METEOR works at the unigram level, may consider word stemming and, for the case of English is also able to perform a lookup for synonymy in WordNet (Fellbaum, 1998).
and exhibits a potential absolute accuracy gain around 11% (See Table 1).
2. With the purpose of evaluating the changes related only to this small set of very promising phrases, we introduce a new measure, Apt, which computes ?phrase translation??accuracy for a given list of source phrases.
For every test case, Apt counts the proportion of phrases from the list appearing in the source sentence which have a valid9 translation both in the target sentence and in any of the reference translations.
In fact, because in general source-totarget alignments are not known, Apt calculates an approximate10 solution.
3. We evaluate overall MT quality on the basis of ?Human Likeness??
In particular, we use the QUEEN11 meta-measure from the QARLA Framework (Amigo et al., 2005).
QUEEN operates under the assumption that a good translation must be similar to all human references according to all metrics.
Given a set of automatic translations A, a set of similarity metrics X, and a set of human references R, QUEEN is defined as the probability, over RRR, that for every metric in X the automatic translation a is more similar to a reference r than two other references rprime and rprimeprime to each other.
Formally: QUEENX,R(a) = Prob(?x ??X : x(a,r) ??x(rprime,rprimeprime)) QUEEN captures the features that are common to all human references, rewarding those automatic translations which share them, and penalizing those which do not.
Thus, QUEEN provides a robust means of combining several metrics into a single measure of quality.
Following the methodology described by Gimenez and Amigo (2006), we compute the QUEEN measure over the metric combination with highest KING, i.e., discriminative power.
We have considered all the lexical metrics12 provided by 9Valid translations are provided by the translation table.
10Current Apt implementation searches phrases from left to right in decreasing length order.
11QUEEN is available inside the IQMT package for MT Evaluation based on ?Human Likeness??(Gimenez and Amigo, 2006).
http://www.lsi.upc.edu/?nlp/IQMT 12Consult the IQMT Technical Manual v1.3 for a detailed description of the metric set.
http://www.lsi.upc.edu/ ?nlp/IQMT/IQMT.v1.3.pdf 162 QUEEN Apt BLEU METEOR ROUGE P(e) +PMLE(f|e) 0.43 0.86 0.59 0.77 0.42 P(e) +PMLE(e|f) 0.45 0.87 0.62 0.77 0.43 P(e) +PDPT(e|f) 0.47 0.89 0.62 0.78 0.44 Table 2: Automatic evaluation of the ?full translation??results on the test set.
IQMT. The optimal set is: { METEORwnsyn, ROUGEw 1.2 } which includes variants of METEOR, and ROUGE (Lin and Och, 2004).
4.2 Adjustment
of Parameters Models are combined in a log-linear fashion: logP(e|f) ??lmlogP(e) + glogPMLE(f|e) + dlogPMLE(e|f) + DPTlogPDPT(e|f) P(e) is the language model probability.
PMLE(f|e) corresponds to the MLE-based generative translation model, whereas PMLE(e|f) corresponds to the analogous discriminative model.
PDPT(e|f) corresponds to the DPT model which uses SVM-based predictions in a wider feature context.
In order to perform fair comparisons, model weights must be adjusted.
Because we have focused on a reduced set of frequent phrases, in order to translate the whole test set we must provide alternative translation probabilities for all the source phrases in the vocabulary which do not have a DPT prediction.
We have used MLE predictions to complete the model.
However, interaction between DPT and MLE models is problematic.
Problems arise when, for a given source phrase, fi, DPT predictions must compete with MLE predictions for larger phrases fj overlapping with or containing fi (See Section 4.3).
We have alleviated these problems by splitting DPT tables in 3 subtables: (1) phrases with DPT prediction, (2) phrases with DPT prediction only for subphrases of it, and (3) phrases with no DPT prediction for any subphrase; and separately adjusting their weights.
Counting on a reliable automatic measure of quality is a crucial issue for system development.
Optimal configurations may vary very significantly depending on the metric governing the optimization process.
We optimize the system parameters over the QUEEN measure, which has proved to lead to more robust system configurations than BLEU (Lambert et al., 2006).
We exhaustively try all possible parameter configurations, at a resolution of 0.1, over the development set and select the best one.
In order to keep the optimization process feasible, in terms of time, the search space is pruned13 during decoding.
4.3 Results
We compare the systems using the generative and discriminative MLE-based translation models to the discriminative translation model which uses DPT predictions for the set of 41 very ?frequent??source phrases.
Table 2 shows automatic evaluation results on the test set, according to several metrics.
Phrase translation accuracy (over the ?frequent??set of phrases) and MT quality are evaluated by means of the Apt and QUEEN measures, respectively.
For the sake of informativeness, BLEU, METEORwnsyn and ROUGEw 1.2 scores are provided as well.
Interestingly, discriminative models outperform the (noisy-channel) default generative model.
Improvement in Apt measure also reveals that DPT predictions provide a better translation for the set of ?frequent??phrases than the MLE models.
This improvement remains when measuring overall translation quality via QUEEN.
If we take into account that DPT predictions are available for only 25% of the words in the test set, we can say that the gain reported by the QUEEN and Apt measures is consistent with the accuracy prospectives predicted in Table 1.
METEORwnsyn and ROUGEw 1.2 reflect a slight improvement as well.
However, according to BLEU there is no difference between both systems.
We suspect that BLEU is unable to accurately reflect the possible gains attained by a better ?phrase selection?? over a small set of phrases because of its tendency 13For each phrase only the 30 top-scoring translations are used.
At all times, only the 100 top-scoring solutions are kept.
We also disabled distortion and word penalty models.
Therefore, translations are monotonic, and source and target tend to have the same number of words (that is not mandatory).
163 to reward long n-gram matchings.
In order to clarify this scenario a rigorous process of manual evaluation has been conducted.
We have selected a subset of sentences based on the following criteria: ??sentence length between 10 and 30 words.
??at least 5 words have a DPT prediction.
??DPT and MLE outputs differ.
A total of 114 sentences fulfill these requirements.
In each translation case, assessors must judge whether the output by the discriminative ?MLE??system is better, equal to or worse than the output by the ?DPT??system, with respect to adequacy, fluency, and overall quality.
In order to avoid any bias in the evaluation, we have randomized the respective position in the display of the sentences corresponding to each system.
Four judges participated in the evaluation.
Each judge evaluated only half of the cases.
Each case was evaluated by two different judges.
Therefore, we count on 228 human assessments.
Table 3 shows the results of the manual system comparison.
Statistical significance has been determined using the sign-test (Siegel, 1956).
According to human assessors, the ?DPT??system outperforms the ?MLE??system very significantly with respect to adequacy, whereas for fluency there is a slight advantage in favor of the ?MLE??system.
Overall, there is a slight but significant advantage in favor of the ?DPT??system.
Manual evaluation confirms our suspicion that the BLEU metric is less sensitive than QUEEN to improvements related to adequacy.
Error Analysis Guided by the QUEEN measure, we carefully inspect particular cases.
We start, in Table 4, by showing a positive case.
The three phrases highlighted in the source sentence (?tiene??
?se?nora??and ?una cuestion?? find a better translation with the help of the DPT models: ?tiene??translates into ?has??instead of ?i give??
?se?nora??into ?mrs??instead of ?lady?? and ?una cuestion??into ?a point??instead of ?a...
motion?? In contrast, Table 5 shows a negative case.
The translation of the Spanish word ?se?nora??as ?mrs??is acceptable.
However, it influences very negatively the translation of the following word ?diputada?? whereas the ?MLE??system translates the phrase ?se?nora diputada?? which does not have a DPT prediction, as a whole.
Similarly, the translation of Adequacy Fluency Overall MLE > DPT 39 84 83 MLE = DPT 100 76 46 MLE < DPT 89 68 99 Table 3: Manual evaluation of the ?full translation?? results on the test set.
Counts on the number of translation cases for which the ?MLE??system is better than (>), equal to (=), or worse than (<) the ?DPT??system, with respect to adequacy, fluency, and overall MT quality, are presented.
?cuestion??as ?matter?? although acceptable, is breaking the phrase ?cuestion de orden??of high cohesion, which is commonly translated as ?point of order??
The cause underlying these problems is that DPT predictions are available only for a subset of phrases.
Thus, during decoding, for these cases our DPT models may be in disadvantage.
5 Related
Work Recently, there is a growing interest in the application of WSD technology to MT.
For instance, Carpuat and Wu (2005b) suggested integrating WSD predictions into a SMT system in a ?hard?? manner, either for decoding, by constraining the set of acceptable translation candidates for each given source word, or for post-processing the SMT system output, by directly replacing the translation of each selected word with the WSD system prediction.
They did not manage to improve MT quality.
They encountered several problems inherent to the SMT architecture.
In particular, they described what they called the ?language model effect??in SMT: ?The lexical choices are made in a way that heavily prefers phrasal cohesion in the output target sentence, as scored by the language model.??
This problem is a direct consequence of the ?hard??interaction between their WSD and SMT systems.
WSD predictions cannot adapt to the surrounding target context.
In a later work, Carpuat and Wu (2005a) analyzed the converse question, i.e. they measured the WSD performance of SMT models.
They showed that dedicated WSD models significantly outperform current state-of-the-art SMT models.
Consequently, SMT should benefit from WSD predictions.
Simultaneously, Vickrey et al.(2005) studied the 164 Source tiene la palabra la se?nora mussolini para una cuestion de orden . Ref 1 mrs mussolini has the floor for a point of order . Ref 2 you have the floor, missus mussolini, for a question of order . Ref 3 ms mussolini has now the floor for a point of order . P(e) + PMLE(e|f) i give the floor to the lady mussolini for a procedural motion . P(e) + PDPT(e|f) has the floor the mrs mussolini on a point of order . Table 4: Case of Analysis of sentence #422.
DPT models help.
Source se?nora diputada, esta no es una cuestion de orden . Ref 1 mrs mussolini, that is not a point of order . Ref 2 honourable member, this is not a question of order . Ref 3 my honourable friend, this is not a point of order . P(e) + PMLE(e|f) honourable member, this is not a point of order . P(e) + PDPT(e|f) mrs karamanou, this is not a matter of order . Table 5: Case of Analysis of sentence #434.
DPT models fail.
application of discriminative models based on WSD technology to the ?blank-filling??task, a simplified version of the translation task, in which the target context surrounding the word translation is available.
They did not encounter the ?language model effect??because they approached the task in a ?soft?? way, i.e., allowing their WSD models to interact with other models during decoding.
Similarly, our DPT models are, as described in Section 2.2, softly integrated in the decoding step, and thus do not suffer from the detrimental ?language model effect??either, in the context of the ?full translation??task.
Besides, DPT models enforce phrasal cohesion by considering disambiguation at the level of phrases.
6 Conclusions
and Further Work Despite the fact that measuring improvements in word selection is a very delicate issue, we have showed that dedicated discriminative translation models considering a wider feature context provide a useful mechanism in order to improve the quality of current phrase-based SMT systems, specially with regard to adequacy.
However, the fact that no gain in fluency is reported indicates that the integration of these probabilities into the statistical framework requires further study.
Moreover, there are several open issues.
First, for practical reasons, we have limited to a reduced set of ?frequent??phrases, and we have disabled reordering and word penalty models.
We are currently studying the impact of a larger set of phrases, covering over 99% of the words in the test set.
Experiments with enabled reordering and word penalty models should be conducted as well.
Second, automatic evaluation of the results revealed a low agreement between BLEU and other metrics.
For system comparison, we solved this through a process of manual evaluation.
However, this is impractical for the adjustment of parameters, where hundreds of different configurations are tried.
In this work we have relied on automatic evaluation based on ?Human Likeness??which allows for metric combinations and provides a stable and robust criterion for the metric set selection.
Other alternatives could be tried.
The crucial issue, in our opinion, is that the metric guiding the optimization is able to capture the changes.
Finally, we argue that, if DPT models considered features from the target side, and from the correspondence between source and target, results could further improve.
However, at the short term, the incorporation of these type of features will force us to either build a new decoder or extend an existing one, or to move to a new MT architecture, for instance, in the fashion of the architectures suggested by Tillmann and Zhang (2006) or Liang et al.(2006). Acknowledgements This research has been funded by the Spanish Ministry of Education and Science, projects OpenMT (TIN2006-15307-C03-02) and TRAN165 GRAM (TIN2004-07925-C03-02).
We are recognized as a Quality Research Group (2005 SGR00130) by DURSI, the Research Department of the Catalan Government.
Authors are thankful to the TC-STAR Consortium for providing such very valuable data sets.
References Enrique Amigo, Julio Gonzalo, Anselmo Pe?nas, and Felisa Verdejo.
2005. QARLA: a Framework for the Evaluation of Automatic Sumarization.
In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics.
Satanjeev Banerjee and Alon Lavie.
2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.
In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.
Christopher M.
Bishop. 1995.
6.4: Modeling conditional distributions.
In Neural Networks for Pattern Recognition, page 215.
Oxford University Press.
Marine Carpuat and Dekai Wu.
2005a. Evaluating the Word Sense Disambiguation Performance of Statistical Machine Translation.
In Proceedings of IJCNLP.
Marine Carpuat and Dekai Wu.
2005b. Word Sense Disambiguation vs.
Statistical Machine Translation.
In Proceedings of ACL.
Xavier Carreras, Isaac Chao, Llus Padro, and Muntsa Padro.
2004. FreeLing: An Open-Source Suite of Language Analyzers.
In Proceedings of the 4th LREC.
Xavier Carreras, Llus Marquez, and Jorge Castro.
2005. Filtering-ranking perceptron learning for partial parsing.
Machine Learning, 59:1??1.
C. Fellbaum, editor.
1998. WordNet.
An Electronic Lexical Database.
The MIT Press.
Jesus Gimenez and Enrique Amigo.
2006. IQMT: A Framework for Automatic Machine Translation Evaluation.
In Proceedings of the 5th LREC.
Jesus Gimenez and Llus M`arquez.
2004. SVMTool: A general POS tagger generator based on Support Vector Machines.
In Proceedings of 4th LREC.
Jesus Gimenez and Llus M`arquez.
2005. Combining Linguistic Data Views for Phrase-based SMT.
In Proceedings of the Workshop on Building and Using Parallel Texts, ACL.
T. Joachims.
1999. Making large-Scale SVM Learning Practical.
In B.
Scholkopf, C.
Burges, and A.
Smola, editors, Advances in Kernel Methods Support Vector Learning.
The MIT Press.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation.
In Proceedings of HLT/NAACL.
Philipp Koehn.
2004. Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models.
In Proceedings of AMTA.
Patrik Lambert, Jesus Gimenez, Marta R.
Costa-jussa, Enrique Amigo, Rafael E.
Banchs, Llus Marquez, and J.A.
R. Fonollosa.
2006. Machine Translation System Development based on Human Likeness.
In Proceedings of IEEE/ACL 2006 Workshop on Spoken Language Technology.
Percy Liang, Alexandre Bouchard-C?ote, Dan Klein, and Ben Taskar.
2006. An End-to-End Discriminative Approach to Machine Translation.
In Proceedings of COLING-ACL06.
Chin-Yew Lin and Franz Josef Och.
2004. Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statics.
In Proceedings of ACL.
Franz Josef Och and Hermann Ney.
2003. A Systematic Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19??1.
Franz Josef Och.
2002. Statistical Machine Translation: From Single-Word Models to Alignment Templates.
Ph.D. thesis, RWTH Aachen, Germany.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001. Bleu: a method for automatic evaluation of machine translation, rc22176.
Technical report, IBM T.J.
Watson Research Center.
Sidney Siegel.
1956. Nonparametric Statistics for the Behavioral Sciences.
McGraw-Hill. Andreas Stolcke.
2002. SRILM An Extensible Language Modeling Toolkit.
In Proceedings of ICSLP.
Christoph Tillmann and Tong Zhang.
2006. A Discriminative Global Training Algorithm for Statistical MT.
In Proceedings of COLING-ACL06.
D. Vickrey, L.
Biewald, M.
Teyssier, and D.
Koller. 2005.
Word-Sense Disambiguation for Machine Translation.
In Proceedings of HLT/EMNLP.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Hermann Ney.
2006. Error Analysis of Machine Translation Output.
In Proceedings of the 5th LREC.
David Yarowsky, Silviu Cucerzan, Radu Florian, Charles Schafer, and Richard Wicentowski.
2001. The Johns Hopkins Senseval2 System Descriptions.
In Proceedings of Senseval-2: Second International Workshop on Evaluating Word Sense Disambiguation Systems .

