<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>S Bernardini</author>
<author>M Baroni</author>
</authors>
<title>Spotting translationese: A corpus-driven approach using support vector machines</title>
<date>2005</date>
<booktitle>Proceedings of Corpus Linguistics</booktitle>
<pages>9</pages>
<marker>Bernardini, Baroni, 2005</marker>
<rawString>Bernardini, S &amp; Baroni, M. (2005). Spotting translationese: A corpus-driven approach using support vector machines. Proceedings of Corpus Linguistics 2005, page 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bilenko</author>
<author>R J Mooney</author>
</authors>
<title>Adaptive duplicate detection using learnable string similarity measures</title>
<date>2003</date>
<booktitle>In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2003</booktitle>
<pages>39--48</pages>
<contexts>
<context>oaches use only positive examples (i.e. synonymous strings) to tune the cost of editing (Yeganova et al. 2004; Cohen &amp; Minkov 2006). Other, more recent approaches use also negative examples to learn (Bilenko &amp; Mooney 2003; McCallum et al. 2005; Tsurouka et al. 2007) hence enabling the adaption to features that discriminate negative from positive examples. Modeling the cost function is just one of the steps, though. On</context>
</contexts>
<marker>Bilenko, Mooney, 2003</marker>
<rawString>Bilenko, M. and Mooney, R. J. (2003). Adaptive duplicate detection using learnable string similarity measures. In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2003), pages 39–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ciravegna</author>
</authors>
<title>Understanding Messages in a Diagnostic Domain</title>
<date>1995</date>
<booktitle>In Information Processing and Management</booktitle>
<volume>31</volume>
<pages>687--701</pages>
<contexts>
<context>y issue. Technical terms can be very complex from a linguistic point of view; an average technical domain has thousands of official terms and several thousands of synonyms can be found. For example, (Ciravegna, 1995) discusses how in the car domain, technical lists can contain around 30,000 terms and synonymic terms can be represented in Italian via complex noun phrases up to ten words long. Studies in the biolo</context>
</contexts>
<marker>Ciravegna, 1995</marker>
<rawString>Ciravegna, F. (1995). Understanding Messages in a Diagnostic Domain. In Information Processing and Management, Vol. 31, No. 5, pp 687-701, 1995 Cohen, W.  Ravikumar, P. Fienberg, S. (2003). A Comparison of String Metrics for Matching Names and Records. pages 1, 2, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Cohen</author>
<author>E Minkov</author>
</authors>
<title>A graph-search framework for associating gene identifies with documents</title>
<date>2006</date>
<journal>BMC Bioinformatics</journal>
<volume>7</volume>
<contexts>
<context>ically the cost of edit operation), either manually or automatically. Some automatic approaches use only positive examples (i.e. synonymous strings) to tune the cost of editing (Yeganova et al. 2004; Cohen &amp; Minkov 2006). Other, more recent approaches use also negative examples to learn (Bilenko &amp; Mooney 2003; McCallum et al. 2005; Tsurouka et al. 2007) hence enabling the adaption to features that discriminate negat</context>
</contexts>
<marker>Cohen, Minkov, 2006</marker>
<rawString>Cohen, W. W. and Minkov, E. (2006). A graph-search framework for associating gene identifies with documents. BMC Bioinformatics, 7(440).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Fang</author>
<author>K Murphy</author>
<author>Y Jin</author>
<author>J S Kim</author>
<author>P S White</author>
</authors>
<title>Human gene name normalization using text matching with automatically extracted synonym dictionaries</title>
<date>2006</date>
<booktitle>In Proceedings of BioNLP’06 Harris, Z</booktitle>
<publisher>John Wiley &amp; Sons</publisher>
<location>New York</location>
<marker>Fang, Murphy, Jin, Kim, White, 2006</marker>
<rawString>Fang, H., Murphy, K., Jin, Y., Kim, J. S., and White, P. S. (2006). Human gene name normalization using text matching with automatically extracted synonym dictionaries. In Proceedings of BioNLP’06 Harris, Z. (1968). Mathematical Structures of Language. John Wiley &amp; Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Krauthammer</author>
<author>G Nenadić</author>
</authors>
<title>Term Identification in the Biomedical Literature</title>
<date>2004</date>
<journal>Journal of Biomedical Informatics, Volume</journal>
<volume>37</volume>
<pages>8--17</pages>
<contexts>
<context>lists can contain around 30,000 terms and synonymic terms can be represented in Italian via complex noun phrases up to ten words long. Studies in the biological domain have shown a similar situation (Krauthammer &amp; Nenadić, 2004). There are a number of issues that affect terminology recognition. From simple orthographical differences (change in case, hyphenation, use of Arabic Vs Roman numerals, etc.) to more complex issues </context>
</contexts>
<marker>Krauthammer, Nenadić, 2004</marker>
<rawString>Krauthammer, M., Nenadić, G.. (2004). Term Identification in the Biomedical Literature, Journal of Biomedical Informatics, Volume 37, Issue 6, Levenshtein, V. I. (1965). Binary codes capable of correcting spurious insertions and deletions of ones. Problems of Information Transmission, 1(1), 8–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Martin</author>
</authors>
<title>Knowledge Extraction</title>
<date>1990</date>
<booktitle>In Proceedings of the Twelfth Annual Conference of the Cognitive Science Society. Hillsdale, NJ: Lawrence Erlbaum Associates</booktitle>
<pages>252--262</pages>
<marker>Martin, 1990</marker>
<rawString>Martin, L.E. (1990). Knowledge Extraction. In Proceedings of the Twelfth Annual Conference of the Cognitive Science Society. Hillsdale, NJ: Lawrence Erlbaum Associates, pp. 252--262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>K Bellare</author>
<author>F Pereira</author>
</authors>
<title>A conditional random field for discriminatively-trained finite-state string edit distance</title>
<date>2005</date>
<booktitle>In Proceedings of Conference on Uncertainty in AI (UAI</booktitle>
<contexts>
<context>e examples (i.e. synonymous strings) to tune the cost of editing (Yeganova et al. 2004; Cohen &amp; Minkov 2006). Other, more recent approaches use also negative examples to learn (Bilenko &amp; Mooney 2003; McCallum et al. 2005; Tsurouka et al. 2007) hence enabling the adaption to features that discriminate negative from positive examples. Modeling the cost function is just one of the steps, though. Once the metrics has bee</context>
</contexts>
<marker>McCallum, Bellare, Pereira, 2005</marker>
<rawString>McCallum, A., Bellare, K., and Pereira, F. (2005). A conditional random field for discriminatively-trained finite-state string edit distance. In Proceedings of Conference on Uncertainty in AI (UAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tsujii McNaught</author>
<author>J Ananiadou</author>
<author>S</author>
</authors>
<title>Learning string similarity measures for gene/protein name dictionary look-up using logistic regression. In Bioinformatics Advance Access</title>
<date>2007</date>
<marker>McNaught, Ananiadou, S, 2007</marker>
<rawString>Tsuruoka, Y,. McNaught, J. Tsujii, J. Ananiadou, S. (2007). Learning string similarity measures for gene/protein name dictionary look-up using logistic regression. In  Bioinformatics Advance Access.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W E Winkler</author>
</authors>
<title>The state of record linkage and current research problems</title>
<date>1999</date>
<journal>Statistical Research Division, U.S. Bureau of the Census</journal>
<tech>Technical report</tech>
<marker>Winkler, 1999</marker>
<rawString>Winkler, W. E. (1999). The state of record linkage and current research problems. Technical report, Statistical Research Division, U.S. Bureau of the Census.</rawString>
</citation>
</citationList>
</algorithm>

