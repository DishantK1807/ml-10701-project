<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>J-J Aucouturiera</author>
<author>B Defreville</author>
<author>F Pachet</author>
</authors>
<title>The bag-of-frames approach to audio pattern recognition: A sufficient model for urban soundscapes but not for polyphonic music</title>
<date>2007</date>
<journal>Journal of the Acoustic Society of America</journal>
<volume>122</volume>
<pages>881--891</pages>
<marker>Aucouturiera, Defreville, Pachet, 2007</marker>
<rawString>Aucouturiera, J.-J., Defreville, B., Pachet, F. (2007). The bag-of-frames approach to audio pattern recognition: A sufficient model for urban soundscapes but not for polyphonic music. Journal of the Acoustic Society of America, 122(2), pp. 881--891.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Casalea</author>
<author>A Russo</author>
<author>S Serranoa</author>
</authors>
<title>Multistyle classification of speech under stress using feature subset selection based on genetic algorithms</title>
<date>2007</date>
<journal>Speech Communication</journal>
<pages>49--10</pages>
<marker>Casalea, Russo, Serranoa, 2007</marker>
<rawString>Casalea, S., Russo, A., Serranoa, S. (2007). Multistyle classification of speech under stress using feature subset selection based on genetic algorithms. Speech Communication, 49(10-11), pp. 801--810.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Casey</author>
</authors>
<title>MPEG-7 sound-recognition tools</title>
<date>2001</date>
<booktitle>IEEE Transactions on Circuits and Systems for Video Technology</booktitle>
<volume>11</volume>
<pages>731--747</pages>
<contexts>
<context>and based TEO autocorrelation envelope area (TEO-CB-Auto-Env) features (Zhou et al. 2001), genetic algorithm selected features (16-GA and 48-GA) (Casalea, 2007), MPEG-7 sound recognition descriptors (Casey, 2001), Mel-frequency cepstral coefficients (Slaney, 1998), wavelet packet based audio descriptors (Sarikaya &amp; Hansen, 2000), pitch, duration, intensity, are evaluated with respect to their capability to d</context>
</contexts>
<marker>Casey, 2001</marker>
<rawString>Casey, M. (2001). MPEG-7 sound-recognition tools. IEEE Transactions on Circuits and Systems for Video Technology, 11(6), pp. 731--747.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ch Clavel</author>
<author>I Vasilescu</author>
</authors>
<title>Fiction database for emotion detection in abnormal situations</title>
<date>2004</date>
<booktitle>In Proceedings of the INTERSPEECH-2004</booktitle>
<pages>2277--2280</pages>
<contexts>
<context>wever, natural corpora with extreme emotional manifestation for surveillance applications are not publicly available because of the private character of the data, their scarcity and unpredictability (Clavel &amp; Vasilescu, 2004; Clavel et al. 2006). In the present work, we report on the current development and analysis of a dedicated database of speech and sounds, which are indicative of atypical events. 2. Objectives and D</context>
<context>allow for the quick retrieval of the sound effect. In Phases II and III, the annotation of speech and audio events is performed by employing the Anvil tool (Kipp, 2001), following the methodology of (Clavel &amp; Vasilescu, 2004). Anvil was selected for this phase, since it provides the means for annotating not only the emotions manifested in the specific segment, but also for capturing context and the situation in details. </context>
</contexts>
<marker>Clavel, Vasilescu, 2004</marker>
<rawString>Clavel, Ch., Vasilescu, I. (2004). Fiction database for emotion detection in abnormal situations. In Proceedings of the INTERSPEECH-2004, pp. 2277--2280.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ch Clavel</author>
<author>I Vasilescu</author>
<author>L Devillers</author>
<author>T Ehrette</author>
<author>G Richard</author>
</authors>
<marker>Clavel, Vasilescu, Devillers, Ehrette, Richard, </marker>
<rawString>Clavel, Ch., Vasilescu, I., Devillers, L., Ehrette, T., Richard, G.</rawString>
</citation>
<citation valid="true">
<title>Fear-type emotions of the SAFE corpus: annotation issues</title>
<date>2006</date>
<booktitle>In Proceedings of the LREC-2006</booktitle>
<marker>2006</marker>
<rawString>(2006). Fear-type emotions of the SAFE corpus: annotation issues. In Proceedings of the LREC-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Coskun</author>
<author>R Hamzah</author>
<author>C Kwong</author>
<author>M Mesilogou</author>
<author>R Mohan</author>
<author>C Park</author>
<author>H-Q Yu</author>
<author>M Grabowski</author>
</authors>
<title>The Washington State Ferries Risk Assessment Project. Puget Sound Event Database Analysis. Prepared for: Blue Ribbon Panel on Washington State Ferry Safety and Washington State Transportation Commission</title>
<date>1999</date>
<location>Olympia, Washington</location>
<contexts>
<context>comparison of the time-frequency characteristics of the unknown sounds with the ones that have been extracted from real-world recordings of events captured in such situations. Beside acoustic events (Coskun et al. 1999), human speech is also indicative source, which can facilitate the identification of crisis or danger (Yang &amp; Rothkrantz, 2007). It is well-known that the accuracy and performance of automatic recogn</context>
</contexts>
<marker>Coskun, Hamzah, Kwong, Mesilogou, Mohan, Park, Yu, Grabowski, 1999</marker>
<rawString>Coskun, E., Hamzah, R., Kwong, C., Mesilogou, M., Mohan, R., Park, C., Yu, H.-Q., Grabowski, M. (1999). The Washington State Ferries Risk Assessment Project. Puget Sound Event Database Analysis. Prepared for: Blue Ribbon Panel on Washington State Ferry Safety and Washington State Transportation Commission Olympia, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Couvreur</author>
<author>V Fontaine</author>
<author>P Gaunard</author>
<author>C G Mubikangiey</author>
</authors>
<title>Automatic classification of environmental noise events by hidden Markov models</title>
<date>1998</date>
<journal>Applied Acoustics</journal>
<volume>54</volume>
<pages>187--206</pages>
<marker>Couvreur, Fontaine, Gaunard, Mubikangiey, 1998</marker>
<rawString>Couvreur, C., Fontaine, V., Gaunard, P., Mubikangiey, C.G. (1998). Automatic classification of environmental noise events by hidden Markov models. Applied Acoustics, 54, pp. 187--206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Haritaoglu</author>
<author>D Harwood</author>
<author>L Davis</author>
</authors>
<title>W4: real-time surveillance of people and their activities</title>
<date>2000</date>
<journal>IEEE Trans. on Pattern Analysis and Machine Intelligence</journal>
<volume>22</volume>
<pages>809--830</pages>
<marker>Haritaoglu, Harwood, Davis, 2000</marker>
<rawString>Haritaoglu, I., Harwood, D., Davis, L. (2000). W4: real-time surveillance of people and their activities. IEEE Trans. on Pattern Analysis and Machine Intelligence, 22 (8), pp. 809--830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Jaques</author>
</authors>
<title>Issue management and crisis management: An integrated, non-linear, relational construct</title>
<date>2007</date>
<journal>Public Relations Review</journal>
<volume>33</volume>
<pages>147--157</pages>
<contexts>
<context>egorization of such events. Automatic recognition of atypical events is of great importance for prediction, early detection and prevention of danger, emergency, crisis and other high-risk situations (Jaques, 2007). The state-of-art technology for automatic sound recognition (Couvreur, 1998; Aucouturiera, 2007) relies on the statistical comparison of the time-frequency characteristics of the unknown sounds wit</context>
</contexts>
<marker>Jaques, 2007</marker>
<rawString>Jaques, T. (2007). Issue management and crisis management: An integrated, non-linear, relational construct. Public Relations Review, 33, pp. 147--157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kipp</author>
</authors>
<title>Anvil A Generic Annotation Tool for Multimodal Dialogue</title>
<date>2001</date>
<booktitle>In Proceedings of the INTERSPEECH-2001</booktitle>
<pages>1367--1370</pages>
<contexts>
<context> the efficient annotation tags in order to allow for the quick retrieval of the sound effect. In Phases II and III, the annotation of speech and audio events is performed by employing the Anvil tool (Kipp, 2001), following the methodology of (Clavel &amp; Vasilescu, 2004). Anvil was selected for this phase, since it provides the means for annotating not only the emotions manifested in the specific segment, but </context>
</contexts>
<marker>Kipp, 2001</marker>
<rawString>Kipp, M. (2001). Anvil A Generic Annotation Tool for Multimodal Dialogue. In Proceedings of the INTERSPEECH-2001, pp. 1367--1370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Sarikaya</author>
<author>J H L Hansen</author>
</authors>
<title>High resolution speech feature parameterization for monophone-based stressed speech recognition</title>
<date>2000</date>
<journal>IEEE Signal Processing Letters</journal>
<volume>7</volume>
<pages>182--185</pages>
<contexts>
<context>hm selected features (16-GA and 48-GA) (Casalea, 2007), MPEG-7 sound recognition descriptors (Casey, 2001), Mel-frequency cepstral coefficients (Slaney, 1998), wavelet packet based audio descriptors (Sarikaya &amp; Hansen, 2000), pitch, duration, intensity, are evaluated with respect to their capability to discriminate audio events, which indicate divergence from normal situations. As a special case we study a redundant set</context>
</contexts>
<marker>Sarikaya, Hansen, 2000</marker>
<rawString>Sarikaya, R., Hansen, J.H.L. (2000). High resolution speech feature parameterization for monophone-based stressed speech recognition. IEEE Signal Processing Letters, 7(7), pp. 182--185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Slaney</author>
</authors>
<title>Auditory Toolbox. Version 2</title>
<date>1998</date>
<tech>Technical Report #1998-010</tech>
<institution>Interval Research Corporation</institution>
<contexts>
<context>-Auto-Env) features (Zhou et al. 2001), genetic algorithm selected features (16-GA and 48-GA) (Casalea, 2007), MPEG-7 sound recognition descriptors (Casey, 2001), Mel-frequency cepstral coefficients (Slaney, 1998), wavelet packet based audio descriptors (Sarikaya &amp; Hansen, 2000), pitch, duration, intensity, are evaluated with respect to their capability to discriminate audio events, which indicate divergence </context>
</contexts>
<marker>Slaney, 1998</marker>
<rawString>Slaney, M. (1998). Auditory Toolbox. Version 2. Technical Report #1998-010, Interval Research Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Yang</author>
<author>L J M Rothkrantz</author>
</authors>
<title>Emotion Sensing for Context Sensitive Interpretation of Crisis Reports</title>
<date>2007</date>
<booktitle>In ISCRAM-2007</booktitle>
<pages>507--514</pages>
<editor>B. Van de Walle, P. Burghardt and C. Nieuwenhuis eds</editor>
<contexts>
<context>rld recordings of events captured in such situations. Beside acoustic events (Coskun et al. 1999), human speech is also indicative source, which can facilitate the identification of crisis or danger (Yang &amp; Rothkrantz, 2007). It is well-known that the accuracy and performance of automatic recognition systems are largely dependent on the availability of appropriate data sources, which are utilized for training statistica</context>
</contexts>
<marker>Yang, Rothkrantz, 2007</marker>
<rawString>Yang, Z., Rothkrantz, L.J.M. (2007). Emotion Sensing for Context Sensitive Interpretation of Crisis Reports. In ISCRAM-2007, B. Van de Walle, P. Burghardt and C. Nieuwenhuis eds., pp. 507--514.</rawString>
</citation>
</citationList>
</algorithm>

