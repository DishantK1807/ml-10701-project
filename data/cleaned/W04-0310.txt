Incrementality in Syntactic Processing: Computational Models and Experimental Evidence Patrick STURT Human Communication Research Centre Department of Psychology University of Glasgow 58 Hillhead Street Glasgow, SCOTLAND patrick@psy.gla.ac.uk Abstract It is a well-known intuition that human sentence understanding works in an incremental fashion, with a seemingly constant update of the interpretation through the left-to-right processing of a string.
Such intuitions are backed up by experimental evidence dating from at least as far back as Marslen-Wilson (1973), showing that under many circumstances, interpretations are indeed updated very quickly.
From a parsing point of view it is interesting to consider the structure-building processes that might underlie incremental interpretation| what kinds of partial structures are built during sentence processing, and with what timecourse?
In this talk I will give an overview of the stateof-the-art of experimental psycholinguistic research, paying particular attention to the timecourse of structure-building.
The discussion will focus on a new line of research (some as yet unpublished) in which syntactic phenomena such as binding relations (e.g., Sturt, 2003) and unbounded dependencies (e.g., Aoshima, Phillips, & Weinberg, in press) are exploited to make a very direct test of the availability of syntactic structure over time.
The experimental research will be viewed from the perspective of a space of computational models, which make difierent predictions about time-course of structure building.
One dimension in this space is represented by the parsing algorithm used: For example, within the framework of Generalized Left Corner Parsing (Demers, 1977), algorithms can be characterized in terms of the point at which a context-free rule is recognized, in relation to the recognition-point of the symbols on its righthand side.
Another relevant dimension is represented by the type of grammar formalism that is assumed.
For example, with bottom-up parsing algorithms, the degree to which structurebuilding is delayed in right-branching structures depends heavily on whether we employ a traditional phrase-structure formalism with rigid constituency, or a cateogorial formalism with exible constituency (e.g., Steedman, 2000).
I will argue that the evidence is incompatible with models which predict systematic delays in the construction of syntactic structure.
In particular, I will argue against both head-driven strategies (e.g., Mulders, 2002), and purely bottom-up parsing strategies, even when exible constituency is employed.
Instead, I will argue that to capture the data in the most parsimonious way, we should turn our attention to those models in which a fully connected syntactic structure is maintained throughout the processing of a string.
References Aoshima, S., Phillips, C., & Weinberg, A.
(in press).
Processing flller-gap dependencies in a head-flnal language.
To appear in Journal of Memory and Language.
Demers, A.
J. (1977).
Generalized left corner parsing.
In Proceedings of the 4th acm sigact-sigplan symposium on principles of programming languages (pp.
170 182).
ACM Press.
Marslen-Wilson, W.
(1973). Linguistic structure and speech shadowing at very short latencies.
Nature, 244, 522533.
Mulders, I.
(2002). Transparent parsing: Headdriven processing of verb-flnal structures.
Utrecht: LOT.
Steedman, M.
(2000). The syntactic process.
Cambridge, MA: MIT press.
Sturt, P.
(2003). The time-course of the application of binding constraints in reference resolution.
Journal of Memory and Language, 48(3), 542562.

