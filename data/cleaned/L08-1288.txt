<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Breck Baldwin</author>
<author>Tom Morton</author>
<author>Amit Bagga</author>
<author>Jason Baldridge</author>
<author>Raman Chandraseker</author>
<author>Alexis Dimitriadis</author>
<author>Kieran Snyder</author>
<author>Magdalena Wolska</author>
</authors>
<title>Description of the UPenn camp system as used for coreference</title>
<date>1997</date>
<booktitle>In Message Understanding Conference Proceedings</booktitle>
<contexts>
<context>me other engine, for example, an Information Extraction system, we might want to have a classifier with a high precision level and therefore opt for another scoring scheme, such as the BCUBED metric (Baldwin et al., 1997). The corpus is very small and simply does not contain enough material for training (the “formal training” documents provided by MUC-7 are not annotated). Our classifiers show no signs of convergence</context>
</contexts>
<marker>Baldwin, Morton, Bagga, Baldridge, Chandraseker, Dimitriadis, Snyder, Wolska, 1997</marker>
<rawString>Breck Baldwin, Tom Morton, Amit Bagga, Jason Baldridge, Raman Chandraseker, Alexis Dimitriadis, Kieran Snyder, and Magdalena Wolska. 1997. Description of the UPenn camp system as used for coreference. In Message Understanding Conference Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catalina Barbu</author>
<author>Richard Evans</author>
<author>Ruslan Mitkov</author>
</authors>
<title>A corpus based investigation of morphological disagreement in anaphoric relations</title>
<date>2002</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference</booktitle>
<contexts>
<context>oblem, state-of-the-art coreference resolution algorithms still only have a moderate performance (around 60% F-measure for coreference chains on the MUC-7 data). Recent studies (Cristea et al., 2002; Barbu et al., 2002) claim that existent (knowledge-poor) algorithms are only able to account for “easy” coreference links and suggest more sophisticated frameworks to deal with complex anaphora resolution cases. We hav</context>
<context> error analysis shows that some coreference links are intrinsically difficult and can only be accounted for by deep analysis. These are complex anaphora cases, mentioned by Cristea et al. (2002) and (Barbu et al., 2002), including, for example, nominal anaphora or tricky 1st and 2nd person pronouns. A lot of coreference links, however, can still be potentially established by shallow algorithms. In the following sec</context>
</contexts>
<marker>Barbu, Evans, Mitkov, 2002</marker>
<rawString>Catalina Barbu, Richard Evans, and Ruslan Mitkov. 2002. A corpus based investigation of morphological disagreement in anaphoric relations. In Proceedings of the Language Resources and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics</booktitle>
<pages>132--139</pages>
<contexts>
<context>n our classifier on an already existing larger corpus (ACE). 4. Preprocessing modules We rely on external modules for segmenting MUC documents into sentences2 (Reynar and Ratnaparkhi, 1997), parsing (Charniak, 2000), NE-tagging (Curran and Clark, 2003) and determining semantic properties of our markables (Miller, 1990). The first three modules are fully automatic corpus-based NLP systems. The WordNet ontology i</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Cristea</author>
<author>Oana Postolache</author>
<author>Ruslan Mitkov</author>
</authors>
<title>Handling complex anaphora resolution cases</title>
<date>2002</date>
<booktitle>In Proceedings of the 4th Discourse Anaphora and Anaphor Resolution Colloquium</booktitle>
<contexts>
<context>ch attention to the problem, state-of-the-art coreference resolution algorithms still only have a moderate performance (around 60% F-measure for coreference chains on the MUC-7 data). Recent studies (Cristea et al., 2002; Barbu et al., 2002) claim that existent (knowledge-poor) algorithms are only able to account for “easy” coreference links and suggest more sophisticated frameworks to deal with complex anaphora reso</context>
</contexts>
<marker>Cristea, Postolache, Mitkov, 2002</marker>
<rawString>Dan Cristea, Oana Postolache, and Ruslan Mitkov. 2002. Handling complex anaphora resolution cases. In Proceedings of the 4th Discourse Anaphora and Anaphor Resolution Colloquium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
<author>Stephen Clark</author>
</authors>
<title>Language independent NER using a maximum entropy tagger</title>
<date>2003</date>
<booktitle>In Proceedings of the Seventh Conference on Natural Language Learning</booktitle>
<pages>164--167</pages>
<contexts>
<context>y existing larger corpus (ACE). 4. Preprocessing modules We rely on external modules for segmenting MUC documents into sentences2 (Reynar and Ratnaparkhi, 1997), parsing (Charniak, 2000), NE-tagging (Curran and Clark, 2003) and determining semantic properties of our markables (Miller, 1990). The first three modules are fully automatic corpus-based NLP systems. The WordNet ontology is a large manually created resource. </context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>James R. Curran and Stephen Clark. 2003. Language independent NER using a maximum entropy tagger. In Proceedings of the Seventh Conference on Natural Language Learning, pages 164–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Abe Ittycheriah</author>
<author>Hongyan Jing</author>
<author>Nanda Kambhatla</author>
<author>Salim Roukos</author>
</authors>
<title>A mentionsynchronous coreference resolution algorithm based on the bell tree</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics</booktitle>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda Kambhatla, and Salim Roukos. 2004. A mentionsynchronous coreference resolution algorithm based on the bell tree. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
</authors>
<title>Wordnet: An on-line lexical database</title>
<date>1990</date>
<journal>International Journal of Lexicography</journal>
<contexts>
<context>odules for segmenting MUC documents into sentences2 (Reynar and Ratnaparkhi, 1997), parsing (Charniak, 2000), NE-tagging (Curran and Clark, 2003) and determining semantic properties of our markables (Miller, 1990). The first three modules are fully automatic corpus-based NLP systems. The WordNet ontology is a large manually created resource. All the modules have some shortages that may decrease the performanc</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>George Miller. 1990. Wordnet: An on-line lexical database. International Journal of Lexicography.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>104--111</pages>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Olga Uryupina</author>
<author>Renata Vieira</author>
<author>Mijail Alexandrov-Kabadjov</author>
<author>Rodrigo Goulart</author>
</authors>
<title>Discourse-new detectors for definite description resolution: a survey and preliminary proposal</title>
<date>2004</date>
<booktitle>In Proceedings of the Reference Resolution Workshop at ACL’04</booktitle>
<contexts>
<context> such cases we need a deeper analysis, involving multiple linguistic factors. At least some spurious links between same-head noun phrases can be eliminated by discarding discourse-new markables (see (Poesio et al., 2004) for an overview of relevant algorithms). If a candidate anaphor is likely to be a discourse new entity, the link is highly implausible: If you have a ship that can fire Tomahawk missiles, and fire a</context>
</contexts>
<marker>Poesio, Uryupina, Vieira, Alexandrov-Kabadjov, Goulart, 2004</marker>
<rawString>Massimo Poesio, Olga Uryupina, Renata Vieira, Mijail Alexandrov-Kabadjov, and Rodrigo Goulart. 2004. Discourse-new detectors for definite description resolution: a survey and preliminary proposal. In Proceedings of the Reference Resolution Workshop at ACL’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing</booktitle>
<contexts>
<context>ep in this direction, we plan to re-train our classifier on an already existing larger corpus (ACE). 4. Preprocessing modules We rely on external modules for segmenting MUC documents into sentences2 (Reynar and Ratnaparkhi, 1997), parsing (Charniak, 2000), NE-tagging (Curran and Clark, 2003) and determining semantic properties of our markables (Miller, 1990). The first three modules are fully automatic corpus-based NLP syste</context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proceedings of the Fifth Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases</title>
<date>2001</date>
<journal>Computational Linguistics (Special Issue on Computational Anaphora Resolution</journal>
<volume>27</volume>
<contexts>
<context>rformance for a variety of publicly available machine learners (SVMa0a2a1a4a3a6a5a8a7 , C4.5, Ripper, Slipper, MaxEnt), observing a consistent significant improvement over the state-of-the-art level (Soon et al., 2001). The system’s performance with the SVMa0a2a1a4a3a6a5a8a7 classifier (F-score of 65.4%) is, to our knowledge, the best result on the MUC-7 data reported so far in the literature. Moreover, our learni</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics (Special Issue on Computational Anaphora Resolution), 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Uryupina</author>
</authors>
<title>Coreference resolution with and without linguistic knowledge</title>
<date>2006</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference</booktitle>
<contexts>
<context>ggest more sophisticated frameworks to deal with complex anaphora resolution cases. We have built a learning-based coreference resolution engine incorporating various kinds of linguistic information (Uryupina, 2006; Uryupina, 2007). Our system relies on 351 nominal features (1096 boolean/continuous), representing surface (122 features), syntactic (64), semantic (29), and salience-based (136) properties of marka</context>
</contexts>
<marker>Uryupina, 2006</marker>
<rawString>Olga Uryupina. 2006. Coreference resolution with and without linguistic knowledge. In Proceedings of the Language Resources and Evaluation Conference.</rawString>
</citation>
</citationList>
</algorithm>

