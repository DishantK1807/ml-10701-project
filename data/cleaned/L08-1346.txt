<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>R Mitkov D Blagoev</author>
<author>A Mulloni</author>
</authors>
<title>Finding translations for low-frequency words in comparable corpora</title>
<date>2007</date>
<booktitle>In In Sixth International and Interdisciplinary Conference on Modeling and Using Context</booktitle>
<marker>Blagoev, Mulloni, 2007</marker>
<rawString>V. Pekar. R. Mitkov. D. Blagoev. and A. Mulloni. 2007. Finding translations for low-frequency words in comparable corpora. In In Sixth International and Interdisciplinary Conference on Modeling and Using Context.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database (Language, Speech, and Communication</title>
<date>1998</date>
<publisher>The MIT Press</publisher>
<contexts>
<context>quency list of the nouns in the British National Corpus. They use three measures for detecting the substitutions; an oddity measure compared to data found on th Web, a semantic measure using WordNet (Fellbaum, 1998), and finally a measure that counts the frequency of the left and right bigrams around the target word. Word obfuscation detection is one of the many natural langugae processing tasks that can benefi</context>
<context>be regarded as a consistent element of that context. In this vein, (Hirst and St-Onge, 1997) demonstrates the construction of chains from WordNet defines a coherence relation with respect to WordNet (Fellbaum, 1998) to detect spelling errors. While such approaches arguably provide much better insight into the linguistic processing capabilities of humans, as a practical approach they are handicapped by relying o</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum. 1998. WordNet: An Electronic Lexical Database (Language, Speech, and Communication). The MIT Press, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S W Fong</author>
<author>D B Skillcorn</author>
</authors>
<title>Detecting word substitution in adversarial communication. Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava</title>
<date>2006</date>
<contexts>
<context> judge if a word is used out of its context. Section 5. describes our experimental setup and our results. Finally section 6. ends with some concluding remarks and our future direction. 2. Background (Fong and Skillcorn, 2006) address a very similar problem of detecting word substitution in intercepted communication. One of the major challenges in this problem is using appropriate data for evaluation. Since the problem is</context>
</contexts>
<marker>Fong, Skillcorn, 2006</marker>
<rawString>S. W. Fong and D. B. Skillcorn. 2006. Detecting word substitution in adversarial communication. Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.</rawString>
</citation>
<citation valid="true">
<title>Fbk-irst: Lexical substitution task exploiting domain and syntagmatic coherence</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007). Association for Computational Linguistics</booktitle>
<marker>2007</marker>
<rawString>2007. Fbk-irst: Lexical substitution task exploiting domain and syntagmatic coherence. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007). Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfio Massimilliano Gliozzo</author>
</authors>
<title>Semantic Domains in Computational Linguistics</title>
<date>2005</date>
<tech>Ph.D. thesis</tech>
<contexts>
<context>rd space, representing their context in the same space more of a challenge. For example, (Sch¨utze, 1998) defines “context vectors” to be the centroid of the vectors of content words in the context. (Gliozzo, 2005) represents both words and contexts in a domain space, and uses domain vectors to represent words and contexts. 3. A Probabilistic model of context In this work, we choose to model contexts probabili</context>
</contexts>
<marker>Gliozzo, 2005</marker>
<rawString>Alfio Massimilliano Gliozzo. 2005. Semantic Domains in Computational Linguistics. Ph.D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
<author>D St-Onge</author>
</authors>
<title>Lexical chains as representation of context for the detection and correction malapropisms</title>
<date>1997</date>
<contexts>
<context>hains: if a word can be linked to the chain defined by its context, then it contributes to the coherence of the text, and hence can be regarded as a consistent element of that context. In this vein, (Hirst and St-Onge, 1997) demonstrates the construction of chains from WordNet defines a coherence relation with respect to WordNet (Fellbaum, 1998) to detect spelling errors. While such approaches arguably provide much bett</context>
</contexts>
<marker>Hirst, St-Onge, 1997</marker>
<rawString>G. Hirst and D. St-Onge. 1997. Lexical chains as representation of context for the detection and correction malapropisms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Jarmasz</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Roget’s thesaurus and semantic similarity</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP-03</booktitle>
<pages>212--219</pages>
<contexts>
<context>ring their distance in this space. These measures of distance rely either on hand–crafted semantic networks (e.g. Wordnet, Roget’s thesaurus , etc.) (Slator, 1989; Leacock et al., 1998; Resnik, 1999; Jarmasz and Szpakowicz, 2003) or on information found in corpora (Lee, 1997; Lee and Pereira, 1999; Lee, 1999; Blagoev. and Mulloni, 2007). Despite the accuracy and accessibility of information in hand–crafted resources, there a</context>
</contexts>
<marker>Jarmasz, Szpakowicz, 2003</marker>
<rawString>Mario Jarmasz and Stan Szpakowicz. 2003. Roget’s thesaurus and semantic similarity. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP-03), pages 212–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Foltz P W Landauer T K</author>
<author>D Laham</author>
</authors>
<title>Introduction to latent semantic analysis</title>
<date>1998</date>
<booktitle>Discourse Processes, 25:259–284. Claudia</booktitle>
<marker>K, Laham, 1998</marker>
<rawString>Foltz P. W. Landauer T. K. and Laham D. 1998. Introduction to latent semantic analysis. Discourse Processes, 25:259–284. Claudia Leacock, George A. Miller, and Martin Chodorow.</rawString>
</citation>
<citation valid="true">
<title>Using corpus statistics and wordnet relations for sense identification</title>
<date>1998</date>
<journal>Comput. Linguist</journal>
<volume>24</volume>
<marker>1998</marker>
<rawString>1998. Using corpus statistics and wordnet relations for sense identification. Comput. Linguist., 24(1):147–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
<author>Fernando Pereira</author>
</authors>
<title>Distributional similarity models: clustering vs. nearest neighbors</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</booktitle>
<pages>33--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics</institution>
<contexts>
<context>hand–crafted semantic networks (e.g. Wordnet, Roget’s thesaurus , etc.) (Slator, 1989; Leacock et al., 1998; Resnik, 1999; Jarmasz and Szpakowicz, 2003) or on information found in corpora (Lee, 1997; Lee and Pereira, 1999; Lee, 1999; Blagoev. and Mulloni, 2007). Despite the accuracy and accessibility of information in hand–crafted resources, there are many concerns with the coverage of the resources, and any distance </context>
</contexts>
<marker>Lee, Pereira, 1999</marker>
<rawString>Lillian Lee and Fernando Pereira. 1999. Distributional similarity models: clustering vs. nearest neighbors. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 33–40. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Jane Lee</author>
</authors>
<title>Similarity-based approaches to natural language processing</title>
<date>1997</date>
<booktitle>Ph.D. thesis</booktitle>
<location>Cambridge, MA, USA</location>
<contexts>
<context> either on hand–crafted semantic networks (e.g. Wordnet, Roget’s thesaurus , etc.) (Slator, 1989; Leacock et al., 1998; Resnik, 1999; Jarmasz and Szpakowicz, 2003) or on information found in corpora (Lee, 1997; Lee and Pereira, 1999; Lee, 1999; Blagoev. and Mulloni, 2007). Despite the accuracy and accessibility of information in hand–crafted resources, there are many concerns with the coverage of the resou</context>
</contexts>
<marker>Lee, 1997</marker>
<rawString>Lillian Jane Lee. 1997. Similarity-based approaches to natural language processing. Ph.D. thesis, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</booktitle>
<pages>25--32</pages>
<contexts>
<context>etworks (e.g. Wordnet, Roget’s thesaurus , etc.) (Slator, 1989; Leacock et al., 1998; Resnik, 1999; Jarmasz and Szpakowicz, 2003) or on information found in corpora (Lee, 1997; Lee and Pereira, 1999; Lee, 1999; Blagoev. and Mulloni, 2007). Despite the accuracy and accessibility of information in hand–crafted resources, there are many concerns with the coverage of the resources, and any distance measure def</context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lillian Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>Semeval2007 task 10: English lexical substitution task</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007). Association for Computational Linguistics</booktitle>
<contexts>
<context>t, models of context can be used in all tasks that are designed to evaluate the word sense disambiguation systems. For instance for the Lexical Substitution task which was introduced in Semeval 2007 (McCarthy and Navigli, 2007), systems were asked to find a set of words that could be substituted with a target word in its given context (Giuliano et al., 2007; Zhao et al., 2007; Yuret, 2007). The approaches to this problem c</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2007. Semeval2007 task 10: English lexical substitution task. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007). Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Mirella Lapata</author>
</authors>
<title>Dependencybased construction of semantic space models</title>
<date>2007</date>
<journal>Comput. Linguist</journal>
<volume>33</volume>
<marker>Pad´o, Lapata, 2007</marker>
<rawString>Sebastian Pad´o and Mirella Lapata. 2007. Dependencybased construction of semantic space models. Comput. Linguist., 33(2):161–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Procter</author>
</authors>
<title>Longman’s Dictionary of Contemporary English</title>
<date>1978</date>
<publisher>Longman Group Limited</publisher>
<marker>Procter, 1978</marker>
<rawString>Paul Procter. 1978. Longman’s Dictionary of Contemporary English. Longman Group Limited.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
</authors>
<title>The generative lexicon</title>
<date>1991</date>
<journal>Comput. Linguist</journal>
<volume>17</volume>
<contexts>
<context>this problem could be divided to two groups. The first method assumes that if a word is used in context, it is formally and logically consistent with its context. For instance the Generative Lexicon (Pustejovsky, 1991) defines a coherent context by propagating type constraints and type coercions through entries in a lexicon, and thus allowed words are those which satisfy the constraints defined by the surrounding </context>
</contexts>
<marker>Pustejovsky, 1991</marker>
<rawString>James Pustejovsky. 1991. The generative lexicon. Comput. Linguist., 17(4):409–441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research</journal>
<pages>11--95</pages>
<contexts>
<context>ished by measuring their distance in this space. These measures of distance rely either on hand–crafted semantic networks (e.g. Wordnet, Roget’s thesaurus , etc.) (Slator, 1989; Leacock et al., 1998; Resnik, 1999; Jarmasz and Szpakowicz, 2003) or on information found in corpora (Lee, 1997; Lee and Pereira, 1999; Lee, 1999; Blagoev. and Mulloni, 2007). Despite the accuracy and accessibility of information in h</context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik. 1999. Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language. Journal of Artificial Intelligence Research, 11:95–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Part of speech filtered word spaces</title>
<date>2007</date>
<booktitle>In In Sixth International and Interdisciplinary Conference on Modeling and Using Context</booktitle>
<marker>Sch¨utze, 2007</marker>
<rawString>K. Rothenhusler. and H. Sch¨utze. 2007. Part of speech filtered word spaces. In In Sixth International and Interdisciplinary Conference on Modeling and Using Context.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model: using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces</title>
<date>2006</date>
<tech>Ph.D. thesis</tech>
<contexts>
<context>-occurrence) matrices, and the set of words selected as the features of this space and what sort of information is countable for a given dimension are the major distinguishing factors in such models (Sahlgren, 2006). For instance, some models consider unconditional co-occurrence of words, while other models consider co-occurrences within certain grammatical relationships (Pad´o and Lapata, 2007; Rothenhusler. a</context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model: using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces. Ph.D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination</title>
<date>1998</date>
<journal>Comput. Linguist</journal>
<volume>24</volume>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Comput. Linguist., 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B M Slator</author>
</authors>
<title>Extracting lexical knowledge from dictionary text</title>
<date>1989</date>
<journal>SIGART Bull</journal>
<contexts>
<context>heir consistency can then be accomplished by measuring their distance in this space. These measures of distance rely either on hand–crafted semantic networks (e.g. Wordnet, Roget’s thesaurus , etc.) (Slator, 1989; Leacock et al., 1998; Resnik, 1999; Jarmasz and Szpakowicz, 2003) or on information found in corpora (Lee, 1997; Lee and Pereira, 1999; Lee, 1999; Blagoev. and Mulloni, 2007). Despite the accuracy a</context>
</contexts>
<marker>Slator, 1989</marker>
<rawString>B. M. Slator. 1989. Extracting lexical knowledge from dictionary text. SIGART Bull., (108):173–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
</authors>
<title>Ku: Word sense disambiguation by substitution</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007</booktitle>
<location>Prague, Czech Republic</location>
<contexts>
<context>ed in Semeval 2007 (McCarthy and Navigli, 2007), systems were asked to find a set of words that could be substituted with a target word in its given context (Giuliano et al., 2007; Zhao et al., 2007; Yuret, 2007). The approaches to this problem could be divided to two groups. The first method assumes that if a word is used in context, it is formally and logically consistent with its context. For instance the</context>
</contexts>
<marker>Yuret, 2007</marker>
<rawString>Deniz Yuret. 2007. Ku: Word sense disambiguation by substitution. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007), Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Shiqi Zhao</author>
<author>Lin Zhao</author>
<author>Yu Zhang</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<marker>Zhao, Zhao, Zhang, Liu, Li, </marker>
<rawString>Shiqi Zhao, Lin Zhao, Yu Zhang, Ting Liu, and Sheng Li.</rawString>
</citation>
</citationList>
</algorithm>

