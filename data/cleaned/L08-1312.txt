<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>W Byrne</author>
<author>D Doermann</author>
<author>M Franz</author>
<author>S Gustman</author>
<author>J Hajic</author>
<author>D Oard</author>
<author>M Picheny</author>
<author>J Psutka</author>
<author>B Ramabhadran</author>
<author>D Soergel</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>Automatic Recognition of Spontaneous Speech for Access to Multilingual Oral History Archives</title>
<date>2004</date>
<journal>IEEE Transactions on Speech and Audio Processing</journal>
<volume>12</volume>
<pages>420--435</pages>
<contexts>
<context>o the disclosure of spoken word archives has been made many times, see (Goldman et al., 2005), and several initiatives have been undertaken to develop this technology for such audio collections, see (Byrne et al., 2004; Hansen et al., 2005). Still there are only few examples of online access to spoken audio from the CH domain. For retrieval of spoken documents we aim to employ a combination of information retrieval</context>
<context>ollections that lie in the range of 30-60% on e.g., a collection of historic speeches, news broadcasts and debates (Hansen et al., 2005), and the MALACH collection of interviews with Shoah survivors (Byrne et al. 2004). In a retrieval context, however, word error rate is a flawed optimization criterion because (i) it is only defined on a (literal) transcription and cannot be calculated on other types of speech rec</context>
</contexts>
<marker>Byrne, Doermann, Franz, Gustman, Hajic, Oard, Picheny, Psutka, Ramabhadran, Soergel, Ward, Zhu, 2004</marker>
<rawString>Byrne, W., Doermann, D., Franz, M., Gustman, S., Hajic, J., Oard, D., Picheny, M., Psutka, J., Ramabhadran, B., Soergel, D., Ward, T., and Zhu, W-J. (2004). Automatic Recognition of Spontaneous Speech for Access to Multilingual Oral History Archives. IEEE Transactions on Speech and Audio Processing, 12(4), pp. 420--435.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M G Christel</author>
<author>J Richardson</author>
<author>H D Wactlar</author>
</authors>
<title>Facilitating access to large digital oral history archives through Informedia technologies</title>
<date>2006</date>
<booktitle>In Proceedings of JCDL '06</booktitle>
<pages>194--195</pages>
<contexts>
<context>ds on the type and amount of metadata that are already available for a collection. If manual transcripts or elaborate summaries are available, alignment of text and speech can be employed, see e.g., (Christel et al., 2006; Munteanu et al., 2006). If transcripts are not available, a textual representation can be generated through automatic speech recognition, see e.g., (Goldman et al., 2005, Hansen et al., 2005). At th</context>
</contexts>
<marker>Christel, Richardson, Wactlar, 2006</marker>
<rawString>Christel, M.G., Richardson J., and Wactlar, H.D. (2006). Facilitating access to large digital oral history archives through Informedia technologies. In Proceedings of JCDL '06, pp. 194--195.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J S Garofolo</author>
<author>C G P Auzanne</author>
<author>E M Voorhees</author>
</authors>
<marker>Garofolo, Auzanne, Voorhees, </marker>
<rawString>Garofolo, J.S., Auzanne, C.G.P., and Voorhees, E.M.</rawString>
</citation>
<citation valid="true">
<title>The TREC Spoken Document Retrieval Track: A success story</title>
<date>2000</date>
<booktitle>In Proceedings of RIAO</booktitle>
<pages>1--20</pages>
<marker>2000</marker>
<rawString>(2000). The TREC Spoken Document Retrieval Track: A success story. In Proceedings of RIAO 2000, pp. 1--20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Renals Goldman</author>
<author>S Bird</author>
<author>S de Jong</author>
<author>F Federico</author>
<author>M Fleischhauer</author>
<author>C Kornbluh</author>
<author>M Lamel</author>
<author>L Oard</author>
<author>D Stewart</author>
<author>C</author>
<author>R Wright</author>
</authors>
<title>Accessing the Spoken Word</title>
<date>2005</date>
<journal>International Journal on Digital Libraries</journal>
<volume>5</volume>
<pages>287--298</pages>
<marker>Goldman, Bird, de Jong, Federico, Fleischhauer, Kornbluh, Lamel, Oard, Stewart, C, Wright, 2005</marker>
<rawString>Goldman, J. Renals, S., Bird, S., de Jong, F., Federico, M., Fleischhauer, C., Kornbluh, M., Lamel, L., Oard, D., Stewart, C., and Wright, R. (2005). Accessing the Spoken Word. International Journal on Digital Libraries, 5(4), pp. 287--298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H L Hansen</author>
<author>R Huang</author>
<author>B Zhou</author>
<author>M Deadle</author>
<author>J R Deller</author>
<author>A R Gurijala</author>
<author>M Kurimo</author>
<author>P Angkititrakul</author>
</authors>
<title>SpeechFind: Advances in spoken document retrieval for a National Gallery of the Spoken Word</title>
<date>2005</date>
<journal>IEEE Transactions on Speech and Audio Processing</journal>
<volume>13</volume>
<pages>712--730</pages>
<contexts>
<context>spoken word archives has been made many times, see (Goldman et al., 2005), and several initiatives have been undertaken to develop this technology for such audio collections, see (Byrne et al., 2004; Hansen et al., 2005). Still there are only few examples of online access to spoken audio from the CH domain. For retrieval of spoken documents we aim to employ a combination of information retrieval and word spotting te</context>
<context>.g., (Christel et al., 2006; Munteanu et al., 2006). If transcripts are not available, a textual representation can be generated through automatic speech recognition, see e.g., (Goldman et al., 2005, Hansen et al., 2005). At the University of Twente (UT) both methods have been applied to Dutch audiovisual collections, and the results are presented in the following subsections. 2.1 Quality of Automatically Generated </context>
<context> the cultural heritage domain are somewhat lower than results reported for English collections that lie in the range of 30-60% on e.g., a collection of historic speeches, news broadcasts and debates (Hansen et al., 2005), and the MALACH collection of interviews with Shoah survivors (Byrne et al. 2004). In a retrieval context, however, word error rate is a flawed optimization criterion because (i) it is only defined </context>
</contexts>
<marker>Hansen, Huang, Zhou, Deadle, Deller, Gurijala, Kurimo, Angkititrakul, 2005</marker>
<rawString>Hansen, J.H.L., Huang, R., Zhou, B., Deadle, M., Deller, J.R., Gurijala, A.R., Kurimo, M., and Angkititrakul, P. (2005). SpeechFind: Advances in spoken document retrieval for a National Gallery of the Spoken Word. IEEE Transactions on Speech and Audio Processing, 13(5), pp. 712--730.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Huijbregts</author>
<author>R Ordelman</author>
<author>F A de Jong</author>
</authors>
<title>Spoken Document Retrieval Application in the Oral History Domain</title>
<date>2005</date>
<booktitle>in Proceedings of 10th SPECOM</booktitle>
<pages>699--702</pages>
<marker>Huijbregts, Ordelman, de Jong, 2005</marker>
<rawString>Huijbregts, M., Ordelman, R., and de Jong, F. A. (2005). Spoken Document Retrieval Application in the Oral History Domain, in Proceedings of 10th SPECOM, pp. 699--702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Huijbregts</author>
<author>R Ordelman</author>
<author>F de Jong</author>
</authors>
<date>2007</date>
<marker>Huijbregts, Ordelman, de Jong, 2007</marker>
<rawString>Huijbregts, M., Ordelman, R., and de Jong, F. (2007a).</rawString>
</citation>
<citation valid="true">
<title>Annotation of Heterogeneous multimedia content using automatic speech recognition</title>
<date>2007</date>
<booktitle>In Proceedings of SAMT</booktitle>
<pages>78--90</pages>
<marker>2007</marker>
<rawString>Annotation of Heterogeneous multimedia content using automatic speech recognition. In Proceedings of SAMT 2007, pp. 78--90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Huijbregts</author>
<author>C Wooters</author>
<author>R Ordelman</author>
</authors>
<title>Filtering the Unknown: Speech Activity Detection in Heterogeneous Video Collections</title>
<date>2007</date>
<booktitle>In Proceedings of Interspeech</booktitle>
<pages>2925--2928</pages>
<contexts>
<context> Twente, a system has been developed for automatically generating transcripts for Dutch broadcast news recordings. This system typically generates textual transcripts with an error rate of 20 to 30% (Huijbregts et al., 2007a) and these transcripts are of high enough quality to be applied in SDR applications. As a comparison, error rates reported for English broadcast news lie in the range of 10-20% (see e.g., Pallett et</context>
<context> data set contains speech and audible non-speech from various unknown sources. Without optimization, the error rate of speech activity detection was 18.3%, but it reduced to 11.4% after optimization (Huijbregts et al., 2007b). Another important pre-processing step is speaker clustering or speaker diarization, see e.g., (Tranter and Reynolds, 2006). Speaker diarization is the process of automatically clustering all speec</context>
<context>formance, e.g., Vocal Tract Length Normalization. On the TRECVID 2007 collection this technique improved the transcripts with 4.4% word error rate, resulting in an optimized word error rate of 64.0% (Huijbregts et al., 2007a). Note, however, that the word error rate can easily exceed 100% due to its definition, i.e. the sum of the numbers of substitutions, insertions and deletions divided by the number of words in the r</context>
</contexts>
<marker>Huijbregts, Wooters, Ordelman, 2007</marker>
<rawString>Huijbregts, M., Wooters, C. and Ordelman, R. (2007b) Filtering the Unknown: Speech Activity Detection in Heterogeneous Video Collections. In Proceedings of Interspeech 2007, pp. 2925--2928.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kalyuga</author>
<author>P Chandler</author>
<author>J Sweller</author>
</authors>
<title>Managing split-attention and redundancy in multimedia instruction</title>
<date>1999</date>
<journal>Applied Cognitive Psychology</journal>
<volume>13</volume>
<pages>351--371</pages>
<contexts>
<context>lp their short term memory of the content. This was probably due to the fact that cognitive load was increased by presenting the spoken words both visually and auditorily at the same time, see e.g., (Kalyuga et al., 1999). In line with expectations we found that content navigation was supported by visual information on the exact positions of relevant words. If available, this information should therefore always be pr</context>
</contexts>
<marker>Kalyuga, Chandler, Sweller, 1999</marker>
<rawString>Kalyuga, S, Chandler, P., and Sweller, J. (1999) Managing split-attention and redundancy in multimedia instruction, Applied Cognitive Psychology, 13, pp. 351--371.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Klijn</author>
<author>Y de Lusenet</author>
</authors>
<title>Tracking the reel world. A survey of audiovisual collections in Europe. Amsterdam: European Commission on Preservation and Access</title>
<date>2008</date>
<marker>Klijn, de Lusenet, 2008</marker>
<rawString>Klijn, E. and de Lusenet, Y. (2008). Tracking the reel world. A survey of audiovisual collections in Europe. Amsterdam: European Commission on Preservation and Access.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Munteanu</author>
<author>R Baecker</author>
<author>G Penn</author>
<author>E Toms</author>
<author>D James</author>
</authors>
<title>The effect of speech recognition accuracy rates on the usefulness and usability of webcast archives</title>
<date>2006</date>
<booktitle>In Proceedings of CHI</booktitle>
<pages>493--502</pages>
<contexts>
<context>nt of metadata that are already available for a collection. If manual transcripts or elaborate summaries are available, alignment of text and speech can be employed, see e.g., (Christel et al., 2006; Munteanu et al., 2006). If transcripts are not available, a textual representation can be generated through automatic speech recognition, see e.g., (Goldman et al., 2005, Hansen et al., 2005). At the University of Twente </context>
<context>ons of speech browsers that exploit time-stamped metadata have shown that users only benefit from on-screen transcripts when these are of high-quality, i.e. contain only few recognition errors (e.g., Munteanu et al., 2006), and that visual feedback on the positions of relevant sections in the audio is helpful (e.g., Whittaker et al., 1999). The functionality of showing query term locations for easier navigation, and o</context>
</contexts>
<marker>Munteanu, Baecker, Penn, Toms, James, 2006</marker>
<rawString>Munteanu, C., Baecker, R., Penn, G., Toms, E., and James, D. (2006). The effect of speech recognition accuracy rates on the usefulness and usability of webcast archives. In Proceedings of CHI 2006, pp. 493--502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pallett</author>
<author>J Fiscus</author>
<author>J Garofolo</author>
<author>A Martin</author>
<author>Mark Przybocki</author>
</authors>
<title>Broadcast News Benchmark Test Results: English and Non-English Word Error Rate Performance Measures</title>
<date>1998</date>
<booktitle>In Proceedings of 1999 DARPA Broadcast News Workshop</booktitle>
<contexts>
<context> al., 2007a) and these transcripts are of high enough quality to be applied in SDR applications. As a comparison, error rates reported for English broadcast news lie in the range of 10-20% (see e.g., Pallett et al., 1998). Obtaining high quality, automatically generated transcripts for types of data other than broadcast news is not easy. For example, automatically generated transcripts were created for a number of le</context>
</contexts>
<marker>Pallett, Fiscus, Garofolo, Martin, Przybocki, 1998</marker>
<rawString>Pallett, D., Fiscus, J., Garofolo, J., Martin, A., and Mark Przybocki. (1998). 1998 Broadcast News Benchmark Test Results: English and Non-English Word Error Rate Performance Measures. In Proceedings of 1999 DARPA Broadcast News Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sparck Jones</author>
<author>K Walker</author>
<author>S</author>
<author>S Robertson</author>
</authors>
<title>A probabilistic model of information retrieval: development and comparative experiments (part 1 and 2). Information Processing</title>
<date>2000</date>
<journal>Management</journal>
<volume>36</volume>
<pages>779--840</pages>
<contexts>
<context>ch recognition in an SDR context. Instead of simply counting the number of errors, each error is weighted using the same weight that is also used for calculating the rank of documents, tf.idf (Sparck Jones et al., 2000). Since users are expected to enter query terms that are discriminative towards the documents they are seeking and term weights are optimized towards ranking documents for expected relevance, the use</context>
</contexts>
<marker>Jones, Walker, S, Robertson, 2000</marker>
<rawString>Sparck Jones, K., Walker, S. and Robertson, S. (2000). A probabilistic model of information retrieval: development and comparative experiments (part 1 and 2). Information Processing &amp; Management, 36(6), pp. 779--840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tranter</author>
<author>D Reynolds</author>
</authors>
<title>An overview of automatic diarization systems</title>
<date>2006</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing</journal>
<volume>14</volume>
<pages>1557--1565</pages>
<contexts>
<context>ech activity detection was 18.3%, but it reduced to 11.4% after optimization (Huijbregts et al., 2007b). Another important pre-processing step is speaker clustering or speaker diarization, see e.g., (Tranter and Reynolds, 2006). Speaker diarization is the process of automatically clustering all speech fragments of each individual speaker in a recording. Once it is known which speech fragments are pronounced by a particular</context>
</contexts>
<marker>Tranter, Reynolds, 2006</marker>
<rawString>Tranter, S., and Reynolds, D. (2006). An overview of automatic diarization systems. IEEE Transactions on Audio, Speech and Language Processing, 14(5), pp. 1557--1565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Van der Werff</author>
<author>W Heeren</author>
<author>R Ordelman</author>
<author>F De Jong</author>
</authors>
<title>Radio Oranje: Enhanced access to a historical spoken word collection</title>
<date>2007</date>
<booktitle>In Proceedings of. CLIN</booktitle>
<pages>207--218</pages>
<marker>Van der Werff, Heeren, Ordelman, De Jong, 2007</marker>
<rawString>Van der Werff, L., Heeren, W., Ordelman, R, and De Jong, F. (2007). Radio Oranje: Enhanced access to a historical spoken word collection. In Proceedings of. CLIN 2007, pp. 207--218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Van der Werff</author>
<author>W Heeren</author>
</authors>
<title>Evaluating ASR Output for Information Retrieval</title>
<date>2007</date>
<booktitle>In Proceedings of ACM SIGIR SSCS Workshop</booktitle>
<pages>7--14</pages>
<marker>Van der Werff, Heeren, 2007</marker>
<rawString>Van der Werff, L., and Heeren, W. (2007). Evaluating ASR Output for Information Retrieval. In Proceedings of ACM SIGIR SSCS Workshop, pp. 7--14.</rawString>
</citation>
</citationList>
</algorithm>

