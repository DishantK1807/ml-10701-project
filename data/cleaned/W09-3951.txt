Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 341–348,
Queen Mary University of London, September 2009. c©2009 Association for Computational Linguistics
Unsupervised Classification of Dialogue Acts using a Dirichlet Process
Mixture Model
Nigel Crook, Ramon Granell, and Stephen Pulman
OxfordUniversityComputingLaboratory
WolfsonBuilding
ParksRoad,OXFORD,UK
nigc@comlab.ox.ac.uk
ramg@comlab.ox.ac.uk
sgp@clg.ox.ac.uk
Abstract
In recent years Dialogue Acts have be-
come a popular means of modelling the
communicative intentions of human and
machine utterances in many modern di-
alogue systems. Many of these systems
relyheavilyontheavailabilityofdialogue
corporathathavebeenannotatedwithDi-
alogue Act labels. The manual annota-
tion of dialogue corpora is both tedious
and expensive. Consequently, there is a
growing interest in unsupervised systems
thatarecapableofautomatingtheannota-
tion process. This paper investigates the
use of a Dirichlet Process Mixture Model
as a means of clustering dialogue utter-
ances in an unsupervised manner. These
clusters can then be analysed in terms of
thepossibleDialogueActsthattheymight
represent. The results presented here are
from the application of the Dirichlet Pro-
cessMixtureModeltotheDihanacorpus.
1 Introduction
Dialogue Acts (DAs) are an important contribu-
tion from discourse theory to the design of di-
alogue systems. These linguistics abstractions
are based on the illocutionary force of speech
acts (Austin, 1962) and try to capture and model
the communicative intention of human or ma-
chine utterances. In recent years, several dia-
logue systems have made use of DAs for mod-
ellingdiscoursephenomenaineithertheDialogue
Manager (Keizer et al., 2008), Automatic Speech
Recogniser (Stolcke et al., 2000) or the Auto-
matic Speech Synthesiser (Zovato and Romportl,
2008). Additionally, they have been used also in
othertaskssuchassummarisation,(Murrayetal.,
2006). Therefore,acorrectDAclassificationofdi-
alogueturnscanbringbenefitstotheperformance
ofthesemodulesandtasks.
Many machine learning approaches have been
used to automatically label DAs. They are usu-
ally based on Supervised Learning techniques
involving combinations of Ngrams and Hidden
Markov Models (Stolcke et al., 2000; Mart´ınez-
Hinarejosetal.,2008),NeuralNetworks(Garfield
and Wermter, 2006) or Graphical Models (Ji and
Bilmes, 2005). Relatively few approaches to DA
classification have been based on unsupervised
learning methods. Some promising results were
reported by Anderach et al (Andernach et al.,
1997; Andernach, 1996) who applied Kohonen
Self Organising Maps (SOMs) to the problem of
DAclassification. AlthoughtheSOMis nonpara-
metric in the sense that it doesn’t require that the
numberofclusterstobefoundinthedatabeapa-
rameter of the SOM that is specified before clus-
teringbegins,it’scapacitytodetectclustersislim-
itedtothesizeofthetwo-dimensionallatticeonto
which the clusters are projected, and the size of
this lattice is determined prior to clustering. This
paperinvestigatestheuseofanunsupervised,non-
parametric Bayesian approach to automatic DA
labelling: namely the Dirichlet Process Mixture
Model(DPMM).Specifically,thepaperreportsre-
sults from applying the Chinese Restaurant Pro-
cess (CRP), a popular approach to DPMMs, to
the automatic labelling of DAs in the Dihana cor-
pus. TheDihanacorpus(J.M.Bened´ıetal., 2006)
has previously been used for the same task but
with a supervised learning approach (Mart´ınez-
Hinarejos et al., 2008). The results reported here
indicate that, treating each utterance as a bag of
words, the CRP is capable of automatically clus-
341
tering most utterances according to speaker, level
1 and in some cases level 2 DA annotations (see
below).
2 The
Dihana corpus
The Dihana corpus consists of human-computer
spoken dialogues in Spanish about queuing infor-
mation of train fares and timetables. The acquisi-
tionwasperformedusingtheWizardofOz(WoZ)
technique, where a human simulates the system
following a prefixed strategy. User and system
utterances are different in nature, user utterances
are completely spontaneous speech whereas sys-
tem utterances are based on pre-written patterns
that the WoZ selected according to what the user
saidinthepreviousturn,thecurrentdialoguestate
and the WoZ strategy. There is a total of 900 dia-
logueswithavocabularyof823words. However,
afterapplyingaprocessofnameentityrecognition
(cities,times,number,...) andmakingthedistinc-
tionbetweensystemanduserwordsthereare964
different words. The same process of name en-
tityrecognitionwasalsousedbyMartinezHinare-
jos(Mart´ınez-Hinarejosetal.,2008)
2.1 Annotation
scheme
Dialogues were manually annotated using a dia-
logue act annotation scheme based on three lev-
els (see Table 1). The first level corresponds to
the general intention of the speaker (speech act),
the second level represents the implicit informa-
tionthatisreferredtointhefirstlevelandthethird
levelisthespecificdataprovidedintheutterance.
Using these three levels and making the distinc-
tionbetweenuserandsystemlabels,thereare248
differentlabels(153fortheuserand95forthesys-
tem). Combiningonlyfirstandsecondlevelthere
are 72 labels (45 for user and 27 for system), and
withonlyfirstlevelthereare16labels(7foruser
and9forsystem).
Annotation was done at utterance level. That
is, each dialogue turn was divided (segmented)
intoutterancessuchthateachonecorrespondstoa
uniqueDAlabel. Anexampleofthesegmentation
and annotation of two turns of a dialogue can be
seeninFigure1
3 Dirichlet
Process Mixture Models
This paper present a Dirichlet Process Mixture
Model (DPMM) (Maceachern and M¨uller, 1998;
Escobar and West, 1995; Antoniak, 1974) for the
Level Labels
First Opening,Closing,Confirmation,
Undefined,Not-understood,Waiting,
Consult,Acceptance,Rejection
Second Departure-hour,Arrival-hour,
Fare,Origin,Destination,Day,
Train-Type,Service,Class,Trip-time
Third Departure-hour,Arrival-hour,
Fare,Origin,Destination,Day,
Train-Type,Service,Class,
Trip-time,Order-number,
Number-trains,Trip-type
Table1: SetofdialogueactlabelsusedintheDi-
hanacorpus
automatic, unsupervised clustering of the utter-
ances in the Dihana corpus. This approach treats
eachutteranceasabag of words(i.e. anunordered
collection of words) (Sebastiani, 2002). Utter-
ancesareclusteredaccordingtotherelativecounts
ofwordoccurrencesthattheycontainsothatutter-
anceswithsimilarhistogramsofwordcountswill,
ingeneral,appearinthesamecluster.
Bayesian methods for unsupervised data clus-
tering divide into parametric and nonparametric
approaches. Parametric approaches to clustering
suchasFiniteBayesianMixtureModels(Mclach-
lanandPeel,2000)requirepriorestimationofthe
number of clusters that are expected to be found
in the data. However, it is not always possible to
know this in advance and often it is necessary to
repeat a modelling experiment many times over a
rangeofchoicesofclusternumberstofindanop-
timalnumberofclusters. Sub-optimalchoicesfor
the number of clusters can lead to a degradation
in the generalisation performance of the model.
Nonparametric approaches to mixture modelling,
on the other hand, do not require prior estimates
of the number of clusters in the data; this is dis-
covered automatically as the model clusters the
data. DirichletProcessesofferoneapproachtode-
velopingBayesiannonparametricmixturemodels.
The remainder of this section briefly introduces
DPMMs, beginning with a brief look at finite
Bayesianmixturemodelswhichwillserveasuse-
fulbackgroundforpresentingtheChineseRestau-
rant Process, the Dirichlet Process paradigm used
inthispaper.
342
Speaker Utterance Transcription
Level1 Level2 Level3
S S1 Welcometotherailwayinformationsystem. HowmayIhelpyou?
Opening Nil Nil
U U1 CouldyoutellmethedeparturetimesfromValencia
Question Departure-hour Origin
U2 toMadrid.
Question Departure-hour Destination
Figure1: AnexampleofsometurnsfromanannotateddialogueofDIHANA corpus.
Figure 2: A 3-simplex with two examples points
andthecorrespondingdistributions
3.1 Finite
Bayesian Mixture Models
A Dirichlet distribution is defined as a measure
on measures. Specifically, a Dirichlet distribution
defines a probability measure over the k-simplex.
Thek-simplexisaconvexhullconstructedsothat
eachpointonthesurfaceofthesimplexdescribes
aprobabilitydistributionoverk outcomes:
Qk = {(x1,...,xk) : xi ≥ 0
∀i ∈ {1...k},
ksummationdisplay
i=1
xi = 1}
Figure 2 shows a 3-simplex with two example
points and the corresponding distributions. The
Dirichletdistributionplacesaprobabilitymeasure
overthek-simplexsothatcertainsubsetsofpoints
on the simplex (i.e. certain distributions) have
higher probabilities than others (Figure 3). The
probability measure in the Dirichlet is parame-
terised by a set of positive, non-zero concentra-
tionconstantsα = {α1,...αk : αi > 0},written
Dirichletk(α1,...αk). The effects of different
valuesofαforthe3-simplexareshowninFigure
3.
TheprobabilitydensityfunctionoftheDirichlet
Figure 3: Three example Dirichlet Distributions
over the 3-simplex with darker regions showing
areas of high probability: (a) Dirichlet(5,5,5), (b)
Dirichlet(0.2,5,0.2),(c)Dirichlet(0.5,0.5,0.5).
343
distributionisgivenby:
Dirichletk(α1,...,αk) = f(x1,...,xk;α1,...,αk)
= Γ(
summationtextk
i=1 αi)producttext
k
i=1 Γ(αi)
kproductdisplay
i=1
xai−1i
where Γ(x) (= integraltext∞0 t(x−1)e−tdt) extends the fac-
torial function to the real numbers. Since a
draw from a Dirichlet distribution (written β ∼
Dirichletk(α)) gives a distribution, a Dirichlet
canbeusedasthepriorforaBayesianfinitemix-
turemodel:
β ∼ Dirichletk(α1,...,αk)
β is a distribution over the k components φ of
the finite mixture model. Each component φzi is
drawn from a base measure G0 (φzi ∼ G0). The
choice of distribution G0 depends on the nature
of the data to be clustered; with data that is rep-
resented using the bag of words model, G0 must
generate distributions over the word vocabulary.
Hence the Dirichlet distribution is an appropriate
choiceinthiscase:
φzi ∼ Dirichletv(α1,...,αv)
wherev isthesizeofthevocabulary.
For each data point (utterance) xi a component
φzi is selected by a draw zi from the multinomial
distributionβ:
zi ∼ Multinomialk(β)
AsuitabledistributionF(φzi)isthenusedtodraw
the data point (utterance). In the bag of words
model, the multinomial distribution is used to
drawthewordsforeachdatapointxi:
xi ∼ Multinomialv(φzi)
A small example will illustrate this generative
process. Imagine that there are just two types
of utterances with a vocabulary consisting sim-
ply of the words A, B and C. A finite Bayesian
mixture model in this case would first draw β
from a suitable Dirichlet distribution (e.g. β ∼
Dirichlet2(0.5,1)) as, for example, is shown in
Figure 4(a). Next the two components φz1 and
φz2 wouldbedrawnfromasuitablebasedistribu-
tion G0 (e.g. φz1 ∼ Dirichlet3(1,0.5,0.5) and
φz2 ∼ Dirichlet3(0.5,0.5,1), see Figure 4(b)
and 4(c)). In this case, φz1 will tend to generate
Figure 4: An example finite Bayesian mixture
model. (a)Thepriordistributionovercomponents
φz1 (b)andφz2 (c)
utterances containing more occurrences of word
A than B or C, whilst φz2 will tend to gener-
ate utterances with more C’s than A’s or B’s. A
component zi is then selected for each utterance
(zi ∼ Multinomialk(β)). Note that in this ex-
ample,thedistributionβ wouldleadtomoreutter-
ancesgeneratedby φz2 thanby φz1. Supposethat
five utterances are to be generated by this model
and that the components for each utterance are
z1 = 1, z2 = 2, z3 = 2, z4 = 1 and z5 = 2.
The words in each utterance are then generated
by repeated draws from the corresponding com-
ponent (e.g. x1 = ACAAB, x2 = ACCBCC,
x3 = CCC,x4 = CABAAC andx5 = ACC).
3.2 Dirichlet
Processes
ADirichletProcesscanbethoughtofasanexten-
sion of a Dirichlet distribution where the dimen-
sions of the distribution are infinite. The prob-
lem with the infinite dimension Dirichlet distri-
bution, though, is that its probability mass would
bedistributedacrossthewholeofthedistribution.
However,inmostpracticalapplicationsofmixture
modellingtherewillbeafinitenumberofclusters.
The solution is to have a process which will tend
to place most of the probability mass at the be-
ginning of the infinite distribution, thereby mak-
ing it possible to assign probabilities to clusters
without restricting the number of clusters avail-
able. The GEM stick breaking construction (the
namecomes fromthefirstlettersof Griffiths, En-
genandMcCloskey(Pitman,2002))achievespre-
cisely this (Pitman and Yor, 1997). Starting with
344
a stick of unit length, random portions β′k are re-
peatedly broken off the stick, with each part that
is broken off representing the proportion of prob-
abilityassignedtoacomponent:
β′k ∼ Beta(1,α) βk =producttextk−1i+1 (1−β′i)·β′k
The Dirichlet Process mixture model can now
bespecifiedas:
β ∼GEM(α) φzi ∼G0 zi ∈ (1...∞)
zi ∼Multinomial(β) xi ∼F(φzi)
3.3 Chinese
Restaurant Process
TheChineseRestaurantProcess(CRP)isapopu-
lar Dirichlet Process paradigm that has been suc-
cessfullyappliedtomanyclusteringproblems. In
theCRP,oneisaskedtoimagineaChineserestau-
rant with an infinite number of tables. The cus-
tomersentertherestaurantandselect,accordingto
agivendistribution,atableatwhichtosit. Allthe
customers on the same table share the same dish.
Inthisparadigm,thetablesrepresentdataclusters,
the customers represent data points (xi) and the
dishes represent components (φz). As each cus-
tomer(datapoint)enterstherestaurantthechoice
of which table (cluster) and therefore which dish
(component)isdeterminedbyadrawfromthefol-
lowingdistribution:
φi|φ1,...,φi−1 ∼ 1(α + i−1)


i−1summationdisplay
j=1
δφj + αG0


where α is the concentration parameter for the
CRP. The summation over the δφj’s counts the
number of customers sat at each of the occupied
tables. Theprobabilityofsittingatanalreadyoc-
cupiedtable,therefore,isproportionaltothenum-
berofcustomersalreadysatatthetable,whilstthe
probability of starting a new table is proportional
to αG0. Figure 5 illustrates four iterations of this
initialclusteringprocess.
Once all the customers (data points) have been
placed at tables (clusters), the inference process
begins. The posterior p(β,φ,z|x) cannot be cal-
culated exactly, but Gibbs sampling can be used.
Gibbs sampling for the CRP involves iteratively
removingarandomlyselectedcustomerfromtheir
table, calculating the posterior probability distri-
butionacrossalltheoccupiedtablestogetherwith
apotentialnewtable(witharandomlydrawndish,
Figure5: Thefirstfourstepsoftheinitialcluster-
ing process of the CRP. The probability distribu-
tionoverthetablesisalsoshownineachcase.
i.e. component),andmakingadrawfromthatdis-
tribution to determine the new table for that cus-
tomer. Theposteriordistributionacrossthetables
iscalculatedasfollows:
φi|φ1,...,φi−1,x
∼ 1B


i−1summationdisplay
j=1
δφjp(xi|φj) + αG0p(xi|φi)


where B = αp(xk) +summationtexti−1j=1 p(xi|φi) is the nor-
malising constant. After a predetermined number
ofsamples,thedish(component)ofeachoccupied
tableisupdatedtofurtherresemblethecustomers
(datapoints)sittingaroundit. Inthe bag of words
approach used here, this involves converting the
histogramofwordcountsineachcustomer(utter-
ance)sittingatthetableintoanempiricaldistribu-
tion H(xi), taking the average of these empirical
distributions and modifying the dish (component)
tofurtherresemblethisdistribution:
φi = φi + µm
i
misummationdisplay
j=1
H(xj)
where µ (0 ≤ µ < 1) is the learning con-
stant and mi is the number of customers around
345
table i. The inference process continues to it-
erate between Gibbs sampling and updating the
table dishes (components) until the process con-
verges. Convergence can be estimated by observ-
ing n consecutive samples in which the customer
was returned to the same table they were taken
from.
4 Results
The CRP with Gibbs sampling was used to clus-
ter both user and system utterances from the 900
dialoguesintheDihanacorpus. Eachutteranceis
treated as an independent bag of words where all
information about the dialogue that it came from
and the context in which it was uttered is ignored
duringtraining. Intra-clusterandinter-clustersim-
ilaritymeasureswereusedtoevaluatetheresulting
clusters. Intra-cluster similarity S′i is calculated
by averaging the Euclidean distance between ev-
erypairofdatapointsintheclusteri:
S′i = 12m
i
misummationdisplay
i=1;j=1
|xi −xj|
Inter-cluster similarity S′′ is calculated by sum-
mingtheEuclideandistancebetweenthecentroids
ofallpairsofclusters:
S′′ =
nsummationdisplay
i=1;j=1
|Ci −Cj|
where Ci is the centroid of cluster i and n is the
numberofclusters.
Two classification error measures were also
used, one from the cluster (table) perspective E′,
andtheotherfromtheperspectiveoftheDialogue
Act (DA) annotations (first level) of the Dihana
corpus E′′. The cluster classification error of ta-
bleiiscalculatedbysumminguptheoccurrences
of each DA on the table, finding the DA with the
largest total and allocating that DA as the correct
classification for that table Di. The number of
false positives fpi for that table is the count of all
customers(utterances)withDAannotationsnotin
Di. Thenumberoffalsenegativesfni isthecount
ofutteranceswithlabel Di thatoccuronotherta-
bles. The cluster classification error for table i is
therefore:
E′i = 1n(fpi + fni )
The DA classification error E′′i measures how
wellDAihasbeenclustered,usingthesizeofthe
Cluster
No. Ans Ask Clo Not Rej Und
1 1 5
4 2 91 2
9 2 1 9
12 7 161 1 1
13 273 26 8
14 382 12 1 5
15 6 1 909 1 327 22
17 47 39 1 1
18 73 1 3
19 1 4
20 131 115 1 3 1
22 270 29 3 3
23 135 8 2 2
25 83 31 1 4
28 247 16 1 4
29 349 6 1 12
33 13 3 5 1 4 25
41 202 45 1 2 3
46 4 1
49 6 251 1 2 4
51 124 896 1 12
53 45 477 10
Table 2: Clusters of user utterances, with the
counts for each level 1 speech act. The largest
clusterforeachspeechactisinbold. Theabbrevi-
ations are: Und = Undefined, Ans = Answering,
Ask = Asking, Clo = Closing, Rej = Rejection,
Not = Not-understood.
DAclassNci ,thesizeofthelargestclusterofutter-
ancesfromthatDAclassMci ,andthetotalnumber
ofutterancesninthecorpus:
E′′i = 1n(Nci −Mci )
Table 6 summarises the results from three sep-
arate runs of the CRP, each increasing in number
ofepochs. ItshouldbenotedherethattheDihana
corpushas72DAcategories, sotheidealnumber
ofclustersdiscoveredbytheCRPwouldbe72. It
should also be noted that given an initial random
clustering,agoodclusteringalgorithmwillreduce
intra-cluster similarity (¯S′), increase inter-cluster
similarity(S′′)andreducetheclassificationerrors
( ¯E′ and ¯E′′).
346
Epochs(K) No. Clusters ¯S′ S′′ ¯E′ ¯E′′
0 70 99703.6 243.74 0.05303 0.00979
1000 44 14975.4 217.56 0.01711 0.00385
1500 54 10093.7 336.15 0.01751 0.00435
Figure6: TheresultsfromthreeseparaterunsoftheCRPonutterancesfromtheDihanacorpus. Cluster
similaritymeasuresandclassificationerrorvaluesareshownafter0(i.e. randomclustering),1000K,and
1500Kepochs. ¯S′, ¯E′ and ¯E′′ areaveragedvalues.
Level1 Level2 Cluster
No.
Answering Day 14
Destination 22
Fare 29
Departure-hour 28,41
Asking Departure-hour,Fare 4
Train-type 12
Fare 49
Departure-hour 51,53
Table 3: Clusters that have specialised on level 1
andlevel2annotations.
5 Discussion
The first row of the table in Figure 6 shows the
cluster similarity measures and classification er-
rorsafter0epochsoftheinferenceprocedure(i.e.
forarandomclusteringofutterances). Thisgivesa
baselineforthemeasuresanderrorvaluesusedin
subsequentruns. Thesecondrowofvaluesshows
the results after a run of 1000K epochs of the in-
ferenceprocedure. Thisrunfindsonly44clusters
buthasamuchlowervaluefor ¯S′ thanwasfound
intherandomclustering,showingasignificantin-
crease in the similarity between utterances within
eachcluster. Surprisingly,thevaluefor S′′ isalso
reduced, showing that the differentiation between
theclustersformedatthisstageisevenlowerthan
there was with the random clustering. ¯E′ and ¯E′′
showsuitablereductionsindicatingthattheclassi-
fication errors are being reduced by the inference
process. The third row of values show that after
1500K epochs 54 clusters have been found, intra-
cluster similarity is increased beyond that for the
randomclustering,buttheclassificationerrorsre-
mainessentiallythesameasforthe1500Krun.
Although the 1500K epoch run found only 54
clusters,itwasabletoclearlydistinguishbetween
system and user utterances: with 30 clusters con-
taining system utterances only, 22 clusters con-
taininguserutterancesonlyand2clusterscontain-
inginstancesofboth. Giventhatthesystemutter-
ances in the Dihana corpus are generated from a
restricted set of sentences, it is not surprising that
these were easy to cluster and differentiate from
user utterances. However, the CRP was also able
to cluster user utterances well, which is more of
a challenge. Table 2 shows the clusters that have
specialised on user utterances, with the counts of
the level 1 annotations in each case. The largest
clusterforeachlevel1annotationisshowninbold
typeface. From here it can be seen that cluster 15
hasspecialisedonbothClosingandRejection. Itis
notsurprisingthatthesefallwithinthesameclus-
tersincethewordsusedineachareoftenthesame
(e.g. “No thank you” can act as either a closing
statement or a rejection statement). Clusters 14,
22,29,28and41havespecialisedtotheAnswer-
ingannotation,whilstclusters4,1249,51and53
have specialised to Asking. Table 3 shows how
each of these clusters have specialised to level 2
annotations. Cluster 14, for example, specialises
on the Answering:Day pair, whilst 22 specialises
onAnswering:Destinationpair.
These initial results show that, at least for the
Dihana corpus, the DPMM can successfully clus-
ter utterances into Speaker, Level 1, and Level2
classes. Whilst this looks promising, it must be
acknowledgedthattheDihanacorpusisrestricted
to train service inquiries and it remains unclear
whether this approach will generalise to other di-
aloguecorporawithabroaderrangeoftopicsand
wider vocabularies. Future work will include in-
vestigating the use of ngrams of words, syntactic
features, the DAs of previous utterances and ex-
perimentation with other corpora such as Switch-
board(Godfreyetal.,1992).
Acknowledgments
ThisworkwasfundedbytheCompanionsproject
(www.companions-project.org) sponsored by the
347
European Commission as part of the Information
SocietyTechnologies(IST)programmeunderEC
grant number IST-FP6-034434. We thank Jeff
Bilmes(UniversityofWashington)formanyvery
helpful discussions about Dirichlet processes and
theirapplication.
References
Toine Andernach, Mannes Poel, and Etto Salomons.
1997. Finding classes of dialogue utterances with
kohonennetworks. In In Daelemans,pages85–94.
J.A. Andernach. 1996. A machine learning approach
totheclassificationandpredictionofdialogueutter-
ances. InProceedings of the 2nd International Con-
ference on New Methods in Language Processing,
pages98–109.
Charles E. Antoniak. 1974. Mixtures of dirichlet pro-
cesses with applications to bayesian nonparametric
problems. The Annals of Statistics,2(6):1152–1174.
J.L. Austin. 1962. How to do things with words. Ox-
ford: ClarendonPress.
Michael D. Escobar and Mike West. 1995. Bayesian
density estimation and inference using mixtures.
Journal of the American Statistical Association,
90(430):577–588.
Sheila Garfield and Stefan Wermter. 2006. Call clas-
sification using recurrent neural networks, support
vector machines and finite state automata. Knowl.
Inf. Syst.,9(2):131–156.
J. J. Godfrey, E. C. Holliman, and J. Mcdaniel. 1992.
SWITCHBOARD: telephone speech corpus for re-
search and development. In Proc. ICASSP, vol-
ume1,pages517–520vol.1.
GangJiandJ.Bilmes. 2005. Dialogacttaggingusing
graphical models. In Acoustics, Speech, and Signal
Processing, 2005. Proceedings. (ICASSP ’05). IEEE
International Conference on, volume 1, pages 33–
36.
J.M.Bened´ı, E.Lleida, A. Varona, M.J.Castro,
I.Galiano, R.Justo, I. L´opez, and A. Miguel. 2006.
Design and acquisition of a telephone spontaneous
speechdialoguecorpusinspanish: Dihana. In Fifth
International Conference on Language Resources
and Evaluation (LREC),pages1636–1639,Genova,
Italy,May.
S. Keizer, M. Gasic, F. Mairesse, B. Thomson, K. Yu,
and S. Young. 2008. Modelling user behaviour
in the his-pomdp dialogue manager. In IEEE SLT,
pages121–124,Dec.
Steven N. Maceachern and Peter M¨uller. 1998. Esti-
mating mixture of dirichlet process models. Jour-
nal of Computational and Graphical Statistics,
7(2):223–238.
C. D. Mart´ınez-Hinarejos, J. M. Bened´ı, and
R. Granell. 2008. Statistical framework for a span-
ish spoken dialogue corpus. Speech Communica-
tion,50:992–1008.
GeoffreyMclachlanandDavidPeel. 2000. Finite Mix-
ture Models. WileySeriesinProbabilityandStatis-
tics.Wiley-Interscience,October.
Gabriel Murray, Steve Renals, Jean Carletta, and Jo-
hanna Moore. 2006. Incorporating speaker and
discourse features into speech summarization. In
Proceedings of the main conference on Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, pages 367–374, Morristown, NJ, USA.
AssociationforComputationalLinguistics.
J. Pitman and M. Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability,25(2):855–900.
J.Pitman. 2002. Combinatorialstochasticprocesses.
Fabrizio Sebastiani. 2002. Machine learning in au-
tomated text categorization. ACM Comput. Surv.,
34(1):1–47,March.
Andreas Stolcke, Noah Coccaro, Rebecca Bates, Paul
Taylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-
beth Shriberg, Daniel Jurafsky, Rachel Martin, and
Marie Meteer. 2000. Dialogue act modeling for
automatictaggingandrecognitionofconversational
speech. Comput. Linguist.,26(3):339–373.
E. Zovato and J. Romportl. 2008. Speech synthesis
andemotions: acompromisebetweenflexibilityand
believability. InProceedings of FourthInternational
Workshop on Human-Computer Conversation, Bel-
lagio,Italy.
348

