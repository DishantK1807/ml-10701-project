Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 180–183,
Los Angeles, California, June 2010. c©2010 Association for Computational Linguistics
Preliminary Experience with Amazon’s Mechanical Turk  
for Anotating Medical Named Entities 
 
Meliha Yetisgen-Yildiz, Imre Solti Fei Xia, Scott Rusel Halgrim 
Biomedical & Health Informatics Department of Linguistics 
University of Washington University of Washington 
Seatle, WA 98195, USA Seatle, WA 98195, USA 
{melihay,solti}@uw.edu {fxia,captnpi}@uw.edu 
 
Abstract 
Amazon’s Mechanical Turk (MTurk) service 
is becoming increasingly popular in Natural 
Language Procesing (NLP) research. In this 
paper, we report our findings in using MTurk 
to anotate medical text extracted from clini-
cal trial descriptions with three entity types: 
medical condition, medication, and laboratory 
test. We compared MTurk anotations with a 
gold standard manualy created by a domain 
expert. Based on the god performance re-
sults, we conclude that MTurk is a very prom-
ising tool for anotating large-scale corpora 
for biomedical NLP tasks. 
1 Introduction

The manual construction of anotated corpora is ex-
tremely expensive both in terms of time and money. 
Snow et al. (208) demonstrated the potential power of 
Amazon’s Mechanical Turk (MTurk) service in annotat-
ing large corpora for natural language tasks cheaply and 
quickly. We are working on a Natural Language Proc-
essing (NLP) project to automate the clinical trial eligi-
bility screning of patients. This project involves 
building statistical models for medical named entity 
recognition which requires a large-scale anotated cor-
pus for training. As part of corpus development, we 
tested the feasibility of using MTurk for the anotation 
of medical named entities in biomedical text and we 
report our findings in this paper. 
In the folowing sections we describe how we used 
MTurk to anotate the biomedical corpus created from 
publicly available clinical trial announcements. The 
main goal of our study was to understand how el non-
experts perform compared to medical expert in annotat-
ing the biomedical text. 
2 Related
Work 
MTurk
1
 is an online micro-task market that allows re-
questers to distribute work to a large number of workers 
from al over the world. The inspiration of the system 
                                                             
1
 https:/ww.MTurk.com/MTurk/welcome 
was to have human workers complete simple tasks that 
would otherwise be extremely dificult for computers to 
perform (Kitur et al., 2008). A complex task is broken 
down into simple, one-time tasks called Human Inteli-
gence Tasks (HITs). Requesters post their HITs on the 
MTurk marketplace by specifying the amount paid for 
the completion of each task, and the workers select from 
the available HITs the ones that they would like to work 
on. In 2007, Amazon claimed that the user base of 
MTurk consisted of over 10,00 users from 10 coun-
tries
2
. 
MTurk has ben adopted for a variety of uses both in 
industry and academia, ranging from user studies (Kitur 
et al., 208) to image labeling (Sorokin and Forsyth, 
2008). Snow et al. (2008) examined the quality of labels 
created by MTurk workers for various NLP tasks in-
cluding word sense disambiguation, word similarity, 
text entailment, and temporal ordering. Since the publi-
cation of Snow et al.’s paper, MTurk has become in-
creasingly popular as an annotation tol for NLP 
research. Nakov (208) used MTurk to create a manu-
ally annotated resource for noun-noun compound inter-
pretation based on paraphrasing verbs. In a diferent 
NLP task, Calison-Burch (209) used MTurk to evalu-
ate machine translation quality. With a budget of only 
$10, Calison-Burch demonstrated the feasibility of per-
forming manual evaluations of machine translation 
quality by recreating judgments from a WMT08 transla-
tion task. 
In our pilot study we used MTurk to anotate entities in 
the biomedical text. To our knowledge, this is the first 
study that investigates the feasibility of MTurk for bio-
medical named entity anotation. 
3 Anotation
Task Description 
In this section we will describe the types of entities in 
our annotation task and the details of our corpus crea-
tion proces.  
                                                             
2
 Source: New York Times article  “Artificial Intelligence, 
With Help from the Humans” , Available at: 
http:/ww.nytimes.com/2007/03/25/busines/yourmoney/25S
tream.html 
180
3.1 Entity
Types 
We used MTurk to anotate the biomedical text for the 
folowing thre entity types:  
• Medical Conditions 
Example: First-degre relative who developed 
<Medical_Condition>breast cancer 
</Medical_Condition> at ≤ 50 years of age. 
• Medications 
Example: Previous treatment with an <Medica-
tion>anthracycline</Medication> i the mtas-
tatic breast cancer setting. 
• Laboratory Test 
Example: <Laboratory_Test>Platelet count 
>=10,00 cels/mL</Laboratory_Test>. 
3.2 Corpus

Our corpus came from the publicly available clinical 
trial anouncements available at the ClinicalTrials.gov 
website. This website is a registry of federaly and pri-
vately supported clinical trials conducted in the United 
States and around the world. The objectives and proce-
dures of each clinical trial are explained in detail along 
with participant selection criteria and logistical informa-
tion such as locations and contact information.  
For this task we selected 50,109 anouncements from 
the roughly 85,00 anouncements posted on the Clini-
calTrials.gov site. For selection criteria we relied on the 
folowing keywords: "heart | cancer | tumor | influenza | 
alzheimer | parkinson | malignant | stroke | respiratory | 
diabetes | pneumonia | nephritis | nephrotic | nephrosis | 
septicemia | liver | cirhosis | hypertension | renal | neo-
plasm". We chose thes keywords because they were 
part of the phrases of diagnoses for the top 12 leading 
causes of death excluding suicide, homicide and aci-
dents (Heron et al., 209). We limited the selection to 
trials for "Adult" or "Senior" patients.  
After downloading the corpus of XML files we con-
verted them to ANSI text using ABC Amber XML 
Converter
3
. 49,794 files successfuly converted to ANSI 
text format. Using a simple regular expresion search 
we selected documents that had both the "Inclusion Cri-
teria" and "Exclusion Criteria" phrases. The final selec-
tion proces resulted in 35,385 files. From this latest set 
we randomly selected 100 files to build the corpus for 
our pilot study. One of the authors, who has medical 
training, then manually anotated the thre entity types 
in those selected files. We used this anotated set as the 
gold standard to measure the quality of the MTurk 
workers’ anotations.  
4 HIT
Design 
                                                             
3
 ABC Amber XML Converter. Available at: 
http:/www.procestext.com/abcxml.html. 
Biomedical text is ful of jargon, and finding the thre 
entity types in such text can be dificult for non-expert 
annotators. To make the annotation task more conven-
ient for the MTurk workers, we used a customized user 
interface and provided detailed anotation guidelines. 
We also tested the bonus system available in the MTurk 
environment and evaluated the performance of the 
workers. 
4.1 User
Interface 
In order to adapt the task of entity anotation to the 
MTurk format, we used an in-house web-based graphi-
cal user interface that allows the worker to select a span 
of text with the mouse cursor. The interface also uses 
simple tokenization heuristics to divide the text into 
highlightable spans and resolve partial token highlights 
or double-clicks into the next largest span. For instance, 
highlighting the word “cancer” from the second “c” to 
“e” wil result in the entire span “cancer” being high-
lighted.  
4.2 Annotation
Guidelines 
We created thre separate anotation tasks, one for each 
entity type. For each task, we wrote annotation guide-
lines that explained the task and showed examples of 
entities that should be taged and the ones that should 
not. 
4.3 Bonus
System 
MTurk provides two methods for paying workers – 
fixed rates on each docuent and bonuses to workers 
for especialy god work. In this study, we experi-
mented with the bonus system to se its efect on per-
formance and anotation time. Anotating a document 
would receive a base rate of $0.01-$0.05, but each 
taged entity span could elicit a bonus of $0.01. The 
base rate would cover the case where the document 
truly contained no entities, but the bonus amount could 
potentialy be much larger than the base rate if the 
document was entity-rich. Bonuses for each taged en-
tity span were awarded based on an agrement threshold 
with per workers. In this study, each document was 
annotated by four workers and we granted bonuses for 
entity spans that were agreed upon by at least three 
workers. 
4.4 Performance
Monitoring 
We monitored a worker’s performance by comparing 
the worker’s anotations with his/her per workers’ 
annotations. After we posted the HITs, we continuously 
monitored the workers’ performance and rejected the 
annotations from the ones who tried to cheat the system 
by either not doing any annotations (e.g., imediately 
submiting the document after acepting it) or con-
181
stantly doing wrong anotations (e.g., always anotating 
the first word of the text). Those rejected documents 
were automaticaly re-posted on the MTurk so other 
workers could work on them. In this pilot study, per-
formance monitoring was done mainly manually. As 
future work, we plan to automate the proces in order to 
scale it for larger anotation tasks. 
4.5 Comunication
with Workers 
The workers could send us their questions and com-
ments about the individual documents or the general 
annotation task through a text box in the interface. Dur-
ing this study we received more than 10 mesages 
from the workers. The majority of the messages were 
positive mesages (“thank you”, “easy hit!”). However, 
some of the coments included questions such as: “Is 
pregnancy a medical condition?” or “Text doesn’t men-
tion the type of insulin but I highlighted it because insu-
lin is a medication!”. We responded to the questions in 
a timely manner to increase the quality of annotations.  
5 Anotation
Experiments 
In our anotation experiments, each of 10 documents 
in our corpus was anotated by four workers, resulting 
in 10×4=400 files per experiment. We experimented 
with diferent pay scales to understand how they afect 
the quality and sped of the anotations. 
5.1 Cost
of Annotations 
We investigated the cost of anotations both in terms of 
money and time. The summary of the results is in Table 
1. We ran five diferent MTurk annotation experiments 
for our corpus of 10 documents. A total of 139 workers 
were involved in our experiments, and we identified 
eight of those workers as cheaters and rejected their 
annotation. The remaining workers spent 138.86 hours 
to complete 1872 files. The slowest experiment was 
MedicalCondition-I, in which we paid a base document 
rate of $0.01 without any bonuses. With this pay scale, 
it took 71.16 hours for workers to anotate 272 out of 
400 files. We suspected we could not atract enough 
workers to finish the anotation task on time so we 
stoped the experiment before al 40 files were com-
pleted. When we compiled the results, we noticed that 
there was a general tendency for the workers to tag the 
first one or two entities and then ignore the rest of the 
document. Based on this observation, we decided to ad 
bonuses to motivate the workers to read through the 
whole document. We ran the same anotation task, 
MedicalCondition-II, with a higher base document rate 
of $0.05 and a bonus rate of $0.01. With this new pay-
ment scale the anotation task was fuly completed in 
7.28 hours. 
We also compared the efect of base rates when the bo-
nus amounts were kept the same. For medication anno-
tations, increasing the base document rate from $0.01 to 
$0.05 decreased the total amount of annotation time 
from 31.65 hours to 4.36 hours and also decreased the 
number of workers from 45 to 17. We ordered the 
workers based on the number files they anotated. The 
top ranked 5 workers in Medication-I annotated 187 
files (46%) and the top ranked 5 workers in Medication-
II anotated 313 files (78%). The diference between 
those two values was interesting since it indicated that 
by increasing the base rate, we managed to atract work-
ers who worked on more documents. 
The average amount of time workers spent per docu-
ment varied based on entity type. They spent the longest 
amount of time for medical condition and shortest 
amount of time for laboratory test. This can be ex-
plained by the richnes of documents in terms of enti-
ties. In the manually created gold standard there were 
1159 mentions of medical condition, 518 mentions of 
medication, and 249 mentions of laboratory tests. An-
other observation was that the change in pay scales did 
not afect the average annotation time per document. 
5.2 Quality
of Anotations 
We measured the quality of the MTurk anotations at 
diferent inter-annotator agreement levels by comparing 
the agred entity spans with the spans in the gold stan-
dard. 
Table 1. Cost analysis of anotation experiments (“File” in this table  means the anotation of a document. 
There are 10 documents, and each document is anotated by four workers.) 
MONETARY COST TIME COST 
File Count 
Pay Rate ($) Total Cost ($) Completion Time 
Experiment Label 
Total Completed 
Total 
Worker 
Count File Bonus File Bonus 
Per file 
(seconds) 
Total 
(hours) 
MedicalCondition-I 400 272 45 0.01 0 2.72 0 156.09 71.16 
MedicalCondition-II 400 400 30 0.05 0.01 20 22.61 162.66 7.28 
Medication-I 400 400 45 0.01 0.01 4 4.43 87.96 31.65 
Medication-II 400 400 17 0.05 0.01 20 6.11 89.06 4.36 
Laboratory Test  400 400 26 0.05 0.01 20 1.49 75.61 24.41 
 
182
Given a document anotated by multiple workers and an 
agreement level k, there are different ways of creating a 
new span file that includes only the spans that are 
agreed by at least k workers. One method is to go over 
each span in each annotation and output only the spans 
that are marked by at least k workers. This method does 
not work wel when the spans are long and the workers 
could disagree on the boundary. We used an alternative 
method which first goes over each word position in the 
document and marks the positions that are part of spans 
in at least k anotations, and then outputs the spans that 
cover those marked positions. We call the new span file 
agrement-k file. 
Once we have created agrement-k file, we compare it 
with the gold standard to calculate precision, recall, and 
F-measure. A span in agrement-k file and a span in the 
gold standard are caled an exact match if they are iden-
tical and are called an overlap match if they overlap 
(exact match is a special case of overlap match). Table 2 
shows the performance for the MedicalCondition-II, 
Medication-II, and LaboratoryTest experiments at dif-
ferent agrement levels (k). As can be sen from the 
table, as the value of k increased, the precision values 
increased and the recall values decreased. For all of the 
experiments, the best F-Score was achieved at agree-
ment-level 2. 
Of the thre entity types, laboratory test was the hardest 
partly because laboratory test entities tend to be longer 
(the average length for entities in gold standard was 
5.25 words, compared to 1.84 words for medication and 
3.18 words for medical condition), making the exact 
boundary harder to define. The results for MedicalCon-
dition-II and Medication-II were higher than Laborato-
ryTest. In adition, acuracy for Medication-I (not 
shown here due to space limit) and edication-II were 
similar, indicating that pay rate did not afect acuracy 
much in our experiments. In the future, we plan to in-
crease the number of annotations for each document, 
which we believe could further improve the perform-
ance. 
6  Conclusion 
Human anotation is crucial for many NLP tasks. In this 
paper, we demonstrated the potential of using MTurk 
for anotating medical text. By continuously monitoring 
the workers’ performance and using the bonus system, 
we acquired high quality anotations from non-expert 
MTurk workers with limited time and budget.  
As future work, we plan to analyze the MTurk anota-
tions in detail in order to understand the problematic 
areas. Based on our observations, we wil redesign our 
annotation tasks and continue our experiments with 
MTurk to create large-scale anotated corpora to be 
used in biomedical NLP projects. 
Acknowledgement 
This project was suported in part by NIH Grants 
1K99LM010227-0110 and 5 U54 LM008748. 
References 
[1] Chris Calison-Burch. 2009. Fast, Cheap, and Crea-
tive: Evaluating Translation Quality Using Amazon’s 
Mechanical Turk. In Procedings of EMNLP’09. 
[2] Melonie Heron, Dona L. Hoyert, Shery L. Mur-
phy, Jiaquan Xu, Kenneth D. Kochanek, and Bet-
zaida Tejada-Vera. 209. Deaths: Final data for 
2006. National Vital Statistics Reports, 57:14. 
[3] Aniket Kitur, Ed H. Chi, and Bongwon Suh. 208. 
Crowdsourcing User Studies with Mechanical Turk. 
In Procedings of CHI’08. 
[4] Preslav Nakov. 208. Noun compound interpretation 
using paraphrasing verbs: Feasibility study. In Pro-
ceedings of the 13th international conference on Arti-
ficial Inteligence: Methodology, Systems and 
Applications (AISA 208), 103–117. 
[5] Philip V. Ogren. 206. Knowtator: a protégé plug-in 
for anotated corpus construction. In Procedings 
NAACL HLT’06, 273-275. 
[6] Rion Snow, Brendan O'Conor, Daniel Jurafsky and 
Andrew Y. Ng. 208. Cheap and Fast But is it 
Good? Evaluating Non-Expert Anotations for Natu-
ral Language Tasks. In Procedings of EMNLP’08, 
254-263. 
 [7] Alexander Sorokin and David Forsyth. Utility data 
annotation with Amazon Mechanical Turk. In Pro-
ceedings of Computer Vision and Pattern Recogni-
tion Workshop at CVPR’08. 
 
Table 2. Quality measurement of MTurk anotations (k: Agrement level, P: Precision, R: Recall, F: F-measure; 
the highest value for each column is in boldface) 
Medical Condition-II Medication-II Laboratory Test 
Exact Overlap Exact Overlap Exact Overlap k 
P R F P R F P R F P R F P R F P R F 
1 0.51 0.66 0.58 0.70 0.99 0.79 0.43 0.73 0.54 0.50 0.84 0.62 0.30 0.52 0.38 0.42 0.73 0.53 
2 0.64 0.66 0.65 0.84 0.87 0.86 0.71 0.66 0.68 0.79 0.73 0.76 0.47 0.43 0.45 0.72 0.65 0.68 
3 0.63 0.52 0.57 0.89 0.73 0.80 0.78 0.38 0.51 0.93 0.45 0.61 0.29 0.13 0.18 0.86 0.40 0.54 
4 0.60 0.31 0.41 0.93 0.48 0.63 0.76 0.10 0.18 0.89 0.12 0.21 0.05 0.00 0.01 1.00 0.08 0.14 
 
183

