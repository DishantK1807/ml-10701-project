Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 311–316,
Portland, Oregon, June 19-24, 2011. c©2011 Association for Computational Linguistics
Improving Classification of Medical Assertions in Clinical Notes 
 
 
Youngjun Kim Ellen Riloff Stéphane M. Meystre 
School of Computing School of Computing Department of Biomedical Informatics 
University of Utah University of Utah University of Utah 
Salt Lake City, UT Salt Lake City, UT Salt Lake City, UT 
youngjun@cs.utah.edu riloff@cs.utah.edu stephane.meystre@hsc.utah.edu 
 
 
 
 
 
 
Abstract 
We present an NLP system that clasifies the 
assertion type of medical problems in clinical 
notes used for the Fourth i2b2/VA Chalenge. 
Our clasifier uses a variety of linguistic fea-
tures, including lexical, syntactic, lexico-
syntactic, and contextual features. To overcome 
an extremely unbalanced distribution of asser-
tion types in the data set, we focused our eforts 
on adding features specificaly to improve the 
performance of minority clases. As a result, 
our system reached 94.17% micro-averaged and 
79.76% macro-averaged F
1
-measures, and 
showed substantial recal gains on the minority 
classes. 
1 Introduction

Since the begining of the new milennium, there 
has been a growing need in the medical community 
for Natural Language Procesing (NLP) technolo-
gy to provide computable information from nara-
tive text and enable improved data quality and de-
cision-making. Many NLP researchers working 
with clinical text (i.e. documents in the electronic 
health record) are also realizing that the transition 
to machine learning techniques from traditional 
rule-based methods can lead to more eficient ways 
to proces increasingly large colections of clinical 
naratives. As evidence of this transition, nearly al 
of the best-performing systems in the Fourth 
i2b2/VA Chalenge (Uzuner and DuVall, 2010) 
used machine learning methods. 
In this paper, we focus on the medical asertions 
classification task. Given a medical problem men-
tioned in a clinical text, an asertion clasifier must 
look at the context and chose the status of how 
the medical problem pertains to the patient by as-
signing one of six labels: present, absent, hypothet-
ical, possible, conditional, or not associated with 
the patient. The corpus for this task consists of dis-
charge sumaries from Partners HealthCare (Bos-
ton, MA) and Beth Israel Deacones Medical Cen-
ter, as well as discharge sumaries and progres 
notes from the University of Pitsburgh Medical 
Center (Pitsburgh, PA). 
Our system performed wel in the i2b2/VA 
Chalenge, achieving a micro-averaged F
1
-measure 
of 93.01%. However, two of the asertion catego-
ries (present and absent) acounted for nearly 90% 
of the instances in the data set, while the other four 
classes were relatively infrequent. When we ana-
lyzed our results, we saw that our performance on 
the four minority clases was weak (e.g., recal on 
the conditional class was 2.2%). Even though 
the minority clases are not comon, they are ex-
tremely important to identify acurately (e.g., a 
medical problem not associated with the patient 
should not be assigned to the patient). 
In this paper, we present our efforts to reduce 
the performance gap betwen the dominant aser-
tion clases and the minority clases. We made 
thre types of changes to adres this issue: we 
changed the multi-class learning strategy, filtered 
the training data to remove redundancy, and aded 
new features specificaly designed to increase re-
call on the minority clases. We compare the per-
formance of our new clasifier with our original 
311
i2b2/VA Challenge clasifier and show that it per-
forms substantialy beter on the minority classes, 
while increasing overall performance as well. 
2 Related
Work 
During the Fourth i2b2/VA Chalenge, the aser-
tion clasification task was tackled by participating 
researchers. The best performing system (Berry de 
Bruijn et al., 201) reached a micro-averaged F
1
-
measure of 93.62%. Their breakdown of F
1
 scores 
on the individual clases was: present 95.94%, ab-
sent 94.23%, possible 64.33%, conditional 
26.26%, hypothetical 88.40%, and not associated 
with the patient 82.35%. Our system had the 6
th
 
best score out of 21 teams, with a micro-averaged 
F
1
-measure of 93.01%. 
    Previously, some researchers had developed sys-
tems to recognize specific asertion categories. 
Chapman et al. (201) created the NegEx algo-
rithm, a simple rule-based system that uses regular 
expressions with triger terms to determine wheth-
er a medical term is absent in a patient. They re-
ported 77.8% recal and 84.5% precision for 1,235 
medical problems in discharge summaries. Chap-
man et al. (207) also introduced the ConText al-
gorithm, which extended the NegEx algorithm to 
detect four asertion categories: absent, hypothet-
ical, historical, and not associated with the patient.   
Uzuner et al. (209) developed the Statistical As-
sertion Classifier (StAC) and showed that a ma-
chine learning approach for assertion classification 
could achieve results competitive with their own 
implementation of Extended NegEx algorithm 
(ENegEx). They used four asertion clases: pre-
sent, absent, uncertain in the patient, or not asso-
ciated with the patient. 
3 The
Assertion Classifier 
We aproach the asertion clasification task as a 
supervised learning problem. The clasifier is giv-
en a medical term within a sentence as input and 
must asign one of the six asertion categories to 
the medical term based on its surounding context.    
3.1 Pipeline
Architecture 
We built a UIMA (Feruci and Lally, 204; 
Apache, 208) based pipeline with multiple com-
ponents, as depicted in Figure 1. The architecture 
includes a section detector (adapted from earlier 
work by Meystre and Haug (205), a tokenizer 
(based on regular expresions to split text on white 
space characters), a part-of-spech (POS) tager 
(OpenNLP (Baldridge et al., 205) module with 
trained model from cTAKES (Savova et al., 
2010)), a context analyzer (local implementation of 
the ConText algorithm (Chapman et al., 201), 
and a normalizer based on the LVG (Lexical Vari-
ants Generation) (LVG, 2010) annotator from 
cTAKES to retrieve normalized word forms. 
Figure 1: System Pipeline 
 
The asertion clasifier uses features extracted 
by the subcomponents to represent training and test 
instances. We used LIBSVM, a library for support 
vector machines (SVM), (Chang and Lin, 201) 
for multi-class classification with the RBF (Radial 
Basis Function) kernel. 
3.2 Original
i2b2 Feature Set 
The asertion clasifier that we created for the 
i2b2/VA Challenge used the features listed below, 
which we developed by manually examining the 
training data: 
Lexical Features: The medical term itself, the 
thre words preceding it, and the thre words fol-
lowing it. We used the LVG anotator in Lexical 
Tools (McCray et al., 194) to normalize each 
word (e.g., with respect to case and tense). 
Syntactic Features: Part-of-spech tags of the 
thre words preceding the medical term and the 
thre words following it. 
312
Lexico-Syntactic Features: We also defined 
features representing words coresponding to sev-
eral parts-of-spech in the same sentence as the 
medical term. The value for each feature is the 
normalized word string. To mitigate the limited 
window size of lexical features, we defined one 
feature each for the nearest preceding and folow-
ing adjective, adverb, preposition, and verb, and 
one additional preceding adjective and preposition 
and one additional folowing verb and preposition. 
Contextual Features: We incorporated the 
ConText algorithm (Chapman et al., 201) to de-
tect four contextual properties in the sentence: ab-
sent (negation), hypothetical, historical, and not 
associated with the patient. The algorithm asigns 
one of thre values to each feature: true, false, or 
possible. We also created one feature to represent 
the Section Header with a string value normalized 
using (Meystre and Haug, 2005). The system only 
using contextual features gave reasonable results: 
F
1
-measure overal 89.96%, present 91.39%, ab-
sent 86.58%, and hypothetical 72.13%. 
Feature Pruning: We created an UNKNOWN 
feature value to cover rarely sen feature values. 
Lexical feature values that had frequency < 4 and 
other feature values that had frequency < 2 were al 
encoded as UNKNOWNs. 
3.3 New
Features for Improvements 
After the i2b2/VA Challenge submission, we add-
ed the folowing new features, specifically to try to 
improve performance on the minority clases: 
Lexical Features: We created a second set of 
lexical features that were case-insensitive. We also 
created thre aditional binary features for each 
lexical feature. We computed the average tf-idf 
score for the words comprising the medical term 
itself, the average tf-idf score for the thre words to 
its left, and the average tf-idf score for the thre 
words to its right. Each binary feature has a value 
of true if the average tf-idf score is smaller than a 
threshold (e.g. 0.5 for the medical term itself), or 
false otherwise. Finaly, we created another binary 
feature that is true if the medical term contains a 
word with a negative prefix.
1
 
Lexico-Syntactic Features:  We defined two 
binary features that check for the presence of a 
                                                             
1
 Negative prefixes: ab, de, di, il, im, in, ir, re, un, no, mel, 
mal, mis. In retrospect, some of these are too general and 
should be tightened up in the future. 
coma or question mark adjacent to the medical 
term. We also defined features for the nearest pre-
ceding and folowing modal verb and wh-adverb 
(e.g., where and when). Finaly, we reduced the 
scope of these features from the entire sentence to 
a context window of size eight around the medical 
term. 
Sentence Features: We created two binary fea-
tures to represent whether a sentence is long (> 50 
words) or short (<= 50 words), and whether the 
sentence contains more than 5 punctuation marks, 
primarily to identify sentences containing lists. 
2
 
Context Features: We created a second set of 
ConText algorithm properties for negation restrict-
ed to the six word context window around the 
medical term. According to the assertion annota-
tion guidelines, problems asociated with allergies 
were defined as conditional. So we added one bi-
nary feature that is true if the section headers con-
tain terms related to allergies (e.g., “Medication 
allergies”). 
Feature Pruning: We changed the pruning 
strategy to use document frequency values instead 
of corpus frequency for the lexical features, and 
used document frequency > 1 for normalized 
words and > 2 for case-insensitive words as 
thresholds. We also removed 57 redundant in-
stances from the training set. Finaly, when a med-
ical term co-exists with other medical terms (prob-
lem concepts) in the same sentence, the others are 
excluded from the lexical and lexico-syntactic fea-
tures. 
3.4 Multi-clas Learning Strategies 
Our original i2b2 system used a 1-vs-1 clasifica-
tion strategy. This aproach creates one classifier 
for each posible pair of labels (e.g., one clasifier 
decides whether an instance is present vs. absent, 
another decides whether it is present vs. condition-
al, etc.). Al of the classifiers are applied to a new 
instance and the label for the instance is deter-
mined by summing the votes of the clasifiers. 
However, Huang et al. (201) reported that this 
approach did not work wel for data sets that had 
highly unbalanced class probabilities.  
   Therefore we experimented with an alternative 1-
vs-all classification strategy. In this approach, we 
                                                             
2
 We hoped to help the clasifier recognize lists for nega-
tion scoping, although no scoping features were aded per 
se. 
313
create one classifier for each type of label using 
instances with that label as positive instances and 
instances with any other label as negative instanc-
es. The final clas label is asigned by choosing the 
class that was asigned with the highest confidence 
value (i.e., the clasifier’s score). 
4 Evaluation

After changing to the 1-vs-all multi-class strategy 
and adding the new feature set, we evaluated our 
improved system on the test data and compared its 
performance with our original system. 
4.1 Data

The training set includes 349 clinical notes, with 
11,967 asertions of medical problems. The test set 
includes 47 texts with 18,50 assertions. These 
assertions were distributed as folows (Table 1): 
 
 Training (%) Testing (%) 
Present 67.28   70.22   
Absent 21.18   19.46   
Hypothetical           5.44   3.87   
Posible  4.47   4.76   
Conditional 0.86   0.92   
Not Patient 0.77   0.78   
 
Table 1: Assertions Distribution 
4.2 Results

For the i2b2/VA Chalenge submision, our system 
showed god performance, with 93.01% micro-
averaged F
1
-measure. However, the macro F
1
-
measure was much lower because our recal on the 
minority clases was weak. For example, most of 
the conditional test cases were misclasified as 
present. Table 2 shows the comparative results of 
the two systems (named ‘i2b2’ for the i2b2/VA 
Chalenge system, and ‘new’ for our improved sys-
tem). 
 
 Recal Precision F
1
-measure 
 i2b2 New i2b2 New i2b2 New 
Present 97.89 98.07 93.11 94.46 95.44 96.23 
Absent 92.99 94.71 94.30 96.31 93.64 95.50 
Posible 45.30 54.36 80.00 78.30 57.85 64.17 
Conditional 22.22 30.41 90.48 81.25 35.68 44.26 
Hypothetical 82.98 87.45 92.82 92.07 87.63 89.70 
Not patient 78.62 81.38 100.0 97.52 88.03 88.72 
Micro Avg.  93.01 94.17 93.01 94.17 93.01 94.17 
Macro Avg. 70.00 74.39 91.79 89.99 76.38 79.76 
 
Table 2: Result Comparison of Test Data 
 
The micro-averaged F
1
-measure of our new system 
is 94.17%, which now outperforms the best oficial 
score reported for the 2010 i2b2 chalenge (which 
was 93.62%). The macro-averaged F
1
-measure 
increased from 76.38% to 79.76% because perfor-
mance on the minority clases improved. The F
1
-
measure improved in all classes, but we saw espe-
cially large improvements with the possible class 
(+6.32%) and the conditional class (+8.58%). Alt-
hough the improvement on the dominant clases 
was limited in absolute terms (+.79% F
1
-measure 
for present and +1.86% for absent), the relative 
reduction in error rate was greater than for the mi-
nority clases: -29.25% reduction in error rate for 
absent assertions, -17.32% for present assertions, 
and -13.3% for conditional assertions. 
 
Present Absent Posible Conditional Hypothetical Not patient 
 R P R P R P R P R P R P 
i2b2 98.36 93.18 94.52 95.31 48.22 84.59 9.71 100.0 86.18 95.57 55.43 98.08 
+ 1-vs-all 97.28 94.56 95.07 94.88 57.38 75.25 27.18 77.78 90.32 93.33 72.83 95.71 
+ Pruning 97.45 94.63 94.91 94.75 60.34 79.26 33.01 70.83 89.40 94.48 69.57 95.52 
+Lex+LS+Sen 97.51 94.82 95.11 95.50 63.35 78.74 33.98 71.43 88.63 93.52 70.65 97.01 
+ Context 97.60 94.94 95.39 95.97 63.72 78.11 35.92 71.15 88.63 93.52 69.57 96.97 
 
Table 3: Cros Validation on Training Data: Results from Aplying New Features Cumulatively 
(Lex=Lexical features; LS=Lexico-Syntactic features; Sen=Sentence features) 
 
314
4.3 Analysis

We performed five-fold cros validation on the 
training data to measure the impact of each of the 
four subsets of features explained in Section 3. Ta-
ble 3 shows the cross validation results when cu-
mulatively ading each set of features. Applying 
the 1-vs-all strategy showed interesting results: 
recal went up and precision went down for al 
classes except present. Although the overal F
1
-
measure remained almost same, it helped to in-
crease the recall on the minority clases, and we 
were able to gain most of the precision back (with-
out sacrificing this recal) by adding the new fea-
tures. 
The new lexical features including negative pre-
fixes and binary tf-idf features primarily increased 
performance on the absent class. Using document 
frequency to prune lexical features showed smal 
gains in all clases except absent. Sentence fea-
tures helped recognize hypothetical assertions, 
which often ocur in relatively long sentences. 
The possible class benefited the most from the 
new lexico-syntactic features, with a 3.38% recall 
gain. We observed that many possible concepts 
were preceded by a question mark ('?') in the train-
ing corpus. The new contextual features helped 
detect more conditional cases. Five allergy-related 
section headers (i.e. “Allergies”, “Allergies and 
Medicine Reactions”, “Allergies/Sensitivities”, 
“Allergy”, and “Medication Alergies”) were aso-
ciated with conditional assertions. Together, al 
the new features increased recall by 26.21% on the 
conditional class, 15.5% on possible, and 14.14% 
on not associated with the patient. 
5.   Conclusions  
We created a more acurate asertion clasifier that 
now achieves state-of-the-art performance on as-
sertion labeling for clinical texts. We showed that 
it is posible to improve performance on recogniz-
ing minority clases by 1-vs-all strategy and richer 
features designed with the minority clases in 
mind. However, performance on the minority clas-
ses stil lags behind the dominant classes, so more 
work is neded in this area. 
Acknowledgments 
We thank the i2b2/VA challenge organizers for 
their eforts, and gratefully acknowledge the sup-
port and resources of the VA Consortium for 
Healthcare Informatics Research (CHIR), VA HSR 
HIR 08-374 Translational Use Case Projects; Utah 
CDC Center of Excelence in Public Health Infor-
matics (Grant 1 P01HK0069-01), the National 
Science Foundation under grant IS-1018314, and 
the University of Utah Department of Biomedical 
Informatics. We also wish to thank our other i2b2 
team embers: Guy Divita, Qing Z. Treitler, Doug 
Red, Adi Gundlapali, and Sasikiran Kandula. 
Finaly, we truly appreciate Bery de Bruijn and 
Colin Chery for the prompt responses to our in-
quiry. 
References 
Apache UIMA 208. Available at http:/uima.apache.org. 
Jason Baldridge, Tom Morton, and Gan Bierner. 205. 
OpenNLP Maxent Package in Java, Available at: 
http:/incubator.apache.org/opennlp/. 
Bery de Bruijn, Colin Chery, Svetlana Kiritchenko, 
Joel Martin, and Xiaodan Zhu. 2011. Machine-
Learned Solutions for Thre Stages of Clinical In-
formation Extraction: the State of the Art at i2b2 
2010. J Am Med Inform Assoc. 
Chih-Chung Chang and Chih-Jen Lin, LIBSVM: a Li-
brary for Support Vector Machines, 2001. Available 
at htp:/ww.csie.ntu.edu.tw/~cjlin/libsvm. 
Wendy W. Chapman, Wil Bridewel, Paul Hanbury, 
Gregory F. Coper, and Bruce G. Buchanan. 201.  
A Simple Algorithm for Identifying Negated Find-
ings and Diseases in Discharge Sumaries. Journal 
of Biomedical Informatics, 34:301-310. 
Wendy W. Chapman, David Chu, and John N. Dowling. 
2007. ConText: An Algorithm for Identifying Con-
textual Features from Clinical Text. BioNLP 207: 
Biological, translational, and clinical language pro-
cessing, Prague, CZ. 
David Feruci and Adam Laly. 204. UIMA: An Ar-
chitectural Approach to Unstructured Information 
Processing in the Corporate Research Environment. 
Journal of Natural Language Engineering, 10(3-4): 
327-348. 
Tzu-Kuo Huang, Ruby C. Weng, and Chih-Jen Lin. 
2006. Generalized Bradley-Tery Models and Mul-
ticlas Probability Estimates. Journal of Machine 
Learning Research, 7:85-115. 
i2b2/VA 2010 Challenge Asertion Anotation Guidelines. 
https:/ww.i2b2.org/NLP/Relations/asets/Asertion
%20Anotation%20Guideline.pdf. 
315
LVG (Lexical Variants Generation). 2010. Available at: 
http:/lexsrv2.nlm.nih.gov/LexSysGroup/Projects/lvg. 
Alexa T. McCray, Suresh Srinivasan, and Allen C. 
Browne. 194. Lexical Methods for Managing Varia-
tion in Biomedical Terminologies. Proc Anu Symp 
Comput Apl Med Care.:235–239. 
Stéphane M. Meystre and Peter J. Haug. 205. Automa-
tion of a Problem List Using Natural Language Pro-
cessing. BMC Med Inform Decis Mak, 5:30. 
Guergana K. Savova, James J. Masanz, Philip V. Ogren, 
Jiaping Zheng, Sunghwan Sohn, Karin C. Kiper
Schuler, and Christopher G. Chute. 2010. Mayo clin-
ical Text Analysis and Knowledge Extraction System 
(cTAKES): architecture, component evaluation and 
applications. J Am Med Inform Asoc., 17(5):507-
513. 
Özlem Uzuner and Scot DuVal. 2010. Fourth i2b2/VA 
Chalenge. In http:/ww.i2b2.org/NLP/Relations/. 
Özlem Uzuner, Xiaoran Zhang, and Sibanda Tawanda. 
2009. Machine Learning and Rule-based Approaches 
to Asertion Clasification. J Am Med Inform Assoc., 
16:109-115.
316

