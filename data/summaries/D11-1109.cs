(SRL)(Lietal.,2010),jointwordsensedisambigua-
P10-1113
than the hybrid model of Zhang and Clark (2008b).
D08-1059 P08-1101
entropy (Ratnaparkhi, 1996), conditional random
W96-0213
ing Koo and Collins (2010), we eliminate unlikely
P10-1001
Chris Dyer. 2009. Using a maximum entropy model
N09-1046
In his PhD thesis, McDonald (2006) extends his
E06-1011
Joakim Nivre. 2008. Algorithms for deterministic in-
J08-4003
Jason Eisner. 1996. Three new probabilistic models for
C96-1058
gorithm, McDonald et al. (2005) propose the first-
P05-1012
and outperforms the one of Carreras (2007) according to the
D07-1101
Model1 in Kooand Collins (2010), butwithout
P10-1001
Xiangyu Duan, Jun Zhao, , and Bo Xu. 2007. Proba-
D07-1098
dependencyparsingaswell(KooandCollins,2010).
P10-1001
ference on Computational Linguistics (Coling 2010),
P10-1113 P10-1113
Adwait Ratnaparkhi. 1996. A maximum entropy model
W96-0213
tionandSRL(CheandLiu, 2010), jointtokenization
C10-1019 C10-1080 P10-1113
Koo and Collins (2010) also briefly discuss that their
P10-1001
Wanxiang Che and Ting Liu. 2010. Jointly modeling
C10-1019 C10-1080 P10-1113
graph-based model of McDonald et al. (2005), while
P05-1012
ald, 2006; Carreras, 2007; Koo and Collins, 2010).
D07-1101 P10-1001
Roger Levy and Christopher D. Manning. 2003. Is it
P03-1056
Shay B. Cohen and Noah A. Smith. 2007. Joint morpho-
D07-1022
order model of Carreras (2007) incorporates both
D07-1101
adopt pipelined approaches (Surdeanu et al., 2008;
W08-2121
Slav Petrov and Dan Klein. 2007. Improved inference
N07-1051
Goldberg and Tsarfaty, 2008), joint named en-
P08-1043
bepossible, sinceKooandCollins(2010)showsthat
P10-1001
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
P10-1110 P10-1113
Ryan McDonald and Fernando Pereira. 2006. On-
E06-1011
tokenization and parsing (Cohen and Smith, 2007;
D07-1022
ing (Charniak and Johnson, 2005; Petrov and Klein,
P05-1022
Huang and Sagae (2010). They greatly expand the
P10-1110
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
P05-1022
Michael Collins. 2002. Discriminative training meth-
W02-1001
children. Koo and Collins (2010) propose efficient
P10-1001
(Collins, 2002). We use perceptron to build our POS
W02-1001
Yoav Goldberg and Reut Tsarfaty. 2008. A single gener-
P08-1043
Jenny Rose Finkel and Christopher D. Manning. 2009.
N09-1037
Kristina Toutanova and Colin Cherry. 2009. A global
P09-1055
graph-based models (Koo and Collins, 2010).
P10-1001
tags for Chinese data (Duan et al., 2007; Zhang and
D07-1098
Yue Zhang and Stephen Clark. 2008a. Joint word seg-
D08-1059 P08-1101
results in Koo and Collins (2010).
P10-1001
Eisner (1996) proposes an O(n3) decoding al-
C96-1058
Zhang and Clark (2008a). Three feature sets are
D08-1059 P08-1101
Model 1 in Koo and Collins (2010).
P10-1001
Yang Liu and Qun Liu. 2010. Joint parsing and trans-
C10-1080 P10-1113
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
P07-1096
(about 97% (Collins, 2002)). Our experimental re-
W02-1001
Terry Koo and Michael Collins. 2010. Efficient third-
P10-1001
Kruengkrai et al., 2009), joint lemmatization and
P09-1058
refers to the results of Duan et al. (2007). They
D07-1098
Yue Zhang and Stephen Clark. 2008b. A tale of two
D08-1059 P08-1101
Ryan McDonald. 2006. Discriminative Training and
E06-1011
POS tagging (Toutanova and Cherry, 2009), joint
P09-1055
et al., 2005). Following the setup of Duan et al.(2007), Zhang and Clark (2008b) and Huang and
D07-1098 D08-1059 P08-1101
and Shouxun Lin. 2010. Joint tokenization and trans-
P10-1113
arc-standard strategy (Nivre, 2008).
J08-4003
Junhui Li, Guodong Zhou, and Hwee Tou Ng. 2010.
P10-1113
much less training time (Shen et al., 2007). Sec-
P07-1096
Clark, 2008b; Huang and Sagae, 2010). In this pa-
P10-1110
ging (Zhang and Clark, 2008a; Jiang et al., 2008;
D08-1059 P08-1101 P08-1102
Llu´ısM`arquez, andJoakimNivre. 2008. TheCoNLL-
J08-4003
we extend decoding algorithms of McDonald et al.(2005)andKooandCollins (2010), andpropose two
P05-1012 P10-1001
and machine translation (MT) (Dyer, 2009; Xiao et
N09-1046
Xavier Carreras. 2007. Experiments with a higher-
D07-1101
