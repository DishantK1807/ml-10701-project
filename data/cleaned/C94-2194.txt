COMMUNICATING WITH MULTIPLE AGENTS* Elizabeth A.
Hinkelman and Stephen P.
Spackman Deutsches l'~orschungszentrum f/it Kfinstliche lntelligenz Sl;uhlsatzenhausweg 3, 66123 Gerlmmy hinkelman@dtki.uni-sb.de, stephen@_acm.org Abstract Previous dialogue systems have focussed on dia.
logues betwe(:n two agents.
Many ~q)plications, however, require conversations between several l)articipants.
This paper extends speech act deftnitions to handle multi-agent conversations, based on a model of multi-agent belief attribution with some unique properties.
Our approach has |,lie advantage of capturing a lnnnlmr of interesting phenomena in a straightforward way.
Motivation The rise of spoken language NI, P applications has le.d to increased inte.rest in such issues as real time pro-cessing and on-line error recovery.
But dialogue is an inherently online process; this manifests in such linguistic phenomena as turntaking \[Sacks et al., 1974\], repair \[Schegtoff et el., 1977\], and content grounding \[Chu:k and Schaefer, 1989\].
Grounding is the phenonlenon that establishes shared beliefs, lbr which simply making or hearing an utterance does )tot suffice, It makes hearers into hill participants who actively signal success or failure of communication, as in this excltange: Steph: |;hat's friday at seven then.
Lgnn : at, seven.
Our long term goal is to show how, Ji'om the perspective of a pattie|peril, one plans and acts in an environment with other communicating individuals, even when those other individuals are not perfectly reliable, and ewm when the groups involw~.d may be large enough that it is impractical to model all participants.
For examl)le, consider this familiar exchange, from the point of view of someone who remembers that |;tie next grou I) meeting is on tuesday: Jan : so we should dro I) the ram.
cancel tit('.
meeting on thursday.
Les tuesday All,; tuesday Lou yeah.
Jan \[yes, right.
Ilere both our sub jeer, and another participant of_ fer a correction, which is confirmed by l,ou and by the original speaker.
Other participants may I)e pre.sent.
In this paper, we focus on the elfects of comnmnicative actions on the particil)ant's model of the situat, ion.
In contrast with previous diMogue work, we are *The wm'k underlying this paper was suppovlx,.d by a resi!al'(:h gratl(;, \])'KZ I'FW 9002 0, fl'om t, he Gel'tn&n thmdesnfinisterium ffir 1,'orschung mid Technologie to the 1)FK1 project DISCO.
interested ill czuses whe.re there are more than three agents, in a group of ten or more, it is hard to imagine.
how a participant can track the beliefs and disbeliet~s of others accurately; it, may not even be practical to track who they all are.
The advantages of analysing natural language utterelites ~us coilimnnicative actioi,s are by now well unde> stood; they serve to slmnnarise conversations for longterm storage \[Schupeta, t993\], as a basis for generation \[Moore and Paris, 1989\], in top--down prediction of utterance flmction and structure \[Alexanderssou el el., 1!)94\], and most importantly, to provide a representation of natural language utterances that is uniform with that used in general facilities for planning and action \[Allen, 1)83\].
We tollow \[Traum and llinkehnan, 1992\] in regarding speech acts as fully joint actions between conversational participants.
Not only are joint speech acts co..operatively undertake.n, but they have at least nora-.
|nelly.joint etDcts: if they complete but still fail to result in shared goals or shared beliefs, this should be attributable to politeness, dishonesty (of.
\[Perrault, 1991)\]), or other social funelions.
This perslmctiw; on speech act processing forces us to deal with issues of jointly hehl goals and beliefiq at a w.'ry b~usic level.
These matters are by now quite wellstudied, but analytic solutions from logical tirst prim cipies tend to be relatiw;ly complex, yielding neither perspicuous notations nor plausible computational or cognitive models.
In short, normative analyses are not necessarily descriptive, ones.
Aside from relatiw~ly involved calculations, there are sew:ral sources of dilliculty: • when nrultiple participants are involved, the nnmI)er of 'belief spaces ) (patterns of modal embedding) tends to blow ul) rapidly; • when the actual state of atl;~irs regarding the extent of others' knowledge is unknown (a~s is the ease \['or an online converse(renal participant) the nmnber of cases to be considered can become large;,, when dealing with large organisations, some form of aggregate modelling I)ecomes an absolute necessity.
Consider, for instanee, (;lie case in which you believe that the governnieut knows your iltcome, frol-n lash year.
What you believe is not that each individnal governmen| mnl)loyee knows it, bnt that anyone from tile tax department, wl,ose 1)usiness it is to know, and who actually wants to, will.
Thus we would typically as~';ilnie that an employee of the tax department who, in a professional capacity, lnakes contact, with your accountant, would actually have this information to hand.
We want to abstract away from the commuui1191 cation channels that make this possible while retaining the availability of the conclusion; and we would ideally like to do so in a manner consistent with the needs of online dialogue processing.
In the next section we describe a method of representing the information necessary for processing multiparticipant speech acts, one which treats groups as agents in their own right, and does so in such a way that speech acts can operate on them naturally and directly.
Corporate Agents and Attitude Propagation The basis of our model is the corporate agent.
These 'agents' represent groups, but, like individual agents, they may have beliefs (which we write BagentP ) and goals (GagentP) ascribed to them directly.
Thus, they can be thought of as intermediate in status between sets of component agents and prototypical group members.
They differ from simple sets in three striking ways: first, they are intensional structures that may be distinct even when co-extensive (as, for example, when the members of the marketing department form a volleyball team on tuesday nights); second, attitudes are ascribed to them directly, and potentially at variance with any attitudes we might a.scribe to their members, or other subagents (see the discussion section); and third, that (other than perhaps in the case of a 'real', singleton agent) subsethood is not a sufficient condition for subagency--some intramural volleyball teams are clearly, as social entities, agents of the company, and others rather the reverse.
While not in a position to make detailed psychological claims, we believe that structures of this kind are compatible with what we know about the linguistic and cognitive handling of aggregates in other contexts.
These corporate agents will be used to represent both long-term social groups and transient groups of conversational participants.
(sfe~veJa~y~S also a subagent ~ 17 eryonc D_J =t U' U U Figure 1: Agents In the remainder of this paper we illustrate relationships between agents with diagrams such as that in figure 1.
Here the playing-card shapes represent agents and the heavy lines connecting them the subagent relation (_): the agents include the system itself (sec.Jan), the system's boss (Jan), Jan's office, their coworker Les, their common corporate employer (WidgetCorp), and another 'random' person, Steph, who does not belong to WidgetCorp.
Later we will represent attitudes ascribed to agents by small stlapes within the playing cards and their propagation by thin curved arrows between them.
Note that since we are discussing the system's own representation of the world, the double-bordered playing card really represents the system's self-model and the whole diagram the system; but we shall not belabour the point here, assuming that information passes freely between the two.
The model we use to compute the transfer of attitudes between agents is approximate, partly to simplify computation and partly because it is in any case unusual to have enough empirical data to compute an exact solution.
The same model (with parametric variation in the domain knowledge) is applied to all the attitudes the system ascribes to agents; in our present implementation, these may be either beliefs or goals) Unlike representations based on conventional logics of belief, it does not introduce nested contexts unless they are explicitly represented in the content of utterances (as would be the case with a sentence like "But Lynn thinks we should have the meeting anyway."), though extended reasoning processes may compatibly do so.
In this simplified model, the propagation of attitudes is performed lazily, as determined by their semantic relevance to the varions agents involved.
Ideally, this judgment would derive from social worldknowledge and information about the purposes of groups; in our current implementation it is approximated using a simple classification of corporate agents, participant roles and message topics into the domain ontology.
Delays and chance are not modelled; all relevant attitudes are presumed to propagate between agents, subject to the following constraints: idgetCorp "'"/\] ~ i'"'"'"'">'x., impossible/ / /..
",, 7L27 "°" ~_~'sec'Jan UJan OSteph Figure 2: Common context constraint 1Since our nlodel is not analytic we do not want or need a notion of 'knowledge': the system lacks direct access to empirical truth and to social consensus, and does not have the cognitive sophistication to validate argmnents against stone independent notion of rationality.
In short, none of the usual theories of truth can in principle be made to apply.
1192 l.
Attitudes propagate only between superageut and subagent (or vice-versa).
This stipulation anlounts to saying that comnmnication only occurs between agents in the presence of some common social context.
Of course, new corporations can be introduced when new social groups form.
Thus in ligure 2, beliefs ascribed to WidgetCorp can propagate to the subagents, Jan and Jan's electronic secretary; but they do not reach Steph, who is not a subagent of WidgetCorp.
~ We use the convention that attitudes are drawn filled in if they are known by direct evidence, hollow otherwise; and that dotted structures are 'negated'---they do not arise ms drawn because they violate some constraint under discussion.
~i filet'Jan '''' conflicting prior J ~blockedbelief ~____~sec'Jau W Jan Figure 3: Propagation as default 2.
Attitude propagation is only a default (the particular choice of default logic need not concern us here).
If there is direct evidence for ascribing a contrary attitude to an agent, propagation from an external som:ce is inhibited.
This property is crucial to modelling dishonesty, negotiation, compromise, and atypical group members in general.
Such blocking is illustrated in figure 3.
In this ease our model of Jan fails to inherit a goal from otfice.Jan because it conflicts with another goal (the square box) for the ascription of which to Jan we have prior independent evidence.
3. The system may never assume that its own attitudes antomatically propagate upwards to a snperagent.
The ut)ward attitude propagation path models tt,e effect of external agents independently attending to their various communicative goals, but the system nmst still plan--and executeits own actions.
Thus, in figure 4 the system--see.Jan--is prohibited from simply assuming that its own beliet~ are shared by its employer, though those of fellow employees would be propagated when otherwise consistent.
(Some hmnans seem to relax this constraint).
2In order to place some limit on the promiscuity of attitude propagation, it seems best to insist that indirect transfer must occur through a siligle agent that is a transitive Sllperor Sill)agellt of both terniinal agents, Thus, even if Jan and Steph both belonged to some peer of WidgetCorp with a similar seniantic domain, propagation would still be not permitted along the resulting N-shaped path.
Gonlmon membership in Everyone will not transmit beliefs eithe.r, because its relewuice tilter is maximally restrictive.
/ ffice.Jan //,// \ requires explicit action 2 U'" Figure 4: The exceptional natnre of self ~ office.Jan Figure 5: The need for speech 4.
Nonce corporations, introduced dynamically to represent the temporary grouping of participants in an active conversation, never inherit attitudes from their subagents, but must acquire them <as the effects of observable actions.
The idea here is that while participating in (or directly observing) a conversation, the system is in a position to observe the construction of the public record of that conversation \[Lewis, 1983\] directly, and this record consists exactly to the attitudes we wish to ascribe to tile conversational group itself.
In conversation even a new 'unspoken understanding' should be based on inference fi'om observed communication, and not just the system's private beliefs about other participants' views.
The fact that we still permit conversational groups to inherit from superagents allows us to place a discussion within a social context that supplies shared background a,ssumptions.
The fact that we permit their subagents t,o inherit from them models the actual adoption of information from the public record by individual participants, including the system itself, without additional mechanism.
Figure 5 depicts this situation: noncel, the collversational grouping, represents a shared social constrnct distinct from our understanding of Jan's private views.
This allows us to deal gracefully with the situation in which we, see.Jan, catch (or perhaps even conspire with) Jan in telling a lie.
The most important property of this model of attitude ascription is that the only belief spaces it im troduces are those that are independently motivated 1193 by identified social groupings or the records of the ac~ tual conversations in which the system participates.
This reduces the chance that the system will become mired in irrelevant structural detail, and specifically avoids the 'spy novel' style of belief space nesting that is characteristic of cla,ssical normative models.
Attribution by default inference allows an individual to be represented as a member of several different groups holding conflicting beliefs, and inheriting only those beliefs consistent with those represented as being held privately.
The results are thus substantially different from those obtail, ed in classical logics \[Allen, 1983; Kraus and Lehmann, 1988; Appelt, 1985; Cohen and Levesqne, 1990\].
They differ from other path-based algorithms \[Ballim and Wilks, 1991\] in the provision of semantic relevance conditions and in addressing the need for shared attitudes by ascribing them directly to groups, rather than by maintaining complex accounts of which agents believe what.
This allows us to describe and process conversational mechanics withont recourse to nested (x believes that y believes that)... belief spaces, though snch structures may remain necessary for other, less routine feats of cognition.
In the next section we show how our model of attitude ascription can be used to implement multiparticipant speech act processing.
Multiparticipant Speech Acts As in \[Traum and Hinkelman, 1992\], we assume that a core speech act is ultimately realised by a sequence of utterances that ernbody the grounding process.
The model requires thai; the definitions of the speech acts themselves abstract away from grounding and provide high level actions that can be integrated with nonlinguistic domain actions in planning.
Using our multiagent attitude attribution mechanism, we can simplify matters fllrther, defining speech acts as joint actions whose effects apply directly to the conversational group being modelled.
Consider the generalised action operator representing one simple core speech act: Informal) conditions : BbPA b K aAlivea effects : Bap This Inform is a true joint action.
Agent a is the nonce corporation representing all the participants taken jointly (the live predicate requires that this nonce correspond to an a ongoing conversation).
Though the singleton subagent b is the source of the information, the action has its effect directly on our model of the group.
From that point propagation downwards to the individual participants is a function of the attitude ascription model, and is subject to the constraints given above.
(The system effectively assumes that corresponding updates actually take place in the minds of the conversational participants).
The correctness of this formulation relies on two facts.
The first is that the grounding structure realis3Our current implementation actually deals with (mail rather than live speech, and must cope with mul(.iple acl;ive dialogues.
ing the core speech act operator ensures the content is successfully conveyed.
The second is that if a speech act that has an efl~ct on the conversational gronp is flflly realised and properly grounded, then any hearer who dissents from those effects must explicitly act to cancel them.
That is, acknowledgement of receipt of a message establishes a presumption of assent to the content.
Note, however, that when the speech act remains unchallenged this means only that the conversational participants will let it stand as part of the public record; it does not mean that they are tndy persuaded of its content, and the rules we have given only predict that they adopt it if there is no evidence to the contrary.
St, ecessful requests have effects on the goals rather than the beliefs of the group.
It is crucial that both communicative and noncommnnicative are introduced.
The first goal below is noncommunicative and represents simply that the requested action be performed.
Note that although the requested action's (possibly corporate) agent participates in the dialogue, there is no restriction that it not include the requester.
Writing Bifap for Bop V Ba~p and O for eventually, we have Requestae conditions : agent e E a A live a effe.cts : (~a<~ e A GaBifa@ e The second goal in the effects is a communicative one; the group acquires the goal of finding out whether the requested action will be performed.
The consequence of this is that the requester gets an indication of whether the request was successful: even lmder the assunrption of co-operativity, goal conflicts and plan constraints sometimes lead to the rejection of a successfully communicated request.
In tile next section we describe how OOSMA, our implemented calendar management system, processes all actual exchange.
Processing an N-Way Speech Act Speech acts like the above can now fignre in the planand inferencebased algorithms of communicative intelligent agents.
Since dialogue may include unpredicted events, such agents must be able to react to changing circumstances rather than relying completely on advance planning.
As each incoming speech act arrives, the agent updates its beliefs and goals; these beliefs and goals are the basis for subsequent action.
This is not only appropriate for the interface between task and dialogue, but absolutely crucial for the grounding process.
A typical application task for Noway speech acts in a multiagent environment is appointment scheduling, with dialogue systems serving as personal appointment secretaries to human agents.
Our implemented system, COSMA, operates in this domain.
We model a human/secretarial pair as a kind of corporate agent in which beliefs about appointments propagate up and down from both members, and in which goals about appointments propagate fl'om the hnman up to the pair and from there down to the secretary.
When this example begins, the dialogue system 1194 (see.Jan) has the role of a personal aplmintment secretary to a human agent (Jan), forming the human/secretarial corporation ofliee..hm.
Jan sends sec.aaa email text; referring to a pre-existing appointInent: Jau: Cancel tit(', meeting with the hardware group.
\[--',sec.a an\] '\]_'he COSMA system interprets this input by first constructing a nonce corporation for the new dialogue, nonce1, with dan, see.Jan E nonce/ E ofl~ce.aan Q =GOcancel~e~.ja.meeting2 ~ office.Jan i =G Blrn~*l~canc°l~'l=" meeling2 :~.
Transmission by core al~eech act i~ L _~ ec.Jatl Jan Figure 6: Making a request All members of a nonce corporation inherit belief~ and communicative goals from it;.
The interpretation of the first utterance as a speech act is: RequestJan, sec.Jan} Cancel sec.danmeeting2 This interpretation is checked for consistency with context according to the method of \[llinkelmau, 1992\], and forms an acceptable reading.
Its effects on the group are asserted (Q, @ in figure 6): G norn:e l ~ ca heel see.dan meeting2 G nonce l B if nonce l 0 cancelsec.3a n meeting2 _~officc..lau ~ =(y ~'cancol 8ec,l~n lnteetlllg 2 I=GB r Ocancel. meethlg2 A =Ocancol ~eaanltlemhng2 / / g, Figure 7: t/.esponding When it has finished processing all inputs, the system exalnines its own goals in order to determine what actions, if any, it will perform.
It finds no immediate priwtte goals, but there are two that it inherits.
Because it is a participant; in the ongoing discussion with Jan it inherits the nom'e's communicative goal gif...
(@ in figure 7).
It also inherits the goal to ensure that the cancellation actually does happen (@)4 (A less compliant agent thau the current COSMA wouhl not acquire non-communicative goals directly from the nonce, but would obtain the cancellation goal indirectly through office.Jan.
The implementation could be w.'ry similar, because the indirect inheritance path can be compiled out when the nonce is initially constructed).
The dialogue system thus retrieves the following goals: Gsec.Jan ~ canc elsec.Janmeeting2 Gsec.,lanBifnoncel 0 cancelsec.danmeeting2 These goals become input for the planning process.
The first goal can be achieved in the current context by ltrst opening the appointment file, then performing a stored subplan \['or cancelling appointments that includes modifying the database entry and notifying the participants.
Our reactive algorithm allows comnlunicative phms to be freely embedded within domain actiotls, and vice versa.
ltaving found this sequence of actions, the system now knows that the ~> ...
part holds.
It is therefore able to plan to Informnoncel(O ...), satisfying the second goal (@).
The output for the second goal is: hfformnoncel (O ca ned see.dan meeting2) see.,lan: Ok, I'll cancel it.
\[--~-3all\] WidgctCorp Figure 8: Informing the group Finally, it must conlph.'te the execution of its plan to satisfy the first goal by updating the appointment file and notifying MI participants.
The notification step involves constructing a suitable conversational I:tOltce, this time a descendant of WidgetCorp itself (in spo ken dialogue this requires, aside front setting up the necessary internal data structures, meeting the addressees and greeting them; when communicating via email the analogous requirement is just composing a suit~ble mail message header).
Then, as show in figure 8, the system initiates a further Inform action of its OWll', Informnonce4 (-,7) meeting2) whk:h (:an be verbalised as follows: see.dan: The meeting of Monday, Feb.
13 at 3 I'M will not take place.
\[~sec.aan, Jan, gou, Le G Lee\] 4Nol.e that if the system were asked, it could now inflw that, Jan also has these goals, but tohat this is not part of the speech act interpretation algorithm itself.
1195 Discussion An important property of the corporate agent model presented in this paper is its scaling behaviour.
Although the number of 'agents' in a nontrivial world model may be large, we only introduce belief spaces corresponding to 'actual' objects about which the system has knowledge.
In particular, the corporate agents that are used correspond to either durable social gronpings or records of actual conversations.
Individual speech act definitions, though they acconnt for all the agents in the dialogue, need make reference to at most two agents.
In contrast with normative models, our speech act processing model at no point requires that individual addressees be modelled.
Of course, dialogue is typically motivated by the desire to modify the addressees' mental states, but our system is free to make these updates on demand.
Thus, so long as constructing detailed partner models is not independently necessary, the effort required to plan and respond to speech acts remains almost constant as the number of conversational participants grows.
We have thus achieved the extension of speech acts to multiagent environments, a step beyond other speech act based models\[Dols and van der Sloot, 1992; Meyer, 1992; Bunt, 1989; Traum and Ilinkehnan, 1992\].
In the process, we have reduced the complexity of the task/dialogue interface.
WidgetCorp ~~ ~'I nonce5 Figure 9: WidgetCorp expands An interesting consequence of not needing to nlodel all members of a conversational group is that it becomes unnecessary to identify them.
While in some circumstances this may be an advantage, it does leave the door open to an interesting glitch: without an independent check, the system's model of who it is addressing may turn out to be inaccurate.
A related thing can happen when the system plans on the basis of attitude propagation: it can perform an action that 'ought' to result in a given agent's coming to hold some view through social processes, but since social channels are quite imperfect, the message never gets through.
Human agents are at times more cautious, and may model delays in the grapevine, but this 'lost sheep' phenomenon occurs sufficiently ofl;en in real life to make the utterance "Oh, I'm sorry, I guess you weren't at the meeting".
sound very familiar.
Generalisatiou remains a hard problem, of course.
Our system has no special advantages when faced with a question like "Are conservatives porcupine-lovers"?
Vague questions about large groups require extensive search or some independent theory of generalisation, and seem to be difficult even for hunians.
Related to this is the Nixon diamond anomaly faced by many default inference systems.
In our case, when we find propagation paths that will support the ascription of contradictory attitudes to a single agent, how should we choose between them?
It turns out that selecting whichever result we first discover we would like to use is a surprisingly good solution.
Such 'arbitrary' judgments tend to facilitate the conversation by using the inference mechanism not to seek a reliable proof but to find the most convenient supportable argument, regardless of actual truth.
Perhaps the most interesting deviation of our model from the behaviour of systems founded on mutual belief is the social fiction anomaly: one can fairly e~ily reach a state in which an attitude is ascribed to a corporation which is held by none of its members.
Incredibly, this also corresponds to a familiar situation in everyday life.
Three examples should serve to illustrate the point.
In the first place, consider this exchange, in which Jan asks Les to compile a tedious report: Jan: I'll need that on my desk by friday.
Les: friday, no problem.
Such a dialogue may occur even when Jan will not have time to look at the report until the middle of the following week, and Les knows that the work cannot possibly be completed before the weekend.
'~ Secondly, we propose the example of a couple who are jointly but not severally on a diet.
When together, neither partner ever takes dessert, and this policy is verbally reinforced.
Yet either of' them will happily join you in eating a large slice of strawberry cheesecake, if they are apart.
Finally, imagine that you are a lone bicyclist approaching a Very Large Hill.
You might now say to yourself--a conversational nonce of one--"It's not far to the top"!
Processing this speech act results in another unsupported belief.
You can now try to be convinced.
In light of all of tile above anomalies, it begins to appear that human agents may be struggling with limitations similar to those of our own model.
It may still be objected that onr model falls short in failing to support the detailed 'spy novel' reasoning used in conventional logics of belief, keeping track of whether Loll believes that Lee believes that p, and whether or not common beliefs are truly mutual.
Our response is threefold: * Reflective l)rol)lem solving is always an option, but in humans appears to be an independent process.
Responsiveness demands make it unsuitable for mandatory online use m dialogue processing, though it may be important to use models (like ours) with which it can be integrated simply.
* Conversational mechanisms have evolved to cope with the cognitive shortcomings of humans, qb the SThe authors disagree as to whether t, his par~,iculm' pattern is more likely to arise througla malice or optinlism.
7196 extent that the pertBrmance errors of a dialogue agent mirror huinan failings, co-operatiw'.
recovery performance wil;h hlltrla\[l communicative tools should be enhanced.
• Even given access to an ideal normative dialogue model, a fltll system would benetit li'om running a less precise and lnore descriptive model in parallel.
This would a~ssist in isolating those parts of a communicative plait where confllsioll on tim part of other agents is predictable.
Corporate agents are an alternative to normative logics of belief which capture a mind)or of interesting social and commtmicative phenonmna straightforwardly.
With their help, we can refornmlate core speech act delinitions cleanly and seahd)ly tbr the case of nrany agents.
The level of planning abstraction that results seems well-suited to the needs of intelligent communicative agents operating in an environment that in-.
(:htdes I/laity lnlnlall agents.
References \[Alexandersson ctal., 1994\] Jan A lexandersson, l';lisalmth Maier, and Norbert leithinger.
A robust and eflicient three-layered dialog component for a speech-to-speech translation system, subm.
(Jonference on Applied Natural Language Processing, Stuttgart, 1994.
\[Allen, 1983\] James Allen.
ll.eeognizing intentions from natural langm~ge utterances.
In Michael Brady and lobert C.
Berwiek, editors, Computalioual Models of Discourse.
MIT Press, 1983.
\[Appelt, 198'5\] D.
E. Appelt.
Planning English S'entences.
Cambridge University l'ress, Can,bridge, 1985.
\[Ballim and Wilks, 1991\] At~al Ballim and Yorick Wilks.
Arti\]icial Believers: The Aseriplion of Helief.
Lawrence I';rlbaum Assoeiates~ 1991.
\[Bunt, 1989\] II.
C. lhmt.
Information dialogues as communicative action in relation to t)artner modelling and inforntation l)roeessing.
In M,M Taylor, 1".
Neel, and 1).
G. llouwhuis, editors, 7'he Struetm'e of Multimodal Dialogue.
Elsevier Science Publishers I/.V., :1989.
\[Clark and Schaefer, 1989\] flerbert \[1.
('lark and l';d ward F.
Schaefer. Contrilmting to discourse.
(,'oquilive Science, 13:259 294, 1.98!/.
\[Cohen and Lew'.sque, 11990\] Phillip R.
Cohen aud Hector J.
Levesque. I'ersistence, intention, and eolnmitment.
In P.
R. Cohen,,/.
Morgan, and M.
E. Pollack, editors, Intentions in Communication.
MIT Press, 1990.
\[l)ols and van der Sloot, 1992\] I,'.
J. I1.
l)ols and K.
van der Sloot.
Modelling mutual elfi~et,; in beliet: based interactive systems.
In W.
J. Black, G.
Sabah, and T.
J. Wachtel, editors, Abduetion, Beliefs and (;ontext: Proeeedings of the seeotM 1'2,S'111117 ' PLUS' workshop in computational pra.qmatie.s, 1992.
\[llinkehnan, 1992\] Elizabe.th A.
llinkehnan, intonation and the Request/(,~uestion l)istinetiou.
In Proceedings of the h~ternational Co,fi~reuce on,~,'poken Language Processing, P, anlf, Canada, 19!12.
\[Kraus and l,ehmann, :1988\] Sarit graus and DauM l,ehmann.
Knowledge, belief and time.
In "\['hcore& ical (Jomputer,5'cieuce, vohune 58, pages 155-174, 1988.
\[Lewis, 198:t\] I).
Lewis. Scorekeeping in a Language Game, volume 1, chapter.
13. Oxford University Press, Oxford, 1983.
l)hilosophical Papers.
\[Meyer, 1992\] lt.alph Meyer.
Towards a conceptual model of belief and intention in dialogue situations.
In W.
J. Black, G.
Sabah, and T.
J. Wachtel, editors, Abduction, Beliefs and Context: PTveeedings of the second E,qPRIT PLU,q workshop in computational pragmatics, 1992.
\[Moore and Paris, 1989\] J.D.
Moore and C.L.
Paris. Planning text for advisory dialogues.
In Proe.
Conf of the 27th Ammal Meeting of the ACL, pages 203211, Varmouver, l !/8!).
\[Perrault, 1!)90\] C.
II. l'erranlt.
An al)plication of default logic to speech act theory.
In P.
\[{. Cohen, J.
Morgan, ~tnd M.
E. Pollaek, editors, Intentions in Communication.
MIT Press, 1990.
\[Sacks et al., :1974\] 11.
Sacks, E.A.
Schegloff, and (I.A.
Jef\[hrson. A simplest systematies for the organization of tin:n-taking in conversation.
Language, 50:696 7a5, 1974.
\[Schegloff el al., 1977\] E.
A. Schegloff, G..letferson, and I I.
Sacks. The preference for self correction in the organization of repair in conversation.
Language, 5:t:36l,'t82, 1977.
\[Schnpeta, :199:{\] Achinl W.
Sclmpeta. Cosma: F, in verteilter terminplauer als fidlstudie der verteilten ld.
In Jucrgen Mueller and l)onald,qteinel:, editors, lk'oopericrende A.qenten, Saarbriieken, Get:many, 1!19:t.
\[Tranln and llinkehnan, 19!12\] I)avid 11..
Traum and Elizabeth A.
Ilinkehnan. Conversation acts in taskoriented spokeu dialogue.
Computational Intelligence, 8(:11:,575 .599, 1992.
Special Issue on Nonliteral language. 1197

