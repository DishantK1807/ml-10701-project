models, Johnson (2007) shows that VB is more ef-
D07-1031
Robert C. Moore. 2003. Learning translations of named-
E03-1035
Franz Och and Hermann Ney. 2003. A systematic com-
J03-1002 P03-1021
and Vogel et al. (1996). As these word-level align-
C96-2141
Daniel Marcu and William Wong. 2002. A phrase-based,
W02-1018
Mark Johnson. 2007. Why doesnâ€™t EM find good
D07-1031
Hao Zhang and Daniel Gildea. 2005. Stochastic lexical-
P05-1059
David Chiang. 2005. A hierarchical phrase-based model
P05-1033
conditional generative models of Brown et al. (1993)
J93-2003
Cherry and Lin (2007) use GIZA++ intersections
W07-0403
like Johnson (2007), who found optimal perfor-
D07-1031
Franz Josef Och. 2003. Minimum error rate training in
J03-1002 P03-1021
Goldwater and Griffiths (2007) and Johnson (2007)
D07-1031 P07-1094
Dekai Wu. 1997. Stochastic inversion transduction
J97-3002
(Och, 2003) over BLEU was used to optimize the
J03-1002 P03-1021
Franz Josef Och and Hermann Ney. 2004. The align-
J04-4002
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
W07-0403
bles smoothing. As pointed out by Johnson (2007),
D07-1031
Sharon Goldwater and Tom Griffiths. 2007. A fully
P07-1094
(Och and Ney, 2003) following a regimen of 5 it-
J03-1002 P03-1021
tion Grammar (Wu, 1997) as the generative frame-
J97-3002
and Chiang (2005) with the additional constraint
P05-1033
Compositional Constraint of Cherry and Lin (2007).
W07-0403
in Moore (2003) and Vogel (2005). Like Zhang and
E03-1035
according to the definition of Och and Ney (2004)
J04-4002
