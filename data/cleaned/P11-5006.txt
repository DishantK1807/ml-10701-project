Dual Decomp osition
for Natural Language Pro cessing
Alexander M. Rush and Michael Collins
Deco ding complexit y
fo cus: deco ding problem fo r natural language tasks
y = arg max
y
f (y )
motivation:
 richer mo del structure often leads to imp roved accuracy
 exact deco ding fo r complex mo dels tends to b e intractable
Deco ding ta sks
many common problems are intractable to dec o de exactly
high complexit y
 combined pa rsing and pa rt-of-sp eech tagging (Rush et al.,
2010)
 \lo op y" HMM pa rt-of-sp eech tagging
 syntactic machine translation (Rush and Collins, 2011)
NP-Ha rd
 symmetric HMM alignment (DeNero and Macherey, 2011)
 phrase-based translation
 higher-o rder non-p rojective dep endency pa rsing (Ko o et al.,
2010)
in practice:
 app ro ximate deco d ing metho ds (coa rse-tone, b eam sea rch,
cub e pruning, gibbs sampling, b elief propagation)
 app ro ximate mo d e ls (mean  eld, va riational mo dels)
Motivation
cannot hop e to  nd exact algo rithms (pa rticula rly when NP-Ha rd)
aim: develop deco ding algo rithms with fo rmal gua rantees
metho d:
 derive fast algo rithms that provide certi cates of optimalit y
 sho w tha t fo r practic a l instances, these algo rithms often yield
exact solutions
 provide str ate gi e s fo r imp roving solutions or  nding
app ro ximate solutions when no certi cate is found
dual decomp osition helps us develop algo rithms of this fo rm
Dual Decomp osition (Komo dakis et al., 2010; Lema r echal, 2001)
goal: solve com pli c a ted optimization problem
y = arg max
y
f (y )
metho d: decomp ose into sub problems, solve iteratively
b ene t : can cho ose decomp osition to provide \easy" subp roblems
aim fo r simple and e  cient combinato rial algo rithms
 dynamic programming
 minimum spanning tree
 sho rtest path
 min-cut
 bipa rtite match
 etc.
Related work
there are related metho ds used NLP with sim ila r motivation
related metho ds:
 b elief propagation (pa rticula rly max-p ro duct) (Smith and
Eisner, 2008)
 facto red A* sea rch (Klein and Manning, 2003)
 exact coa rse-tone (Raphael, 2001)
aim to  nd exact solutions without explo ring the full sea rch space
Tuto rial outline
fo cus:
 developing dual d e comp osition algo rithms fo r new NLP tasks
 understanding fo rmal gua rantees of the algo rithms
 extensions to imp rove exactness and select solution s
outline:
1. w ork ed algo rithm fo r combined pa rsing and tagging
2. imp ortant theo re ms and fo rmal derivation
3. mo re examples from pa rs ing, sequence lab eling, MT
4. practical c onsid e ra tions fo r implementing dual decomp osition
5. relationship to linea r programming re la xations
6. further va riations and advanced examples
1. W orked example
aim: w alk thr ough a dual decomp osition algo rithm fo r combined
pa rsing and pa rt-of-sp eech tagging
 intro duce fo rmal n otation fo r pa rsing and tagging
 give assumptions necessa ry fo r deco ding
 step through a run of the dual decomp osition algo rithm
Combined pa rsing and pa rt-of-sp eech tagging
S
NP
N
United
VP
V
 ies
NP
D
some
A
large
N
jet
goal:  nd pa rse tree that optimizes
sco re (S ! NP VP ) + sco re (VP ! V NP ) +
::: + sco re (United 1;N ) + sco re (V;N ) + :::
Constituency pa rsing
notation:
 Y is set of constituency pa rses fo r in put
 y 2Y is a valid pa rse
 f (y ) sco res a pa rse tree
goal:
arg max
y2Y
f (y )
example: a context-free gramma r fo r constituency pa rsing
S
NP
N
United
VP
V
 ies
NP
D
some
A
large
N
jet
Part-of-sp eech tagging
notation:
 Z is set of tag sequences fo r input
 z 2Z is a valid tag sequence
 g (z ) sco res of a tag sequence
goal:
arg max
z2Z
g (z )
example: an HMM fo r pa rt-of sp eec h tagging
United 1 flies 2 some 3 large 4 jet 5
N V D A N
Identifying tags
notation: identify the tag lab els selected by each mo del
 y (i;t ) = 1 when pa rse y selects tag t at p osition i
 z (i;t ) = 1 when tag se qu e n c e z selects tag t at p osition i
example: a pa rse and tagging with y (4 ;A ) = 1 and z (4 ;A ) = 1
S
NP
N
United
VP
V
 ies
NP
D
some
A
large
N
jet
yUnited
1 flies 2 some 3 large 4 jet5
N V D A N
z
Combined optimization
goal:
arg max
y2Y ;z2Z
f (y ) + g (z )
such that fo r all i = 1 :::n , t 2T ,
y (i;t ) = z (i;t )
i.e.  nd the b est pa rse and tagging pair th at agree on tag lab els
equivalent fo rmulation:
arg max
y2Y
f (y ) + g (l(y ))
where l : Y ! Z extracts the tag sequence from a pa rse tree
Dynamic programming inter sec tion
can solve by solving the pro duct of th e tw o mo d e ls
example:
 pa rsing mo del is a context-free gramma r
 tagging mo del is a  rst-o rder HMM
 can solve as CF G an d  nite-state automata interse ction
replace S ! NP VP with
SN ;N ! NP N ;V VP V ;N
S
NP
N
United
VP
V
 ies
NP
D
some
A
large
N
jet
Parsing assumption
the structure of Y is op en (could b e CF G, TA G, e tc.)
assumption: optimization with u can b e solved e ciently
arg max
y2Y
f (y ) +
X
i;t
u (i;t )y (i;t )
generally b enign since u can b e inco rp orated into the structure of f
example: CF G with rule sco ri ng function h
f (y ) =
X
X!Y Z2y
h (X ! Y Z ) +
X
(i;X )2y
h (X ! w i )
where
arg max y2Y f (y ) +
X
i;t
u (i;t )y (i;t ) =
arg max y2Y
X
X!Y Z2y
h (X ! Y Z ) +
X
(i;X )2y
(h (X ! w i ) + u (i;X ))
Tagging assumption
w e mak e a simila r assumption fo r the set Z
assumption: optimization with u can b e solved e ciently
arg max
z2Z
g (z )  
X
i;t
u (i;t )z (i;t )
example: HMM with sco res fo r transitions T and observations O
g (z ) =
X
t!t02z
T (t ! t0) +
X
(i;t )2z
O (t ! w i )
where
arg max z2Z g (z )  
X
i;t
u (i;t )z (i;t ) =
arg max z2Z
X
t!t02z
T (t ! t0) +
X
(i;t )2z
(O (t ! w i )  u (i;t ))
Dual decomp osition algo rithm
Set u (1) (i;t ) = 0 fo r all i , t 2T
F or k = 1 to K
y (k )  arg max
y2Y
f (y ) +
X
i;t
u (k ) (i;t )y (i;t ) [P arsing]
z (k )  arg max
z2Z
g (z )  
X
i;t
u (k ) (i;t )z (i;t ) [T agging]
If y (k ) (i;t ) = z (k ) (i;t ) fo r all i;t Return (y (k );z (k ) )
Else u (k +1) (i;t )  u (k ) (i;t )   k (y (k ) (i;t )  z (k ) (i;t ))
Algo rithm step-b y-step
[Animation]
Main theo rem
theo rem : if at any iteration, fo r all i , t 2T
y (k ) (i;t ) = z (k ) (i;t )
then (y (k );z (k ) ) is the global optimum
pro of: fo cus of the next section
2. Formal prop erties
aim: fo rmal derivation of the algo rithm given in the previous
section
 derive Lagrangian dual
 prove th re e prop erties
I upp er b ound
I convergence
I optimalit y
 describ e subgradient metho d
Lagrangian
goal:
arg max
y2Y ;z2Z
f (y ) + g (z ) such that y (i;t ) = z (i;t )
Lagrangian:
L(u;y;z ) = f (y ) + g (z ) +
X
i;t
u (i;t ) (y (i;t )  z (i;t ))
redistribute terms
L(u;y;z ) =
0
@f (y ) +
X
i;t
u (i;t )y (i;t )
1
A +
0
@g (z )  
X
i;t
u (i;t )z (i;t )
1
A
Lagrangian dua l
Lagrangian:
L(u;y;z ) =
0
@f (y ) +
X
i;t
u (i;t )y (i;t )
1
A +
0
@g (z )  
X
i;t
u (i;t )z (i;t )
1
A
Lagrangian dual:
L(u ) = max
y2Y ;z2Z
L(u;y;z )
= max
y2Y
0
@f (y ) +
X
i;t
u (i;t )y (i;t )
1
A +
max
z2Z
0
@g (z )  
X
i;t
u (i;t )z (i;t )
1
A
Theo rem 1. Upp er bound
de ne:
 y ;z is the optimal combined pa rsing and tagging solution
with y (i;t ) = z (i;t ) fo r all i;t
theo rem : fo r any value of u
L(u )  f (y ) + g (z )
L(u ) prov ide s an upp er b ound on the sco re of the optimal solution
note: upp er b ound ma y b e useful as input to branch and b ound or
A* sea rch
Theo rem 1. Upp er bound (p ro of )
theo rem : fo r any value of u , L(u )  f (y ) + g (z )
pro of:
L(u ) = m a x
y2Y ;z2Z
L(u;y;z ) (1)
 max
y2Y ;z2Z :y = z
L(u;y;z ) (2)
= max
y2Y ;z2Z :y = z
f (y ) + g (z ) (3)
= f (y ) + g (z ) (4)
Formal algo rithm (reminder)
Set u (1) (i;t ) = 0 fo r all i , t 2T
F or k = 1 to K
y (k )  arg max
y2Y
f (y ) +
X
i;t
u (k ) (i;t )y (i;t ) [P arsing]
z (k )  arg max
z2Z
g (z )  
X
i;t
u (k ) (i;t )z (i;t ) [T agging]
If y (k ) (i;t ) = z (k ) (i;t ) fo r all i;t Return (y (k );z (k ) )
Else u (k +1) (i;t )  u (k ) (i;t )   k (y (k ) (i;t )  z (k ) (i;t ))
Theo rem 2. Convergence
notation:
 u (k +1) (i;t )  u (k ) (i;t ) +  k (y (k ) (i;t )  z (k ) (i;t )) is up date
 u (k ) is the p e n alt y vecto r at iteration k
  k is the up date rate at iteration k
theo rem : fo r any sequence  1; 2; 3;::: such that
lim
t!1
 t = 0 and
1X
t =1
 t = 1;
w e have
lim
t!1
L(u t ) = min
u
L(u )
i.e. the algo rithm converges to the tightes t p ossible upp er b ound
pro of: by sub gradient convergence (next section)
Dual solutions
de ne:
 fo r any value of u
yu = arg m a x
y2Y
0
@f (y ) +
X
i;t
u (i;t )y (i;t )
1
A
and
zu = arg max
z2Z
0
@g (z )  
X
i;t
u (i;t )z (i;t )
1
A
 yu and zu are the dual solutions fo r a given u
Theo rem 3. Optimalit y
theo rem : if there exists u such that
yu (i;t ) = zu (i;t )
fo r a ll i;t then
f (yu ) + g (zu ) = f (y ) + g (z )
i.e. if the dual solutions agree, w e have an optimal sol ution
(yu;zu )
Theo rem 3. Optimalit y (p ro of )
theo rem : if u such that yu (i;t ) = zu (i;t ) fo r all i;t then
f (yu ) + g (zu ) = f (y ) + g (z )
pro of: by th e de nitions of yu and zu
L(u ) = f (yu ) + g (zu ) +
X
i;t
u (i;t )( yu (i;t )  zu (i;t ))
= f (yu ) + g (zu )
since L(u )  f (y ) + g (z ) fo r all values of u
f (yu ) + g (zu )  f (y ) + g (z )
but y and z are optimal
f (yu ) + g (zu )  f (y ) + g (z )
Dual optimization
Lagrangian dual:
L(u ) = max
y2Y ;z2Z
L(u;y;z )
= max
y2Y
0
@f (y ) +
X
i;t
u (i;t )y (i;t )
1
A +
max
z2Z
0
@g (z )  
X
i;t
u (i;t )z (i;t )
1
A
goal: dual problem is to  n d the tightest u pp er b ound
min
u
L(u )
Dual subgradient
L(u ) = max
y2Y
0
@f (y ) +
X
i;t
u (i;t )y (i;t )
1
A + max
z2Z
0
@g (z )  
X
i;t
u (i;t )z (i;t )
1
A
prop erties:
 L(u) is convex in u (no lo cal minima)
 L(u) is not di erentiable (b ecause of max op erato r)
handle non-di erentiabilit y by using su bgradient descent
de ne: a subgradient of L(u ) at u is a vecto r gu such that fo r all v
L(v )  L(u ) + gu  (v  u )
Subgradient algo rithm
L(u ) = max
y2Y
0
@f (y ) +
X
i;t
u (i;t )y (i;t )
1
A + max
z2Z
0
@g (z )  
X
i;j
u (i;t )z (i;t )
1
A
recall, yu and zu are the argmaxâ€™s of the tw o terms
subgradient:
gu (i;t ) = yu (i;t )  zu (i;t )
subgradient descent: move along the subgradient
u0(i;t ) = u (i;t )   (yu (i;t )  zu (i;t ))
gua ranteed to  nd a minimum with conditions given ea rlier fo r  
3. Mo re examples
aim: demonstrate simila r algo rithms that can b e applied to other
deco ding applications
 context-free pa rsing combined with dep endency pa rsing
 co rpus-level pa rt-of-sp eech tagging
 combined translation a lignme n t
Combined constituency and dep endency pa rsing
setup: assume sepa rate mo dels trained fo r constituency and
dep endency pa rsing
probl e m :  nd constituency pa rse that maximizes the sum of the
tw o mo dels
example:
 combine lexicalized CF G with second-o rder dep endency pa rser
Lexicalized constituency pa rsing
notation:
 Y is set of lexicalized constitue n c y pa rses fo r input
 y 2Y is a valid pa rse
 f (y ) sco res a pa rse tree
goal:
arg max
y2Y
f (y )
example: a lexicalized context-free gramma r
S( ies)
NP(United)
N
United
VP( ies)
V
 ies
NP(jet)
D
some
A
large
N
jet
Dep endency pa rsing
de ne:
 Z is set of dep endency pa rses fo r input
 z 2Z is a valid dep endency pa rse
 g (z ) sco res a dep endency pa rse
example:
*0 United 1 flies 2 some 3 large 4 jet 5
Identifying dep endencies
notation: identify the dep endencies selected by each mo del
 y (i;j ) = 1 when constituency pa rse y selects w ord i as a
mo di er of w ord j
 z (i;j ) = 1 when dep endency pa rse z selects w ord i as a
mo di er of w ord j
example: a constituency and dep endency pa rse with y (3 ; 5) = 1
and z (3 ; 5) = 1
S( ies)
NP(United)
N
United
VP( ies)
V
 ies
NP(jet)
D
some
A
large
N
jet
y
*0 United 1 flies 2 some 3 large 4 jet5
z
Combined optimization
goal:
arg max
y2Y ;z2Z
f (y ) + g (z )
such that fo r all i = 1 :::n , j = 0 :::n ,
y (i;j ) = z (i;j )
Algo rithm step-b y-step
[Animation]
Co rpus-level tagging
setup: given a co rpus of se n te nces and a trained sentence-level
tagging mo del
probl e m :  nd b est tagging fo r each sentence, while at the same
time e n fo rcing inter-sentence soft constraints
example:
 test-time d e co ding with a trigram ta gge r
 constraint that each w ord typ e prefer a single POS tag
Co rpus-level tagging
full mo del fo r co rpus-level taggin g
He sa w an American man
The sma rt man sto o d outside
Man is the b est measure
N
Sentence-level deco ding
notation:
 Yi is set of tag sequences fo r input sentence i
 Y = Y1  ::: Ym is set of tag sequences fo r th e inp ut c orpus
 Y 2Y is a valid tag sequence fo r the c orpus
 F (Y ) =
X
i
f (Y i ) is the sco re fo r tagging the whole co rpus
goal:
arg max
Y2Y
F (Y )
example: deco de each sentence with a trigram tagger
He
P
saw
V
an
D
American
A
man
N
The
D
sma rt
A
man
N
sto od
V
outside
R
Inter-sentence constraints
notation:
 Z is set of p ossible assignments of tags to w ord typ es
 z 2Z is a valid tag assignment
 g (z ) is a sco ring function fo r assignments to w ord typ es
(e.g. a ha rd constraint all w ord typ es only have one tag)
example: an MRF mo del that encourages w ords of the same typ e
to cho ose the same tag
z1
man
N
man
N
man
N
N
z2
man
N
man
N
man
A
N
g (z1 ) > g (z2 )
Identifying word tags
notation: identify the tag lab els selected by each mo del
 Y s (i;t ) = 1 when the tagger fo r sentence s at p osition i
selects ta g t
 z (s;i;t ) = 1 when the constrai nt assigns at se n tence s
p osition i the tag t
example: a pa rse and tagging with Y 1 (5 ;N ) = 1 and
z (1 ; 5;N ) = 1
He saw an American man
The smart man stood outside
Y man man man z
Combined optimization
goal:
arg max
Y 2Y ;z2Z
F (Y ) + g (z )
such that fo r all s = 1 :::m , i = 1 :::n , t 2T ,
Y s (i;t ) = z (s;i;t )
Algo rithm step-b y-step
[Animation]
Combined alignment (DeNero and Macherey, 2011)
setup: assume sepa rate mo dels trained fo r English-to-F rench and
F rench-to-English alignment
probl e m :  nd an align m ent that maximizes the sco re of b oth
mo dels with soft agreement
example:
 HMM mo dels for b oth directional alignments (assume co rrect
alignment is one-to-one fo r simplicit y)
English-to-F rench alignment
de ne:
 Y is set of all p ossible English-toF rench alignments
 y 2Y is a valid alignment
 f (y ) sco res of the alignment
example: HMM alignment
The 1 ugly 2 dog 3 has 4 red 5 fur 6
1 3 2 4 6 5
French-to-English alignment
de ne:
 Z is set of all p ossible F rench-to-English alignments
 z 2Z is a valid alignment
 g (z ) sco res of an al ignme nt
example: HMM alignment
Le 1 chien 2 laid 3 a4 fourrure 5 rouge 6
1 2 3 4 6 5
Identifying word alignments
notation: identify the tag lab els selected by each mo del
 y (i;j ) = 1 when e-to-f align m ent y selects F rench w ord i to
align with English w ord j
 z (i;j ) = 1 when f-to-e align m ent z selects F rench w ord i to
align with English w ord j
example: tw o HMM alignment mo dels with y (6 ; 5) = 1 and
z (6 ; 5) = 1
The1 ugly2 dog3 has4 red5 fur6
1 3 2 4 6 5
y
Le1 chien2 laid3 a4 fourrure5 rouge6
1 2 3 4 6 5
z
Combined optimization
goal:
arg max
y2Y ;z2Z
f (y ) + g (z )
such that fo r all i = 1 :::n , j = 1 :::n ,
y (i;j ) = z (i;j )
Algo rithm step-b y-step
[Animation]
4. Practical issues
aim: overview of practical dual decom p osition techniques
 tracking the progress of the algo rithm
 extracting solutions if algo rithm do es not converge
 lazy up date of dual solutions
Tracking prog ress
at each stage of the algo rithm there are several use ful values
track:
 y (k ) , z (k ) are curr e n t dual solutions
 L(u (k ) ) is the current dual value
 y (k ) , l(y (k ) ) is a p otential primal feasible solution
 f (y (k ) ) + g (l(y (k ) )) is the p otential primal value
useful signals:
 L(u (k ) )  L(u (k 1) ) is the dual change (ma y b e p ositive)
 min
k
L(u (k ) ) is the b est dual value (tightest upp er b ound)
 max
k
f (y (k ) ) + g (l(y (k ) )) is the b est primal value
the optimal value must b e b et w een the b est dual and primal values
App ro ximate solution
up on agreement the solution is e xa c t, but this m ay not o ccur
otherwise, there is an easy w ay to  nd an app ro ximate solution
cho ose: the structure y (k
0)
where
k0 = arg m a x
k
f (y (k ) ) + g (l(y (k ) ))
is the iteration with the b es t primal sco re
gua rantee: the solution y k
0
is non-optimal by at mos t
(min
t
L(u t ))  (f (y (k
0)
) + g (l(y (k
0)
)))
there are other metho ds to estimate solutions, for instance by
averaging solutions (see Nedi  c and Ozdagla r (2009))
Lazy deco ding
idea: donâ€™t recompute y (k ) or z (k ) from scratch each iteration
lazy deco ding: if subgradient u (k ) is spa rse, then y (k ) ma y b e
very easy to compute fr om y (k 1)
use:
 very helpful if y or z facto rs na turally into several pa rts
 decomp ositions with th is prop ert y are very fast in practice
example:
 in co rpusle vel tagging, only nee d to recompute sentences
with a w ord typ e that received an up date
5. Linea r programming
aim: explo re the connections b et w een dual decomp osition and
linea r programming
 basic optimization over the simplex
 fo rmal prop erties of linea r programming
 full example with fractional optimal solutions
 tightening linea r program relaxations
Simplex
de ne:
  y is the simplex over Y where  2  y implies
 y  0 and
X
y
 y = 1
  z is the simplex over Z
  y : Y !  y maps eleme nts to the simplex
example:
Y = fy1;y2;y3g
vertices
  y (y1 ) = (1 ; 0; 0)
  y (y2 ) = (0 ; 1; 0)
  y (y3 ) = (0 ; 0; 1)
 y (y1 )
 y (y2 )  y (y 3)
 y
Linea r programming
optimize over the simplices  y and  z instead of the discrete sets
Y and Z
goal: optimize linea r program
max
 2 y; 2 z
X
y
 y f (y ) +
X
z
 z g (z )
such that fo r all i;t
X
y
 y y (i;t ) =
X
z
 z z (i;t )
Lagrangian
Lagrangian:
M (u; ; ) =
X
y
 y f (y ) +
X
z
 z g (z ) +
X
i;t
u(i;t)
 
X
y
 y y (i;t)  
X
z
 z z (i;t)
!
=
 
X
y
 y f (y ) +
X
i;t
u(i;t)
X
y
 y y (i;t)
!
+
 
X
z
 z g (z )  
X
i;t
u(i;t)
X
z
 z z (i;t)
!
Lagrangian dual:
M (u ) = max
 2 y; 2 z
M (u; ; )
Strong dualit y
de ne:
   ;  is the optimal assignment to  ; in the linea r program
theo rem :
min
u
M (u ) =
X
y
  y f (y ) +
X
z
  z g (z )
pro of: by li ne ar programming dualit y
Dual relationship
theo rem : fo r any value of u ,
M (u ) = L(u )
note: solving the original Lagrangian dual also solves dual of the
linea r program
Primal relationship
de ne:
 Q   y   z co rresp onds to feasible solutions of the original
problem
Q = f( y (y ); z (z )) : y 2Y;z 2Z;
y (i;t ) = z (i;t ) fo r all (i;t )g
 Q0  y   z is the set of feasible solutions to the LP
Q0 = f( ; ) :  2  Y; 2  Z;
P
y  y y (i;t ) =
P
z  z z (i;t ) fo r all (i;t )g
 Q  Q0
solutions:
max
q2Q
h (q )  max
q2Q0
h (q ) fo r any h
Concrete example
 Y = fy1;y2;y3g
 Z = fz1;z2;z3g
  y  R 3 ,  z  R 3
Y
x
a
He
a
is
y1
x
b
He
b
is
y2
x
c
He
c
is
y3
Z a
He
b
is
z1
b
He
a
is
z2
c
He
c
is
z3
Simple solution
Y
x
a
He
a
is
y1
x
b
He
b
is
y2
x
c
He
c
is
y3
Z a
He
b
is
z1
b
He
a
is
z2
c
He
c
is
z3
cho ose:
  (1) = (0 ; 0; 1) 2  y is rep re sentation of y3
  (1) = (0 ; 0; 1) 2  z is rep resentation of z3
con rm: X
y
 (1)y y (i;t ) =
X
z
 (1)z z (i;t )
 (1) and  (1) satisfy agreement constraint
Fractional solution
Y
x
a
He
a
is
y1
x
b
He
b
is
y2
x
c
He
c
is
y3
Z a
He
b
is
z1
b
He
a
is
z2
c
He
c
is
z3
cho ose:
  (2) = (0 :5; 0:5; 0) 2  y is combination of y1 and y2
  (2) = (0 :5; 0:5; 0) 2  z is combination of z1 and z2
con rm: X
y
 (2)y y (i;t ) =
X
z
 (2)z z (i;t )
 (2) and  (2) satisfy agreement constraint, but not integral
Optimal solut ion
w eight s:
 the choice of f and g determines the optimal solution
 if (f ;g ) favo rs ( (2) ; (2) ), the optimal solution is fr act ional
example: f = [1 1 2] and g = [1 1  2]
 f   (1) + g   (1) = 0 vs f   (2) + g   (2) = 2
  (2) ; (2) is optimal, even though it is fractional
Algo rithm run
[Animation]
Tightening (Sherali and Adams, 1994; Sontag et al., 2008)
mo dify:
 extend Y, Z to identify bigrams of pa rt-of-sp eech tags
 y (i;t1;t2 ) = 1 $ y (i;t1 ) = 1 and y (i + 1;t2 ) = 1
 z (i;t1;t2 ) = 1 $ z (i;t1 ) = 1 and z (i + 1;t2 ) = 1
all bigram constraints: valid to add fo r all i , t1;t2 2T
X
y
 y y (i;t1;t2 ) =
X
z
 z z (i;t1;t2 )
ho w e ver this w ould mak e deco ding exp ensive
single bigram constraint: cheap er to implem ent
X
y
 y y (1 ;a;b ) =
X
z
 z z (1 ;a;b )
the solution  (1) ; (1) trivially passes this constraint, while
 (2) ; (2) violates it
Dual decomp osition with tightening
tightened decomp osition includ e s an additional Lagrange m u ltiplier
yu;v = arg m a x
y2Y
f (y ) +
X
i;t
u (i;t )y (i;t ) + v (1 ;a;b )y (1 ;a;b )
zu;v = arg m a x
z2Z
g (z )  
X
i;t
u (i;t )z (i;t )  v (1 ;a;b )z (1 ;a;b )
in ge n e ral, this term can mak e the deco ding problem mo re di cult
example:
 fo r sm a ll examples, these p enalties are easy to compute
 fo r CF G pa rsing, need to include ext ra states that m a intain
tag bigrams (still faster than full intersection)
Tightening step-b y-step
[Animation]
6. Advanced examples
aim: demonstrate some di erent relaxation tec hn iques
 higher-o rder non-p rojective dep endency pa rsing
 syntactic machine translation
Higher-o rder non-p rojective dep endency pa rsing
setup: given a mo del fo r higher-o rder non-p rojective dep endency
pa rsing (s ibling features)
probl e m :  nd n on-p rojective dep endency pa rse th at maximizes the
sco re of this mo del
di cult y:
 mo del is N P-ha rd to deco de
 complexit y of the mo del com es from enfo rcing combinato rial
constraints
strategy: design a decomp osition that sepa rates combinato rial
constraints from direct implementation of the sco ring fun c tion
Non-p rojective dep endency pa rsing
structure:
 sta rts a t the ro ot symb ol *
 each w ord has a exactly one pa rent w ord
 pro duces a tree structure (no cycles)
 dep endencies can c ros s
example:
*0 John 1 sa w 2 a3 mo vie 4 to da y5 that 6 he 7 lik ed 8
*0 John 1 sa w 2 a3 mo vie 4 to da y5 that 6 he 7 lik ed 8
Arc-F acto red
* 0 John 1 sa w 2 a 3 movie 4 to da y 5 that 6 he 7 lik ed 8
f (y ) = sco re (head = 0;mo d = sa w 2 ) + sco re (sa w 2;John 1 )
+ sco re (sa w 2;mo vie 4 ) + sco re (sa w 2;to da y5 )
+ sco re (mo vie 4;a3 ) + :::
e.g. sco re ( 0;sa w 2 ) = log p (sa w 2j 0 ) (generative mo del)
or sco re ( 0;sa w 2 ) = w   (sa w 2; 0 ) (CRF/p erceptron mo del)
y = arg max
y
f (y ) ( Minimum Spanning Tree Algo rithm
Sibling mo dels
* 0 John 1 sa w 2 a 3 movie 4 to da y 5 that 6 he 7 lik ed 8
f (y ) = sco re (head =  0;prev = NULL ;mo d = sa w 2 )
+ sco re (sa w 2;NULL ;John 1 )+ sco re (sa w 2;NULL ;mo vie 4 )
+ sco re (sa w 2;mo vie 4;to da y5 ) + :::
e.g. sco re (sa w 2;mo vie 4;to da y5 ) = log p (to da y5jsa w 2;mo vie 4 )
or sco re (sa w 2;mo vie 4;to da y5 ) = w   (sa w 2;mo vie 4;to da y5 )
y = arg max
y
f (y ) ( NP-Ha rd
Thought exp erime nt: individual deco ding
* 0 John 1 sa w 2 a 3 movie 4 to da y 5 that 6 he 7 lik ed 8
sco re (sa w 2;NULL ;John 1 ) + sco re (sa w 2;NULL ;mo vie 4 )
+ sco re (sa w 2;mo vie 4;to da y5 )
sco re (sa w 2;NULL ;John 1 ) + sco re (sa w 2;NULL ;that 6 )
sco re (sa w 2;NULL ;a3 ) + sco re (sa w 2;a3;he 7 )
2n 1
p ossibi lities
under sibling mo del, can solve fo r each w ord with Viterbi deco ding .
Thought exp eriment continued
* 0 John 1 sa w 2 a 3 movie 4 to da y 5 that 6 he 7 lik ed 8
idea: do individual dec o ding fo r each head w ord using dynamic
programming
if w eâ€™re lucky , w eâ€™ll end up with a valid  nal tree
but w e might violate some constraints
Dual decomp osition structure
goal:
y = arg max
y2Y
f (y )
rewrite:
arg max
y2 Y z2 Z,
f (y ) + g (z )
such that fo r all i;j
y (i;j ) = z (i;j )
Algo rithm step-b y-step
[Animation]
Syntactic translation deco ding
setup: assume a trained mo del fo r syntactic machine translation
probl e m :  nd b est derivation that maximizes the sco re of this
mo del
di cult y:
 need to inco rp orate language mo del in deco ding
 empirically , relaxation is of te n not tight, so dual
decomp osition do es not alw ays converge
strategy:
 use a di erent relaxation to han dle language mo del
 incrementally add constraints to  nd exact solution
Syntactic translation example
[Animation]
Summa ry
presented dual decomp osition as a metho d fo r deco ding in NLP
fo rm al gua rantees
 gives certi cate or app ro ximate solution
 can imp rove app ro ximate solutions by tightening relaxation
e cient algo rithms
 uses fast combinato rial algo rithms
 can imp rove sp eed with lazy deco d ing
widely applicable
 demonstrated algo rithms fo r a wide range of NLP tasks
(pa rsing, tagging, alignment, mt deco ding)
References I
J. DeNe ro and K. Macherey . Mo del-Based Align e r Combination
Using Dual Decomp osition. In Pro c. ACL , 2011.
D. Klein and C.D. Manning. F acto red A* Sea rch fo r Mo dels over
Sequences and Trees. In Pro c IJCAI , volume 18, pages
1246{1251. Citeseer, 2003.
N. Komo dakis, N. P aragios, a nd G. Tziritas. Mrf energy
minimization and b ey on d via dual decomp osition. IEEE
Tran saction s on Pattern Analysis and Machine Intelligenc e,
2010. ISSN 0162-8828.
Terry Ko o, Alexander M. Rush, Michael Collins, Tommi Jaakk ola,
and David Sontag. Dual decomp osition fo r pa rsing with
non-p rojective head automata. In EMNLP , 2010. URL
http://www.aclweb.org/anthology/D101125 .
B.H. Ko rte and J. Vygen. Combinato rial Optimiz ation: Theo ry and
Algo rithms . Sp ringer V erlag, 2008.
References II
C. Lema r echal. Lagrangian Relaxation. In Computational
Combinato rial Optimization, Optimal or Provably Nea r-Optimal
Solutions [based on a Sp ring School] , pages 112{156, London,
UK, 2001. Sp ringer-V erlag. ISBN 3540-42877-1.
Angelia Nedi  c and Asuman Ozdagla r. App ro ximate prim a l
solutions and rate analysis fo r dua l subgradient metho ds. SIAM
Journal on Optimiz ation , 19(4):1757{1780, 2009.
Christopher Raphael. Coa rse-tone dynamic programming. IEEE
Tran saction s on Pattern Analysis and Machine Intelligenc e, 23:
1379{1390, 2001.
A.M. Rush and M. Collins. Exact Deco ding of Syntactic
Tran sla tion Mo dels through Lagrangian Relaxation. In Pro c.
ACL , 2011.
A.M. Rush, D. Sontag, M. Collins, and T. Jaakk ola. On Dual
Decomp osition and Linea r Programming Relaxations fo r Natural
Language Pro cessing. In Pro c. EMNLP , 2010.
References III
Hanif D. Sherali and W arren P. Adams. A hiera rc h y of relaxations
and convex hull cha racterizations fo r mixed-integer zero{one
programming problems. Discrete Applied Mathematics , 52(1):83
{ 106, 1994.
D.A. Smith and J. Eisner. Dep endency P arsing by Belief
Propagation. In Pro c. EMNLP , pages 145{156, 2008. URL
http://www.aclweb.org/anthology/D081016 .
D. Sontag, T. Meltzer, A. G lob erson, T. Jaakk ola, and Y. W eiss.
Tightening LP relaxations fo r MAP u sin g message passing. In
Pro c. UAI , 2008.

