<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>M Agosti</author>
<author>G M Di Nunzio</author>
<author>N Ferro</author>
<author>D Harman</author>
<author>C Peters</author>
</authors>
<title>The Future of Large-scale Evaluation Campaigns for Information Retrieval in Europe</title>
<date>2007</date>
<booktitle>In Proceedings ECDL 2007). LNCS 4675</booktitle>
<pages>509--512</pages>
<publisher>Springer</publisher>
<location>Heidelberg, Germany</location>
<marker>Agosti, Di Nunzio, Ferro, Harman, Peters, 2007</marker>
<rawString>Agosti, M., Di Nunzio, G. M., Ferro, N., Harman, D., &amp; Peters, C. (2007). The Future of Large-scale Evaluation Campaigns for Information Retrieval in Europe. In Proceedings ECDL 2007). LNCS 4675, Springer, Heidelberg, Germany, pp 509–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Braschler</author>
</authors>
<title>Robust Multilingual Information Retrieval, Dissertation, Université de Neuchatel</title>
<date>2004</date>
<contexts>
<context>-French and English-German retrieval. This activity continued for three years at TREC, including also Italian as an additional language, and was then moved to Europe in 2000 with the launch of CLEF. (Braschler, 2004) provides a comparison between effectiveness scores from the 1996 TREC-6 campaign and the CLEF 2003 campaign in which retrieval tasks were offered for eight European languages. While in 1996 systems </context>
</contexts>
<marker>Braschler, 2004</marker>
<rawString>Braschler, Martin (2004). Robust Multilingual Information Retrieval, Dissertation, Université de Neuchatel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Braschler</author>
<author>C Peters</author>
</authors>
<title>Cross-Language Evaluation Forum: Objectives, Results, Achievements</title>
<date>2004</date>
<journal>In Information Retrieval</journal>
<volume>7</volume>
<pages>7--31</pages>
<publisher>Kluwer Academic Publishers</publisher>
<contexts>
<context>ovided evidence along the years as to which methods give the best results in certain key areas, such as multilingual indexing, query translation, resolution of translation ambiguity, results merging (Braschler &amp; Peters, 2004). There is also substantial proof of significant increase in retrieval effectiveness in multilingual settings by the systems of CLEF participants. The first evaluation of cross-language systems on Eu</context>
</contexts>
<marker>Braschler, Peters, 2004</marker>
<rawString>Braschler, M. &amp; Peters, C. (2004), Cross-Language Evaluation Forum: Objectives, Results, Achievements. In Information Retrieval, 7, (1/2), pp. 7-31, Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buettcher</author>
<author>C Clarke</author>
<author>P Yeung</author>
<author>I Soboroff</author>
</authors>
<title>Reliable Information Retrieval Evaluation with Incomplete and Biased Judgements</title>
<date>2007</date>
<journal>Aslib Proceedings</journal>
<booktitle>in the Proceedings of ACM SIGIR Conference Cleverdon, C.W</booktitle>
<volume>19</volume>
<pages>173--192</pages>
<contexts>
<context>ef. However, such attitudes ignore the flood of research currently being conducted on new measures and new methodologies that allow building test collections more efficiently (Sanderson &amp; Joho, 2004; Buettcher, et al, 2007) along with new measures that work well with the new test collections. TrebleCLEF aims at identifying and collating the latest research in methods for forming test collections quickly and efficiently</context>
</contexts>
<marker>Buettcher, Clarke, Yeung, Soboroff, 2007</marker>
<rawString>Buettcher, S., Clarke C., Yeung P., Soboroff  I. (2007) Reliable Information Retrieval Evaluation with Incomplete and Biased Judgements, in the Proceedings of ACM SIGIR Conference Cleverdon, C.W. (1967) The Cranfield tests on index language devices. Aslib Proceedings, 19, pp 173-192 Crivellari, F., Di Nunzio, G. M., and Ferro, N. (2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Amati</author>
<author>C Carpineto</author>
<author>G Romano</author>
<author>Eds</author>
</authors>
<title>How to Compare Bilingual to Monolingual Cross-Language Information Retrieval. In</title>
<date>2006</date>
<booktitle>In Proceedings ECIR 2007, LNCS 4425</booktitle>
<pages>533--540</pages>
<publisher>Springer</publisher>
<location>Heidelberg, Germany</location>
<marker>Amati, Carpineto, Romano, Eds, 2006</marker>
<rawString>How to Compare Bilingual to Monolingual Cross-Language Information Retrieval. In Amati, G., Carpineto, C., and Romano, G., Eds, In Proceedings ECIR 2007, LNCS 4425, Springer, Heidelberg, Germany, pp 533–540 Di Nunzio, G. M., &amp; Ferro, N. (2006). Scientific Evaluation of a DLMS: a service for evaluating information access components. In J. Gonzalo, C.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M F Verdejo Thanos</author>
<author>R C Carrasco</author>
</authors>
<booktitle>Proc. ECDL 2006. LNCS 4172, Springer</booktitle>
<pages>536--539</pages>
<location>Heidelberg, Germany</location>
<marker>Thanos, Carrasco, </marker>
<rawString>Thanos, M. F. Verdejo, &amp; R. C. Carrasco (Eds.), Proc. ECDL 2006. LNCS 4172, Springer, Heidelberg, Germany, pp 536–539.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G M Di Nunzio</author>
<author>N Ferro</author>
<author>T Mandl</author>
<author>C Peters</author>
</authors>
<title>CLEF 2006: Ad Hoc Track Overview. In C</title>
<date>2006</date>
<marker>Di Nunzio, Ferro, Mandl, Peters, 2006</marker>
<rawString>Di Nunzio, G. M., Ferro, N., Mandl, T., and Peters, C. (2006). CLEF 2006: Ad Hoc Track Overview. In C.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Clough Peters</author>
<author>F C Gey</author>
<author>J Karlgren</author>
<author>B Magnini</author>
<author>D W Oard</author>
</authors>
<booktitle>Evaluation of Multilingual and Multi-modal Information Retrieval : Seventh Workshop of the Cross--Language Evaluation Forum (CLEF 2006).. LNCS 4730</booktitle>
<pages>21--34</pages>
<publisher>Springer</publisher>
<location>Heidelberg, Germany</location>
<marker>Peters, Gey, Karlgren, Magnini, Oard, </marker>
<rawString>Peters, P. Clough, F. C. Gey, J. Karlgren, B. Magnini, D. W. Oard, et al. (Eds.), Evaluation of Multilingual and Multi-modal Information Retrieval : Seventh Workshop of the Cross--Language Evaluation Forum (CLEF 2006).. LNCS 4730, Springer, Heidelberg, Germany, pp 21–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dussin</author>
<author>N Ferro</author>
</authors>
<title>Design of the User Interface of a Scientific Digital Library System for Large-Scale Evaluation Campaigns</title>
<date>2007</date>
<booktitle>In Thanos, C. &amp; Borri, F. (Eds.), Working Notes of the Second DELOS Conference</booktitle>
<contexts>
<context> design and development of the user interface of DLS of this type, since it needs to be able to support high-level cognitive tasks and the investigation and understanding of the experimental results (Dussin &amp; Ferro, 2007). Scientific data, their enrichment and interpretation are essential components of scientific research. The so-called “Cranfield methodology”(Cleverdon, 1967), which is the paradigm usually followed </context>
</contexts>
<marker>Dussin, Ferro, 2007</marker>
<rawString>Dussin, M., &amp; Ferro, N. (2007). Design of the User Interface of a Scientific Digital Library System for Large-Scale Evaluation Campaigns. In Thanos, C. &amp; Borri, F. (Eds.), Working Notes of the Second DELOS Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hull</author>
</authors>
<title>Using Statistical Testing in the Evaluation of Retrieval Experiments</title>
<date>1993</date>
<booktitle>Proc. SIGIR</booktitle>
<pages>329--338</pages>
<editor>In Korfhage, R., Rasmussen, E., and Willett, P., editors</editor>
<publisher>ACM Press</publisher>
<location>New York, USA</location>
<contexts>
<context>es how these scientific data have to be produced, while the statistical analysis of experiments provide the means for further elaborating and interpreting the experimental results, as pointed out by (Hull, 1993). Nevertheless, current methodologies do not imply any particular coordination or synchronization between the basic scientific data and the analyses on them, which are treated as almost separate item</context>
</contexts>
<marker>Hull, 1993</marker>
<rawString>Hull, D. (1993). Using Statistical Testing in the Evaluation of Retrieval Experiments. In Korfhage, R., Rasmussen, E., and Willett, P., editors, Proc. SIGIR 1993, pp 329–338. ACM Press, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Petrelli</author>
</authors>
<title>On the role of User-Centred Evaluation</title>
<date>2008</date>
<booktitle>in the Advancement of Interactive Information Retrieval. Information Processing and Management</booktitle>
<volume>44</volume>
<pages>22--38</pages>
<marker>Petrelli, 2008</marker>
<rawString>Petrelli, D. (2008) On the role of User-Centred Evaluation in the Advancement of Interactive Information Retrieval. Information Processing and Management, 44(1), pp 22-38.</rawString>
</citation>
</citationList>
</algorithm>

