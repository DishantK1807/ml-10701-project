tures are provided by Li and Khudanpur (2008b).
W08-0402
David Talbot and Miles Osborne. 2007. Randomised
P07-1065
Jason Eisner. 2003. Learning non-isomorphic tree
P03-2041
Shankar Kumar and William Byrne. 2004. Minimum
N04-1022
niques: beam and cube pruning (Chiang, 2007).
J07-2003
Zhifei Li and Sanjeev Khudanpur. 2008a. Large-scale
W08-0402
scribed in Chiang (2007) and Li and Khudanpur
J07-2003
Callison-Burch et al. (2005; Lopez (2007) and use
D07-1104 P05-1032
Zhifei Li and Sanjeev Khudanpur. 2008b. A scalable
W08-0402
tecture (Lopez, 2007), the SRILM toolkit (Stol-
D07-1104
2005; Lopez, 2007) and minimum error rate train-
D07-1104
grammar extraction (Lopez, 2007) and minimum
D07-1104
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
P06-1077
stead of using the regular MERT (Och, 2003)
J03-1002 P03-1021
translation (e.g., Chiang (2007), Quirk et al.(2005), Galley et al. (2006), and Liu et al. (2006))
P05-1034 P06-1077 P06-1121 J07-2003
method of Och (2003). Each iteration of our
J03-1002 P03-1021
David Chiang. 2007. Hierarchical phrase-based trans-
J07-2003
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
P05-1034
Liang Huang and David Chiang. 2005. Better k-best
W05-1506
ing (Och, 2003). Additionally, parallel and dis-
J03-1002 P03-1021
toolkit (Och and Ney, 2003), a suffix-array archi-
J03-1002 P03-1021
Adam Lopez. 2007. Hierarchical phrase-based trans-
D07-1104
Talbot and Osborne (2007).
P07-1065
Franz Josef Och. 2003. Minimum error rate training
J03-1002 P03-1021
Franz Josef Och and Hermann Ney. 2003. A sys-
J03-1002 P03-1021
David A. Smith and Jason Eisner. 2006. Minimum risk
P06-2101
error rate training (Och, 2003). Additionally, par-
J03-1002 P03-1021
kind extracted by Heiro (Chiang, 2007), but is eas-
J07-2003
translation probability p(e|f) (Koehn et al., 2003).
N03-1017
