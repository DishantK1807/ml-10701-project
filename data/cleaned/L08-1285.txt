<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Julio Gonzalo</author>
<author>Anselmo Pe˜nas</author>
<author>Felisa Verdejo</author>
</authors>
<title>QARLA: a Framework for the Evaluation of Automatic Sumarization</title>
<date>2005</date>
<booktitle>In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics</booktitle>
<marker>Amig´o, Gonzalo, Pe˜nas, Verdejo, 2005</marker>
<rawString>Enrique Amig´o, Julio Gonzalo, Anselmo Pe˜nas, and Felisa Verdejo. 2005. QARLA: a Framework for the Evaluation of Automatic Sumarization. In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Jes´us Gim´enez</author>
<author>Julio Gonzalo</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>MT Evaluation: Human-Like vs. Human Acceptable</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL</booktitle>
<marker>Amig´o, Gim´enez, Gonzalo, M`arquez, 2006</marker>
<rawString>Enrique Amig´o, Jes´us Gim´enez, Julio Gonzalo, and Llu´ıs M`arquez. 2006. MT Evaluation: Human-Like vs. Human Acceptable. In Proceedings of the Joint 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments</title>
<date>2005</date>
<booktitle>In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization. 5Currently</booktitle>
<location>most</location>
<contexts>
<context>p://www.lsi.upc.edu/˜nlp/IQMT NIST. Modified BLEU (Doddington, 2002). GTM. F-measure (Melamed et al., 2003). ROUGE. Recall oriented (Lin and Och, 2004a). METEOR. F-measure based on unigram alignment (Banerjee and Lavie, 2005). TER. Translation Edit Rate (Snover et al., 2006). 2.2. Shallow Syntactic Similarity a2 On Shallow Parsing (SP) SP-a3a5a4 -a6 Lexical overlapping according to the partof-speech ‘a7 ’. For instance, </context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization. 5Currently, most metrics are only available for English. Some are also available for Spanish and Catalan. Chris Callison-Burch, Miles Osborne, and Philipp Koehn.</rawString>
</citation>
<citation valid="true">
<title>Re-evaluating the Role of BLEU in Machine Translation Research</title>
<date>2006</date>
<booktitle>In Proceedings of EACL</booktitle>
<contexts>
<context>23 grammatical categories. DP-HWC(i)a25 -a23 grammatical relations. DP-a3 a16a27a26a3 a12a28a26a3a5a25 These metrics correspond exactly to the LEVEL, GRAM and TREE metrics introduced by Amig´o et al. (2006). DP-a3 a16 -a23 Overlapping between words hanging at level ‘a29 ’, or deeper. DP-a3 a12 -a6 Overlapping between words directly hanging from terminal nodes (i.e. grammatical categories) of type ‘a7 ’.</context>
<context>fically, all systems are statistical except one, LinearB, which is human-aided. A brief numerical description of this test bed is available in Table 1. This data set was used by Callison-Burch et al. (2006) to discuss the strong tendency of BLEU to favor statistical systems. Table 2 illustrates this fact by showing overall scores for LinearB and the best statistical system according to BLEU and human as</context>
</contexts>
<marker>2006</marker>
<rawString>2006. Re-evaluating the Role of BLEU in Machine Translation Research. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Michael Gamon</author>
<author>Chris Brockett</author>
</authors>
<title>A Machine Learning Approach to the Automatic Evaluation of Machine Translation</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>140--147</pages>
<contexts>
<context>ree matching (‘DR-STM-9’). Meta-Evaluation Metric quality has been evaluated on the basis of human likeness, i.e., in terms of the metric ability to discern between manual and automatic translations (Corston-Oliver et al., 2001; Lin and Och, 2004b; Kulesza and Shieber, 2004; Amig´o et al., 2005; Gamon et al., 2005). We have computed human likeness through the KING measure defined inside the QARLA Framework (Amig´o et al., 2</context>
</contexts>
<marker>Corston-Oliver, Gamon, Brockett, 2001</marker>
<rawString>Simon Corston-Oliver, Michael Gamon, and Chris Brockett. 2001. A Machine Learning Approach to the Automatic Evaluation of Machine Translation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics (ACL), pages 140–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd Internation Conference on Human Language Technology</booktitle>
<pages>138--145</pages>
<contexts>
<context>Rate (Nießen et al., 2000). PER. Position-independent Word Error Rate (Tillmann et al., 1997). BLEU. Precision oriented (Papineni et al., 2001). 1http://www.lsi.upc.edu/˜nlp/IQMT NIST. Modified BLEU (Doddington, 2002). GTM. F-measure (Melamed et al., 2003). ROUGE. Recall oriented (Lin and Och, 2004a). METEOR. F-measure based on unigram alignment (Banerjee and Lavie, 2005). TER. Translation Edit Rate (Snover et al</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proceedings of the 2nd Internation Conference on Human Language Technology, pages 138–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Anthony Aue</author>
<author>Martine Smets</author>
</authors>
<title>Sentence-Level MT evaluation without reference translations: beyond language modeling</title>
<date>2005</date>
<booktitle>In Proceedings of EAMT</booktitle>
<pages>103--111</pages>
<contexts>
<context>n likeness, i.e., in terms of the metric ability to discern between manual and automatic translations (Corston-Oliver et al., 2001; Lin and Och, 2004b; Kulesza and Shieber, 2004; Amig´o et al., 2005; Gamon et al., 2005). We have computed human likeness through the KING measure defined inside the QARLA Framework (Amig´o et al., 2005)3. Given a metric a78 , a set of human references a79 , and a set of automatic trans</context>
</contexts>
<marker>Gamon, Aue, Smets, 2005</marker>
<rawString>Michael Gamon, Anthony Aue, and Martine Smets. 2005. Sentence-Level MT evaluation without reference translations: beyond language modeling. In Proceedings of EAMT, pages 103–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´is M`arquez</author>
</authors>
<title>On the Robustness of Linguistic Features for Automatic MT Evaluation. (Under submission</title>
<date>2008</date>
<marker>Gim´enez, M`arquez, 2008</marker>
<rawString>Jes´us Gim´enez and Llu´is M`arquez. 2008. On the Robustness of Linguistic Features for Automatic MT Evaluation. (Under submission).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Linguistic Features for Automatic Evaluation of Heterogeneous MT Systems</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL Workshop on Statistical Machine Translation</booktitle>
<pages>256--264</pages>
<marker>Gim´enez, M`arquez, 2007</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2007. Linguistic Features for Automatic Evaluation of Heterogeneous MT Systems. In Proceedings of the ACL Workshop on Statistical Machine Translation, pages 256–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
</authors>
<date>2007</date>
<tech>IQMT v2.1. Technical Manual (LSI-07-29-R). Technical report, TALP</tech>
<note>Research Center. LSI Department. http://www.lsi.upc.edu/˜nlp/IQMT/IQMT.v2.1.pdf</note>
<marker>Gim´enez, 2007</marker>
<rawString>Jes´us Gim´enez. 2007. IQMT v2.1. Technical Manual (LSI-07-29-R). Technical report, TALP Research Center. LSI Department. http://www.lsi.upc.edu/˜nlp/IQMT/IQMT.v2.1.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
</authors>
<title>A Theory of Truth and Semantic Representation</title>
<date>1981</date>
<booktitle>Formal Methods in the Study of Language</booktitle>
<pages>277--322</pages>
<editor>In J.A.G. Groenendijk, T.M.V. Janssen, and M.B.J. Stokhof, editors</editor>
<location>Amsterdam</location>
<contexts>
<context>y a49 On Discourse Representations (DR) We have designed a family of metrics operating over semantic trees (Gim´enez and M`arquez, 2008). These trees are based on the Discourse Representation Theory (Kamp, 1981). Three kinds of metrics are defined: DR-STM(i)-a50 These metrics are similar to the ‘CPSTM’ variants discussed above, in this case applied to DR structures instead of constituency trees. DR-a41 a42 </context>
</contexts>
<marker>Kamp, 1981</marker>
<rawString>Hans Kamp. 1981. A Theory of Truth and Semantic Representation. In J.A.G. Groenendijk, T.M.V. Janssen, and M.B.J. Stokhof, editors, Formal Methods in the Study of Language, pages 277–322. Mathematisch Centrum, address = Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for Automatic Evaluation</title>
<date>2006</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technology and the North American Chapter of the Association for Computational Linguistics (HLT-NAACL</booktitle>
<pages>455--462</pages>
<contexts>
<context>system outputs and reference translations, other authors have suggested taking advantage of paraphrasing support so as to extend the reference material (Russo-Lassner et al., 2005; Zhou et al., 2006; Kauchak and Barzilay, 2006; Owczarzak et al., 2006). We believe the two approaches could be combined. 4. Conclusions and Future Work We have presented a valid path towards heterogeneous automatic MT error analysis. Our approac</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for Automatic Evaluation. In Proceedings of the Joint Conference on Human Language Technology and the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 455–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Kulesza</author>
<author>Stuart M Shieber</author>
</authors>
<title>A learning approach to improving sentence-level MT evaluation</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation</booktitle>
<contexts>
<context> quality has been evaluated on the basis of human likeness, i.e., in terms of the metric ability to discern between manual and automatic translations (Corston-Oliver et al., 2001; Lin and Och, 2004b; Kulesza and Shieber, 2004; Amig´o et al., 2005; Gamon et al., 2005). We have computed human likeness through the KING measure defined inside the QARLA Framework (Amig´o et al., 2005)3. Given a metric a78 , a set of human refe</context>
</contexts>
<marker>Kulesza, Shieber, 2004</marker>
<rawString>Alex Kulesza and Stuart M. Shieber. 2004. A learning approach to improving sentence-level MT evaluation. In Proceedings of the 10th International Conference on Theoretical and Methodological Issues in Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Audrey Le and Mark Przybocki</author>
</authors>
<title>NIST 2005 machine translation evaluation official results. In Official release of automatic evaluation scores for all submissions</title>
<date>2005</date>
<contexts>
<context>ation campaigns. In the following, we exemplify the application of heterogeneous MT error analyses through the case of the Arabic-to-English exercise from the 2005 NIST MT evaluation campaign (Le and Przybocki, 2005). This test bed presents the particularity of providing automatic translations produced by heterogeneous MT systems (i.e., systems belonging to different paradigms). Specifically, all systems are sta</context>
</contexts>
<marker>Przybocki, 2005</marker>
<rawString>Audrey Le and Mark Przybocki. 2005. NIST 2005 machine translation evaluation official results. In Official release of automatic evaluation scores for all submissions, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statics</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context> al., 1997). BLEU. Precision oriented (Papineni et al., 2001). 1http://www.lsi.upc.edu/˜nlp/IQMT NIST. Modified BLEU (Doddington, 2002). GTM. F-measure (Melamed et al., 2003). ROUGE. Recall oriented (Lin and Och, 2004a). METEOR. F-measure based on unigram alignment (Banerjee and Lavie, 2005). TER. Translation Edit Rate (Snover et al., 2006). 2.2. Shallow Syntactic Similarity a2 On Shallow Parsing (SP) SP-a3a5a4 -a</context>
<context>ta-Evaluation Metric quality has been evaluated on the basis of human likeness, i.e., in terms of the metric ability to discern between manual and automatic translations (Corston-Oliver et al., 2001; Lin and Och, 2004b; Kulesza and Shieber, 2004; Amig´o et al., 2005; Gamon et al., 2005). We have computed human likeness through the KING measure defined inside the QARLA Framework (Amig´o et al., 2005)3. Given a metr</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004a. Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statics. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context> al., 1997). BLEU. Precision oriented (Papineni et al., 2001). 1http://www.lsi.upc.edu/˜nlp/IQMT NIST. Modified BLEU (Doddington, 2002). GTM. F-measure (Melamed et al., 2003). ROUGE. Recall oriented (Lin and Och, 2004a). METEOR. F-measure based on unigram alignment (Banerjee and Lavie, 2005). TER. Translation Edit Rate (Snover et al., 2006). 2.2. Shallow Syntactic Similarity a2 On Shallow Parsing (SP) SP-a3a5a4 -a</context>
<context>ta-Evaluation Metric quality has been evaluated on the basis of human likeness, i.e., in terms of the metric ability to discern between manual and automatic translations (Corston-Oliver et al., 2001; Lin and Och, 2004b; Kulesza and Shieber, 2004; Amig´o et al., 2005; Gamon et al., 2005). We have computed human likeness through the KING measure defined inside the QARLA Framework (Amig´o et al., 2005)3. Given a metr</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004b. ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation. In Proceedings of the 20th International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic Features for Evaluation of Machine Translation</title>
<date>2005</date>
<booktitle>In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization</booktitle>
<contexts>
<context> on lexical similarity. However, in the last few years, there have been several approaches based on similarity assumptions at deeper linguistic levels. For instance, we may find syntax-based metrics (Liu and Gildea, 2005; Amig´o et al., 2006; Mehay and Brew, 2007; Owczarzak et al., 2007), which compute similarities over dependency or constituency trees, metrics at the level of shallow-semantics, e.g., over semantic r</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic Features for Evaluation of Machine Translation. In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Mehay</author>
<author>Chris Brew</author>
</authors>
<title>BLEUATRE: Flattening Syntactic Dependencies for MT Evaluation</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI</booktitle>
<contexts>
<context>t few years, there have been several approaches based on similarity assumptions at deeper linguistic levels. For instance, we may find syntax-based metrics (Liu and Gildea, 2005; Amig´o et al., 2006; Mehay and Brew, 2007; Owczarzak et al., 2007), which compute similarities over dependency or constituency trees, metrics at the level of shallow-semantics, e.g., over semantic roles and named entities (Gim´enez and M`arq</context>
</contexts>
<marker>Mehay, Brew, 2007</marker>
<rawString>Dennis Mehay and Chris Brew. 2007. BLEUATRE: Flattening Syntactic Dependencies for MT Evaluation. In Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Ryan Green</author>
<author>Joseph P Turian</author>
</authors>
<title>Precision and Recall of Machine Translation</title>
<date>2003</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technology and the North American Chapter of the Association for Computational Linguistics (HLT-NAACL</booktitle>
<contexts>
<context>sition-independent Word Error Rate (Tillmann et al., 1997). BLEU. Precision oriented (Papineni et al., 2001). 1http://www.lsi.upc.edu/˜nlp/IQMT NIST. Modified BLEU (Doddington, 2002). GTM. F-measure (Melamed et al., 2003). ROUGE. Recall oriented (Lin and Och, 2004a). METEOR. F-measure based on unigram alignment (Banerjee and Lavie, 2005). TER. Translation Edit Rate (Snover et al., 2006). 2.2. Shallow Syntactic Simila</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>I. Dan Melamed, Ryan Green, and Joseph P. Turian. 2003. Precision and Recall of Machine Translation. In Proceedings of the Joint Conference on Human Language Technology and the North American Chapter of the Association for Computational Linguistics (HLT-NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Franz Josef Och</author>
<author>Gregor Leusch</author>
<author>Hermann Ney</author>
</authors>
<title>An Evaluation Tool for Machine Translation: Fast Evaluation for MT Research</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd International Conference on Language Resources and Evaluation</booktitle>
<contexts>
<context> reference translations. Below, we provide only a brief description. Extensive details may be found in the IQMT technical manual (Jes´us Gim´enez, 2007). 2.1. Lexical Similarity WER. Word Error Rate (Nießen et al., 2000). PER. Position-independent Word Error Rate (Tillmann et al., 1997). BLEU. Precision oriented (Papineni et al., 2001). 1http://www.lsi.upc.edu/˜nlp/IQMT NIST. Modified BLEU (Doddington, 2002). GTM. F</context>
</contexts>
<marker>Nießen, Och, Leusch, Ney, 2000</marker>
<rawString>Sonja Nießen, Franz Josef Och, Gregor Leusch, and Hermann Ney. 2000. An Evaluation Tool for Machine Translation: Fast Evaluation for MT Research. In Proceedings of the 2nd International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Declan Groves</author>
<author>Josef Van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation</title>
<date>2006</date>
<journal>Karolina Owczarzak, Josef</journal>
<booktitle>In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA</booktitle>
<pages>148--155</pages>
<marker>Owczarzak, Groves, Van Genabith, Way, 2006</marker>
<rawString>Karolina Owczarzak, Declan Groves, Josef Van Genabith, and Andy Way. 2006. Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA), pages 148–155. Karolina Owczarzak, Josef van Genabith, and Andy Way.</rawString>
</citation>
<citation valid="true">
<title>Dependency-Based Automatic Evaluation for Machine Translation</title>
<date>2007</date>
<booktitle>In Proceedings of SSST, NAACLHLT/AMTA Workshop on Syntax and Structure in Statistical Translation</booktitle>
<pages>80--87</pages>
<marker>2007</marker>
<rawString>2007. Dependency-Based Automatic Evaluation for Machine Translation. In Proceedings of SSST, NAACLHLT/AMTA Workshop on Syntax and Structure in Statistical Translation, pages 80–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation</title>
<date>2001</date>
<journal>IBM T.J. Watson Research</journal>
<tech>rc22176. Technical report</tech>
<contexts>
<context>hnical manual (Jes´us Gim´enez, 2007). 2.1. Lexical Similarity WER. Word Error Rate (Nießen et al., 2000). PER. Position-independent Word Error Rate (Tillmann et al., 1997). BLEU. Precision oriented (Papineni et al., 2001). 1http://www.lsi.upc.edu/˜nlp/IQMT NIST. Modified BLEU (Doddington, 2002). GTM. F-measure (Melamed et al., 2003). ROUGE. Recall oriented (Lin and Och, 2004a). METEOR. F-measure based on unigram alig</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation, rc22176. Technical report, IBM T.J. Watson Research Center. Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.</rawString>
</citation>
<citation valid="true">
<title>A Paraphrase-Based Approach to Machine Translation Evaluation</title>
<date>2005</date>
<tech>Technical report</tech>
<institution>University of Maryland, College Park</institution>
<contexts>
<context> Chunk IOB labels2 2.3. Syntactic Similarity a2 On Dependency Parsing (DP) DP-HWC(i)-a23 These metrics correspond to variants of the head-word chain matching (HWCM) metric presented by Liu and Gildea (2005) slightly modified so as to consider different head-word chain types: DP-HWC(i)a24 -a23 words. DP-HWC(i)a12 -a23 grammatical categories. DP-HWC(i)a25 -a23 grammatical relations. DP-a3 a16a27a26a3 a12a</context>
<context>categories, and relationships, respectively. a2 On Constituency Parsing (CP) CP-STM(i)-a23 These metrics correspond to variants of the syntactic tree matching (STM) metric presented by Liu and Gildea (2005). CP-a3a5a4 -a6 Similarly to the ‘SP-a8a10a9 -a32 ’ metrics, these metrics compute lexical overlapping according to the part-of-speech ‘a7 ’. CP-a3 a12 -a6 These metrics compute lexical overlapping ac</context>
</contexts>
<marker>2005</marker>
<rawString>2005. A Paraphrase-Based Approach to Machine Translation Evaluation. Technical report, University of Maryland, College Park.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation</title>
<date>2006</date>
<booktitle>In Proceedings of AMTA</booktitle>
<pages>223--231</pages>
<contexts>
<context>ington, 2002). GTM. F-measure (Melamed et al., 2003). ROUGE. Recall oriented (Lin and Och, 2004a). METEOR. F-measure based on unigram alignment (Banerjee and Lavie, 2005). TER. Translation Edit Rate (Snover et al., 2006). 2.2. Shallow Syntactic Similarity a2 On Shallow Parsing (SP) SP-a3a5a4 -a6 Lexical overlapping according to the partof-speech ‘a7 ’. For instance, ‘SP-a8a10a9 -NN’ roughly reflects the proportion o</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, , and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of AMTA, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>S Vogel</author>
<author>H Ney</author>
<author>A Zubiaga</author>
<author>H Sawaf</author>
</authors>
<title>Accelerated DP based Search for Statistical Translation</title>
<date>1997</date>
<booktitle>In Proceedings of European Conference on Speech Communication and Technology</booktitle>
<contexts>
<context>on. Extensive details may be found in the IQMT technical manual (Jes´us Gim´enez, 2007). 2.1. Lexical Similarity WER. Word Error Rate (Nießen et al., 2000). PER. Position-independent Word Error Rate (Tillmann et al., 1997). BLEU. Precision oriented (Papineni et al., 2001). 1http://www.lsi.upc.edu/˜nlp/IQMT NIST. Modified BLEU (Doddington, 2002). GTM. F-measure (Melamed et al., 2003). ROUGE. Recall oriented (Lin and Oc</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, Sawaf, 1997</marker>
<rawString>C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf. 1997. Accelerated DP based Search for Statistical Translation. In Proceedings of European Conference on Speech Communication and Technology.</rawString>
</citation>
</citationList>
</algorithm>

