Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1568–1576,
Edinburgh, Scotland, UK, July 27–31, 2011. c 2011 Association for Computational Linguistics
 
 
Twiter Catches The Flu: Detecting Influenza Epidemics using Twiter 
 
 
Eiji ARAMAKI Sachiko MASKAWA Mizuki MORITA 
The University of Tokyo The University of Tokyo National Institute of 
JST PRESTO  Biomedical Innovation  
Tokyo, Japan Tokyo, Japan 
Osaka, Japan 
eiji.aramaki@gmail.com sachiko.maskawa@gmail.com morita.mizuki@gmail.com 
 
 
 
 
 
Abstract 
With the recent rise in popularity and scale 
of social media, a growing need exists for 
systems that can extract useful information 
from huge amounts of data. We address the 
issue of detcting influenza epiemics. 
First, the proposed system extracts influen-
za related tweets using Twitter API. Then, 
only twets that mention actual influenza 
patients are extracted by the support vector 
machine (SVM) based clasifier. The ex-
periment results demonstrate the feasibility 
of the proposed approach (0.89 correlation 
to the gold standard). Especially at the out-
break and early spread (early epidemic 
stage), the proposed method shows high 
corelation (0.97 correlation), which out-
performs the state-of-the-art methods. This 
paper describes that Twiter texts reflect 
the real world, and that NLP techniques 
can be applied to extract only twets that 
contain useful information. 
1 Introduction

Twiter
1, a popular micro-blogging service, has 
received much atention recently. It is an online 
network used by milions of people around the 
world to stay conected to their friends, family 
members, and co-workers through their computers 
and mobile telephones (Milstein et al., 2010). 
Nowadays, Twiter users have increased rapidly. 
Its comunity estimated as 120 milion worldwide, 
                                                             
1
 http:/twiter.com/ 
posts more than 5.5 milion mesages (twets) eve-
ry day (reported by Twiter.com in March 201). 
Twiter can potentialy serve as a valuable infor-
mation resource for various aplications. Huber-
man et al. (209) analyzed the relations among 
friends. Boyd et al. (2010) investigated comuta-
tion activity. Sakaki et al. (2010) adresed the 
detection of earthquakes. Among the numerous 
potential aplications, this study addreses the is-
sue of detcting influenza epidemics, which pre-
sents two outstanding advantages over curent 
methods. 
 
 Large Scale: More than a thousand mesages 
include the word “influenza” each day (Nov. 
2008 – Oct. 209). Such a huge dat volume 
dwarfs traditional surveilance resources. 
 
 Real-time: Twitter enables real-time and di-
rect surveilance. This characteristic is ex-
tremely suitable for influenza epidemic 
detection because early stage detection is im-
portant for influenza warnings. 
 
Although Twiter based influenza warnigs poten-
tially ofer the advantages noted above, it might 
also expose inaccurate or biased information from 
twets like the following (brackets []	  indicate the 
coments): 
 
 Headache? You might have flu. [Suspi-­‐
cions]	  
 The World Health Organization reports 
the avian influenza, or bird flu, epidemic 
has spread to nine Asian countries in the 
past few weeks. [General	  News] 
1568
 
 
 Are you coming down with influenza? 
[Question] 
 
Although these tweets include mention of “influ-
enza” or “flu”, they do not indicate that an influen-
za patient is present nearby. We regrd such 
mesages (merely suspicions/questions, general 
news, etc.) as negative influenza twets. We call 
others positive influenza twets. In our experi-
ments, 42% of al twets that include “influenza” 
are negative influenza twets. The huge volume of 
such negative twets biases the results. 
This paper presents a proposal of a machine-
learning based clasifier to filter out negative in-
fluenza twets. First, we build an anotated corpus 
of pairs of a twet and positve/negative labels. 
Then, a suport vector machine (SVM) (Cortes and 
Vapnik, 1995) based sentence classifier extracts 
only positive influenza twets from twets. In the 
experiments, the results demonstrated the high cor-
relation (0.89 of the correlation), which is equal 
performance to that of the state-of-the-art method. 
 
The specified research point of this study is two-
fold: 
(1) This report describes that an SVM-based clas-
sifier can filter out the negative influenza 
twets (f-measure=0.76). 
(2) Experiments empiricaly demonstrate that the 
proposed method detects the influenza epidem-
ics with high acuracy (corelation ratio=0.89): 
it outperforms the state-of-the-art method. 
2 Influenza
Epidemic Detection 
The detection of influenza epidemics is a national 
mision in every country for two reasons. 
(1) Anti-influenza drugs, which differ among in-
fluenza types, must be prepared before the epi-
demics. 
(2) We can only slightly predict what type of in-
fluenza wil spread in any given season. 
 
This situation naturally demands the early detec-
tion of influenza epidemics. This section presents a 
description of previous methods of influenza epi-
demic detection. 
2.1 Traditional
Aproaches 
Most countries have their own influenza surveil-
lance organization/center: the U.S. has the Centers 
for Disease Control and Prevention (CDC)
2, the 
E.U. has its European Influenza Surveilance 
Scheme (EISS), and Japan has its Infection Disease 
Surveillance Center (IDSC). Their surveilance 
systems fundamentaly rely on both virology and 
clinical data. For example, the IDSC gathers influ-
enza patient data from 5,00 clinics and releases 
sumary reports. Such manual systems typically 
have a 1–2 wek reporting lag. This time lag is 
sometimes pointed out as a major flaw. 
2.2 Recent
Approaches 
In an atempt to provide earlier influenza detection, 
various new approaches are proposed each year. 
Espino et al. (203) described a telephone triage 
service, a public service, to give advice to users via 
telephone. They investigated the number of tele-
phone cals and reported a signifcant corelation 
with influenza epidemics. 
Magruder (203) used the amount of over-the-
counter drug sales. Because an influenza patient 
usually requires anti-influenza drugs, this aproach 
is reasonable. However, in most countries, anti-
influenza drugs are not available at the drug store 
(only hospitals provide such drugs). 
The state-of-the-art approach is that proposed by 
Ginsberg et al. (209). They used Google web 
search queries that corelate with an influenza epi-
demic. Their aproach demonstrated high acuracy 
(average correlation ratio of 0.97; min=0.92; 
max=0.9)
3
. Several research groups have used 
similar aproaches. Polgren et al. (208) used a 
Yaho! query log. Hulth et al. (209) used a query 
log of a Switzerland web search engine.  
Although the above aproaches use diferent in-
formation, they share the same aproach, which is 
to observe patient actions directly. This approach 
was suficient to obtain more numerous data than 
traditional services. Nevertheles, such information 
is unfortunately limited only to the service pro-
vider. For example, web search queries are avail-
able only for several companies: Gogle, Yaho!, 
and Microsoft. 
This paper examines Twitter data, which are 
widely available. Note that Paul and Dredze (201) 
also propose a similar Twiter based approach. 
While they focus on a word distribution, this paper 
                                                             
2
 http://ww.cdc.gov/flu/wekly/ 
3
 Their service is available at http:/ww.google.org/flutrends/ 
(Gogle Flu Trend). 
1569
 
 
employs a sentence classification (discrimination 
of negative influenza tweets). 
3 Influenza
Corpus 
As described in Section 1, it is necesary to filter 
out negative influenza twets to infer precise 
amounts of influenza epidemics. To do so, we con-
structed the influenza corpus (Section 3). Then, we 
trained the SVM-based clasifier using the corpus 
(Section 4). 
The corpus comprises pairs of sentences and a 
label (positive or negative). Several examples are 
presented in Table 1. This corpus was built using 
the following procedure. 
3.1 Influenza
Twet 
First, we colected 30 milion twets, starting 
from 208 November to 2010 June, via Twiter 
API. Crawling results are presented in Figure 1. 
We extracted only influenza-related twets using a 
simple word look-up of “influenza”. This operation 
gave us 0.4 milion twets. We separated the data 
into two data groups. 
Training Data are 5,00 tweets sent in Novem-
ber 2008. These were anotated by human anota-
tors, and were then used for training. 
Test Data are the other dat. They were used in 
experiments of influenza epidemics detection. Be-
cause of the three dropout periods (Figure 1), the 
test data were separated into four periods (winter 
2008, summer 2009, winter 2009, and summer 
2010). 
3.2 Positive–negative Annotation 
To each twet in the trainig dataset, a human an-
notator assigned one of two labels: positive or neg-
ative. In this labeling procedure, we regarded a 
twet that mets the following two conditions as 
positive data. 
 
 
Condition 1 (A Twet person or Surounding 
persons have Flu): one or more people who have 
influenza should exist around the twet person. 
Here, we regard “around” as a distance in the same 
city. In cases in which the distance is unknown, we 
regard it as negative. Because of this annotation 
policy, the re-twet type mesage is negative. 
 
 
 
Figure 1: Twiter Data used in this Study. 
The data include thre dropout periods because the Twiter API 
specifications changed in those periods. The dropout periods 
were removed from evaluation in the experiments (Section 5). 
 
Table 1: Corpus (Twets with a Positive or Nega-
tive Label) 
Positive(+1)/	  
Negative(-­‐1)	  
Twet	  
+1	   A	  bad	  	  influenza	  is	  going	  around	  in	  our	  lab.	  
+1	   I	  caught	  the	  flu.	  I	  was	  burning	  up.	  
+1	   I	  think	  	  I'm	  coming	  down	  with	  the	  flu.	  
+1	   It's	  the	  flu	  season.	  I	  had	  it	  and	  now	  he	  do	  es.	  
+1	   Don't	  give	  me	  the	  flu.	  
(Nearby	  people	  have	  the	  flu)	  
+1	   My	  flu	  is	  worse	  than	  it	  was	  yesterday.	  
-­‐1	   In	  the	  normal	  flu	  season,	  80	  percent	  of	  deaths	  
occur	  in	  people	  over	  65	  
(Simply	  a	  fact)	  
-­‐1	   Influenza	  is	  now	  raging	  throughout	  Japan.	  
(Too	  general.)	  
-­‐1	   His	  wife	  also	  contracted	  the	  bird	  flu,	  but	  has	  
recovered.	  
(Where	  is	  his	  wife?)	  
-­‐1	   You	  might	  have	  the	  flu.	  Has	  anyone	  around	  
you	  had	  it?	  
(Where	  are	  you?)	  
-­‐1	   Bird	  flu	  damage	  is	  spreading	  in	  Japan.	  
(Too	  general.)	  
“+1” indicates a positive influenza twet. “-1” indicates a 
negative influenza twet. The case arc “()” indicates the rea-
son for the positive or negative anotation. 
 
 
 
Figure 2: Feature Representation. 
The word boundary is detected by a morph analyzer JUMAN
4
. 
 
                                                             
4
 http:/nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html 
1570
 
 
Condition 2 (Tense/Modality): The tense should 
be the present tense (current) or recent past. Here, 
we define the “recent past” as the prior 24 hour 
period (such as “yesterday”). The sentence should 
be afirmative (not interogative and not subjunc-
tive). 
4 Influenza
Positive–negative Classifier 
Using the corpus (Section 3), we built a clasifier 
that judges whether a given twet is positve or 
negative. This task seting is similar to a sentence 
classification (such as spam e-mail filtering, senti-
ment analysis, and so on). We used a popular 
means for sentence clasification, which is based 
on a machine learning clasifier under the bag-of-
words (BOW) representation (Figure 2). The 
parameters were investigated in preliminary ex-
periments in terms of feature window size (Section 
4.1) and machine-learning methods (Section 4.2). 
These preliminary experiments were conducted 
under the ten-fold cross variation maner using the 
training set. 
4.1 Feature
(window size) 
Performance was dependent on the window size 
(the number of left/right side words). Figure 3 de-
picts the performance obtained using various win-
dow sizes. The best performance was scored at the 
BOTH=6 setting. Therefore, this window size was 
used for the following experiments. These results 
also indicated that entire sentences (BOTH=∞) are 
unsuitable for this task. 
4.2 Machine
Learning Method 
We compared various machine-learning methods 
from two points of view: acuracy and time. The 
result, presented in Table 2, shows that SVM with 
a polynomial kernel showed feasibility from both 
viewpoints of acuracy and the training time. 
5 Experiments

We asesed the detection performance using actu-
al influenza reports provided by the Japanese IDSC. 
5.1 Comparable
Methods 
We compared the various methods as follows: 
 
 
 
 
 
Figure 3: Window size and Acuracy (F-measure). 
RIGHT shows a method used only the right context. LEFT 
shows a method used only the left context. BOTH represents a 
method using both the right and left context. The number 
shows the window size. ∞ uses all words in each context di-
rection. 
 
 
Clasifier	   F-­‐
Measure	  
Training	  
Time	  (sec)	  
AdaBoost	  	  (Freund	  196)	   0.592	   40.192	  
Baging	  	  (Breiman	  196)	   0.739	   	  30.310	  
Decision	  Tre	  (Quinlan1993)	   0.698	   239.446	  
Logistic	  Regression	   0.729	   696.704	  
Naive	  Bayes	   0.	  741	   7.383	  
Nearest	  Neighbor	   0.695	   22.441	  
Random	  Forest	  (Breiman	  201)	   0.729	   38.683	  
SVM	  (RBF	  kernel)	  	  
(Cortes	  and	  Vapnik	  195)	  
0.738	   92.723	  
SVM	  (polynomial	  kernel;	  d=2)	   0.756	   13.256	  
Table 2: Machine Learning Methods and Perform-
ance (F-measure and Training Time) 
 
 
 
 TWEET-SVM: The proposed SVM-based 
method (window size = 6). 
 TWEET-RAW: A simple frequency-based 
method. This aproach outputs the relative 
frequency of word “influenza” appearing in 
Twiter. 
 DRUG: The amounts of drug sales (sales of 
cold medicines). Statistics are provided by 
the Japanese Ministry of Health, Labor and 
Welfare. 
 GOOGLE: Gogle flu trend detection (Japane-
se version). This method uses a query log of 
the Gogle search engine (Ginsberg et al., 
2009)
5
. 
                                                             
5
 http:/ww.google.org/flutrends/ 
 
1571
 
 
5.2 Gold
Standard and Test-Set 
For gold standard data, we used data that are de-
scribed in Section 2, as reported from IDSC. The 
report is released once a wek. Therefore, the 
evaluation is done on a weekly basis. 
We split the data into four seasons as follows: 
 Season I: winter 208, 
 Season I: sumer 209, 
 Season II: winter 209, 
 Season IV: sumer 2010. 
 
To investigate further detailed evaluations, we split 
the winters into two sub-seasons: before the peak 
and after the peak. We regard the peak point as 
the day with the highest number in that season. The 
statistics derived from the data are presented in 
Table 3. 
 
Excesive News Period: In our experimental data, 
Season I and the earlier peak of Season II are 
special periods because news related to swine flu 
(H1N1 flu) is extremely hot in those seasons (Fig. 
4). This paper cals them Excesive News Periods. 
We also investigated the results with and without 
the excessive news period. 
 
 
Figure 4: A CN news on “swine flu” in June 
2009 (Season I in our experiment). 
Experimental data include such excesive news peri-
ods. 
 
5.3 Evaluation
Metric 
The evaluation metric is based on corelation 
(Pearson correlation) betwen the gold standard 
value and the estimated value. 
5.4 Result

The results are presented in Table 4. In the non-
excessive news period, the proposed method 
achieved the highest performance (0.890 corela-
tion). This corelation is considerably higher than 
the query-based approach (GOOGLE), demonstrat-
ing the basic feasibility of the proposed approach. 
However, during the excessive news periods, the 
proposed method suffers from an avalanche of 
news, generating a news bias. This phenomenon is 
a remaining problem to be resolved in future stud-
ies. 
 
6 Discusion

6.1 SVM-based Negative Filtering contributes 
to Performance 
In most seasons, the proposed SVM aproach 
(TWEET-SVM) shows higher correlation than the 
simple word lookup method (TWEET-RAW). The 
average improvement is 0.196 (max 0.56; min-
0.009), which significantly boosts the correlation. 
This result demonstrates the basic feasibility of the 
proposed approach. In the future, more advantages 
attributable to the proposed approach can be ob-
tained if the clasification performance improves. 
6.2 All
Methods Suffer from News Bias in 
Excessive News Period 
All methods expose the por performance that pre-
vails during the excesive news period (from Sea-
son I to Season II before the peak). Especialy, 
twet-based methods show dramaticaly reduced 
correlation, which indicates that Twitter is vulner-
able to newswire bias. 
One reason for that vulnerability is that Twiter 
is a kind of comunication tool by which a twet 
affects other people. Consequently, the possibility 
exists that a few tweets related to “flu” might 
spread widely, generating an explosive burst of 
influenza-related twets. Future studies must ad-
dres this burst phenomenon. 
 
1572
 
 
6.3 Twets
have Advantages in Early Stage 
Detection 
From practical viewpoints, the most important task 
is to detect influenza epideics before the peak 
(early stage detection). Consequently, the corela-
tion of the two seasons, Season I before the peak 
and Season II before the peak, presents the practi-
cal performance. Figure 5 portrays detailed results 
of al methods. 
In Season I before the peak (Figure 5 Left), the 
proposed method (TWEET-SVM) shows the best 
performance among al methods. 
In Season II before the peak (Figure 5 Right), 
all methods including the proposed method showed 
poor corelation because they are included in the 
excessive news periods. During that season, the 
newswires heavily reported the swine flu twice 
(April 209 and May 2009). Because of this news, 
we can se two peaks in Twiter-based methods 
(TWET-SVM and TWET-RAW), which indi-
cates that Twitter is more sensitive to the news-
wires. 
 
 
 
Table 3: Test-set Tracks and the number of data points (=weks). 
The number in the bracket indicates the statistical significance level. 
 
 
	  
TWEET-RAW TWEET-SVM 
(Proposed 
Method) 
DRUG GOOGLE 
Excessive	  news	  period	  	   0.001	   0.060	   0.844	   0.918	  
Non-­‐ excessive	  news	  period	   0.831	   0.890	   0.308	   0.847	  
	  	   0.683	   0.816	   -­‐0.208	   0.817	  
Before	  peak	   0.914	   0.974	   -­‐0.155	   0.962	  
	  
	  
Season	  I	  
After	  peak	   0.952	   0.955	   0.557	   0.959	  
Season	  II	  	   -­‐0.009	   -­‐0.018	   0.406	   0.232	  
	   0.382	   0.474	   0.684	   0.881	  
Before	  peak	   0.390	   0.474	   0.919	   0.924	  
	  
	  
Season	  III	  
After	  peak	   0.960	   0.944	   0.364	   0.936	  
Season	  IV	   0.391	   0.957	   0.130	   0.976	  
Table 4: Results (Correlation Ratio). 
The number in bold indicates the significance corelation (p=0.05). The number with underline indicates the highest value in each 
season. 
 
 
All	  Season	  
79	  weks	  	  (0.221)	  
Season	  I	  
	  
2008/11/9	  -­‐	  2009/4/5	  
Season	  I	  
	  
2009/4/12	  -­‐	  	  
2009/7/5	  
Season	  II	  
	  
2009/7/12	  -­‐	  2010/2/14	  
Season	  IV	  
	  
2010/2/21	  -­‐	  
2010/7/4	  
22	  weks	  	  (0.423)	   26	  	  weks	  (0.38)	  
Before	  peak	  
2008/11/9-­‐2009/1/25	  
After	  peak	  
2009/2/1-­‐2009/4/5	  
Before	  peak	  
2009/7/12-­‐2009/11/29	  
After	  peak	  
2009/12/6-­‐2010/2/14	  
12weeks	  (0.576)	   10	  weks	  
(0.632)	  
	  
13	  weks	  
(0.53)	  
15	  weks	  
(0.514)	  
11	  weks	  
(0.602)	  
	  
18	  weks	  
(0.468)	  
Non-­‐excessive	  news	  period	  	  
	  
Excesive	  news	  period	  	   Non-­‐excessive	  news	  period	  	  
1573
 
 
6.4 Human
Action is Sensitive before Epi-
demics 
Figure 6 presents the distribution betwen the de-
tected values (using GOOGLE and using TWEET-
SVM) and the gold standard value (before the peak 
is shown by “+”; that after the peak is shown as “-
”). Although the detected values fundamentaly 
corelate with the gold standard, we can see difer-
ent sensitivity before and after peak (The distribu-
tion before peak “+” is a higher value than after 
peak “-”.). 
 Results show that human action, a web search 
(GOOGLE) and a tweet (TWEET-SVM), highly cor-
responds to the real influenza before the epidemic 
peaks, and vice versa. More acute detection is pos-
sible if we incorporate a model considering this 
aspect of human nature. 
7 Related
Works 
The core technology of the proposed method is to 
classify whether the event is positve or negative. 
This task is similar to negation identification, 
which is a traditional topic, especialy in medical 
fields. Therefore, we can find many previous stud-
ies of the topic in the relevant literature. An algo-
rithm based aproach, NegEx (Chapman et al., 
2001), Negfinder (Mutalik et al., 201), and Con-
Text (Chapman et al., 207), a machine learning 
based approach (Elkin et al., 2005; Huang and H.J. 
Lowe, 207). 
 
 
 
 
 
 
 
 
 
 
 
Figure 5: Predicted Values in Season I (Left) and Season I (Right): 
the X-axis shows the date; the Y-axis shows the relative predicted value using each method. 
 
 
 
       
Figure 6: Patient Actions (Web Search Query and Twet) is Sensitive before the Epidemic Peaks. 
Distribution between the gold standard and Detected Values (Search Engine Query (Left) and Tweet (Right):  “+” denotes the 
distribution before the peak; “-” denotes the distribution after the peak. 
 
1574
 
 
 
 Previous	  
Negation	  
	  
(Syntactic)	  
This	  study:	  
Negative	  
Influenza	  
(Semantic)	  
I	  caught	  a	  flu.	  
 
Positive	  
sentence	  
Positive	  
Influenza 
I	  don’t	  have	  the	  flu!	  
 
Negative	  
sentence	  
Negative	  
Influenza 
I	  have	  enough	  flu	  drugs.	   Positive	  
sentence	  
Negative	  
Influenza 
I	  have	  not	  recovered	  from	  
the	  flu. 
Negative	  
sentence 
Positive	  
Influenza 
Table 5: Our target influenza negation (semantic) 
and previous negation (syntactic) 
 
 
Although these aproaches specificaly examine 
the syntactic negation, this study detects the nega-
tive influenza, which is a specified semantic nega-
tion. Table 5 presents the difference betwen both 
negations. In general, the semantic operation is 
dificult in general. However, this paper revealed 
that the domain (influenza domain) specific seman-
tic operation provides reasonable results. 
Another aspect of this study is the target mate-
rial, Twiter data, which have drawn much aten-
tion. Twitter can provide suitable material for 
many aplications such as named entity recogni-
tion (NER) (Finin et al., 2010) and sentiment 
analysis (Barbosa and Feng, 2010). Although these 
studies specifically examine the fundamental NLP 
techniques, this study directly targets an NLP ap-
plication that can contribute to our daily life. 
8 Conclusion

This paper proposed a new Twiter-based influenza 
epidemics detection method, which relies on the  
Natural Language Procesing (NLP). Our proposed 
method could sucesfuly filter out the negative 
influenza twets (f-measure=0.76), which are post-
ed by the ones who did not actually catch the influ-
enza. The experiments with the tes dat 
empirically demonstrate that the proposed method 
detects influenza epidemics with high corelation 
(correlation ratio=0.89), which outperforms the 
state-of-the-art Gogle method. This result shows 
that Twitter texts precisely reflect the real world, 
and that the NLP technique can extract the useful 
information from Twitter streams. 
 
 
Figure 7:  An influenza severance system “INFLU 
kun” using the proposed method is available at 
http:/mednlp.jp/influ/. 
 
 
Figure 8: The Timeline of Influenza Epidemics in 
Fukushima. While the Infection Disease Surveil-
lance Center (IDSC) sometimes stops (gold stan-
dard) due to the Great East Japan Earthquake, the 
proposed system could continue to work (Our Sys-
tem). 
 
Available Resources 
Corpus: The corpus of this study is provided at the 
http:/mednlp.jp/~aramaki/KAZEMIRU/. 
Web System: The web service is also released at 
http:/mednlp.jp/influ/ (Figure 7 and Figure 8). 
1575
 
 
References 
Barbosa, L. and J. Feng. 2010. Robust Sentiment Detec-
tion on Twitter from Biased and Noisy Data. In Proc. 
23rd Intl. Conf. on Computational Linguistics 
(COLING). 
Boyd, D., S. Golder, and G. Lotan. 2010. Twet, twet, 
retwet: Conversational aspects of retweting on 
Twiter. In Proc. HICS43. 
Breiman L. Random Forests. 2001. Machine learning, 
45(1): 5–32. 
Breiman, L. Baging predictors. 196. Machine learn-
ing, 24(2):123–140. 
Cortes C. and V. Vapnik. 195. Suport vector net-
works. In Machine Learning, p. 273–297. 
Chapman, W., W. Bridewel, P. Hanbury, G.F. Coper, 
and B. Buchanan. 2001. A simple algorithm for iden-
tifying negated findings and diseases in discharge 
sumaries. Journal of Biomedical Informatics, 
5:301-310. 
Chapman, W., J. Dowling, and D. Chu. 2007. ConText: 
An algorithm for identifying contextual features from 
clinical text. Biological, translational, and clinical 
language processing (BioNLP207), p. 81–88. 
Elkin, P.L., S.H. Brown, B.A. Bauer, C.S. Huser, W. 
Carruth, L.R. Bergstrom, and D.L. Wahner-Roedler. 
2005. A controlled trial of automated classification of 
negation from clinical notes. BMC Medical Informat-
ics and Decision Making 5:13. 
Espino, J., W. Hogan, and M. Wagner. 203. Telephone 
triage: A timely data source for surveillance of influ-
enza-like diseases. In Proc. of Annual Symposium of 
AMIA, p. 215–219. 
Finin, T., W. Murnae, A. Karndikar, N. Keler, J. 
Martineau, and M. Dredze. 2010. Anotating named 
entities in Twiter data with crowdsourcing. In Proc. 
NAACL HLT 2010 Workshop on Creating Speech 
and Language Data with Amazon's Mechanical Turk 
(CSLDAMT '10), p. 80-88. 
Freund, Y. and R. Schapire. 196. Experiments with a 
new boosting algorithm. In Machine Learning Intl. 
Workshop, p.148–156. 
Ginsberg, J., M.H. Mohebi, R.S. Patel, and L. Bram-
mer. 209. Detecting influenza epidemics using 
search engine query data, Nature Vol. 457 (19). 
Huang, Y. and H.J. Lowe. 2007. A novel hybrid ap-
proach to automated negation detection in clinical ra-
diology reports. Journal of the American Medical 
Informatics Asociation, 14(3):304-311. 
Huberman, B. and D. R. F. Wu. 209. Social networks 
that matter: Twitter under the microscope. First 
Monday, Vol. 14. 
Hulth, A., G. Rydevik, and A. Linde. 209. Web Queri-
es as a Source for Syndromic Surveilance. PLoS 
ONE 4(2). 
Johnson, HA., MM. Wagner, WR. Hogan, W. Chapman, 
RT. Olszewski, J. Dowling, and G. Barnas. 204. 
Analysis of Web aces logs for surveilance of in-
fluenza. Stud. Health Technol. Inform. 107(Pt 
2):1202-1206. 
Magruder, S. 2003. Evaluation of over-the-counter 
pharmaceutical sales as a possible early warning indi-
cator of human disease. Johns Hopkins University 
APL Technical Digest 24:349–353. 
Milstein, S., A. Chowdhury, G. Hochmuth, B. Lorica, 
and R. Magoulas. 208. Twiter and the micro-
mesaging revolution: Communication, connections, 
and imediacy, 140 characters at a time. O’Reily 
Media. 
Mutalik, P.G., A. Deshpande, and P.M. Nadkarni. 2001. 
Use of general purpose negation detection to augment 
concept indexing of medical documents: A quantita-
tive study using theUMLS. Journal of the American 
Medical Informatics Association, 8(6):598-609. 
Paul, MJ. and M. Dredze. 201. You Are What You 
Twet: Analyzing Twiter for Public Health. In Proc. 
of the 5th International AAI Conference on We-
blogs and Social Media (ICWSM). 
Polgren, PM., Y. Chen, D.M. Penock, and F.D. Nel-
son. 208. Using Internet Searches for Influenza Sur-
veilance, Clinical Infectious Diseases Vol. 47 (11) 
pp. 1443-1448. 
Quinlan. J. 193. C4. 5: programs for machine learning. 
Morgan Kaufman. 
Sakaki, T., M. Okazaki, and Y. Matsuo. 2010. Earth-
quake shakes Twiter users: real-time event detection 
by social sensors, in Proc. of Conf. on World Wide 
Web (WWW). 
 
 
 
 
 
1576

