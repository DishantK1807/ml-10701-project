This paper presents an empirical study measuring the effectiveness of our evaluation functions at selecting training sentences from the Wall Street Journal (WSJ) corpus (Marcuset al., 1993) for inducing grammars.
J93-2004
For instance, Hermjakob and Mooney (1997) have described a learning system that can build a deterministic shiftreduce parser from a small set of training examples with the aid of detailed morphological, syntactical, and semantic knowledge databases and step-by-step guidance from human experts.
P97-1062
There are two possible directions: one might attempt to reduce the amount of annotations in each sentence, as was explored by Hwa (1999); alternatively, one might attempt to reduce the number of training sentences.
P99-1010
The learning algorithm we use is a variant of the Inside-Outside algorithm that induces grammars expressed in the Probabilistic Lexicalized Tree Insertion Grammar representation (Schabes and Waters, 1993; Hwa, 1998).
P98-1091
As shown in Pereira and Schabes (1992), an essentially unsupervised learning algorithm such as the InsideOutside re-estimation process (Baker, 1979; Lari and Young, 1990) can be modified to take advantage of these bracketing constraints.
H92-1024
Some examples include text categorization (Lewis and Gale, 1994), part-of-speech tagging (Engelson and Dagan, 1996), word-sense disambiguation (Fujii et al., 1998), and prepositional-phrase attachment (Hwa, 2000).
J98-4002 P96-1042
The success of recent high-quality parsers (Charniak, 1997; Collins, 1997) relies on the availability of such treebank corpora.
P97-1003
