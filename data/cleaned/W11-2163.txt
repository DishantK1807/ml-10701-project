Proceedings of the 6th Workshop on Statistical Machine Translation, pages 496–500,
Edinburgh, Scotland, UK, July 30–31, 2011. c©2011 Association for Computational Linguistics
HierarchicalPhrase-BasedMTattheCharlesUniversity
fortheWMT2011 SharedTask
DanielZeman
CharlesUniversityinPrague,InstituteofFormalandAppliedLinguistics(ÚFAL)
UniverzitaKarlovavPraze,Ústavformálníaaplikovanélingvistiky(ÚFAL)
Malostranskénáměstí25,Praha,CZ-11800,Czechia
zeman@ufal.mff.cuni.cz
Abstract
We describeour experimentswith hier-
archicalphrase-basedmachinetranslation
for the WMT 2011 Shared Task. We
traineda systemfor all 8 translationdi-
rectionsbetweenEnglishon onesideand
Czech, German, Spanish or French on
theotherside,thoughwefocusedslightly
more on the English-to-Czechdirection.
We provide a detaileddescriptionof our
configurationand data so the resultsare
replicable.
1 Introduction
Withsomanyofficiallanguages,Europeisapar-
adiseformachinetranslationresearch.Oneofthe
largestbodiesof electronicallyavailableparallel
textsisbeingnowadaysgeneratedbytheEuropean
Unionand itsinstitutions. Atthesametime,the
EUalsoprovidesmotivationandboostspotential
marketformachinetranslationoutcomes.
Mostof themajorEuropeanlanguagesbelong
to one of the following three branches of the
Indo-Europeanlanguagefamily: Germanic,Ro-
manceor Slavic. Suchrelatednessisresponsible
for manystructuralsimilaritiesin Europeanlan-
guages, although significantdifferencesstillex-
ist. Withinthelanguageportfolioselectedforthe
WMTsharedtask, English,Frenchand Spanish
seemtobeclosertoeachotherthantotherest.
German,despitebeinggeneticallyrelatedtoEn-
glish, differs in many properties. Its word or-
derrules,shiftingverbsfromoneendofthesen-
tenceto theother, easilycreatelong-distancede-
pendencies. Long Germancompoundwordsare
notorious for increasingout-of-vocabularyrate,
whichhas led many researchersto devisingun-
supervisedcompound-splittingtechniques. Also,
uppercase/lowercasedistinctionismoreimportant
becauseallGermannounsstartwithanuppercase
letterbytherule.
Czechisalanguagewithrichmorphology(both
inflectionaland derivational)and relativelyfree
wordorder. Infact,thepredicate-argumentstruc-
ture,oftenencodedbyfixedwordorderinEnglish,
is usually capturedby inflection(especiallythe
systemof 7 grammaticalcases)in Czech. While
the free wordorderof Czechis a problemwhen
translatingto English(the text should be parsed
firstin orderto determinethesyntacticfunctions
andtheEnglishwordorder),generatingcorrectin-
flectionalaffixesisindeedachallengeforEnglish-
to-Czechsystems. Furthermore,the multitude
of possibleCzechword forms(at leastorder of
magnitudehigherthaninEnglish)makesthedata
sparsenessproblemreallysevere,hinderingboth
directions.
There are numerous ways how these issues
could be addressed. For instance, parsing and
syntax-awarereordering of the source-language
sentencescan help with the word order differ-
ences(samegoalcouldbeachievedby areorder-
ingmodelorasynchronouscontext-freegrammar
in a hierarchicalsystem). Factoredtranslation,a
secondarylanguagemodelof morphologicaltags
orevenamorphologicalgeneratoraresomeofthe
possiblesolutionstothepoor-to-richtranslationis-
sues.
Our goal is to run one systemunder as simi-
lar conditionsas possibleto all eight translation
directions,tocomparetheirtranslationaccuracies
andseewhysomedirectionsareeasierthanothers.
Futureworkwillbenefitfromknowingwhatare
thespecialprocessingneedsforagivenlanguage
pair. Thecurrentversionofthesystemdoesnotin-
cludereallylanguage-specifictechniques:wenei-
thersplitGermancompounds,nordo weaddress
thepeculiaritiesofCzechmentionedabove. Still,
comparabilityoftheresultsislimited,asthequal-
ityandquantityofEnglish-Czechdatadiffersfrom
thatoftheotherpairs.
496
2 TheTranslationSystem
Our translationsystembelongs to the hierarchi-
calphrase-basedclass(Chiang,2007),i.e. phrase
pairs with nonterminals(rulesof a synchronous
context-freegrammar)are extractedfrom sym-
metrizedwordalignmentsandsubsequentlyused
bythedecoder. WeuseJoshua,aJava-basedopen-
sourceimplementationofthehierarchicaldecoder
(Lietal.,2009),release1.3. 1
Word alignmentwascomputedusing the first
three steps of thetrain-factored-phrase-
model.perlscriptpackedwithMoses 2 (Koehnet
al.,2007). Thisincludestheusualcombinationof
wordclusteringusingmkcls3 (Och,1999), two-
way word alignmentusing GIZA++ 4 (Och and
Ney, 2003), and alignmentsymmetrizationusing
the grow-diag-final-and heuristic(Koehn et al.,
2003).
For language modeling we use the SRILM
toolkit 5 (Stolcke, 2002) with modified Kneser-
Neysmoothing(KneserandNey,1995;Chenand
Goodman,1998).
We use the Z-MERT implementationof mini-
mumerrorratetraining(Zaidan,2009). Thefol-
lowingsettingshavebeenusedforJoshuaandZ-
MERT(forthesakeofreproducibility,wekeepthe
originalnamesoftheoptions;fortheirdetailedex-
planationpleaserefertothedocumentationavail-
ableon-lineattheJoshuaprojectsite).-ipiisthe
numberofintermediateinitialpointsperZ-MERT
iteration.
• Grammarextraction:
maxPhraseSpan=10maxPhraseLength=5
maxNonterminals=2maxNontermi-
nalSpan=2requireTightSpans=true
edgeXViolates=truesentenceIni-
tialX=truesentenceFinalX=true
ruleSampleSize=300
• Languagemodelorder:6(hexagram)
• Decoding:span_limit=10fuzz1=0.1
fuzz2=0.1max_n_items=30rela-
tive_threshold=10.0max_n_rules=50
rule_relative_threshold=10.0
1http://sourceforge.net/projects/joshua/
2http://www.statmt.org/moses/
3http://fjoch.com/mkcls.html
4http://fjoch.com/GIZA++.html
5http://www-speech.sri.com/projects/srilm/
• N-bestdecoding:use_unique_nbest=true
use_tree_nbest=false
add_combined_cost=truetop_n=300
• Z-MERT:-mBLEU4closest-maxIt5
-ipi20
3 DataandPre-processingPipeline
Weappliedoursystemtoalleightlanguagepairs.
From the data point of view the experiments
wereeven moreconstrainedthan the organizers
of the shared task suggested. We used neither
the French/Spanish-EnglishUN corpora nor the
109 French-Englishcorpus. For7 translationdi-
rectionswe used the Europarl ver6 and News-
Commentaryver6corpora 6 fortraining.Thetarget
sideofthecorporawasouronlysourceofmonolin-
gualdatafortrainingthelanguagemodel.Table1
showsthesizeofthetrainingdata.
For the English-Czech direction, we used
CzEng0.9 (Bojarand Žabokrtský,2009) 7 asour
mainparallelcorpus. FollowingCzEngauthors’
request,wedidnotusesections8*and9*reserved
forevaluationpurposes.
In addition, we also used the EMEAcorpus 8
(Tiedemann,2009). 9
Czechwas also the only language wherewe
used extra monolingual data for the language
model.Itwasthesetprovidedbytheorganizersof
WMT2010 (13,042,040 sentences,210,507,305
tokens).
We use a slightlymodifiedtokenizationrules
comparedtoCzEngexportformat.Mostnotably,
we normalizeEnglish abbreviatednegation and
auxiliaryverbs (“couldn’t” → “could not”) and
attemptatnormalizingquotationmarksto distin-
guishbetweenopeningandclosingonefollowing
propertypesettingrules.
Therestofourpre-processingpipelinematches
the processing employed in CzEng (Bojar and
Žabokrtský,2009). 10 Weuse“supervisedtruecas-
ing”,meaningthatwecastthecaseofthelemma
totheform,relyingonourmorphologicalanalyz-
ersandtaggerstoidentifypropernames,allother
6 Availablefor
download athttp://www.statmt.org/
wmt11/translation-task.htmlusing the link “Parallel
corpustrainingdata”.
7http://ufal.mff.cuni.cz/czeng/
8http://urd.let.rug.nl/tiedeman/OPUS/EMEA.php
9 Unfortunately, theEMEAcorpusisbadlytokenizedon
theCzechsidewithfractionalnumberssplitintoseveralto-
kens(e.g. “3,14”). Weattemptedtoreconstructtheoriginal
detokenizedformusingasmallsetofregularexpressions.
497
Corpus SentPairs Tokensxx Tokensen
cs-en 583,124 13,224,596 15,397,742
de-en 1,857,087 48,834,569 51,243,594
es-en 1,903,562 54,488,621 52,369,658
fr-en 1,920,363 61,030,918 52,686,784
en-cs 7,543,152 79,057,403 89,018,033
Table1: Numberofsentencepairsandtokensfor
every languagepair in the paralleltrainingcor-
pus. Languagesareidentifiedby theirISO639
codes:cs=Czech,de=German,en=English,es=
Spanish,fr=French.Theen-cslinedescribesthe
CzEng+EMEAcombinedcorpus,allotherlines
correspondtotherespectiveversionsofEuroParl
+NewsCommentary.
wordsarelowercased.
Notethatinsomecasesthegrammarextraction
algorithminJoshuafailsifthetrainingcorpuscon-
tainssentencesthataretoo long. Removingsen-
tencesof100ormoretokens(peradvicebyJoshua
developers)effectivelyhealedallfailures. 11
TheNewsTest2008dataset 12 (2051sentences
in eachlanguage)wasusedasdevelopmentdata
for MERT. BLEUscoresreportedin this paper
werecomputedon theNewsTest2011 set(3003
sentenceseachlanguage).WedonotusetheNews
Test2009and2010.
4 Experiments
All BLEU scores were computed directly by
Joshua on the NewsTest 2011 set. Note that
theydifferfromwhattheofficialevaluationscript
wouldreport,duetodifferenttokenization.
4.1 BaselineExperiments
Thesetofbaselineexperimentswithalltranslation
directionsinvolvedrunningthesystemon lower-
casedNewsCommentarycorpora. Word align-
mentswerecomputedon lowercased4-character
stems. A hexagramlanguagemodelwastrained
onthetargetsideoftheparallelcorpus.
In theen-cscase,wordalignmentswerecom-
puted on lemmatizedversionof the parallelcor-
10 Duetothesubsequentprocessing,incl.parsing,thetok-
enizationofEnglishfollowsPennTreebenkstyle.Therather
unfortunateconventionoftreatinghyphenatedwordsassin-
gletokensincreasesourout-of-vocabularyrate.
11 Table1presentsstatistics
before removingthelongsen-
tences.
12http://www.statmt.org/wmt11/translation-
task.html
pus. Hexagramlanguagemodelwastrainedon
themonolingualdata. Truecaseddatawereused
fortraining,asdescribedabove;theBLEUscore
ofthisexperimentinTable2iscomputedontrue-
casedsystemoutput.
Direction BLEUJ BLEUl BLEUt
en-cs 0.1274 0.141 0.123
en-de 0.1324 0.128 0.052
en-es 0.2756 0.274 0.221
en-fr 0.2727 0.212 0.174
cs-en 0.1782 0.178 0.137
de-en 0.1957 0.187 0.137
es-en 0.2630 0.255 0.197
fr-en 0.2471 0.248 0.193
Table2: LowercasedBLEUscoresofthebaseline
experimentson NewsTest2011 data: BLEUJ is
computedby the system, BLEUl is the official
evaluationbymatrix.statmt.org(itdiffersbe-
causeof differenttokenization). BLEUt isoffi-
cialtruecasedevaluation.
Aninterestingperspectiveonthemodelsispro-
vided by the feature weights optimized during
MERT. We can see in Table 3 that translation
models are trusted significantlymore than lan-
guage modelsfor the en-de,de-enand es-endi-
rections.Infact,thelanguagemodelhasalowrel-
ativeweightinalllanguagepairsbuten-cs,which
was the only pair where we used a significant
amountof extramonolingualdata. In thefuture,
weshouldprobablyusetheGigawordcorpusfor
theto-Englishdirections.
Setup LM Pt0 Pt1 Pt2 WP
en-cs 1.0 1.04 0.84 −0.06 −1.19
en-de 1.0 2.60 0.57 0.47 −3.17
en-es 1.0 1.67 0.81 0.60 −2.96
en-fr 1.0 1.41 0.92 0.53 −2.80
cs-en 1.0 1.48 0.94 1.08 −4.55
de-en 1.0 2.28 1.11 0.34 −2.88
es-en 1.0 2.26 1.67 0.23 −0.84
fr-en 1.0 1.89 1.32 0.13 −0.04
Table3: Featureweightsarerelativetotheweight
of LM, the scoreby the languagemodel. Then
there are the three translationfeatures: Pt0 =
P(e|f), Pt1 = Plex(f|e) and Pt2 = Plex(e|f).
WP isthewordpenalty.
498
4.2 Efficiency
Themachinesonwhichtheexperimentswerecon-
ducted are 64bit Intel Xeon dual core 2.8 GHz
CPUswith32GBRAM.
Wordalignmentofeachparallelcorpuswasthe
mostresource-consumingsubtask.Ittookbetween
12and48hours,thoughitcouldbecuttoonehalf
by running both GIZA++directionsin parallel.
Thetimeneededfordatapreprocessingandtrain-
ing ofthelanguagemodelwasnegligible. Paral-
lelizedgrammarextractiontook19processorsfor
aboutanhour. Fordecodingthetestdataweresplit
into20chunksthatwereprocessedinparallel.One
MERTiteration,includingdecoding,tookfrom30
minutesto1hour.
Trainingoflargemodelsrequiressomecareful
engineering. Thegrammarextractioneasilycon-
sumesover 20 GBmemoryso it is importantto
makesure Javareallyhas accessto it. The de-
codermustuse the SWIG-linkedSRILMlibrary
becauseJava-basedlanguagemodelingistooslow
andmemory-consuming.
4.3 SupervisedTruecasing
Ourbaselineexperimentsoperatedonlowercased
data,exceptforen-cs,wheretruecasedwordforms
wereobtainedusinglemmasfrommorphological
annotation(notethatguessingof thetruecaseis
only neededfor the sentence-initialtoken, other
wordscanjustbeleftintheiroriginalform).
Ascontrastiveruns weappliedthe supervised
truecasingto other directionsas well. We used
theMorčetaggerforEnglishlemmatization,Tree-
TaggerforGermanandtwosimplerule-basedap-
proaches to Spanish and French lemmatization.
AllthesetoolsareembeddedintheTectoMTanal-
ysisframework(Žabokrtskýetal.,2008).
TheresultsareinTable4. BLEUthasincreased
inallcasesw.r.t.thebaselineresults.
4.4 AlignmentonLemmas
Onceweareabletolemmatizeallfivelanguages
we can also experimentwith word alignments
basedon lemmas. Table5 showsthatthediffer-
encesinBLEUareinsignificant.
5 Conclusion
We have describedthe hierarchicalphrase-based
SMTsystemweusedfor the WMT2011 shared
task. We discussedexperimentswithlarge data
Direction BLEUJ BLEUl BLEUt
en-cs 0.1191 0.126 0.119
en-de 0.1337 0.131 0.127
en-es 0.2573 0.276 0.265
en-fr 0.2591 0.211 0.189
cs-en 0.1692 0.180 0.168
de-en 0.1885 0.191 0.178
es-en 0.2446 0.260 0.236
fr-en 0.2243 0.245 0.221
Table4: Resultsof experimentswithsupervised
truecasing.Notethattrainingontruecasedcorpus
slightlyinfluencedeventhelowercasedBLEU(cf.
withTable2). Thisisbecauseprobabilitiesofto-
kensthatmayappearbothuppercasedandlower-
cased(withdifferentmeanings)havechanged,and
thusdifferenttranslationmayhavebeenchosen.
Direction BLEUJl4 BLEUJlm
en-cs 0.1191 0.1193
en-de 0.1337 0.1318
en-es 0.2573 0.2590
en-fr 0.2591 0.2592
cs-en 0.1692 0.1690
de-en 0.1885 0.1892
es-en 0.2446 0.2452
fr-en 0.2243 0.2244
Table5: Resultsofexperimentswithwordalign-
mentcomputedondifferentfactors. BLEUJl4is
thescorecomputedby Joshuaon lowercasedtest
datafortheoriginalexperiments(alignmentbased
on lowercased4-characterprefixes). BLEUJlm
isthecorrespondingscoreforalignmentbasedon
lemmas.
fromthepointofviewofboththetranslationac-
curacyandefficiency. Weusedmoderately-sized
trainingdata and took advantagefrom their ba-
siclinguisticannotation(lemmas).Thetruecasing
techniquehelpedustobettertargetnamedentities.
Acknowledgements
The work on this projectwas supported by the
grantP406/11/1499oftheCzechScienceFounda-
tion(GAČR).
References
OndřejBojarand ZdeněkŽabokrtský. 2009. Czeng
0.9: Large paralleltreebankwith rich annotation.
499
The Prague Bulletin of MathematicalLinguistics ,
92:63–83.
StanleyF.ChenandJoshuaGoodman. 1998. Anem-
piricalstudyof smoothingtechniquesforlanguage
modeling.In TechnicalreportTR-10-98,Computer
Science Group , Harvard,MA,USA,August.Har-
vardUniversity.
David Chiang. 2007. Hierarchical Phrase-
Based Translation. Computational Linguistics ,
33(2):201–228.
Reinhard Kneser and HermannNey. 1995. Im-
provedbacking-offform-gramlanguagemodeling.
In Proceedings of the IEEE International Confer-
ence on Acoustics,Speech and Signal Processing ,
pages 181–184, Los Alamitos, California, USA.
IEEEComputerSocietyPress.
PhilippKoehn, FranzJosefOch, and DanielMarcu.
2003. Statisticalphrase-basedtranslation. In
NAACL’03: Proceedings of the 2003 Conference
of the North AmericanChapter of the Association
for ComputationalLinguisticson HumanLanguage
Technology , pages 48–54, Morristown,NJ, USA.
AssociationforComputationalLinguistics.
PhilippKoehn, HieuHoang, AlexandraBirch,Chris
Callison-Burch,MarcelloFederico,NicolaBertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
RichardZens,ChrisDyer,OndřejBojar,Alexandra
Constantin,andEvanHerbst. 2007. Moses:Open
SourceToolkit for StatisticalMachineTranslation.
In Proceedings of the 45th Annual Meetingof the
AssociationforComputationalLinguisticsCompan-
ionVolumeProceedingsoftheDemoandPosterSes-
sions ,pages177–180,Praha,Czechia,June.Associ-
ationforComputationalLinguistics.
ZhifeiLi, ChrisCallison-Burch,SanjeevKhudanpur,
and Wren Thornton. 2009. Decodingin Joshua:
Open Source,Parsing-BasedMachineTranslation.
The Prague Bulletin of MathematicalLinguistics ,
91:47–56,1.
FranzJosefOchandHermannNey. 2003. Asystematic
comparisonofvariousstatisticalalignmentmodels.
ComputationalLinguistics ,29(1):19–51.
FranzJosefOch. 1999. Anefficientmethodfordeter-
miningbilingualwordclasses.In Proceedingsofthe
NinthConferenceoftheEuropeanChapteroftheAs-
sociationforComputationalLinguistics(EACL’99) ,
pages71–76,Bergen,Norway,June.Associationfor
ComputationalLinguistics.
AndreasStolcke.2002. Srilm–anextensiblelanguage
modelingtoolkit. In Proceedings of International
Conference on Spoken Language Processing , Den-
ver,Colorado,USA.
JörgTiedemann.2009. Newsfromopus–acollection
ofmultilingualparallelcorporawithtoolsandinter-
faces.In RecentAdvancesinNaturalLanguagePro-
cessing(vol.V) ,pages237–248.JohnBenjamins.
ZdeněkŽabokrtský,JanPtáček,andPetrPajas.2008.
TectoMT: Highly modular MT system with tec-
togrammaticsusedastransferlayer. In ACL2008
WMT: Proceedingsof theThird Workshopon Statis-
tical MachineTranslation , pages167–170, Colum-
bus,OH,USA.AssociationforComputationalLin-
guistics.
OmarF. Zaidan. 2009. Z-mert:Afullyconfigurable
opensourcetoolforminimumerrorratetrainingof
machinetranslationsystems. ThePragueBulletinof
MathematicalLinguistics ,91:79–88.
500

