Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 16–22,
Uppsala, Sweden, 15 July 2010. c©2010 Association for Computational Linguistics
Domain Adaptation to Sumarize Human Conversations 
 
 
Oana Sandu, Giuseppe Carenini, Gabriel Murray, and Raymond Ng 
University of British Columbia 
Vancouver, Canada 
{oanas,carenini,gabrielm,rng}@cs.ubc.ca 
 
  
 
Abstract 
 
We are interested in improving the sum-
marization of conversations by using 
domain adaptation. Since very few email 
corpora have ben anotated for suma-
rization purposes, we atempt to leverage 
the labeled data available in the multi-
party metings domain for the sumari-
zation of email threads. In this paper, we 
compare several aproaches to super-
vised domain adaptation using out-of-
domain labeled data, and also try to use 
unlabeled data in the target domain 
through semi-supervised domain adapta-
tion. From the results of our experiments, 
we conclude that with some in-domain 
labeled data, training in-domain with no 
adaptation is most efective, but that 
when there is no labeled in-domain data, 
domain adaptation algorithms such as 
structural corespondence learning can 
improve sumarization. 
1 Introduction

On a given day, many people engage in conver-
sations via several modalities, including face-to-
face spech, telephone, email, SMS, chat, and 
blogs. Being able to produce automatic suma-
ries of multi-party conversations ocuring in one 
or several of these modalities would enable the 
parties involved to kep track of and make sense 
of this diverse data. However, sumarizing spo-
ken dialogue is more chalenging than sumariz-
ing writen monologues such as boks and arti-
cles, as spech tends to be more fragmented and 
disfluent. 
We are interested in using both fuly and semi-
supervised techniques to produce extractive 
sumaries for conversations, where each sen-
tence of a text is labeled with its informativenes, 
and a subset of sentences are concatenated into 
an extractive sumary of the text. In previous 
work (Muray and Carenini, 208), it has ben 
shown that conversations in diferent modalities 
can be efectively characterized by a set of “con-
versational” features that are useful in detecting 
informativenes for the task of extractive sum-
marization. However, because of privacy con-
cerns, anotated corpora are rarely publicly 
available for conversational data, including for 
the email domain. One promising solution to this 
problem is domain adaptation, which aims to use 
labeled data in a wel-studied source domain and 
a limited amount of labeled data from a diferent 
target domain to train a model that performs wel 
in that target domain. In this work, we investi-
gate using domain adaptation that leverages la-
beled data in the domain of metings along with 
labeled and unlabeled email data for sumariz-
ing email threads. We evaluate several domain 
adaptation algorithms, using both a smal set of 
conversational features and a large set of simple 
lexical features to determine what setings wil 
yield the best results for sumarizing email con-
versations. In our experiments, we do not get a 
significant improvement from using out-of-
domain data in adition to in-domain data in su-
pervised domain adaptation, though in the seting 
where only unlabeled in-domain data is avail-
able, we gain from using it through structural 
corespondence learning. We also observe that 
conversational features are more useful in super-
vised methods, whereas lexical features are bet-
ter leveraged in semi-supervised adaptation. 
The next section surveys past research in do-
main adaptation and in sumarizing conversa-
tional data. In section 3 we present the corpora 
and feature sets we used, and we describe our 
experimental seting in section 4. We then com-
pare the performance of diferent methods in sec-
tion 5 and draw conclusions in section 6. 
16
2 Related
Work 
We give an overview first of work on supervised 
and semi-supervised domain adaptation, then of 
research on sumarization of conversations. 
2.1 Supervised
Domain Adaptation 
Many domain adaptation methods have ben 
proposed for the supervised case, where a smal 
amount of labeled data in the target domain is 
used along with a larger amount of labeled 
source data. Two baseline aproaches are to train 
only on the source data or only on target training 
data. One way of using information from both 
domains is merging the source and target labeled 
data sets and training a model on the combina-
tion. A method inspired by bosting is to take a 
linear combination of the predictions of two clas-
sifiers, one trained on the source and one trained 
on the target training data. Another simple me-
thod is to train a predictor on the source data, run 
it on the target data, and then use its predictions 
on each instance as aditional features for a 
target-trained model. This was first introduced 
by Florian et al. (204), who aplied it to 
multilingual named entity recognition. 
The prior method of domain adaptation by 
Chelba and Acero (206) involves using the 
source data to find optimal parameter values of a 
maximum entropy model on that data, and then 
seting these as a prior on the values of a model 
trained on the target data. They find improve-
ment in a capitalizer that adapts using out-of-
domain and a smal amount of in-domain data 
versus only training on out-of-domain WSJ data. 
Similar to the prior method, Daume’s MEGA 
model also trains a MEM. It achieves domain 
adaptation through hyperparameters that indicate 
whether an instance is generated by a source, 
target, or general distribution, and finds the op-
timal values of the parameters through condi-
tional EM (Daume and Marcu, 206). A simpler 
method of domain adaptation, that achieves a 
performance similar to prior and MEGA, was 
proposed by Daume (207) and sucesfuly ap-
plied to a variety of NPL sequence labeling prob-
lems, such as named entity recognition, shalow 
parsing, and part-of-spech (POS) taging. Fur-
thermore, this aproach is straightforward to ap-
ply by copying feature values so there is a source 
version, a target version, and a general version of 
the feature, and was found to be faster to train 
than MEGA and prior. For al these reasons, we 
use Daume’s method and not the other two in our 
experiments. 
2.2 Semi-supervised Domain Adaptation 
Because unlabeled data is usualy much easier to 
colect than labeled data in a new domain, semi-
supervised domain adaptation methods that ex-
ploit unlabeled data are potentialy very useful. 
In self-training, a training set is used that is 
originaly composed of labeled data, and repeat-
edly augmented with the highest confidence pre-
dictions on unlabeled data. McClosky et al. 
(206) aply this in a domain adaptation seting 
for parsing: with only unlabeled data in the target 
Brown domain, and labeled and unlabeled 
datasets in the news domain (WSJ and NANC 
respectively), a self-trained reranking parser per-
forms almost as wel as a parser trained only on 
Brown labeled data. However, McClosky con-
cludes that self-training alone is not beneficial, 
and most of the improvements they get over pre-
vious work on domain adaptation for parsing are 
due to using the reranker to select the candidate 
instances produced in each iteration of self-
training. Thus, one of the isues adresed in this 
paper is to ases whether self-training is useful 
for domain adaptation. 
A more sophisticated semi-supervised domain 
adaptation method is structural corespondence 
learning (SCL). SCL uses unlabeled data to de-
termine corespondences betwen features in the 
two domains by corelating them with so-caled 
pivot features, which are features exhibiting 
similar behaviors in the source and target do-
mains. Blitzer aplied this algorithm sucesfuly 
to POS taging (Blitzer et al., 2006) and senti-
ment clasification (Blitzer et al., 2007). SCL 
sems promising for other tasks as wel, for ex-
ample parse disambiguation (Plank, 209). 
2.3 Sumarization

We would like to use domain adaptation to aid in 
sumarizing multi-party conversations hailing 
from diferent modalities. This contrasts with 
much of previous work on sumarization of 
conversations, which has focused on domain-
specific features (e.g., Rambow et al, 2004). We 
wil treat sumarization as a supervised binary 
clasification problem where the sentences of a 
conversation are rated by their informativenes 
and a subset is selected to form an extractive 
sumary. Research in meting sumarization 
relevant to our task has investigated the utility of 
employing a large feature set including prosodic 
information, speaker status, lexical and structural 
discourse features (Muray et al., 206; Galey, 
206). For email sumarization, we view an 
17
email thread as a conversation. For sumarizing 
email threads, Rambow (204) used lexical fea-
tures such as tf.idf, features that considered the 
thread to be a sequence of turns, and email-
specific features such as number of recipients 
and the subject line. Asynchronous multi-party 
conversations were sucesfuly represented for 
sumarization through a smal number of con-
versational features by Muray and Carenini 
(208). This paved the way to cros-domain 
conversation sumarization by representing both 
email threads and metings with a set of comon 
conversational features. The work we present 
here investigates using data from both emails and 
metings in sumarizing emails, and compares 
using conversational versus lexical features. 
3 Summarization
setting 
Because the metings domain has a large corpus, 
AMI, anotated for sumarization, we wil use it 
as the source domain for adaptation and the 
email domain as the target, with data from the 
Enron corpus as unlabeled email data, and the 
BC3 corpus as test data. 
3.1 Datasets

The AMI meting corpus: We use the scenario 
portion of the AMI corpus (Carleta et al., 205), 
for which groups of four participants take part in 
a series of four metings and play roles within a 
fictitious company. While the scenario given to 
them is artificial, the spech and the actions are 
completely spontaneous and natural. The dataset 
contains aproximately 1500 dialogue act 
(DA) segments. For the anotation, anotators 
wrote abstract sumaries of each meting and 
extracted transcript DA segments that best con-
veyed or suported the information in the ab-
stracts. A many-to-many maping betwen tran-
script DAs and sentences from the human ab-
stract was obtained for each anotator, with thre 
anotators asigned to each meting. We con-
sider a dialogue act to be a positive example if it 
is linked to a given human sumary, and a nega-
tive example otherwise. Aproximately 13% of 
the total DAs are ultimately labeled as positive. 
The BC3 email corpus
1
: composed of 40 
email threads from the World Wide Web Con-
sortium (W3C) mailing list which feature a vari-
ety of topics such as web acesibility and plan-
ning face-to-face metings. Each thread is ano-
tated similarly to the AMI corpus, with thre an-
                                                
1
 htp:/ww.cs.ubc.ca/labs/lci/bc3.html 
notators authoring abstracts and linking email 
thread sentences to the abstract sentences. 
The Enron email corpus
2
: a colection of 
emails released as part of the investigation into 
the Enron corporation, it has become a popular 
corpus for NLP research due to being realistic, 
naturaly-ocuring data from a corporate envi-
ronment. We use 39 threads from this corpus to 
suplement the BC3 email data. 
3.2 Features
Used 
We consider two sets of features for each sen-
tence: a smal set of conversational structure fea-
tures, and a large set of lexical features. 
Conversational features: We extract 24 con-
versational features from both the email and 
metings domain, and which consider both 
emails and metings to be conversations com-
prised of turns betwen multiple participants. For 
an email thread, a turn consists of a single email 
fragment in the exchange. Similarly, for met-
ings, a turn is a sequence of dialogue acts by the 
same speaker. The conversational features, 
which are described in detail in (Muray and 
Carenini, 208), include sentence length, sen-
tence position in the conversation and in the cur-
rent turn, pause-style features, lexical cohesion, 
centroid scores, and features that measure how 
terms cluster betwen conversation participants 
and conversation turns. 
Lexical features: We derive an extensive set of 
lexical features, originaly proposed in (Muray 
et al., 2010) from the AMI and BC3 datasets, and 
then compute their ocurence in the Enron cor-
pus. After throwing out features that ocur les 
than five times, we end up with aproximately 
20,00 features. The features derived are: char-
acter trigrams, word bigrams, POS tag bigrams, 
word pairs, POS pairs, and varying instantiation 
ngram (VIN) features. For word pairs, we extract 
the ordered pairs of words that ocur in the same 
sentence, and similarly for POS pairs. To derive 
VIN features, we take each word bigram w1,w2 
and further represent it as two paterns p1,w2 and 
w1,p2 each consisting of a word and a POS tag. 
3.3 Clasifier

In al of our experiments, we train logistic re-
gresion clasifiers using the liblinear tolkit
3
. 
This choice was partly motivated by our earlier 
sumarization research, where logistic regres-
sion clasifiers were compared alongside suport 
                                                
2
 htp:/ww.cs.cmu.edu/˜enron/ 
3
 htp:/ww.csie.ntu.edu.tw/˜cjlin/liblinear/ 
18
vector machines. The two types of clasifier 
yielded very similar results, with logistic regres-
sion clasifiers being much faster to train. 
3.4 Evaluation
Metric 
Given the predicted labels on a test set and the 
existing gold-standard labels of the test set data, 
in each of our experiments we compute the area 
under the receiver operator curve as a measure of 
performance. The area under the ROC (auROC) 
is a comon sumary statistic used to measure 
the quality of binary clasification, where a per-
fect clasifier would achieve an auROC of 1.0, 
and a random clasifier, near 0.5. 
4 Experiments

4.1 Experimental
Design 
The available labeled BC3 data totals about 300 
sentences, and the available labeled AMI data 
totals over 10,00 sentences, so for both efi-
ciency and to not overwhelm the in-domain data, 
in each of our runs we subsample 10,00 sen-
tences from the AMI data to use for training. Af-
ter some initial experiments, where increasing 
the amount of target data beyond this did not im-
prove acuracy, we decided not to incur the run-
time cost of training on larger amounts of source 
data. Similarly, given that we extracted about 
20,00 lexical features from our corpora, from 
our initial experiments trading of auROC and 
runtime, we decided to select a subset of 10,00 
lexical features chosen by having the top mutual 
information with respect to the sumarization 
labels. We did 5-fold cros-validation to split the 
target set into training and testing portions, and 
ran al the domain adaptation methods using the 
same split. We report the auROC performance of 
each method averaged over thre runs of the 5-
fold cros-validation. To test for significant dif-
ferences betwen the performances of the various 
methods, we compute pairwise t-tests betwen 
the auROC values obtained on the same run. To 
acount for an increased chance of false positives 
in reporting results of several pairwise t-tests, we 
report significance for p-values < 0.05 rather 
than at the customary 0.05 level. 
4.2 Methods
Implemented 
We compare supervised domain adaptation me-
thods to the baseline INDOMAIN, in which only 
the training folds of the target data are used for 
training. In the MERGE method, we simply 
combine the labeled source and target sets and 
train on their combination. For ENSEMBLE, we 
train a clasifier on the source training data, a 
clasifier on the target training data, run each of 
them on the target test data, and for each test in-
stance compute the average of the two probabili-
ties predicted by the clasifiers and use it to 
make a label prediction. We could vary the trade-
of betwen the contribution of the source and 
target clasifier in ENSEMBLE and determine 
the optimal parameter by cros-validation, 
though for simplicity we used 0.5 which pro-
duced satisfying results. For the PRED aproach, 
we use the source data to train a clasifier, use it 
to make a prediction for the label of each point in 
the target data, and ad the predicted probability 
as an aditional feature to an in-domain trained 
clasifier. The final supervised method FEAT-
COPY (Daume, 207) takes the existing features 
and extends the feature space by making a gen-
eral, a source-specific, and a target-specific ver-
sion of each feature. Hence, a sentence with fea-
tures (x) gets represented as (x, x, 0) if it comes 
from the source domain, and as (x, 0, x) if it 
comes from the target domain. 
For semi-supervised domain adaptation meth-
ods, our baseline does not exploit any unlabeled 
target data. We train a clasifier on the source 
data only, and cal this TRANSFER. In contrast 
our two semi-supervised methods try to leverage 
unlabeled target data to help a clasifier trained 
with labeled source data be more suited to the 
target domain. 
For the SCL aproach, we implemented Blit-
zer’s structural corespondence learning (SCL) 
algorithm. An important part of the algorithm is 
training a clasifier for each of a set of m selected 
pivot features to determine the corelations of the 
other features with respect to the pivot. The m 
models’ weights are combined in a matrix, and 
its SVD with truncation factor of k is then ap-
plied to the data to yield k new features for the 
data, that are aded to the existing features. For 
the larger set of lexical features, we ran SCL 
with Blitzer’s original choice of m=100 and 
k=50, but since the computation was extremely 
time consuming we scale down m to 10. For the 
tests with conversational features, since the 
number of features is 24, we picked m=24 and 
k=24. We also test SCLSMAL, which uses the 
same algorithm as SCL to find augmented fea-
tures, except it then uses only these k features to 
train, not ading them to the original features. 
This posibility was sugested in (Blitzer 208). 
As a second semi-supervised method, we im-
plemented SELFTRAIN. The standard self-
training algorithm we implemented, inspired by 
19
Blum and Mitchel (198), is to start with a la-
beled training set T, create a subset of a fixed 
size of the unlabeled data U, and then iterate 
training a clasifier on T, making a prediction on 
the data in U, and take the highest-confidence 
positive p predictions and highest-confidence 
negative n predictions from U with their pre-
dicted labels to ad to T before replenishing U 
from the rest of the unlabeled data. We picked 
the size of the subset U as 20, and to select the 
top p=3 and botom n=17 predictions at each step 
in order to achieve a ratio of sumary to total 
sentences of 15%, which is near to the known 
ratio of the labels for AMI. 
 
method indomain merge 
ensem-
ble 
featcopy pred transfer selftrain scl sclsmal 
using conversational features 
auROC 0.838 0.747 0.751 0.839 0.838 
0.677 0.678 0.663 0.646 
time(s) 0.79 2.42 2.64 8.4 5.38 
2.08 100.2 52.85 66.74 
using lexical features 
auROC 0.623 0.638 0.67 0.615 0.625 
0.636 0.636 0.651 0.742 
time(s) 4.87 13.64 13.7 78.63 30.9 
9.73 448.8 813.7 828.3 
Table 1. Performance and time of domain adaptation methods with the two feature sets 
5 Results

In our first experiment, we ran al the domain 
adaptation methods on the data with conversa-
tional features; in our second experiment, we did 
the same on the data with lexical features. We 
computed the average of the auROCs and run-
ning times obtained for each method in each ex-
periment. Table 1 lists the results of the super-
vised methods MERGE, ENSEMBLE, and 
FEATCOPY with baseline INDOAIN, and the 
semi-supervised methods SELFTRAIN, SCL, 
and SCLSMAL with baseline TRANSFER. 
The best results for supervised methods (and 
overal) are achieved by FEATCOPY, PRED, 
and INDOMAIN with the conversational fea-
tures, with a similar performance that is signifi-
cantly beter than for MERGE and ENSEMBLE. 
However, for lexical features MERGE and EN-
SEMBLE beat their performance, with the sig-
nificant diferences from the baseline INDO-
MAIN being those of ENSEMBLE and FEAT-
COPY, the later now being the worst performer. 
For the set of lexical features, al semi-
supervised methods improve on TRANSFER. In 
this seting, al of the diferences are significant, 
with SCLSMAL generating a considerable gain 
of 10%. For the set of conversational features, 
SELFTRAIN yields an auROC similar to 
TRANSFER, and the smal diference betwen 
the two is not significant. Unlike when using 
lexical features, SCL and SCLSMAL perform 
significantly worse than TRANSFER, though 
this is not unexpected. Because it relies on de-
termining corelation betwen features, we be-
lieve that structural corespondence learning is 
more apropriate in a high rather than low-
dimensional feature space. 
Figure 1 shows, for each of the methods, a dark 
grey bar representing the auROC obtained with 
the set of conversational features next to a lighter 
grey one for the lexical features. For the super-
vised methods on the left (INDOMAIN to 
PRED), the conversational features yield beter 
performance, and this by an absolute ROC dif-
ference of more than 5%. However, notice that 
no method outperform the baseline INDOMAIN. 
For the semi-supervised methods on the right, the 
diference in performance betwen the two fea-
ture sets is les marked, although the auROC of 
SCLSMAL with lexical features is exception-
aly larger. 
As shown in Table 1, every one of the domain 
adaptation methods has a higher average time 
with lexical features than with conversational 
features. The semi-supervised methods take 
longer than the fuly supervised methods, and 
this is due to their algorithms involving more 
steps. Both SCL and SELFTRAIN take minutes 
instead of seconds to make a prediction, though 
their runing times are more reasonable than 
with the initial parameter setings we used in pre-
liminary experiments. 
 
20
 
Figure 1. Comparison of auROCs of al domain 
adaptation methods and baselines 
6 Conclusions
and Future Work 
This paper is a comparative study of the per-
formance of several domain adaptation methods 
on the task of sumarizing conversational data 
when a large amount of anotated data is avail-
able in the domain of metings and a smaler (or 
no) amount of anotation exists in the target do-
main of email threads. 
One surprising finding of our experiments is 
that of the methods we implemented, the best 
performance is achieved by training on in-
domain data using conversational features. 
Hence, it sems that when suficient labeled in-
domain data is available, supervised domain ad-
aptation is not useful for sumarization of 
emails with the features and amounts of labeled 
data we used. 
However, semi-supervised methods using unla-
beled data and labeled out-of-domain data are 
useful in the absence of these labels, with the 
SCLSMAL method greatly outperforming the 
baseline. This is a promising result for using an-
notated corpora in wel-studied domains or con-
versational modalities to sumarize data in new 
domains. 
In our experiments, we have explored the efec-
tivenes of conversational and lexical features 
separately. The two sets of features difer in their 
impact on domain adaptation: with conversa-
tional features, no method improves significantly 
over the baseline, whereas with lexical features, 
the semi-supervised methods given no labeled 
target data perform beter than the supervised 
baseline of training in-domain. One hypothesis to 
explain this is that lexical features behave simi-
larly in the two domains, so training on the larger 
amount of labeled target data is beneficial, while 
conversational features are more domain spe-
cific, likely because emails and metings are 
structured diferently. As the next step in our 
work, we intend to combine the two sets of fea-
tures. In doing this, we wil have to ensure that 
the conversational features are not washed out by 
a very large number of lexical features.  
A scenario of practical interest in domain adap-
tation for new domains is when the target domain 
has a considerable amount of unlabeled data and 
a subset of this data can easily be anotated by 
hand, for example five threads in the email do-
main. We are curently exploring injecting a 
smal amount of labeled target data into the semi-
supervised methods we have implemented to ac-
count for diferences that canot be observed in 
the unlabeled data. Blitzer (208) did such an 
adjustment to SCL using a smal amount of la-
beled target data to corect misaligned features 
and thus improve acuracy. 
Finaly, it may be worth investigating how to 
combine several of the methods, for example by 
ading the feature of PRED based on training a 
clasifier on the source, alongside augmented 
features using more unlabeled data through SCL, 
and ading the highest-confidence labels from 
SELFTRAIN to the training set. 
References  
Blitzer, J. (208). Domain Adaptation of Natural 
Language Procesing Systems. PhD Thesis. 
Blitzer, J., Dredze, M., & Pereira, F. (207). Biogra-
phies, Bolywod, Bom-boxes and Blenders: 
Domain Adaptation for Sentiment Clasification. 
In Proc. of ACL 207. 
Blitzer, J., McDonald, R., & Pereira, F. (206). Do-
main adaptation with structural corespondence 
learning. In Proc. of EMNLP 206. 
Blum, A., & Mitchel, T. (198). Combining labeled 
and unlabeled data with co-training. In Proc. CLT. 
Carleta, J., Ashby, S., Bourban, S., Flyn, M., 
Guilemot, M. et al. (205). The AMI meting cor-
pus: A pre-anouncement. In Proc. of MLMI 205. 
Chelba, C., & Acero, A. (206). Adaptation of maxi-
mum entropy capitalizer: Litle data can help a lot. 
Computer Spech & Language, 20(4), 382-399. 
Daume II, H. (207). Frustratingly easy domain ad-
aptation. In Proc. of ACL 207. 
Daume II, H., & Marcu, D. (206). Domain Adapta-
tion for Statistical Clasifiers. Journal of Artificial 
Inteligence Research, 26, 101–126. 
Florian, R., Hasan, H., Itycheriah, A., Jing, H., 
Kambhatla, N., Luo, X., et al. (204). A statistical 
21
model for multilingual entity detection and track-
ing. In Proc. HLT-NACL 204. 
Galey, M. (206). A skip-chain conditional random 
field for ranking meting uterances by importance. 
In Proc. of EMNLP 206. 
McClosky, D., Charniak, E., & Johnson, M. (206). 
Efective self-training for parsing. In Proc. of HLT-
NACL 206 . 
Muray, G., & Carenini, G. (208). Sumarizing 
spoken and writen conversations. In Proc. of 
EMNLP 208. 
Muray, G., Carenini, G., & Ng, R. (2010). Interpreta-
tion and transformation for abstracting conversa-
tions. In Proc. of HLT-NACL 2010. 
Muray, G., Renals, S., More, J., & Carleta, J. 
(206). Incorporating speaker and discourse fea-
tures into spech sumarization. In Proc. of HLT-
NACL 206. 
Plank, B. (209). Structural corespondence learning 
for parse disambiguation. In Proc. of EACL 209: 
Student Research Workshop. 
Rambow, O., Shrestha, L., & Chen, J. (204). Sum-
marizing email threads. In Proc. of HLT-NACL 
2004. 
22

