<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Douglas Biber</author>
</authors>
<title>Dimensions of Register Variation : A Cross-linguistic Comparison</title>
<date>1988</date>
<publisher>Cambridge University Press</publisher>
<location>Cambridge</location>
<contexts>
<context> is largely ineffective to manually check these dictionaries. Moreover, it is well known that no resource can ever be complete, since lexical information depends on genre, domain and discourse types (Biber, 1988). Automatic acquisition paired with lexical tuning thus remains the most promising approach to overcome these shortcomings (Wilks et al., 1996). It is then relevant to add a more practical approach t</context>
</contexts>
<marker>Biber, 1988</marker>
<rawString>Douglas Biber. 1988. Dimensions of Register Variation : A Cross-linguistic Comparison. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Is the End of Supervised Parsing in Sight</title>
<date>2007</date>
<booktitle>In Association for Computational Linguistics</booktitle>
<pages>400--408</pages>
<location>Prague</location>
<contexts>
<context> the complement. Most of the time, parsed trees obtained by syntactic parsersareplausibleandcanbeperfectlyfineformostNLP tasks, even if not always completely comparable to manually annotated corpora (Bod, 2007). Hand-crafted data used as a gold standard (e.g. the TLFI) do not contain any information about productivity of the different SCFs. Since this element is a key point for stochasticparsers,theyobtain</context>
<context>aims that“itiswellknownthatanyevaluationonhand-annotated corpora unreasonably favours supervised parsers. There is thus a quest for designing an evaluation scheme that is independent of annotations” (Bod, 2007). He proposes to evaluate against a practical task (machine translation in this case). Analternativewayofevaluatingalexicalresourceisthusto integrate it in a practical application. For example, a set</context>
<context>rs have shown that extrinsic evaluation yields interesting results for a large number of tasks, either data-oriented (e.g. lexical acquisition (Poibeau et al., 2002)), module-oriented (e.g. parsing, (Bod, 2007)) or user-oriented (e.g. automatic summarization, information extraction, machine translation, (Dorr et al., 2005)). 7. Acknowledgement This research is part of the ANR MDCO project CroTal. Cédric Me</context>
</contexts>
<marker>Bod, 2007</marker>
<rawString>Rens Bod. 2007. Is the End of Supervised Parsing in Sight? In Association for Computational Linguistics, pages 400–408, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
<author>John Carroll</author>
</authors>
<title>Automatic Extraction of Subcategorization from Corpora</title>
<date>1997</date>
<booktitle>In 5th ACL Conference on Applied Natural Language Processing</booktitle>
<pages>356--363</pages>
<location>Washington, DC</location>
<contexts>
<context>ngalexicalresourceisthusto integrate it in a practical application. For example, a set of verbs with SCFs acquired from a representative corpus has been integrated in a parser by Carroll and Briscoe (Briscoe and Carroll, 1997). Then, they evaluate the contribution of SCFs for parsing. They obtain better results when the SCFs areintegratedintotheirparser,comparedtowhentheparser is purely non-lexicalized. Practical tasks su</context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>Ted Briscoe and John Carroll. 1997. Automatic Extraction of Subcategorization from Corpora. In 5th ACL Conference on Applied Natural Language Processing, pages 356–363, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
</authors>
<title>From Dictionary to Corpus to SelfOrganizing Dictionary: Learning Valency Associations in the Face of Variation and Change</title>
<date>2001</date>
<booktitle>In Corpus Linguistics Conference</booktitle>
<location>Lancaster University</location>
<contexts>
<context>unds. A few research groups have elaborated a multi-year, multi-institutions project to achieve this goal, but the result will not be ready before several years (http://lexsynt.inria.fr/). Moreover, (Briscoe, 2001) notes that (semi-)manually developed lexicons tend to show high precision but disappointing recall (even when merging several large, already existing dictionaries to get a reference ). It is often d</context>
</contexts>
<marker>Briscoe, 2001</marker>
<rawString>Ted Briscoe. 2001. From Dictionary to Corpus to SelfOrganizing Dictionary: Learning Valency Associations in the Face of Variation and Change. In Corpus Linguistics Conference, Lancaster University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paula Chesley</author>
<author>Susanne Salmon-Alt</author>
</authors>
<title>Automatic Extraction of Subcategorization Frames for French</title>
<date>2006</date>
<booktitle>In Language Resources and Evaluation Conference (LREC</booktitle>
<location>Genoa (Italy</location>
<marker>Chesley, Salmon-Alt, 2006</marker>
<rawString>Paula Chesley and Susanne Salmon-Alt. 2006. Automatic Extraction of Subcategorization Frames for French. In Language Resources and Evaluation Conference (LREC), Genoa (Italy).</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Croft</author>
<author>Alan Cruse</author>
</authors>
<title>Cognitive Linguistics</title>
<date>2004</date>
<publisher>Cambridge University Press</publisher>
<location>Cambridge</location>
<contexts>
<context> for the distinction between arguments and adjuncts: it shows that probability distribution is a relevant factor for the issue. It also reflects the relations between words, idioms and constructions (Croft and Cruse, 2004) and, therefore, the fact that it is hard to evaluate them separately. Practical tasks require to deal with all these levels at the same time, whereas they are artificially split up when performing a</context>
</contexts>
<marker>Croft, Cruse, 2004</marker>
<rawString>William Croft and Alan Cruse. 2004. Cognitive Linguistics. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
<author>Christof Monz</author>
<author>Stacy President</author>
<author>Richard Schwartz</author>
<author>David Zajic</author>
</authors>
<title>A Methodology for Extrinsic Evaluation of Text Summarization: Does ROUGE Correlate</title>
<date>2005</date>
<booktitle>In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/orSummarization,pages1–8,AnnArbor,Michigan. Association for Computational Linguistics</booktitle>
<contexts>
<context> an information extraction task. 5.2. Extrinsic Evaluation: Does it Correlate with Intrinsic Evaluation? The usefulness of extrinsic evaluation has been demonstrated by several authors (among others (Dorr et al., 2005), from which this title is inspired). The question is then: does this other kind of evaluation correlates with intrinsic evaluation? We have shown that the comparison with a gold standard is not alwa</context>
</contexts>
<marker>Dorr, Monz, President, Schwartz, Zajic, 2005</marker>
<rawString>Bonnie Dorr, Christof Monz, Stacy President, Richard Schwartz, and David Zajic. 2005. A Methodology for Extrinsic Evaluation of Text Summarization: Does ROUGE Correlate? In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/orSummarization,pages1–8,AnnArbor,Michigan. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Gardent</author>
<author>Bruno Guillaume</author>
<author>Guy Perrier</author>
<author>Ingrid Falk</author>
</authors>
<date>2005</date>
<booktitle>Maurice Gross’ Grammar Lexicon and Natural Language Processing. In 2nd Language and Technology Conference</booktitle>
<location>Poznan</location>
<contexts>
<context>number of verb compounds) encoded through binaries features; Information in LG is dispatched through these features and must be translated into a format which is more amenable for use by NLP systems (Gardent et al., 2005). Only a part of the resource is publicly available. • Lefff (http://alpage.inria.fr/catalo gue.fr.html#Lefff) (Sagot et al., 2006) is a syntactic lexicon that distinguishes two levels of lexical des</context>
<context>needs to be translated in orderto beusable. Even aresource likeLG, which is an electronic resource intended to be used in computational systems, has to be translated in order to obtain explicit SCFs (Gardent et al., 2005). A resource like Dicovalence is encoded using the Pronominal Approach (Van Den Eynde and Blanche-Benveniste, 1978), which makes it not so easy to use: sets of pronouns have to be translated into pos</context>
</contexts>
<marker>Gardent, Guillaume, Perrier, Falk, 2005</marker>
<rawString>Claire Gardent, Bruno Guillaume, Guy Perrier, and Ingrid Falk. 2005. Maurice Gross’ Grammar Lexicon and Natural Language Processing. In 2nd Language and Technology Conference, Poznan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maurice Gross</author>
</authors>
<title>Constructing Lexicon-Grammars. InComputationalApproachestotheLexicon,pages213– 263</title>
<date>1994</date>
<publisher>University Press</publisher>
<location>Oxford. Oxford</location>
<contexts>
<context>uns, which describes intentionally all possible lexicalizations. • Lexicon-Grammar (http://infolingu.univmlv.fr/) is a hand-crafted dictionary developed by a team of researchers led by Maurice Gross (Gross, 1994). The Lexicon-Grammar (LG) for French includes syntactic information for a large number of French words (including verbs, nouns and adjectives – the resource includes 5,000 entries for simple verbs a</context>
</contexts>
<marker>Gross, 1994</marker>
<rawString>Maurice Gross. 1994. Constructing Lexicon-Grammars. InComputationalApproachestotheLexicon,pages213– 263, Oxford. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
<author>G Gorrell</author>
<author>D McCarthy</author>
</authors>
<title>Statistical filtering and subcategorization frame acquisition</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora</booktitle>
<location>Hong Kong</location>
<contexts>
<context>rrences the system analyses correctly) and F-measure (harmonic mean of precision and recall). It yields 0.79 precision, 0.55 recall and 0.65 F-measure. These results are similar to those obtained by (Korhonen et al., 2000) despite the apparent differences between French and English and the absence of a predefined list of frames for French. The only comparable previous experiment for French, (Chesley and SalmonAlt, 200</context>
</contexts>
<marker>Korhonen, Gorrell, McCarthy, 2000</marker>
<rawString>Anna Korhonen, G. Gorrell, and D. McCarthy. 2000. Statistical filtering and subcategorization frame acquisition. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Korhonen</author>
</authors>
<title>Subcategorization Acquisition</title>
<date>2002</date>
<tech>Ph.D. thesis</tech>
<institution>University of Cambridge</institution>
<contexts>
<context>think that this example is relevant when discussing problems related to the gold standard approach for evaluation (see (Messiant et al., 2008) and (Messiant, 2008) for the description of our system; (Korhonen, 2002) or (Schulte im Walde, 2006) for other systems concerning different languages). We will first describe the task (SCF acquisition) and show how it is a typical NLP task (section 2). We will then very </context>
<context>old standard. Of course, dictionaries are not the only possible gold standards for the evaluation of SCF acquisition: for example, large annotated corpora have also been used, especially for English (Korhonen, 2002). It is self-evident that a proper evaluation should take into account these various sources of information (dictionaries and annotated data). However, the comparison with a dictionary, considered as</context>
<context>so far for lexical acquisition are based on a comparison against a hand-crafted gold standard. The first experiments have been done on languages for which such a gold standard was available (English (Korhonen, 2002), German(Schulte im Walde, 2006)). Insuch a case, it is possible to check if a given verb has received a list of “correct” SCFs (i.e. the acquired SCF is also registered in the gold standard), if som</context>
<context>should be built by comparing the differentresourceexistingforFrench,mergingtheirrespective SCF and cross-validating the results against a representative corpus. This approach is the one described in (Korhonen, 2002). In this experiment, two large dictionaries for English (ANLT and COMLEX) are merged and a large corpus is annotated. The evaluation is made against this set of crossvalidated resources, thus offeri</context>
</contexts>
<marker>Korhonen, 2002</marker>
<rawString>Anna Korhonen. 2002. Subcategorization Acquisition. Ph.D. thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Kup´s´c</author>
</authors>
<title>Extraction automatique de cadres de sous-catégorisation verbale pour le franais à partir d’un corpus arboré</title>
<date>2007</date>
<booktitle>In Conférence Traitement Automatique du Langage Naturel</booktitle>
<location>Toulouse</location>
<marker>Kup´s´c, 2007</marker>
<rawString>Anna Kup´s´c. 2007. Extraction automatique de cadres de sous-catégorisation verbale pour le franais à partir d’un corpus arboré. In Conférence Traitement Automatique du Langage Naturel, Toulouse.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
</authors>
<title>Probabilistic syntax. In</title>
<date>2003</date>
<booktitle>Probabilistic Linguistics</booktitle>
<pages>289--341</pages>
<editor>R. Bod, J. Hay, and S. Jannedy, editors</editor>
<publisher>MIT Press</publisher>
<contexts>
<context>e deleted without changing the meaning of the sentence? Can it be moved easily? Can it be pronominalized? etc.) but none of these tests is sufficient or discriminatory enough. As outlined by Manning (Manning, 2003) “rather than maintainingacategoricalargument/adjunctdistinctionand having to make in/out decisions about such cases, we might instead try to represent SCF information as a probability distribution o</context>
</contexts>
<marker>Manning, 2003</marker>
<rawString>Christopher D. Manning. 2003. Probabilistic syntax. In R. Bod, J. Hay, and S. Jannedy, editors, Probabilistic Linguistics, pages 289–341. MIT Press. Cédric Messiant, Anna Korhonen, and Thierry Poibeau.</rawString>
</citation>
<citation valid="true">
<title>LexSchem: A Large Subcategorization Lexicon for French Verbs</title>
<date>2008</date>
<booktitle>In Language Resource and Evaluation Conference (LREC</booktitle>
<location>Marrakech</location>
<marker>2008</marker>
<rawString>2008. LexSchem: A Large Subcategorization Lexicon for French Verbs. In Language Resource and Evaluation Conference (LREC), Marrakech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cédric Messiant</author>
</authors>
<title>A Subcategorization Frames Acquisition System for French Verbs</title>
<date>2008</date>
<booktitle>In Association for Computational Linguistics (ACL, Student Research Workshop</booktitle>
<location>Columbus, Ohio. Thierry</location>
<contexts>
<context>d for the automatic acquisition of SCF here, but we think that this example is relevant when discussing problems related to the gold standard approach for evaluation (see (Messiant et al., 2008) and (Messiant, 2008) for the description of our system; (Korhonen, 2002) or (Schulte im Walde, 2006) for other systems concerning different languages). We will first describe the task (SCF acquisition) and show how it i</context>
<context> it only stands as an example to discuss the evaluation framework and the use of gold standards in evaluation. More detailed explanations can be found other publications (see (Messiant et al., 2008) (Messiant, 2008)). The SCF acquisition system takes as input a large corpus and produces a list of frames for each verb that occurred enough in the corpus. Partial lists of SCF associated with verbsalreadyexistforFr</context>
</contexts>
<marker>Messiant, 2008</marker>
<rawString>Cédric Messiant. 2008. A Subcategorization Frames Acquisition System for French Verbs. In Association for Computational Linguistics (ACL, Student Research Workshop), Columbus, Ohio. Thierry Poibeau, Dominique Dutoit, and Sophie Bizouard.</rawString>
</citation>
<citation valid="true">
<title>Evaluating Resource Acquisition Tools for Information Extraction</title>
<date>2002</date>
<booktitle>In Language Resources and Evaluation Conference (LREC), Las</booktitle>
<location>Palmas</location>
<marker>2002</marker>
<rawString>2002. Evaluating Resource Acquisition Tools for Information Extraction. In Language Resources and Evaluation Conference (LREC), Las Palmas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thierry Poibeau</author>
</authors>
<title>Semantic annotation: Mapping Text to Ontologies</title>
<date>2007</date>
<journal>International Journal of Metadata, Semantics and Ontologies</journal>
<volume>2</volume>
<contexts>
<context>cessary to know what elements are dependent from a given predicate and what is their role in the action expressed by the predicate. We have shown in several experiments (e.g. (Poibeau et al., 2002), (Poibeau, 2007)) that automatic acquisition from corpora allows one to find specialized items that are not mentioned in a general domain resource. These elements make up from 30 to 45% of the useful information. Th</context>
</contexts>
<marker>Poibeau, 2007</marker>
<rawString>Thierry Poibeau. 2007. Semantic annotation: Mapping Text to Ontologies. International Journal of Metadata, Semantics and Ontologies, 2(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benot Sagot</author>
</authors>
<title>Lionel Clment, Eric de La Clergerie, and</title>
<date>2006</date>
<booktitle>In Language Resource and Evaluation Conference (LREC</booktitle>
<location>Genoa</location>
<marker>Sagot, 2006</marker>
<rawString>Benot Sagot, Lionel Clment, Eric de La Clergerie, and Pierre Boullier. 2006. The Lefff 2 Syntactic Lexicon for French: Architecture, Acquisition, Use. In Language Resource and Evaluation Conference (LREC), Genoa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Experiments on the Automatic Induction of German Semantic Verb Classes</title>
<date>2006</date>
<journal>Computational Linguistics</journal>
<volume>32</volume>
<contexts>
<context>vant when discussing problems related to the gold standard approach for evaluation (see (Messiant et al., 2008) and (Messiant, 2008) for the description of our system; (Korhonen, 2002) or (Schulte im Walde, 2006) for other systems concerning different languages). We will first describe the task (SCF acquisition) and show how it is a typical NLP task (section 2). We will then very briefly describe our SCF acq</context>
<context>based on a comparison against a hand-crafted gold standard. The first experiments have been done on languages for which such a gold standard was available (English (Korhonen, 2002), German(Schulte im Walde, 2006)). Insuch a case, it is possible to check if a given verb has received a list of “correct” SCFs (i.e. the acquired SCF is also registered in the gold standard), if some are missing (i.e. a SCF is pre</context>
</contexts>
<marker>Walde, 2006</marker>
<rawString>Sabine Schulte im Walde. 2006. Experiments on the Automatic Induction of German Semantic Verb Classes. Computational Linguistics, 32(2):159–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sparck Jones</author>
<author>Jean Gallier</author>
</authors>
<title>Evaluating Natural Language Processing Systems: An Analysis and Review</title>
<date>1996</date>
<publisher>Oxford University Press</publisher>
<institution>Karel Van Den Eynde and Claire Blanche-Benveniste</institution>
<location>Oxford</location>
<contexts>
<context>sic vs Extrinsic Evaluation In this section, we describe two different ways of evaluating practical results. Our proposal is not new, since it corresponds to the classical distinction made by (Sparck Jones and Gallier, 1996) between intrinsic evaluation (evaluation of the resource – or of the task – for itself) and extrinsic evaluation (evaluating the resource by integrating it in a practical application). 5.1. Intrinsi</context>
</contexts>
<marker>Jones, Gallier, 1996</marker>
<rawString>Karen Sparck Jones and Jean Gallier. 1996. Evaluating Natural Language Processing Systems: An Analysis and Review. Oxford University Press, Oxford. Karel Van Den Eynde and Claire Blanche-Benveniste.</rawString>
</citation>
<citation valid="true">
<title>Syntaxe et Mécanismes Descriptifs : Présentation de l’approche pronominale. Cahiers de Lexicologie</title>
<date>1978</date>
<pages>32--3</pages>
<marker>1978</marker>
<rawString>1978. Syntaxe et Mécanismes Descriptifs : Présentation de l’approche pronominale. Cahiers de Lexicologie, 32:3–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karel Van Den Eynde</author>
<author>Piet Mertens</author>
</authors>
<title>Le dictionnaire de valence Dicovalence : manuel d’utilisation</title>
<date>2006</date>
<location>Manuscript, Leuven</location>
<marker>Van Den Eynde, Mertens, 2006</marker>
<rawString>Karel Van Den Eynde and Piet Mertens. 2006. Le dictionnaire de valence Dicovalence : manuel d’utilisation. Manuscript, Leuven.</rawString>
</citation>
</citationList>
</algorithm>

