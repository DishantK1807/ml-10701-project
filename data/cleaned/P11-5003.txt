1
1
Automatic Summarization
Ani Nenkova University of Pennsylvania
Sameer Maskey IBM Research
Yang Liu University of Texas at Dallas
2
Why summarize?
2
3
Text summarization
News articles
Scientific Articles
Emails
Books
Websites
Social Media 
Streams
4
Speech summarization
MeetingPhone Conversation
Classroom
Radio NewsBroadcast News
Talk Shows
Lecture
Chat
3
5
How to 
summarize
Text & Speech?
-Algorithms
-Issues
-Challenges
-Systems
Tutorial
6
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation Manual (Pyramid), Automatic (Rouge, F-Measure)Fully Automatic
Frequency, Lexical chains, TF*IDF,
Topic Words, Topic Models [LSA, EM, Bayesian]
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
4
7
Motivation: where does summarization 
help?
square6 Single document summarization 
boxshadowdwn Simulate the work of intelligence analyst
boxshadowdwn Judge if a document is relevant to a topic of interest
“Summaries as short as 17% of the full text length speed up 
decision making twice, with no significant degradation in 
accuracy.”
“Query-focused summaries enable users to find more relevant 
documents more accurately, with less need to consult the full text 
of the document.”
[Mani et al., 2002]
8
Motivation: multi-document summarization 
helps in compiling and presenting
square6 Reduce search time, especially when the goal of the 
user is to find as much information as possible about a 
given topic
boxshadowdwn Writing better reports, finding more relevant information, 
quicker
square6 Cluster similar articles and provide a multi-document 
summary of the similarities
square6 Single document summary of the information unique to 
an article
[Roussinov and Chen, 2001; Mana-Lopez et al., 2004; McKeown et al., 2005 ]
5
9
Benefits from speech summarization
square6 Voicemail
boxshadowdwn Shorter time spent on listening (call centers)
square6 Meetings
boxshadowdwn Easier to find main points
square6 Broadcast News
boxshadowdwn Summary of story from mulitiple channels
square6 Lectures
boxshadowdwn Useful for reviewing of course materials
[He et al., 2000; Tucker and Whittaker, 2008; Murray et al., 2009]
10
Assessing summary quality: overview
square6 Responsiveness
boxshadowdwn Assessor directly rate each summary on a scale
boxshadowdwn In official evaluations but rarely reported in papers
square6 Pyramid
boxshadowdwn Assessors create model summaries
boxshadowdwn Assessors identifies semantic overlap between summary 
and models
square6 ROUGE
boxshadowdwn Assessors create model summaries
boxshadowdwn ROUGE automatically computes word overlap
6
11
Tasks in summarization
Content (sentence) selection
boxshadowdwn Extractive summarization
Information ordering
boxshadowdwn In what order to present the selected sentences, especially 
in multi-document summarization
Automatic editing, information fusion and compression
boxshadowdwn Abstractive summaries
12
Extractive (multi-document) summarization
Input text2Input text1 Input text3
Summary
1. Selection
2. Ordering
3. Fusion
Compute Informativeness
7
13
Computing informativeness
rhombus6 Topic models (unsupervised)
boxshadowdwn Figure out what the topic of the input
square6 Frequency, Lexical chains, TF*IDF
square6 LSA, content models (EM, Bayesian) 
boxshadowdwn Select informative sentences based on the topic
square6 Graph models (unsupervised)
boxshadowdwn Sentence centrality
square6 Supervised approaches
boxshadowdwn Ask people which sentences should be in a summary
boxshadowdwn Use any imaginable feature to learn to predict human 
choices
14
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, Lexical chains, TF*IDF, 
Topic Words,Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
8
15
Frequency as document topic proxy
10 incarnations of an intuition
square6 Simple intuition, look only at the document(s)
boxshadowdwn Words that repeatedly appear in the document are likely to 
be related to the topic of the document
boxshadowdwn Sentences that repeatedly appear in different input 
documents represent themes in the input
square6 But what appears in other documents is also helpful 
in determining the topic
boxshadowdwn Background corpus probabilities/weights for word 
16
What is an article about?
square6 Word probability/frequency
boxshadowdwn Proposed by Luhn in 1958 [Luhn 1958]
boxshadowdwn Frequent content words would be indicative of the 
topic of the article
square6 In multi-document summarization, words or 
facts repeated in the input are more likely to 
appear in human summaries [Nenkova et al., 2006]
9
17
Word probability/weights 
Libya
bombing
trail
Gadafhi
suspects
Libya refuses 
to surrender 
two Pan Am 
bombing 
suspects 
Pan Am
INPUT
SUMMARY
WORD PROBABILITY TABLE
Word Probability
pan 0.0798
am 0.0825
libya 0.0096
suspects 0.0341
gadafhi 0.0911
trail 0.0002
….
usa 0.0007
HOW?
UK and 
USA
18
HOW: Main steps in sentence selection 
according to word probabilities
Step 1 Estimate word weights (probabilities)
Step 2 Estimate sentence weights
Step 3 Choose best sentence
Step 4 Update word weights
Step 5 Go to 2 if desired length not reached
)()( SentwCFSentWeight i ∈=
10
19
More specific choices [Vanderwende et al., 2007; Yih et al., 
2007; Haghighi and Vanderwende, 2009]
square6 Select highest scoring sentence
square6 Update word probabilities for the selected sentence 
to reduce redundancy
square6 Repeat until desired summary length
∑
∈
=
Sw
wpSSScore )(|| 1)(
pnew(w) = pold (w).pold (w)
20
Is this a reasonable approach: yes, people 
seem to be doing something similar
square6 Simple test
boxshadowdwn Compute word probability table from the input
boxshadowdwn Get a batch of summaries written by H(umans) and S(ystems)
boxshadowdwn Compute the likelihood of the summaries given the word 
probability table 
square6 Results
boxshadowdwn Human summaries have higher likelihood
HSSSSSSSSSSHSSSHSSHHSHHHHH
HIGH LIKELIHOODLOW
11
21
Obvious shortcomings of the pure 
frequency approaches
square6 Does not take account of related words
boxshadowdwn suspects -trail
boxshadowdwn Gadhafi – Libya
square6 Does not take into account evidence from 
other documents
boxshadowdwn Function words: prepositions, articles, etc.
boxshadowdwn Domain words: “cell” in cell biology articles
square6 Does not take into account many other 
aspects
22
Two easy fixes
square6 Lexical chains [Barzilay and Elhadad, 1999, Silber and McCoy, 
2002, Gurevych and Nahnsen, 2005]
boxshadowdwn Exploits existing lexical resources (WordNet)
square6 TF*IDF weights [most summarizers]
boxshadowdwn Incorporates evidence from a background corpus
12
23
Lexical chains and WordNet relations
square6 Lexical chains
boxshadowdwn Word sense disambiguation is performed 
boxshadowdwn Then topically related words represent a topic
square6 Synonyms, hyponyms, hypernyms
boxshadowdwn Importance is determined by frequency of the words in a 
topic rather than a single word
boxshadowdwn One sentence per topic is selected 
square6 Concepts based on WordNet [Schiffman et al., 2002, Ye et al., 
2007]
boxshadowdwn No word sense disambiguation is performed
square6 {war, campaign, warfare, effort, cause, operation}
square6 {concern, carrier, worry, fear, scare}
24
TF*IDF weights for words
Combining evidence for document topics from the 
input and from a background corpus
square6 Term Frequency (TF)
boxshadowdwn Times a word occurs in the input 
square6 Inverse Document Frequency (IDF)
boxshadowdwn Number of documents (df) from a background 
corpus of N documents that contain the word
)/log(* dfNtfIDFTF ×=
13
25
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
26
Topic words (topic signatures)
square6 Which words in the input are most descriptive?
boxshadowdwn Instead of assigning probabilities or weights to all words, 
divide words into two classes: descriptive or not
boxshadowdwn For iterative sentence selection approach, the binary 
distinction is key to the advantage over frequency and 
TF*IDF
boxshadowdwn Systems based on topic words have proven to be the most 
successful in official summarization evaluations 
14
27
Example input and associated topic words
square6 Input for summarization: articles relevant to the 
following user need
Title: Human Toll of Tropical 
Storms Narrative: What has been the human toll in death or injury 
of tropical storms in recent years? Where and when have each of 
the storms caused human casualties? What are the approximate 
total number of casualties attributed to each of the storms?
ahmed, allison, andrew, bahamas, bangladesh, bn, caribbean, carolina, caused, cent, 
coast, coastal, croix, cyclone, damage, destroyed, devastated, disaster, dollars, drowned, 
flood, flooded, flooding, floods, florida, gulf, ham, hit, homeless, homes, hugo, hurricane, 
insurance, insurers, island, islands, lloyd, losses, louisiana, manila, miles, nicaragua, 
north, port, pounds, rain, rains, rebuild, rebuilding, relief, remnants, residents, roared, salt, 
st, storm, storms, supplies, tourists, trees, tropical, typhoon, virgin, volunteers, weather, 
west, winds, yesterday.
Topic Words
28
Formalizing the problem of identifying topic 
words 
square6 Given
boxshadowdwn t: a word that appears in the input
boxshadowdwn T: cluster of articles on a given topic (input)
boxshadowdwn NT: articles not on topic T (background corpus)
square6 Decide if t is a topic word or not
square6 Words that have (almost) the same probability in T 
and NT are not topic words
15
29
Computing probabilities
square6 View a text as a sequence of Bernoulli trails
boxshadowdwn A word is either our term of interest t or not
boxshadowdwn The likelihood of observing term t which occurs with 
probability p in a text consisting of N words is given by 
square6 Estimate the probability of t in three ways
boxshadowdwn Input + background corpus combines
boxshadowdwn Input only
boxshadowdwn Background only
t
30
Testing which hypothesis is more 
likely: log-likelihood ratio test
has a known statistical distribution: chi-square 
At a given significance level, we can decide if a word is 
descriptive of the input or not.
This feature is used in the best performing systems for 
multi-document summarization of news [Lin and Hovy, 
2000; Conroy et al., 2006]
Likelihood of the data given H1
Likelihood of the data given H2
λ =
-2 log λ
16
31
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
32
The background corpus takes more 
central stage
square6 Learn topics from the background corpus
boxshadowdwn topic ~ themes often discusses in the background
boxshadowdwn topic representation ~ word probability tables
boxshadowdwn Usually one time training step
square6 To summarize an input 
boxshadowdwn Select sentences from the input that correspond 
to the most prominent topics
17
33
Latent semantic analysis (LSA) [Gong and Liu, 
2001, Hachey et al., 2006, Steinberger et al., 2007]
square6 Discover topics from the background corpus with n unique 
words and d documents
boxshadowdwn Represent the background corpus as nxd matrix A
boxshadowdwn Rows correspond to words
boxshadowdwn Aij=number of times word I appears in document j
boxshadowdwn Use standard change of coordinate system and dimensionality 
reduction techniques
boxshadowdwn In the new space each row corresponds to the most important 
topics in the corpus
boxshadowdwn Select the best sentence to cover each topic
TUPVA=
34
Notes on LSA and other approaches
square6 The original article that introduced LSA for 
single document summarization of news did 
not find significant difference with TF*IDF
square6 For multi-document summarization of news 
LSA approaches have not outperformed topic 
words or extensions of frequency approaches
square6 Other topic/content models have been much 
more influential
18
35
Domain dependent content models
square6 Get sample documents from the domain
boxshadowdwn background corpus
square6 Cluster sentences from these documents 
boxshadowdwn Implicit topics
square6 Obtain a word probability table for each topic
boxshadowdwn Counts only from the cluster representing the 
topic
square6 Select sentences from the input with highest 
probability for main topics 
36
Text structure can be learnt
square6 Human-written examples from a domain
Location, time
relief efforts
magnitude
damage
19
37
Topic = cluster of similar sentences from 
the background corpus
square6 Sentences cluster from earthquake articles
square6 Topic “earthquake location”
square6 The Athens seismological institute said the temblor’s epicenter 
was located 380 kilometers (238 miles) south of the capital.
square6 Seismologists in Pakistan’s Northwest Frontier Province said the 
temblor’s epicenter was about 250 kilometers (155 miles) north of 
the provincial capital Peshawar.
square6 The temblor was centered 60 kilometers (35 miles) northwest of 
the provincial capital of Kunming, about 2,200 kilometers (1,300
miles) southwest of Beijing, a bureau seismologist said.
38
Content model [Barzilay and Lee, 2004, Pascale et al., 2003] 
square6 Hidden Markov Model (HMM)-based 
boxshadowdwn States clusters of related sentences “topics”
boxshadowdwn Transition prob. sentence precedence in corpus
boxshadowdwn Emission prob. bigram language model
location, 
magnitude casualties
relief efforts
)|()|(),|,( 11111 +++++ ⋅=><>< iieiitiiii hsphhphshsp
Earthquake reportsTransition from previous 
topic
Generating 
sentence in 
current topic
20
39
Learning the content model
square6 Many articles from the same domain
square6 Cluster sentences: each cluster represents a topic from 
the domain
boxshadowdwn Word probability tables for each topic
square6 Transitions between clusters can be computed from 
sentence adjacencies in the original articles  
boxshadowdwn Probabilities of going from one topic to another
square6 Iterate between clustering and transition probability 
estimation to obtain domain model
40
To select a summary
square6 Find main topics in the domain
boxshadowdwn using a small collection of summary-input pairs
square6 Find the most likely topic for each sentence in 
the input 
square6 Select the best sentence per main topic
21
41
Historical note
square6 Some early approaches to multi-document 
summarization relied on clustering the 
sentences in the input alone [McKeown et al., 1999, 
Siddharthan et al., 2004]
boxshadowdwn Clusters of similar sentences represent a theme in 
the input
boxshadowdwn Clusters with more sentences are more important
boxshadowdwn Select one sentence per important cluster
42
Example cluster
Choose one sentence to represent the cluster
1. PAL was devastated by a pilots' strike in June and by the 
region's currency crisis.
2. In June, PAL was embroiled in a crippling three-week 
pilots' strike.
3. Tan wants to retain the 200 pilots because they stood by 
him when the majority of PAL's pilots staged a 
devastating strike in June.
22
43
Bayesian content models
square6 Takes a batch of inputs for summarization
square6 Many word probability tables
boxshadowdwn One for general English
boxshadowdwn One for each of the inputs to be summarized
boxshadowdwn One for each document in any input
To select a summary S with L words from 
document collection D given as input
The goal is to select the summary, not a 
sentence. Greedy selection vs. global will 
be discussed in detail later
S* = minS:words(S)≤LKL(PD||PS)
44
KL divergence
square6 Distance between two probability distributions: P, Q
square6 P, Q: Input and summary word distributions  
KL (P ||Q) = pP (w)log2 pP (w)p
Q(w)w
∑
23
45
Intriguing side note
square6 In the full Bayesian topic models, word 
probabilities for all words is more important 
than binary distinctions of topic and non-topic 
word
square6 Haghighi and Vanderwende report that a 
system that chooses the summary with 
highest expected number of topic words 
performs as SumBasic
46
Review
square6 Frequency based informativeness has been 
used in building summarizers
square6 Topic words probably more useful
square6 Topic models
boxshadowdwn Latent Semantic Analysis
boxshadowdwn Domain dependent content model
boxshadowdwn Bayesian content model
24
47
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
48
Using graph representations [Erkan and Radev, 
2004; Mihalcea and Tarau, 2004; Leskovec et al., 2005 ]
square6 Nodes
boxshadowdwn Sentences
boxshadowdwn Discourse entities
square6 Edges
boxshadowdwn Between similar sentences
boxshadowdwn Between syntactically related entities
square6 Computing sentence similarity
boxshadowdwn Distance between their TF*IDF weighted vector 
representations
25
49
50
Sentence :
Iraqi vice president…
Sentence :
Ivanov contended…
Sim(d1s1, d3s2)
26
51
Advantages of the graph model
square6 Combines word frequency and sentence 
clustering
square6 Gives a formal model for computing 
importance: random walks
boxshadowdwn Normalize weights of edges to sum to 1
boxshadowdwn They now represent probabilities of transitioning 
from one node to another
52
Random walks for summarization
square6 Represent the input text as graph
square6 Start traversing from node to node 
boxshadowdwn following the transition probabilities 
boxshadowdwn occasionally hopping to a new node
square6 What is the probability that you are in any 
particular node after doing this process for a 
certain time? 
boxshadowdwn Standard solution (stationary distribution)
boxshadowdwn This probability is the weight of the sentence
27
53
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
54
Supervised methods 
square6 For extractive summarization, the task can be 
represented as binary classification
boxshadowdwn A sentence is in the summary or not
square6 Use statistical classifiers to determine the score of a 
sentence: how likely it’s included in the summary
boxshadowdwn Feature representation for each sentence
boxshadowdwn Classification models trained from annotated data
square6 Select the sentences with highest scores (greedy for 
now, see other selection methods later)
28
55
Features
square6 Sentence length
boxshadowdwn long sentences tend to be more important
square6 Sentence weight
boxshadowdwn cosine similarity with documents
boxshadowdwn sum of term weights for all words in a sentence
boxshadowdwn calculate term weight after applying LSA
56
Features
square6 Sentence position
boxshadowdwn beginning is often more important
boxshadowdwn some sections are more important (e.g., in 
conclusion section)
square6 Cue words/phrases 
boxshadowdwn frequent n-grams
boxshadowdwn cue phrases (e.g., in summary, as a conclusion)
boxshadowdwn named entities
29
57
Features
square6 Contextual features
boxshadowdwn features from context sentences
boxshadowdwn difference of a sentence and its neighboring ones 
square6 Speech related features (more later):
boxshadowdwn acoustic/prosodic features
boxshadowdwn speaker information (who said the sentence, is the 
speaker dominant?)
boxshadowdwn speech recognition confidence measure 
58
Classifiers
square6 Can classify each sentence individually, or 
use sequence modeling
square6 Maximum entropy [Osborne, 2002]
square6 Condition random fields (CRF) [Galley, 2006]
square6 Classic Bayesian Method [Kupiec et al., 1995]
square6 HMM [Conroy and O'Leary, 2001; Maskey, 2006 ]
square6 Bayesian networks 
square6 SVMs [Xie and Liu, 2010]
square6 Regression [Murray et al., 2005]
square6 Others
30
59
So that is it with supervised methods? 
square6 It seems it is a straightforward classification 
problem
square6 What are the issues with this method?
boxshadowdwn How to get good quality labeled training data
boxshadowdwn How to improve learning
square6 Some recent research has explored a few 
directions
boxshadowdwn Discriminative training, regression, sampling, co-
training, active learning
60
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
31
61
Improving supervised methods: different 
training approaches
square6 What are the problems with standard training 
methods?
boxshadowdwn Classifiers learn to determine a sentence’s label 
(in summary or not)   
boxshadowdwn Sentence-level accuracy is different from 
summarization evaluation criterion (e.g., 
summary-level ROUGE scores)
boxshadowdwn Training criterion is not optimal
boxshadowdwn Sentences’ labels used in training may be too 
strict (binary classes)
62
Improving supervised methods: MERT 
discriminative training
square6 Discriminative training based on MERT [Aker et 
al., 2010]
boxshadowdwn In training, generate multiple summary candidates 
(using A* search algorithm)
boxshadowdwn Adjust model parameters (feature weights) 
iteratively to optimize ROUGE scores
Note: MERT has been used for machine translation discriminative training
32
63
Improving supervised methods: ranking 
approaches 
square6 Ranking approaches [Lin et al. 2010]
boxshadowdwn Pair-wise training
square6 Not classify each sentence individually
square6 Input to learner is a pair of sentences
square6 Use Rank SVM to learn the order of two sentences
boxshadowdwn Direct optimization
square6 Learns how to correctly order/rank summary candidates 
(a set of sentences)
square6 Use AdaRank [Xu and Li 2007] to combine weak rankers
64
Improving supervised methods: regression 
model
square6 Use regression model [Xie and Liu, 2010]
boxshadowdwn In training, a sentence’s label is not +1 and -1
boxshadowdwn Each one is labeled with numerical values to 
represent their importance
square6 Keep +1 for summary sentence
square6 For non-summary sentences (-1), use their similarity to 
the summary as labels
boxshadowdwn Train a regression model to better discriminate 
sentence candidates
33
65
Improving supervised methods: sampling
square6 Problems -in binary classification setup for 
summarization, the two classes are 
imbalanced
boxshadowdwn Summary sentences are minority class. 
boxshadowdwn Imbalanced data can hurt classifier training
square6 How can we address this?
boxshadowdwn Sampling to make distribution more balanced to 
train classifiers
boxshadowdwn Has been studied a lot in machine learning
66
Improving supervised methods: sampling
square6 Upsampling: increase minority samples
boxshadowdwn Replicate existing minority samples
boxshadowdwn Generate synthetic examples (e.g., by some kind 
of interpolation)
square6 Downsampling: reduce majority samples
boxshadowdwn Often randomly select from existing majority 
samples
34
67
Improving supervised methods: sampling
square6 Sampling for summarization [Xie and Liu, 2010]
boxshadowdwn Different from traditional upsampling and downsampling
boxshadowdwn Upsampling
square6 select non-summary sentences that are like summary 
sentences based on cosine similarity or ROUGE scores
square6 change their label to positive 
boxshadowdwn Downsampling: 
square6 select those that are different from summary sentences
boxshadowdwn These also address some human annotation disagreement
square6 The instances whose labels are changed are often the ones 
that humans have problems with
68
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-raining
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
35
69
Supervised methods: data issues
square6 Need labeled data for model training
square6 How do we get good quality training data? 
boxshadowdwn Can ask human annotators to select extractive 
summary sentences
boxshadowdwn However, human agreement is generally low
square6 What if data is not labeled at all? or it only 
has abstractive summary?
70
square6 Distributions of content units and words are similar
square6 Few units are expressed by everyone; many units 
are expressed by only one person
Do humans agree on summary sentence 
selection? Human agreement on word/sentence/fact selection
36
71
Supervised methods: semi-supervised 
learning
square6 Question – can we use unlabeled data to 
help supervised methods? 
square6 A lot of research has been done on semi-
supervised learning for various tasks
square6 Co-training and active learning have been 
used in summarization
72
Co-training
square6 Use co-training to leverage unlabeled data
boxshadowdwn Feature sets represent different views
boxshadowdwn They are conditionally independent given the 
class label
boxshadowdwn Each is sufficient for learning
boxshadowdwn Select instances based on one view, to help the 
other classifier
37
73
Co-training in summarization
square6 In text summarization [Wong et al., 2008]
boxshadowdwn Two classifiers (SVM, naïve Bayes) are used on 
the same feature set
square6 In speech summarization [Xie et al., 2010]
boxshadowdwn Two different views: acoustic and lexical features
boxshadowdwn They use both sentence and document as 
selection units
74
Active learning in summarization
square6 Select samples for humans to label
boxshadowdwn Typically hard samples, machines are not 
confident, informative ones
square6 Active learning in lecture summarization [Zhang 
et al. 2009]
boxshadowdwn Criterion: similarity scores between the extracted 
summary sentences and the sentences in the 
lecture slides are high
38
75
Supervised methods: using labeled 
abstractive summaries
square6 Question -what if I only have abstractive 
summaries, but not extractive summaries? 
square6 No labeled sentences to use for classifier 
training in extractive summarization 
square6 Can use reference abstract summary to 
automatically create labels for sentences
boxshadowdwn Use similarity of a sentence to the human written 
abstract (or ROUGE scores, other metrics)
76
Comment on supervised performance
square6 Easier to incorporate more information
square6 At the cost of requiring a large set of human 
annotated training data
square6 Human agreement is low, therefore labeled 
training data is noisy
square6 Need matched training/test conditions
boxshadowdwn may not easily generalize to different domains
square6 Effective features vary for different domains
boxshadowdwn e.g., position is important for news articles
39
77
Comments on supervised performance
square6 Seems supervised methods are more 
successful in speech summarization than in 
text
boxshadowdwn Speech summarization is almost never multi-
document
boxshadowdwn There are fewer indications about the topic of the 
input in speech domains
boxshadowdwn Text analysis techniques used in speech 
summarization are relatively simpler 
78
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
40
79
Parameters to optimize
square6 In summarization methods we try to find 
1. Most significant sentences
2. Remove redundant ones
3. Keep the summary under given length
boxshadowdwn Can we combine all 3 steps in one?
boxshadowdwn Optimize all 3 parameters at once
80
Summarization as an optimization problem
square6 Knapsack Optimization Problem 
Select boxes such that amount of money is 
maximized while keeping total weight under X Kg
square6 Summarization Problem 
Select sentences such that summary relevance is 
maximized while keeping total length under X words
square6 Many other similar optimization problems  
square6 General Idea: Maximize a function given a set of 
constraints
41
81
Optimization methods for summarization
square6 Different flavors of solutions
boxshadowdwn Greedy Algorithm
square6 Choose highest valued boxes
square6 Choose the most relevant sentence 
boxshadowdwn Dynamic Programming algorithm
square6 Save intermediate computations
square6 Look at both relevance and length
boxshadowdwn Integer Linear Programming
square6 Exact Inference
square6 Scaling Issues
We will now discuss these 3 types of optimization solutions
82
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
42
83
Greedy optimization algorithms
square6 Greedy solution is an approximate algorithm which 
may not be optimal
square6 Choose the most relevant + least redundant 
sentence if the total length does not exceed the 
summary length
boxshadowdwn Maximal Marginal Relevance is one such greedy algorithm 
proposed by [Carbonell et al., 1998]
84
Maximal Marginal Relevance (MMR) 
[Carbonell et al., 1998]
square6 Summary: relevant and non-redundant information
boxshadowdwn Many summaries are built based on sentences ranked by 
relevance
boxshadowdwn E.g. Extract most relevant 30% of sentences
Relevance Redundancyvs.
square6 Summary should maximize relevant information as 
well as reduce redundancy
43
85
Marginal relevance
square6 “Marginal Relevance” or “Relevant Novelty”
boxshadowdwn Measure relevance and novelty separately
boxshadowdwn Linearly combine these two measures
square6 High Marginal relevance if
boxshadowdwn Sentence is relevant to story (significant information)
boxshadowdwn Contains minimal similarity to previously selected sentences 
(new novel information)
square6 Maximize Marginal Relevance to get summary that 
has significant non-redundant information
86
Relevance with query or centroid
square6 We can compute relevance of text snippet 
with respect to query or centroid
square6 Centroid as defined in [Radev, 2004]
boxshadowdwn based on the content words of  a document 
boxshadowdwn TF*IDF vector of all documents in corpus
boxshadowdwn Select words above a threshold : remaining vector 
is a centroid vector
44
87
Maximal Marginal Relevance (MMR) 
[Carbonell et al., 1998]
square6 Q – document centroid/user query
square6 D – document collection
square6 R – ranked listed
square6 S – subset of documents in R already selected
square6 Sim – similarity metric 
square6 Lambda =1 produces most significant ranked list
square6 Lambda = 0 produces most diverse ranked list
MMR≈ Argmax(Di∈R−S)[λ(Sim1(Di,Q))−(1−λ)max(Dj∈S)Sim2(Di,Dj)]
88
MMR based Summarization [Zechner, 2000]
Iteratively select next sentence
Next Sentence = 
Frequency Vector 
of all content words
centroid
45
89
MMR based summarization
square6 Why this iterative sentence selection process 
works?
boxshadowdwn 1st Term: Find relevant sentences similar to 
centroid of the document
boxshadowdwn 2nd Term: Find redundancy ─ sentences that are 
similar to already selected sentences are not 
selected
90
square6 MMR is an iterative sentence selection 
process
boxshadowdwn decision made for each sentence
boxshadowdwn Is this selected sentence globally optimal?
Sentence selection in MMR
Sentence with same level of relevance but shorter may not be 
selected if a longer relevant sentence is already selected
46
91
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
92
Global inference
D=t1,t2,,tn−1,tn
square6 Modify our greedy algorithm 
boxshadowdwn add constraints for sentence length as well
square6 Let us define document D with tn textual 
units
47
93
Global inference
square6 Let us define
Relevance of ti to be in the 
summary
Redundancy between ti and tj
Length of til(i)
Red(i,j)
Rel(i)
94
Inference problem [McDonald, 2007]
square6 Let us define inference problem as 
Summary Score
Pairwise RedundancyMaximum Length
48
95
Greedy solution [McDonald, 2007]
Sort by Relevance
Select Sentence
square6 Sorted list may have longer sentences at the top
square6 Solve it using dynamic programming
square6 Create table and fill it based on length and redundancy 
requirements
No consideration of
sentence length
96
Dynamic programming solution [McDonald, 2007]
High scoring summary
of length k and i-1
text unitsHigh scoring summary of
length k-l(i) +
ti
Higher ?
49
97
square6 Better than the previously shown greedy 
algorithm
square6 Maximizes the space utilization by not 
inserting longer sentences
square6 These are still approximate algorithms: 
performance loss?
Dynamic programming algorithm [McDonald, 2007]
98
Inference algorithms comparison
[McDonald, 2007]
System 50 100 200
Baseline 26.6/5.3 33.0/6.8 39.4/9.6
Greedy 26.8/5.1 33.5/6.9 40.1/9.5
Dynamic Program 27.9/5.9 34.8/7.3 41.2/10.0
Summarization results: Rouge-1/Rouge-2
Sentence Length
50
99
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
100
Integer Linear Programming (ILP) [Gillick 
and Favre, 2009; Gillick et al., 2009; McDonald, 2007]
square6 Greedy algorithm is an approximate solution
square6 Use exact solution algorithm with ILP (scaling issues 
though)
square6 ILP is constrained optimization problem
boxshadowdwn Cost and constraints are linear in a set of integer variables
square6 Many solvers on the web
square6 Define the constraints based on relevance and 
redundancy for summarization
boxshadowdwn Sentence based ILP
boxshadowdwn N-gram based ILP
51
101
Sentence-level ILP formulation [McDonald, 
2007]
1 if ti in summary
Constraints
Optimization Function
102
N-gram ILP formulation [Gillick and Favre, 2009; 
Gillick et al., 2009]
square6 Sentence-ILP constraint on redundancy is 
based on sentence pairs
square6 Improve by modeling n-gram-level 
redundancy
square6 Redundancy implicitly defined
Ci indicates presence
of n-gram i in summary 
and its weight is wi
summationtext
i wici
52
103
N-gram ILP formulation [Gillick and Favre, 2009]
Constraints
Optimization Function n-gram level ILP has different  optimization function than one shown before
104
Sentence vs. n-gram ILP
System ROUGE-2 Pyramid
Baseline 0.058 0.186
Sentence ILP
[McDonald, 2007] 0.072 0.295
N-gram ILP
[Gillick and Favre, 2009] 0.110 0.345
53
105
Other optimization based summarization 
algorithms
square6 Submodular selection [Lin et al., 2009]
boxshadowdwn Submodular set functions for optimization
square6 Modified greedy algorithm [Filatova, 2004]
boxshadowdwn Event based features
square6 Stack decoding algorithm [Yih et al., 2007]
boxshadowdwn Multiple stacks, each stack represents hypothesis of different 
length
square6 A* Search [Aker et al., 2010]
boxshadowdwn Use scoring and heuristic functions
106
Submodular selection for summarization 
[Lin et al., 2009]
square6 Summarization Setup
boxshadowdwn V – set of all sentences in document
boxshadowdwn S – set of extraction sentences
boxshadowdwn f(.) scores the quality of the summary
square6 Submodularity been used in solving many 
optimization problems in near polynomial time
square6 For summarization: 
Select subset S (sentences) representative of V 
given the constraint |S| =< K (budget)
54
107
Submodular selection [Lin et al., 2009]
square6 If V are nodes in a Graph G=(V,E) representing 
sentences
square6 And E represents edges (i,j) such that w(i,j) 
represents similarity between sentences i and j
square6 Introduce submodular set functions which measures 
“representative” S of entire set V
square6 [Lin et al., 2009] presented 4 submodular set functions
108
Submodular selection for summarization 
[Lin et al., 2009]
Comparison of results using different methods
55
109
Review: optimization methods
square6 Global optimization methods have shown to be 
superior than 2-step selection process and reduce 
redundancy
square6 3 parameters are optimized together
boxshadowdwn Relevance
boxshadowdwn Redundancy
boxshadowdwn Length
square6 Various Algorithms for Global Inference
boxshadowdwn Greedy
boxshadowdwn Dynamic Programming 
boxshadowdwn Integer Linear Programming
boxshadowdwn Submodular Selection
110
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
56
111
Speech summarization
square6 Increasing amount of data available in 
speech form
boxshadowdwn meetings, lectures, broadcast, youtube, voicemail
square6 Browsing is not as easy as for text domains
boxshadowdwn users need to listen to the entire audio
square6 Summarization can help effective information 
access
square6 Summary output can be in the format of text 
or speech
112
Domains 
square6 Broadcast news
square6 Lectures/presentations
square6 Multiparty meetings
square6 Telephone conversations
square6 Voicemails
57
113
Example
Meeting transcripts and summary sentences (in red)
so it’s possible that we could do something like a 
summary node of some sort that
me003
but there is some technology you could try to applyme010
yeahme010
now I don’t know that any of these actually apply in 
this case
me010
uh so if you coyou could imaand i-me010
mmmme003
there’re ways to uh sort of back off on the purity of 
your bayes-net-edness
me010
andme010
uh ii slipped a paper to bhaskara and about noisy-
or’s and noisy-maxes
me010
which is there are technical ways of doing itme010
uh let me just mention something that i don’t want 
to pursue today
me010
there there are a variety of ways of doing itme010
Broadcast news transcripts and summary (in red)
try to use electrical appliances before p.m. and after p.m. and 
turn off computers, copiers and lights when they're not being 
used
set your thermostat at 68 degrees when you're home, 55 
degrees when  you're away
energy officials are offering tips to conserve electricity, they say, 
to delay holiday lighting until after at night
the area shares power across many states
meanwhile, a cold snap in the pacific northwest is putting an 
added strain on power supplies
coupled with another unit, it can provide enough power for about
2 million people
it had been shut down for maintenance
a unit at diablo canyon nuclear plant is expected to resume 
production today
california's strained power grid is getting a boost today which 
might help increasingly taxed power supplies
114
Speech vs. text summarization: similarities
square6 When high quality transcripts are available
boxshadowdwn Not much different from text summarization
boxshadowdwn Many similar approaches have been used
boxshadowdwn Some also incorporate acoustic information
square6 For genres like broadcast news, style is also 
similar to text domains
58
115
Speech vs. text summarization: differences
square6 Challenges in speech summarization
boxshadowdwn Speech recognition errors can be very high
boxshadowdwn Sentences are not as well formed as in most text 
domains: disfluencies, ungrammatical
boxshadowdwn There are not clearly defined sentences
boxshadowdwn Information density is also low (off-topic 
discussions, chit chat, etc.)
boxshadowdwn Multiple participants
116
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
59
117
What should be extraction units in speech 
summarization?
square6 Text domain
boxshadowdwn Typically use sentences (based on punctuation 
marks)
square6 Speech domain
boxshadowdwn Sentence information is not available
boxshadowdwn Sentences are not as clearly defined
Utterance from previous example:
there there are a variety of ways of doing it uh let me just mention something 
that i don’t want to pursue today which is there are technical ways of doing it
118
Automatic sentence segmentation (side note) 
square6 For a word boundary, determine whether it’s a sentence 
boundary
square6 Different approaches: 
boxshadowdwn Generative: HMM
boxshadowdwn Discriminative: SVM, boosting, maxent, CRF
boxshadowdwn Information used: word n-gram, part-of-speech, parsing 
information, acoustic info (pause, pitch, energy)
60
119
What is the effect of different 
units/segmentation on summarization?
square6 Research has used different units in speech 
summarization
boxshadowdwn Human annotated sentences or dialog acts
boxshadowdwn Automatic sentence segmentation
boxshadowdwn Pause-based segments
boxshadowdwn Adjacency pairs
boxshadowdwn Intonational phrases 
boxshadowdwn Words
120
What is the effect of different 
units/segmentation on summarization?
square6 Findings from previous studies
boxshadowdwn Using intonational phrases (IP) is better than 
automatic sentence segmentation, pause-based 
segmentation [Maskey, 2008 ]
square6 IPs are generally smaller than sentences, also 
linguistically meaningful
boxshadowdwn Using sentences is better than words, between 
filler segments [Furui et al., 2004]
boxshadowdwn Using human annotated dialog acts is better than 
automatically generated ones [Liu and Xie, 2008]
61
121
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
122
Using acoustic information in 
summarization
square6 Acoustic/prosodic features: 
boxshadowdwn F0 (max, min, mean, median, range)
boxshadowdwn Energy (max, min, mean, median, range)
boxshadowdwn Sentence duration
boxshadowdwn Speaking rate (# of words or letters)
boxshadowdwn Need proper normalization
square6 Widely used in supervised methods, in 
combination with textual features
62
123
Using acoustic information in 
summarization
square6 Are acoustic features useful when combining 
it with lexical information?
square6 Results vary depending on the tasks and 
domains 
boxshadowdwn Often lexical features are ranked higher
boxshadowdwn But acoustic features also contribute to overall 
system performance
boxshadowdwn Some studies showed little impact when adding 
speech information to textual features [Penn and Zhu, 
2008]
124
Using acoustic information in 
summarization
square6 Can we use acoustic information only for speech 
summarization?
boxshadowdwn Transcripts may not be available
boxshadowdwn Another way to investigate contribution of acoustic 
information
square6 Studies showed using just acoustic information can 
achieve similar performance to using lexical 
information [Maskey and Hirschberg, 2005; Xie et al., 2009; Zhu et al., 
2009]
boxshadowdwn Caveat: in some experiments, lexical information is used 
(e.g., define the summarization units)
63
125
Speech recognition errors
square6 ASR is not perfect, often high word error rate
boxshadowdwn 10-20% for read speech
boxshadowdwn 40% or even higher for conversational speech
square6 Recognition errors generally have negative 
impact on summarization performance
boxshadowdwn Important topic indicative words are incorrectly 
recognized
boxshadowdwn Can affect term weighting and sentence scores
126
Speech recognition errors
square6 Some studies evaluated effect of recognition 
errors on summarization by varying word 
error rate [Christensen et al., 2003; Penn and Zhu, 2008; Lin et al., 
2009]
square6 Degradation is not much when word error 
rate is not too low (similar to spoken 
document retrieval)
boxshadowdwn Reason: better recognition accuracy in summary 
sentences than overall  
64
127
What can we do about ASR errors? 
square6 Deliver summary using original speech 
boxshadowdwn Can avoid showing recognition errors in the 
delivered text summary
boxshadowdwn But still need to correctly identify summary 
sentences/segments
square6 Use recognition confidence measure and 
multiple candidates to help better summarize
128
Address problems due to ASR errors
square6 Re-define summarization task: select 
sentences that are most informative, at the 
same time have high recognition accuracy
boxshadowdwn Important words tend to have high recognition 
accuracy
square6 Use ASR confidence measure or n-gram 
language model scores in summarization
boxshadowdwn Unsupervised methods [Zechner, 2002; Kikuchi et al., 2003; 
Maskey, 2008]
boxshadowdwn Use as a feature in supervised methods
65
129
Address problems due to ASR errors
square6 Use multiple recognition candidates
boxshadowdwn n-best lists [Liu et al., 2010]
boxshadowdwn Lattices [Lin et al., 2010]
boxshadowdwn Confusion network [Xie and Liu, 2010]
square6 Use in MMR framework
square6 Summarization segment/unit contains all the word 
candidates (or pruned ones based on probabilities)
square6 Term weights (TF, IDF) use candidate’s posteriors
square6 Improved performance over using 1-best recognition 
output
130
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency
66
131
Disfluencies and summarization
square6 Disfluencies (filler words, repetitions, revisions, 
restart, etc) are frequent in conversational speech
boxshadowdwn Example from meeting transcript:
so so does ijust remind me of what what you were going to do with the 
what what what what's
yyou just described what you've been doing
square6 Existence may hurt summarization systems, also 
affect human readability of the summaries
132
Disfluencies and summarization
square6 Natural thought: remove disfluenices 
square6 Word-based selection can avoid disfluent 
words 
boxshadowdwn Using n-gram scores tends to select fluent 
parts [Hori and Furui, 2001]
square6 Remove disfluencies first, then perform 
summarization 
boxshadowdwn Does it work? not consistent results 
square6 Small improvement [Maskey, 2008; Zechner, 2002]
square6 No improvement [Liu et al., 2007]
67
133
Disfluencies and summarization
square6 In supervised classification, information related to 
disfluencies can be used as features for 
summarization 
boxshadowdwn Small improvement on Switchboard data [Zhu and Penn, 2006]
square6 Going beyond disfluency removal, can perform 
sentence compression in conversational speech to 
remove un-necessary words [Liu and Liu, 2010]
boxshadowdwn Help improve sentence readability
boxshadowdwn Output is more like abstractive summaries
boxshadowdwn Compression helps summarization
134
Review on speech summarization
square6 Speech summarization has been performed 
for different domains
square6 A lot of text-based approaches have been 
adopted
square6 Some speech specific issues have been 
investigated
boxshadowdwn Segmentation 
boxshadowdwn ASR errors
boxshadowdwn Disfluencies
boxshadowdwn Use acoustic information
68
135
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
136
Manual evaluations
square6 Task-based evaluations 
boxshadowdwn too expensive
boxshadowdwn Bad decisions possible, hard to fix
square6 Assessors rate summaries on a scale
boxshadowdwn Responsiveness
square6 Assessors compare with gold-standards
boxshadowdwn Pyramid
69
137
Automatic and fully automatic 
evaluation
square6 Automatically compare with gold-standard
boxshadowdwn Precision/recall (sentence level)
boxshadowdwn ROUGE (word level)
square6 No human gold-standard is used
boxshadowdwn Automatically compare input and summary
138
Precision and recall for extractive 
summaries
square6 Ask a person to select the most important 
sentences
Recall: system-human choice 
overlap/sentences chosen by human
Precision: system-human choice 
overlap/sentences chosen by system
70
139
Problems?
square6 Different people choose different sentences
square6 The same summary can obtain a recall score 
that is between 25% and 50% different 
depending on which of two available human 
extracts is used for evaluation
square6 Recall more important/informative than 
precision?
140
More problems?
square6 Granularity
We need help. Fires have spread in the nearby 
forest and threaten several villages in this remote 
area.
square6 Semantic equivalence
boxshadowdwn Especially in multi-document summarization
boxshadowdwn Two sentences convey almost the same 
information: only one will be chosen in the human 
summary
71
141
Pyramid
Responsiveness
ROUGE
Fully automatic
Model 
summaries
Manual comparison/ 
ratings
Evaluation methods for content
142
Pyramid method [Nenkova and Passonneau, 2004; Nenkova et al., 
2007]
square6 Based on Semantic Content Units (SCU)
square6 Emerge from the analysis of several texts
square6 Link different surface realizations with the 
same meaning
72
143
SCU example
S1 Pinochet arrested in London on Oct 16 at a 
Spanish judge’s request for atrocities against 
Spaniards in Chile.
S2 Former Chilean dictator Augusto Pinochet has 
been arrested in London at the request of the 
Spanish government.
S3 Britain caused international controversy and 
Chilean turmoil by arresting former Chilean 
dictator Pinochet in London.
144
SCU: label, weight, contributors 
Label London was where Pinochet was 
arrested
Weight=3
S1 Pinochet arrested in London on Oct 16 at a Spanish 
judge’s request for atrocities against Spaniards in Chile.
S2 Former Chilean dictator Augusto Pinochet has been
arrested in London at the request of the Spanish
government.
S3 Britain caused international controversy and Chilean 
turmoil by arresting former Chilean dictator Pinochet in 
London.
73
145
Ideally informative summary
square6 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
146
Ideally informative summary
square6 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
74
147
Ideally informative summary
square6 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
148
Ideally informative summary
square6 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
75
149
Ideally informative summary
square6 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
150
Ideally informative summary
square6 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
76
151
Different equally good summaries
square6 Pinochet arrested
square6 Arrest in London
square6 Pinochet is a former 
Chilean dictator
square6 Accused of atrocities 
against Spaniards
152
Different equally good summaries
square6 Pinochet arrested
square6 Arrest in London
square6 On Spanish warrant
square6 Chile protests
77
153
Diagnostic ─ why is a summary bad?
square6 Good box4 Less relevant 
summary
154
Importance of content 
square6 Can observe distribution in human 
summaries
boxshadowdwn Assign relative importance
boxshadowdwn Empirical rather than subjective
square6 The more people agree, the more important
78
155
Pyramid score for evaluation
square6 New summary with n content units
square6 Estimates the percentage of information that is 
maximally important
IdealWight
ightObservedWe
Ideal
Weight
n
i
i
n
i
i
=
∑
∑
=
=
1
1
156
ROUGE [Lin, 2004]
square6 De facto standard for evaluation in text 
summarization
boxshadowdwn High correlation with manual evaluations in that 
domain
square6 More problematic for some other domains, 
particularly speech
boxshadowdwn Not highly correlated with manual evaluations
boxshadowdwn May fail to distinguish human and machine 
summaries
79
157
ROUGE details
square6 In fact a suite of evaluation metrics
boxshadowdwn Unigram
boxshadowdwn Bigram
boxshadowdwn Skip bigram
boxshadowdwn Longest common subsequence
square6 Many settings concerning
boxshadowdwn Stopwords
boxshadowdwn Stemming
boxshadowdwn Dealing with multiple models
158
How to evaluate without human 
involvement? [Louis and Nenkova, 2009]
square6 A good summary should be similar to the 
input
square6 Multiple ways to measure similarity
boxshadowdwn Cosine similarity
boxshadowdwn KL divergence
boxshadowdwn JS divergence
square6 Not all work!
80
159
square6 Distance between two distributions as 
average KL divergence from their mean 
distribution
JS divergence between input and 
summary
)]||()||([)||( 21 ASummKLAInpKLSummInpJS +=
SummaryandInputofondistributimeanSummInpA ,2+=
160
Summary likelihood given the input
square6 Probability that summary is generated according to 
term distribution in the input
Higher likelihood ~ better summary
square6 Unigram Model
square6 Multinomial Model
ii
n
rInp
n
Inp
n
Inp
wwordofsummaryincountn
vocabularysummaryr
wpwpwp r
=
−
)()()( 21 21 K
sizesummarynN
wpwpwp
i
i
n
rInp
n
Inp
n
Inpnn
N r
r
==∑
)()()( 21
1 21!!
! K
K
81
161
square6 Fraction of summary = input’s topic words
square6 % of input’s topic words also appearing in summary 
boxshadowdwn Capture variety
square6 Cosine similarity: input’s topic words and all summary 
words
boxshadowdwn Fewer dimensions, more specific vectors
Topic words identified by log-likelihood 
test
162
How good are these metrics? 
48 inputs, 57 systems
JSD -0.880 -0.736
0.795 0.627
-0.763 -0.694
0.712 0.647
0.712 0.602
-0.688 -0.585
-0.188 -0.101
0.222 0.235
% input’s topic in summary
KL div summ-input
Cosine similarity
% of summary = topic words
KL div input-summ
Unigram summ prob.
Multinomial summ prob.
-0.699 0.629Topic word similarity
Pyramid Responsiveness
Spearman correlation on macro level for the query focused task.
82
163
square4 JSD correlations with pyramid scores even better than 
R1-recall
square4 R2-recall is consistently bettersquare4
Can extend features using higher order n-grams 
How good are these metrics?
0.870.90R2-recall
0.800.85R1-recall
-0.73-0.88JSD
Resp.Pyramid
164
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
83
165
Current summarization research  
square6 Summarization for various new genres
boxshadowdwn Scientific articles
boxshadowdwn Biography
boxshadowdwn Social media (blog, twitter)
boxshadowdwn Other text and speech data 
square6 New task definition 
boxshadowdwn Update summarization 
boxshadowdwn Opinion summarization
square6 New summarization approaches 
boxshadowdwn Incorporate more information (deep linguistic knowledge, information 
from the web)
boxshadowdwn Adopt more complex machine learning techniques
square6 Evaluation issues
boxshadowdwn Better automatic metrics
boxshadowdwn Extrinsic evaluations And more…
166
square6 Check out summarization papers at ACL this 
year
square6 Workshop at ACL-HLT 2011:
boxshadowdwn Automatic summarization for different genres, 
media, and languages [June 23, 2011]
square6 http://www.summarization2011.org/
84
167
References
square6 Ahmet Aker, Trevor Cohn, Robert Gaizauska. 2010. Multi-document summarization using A* search and 
discriminative training. Proc. of EMNLP.
square6 R. Barzilay and M. Elhadad. 2009. Text summarizations with lexical chains. In: I. Mani and M. Maybury (eds.): 
Advances in Automatic Text Summarization.
square6 Jaime Carbonell and Jade Goldstein. 1998. The Use of MMR, Diversity-Based reranking for Reordering 
Documents and Producing Summaries. Proceedings of the 21st Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval.
square6 H. Christensen, Y. Gotoh, B. Killuru, and S. Renals. 2003. Are Extractive Text Summarization Techniques 
Portable to Broadcast News? Proc. of ASRU.
square6 John Conroy and Dianne O'Leary. 2001. Text Summarization via Hidden Markov Models. Proc. of SIGIR.
square6 J. M. Conroy, J. D. Schlesinger, and D. P. OLeary. 2006. Topic-Focused Multi-Document Summarization Using an 
Approximate Oracle Score. Proc. COLING/ACL 2006. pp. 152-159.
square6 Thomas Cormen, Charles E. Leiserson, and Ronald L. Rivest.1990. Introduction to algorithms. MIT Press.
square6 G. Erkan and D. R. Radev.2004. LexRank: Graph-based Centrality as Salience in Text Summarization. Journal of 
Artificial Intelligence Research (JAIR).
square6 Pascale Fung, Grace Ngai, and Percy Cheung. 2003. Combining optimal clustering and hidden Markov models for 
extractive summarization. Proceedings of ACL Workshop on Multilingual Summarization.Sadoki Furui, T. Kikuchi, 
Y. Shinnaka, and C. Hori. 2004. Speech-to-text and Speech-to-speech Summarization of Spontaneous Speech. 
IEEE Transactions on Audio, Speech, and Language Processing. 12(4), pages 401-408.
square6 Michel Galley. 2006. A Skip-Chain Conditional Random Field for Ranking Meeting Utterances by Importance. 
Proc. of EMNLP.
square6 Dan Gillick, Benoit Favre. 2009. A scalable global model for summarization. Proceedings of the Workshop on 
Integer Linear Programming for Natural Language Processing.
square6 Dan Gillick, Korbinian Riedhammer, Benoit Favre, Dilek Hakkani-Tur. 2009. A global optimization framework for 
meeting summarization. Proceedings of ICASSP.
square6 Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark Kantrowitz. 2000. Multi-document summarization by 
sentence extraction. Proceedings of the 2000 NAACL-ANLP Workshop on Automatic Summarization.
168
References
square6 Y. Gong and X. Liu. 2001. Generic text summarization using relevance measure and latent semantic analysis. 
Proc. ACM SIGIR.
square6 I. Gurevych and T. Nahnsen. 2005. Adapting Lexical Chaining to Summarize Conversational Dialogues. Proc. 
RANLP.
square6 B. Hachey, G. Murray, and D. Reitter.2006. Dimensionality reduction aids term co-occurrence based multi-
document summarization. In: SumQA 06: Proceedings of the Workshop on Task-Focused Summarization and 
Question Answering. 
square6 Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. Proc. of 
NAACL-HLT.
square6 L. He, E. Sanocki, A. Gupta, and J. Grudin. 2000. Comparing presentation summaries: Slides vs. reading vs.
listening. Proc. of SIGCHI on Human factors in computing systems.
square6 C. Hori and Sadaoki Furui. 2001. Advances in Automatic Speech Summarization. Proc. of Eurospeech.
square6 T. Kikuchi, S. Furui, and C. Hori. 2003. Automatic Speech Summarization based on Sentence Extractive and 
Compaction. Proc. of ICSLP.  
square6 Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A Trainable Document Summarizer. Proc. of SIGIR.
square6 J. Leskovec, N. Milic-frayling, and M. Grobelnik. 2005. Impact of Linguistic Analysis on the Semantic Graph 
Coverage and Learning of Document Extracts. Proc. AAAI.
square6 Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries, Workshop on Text 
Summarization Branches Out. 
square6 C.Y. Lin and E. Hovy. 2000. The automated acquisition of topic signatures for text summarization. Proc. COLING.
square6 Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. 
Proc. of NAACL.
square6 Hui Lin and Jeff Bilmes and Shasha Xie. 2009. Graph-based Submodular Selection for Extractive Summarization. 
Proceedings of ASRU.
square6 Shih-Hsiang Lin and Berlin Chen. 2009. Improved Speech Summarization with Multiple-hypothesis 
Representations and Kullback-Leibler Divergence Measures. Proc. of Interspeech.
square6 Shih-Hsiang Lin, Berlin Chen, and H. Min Wang. 2009. A Comparative Study of Probabilistic Ranking Models for 
Chinese Spoken Document Summarization. ACM Transactions on Asian Language Information Processing.
85
169
References
square6 Shih Hsiang Lin, Yu Mei Chang, Jia Wen Liu, Berlin Chen. 2010 Leveraging Evaluation Metric-related Training 
Criteria for Speech Summarization. Proc. of ICASSP.
square6 Fei Liu and Yang Liu. 2009. From Extractive to Abstractive Meeting Summaries: Can it be done by sentence 
compression? Proc. of ACL.
square6 Fei Liu and Yang Liu. 2010. Using Spoken Utterance Compression for Meeting Summarization: A pilot study. Proc. 
of IEEE SLT.
square6 Yang Liu and Shasha Xie. 2008. Impact of Automatic Sentence Segmentation on Meeting Summarization. Proc. of 
ICASSP.
square6 Yang Liu, Feifan Liu, Bin Li, and Shasha Xie. 2007. Do Disfluencies Affect Meeting Summarization: A pilot study 
on the impact of disfluencies. Poster at MLMI.
square6 Yang Liu, Shasha Xie, and Fei Liu. 2010. Using n-best Recognition Output for Extractive Summarization and 
Keyword Extraction in Meeting Speech. Proc. of ICASSP.
square6 Annie Louis and Ani Nenkova. 2009. Automatically evaluating content selection in summarization without 
human models. Proceedings of EMNLP
square6 H.P. Luhn. 1958. The Automatic Creation of Literature Abstracts. IBM Journal of Research and Development 2(2).
square6 Inderjeet Mani, Gary Klein, David House, Lynette Hirschman, Therese Firmin, and Beth Sundheim. 2002. 
SUMMAC: a text summarization evaluation. Natual Language Engineering. 8,1 (March 2002), 43-68.
square6 Manuel J. Mana-Lopez, Manuel De Buenaga, and Jose M. Gomez-Hidalgo. 2004. Multidocument summarization: 
An added value to clustering in interactive retrieval. ACM Trans. Inf. Systems.
square6 Sameer Maskey. 2008. Automatic Broadcast News Summarization. Ph.D thesis. Columbia University.
square6 Sameer Maskey and Julia Hirschberg. 2005. Comparing lexical, acoustic/prosodic, discourse and structural 
features for speech summarization. Proceedings of Interspeech.
square6 Sameer Maskey and Julia Hirschberg. 2006. Summarizing Speech Without Text Using Hidden Markov Models. 
Proc. of HLT-NAACL.
square6 Ryan McDonald. 2007. A Study of Global Inference Algorithms in Multi-document Summarization. Lecture Notes in 
Computer Science. Advances in Information Retrieval. 
square6 Kathleen McKeown, Rebecca J. Passonneau, David K. Elson, Ani Nenkova, and Julia Hirschberg. 2005. Do 
summaries help?. Proc. of SIGIR.
square6 K. McKeown, J. L. Klavans, V. Hatzivassiloglou, R. Barzilay, and E. Eskin.1999. Towards multidocument
summarization by reformulation: progress and prospects. Proc. AAAI 1999.
170
References
square6 R. Mihalcea and P. Tarau .2004. Textrank: Bringing order into texts. Proc. of EMNLP 2004. 
square6 G. Murray, S. Renals, J. Carletta, J. Moore. 2005. Evaluating Automatic Summaries of Meeting Recordings. Proc. 
of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation.
square6 G. Murray, T. Kleinbauer, P. Poller, T. Becker, S. Renals, and J. Kilgour. 2009. Extrinsic Summarization 
Evaluation: A Decision Audit Task. ACM Transactions on Speech and Language Processing.
square6 A. Nenkova and R. Passonneau. 2004. Evaluating Content Selection in Summarization: The Pyramid Method. 
Proc. HLT-NAACL.
square6 A. Nenkova, L. Vanderwende, and K. McKeown. 2006. A compositional context sensitive multi-document 
summarizer: exploring the factors that influence summarization. Proc. ACM SIGIR.
square6 A. Nenkova, R. Passonneau, and K. McKeown. 2007. The Pyramid Method: Incorporating human content 
selection variation in summarization evaluation. ACM Trans. Speech Lang. Processing.
square6 Miles Osborne. 2002. Using maximum entropy for sentence extraction. Proc. of ACL Workshop on Automatic 
Summarization.
square6 Gerald Penn and Xiaodan Zhu. 2008. A critical Reassessement of Evaluation Baselines for Speech 
Summarization. Proc. of ACL-HLT.
square6 Dmitri G. Roussinov and Hsinchun Chen. 2001. Information navigation on the web by clustering and summarizing 
query results. Inf. Process. Manage. 37, 6 (October 2001), 789-816. 
square6 B. Schiffman, A. Nenkova, and K. McKeown. 2002. Experiments in Multidocument Summarization. Proc. HLT.
square6 A. Siddharthan, A. Nenkova, and K. Mckeown.2004. Syntactic Simplification for Improving Content Selection in 
Multi-Document Summarization. Proc. COLING.
square6 H. Grogory Silber and Kathleen F. McCoy. 2002. Efficiently computed lexical chains as an intermediate 
representation for automatic text summarization. Computational. Linguist. 28, 4 (December 2002), 487-496.
square6 J. Steinberger, M. Poesio, M. A. Kabadjov, and K. Jeek. 2007. Two uses of anaphora resolution in summarization. 
Inf. Process. Manage. 43(6).
square6 S. Tucker and S. Whittaker. 2008. Temporal compression of speech: an evaluation. IEEE Transactions on Audio, 
Speech and Language Processing, pages 790-796.
square6 L. Vanderwende, H. Suzuki, C. Brockett, and A. Nenkova. 2007. Beyond SumBasic: Task-focused summarization 
with sentence simplification and lexical expansion. Information Processing and Management 43.
86
171
References
square6 Kam-Fai Wong, Mingli Wu, and Wenjie Li. 2008. Extractive Summarization using Supervised and Semi-supervised 
learning. Proc. of ACL.
square6 Shasha Xie and Yang Liu. 2010. Improving Supervised Learning for Meeting Summarization using Sampling and 
Regression. Computer Speech and Language. V24, pages 495-514.
square6 Shasha Xie and Yang Liu. 2010. Using Confusion Networks for Speech Summarization. Proc. of NAACL.
square6 Shasha Xie, Dilek Hakkani-Tur, Benoit Favre, and Yang Liu. 2009. Integrating Prosodic Features in Extractive 
Meeting Summarization. Proc. of ASRU.
square6 Shasha Xie, Hui Lin, and Yang Liu. 2010. Semi-supervised Extractive Speech Summarization via Co-training 
Algorithm. Proc. of Interspeech.
square6 S. Ye, T.-S. Chua, M.-Y. Kan, and L. Qiu. 2007. Document concept lattice for text understanding and 
summarization. Information Processing and Management 43(6).
square6 W. Yih, J. Goodman, L. Vanderwende, and H. Suzuki. 2007. Multi-Document Summarization by Maximizing 
Informative Content-Words. Proc. IJCAI 2007.
square6 Klaus Zechner. 2002. Automatic Summarization of Open-domain Multiparty Dialogues in Diverse Genres. 
Computational Linguistics. V28, pages 447-485.
square6 Klaus Zechner and Alex Waibel. 2000. Minimizing word error rate in textual summaries of spoken language. 
Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference.
square6 Justin Zhang and Pascale Fung. 2009. Extractive Speech Summarization by Active Learning. Proc. of ASRU.
square6 Xiaodan Zhu and Gerald Penn. 2006. Comparing the Roles of Textual, Acoustic and Spoken-language Features 
on Spontaneous Conversation Summarization. Proc. of HLT-NAACL.
square6 Xiaodan Zhu, Gerald Penn, and F. Rudzicz. 2009. Summarizing Multiple Spoken Documents: Finding Evidence 
from Untranscribed Audio. Proc. of ACL.

