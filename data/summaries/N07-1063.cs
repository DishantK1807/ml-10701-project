The alternative, simply decoding without the n-gram LM and rescoring N-best alternative translations, results in substantially more search errors, as shown in (Zollmann and Venugopal, 2006).
W06-3119
Approaches We evaluate our two pass hypergraph search “H.Search” against the strong single pass Cube Pruning (CP) baseline as mentioned in (Chiang, 2005) and detailed in (Chiang, 2007).
P05-1033
(Galley et al., 2006) use syntactic constituents for the PSCFG nonterminal set and (Zollmann and Venugopal, 2006) take advantage of CCG (Steedman, 1999) categories, while (Chiang, 2005) uses a single generic nonterminal.
P05-1033 P06-1121 P99-1039 W06-3119
Klein. 2003.
P03-1054
(Zhang et al., 2006) “binarize” grammars into CNF normal form, while (Watanabe et al., 2006) allow only Griebach-Normal form grammars.
N06-1033 P06-1098
We present results in the form of search error analysis and translation quality as measured by the BLEU score (Papineni et al., 2002) on the IWSLT 06 text translation task (Eck and Hori, 2005)1, comparing Cube Pruning with our two-pass approach.
P02-1040
Instead of generating allconsequents, andthenpruningawaythepoorperformers, CP uses the K-Best extraction approach of (Huang and Chiang, 2005) to select the best K consequents only, at the cost of potential search errors.
P05-1033 W05-1506
Grammar rules were induced with the syntaxbased SMT system “SAMT” described in (Zollmann and Venugopal, 2006), which requires initial phrase alignments that we generated with “GIZA++” (Koehn et al., 2003), and syntactic parse trees of the target training sentences, generated by the Stanford Parser (D.
N03-1017 W06-3119
We use the extraction method from (Huang and Chiang, 2005).
P05-1033 W05-1506
We present our work under the construction in (Wu, 1996), following notation from (Chiang, 2007), extending the formal description to reflect grammars with an arbitrary number of nonterminals in each rule.
P96-1021
Klein, 2003) pre-trained on the Penn Treebank.
P03-1054
In practice, this approach performs poorly (Chiang, 2007; Zollmann and Venugopal, 2006).
W06-3119
Syntax-driven (Galley et al., 2006) and hierarchical translation models (Chiang, 2005) take advantage of probabilistic synchronous context free grammars (PSCFGs) to represent structured, lexical reordering constraints during the decoding process.
P05-1033 P06-1121
Parameters λ used to calculate P(D) are trained using MER training (Och, 2003) on development data.
P03-1021
(Wellington et al., 2006) argue that these restrictions reduce our ability to model translation equivalence effectively.
P06-1123
We experiment with 2 grammars, one syntaxbased (3688 nonterminals, 0.3m rules), and one purely hierarchical (1 generic nonterminal, 0.05m rules) as in (Chiang, 2005).
P05-1033
Backpointers to antecedent cells are typically retained to allow N-Best extraction using an algorithm such as (Huang and Chiang, 2005).
P05-1033 W05-1506
