<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<title>Evaluating cross-language annotation transfer in the multisemcor corpus</title>
<date>2004</date>
<booktitle>In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics</booktitle>
<pages>364</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics</institution>
<location>Morristown, NJ, USA</location>
<marker>2004</marker>
<rawString>2004. Evaluating cross-language annotation transfer in the multisemcor corpus. In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics, page 364, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>Y Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP-06</booktitle>
<pages>594--602</pages>
<location>Sydney, Australia</location>
<contexts>
<context>fine latent semantic features (Picca et al., May 2007) (Koo and Collins, 2005). Using the Semcor corpus, a fraction of the Brown corpus annotated with WordNet word senses, a SST has been implemented (Ciaramita and Altun, 2006) which can be used for annotating large collections of English text 2. The SST implements a Hidden Markov Model, trained with the perceptron algorithm introduced in (Collins, 2002). Perceptron sequen</context>
<context>SuperSense. Even if the micro figure obtained is sensibly lower than the corresponding value for English (which is around 0.77, evaluated with the same procedure on the English SemCor as reported in (Ciaramita and Altun, 2006)), the results are really encouraging, achieving a micro F1 of 0.63%. If we cast a deeper glance at the Table 1, we can clearly notice that for some category the F1 is exceptionally high. Some of tho</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>M. Ciaramita and Y. Altun. 2006. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In Proceedings of EMNLP-06, pages 594–602, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>J Atserias</author>
</authors>
<title>Pos tagging with a named entity tagger. Intelligenza Artificiale</title>
<date>2007</date>
<pages>4--28</pages>
<contexts>
<context>ces for the Italian language: 1. An Italian POS tagger 2. An Italian Sense Tagged corpus for training, where words are tagged with WordNet super-senses As a PoS tagger, we adopted the Evalita Tagset (Ciaramita and Atserias, 2007), a tool for annotating text with part-ofspeech and lemma information. As a source of sense tagged data, we adopted MultiSemCor (Bentivogli et al., 2004), an Italian corpus composed of 116 texts whic</context>
</contexts>
<marker>Ciaramita, Atserias, 2007</marker>
<rawString>M. Ciaramita and J. Atserias. 2007. Pos tagging with a named entity tagger. Intelligenza Artificiale, 4:28–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>M Johnson</author>
</authors>
<title>Supersense tagging of unknown nouns in wordnet</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP-03</booktitle>
<pages>168--175</pages>
<location>Sapporo, Japan</location>
<contexts>
<context>ross different languages. Thus, semantic annotations of this kind could be used for multilingual inference in several language tasks; e.g., information retrieval or machine translation. To this aim, (Ciaramita and Johnson, 2003) developed a SuperSense Tagging (SST) technology for English, demonstrating that reasonably high accuracy in tagging can be obtained even in open domain contexts. This technology has been also adopte</context>
<context>the English and Italian SSTs for multilingual knowledge acquisition problems. 2. The English SuperSense Tagger WordNet (Fellbaum, 1998) defines 45 lexicographer’s categories, also called supersenses (Ciaramita and Johnson, 2003), used by lexicographers to provide an initial broad classification for the lexicon entries1. Although simplistic in many ways, the supersense ontology has several attractive features for NLP purpose</context>
<context>ession of the life of people in a community (noun.communication) Previous work has showed that supersenses can be useful in lexical acquisition to provide a first guess at the meaning of novel words (Ciaramita and Johnson, 2003), and in syntactic parse re-ranking, to define latent semantic features (Picca et al., May 2007) (Koo and Collins, 2005). Using the Semcor corpus, a fraction of the Brown corpus annotated with WordNe</context>
<context>in Section 2. (obtained from the original distribution) in the corpus generated so far, and we optimized the required parameters by adopting a cross validation technique. As for the English settings (Ciaramita and Johnson, 2003), the best results have been obtained by setting 50 trials and 10 epochs to train the perceptron algorithm. Results and error analysis are presented in the following section. 4. Evaluation We evaluat</context>
</contexts>
<marker>Ciaramita, Johnson, 2003</marker>
<rawString>M. Ciaramita and M. Johnson. 2003. Supersense tagging of unknown nouns in wordnet. In Proceedings of EMNLP-03, pages 168–175, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP-02</booktitle>
<contexts>
<context>mented (Ciaramita and Altun, 2006) which can be used for annotating large collections of English text 2. The SST implements a Hidden Markov Model, trained with the perceptron algorithm introduced in (Collins, 2002). Perceptron sequence learning provides an excellent trade-off accuracy/performance, sometimes outperforming more complex models such as CFR (Nguyen and Guo, 2007). The tagset used by the tagger defi</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet. An Electronic Lexical Database</title>
<date>1998</date>
<publisher>MIT Press</publisher>
<contexts>
<context>y, Section 5. summarizes the achieved results proposing some idea to apply both the English and Italian SSTs for multilingual knowledge acquisition problems. 2. The English SuperSense Tagger WordNet (Fellbaum, 1998) defines 45 lexicographer’s categories, also called supersenses (Ciaramita and Johnson, 2003), used by lexicographers to provide an initial broad classification for the lexicon entries1. Although sim</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet. An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>Beth Sundheim</author>
</authors>
<title>Message understanding conference-6: a brief history</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics</booktitle>
<pages>466--471</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics</institution>
<location>Morristown, NJ, USA</location>
<contexts>
<context> tasks 1. Introduction Developed in the Information Extraction field, Named Entity Recognition (NER) is a basic task in Natural Language Processing. NER, originally exploited for business activities (Grishman and Sundheim, 1996), has been extended beyond this field. In particular, NER can be a useful step for broad-coverage ontology engineering. For example, named entity categories could be used for ontology population and </context>
</contexts>
<marker>Grishman, Sundheim, 1996</marker>
<rawString>Ralph Grishman and Beth Sundheim. 1996. Message understanding conference-6: a brief history. In Proceedings of the 16th conference on Computational linguistics, pages 466–471, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>M Collins</author>
</authors>
<title>Hidden-variable models for discriminative reranking</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP-05</booktitle>
<location>Vancouver, Canada</location>
<contexts>
<context>xical acquisition to provide a first guess at the meaning of novel words (Ciaramita and Johnson, 2003), and in syntactic parse re-ranking, to define latent semantic features (Picca et al., May 2007) (Koo and Collins, 2005). Using the Semcor corpus, a fraction of the Brown corpus annotated with WordNet word senses, a SST has been implemented (Ciaramita and Altun, 2006) which can be used for annotating large collections</context>
</contexts>
<marker>Koo, Collins, 2005</marker>
<rawString>T. Koo and M. Collins. 2005. Hidden-variable models for discriminative reranking. In Proceedings of EMNLP-05, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>Nouns in wordnet: a lexical inheritance system</title>
<date>1990</date>
<journal>International Journal of Lexicography</journal>
<volume>3</volume>
<contexts>
<context>ies defined by WordNet. WordNet as been organized according to psycholinguistic theories on the principles governing lexical memory. As an example, several psycho-linguistic experiments discussed in (Miller, 1990) suggest correlations between reaction times and the hierarchical structural of the lexicon. Thus the broadest WordNet’s categories can serve as a principled basis for a set of categories which exhau</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>G. A. Miller. 1990. Nouns in wordnet: a lexical inheritance system,. International Journal of Lexicography, 3(4):245–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nam Nguyen</author>
<author>Yunsong Guo</author>
</authors>
<title>Comparison of sequence labeling algorithms and extensions</title>
<date>2007</date>
<booktitle>In Proceedings of ICML</booktitle>
<pages>681--688</pages>
<contexts>
<context>ith the perceptron algorithm introduced in (Collins, 2002). Perceptron sequence learning provides an excellent trade-off accuracy/performance, sometimes outperforming more complex models such as CFR (Nguyen and Guo, 2007). The tagset used by the tagger defines 26 supersense labels for nouns and 15 supersense labels for verbs. The basic feature set includes: word = lower-cased form of each token for the current positi</context>
</contexts>
<marker>Nguyen, Guo, 2007</marker>
<rawString>Nam Nguyen and Yunsong Guo. 2007. Comparison of sequence labeling algorithms and extensions. In Proceedings of ICML 2007, pages 681–688. Davide Picca, Alfio Gliozzo, and Massimiliano Ciaramita.</rawString>
</citation>
<citation valid="true">
<title>Semantic domains and supersens tagging for domain-specific ontology learning</title>
<date>2007</date>
<booktitle>In proceedings RIAO</booktitle>
<marker>2007</marker>
<rawString>May 2007. Semantic domains and supersens tagging for domain-specific ontology learning. In proceedings RIAO 2007.</rawString>
</citation>
</citationList>
</algorithm>

