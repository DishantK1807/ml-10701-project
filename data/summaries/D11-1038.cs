2007), information extraction (Wu and Weld, 2010),
P10-1013
dataset used in Zhu et al. (2010), an aligned cor-
C10-1152
tions like Zhu et al. (2010) obtains superior results
C10-1152
Nelken and Schieber, 2006).
E06-1021
Cohn, Trevor and Mirella Lapata. 2008. Sentence
C08-1018
Nelken, Rani and Stuart Schieber. 2006. Towards
E06-1021
neering. In contrast to Yatskar et al. (2010) and Zhu
N10-1056
(Das and Smith, 2009), question answering (Wang
D09-1086 P09-1053
Das, Dipanjan and Noah A. Smith. 2009. Paraphrase
D09-1086 P09-1053
Yamangil, Elif and Rani Nelken. 2008. Mining
P08-2035
Kaji et al., 2002).
P02-1028
Vickrey, David and Daphne Koller. 2008. Sentence
P08-1040
Yamada, Kenji and Kevin Knight. 2001. A syntax-
P01-1067
Table 4: Example simplifications produced by the systems in this paper (RevILP, AlignILP) and Zhu et al.’s (2010)
C10-1152
simplified 64 sentences from Zhu et al.’s (2010) test
C10-1152
defined dictionary (Devlin, 1999; Inui et al., 2003;
W03-1602
However, we refrained from doing so as Zhu et al.(2010) show that Moses performs poorly, it cannot
C10-1152
to measure similarity (Barzilay and Elhadad, 2003;
W03-1004
Smith, David A. and Jason Eisner. 2009. Parser
D09-1086
Barzilay, Regina and Noemie Elhadad. 2003. Sen-
W03-1004
lexical substitution, Zhu et al.’s 2010 model, and two ver-
C10-1152
of-the-art methods (Zhu et al., 2010) our model
C10-1152
by syntax-based SMT (Yamada and Knight, 2001),
P01-1067
Klein, Dan and Christopher D. Manning. 2003. Ac-
P03-1054
2010; Zhu et al., 2010). Advantageously, the Sim-
C10-1152
of parsers (Chandrasekar et al., 1996), summarizers
C96-2183
labelers (Vickrey and Koller, 2008).
P08-1040
(QG, Smith and Eisner 2006). QG allows to describe
W06-3104
Sauper, Christina and Regina Barzilay. 2009. Au-
P09-1024
Yatskar et al. (2010) learn lexical simplifications
N10-1056
More recently, Yatskar et al. (2010) explore
N10-1056
parser (Klein and Manning, 2003) in order to la-
P03-1054
2008 and Zhao et al. 2009 for notable exceptions).
P09-1094
ent from the reference, followed by Zhu et al. (2010)
C10-1152
2002) and TERp (Snover et al., 2009). Both mea-
W09-0441
Zhu et al. (2010) is the least grammatical model.
C10-1152
(Klein and Manning, 2003) but any other broadly
P03-1054
Wu, Fei and Daniel S. Weld. 2010. Open infor-
P10-1013
paraphrase extraction (Barzilay, 2003). We must not
W03-1004
Barzilay, Regina. 2003. Information Fusion for
W03-1004
(Smith and Eisner, 2009), paraphrase identification
D09-1086
tent. Zhu et al. (2010) also use Wikipedia to learn
C10-1152
training (Yamangil and Nelken, 2008; Yatskar et al.,
P08-2035
Smith, David and Jason Eisner. 2006. Quasi-
W06-3104
