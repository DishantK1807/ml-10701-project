in Bod (2003) and Goodman (2003: 136), we additionally use a correction factor to redress DOP's bias discussed in Johnson (2002).
E03-1005 J02-1005
We next tested UDOP on two additional domains from Chinese and German which were also used in Klein and Manning (2002, 2004): the Chinese treebank (Xue et al.2002) and the NEGRA corpus (Skut et al.1997). The CTB10 is the subset of p-o-s strings from the Penn Chinese treebank containing 10 words or less after removal of punctuation (2437 strings).
A97-1014 C02-1145 P02-1017 P04-1061
As shown by Klein and Manning (2002, 2004), the extension to inducing trees for words instead of p-o-s tags is rather straightforward since there exist several unsupervised part-of-speech taggers with high accuracy, which can be combined with unsupervised parsing (see e.g.
P02-1017 P04-1061
In fact, in Bod et al.(2003) we showed that the most probable parse tree as defined above has a tendency to be constructed by the shortest derivation (consisting of the fewest and thus largest subtrees).
E03-1005
(Collins and Duffy 2002 show how a tree kernel can be used for an all-subtrees representation, which we will not discuss here.) Goodman's reduction method first assigns every node in every tree a unique number which is called its address.
P02-1034
Schütze 1996; Clark 2000).
W00-0717
Table 4 indicates that U-DOP's performance remains still far behind that of S-DOP (and indeed of other state-of-the-art supervised parsers such as Bod 2003 or Charniak and Johnson 2005).
E03-1005 P05-1022
In fact, the problem of computing the most probable tree in DOP is known to be NP hard (Sima'an 1996).
C96-2215
Although such a heuristic does not guarantee that the most probable parse is actually found, it is shown in Bod (2000) to perform at least as well as the estimation of the most probable parse with Monte Carlo techniques.
P00-1009
And the hybrid approach of Klein and Manning (2004), which combines a constituency and a dependency model, leads to a further increase of 77.6% f-score.
P04-1061
To make our parse results comparable to those of Klein and Manning (2002, 2004, 2005), we will use exactly the same evaluation metrics for unlabeled precision (UP) and unlabeled recall (UR), defined in Klein (2005: 21-22).
P02-1017 P04-1061
To give a brief overview: van Zaanen (2000) achieved 39.2% unlabeled f-score on ATIS word strings by a sentence-aligning technique called ABL.
C00-2139
We refer to Zollmann and Sima'an (2005) for a recently proposed estimator that is statistically consistent (though it is not yet known how this estimator performs on the WSJ) and to Zuidema (2006) for a theoretical comparison of existing estimators for DOP.
E06-2025
Table 1 shows the results of U-DOP in terms of UP, UR and F1 compared to the results of the CCM model by Klein and Manning (2002), the DMV dependency learning model by Klein and Manning (2004) together with their combined model DMV+CCM.
P02-1017 P04-1061
Clark (2001) reports 42.0% unlabeled f-score on the same data using distributional clustering, and Klein and Manning (2002) obtain 51.2% unlabeled f-score on ATIS part-of-speech strings using a constituent-context model called CCM.
P02-1017 W01-0713
Moreover, if S-DOP is not post-binarized, its average f-score on the WSJ40 is 90.1% -and there are some hybrid DOP models that obtain even higher scores (see Bod 2003).
E03-1005
Moreover, on Penn Wall Street Journal p-os-strings ≤ 10 (WSJ10), Klein and Manning (2002) report 71.1% unlabeled f-score.
P02-1017
It should be kept in mind that while the probabilities of all parse trees generated by DOP sum up to 1, these probabilities do not converge to the "true" probabilities if the corpus grows to infinity (Johnson 2002).
J02-1005
(The number 15 is admittedly ad hoc, and was inspired by the performance of the socalled SL-DOP model in Bod 2002, 2003).
E03-1005
(We can considerably improve efficiency by using k-best hypergraph parsing as recently proposed by Huang and Chiang 2005, but this will be left to future research).
W05-1506
For a subtle discussion on this issue, see Clark (2001) or Klein (2005).
W01-0713
An approach that may seem apt in this respect is an allsubtrees approach (e.g Bod 2003; Goodman 2003; Collins and Duffy 2002).
E03-1005 P02-1034
