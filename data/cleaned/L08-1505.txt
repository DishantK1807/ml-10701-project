<algorithm name="ParsCit" version="080917">
<citationList>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Robert MacIntyre</author>
</authors>
<title>Bracketing guidelines for Treebank II style, Penn Treebank project</title>
<date>1995</date>
<tech>Technical Report Tech Report MS-CIS-95-06</tech>
<institution>University of Pennsylvania</institution>
<location>Philadelphia, PA</location>
<contexts>
<context>to produce the gold standard parse trees, the test sentences were manually parsed by one annotator, using as references the Penn Treebank trees themselves and the Penn Treebank bracketing guidelines (Bies et al., 1995). Text Type # Example Highlighted 34 Podvig also prominent in the Crime and Punishment notebooks, gets relegated in the final text to the Epilogue where it is seen at its simplest in the mitigating c</context>
</contexts>
<marker>Bies, Ferguson, Katz, MacIntyre, 1995</marker>
<rawString>Ann Bies, Mark Ferguson, Karen Katz, and Robert MacIntyre. 1995. Bracketing guidelines for Treebank II style, Penn Treebank project. Technical Report Tech Report MS-CIS-95-06, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SteveAbney EzraBlack</author>
<author>ClaudiaGdaniec DanFlickinger</author>
<author>Robert Grishman</author>
<author>Philip Harrison</author>
<author>Donald Hindle</author>
<author>Robert Ingria</author>
<author>Fred Jelinek</author>
<author>Judith Klavans</author>
<author>Mark Liberman</author>
<author>Mitchell Marcus</author>
<author>Salim Roukos</author>
<author>Beatrice Santorini</author>
<author>Tomek Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of english grammars</title>
<date>1991</date>
<booktitle>In Proceedings of the 1991 DARPA Speech and Natural Language Workshop</booktitle>
<pages>306--311</pages>
<marker>EzraBlack, DanFlickinger, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>EzraBlack, SteveAbney,DanFlickinger,ClaudiaGdaniec, Robert Grishman, Philip Harrison, Donald Hindle, Robert Ingria, Fred Jelinek, Judith Klavans, Mark Liberman, Mitchell Marcus, Salim Roukos, Beatrice Santorini, and Tomek Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of english grammars. In Proceedings of the 1991 DARPA Speech and Natural Language Workshop, pages 306– 311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>User reference guide for the British National Corpus</title>
<date>2000</date>
<tech>Technical report</tech>
<institution>Oxford University Computing Services</institution>
<contexts>
<context>rser2 parser3 parser4 F-Score 89.1 91.3 92.1 91.7 Table 1: Parseval Results on WSJ23 3. BNC Test Set The new English test set consists of 1,000 sentences taken from the British National Corpus (BNC) (Burnard, 2000). TheBNC is a onehundredmillion word balancedcorpusof British English from the late twentieth century. Ninety per cent of it is written text, and the remaining 10% consists of transcribed spontaneous</context>
</contexts>
<marker>Burnard, 2000</marker>
<rawString>Lou Burnard. 2000. User reference guide for the British National Corpus. Technical report, Oxford University Computing Services.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
</authors>
<title>Anette Frank, Dekang Lin, Detlef Prescher, and Hans Uszkoreit</title>
<date>2002</date>
<booktitle>Proceedings of the “Beyond Parseval Towards Improved Evaluation Measures for Parsing Systems” Workshop at the 3rd International Conference on Linguistic Resources and Evaluation (LREC-02), Las Palmas, Gran Canaria</booktitle>
<editor>editors</editor>
<marker>Carroll, 2002</marker>
<rawString>John Carroll, Anette Frank, Dekang Lin, Detlef Prescher, and Hans Uszkoreit, editors. 2002. Proceedings of the “Beyond Parseval Towards Improved Evaluation Measures for Parsing Systems” Workshop at the 3rd International Conference on Linguistic Resources and Evaluation (LREC-02), Las Palmas, Gran Canaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Course-tofine n-best-parsing and maxent discriminative reranking</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the ACL (ACL-05</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, Michigan</location>
<contexts>
<context>he second parser (parser2) extends the first parser by incorporating a discriminative reranker which uses features ranging over the entire parse tree to re-order the nbest parses returned by parser1 (Charniak and Johnson, 2005). The reranking parser achieves an f-score of 91.3% on WSJ23, a significant improvement over the first-stage parser. Thethird parser(parser3) is theself-trainedparserreported in McClosky et al. (2006</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Course-tofine n-best-parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting of the ACL (ACL-05), pages 173–180, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser</title>
<date>2000</date>
<booktitle>In Proceedings of the Annual Meeting of the North American Association for Computational Linguistics (NAACL-00</booktitle>
<pages>132--139</pages>
<location>Seattle, Washington</location>
<contexts>
<context> the Penn Treebank (WSJ23) (Marcus et al., 1994). The first parser (parser1) is Charniak’s lexicalized history-based generative statistical parser which achieves a Parseval f-score of 89.1% on WSJ23 (Charniak, 2000). The second parser (parser2) extends the first parser by incorporating a discriminative reranker which uses features ranging over the entire parse tree to re-order the nbest parses returned by parse</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the Annual Meeting of the North American Association for Computational Linguistics (NAACL-00), pages 132–139, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Chrupała</author>
<author>Nicolas Stroppa</author>
<author>Josef van Genabith</author>
<author>Georgiana Dinu</author>
</authors>
<title>Better training for function labeling</title>
<date>2007</date>
<booktitle>In Proceedings of the Recent Advances in Natural Language Processing Conference (RANLP-07</booktitle>
<pages>133--138</pages>
<location>Borovets, Bulgaria</location>
<marker>Chrupała, Stroppa, van Genabith, Dinu, 2007</marker>
<rawString>Grzegorz Chrupała, Nicolas Stroppa, Josef van Genabith, and Georgiana Dinu. 2007. Better training for function labeling. In Proceedings of the Recent Advances in Natural Language Processing Conference (RANLP-07), pages 133–138, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Joachim Wagner</author>
<author>Djam´e Seddah</author>
<author>Josef van Genabith</author>
</authors>
<title>Adapting WSJ-trained parsers to the British National Corpus using in-domain selftraining</title>
<date>2007</date>
<booktitle>InProceedingsoftheTenthInternationalWorkshop on Parsing Technologies (IWPT-07</booktitle>
<pages>33--35</pages>
<location>Prague, Czech Republic</location>
<marker>Foster, Wagner, Seddah, van Genabith, 2007</marker>
<rawString>Jennifer Foster, Joachim Wagner, Djam´e Seddah, and Josef van Genabith. 2007. Adapting WSJ-trained parsers to the British National Corpus using in-domain selftraining. InProceedingsoftheTenthInternationalWorkshop on Parsing Technologies (IWPT-07), pages 33–35, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English</title>
<date>2007</date>
<booktitle>In Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek, and Mare Koit, editors, Proceedings of NODALIDA 2007</booktitle>
<pages>105--112</pages>
<location>Tartu, Estonia</location>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek, and Mare Koit, editors, Proceedings of NODALIDA 2007, pages 105–112, Tartu, Estonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Heike Telljohann</author>
</authors>
<title>Towards a dependency-oriented evaluation for partial parsing</title>
<date>2002</date>
<booktitle>In Carroll et</booktitle>
<pages>9--16</pages>
<marker>K¨ubler, Telljohann, 2002</marker>
<rawString>Sandra K¨ubler and Heike Telljohann. 2002. Towards a dependency-oriented evaluation for partial parsing. In Carroll et al. (Carroll et al., 2002), pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based evaluation of MINIPAR</title>
<date>1998</date>
<booktitle>Proceedings of The EvaluationofParsingSystems Workshopatthe3rdInternational Conference on Linguistic Resources and Evaluation (LREC), Cognitive Science Research Papers 489</booktitle>
<pages>48--56</pages>
<editor>In John Carroll, editor</editor>
<institution>University of Sussex</institution>
<location>Brighton, England</location>
<contexts>
<context>ser2 83.5 83.3 83.4 parser3 84.0 83.9 83.9 parser4 85.6 85.2 85.4 Table 3: Parseval Results on BNC Test Set 4.2. Leaf-Ancestor Evaluation The drawbacks of the Parseval metric have been noted by many (Lin, 1998; Carroll et al., 2002). Some of these criticisms relate to phrase-structure-based evaluation in general, i.e. evaluation based on phrase-structure constituents abstracts away from basic predicate-arg</context>
<context>of dependencygrammar arguethat dependency relations between words are a more useful source of informationthanconstituentstructure. Forparserevaluation,the use of dependencies has also been advocated (Lin, 1998; K¨ubler and Telljohann, 2002). We can evaluate constituent parsers using a dependency-based evaluation by automatically extracting dependency relationships from constituent structure. The quality of</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based evaluation of MINIPAR. In John Carroll, editor, Proceedings of The EvaluationofParsingSystems Workshopatthe3rdInternational Conference on Linguistic Resources and Evaluation (LREC), Cognitive Science Research Papers 489, pages 48–56. University of Sussex, Brighton, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>ThePenn Treebank: Annotating Predicate Argument Structure</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology</booktitle>
<pages>110--115</pages>
<location>Princeton, NJ</location>
<contexts>
<context> versions of the Charniak parser, a constituency parser with state-of-the-art performance on the standard English test set, Section 23 of the Wall Street Journal section of the Penn Treebank (WSJ23) (Marcus et al., 1994). The first parser (parser1) is Charniak’s lexicalized history-based generative statistical parser which achieves a Parseval f-score of 89.1% on WSJ23 (Charniak, 2000). The second parser (parser2) ex</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. ThePenn Treebank: Annotating Predicate Argument Structure. In Proceedings of the ARPA Workshop on Human Language Technology, pages 110–115, Princeton, NJ. David McClosky, Eugene Charniak, and Mark Johnson.</rawString>
</citation>
<citation valid="true">
<authors>
<author>2006a</author>
</authors>
<title>Effective self-training for parsing</title>
<booktitle>In Proceedings of the Human Language Technology Conference and North American chapter of the ACL annual meeting (HLT-NAACL-06</booktitle>
<pages>152--159</pages>
<location>New York, June. David</location>
<marker>2006a, </marker>
<rawString>2006a. Effective self-training for parsing. In Proceedings of the Human Language Technology Conference and North American chapter of the ACL annual meeting (HLT-NAACL-06), pages 152–159, New York, June. David McClosky, Eugene Charniak, and Mark Johnson.</rawString>
</citation>
<citation valid="true">
<authors>
<author>2006b Rerankingandself-trainingforparseradaptation</author>
</authors>
<date></date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL (COLING-ACL-06</booktitle>
<pages>337--344</pages>
<location>Sydney, Australia</location>
<marker>Rerankingandself-trainingforparseradaptation, </marker>
<rawString>2006b. Rerankingandself-trainingforparseradaptation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL (COLING-ACL-06), pages 337–344, Sydney, Australia, July.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan Mac Donald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<marker>Nivre, Hall, K¨ubler, Donald, Nilsson, Riedel, Yuret, </marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mac Donald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.</rawString>
</citation>
<citation valid="true">
<title>The CoNLL 2007 shared task on dependency parsing</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007</booktitle>
<pages>915--932</pages>
<location>Prague, Czech Republic</location>
<contexts>
<context>dard”dependencygraphsproducedby applying the conversion procedure to the gold standard phrase structure trees. To extract dependencies, we use the conversion procedure providedby Johansson and Nugues (2007). This is the procedure used in the CONLL 2007 Shared Task on dependency parsing (Nivre et al., 2007), and it improves upon the constituent-to-dependency conversion procedure provided by Yamada and Ma</context>
<context>-distance dependencies. Because the BNC gold standard trees have not yet been annotated with functional tags and traces, we apply the machine-learning based functional tag labeller of Chrupala et al. (2007) to both the gold standard trees and the parser output trees before applying the constituentto-dependency conversion tool. This WSJ-trained labeller takes phrase-structure trees as input and labels th</context>
</contexts>
<marker>2007</marker>
<rawString>2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 915–932, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LeonBarrett SlavPetrov</author>
<author>andDanKlein RomainThibaux</author>
</authors>
<title>Learning accurate, compact, and interpretabletree annotation</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>433--440</pages>
<location>Sydney, Australia</location>
<marker>SlavPetrov, RomainThibaux, 2006</marker>
<rawString>SlavPetrov,LeonBarrett, RomainThibaux,andDanKlein. 2006. Learning accurate, compact, and interpretabletree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ines Rehbein</author>
<author>Josef van Genabith</author>
</authors>
<title>Treebank annotation schemes and parser evaluation for German</title>
<date>2007</date>
<booktitle>In Proceedings of the Joint EMNLP-CoNLL 2007</booktitle>
<pages>630--639</pages>
<location>Prague, Czech Republic</location>
<marker>Rehbein, van Genabith, 2007</marker>
<rawString>Ines Rehbein and Josef van Genabith. 2007. Treebank annotation schemes and parser evaluation for German. In Proceedings of the Joint EMNLP-CoNLL 2007, pages 630–639, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>GeoffreySampson</author>
<author>Anna Babarczy</author>
</authors>
<title>A test of the leaf-ancestor metric for parse accuracy</title>
<date>2002</date>
<booktitle>In Carroll et</booktitle>
<marker>GeoffreySampson, Babarczy, 2002</marker>
<rawString>GeoffreySampson and Anna Babarczy. 2002. A test of the leaf-ancestor metric for parse accuracy. In Carroll et al. (Carroll et al., 2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Wagner</author>
<author>Djam´e Seddah</author>
<author>Jennifer Foster</author>
<author>Josef vanGenabith</author>
</authors>
<title>C-structuresandf-structuresforthe British National Corpus</title>
<date>2007</date>
<booktitle>In Proceedings of the 12th International Workshop on Lexical Functional Grammar</booktitle>
<location>Stanford, CA</location>
<contexts>
<context>s per hour. Difficult parsing decisions were documented. Some pre-processing was carried on the BNC test sentences to ensure that they were tokenized in a similar way to Penn Treebank sentences (see (Wagner et al., 2007) for details). 4. Parser Evaluation 4.1. Parseval Evaluation The Parseval metric (Black et al., 1991) calculates precision and recall over the constituents in a parse tree. According, to the stronger</context>
</contexts>
<marker>Wagner, Seddah, Foster, vanGenabith, 2007</marker>
<rawString>Joachim Wagner, Djam´e Seddah, Jennifer Foster, and Josef vanGenabith. 2007. C-structuresandf-structuresforthe British National Corpus. In Proceedings of the 12th International Workshop on Lexical Functional Grammar, Stanford, CA.</rawString>
</citation>
</citationList>
</algorithm>

