Research Ramshaw and Marcus (1995), Mu˜noz et al.(1999), Argamon et al.(1998), Daelemans et al.(1999a) find NP chunks, using Wall Street Journal training material of about 9000 sentences.
P98-1010 W95-0107 W99-0707
The conversion program is the same as used for the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000).
W00-0726
task (Church, 1988; Brill, 1993; Ratnaparkhi, 1996; Daelemans et al., 1996), and reported errors in the range of 2–6% are common.
A88-1019 W96-0102 W96-0213
Our chunks and functions are based on the annotations in the third release of the Penn Treebank (Marcus et al., 1993).
J93-2004
To investigate the effect of using automatically assigned tags, we trained MBT, a memory-based tagger (Daelemans et al., 1996), on the training portions of our 10-fold crossvalidation experiment for the maximal data and let it predict tags for the test material.
W96-0102
Chunks as a separate level have also been used in Collins (1996) and Ratnaparkhi (1997).
P96-1025 W97-0301
Ferro et al.(1999) describe a system for finding grammatical relations in automatically tagged and manually chunked text.
W99-0706
Li and Roth (2001) show that for the chunking task it is specialized in, their shallow parser is more accurate and more robust than a general-purpose, i.e. full, parser.
W01-0706
Memory-based learning (Stanfill and Waltz, 1986; Aha et al., 1991; Daelemans et al., 1999b) is a supervised inductive learning algorithm for learning classification tasks.
W99-0707
In one experiment, it has to be performed on the basis of the “gold-standard”, assumed-perfect POS taken directly from the training data, the Penn Treebank (Marcus et al., 1993), so as to abstract from a particular POS tagger and to provide an upper bound.
J93-2004
In modern lexicalized parsers, POS tagging is often interleaved with parsing proper instead of being a separate preprocessing module (Collins, 1996; Ratnaparkhi, 1997).
P96-1025 W97-0301
Second, there are indications that increasing the training set of language processing tasks produces much larger performance gains than varying among algorithms at fixed training set sizes; moreover, these differences also tend to get smaller with larger data sets (Banko and Brill, 2001).
P01-1005
Church. 1988.
A88-1019
This and the previous parameter setting turned out best for a chunking task using the same algorithm as reported by Veenstra and van den Bosch (2000).
W00-0735
Daelemans. 1999.
W99-0707
Charniak. 2000.
A00-2018
Tjong Kim Sang and Buchholz (2000) give an overview of the CoNLL shared task of chunking.
W00-0726
Charniak (2000) notes that having his generative parser generate the POS of a constituent’s head before the head itself increases performance by 2 points.
A00-2018
Buchholz et al.(1999) achieve 71.2 F-score for grammatical relation assignment on automatically tagged and chunked text after training on about 40,000 Wall Street Journal sentences.
W99-0629
For the chunk part of the code, we adopt the “Inside”, “Outside”, and “Between” (IOB) encoding originating from (Ramshaw and Marcus, 1995).
W95-0107
